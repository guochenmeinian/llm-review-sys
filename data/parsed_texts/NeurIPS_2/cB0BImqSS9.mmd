# Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture

Daniel Y. Fu\({}^{1}\), Simran Arora\({}^{*,1}\), Jessica Grogan\({}^{*,2}\), Isys Johnson\({}^{*,2}\), Sabri Eyuboglu\({}^{*,1}\), Armin W. Thomas\({}^{*,3}\), Benjamin Spector\({}^{1}\), Michael Poli\({}^{1}\), Atri Rudra\({}^{2}\), Christopher Re\({}^{1}\)

\({}^{*}\)Equal Contribution. \({}^{1}\)Department of Computer Science, Stanford University.

\({}^{2}\)Department of Computer Science and Engineering, University at Buffalo, SUNY.

\({}^{3}\)Department of Psychology, Stanford University.

danfu@cs.stanford.edu, simarora@stanford.edu, {jrgrogan,isysjohn}@buffalo.edu, {eyuboglu,athms,bfs,poli}@stanford.edu, attri@buffalo.edu, chrismre@cs.stanford.edu

###### Abstract

Machine learning models are increasingly being scaled in both sequence length and model dimension to reach longer contexts and better performance. However, existing architectures such as Transformers scale quadratically along both these axes. We ask: are there performant architectures that can scale _sub-quadratically_ along sequence length and model dimension? We introduce Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension: Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms, achieves high hardware efficiency on GPUs, and scales sub-quadratically. As a proof of concept, we explore the performance of M2 in three domains: non-causal BERT-style language modeling, ViT-style image classification, and causal GPT-style language modeling. For non-causal BERT-style modeling, M2 matches BERT-base and BERT-large in downstream GLUE quality with up to 27% fewer parameters, and achieves up to 9.1\(\times\) higher throughput at sequence length 4K. On ImageNet, M2 outperforms ViT-b by 1% in accuracy, with only half the parameters. Causal GPT-style models introduce a technical challenge: enforcing causality via masking introduces a quadratic bottleneck. To alleviate this bottleneck, we develop a novel theoretical view of Monarch matrices based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic. Using this parameterization, M2 matches GPT-style Transformers at 360M parameters in pretraining perplexity on The PILE--showing for the first time that it may be possible to match Transformer quality without attention or MLPs.1

Footnote 1: Code is available at [https://github.com/HazyResearch/m2](https://github.com/HazyResearch/m2).

## 1 Introduction

Machine learning models in natural language processing and computer vision are being stretched to longer sequences and higher-dimensional representations to enable longer context and higher quality, respectively [6, 10, 62, 84]. However, existing architectures exhibit time and space complexities that grow quadratically in sequence length and/or model dimension--which limits context length and makes scaling expensive. For example, attention and MLP in Transformers scale quadratically in sequence length and model dimension [15]. In this paper, we explore a natural question: _can we find a performant architecture that is sub-quadratic in both sequence length and model dimension?_In our exploration, we seek a sub-quadratic primitive for both the sequence length and model dimension. Our framing takes inspiration from work such as MLP-mixer [73] and ConvMixer [74], which observed that many machine learning models operate by repeatedly _mixing_ information along the sequence and model dimension axes, and used a single operator for both axes. Finding mixing operators that are expressive, sub-quadratic, and hardware-efficient is challenging. For example, the MLPs in MLP-mixer and convolutions in ConvMixer are expressive, but they both scale quadratically in their input dimension [73, 74]. Several recent studies have proposed sub-quadratic sequence mixing with long convolutions or state space models [27, 64, 77]--both computed using the FFT--but these models have poor FLOP utilization (3-5% [28]) and maintain quadratic scaling in model dimension. Meanwhile, there has been promising work in sparsifying dense MLP layers without losing quality, but some of the models can actually be _slower_ than their dense counterparts, due to low hardware utilization [7, 8, 14, 26, 35].

We turn to an expressive class of sub-quadratic structured matrices called _Monarch matrices_[14] (Figure 1 left) to propose Monarch Mixer (M2). Monarch matrices are a family of structured matrices that generalize the fast Fourier transform (FFT) and have been shown to capture a wide class of linear transforms including Hadamard transforms, Toeplitz matrices [32], AFDF matrices [57], and convolutions. They are parameterized as the products of block-diagonal matrices, called _monarch factors_, interleaved with permutation. Their compute scales sub-quadratically: setting the number of factors to \(p\) results in computational complexity of \(O(pN^{(p+1)/p})\) in input length \(N\), allowing the complexity to interpolate between \(O(N\log N)\) at \(p=\log N\) and \(O(N^{3/2})\) at \(p=2\).2

Footnote 2: Monarch matrices were originally [14] parameterized with \(p=2\), but the general \(p\) case is a natural extension.

M2 uses Monarch matrices to mix information along the sequence and model dimension axes. It is both simple to implement and hardware-efficient: the block-diagonal Monarch factors can be computed efficiently on modern hardware using GEMMs (generalized matrix multiply algorithms). Our proof-of-concept implementation of an M2 layer, written in less than 40 lines of pure PyTorch (including imports), relies only on matrix multiplication, transpose, reshape, and elementwise products (see pseudocode in Figure 1 middle) and achieves 25.6% FLOP utilization3 for inputs of size 64K on an A100 GPU. On newer architectures such as the RTX 4090, a simple CUDA implementation achieves 41.4% FLOP utilization at the same size.

Footnote 3: For context, the most optimized attention implementations achieve 25% FLOP utilization, while unoptimized implementations of attention can have as low as 10% FLOP utilization [15].

Non-Causal SettingsAs a first proof of concept of M2, we evaluate how it compares to Transformers in terms of speed and quality in non-causal settings such as BERT-style masked language modeling [21] and ImageNet classification. We introduce M2-BERT, which replaces the attention blocks in BERT with bidirectional gated convolutions implemented using Monarch matrices and replaces the dense matrices in the MLP with Monarch matrices. M2-BERT reduces parameter count but maintains quality--matching BERT-base and BERT-large in downstream GLUE quality with 27% and 24% fewer parameters, respectively. Sub-quadratic scaling in sequence length enables high throughput at longer sequences--up to 9.1\(\times\) higher throughput at sequence length 4K than

Figure 1: Monarch matrices are a simple, expressive, and hardware-efficient class of sub-quadratic structured matrices. Monarch Mixer (M2) uses Monarch matrices to mix inputs first along the sequence dimension and then along the model dimension. See the Appendix for PyTorch implementation of an M2 layer.

HuggingFace BERT, and 3.1\(\times\) higher throughput at sequence length 8K than BERT optimized with FlashAttention [15].

For image classification, we adapt HyenaViT-b [64], an attention-free vision transformer based on gated convolutions. We replace the convolution operation with M2 primitives and replace the MLP layers with an M2 block as well. These changes reduce the parameter count compared to a ViT-b [22] model with the same model width and depth by a factor of 2. Surprisingly, despite this parameter reduction, we find that M2 slightly outperforms ViT-b and HyenaViT-b baselines, achieving 1% higher accuracy on ImageNet [18].

Causal SettingsCausal settings such as GPT-style [65] auto-regressive language modeling present a technical challenge: masking out the upper triangular elements in an attention matrix (or equivalent structure) introduces a quadratic bottleneck. To alleviate this quadratic bottleneck with Monarch matrices, we develop new theory to characterize which parameterizations of Monarch matrices maintain causality. To do so, we take a view of \(p\)-order Monarch matrix multiplication as \(p\)-variate polynomial evaluation and interpolation (e.g., \(p=2\) factors corresponds to bivariate polynomials, Figure 2 left). Using this view, we show that the M2 convolution shown in Figure 1 (middle) can be viewed as manipulation of modular polynomial multiplication. This result allows us to develop conditions (Theorem 3) under which M2 is causal. We can use this causal parameterization to outperform GPT-style language models on causal language modeling by 0.2 PPL points on the PILE at model size 360M-without using either attention or MLP blocks.

SummaryOverall, our results present a potential path to building machine learning models with sub-quadratic primitives. We hope our work can serve as a starting point to explore models that are more efficient in both sequence length and model dimension.

## 2 Preliminaries

In this section, we provide some background on the key components behind the cost of operations on GPUs, and then discuss the scaling characteristics of some common primitives used to mix information across the sequence dimension and model dimension in modern machine learning models.

GPU Accelerator Cost ModelWe provide a brief discussion of relevant factors affecting runtime performance of deep learning operations on GPUs. Depending on the balance of computation and memory accesses, operations can be classified as either compute-bound or memory-bound [44]. In compute-bound operations, the time accessing GPU memory is relatively small compared to the time spent doing arithmetic operations. Typical examples are matrix multiply with large inner dimension, and short convolution kernels with a large number of channels.

The speed of these operations is determined by the FLOP/s available on compute units, and the number of FLOPs necessary to complete the operation. In our paper, we exploit fast matrix multiply units such as tensor cores. On the A100, tensor cores can achieve 312 TFLOP/s in half-precision matrix multiply operations, while non-matrix multiply operations are limited to 19 TFLOP/s [59]. This trend began with tensor cores in the V100 [58], and is continuing into the next-generation H100 chips [60].

In memory-bound operations, the time taken by the operation is determined by the number of memory accesses, while time spent in computation is much smaller. Examples include most elementwise operations (e.g., activation, dropout) and reductions (e.g., sum, softmax, batch norm, layer norm).

The runtime of memory-bound operations is determined by the memory bandwidth of different layers of the _memory hierarchy_. GPU memory is large but relatively slow--up to 80 GB on A100, but with bandwidth of 2 TB/s [59]. Higher levels of the memory hierarchy such as caches are much smaller (20 MB) but an order of magnitude faster (19 TB/s).

\begin{table}
\begin{tabular}{r r r} \hline \hline Layer & FLOP Cost & Util \\ \hline MLP & \(N^{2}\) & 95.5\% \\ FlashAttn & \(N^{2}\) & 24.0\% \\ FFT & \(N\log N\) & 3.0\% \\ \hline M2 Conv & \(N^{3/2}\) & 41.4\% \\ \hline \hline \end{tabular}
\end{table}
Table 1: FLOP cost and utilization of various mixer layers, input dimension 64K on an RTX 4090.

Common Mixer PrimitivesTo help contextualize our work, we provide scaling and hardware utilization characteristics for a few common operations that are used to mix information in machine learning models, summarized in Table 1.

Transformers [75] use attention to mix information across the sequence dimension, and MLP blocks to mix information across the model dimension. Both of these blocks scale quadratically in input length. MLP layers are compute-bound, so they have high FLOP utilization out of the box. Attention blocks are memory-bound, so even the most optimized implementations such as FlashAttention [15] have relatively lower FLOP utilization.

Recent work has made progress towards attention-free models by replacing attention layers with long convolution layers, interleaved with elementwise gating [67, 68, 69, 27, 28, 36, 28, 36]. These layers are computed using FFT operations using the FFT convolution theorem: \(y=\mathbf{K}*\mathbf{X}=FFT^{-1}(FFT(\mathbf{X})*FFT(\mathbf{K}))\). While the FFT scales asymptotically well in \(O(N\log N)\), it is often memory-bound and thus has low FLOP utilization. In our work, we aim to construct a mixer that has both sub-quadratic scaling and high FLOP utilization.

## 3 Monarch Mixer

In this section, we recall Monarch matrices, introduce how M2 uses Monarch matrices to mix along the sequence and model dimensions, and benchmark a M2 convolution in terms of hardware utilization.

### Monarch Matrices

Monarch matrices [14] are a sub-quadratic class of structured matrices that are hardware-efficient and expressive. They can represent many linear transforms, including convolutions, Toeplitz-like transforms, low-displacement rank transforms, and orthogonal polynomials. Directly implementing these different structured transforms on GPUs as dense matrices can be inefficient. In contrast, their Monarch decompositions can be computed by interleaving matrix multiplications with tensor permutations.

A Monarch matrix \(\mathbf{M}\in\mathbb{R}^{N\times N}\) of order \(p\) is defined by the following:

\[\mathbf{M}=\left(\prod_{i=1}^{p}\mathbf{P}_{i}\mathbf{B}_{i}\right)\mathbf{P}_ {0}, \tag{1}\]

where each \(\mathbf{P}_{i}\) is related to the 'base \(\sqrt[p]{N}\)' variant of the bit-reversal permutation, and \(\mathbf{B}_{i}\) is a block-diagonal matrix with block size \(b\). Setting \(b=\sqrt[p]{N}\) achieves _sub-quadratic_ compute cost. For example, for \(p=2,b=\sqrt{N}\), Monarch matrices require \(O(N^{3/2})\) compute in sequence length \(N\).

In this paper, we use Monarch matrices to construct architectures that are sub-quadratic in both sequence length \(N\) and model dimension \(d\). We will often parameterize order-2 Monarch matrices, written as \(\mathbf{M}=\mathbf{PLPRP}\), where \(\mathbf{L}\) and \(\mathbf{R}\) are block-diagonal matrices (for "left" and "right"), and \(\mathbf{P}=\mathbf{P}_{2}=\mathbf{P}_{1}=\mathbf{P}_{0}\) is a permutation that reshapes the input to 2D, transposes it, and flattens it to 1D. A common case is to set \(\mathbf{L}=\mathbf{R}=(\mathbf{I}_{\sqrt{N}}\otimes\mathbf{F}_{\sqrt{N}})\), where \(\mathbf{F}_{\sqrt{N}}\) is a \(\sqrt{N}\) DFT matrix, and \(\otimes\) is the Kronecker product.

### Monarch Mixer Architecture

We describe how Monarch Mixer uses Monarch matrices and elementwise operations to construct sub-quadratic architectures (Figure 1 middle). We take a mixer view of model architectures, where each layer is a sequence of mixing operations across the sequence and the model dimension axes. Each layer takes as input a sequence of embeddings \(\mathbf{X}\in\mathbb{R}^{N\times d}\), and outputs a sequence \(\mathbf{Y}\in\mathbb{R}^{N\times d}\), where \(N\) is the sequence length, and \(d\) is the model dimension. For simplicity, we show the order-2 case here, though we can use higher-order blocks to scale to longer sequences and larger model dimensions.

Let \(\mathbf{M}_{1},\mathbf{M}_{2}\in\mathbb{R}^{N\times N}\) and \(\mathbf{M}_{3},\mathbf{M}_{4}\in\mathbb{R}^{d\times d}\) be order-2 Monarch matrices, let \(\mathbf{K}_{1}\in\mathbb{R}^{N\times d}\), let \(\sigma\) be an optional point-wise non-linearity (_e.g._ ReLU), and let \(\odot\) be elementwise multiplication. M2uses Monarch matrices to construct expressive architectures. For example, a convolutional block with a sparse MLP can be expressed as follows:

1. Mix along **sequence** axis: \[\mathbf{\widetilde{X}}=\mathbf{M}_{2}(\mathbf{K}_{1}\odot\mathbf{M}_{1}\mathbf{X})\] (2)
2. Mix along **embedding** axis: \[\mathbf{Y}^{\top}=\mathbf{M}_{4}\sigma(\mathbf{M}_{3}\mathbf{\widetilde{X}}^{ \top})\] (3)

When \(\mathbf{M}_{1}\) is set to the DFT and \(\mathbf{M}_{2}\) is set to the inverse DFT, Equation 2 exactly corresponds to a convolution with kernel \(\mathbf{K}_{1}\) parameterized in frequency space. Equation 3 corresponds to an MLP with the dense matrices replaced by Monarch matrices. More expressive layers are also easily expressible; for example, replacing Equation 2 with \(\mathbf{V}\odot\mathbf{M}_{2}(\mathbf{K}_{1}\odot\mathbf{M}_{1}(\mathbf{Q} \odot\mathbf{K}))\), where \(\mathbf{Q},\mathbf{K},\mathbf{V}\) are linear projections of \(\mathbf{X}\), reproduces a gated convolution block, as in [27; 28; 64].

The basic M2 layer is simple to implement; pseudocode is shown in Figure 1 (middle), and the Appendix gives an efficient implementation of M2 in under 40 lines of pure PyTorch (including imports). The convolution case with Monarch matrices fixed to DFT and inverse DFT matrices also admits implementations based on FFT algorithms [11].

### Architecture Benchmarks

We benchmark the efficiency of the \(\mathbf{M}(\mathbf{K}\odot\mathbf{M}\mathbf{X})\) convolution operator (Equation 2) implemented in a simple CUDA kernel (calling standard cuBLAS sub-routines [61]), as the dimension \(N\) increases. Equation 3 scales similarly, as dimension \(d\) increases. We keep the block size \(b\) fixed to \(\sqrt{N}\).

Table 2 shows the FLOP cost and utilization of a M2 operator as a function of the input size on an A100 as well as on an RTX 4090. On the A100, the operator is more dominated by the data movement costs of the permutation operations (see the Appendix for a roofline analysis). For longer inputs, the sub-quadratic scaling allows Monarch Mixer to outperform dense matrix multiplication. On the RTX 4090, which has a larger and faster L2 cache than the A100, we can manually optimize an implementation to amortize data movement costs.

## 4 Theoretical Analysis: M2 as Polynomial Multiplication

In this section, we develop theory to make the M2 layer causal in the input \(\mathbf{X}\)--e.g., ensure that an output \(Y_{i}\) of the M2 should only depend on \(X_{1}\),..., \(X_{i}\). Our approach involves interpreting Monarch matrix multiplication as multivariate polynomial evaluation and interpolation. We then show that an M2 convolution is equivalent to modular polynomial manipulation in a univariate basis.

The challenge is controlling the degrees of the resulting univariate polynomials, to prevent "underflow" under modular multiplication (see Figure 2 for an overview). Our key result is deriving sufficient conditions on the degrees of the bivariate polynomials defining the Monarch factors to prevent such underflow. We focus on the bivariate case (order \(p\) = 2) in the body, and give the general multivariate case in the Appendix. We present proof sketches in the main body, and leave proofs and additional results for the Appendix.

\begin{table}
\begin{tabular}{r c c c c} \hline \hline  & \(N\) & 4K & 16K & 64K & 256K \\ \hline Dense Matmul TFLOP Cost & 0.025 & 0.412 & 6.60 & 106.0 \\ M2 TFLOP Cost & 0.002 & 0.013 & 0.103 & 0.824 \\ \hline Dense FLOP Utilization (A100) & 63.0\% & 78.0\% & 80.0\% & OOM \\ M2 FLOP Utilization (A100) & 4.78\% & 12.7\% & 25.6\% & 42.8\% \\ Wall-Clock Speedup (A100) & 1.2\(\times\) & 5.1\(\times\) & 20.6\(\times\) & \textgreater{}55.0\(\times\) \\ \hline Dense FLOP Utilization (4090) & 74.6\% & 96.7\% & 98.0\% & OOM \\ M2 FLOP Utilization (4090) & 11.1\% & 32.1\% & 41.4\% & 53.7\% \\ Wall-Clock Speedup (4090) & 2.2\(\times\) & 10.5\(\times\) & 27.0\(\times\) & \textgreater{}69.1\(\times\) \\ \hline \hline \end{tabular}
\end{table}
Table 2: FLOP cost and utilization of M2 compared to dense MLP at different input sizes \(N\), with block size \(\sqrt{N}\), on an A100 and RTX 4090.

Monarch Multiplication as Polynomial EvaluationFirst, we show that order-2 Monarch matrix-vector multiplication \(\mathbf{M}\cdot\mathbf{u}\) is equivalent to bivariate polynomial evaluation.

Fix a Monarch matrix \(\mathbf{M}\in\mathbb{R}^{N\times N}=\mathbf{PLPRP}\), for two block-diagonal matrices \(\mathbf{L}\) and \(\mathbf{R}\) with blocks of size \(b=\sqrt{N}\). We can interpret Monarch matrices as bivariate polynomial evaluation by setting \(A=\{\omega_{0},\ldots,\omega_{b-1}\}\) as a set of evaluation points (e.g., the \(b\)th roots of unity), and letting \(\{\ell_{0}(X,Y),\ldots,\ell_{b-1}(X,Y)\}\), \(\{r_{0}(Y),\ldots,r_{N-1}(Y)\}\) be sets of basis polynomials with individual degrees of \(X,Y\) being \(<\sqrt{N}\). The values of \(\{\ell_{0}(X,Y),\ldots,\ell_{b-1}(X,Y)\}\) evaluated on \(A^{2}\) determine the entries of \(\mathbf{L}\), and the values of \(\{r_{0}(Y),\ldots,r_{N-1}(Y)\}\) evaluated on \(A\) determine the entries of \(\mathbf{R}\). We give the mapping from \(\ell,r,\) and \(A\) to \(\mathbf{L}\) and \(\mathbf{R}\) in the Appendix.

Then, matrix-vector multiplication between \(\mathbf{M}\) and a vector \(\mathbf{u}\) is equivalent to polynomial evaluation of the basis functions \(\ell,r\) on the evaluation points \(A^{2}\):

**Theorem 1**.: _Let \(m(j)=j\mod\sqrt{N}\). For any vector \(\mathbf{u}\in\mathbb{R}^{N}\), \(\mathbf{M}\mathbf{u}\) is a bivariate polynomial \(u(X,Y)\) evaluated at \(A^{2}\), with \(u(X,Y)=\sum_{j=0}^{N-1}u_{j}f_{j}(X,Y),\) where \(f_{j}(X,Y)=\ell_{m(j)}(X,Y)r_{j}(Y)\)._

Monarch Inverse as Polynomial InterpolationNext, we exploit the fact that Monarch inverse multiplication \(\mathbf{M}^{-1}\cdot\mathbf{u}\) is equivalent to polynomial interpolation in the basis polynomials of \(\mathbf{M}\).

**Theorem 2**.: _Let \(\mathbf{M}_{0},\mathbf{M}_{1},\mathbf{M}_{2}\) be Monarch matrices, and let \(A\) be the set of \(\sqrt{N}\) roots of unity. Then, the operation_

\[\mathbf{f}=\mathbf{M}_{0}^{-1}\left((\mathbf{M}_{1}\mathbf{k})\odot(\mathbf{M }_{2}\mathbf{u})\right). \tag{4}\]

_is equivalent to representing the polynomial_

\[h(X,Y)=k(X,Y)u(X,Y)\mod(X^{\sqrt{N}}-1,Y^{\sqrt{N}}-1)\]

_in terms of the basis polynomials \(\ell,r\) corresponding to \(\mathbf{M}_{0}\), and where \(k(X,Y)\) and \(u(X,Y)\) are the polynomials corresponding to \(\mathbf{M}_{1}\mathbf{k}\) and \(\mathbf{M}_{2}\mathbf{u}\), respectively._

The above follows from Theorem 1 and the fact that Monarch matrix-vector multiplication with an inverse Monarch matrix is equivalent to polynomial interpolation in a given basis. The \(\mod\) part comes from the fact that \(A\) is the set of roots of the polynomial \(Z^{\sqrt{N}}-1\).

Causal Monarch MapsNow, we give a class of Monarch matrices from which we can build a causal map. First, we define a polynomial with _minimum_ degree \(j\):

**Definition 1**.: _A polynomial of minimum degree \(j\) (and maximum degree \(N-1\)) is defined as \(\bar{q}_{j}(Z)=\sum_{a=j}^{N-1}\bar{q}_{j}[a]Z^{a}\)._

To ensure causality, we first convert the bivariate polynomial basis into a univariate basis, and then we expand the degree of the univariate polynomial. The resulting univariate polynomial multiplication is naturally causal (exploiting similar properties as the causal FFT convolution).

We use the Kronecker substitution \((X\gets Z,Y\gets Z^{\sqrt{N}})\) to convert the bivariate polynomial basis into a univariate basis:

\[q_{j}(Z)=\ell_{m(j)}(Z)r_{j}\left(Z^{\sqrt{N}}\right), \tag{5}\]

Figure 2: Monarch multiplication can be interpreted as polynomial evaluation and interpolation. We derive sufficient conditions on the polynomial formulation of Monarch matrices for M2 to be causal.

[MISSING_PAGE_EMPTY:7]

match BERT-base in GLUE quality with 27% fewer parameters--or outperform BERT-base in quality by 1.3 points when parameter matched. M2-BERT-large matches BERT-large with 24% fewer parameters, and outperforms by 0.7 points when parameter matched.

GPU Throughput by Sequence LengthNext, we evaluate throughput of M2-BERT models by sequence length, compared to HuggingFace implementations of BERT, as well as optimized implementations of BERT running FlashAttention [15]. Table 5 shows forward throughput for BERT-base equivalent models, and the appendix shows throughput for BERT-large (where the performance trends are similar). Inference times are reported in tokens/ms on an A100-40GB GPU. M2-BERT-base achieves higher throughput than even highly-optimized BERT models, and up to 9.1\(\times\) faster throughput than a standard HuggingFace implementation at sequence length 4K.

CPU Inference LatencyFinally, we report CPU inference latency for M2-BERT-base (80M) compared to BERT-base, running direct PyTorch implementations for both. In short sequences, the impacts of data locality still dominate the FLOP reduction, and operations such as filter generation (which are not present in BERT) pay a higher cost. Starting at sequences 1K and longer, M2-BERT-base starts to have speedup over BERT-base, up to 6.5\(\times\) at sequence length 8K. We believe further optimization and applying IO-aware principles can further improve CPU performance.

### Image Classification

To validate that our methods generalize to images as well as language for non-causal modeling, we next evaluate M2 on image classification. We compare M2 to ViT-style models and recent work, HyenaViT-b [64], which uses gated long convolutions to replace the attention layers in ViT-b. In our work, M2-ViT builds off HyenaViT-b and replaces the long convolutions with the M2 operator in Equation 2 (again setting the Monarch matrices to the DFT and inverse DFT). We replace the MLP blocks in HyenaViT-b with block-diagonal matrices, similarly to M2-BERT. Appendix B additionally compares M2 to the Swin-family of architectures [50; 51].

Table 7 shows the performance of Monarch Mixer against ViT-b, HyenaViT-b, and ViT-b-Monarch (which replaces the MLP blocks of standard ViT-b with Monarch matrices) on ImageNet-1k. Monarch Mixer outperforms the other models with only half the parameters of the original ViT-s model. Surprisingly, Monarch Mixer also outperforms ResNet-152, with fewer parameters--even though the latter was explicitly designed for ImageNet performance.

### Causal Language Modeling

GPT-style causal language modeling is a critical application for Transformers [6; 31; 43]. We introduce M2-GPT, a M2-based architecture for causal language modeling. For the sequence mixer, M2-GPT combines the convolutional filter from Hyena [64], the state-of-the-art attention-free

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Model** & **GLUE Score** & \(\Delta\) **Params** & \(\Delta\) **GLUE Score** \\ \hline BERT-large (340M) & 82.1 & -0\% & +0.0 \\ \hline M2-BERT-large (260M) & 82.2 & -24\% & +0.1 \\ M2-BERT-large (341M) & **82.8** & +0.2\% & +0.7 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Average GLUE Score for M2-BERT-large compared to BERT-large [20], along with change in parameters and GLUE score.

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Model** & **GLUE Score** & \(\Delta\) **Params** & \(\Delta\) **GLUE Score** \\ \hline BERT-base (110M) & 79.6 & -0\% & +0.0 \\ \hline M2-BERT-base (80M) & 79.9 & -27\% & +0.3 \\ M2-BERT-base (110M) & **80.9** & -0\% & +1.3 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Average GLUE Score for M2-BERT-base compared to BERT-base [20], along with change in parameters and GLUE score.

language model, with parameter sharing across multiple heads from H3 [27]. We use the causal parameterization of Equation 2 to replace the FFT in these architectures, and we remove the MLP layers entirely. The resulting architecture is entirely attention- and MLP-free.

We pretrain M2-GPT on the PILE, a standard dataset for causal language modeling. Following prior work [64, 28], we train models at two model sizes, with varying amounts of training data--decaying the learning rate appropriately for each experiment. Table 8 shows the results. Even though our model is attention- and MLP-free, it outperforms both Transformers and Hyena in perplexity on pretraining. These results suggest that radically different architectures than Transformers may be performant on causal language modeling.

## 6 Related Work

Long ConvolutionsRecent work proposes to use long convolution layers as a replacement for the Transformer attention layers in sequence modeling [64, 67, 68, 69, 28]. Many of these models rely on the FFT convolution theorem to compute the long convolutions. We build on the insights in many of these architectures in constructing our M2 architectures, and additionally replaces the FFT operations with Monarch matrices.

Our work is also related to a rich literature in convolutions in other bases, such as Chebyshev bases [82] or orthogonal polynomial bases [34]. These approaches have analogues in our multivariate analysis; replacing the basis polynomials of the Monarch matrices in Monarch Mixer may be able to approximate some of these operations. An interesting question for future work would be to study how well our techniques and concerns about causality and hardware utilization translate to these alternative convolution bases.

Optimization of deep learning primitivesThere is a rich history of the optimization of deep learning primitives, as accelerating their performance can yield substantial savings in compute and cost for large models. There are many approaches to speed up these operations, but they usually either reduce data movement or compute.

Reducing data movement: In many applications, the major bottleneck is the storage and movement of large amounts of memory. One popular approach to reducing data movement is checkpointing, wherein one stores fewer intermediate results and recomputes the others on-the-fly where they are needed, trading additional compute for memory [78, 46]. Another approach is kernel fusion, wherein algorithms initially described as sequential steps can often be fused in ways that improve their properties. For example, it is generally faster to implement a dot-product through a multiply

\begin{table}
\begin{tabular}{r c c c c c} \hline \hline Model & 512 & 1024 & 2048 & 4096 & 8192 \\ \hline BERT-base (110M) & **182** & 389 & 918 & 2660 & 11820 \\ M2-BERT-base (80M) & 289 & **361** & **651** & **948** & **1820** \\ \hline Speedup & 0.6\(\times\) & 1.1\(\times\) & 1.4\(\times\) & 2.8\(\times\) & 6.5\(\times\) \\ \hline \hline \end{tabular}
\end{table}
Table 6: CPU inference latency in milliseconds with a batch size of 1 at varied input sequence lengths. Measurements averaged over 10 examples on a 48 vCPU, 96 GB RAM instance from the GCP n2-standard-48 series, which runs Intel Cascade Lake processors. This is based on the protocol in [29].

\begin{table}
\begin{tabular}{r c c c c c} \hline \hline  & **Model** & **512** & **1024** & **2048** & **4096** & **8192** \\ \hline HF BERT-base (110M) & 206.1 & 130.8 & 71.3 & 39.0 & OOM \\ FlashAttention BERT-base (110M) & 367.4 & 350.1 & 257.2 & 179.1 & 102.4 \\ M2-BERT-base (80M) & **386.3** & **380.7** & **378.9** & **353.9** & **320.1** \\ \hline M2 Speedup over HF BERT-base (110M) & 1.9\(\times\) & 2.9\(\times\) & 5.2\(\times\) & 9.1\(\times\) & \(-\) \\ \hline \hline \end{tabular}
\end{table}
Table 5: Throughput in tokens/ms by context length for M2-BERT-base (80M) compared to BERT-base.

accumulate rather than first multiplying and then accumulating. Recently, libraries such as PyTorch 2.0 [63] have added kernel fusion capabilities, although the very best performance usually still arises from handwritten kernels. Third, in order to better exploit memory locality, it is often fastest to load small blocks of memory, do intensive computation on them, and then write the results a tile at a time [83]. Finally, many algorithms also have hand-optimizations that can remove unnecessary computation or memory accesses [55].

Efficient algorithms usually make use of a combination of these techniques. For example, FlashAttention [15] uses all four to dramatically decrease both the latency and memory consumption of multi-head attention. Though we have made a modest effort to implement Monarch Mixer efficiently, we think it likely that Monarch Mixer could be further optimized by these techniques.

Reducing flops: A first target for optimization is the multi-layer perceptron (MLP), owing to its ubiquity. A variety of structured sparse factorizations exist, many of which we draw on in this work [7, 11, 14, 16, 17, 19, 26, 91]. Attention is also a popular target for optimization. Recently, a plethora of sub-quadratic approximations of attention have emerged, that aim to approximate attention to reduce its quadratic complexity. Some methods rely on sparsification, relying on the fact that the attention matrix is extremely sparse at long sequence lengths [2, 23, 24, 42, 53]. Others use low-rank approximations of the attention matrix [13, 79, 91] or kernel methods instead [9, 41]. A subset use a combination of these techniques, such as [8, 72]. Finally, a third category of methods [27, 64] aim to replace attention entirely, relying on state-space models [33].

## 7 Discussion and Conclusion

We explore Monarch Mixer (M2), a new architecture that is sub-quadratic in both sequence length and model dimension and is hardware-efficient on modern accelerators. We motivate M2 from both theoretical and systems performance perspectives and conduct a preliminary proof-of-concept investigation into performance on masked language modeling, image classification, and causal language modeling.

While our initial results are promising, our work is only a first step in this direction. The M2 layer can likely be further optimized with systems optimization techniques such as kernel fusion. Our work has also not been optimized for inference like more well-established models such as Transformers, or even more recent models such as state space models. It also remains to be seen whether M2 layers can have as widespread applicability as Transformers. We hope that these can be fruitful directions for future work.

A discussion of broader impacts can be found in the Appendix.

\begin{table}
\begin{tabular}{r r r r} \hline \hline Model & Top-1\% & Top-5\% & Description \\ \hline ResNet-152 (60M) & 78.6 & 94.3 & ConvNet, MLP \\ \hline ViT-b (87M) & 78.5 & 93.6 & Attention, MLP \\ ViT-b + Monarch (33M) & 78.9 & 94.2 & Attention, MLP-Free \\ HyenaViT-b (88M) & 78.5 & 93.6 & Attention-Free, MLP \\ \hline M2-ViT-b (45M) & **79.5** & **94.5** & Attention-Free, MLP-Free \\ \hline \hline \end{tabular}
\end{table}
Table 7: Accuracy on ImageNet-1k. ResNet-152 provided for reference.

\begin{table}
\begin{tabular}{r r r r r} \hline \hline Model & 5B & 10B & 15B & Description \\ \hline Transformer (125M) & 13.3 & 11.9 & 11.2 & Attention, MLP \\ Hyena (155M) & 13.1 & 11.8 & 11.1 & Attention-Free, MLP \\ M2-GPT (145M) & **12.9** & **11.6** & **10.9** & Attention-Free, MLP-Free \\ \hline Transformer (355M) & 11.4 & 9.8 & 9.1 & Attention, MLP \\ Hyena (360M) & 11.3 & 9.8 & 9.2 & Attention-Free, MLP \\ M2-GPT (360M) & **11.0** & **9.6** & **9.0** & Attention-Free, MLP-Free \\ \hline \hline \end{tabular}
\end{table}
Table 8: Perplexity on the PILE when trained for different numbers of tokens.

## Acknowledgments

We gratefully acknowledge the support of DARPA under Nos. FA86501827865 (SDH) and FA86501827882 (ASED); NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No. N000141712266 (Unifying Weak Supervision); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Microsoft Azure, Swiss Re, Brown Institute for Media Innovation, Department of Defense (DoD) through the National Defense Science and Engineering Graduate Fellowship (NDSEG) Program, Fannie and John Hertz Foundation, National Science Foundation Graduate Research Fellowship Program, Texas Instruments Stanford Graduate Fellowship in Science and Engineering, and members of the Stanford DAWN project: Teradata, Facebook, Google, Ant Financial, NEC, VMWare, and Infosys. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government. JG and AR's work is supported by NSF grant# CCF-2247014. IJ's work is supported by an NSF Graduate Fellowship.

## References

* [1]I. Beltagy, K. Lo, and A. Cohan (2019) Scibert: a pretrained language model for scientific text. arXiv preprint arXiv:1903.10676. Cited by: SS1.
* [2]I. Beltagy, M. E. Peters, and A. Cohan (2020) Longformer: the long-document transformer. arXiv preprint arXiv:2004.05150. Cited by: SS1.
* [3]E. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell (2021) On the dangers of stochastic parrots: can language models be too big?. In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21, New York, NY, USA, pp. 610-623. External Links: ISBN 978-1-4503-3874-3, Link, Document Cited by: SS1.
* [4]L. Beyer, X. Zhai, and A. Kolesnikov (2022) Better plain vit baselines for imagenet-1k. arXiv preprint arXiv:2205.01580. Cited by: SS1.
* [5]R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al. (2021) On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258. Cited by: SS1.
* [6]T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al. (2020) Language models are few-shot learners. Advances in neural information processing systems33, pp. 1877-1901. Cited by: SS1.
* [7]B. Chen, T. Dao, K. Liang, J. Yang, Z. Song, A. Rudra, and C. Re (2021) Pixelated butterfly: simple and efficient sparse training for neural network models. External Links: 2108.07258 Cited by: SS1.
* [8]B. Chen, T. Dao, E. Winsor, Z. Song, A. Rudra, and C. Re (2021) Scatterbrain: unifying sparse and low-rank attention. In Advances in Neural Information Processing Systems (NeurIPS), Cited by: SS1.
* [9]K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, et al. (2020) Rethinking attention with performers. arXiv preprint arXiv:2009.14794. Cited by: SS1.
* [10]A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann, et al. (2022) Palm: scaling language modeling with pathways. arXiv preprint arXiv:2204.02311. Cited by: SS1.
* [11]J. W. Cooley and J. W. Tukey (1965) An algorithm for the machine calculation of complex fourier series. Mathematics of computation19 (90), pp. 297-301. Cited by: SS1.

* [12] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops_, pages 702-703, 2020.
* [13] Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. _Advances in neural information processing systems_, 33:4271-4282, 2020.
* [14] Tri Dao, Beidi Chen, Nimit S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, and Christopher Re. Monarch: Expressive structured matrices for efficient and accurate training. In _International Conference on Machine Learning_. PMLR, 2022.
* [15] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher Re. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In _Advances in Neural Information Processing Systems_, 2022.
* [16] Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher Re. Learning fast algorithms for linear transforms using butterfly factorizations, 2020.
* [17] Tri Dao, Nimit S. Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan Leszczynski, Atri Rudra, and Christopher Re. Kaleidoscope: An efficient, learnable representation for all structured linear maps, 2021.
* [18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* [19] Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing performance. _arXiv preprint arXiv:1907.04840_, 2019.
* [20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* [21] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In _arXiv:1810.04805_, 2019.
* [22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. _arXiv preprint arXiv:2010.11929_, 2020.
* [23] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In _International Conference on Machine Learning_, pages 5547-5569. PMLR, 2022.
* [24] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. _The Journal of Machine Learning Research_, 23(1):5232-5270, 2022.
* [25] Wikimedia Foundation. Wikimedia downloads.
* [26] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. _arXiv preprint arXiv:1803.03635_, 2018.
* [27] Daniel Y Fu, Tri Dao, Khaled K Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry huppos: Towards language modeling with state space models. _International Conference on Learning Representations_, 2023.

* [28] Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher Re. Simple hardware-efficient long convolutions for sequence modeling. _International Conference on Machine Learning_, 2023.
* part 1, 2021.
* [30] Jonas Geiping and Tom Goldstein. Cramming: Training a language model on a single gpu in one day. _arXiv:2212.14034v1_, 2022.
* [31] Google. Bard, [https://bard.google.com/](https://bard.google.com/). 2023.
* [32] Robert M Gray et al. Toeplitz and circulant matrices: A review. _Foundations and Trends(r) in Communications and Information Theory_, 2(3):155-239, 2006.
* [33] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. _arXiv preprint arXiv:2111.00396_, 2021.
* [34] Nicholas Hale and Alex Townsend. An algorithm for the convolution of legendre series. _SIAM Journal on Scientific Computing_, 36(3):A1207-A1220, 2014.
* [35] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. _arXiv preprint arXiv:1510.00149_, 2015.
* [36] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. _arXiv preprint arXiv:2209.12951_, 2022.
* [37] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshminarayanan. Augmix: A simple data processing method to improve robustness and uncertainty. _arXiv preprint arXiv:1912.02781_, 2019.
* [38] Peter Izsak, Moshe Berchansky, and Omer Levy. How to train bert with an academic budget. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 10644-10652, 2021.
* [39] Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is bert really robust? a strong baseline for natural language attack on text classification and entailment. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 8018-8025, 2020.
* [40] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. Spanbert: Improving pre-training by representing and predicting spans. _Transactions of the Association for Computational Linguistics_, 8:64-77, 2020.
* [41] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In _International Conference on Machine Learning_, pages 5156-5165. PMLR, 2020.
* [42] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. _arXiv preprint arXiv:2001.04451_, 2020.
* [43] Jan Kocon, Igor Cichecki, Olivier Kaszyca, Mateusz Kochanek, Dominika Szydlo, Joanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz, et al. Chatgpt: Jack of all trades, master of none. _arXiv preprint arXiv:2302.10724_, 2023.
* [44] Elias Konstantinidis and Yiannis Cotronis. A practical performance model for compute and memory bound gpu kernels. In _2015 23rd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing_, pages 651-658. IEEE, 2015.
* [45] MV Koroteev. Bert: a review of applications in natural language processing and understanding. _arXiv preprint arXiv:2103.11943_, 2021.

* [46] Mitsuru Kusumoto, Takuya Inoue, Gentaro Watanabe, Takuya Akiba, and Masanori Koyama. A graph theoretic framework of recomputation algorithms for memory-efficient backpropagation. _Advances in Neural Information Processing Systems_, 32, 2019.
* [47] Lagrange polynomial. Lagrange polynomial -- Wikipedia, the free encyclopedia, 2005. [https://en.wikipedia.org/wiki/Lagrange_polynomial](https://en.wikipedia.org/wiki/Lagrange_polynomial).
* [48] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. Biobert: a pre-trained biomedical language representation model for biomedical text mining. _Bioinformatics_, 36(4):1234-1240, 2020.
* [49] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. _arXiv preprint arXiv:1907.11692_, 2019.
* [50] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, and Baining Guo. Swin transformer v2: Scaling up capacity and resolution. In _International Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.
* [51] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2021.
* [52] Xiaofei Ma, Zhiguo Wang, Patrick Ng, Ramesh Nallapati, and Bing Xiang. Universal text representation from bert: An empirical study. _arXiv preprint arXiv:1910.07973_, 2019.
* [53] Xuezhe Ma, Xiang Kong, Sinong Wang, Chunting Zhou, Jonathan May, Hao Ma, and Luke Zettlemoyer. Luna: Linear unified nested attention. _Advances in Neural Information Processing Systems_, 34:2441-2453, 2021.
* [54] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: moving average equipped gated attention. _arXiv preprint arXiv:2209.10655_, 2022.
* [55] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. _arXiv preprint arXiv:1805.02867_, 2018.
* [56] Derek Miller. Leveraging bert for extractive text summarization on lectures. _arXiv preprint arXiv:1906.04165_, 2019.
* [57] Marcin Moczulski, Misha Denil, Jeremy Appleyard, and Nando de Freitas. Acdc: A structured efficient linear layer. _arXiv preprint arXiv:1511.05946_, 2015.
* [58] NVIDIA. Nvidia Tesla V100 GPU architecture, 2017.
* [59] NVIDIA. Nvidia A100 tensor core GPU architecture, 2020.
* [60] NVIDIA. Nvidia H100 tensor core GPU architecture, 2022.
* [61] NVIDIA. cuBLAS, 2023.
* [62] OpenAI. Gpt-4 technical report, 2023.
* [63] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. _Advances in neural information processing systems_, 32, 2019.
* [64] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. Hyena hierarchy: Towards larger convolutional language models. _International Conference on Machine Learning_, 2023.

* [65] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018.
* [66] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _arXiv preprint arXiv:1910.10683_, 2019.
* [67] David W Romero, R Bruintjes, Erik J Bekkers, Jakub M Tomczak, Mark Hoogendoorn, and JC van Gemert. Flexconv: Continuous kernel convolutions with differentiable kernel sizes. In _10th International Conference on Learning Representations_, 2022.
* [68] David W Romero, David M Knigge, Albert Gu, Erik J Bekkers, Efstratios Gavves, Jakub M Tomczak, and Mark Hoogendoorn. Towards a general purpose cnn for long range dependencies in \(\{N\}\) d. _arXiv preprint arXiv:2206.03398_, 2022.
* [69] David W Romero, Anna Kuzina, Erik J Bekkers, Jakub Mikolaj Tomczak, and Mark Hoogendoorn. Ckconv: Continuous kernel convolution for sequential data. In _International Conference on Learning Representations_, 2021.
* [70] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. How to train your vit? data, augmentation, and regularization in vision transformers. _arXiv preprint arXiv:2106.10270_, 2021.
* [71] G. Szego. _Orthogonal Polynomials_. Number v.23 in American Mathematical Society colloquium publications. American Mathematical Society, 1967.
* [72] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. _ACM Computing Surveys_, 55(6):1-28, 2022.
* [73] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al. Mlp-mixer: An all-mlp architecture for vision. _Advances in neural information processing systems_, 34:24261-24272, 2021.
* [74] Asher Trockman and J Zico Kolter. Patches are all you need? _Transactions on Machine Learning Research_, 2023.
* [75] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. volume 30, 2017.
* [76] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. _arXiv:1804.07461_, 2018.
* [77] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. _arXiv preprint arXiv:2212.10544_, 2022.
* [78] Qipeng Wang, Mengwei Xu, Chao Jin, Xinran Dong, Jinliang Yuan, Xin Jin, Gang Huang, Yunxin Liu, and Xuanzhe Liu. Melon: Breaking the memory wall for resource-efficient on-device machine learning. In _Proceedings of the 20th Annual International Conference on Mobile Systems, Applications and Services_, pages 450-463, 2022.
* [79] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. _arXiv preprint arXiv:2006.04768_, 2020.
* [80] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. Ethical and social risks of harm from language models. _arXiv preprint arXiv:2112.04359_, 2021.
* [81] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Perric Cistac, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations_, 2020.

* [82] Kuan Xu and Ana F. Loureiro. Spectral approximation of convolution operators. _SIAM Journal on Scientific Computing_, 40(4):A2336-A2355, 2018.
* [83] Yufan Xu, Saurabh Raje, Atanas Rountev, Gerald Sabin, Aravind Sukumaran-Rajam, and P Sadayappan. Training of deep learning pipelines on memory-constrained gpus via segmented fused-tiled execution. In _Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction_, pages 104-116, 2022.
* [84] Lili Yu, Daniel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. Megabyte: Predicting million-byte sequences with multiscale transformers, 2023.
* [85] Shanshan Yu, Jindian Su, and Da Luo. Improving bert-based text classification with auxiliary sentence and domain knowledge. _IEEE Access_, 7:176600-176612, 2019.
* [86] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 558-567, 2021.
* [87] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 6023-6032, 2019.
* [88] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. _arXiv preprint arXiv:1710.09412_, 2017.
* [89] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. _arXiv preprint arXiv:1904.09675_, 2019.
* [90] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 13001-13008, 2020.
* [91] Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro. Long-short transformer: Efficient transformers for language and vision. _Advances in Neural Information Processing Systems_, 34:17723-17736, 2021.
* [92] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In _The IEEE International Conference on Computer Vision (ICCV)_, December 2015.

### Author Contributions

**D.Y.F.**: Conceptualized the research; coordinated collaborations; developed M2 architectures; led experimental and implementation efforts; assisted in development of theoretical results; coordinated writing.
**S.A.**: Worked on developing M2 architectures; worked on implementing and conducting BERT experiments; worked on implementing and performing CPU experiments; assisted in writing and framing the work.
**J.G.**: Led development of theory and causal algorithms; wrote Appendix D.
**I.J.**: Led development of theory and causal algorithms; wrote Appendix D.
**S.E.**: Worked on writing and framing the work; assisted in development of M2 architectures; assisted in the optimized M2 implementation; conducted mixer benchmarks; assisted with BERT experiments; conducted Swin ImageNet experiments.
**A.W.T.**: Conducted ViT experiments; assisted in writing.
**B.S.**: Assisted in optimized M2 implementation; conducted mixer benchmarks; assisted in writing.
**M.P.**: Assisted in development of M2-GPT architecture.
**A.R.**: Supervised theory development; developed proofs; reviewed manuscript.
**C.R.**: Supervised research; reviewed manuscript.

_Simran Arora, Jessica Grogan, Isys Johnson, Sabri Eyuboglu, and Armin Thomas contributed equally to this work._

## Appendix A Broader Impacts

Our work seeks to understand the fundamental capabilities and limitations of newly-emerging model architectures. As the amount of data and model size grows, we also seek to understand how to make training these models more efficient, both in terms of the amount of training context and the model size. This potentially connects to energy savings during model development and deployment, as well as making machine learning models accessible to a larger population of people.

However, as with any machine learning models, developing new techniques may impact a wide range of applications, each with potential benefits and harms. Making language model training cheaper and longer context may make it cheaper to spread disinformation. Similarly, improving the efficiency of model training may not reduce the overall environmental footprint of training, since the same resources may be used to train more models, or train the same models for longer. While our work makes partial progress on the fronts of efficiency and understanding, it does not explicitly address the issues of fairness and bias in language models. In addition, our work demonstrates a proof-of-concept; it has error modes, and we recognize the inherent risks of training and using machine learning models, including language models. Detailed discussions of these risks are in [3, 5, 80].

## Appendix B Additional Experiments

### Per-Task GLUE Numbers

We report full GLUE numbers for M2-BERT-base and M2-BERT-large in Table 9.

### Additional Throughput Results

We report the throughput of M2-BERT-base (80M) compared to BERT models of the same size (BERT-base with fewer parameters), as well as the throughput of M2-BERT-large (260M) compared to BERT-large.

Table 10 compares the performance of M2-BERT-base (80M) to BERT models parameter-matched to 80M parameters. M2 is slower than FlashAttention for sequence lengths 512 and 1K, but outperforms FlashAttention starting at sequence length 2K. We believe further optimization of the M2 kernel can close the gap to FlashAttention for short sequences.

Table 11 compares M2-BERT-large (260M) to BERT-large. Trends are mostly similar to comparisons against BERT-base; M2 nearly matches FlashAttention at sequence length 512, and outperforms it for sequence length 1K and longer. We also see up to 4.3\(\times\) speedup over HuggingFace BERT-large at sequence length 2K.

### ImageNet Comparison against Swin

Table 12 reports the results of replacing attention and MLP in Swin-V2 using M2 as a drop-in replacement. Surprisingly, Swin-M2 outperforms Swin-MLP-B, is competitive with Swin-V1-B, and comes within 1 point of Swin-V2-B, even without any hyperparameter tuning or architecture adjustment from the ViT formula. We expect that performance may improve further with hyperparameter tuning specific to M2.

### Speech Applications

Table 13 presents the performance of M2 on Speech Commands-10, a speech classification task over raw 1-second clips sampled at 16 kHz. M2 is competitive with state-of-the-art architectures on this task.

### Cifar10

Table 14 shows the performance of Monarch Mixer on CIFAR10. The trends are largely the same as on ImageNet.

### Learnable Monarch Matrices in Sequence Mixer

In most of our models, we have used fixed Monarch matrices for the sequence mixer, and learnable Monarch matrices for the dimension mixer. Table 15 presents an experiment evaluating using learnable Monarch matrices for the sequence mixer on the sequential CIFAR task. We use a non-gated convolutional architecture based off long convolutions, as presented in [28]. Learning the Monarch matrices in the sequence mixer yields 1.5 points of lift.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multicolumn{1}{c}{\multirow{2}{*}{Model}} & \multicolumn{1}{c}{\multirow{2}{*}{**512**}} & \multicolumn{1}{c}{\multirow{2}{*}{**1024**}} & \multicolumn{1}{c}{\multirow{2}{*}{**2048**}} & \multicolumn{1}{c}{\multirow{2}{*}{**4096**}} & \multicolumn{1}{c}{\multirow{2}{*}{**8192**}} \\ \cline{2-2} \cline{5-6}  & HF BERT (79M) & & & & & & & \\ FlashAttention BERT (79M) & **433.3** & **425.1** & 335.2 & 217.4 & 122.6 \\ M2-BERT-base (80M) & 386.3 & 380.7 & **378.9** & **353.9** & **320.1** \\ \hline M2 Speedup over HF BERT (80M) & 1.6\(\times\) & 2.4\(\times\) & 4.4\(\times\) & 7.5\(\times\) & – \\ \hline \hline \end{tabular}
\end{table}
Table 10: Throughput in tokens/ms by context length for M2-BERT-base (80M) compared to 80M BERT models.

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline Model & MNLI (m / mm) & RTE & QNL1 & QQP & SST2 & STS-B & CoLA & MRPC & **Average** \\ \hline M2-BERT-base (80M) & 78.4 / 78.6 & 68.5 & 84.6 & 86.7 & 92.0 & 86.3 & 53.0 & 89.8 & 79.9 \\ M2-BERT-base (110M) & 79.6 / 80.5 & 69.3 & 86.0 & 87.0 & 92.3 & 86.9 & 56.0 & 89.2 & 80.9 \\ \hline M2-BERT-large (260M) & 81.7 / 81.9 & 72.8 & 84.7 & 87.8 & 93.3 & 88.0 & 59.2 & 90.0 & 82.2 \\ M2-BERT-large (341M) & 82.2 / 82.3 & 75.0 & 87.0 & 87.7 & 92.4 & 88.3 & 59.6 & 90.1 & 82.8 \\ \hline \hline \end{tabular}
\end{table}
Table 9: Fine-tuning performance on GLUE [76]. We report the standard metrics – F1 scores for QQP and MRPC, Matthew’s correlation for CoLA, Spearman’s correlation for STS-B, and accuracy for the remaining tasks, following the procedure from [38].

### Roofline Analysis

Figure 4 shows a Roofline analysis of a simple PyTorch implementation of a single M2 operator \(\mathbf{M}^{-1}(\mathbf{M}\mathbf{u}\odot\mathbf{M}\mathbf{k}\) on an A100 GPU, with 4K input length. The operation is more dominated by the data movement operations, which helps explain why performance is higher on newer architectures like RTX 4090 (which have faster and larger L2 cache).

### Associative Recall

In Table 16, we present a simple experiment demonstrating the causal parameterization of M2 on associative recall, a synthetic language designed to test in-context learning. The model demonstrates in-context learning abilities in sequences up to 128K tokens, but Transformers do not scale past 8K.

### BERT Experiments with Alternative Architecture

Here, we report results using an older version of the M2-BERT architecture, that uses non-gated convolutions and is trained on English Wikipedia [25] and English Bookcorpus [92]. For clarity, we refer to this model as M1-BERT.

We found that M1-BERT could match Transformers on MLM quality, but underperformed on downstream fine-tuning. We attribute this gap in performance to sub-optimal training hyperparameters (optimized for throughput using NVIDIA MLPerf hyperparameters) as well as a sub-optimal architecture. We report results here for completeness, but refer to the gated convolution architecture in the main body as the proper M2-BERT model.

These models followed the reference implementations and hyperparameters from Hugging Face Transformers examples [81] and Nvidia Deep Learning examples ([https://github.com/NVIDIA/DeepLearningExamples](https://github.com/NVIDIA/DeepLearningExamples)). In particular, we use the LAMB optimizer with a learning rate of \(5e-3\). For each sequence length, we use as large a minibatch size as possible that fits on the GPU (A100-80GB in Table 17 and V100 in Table 18). We set the gradient accumulation to reach a global batch size of \(65,536\) sequences. To investigate the effect of sequence length, each model is trained for a fixed sequence length in a single phase of training (in contrast to some training protocols, which train the model in multiple phases, each at different sequence lengths).

Time to a Fixed Pretraining Quality on 8xA100We compare time to a fixed pretraining quality, training M1-BERT-base on English Wikipedia [25] and English Bookcorpus [92]. We compare against BERT-base trained with FlashAttention[15], as well as the Monarch-BERT-base implementation from the original Monarch paper [14]. We measure wall-clock time for M1-BERT and the base Transformer to reach 50% in masked language modeling accuracy on 8xA100 Nvidia GPUs with 80GB memory each. Table 17 summarizes results. In short sequence lengths, M1-BERT is

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline
**Model** & **ImageNet (acc@1)** & **ImageNet (acc@5)** \\ \hline Swin-MLP-B & 81.3 & 95.3 \\ Swin-V1-B & 83.5 & 96.5 \\ Swin-V2-B & **84.2** & **96.9** \\ M2-Swin-B & 83.5 & 96.7 \\ \hline \hline \end{tabular}
\end{table}
Table 12: ImageNet accuracy of Swin models.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline
**Model** & **512** & **1024** & **2048** & **4096** & **8192** \\ \hline HF BERT-large (340M) & 75.4 & 47.1 & 25.2 & OOM & OOM \\ FlashAttention BERT-large (340M) & **125.0** & 111.9 & 91.6 & 54.5 & OOM \\ M2-BERT-large (260M) & 122.5 & **118.6** & **109.4** & **94.5** & **75.0** \\ \hline M2 Speedup over HF BERT-large (340M) & 1.6\(\times\) & 2.5\(\times\) & 4.3\(\times\) & - & - \\ \hline \hline \end{tabular}
\end{table}
Table 11: Throughput in tokens/ms by context length for M2-BERT-large (260M) compared to BERT-large.

comparable to FlashAttention, even without using a heavily-optimized fused kernel. In longer sequence lengths, the FLOP savings make M1-BERT more efficient--up to 2.4\(\times\) faster than BERT with FlashAttention at sequence length \(4096\).

BERT in Half a DayInspired by recent work focusing on training under limited resource constraints [30], we measure how far we can get when training on a single V100 GPU in 12 hours. In Table 18, we report the masked language modeling accuracy achieved by the same set of models and sequence lengths (except for the FlashAttention baseline, which is not supported on V100). We observe M1-BERT both achieves higher accuracy within the time limit and can be trained at longer sequence lengths than the baseline architectures.

Downstream Fine-TuningWe evaluate the quality of M1-BERT-base models on the GLUE benchmark [76]. Table 19 shows fine-tuning performance on the GLUE tasks, using the same hyperparameters and 5 epochs for all tasks and both models. M1-BERT-base is competitive with Transformers trained using MLPerf hyperparameters on Bookcorpus and Wikitext, but underperforms fully-trained transformers and M2-BERT-base.

## Appendix C Experiment Details

### Model Architectures

In this section, we describe the exact model architectures we used for each task, including the design of the block (residuals and gating). We additionally release our code for reproducibility,

BERT Language ModelingThe M2-BERT architectures use a standard BERT backbone, but replace the attention with bidirectional gated convolutions and replace the linear layers in the MLPs with block-diagonal matrices. All the M2-BERT architectures use an expansion factor of four. M2-BERT-base (80M) has a model width of 768 and 12 layers; M2-BERT-base (110M) has a model width of 960 and 12 layers; M2-BERT-large (260M) has a model width of 1536 and 12 layers; and M2-BERT-large (341M) has a model width of 1792 and 12 layers. We train all these models on C4 for 70,000 steps, with sequence length 128, and global batch size 4096 sequences. For all the models, we use decoupled AdamW with learning rate 8e-4 and decoupled weight decay 1e-5. We use linear learning rate decay with a warmup of 6% of the steps, and we use MLM masking percentage of 30%.

For GLUE fine-tuning, we do a small search of learning rate, weight decay, and number of epochs. Following [38], we fine-tune RTE, MRPC, and STS-B from the MNLI checkpoint. We fine-tune all tasks with sequence length 128. For some tasks, we also pool the embeddings of all the non-padding tokens instead of using the CLS token.

The final hyperparameters for M2-BERT-base (80M) are decoupled AdamW with learning rate 5e-5 and weight decay 5e-6 for 3 epochs for MNLI; AdamW with learning rate 5e-5 and weight decay 0.01 for 6 epochs for RTE; AdamW with learning rate 3e-5 and weight decay 0.01 for 10 epochs on QQP; AdamW with learning rate 5e-5 and weight decay 1e-5 for 10 epochs with average pooling for QNLI; decoupled AdamW with learning rate 3e-5 and weight decay 3ed-6 for 3 epochs for SST-2; AdamW

\begin{table}
\begin{tabular}{r c c c c} \hline \hline M2 & S4 & WaveGan-D & Transformer & Performer & CKConv \\ \hline
**97.9** & 97.5 & 96.3 & x & 30.8 & 71.7 \\ \hline \hline \end{tabular}
\end{table}
Table 13: Accuracy on Speech-Commands 10. An “x” means that the model did not fit in memory.

\begin{table}
\begin{tabular}{r c c} \hline \hline Model & Top-1\% & Description \\ \hline ViT (1.2M) & 78.6 & Attention + MLP \\ ViT + Monarch (607K) & 79.0 & Attention, MLP-Free \\ HyenaViT (1.3M) & 80.6 & Attention-Free + MLP \\ HyenaViT-M2 (741K) & **80.8** & Attention-Free + MLP Free \\ \hline \hline \end{tabular}
\end{table}
Table 14: Accuracy on CIFAR-10.

[MISSING_PAGE_FAIL:21]

We adapt the ViT architecture by replacing its MLP and/or attention components with Monarch Matrices (similar to our adaptation of BERT):

We replace the MLP with randomly initialized Monarch Matrices of the same dimension as the dense matrices of the MLP and learn those matrices during training, setting the number of blocks in the block-diagonal matrices to \(4\).

We replace attention with the recently introduced Hyena operator [64]. The Hyena operator represents a recurrence of two efficient sub-quadratic primitives, an implicit long convolution and multiplicative element-wise gating of the projected input. Hyena operators apply the FFT algorithm to achieve fast long convolutions in sub-quadratic time. We further adapt the Hyena operator by replacing its long convolutions with the M2 operator and setting the Monarch Matrices to the DFT and inverse DFT.

ViT for ImageNet-1kIn line with other work [4, 14, 64, 70], we use a ViT-base architecture with \(12\) layers, a hidden size of \(768\), \(12\) attention heads per layer, an intermediate size of the MLP projection of \(3,072\), and a patch size of \(16\times 16\) pixels. For optimization, we follow the training procedure of T2T-ViT [86], including augmentations such as RandAugment [12] (magnitude \(=9\), magnitude-std \(=0.5\), layers \(=2\)), Mixup [88] (\(\alpha=0.8\)), CutMix [87] (\(\alpha=1.0\)), Random erasing [90] (rate \(=0.25\)), and AugMix [37]. See Table 20 for all other training settings.

ViT for CIFAR-10We use a ViT architecture with \(6\) layers, a hidden size of \(128\), \(8\) attention heads per layer, an intermediate size of the MLP projection of \(512\), and a patch size of \(4\times 4\) pixels. We further tune weight decay (\(0\) or \(0.1\)), stochastic depth rate (\(0\) or \(0.1\)), and base learning rate (\(1e-4\) or \(3e-4\) or \(1e-3\)) and report the test performance for the model variant that achieved the highest accuracy in a separate held-out validation dataset (randomly selected \(10\%\) of training data). We also apply an early stopping rule such that training is stopped if the model's validation loss does not improve for \(10\) training epochs. See Table 20 for all other training settings.

GPT Causal Language ModelingSimilarly to our ViT approach, we also replace attention with the Hyena operator, using the same architecture as in [64] as a starting point. The Hyena architecture has two convolutions, which can be computed using the FFT convolution theorem. In our architecture, we additionally replace these FFT operations with causal Monarch matrices.

In addition, we re-use the heads extension from the H3 architecture [27]. The heads extension groups the model dimension into heads, ties together the long convolution parameters in each head, and then computes the outer product between different input projections. An algorithmic listing adapted from the H3 paper [27] is provided in Listing 1, with updates to replace the SSM layers with Hyena convolutions. We use a head dimension of 16. Setting the head dimension to be 1 and replacing the Monarch matrices with FFT is equivalent to the Hyena layer.

\begin{table}
\begin{tabular}{r r r r r r} \hline \hline Model & 0.5K & 2K & 8K & 32K & 128K \\ \hline Transformer & 100.0 & 100.0 & 100.0 & ✗ & ✗ \\ Monarch Mixer & 98.7 & 99.4 & 99.4 & 99.4 & 99.4 \\ \hline \hline \end{tabular}
\end{table}
Table 16: In-context learning performance on associative recall at various sequence lengths, vocab size 20. ✗ indicates the Transformer did not finish in a week.

\begin{table}
\begin{tabular}{r r r r r r} \hline \hline Model & 512 & 1024 & 2048 & 4096 & Architecture Details \\ \hline BERT-base-FlashAttention (110M) & 2.7 & 3.8 & 5.7 & 13.2 & Attention, MLP \\ BERT-base-HuggingFace (110M) & 3.3 & 5.6 & 13.1 & 26.7 & Attention, MLP \\ BERT-Monarch-base (80M) & 3.1 & 4.7 & 10.3 & 22.1 & Attention, MLP-free \\ M1-BERT-base (55M) & 2.5 & 3.5 & 4.0 & 5.5 & Attention-Free, MLP-free \\ \hline Speedup & 1.1\(\times\) & 1.1\(\times\) & 1.3\(\times\) & 2.4\(\times\) & \\ \hline \hline \end{tabular}
\end{table}
Table 17: Time in hours to reach 50% masked language modeling validation accuracy on 8xA100 with different sequence lengths.

Finally, we remove the MLP layers entirely (equivalent to replacing the layer with an identity), and make the model wider to compensate (the depths match the equivalent Hyena models). The small model has a model width of 1160 with 18 layers and uses a learning rate of 0.0006, and the medium model has model width of 1344 with 40 layers and uses a learning rate of 0.0008. All other hyperparameters match the Hyena models [64].

## Appendix D Missing details from Section 4

This section contains all the missing details (including proofs) from Section 4.

In Appendix D.1, we review some definitions and results on multi-variate polynomials and set some notation needed for this section. In Appendix D.2, we explicitly connect Monarch matrices for \(p=2\) and bivariate polynomial evaluation. Specifically, we prove Theorem 1 and Theorem 2. Then in Appendix D.3 we show how to instantiate the bivariate basis polynomials so that we get a causal map. This includes converting the bivariate polynomials to univariate polynomials (with evaluations over the \(N\)th roots of unity) and this proves Theorem 3. We then show how this causal map can be implemented only using GEMMs (and \(O\left(N^{3/2}\right)\) FLOPs) in Appendix D.4.

Next, we note that while our evaluations points are over complex numbers, our input and output to the Monarch convolution layers are over reals. Hence, it is natural to wonder if we can implement the entire layer just with operations over real numbers. One potential advantage of this is that we theoretically only have to keep \(N\) real numbers for intermediate results (instead of \(2N\) reals

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline Model & 512 & 1024 & 2048 & 4096 & 8192 & Architecture Details \\ \hline BERT-base (110M) & 11.5 & 7.8 & 6.8 & ✗ & ✗ & Attention, MLP \\ BERT-Monarch-base & 6.9 & 8.5 & 6.8 & ✗ & ✗ & Attention, MLP-Free \\ M1-BERT-base & 20.2 & 20.2 & 20.1 & 17.1 & 12.9 & Attention-Free, MLP-Free \\ \hline \hline \end{tabular}
\end{table}
Table 18: Masked language modeling validation accuracy achieved on a single V100 in 12 hours with different sequence lengths. ✗ indicates the model does not fit on device with a batch size of \(1\).

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline Model & MNLI (m / mm) & RTE & QNLI & QQP & SST2 & STS-B & CoLA & MRPC & Architecture Details \\ \hline BERT no pretrain & 34.1 / 34.1 & 47.3 & 50.0 & 68.6 & 79.9 & 17.8 & 0.0 & **77.9** & Attention, MLP \\ BERT-base & **74.5** / **74.7** & **55.6** & 69.3 & **81.8** & 83.9 & 19.8 & 12.1 & 74.2 & Attention, MLP \\ M1-BERT-base & 69.9 / 70.5 & 53.1 & **73.2** & 81.4 & **85.2** & **68.1** & **33.6** & 75.4 & Attention-free, MLP-free \\ \hline \hline \end{tabular}
\end{table}
Table 19: Fine-tuning performance on the GLUE benchmark [76], after pretraining on Wikipedia and Bookcorpus. We report the standard metrics – F1 scores for QQP and MRPC, Matthew’s correlation for CoLA, Spearman’s correlation for STS-B, and accuracy for the remaining tasks [21].

numbers when we keep track of vectors in \(\mathbb{C}^{N}\)). This can reduce the data movement costs. Further, multiplication of two complex numbers requires six operations over real numbers (four multiplication and two addition). Thus, moving to an implementation that only uses real numbers could potentially lead to wall clock time speedup. We propose one such scheme in Appendix D.5 that proves a version of Theorem 3 just over reals by moving to the Chebyshev basis (instead of the standard monomial basis). This creates new technical challenges, which we also address.

Finally, we generalize our results to arbitrary \(p\geq 2\) in Appendix D.6. We would like to point out that to get a causal map (in Theorem 17) we need to 'embed' input vectors of size \(n\) into vectors of size \(N=2^{p}\cdot n+O\left(n^{1-1/p}\right)\). For \(p=2\), we avoided the blowup of \(2^{2}=4\) with a blowup of \(2\) instead (via Theorem 3). Whether this is possible to do (i.e. have a blowup of \(2\) instead of \(2^{p}\)) for \(p>2\) is an interesting direction for future work. Further, the matrices that lead to causal map can be represented with \(O\left(pN^{2/p}\right)\) parameters while the matrices in Theorem 3 use more parameters. Extending the causal map for \(p>2\) that uses \(O\left(N^{1+\frac{1}{p}}\right)\) parameters is an exciting direction for future work.

### Background and Notation

We collect known facts and definitions about multi-variate polynomials in Appendix D.1.1 and recall some notation from [14] in Appendix D.1.2. These will be needed throughout this appendix section.

#### d.1.1 Multi-variate Polynomials

Basic DefinitionsLet \(p\geq 1\) be an integer. We recollect some definitions on \(p\)-variate polynomials (over \(\mathbb{R}\)) in variables \(X_{0},\ldots,X_{p-1}\). When \(p\in\{1,2\}\), we will use variables in \(\{X,Y,Z\}\) for notational simplicity.

We will use \(\mathbf{X}\) to denote the vector of variables \((X_{0},\ldots,X_{p-1})\). Further for \(\mathbf{j}\in\mathbb{Z}_{\geq 0}^{p}\), we use the notation

\[\mathbf{X^{j}}=\prod_{a=0}^{p-1}X_{a}^{j_{a}}.\]

\(\mathbf{X^{j}}\) is a (standard basis) _monomial_, where \(\mathbf{j}=(j_{0},\ldots,j_{p-1})\).

A generic \(p\)-variate polynomial is defined as (with standard monomial representation)

\[q(\mathbf{X})=\sum_{\mathbf{j}\in\mathbb{Z}_{\geq 0}^{p}}q_{\mathbf{j}}\cdot \mathbf{X^{j}},\]

where the _coefficient_\(q_{\mathbf{j}}\in\mathbb{R}\).

We will need the following notion of degrees:

\begin{table}
\begin{tabular}{r r} \hline \hline  & ImageNet-1k & CIFAR-10 \\ \hline Optimizer & AdamW \\ Optimizer momentum & \(\beta_{1},\beta_{2}=0.9,0.999\) \\ Learning rate schedule & Cosine decay w/ linear warmup \\ Dropout rate & 0 \\ Label smoothing & 0.1 \\ \hline Image size & 224 x 224 & 32 x 32 \\ Base learning rate & 1e-3 & \{1e-4, 3e-4, 1e-3\} \\ Batch size & 1024 & 512 \\ Training epochs & 300 & up to 500 \\ Warmup epochs & 10 & 5 \\ Stochastic depth rate & 0.1 & \{0, 0.1\} \\ Weight decay & 0.05 & \{0, 0.1\} \\ \hline \hline \end{tabular}
\end{table}
Table 20: ViT training settings.

**Definition 2** (Degree).: _Let \(0\leq a<p\). The degree of \(X_{a}\) in \(\mathbf{X^{j}}\) (with \(\mathbf{j}=(j_{0},\ldots,j_{p-1})\)) is \(j_{a}\). The degree of \(X_{a}\) of \(q(\mathbf{X})\), denoted by \(\deg_{X_{a}}(q)\) is the maximum degree of \(X_{a}\) over all monomials \(\mathbf{X^{j}}\) with \(q_{\mathbf{j}}\neq 0\)._

Note that for \(p=1\) the above coincides with the usual notion of degree of a univariate polynomial \(q(Z)\), in which case we just use \(\deg(q(Z))\) to denote \(\deg_{Z}(q(Z))\).

We will need the notion of taking \(\mod\) of a \(p\)-variate polynomial with \(p\)-tuple of polynomials. The notion of \(\mod\) is well defined for a univariate polynomial (which we will assume as a given below) but in general for arbitrary \(p\)-variate polynomials \(q(\mathbf{X})\) and \(q^{\prime}(\mathbf{X})\), the operation \(q(\mathbf{X})\mod q^{\prime}(\mathbf{X})\) is not well defined. However, we will only need the following restricted operation:

**Definition 3**.: _Let \(p\geq 1\). Fix a \(p\)-tuple of polynomials \(R_{0}(X_{0}),\ldots,R_{p-1}(X_{p-1})\). Then for any \(\mathbf{j}\in\mathbb{Z}_{\geq 0}^{p}\), we define_

\[\mathbf{X^{j}}\mod\left(R_{0}\left(X_{0}\right),\ldots,R_{p-1}\left(X_{p-1} \right)\right)=\prod_{a=0}^{p-1}\left(X^{j_{a}}\mod\left(R_{a}\left(X_{a} \right)\right)\right).\]

_For a general polynomial \(p(\mathbf{X})\),_

\[p(\mathbf{X})\mod\left(R_{0}\left(X_{0}\right),\ldots,R_{p-1}\left(X_{p-1} \right)\right)\]

_is defined by extending the definition for \(\mathbf{X^{j}}\) by linearity._

Polynomial EvaluationGiven a \(p\)-variate polynomial \(q(\mathbf{X})\) and an point \(\mathbf{a}\in\mathbb{R}^{p}\), the _evaluation_ of \(q\) at \(\mathbf{a}\) denoted by \(q(\mathbf{a})\) is evaluation of \(q\) as a function at \(\mathbf{a}\).

Given subsets \(S_{a}\subseteq\mathbb{C}\), we define \(q(\mathbf{X})\) evaluated at \(\times_{a=0}^{p-1}S_{a}\) as the vector of values \(q(\mathbf{a})\) overall \(\mathbf{a}\in\times_{a=0}^{p-1}S_{a}\).

In this paper, we will in many cases evaluate polynomials at the appropriate roots of unity. Specifically for an integer \(N\), we will define

\[\omega_{N}=e^{2\pi\iota/N}\]

and note that the \(N\)th roots of unity is the set \(\{\omega_{N}^{i}|0\leq i<N\}\).

Polynomial InterpolationWe now recall univariate and bivariate polynomial interpolation results (proved via the Lagrange basis), which we will use in later subsections.

**Theorem 4**.: _Let \(D\geq 1\) be an integer. Given \(y_{i}\) for \(0\leq i<D\) and \(\alpha_{i}\) for \(0\leq i<D\) there exists a unique univariate polynomial \(P(X)\) with \(\deg(P)<D\), such that for all \(0\leq i<D\),_

\[P(\alpha_{i})=y_{i}. \tag{7}\]

Proof.: This proof is based on the Wikipedia entry for Lagrange polynomials [47].

Given a sequence of values \(\alpha_{i}\) for \(0\leq i<D\) s.t. \(\alpha_{i}\neq\alpha_{j}\), \(i\neq j\), the Lagrange basis for polynomials of degree \(<D\) for these values is the set of each polynomials \(\{p_{0}(X),p_{1}(X),\ldots p_{D-1}(X)\}\) each of degree \(D-1\). Each basis polynomial are defined as:

\[p_{i}(X)=\frac{X-\alpha_{0}}{\alpha_{i}-\alpha_{0}}\cdots\frac{X-\alpha_{i-1} }{\alpha_{i}-\alpha_{i-1}}\cdot\frac{X-\alpha_{i+1}}{\alpha_{i}-\alpha_{i+1}} \cdots\frac{X-\alpha_{D-1}}{\alpha_{i}-\alpha_{D-1}}=\prod_{\begin{subarray}{ c}0\leq j<D\\ j\neq i\end{subarray}}\frac{X-\alpha_{j}}{\alpha_{i}-\alpha_{j}}. \tag{8}\]

By definition,

\[p_{i}(\alpha_{j})=\begin{cases}1&\text{for }j=i\\ 0&\text{otherwise}\end{cases}. \tag{9}\]

The Lagrange interpolating polynomial for those nodes through the corresponding values \(y_{i}\) for \(0\leq i<D\) is the linear combination:

\[P(X)=\sum_{i=0}^{D-1}y_{i}\cdot p_{i}(X). \tag{10}\]

[MISSING_PAGE_EMPTY:26]

5. \(\mathbf{R}\in\mathcal{BD}^{(b,N)}\) meaning it's a block diagonal \(N\times N\) matrix with block size \(b\times b\).
6. We have a class of permutation matrices defined as \(\sigma_{(b,N)}(i)=i_{0}\cdot\frac{N}{b}+i_{1}\). This can be denoted by an \(N\times N\) matrix, \(P_{(b,N)}\), where the \(i^{\text{th}}\) row is \(\mathbf{e}_{\sigma(b,N)(i)}\).
7. We'll use \(i\) or pair notation \(\left(i_{1},i_{0}\right)_{b}\) to denote the rows, and \(j\) or pair notation \(\left(j_{1},j_{0}\right)_{b}\) to denote columns. It should be clear from context which one we're using.

For any \(0\leq j_{1}<\sqrt{N}\), let \(\ell_{j_{1}}(X,Y)\) be an arbitrary bivariate polynomial with \(\deg_{X}(\ell_{j_{1}})\), \(\deg_{Y}(\ell_{j_{1}})<\sqrt{N}\).

For any \(0\leq j_{1},j_{0}<\sqrt{N}\), let \(r_{j_{1},j_{0}}(Y)\) be an arbitrary univariate polynomial of degree \(<\sqrt{N}\).

Let \(A=(\alpha_{0},\ldots,\alpha_{\sqrt{N}-1})\), \(B=(\beta_{0},\ldots,\beta_{\sqrt{N}-1})\) each be a sequence of distinct eval points. Note that \(A\) and \(B\) need not be disjoint.

From the proof of Theorem 3 in the Appendix C of the Monarch paper [14] we get,

\[\mathbf{L} =\mathbf{P}_{(b,N)}\cdot\overline{\mathbf{L}}\cdot\mathbf{P}_{(b, N)}^{\top}\] \[=\mathbf{P}_{(b,N)}\cdot\overline{\mathbf{L}}\cdot\mathbf{P}_{( \frac{N}{b},N)}.\]

Therefore,

\[\overline{\mathbf{L}} =\mathbf{P}_{(b,N)}^{\top}\cdot\mathbf{L}\cdot\mathbf{P}_{(b,N)}\] \[=\mathbf{P}_{(\frac{N}{b},N)}\cdot\mathbf{L}

[MISSING_PAGE_EMPTY:28]

[MISSING_PAGE_EMPTY:29]

are the basis polynomials corresponding to \(\mathbf{M}_{1}\) and \(\mathbf{M}_{2}\). For the coefficient vector \(\mathbf{k}=(k_{j_{1},j_{0}})_{0\leq j_{1},j_{0}<\sqrt{N}}\) and similarly for \(\mathbf{u}=(u_{j_{1},j_{0}})_{0\leq j_{1},j_{0}<\sqrt{N}}\), we can construct two polynomials

\[k(X,Y) =\sum_{0\leq j_{1},j_{0}<\sqrt{N}}k_{j_{1},j_{0}}\cdot\ell_{j_{0}} (X,Y)\cdot r_{j_{0},j_{1}}(Y)\] \[u(X,Y) =\sum_{0\leq j_{1},j_{0}<\sqrt{N}}u_{j_{1},j_{0}}\cdot\bar{\ell}_ {j_{0}}(X,Y)\cdot\bar{r}_{j_{0},j_{1}}(Y)\]

whose evaluation over \((\alpha_{i_{1}},\beta_{i_{0}})=(\omega^{i_{1}},\omega^{i_{0}})\) where recall as in Appendix D.1.1\(\omega=e^{\frac{2\pi\iota}{\sqrt{N}}}\), by Theorem 1 is equivalent to the products \(\mathbf{M}_{1}\cdot\mathbf{k}\) and \(\mathbf{M}_{2}\cdot\mathbf{u}\), respectively. Taking the component-wise product, \(\mathbf{y}\)=\((\mathbf{M}_{1}\cdot\mathbf{k})\odot(\mathbf{M}_{2}\cdot\mathbf{u})\), the entry at \(i=(i_{1},i_{0})\) is given by

\[\mathbf{y}[(i_{1},i_{0})]=k(\omega^{i_{1}},\omega^{i_{0}})\cdot u(\omega^{i_{1 }},\omega^{i_{0}}).\]

Noting that the element of \(A\), i.e. the \(\sqrt{N}\)-th roots of unity, satisfy \(Z^{\sqrt{N}}=1\) means that the above are evaluations of

\[h(X,Y)=k(X,Y)\cdot u(X,Y)\mod(X^{\sqrt{N}}-1,Y^{\sqrt{N}}-1)\]

at \(A\times A\). Finally, Theorem 1 and the fact that \(\mathbf{M}_{0}^{-1}\) exists implies \(\mathbf{M}_{0}\cdot\mathbf{y}\) is polynomial interpolation into basis polynomials corresponding to \(\mathbf{M}_{0}\). (Here we use the well known fact polynomial interpolation is the inverse of polynomial evaluation).

### Proof of Theorem 3

We review some concepts in Appendix D.3.1. In Appendix D.3.2, we discuss square matrices and causality in terms of operations on univariate polynomials. This allows us to define a general class of operators for causal 1D convolution. In Appendix D.3.3, we give a class of matrices suitable for perform causal Monarch convolution. Specifically, we prove Theorem 3.

#### d.3.1 Review

Consider the linear operation on an input vector \(\mathbf{u}\):

\[\mathbf{y}=\mathbf{A}\cdot\mathbf{u}.\]

We say that the map is causal to mean the entry \(\mathbf{y}[i]\) only depends on \(\mathbf{u}[0],\mathbf{u}[1],\ldots\mathbf{u}[i]\). This will be the case when \(\mathbf{A}\) is a lower triangular matrix (we index the top left entry of \(\mathbf{A}\) as \((0,0)\)). When \(\mathbf{A}\) is a lower triangular Toeplitz matrix with entries corresponding to some coefficient vector \(\mathbf{k}\), this operation is exactly the 1D convolution

\[\mathbf{y}=\mathbf{k}*\mathbf{u}=\left(\mathbf{F}_{2n}^{-1}\cdot((\mathbf{F}_{ 2n}\cdot\mathbf{k}^{\prime})\circ(\mathbf{F}_{2n}\cdot\mathbf{u}^{\prime})) \right)[0:n-1],\]

where \(\mathbf{k}^{\prime}=(\mathbf{k},\mathbf{0}_{n}),\ \mathbf{u}^{\prime}=( \mathbf{u},\mathbf{0}_{n})\), and \(\mathbf{F}_{n}\) is the \(n\times n\) DFT matrix.

**Definition 4**.: _For a matrix \(\overline{\mathbf{M}}\in\mathbb{R}^{n\times n}\), let us define the map_

\[\mathbf{y}=\mathbf{M}^{-1}(\overline{\mathbf{M}}\cdot\mathbf{k}\odot\overline {\mathbf{M}}\cdot\mathbf{u}) \tag{31}\]

_as matrix convolution. When \(\overline{\mathbf{M}}\) is a Monarch matrix, (31) is called Monarch convolution._

In this section, we are interested in determining large subclasses of matrices \(\overline{\mathbf{M}}\) such that for any coefficient vector \(\mathbf{k}\), (31) is causal in \(\mathbf{u}\). We provide a class of matrices for which Monarch convolution is causal.

We note that for general Monarch matrix \(\mathbf{M}\), (31) is not causal in \(\mathbf{u}\). By Theorem 2, we have

\[y(X,Y)=k(X,Y)\cdot u(X,Y)\mod(X^{\sqrt{N}-1},Y^{\sqrt{N}-1}).\]

This is not causal because the \(\mod(X^{\sqrt{N}-1},Y^{\sqrt{N}-1})\) term condenses higher order terms into lower order terms, hence the \(\mathbf{y}[i]\) wouldn't just depend on input information up to value \(i\).

#### d.3.2 Univariate Matrix Convolutions

We start with a couple of notation assumptions.

**Assumption 1**.: \(N\) _is a perfect square._

**Assumption 2**.: _We will not use pair notation for this subsection since throughout we have \(i=i_{1}\sqrt{N}+i_{0}\) and \(j=j+1\sqrt{N}+j_{0}\)._

In order to discuss square matrices in terms of univariate polynomials, we give univariate analogs of Theorem1 and Theorem2 for general univariate basis. With an eye toward towards performing causal convolution, we restrict our analysis to certain classes of univariate polynomials.

We first define matrices whose \(j^{\text{th}}\) columns are the evaluation of a minimum degree \(j\) (and maximum degree \(N-1\)) polynomial (recall Definition1). We generalize Theorem3 to such matrices.

**Lemma 1**.: _For sequence of points \(A=\{1,\omega_{N},\cdots\omega_{N}^{N-1}\}\) where \(\omega_{N}\) is the \(N^{\text{th}}\) root of unity, let \(\overline{\mathbf{M}}\) be defined as_

\[\overline{\mathbf{M}}[i,j]=\bar{q}_{j}(\omega_{N}^{i}) \tag{32}\]

_where \(\bar{q}_{j}(Z)\) is defined as in Definition1. Then for any vector \(\boldsymbol{v}\in\mathbb{R}^{N}\), \(\overline{\mathbf{M}}\cdot\boldsymbol{v}\) is equivalent to evaluating the polynomial_

\[v(Z)=\sum_{j=0}^{N-1}v_{j}\cdot\bar{q}_{j}(Z) \tag{33}\]

_at \(\{1,\omega_{N},\cdots\omega_{N}^{N-1}\}\)._

Proof.: By our definition of \(\overline{\mathbf{M}}\), the column \(\overline{\mathbf{M}}[:,j]\) is exactly the evaluation of the polynomial \(\overline{q}_{j}(Z)\) at each point in \(A\). The claimed result comes from the definition of matrix vector multiplication and (33). 

Note that \(\overline{\mathbf{M}}\) or any \(\mathbf{M}\)'s in this sub-section are not necessarily Monarch matrices.

Next, we state the following intermediate result:

**Proposition 1**.: _Let \(A\) be the set of the \(N\)-th roots of unity. Then for \(\mathbf{M}_{1},\mathbf{M}_{2}\) defined as in (32)_

\[\mathbf{y}=(\mathbf{M}_{1}\cdot\mathbf{k})\odot(\mathbf{M}_{2}\cdot\mathbf{u})\]

_is the same as evaluating the polynomial_

\[p(Z):=k(Z)\cdot u(Z)\mod(Z^{N}-1)\]

_over \(A\) where \(k(Z),\ u(Z)\) are of the form (33), corresponding to \(\mathbf{M}_{1}\) and \(\mathbf{M}_{2}\), respectively. In other words, for any \(0\leq i<N\),_

\[\mathbf{y}[i]=p\left(\omega_{N}^{i}\right).\]

Proof.: This result follows from Lemma1 and the definition of the Hadamard product. 

Next, we state a re-interpretation of \(\mathbf{M}^{-1}\mathbf{y}\):

**Proposition 2**.: _Let \(\mathbf{M}\) be a full rank matrix whose columns are the evaluations of the basis polynomials \(\bar{q}_{j}(Z)\) from Definition1 for \(0\leq j<N\), and let \(\mathbf{y}\in\mathbb{R}^{N}\) be an arbitrary vector. If \(\mathbf{u}=\mathbf{M}^{-1}\mathbf{y}\), then for all \(0\leq i<N\)_

\[\mathbf{y}[i]=u(\omega^{i})\]

_where \(u(Z)\) is the same as in Lemma1 for \(\mathbf{M}\). In other words, \(\mathbf{M}^{-1}\mathbf{y}\) is the polynomial interpolaton problem for the polynomial basis \(\bar{q}_{j}(Z)\) for \(0\leq j<N\)._

Proof.: This follows from Lemma1 and the fact that \(\mathbf{M}\) is invertible. 

From Propositions1 and 2, we get the following generalization of Theorem2:

**Theorem 7**.: _For matrices \(\mathbf{M}_{0},\mathbf{M}_{1},\mathbf{M}_{2}\) as defined above, the operation_

\[\mathbf{f}=\mathbf{M}_{0}^{-1}\cdot((\mathbf{M}_{1}\cdot\mathbf{k})\circ( \mathbf{M}_{2}\cdot\mathbf{u}))\]

_is equivalent to representing the polynomial_

\[f(Z)=k(Z)\cdot u(Z)\mod(Z^{N}-1)\]

_in terms of the basis polynomials_

\[\hat{q}_{j}(Z)\text{ for }\ j=0,\ldots,N-1\]

_where \(k(Z),u(Z)\) are defined as in Lemma 1 in terms of the basis polynomials corresponding to \(\mathbf{M}_{1}\) and \(\mathbf{M}_{2}\), respectively, and \((\hat{q}_{j}(Z))_{0\leq j<N}\) corresponds to \(\mathbf{M}_{0}\)._

Proof.: Follows from Propositions 1 and 2. 

Now we give the class of matrices from which we can build a causal map. Specifically we prove a generalization of Theorem 3:

**Theorem 8**.: _Let \(n\geq 1\), let \(N=\left\lceil\sqrt{2n}\right\rceil^{2}\). Then define the basis polynomial \(\bar{q}_{j}(Z)\) to have minimum degree \(j\) and maximum degree \(n-1\) for \(0\leq j<n\), and for \(n\leq j<N\), \(\bar{q}_{j}(Z)\) has minimum degree \(j\) and maximum degree \(N-1\)._

_For all \(\mathbf{M}_{N}\) with basis columns defined by \(\left(\bar{q}_{j}(Z)\right)_{0\leq j<N}\) as above, the operation_

\[\mathbf{u}\mapsto\left(\mathbf{M}_{N}^{-1}(\mathbf{M}_{N}\cdot(\mathbf{k}, \mathbf{0}_{N-n})\circ\mathbf{M}_{N}\cdot(\mathbf{u},\mathbf{0}_{N-n}))\right) \left[0:n-1\right] \tag{34}\]

_gives a causal map._

Proof.: To prove this is causal means each entry, \(\mathbf{f}[i]\) is dependent only on \(\mathbf{u}[0]\), \(\mathbf{u}[1],\ldots\mathbf{u}[i]\), where \(\mathbf{f}=\left(\mathbf{M}_{N}^{-1}(\mathbf{M}_{N}\cdot(\mathbf{k},\mathbf{0} _{N-n})\circ\mathbf{M}_{N}\cdot(\mathbf{u},\mathbf{0}_{N-n}))\right)\). By Theorem 7, we have

\[f(Z)=k(Z)\cdot u(Z)\mod(Z^{N}-1),\]

where

\[k(Z)=\sum_{j=0}^{n-1}k_{j}\bar{q}_{j}\left(Z\right)\quad u(Z)=\sum_{j^{\prime }=0}^{n-1}u_{j^{\prime}}\bar{q}_{j^{\prime}}\left(Z\right).\]

Since \(\deg(k(Z)\cdot u(Z))\leq 2n-2\leq N-2\), this is equivalent to

\[f(Z) =k(Z)\cdot u(Z)\] \[=\sum_{j,j^{\prime}=0}^{n-1}k_{j^{\prime}}\cdot u_{j}\cdot\bar{q }_{j^{\prime}}(Z)\cdot\bar{q}_{j}(Z).\]

By our choice of \(\bar{q}_{j}\), we ensure that \(\bar{q}_{j}\cdot\bar{q}_{j^{\prime}}\) has minimum degree \(j+j^{\prime}\) and \(\deg(\bar{q}_{j}\cdot\bar{q}_{j^{\prime}})\leq 2n-2<N\) for any \(0\leq j,j^{\prime}<n\). Then by Lemma 2 (see below), there exists coefficients \(\alpha_{j+j^{\prime},i^{\prime}}\) such that,

\[f(Z) =\sum_{j,j^{\prime}=0}^{n-1}k_{j^{\prime}}\cdot u_{j}\ \cdot\sum_{i^{ \prime}=j+j^{\prime}}^{N-1}\alpha_{j+j^{\prime},i^{\prime}}\cdot\bar{q}_{i^{ \prime}}(Z)\] \[=\sum_{i=0}^{N-1}\left(\sum_{\begin{subarray}{c}j,j^{\prime}=0\\ j+j^{\prime}\leq i\end{subarray}}^{n-1}\alpha_{j+j^{\prime},i}\cdot k_{j^{ \prime}}\cdot u_{j}\right)\cdot\bar{q}_{i}(Z).\]

If we define\[f(Z)=\sum_{i=0}^{N-1}f_{i}\cdot\bar{q}_{i}(Z),\]

then for \(0\leq i<n\), we get:

\[f_{i}=\sum_{\begin{subarray}{c}j,j^{\prime}=0\\ j+j^{\prime}\leq i\end{subarray}}^{n-1}\alpha_{j+j^{\prime},i}\cdot k_{j^{\prime }}\cdot u_{j}.\]

Note that \(f_{i}\) only depends on \(\mathbf{u}[0],\mathbf{u}[1],\ldots\mathbf{u}[i]\), as desired. 

We used the following lemma in the proof of Theorem 8.

**Lemma 2**.: _Let \(\bar{q}_{j}(Z)\) be defined as in Theorem 8. Then for any \(0\leq j,j^{\prime}<n\),_

\[\bar{q}_{j}(Z)\cdot\bar{q}_{j^{\prime}}(Z)=\sum_{i=j+j^{\prime}}^{N-1}\alpha_{ j+j^{\prime},i}\cdot\bar{q}_{i}(Z). \tag{35}\]

_for some set of coefficients \(\alpha_{j+j^{\prime},i}\)._

Proof.: We first note that by our choice of \(\bar{q}_{j}\), the minimum degree of \(\bar{q}_{j}(Z)\cdot\bar{q}_{j^{\prime}}(Z)\) is \(j+j^{\prime}\), and \(\deg(\bar{q}_{j}(Z)\cdot\bar{q}_{j^{\prime}}(Z))\leq 2n-2\leq N-1\). Our claim follows from that fact that any polynomial \(p_{d}(Z)\) of minimum degree \(d\) and \(\deg(p_{d})<N\) can be expressed as a linear combination of \(\bar{q}_{d}(Z),\bar{q}_{d+1}(Z),\ldots,\bar{q}_{N-1}(Z)\). 4 

Footnote 4: This claim can be shown using downward induction on \(d=N-1,N-2,\ldots\).

#### d.3.3 Causal Monarch Convolutions

In this section we will prove Theorem 3. We will do so by showing that the basis polynomials \(q_{j}(Z)\) as defined in Theorem 3 are a special case of the basis polynomials \(\bar{q}_{j}(Z)\) as defined in Theorem 8.

We start with a couple of notation assumptions.

**Assumption 3**.: _In this sub-section we will using block size \(b=\sqrt{N}\) therefore, we are dropping the block size from index notation. For example, \((i_{1},i_{0})_{\sqrt{N}}\) becomes \((i_{1},i_{0})\) for this section._

**Assumption 4**.: _Permutation matrices in this subsection are all the same \(\mathbf{P}_{(\sqrt{N},N)}\) so we drop the subscript and just use \(\mathbf{P}\)._

**Definition 5**.: _Define_

\[q^{\prime}_{(j_{1},j_{0})}(Z)\stackrel{{\text{def}}}{{=}}\ell_{j _{1}}(Z)\cdot r_{j_{1},j_{0}}\left(Z^{\sqrt{N}}\right). \tag{36}\]

\(\ell_{j_{1}}(Z)\) _has minimum degree \(j_{1}\), and \(r_{j_{1},j_{0}}(Z)\) has minimum degree \(j_{0}\). All polynomials \(q^{\prime}_{(j_{1},j_{0})}(Z)\) have maximum degree \(\leq N 

[MISSING_PAGE_FAIL:34]

### Block Algorithms for Complex Numbers and Block Size \(\sqrt{N}\)

In the following subsections we restate the results in Section D.3.3 in terms of block operations. As mentioned earlier, this is so that we can do computations on Monarch matrices using only GEMM operations (and simple data movement operations like permutations).

In Appendix D.4.1 we consider arbitrary Monarch matrices and in Appendix D.4.2 we consider the sub-class of Monarch matrices corresponding to Theorem 3.

#### d.4.1 General Block Monarch Convolution

In this subsection we re-state general Monarch convolutions in terms of block operations.

Recall from equation (22) we defined the block diagonal matrix \(\mathbf{R}\) as follows (\(0\leq i_{0},j_{1},j_{0}<\sqrt{N}\)):

\[\mathbf{R}_{j_{1},j_{1}}[i_{0},j_{0}]\leftarrow\tilde{r}_{j_{1},j_{0}}\left( \omega_{\sqrt{N}}^{i_{0}}\right), \tag{39}\]

where \(\deg(\tilde{r}_{j_{1},j_{0}})<\sqrt{N}\).

To do so, we will first work with \(\mathbf{M}^{\prime}_{N}\) such that \(\mathbf{M}_{N}=\mathbf{M}^{\prime}_{N}\cdot\mathbf{P}\). We want to express Monarch matrices, \(\mathbf{M}_{N}\), as univariate polynomial evaluation over \(\{1,\omega_{N},\ldots,\omega_{N}^{N-1}\}\). Towards that end, define

\[r_{j_{1},j_{0}}(Z)=\tilde{r}_{j_{1},j_{0}}\left(Z^{\sqrt{N}}\right).\]

By simple observation that \(\omega_{N}^{\binom{i_{1}\sqrt{N}+i_{0}}{\sqrt{N}}}=\omega_{N}^{i_{0}\sqrt{N}} =\omega_{\sqrt{N}}^{i_{0}}\), we have

\[r_{j_{1},j_{0}}(\omega_{N}^{i_{1}\sqrt{N}+i_{0}})=\tilde{r}_{j_{1},j_{0}}\left( \omega_{\sqrt{N}}^{i_{0}}\right).\]

In other words we have,

\[\mathbf{R}_{j_{1},j_{1}}[i_{0},j_{0}]=r_{j_{1},j_{0}}\left(\omega_{N}^{i} \right).\]

Fix \(0\leq j_{1},j_{0}<\sqrt{N}\) so we're looking at the \(j_{0}^{th}\) column of block \(\mathbf{R}_{j_{1},j_{1}}\), going down the column we evaluate the polynomial \(\tilde{r}_{j_{1},j_{0}}\) at points \(\left(1,\omega_{\sqrt{N}},\ldots,\omega_{\sqrt{N}}^{\sqrt{N}-1}\right)\). Which is equivalent to a matrix multiplication of Fourier matrix of size \(\sqrt{N}\) and a matrix of the coefficients of the \(\tilde{r}\) polynomials.

So we can think of the blocks of \(\mathbf{R}\) as a Fourier matrix times a coefficient matrix. In other words, let us define a matrix \(\widetilde{\mathbf{R}}\in\mathbb{R}^{N\times\sqrt{N}}\) which will hold \(\sqrt{N}\) coefficient blocks \(\widetilde{\mathbf{R}}_{0},\widetilde{\mathbf{R}}_{1},\ldots\widetilde{ \mathbf{R}}_{\sqrt{N}-1}\in\mathbb{R}^{\sqrt{N}\times\sqrt{N}}\) such that,

\[\widetilde{\mathbf{R}}_{j_{1}}[a,j_{0}]=\tilde{r}_{j_{1},j_{0}}[a],\]

where

\[\tilde{r}_{j_{1},j_

[MISSING_PAGE_FAIL:36]

[MISSING_PAGE_FAIL:37]

[MISSING_PAGE_FAIL:38]

[MISSING_PAGE_EMPTY:39]

**Lemma 6**.: _For any fixed \(n\geq 1\), let \(N=\left\lceil\sqrt{2n}\right\rceil^{2}\). Define \(\widetilde{\mathbf{L}},\widetilde{\mathbf{R}}\in\mathbb{R}^{N\times\sqrt{N}}\) such that for every \(0\leq j_{1}<\sqrt{N}\),_

\[\widetilde{\mathbf{L}}=\begin{bmatrix}\widetilde{\mathbf{L}}_{0}\\ \vdots\\ \widetilde{\mathbf{L}}_{\sqrt{N}-1}\end{bmatrix}\text{ and }\widetilde{\mathbf{R}}= \begin{bmatrix}\widetilde{\mathbf{R}}_{0}\\ \vdots\\ \widetilde{\mathbf{R}}_{\sqrt{N}-1}\end{bmatrix}.\]

_Define \(\widetilde{\mathbf{L}}^{\prime},\widetilde{\mathbf{R}}^{\prime}\in\mathbb{R}^{ N\times\sqrt{N}}\) with corresponding coefficient blocks \(\left(\widetilde{\mathbf{L}}^{\prime}_{k}\right)_{0\leq k<\sqrt{N}},\;\left( \widetilde{\mathbf{R}}^{\prime}_{k}\right)_{0\leq k<\sqrt{N}}\) as_

\[\widetilde{\mathbf{L}}^{\prime}=\begin{bmatrix}\widetilde{\mathbf{L}}^{\prime}_ {0}\\ \mathbf{0}_{\sqrt{N}\times\sqrt{N}}\\ \vdots\\ \mathbf{0}_{\sqrt{N}\times\sqrt{N}}\end{bmatrix}\text{ and }\widetilde{\mathbf{R}}^{ \prime}=\begin{bmatrix}\widetilde{\mathbf{R}}^{\prime}_{0}\\ \widetilde{\mathbf{R}}^{\prime}_{1}\\ \vdots\\ \widetilde{\mathbf{R}}^{\prime}_{\sqrt{N}-1}\end{bmatrix}\]

_where_

\[\widetilde{\mathbf{L}}^{\prime}_{0}[\left(i_{1},i_{0}\right),j_{1}]=\begin{cases} \widetilde{\mathbf{L}}_{0}[i_{0},j_{1}]\;\text{ if }i_{1}=0\text{ and }i_{0}\geq j_{1}\\ 0\text{ otherwise }\end{cases},\;\;\;\widetilde{\mathbf{L}}^{\prime}_{k}=\mathbf{0}_{ \sqrt{N}\times\sqrt{N}}\;\text{ for }k=1,\cdots,\sqrt{N}-1\]

\[\widetilde{\mathbf{R}}^{\prime}_{j_{1}}[i_{0},j_{0}]=\begin{cases}0\;\text{ if }i_{0}<j_{0}\text{ or }(\left(i_{0}\geq\left\lfloor\frac{\sqrt{N}}{2}\right\rfloor\right)\text{ and }\left(j_{0}<\left\lfloor\frac{\sqrt{N}}{2}\right\rfloor\right))\\ \widetilde{\mathbf{R}}_{j_{1}}[i_{0},j_{0}]\text{ otherwise }\end{cases}\text{ for }j_{1}=0,\ldots,\sqrt{N}-1.\]

_Further, we require that \(\widetilde{\mathbf{R}}^{\prime}_{j_{1}}[j_{0},j_{0}],\;\widetilde{\mathbf{L}} ^{\prime}_{0}[j_{1},j_{1}]\) are all non-zero entries for all \(0\leq j_{0},j_{1}<\sqrt{N}\). Then for \(j_{0}\sqrt{N}+j_{1}<\sqrt{N}\left\lfloor\frac{\sqrt{N}}{2}\right\rfloor\), the basis polynomial \(q^{\prime}_{(j_{1},j_{0})}(Z)=\ell^{\prime}_{j_{1}}(Z)r^{\prime}_{j_{1},j_{0}} \left(Z^{\sqrt{N}}\right)\) of \(\mathbf{M}^{\prime}=\mathbf{P}\mathbf{L}\mathbf{P}\mathbf{R}\) has minimum degree \(j_{0}\sqrt{N}+j_{1}\) and maximum degree \(\leq\frac{N}{2}-1\), and for \(\sqrt{N}\left\lfloor\frac{\sqrt{N}}{2}\right\rfloor\leq j_{0}\sqrt{N}+j_{1}<N\), the basis polynomial \(q^{\prime}_{(j_{1},j_{0})}(Z)\) has minimum degree \(j_{0}\sqrt{N}+j_{1}\) and maximum degree \(\leq N-1\)._

Note that in \(\mathbf{M}=\mathbf{M}^{\prime}\mathbf{P}\) the \((j_{1},j_{0})\) basis polynomial \(q_{j_{1},j_{0}}(Z)=q^{\prime}_{j_{0},j_{1}}(Z)\) has the required degree

[MISSING_PAGE_FAIL:41]

[MISSING_PAGE_EMPTY:42]

We also utilize the following result:

**Lemma 8**.: _Let \(\bar{q}_{j}^{\lfloor\frac{N}{2}\rfloor}(Z)\) be defined as in (48). Then for any \(0\leq j,j^{\prime}<N\),_

\[\bar{q}_{j}^{\lfloor\frac{N}{2}\rfloor}(Z)\cdot\bar{q}_{j^{\prime}}^{\lfloor \frac{N}{2}\rfloor}(Z)=\sum_{i=j+j^{\prime}}^{N-1}\alpha_{j+j^{\prime},i}\cdot \bar{q}_{i}^{N}(Z). \tag{54}\]

_for some set of coefficients \(\alpha_{j+j^{\prime},i}\)._

Proof.: From (48), we have

\[\bar{q}_{j}^{\lfloor\frac{N}{2}\rfloor}(Z)\cdot\bar{q}_{j^{\prime}}^{\lfloor \frac{N}{2}\rfloor}(Z)=\sum_{a=0}^{\lfloor\frac{N}{2}\rfloor-j-1}q_{j}[a]T_{a} (Z)\cdot\sum_{a^{\prime}=0}^{\lfloor\frac{N}{2}\rfloor-j^{\prime}-1}q_{j^{ \prime}}^{\lfloor\frac{N}{2}\rfloor-j^{\prime}-1}q_{j^{\prime}}^{\lfloor\frac{ N}{2}\rfloor}[a^{\prime}]T_{a^{\prime}}(Z).\]

Recall that within a \(m\) dimensional vector space of polynomials, we can define a basis by choosing any set of polynomials with degrees \(0,\ldots,m-1\). Because \(\deg\left(\bar{q}_{j}^{\lfloor\frac{N}{2}\rfloor}\cdot\bar{q}_{j^{\prime}}^{ \lfloor\frac{N}{2}\rfloor}\right)\leq 2\left\lfloor\frac{N}{2}\right\rfloor-(j+ j^{\prime})-2\), it can be written as a linear combination of any set of \(2\left\lfloor\frac{N}{2}\right\rfloor-(j+j^{\prime})-1\) polynomials where for \(0\leq a\leq 2\left\lfloor\frac{N}{2}\right\rfloor-(j+j^{\prime})-2\), the \(a\)-th polynomial has degree \(a\). In other words we can choose the \(a\)-th polynomial as \(q_{N-a-1}^{N}(Z)\). Thus we have

\[\bar{q}_{j}^{\lfloor\frac{N}{2}\rfloor}(Z)\cdot\bar{q}_{j^{\prime}}^{\lfloor \frac{N}{2}\rfloor}(Z)=\sum_{a=0}^{2\left\lfloor\frac{N}{2}\right\rfloor-(j+j^ {\prime})-2}\bar{\alpha}_{j+j^{\prime},a}\cdot\bar{q}_{N-a-1}^{N}(Z)\]

for some set of coefficients \(\alpha_{j+j^{\prime},a}\). Then after reindexing \(i\gets N-a-1\) we have

\[\bar{q}_{j}^{\lfloor\frac{N}{2}\rfloor}(Z)\cdot\bar{q}_{j^{\prime}}^{\lfloor \frac{N}{2}\rfloor}(Z)=\sum_{i=\left(N-2\left\lfloor\frac{N}{2}\right\rfloor \right)+j+j^{\prime}+1}^{N-1}\bar{\alpha}_{j+j^{\prime},N-i-1}\cdot\bar{q}_{i}^ {N}(Z).\]

The claim follows by setting \(\alpha_{j+j^{\prime},j+j^{\prime}+1}=0\), and if \(N\) is odd, \(\alpha_{j+j^{\prime}+1,j+j^{\prime}+2}=0\), and \(\alpha_{j+j^{\prime},i}=\bar{\alpha}_{j+j^{\prime},N-i-1}\) for other \(i\). 

This allows us to prove the following causality result for convolutions over real evaluation points.

**Theorem 13**.: _Fix a family of basis polynomials \(\bar{q}_{0}^{N},\bar{q}_{1}^{N}\ldots,\bar{q}_{N-1}^{N}\) as defined in (48). Let \(N\geq 1\) be a perfect square, \(n\leq\left\lfloor\frac{N}{2}\right\rfloor\), \(\mathbf{k},\mathbf{u}\in\mathbb{R}^{n}\) and \(\mathbf{M}_{N}\) defined by basis \(\left(\bar{q}_{j}^{N}(Z)\right)_{0\leq j<N}\). Let \(\mathbf{k}^{\prime\prime}=\left(\mathbf{k},\mathbf{0}\lfloor\frac{N}{2} \rfloor-n\right)\) and \(\mathbf{u}^{\prime\prime}=\left(\mathbf{u},\mathbf{0}\lfloor\frac{N}{2}\rfloor -n\right)\). Then the operation_

\[\mathbf{u}\mapsto\left(\mathbf{M}_{N}^{-1}\left(\mathbf{M}_{N}\cdot\left( \mathbf{0}_{\lceil\frac{N}{2}\rceil},\mathbf{k}^{\prime\prime}\right)\circ \mathbf{M}_{N}\cdot\left(\mathbf{0}_{\lceil\frac{N}{2}\rceil},\mathbf{u}^{ \prime\prime}\right)\right)\right)[0:n-1] \tag{55}\]

_defines a causal map in \(\mathbf{u}\)._

Proof.: Let

\[\mathbf{f}=\left(\mathbf{M}_{N}^{-1}\left(\mathbf{M}_{N}\cdot\left(\mathbf{0}_ {\lceil\frac{N}{2}\rceil},\mathbf{k}^{\prime\prime}\right)\circ\mathbf{M}_{N }\cdot\left(\mathbf{0}_{\lceil\frac{N}{2}\rceil},\mathbf{u}^{\prime\prime} \right)\right)\right)[0:n-1]. \tag{56}\]

In order to prove that (55) is actually causal in the input \(\mathbf{u}\in\mathbb{R}^{n}\), we must show that for all \(0\leq i<N\), \(\mathbf{f}[i]\) is dependent only on \(\mathbf{u}[0],\mathbf{u}[1],\ldots\mathbf{u}[i]\). Let \(\mathbf{k}^{\prime}=\left(\mathbf{0}_{\lceil\frac{N}{2}\rceil},\mathbf{k}^{ \prime\prime}\right)\) and \(\mathbf{u}^{\prime}=\left(\mathbf{0}_{\lceil\frac{N}{2}\rceil},\mathbf{u}^{ \prime\prime}\right)\). By Lemma 7, \(\mathbf{M}_{N}\cdot\mathbf{k}^{\prime}\) and \(\mathbf{M}_{N}\cdot\mathbf{u}^{\prime}\) correspond to the evaluations of the polynomials

\[k^{\prime}(Z)=\sum_{j=0}^{N-1}k^{\prime}_{j}\cdot\bar{q}_{j}^{N}(Z),\quad \text{and}\quad u^{\prime}(Z)=\sum_{j^{\prime}=0}^{N-1}u^{\prime}_{j^{\prime} }\cdot\bar{q}_{j^{\prime}}^{N}(Z). \tag{57}\]Let us define

\[k(Z)=\sum_{j=0}^{n-1}k_{j}\cdot\bar{q}_{j}^{\left\lfloor\frac{N}{2}\right\rfloor}(Z),\quad\text{and}\quad u(Z)=\sum_{j^{\prime}=0}^{n-1}u_{j}\cdot\bar{q}_{j^{ \prime}}^{\left\lfloor\frac{N}{2}\right\rfloor}(Z). \tag{58}\]

Note that for \(0\leq j<\left\lceil\frac{N}{2}\right\rceil\), the coefficients \(k^{\prime}_{j}=u^{\prime}_{j}=0\). Then (57) becomes

\[k^{\prime}(Z)=\sum_{j=\left\lceil\frac{N}{2}\right\rceil}^{N-1}k^{\prime}_{j^{ \prime}}\cdot\bar{q}_{j}^{N}(Z),\quad\text{and}\quad u^{\prime}(Z)=\sum_{j^{ \prime}=\left\lceil\frac{N}{2}\right\rceil}^{N-1}u^{\prime}_{j}\cdot\bar{q}_{j ^{\prime}}^{N}(Z),\]

which is equivalent to

\[k^{\prime}(Z)=\sum_{j=0}^{\left\lfloor\frac{N}{2}\right\rfloor-1}k^{\prime}_{j+ \left\lceil\frac{N}{2}\right\rceil}\cdot\bar{q}_{j+\left\lceil\frac{N}{2} \right\rceil}^{N}(Z),\quad\text{and}\quad u^{\prime}(Z)=\sum_{j^{\prime}=0}^{ \left\lfloor\frac{N}{2}\right\rfloor-1}u^{\prime}_{j^{\prime}+\left\lceil\frac {N}{2}\right\rceil}\cdot\bar{q}_{j^{\prime}+\left\lceil\frac{N}{2}\right\rceil }^{N}(Z).\]

For \(0\leq j<\left\lfloor\frac{N}{2}\right\rfloor\), \(\deg\left(\bar{q}_{j}^{\left\lfloor\frac{N}{2}\right\rfloor}\right)=\left\lfloor \frac{N}{2}\right\rfloor-j-1\), and \(\deg\left(\bar{q}_{j+\left\lceil\frac{N}{2}\right\rceil}^{N}\right)=N-\left \lceil\frac{N}{2}\right\rceil-j-1=\left\lfloor\frac{N}{2}\right\rfloor-j-1\). This implies that \(\bar{q}_{j}^{\left\lfloor\frac{N}{2}\right\rfloor}(Z)\) and \(\bar{q}_{j+\left\lceil\frac{N}{2}\right\rceil}^{N}(Z)\) are both linear combinations of \(\left\lfloor\frac{N}{2}\right\rfloor-j-1\) Chebychev polynomials. Then we can set \(\bar{q}_{j}^{\left\lfloor\frac{N}{2}\right\rfloor}(Z)=\bar{q}_{j+\left\lceil \frac{N}{2}\right\rceil}^{N}(Z)\). Similarly, note that for \(0\leq j<n\), \(k^{\prime}_{j+\left\lceil\frac{N}{2}\right\rceil}=k_{j}\). Then it follows that \(k(Z)=k^{\prime}(Z)\), and by a similar argument, \(u(Z)=u^{\prime}(Z)\). Then by Theorem12 we have

\[\begin{split} f(Z)&=k(Z)\cdot u(Z)\mod\ T_{N}(Z)\\ &=k(Z)\cdot u(Z)\end{split} \tag{59}\]

where the last statement follows since \(\deg\left(k(Z)\right),\deg\left(u(Z)\right)\leq n-1<\left\lfloor\frac{N}{2} \right\rfloor\), implying that their product has \(\deg\left(\bar{k}(Z)\cdot\bar{u}(Z)\right)<\deg\left(T_{N}(Z)\right)=N\). We want to write \(f(Z)\) in the form

\[f(Z)=\sum_{i=0}^{N-1}f_{i}\cdot\bar{q}_{i}^{N}(Z)\]

for some set of coefficients set of coefficients \(f_{i}\). From (58), (59) becomes

\[f(Z)=\sum_{j=0}^{n-1}\sum_{j^{\prime}=0}^{n-1}k_{j^{\prime}}\cdot u_{j}\cdot \bar{q}_{j^{\prime}}^{\left\lfloor\frac{N}{2}\right\rfloor}(Z)\cdot\bar{q}_{j }^{\left\lfloor\frac{N}{2}\right\rfloor}(Z).\]

Then by Lemma8 we have

\[\sum_{i=0}^{N-1}f_{i}\cdot\bar{q}_{i}^{N}(Z)=\sum_{j,j^{\prime}=0}^{n-1}k_{j^{ \prime}}\cdot u_{j}\cdot\sum_{i=j+j^{\prime}}^{N-1}\alpha_{j+j^{\prime},i} \bar{q}_{i}^{N}(Z).\]

We show that for all \(0\leq i<N\), \(f_{i}\) is a function of \((u_{i^{\prime}})_{0\leq i^{\prime}\leq i}\). Then note from the above that each \(k_{j}\) and \(u_{j^{\prime}}\) appears in terms of \(\bar{q}_{i}^{N}\) where \(i\geq j+j^{\prime}\). Then we have

\[f_{i}=\sum_{\begin{subarray}{c}j,j=0\\ j+j^{\prime}\leq i\end{subarray}}^{N-1}\alpha_{j+j^{\prime},i}\cdot k_{j^{ \prime}}\cdot u_{j},\]

as desired.

#### d.5.2 Structured Causal Matrices

In this section we narrow our scope from the general class of matrices defined in Appendix D.5.1 to a particular structured subclass. Now let us define the class of structured causal matrices over the real numbers.

**Definition 6**.: _Define_

\[\ell_{j_{1}}(Z)\stackrel{{\text{def}}}{{=}}\sum_{a=0}^{j_{1}}\ell_{ j_{1}}[a]\:T_{a}(Z),\quad\quad\tilde{r}_{j_{1},j_{0}}(Z)\stackrel{{ \text{def}}}{{=}}\sum_{a=0}^{j_{0}}\tilde{r}_{j_{1},j_{0}}[a]\:T_{a}(Z) \tag{60}\]

_where_

\[\tilde{r}_{j_{1},j_{0}}[a]=0\text{ if }(j_{0}-a)\text{ is odd,} \tag{61}\]

_We define structured causal (SC) matrix polynomials as_

\[q^{N}_{(j_{1},j_{0})}{}_{\sqrt{N}}(Z)=\ell_{\sqrt{N}-j_{1}-1}(Z)\cdot\tilde{r }_{\sqrt{N}-j_{1}-1,\sqrt{N}-j_{0}-1}\left(T_{\sqrt{N}}(Z)\right). \tag{62}\]

_A \(N\times N\) SC matrix is defined over the set of real evaluation points as_

\[\mathbf{M}^{\prime}[i,(j_{0},j_{1})]=q^{\prime N}_{(j_{1},j_{0})}{}_{\sqrt{N} }(\omega_{N,i})=q^{N}_{(j_{0},j_{1})}{}_{\sqrt{N}}(\omega_{N,i}). \tag{63}\]

We note that in (63) we deviate from the usual ordering of indices \((j_{1},j_{0})\) to \((j_{0},j_{1})\). We show that the SC matrix falls under the category of matrices defined by (50).

**Lemma 9**.: _Let \(\mathcal{M}^{C}\) denote the set of all matrices defined by of (50), and \(\mathcal{M}^{SC}\) denote the set of all matrices defined by Definition 6. Then \(\mathcal{M}^{SC}\subset\mathcal{M}^{C}\)._

Proof.: To show that \(\mathcal{M}^{SC}\subset\mathcal{M}^{C}\), then it is sufficient to show that for \(j=j_{0}\sqrt{N}+j_{1}\), any \(q^{\prime N}_{(j_{1},j_{0})}{}_{\sqrt{N}}(Z)\) is equivalent to some \(\bar{q}^{N}_{j}(Z)\) as in (48). From (63) we have

\[q^{\prime N}_{(j_{1},j_{0})}{}_{\sqrt{N}}(Z)=\ell_{\sqrt{N}-j_{0}-1}(Z)\cdot \tilde{r}_{\sqrt{N}-j_{0}-1,\sqrt{N}-j_{1}-1}\left(T_{\sqrt{N}}(Z)\right). \tag{64}\]

Note that

\[\begin{split}\deg\left(q^{\prime N}_{(j_{1},j_{0})}{}_{\sqrt{N} }\right)&=\sqrt{N}-j_{0}-1+(\sqrt{N}-j_{1}-1)\sqrt{N}\\ &=N-\sqrt{N}j_{1}-j_{0}-1\\ &=N-j-1.\end{split}\]

From the fact that \(\deg\left(T_{a}\right)=a\), we can use \(T_{a}\) as a polynomial basis. Then \(q^{\prime N}_{(j_{1},j_{0})}{}_{\sqrt{N}}\) can be represented as a linear combination of \(T_{a}(Z)\) like so:

\[q^{\prime N}_{(j_{1},j_{0})}{}_{\sqrt{N}}(Z)=\sum_{a=0}^{N-j-1}q_{(j_{1},j_{0 })}[a]T_{a}(Z),\]

which is exactly the form of (48).

Lemma 9 allows us to apply the causality result from Appendix D.5.1.

**Corollary 7**.: _Fix a family of basis polynomials \(q^{N}_{0,0},q^{N}_{0,1},\ldots,q^{N}_{\sqrt{N}-1,\sqrt{N}-1}\) as defined in (62). For any perfect square \(N\geq 1\), \(n\leq\left\lfloor\frac{N}{2}\right\rfloor\), \(\mathbf{k},\mathbf{u}\in\mathbb{R}^{n}\) and \(\mathbf{M}^{\prime}_{N}\) defined by basis \(\left(q^{\prime N}_{(j_{1},j_{0})}{}_{\sqrt{N}}(Z)\right)_{0\leq j_{1},j_{0}< \sqrt{N}}\) as in (63). Then the operation (55) with \(\mathbf{M}_{N}\leftarrow\mathbf{M}^{\prime}_{N}\) defines a causal map in \(\mathbf{u}\)._

Proof.: Follows from Theorem 13 and Lemma 9.

[MISSING_PAGE_EMPTY:46]

[MISSING_PAGE_EMPTY:47]

[MISSING_PAGE_EMPTY:48]

[MISSING_PAGE_FAIL:49]

[MISSING_PAGE_EMPTY:50]

Note that \(\mathbf{P}_{b^{\prime},N,N}\) is exactly the same as \(\mathbf{P}_{b^{\prime},N}\) from earlier.

If we use \(\sigma(b,M,N)\) to denote the corresponding permutation then it takes the input \(\left(i_{p-1},\ldots,i_{0}\right)_{b}\) and maps it to \(\left(i_{p-1},\ldots,i_{p^{\prime}},i_{p^{\prime}-2},\ldots,i_{0},i_{p^{ \prime}}\right)_{b}\)(where \(p=\log_{b}N\) and \(p^{\prime}=\log_{b}M\)). I.e. \(\sigma(b,M,N)\) does not change the first \(p-p^{\prime}\) sub-indices and then does a left rotation on the remaining \(p^{\prime}\) sub-indices.

For the rest of the section, assume \(b=\sqrt[p]{N}\) and then consider the following'sub-index reversal' permutation matrix8:

Footnote 8: \(b=2\) gives the well known bit reversal permutation.

\[\mathbf{P}_{b,b^{\prime}}^{R}=\prod_{a=0}^{p-2}\mathbf{P}_{b^{p-a-1},b^{p-a},b ^{p}}.\]

If \(\sigma^{R}(b,N)\) is the permutation corresponding to the permutation matrix above, then \(\sigma^{R}(b,N)\) maps \(\left(i_{p-1},\ldots,i_{0}\right)\mapsto\left(i_{0},\ldots,i_{p-1}\right)\).

#### d.6.2 Generalizing \(\mathrm{Theorem}\) 1 and \(\mathrm{Theorem}\) 2

For a specific \(0\leq a<p\), let

\[\ell_{\mathbf{j},\mathbf{i}}^{(a)}\left(X_{a}\right)=\sum_{m=0}^{\sqrt[p]{N}-1 }\ell_{(j_{a+1},\ldots,j_{p-1}),(i_{0},\ldots,i_{a-1})}^{(a)}\left[m\right] \cdot T_{m}(X_{a}) \tag{73}\]

be an arbitrary polynomial of degree \(<\sqrt[p]{N}\) in the Chebyshev polynomial basis (see (47)).

We will be interested in the evaluations of the above polynomials over the set

\[A\stackrel{{\text{\tiny{def}}}}{{=}}\left(\omega\sqrt[p]{N},0, \ldots,\omega\sqrt[p]{N},\sqrt[p]{N}_{-1}\right), \tag{74}\]

where \(\omega_{\sqrt[p]{N},i}\) is defined as in (**??**).

The \(p\)-variate version of Monarch matrices \(\mathbf{M}^{\prime}\in\mathbb{R}^{N\times N}\) as follows. (See Appendix D.6.4 to see how these are exactly related to the definition in (1)). For every row index \(\mathbf{i}\in[0,\sqrt[p]{N})^{p}\) and column index \(\mathbf{j}\in[0,\sqrt[p]{N})^{p}\), we have

\[\mathbf{M}^{\prime}\left[\mathbf{i},\mathbf{j}\right]=\prod_{a=0}^{p-1}\ell_{ \mathbf{j},\mathbf{i}}^{(a)}\left(\omega\sqrt[p]{N},i_{a}\right). \tag{75}\]

To express the above in terms of a polynomial basis we will need the following definition. For any \(0\leq a<p\) and \(\mathbf{i}\) define the Lagrange basis polynomial \(\Delta_{\mathbf{i}}^{(a)}\left(X_{0},\ldots,X_{a-1}\right)\) such that for any \(0\leq m_{0},\ldots,m_{a-1}<\sqrt[p]{N}\), we have

\[\Delta_{\mathbf{i}}^{(a)}\left(\omega\sqrt[p]{N},m_{0},\ldots,\omega\sqrt[p]{ N},m_{a-1}\right)=\begin{cases}1&\text{if }i_{0}=m_{0},i_{1}=m_{1},\ldots,i_{a-1}=m_{a-1}\\ 0&\text{otherwise.}\end{cases}\]

We use the above to convert the polynomials in (73) to not depend on the sub-indices in \(\mathbf{i}\) (at least for the definition in (75)). For every \(\mathbf{j}\) and \(0\leq a<p\), define:

\[\ell_{\mathbf{j}}^{(a)}\left(X_{0},\ldots,X_{a}\right)=\sum_{\mathbf{i}=(i_{0},\ldots,i_{a-1},\mathbf{0}_{p-a}),i_{0},\ldots,i_{a-1}\in[0,\sqrt[p]{N})} \Delta_{\mathbf{i}}^{(a)}\left(X_{0},\ldots,X_{a-1}\right)\cdot\ell_{\mathbf{j },\mathbf{i}}^{(a)}\left(X_{a}\right).\]

Note that the summation fixes the last \(p-a\) sub-indices in \(\mathbf{i}\) since the definition of \(\ell_{\mathbf{j},\mathbf{i}}^{(a)}\left(X_{a}\right)\) only depends on \((i_{0},\ldots,i_{a-1})\). This implies that for any \(0\leq i_{0},\ldots,i_{a}<\sqrt[p]{N}\), we have

\[\ell_{\mathbf{j}}^{(a)}\left(\omega\sqrt[p]{N},i_{0},\ldots,\omega\sqrt[p]{N},i_{a}\right)=\ell_{\mathbf{j},\mathbf{i}}^{(a)}\left(\omega\sqrt[p]{N},i_{a} \right). \tag{76}\]

[MISSING_PAGE_FAIL:52]

Note that now the RHS only depends on \(a\) and \(j_{a}\) (let us call the RHS \(\widehat{\ell}^{\binom{a,\sqrt[V]{N}}{j_{a}}}_{j_{a}}\left(X_{a}\right)\)), so the next definition becomes easier:

\[\widehat{\ell}^{\binom{a}{j}}_{\mathbf{j}}\left(X_{0},\ldots,X_{a}\right)= \widehat{\ell}^{\binom{a}{j_{a}}\,\sqrt[V]{N}}_{j_{a}}\left(X_{a}\right).\]

We are now ready to define our causal basis polynomials. For any index \(\mathbf{j}\), define

\[\widehat{q}^{N}_{\mathbf{j}}\left(X_{0},\ldots,X_{p-1}\right)=\prod_{a=0}^{p-1 }\widehat{\ell}^{\binom{a}{j}}_{\mathbf{j}}\left(X_{0},\ldots,X_{a}\right). \tag{83}\]

These polynomials form a structured subclass of the polynomials defined in (77).

**Lemma 17**.: _The class of polynomials \(\widetilde{q}^{N}_{\mathbf{j}}\left(X_{0},\ldots,X_{p-1}\right)\) defined in (83) are a special case of (77)._

Proof.: This follows from the fact that (82) is a special case of (73) for every \(\mathbf{i},\mathbf{j},0\leq a<p\). 

We show that the product of two \(\widetilde{q}^{\left\lfloor\frac{\sqrt[V]{N}}{2}\right\rfloor^{p}}_{\mathbf{j} }\left(X_{0},\ldots,X_{p-1}\right)\) type polynomials can be written has a linear combination of \(\widetilde{q}^{N}_{\mathbf{m}}\left(X_{0},\ldots,X_{p-1}\right)\) with the indices of \(\mathbf{m}\) being lexicographically larger than the original indices.

**Lemma 18**.: _Let \(\widetilde{q}^{\left\lfloor\frac{\sqrt[V]{N}}{2}\right\rfloor^{p}}_{\mathbf{j} }\left(X_{0},\ldots,X_{p-1}\right)\) be defined as in (83). Then for any \(\mathbf{j},\mathbf{j}^{\prime}\in\left[0,\frac{\sqrt[V]{N}}{2}\right)^{p}\),_

\[\widetilde{q}^{\left\lfloor\frac{\sqrt[V]{N}}{2}\right\rfloor^{p}}_{\mathbf{j} }\left(X_{0},\ldots,X_{p-1}\right)\cdot\widetilde{q}^{\left\lfloor\frac{\sqrt[ V]{N}}{2}\right\rfloor^{p}}_{\mathbf{j}}\left(X_{0},\ldots,X_{p-1}\right)=\sum_{ \mathbf{j}+\mathbf{j}^{\prime}\preceq\mathbf{m}\in\left[0,\sqrt[V]{N}\right)^ {p}}\alpha_{\mathbf{j}+\mathbf{j}^{\prime},\mathbf{m}}\,\widetilde{q}^{N}_{ \mathbf{m}}\left(X_{0},\ldots,X_{p-1}\right) \tag{84}\]

_for some set of coefficients \(\alpha_{\mathbf{j}+\mathbf{j}^{\prime},\mathbf{m}}\)._

Proof.: From (83) we have,

\[\widetilde{q}^{\left\lfloor\frac{\sqrt[V]{N}}{2}\right\rfloor^{p}}_{\mathbf{j} }\left(X_{0},\ldots,X_{p-1}\right)\cdot\widetilde{q}^{\left\lfloor\frac{\sqrt[ V]{N}}{2}\right\rfloor^{p}}_{\mathbf{j}}\left(X_{0},\ldots,X_{p-1}\right)= \prod_{a=0}^{p-1}\widehat{\ell}^{\binom{a,\left\lfloor\frac{\sqrt[V]{N}}{2} \right\rfloor}{j_{a}}}_{\mathbf{j}_{a}}\left(X_{a}\right)\cdot\widehat{\ell}^{ \binom{a,\left\lfloor\frac{\sqrt[V]{N}}{2}\right\rfloor}{j_{a}^{\prime}}}_{ \mathbf{j}_{a}}\left(X_{a}\right) \tag{85}\]

Let us fix \(0\leq a<p\).

Because (82) is of the same form as in (48), we can apply Lemma 8 to each product \(\widehat{\ell}^{\binom{a,\left\lfloor\frac{\sqrt[V]{N}}{2}\right\rfloor}{j_{a} ^{\prime}}}_{\mathbf{j}_{a}}\left(X_{a}\right)\), which gives us

\[\widetilde{\ell}^{\binom{a,\left\lfloor\frac{\sqrt[V]{N}}{2}\right\rfloor}{j_ {a}}}_{\mathbf{j}_{a}}\left(X_{a}\right)\cdot\widehat{\ell}^{\binom{a,\left \lfloor\frac{\sqrt[V]{N}}{2}\right\rfloor}{j_{a}^{\prime}}}_{\mathbf{j}_{a}} \left(X_{a}\right)=\sum_{m_{a}=j_{a}+j_{a}^{\prime}}^{\sqrt[V]{N}-1}\alpha^{ \binom{a}{j_{a}+j_{a}^{\prime},m_{a}}}_{j_{a}+j_{a}^{\prime},m_{a}}\cdot \widehat{\ell}^{\binom{a,\sqrt[V]{N}}{2}}_{\mathbf{m}_{a}}\left(X_{a}\right).\]

Going back to (85), we get

\[\widetilde{q}^{\left\lfloor\frac{\sqrt[V]{N}}{2}\right\rfloor^{p}}_{\mathbf{j} }\left(X_{0},\ldots,X_{p-1}\right)\cdot\widetilde{q}^{\left\lfloor\frac{\sqrt[V] {N}}{2}\right\rfloor^{p}}_{\mathbf{j}}\left(X_{0},\ldots,X_{p-1}\right)= \prod_{a=0}^{p-1}\sum_{m_{a}=j_{a}+j_{a}^{\prime}}^{N-1}\alpha^{\binom{a}{j_{a }+j_{a}^{\prime},m_{a}}}_{j_{a}+j_{a}^{\prime},m_{a}}\cdot\widehat{\ell}^{ \binom{a,\sqrt[V]{N}}{2}}_{\mathbf{m}_{a}}\left(X_{a}\right).\]

Let \(\alpha_{\mathbf{j}+\mathbf{j}^{\prime},\mathbf{m}}=\prod_{a=0}^{p-1}\alpha^{ \binom{a}{j_{a}+j_{a}^{\prime},m_{a}}}_{\mathbf{j}_{a}+j_{a}^{\prime},m_{a}}\

[MISSING_PAGE_FAIL:54]

\(\left(\left\lfloor\frac{\sqrt{N}}{2}\right\rfloor-j_{0}-1,\ldots,\left\lfloor\frac{ \sqrt{N}}{2}\right\rfloor-j_{p-1}-1\right)\) for \(\mathbf{j}=\left(j_{0},\cdots,j_{p-1}\right)\) such that \(0\leq j_{a}<\left\lceil\frac{\sqrt{N}}{2}\right\rceil\) for \(0\leq a<p\). Further, for \(\mathbf{j^{\prime}}=\left(j_{0}+\left\lceil\frac{\sqrt{N}}{2}\right\rceil, \cdots,j_{p-1}+\left\lceil\frac{\sqrt{N}}{2}\right\rceil\right)\) we have

\[\deg\left(\widehat{q}_{\mathbf{j^{\prime}}}^{N}\right) =\left(\sqrt[q]{N}-j_{0}^{\prime}-1,\ldots,\sqrt[q]{N}-j_{p-1}^{ \prime}-1\right)\] \[=\left(\sqrt[q]{N}-\left\lceil\frac{\sqrt[q]{N}}{2}\right\rceil-j_ {0}-1,\ldots,\sqrt[q]{N}-\left\lceil\frac{\sqrt[q]{N}}{2}\right\rceil-j_{p-1} -1\right)\] \[=\left(\left\lfloor\frac{\sqrt[q]{N}}{2}\right\rfloor^{p}-j_{0}-1,\ldots,\left\lfloor\frac{\sqrt[q]{N}}{2}\right\rfloor^{p}-j_{p-1}-1\right).\]

Since \(\deg\left(\widehat{q}_{\mathbf{j^{\prime}}}^{\left\lfloor\frac{\sqrt{N}}{2} \right\rfloor^{p}}\right)=\deg\left(\widehat{q}_{\mathbf{j^{\prime}}}^{N}\right)\), we can set \(\widehat{\ell}_{\mathbf{j^{\prime}}\mathbf{j}}^{\left(a\right)}\left(X_{a} \right)=\widehat{\ell}_{\mathbf{j^{\prime}}\mathbf{j}}^{\left(a\right)}\left( X_{a}\right)\). Similarly, note that for \(\mathbf{j},\mathbf{j^{\prime}}\) as above, \(k_{\mathbf{j^{\prime}}}^{\prime}=k_{\mathbf{j}}\) and \(u_{\mathbf{j^{\prime}}}^{\prime}=u_{\mathbf{j}}\). Then it follows that \(k\left(X_{0},\ldots,X_{p-1}\right)=k^{\prime}\left(X_{0},\ldots,X_{p-1}\right)\), and by a similar argument, \(u\left(X_{0},\ldots,X_{p-1}\right)=u^{\prime}\left(X_{0},\ldots,X_{p-1}\right)\). Then by Theorem 16 we have

\[f\left(X_{0},\ldots,X_{p-1}\right) =k\left(X_{0},\ldots,X_{p-1}\right)\cdot u\left(X_{0},\ldots,X_{p -1}\right)\mod\left(T_{\sqrt[q]{N}}\left(X_{0}\right),\ldots,T_{\sqrt[q]{N}} \left(X_{p-1}\right)\right)\] \[=k\left(X_{0},\ldots,X_{p-1}\right)\cdot u\left(X_{0},\ldots,X_{p -1}\right) \tag{90}\]

where the second line follows by observing that each \(0\leq a<p\), we have \(\deg_{X_{a}}\left(k(\mathbf{X})\cdot u(\mathbf{X})\right)<2\left\lfloor\frac{ \sqrt[q]{N}}{2}\right\rfloor\) and observing that \(2\left\lfloor\frac{\sqrt[q]{N}}{2}\right\rfloor\leq\sqrt[q]{N}\). We want to write \(f\left(X_{0},\ldots,X_{p-1}\right)\) in the form

\[f\left(X_{0},\ldots,X_{p-1}\right)=\sum_{\mathbf{m}\in\left[0,\sqrt[q]{N} \right)^{p}}f_{\mathbf{m}}\cdot\widehat{q}_{\mathbf{m}}^{N}\left(X_{0},\ldots,X_{p-1}\right),\]

for a set of coefficients \(f_{\mathbf{m}}\). From (89) and (90) we get

\[f\left(X_{0},\ldots,X_{p-1}\right)=\sum_{\mathbf{j^{\prime}}\in\left[0,\left \lfloor\frac{\sqrt[q]{N}}{2}\right\rfloor\right)^{p}}k_{\mathbf{j^{\prime}}}u_ {\mathbf{j}}\cdot\widehat{q}_{\mathbf{j^{\prime}}}^{\left\lfloor\frac{\sqrt[q] {N}}{2}\right\rfloor^{p}}\left(X_{0},\ldots,X_{p-1}\right)\cdot\widehat{q}_{ \mathbf{j^{\prime}}}^{\left\lfloor\frac{\sqrt[q]{N}}{2}\right\rfloor^{p}}\left( X_{0},\ldots,X_{p-1}\right).\]

Then by Lemma 18 we have

\[f\left(X_{0},\ldots,X_{p-1}\right)=\sum_{\mathbf{j^{\prime}}\in\left[0,\left \lfloor\frac{\sqrt[q]{N}}{2}\right\rfloor\right)^{p}}k_{\mathbf{j^{\prime}}}u_ {\mathbf{j}}\cdot\sum_{\mathbf{j+j^{\prime}}\preceq\mathbf{m}\in\left[0,\sqrt [q]{N}\right)^{p}}\alpha_{\mathbf{j+j^{\prime}}\mathbf{m}}\ \widehat{q}_{\mathbf{m}}^{N}\left(X_{0},\ldots,X_{p-1}\right).\]

Thus, for any \(\mathbf{m}\in\left[0,\sqrt[q]{N}\right)^{p}\), we have

\[f_{\mathbf{m}}=\sum_{\mathbf{j+j^{\prime}}\preceq\mathbf{m}}\alpha_{\mathbf{j +j^{\prime}}\mathbf{m}}\cdot k_{\mathbf{j^{\prime}}}u_{\mathbf{j}}\]

implying that \(f_{\mathbf{m}}\) depends only on \(\mathbf{u_{j}}\) for \(\mathbf{j}\preceq\mathbf{m}\), as desired.

#### d.6.4 \(p\)-variate Monarch

Recall that we have fixed \(b=\sqrt[q]{N}\) (and hence \(N=b^{p}\)).

Define the \(p\)-variate Monarch matrix as follows:

\[\mathbf{M}^{\prime}=\mathbf{P}_{b,N}^{R}\left(\prod_{a=0}^{p-2}\mathbf{B}_{p-1 -a}\cdot\left(\mathbf{P}_{b^{a+1},b^{a+2},N}\right)^{\top}\right)\mathbf{B}_{0}, \tag{91}\]

Where each \(\mathbf{B}_{a}\) is block diagonal with \(b\times b\) blocks for every \(0\leq a<p\). Recall that Equation (1) has a permutation \(\mathbf{P}_{0}\) at the end while the above definition does not have any permutation at the end. One trivial way to show the equivalence of above to Equation (1) is to define \(\mathbf{P}_{0}=\mathbf{I}\). In Appendix D.6.5,we show that exists other non-trivial choices for \(\mathbf{P}_{0}\). Further for \(1\leq i\leq p\), the \(\mathbf{P}_{i}\) in Equation (1) connect to the above definition as follows:

\[\mathbf{P}_{i}=\begin{cases}\left(\mathbf{P}_{bp^{-i},bp^{-i+1},N}\right)^{\top }&\text{for }1\leq i\leq p-1\\ \mathbf{P}_{b,N}^{R}&\text{for }i=p\end{cases}.\]

Finally we connect the above definition of \(p\)-variate Monarch to the earlier definition based on polynomial evaluation:

**Lemma 19**.: _Equation (75) can be written as Equation (91)._

Proof.: In (91), \(\mathbf{B}_{a}\) would correspond to the evaluations \(\ell_{\mathbf{j},\mathbf{i}}^{(a)}\left(\omega\,\wp_{\overline{N},i_{a}}\right)\) from earlier. Specifically the following holds for any \(0\leq a<p\). We index the \(q^{p-1}\) blocks of \(\mathbf{B}_{a}\) by \(\left(i_{0},\ldots,i_{a-1}\right),\left(j_{p-1},\ldots,j_{a+1}\right)\) and the row and column 'offsets' within each such block are indexed by \(i_{a}\) and \(j_{a}\) respectively. Connecting back to \(\ell_{\mathbf{j},\mathbf{i}}^{(a)}\left(\cdot\right)\) we set

\[\mathbf{B}_{a}^{\left(\left(i_{0},\ldots,i_{a-1}\right),\left(j_{p-1},\ldots,j _{a+1}\right)\right)}\left[i_{a},j_{a}\right]=\ell_{\mathbf{j},\mathbf{i}}^{(a )}\left(\omega\,\wp_{\overline{N},i_{a}}\right). \tag{92}\]

We will prove the claim as follows. Let \(\mathbf{e}_{\mathbf{j}}\in\mathbb{R}^{N}\) have a \(1\) in the \(\mathbf{j}^{\text{th}}\) location and \(0\)'s elsewhere. Our goal is to multiply this vector on the right of Equation (91) and show that we get the \(\mathbf{j}^{\text{th}}\) column of \(\mathbf{M}^{\prime}\) as defined in Equation (75). We will do so by induction.

Define \(\mathbf{y}_{0}=\mathbf{e}_{\mathbf{j}}\) and \(\mathbf{y}_{1}=\mathbf{B}_{0}\cdot\mathbf{y}_{0}\). Then for every \(1\leq a<p\), define

\[\mathbf{y}_{a+1}=\mathbf{B}_{a}\left(\mathbf{P}_{b^{p-a},b^{p-a+1},N}\right)^ {\top}\mathbf{y}_{a}.\]

Note that we have

\[\mathbf{M}^{\prime}\cdot\mathbf{e}_{\mathbf{j}}=\mathbf{P}_{b,N}^{R}\cdot \mathbf{y}_{p}. \tag{93}\]

Next, we claim that for every \(1\leq a\leq p\), we have for every \(\left(i_{0},\ldots,i_{a-1}\right)\in\left[0,\sqrt[p]{N}\right)^{a-1}\),

\[\mathbf{y}_{a}\left[\left(\left(i_{0},\ldots,i_{a-2}\right),\left(j_{p-1}, \ldots,j_{a}\right),i_{a-1}\right)\right]=\prod_{b=0}^{n-1}\ell_{\mathbf{j}, \mathbf{i}}^{(b)}\leftWe claim that Equation (94) is true for \(a+1\) from the above along with Equation (95) and Equation (92). Indeed, fix any \((i_{0},\ldots,i_{a-1})\). Then in Equation (95) the entry \(\mathbf{z}_{a}\left[\left((i_{0},\ldots,i_{a-1})\,,(j_{p-1},\ldots,j_{a})\right)\right]\) gets multiplied by the entries \(\mathbf{B}_{a}^{((i_{0},\ldots,i_{a-1}),(j_{p-1},\ldots,j_{a+1}))}\left[i_{a},j _{a}\right]\) for all values of \(i_{a}\in[0,\sqrt[p]{N})\). The inductive hypothesis and Equation (92) then proves the inductive step, as desired. 

Finally, we note that we can generalize Algorithm 4 for constructing \(\mathbf{B}_{a}\) for \(0\leq a<p\) (since this is the multivariate case some of the steps are a bit simplified):

```
0:\(\widetilde{\mathbf{B}}_{(0)},\ldots,\widetilde{\mathbf{B}}_{(p-1)}\in\mathbb{R}^ {N\times b}\) where \(b=\sqrt[p]{N}\)
0: Block diagonal matrices \(\mathbf{B}_{0},\ldots,\mathbf{B}_{p-1}\in\mathbb{R}^{N\times N}\)
1:for\(y\gets 0\) to \(p-1\)do
2:for\(a\gets 0\) to \(\frac{N}{b}-1\)do
3:\(\mathbf{B}_{y}^{(a)}\leftarrow\mathbf{C}_{b}\cdot\widetilde{\mathbf{B}}_{(y)} ^{(a)}[ab:ab+b-1,:]\)
4:\(\mathbf{B}_{y}\leftarrow\text{diag}(\mathbf{B}_{y}^{(0)},\ldots\mathbf{B}_{y} ^{(\frac{N}{b}-1)})\)
5:return\(\mathbf{B}_{0},\ldots,\mathbf{B}_{p-1}\)
```

**Algorithm 6**Blocky MultiVar Monarch\((\widetilde{\mathbf{B}}_{(0)},\ldots\widetilde{\mathbf{B}}_{(p-1)},N,p)\)

#### d.6.5 Extensions and Open Questions

In this sub-section we outline certain (fairly) straightforward extension of our theoretical results and conclude with some open questions.

Comparing Equation (91) to Equation (1) We note that we can post multiply \(\mathbf{M}\) in Equation (91) with a large class of permutations for which Theorem 17 still holds. We outline the technical reason why this is true. At the heart of the argument for why Equation (86) gives a causal map is Lemma 18. Specifically note that the sum in RHS in Equation (84), is over all \(\mathbf{j}+\mathbf{j}^{\prime}\preceq\mathbf{m}\). The main observation is that this partial order still holds if we permute the \(b\)-variate representation of \(\mathbf{j},\mathbf{j}^{\prime}\) and \(\mathbf{m}\) in the same way. In other words, for any permutation \(\sigma:[0,p)\rightarrow[0,p)\) if we define \(\sigma(\mathbf{j})=\left(j_{\sigma(0)},\ldots,j_{\sigma(p-1)}\right)\) and similarly \(\sigma(\mathbf{j}^{\prime}),\sigma(\mathbf{m})\). Then we still have \(\sigma(\mathbf{j})+\sigma(\mathbf{j}^{\prime})\preceq\sigma(\mathbf{m})\). This in turn implies the following. Let \(\mathbf{P}_{\sigma}\) be a permutation that maps \(\mathbf{j}\in[0,\sqrt[p]{N})^{p}\) to \(\sigma(\mathbf{j})\in[0,\sqrt[p]{N})^{p}\). Then Theorem 17 holds if we replace \(\mathbf{M}^{\prime}\) by \(\mathbf{M}\cdot\mathbf{P}_{\sigma}\) with \(\mathbf{M}\) as in Equation (91).

Evaluation pointsOur results as presented are for specific classes of evaluation points. A natural question to ask is if our results can be extended to more general set of evaluation points. It turns out that our results for \(p\)-variate Monarch matrices can be extended to a wider class of evaluation points. Specifically, for each \(0\leq a<p\), let \(S_{a}\subset\mathbb{C}\) with \(|S_{a}|=\sqrt[p]{N}\). Then our results in this sub-section hold if we replace the evaluation points from \(A^{p}\) to \(\times_{a=0}^{p-1}S_{a}\). The only thing that changes in our proofs is that in Theorem 16, we replace \(\mod\left(T_{\sqrt[p]{N}}\left(X_{0}\right),\ldots,T_{\sqrt[p]{N}}\left(X_{p- 1}\right)\right)\) by \(\mod\left(q_{S_{0}}\left(X_{0}\right),\ldots,q_{S_{p-1}}\left(X_{p-1}\right)\right)\), where \(q_{A}(Z)\) is as defined in Equation (81). This result can then be propagated throughout the rest of our proofs.

On the other hand, our results in Appendix D.3 and Appendix D.5 do exploit specific properties of the evaluation points (specifically \(\left(\omega_{N}^{i}\right)^{\sqrt{N}}=\omega_{\sqrt{N}}^{i_{0}}\) for Appendix D.3 and \(T_{\sqrt{N}}\left(\omega_{N,i}\right)=(-1)^{i_{1}}\omega_{\sqrt{N},i_{0}}\) for Appendix D.5). To generalize these results to other sets of evaluation points, we need the existence of degree \(\sqrt{N}\) polynomial that maps (in a \(\sqrt{N}\)-to-1 fashion) \(A\) to a set of \(\sqrt{N}\) elements. Another interesting open question is to avoid the blowup \(n\to 2^{p}\cdot n\) in Theorem 17 and ideally only pay a blowup \(n\to 2n\) for every \(p\geq 2\) as we were able to do in Appendix D.3 and Appendix D.5 (with \(p=2\)).
```
1fromeinopsimportrearrange
2importtorch
3fromtorchimportnn
4
5defblockdiag_matmul(x,w):
6returntorch.einsum(
7"bnm,...bm->...bn",w,x.view(*x.shape[:-1],w.shape[0],w.shape[-1])
8).reshape(*x.shape)
9
10classMonarchMatrix(nn.Module):
11
12def__init__(self,sqrt_n:int):
13super()__init__()
14self.sqrt_n=sqrt_n
15self.L=nn.Parameter(torch.randn((sqrt_n,sqrt_n,sqrt_n)))
16self.R=nn.Parameter(torch.randn((sqrt_n,sqrt_n,sqrt_n)))
17
18defforward(self,x):
19x=rearrange(x,"...(m)->...(n)=",n=self.sqrt_n)
20x=blockdiag_matmul(x,self.L)
21x=rearrange(x,"...(m)->...(n)=",n=self.sqrt_n)
22x=blockdiag_matmul(x,self.R)
23returnrearrange(x,"...(m)->...(n)=",n=self.sqrt_n)
24
25classMonarchMixerLayer(nn.Module):
26def__init__(self,sqrt_n:int,sqrt_d:int):
27super()__init__()
28self.m1=MonarchMatrix(sqrt_n)
29self.m2=MonarchMatrix(sqrt_n)
30self.m3=MonarchMatrix(sqrt_d)
31self.m4=MonarchMatrix(sqrt_d)
32
33self.n_kernel=nn.Parameter(torch.randn(sqrt_d**2,sqrt_n
34**2))
35self.d_kernel=nn.Parameter(torch.randn(1,sqrt_d**2))
36self.layer_norm=nn.LayerNorm(sqrt_d**2)
37defforward(self,x:torch.Tensor):#x.shape=(b,n,d)
38x_tilde=self.m2(self.n_kernel*self.m1(x.transpose(-1,-2))).transpose(-1,-2)#mixsequence
39y=self.m4(torch.relu(self.d_kernel*self.m3(x_tilde)))#mixfeatures
40returnself.layer_norm(y+x_tilde)#skipconnection
```

Listing 1: A basic implementation of the M2 layer.