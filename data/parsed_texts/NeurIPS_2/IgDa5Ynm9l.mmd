# Efficient Model-Free Exploration in Low-Rank MDPs

 Zakaria Mhammedi MIT

mhammedi@mit.edu &Adam Block MIT

ablock@mit.edu &Dylan J. Foster

Microsoft Research

dylanfoster@microsoft.com &Alexander Rakhlin

MIT

rakhlin@mit.edu

###### Abstract

A major challenge in reinforcement learning is to develop practical, sample-efficient algorithms for exploration in high-dimensional domains where generalization and function approximation is required. _Low-Rank Markov Decision Processes_--where transition probabilities admit a low-rank factorization based on an unknown feature embedding--offer a simple, yet expressive framework for RL with function approximation, yet existing algorithms either (1) are computationally intractable, or (2) require restrictive statistical assumptions such as latent variable structure or access to model-based function approximation. In this work, we propose the first provably sample-efficient algorithm for exploration in Low-Rank MDPs that is both computationally efficient and model-free, allowing for general function approximation while requiring no structural assumptions beyond a reachability condition that we show is substantially weaker than that assumed in prior work. Our algorithm, SpanRL, uses the notion of a _barycentric spanner_ for the feature embedding as an efficiently computable basis for exploration, performing efficient spanner computation by interleaving representation learning and policy optimization subroutines. Our analysis--which is appealingly simple and modular--carefully combines several techniques, including a new approach to error-tolerant barycentric spanner computation, and a new analysis of a certain minimax representation learning objective found in prior work.

## 1 Introduction

In reinforcement learning and control, many of the most promising application domains require the agent to navigate complex, high-dimensional state and action spaces, where generalization and function approximation is necessary. The last decade has witnessed impressive empirical success in domains where data are abundant [34, 38, 26, 28, 27], but when data are limited, ensuring efficient exploration in large domains is a major research question. For _statistical efficiency_, the foundations have recently begun to take shape, with a line of research providing structural conditions that facilitate sample-efficient exploration, as well as fundamental limits [37, 21, 39, 42, 14, 24, 17, 18]. _Computational efficiency_, however, remains a major challenge: outside of simple settings [7, 23], existing algorithms with provable sample complexity guarantees are computationally inefficient, and typically require solving intractable non-convex optimization problems [21, 11, 24, 10]. The prospect of developing practical algorithms for exploration in high-dimensional state spaces that are both computationally and statistically efficient raises three fundamental questions:

1. What are the right computational primitives for exploration? That is, how can one efficiently represent and compute exploratory policies that allow the learner to explore the state space and gather useful data?
2. How should one leverage function approximation--for example, via representation learning--to discover such primitives in a computationally and statistically efficient fashion?
3. Given answers to the first two questions, how can one efficiently interleave function approximation and exploration to provide provably efficient algorithms?In this paper, we investigate these questions through the _Low-Rank MDP_ model [36, 45, 1]. In a Low-Rank MDP, the state space is large and potentially continuous, but the transition probabilities admit an (unknown) low-rank factorization. Concretely, for a finite-horizon Low-Rank MDP with horizon \(H\), the transition densities for layer \(h\in[H]\) satisfy

\[T_{h}(x_{h+1}\mid x_{h},a_{h})=\mu^{*}_{h+1}(x_{h+1})^{\top}\phi^{*}_{h}(x_{h}, a_{h}), \tag{1}\]

where \(\phi^{*}_{h}(\cdot,\cdot)\in\mathbb{R}^{d}\) and \(\mu^{*}_{h+1}(\cdot)\in\mathbb{R}^{d}\) are state-action and next-state embeddings. The low-rank structure in (1) facilitates tractable exploration: if the embedding \(\phi^{*}_{h}\) is known to the learner, one can efficiently learn a near-optimal policy with sample complexity polynomial in the feature dimension \(d\), and independent of the size of the state space [23]; in this regard, \(\phi^{*}_{h}\) can be thought of as a low-dimensional _representation_ that enables sample-efficient RL. Following Agarwal et al. [1], we consider the challenging setting in which both \(\phi^{*}_{h}\) and \(\mu^{*}_{h+1}\) are _unknown_ to the learner. This formulation generalizes well-known frameworks such as the _Block MDP_ (BMDP) model [12, 32], and necessitates the use of _representation learning_: the agent must learn an embedding that approximates \(\phi^{*}_{h}\) as it explores the environment, and must use this learned embedding to drive subsequent exploration. This form of function approximation allows for great flexibility, as \(\phi^{*}_{h}\) can be an arbitrary, nonlinear function of the state; in practice, it is common to model \(\phi^{*}_{h}\) as a neural net [49].

The Low-Rank MDP is perhaps the simplest MDP structure that demands systematic exploration and nonlinear function approximation while allowing for a continuum of states, yet understanding of _efficient_ algorithm design for this model is surprisingly limited. Existing algorithms suffer from at least one of the following drawbacks:

1. Computational intractability [21, 24, 14, 9, 43].
2. Strong modeling assumptions (e.g., ability to model \(\mu^{*}_{h+1}(\cdot)\), which facilitates application of model-based RL techniques) [1, 40, 10]; in this work, we aim for _model-free_ methods that only require learning \(\phi^{*}_{h}\).
3. Restrictive structural assumptions (e.g., non-negativity or latent variable structure for the embeddings in (1)) [35, 49].

At the root of these limitations is the complex interplay between exploration and representation learning: the agent must learn a high-quality representation to guide in exploring the state space, but learning such a representation requires gathering diverse and informative data, which is difficult to acquire without having already explored the state space to begin with. Overcoming this challenge--particularly where computational efficiency is concerned--requires (1) representation learning procedures that lead to sufficiently expressive representations for downstream applications, (2) efficient exploration procedures that are robust to errors in learned representations, and 3) understanding the interaction between these procedures, which must be interleaved. In this work, we propose an algorithm that addresses each of these challenges, as detailed below.

Contributions.We provide the first provably computationally efficient and model-free algorithm for general Low-Rank MDPs. Our algorithm, SpanRL, uses the notion of a _barycentric spanner_[6] for the embedding \(\phi^{*}_{h}\) as an efficiently computable basis for exploration, and combines this with a minimax representation learning approach [35, 49]. SpanRL interleaves exploration with representation learning in a layer-wise fashion, learning a new representation at each layer \(h\) using exploratory data gathered at previous layers, then uses this representation to facilitate computation of a collection of exploratory policies (a _policy cover_), which act as an approximate barycentric spanner for the features at layer \(h+1\), ensuring good coverage for subsequent iterations. SpanRL is simple and modular, and its analysis is surprisingly compact given the greater generality compared to prior work [49, 35, 31].

SpanRL can accommodate general-purpose function approximation to learn the representation \(\phi^{*}\) (e.g., neural nets or other flexible classes) whenever a certain minimax representation learning objective [35, 49] can be solved efficiently for the function class of interest. Compared to efficient algorithms from prior work, SpanRL: (1) is model-free (i.e., only requires access to a function class \(\Phi\) capable of modeling \(\phi^{*}\), and does not need to model \(\mu^{*}_{h+1}\)), and (2) applies to general Low-Rank MDPs, replacing strong additional assumptions such as non-negativity of the feature embeddings (so-called _latent variable_ structure) or block structure (see Table 1) with a reachability assumption that we show is substantially weaker than that assumed in prior work (see Appendix H). As a secondary benefit, the algorithm is reward-free. Our analysis carefully combines several new techniques, including (1) an error-tolerant variant of the classical barycentric spanner computation algorithm of Awerbuchand Kleinberg [6], and (2) a new analysis of a minimax representation learning objective introduced in Modi et al. [35], Zhang et al. [49], which shows for the first time that this objective can lead to meaningful guarantees in general Low-Rank MDPs without latent variable structure; this increased generality is meaningful, as we show in Appendix H that there is an exponential separation between our guarantees and those that require such a structure.

Organization.Section 2 formally introduces the Low-Rank MDP model and the online reinforcement learning framework we consider. In Section 3, we highlight challenges faced by previous approaches, introduce our main algorithm, SpanRL, and show how it overcomes these challenges, and then present its main sample complexity guarantee.

Comparison to ArXiv Version.After the initial submission of this work, we developed a substantially improved of the algorithm that removes the reachability assumption at the cost of a larger (but still polynomial) sample complexity guarantee. We have included this algorithm and its analysis in the ArXiv version of this paper [30].

## 2 Problem Setting

### Low-Rank MDP Model

We work in an episodic, finite-horizon reinforcement learning framework, where \(H\in\mathbb{N}\) denotes the horizon. A _Low-Rank MDP_[36, 45, 1] is a tuple \(\mathcal{M}=(\mathcal{X},\mathcal{A},(\phi^{*}_{h})_{h\in[H]},(\mu^{*}_{h})_{ h\in[H]},\rho)\) consisting of a _state space_\(\mathcal{X}\), _action space_\(\mathcal{A}\) with \(|\mathcal{A}|=A\), distribution over initial states \(\rho\in\Delta(\mathcal{X})\), and mappings \(\mu^{*}_{h+1}:\mathcal{X}\rightarrow\mathbb{R}^{d}\) and \(\phi^{*}_{h}:\mathcal{X}\times\mathcal{A}\rightarrow\mathbb{R}^{d}\).3 Beginning with \(\mathbf{x}_{1}\sim\rho\), an episode proceeds in \(H\) steps, where for each step \(h\in[H]\), the state \(\mathbf{x}_{h}\) evolves as a function of the agent's action \(\mathbf{a}_{h}\) via

Footnote 3: For the stated sample complexity, FLAMBE requires access to a sampling oracle for the learner model. Without this oracle, the results require additional latent variable structure and a reachability assumption.

\[\mathbf{x}_{h+1}\sim T_{h}(\cdot\mid\mathbf{x}_{h},\mathbf{a}_{h}),\]

where \(T_{h}\) is a probability transition kernel, which is assumed to factorize based on \(\phi^{*}_{b}\) and \(\mu^{*}_{h}\). In detail, we assume that there exists a \(\sigma\)-finite measure \(\nu\) on \(\mathcal{X}\) such that for all \(1\leq h\leq H-1\), and for all \(x\in\mathcal{X}\) and \(a\in\mathcal{A}\), the function \(x^{\prime}\mapsto\mu^{*}_{h+1}(x^{\prime})^{\top}\phi^{*}_{h}(x,a)\) is a probability density with respect to \(\nu\) (i.e. the function is everywhere non-negative and integrates to \(1\) under \(\nu\)). For any \(\mathcal{X}^{\prime}\subseteq\mathcal{X}\), the probability that \(\mathbf{x}_{h+1}\in\mathcal{X}^{\prime}\) under \(\mathbf{x}_{h+1}\sim T_{h}(\cdot\mid x_{h},a_{h})\) is then assumed to follow the law

\[T_{h}(\mathcal{X}^{\prime}\mid x_{h},a_{h})=\int_{\mathcal{X}^{\prime}}\mu^{* }_{h+1}(x)^{\top}\phi^{*}_{h}(x_{h},a_{h})\mathrm{d}\nu(x). \tag{2}\]

For notational compactness, we assume (following, e.g., Jiang et al. [21]) that the MDP \(\mathcal{M}\) is _layered_ so that \(\mathcal{X}=\mathcal{X}_{1}\cup\dots\cup\mathcal{X}_{H}\) for \(\mathcal{X}_{i}\cap\mathcal{X}_{j}=\varnothing\) for all \(i\neq j\), where \(\mathcal{X}_{h}\subseteq\mathcal{X}\) is the subset of states in \(\mathcal{X}\)

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & Comp. efficient & Model-free & General low rank & Sample comp. \\ \hline OLIVE [21] & ✗ & ✓ & ✓ & \(\frac{d^{3}\text{d}\mu^{5}_{h}\log[\lvert\Phi\rvert]}{\varepsilon^{2}}\) \\ (see also [24, 14, 9, 43]) & ✗ & ✓1 & \(\frac{d^{7}\text{d}\mu^{22}_{\log[\lvert\Phi\rvert]}\Upsilon)}{\varepsilon^{10}}\) \\ FLAMBE [1] & ✓ & ✗ & ✓1 & \(\frac{d^{4}\text{d}^{2}\text{d}^{5}_{\log[\lvert\Phi\rvert]}\Upsilon)}{ \varepsilon^{2}}\) \\ Rep-UCB [40] & ✓ & ✗ & ✓ & \(\frac{d^{4}\text{d}^{2}\text{d}^{2}\text{d}^{5}_{\log[\lvert\Phi\rvert]} \Upsilon)}{\varepsilon^{2}}\) \\ (see also [10]) & ✓ & ✗ & ✓ & \(\frac{d^{10}\text{d}^{2}\text{d}^{3}\text{d}^{2}\text{d}^{10}\log[\lvert\Phi\rvert]} {\varepsilon^{3}\text{d}^{7}\gamma\gamma^{11}}\) \\ MOFFLE [35]2 & ✓ & ✓ & ✗ & \(\frac{d^{10}\text{d}^{2}\text{d}^{3}\text{d}^{10}\log[\lvert\Phi\rvert]}{ \varepsilon^{4}}\) \\ BRIEE [49] & ✓ & ✓ & ✗ & Block MDP & \(\frac{\lvert\mathcal{S}^{8}\text{d}^{4}\text{d}^{10}\log[\lvert\Phi\rvert]}{ \varepsilon^{4}}\) \\ SpanRL (this paper) & ✓ & ✓ & ✓ & \(\frac{A^{4}\text{d}^{0}\text{d}^{4}\text{d}^{4}\text{d}\log[\lvert\Phi\rvert]} {\varepsilon^{2}\text{d}\nu^{2}}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Comparison of sample complexity required learn an \(\varepsilon\)-optimal policy. \(\Phi\) denotes the feature class, and \(\Upsilon\) denotes an additional feature class capturing model-based function approximation. For approaches that require non-negative (latent variable) structure, \(d_{\text{\tiny LV}}\) [resp. \(\gamma\)] denotes the latent variable dimension [resp. the reachability parameter in the latent representation], and for BMDPs, \(\lvert\mathcal{S}\rvert\) denotes the size of the latent state space. For SpanRL, \(\eta\) denotes the reachability parameter.

that are reachable at layer \(h\in[H]\). This can be seen to hold without loss of generality (modulo dependence on \(H\)), by augmenting the state space to include the layer index.

**Remark 2.1** (Comparison to previous formulations).: _Our formulation, in which the transition dynamics (2) are stated with respect to a base measure \(\nu\), are a rigorous generalization of Low-Rank MDP formulations found in previous works [23; 1], which tend to implicitly assume the state space is countable and avoid rigorously defining integrals. We adopt this more general formulation to emphasize the applicability our results to continuous domains. However, in the special case where state space is countable, choosing \(\nu\) as the counting measure yields \(T_{h}(\mathcal{X}^{\prime}\mid x_{h},a_{h})=\sum_{x\in\mathcal{X}^{\prime}}\mu^ {*}_{h+1}(x)^{\top}\phi^{*}_{h}(x_{h},a_{h})\), which is consistent with prior work._

**Policies and occupancy measures.** We define \(\Pi_{\mathsf{H}}=\{\pi:\mathcal{X}\to\Delta(\mathcal{A})\}\) as the set of all randomized, Markovian policies. For a policy \(\pi\in\Pi_{\mathsf{H}}\), we let \(\mathbb{P}^{\pi}\) denote the law of \(\big{(}\mathbf{x}_{1},\mathbf{a}_{1}\big{)},\ldots,\big{(}\mathbf{x}_{H},\mathbf{a}_{H}\big{)}\) under \(\mathbf{a}_{h}\sim\pi(\mathbf{x}_{h})\), and let \(\mathbb{E}^{\pi}\) denote the corresponding expectation. For any \(\mathcal{X}^{\prime}\subseteq\mathcal{X}_{h}\), we let \(\mathbb{P}^{\pi}_{h}[\mathcal{X}^{\prime}]=\mathbb{P}^{\pi}\big{[}\mathbf{x}_{h} \in\mathcal{X}^{\prime}\big{]}\) denote the marginal law of \(\mathbf{x}_{h}\) under \(\pi\). For \(x\in\mathcal{X}_{h}\), we define the _occupancy measure_\(d^{\pi}(x):=\frac{\mathrm{d}\mathbb{P}^{\pi}_{h}}{\mathrm{d}\nu}(x)\) as the density of \(\mathbb{P}^{\pi}_{h}\) with respect to \(\nu\).

### Online Reinforcement Learning and Reward-Free Exploration

We consider a standard _online reinforcement learning_ framework where the Low-Rank MDP \(\mathcal{M}\) is unknown, and the learning agent interacts with it in _episodes_, where at each episode the agent executes a policy of the form \(\pi:\mathcal{X}\to\Delta(\mathcal{A})\) and observes the resulting trajectory \((\mathbf{x}_{1},\mathbf{a}_{1}),\ldots,(\mathbf{x}_{H},\mathbf{a}_{H})\). While the ultimate goal of reinforcement learning is to optimize a policy with respect to a possibly unknown reward function, here we focus on the problem of _reward-free exploration_, which entails learning a collection of policies that almost optimally "covers" the state space, and can be used to efficiently optimize any downstream reward function [12; 33; 15; 31]. To wit, we aim to construct an _policy cover_, a collection of policies that can reach any state with near-optimal probability.

**Definition 2.1** (Policy cover).: _For \(\alpha\in(0,1]\), a subset \(\Psi\subseteq\Pi_{\mathsf{H}}\) is an \(\alpha\)-policy cover for layer \(h\) if_

\[\forall x\in\mathcal{X}_{h},\quad\max_{\pi\in\Psi}d^{\pi}(x)\geq\alpha\cdot\max _{\pi^{\prime}\in\Pi_{\mathsf{H}}}d^{\pi^{\prime}}(x). \tag{3}\]

We show (Appendix G) that given access to such a policy cover with constant \(\alpha\), it is possible to optimize any downstream reward function with polynomial sample complexity.

**Assumptions.** To facilitate learning a policy cover, we make the following _reachability_ assumption.

**Assumption 2.1** (\(\eta\)-reachability).: _For any \(h\in[H]\) and \(x\in\mathcal{X}_{h}\), \(\max_{\pi\in\Pi_{\mathsf{H}}}d^{\pi}(x)\geq\eta\cdot\|\mu^{*}_{h}(x)\|\)._

Reachability is necessary if one aims to build a policy cover that satisfies (3) uniformly for all states; without such a condition, one gives up on covering hard-to-reach states. Some notion of reachability is required in essentially all prior work on efficient model-free algorithms for Low-Rank MDPs [32; 35; 5], and was only very recently removed in the (more restrictive) BMDP setting [31; 49].

**Remark 2.2** (Comparison to other reachability-like assumptions).: _Assumption 2.1 generalizes and subsumes all previous reachability-like conditions of which we are aware [33; 46; 1; 35]. Notably, reachability is implied by the notion of feature coverage [5] (used in the context of transfer learning in Low-Rank MDPs), which asserts that \(\sup_{\pi\in\Pi_{\mathsf{H}}}\lambda_{\min}\big{(}\mathbb{E}^{\pi}[\phi^{*}_{ h}(\mathbf{x}_{h},\mathbf{a}_{h})\phi^{*}_{h}(\mathbf{x}_{h},\mathbf{a}_{h})^{\top}\big{)}\big{]} \geq\eta\), for some \(\eta>0\). It is also implied by explorability [46], which is similar to feature coverage, but involves the first moments of \(\phi^{*}_{h}\). Our reachability assumption is also weaker than that used in [1; 35] under the latent variable model, and generalizes that made for BMDPs [33]. See Appendix H for details, as well as an exponential separation between our assumptions and analogous assumptions in [1; 35]._

Beyond reachability, we assume (following [1; 35]) for normalization that, for all \(h\in[H]\) and \((x,a)\in\mathcal{X}_{h}\times\mathcal{A}\), \(\|\phi^{*}_{h}(x,a)\|\leq 1\), and that for all \(g:\mathcal{X}_{h}\to[0,1]\),

\[\left\|\int_{\mathcal{X}_{h}}\mu^{*}_{h}(x)g(x)\mathrm{d}\nu(x)\right\|\leq\sqrt {d}. \tag{4}\]

Function approximation and desiderata.We do not assume that the true features \((\phi^{*}_{h})_{h\in[H]}\) or the mappings \((\mu^{*}_{h})_{h\in[H]}\) are known to the learner. To provide sample-efficient learning guarantees we make use of function approximation as in prior work [3; 35], and assume access to a _feature class_\(\Phi\subseteq\{\phi:\mathcal{X}\times\mathcal{A}\to\mathbb{R}^{d}\}\) that contains \(\phi^{*}_{h}\), for \(h\in[H-1]\).

**Assumption 2.2** (Realizability).: _The feature class \(\Phi\in\{\phi:\mathcal{X}\times\mathcal{A}\to\mathbb{R}^{d}\}\) has \(\phi^{*}_{h}\in\Phi\) for all \(h\in[H]\). Moreover, for all \(\phi\in\Phi\), \(x\in\mathcal{X}\), and \(a\in\mathcal{A}\), it holds that \(\left|\phi(x,a)\right|\leq 1\)._

The class \(\Phi\) may consist of linear functions, neural networks, or other standard models depending on the application, and reflects the learner's prior knowledge of the underlying MDP. We assume that \(\Phi\) is finite to simplify presentation, but extension to infinite classes is straightforward, as our results only invoke finiteness through standard uniform convergence arguments. Note that unlike model-based approaches [1, 40, 10, 2], we do not assume access to a class capable of realizing the features \(\mu^{*}_{h}\), and our algorithm does not attempt to learn these features; this is why we distinguish our results as _model-free_.

For constant \(\alpha\), our goal is to learn an \(\alpha\)-policy cover using \(\mathrm{poly}(d,A,H,\log\lvert\Phi\rvert,\eta^{-1})\) episodes of interaction. This guarantee scales with the dimension \(d\) of the feature map and the complexity \(\log\lvert\Phi\rvert\) of the feature class but, critically, does not depend on the size of the state space \(\mathcal{X}\); by [10], dependence on \(H\) and \(A=\lvert\mathcal{A}\rvert\) is necessary when \(\phi^{*}\) is unknown. Given such a guarantee, we show in Appendix G how to optimize any downstream reward function to error \(\varepsilon\) with polynomial sample complexity.

Additional preliminaries.For any \(m,n\in\mathbb{N}\), we denote by \([m\mathinner{.\,.}n]\) the integer interval \(\{m,\ldots,n\}\). We also let \([n]\coloneqq[1\mathinner{.\,.}n]\). For any sequence of objects \(o_{1},o_{2},\ldots\), we define \(o_{m\mathinner{.\,.}n}\coloneqq(o_{i})_{i\in[m\mathinner{.\,.}n]}\). A _partial policy_ is a policy defined over a contiguous subset of layers \([\ell\mathinner{.\,.}r]\subseteq[H]\). We denote by \(\Pi^{\ell\tau}_{\mathsf{H}}\coloneqq\{\pi:\cup_{h=\ell}^{\mathsf{L}}\mathcal{ X}_{h}\to\Delta(\mathcal{A})\}\) the set of all partial policies over layers \(\ell\) to \(r\); note that \(\Pi_{\mathsf{H}}\equiv\Pi^{1:H}_{\mathsf{H}}\). For a policy \(\pi\in\Pi^{\ell\tau}_{\mathsf{H}}\) and \(h\in[\ell\mathinner{.\,.}r]\), \(\pi(x_{h})\) denotes the action distribution for the policy at layer \(h\) when \(x_{h}\in\mathcal{X}_{h}\) is the current state. For \(1\leq t\leq h\leq H\) and any pair of partial policies \(\pi\in\Pi^{1:t-1}_{\mathsf{H}},\pi^{t}\in\Pi^{t:h}_{\mathsf{H}}\), we define \(\pi_{\circ}\,\pi^{\prime}\in\Pi^{1:h}_{\mathsf{H}}\) as the partial policy given by \((\pi\circ_{t}\pi^{\prime})(x_{\ell})=\pi(x_{\ell})\) for all \(\ell<t\) and \((\pi\circ_{t}\pi^{\prime})(x_{\ell})=\pi^{\prime}(x_{\ell})\) for all \(\ell\in[t\mathinner{.\,.}h]\).

We use the \(\mathbf{x}_{h}\sim\pi\) as shorthand to indicate that \(\mathbf{x}_{h}\) is drawn from the law \(\mathbb{P}^{\pi}\), and likewise for \((\mathbf{x}_{h},\mathbf{a}_{h})\sim\pi\) and so on. For a set of partial policies \(\Psi\coloneqq\{\pi^{(i)}:i\in[N]\}\), we define \(\mathsf{unif}(\Psi)\) as the random partial policy obtained by sampling \(\mathbf{i}\sim\mathsf{unif}([N])\) and playing \(\pi^{(\mathbf{i})}\). We define \(\pi_{\mathsf{unif}}\in\Pi_{\mathsf{H}}\) as the random policy that selects actions in \(\mathcal{A}\) uniformly at random at each layer. We use \(\left\lVert\cdot\right\rVert\) to denote the Euclidean norm, \(\left\lVert\cdot\right\rVert_{\infty}\) to denote the supremum norm on functions, and let \(\mathcal{B}(r)\in\mathbb{R}^{d}\) denote the Euclidean ball of radius \(r\). We refer to a scalar \(c>0\) as an _absolute constant_ to indicate that it is independent of all problem parameters and use \(\tilde{O}(\cdot)\) to denote a bound up to factors polylogarithmic in parameters appearing in the expression.

## 3 SpanRL: Algorithm and Main Results

In this section, we present the SpanRL algorithm. We begin by describing challenges in deriving efficient, model-free algorithms using existing approaches (Section 3.1). We then formally describe SpanRL (Section 3.2) and build intuition as to how it is able to overcome these challenges, and finally state our main sample complexity guarantee (Section 3.3).

### Challenges and Related Work

Designing algorithms with provable guarantees in the Low-Rank MDP setting is challenging because of the complicated interplay between representation learning and exploration. Indeed, while there are many efficient algorithms for the so-called _linear MDP_ setting where the feature maps \((\phi^{*}_{h})_{h\in[H]}\) are known (removing the need for representation learning) [23, 47, 4, 41], these approaches do not readily generalize to accommodate unknown features. For Low-Rank MDPs, previous algorithms suffer from at least one of the following three drawbacks: (1) the algorithms are computationally inefficient; (2) the algorithms are model-based; or (3) the algorithms place strong assumptions on the MDP that are unlikely to hold in practice. To motivate the SpanRL algorithm, we briefly survey these results, highlighting several key challenges in avoiding these pitfalls.

Let us first discuss the issue of computational efficiency. While there are a number of algorithms--all based on the principle of _optimism in the face of uncertainty_--that provide tight sample complexity guarantees for Low-Rank MDPs in reward-based [21, 24, 14] and reward-free [9, 43] settings, these algorithms involve intractable optimization problems, and cannot be implemented efficiently even when the learner has access to an optimization oracle for the representation class \(\Phi\)[11]. This intractability arises because these algorithms implement optimism via a "global" approach, in which the algorithm explores at each round by choosing the most optimistic value function in a certain _version space_ of candidate value functions; optimizing over this version space is challenging,as it involves satisfying non-convex constraints with a complicated dependence on the learned representation, and because the constraints are coupled globally across layers \(h\in[H]\).

To avoid the intractability of global optimism, several works have restricted attention to a simpler _model-based_ setting. Here, in addition to assuming that the feature maps \((\phi^{*}_{h})_{h\in[H]}\) are realizable with respect to \(\Phi\), one assumes access to a second feature class \(\Upsilon\) capable of modeling the mappings \((\mu^{*}_{h})_{h\in[H]}\); this facilitates direct estimation of the transition probability kernel \(T_{h}(\cdot\mid x,a)\). For the model-based setting, it is possible to efficiently implement certain "local" forms of optimism [40, 10, 48], as well as certain non-optimistic exploration techniques based on policy covers [1]. Unfortunately, model-based realizability is a restrictive assumption, and falls short of the model-free guarantees we aim for in this work; indeed, in general, one cannot hope to estimate the feature map \(\mu^{*}_{h+1}\) without sample complexity scaling with the number of states.4

Footnote 4: For example, in the special case of the Block MDP setting [12, 32], model-based realizability entails modeling a certain emission process, which is not required by model-free approaches.

When one moves from model-based learning to model-free learning, representation learning becomes substantially more challenging--both for optimistic and non-optimistic approaches. Here, a key challenge is to develop representation learning procedures that are (1) efficient, yet (2) provide meaningful guarantees when the learned features are used downstream for exploration. To our knowledge, the only proposal for a representation learning procedure satisfying both desiderata comes from the work of Modi et al. [35], who introduced a promising "minimax" representation learning objective (described in detail in the sequel; cf. Algorithm 5), which Zhang et al. [49] subsequently showed to have encouraging empirical performance. However, to provide guarantees for this objective, both works place substantial additional restrictions on the low-rank factorization. In particular, Modi et al. [35] make the so-called _latent variable_ assumption [1], which asserts that \(\phi^{*}_{h}\) and \(\mu^{*}_{h}\) are non-negative coordinate-wise, and Zhang et al. [49] further restrict to the Block MDP model [12, 32]. Non-negativity is a substantial restriction, as the best non-negative factorization can have exponentially large dimension relative to the best unrestricted factorization [1], even when reachability is assumed (cf. Appendix H.1). The source of this restriction is the problem of how to quantify how close a learned representation \(\hat{\phi}\) is to the ground truth \(\phi^{*}\), which depends strongly on the downstream exploration strategy. In what follows, we show that with the right exploration strategy, this challenge can be ameliorated, but prior to our work it was unclear whether the minimax objective could lead to meaningful guarantees in the absence of non-negativity.

### The SpanRL Algorithm

Our algorithm, SpanRL, is presented in Algorithm 1. The algorithm proceeds by building a policy cover layer-by-layer in an inductive fashion. For each layer \(h\geq 2\), SpanRL uses a policy cover \(\Psi^{(h)}\) built at a previous iteration within a subroutine, RepLearn (Algorithm 5; deferred to Appendix B) to produce a feature map \(\hat{\phi}^{(h)}\) that approximates \(\phi^{*}_{h}\). Using this feature map, the algorithm invokes a second subroutine, RobustSpanner (Algorithm 2 in Appendix B) to produce a collection of policies \(\pi_{1},\ldots,\pi_{d}\) that act as a _barycentric spanner_ for the feature map, ensuring maximal coverage in a certain sense; given these policies, a new policy cover for layer \(h+2\) is formed via \(\Psi^{(h+2)}=\{\pi_{i}\circ_{h+1}\pi_{\text{unif}}:i\in[d]\}\). To invoke the RobustSpanner subroutine, SpanRL makes use of additional subroutines for policy optimization (PSDP; Algorithm 3 in Appendix B) and estimation of certain vector-valued functionals (EstVec; Algorithm 7 in Appendix B). We now describe each component of the algorithm in detail, highlighting how they allow us to overcome the challenges in the prequel.

Barycentric spanners.At the heart of SpanRL is the notion of a _barycentric spanner_[6] as an efficient basis for exploration. We begin by defining a barycentric spanner for an abstract set.

**Definition 3.1** (Awerbuch and Kleinberg [6]).: _Given a set \(\mathcal{W}\in\mathbb{R}^{d}\) such that \(\operatorname{span}(\mathcal{W})=\mathbb{R}^{d}\), we say that a set \(\{w_{1},\ldots,w_{d}\}\subseteq\mathcal{W}\) is a \((C,\varepsilon)\)-approximate barycentric spanner for \(\mathcal{W}\) if for every \(w\in\mathcal{W}\), there exist \(\beta_{1},\ldots,\beta_{d}\in[-C,C]\) such that \(\|w-\sum_{i=1}^{d}\beta_{i}w_{i}\|\leq\varepsilon\).5_

Footnote 5: Note that our definition is a slight generalization of [6, Definition 2.1]; the latter is recovered with \(\varepsilon=0\).

The utility of barycentric spanners for reward-free exploration is highlighted in the following lemma.

**Lemma 3.1**.: _Suppose that Assumption 2.1 holds. If \(\Psi\subseteq\Pi_{\mathsf{H}}\) is a collection of policies such that \(\{\mathbb{E}^{\pi}\left[\phi^{*}_{h}(\mathbf{x}_{h},\mathbf{a}_{h})\right]\mid\pi\in \Psi\}\subseteq\mathbb{R}^{d}\) is a \((C,\varepsilon)\)-approximate barycentric spanner for \(\mathcal{W}_{h}:=\{\mathbb{E}^{\pi}\left[\phi^{*}_{h}(\mathbf{x}_{h},\mathbf{a}_{h}) \right]\mid\pi\in\Pi_{\mathsf{H}}\}\) with \(\varepsilon\leq\frac{\eta}{2}\), then \(\Psi\) is an \(\alpha\)-policy cover for layer \(h+1\) with \(\alpha=(2dC)^{-1}\)._

[MISSING_PAGE_FAIL:7]

1. For all \(\theta\in\mathbb{R}^{d}\) with \(\left\lVert\theta\right\rVert=1\), the output \(\hat{z}_{\theta}:=\texttt{LinOpt}(\theta)\) satisfies \(\theta^{\intercal}w^{z_{\theta}}\geq\sup_{z\in\mathcal{Z}}\theta^{\intercal}w^{ z}-\varepsilon\).
2. For all \(z\in\mathcal{Z}\), the output \(\hat{w}_{z}:=\texttt{LinEst}(z)\) satisfies \(\left\lVert\hat{w}_{z}-w^{z}\right\rVert\leq\varepsilon\).

The RobustSpanner algorithm (Algorithm 2) computes a \((C,\varepsilon)\)-approximate spanner for \(\mathcal{W}\) using \(O(d\log(d/\varepsilon))\) total calls to LinOpt and LinEst. RobustSpanner is an error-tolerant variant of the classical spanner computation algorithm of Awerbuch and Kleinberg [6], which was originally introduced and analyzed for spanner computation with an _exact_ linear optimization oracle. Tolerance to approximation errors in the linear optimization oracle is critical for our application to RL, where additive errors will arise in sampling trajectories, as well as estimating the feature maps \((\phi_{h}^{*})_{h\in[H]}\). RobustSpanner achieves error tolerance by perturbing the vectors returned by LinOpt(\(\theta\)) in the direction of \(\theta\), which amounts to running the classical algorithm on an \(\varepsilon\)-fattening of \(\mathcal{W}\), and is necessary in order to ensure that the approximation error of LinOpt does not swamp the signal in directions \(\theta\) in which \(\mathcal{W}\) is too "skinny." This technique may be of independent interest; see Appendix C for additional details and formal guarantees.

Representation learning.Ideally, we would like to use RobustSpanner to construct a barycentric spanner for the set \(\{\mathbb{E}^{\pi}[\phi_{h}^{*}(\mathbf{x}_{h},\mathbf{a}_{h})]\mid\pi\in\Pi_{\mathsf{ H}}\}\) with \(\mathcal{Z}=\Pi_{\mathsf{H}}\). Because we do not have access to \(\phi_{h}^{*}\), we instead apply RobustSpanner with \(\mathcal{W}\coloneqq\{\mathbb{E}^{\pi}[\hat{\phi}^{(h)}(\mathbf{x}_{h},\mathbf{a}_{h}) ]\mid\pi\in\Pi_{\mathsf{H}}\}\), where \(\hat{\phi}^{(h)}\) is a learned representation. We now describe how the feature map \(\hat{\phi}^{(h)}\) is learned, then show how to use these learned features to efficiently implement the oracles LinOpt(\(\cdot\)) and LinEst(\(\cdot\)).

To learn a representation for layer \(h\), we use the Replenarn algorithm (Algorithm 5), which was originally introduced in Modi et al. [35], Zhang et al. [49]. The algorithm gathers a collection of triples \((\mathbf{x}_{h},\mathbf{a}_{h},\mathbf{x}_{h+1})\) by rolling in to \(\mathbf{x}_{h}\) with a policy sampled uniformly from the policy cover \(\Psi^{(h)}\) and selecting \(\mathbf{a}_{h}\) uniformly at random. Using this dataset, the algorithm solves a sequence of adversarial training sub-problems (Line 9 of Algorithm 5) which involve the feature class \(\Phi\) and an auxiliary discriminator class \(\mathcal{F}:\mathcal{X}\rightarrow\mathbb{R}\). As we discuss in detail in the sequel, these sub-problems, described in (7), are amenable to standard gradient-based training methods. The sub-problems are designed to approximate the following "idealized" max-min-max representation learning objective:

\[\hat{\phi}^{(h)}\in\operatorname*{arg\,min}_{\phi\in\Phi}\sup_{f\in\mathcal{F }}\inf_{w}\mathbb{E}^{\mathsf{uni}\operatorname{rf}(\Psi^{(h)})_{\Theta^{*} \pi_{\mathsf{uni}}}}\left[\left(\phi(\mathbf{x}_{h},\mathbf{a}_{h})^{\intercal}w- \mathbb{E}[f(\mathbf{x}_{h+1})\mid\mathbf{x}_{h},\mathbf{a}_{h})\right]^{2}\right]. \tag{6}\]

The intuition for this objective comes from the fact that in a Low-Rank MDP, for any function \(f:\mathcal{X}\rightarrow\mathbb{R}\), the quantity \(\mathbb{E}[f(\mathbf{x}_{h+1})\mid\mathbf{x}_{h}=x,\mathbf{a}_{h}=a]\) is linear in \(\phi_{h}^{*}(x,a)\). Thus, if \(\mathcal{F}\) is sufficiently expressive, we may hope that \(\hat{\phi}^{(h)}\) and \(\phi^{*}\) are close. We adopt the simple discriminator class \(\mathcal{F}=\{\,x\mapsto\max_{a\in\mathcal{A}}\theta^{\intercal}\phi(x,a)\mid \theta\in\mathcal{B}(1),\,\phi\in\Phi\}\). We show that solving (6) with this choice for \(\mathcal{F}\), which is simpler than that in Modi et al. [35], Zhang et al. [49], yields an approximation guarantee for \(\hat{\phi}^{(h)}\) that is suitable for downstream use in spanner computation for general Low-Rank MDPs.

**Remark 3.1** (Improved analysis of Replenarn).: _To facilitate an analysis of SpanRL that does not require reachability assumptions, we use slightly different parameter values for_ Replenarn _than in Modi et al. [35], Zhang et al. [49], and provide a tighter sample complexity bound (Theorem E.1) which may be of independent interest._

_In more detail, prior work shows that the_ Replenarn _algorithm solves a variant of (6) with \(w\in\mathcal{B}(d^{1/2}\cdot\operatorname{poly}(\varepsilon^{-1}))\), where \(\varepsilon>0\) is the desired bound on mean-squared error. Due to the polynomial dependence on \(\varepsilon^{-1}\), such a guarantee would lead to vacuous guarantees when invoked within our analysis of_ SpanRL_. Our improved analysis of_ Replenarn_, which is based on a determinantal potential argument, shows that \(w\in\mathcal{B}(\operatorname{poly}(d))\) suffices. A secondary benefit of our improved bound is a faster rate with respect to the number of trajectories._

Putting everything together.Having learned \(\hat{\phi}^{(h)}\) using RepLearn, in SpanRL we apply RobustSpanner with \(\mathcal{W}:=\{\mathbb{E}^{\pi}[\hat{\phi}^{(h)}(\mathbf{x}_{h},\mathbf{a}_{h})]\mid \pi\in\Pi_{\mathsf{H}}\}\), \(\mathcal{Z}=\Pi_{\mathsf{H}}\), and \(C=2\); that is, we plug-in the learned representation \(\hat{\phi}^{(h)}\) for the true representation \(\phi_{h}^{*}\).7 With this choice, implementing LinOpt entails (approximately) solving \(\operatorname*{arg\,max}_{\pi\in\Pi_{\mathsf{H}}}\mathbb{E}^{\pi}[\theta^{ \intercal}\hat{\phi}^{(h)}(\mathbf{x}_{h},\mathbf{a}_{h})]\) for a given \(\theta\in\mathcal{B}(1)\), and implementing the LinEst oracle entails estimating \(\mathbb{E}^{\pi}[\hat{\phi}^{(h)}(\mathbf{x}_{h},\mathbf{a}_{h})]\) for a given \(\pi\in\Pi_{\mathsf{H}}\). We instantiate\(\texttt{LinEst}(\pi)\) as the Monte Carlo algorithm \(\texttt{EstVec}\) (Algorithm 7), which simply samples trajectories according to \(\pi\) and returns the sample average of \(\hat{\phi}^{(h)}(\mathbf{x}_{h},\mathbf{a}_{h})\). To implement \(\texttt{LinOpt}(\theta)\), we appeal to \(\texttt{PSDP}\) (Algorithm 3). \(\texttt{PSDP}\), given an arbitrary reward function \(r_{1:h}:\mathcal{X}\times\mathcal{A}\rightarrow\mathbb{R}\) and a function class \(\mathcal{G}\subseteq\{g:\mathcal{X}\times\mathcal{A}\rightarrow\mathbb{R}\}\) capable of realizing all possible value functions induced by these rewards, can use the policy covers \(\Psi^{(1:h)}\) to efficiently compute a policy \(\hat{\pi}=\texttt{PSDP}(h,r_{1:h},\mathcal{G},\Psi^{(1:h)},n)\) that approximately solves \(\arg\max_{\pi\in\Pi_{h}}\mathbb{E}^{\pi}[\sum_{t=1}^{h}r_{t}(\mathbf{x}_{t},\mathbf{a }_{t})]\), and does so using polynomially many episodes; see Appendix D for details and formal guarantees.8 Thus, implementing \(\texttt{LinOpt}(\theta)\) is as simple as invoking \(\texttt{PSDP}\) with the rewards

Footnote 8: This is the main place where the analysis uses the inductive hypothesis that \(\Psi^{(1:h)}\) are policy covers.

\[r_{t}(x,a;\theta)\coloneqq\left\{\begin{array}{ll}\hat{\phi}^{(h)}(x,a)^{ \top}\theta,&\text{for }t=h,\\ 0,&\text{otherwise.}\end{array}\right.\]

With this, we have all the ingredients needed for spanner computation, and the algorithm is complete.

### Main Guarantee for SpanRL

The following result is the main sample complexity guarantee for \(\texttt{SpanRL}\) (Algorithm 1).

**Theorem 3.2** (Main theorem for \(\texttt{SpanRL}\)).: _Let \(\delta\in(0,1)\) be given, and suppose that realizability holds (Assumption 2.2) and that reachability (Assumption 2.1) is satisfied with parameter \(\eta>0\). If \(\varepsilon=\frac{\eta}{36d^{6/2}}\) and \(\mathfrak{c}=\operatorname{polylog}(A,H,d,\log(|\Phi|/\delta))\) is sufficiently large, then the policies \(\Psi^{(1:H)}\) produced by \(\texttt{SpanRL}\left(\Phi,\varepsilon,\mathfrak{c},\delta\right)\) are a \(\left(\frac{1}{4Ad},0\right)\)-policy cover with probability at least \(1-\delta\). The total number of episodes used by \(\texttt{SpanRL}\) is at most:_

\[\widetilde{\mathcal{O}}\left(A^{4}d^{9}H^{4}(d+\log(|\Phi|/\delta))\cdot 1/ \eta^{2}\right).\]

Theorem 3.2 is the first provable, model-free sample complexity guarantee for general Low-Rank MDPs that is attained by an efficient algorithm. Prior to our work, all efficient model-free algorithms required non-negative features (latent variable structure) or stronger assumptions [35, 49], even in the presence of similar reachability assumptions; see Table 1.

**Remark 3.2** (On the reachability assumption).: _While the reachability assumption is shared by the best prior efficient algorithms [35], which require non-negativity in addition to this assumption, it is natural to ask to what extent reachability restricts the generality of the Low-Rank MDP model. In Appendix H, we show that even when reachability holds, the embedding dimension in our model can be exponentially smaller than the best embedding dimension for the best non-negative (latent variable) embedding [35]. Hence, our results are meaningfully more general than prior work._

While our guarantee is polynomial in all relevant problem parameters, improving the dependence further (e.g., to match that of the best known inefficient algorithms) is an interesting direction for future research, as is removing the reachability assumption.

Application to reward-based RL.By using the policy cover produced by \(\texttt{SpanRL}\) within \(\texttt{PSDP}\) (Algorithm 3), we can optimize any downstream reward function to error \(\varepsilon\) using \(\operatorname{poly}(d,A,H,\log|\Phi|)\cdot 1/\varepsilon^{2}\) episodes. See Appendix G for details.

Efficiency and practicality.We observe that \(\texttt{SpanRL}\) is simple and practical. Defining \(\mathcal{L}_{\mathcal{D}}(\phi,w,f)\coloneqq\sum_{(x,a,x^{\prime})\in \mathcal{D}}(\phi(x,a)^{\top}w-f(x^{\prime}))^{2}+\lambda\|w\|^{2}\), where \(\mathcal{D}\) is a dataset consisting of \((\mathbf{x}_{h},\mathbf{a}_{h},\mathbf{r}_{h},\mathbf{x}_{h+1})\) tuples, the algorithm is provably efficient whenever the adversarial objective

\[f^{(t)}\in\operatorname*{arg\,max}_{f\in\mathcal{F}}\max_{\phi\in\Phi}\left\{ \min_{w}\mathcal{L}_{\mathcal{D}}(\phi^{(t)},w,f)-\min_{\tilde{w}}\mathcal{L }_{\mathcal{D}}(\tilde{\phi},\tilde{w},f)\right\}, \tag{7}\]

in Line 9 of \(\texttt{Replen}\) (Algorithm 5), can be implemented efficiently (note that by the definition of \(\mathcal{L}_{\mathcal{D}}\), the "inner" minima over \(w\) and \(\tilde{w}\) in (7) can be solved in closed form). This objective was also assumed to be efficiently solvable in Modi et al. [35], Zhang et al. [49] and was empirically shown to be practical in [49]; note that the objective is amenable to standard gradient-based optimization techniques, and that \(\mathcal{F}\) can be over-parameterized. While a detailed experimental evaluation is outside of the scope of this paper, we are optimistic about the empirical performance of the algorithm in light of the encouraging results based on the same objective in Zhang et al. [49]Outside of representation learning, the only overhead in SpanRL is the RobustSpanner subroutine, which has polynomial runtime. Indeed, RobustSpanner requires only polynomially many calls to the linear optimization oracle, instantiated as PSDP, which is efficient whenever standard least-squares regression problems based on the class \(\Phi\) can be solved efficiently, analogous to [33, 31].

Analysis and proof techniques.The proof of Theorem 3.2, which is given in Appendix F, is appealing in its simplicity and modularity. The crux of the proof is to show that the representation learning guarantee in (6) is strong enough to ensure that the downstream spanner computation in RobustSpanner succeeds. It is straightforward to show that spanner computation would succeed if we had access to an estimated representation that \(\hat{\phi}^{(h)}\) that approximates \(\phi^{*}_{h}\) point-wise (i.e., uniformly for all \((x,a)\) pairs), but the key challenge is that the guarantee in (6) only holds _on average_ under the roll-in distribution \(\texttt{unif}(\Psi^{(h)})\). Prior works that make use of the same representation learning objective (BRIEE [49] and MOFFLE [35]) do not make use of spanners; instead, they appeal to exploration strategies based on elliptic bonuses, addressing the issue of approximation errors through additional assumptions (non-negativity of the factorization for MOFFLE, and Block MDP structure for BRIEE). Perhaps the most important observation in our proof is that barycentric spanners are robust to the average-case approximation error guarantee in (6) as-is, without additional structural assumptions. Intuitively, this benefit seems to arise from the fact that the spanner property only concerns the _first moment_ of the feature map \(\phi^{*}\), while algorithms based on elliptic bonuses require approximation guarantees for the _second moment_; understanding this issue more deeply is an interesting question for future work. Another useful feature of our proof is to show that the notion of reachability in Assumption 2.1, which generalizes and extends all previous reachability conditions in the Low-Rank MDP and Block MDP literature [46, 5, 13, 33, 1, 35, 31], is sufficient to build a policy cover. We anticipate that this observation will find broader use.

## Acknowledgments and Disclosure of Funding

We thank Noah Golowich, Dhruv Rohatgi, and Ayush Sekhari for several helpful discussions. ZM and AR acknowledge support from the ONR through awards N00014-20-1-2336 and N00014-20-1-2394, and ARO through award W911NF-21-1-0328. AB acknowledges support from the National Science Foundation Graduate Research Fellowship under Grant No. 1122374.

## References

* Agarwal et al. [2020] A. Agarwal, S. Kakade, A. Krishnamurthy, and W. Sun. FLAMBE: Structural complexity and representation learning of low rank MDPs. _arXiv preprint arXiv:2006.10814_, 2020.
* Agarwal et al. [2020] A. Agarwal, S. Kakade, and L. F. Yang. Model-based reinforcement learning with a generative model is minimax optimal. In _Conference on Learning Theory_, pages 67-83. PMLR, 2020.
* Agarwal et al. [2020] A. Agarwal, S. M. Kakade, A. Krishnamurthy, and W. Sun. FLAMBE: structural complexity and representation learning of low rank mdps. In _Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual_, 2020.
* Agarwal et al. [2022] A. Agarwal, Y. Jin, and T. Zhang. Vo \(q\) l: Towards optimal regret in model-free rl with nonlinear function approximation. _arXiv preprint arXiv:2212.06069_, 2022.
* Agarwal et al. [2022] A. Agarwal, Y. Song, W. Sun, K. Wang, M. Wang, and X. Zhang. Provable benefits of representational transfer in reinforcement learning. _CoRR_, abs/2205.14571, 2022.
* Awerbuch and Kleinberg [2008] B. Awerbuch and R. Kleinberg. Online linear optimization and adaptive routing. _Journal of Computer and System Sciences_, 74(1):97-114, 2008.
* Azar et al. [2017] M. G. Azar, I. Osband, and R. Munos. Minimax regret bounds for reinforcement learning. In _International Conference on Machine Learning_, pages 263-272, 2017.
* Bagnell et al. [2003] J. Bagnell, S. M. Kakade, J. Schneider, and A. Ng. Policy search by dynamic programming. _Advances in neural information processing systems_, 16, 2003.
* Chen et al. [2022] F. Chen, Y. Bai, and S. Mei. Partially observable rl with b-stability: Unified structural condition and sharp sample-efficient algorithms. _arXiv preprint arXiv:2209.14990_, 2022.
* Cheng et al. [2023] Y. Cheng, R. Huang, Y. Liang, and J. Yang. Improved sample complexity for reward-free reinforcement learning under low-rank mdps. In _The Eleventh International Conference on Learning Representations_, 2023.
* Dann et al. [2018] C. Dann, N. Jiang, A. Krishnamurthy, A. Agarwal, J. Langford, and R. E. Schapire. On oracle-efficient PAC RL with rich observations. In _Advances in neural information processing systems_, pages 1422-1432, 2018.
* Du et al. [2019] S. S. Du, A. Krishnamurthy, N. Jiang, A. Agarwal, M. Dudik, and J. Langford. Provably efficient RL with rich observations via latent state decoding. _arXiv preprint arXiv:1901.09018_, 2019.
* Du et al. [2019] S. S. Du, Y. Luo, R. Wang, and H. Zhang. Provably efficient Q-learning with function approximation via distribution shift error checking oracle. In _Advances in Neural Information Processing Systems_, pages 8060-8070, 2019.
* Du et al. [2021] S. S. Du, S. M. Kakade, J. D. Lee, S. Lovett, G. Mahajan, W. Sun, and R. Wang. Bilinear classes: A structural framework for provable generalization in RL. _arXiv preprint arXiv:2103.10897_, 2021.
* Efroni et al. [2021] Y. Efroni, D. Misra, A. Krishnamurthy, A. Agarwal, and J. Langford. Provably filtering exogenous distractors using multistep inverse dynamics. In _International Conference on Learning Representations_, 2021.
* Ernst et al. [2005] D. Ernst, P. Geurts, and L. Wehenkel. Tree-based batch mode reinforcement learning. _Journal of Machine Learning Research_, 6, 2005.
* Foster et al. [2021] D. J. Foster, S. M. Kakade, J. Qian, and A. Rakhlin. The statistical complexity of interactive decision making. _arXiv preprint arXiv:2112.13487_, 2021.
* Foster et al. [2023] D. J. Foster, N. Golowich, and Y. Han. Tight guarantees for interactive decision making with the decision-estimation coefficient. _Conference on Learning Theory (COLT)_, 2023.

* [19] N. Golowich, A. Moitra, and D. Rohatgi. Learning in observable pomdps, without computationally intractable oracles. In _Advances in Neural Information Processing Systems_, 2022.
* [20] A. Huang, J. Chen, and N. Jiang. Reinforcement learning in low-rank mdps with density features. _International Conference on Machine Learning (ICML)_, 2023.
* [21] N. Jiang, A. Krishnamurthy, A. Agarwal, J. Langford, and R. E. Schapire. Contextual decision processes with low Bellman rank are PAC-learnable. In _International Conference on Machine Learning_, pages 1704-1713, 2017.
* [22] C. Jin, P. Netrapalli, R. Ge, S. M. Kakade, and M. I. Jordan. A short note on concentration inequalities for random vectors with subgaussian norm. _arXiv preprint arXiv:1902.03736_, 2019.
* [23] C. Jin, Z. Yang, Z. Wang, and M. I. Jordan. Provably efficient reinforcement learning with linear function approximation. In _Conference on Learning Theory_, pages 2137-2143, 2020.
* [24] C. Jin, Q. Liu, and S. Miryoosefi. Bellman eluder dimension: New rich classes of RL problems, and sample-efficient algorithms. _arXiv preprint arXiv:2102.00815_, 2021.
* [25] S. M. Kakade. _On the sample complexity of reinforcement learning_. University of London, University College London (United Kingdom), 2003.
* [26] J. Kober, J. A. Bagnell, and J. Peters. Reinforcement learning in robotics: A survey. _The International Journal of Robotics Research_, 32(11):1238-1274, 2013.
* [27] J. Li, W. Monroe, A. Ritter, D. Jurafsky, M. Galley, and J. Gao. Deep reinforcement learning for dialogue generation. In _EMNLP_, 2016.
* [28] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous control with deep reinforcement learning. _arXiv preprint arXiv:1509.02971_, 2015.
* [29] Z. Mhammedi, D. J. Foster, M. Simchowitz, D. Misra, W. Sun, A. Krishnamurthy, A. Rakhlin, and J. Langford. Learning the linear quadratic regulator from nonlinear observations. _Advances in Neural Information Processing Systems_, 33:14532-14543, 2020.
* [30] Z. Mhammedi, A. Block, D. J. Foster, and A. Rakhlin. Efficient model-free exploration in low-rank mdps. _arXiv preprint arXiv:2307.03997_, 2023.
* [31] Z. Mhammedi, D. J. Foster, and A. Rakhlin. Representation learning with multi-step inverse kinematics: An efficient and optimal approach to rich-observation rl. _International Conference on Machine Learning (ICML)_, 2023.
* [32] D. Misra, M. Henaff, A. Krishnamurthy, and J. Langford. Kinematic state abstraction and provably efficient rich-observation reinforcement learning. _arXiv preprint arXiv:1911.05815_, 2019.
* [33] D. Misra, M. Henaff, A. Krishnamurthy, and J. Langford. Kinematic state abstraction and provably efficient rich-observation reinforcement learning. In _International conference on machine learning_, pages 6961-6971. PMLR, 2020.
* [34] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, et al. Human-level control through deep reinforcement learning. _Nature_, 518(7540):529, 2015.
* [35] A. Modi, J. Chen, A. Krishnamurthy, N. Jiang, and A. Agarwal. Model-free representation learning and exploration in low-rank mdps. _CoRR_, abs/2102.07035, 2021.
* [36] S. Rendle, C. Freudenthaler, and L. Schmidt-Thieme. Factorizing personalized markov chains for next-basket recommendation. In _Proceedings of the 19th International Conference on World Wide Web, WWW 2010, Raleigh, North Carolina, USA, April 26-30, 2010_, pages 811-820. ACM, 2010.
* [37] D. Russo and B. Van Roy. Eluder dimension and the sample complexity of optimistic exploration. In _Advances in Neural Information Processing Systems_, pages 2256-2264, 2013.
* [38] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. _nature_, 529(7587):484, 2016.
* [39] W. Sun, N. Jiang, A. Krishnamurthy, A. Agarwal, and J. Langford. Model-based RL in contextual decision processes: PAC bounds and exponential improvements over model-free approaches. In _Conference on learning theory_, pages 2898-2933. PMLR, 2019.

* [40] M. Uehara, X. Zhang, and W. Sun. Representation learning for online and offline RL in low-rank mdps. In _The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022_, 2022.
* [41] R. Wang, S. S. Du, L. Yang, and R. R. Salakhutdinov. On reward-free reinforcement learning with linear function approximation. _Advances in neural information processing systems_, 33:17816-17826, 2020.
* [42] R. Wang, R. Salakhutdinov, and L. F. Yang. Provably efficient reinforcement learning with general value function approximation. _arXiv preprint arXiv:2005.10804_, 2020.
* [43] T. Xie, D. J. Foster, Y. Bai, N. Jiang, and S. M. Kakade. The role of coverage in online reinforcement learning. _arXiv preprint arXiv:2210.04157_, 2022.
* [44] L. Yang and M. Wang. Sample-optimal parametric q-learning using linearly additive features. In _Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA_, volume 97 of _Proceedings of Machine Learning Research_, pages 6995-7004. PMLR, 2019.
* [45] H. Yao, C. Szepesvari, B. A. Pires, and X. Zhang. Pseudo-mdps and factored linear action models. In _2014 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning, ADPRL 2014, Orlando, FL, USA, December 9-12, 2014_, pages 1-9. IEEE, 2014.
* [46] A. Zanette, A. Lazaric, M. J. Kochenderfer, and E. Brunskill. Provably efficient reward-agnostic navigation with linear value iteration. _Advances in Neural Information Processing Systems_, 33:11756-11766, 2020.
* [47] T. Zhang. Feel-good thompson sampling for contextual bandits and reinforcement learning. _arXiv preprint arXiv:2110.00871_, 2021.
* [48] T. Zhang, T. Ren, M. Yang, J. Gonzalez, D. Schuurmans, and B. Dai. Making linear mdps practical via contrastive representation learning. In _International Conference on Machine Learning_, pages 26447-26466. PMLR, 2022.
* [49] X. Zhang, Y. Song, M. Uehara, M. Wang, A. Agarwal, and W. Sun. Efficient reinforcement learning in block mdps: A model-free representation learning approach. In _International Conference on Machine Learning_, pages 26517-26547. PMLR, 2022.

###### Contents of Appendix

* A Additional Related Work
* B Omitted Algorithms
* C Generic Guarantee for RobustSpanner
* D Generic Guarantee for PSDP
* E Guarantee for RepLearn
* F Analysis
* F.1 Proof Strategy
* F.2 Guarantee for PSDP as a Subroutine for RobustSpanner
* F.3 Guarantee for RobustSpanner as a Subroutine for SpanRL
* F.4 Guarantee for RepLearn as a Subroutine for SpanRL
* F.5 Concluding the Proof of Theorem 3.2
* F.6 Proof of Lemma 3.1
* G Application to Reward-Based RL
* H Properties of Reachability Assumption
* H.1 Comparison to Latent Variable Model
* H.2 Relation to Other Reachability Assumptions

## Appendix A Additional Related Work

In this section, we discuss relevant related work not already covered.

Block MDPs.A particularly well-studied special case low-rank MDPs with the latent variable assumed in Modi et al. [35] (defined in Definition H.1) is the _Block MDP (BMDP) model_ Du et al. [13], Misra et al. [32], Zhang et al. [49], Mhammedi et al. [31]. For this setting, Du et al. [13], Misra et al. [32] provide algorithms that conduct exploration in a provably oracle-efficient manner under a reachability assumption. This reachability assumption was removed by subsequent work of Zhang et al. [49] (with a suboptimal rate) and Mhammedi et al. [31] (with optimal error dependence), but the analysis in these works is tailored to the BMDP model.

Barycentric spanners.Huang et al. [20] consider a variant of the Low-Rank MDP framework in which we are given a class \(\Upsilon\) that realizes the next-state feature map \(\mu^{\star}\), but do not have access to a class \(\Phi\) for the feature map \(\phi^{\star}\), which is unknown. Their algorithm, like SpanRL, is based on barycentric spanners, though the algorithm design considerations and analysis are significantly different. Notably, their algorithm is not computationally efficient, and their analysis takes advantage of the fact that realizability of \(\mu^{\star}\) facilitates estimation of the occupancies \(\{d^{\pi}(\cdot)\}_{\pi\in\Pi_{\pi}}\) in \(\ell_{1}\)-error. Barycentric spanners were also in the work of Golowich et al. [19] for reinforcement learning in Partially Observable MDPs (POMDPs). Their analysis is substantially different from ours, and their algorithm appeals to the barycentric spanner computation approach in Awerbuch and Kleinberg [6] in an off-the-shelf fashion.

[MISSING_PAGE_FAIL:15]

[MISSING_PAGE_FAIL:16]

[MISSING_PAGE_FAIL:17]

Generic Guarantee for RobustSpanner

In this section, we give a generic guarantee for the RobustSpanner algorithm when invoked with oracles \(\mathtt{LinOpt}\) and \(\mathtt{LinEst}\) satisfying the following assumption.

**Assumption C.1** (\(\mathtt{LinOpt}\) and \(\mathtt{LinEst}\) as approximate Linear Optimization Oracles).: _For some abstract set \(\mathcal{Z}\) and a collection of vectors \(\{w^{z}\in\mathbb{R}^{d}\mid z\in\mathcal{Z}\}\) indexed by elements in \(\mathcal{Z}\), there exists \(\varepsilon^{\prime}>0\) such that for any \(\theta\in\mathbb{R}^{d}\setminus\{0\}\) and \(z\in\mathcal{Z}\), the outputs \(\hat{z}_{\theta}\coloneqq\mathtt{LinOpt}(\theta/\|\theta\|)\) and \(\hat{w}_{z}\coloneqq\mathtt{LinEst}(z)\) satisfy_

\[\sup_{z\in\mathcal{Z}}\theta^{\intercal}w^{z}\leq\theta^{\intercal}w^{\hat{z}_{ \theta}}+\varepsilon^{\prime}\cdot\|\theta\|,\quad\text{and}\quad\|\hat{w}_{z} -w^{z}\|\leq\varepsilon^{\prime}.\]

Letting \(\mathcal{W}\coloneqq\{w^{z}\mid z\in\mathcal{Z}\}\) and assuming that \(\mathcal{W}\subseteq\mathcal{B}(1)\), the next theorem bounds the number of iterations of \(\mathtt{RobustSpanner}(\mathtt{LinOpt}(\cdot),\mathtt{LinEst}(\cdot),\cdot,\cdot)\) under Assumption C.1, and shows that the output is an approximate barycentric spanner for \(\mathcal{W}\) (Definition 3.1). Our result extends those of Awerbuch and Kleinberg [6], in that it only requires an _approximate_ linear optimization oracle, which is potentially of independent interest.

**Proposition C.1**.: _Fix \(C>1\) and \(\varepsilon\in(0,1)\) and suppose that \(\{w^{z}\mid z\in\mathcal{Z}\}\subseteq\mathcal{B}(1)\). If \(\mathtt{RobustSpanner}\) (Algorithm 2) is ran with parameters \(C,\varepsilon>0\) and oracles \(\mathtt{LinOpt}\), \(\mathtt{LinEst}\) satisfying Assumption C.1 with \(\varepsilon^{\prime}=\varepsilon/2\), then it terminates after \(d+\lceil\frac{d}{2}\log_{C}\frac{100d}{\varepsilon^{2}}\rceil\) iterations, and requires at most twice that many calls to each of \(\mathtt{LinOpt}\) and \(\mathtt{LinEst}\). Furthermore, the output \(z_{1:d}\) has the property that for all \(z\in\mathcal{Z}\), there exist \(\beta_{1},\ldots,\beta_{d}\in[-C,C]\), such that_

\[\left\|w^{z}-\sum_{i=1}^{d}\beta_{i}w^{z_{i}}\right\|\leq\frac{3Cd\cdot \varepsilon}{2}.\]

Proof of Proposition c.1.: The proof will follows similar steps to those in Awerbuch and Kleinberg [6, Lemma 2.6], with modifications to account for the fact that linear optimization over the set \(\mathcal{W}\coloneqq\{w^{z}\mid z\in\mathcal{Z}\}\) is only performed approximately.

Part I: Bounding the number of iterations.In Algorithm 2, there are two loops, both of which require two calls to \(\mathtt{LinOpt}\) and \(\mathtt{LinEst}\) per iteration. As the first loop has exactly \(d\) iterations, it suffices to bound the number of iterations in the second loop.

Let \(M^{(i)}\coloneqq(w_{1},\ldots,w_{i},e_{i+1},\ldots,e_{d})\) be the matrix whose columns are the vectors at end of the \(i\)th iteration of the first loop (Line 2) of Algorithm 2; note that columns \(i+1\) through \(d\) are unchanged at this point in the algorithm. For \(i\in[d]\), we define \(\ell_{i}(w)\coloneqq\det(w,M^{(i)}_{-i})\) and \(\theta_{i}\coloneqq\big{(}\det\big{(}e_{j},M^{(i)}_{-i}\big{)}\big{)}_{j\in[d ]}\in\mathbb{R}^{d}\), where we recall that for any matrix \(A\), the matrix \(A_{-i}\) is defined as the result of removing the \(i\)th column from \(A\). Note that \(\ell_{i}\) is linear in \(w\), and in particular

\[\ell_{i}(w)\coloneqq w^{\top}\theta_{i}.\]

Let \(W^{(0)}\coloneqq M^{(d)}=(w_{1},\ldots,w_{d})\), and let \(W^{(j)}\) denote the resulting matrix after \(j\) iterations of the second loop (Line 10) of Algorithm 2. We will show that for any \(J\geq 1\),

\[\det(W^{(J)})\leq\det(W^{(0)})\cdot\left(\frac{100d}{\varepsilon^{2}}\right)^ {\frac{d}{2}}. \tag{9}\]

By construction of the loop, we have \(\det(W^{(j)})\geq C\cdot\det(W^{(j-1)})\) for each \(j\in[J]\), and thus \(\det(W^{(J)})\geq\det(W^{(0)})\cdot C^{J}\). Combining these two facts will establish the bound on the iteration complexity. We now prove (9).

Let \(u_{i}=e_{i}^{\top}\big{(}M^{(i)}\big{)}^{-1}\) (note that \(u_{i}\) is a _row vector_) and let \(U\) denote the matrix whose \(i\)th row is \(u_{i}\). We observe that for all \(w\in\mathbb{R}^{d}\),

\[u_{i}w=\frac{\ell_{i}(w)}{\ell_{i}(w_{i})},\]where we note that \(\ell_{i}(w_{i})\neq 0\) by construction; indeed, the columns of \(M^{(i)}\) are a basis for \(\mathbb{R}^{d}\) because \(\det(M^{(i)})\neq 0\), and the equality holds on the columns, so the two linear functions must be equal. Now, since Assumption C.1 holds with \(\varepsilon^{\prime}=\varepsilon/2\), we have

\[\theta_{i}^{\top}w_{i}^{+}\geq\sup_{z\in\mathcal{Z}}\theta_{i}^{ \top}w^{z}-\frac{\varepsilon}{2}\|\theta_{i}\|,\quad\text{and}\quad\theta_{i}^ {\top}w_{i}^{-}\leq\inf_{z\in\mathcal{Z}}\theta_{i}^{\top}w^{z}+\frac{ \varepsilon}{2}\|\theta_{i}\|, \tag{10}\]

where \(w_{i}^{\pm}=\mathsf{LinOpt}(z_{i}^{\pm})\). We will now show that

\[\ell_{i}(w_{i})\geq\frac{\varepsilon}{2}\cdot\|\theta_{i}\|. \tag{11}\]

There are two cases. First, suppose that \(\theta_{i}^{\top}w_{i}^{+}\geq-\theta_{i}^{\top}w_{i}^{-}\), corresponding to the conditional in Line 6 of Algorithm 2 being satisfied. Combining this with (10), we have

\[\theta_{i}^{\top}w_{i}^{+} \geq\left(\sup_{z\in\mathcal{Z}}\theta_{i}^{\top}w^{z}-\frac{ \varepsilon}{2}\|\theta_{i}\|\right)\vee\left(-\theta_{i}^{\top}w_{i}^{-} \right),\] \[\geq\left(\sup_{z\in\mathcal{Z}}\theta_{i}^{\top}w^{z}-\frac{ \varepsilon}{2}\|\theta_{i}\|\right)\vee\left(\sup_{z\in\mathcal{Z}}-\theta_{i} ^{\top}w^{z}-\frac{\varepsilon}{2}\|\theta_{i}\|\right),\quad\text{(by \eqref{eq:10})}\] \[=\left(\sup_{z\in\mathcal{Z}}\theta_{i}^{\top}w^{z}\right)\vee \left(\sup_{z\in\mathcal{Z}}-\theta_{i}^{\top}w^{z}\right)-\frac{\varepsilon}{ 2}\|\theta_{i}\|,\] \[\geq-\frac{\varepsilon}{2}\|\theta_{i}\|. \tag{12}\]

Because the conditional is satisfied, \(w_{i}=w_{i}^{+}+\varepsilon\cdot\frac{\theta_{i}}{\|\theta_{i}\|}\), and so by plugging this into (12), we have

\[\ell_{i}(w_{i})=\theta_{i}^{\top}w_{i}\geq\frac{\varepsilon}{2} \cdot\|\theta_{i}\|.\]

The case that \(\theta_{i}^{\top}w_{i}^{+}\leq-\theta_{i}^{\top}w_{i}^{-}\) is essentially identical, establishing (11). Now, recall that \(\mathcal{W}\coloneqq\{w^{z}\mid z\in\mathcal{Z}\}\) and let \(\mathcal{W}\oplus\mathcal{B}\left(\frac{3\varepsilon}{2}\right)\coloneqq \left\{w+b\mid w\in\mathcal{W}\right\}\) and \(b\in\mathcal{B}\left(\frac{3\varepsilon}{2}\right)\) denote the Minkowski sum with \(\mathcal{B}\left(\frac{3\varepsilon}{2}\right)\). By Cauchy-Schwarz, it holds that for all \(w^{\prime}=w+b\in\mathcal{W}\oplus\mathcal{B}\left(\frac{3\varepsilon}{2}\right)\),

\[\ell_{i}(w^{\prime})=\theta_{i}^{\top}w^{\prime}=\theta_{i}^{ \top}w+\theta_{i}^{\top}b\leq\left(1+\frac{3\varepsilon}{2}\right)\cdot\| \theta_{i}\|,\]

where we used that \(\mathcal{W}\subseteq\mathcal{B}(1)\) (by assumption). Thus, for any \(w^{\prime}\in\mathcal{W}\oplus\mathcal{B}\left(\frac{3\varepsilon}{2}\right)\), we have

\[|u_{i}w^{\prime}|=\frac{\ell_{i}(w^{\prime})}{\ell_{i}(w_{i})} \leq 1+\frac{3\varepsilon}{2}.\]

We now observe that by construction and the fact that Assumption C.1 holds with \(\varepsilon^{\prime}=\varepsilon/2\), the \(k\)th column \(w_{k}^{\prime}\) of \(W^{(J)}\) belongs to \(\mathcal{W}\oplus\mathcal{B}\left(\frac{3\varepsilon}{2}\right)\), for any \(k\in[d]\). Thus, the \((i,k)\) entry \(u_{i}w_{k}^{\prime}\) of \(UW^{(J)}\) satisfies \(u_{i}w_{k}^{\prime}\in\left[-1-\frac{3\varepsilon}{2},1+\frac{3\varepsilon}{2}\right]\), and so the columns of \(UW^{(J)}\) have Euclidean norm at most \(\frac{10\sqrt{d}}{\varepsilon}\). Since the magnitude of the determinant of a matrix is upper bounded by the product of the Euclidean norms of its columns, it holds that \(|\det(UW^{(J)})|\leq\left(\frac{100d}{\varepsilon^{2}}\right)^{\frac{d}{2}}\).

On the other hand, again by construction, we see that the columns \(w_{1},\ldots,w_{d}\) of \(W^{(0)}\) satisfy \(u_{i}w_{j}=0\), for \(j<i\), and \(u_{i}w_{i}=1\). Thus, \(UW^{(0)}\) is an upper-triangular matrix with \(1\)s on the diagonal, and hence has determinant \(1\). Because determinants are multiplicative, this implies that \(\det(U)\neq 0\). We now compute:

\[|\det(W^{(J)})|=\frac{|\det(UW^{(J)})|}{|\det(U)|}=\frac{|\det(UW^{(J)})|}{| \det(UW^{(0)})|}\leq\left(\frac{100d}{\varepsilon^{2}}\right)^{\frac{d}{2}}.\]

Thus, the upper bound on \(|\det(W^{(J)})|\) holds and the claim is proven. Therefore, we have

\[C^{J}\leq\left(\frac{100d}{\varepsilon^{2}}\right)^{\frac{d}{2}},\]

and so \(JPart II: Spanner property for the output.Having shown that the algorithm terminates, we now show that the result is an approximate barycentric spanner for \(\mathcal{W}\). Let \(W\coloneqq(w_{1},\ldots,w_{d})\) be the matrix at termination of the algorithm. By definition, if the second loop (Line 10) has terminated, then for all \(i\in[d]\),

\[\max(\theta_{i}^{\top}w_{i}^{*},-\theta_{i}^{\top}w_{i}^{-})+ \varepsilon\cdot\|\theta_{i}\|\leq C\cdot|\det(w_{i},W_{-i})|,\]

where \(\theta_{i}=(\det(e_{j},W_{-i}))_{j\in[d]}\in\mathbb{R}^{d}\). On the other hand, by Assumption C.1, (10) holds, and so

\[\forall z\in\mathcal{Z},\forall i\in[d],\quad|\det(w^{z},W_{-i})|= |\theta_{i}^{\top}w^{z}| \leq\max(\theta_{i}^{\top}w_{i}^{*},-\theta_{i}^{\top}w_{i}^{-})+ \varepsilon\cdot\|\theta_{i}\|,\] \[\leq C\cdot|\det(w_{i},W_{-i})|. \tag{13}\]

Now, fix \(z\in\mathcal{Z}\). Since \(\det(W)\neq 0\), there exist \(\beta_{1:d}\in\mathbb{R}\) such that \(w^{z}=\sum_{i=1}^{d}\beta_{i}w_{i}\). By plugging this into (13) and using the linearity of the determinant, we have

\[\forall i\in[d],\quad C\cdot|\det(w_{i},W_{-i})|\geq|\det(w^{z},W_ {-i})|=\left|\sum_{j=1}^{d}\beta_{i}\det(w_{j},W_{-i})\right|=|\beta_{i}|\cdot |\det(w_{i},W_{-i})|.\]

Therefore, \(|\beta_{i}|\leq C\), for all \(i\in[d]\). Now, by definition of \(w_{1:d}\) and \(\widetilde{w}_{1:d}\), for all \(i\in[d]\), we have that \(\|w_{i}-\widetilde{w}_{i}\|\leq\varepsilon\). Furthermore, by Assumption C.1, we also have that \(\|\widetilde{w}_{i}-w^{z_{i}}\|\leq\varepsilon/2\). Therefore, by the triangle inequality, we have

\[\left\|w^{z}-\sum_{i=1}^{d}\beta_{i}w^{z_{i}}\right\|\leq\left\|w ^{z}-\sum_{i=1}^{d}\beta_{i}w_{i}\right\|+\sum_{i=1}^{d}|\beta_{i}|\| \widetilde{w}_{i}-w^{z_{i}}|+\sum_{i=1}^{d}|\beta_{i}|\|\widetilde{w}_{i}-w_{i }\|\leq 3dC\varepsilon/2.\]

This completes the proof. 

## Appendix D Generic Guarantee for PSDP

In this section, we present a generic guarantee for PSDP (Algorithm 3). We show that given any reward functions \(r_{1:h}:\mathcal{X}\times\mathcal{A}\rightarrow\mathbb{R}\) and a function class \(\mathcal{G}\subseteq\{g:\mathcal{X}\times\mathcal{A}\rightarrow\mathbb{R}\}\) that "realizes" these reward functions (we formalize this in the next definition), if \(\Psi^{(1:h)}\) are \(\alpha\)-policy covers for layers 1 through \(h\), then for sufficiently large \(n\geq 1\) and with high probability, the output \(\hat{\pi}=\mathsf{PSDP}(h,r_{1:h},\mathcal{G},\Psi^{(1:h)},n)\) is an approximate maximizer of the objective

\[\max_{\pi\in\Pi_{\mathsf{R}}}\mathbb{E}^{\pi}\left[\sum_{t=1}^{h}r_{t}( \boldsymbol{x}_{t},\boldsymbol{a}_{t})\right].\]

To formalize this result, we define the notion of realizability we require for the function class \(\mathcal{G}\).

**Definition D.1**.: _We say that the function class \(\mathcal{G}\subseteq\{g:\mathcal{X}\times\mathcal{A}\rightarrow\mathbb{R}\}\) realizes the reward functions \(r_{1:h}:\mathcal{X}\times\mathcal{A}\rightarrow\mathbb{R}\) if for all \(t\in[h]\) and all \(\pi\in\Pi_{\mathsf{R}}^{t:h}\),_

\[Q_{t}^{\pi}\in\mathcal{G},\quad\text{where}\qquad Q_{t}^{\pi}(x,a)\coloneqq r _{t}(x,a)+\mathbb{E}^{\pi}\left[\sum_{\ell=t+1}^{h}r_{\ell}(\boldsymbol{x}_{ \ell},\boldsymbol{a}_{\ell})\ \bigg{|}\ \boldsymbol{x}_{t}=x,\boldsymbol{a}_{t}=a \right]. \tag{14}\]

Note that \(Q_{t}^{\pi}\) in (14) represents the _state-action value function_ (\(Q\)-function) at layer \(t\in[h]\) with respect to the rewards \(r_{1:h}\) and partial policy \(\pi\).

In what follows, given a function class \(\mathcal{G}\subseteq\{g:\mathcal{X}\times\mathcal{A}\rightarrow\mathbb{R}\}\), we use \(\mathcal{N}_{\mathcal{G}}(\varepsilon)\) to denote the \(\varepsilon\)-covering number of \(\mathcal{G}\) in \(\ell_{\infty}\) distance. With this, we now state a guarantee for PSDP.

**Theorem D.2**.: _Let \(\varepsilon,\delta\in(0,1)\), \(B>0\), and \(h\in[H]\). Suppose reward functions \(r_{1:h}:\mathcal{X}\times\mathcal{A}\rightarrow\mathbb{R}\), function class \(\mathcal{G}\), a collection of policies \(\Psi^{(1:h)}\), and a parameter \(n\geq 1\) satisfy the following:_

* _The function class_ \(\mathcal{G}\) _realizes the reward functions_ \(r_{1:h}\) _(in the sense of Definition_ D.1_), functions in_ \(\mathcal{G}\) _are uniformly bounded by_ \(B\)_, and_ \(\lim_{n\rightarrow\infty}n^{-1}\cdot\log\mathcal{N}_{\mathcal{G}}(1/n)=0\)_._
* _For some_ \(0<\alpha\leq 1\)_, for each_ \(1\leq t\leq h\)_, it holds that_ \(\Psi^{(t)}\) _is an_ \(\alpha\)_-policy cover for layer_ \(t\) _and moreover_ \(|\Psi^{(t)}|\leq d\)_._
* _The parameter_ \(n\) _is chosen such that_ \(cdH\alpha^{-1}\cdot\varepsilon_{\mathrm{stat}}(n,\delta/H)\leq\varepsilon\)_, where_ \(\varepsilon_{\mathrm{stat}}(n,\delta^{\prime})\coloneqq\sqrt{B^{2}n^{-1}\cdot( \log\mathcal{N}_{\mathcal{G}}(1/n)+\log(n/\delta))}\) _and_ \(c>0\) _is a large enough absolute constant.__Then, with probability at least \(1-\delta\), the policy \(\hat{\pi}=\mathsf{PSDP}(h,r_{1:h},\mathcal{G},\Psi^{(1:h)},n)\) coming from Algorithm 3, satisfies the following guarantee:_

\[\max_{\pi\in\Pi_{\mathsf{h}}}\mathbb{E}^{\pi}\left[\sum_{t=1}^{h}r_{t}(\mathbf{x}_{ t},\mathbf{a}_{t})\right]\leq\mathbb{E}^{\hat{\pi}}\left[\sum_{t=1}^{h}r_{t}(\mathbf{x}_{ t},\mathbf{a}_{t})\right]+\varepsilon.\]

To prove the theorem, we need two intermediate results. The first shows that the \(Q\) function is the Bayes-optimal predictor of the sum of rewards when rolling out with policy \(\pi\).

**Lemma D.1**.: _Let \(t\in[H]\), \(r_{1:h}:\mathcal{X}\times\mathcal{A}\to\mathbb{R}\) be reward functions, \(\pi\in\Pi_{\mathsf{h}}\), and let \(g^{\pi}_{\text{bayes}}\) denote the Bayes-optimal predictor9 for the sum of rewards under policy \(\pi\), i.e.,_

Footnote 9: Observe that because the loss is strongly convex, this predictor is unique up to sets of measure zero, justifying our usage of “the” Bayes optimal reward.

\[g^{\pi}_{\text{bayes}}\in\operatorname*{arg\,min}_{g:\mathcal{X}_{t}\times \mathcal{A}\to\mathbb{R}}\mathbb{E}^{\pi}\left[\left(g(\mathbf{x}_{t},\mathbf{a}_{t}) -\sum_{t=t}^{h}r_{\ell}(\mathbf{x}_{\ell},\mathbf{a}_{\ell})\right)^{2}\right], \tag{15}\]

_Then, \(g^{\pi}_{\text{bayes}}=Q^{\pi}_{t}\), where \(Q^{\pi}_{t}\) is the \(Q\)-function at layer \(t\in[h]\) with respect to the policy \(\pi\) and rewards \(r_{1:h}\) defined in (14)._

**Proof of Lemma D.1**.: The least-squares solution \(g^{\pi}_{\text{bayes}}\) of the problem in (15) satisfies, for all \(a\in\mathcal{A}\) and \(x\in\mathcal{X}_{t}\),

\[g^{\pi}_{\text{bayes}}(x,a) =\mathbb{E}^{\pi}\left[\left.\sum_{\ell=t}^{h}r_{\ell}(\mathbf{x}_{ \ell},\mathbf{a}_{\ell})\right|\mathbf{x}_{t}=x,\mathbf{a}_{t}=a\right],\] \[=\mathbb{E}[r_{t}(\mathbf{x}_{t},\mathbf{a}_{t})\mid\mathbf{x}_{t}=x,\mathbf{a}_{ t}=a]+\mathbb{E}^{\pi}\left[\left.\sum_{\ell=t+1}^{h}r_{\ell}(\mathbf{x}_{\ell},\mathbf{a}_ {\ell})\right|\mathbf{x}_{t}=x,\mathbf{a}_{t}=a\right],\] \[=r_{t}(x,a)+\mathbb{E}^{\pi}\left[\left.\sum_{\ell=t+1}^{h}r_{ \ell}(\mathbf{x}_{\ell},\mathbf{a}_{\ell})\right|\mathbf{x}_{t}=x,\mathbf{a}_{t}=a\right],\] \[=Q^{\pi}_{t}(s,a),\]

where the last step follows by the definition of the \(Q\)-function in (14). 

We now show that the solution \(\hat{g}^{(t)}\) of the least-squares problem in (6) of Algorithm 3 is close to the \(Q\)-function in the appropriate sense.

**Lemma D.2**.: _Let \(\varepsilon,\delta\in(0,1)\), \(B>0\), and \(1\leq t\leq h\leq H\). Further, let \((\varepsilon_{\mathrm{stat}},r_{1:h},\mathcal{G},\Psi^{(1:h)},n)\) be as in Theorem D.2. Then, the solution \(\hat{g}^{(t)}\) of the least-squares problem in (6) in Algorithm 3 when calling \(\mathsf{PSDP}(h,r_{1:h},\mathcal{G},\Psi^{(1:h)},n)\) satisfies with probability at least \(1-\delta\),_

\[\mathbb{E}^{\text{unif}(\Psi^{(t)})}\left[\max_{a\in\mathcal{A}}\left(\hat{g}^{ (t)}(\mathbf{x}_{t},a)-Q^{\hat{\pi}^{(t+1)}}_{t}(\mathbf{x}_{t},a)\right)^{2}\right] \leq c^{2}\cdot\varepsilon^{2}_{\mathrm{stat}}(n,\delta),\]

_where \(\hat{\pi}^{(t+1)}\in\Pi^{t+1:h}_{\mathsf{h}}\) is as in Algorithm 3, and \(c>0\) is an absolute constant._

**Proof of Lemma D.2**.: Fix \(t\in[h]\) and let \(g^{(t)}_{\text{bayes}}=g^{\pi}_{\text{bayes}}\) be as in Lemma D.1 with

\[\pi=\text{unif}(\Psi^{(t)})\circ_{t}\pi_{\text{unif}}\circ_{t+1}\hat{\pi}^{(t +1)},\]

and reward functions \(r_{1:h}\) as in the lemma's statement. By Lemma D.1, \(g^{(t)}_{\text{bayes}}\) is the Bayes-optimal solution of the least-squares problem in (6) of Algorithm 3. Thus, since \(\mathcal{G}\) realizes the reward functions \(r_{1:h}\), a standard uniform-convergence guarantee for least-square regression (see e.g. Mhammedi et al. [29, Proposition B.1] with \(\mathbf{e}=0\) almost surely) implies that there exists an absolute constant \(c>0\) (independent of \(t,h\), and any other problem parameters) such that with probability at least \(1-\delta\),

\[\mathbb{E}^{\text{unif}(\Psi^{(t)})}\left[\max_{a\in\mathcal{A}}\left(\hat{g}^{ (t)}(\mathbf{x}_{t},a)-g^{(t)}_{\text{bayes}}(\mathbf{x}_{t},a)\right)^{2}\right]\leq c ^{2}B^{2}\cdot\frac{\log\mathcal{N}_{\mathcal{G}}(1/n)+\log(n/\delta)}{n}.\]

The desired result follows by the fact that \(g^{(t)}_{\text{bayes}}\equiv Q^{\mathbb{Z}^{(t+1)}}_{t}\); see Lemma D.1. 

We also require the classical performance difference lemma from Kakade [25].

[MISSING_PAGE_EMPTY:22]

Thus, under the event \(\mathcal{E}\coloneqq\bigcup_{t=1}^{h}\mathcal{E}_{t}\), we have that

\[\mathbb{E}^{\pi_{*}}\left[\sum_{t=1}^{h}r_{t}(\mathbf{x}_{t},\mathbf{a}_{t})\right]- \mathbb{E}^{\pi}\left[\sum_{t=1}^{h}r_{t}(\mathbf{x}_{t},\mathbf{a}_{t})\right]\leq\varepsilon.\]

The desired result follows by the fact that a union bound implies \(\mathbb{P}[\mathcal{E}]\geq 1-\delta\). 

## Appendix E Guarantee for RepLearn

In this section, we give a generic guarantee for RepLearn (Algorithm 6). Compared to previous guarantees in Modi et al. [35], Zhang et al. [49], we prove a fast \(1/n\)-type rate of convergence for RepLearn, and show that the algorithm succeeds even when the norm of the weight \(w\) in Line 10 does not grow with the number of iterations. We also use the slightly simpler discriminator class:

\[\mathcal{F}\coloneqq\left\{f:x\mapsto\max_{a\in\mathcal{A}}\theta^{\top}\phi( x,a)\,\middle|\,\theta\in\mathcal{B}(1),\phi\in\Phi\right\}. \tag{20}\]

The main guarantee for RepLearn is as follows.

**Theorem E.1**.: _Let \(h\in[H]\), \(\delta\in(0,e^{-1})\), and \(n\in\mathbb{N}\) be given, and suppose that \(\mu^{*}_{h+1}\) satisfies the normalization assumption in Eq. (4). For any function \(f\in\mathcal{F}\), define_

\[w_{f}=\int_{\mathcal{X}_{h+1}}f(x)\mu^{*}_{h+1}(x)\mathrm{d}\nu(x).\]

_Let \(P\in\Delta(\Pi_{\mathfrak{N}})\) be a distribution over policies, \(\mathcal{F}\) be as (20), and \(\Phi\) be a feature class satisfying Assumption 2.2. With probability at least \(1-\delta\), \(\mathtt{RepLearn}\) with input \((h,\mathcal{F},\Phi,P,n)\) terminates after \(t\leq T\coloneqq\left\lceil d\log_{3/2}(2nd^{-1/2})\right\rceil\) iterations, and its output \(\phi^{(t)}\) satisfies_

\[\sup_{f\in\mathcal{F}}\inf_{w\in\mathcal{B}(3d^{9/2})}\mathbb{E}_{\pi\sim P} \mathbb{E}^{\pi_{0}\pi_{\mathrm{init}}}\left[\left(w^{\top}\phi^{(t)}(\mathbf{x}_ {h},\mathbf{a}_{h})-w^{\top}_{f}\phi^{*}_{h}(\mathbf{x}_{h},\mathbf{a}_{h})\right)^{2} \right]\leq\varepsilon^{2}_{\mathtt{RepLearn}}(n,\delta), \tag{21}\]

_where \(\varepsilon^{2}_{\mathtt{RepLearn}}(n,\delta)\coloneqq cTd^{3}n^{-1}\log(| \Phi|/\delta)\), for some sufficiently large absolute constant \(c>0\)._

To prove the theorem, we need a technical lemma, which follows from Modi et al. [35, Lemma 14].

**Lemma E.1**.: _Consider a call to \(\mathtt{RepLearn}\big{(}h,\mathcal{F},\Phi,P,n\big{)}\) (Algorithm 6) in the setting of Theorem E.1. Further, let \(\mathcal{L}_{\mathcal{D}}\) be as in Algorithm 6 and define_

\[(\phi^{(t)},\widehat{w}_{1}^{(t)},\ldots,\widehat{w}_{t-1}^{(t)})\in\operatorname {arg\,min}_{\phi\in\Phi,(w_{1},\ldots,w_{t-1})\in\mathcal{B}(2\sqrt{d})^{t-1}} \sum_{\ell=1}^{t-1}\mathcal{L}_{\mathcal{D}}(\phi,w_{\ell},f^{(\ell)}). \tag{22}\]

_For any \(\delta\in(0,1)\), there is an event \(\mathcal{E}^{(t)}(\delta)\) of probability at least \(1-\delta\) such that under \(\mathcal{E}^{(t)}(\delta)\), if Algorithm 6 does not terminate at iteration \(t\geq 1\), then for \(w^{(\ell)}\coloneqq w_{f^{(\ell)}}\):_

\[\sum_{\ell=1}^{t-1}\mathbb{E}_{\pi\sim P}\mathbb{E}^{\pi_{0}\pi_{ \mathrm{init}}}\left[\left(\phi^{(t)}(\mathbf{x}_{h},\mathbf{a}_{h})^{\top}\widehat{w }_{\ell}^{(t)}-\phi^{*}_{h}(\mathbf{x}_{h},\mathbf{a}_{h})^{\top}w^{(\ell)}\right)^{2} \right]\leq t\varepsilon^{2}_{\mathtt{stat}}(n,\delta), \tag{23}\] \[\inf_{w\in\frac{1}{2}\mathcal{B}(d^{3/2})}\mathbb{E}_{\pi\sim P} \mathbb{E}^{\pi_{0}\pi_{\mathrm{init}}}\left[\left(\phi^{(t)}(\mathbf{x}_{h},\mathbf{ a}_{h})^{\top}w-\phi^{*}_{h}(\mathbf{x}_{h},\mathbf{a}_{h})^{\top}w^{(t)}\right)^{2} \right]>8dt\varepsilon^{2}_{\mathtt{stat}}(n,\delta),\]

_where \(\varepsilon^{2}_{\mathtt{stat}}\big{(}n,\delta\big{)}\coloneqq cd^{2}n^{-1} \log(|\Phi|/\delta\big{)}\) and \(c\geq 1\) is a sufficiently large absolute constant._

With this, we prove Theorem E.1.

**Proof of Theorem E.1.** Let us abbreviate \(\varepsilon\coloneqq\varepsilon_{\mathtt{stat}}\big{(}n,\delta\big{)}\), with \(\varepsilon_{\mathtt{stat}}\big{(}n,\delta\big{)}\) defined as in Lemma E.1. Further, let \(N\coloneqq 1+\left\lceil d\log_{3/2}(2d^{3/2}/\varepsilon)\right\rceil\), \(\delta^{\prime}\coloneqq\frac{\delta}{2N}\), and define

\[\tilde{\varepsilon}_{\mathtt{stat}}\coloneqq\varepsilon_{\mathtt{stat}} \big{(}n,\delta^{\prime}\big{)}. \tag{24}\]

Note that \(\varepsilon\leq\tilde{\varepsilon}_{\mathtt{stat}}\) and \(N-1\leq T\), where \(T\) is the number of iterations in the theorem statement; the latter inequality follows by the facts that the absolute constant \(c\) in Lemma E.1 is at least \(1\) and \(\log(|\Phi|/\delta)\geq 1\). We define an event \(\mathcal{E}\coloneqq\mathcal{E}^{(1)}(\delta^{\prime})\cap\cdots\cap\mathcal{ E}^{(N)}(\delta^{\prime})\), where \((\mathcal{E}^{t}(\cdot))_{t}\) are the success events in Lemma E.1. Note that \(\mathbb{P}[\mathcal{E}]\geq 1-\delta/2\) by the union bound. Throughout this proof, we condition on the event \(\mathcal{E}\).

To begin the proof, we define a sequence of vectors \((v^{(\ell)}_{1:d})_{\ell\geq 0}\) in an inductive fashion, with \(v^{(\ell)}_{i}\in\mathbb{R}^{d}\) for all \(i\in[d]\) and \(\ell\geq 0\). For \(\ell=0\), we let \(v^{(0)}_{i}=\varepsilon e_{i}/d\), for all \(i\in[d]\). For \(\ell\geq 1\), we consider two cases:

* **Case I:** If \[\mathcal{J}^{(\ell)}\coloneqq\left\{j\in[d]\ \Big{|}\left|\det(V^{(\ell-1)}_{-j},w^{( \ell)})\right|>(1+C)\cdot|\det(V^{(\ell-1)})|\right\}\neq\varnothing,\] where \(V^{(\ell-1)}\coloneqq(v^{(\ell-1)}_{1},\ldots,v^{(\ell-1)}_{d})\in\mathbb{R}^{ d\times d}\) and \(w^{(\ell)}\coloneqq w_{f^{(\ell)}}\), then we let \(j\coloneqq\arg\min_{j^{\prime}\in\mathcal{J}^{(\ell)}}j^{\prime}\) and define \[v^{(\ell)}_{i}\coloneqq\left\{\begin{array}{ll}w^{(\ell)},&\text{if }i=j,\\ v^{(\ell-1)}_{i},&\text{otherwise}.\end{array}\right.\]
* **Case II:** If \(\mathcal{J}^{(\ell)}=\varnothing\), we let \(v^{(\ell)}_{i}=v^{(\ell-1)}_{i}\), for all \(i\in[d]\).

We first show that \(\mathcal{J}^{(t)}\neq\emptyset\) at any iteration \(t\in[N]\) where \(\mathsf{RepLearn}\) does not terminate. Let \(t\in[N]\) be an iteration where the algorithm does not terminate, and suppose that \(\mathcal{J}^{(t)}=\emptyset\). This means that

\[\forall j\in[d],\quad|\det(V^{(t-1)}_{-j},w^{(t)})|\leq(1+C)\cdot|\det(V^{(t-1 )})|. \tag{25}\]

Now, since \(\det(V^{(t-1)})\neq 0\) (note that \(\left|\det(V^{(t)})\right|\) is non-decreasing with \(t\)), we have that \(\mathrm{span}(V^{(t-1)})=\mathbb{R}^{d}\). Thus, there exist \(\beta_{1},\ldots,\beta_{d}\in\mathbb{R}\) be such that \(w^{(t)}=\sum_{i=1}^{d}\beta_{i}v^{(t-1)}_{i}\). By the linearity of the determinant and (25), we have

\[\forall j\in[d],\quad(1+C)|\cdot\det(V^{(t-1)}) |\geq|\det(V^{(t-1)}_{-j},w^{(t)})|,\] \[=\left|\det\left(V^{(t-1)}_{-j},\sum_{i=1}^{d}\beta_{i}v^{(t-1)}

Using that \(C=1/2\), we conclude that the right-hand side of this inequality is bounded by \(8d\tilde{\varepsilon}^{2}_{\mathrm{stat}}\) which is a contradiction, since \(\widehat{w}^{(t)}_{t}\in(1+C)\mathcal{B}(2d^{3/2})=\mathcal{B}(3d^{3/2})\) and by Lemma E.1, we must have

\[\inf_{w\in\mathcal{B}(3d^{3/2})}\mathbb{E}_{\pi\sim P}\mathbb{E}^{\pi\circ_{h} \pi_{\mathrm{uni}}}\left[\left(\phi^{(t)}(\mathbf{x}_{h},\mathbf{a}_{h})^{\top}w-\phi^{ *}_{h}(\mathbf{x}_{h},\mathbf{a}_{h})^{\top}w^{(t)}\right)^{2}\right]>8t\tilde{ \varepsilon}_{\mathrm{stat}}\]

if \(\mathsf{RepLearn}\) does not terminate at round \(t\). Therefore, we have that \(\mathcal{J}^{(t)}\neq\emptyset\), for any iteration \(t\in[2\,..\,N]\) where \(\mathsf{RepLearn}\) does not terminate.

We now bound the iteration count and prove that the guarantee in Eq.21 holds at termination. Note that whenever \(\mathcal{J}^{(\ell)}\neq\emptyset\) for \(\ell>1\), we have by construction:

\[|\det(V^{(\ell)})|>3/2\cdot|\det(V^{(\ell-1)})|.\]

Thus, if \(\mathsf{RepLearn}\) runs for \(t\in[2\,..\,N]\) iterations, then

\[|\det(V^{(t)})|>(3/2)^{t-1}\cdot|\det(V^{(1)})|. \tag{30}\]

On the other hand, since the determinant of a matrix is bounded by the product of the norms of its columns and \(v^{(t)}_{1:d}\in\mathcal{B}(2\sqrt{d})\), we have

\[|\det(V^{(t)})|\leq 2^{d}d^{d/2}.\]

Note also that \(|\det(V^{(0)})|=(\varepsilon/d)^{d}\). Plugging this into (30), we conclude that

\[(3/2)^{t-1}<(2d^{3/2}/\varepsilon)^{d}.\]

Taking the logarithm on both sides and rearranging yields

\[t<1+d\log_{3/2}(2d^{3/2}/\varepsilon)\leq N.\]

Thus, the algorithm must terminate after most \(N-1\) iterations. Furthermore, by [35, Lemma 14], we have that with probability at least \(1-\frac{\delta}{2N}\), if the algorithm terminates at iteration \(t\), then

\[\max_{f\in\mathcal{F}}\inf_{w\in\mathcal{B}(3d^{3/2})}\mathbb{E}_ {\pi\sim P}\mathbb{E}^{\pi\circ_{h}\pi_{\mathrm{uni}}}\left[\left(w^{\top} \phi^{(t)}(\mathbf{x}_{h},\mathbf{a}_{h})-w^{\top}_{f}\phi^{*}_{h}(\mathbf{x}_{h},\mathbf{a}_ {h})\right)^{2}\right] \leq 32t\tilde{\varepsilon}^{2}_{\mathrm{stat}},\] \[\leq 32(N-1)\tilde{\varepsilon}^{2}_{\mathrm{stat}},\] \[\leq 32T\tilde{\varepsilon}^{2}_{\mathrm{stat}}.\]

Applying a union bound completes the proof. 

## Appendix F Analysis

In this section, we prove the main guarantee for \(\mathsf{SpanRL}\) (Theorem3.2). First, we outline our proof strategy in AppendixF.1. Then, in AppendixF.2 and AppendixF.3, we present guarantees for the instances of \(\mathsf{PSDP}\) (Algorithm3) and \(\mathsf{RobustSpanner}\) (Algorithm2) used within \(\mathsf{SpanRL}\). We then combine these results in AppendixF.5 to complete the proof of Theorem3.2. A self-contained guarantee for \(\mathsf{RobustSpanner}\)(Lemma3.1) is given in AppendixF.6.

### Proof Strategy

Like our algorithm, our analysis is inductive. For fixed \(h\), we assume that the policy set \(\Psi^{(1:h+1)}\) produced by \(\mathsf{SpanRL}\) satisfies the property:

\[\Psi^{(1)},\ldots\Psi^{(h+1)}\text{ are }\big{(}\tfrac{1}{4Ad},0\big{)}\text{- policy covers for layers }1\text{ through }h+1,\text{ and }\max_{t\in[h+1]}|\Psi^{(t)}|\leq d. \tag{31}\]

Conditioned on this claim, we show that with high probability, the set \(\Psi^{(h+2)}\) is a \(\big{(}\tfrac{1}{4Ad},0\big{)}\)-policy cover for layer \(h+2\). To prove this, we use the inductive assumption to show that \(\mathsf{PSDP}\) acts as an approximate linear optimization oracle over \(\mathcal{W}=\{\mathbb{E}^{\pi}\left[\phi^{(h)}(\mathbf{x}_{h},\mathbf{a}_{h})\right] |\neq\pi\in\Pi_{\mathsf{M}}\}\) (AppendixF.2). Using this, we then instantiate the guarantee of \(\mathsf{RobustSpanner}\) from LemmaF.3 with \(\mathsf{LinOpt}\) and \(\mathsf{LinEst}\) instantiated with \(\mathsf{PSDP}\) and \(\mathsf{EstVec}\). To conclude the proof of the inductive step, we the main guarantee for \(\mathsf{RobustSpanner}\) together with the main guarantee for \(\mathsf{RepLearn}\) (TheoremE.1), along with a change of measure argument enabled by the assumption that \(\Psi^{(1:h)}\) are policy covers (i.e. (31)).

### Guarantee for PSDP as a Subroutine for RobustSpanner

We begin by showing that PSDP instantiates the approximate linear optimization oracle required by RobustSpanner. In particular, we fix a layer \(h\) and assume that \(\Psi^{(1:h+1)}\) satisfy (31) and apply the results of Appendix D.

More precisely, we need to show that, for any \(\theta\in\mathbb{R}^{d}\setminus\{0\}\) and \(\phi\in\Phi\), PSDP approximately solves

\[\max_{\pi\in\Pi_{n}}\theta^{\top}\mathbb{E}^{\pi}\left[\phi(\mathbf{x}_{h},\mathbf{a} _{h})\right]. \tag{32}\]

We can equivalently formulate (32) as, for fixed \(\theta\in\mathbb{R}^{d}\setminus\{0\}\) and \(\phi\in\Phi\), maximizing the sum of the reward functions \(r_{1:h}(\cdot,\cdot;\theta,\phi)\) given by:

\[\forall(x,a)\in\mathcal{X}\times\mathcal{A},\quad r_{t}(x,a;\theta,\phi):=\left\{ \begin{array}{ll}\phi(x,a)^{\top}\frac{\theta}{\|\theta\|},&\text{for $t=h$},\\ 0,&\text{otherwise}.\end{array}\right. \tag{33}\]

Note that this matches the choice of reward functions in SpanRL (Algorithm 1) at iteration \(h\) with \(\phi=\hat{\phi}^{(h)}\), the feature map returned by RepLearn in Line 8. With these reward functions and the function class

\[\mathcal{G}\coloneqq\{g:(x,a)\mapsto\phi(x,a)^{\top}w\mid\phi\in\Phi,w\in \mathcal{B}(\sqrt{d})\}, \tag{34}\]

we show that the output \(\hat{\pi}=\texttt{PSDP}(h,r_{1:h}(\cdot,\cdot;\theta,\phi),\mathcal{G},\Psi^{( 1:h)},n)\) approximately solves (32) with high probability if \(n\geq 1\) is sufficiently large. We first verify that the class \(\mathcal{G}\) realizes the reward functions specified in (33) in the sense of Definition D.1.

**Lemma F.1**.: _Under Assumption 2.2, the function class \(\mathcal{G}\) in (34) realizes the reward functions in (33), for any \(\phi\in\Phi\) and \(\theta\in\mathbb{R}^{d}\setminus\{0\}\). Furthermore, we have that functions in \(\mathcal{G}\) are uniformly bounded by \(h\leq H\), and \(\log\mathcal{N}_{\mathcal{G}}(\varepsilon)\leq\log|\Phi|+d\log(H/\varepsilon)\), where we recall that \(\mathcal{N}_{\mathcal{G}}(\varepsilon)\) denotes the \(\varepsilon\)-covering number of \(\mathcal{G}\) in \(\ell_{\infty}\) distance._

Proof.: Fix \(\phi\in\Phi\) and \(\theta\in\mathbb{R}^{d}\setminus\{0\}\), and let \(r_{\ell}(\cdot,\cdot)\equiv r_{\ell}(\cdot,\cdot;\theta,\phi)\), for \(\ell\in[h]\). For \(t=h\), we clearly have that for any \(\pi\in\Pi_{n}^{kh}\), \(Q_{t}^{\pi}(\cdot,\cdot)=r_{t}(\cdot,\cdot)\in\mathcal{G}\). For \(t<h\) and \(\pi\in\Pi_{n}^{t:h}\), we have by the low-rank structure that

\[Q_{t}^{\pi}(x,a) =\int_{\mathcal{X}_{t+1}}\mathbb{E}^{\pi}[r_{h}(\mathbf{x}_{h},\mathbf{a }_{h})\mid\mathbf{x}_{t+1}=y,\mathbf{a}_{t+1}=\pi(y)]\cdot\phi_{t}^{*}(x,a)^{\top}\mu_ {t+1}^{*}(y)\mathrm{d}\nu(y),\] \[=\phi_{t}^{*}(x,a)^{\top}\left(\int_{\mathcal{X}_{t+1}}\mathbb{E} ^{\pi}[r_{h}(\mathbf{x}_{h},\mathbf{a}_{h})\mid\mathbf{x}_{t+1}=y,\mathbf{a}_{t+1}=\pi(y)] \cdot\mu_{t+1}^{*}(y)\mathrm{d}\nu(y)\right). \tag{35}\]

Now, by the fact that \(\mathbb{E}^{\pi}[r_{h}(\mathbf{x}_{h},\mathbf{a}_{h})\mid\mathbf{x}_{t+1}=y,\mathbf{a}_{t+1}= \pi(y)]\in[-1,1]\), for all \(y\in\mathcal{X}_{t+1}\) (since \(\phi(\cdot,\cdot)\in\mathcal{B}(1)\), for all \(\phi\in\Phi\)), and the normalizing assumption made on \((\mu_{h}^{*})_{h\in[H]}\) in Section 2.2 (i.e. that for all \(g:\mathcal{X}_{t+1}\to[0,1]\), \(\left\|\mathcal{X}_{t+1}\mu_{t+1}^{*}(y)g(y)\mathrm{d}\nu(y)\right\|\leq\sqrt{d}\)), we have that

\[w_{t}\coloneqq\int_{\mathcal{X}_{t+1}}\mathbb{E}^{\pi}[r_{h}(\mathbf{x}_{h},\mathbf{a }_{h})\mid\mathbf{x}_{t+1}=y,\mathbf{a}_{t+1}=\pi(y)]\cdot\mu_{t+1}^{*}(y)\mathrm{d} \nu(y)\in\mathcal{B}(\sqrt{d}).\]

This together with (35) and the fact that \(\phi_{t}^{*}\in\Phi\) (by Assumption 2.2), we have that \(Q_{t}^{\pi}\in\mathcal{G}\). 

Combining Lemma F.1 with Theorem D.2 results in the following bound on the quality of PSDP as an approximate linear optimization oracle over the space of policies.

**Corollary F.1**.: _Let \(\varepsilon,\delta\in(0,1)\) and \(h\in[H]\). Further, let \(\theta\in\mathbb{R}^{d}\setminus\{0\}\), \(\phi\in\Phi\), and \(\hat{\pi}=\texttt{PSDP}(h,r_{1:h}(\cdot,\cdot;\theta,\phi),\mathcal{G},\Psi^{( 1:h)},n)\), where_

* _The reward functions_ \(r_{1:h}(\cdot,\cdot;\theta,\phi)\) _are as in (_33_)._
* _The function class_ \(\mathcal{G}\) _is as in (_34_)._
* _The collection of policies_ \(\Psi^{(1:h)}\) _satisfy (_31_)._
* _The parameter_ \(n\) _is chosen such that_ \(cHACd^{2}\cdot\varepsilon_{\mathrm{stat}}(n,\delta/H)\leq\varepsilon\)_, where_ \(\varepsilon_{\mathrm{stat}}(n,\delta^{\prime})\coloneqq\sqrt{dn^{-1}\cdot(d\log( nH)+\log(|\Phi|/\delta^{\prime}))}\) _and_ \(c>0\) _is some large enough absolute constant._

_Then, under Assumption 2.2, with probability at least \(1-\delta\), we have that_

\[\max_{\pi\in\Pi_{n}}\theta^{\top}\mathbb{E}^{\pi}\big{[}\phi(\mathbf{x}_{h},\mathbf{a} _{h})\big{]}\leq\theta^{\top}\mathbb{E}^{\hat{\pi}}\big{[}\phi(\mathbf{x}_{h},\mathbf{a} _{h})\big{]}+\varepsilon/2.\]

We emphasize that the inductive assumption that \(\Psi^{(1:h)}\) is a policy cover of bounded size enters only in the statement of Theorem D.2. We now give a guarantee for RobustSpanner as used in SpanRL.

```
0:
* Target layer \(h\in[H]\).
* Vector-valued function \(F:\mathcal{X}\times\mathcal{A}\to\mathbb{R}^{d}\).
* Policy \(\pi\in\Pi_{\mathsf{H}}\).
* Number of samples \(n\in\mathbb{N}\).
1:\(\mathcal{D}\leftarrow\varnothing\).
2:for\(n\) times do
3: Sample \((\mathbf{x}_{h},\mathbf{a}_{h})\sim\pi\).
4: Update dataset: \(\mathcal{D}\leftarrow\mathcal{D}\cup\{(\mathbf{x}_{h},\mathbf{a}_{h})\}\).
5:Return:\(\bar{F}=\frac{1}{n}\sum_{(x,a)\in\mathcal{D}}F(x,a)\).
```

**Algorithm 7**EstVec(\(h,F,\pi,n\)): Estimate \(\mathbb{E}^{\pi}\{F(\mathbf{x}_{h},\mathbf{a}_{h})\}\) for given policy \(\pi\) and function \(F\).

### Guarantee for RobustSpanner as a Subroutine for SpanRL

In this section, we prove a guarantee for the instantiation of RobustSpanner in SpanRL, which we require in the proof of the main theorem (Theorem 3.2). We first show that the LinEst subroutine passed to RobustSpanner can be taken to be EstVec (Algorithm 7), which simply estimates the expected feature imbedding of \((\mathbf{x}_{h},\mathbf{a}_{h})\) under policy \(\pi\) by sampling sufficiently many trajectories and taking the empirical mean.

**Lemma F.2** (Guarantee of EstVec).: _Let \(\delta\in(0,1)\) and \(\varepsilon>0\). For \(h\in[H]\), \(\phi\in\Phi\), \(\pi\in\Pi_{\mathsf{H}}\), and \(n\in\mathbb{N}\) such that \(n\geq\frac{\varepsilon}{\varepsilon^{2}}\log(d/\delta)\) for some large enough absolute constant \(c>0\), the output \(\overline{\phi}_{h}=\texttt{EstVec}(h,\phi,\pi,n)\) (Algorithm 7) satisfies, with probability at least \(1-\delta\),_

\[\|\overline{\phi}_{h}-\mathbb{E}^{\pi}\big{[}\phi(\mathbf{x}_{h},\mathbf{a}_{h})\big{]} \|\leq\varepsilon/2.\]

**Proof.** By Hoeffding's inequality (see for example [22, Corollary 7]) and the fact that \(\|\phi(x,a)\|\leq 1\) for all \(x\in\mathcal{X}\) and \(a\in\mathcal{A}\), there exists an absolute constant \(c>0\) such that with probability at least \(1-\delta\),

\[\|\overline{\phi}_{h}-\mathbb{E}^{\pi}\big{[}\phi(\mathbf{x}_{h},\mathbf{a}_{h})\big{]} \|\leq c\cdot\sqrt{\frac{\log(d/\delta)}{n}}.\]

Setting \(n\) as in the statement of the theorem concludes the proof. 

In SpanRL, we instantiate RobustSpanner passing PSDP as LinOpt and EstVec as LinEst. Combining Corollary F.1 and Lemma F.2 with the general guarantee of RobustSpanner in Proposition C.1, we have the following result.

**Lemma F.3**.: _Consider iteration \(h\in[H]\) of \(\texttt{SpanRL}(\Phi,\varepsilon,\alpha,\delta)\) (Algorithm 1) with \(\varepsilon,\mathsf{c}>0\), \(\delta\in(0,1)\), and feature class \(\Phi\) satisfying Assumption 2.2. Further, let \(\hat{\phi}^{(h)}\) denote the feature map returned by_ RepLearn _in Algorithm 1 at iteration \(h\). If \(\Psi^{(1:h)}\) satisfy (31) and \(\mathsf{c}=\operatorname*{polylog}(A,d,H,\log(|\Phi|/\delta))\) is large enough, then there is an event \(\mathcal{E}_{h}\) with probability at least \(1-\frac{\delta}{2H}\) such that_

* _The number of iterations of_ RobustSpanner _in Line_ 12 _of Algorithm_ 1 _is at most_ \(N=\big{\lceil}\frac{d}{2}\log_{2}\big{(}\frac{100d}{\varepsilon}\big{)}\big{\rceil}<\infty\)_, and_
* _The output_ \((\pi_{1},\ldots,\pi_{d})\) _of_ RobustSpanner _has the property that for all_ \(\pi\in\Pi_{\mathsf{H}}\)_, there exist_ \(\beta_{1},\ldots,\beta_{d}\in[-2,2]\) _such that_ \[\left\|\hat{\phi}^{(h),\pi}-\sum_{i=1}^{d}\beta_{i}\hat{\phi}^{(h),\pi_{i}} \right\|\leq 3d\varepsilon,\quad\text{where}\quad\hat{\phi}^{(h),\pi^{\prime}} \coloneqq\mathbb{E}^{\pi^{\prime}}\big{[}\hat{\phi}^{(h)}(\mathbf{x}_{h},\mathbf{a}_{h} )\big{]}.\]

**Proof.** By Proposition C.1, on the event that the instances of PSDP and EstVec used by RobustSpanner satisfy Assumption C.1 with \(\varepsilon^{\prime}=\frac{\varepsilon}{2}\), the two desiderata of the lemma hold.10 Weclaim that each call to PSDP and to EstVec satisfies Assumption C.1 with probability at least \(1-\frac{\delta}{8dN}H\). Because each of PSDP and EstVec get called at most \(4dN\) times per iteration of RobustSpanner, a union bound concludes the proof contingent on the above claim.

We now prove the claim. First, note that the instance of PSDP that RobustSpanner uses within Algorithm 1 is of the form:

\[\texttt{PSDP}(h,r_{1:h}(\cdot,\cdot,\theta),\mathcal{G},\Psi^{1:h},n_{\texttt{ PSDP}})\]

with \(r_{1:h}\) and \(\mathcal{G}\) as in Algorithm 1; this matches the form in Corollary F.1 (PSDP's guarantee) with \(\phi=\hat{\phi}^{(h)}\). Thus, by choosing

\[n_{\texttt{PSDP}}=\mathfrak{c}\cdot\frac{A^{2}d^{5}H^{2}\cdot(d\log(H)+\log(8dH ^{2}N|\Phi|/\delta))}{\varepsilon^{2}},\]

for \(\mathfrak{c}=\operatorname{polylog}(A,d,H,\log(|\Phi|/\delta))\) sufficiently large, the conditions of Corollary F.1 are satisfied, and its conclusion implies the claim for the PSDP instance used by RobustSpanner. Similarly, the choice of \(n_{\texttt{EstVec}}\) in Algorithm 1 ensures that the claim holds for the instance of EstVec that RobustSpanner uses by Lemma F.2. The result follows.

### Guarantee for RepLearn as a Subroutine for SpanRL

In this section, we prove a guarantee for the invocation of RepLearn within SpanRL

Recall that \(P^{(h)}=\texttt{unif}(\Psi^{(h)})\) is the distribution over policies that SpanRL passes to RepLearn at iteration \(h\in[H-2]\) to compute feature map \(\phi^{(h)}\). Thus, by invoking Theorem E.1 in Appendix E and using the choice of \(n_{\texttt{RepLearn}}\) in Algorithm 1, we immediately obtain the following corollary.

**Corollary F.2**.: _Let \(\delta,\varepsilon\in(0,1)\), and \(\mathcal{F}\) be as in Algorithm 1, and fix \(h\in[H-2]\). Suppose that the feature class \(\Phi\) satisfies Assumption 2.2. Then, with probability at least \(1-\frac{\delta}{2H}\), the instance of RepLearn in Line 9 of Algorithm 1 runs for \(t\leq\mathfrak{c}\cdot d\) iterations for \(\mathfrak{c}=\operatorname{polylog}(A,d,H,\log(|\Phi|/\delta))\) sufficiently large, and returns output \(\phi^{(h)}\) such that for all \(f\in\mathcal{F}\), there exists \(w_{f}^{(h)}\in\mathcal{B}(3d^{3/2})\) satisfying_

\[\mathbb{E}^{\texttt{unif}(\Psi^{(h)})}\left[\sum_{a\in\mathcal{A}}\left(\phi^{ (h)}(\mathbf{x}_{h},a)^{\top}w_{f}^{(h)}-\phi_{h}^{*}(\mathbf{x}_{h},a)^{\top}w_{f} \right)^{2}\right]\leq\frac{\eta^{2}}{64A^{2}d^{2}},\]

_where \(w_{f}\coloneqq\int_{\mathcal{X}_{h+1}}f(y)\mu_{h+1}^{*}(y)\mathrm{d}\nu(y)\)._

### Concluding the Proof of Theorem 3.2

In this section, we conclude the proof of the main guarantee (Theorem 3.2). We derive the guarantee from the following inductive claim.

**Theorem F.1**.: _Consider iteration \(h\in[H]\) of \(\textsf{SpanRL}(\Phi,\varepsilon,\mathfrak{c},\delta)\) (Algorithm 1) with parameters \(\varepsilon,\mathfrak{c}>0\), \(\delta\in(0,1)\) and a feature class \(\Phi\) satisfying Assumption 2.2. Further, assume that:_

* _The collection of policies_ \(\Psi^{(1:h+1)}\) _at the start of the_ \(h\)_th iteration of_ SpanRL _satisfy (_31_)._
* _Assumption_ 2.1 _(reachability) holds with_ \(\eta>0\)_._
* _The input parameter_ \(\varepsilon\) _to_ SpanRL _is set to_ \(\varepsilon=\frac{\eta}{36d^{3/2}}\)_._
* _The input parameter_ \(\mathfrak{c}=\operatorname{polylog}(A,d,H,\log(|\Phi|/\delta))\) _is sufficiently large._

_Then, with probability at least \(1-\frac{\delta}{H}\), the set of policies \(\Psi^{(h+2)}\) produced by \(\textsf{SpanRL}(\Phi,\varepsilon,\mathfrak{c},\delta)\) at the end of iteration \(h\) is an \((\frac{1}{4Ad},0)\)-policy cover for layer \(h+2\)._

With this, we can now prove Theorem 3.2.

Proof of Theorem 3.2.: Note that it suffices to prove that (31) holds for \(h=H-1\) with probability at least \(1-\delta\). To do this, we proceed by induction over \(h=1,\ldots,H-1\). The base case of \(h=1\) trivially holds because \(\Psi^{(1)}=\varnothing\) and \(\Psi^{(2)}=\{\pi_{\texttt{unif}}\}\). The induction step now follows by Theorem F.1 and the union bound (see e.g. [30, Lemma I.2]).

[MISSING_PAGE_FAIL:30]

Applying the guarantee for RepLearn.Moving forward, let \(\phi^{(h)}\) be the feature map returned by RepLearn at the \(h\)th iteration of Algorithm 1, and define \(\phi^{(h),\pi}\coloneqq\mathbb{E}^{\pi}[\phi^{(h)}(\mathbf{x}_{h},\mathbf{a}_{h})]\), for any \(\pi\in\Pi_{\text{H}}\). Further, let \(w_{x}^{(h)}\) be the vector \(w_{f}^{(h)}\) in Corollary F.2 with \(f=f_{x}\), and note that

\[\|w_{x}^{(h)}\|\leq 3d^{3/2}. \tag{41}\]

By Jensen's inequality, we compute

\[\big{(}(w_{x}^{(h)})\phi^{(h),\pi_{x}}-(w_{x})\phi_{h}^{*,\pi_{x}} \big{)}^{2}\] \[\leq\mathbb{E}^{\pi_{x}}\bigg{[}\big{(}\phi^{(h)}(\mathbf{x}_{h},\mathbf{ a}_{h})^{\top}w_{x}^{(h)}-\phi_{h}^{*}(\mathbf{x}_{h},\mathbf{a}_{h})^{\top}w_{x}\big{)}^{2 }\bigg{]},\quad\text{(Jensen's inequality)}\] \[=\int_{\mathcal{X}_{h}}\big{(}\phi^{(h)}(y,\pi_{x}(y))^{\top}w_{x }^{(h)}-\phi_{h}^{*}(y,\pi_{x}(y))^{\top}w_{x}\big{)}^{2}\,\mu_{h}^{*}(y)^{\top }\phi_{h-1}^{*,\pi_{x}}\mathrm{d}\nu(y),\quad\text{(Low-Rank MDP)}\] \[\leq\alpha^{-1}\max_{\tilde{\pi}\in\Psi^{(h)}}\int_{\mathcal{X}_{ h}}\big{(}\phi^{(h)}(y,\pi_{x}(y))^{\top}w_{x}^{(h)}-\phi_{h}^{*}(y,\pi_{x}(y))^{ \top}w_{x}\big{)}^{2}\,\mu_{h}^{*}(y)^{\top}\phi_{h-1}^{*,\tilde{\pi}}\mathrm{ d}\nu(y),\quad\text{(by \eqref{eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq: eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq: eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq:eq: eq:where the inequality follows from the non-negativity of \(\mu^{\star}_{h+1}(\)\()^{\top}\phi^{\star}_{h+1}(x,a)\), for all \((x,a)\in\mathcal{X}_{h}\times\mathcal{A}\) (due to Lemma F.4), and (46) follows from the definition of \(\Psi^{(h+2)}\) in Line 13 of Algorithm 1. Combining (45) and (46) then implies that

\[\frac{1}{\big{\|}\mu^{\star}_{h+2}(x)\big{\|}}\mu^{\star}_{h+2}(x) ^{\top}\phi^{\star,\pi_{x}}_{h+1}=\theta^{\top}_{x}\phi^{\star,\pi_{x}}_{h+1}=w ^{\top}_{x}\phi^{\star,\pi_{x}}_{h} \leq 4d\cdot\max_{i\in[d]}w^{\top}_{x}\phi^{\star,\pi_{i}}_{h},\] \[\leq\frac{4Ad}{\big{\|}\mu^{\star}_{h+2}(x)\big{\|}}\max_{\pi\in \Psi^{(h+2)}}\mu^{\star}_{h+2}(x)^{\top}\phi^{\star,\pi}_{h+1}.\]

This, together with Lemma F.4, implies that (37) holds. Since this argument holds uniformly for all \(x\in\mathcal{X}_{h+2}\), this completes the proof. 

### Proof of Lemma 3.1

By definition for \(x\in\mathcal{X}_{h+1}\), we have \(d^{\pi}(x)=\mathbb{E}^{\pi}\big{[}\mu^{\star}_{h+1}(x)^{\top}\phi^{\star}_{h} (\mathbf{x}_{h},\mathbf{a}_{h})\big{]}\). Let \(\pi_{x}\) denote the policy maximizing \(d^{\pi}(x)\) (if no such maximizer exists, we may pass to a maximizing sequence) and let \(\Psi=\{\pi_{1},\ldots,\pi_{d}\}\). Then, we have for some \(\beta_{1},\ldots,\beta_{d}\in[-C,C]\),

\[d^{\pi_{x}}(x) =\mu^{\star}_{h+1}(x)^{\top}\bigg{(}\sum_{i=1}^{d}\beta_{i}\phi^{ \star,\pi_{i}}_{h}\bigg{)}+\mu^{\star}_{h+1}(x)^{\top}\bigg{(}\phi^{\star,\pi_ {x}}_{h}-\sum_{i=1}^{d}\beta_{i}\phi^{\star,\pi_{i}}_{h}\bigg{)},\] \[\leq Cd\cdot\max_{i\in[d]}\mu^{\star}_{h+1}(x)^{\top}\phi^{\star, \pi_{i}}_{h}+\varepsilon\cdot\|\mu^{\star}_{h+1}(x)\|,\quad\text{(Cauchy- Schwarz)}\] \[\leq Cd\cdot\max_{i\in[d]}\mu^{\star}_{h+1}(x)^{\top}\phi^{\star, \pi_{i}}_{h}+\frac{1}{2}d^{\pi_{x}}(x),\]

where the inequality follows by the fact that Assumption 2.1 holds with \(\varepsilon\leq\eta/2\). The result now follows by rearranging.

## Appendix G Application to Reward-Based RL

In this section, we explain how the output \(\Psi^{(1:H)}\) of \(\mathsf{SpanRL}\) (Algorithm 1) can be used to optimize downstream reward functions \(r_{1:H}\); our treatment is standard. Since the output of \(\mathsf{SpanRL}\) is a policy cover, one way to optimize the sum of rewards \(S_{H}\coloneqq\sum_{h=1}^{H}r_{h}\) is by first generating trajectories using policies in \(\Psi^{(1:H)}\), then applying an offline RL algorithm, e.g. Fitted Q-Iteration (FQI) [16], to optimize \(S_{H}\). It is also possible to use \(\mathsf{PSDP}\) with the policy cover \(\Psi^{(1:H)}\) to achieve the same goal. We will showcase the latter approach since we have already stated a guarantee for \(\mathsf{PSDP}\).

As in Appendix D, we assume access to a function class \(\mathcal{G}\in\{g:\mathcal{X}\times\mathcal{A}\to\mathbb{R}\}\) that realizes the rewards \(r_{1:H}\) in the following sense: for all \(h\in[H]\) and all \(\pi\in\Pi^{h:H}_{\mathsf{H}}\),

\[Q^{\pi}_{h}\in\mathcal{G},\quad\text{where}\quad Q^{\pi}_{h}(x,a)\coloneqq r_ {h}(x,a)+\mathbb{E}^{\pi}\bigg{[}\sum_{t=h+1}^{H}r_{t}(\mathbf{x}_{t},\mathbf{a}_{t}) \bigg{]}\ \mathbf{x}_{h}=x,\mathbf{a}_{h}=a\bigg{]}.\]

Note that when the reward functions \(r_{1:H}\) are linear in the feature map \(\phi^{\star}_{h}\); that is, when for all \(h\in[H]\) and \((x,a)\in\mathcal{X}_{h}\times\mathcal{A}\), \(r_{h}(x,a)=\theta^{\top}_{h}\phi^{\star}_{h}(x,a)\) for some \(\theta_{h}\in\mathcal{B}(1)\) (a common assumption in the context of RL in Low-Rank MDPs [32, 31, 49, 35]), then the function class

\[\mathcal{G}\coloneqq\{g:(x,a)\mapsto\phi(x,a)^{\top}w\mid\phi\in\Phi,w\in \mathcal{B}(2H\sqrt{d})\},\]

realizes \(r_{1:H}\). Note that \(\mathcal{G}\) is the same function class we used for the \(\mathsf{PSDP}\) subroutine in Algorithm 3, albeit with a larger ball for the \(w\)'s. For the sake of generality, we state the next result (which shows how to use a policy cover to optimize a downstream reward function) for general \(r_{1:H}\) and a function class \(\mathcal{G}\) that realizes \(r_{1:H}\).

**Theorem G.1**.: _Let \(\varepsilon>0\) and \(\delta\in(0,1)\). Suppose reward functions \(r_{1:H}:\mathcal{X}\times\mathcal{A}\to\mathbb{R}\), a collection of policies \(\Psi^{(1:H)}\), and a parameter \(n\geq 1\) satisfy the following:_

* _The function class_ \(\mathcal{G}\) _realizes the reward functions_ \(r_{1:H}\) _(in the sense of Definition_ D.1_), and_ \(\lim_{n\to\infty}n^{-1}\cdot\log\mathcal{N}_{\mathcal{G}}\big{(}1/n\big{)}=0\)_, where_ \(\mathcal{N}_{\mathcal{G}}\big{(}1/n\big{)}\) _to denote the_ \(\frac{1}{n}\)_-covering number of_ \(\mathcal{G}\) _in the supremum norm. Furthermore, we suppose that functions in_ \(\mathcal{G}\) _are uniformly bounded by_ \(H\sqrt{d}\)* _For some_ \(0<\alpha\leq 1\)_, for each_ \(1\leq h\leq H\)_, it holds that_ \(\Psi^{(h)}\) _is an_ \(\alpha\)_-policy cover for layer_ \(h\) _and moreover_ \(|\Psi^{(h)}|\leq d\)_._
* _The parameter_ \(n\) _is chosen such that_ \(cdH\alpha^{-1}\cdot\varepsilon_{\mathrm{stat}}(n,\delta/H)\leq\varepsilon\)_, where_ \(\varepsilon_{\mathrm{stat}}(n,\delta^{\prime})\coloneqq\sqrt{dH^{2}n^{-1} \cdot\left(\log\mathcal{N}_{\mathcal{G}}(1/n)+\log(1/\delta)\right)}\) _and_ \(c>0\) _is a large enough absolute constant._

_Then, with probability at least \(1-\delta\), the policy \(\hat{\pi}=\mathsf{PSDP}(H,r_{1:H},\mathcal{G},P^{(1:H)},n)\) (where \(P^{(t)}\coloneqq\mathsf{unif}(\Psi^{(t)})\), for each \(t\in[h]\)) coming from Algorithm 3, satisfies the following guarantee:_

\[\max_{\pi\in\Pi_{h}}\mathbb{E}^{\pi}\left[\sum_{h=1}^{H}r_{h}(\mathbf{x}_{h},\mathbf{ a}_{h})\right]\leq\mathbb{E}^{\hat{\pi}}\left[\sum_{h=1}^{H}r_{h}(\mathbf{x}_{h}, \mathbf{a}_{h})\right]+\varepsilon.\]

_Moreover, the number of episodes used by \(\mathsf{PSDP}\) in this case is_

\[\widetilde{O}\left(\frac{A^{2}d^{5}H^{5}(\log\mathcal{N}_{\mathcal{G}}( \varepsilon)+\log(1/\delta))}{\varepsilon^{2}}\right).\]

Proof.: This is simply a restatement of Theorem D.2 with \(h=H\). The number of trajectories follows by the fact that each call to \(\mathsf{PSDP}\) requires \(Hn\) trajectories. 

## Appendix H Properties of Reachability Assumption

In this section, we compare \(\eta\)-reachability (Assumption 2.1) to different reachability assumptions used in the literature in the context of RL in Low-Rank MDPs and show that ours is the weakest among those commonly assumed. In Appendix H.1, we demonstrate an exponential separation between our notion of reachability and that considered with respect to the popular _latent variable model_[1, 35]. In Appendix H.2, we consider a number of other reachability assumptions made outside the latent variable model and show how they imply Assumption 2.1.

### Comparison to Latent Variable Model

In this subsection, we show that our reachability assumption is implied a reachability assumption used by [1, 35] in the latent variable/non-negative feature model, and show that our reachability assumption can hold even when the best possible latent variable embedding dimension is exponential in the dimension \(d\). We begin by defining the latent variable model.

**Definition H.1** (Latent variable representation).: _Given a transition operator \(T:\mathcal{X}\times\mathcal{A}\to\Delta(\mathcal{X})\), a latent variable representation consists of a countable latent space \(\mathcal{Z}\) and functions \(\psi:\mathcal{X}\times\mathcal{A}\to\Delta(\mathcal{Z})\) and \(q:\mathcal{Z}\to\Delta(\mathcal{X})\), such that \(T(\cdot\mid x,a)=\sum_{z\in\mathcal{Z}}q(\cdot\mid z)\psi(z\mid x,a)\). The latent variable dimension of \(T\), denoted \(d_{\mathsf{LV}}\) is the cardinality of smallest latent space \(\mathcal{Z}\) for which \(T\) admits a latent variable representation._

The interpretation for the latent variable model is as follows:

1. Each \((x,a)\) pair induces a distribution \(\psi(x,a)\in\Delta(\mathcal{Z})\) over \(z\in\mathcal{Z}\).
2. The latent variable is sampled as \(\mathbf{z}\sim\psi(x,a)\).
3. The next state is sampled as \(\mathbf{x}^{\prime}\sim q(\cdot\mid\mathbf{z})\).

Note that in discrete state spaces, all transition operators admit a trivial latent variable representation, as we may take \(\psi(x,a)=T(\cdot\mid x,a)\), but the dimension of such a representation is potentially infinite. A latent variable representation certifies that there exists a factorization \(T(x^{\prime}\mid x,a)=\psi(x,a)^{\top}q(x^{\prime})\) with embedding dimension \(|\mathcal{Z}|\), and so \(d_{\mathsf{LV}}\), and hence gives an upper bound on the rank of the transition operator. On the other hand, compared with the general Low-Rank factorization, the latent variable factorization additionally requires that \(\psi(x,a)\) and \(q(\cdot\mid z)\) are probability distributions, and thus non-negative, for all \(z\in\mathcal{Z}\) and \((x,a)\in\mathcal{X}\times\mathcal{A}\), implying that \(d_{\mathsf{LV}}\) is equivalent to the _non-negative rank_[1] of the transition operator.

Assuming that a latent variable representation exists, [1, 35] consider the following notion of reachability.

**Definition H.2** (Reachability in latent variable model).: _There exists \(\eta>0\) such that_

\[\forall h\in[H-1],\forall z\in\mathcal{Z}_{h+1},\quad\sup_{\pi\in\Pi_{h}} \mathbb{P}^{\pi}[\mathbf{z}_{h+1}=z]\geq\eta. \tag{47}\]

[MISSING_PAGE_FAIL:34]

[MISSING_PAGE_FAIL:35]

where we have used that \(\|\theta\|=1\) and \(\|\phi^{*}_{h}(x,a)\|\leq 1\) for all \((x,a)\in\mathcal{X}_{h}\times\mathcal{A}\). Rearranging (49) and using that \(\theta^{\tau}\phi^{*}_{h}\geq 0\) (it is a scaled conditional density), have

\[\mathbb{P}^{\pi}[\theta^{\tau}\phi^{*}_{h}\geq\sqrt{\eta/2}]=\mathbb{P}^{\pi}[ (\theta^{\tau}\phi^{*}_{h})^{2}\geq\eta/2]\geq\eta/2.\]

Now, by Markov's inequality, we have that

\[\theta^{\tau}\phi^{*,\pi}_{h}=\mathbb{E}^{\pi}[\theta^{\tau}\phi^{*}_{h}]\geq \sqrt{\eta/2}\cdot\mathbb{P}^{\pi}[\theta^{\tau}\phi^{*}_{h}\geq\sqrt{\eta/2}] \geq(\eta/2)^{3/2},\]

where we have once more used that \(\theta^{\tau}\phi^{*}_{h}\geq 0\) almost surely. 

#### h.2.2 Explorability

We now consider the _explorability_ assumption of [46], which involves the first moment of the feature map \(\phi^{*}_{h}\). This notion is defined as follows.

**Definition H.5** (\(\eta\)-explorability).: _We say that a linear MDP satisfies \(\eta\)-explorability if for any \(h\in[H]\) and any \(\theta\in\mathbb{R}^{d}\setminus\{0\}\) it holds that_

\[\sup_{\pi\in\Pi_{\mathbf{s}}}|\theta^{\tau}\mathbb{E}^{\pi}[\phi^{*}_{h}( \boldsymbol{x}_{h},\boldsymbol{a}_{h})]|\geq\eta\cdot|\theta|.\]

We now show that \(\eta\)-explorability is a special case of \(\eta\)-reachability:

**Lemma H.3**.: _Suppose that the explorability condition in Definition H.5 is satisfied with \(\eta>0\). Then, \(\eta\)-reachability is satisfied._

Proof of Lemma H.3.: Let \(x\in\mathcal{X}_{h+1}\) and define \(\theta\coloneqq\mu^{*}_{h+1}(x)\). By explorability, we have that

\[\sup_{\pi\in\Pi_{\mathbf{s}}}d^{\pi}(x) =\sup_{\pi\in\Pi_{\mathbf{s}}}\mathbb{E}^{\pi}[\mu^{*}_{h+1}(x)^{ \top}\phi^{*}_{h}(\boldsymbol{x}_{h},\boldsymbol{a}_{h})],\] \[=\sup_{\pi\in\Pi_{\mathbf{s}}}|\mathbb{E}^{\pi}[\mu^{*}_{h+1}(x)^ {\top}\phi^{*}_{h}(\boldsymbol{x}_{h},\boldsymbol{a}_{h})]|,\quad(\mu^{*}_{h+ 1}(\cdot)^{\top}\phi^{*}_{h}(x,a)\text{ is a condition law})\] \[=\sup_{\pi\in\Pi_{\mathbf{s}}}|\theta^{\tau}\mathbb{E}^{\pi}[\phi ^{*}_{h}(\boldsymbol{x}_{h},\boldsymbol{a}_{h})]|,\] \[\geq\eta\cdot\|\theta\|,\quad\text{(by explorability)}\] \[=\eta\cdot\|\mu^{*}_{h+1}(x)\|.\]

This shows that Assumption 2.1 is satisfied with parameter \(\eta\).