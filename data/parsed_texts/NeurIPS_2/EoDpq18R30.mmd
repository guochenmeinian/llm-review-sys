# Reconciling Competing Sampling Strategies of Network Embedding

Yuchen Yan, Baoyu Jing, Lihui Liu, Ruijie Wang, Jinning Li,

**Tarek Abdelzaher, Hanghang Tong**

University of Illinois at Urbana Champaign, IL, USA

{yucheny5, baoyuj2, lihuil2, ruijiew2, jinning4, zaher, htong}@illinois.edu

###### Abstract

Network embedding plays a significant role in a variety of applications. To capture the topology of the network, most of the existing network embedding algorithms follow a _sampling_ training procedure, which maximizes the similarity (e.g., embedding vectors' dot product) between positively sampled node pairs and minimizes the similarity between negatively sampled node pairs in the embedding space. Typically, close node pairs function as positive samples while distant node pairs are usually considered as negative samples. However, under different or even competing sampling strategies, some methods champion sampling distant node pairs as positive samples to encapsulate longer distance information in link prediction, whereas others advocate adding close nodes into the negative sample set to boost the performance of node recommendation. In this paper, we seek to understand the intrinsic relationships between these competing strategies. To this end, we identify two properties (_discrimination_ and _monotonicity_) that given any node pair proximity distribution, node embeddings should embrace. Moreover, we quantify the empirical error of the trained similarity score w.r.t. the sampling strategy, which leads to an important finding that the _discrimination_ property and the _monotonicity_ property for _all_ node pairs can not be satisfied simultaneously in real-world applications. Guided by such analysis, a simple yet novel model (Sensei) is proposed, which seamlessly fulfills the _discrimination_ property and the _partial monotonicity_ within the top-\(K\) ranking list. Extensive experiments show that Sensei outperforms the state-of-the-arts in plain network embedding.

## 1 Introduction

In the era of big data, network embedding [36; 14; 21; 7; 42] maps nodes in the network to low-dimensional vectors in the embedding space, which plays an important role in many tasks such as node recommendation [54; 61; 38; 8; 20; 19; 1; 2], networked time series imputation [44; 18; 48; 9], knowledge graph completion [49; 46; 47; 45; 28], and network alignment [52; 53; 60; 62; 16; 27; 59]. To distill the topology information of the network, most existing network embedding methods follow a _sampling_ training procedure. Given any central node to be considered, existing network embedding methods build a positively sampled node pair set and a negatively sampled node pair set. Then, they optimize the embeddings of nodes by maximizing/minimizing the similarity between the positively/negatively sampled node pairs in the embedding space. Explicitly or implicitly, these methods are based on an assumption that nodes close to the central node should be included in the positively sampled node pair set, whereas distant nodes are likely to be considered as negative samples. For example, DeepWalk [36] and metapath2vec [7] explicitly construct random walk containing positively sampled nodes and select negative nodes according to the degree distribution of the network. The message-passing mechanism in GraphSAGE [15] and graph auto-encoder (GAE) [22] is built on graph Laplacian regularization [21; 57], which makes connected node pairs to besimilar. In this case, the close/distant nodes become implicit positive/negative samples for the central node, which can be regarded as a general form of such sampling training procedure.

Numerous network embedding algorithms have been proposed, which focus on improving either the positive sampling strategy or the negative sampling strategy based on various intuitions. Consequentially, disparate or even competing sampling strategies emerge in various methods. To name a few, rather than solely favor nodes within two hops as positive samples in LINE [41], node2vec [14] allows a longer random walk length. For graph convolution network (GCN) [21], APPNP [23] utilizes personalized pagerank [33] to sample more distant nodes w.r.t the central node for feature aggregation. The aforementioned two algorithms accommodate farther nodes in the positively sampled node pair set. Meanwhile, SPNE [12], KBGAN [4] and RecNS [56] encourage choosing closer nodes as _difficult_ negative samples to promote the performance of node recommendation.

Thus, some fundamental questions arise: _what is the theoretic root cause behind such antithetical sampling strategies? what are possible and what are impossible for sampling of network embedding? how can we develop a practical embedding algorithm that simultaneously embraces these competing strategies?_

In this paper, we hammer at bringing the intrinsic relationships behind these competing sampling strategies in light. Concretely, we start from two fundamental tasks of graph learning: the _link prediction_ task and the _node recommendation_ task. To tackle the above two tasks synchronously, we identify two desirable properties, including the _discrimination_ property and the _monotonicity_ property, that network embedding vectors should satisfy given a node pair proximity distribution. The _discrimination_ property means that the node pair with high proximity should be distinguished from the node pair owning low proximity in the embedding space (_link prediction_). For the _monotonicity_ property, the ranking list of nodes recommended to the query node needs to be consistent with the proximity list in the descending order. Theoretically, we analyze the general form of network embedding algorithms' loss functions. We show that the negative sampling distribution should be negatively correlated with the node pair proximity distribution. Furthermore, we show that in the ideal case where the algorithm can sample an infinite number1 of positive nodes for the central node, both the _discrimination_ property and the _monotonicity_ property can be fulfilled (i.e., possibility result). However, due to the limited sample size of the real-world data, there exists an inevitable error between the ideal optimal similarity scores and the empirical optimal similarity scores in the embedding space. Regardless of the specific sampling strategy, the _discrimination_ property and the _monotonicity_ property for _all_ node pairs in the network can not be fulfilled at the same time (i.e., impossibility result).

Footnote 1: Instead of having an infinite number of nodes in the network, we could sample positive pairs infinite times to approximate the true distributions.

Fortunately, in real-world applications, ranking all nodes in the network for the query node is often unnecessary. In many cases, only the top-\(K\) recommendation list and its internal order matters, which suggests that we can first achieve the _discrimination_ property to detect candidate nodes to be recommended. After that, we can attain the _monotonicity_ property within the top-\(K\) ranking list (_partial monotonicity_). Guided by this intuition and the theoretical results, we propose a simple yet novel model (Sensei). In detail, Sensei adopts a commonly used proximity measurement (personalized pagerank [33]) and decomposes the embedding process into two steps. The first step is to satisfy the _discrimination_ property. In addition to sampling nodes with large proximity as positive samples, Sensei also includes nodes with intermediate proximity in the positively sampled node pair set, which reduces the empirical error of the similarity scores for these nodes. Then, in the second step, Sensei pays attention to the _monotonicity_ property within the positively sampled node pairs from the first step. Nodes are ranked w.r.t. their proximities and importantly, some positively sampled nodes are turned into negative samples, which resembles the strategy of selecting _difficult_ negative samples in some existing methods [4, 55]. In this way, Sensei creatively integrates these two competing sampling strategies in one integral framework. The experiments demonstrate that Sensei greatly outperforms various baselines.

To summarize, our contributions are three folds:

* **Theoretical Analysis.** We reveal the underlying relationships between the competing sampling strategies of existing network embedding methods. Specially, we prove that any sampling strategy bears an inevitable error gap between the empirical and ideal optimal embedding similarity scores.
* **Simple yet Novel Model.** Based on the theoretical results, we propose a two-step model Sensei which creatively integrates competing sampling strategies in one network embedding framework. It satisfies the _discrimination_ property in the first step and obtains the _partial monotonicity_ property within the positively sampled node pair set in the second step.
* **Experimental Results.** Extensive experiments show that Sensei outperforms the state-of-the-arts in plain network embedding.

Problem setting.We primarily focus on plain network embedding, in which given the adjacency matrix of a plain network \(\mathbf{A}\), we aim to output the embedding matrix \(\mathbf{F}\).

## 2 Analysis

In this section, we uncover the intrinsic relationships of existing competing sampling strategies. First, we describe the sampling-based network embedding process mathematically. Then, we propose two desired properties for the learned embeddings: the _discrimination_ property and the _monotonicity_ property, which correspond to two common learning tasks: _link prediction_ and _node recommendation_. Theoretically, we analyze the general form of network embedding algorithms' loss functions. We show that the negative sampling distribution should be _negatively_ correlated with the given node pair proximity distribution. Guided by this critical insight, we give both possibility and impossibility results for network embedding. First (possibility result), we show that, in the ideal case where an infinite number of positive samples can be obtained, both the _discrimination_ property and the _monotonicity_ property can be satisfied with the optimal solution of the loss function. Second (impossibility result), for the empirical loss function with a limited number of positive samples, we prove that there always exists an error gap between the empirical optimal solution and the ideal optimal solution. Regardless of the specific sampling strategy, the _discrimination_ property and the _monotonicity_ property can not be simultaneously satisfied for _all_ node pairs in the network in the empirical situation.

Current sampling-based network embedding algorithms can be divided into two phases. The first phase is to construct a node pair proximity distribution \(p(u|v)\), which serves as the positive sampling distribution in the training process. For example, personalized pagerank [33] in APPNP [23] is an explicit node pair proximity distribution, while random walk in DeepWalk [36] and node2vec [14] implicitly constructs such node pair proximity distribution. The second phase is to design a negative sampling distribution \(p_{n}(u|v)\) and to utilize the network embedding loss function to train the embedding model. Similar to previous works [55, 56], we start from analyzing the second phase of the sampling-based network embedding process as follows:

**Definition 1**.: _Sampling-based Network Embedding Process in the Second Phase. Given a node proximity distribution \(p\), where \(p(u|v)\) refers to the proximity of node \(u\) w.r.t. the central node \(v\) and \(\sum_{u}p(u|v)=1\), the algorithm designs a negative sampling distribution \(p_{n}\) and adopts a loss function \(J\) to obtain similarity scores \(s(u,v)\in[0,1]\) of all node pairs \((u,v)\), which are calculated by the node embedding matrix \(\mathbf{F}\)._

The similarity score \(s(u,v)\) defined here is general, which can be implemented with various similarity measurements (e.g., the sigmoid function \(\sigma(\mathbf{F}(u,:)^{\top}\mathbf{F}(v,:))\) or the cosine function \(cos(\mathbf{F}(u,:),\mathbf{F}(v,:)))\) in various algorithms.

Then, to solve both link prediction and node recommendation tasks, we propose that the general similarity score \(s(u,v)\) calculated by node embeddings should possess two properties: the _discrimination_ property and the _monotonicity_ property, which are defined as the following:

**Definition 2**.: _Discrimination and Monotonicity._

_Discrimination Property: Given the central node \(v\), node pair \((u,v)\) with large \(p(u|v)\) should be clearly distinguished with node pair \((w,v)\) with small \(p(w|v)\) in the embedding space, i.e., \(\lim\frac{s(u,v)}{s(w,v)}=+\infty\) when \(\frac{p(w|v)}{p(u|v)}\to 0\)._

_Monotonicity Property: Given the central node \(v\) and two arbitrary nodes \(u\), \(w\) in the network, if \(p(u|v)>p(w|v)\), then \(s(u,v)>s(w,v)\)._The _discrimination_ property bears subtle difference from the _monotonicity_ property. Let us look at an illustrative example in the first line in Figure 1. For the central node \(v\), we assume that \(p(u_{1}|v)=0.45\), \(p(u_{2}|v)=0.4\) and \(p(w|v)=0.001\). _Case 1_: If \(s(u_{1},v)=0.7\), \(s(u_{2},v)=0.8\) and \(s(w,v)=0.01\), the final node embeddings satisfy the _discrimination_ property (\(\frac{s(w,v)}{s(u_{1},v)}=0.0143\) and \(\frac{s(w,v)}{s(u_{2},v)}=0.0125\), both approaching zero) but do not satisfy the _monotonicity_ property (\(p(u_{1}|v)>p(u_{2}|v)\) but \(s(u_{1},v)<s(u_{2},v)\)). _Case 2_: If \(s(u_{1},v)=0.51\), \(s(u_{2},v)=0.50\) and \(s(w,v)=0.49\), the final node embeddings fulfill the _monotonicity_ property (\(s(u_{1},v)>s(u_{2},v)>s(w,v)\)) but do not satisfy the _discrimination_ property (\(\frac{s(u_{1},v)}{s(w,v)}=1.04\) and \(\frac{s(u_{2},v)}{s(w,v)}=1.02\)). From the above example, we can see that the _discrimination_ property focuses on making node pairs with large proximity closer and pushing away node pairs with small proximity in the embedding space, which is desirable for tasks like binary classification for link prediction (i.e., predicting the existence of a link between two nodes). On the other hand, the _monotonicity_ property pays more attention to the rank of the similarity scores of node pairs, which is critical for tasks like ranking for node recommendation.

### Possibility Results in the Ideal Case

We conduct theoretic analysis about the loss function of network embedding. Previous works [24; 37; 55] have demonstrated that most existing network embedding algorithms can be regarded as implicit matrix factorization and there exists a general form of loss function for the given central node \(v\). If the sigmoid function is adopted as the similarity function. The loss function is:

\[J=-\mathbb{E}_{u\sim p(u|v)}\log\sigma(\mathbf{F}(u,\cdot)^{\top} \mathbf{F}(v,\cdot))-k\mathbb{E}_{w\sim p_{n}(w|v)}\log(1-\sigma(\mathbf{F}(w, \cdot)^{\top}\mathbf{F}(v,\cdot))) \tag{1}\]

where \(\sigma(\cdot)\) is the sigmoid function, \(\mathbf{F}(u,\cdot),\mathbf{F}(v,\cdot)\) and \(\mathbf{F}(w,\cdot)\) are the embeddings of \(u,v\) and \(w\) respectively, and \(k\) is the number of negative samples for each positive sample. If the sigmoid function is replaced by other similarity measurements, we can obtain a more general form of this loss function:

\[J=-\mathbb{E}_{u\sim p(u|v)}\log(s(u,v))-k\mathbb{E}_{w\sim p_{n} (w|v)}\log(1-s(w,v)) \tag{2}\]

where \(s(\cdot,\cdot)\) is a general form of similarity measurement.

In the ideal case where an infinite number of positive samples from \(p(u|v)\) are available, we have the following theorem about the optimal solution of \(s\):

**Theorem 1**.: _Optimal Solution of \(s\). The optimal solution of the similarity function \(s\) satisfies that for each node pair \((u,v)\),_

\[s(u,v)=\frac{p(u|v)}{p(u|v)+kp_{n}(u|v)} \tag{3}\]

Figure 1: _Discrimination_ and _monotonicity_ properties. We use star nodes to denote nodes with large \(p(u|v)\), round nodes to denote nodes with intermediate \(p(u|v)\) and rectangle nodes to denote nodes with small \(p(u|v)\). We assume that \(p(u_{1}|v)>p(u_{2}|v)>\cdots>p(u_{10}|v)\).

Proof.: \[J=-\mathbb{E}_{u\sim p(u|v)}\log s(u,v)-k\mathbb{E}_{w\sim p_{n}(u|v)}\log(1-s(w,v))\] (4)

Since \(s(u,v)\) is the parameter to be optimized, we can calculate \(\nabla_{s(u,v)}J\) as following:

\[\nabla_{s(u,v)}J=-p(u|v)\frac{1}{s(u,v)}-kp_{n}(u|v)\frac{1}{s(u,v)-1} \tag{5}\]

Let the derivative to be \(0\), we can obtain \(s(u,v)=\frac{p(u|v)}{p(u|v)+kp_{n}(u|v)}\), which is the optimal solution for \(J\). 

With the optimal solution of \(s(u,v)\), we give the following proposition about the design of \(p_{n}\):

**Proposition 1**.: _The Design of Negative Sampling. In the sampling-based network embedding process, the negative sampling distribution \(p_{n}\) should be **negatively** correlated with the given node pair proximity distribution \(p\) to fulfill the monotonicity property._

Proof.: Here we prove the above proposition in the ideal case by contradiction: In the ideal case, assume that the best embeddings and corresponding similarity scores to satisfy the _discrimination_ property and the _monotonicity_ property are obtained with \(p_{n}\), and \(p_{n}\) is not negatively correlated with \(p\). It is equivalent to the situation that for one central node \(v\), there must exist two nodes \(u\), \(w\) with \(p(u|v)>p(w|v)\) and \(p_{n}(u|v)>p_{n}(w|v)\). From Theorem 1 we know that \(s(u,v)=\frac{p(u|v)}{p(u|v)+kp_{n}(u|v)}\) and \(s(w,v)=\frac{p(w|v)}{p(w|v)+kp_{n}(w|v)}\). Now, we interchange \(p_{n}(u|v)\) with \(p_{n}(w|v)\) and we can get new similarity scores \(s^{\prime}(u,v)\) and \(s^{\prime}(w,v)\) for \((u,v)\) and \((w,v)\). \(s^{\prime}(u,v)=\frac{p(u|v)}{p(u|v)+kp_{n}(w|v)}>\frac{p(u|v)}{p(u|v)+kp_{n}( u|v)}=s(u,v)\). In addition, \(s^{\prime}(w,v)=\frac{p(w|v)}{p(w|v)+kp_{n}(u|v)}\)\(<\frac{p(w|v)}{p(w|v)+kp_{n}(w|v)}=s(w,v)\). So, we get two new similarity scores that can satisfy the _monotonicity_ property better, which is contradictory with the assumption that we have already achieved the best embeddings and corresponding similarity scores. 

Intuitively, this proposition makes sense because if one node is more likely to be sampled as a positive sample for the central node, it is less likely that this node also acts as a negative sample.

From Theorem 1, we can see that if \(p_{n}\) is negatively correlated with \(p\), when \(p(u|v)\) is large and \(p_{n}(u|v)\) is small, \(s(u,v)\to 1\); and when \(p(u|v)\) is small and \(p_{n}(u|v)\) is large, \(s(u,v)\to 0\), which satisfies the _discrimination_ property. For all nodes in the network with \(p(u_{1}|v)>p(u_{2}|v)>\cdots>p(u_{n}|v)\), if we set \(p_{n}(u_{1}|v)<p_{n}(u_{2}|v)<\cdots<p_{n}(u_{n}|v)\), the _monotonicity_ property can be fulfilled with \(s(u_{1},v)>s(u_{2},v)>\cdots>s(u_{n},v)\).

### Impossibility Results in the Empirical Case

However, in real-world applications, we can _not_ sample an infinite number of nodes from \(p(u|v)\). Empirically, existing algorithms set a fixed number \(T\) of positive samples for the central node and the general loss function in Eq. (2) turns into:

\[J_{e}=-\frac{1}{T}\sum_{i=1}^{T}\log(s(u_{i},v))-\frac{1}{T}\sum_{i=1}^{kT} \log(1-s(w_{i},v)) \tag{6}\]

where \(u_{i}\) is a positive sample from \(p(u|v)\) and \(w_{i}\) is a negative sample from \(p_{n}(w|v)\). We use \(S=[s(u_{1},v),s(u_{2},v),\ldots,s(u_{n},v)]\) to denote the final similarity scores to be optimized w.

The proof for Theorem 2 is attached in Appendix A.1. If the sigmoid function is adopted as the similarity function, the expectation of the similarity error in Theorem 2 degenerates to its special case:

\[\mathbb{E}[||\mathbf{F}_{e}(u,:)^{\top}\mathbf{F}_{e}(v,:)-\mathbf{F}^{*}(u,:)^{ \top}\mathbf{F}^{*}(v,:)||^{2}]=\frac{1}{T}(\frac{1}{p(u|v)}+\frac{1}{kp_{n}(u |v)}-1-\frac{1}{k}), \tag{8}\]

where \(\mathbf{F}_{e}(u,:)\) and \(\mathbf{F}^{*}(u,:)\) are the optimal embedding vectors of node \(u\) in \(J_{e}\) and \(J\). Eq. (8) was first discovered in MCNS [55]. Based on Eq. (8), MCNS advocates that, in order to bound the dot product error for nodes with large \(p(u|v)\), \(p_{n}\) should be _positively_ correlated with \(p\). Unfortunately, this is not always the best choice due to the following reason. In MCNS, the authors set \(p_{n}(u|v)\propto p(u|v)^{a}\), where \(0<a<1\). With this negative sampling distribution, the right part of Eq. (8) changes into \(\frac{1}{T}(\frac{1}{p(u|v)}(1+\frac{p(u|v)^{1-a}}{c})-1-\frac{1}{k})\), where \(c\) is a constant. If \(p(u|v)\) is large, this term can indeed be bounded. However, for distant nodes with small \(p(u|v)\) (i.e., \(p(u|v)\to 0\)), its positively correlated negative sampling probability \(p_{n}(u|v)\) is also very small and \(p_{n}(u|v)\to 0\), which leads to an extremely large error considering that the term \(\frac{1}{p(u|v)}+\frac{1}{kp_{n}(u|v)}\rightarrow\infty\), which may fail to fulfill the _discrimination_ property.

_Discussion_. Let us analyze the implications of Theorem 2. If \(p_{n}(u|v)\) is negatively correlated with \(p(u|v)\), it means that \(\frac{kp_{n}(u|v)}{p(u|v)}\to 0\) for nodes with the largest \(p(u|v)\)s and \(\frac{kp_{n}(u|v)}{p(u|v)}\rightarrow\infty\) for nodes with the smallest \(p(u|v)\)s. Let us consider the case that \(p(u|v)\) is large and \(\frac{kp_{n}(u|v)}{p(u|v)}\to 0\). \(\mathbb{E}[||(S_{e}-S^{*})_{u}||^{2}]<\frac{1}{T(\frac{1}{p_{n}(u|v)})^{2}}( \frac{1}{p(u|v)}+\frac{1}{kp_{n}(u|v)}-1-\frac{1}{k})<\frac{1}{T}(\frac{kp_{n} (u|v)}{p(u|v)})^{2}(\frac{1}{p(u|v)}+\frac{1}{kp_{n}(u|v)})\). It can be rewritten as \(\frac{1}{T}((\frac{kp_{n}(u|v)}{p(u|v)})^{2}\frac{1}{p(u|v)}+\frac{kp_{n}(u|v )}{p(u|v)}\frac{1}{p(u|v)})\). Since \(p(u|v)\) is among the largest proximity scores, \(\frac{1}{p(u|v)}\) is approximately bounded by a finite number \(\frac{1}{n}\), where \(n\) is the number of nodes in the network. As a result, \(\frac{kp_{n}(u|v)}{p(u|v)}\frac{1}{p(u|v)}\to 0\) and \((\frac{kp_{n}(u|v)}{p(u|v)})^{2}\frac{1}{p(u|v)}\to 0\). Therefore, for nodes with large \(p(u|v)\), the empirical embedding similarity score \(s_{e}(u,v)\approx s^{*}(u,v)\). With a similar analysis on nodes with small \(p(u|v)\), we reach the same conclusion that \(s_{e}(u,v)\approx s^{*}(u,v)\). Based on the above analysis, we can see that the empirical optimal solution can achieve the _discrimination_ property with a negatively correlated \(p_{n}\): for node pairs with large/small \(p(u|v)\), their similarity scores approach the ideal optimal solution. However, there might exist some nodes with \(p(u|v)\approx kp_{n}(u|v)\), which are referred to as nodes with _intermediate_\(p(u|v)\). For these nodes, the error in Theorem 2 is larger than nodes with small/large \(p(u|v)\) because \((2+\frac{kp_{n}(u|v)}{p(u|v)}+\frac{p(u|v)}{kp_{n}(u|v)})^{2}\) achieves its minimum when \(p(u|v)=kp_{n}(u|v)\). This can also be explained intuitively as follows. Nodes with large \(p(u|v)\) and large \(p_{n}(u|v)\) (small \(p(u|v)\)) are always sampled as positive samples and negative samples, while nodes with intermediate \(p(u|v)\) and \(p_{n}(u|v)\) might be ignored and are less likely to be sampled as positive samples or negative samples. As such, the embedding similarity scores for these nodes bear more uncertainty and large error between the empirical optimal solution and the ideal optimal solution (Theorem 2).

With these findings, we can now unveil the underlying reason for the competing sampling strategies in existing methods. The first category of methods (e.g., [36, 14, 23]) champion sampling nodes with longer distance from the given central node as positive samples. The essence of this positive sampling strategy is to include more nodes with intermediate \(p(u|v)\) in the positively sampled node pair set. In this way, these methods realize the goal to minimize the error in Theorem 2 for these nodes with intermediate \(p(u|v)\). However, this design has cost. When choosing more nodes with intermediate \(p(u|v)\) as positive samples, it implicitly decreases the \(p(u|v)\) for nodes with large \(p(u|v)\) (since \(\sum_{u}p(u|v)=1\)). As shown on the second line in Figure 1, it hurts the _monotonicity_ between these nodes (round nodes) and nodes with large \(p(u|v)\) (star nodes). On the contrary, for the second category of methods (e.g., [4, 56, 55]), they prefer to select nodes with intermediate \(p(u|v)\) as _difficult_ negative samples in the recommendation task, they successfully fulfill the _monotonicity_ property between nodes with large \(p(u|v)\) (star nodes) and nodes with intermediate \(p(u|v)\) (round nodes). Unfortunately, as our discussion about Eq. (8) has demonstrated, this _positively_ correlated negative sampling distribution could make the empirical error of the embedding similarity scores of nodes with small \(p(u|v)\) quite large. It does harm the _discrimination_ property, which is shown on the last line in Figure 1 (e.g., \(s(u_{8},v)\)).

To conclude, due to the limited number \(T\) of positive samples, the empirical loss tends to sample nodes with large \(p(u|v)\) or \(p_{n}(u|v)\) and there always exist some nodes that can not be sampled as positive/negative samples and thus are ignored. Therefore, the _discrimination_ property and the _monotonicity_ property for _all_ node pairs in the network can not be satisfied simultaneously in the empirical situation.

## 3 Model

In many real-world applications, given the query node, we primarily care about the top-\(K\) recommendation list. This suggests that we only need to satisfy the _discrimination_ property and the _partial monotonicity_ property within nodes that are likely to appear in the top-\(K\) recommendation list. Guided by this and the theoretical results in Section 2, we propose a simple yet novel model named Sensei.

**Key Idea.** The key idea of Sensei is to seamlessly integrate two competing sampling strategies into one model together. Concretely, it means that we can decompose the proposed Sensei into two steps. The first step is to satisfy the _discrimination_ property and the second step is to achieve the _monotonicity_ property within nodes that are likely to appear in the top-\(K\) recommendation list. As shown in Figure 2, in the first step, Sensei samples nodes with intermediate \(p(u|v)\), which is the strategy by the first category of methods mentioned in Section 2. Sensei constructs a _combined_ positive sample set, including nodes with large \(p(u|v)\) and nodes with intermediate \(p(u|v)\). During the training process, Sensei treats these nodes equivalently as positive samples and maximizes their similarity scores with the given central node. At the same time, it minimizes the similarity scores for nodes with small \(p(u|v)\) (large \(p_{n}(u|v)\)) to fulfill the _discrimination_ property. In the second step, Sensei focuses on the _difficult_ negative samples, which is the strategy by the second category of methods in Section 2. Sensei pays attention to the _monotonicity_ property within the _combined_ positive sample set in the first step. In detail, the previously sampled positive nodes have two roles: they act as the positive samples compared to nodes with smaller \(p(u|v)\) in the first step and meanwhile function as the _difficult_ negative samples compared to nodes with larger \(p(u|v)\) in the second step.

**Details.** For \(p(u|v)\), we run personalized pagerank [33], which is used throughout Sensei. In addition, we normalize node embeddings to have unit L2 norm and adopt the dot product (\(\mathbf{F}(u,.)^{\top}\mathbf{F}(v,:)\)) as the similarity function \(s(u,v)\).

**Step 1: Fulfill the _Discrimination_ Property.** In this step, to satisfy the _discrimination_ property, we construct a _combined_ positive sample set \(\mathcal{P}(v)\) for \(v\) in \(\mathbf{A}\), which includes nodes with large and intermediate \(p(u|v)\). Specifically, we rank the proximity \(p(u|v)\) for all nodes in \(\mathbf{A}\) to obtain a descending list \([p(u_{1}|v),p(u_{2}|v),\ldots,p(u_{n}|v)]\). Then, we set a threshold \(\tau\). Nodes with \(p(u|v)>\tau\) form the _combined_ positive sample set \(\mathcal{P}(v)\) for the central node \(v\) and the remaining nodes are added into the set \(\mathcal{N}(v)\), where negative samples will be randomly selected. So, the loss for node \(v\) is:

\[J(v)=-\frac{1}{|\mathcal{P}(v)|}(\sum_{u\in\mathcal{P}(v)}(k\mathbf{F}(u,:)^{ \top}\mathbf{F}(v,:)-\sum_{m=1}^{m=k}\mathbf{F}(w_{m},:)^{\top}\mathbf{F}(v,: ))) \tag{9}\]

where \(k\) is the negative sample number for each positive sample, \(u\) is the positively sampled node for \(v\) and \(w_{m}\in\mathcal{N}(v)\) is the negatively sampled node. By adding the loss for all \(v\)s in \(\mathbf{A}\), we obtain the

Figure 2: **The training process of Sensei.** Similar to Figure 1, we still utilize star nodes to denote nodes with large \(p(u|v)\), round nodes to denote nodes with intermediate \(p(u|v)\) and rectangle nodes to denote nodes with small \(p(u|v)\). In Step 1, Sensei includes round nodes (nodes with intermediate \(p(u|v)\)) as positive samples. In Step 2, the round nodes are used as hard negative samples for the star nodes.

overall loss in Step 1 as:

\[J_{P}=\sum_{v\in\mathbf{A}}J(v) \tag{10}\]

**Step 2: Fulfill the _Partial Monotonicity_ Property.** In this step, we focus on the _partial monotonicity_ property within the _combined_ positive sample set in Step 1. For any two nodes \(u_{l}\) and \(u_{m}\) in the positive sample set \(\mathcal{P}(v)\), if \(p(u_{l}|v)>p(u_{m}|v)\), we want to retain the _monotonicity_ between them with \(\mathbf{F}(u_{l},:)^{\top}\mathbf{F}(v,:)>\mathbf{F}(u_{m},:)^{\top}\mathbf{F}( v,:)\). Node \(u_{m}\) changes from a positive sample for \(v\) in Step 1 to a negative sample for \(v\) compared with \(u_{l}\), which is consistent with the idea of sampling _difficult_ negative samples. The margin-based ranking loss is:

\[L(v)=\sum\mathbbm{1}_{p(u_{l}|v)>p(u_{m}|v)}(\max\{\mathbf{F}(u_{m},:)^{\top} \mathbf{F}(v,:)-\mathbf{F}(u_{l},:)^{\top}\mathbf{F}(v,:)+\gamma,0\}) \tag{11}\]

where \(u_{l},u_{m}\in\mathcal{P}(v)\), \(\mathbbm{1}_{p(u_{l}|v)>p(u_{m}|v)}\) is the indicator function and \(\gamma\) is a positive margin. When \(p(u_{l}|v)>p(u_{m}|v)\), \(\mathbbm{1}_{p(u_{l}|v)>p(u_{m}|v)}=1\), otherwise, \(\mathbbm{1}_{p(u_{l}|v)>p(u_{m}|v)}=0\). Similar to Eq. (10) in Step 1, we have the loss \(L_{p}\) in Step 2 as:

\[L_{P}=\sum_{v\in\mathbf{A}}L(v). \tag{12}\]

In fact, Step 2 can be regarded as a fine-tuning process to fulfill the _partial monotonicity_ property. Therefore, this step usually has a small learning rate and a small positive margin \(\gamma\). The pseudocode of this algorithm can be found in Appendix A.2.

**Complexity Analysis.** We give a complexity analysis of Sensei here. For Sensei, the time complexity can be analyzed as the following: (1) Calculating personalized pagerank has the complexity \(O(\text{iter}_{\max}\cdot\|\mathcal{E}\|)\), where \(\|\mathcal{E}\|\) is the number of edges in the graph and \(\text{iter}_{\max}\) is the maximum iterations or hops for personalized pagerank; (2) the time complexity for sorting the proximity distribution \(p(u|v)\) and sampling positive/negative nodes for node \(v\) is \(O(n\cdot\log(n))\); (3) Calculating \(J(v)\) takes \(O(\|\mathcal{P}(v)\|\cdot k\cdot d)\), where \(\|\mathcal{P}(v)\|\) is the positive sample set, \(k\) is the number of negative samples and \(d\) is the dimension of node embedding. Obtaining \(J_{P}\) takes \(O(n)\) time; (4) Computing \(L(v)\) in Eq. (11) takes \(O(\|\mathcal{P}(v)\|^{2}\cdot d)\) and calculating \(L_{p}\) costs \(O(n)\). Most of these computations can be further parallelized.

## 4 Experiment

In this section, we evaluate the effectiveness of the proposed algorithm (Sensei) for solving link prediction and node recommendation simultaneously in plain networks.

### Experimental Setup

**Datasets.** We use 4 public real-world datasets to evaluate the proposed Sensei model: C.ele [51], Cora [39], Citeseer [39], NS [31].

**Baselines.** We compare the proposed Sensei with the following 5 plain network embedding methods: node2vec [14], VGAE [22], GAT [43], ARGVA [35] and RBGE [17].

**Metrics.** For link prediction and node recommendation in plain networks, we randomly split edges in every dataset into 70/10/20% for training, validation, and test.2 The same amount of additionally sampled non-existent edges are taken as negative edges for training, validation and test in the link prediction task.

Footnote 2: The node recommendation task in plain networks is to recommend potential nodes that are most likely to have edges connected to the query node.

**Settings.** For the link prediction task in plain networks, we use Area Under the ROC and Precision-Recall Curves (i.e., AUC-ROC and AUC-PR) as the metrics to evaluate the performance of different methods. For the node recommendation task in plain networks, we take Hit@\(K\) as the metric. Only when the model ranks the correct node as top-\(K\) in the recommendation list, it is counted as a hit for this query node in the test set. Therefore, the average hit number for all query nodes is Hit@\(K\). We set \(K=10\) for Sensei.

**Additional Contents.** In Appendix, we provide more contents related to experiments, including (1) dataset statistics (Appendix A.3); (2) implementation details (Appendix A.3); (3) the comparison with additional GCN-based methods: GraphSAGE [15], Cluster-gcn [5], APPNP [13], GPRGNN [6] and H2GCN [63] (Appendix A.4).

### Effectiveness of Sensei

The results of link prediction on all 4 plain networks are presented in Table 1. Our proposed Sensei generally outperforms all baselines on all 4 datasets. In particular, Sensei achieves about 2% improvement in AUC-PR compared to the best competitor (e.g., VGAE on Citeseer and node2vec on C.ele). For the node recommendation task, as shown in Table 2, node2vec obtains the best Hit@10 on Cora and NS among all baselines, which indicates that including more positive samples (\(T\)) by setting a larger walk length (80) can indeed reduce the error in Theorem 2. Notice that the proposed Sensei still outperforms node2vec on all 4 datasets, which demonstrates the ability of Sensei to fulfill the _partial monotonicity property_.

### Ablation Study and Sensitivity Study

In this subsection, we conduct the ablation study to validate the importance of Step 2 of Sensei. As illustrated in Table 2, for the node recommendation task (Hit@10), Step 2 consistently improves the performance of Sensei. The results verify that Step 2 is indeed beneficial by fine-tuning node embeddings to satisfy the _partial monotonicity_ property. In addition, we carry out a sensitivity study on different \(K\)s for Hit@\(K\), which is presented in Figure 3. We observe that when \(K\) increases from 1 to 30, Hit@\(K\) gradually increases and Hit@30 is much higher than Hit@1, which is expected.

## 5 Related works

**Network Embeddings.** Network embedding maps nodes in the network to low dimensional vectors. It can be traced back to matrix low rank approximation [26] and spectral clustering [40]. DeepWalk [36] and node2vec [14] rely on random walk to encode the topological information of homogeneous networks. For multi-layered networks or heterogeneous networks, metapath2vec [7] and Hin2vec [10] design metapaths to capture the connectivity between different layers or different types of nodes. Since graph convolutional network (GCN) [21] emerges, many GCN-based network embedding methods have been proposed. For example, variational graph auto-encoder (VGAE) [22] adopts the same message-passing mechanism as GCN to embed the homogeneous network. More recently, GCN has been integrated with random walk based heterogeneous network embedding methods, which leads to heterogeneous graph convolutional networks like MAGNN [11] and HAN [50]. MANE [25] successfully employs network embedding on multi-layered networks and DMGC [29] accomplishes both network embedding and clustering by utilizing cross-layer links as regularization. In addition, many embedding algorithms for knowledge graph have been proposed such as translational distance model [3] and semantic matching model [32].

\begin{table}
\begin{tabular}{c|c c c c c c c} \hline \hline Models & Cora & Citeseer & NS & Cele \\ \hline node2vec[14] & 19.78\(\pm\)0.84 & 18.50\(\pm\)1.30 & 59.88\(\pm\)2.68 & 9.43\(\pm\)2.41 \\ VGAE [22] & 8.19\(\pm\)1.62 & 8.93\(\pm\)0.91 & 37.26\(\pm\)2.36 & 9.34\(\pm\)1.32 \\ GAT [43] & 5.77\(\pm\)0.64 & 5.99\(\pm\)2.25 & 32.96\(\pm\)

**Sampling in Network Embedding.** Sampling is an important technique, which appears in recommendation [34] and text embedding [30] to speed up the training process. For network embedding algorithms, both the positive sampling strategy and the negative sampling strategy have been applied and improved. For matrix factorization [26], spectral clustering [40] and LINE [41], direct or two-hop neighboring nodes act as positive samples and negative nodes are sampled uniformly or according to the degree distribution. In DeepWalk [36], node2vec [14] and metapath2vec [7], the positive sampling strategy is modified based on truncated random walk starting from the central node. The positive and negative sampling are implemented implicitly with a message-passing mechanism in GCN-based methods, where nodes to be aggregated can be viewed as positive samples and the remaining nodes are implicit negative samples. For the GCN related works, efforts are mainly made to improve the positive sampling strategy. For example, APPNP [13] manipulates positive samples with personalized pagerank [33] to catch long distance information and PGNN [58] randomly fixes an anchor node set for aggregation in each training epoch. Recently, more attention has been paid to improve the negative sampling strategy. Many works suggest to sample closer nodes as _difficult_ negative samples. For instance, MCNS [55] proposes that the negative sampling distribution should be positively correlated with the positive sampling distribution. KBGAN [4] samples most similar entities to replace the groundtruth positive entity in knowledge graph embedding and RecNS [56] tends to sample negative nodes from the intermediate distance region.

## 6 Conclusion and Limitations

In this paper, we study the sampling strategies of network embedding. To uncover the underlying relationships of existing competing sampling strategies, we conduct theoretical analysis on the sampling-based network embedding process. In the analysis, we identify two desirable properties for the similarity scores of node embedding, including the _discrimination_ property and the _monotonicity_ property. Furthermore, we prove that there always exists an error gap between the empirical and ideal optimal embedding similarity scores. Guided by such analysis, we propose a simple yet novel model (Sensei), which creatively integrates the two competing sampling strategies to fulfill the the _discrimination_ property and the _partial monotonicity_ property. The effectiveness of Sensei is verified by extensive experiments.

This paper studies sampling strategies of network embedding, which has no negative ethical impacts on society. The limitations of our paper lie in that the theoretical analysis is conducted on proximity scores and the proposed Sensei model is designed for plain (non-attributed) networks rather than attributed networks. The key to generalize the proposed properties/Sensei model to attributed networks is a new definition of the node pair similarity, which we leave for future exploration.

## 7 Acknowledgement

This work is supported by NSF (1947135, 2134079, 2316233, 1939725, and 2324770), DARPA (HR001121C0165), NIFA (2020-67021-32799), DHS (17STQAC00001-07-00), ARO (W911NF2110088). The content of the information in this document does not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.

## References

* [1] Yikun Ban, Yuchen Yan, Arindam Banerjee, and Jingrui He. Ee-net: Exploitation-exploration neural networks in contextual bandits. _arXiv preprint arXiv:2110.03177_, 2021.

Figure 3: Hit@\(K\) for Sensei.

* [2] Yikun Ban, Yuchen Yan, Arindam Banerjee, and Jingrui He. Neural exploitation and exploration of contextual bandits. _arXiv preprint arXiv:2305.03784_, 2023.
* [3] Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. _Advances in neural information processing systems_, 26, 2013.
* [4] Liwei Cai and William Yang Wang. Kbgan: Adversarial learning for knowledge graph embeddings. _arXiv preprint arXiv:1711.04071_, 2017.
* [5] Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-gcn: An efficient algorithm for training deep and large graph convolutional networks. In _Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining_, pages 257-266, 2019.
* [6] Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized pagerank graph neural network. _arXiv preprint arXiv:2006.07988_, 2020.
* [7] Yuxiao Dong, Nitesh V Chawla, and Ananthram Swami. metapath2vec: Scalable representation learning for heterogeneous networks. In _KDD '17_, pages 135-144. ACM, 2017.
* [8] Boxin Du, Si Zhang, Yuchen Yan, and Hanghang Tong. New frontiers of multi-network mining: Recent developments and future trend. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, pages 4038-4039, 2021.
* [9] Dongqi Fu, Liri Fang, Ross Maciejewski, Vetle I Torvik, and Jingrui He. Meta-learned metrics over multi-evolution temporal graphs. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 367-377, 2022.
* [10] Tao-yang Fu, Wang-Chien Lee, and Zhen Lei. Hin2vec: Explore meta-paths in heterogeneous information networks for representation learning. In _Proceedings of the 2017 ACM on Conference on Information and Knowledge Management_, pages 1797-1806, 2017.
* [11] Xinyu Fu, Jiani Zhang, Ziqiao Meng, and Irwin King. Magnn: Metapath aggregated graph neural network for heterogeneous graph embedding. In _Proceedings of The Web Conference 2020_, pages 2331-2341, 2020.
* [12] Hongchang Gao and Heng Huang. Self-paced network embedding. In _Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 1406-1415, 2018.
* [13] Johannes Gasteiger, Aleksandar Bojchevski, and Stephan Gunnemann. Predict then propagate: Graph neural networks meet personalized pagerank. _arXiv preprint arXiv:1810.05997_, 2018.
* [14] Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In _Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 855-864, 2016.
* [15] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. _Advances in neural information processing systems_, 30, 2017.
* [16] Xiaotian Han, Zhimeng Jiang, Ninghao Liu, and Xia Hu. G-mixup: Graph data augmentation for graph classification. In _International Conference on Machine Learning_, pages 8230-8248. PMLR, 2022.
* [17] Zexi Huang, Arlei Silva, and Ambuj Singh. A broader picture of random-walk based graph embedding. In _Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining_, pages 685-695, 2021.
* [18] Baoyu Jing, Hanghang Tong, and Yada Zhu. Network of tensor time series. In _Proceedings of the Web Conference 2021_, pages 2425-2437, 2021.
* [19] Baoyu Jing, Yuchen Yan, Kaize Ding, Chanyoung Park, Yada Zhu, Huan Liu, and Hanghang Tong. Sterling: Synergistic representation learning on bipartite graphs. _arXiv preprint arXiv:2302.05428_, 2023.
* [20] Baoyu Jing, Yuchen Yan, Yada Zhu, and Hanghang Tong. Coin: Co-cluster infomax for bipartite graphs. _arXiv preprint arXiv:2206.00006_, 2022.
* [21] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. _arXiv preprint arXiv:1609.02907_, 2016.

* [22] Thomas N Kipf and Max Welling. Variational graph auto-encoders. _arXiv preprint arXiv:1611.07308_, 2016.
* [23] Johannes Klicpera, Aleksandar Bojchevski, and Stephan Gunnemann. Predict then propagate: Graph neural networks meet personalized pagerank. _arXiv preprint arXiv:1810.05997_, 2018.
* [24] Omer Levy and Yoav Goldberg. Neural word embedding as implicit matrix factorization. _Advances in neural information processing systems_, 27, 2014.
* [25] Jundong Li, Chen Chen, Hanghang Tong, and Huan Liu. Multi-layered network embedding. In _Proceedings of the 2018 SIAM International Conference on Data Mining_, pages 684-692. SIAM, 2018.
* [26] Chih-Jen Lin. Projected gradient methods for nonnegative matrix factorization. _Neural computation_, 19(10):2756-2779, 2007.
* [27] Hongyi Ling, Zhimeng Jiang, Meng Liu, Shuiwang Ji, and Na Zou. Graph mixup with soft alignments. _arXiv preprint arXiv:2306.06788_, 2023.
* [28] Lihui Liu, Boxin Du, Jiejun Xu, Yinglong Xia, and Hanghang Tong. Joint knowledge graph completion and question answering. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 1098-1108, 2022.
* [29] Dongsheng Luo, Jingchao Ni, Suhang Wang, Yuchen Bian, Xiong Yu, and Xiang Zhang. Deep multi-graph clustering via attentive cross-graph association. In _Proceedings of the 13th international conference on web search and data mining_, pages 393-401, 2020.
* [30] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations of words and phrases and their compositionality. _Advances in neural information processing systems_, 26, 2013.
* [31] Mark EJ Newman. Finding community structure in networks using the eigenvectors of matrices. _Physical review E_, 74(3):036104, 2006.
* [32] Maximilian Nickel, Volker Tresp, and Hans-Peter Kriegel. A three-way model for collective learning on multi-relational data. In _Icml_, 2011.
* [33] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation ranking: Bringing order to the web. Technical report, Stanford InfoLab, 1999.
* [34] Rong Pan, Yunhong Zhou, Bin Cao, Nathan N Liu, Rajan Lukose, Martin Scholz, and Qiang Yang. One-class collaborative filtering. In _2008 Eighth IEEE International Conference on Data Mining_, pages 502-511. IEEE, 2008.
* [35] Shirui Pan, Ruiqi Hu, Guodong Long, Jing Jiang, Lina Yao, and Chengqi Zhang. Adversarially regularized graph autoencoder for graph embedding. _arXiv preprint arXiv:1802.04407_, 2018.
* [36] Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social representations. In _Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 701-710, 2014.
* [37] Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, and Jie Tang. Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec. In _Proceedings of the eleventh ACM international conference on web search and data mining_, pages 459-467, 2018.
* [38] Shane Roach, Connie Ni, Alexei Kopylov, Tsai-Ching Lu, Jiejun Xu, Si Zhang, Boxin Du, Dawei Zhou, Jun Wu, Lihui Liu, et al. Canon: Complex analytics of network of networks for modeling adversarial activities. In _2020 IEEE International Conference on Big Data (Big Data)_, pages 1634-1643. IEEE, 2020.
* [39] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. _AI magazine_, 29(3):93-93, 2008.
* [40] Jianbo Shi and Jitendra Malik. Normalized cuts and image segmentation. _IEEE Transactions on pattern analysis and machine intelligence_, 22(8):888-905, 2000.
* [41] Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale information network embedding. In _Proceedings of the 24th international conference on world wide web_, pages 1067-1077, 2015.
* [42] Yuxin Tang, Zhimin Ding, Dimitrije Jankov, Binhang Yuan, Daniel Bourgeois, and Chris Jermaine. Auto-differentiation of relational computations for very large scale machine learning. _arXiv preprint arXiv:2306.00088_, 2023.

* [43] Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. _arXiv preprint arXiv:1710.10903_, 2017.
* [44] Dingsu Wang, Yuchen Yan, Ruizhong Qiu, Yada Zhu, Kaiyu Guan, Andrew Margenot, and Hanghang Tong. Networked time series imputation via position-aware graph enhanced variational autoencoders. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 2256-2268, 2023.
* [45] Ruijie Wang, Baoyu Li, Yichen Lu, Dachun Sun, Jinning Li, Yuchen Yan, Shengzhong Liu, Hanghang Tong, and Tarek F Abdelzaher. Noisy positive-unlabeled learning with self-training for speculative knowledge graph reasoning. _arXiv preprint arXiv:2306.07512_, 2023.
* [46] Ruijie Wang, Zheng Li, Dachun Sun, Shengzhong Liu, Jinning Li, Bing Yin, and Tarek Abdelzaher. Learning to sample and aggregate: Few-shot reasoning over temporal knowledge graphs. _Advances in Neural Information Processing Systems_, 35:16863-16876, 2022.
* [47] Ruijie Wang, Zheng Li, Jingfeng Yang, Tianyu Cao, Chao Zhang, Bing Yin, and Tarek Abdelzaher. Mutually-paced knowledge distillation for cross-lingual temporal knowledge graph reasoning. In _Proceedings of the ACM Web Conference 2023_, pages 2621-2632, 2023.
* [48] Ruijie Wang, Zheng Li, Danqing Zhang, Qingyu Yin, Tong Zhao, Bing Yin, and Tarek Abdelzaher. Rete: Retrieval-enhanced temporal event forecasting on unified query product evolutionary graph. _arXiv preprint arXiv:2202.06129_, 2022.
* [49] Ruijie Wang, Yuchen Yan, Jialu Wang, Yuting Jia, Ye Zhang, Weinan Zhang, and Xinbing Wang. Acekg: A large-scale knowledge graph for academic data mining. In _Proceedings of the 27th ACM international conference on information and knowledge management_, pages 1487-1490, 2018.
* [50] Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S Yu. Heterogeneous graph attention network. In _The World Wide Web Conference_, pages 2022-2032, 2019.
* [51] Duncan J Watts and Steven H Strogatz. Collective dynamics of'small-world'networks. _nature_, 393(6684):440-442, 1998.
* [52] Yuchen Yan, Lihui Liu, Yikun Ban, Baoyu Jing, and Hanghang Tong. Dynamic knowledge graph alignment. In _Proceedings of the AAAI conference on artificial intelligence_, volume 35, pages 4564-4572, 2021.
* [53] Yuchen Yan, Si Zhang, and Hanghang Tong. Bright: A bridging algorithm for network alignment. In _Proceedings of the Web Conference 2021_, pages 3907-3917, 2021.
* [54] Yuchen Yan, Qinghai Zhou, Jinning Li, Tarek Abdelzaher, and Hanghang Tong. Dissecting cross-layer dependency inference on multi-layered inter-dependent networks. In _Proceedings of the 31st ACM International Conference on Information & Knowledge Management_, pages 2341-2351, 2022.
* [55] Zhen Yang, Ming Ding, Chang Zhou, Hongxia Yang, Jingren Zhou, and Jie Tang. Understanding negative sampling in graph representation learning. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 1666-1676, 2020.
* [56] Zhen Yang, Ming Ding, Xu Zou, Jie Tang, Bin Xu, Chang Zhou, and Hongxia Yang. Region or global a principle for negative sampling in graph-based recommendation. _IEEE Transactions on Knowledge and Data Engineering_, pages 1-1, 2022.
* [57] Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with graph embeddings. In _International conference on machine learning_, pages 40-48. PMLR, 2016.
* [58] Jiaxuan You, Rex Ying, and Jure Leskovec. Position-aware graph neural networks. In _International Conference on Machine Learning_, pages 7134-7143. PMLR, 2019.
* [59] Zhichen Zeng, Boxin Du, Si Zhang, Yinglong Xia, Zhining Liu, and Hanghang Tong. Hierarchical multi-marginal optimal transport for network alignment. _arXiv preprint arXiv:2310.04470_, 2023.
* [60] Zhichen Zeng, Si Zhang, Yinglong Xia, and Hanghang Tong. Parrot: Position-aware regularized optimal transport for network alignment. In _Proceedings of the ACM Web Conference 2023_, pages 372-382, 2023.

* [61] Qinghai Zhou, Liangyue Li, Nan Cao, Lei Ying, and Hanghang Tong. Admiring: Adversarial multi-network mining. In _2019 IEEE International Conference on Data Mining (ICDM)_, pages 1522-1527. IEEE, 2019.
* [62] Qinghai Zhou, Liangyue Li, Xintao Wu, Nan Cao, Lei Ying, and Hanghang Tong. Attent: Active attributed network alignment. In _Proceedings of the Web Conference 2021_, pages 3896-3906, 2021.
* [63] Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond homophily in graph neural networks: Current limitations and effective designs. _Advances in Neural Information Processing Systems_, 33:7793-7804, 2020.

Appendix

The appendix is structured as follows:

* Subsection A.1 gives the proof of Theorem 2;
* Subsection A.2 gives the detailed algorithm of Sensei;
* Subsection A.3 introduces the statistics of datasets and the implementation details of Sensei;
* Subsection A.4 compares the performance of Sensei with 5 additional GCN baselines (Cluster-gcn, GraphSAGE, APPNP, GPRGNN and H2GCN) on Cora and Citeseer.

### Proofs

**Theorem 2**.: _Mean Squared Error of \(S\) between the Ideal Loss and the Empirical Loss. For the mean squared error between \(S^{*}\) and \(S_{e}\), we have_

\[\begin{split}&\mathbb{E}[||(S_{e}-S^{*})_{u}||^{2}]=\\ &\frac{1}{T(2+\frac{kp_{n}(u|v)}{p(u|v)}+\frac{p(u|v)}{kp_{n}(u|v )})^{2}}(\frac{1}{p(u|v)}+\frac{1}{kp_{n}(u|v)}-1-\frac{1}{k})\end{split} \tag{13}\]

_where \(\mathbb{E}\) is the expectation and \(S^{*}=[\frac{p(u_{1}|v)}{p(u_{1}|v)+kp_{n}(u_{1}|v)},\ldots,\frac{p(u_{n}|v)}{ p(u_{n}|v)+kp_{n}(u_{n}|v)}]\)._

Proof.: In this proof, since \(S=[s(u_{1},v),s(u_{2},v),\ldots,s(u_{n},v)]\) is the parameter to be optimized, we can write the empirical loss \(J_{e}\) as a function of \(S\) as \(J_{e}(S)\). We prove this theorem with the help of Taylor expansion of \(\nabla_{S}J_{e}(S_{e})\) around \(S^{*}\). Because \(S_{e}\) is the solution to minimize \(J_{e}\), \(\nabla_{S}J_{e}(S_{e})=\mathbf{0}\). So, it can be expressed as:

\[\nabla_{S}J_{e}(S_{e})=\nabla_{S}J_{e}(S^{*})+\nabla_{S}^{2}J_{e}(S^{*})(S_{e} -S^{*})+O(||S_{e}-S^{*}||^{2})=\mathbf{0} \tag{14}\]

Thus, up to terms of order \(O(||S_{e}-S^{*}||^{2})\), we obtain

\[S_{e}-S^{*}=-(\nabla_{S}^{2}J_{e}(S^{*}))^{-1}\nabla_{S}J_{e}(S^{*}) \tag{15}\]

Then, we discuss the term \(-(\nabla_{S}^{2}J_{e}(S^{*}))^{-1}\) and the term \(\nabla_{S}J_{e}(S^{*})\) respectively.

For the term \(-(\nabla_{S}^{2}J_{e}(S^{*}))^{-1}\), let \(\mathbf{e}_{(u_{i})}\) be the one-hot vector, where only the \(u_{i}\)-th entry is 1.

\[\begin{split} J_{e}(S)&=-\frac{1}{T}\sum_{i=1}^{T} \log(s(u_{i},v))-\frac{1}{T}\sum_{i=1}^{kT}\log(1-s(w_{i},v))\\ \nabla_{S}J_{e}(S)&=-\frac{1}{T}\sum_{i=1}^{T}\frac{ 1}{s(u_{i},v)}\mathbf{e}_{(u_{i})}-\frac{1}{T}\sum_{i=1}^{kT}\frac{1}{s(w_{i}, v)-1}\mathbf{e}_{(w_{i})}\\ \nabla_{S}^{2}J_{e}(S)&=\frac{1}{T}\sum_{i=1}^{T} \frac{1}{s(u_{i},v)^{2}}\mathbf{e}_{(u_{i})}\mathbf{e}_{(u_{i})}^{\top}+\frac{ 1}{T}\sum_{i=1}^{kT}\frac{1}{(s(w_{i},v)-1)^{2}}\mathbf{e}_{(w_{i})}\mathbf{e }_{(w_{i})}^{\top

[MISSING_PAGE_FAIL:16]

Because \(s^{*}(u_{i},v)=\frac{p(u_{i}|v)}{p(u_{i}|v)+kp_{n}(u_{i}|v)}\), \(s^{*}(u_{j},v)=\frac{p(u_{j}|v)}{p(u_{j}|v)+kp_{n}(u_{j}|v)}\), \(s^{*}(w_{i},v)=\frac{p(w_{i}|v)}{p(w_{i}|v)+kp_{n}(u_{i}|v)}\) and \(s^{*}(w_{j},v)=\frac{p(u_{j}|v)}{p(w_{i}|v)+kp_{n}(u_{j}|v)}\), Eq. (21) becomes:

\[\begin{split}&\text{Cov}(\nabla_{S}J_{e}(S^{*}))=\frac{1}{T}\sum_{u} \frac{(p(u|v)+kp_{n}(u|v))^{3}}{kp_{n}(u|v)p(u|v)}\mathbf{e}_{(u)}\mathbf{e}_{ (u)}^{\top}\\ &+\sum_{u_{i},u_{j}}(1-\frac{1}{T})(p(u_{i}|v)+kp_{n}(u_{i}|v))(p (u_{j}|v)+kp_{n}(u_{j}|v))\mathbf{e}_{(u_{i})}\mathbf{e}_{(u_{j})}^{\top}\\ &-\sum_{u_{i},w_{j}}(p(u_{i}|v)+kp_{n}(u_{i}|v))(p(w_{j}|v)+kp_{n} (w_{j}|v))\mathbf{e}_{(u_{i})}\mathbf{e}_{(u_{j})}^{\top}\\ &-\sum_{u_{j},w_{i}}(p(u_{j}|v)+kp_{n}(u_{j}|v))(p(w_{i}|v)+kp_{n} (w_{i}|v))\mathbf{e}_{(w_{i})}\mathbf{e}_{(u_{j})}^{\top}\\ &+\sum_{w_{i},w_{j}}(1-\frac{1}{kT})(p(w_{i}|v)+kp_{n}(w_{i}|v))(p (w_{j}|v)+kp_{n}(w_{j}|v))\mathbf{e}_{(w_{i})}\mathbf{e}_{(w_{j})}^{\top}\\ &=\frac{1}{T}\sum_{u}\frac{(p(u|v)+kp_{n}(u|v))^{3}}{kp_{n}(u|v)p(u |v)}\mathbf{e}_{(u)}\mathbf{e}_{(u)}^{\top}\\ &-(1+\frac{1}{k})\frac{1}{T}\sum_{u_{i},u_{j}}(p(u_{i}|v)+kp_{n} (u_{i}|v))(p(u_{j}|v)+kp_{n}(u_{j}|v))\mathbf{e}_{(u_{i})}\mathbf{e}_{(u_{j})} ^{\top}\end{split} \tag{22}\]

Based on Eq. (15), Eq. (18), Eq. (22) and \(\mathbb{E}[\|(S_{e}-S^{*})_{u}\|^{2}]=\text{Cov}(S_{e}-S^{*})(u,u)\), we have

\[\begin{split}&\mathbb{E}[\|(S_{e}-S^{*})_{u}\|^{2}]=\\ &\frac{1}{T}\frac{kp_{n}(u|v)p(u|v)}{(p(u|v)+kp_{n}(u|v))^{3}} \left(\frac{(p(u|v)+kp_{n}(u|v))^{3}}{kp_{n}(u|v)p(u|v)}\right.\\ &\left.-(1+\frac{1}{k})(p(u|v)+kp_{n}(u|v))^{2}\right)\frac{kp_{n }(u|v)p(u|v)}{(p(u|v)+kp_{n}(u|v))^{3}}\end{split} \tag{23}\]

which can be simplified as:

\[\begin{split}&\mathbb{E}[\|(S_{e}-S^{*})_{u}\|^{2}]\\ &=\frac{1}{T}\frac{(kp_{n}(u|v)p(u|v))^{2}}{(p(u|v)+kp_{n}(u|v))^{ 4}}(\frac{kp_{n}(u|v)+p(u|v)}{kp_{n}(u|v)p(u|v)}-1-\frac{1}{k})\\ &=\frac{1}{T(2+\frac{kp_{n}(u|v)}{p(u|v)}+\frac{p(u|v)}{kp_{n}(u| v)})^{2}}(\frac{1}{p(u|v)}+\frac{1}{kp_{n}(u|v)}-1-\frac{1}{k})\end{split} \tag{24}\]

### The Algorithm of Sensei

In this section, we give the detailed algorithm of Sensei in Algorithm 1.

### Dataset Statistics and Implementation Details.

In this section, we introduce the statstics of datasets and the implementation details of Sensei. The statistics of datasets are present in Table 3.

**Implementation Details.** For all the methods, we run 5 times to calculate the standard deviation and for all baselines, we use their default parameters. For Sensei on 4 datasets: {C.ele, Cora, Citeseer,

\begin{table}
\begin{tabular}{|c|c|c|c|} \hline Dataset & Layers & Nodes & Edges \\ \hline C.ele & 1 & 297 & 2,148 \\ Cora & 1 & 2,708 & 5,429 \\ Citeseer & 1 & 3,327 & 4,732 \\ NS & 1 & 1,589 & 2,742 \\ \hline \end{tabular}
\end{table}
Table 3: Dataset Statistics.

NS}, we set the threshold \(\tau\) as \(\{0.008,0.01,0.005,0.05\}\), the number of epochs in Step 1 as {40, 100, 20, 20}, the number of epochs in Step 2 as {40, 100, 50, 20}, the learning rate in Step 1 as {0.02, 0.1, 0.1, 0.2}, the learning rate in Step 2 as {0.01, 0.01, 0.005, 0.1}, the positive margin \(\gamma\) as {0.05, 0.0001, 0.0001, 0.1} and the negative sample number \(k\) as 40 on all 4 datasets. All experiments are run on a Tesla-V100 GPU. 3

Footnote 3: The simplified code of Sensei is on: [https://github.com/yucheny5/SENSEI](https://github.com/yucheny5/SENSEI).

### Additional Baselines of Plain Network Embedding

We have added another 5 GNNs as additional baselines: GraphSAGE [15], Cluster-gcn [5], APPNP [13], GPRGNN [6] and H2GCN [63]. the results are demonstrated in Table 4. We can see that for both link prediction and node recommendation, Sensei generally beats these GNN methods.

\begin{table}
\begin{tabular}{c|c c c|c c c} \hline \hline \multirow{2}{*}{Models} & \multicolumn{3}{c|}{Cora} & \multicolumn{3}{c}{Citeseer} \\ \cline{2-7}  & AUC-ROC & AUC-PR & Hit@10 & AUC-ROC & AUC-PR & Hit@10 \\ \hline GraphSAGE [15] & 64.13 & 65.36 & 3.92 & 66.10 & 68.94 & 4.74 \\ Cluster-gcn [5] & 65.93 & 72.39 & 4.65 & 67.33 & 70.45 & 5.42 \\ APPNP [13] & 68.77 & 72.39 & 11.72 & 66.45 & 71.57 & 11.96 \\ GPRGNN [6] & 64.43 & 70.15 & 13.31 & 62.93 & 68.69 & 10.64 \\ H2GCN [63] & 61.90 & 61.89 & 1.96 & 60.95 & 61.94 & 3.53 \\ \hline Sensei & **81.27** & **83.59** & **20.51** & **76.28** & **82.02** & **19.03** \\ \hline \hline \end{tabular}
\end{table}
Table 4: The AUC-ROC and AUC-PR of link prediction and Hit@10 of node recommendation on Cora and Citeseer(%).