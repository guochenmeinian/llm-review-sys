# Provable Benefits of Complex Parameterizations

for Structured State Space Models

 Yuval Ran-Milo\({}^{1}\)  Eden Lumbroso\({}^{1}\)  Edo Cohen-Karlik\({}^{1}\)

**Raja Giryes\({}^{1}\)  Amir Globerson\({}^{1}\) 11  Nadav Cohen\({}^{1}\)**

###### Abstract

Structured state space models (SSMs), the core engine behind prominent neural networks such as S4 and Mamba, are linear dynamical systems adhering to a specified structure, most notably diagonal. In contrast to typical neural network modules, whose parameterizations are real, SSMs often use complex parameterizations. Theoretically explaining the benefits of complex parameterizations for SSMs is an open problem. The current paper takes a step towards its resolution, by establishing formal gaps between real and complex diagonal SSMs. Firstly, we prove that while a moderate dimension suffices in order for a complex SSM to express all mappings of a real SSM, a much higher dimension is needed for a real SSM to express mappings of a complex SSM. Secondly, we prove that even if the dimension of a real SSM is high enough to express a given mapping, typically, doing so requires the parameters of the real SSM to hold exponentially large values, which cannot be learned in practice. In contrast, a complex SSM can express any given mapping with moderate parameter values. Experiments corroborate our theory, and suggest a potential extension of the theory that accounts for selectivity, a new architectural feature yielding state of the art performance.1

Footnote 1: Tel Aviv University 11Google. Correspondence to: Yuval Milo <yuvalmilo@mail.tau.ac.il>

Footnote 1: Due to lack of space, a portion of the paper is deferred to the appendices. We refer the reader to [44] for a self-contained version of the text.

## 1 Introduction

_Structured state space models_ (_SSMs_) are the core engine behind prominent neural network architectures such as S4 [21], Mamba [20], LRU [41], Mega [37], S5 [50] and more [23; 31; 38; 34]. In their typical form, SSMs can be thought of as single-input single-output linear dynamical systems, wherein the state transition matrix has a specified structure, most notably diagonal [22; 23; 41; 50; 37; 20]. A salient characteristic of SSMs is that their parameterizations are often _complex_ (take values in \(\mathbb{C}\)), in contrast to typical neural network modules whose parameterizations are conventionally real (take values in \(\mathbb{R}\)).

There has been mixed evidence regarding benefits of complex parameterizations over real parameterizations for SSMs. Some prior works have demonstrated that complex parameterizations are essential for strong performance [22; 41], whereas others have shown that in various settings real parameterizations lead to comparable (and in some cases better) performance [37; 20]. It was conjectured [20] that in the context of _diagonal SSMs_ (namely, SSMs with diagonal state transition matrix), complex parameterizations are preferable for continuous data modalities (_e.g._, audio, video), whereas for discrete data modalities (_e.g._, text, DNA) real parameterizations suffice. Unfortunately, to date, formal support for this conjecture is lacking. The extent to which complex parameterizations benefit diagonal SSMs remains to be an open question.

In this paper, we take a step towards theoretically addressing the foregoing question. Specifically, we provide two theoretical contributions establishing provable benefits of complex parameterizationsfor diagonal SSMs. Our first contribution establishes that, although both real and complex diagonal SSMs are _universal_--in the sense that both can precisely express any _linear time-invariant_ (_LTI_) mapping up to any time \(t\) when their dimensions are equal to or greater than \(t\)--there is a strong separation between the SSMs in terms of expressiveness. Namely, denoting the dimensions of the real and complex SSMs by \(n_{\mathbb{R}}\) and \(n_{\mathbb{C}}\), respectively, we prove that for any \(n_{\mathbb{C}}\), there are various oscillatory mappings expressible by the complex SSM which cannot be approximately expressed up to time \(t\) by the real SSM unless \(n_{\mathbb{R}}\) is on the order of \(t\), which may be arbitrarily larger than \(n_{\mathbb{C}}\). This is in stark contrast to the fact that all mappings expressible by the real SSM can be precisely expressed (up to any time) by the complex SSM whenever \(n_{\mathbb{C}}\geq n_{\mathbb{R}}\).

Given the prevalence of overparameterization in machine learning, one may question how consequential the above separation (between the real and complex SSMs) is in practice. Indeed, in an overparameterized regime where a given LTI mapping is to be approximated up to a given time \(t\) and \(n_{\mathbb{R}},n_{\mathbb{C}}\geq t\), universality implies that both the real and complex SSMs can precisely express the mapping up to time \(t\). Accordingly, in this overparameterized regime, it is a priori unclear whether the complex SSM offers an advantage over the real SSM. Our second contribution shows that it does. Specifically, we prove a surprising result by which, if the given mapping satisfies a mild condition, then in order to approximately express the mapping up to time \(t\), the real SSM must have dimension or parameter magnitude exponential in \(t\). This is in stark contrast to the complex SSM, which can precisely express the given mapping up to time \(t\) with dimension and parameter magnitudes that are at most linear in \(t\). The aforementioned mild condition is satisfied by the canonical copy mapping, by a basic oscillatory mapping, and with high probability by a random (generic) mapping. In such important cases, practical learning of the given mapping necessitates using a complex SSM.

Our theory is corroborated by controlled experiments, demonstrating that complex parameterizations for SSMs significantly improve performance. We also evaluate SSMs with _selectivity_--a new architectural feature yielding state of the art performance [20, 31, 4, 57]. Our experiments with selectivity portray a more nuanced picture: complex parameterizations are beneficial for some tasks, whereas for others, selectivity allows real parameterizations to achieve comparable (and in some cases better) performance. These findings align with the mixed evidence reported in the literature. Moreover, they suggest a potential extension of our theory that accounts for selectivity and may elucidate this evidence, thereby fully delineating the benefits of complex parameterizations for SSMs.

## 2 Preliminaries

### Notations

We use non-boldface lowercase letters for denoting scalars (_e.g._\(\alpha\in\mathbb{R}\), \(c\in\mathbb{C}\), \(n\in\mathbb{N}\)), boldface lowercase letters for denoting vectors (_e.g._\(\mathbf{x}\in\mathbb{R}^{n}\), \(\mathbf{v}\in\mathbb{C}^{n}\)), and non-boldface uppercase letters for denoting matrices (_e.g._\(A\in\mathbb{R}^{n,m}\), \(B\in\mathbb{C}^{n,m}\)). Series (finite or infinite) of scalars, vectors or matrices are viewed as functions of time and denoted accordingly (_e.g._\((\mathbf{x}(t)\in\mathbb{R}^{n})_{t\in\mathbb{N}}\), \((A(t)\in\mathbb{C}^{n,m})_{t=1,2,\ldots,k}\)). For series of scalars, we also use as notation boldface uppercase letters (_e.g._\(\mathbf{S}=(s(t)\in\mathbb{R})_{t\in\mathbb{N}}\), \(\mathbf{I}=(i(t)\in\mathbb{C})_{t=1,2,\ldots,k}\)). Given \(k\in\mathbb{N}\) and a series of scalars \(\mathbf{S}\) whose length is greater than or equal to \(k\), we use \(\mathbf{S}_{k}\) to denote the \(k\)th element of \(\mathbf{S}\), and \(\mathbf{S}_{:k}\) to denote the truncation of \(\mathbf{S}\) to length \(k\) (_i.e._ the series comprising the first \(k\) elements of \(\mathbf{S}\)), allowing ourselves to regard this truncated series as a vector of dimension \(k\). For \(k\in\mathbb{N}\cup\{\infty\}\), we use \([k]\) as shorthand for the set \(\{1,2,\ldots,k\}\). Given a complex number \(c\in\mathbb{C}\), we denote its magnitude by \(|c|\in\mathbb{R}_{\geq 0}\), its phase by \(\arg(c)\in[0,2\pi)\), its real part by \(\Re(c)\in\mathbb{R}\), and its imaginary part by \(\Im(c)\in\mathbb{R}\) (meaning \(c=|c|\exp(i\arg(c))=\Re(c)+i\Im(c)\)). We let \(\mathbf{0}\) and \(\mathbf{1}\) stand for vectors whose entries are all zeros and all ones, respectively, with dimension to be inferred from context. The Hadamard (element-wise) product, defined between vectors or matrices of the same size, and between scalar series of the same length, is denoted by \(\odot\). The convolution operator, defined between two (finite or infinite) scalar series, is denoted by \(*\). Namely, given two scalar series \(\mathbf{S}=(s(t))_{t\in[k]},\mathbf{S}=(\bar{s}(t))_{t\in[\bar{k}]}\) of lengths \(k,\bar{k}\in\mathbb{N}\cup\{\infty\}\) respectively, \(\mathbf{S}*\bar{\mathbf{S}}\) is the scalar series of length \(k+\bar{k}-1\) whose \(m\)th element, for \(m\in[k+\bar{k}-1]\), is given by \(\sum_{t=\max\{m-\bar{k}+1,1\}}^{\min\{m,k\}}s(t)\bar{s}

constrained to be diagonal); \(B\in\mathbb{K}^{n,1}\), an _input matrix_; and \(C\in\mathbb{K}^{1,n}\), an _output matrix_.23 Given the values of \(A\), \(B\) and \(C\), the SSM realizes a mapping \(\phi_{n,(A,B,C)}:\mathbb{R}^{\mathbb{N}}\to\mathbb{R}^{\mathbb{N}}\) which receives as input a real scalar series \((u(t))_{t\in\mathbb{N}}\), and produces as output a real scalar series \((y(t))_{t\in\mathbb{N}}\) defined through the following recursive formula:

Footnote 2: Various SSMs include _discretization_[20, 21, 23, 16], which amounts to replacing parameter matrices by certain transformations that depend on an additional parameter \(\Delta\in\mathbb{R}_{>0}\) (_e.g._, replacing \(A\) by \((I-\Delta/2\cdot A)^{-1}(I+\Delta/2\cdot A)\)[23, 22]). With slight modifications, our main theoretical results apply to SSMs with common discretizations—see Appendix A.1 for details.

Footnote 3: Some SSMs include an additional _feedthrough_ parameter \(D\in\mathbb{K}^{1,1}\)[41]. With feedthrough, the expression for \(y(t)\) in Equation (1) becomes \(\Re(C\mathbf{x}(t)+Du(t))\). Our theory essentially applies as is to SSMs with feedthrough—see Appendix A.2 for details.

\[\mathbf{x}(t)=A\mathbf{x}(t-1)+Bu(t)\enspace,\enspace y(t)=\Re\big{(}C\mathbf{ x}(t)\big{)}\enspace,\enspace t\in\mathbb{N}\,, \tag{1}\]

where \((\mathbf{x}(t)\in\mathbb{K}^{n})_{t\in\mathbb{N}}\) is a vector series of _states_, and \(\mathbf{x}(0)=\mathbf{0}\in\mathbb{K}^{n}\). If \(\mathbb{K}=\mathbb{R}\) we say that the SSM is _real_, and if \(\mathbb{K}=\mathbb{C}\) we say that it is _complex_.4 We refer to the SSM as _stable_ if all eigenvalues of \(A\) have magnitude strictly smaller than one; otherwise we refer to the SSM as _unstable_. For convenience, we often identify an SSM with the triplet \((A,B,C)\) holding its parameter matrices, and regard the (single column) matrices \(B\) and \(C^{\top}\) as vectors.

Footnote 4: It is possible to consider a hybrid setting where \(A\) is allowed to be complex while \(B\) and \(C\) are restricted to be real. This setting enjoys all provable benefits of the complex setting (\(\mathbb{K}=\mathbb{C}\))—see Appendix A.3 for details.

Perhaps the most prominent form of structure imposed on SSMs is _stable diagonality_, _i.e._ stability combined with diagonality [23, 41, 37, 22, 20]. Accordingly, unless stated otherwise, we assume that the state transition matrix \(A\) of an SSM is diagonal and has entries with magnitude strictly smaller than one.

### Linear Time-Invariant Mappings

Let \(\phi:\mathbb{R}^{\mathbb{N}}\to\mathbb{R}^{\mathbb{N}}\) be a mapping from the space of (infinite) real scalar series to itself. We say that \(\phi(\cdot)\) is _linear_ if for all \(\alpha\in\mathbb{R}\) and \(\mathbf{S},\mathbf{S}\in\mathbb{R}^{\mathbb{N}}\) it holds that \(\phi(\alpha\mathbf{S}+\mathbf{\bar{S}})=\alpha\phi(\mathbf{S})+\phi(\mathbf{ \bar{S}})\). For every \(k\in\mathbb{N}\), define the _\(k\) step delay_\(\delta_{k}:\mathbb{R}^{\mathbb{N}}\to\mathbb{R}^{\mathbb{N}}\) to be the operator that adds \(k\) preceding zeros to the series it receives as input.5 We say that the mapping \(\phi(\cdot)\) is _linear time-invariant_ (_LTI_) if it is linear, and it commutes with \(\delta_{k}(\cdot)\) (meaning \(\phi(\delta_{k}(\cdot))=\delta_{k}(\phi(\cdot))\)) for every \(k\in\mathbb{N}\). It is well known [40] that if \(\phi(\cdot)\) is LTI then it is given by \(\phi(\mathbf{S})=\mathbf{S}*\phi(\mathbf{I})\), where \(\mathbf{I}:=(1,0,0,\ldots)\in\mathbb{R}^{\mathbb{N}}\) is the _impulse_ series, and \(\phi(\mathbf{I})\) is referred to as the _impulse response_ of \(\phi(\cdot)\). Conversely, for any \(\mathbf{R}\in\mathbb{R}^{\mathbb{N}}\), the mapping defined by \(\mathbf{S}\mapsto\mathbf{S}*\mathbf{R}\) is LTI, and its impulse response is \(\mathbf{R}\).

Footnote 5: That is, for any \(\mathbf{S}=(s(t))_{t\in\mathbb{N}}\in\mathbb{R}^{\mathbb{N}}\), \(\delta_{k}(\mathbf{S})\in\mathbb{R}^{\mathbb{N}}\) is the series whose \(m\)th element, for \(m\in\mathbb{N}\), equals \(s(m-k)\) if \(m>k\) and \(0\) otherwise.

We will identify LTI mappings with their impulse responses. More specifically, for any \(k\in\mathbb{N}\), we identify an LTI mapping up to time \(k\), with the truncation of its impulse response to length \(k\). Accordingly, for any LTI mappings \(\phi(\cdot),\phi(\cdot)\) and any \(\epsilon\in\mathbb{R}_{\geq 0}\), we say that \(\bar{\phi}(\cdot)\)\(\epsilon\)_-approximates_\(\phi(\cdot)\)_up to time \(k\)_ if \(\|\phi(\mathbf{I})_{:k}-\bar{\phi}(\mathbf{I})_{:k}\|_{1}\leq\epsilon\). If the latter inequality holds with \(\epsilon=0\), we also say that \(\bar{\phi}(\cdot)\)_matches_\(\phi(\cdot)\)_up to time \(k\)_.

Let \((A,B,C)\) be an SSM of dimension \(n\in\mathbb{N}\), realizing the mapping \(\phi_{n,(A,B,C)}:\mathbb{R}^{\mathbb{N}}\to\mathbb{R}^{\mathbb{N}}\) (see Section 2.2). It is straightforward to see that \(\phi_{n,(A,B,C)}(\cdot)\) is LTI, and that its impulse response is given by:

\[\phi_{n,(A,B,C)}(\mathbf{I})=\big{(}\Re(CB),\Re(CAB),\Re(CA^{2}B),\ldots\big{)}\,. \tag{2}\]

For real and complex settings (_i.e._ for \(\mathbb{K}=\mathbb{R}\) and \(\mathbb{K}=\mathbb{C}\)), we will study the extent to which varying \((A,B,C)\) as well as \(n\), can lead \(\phi_{n,(A,B,C)}(\cdot)\) to \(\epsilon\)-approximate different LTI mappings up to different times.

## 3 Theoretical Analysis

Throughout this section, we consider a real SSM \((A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})\) of dimension \(n_{\mathbb{R}}\) realizing the mapping \(\phi_{n_{\mathbb{R}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}(\cdot)\), and a complex SSM \((A_{\mathbb{C}},B_{\mathbb{C}},C_{\mathbb{C}})\) of dimension \(n_{\mathbb{C}}\) realizing the mapping \(\phi_{n_{\mathbb{C}},(A_{\mathbb{C}},B_{\mathbb{C}},C_{\mathbb{C}})}(\cdot)\) (see Section 2.2).

### Universality

It is known (see, _e.g._, [14]) that the real SSM is _universal_, in the sense that it can precisely express any LTI mapping up to any time \(t\) when its dimension is equal to or greater than \(t\). Trivially, this implies the same for the complex SSM. Proposition 1 below formalizes these facts for completeness.

**Proposition 1**.: _Let \(\phi:\mathbb{R}^{\mathbb{N}}\to\mathbb{R}^{\mathbb{N}}\) be an arbitrary LTI mapping, and let \(t\in\mathbb{N}\). Then, the following holds for both \(\mathbb{K}=\mathbb{R}\) and \(\mathbb{K}=\mathbb{C}\). If \(n_{\mathbb{K}}\geq t\), there exist assignments for \((A_{\mathbb{K}},B_{\mathbb{K}},C_{\mathbb{K}})\) with which \(\phi_{n_{\mathbb{K}},(A_{\mathbb{K}},B_{\mathbb{K}},C_{\mathbb{K}})}(\cdot)\) matches \(\phi(\cdot)\) up to time \(t\).6_

Footnote 6: It is possible to strengthen this result, _e.g._, by showing that the requirement \(n_{\mathbb{K}}\geq t\) can be replaced by \(n_{\mathbb{K}}\geq\lceil(t+1)/2\rceil\) when \(\mathbb{K}=\mathbb{C}\). We omit details, as the current form of the result suffices for our purposes.

Proof sketch (proof in Appendix D.1).: Beginning with the real SSM (\(\mathbb{K}=\mathbb{R}\)), the proof shows that \(\phi_{n_{\mathbb{K}},(A_{\mathbb{R}},B_{\mathbb{K}},C_{\mathbb{K}})}(\cdot)\) matches \(\phi(\cdot)\) up to time \(t\) if \(V(A_{\mathbb{R}})(C_{\mathbb{R}}^{\top}\odot B_{\mathbb{R}})=\phi(\mathbf{I}) _{:t}\), where \(V(A_{\mathbb{R}})\) is a Vandermonde matrix that has full rank when the diagonal entries of \(A_{\mathbb{R}}\) are distinct. Assigning \(A_{\mathbb{R}}\) this way, there must exist \(\mathbf{v}\in\mathbb{R}^{n_{\mathbb{K}}}\) with which \(V(A_{\mathbb{R}})\mathbf{v}=\phi(\mathbf{I})_{:t}\). Assigning \(B_{\mathbb{R}}=\mathbf{v}\) and \(C_{\mathbb{R}}^{\top}=\mathbf{1}\) concludes the proof for the real SSM. The complex SSM (\(\mathbb{K}=\mathbb{C}\)) can be treated analogously. 

### Separation in Expressiveness

Proposition 2 and Theorem 1 below together establish that although both the real and complex SSMs are universal (see Section 3.1), there is a strong separation between the two in terms of expressiveness. Proposition 2 formalizes an obvious fact: all mappings expressible by the real SSM can be precisely expressed (up to any time) by the complex SSM whenever \(n_{\mathbb{C}}\geq n_{\mathbb{R}}\) (_i.e._, whenever the dimension of the complex SSM is equal to or greater than the dimension of the real SSM). Theorem 1 proves a much less obvious result: for any \(n_{\mathbb{C}}\), there are various oscillatory mappings (_i.e._ mappings with oscillatory impulse responses) expressible by the complex SSM which cannot be approximately expressed up to time \(t\) by the real SSM unless \(n_{\mathbb{R}}\) is on the order of \(t\), which may be arbitrarily larger than \(n_{\mathbb{C}}\).

**Proposition 2**.: _Consider an arbitrary assignment for \((A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})\), and assume that \(n_{\mathbb{C}}\geq n_{\mathbb{R}}\). Then, there exist assignments for \((A_{\mathbb{C}},B_{\mathbb{C}},C_{\mathbb{C}})\) with which \(\phi_{n_{\mathbb{C}},(A_{\mathbb{C}},B_{\mathbb{C}},C_{\mathbb{C}})}(\cdot)= \phi_{n_{\mathbb{R}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}(\cdot)\)._

Proof.: It suffices to prove the sought after result for \(n_{\mathbb{C}}=n_{\mathbb{R}}\), since one can effectively reduce the dimension of the complex SSM by zeroing out entries of \(B_{\mathbb{C}}\) (or \(C_{\mathbb{C}}\)). Assuming that \(n_{\mathbb{C}}=n_{\mathbb{R}}\), we may assign to \((A_{\mathbb{C}},B_{\mathbb{C}},C_{\mathbb{C}})\) the values of \((A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})\). Under this assignment \(\phi_{n_{\mathbb{C}},(A_{\mathbb{C}},B_{\mathbb{C}},C_{\mathbb{C}})}(\cdot)= \phi_{n_{\mathbb{R}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}(\cdot)\), as required. 

**Theorem 1**.: _Let \(t\in\mathbb{N}\) and \(\epsilon\in\mathbb{R}_{\geq 0}\). Assume without loss of generality that \(n_{\mathbb{C}}=1\),7 in which case \(A_{\mathbb{C}}\), \(B_{\mathbb{C}}\) and \(C_{\mathbb{C}}\) can be regarded as scalars. Suppose \(|\mathrm{sin}(\arg(A_{\mathbb{C}}))|\geq 0.2\), \(|A_{\mathbb{C}}|\geq 0.5^{1/t}\) and \(|B_{\mathbb{C}}\cdot C_{\mathbb{C}}|\geq 1\). Then, if \(\phi_{n_{\mathbb{R}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}(\cdot)\)\(\epsilon\)-approximates \(\phi_{n_{\mathbb{C}},(A_{\mathbb{C}},B_{\mathbb{C}},C_{\mathbb{C}})}(\cdot)\) up to time \(t\), it must be that \(n_{\mathbb{R}}\geq\lfloor t/9\rfloor-1-4\epsilon\)._

Footnote 7: This does not limit generality since one can effectively reduce the dimension of the complex SSM by zeroing out entries of \(B_{\mathbb{C}}\) (or \(C_{\mathbb{C}}\)).

Proof sketch (proof in Appendix D.2).: The idea behind the proof is as follows. The complex SSM realizes an oscillatory mapping, in the sense that elements \(1,3,\ldots,2\lceil t/2\rceil-1\) of its impulse response alternate \(\Theta(t)\) times between being greater than or equal to \(1/4\), and being smaller than or equal to \(-1/4\). The real SSM on the other hand is limited in its ability to realize oscillations, insofar as elements \(1,3,\ldots,2\lceil t/2\rceil-1\) of the impulse response it gives rise to are a linear combination of decaying exponentials, and therefore can change sign at most \(\mathcal{O}(n_{\mathbb{R}})\) times. Combining these two observations leads to the desired result. 

### Separation in Practical Learnability

Let \(\phi:\mathbb{R}^{\mathbb{N}}\to\mathbb{R}^{\mathbb{N}}\) be an LTI mapping with bounded impulse response, which we would like to \(\epsilon\)-approximate up to time \(t\) for some \(\epsilon\in\mathbb{R}_{\geq 0}\) and \(t\in\mathbb{N}\). Assume that the dimensions of the real and complex SSMs are greater than or equal to \(t\). By Proposition 1, both the real and complex SSMs can express mappings that match \(\phi(\cdot)\) up to time \(t\), and in particular that achieve the desired approximation. The current subsection establishes that despite this parity in terms of expressiveness, there is a strong separation between the real and complex SSMs in terms of _practical learnability_.

Section 3.3.1 proves that under a mild condition on \(\phi(\cdot)\), in order for the real SSM to achieve the desired approximation, either its dimension or the magnitude of its parameters must be exponential in \(t\). Section 3.3.2 then explains that such exponentiality impedes practical learning via gradient descent. Finally, Section 3.3.3 proves that in stark contrast to the real SSM, the complex SSM can achieve the desired approximation with dimension and parameter magnitudes that are at most linear in \(t\).

#### 3.3.1 Real Parameterizations Suffer from Exponentiality

Definition 1 below formalizes the notion of _forward difference_ for a real scalar series--a discrete analogue of derivative for a differentiable real function. Our main theoretical result, Theorem 2, then establishes that if forward differences associated with \(\phi(\mathbf{I})\)--the impulse response of \(\phi(\cdot)\)--satisfy a certain condition, then in order for the real SSM to express a mapping that \(\epsilon\)-approximates \(\phi(\cdot)\) up to time \(t\), either the dimension of the real SSM \(n_{\mathbb{R}}\) or the magnitude of its parameters \((B_{\mathbb{R}},C_{\mathbb{R}})\) must be exponential in \(t\). Roughly speaking, the aforementioned condition on forward differences associated with \(\phi(\mathbf{I})\) is that there exists some \(d\in\Theta(t)\) such that the \(d\)th forward difference of the restriction of \(\phi(\mathbf{I})\) to either odd or even elements has magnitude greater than \(2^{d}\epsilon\). Perhaps surprisingly, this condition is especially mild, as the magnitude of the \(d\)th forward difference of a real scalar series typically scales exponentially with \(d\). Several important cases where the condition is satisfied are presented below.

**Definition 1**.: Let \(\mathbf{S}\) be a real scalar series of length \(k\in\mathbb{N}\cup\{\infty\}\). The _forward difference_ of \(\mathbf{S}\), denoted \(\mathbf{S}^{(1)}\), is the scalar series of length \(k-1\) whose \(m\)th element, for \(m\in[k-1]\), is given by \(\mathbf{S}_{m+1}-\mathbf{S}_{m}\). For \(d\in\{2,3,\ldots,k-1\}\), the _\(d\)th forward difference_ of \(\mathbf{S}\), denoted \(\mathbf{S}^{(d)}\), is recursively defined to be the forward difference of \(\mathbf{S}^{(d-1)}\).

**Theorem 2**.: _Suppose \(\phi_{n_{\mathbb{R}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}(\cdot)\)\(\epsilon\)-approximates \(\phi(\cdot)\) up to time \(t\). Then:_

\[n_{\mathbb{R}}\|C_{\mathbb{R}}{}^{\top}\odot B_{\mathbb{R}}\|_{\infty}\geq\max_{ \begin{subarray}{c}d,m\in\mathbb{N},d+m\leq\lfloor t/2\\ \sigma\in\{\text{odd, even}\}\end{subarray}}\left\{2^{d+2\min\{d,m\}}\left(2^ {-d}\big{|}(\phi(\mathbf{I})|_{\sigma})_{m}^{(d)}\big{|}-\epsilon\right)\right\}\,, \tag{3}\]

_where: \(\phi(\mathbf{I})|_{\text{odd}}\) and \(\phi(\mathbf{I})|_{even}\) are the restrictions of the impulse response \(\phi(\mathbf{I})\) to odd and even elements, respectively; and \((\phi(\mathbf{I})|_{\text{odd}})_{m}^{(d)}\) and \((\phi(\mathbf{I})|_{even})_{m}^{(d)}\) stand for the mth element of the \(d\)th forward difference of \(\phi(\mathbf{I})|_{\text{odd}}\) and \(\phi(\mathbf{I})|_{\text{even}}\), respectively._

Proof sketch (proof in Appendix D.3).: The idea behind the proof is as follows. The restrictions of the impulse response of \(\phi_{n_{\mathbb{R}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}(\cdot)\) to odd and even elements--_i.e._, \(\phi_{n_{\mathbb{R}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}(\mathbf{ I})|_{\text{odd}}\) and \(\phi_{n_{\mathbb{R}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}(\mathbf{ I})|_{\text{even}}\), respectively--are each a linear combination of \(n_{\mathbb{R}}\) decaying exponentials, where the coefficients of the linear combination have absolute value no greater than \(\|C_{\mathbb{R}}{}^{\top}\odot B_{\mathbb{R}}\|_{\infty}\). Forward differences of decaying exponentials are exponentially small. Therefore, by linearity of forward differences, requiring \(\phi_{n_{\mathbb{R}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}(\mathbf{ I})|_{\text{odd}}\) or \(\phi_{n_{\mathbb{R}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}(\mathbf{I}) |_{\text{even}}\) to have a forward difference that is not exponentially small implies an exponentially large lower bound on \(n_{\mathbb{R}}\|C_{\mathbb{R}}{}^{\top}\odot B_{\mathbb{R}}\|_{\infty}\). When \(\phi_{n_{\mathbb{R}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}(\cdot)\)\(\epsilon\)-approximates \(\phi(\cdot)\) up to time \(t\), forward differences of \(\phi_{n_{\mathbb{R}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}(\mathbf{I}) |_{\text{odd}}\) and \(\phi_{n_{\mathbb{R}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}(\mathbf{ I})|_{\text{even}}\) are close to those of \(\phi(\mathbf{I})|_{\text{odd}}\) and \(\phi(\mathbf{I})|_{\text{even}}\), respectively. We thus conclude that if \(\phi(\mathbf{I})|_{\text{odd}}\) or \(\phi(\mathbf{I})|_{\text{even}}\) has a forward difference that is not especially small, then \(n_{\mathbb{R}}\|C_{\mathbb{R}}{}^{\top}\odot B_{\mathbb{R}}\|_{\infty}\) must be exponentially large. This conclusion is formalized via Equation (3), the sought after result. 

Special cases.Theorem 2 implies that the real SSM suffers from exponentiality (namely, its dimension \(n_{\mathbb{R}}\) or the magnitude of its parameters \((B_{\mathbb{R}},C_{\mathbb{R}})\) must be exponential in \(t\) in order for it to express a mapping that \(\epsilon\)-approximates \(\phi(\cdot)\) up to time \(t\)) in various important cases. Indeed, Corollaries 1 and 2 below respectively show that the real SSM suffers from exponentiality when \(\phi(\cdot)\) is a canonical copy (delay) mapping, and with high probability when \(\phi(\cdot)\) is a random (generic) mapping. In light of Theorem 1 (namely, of the inability of the real SSM to compactly approximate various oscillatory mappings expressible by the complex SSM), it is natural to ask if the real SSM suffers from exponentiality in cases where \(\phi(\cdot)\) is oscillatory, _i.e._ where its impulse response oscillates. Corollary 3 below shows that exponentiality indeed transpires in a case where \(\phi(\cdot)\) is a basic oscillatory mapping. On the other hand, there are simple cases where \(\phi(\cdot)\) is oscillatory yet exponentiality does not take place, _e.g._ the case where the impulse response of \(\phi(\cdot)\) is \((+1,-1,+1,-1,\ldots)\).8 Precise delineation of the type of oscillations that lead to exponentiality is deferred to future work (see Section 6).

Footnote 8: To see that exponentiality does not take place when \(\phi(\mathbf{I})=(+1,-1,+1,-1,\ldots)\), note that in this case, with any \(n_{\mathbb{R}}\), \(\phi_{n_{\mathbb{R}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}(\cdot)\)\(\epsilon\)-approximates \(\phi(\cdot)\) up to time \(t\) whenever: \(C_{\mathbb{R}}\!\!^{\top}\odot B_{\mathbb{R}}\) holds one in its first entry and zeros elsewhere; and \(A_{\mathbb{R}}\) holds \(\min\{0,(-1+\epsilon/t^{2})\}\) in its first diagonal entry.

**Corollary 1**.: _Suppose \(t\geq 9\) and \(\phi(\cdot)=\delta_{((t-1)/2]}(\cdot)\), where as defined in Section 2.3, \(\delta_{((t-1)/2]}(\cdot)\) is the \(\lfloor(t-1)/2\rfloor\) step delay mapping. Assume also that \(\epsilon\leq 1/\big{(}8\sqrt{t}\,\big{)}\). Then, if \(\phi_{n_{\mathbb{R}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}(\cdot)\)\(\epsilon\)-approximates \(\phi(\cdot)\) up to time \(t\), it must hold that:_

\[n_{\mathbb{R}}\|C_{\mathbb{R}}^{\top}\odot B_{\mathbb{R}}\|_{\infty}\geq 2^{t /2}\big{/}\big{(}32\sqrt{t}\,\big{)}\,. \tag{4}\]

Proof sketch (proof in Appendix D.4).: The proof computes forward differences associated with \(\delta_{\lfloor(t-1)/2\rfloor}(\mathbf{I})\) (impulse response of \(\delta_{\lfloor(t-1)/2\rfloor}(\cdot)\)), and plugs them into Theorem 2 (Equation (3)). 

**Corollary 2**.: _Let \(\alpha\in\mathbb{R}_{>0}\), and let \(\mathbf{R}\in\mathbb{R}^{\mathbb{N}}\) be generated by a random process where each element of \(\mathbf{R}\) is independently drawn from a uniform distribution over the interval \([-\alpha,\alpha]\). Suppose that \(t\geq 8\) and that \(\phi(\cdot)\) is the mapping whose impulse response is \(\mathbf{R}\) (i.e., \(\phi(\cdot)\) is defined by \(\phi(\mathbf{S})=\mathbf{S}*\mathbf{R}\)). Let \(p\in(0,1)\), and assume that \(\epsilon\leq\alpha\sqrt{p/t}\). Then, with probability at least \(1-p\), if \(\phi_{n_{\mathbb{R}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}(\cdot)\)\(\epsilon\)-approximates \(\phi(\cdot)\) up to time \(t\), it must hold that:_

\[n_{\mathbb{R}}\|C_{\mathbb{R}}^{\top}\odot B_{\mathbb{R}}\|_{\infty}\geq 2^{t /2}\alpha\sqrt{p}\big{/}\big{(}8\sqrt{t}\big{)}\,. \tag{5}\]

Proof sketch (proof in Appendix D.5).: The proof derives lower bounds (holding with probability at least \(1-p\)) on forward differences associated with \(\mathbf{R}\), and plugs them into Theorem 2 (Equation (3)). 

**Corollary 3**.: _Suppose that \(\phi(\mathbf{I})=(+1\,,\ 0\,,-1\,,\ 0\,,+1\,,\ 0\,,-1\,,\ 0\,,\ldots)\) and \(\epsilon\leq 0.5\). Then, if \(\phi_{n_{\mathbb{R}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}(\cdot)\)\(\epsilon\)-approximates \(\phi(\cdot)\) up to time \(t\), it must hold that:_

\[n_{\mathbb{R}}\|C_{\mathbb{R}}^{\top}\odot B_{\mathbb{R}}\|_{\infty}\geq 2^{3t /4-4}\,. \tag{6}\]

Proof sketch (proof in Appendix D.6).: The proof computes forward differences associated with the series \((+1\,,\ 0\,,-1\,,\ 0\,,+1\,,\ 0\,,-1\,,\ 0\,,\ldots)\), and plugs them into Theorem 2 (Equation (3)). 

#### 3.3.2 Exponentiality Impedes Practical Learning

For any value of \(t\) that is not especially small, exponentiality in \(t\) for the real SSM as put forth in Section 3.3.1--_i.e._, exponentiality in \(t\) of the dimension of the real SSM \(n_{\mathbb{R}}\) or the magnitude of its parameters \((B_{\mathbb{R}},C_{\mathbb{R}})\)--impedes practical learning. This impediment is obvious in the case where \(n_{\mathbb{R}}\) is exponential in \(t\) (in this case, it is impractical to even store the parameters of the real SSM, let alone learn them). Appendix B treats the complementary case, _i.e._ it shows that learning is impractical when the required values for the parameters \((B_{\mathbb{R}},C_{\mathbb{R}})\) are exponential in \(t\) (this is deferred to an appendix due to space constraints). The results of Section 3.3.1 therefore imply that the real SSM cannot practically learn a mapping that \(\epsilon\)-approximates \(\phi(\cdot)\) up to time \(t\) under important choices of \(\phi(\cdot)\).

#### 3.3.3 Complex Parameterizations Do Not Suffer from Exponentiality

Section 3.3.1 established that under a mild condition on \(\phi(\cdot)\), in order for the real SSM to express a mapping that \(\epsilon\)-approximates \(\phi(\cdot)\) up to time \(t\), either the dimension of the real SSM \(n_{\mathbb{R}}\) or the magnitude of its parameters \((B_{\mathbb{R}},C_{\mathbb{R}})\) must be exponential in \(t\). Proposition 3 below proves that in stark contrast, for any choice of \(\phi(\cdot)\) (whose impulse response \(\phi(\mathbf{I})\) is bounded), the complex SSM can express mappings that match \(\phi(\cdot)\) up to time \(t\) with dimension \(n_{\mathbb{C}}\) and magnitude of parameters \((B_{\mathbb{C}},C_{\mathbb{C}})\) that are at most linear in \(t\).

**Proposition 3**.: _For any choice of \(n_{\mathbb{C}}\) greater than or equal to \(t\), there exist assignments for \((A_{\mathbb{C}},B_{\mathbb{C}},C_{\mathbb{C}})\) with which \(\phi_{n_{\mathbb{C}},(A_{\mathbb{C}},B_{\mathbb{C}},C_{\mathbb{C}})}(\cdot)\) matches \(\phi(\cdot)\) up to time \(t\), and wherein:_ (i)_\(\|B_{\mathbb{C}}\|_{2}\leq 2\|\phi(\mathbf{I})_{t}\|_{2}\)_; and_ (ii)_\(\|C_{\mathbb{C}}^{\top}\|_{2}\leq 1\)_._9__

Footnote 9: It is possible to develop a variant of this result that only requires \(n_{\mathbb{C}}\geq\lceil(t+1)/2\rceil\). We omit details, as the current form of the result suffices for our purposes.

Proof sketch (proof in Appendix D.7).: The proof employs the theory of _discrete Fourier transform_ (_DFT_). It begins by assigning (scaled versions of) the \(t\)th roots of unity to the diagonal entries of \(A_{\mathbb{C}}\). Then, it uses the inverse DFT formula to derive assignments for \(B_{\mathbb{C}}\) and \(C_{\mathbb{C}}\) leading \(\phi_{n_{\mathbb{C}},(A_{\mathbb{C}},B_{\mathbb{C}},C_{\mathbb{C}})}(\cdot)\) to match \(\phi(\cdot)\) up to time \(t\). Finally, the proof applies Plancheral theorem to show that the derived assignments for \(B_{\mathbb{C}}\) and \(C_{\mathbb{C}}\) satisfy the desired criteria. 

## 4 Experiments

This section presents controlled experiments corroborating our theory. Section 4.1 demonstrates that complex parameterizations significantly improve performance of SSMs in the theoretically analyzed setting. Section 4.2 shows that this improvement extends to a real-world setting beyond our theory. Finally, Section 4.3 evaluates SSMs with _selectivity_--a new architectural feature yielding state of the art performance [20; 31; 4; 57]. The experiments with selectivity portray a nuanced picture: complex parameterizations are beneficial for some tasks, whereas for others, selectivity allows real parameterizations to achieve comparable (and in some cases better) performance. These findings align with mixed evidence reported in the literature (see Section 1). Moreover, they suggest a potential extension of our theory that accounts for selectivity and may elucidate this evidence, thereby fully delineating the benefits of complex parameterizations for SSMs.

For conciseness, we defer some of the details behind our implementation to Appendix F. Code for reproducing our experiments is available at [https://github.com/edenlum/SSMComplexParamBenefits](https://github.com/edenlum/SSMComplexParamBenefits).

### Theoretically Analyzed Setting

To empirically demonstrate our theoretical findings, we trained the analyzed real and complex SSMs (see Section 2.2) to approximate up to time \(t\) the mapping \(\phi(\cdot)\) (see Section 2.3), with \(t\) varying and with the following choices for \(\phi(\cdot)\): the canonical copy (delay) mapping from Corollary 1; the random (generic) mapping from Corollary 2; and the basic oscillatory mapping from Corollary 3. Throughout, the dimension of the real or complex SSM (\(n_{\mathbb{R}}\) or \(n_{\mathbb{C}}\), respectively) was set to at least \(t\), which, by universality (Section 3.1), implies that the SSM can express a mapping that precisely matches \(\phi(\cdot)\) up to time \(t\). Our theory (Section 3.3) establishes that despite this parity between the real and complex SSMs in terms of expressiveness, there is a strong separation between the SSMs in terms of practical learnability. In particular, with the above choices of \(\phi(\cdot)\), there are exponential barriers that apply only to the real SSM, and prevent its training from being able to yield a mapping that closely approximates \(\phi(\cdot)\) up to time \(t\). Tables 1 and 2 present results obtained with the real and complex SSMs, respectively. They confirm the predictions of our theory.

### Real-World Setting

To empirically demonstrate the benefits of complex parameterizations for SSMs in settings beyond our theory, we evaluated the prominent S4 neural network architecture [21] on the real-world sequential

\begin{table}
\begin{tabular}{c c c c} \hline \hline
**Optimizer** & **Approx. of Copy** & **Approx. of Random** & **Approx. of Oscillatory** \\ \hline Adam & \(0.767\) & \(0.536\) & \(0.812\) \\ AdamW & \(0.772\) & \(0.541\) & \(0.809\) \\ RAdam & \(0.767\) & \(0.538\) & \(0.811\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: In accordance with our theory, the analyzed real SSM (see Section 2.2) cannot practically learn to closely approximate \(\phi(\cdot)\) up to time \(t\) under important choices of \(\phi(\cdot)\), even when \(t\) is moderate. This table reports the approximation error attained by the real SSM, _i.e._ the minimum \(\epsilon\) with which a mapping learned by the real SSM \(\epsilon\)-approximates \(\phi(\cdot)\) up to time \(t\) (see Section 2.3), when \(t=32\) and \(\phi(\cdot)\) varies over the following possibilities: the canonical copy (delay) mapping from Corollary 1; the random (generic) mapping from Corollary 2; and the basic oscillatory mapping from Corollary 3. Learning was implemented by applying one of three possible gradient-based optimizers—Adam [29], AdamW [36] or RAdam [33]—to a loss as in our theory (see Appendix C). For each choice of \(\phi(\cdot)\), reported approximation errors are normalized (scaled) such that a value of one is attained by the trivial zero mapping. Each configuration was evaluated with five random seeds, and its reported approximation error is the minimum (best) that was attained. The dimension of the real SSM (\(n_{\mathbb{R}}\)) was set to \(1024\); other choices of dimension led to qualitatively identical results. For further implementation details see Appendix F.1.

CIFAR-10 dataset from the widely recognized Long Range Arena benchmark [52]. Our implementation is based on the official S4 repository,10 where unless stated otherwise, hyperparameters (pertaining to the neural network architecture and its training) were kept at their default values. A single run with complex parameterization yielded a test accuracy of \(89.10\%\), significantly higher than the highest test accuracy of \(~{}78.27\%\) attained with real parameterization across three random seeds. Modifying the optimizer and initialization scheme with the real parameterization did not improve the test accuracy--see Appendix F.2 for details. The overarching conclusion from our theory--namely, that SSMs benefit from complex parameterizations--thus extends to this real-world setting.

Footnote 10: [https://github.com/state-spaces/s4](https://github.com/state-spaces/s4) (Apache-2.0 license).

### Selectivity

A new architectural feature for SSMs that yields state of the art performance is _selectivity_[20, 31, 4, 57]. In its original form--proposed as part of the Mamba neural network architecture [20]--selectivity amounts to replacing the parameters \(B\) and \(C\) (see Section 2.2), as well as an additional _discretization_ parameter \(\Delta\in\mathbb{R}_{>0}\),2 by certain functions of the input \((u(t))_{t\in\mathbb{N}}\). We empirically study the impact of complex parameterizations on SSMs with selectivity by evaluating a Mamba neural network on two synthetic tasks regarded as canonical in the SSM literature [27, 20]: _(i) copy_, which was shown by our theory (Section 3.3) and earlier experiments (Section 4.1) to pose difficulties for real parameterizations in SSMs with no selectivity; and _(ii) induction-head_, which can be seen as a generalization of copy in which the delay is input-specified (rather than being fixed). Our implementation is based on a widely adopted Mamba repository easily amenable to modification.11 Unless stated otherwise, repository hyperparameters (pertaining to the neural network architecture and its training) were kept at their default values. Further details concerning our implementation, including detailed descriptions of the copy and induction-head tasks, can be found in Appendix F.3.

Footnote 11: [https://github.com/alxndrTL/mamba.py](https://github.com/alxndrTL/mamba.py) (MIT license).

Our first experiment with the Mamba neural network compared real and complex parameterizations for the underlying SSMs, with selectivity included. On the copy task, across three random seeds for each configuration, the highest accuracy attained with the real parameterization was \(80.17\%\), whereas the lowest accuracy attained with the complex parameterization was \(93.05\%\). This gap in performance in favor of the complex parameterization aligns with our theoretical and empirical findings for SSMs without selectivity. In stark contrast, on the induction-head task, there is no such gap (in fact, there is a slight advantage to the real parameterization): across three random seeds for each configuration, accuracies attained with the real parameterization ranged between \(97.35\%\) and \(98.3\%\), whereas those attained with the complex parameterization ranged between \(93.93\%\) and \(97.64\%\). These results align with mixed evidence reported in the SSM literature, by which complex parameterizations are essential for strong performance on some tasks [22, 41, 22], whereas on others, real parameterizations lead to comparable (and in some cases better) performance [37, 20].

To gain insight into the induction-head task not benefiting from the complex parameterization, we conducted an ablation experiment with partial versions of selectivity (_i.e._, where not all of the parameters \(B\), \(C\) and \(\Delta\) were replaced by functions of the input). The results of this experiment, reported in Table 3, reveal that when selectivity is fully or partially removed (more precisely, when

\begin{table}
\begin{tabular}{c c c c} \hline \hline \(t\) & **Approx. of Copy** & **Approx. of Random** & **Approx. of Oscillatory** \\ \hline \(32\) & \(1.6\times 10^{-5}\) & \(6.3\times 10^{-5}\) & \(1.6\times 10^{-4}\) \\ \(64\) & \(1.7\times 10^{-5}\) & \(1.6\times 10^{-5}\) & \(3.7\times 10^{-4}\) \\ \(128\) & \(4.9\times 10^{-5}\) & \(4.8\times 10^{-5}\) & \(2.6\times 10^{-3}\)\(B\), \(C\) or both are not replaced by functions of the input), the complex parameterization regains its advantage. This suggests that selectivity, which is not covered by our theory, may be the key factor enabling real parameterizations to perform as well as complex parameterizations for SSMs on certain tasks. In other words, selectivity may be the dominant factor behind the aforementioned evidence in the SSM literature being mixed. In Section 7 we outline a potential extension of our theory that accounts for selectivity and may elucidate this evidence, thereby fully delineating the benefits of complex parameterizations for SSMs.

## 5 Related Work

SSMs are closely related to linear dynamical systems--a classic object of study in areas such as systems theory [3] and control theory [51]. Although there exists extensive literature concerning properties of real and complex linear dynamical systems [10; 5; 6; 9], this literature does not readily establish benefits of complex parameterizations for SSMs, primarily due to the following reasons: _(i)_ the output of a complex SSM is turned real by disregarding imaginary components (see Section 2.2), therefore it differs from a complex linear dynamical system; and _(ii)_ the structures typically imposed on state transition matrices of SSMs (_e.g._ stable diagonality; see Section 2.2) are generally uncommon in the literature on linear dynamical systems.

SSMs can be viewed as a special case of recurrent neural networks [48], which received significant theoretical attention [46; 39; 25; 11; 53]. In this context, several works focused specifically on SSMs [42; 30; 24; 2; 28; 58; 12; 32; 55]. However, to our knowledge, the only prior work to formally and explicitly treat benefits of complex parameterizations for SSMs is [42]. The treatment of [42] (see Section 4.1 therein) can be viewed as a special case of ours. Indeed, [42] considered a task that, using our notation (see Section 2.2), amounts to linearly reconstructing an input element \(u(t)\) from the state \(\mathbf{x}(t^{\prime})\) of an SSM, where \(t,t^{\prime}\in\mathbb{N},t^{\prime}\geq t\). This is equivalent to assigning the output matrix of the SSM \(C\) such that the mapping realized by the SSM \(\phi_{n,(A,B,C)}(\cdot)\) is a canonical copy (delay) mapping. Roughly speaking, [42] showed that this task requires linear operations with exponential parameters if the SSM is real, whereas linear operations with moderate parameters suffice if the SSM is complex. The same result follows from our Corollary 1 and Proposition 3. We stress that our theory goes far beyond this result, for example in that it covers various mappings beyond copy, including a random (generic) mapping--see Section 3.3 for details.

With regards to related empirical work, the literature includes several experimental comparisons between real and complex parameterizations for SSMs [20; 22; 42]. Nonetheless, to our knowledge, the controlled experiments we conducted (see Section 4) are reported herein for the first time.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline
**Selective \(B\)** & **Selective \(C\)** & **Selective \(\Delta\)** & **Real Accuracy (\%)** & **Complex Accuracy (\%)** \\ \hline Yes & Yes & Yes & \(97.35\) to \(98.3\) & \(93.93\) to \(97.64\) \\ Yes & Yes & No & \(97.9\) to \(98.62\) & \(90.18\) to \(95.86\) \\ Yes & No & Yes & \(61.82\) to \(71.28\) & \(91.93\) to \(96.77\) \\ Yes & No & No & \(49.91\) to \(52.5\) & \(58.93\) to \(73.77\) \\ No & Yes & Yes & \(56.78\) to \(69.54\) & \(92.52\) to \(96.91\) \\ No & Yes & No & \(41.01\) to \(43.89\) & \(57.44\) to \(64.67\) \\ No & No & Yes & \(15.48\) to \(26.33\) & \(68.54\) to \(79.71\) \\ No & No & No & \(23.86\) to \(29.62\) & \(37.61\) to \(50.19\) \\ \hline \hline \end{tabular}
\end{table}
Table 3: Ablation experiment demonstrating that real parameterizations can compare (favorably) to complex parameterizations for SSMs with selectivity, but complex parameterizations become superior when selectivity is fully or partially removed. This table reports test accuracies attained by a Mamba neural network [20] on a synthetic induction-head task regarded as canonical in the SSM literature [27; 20]. Evaluation included multiple configurations for the SSMs underlying the neural network. Each configuration corresponds to either real or complex parameterization, and to a specific partial version of selectivity—_i.e._, to a specific combination of parameters that are selective (replaced by functions of the input), where the parameters that may be selective are: the input matrix \(B\); the output matrix \(C\); and a discretization parameter \(\Delta\). For each configuration, the highest and lowest accuracies attained across three random seeds are reported. Notice that when both \(B\) and \(C\) are selective, the real parameterization compares (favorably) to the complex parameterization, whereas otherwise, the complex parameterization is superior. For further implementation details see Appendix F.3.

## 6 Limitations

While this paper offers meaningful contributions regarding benefits of complex parameterizations for SSMs, it is important to acknowledge several of its limitations. First, while we establish a separation between real and complex parameterizations in terms of expressiveness (Section 3.2), we do not quantify how prevalent this separation is, _i.e._, what proportion of the mappings expressible with complex parameterizations cannot be compactly approximated with real parameterizations. Second, while we prove that real parameterizations suffer from exponentiality that impedes practical learning (Sections 3.3.1 and 3.3.2), and that complex parameterizations do not suffer from exponentiality (Section 3.3.3), we do not formally establish practical learnability with complex parameterizations--our evidence for this is purely empirical (Section 4). Third, our theory does not treat all fundamental aspects of learning where real and complex parameterizations may differ, for example it does not treat implicit bias of gradient-based optimization [49]. Fourth, our experiments (Section 4) include only a single real-world setting. Finally, while we establish that a separation between real and complex parameterizations in terms of practical learnability takes place in three important cases (see Corollaries 1 to 3), these cases are likely far from being exhaustive. Indeed, Theorem 2 provides a mild sufficient condition for separation in terms of practical learnability--namely, that certain forward differences are not especially small--and we believe it is possible to apply analytical tools (_e.g._, the Norlund-Rice integral [17]) for translating this condition into interpretable properties satisfied in various important cases beyond those considered. Pursuing the latter direction, and more broadly, addressing the aforementioned limitations, are regarded as important directions for future work.

## 7 Discussion

The extent to which complex parameterizations benefit SSMs is an important open question in machine learning. Evidence in the literature is mixed: while some works demonstrate that complex parameterizations are essential for strong performance, others show that in various settings, real parameterizations lead to comparable (and in some cases better) performance. It was conjectured by Gu and Dao [20] that complex parameterizations are preferable for continuous data modalities (_e.g._, audio, video), whereas for discrete data modalities (_e.g._, text, DNA) real parameterizations suffice.

Since a complex SSM includes twice as many parameters as a real SSM of the same dimension, a priori, one might expect that a real SSM would benefit from becoming complex similarly to how it would benefit from doubling its dimension. Our theory showed that this is not the case, and in fact the former benefit far exceeds the latter. Indeed, we established separations between real and complex SSMs, by which a real SSM can only match a complex SSM if either the dimension of the real SSM or the number of iterations required for its training is exponentially large. Experiments corroborated our theory, and suggested that selectivity--a new architectural feature yielding state of the art performance--may be the dominant factor behind the aforementioned evidence in the literature being mixed.

We now outline a potential extension of our theory that accounts for selectivity. Roughly speaking, the separations we established between real and complex SSMs arise from a gap in their ability to express oscillations, _i.e._, to express frequency components in their impulse response: while a complex SSM can easily express any frequency, a real SSM struggles to do so. Adding selectivity to a real SSM makes its parameters input-dependent, resulting in what can be viewed as an input-dependent impulse response. We hypothesize that this dependence allows importing frequency components from the input to the impulse response. If confirmed, this hypothesis would imply that when the input data is sufficiently rich in frequency content, selectivity can endow real SSMs with all the benefits we proved for complex SSMs. Such an outcome aligns with the conjecture of Gu and Dao [20]: continuous data modalities often consist of only low frequencies, whereas discrete data modalities typically have a "whiter spectrum," _i.e._, a more uniform mix of frequencies [54].

We believe that extending our theory as described may elucidate the mixed evidence in the literature, thereby fully delineating the benefits of complex parameterizations for SSMs.

## Acknowledgments and Disclosure of Funding

We thank Itamar Zimerman for illuminating discussions. This work was supported the European Research Council (ERC) grants HOLI 819080 and NN4C 101164614, a Google Research Scholar Award, a Google Research Gift, Meta, the Yandex Initiative in Machine Learning, the Israel ScienceFoundation (ISF) grant 1780/21, the Tel Aviv University Center for AI and Data Science, the Adelis Research Fund for Artificial Intelligence, Len Blavatnik and the Blavatnik Family Foundation, and Amnon and Anat Shashua.

## References

* Abadi et al. [2015] Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. URL [https://www.tensorflow.org/](https://www.tensorflow.org/). Software available from tensorflow.org.
* Ali et al. [2024] Ameen Ali, Itamar Zimerman, and Lior Wolf. The hidden attention of mamba models, 2024.
* Alligood et al. [1998] Kathleen T Alligood, Tim D Sauer, James A Yorke, and David Chillingworth. Chaos: an introduction to dynamical systems. _SIAM Review_, 40(3):732-732, 1998.
* Anthony et al. [2024] Quentin Anthony, Yury Tokpanov, Paolo Glorioso, and Beren Millidege. Blackmamba: Mixture of experts for state-space models, 2024.
* Antsaklis [1997] Panos J. Antsaklis. _Linear systems / Panos J. Antsaklis, Anthony N. Michel_. McGraw-Hill,, New York, 1997. ISBN 0070414335 (alk. paper).
* Aoki [2013] Masanao Aoki. _State space modeling of time series_. Springer Science & Business Media, 2013.
* Arora et al. [2022] Sanjeev Arora, Zhiyuan Li, and Abhishek Panigrahi. Understanding gradient descent on edge of stability in deep learning, 2022.
* Ben-Kish et al. [2024] Assaf Ben-Kish, Itamar Zimerman, Shady Abu-Hussein, Nadav Cohen, Amir Globerson, Lior Wolf, and Raja Giryes. Decimamba: Exploring the length extrapolation potential of mamba, 2024. URL [https://arxiv.org/abs/2406.14528](https://arxiv.org/abs/2406.14528).
* Brockett [2015] Roger W Brockett. _Finite dimensional linear systems_. SIAM, 2015.
* Casti [1986] John L Casti. _Linear dynamical systems_. Academic Press Professional, Inc., 1986.
* Chen et al. [2019] Minshuo Chen, Xingguo Li, and Tuo Zhao. On generalization bounds of a family of recurrent neural networks, 2019. URL [https://arxiv.org/abs/1910.12947](https://arxiv.org/abs/1910.12947).
* Cirone et al. [2024] Nicola Muca Cirone, Antonio Orvieto, Benjamin Walker, Cristopher Salvi, and Terry Lyons. Theoretical foundations of deep selective state-space models, 2024. URL [https://arxiv.org/abs/2402.19047](https://arxiv.org/abs/2402.19047).
* Cohen et al. [2021] Jeremy M. Cohen, Simran Kaur, Yuanzhi Li, J. Zico Kolter, and Ameet Talwalkar. Gradient descent on neural networks typically occurs at the edge of stability. _ArXiv_, abs/2103.00065, 2021. URL [https://api.semanticscholar.org/CorpusID:232076011](https://api.semanticscholar.org/CorpusID:232076011).
* Cohen-Karlik et al. [2023] Edo Cohen-Karlik, Itamar Menuhin-Gruman, Raja Giryes, Nadav Cohen, and Amir Globerson. Learning low dimensional state spaces with overparameterized recurrent neural nets. _International Conference on Learning Representations_, 2023.
* Damian et al. [2022] Alexandru Damian, Eshaan Nichani, and Jason D. Lee. Self-stabilization: The implicit bias of gradient descent at the edge of stability. _ArXiv_, abs/2209.15594, 2022. URL [https://api.semanticscholar.org/CorpusID:252668622](https://api.semanticscholar.org/CorpusID:252668622).
* Dao and Gu [2024] Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality, 2024. URL [https://arxiv.org/abs/2405.21060](https://arxiv.org/abs/2405.21060).
* Flajolet and Sedgewick [1995] Philippe Flajolet and Robert Sedgewick. Mellin transforms and asymptotics: Finite differences and rice's integrals. _Theoretical Computer Science_, 144(1):101-124, 1995. ISSN 0304-3975. doi: [https://doi.org/10.1016/0304-3975](https://doi.org/10.1016/0304-3975)(94)00281-M. URL [https://www.sciencedirect.com/science/article/pii/030439759400281M](https://www.sciencedirect.com/science/article/pii/030439759400281M).
* Fu et al. [2022] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry huppos: Towards language modeling with state space models. In _The Eleventh International Conference on Learning Representations_, 2022.

* Goel et al. [2022] Karan Goel, Albert Gu, Chris Donahue, and Christopher Re. It's raw! audio generation with state-space models, 2022.
* Gu and Dao [2023] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2023.
* Gu et al. [2022] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces, 2022.
* Gu et al. [2022] Albert Gu, Ankit Gupta, Karan Goel, and Christopher Re. On the parameterization and initialization of diagonal state space models, 2022.
* Gupta et al. [2022] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces, 2022.
* Hanson and Raginsky [2019] Joshua Hanson and Maxim Raginsky. Universal approximation of input-output maps by temporal convolutional nets, 2019.
* Hanson and Raginsky [2022] Joshua Hanson and Maxim Raginsky. Universal simulation of stable dynamical systems by recurrent neural nets. In Alexandre M. Bayen, Ali Jadbabaie, George Pappas, Pablo A. Parrilo, Benjamin Recht, Claire Tomlin, and Melanie Zeilinger, editors, _Proceedings of the 2nd Conference on Learning for Dynamics and Control_, volume 120 of _Proceedings of Machine Learning Research_, pages 384-392. PMLR, 10-11 Jun 2020. URL [https://proceedings.mlr.press/v120/hanson20a.html](https://proceedings.mlr.press/v120/hanson20a.html).
* ieee standard for floating-point arithmetic, July 2019.
* Jelassi et al. [2024] Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and Eran Malach. Repeat after me: Transformers are better than state space models at copying, 2024. URL [https://arxiv.org/abs/2402.01032](https://arxiv.org/abs/2402.01032).
* Kanai et al. [2024] Sekitoshi Kanai, Yasutoshi Ida, Kazuki Adachi, Mihiro Uchida, Tsukasa Yoshida, and Shin'ya Yamaguchi. Evaluating time-series training dataset through lens of spectrum in deep state space models, 2024. URL [https://arxiv.org/abs/2408.16261](https://arxiv.org/abs/2408.16261).
* Kingma and Ba [2017] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.
* Li et al. [2022] Zhong Li, Jiequn Han, Weinan E, and Qianxiao Li. Approximation and optimization theory for linear continuous-time recurrent neural networks. _Journal of Machine Learning Research_, 23(42):1-85, 2022. URL [http://jmlr.org/papers/v23/21-0368.html](http://jmlr.org/papers/v23/21-0368.html).
* Lieber et al. [2024] Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam Rozen, Erez Shwartz, Mor Zusman, and Yoav Shoham. Jamba: A hybrid transformer-mamba language model, 2024.
* Liu and Li [2024] Fusheng Liu and Qianxiao Li. From generalization analysis to optimization designs for state space models, 2024. URL [https://arxiv.org/abs/2405.02670](https://arxiv.org/abs/2405.02670).
* Liu et al. [2021] Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han. On the variance of the adaptive learning rate and beyond, 2021. URL [https://arxiv.org/abs/1908.03265](https://arxiv.org/abs/1908.03265).
* Liu et al. [2024] Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, and Yunfan Liu. Vmamba: Visual state space model, 2024.
* Loshchilov and Hutter [2017] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts, 2017. URL [https://arxiv.org/abs/1608.03983](https://arxiv.org/abs/1608.03983).
* Loshchilov and Hutter [2019] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization, 2019. URL [https://arxiv.org/abs/1711.05101](https://arxiv.org/abs/1711.05101).
* Ma et al. [2023] Xuezhe Ma, Chunting Zhou, Xiang Kong, Junxian He, Liangke Gui, Graham Neubig, Jonathan May, and Luke Zettlemoyer. Mega: Moving average equipped gated attention, 2023.
* Ma et al. [2024] Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, and Chunting Zhou. Megalodon: Efficient llm pretraining and inference with unlimited context length, 2024. URL [https://arxiv.org/abs/2404.08801](https://arxiv.org/abs/2404.08801).
* Nakamura and Nakagawa [2009] Yuichi Nakamura and Masahiro Nakagawa. Approximation capability of continuous time recurrent neural networks for non-autonomous dynamical systems. In _International Conference on Artificial Neural Networks_, 2009. URL [https://api.semanticscholar.org/CorpusID:36553223](https://api.semanticscholar.org/CorpusID:36553223).

* Oppenheim and Willsky [1997] Alan V. Oppenheim and Alan S. Willsky with S. Hamid Nawab. _Signals and Systems_. Prentice Hall, Upper Saddle River, NJ, 2 edition, 1997. ISBN 978-0138147570.
* Orvieto et al. [2023] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences, 2023.
* Orvieto et al. [2024] Antonio Orvieto, Soham De, Caglar Gulcehre, Razvan Pascanu, and Samuel L. Smith. Universality of linear recurrences followed by non-linear projections: Finite-width guarantees and benefits of complex eigenvalues, 2024.
* Contributors [2023] PyTorch Contributors. Pytorch documentation, 2023. URL [https://pytorch.org/docs/stable/](https://pytorch.org/docs/stable/). Accessed: 2024-09-22.
* Ran-Milo et al. [2024] Yuval Ran-Milo, Eden Lumbroso, Edo Cohen-Karlik, Raja Giryes, Amir Globerson, and Nadav Cohen. Provable benefits of complex parameterizations for structured state space models, 2024. URL [https://arxiv.org/abs/2410.14067](https://arxiv.org/abs/2410.14067).
* Robbins and Monro [1951] Herbert Robbins and Sutton Monro. A stochastic approximation method. _The annals of mathematical statistics_, pages 400-407, 1951.
* ICANN 2006_, pages 632-640, Berlin, Heidelberg, 2006. Springer Berlin Heidelberg. ISBN 978-3-540-38627-8.
* Schiff et al. [2024] Yair Schiff, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. Caduceus: Bi-directional equivariant long-range dna sequence modeling, 2024. URL [https://arxiv.org/abs/2403.03234](https://arxiv.org/abs/2403.03234).
* Sherstinsky [2020] Alex Sherstinsky. Fundamentals of recurrent neural network (rnn) and long short-term memory (lstm) network. _Physica D: Nonlinear Phenomena_, 404:132306, March 2020. ISSN 0167-2789. doi: 10.1016/j.physd.2019.132306. URL [http://dx.doi.org/10.1016/j.physd.2019.132306](http://dx.doi.org/10.1016/j.physd.2019.132306).
* Slutzky et al. [2024] Yonatan Slutzky, Yotam Alexander, Noam Razin, and Nadav Cohen. The implicit bias of structured state space models can be poisoned with clean labels. _arxiv preprint_, 2024.
* Smith et al. [2023] Jimmy T. H. Smith, Andrew Warrington, and Scott W. Linderman. Simplified state space layers for sequence modeling, 2023.
* Sontag [2013] Eduardo D Sontag. _Mathematical control theory: deterministic finite dimensional systems_, volume 6. Springer Science & Business Media, 2013.
* Tay et al. [2020] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers, 2020. URL [https://arxiv.org/abs/2011.04006](https://arxiv.org/abs/2011.04006).
* Tu et al. [2020] Zhuozhuo Tu, Fengxiang He, and Dacheng Tao. Understanding generalization in recurrent neural networks. In _International Conference on Learning Representations_, 2020. URL [https://api.semanticscholar.org/CorpusID:214346647](https://api.semanticscholar.org/CorpusID:214346647).
* Vetterli et al. [2014] Martin Vetterli, Jelena Kovacevic, and Vivek K Goyal. _Foundations of Signal Processing_. Cambridge University Press, 2014.
* Wang and Xue [2023] Shida Wang and Beichen Xue. State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory, 2023. URL [https://arxiv.org/abs/2309.13414](https://arxiv.org/abs/2309.13414).
* Widrow and Kollar [2008] Bernard Widrow and Istvan Kollar. _Quantization Noise: Roundoff Error in Digital Computation, Signal Processing, Control, and Communications_. Cambridge University Press, 2008.
* Zhu et al. [2024] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model, 2024.
* Zucchet and Orvieto [2024] Nicolas Zucchet and Antonio Orvieto. Recurrent neural networks: vanishing and exploding gradients are not the end of the story, 2024. URL [https://arxiv.org/abs/2405.21064](https://arxiv.org/abs/2405.21064).

Extensions of Theoretical Analysis

### Discretization

Various SSMs incorporate _discretization_[20, 21, 23, 16], which involves replacing parameter matrices with specific transformations that depend on an additional parameter \(\Delta\in\mathbb{R}_{>0}\). With slight modifications, our main theoretical results, Theorem 1 and Theorem 2, apply directly to SSMs with common discretizations. Below, we exemplify this for one of the most common discretizations. Other discretizations can be treated similarly.

One of the most common discretizations is the bilinear discretization [21, 22], in which an SSM \((\overline{A}_{\mathbb{K}},\overline{B}_{\mathbb{K}},\overline{C}_{\mathbb{K}})\) is parameterized in the following way:

\[\overline{A}_{\mathbb{K}}=\left(I-\Delta/2\cdot A_{\mathbb{K}}\right)^{-1} \left(I+\Delta/2\cdot A_{\mathbb{K}}\right),\quad\overline{B}_{\mathbb{K}}^{ \mathrm{r}}=\left(I-\Delta/2\cdot A_{\mathbb{K}}\right)^{-1}\Delta B_{\mathbb{ K}},\quad\overline{C}_{\mathbb{K}}^{\mathrm{r}}=C_{\mathbb{K}}\,,\]

where \(\Delta\in\mathbb{R}_{>0}\), the matrices \(A_{\mathbb{K}},B_{\mathbb{K}}\) and \(C_{\mathbb{K}}\) share the same dimensions and are defined over the same fields as \(\overline{A}_{\mathbb{K}},\overline{B}_{\mathbb{K}}\) and \(\overline{C}_{\mathbb{K}}\), respectively, and the real parts of the diagonal entries of \(A_{\mathbb{K}}\) are negative. Since the set of functions expressible by a real or complex SSM remains unchanged under this discretization, Theorem 1 holds without modification. Furthermore, Theorem 2 applies to this discretization if the left-hand side of Equation (3) is replaced by \(n_{\mathbb{R}}\Delta\|C_{\mathbb{R}}{}^{\top}\odot B_{\mathbb{K}}\|_{\infty}\). This follows directly from Lemma 13, which shows that for any \(i\in[n_{\mathbb{R}}]\), we have \(|\Delta\left(B_{\mathbb{R}}\right)_{i}|\geq|\left(\overline{B}_{\mathbb{R}} \right)_{i}|\). The latter inequality justifies replacing \(\overline{B_{\mathbb{R}}^{\mathrm{r}}}\) with \(\Delta B_{\mathbb{R}}\) on the left-hand side of Equation (3), leading to the desired result.

### Feedthrough

Some SSMs include an additional _feedthrough_ parameter \(D\in\mathbb{K}^{1,1}\)[41]. With feedthrough, the expression for \(y(t)\) in Equation (1) becomes \(\Re(C\mathbf{x}(t)+Du(t))\). Our theory essentially applies as is to SSMs with feedthrough, as explained below.

Obviously, an SSM of dimension \(n\) without a feedthrough parameter can be emulated by an SSM of dimension \(n\) with a feedthrough parameter \(D\) by setting \(D=0\). Conversely, an SSM \((A,B,C)\) of dimension \(n\) with a feedthrough parameter \(D\) can be emulated by an SSM \((A^{\prime},B^{\prime},C^{\prime})\) of dimension \(n+1\) without a feedthrough parameter. This is achieved, for example, by setting the diagonal entries of \(A^{\prime}\) to be those of \(A\) followed by zero, the entries of \(B^{\prime}\) to be those of \(B\) followed by one, and the entries of \(C^{\prime}\) to be those of \(C\) followed by \(D\). For any result where an SSM without feedthrough is assumed, a corresponding result for an SSM with feedthrough holds. Such a result can be attained by either emulating an SSM with feedthrough by an SSM without (while increasing dimension from \(n\) to \(n+1\)) or by emulating an SSM without feedthrough by an SSM with feedthrough, (while maintaining the dimension) depending on the context.

### Complex \(A\) with Real \(B\) and \(C\)

It is possible to consider hybrid SSMs where \(A\) is allowed to be complex while \(B\) and \(C\) are restricted to be real. Below we show that such hybrid SSMs enjoy all provable benefits of complex SSMs (_i.e._, of SSMs where \(A,B\) and \(C\) can all be complex).

There are four results concerning complex SSMs: Proposition 1, Proposition 2, Theorem 1 and Proposition 3. The first three results can be trivially extended to the setting where only \(A\) is complex, as their proofs do not assume complex \(B\) or \(C\). An extension of Proposition 3 is also straightforward. Indeed, the proof of Proposition 3 utilizes the fact that for a complex SSM \((A_{\mathbb{C}},B_{\mathbb{C}},C_{\mathbb{C}})\) of dimension \(n_{\mathbb{C}}\), the impulse response can be any linear combination of \(n_{\mathbb{C}}\) arbitrary decaying sine and cosine waves, which, by Fourier theory, can approximate any sequence of length \(n_{\mathbb{C}}\). If only \(A_{\mathbb{C}}\) is allowed to be complex, the impulse response can consist of any linear combination of \(n_{\mathbb{C}}\) arbitrary decaying cosine waves, which can still represent any sequence of length \(n_{\mathbb{C}}\) via the discrete cosine transform. Thus, the setting remains fundamentally similar, allowing for the extension of Proposition 3 to hybrid SSMs.

## Appendix B Exponentiality Impedes Practical Learning

In this appendix, we show that learning the parameters of the real SSM, _i.e._\((A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})\), is impractical when the required values for \((B_{\mathbb{R}},C_{\mathbb{R}})\) are exponential in \(t\). We account for two aspects of this impracticality: the number of iterations required by gradient descent; and the precision (numberof bits) required for representing the parameters. Note that we will not preclude the possibility of overcoming the impracticality through development of new techniques (_e.g._ new parameterizations for \(B_{\mathbb{R}}\) and \(C_{\mathbb{R}}\)). We believe our account herein may assist in such developments.

Exponential number of iterations.For the sake of illustration, suppose that training the real SSM to realize a mapping \(\phi_{n_{\mathbb{Z}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}(\cdot)\) that \(\epsilon\)-approximates \(\phi(\cdot)\) up to time \(t\), is implemented via gradient descent over a loss function \(\ell(\cdot)\) that measures the squared error of the output at time \(t\), where input elements are drawn independently from the standard normal distribution:

\[\ell(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}}):=\mathbb{E}_{\mathbf{U} \in\mathbb{R}^{\mathbb{N}},\mathbf{U}_{1},\mathbf{U}_{2},\ldots\stackrel{{ \mathrm{i.i.d.}}}{{\sim}}\mathcal{N}_{(0,1)}}\left[(\phi_{n_{\mathbb{Z}},(A_{ \mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}(\mathbf{U})_{t}-\phi(\mathbf{U})_ {t})^{2}\right]. \tag{7}\]

By a simple derivation (see Appendix C) \(\ell(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})=\|\phi_{n_{\mathbb{Z}},(A_{ \mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}(\mathbf{I})_{:t}-\phi(\mathbf{I})_ {:t}\|_{2}^{2}\), therefore sufficient minimization of the loss \(\ell(\cdot)\) (namely, minimization of \(\ell(\cdot)\) to or below the value \(\epsilon^{2}/t\)) indeed guarantees that \(\phi_{n_{\mathbb{Z}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}(\cdot)\)\(\epsilon\)-approximates \(\phi(\cdot)\) up to time \(t\). Assume the following regularity conditions on gradient descent: _(i)_ the step sizes (learning rates) for all iterations are upper bounded by a constant (independent of \(t\)); _(ii)_ the step size for each iteration is _stable_, in the sense that it is upper bounded by \(2/\lambda_{max}\), where \(\lambda_{max}\) represents the maximum eigenvalue of the Hessian of \(\ell(\cdot)\) at the respective iteration [13, 7, 15]; and _(iii)_ the values of \(\ell(\cdot)\) throughout all iterations are upper bounded by a constant (independent of \(t\)) times the value of \(\ell(\cdot)\) at initialization. Proposition 4 below establishes that under said regularity conditions, during gradient descent, the magnitude of the parameters \((B_{\mathbb{R}},C_{\mathbb{R}})\) grows at most linearly in the number of iterations. Accordingly, if the required values for \((B_{\mathbb{R}},C_{\mathbb{R}})\) are exponential in \(t\) and the initialization of \((B_{\mathbb{R}},C_{\mathbb{R}})\) is not, attaining the required values necessitates an exponential (in \(t\)) number of iterations.

**Proposition 4**.: _Consider an application of gradient descent to the loss \(\ell(\cdot)\) in Equation (7):_

\[\left(A_{\mathbb{R}}^{(i)},B_{\mathbb{R}}^{(i)},C_{\mathbb{R}}^{(i)}\right)= \left(A_{\mathbb{R}}^{(i-1)},B_{\mathbb{R}}^{(i-1)},C_{\mathbb{R}}^{(i-1)} \right)-\eta^{(i)}\nabla\ell\big{(}A_{\mathbb{R}}^{(i-1)},B_{\mathbb{R}}^{(i-1 )},C_{\mathbb{R}}^{(i-1)}\big{)}\;,\;i\in\mathbb{N}\,,\]

_where \(\left(A_{\mathbb{R}}^{(0)},B_{\mathbb{R}}^{(0)},C_{\mathbb{R}}^{(0)}\right)\) is a chosen initialization, and \(\eta^{(i)}\in\mathbb{R}_{>0}\) represents the step size selected for iteration \(i\in\mathbb{N}\). Assume \(\exists c_{1}\in\mathbb{R}_{>0}\) such that \(\forall i\in\mathbb{N}:\eta^{(i)}\leq c_{1}\). Assume also:_

\[\forall i\in\mathbb{N}:\eta^{(i)}\leq 2\Big{/}\max\left\{\lambda_{max}\big{(} \nabla^{2}\ell\big{(}A_{\mathbb{R}}^{(i-1)},B_{\mathbb{R}}^{(i-1)},C_{\mathbb{ R}}^{(i-1)}\big{)}\big{)}\,,\,0\right\},\]

_where \(\lambda_{max}(M)\), for a symmetric matrix \(M\), is the maximum eigenvalue of \(M\). Finally, assume:_

\[\exists c_{2}\in\mathbb{R}_{>0}\;\;\text{such that}\;\;\forall i\in\mathbb{N}: \ell\big{(}A_{\mathbb{R}}^{(i)},B_{\mathbb{R}}^{(i)},C_{\mathbb{R}}^{(i)} \big{)}\leq c_{2}\cdot\ell\big{(}A_{\mathbb{R}}^{(0)},B_{\mathbb{R}}^{(0)},C_ {\mathbb{R}}^{(0)}\big{)}\,.\]

_Then:_

\[\forall i\in\mathbb{N}:\max\left\{\left\|B_{\mathbb{R}}^{(i)} \right\|_{\infty},\,\left\|(C_{\mathbb{R}}^{(i)})^{\top}\right\|_{\infty}\right\} \leq\max\left\{\left\|B_{\mathbb{R}}^{(0)}\right\|_{\infty},\,\left\|(C_{ \mathbb{R}}^{(0)})^{\top}\right\|_{\infty}\right\}\] \[\qquad\qquad\qquad\qquad+i\cdot\left(4c_{1}c_{2}t\cdot\ell\big{(}A _{\mathbb{R}}^{(0)},B_{\mathbb{R}}^{(0)},C_{\mathbb{R}}^{(0)}\big{)}\right)^{0. 5}.\]

Proof sketch (proof in Appendix D.8).: Taking into account the form of \(\ell(\cdot)\) (see Appendix C) and the assumptions made, the proof shows that at each iteration of gradient descent, each entry in \(B_{\mathbb{R}}\) or \(C_{\mathbb{R}}\) changes by at most \((4c_{1}c_{2}t\cdot\ell(A_{\mathbb{R}}^{(0)},B_{\mathbb{R}}^{(0)},C_{\mathbb{R }}^{(0)}))^{0.5}\). This readily leads to the desired result. 

Prohibitive precision.With conventional floating-point representation, a real number \(\rho\) is represented as \(s\cdot k\cdot 2^{m}\), where: \(s\in\{-1,1\}\) (the sign) is represented by one bit; \(k\in\mathbb{N}\cup\{0\}\) (the significand) is represented by \(b_{k}\in\mathbb{N}\) bits; and \(m\in\mathbb{Z}\) (the exponent) is represented by \(b_{m}\in\mathbb{N}\) bits. The precision of the floating-point representation is the total number of bits used for \(s\), \(k\) and \(m\), _i.e._ it is \(1+b_{k}+b_{m}\). For example, with the widespread IEEE 754 standard [26]: single precision corresponds to \(32\) bits with \(b_{k}=23\), \(b_{m}=8\); and double precision corresponds to \(64\) bits with \(b_{k}=52,b_{m}=11\). It is customary to model quantization error as an additive random variable uniformly distributed over the interval \([-\xi/2,\xi/2]\), where \(\xi\in\mathbb{R}_{>0}\) is the quantization step size [56]. With the floating-point representation of \(\rho\), the best (lowest) achievable quantization step size is on the order of \(|\rho|\cdot 2^{-b_{k}}\), thus we may model quantization error as a multiplicative random variable uniformly distributed over \([1-q/2,1+q/2]\), where \(q=\Theta(2^{-b_{k}})\). Definition 2 below employs this model to formalize the notion of robustness to quantization for values of \((A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})\) with which \(\phi_{n_{\mathbb{Z}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}(\cdot)\)\(\epsilon\)-approximates \(\phi(\cdot)\) up to time \(t\). Specifically, Definition 2 defines _robustness to \(q\)-quantization_ for such values to be the probability that \(\phi_{n_{\mathbb{Z}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}(\cdot)\) continuesto \(\epsilon\)-approximate \(\phi(\cdot)\) up to time \(t\), when the values are multiplied by random variables uniformly distributed over \([1-q/2,1+q/2]\). Proposition 5 below then proves that robustness to \(q\)-quantization is (at most) inversely proportional to \(q\) times the magnitude of \((B_{\mathbb{R}},C_{\mathbb{R}})\). Recalling that \(q=\Theta(2^{-b_{k}})\), we conclude that if the values of \((B_{\mathbb{R}},C_{\mathbb{R}})\) are exponential in \(t\), a non-negligible robustness to quantization necessitates having \(b_{k}=\Omega(t)\), meaning a floating-point precision that scales (at least) linearly in \(t\). The latter requirement is prohibitive, since virtually all computing systems implementing neural networks entail fixed options for floating-point precision (typically \(16,32\) and \(64\)[1, 43]), all much smaller than what the time \(t\) can be (tens of thousands or more [20, 47, 19, 8])

**Definition 2**.: Let \(q\in[0,1]\), and let \(Q_{A_{\mathbb{R}}}\), \(Q_{B_{\mathbb{R}}}\) and \(Q_{C_{\mathbb{R}}}\) be random matrices of the same sizes as \(A_{\mathbb{R}}\), \(B_{\mathbb{R}}\) and \(C_{\mathbb{R}}\), respectively, wherein each entry is independently drawn from the uniform distribution over \([1-q/2,1+q/2]\). Let \((A_{\mathbb{R}}^{*},B_{\mathbb{R}}^{*},C_{\mathbb{R}}^{*})\) be values for \((A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})\) with which \(\phi_{n_{\mathbb{R}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}(\cdot)\)\(\epsilon\)-approximates \(\phi(\cdot)\) up to time \(t\). The _robustness to \(q\)-quantization_ of \((A_{\mathbb{R}}^{*},B_{\mathbb{R}}^{*},C_{\mathbb{R}}^{*})\) is the probability that \(\phi_{n_{\mathbb{R}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}(\cdot)\)\(\epsilon\)-approximates \(\phi(\cdot)\) up to time \(t\) when \((A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})\) take the values \((A_{\mathbb{R}}^{*}\odot Q_{A_{\mathbb{R}}},B_{\mathbb{R}}^{*}\odot Q_{B_{ \mathbb{R}}},C_{\mathbb{R}}^{*}\odot Q_{C_{\mathbb{R}}})\).

**Proposition 5**.: _Suppose that \(\phi_{n_{\mathbb{R}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}(\cdot)\)\(\epsilon\)-approximates \(\phi(\cdot)\) up to time \(t\). Then, for any \(q\in[0,1]\), the robustness to \(q\)-quantization of the values held by \((A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})\) is at most:_

\[2\epsilon\big{/}\big{(}q\|C_{\mathbb{R}}^{\mathsf{T}}\odot B_{\mathbb{R}}\|_{ \infty}\big{)}\,.\]

Proof sketch (proof in Appendix D.9).: Denote by \((A_{\mathbb{R}}^{*},B_{\mathbb{R}}^{*},C_{\mathbb{R}}^{*})\) the values held by \((A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})\), and let \(Q_{A_{\mathbb{R}}}\), \(Q_{B_{\mathbb{R}}}\) and \(Q_{C_{\mathbb{R}}}\) be random matrices as in Definition 2. When \((A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})\) take the values \((A_{\mathbb{R}}^{*}\odot Q_{A_{\mathbb{R}}},B_{\mathbb{R}}^{*}\odot Q_{B_{ \mathbb{R}}},C_{\mathbb{R}}^{*}\odot Q_{C_{\mathbb{R}}})\), the first entry of the impulse response of \(\phi_{n_{\mathbb{R}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}(\cdot)\) equals \((C_{\mathbb{R}}^{*}\odot Q_{C_{\mathbb{R}}})(B_{\mathbb{R}}^{*}\odot Q_{B_{ \mathbb{R}}})\). It suffices to restrict attention to this first entry, and show that \(|(C_{\mathbb{R}}^{*}\odot Q_{C_{\mathbb{R}}})(B_{\mathbb{R}}^{*}\odot Q_{B_{ \mathbb{R}}})-\phi(\mathbf{I})_{1}|\leq\epsilon\) with probability at most \(2\epsilon/(q\|C_{\mathbb{R}}^{\mathsf{T}}\odot B_{\mathbb{R}}\|_{\infty})\). The proof establishes the latter anti-concentration result. 

## Appendix C Loss Functions

Consider a loss function as in our theory (see Equation (7)), applied to both real and complex SSMs:

\[\ell(A_{\mathbb{K}},B_{\mathbb{K}},C_{\mathbb{K}})=\mathbb{E}_{ \mathbf{U}\in\mathbb{R}^{\mathbb{N}},\,\mathbf{U}_{1},\mathbf{U}_{2},\ldots \stackrel{{ iid}}{{\sim}}\mathcal{N}(0,1)}\,\left[\big{(}\phi_{ n_{\mathbb{K}},(A_{\mathbb{K}},B_{\mathbb{K}},C_{\mathbb{K}})}(\mathbf{U})_{t}- \phi(\mathbf{U})_{t}\big{)}^{2}\right]\,,\]

where \(\mathbb{K}=\mathbb{R}\) or \(\mathbb{K}=\mathbb{C}\). Below we prove that:

\[\ell(A_{\mathbb{K}},B_{\mathbb{K}},C_{\mathbb{K}})=\left\|\phi_{n_{\mathbb{K} },(A_{\mathbb{K}},B_{\mathbb{K}},C_{\mathbb{K}})}(\mathbf{I})_{:t}-\phi(\mathbf{ I})_{:t}\right\|_{2}^{2}.\]

Indeed, as discussed in Section 2.3, for any \(\mathbf{U}\in\mathbb{R}^{\mathbb{N}}\):

\[\phi_{n_{\mathbb{K}},(A_{\mathbb{K}},B_{\mathbb{K}},C_{\mathbb{K}})}(\mathbf{ U})_{t}=\left(\mathbf{U}*\phi_{n_{\mathbb{K}},(A_{\mathbb{K}},B_{\mathbb{K}},C_{ \mathbb{K}})}(\mathbf{I})\right)_{t},\quad\phi(\mathbf{U})_{t}=\left(\mathbf{U}* \phi(\mathbf{I})\right)_{t}.\]

Thus:

\[\phi_{n_{\mathbb{K}},(A_{\mathbb{K}},B_{\mathbb{K}},C_{\mathbb{K}})}(\mathbf{ U})_{t}-\phi(\mathbf{U})_{t}=\sum_{j=1}^{t}\mathbf{U}_{j}\left(\phi_{n_{\mathbb{K}},(A_{ \mathbb{K}},B_{\mathbb{K}},C_{\mathbb{K}})}(\mathbf{I})_{t-j+1}-\phi(\mathbf{ I})_{t-j+1}\right).\]

It holds that:

\[\ell(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}}) =\mathbb{E}_{\mathbf{U}\in\mathbb{R}^{\mathbb{N}},\,\mathbf{U}_{ 1},\mathbf{U}_{2},\ldots\stackrel{{ iid}}{{\sim}}\mathcal{N}(0,1)} \left[\left(\sum_{j=1}^{t}\mathbf{U}_{j}\left(\phi_{n_{\mathbb{K}},(A_{ \mathbb{K}},B_{\mathbb{K}},C_{\mathbb{K}})}(\mathbf{I})_{t-j+1}-\phi(\mathbf{ I})_{t-j+1}\right)\right)^{2}\right]\] \[=\sum_{j=1}^{t}\left(\phi_{n_{\mathbb{K}},(A_{\mathbb{K}},B_{ \mathbb{K}},C_{\mathbb{K}})}(\mathbf{I})_{t-j+1}-\phi(\mathbf{I})_{t-j+1}\right)^{2} \mathbb{E}_{\mathbf{U}_{j}\sim\mathcal{N}(0,1)}\left[\mathbf{U}_{j}^{2}\right]\] \[=\left\|\phi_{n_{\mathbb{K}},(A_{\mathbb{K}},B_{\mathbb{K}},C_{ \mathbb{K}})}(\mathbf{I})_{:t}-\phi(\mathbf{I})_{:t}\right\|_{2}^{2}\,,\]

as required.

Deferred Proofs

### Proof of Proposition 1

Beginning with the real SSM (\(\mathbb{K}=\mathbb{R}\)), assume that \(n_{\mathbb{R}}\geq t\). We would like to show that there exist assignments for \((A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})\) with which:

\[\phi(\mathbf{I})_{\cdot t}=(C_{\mathbb{R}}B_{\mathbb{R}},C_{\mathbb{R}}A_{ \mathbb{R}}B_{\mathbb{R}},C_{\mathbb{R}}A_{\mathbb{R}}^{2}B_{\mathbb{R}}, \ldots,C_{\mathbb{R}}A_{\mathbb{R}}^{t-1}B_{\mathbb{R}})\,,\]

where \(\phi(\mathbf{I})_{\cdot t}\) stands for the first \(t\) elements of the impulse response of \(\phi(\cdot)\). Regarding \(\phi(\mathbf{I})_{\cdot t}\), \(B_{\mathbb{R}}\) and \(C_{\mathbb{R}}^{\top}\) as (column) vectors, we may write the sought after equality as \(V(A_{\mathbb{R}})(C_{\mathbb{R}}^{\top}\odot B_{\mathbb{R}})=\phi(\mathbf{I}) _{\cdot t}\), where \(V(A_{\mathbb{R}})\in\mathbb{R}^{t,n_{\mathbb{R}}}\) is a matrix whose \((i,j)\)th entry holds the \(j\)th diagonal entry of \(A_{\mathbb{R}}\) exponentiated by \(i-1\). Notice that \(V(A_{\mathbb{R}})\) is a Vandermonde matrix comprising powers of the diagonal entries of \(A_{\mathbb{R}}\). Any assignment for \(A_{\mathbb{R}}\) with distinct diagonal entries leads the Vandermonde matrix to have full rank, which in turn means that there exists a vector \(\mathbf{v}\in\mathbb{R}^{n_{\mathbb{R}}}\) satisfying \(V(A_{\mathbb{R}})\mathbf{v}=\phi(\mathbf{I})_{\cdot t}\). Assigning \(B_{\mathbb{R}}=\mathbf{v}\) and \(C_{\mathbb{R}}^{\top}=\mathbf{1}\) concludes the proof for the real SSM. The complex SSM (\(\mathbb{K}=\mathbb{C}\)) can be treated analogously, while assigning real values to \(A_{\mathbb{C}}\), \(B_{\mathbb{C}}\) and \(C_{\mathbb{C}}\) in order to allow disregarding the fact that only real parts of outputs are taken (_i.e._, disregarding the operator \(\Re(\cdot)\) in Equation (2)). 

### Proof of Theorem 1

To facilitate the proof of the theorem, we first outline essential definitions and lemmas to quantify a lower bound for the number of times the impulse response alternates between being greater than or equal to \(1/4\), and being smaller than or equal to \(-1/4\). Building on this groundwork, we then unpack the main theorem's proof in Appendix D.2.2.

#### d.2.1 Setup for the proof

**Definition 3**.: For an angle \(\theta\) in \(\mathbb{R}\) we will say that \(\theta\) is in the **mostly-positive region** if

\[\theta\bmod 2\pi\in\left[0,\frac{\pi}{3}\right]\cup\left[\frac{5\pi}{3},2\pi\right]\]

and we say that \(\theta\) is in the **mostly-negative region** if

\[\theta\bmod 2\pi\in\ \left[\frac{2\pi}{3},\frac{4\pi}{3}\right]\]

**Remark 1**.: _Under this definition, a non-zero complex number \(x\) has that its argument is in the mostly-positive region if and only if \(\Re(x)\geq\frac{|x|}{2}\). Conversely, \(x\) has that its argument is in the mostly-negative region if and only if \(\Re(x)\leq-\frac{|x|}{2}\)._

**Definition 4**.: We say that a set is **balanced** if at least a sixth of its elements are in the mostly-positive region and at least a sixth of its elements are in the mostly-negative region.

**Definition 5**.: The **balancing number** of a complex number \(x\) with argument \(\theta\) is defined as the minimum \(p\in\mathbb{N}\) such that for any non-zero real number \(b\) the set \(\{b+\theta,b+2\theta,\ldots,b+p\theta\}\) is balanced. If no such \(p\) exists, we say that the balancing number of \(x\) is infinity. We denote the balancing number of \(x\) by \(\beta(x)\).

We will begin by establishing an upper bound on \(\beta(x)\) for \(x\) in a specific region in the complex plane. Subsequently, we generalize this result to give an upper bound of the balancing number for almost all complex numbers.

**Lemma 1**.: _For any natural number \(p\geq 4\), if a non-zero complex number \(x\) has an argument \(\theta\) such that \(\theta\) is in \([\frac{2\pi}{p},\frac{2\pi}{3}]\), then \(\beta(x)\leq p\). Moreover, for any natural number \(p\geq 9\), if \(x\) has an argument \(\theta\) such that \(\theta\) is in \([\pi+\frac{2\pi}{p-1},\frac{4\pi}{3}]\) then \(\beta(x)\leq p\)._

Proof.: Since the mostly-positive and mostly-negative regions are defined by looking at the angle of a complex number mod \(2\pi\), to prove that \(\beta(x)\leq p\), it is enough to show there exists some number \(k\leq p\) such that for any real \(b\) the following set

\[\{(b+\theta)\bmod 2\pi,(b+2\cdot\theta)\bmod 2\pi,\ldots,(b+k\cdot\theta)\bmod 2\pi\}\]

is balanced.

Indeed, let there be some real \(b\).

1. If \(\theta\) is in \([\frac{2\pi}{p},\frac{2\pi}{3}]\) and \(p\geq 4\), Let \(k\) be the natural number such that \(2\pi<\theta k\leq 2\pi+\theta\). It follows that \(4\leq k\leq p\) and that \(\theta\leq\frac{2\pi}{k-1}\). By Lemma 5 we have that the set \[\{(b+\theta)\bmod 2\pi,(b+2\cdot\theta)\bmod 2\pi,\ldots,(b+k\cdot\theta) \bmod 2\pi\}\] has at least a single point in any continuous interval (in \(\bmod 2\pi\)) of length \(\theta\). As such, any continuous interval \(\bmod 2\pi\) of length \(\frac{2\pi}{3}\) must contain at least \[\left\lfloor\frac{\frac{2\pi}{3}}{\theta}\right\rfloor\geq\left\lfloor\frac{ \frac{2\pi}{3}}{\frac{\pi}{k-1}}\right\rfloor=\left\lfloor\frac{k-1}{3}\right\rfloor \geq\frac{k}{6}\] points, where the last inequality is true for any \(k\geq 4\). Since the mostly-positive and mostly-negative regions are continuous intervals (\(\bmod 2\pi\)) of length \(\frac{2\pi}{3}\), we have that the set \[\{(b+\theta)\bmod 2\pi,(b+2\cdot\theta)\bmod 2\pi,\ldots,(b+k\cdot\theta) \bmod 2\pi\}\] has at least a sixth of its points in the mostly-positive and mostly-negative regions, and is therefore balanced, as needed.
2. If \(\theta\) is in \([\frac{\pi}{2}+\frac{2\pi}{p-1},\frac{4\pi}{3}]\) and \(p\geq 9\), then \(2\theta\bmod\pi\) is in the interval \(\left\lceil\frac{2\pi}{\frac{p-1}{2}},\frac{2\pi}{3}\right\rceil\) which is contained in the interval \(\left\lceil\frac{2\pi}{\left\lfloor\frac{2\pi}{2}\right\rfloor},\frac{2\pi}{3}\right\rceil\). Therefore, by the previous section, there exists some \(k\leq\left\lfloor\frac{p}{2}\right\rfloor\) such that the following set: \[\{b+2\theta\bmod 2\pi,\ldots,b+k\cdot 2\theta\bmod 2\pi\}\] and the set \[\{(b-\theta)+2\theta\bmod 2\pi,\ldots,(b-\theta)+k\cdot 2\theta\bmod 2\pi\}\] are both balanced. Overall, their union, the set \[\{(b+\theta)\bmod 2\pi,(b+2\cdot\theta)\bmod 2\pi,\ldots,(b+2k\cdot\theta) \bmod 2\pi\}\] is also balanced, showing that the \(\beta(x)\leq 2k\leq p\) as needed.

**Lemma 2**.: _For any natural numbers \(r,\theta\) the following holds:_

\[\beta\left(e^{r+i\theta}\right)=\beta\left(e^{r-i\theta}\right)\]

Proof.: For any rational numbers \(\theta\) and \(b\), and for any natural number \(p\), we will define the set \(S_{\theta,b,p}\) in the following way:

\[S_{\theta,b,p}=\{(b+\theta)\bmod 2\pi,(b+2\cdot\theta)\bmod 2\pi,\ldots,(b+p \cdot\theta)\bmod 2\pi\}\]

We will also define the set \(S_{\theta,p}\) as

\[S_{\theta,p}=\{S_{\theta,b,p}|b\in\mathbb{C}\}\,.\]

Given this notation, we have that \(\beta\left(e^{r-i\theta}\right)=p\) if and only if \(p\) is the minimum natural number such that every set in \(S_{\theta,p}\) is balanced. For any rational numbers \(\theta\) and \(b\) and for any natural number \(p\), we have

\[S_{-\theta,-b,p}=-S_{\theta,b,p}\]

(Where minus is taken element-wise mod \(2\pi\)). This immediately gives:

\[S_{-\theta,p}=-S_{\theta,p}\,.\]

Since the mostly-positive and mostly-negative regions are invariant to taking a minus mod \(2\pi\), we have that all the sets in \(S_{\theta,p}\) are balanced if and only if so are all the sets of \(S_{-\theta,p}\). This directly implies that \(\beta\left(e^{r+i\theta}\right)=\beta\left(e^{r-i\theta}\right)\) as needed. 

**Corollary 4**.: _If \(x\in\mathbb{C}\) has that \(\arg(x)\bmod\pi\in[\frac{2\pi}{p-1},\pi-\frac{2\pi}{p-1}]\) for some \(p\geq 9\) then \(\beta(x)\leq p\)_Proof.: Denote \(\theta\equiv\arg(x)\). From Lemma 1 if the angle \(\theta\) lies within the interval

\[I=\left[\frac{2\pi}{p},\frac{2\pi}{3}\right]\cup\left[\pi+\frac{2\pi}{p-1},\frac{ 4\pi}{3}\right]\]

then \(\beta(x)\leq p\). Additionally, Lemma 2 indicates that the same holds if \(\theta\) is in the interval \(-I\) (where operations are element-wise mod \(2\pi\)). Hence, we have that if \(\theta\) is contained in interval \(I^{\prime}\), where

\[I^{\prime} =I\cup(-I)\] \[=\left[\frac{2\pi}{p},\frac{2\pi}{3}\right]\cup\left[\pi+\frac{2 \pi}{p-1},\frac{4\pi}{3}\right]\cup\left[\frac{4\pi}{3},2\pi-\frac{2\pi}{p} \right]\cup\left[\frac{2\pi}{3},\pi-\frac{2\pi}{p-1}\right]\] \[=\left[\frac{2\pi}{p},\pi-\frac{2\pi}{p-1}\right]\cup\left[\pi+ \frac{2\pi}{p-1},2\pi-\frac{2\pi}{p}\right]\]

then \(\beta(x)\leq p\). Overall, this means that if \(\theta\) has that

\[\theta\bmod\pi\in\left[\frac{2\pi}{p-1},\pi-\frac{2\pi}{p-1}\right]\]

then \(\beta(x)\leq p\), which gives the Corollary. 

#### d.2.2 The Proof

Let \(\epsilon>0\) be any positive number. For any sequence \(X=\{X_{1},X_{2},\ldots,X_{t}\}\), let \(X|_{1}\) denote the subsequence consisting of elements at odd indices, i.e., \(X|_{1}=\{X_{1},X_{3},\ldots\}\).

First, we notice that from the assumption that \(|\sin(\arg(A_{\mathbb{C}}))|\geq 0.2\) we have that \(\arg(A_{\mathbb{C}})\in\left[\frac{\pi}{16},\pi-\frac{\pi}{16}\right]\), and so

\[\arg(A_{\mathbb{C}}^{2})\bmod\pi=2\arg(A_{\mathbb{C}})\bmod\pi\in 2\cdot\left[ \frac{\pi}{16},\pi-\frac{\pi}{16}\right]\bmod\pi=\left[\frac{2\pi}{9-1},\pi- \frac{2\pi}{9-1}\,\right]\]

This implies, by Corollary 4, that the balancing number of \(A_{\mathbb{C}}^{2}\) is less than \(9\). Now, if \(\phi_{n_{2},(A_{\mathbb{R}},B_{\mathbb{C}},C_{\mathbb{C}})}\left(\cdot\right)\)\(\epsilon\)-approximates \(\phi_{n_{\mathbb{C}},(A_{\mathbb{C}},B_{\mathbb{C}},C_{\mathbb{C}})}\left(\cdot\right)\) up to time \(t\), then, by definition,

\[\|\phi_{n_{2},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}\left(\mathbf{I} \right)_{:t}-\phi_{n_{\mathbb{C}},(A_{\mathbb{C}},B_{\mathbb{C}},C_{\mathbb{C }})}\left(\mathbf{I}\right)_{:t}\|_{1}\leq\epsilon\,.\]

This implies

\[\|\phi_{n_{2},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}\left(\mathbf{I} \right)_{:t}|_{1}-\phi_{n_{\mathbb{C}},(A_{\mathbb{C}},B_{\mathbb{C}},C_{ \mathbb{C}})}\left(\mathbf{I}\right)_{:t}|_{1}\|_{1}\leq\epsilon\,.\]

Now, we have that

\[\left(\phi_{1,(A_{\mathbb{C}},B_{\mathbb{C}},C_{\mathbb{C}})}\left( \mathbf{I}\right)_{:t}|_{1}\right)_{k} =\left(\phi_{1,(A_{\mathbb{C}},B_{\mathbb{C}},C_{\mathbb{C}})} \left(\mathbf{I}\right)_{:t}\right)_{2k-1}\] \[=\Re\left(B_{\mathbb{C}}C_{\mathbb{C}}A_{\mathbb{C}}^{2k-2}\right)\]

and because the balancing of \(A_{\mathbb{C}}^{2}\) is at most \(9\), this implies that there exists a subsequence of \(1,2,\ldots,\lfloor t/2\rfloor\) with increasing indices \(k_{j}\) of length at least \(\lfloor\frac{t}{9}\rfloor-1\) such that if \(j\) is even then \(B_{\mathbb{C}}C_{\mathbb{C}}\left(A_{\mathbb{C}}^{2}\right)^{k_{j}-1}\) is in the mostly-positive region, which implies

\[\Re\left(B_{\mathbb{C}}C_{\mathbb{C}}\left(A_{\mathbb{C}}^{2}\right)^{k_{j}- 1}\right)\geq|B_{\mathbb{C}}C_{\mathbb{C}}|\left|\frac{\left(A_{\mathbb{C}}^{ 2}\right)^{k_{j}-1}}{2}\right|\geq\frac{1}{4}\,,\]

where the first inequality is due to the definition of the mostly-positive region and the second is implied from the assumption that \(|B_{\mathbb{C}}\cdot C_{\mathbb{C}}|\geq 1\) and \(|A_{\mathbb{C}}|\geq 0.5^{1/t}\). Conversely, if \(j\) is odd it is in the mostly-negative region, which implies

\[\Re\left(B_{\mathbb{C}}C_{\mathbb{C}}\left(A_{\mathbb{C}}^{2}\right)^{k_{j}- 1}\right)\leq-|B_{\mathbb{C}}C_{\mathbb{C}}|\left|\frac{\left(A_{\mathbb{C}}^{ 2}\right)^{k_{j}-1}}{2}\right|\leq-\frac{1}{4}\,.\]

Next, we will show that the impulse response of any real system, when restricted to even indices, can change its sign at most \(n_{\mathbb{R}}-1\) times. This will prove the theorem since

\[\frac{1}{4}\left(\left\lfloor\frac{t}{9}\right\rfloor-n_{\mathbb{R}}-1\right) \leq\|\phi_{n_{2},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}\left(\mathbf{ I}\right)_{:t}|_{1}-\phi_{n_{\mathbb{C}},(A_{\mathbb{C}},B_{\mathbb{C}},C_{ \mathbb{C}})}\left(\mathbf{I}\right)_{:t}|_{1}\|_{1}\leq\epsilon\,.\]The first inequality results from the fact that there would be at least \(\left\lfloor\frac{t}{p}\right\rfloor-n_{\mathbb{R}}-1\) indices where \(\frac{1}{4}\leq\left|\left(\phi_{1,(A_{\mathbb{C}},B_{\mathbb{C}},C_{\mathbb{R}} )}\left(\mathbf{I}\right)_{:t}|_{1}\right)_{k}\right|\) and \(\phi_{n_{\mathbb{R}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}\left( \mathbf{I}\right)_{:t}|_{1}\) and \(\phi_{1,(A_{\mathbb{C}},B_{\mathbb{C}},C_{\mathbb{R}})}\left(\mathbf{I}\right) _{:t}|_{1}\) disagree on sign, which yields the theorem.

Indeed, let us upper bound the number of sign changes of the real impulse response when restricted to odd indices. We have that

\[\left(\phi_{n_{\mathbb{R}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{ \mathbb{R}})}\left(\mathbf{I}\right)_{:t}|_{1}\right)_{k} =\left(\phi_{n_{\mathbb{R}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{ \mathbb{R}})}\left(\mathbf{I}\right)_{:t}\right)_{2k-1}\] \[=\Re\left(C_{\mathbb{R}}A_{\mathbb{R}}^{2k-2}B_{\mathbb{R}}\right)\] \[=C_{\mathbb{R}}A_{\mathbb{R}}^{2k-2}B_{\mathbb{R}}\] \[=\sum_{j=1}^{n_{\mathbb{R}}}\left(B_{\mathbb{R}}\right)_{j}(C_{ \mathbb{R}})_{j}(A_{\mathbb{R}})_{j,j}^{2k-2}\,.\]

By the mean value theorem, the number of times \(\phi_{n_{\mathbb{R}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}\left( \mathbf{I}\right)_{:t}|_{1}\) changes signs is bounded by the number of zeros of the following continuous function:

\[f(x)=\sum_{j=1}^{n_{\mathbb{R}}}\left(B_{\mathbb{R}}\right)_{j}(C_{\mathbb{R}} )_{j}\left(\left(A_{\mathbb{R}}\right)_{j,j}^{2}\right)^{x}\,.\]

This function is a linear combination of \(n_{\mathbb{R}}\) exponential functions with a positive base, and so by Lemma 4, it has at most \(n_{\mathbb{R}}-1\) zeros, as needed.

### Proof of Theorem 2

Let \(d,m\) be natural numbers such that \(d+m\leq\left\lfloor t/2\right\rfloor\), and let \(\sigma\) be in \(\{\text{odd},\text{ even}\}\). We will show that

\[n_{\mathbb{R}}\|C_{\mathbb{R}}^{\top}\odot B_{\mathbb{R}}\|_{\infty}\geq 2^{d+2 \min\{d,m\}}\left(2^{-d}\left|\left(\phi\left(\mathbf{I}\right)|_{\sigma} \right)_{m}^{(d)}\right|-\epsilon\right)\,,\]

thus proving the theorem.

Let \(\phi_{n_{\mathbb{R}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}\left( \cdot\right)\) be a real system that \(\epsilon\)-approximates \(\phi\left(\cdot\right)\) up to time \(t\). For convenience, we will denote \(\phi_{n_{\mathbb{R}},(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}})}\left( \mathbf{I}\right)_{:t}\) by \(\mathbf{Y}\) and \(\phi\left(\mathbf{I}\right)_{:t}\) by \(\mathbf{T}\). Then, by definition, we have that

\[\left\|\mathbf{T}-\mathbf{Y}\right\|_{1}\leq\epsilon,\]

which implies that

Lemma 9 guarantees that there exist some parameters \(\left(\tilde{A_{\mathbb{R}}},\tilde{B_{\mathbb{R}}},\tilde{C_{\mathbb{R}}}\right)\) where \(\tilde{A_{\mathbb{R}}}\) is non-negative and the following two things hold:

\[\|C_{\mathbb{R}}^{T}\odot B_{\mathbb{R}}\|_{\infty}\geq\|\tilde{C_{\mathbb{R} }}^{T}\odot\tilde{B_{\mathbb{R}}}\|_{\infty}\]

\[\mathbf{Y}|_{\sigma}=\phi_{n_{\mathbb{R}},\left(\tilde{A_{\mathbb{R}}},\tilde {B_{\mathbb{R}}},\tilde{C_{\mathbb{R}}}\right)}\left(\mathbf{I}\right)_{:t}\,.\]

Because \(\tilde{A_{\mathbb{R}}}\) is non-negative, \(\mathbf{Y}|_{\sigma}\) can be viewed as a linear combination of decaying exponentials with positive coefficients. Since the forward difference is linear, Lemma 6 ensures that the absolute value of the \(d\)th forward difference of \(\mathbf{Y}|_{\sigma}\) at index \(m\) is upper bounded by

\[\left(\frac{m}{d+m}\right)^{m}\left(\frac{d}{d+m}\right)^{d}\cdot n_{\mathbb{R }}\|\tilde{C_{\mathbb{R}}}^{T}\odot\tilde{B_{\mathbb{R}}}\|_{\infty}\,.\]

Using Lemma 12, we get

\[\left|\left(\mathbf{Y}|_{\sigma}\right)_{m}^{(d)}\right|\leq\frac{n_{\mathbb{R }}\|\tilde{C_{\mathbb{R}}}^{T}\odot\tilde{B_{\mathbb{R}}}\|_{\infty}}{2^{2\min \left(m,d\right)}}\,.\]

[MISSING_PAGE_FAIL:21]

[MISSING_PAGE_FAIL:22]

[MISSING_PAGE_FAIL:23]

Now, we will derive assignments for the SSM's parameters so that its impulse response will equate this sum. Indeed, we fix the diagonal entries of \(A\) to be a scaled version of the t roots of unity, and \(C\) to be the inverse of the square root of \(t\) as follows:

\[A_{j,j}=\alpha e^{2\pi i\frac{j-1}{t}},\quad C_{j}=\frac{1}{\sqrt{t}}\]

We know (see Section 2.3) that the impulse response of a complex system at index \(k\) is given by:

\[\left(\phi_{t,(A,B,C)}(\mathbf{I})_{:t}\right)_{k}=\Re\left(\sum_{j=1}^{t}A_{j, j}^{k-1}B_{j}C_{j}\right)\,.\]

Which implies that

\[\Re\left(\sum_{j=1}^{t}A_{j,j}^{k-1}B_{j}C_{j}\right)=\sum_{j=1}^ {t}\Re(C_{j}A_{j,j}^{k-1})\Re(B_{j})-\Im(C_{j}A_{j,j}^{k-1})\Im(B_{j})\] \[=\alpha^{k-1}\frac{1}{\sqrt{t}}\left(\sum_{j=0}^{t-1}\cos\left(2 \pi(k-1)\frac{j}{t}\right)\Re(B_{j})-\sum_{j=0}^{t-1}\sin\left(2\pi(k-1)\frac{ j}{t}\right)\Im(B_{j})\right)\,.\]

By the Plancherel theorem the following holds:

\[\|a+bi\|_{2}=\sqrt{t}\left\|\tilde{\phi}(\mathbf{I})_{:t}\right\|_{2}\leq\frac {\sqrt{t}\|\phi(\mathbf{I})_{:t}\|_{2}}{\alpha^{t-1}}\,.\]

Therefore, if we define \(B=\frac{a-ib}{\sqrt{t}}\), we get

\[\Re\left(\sum_{j=1}^{t}A_{j,j}^{k-1}B_{j}C_{j}\right) =\frac{\alpha^{k-1}}{t}\left(\sum_{j=0}^{t-1}a_{j+1}\cos\left(2 \pi(k-1)\frac{j}{t}\right)+\sum_{j=0}^{t-1}b_{j+1}\sin\left(2\pi(k-1)\frac{j}{ t}\right)\right)\] \[=(\phi(\mathbf{I})_{:t})_{k}\]

and

\[\|B\|_{2}=\left\|\frac{a-bi}{\sqrt{t}}\right\|_{2}\leq\frac{\|\phi(\mathbf{I}) _{:t}\|_{2}}{\alpha^{t-1}},\quad\|C\|_{2}=\sqrt{t\cdot\left(\frac{1}{\sqrt{t}} \right)^{2}}=1\,,\]

and by choosing \(\alpha=\left(\frac{1}{2}\right)^{\frac{1}{t-1}}\) we get the required assignment. 

### Proof of Proposition 4

For simplicity of notation, let \(A_{\mathbb{R}}^{(i)},B_{\mathbb{R}}^{(i)},C_{\mathbb{R}}^{(i)}\) be denoted by \(A^{(i)},B^{(i)},C^{(i)}\), respectively, and \(\ell(A^{(i)},B^{(i)},C^{(i)})\) by \(\ell^{(i)}\). We begin by writing the loss explicitly (see Appendix C):

\[\ell(A,B,C)=\sum_{j=1}^{t}\left(\sum_{k=1}^{n_{\mathbb{R}}}A_{k}^{j-1}B_{k}C_{ k}-\phi\left(\mathbf{I}\right)_{j}\right)^{2}\,.\]

Fixing any \(k\in[n_{\mathbb{R}}]\), we have

\[\frac{d\ell(A,B,C)}{dB_{k}}=2\sum_{j=1}^{t}A_{k}^{j-1}C_{k}\left(\sum_{m=1}^{n _{\mathbb{R}}}A_{m}^{j-1}B_{m}C_{m}-\phi\left(\mathbf{I}\right)_{j}\right)\,,\]

which, combined with the fact that \(|A_{m}|\leq 1\), leads to

\[\left|\frac{d\ell(A,B,C)}{dB_{k}}\right| \leq 2\left|C_{k}\right|\sum_{j=1}^{t}\left|\sum_{m=1}^{n_{ \mathbb{R}}}A_{m}^{j-1}B_{m}C_{m}-\phi\left(\mathbf{I}\right)_{j}\right|\] \[=2\left|C_{k}\right|\left\|\phi_{n,(A,B,C)}\left(\mathbf{I} \right)_{:t}-\phi\left(\mathbf{I}\right)_{:t}\right\|_{1}\] \[\leq 2\sqrt{t}\left|C_{k}\right|\left\|\phi_{n,(A,B,C)}\left( \mathbf{I}\right)_{:t}-\phi\left(\mathbf{I}\right)_{:t}\right\|_{2}\] \[\This implies

\[\|B^{(i)}\|_{\infty}\leq\|B^{(i-1)}\|_{\infty}+\eta^{(i)}\cdot 2\sqrt{t\cdot\ell^{(i- 1)}}\max\left(\left\|B^{(i-1)}\right\|_{\infty},\left\|C^{(i-1)}\right\|_{ \infty}\right)\,.\]

Next, we will demonstrate that

\[\eta^{(i)}\max\left(\left\|B^{(i-1)}\right\|_{\infty},\left\|C^{(i-1)}\right\|_ {\infty}\right)\leq\sqrt{c_{1}},\]

which concludes the proof since this aforementioned result will establish that for all \(i\in\mathbb{N}\), the following holds:

\[\|B^{(i)}\|_{\infty} \leq\|B^{(i-1)}\|_{\infty}+2\sqrt{t\ell^{(i-1)}c_{1}}\] \[\leq\|B^{(i-1)}\|_{\infty}+2\sqrt{tc_{2}\ell^{(0)}c_{1}} \tag{1}\] \[\leq\|B^{(0)}\|_{\infty}+2i\sqrt{tc_{2}\ell^{(0)}c_{1}}. \tag{2}\]

Where (1) holds because we assume \(\ell^{(i-1)}\leq c_{2}\ell^{(0)}\) and (2) is by recursion.

Repeating the same argument for \(C^{(i)}\) completes the proof.

Finally, let us show that \(\eta^{(i)}\max\left(\left\|B^{(i-1)}\right\|_{\infty},\left\|C^{(i-1)}\right\| _{\infty}\right)\leq\sqrt{c_{1}}\). Indeed, Lemma 3 combined with the assumptions that

\[\forall i\in\mathbb{N}:\eta^{(i)}\leq\frac{2}{\max\left\{\lambda_{max}\nabla^{ 2}\ell^{(i-1)},0\right\}}\]

and \(\forall i\in\mathbb{N}:\eta^{(i)}\leq c_{1}\) gives that

\[\eta^{(i)}\leq\min\left\{\frac{2}{2\max\left\{\left\|B^{(i-1)}\right\|_{\infty },\left\|C^{(i-1)}\right\|_{\infty}\right\}^{2}},c_{1}\right\}.\]

Considering the cases where \(\max\left\{\left\|B^{(i-1)}\right\|_{\infty},\left\|C^{(i-1)}\right\|_{\infty}\right\}\) is bigger and smaller than \(1/\sqrt{c_{1}}\) we immediately get the required result. 

**Lemma 3**.: _In the setting of Proposition 4 we have that for any \(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}}\) the following holds:_

\[\lambda_{max}\big{(}\nabla^{2}\ell\big{(}A_{\mathbb{R}},B_{\mathbb{R}},C_{ \mathbb{R}}\big{)}\big{)}\geq 2\max\left\{\left\|B_{\mathbb{R}}\right\|_{ \infty},\left\|C_{\mathbb{R}}\right\|_{\infty}\right\}^{2}\]

Proof.: For simplicity of notation, let \(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}}\) be denoted by \(A,B,C\), respectively. We will assume, without loss of generality, that \(\|C\|_{\infty}\geq\|B\|_{\infty}\), and denote \(i=\arg\max_{i}C_{i}\) and by \(e_{i}\) the one-hot vector with 1 at index \(i\) and zero elsewhere. We will also denote the following function

\[I_{i}(x)=\phi_{n_{\mathbb{R}},(A,B+x\cdot e_{i},C)}(\mathbf{I})_{:t}\]

by \(I_{i}(x)\), and the following loss function:

\[\tilde{\ell}_{i}(x)=\ell(I_{i}(x))=\tilde{\ell}(A,B+x\cdot e_{i},C)\]

by \(\tilde{\ell}_{i}\). Next, We notice that by definition of the Hessian, we have that

\[\left(\nabla^{2}\ell(A,\cdot,C)[B]\right)_{i,i}=\tilde{\ell}^{\prime\prime}_{ i}(0)\,.\]

Therefore, \(\lambda_{max}\left(\nabla^{2}\ell\right)\) at \(A,B,C\) is lower-bounded by \(\tilde{\ell}^{\prime\prime}_{i}(0)\). It is therefore sufficient to show that \(|\tilde{\ell}^{\prime\prime}_{i}(0)|\geq 2\|C\|_{\infty}^{2}\). Indeed, by Appendix C, the following holds\[\left|\tilde{\ell}_{i}^{\prime\prime}(0)\right| =\left|\frac{d\ell(A,B,C)}{\left(dB_{i}\right)^{2}}\right|\] \[=\left|\frac{d\left(\sum_{j=1}^{t}\left(\sum_{m=1}^{n_{\mathbb{R}} }A_{m}^{j-1}B_{m}C_{m}-\phi\left(\mathbf{I}\right)_{j}\right)^{2}\right)}{ \left(dB_{i}\right)^{2}}\right|\] \[=\left|\sum_{j=1}^{t}\frac{d}{dB_{i}}\left(\left(\sum_{m=1}^{n_{ \mathbb{R}}}A_{m}^{j-1}B_{m}C_{m}-\phi\left(\mathbf{I}\right)_{j}\right)\cdot 2 A_{i}^{j-1}C_{i}\right)\right|\] \[=\left|\sum_{j=1}^{t}2A_{i}^{j-1}C_{i}\cdot A_{i}^{j-1}C_{i}\right|\] \[\geq 2\|C\|_{\infty}^{2}\,,\]

which concludes the proof. 

### Proof of Proposition 5

For simplicity of notation, we will denote \(A_{\mathbb{R}},B_{\mathbb{R}},C_{\mathbb{R}},n_{\mathbb{R}}\) by \(A,B,C,n\). The robustness to \(q\)-quantization is by definition the probability:

\[\mathbb{P}\left[\left\|\phi_{n,(A\odot Q_{A},B\odot Q_{B},C\odot Q _{C})}(\mathbf{I})_{:t}-\phi(\mathbf{I})_{:t}\right\|_{1}\leq\epsilon\right]\] \[\leq\mathbb{P}\left[\left|\phi_{n,(A\odot Q_{A},B\odot Q_{B},C \odot Q_{C})}(\mathbf{I})_{1}-\phi(\mathbf{I})_{1}\right|\leq\epsilon\right]\,.\]

Let us denote the random variable \(\phi_{n,(A\odot Q_{A},B\odot Q_{B},C\odot Q_{C})}(\mathbf{I})_{1}\) by \(X\), and its PDF by \(p_{X}\). The probability \(\mathbb{P}\left[\left|X-Y\right|\leq\epsilon\right]\) is equal to the integral \(\int_{Y-\epsilon}^{Y+\epsilon}p_{X}(x)dx\leq 2\epsilon\max_{x\in\mathbb{R}}p_{X}(x)\). We get that

\[\mathbb{P}\left[\left\|\phi_{n,(A\odot Q_{A},B\odot Q_{B},C\odot Q _{C})}(\mathbf{I})-\phi(\mathbf{I})\right\|_{1}\leq\epsilon\right]\leq 2 \epsilon\max_{x\in\mathbb{R}}p_{X}(x)\,.\]

and so, to prove the theorem, it suffices to show that

\[\max_{x\in\mathbb{R}}p_{X}(x)\leq\frac{1}{q\|B\odot C\|_{\infty}}\,.\]

Let us investigate the random variable \(X\):

\[X =\sum_{i=1}^{n}\left(B\odot Q_{B}\right)_{i}\left(C\odot Q_{C} \right)_{i}=\sum_{i=1}^{n}B_{i}C_{i}(1+q_{B}{}_{i})(1+q_{C}{}_{i})\] \[=\sum_{i=1}^{n}B_{i}C_{i}(1+q_{B}{}_{i}+q_{C}{}_{i}+q_{B}{}_{i}q_ {C}{}_{i})\,,\]

where \(q_{B}{}_{i},q_{C}{}_{i}\sim\mathcal{U}(-q/2,q/2)\). As a sum of independent random variables, the maximum of \(p_{XAuxiliary Lemmas

**Lemma 4**.: _For any \(n\) non-negative numbers \(a_{1},\ldots,a_{n}\), and \(n\) real numbers \(b_{1},\ldots,b_{n}\), the function \(f\) defined below_

\[f(x)=\sum_{i=1}^{n}b_{i}{a_{i}}^{x}\]

_can have at most \(n-1\) zeros._

Proof.: We will show this by induction. The claim is obvious for \(n=1\), let us assume the induction claim is true for \(n=k\) and we will show that it is true for \(n=k+1\). Indeed, let \(f\) be the function defined by

\[f(x)=\sum_{i=1}^{k+1}b_{i}{a_{i}}^{x}.\]

If \(b_{1}=0\) or \(a_{1}=0\) we can immediately use the induction step on \(f\). Otherwise, we can write \(f\) in the following way:

\[f(x)=b_{1}a_{1}^{x}\left(1+\sum_{i=2}^{k+1}\frac{b_{i}}{b_{1}}\bigg{(}\frac{a_ {i}}{a_{1}}\bigg{)}^{x}\right)\,.\]

Since \(b_{1}a_{1}^{x}\) is always non-zero, the amount of roots of \(f\) is equal to the number of roots of the function \(g\) defined by

\[g(x)=1+\sum_{i=2}^{n+1}\frac{b_{i}}{b_{1}}\bigg{(}\frac{a_{i}}{a_{1}}\bigg{)}^ {x}\,.\]

By Rolle's theorem, the number of roots of a function is bounded by the number of roots of its derivative plus one. We have that \(g^{\prime}(x)\) is given by:

\[g^{\prime}(x)=\sum_{i=2}^{k+1}\frac{b_{i}}{b_{1}}\bigg{(}\frac{a_{i}}{a_{1}} \bigg{)}^{x}\ln\bigg{(}\frac{a_{i}}{a_{1}}\bigg{)}\,.\]

Which, by the induction step, can have at most \(k\) roots, as needed. 

**Lemma 5**.: _Let \(\theta\) be a real number in the open interval \((0,2\pi)\), let \(b\) be a real number in \([0,2\pi]\), and let \(p\) be a natural number such that \(\theta\cdot p\geq 2\pi\). Then the set_

\[\{(b+\theta)\bmod 2\pi,(b+2\theta)\bmod 2\pi,\ldots,(b+p\cdot\theta)\bmod 2\pi\}\]

_contains at least one point in any closed and continuous-mod-\(2\pi\) interval of length \(\theta\) within \([0,2\pi]\)._

Proof.: Let \(x_{k}=(b+k\theta)\) for \(k=1,2,\ldots,p\). Define the intervals

\[I_{k}=[x_{k-1},x_{k})\bmod 2\pi\quad\text{for}\quad k=1,2,\ldots,p\;.\]

Each interval \(I_{k}\) has a length of \(\theta\). Since \(p\theta\geq 2\pi\) we have that

\[\bigcup_{k=1}^{p}I_{k}=[0,2\pi].\]

Now, consider any closed and continuous-mod-\(2\pi\) interval 

[MISSING_PAGE_EMPTY:28]

[MISSING_PAGE_EMPTY:29]

Proof.: Using the Stirling's formula for factorials, we know that \(n!\) is bounded by:

\[\sqrt{2\pi n}\left(\frac{n}{e}\right)^{n}e^{\left(\frac{1}{2n}-\frac{1}{360n^{3} }\right)}<n!<\sqrt{2\pi n}\left(\frac{n}{e}\right)^{n}e^{\frac{1}{2n}}.\]

which can be simplified to:

\[\sqrt{2\pi n}\left(\frac{n}{e}\right)^{n}<n!<\sqrt{2\pi n}\left(\frac{n}{e} \right)^{n}e^{\frac{1}{2n}}.\]

We can plug the bound in the factorial form of the binomial coefficients:

\[\binom{2n}{n}=\frac{(2n)!}{n!\cdot n!}>\frac{\sqrt{2\pi 2n}\left(\frac{2n}{e} \right)^{2n}e^{\frac{1}{6n}}}{(\sqrt{2\pi n})^{2}\left(\frac{n}{e}\right)^{2n} e^{\frac{1}{6n}}}=\frac{2^{2n}}{\sqrt{\pi n}e^{\frac{1}{6n}}}>\frac{2^{2n}}{ 2\sqrt{n}}\]

Where the last inequality is due to \(\sqrt{\pi}e^{\frac{1}{6n}}<2\) for all \(n\geq 2\). To complete the proof for all positive integers notice we get equality for \(n=1\). 

**Lemma 11**.: _For any positive integer \(n\), the following inequalitie hold:_

\[\binom{n}{\lfloor\frac{n}{2}\rfloor}\geq\frac{2^{n}}{4\sqrt{2n}}\]

Proof.: Utilizing Lemma 10

we have

\[\binom{2m}{m}\geq\frac{2^{2m}}{2\sqrt{m}}.\]

To validate the lemma, we consider cases based on the parity of \(n\). For even \(n\),

\[\binom{n}{\lfloor\frac{n}{2}\rfloor}=\binom{n}{\frac{n}{2}}\geq\frac{2^{n}}{2 \sqrt{\frac{n}{2}}}>\frac{2^{n}}{4\sqrt{2n}},\]

and for odd \(n\),

\[\binom{n}{\lfloor\frac{n}{2}\rfloor}=\binom{n}{\frac{n-1}{2}}\geq\binom{n-1}{ \frac{n-1}{2}}\geq\frac{2^{n-2}}{2\sqrt{\frac{n-1}{2}}}\geq\frac{2^{n}}{4\sqrt {2n}},\]

thereby proving the lemma as required. 

**Lemma 12**.: _For any positive integers \(n\) and \(m\), it holds that_

\[\left(\frac{m}{n+m}\right)^{m}\!\!\left(\frac{n}{n+m}\right)^{n}\leq\frac{1}{2 ^{2\min(m,n)}}\]

Proof.: We establish this inequality through direct analysis:

\[\left(\frac{m}{n+m}\right)^{m}\!\left(\frac{n}{n+m}\right)^{n} \leq\left(\frac{m}{n+m}\right)^{\min(m,n)}\left(\frac{n}{n+m} \right)^{\min(m,n)}\] \[=\left(\frac{mn}{\left(n+m\right)^{2}}\right)^{\min(m,n)}\] \[\leq\left(\frac{mn}{4nm}\right)^{\min(m,n)}\] \[=\frac{1}{2^{2\min(m,n)}}\]

This last inequality leverages the fact that \(\left(n+m\right)^{2}-4nm=\left(n-m\right)^{2}>0\). 

**Lemma 13**.: _For any complex number \(x=a+bi\) with \(a\leq 0\), it holds that_

\[\left|\frac{1}{1-x}\right|\leq 1.\]

Proof.: It suffices to show that \(|1-x|\geq 1\). Indeed, we have

\[|1-x|=\sqrt{(1-a)^{2}+b^{2}}\geq\sqrt{(1-a)^{2}}=|1-a|=1-a\geq 1,\]

as required.

## Appendix F Implementation Details

Appendices F.1 to F.3 below present implementation details omitted from Sections 4.1 to 4.3, respectively. Source code for reproducing the results of Sections 4.1 and 4.3 can be found at [https://github.com/edenlum/SSMComplexParamBenefits](https://github.com/edenlum/SSMComplexParamBenefits). The results of Section 4.2 were obtained using the official S4 repository, available at [https://github.com/state-spaces/s4](https://github.com/state-spaces/s4) (Apache-2.0 license). All experiments were conducted on a single NVIDIA A6000 GPU.

### Theoretically Analyzed Setting

In all experiments, we parameterized state transition matrices in a manner that ensures stability, similarly to the LRU architecture [41]. For real SSMs, we performed a grid search for each optimizer, varying learning rates and initialization schemes. Namely, we evaluated learning rates of \(1\times 10^{-4},1\times 10^{-5}\) and \(1\times 10^{-6}\), and randomly initialized the diagonal elements of \(A_{\mathbb{R}}\) uniformly in \([-1,1]\) or in \([-1,0.99]\cup[0.99,1]\). For complex SSMs, we used a learning rate of \(1\times 10^{-5}\) and initialized the diagonal elements of \(A_{\mathbb{C}}\) similarly to [41], by sampling uniformly from the complex ring with radii \(0.99\) to \(1\). For all SSMs, we employed a cosine learning rate scheduler [35] and trained for half a million steps.

### Real-World Setting

Our implementation is based on the official S4 repository, available at [https://github.com/state-spaces/s4](https://github.com/state-spaces/s4) (Apache-2.0 license). Unless stated otherwise, repository hyperparameters (pertaining to the neural network architecture and its training) were kept at their default values. The SSM powering the architecture was adapted to a diagonal structure for the state transition matrix \(A\). For real SSMs, we began with the same default configuration as for complex SSMs, and extended the configuration by performing a grid search over different optimizers (Adam [29], AdamW [36], SGD [45]) and initialization schemes (diagonal-real, diagonal-linear, and legs, as defined in the official S4 repository). We ran three random seeds with the default configuration, and a single random seed with all others.

### Selectivity

Copy task.Our version of copy task has a delay of \(t=64\). It entails input sequences of length \(2t\), where tokens are sampled independently and uniformly as one-hot vectors of dimension \(16\). The corresponding output sequences are generated by shifting the input sequences \(t\) positions to the right, while introducing zero padding. That is, given an input sequence \([a_{1},a_{2},\dots,a_{2t}]\), the corresponding output sequence is \([0,0,\dots,0,a_{1},a_{2},\dots,a_{t}]\). Training and evaluation (measurement of test accuracy) in this task are based on randomly generated examples, where the input is sampled from a uniform distribution. In training, a fresh batch of examples is generated for each iteration, and the loss corresponding to an example is the cross-entropy loss averaged over the last \(t\) tokens of the output. In evaluation, the zero-one loss is averaged over the last \(t\) tokens of the output across one thousand freshly generated examples.

Induction-head task.In the induction-head task [18], the goal is to teach a model to identify and copy a specific subsequence of the input. Figure 1 illustrates this task, which in our case has an induction head size of \(h=128\). The task entails input sequences of length \(3h\), _i.e._, input sequences with \(3h\) tokens. The first (less than \(h\)) tokens in an input sequence are sampled independently and uniformly as one-hot vectors of dimension \(16\). These tokens are followed by a special _copy token_,

Figure 1: Illustration of the induction-head task. See Appendix F.3 for details.

\(c=\mathbf{0}\). After the copy token comes a sequence of \(h\) tokens, sampled as before and denoted \(\mathbf{X}\). The \(2h\)th token is another copy token \(c\), after which the first to penultimate tokens of the sequence \(\mathbf{X}\) repeat. The tokens between the first appearance of \(\mathbf{X}\) and the second appearance of \(c\) are irrelevant for the task. The output sequence corresponding to the above-described input sequence is equal to the input sequence shifted one position to the left (with the last token of \(\mathbf{X}\) placed on the right). Training and evaluation (measurement of test accuracy) in this task are based on randomly generated examples, where the following aspects of the input are sampled from uniform distributions: the location of the first copy token \(c\), the sequence \(\mathbf{X}\), and the irrelevant tokens. In training, a fresh batch of examples is generated for each iteration, and the loss corresponding to an example is the cross-entropy loss averaged over the last \(h\) tokens of the output. In evaluation, the zero-one loss is averaged over the last \(h\) tokens of the output across one thousand freshly generated examples.

Hyperparameters.Our implementation is based on a widely adopted Mamba repository, available at [https://github.com/alxmdrTL/mamba.py](https://github.com/alxmdrTL/mamba.py) (MIT license). Unless stated otherwise, repository hyperparameters (pertaining to the neural network architecture and its training) were kept at their default values. The Mamba neural network was configured to have two layers with a hidden dimension of \(64\). We trained the network with a batch size of \(8\) and a learning rate of \(1\times 10^{-3}\). Training comprised \(1{,}000{,}000\) steps on the copy task, and \(250{,}000\) steps on the induction-head task (the latter task warranted a shorter run-time since it was experimented with much more). For complex parameterization, we changed the state transition matrix \(A\), the input matrix \(B\) and the output matrix \(C\) to be complex, while keeping the discretization parameter \(\Delta\) real. To maintain parameter count, the dimension of SSMs with complex parameterization was half the dimension with real parameterization, namely, \(8\) as opposed to \(16\).

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: The abstract and introduction clearly differentiate between formal contributions and informal discussions.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: The paper includes a dedicated limitations section (Section 6).
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes] Justification: Every theoretical result is formally stated (as a lemma, proposition, theorem or corollary) and proven.
4. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: The paper provides (in Section 4 and Appendix F) references to code for reproducing experiments.
5. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: The paper provides (in Section 4 and Appendix F) a detailed account of implementation details.
6. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: The paper reports (in Section 4 and Appendix F) minimum and maximum values across multiple random seeds, while specifying the number of seeds.
7. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: The paper specifies (in Appendix F) the type of hardware used for compute, as well as details determining the computational load of each experiment (number of training steps, neural network architecture _etc._).
8. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?)Answer: [Yes] Justification: The paper is theoretical in nature, with experiments on synthetic or standard datasets designed to corroborate the theory. As such, it inherently conforms with the NeurIPS Code of Ethics.
9. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The paper is theoretical in nature, with experiments on synthetic or standard datasets designed to corroborate the theory. As such, its societal impacts are self explanatory.
10. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA]. Justification: The paper does not offer data or models that have a high risk for misuse.
11. **Licensees for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The paper specifies (in Section 4 and Appendix F) all assets used, along with applicable license details.
12. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: The paper is accompanied by code for reproducing experiments ([https://github.com/edenlum/SSMComplexParamBenefits](https://github.com/edenlum/SSMComplexParamBenefits)), and this code is well documented.
13. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not include crowdsourcing experiments or research with human subjects.
14. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not include research with human subjects.