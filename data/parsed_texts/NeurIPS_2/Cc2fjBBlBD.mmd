# Spuriosity Didn't Kill the Classifier: Using Invariant Predictions to Harness Spurious Features

Cian Eastwood\({}^{*1,2}\)   Shashank Singh\({}^{*1}\)

Andrei L. Nicolicioiu\({}^{1}\)   Marin Vlastelica\({}^{1}\)   Julius von Kugelgen\({}^{1,3}\)   Bernhard Scholkopf\({}^{1}\)

\({}^{1}\) Max Planck Institute for Intelligent Systems, Tubingen

\({}^{2}\) University of Edinburgh  \({}^{3}\) University of Cambridge

Equal contribution. Correspondence to c.eastwood@ed.ac.uk or shashankssingh44@gmail.com.

###### Abstract

To avoid failures on out-of-distribution data, recent works have sought to extract features that have an invariant or _stable_ relationship with the label across domains, discarding "spurious" or _unstable_ features whose relationship with the label changes across domains. However, unstable features often carry _complementary_ information that could boost performance if used correctly in the test domain. In this work, we show how this can be done _without test-domain labels_. In particular, we prove that pseudo-labels based on stable features provide sufficient guidance for doing so, provided that stable and unstable features are conditionally independent given the label. Based on this theoretical insight, we propose Stable Feature Boosting (SFB), an algorithm for: (i) learning a predictor that separates stable and conditionally-independent unstable features; and (ii) using the stable-feature predictions to adapt the unstable-feature predictions in the test domain. Theoretically, we prove that SFB can learn an asymptotically-optimal predictor without test-domain labels. Empirically, we demonstrate the effectiveness of SFB on real and synthetic data.

## 1 Introduction

Machine learning systems can be sensitive to distribution shift [26]. Often, this sensitivity is due to a reliance on "spurious" features whose relationship with the label changes across domains, ultimately leading to degraded performance in the test domain of interest [21]. To avoid this pitfall, recent works on domain or out-of-distribution (OOD) generalization have sought predictors which only make use of features that have a _stable_ or invariant relationship with the label across domains, discarding the spurious or _unstable_ features [45, 1, 35, 15]. However, despite their instability, spurious features can often provide additional or _complementary_ information about the target label. Thus, if a predictor could be adjusted to use spurious features optimally in the test domain, it would boost performance substantially. That is, perhaps we don't need to discard spurious features at all but rather _use them in the right way_.

As a simple but illustrative example, consider the ColorMNIST or CMNIST dataset [1]. This transforms the original MNIST dataset into a binary classification task (digit in 0-4 or 5-9) and then: (i) flips the label with probability 0.25, meaning that, across all 3 domains, digit shape correctly determines the label with probability 0.75; and (ii) colorizes the digit such that digit color (red or green) is a more informative but spurious feature (see Fig. 0(a)). Prior work focused on learning an invariant predictor that uses only shape and avoids using color--a spurious feature whose relationship with the label changes across domains. However, as shown in Fig. 0(b), the invariant predictor is suboptimal test domains where color can be used in a domain-specific manner to improve performance. We thus ask: when and how can such informative but spurious features be safely harnessed _without labels_?

[MISSING_PAGE_FAIL:2]

**Test-domain adaptation _with labels_ (few-shot fine-tuning).** Fine-tuning part of a model using a small number of labeled test-domain examples is a common way to deal with distribution shift [16; 17; 13]. More recently, it has been shown that simply retraining the last layer of an ERM-trained model outperforms more robust feature-learning methods on spurious correlation benchmarks [50; 32; 74]. Similar to our approach, Jiang and Veitch [31] separate stable and conditionally-independent unstable features and then adapt their use of the latter in the test domain. However, in contrast to our approach, theirs requires test-domain labels. In addition, they assume data is drawn from an anti-causal generative model, which is strictly stronger than our "complementarity" assumption (see SS 4).

Table 1 summarizes related work while App. H discusses further related work.

## 3 Problem Setup: Extracting and Harnessing Unstable Features

**Setup.** We consider the problem of domain generalization (DG) [8; 42; 24] where predictors are trained on data from multiple training domains and with the goal of performing well on data from unseen test domains. More formally, we consider datasets \(D^{e}=\{(X^{e}_{i},Y^{e}_{i})\}_{i=1}^{n_{e}}\) collected from \(m\) different training domains or _environments_\(\mathcal{E}_{\text{tr}}:=\{E_{1},\ldots,E_{m}\}\), with each dataset \(D^{e}\) containing data pairs \((X^{e}_{i},Y^{e}_{i})\) sampled i.i.d. from \(\mathds{P}(X^{e},Y^{e})\).2 The goal is then to learn a predictor \(f(X)\) that performs well on a larger set \(\mathcal{E}_{\text{all}}\supset\mathcal{E}_{\text{tr}}\) of possible domains.

Footnote 2: We drop the domain superscript \(e\) when referring to random variables from any environment.

**Average performance: use all features.** The first approaches to DG sought predictors that perform well _on average_ over domains [8; 42] using empirical risk minimization (ERM, [66]). However, predictors that perform well on average provably lack robustness [43; 49], potentially performing quite poorly on large subsets of \(\mathcal{E}_{\text{all}}\). In particular, minimizing the average error leads predictors to make use of any features that are informative about the label (on average), including "spurious" or "shortcut" [21] features whose relationship with the label is subject to change across domains. In test domains where these feature-label relationships change in new or more severe ways than observed during training, this usually leads to significant performance drops or even complete failure [73; 4].

**Worst-case or robust performance: use only stable features.** To improve robustness, subsequent works sought predictors that only use _stable or invariant_ features, i.e., those that have a stable or invariant relationship with the label across domains [45; 1; 47; 70; 58]. For example, Arjovsky et al. [1] do so by enforcing that the classifier on top of these features is optimal for all domains simultaneously. We henceforth use _stable features_ and \(X_{S}\) to refer to these features, and stable predictors to refer to predictors which use only these features. Analogously, we use _unstable features_ and \(X_{U}\) to refer to features with an unstable or "spurious" relationship with the label across domains. Note that \(X_{S}\) and \(X_{U}\) partition the components of \(X\) which are informative about \(Y\), as depicted in Fig. 0(c), and that formal definitions of \(X_{S}\) and \(X_{U}\) are provided in SS 4.

### Harnessing unstable features _with labels_

A stable predictor \(f_{S}\) is unlikely to be the best predictor in any given domain. As illustrated in Fig. 0(b), this is because it excludes unstable features \(X_{U}\) which are informative about \(Y\) and can boost performance _if used in an appropriate, domain-specific manner_. Assuming we can indeed learn a stable predictor with prior methods, we start by showing how \(X_{U}\) can be harnessed _with test-domain labels_.

\begin{table}
\begin{tabular}{l c c c|c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{4}{c}{**Components of \(X\) Used**} \\ \cline{2-6}  & **Stable** & **Complementary** & **All** & **Robust** & **No test-domain labels** \\ \hline ERM [65] & & & & & \\ IRM [1] & & & & & \\ QRM [15] & & & & & \\ DARE [50] & & & & & \\ ACTIR [31] & & & & & \\ \hline SFB (**Ours**) & & & & & \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Comparison with related work.**\({}^{*}\)QRM [15] uses a hyperparameter \(\alpha\in[0,1]\) to balance the probability of robust generalization and using more information from \(X\).

**Boosting the stable predictor.** We describe boosted joint predictions \(f^{e}(X)\) in domain \(e\) as some combination \(C\) of stable predictions \(f_{S}(X)\) and domain-specific unstable predictions \(f^{e}_{U}(X)\), i.e., \(f^{e}(X)=C(f_{S}(X),f^{e}_{U}(X))\). To allow us to adapt only the \(X_{U}\)-\(Y\) relation, we decompose the stable \(f_{S}(X)=h_{S}(\Phi_{S}(X))\) and unstable \(f^{e}_{U}(X)=h^{e}_{U}(\Phi_{U}(X))\) predictions into feature extractors \(\Phi\) and classifiers \(h\). \(\Phi_{S}\) extracts stable components \(X_{S}=\Phi_{S}(X)\) of \(X\), \(\Phi_{U}\) extracts unstable components \(X_{U}=\Phi_{U}(X)\) of \(X\), \(h_{S}\) is a classifier learned on top of \(\Phi_{S}\) (shared across domains), and \(h^{e}_{U}\) is a _domain-specific_ unstable classifier learned on top of \(\Phi_{U}\) (one per domain). Putting these together,

\[f^{e}(X)=C(f_{S}(X),f^{e}_{U}(X))=C(h_{S}(\Phi_{S}(X)),h^{e}_{U}(\Phi_{U}(X))) =C(h_{S}(X_{S}),h^{e}_{U}(X_{U})), \tag{3.1}\]

where \(C:[0,1]\times[0,1]\to[0,1]\) is a _combination function_ that combines the stable and unstable predictions. For example, Jiang and Veitch [31, Eq. 2.1] add stable \(p_{S}\) and unstable \(p_{U}\) predictions in logit space, i.e., \(C(p_{S},p_{U})=\sigma(\text{logit}(p_{S})+\text{logit}(p_{U}))\). Since it is unclear, _a priori_, how to choose \(C\), we will leave it unspecified until Thm. 4.4 in SS 4, where we derive a principled choice.

**Adapting with labels.** Given a new domain \(e\) and labels \(Y^{e}\), we can boost performance by adapting \(h^{e}_{U}\). Specifically, letting \(\ell:\mathcal{Y}\times\mathcal{Y}\to\mathds{R}\) be a loss function (e.g., cross-entropy) and \(R^{e}(f)=\mathbf{E}_{(X,Y)}\left[\ell(Y,f(X))|E=e\right]\) the risk of predictor \(f:\mathcal{X}\to\mathcal{Y}\) in domain \(e\), we can adapt \(h^{e}_{U}\) to solve:

\[\min_{h_{U}}R^{e}(C(h_{S}\circ\Phi_{S},h_{U}\circ\Phi_{U})) \tag{3.2}\]

### Harnessing unstable features _without labels_

We now consider the main question of this work--can we reliably harness \(X_{U}\)_without_ test-domain labels? We could, of course, simply select a _fixed_ unstable classifier \(h^{e}_{U}\) by relying solely on the training domains (e.g., by minimizing average error), and hope that this works for the test-domain \(X_{U}\)-\(Y\) relation. However, by definition of \(X_{U}\) being unstable, this is clearly not a robust or reliable approach--the focus of our efforts in this work, as illustrated in Table 1. As in SS 3.1, we assume that we are able to learn a stable predictor \(f_{S}\) using prior methods, e.g., IRM [1] or QRM [15].

**From stable predictions to robust pseudo-labels.** While we don't have labels in the test domain, we _do_ have stable predictions. By definition, these are imperfect (i.e., _noisy_) but robust, and can be used to form _pseudo-labels_\(\widehat{Y}_{i}=\arg\max_{j}(f_{S}(X_{i}))_{j}\), with \((f_{S}(X_{i}))_{j}\approx\Pr[Y_{i}=j|X_{S}]\) denoting the \(j^{\text{th}}\) entry of the stable prediction for \(X_{i}\). Can we somehow use these noisy but robust pseudo-labels to guide our updating of \(h^{e}_{U}\), and, ultimately, our use of \(X_{U}\) in the test domain?

**From joint to unstable-only risk.** If we simply use our robust pseudo-labels as if they were true labels--updating \(h^{e}_{U}\) to minimize the joint risk as in Eq. (3.2)--we arrive at trivial solutions since \(f_{S}\) already predicts its own pseudo-labels with 100% accuracy. For example, if we follow [31, Eq. 2.1] and use the combination function \(C(p_{S},p_{U})=\sigma(\text{logit}(p_{S})+\text{logit}(p_{U}))\), then the trivial solution \(\text{logit}(h^{e}_{U}(\cdot))=0\) achieves 100% accuracy (and minimizes cross-entropy; see Prop. D.1 of App. D). Thus, we cannot minimize a joint loss involving \(f_{S}\)'s predictions when using \(f_{S}\)'s pseudo-labels. A sensible alternative is to update \(h^{e}_{U}\) to minimize the _unstable-only risk_\(R^{e}(h^{e}_{U}\circ\Phi_{U})\).

**More questions than answers.** While this new procedure _could_ work, it raises questions about _when_ it will work, or, more precisely, the conditions under which it can be used to safely harness \(X_{U}\). We now summarise these questions before addressing them in SS 4:

1. **Does it make sense to minimize the unstable-only risk?** In particular, when can we minimize the unstable-predictor risk _alone_ or separately, and then arrive at the optimal joint predictor? This cannot always work; e.g., for independent \(X_{S}\), \(X_{U}\sim\text{Bernoulli}(1/2)\) and \(Y=X_{S}\) XOR \(X_{U}\), \(Y\) is independent of each of \(X_{S}\) and \(X_{U}\) and hence cannot be predicted from either alone.
2. **How should we combine predictions?** Is there a principled choice for the combination function \(C\) in Eq. (3.1)? In particular, is there a \(C\) that correctly weights stable and unstable predictions in the test domain? As \(X_{U}\) could be very strongly or very weakly predictive of \(Y\) in the test domain, this seems a difficult task. Intuitively, correctly weighting stable and unstable predictions requires them to be properly calibrated: do we have any reason to believe that, after training on \(f_{S}\)'s pseudo-labels, \(h^{e}_{U}\) will be properly calibrated in the test domain?
3. **Can the student outperform the teacher?** Stable predictions likely make mistakes--indeed, this is the motivation for trying to improve them. Is it possible to correct these mistakes with \(X_{U}\)? Is itpossible to learn an unstable "student" predictor that outperforms its own supervision signal or "teacher"? Perhaps surprisingly, we show that, for certain types of features, the answer is yes. In fact, even a very weak stable predictor, with performance just above chance, can be used to learn an _optimal_ unstable classifier in the test domain given enough unlabeled data.

## 4 Theory: When Can We Safely Harness Unstable Features Without Labels?

Suppose we have already identified a stable feature \(X_{S}\) and a potentially unstable feature \(X_{U}\) (we will return to the question of how to learn/extract \(X_{S}\) and \(X_{U}\) themselves in SS 5). In this section, we analyze the problem of using \(X_{S}\) to leverage \(X_{U}\) in the test domain without labels. We first reduce this to a special case of the so-called "marginal problem" in probability theory, i.e., the problem of identifying a joint distribution based on information about its marginals. In the special case where two variables are conditionally independent given a third, we show this problem can be solved exactly. This solution, which may be of interest beyond the context of domain generalization/adaptation, motivates our test-domain adaptation algorithm (Alg. 1), and forms the basis of Thm. 4.6 which shows that Alg. 1 converges to the best possible classifier given enough unlabeled data.

We first pose a population-level model of our domain generalization setup. Let \(E\) be a random variable denoting the _environment_. Given an environment \(E\), we have that the stable feature \(X_{S}\), unstable feature \(X_{U}\) and label \(Y\) are distributed according to \(P_{X_{S},X_{U},Y|E}\). We can now formalize the three key assumptions underlying our approach, starting with the notion of a stable feature, motivated in SS 3:

**Definition 4.1** (Stable and Unstable Features).: \(X_{S}\) _is a stable feature with respect to \(Y\) if \(P_{Y|X_{S}}\) does not depend on \(E\); equivalently, if \(Y\) and \(E\) are conditionally independent given \(X_{S}\) (\(Y\perp\!\!\!\perp E|X_{S}\)). Conversely, \(X_{U}\) is an unstable feature with respect to \(Y\) if \(P_{Y|X_{U}}\) depends on \(E\); equivalently, if \(Y\) and \(E\) are conditionally dependent given \(X_{U}\) (\(Y\not\perp\!\!\perp E|X_{U}\))._

Next, we state our complementarity assumption, which we will show justifies the approach of separately learning the relationships \(X_{S}\)-\(Y\) and \(X_{U}\)-\(Y\) and then combining them:

**Definition 4.2** (Complementary Features).: \(X_{S}\) _and \(X_{U}\) are complementary features with respect to \(Y\) if \(X_{S}\perp\!\!\!\perp X_{U}|(Y,E)\); i.e., if \(X_{S}\) and \(X_{U}\) share no redundant information beyond \(Y\) and \(E\)._

Finally, to provide a useful signal for test-domain adaptation, the stable feature needs to help predict the label in the test domain. Formally, we assume:

**Definition 4.3** (Informative Feature).: \(X_{S}\) _is said to be informative of \(Y\) in environment \(E\) if \(X_{S}\not\perp\!\!\!\perp Y|E\); i.e., \(X_{S}\) is predictive of \(Y\) within the environment \(E\)._

We will discuss the roles of these assumptions after stating our main result (Thm. 4.4) that uses them. To keep our results as general as possible, we avoid assuming a particular causal generative model, but the above conditional (in)dependence assumptions can be interpreted as constraints on such a causal model. App. D.2 formally characterizes the set of causal models that are consistent with our assumptions and shows that our setting generalizes those of prior works [49; 68; 31; 69].

**Reduction to the marginal problem with complementary features.** By Defn. 4.1, we have the same stable relationship \(P_{Y|X_{S},E}=P_{Y|X_{S}}\) in training and test domains. Now, suppose we have used the training data to learn this stable relationship and thus know \(P_{Y|X_{S}}\). Also suppose that we have enough unlabeled data from test domain \(E\) to learn \(P_{X_{S},X_{U}|E}\), and recall that our ultimate goal is to predict \(Y\) from \((X_{S},X_{U})\) in test domain \(E\). Since the rest of our discussion is conditioned on \(E\) being the test domain, we omit \(E\) from the notation. Now note that, if we could express \(P_{Y|X_{S},X_{U}}\) in terms of \(P_{Y|X_{S}}\) and \(P_{X_{S},X_{U}}\), we could then use \(P_{Y|X_{S},X_{U}}\) to optimally predict \(Y\) from \((X_{S},X_{U})\). Thus, our task thus becomes to reconstruct \(P_{Y|X_{S},X_{U}}\) from \(P_{Y|X_{S}}\) and \(P_{X_{S},X_{U}}\). This is an instance of the classical "marginal problem" from probability theory [28; 29; 19], which asks under which conditions we can recover the joint distribution of a set of random variables given information about its marginals. In general, although one can place bounds on the conditional distributions \(P_{Y|X_{U}}\) and \(P_{Y|X_{S},X_{U}}\), they cannot be completely inferred from \(P_{Y|X_{S}}\) and \(P_{X_{S},X_{U}}\)[19]. However, the following section demonstrates that, _under the additional assumptions that \(X_{S}\) and \(X_{U}\) are complementary and \(X_{S}\) is informative_, we can exactly recover \(P_{Y|X_{U}}\) and \(P_{Y|X_{S},X_{U}}\) from \(P_{Y|X_{S}}\) and \(P_{X_{S},X_{U}}\).

### Solving the marginal problem with complementary features

We now present our main result which shows how to reconstruct \(P_{Y|X_{S},X_{U}}\) from \(P_{Y|X_{S}}\) and \(P_{X_{S},X_{U}}\) when \(X_{S}\) and \(X_{U}\) are complementary and \(X_{S}\) is informative. To simplify notation, we assume the label \(Y\) is binary and defer the multi-class extension to App. C.

**Theorem 4.4** (Solution to the marginal problem with binary labels and complementary features).: _Consider three random variables \(X_{S}\), \(X_{U}\), and \(Y\), where (i) \(Y\) is binary (\(\{0,1\}\)-valued), (ii) \(X_{S}\) and \(X_{U}\) are complementary features for \(Y\) (i.e., \(X_{S}\perp\!\!\!\perp X_{U}|Y\)), and (iii) \(X_{S}\) is informative of \(Y\) (\(X_{S}\perp\!\!\!\perp Y\)). Then, the joint distribution of \((X_{S},X_{U},Y)\) can be written in terms of the joint distributions of \((X_{S},Y)\) and \((X_{S},X_{U})\). Specifically, if \(\widehat{Y}|X_{S}\sim\text{Bernoulli}(\Pr[Y=1|X_{S}])\) is a pseudo-label3 and_

Footnote 3: Our _stochastic_ pseudo-labels differ from hard (\(\widehat{Y}=1\{\Pr[Y=1|X_{S}]>1/2\}\)) pseudo-labels often used in practice [20, 36, 52]. By capturing irreducible error in \(Y\), stochastic pseudo-labels ensure \(\Pr[Y|X_{U}]\) is well-calibrated, allowing us to combine \(\Pr[Y|X_{S}]\) and \(\Pr[Y|X_{U}]\) in Eq. (4.4).

\[\epsilon_{0}:=\Pr[\widehat{Y}=0|Y=0]\quad\text{ and }\quad\epsilon_{1}:=\Pr[ \widehat{Y}=1|Y=1] \tag{4.1}\]

_are the accuracies of the pseudo-labels on classes \(0\) and \(1\), respectively. Then, we have:_

\[\epsilon_{0}+\epsilon_{1}>1, \tag{4.2}\]

\[\Pr[Y=1|X_{U}]=\frac{\Pr[\widehat{Y}=1|X_{U}]+\epsilon_{0}-1}{\epsilon_{0}+ \epsilon_{1}-1},\qquad\text{and} \tag{4.3}\]

\[\Pr[Y=1|X_{S},X_{U}]= \,\sigma\left(\text{logit}(\Pr[Y=1|X_{S}])+\text{logit}(\Pr[Y=1|X_ {U}])-\text{logit}(\Pr[Y=1])\right). \tag{4.4}\]

Intuitively, suppose we generate pseudo-labels \(\widehat{Y}\) based on feature \(X_{S}\) and train a model to predict \(\widehat{Y}\) using feature \(X_{U}\). For complementary \(X_{S}\) and \(X_{U}\), Eq. (4.3) shows how to transform this into a prediction of the _true_ label \(Y\), correcting for differences between \(\widehat{Y}\) and \(Y\). Crucially, given the conditional distribution \(P_{Y|X_{S}}\) and observations of \(X_{S}\), we can estimate class-wise pseudo-label accuracies \(\epsilon_{0}\) and \(\epsilon_{1}\) in Eq. (4.3) even without new labels \(Y\) (see App. A.1, Eq. (A.2)). Finally, Eq. (4.4) shows how to weight predictions based on \(X_{S}\) and \(X_{U}\), justifying the combination function

\[C_{p}(p_{S},p_{U})=\sigma(\text{logit}(p_{S})+\text{logit}(p_{U})-\text{logit }(p)) \tag{4.5}\]

in Eq. (3.1), where \(p=\Pr[Y=1]\) is a constant independent of \(x_{S}\) and \(x_{U}\). We now sketch the proof of Thm. 4.4, elucidating the roles of informativeness and complementarity (full proof in App. A.1).

Proof Sketch of Thm. 4.4.: We prove Eq. (4.2), Eq. (4.3), and Eq. (4.4) in order.

Proof of Eq. (4.2):The informativeness condition (iii) is equivalent to the pseudo-labels having predictive accuracy above random chance; formally, App. A.1 shows:

**Lemma 4.5**.: \(\epsilon_{0}+\epsilon_{1}>1\) _if and only if \(X_{S}\) is informative of \(Y\) (i.e., \(X_{S}\not\perp\!\!\perp Y\))._

Together with Eq. (4.3), it follows that _any_ dependence between \(X_{S}\) and \(Y\) allows us to fully learn the relationship between \(X_{U}\) and \(Y\), affirmatively answering our question from SS 3: _Can the student outperform the teacher?_ While a stronger relationship between \(X_{S}\) and \(Y\) is still helpful, it only improves the (unlabeled) _sample complexity_ of learning \(P_{Y|X_{U}}\) and not _consistency_ (Thm. 4.6 below), mirroring related results in the literature on learning from noisy labels [44, 7]. In particular, a weak relationship corresponds to \(\epsilon_{0}+\epsilon_{1}\approx 1\), increasing the variance of the bias-correction in Eq. (4.3). With a bit more work, one can formalize this intuition to show that our approach has a relative statistical efficiency of \(\epsilon_{0}+\epsilon_{1}-1\in[0,1]\), compared to using true labels \(Y\).

Proof of Eq. (4.3):The key observation behind the bias-correction (Eq. (4.3)) is that, due to complementarity (\(X_{S}\perp\!\!\!\perp X_{U}|Y\)) and the fact that the pseudo-label \(\widehat{Y}\) depends only on \(X_{S}\), \(\widehat{Y}\) is conditionally independent of \(X_{U}\) given the true label \(Y\) (\(\widehat{Y}\perp\!\!\perp X_{U}|Y\)); formally:

\[\Pr[\widehat{Y}=1|X_{U}] =\Pr[\widehat{Y}=1|Y=0,X_{U}]\Pr[Y=0|X_{U}]\] \[\qquad+\Pr[\widehat{Y}=1|Y=1,X_{U}]\Pr[Y=1|X_{U}]\quad\text{ (Law of Total Probability)}\] \[=\Pr[\widehat{Y}=1|Y=0]\Pr[Y=0|X_{U}]\] \[\qquad+\Pr[\widehat{Y}=1|Y=1]\Pr[Y=1|X_{U}]\] (Complementarity) \[=(\epsilon_{0}+\epsilon_{1}-1)\Pr[Y=1|X_{U}]+1-\epsilon_{0}.\] (Definitions of \[\epsilon_{0}\] and \[\epsilon_{1}\])Here, complementarity allowed us to approximate the unknown \(\Pr[\widehat{Y}=1|Y=0,X_{U}]\) by its average \(\Pr[\widehat{Y}=1|Y=0]=\mathbb{E}_{X_{U}}[\Pr[\widehat{Y}=1|Y=0,X_{U}]]\), which depends only on the known distribution \(P_{X_{S},Y}\). By informativeness, Lemma 4.5 allows us to divide by \(\epsilon_{0}+\epsilon_{1}-1\), giving Eq. (4.3).

```
Input: Calibrated stable classifier \(f_{S}(x_{S})=\Pr[Y=1|X_{S}=x_{S}]\), unlabelled data \(\{(X_{S,i},X_{U,i})\}_{i=1}^{n}\) Output: Joint classifier \(\widehat{f}(x_{S},x_{U})\) estimating \(\Pr[Y=1|X_{S}=x_{S},X_{U}=x_{U}]\)
1 Compute soft pseudo-labels (PLs) \(\{\widehat{Y}_{i}\}_{i=1}^{n}\) with \(\widehat{Y}_{i}=f_{S}(X_{S,i})\)
2 Compute soft class-1 count \(n_{1}=\sum_{i=1}^{n}\widehat{Y}_{i}\)
3 Estimate PL accuracies \((\widehat{\epsilon}_{0},\widehat{\epsilon}_{1})=\left(\frac{1}{n-n_{1}}\sum_{ i=1}^{n}(1-\widehat{Y}_{i})(1-f_{S}(X_{S,i})),\frac{1}{n_{1}}\sum_{i=1}^{n} \widehat{Y}_{i}f_{S}(X_{S,i})\right)\) // Eq. (4.1)
4 Fit unstable classifier \(\widehat{f}_{U}(x_{U})\) to pseudo-labelled data \(\{(X_{U,i},\widehat{Y}_{i})\}_{i=1}^{n}\) // \(\approx\Pr[\widehat{Y}=1|X_{U}=x_{U}]\)
5 Bias-correct \(\widehat{f}_{U}(x_{U})\mapsto\max\left\{0,\min\left\{1,\frac{\widehat{f}_{U}( x_{U})+\widehat{\epsilon}_{0}-1}{\epsilon_{0}+\epsilon_{1}-1}\right\}\right\}\) // Eq. (4.3), \(\approx\Pr[Y=1|X_{U}=x_{U}]\) return\(\widehat{f}(x_{S},x_{U})\!\mapsto\!C_{\frac{1}{n}}(f_{S}(x_{S}),\widehat{f}_{U}(x_{U}))\) // Eq. (4.4)/(4.5), \(\approx\Pr[Y=1|X_{S}=x_{S},X_{U}=x_{U}]\)
```

**Algorithm 1**Bias-corrected adaptation procedure. Multi-class version given by Algorithm 2.

Proof of Eq. (4.4):While the exact proof of Eq. (4.4) is a bit more algebraically involved, the key idea is simply that complementarity allows us to decompose \(\Pr[Y|X_{S},X_{U}]\) into separately-estimatable terms \(\Pr[Y|X_{S}]\) and \(\Pr[Y|X_{U}]\): for any \(y\in\mathcal{Y}\),

\[\Pr[Y=y|X_{S},X_{U}] \propto_{X_{S},X_{U}}\Pr[X_{S},X_{U}|Y=y]\Pr[Y=y]\] (Bayes' Rule) \[=\Pr[X_{S}|Y=y]\Pr[X_{U}|Y=1]\Pr[Y=y]\] (Complementarity) \[\propto_{X_{S},X_{U}}\frac{\Pr[Y=y|X_{S}]\Pr[Y=1|X_{U}]}{\Pr[Y=1]},\] (Bayes' Rule)

where, \(\propto_{X_{S},X_{U}}\) denotes proportionality with a constant depending only on \(X_{S}\) and \(X_{U}\), not on \(y\). Directly estimating these constants involves estimating the density of \((X_{S},X_{U})\), which may be intractable without further assumptions. However, in the binary case, since \(1-\Pr[Y=1|X_{S},X_{U}]=\Pr[Y=0|X_{S},X_{U}]\), these proportionality constants conveniently cancel out when the above relationship is written in logit-space, as in Eq. (4.4). In the multi-class case, App. C shows how to use the constraint \(\sum_{y\in\mathcal{Y}}\Pr[Y=y|X_{S},X_{U}]=1\) to avoid computing the proportionality constants. 

### A provably consistent algorithm for unsupervised test-domain adaptation

Having learned \(P_{Y|X_{S}}\) from the training domain(s), Thm. 4.4 implies we can learn \(P_{Y|X_{S},X_{U}}\) in the test domain by learning \(P_{X_{S},X_{U}}\)--the latter only requiring _unlabeled_ test-domain data. This motivates our Alg. 1 for test-domain adaptation, which is a finite-sample version of the bias-correction and combination equations (Eqs. (4.3) and (4.4)) in Thm. 4.4. Alg. 1 comes with the following guarantee:

**Theorem 4.6** (Consistency Guarantee, Informal).: _Assume (i) \(X_{S}\) is stable, (ii) \(X_{S}\) and \(X_{U}\) are complementary, and (iii) \(X_{S}\) is informative of \(Y\) in the test domain. As \(n\to\infty\), if \(\widehat{f}_{U}\to\Pr[\widehat{Y}=1|X_{U}]\) then \(\widehat{f}\to\Pr[Y=1|X_{S},X_{U}]\)._

In words, as the amount of unlabeled data from the test domain increases, if the unstable classifier on Line 4 of Alg. 1 learns to predict the pseudo-label \(\widehat{Y}\), then the joint classifier output by Alg. 1 learns to predict the true label \(Y\). Convergence in Thm. 4.6 occurs \(P_{X_{S},X_{U}}\)-a.e., both weakly (in prob.) and strongly (a.s.), depending on the convergence of \(\widehat{f}_{U}\). Formal statements and proofs are in Appendix B.

## 5 Algorithm: Stable Feature Boosting (SFB)

Using theoretical insights from SS 4, we now propose Stable Feature Boosting (SFB): an algorithm for safely harnessing unstable features without test-domain labels. We first describe learning a stable predictor and extracting complementary unstable features from the training domains. We then describe how to use these with Alg. 1, adapting our use of the unstable features to the test domain.

**Training domains: Learning stable and complementary features.** Using the notation of Eq. (3.1), our goal on the training domains is to learn stable and unstable features \(\Phi_{S}\) and \(\Phi_{U}\), a stable predictor \(f_{S}\), and domain-specific unstable predictors \(f_{U}^{e}\) such that:

1. \(f_{S}\) is stable, informative, and calibrated (i.e., \(f_{S}(x_{S})=\Pr[Y=1|X_{S}=x_{S}]\)).
2. In domain \(e\), \(f_{U}^{e}\) boosts \(f_{s}\)'s performance with complementary \(\Phi_{U}(X^{e})\perp\!\!\!\perp\!\!\!\perp\Phi_{S}(X^{e})|Y^{e}\).

To achieve these learning goals, we propose the following objective:

\[\begin{split}\min_{\Phi_{S},\Phi_{U}\!\!\perp\!\!\!\perp_{S},H_{U} ^{e}}&\sum_{e\in\mathcal{E}_{u}}R^{e}(h_{S}\circ\Phi_{S})+R^{e}(C (h_{S}\circ\Phi_{S},h_{U}^{e}\circ\Phi_{U}))\\ &+\lambda_{S}\cdot P_{\text{Stability}}(\Phi_{S},h_{S},R^{e})+ \lambda_{C}\cdot P_{\text{CondIndep}}(\Phi_{S}(X^{e}),\Phi_{U}(X^{e}),Y^{e}) \end{split} \tag{5.1}\]

The first term encourages good stable predictions \(f_{S}(X)=h_{S}(\Phi_{S}(X))\) while the second encourages improved domain-specific joint predictions \(f^{e}(X^{e})=C(h_{S}(\Phi_{S}(X^{e})),h_{U}^{e}(\Phi_{U}(X^{e})))\) via a domain-specific use \(h_{U}^{e}\) of the unstable features \(\Phi_{U}(X^{e})\). For binary \(Y\), the combination function \(C\) takes the simplified form of Eq. (4.5). Otherwise, \(C\) takes the more general form of Eq. (C.1). \(P_{\text{Stability}}\) is a penalty encouraging stability while \(P_{\text{CondIndep}}\) is a penalty encouraging complementarity or conditional independence, i.e., \(\Phi_{U}(X^{e})\perp\!\!\!\perp\Phi_{S}(X^{e})|Y^{e}\). Several approaches exist for enforcing stability [1, 35, 58, 47, 15, 67, 40, 77] (e.g., IRM [1]) and conditional independence (e.g., conditional HSIC [22]). \(\lambda_{S}\in[0,\infty)\) and \(\lambda_{C}\in[0,\infty)\) are regularization hyperparameters. While another hyperparameter \(\gamma\in[0,1]\) could control the relative weighting of stable and joint risks, i.e., \(\gamma R^{e}(h_{S}\circ\Phi_{S})\) and \((1-\gamma)R^{e}(C(h_{S}\circ\Phi_{S},h_{U}^{e}\circ\Phi_{U}))\), we found this unnecessary in practice. Finally, note that, in principle, \(h_{U}^{e}\) could take any form and we could learn completely separate \(\Phi_{S},\Phi_{U}\). In practice, we simply take \(h_{U}^{e}\) to be a linear classifier and split the output of a shared \(\Phi(X)=(\Phi_{S}(X),\Phi_{U}(X))\).

**Post-hoc calibration.** As noted in SS 4.2, the stable predictor \(f_{S}\) must be properly calibrated to (i) form unbiased unstable predictions (Line 5 of Alg. 1) and (ii) correctly combine the stable and unstable predictions (Line 6 of Alg. 1). Thus, after optimizing the objective (5.1), we apply a post-processing step (e.g., temperature scaling [25]) to calibrate \(f_{S}\).

**Test-domain adaptation without labels.** Given a stable predictor \(f_{S}=h_{S}\circ\Phi_{S}\) and complementary features \(\Phi_{U}(X)\), we now adapt the unstable classifier \(h_{U}^{e}\) in the test domain to safely harness (or make optimal use of) \(\Phi_{U}(X)\). To do so, we use the bias-corrected adaptation algorithm of Alg. 1 (or Alg. 2 for the multi-class case) which takes as input the stable classifier \(h_{S}\)4 and unlabelled test-domain data \(\{\Phi_{S}(x_{i}),\Phi_{U}(x_{i})\}_{i=1}^{n_{e}}\), outputting a joint classifier adapted to the test domain.

Footnote 4: Note: while Sections 3 and 5 use \(h\) for the classifier and \(f=h\circ\Phi\) for the classifier-representation composition, Section 4 and Alg. 1 use \(f\) for the classifier, since no representation \(\Phi\) is being learned.

## 6 Experiments

We now evaluate the performance of our algorithm on synthetic and real-world datasets requiring out-of-distribution generalization. App. E contains full details on these datasets and a depiction of their samples (see Fig. 4). In the experiments below, SFB uses IRM [1] for \(P_{\text{Stability}}\) and the conditional-independence proxy of Jiang and Veitch [31, SS3.1] for \(P_{\text{CondIndep}}\), with App. F.1.2 giving results with other stability penalties. App. F contains further results, including ablation studies (F.1.1) and results on additional datasets (F.2). In particular, App. F.2 contains results on the Camelyon17 medical dataset [3] from the WILDS package [33], where we find that all methods perform similarly _when properly tuned_ (see discussion in App. F.2). Code is available at: [https://github.com/cianeastwood/sfb](https://github.com/cianeastwood/sfb).

**Synthetic data.** We consider two synthetic datasets: anti-causal (AC) data and cause-effect data with direct \(X_{S}\)-\(X_{U}\) dependence (CE-DD). AC data satisfies the structural equations

\[\begin{split} Y&\leftarrow\text{Rad}(0.5);\\ X_{S}&\gets Y\cdot\text{Rad}(0.75);\\ X_{U}&\gets Y\cdot\text{Rad}(\beta_{e}),\end{split} \tag{6.1}\]

where the input \(X=(X_{S},X_{U})\) and \(\text{Rad}(\beta)\) denotes a Rademacher random variable thatis \(-1\) with probability \(1-\beta\) and \(+1\) with probability \(\beta\). Following [31, SS6.1], we create two training domains with \(\beta_{e}\in\{0.95,0.7\}\), one validation domain with \(\beta_{e}=0.6\) and one test domain with \(\beta_{e}=0.1\). CE-DD data is generated according to the structural equations

\[X_{S} \leftarrow\text{Bern}(0.5);\] \[Y \leftarrow\text{XOR}(X_{S},\text{Bern}(0.75));\] \[X_{U} \leftarrow\text{XOR}(\text{XOR}(Y,\text{Bern}(\beta_{e})),X_{S}),\]

where \(\text{Bern}(\beta)\) denotes a Bernoulli random variable that is 1 with probability \(\beta\) and 0 with probability \(1-\beta\). Note that \(X_{S}\not\perp X_{U}|Y\), since \(X_{S}\) directly influences \(X_{U}\). Following [31, App. B], we create two training domains with \(\beta_{e}\in\{0.95,0.8\}\), one validation domain with \(\beta_{e}=0.2\), and one test domain with \(\beta_{e}=0.1\). For both datasets, the idea is that, during training, prediction based on the stable \(X_{S}\) results in lower accuracy (75%) than prediction based on the unstable \(X_{U}\). Thus, models optimizing for prediction accuracy only--and not stability--will use \(X_{U}\) and ultimately end up with only 10% in the test domain. Importantly, while the stable predictor achieves 75% accuracy in the test domain, this can be improved to 90% if \(X_{U}\) is used correctly. Following [31], we use a simple 3-layer network for both datasets and choose hyperparameters using the validation-domain performance: see App. G.2 for further implementation details.

On the AC dataset, Table 2 shows that ERM performs poorly as it misuses \(X_{U}\), while IRM, ACTIR, and SFB-no-adpt. do well by using only \(X_{S}\). Critically, only SFB (with adaptation) is able to harness \(X_{U}\) in the test domain _without labels_, leading to a near-optimal performance boost.

On the CE-DD dataset, Table 2 again shows that ERM performs poorly while IRM and SFB-no-adpt. do well by using only the stable \(X_{S}\). However, we now see that ACTIR performs poorly since its assumption of anti-causal structure no longer holds. This highlights another key advantage of SFB over ACTIR: any stability penalty can be used, including those with weaker assumptions than ACTIR's anti-causal structure (e.g., IRM). Perhaps more surprisingly, SFB (with adaptation) performs well despite the complementarity assumption \(X_{S}\perp\!\!\!\perp X_{U}|Y\) being violated. One explanation for this is that complementarity is only weakly violated in the test domain. Another is that complementarity is not _necessary_ for SFB, with some weaker, yet-to-be-determined condition(s) sufficing. In App. I, we provide a more detailed explanation and discussion of this observation.

**ColorMNIST.** We now consider the ColorMNIST dataset [1], described in SS 1 and Fig. 0(a). We follow the experimental setup of Eastwood et al. [15, SS6.1]; see App. G.3 for details. Table 3 shows that: (i) SFB learns a stable predictor ("no adpt.") with performance comparable to other stable/invariant

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{3}{c}{Synthetic} & \multicolumn{3}{c}{PACS} & \multicolumn{1}{c}{} \\
**Algorithm** & AC & CE-DD & P & A & C & S \\ \hline ERM & \(9.9\pm 0.1\) & \(11.6\pm 0.7\) & \(93.0\pm 0.7\) & \(79.3\pm 0.5\) & \(74.3\pm 0.7\) & \(65.4\pm 1.5\) \\ ERM + PL & \(9.9\pm 0.1\) & \(11.6\pm 0.7\) & \(93.7\pm 0.4\) & \(79.6\pm 1.5\) & \(74.1\pmmethods like IRM [1]; and (ii) only SFB (with adaptation) is capable of harnessing the spurious color feature in the test domain _without labels_, leading to a near-optimal boost in performance. Note that "Oracle no adpt." refers to an ERM model trained on grayscale images, while "Oracle" refers to an ERM model trained on labeled test-domain data. Table 6 of App. F.1.3 compares to additional baseline methods, including V-REx [35], EQRM [15], Fishr [48] and more. Fig. 2 gives more insight by showing performance across test domains of varying color-label correlation. On the left, we see that SFB outperforms ERM and IRM, as well as additional adaptive baseline methods in IRM + pseudo-labeling (PL, [36]) and IRM + T3A [30] (see App. G.1 for details). On the right, ablations show that: (i) bias-correction (BC), post-hoc calibration (CA), and multiple rounds of pseudo-labeling (Rn) improve adaptation performance; and (ii) without labels, SFB harnesses the spurious color feature near-optimally in test domains of varying color-label correlation--the original goal we set out to achieve in Fig. 0(b). Further results and ablations are provided in App. F.1.

**PACS.** Table 2 shows that SFB's stable ("no adpt.") performance is comparable to that of the other stable/invariant methods (IRM, ACTIR). One exception is the sketch domain (S)--the most severe shift based on performance drop--where SFB's stable predictor performs best. Another is on domains A and C, where ACTIR performs better than SFB's stable predictor. Most notable, however, is: (i) the consistent performance boost that SFB gets from unsupervised adaptation; and (ii) SFB performing best or joint-best on 3 of the 4 domains. These results suggest SFB can be useful on real-world datasets where it is unclear if complementarity holds. In App. I, we discuss why this may be the case.

## 7 Conclusion & Future Work

This work demonstrated, both theoretically and practically, how to adapt our usage of spurious features to new test domains using only a stable, complementary training signal. By using invariant predictions to safely harness complementary spurious features, our proposed Stable Feature Boosting algorithm can provide significant performance gains compared to only using invariant/stable features or using unadapted spurious features--without requiring any true labels in the test domain.

**Stable and calibrated predictors.** Perhaps the greatest challenge in applying SFB in practice is the need for a stable and calibrated predictor. While stable features may be directly observable in some cases (e.g., using prior knowledge of causal relationships between the domain, features, and label, as in Prop. D.2), they often need to be extracted from high-dimensional observations (e.g., images). Several methods for stable-feature extraction have recently been proposed [1, 35, 58, 70, 15], with future improvements likely to benefit SFB. Calibrating complex predictors like deep neural networks is also an active area of research [18, 25, 72, 59], with future improvements likely to benefit SFB.

**Weakening the complementarity condition.** SFB also assumes that stable and unstable features are complementarity, i.e., conditionally independent given the label. This assumption is implicit in the causal generative models assumed by prior work [49, 68, 31], and future work may look to weaken it. However, our experimental results suggest that SFB may be robust to violations of complementarity in practice: on our synthetic data where complementarity does not hold (CE-DD) and real data where we have no reason to believe it holds (PACS), SFB still outperformed baseline methods. We discuss potential reasons for this in App. I and hope that future work can identify weaker sufficient conditions.

**Exploiting newly-available test-domain features without labels.** While we focused on domain generalization (DG) and the goal of (re)learning how to use the same spurious features (e.g., color) in a new way, our solution to the "marginal problem" in SS 4.1 can be used to exploit a completely new set of (complementary) features in the test domain that weren't available in the training domains. For example, given a stable predictor of diabetes based on causal features (e.g., age, genetics), SFB could exploit new unlabeled data containing previously-unseen effect features (e.g., glucose levels). We hope future work can explore such uses of SFB.

## Acknowledgments and Disclosure of Funding

The authors thank Chris Williams and Ian Mason for providing feedback on an earlier draft, as well as the MPI Tubingen causality group for helpful discussions and comments. This work was supported by the Tubingen AI Center (FKZ: 01IS18039B) and by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy - EXC number 2064/1 - Project number 390727645. The authors declare no competing interests.

## References

* Arjovsky et al. [2020] Arjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, D. (2020). Invariant risk minimization. arXiv:1907.02893. [Cited on pages 1, 2, 3, 4, 8, 9, 10, 26, 27, 29, and 31.]
* Ba and Caruana [2014] Ba, J. and Caruana, R. (2014). Do deep nets really need to be deep? _Advances in Neural Information Processing Systems_, 27. [Cited on page 19.]
* Bandi et al. [2018] Bandi, P., Geessink, O., Manson, Q., Van Dijk, M., Balkenhol, M., Hermsen, M., Bejnordi, B. E., Lee, B., Paeng, K., Zhong, A., et al. (2018). From detection of individual metastases to classification of lymph node status at the patient level: the camelyon17 challenge. _IEEE Transactions on Medical Imaging_, 38(2):550-560. [Cited on pages 8, 27, 28, and 29.]
* Beery et al. [2018] Beery, S., Van Horn, G., and Perona, P. (2018). Recognition in terra incognita. In _Proceedings of the European Conference on Computer Vision_, pages 456-473. [Cited on page 3.]
* Ben-David et al. [2010] Ben-David, S., Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Vaughan, J. W. (2010). A theory of learning from different domains. _Machine Learning_, 79(1):151-175. [Cited on page 32.]
* Bickel et al. [2009] Bickel, S., Bruckner, M., and Scheffer, T. (2009). Discriminative learning under covariate shift. _Journal of Machine Learning Research_, 10(9). [Cited on page 26.]
* Blanchard et al. [2016] Blanchard, G., Flaska, M., Handy, G., Pozzi, S., and Scott, C. (2016). Classification with asymmetric label noise: Consistency and maximal denoising. _Electronic Journal of Statistics_, 10:2780-2824. [Cited on pages 6, 19, and 32.]
* Blanchard et al. [2011] Blanchard, G., Lee, G., and Scott, C. (2011). Generalizing from several related classification tasks to a new unlabeled sample. In _Advances in Neural Information Processing Systems_, volume 24. [Cited on page 3.]
* Blitzer et al. [2007] Blitzer, J., Crammer, K., Kulesza, A., Pereira, F., and Wortman, J. (2007). Learning bounds for domain adaptation. _Advances in Neural Information Processing Systems_, 20. [Cited on page 32.]
* Blum and Mitchell [1998] Blum, A. and Mitchell, T. (1998). Combining labeled and unlabeled data with co-training. In _Proceedings of the eleventh annual conference on Computational learning theory_, pages 92-100. [Cited on page 32.]
* Bucilua et al. [2006] Bucilua, C., Caruana, R., and Niculescu-Mizil, A. (2006). Model compression. In _Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 535-541. [Cited on page 19.]
* Bui et al. [2021] Bui, M.-H., Tran, T., Tran, A., and Phung, D. (2021). Exploiting domain-specific features to enhance domain generalization. In _Advances in Neural Information Processing Systems_, volume 34. [Cited on page 2.]
* Eastwood et al. [2021] Eastwood, C., Mason, I., and Williams, C. (2021). Unit-level surprise in neural networks. In _I (Still) Can't Believe It's Not Better! NeurIPS 2021 Workshop_. [Cited on page 3.]
* Eastwood et al. [2022a] Eastwood, C., Mason, I., Williams, C., and Scholkopf, B. (2022a). Source-free adaptation to measurement shift via bottom-up feature restoration. In _International Conference on Learning Representations_. [Cited on page 2.]
* Eastwood et al. [2022b] Eastwood, C., Robey, A., Singh, S., von Kugelgen, J., Hassani, H., Pappas, G. J., and Scholkopf, B. (2022b). Probable domain generalization via quantile risk minimization. In _Advances in Neural Information Processing Systems_. [Cited on pages 1, 2, 3, 4, 8, 9, 10, 29, and 31.]
* Fei-Fei et al. [2006] Fei-Fei, L., Fergus, R., and Perona, P. (2006). One-shot learning of object categories. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 28(4):594-611. [Cited on page 3.]
* Finn et al. [2017] Finn, C., Abbeel, P., and Levine, S. (2017). Model-agnostic meta-learning for fast adaptation of deep networks. In _International Conference on Machine Learning_, pages 1126-1135. [Cited on page 3.]
* Flach [2016] Flach, P. A. (2016). Classifier calibration. In _Encyclopedia of machine learning and data mining_. Springer US. [Cited on page 10.]* [19] Frechet, M. (1951). Sur les tableaux de correlation dont les marges sont donnees. _Ann. Univ. Lyon, 3\({}^{\ast}\) e serie, Sciences, Sect. A_, 14:53-77.
* [20] Galstyan, A. and Cohen, P. R. (2008). Empirical comparison of "hard" and "soft" label propagation for relational classification. In _Inductive Logic Programming: 17th International Conference, ILP 2007, Corvallis, OR, USA, June 19-21, 2007, Revised Selected Papers 17_, pages 98-111. Springer.
* [21] Geirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R., Brendel, W., Bethge, M., and Wichmann, F. A. (2020). Shortcut learning in deep neural networks. _Nature Machine Intelligence_, 2:665-673.
* [22] Gretton, A., Bousquet, O., Smola, A., and Scholkopf, B. (2005). Measuring statistical dependence with Hilbert-Schmidt norms. In _Algorithmic Learning Theory: 16th International Conference, ALT 2005, Singapore, October 8-11, 2005. Proceedings 16_, pages 63-77. Springer.
* [23] Gretton, A., Smola, A., Huang, J., Schmittfull, M., Borgwardt, K., and Scholkopf, B. (2009). Covariate shift by kernel mean matching. _Dataset shift in machine learning_, 3(4):5.
* [24] Gulrajani, I. and Lopez-Paz, D. (2020). In search of lost domain generalization. _arXiv preprint arXiv:2007.01434_.
* [25] Guo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. (2017). On calibration of modern neural networks. In _International Conference on Machine Learning_, pages 1321-1330.
* [26] Hendrycks, D. and Dietterich, T. (2019). Benchmarking neural network robustness to common corruptions and perturbations. In _International Conference on Learning Representations_.
* [27] Hinton, G., Vinyals, O., and Dean, J. (2015). Distilling the knowledge in a neural network. _arXiv preprint arXiv:1503.02531_.
* [28] Hoeffding, W. (1940). Masstabinvariante korrelationstheorie. _Schriften des Mathematischen Instituts und Instituts fur Angewandte Mathematik der Universitat Berlin_, 5:181-233.
* [29] Hoeffding, W. (1941). Masstabinvariante korrelationsmasse fur diskontinuierliche verteilungen. _Archiv fur mathematische Wirtschafts-und Sozialforschung_, 7:49-70.
* [30] Iwasawa, Y. and Matsuo, Y. (2021). Test-time classifier adjustment module for model-agnostic domain generalization. In _Advances in Neural Information Processing Systems_.
* [31] Jiang, Y. and Veitch, V. (2022). Invariant and transportable representations for anti-causal domain shifts. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho, K., editors, _Advances in Neural Information Processing Systems_.
* [32] Kirichenko, P., Izmailov, P., and Wilson, A. G. (2022). Last layer re-training is sufficient for robustness to spurious correlations. In _Advances in Neural Information Processing Systems_.
* [33] Koh, P. W., Sagawa, S., Marklund, H., Xie, S. M., Zhang, M., Balsubramani, A., Hu, W., Yasunaga, M., Phillips, R. L., Gao, I., Lee, T., David, E., Stavness, I., Guo, W., Earnshaw, B. A., Haque, I. S., Beery, S., Leskovec, J., Kundaje, A., Pierson, E., Levine, S., Finn, C., and Liang, P. (2021). WILDS: A benchmark of in-the-wild distribution shifts. In _International Conference on Machine Learning_.
* [34] Krogel, M.-A. and Scheffer, T. (2004). Multi-relational learning, text mining, and semi-supervised learning for functional genomics. _Machine Learning_, 57:61-81.
** [35] Krueger, D., Caballero, E., Jacobsen, J.-H., Zhang, A., Binas, J., Zhang, D., Priol, R. L., and Courville, A. (2021). Out-of-distribution generalization via risk extrapolation (REx). In _International Conference on Machine Learning_, volume 139, pages 5815-5826. [Cited on pages 1, 8, 10, 29, and 31.]
* [36] Lee, D.-H. et al. (2013). Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In _Workshop on Challenges in Representation Learning, ICML_, volume 3. [Cited on pages 2, 6, 10, and 30.]
* [37] Li, D., Yang, Y., Song, Y.-Z., and Hospedales, T. M. (2017a). Deeper, broader and artier domain generalization. In _Proceedings of the IEEE International Conference on Computer Vision (ICCV)_. [Cited on page 27.]
* [38] Li, Y., Yang, J., Song, Y., Cao, L., Luo, J., and Li, L.-J. (2017b). Learning from noisy labels with distillation. In _Proceedings of the IEEE international conference on computer vision_, pages 1910-1918. [Cited on page 32.]
* [39] Liang, J., Hu, D., and Feng, J. (2020). Do we really need to access the source data? Source hypothesis transfer for unsupervised domain adaptation. In _International Conference on Machine Learning (ICML)_, pages 6028-6039. [Cited on page 2.]
* [40] Makar, M., Packer, B., Moldovan, D., Blalock, D., Halpern, Y., and D'Amour, A. (2022). Causally motivated shortcut removal using auxiliary labels. In _International Conference on Artificial Intelligence and Statistics_, pages 739-766. PMLR. [Cited on pages 2, 8, and 29.]
* [41] Mansour, Y., Mohri, M., and Rostamizadeh, A. (2008). Domain adaptation with multiple sources. _Advances in neural information processing systems_, 21. [Cited on page 32.]
* [42] Muandet, K., Balduzzi, D., and Scholkopf, B. (2013). Domain generalization via invariant feature representation. In _International Conference on Machine Learning_, pages 10-18. [Cited on page 3.]
* [43] Nagarajan, V., Andreassen, A., and Neyshabur, B. (2021). Understanding the failure modes of out-of-distribution generalization. In _International Conference on Learning Representations_. [Cited on page 3.]
* [44] Natarajan, N., Dhillon, I. S., Ravikumar, P. K., and Tewari, A. (2013). Learning with noisy labels. _Advances in neural information processing systems_, 26. [Cited on pages 6, 19, and 32.]
* [45] Peters, J., Buhlmann, P., and Meinshausen, N. (2016). Causal inference by using invariant prediction: identification and confidence intervals. _Journal of the Royal Statistical Society. Series B (Statistical Methodology)_, pages 947-1012. [Cited on pages 1, 2, and 3.]
* [46] Pezeshki, M., Kaba, O., Bengio, Y., Courville, A. C., Precup, D., and Lajoie, G. (2021). Gradient starvation: A learning proclivity in neural networks. _Advances in Neural Information Processing Systems_, 34:1256-1272. [Cited on page 29.]
* [47] Puli, A. M., Zhang, L. H., Oermann, E. K., and Ranganath, R. (2022). Out-of-distribution generalization in the presence of nuisance-induced spurious correlations. In _International Conference on Learning Representations_. [Cited on pages 3, 8, and 29.]
* [48] Rame, A., Dancette, C., and Cord, M. (2022). Fishr: Invariant gradient variances for out-of-distribution generalization. In _International Conference on Machine Learning_, pages 18347-18377. [Cited on pages 10 and 29.]
* [49] Rojas-Carulla, M., Scholkopf, B., Turner, R., and Peters, J. (2018). Invariant models for causal transfer learning. _The Journal of Machine Learning Research_, 19(1):1309-1342. [Cited on pages 3, 5, 10, 25, and 26.]
* [50] Rosenfeld, E., Ravikumar, P., and Risteski, A. (2022). Domain-adjusted regression or: ERM may already learn features sufficient for out-of-distribution generalization. _arXiv preprint arXiv:2202.06856_. [Cited on page 3.]* Rothenhausler et al. [2021] Rothenhausler, D., Meinshausen, N., Buhlmann, P., and Peters, J. (2021). Anchor regression: Heterogeneous data meet causality. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 83(2):215-246. [Cited on page 2.]
* Rusak et al. [2022] Rusak, E., Schneider, S., Pachitariu, G., Eck, L., Gehler, P. V., Bringmann, O., Brendel, W., and Bethge, M. (2022). If your data distribution shifts, use self-learning. _Transactions on Machine Learning Research_. [Cited on pages 2 and 6.]
* Sagawa et al. [2019] Sagawa, S., Koh, P. W., Hashimoto, T. B., and Liang, P. (2019). Distributionally robust neural networks. In _International Conference on Learning Representations_. [Cited on page 29.]
* Schapire [1990] Schapire, R. E. (1990). The strength of weak learnability. _Machine Learning_, 5:197-227. [Cited on page 32.]
* Scholkopf [2022] Scholkopf, B. (2022). Causality for machine learning. In _Probabilistic and Causal Inference: The Works of Judea Pearl_, pages 765-804. Association for Computing Machinery. [Cited on pages 2 and 26.]
* Scott et al. [2013] Scott, C., Blanchard, G., and Handy, G. (2013). Classification with asymmetric label noise: Consistency and maximal denoising. In _Conference on learning theory_, pages 489-511. PMLR. [Cited on page 32.]
* Shi et al. [2022a] Shi, Y., Seely, J., Torr, P., N, S., Hannun, A., Usunier, N., and Synnaeve, G. (2022a). Gradient matching for domain generalization. In _International Conference on Learning Representations_. [Cited on page 29.]
* Shi et al. [2022b] Shi, Y., Seely, J., Torr, P., Siddharth, N., Hannun, A., Usunier, N., and Synnaeve, G. (2022b). Gradient matching for domain generalization. In _International Conference on Learning Representations_. [Cited on pages 3, 8, 10, and 29.]
* Silva Filho et al. [2023] Silva Filho, T., Song, H., Perello-Nieto, M., Santos-Rodriguez, R., Kull, M., and Flach, P. (2023). Classifier calibration: a survey on how to assess and improve predicted class probabilities. _Machine Learning_, pages 1-50. [Cited on page 10.]
* Song et al. [2022] Song, H., Kim, M., Park, D., Shin, Y., and Lee, J.-G. (2022). Learning from noisy labels with deep neural networks: A survey. _IEEE Transactions on Neural Networks and Learning Systems_. [Cited on page 32.]
* Sugiyama and Kawanabe [2012] Sugiyama, M. and Kawanabe, M. (2012). _Machine learning in non-stationary environments: Introduction to covariate shift adaptation_. MIT press. [Cited on page 26.]
* Sugiyama et al. [2007] Sugiyama, M., Krauledat, M., and Muller, K.-R. (2007). Covariate shift adaptation by importance weighted cross validation. _Journal of Machine Learning Research_, 8(5). [Cited on page 26.]
* Sun et al. [2022] Sun, Q., Murphy, K., Ebrahimi, S., and D'Amour, A. (2022). Beyond invariance: Test-time label-shift adaptation for distributions with" spurious" correlations. _arXiv preprint arXiv:2211.15646_. [Cited on page 2.]
* Tanaka et al. [2018] Tanaka, D., Ikami, D., Yamasaki, T., and Aizawa, K. (2018). Joint optimization framework for learning with noisy labels. In _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, pages 5552-5560. [Cited on page 32.]
* Vapnik [1991] Vapnik, V. (1991). Principles of risk minimization for learning theory. _Advances in Neural Information Processing Systems_, 4. [Cited on pages 3 and 26.]
* Vapnik [1998] Vapnik, V. N. (1998). _Statistical Learning Theory_. Wiley, New York, NY. [Cited on page 3.]
* Veitch et al. [2021] Veitch, V., D'Amour, A., Yadlowsky, S., and Eisenstein, J. (2021). Counterfactual invariance to spurious correlations: Why and how to pass stress tests. In _Advances in Neural Information Processing Systems_. [Cited on pages 2, 8, and 29.]
* von Kugelgen et al. [2019] von Kugelgen, J., Mey, A., and Loog, M. (2019). Semi-generative modelling: Covariate-shift adaptation with cause and effect features. In _The 22nd International Conference on Artificial Intelligence and Statistics_, pages 1361-1369. PMLR. [Cited on pages 5, 10, 25, and 26.]* von Kugelgen et al. [2020] von Kugelgen, J., Mey, A., Loog, M., and Scholkopf, B. (2020). Semi-supervised learning, causality, and the conditional cluster assumption. In _Conference on Uncertainty in Artificial Intelligence_, pages 1-10. PMLR. [Cited on pages 5 and 26.]
* Wald et al. [2021] Wald, Y., Feder, A., Greenfeld, D., and Shalit, U. (2021). On calibration and out-of-domain generalization. _Advances in neural information processing systems_, 34:2215-2227. [Cited on pages 3, 10, and 29.]
* Wang et al. [2021a] Wang, D., Shelhamer, E., Liu, S., Olshausen, B., and Darrell, T. (2021a). Tent: Fully test-time adaptation by entropy minimization. In _International Conference on Learning Representations_. [Cited on page 2.]
* Wang et al. [2021b] Wang, D.-B., Feng, L., and Zhang, M.-L. (2021b). Rethinking calibration of deep neural networks: Do not be afraid of overconfidence. _Advances in Neural Information Processing Systems_, 34:11809-11820. [Cited on page 10.]
* Zech et al. [2018] Zech, J. R., Badgeley, M. A., Liu, M., Costa, A. B., Titano, J. J., and Oermann, E. K. (2018). Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study. _PLoS Medicine_, 15(11). [Cited on page 3.]
* Zhang et al. [2022] Zhang, J., Lopez-Paz, D., and Bottou, L. (2022). Rich feature construction for the optimization-generalization dilemma. In _International Conference on Machine Learning_. [Cited on pages 3 and 31.]
* Zhao et al. [2019] Zhao, H., Des Combes, R. T., Zhang, K., and Gordon, G. (2019). On learning invariant representations for domain adaptation. In _International Conference on Machine Learning_, pages 7523-7532. PMLR. [Cited on page 32.]
* Zhao et al. [2018] Zhao, H., Zhang, S., Wu, G., Moura, J. M., Costeira, J. P., and Gordon, G. J. (2018). Adversarial multiple source domain adaptation. _Advances in Neural Information Processing Systems_, 31. [Cited on page 32.]
* Zheng and Makar [2022] Zheng, J. and Makar, M. (2022). Causally motivated multi-shortcut identification & removal. _Advances in Neural Information Processing Systems_. [Cited on pages 2, 8, and 29.]

[MISSING_PAGE_EMPTY:16]

Proof and Further Discussion of Theorem 4.4

### Proof of Theorem 4.4

In this section, we prove our main results regarding the marginal generalization problem presented in Section 4, namely Thm. 4.4. For the reader's convenience, we restate Thm. 4.4 here:

**Theorem 4.4** (Marginal generalization with for binary labels and complementary features).: _Consider three random variables \(X_{S}\), \(X_{U}\), and \(Y\), where_

1. \(Y\) _is binary (_\(\{0,1\}\)_-valued),_
2. \(X_{S}\) _and_ \(X_{U}\) _are complementary features for_ \(Y\) _(i.e.,_ \(X_{S}\perp\!\!\!\perp X_{U}|Y\)_), and_
3. \(X_{S}\) _is informative of_ \(Y\) _(_\(X_{S}\not\perp\!\!\!\perp Y\)_)._

_Then, the joint distribution of \((X_{S},X_{U},Y)\) can be written in terms of the joint distributions of \((X_{S},Y)\) and \((X_{S},X_{U})\). Specifically, if \(\widehat{Y}|X_{S}\sim\operatorname{Bernoulli}(\Pr[Y=1|X_{S}])\) is pseudo-label and_

\[\epsilon_{0}:=\Pr[\widehat{Y}=0|Y=0]\quad\text{ and }\quad\epsilon_{1}:=\Pr[ \widehat{Y}=1|Y=1]\] (A.1)

_are the conditional probabilities that \(\widehat{Y}\) and \(Y\) agree, given \(Y=0\) and \(Y=1\), respectively, then,_

1. \(\epsilon_{0}+\epsilon_{1}>1\)_,_
2. \(\Pr[Y=1|X_{U}]=\frac{\Pr[\widehat{Y}=1|X_{U}]+\epsilon_{0}-1}{\epsilon_{0}+ \epsilon_{1}-1}\)_, and_
3. \(\Pr[Y=1|X_{S},X_{U}]=\sigma\left(\log\!\left(\Pr[Y=1|X_{S}]\right)+\log\!\left( \Pr[Y=1|X_{U}]\right)-\log\!\left(\Pr[Y=1]\right)\right)\)_._

Before proving Thm. 4.4, we provide some examples demonstrating that the complementarity and informativeness assumptions in Thm. 4.4 cannot be dropped.

**Example A.1**.: Suppose \(X_{S}\) and \(X_{U}\) have independent \(\operatorname{Bernoulli}(1/2)\) distributions. Then, \(X_{S}\) is informative of both of the binary variables \(Y_{1}=X_{S}X_{U}\) and \(Y_{2}=X_{S}(1-X_{U})\) and both have identical conditional distributions given \(X_{S}\), but \(Y_{1}\) and \(Y_{2}\) have different conditional distributions given \(X_{U}\):

\[\Pr[Y_{1}=1|X_{U}=0]=0\neq 1/2=\Pr[Y_{2}=1|X_{U}=0].\]

Thus, the complementarity condition cannot be omitted.

On the other hand, \(X_{S}\) and \(X_{U}\) are complementary for both \(Y_{3}=X_{U}\) and an independent \(Y_{4}\sim\operatorname{Bernoulli}(1/2)\) and both \(Y_{3}\) and \(Y_{4}\) both have identical conditional distributions given \(X_{S}\), but \(Y_{1}\) and \(Y_{2}\) have different conditional distributions given \(X_{U}\):

\[\Pr[Y_{3}=1|X_{U}=1]=1/2\neq 1=\Pr[Y_{4}=1|X_{U}=1].\]

Thus, the informativeness condition cannot be omitted.

Before proving Thm. 4.4, we prove Lemma 4.5, which allows us to safely divide by the quantity \(\epsilon_{0}+\epsilon_{1}-1\) in the formula for \(\Pr[Y=1|X_{U}]\), under the condition that \(X_{S}\) is informative of \(Y\).

**Lemma 4.5**.: _In the setting of Thm. 4.4, let \(\epsilon_{0}\) and \(\epsilon_{1}\) be the class-wise pseudo-label accuracies defined in as in Eq. (A.1). Then, \(\epsilon_{0}+\epsilon_{1}=1\) if and only if \(X_{S}\) and \(Y\) are independent._

Note that the entire result also holds, with almost identical proof, in the multi-environment setting of Sections 3 and 5, conditioned on a particular environment \(E\).

Proof.: We first prove the forward implication. Suppose \(\epsilon_{0}+\epsilon_{1}=1\). If \(\Pr[Y=1]\in\{0,1\}\), then \(X_{S}\) and \(Y\) are trivially independent, so we may assume \(\Pr[Y=1]\in(0,1)\). Then,

\[\mathbb{E}[\widehat{Y}] =\epsilon_{1}\Pr[Y=1]+(1-\epsilon_{0})(1-\Pr[Y=1])\] (Law of Total Expectation) \[=(\epsilon_{0}+\epsilon_{1}-1)\Pr[Y=1]+1-\epsilon_{0}\] \[=1-\epsilon_{0}\] ( \[\epsilon_{0}+\epsilon_{1}=1\] ) \[=\mathbb{E}[\widehat{Y}|Y=0].\] (Definition of \[\epsilon_{0}\] )

[MISSING_PAGE_FAIL:18]

[MISSING_PAGE_FAIL:19]

Proof of Theorem 4.6

This appendix provides a proof of Thm. 4.6, which provides conditions under which our proposed domain adaptation procedure (Alg. 1) is consistent.

We state a formal version of Thm. 4.6:

**Theorem 4.6** (Consistency of the bias-corrected classifier).: _Assume_

1. \(X_{S}\) _is stable,_
2. \(X_{S}\) _and_ \(X_{U}\) _are complementary, and_
3. \(X_{S}\) _is informative of_ \(Y\) _(i.e.,_ \(X_{S}\not\perp Y\)_)._

_Let \(\widehat{\eta}_{n}:\mathcal{X}_{S}\times X_{U}\to[0,1]\) given by_

\[\widehat{\eta}_{n}(x_{S},x_{U})=\sigma\left(f_{S}(x_{S})+\mathrm{logit}\left( \frac{\widehat{\eta}_{U,n}(x_{U})+\widehat{e}_{0,n}-1}{\widehat{e}_{0,n}+ \widehat{e}_{1,n}-1}\right)-\beta_{1}\right),\quad\text{for all }(x_{S},x_{U})\in \mathcal{X}_{S}\times\mathcal{X}_{U},\]

_denote the bias-corrected regression function estimate proposed in Alg. 1, and let \(\widehat{h}_{n}:\mathcal{X}_{S}\times\mathcal{X}_{U}\to\{0,1\}\) given by_

\[\widehat{h}_{n}(x_{S},x_{U})=1\{\widehat{\eta}(x_{S},x_{U})>1/2\},\quad\text {for all }(x_{S},x_{U})\in\mathcal{X}_{S}\times\mathcal{X}_{U},\]

_denote the corresponding hard classifier. Let \(\eta_{U}:\mathcal{X}_{U}\to[0,1]\), given by \(\eta_{U}(x_{U})=\Pr[Y=1|X_{U}=x_{U},E=1]\) for all \(x_{U}\in\mathcal{X}_{U}\), denote the true regression function over \(X_{U}\), and let \(\widehat{\eta}_{U,n}\) denote its estimate as assumed in Line 4 of Alg. 1. Then, as \(n\to\infty\),_

1. _if, for_ \(P_{X_{U}}\)_-almost all_ \(x_{U}\in\mathcal{X}_{U}\)_,_ \(\widehat{\eta}_{U,n}(x_{U}))\to\eta_{U}(x_{U})\) _in probability, then_ \(\widehat{\eta}_{n}\) _and_ \(\widehat{h}_{n}\) _are weakly consistent (i.e.,_ \(\widehat{\eta}_{n}(x_{S},x_{U})\to\eta(x_{S},x_{U})\)__\(P_{X_{S},X_{U}}\)_-almost surely and_ \(R(\widehat{h}_{n})\to R(h^{*})\) _in probability)._
2. _if, for_ \(P_{X_{U}}\)_-almost all_ \(x_{U}\in\mathcal{X}_{U}\)_,_ \(\widehat{\eta}_{U,n}(x_{U}))\to\eta_{U}(x_{U})\) _almost surely, then_ \(\widehat{\eta}_{n}\) _and_ \(\widehat{h}_{n}\) _are strongly consistent (i.e.,_ \(\widehat{\eta}_{n}(x_{S},x_{U})\to\eta(x_{S},x_{U})\)__\(P_{X_{S},X_{U}}\)_-almost surely and_ \(R(\widehat{h}_{n})\to R(h^{*})\) _a.s.)._

Before proving Thm. 4.6, we provide a few technical lemmas. The first shows that almost-everywhere convergence of regression functions implies convergence of the corresponding classifiers in classification risk:

**Lemma B.1**.: _Consider a sequence of regression functions \(\eta,\eta_{1},\eta_{2},...:\mathcal{X}\to[0,1]\). Let \(h,h_{1},h_{2},...:\mathcal{X}\to\{0,1\}\) denote the corresponding classifiers_

\[h(x)=1\{\eta(x)>1/2\}\quad\text{ and }\quad h_{i}(x)=1\{\eta_{i}(x)>1/2\},\quad \text{for all }i\in\mathbb{N},x\in\mathcal{X}.\]

1. _If_ \(\eta_{n}(x)\to\eta(x)\) _for_ \(P_{X}\)_-almost all_ \(x\in\mathcal{X}\) _in probability, then_ \(R(h_{n})\to R(h^{*})\) _in probability._
2. _If_ \(\eta_{n}(x)\to\eta(x)\) _for_ \(P_{X}\)_-almost all_ \(x\in\mathcal{X}\) _almost surely as_ \(n\to\infty\)_, then_ \(R(h_{n})\to R(h)\) _almost surely._

Proof.: Note that, since \(h_{n}(x)\neq h(x)\) implies \(|\eta_{n}(x)-\eta(x)|\geq|\eta(x)-1/2|\),

\[1\{h_{n}(x)\neq h(x)\}\leq 1\{|\eta_{n}(x)-\eta(x)|\geq|\eta(x)-1/2|\}.\] (B.1)

We utilize this observation to prove both (a) and (b).

Proof of (a)Let \(\delta>0\). By Inequality (B.1) and partitioning \(\mathcal{X}\) based on whether \(|2\eta(X)-1|\leq\delta/2\),

\[\mathbb{E}_{X}\left[|2\eta(X)-1|1\{h_{n}(X)\neq h(X)\}\right]\] \[\leq\mathbb{E}_{X}\left[|2\eta(X)-1|1\{|\eta_{n}(X)-\eta(X)|\geq| \eta(X)-1/2|\}\right]\] \[=\mathbb{E}_{X}\left[|2\eta(X)-1|1\{|\eta_{n}(X)-\eta(X)|\geq| \eta(X)-1/2|\}1\{|2\eta(X)-1|>\delta/2\}\right]\] \[\qquad+\mathbb{E}_{X}\left[|2\eta(X)-1|1\{|\eta_{n}(X)-\eta(X)|\geq |\eta(X)-1/2|\}1\{|2\eta(X)-1|\leq\delta/2\}\right]\] \[\leq\mathbb{E}_{X}\left[1\{|\eta_{n}(X)-\eta(X)|>\delta/2\}\right]+ \delta/2.\]Hence,

\[\lim_{n\to\infty}\Pr_{\eta_{n}}\big{[}\mathbb{E}_{X}\left[|2\eta(X)-1 |1\{h_{n}(X)\neq h(X)\}\right]>\delta\big{]}\] \[\leq\lim_{n\to\infty}\Pr_{\eta_{n}}\big{[}\mathbb{E}_{X}\left[1\{| \eta_{n}(X)-\eta(X)|>\delta/2\}\right]>\delta/2\big{]}\] \[\leq\lim_{n\to\infty}\frac{2}{\delta}\,\mathbb{E}_{\eta_{n}}\left[ \mathbb{E}_{X}\left[1\{|\eta_{n}(X)-\eta(X)|>\delta/2\}\right]\right]\] (Markov's Inequality) \[=\lim_{n\to\infty}\frac{2}{\delta}\,\mathbb{E}_{X}\left[\mathbb{E} _{\eta_{n}}\left[1\{|\eta_{n}(X)-\eta(X)|>\delta/2\}\right]\right]\] (Fubini's Theorem) \[=\frac{2}{\delta}\,\mathbb{E}_{X}\left[\lim_{n\to\infty}\Pr_{\eta _{n}}\left[|\eta_{n}(X)-\eta(X)|>\delta/2\right]\right]\] (Dominated Convergence Theorem) \[=0.\] ( \[\eta_{n}(X)\to\eta(X)\], \[P_{X}\] -a.s., in probability)

Proof of (b)For any \(x\in\mathcal{X}\) with \(\eta(x)\neq 1/2\), if \(\eta_{n}(x)\to\eta(x)\) then \(1\{|\eta_{n}(x)-\eta(x)|\geq|\eta(x)-1/2|\}\to 0\). Hence, by Inequality (B.1), the dominated convergence theorem (with \(|2\eta(x)-1|1\{|\eta_{n}(x)-\eta(x)|\geq|\eta(x)-1/2|\}\leq 1\)), and the assumption that \(\eta_{n}(x)\to\eta(x)\) for \(P_{X}\)-almost all \(x\in\mathcal{X}\) almost surely,

\[\lim_{n\to\infty}\mathbb{E}_{X}\left[|2\eta(X)-1|1\{h_{n}(X)\neq h (X)\}\right]\] \[\leq\lim_{n\to\infty}\mathbb{E}_{X}\left[|2\eta(X)-1|1\{|\eta_{n}( X)-\eta(X)|\geq|\eta(X)-1/2|\}\right]\] \[=\mathbb{E}_{X}\left[\lim_{n\to\infty}|2\eta(X)-1|1\{|\eta_{n}(x) -\eta(x)|\geq|\eta(x)-1/2|\}\right]\] \[=0,\quad\text{ almost surely.}\]

Our next lemma concerns an edge case in which the features \(X_{S}\) and \(X_{U}\) provide perfect but contradictory information about \(Y\), leading to Equation (4.4) being ill-defined. We show that this can happen only with probability \(0\) over \((X_{S},X_{U})\sim P_{X_{S},X_{U}}\) can thus be safely ignored:

**Lemma B.2**.: _Consider two predictors \(X_{S}\) and \(X_{Y}\) of a binary label \(Y\). Then,_

\[\Pr_{X_{S},X_{U}}[\mathbb{E}[Y|X_{S}]=1\text{ and }\mathbb{E}[Y|X_{U}]=0]=\Pr_{X _{S},X_{U}}[\mathbb{E}[Y|X_{S}]=0\text{ and }\mathbb{E}[Y|X_{U}]=1]=0.\]

Proof.: Suppose, for sake of contradiction, that the event

\[A:=\big{\{}(x_{S},x_{U}):\mathbb{E}[Y|X_{S}=x_{S}]=1\text{ and }\mathbb{E}[Y|X_{U }=x_{U}]=0\big{\}}\]

has positive probability. Then, the conditional expectation \(\mathbb{E}[Y|A]\) is well-defined, giving the contradiction

\[1=\mathbb{E}_{X_{S}}[\mathbb{E}[Y|E,X_{S}]]=\mathbb{E}[Y|A]=\mathbb{E}_{X_{U}} [\mathbb{E}[Y|E,X_{U}]]=0.\]

The case \(\mathbb{E}[Y|X_{S}]=0\) and \(\mathbb{E}[Y|X_{U}]=1\) is similar. 

We now utilize Lemmas B.1 and B.2 to prove Thm. 4.6.

Proof.: By Lemma B.1, it suffices to prove that \(\widehat{\eta}(x_{S},x_{U})\to\eta(x_{S},x_{U})\), for \(P_{X_{S},X_{U}}\)-almost all \((x_{S},x_{U})\in\mathcal{X}_{S}\times\mathcal{X}_{U}\), in probability (to prove (a)) and almost surely (to prove (b)).

Finite caseWe first consider the case when both \(\Pr[Y|X_{S}=x_{S}],\Pr[Y|X_{U}=x_{U}]\in(0,1)\), so that \(f_{S}(x_{S})\) and \(\text{logit}\left(\frac{\widehat{\eta}(x_{U})+\epsilon_{0}-1}{\epsilon_{0}+ \epsilon_{1}-1}\right)\) are both finite. Since

\[\widehat{\eta}_{S,U}(x_{S},x_{U})-\eta_{S,U}(x_{S},x_{U})\] \[=\sigma\left(f_{S}(x_{S})+\text{logit}\left(\frac{\widehat{\eta }_{U,1}(x_{U})+\widehat{e}_{0}-1}{\widehat{e}_{0}+\widehat{e}_{1}-1}\right)- \widehat{\beta}_{1,n}\right)-\sigma\left(f_{S}(x_{S})+\text{logit}\left( \frac{\widetilde{\eta}(x_{U})+\epsilon_{0}-1}{\epsilon_{0}+\epsilon_{1}-1} \right)-\beta_{1}\right),\]

where the sigmoid \(\sigma:\mathbb{R}\to[0,1]\) is continuous, by the continuous mapping theorem and the assumption that \(\widehat{\eta}_{U,1}(x_{U})\to\widetilde{\eta}(x_{U})\), to prove both of these, it suffices to show:1. \(\widehat{\epsilon}_{0}\rightarrow\epsilon_{0}\) and \(\widehat{\epsilon}_{1}\rightarrow\epsilon_{1}\) almost surely as \(n\rightarrow\infty\).
2. \(\widehat{\beta}_{1,n}\rightarrow\beta_{1}\in(-\infty,\infty)\) almost surely as \(n\rightarrow\infty\).
3. The mapping \((a,b,c)\mapsto\text{logit}\left(\frac{a+b-1}{b+c-1}\right)\) is continuous at \((\widetilde{\eta}(x_{U}),\epsilon_{0},\epsilon_{1})\).

We now prove each of these in turn.

Proof of (i)Since \(\widehat{Y}_{i}\perp\!\!\!\perp Y_{i}|X_{S}\) and \(0<\Pr[\widehat{Y}=1]\), by the strong law of large numbers and the continuous mapping theorem,

\[\widehat{\epsilon}_{1}=\frac{1}{n_{1}}\sum_{i=1}^{n}\widehat{Y}_{i}\sigma(f_{S }(X_{i}))=\frac{\frac{1}{n}\sum_{i=1}^{n}\widehat{Y}_{i}\sigma(f_{S}(X_{i}))} {\frac{1}{n}\sum_{i=1}^{n}\widehat{Y}_{i}}\rightarrow\frac{\mathbb{E}[\sigma(f _{S}(X))1\{\widehat{Y}=1\}]}{\Pr[\widehat{Y}=1]}=\mathbb{E}[\sigma(f_{S}(X))| \widehat{Y}=1]=\epsilon_{1}\]

almost surely as \(n\rightarrow\infty\). Similarly, since \(\Pr[\widehat{Y}=0]=1-\Pr[\widehat{Y}=1]>0\), \(\widehat{\epsilon}_{0}\rightarrow\epsilon_{0}\) almost surely.

Proof of (ii)Recall that

\[\widehat{\beta}_{1,n}=\text{logit}\left(\frac{1}{n}\sum_{i=1}^{n}\widehat{Y}_{ i}\right).\]

By the strong law of large numbers, \(\frac{1}{n}\sum_{i=1}^{n}\widehat{Y}_{i}\rightarrow\Pr[\widehat{Y}=1|E=1]=\Pr[Y =1|E=1]\). Since we assumed \(\Pr[Y=1|E=1]\in(0,1)\), it follows that the mapping \(a\mapsto\text{logit}(a)\) is continuous at \(a=\Pr[Y=1|E=1]\). Hence, by the continuous mapping theorem, \(\widehat{\beta}_{1,n}\rightarrow\text{logit}\left(\Pr[Y=1|E=1]\right)=\beta_{1}\) almost surely.

Proof of (iii)Since the logit function is continuous on the open interval \((0,1)\) and we assumed \(\epsilon_{0}+\epsilon_{1}>1\), it suffices to show that \(0<\widetilde{\eta}(x_{U})+\epsilon_{0}-1<\epsilon_{0}+\epsilon_{1}-1\). Since, according to Thm. 4.4,

\[\widetilde{\eta}(x_{U})=(\epsilon_{0}+\epsilon_{1}-1)\eta^{*}(x_{U}))+1- \epsilon_{0},\]

this holds as long as \(0<\eta^{*}(x_{U})<1\), as we assumed for \(P_{X_{U}}\)-almost all \(x_{U}\in\mathcal{X}_{U}\).

Infinite caseWe now address the case where either \(\Pr[Y|X_{S}=x_{S}]\in\{0,1\}\) or \(\Pr[Y|X_{U}=x_{U}]\in\{0,1\}\). By Lemma B.2, only one of these can happen at once, \(P_{X_{S},X_{U}}\)-almost surely. Hence, since \(\lim_{n\rightarrow\infty}\widehat{\beta}_{1,n}\) is also finite almost surely, if \(\Pr[Y|X_{S}=x_{S}]\in\{0,1\}\), then \(\widetilde{\eta}(x_{S},x_{U})=\sigma(\text{logit}(\Pr[Y|X_{S}=x_{S}]))=\eta(x_ {S},x_{U})\), while, if \(\Pr[Y|X_{U}=x_{U}]\in\{0,1\}\), then \(\widetilde{\eta}(x_{S},x_{U})\rightarrow\sigma\left(\text{logit}(\Pr[Y|X_{U}=x _{U}])\right)=\eta(x_{S},x_{U})\), in probability or almost surely, as appropriate. 

## Appendix C Multiclass Case

In the main paper, to simplify notation, we presented our unsupervised test-domain adaptation method in the case of binary labels \(Y\). However, in many cases, including several of our experiments in Section 6, the label \(Y\) can take more than 2 distinct values. Hence, in this section, we show how to generalize our method to the multiclass setting and then present the exact procedure (Alg. 2) used in our multiclass experiments in Section 6.

Suppose we have \(K\geq 2\) classes. We "one-hot encode" these classes, so that \(Y\) takes values in the set

\[\mathcal{Y}=\{(1,0,...,0),(0,1,0,...,0),...,(0,...,0,1)\}\subseteq\{0,1\}^{K}.\]

Let \(\epsilon\in[0,1]^{\mathcal{Y}\times\mathcal{Y}}\) with

\[\epsilon_{y,\mathcal{Y}}=\Pr[\widehat{Y}=y|Y=y^{\prime}]\]

denote the class-conditional confusion matrix of the pseudo-labels. Then, we have

\[\mathbb{E}[\widehat{Y}|X_{U}] =\sum_{y\in\mathcal{Y}}\mathbb{E}[\widehat{Y}|Y=y,X_{U}]\Pr[Y=y| X_{U}]\] (Law of Total Expectation) \[=\sum_{y\in\mathcal{Y}}\mathbb{E}[\widehat{Y}|Y=y]\Pr[Y=y|X_{U}]\] (Complementary) \[=\epsilon\,\mathbb{E}[Y|X_{U}].\] (Definition of \[\epsilon\]

[MISSING_PAGE_FAIL:23]

for \(Q\in\mathbb{R}^{\mathcal{Y}}\) defined by

\[Q_{y}=\frac{f_{S,y}(X_{S})f_{U,y}(X_{U})}{\Pr[Y=y]}\quad\text{ for each }y\in \mathcal{Y}.\]

In particular, applying the sigmoid function to each side, we have

\[\Pr[Y|X_{S},X_{U}]=\frac{Q}{\|Q\|_{1}}.\]

We can estimate \(Q_{y}\) by

\[\widehat{Q}_{y}=\frac{f_{S,y}(X_{S})f_{U,y}(X_{U})}{\frac{1}{n}\sum_{i=1}^{n}f _{S,y}(X_{S,i})}.\]

In matrix notation, this is

\[\widehat{Q}=\frac{f_{S}(X_{S})\circ f_{U}(X_{U})}{\frac{1}{n}\sum_{i=1}^{n}f_{ S}(X_{S,i})},\]

where \(\circ\) denotes element-wise multiplication. It follows that, for \(p\in\Delta^{\mathcal{Y}}\) (we will use \(p_{y}=\Pr[Y=y]\)), we can use the multiclass combination function \(C:\Delta^{\mathcal{Y}}\times\Delta^{\mathcal{Y}}\to\Delta^{\mathcal{Y}}\) with

\[C_{p}(p_{S},p_{U})=\text{Normalize}\left(\frac{p_{S}p_{U}}{p}\right),\] (C.1)

where the multiplication and division are performed element-wise and \(\text{Normalize}(x)=\frac{x}{\|x\|_{1}}\), to generalize Eq. (4.5). Putting these derivations together gives us our multiclass version of Alg. 1, presented in Alg. 2, where \(\Delta^{\mathcal{Y}}=\{z\in[0,1]^{K}:\sum_{y\in\mathcal{Y}}z_{y}=1\}\) denotes the standard probability simplex over \(\mathcal{Y}\).

```
0: Calibrated stable classifier \(f_{S}:\mathcal{X}\to\Delta^{\mathcal{Y}}\) with \(f_{S,y}(x_{S})=\Pr[Y=y|X_{S}=x_{S}]\), \(n\) unlabeled samples \(\{(X_{S,i},X_{U,i})\}_{i=1}^{n}\)
0: Joint classifier \(\widehat{f}:\mathcal{X}_{S}\times\mathcal{X}_{U}\to\Delta^{\mathcal{Y}}\) estimating \(\Pr[Y=y|X_{S}=x_{S},X_{U}=x_{U}]\)
1: Compute soft pseudo-labels \(\{\widehat{Y}_{i}\}_{i=1}^{n}\) with \(\widehat{Y}_{i}=f_{S}(X_{S,i})\)
2: Compute soft class counts \(\widehat{n}=\sum_{i=1}^{n}\widehat{Y}_{i}\)
3: Estimate class-conditional pseudo-label confusion matrix \(\widehat{\epsilon}\gets f_{S}^{\intercal}(X_{S})\) Normalize\((f_{S}^{\intercal}(X_{S}))\)
4: Fit unstable classifier \(\widehat{f}_{U}(x_{U})\) to pseudo-labelled data \(\{(X_{U,i},\widehat{Y}_{i})\}_{i=1}^{n}\)// \(\approx\Pr[\widehat{Y}=y|X_{U}]\)
5: Bias-correction \(\widehat{f}_{U}(x_{U})\mapsto\arg\min_{p\in\Delta^{\mathcal{Y}}}\|\epsilon p -\widehat{f}_{U}(x_{U})\|_{2}\)// \(\approx\Pr[Y=y|X_{U}]\)
6:return\(\widehat{f}(x_{S},x_{U})\mapsto C_{\widehat{n}/n}(f_{S}(x_{S}),\widehat{f}_{U}(x_ {U}))\)// Eq. (C.1), \(\approx\Pr[Y=y|X_{S},X_{U}]\)
```

**Algorithm 2**Multiclass bias-corrected adaptation procedure.

## Appendix D Supplementary Results

### Trivial solution to joint-risk minimization

In Prop. D.1 below, we assume that the stable \(f_{S}(X)\) and unstable \(f_{U}(X)\) predictors output _logits_. In contrast, throughout the rest of the paper, we assume that \(f_{S}(X)\) and \(f_{U}(X)\) output _probabilities_ in \([0,1]\).

**Proposition D.1**.: _Suppose \(\widehat{Y}|f_{S}(X)\sim\text{Bernoulli}(\sigma(f_{S}(X)))\), such that \(\widehat{Y}\perp\!\!\!\perp f_{U}(X)|f_{S}(X)\). Then,_

\[0\in\operatorname*{arg\,min}_{f_{U}:X\to\mathbb{R}}\mathds{E}[\ell(\widehat{Y },\sigma(f_{S}(X)+f_{U}(X)))],\]

_where \(\ell(x,y)=-x\log y-(1-x)\log(1-y)\) denotes the cross-entropy loss._Proof.: Suppose \(\widehat{Y}|f_{S}(X)\sim\text{Bernoulli}(\sigma(f_{S}(X)))\), such that \(\widehat{Y}\perp\!\!\!\perp f_{U}(X)|f_{S}(X)\). Then,

\[-\mathbb{E}[\ell(\widehat{Y},\sigma(f_{S}(X)+f_{U}(X)))]\] \[=\mathbb{E}[\mathbb{E}[\ell(\widehat{Y},\sigma(f_{S}(X)+f_{U}(X)) )]]\] (Law of Total Expectation) \[=\mathbb{E}[\mathbb{E}[\widehat{Y}\log\sigma(f_{S}(X)+f_{U}(X))\] \[\qquad+(1-Y)\log(1-\sigma(f_{S}(X)+f_{U}(X)))|f_{S}(X)]]\] \[=\mathbb{E}[\mathbb{E}[\widehat{Y}|f_{S}(X_{S})]\mathbb{E}[\log \sigma(f_{S}(X)+f_{U}(X))|f_{S}(X_{S})]\] \[\qquad+\mathbb{E}[(1-\widehat{Y})|f_{S}(X_{S})]\mathbb{E}[\log(1 -\sigma(f_{S}(X)+f_{U}(X)))|f_{S}(X)]]\] ( \[\widehat{Y}\perp\!\!\!\perp f_{U}(X)|f_{S}(X)\] ) \[=\mathbb{E}[\sigma(f_{S}(X))\log\sigma(f_{S}(X)+f_{U}(X))\] \[\qquad+(1-\sigma(f_{S}(X)))\log(1-\sigma(f_{S}(X)+f_{U}(X)))]. ( \[\widehat{Y}|f_{S}(X)\sim\text{Bernoulli}(\sigma(f_{S}(X)))\] ).\[\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad\qquad \qquad\qquad\qquad\qquad\_These are the four structural assumptions under which Theorems 4.4 and 4.6 show that the SFB algorithm learns the conditional distribution \(P_{Y|X_{S},X_{U}}\) in the test domain. Additionally, suppose_
5. \(X_{U}\) _is unstable (i.e.,_ \(E\not\perp Y|X_{U}\)_), This is the case in which empirical risk minimization [ERM; 65] may suffer bias due to distribution shift, and hence when SFB may outperform ERM._
6. \(X_{U}\) _contains some information about_ \(Y\) _that is not included in_ \(X_{S}\) _(i.e.,_ \(X_{U}\not\perp Y|X_{S}\)_). This is information we expect invariant risk minimization [IRM; 1] is unable to learn, and hence when we expect SFB to outperform IRM._

_Then, \(X_{U}\) consists of causal descendants ("effects") of \(Y\), while three types of stable features are possible:_

1. _causal ancestors_ \(X_{S,C}\) _of_ \(Y\)_,_
2. _causal descendants_ \(X_{S,E}\) _of_ \(Y\) _that are not also descendants of_ \(E\)_,_
3. _causal spouses_ \(X_{S,S}\) _of_ \(Y\) _(i.e., causal ancestors of_ \(X_{S,E}\)_)._

Notable special cases of the DAG in Figure 3 include:

1. the "cause-effect" settings, studied by Rojas-Carulla et al. [49], von Kugelgen et al. [68, 69], where \(X_{S}\) is a cause of \(Y\), \(X_{U}\) is an effect of \(Y\), and \(E\) may affect both \(X_{S}\) and \(X_{U}\) but may affect \(Y\) only indirectly through \(X_{S}\). Note that this generalizes the commonly used "covariate shift" assumption, as not only the covariate distribution \(P_{X_{S},X_{U}}\) but also the conditional distribution \(P_{Y|X_{U}}\) can change between environments.
2. the "anti-causal" setting, studied by Jiang and Veitch [31], where \(X_{S}\) and \(X_{U}\) are both effects of \(Y\), but \(X_{S}\) is unaffected by \(E\).
3. the widely studied "covariate shift" setting [62, 23, 6, 61], which corresponds (see Sections 3 and 5 of Scholkopf [55]) to a causal factorization \(P(X,Y)=P(X)P(Y|X)\) (i.e., in which the only stable components \(X_{S}\) are causes \(X_{S,C}\)) of \(Y\) or unconditionally independent (e.g., causal spouses \(X_{S,S}\))) of \(Y\).

However, this model is more general than these special cases. Also, for sake of simplicity, we assumed causal sufficiency here; however, in the presence of unobserved confounders, other types of stable features are also possible; for example, if we consider the possibility of unobserved confounders \(U\) influencing \(Y\) that are independent of \(E\) (i.e., invariant across domains), then our method can also utilize stable features that are descendants of \(U\) (i.e., "siblings" of \(Y\)).

## Appendix E Datasets

In our experiments, we consider five datasets: two (synthetic) numerical datasets and three image datasets. We now describe each dataset.

Synthetic: Anti-causal (AC).We consider an anti-causal synthetic dataset based on that of Jiang and Veitch [31, SS6.1] where data is generated according to the following structural equations:

\[Y \leftarrow\text{Rad}(0.5);\] \[X_{S} \gets Y\cdot\text{Rad}(0.75);\] \[X_{U} \gets Y\cdot\text{Rad}(\beta^{e}),\]

where the input \(X=(X_{S},X_{U})\) and \(\text{Rad}(\beta)\) denotes a Rademacher random variable that is \(-1\) with probability \(1-\beta\) and \(+1\) with probability \(\beta\). Following Jiang and Veitch [31, SS6.1], we create two training domains with \(\beta_{e}\in\{0.95,0.7\}\), one validation domain with \(\beta_{e}=0.6\) and one test domain with \(\beta_{e}=0.1\).

Synthetic: Cause-effect with a direct \(X_{S}\)-\(X_{U}\) dependence (CE-DD).We also consider a synthetic cause-effect dataset in which there is a direct dependence between \(X_{S}\) and \(X_{U}\). In particular, following Jiang and Veitch [31, App. B], data is generated according to the following structural equations:\[X_{S} \leftarrow\text{Bern}(0.5);\] \[Y \leftarrow\text{XOR}(X_{S},\text{Bern}(0.75));\] \[X_{U} \leftarrow\text{XOR}(\text{XOR}(Y,\text{Bern}(\beta_{e})),X_{S}),\]

where the input \(X=(X_{S},X_{U})\) and \(\text{Bern}(\beta)\) denotes a Bernoulli random variable that is 1 with probability \(\beta\) and 0 with probability \(1-\beta\). Note that \(X_{S}\not\perp X_{U}|Y\), since \(X_{S}\) directly influences \(X_{U}\). Following Jiang and Veitch [31, App. B], we create two training domains with \(\beta_{e}\in\{0.95,0.8\}\), one validation domain with \(\beta_{e}=0.2\), and one test domain with \(\beta_{e}=0.1\).

ColorMNIST.We next consider the ColorMNIST dataset [1]. This transforms the original MNIST dataset into a binary classification task (digit in 0-4 or 5-9) and then: (i) flips the label with probability 0.25, meaning that, across all 3 domains, digit shape correctly determines the label with probability 0.75; and (ii) colorizes the digit such that digit color (red or green) is a more informative but spurious feature (see Fig. 4).

Pacs.We next consider the PACS dataset [37]--a 7-class image-classification dataset consisting of 4 domains: photos (P), art (A), cartoons (C) and sketches (S), with examples shown in Fig. 4. Model performances are reported for each domain after training on the other 3 domains.

Camelyon17.Finally, in the additional experiments of App. F.2, we consider the Camelyon17[3] dataset from the WILDS benchmark [33]: a medical dataset with histopathology images from 5 hospitals which use different staining and imaging techniques (see Fig. 4). The goal is to determine whether or not a given image contains tumor tissue, making it a binary classification task across 5 domains (3 training, 1 validation, 1 test).

## Appendix F Further Experiments

This appendix provides further experiments which supplement those in the main text. In particular, it provides: (i) an ablation on the ColorMNIST dataset showing the effects of bias correction, post-hoc calibration and multiple rounds of pseudo-labelling on SFB's performance (F.1.1); (ii) the

Figure 4: Examples from ColorMNIST [1], PACS [37] and Camelyon17 [3]. Figure and examples based on Gulrajani and Lopez-Paz [24, Table 3] and Koh et al. [33, Figure 4]. For ColorMNIST, we follow the standard approach [1] and use the first two domains for training and the third for testing. For PACS [37], we follow the standard approach [37, 24] and use each domain in turn for testing, using the remaining three domains for training. For Camelyon17 [3], we follow WILDS [33] and use the first three domains for training, the fourth for validation, and the fifth for testing.

performance of SFB on the ColorMNIST dataset when using different stability penalties (F.1.2); and (iii) results on a real-world medical dataset, Camelyon17 [3], where we find that all methods perform similarly _when properly tuned_ (F.2).

### ColorMNIST

We now provide ablations on the ColorMNIST dataset to illustrate the effectiveness of the different components of SFB. In particular, we focus on bias correction and calibration, while also showing how multiple rounds of pseudo-labeling can improve performance in practice.

#### f.1.1 Ablations

Bias correction.To adapt the unstable classifier in the test domain, SFB employs the bias-corrected adaptation algorithm of Alg. 1 (or Alg. 2 for the multi-class case) which corrects for biases caused by possible disagreements between the stable-predictor pseudo-labels \(\widehat{Y}\) and the true label \(Y\). In this (sub)section, we investigate the performance of SFB with and without bias correction (BC).

Calibration.As discussed in SS 4.2, correctly combining the stable and unstable predictions post-adaptation requires them to be properly calibrated. In particular, it requires the stable predictor \(f_{\text{S}}\) to be calibrated with respect to the true labels \(Y\) and the unstable predictor \(f_{U}\) to be calibrated with respect to the pseudo-labels \(\widehat{Y}\). In this (sub)section, we investigate the performance of SFB with and without post-hoc calibration (in particular, simple temperature scaling [25]). More specifically, we investigate the effect of calibrating the stable predictor (CS) and calibrating the unstable predictor (CU).

Multiple rounds of pseudo-labeling.While SFB learns the optimal unstable classifier \(I^{e}_{U}\) in the test domain _given enough unlabelled data_, SS 4.1 discussed how more accurate pseudo-labels \(\widehat{Y}\) improve the sample efficiency of SFB. In particular, in a restricted-sample setting, more accurate pseudo-labels result in an unstable classifier \(I^{e}_{U}\) which better harnesses \(X_{U}\) in the test domain. With this in mind, note that, after adapting, we expect the joint predictions of SFB to be more accurate than its stable-only predictions. This raises the question: can we use these improved predictions to form more accurate pseudo-labels, and, in turn, an unstable classifier \(I^{e}_{U}\) that leads to even better performance? Furthermore, can we repeat this process, using multiple rounds of pseudo-labelling to refine our pseudo-labels and ultimately \(I^{e}_{U}\)? While this multi-round approach loses the asymptotic guarantees of SS 4.2, we found it to work quite well in practice. In this (sub)section, we thus investigate the performance of SFB with and without multiple rounds of pseudo-labeling (PL rounds).

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Algorithm & Bias Correction & Calibration Stable & PL Rounds Unstable & Test Acc. \\ \hline SFB no adpt. & & & & 1 & 70.6 \(\pm\) 1.8 \\ \hline SFB & & & & 1 & 78.0 \(\pm\) 2.9 \\ +BC & & & & 1 & 83.4 \(\pm\) 2.8 \\ +CS & & & & 1 & 80.6 \(\pm\) 3.4 \\ +CU & & & & 1 & 76.6 \(\pm\) 2.4 \\ +BC+CS+CU & & & & 1 & 84.4 \(\pm\) 2.2 \\ +BC+CS & & & & 1 & 84.9 \(\pm\) 2.6 \\ +BC+CS & & & & 2 & 87.4 \(\pm\) 1.9 \\ +BC+CS & & & & 3 & 88.1 \(\pm\) 1.8 \\ +BC+CS & & & & 4 & 88.6 \(\pm\) 1.3 \\ +BC+CS & & & & 5 & 88.7 \(\pm\) 1.3 \\ \hline SFB GT adpt. & & & & 1 & 89.0 \(\pm\) 0.3 \\ \hline \hline \end{tabular}
\end{table}
Table 4: SFB ablations on CMNIST. Means and standard errors are over 3 random seeds. _BC:_ bias correction. _CS:_ post-hoc calibration of the stable classifier. _CU:_ post-hoc calibration of the unstable classifier. _PL Rounds:_ Number of pseudo-labeling rounds used. _GT adpt:_ ground-truth adaptation using true labels in the test domain.

Results.Table 4 reports the ablations of SFB on ColorMNIST. Here we see that: (i) bias correction significantly boosts performance (+BC); (ii) calibrating the stable predictor also boosts performance without (+CS) and with (+BC+CS) bias correction, with the latter leading to the best performance; (iii) calibrating the unstable predictor (with respect to the pseudo-labels) slightly hurts performance without (+CU) and with (+BC+CS+CU) bias correction and stable-predictor calibration; (iv) multiple rounds of pseudo-labeling boosts performance, while also reducing the performance variation across random seeds; (v) using bias correction, stable-predictor calibration and 5 rounds of pseudo-labeling results in near-optimal adaptation performance, as indicated by the similar performance of SFB when using true labels \(Y\) to adapt \(h^{\text{c}}_{\text{LI}}\) (denoted "SFB GT adpt." in Table 4).

#### f.1.2 Different stability penalties

In our experiments of SS 6, we used IRM for the stability term of our SFB method, given in Eq. (5.1). However, as discussed in SS 5, many other approaches exist for enforcing stability [35, 58, 47, 15, 67, 40, 77], and, in principle, any of these could be used. To illustrate this point, we now evaluate the performance of SFB when using different stability penalties, namely IRM [1], VREx [35], EQRM [15] and CLOvE [70]. For all penalties, we use SFB with bias correction, post-hoc calibration of the stable predictor, and 5 rounds of pseudo-labeling (see the ablation study of App. F.1.1).

#### f.1.3 Full results

We now provide extended/full results of those provided in the main text. In particular, Table 6 represents an extended version of Table 3 in the main text, comparing against many more baseline methods. In addition, Table 7 provides the full numerical results for all adaptive baseline methods (described in App. G.1), which correspond to the plots of Fig. 2 in the main text.

### Camelyon17

We now provide results on the Camelyon17 [3] dataset. See App. E for a description of the dataset, and App. G.5 for implementation details.

\begin{table}
\begin{tabular}{l c} \hline \hline
**Algorithm** & **Test Acc.** \\ \hline ERM & 27.9 \(\pm\) 1.5 \\ GroupDRO [53] & 29.0 \(\pm\) 1.1 \\ IRM [1] & 69.7 \(\pm\) 0.9 \\ SD [46] & 70.3 \(\pm\) 0.6 \\ IGA [57] & 57.7 \(\pm\) 3.3 \\ Fishr [48] & 70.1 \(\pm\) 0.7 \\ V-REx [35] & 71.6 \(\pm\) 0.5 \\ EQRM [15] & 71.4 \(\pm\) 0.4 \\ SFB no adpt. & 70.6 \(\pm\) 1.8 \\ SFB & **88.1 \(\pm\) 1.8** \\ \hline Oracle no adpt. & 72.1 \(\pm\) 0.7 \\ Oracle & 89.9 \(\pm\) 0.1 \\ \hline \hline \end{tabular}
\end{table}
Table 6: CMNIST test-domain accuracies. Mean and standard error are over 10 seeds. Extended/full version of Table 3 in the main text.

\begin{table}
\begin{tabular}{l c c} \hline \hline
**Algorithm** & **Without Adaptation** & **With Adaptation** \\ \hline SFB w. IRM & 70.6 \(\pm\) 1.8 & 88.7 \(\pm\) 1.3 \\ SFB w. VREx & 72.5 \(\pm\) 1.0 & 88.7 \(\pm\) 1.5 \\ SFB w. EQRM & 69.0 \(\pm\) 2.8 & 88.2 \(\pm\) 2.5 \\ SFB w. CLOvE & 67.0 \(\pm\) 3.7 & 77.0 \(\pm\) 6.6 \\ \hline \hline \end{tabular}
\end{table}
Table 5: CMNIST test-domain accuracies for SFB with different stability penalties. Shown are the mean and standard error over 10 seeds.

Table 2 shows that ERM, IRM and SFB perform similarly on Camelyon17. In line with [24], we found that a properly-tuned ERM model can be difficult to beat on real-world datasets, particularly when the model is pre-trained on ImageNet and the dataset doesn't contain severe distribution shift. While we conducted this proper tuning for ERM, IRM, and SFB (see App. G.5), doing so for ACTIR was non-trivial. We thus report the result from their paper [31, Table 1], which is likely lower due to sub-optimal hyperparameters. In particular, we found that, for ERM and IRM, using a lower learning rate (1e-5 vs 1e-4) and early stopping (1 vs 25 epochs) improved performance by 20 percentage points, from around 70% [31, Table 1] to around 90% (Table 8 below). It remains to be seen whether or not ACTIR can improve over a properly-tuned ERM model on Camelyon17.

While it may seem disappointing that SFB does not outperform the simpler methods of IRM and ERM on Camelyon17, we note that SFB can only be expected to do well when there is some gain in out-of-distribution performance from enforcing stability, e.g., when IRM outperforms ERM. The identical performances of IRM and ERM in Table 8 indicate that, with ImageNet pre-training and proper hyperparameter tuning, this is not the case for Camelyon17. Finally, despite the similar performances, we note that adapting SFB on Camelyon17 still gives a small performance boost and reduces the variance across random seeds.

## Appendix G Implementation Details

Below we provide further implementation details for the experiments of this work. Code is available at: [https://github.com/cianeastwood/sfb](https://github.com/cianeastwood/sfb).

### Adaptive baselines

For both the synthetic and CNNIST datasets, we compare against adaptive baseline methods by using pseudo-labeling (PL, [36]) and test-time classifier adjustment (T3A, [30]) on top of both ERM and IRM, choosing all adaptation hyperparameters using leave-one-domain-out cross-validation:

* _ERM/IRM + PL (last):_ After training with ERM/IRM, we update the last layer using the model's own pseudo-labels [36].
* _ERM/IRM + PL (all):_ After training with ERM/IRM, we update all layers using the model's own pseudo-labels [36].

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c} Algorithm & \multicolumn{10}{c}{Domain (Color-Label Correlation)} \\ \hline  & 1 & 0.9 & 0.8 & 0.7 & 0.6 & 0.5 & 0.6 & -0.7 & -0.8 & -0.9 & -1.0 \\ \hline ERM & 975\(\pm\)0.3 & 88.5\(\pm\)0.4 & 79.7\(\pm\)0.4 & 70.6\(\pm\)0.5 & 61.4\(\pm\)0.4 & 52.5\(\pm\)0.4 & 43.5\(\pm\)0.7 & 34* _ERM/IRM + T3A:_ After training with ERM/IRM, we replace the classifier (final layer) with the template-based classifier of T3A [30]. This means: (i) computing template representations for each class using pseudo-labeled test-domain data; and (ii) classifying each example based on its distance to these templates.

### Synthetic experiments

Following Jiang and Veitch [31], we use a simple three-layer network with 8 units in each hidden layer and the Adam optimizer, choosing hyperparameters using the validation domain.

For SFB, we sweep over \(\lambda_{S}\) in \(\{0.01,0.1,1,5,10,20\}\) and \(\lambda_{C}\) in \(\{0.01,0.1,1\}\). For SFB's unsupervised adaptation, we employ the bias correction of Alg. 1 and calibrate the stable predictor using post-hoc temperature scaling, choosing the temperature to minimize the expected calibration error (ECE, [25]) on the validation domain. In addition, we use the Adam optimizer with an adaptation learning rate of 0.01, choosing the number of adaptation steps in \([1,20]\) (via early stopping) using the validation domain. Finally, we report the mean and standard error over 100 random seeds.

### ColorMNIST experiments

Training details.We follow the setup of Eastwood et al. [15, SS6.1] and build on their open-source code5. In particular, we use the original MNIST training set to create training and validation sets for each domain, and the original MNIST test set for the test sets of each domain. For all methods, we use a 2-hidden-layer MLP with 390 hidden units, the Adam optimizer, a learning rate of 0.0001 with cosine scheduling, and dropout with \(p=0.2\). In addition, we use full batches (size 25000), 400 steps for ERM pre-training (which directly corresponds to the delicate penalty "annealing" or warm-up periods used by penalty-based methods on ColorMNIST [1, 35, 15, 74]), and 600 total steps. We sweep over stability-penalty weights in \(\{50,100,500,1000,5000\}\) for IRM, VREx and SFB and \(\alpha\)'s in \(1-\{e^{-100},e^{-250},e^{-500},e^{-750},e^{-1000}\}\) for EQRM. As the stable (shape) and unstable (color) features are conditionally independent given the label, we fix SFB's conditional-independence penalty weight \(\lambda_{C}=0\). As is the standard for ColorMNIST, we use a test-domain validation set to select the best settings (after the total number of steps), and then report the mean and standard error over 10 random seeds on a test-domain test set. As in previous works, the hyperparameter ranges of all methods are selected by peeking at test-domain performance. While far from ideal, this is quite difficult to avoid with ColorMNIST and highlights a core problem with hyperparameter selection in DG--as discussed by many previous works [1, 35, 24, 74, 15].

Footnote 5: [https://github.com/cianeastwood/qrm/tree/main/CMNIST](https://github.com/cianeastwood/qrm/tree/main/CMNIST)

SFB adaptation details.For SFB's unsupervised adaptation in the test domain, we use a batch size of 2048 and employ the bias correction of Alg. 1. In addition, we calibrate the stable predictor using post-hoc temperature scaling, choosing the temperature to minimize the expected calibration error (ECE, [25]) across the two training domains. Again using the two training domains for hyperparameter selection, we sweep over adaptation learning rates in \(\{0.1,0.01\}\), choose the best adaptation step in \([5,20]\) (via early stopping), and sweep over the number of pseudo-labeling rounds in \([1,5]\). Finally, we report the mean and standard error over 3 random seeds for adaptation.

### PACS experiments

We follow the setup of Jiang and Veitch [31, SS 6.4] and build on their open-source code6. This means using an ImageNet-pretrained ResNet-18, the Adam optimizer with a learning rate of \(10^{-4}\), and choosing hyperparameters using leave-one-domain-out cross-validation (akin to K-fold cross-validation, except with domains). In particular, for each held-out test domain, we train 3 models--each time leaving out 1 of the 3 training domains for validation--and then select hyperparameters based on the best average performance across the held-out validation domains. Finally, we use the selected hyperparameters to retrain the model using all 3 training domains.

Footnote 6: [https://github.com/ybjiaang/ACTIR](https://github.com/ybjiaang/ACTIR).

For SFB, we sweep over \(\lambda_{S}\) in \(\{0.01,0.1,1,5,10,20\}\), \(\lambda_{C}\) in \(\{0.01,0.1,1\}\), and learning rates in \(\{10^{-4},50^{-4}\}\). For SFB's unsupervised adaptation, we employ the multi-class bias correction of Alg. 2 and calibrate the stable predictor using post-hoc temperature scaling, choosing the temperature to minimize the expected calibration error (ECE, [25]) across the three training domains. In addition, we use the Adam optimizer with an adaptation learning rate of \(0.01\), choosing the number of adaptation steps in \([1,20]\) (via early stopping) using the training domains. Finally, we report the mean and standard error over 5 random seeds.

### Camelyon17 experiments

We follow the setup of Jiang and Veitch [31, SS 6.3] and build on their open-source code7. This means using an ImageNet-pretrained ResNet-18, the Adam optimizer, and, following [33], choosing hyperparameters using the validation domain (hospital 4). In contrast to [31], we use a learning rate of \(10^{-5}\) for all methods, rather than \(10^{-4}\), and employ early stopping using the validation domain. We found this to significantly improve all methods. E.g., the baselines of ERM and IRM improve by approximately 20 percentage points, jumping from \(\approx 70\%\) to \(\approx 90\%\).

Footnote 7: See Footnote 6.

For SFB, we sweep over \(\lambda_{S}\) in \(\{0.01,0.1,1,5,10,20\}\) and \(\lambda_{C}\) in \(\{0.01,0.1,1\}\). For SFB's unsupervised adaptation, we employ the bias correction of Alg. 1 and calibrate the stable predictor using post-hoc temperature scaling, choosing the temperature to minimize the expected calibration error (ECE, [25]) on the validation domain. In addition, we use the Adam optimizer with an adaptation learning rate of \(0.01\), choosing the number of adaptation steps in \([1,20]\) (via early stopping) using the validation domain. Finally, we report the mean and standard error over 5 random seeds.

## Appendix H Further Related Work

**Learning with noisy labels.** An intermediate goal in our work, namely learning a model to predict \(Y\) from \(X_{U}\) using pseudo-labels based on \(X_{S}\), is an instance of _learning with noisy labels_, a widely studied problem [56, 44, 7, 60, 38, 64]. Specifically, under the complementarity assumption (\(X_{S}\perp\!\!\!\perp X_{U}|Y\)), the accuracy of the pseudo-labels on each class is independent of \(X_{U}\), placing us in the so-called _class-conditional random noise model_[56, 44, 7]. As we discuss in Section 4, our theoretical insights about the special structure of pseudo-labels complement existing results on learning under this model. Our bias-correction (Eq. (4.3)) for \(P_{Y|X_{U}}\) is also closely related to the "method of unbiased estimators" [44]. However, rather than correcting the loss used in ERM, our post-hoc bias correction applies to any calibrated classifier. Moreover, our ultimate goal, learning a predictor of \(Y\)_jointly_ using \(X_{S}\) and \(X_{U}\), is not captured by learning with noisy labels.

**Co-training.** Our use of stable-feature pseudo-labels to train a classifier based on a disjoint subset of (unstable) features is reminiscent of co-training [10]. Both methods benefit from conditional independence of the two feature subsets given the label to ensure that they provide complementary information.8 The key difference is that while co-training requires (a small number of) labeled samples from the _same distribution as the test data_, our method instead uses labeled data from _a different distribution_ (training domains), along with the assumption of a stable feature. Additionally, while co-training iteratively refines two pre-trained classifiers symmetrically based on each other's predictions, our method only trains the unstable classifier, in a single iteration, using the stable classifier's predictions.

Footnote 8: See Krogel and Scheffer [34], Blum and Mitchell [10, Theorem 1] for discussion of this assumption.

Boosting.Our method of building a strong (albeit unstable) classifier using a weak (but stable) one is reminiscent of boosting, in which one ensembles weak classifiers to create a single strong classifier [54] and which inspires the name of our approach, "stable feature boosting (SFB)". However, whereas traditional boosting improves weak classifiers by examining how their predictions differ from true labels, our adaptation method utilizes only pseudo-labels and needs no true labels from the test domain. For example, while traditional boosting only refines functions of existing features, SFB can utilize new features that are only available in the test domain.

Learning theory for domain generalization.In addition to often assuming particular kinds of distribution shifts (e.g., covariate shift), existing error bounds for domain generalization often depend on some notion of distance between training and test domains (which does not vanish as more data is collected within domains) [9, 5, 76, 75] or assume that the test domain has a particular structural relationship with the training domains (e.g., is a convex combination of training domains [41]).

In contrast, under the structure of invariant and complementary features, we show that consistent generalization (i.e., with generalization error vanishing as more data is collected within domains) is possible in _any_ test domain. Additionally, whereas these prior works derive uniform convergence bounds (implying good generalization for ERM), our results demonstrate the benefit of an additional bias-correction step after training. We also note that, in much of this literature, "invariance" refers to invariance of the covariate marginal distribution \(P_{X}\) across domains; in contrast, our notion of stable features (Defn. 4.1) refers to invariance of the conditional \(P_{Y|X}\).

## Appendix I Performance When Complementarity is Violated

Thm. 4.4 justifies the bias correction of Eq. (4.3) under the assumption that stable \(X_{S}\) and unstable \(X_{U}\) features are complementary, i.e., conditionally independent given the label \(Y\). In this section, we discuss what happens if this assumption is relaxed and provide some intuition for why the bias correction appears to help even when complementarity is violated (as we observed in some of our experiments). In particular, we provide an argument that, in most cases, the bias correction should improve the accuracy of a naive classifier by making it agree more often with the Bayes-optimal classifier. While not a rigorous proof, we believe that this argument provides some insight into SFB's strong performance even when complementarity is violated.

In the absence of complementarity, the quantity \(\Pr[\widehat{Y}=1|Y=1,X_{u}=x_{U}]\) no longer reduces to the class-wise accuracy \(\Pr[\widehat{Y}=1|Y=1]\); thus we write more generally \(\epsilon_{1}(x_{U})=\Pr[\widehat{Y}=1|Y=1,X_{u}=x_{U}]\), and we write \(\overline{\epsilon_{1}}=\mathbf{E}_{X_{U}}[\epsilon_{1}(X_{U})]=\Pr[\widehat {Y}=1|Y=1]\) instead of simply \(\epsilon_{1}\) for the accuracy on class \(1\). Similarly, we write \(\epsilon_{0}(x_{U})=\Pr[\widehat{Y}=0|Y=0,X_{u}=x_{U}]\), and we write \(\overline{\epsilon_{0}}=\mathbf{E}_{X_{U}}[\epsilon_{0}(X_{U})]=\Pr[\widehat{Y }=0|Y=0]\) instead of simply \(\epsilon_{0}\) for the accuracy on class \(0\).

Let \(f_{*}(x_{U})=\Pr[Y=1|X_{U}=x_{U}]\) denote the true regression function, and let \(h_{*}(x_{U})=1\{f_{*}(x_{U})>0.5\}\) denote the Bayes-optimal classifier. It is well known that the Bayes-optimal classifier \(h_{*}\) has the maximum possible accuracy out of all classifiers. Thus, the sub-optimality of a classifier \(h\) can be measured by the probability \(S(h)=\Pr_{X_{U}}[h(X_{U})\neq h_{*}(X_{U})]\) that it disagrees with the Bayes-optimal classifier. Our next result expresses \(S(h)\) in terms of the true regression function \(f_{*}\), the functions \(\epsilon_{0}\) and \(\epsilon_{1}\), and the distribution of \(X_{U}\), when \(h\) is the bias-corrected classifier

\[h_{BC}(x_{U}):=1\left\{\frac{\Pr[\widehat{Y}=1|X_{U}=x_{U}]+\overline{\epsilon }_{0}-\overline{\epsilon}_{1}}{\overline{\epsilon}_{0}+\overline{\epsilon}_{1} -1}>0.5\right\}\]

from Thm. 4.4 or when \(h\) is the "naive" classifier

\[h_{Native}(x_{U}):=1\left\{\Pr[\widehat{Y}=1|X_{U}=x_{U}]>0.5\right\}\]

that simply treats the pseudo-labels as true labels.

**Proposition I.1**.: \[S\left(h_{BC}\right) =\Pr_{X_{U}}\left[|f_{*}(X_{U})-0.5|\leq\frac{\left|\epsilon_{0} (X_{U})-\epsilon_{1}(X_{U})-\mathbb{E}_{X_{U}}\left[\epsilon_{0}(X_{U})- \epsilon_{1}(X_{U})\right]\right|}{2(\epsilon_{0}(X_{U})+\epsilon_{1}(X_{U}) -1)}\right],\] _and_ \[S\left(h_{Native}\right) =\Pr_{X_{U}}\left[|f_{*}(X_{U})-0.5|\leq\frac{\left|\epsilon_{0} (X_{U})-\epsilon_{1}(X_{U})\right|}{2(\epsilon_{0}(X_{U})+\epsilon_{1}(X_{U}) -1)}\right].\]

These two formulae for \(S\left(h_{BC}\right)\) and \(S\left(h_{Native}\right)\) differ only in the numerator of the right-hand side; letting \(Z:=\epsilon_{0}(X_{U})-\epsilon_{1}(X_{U})\), the sub-optimality of \(h_{BC}\) scales with \(|Z-\mathbb{E}[Z]|\), whereas the sub-optimality of \(h_{Naive}\) scales with \(|Z|\). Intuitively, for all except very pathological random variables \(Z\), \(|Z-\mathbb{E}[Z]|\) is typically smaller than \(|Z|\). Although not a rigorous proof that the bias correction is always better than the naive classifier, this analysis provides an argument that, in most cases, the bias correction should improve on the accuracy of the naive classifier, by making it agree more often with the Bayes-optimal classifier.

We conclude by sketching the proof of Proposition I.1:Proof.: By construction, a thresholding classifier \(h(x)=1\{f(x)>0.5\}\) disagrees with the Bayes-optimal classifier if and only if

\[f(x)\leq 0.5<f_{*}(x)\quad\text{ or }\quad f_{*}(x)\leq 0.5<f(x).\]

Expanding these inequalities in the cases \(f(x)=\frac{\Pr[\widehat{Y}=1|X_{U}=x]+\widehat{e}_{0}-\overline{e}_{1}}{ \varepsilon_{0}+\overline{e}_{1}-1}\) and \(f(x)=\Pr[\widehat{Y}=1|X_{U}=x]\) and solving for the quantity \(f_{*}(x)-0.5\) in each case gives Proposition 1.1.