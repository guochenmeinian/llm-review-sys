[MISSING_PAGE_FAIL:1]

In practice, however, real data always exhibits certain structures such as low intrinsic dimensionality, and isotropic data assumptions fail to capture this fact. In statistics methodology, it is known that the directions along which the input \(\mathbf{x}\) has high variance are often good predictors of the target \(\mathbf{\eta}\) [17]; indeed, this is the main reason principal component analysis is used in pretraining [16]. A fundamental model that captures such a structure is the _spiked matrix model_ in which \(\mathbf{x}\sim\mathcal{N}(0,\mathbf{I}_{d}+\kappa\mathbf{\theta}\mathbf{\theta}^{\top})\) for some unit direction \(\mathbf{\theta}\in\mathbb{R}^{d}\) and \(\kappa>0\)[15]. Along the direction \(\mathbf{\theta}\), data has higher variability and predictive power. In single index models, such predictive power translates to a non-trivial alignment between the vectors \(\mathbf{u}\) and \(\mathbf{\theta}\) -- our focus is to investigate the effect of such alignment on the sample complexity of gradient-based training.

### Contributions: learning single index models under spiked covariance

In this paper, we study the sample complexity of learning a single index model using a two-layer neural network and show that it is determined by an interplay between

* **spike-target alignment:**\(\langle\mathbf{u},\mathbf{\theta}\rangle\asymp d^{-r_{1}},r_{1}\in[0,1/2]\),
* **spike magnitude:**\(\kappa\asymp d^{r_{2}}\), for \(r_{2}\in[0,1]\).

Our contributions can be summarized as follows.

1. We show that even in the case of perfect spike-target alignment (\(r_{1}=0\)), the spherical gradient flow commonly employed in recent literature (see e.g. [1, 2]) cannot recover the target direction for moderate spike magnitudes in the population limit. The failure of this covariance-agnostic procedure under anisotropic structure insinuates the necessity of an appropriate covariance-aware normalization to effectively learn the single index model.
2. We show that a covariance-aware normalization that resembles _batch normalization_ resolves this issue. Indeed, the resulting gradient flow can successfully recover the target direction \(\mathbf{u}\) in this case, and depending on the amount of spike-target alignment, the sample complexity can significantly improve compared to the isotropic case.
3. Under the spiked covariance model, we prove a three-stage phase transition for the sample complexity depending on the quantities \(r_{1}\) and \(r_{2}\). For a suitable direction and magnitude of the spike, the sample complexity can be made \(\tilde{\mathcal{O}}(d^{3+\nu})\) for any \(\nu>0\) which is independent of the information exponent \(s\). This should be compared against the known complexity of \(\tilde{\mathcal{O}}(d^{s})\) under isotropic data.
4. We finally show that preconditioning the training dynamics with the inverse covariance improves the sample complexity. This is particularly significant for the spiked covariance model where \(\tilde{\mathcal{O}}(d^{3+\nu})\) samples can be reduced to \(\tilde{\mathcal{O}}(d^{1+\nu})\) for any \(\nu>0\), i.e. almost linear in \(d\). The three-stage phase transition also emerges, as illustrated in Figure 1: in the "hard" regime, the complexity remains \(\tilde{\mathcal{O}}(d^{s})\) regardless of the magnitude and direction of the spike, while in the "easy" regime the complexity only depends on the spike magnitude and not its direction. The "intermediate" regime interpolates between these two; smaller \(r_{1}\) and larger \(r_{2}\) improve the sample complexity.

The rest of the paper is organized as follows. We discuss the notation and the related work in the remainder of this section. We provide preliminaries on the statistical model and the training procedure in Section 2, and provide a negative result on the covariance-agnostic gradient flow in Section 2.1. Our main sample complexity result on a single neuron is presented in Section 3.2. We provide our results on multi-neuron neural networks in Section 4 and also discuss extensions such as preconditioning and its implications. We provide a technical summary in Section 5 and conclude in Section 6.

**Notation.** We use \(\langle\cdot,\cdot\rangle\) and \(\lVert\cdot\rVert\) to denote Euclidean inner product and norm. For matrices, \(\lVert\cdot\rVert\) denotes the usual operator norm, and \(\lambda_{\max}(\cdot)\) and \(\lambda_{\min}(\cdot)\) denote the largest and smallest eigenvalues respectively. We reserve \(\gamma\) for the standard Gaussian distribution on \(\mathbb{R}\), and let \(\lVert\cdot\rVert_{\gamma}\) denote the \(L^{2}(\gamma)\) norm. \(\mathbb{S}^{d-1}\) is the unit \(d\)-dimensional sphere. For quantities \(a\) and \(b\), we will use \(a\lesssim b\) to convey there exists a constant \(C\) (a universal constant unless stated otherwise, in which case may depend on polylogarithmic factors of \(d\)) such that \(a\leq Cb\), and \(a\asymp b\) signifies that \(a\lesssim b\) and \(b\lesssim a\).

Figure 1: Sample complexity to learn \(\mathbf{u}\) and \(g\) under the spiked model. Smaller \(r_{1}\) denotes a better spike-target alignment, while larger \(r_{2}\) denotes a larger spike magnitude. The sample complexities are based on Corollary 8.

### Further related work

**Non-Linear Feature Learning with Neural Networks.** Recently, two popular scaling regimes of neural networks have emerged for theoretical studies. A large initialization variance leads to the _lazy training_ regime, where the weights do not move significantly, and the training dynamics is captured by the neural tangent kernel (NTK) [1, 12]. However, there are many instances of function classes that are efficiently learnable by neural networks and not efficiently learnable by the NTK [22, 13]. Under a smaller initialization scaling, gradient descent on infinite-width neural networks becomes equivalent to Wasserstein gradient flow on the space of measures, known as the mean-field limit [1, 14, 15, 16, 17, 18], which can learn certain low-dimensional target functions efficiently [19, 1, 2, 10].

As for neural networks with smaller width, recent works showed that a two-stage feature learning procedure can outperform the NTK when the data is sampled uniformly from the hypercube [1] or isotropic Gaussian [20, 1, 18, 15, 14, 16]. However, these results do not take into account the additional structure that might be present in the covariance matrix of the input data. Two notable exceptions are [13, 14], where the authors analyzed a spiked covariance and Gaussian mixture data, respectively. Our setting is closer to [13], however, they do not provide optimization guarantees through gradient-based training. Furthermore, in a companion work [1], we zoom into the setting where the spike and target are perfectly aligned (\(r_{1}=0\)), and prove learnability in the \(n\asymp d\) regime for both kernel regression and two-layer neural network. Finally, we go over some results concurrent to our work in Appendix A.

**Learning Single Index Models.** The problem of estimating the relevant direction in a single index model is classical in statistics [10], with efficient dedicated algorithms ([17, 18] among others). However, these algorithms are non-standard and instead, we are concerned with standard iterative algorithms like training neural networks with gradient descent. Recently, [1] considered an iterative optimization procedure for learning such models with a polynomial sample complexity that is controlled by the smoothness of the link function. [1] considered the effect of taking a single gradient step on the ability of a two-layer neural network to learn a single index model, and [15, 16] considered training a special two-layer neural network architecture where all neurons share the same weight with gradient flow or online SGD. However, these works only consider the isotropic Gaussian input, and the effect of anisotropy in the covariance matrix when training a neural network to learn a single index model has remained unclear.

**Training a Single Neuron with Gradient Descent.** When training the first layer, we consider a setting where there is only one effective neuron. A large body of works exists on training a single neuron using variants of gradient descent. In the realizable setting (i.e. identical link and activation), the typical assumptions on the activation correspond to information exponent 1 as the activations are required to be monotone or have similar properties, see e.g. [12, 13, 14]. In the agnostic setting, [15] considered initializing from the origin which is a saddle point for information exponent larger than 1. [1] also considered the agnostic learning of a ReLU activation, albeit their sample complexity is not explicit other than being polynomial in dimension.

## 2 Preliminaries: Statistical Model and Training Procedure

For a \(d\)-dimensional input \(\mathbf{x}\) and a link function \(g\in L^{2}(\gamma)\), consider the single index model

\[y=g\Big{(}\frac{\langle\mathbf{u},\mathbf{x}\rangle}{\|\mathbf{\Sigma}^{1/2}\mathbf{u}\|}\Big{)} +\epsilon\ \ \text{with}\ \ \mathbf{x}\sim\mathcal{N}(0,\mathbf{\Sigma}), \tag{2.1}\]

where \(\epsilon\) is a zero-mean noise with \(\mathcal{O}(1)\) sub-Gaussian norm and \(\mathbf{u}\in\mathbb{S}^{d-1}\). Learning the model (2.1) corresponds to approximately recovering the unknown link \(g\) and the unknown direction \(\mathbf{u}\). Note that a normalization is needed to make this problem well-defined; without loss of generality, we write \(\langle\mathbf{u},\mathbf{x}\rangle/\|\mathbf{\Sigma}^{1/2}\mathbf{u}\|\) to ensure that the input variance and the scaling of \(g\) both remain independent of the conditioning of \(\mathbf{\Sigma}\). For this learning task, we will use a two-layer neural network of the form

\[\hat{y}(\mathbf{x};\mathbf{W},\mathbf{a},\mathbf{b})\coloneqq\sum_{i=1}^{m}a_{i}\phi(\langle \mathbf{w}_{i},\mathbf{x}\rangle+b_{i}), \tag{2.2}\]

where \(\mathbf{W}=\{\mathbf{w}_{i}\}_{i=1}^{m}\) is the \(m\times d\) matrix whose rows corresponds to first-layer weights \(\mathbf{w}_{i}\), \(\mathbf{a}=\{a_{i}\}_{i=1}^{m}\) denote the second-layer weights, \(\mathbf{b}=\{b_{i}\}_{i=1}^{m}\) denote the biases, and \(\phi\) is the non-linearactivation function. We assume \(g\) and \(\phi\) are weakly differentiable with weak derivatives \(g^{\prime}\) and \(\phi^{\prime}\) respectively, and \(g,g^{\prime},\phi,\phi^{\prime}\in L^{2}(\gamma)\). We are interested in the high-dimensional regime; thus, \(d\) is assumed to be sufficiently large throughout the paper. Our ultimate goal is to learn both unknowns \(g\) and \(\mathbf{u}\) by minimizing the population risk

\[R(\mathbf{W},\mathbf{a},\mathbf{b})\coloneqq\frac{1}{2}\operatorname{\mathbb{E}}\bigl{[}( \hat{y}(\mathbf{x};\mathbf{W},\mathbf{a},\mathbf{b})-y)^{2}\bigr{]}, \tag{2.3}\]

using a gradient-based training method such as gradient flow.

We follow the two-step training procedure employed in recent works [1, 2, 3, 1]: First, we train the first-layer weights \(\mathbf{W}\) to learn the unknown direction \(\mathbf{u}\); at the end of this stage, the neurons \(\mathbf{w}_{i}\) align with \(\mathbf{u}\). Here, the goal is to recover only the direction. Next, using random biases and training the second-layer weights, we obtain a good approximation for the unknown link function \(g\). In the majority of this work, we focus on the first part of this two-stage procedure as the alignment between \(\mathbf{w}_{i}\)'s and \(\mathbf{u}\) essentially determines the sample complexity of the overall procedure. This problem is somewhat equivalent to the simplified problem of minimizing (2.3) with \(m=1\), \(a_{1}=1\), \(b_{1}=0\), i.e., \(\hat{y}(\mathbf{x};\mathbf{W},\mathbf{a},\mathbf{b})\) is replaced with \(\hat{y}(\mathbf{x};\mathbf{w})\coloneqq\phi(\langle\mathbf{w},\mathbf{x}\rangle)\) and we write \(R(\mathbf{w})\coloneqq R(\mathbf{W},\mathbf{a},\mathbf{b})\) for simplicity. We emphasize that unless \(\phi=g\) (i.e. the link function is known), the first stage of training only recovers the relevant direction \(\mathbf{u}\) and is not able to approximate \(g\). Indeed, \(m>1\) is often needed to learn the non-linear link function; this is the focus of Section 4.2 where we derive a complete learnability result for a two-layer neural network with \(m>1\).

Characteristics of the link function play an important role in the complexity of learning the model. As such, a central part of our analysis will rely on a particular property based on the Hermite expansion of functions in a basis defined by the normalized Hermite polynomials \(\{h_{j}\}_{j\geq 0}\) given as

\[h_{j}(z)=\tfrac{(-1)^{j}e^{\mathbf{x}^{2}/2}}{\sqrt{j!}}\tfrac{\mathrm{d}^{j}}{ \mathrm{d}z^{j}}e^{-z^{2}/2}. \tag{2.4}\]

These polynomials form an orthonormal basis in the space \(L^{2}(\gamma)\), and the resulting expansion yields the following measure of complexity for \(g\), which is termed as _the information exponent_.

**Definition 1** (Information exponent).: _Let \(g=\sum_{j\geq 0}\alpha_{j}h_{j}\) be the Hermite expansion of \(g\). The information exponent of \(g\) is defined to be \(s\coloneqq\inf\{j>0\,:\,\alpha_{j}\neq 0\}\)._

This concept was introduced in [1] in a more general framework, and our definition is more in line with the setting in [1]. We remark that the definition of [1] can be modified to handle anisotropy in which case one arrives at Definition 1. We provide a detailed discussion on this concept together with some properties of the Hermite expansion in Appendix B. Throughout the paper, we assume that the information exponent does not grow with dimension.

In the case where the \(d\)-dimensional input data is isotropic, [1] showed that learning a single index target with full-batch gradient flow requires a sample complexity of \(\tilde{\mathcal{O}}(d^{s})\) for \(s\geq 3\) where \(s\) is the information exponent of \(g\). We will show that this sample complexity can be improved under anisotropy. More specifically, if the input covariance \(\mathbf{\Sigma}\) has non-trivial alignment with the unknown direction \(\mathbf{u}\), we prove in Section 3 that the resulting sample complexity can be even made independent of the information exponent if we use a certain normalization in the training. In what follows, we prove that such a normalization in training procedure is indeed necessary.

### The spiked model and limitations of covariance-agnostic training

In practice, data often exhibit a certain structure which may have a profound impact on the statistical procedure. A well-known model that captures such a structure is the _spiked model_[2] for which one or several large eigenvalues of the input covariance matrix \(\mathbf{\Sigma}\) are separated from the bulk of the spectrum (see also [1, 2]). Although our results hold for generic covariance matrices, they reveal interesting phenomena under the following spiked model assumption.

**Assumption 1**.: _The covariance \(\mathbf{\Sigma}\) follows the \((\kappa,\mathbf{\theta})\)-spiked model if \(\mathbf{\Sigma}=\tfrac{\mathbf{I}_{d}+\kappa\mathbf{\theta}\mathbf{\theta}^{\top}}{1+\kappa}\) where \(\|\mathbf{\theta}\|=1\)._

In pursuit of the target (unit) direction \(\mathbf{u}\), the magnitude of the neuron \(\mathbf{w}\) is immaterial; thus, recent works take advantage of this and simplify the optimization trajectory by projecting \(\mathbf{w}\) onto unit sphere \(\mathbb{S}^{d-1}\) throughout the training process [1, 1]. In the sequel, we study the same dynamics which is agnostic to the input covariance in order to motivate our investigation of normalized gradient flow in Section 3. More specifically, we consider the spherical population gradient flow

\[\frac{\mathrm{d}\mathbf{w}^{t}}{\mathrm{d}t}=-\nabla^{S}R(\mathbf{w}^{t})\;\;\text{where} \;\;\nabla^{S}R(\mathbf{w})=\nabla R(\mathbf{w})-\langle\nabla R(\mathbf{w}),\mathbf{w}\rangle \mathbf{w}. \tag{2.5}\]

where \(\nabla^{S}\) is the spherical gradient at the current iterate. It is straightforward to see that when the initialization \(\mathbf{w}^{0}\) is on the unit sphere, the entire flow will remain on the unit sphere, i.e. \(\mathbf{w}^{t}\in\mathbb{S}^{d-1}\) for all \(t\geq 0\). The flow (2.5) has been proven useful for learning the direction \(\mathbf{u}\)[1] in the isotropic case \(\mathbf{\Sigma}=\mathbf{I}_{d}\) when the activation \(\phi\) is ReLU. In contrast, when \(\mathbf{\Sigma}\) follows a spiked model, we show that it can get stuck at stationary points that are almost orthogonal to \(\mathbf{u}\). Indeed, when the input covariance \(\mathbf{\Sigma}\) has a spike in the target direction \(\mathbf{u}\), i.e. \(\mathbf{\theta}=\mathbf{u}\), one expects that the training procedure benefits from this as the input \(\mathbf{x}\) contains information about the sought unknown \(\mathbf{u}\) without even querying the response \(y\). The following result proves the contrary; for moderate spike magnitudes, the alignment between the first-layer weights and target \(\langle\mathbf{w}^{t},\mathbf{u}\rangle\) will be insignificant for all \(t\).

**Theorem 2**.: _Let \(s>2\) be the information exponent of \(g\) with \(\mathbb{E}[g]=0\), and assume \(\mathbf{\Sigma}\) follows the \((\kappa,\mathbf{u})\)-spiked model with \(\Omega(1)\leq\kappa\leq\mathcal{O}(d^{\frac{s-2}{2}})\). For ReLU activation, let \(\mathbf{w}^{t}\) denote the solution to (2.5) initialized uniformly at random over \(\mathbb{S}^{d-1}\), then with probability at least \(0.99\),_

\[\sup_{t\geq 0}\bigl{|}\langle\mathbf{w}^{t},\mathbf{u}\rangle\bigr{|}\lesssim 1/\sqrt{d}, \tag{2.6}\]

A non-trivial alignment between the first-layer weights \(\mathbf{w}^{t}\) and the target direction \(\mathbf{u}\) is required to learn the single index model (2.1). However, the above result implies that in high dimensions when \(d\gg 1\), the alignment is negligible in the population limit (when the number of samples goes to infinity). We remark that when the spike magnitude is large, i.e. \(\kappa\geq\Omega(d)\), the flow (2.5) can achieve alignment as the problem essentially becomes one-dimensional, as we demonstrate in Appendix C.

To see why the flow (2.5) gets stuck at saddle points and fails to recover the true direction, notice that

\[R(\mathbf{w})=\tfrac{1}{2}\,\mathbb{E}\Bigl{[}\bigl{(}\phi(\langle\mathbf{w},\mathbf{x} \rangle)-y\bigr{)}^{2}\Bigr{]}=\tfrac{1}{2}\,\mathbb{E}\bigl{[}\phi(\langle \mathbf{w},\mathbf{x}\rangle)^{2}\bigr{]}-\mathbb{E}[\phi(\langle\mathbf{w},\mathbf{x}\rangle) y]+\tfrac{1}{2}\,\mathbb{E}\bigl{[}y^{2}\bigr{]}. \tag{2.7}\]

If the input was isotropic, i.e. \(\mathbf{x}\sim\mathcal{N}(0,\mathbf{I}_{d})\), the first term in (2.7) would be equal to \(\|\phi\|_{\gamma}^{2}\), which is independent of \(\mathbf{w}\). Thus, minimizing \(R(\mathbf{w})\) in this case is equivalent to maximizing the "correlation" term \(\mathbb{E}[\phi(\langle\mathbf{w},\mathbf{x}\rangle)y]\). However, under the spiked model, the alignment between \(\mathbf{w}\) and \(\mathbf{u}\) breaks the symmetry; consequently, the first term in the decomposition grows with \(\langle\mathbf{w},\mathbf{u}\rangle\), creating a repulsive force that traps the dynamics around the equator where \(\mathbf{w}\) is almost orthogonal to \(\mathbf{u}\).

## 3 Main Results: Alignment via Normalized Dynamics

Having established that the covariance-agnostic training dynamics (2.5) is likely to fail, we consider a covariance-aware normalized flow in this section and show that it can achieve alignment with the unknown target and enjoy better sample complexity compared to the existing results [1, 1] in the isotropic case. We start with the population dynamics.

### Warm-up: Population dynamics

To simplify the exposition, we define \(\mathbf{z}\coloneqq\mathbf{\Sigma}^{-1/2}\mathbf{x}\), \(\overline{\mathbf{w}}\coloneqq\mathbf{\Sigma}^{1/2}\mathbf{w}/\|\mathbf{\Sigma}^{1/2}\mathbf{w}\|\) and similarly define \(\overline{\mathbf{u}}\), and consider the prediction function \(\hat{y}(\mathbf{x};\overline{\mathbf{w}}):=\phi(\langle\overline{\mathbf{w}},\mathbf{z}\rangle)\). Due to symmetry, the second moment of the prediction is \(\mathbb{E}\bigl{[}\hat{y}(\mathbf{x};\overline{\mathbf{w}})^{2}\bigr{]}=\|\phi\|_{ \gamma}^{2}\) which is independent of \(\mathbf{w}\); thus, the population risk reads

\[\mathcal{R}(\mathbf{w})\coloneqq\tfrac{1}{2}\,\mathbb{E}\Bigl{[}\bigl{(}\hat{y}( \mathbf{x};\overline{\mathbf{w}})-y\bigr{)}^{2}\Bigr{]}=\tfrac{1}{2}\|\phi\|_{\gamma}^{ 2}+\tfrac{1}{2}\,\mathbb{E}\bigl{[}y^{2}\bigr{]}-\mathbb{E}[\phi(\langle \overline{\mathbf{w}},\mathbf{z}\rangle)y]. \tag{3.1}\]

In (3.1), the only term that depends on the weights \(\mathbf{w}\) is the correlation term and the source of the repulsive force in (2.7) is eliminated; we have \(\nabla_{\mathbf{w}}\mathcal{R}(\mathbf{w})=-\nabla_{\mathbf{w}}\,\mathbb{E}[\phi(\langle \overline{\mathbf{w}},\mathbf{z}\rangle)y]\). Based on this, we use the following normalized gradient flow for training

\[\frac{\mathrm{d}\mathbf{w}^{t}}{\mathrm{d}t}=-\eta(\mathbf{w}^{t})\nabla_{\mathbf{w}} \mathcal{R}(\mathbf{w}^{t})\;\;\text{where}\;\;\eta(\mathbf{w})=\|\mathbf{\Sigma}^{1/2} \mathbf{w}\|^{2}. \tag{3.2}\]We remark that, though not identical, this normalization is closely related to batch normalization which is commonly employed in practice [15]. Under the invariance provided by the current normalization, minimizing \(\hat{\mathcal{R}}(\mathbf{w})\) corresponds to maximizing \(\mathbb{E}[\phi(\langle\overline{\mathbf{w}},\mathbf{z}\rangle)y]\). Thus, instead of \(\mathbf{w}\), it will be more useful to track the dynamics of its normalized counterpart \(\overline{\mathbf{w}}\), which is made possible by the following intermediary result that follows from Stein's lemma; also see e.g. [1, 23].

**Lemma 3**.: _Suppose we train \(\mathbf{w}^{t}\) using the gradient flow (3.2). Then \(\overline{\mathbf{w}}^{t}\) solves the following ODE_

\[\frac{\mathrm{d}\overline{\mathbf{w}}^{t}}{\mathrm{d}t}=-\zeta_{\phi,g}(\langle \overline{\mathbf{w}}^{t},\overline{\mathbf{u}}\rangle)(\mathbf{I}_{d}-\overline{\mathbf{ w}}^{t}\overline{\mathbf{w}}^{t^{\top}})\mathbf{\Sigma}(\mathbf{I}_{d}-\overline{\mathbf{w}}^{t} \overline{\mathbf{w}}^{t^{\top}})\overline{\mathbf{u}}, \tag{3.3}\]

_where \(\zeta_{\phi,g}(\langle\overline{\mathbf{w}},\overline{\mathbf{u}}\rangle)\coloneqq- \mathbb{E}[\phi^{\prime}(\langle\overline{\mathbf{w}},\mathbf{z}\rangle)g^{\prime}( \langle\overline{\mathbf{u}},\mathbf{z}\rangle)]\)._

We will investigate if the modified flow (3.3) achieves alignment; in this context, alignment corresponds to \(\langle\overline{\mathbf{w}}^{t},\overline{\mathbf{u}}\rangle\approx 1\). Towards that end, we make the following assumption.

**Assumption 2**.: _Let \(g=\sum_{j\geq 0}\alpha_{j}h_{j}\) and \(\phi=\sum_{j\geq 0}\beta_{j}h_{j}\) be the Hermite decomposition of \(g\) and \(\phi\) respectively. Let \(s\) be the information exponent of \(g\). For some universal constant \(c>0\), we assume_

\[\zeta_{\phi,g}(\omega)=-\sum_{j>0}j\alpha_{j}\beta_{j}\,\omega^{j-1}\leq-c\, \omega^{s-1},\quad\quad\forall\omega\in(0,1).\]

There are several important examples that readily satisfy Assumption 2. The obvious example is when the link function is known as in [1], i.e. \(\phi=g\). A more interesting example is when \(\phi\) is an activation with degree \(s\) non-zero Hermite coefficient (e.g. ReLU when \(s\) is even, see [1, 2]) and \(g\) is a degree \(s\) Hermite polynomial, which for \(s=2\) corresponds to the phase retrieval problem. In this case, the assumption is satisfied if \(\alpha_{s}\) and \(\beta_{s}\) have the same sign, which occurs with probability \(0.5\) if we randomly choose the sign of the second layer.

Under this condition, the following result shows that the population flow (3.3) can achieve alignment.

**Proposition 4**.: _Suppose Assumption 2 holds and consider the gradient flow given by (3.3) with initialization satisfying \(\langle\overline{\mathbf{w}}^{0},\overline{\mathbf{u}}\rangle>0\). Then, we have \(\langle\overline{\mathbf{w}}^{T},\overline{\mathbf{u}}\rangle\geq 1-\varepsilon\) as soon as_

\[T\asymp\frac{\tau_{s}\big{(}\langle\overline{\mathbf{w}}^{0},\overline{\mathbf{u}} \rangle\big{)}+\ln(1/\varepsilon)}{\lambda_{\min}(\mathbf{\Sigma})}\;\;\text{ where}\;\;\tau_{s}(z)\coloneqq\begin{cases}1&s=1\\ \ln(1/z)&s=2\\ (1/z)^{s-2}&s>2\end{cases}. \tag{3.4}\]

We remark that the information exponent enters the rate in (3.4) through the function \(\tau_{s}\), and time needed to achieve \(\varepsilon\) alignment gets worse with larger information exponent. Indeed, it is understood that this quantity serves as a measure of complexity for the target function being learned.

### Empirical dynamics and sample complexity

Given \(n\) i.i.d. samples \(\{(\mathbf{x}^{(i)},y^{(i)})\}_{i=1}^{n}\) from the single index model (2.1), we consider the flow

\[\frac{\mathrm{d}\mathbf{w}^{t}}{\mathrm{d}t}=-\eta(\mathbf{w}^{t})\nabla\hat{\mathcal{ R}}(\mathbf{w}^{t})\;\;\text{with}\;\;\nabla\hat{\mathcal{R}}(\mathbf{w})\coloneqq- \nabla_{\mathbf{w}}\bigg{\{}\frac{1}{n}\sum_{i=1}^{n}\phi\bigg{(}\frac{\langle \mathbf{w},\mathbf{x}^{(i)}\rangle}{\|\hat{\mathbf{\Sigma}}^{1/2}\mathbf{w}\|}\bigg{)}y^{(i)} \bigg{\}}, \tag{3.5}\]

where we estimate the covariance matrix \(\mathbf{\Sigma}\) using the sample mean \(\hat{\mathbf{\Sigma}}\coloneqq\frac{1}{n^{\prime}}\sum_{i=1}^{n^{\prime}}\mathbf{x}^{ (i)}{\mathbf{x}^{(i)}}^{\top}\) over \(n^{\prime}\) i.i.d. samples; the above dynamics defines an empirical gradient flow. Notice that we ignored the gradient associated with the term \(\phi^{2}\) since the population dynamics ensures that its gradient will concentrate around zero; thus, it is redundant to estimate this term. Below, we will use \(n^{\prime}=n\) for smooth activations, i.e. the same dataset can be used for covariance estimation; For ReLU, we require a more accurate covariance estimator, thus, we use \(n^{\prime}\gtrsim n^{2}\) by assuming access to an additional \(n^{\prime}-n\) unlabeled data points. Similar to the previous section, we track the dynamics of normalized \(\mathbf{w}\) by defining \(\overline{\mathbf{w}}\coloneqq\hat{\mathbf{\Sigma}}^{1/2}\mathbf{w}/\|\hat{\mathbf{\Sigma}}^{1 /2}\mathbf{w}\|\) (and leave \(\overline{\mathbf{u}}\) unchanged from Section 3.1). The same arguments as in Lemma 3 allow us to track the evolution of \(\overline{\mathbf{w}}\), which ultimately yields the following alignment result under general covariance structure.

**Theorem 5**.: _Let \(s\) be the information exponent of \(g\), and assume it satisfies \(|g(\cdot)|\lesssim 1+|\cdot|^{p}\) for some \(p>0\). For \(\phi\) denoting either the ReLU activation or a smooth activation satisfying \(|\phi^{\prime}|\vee|\phi^{\prime\prime}|\lesssim 1\), suppose Assumption 2 holds. For any \(\varepsilon>0\), suppose we run the finite sample gradient flow (3.5) with \(\eta(\mathbf{w})=\|\hat{\mathbf{\Sigma}}^{1/2}\mathbf{w}\|^{2}\), initialized such that \(\langle\overline{\mathbf{w}}^{0},\overline{\mathbf{u}}\rangle>0\), and with number of samples_

\[n\gtrsim d\varkappa(\mathbf{\Sigma})^{2}\Big{\{}\langle\overline{\mathbf{w}}^{0}, \overline{\mathbf{u}}\rangle^{2(1-s)}\vee\varepsilon^{-2}\Big{\}},\]_where \(\varkappa(\mathbf{\Sigma})\) is the condition number of \(\mathbf{\Sigma}\). Then, for \(T\asymp\frac{\tau_{s}\left(\left\langle\overline{\mathbf{w}}^{0},\overline{\mathbf{u}} \right\rangle\right)+\ln(1/\varepsilon)}{\lambda_{\min}(\mathbf{\Sigma})}\), we have_

\[\left\langle\overline{\mathbf{w}}^{T},\overline{\mathbf{u}}\right\rangle\geq 1- \varepsilon, \tag{3.6}\]

_with probability at least \(1-c_{1}d^{-c_{2}}\) for some universal constants \(c_{1},c_{2}>0\) over the randomness of the dataset. Here, \(\tau_{s}\) is defined in (3.4) and \(\gtrsim\) hides poly-logarithmic factors._

RemarkWe make the following remarks on the above theorem.

* The initial condition \(\left\langle\overline{\mathbf{w}}^{0},\overline{\mathbf{u}}\right\rangle>0\) is required when we have odd information exponent. When \(\mathbf{w}^{0}\) is initialized uniformly over \(\mathbb{S}^{d-1}\), the condition holds with probability \(0.5\) over the initialization. See [1, Remark 1.8] for further discussion on this condition.
* Although \(\overline{\mathbf{w}}\) is defined using the empirical covariance unlike \(\overline{\mathbf{u}}\) which is defined by population covariance, this definition is the suitable choice to approximate the target function \(g\) (c.f. Theorem 9), since it ensures the arguments of \(\phi\) and \(g\) are sufficiently close when \(\overline{\mathbf{w}}\) recovers \(\overline{\mathbf{u}}\).

The intuition behind the proof of Theorem 5 is presented in Section 5 with the complete proof in the appendix. We highlight that the improvement in the sample complexity compared to the isotropic setting occurs whenever the covariance structure induces a stronger initial alignment and consequently stronger signal. The following corollary demonstrates a concrete example of such improvement by specializing Theorem 5 for a spiked covariance model.

**Corollary 6**.: _Consider the setting of Theorem 5 with \(\mathbf{\Sigma}\) following the \((\kappa,\mathbf{\theta})\)-spiked model, where \(\left\langle\mathbf{u},\mathbf{\theta}\right\rangle\asymp d^{-r_{1}}\) and \(\kappa\asymp d^{r_{2}}\) with \(r_{1}\in[0,1/2]\) and \(r_{2}\in[0,1]\). Suppose \(\mathbf{w}^{0}\) is sampled uniformly from \(\mathbb{S}^{d-1}\). Then, when conditioned on \(\left\langle\overline{\mathbf{w}}^{0},\overline{\mathbf{u}}\right\rangle>0\), the sample complexity in Theorem 5 reads_

\[n\gtrsim\begin{cases}d^{1+2r_{2}}\big{(}d^{s-1}\vee\varepsilon^{-2}\big{)}&0<r _{2}<r_{1}\\ d^{1+2r_{2}}\big{(}d^{(s-1)(1-2(r_{2}-r_{1}))}\vee\varepsilon^{-2}\big{)}&r_{ 1}<r_{2}<2r_{1}\\ d^{1+2r_{2}}\big{(}d^{(s-1)(1-r_{2})}\vee\varepsilon^{-2}\big{)}&2r_{1}<r_{2}< 1\end{cases}, \tag{3.7}\]

_where \(\gtrsim\) hides poly-logarithmic factors of \(d\)._

RemarkWe have the following observations on the above sample complexity.

* Corollary 6 demonstrates that structured data can lead to better sample complexity when the right normalization is used during training. This complements Theorem 2 where we recall that spherical training dynamics ignores the structure in data and the target direction cannot be recovered.
* When \(g\) is a polynomial of degree \(p\), the lower bound for rotationally invariant kernels (including the neural tangent kernel at initialization) implies a complexity of at least \(d^{\Omega((1-r_{2})p)}\)[15]. Thus the sample complexity of Corollary 6 can always outperform the kernel lower bound when \(p\) is sufficiently large and \(s\) remains constant.

Three-step phase transitionRecall that in the isotropic setting \(\mathbf{\Sigma}=\mathbf{I}_{d}\), the sample complexity of learning \(g\) with information exponent \(s\) using full-batch gradient flow is \(\tilde{\mathcal{O}}(d^{s})\) for \(s\geq 3\)[1]. The sample complexity in Corollary 6 is strictly smaller than \(\tilde{\mathcal{O}}(d^{s})\) as soon as \((s-1)r_{1}/(s-2)<r_{2}\). Furthermore, for any \(\nu>0\) it is at most \(\tilde{\mathcal{O}}(d^{3+\nu})\) as soon as \(r_{2}\geq 1-\nu/(s-3)\) and \(2r_{1}<r_{2}\), in which case the sample complexity becomes independent of the information exponent. Interestingly, the complexity becomes independent of \(r_{1}\) when \(r_{2}>2r_{1}\) or \(r_{2}<r_{1}\), i.e. the direction of the spike becomes irrelevant when the spike magnitude is sufficiently large or small.

The three-stage phase transition of Corollary 6 is due to the different behaviour of the inner product \(\left\langle\overline{\mathbf{w}}^{0},\overline{\mathbf{u}}\right\rangle\) in different regimes of \(r_{1}\) and \(r_{2}\). When \(r_{2}<r_{1}\), we have \(\left\langle\overline{\mathbf{w}}^{0},\overline{\mathbf{u}}\right\rangle\asymp\left \langle\mathbf{w}^{0},\mathbf{u}\right\rangle\), thus the initial alignment is just as uninformative as the isotropic case providing no improvement. Moreover, a potentially large condition number may hurt the sample complexity in this case. On the other hand, when \(r_{1}<r_{2}<2r_{1}\) we have \(\left\langle\overline{\mathbf{w}}^{0},\overline{\mathbf{u}}\right\rangle\asymp\kappa \left\langle\mathbf{u},\mathbf{\theta}\right\rangle\left\langle\mathbf{w}^{0},\mathbf{\theta}\right\rangle\), and \(r_{2}>2r_{1}\) leads to \(\left\langle\overline{\mathbf{w}}^{0},\overline{\mathbf{u}}\right\rangle\asymp\sqrt{ \kappa}\left\langle\mathbf{w}^{0},\mathbf{\theta}\right\rangle\), thus large \(\kappa\) or \(\left\langle\mathbf{u},\mathbf{\theta}\right\rangle\) in this regime may improve the sample complexity.

Implications to Neural Networks and Further Improvements

### Improving Sample Complexity via Preconditioning

We now demonstrate that preconditioning the training dynamics with \(\hat{\mathbf{\Sigma}}^{-1}\) can remove the dependency on \(\varkappa(\mathbf{\Sigma})\), ultimately improving the sample complexity. Consider the preconditioned gradient flow

\[\frac{\mathrm{d}\mathbf{w}^{t}}{\mathrm{d}t}=-\eta(\mathbf{w}^{t})\hat{\mathbf{\Sigma}}^{-1 }\nabla\hat{\mathcal{R}}(\mathbf{w}^{t})\ \ \text{ with }\ \ \eta(\mathbf{w})=\|\hat{\mathbf{\Sigma}}^{1/2}\mathbf{w}\|^{2}. \tag{4.1}\]

We have the following alignment result.

**Theorem 7**.: _Consider the same setting as Theorem 5, and assume we run the preconditioned empirical gradient flow (4.1) with number of samples_

\[n\gtrsim d\Big{\{}\big{\langle}\overline{\mathbf{w}}^{0},\overline{\mathbf{u}}\big{\rangle} ^{2(1-s)}\vee\varepsilon^{-2}\Big{\}},\]

_where \(\gtrsim\) hides poly-logarithmic factors of \(d\). Then, for \(T\asymp\tau_{s}\big{(}\big{\langle}\overline{\mathbf{w}}^{0},\overline{\mathbf{u}} \big{\rangle}\big{)}+\ln(1/\varepsilon),\) we have_

\[\big{\langle}\overline{\mathbf{w}}^{T},\overline{\mathbf{u}}\big{\rangle}\geq 1- \varepsilon,\]

_with probability at least \(1-c_{1}d^{-c_{2}}\) for some universal constants \(c_{1},c_{2}>0\)._

Preconditioning removes the condition number dependence, which is particularly important in the spiked model case where this quantity can be large.

**Corollary 8**.: _Consider the setting of Theorem 7, and assume we run the preconditioned empirical gradient flow (4.1) for the \((\kappa,\mathbf{\theta})\)-spiked model where \(\langle\mathbf{u},\mathbf{\theta}\rangle\asymp d^{-r_{1}}\) and \(\kappa\asymp d^{r_{2}}\) with \(r_{1}\in[0,1/2]\) and \(r_{2}\in[0,1]\). Suppose \(\mathbf{w}^{0}\) is sampled uniformly from \(\mathbb{S}^{d-1}\). Then, when conditioned on \(\big{\langle}\overline{\mathbf{w}}^{0},\overline{\mathbf{u}}\big{\rangle}>0\), the sample complexity of Theorem 7 reads_

\[n\gtrsim\begin{cases}d\big{(}d^{s-1}\vee\varepsilon^{-2}\big{)}&0<r_{2}<r_{1} \\ d\big{(}d^{(s-1)(1-2(r_{2}-r_{1}))}\vee\varepsilon^{-2}\big{)}&r_{1}<r_{2}<2r_ {1}\\ d\big{(}d^{(s-1)(1-r_{2})}\vee\varepsilon^{-2}\big{)}&2r_{1}<r_{2}<1\end{cases}, \tag{4.2}\]

_where \(\gtrsim\) hides poly-logarithmic factors of \(d\)._

The above result improves upon Corollary 6; thus, making a case for preconditioning in practice. The complexity results also strictly improve upon the \(\tilde{\mathcal{O}}(d^{s})\) complexity in the isotropic case [2] when \(r_{2}>r_{1}\). Further, for any \(\nu>0\), we can obtain the complexity of \(\tilde{\mathcal{O}}(d^{1+\nu})\) (nearly linear in dimension) when \(r_{2}>1-\nu/(s-1)\) and \(r_{2}>2r_{1}\) or \(r_{1}+1/2(1-\nu/(s-1))<r_{2}<2r_{1}\). In addition to the remarks of Corollary 6, we note that the complexity is independent of both \(r_{1}\) and \(r_{2}\) when \(r_{2}<r_{1}\) (cf. Figure 1 hard regime), i.e. the spike magnitude and the spike-target alignment have no effect on the complexity unless \(r_{2}\geq r_{1}\).

Under the spiked covariance model, one could improve the above results by instead using spectral initialization, i.e. initializing at \(\mathbf{\theta}\), which can be estimated from unlabeled data. Assuming perfect access to \(\mathbf{\theta}\), using the statement of Theorems 5 and 7, this initialization would imply a sample complexity of \(\tilde{\mathcal{O}}(d^{1+2r_{2}+((s-1)(2r_{1}-r_{2})\lor 0)})\) without and \(\tilde{\mathcal{O}}(d^{1+((s-1)(2r_{1}-r_{2})\lor 0)})\) with preconditioning.

### Two-layer neural networks and learning the link function

Our main focus so far was learning the target direction \(\mathbf{u}\). Next, we consider learning the unknown link function with a neural network, providing a complete learnability result for single index models.

We use Algorithm 1 and train the first-layer of the neural network with either the empirical gradient flow (3.5) or the preconditioned version (4.1). Then, we randomly choose the bias units and minimize the second layer weights using another gradient flow. Our goal is to track the sample complexity \(n\) needed to learn the single index target which we compare against the results of [2]. We highlight that layer-wise training in Algorithm 1 is frequently employed in the literature [1, 2, 2, 2] and in particular [2] also used gradient flow for training.

**Theorem 9**.: _Let \(g\) be twice weakly differentiable with information exponent \(s\) and assume \(g^{\prime\prime}\) has at most polynomial growth. Suppose \(\phi\) is the ReLU activation, Assumption 2 holds and we run Algorithm 1 with \(\mathbf{w}^{0}\) initialized uniformly over \(\mathbb{S}^{d-1}\). For any \(\varepsilon>0\), let \(n\) and \(T\) be chosen according to Theorem 5 when we run the gradient flow (3.5) and Theorem 7 when we run the preconditioned gradient flow (4.1). Then, for \(\Delta\asymp\sqrt{\ln(nd)}\), some regime of \(\lambda\) given by (E.3) and sufficiently large \(T^{\prime}\) given by (E.4), we have_

\[\mathbb{E}_{(\mathbf{x},y)}\bigg{[}\Big{(}\hat{y}(\mathbf{x};\mathbf{W}^{T}, \mathbf{a}^{T^{\prime}},\mathbf{b})-y\Big{)}^{2}\bigg{]}\leq C_{1}\,\mathbb{E}\big{[} \epsilon^{2}\big{]}+C_{2}(\varepsilon+1/m), \tag{4.3}\]

_conditioned on \(\big{\langle}\overline{\mathbf{w}}^{0},\overline{\mathbf{u}}\big{\rangle}>0\) with probability at least \(0.99\) over the randomness of the dataset, biases, and initialization, where \(C_{1}\) is a universal constant and \(C_{2}\) hides \(\mathrm{polylog}(m,n,d)\) factors._

The next result immediately follows from the previous theorem together with Corollaries 6 & 8.

**Corollary 10**.: _In the setting of Theorem 9, if \(\mathbf{\Sigma}\) follows the \((\kappa,\mathbf{\theta})\)-spiked model, the sample complexity \(n\) is given by (3.7) if we use the empirical gradient flow and (4.2) if we use the preconditioned version._

We remark that for fixed \(\varepsilon\), the sample complexity to learn \(g\) in the isotropic case is \(\tilde{\mathcal{O}}(d^{s})\)[1]. Under the spiked model, if we assume that \(r_{2}\) is sufficiently large and \(r_{1}\) is sufficiently small as discussed in the previous section, Corollary 10 improves this rate to either (3.7) when the empirical gradient flow is used without preconditioning or to (4.2) with preconditioning.

## 5 Technical Overview

In this section, we briefly discuss the key intuitions that lead to the proof of our main results. We first review the case \(\mathbf{\Sigma}=\mathbf{\mathrm{I}}_{d}\), where we have the following decomposition for population loss

\[R(\mathbf{w})\coloneqq\tfrac{1}{2}\,\mathbb{E}\big{[}(\phi(\langle \mathbf{w},\mathbf{x}\rangle)-y)^{2}\big{]}=\tfrac{1}{2}\|\phi\|_{\gamma}^{2}+\tfrac{1 }{2}\,\mathbb{E}\big{[}y^{2}\big{]}-\mathbb{E}[\phi(\langle\mathbf{w},\mathbf{x} \rangle)g(\langle\mathbf{u},\mathbf{x}\rangle)]. \tag{5.1}\]

Notice that the only term contributing to the population gradient is the last term which measures the correlation between \(\phi\) and \(g\). Following the gradient flow and applying Stein's lemma yields

\[\frac{\mathrm{d}\langle\mathbf{w}^{t},\mathbf{u}\rangle}{\mathrm{d}t}= \mathbb{E}\big{[}\phi^{\prime}(\langle\mathbf{w}^{t},\mathbf{x}\rangle)g^{\prime}( \langle\mathbf{u},\mathbf{x}\rangle)\big{]}(1-\big{\langle}\mathbf{w}^{t},\mathbf{u}\big{\rangle} ^{2})=(1-\big{\langle}\mathbf{w}^{t},\mathbf{u}\big{\rangle}^{2})\sum_{j\geq s}j\alpha _{j}\beta_{j}\big{\langle}\mathbf{w}^{t},\mathbf{u}\big{\rangle}^{j-1},\]

where the second identity follows from the Hermite expansion; see also [1, 19]. Assume \(\alpha_{s}\beta_{s}>0\) to ensure that the population dynamics will move towards \(\mathbf{u}\) at least near initialization. When replacing the population gradient with a full-batch gradient, we need the estimation noise to be smaller than the signal existing in the gradient. When \(\big{\langle}\mathbf{w}^{0},\mathbf{u}\big{\rangle}\ll 1\), this signal is roughly of the order \(\big{\langle}\mathbf{w}^{0},\mathbf{u}\big{\rangle}^{s-1}\). As the uniform concentration error over \(\mathbb{S}^{d-1}\) scales with \(\sqrt{d/n}\), we need \(n\asymp d\big{\langle}\mathbf{w}^{0},\mathbf{u}\big{\rangle}^{2(s-1)}\) to ensure the signal remains dominant and \(\mathbf{w}^{t}\) moves towards \(\mathbf{u}\). When \(\mathbf{w}^{0}\) is initialized uniformly over \(\mathbb{S}^{d-1}\) this translates to a sample complexity of \(n\asymp d^{s}\), which is indeed obtained by [1] via similar arguments.

[MISSING_PAGE_FAIL:10]

## Acknowledgments

The authors thank Alberto Bietti and Zhichao Wang for discussions and feedback on the manuscript. TS was partially supported by JSPS KAKENHI (20H00576) and JST CREST (JPMJCR2015). MAE was partially supported by NSERC Grant [2019-06167], CIFAR AI Chairs program, CIFAR AI Catalyst grant.

## References

* [AAM22] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz, _The merged-staircase property: a necessary and nearly sufficient condition for sgd learning of sparse functions on two-layer neural networks_, Conference on Learning Theory, 2022.
* [ABAM23] Emmanuel Abbe, Enric Boix-Adsera, and Theodor Misiakiewicz, _Sgd learning on neural networks: leap complexity and saddle-to-saddle dynamics_, arXiv preprint arXiv:2302.11055 (2023).
* [ASKL23] Luca Arnaboldi, Ludovic Stephan, Florent Krzakala, and Bruno Loureiro, _From high-dimensional & mean-field dynamics to dimensionless odes: A unifying approach to sgd in two-layers networks_, arXiv preprint arXiv:2302.05882 (2023).
* [ATV22] Pranjal Awasthi, Alex Tang, and Aravindan Vijayaraghavan, _Agnostic learning of general relu activation using gradient descent_, arXiv preprint arXiv:2208.02711 (2022).
* [BAGJ21] Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath, _Online stochastic gradient descent on non-convex losses from high-dimensional inference_, J. Mach. Learn. Res. **22** (2021), 106-1.
* [BBAP05] Jinho Baik, Gerard Ben Arous, and Sandrine Peche, _Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices_.
* [BBSS22] Alberto Bietti, Joan Bruna, Clayton Sanford, and Min Jae Song, _Learning single-index models with shallow neural networks_, Advances in Neural Information Processing Systems, 2022.
* [BEG\({}^{+}\)22] Boaz Barak, Benjamin L Edelman, Surbhi Goel, Sham Kakade, Eran Malach, and Cyril Zhang, _Hidden Progress in Deep Learning: SGD Learns Parities Near the Computational Limit_, arXiv preprint arXiv:2207.08799 (2022).
* [BES\({}^{+}\)19] Jimmy Ba, Murat Erdogdu, Taiji Suzuki, Denny Wu, and Tianzong Zhang, _Generalization of two-layer neural networks: An asymptotic viewpoint_, International Conference on Learning Representations, 2019.
* [BES\({}^{+}\)22] Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang, _High-dimensional Asymptotics of Feature Learning: How One Gradient Step Improves the Representation_, arXiv preprint arXiv:2205.01445 (2022).
* [BES\({}^{+}\)23] Jimmy Ba, Murat A. Erdogdu, Taiji Suzuki, Zhichao Wang, and Denny Wu, _Learning in the presence of low-dimensional structure: a spiked random matrix perspective_, Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023), 2023.
* [BF02] Nader H Bshouty and Vitaly Feldman, _On using extended statistical queries to avoid membership queries_, Journal of Machine Learning Research **2** (2002), no. Feb, 359-395.
* [BMZ23] Raphael Berthier, Andrea Montanari, and Kangjie Zhou, _Learning time-scales in two-layers neural networks_, arXiv preprint arXiv:2303.00055 (2023).
* [BPVZ23] Joan Bruna, Loucas Pillaud-Vivien, and Aaron Zweig, _On single index models beyond gaussian data_, arXiv preprint arXiv:2307.15804 (2023).
* [BS06] Jinho Baik and Jack W Silverstein, _Eigenvalues of large sample covariance matrices of spiked population models_, Journal of multivariate analysis **97** (2006), no. 6, 1382-1408.

* [CB18] Lenaic Chizat and Francis Bach, _On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport_, Advances in Neural Information Processing Systems, 2018.
* [Chi22] Lenaic Chizat, _Mean-field langevin dynamics: Exponential convergence and annealing_, arXiv preprint arXiv:2202.01009 (2022).
* [CM20] Sitan Chen and Raghu Meka, _Learning polynomials in few relevant dimensions_, Conference on Learning Theory, 2020.
* [COB19] Lenaic Chizat, Edouard Oyallon, and Francis Bach, _On Lazy Training in Differentiable Programming_, Advances in Neural Information Processing Systems, 2019.
* [CWPPS23] Elizabeth Collins-Woodfin, Courtney Paquette, Elliot Paquette, and Inbar Seroussi, _Hitting the high-dimensional notes: An ode for sgd learning dynamics on glms and multi-index models_, arXiv preprint arXiv:2308.08977 (2023).
* [DH18] Rishabh Dudeja and Daniel Hsu, _Learning single-index models in gaussian space_, Conference On Learning Theory, PMLR, 2018, pp. 1887-1930.
* [DKL\({}^{+}\)23] Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan, _Learning two-layer neural networks, one (giant) step at a time_, arXiv preprint arXiv:2305.18270 (2023).
* [DKTZ22] Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis, _Learning a single neuron with adversarial label noise via gradient descent_, Conference on Learning Theory, PMLR, 2022, pp. 4313-4361.
* [DLS22] Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi, _Neural Networks can Learn Representations with Gradient Descent_, Conference on Learning Theory, 2022.
* [DNGL23] Alex Damian, Eshaan Nichani, Rong Ge, and Jason D Lee, _Smoothing the landscape boosts the signal for sgd: Optimal sample complexity for learning single index models_, arXiv preprint arXiv:2305.10633 (2023).
* [DWY21] Konstantin Donhauser, Mingqi Wu, and Fanny Yang, _How rotational invariance of common kernels prevents generalization in high dimensions_, International Conference on Machine Learning, 2021.
* [EBD19] Murat A. Erdogdu, Mohsen Bayati, and Lee H. Dicker, _Scalable Approximations to Generalized Linear Problems_, Journal of Machine Learning Research (2019).
* [EDB16] Murat A Erdogdu, Lee H Dicker, and Mohsen Bayati, _Scaled least squares estimator for glms in large-scale problems_, Advances in Neural Information Processing Systems **29** (2016).
* [Erd15] Murat A Erdogdu, _Newton-stein method: a second order method for glms via stein's lemma_, Proceedings of Advances in Neural Information Processing Systems, 2015, pp. 1216-1224.
* [FCG20] Spencer Frei, Yuan Cao, and Quanquan Gu, _Agnostic learning of a single neuron with gradient descent_, Advances in Neural Information Processing Systems, vol. 33, Curran Associates, Inc., 2020, pp. 5417-5428.
* [GKK19] Surbhi Goel, Sushrut Karmalkar, and Adam Klivans, _Time/accuracy tradeoffs for learning a relu with respect to gaussian marginals_, Advances in neural information processing systems **32** (2019).
* [GMMM19] B. Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari, _Limitations of Lazy Training of Two-layers Neural Networks_, Advances in Neural Information Processing Systems, 2019.
* [GMMM20] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari, _When Do Neural Networks Outperform Kernel Methods?_, Advances in Neural Information Processing Systems, 2020.

* [HC22] Karl Hajjar and Lenaic Chizat, _On the symmetries in the dynamics of wide two-layer neural networks_, arXiv preprint arXiv:2211.08771 (2022).
* [HITF09] Trevor Hastie, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman, _The elements of statistical learning: data mining, inference, and prediction_, vol. 2, Springer, 2009.
* [IS15] Sergey Ioffe and Christian Szegedy, _Batch normalization: Accelerating deep network training by reducing internal covariate shift_, International conference on machine learning, pmlr, 2015, pp. 448-456.
* [JGH18] Arthur Jacot, Franck Gabriel, and Clement Hongler, _Neural Tangent Kernel: Convergence and Generalization in Neural Networks_, Advances in Neural Information Processing Systems, 2018.
* [Joh01] Iain M Johnstone, _On the distribution of the largest eigenvalue in principal components analysis_, The Annals of statistics **29** (2001), no. 2, 295-327.
* [JWHT13] Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani, _An introduction to statistical learning_, vol. 112, Springer, 2013.
* [KKSK11] Sham M Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai, _Efficient learning of generalized linear and single index models with isotonic regression_, Advances in Neural Information Processing Systems **24** (2011).
* [LD89] Ker-Chau Li and Naihua Duan, _Regression Analysis Under Link Violation_, The Annals of Statistics (1989).
* [LMZ20] Yuanzhi Li, Tengyu Ma, and Hongyang R Zhang, _Learning over-parametrized two-layer neural networks beyond NTK_, Conference on Learning Theory, 2020.
* [MHD\({}^{+}\)23] Arvind Mahankali, Jeff Z Haochen, Kefan Dong, Margalit Glasgow, and Tengyu Ma, _Beyond ntk with vanilla gradient descent: A mean-field analysis of neural networks with polynomial width, samples, and time_, arXiv preprint arXiv:2306.16361 (2023).
* [MHPG\({}^{+}\)23] Alireza Mousavi-Hosseini, Sejun Park, Manuela Girotti, Ioannis Mitliagkas, and Murat A Erdogdu, _Neural networks efficiently learn low-dimensional representations with SGD_, The Eleventh International Conference on Learning Representations, 2023.
* [MMM19] Song Mei, Theodor Misiakiewicz, and Andrea Montanari, _Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit_, Conference on Learning Theory, 2019.
* [MMN18] Song Mei, Andrea Montanari, and Phan-Minh Nguyen, _A mean field view of the landscape of two-layer neural networks_, Proceedings of the National Academy of Sciences **115** (2018), no. 33, E7665-E7671.
* [NWS22] Atsushi Nitanda, Denny Wu, and Taiji Suzuki, _Convex analysis of the mean field langevin dynamics_, International Conference on Artificial Intelligence and Statistics, PMLR, 2022, pp. 9741-9757.
* [O'D14] Ryan O'Donnell, _Analysis of boolean functions_, Cambridge University Press, 2014.
* [PSE22] Sejun Park, Umut Simsekli, and Murat A. Erdogdu, _Generalization Bounds for Stochastic Gradient Descent via Localized \(\varepsilon\)-Covers_, arXiv preprint arXiv:2209.08951 (2022).
* [Rey20] Lev Reyzin, _Statistical queries and statistical algorithms: Foundations and applications_, arXiv preprint arXiv:2004.00557 (2020).
* [RGKZ21] Maria Refinetti, Sebastian Goldt, Florent Krzakala, and Lenka Zdeborova, _Classifying high-dimensional Gaussian mixtures: Where kernel methods fail and neural networks succeed_, International Conference on Machine Learning, 2021.

* [RVE18] Grant M Rotskoff and Eric Vanden-Eijnden, _Neural networks as Interacting Particle Systems: Asymptotic convexity of the Loss Landscape and Universal Scaling of the Approximation Error_, arXiv preprint arXiv:1805.00915 (2018).
* [Sol17] Mahdi Soltanolkotabi, _Learning relus via gradient descent_, Advances in neural information processing systems **30** (2017).
* [SWON23] Taiji Suzuki, Denny Wu, Kazusato Oko, and Atsushi Nitanda, _Feature learning via mean-field langevin dynamics: classifying sparse parities and beyond_, Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS 2023), 2023.
* [Tel23] Matus Telgarsky, _Feature selection and low test error in shallow low-rotation relu networks_, The Eleventh International Conference on Learning Representations, 2023.
* [Ver18] Roman Vershynin, _High-dimensional probability: An introduction with applications in data science_, Cambridge University Press, 2018.
* [VH16] Ramon Van Handel, _Probability in high dimension_, 2016.
* [Wai19] Martin J. Wainwright, _High-dimensional statistics: A non-asymptotic viewpoint_, Cambridge University Press, 2019.
* [WLLM19] Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma, _Regularization matters: Generalization and optimization of neural nets vs their induced kernel_, Advances in Neural Information Processing Systems **32** (2019).
* [YO20] Gilad Yehudai and Shamir Ohad, _Learning a single neuron with gradient methods_, Proceedings of Thirty Third Conference on Learning Theory, Proceedings of Machine Learning Research, vol. 125, PMLR, 2020, pp. 3756-3786.
* [YS19] Gilad Yehudai and Ohad Shamir, _On the Power and Limitations of Random Features for Understanding Neural Networks_, Advances in Neural Information Processing Systems, 2019.

Concurrent Works

In this paragraph we briefly summarize a few relevant results concurrent to our submission. [1] provided a precise analysis of the two-timescale dynamics in learning a single index model with information exponent \(s=1\). [13] considered the learning of a single index target with \(s=2\) using a neural network in the mean-field regime. [1] extended the information exponent-based characterization of online SGD to input data beyond Gaussian. [2] showed that a gradient-smoothed dynamics can improve the sample complexity and match the CSQ lower bound. Finally, beyond the single-index setting, [3, 14] considered learning low-dimensional target functions supported on \(k>1\) dimensions via gradient-based feature learning.

## Appendix B Background on Hermite Expansion

The normalized Hermite polynomials \(\{h_{j}\}_{j\geq 0}\) given by (2.4) provide an orthonormal basis for \(L^{2}(\gamma)\), thus for every \(f\in L^{2}(\gamma)\) we have

\[f=\sum_{j=0}^{\infty}\langle f,h_{j}\rangle_{\gamma}h_{j},\]

where \(\langle f,h_{j}\rangle_{\gamma}\coloneqq\mathbb{E}_{z\sim\mathcal{N}(0,1)}[f(z )h_{j}(z)]\). We will commonly invoke the following well-known properties of Hermite polynomials. If \(j\geq 1\), then \(h^{\prime}_{j}=\sqrt{j}h_{j-1}\), where \(h^{\prime}_{j}\) stands for the derivative of \(h_{j}\). Furthermore, if \(z_{1}\) and \(z_{2}\) are two standard Gaussian random variables with \(\mathbb{E}[z_{1}z_{2}]=\rho\), then \(\mathbb{E}[h_{i}(z_{1})h_{j}(z_{2})]=\delta_{ij}\rho^{j}\) where \(\delta_{ij}\) is the Kronecker delta. We refer the interested reader to [12, Chapter 11.2] for additional discussions and properties of these polynomials.

We will now discuss how our Definition 1 relates to the original definition of information exponent of [1]. In their setting, they assume the true data distribution \(\mathbb{P}_{\mathbf{u}}\) is parameterized by some unit vector \(\mathbf{u}\in\mathbb{S}^{d-1}\), and we know the parametric family \(\{\mathbb{P}_{\mathbf{w}}\}_{\mathbf{w}\in\mathbb{S}^{d-1}}\); thus the problem is to estimate the direction \(\mathbf{u}\). Furthermore, they assume the population loss, which is the expectation of some per-sample loss, has spherical symmetry, i.e. the population loss \(R(\mathbf{w})\) can be written as \(R(\mathbf{w})=\tilde{R}(\langle\mathbf{w},\mathbf{u}\rangle)\). Then, [1, Definition 1.2] defines the information exponent to be the degree of the first non-zero coefficient of \(\tilde{R}\) in its Taylor expansion around the origin. In other words, we say \(R\) has information exponent \(s\) if

\[\begin{cases}\frac{\mathrm{d}^{k}\,\tilde{R}}{\mathrm{d}^{2}\tilde{R}}(0)=0&1 \leq k<s\\ \frac{\mathrm{d}^{2}\tilde{R}}{\mathrm{d}^{2}\tilde{R}}(0)=-c<0&k=s\\ \left|\frac{\mathrm{d}^{2}\tilde{R}}{\mathrm{d}^{2}\tilde{R}}(z)\right|\leq C&k >s,\forall z\in[-1,1]\end{cases},\]

where \(C,c>0\) are universal constants. To specialize the above abstract definition to the Gaussian case, consider the setting where the input data is standard Gaussian \(\mathbf{x}\sim\mathcal{N}(0,\mathbf{I}_{d})\) and the problem is to estimate \(\mathbf{u}\in\mathbb{S}^{d-1}\) given a response variable \(y=f(\langle\mathbf{u},\mathbf{x}\rangle)\) with known \(f\). Via the Hermite expansion of \(f\), one can write

\[\tilde{R}(\langle\mathbf{w},\mathbf{u}\rangle)\coloneqq\frac{1}{2}\,\mathbb{E}\big{[} (f(\langle\mathbf{w},\mathbf{x}\rangle)-f(\langle\mathbf{u},\mathbf{x}\rangle))^{2}\big{]}=- \sum_{j\geq 1}\langle f,h_{j}\rangle_{\gamma}^{2}\langle\mathbf{w},\mathbf{u}\rangle^{j} +\text{const.}\]

Thus, the information exponent of \(\tilde{R}\) is indeed the degree of the first non-zero term in the Hermite expansion of \(f\).

Now consider the general case where \(\mathbf{x}\sim\mathcal{N}(0,\mathbf{\Sigma})\). The spherical symmetry assumed in [1] no longer holds. However, after proper normalization of weights, if we consider the population loss

\[R(\mathbf{w})\coloneqq\frac{1}{2}\,\mathbb{E}\Bigg{[}\Bigg{(}f\Bigg{(}\frac{ \langle\mathbf{w},\mathbf{x}\rangle}{\|\mathbf{\Sigma}^{1/2}\mathbf{w}\|}\Bigg{)}-f\Bigg{(} \frac{\langle\mathbf{u},\mathbf{x}\rangle}{\|\mathbf{\Sigma}^{1/2}\mathbf{u}\|}\Bigg{)}\Bigg{)} ^{2}\Bigg{]},\]

then \(R(\mathbf{w})=\tilde{R}\Big{(}\frac{\langle\mathbf{w},\mathbf{\Sigma}\mathbf{u}\rangle}{\|\mathbf{ \Sigma}^{1/2}\mathbf{w}\|\|\mathbf{\Sigma}^{1/2}\mathbf{u}\|}\Big{)}\). Indeed, a close examination of the arguments of [1] reveals that for their results to hold, the proper symmetry to consider is the ellipsoidal symmetry, and 

[MISSING_PAGE_FAIL:16]

[MISSING_PAGE_EMPTY:17]

[MISSING_PAGE_EMPTY:18]

[MISSING_PAGE_FAIL:19]

Consequently, the lower bound of the time derivative of \(\left\langle\mathbf{w}^{t},\mathbf{u}\right\rangle\) becomes larger as \(\left\langle\mathbf{w}^{t},\mathbf{u}\right\rangle\) increases. Therefore, assuming \(\left\langle\mathbf{w}^{0},\mathbf{u}\right\rangle>0\), we only need to control this lower bound at initialization. Assume

\[\kappa\geq\frac{4}{\left\langle\mathbf{w}^{0},\mathbf{u}\right\rangle^{2}(\alpha-1/2)} \Bigg{\{}\alpha\vee\frac{(\|g^{\prime}\|_{\gamma}+\sqrt{8}\|g\|_{\gamma})^{2}}{ \alpha-1/2}\Bigg{\}}.\]

From this, we conclude that when \(\left\langle\mathbf{w}^{t},\mathbf{u}\right\rangle>0\) and \(\left\langle\phi,g\right\rangle_{\gamma}=\alpha>1/2\), we have

\[\frac{\mathrm{d}\langle\mathbf{w}^{t},\mathbf{u}\rangle}{\mathrm{d}t}\geq\frac{\alpha- 1/2}{2}(1-\left\langle\mathbf{w}^{t},\mathbf{u}\right\rangle^{2}),\]

integration yields the desired result. 

## Appendix D Proofs of Section 3

We begin by stating the closed-form expression for the population gradient, i.e. the counterpart of Lemma 12 in the normalized setting.

**Lemma 15**.: _Consider the population risk \(\mathcal{R}(\mathbf{w})\) defined by (3.1), recall that_

\[\mathcal{R}(\mathbf{w})=\mathbb{E}\Bigg{[}-\phi\Bigg{(}\frac{\left\langle\mathbf{w}, \mathbf{x}\right\rangle}{\|\mathbf{\Sigma}^{1/2}\mathbf{w}\|}\Bigg{)}g(\left\langle\overline {\mathbf{u}},\mathbf{z}\right\rangle)\Bigg{]}+\frac{1}{2}\|\phi\|_{\gamma}^{2}+\frac{1 }{2}\,\mathbb{E}\big{[}y^{2}\big{]}.\]

_Then,_

\[\nabla\mathcal{R}(\mathbf{w})=\frac{\mathbf{\Sigma}^{1/2}(\mathbf{I}_{d}-\overline{\bm {w}}\,\overline{\mathbf{w}}^{\top})\zeta_{\phi,g}(\left\langle\overline{\mathbf{w}}, \overline{\mathbf{u}}\right\rangle)\overline{\mathbf{u}}}{\|\mathbf{\Sigma}^{1/2}\mathbf{w}\|},\] (D.1)

_where_

\[\zeta_{\phi,g}(\left\langle\overline{\mathbf{w}},\overline{\mathbf{u}}\right\rangle) \coloneqq-\mathbb{E}[\phi^{\prime}(\left\langle\overline{\mathbf{w}},\mathbf{z}\right ))g^{\prime}(\left\langle\overline{\mathbf{u}},\mathbf{z}\right\rangle)]=-\sum_{j\geq s }j\alpha_{j}\beta_{j}\langle\overline{\mathbf{w}},\overline{\mathbf{u}}\rangle^{j-1}.\] (D.2)

**Proof.** Recall from (3.1) that

\[\nabla\mathcal{R}(\mathbf{w}) =\nabla_{\mathbf{w}}\,\mathbb{E}\Bigg{[}-\phi\Bigg{(}\frac{\left\langle \mathbf{w},\mathbf{x}\right\rangle}{\|\mathbf{\Sigma}^{1/2}\mathbf{w}\|}\Bigg{)}g(\left\langle \overline{\mathbf{u}},\mathbf{z}\right\rangle)\Bigg{]}\] \[=\frac{\mathbf{\Sigma}^{1/2}(\mathbf{I}_{d}-\overline{\mathbf{w}}\, \overline{\mathbf{w}}^{\top})}{\|\mathbf{\Sigma}^{1/2}\mathbf{w}\|}\{\zeta_{\phi,g}( \left\langle\overline{\mathbf{w}},\overline{\mathbf{u}}\right\rangle)\overline{\mathbf{u} }+\psi_{\phi,g}(\left\langle\overline{\mathbf{w}},\overline{\mathbf{u}}\right\rangle) \overline{\mathbf{w}}\}\qquad\text{(by Lemma~{}\ref{lem:def_def_def_def_def_def_ def_

### Proof of Proposition 4

From Lemma 3, we have

\[\frac{\mathrm{d}\langle\overline{\mathbf{w}}^{t},\overline{\mathbf{u}}\rangle}{ \mathrm{d}t} =-\zeta_{\phi,g}(\langle\langle\overline{\mathbf{w}}^{t},\overline{\mathbf{u}} \rangle\rangle\|\mathbf{\Sigma}^{1/2}\overline{\mathbf{u}}_{\perp}^{t}\|^{2}\] \[\geq c\lambda_{\min}(\mathbf{\Sigma})\langle\overline{\mathbf{w}}^{t}, \overline{\mathbf{u}}\rangle^{s-1}(1-\langle\overline{\mathbf{w}}^{t},\overline{\mathbf{u} }\rangle^{2}),\]

where \(\overline{\mathbf{u}}_{\perp}^{t}\coloneqq\overline{\mathbf{u}}-\langle\overline{\mathbf{w} }^{t},\overline{\mathbf{u}}\rangle\overline{\mathbf{w}}^{t}\). The above inequality and the fact that \(\langle\overline{\mathbf{w}}^{0},\overline{\mathbf{u}}\rangle>0\) imply that \(\langle\overline{\mathbf{w}}^{t},\overline{\mathbf{u}}\rangle\) is non-decreasing in time. Let

\[T_{1}\coloneqq\sup\{t>0:\langle\overline{\mathbf{w}}^{t},\overline{\mathbf{u}}\rangle< 1/2\}.\]

Then, on \(t\in[0,T_{1}]\), we have

\[\frac{\mathrm{d}\langle\overline{\mathbf{w}}^{t},\overline{\mathbf{u}}\rangle}{ \mathrm{d}t}\geq\frac{3c\lambda_{\min}(\mathbf{\Sigma})}{4}\langle\overline{\mathbf{w} }^{t},\overline{\mathbf{u}}\rangle^{s-1},\]

and integration yields

\[T_{1}\leq 0\vee\frac{4}{3c\lambda_{\min}(\mathbf{\Sigma})}\begin{cases}1/2-\langle \overline{\mathbf{w}}^{0},\overline{\mathbf{u}}\rangle&s=1\\ \ln(1/(2\langle\overline{\mathbf{w}}^{0},\overline{\mathbf{u}}\rangle))&s=2\,.\\ \frac{1}{s-2}\big{(}(1/\langle\overline{\mathbf{w}}^{0},\overline{\mathbf{u}}\rangle)^{ s-2}-2^{s-2}\big{)}&s>2\end{cases}\]

Therefore, \(T_{1}\lesssim\tau_{s}(\langle\langle\overline{\mathbf{w}}^{0},\overline{\mathbf{u}} \rangle)/\lambda_{\min}(\mathbf{\Sigma})\).

For \(t>T_{1}\), we have

\[\frac{\mathrm{d}\langle\overline{\mathbf{w}}^{t},\overline{\mathbf{u}}\rangle}{ \mathrm{d}t}\geq\frac{c\lambda_{\min}(\mathbf{\Sigma})}{2^{s-1}}(1-\langle \overline{\mathbf{w}}^{t},\overline{\mathbf{u}}\rangle^{2}).\]

Let

\[T_{2}=\sup\bigl{\{}t>0:\langle\overline{\mathbf{w}}^{t},\overline{\mathbf{u}}\rangle< 1-\varepsilon\bigr{\}}.\]

Once again, integration implies

\[T_{2}\leq T_{1}+\frac{2^{s-2}}{c\lambda_{\min}(\mathbf{\Sigma})}\ln(2/(3 \varepsilon)),\]

which completes the proof. 

### Preliminary Lemmas for proving Theorem 5

We first introduce a number of concentration (and anti-concentration) lemmas that will be useful for proving Theorem 5.

**Lemma 16**.: _Suppose \(\{\mathbf{z}^{(i)}\}_{i=1}^{n}\stackrel{{ i.i.d.}}{{\sim}}\mathcal{N }(0,\mathbf{I}_{d})\), and \(g:\mathbb{R}\to\mathbb{R}\) satisfies \(|g(\cdot)|\leq C(1+|\cdot|^{p})\) for some \(C>0\) and \(p\geq 1\). Additionally, suppose \(\{\epsilon^{(i)}\}_{i=1}\) are i.i.d. \(\sigma\)-sub-Gaussian zero-mean noise independent of \(\{\mathbf{z}^{(i)}\}_{i=1}^{n}\). Let \(y^{(i)}\coloneqq g(\langle\overline{\mathbf{u}},\mathbf{z}^{(i)}\rangle)+\epsilon^{(i)}\) for some \(\overline{\mathbf{u}}\in\mathbb{S}^{d-1}\). Then, for any \(q>0\), with probability at least \(1-4d^{-q}\), we have_

\[\left|y^{(i)}\right|\leq C+C(2\ln(nd^{q}))^{p/2}+\sigma\sqrt{2\ln(nd^{q})} \lesssim\ln(nd^{q})^{p/2},\]

_for all \(i\)._

**Proof.** Notice that \(\langle\overline{\mathbf{u}},\mathbf{z}^{(i)}\rangle\sim\mathcal{N}(0,1)\), thus

\[\mathbb{P}\Big{(}\Big{|}\Big{\langle}\overline{\mathbf{u}},\mathbf{z}^{(i)}\Big{\rangle} \Big{|}\geq\sqrt{2\ln(nd^{q})}\Big{)}\leq 2n^{-1}d^{-q}.\]

Similarly, by the sub-Gaussian and zero-mean property of \(\epsilon^{(i)}\),

\[\mathbb{P}\Big{(}\Big{|}\epsilon^{(i)}\Big{|}\geq\sigma\sqrt{2\ln(nd^{q})} \Big{)}\leq 2n^{-1}d^{-q}.\]

Thus, by a union bound, we have

\[\left|\Big{\langle}\overline{\mathbf{u}},\mathbf{z}^{(i)}\Big{\rangle}\right|\leq\sqrt{ 2\ln(nd^{q})}\quad\text{and}\quad\left|\epsilon^{(i)}\right|\leq\sigma\sqrt{2 \ln(nd^{q})},\quad\text{for all }1\leq i\leq n,\]

with probability at least \(1-4d^{-q}\). Using the upper bound on \(|g|\) finishes the proof.

**Lemma 17**.: _Suppose \(\{\overline{\mathbf{\overline{x}}}^{(i)}\}_{i=1}^{n}\) are i.i.d. samples drawn uniformly from \(\mathbb{S}^{d-1}\). Then,_

\[\mathbb{P}\Biggl{(}\sup_{\overline{\mathbf{w}}\in\mathbb{S}^{d-1}}\sum_{i=1}^{n} \mathbf{1}\Bigl{(}\Bigl{|}\Bigl{\langle}\overline{\mathbf{w}},\overline{\mathbf{ \overline{z}}}^{(i)}\Bigr{\rangle}\Bigr{|}\leq\frac{3\sqrt{d}}{8n}\Biggr{)}\geq 3 d\Bigl{(}2+\ln(8n/\sqrt{d})\Bigr{)}\Biggr{)}\leq e^{-d}.\]

**Proof.** Fix some \(\epsilon\in(0,1)\). Let \(N_{\epsilon}\) be a minimal \(\epsilon\)-covering of \(\mathbb{S}^{d-1}\). Let \(\hat{\mathbf{w}}\) be the projection of \(\overline{\mathbf{w}}\) onto \(N_{\epsilon}\). Notice that by the triangle inequality and the union bound

\[\mathbb{P}\Biggl{(}\sup_{\overline{\mathbf{w}}\in\mathbb{S}^{d-1}}\sum _{i=1}^{n}\mathbf{1}\Bigl{(}\Bigl{|}\Bigl{\langle}\overline{\mathbf{w}},\overline {\mathbf{\overline{z}}}^{(i)}\Bigr{\rangle}\Bigr{|}\leq\epsilon\Bigr{)}\geq\alpha \Biggr{)} \leq\mathbb{P}\Biggl{(}\sup_{\hat{\mathbf{w}}\in N_{\epsilon}}\sum_{i=1 }^{n}\mathbf{1}\Bigl{(}\Bigl{|}\Bigl{\langle}\hat{\mathbf{w}},\overline{\mathbf{ \overline{z}}}^{(i)}\Bigr{\rangle}\Bigr{|}\leq 2\epsilon\Bigr{)}\geq\alpha \Biggr{)}\] \[\leq(3/\epsilon)^{d}\mathbb{P}\Biggl{(}\sum_{i=1}^{n}\mathbf{1} \Bigl{(}\Bigl{|}\Bigl{\langle}\hat{\mathbf{w}},\overline{\mathbf{\overline{z}}}^{(i)} \Bigr{\rangle}\Bigr{|}\leq 2\epsilon\Bigr{)}\geq\alpha\Biggr{)}.\]

Moreover, due to [BBSS22, Lemma A.7],

\[\mathbb{E}[\mathbf{1}(|\langle\hat{\mathbf{w}},\overline{\mathbf{z}}\rangle|\leq 2 \epsilon)]=\mathbb{P}(\langle\hat{\mathbf{w}},\overline{\mathbf{z}}\rangle\leq 2 \epsilon)\leq 8\sqrt{d}\epsilon.\]

Choose \(\epsilon=\frac{3\sqrt{d}}{8n}\). By Lemma 25

\[\mathbb{P}\Biggl{(}\sum_{i=1}^{n}

Let \(\tilde{\mathbf{z}}^{(i)}\coloneqq\hat{\mathbf{\Sigma}}^{-1/2}\mathbf{x}^{(i)}=\hat{\mathbf{\Sigma}} ^{-1/2}\mathbf{\Sigma}^{1/2}\mathbf{z}^{(i)}\). Then,

\[\frac{\mathrm{d}\mathbf{\overline{w}}^{t}}{\mathrm{d}t}=(\mathbf{I}_{d}-\mathbf{ \overline{w}}^{t}\mathbf{\overline{w}}^{t\top})\hat{\mathbf{\Sigma}}(\mathbf{I}_{d}- \mathbf{\overline{w}}^{t}\mathbf{\overline{w}}^{t\top})\Bigg{\{}\frac{1}{n}\sum_{i=1}^ {n}\phi^{\prime}\bigg{(}\frac{\left\langle\mathbf{w}^{t},\mathbf{x}^{(i)}\right\rangle} {\left\|\hat{\mathbf{\Sigma}}^{1/2}\mathbf{w}^{t}\right\|}y^{(i)}\tilde{\mathbf{z}}^{(i)} \Bigg{\}}\]

To simplify the notation, define

\[\mathbf{\nu}(\mathbf{\overline{w}})\coloneqq(\mathbf{I}_{d}-\mathbf{\overline{w}}\,\mathbf{ \overline{w}}^{\top})\hat{\mathbf{\Sigma}}(\mathbf{I}_{d}-\mathbf{\overline{w}}\,\mathbf{ \overline{w}}^{\top})\overline{\mathbf{u}}.\]

Then

\[\frac{\mathrm{d}\langle\mathbf{\overline{w}}^{t},\mathbf{\overline{u}}\rangle}{\mathrm{ d}t}=\Bigg{\langle}\mathbf{\nu}(\mathbf{\overline{w}}^{t}),\frac{1}{n}\sum_{i=1}^{n}\phi^{ \prime}\Big{(}\Big{\langle}\mathbf{\overline{w}}^{t},\tilde{\mathbf{z}}^{(i)}\Big{)} \Big{\rangle}y^{(i)}\tilde{\mathbf{z}}^{(i)}\Bigg{\rangle}.\]

We can decompose the above dynamics into a population term and three different error terms in the following manner:

\[\frac{\mathrm{d}\langle\mathbf{\overline{w}}^{t},\mathbf{\overline{u}}\rangle}{ \mathrm{d}t}=\] \[+\underbrace{\Bigg{\langle}\mathbf{\nu}(\mathbf{\overline{w}}^{t}),\frac{ 1}{n}\sum_{i=1}^{n}\phi^{\prime}\Big{(}\Big{\langle}\mathbf{\overline{w}}^{t},\mathbf{ z}^{(i)}\Big{)}\Big{\rangle}y^{(i)}\mathbf{z}^{(i)}-\mathbb{E}_{\mathbf{z},y}\big{[}\phi^{ \prime}\big{(}\langle\mathbf{\overline{w}}^{t},\mathbf{z}\rangle\big{)}y\mathbf{z}\Big{]} \Bigg{\rangle}}_{=\mathcal{E}_{1}}\] \[+\underbrace{\frac{1}{n}\sum_{i=1}^{n}\Bigl{\{}\phi^{\prime}\Big{(} \Big{\langle}\mathbf{\overline{w}},\tilde{\mathbf{z}}^{(i)}\Big{)}\Big{\rangle}y^{(i)} \Big{\langle}\tilde{\mathbf{z}}^{(i)}-\mathbf{z}^{(i)},\mathbf{\nu}(\mathbf{\overline{w}}^{t}) \Big{\rangle}\Bigr{\}}}_{=\mathcal{E}_{3}}.\]

We will proceed in three steps. In the first, we bound \(\mathcal{E}_{1}\), the concentration error. In the second, we bound \(\mathcal{E}_{2}\) and \(\mathcal{E}_{3}\), the errors due to estimating \(\mathbf{\Sigma}\) with \(\hat{\mathbf{\Sigma}}\) (i.e. replacing \(\mathbf{z}^{(i)}\) with \(\tilde{\mathbf{z}}^{(i)}\)). Finally, we will analyze the convergence time similar to that of Proposition 4. Throughout the proof, we will assume that the event \(\mathcal{G}\) of Lemma 18 occurs.

**Step 1. Controlling the concentration error \(\mathcal{E}_{1}\).** Let \(K\times\ln(nd^{q})^{p/2}\), and notice that on event \(\mathcal{G}\) we have \(\big{|}y^{(i)}\big{|}\lesssim K\) for all \(i\). Let \(y_{K}\coloneqq y\mathbf{1}(|y|\leq K)\). On the event \(\mathcal{G}\), we have \(y_{K}^{(i)}=y^{(i)}\) for all \(i\), and

\[\mathcal{E}_{1}=\left\langle\mathbf{\nu}(\mathbf{\overline{w}}^{t}),\mathbf{\Delta}_{n} \right\rangle\geq-\|\mathbf{\Delta}_{n}\|\|\mathbf{\nu}(\mathbf{\overline{w}}^{t})\|,\]

where

\[\mathbf{\Delta}_{n}\coloneqq\frac{1}{n}\sum_{i=1}^{n}\phi^{\prime}\Big{(}\Big{\langle} \mathbf{\overline{w}},\mathbf{z}^{(i)}\Big{\rangle}\Big{)}y_{K}^{(i)}\mathbf{z}^{(i)}- \mathbb{E}_{\mathbf{z},y}[\phi^{\prime}(\langle\mathbf{\overline{w}},\mathbf{z}\rangle)y \mathbf{z}].\]

Thus, our objective is to bound \(\|\mathbf{\Delta}_{n}\|\) uniformly for all \(\mathbf{\overline{w}}\in\mathbb{S}^{d-1}\). To that end, we first modify the expectation in the above definition so that the empirical average and expected value match in terms of their random variables. Specifically,

\[\sup_{\mathbf{\overline{w}},\mathbf{v}\in\mathbb{S}^{d-1}}\langle\mathbf{ \Delta}_{n},\mathbf{v}\rangle=\sup_{\mathbf{\overline{w}},\mathbf{v}\in\mathbb{S}^{d-1}} \frac{1}{n}\sum_{i=1}^{n}\phi^{\prime}\Big{(}\Big{\langle}\mathbf{ \overline{w}},\mathbf{z}^{(i)}\Big{\rangle}\Big{)}y_{K}^{(i)}\Big{\langle}\mathbf{z}^ {(i)},\mathbf{v}\Big{\rangle}-\mathbb{E}_{\mathbf{z},y}[\phi^{\prime}(\langle\mathbf{ \overline{w}},\mathbf{z}\rangle)y_{K}\langle\mathbf{z},\mathbf{v}\rangle]\] \[-\mathbb{E}_{\mathbf{z},y}[\phi^{\prime}(\langle\mathbf{\overline{w}},\mathbf{ z}\rangle)y(\mathbf{z},\mathbf{v})\mathbf{1}(|y|>K)].\]

By the Cauchy-Schwartz inequality,

\[\begin{split}\left\|\mathbb{E}_{\mathbf{z},y}\big{[}\phi^{\prime}( \langle\mathbf{\overline{w}},\mathbf{z}\rangle)y\langle\mathbf{z},\mathbf{v}\rangle\mathbf{1}(|y |>K)\big{]}\right\|&\leq\mathbb{E}_{\mathbf{z},y}\Big{[}\phi^{\prime}( \langle\mathbf{\overline{w}},\mathbf{z}\rangle)^{2}y^{2}\langle\mathbf{z},\mathbf{v}\rangle^{2} \Big{]}^{1/2}\,\mathbb{E}[\mathbf{1}(|y|>K)]^{1/2}\\ &\lesssim\mathbb{E}\big{[}y^{4}\big{]}^{1/4}\,\mathbb{E}_{\mathbf{z}} \Big{[}\langle\mathbf{z},\mathbf{v}\rangle^{4}\Big{]}^{1/4}\mathbb{P}(|y|>K)^{1/2}\\ &\lesssim d^{-q/2},\end{split}\]where the last inequality follows from Lemma 16. Hence,

\[\sup_{\overline{\mathbf{w}},\mathbf{v}\in\mathbb{S}^{d-1}}\langle\mathbf{\Delta} _{n},\mathbf{v}\rangle\leq \sup_{\overline{\mathbf{w}},\mathbf{v}\in\mathbb{S}^{d-1}}\frac{1}{n}\sum_{i=1 }^{n}\phi^{\prime}\Big{(}\Big{\langle}\overline{\mathbf{w}},\mathbf{z}^{(i)}\Big{\rangle} \Big{)}y_{K}^{(i)}\Big{\langle}\mathbf{z}^{(i)},\mathbf{v}\Big{\rangle}-\mathbb{E}[\phi^ {\prime}(\langle\overline{\mathbf{w}},\mathbf{z}\rangle)y_{K}\langle\mathbf{z},\mathbf{v}\rangle]\] \[+\mathcal{O}(d^{-q/2}).\]

Next, we need to establish high-probability bounds via a covering argument. To simplify the exposition, define the stochastic process indexed by \(\overline{\mathbf{w}}\in\mathbb{S}^{d-1}\) and \(\mathbf{v}\in\mathbb{S}^{d-1}\) via

\[X_{\overline{\mathbf{w}},\mathbf{v}}^{(i)}\coloneqq\phi^{\prime}\Big{(}\Big{\langle} \overline{\mathbf{w}},\mathbf{z}^{(i)}\Big{\rangle}\Big{)}y_{K}^{(i)}\Big{\langle}\mathbf{ z}^{(i)},\mathbf{v}\Big{\rangle}.\]

Fix some \(\epsilon_{w},\epsilon_{v}>0\). Let \(\Theta_{w}\) and \(\Theta_{v}\) be \(\epsilon_{w}\) and \(\epsilon_{v}\) coverings of \(\mathbb{S}^{d-1}\), and let \(\hat{\mathbf{w}}\) and \(\hat{\mathbf{v}}\) denote the projection of \(\overline{\mathbf{w}}\) onto \(\Theta_{w}\) and of \(\mathbf{v}\) onto \(\Theta_{v}\) respectively, then

\[\sup_{\overline{\mathbf{w}},\mathbf{v}\in\mathbb{S}^{d-1}}\frac{1}{n}\sum _{i=1}^{n}X_{\overline{\mathbf{w}},\mathbf{v}}^{(i)}-\mathbb{E}[X_{\overline{\mathbf{w}}, \mathbf{v}}]= \sup_{\overline{\mathbf{w}},\mathbf{v}\in\mathbb{S}^{d-1}}\frac{1}{n}\sum_{i=1 }^{n}\Bigl{(}X_{\overline{\mathbf{w}},\mathbf{v}}^{(i)}-X_{\overline{\mathbf{w}},\hat{\mathbf{ v}}}^{(i)}\Bigr{)}+\frac{1}{n}\sum_{i=1}^{n}\Bigl{(}X_{\overline{\mathbf{w}},\hat{\mathbf{ v}}}^{(i)}-X_{\hat{\mathbf{w}},\hat{\mathbf{v}}}^{(i)}\Bigr{)}\] \[+\mathbb{E}_{\mathbf{z},y}[X_{\overline{\mathbf{w}},\hat{\mathbf{v}}}-X_{ \overline{\mathbf{w}},\mathbf{v}}]+\mathbb{E}_{\mathbf{z},y}[X_{\hat{\mathbf{w}},\hat{\mathbf{v}}} -X_{\overline{\mathbf{w}},\hat{\mathbf{v}}}]\] \[+\frac{1}{n}\sum_{i=1}^{n}X_{\hat{\mathbf{w}},\hat{\mathbf{v}}}^{(i)}- \mathbb{E}_{\mathbf{z},y}[X_{\hat{\mathbf{w}},\hat{\mathbf{v}}}].\]

We bound each of the terms using Cauchy-Schwartz. Specifically,

\[\frac{1}{n}\sum_{i=1}^{n}\Bigl{(}X_{\overline{\mathbf{w}},\mathbf{v}}^{(i)}-X_{ \overline{\mathbf{w}},\hat{\mathbf{v}}}^{(i)}\Bigr{)}\leq\sqrt{\frac{1}{n}\sum_{i=1}^ {n}\phi^{\prime}(\langle\overline{\mathbf{w}},\mathbf{z}^{(i)}\rangle)^{2}y_{K}^{(i)} }\sqrt{\frac{1}{n}\sum_{i=1}^{n}\bigl{\langle}\mathbf{z}^{(i)},\mathbf{v}-\hat{\mathbf{v}} \bigr{\rangle}^{2}}\lesssim K\epsilon_{v},\]

where we used the upper bound on the operator norm of \(\frac{1}{n}\sum_{i=1}^{n}\mathbf{z}^{(i)}\mathbf{z}^{(i)}^{\top}\) from Lemma 18 together with the fact that \(n\gtrsim d\). Similarly,

\[\mathbb{E}_{\mathbf{z},y}[X_{\overline{\mathbf{w}},\hat{\mathbf{v}}}-X_{\overline{\mathbf{w}}, \mathbf{v}}]\leq\mathbb{E}_{\mathbf{z},y}\bigl{[}\phi^{\prime}(\langle\overline{\mathbf{w }},\mathbf{z}\rangle)^{2}y_{K}^{2}\bigr{]}^{1/2}\mathbb{E}_{\mathbf{z}}\Bigl{[}\langle \mathbf{z},\mathbf{v}-\hat{\mathbf{v}}\rangle^{2}\Bigr{]}^{1/2}\lesssim K\epsilon_{v}.\]

To bound the differences when we replace \(\overline{\mathbf{w}}\) with \(\hat{\mathbf{w}}\), we need to make a distinction between ReLU and smooth activations as the respective arguments are to some extent different. When \(\phi^{\prime}\) is Lipschitz,

\[\frac{1}{n}\sum_{i=1}^{n}\Bigl{(}X_{\overline{\mathbf{w}},\hat{\mathbf{v}}}^{(i)}-X_{ \hat{\mathbf{w}},\hat{\mathbf{v}}}^{(i)}\Bigr{)}\leq\sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_ {K}^{(i)})^{2}\bigl{(}\phi^{\prime}(\langle\overline{\mathbf{w}},\mathbf{z}^{(i)} \rangle)-\phi^{\prime}(\langle\hat{\mathbf{w}},\mathbf{z}^{(i)}\rangle)\bigr{)}^{2}} \sqrt{\frac{1}{n}\sum_{i=1}^{n}\langle\mathbf{z}^{(i)},\hat{\mathbf{v}}^{2}\rangle} \lesssim K\epsilon_{w},\]

and

\[\mathbb{E}[X_{\hat{\mathbf{w}},\hat{\mathbf{v}}}-X_{\overline{\mathbf{w}},\hat{\mathbf{v}}}] \leq\mathbb{E}\Bigl{[}y_{K}^{2}(\phi^{\prime}(\langle\overline{\mathbf{w}},\mathbf{z} \rangle)-\phi^{\prime}(\langle\hat{\mathbf{w}},\mathbf{z}\rangle))^{2}\Bigr{]}^{1/2} \mathbb{E}\Bigl{[}\langle\mathbf{z},\hat{\mathbf{v}}\rangle^{2}\Bigr{]}^{1/2}\lesssim K \epsilon_{w}.\]

Therefore, for a smooth activation \(\phi\) we choose \(\epsilon_{v}=\epsilon_{w}=\sqrt{d/n}\), and obtain

\[\sup_{\overline{\mathbf{w}},\mathbf{v}\in\mathbb{S}^{d-1}}\frac{1}{n}\sum _{i=1}^{n}X_{\overline{\mathbf{w}},\mathbf{v}}^{(i)}-\mathbb{E}_{\mathbf{z},y}[X_{\overline {\mathbf{w}},\mathbf{v}}]\leq\sup_{\hat{\mathbf{w}},\hat{\mathbf{v}}}\frac{1}{n}\sum_{i=1}^{n}X_{ \hat{\mathbf{w}},\hat{\mathbf{v}}}^{(i)}-\mathbb{E}_{\mathbf{z},y}[X_{\hat{\mathbf{w}},\hat{ \mathbf{v}}}]+\tilde{\mathcal{O}}(\sqrt{d/n}).\]

When \(\phi\) is the ReLU activation, we need to show that the sign of the preactivation changes only for a small number of samples when we change the weight \(\overline{\mathbf{w}}\) to \(\hat{\mathbf{w}}\). Notice that

\[\operatorname{sign}\Bigl{(}\Big{\langle}\overline{\mathbf{w}},\mathbf{z} ^{(i)}\Big{\rangle}\Bigr{)}\neq\operatorname{sign}\Bigl{(}\Big{\langle} \hat{\mathbf{w}},\mathbf{z}^{(i)}\Big{\rangle}\Bigr{)} \implies\Big{|}\Big{\langle}\overline{\mathbf{w}},\mathbf{z}^{(i)}\Big{\rangle} \Big{|}\leq\Big{|}\Big{\langle}\hat{\mathbf{w}}-\overline{\mathbf{w}},\mathbf{z}^{(i)}\Big{\rangle} \Big{|}\] \[\implies\Big{|}\Big{\langle}\overline{\mathbf{w}},\overline{\mathbf{z}}^{(i)} \Big{\rangle}\Big{|}\leq\epsilon_{w}.\]

Recall that \(\overline{\mathbf{z}}^{(i)}\coloneqq\mathbf{z}^{(i)}/\|\mathbf{z}^{(i)}\|\). Choose \(\epsilon_{w}\asymp\sqrt{d}/n\). On event \(\mathcal{G}\), we know from Lemma 18 that at most \(\mathcal{O}(d\ln(n/\sqrt{d}))\) samples can satisfy the above condition. Therefore,

\[\frac{1}{n}\sum_{i=1}^{n}\Bigl{(}X_{\overline{\mathbf{w}},\hat{\mathbf{v}}}^{(i)}-X_{ \hat{\mathbf{w}},\hat{\mathbf{v}}}^{(i)}\Bigr{)}\] \[\lesssim K\sqrt{\frac{d\ln(n/\sqrt{d})}{n}}.\]and

\[\mathbb{E}[X_{\hat{\mathbf{w}},\hat{\mathbf{v}}}-X_{\overline{\mathbf{w}},\hat{ \mathbf{v}}}] \leq\mathbb{E}\Big{[}y_{K}^{2}(\phi^{\prime}(\langle\overline{\mathbf{w}},\mathbf{z}\rangle)-\phi^{\prime}(\langle\hat{\mathbf{w}},\mathbf{z}\rangle))^{2}\Big{]}^{1/ 2}\mathbb{E}\Big{[}\langle\mathbf{z},\hat{\mathbf{v}}\rangle^{2}\Big{]}^{1/2}\] \[\leq K\mathbb{P}(\operatorname{sign}(\langle\overline{\mathbf{w}}, \mathbf{z}\rangle)\neq\operatorname{sign}(\langle\hat{\mathbf{w}},\mathbf{z}\rangle))^{1/2}\] \[\leq K\mathbb{P}(|\langle\overline{\mathbf{w}},\mathbf{z}\rangle|\leq \epsilon_{w})^{1/2}\] \[\leq 2K\sqrt{d^{1/2}\epsilon_{w}},\]

where the last inequality follows from the anti-concentration on the sphere [2, Lemma A.7]. Thus, for ReLU we choose \(\epsilon_{v}\asymp\sqrt{d/n}\) and \(\epsilon_{w}\asymp\sqrt{d}/n\), and once again obtain

\[\sup_{\overline{\mathbf{w}},\mathbf{v}\in\mathbb{S}^{d-1}}\frac{1}{n}\sum_{i=1}^{n}X_{ \overline{\mathbf{w}},\mathbf{v}}^{(i)}-\mathbb{E}_{\mathbf{z},y}[X_{\overline{\mathbf{w}}, \mathbf{v}}]\leq\sup_{\hat{\mathbf{w}},\hat{\mathbf{v}}}\frac{1}{n}\sum_{i=1}^{n}X_{\hat{ \mathbf{w}},\hat{\mathbf{v}}}^{(i)}-\mathbb{E}_{\mathbf{z},y}[X_{\hat{\mathbf{w}},\hat{\mathbf{v}} }]+\tilde{\mathcal{O}}(\sqrt{d/n}).\]

It remains to bound the term

\[\sup_{\hat{\mathbf{w}},\hat{\mathbf{v}}}\frac{1}{n}\sum_{i=1}^{n}X_{\hat{\mathbf{w}},\hat{ \mathbf{v}}}^{(i)}-\mathbb{E}[X_{\hat{\mathbf{w}},\hat{\mathbf{v}}}].\]

Notice that for fixed \(\hat{\mathbf{w}},\hat{\mathbf{v}}\), \(X_{\hat{\mathbf{w}},\hat{\mathbf{v}}}\) is sub-Gaussian with sub-Gaussian norm \(\mathcal{O}(K)\). Thus, via the sub-Gaussian maximal inequality [16, Lemma 5.2],

\[\sup_{\hat{\mathbf{w}},\hat{\mathbf{v}}}\frac{1}{n}\sum_{i=1}^{n}X_{\hat{\mathbf{w}},\hat{ \mathbf{v}}}^{(i)}-\mathbb{E}[X_{\hat{\mathbf{w}},\hat{\mathbf{v}}}]\lesssim\sqrt{K^{2}d/ n\ln(1/(\epsilon_{w}\epsilon_{v}))},\]

with probability at least \(1-e^{-d}\). Consequently, we have

\[\sup_{\overline{\mathbf{w}}\in\mathbb{S}^{d-1}}\lVert\mathbf{\Delta}_{n}\rVert\leq \tilde{\mathcal{O}}(\sqrt{d/n}+d^{-q/2}),\]

with probability at least \(1-\mathcal{O}(d^{-q})\). Assuming that \(n\) grows at most polynomially in dimension and choosing a sufficiently large \(q\), we have \(\sup_{\overline{\mathbf{w}}\in\mathbb{S}^{d-1}}\lVert\mathbf{\Delta}_{n}\rVert\leq \tilde{\mathcal{O}}(\sqrt{d/n})\) with probability at least \(1-\mathcal{O}(d^{-q})\).

Finally, by Lemma 23,

\[\lVert\mathbf{\nu}(\overline{\mathbf{w}}^{t})\rVert\leq\lambda_{\max}\Big{(}\hat{\mathbf{ \Sigma}}\Big{)}\lesssim\lambda_{\max}(\mathbf{\Sigma}),\] (D.6)

with probability at least \(1-e^{-n^{\prime}/2}\). Combining the above with the bound on \(\lVert\mathbf{\Delta}_{n}\rVert\), we have \(\mathcal{E}_{1}\geq-\lambda_{\max}(\mathbf{\Sigma})\tilde{\mathcal{O}}(\sqrt{d/n})\) with probability at least \(1-\mathcal{O}(d^{-q})\), which concludes the first step of the proof.

**Step 2. Bounding the error due to the estimation of \(\mathbf{\Sigma}\), i.e. \(\mathcal{E}_{2}\) and \(\mathcal{E}_{3}\).** Recall that we are considering the event \(\mathcal{G}\), thus \(y^{(i)}=y^{(i)}_{K}\). We can control each of the error terms separately. We begin by \(\mathcal{E}_{2}\), where by Cauchy-Schwartz

\[\mathcal{E}_{2} =\frac{1}{n}\sum_{i=1}^{n}\Bigl{\{}\phi^{\prime}\Big{(}\langle \langle\overline{\mathbf{w}}^{t},\tilde{\mathbf{z}}^{(i)}\rangle\Big{)}-\phi^{\prime }\Big{(}\langle\langle\overline{\mathbf{w}}^{t},\mathbf{z}^{(i)}\rangle\Big{)}\Big{\}} y^{(i)}_{K}\Big{\langle}\mathbf{z}^{(i)},\mathbf{\nu}(\overline{\mathbf{w}}^{t})\Big{\rangle}\] \[\geq-K\lVert\mathbf{\nu}(\overline{\mathbf{w}}^{t})\rVert\sqrt{\frac{1}{ n}\sum_{i=1}^{n}\Bigl{\{}\phi^{\prime}\big{(}\langle\langle\overline{\mathbf{w}}^{t}, \mathbf{z}^{(i)}\rangle\big{)}-\phi^{\prime}\Big{(}\langle\overline{\mathbf{w}}^{t}, \tilde{\mathbf{z}}^{(i)}\rangle\Big{)}\Big{\}}^{2}},\]

where the last line follows from Lemma 18 and the fact that \(n\gtrsim d\). When \(\phi^{\prime}\) is additionally Lipschitz, we have

\[\mathcal{E}_{2}\gtrsim-K\lVert\mathbf{\nu}(\overline{\mathbf{w}}^{t})\rVert\sqrt{\frac {1}{n}\sum_{i=1}^{n}\Bigl{\langle}\overline{\mathbf{w}}^{t},\mathbf{z}^{(i)}-\tilde{ \mathbf{z}}^{(i)}\Bigr{\rangle}^{2}}.\]Moreover, for any \(\overline{\mathbf{w}}\in\mathbb{S}^{d-1}\),

\[\frac{1}{n}\sum_{i=1}^{n}\Bigl{\langle}\overline{\mathbf{w}},\mathbf{z}^{(i) }-\tilde{\mathbf{z}}^{(i)}\Bigr{\rangle}^{2} \leq\|\frac{1}{n}\sum_{i=1}^{n}\mathbf{z}^{(i)}{\mathbf{z}^{(i)}}^{\top}\| ^{2}\|(\mathbf{I}_{\mathbf{d}}-\hat{\mathbf{\Sigma}}^{-1/2}\mathbf{\Sigma}^{1/2}) \overline{\mathbf{w}}\|^{2}\] \[\leq\|\frac{1}{n}\sum_{i=1}^{n}\mathbf{z}^{(i)}{\mathbf{z}^{(i)}}^{\top} \|^{2}\|\mathbf{I}_{d}-\hat{\mathbf{\Sigma}}^{-1/2}\mathbf{\Sigma}^{1/2}\|^{2}\] \[\lesssim\frac{d}{n^{\prime}},\]

where the last inequality holds with probability at least \(1-2e^{-d}\) on the event of Lemma 24. Hence for smooth activations we conclude

\[\mathcal{E}_{2}\gtrsim-K\|\mathbf{\nu}(\overline{\mathbf{w}}^{t})\|\sqrt{d/n^{\prime}}.\]

When \(\phi\) is the ReLU activation, we need a more involved argument to control \(\mathcal{E}_{2}\). In particular, we will show that for any \(\overline{\mathbf{w}}\), at most only \(\tilde{\mathcal{O}}(d)\) datapoints can have \(\operatorname{sign}\bigl{(}\bigl{\langle}\overline{\mathbf{w}},\mathbf{z}^{(i)}\bigr{ }\bigr{\rangle}\bigr{)}\neq\operatorname{sign}\Bigl{(}\bigl{\langle}\overline{ \mathbf{w}},\tilde{\mathbf{z}}^{(i)}\bigr{\rangle}\Bigr{)}\). Notice that

\[\operatorname{sign}\Bigl{(}\bigl{\langle}\overline{\mathbf{w}},\mathbf{z }^{(i)}\bigr{\rangle}\Bigr{)}\neq\operatorname{sign}\Bigl{(}\bigl{\langle} \overline{\mathbf{w}},\tilde{\mathbf{z}}^{(i)}\bigr{\rangle}\Bigr{)} \implies\Bigl{|}\bigl{\langle}\overline{\mathbf{w}},\mathbf{z}^{(i)} \bigr{\rangle}\Bigr{|}\leq\Bigl{|}\bigl{\langle}\overline{\mathbf{w}},\mathbf{z}^{(i)} -\tilde{\mathbf{z}}^{(i)}\Bigr{\rangle}\Bigr{|}\] \[\implies\Bigl{|}\bigl{\langle}\overline{\mathbf{w}},\overline{\mathbf{z} }^{(i)}\bigr{\rangle}\Bigr{|}\leq\|\mathbf{I}_{d}-\hat{\mathbf{\Sigma}}^{-1/2}\mathbf{ \Sigma}^{1/2}\|\] (D.7)

where \(\overline{\mathbf{z}}^{(i)}\coloneqq\frac{\mathbf{z}^{(i)}}{\|\mathbf{z}^{(i)}\|}\) is distributed uniformly over \(\mathbb{S}^{d-1}\). From Lemma 24 we have \(\|\mathbf{I}_{d}-\hat{\mathbf{\Sigma}}^{-1/2}\mathbf{\Sigma}^{1/2}\|\lesssim\sqrt{ \frac{d}{n^{\prime}}}\) with probability at least \(1-2e^{-d}\). On the other hand, from Lemma 17 we know with probability at least \(1-e^{-d}\), for any \(\overline{\mathbf{w}}\in\mathbb{S}^{d-1}\) at most \(\tilde{\mathcal{O}}(d)\) of the labeled samples have \(\bigl{|}\bigl{\langle}\overline{\mathbf{w}},\mathbf{z}^{(i)}\bigr{\rangle}\bigr{|} \lesssim\sqrt{d}/n\). Recall that \(n^{\prime}\gtrsim n^{2}\) when using the ReLU activation. This is precisely why we make this choice for the ReLU activation, as we need to balance the RHS of (D.7) which is of order \(\sqrt{d/n^{\prime}}\) with the LHS of (D.7) which should at most be of order \(\sqrt{d}/n\) if we want to ensure only \(\tilde{\mathcal{O}}(d)\) samples satisfy the bound. When \(n^{\prime}=n^{2}\) we can balance these two terms, thus with probability at least \(1-3e^{-d}\) the sign change can occur for at most \(\tilde{\mathcal{O}}(d)\) many samples, and

\[\frac{1}{n}\sum_{i=1}^{n}\Bigl{\{}\phi^{\prime}\Bigl{(}\bigl{\langle} \overline{\mathbf{w}}^{t},\mathbf{z}^{(i)}\bigr{\rangle}\Bigr{)}-\phi^{\prime}\Bigl{(} \bigl{\langle}\overline{\mathbf{w}}^{t},\tilde{\mathbf{z}}^{(i)}\bigr{\rangle}\Bigr{)} \Bigr{\}}^{2}\leq\tilde{\mathcal{O}}\biggl{(}\frac{d}{n}\biggr{)}.\]

In this case, we end up with

\[\mathcal{E}_{2}\geq-K\|\mathbf{\nu}(\overline{\mathbf{w}}^{t})\|\tilde{\mathcal{O}}( \sqrt{d/n}).\]

Bounding \(\mathcal{E}_{3}\) for ReLU and Lipschitz \(\phi^{\prime}\) is identical. In both cases, by Cauchy-Schwartz,

\[\mathcal{E}_{3} \geq-\sqrt{\frac{1}{n}\sum_{i=1}^{n}\phi^{\prime}(\bigl{\langle} \overline{\mathbf{w}},\mathbf{z}^{(i)}\bigr{\rangle})^{2}y_{K}^{(i)}}\sqrt{\frac{1}{n }\sum_{i=1}^{n}\Bigl{\langle}\mathbf{z}^{(i)}-\tilde{\mathbf{z}}^{(i)},\mathbf{\nu}( \overline{\mathbf{w}}^{t})\Bigr{\rangle}^{2}}\] \[\gtrsim-K\|\mathbf{\nu}(\overline{\mathbf{w}}^{t})\|\sqrt{d/n^{\prime}},\]

which holds on the intersection of event \(\mathcal{G}\) and of Lemma 23. At last, using the bound on \(\|\mathbf{\nu}(\overline{\mathbf{w}}^{t})\|\) from (D.6), we obtain

\[\mathcal{E}_{2}\wedge\mathcal{E}_{3}\geq-\lambda_{\max}(\mathbf{\Sigma})\tilde{ \mathcal{O}}(\sqrt{d/n}),\]

with probability at least \(1-\mathcal{O}(d^{-q})\).

**Step 3. Analyzing the Convergence.** As a result of the previous steps, we have established

\[\frac{\mathrm{d}\langle\overline{\mathbf{w}}^{t},\overline{\mathbf{u}}\rangle}{\mathrm{d }t}\geq\bigl{\langle}\mathbf{\nu}(\overline{\mathbf{w}}^{t}),\mathbb{E}\bigl{[}\phi^{ \prime}(\bigl{\langle}\overline{\mathbf{w}}^{t},\mathbf{z}\bigr{\rangle})y\mathbf{z} \bigr{]}\bigr{\rangle}-\lambda_{\max}(\mathbf{\Sigma})\tilde{\mathcal{O}}(\sqrt{d/n }).\]Thanks to Lemma 11, we can write

\[\mathbb{E}[\phi^{\prime}(\langle\overline{\mathbf{w}},\mathbf{z}\rangle)yz] =\mathbb{E}[\phi^{\prime}(\langle\overline{\mathbf{w}},\mathbf{z}\rangle)g (\langle\overline{\mathbf{u}},\mathbf{z}\rangle)\mathbf{z}]\] \[=-\zeta_{\phi,g}(\langle\overline{\mathbf{w}},\overline{\mathbf{u}} \rangle)\overline{\mathbf{u}}-\psi_{\phi,g}(\langle\overline{\mathbf{w}}^{t},\overline {\mathbf{u}}\rangle)\overline{\mathbf{w}},\]

where \(\zeta_{\phi,g}\) and \(\psi_{\phi,g}\) were introduced in (D.2) and (D.3) respectively. Recall the definition of \(\mathbf{\nu}(\overline{\mathbf{w}}^{t})\),

\[\mathbf{\nu}(\overline{\mathbf{w}}^{t})\coloneqq(\mathbf{I}_{d}-\overline{\mathbf{w}}^{t} \overline{\mathbf{w}}^{t\top})\hat{\mathbf{\Sigma}}(\mathbf{I}_{d}-\overline{\mathbf{w}}^ {t}\overline{\mathbf{w}}^{t\top})\overline{\mathbf{u}}.\]

Therefore,

\[\frac{\mathrm{d}\langle\overline{\mathbf{w}}^{t},\overline{\mathbf{u}} \rangle}{\mathrm{d}t} \geq-\zeta_{\phi,g}\big{(}\langle\overline{\mathbf{w}}^{t},\overline{ \mathbf{u}}\rangle\big{)}\overline{\mathbf{u}}^{\top}(\mathbf{I}_{d}-\overline{\mathbf{w} }^{t}\overline{\mathbf{w}}^{t\top})\hat{\mathbf{\Sigma}}(\mathbf{I}_{d}-\overline{\bm {w}}^{t}\overline{\mathbf{w}}^{t\top})\overline{\mathbf{u}}-\lambda_{\max}(\mathbf{\Sigma} )\tilde{\mathcal{O}}(\sqrt{d/n})\] \[\geq c\langle\overline{\mathbf{w}}^{t},\overline{\mathbf{u}}\rangle^{s-1} \Big{\langle}\overline{\mathbf{u}}_{\perp}^{t},\hat{\mathbf{\Sigma}}\overline{\mathbf{u}} _{\perp}^{t}\Big{\rangle}-\lambda_{\max}(\mathbf{\Sigma})\tilde{\mathcal{O}}(\sqrt {d/n})\qquad\text{(By Assumption \ref{eq:

**Lemma 19**.: _Suppose \(\mathbf{\Sigma}\) follows the \((\kappa,\mathbf{\theta})\)-spiked model, \(\mathbf{w}^{0}\) is sampled uniformly from \(\mathbb{S}^{d-1}\), \(n^{\prime}\gtrsim d\), and there exist universal constants \(C_{2},C_{2}^{\prime},C_{3},C_{3}^{\prime}>0\) such that_

\[C_{2}d^{r_{2}}\leq\kappa\leq C_{2}^{\prime}d^{r_{2}}\qquad\text{ and}\qquad C_{3}d^{-r_{1}}\leq\langle\mathbf{u},\mathbf{\theta}\rangle\leq C_{3}^{\prime}d^{-r_{1}}\]

_for \(r_{1}\in[0,1/2]\) and \(r_{2}\in[0,1]\). Then, conditioned on \(\left\langle\overline{\mathbf{w}}^{0},\overline{\mathbf{u}}\right\rangle>0\), with any arbitrarily large constant probability \(1-\delta\), for sufficiently large \(d\) (that depends on \(\delta\)) we have_

\[\langle\overline{\mathbf{w}}^{0},\overline{\mathbf{u}}\rangle\gtrsim\begin{cases}d^{-1 /2}&0\leq r_{2}<r_{1}\\ d^{r_{2}-r_{1}-1/2}&r_{1}<r_{2}<2r_{1}\\ d^{(r_{2}-1)/2}&2r_{1}<r_{2}<1\end{cases}.\] (D.8)

**Proof.** By definition,

\[\left\langle\overline{\mathbf{w}}^{0},\overline{\mathbf{u}}\right\rangle=\frac{\left \langle\hat{\mathbf{\Sigma}}^{1/2}\mathbf{w}^{0},\mathbf{\Sigma}^{1/2}\mathbf{u}\right\rangle} {\|\hat{\mathbf{\Sigma}}^{1/2}\mathbf{w}^{0}\|\|\mathbf{\Sigma}^{1/2}\mathbf{u}\|}.\]

Recall that we are conditioning our arguments on \(\left\langle\overline{\mathbf{w}}^{0},\overline{\mathbf{u}}\right\rangle>0\), hence the numerator of the above fraction is positive. To translate the sample complexities of Theorems 5 and 7 to the spiked model, our goal is to lower bound \(\left\langle\overline{\mathbf{w}}^{0},\overline{\mathbf{u}}\right\rangle\) in terms of \(d\), \(r_{1}\), and \(r_{2}\).

We begin by observing that

\[\|\hat{\mathbf{\Sigma}}^{1/2}\mathbf{w}\|\leq\|\hat{\mathbf{\Sigma}}^{1/2}\mathbf{\Sigma}^{-1/2 }\|\|\mathbf{\Sigma}^{1/2}\mathbf{w}\|\lesssim\|\mathbf{\Sigma}^{1/2}\mathbf{w}\|,\]

where the last inequality holds on the event of Lemma 24, which happens with probability at least \(1-2e^{-d}\). Consequently,

\[\left\langle\overline{\mathbf{w}}^{0},\overline{\mathbf{u}}\right\rangle\gtrsim\frac{ \left\langle\hat{\mathbf{\Sigma}}^{1/2}\mathbf{w}^{0},\mathbf{\Sigma}^{1/2}\mathbf{u}\right\rangle }{\|\mathbf{\Sigma}^{1/2}\mathbf{w}^{0}\|\|\mathbf{\Sigma}^{1/2}\mathbf{u}\|}=\frac{\left\langle \mathbf{w}^{0},\mathbf{\Sigma}\mathbf{u}\right\rangle+\left\langle\mathbf{w}^{0},(\hat{\mathbf{ \Sigma}}^{1/2}-\mathbf{\Sigma}^{1/2})\mathbf{\Sigma}^{1/2}\mathbf{u}\right\rangle}{\|\mathbf{ \Sigma}^{1/2}\mathbf{w}^{0}\|\|\mathbf{\Sigma}^{1/2}\mathbf{u}\|}.\]

Furthermore, due to the Markov inequality,

\[\mathbb{P}\bigg{(}\left\langle\mathbf{w}^{0},\mathbf{\theta}\right\rangle^{2}\geq\frac {C_{1}}{d}\bigg{)}\leq 1/C_{1}.\]

Similarly (by conditioning on \(\hat{\mathbf{\Sigma}}\))

\[\mathbb{P}\bigg{(}\bigg{\langle}\mathbf{w}^{0},(\hat{\mathbf{\Sigma}}^{1/2}-\mathbf{ \Sigma}^{1/2})\mathbf{\Sigma}^{1/2}\mathbf{u}\bigg{\rangle}^{2}\geq\frac{C_{1}\|(\hat {\mathbf{\Sigma}}^{1/2}-\mathbf{\Sigma}^{1/2})\mathbf{\Sigma}^{1/2}\mathbf{u}\|^{2}}{d}\bigg{)} \leq 1/C_{1}.\]

Additionally, on the event of Lemma 24,

\[\|(\hat{\mathbf{\Sigma}}^{1/2}-\mathbf{\Sigma}^{1/2})\mathbf{\Sigma}^{1/2}\mathbf{u}\|\leq\| \hat{\mathbf{\Sigma}}^{1/2}\mathbf{\Sigma}^{-1/2}-\mathbf{I}_{d}\|\|\mathbf{\Sigma}\mathbf{u} \|\lesssim\sqrt{d/n^{\prime}}\|\mathbf{\Sigma}\mathbf{u}\|.\]

Therefore, on the above events, for some absolute constant \(C>0\)

\[\left\langle\overline{\mathbf{w}}^{0},\overline{\mathbf{u}}\right\rangle \gtrsim\frac{\left\langle\mathbf{w}^{0},\mathbf{\Sigma}\mathbf{u}\right\rangle -C\|\mathbf{\Sigma}\mathbf{u}\|\sqrt{1/n^{\prime}}}{\sqrt{\frac{1+C_{2}^{\prime}C_{1} }{1+\kappa}}\|\mathbf{\Sigma}\mathbf{u}\|}\] \[\gtrsim\frac{\left\langle\mathbf{w}^{0},\mathbf{u}\right\rangle+\kappa \langle\mathbf{w}^{0},\mathbf{\theta}\rangle\langle\mathbf{u},\mathbf{\theta}\rangle-C(1+ \kappa|\langle\mathbf{u},\mathbf{\theta}\rangle|)\sqrt{1/n^{\prime}}}{\sqrt{1+C_{2}^ {\prime}C_{1}}\sqrt{1+\kappa\langle\mathbf{u},\mathbf{\theta}\rangle^{2}}}.\]

Recall that \(C_{2}d^{r_{2}}\leq\kappa\leq C_{2}^{\prime}d^{r_{2}}\) and \(C_{3}d^{-r_{1}}\leq\langle\mathbf{u},\mathbf{\theta}\rangle\leq C_{3}^{\prime}d^{-r_{ 1}}\) (notice that changing \(\mathbf{\theta}\) to \(-\mathbf{\theta}\) does not change the spiked model of Assumption 1, thus we can assume \(\langle\mathbf{u},\mathbf{\theta}\rangle\geq 0\) without loss of generality). Then,

\[\left\langle\overline{\mathbf{w}}^{0},\overline{\mathbf{u}}\right\rangle\gtrsim\frac{ \left\langle\mathbf{w}^{0},\mathbf{u}\right\rangle+\kappa\langle\mathbf{w}^{0},\mathbf{ \theta}\rangle\langle\mathbf{u},\mathbf{\theta}\rangle-C(1+\kappa\langle\mathbf{u},\mathbf{ \theta}\rangle)\sqrt{1/n^{\prime}}}{\sqrt{1+C_{2}^{\prime}C_{1}}\sqrt{1+C_{2}^ {\prime}{C_{3}^{\prime}}^{2}d^{r_{2}-2r_{1}}}}.\] (D.9)

[MISSING_PAGE_FAIL:29]

The rest of the analysis is identical to that of the proof of Theorem 5 in Appendix D.3. Specifically, by defining

\[\overline{\mathbf{u}}_{\perp}^{t}\coloneqq\overline{\mathbf{u}}-\langle\overline{\mathbf{w}}^{ t},\overline{\mathbf{u}}\rangle\overline{\mathbf{w}}^{t},\]

we have

\[\frac{\mathrm{d}\langle\overline{\mathbf{w}}^{t},\overline{\mathbf{u}}\rangle }{\mathrm{d}t}= \langle\overline{\mathbf{u}}_{\perp}^{t},\mathbb{E}_{\mathbf{z},y}\big{[} \phi^{\prime}(\langle\overline{\mathbf{w}}^{t},\mathbf{z}\rangle)y\mathbf{z}\big{]}\rangle\] \[+\underbrace{\frac{1}{n}\sum_{i=1}^{n}\Bigl{\{}\phi^{\prime}\Bigl{(} \langle\overline{\mathbf{w}}^{t},\mathbf{z}^{(i)}\rangle\Bigr{)}-\phi^{\prime}\Bigl{(} \langle\overline{\mathbf{w}}^{t},\mathbf{z}^{(i)}\rangle\Bigr{)}\Bigr{\}}y^{(i)}\Bigl{ \langle}\mathbf{z}^{(i)},\overline{\mathbf{u}}_{\perp}^{t}\Bigr{\rangle}}_{=\in \mathcal{E}_{2}}\] \[+\underbrace{\frac{1}{n}\sum_{i=1}^{n}\phi^{\prime}\Bigl{(} \langle\overline{\mathbf{w}},\tilde{\mathbf{z}}^{(i)}\rangle\Bigr{)}y^{(i)}\Bigl{\langle} \tilde{\mathbf{z}}^{(i)}-\mathbf{z}^{(i)},\overline{\mathbf{u}}_{\perp}^{t}\Bigr{\rangle}}_ {=\in\mathcal{E}_{3}}.\]

As long as \(n\gtrsim d\), \(n^{\prime}=n\) for the smooth case, and \(n^{\prime}\gtrsim n^{2}\) for the ReLU case, the first two steps of the proof of Theorem 5 in Appendix D.3 implies that

\[\mathcal{E}_{1}\wedge\mathcal{E}_{2}\wedge\mathcal{E}_{3}\geq-\|\overline{\mathbf{ u}}_{\perp}^{t}\|\tilde{\mathcal{O}}(\sqrt{d/n})\geq-\tilde{\mathcal{O}}( \sqrt{d/n}).\]

Once again, we apply Lemma 11 to obtain

\[\mathbb{E}[\phi^{\prime}(\langle\overline{\mathbf{w}},\mathbf{z}\rangle)y\mathbf{z}]=- \zeta_{\phi,g}(\langle\overline{\mathbf{w}},\overline{\mathbf{u}}\rangle)\overline{\bm {u}}-\psi_{\phi,g}(\langle\overline{\mathbf{w}}^{t},\overline{\mathbf{u}}\rangle) \overline{\mathbf{w}},\]

with \(\zeta_{\phi,g}\) and \(\psi_{\phi,g}\) given in (D.2) and (D.3) respectively. As a result,

\[\frac{\mathrm{d}\langle\overline{\mathbf{w}}^{t},\overline{\mathbf{u}} \rangle}{\mathrm{d}t} \geq-\zeta_{\phi,g}(\langle\overline{\mathbf{w}}^{t},\overline{\mathbf{u} }\rangle)\|\overline{\mathbf{u}}_{\perp}^{t}\|^{2}-\tilde{\mathcal{O}}(\sqrt{d/n})\] \[\geq c\langle\overline{\mathbf{w}}^{t},\overline{\mathbf{u}}\rangle^{s-1} (1-\langle\overline{\mathbf{w}}^{t},\overline{\mathbf{u}}\rangle^{2})-\tilde{\mathcal{ O}}(\sqrt{d/n})\qquad\text{(By Assumption \ref{eq:def_eq})}.\]

We need to ensure the noise term, i.e. the second term on the RHS remains smaller than the signal, i.e. the first term. The signal term attains its minimum at either initialization \(t=0\) or at the end of the trajectory \(t=T\) where \(\bigl{\langle}\overline{\mathbf{w}}^{T},\overline{\mathbf{u}}\bigr{\rangle}=1-\varepsilon\), which imposes the following sufficient conditions on \(n\). Namely, at initialization we require

\[n\gtrsim Cd\langle\overline{\mathbf{w}}^{0},\overline{\mathbf{u}}\rangle^{2(1-s)},\]

while at \(t=T\) we require

\[n\gtrsim Cd/\varepsilon^{2},\]

where \(C\) hides constant that only depend on \(s\) and polylogarithmic factors of \(d\). Hence, we obtain

\[\frac{\mathrm{d}\langle\overline{\mathbf{w}}^{t},\overline{\mathbf{u}}\rangle}{ \mathrm{d}t}\geq c^{\prime}\langle\overline{\mathbf{w}}^{t},\overline{\mathbf{u}} \rangle^{s-1}(1-\langle\overline{\mathbf{w}}^{t},\overline{\mathbf{u}}\rangle^{2}).\]

for some universal constant \(c^{\prime}>0\). Via integration (similar to the proof of Proposition 4 in Appendix D.1), for

\[T_{1}\coloneqq\sup\{t>0:\langle\overline{\mathbf{w}}^{t},\overline{\mathbf{u}}\rangle <1/2\}\]

we obtain

\[T_{1}\lesssim\tau_{s}(\langle\overline{\mathbf{w}}^{0},\overline{\mathbf{u}}\rangle),\]

and for

\[T_{2}\coloneqq\sup\{t>0:\langle\overline{\mathbf{w}}^{t},\overline{\mathbf{u}}\rangle <1-\varepsilon\}\]

we obtain \(T_{2}-T_{1}\lesssim\ln(1/\varepsilon)\), which completes the proof. We conclude by remarking that the proof of Corollary 8 is immediate given Theorem 7 and Lemma 19.

### Preliminary Lemmas for Proving Theorem 9

We will adapt the following lemma from [23], which provides a non-parametric approximation of \(g\) via random biases.

**Lemma 20**.: [23, Lemma 22] _For any smooth \(g:\mathbb{R}\to\mathbb{R}\) and \(\Delta>0\), let \(\tilde{g}:\mathbb{R}\to\mathbb{R}\) be a smooth function such that \(\tilde{g}(z)=g(z)\) for \(|z|\leq\Delta\) and \(\tilde{g}(-2\Delta)=\tilde{g}^{\prime}(-2\Delta)=0\). Suppose \(\{b_{j}\}_{j=1}^{m}\stackrel{{ i.i.d.}}{{\sim}}\text{Unif}(-2 \Delta,2\Delta)\), and let \(\Delta_{*}:=\Delta\sup_{|z|\leq 2\Delta}\lvert\tilde{g}^{\prime\prime}(z)\rvert\). Then, there exist second layer weights \(\{a_{j}(b_{j})\}_{j=1}^{m}\) with \(\|\mathbf{a}\|\lesssim\Delta_{*}/\sqrt{m}\), such that for any fixed \(z\in[-\Delta,\Delta]\) and any \(\delta>0\), with probability at least \(1-\delta\) over the random biases,_

\[\left\lvert\sum_{j=1}^{m}a(b_{j})\phi(z+b_{j})-g(b_{j})\right\rvert\lesssim \Delta\Delta_{*}\sqrt{\frac{\ln(1/\delta)}{m}},\]

_where \(\phi\) is the ReLU activation._

We use the above lemma to show the existence of a second layer with \(\tilde{\mathcal{O}}(1/\sqrt{m})\) norm with training error of order \(\tilde{\mathcal{O}}(1/m)\).

**Lemma 21**.: _For any \(\varepsilon<1\), suppose \(\langle\overline{\mathbf{w}},\overline{\mathbf{u}}\rangle\geq 1-\varepsilon\). Then for any \(q>0\), sufficiently large \(d\), \(n\gtrsim d/\varepsilon^{2}\), with probability at least \(1-\tilde{\mathcal{O}}(d^{-q})\) over the random biases and the dataset, there exists a second layer \(\mathbf{a}\) with \(\|\mathbf{a}\|\leq\tilde{\mathcal{O}}(1/\sqrt{m})\) described by Lemma 20 such that_

\[\frac{1}{n}\sum_{i=1}^{n}\!\left(\sum_{j=1}^{m}a_{j}\phi\!\left(\left\langle \overline{\mathbf{w}},\tilde{\mathbf{z}}^{(i)}\right\rangle+b_{j}\right)-y^{(i)}\right) ^{2}\lesssim\mathbb{E}\!\left[\varepsilon^{2}\right]+\tilde{\mathcal{O}}(1/m+ \varepsilon),\]

_where \(\phi\) is the ReLU activation._

**Proof.** We begin by replacing \(\overline{\mathbf{w}}\) and \(\tilde{\mathbf{z}}^{(i)}\) with \(\overline{\mathbf{u}}\) and \(\mathbf{z}^{(i)}\). Specifically, via Jensen's inequality,

\[\frac{1}{n}\sum_{i=1}^{n}\!\left\{\sum_{j=1}^{m}a_{j}\phi\!\left( \left\langle\overline{\mathbf{w}},\tilde{\mathbf{z}}^{(i)}\right\rangle+b_{j}\right)- y^{(i)}\right\}^{2} \leq\underbrace{\frac{4}{n}\sum_{i=1}^{n}\!\left\{\sum_{j=1}^{m} a_{j}\phi\!\left(\left\langle\overline{\mathbf{u}},\mathbf{z}^{(i)}\right\rangle+b_{j}\right)-g \!\left(\left\langle\overline{\mathbf{u}},\mathbf{z}^{(i)}\right\rangle\right)\right\} ^{2}}_{=\mathcal{E}_{1}}\] \[\leq\underbrace{\frac{4}{n}\sum_{i=1}^{n}\!\left\{y^{(i)}-g\! \left(\left\langle\overline{\mathbf{u}},\mathbf{z}^{(i)}\right\rangle\right)\right\} ^{2}}_{=\mathcal{E}_{2}}\] \[\leq\underbrace{\frac{4}{n}\sum_{i=1}^{n}\!\left(\sum_{j=1}^{m} a_{j}\Big{\{}\phi\!\left(\left\langle\overline{\mathbf{w}},\tilde{\mathbf{z}}^{(i)} \right\rangle+b_{j}\right)-\phi\!\left(\left\langle\overline{\mathbf{w}},\mathbf{z}^ {(i)}\right\rangle+b_{j}\right)\Big{\}}\right)^{2}}_{=\mathcal{E}_{3}}\]

We bound each term separately. For \(\mathcal{E}_{1}\), we can invoke Lemma 20 which implies that each term in the sum can be bounded by \(\tilde{\mathcal{O}}(1/m)\) with probability at least \(1-1/(nd^{q})\), thus by a union bound, with probability at least \(1-d^{-q}\) over the random biases,

\[\mathcal{E}_{1}\leq\tilde{\mathcal{O}}(1/m).\]

By sub-Guassianity of \(\epsilon^{(i)}\) (hence sub-exponentiality of \(\epsilon^{(i)}\)2), for \(n\gtrsim d\) (with a sufficiently large constant) we have

Footnote 2: Note that the term \(\tilde{\mathcal{O}}(1/m)\) is not bounded by \(\tilde{\mathcal{O}}(1/m)\), but it is not bounded by \(\tilde{\mathcal{O}}(1/m)\).

\[\mathcal{E}_{2}\lesssim\mathbb{E}\!\left[\varepsilon^{2}\right]+\sqrt{d/n},\]with probability at least \(1-e^{-d}\).

For \(\mathcal{E}_{3}\), via the Lipschitzness of ReLU and the Cauchy-Schwartz inequality we can write

\[\mathcal{E}_{3}\leq\frac{\tilde{\mathcal{O}}(1)}{n}\sum_{i=1}^{n}\Bigl{\langle} \overline{\mathbf{w}},\tilde{\mathbf{z}}^{(i)}-\mathbf{z}^{(i)}\Bigr{\rangle}^{2}\leq \tilde{\mathcal{O}}(1)\|\mathbf{I}-\hat{\mathbf{\Sigma}}^{-1/2}\mathbf{\Sigma}^{1/2}\|^ {2}\leq\tilde{\mathcal{O}}(d/n^{\prime}),\]

where we used the event of Lemma 24 which happens with probability at least \(1-2e^{-d}\), and \(\tilde{\mathcal{O}}(1)\) represents a constant that depends at most polylogarithmically on \(d\).

Finally, we bound the last term. Once again via the Lipschitzness of the ReLU activation and the Cauchy-Schwartz inequality

\[\mathcal{E}_{4}\leq\frac{\tilde{\mathcal{O}}(1)}{n}\sum_{i=1}^{n}\Bigl{\langle} \overline{\mathbf{w}}-\overline{\mathbf{u}},\mathbf{z}^{(i)}\Bigr{\rangle}^{2}\leq\tilde{ \mathcal{O}}(\|\overline{\mathbf{w}}-\overline{\mathbf{u}}\|^{2})\leq\tilde{\mathcal{ O}}(\varepsilon),\]

where once again we used the event of Lemma 24. On the intersection of all desired events, we have

\[\frac{1}{n}\sum_{i=1}^{n}\Biggl{(}\sum_{j=1}^{m}a_{j}\phi\bigl{\langle} \bigl{\langle}\overline{\mathbf{w}},\tilde{\mathbf{z}}^{(i)}\bigr{\rangle}+b_{j})-y^{ (i)}\Biggr{)}\lesssim\mathbb{E}\bigl{[}\epsilon^{2}\bigr{]}+\tilde{\mathcal{O} }(1/m+\sqrt{d/n}+d/n^{\prime}+\varepsilon).\]

We conclude the proof by noticing that \(n^{\prime}\gtrsim n^{2}\) and \(n\gtrsim d\varepsilon^{-2}\). 

Additionally, we will use the following standard Lemma on the Rademacher complexity of two-layer neural networks, which in particular is a restatement of [13, Lemma 18] in a way suitable for our analysis.

**Lemma 22**.: _Let \(\mathcal{F}\) be a class of real-valued functions on \((\mathbf{z},y)\). Given \(n\) samples \(\{\mathbf{z}^{(i)},y\}_{i=1}^{n}\), define the empirical Rademacher complexity of \(\mathcal{F}\) as_

\[\hat{\mathcal{R}}_{n}(\mathcal{F})\coloneqq\mathbb{E}_{(\varsigma_{i})_{i=1}^{ n}}\Biggl{[}\sup_{f\in\mathcal{F}}\frac{1}{n}\sum_{i=1}^{n}\varsigma_{i}f(\mathbf{z}^ {(i)},y^{(i)})\Biggr{]},\]

_where \((\varsigma_{i})\) are i.i.d. Rademacher random variables (i.e. \(\pm 1\) with equal probability). Suppose \(\mathcal{F}\) is given by_

\[\mathcal{F}\coloneqq\Biggl{\{}(\mathbf{z},y)\mapsto\left(\sum_{j=1}^{m}a_{j}\phi( \langle\overline{\mathbf{u}},\mathbf{z}\rangle+b_{j})-y\right)^{2}\wedge C\,:\,\|\mathbf{ a}\|\leq r_{a}/\sqrt{m},\quad|b_{j}|\leq r_{b},\forall\,1\leq j\leq m\Biggr{\}},\]

_for some fixed \(\overline{\mathbf{u}}\in\mathbb{S}^{d-1}\). Suppose \(\{\mathbf{z}^{(i)}\}_{i=1}^{n}\stackrel{{ i.i.d.}}{{\sim}}\mathcal{N }(0,\mathbf{I}_{d})\), and suppose \(|\phi^{\prime}|\leq 1\). Then,_

\[\mathbb{E}_{(\mathbf{z}^{(i)},y^{(i)})_{i=1}^{n}}\Bigl{[}\hat{\mathcal{R}}_{n}( \mathcal{F})\Bigr{]}\leq\frac{2\sqrt{2C}(1+r_{b})r_{a}}{\sqrt{n}}.\]

**Proof.** See the proof of [13, Lemma 18]. 

### Proof of Theorem 9

Throughout the proof, we will assume \(\langle\overline{\mathbf{w}},\overline{\mathbf{u}}\rangle\geq 1-\varepsilon\) where we recall

\[\overline{\mathbf{w}}\coloneqq\frac{\hat{\mathbf{\Sigma}}^{1/2}\mathbf{w}}{\|\hat{\mathbf{ \Sigma}}^{1/2}\mathbf{w}\|}\quad\text{and}\quad\overline{\mathbf{u}}\coloneqq\frac{ \mathbf{\Sigma}^{1/2}\mathbf{u}}{\|\Sigma^{1/2}\mathbf{u}\|}.\]

From either Theorem 5 or Theorem 7, we can assume \(\langle\overline{\mathbf{w}},\overline{\mathbf{u}}\rangle\geq 1-\varepsilon\) with probability at least \(1-\mathcal{O}(d^{-q})\) for any fixed \(q>0\). For simplicity, let

\[\hat{y}(\mathbf{\tilde{z}};\overline{\mathbf{w}})=\sum_{j=1}^{m}a_{j}\phi(\langle \overline{\mathbf{w}},\tilde{\mathbf{z}}\rangle+b_{j}),\]and similarly define \(\hat{y}(\mathbf{z};\mathbf{\overline{u}})\). We define the following quantities,

\[\mathcal{R}(\mathbf{\overline{w}})\coloneqq\mathbb{E}_{\mathbf{z},y}\Big{[}(\hat{y}(\mathbf{z };\mathbf{\overline{w}})-y)^{2}\Big{]}\qquad\text{and}\qquad R(\mathbf{\overline{u}}) \coloneqq\mathbb{E}_{\mathbf{z},y}\Big{[}(\hat{y}(\mathbf{z};\mathbf{\overline{u}})-y)^{2} \Big{]},\] (E.1)

and similarly define their empirical counterparts,

\[\hat{\mathcal{R}}(\mathbf{\overline{w}})\coloneqq\frac{1}{n}\sum_{i=1}^{n}\Bigl{(} \hat{y}(\mathbf{z}^{(i)};\mathbf{\overline{w}})-y^{(i)}\Bigr{)}^{2}\quad\text{and} \quad\hat{R}(\mathbf{\overline{u}})\coloneqq\frac{1}{n}\sum_{i=1}^{n}\Bigl{(}\hat{ y}(\mathbf{z}^{(i)};\mathbf{\overline{u}})-y^{(i)}\Bigr{)}^{2}.\] (E.2)

Notice that ultimately, we are interested in bounding \(\mathcal{R}(\mathbf{\overline{w}})\). We break down the proof into three steps. In the first step, we show that \(\mathcal{R}(\mathbf{\overline{w}})\) can be upper bounded by \(R(\mathbf{\overline{u}})\). Then, via a generalization bound, we show that the \(R(\mathbf{\overline{u}})\) can be upper bounded by \(\hat{R}(\mathbf{\overline{u}})\). Finally, we show that \(\hat{R}(\mathbf{\overline{u}})\) can be upper bounded by the training error, i.e. \(\hat{\mathcal{R}}(\mathbf{\overline{w}})\), and convex optimization of the last layer can attain the near-optimal value of this training error which is bounded by Lemma 21.

**Step 1. Bounding \(\mathcal{R}(\mathbf{\overline{w}})\) via \(R(\mathbf{\overline{u}})\).** By Jensen's inequality,

\[\mathbb{E}_{\mathbf{z},y}\Big{[}\bigl{(}\hat{y}(\mathbf{z};\mathbf{\overline{w}})-y\bigr{)} ^{2}\Big{]}\leq 3\,\mathbb{E}_{\mathbf{z}}\Big{[}(\hat{y}(\mathbf{z};\mathbf{ \overline{w}})-\hat{y}(\mathbf{z};\mathbf{\overline{w}}))^{2}\Big{]}+3\,\mathbb{E}_{ \mathbf{z}}\Big{[}\bigl{(}\hat{y}(\mathbf{z};\mathbf{\overline{w}})-\hat{y}(\mathbf{z};\mathbf{ \overline{u}})\bigr{)}^{2}\Big{]}+3\,\mathbb{E}_{\mathbf{z},y}\Big{[}\bigl{(}\hat{ y}(\mathbf{z};\mathbf{\overline{u}})-y\bigr{)}^{2}\Big{]}.\]

Suppose \(\|\mathbf{a}\|\leq r_{a}/\sqrt{m}\). For the first term, by Lipschitzness of \(\phi\) and the Cauchy-Schwartz inequality

\[\mathbb{E}_{\mathbf{z}}\Big{[}\bigl{(}\hat{y}(\mathbf{z};\mathbf{\overline{w }})-\hat{y}(\mathbf{z};\mathbf{\overline{w}})\bigr{)}^{2}\Big{]} =\mathbb{E}_{\mathbf{z}}\left[\left(\sum_{j=1}^{m}a_{j}\Bigl{\{}\phi \Bigl{(}\Bigl{(}\mathbf{\overline{w}},\mathbf{z}^{(i)}\Bigr{)}+b_{j}\Bigr{)}-\phi \Bigl{(}\Bigl{(}\mathbf{\overline{w}},\mathbf{z}^{(i)}\Bigr{)}+b_{j}\Bigr{)}\Bigr{\}} \right)^{2}\right]\] \[\leq r_{a}^{2}\,\mathbb{E}_{\mathbf{z}}\Big{[}\bigl{(}\mathbf{\overline{ w}},\mathbf{\tilde{z}}-\mathbf{z}\bigr{)}^{2}\Big{]}\] \[\leq r_{a}^{2}\|\mathbf{1}_{d}-\hat{\mathbf{\Sigma}}^{-1/2}\mathbf{\Sigma}^ {1/2}\|^{2}\lesssim r_{a}^{2}d/n^{\prime},\]

where the last inequality holds with probability at least \(1-2e^{-d}\) on the event of Lemma 24.

For the middle term, via a similar argument,

\[\mathbb{E}_{\mathbf{z}}\Big{[}\bigl{(}\hat{y}(\mathbf{z};\mathbf{\overline{w}})-\hat{y}( \mathbf{z};\mathbf{\overline{u}})\bigr{)}^{2}\Big{]}\leq r_{a}^{2}\,\mathbb{E}_{\mathbf{z }}\Big{[}\bigl{(}\mathbf{\overline{w}}-\mathbf{\overline{u}},\mathbf{z}\bigr{)}^{2}\Big{]} \leq 2r_{a}\varepsilon.\]

In what follows, we will restrict the analysis to the case where \(r_{a}=\tilde{\mathcal{O}}(1)\). Therefore, we have

\[\mathbb{E}_{\mathbf{z},y}\Big{[}\bigl{(}\hat{y}(\mathbf{z};\mathbf{\overline{w}})-y\bigr{)} ^{2}\Big{]}\leq 3\,\mathbb{E}_{\mathbf{z},y}\Big{[}\bigl{(}\hat{y}(\mathbf{z};\mathbf{ \overline{u}})-y\bigr{)}^{2}\Big{]}+\tilde{\mathcal{O}}(d/n^{\prime}+ \varepsilon).\]

**Step 2. Generalization: Bounding \(R(\mathbf{\overline{u}})\) via \(\hat{R}(\mathbf{\overline{u}})\).**

Define the event

\[E\coloneqq\Big{\{}|\langle\mathbf{\overline{u}},\mathbf{z}\rangle|\vee|\epsilon|\leq \sqrt{2\ln(nd^{q})}\Big{\}}.\]

and similarly define \(E^{(i)}\) by replacing \(\mathbf{z}\) and \(\epsilon\) with \(\mathbf{z}^{(i)}\) and \(\epsilon^{(i)}\) respectively. Via the Cauchy-Schwartz and Jensen inequalities

\[\mathbb{E}_{\mathbf{z},y}\Big{[}\bigl{(}\hat{y}(\mathbf{z};\mathbf{\overline{ u}})-y\bigr{)}^{2}\Big{]} =\mathbb{E}_{\mathbf{z},y}\Big{[}(\hat{y}(\mathbf{z};\mathbf{\overline{u}})-y) ^{2}\mathbf{1}(E)\Big{]}+\mathbb{E}_{\mathbf{z},y}\Big{[}\bigl{(}\hat{y}(\mathbf{z};\mathbf{ \overline{u}})-y\bigr{)}^{2}\mathbf{1}(E^{C})\Big{]}\] \[\leq\mathbb{E}_{\mathbf{z},y}\Big{[}(\hat{y}(\mathbf{z};\mathbf{\overline{u}})-y )^{2}\mathbf{1}(E)\Big{]}+\sqrt{8}\bigl{(}\mathbb{E}\bigl{[}\hat{y}(\mathbf{z};\mathbf{ \overline{u}})^{4}\bigr{]}+\mathbb{E}\bigl{[}y^{4}\bigr{]}\bigr{)}^{1/2}\mathbb{P }\bigl{(}E^{C}\bigr{)}^{1/2}\]

Moreover, \(\mathbb{E}\left[y^{4}\right]\lesssim 1\), \(\mathbb{E}_{\mathbf{z},y}\big{[}\hat{y}(\mathbf{z};\mathbf{\overline{u}})^{4}\big{]}\leq\tilde {\mathcal{O}}(1)\), and \(\mathbb{P}\bigl{(}E^{C}\bigr{)}\leq 4/(nd^{q})\) (via a standard sub-Gaussian tail bound). Consequently,

\[\mathbb{E}_{\mathbf{z},y}\Big{[}\bigl{(}\hat{y}(\mathbf{z};\mathbf{\overline{ u}})-y\bigr{)}^{2}\Big{]}\leq\mathbb{E}_{\mathbf{z},y}\Big{[}(\hat{y}(\mathbf{z};\mathbf{ \overline{u}})-y)^{2}\mathbf{1}(E)\Big{]}+\tilde{\mathcal{O}}(n^{-1}d^{-q}),\]

Let

\[\ell(\mathbf{z}^{(i)},y^{(i)};\mathbf{a},\mathbf{b})\coloneqq\left(\sum_{j=1}^{m}a_{j} \phi\Bigl{(}\Bigl{\langle}\mathbf{\overline{u}},\mathbf{z}^{(i)}\Bigr{\rangle}+b_{j} \Bigr{)}-y^{(i)}\right)^{2}\mathbf{1}(E).\]

[MISSING_PAGE_FAIL:34]

Let \(\{\mathbf{a}^{t}\}_{t\geq 0}\) be the solution to the gradient flow of \(\mathbf{a}\). Then,

\[\frac{\mathrm{d}\|\mathbf{a}^{t}-\mathbf{a}^{*}\|^{2}}{\mathrm{d}t}=-2\Big{\langle}\mathbf{a }^{t}-\mathbf{a}^{*},\nabla\hat{\mathcal{R}}_{\lambda}(\mathbf{a}^{t})\Big{\rangle},\]

and by the first-order condition of strong convexity

\[\Big{\langle}\mathbf{a}^{t}-\mathbf{a}^{*},\nabla\hat{\mathcal{R}}_{\lambda}(\mathbf{a}^{t} )\Big{\rangle}\geq\lambda\|\mathbf{a}^{t}-\mathbf{a}^{*}\|^{2},\]

therefore

\[\|\mathbf{a}^{T^{\prime}}-\mathbf{a}^{*}\|^{2}\leq e^{-2\lambda T^{\prime}}\|\mathbf{a}^{0 }-\mathbf{a}^{*}\|^{2}.\]

As the training error (of the regularized problem) is \(\lambda\)-strongly convex in \(\mathbf{a}\), by applying the standard Polyak-Lojasiewicz condition, gradient flow for training \(\mathbf{a}\) obtains

\[\hat{\mathcal{R}}_{\lambda}(\mathbf{a}^{T^{\prime}})-\hat{\mathcal{R}}_{\lambda}^{ *}\leq\Big{(}\hat{\mathcal{R}}_{\lambda}(\mathbf{a}^{0})-\hat{\mathcal{R}}_{\lambda }^{*}\Big{)}e^{-2\lambda T^{\prime}}.\]

Furthermore, since

\[\|\mathbf{a}^{*}\|^{2}-\|\mathbf{a}^{T^{\prime}}\|^{2}\leq 2\|\mathbf{a}^{*}\|\|\mathbf{a}^{T^{ \prime}}-\mathbf{a}^{*}\|-\|\mathbf{a}^{T^{\prime}}-\mathbf{a}^{*}\|^{2}\leq 2\|\mathbf{a}^{T^{ \prime}}-\mathbf{a}^{*}\|\|\mathbf{a}^{*}\|,\]

we have

\[\hat{\mathcal{R}}(\mathbf{a}^{T^{\prime}})-\hat{\mathcal{R}}^{*}\leq 2\|\mathbf{a}^{0 }-\mathbf{a}^{*}\|\|\mathbf{a}^{*}\|e^{-\lambda T^{\prime}}+\Big{(}\hat{\mathcal{R}}_{ \lambda}(\mathbf{a}^{0})-\hat{\mathcal{R}}_{\lambda}^{*}\Big{)}e^{-2\lambda T^{ \prime}}.\]

Consequently, choosing

\[T^{\prime}\geq\frac{\ln\Big{(}\frac{\|\mathbf{a}^{0}-\mathbf{a}^{*}\|}{\|\mathbf{a}^{*}\|} \Big{)}}{\lambda}\vee\frac{\ln\Big{(}\frac{4\|\mathbf{a}^{0}-\mathbf{a}^{*}\|\|\mathbf{a}^ {*}\|}{\varepsilon}\Big{)}}{\lambda}\vee\frac{\ln\Big{(}\frac{2(\hat{\mathcal{ R}}_{\lambda}(\mathbf{a}^{0})-\hat{\mathcal{R}}_{\lambda}^{*})}{\varepsilon}\Big{)}}{2 \lambda},\] (E.4)

implies

\[\hat{\mathcal{R}}(\mathbf{a}^{T^{\prime}})\leq\hat{\mathcal{R}}^{*}+\varepsilon\quad \text{and}\quad\|\mathbf{a}^{T^{\prime}}\|\leq 2\|\mathbf{a}^{*}\|\lesssim\tilde{ \mathcal{O}}(1/\sqrt{m}).\]

Therefore

\[\hat{\mathcal{R}}(\mathbf{a}^{T^{\prime}})\lesssim\mathbb{E}\big{[}\epsilon^{2} \big{]}+\tilde{\mathcal{O}}(1/m+\varepsilon).\]

Recall that

\[\hat{\mathcal{R}}(\mathbf{a}^{T^{\prime}})=\frac{1}{n}\sum_{i=1}^{n}\Bigl{(}\hat{y} (\tilde{\mathbf{z}}^{(i)};\overline{\mathbf{w}})-y^{(i)}\Bigr{)}^{2},\]

is the final training error which we also denoted by \(\hat{\mathcal{R}}(\overline{\mathbf{w}})\) earlier in this section when were not focusing on the second layer. From the previous two steps, we know how to bound \(\mathcal{R}(\overline{\mathbf{w}})\) via \(\hat{R}(\overline{\mathbf{u}})\). Thus the last step is to upper bound \(\hat{R}(\overline{\mathbf{u}})\) via \(\hat{\mathcal{R}}(\overline{\mathbf{w}})\). To that end, via Jensen's inequality

\[\frac{1}{n}\sum_{i=1}^{n}\Bigl{(}\hat{y}(\mathbf{z}^{(i)};\overline{ \mathbf{u}})-y^{(i)}\Bigr{)}^{2}\leq \frac{3}{n}\sum_{i=1}^{n}\Bigl{(}\hat{y}(\tilde{\mathbf{z}}^{(i)}; \overline{\mathbf{w}})-y^{(i)}\Bigr{)}^{2}\] \[+\frac{3}{n}\sum_{i=1}^{n}\Bigl{(}\hat{y}(\mathbf{z}^{(i)};\overline{ \mathbf{w}})-\hat{y}(\tilde{\mathbf{z}}^{(i)};\overline{\mathbf{w}})\Bigr{)}^{2}\] \[+\frac{3}{n}\sum_{i=1}^{n}\Bigl{(}\hat{y}(\mathbf{z}^{(i)};\overline{ \mathbf{w}})-\hat{y}(\mathbf{z}^{(i)};\overline{\mathbf{u}})\Bigr{)}^{2}.\]

The first term on the RHS is \(\hat{\mathcal{R}}(\overline{\mathbf{w}})\) for which we developed a bound earlier in this step. Bounding the latter two terms can be performed similarly to the arguments in the previous sections. In particular,

\[\frac{1}{n}\sum_{i=1}^{n}\Bigl{(}\hat{y}(\mathbf{z}^{(i)};\overline{\mathbf{w}})-\hat{ y}(\tilde{\mathbf{z}}^{(i)};\overline{\mathbf{w}})\Bigr{)}^{2}\leq r_{a}^{2}\|\mathbf{I}_{d}- \hat{\mathbf{\Sigma}}^{-1/2}\mathbf{\Sigma}^{1/2}\|^{2}\|\frac{1}{n}\sum_{i=1}^{n}\mathbf{z }^{(i)}{\mathbf{z}^{(i)}}^{\top}\|^{2}\leq\tilde{\mathcal{O}}(d/n^{\prime}),\]

where the last inequality holds with probability at least \(1-2e^{-d}\) (over the event of Lemma 24). Similarly,

\[\frac{1}{n}\sum_{i=1}^{n}\Bigl{(}\hat{y}(\mathbf{z}^{(i)};\overline{\mathbf{w}})-\hat{ y}(\mathbf{z}^{(i)};\overline{\mathbf{u}})\Bigr{)}^{2}\leq r_{a}\|\overline{\mathbf{w}}- \overline{\mathbf{u}}\|^{2}\leq\tilde{\mathcal{O}}(\varepsilon).\]

[MISSING_PAGE_FAIL:36]

Moreover, by [22, Example 6.2], we have with probability at least \(1-2e^{-d}\),

\[\lambda_{\max}\Bigg{(}\frac{1}{n^{\prime}}\sum_{i=1}^{n^{\prime}}{ \boldsymbol{z}^{(i)}{\boldsymbol{z}^{(i)}}^{\top}}\Bigg{)}\leq 1+(\sqrt{2}+1)\sqrt{ \frac{d}{n^{\prime}}}\quad\text{and}\quad\lambda_{\min}\Bigg{(}\frac{1}{n^{ \prime}}\sum_{i=1}^{n^{\prime}}{\boldsymbol{z}^{(i)}{\boldsymbol{z}^{(i)}}^{ \top}}\Bigg{)}\geq 1-(\sqrt{2}+1)\sqrt{\frac{d}{n^{\prime}}}.\]

Thus, for \(n^{\prime}\gtrsim d\) (with a sufficiently large absolute constant), we have

\[\|\mathbf{I}_{d}-\hat{\boldsymbol{\Sigma}}^{1/2}\hat{\boldsymbol{\Sigma}}^{-1 /2}\|\vee\|\mathbf{I}_{d}-\hat{\boldsymbol{\Sigma}}^{-1/2}\boldsymbol{\Sigma} ^{1/2}\|\lesssim\sqrt{\frac{d}{n^{\prime}}}\]

with probability at least \(1-2e^{-d}\). 

**Lemma 25** (Chernoff's Inequality).: _Suppose \(X_{1},\ldots,X_{n}\) are i.i.d. Bernoulli random variables, and further assume that \(\mathbb{E}[\sum_{i}X_{i}]\leq\mu\). Then, for any \(\delta\geq 1\),_

\[\mathbb{P}\Bigg{(}\sum_{i=1}^{n}X_{i}\geq\mu(1+\delta)\Bigg{)}\leq e^{-\mu \delta/3}.\] (F.1)

**Proof.** The proof follows from a standard Chernoff bound. From [20, Theorem 2.3.1]

\[\mathbb{P}\Bigg{(}\sum_{i}X_{i}\geq\mu(1+\delta)\Bigg{)}\leq e^{\mu(\delta-(1+ \delta)\ln(1+\delta))},\]

(notice that the statement of [20, Theorem 2.3.1] holds true even when \(\mathbb{E}[\sum_{i}X_{i}]=\mu\) is replaced with \(\mathbb{E}[\sum_{i}X_{i}]\leq\mu\)). We conclude by remarking that \(\delta-(1+\delta)\ln(1+\delta)\leq-\delta/3\) for \(\delta\geq 1\).