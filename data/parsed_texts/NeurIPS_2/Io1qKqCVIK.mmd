[MISSING_PAGE_FAIL:1]

Introduction

Polygonal meshes are widely used in modeling and animation due to their diverse, compact and explicit configuration. Recent AI progress has spurred efforts to integrate mesh generation into machine learning, but challenges like varying topology hinder suitable differentiable mesh representations. This limitation leads to reliance on differentiable intermediates like implicit functions, and subsequent iso-surface extraction for mesh creation (Liao et al., 2018; Guillard et al., 2021; Munkberg and others, 2022; Shen and others, 2023, 2021; Liu and others, 2023b). However, meshes generated by such approaches can be misaligned at sharp regions and unnecessarily dense (Shen and others, 2023), not suitable for down-stream applications that require light-weight meshes. This limitation necessitates us to develop a truly differentiable mesh representation, not the intermediate forms.

The fundamental challenge in creating a differentiable mesh representation lies in formulating both the vertices' geometric features and their connectivity, defined as edges and faces, in a differentiable way. Given a vertex set, predicting their connectivity in a free-form way using existing machine learning data-structures can cost significant amount of computation and be difficult to avoid irregular and intersecting faces. Consequently, most studies on differentiable meshes simplify the task by using a mesh with a pre-determined topology and modifying it through various operations (Zhou and others, 2020; Hanocka and others, 2019; Palfinger, 2022; Nicolet and others, 2021). This work, on the contrary, ambitiously aims to establish a general 3D mesh representation, named as DMesh, where both mesh topology and geometric features (e.g. encoded in vertex location) can be simultaneously optimized through gradient-based techniques.

Our core insight is to use differentiable Weighted Delaunay Triangulation (WDT) to divide a convex domain, akin to amber encapsulating a surface mesh, into tetrahedra to form a mesh. To create a mesh with arbitrary topology, we select only a subset of triangular faces from the tetrahedra, termed the "real part", as our final mesh. The other faces, the "imaginary part", support the real part but are not part of the final mesh (Figure 4). We introduce a method to assess the probability of a face being part of the mesh based on weighted points that carry positional and inclusiveness information. Optimization is then focused on the points' features to generate the triangular mesh. The probability determination allows us to compute geometric losses and rendering losses during gradient-based optimization that optimizes **connectivity and positioning**.

The key contributions of our work can be summarized as follows.

* We present a novel differentiable mesh representation, **DMesh**, which is versatile to accommodate various types of mesh (Figure 2). The generated meshes can represent shapes more effectively, with much less number of vertices and faces (Table 2).
* We propose a computationally efficient approach to differentiable WDT, which produces robust probability estimations. While exhaustive approach (Rakotosaona and others, 2021) requires quadratic computational cost, our method runs in approximately _linear time_.
* We provide efficient algorithms for reconstructing surfaces from both point clouds and multi-view images using DMesh as an intermediate representation.
* We finally propose an effective regularization term which can be used for mesh simplification and enhancing triangle quality.

Additionally, to further accelerate the algorithm, we implemented our main algorithm and differentiable renderer in CUDA, which is made available for further research.

## 2 Related Work

### Shape Representations for Optimization

Recently, using neural implicit functions for shape representation gained popularity in graphics and vision applications (Mildenhall and others, 2021; Zhang and others, 2020; Liu and others, 2020; Chen and others, 2022; Wang and others, 2021; Yariv and others, 2020). They mainly use volume density, inspired by (Mildenhall and others, 2021), to represent a shape. However, because of its _limited accuracy_ in 3D surface representation, neural signed distance functions (SDFs) (Yariv and others, 2021; Wang and others, 2021, 2023; Oechsle and others, 2021) or unsigned distance functions (UDFs) (Liu and others, 2023a; Long and others, 2023) are often preferred.

After optimization, one can recover meshes using iso-surface extraction techniques (Lorensen and Cline, 1998; Ju u. a., 2002).

Differing from neural representations, another class of methods directly produce meshes and optimize them. However, they assume that the overall mesh topology is fixed (Chen et al., 2019; Nicolet and others, 2021; Liu and others, 2019; Laine and others, 2020), only allowing local connectivity changes through remeshing (Palfinger, 2022). Learning-based approaches like BSP-Net (Chen and others, 2020) allow topological variation, but their meshing process is not differentiable. Recently, differentiable iso-surface extraction techniques have been developed, resulting in high-quality geometry reconstruction of various topologies (Liao and others, 2018; Shen and others, 2021, 2023; Wei and others, 2023; Munkberg and others, 2022; Liu and others, 2023b; Mehta and others, 2022). Unfortunately, meshes relying on iso-surface extraction algorithms (Lorensen and Cline, 1998; Ju u. a., 2002) often result in unnecessarily dense meshes that could contain geometric errors. In contrast, **our approach addresses these issues: we explicitly define faces and their existence probabilities, and devise regularizations that yield simplified, but accurate meshes based on them** (Table 2). See Table 3 for more detailed comparisons to these other methods.

### Delaunay Triangulation for Geometry Processing

Delaunay Triangulation (DT) (Aurenhammer and others, 2013) has been proven to be useful for reconstructing shapes from unorganized point sets. It's been shown that DT of dense samples on a smooth 2D curve includes the curve within its edges (Brandt and Algazi, 1992; Amenta and others, 1998a). This idea of using DT to approximate shape has been successfully extended to 3D, to reconstruct three-dimensional shapes (Amenta and others, 1998b) for point sets that satisfy certain constraints. However, these approaches are deterministic. Our method can be considered as a differentiable version of these approaches, which admits gradient-based optimization.

More recently, Rakotosaona and others (2021) focused on this DT's property to connect points and tessellate the domain, and proposed a differentiable WDT algorithm to compute smooth inclusion, namely existence score of \(2\)-simplexes (triangles) in 2 dimensional space. However, it is not suitable to apply this approach to our 3D case, as there are computational challenges (Section 3.2). Other related work, VoroMesh (Maruani and others, 2023), also used Voronoi diagrams in point cloud reconstruction, but their formulation cannot represent open surfaces and is only confined to handle point clouds.

## 3 Preliminary

### Probabilistic Approach to Mesh Connectivity

To define a traditional, non-differentiable mesh, we specify the vertices and their connectivity. This connectivity is discrete, meaning for any given triplet of vertices, we check if they form a face in the mesh, returning 1 if they do and 0 otherwise. To overcome this discreteness, we propose a

Figure 3: Our overall framework to optimize mesh according to the given observations. **(a)**: Each point is defined by a \(5\)-dimensional feature vector: position, weight, and real value. Points with larger real values are rendered in red. **(b)**: Given a set of points, we gather possibly existing faces in the mesh and evaluate their probability in differentiable manner. **(c)**: We can compute reconstruction loss based on given observations, such as mesh, point cloud, or multi-view images. **(d)**: To facilitate the optimization process and enhance the mesh quality, we can use additional regularizations.

**probabilistic** approach to create a fully differentiable mesh - given a triplet of vertices, we evaluate the probability of a face existing. This formulation enables differentiability not only of **vertex positions** but also of their **connectivity**.

Note that we need a procedure that tells us the existence probability of any given face to realize this probabilistic approach. This procedure must be **1)** differentiable, **2)** computationally efficient, and **3)** maintain desirable mesh properties, such as avoiding non-manifoldness and self-intersections, when determining the face probabilities.

Among many possible options, we use _Weighted Delaunay Triangulation (WDT)_ (Figure 6(a)) and a point-wise feature called the "real" value (\(\psi\)) to define our procedure. Each vertex in our framework is represented as a 5-dimensional2 vector including position (3), WDT weight (1), and real value (1) (Figure 3(a)). Given the precomputed WDT based on vertices' positions and weights, we check the face existence probability of each possible triplets. Specifically, **1)** a face \(F\) must exist in the WDT, and then **2)** satisfy a condition on the real values of its vertices to exist on the actual surface. We describe the probability functions for these conditions as \(\Lambda_{wdt}\) and \(\Lambda_{real}\):

Footnote 2: In 2D case, a vertex is a 4-dimensional vector including position (2), WDT weight (1), and real value (1).

\[\Lambda_{wdt}(F)=P(F\in\text{WDT}),\quad\Lambda_{real}(F)=P(F\in\text{Mesh} \,|\,F\in\text{WDT}). \tag{1}\]

Then we get the final existence probability function, which can be used in downstream applications (Figure 3), as follows:

\[\Lambda(F)=P(F\in\text{Mesh})=\Lambda_{wdt}(F)\cdot\Lambda_{real}(F). \tag{2}\]

This formulation attains one nice property in determining the final mesh - that is, it prohibits self-intersections between faces. When it comes to the other two criteria about this procedure, \(\Lambda_{wdt}\) function's differentiability and efficiency is crucial, as we design \(\Lambda_{real}\) to be a very efficient differentiable function based on real values (Section 4.2). Thus, we first introduce how we can evaluate \(\Lambda_{wdt}\) in a differentiable and efficient manner, which is one of our main contributions.

### Basic Principles

To begin with, we use \((d,k)\) pair to denote a \(k\)-simplex (\(\Delta^{k}\)) in \(d\)-dimensional space. For a 3D mesh, we observe that our face \(F\) corresponds to \((d=3,k=2)\) in Figure 5(b). To compute the probability \(\Lambda_{wdt}\) for the face, we use power diagram (PD), which is the dual structure of WDT (Figure 6(a)). While previously Rakotosaona et al. (2021) proposed a differentiable 2D triangulation method for the \((d=k=2)\) case (Figure 5(c)), it suffers from quadratically increasing computational cost (e.g. it takes 4.3 seconds to process \(10K\) points in 2D, Table 4) and unreliable estimation when \((k<d)\). We will discuss later how our formulation conquer these computational challenges. Our setting for 3D meshes are similar to 2D meshes, the \((d=2,k=1)\) case in Figure 5(d), where a triangular face reduces to a line. Therefore, we will mainly use this setting to describe and visualize basic concepts for simplicity. However, note that it can be generalized to any \((d,k)\) case without much problem.

Figure 4: Illustration of our mesh representation for 2D and 3D cases. **(a):** Our representation in 2D for a letter “A”. **(b):** Our representation in 3D for a dragon model. Blue faces are _“real part”_ and yellow ones are _“imaginary part”_.

Figure 5: Renderings of \(\Delta^{k}\)s for different pairs of \((d,k)\). Different \(\Delta^{k}\)s are rendered in different colors.

We generalize the basic principles suggested by Rakotosaona u. a. (2021) to address our cases. For formal definitions of the concepts in this section, please refer to Appendix B.

Let \(S\) be a finite set of points in \(\mathbb{R}^{d=2}\) with weights. For a given point \(p\in S\), we denote its weight as \(w_{p}\). We call those weighted points in \(S\) as "vertices", to distinguish them from general unweighted points in \(\mathbb{R}^{2}\). Then, we adopt power distance \(\pi(p,q)=d(p,q)^{2}-w_{p}-w_{q}\) as the distance measure between two vertices in \(S\). As depicted in Figure 6, \(\Delta^{k=1}\) is a 1-simplex, which is a line connecting two vertices \(p_{i}\) and \(p_{j}\) in \(S\). Its dual form \(D_{\Delta^{1}}\) (red line) is the set of unweighted points3 in \(\mathbb{R}^{2}\) that are located at the same power distance to \(p_{i}\) and \(p_{j}\). In the power diagram, the power cell \(C_{p}\) (gray cell) of a vertex \(p\) is the set of unweighted points that are closer to \(p\) than to any other vertices in \(S\). We can use \(C_{p}\) and \(D_{\Delta^{1}}\) to measure the existence of \(\Delta^{1}\). From Figure 6, we can see that when \(\Delta^{1}=\{p_{i},p_{j}\}\) exists in WDT, its dual line \(D_{\Delta^{1}}\) aligns exactly with \(C_{p_{i}}\)'s boundary, while when \(\Delta^{1}=\{p_{i},p_{j}\}\) doesn't exist in WDT, \(D_{\Delta^{1}}\) is outside \(C_{p_{i}}\).

Footnote 3: We treat unweighted points’ weight as \(0\) when computing the power distance.

To make this measurement less binary when \(\Delta^{1}\) exists, we use the expanded version of power cell called "reduced" power cell (\(R_{p|\Delta}\)), introduced by Rakotosaona u. a. (2021). The reduced power cell of \(p_{i}\in S\) for \(\Delta^{1}=\{p_{i},p_{j}\}\) is computed by excluding \(p_{j}\) from \(S\) when constructing the power cell 4. For example, when \(\Delta^{1}\) exists, \(R_{p_{i}|\Delta^{1}}\) will expand towards \(p_{j}\)'s direction (Figure 6(c)), and \(D_{\Delta^{1}}\) will "go through" \(R_{p_{i}|\Delta^{1}}\). In contrast, when \(\Delta\) doesn't exists, even though we have removed \(p_{j}\), \(R_{p_{i}|\Delta^{1}}\) will not expand (Figure 6(d)), and thus \(D_{\Delta^{1}}\) stays outside of \(R_{p_{i}|\Delta^{1}}\).

Footnote 4: In 3D case where \(k=2\) and \(\Delta^{2}=\{p_{i},p_{j},p_{k}\}\), \(p_{j}\) and \(p_{k}\) would be ignored for \(R_{p_{i}|\Delta^{2}}\).

Now we can define a signed distance field \(\tau_{pt}(x,R)\) for a given reduced power cell \(R\), where the signed distance is measured as the distance from the point \(x\in\mathbb{R}^{d}\) to the boundary of the reduced power cell (sign is positive when inside). Then, we can induce the signed distance between a dual form \(D\) and a reduced power cell \(R\):

\[\tau(D,R)=\max_{x\in D}\tau_{pt}(x,R). \tag{3}\]

As illustrate in Figure 6(c) and (d), \(\tau(D_{\Delta^{1}},R_{p_{i}|\Delta^{1}})\) is positive when \(\Delta^{1}\) exists, while negative when it does not exist in WDT. This observation can be generalized as follows:

**Remark 3.1**.: \(\Delta^{k}\) exists in WDT if and only if \(\forall p_{i}\in\Delta^{k},\tau(D_{\Delta^{k}},R_{p_{i}|\Delta^{k}})>0\).

In fact, the sign of every \(\tau(D_{\Delta^{k}},R_{p_{i}|\Delta^{k}})\) is same for every \(p_{i}\in\Delta^{k}\). Therefore, we can use its average to measure the existence probability of \(\Delta_{k}\), along with sigmoid function \(\sigma\):

\[\Lambda_{wdt}(\Delta^{k})=\frac{1}{k+1}\sum_{p\in\{\Delta^{k}\}}\sigma(\tau(D _{\Delta_{k}},R_{p|\Delta^{k}})\cdot\alpha_{wdt}), \tag{4}\]

where \(\alpha_{wdt}\) is a constant value used for the sigmoid function. \(\Lambda_{wdt}(\Delta^{k})\) is greater than 0.5 when \(\Delta^{k}\) exists, aligning with our probabilistic viewpoint and being differentiable.

Figure 6: **Basic concepts to compute existence probability of given \(1\)-simplex (\(\Delta^{1}\)) when \(d=2\). (a): WDT and PD of given set of weighted vertices are rendered in solid and dotted lines. The size of a vertex represents its weight. (b): Power cell of \(p_{1}\) (\(C_{p_{1}}\)) is rendered in grey. Also, \(\Delta^{1}\) is rendered in black line, of which dual line (\(D_{\Delta^{1}}\)) is rendered in red. (c), (d): For given \(\Delta^{1}\), reduced power cell of \(p_{1}\) for the \(\Delta^{1}\) (\(R_{p_{1}|\Delta^{1}}\)) is rendered in blue, with the original power cell (grey). We can evaluate the existence of \(\Delta^{1}\) in WDT by computing the signed distance from \(D_{\Delta^{1}}\) to \(R_{p_{1}|\Delta^{1}}\).**

[MISSING_PAGE_FAIL:6]

### Loss Functions

DMesh can be reconstructed from various inputs, such as normal meshes, point clouds, and multi-view images. With its per-vertex features and per-face existence probabilities \(\Lambda(F)\), we can optimize it with various reconstruction losses and regularization terms. Please see details in Appendix C.

#### 4.3.1 Reconstruction Loss (\(L_{recon}\))

First, if we have a normal mesh with vertices \(\mathbb{P}\) and faces \(\mathbb{F}\), and we want to represent it with DMesh, we should compute the additional two per-vertex attributes, WDT weights and real values. We optimize them by maximizing \(\Lambda(\mathbb{F})\) since these faces lies on the reference mesh. Conversely, for the remaining set of faces \(\mathbb{F}\) that can be defined on \(\mathbb{P}\), we should minimize \(\Lambda(\mathbb{F})\). Together, they define the reconstruction loss for mesh input (Appendix C.1).

For reconstruction from point clouds or multi-view images, we need to optimize for all features including positions. For point clouds, we define our loss using Chamfer Distance (CD) and compute the expected CD using our face probabilities (Appendix C.2). For multi-view images, we define the loss as the \(L_{1}\) loss between the given images and the rendering of DMesh, interpreting face probabilities as face opacities. We implemented efficient differentiable renderers to allow gradients to flow across face opacities (Appendix C.3).

#### 4.3.2 Regularizations

Being fully differentiable for both vertices and faces, DMesh allows us to develop various regularizations to improve the optimization process and enhance the final mesh quality. The first is **weight regularization** (\(L_{weight}\)), applied to the dual Power Diagram of the WDT (Appendix C.4). This regularization reduces the structural complexity of the WDT, controlling the final mesh complexity (Figure 7). The next is **real regularization** (\(L_{real}\)), which enforces nearby points to have similar real values and increases the real values of points adjacent to high real value points (Appendix C.5). This helps remove holes or inner structures and makes faces near the current surface more likely to be considered (Appendix D). The final regularization, **quality regularization** (\(L_{qual}\)), aims to improve the quality of triangle faces by minimizing the average expected aspect ratio of the faces, thus removing thin triangles (Appendix C.6).

To sum up, our final loss function can be written as follows:

\[L=L_{recon}+\lambda_{weight}\cdot L_{weight}+\lambda_{real}\cdot L_{real}+ \lambda_{qual}\cdot L_{qual},\]

where \(\lambda\) values are hyperparameters. In Appendix E, we provide values for these hyperparameters for every experiment. Also, in Appendix E.3, we present ablation studies for these regularizations.

## 5 Experiments and Applications

In this section, we provide experimental results to demonstrate the efficacy of our approach. First, we optimize vertex attributes to restore a given ground truth mesh, directly proving the differentiability of our design. Next, we conduct experiments on 3D reconstruction from point clouds and multi-view images, showcasing how our differentiable formulation can be used in downstream applications.

For the mesh reconstruction problem, we used three models from the Stanford 3D Scanning Repository (Curless und Levoy, 1996). For point cloud and multi-view reconstruction tasks, we used four closed-surface models from the Thingi32 dataset, four open-surface models from the DeepFashion3D dataset, and three additional models with both closed and open surfaces from the Obiverse dataset

\begin{table}
\begin{tabular}{c c c c} \hline - & Bunny & Dragon & Buddha \\ \hline RE & 99.78\% & 99.72\% & 99.64\% \\ FP & 0.00\% & 0.55\% & 0.84\% \\ \hline \end{tabular}
\end{table}
Table 1: Mesh reconstruction results.

Figure 7: **Results with different \(\lambda_{weight*}\)**and Adobe Stock. These models are categorized as "closed," "open," and "mixed" in this section. Additionally, we use nonconvex polyhedra of various Euler characteristics and non-orientable geometries to prove our method's versatility.

We implemented our main algorithm for computing face existence probabilites and differentiable renderer used for multi-view image reconstruction in CUDA (Nickolls u. a., 2008). Since we need to compute WDT before running the CUDA algorithm, we used WDT implementation of CGAL (Jamin u. a., 2023). We implemented the rest of logic with Pytorch (Paszke u. a., 2017). All of the experiments were run on a system with AMD EPYC 7R32 CPU and Nvidia A10 GPU.

### Mesh to DMesh

In this experiment, we demonstrate that we can preserve most of the faces in the original normal triangular mesh after converting it to DMesh using the mesh reconstruction loss introduced in 4.3.

In Table 1, we show the recovery ratio (RE) and false positive ratio (FP) of faces in our reconstructed mesh. Note that we could recover over 99% of faces in the original mesh, while only having under 1% of false faces. Please see Appendix E.1 for more details. This result successfully validates our differentiable formulation, but also reveals its limitation in reconstructing some abnormal triangles in the original mesh, such as long, thin triangles.

### Point Cloud & Multi-View Reconstruction

In this experiment, we aim to reconstruct a mesh from partial geometric data, such as (oriented) point clouds or multi-view images. For point cloud reconstruction, we sampled 100K points from the ground truth mesh. We can additionally use point orientations, if they are available. For multi-view reconstruction, we rendered diffuse and depth images of the ground truth mesh from 64 view points.

\begin{table}
\begin{tabular}{l|l|l|l|l|l|l|l|l|l}  & Methods & CD(\(\times 10^{-5}\))\(\downarrow\) & F\(\uparrow\) & N\(\uparrow\) & ECD\(\downarrow\) & EF\(\uparrow\) & \# Verts\(\downarrow\) & \# Faces\(\downarrow\) & Time (sec)\(\downarrow\) \\ \hline \hline \multirow{4}{*}{PC} & PSR & 690 & 0.770 & 0.931 & 0.209 & 0.129 & 159K & 319K & 10.6 \\ \cline{2-10}  & VORMesh & \(\downarrow\)1K & 0.671 & 0.819 & \(\downarrow\)1K & 0.263 & 121K & 524K & 12.2 \\ \cline{2-10}  & NVIDE & 3.611 & 0.874 & 0.936 & **0.022** & 0.421 & 20.7K & 42.8K & **5.49** \\ \cline{2-10}  & Ours (w/o normal) & 3.726 & 0.866 & 0.936 & 0.067 & 0.342 & 3.87K & 10.4K & 775 \\ \cline{2-10}  & Ours (w/ normal) & **3.364** & **0.886** & **0.952** & 0.141 & **0.438** & **3.56** & **7.54K** & 743 \\ \hline \multirow{4}{*}{MV} & NIE & 555 & 0.439 & 0.548 & 0.064 & 0.023 & 74.5K & 149K & 6696 \\ \cline{2-10}  & FlexCube & 273 & 0.591 & 0.881 & **0.939** & **0.152** & 10.9K & 21.9K & **56.47** \\ \cline{1-1} \cline{2-10}  & Ours & **34.6** & **0.685** & **0.892** & 0.094 & 0.113 & **4.19K** & **8.80K** & 1434 \\ \end{tabular}
\end{table}
Table 2: **Quantitative comparison for point cloud and multi-view reconstruction results.** Best results are written in bold.

Figure 8: **Point cloud and multi-view reconstruction results.****(a)**: Ground truth mesh. **(b), (f)**: Our method restores the original shape without losing much detail. **(c), (d), (g), (h)**: PSR (Kazhdan und Hoppe, 2013), VoroMesh (Maruani u. a., 2023), FlexiCube (Shen u. a., 2023), and NIE (Mehta u. a., 2022) fail for open and mixed surfaces. **(e)**: NDC (Chen u. a., 2022b) exhibits artifacts from grids.

In Appendix E, we illustrated the example inputs for these experiments. Also, please see Appendix D to see the initialization and densification strategy we took in these experiments.

To validate our approach, we compare our results with various approaches. When it comes to point cloud reconstruction, we first compare our result with classical Screened Poisson Surface Reconstruction (PSR) method (Kazhdan und Hoppe, 2013) 6. Then, to compare our method with optimization based approach, we use recent VoroMesh (Maruani u. a., 2023) method. Note that these two methods are not tailored for open surfaces. To compare our method also for the open surfaces, we use Neural Dual Contouring (NDC) (Chen u. a., 2022b), even though it is learning-based approach. Finally, for multi-view reconstruction task, we compare our results with Flexicube (Shen u. a., 2023) and Neural Implicit Evolution (NIE) (Mehta u. a., 2022), which correspond to volumetric approaches that can directly produce meshes of varying geometric topology for given visual inputs.

Footnote 6: We provide point orientations for PSR, which is optional for our method.

In Figure 8, we visualize the reconstruction results along with the ground truth mesh for qualitative evaluation. For closed meshes, in general, volumetric approaches like PSR, VoroMesh, and Flexicube, capture fine details better than our methods. This is mainly because we currently have limitation in the mesh resolution that we can produce with our method. NIE, which is also based on volumetric principles, generates overly smoothed reconstruction results. However, when it comes to open or mixed mesh models, which are more ubiquitous in real applications, we can observe that these methods fail, usually with false internal structures or self-intersecting faces (Appendix E.2). Since NDC leverages unsigned information, it can handle these cases without much problem as ours. However, we can observe step-like visual artifacts coming from its usage of grid in the final output, which requires post-processing. Additionally, to show the versatility of our representation, we also visualize various shapes reconstructed from oriented point clouds in Figure 2.

Table 2 presents quantitative comparisons with other methods. We used following metrics from Chen u. a. (2022b) to measure reconstruction accuracy: Chamfer Distance (CD), F-Score (F1), Normal Consistency (NC), Edge Chamfer Distance (ECD), and Edge F-Score (EF1) to the ground truth mesh. Also, we report number of vertices and faces of the reconstructed mesh to compare mesh complexity, along with computational time. All values are average over 11 models that we used. In general, our method generates mesh of comparable, or better accuracy than the other methods. However, when it comes to ECD and EF1, which evaluate the edge quality of the reconstructed mesh, our results showed some weaknesses, because our method cannot prevent non-manifold edges yet. However, our method showed superior results in terms of mesh complexity - this is partially due to the use of weight regularization. Please see Appendix E.3 to see how the regularization works through ablation studies. Likewise, our method shows promising results in producing compact and accurate mesh. However, we also note that our method requires more computational cost than the other methods in the current implementation.

Before moving on, we present an experimental result about shape interpolation using DMesh in Figure 9. We used multi-view images to reconstruct a torus first, and then optimized the DMesh again

Figure 9: (\(\rightarrow\)) **Shape interpolation using DMesh exhibiting topology change.** After fitting DMesh to a torus (upper left), we optimize it again to reconstruct a double torus (lower right), which has a different genus. We use multi-view images for the optimization.

to fit a double torus. The results show that DMesh effectively reconstructs the double torus, even when initialized from a converged single torus, highlighting the method's robustness to local minima. However, this also indicates that our representation lacks meaningful shape interpolation, as it does not assume any specific shape topology.

## 6 Conclusion

### Limitations

As shown above, our method achieves a more effective and complete forms of differentiable meshes of various topology than existing methods, but still has several limitations to overcome.

**Computational Cost.** Currently, the resolution of DMesh is limited by computational cost. Although our theoretical relaxation and CUDA implementation reduce this burden, processing meshes with over 100K vertices remains challenging due to the computational bottleneck of constructing the WDT at each optimization step. In Figure 10, we analyze computational costs relative to the number of points. As shown, costs rise sharply beyond 20K points, with WDT construction consuming most of the time. This limits our method's ability to handle high resolution mesh.

**Non-Manifoldness.** As we have claimed so far, DMesh shows much better generalization than the other methods as it does not have any constraints on the shape topology and mesh connectivity. However, due to this relaxation of constraint, we can observe spurious non-manifold errors in the mesh, even though we adopted measures to minimize them (Appendix D.2.7).

Specifically, an edge must have at most two adjacent faces to be a "manifold" edge. Similarly, a "manifold" vertex should be adjacent to a set of faces that form a closed or open fan. We refer to edges or vertices that do not satisfy these definitions as "non-manifold." In our results, we found that 5.50% of edges and 0.38% of vertices were non-manifold for point cloud reconstruction. For multi-view reconstruction, 6.62% of edges and 0.25% of vertices were non-manifold. Therefore, we conclude that non-manifold edges are more prevalent than non-manifold vertices in our approach.

### Future Work

To address the computational cost issue, we can explore methods that reduce reliance on the WDT algorithm, as its cost increases significantly with the number of points. This is crucial since representing complex shapes with fine details often requires over 100K vertices. To tackle the non-manifold issue, we could integrate approaches based on (un)signed distance fields (Shen u. a., 2023; Liu u. a., 2023b) into our method, ensuring manifold mesh generation. Finally, future research could extend this work to solve other challenging problems, such as 3D reconstruction from real-world images, or applications like generative models for 3D shapes. This could involve encoding color or texture information within our framework, opening up exciting new directions for exploration.

Figure 10: **Analysis of computational cost for computing face existence probabilities (\(\Lambda(F)\)).** The computational cost rises sharply beyond 20K points, with most of the time spent on WDT construction (“WDT”), while the probability computation (“Prob”) requires significantly less time.

AcknowledgementsWe thank Zhiqin Chen and Matthew Fisher for helpful advice. This research is a joint collaboration between Adobe and University of Maryland at College Park. This work has been supported in part by Adobe, IARPA, UMD-ARL Cooperate Agreement, and Dr. Barry Mersky and Capital One Endowed E-Novate Professorships.

## References

* [Amenta u. a. 1998a] Amenta, Nina ; Bern, Marshall ; Eppstein, David: The crust and the \(\beta\)-skeleton: Combinatorial curve reconstruction. In: _Graphical models and image processing_ 60 (1998), Nr. 2, S. 125-135
* [Amenta u. a. 1998b] Amenta, Nina ; Bern, Marshall ; Kamvysselis, Manolis: A new Voronoi-based surface reconstruction algorithm. In: _Proceedings of the 25th annual conference on Computer graphics and interactive techniques_, 1998, S. 415-421
* [Aurenhammer u. a. 2013] Aurenhammer, Franz ; Klein, Rolf ; Lee, Der-Tsai: _Voronoi diagrams and Delaunay triangulations_. World Scientific Publishing Company, 2013
* [Brandt und Algazi 1992] Brandt, Jonathan W. ; Algazi, V R.: Continuous skeleton computation by Voronoi diagram. In: _CVGIP: Image understanding_ 55 (1992), Nr. 3, S. 329-338
* [Chen u. a. 2022a] Chen, Anpei ; Xu, Zexiang ; Geiger, Andreas ; Yu, Jingyi ; Su, Hao: TensoRF: Tensorial Radiance Fields. In: _European Conference on Computer Vision (ECCV)_, 2022
* [Chen u. a. 2023] Chen, Anpei ; Xu, Zexiang ; Wei, Xinyue ; Tang, Siyu ; Su, Hao ; Geiger, Andreas: Dictionary Fields: Learning a Neural Basis Decomposition. In: _ACM Trans. Graph._ (2023)
* [Chen u. a. 2019] Chen, Wenzheng ; Ling, Huan ; Gao, Jun ; Smith, Edward ; Lehtinen, Jaakko ; Jacobson, Alec ; Fidler, Sanja: Learning to predict 3d objects with an interpolation-based differentiable renderer. In: _Advances in neural information processing systems_ 32 (2019)
* [Chen u. a. 2022b] Chen, Zhiqin ; Tagliasacchi, Andrea ; Funkhouser, Thomas ; Zhang, Hao: Neural dual contouring. In: _ACM Transactions on Graphics (TOG)_ 41 (2022), Nr. 4, S. 1-13
* [Chen u. a. 2020] Chen, Zhiqin ; Tagliasacchi, Andrea ; Zhang, Hao: Bsp-net: Generating compact meshes via binary space partitioning. In: _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2020, S. 45-54
* [Cheng u. a. 2013] Cheng, Siu-Wing ; Dey, Tamal K. ; Shewchuk, Jonathan ; Sahni, Sartaj: _Delaunay mesh generation_. CRC Press Boca Raton, 2013
* [Cignoni u. a. 2008] Cignoni, Paolo ; Callieri, Marco ; Corsini, Massimiliano ; Dellepiane, Matteo ; Ganovelli, Fabio ; Ranzuglia, Guido u. a.: Meshlab: an open-source mesh processing tool. In: _Eurographics Italian chapter conference_ Bd. 2008 Salerno, Italy (Veranst.), 2008, S. 129-136
* [Curless und Levoy 1996] Curless, Brian ; Levoy, Marc: A volumetric method for building complex models from range images. In: _Proceedings of the 23rd annual conference on Computer graphics and interactive techniques_, 1996, S. 303-312
* [Guillard u. a. 2021] Guillard, Benoit ; Remelli, Edoardo ; Lukoianov, Artem ; Richter, Stephan R. ; Bagautdinov, Timur ; Baque, Pierre ; Fua, Pascal: DeepMesh: Differentiable iso-surface extraction. In: _arXiv preprint arXiv:2106.11795_ (2021)
* [Hanocka u. a. 2019] Hanocka, Rana ; Hertz, Amir ; Fish, Noa ; Giryes, Raja ; Fleishman, Shachar ; Cohen-Or, Daniel: Meshcnn: a network with an edge. In: _ACM Transactions on Graphics (ToG)_ 38 (2019), Nr. 4, S. 1-12
* URL [https://doc.cgal.org/5.6/Manual/packages.html#PkgTriangulation3](https://doc.cgal.org/5.6/Manual/packages.html#PkgTriangulation3)
* [Ju u. a. 2002]Ju, Tao ; Losasso, Frank ; Schaefer, Scott ; Warren, Joe: Dual contouring of hermite data. In: _Proceedings of the 29th annual conference on Computer graphics and interactive techniques_, 2002, S. 339-346
* [Kazhdan und Hoppe 2013]Kazhdan, Michael ; Hoppe, Hugues: Screened poisson surface reconstruction. In: _ACM Transactions on Graphics (ToG) 32_ (2013), Nr. 3, S. 1-13
* [Kerbl u. a. 2023]Kerbl, Bernhard ; Kopanas, Georgios ; Leimkuhler, Thomas ; Drettakis, George: 3D Gaussian Splatting for Real-Time Radiance Field Rendering. In: _ACM Transactions on Graphics_ 42 (2023), Nr. 4
* [Kingma und Ba 2014]Kingma, Diederik P. ; Ba, Jimmy: Adam: A method for stochastic optimization. In: _arXiv preprint arXiv:1412.6980_ (2014)
* [Laine u. a. 2020]Laine, Samuli ; Hellsten, Janne ; Karras, Tero ; Seol, Yeongho ; Lehtinen, Jaakko ; Aila, Timo: Modular primitives for high-performance differentiable rendering. In: _ACM Transactions on Graphics (TOG) 39_ (2020), Nr. 6, S. 1-14
* [Lee 2010]Lee, John: _Introduction to topological manifolds_. Bd. 202. Springer Science & Business Media, 2010
* [Liao u. a. 2018]Liao, Yiyi ; Donne, Simon ; Geiger, Andreas: Deep marching cubes: Learning explicit surface representations. In: _Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition_, 2018, S. 2916-2925
* [Liu u. a. 2020]Liu, Lingjie ; Gu, Jiatao ; Lin, Kyaw Z. ; Chua, Tat-Seng ; Theobalt, Christian: Neural Sparse Voxel Fields. In: _NeurIPS_ (2020)
* [Liu u. a. 2019]Liu, Shichen ; Li, Tianye ; Chen, Weikai ; Li, Hao: Soft rasterizer: A differentiable renderer for image-based 3d reasoning. In: _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2019, S. 7708-7717
* [Liu u. a. 2023a]Liu, Yu-Tao ; Wang, Li ; Yang, Jie ; Chen, Weikai ; Meng, Xiaoxu ; Yang, Bo ; Gao, Lin: Neudf: Leaning neural unsigned distance fields with volume rendering. In: _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2023, S. 237-247
* [Liu u. a. 2023b]Liu, Zhen ; Feng, Yao ; Xiu, Yuliang ; Liu, Weiyang ; Paull, Liam ; Black, Michael J. ; Scholkopf, Bernhard: Ghost on the Shell: An Expressive Representation of General 3D Shapes. In: _arXiv preprint arXiv:2310.15168_ (2023)
* [Long u. a. 2023]Long, Xiaoxiao ; Lin, Cheng ; Liu, Lingjie ; Liu, Yuan ; Wang, Peng ; Theobalt, Christian ; Komura, Taku ; Wang, Wenping: Neuraludf: Learning unsigned distance fields for multi-view reconstruction of surfaces with arbitrary topologies. In: _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2023, S. 20834-20843
* [Lorensen und Cline 1998]Lorensen, William E. ; Cline, Harvey E.: Marching cubes: A high resolution 3D surface construction algorithm. In: _Seminal graphics: pioneering efforts that shaped the field_. 1998, S. 347-353
* [Maruani u. a. 2023]Maruani, Nissim ; Klokov, Roman ; Ovsjanikov, Maks ; Alliez, Pierre ; Desbrun, Mathieu: VoroMesh: Learning Watertight Surface Meshes with Voronoi Diagrams. In: _Proceedings of the IEEE/CVF International Conference on Computer Vision_, 2023, S. 14565-14574
* [Mehta u. a. 2022]Mehta, Ishit ; Chandraker, Manmohan ; Ramamoorthi, Ravi: A level set theory for neural implicit evolution under explicit flows. In: _European Conference on Computer Vision_ Springer (Veranst.), 2022, S. 711-729
* [Mildenhall u. a. 2021]Mildenhall, Ben ; Srinivasan, Pratul P. ; Tancik, Matthew ; Barron, Jonathan T. ; Ramamoorthi, Ravi ; Ng, Ren: Nerf: Representing scenes as neural radiance fields for view synthesis. In: _Communications of the ACM_ 65 (2021), Nr. 1, S. 99-106* [Munkberg, Jacob, Hasselgren, Jon, Shen, Tianchang, Gao, Jun, Chen, Wenzheng, Evans, Alex, Muller, Thomas, Fidler, Sanja: Extracting triangular 3d models, materials, and lighting from images. In: _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, 2022, S. 8280-8290
* [Nickolls, John, Buck, Ian, Garland, Michael, Skadron, Kevin: Scalable parallel programming with cuda: Is cuda the parallel programming model that application developers have been waiting for? In: _Queue_ 6 (2008), Nr. 2, S. 40-53
* [Nicolet, Baptiste, Jacobson, Alec, Jakob, Wenzel: Large steps in inverse rendering of geometry. In: _ACM Transactions on Graphics (TOG)_ 40 (2021), Nr. 6, S. 1-13
* [Oechsle, Michael, Peng, Songyou, Geiger, Andreas: UNISURF: Unifying Neural Implicit Surfaces and Radiance Fields for Multi-View Reconstruction. In: _International Conference on Computer Vision (ICCV)_, 2021
* [Palfinger2022]Palfinger, Werner: Continuous remeshing for inverse rendering. In: _Computer Animation and Virtual Worlds_ 33 (2022), Nr. 5, S. e2101
* [Paszke, Adam, Gross, Sam, Chintala, Soumith, Chanan, Gregory ; Yang, Edward ; DeVito, Zachary ; Lin, Zeming ; Desmaison, Alban ; Antiga, Luca ; Lerer, Adam: Automatic differentiation in pytorch. (2017)
* [Rakotosaona, Marie-Julie, Aigerman, Noam, Mitra, Niloy, J. ; Ovsjanikov, Maks ; Guerrero, Paul: Differentiable surface triangulation. In: _ACM Transactions on Graphics (TOG)_ 40 (2021), Nr. 6, S. 1-13
* [Shen, Tianchang, Gao, Jun ; Yin, Kangxue ; Liu, Ming-Yu ; Fidler, Sanja: Deep marching tetrahedra: a hybrid representation for high-resolution 3d shape synthesis. In: _Advances in Neural Information Processing Systems_ 34 (2021), S. 6087-6101
* [Shen, Tianchang, Munkberg, Jacob, Hasselgren, Jon, Yin, Kangxue ; Wang, Zian ; Chen, Wenzheng ; Gojcic, Zan ; Fidler, Sanja ; Sharp, Nicholas ; Gao, Jun: Flexible isosurface extraction for gradient-based mesh optimization. In: _ACM Transactions on Graphics (TOG)_ 42 (2023), Nr. 4, S. 1-16
* [Wang, Peng, Liu, Lingjie, Liu, Yuan ; Theobalt, Christian ; Komura, Taku ; Wang, Wenping: Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. In: _arXiv preprint arXiv:2106.10689_ (2021)
* [Wang, Yiming, Han, Qin, Habermann, Marc, Daniilidis, Kostas ; Theobalt, Christian ; Liu, Lingjie: NeuS2: Fast Learning of Neural Implicit Surfaces for Multi-view Reconstruction. In: _Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)_, 2023
* [Wei, A. a. 2023]Wei, Xinyue ; Xiang, Fanbo ; Bi, Sai ; Chen, Anpei ; Sunkavalli, Kalyan ; Xu, Zexiang ; Su, Hao: NeuManifold: Neural Watertight Manifold Reconstruction with Efficient and High-Quality Rendering Support. In: _arXiv preprint arXiv:2305.17134_ (2023)
* [Yariv, A. a. 2021]Yariv, Lior ; Gu, Jiatao ; Kasten, Yoni ; Lipman, Yaron: Volume rendering of neural implicit surfaces. In: _Thirty-Fifth Conference on Neural Information Processing Systems_, 2021
* [Yariv, A. a. 2020]Yariv, Lior ; Kasten, Yoni ; Moran, Dror ; Galun, Meirav ; Atzmon, Matan ; Ronen, Basri ; Lipman, Yaron: Multiview Neural Surface Reconstruction by Disentangling Geometry and Appearance. In: _Advances in Neural Information Processing Systems_ 33 (2020)
* [Zhang, Kai, Riegler, Gernot ; Snavely, Noah ; Koltun, Vladlen: NeRF++: Analyzing and Improving Neural Radiance Fields. In: _arXiv:2010.07492_ (2020)
* [Zhou, Yao, Yue, Chenglei, Li, Zimo ; Cao, Chen ; Ye, Yuting ; Saragih, Jason ; Li, Hao ; Sheikh, Yaser: Fully convolutional mesh autoencoder using efficient spatially varying kernels. In: _Advances in neural information processing systems_ 33 (2020), S. 9251-9262

## Appendix A Comparison to Other Shape Reconstruction Methods

Here we provide conceptual comparisons between our approach and the other optimization-based 3D reconstruction algorithms, which use different shape representations. To be specific, we compared our method with mesh optimization methods starting from template mesh (Palfinger, 2022; Nicolet u. a., 2021), methods based on neural signed distance fields (SDF) (Wang u. a., 2021, 2023), methods based on neural unsigned distance fields (UDF) (Liu u. a., 2023a; Long u. a., 2023), and methods based on differentiable isosurface extraction (Shen u. a., 2021; Munkberg u. a., 2022; Shen u. a., 2023b). We used following criteria to compare these methods.

* Closed surface: Whether or not the given method can reconstruct, or represent closed surfaces.
* Open surface: Whether or not the given method can reconstruct, or represent open surfaces.
* Differentiable Meshing: Whether or not the given method can produce gradients from the loss computed on the final mesh.
* Differentiable Rendering: Whether or not the given method can produce gradients from the loss computed on the rendering results.
* Geometric topology: Whether or not the given method can change geometric topology of the shape. Here, geometric topology defines the continuous deformation of Euclidean subspaces (Lee, 2010). For instance, genus of the shape is one of the traits that describe geometric topology.
* Mesh topology: Whether or note the given method can produce gradients from the loss computed on the mesh topology, which denotes the structural configuration, or edge connectivity of a mesh.
* Manifoldness: Whether or not the given method guarantees manifold mesh.

In Table 3, we present a comparative analysis of different methods. Note that our method meets all criteria, only except manifoldness. It is partially because our method does not assume volume, which is the same for methods based on neural UDF. However, because our method does not leverage smoothness prior of neural network like those methods, it could exhibit high frequency noises in the final mesh. Because of this reason, we gave \(\triangle\) to the neural UDF methods, while giving X to our approach. When it comes to methods based on differentiable isosurface extraction algorithms, we gave \(\triangle\) to its ability to handle open surfaces, because of (Liu u. a., 2023b). They can represent open surfaces as subset of closed ones, but cannot handle non-orientable open surfaces. Finally, note that our method is currently the only method that can handle mesh topology.

Likewise, DMesh shows promise in addressing the shortcomings found in previous research. Nonetheless, it has its own set of limitations (Section 6). Identifying and addressing these limitations is crucial for unlocking the full potential of our method.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c} Methods & Closed & Open & Diff. Mesh & Diff. Render. & Geo. Topo. & Mesh Topo. & Manifold \\ \hline \hline Template Mesh & O & O & O & O & X & X & O \\ \hline Neural SDF & O & X & X & O & O & X & O \\ \hline Neural UDF & O & O & X & O & O & X & \(\triangle\) \\ \hline Diff. Isosurface & O & \(\triangle\) & O & O & O & X & O \\ \hline
**DMesh (Ours)** & O & O & O & O & O & O & X \\ \end{tabular}
\end{table}
Table 3: Traits of different optimization-based shape reconstruction methods. We compare methods based on template mesh (Palfinger, 2022; Nicolet u. a., 2021), neural SDF (Wang u. a., 2021, 2023), neural UDF (Long u. a., 2023; Liu u. a., 2023a), differentiable isosurface extraction techniques (Shen u. a., 2021; Munkberg u. a., 2022; Shen u. a., 2023; Liu u. a., 2023b) with ours.

Details about Section 3.2

### Mathematical Definitions

Here, we provide formal mathematical definitions of the terms used in Section 3.2. We mainly use notations from Aurenhammer u. a. (2013) and Cheng u. a. (2013).

Generalizing the notations in Section 3.2, let \(S[w]\) be a finite set of weighted points in \(\mathbb{R}^{d}\), where \(w\) is a weight assignment that maps each point \(p\in S\) to its weight \(w_{p}\). We denote a weighted point as \(p[w_{p}]\) and define power distance to measure distance between two weighted points.

**Definition B.1** (Power distance).: Power distance between two weighted points \(p[w_{p}]\) and \(q[w_{q}]\) is measured as:

\[\pi(p[w_{p}],q[w_{q}])=d(p,q)^{2}-w_{p}-w_{q}, \tag{8}\]

where \(d(p,q)\) is the Euclidean distance.

Note that an unweighted point is regarded as carrying weight of \(0\). Based on this power distance, we can define the power cell \(C_{p}\) of a point \(p[w_{p}]\) as the set of unweighted points in \(\mathbb{R}^{d}\) that are closer to \(p[w_{p}]\) than to any other weighted point in \(S[w]\).

**Definition B.2** (Power cell).: Power cell of a point \(p[w_{p}]\in S[w]\) is defined as:

\[C_{p}=\{x\in\mathbb{R}^{d}\,|\,\forall q[w_{q}]\in S[w],\pi(x,p[w_{p}])\leq\pi( x,q[w_{q}])\}. \tag{9}\]

Note that some points may have empty power cells if their weights are relatively smaller than neighboring points. We call them "submerged" points. As we will see later, weight regularization aims at submerging unnecessary points in mesh definition, which leads to mesh simplification.

To construct the power cell, we can use the concept of half space. A half space \(H_{p<q}\) is the set of unweighted points in \(\mathbb{R}^{d}\) that are closer to \(p[w_{p}]\) than \(q[w_{q}]\).

**Definition B.3** (Half space).: Half space \(H_{p<q}\) is defined as:

\[H_{p<q}=\{x\in\mathbb{R}^{d}\,|\,\pi(x,p[w_{p}])\leq\pi(x,q[w_{q}])\}. \tag{10}\]

Note that we can construct a power cell by intersecting half spaces, which proves the convexity of the power cell. Now we call \(H(p,q)\) as a half plane that divides \(\mathbb{R}^{d}\) into two half spaces, \(H_{p<q}\) and \(H_{q<p}\).

**Definition B.4** (Half plane).: Half plane \(H_{p,q}\) is defined as:

\[H_{p,q}=\{x\in\mathbb{R}^{d}\,|\,\pi(x,p[w_{p}])=\pi(x,q[w_{q}])\}. \tag{11}\]

Then, for a given \(k\)-simplex \(\Delta^{k}\) comprised of weighted points \(\{\Delta^{k}\}=\{p_{1}[w_{p_{1}}],\ldots,p_{k+1}[w_{p_{k+1}}]\}\subset S[w]\), the dual structure \(D_{\Delta^{k}}\) is the intersection of half planes between the points in \(\{\Delta^{k}\}\), which is a convex set.

**Definition B.5** (Dual form).: Dual form \(D_{\Delta^{k}}\) of \(\Delta^{k}=\{p_{1}[w_{p_{1}}],\ldots,p_{k+1}[w_{p_{k+1}}]\}\) is defined as:

\[D_{\Delta^{k}}=\bigcap_{i,j=1,\ldots,k+1}H(p_{i},p_{j}), \tag{12}\]

which is equivalent to:

\[D_{\Delta^{k}}=\{x\in\mathbb{R}^{d}\,|\,\forall p_{i}[w_{p_{i}}],p_{j}[w_{p_{j }}]\in S[w],\pi(x,p_{i}[w_{p_{i}}])=\pi(x,p_{j}[w_{p_{j}}])\}. \tag{13}\]

Note than when \(k=d\), \(D_{\Delta^{k}}\) becomes a point, while it becomes a line when \(k=d-1\). As discussed in Section 3.2, we leverage the distance between this dual form \(D_{\Delta^{k}}\) and reduced power cell (Rakotosaona u. a., 2021) of the points in \(D_{\Delta^{k}}\) to query the existence of \(\Delta^{k}\). The reduced power cell \(R_{p|\Delta^{k}}\) is a power cell of \(p\) that does not concern the other points in \(\Delta^{k}\) in its construction.

**Definition B.6** (Reduced power cell).: Reduced power cell of a weighted point \(p[w]\in S[w]\) for given \(\Delta^{k}\) is defined as:

\[R_{p|\Delta^{k}}=\{x\in\mathbb{R}^{d}\,|\,\forall q[w_{q}]\in S[w]-\{\Delta^{k }\},\pi(x,p[w_{p}])\leq\pi(x,q[w_{q}])\}. \tag{14}\]

Using these concepts, we can measure the existence probability of a \(k\)-simplex, as provided in Section 3.2.

### Analysis of previous approach

#### b.2.1 Theoretical Aspect

As mentioned in Section 3.2, the previous approach of Rakotosaona u. a. (2021) has two computational challenges in computational efficiency and precision. These limitations are mainly rooted in not knowing the power diagram structure before evaluating Eq. 3. We summarize the overall procedure of the previous approach, and point out how our method is different from it.

Collecting simplexes to evaluateFirst, we need to collect simplexes that we want to compute probabilities for. Here we assume the number of simplexes increases linearly (\(O(N)\)) as the number of points (\(N\)) increases. This is a plausible assumption, because we often search for \(k\)-nearest neighbors for each point, and combine them to generate the query simplexes.

Sampling a point on the dual forms of the simplexesThe previous method relies on point projections to evaluate Eq. 3. This did not incur any problem for their case (\(d=k=2\)), because the dual form was a single point. However, when \((k<d)\) as in our cases, the dual form contains infinite number of points, which makes unclear how to apply this point-based approach. One possible solution is sampling the most "representative" point on the dual form, and leveraging the point to estimate Eq. 3. By definition, this estimation is lower bound of Eq. 3. However, the problem arises when this sample point does not give reliable result. For instance, we can consider the case shown in Figure 6(c). In the illustration, we can observe that \(\Delta^{1}\) exists in WDT, and thus \(D_{\Delta^{1}}\) goes through \(R_{p_{1}|D_{\Delta^{1}}}\). If we sample a point on \(D_{\Delta^{1}}\) that is included in \(R_{p_{1}|D_{\Delta^{1}}}\), the signed distance from the sample point would have same sign as Eq. 3. However, if the sample point is selected outside \(R_{p_{1}|D_{\Delta^{1}}}\), the sign would be different from the real value. In this case, even if \(\Delta^{k}\) exists, we can recognize it as not existing. Note that this false estimation produces false gradients, which could undermine optimization process.

In contrast, we do not have to concern about this precision issue, because we construct PD, which tells us good sample points that give reliable lower bounds of Eq. 3, when the given simplex exists in WDT. Otherwise, we explicitly compute minimum distance between the dual form and the reduced power cell, as discussed in Section 4.1.

Projecting sample points to reduced power cellsThe final step is point projection, where we project the sample points from dual forms to the reduced power cells to estimate Eq. 3. Based on the definitions in Appendix B, we can observe that a (reduced) power cell's boundaries are comprised of half planes. That is, the boundaries of \(C_{p}\) is comprised of multiple half planes between \(p\) and the other weighted points. However, when we do not know which half planes comprise the boundaries, we have to do exhaustive search to find the signed distance from the sample point to the boundaries of the reduced power cell. As the number of half planes that are associated with a point \(p\) is \(N\), the computational cost to precisely compute the signed distance is \(O(N)\).

Note that it does not hold for our case, because by constructing WDT and PD, we know which half planes form the boundaries of each power cell. Also, note that even when the number of points increases, the average number of half planes that comprise the boundaries of power cells remains constant. Therefore, in our case, this step requires only \(O(1)\) computational cost.

SummaryTo sum up, the computational cost of the previous approach amounts to \(O(N^{2})\), as the number of simplexes to evaluate increases linearly, and the cost for the projection step also increases linearly as the number of points increase. However, the cost for ours remains at \(O(N)\). Moreover, the previous approach does not guarantee satisfactory estimations of Eq. 3.

Before moving on, we point out that the original implementation limited the number of half planes to consider in evaluating Eq. 3 to reduce the computational cost to \(O(N)\). This relaxation is permissible to the case where the precision is not very important. However, the precision is important in our case, because we aim at representing mesh accurately.

#### b.2.2 Experimental Results

To prove the aforementioned theoretical claim, we conducted experiments to measure the computational speed and accuracy of the probability estimation for \((d=2,k=2)\) and \((d=2,k=1)\) cases, for varying number of points. We randomly sampled points in a unit cube uniformly, and set the

[MISSING_PAGE_FAIL:17]

second term is not fully optimized, the resulting DMesh might include faces absent in the ground truth mesh, leading to a higher false positive ratio (Section 5.1). Refer to Appendix D.1 for details on how this reconstruction loss is integrated into the overall optimization process.

### Point Cloud Reconstruction

In the task of point cloud reconstruction, we reconstruct the mesh by minimizing the (\(L_{1}\)-norm based) expected Chamfer Distance (CD) between the given point cloud (\(\mathbb{P}_{gt}\)) and the sample points (\(\mathbb{P}_{ours}\)) from our reconstructed mesh. We denote the CD from \(\mathbb{P}_{gt}\) to \(\mathbb{P}_{ours}\) as \(CD_{gt}\), and the CD from \(\mathbb{P}_{ours}\) to \(\mathbb{P}_{gt}\) as \(CD_{ours}\). The final reconstruction loss is obtained by combining these two distances.

\[L_{recon}=CD_{gt}+CD_{ours}. \tag{16}\]

#### c.2.1 Sampling \(\mathbb{P}_{ours}\)

To compute these terms, we start by sampling \(\mathbb{P}_{ours}\) from our current mesh. First, we sample a set of faces that we will sample points from. We consider the areas of the triangular faces and their existence probabilities. To be specific, we define \(\eta(F)\) for a face \(F\) as

\[\bar{\eta}(F)=\Lambda(F),\quad\eta(F)=F_{area}\cdot\bar{\eta}(F),\]

and define a probability to sample \(F\) from the entires faces \(\mathbb{F}\) as

\[P_{sample}(F)=\frac{\eta(F)}{\sum_{F^{\prime}\in\mathbb{P}}\eta(F^{\prime})}.\]

We sample \(N\) faces from \(\mathbb{F}\) with replacement and then uniformly sample a single point from each selected face to define \(\mathbb{P}_{ours}\). In our experiments, we set \(N\) to \(100K\).

In this formulation, we sample more points from faces with a larger area and higher existence probability to improve sampling efficiency. However, we observed that despite these measures, the sampling efficiency remains low, leading to slow convergence. This issue arises because, during optimization, there is an excessive number of faces with very low existence probability.

To overcome this limitation, we decided to do stratified sampling based on point-wise real values and cull out faces with very low existence probabilities. To be specific, we define two different \(\eta\) functions:

\[\bar{\eta_{1}}(F) =\Lambda_{wdt}(F)\cdot\min(\psi_{i},\psi_{j},\psi_{k}), \eta_{1}(F)=F_{area}\cdot\bar{\eta_{1}}(F)\] \[\bar{\eta_{2}}(F) =\Lambda_{wdt}(F)\cdot\max(\psi_{i},\psi_{j},\psi_{k}), \eta_{2}(F)=F_{area}\cdot\bar{\eta_{2}}(F)\]

where \((\psi_{i},\psi_{j},\psi_{k})\) are the real values of the points that comprise \(F\). Note that \(\eta_{1}\) is the same as \(\eta\)7.

Footnote 7: We do not use differentiable min operator, as we do not require differentiability in the sampling process.

For the faces in \(\mathbb{F}\), we first calculate the \(\bar{\eta_{1}}\) and \(\bar{\eta_{2}}\) values and eliminate faces with values lower than a predefined threshold \(\epsilon_{\eta}\). We denote the set of remaining faces as \(\mathbb{F}_{1}\) and \(\mathbb{F}_{2}\). Subsequently, we sample \(\frac{N}{2}\) faces from \(\mathbb{F}_{1}\) and the other \(\frac{N}{2}\) faces from \(\mathbb{F}_{2}\), using the following two sampling probabilities:

\[P_{sample,1}(F)=\frac{\eta_{1}(F)}{\sum_{F^{\prime}\in\mathbb{F}_{1}}\eta_{1}( F^{\prime})},\quad P_{sample,2}(F)=\frac{\eta_{2}(F)}{\sum_{F^{\prime}\in \mathbb{F}_{2}}\eta_{2}(F^{\prime})}.\]

The rationale behind this sampling strategy is to prioritize (non-existing) faces closer to the current mesh over those further away. In the original \(\eta=\eta_{1}\) function, we focus solely on the minimum real value, leading to a higher sampling rate for existing faces. However, to remove holes in the current mesh, it's beneficial to sample more points from potential faces--those not yet existing but connected to existing ones. This approach, using \(\eta_{2}\), enhances reconstruction results by removing holes more effectively. Yet, there's substantial potential to refine this importance sampling technique, as we haven't conducted a theoretical analysis in this study.

Moreover, when sampling a point from a face, we record the face's existence probability alongside the point. Additionally, if necessary, we obtain and store the face's normal. For a point \(\mathbf{p}\in\mathbb{P}_{ours}\)

[MISSING_PAGE_FAIL:19]

### Multi-View Reconstruction

When we are given multi-view images, we reconstruct the mesh by minimizing the \(L_{1}\) difference between our rendered images and the given images. In this work, we mainly use both diffuse and depth renderings to reconstruct the mesh.

If we denote the (\(N_{img}\)) ground truth images of \(N_{pixel}\) number of pixels as \(\mathcal{I}_{i}^{gt}(i=1,...,N_{img})\), and our rendered images as \(\mathcal{I}_{i}^{ours}\), we can write the reconstruction loss function as

\[L_{recon}=\frac{1}{N_{img}\cdot N_{pixel}}\sum_{i=1,...,N_{img}}||\mathcal{I}_{ i}^{gt}-\mathcal{I}_{i}^{ours}||.\]

Then, we can define our rendered image as follows:

\[I_{i}^{ours}=\mathcal{F}(\mathbb{P},\mathbb{F},\Lambda(\mathbb{F}),\mathbf{M} \mathbf{V_{i}},\mathbf{P_{i}}).\]

where \(\mathcal{F}\) is a differentiable renderer that renders the scene for the given points \(\mathbb{P}\), faces \(\mathbb{F}\), face existence probabilities \(\Lambda(\mathbb{F})\), \(i\)-th modelview matrix \(\mathbf{M}\mathbf{V_{i}}\in\mathbb{R}^{4\times 4}\), and \(i\)-th projection matrix \(\mathbf{P_{i}}\in\mathbb{R}^{4\times 4}\). The differentiable renderer \(\mathcal{F}\) has to backpropagate gradients along \(\mathbb{P}\), \(\mathbb{F}\), and \(\Lambda(\mathbb{F})\) to update our point attributes. Specifically, here we interpret \(\Lambda(\mathbb{F})\) as opacity for faces to use in the rendering process. This is because opacity means the probability that a ray stops when it hits the face, which aligns with our face existence probability well. For this reason, we ignore faces with very low existence probability under some threshold to accelerate the reconstruction, as they are almost transparent and do not contribute to the rendering a lot.

To implement \(\mathcal{F}\), we looked through previous works dedicated for differentiable rendering (Laine u. a., 2020; Liu u. a., 2019). However, we discovered that these methods incur substantial computational costs when rendering a large number of (potentially) semi-transparent triangles, as is the case in our scenario. Consequently, we developed two efficient, partially differentiable renderers that meet our specific requirements. These renderers fulfill distinct roles within our pipeline--as detailed in Appendix D, our optimization process encompasses two phases within a single epoch. The first renderer is employed during the initial phase, while the second renderer is utilized in the subsequent phase.

#### c.3.1 \(\mathcal{F}_{A}\)

If there are multiple semi-transparent faces in the scene, we have to sort the faces that covers a target pixel with their (view-space) depth values, and iterate through them until the accumulated transmittance is saturated to determine the color for the pixel. Conducting this process for each individual pixel is not only costly, but also requires a lot of memory to store information for backward pass.

Recently, 3D Gaussian Splatting (Kerbl u. a., 2023) overcame this issue with tile-based rasterizer. We adopted this approach, and modified their implementation to render triangular faces, instead of gaussian splats. To briefly introduce its pipeline, it first assigns face-wise depth value by computing the view-space depth of its center point. Then, after subdividing the entire screen into \(16\times 16\) tiles, we assign faces to each tiles if they overlap. After that, by using the combination of tile ID and the face-wise depth as a key, we get the face list sorted by depth value in each tile. Finally, for each tile, we iterate through the sorted faces and determine color and depth for each pixel as follows.

\[C=\sum_{i=1,...,k}T_{i}\cdot\alpha_{i}\cdot C_{i},\quad(T_{i}=\Pi_{j=1,...,i-1 }(1-\alpha_{j})),\]

where \(T_{i}\) is the accumulated transmittance, \(\alpha_{i}\) is the opacity of the \(i\)-th face, and \(C_{i}\) is the color (or depth) of the \(i\)-th face. Note that \(\alpha_{i}=\Lambda(F_{i})\), as mentioned above.

Figure 11: \(\mathcal{F}_{B}\) uses tessellation structure to efficiently render overlapped faces in the correct order.

Even though this renderer admits an efficient rendering of large number of semi-transparent faces, there are still two large limitations in the current implementation. First, the current implementation does not produce visibility-related gradients (near face edges) to update point attributes. Therefore, we argue that this renderer is partially differentiable, rather than fully differentiable. Next, since it does not compute precise view-point depth for each pixel, its rendering result can be misleading for some cases, as pointed out in (Kerbl u. a., 2023).

To amend the first issue, we opt to use another differentiable renderer of Laine u. a. (2020), which produces the visibility-related gradients that we lack. Since this renderer cannot render (large number of) transparent faces as ours does, we only render the faces with opacity larger than \(0.5\). Also, we set the faces to be fully opaque. If we call this renderer as \(\mathcal{F}_{A^{\prime}}\), our final rendered image can be written as follows.

\[\mathcal{I}_{i}^{ours}=\frac{1}{2}(\mathcal{F}_{A}(\mathbb{P},\mathbb{F}, \Lambda(\mathbb{F}),\mathbf{MV}_{i},\mathbf{P}_{i})+\mathcal{F}_{A^{\prime}}( \mathbb{P},\mathbb{F},\Lambda(\mathbb{F}),\mathbf{MV}_{i},\mathbf{P}_{i})).\]

In Figure 12, we illustrate rendered images from \(\mathcal{F}_{A}\) and \(\mathcal{F}_{A^{\prime}}\).

Acknowledging that this formulation is not theoretically correct, we believe that it is an intriguing future work to implement a fully differentiable renderer that works for our case. However, we empirically found out that we can reconstruct a wide variety of meshes with current formulation without much difficulty.

As mentioned before, this renderer is used at the first phase of the optimization process, where all of the point attributes are updated. However, in the second phase, we fix the point positions and weights, and only update point-wise real values (Appendix D.2). In this case, we can leverage the tessellation structure to implement an efficient differentiable renderer. As the second renderer does a precise depth testing unlike the first one, it can be used to modify the errors incurred by the second limitation of the first renderer (Figure 13).

#### c.3.2 \(\mathcal{F}_{B}\)

The second renderer performs precise depth ordering in an efficient way, based on the fixed tessellation structure that we have. In Figure 11, we illustrate a 2D diagram that explains our approach. When the green ray, which corresponds to a single ray to determine the color of a single pixel, goes through the tessellation, we can observe that it goes through a sequence of triangles (tetrahedron in 3D), which are denoted as \(T_{1},T_{2},\) and \(T_{3}\). When the ray enters a triangle \(T_{i}\) through one of its three edges, we can see that it moves onto the other adjacent triangle \(T_{i+1}\) only through one of the other edges of \(T_{i}\), because of compact tessellation. Therefore, when the ray hits one edge of \(T_{i}\), it can only examine the other two edges of \(T_{i}\) to find the next edge it hits. Note that we do not have to do depth testing explicitly in this approach. Also, unlike the first approach, this renderer does not have to store all the possible faces that a ray collides for the backward pass, because it can iterate the same process in the opposite way in the backward pass to find the edge that it hit before the last edge. If we only store the last edge that each hits at the forward pass, we can start from the last edge and find the previous edges that it hit to compute gradients. Therefore, this second renderer requires much less memory

Figure 12: Rendered images from two differentiable renderers, \(\mathcal{F}_{A}\) and \(\mathcal{F}_{A^{\prime}}\). Left and right image corresponds to diffuse and depth rendering, respectively. (a) \(\mathcal{F}_{A}\) is our (partially) differentiable renderer based on tile-based approach. (b) Since \(\mathcal{F}_{A}\) does not produce visibility-related gradients, we additionally use \(\mathcal{F}_{A^{\prime}}\)(Laine u. a., 2020) to render images and integrate with ours.

than the first one, and also performs precise depth testing naturally. However, note that this renderer is also partillly differentiable, because it cannot update point positions and weights.

To sum up, we implemented two partially differentiable renderers to solve multi-view reconstruction problem with DMesh. They serve different objectives in our reconstruction process, and we empirically found out that they are powerful enough to reconstruct target meshes in our experiments. However, we expect that we can simplify the process and improve its stability, if we can implement a fully differentiable renderer that satisfy our needs. We leave it as a future work.

### Weight Regularization

Weight regularization aims at reducing the complexity of WDT, which supports our mesh. By using this regularization, we can discard unnecessary points that do not contribute to representing our mesh. Moreover, we can reduce the number of points on the mesh, if they are redundant, which ends up in the mesh simplification effect (Appendix E.3).

We formulate the complexity of WDT as the sum of edge lengths in its dual power diagram. Formally, we can write the regularization as follows,

\[L_{weight}=\sum_{i=1,\dots,N}Length(E_{i}),\]

where \(E_{i}\) are the edges in the dual power diagram, and \(N\) is the number of edges.

### Real Regularization

Real regularization is a regularization that is used for maintaining the real values of the connected points in WDT as similar as possible. Also, we leverage this regularization to make real values of points that are connected to the points with high real values to become higher, so that they can be considered in reconstruction more often than the points that are not connected to those points. To be specific, note that we ignore faces with very low existence probability in the reconstruction process. By using this regularization, it can remove holes more effectively.

Figure 13: Reconstructed mesh from multi-view images, rendered in MeshLab’s (Cignoni u. a., 2008) x-ray mode to see inner structure. In multi-view reconstruction, we divide each epoch in two phases. (a) After the first phase ends, where we do inaccurate depth testing, lots of false inner faces are created. (b) To remove these inner faces, we require a renderer that does the exact depth testing, which we use in the second phase. Also see Appendix D.2 for details about post-processing step to remove the inner structure.

[MISSING_PAGE_EMPTY:23]

Note that during optimization, we allow only small perturbations to the positions of initial points, and fix weights and real values of them to \(1\). This is because we already know that these points correspond to the ground truth mesh vertices, and thus should be included in the final mesh without much positional difference. In our experiments, we set the perturbation bound as \(1\%\) of the model size.

However, we notice that we cannot restore the mesh connectivity with only small perturbations to the initial point positions, if there are no additional points that can aid the process. Therefore, we periodically perform point insertion to add additional points, which is described below.

#### d.1.2 Point Insertion

The point insertion is a subroutine to add additional points to the current point configurations. It is performed periodically, at every fixed step. The additional points are placed at the random place on the faces in \(\bar{\mathbb{F}}\), which correspond to the faces that should not exist in the final mesh. Therefore, these additional points can aid removing these undesirable faces.

However, we found out that inserting a point for every face in \(\bar{\mathbb{F}}\) can be quite expensive. Therefore, we use \(k\)-means clustering algorithm to aggregate them into \(0.1\cdot N_{F}\) clusters, where \(N_{F}\) is the number of faces in \(\bar{\mathbb{F}}\), to add the centroids of the clusters to our running point set. On top of that, we select \(1000\) random faces in \(\bar{\mathbb{F}}\) to put additional points directly on them. This is because there are cases where centroids are not placed on the good positions where they can remove the undesirable faces.

In Figure 14, we render DMesh after point insertion to the initialized mesh. Note that some of the undesirable faces disappear because of the added points.

Figure 14: **Intermediate results in converting bunny model to DMesh.** For given ground truth mesh in (a), we initialize our point attributes using the mesh vertices. (b) Then, the initial mesh becomes convex hull of the original mesh. (c) To remove undesirable faces that were not in the original mesh, we insert additional points on the undesirable faces. Then, some of them disappear because of the inserted points. (d) After optimizing 5000 steps, just before another point insertion, DMesh recovers most of the ground truth connectivity.

```
\(T\leftarrow\) Observation (Point cloud, Multi-view images) \(\mathbb{P},\mathbb{W},\psi\leftarrow\) Initialize point attributes for DMesh (using T if possible) \(\mathbb{F}\leftarrow\) Empty set of faces whileepoch not endeddo \(\mathbb{P},\mathbb{W},\psi\leftarrow\) (If not first epoch) Initialize point attributes with sample points from current DMesh, for mesh refinement // Phase 1 whilestep not endeddo \(WDT,PD\leftarrow\) Run WDT algorithm with \(\mathbb{P},\mathbb{W}\) \(\mathbb{F}\leftarrow\) Update faces to evaluate existence probability for, with \(WDT\) \(\Lambda(\mathbb{F})\leftarrow\) Compute existence probability for faces in \(\mathbb{F}\), with \(\mathbb{P},\psi,WDT,PD\) \(L_{recon}\leftarrow\) Compute reconstruction loss, with \(\mathbb{P},\mathbb{F},\Lambda(\mathbb{F}),T\) \(L_{weight}\leftarrow\) Compute weight regularization, with \(PD\) \(L_{real}\leftarrow\) Compute real regularization, with \(\mathbb{P},\psi,WDT\) \(L_{qual}\leftarrow\) Compute quality regularization, with \(\mathbb{P},\mathbb{F},\Lambda(\mathbb{F})\) \(L\gets L_{recon}+\lambda_{weight}\cdot L_{weight}+\lambda_{real}\cdot L_{ real}+\lambda_{qual}\cdot L_{qual}\)  Update \(\mathbb{P},\mathbb{W},\psi\) to minimize \(L\) end while // Phase 2 \(WDT,PD\leftarrow\) Run WDT algorithm with \(\mathbb{P},\mathbb{W}\) \(\mathbb{F}\leftarrow\) Faces in \(WDT\) \(\Lambda_{wdt}(\mathbb{F})\gets 1\) whilestep not endeddo \(\Lambda(\mathbb{F})\leftarrow\) Compute existence probability for \(\mathbb{F}\), with \(\mathbb{P},\psi,\Lambda_{wdt}(\mathbb{F})\) \(L_{recon}\leftarrow\) Compute reconstruction loss, with \(\mathbb{P},\mathbb{F},\Lambda(\mathbb{F}),T\) \(L_{real}\leftarrow\) Compute real regularization, with \(\mathbb{P},\psi,WDT\) \(L\gets L_{recon}+\lambda_{real}\cdot L_{real}\)  Update \(\psi\) to minimize \(L\)  end while  end while \(M\leftarrow\) Get final mesh from DMesh, after post-processing
```

**Algorithm 2** Point cloud & Multi-view Reconstruction

#### d.1.3 Maintaining \(\mathbb{\bar{F}}\)

In this problem, we minimize the reconstruction loss specified in Eq. 15 to restore the connectivity in the ground truth mesh, and remove faces that do not exist in it. In the formulation, we denoted the faces that are comprised of mesh vertices \(\mathbb{P}\), but are not included in the original mesh as \(\mathbb{\bar{F}}\). Even though we can enumerate all of them, the total number of faces in \(\mathbb{\bar{F}}\) mounts to \(O(N^{3})\), where \(N\) is the number of mesh vertices. Therefore, rather than evaluating all of those cases, we maintain a set of faces \(\mathbb{\bar{F}}\) that we should exclude in our mesh during optimization.

To be specific, at each iteration, we find faces in the current WDT that are comprised of points in \(\mathbb{P}\), but do not exist in \(\mathbb{F}\), and add them to the running set of faces \(\mathbb{\bar{F}}\). On top of that, at every pre-defined number of iterations, in our case 10 steps, we compute \(k\)-nearest neighboring points for each point in \(\mathbb{P}\). Then, we find faces that can be generated by combining each point with 2 of its \(k\)-nearest points, following Rakotosaona u. a. (2021). Then, we add the face combinations that do not belong to \(\mathbb{F}\) to \(\mathbb{\bar{F}}\). In our experiments, we set \(k=8\).

### Point cloud & Multi-view Reconstruction

In Algorithm 2, we describe the overall algorithm that is used for point cloud and multi-view reconstruction tasks. We explain each step in detail below.

#### d.2.1 Two Phase Optimization

We divide each optimization epoch in two phases. In the first phase (phase 1), we optimize all of the point attributes - positions, weights, and real values. However, in the second phase (phase 2), we fix the point positions and weights, and only optimize the real values.

This design aims at removing ambiguity in our differentiable formulation. That is, even though we desire face existence probabilities to converge to either \(0\) and \(1\), those probabilities can converge to the values in between. To alleviate this ambiguity, after the first phase ends, we fix the tessellation to make \(\Lambda_{wdt}\) for each face in \(\mathbb{F}\) to either \(0\) or \(1\). Therefore, in the second phase, we only care about the faces that exist in current \(WDT\), which have \(\Lambda_{wdt}\) value of \(1\). Then, we can only care about real values.

Note that the two differentiable renderers that we introduced in Appendix C.3 are designed to serve for these two phases, respectively.

#### d.2.2 Point Initialization with Sample Points

In this work, we propose two point initialization methods. The first initialization method can be used when we have sample points near the target geometry in hand.

This initialization method is based on an observation that the vertices of Voronoi diagram of a point set tend to lie on the medial axis of the target geometry (Amenta u. a., 1998a,b). Therefore, for the given sample point set \(\mathbb{P}_{sample}\), we first build Voronoi diagram of it, and find Voronoi vertices \(\mathbb{P}_{voronoi}\). Then, we merge them to initialize our point set \(\mathbb{P}\):

\[\mathbb{P}=\mathbb{P}_{sample}\cup\mathbb{P}_{voronoi},\]

all of which weights are initialized to 1. Then, we set the real values (\(\psi\)) of points in \(\mathbb{P}_{sample}\) as \(1\), while setting those of points in \(\mathbb{P}_{voronoi}\) as \(0\).

In Figure 15, we render the mesh that we can get from this initialization method, when we use \(10K\) sample points. Note that the initial mesh has a lot of holes, because there could be Voronoi vertices that are located near the mesh surface, as pointed out by (Amenta u. a., 1998b). However, we can converge to the target mesh faster than the initialization method that we discuss below, because most of the points that we need are already located near the target geometry.

#### d.2.3 Point Initialization without Sample Points

If there is no sample point that we can use to initialize our points, we initialize our points with \(N^{3}\) points regularly distributed on a grid structure that encompasses the domain, all of which has weight \(1\) and \(\psi\) value of \(1\). We set \(N=20\) for every experiment (Figure 15(a)). Then, we optimize the mesh to retrieve a coarse form of the target geometry (Figure 15(b)). Note that we need to refine this mesh in the subsequent epochs, as explained below.

Figure 15: **Initialized DMesh using sample points from ground truth mesh.** (a) From ground truth mesh, we uniformly sample \(10K\) points to initialize DMesh. (b) In the left figure, sample points from the ground truth mesh (\(\mathbb{P}_{sample}\)) are rendered in red. The points that correspond to \(\mathbb{P}_{voronoi}\) are rendered in blue. In the right figure, we render the initial mesh we can get from the points, which has a lot of holes.

#### d.2.4 Point Initialization for Different Inputs

Until now, we introduced two point initialization techniques. When the input is a point cloud, we sample subset of the point cloud to initialize our mesh (Figure 15). However, when the input is multi-view images, we start from initialization without sample points (Figure 16), because there is no sample point cloud that we can make use of.

#### d.2.5 Maintaining \(\mathbb{F}\)

We maintain the running set of faces to evaluate probability existence for in \(\mathbb{F}\). At each iteration, after we get \(WDT\), we insert every face in \(WDT\) to \(\mathbb{F}\), as it has a high possibility to persist in the subsequent optimization steps. Also, as we did int mesh to DMesh conversion (Appendix D.1), at every 10 optimization step, we find \(k\)-nearest neighbors for each point, and form face combinations based on them. Then, we add them to \(\mathbb{F}\).

Figure 16: **Optimization process for multi-view reconstruction for Plant model. At each row, we present the initial state (left) and the last state (right) of each epoch. For each figure, the left rendering shows the point attributes color coded based on real values, while the right one shows the extracted mesh. (a), (b) In the first epoch, we initialize DMesh without sample points. At the end of each epoch, we sample points from the current mesh, and use them for initialization in the next epoch.**

#### d.2.6 Mesh Refinement

At start of each epoch, if it is not the first epoch, we refine our mesh by increasing the number of points. To elaborate, we refine our mesh by sampling \(N\) number of points on the current DMesh, and then initialize point attributes using those sample points as we explained above. We increase \(N\) as number of epoch increases. For instance, in our multi-view reconstruction experiments, we set the number of epochs as \(4\), and set \(N=(1K,3K,10K)\) for the epochs excluding the first one. In Figure 16, we render the initial and the last state of DMesh of each epoch. Note that the mesh complexity increases and becomes more accurate as epoch proceeds, because we use more points. Therefore, this approach can be regarded as a coarse-to-fine approach.

#### d.2.7 Post-Processing

When it comes to multi-view reconstruction, we found out that it is helpful to add one more constraint in defining the face existence. In our formulation, in general, a face \(F\) has two tetrahedra (\(T_{1},T_{2}\)) that are adjacent to each other over the face. Then, we call the remaining point of \(T_{1}\) and \(T_{2}\) that is not included in \(F\) as \(P_{1}\) and \(P_{2}\). Our new constraint requires at least one of \(P_{1}\) and \(P_{2}\) to have \(\psi\) value of \(0\) to let \(F\) exist.

This additional constraint was inspired by the fact that \(F\) is not visible from outside if \(F\) exists in our original formulation, and both of \(P_{1}\) and \(P_{2}\) have \(\psi\) value of \(1\). That is, if it is not visible from outside, we do not recognize its existence. This constraint was also adopted to accommodate our real regularization, which increases the real value of points near surface. If this regularization makes the real value of points inside the closed surface, they would end up in internal faces that are invisible from outside. Because of this invisibility, our loss function cannot generate a signal to remove them. In the end, we can expect all of the faces inside a closed surface will exist, because of the absence of signal to remove them. Therefore, we choose to remove those internal faces by applying this new constraint in the post-processing step.

Note that this discussion is based on the assumption that our renderer does a precise depth testing. If it does not do the accurate depth testing, internal faces can be regarded as visible from outside, and thus get false gradient signal. In Figure 12(a), the final mesh after phase 1 is rendered, and we can see therer are lots of internal faces as the renderer used in phase 1 does not support precise depth testing. However, we can remove them with the other renderer in phase 2, as shown in Figure 12(b), which justifies our implementation of two different renderers.

Finally, we note that this constraint is not necessary for point cloud reconstruction, because if we minimize \(CD_{ours}\) in Appendix C.2, the internal faces will be removed automatically.

## Appendix E Experimental Details

In this section, we provide experimental details for the results in Section 5, and visual renderings of the our reconstructed mesh. Additionally, we provide the results of ablation studies about regularizations that we suggested in Section 4.3.

### Mesh to DMesh

As shown in Table 1, we reconstruct the ground truth connectivity of Bunny, Dragon, and Buddha model from Stanford dataset (Curless und Levoy, 1996). For all these experiments, we optimized for \(20K\) steps, and used an ADAM optimizer (Kingma und Ba, 2014) with learning rate of \(10^{-4}\). For Bunny model, we inserted additional points at every 5000 step. For the other models, we inserted them at every 2000 step.

In Figure 17, we provide the ground truth mesh and our reconstructed mesh. We can observe that most of the connectivity is preserved in our reconstruction, as suggested numerically in Table 1. However, note that the appearance of the reconstructed mesh can be slightly different from the ground truth mesh, because we allow \(1\%\) of positional perturbations to the mesh vertices.

### Point Cloud & Multi-view Reconstruction

#### e.2.1 Hyperparameters for Point Cloud Reconstruction

* Optimizer: ADAM Optimizer, Learning rate = \(10^{-4}\) for open surface meshes and two mixed surface meshes (Bigvegas, Raspberry) / \(3\cdot 10^{-4}\) for closed surface meshes, and one mixed surface mesh (Plant).
* Regularization: \(\lambda_{weight}=10^{-8},\lambda_{real}=10^{-3},\lambda_{qual}=10^{-3}\) for every mesh.
* Number of epochs: Single epoch for every mesh.
* Number of steps per epoch: 1000 steps for phase 1, 500 steps for phase 2 for every mesh.

#### e.2.2 Hyperparameters for Multi-view Reconstruction

* Optimizer: ADAM Optimizer, Learning rate = \(10^{-3}\) in the first epoch, and \(3\cdot 10^{-4}\) in the other epochs for every mesh.
* Weight Regularization: \(\lambda_{weight}=10^{-8}\) for every mesh.
* Real Regularization: \(\lambda_{real}=10^{-3}\) for the first 100 steps in every epoch for open surface meshes and one mixed surface mesh (Plant) / \(10^{-2}\) for the first 100 steps in every epoch for closed surface meshes and two mixed surface meshes (Bigvegas, Raspberry).
* Quality Regularization: \(\lambda_{qual}=10^{-3}\) for every mesh.
* Normal Coefficient: \(\lambda_{normal}=0\) for every mesh (Eq. 17).
* Number of epochs: 4 epochs for every mesh. In the first epoch, use \(20^{-3}\) regularly distributed points for initialization. In the subsequent epochs, sample \(1K,3K\), and \(10K\) points from the current mesh for initialization.

Figure 17: **Reconstruction results for mesh to DMesh experiment. From Left: Bunny, Dragon, and Buddha. We can observe that most of the edge connectivity is perserved in the reconstruction, even though the appearance is slightly different from the ground truth mesh because of small perturbations of vertex positions.*** Number of steps per epoch: 500 steps for phase 1, 500 steps for phase 2 for every mesh.
* Batch size: 64 for open surface meshes, 16 for the other meshes.

#### e.2.3 Visual Renderings

In Figure 22, 23, and 24, we provide visual renderings of our point cloud and multi-view reconstruction results with ground truth mesh. We also provide illustration of input point cloud and diffuse map. Note that we also used depth renderings for multi-view reconstruction experiments.

#### e.2.4 Additional Discussion

Generally, we can observe that reconstruction results from both point cloud and multi-view images capture the overall topology well. However, we noticed that the multi-view reconstruction results are not as good as point cloud reconstruction results. In particular, we can observe small holes in the multi-view reconstruction results. We assume that these artifacts are coming from relatively weaker supervision of multi-view images than dense point clouds. Also, we believe that we can improve these multi-view reconstruction results with more advanced differentiable renderer, and better mesh refinement strategy. In the current implementation, we lose connectivity information at the start of each epoch, which is undesirable. We believe that we can improve this approach by inserting points near the regions of interest, rather than resampling over entire mesh.

Figure 19: **Reconstruction results for the Plant model.** Flexicube (Shen u. a., 2023) can generate redundant, self-intersecting faces for open surfaces, in this case, leaves. To better capture the redundant faces, we rendered the models from upper side, which is shown in the bottom right figures.

Figure 18: **Reconstruction results for a closed surface model in Thingi32 dataset.** Flexicube (Shen u. a., 2023) can generate internal structures, while our approach removes them through post-processing.

Also, regarding comparison to Flexicube (Shen u. a., 2023) in Table 2, we tried to found out the reason why ours give better results than Flexicube in terms of CD to the ground truth mesh for closed surfaces in thingi32 dataset. We could observe that Flexicube's reconstruction results capture fine geometric details on the surface mesh, but also observed that they have lots of false internal structure (Figure 18). Note that this observation not only applies to closed surfaces, but also to open surfaces, where it generates lots of false, self-intersecting faces (Figure 19). Our results do not suffer from these problems, as we do post-processing (Appendix D.2) to remove inner structure, and also our method can represent open surfaces better than the volumetric approaches without self-intersecting faces.

### Ablation studies

In this section, we provide ablation studies for the regularizations that we proposed in Section 4.3. We tested the effect of the regularizations on the point cloud reconstruction task.

#### e.3.1 Weight Regularization

We tested the influence of weight regularzation in the final mesh, by choosing \(\lambda_{weight}\) in \((10^{-6},10^{-5},10^{-4})\). Note that we set the other experimental settings as same as described in Section E.2, except \(\lambda_{quality}\), which is set as 0, to exclude it from optimization.

In Table 5, we provide the quantitative results for the experiments. For different \(\lambda_{weight}\), we reconstructed mesh from point clouds, and computed average Chamfer Distance (CD) and average number of faces across every test data. We can observe that there exists a clear tradeoff between CD and mesh complexity. To be specific, when \(\lambda_{weight}=10^{-6}\), the CD is not very different from the results in Table 2, where we use \(\lambda_{weight}=10^{-8}\). However, when it increases to \(10^{-5}\) and \(10^{-4}\), we can observe that the mesh complexity (in terms of number of faces) decreases, but CD increases quickly.

Figure 20: **Point cloud reconstruction results with different \(\lambda_{weight}\). From Left: \(\lambda_{weight}=10^{-6},10^{-5},\) and \(10^{-4}\).**

The renderings in Figure 20 support these quantitative results. When \(\lambda_{weight}=10^{-6}\), we can observe good reconstruction quality. When \(\lambda_{weight}=10^{-5}\), there are small artifacts in the reconstruction, but we can get meshes of generally good quality with fewer number of faces. However, when it becomes \(10^{-4}\), the reconstruction results deteriorate, making holes and bumpy faces on the smooth surface. Therefore, we can conclude that weight regularization contributes to reducing the mesh complexity. However, we need to choose \(\lambda_{weight}\) carefully, so that it does not harm the reconstruction quality. The experimental results tell us setting \(\lambda_{weight}\) to \(10^{-6}\) could be a good choice to balance between these two contradictory objectives.

#### e.3.2 Quality Regularization

As we did in the previous section, we test the influence of quality regularization in the final mesh by selecting \(\lambda_{real}\) among \((10^{-4},10^{-3},10^{-2})\). We also set the other experimental settings as same as before, except \(\lambda_{weight}=0\).

In Table 6 and Figure 21, we present quantitative and qualitative comparisons between the reconstruction results. We provide statistics about average CD, average number of faces, and average aspect ratio of faces. Interestingly, unlike weight regularization, we could not observe tradeoff between CD and aspect ratio. Rather than that, we could find that CD decreases as aspect ratio gets smaller, and thus the triangle quality gets better.

We find the reason for this phenomenon in the increase of smaller, good quality triangle faces. Note that there is no significant difference between the number of faces between \(\lambda_{qual}=10^{-4}\) and \(10^{-3}\). Also, we cannot find big difference between visual renderings between them, even though the aspect

\begin{table}
\begin{tabular}{l|c|c|c|} \(\lambda_{qual}\) & \(10^{-4}\) & \(10^{-3}\) & \(10^{-2}\) \\ \hline CD & 7.60 & 7.42 & 7.28 \\ \hline Num. Face & 8266 & 8349 & 10806 \\ \hline Aspect Ratio & 2.33 & 2.06 & 1.55 \\ \end{tabular}
\end{table}
Table 6: Ablation study for quality regularization, quantitative results.

Figure 21: **Point cloud reconstruction results with different \(\lambda_{quality}\). From Left: \(\lambda_{real}=10^{-4},10^{-3}\), and \(10^{-2}\).**

[MISSING_PAGE_EMPTY:33]

Figure 22: **Point cloud and Multi-view Reconstruction results for open surface models.** From Left: Ground truth mesh, sample point cloud, point cloud reconstruction results, diffuse rendering, multi-view reconstruction results.

Figure 23: **Point cloud and Multi-view Reconstruction results for closed surface models.** From Left: Ground truth mesh, sample point cloud, point cloud reconstruction results, diffuse rendering, multi-view reconstruction results.

Figure 24: **Point cloud and Multi-view Reconstruction results for mixed surface models.** From Left: Ground truth mesh, sample point cloud, point cloud reconstruction results, diffuse rendering, multi-view reconstruction results.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In abstract and introduction, we described that we are presenting a differentiable mesh, and we are going to explore various aspects of it (e.g. computational cost, reconstruction task, regularization). They are accurately discussed across the entire paper, including Appendix. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We discussed the two limitations of our current approach in Section 6, which are about computational efficiency and manifoldness of the generated mesh. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs**Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [No] Justification: Even though we tried our best to describe every theoretical detail in main paper, especially in Section 3.2, and Appendix B, our method assumes that the reader has some background knowledge about the geoemtrical concepts. However, we specified the sources that the readers can refer to learn details about the theoretical claims we made in the paper. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: In Appendix D and E, we provided the pseudocode of our optimization process, and detailed hyperparameters to reproduce the experimental results. Also, we included code in the supplementary material. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset).

4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Excluding 2 models that we obtained from Adobe Stock, every model that we used is publicly available. Also, we submitted our code. However, due to the size limitation of the supplementary material, we could not submit the entire version of the code, because it depends on many external libraries. But we believe readers can compare the code and the paper to learn details of our paper. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide experimental details in Appendix E. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes]Justification: For the nature of our optimization based experiments, random initialization has little affect in the results. In addition, our experiments exam the effect of resolution and topology varieties, it does not use large amount of testing data. Thus, error bars for showing statistical significance of the experiments are not suitable for our paper. Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provided the machine that we used for running experiments in Section 5, and reported execution time in the experimental results. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: Our research conforms to the every guideline in the Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).

10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Our research is mainly about geometry, especially about triangle mesh formulation. So we believe it does not possess societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Our research does not deliver any trained model, but suggest a method for formulating differentiable mesh. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [No]Justification: We cited the source of assets that we used in experiments in Section 5, but did not include license information about them.

Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We used only existing assets. Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Our research does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained?Answer: [NA]

Justification: Our research does not involve crowdsourcing nor research with human subjects. Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.