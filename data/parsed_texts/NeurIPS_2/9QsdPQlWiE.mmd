# Test-time Training for Matching-based

Video Object Segmentation

Juliette Bertrand\({}^{*1,2}\) Giorgos Kordopatis-Zilos\({}^{*1}\) Yannis Kalantidis\({}^{2}\) Giorgos Tolias\({}^{1}\)

\({}^{1}\)VRG, FEE, Czech Technical University in Prague \({}^{2}\)NAVER LABS Europe

###### Abstract

The video object segmentation (VOS) task involves the segmentation of an object over time based on a single initial mask. Current state-of-the-art approaches use a memory of previously processed frames and rely on matching to estimate segmentation masks of subsequent frames. Lacking any adaptation mechanism, such methods are prone to test-time distribution shifts. This work focuses on matching-based VOS under distribution shifts such as video corruptions, stylization, and sim-to-real transfer. We explore test-time training strategies that are agnostic to the specific task as well as strategies that are designed specifically for VOS. This includes a variant based on mask cycle consistency tailored to matching-based VOS methods. The experimental results on common benchmarks demonstrate that the proposed test-time training yields significant improvements in performance. In particular for the sim-to-real scenario and despite using only a single test video, our approach manages to recover a substantial portion of the performance gain achieved through training on real videos. Additionally, we introduce DAVIS-C, an augmented version of the popular DAVIS test set, featuring extreme distribution shifts like image-/video-level corruptions and stylizations. Our results illustrate that test-time training enhances performance even in these challenging cases. Project page: [https://jbertrand89.github.io/test-time-training-vos/](https://jbertrand89.github.io/test-time-training-vos/)

Figure 1: **Test-time training significantly improves performance in the presence of distribution shifts for the task of video object segmentation (VOS). _Left:_ Example using a model trained on synthetic videos from BL-30K [8] and tested on a real video from DAVIS [38]. _Right:_ Example using a model trained on DAVIS [38] and YouTube-VOS [51] and tested on a cartoonized video from the corrupted DAVIS-C benchmark which we introduce in this work. Second-to-bottom row: Results obtained using the STCN [9] approach. Bottom row: Results after test-time training with the proposed _mask cycle consistency_ loss (\(tt\)-MCC) using the single ground-truth mask provided for the first video frame.**Introduction

In one-shot video object segmentation (VOS), we are provided with a single segmentation mask for one or more objects in the first frame of a video, which we need to segment across all video frames. It is a dense prediction task over time and space, and therefore, collecting training data is highly demanding. Early VOS methods design foreground/background segmentation models that operate on single-frame input and require a two-stage training process [4; 29]. During the off-line stage, the model is trained to segment a variety of foreground objects. During the on-line stage for a specific test video, the model is fine-tuned to adapt to the objects of interest indicated in the provided mask.

Recently, _matching-based_ methods like STCN [9] or XMem [6] have shown impressive performance on the common VOS benchmarks. These methods propagate the segmentation mask from a memory of previously predicted masks to the current frame. They only involve an off-line training stage where the model learns how to perform matching and propagation. Lacking any test-time adaptation mechanism, however, such methods are highly prone to test-time distribution shifts.

Our goal is to improve the generalization of matching-based VOS methods under distribution shifts by fine-tuning the model at test time through a single test video. Such one-shot adaptation is a form of _test-time training_ (TTT), a research direction that is lately attracting much attention in classification tasks and a promising way for generalizing to previously unseen distributions. We argue that TTT is a great fit for matching-based VOS, not only because of the additional temporal dimension to exploit, but also because we are given a ground-truth label for a frame of the test example.

In this work, we focus on two cases of test-time distribution shift for VOS: a) when training on synthetic data and testing on real data, also known as _sim-to-real transfer_, and b) when the test data contain image/video corruption or stylization. We explore three different ways of performing TTT: i) a task-agnostic approach that borrows from the image classification literature and performs entropy minimization [34; 46], ii) an approach tailored to the STCN architecture that employs an auto-encoder loss on the mask provided for the first frame and does not exploit any temporal information, and iii) a mask cycle consistency approach that is tailored to matching-based VOS, and utilizes temporal cycle consistency of segmentation masks to update the model. As we show, these different test-time training strategies do not work equally well, and tailoring TTT to the task and the methods is important and not trivial.

Our experiments demonstrate that some of the proposed approaches constitute a powerful way of generalizing to unseen distributions. When starting from a model trained only on synthetic videos, test-time training based on mask cycle consistency improves STCN by +22 and +11 points in terms of \(\mathcal{J}\&\mathcal{F}\) score on the two most popular VOS benchmarks, YouTube-VOS and DAVIS, respectively. What is even more impressive is that by simply using test-time training, we recover the bulk of the performance gain that training the original model on real videos brings: for YouTube-VOS and DAVIS, TTT recovers 82% and 72% of the performance gains, respectively, that training on real videos brings compared to synthetic ones, but _without requiring any video annotations during off-line training_.

We also study the performance of TTT in the presence of image-level or video-level corruptions and stylization. To that end, we follow a process similar to ImageNet-C [14] and create DAVIS-C, a version of the DAVIS test set with 14 corruptions and style changes at three strength levels. We evaluate models with and without TTT on DAVIS-C and analyze how performance changes as the distribution shift increases. Our results show that TTT significantly improves the performance of STCN by more than 8 points in terms of \(\mathcal{J}\&\mathcal{F}\) score for the hardest case of corruption/stylization. A qualitative example for both types of distribution shift is shown in Figure 1.

## 2 Related Work

Foreground/background models with online fine-tuning for VOS.Early approaches train a segmentation model to separate the foreground object from the background. The offline training stage is followed by an online supervised fine-tuning stage that uses the mask of the first frame for adapting the model [4; 29]. The fine-tuning process is improved by including pseudo-masks derived from highly confident predictions [44] or by additionally including hard-negative examples [48]. Maninis et al. [33] additionally incorporate semantic information about the underlined object categories via an external instance-aware semantic segmentation model. Other methods first produce a coarse segmentation result and then refine it by the guidance of optical flow [15], the appearance cue only [39], or temporal consistency [32]. Hu et al. [16] utilize an RNN model to exploit the long-term temporal structures, while others additionally incorporate optical flow information [16]. The heavy cost of fine-tuning is reduced by meta-learning in the work of Xiao et al. [49] or Meinhardt and Leal-Taixe [35].

Matching-based VOS.Most recent approaches build upon a matching-based prediction model, which is trained offline and neither requires nor incorporates any fine-tuning stage. The current test frame is matched to either the first annotated frame [52, 18, 5, 45, 3], a propagated mask [20, 17, 28], or a memory containing both [31, 36, 9]. Early methods use simple matching process [18] or are inspired by classical label propagation [60], while different methods improve the design of the matching [1, 9, 40, 53, 55, 54], the type and extent of the memory used [6], or exploit interactions between objects or frames [37, 56], among other aspects.

Hybrid and other VOS methods.Mao et al. [34] borrow from both families of approaches, _i.e_. matching-based and those that require online fine-tuning, and jointly integrate two models, but, unlike our work, the matching model is not updated at test time. Some methods differ from matching-based in the way they capture the evolution over time, _e.g_. by RNNs [43] or by spatio-temporal object-level attention [11].

Cycle consistency in VOS.Matching-based methods require dense video annotations for training, which is a limitation. To dispense with the need for mask annotations during training, cycle consistency of pixel or region correspondences over space and time has been successfully used by prior work [19, 24, 61]. This is an unsupervised loss that is also appropriate for test-time training. In our case, we tailor cycle consistency to the task of matching-based VOS, and propose a _supervised_ consistency loss that operates on masks by taking advantage of the provided mask at test-time in order to better adapt to the object of interest. Mask consistency based on the first frame is used by prior work during model training. Khoreva et al. [21] synthesize future frames using appearance and motion priors, while in our case, instead of synthesizing, we use mask predictions of the existing future video frames. Li et al. [26] and HODOR [1] use a temporal cycle consistency loss on a the first frame mask during training that allows learning from videos with a single segmentation mask. In contrast, we utilize the mask frame that is provided at test-time for the first frame of the test video and we are able to outperform HODOR and other state-of-the-art methods for multiple test-time distribution shifts.

Test-time training.A family of approaches adapts the test example to the training domain. Kim et al. [22] chose the appropriate image augmentation that maximizes the loss according to a loss prediction function, while Xiao et al. [50] updates the features of the test sample by energy minimization with stochastic gradient Langevin dynamics. Another family of approaches adapts the model to the test domain. Entropy minimization is a common way to update either batch-normalization statistics only [46] or the whole model [59]. Other self-supervised objectives include rotation prediction [42], contrastive learning [30], or spatial auto-encoding [12]. In our work, we move beyond the image domain and introduce mask cycle consistency as an objective to adapt the model specifically for video object segmentation applications. Azimi et al. [2] evaluate some of the aforementioned TTT techniques on top of self-supervised dense tracking methods under several distribution shifts on videos. Another recent work [57], concurrent to ours, also studies TTT in the video domain but for a classification task; we instead fully tailor the TTT on the VOS task and use temporal _mask_ consistency as our loss.

## 3 Test-time training for matching-based VOS

In this section, we first formulate the task of Video Object Segmentation (VOS) and briefly describe the basic components of the matching-based VOS methods we build on [9, 6]. We then present three ways for test-time training: a task-agnostic baseline based on entropy minimization that has been highly successful in other tasks and two VOS-specific variants that leverage the fact that we are provided with a ground-truth mask at test time; one using an auto-encoder loss and another a temporal cycle-consistency loss on the masks.

[MISSING_PAGE_FAIL:4]

equivalent to a cross-attention operation between the test frame features (queries) and the memory frame features (keys) that aggregates memory mask features (values). Finally, a mask decoder \(d_{m}\) is used to obtain the predicted mask \(\hat{m}_{j}=d_{m}([\hat{v}_{j},u_{j}])\), where \([\hat{v}_{j},u_{j}]\in\mathbb{R}^{W\times H\times(D_{e}+D_{m})}\) is the concatenation of the two representations along the depth dimension.

In this work, we assume that we have access to a matching-based VOS model, trained on real or synthetic data with mask annotations. These models do not generalize well to extreme distribution shifts not encountered during supervised training. Our goal is to improve their performance in such cases by fine-tuning the parameters of the model at test time.

### Test-time training

We explore three different losses to perform TTT for the case of matching-based VOS: (i) based on entropy minimization (\(tt\)_-Ent_), which forms a paradigm transferred from the image domain (image classification in particular) and is a task- and method-agnostic loss, (ii) based on an auto-encoder (\(tt\)_-AE_) that operates on segmentation masks, which is an approach tailored for the STCN architecture, (iii) based on mask cycle consistency (\(tt\)_-MCC_), which is more general and appropriate for a variety of matching-based methods. \(tt\)-MCC exploits the matching-based nature of STCN and includes mask propagation via a memory of masks, while the \(tt\)-AE variant does not include the matching step between multiple frames.

Given a test video, we optimize the parameters of the model using the provided mask and either only the first frame (for \(tt\)-AE) or all its frames (\(tt\)-Ent, \(tt\)-MCC). The test-time loss is optimized, the model parameters are updated, and the model is used to provide predictions for the specific test video. Then, the model is re-initialized back to its original parameters before another test video is processed.

For the two TTT variants that use multiple frames, _i.e_. \(tt\)-MCC and \(tt\)-Ent, we follow the process used during the off-line training phase of STCN and XMem and operate on randomly sampled frame triplets and octuplets, respectively. In the following, we describe the case of triplets which is extended to larger sequences in a straightforward way. All the triplets include the first video frame, while the second (third) triplet frame is uniformly sampled among the \(s\) frames that follow the first (second) triplet frame in the video. For triplet \(x_{0},x_{i},x_{j}\) with \(0<i<j\), the first predicted mask is given by \(\hat{m}_{i}=f(x_{i};\{(x_{0},m_{0})\})\), and the second by \(\hat{m}_{j}=f(x_{j};\{(x_{0},m_{0}),(x_{i},\hat{m}_{i})\})\), _i.e_. the second frame is added to the memory before predicting the last mask of the triplet. Multiple triplets are sampled during test-time training. The per-pixel losses are averaged to form the frame/mask loss. We denote the cross entropy loss between a ground-truth mask \(m\) and a predicted mask \(\hat{m}\) by \(\ell_{CE}(m,\hat{m})\).

#### 3.2.1 Entropy (\(tt\)-Ent) loss

We start from a task- and method-agnostic loss based on Entropy (\(tt\)-Ent) minimization, a common choice for test-time training on image classification [46]. In this case, we force the model to provide confident predictions for the triplets we randomly sample. The entropy of mask \(\hat{m}_{i}\) is denoted by \(H(\hat{m}_{i})\) and the loss for a particular triplet is \(L_{H}=0.5*(H(\hat{m}_{i})+H(\hat{m}_{j}))\), which is minimized over multiple triplets. Prior work on image classification avoids the trivial solution by using a batch of test examples [46] or augmenting the input example and minimizing the entropy over the average prediction of the augmented frames [59]. In the task of semantic segmentation, batches or augmentations are not required since it is a dense prediction task and we have multiple outputs to optimize, _i.e_. a prediction per pixel. In the case of VOS, we have even more outputs due to the temporal dimension and because we are sampling triplets among many frames. However, similar to Wang et al. [46], we observe that training only the batch normalization parameters is a requirement for \(tt\)-Ent to work.

#### 3.2.2 Mask auto-encoder (\(tt\)-AE) loss

The STCN model includes a mask encoder and a mask decoder to decode the mask representation coming from the cross-attention process. We repurpose these components for test-time training and compose an auto-encoder that reconstructs the input mask. The auto-encoder input is the provided ground-truth mask, _i.e_. \(m_{0}\), and the loss is given by \(L_{AE}=\ell_{CE}(m_{0},d_{m}([e_{m}(m_{0}),e_{x}(x_{0})]))\). Note that, together with the mask encoder that gets optimized to better represent the specific object shape, we also optimize the frame encoder with the \(tt\)-AE loss, since the mask representation is concatenated with the frame representation in the input of the mask encoder [9].

[MISSING_PAGE_FAIL:6]

## 4 Experiments

In this section, we first describe the training and implementation details. Then, we introduce the DAVIS-C benchmark and report our experimental results on four datasets and two distribution shifts: sim-to-real transfer and corrupted/stylized test data.

Initial models. We start from two publicly available models for STCN. STCN-BL30K is the model trained on synthetic data from BL-30K [8]. STCN-DY is a model initialized by STCN-BL30K and further trained using the training videos of DAVIS and YouTube-VOS, which include approximately 100K frames annotated with segmentation masks. We use the same two models for XMem, denoted by XMem -BL30K and XMem -DY, respectively.

Training details.We use learning rates \(10^{-5}\) and \(10^{-6}\) for models STCN-BL30K/ XMem -BL30K and STCN-DY/ XMem -DY, respectively since their training data differ significantly. Jump step \(s\) for sampling training frames is set to \(10\). For each test example, we train the models with \(tt\)-MCC and \(tt\)-Ent for 100 iterations and with \(tt\)-AE for 20, using the Adam [23] optimizer and a batch size of 4 sequences for STCN and 1 for XMem. We consider all target objects in the ground truth mask during TTT, and the raw videos are used with no augmentations. We develop our methods on top of the publicly available STCN2 and XMem3 implementations.

Footnote 2: [https://github.com/hkchengrex/STCN](https://github.com/hkchengrex/STCN)

Footnote 3: [https://github.com/hkchengrex/XMem](https://github.com/hkchengrex/XMem)

### Datasets and evaluation

We report results on the two most commonly used benchmarks for video object segmentation, the DAVIS-2017 validation set [38] and the YouTubeVOS-2018 validation set [51]. We further report results on the validation set of the recent MOSE [10] dataset, which includes challenging examples with heavy occlusion and crowded real-world scenes. Finally, we introduce DAVIS-C, a variant of the DAVIS test set that represents distribution shifts via corruptions and stylization. We evaluate all methods using the widely established \(\mathcal{J}\&\mathcal{F}\) measure similar to prior work [9; 6].

DAVIS-C: corrupted and stylized DAVIS.To model distribution shifts during the testing, we process the set of test videos of the original DAVIS dataset to create DAVIS-C. This newly created test set offers a test bed for measuring robustness to corrupted input and generalization to previously unseen appearance. It is the outcome of processing the original videos to introduce image-level and video-level corruption and image-level appearance modification. We perform 14 different transformations to each video, each one applied at three different strengths, namely _low_, _medium_, and _high_ strength, making it a total of 42 different variants of DAVIS.

Figure 4: **The 14 types of corruption in DAVIS-C. Corruptions are depicted at “medium” strength.**For the image-level corruptions, we follow the paradigm of ImageNet-C and adopt 7 of their corruptions, namely gaussian noise, brightness, contrast, saturate, pixelate, defocus blur, and glass blur. The different transformation strengths are obtained by using severity values 3, 4, and 5 from ImageNet-C. For the video-level corruptions, we introduce constant rate factor (CRF) compression for three increasing values of compression and motion blur by averaging every 2, 3, and 4 consecutive frames. For the image-level transformations, we create cartoonization using the mean shift algorithm by increasing the amount of smoothing and image-stylization [13] using four different styles. For stylization, we do not vary the strength, but we preserve the original video colors (low and medium strength) while switching to those of the style image (high strength), making it a more drastic appearance shift. Frame examples for all cases are shown in Figure 4.

Some of the transformations in DAVIS-C do not perfectly represent video distortions. Nevertheless, several of the transformations constitute common real-world video edits (contrast, brightness, crf compression, cartoonization, stylization). We believe DAVIS-C provides a valuable benchmark to study video understanding under distribution shift.

### Results

Sim-to-real transfer.In Figure 4(a), we report results with and without test-time training on four datasets for the case of sim-to-real transfer. All three TTT variants bring gains consistently across datasets, with \(tt\)-MCC improving performance significantly, _i.e_. a relative gain of more than 39%, 15% and 15% for YouTube-VOS, DAVIS and MOSE, respectively. Moreover, we see that naively applying task-agnostic entropy minimization test-time training brings only a small percentage of the gains one can get. It is also worth noting that the gains from TTT are so high that performance using the mask cycle consistency variant becomes comparable to the performance of a model trained on video data. Specifically, \(tt\)-MCC recovers 82%, 72% and 44% of the performance gains that training with all the training videos from YouTube-VOS and DAVIS combined brings. This is very promising in cases where annotated data are scarce but synthetic data is widely available.

Corrupted test examples.We report results for DAVIS-C in Figures 4(b), 5, and the right side of Table 1. Figure 4(b) clearly shows how \(tt\)-MCC outperforms STCN for all corruption levels, with the gains increasing more as corruption strength increases. The entropy minimization approach does not offer any gains, while \(tt\)-AE is effective but worse than \(tt\)-MCC. In Figure 5, we report the \(\mathcal{J}\&\mathcal{F}\) score separately per video of DAVIS-C at the highest strength. We see that the gains come from many transformations and examples, while we see that TTT is able to improve most videos where the original \(\mathcal{J}\&\mathcal{F}\) score was in the lower side of the spectrum. Finally, on the right side of Table 1, we further report results on DAVIS-C for XMem and see that gains persist for one more state-of-the-art method. Interestingly, from Table 1, we also see that the performance gain of XMem over STCN disappears for the sim-to-real transfer case for YouTube-VOS, as well as for the extreme case a model trained only on synthetic videos is tested on corrupted inputs.

Figure 5: **VOS performance under distribution shifts.**_Left:_ performance of STCN-BL30K before and after test-time training for the sim-to-real transfer case on four datasets. _Right:_ performance of STCN-DY on DAVIS-C for input corruptions with different strength levels.

Comparison to the state of the art.In Table 1, we additionally report results for our \(tt\)-MCC variant when starting from the state-of-the-art XMem [6] method. Once again, the gains are consistent, for both cases. It is worth noting that from the fourth column of the table, we can see that \(tt\)-MCC is also able to slightly improve the case when there is no test-time distribution shift, _i.e_. we report 1.4 and 0.4 higher \(\mathcal{J}\&\mathcal{F}\) score on DAVIS for STCN-DY and XMem -DY, respectively. In the table, we also compare our method to HODOR [1] method that uses cycle consistency during the offline training stage and reports results when training without real videos. Our STCN-BL30K model with TTT outperforms HODOR by **+3.6** and **+8.3**\(\mathcal{J}\&\mathcal{F}\) score, on DAVIS and YouTube-VOS respectively.

Impact of sampling longer sequences.Regarding the sequence length used during test-time training, we adopt the setup used by the base methods during their training phase, _i.e_. frame triplets for STCN and octuplets for XMem. We experiment with quadruplets or quintuplets for mask cycle consistency on top of STCN which achieve performance of 81.7 and 82.4, respectively, on DAVIS versus 81.1 for triplets. This gain in performance, however, comes with a significant increase in test-time training time, _i.e_. 33% and 66%, respectively.

Impact of changing the sampling strategy.Additionally to changing the sequence length, another way to affect the size of the temporal context is by the value of the jump step, _i.e_. the interval used to sample frames \(x_{i}\) and \(x_{j}\) for a training triplet. Test-time training with \(tt\)-MCC over the STCN-BL30K model achieves 81.1 on DAVIS, with the jump step \(s\) parameter set to 10 by default. Varying the jump step from 1, 2, 25, and 50, achieves 80.6, 81.0, 80.8, and 79.0, respectively. Neither sampling too close to the first frame nor sampling frames too far from each other is the optimal choice. Switching to random sampling, without any step as a restriction, causes a performance drop to 79.7, while sampling only within the last 5% or 10% of the video also causes a drop to 79.8 and 79.7, respectively.

\begin{table}
\begin{tabular}{l|c c c c|c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{4}{c|}{Training without real videos} & \multicolumn{4}{c}{Corrupted test examples (DAVIS-C)} \\  & DAVIS & DAVIS-C & YouTube-VOS & MOSE & no corr. & low & med & high & avg \\ \hline HODOR [1] & 77.5\({}^{\dagger}\) & – & 71.7\({}^{\dagger}\) & – & 81.3\({}^{\dagger}\) & 69.0 & 64.5 & 55.2 & 65.0 \\ \hline STCN [9] & 70.4 & 41.7 & 57.3 & 38.9 & 85.3 & 76.6 & 72.6 & 58.8 & 73.3 \\ STCN + \(tt\)-MCC (ours) & 81.1 & **70.1** & **79.4** & **44.9** & 86.7 & 78.3 & 75.6 & 67.3 & 77.0 \\ \hline XMem [6] & 78.1 & 53.9 & 65.6 & 40.9 & 87.7 & 80.4 & 77.3 & 69.4 & 78.7 \\ XMem + \(tt\)-MCC (ours) & **82.1** & **70.1** & 78.9 & 44.7 & **88.1** & **81.7** & **78.9** & **72.2** & **80.2** \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Comparison to the state-of-the-art** for two cases of test-time distribution shift. Left part: Results when using models trained without real videos. The HODOR model is trained on COCO [27] images, while STCN and XMem on BL-30K. Right part: Results on DAVIS-C for different levels of corruption. HODOR, STCN and XMem are fine-tuned with DAVIS and YouTube-VOS videos. \({}^{\dagger}\) Results from [1].

Figure 6: **Test-time training on DAVIS-C**. We plot the \(\mathcal{J}\&\mathcal{F}\) score separately per video before (triangles) and after \(tt\)-MCC (circles) for the STCN-DY model on the 14 corruptions of the proposed DAVIS-C benchmark. We report results for the variants with the highest corruption strength. A red (grey) vertical line denotes that performance drops (increases) with test-time training.

Across all strategies and hyperparameter values, we see noticeable improvements using \(tt\)-MCC; the base model without TTT, merely achieves 70.4 \(\mathcal{J}\&\mathcal{F}\) score on DAVIS.

Impact of training iterations.For all results we use a fixed number of 100 iterations during TTT. In Figure 7, we present the performance improvement of \(tt\)-MCC for all 30 DAVIS videos (one curve per video) for a varying number of iterations. One can clearly see that optimal performance is achieved at different iterations per video, and we believe that adaptive early stopping is a promising future direction that may boost the overall improvement even further.

Impact on inference time.We discuss the effect that test-time training has on inference speed, by measuring average inference time over all DAVIS validation videos. The vanilla STCN model requires roughly 2 seconds per video. For \(tt\)-MCC, \(tt\)-Ent, and \(tt\)-AE, it takes 67, 29, and 5.3 seconds, respectively, for 100 iterations. Since TTT is run per video and its speed is only affected by the number of iterations, per frame timings vary with video length, _i.e._ the TTT overhead reduces as video length increases. It is worth noting here that all timings presented above are estimated with a non-optimized TTT implementation that leaves a lot of space for further optimization.

Reducing TTT overhead.As discussed, TTT induces a significant overhead at test time, which is an aspect not often discussed in the relevant literature. The TTT overhead is reduced by training for less iterations with only a minor decrease in performance. As we show Table 3 of the Appendix, using 20 instead of 100 iterations is even advantageous in cases, particularly for larger datasets like YouTube-VOS and MOSE with a model trained on real videos and without a distribution shift. In a real-world application, applying \(tt\)-MCC on all test examples might seem infeasible due to time overheads. Nevertheless, it is very useful for cases of extreme test-time distribution shifts. In scenarios involving a few out-of-distribution examples, where rapid adaptation of the current model is desired for improved performance, \(tt\)-MCC constitutes a straightforward and cost-effective solution, that is far more efficient than retraining the base model, which typically requires approximately 12.5 hours on 2 A100 GPUs to train STCN.

Memory requirements.During inference, STCN requires roughly 8GB of GPU RAM. Test-time training on top of STCN with \(tt\)-AE, \(tt\)-Ent and \(tt\)-MCC requires 8,16 and 23.5GB, respectively, _i.e._ it can still fit in a modest GPU. Note that the calculations for TTT are computed when using a batch size of 4 (the default). Memory requirements can be further reduced by using a smaller batch size, if needed, without significant change in performance.

## 5 Conclusions

In this work we show that test-time training is a very effective way of boosting the performance of state-of-the-art matching-based video object segmentation methods in the case of distribution shifts. We propose a mask cycle consistency loss that is tailored to matching-based VOS and achieves top performance. Its applicability goes beyond the two methods used in this work, as it is compatible with the general family of matching-based VOS methods [36; 60; 56; 1]. We report very strong gains over top performing methods for both sim-to-real transfer and the case of corrupted test examples. We also show that achieving such gains is not trivial and that it is important to tailor the test-time training method to the task and method at hand. A limitation of the proposed approach is the lack of a way for performing early stopping, and selecting the best iteration to stop training, a very promising direction for future work.

Figure 7: **Change in \(\mathcal{J}\&\mathcal{F}\) score during TTT** compared to the score before TTT. Each curve is one video and is normalized wrt. the maximum improvement/decline observed over 200 iterations.

## Acknowledgments and Disclosure of Funding

This work was supported by Naver Labs Europe, by Junior Star GACR GM 21-28830M, and by student grant SGS23/173/OHK3/3T/13. The authors would like to sincerely thank Gabriela Csurka for providing valuable feedback.

## References

* [1] A. Athar, J. Luiten, A. Hermans, D. Ramanan, and B. Leibe. Hodor: High-level object descriptors for object re-segmentation in video learned from static images. In _CVPR_, 2022.
* [2] F. Azimi, S. Palacio, F. Raue, J. Hees, L. Bertinetto, and A. Dengel. Self-supervised test-time adaptation on video data. In _WACV_, 2022.
* [3] H. S. Behl, M. Naja, A. Arnab, and P. H. Torr. Meta-learning deep visual words for fast video object segmentation. In _IROS_, 2020.
* [4] S. Caelles, K.-K. Maninis, J. Pont-Tuset, L. Leal-Taixe, D. Cremers, and L. Van Gool. One-shot video object segmentation. In _CVPR_, 2017.
* [5] Y. Chen, J. Pont-Tuset, A. Montes, and L. Van Gool. Blazingly fast video object segmentation with pixel-wise metric learning. In _CVPR_, 2018.
* [6] H. K. Cheng and A. G. Schwing. Xmem: Long-term video object segmentation with an atkinson-shiftrin memory model. In _ECCV_, 2022.
* [7] H. K. Cheng, J. Chung, Y.-W. Tai, and C.-K. Tang. Cascadepsp: Toward class-agnostic and very high-resolution segmentation via global and local refinement. In _CVPR_, 2020.
* [8] H. K. Cheng, Y.-W. Tai, and C.-K. Tang. Modular interactive video object segmentation: Interaction-to-mask, propagation and difference-aware fusion. In _CVPR_, 2021.
* [9] H. K. Cheng, Y.-W. Tai, and C.-K. Tang. Rethinking space-time networks with improved memory coverage for efficient video object segmentation. _NeurIPS_, 2021.
* [10] H. Ding, C. Liu, S. He, X. Jiang, P. H. Torr, and S. Bai. Mose: A new dataset for video object segmentation in complex scenes. _arXiv_, 2023.
* [11] B. Duke, A. Ahmed, C. Wolf, P. Aarabi, and G. W. Taylor. Sstvos: Sparse spatiotemporal transformers for video object segmentation. In _CVPR_, 2021.
* [12] Y. Gandelsman, Y. Sun, X. Chen, and A. Efros. Test-time training with masked autoencoders. _NeurIPS_, 2022.
* [13] G. Ghiasi, H. Lee, M. Kudlur, V. Dumoulin, and J. Shlens. Exploring the structure of a real-time, arbitrary neural artistic stylization network. In _BMVC_, 2018.
* [14] D. Hendrycks and T. Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. In _ICLR_, 2019.
* [15] P. Hu, G. Wang, X. Kong, J. Kuen, and Y.-P. Tan. Motion-guided cascaded refinement network for video object segmentation. In _CVPR_, 2018.
* [16] Y.-T. Hu, J.-B. Huang, and A. Schwing. Maskrnn: Instance level video object segmentation. _NeurIPS_, 2017.
* [17] Y.-T. Hu, J.-B. Huang, and A. G. Schwing. Unsupervised video object segmentation using motion saliency-guided spatio-temporal propagation. In _ECCV_, 2018.
* [18] Y.-T. Hu, J.-B. Huang, and A. G. Schwing. Videomatch: Matching based video object segmentation. In _ECCV_, 2018.
* [19] A. Jabri, A. Owens, and A. Efros. Space-time correspondence as a contrastive random walk. _NeurIPS_, 2020.
* [20] V. Jampani, R. Gadde, and P. V. Gehler. Video propagation networks. In _CVPR_, 2017.
* [21] A. Khoreva, R. Benenson, E. Ilg, T. Brox, and B. Schiele. Lucid data dreaming for video object segmentation. _IJCV_, 2019.

* [22] I. Kim, Y. Kim, and S. Kim. Learning loss for test-time augmentation. _NeurIPS_, 2020.
* [23] D. Kingma and J. Ba. Adam: A method for stochastic optimization. In _ICLR_, 2015.
* [24] Z. Lai, E. Lu, and W. Xie. Mast: A memory-augmented self-supervised tracker. In _CVPR_, 2020.
* [25] X. Li, T. Wei, Y. P. Chen, Y.-W. Tai, and C.-K. Tang. Fss-1000: A 1000-class dataset for few-shot segmentation. In _CVPR_, 2020.
* [26] Y. Li, N. Xu, J. Peng, J. See, and W. Lin. Delving into the cyclic mechanism in semi-supervised video object segmentation. _NeurIPS_, 2020.
* [27] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollar, and C. L. Zitnick. Microsoft coco: Common objects in context. In _ECCV_, 2014.
* [28] S. Liu, G. Zhong, S. De Mello, J. Gu, V. Jampani, M.-H. Yang, and J. Kautz. Switchable temporal propagation network. In _ECCV_, 2018.
* [29] Y. Liu, Y. Dai, A.-D. Doan, L. Liu, and I. Reid. In defense of osvos. _arXiv_, 2019.
* [30] Y. Liu, P. Kothari, B. Van Delft, B. Bellot-Gurlet, T. Mordan, and A. Alahi. Ttt++: When does self-supervised test-time training fail or thrive? _NeurIPS_, 2021.
* [31] X. Lu, W. Wang, M. Danelljan, T. Zhou, J. Shen, and L. Van Gool. Video object segmentation with episodic graph memory networks. In _ECCV_, 2020.
* [32] J. Luiten, P. Voigtlaender, and B. Leibe. Premvos: Proposal-generation, refinement and merging for video object segmentation. In _ACCV_, 2019.
* [33] K.-K. Maninis, S. Caelles, Y. Chen, J. Pont-Tuset, L. Leal-Taixe, D. Cremers, and L. Van Gool. Video object segmentation without temporal information. _PAMI_, 41(6):1515-1530, 2018.
* [34] Y. Mao, N. Wang, W. Zhou, and H. Li. Joint inductive and transductive learning for video object segmentation. In _ICCV_, 2021.
* [35] T. Meinhardt and L. Leal-Taixe. Make one-shot video object segmentation efficient again. _NeurIPS_, 2020.
* [36] S. W. Oh, J.-Y. Lee, N. Xu, and S. J. Kim. Video object segmentation using space-time memory networks. In _ICCV_, 2019.
* [37] K. Park, S. Woo, S. W. Oh, I. S. Kweon, and J.-Y. Lee. Per-clip video object segmentation. In _CVPR_, 2022.
* [38] J. Pont-Tuset, F. Perazzi, S. Caelles, P. Arbelaez, A. Sorkine-Hornung, and L. Van Gool. The 2017 davis challenge on video object segmentation. _arXiv_, 2017.
* [39] A. Robinson, F. J. Lawin, M. Danelljan, F. S. Khan, and M. Felsberg. Discriminative online learning for fast video object segmentation. _arXiv_, 2019.
* [40] H. Seong, J. Hyun, and E. Kim. Kernelized memory network for video object segmentation. In _ECCV_, 2020.
* [41] J. Shi, Q. Yan, L. Xu, and J. Jia. Hierarchical image saliency detection on extended cssd. _PAMI_, 38(4):717-729, 2015.
* [42] Y. Sun, X. Wang, Z. Liu, J. Miller, A. Efros, and M. Hardt. Test-time training with self-supervision for generalization under distribution shifts. In _ICML_, 2020.
* [43] C. Ventura, M. Bellver, A. Girbau, A. Salvador, F. Marques, and X. Giro-i Nieto. Rvos: End-to-end recurrent network for video object segmentation. In _CVPR_, 2019.
* [44] P. Voigtlaender and B. Leibe. Online adaptation of convolutional neural networks for video object segmentation. In _BMVC_, 2017.
* [45] P. Voigtlaender, Y. Chai, F. Schroff, H. Adam, B. Leibe, and L.-C. Chen. Feelvos: Fast end-to-end embedding learning for video object segmentation. In _CVPR_, 2019.
* [46] D. Wang, E. Shelhamer, S. Liu, B. Olshausen, and T. Darrell. Tent: Fully test-time adaptation by entropy minimization. In _ICLR_, 2021.
* [47] L. Wang, H. Lu, Y. Wang, M. Feng, D. Wang, B. Yin, and X. Ruan. Learning to detect salient objects with image-level supervision. In _CVPR_, 2017.

* [48] Y. Wang, J. Choi, Y. Chen, S. Li, Q. Huang, K. Zhang, M.-S. Lee, and C.-C. J. Kuo. Unsupervised video object segmentation with distractor-aware online adaptation. _Journal of Visual Communication and Image Representation_, 74:102953, 2021.
* [49] H. Xiao, B. Kang, Y. Liu, M. Zhang, and J. Feng. Online meta adaptation for fast video object segmentation. _PAMI_, 42(5):1205-1217, 2019.
* [50] Z. Xiao, X. Zhen, S. Liao, and C. G. Snoek. Energy-based test sample adaptation for domain generalization. _ICLR_, 2023.
* [51] N. Xu, L. Yang, Y. Fan, D. Yue, Y. Liang, J. Yang, and T. Huang. Youtube-vos: A large-scale video object segmentation benchmark. In _ECCV_, 2018.
* [52] L. Yang, Y. Wang, X. Xiong, J. Yang, and A. K. Katsaggelos. Efficient video object segmentation via network modulation. In _CVPR_, 2018.
* [53] S. Yang, X. Wang, Y. Li, Y. Fang, J. Fang, W. Liu, X. Zhao, and Y. Shan. Temporally efficient vision transformer for video instance segmentation. In _CVPR_, 2022.
* [54] Z. Yang and Y. Yang. Decoupling features in hierarchical propagation for video object segmentation. _NeurIPS_, 2022.
* [55] Z. Yang, Y. Wei, and Y. Yang. Collaborative video object segmentation by foreground-background integration. In _ECCV_, 2020.
* [56] Z. Yang, Y. Wei, and Y. Yang. Associating objects with transformers for video object segmentation. _NeurIPS_, 2021.
* [57] C. Yi, S. Yang, Y. Wang, H. Li, Y.-P. Tan, and A. C. Kot. Temporal coherent test-time optimization for robust video classification. In _ICLR_, 2023.
* [58] Y. Zeng, P. Zhang, J. Zhang, Z. Lin, and H. Lu. Towards high-resolution salient object detection. In _ICCV_, 2019.
* [59] M. Zhang, S. Levine, and C. Finn. Memo: Test time robustness via adaptation and augmentation. _NeurIPS_, 2022.
* [60] Y. Zhang, Z. Wu, H. Peng, and S. Lin. A transductive approach for video object segmentation. In _CVPR_, 2020.
* [61] Y. Zhang, L. Li, W. Wang, R. Xie, L. Song, and W. Zhang. Boosting video object segmentation via space-time correspondence learning. In _CVPR_, 2023.

## Appendix

### Contents

* A Datasets and additional implementation details
* A.1 Datasets
* A.2 Additional implementation details
* B Additional experimental results and analysis
* B.1 Measuring the standard deviation of TTT
* B.2 Extended results on the MOSE dataset
* B.3 Results with TTT on top of XMem
* B.4 Varying the number of real video used for training the base model
* B.5 Additional qualitative results
* B.6 Results in the case of no distribution shift
* B.7 Results in the case of training with static images
* B.8 Performance analysis on a per-video and per-object basis
* B.9 Impact of longer test-time training
* B.10 Assessing temporal stability
* B.11 Detailed results on DAVIS-C
* B.12 Failure cases
* C Broader impact
* D Figures for all TTT losses

## Appendix A Datasets and additional implementation details

### Datasets

DAVISThe validation split of the DAVIS-2017 [38] dataset contains 30 videos covering a variety of real-world scenarios, including indoor and outdoor scenes, different lighting conditions, occlusions, and complex motion patterns. Each video contains one to three annotated objects of interest.

YouTubeVOS-2018The validation split of the YouTubeVOS-2018 [51] dataset contains 474 high-quality videos downloaded from YouTube, including indoor and outdoor scenes, different lighting conditions, occlusions, and complex motion patterns. Each video contains one to five annotated objects of interest.

Figure 8: **The four styles in DAVIS-C.**DAVIS-CWe refer the reader to the main paper for details. Here, we highlight the 14 different transformations to each video applied at three different strengths, namely _low_, _medium_, and _high_ strength in Figure 17. The four original images that we use to compute the four stylizations _style 1_, _style 2_, _style 3_, and _style 4_ are displayed in Figure 8.

MoseWe further report results on a newer, much larger dataset, _i.e_. the MOSE dataset [10], which contains challenging examples with heavy occlusions and crowded real-world scenes. The validation split of the MOSE dataset contains 311 high-quality videos. Each video contains one to fifteen annotated objects of interest. In addition, to show the robustness of our method, we also ran our models on the training split of the MOSE dataset, which contains 1507 high-quality videos with up to twenty annotated objects of interest. Because the STCN and XMem models are not trained on the MOSE-train split before, we consider this split as a larger dataset for testing our method. Because of limited space, we present results on MOSE-train only in the supplementary, and as we show in Section B, the observations of the main paper also hold in this case.

### Additional implementation details

The model is in evaluation mode during test-time training, _i.e_. the running statistics of the Batch Normalization layers remain fixed.

Because the last frame \(m_{j}\) of the frame sequence used in our \(tt\)-MCC method is the initial frame of the backward pass, all the target objects in \(m_{0}\) must also be depicted in \(m_{j}\). To guarantee this, we select only the frames where the network can detect all the target objects, _i.e_. there is at least one pixel for each object where it has the largest probability. We do so by extracting pseudo masks for the video frames and resetting the optimizer every ten iterations.

In the MOSE dataset, some videos have up to twenty annotated objects. Because our method is linear with the number of annotated objects in the video, we randomly select up to six objects at each iteration.

## Appendix B Additional experimental results and analysis

### Measuring the standard deviation of TTT

We report mean and standard deviation after running TTT with 3 different seeds in Table 2 for four datasets, namely DAVIS, DAVIS-C, YouTube-VOS, and MOSE.

### Extended results on the MOSE dataset

In Figure 9, we report STCN results with and without test-time training on the two splits of the MOSE dataset for the case of sim-to-real transfer. We can see that on both splits of the MOSE dataset, the three TTT variants boost the performance, and \(tt\)-MCC brings a relative gain of 15% on 18% on the MOSE-valid and MOSE-train datasets, respectively.

### Results with TTT on top of XMem

In Figure 10, we show the performance comparison with and without TTT on top of XMem. We observe that the performance benefits are similar to those of using TTT on top of STCN.

\begin{table}
\begin{tabular}{l|l l l l|l l l l l} \hline \multirow{2}{*}{Method} & \multicolumn{4}{c|}{Training without real videos} & \multicolumn{4}{c}{Corrupted test examples (DAVIS-C)} \\  & DAVIS-C & YT-VOS & MOSE & no corr. & low & med & high & avg \\ \hline STCN [9] & 70.4 & 41.7 & 57.3 & 38.9 & 85.3 & 76.6 & 72.6 & 58.8 & 73.3 \\ STCN + \(tt\)-MCC (ours) & 81.1\(\pm\)0.1 & **70.1**\(\pm\)0.1 & **79.4**\(\pm\)0.2 & **44.9**\(\pm\)0.2 & **86.7**\(\pm\)0.2 & 78.3\(\pm\)0.1 & 75.6\(\pm\)0.1 & 67.3\(\pm\)0.1 & 77.0 \\ \hline XMem [6] & 78.1 & 53.9 & 65.6 & 40.9 & 87.7 & 80.4 & 77.3 & 69.4 & 78.7 \\ XMem + \(tt\)-MCC (ours) & **82.1**\(\pm\)0.2 & **70.1**\(\pm\)0.3 & 78.9 \(\pm\)0.2 & 44.7 \(\pm\)0.2 & **88.1**\(\pm\)0.2 & **81.7**\(\pm\)0.1 & **78.9**\(\pm\)0.2 & **72.2**\(\pm\)0.1 & **80.2** \\ \hline \end{tabular}
\end{table}
Table 2: **Multiple runs of the proposed method on top of STCN and XMem** for two cases of test-time distribution shift. Mean and standard deviation over 3 different seeds is reported. Left part: Results for STCN-BL30K and XMem-BL30K. Right part: Results for STCN-DY and XMem-DY on DAVIS-C for different levels of corruption.

### Varying the number of real video used for training the base model

We increase the number of real videos used to train the STCN model from 0 (BL-30K) to 3531 (DAVIS and YouTube-VOS training sets) and present results in Figure 11. When the model is trained with little to no real videos, test-time training recovers the bulk of the performance achieved using models trained on larger annotated datasets while requiring little to no annotation.

Figure 11: **Performance on DAVIS with respect to the number of real videos used during training. Performance before and after test-time training for models trained using only synthetic videos (BL-30K) and up to 3531 real videos (DAVIS and YouTube-VOS together).**

Figure 10: **XMem performance under distribution shifts.**_Left:_ performance of XMem -BL30K before and after test-time training for the sim-to-real transfer case on four datasets. _Right:_ performance of XMem -DY on DAVIS-C for input corruptions with different strength levels.**

Figure 9: **STCN performance on the MOSE dataset for the sim-to-real transfer case. We report results on both the validation set, as well as the much larger training set of 1507 videos; used as another test set since it is not used as a training set by the models used in this work.**

[MISSING_PAGE_EMPTY:17]

not negatively impact the performance of state-of-the-art methods in these cases. Instead, it offers a method to either maintain or enhance state-of-the-art performance across all test-time video scenarios, whether extreme distribution shifts are present or not.

### Results in the case of training with static images

In this section, we focus on the scenario where the model is trained using only static images. Following STCN, we employ five datasets of static images [47, 41, 58, 7, 25] for the offline training of the networks. Given a training image, several deformed versions are generated to compose an artificial video sequence, which is used for the model training. The corresponding results are presented in the right part of Table 3 for both STCN and XMem across the four datasets. Notably, for STCN, \(tt\)-MCC yields relative performance gains exceeding 5%, 4%, and 3% on DAVIS, YouTube-VOS, and MOSE, respectively. These gains represent substantial recovery of the performance achieved by a model trained with real videos, _i.e_. amounting to 42%, 39%, and 13%, respectively. Performance improvement is even more significant on the DAVIS-C dataset, with a 19% relative performance gain, surpassing the STCN model trained on real videos (70.7 _vs_. 69.3).

### Performance analysis on a per-video and per-object basis

We conduct a comprehensive analysis using the MOSE-train dataset on a per-video and per-object basis and present results in Figure 14. Figure 14 (a) examines the performance gain using TTT in relation to video length, revealing that video length does not significantly affect performance. Figure 14 (b) demonstrates a subtle correlation between performance gain and the size of objects in the first frame, with larger objects showing positive gains and no negative impact. In Figure 14 (c), we explore how the object's size in the initial frame relative to its maximum size in the video affects performance. We observe a slight negative impact when the object is less visible in the first frame compared to subsequent frames. This is attributed to \(tt\)-MCC overfitting to the partially annotated first frame and often segmenting the partial object in future frames. Note that extreme cases of severe occlusion in the first frame can make the video object segmentation task particularly ambiguous. Finally, Figure 14 (d) assesses the impact of object visibility duration within the video sequence,

Figure 14: **Performance gain of tt-MCC for the sim-to-real case on the MOSE-train dataset.** We plot the performance gain _vs_. a) the video length in number of frames, b) the object area in the first frame normalized by the frame area, c) the object area in the first frame normalized by the maximum object area over all frames, d) the percentage of the video length where the object is visible.

\begin{table}
\begin{tabular}{l|c c c c|c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{4}{c|}{Training with real videos} & \multicolumn{4}{c}{Training with static images} \\  & DAVIS & DAVIS-C & YT-VOS & MOSE & DAVIS & DAVIS-C & YT-VOS & MOSE \\ \hline STCN & 85.3 & 69.3 & 84.3 & 52.5 & 75.7 & 59.3 & 76.3 & 42.0 \\ STCN + \(tt\)-MCC-20 (ours) & 86.0 & 72.1 & 84.6 & 53.3 & 78.7 & 68.8 & 78.2 & **43.6** \\ STCN + \(tt\)-MCC-100 (ours) & 86.7 & 73.8 & 84.0 & 51.8 & **79.7** & **70.7** & **79.4** & 43.4 \\ \hline XMem & 87.7 & 75.7 & **86.1** & 60.4 & 72.8 & 56.2 & 77.0 & 42.3 \\ XMem + \(tt\)-MCC-20 (ours) & 88.0 & 76.8 & 85.8 & **60.7** & 75.9 & 63.0 & 76.5 & 41.7 \\ XMem + \(tt\)-MCC-100 (ours) & **88.1** & **77.6** & 85.1 & 59.8 & 78.8 & 67.2 & 78.4 & 42.9 \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Additional results for STCN and Xmem**. The number of TTT iterations is reported next to \(tt\)-MCC. Left part: Results when starting from a model trained with real videos. Right part: Results when starting from a model trained only on static images.

confirming that \(tt\)-MCC is adversely affected when the object is briefly visible. This suggests that our cycle consistency loss, emphasizing long-term temporal consistency, may introduce bias towards objects being visible for extended periods.

### Impact of longer test-time training

To better study the effect of longer training at test-time, we run our method for up to 500 iterations on DAVIS for the sim-to-real case and presents results in Figure 15. In summary, we do not observe any significant further decrease in performance nor any degenerate outputs after longer training. More specifically, the number of DAVIS videos for which VOS performance decreases remained the same from 50 TTT iterations up to approximately 400, _i.e_. the \(\mathcal{J}\&\mathcal{F}\) score decreases in 7 out of 30 videos. Only one extra video shows a decrease in \(\mathcal{J}\&\mathcal{F}\) score by 500 iterations. Visually observing segmentation results, we notice that a common issue is some background pixels of similar appearance to the object are wrongly segmented. It is also worth noting that increasing the iterations only leads to a minor overall decrease in VOS performance, _i.e_. \(\mathcal{J}\&\mathcal{F}\) score drops from 81.1 to 81.0/80.4 for 200/500 iterations, respectively.

### Assessing temporal stability

In Figure 16, we present a performance comparison on a per-video basis, both with and without \(tt\)-MCC. The graph illustrates consistent and lasting performance improvements. Importantly, it highlights the enhanced stability of performance when \(tt\)-MCC is employed. In the case of a briefly visible object (as seen in video 3bfa8379), our use of \(tt\)-MCC results in incorrectly segmenting out the background as the object, significantly impacting the metric in a negative manner.

### Detailed results on DAVIS-C

In Figures 18 and 19, we present the \(\mathcal{J}\&\mathcal{F}\) score change before and after test-time training for the STCN-BL30K and STCN-DY models, respectively, for all three corruption strengths.

[MISSING_PAGE_FAIL:20]

Figure 17: **Impact of the strength of the corruption** for the 14 types of corruption in DAVIS-C

Figure 18: **Test-time training on DAVIS-C for the STCN-BL30K model. We plot the \(\mathcal{J}\&\mathcal{F}\) score separately per video before (triangles) and after \(tt\)-MCC (circles) for the STCN-BL30K model on the 14 corruptions of the proposed DAVIS-C benchmark. We report results for the variants with the highest corruption strength. A red vertical line denotes that performance drops with test-time training.**

Figure 19: **Test-time training on DAVIS-C for the STCN-DY model**. We plot the \(\mathcal{J}\&\mathcal{F}\) score separately per video before (triangles) and after \(tt\)-MCC (circles) for the STCN-DY model on the 14 corruptions of the proposed DAVIS-C benchmark. We report results for the variants with the highest corruption strength. A red vertical line denotes that performance drops with test-time training.

Figure 20: **The \(tt\)-Ent, \(tt\)-AE, and the cross-entropy loss. The function \(f\) represents the overall prediction model. It takes as input the current test frame and a memory \(M\) of predicted masks from the previous frames and outputs the predicted mask for the current frame.**