# Not All Tokens Are What You Need for Pretraining

 Zhenghao Lin\({}^{\star}\times^{\phi}\)  Zhibin Gou\({}^{\star\pi\phi}\)  Yeyun Gong\({}^{\diamond\phi}\)  Xiao Liu\({}^{\phi}\)  Yelong Shen\({}^{\phi}\)

Ruochen Xu\({}^{\phi}\)  Chen Lin\({}^{\diamond\chi\rho}\)  Yujiu Yang\({}^{\circ\pi}\)  Jian Jiao\({}^{\phi}\)  Nan Duan\({}^{\phi}\)  Weizhu Chen\({}^{\phi}\)

\({}^{\chi}\)Xiamen University \({}^{\pi}\)Tsinghua University \({}^{\rho}\)Shanghai AI Laboratory

\({}^{\phi}\)Microsoft

[https://aka.ms/rho](https://aka.ms/rho)

Equal contribution. See author contributions for details. Work done during their internships at Microsoft Research Asia. EUR2: zhenghaolin@stu.xmu.edu.cn; zebgou@gmail.com Correspondence authors.

###### Abstract

Previous language model pre-training methods have uniformly applied a next-token prediction loss to all training tokens. Challenging this norm, we posit that _"Not all tokens in a corpus are equally important for language model training"_. Our initial analysis examines token-level training dynamics of language model, revealing distinct loss patterns for different tokens. Leveraging these insights, we introduce a new language model called Rho-1. Unlike traditional LMs that learn to predict every next token in a corpus, Rho-1 employs Selective Language Modeling (SLM), which selectively trains on useful tokens that aligned with the desired distribution. This approach involves scoring tokens using a reference model, and then training the language model with a focused loss on tokens with higher scores. When continual pretraining on 15B OpenWebMath corpus, Rho-1 yields an absolute improvement in few-shot accuracy of up to 30% in 9 math tasks. After fine-tuning, Rho-1-1B and 7B achieved state-of-the-art results of 40.6% and 51.8% on MATH dataset, respectively -- matching DeepSeeKMath with only 3% of the pretraining tokens. Furthermore, when continual pretraining on 80B general tokens, Rho-1 achieves 6.8% average enhancement across 15 diverse tasks, increasing both data efficiency and performance of the language model pre-training.

Figure 1: We continual pretrain 1B and 7B LMs with 15B OpenWebMath tokens. Rho-1 is trained with our proposed Selective Language Modeling (SLM), while baselines are trained using causal language modeling. SLM improves average few-shot accuracy on GSM8k and MATH by over 16%, achieving the baseline performance 5-10x faster.

## 1 Introduction

Scaling up model parameters and dataset size has consistently elevated the next-token prediction accuracy in large language models, yielding significant advancements in artificial intelligence (Kaplan et al., 2020; Brown et al., 2020; OpenAI, 2023; Team et al., 2023). However, training on all available data is not always optimal or feasible. As a result, the practice of data filtering has become crucial, using various heuristics and classifiers (Brown et al., 2020; Wenzek et al., 2019) to select training documents. These techniques significantly improve data quality and boost model performance.

However, despite thorough document-level filtering, high-quality datasets still contain many noisy tokens that can negatively affect training, as illustrated in Figure 2 (Upper). Removing such tokens might alter the text's meaning, while overly strict filtering could exclude useful data (Welbl et al., 2021; Muennighoff et al., 2024) and lead to biases (Dodge et al., 2021; Longpre et al., 2023). Furthermore, research indicates that the distribution of web data does not inherently align with the ideal distribution for downstream applications (Tay et al., 2022; Wettig et al., 2023). For example, common corpus at the token level may include undesirable content like hallucinations or highly ambiguous tokens that are hard to predict. Applying the same loss to all tokens can lead to inefficient computation on non-essential tokens, potentially restricting LLMs from achieving more advanced levels of intelligence.

To explore how language models learn at the token level, we initially examined training dynamics, particularly how the token-level loss evolves during usual pretraining. In SS2.1, we evaluated the model's token perplexity at different checkpoints and categorized tokens into different types. Our findings reveal that significant loss reduction is limited to a select group of tokens. Many tokens are "easy tokens" that are already learned, and some are "hard tokens" that exhibit variable losses and resist convergence. These tokens can lead to numerous ineffective gradient updates.

Based on these analyses, we introduce Rho-1 models trained with a novel Selective Language Modeling (SLM) objective 3. As shown in Figure 2 (Right), this approach inputs the full sequence into the model and selectively removes the loss of undesired tokens. The detailed pipeline is depicted in Figure 4: First, SLM trains a reference language model on high-quality corpora. This model establishes utility metrics to score tokens according to the desired distribution, naturally filtering out unclean and irrelevant tokens. Second, SLM uses the reference model to score each token in a corpus using its loss (SS2.2). Finally, we train a language model only on those tokens that exhibit a high excess loss between the reference and the training model, selectively learning the tokens that best benefit downstream applications (SS2.2).

Footnote 3: “Rho” denotes selective modeling of tokens with higher information “density (\(\rho\))”.

We show through comprehensive experiments that SLM significantly enhances token efficiency during training and improves performance on downstream tasks. Furthermore, our findings indicate that SLM effectively identifies tokens relevant to the target distribution, resulting in improved perplexity scores

Figure 2: **Upper:** Even an extensively filtered pretraining corpus contains token-level noise. **Left:** Previous Causal Language Modeling (CLM) trains on all tokens. **Right:** Our proposed Selective Language Modeling (SLM) selectively applies loss on those useful and clean tokens.

on benchmarks for models trained with the selected tokens. SS3.2 shows the effectiveness of SLM on math continual pretraining: both 1B and 7B Rho-1 outperform CLM-trained baselines by over 16% on the GSM8k and MATH datasets. SLM reaches baseline accuracy up to 10x faster, as shown in Figure 1. Remarkably, Rho-1-7B matches the state-of-the-art performance of DeepSeekMath-7B using only 15B tokens, compared to the 500B tokens required by DeepSeekMath. Upon fine-tuning, Rho-1-1B and 7B achieve 40.6% and 51.8% on MATH, respectively. Notably, Rho-1-1B is the first 1B LM to exceed 40% accuracy, nearing the early GPT-4's CoT performance of 42.5%. SS3.3 confirms the efficacy of SLM in general continual pretraining: Training Tinyllama-1B on 80B tokens with SLM improves 6.8% on average across 15 benchmarks, with gains over 10% in code and math tasks. In SS3.4, we demonstrate that in settings without high-quality reference data, we can use SLM for self-referencing, leading to an average improvement of up to 3.3% in downstream tasks.

## 2 Selective Language Modeling

### Not All Tokens Are Equal: Training Dynamics of Token Loss

Our investigation begins with a critical look at how individual tokens' losses evolve during standard pre-training. We continue pre-training Tinyllama-1B with 15B tokens from OpenWebMath, saving checkpoints after every 1B tokens. We then evaluate token-level loss at these intervals using the validation set of approximately 320,000 tokens. Figure 3(a) reveals a striking pattern: tokens fall into four categories based on their loss trajectory--persistent high loss (H\(\rightarrow\)H), increasing loss (L\(\rightarrow\)H), decreasing loss (H\(\rightarrow\)L), and consistent low loss (L\(\rightarrow\)L). For further details on these categories, see SSD.1. Our analysis uncovers that a mere 26% of tokens show a notable loss reduction (H\(\rightarrow\)L), while the majority (51%) remain in the L\(\rightarrow\)L category, indicating they have already been learned. Interestingly, 11% of the tokens are persistently challenging (H\(\rightarrow\)H), likely due to high aleatoric uncertainty [16]. Additionally, 12% of tokens experience an unexpected loss increase (L\(\rightarrow\)H) during training.

Our second observation is that a significant number of token losses exhibit persistent fluctuations, and resist convergence. The loss of many L\(\rightarrow\)L and H\(\rightarrow\)H tokens, as depicted in Figure 3 (b) and (c), show high variance during training. In SSD.2, we visualize and analyze the content of these tokens and find that many of them are noisy, which is consistent with our hypothesis.

Consequently, we learn that the loss associated with each token during training does not decrease smoothly like the overall loss; instead, there is a complex training dynamic among different tokens. If we can select the appropriate tokens for the model to focus on during training, we may be able to stabilize the trajectory of the model's training and enhance its data efficiency.

### Selective Language Modeling

OverviewInspired by the practice of reference model in document-level filtering, we propose a simple pipeline of token-level data selection, termed "Selective Language Modeling (SLM)". Our method comprises three steps, as depicted in Figure 4. We begin by training a reference model on a curated, high-quality dataset. This model then assesses the loss of each token within the pretraining

Figure 3: **The loss of four categories of tokens during pretraining.** (a) shows the loss of H\(\rightarrow\)H, L\(\rightarrow\)H, H\(\rightarrow\)L, and L\(\rightarrow\)L tokens during pretraining. (b) and (c) show three cases of fluctuating tokens’ loss in L\(\rightarrow\)L and H\(\rightarrow\)H during pretraining, respectively.

corpus. In the final phase, we train the language model selectively, focusing on tokens with high excess loss between the training and reference model. The intuition is that tokens with high excess loss are more learnable and better aligned with the desired distribution, naturally excluding tokens that are either irrelevant or of low quality. Below, we provide a detailed description of each step.

Reference ModelingWe begin by curating a high-quality dataset that reflects the desired data distribution. We train a reference model (RM) using standard cross-entropy loss on the curated data. The resulting RM is then used to assess the token loss within a larger pretraining corpus. We compute the reference loss (\(\mathcal{L}_{\text{RM}}\)) of a token \(x_{i}\) based on the probability that the RM assigns to this token. The calculation is formalized as follows:

\[\mathcal{L}_{\text{RM}}(x_{i})=-\log P(x_{i}|x_{<i}) \tag{1}\]

By evaluating \(\mathcal{L}_{\text{RM}}\) for each token, we establish the reference loss for selective pretraining, allowing us to focus on the most influential tokens in language modeling.

Selective PretrainingNote that Causal Language Modeling (CLM) employs the cross-entropy loss:

\[\mathcal{L}_{\text{CLM}}(\theta)=-\frac{1}{N}\sum_{i=1}^{N}\log P(x_{i}|x_{<i} ;\theta) \tag{2}\]

Here, \(\mathcal{L}_{\text{CLM}}(\theta)\) represents the loss function parameterized by model \(\theta\). \(N\) is the length of the sequence, \(x_{i}\) is the \(i\)-th token in the sequence, and \(x_{<i}\) represents all tokens before the \(i\)-th token. In contrast, Selective Language Modeling (SLM) trains the language model with a focus on tokens that exhibit a high excess loss when compared to the reference model. The excess loss (\(\mathcal{L}_{\Delta}\)) for a token \(x_{i}\) is defined as the difference between the current training model loss (\(\mathcal{L}_{\theta}\)) and the reference loss:

\[\mathcal{L}_{\Delta}(x_{i})=\mathcal{L}_{\theta}(x_{i})-\mathcal{L}_{\text{RM} }(x_{i}) \tag{3}\]

We introduce a token selection ratio \(k\%\), which determines the proportion of tokens to be included based on their excess loss. The cross-entropy loss for the selected tokens is computed as follows:

\[\mathcal{L}_{\text{SLM}}(\theta)=-\frac{1}{N*k\%}\sum_{i=1}^{N}I_{k\%}(x_{i}) \cdot\log P(x_{i}|x_{<i};\theta) \tag{4}\]

Here, \(N*k\%\) defines the number of tokens that fall within the top \(k\%\) of excess loss. The indicator function \(I_{k\%}(x_{i})\) is defined as:

\[I_{k\%}(x_{i})=\begin{cases}1&\text{if $x_{i}$ ranks in the top $k\%$ by $S(x_{i})$}\\ 0&\text{otherwise}\end{cases} \tag{5}\]

Figure 4:

By default, we use \(\mathcal{L}_{\Delta}\) as the score function \(S\). This ensures that the loss is applied only to the tokens that are deemed most beneficial for the language model to learn from. In practice, token selection can be implemented by ranking the tokens in a batch according to their excess loss and using only the top \(k\%\) of tokens for training. This process eliminates the loss for undesired tokens without incurring additional costs during pretraining, making our approach both efficient and easily integrated.

## 3 Experiments

We continually pretrained models in both mathematical and general domain and designed ablation and analysis experiments to understand the effectiveness of SLM.

### Experimental Setup

Reference Model TrainingTo train our mathematical reference model, we gathered a dataset of 0.5B high-quality, math-related tokens. This dataset is a blend of synthetic data from GPT (Yu et al., 2024; Huang et al., 2024) and manually curated data (Yue et al., 2024; Ni et al., 2024). For the general reference model, we compiled a corpus of 1.9B tokens from open-source datasets, such as Tulu-v2 (Ivison et al., 2023) and OpenHermes-2.5 (Teknium, 2023). We trained the reference models for 3 epochs. The maximum learning rate was set at 5e-5 for 1B models and 1e-5 for 7B models, applying a cosine decay schedule. We set the maximum sequence lengths to 2048 for 1B models and 4096 for 7B models, packing multiple samples into these lengths for model input. In all main experiments, we initialized the continual pretraining model and the reference model with the _same_ base model.

\begin{table}
\begin{tabular}{l c c c|c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Model**} & \multirow{2}{*}{\(|\mathbf{\theta}|\)**Data**} & \multicolumn{2}{c}{\begin{tabular}{c} **Uniq. Train** \\ **Tokes** \\ \end{tabular} } & \multicolumn{2}{c}{\begin{tabular}{c} **Train** \\ **Tokes** \\ \end{tabular} } & \multicolumn{2}{c}{**GSM8K MATH\({}^{\dagger}\)**} & \multicolumn{2}{c}{**SVAMP ASDiv MAWPS**} & \multicolumn{2}{c}{**TAB MQA**} & \multicolumn{2}{c}{
\begin{tabular}{c} **MMLU** \\ **STEM** \\ \end{tabular} } & \multirow{2}{*}{**SAT\({}^{\ddagger}\)**} & \multirow{2}{*}{**AVG**} \\ \cline{8-14} \multicolumn{1}{c}{} & & & & & & & & & & & & & & \\ \hline \multicolumn{14}{c}{1-2B Base Models} \\ \hline Tinyllama & 1.1B & - & - & - & - & 2.9 & 3.2 & 11.0 & 18.1 & 20.4 & 12.5 & 14.6 & 16.1 & 21.9 & 13.4 \\ Phi-1.5 & 1.3B & - & - & - & 32.4 & 4.2 & 43.4 & 53.1 & 66.2 & 24.4 & 14.3 & 21.8 & 18.8 & 31.0 \\ Qwen1.5 & 1.8B & - & - & - & 36.1 & 6.8 & 48.5 & 63.6 & 79.0 & 29.2 & 25.1 & 31.3 & 40.6 & 40.0 \\ Gemma & 2.0B & - & - & - & 18.8 & 11.4 & 38.0 & 56.6 & 72.5 & 36.9 & 26.8 & 34.4 & 50.0 & 38.4 \\ DeepSeeKLM & 1.3B & OWM & 14B & 150B & 11.5 & 8.9 & - & - & - & - & - & 29.6 & 31.3 & - \\ DeepSeeKLM & 1.3B & - & 120B & 150B & 23.8 & 13.6 & - & - & - & - & - & 33.1 & 56.3 & - \\ \hline \multicolumn{14}{c}{Continual Pretraining on Tinyllama-1B} \\ \hline Tinyllama-CT & 1.1B OWM & 14B & 15B & 6.4 & 2.4 & 21.7 & 36.7 & 47.7 & 17.9 & 13.9 & 23.0 & 25.0 & 21.6 \\ Rho-1-Math & 1.1B OWM & 14B & 9B & 29.8 & 14.0 & 49.2 & 61.4 & 79.8 & 25.8 & 30.4 & 24.7 & 28.1 & 38.1 \\ \(\Delta\) & & -40\% & +23.4 & +11.6 & +27.5 & +24.7 & +32.1 & +7.9 & +16.5 & +1.7 & 43.1 & +**16.5** \\ \hline Rho-1-Math & 1.1B OWM & 14B & 30B & 36.2 & 15.6 & 52.1 & 67.0 & 83.9 & 29.0 & 32.5 & 23.3 & 28.1 & 40.9 \\ \hline \multicolumn{14}{c}{\(\geq\) 7B Base Models} \\ \hline LLaMA-2 & 7B & - & - & 14.0 & 3.6 & 39.5 & 51.7 & 63.5 & 30.9 & 12.4 & 32.7 & 34.4 & 31.4 \\ Mistral & 7B & - & - & 41.2 & 11.6 & 64.7 & 68.5 & 87.5 & 52.9 & 33.0 & 49.5 & 59.4 & 52.0 \\ Minerva & 8B & - & 39B & 164B & 16.2 & 14.1 & - & - & - & - & - & 35.6 & - & - \\ Minerva & 62B & - & 39B & 109B & 52.4 & 27.6 & - & - & - & - & - & 53.9 & - & - \\ Minerva & 540B & - & 39B & 26B & 58.8 & 33.6 & - & - & - & - & 63.9 & - & - \\ LLemma & 7B & PPile & 55B & 200B & 38.8 & 17.2 & 56.1 & 69.1 & 82.4 & 48.7 & 41.0 & 45.4 & 59.4 & 50.9 \\ LLemma & 34B PPile & 55B & 50B & 54.2 & 23.0 & 67.9 & 75.7 & 90.1 & 57.0 & 49.8 & 54.7 & 68.8 & 60.1 \\ Interm-Math & 7B & - & 31B & 125B & 41.8 & 14.4 & 61.6 & 66.8 & 83.7 & 50.0 & 57.3 & 24.8 & 37.5 & 48.7 \\ Interm-Math & 20B & - & 31B & 125B & 65.4 & 30.0 & 75.7 & 79.3 & 94.0 & 50.9 & 38.5 & 53.1 & 71.9 & 62.1 \\ DeepSeeKLM & 7B & - & 120B & 500B & 64.1 & 34.2 & 74.0 & 83.9 & 92.4 & 63.4 & 62.4 & 56.4 & 84.4 & 68.4 \\ \hline \multicolumn{14}{c}{Continual Pretraining on Mistral-7B} \\ \hline Mistral-CT & 7B OWM & 14B & 15B & 42.9 & 22.2 & 68.6 & 71.0 & 86.1 & 45.1 & 47.7 & 52.6 & 65.6 & 55.8 \\ Rho-1-Math & 7B OWM & 14B & 10.5B & 66.9 & 31.0 & 77.8 & 79.0 & 93.9 & 49.9 & 58.7 & 54.6 & 84.4 & 66.2 \\ \(\Delta\) & & -30\% & +24.0 & +8.8 & +9.2 & +8.0 & +7.8 & +4.8 & +11.0 & +2.0 & +18.8 & +10.4 \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Few-shot CoT reasoning results of math pretraining.** All models are tested with few-shot prompting. Previous best results are highlighted in blue, while our best results are in purple. \({}^{*}\)Only unique math-related tokens are calculated. For Rho-1, we calculate only the selected tokens that are used for training. \({}^{\dagger}\)We use OpenAI’s MATH subset (Lightman et al., 2023) for evaluation, since some original test samples have been used in public training sets such as PRM800k. \({}^{\ddagger}\)The SAT only has 32 four-choice problems, so we average our results over the last three checkpoints, if available.

Pretraining CorpusFor mathematical reasoning, we utilize the OpenWebMath (OWM) dataset [20], which comprises approximately 14B tokens sourced from math-related web pages in the Common Crawl. In the general domain, we combine the SlimPajama [1] and StarCoderData [11] (both part of the Tinyllama corpus) with OpenWebMath, training on a total of 80 billion tokens with a mix ratio of 6:3:1.

Pretraining SettingFor math pretraining, we continue pretraining on the Tinyllama-1.1B model [13] and the Mistral-7B model [13] with learning rates of 8e-5 and 2e-5, respectively. For the 1.1B model, we conducted our training on 32 x H100 80G GPUs. This configuration allowed us to train approximately 15 billion tokens in around 3.5 hours and 50 billion tokens in about 12 hours. In the case of the 7B model, training the same 15 billion tokens took approximately 18 hours under similar hardware conditions. For general domain, we set the learning rate for Tinyllama-1.1B model to 1e-4 and train 80B tokens under the same hardware conditions, which takes approximately 19 hours. The batch size is uniformly set to 1M tokens for both domains. Regarding the token selection ratio, we use 60% for the Tinyllama-1.1B model and 70% for the Mistral-7B model.

Baseline SettingWe use models that have been continually pretrained (Tinyllama-CT and Mistral-CT) through regular causal language modeling as baselines. Moreover, we compare Rho-1 with well-known and top-performing baselines, including Gemma [23], Owen1.5 [1], Phi-1.5 [11], DeepSeeKLLM [15], DeepSeeKMath [13], CodeLlama [14], Mistral [13], Minerva [15], Thyllama [13], LLemma [1], and InternLM2-Math [22]. For fine-tuning results, we also compare with previous best models MAMmOTH[22] and ToRA[12].

Evaluation SetupTo comprehensively evaluate pretrained models, we compare their few-shot capabilities and fine-tuning performance across a variety of tasks. We adopt the lm-eval-harness4[1] for general tasks, and develop math evaluation suite5 for math tasks. We use vlm (v0.3.2) [21] to speed up inference. Further details on our evaluation can be found in Appendix E.

Footnote 4: [https://github.com/EleutherAI/lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness)

Footnote 5: [https://github.com/ZubinGou/math-evaluation-harness](https://github.com/ZubinGou/math-evaluation-harness)

### Math Pre-training Results

Few-shot CoT Reasoning ResultsWe evalute base models prompting with few-shot chain-of-thought (CoT) [24] examples following previous works [14, 15, 16]. As results shown in Table 1, in comparison to continue pretraining directly, Rho-1-Math has achieved the average few-shot accuracy improvement of

\begin{table}
\begin{tabular}{l c c c|c c c c c c c c|c} \hline \hline
**Model** & **Size** & **Tools** & **SFT Data** & **GSM8k** & **MATH** & **SVAMP** & **ASDiv** & **MAWPS** & **TAB** & **GSM-H** & **AVG** \\ \hline
**Used for SFT2** & & & & ✓ & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ & \multicolumn{1}{c}{} & \\ \hline \multicolumn{11}{c}{Previous Models} \\ \hline GPT4-0314 & - & ✗ & - & 92.0 & 42.5 & 93.1 & 91.3 & 97.6 & 67.1 & 64.7 & 78.3 \\ GPT4-0314 (PAL) & - & ✓ & - & 94.2 & 51.8 & 94.8 & 92.6 & 97.7 & 95.9 & 77.6 & 86.4 \\ MAMmOTH & 70B & ✓ & MI-260K & 76.9 & 41.8 & 82.4 & - & - & - & - & - & - \\ ToRA & 7B & ✓ & ToRA-69k & 68.8 & 40.1 & 68.2 & 73.9 & 88.8 & 42.4 & 54.6 & 62.4 \\ ToRA & 70B & ✓ & ToRA-69k & 84.3 & 49.7 & 82.7 & 86.8 & 93.8 & 74.0 & 67.2 & 76.9 \\ DeepSeeKMath & 7B & ✓ & ToRA-69k & 79.8 & 52.0 & 80.1 & 87.1 & 93.8 & 85.8 & 63.1 & 77.4 \\ \hline \multicolumn{11}{c}{Our Pretrained Model} \\ \hline TinyLlama-CT & 1B & ✓ & ToRA-69k & 51.4 & 38.4 & 53.4 & 66.7 & 81.7 & 20.5 & 42.8 & 50.7 \\ Rho-1-Math & 1B & ✓ & ToRA-69k & 59.4 & 40.6 & 60.7 & 74.2 & 88.6 & 26.7 & 48.1 & 56.9 \\ \(\Delta\) & & & & +8.0 & +2.2 & +7.3 & +7.5 & +6.9 & +6.2 & +5.3 & +6.2 \\ \hline Mistral-CT & 7B & ✓ & ToRA-69k & 77.5 & 48.4 & 76.9 & 83.8 & 93.4 & 67.5 & 60.4 & 72.6 \\ Rho-1-Math & 7B & ✓ & ToRA-69k & 81.3 & 51.8 & 80.8 & 85.5 & 94.5 & 70.1 & 63.1 & 75.3 \\ \(\Delta\) & & & & +3.8 & +3.4 & +3.9 & +1.7 & +1.1 & +2.6 & +2.7 & +2.7 \\ \hline \hline \end{tabular}
\end{table}
Table 2: **Tool-integrated reasoning results of math pretraining.**16.5% on 1B models and 10.4% on 7B models. Furthermore, after training for multiple epochs on OpenWebMath, we find that Rho-1 could further increase the average few-shot accuracy to 40.9%. Compared to DeepSeeMath-7B, which pretrained on 500 billion math-related tokens, Rho-1-7B pretrained on only 15 billion tokens (selecting 10.5 billion tokens) achieved comparable results, demonstrating the efficiency of our approach.

Tool-Integrated Reasoning ResultsWe fine-tune Rho-1 and baseline models on 69k ToRA corpus [14], consisting of 16k GPT-4-generated trajectories in a tool-integrated reasoning format, and 53k answer-augmented samples using LLaMA. As presented in Table 2, Rho-1-1B and Rho-1-7B achieved a state-of-the-art 40.6% and 51.8% on MATH dataset, respectively. On some unseen tasks (_e.g.,_ TabMWP and GSM-Hard), Rho-1 also demonstrates a certain degree of generalizability, with an average few-shot accuracy improvement of 6.2% on the Rho-1-Math-1B and 2.7% on Rho-1-Math-7B.

### General Pre-training Results

We confirm the efficacy of the SLM in general pretraining by continual training Tinyllama-1.1B on 80 billion tokens. The results depicted in Figure 5 indicate that although Tinyllama has already undergone extensive training on the majority of these tokens, the application of SLM yields an average enhancement of 6.8% across 15 benchmarks compared to direct continual pretraining. The improvements were especially pronounced in code and math tasks, exceeding 10%.

### Self-Reference Results

In this section, we demonstrate that SLM can enhance the effectiveness of model pre-training using only pre-training corpora, without the need for additional high-quality data. Specifically, we initially trained the reference model on the OpenWebMath (OWM) corpus, a subset of Proof-Pile-2 (PPile). We evaluated OWM and PPile using the trained reference model and selected tokens for training. In this scenario, we assume the absence of downstream task-related data, a common situation in real-world applications. We hypothesize that the key factor is not scoring the desired distribution but filtering out noisy tokens. Therefore, we employed two different scoring functions based on the reference model loss, \(\mathcal{L}_{\text{RM}}\), and the information entropy of the next token, \(\mathcal{H}_{\text{RM}}\), which measures the uncertainty of the next token. Details are provided in Appendix H.

Figure 5: **General pretraining results.** We continual pretraining Tinyllama-1B on 80G general tokens. Tinyllama-CT is trained with CLM, while Rho-1 is trained with our proposed SLM.

The experimental results, as shown in Table 3, indicate that using only the OWM-trained reference model can effectively guide the model in pre-training on the same corpus, improving average downstream performance by +2.4%. Using only the information entropy as the score function brought about a similar improvement. Additionally, we considered training on the intersection of tokens selected by the two scoring functions and found better performance, with a 40% reduction in tokens and +3.3% performance. Furthermore, training the SLM on the PPile, despite only using the OWM subset to train the reference model, still achieved a 1.8% improvement with 30% fewer tokens used. For more details, please refer to Appendix H.

### Ablation Study and Analysis

Selected Token Loss Aligns Better with Downstream PerformanceWe utilized the reference model to filter tokens and assess their impact on validation and downstream losses after training. As depicted in Figure 6, we pretrained on 4B tokens and tracked loss variations across methods and validation sets. The Rho-1 showed greater loss reduction on selected tokens than regular pretraining. Cross-referencing figures (a), (b), and (c) reveals that selected-token pretraining substantially lowers downstream loss, while traditional pretraining's effect on downstream loss is less pronounced despite initial loss reductions. Therefore, we expect that selecting tokens for pretraining is more efficient.

In Figure 7, we demonstrate that the loss of selected tokens correlates with downstream task performance, following a power law similar to recent findings (Gadre et al., 2024). Our analysis shows that tokens selected by SLM positively impact performance, while those not selected have a negative impact. Thus, reducing loss across all tokens is not imperative for improved model performance. Refer to Appendix F for further details.

What Tokens are Selected with SLM?We aim to analyze the tokens selected by the SLM method in pretraining to further explore its working mechanism. To this end, we visualize the token selection process during the training of Rho-1 using the OpenWebMath. In SSG.1, we have highlighted in blue the tokens that were retained during actual pretraining. We observe that the majority of tokens chosen

\begin{table}
\begin{tabular}{l|c c c c|c c c c c c|c} \hline \hline \multirow{2}{*}{**Model**} & \multirow{2}{*}{\begin{tabular}{c} **Score** \\ **Function** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **Data** \\ **Toks** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **Uniq.** \\ **Toks** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **Train** \\ **Toks** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **Train** \\ **Toks** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **Fain** \\ **Toks** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **GSMSK** \\ **Match** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **MATH** \\ **SVAMP** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **ASDiv** \\ **AMWPS** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **MAWPS** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **MQA** \\ **AVG** \\ \end{tabular} } & \multirow{2}{*}{
\begin{tabular}{c} **AVG** \\ \end{tabular} } \\ \cline{1-1} \cline{8-13} \cline{8-13} \cline{8-13} \multicolumn{1}{c|}{} & & & & & & & & & & & & & \\ \hline TinyLama-CT (RM) & - & OWM & 14B & 15B & 6.3 & 2.6 & 21.7 & 36.7 & 47.7 & 13.9 & 21.5 \\ TinyLama-SLM & \(\mathcal{L}_{\text{BM}}\) & OWM & 14B & 10.5B & 6.7 & 4.6 & 23.3 & 40.0 & 54.5 & 14.3 & 23.9 \\ TinyLama-SLM & \(\mathcal{H}_{\text{RM}}\) & OWM & 14B & 10.5B & 7.0 & 4.8 & 23.0 & 39.3 & 50.5 & 13.5 & 23.0 \\ TinyLama-SLM & \(\mathcal{L}_{\text{EM}}\cap\mathcal{H}_{\text{RM}}\) & OWM & 14B & 9B & 7.1 & 5.0 & 23.5 & 41.2 & 53.8 & 18.0 & 24.8 \\ \hline TinyLama-CT & - & PPIie & 55B & 52B & 8.0 & 6.6 & 23.8 & 41.0 & 54.7 & 14.2 & 24.7 \\ TinyLama-SLM & \(\mathcal{L}_{\text{EM}}\cap\mathcal{H}_{\text{RM}}\) & PPIie & 55B & 36B & 8.6 & 8.4 & 24.4 & 43.6 & 57.9 & 16.1 & 26.5 \\ \hline \hline \end{tabular}
\end{table}
Table 3: **Self-Reference results. We use OpenWebMath (OWM) to train the reference model.**

Figure 6: **The dynamics of pretraining loss and downstream loss. (a) and (c) represent the loss of tokens selected/unselected by SLM during pretraining in both SLM and CLM methods, while (b) represents the loss of the SLM and CLM methods on MetaMath (Yu et al., 2024). We tested the above results through the process of pretraining with a total of 4 billion tokens.**

by the SLM method are closely related to mathematics, effectively training the model on the parts of the original corpus that are pertinent to mathematical content.

Furthermore, we investigated the differences in token filtering across various checkpoints during the training process and tested the perplexity of these tokens on different checkpoints. As illustrated in Figure 9, we found that the tokens selected by later checkpoints tend to have higher perplexity towards the later stages of training and lower perplexity in the earlier stages. This may suggest that the model first optimizes tokens with a larger learnable space, thereby increasing learning efficiency. Moreover, we noticed a sample-wise "double descent" [Nakkiran et al., 2021] on the loss of selected tokens, where the select token's perplexity initially increases before decreases. This might be an effect of selecting tokens based on excess loss, targeting those most in need at each checkpoint.

Effect of Token Select RatioWe investigate the impact of token selecting ratios of the SLM. Generally, the selecting ratio is defined by heuristic rules, similar to the approach previously employed in the training of Masked Language Models (MLMs) [Devlin et al., 2019, Liu et al., 2019]. As shown in Figure 9, the selected tokens is suitable for accounting for about 60% of the original tokens.

## 4 Conclusion

In this paper, we propose using Selective Language Modeling(SLM) to train Rho-1, which select more suitable tokens for current pretraining stage. We conducted the detailed analysis of the loss of tokens during the pretraining process and found that not all tokens are equal during pretraining. Our

Figure 8: **The PPL of tokens selected by different checkpoint. We test the PPL of the tokens selected at 2B, 5B, 8B, 11B, and 14B.** Figure 9: **Effect of token select ratio. We train 1B LM with SLM objective on 5B tokens.**

experiments and analysis in the fields of mathematics and general have demonstrated the effectiveness of the SLM method, emphasizing the importance of token level in the LLM pretraining process. In the future, how to improve pretraining of LLMs from the perspective of token level worthy of in-depth research.

#### Acknowledgments

Zhenghao Lin and Chen Lin were supported by National Key R&D Program of China (No. 2022ZD0160501), the Natural Science Foundation of China (No.62372390,62432011). Zhibin Gou and Yujiu Yang were supported by the Shenzhen Science and Technology Program (JCYJ20220818101001004) and the "Graph Neural Network Project" of Ping An Technology (Shenzhen) Co., Ltd.

## References

* Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. _arXiv preprint arXiv:2001.08361_, 2020.
* Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. _Advances in neural information processing systems_, 33:1877-1901, 2020.
* OpenAI (2023) OpenAI. Gpt-4 technical report, 2023.
* Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. _arXiv preprint arXiv:2312.11805_, 2023.
* Wenzek et al. (2019) Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzman, Armand Joulin, and Edouard Grave. Ccnet: Extracting high quality monolingual datasets from web crawl data. _arXiv preprint arXiv:1911.00359_, 2019.
* Welbl et al. (2021) Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang. Challenges in detoxifying language models. In _Findings of the Association for Computational Linguistics: EMNLP 2021_, pages 2447-2469, 2021.
* Muennighoff et al. (2024) Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin A Raffel. Scaling data-constrained language models. _Advances in Neural Information Processing Systems_, 36, 2024.
* Dodge et al. (2021) Jesse Dodge, Maarten Sap, Ana Marasovic, William Agnew, Gabriel Ilharco, Dirk Groeneveld, Margaret Mitchell, and Matt Gardner. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_, pages 1286-1305, 2021.
* Longpre et al. (2023) Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et al. A pretrainer's guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity. _arXiv preprint arXiv:2305.13169_, 2023.
* Tay et al. (2022) Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q Tran, Dani Yogatama, and Donald Metzler. Scaling laws vs model architectures: How does inductive bias influence scaling? _arXiv preprint arXiv:2207.10551_, 2022.
* Wettig et al. (2023) Alexander Wettig, Tianyu Gao, Zexuan Zhong, and Danqi Chen. Should you mask 15% in masked language modeling? In Andreas Vlachos and Isabelle Augenstein, editors, _Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics_, pages 2985-3000, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.eael-main.217. URL [https://aclanthology.org/2023.eael-main.217](https://aclanthology.org/2023.eael-main.217).
* Hullermeier and Waegeman (2021) Eyke Hullermeier and Willem Waegeman. Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods. _Machine learning_, 110(3):457-506, 2021.
* Yu et al. (2024) Longhui Yu, Weisen Jiang, Han Shi, YU Jincheng, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. In _ICLR_, 2024.
* Yu et al. (2020)Yiming Huang, Xiao Liu, Yeyun Gong, Zhibin Gou, Yelong Shen, Nan Duan, and Weizhu Chen. Key-point-driven data synthesis with its enhancement on mathematical reasoning. _arXiv preprint arXiv:2403.02333_, 2024.
* Yue et al. (2024) Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid instruction tuning. In _ICLR_, 2024.
* Ni et al. (2024) Xinzhe Ni, Yeyun Gong, Zhibin Gou, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Exploring the mystery of influential data for mathematical reasoning, 2024.
* Ivison et al. (2023) Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah A Smith, Iz Beltagy, et al. Camels in a changing climate: Enhancing lm adaptation with tulu 2. _arXiv preprint arXiv:2311.10702_, 2023.
* Teknium (2023) Teknium. Openhermes 2.5: An open dataset of synthetic data for generalist llm assistants, 2023. URL [https://huggingface.co/datasets/teknium/OpenHermes-2.5](https://huggingface.co/datasets/teknium/OpenHermes-2.5).
* Lightman et al. (2023) Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let's verify step by step. _arXiv preprint arXiv:2305.20050_, 2023.
* Paster et al. (2023) Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of high-quality mathematical web text, 2023.
* Daria et al. (2023) Soboleva Daria, Al-Khateeb Faisal, Myers Robert Steeves Jacob R, Hestness Joel, and Dey Nolan. SlimPajama: A 627B token cleaned and deduplicated version of RedPajama. [https://www.cerebras.net/blog/slimpajama-a-627B-token-cleaned-and-deduplicated-version-of-redpajama](https://www.cerebras.net/blog/slimpajama-a-627B-token-cleaned-and-deduplicated-version-of-redpajama), 2023. URL [https://huggingface.co/datasets/cerebras/SlimPajama-627B](https://huggingface.co/datasets/cerebras/SlimPajama-627B).
* Li et al. (2023) Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chin, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishis Davaadorj, Joel Lamy-Poirier, Joao Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy V, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Moustafa-Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Suwayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernie, Carlos Munoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the source be with you! _CoRR_, abs/2305.06161, 2023a.
* Zhang et al. (2024) Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source small language model, 2024.
* Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. _arXiv preprint arXiv:2310.06825_, 2023.
* Team et al. (2024) Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Riviere, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. _arXiv preprint arXiv:2403.08295_, 2024.
* Bai et al. (2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al. Qwen technical report. _arXiv preprint arXiv:2309.16609_, 2023.
* Li et al. (2023) Yuanzhi Li, Sebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. Textbooks are all you need ii: phi-1.5 technical report, 2023b.
* DeepSeek-AI (2024) DeepSeek-AI. Deepseek llm: Scaling open-source language models with longtermism. _arXiv preprint arXiv:2401.02954_, 2024. URL [https://github.com/deepseek-ai/DeepSeek-LIM](https://github.com/deepseek-ai/DeepSeek-LIM).
* Shao et al. (2024) Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, YK Li, Y Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. _arXiv preprint arXiv:2402.03300_, 2024.
* Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, et al. Code llama: Open foundation models for code. _arXiv preprint arXiv:2308.12950_, 2023.
* Xu et al. (2020)Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. _Advances in Neural Information Processing Systems_, 35:3843-3857, 2022.
* Azerbayev et al. (2023) Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. _arXiv preprint arXiv:2310.10631_, 2023.
* Ying et al. (2024) Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, et al. Internlm-math: Open math large language models toward verifiable reasoning. _arXiv preprint arXiv:2402.06332_, 2024.
* Gou et al. (2024) Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen, et al. Tora: A tool-integrated reasoning agent for mathematical problem solving. In _ICLR_, 2024.
* Gao et al. (2023) Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023. URL [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836).
* Kwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Liannin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In _Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles_, 2023.
* Wei et al. (2022a) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In _NIPS_, volume 35, pages 24824-24837, 2022a.
* Gadre et al. (2024) Samir Yitzhak Gadre, Georgios Smyrnis, Vaishaal Shankar, Suchin Gururangan, Mitchell Wortsman, Rulin Shao, Jean Mercat, Alex Fang, Jeffrey Li, Sedrick Keh, Rui Xin, Marianna Nezhurina, Igor Vasiljevic, Jenia Jorissev, Alexandros G. Dimakis, Gabriel Ilharco, Shuran Song, Thomas Kollar, Yair Carmon, Achal Dave, Reinhard Heckel, Niklas Muennighoff, and Ludwig Schmidt. Language models scale reliably with over-training and on downstream tasks. _Preprint_, 2024.
* Nakkiran et al. (2021) Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep double descent: Where bigger models and more data hurt. _Journal of Statistical Mechanics: Theory and Experiment_, 2021(12):124003, 2021.
* Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In _NAACL-HLT (1)_, pages 4171-4186. Association for Computational Linguistics, 2019.
* Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining approach. _CoRR_, abs/1907.11692, 2019.
* Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. _Journal of machine learning research_, 21(140):1-67, 2020.
* Polu and Sutskever (2020) Stanislas Polu and Ilya Sutskever. Generative language modeling for automated theorem proving. _arXiv preprint arXiv:2009.03393_, 2020.
* Gunasekar et al. (2023) Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. _arXiv preprint arXiv:2306.11644_, 2023.
* Lee et al. (2021) Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. Deduplicating training data makes language models better. _arXiv preprint arXiv:2107.06499_, 2021.
* Kandpal et al. (2022) Nikhil Kandpal, Eric Wallace, and Colin Raffel. Deduplicating training data mitigates privacy risks in language models. In _International Conference on Machine Learning_, pages 10697-10707. PMLR, 2022.
* Tirumala et al. (2023) Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari Morcos. D4: Improving llm pretraining via document de-duplication and diversification. In _NIPS_, volume 36, 2023.
* Toutanova et al. (2021)* Xie et al. (2024) Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy S Liang. Data selection for language models via importance resampling. _Advances in Neural Information Processing Systems_, 36, 2024a.
* Albalak et al. (2024) Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, Colin Raffel, Shiyu Chang, Tatsunori Hashimoto, and William Yang Wang. A survey on data selection for language models, 2024.
* Xie et al. (2024) Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy S Liang, Quoc V Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining. _Advances in Neural Information Processing Systems_, 36, 2024b.
* Chen et al. (2024) Mayee Chen, Nicholas Roberts, Kush Bhatia, Jue Wang, Ce Zhang, Frederic Sala, and Christopher Re. Skill-it! a data-driven skills framework for understanding and training language models. _Advances in Neural Information Processing Systems_, 36, 2024.
* Ma et al. (2024) Yingwei Ma, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, and Shanshan Li. At which training stage does code data help LLMs reasoning? In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=KIPJKST4gw](https://openreview.net/forum?id=KIPJKST4gw).
* Li et al. (2023) Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning. _arXiv preprint arXiv:2308.12032_, 2023c.
* Liu et al. (2024) Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning. In _ICLR_, 2024.
* Li et al. (2023) Yunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang, Min Yang, Lei Zhang, Shuzheng Si, Junhao Liu, Tongliang Liu, Fei Huang, et al. One shot learning as instruction data prospector for large language models. _arXiv preprint arXiv:2312.10302_, 2023d.
* Xia et al. (2024) Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. Less: Selecting influential data for targeted instruction tuning. _arXiv preprint arXiv:2402.04333_, 2024.
* Kang et al. (2024) Feiyang Kang, Hoang Anh Just, Yifan Sun, Himanshu Jahagirdar, Yuanzhi Zhang, Rongxing Du, Anit Kumar Sahu, and Ruoxi Jia. Get more for less: Principled data selection for warming up fine-tuning in LLMs. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=QmYNBVukex](https://openreview.net/forum?id=QmYNBVukex).
* Qin et al. (2024) Ziheng Qin, Kai Wang, Zangwei Zheng, Jianyang Gu, Xiangyu Peng, Zhaopan Xu, Daquan Zhou, Lei Shang, Baigui Sun, Xuansong Xie, and Yang You. Infobatch: Lossless training speed up by unbiased dynamic data pruning. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=C61sk5Lsk6](https://openreview.net/forum?id=C61sk5Lsk6).
* Computer (2023) Together Computer. Redpajama: an open dataset for training large language models, 10 2023. URL [https://github.com/togethercomputer/RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data).
* Sener and Savarese (2017) Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set approach. _arXiv preprint arXiv:1708.00489_, 2017.
* Killamsetty et al. (2021) Krishnateja Killamsetty, Xujiang Zhao, Feng Chen, and Rishabh Iyer. Retrieve: Coreset selection for efficient and robust semi-supervised learning. _Advances in neural information processing systems_, 34:14488-14501, 2021.
* Loshchilov and Hutter (2015) Ilya Loshchilov and Frank Hutter. Online batch selection for faster training of neural networks. _arXiv preprint arXiv:1511.06343_, 2015.
* Schaul et al. (2015) Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. _arXiv preprint arXiv:1511.05952_, 2015.
* Chang et al. (2017) Haw-Shiuan Chang, Erik Learned-Miller, and Andrew McCallum. Active bias: Training more accurate neural networks by emphasizing high variance samples. _Advances in Neural Information Processing Systems_, 30, 2017.
* Katharopoulos and Fleuret (2018) Angelos Katharopoulos and Francois Fleuret. Not all samples are created equal: Deep learning with importance sampling. In _International conference on machine learning_, pages 2525-2534. PMLR, 2018.
* Jiang et al. (2019) Angela H Jiang, Daniel L-K Wong, Giulio Zhou, David G Andersen, Jeffrey Dean, Gregory R Ganger, Gauri Joshi, Michael Kaminksy, Michael Kozuch, Zachary C Lipton, et al. Accelerating deep learning by focusing on the biggest losers. _arXiv preprint arXiv:1910.00762_, 2019.
* Liu et al. (2019)* Song et al. (2020) Hwanjun Song, Minseok Kim, Sundong Kim, and Jae-Gil Lee. Carpe diem, seize the samples uncertain* at the moment* for adaptive batch selection. In _Proceedings of the 29th ACM International Conference on Information & Knowledge Management_, pages 1385-1394, 2020.
* Mindermann et al. (2022) Soren Mindermann, Jan M Brauner, Muhammed T Razzak, Mrinank Sharma, Andreas Kirsch, Winnie Xu, Benedikt Holten, Aidan N Gomez, Adrien Morisot, Sebastian Farquhar, and Yarin Gal. Prioritized training on points that are learnable, worth learning, and not yet learnt. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, _Proceedings of the 39th International Conference on Machine Learning_, volume 162 of _Proceedings of Machine Learning Research_, pages 15630-15649. PMLR, 17-23 Jul 2022. URL [https://proceedings.mlr.press/v162/mindermann22a.html](https://proceedings.mlr.press/v162/mindermann22a.html).
* Fan and Jaggi (2023) Simin Fan and Martin Jaggi. Irreducible curriculum for language model pretraining. _arXiv preprint arXiv:2310.15389_, 2023.
* Kaddour et al. (2023) Jean Kaddour, Oscar Key, Piotr Nawrot, Pasquale Minervini, and Matt Kusner. No train no gain: Revisiting efficient training algorithms for transformer-based language models. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023. URL [https://openreview.net/forum?id=thbKgJ8gNK](https://openreview.net/forum?id=thbKgJ8gNK).
* Coleman et al. (2019) Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy Liang, Jure Leskovec, and Matei Zaharia. Selection via proxy: Efficient data selection for deep learning. _arXiv preprint arXiv:1906.11829_, 2019.
* Engstrom et al. (2024) Logan Engstrom, Axel Feldmann, and Aleksander Madry. Dsdm: Model-aware dataset selection with datamodels. _arXiv preprint arXiv:2401.12926_, 2024.
* Oren et al. (2019) Yonatan Oren, Shiori Sagawa, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust language modeling. _arXiv preprint arXiv:1909.02060_, 2019.
* LeCun et al. (1998) Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. _Proceedings of the IEEE_, 86(11):2278-2324, 1998.
* Socher et al. (2013) Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In _Proceedings of the 2013 conference on empirical methods in natural language processing_, pages 1631-1642, 2013.
* Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04805_, 2018.
* Gu et al. (2020) Yuxian Gu, Zhengyan Zhang, Xiaozhi Wang, Zhiyuan Liu, and Maosong Sun. Train no evil: Selective masking for task-guided pre-training. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, _Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)_, pages 6966-6974, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.566. URL [https://aclanthology.org/2020.emnlp-main.566](https://aclanthology.org/2020.emnlp-main.566).
* Lad et al. (2022) Tanish Lad, Himanshu Maheshwari, Shreyas Kottukkal, and Radhika Mamidi. Using selective masking as a bridge between pre-training and fine-tuning. _arXiv preprint arXiv:2211.13815_, 2022.
* Zhong et al. (2021) Qihuang Zhong, Liang Ding, Juhua Liu, Xuebo Liu, Min Zhang, Bo Du, and Dacheng Tao. Revisiting token dropping strategy in efficient BERT pretraining. In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, _Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 10391-10405, Toronto, Canada, July 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.579. URL [https://aclanthology.org/2023.acl-long.579](https://aclanthology.org/2023.acl-long.579).
* Hou et al. (2022) Le Hou, Richard Yuanzhe Pang, Tianyi Zhou, Yuexin Wu, Xinying Song, Xiaodan Song, and Denny Zhou. Token dropping for efficient BERT pretraining. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, _Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)_, pages 3774-3784, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.262. URL [https://aclanthology.org/2022.acl-long.262](https://aclanthology.org/2022.acl-long.262).
* Li et al. (2023) Tianjian Li, Haoran Xu, Philipp Koehn, Daniel Khashabi, and Kenton Murray. Error norm truncation: Robust training in the presence of data noise for text generation models. _arXiv preprint arXiv:2310.00840_, 2023e.
* Rumbelow and Watkins (2023) Jessica Rumbelow and Matthew Watkins. Solidgoldmagikarp (plus, prompt generation). LessWrong, 2023. URL [https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLag/solidgoldmagikarp-plus-prompt-generation](https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLag/solidgoldmagikarp-plus-prompt-generation).
* Land and Bartolo (2024) Sander Land and Max Bartolo. Fishing for magikarp: Automatically detecting under-trained tokens in large language models. _arXiv preprint arXiv:2405.05417_, 2024.
* Li et al. (2020)Naomi Saphra and Adam Lopez. Understanding learning dynamics of language models with svcca. _arXiv preprint arXiv:1811.00225_, 2018.
* Choshen et al. [2021] Leshem Choshen, Guy Hacohen, Daphna Weinshall, and Omri Abend. The grammar-learning trajectories of neural language models. _arXiv preprint arXiv:2109.06096_, 2021.
* Liu et al. [2021] Leo Z Liu, Yizhong Wang, Jungo Kasai, Hannaneh Hajishirzi, and Noah A Smith. Probing across time: What does roberta know and when? _arXiv preprint arXiv:2104.07885_, 2021.
* Power et al. [2022] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Generalization beyond overfitting on small algorithmic datasets. _arXiv preprint arXiv:2201.02177_, 2022.
* Xia et al. [2022] Mengzhou Xia, Mikel Artetxe, Chunting Zhou, Xi Victoria Lin, Ramakanth Pasunuru, Danqi Chen, Luke Zettlemoyer, and Ves Stoyanov. Training trajectories of language models across scales. _arXiv preprint arXiv:2212.09803_, 2022.
* Hernandez et al. [2021] Danny Hernandez, Jared Kaplan, Tom Henighan, and Sam McCandlish. Scaling laws for transfer. _arXiv preprint arXiv:2102.01293_, 2021.
* Hoffmann et al. [2022] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. _arXiv preprint arXiv:2203.15556_, 2022.
* Wei et al. [2022] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models. _arXiv preprint arXiv:2206.07682_, 2022b.
* Isik et al. [2024] Berivan Isik, Natalia Ponomareva, Hussein Hazimeh, Dimitris Paparas, Sergei Vassilvitskii, and Sanmi Koyejo. Scaling laws for downstream task performance of large language models. _arXiv preprint arXiv:2402.04177_, 2024.
* Tirumala et al. [2022] Kushal Tirumala, Aram Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. Memorization without overfitting: Analyzing the training dynamics of large language models. _Advances in Neural Information Processing Systems_, 35:38274-38290, 2022.
* Carlini et al. [2022] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across neural language models. _arXiv preprint arXiv:2202.07646_, 2022.
* Henighan et al. [2023] Tom Henighan, Shan Carter, Tristan Hume, Nelson Elhage, Robert Lasenby, Stanislav Fort, Nicholas Schiefer, and Christopher Olah. Superposition, memorization, and double descent. _Transformer Circuits Thread_, 2023.
* Biderman et al. [2024] Stella Biderman, USVSN PRASHANTH, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, and Edward Raff. Emergent and predictable memorization in large language models. _Advances in Neural Information Processing Systems_, 36, 2024.
* Hernandez et al. [2022] Danny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, et al. Scaling laws and interpretability of learning from repeated data. _arXiv preprint arXiv:2205.10487_, 2022.
* Xue et al. [2024] Fuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, and Yang You. To repeat or not to repeat: Insights from scaling llm under token-crisis. _Advances in Neural Information Processing Systems_, 36, 2024.
* Goodhart and Goodhart [1984] Charles AE Goodhart and CAE Goodhart. _Problems of monetary management: the UK experience_. Springer, 1984.
* Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. _Advances in neural information processing systems_, 35:27730-27744, 2022.
* Cobbe et al. [2021] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reitichro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL [https://arxiv.org/abs/2110.14168](https://arxiv.org/abs/2110.14168).
* Hendrycks et al. [2021] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In _NIPS_, 2021.
* Gao et al. [2022] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. _arXiv preprint arXiv:2211.10435_, 2022.
* Huang et al. [2022]Arkil Patel, Satwik Bhattacharya, and Navin Goyal. Are NLP models really able to solve simple math word problems? In _Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 2080-2094, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.168. URL [https://aclanthology.org/2021.naacl-main.168](https://aclanthology.org/2021.naacl-main.168).
* Miao et al. (2020) Shen-yun Miao, Chao-Chun Liang, and Keh-Yih Su. A diverse corpus for evaluating and developing English math word problem solvers. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 975-984, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.92. URL [https://aclanthology.org/2020.acl-main.92](https://aclanthology.org/2020.acl-main.92).
* Koncel-Kedziorski et al. (2016) Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS: A math word problem repository. In _Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies_, pages 1152-1157, San Diego, California, June 2016. Association for Computational Linguistics. doi: 10.18653/v1/N16-1136. URL [https://aclanthology.org/N16-1136](https://aclanthology.org/N16-1136).
* Lu et al. (2023) Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. In _The Eleventh International Conference on Learning Representations_, 2023. URL [https://openreview.net/forum?id=DByHHRBJUTN](https://openreview.net/forum?id=DByHHRBJUTN).
* Amini et al. (2019) Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. _arXiv preprint arXiv:1905.13319_, 2019.
* Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. _arXiv preprint arXiv:2009.03300_, 2020.
* Suzgun et al. (2022) Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. _arXiv preprint arXiv:2210.09261_, 2022.
* Zhong et al. (2023b) Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Ageival: A human-centric benchmark for evaluating foundation models. _arXiv preprint arXiv:2304.06364_, 2023b.
* Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. _arXiv preprint arXiv:1803.05457_, 2018.
* Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. _arXiv preprint arXiv:1905.10044_, 2019.
* Bisk et al. (2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical commonsense in natural language. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 7432-7439, 2020.
* Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? _arXiv preprint arXiv:1905.07830_, 2019.
* Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winograde: An adversarial winograd schema challenge at scale. _Communications of the ACM_, 64(9):99-106, 2021.
* Mihaylov et al. (2018) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. _arXiv preprint arXiv:1809.02789_, 2018.
* Zheng et al. (2023) Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Lei Shen, Zihan Wang, Andi Wang, Yang Li, et al. Codegeex: A pre-trained model for code generation with multilingual benchmarking on humaneval-x. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 5673-5684, 2023.
* Clark et al. (2020) Jonathan H Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev, and Jennimaria Palomaki. Tydi qa: A benchmark for information-seeking question answering in ty pologically di verse languages. _Transactions of the Association for Computational Linguistics_, 8:454-470, 2020.
* Zellers et al. (2019)Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. _arXiv preprint arXiv:2108.07732_, 2021.
* Guo et al. (2024) Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y Wu, YK Li, et al. Deepseek-coder: When the large language model meets programming-the rise of code intelligence. _arXiv preprint arXiv:2401.14196_, 2024.
* Li et al. (2023) Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization. In _ACL (1)_, pages 12286-12312. Association for Computational Linguistics, 2023f.
* Wan et al. (2024) Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, and Shuming Shi. Knowledge fusion of large language models. In _The Twelfth International Conference on Learning Representations_, 2024. URL [https://openreview.net/forum?id=j1b8k12qcz](https://openreview.net/forum?id=j1b8k12qcz).
* Fu et al. (2023) Yao Fu, Hao Peng, Liu Ou, Ashish Sabharwal, and Tushar Khot. Specializing smaller language models towards multi-step reasoning. In _International Conference on Machine Learning_, pages 10421-10430. PMLR, 2023.

**Appendix**

###### Contents

* A Author Contributions
* B Related Works
* B.1 Pretraining Data Optimization
* B.2 Data Selection
* B.3 Language Model Training Dynamics
* B.4 Scaling Laws
* C Limitations and Future Work
* D Analysis and Visualization of Tokens in Pretraining
* D.1 More Details of Four Categories Tokens
* D.2 Non-Converging Tokens in Pretrainig
* E Evalution Details
* E.1 Math Evalution
* E.2 General Evaluation
* F Relate the Selected Tokens' Loss to Downstream Task Performance
* G Examples of Tokens Selected by SLM
* G.1 Token Selected Examples
* G.2 Dynamic Token Selected
* H Self-Reference Setting
* I Weak-to-Strong Generalization

Author Contributions

Zhenghao Lin designed and implemented detailed token selection process, conducted extensive preliminary experiments, developed the pre-training and evaluation pipeline, conducted most of the pre-training experiments and analysis, implemented baselines, and significantly contributed to the writing. Zhibin Gou presented a preliminary proposal, introduced the method of using excess loss for reweighting tokens, compiled high-quality corpora, trained reference models, set up the fine-tuning and evaluation pipelines, designed the experimental analysis, and significantly contributed to the writing. Yeyun Gong proposed the initial project and co-led the project with Weizhu Chen, they offered extensive advice and guidance on experiments and writing, and oversaw team collaboration and resource management. Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, and Nan Duan offered research mentorship, coordinated the project, and contributed to the writing.

## Appendix B Related Works

### Pretraining Data Optimization

The objective of optimizing pre-training corpora is to maximize the performance and efficiency of language model training by improving the quality and scale of the pretrain data mixture. This includes data collecting through crawling (Raffel et al., 2020) or synthesis (Polu and Sutskever, 2020; Gunasekar et al., 2023), de-duplication (Lee et al., 2021; Kandpal et al., 2022; Tirumala et al., 2023), filtering and selection (Xie et al., 2024; Albalak et al., 2024), as well as data composition (Xie et al., 2024) and curriculum (Chen et al., 2024; Ma et al., 2024).

### Data Selection

Data selection for fine-tuning has been extensively studied, focusing on improving quality (Li et al., 2023), diversity (Liu et al., 2024), and distribution matching (Li et al., 2023; Xia et al., 2024; Ni et al., 2024; Kang et al., 2024). For pretraining, various lightweight filters are utilized (Albalak et al., 2024), including heuristic-based (_e.g._, language and item count filtering), classifier-based (Brown et al., 2020), and perplexity or loss-based approaches (Wenzek et al., 2019; Qin et al., 2024). The massive public RedPajama-Data-v2 dataset (Computer, 2023), for example, leverages over 40 quality indicators for data filtering and reweighting. Nevertheless, strict filtering like blocklist (Raffel et al., 2020) and Safety API filtering (Welbl et al., 2021), have been found to hurt evaluation loss or induce bias (Dodge et al., 2021).

Sample-level selection has been extensively studied in previous research (Sener and Savarese, 2017; Killamsetty et al., 2021), particularly through online batch selection (Loshchilov and Hutter, 2015; Schaul et al., 2015; Chang et al., 2017; Katharopoulos and Fleuret, 2018; Jiang et al., 2019). These approaches have been applied to various classification tasks (Song et al., 2020; Mindermann et al., 2022) and language modeling (Fan and Jaggi, 2023). However, Kaddour et al. (2023) find that batch selection is not computationally efficient.

Many previous works have employed the general idea of using a reference model as a proxy for data selection. For instance, _Selection Via Proxy_ trains a proxy model to select samples with high uncertainty (Coleman et al., 2019). Xie et al. (2024) and Engstrom et al. (2024) utilize n-gram models or datamodels with a target dataset to estimate importance weights. Additionally, Xie et al. (2024) optimize the worst-case excess loss (Oren et al., 2019) relative to a reference model to determine domain weights. One of SLM's scoring functions is excess loss, and the most relevant work related to excess loss is RHO-LOSS (Mindermann et al., 2022), which trains a small model on a holdout set and uses the difference between training loss and holdout loss to select in-batch samples. Although excess loss is mathematically identical to RHO-LOSS, SLM differs in three important ways: 1) The focus is distinct. Motivated by the training dynamics of token loss, the core idea of SLM is to select useful tokens for pre-training. Its score functions are highly flexible and not limited to excess loss (see Appendix H for other functions). In contrast, RHO-LOSS aims to mathematically derive a reducible holdout loss to minimize generalization loss. 2) The meaning and training procedure of the proxy model are different. SLM trains a reference model on high-quality data to reflect the desired data distribution, whereas RHO-LOSS trains a small model on a random holdout set. 3) The selection scale and granularity vary. RHO-LOSS selects sample-level data on a small scale (typically 1K-1M samples) for task-specific fine-tuning tasks such as MNIST (LeCun et al., 1998) and SST-2 (Socheret al., 2013). In contrast, SLM conducts fine-grained token-level selection on large-scale language model pre-training, involving up to 80B tokens.

Token-level training strategies have also been explored, especially for the pre-training of BERT-like models using Masked Language Modeling (MLM) (Devlin et al., 2018). Specifically, "selective masking" involves masking important tokens in the input to focus on learning tokens that are more relevant to downstream tasks (Gu et al., 2020; Lad et al., 2022), whereas "token dropping" aims to reduce training costs by omitting less important tokens (Zhong et al., 2023; Hou et al., 2022). (Li et al., 2023) assesses the quality of each token based on the skewness of its predicted distribution and truncates the noisy tokens during training. Additionally, some research has approached the analysis and detection of under-trained tokens from a tokenization perspective (Rumbelow and Watkins, 2023; Land and Bartolo, 2024). To our knowledge, we are the first to explore token-level data selection for large language model training, aimed at enhancing data quality and information density at the most fundamental granularity.

### Language Model Training Dynamics

Investigating the training dynamics of language models is essential for understanding their behavior throughout the training process. This research includes studying internal representations (Saphra and Lopez, 2018), the acquisition of linguistic knowledge (Choshen et al., 2021; Liu et al., 2021), and the phenomenon of groking (Power et al., 2022). The analysis by Xia et al. (2022) is the most related to ours, which examines token-level training trajectories in models of varying sizes. Our findings, however, diverge from those of Xia et al. (2022), who posit that tokens with little change in perplexity are "already learned". We identify a spectrum of token patterns, including "easy tokens" and "hard tokens" that resist convergence. Recognizing this, we propose a method of selective language modeling that targets the influential tokens, optimizing the learning process.

### Scaling Laws

Scaling laws guide us in discovering the impact of factors such as parameter count, data size, and compute on language model performance and behavior. These studies usually focus on predicable scaling though power law (Kaplan et al., 2020; Hernandez et al., 2021), optimal resource allocation (Hoffmann et al., 2022), downstream tasks (Wei et al., 2022; Isik et al., 2024; Gadre et al., 2024), architectures (Tay et al., 2022), memorization (Tirumala et al., 2022; Carlini et al., 2022; Henighan et al., 2023; Biderman et al., 2024), and repeating data (Hernandez et al., 2022; Muennighoff et al., 2024; Xue et al., 2024). Most scaling laws on model performance study cross-entropy loss on all training tokens, while we focus on the tokens loss of desired distributions.

## Appendix C Limitations and Future Work

GeneralizabilityIn math continual pretraining, as depicted in Figure 6, training exclusively with SLM leads to quickly convergence to the domain focused by the reference model, accompanied by a significant rise in the loss of unselected tokens. Although no adverse effects, like biases, have been observed from the increased loss yet, a general pretraining loss on text and code may prevent overfitting (Goodhart and Goodhart, 1984), as suggested by Ouyang et al. (2022) and Azerbayev et al. (2023). Furthermore, future efforts could broaden the corpus scope of the reference model, and enlarge the pretraining data size, as exemplified by DeepSpeedMath (Shao et al., 2024).

ScalabilityDue to budget constraints, we have only verified the effectiveness of our method on smaller models (<=7B parameters) and smaller datasets (<100B tokens). Smaller models benefit significantly from removing the loss of irrelevant tokens and focusing on important ones. However, it's possible that very large models trained on extensive corpora may naturally develop this inductive bias to compress useful data (_i.e._, compressing everything), although it may sounds inefficient for now. Therefore, future works should study whether this selective language modeling technique can scale to very large models and data (Kaplan et al., 2020).

Is training a reference model necessary?To score tokens, we need a high-quality reference model. This could be a base model trained with a small amount of high-quality data, or a performant open-source model. In fact, since we only need input logprobs or perplexity from reference model,we could even utilize more powerful proprietary model APIs. We can input tokens and use the log probabilities of the input returned by the API as reference scores. We leave this for future works.

How to improve upon SLM?There are many natural extensions of SLM, _e.g.,_ reweighting tokens instead of selecting may improve robustness; using a reference model as a reward model to guide pretraining with reinforcement learning; adopting multiple reference models to reduce overfitting; designing token-level curriculum learning and iterative strategies for continuous improvements, _etc_.

Expanding the use of SLMSLM may be extended to supervised fine-tuning to address the noise and distribution mismatches in many SFT datasets. Another potential application is alignment, _e.g.,_ by training a reference model to emphasize helpfulness, truthfulness, and harmlessness, we may obtain a base model that is natively aligned during the pretraining stage. Meanwhile, we believe that the idea of SLM may find broader applications in multimodal data such as images, videos, and speech, which have a high noise-to-information ratio than text.

## Appendix D Analysis and Visualization of Tokens in Pretraining

### More Details of Four Categories Tokens

We categorize tokens into four categories: H\(\rightarrow\)H, L\(\rightarrow\)H, H\(\rightarrow\)L, L\(\rightarrow\)L. During the training process, we collected the loss of each token after training on each 1 billion tokens training data. We then used linear fitting and took the difference in loss between the first and last points as evidence of whether the loss decreased during the training process.

Specifically, suppose we have a sequence of token's loss \((l_{0},l_{1},...,l_{n})\). Our goal is to minimize the sum of the squares of the differences between each data point and its linear predictive value:

\[f(a,b)=\text{minimize}\sum_{i=0}^{n}(l_{i}-(ax_{i}+b))^{2}, \tag{6}\]

where \(x_{0}=0\) is the initial checkpoint and \(x_{n}=n\) is the final checkpoint. Substituting these into the fitted equation, we can obtain the Loss values at the start and end after fitting: \(\mathcal{L}_{\text{start}}=b\) and \(\mathcal{L}_{\text{end}}=an+b\). The change in loss can then be expressed as: \(\Delta\mathcal{L}=\mathcal{L}_{\text{end}}-\mathcal{L}_{\text{start}}\). Meanwhile, we represent the average Loss of the last checkpoint as \(\mathcal{L}_{\text{mean}}\).

Next, we can classify the tokens based on \(\Delta\mathcal{L}\) and the \(\mathcal{L}_{\text{mean}}\). We categorize tokens with \(\Delta\mathcal{L}<-0.2\) as H\(\rightarrow\)L (loss decreases from high to low) category tokens, and tokens with \(\Delta\mathcal{L}>0.2\) as L\(\rightarrow\)H (loss increases from low to high) category tokens. If \(-0.2\leq\Delta\mathcal{L}\leq 0.2\) and \(l_{n}\leq\mathcal{L}_{\text{mean}}\), then tokens are classified as L\(\rightarrow\)L (loss remains low); if \(l_{n}>\mathcal{L}_{\text{mean}}\), they are classified as H\(\rightarrow\)H (loss remains high). In Figure 10, we have added the tokens' loss curves of the 7B model which is consistent with the other experimental settings in SS2.1, for readers to refer to whether similar phenomena exist on larger models. In Figure 11, we visualize examples of the four categories of tokens in actual text.

Figure 10: **The loss of four categories of tokens during Mistral-7B pretraining on OpenWebMath.** (a) shows the loss of H\(\rightarrow\)H, L\(\rightarrow\)H, H\(\rightarrow\)L, and L\(\rightarrow\)L tokens during pretraining. (b) and (c) show three cases of fluctuating tokens’ loss in L\(\rightarrow\)L and H\(\rightarrow\)H during pretraining, respectively.

### Non-Converging Tokens in Pretraining

In SS2.1, we mentioned that during the training process, only a minority of tokens belong to the H\(\rightarrow\)L category. Among the remaining categories of H\(\rightarrow\)H and L\(\rightarrow\)L tokens, there are tokens that exhibit significant fluctuations during training. Furthermore, there are instances where H\(\rightarrow\)L tokens are not effectively learned. Therefore, in our analysis, we specifically select those tokens from these categories that demonstrate considerable variability and distinct loss. We visualize these tokens that exhibit abnormal behavior during the training process. As illustrated in Figure 12, we find that the majority of these tokens originate from rather chaotic corpora. For instance, the corpora may include a mix of custom symbols, unintelligible gibberish, and information such as timetables and bibliographic references. Within a segment of normal text, there may also be fluctuations in the usage of common conjunctions, word suffixes, and punctuation marks. The latter may not necessarily be disastrous for training; in fact, it could represent a normal occurrence. However, if we can effectively mitigate the losses caused by the former, it might lead to more stable and efficient model training.

## Appendix E Evalution Details

### Math Evalution

We conducted a comprehensive evaluation of the model across various math reasoning benchmarks, encompassing a range of difficulties from elementary to university level, multiple mathematical domains, and diverse question types including multiple-choice and open-ended questions. Our benchmarks include GSM8k [Cobbe et al., 2021], MATH [Hendrycks et al., 2021], GSM-Hard [Gao et al., 2022], SVAMP [Patel et al., 2021], ASDIV [Miao et al., 2020], MAWPS [Koncel-Kedziorski et al., 2016], TabMWP (TAB) [Lu et al., 2023], MathQA (MQA) [Amini et al., 2019], MMLU-STEM [Hendrycks et al., 2020], and SAT [Azerbayev et al., 2023].

### General Evalution

In the evaluation of general domain, we followed the lm-evaluation-harness [Gao et al., 2023] and evalute model on MMLU [Hendrycks et al., 2020], BBH [Suzgun et al., 2022], AGIEval [Zhong et al., 2023b], ARC-Easy and ARC-Challenge [Clark et al., 2018], BoolQ [Clark et al., 2019], PIQA [Bisk et al., 2020], Hellaswag [Zellers et al., 2019], WinoGrande [Sakaguchi et al., 2021], OpenBookQA [Mihaylov et al., 2018]. On HumanEval [Zheng et al., 2023] and TydiQA [Clark et al., 2020], we follow the evaluation pipeline of open-instructur [Ivison et al., 2023] and report Pass@1 and Pass@10 for HumanEval and F1 for TydiQA. For MBPP [Austin et al., 2021] benchmark, we follow the evaluation pipeline of DeepSeek-Coder [Guo et al., 2024], and report Pass@1 and Pass@10.

## Appendix F Relate the Selected Tokens' Loss to Downstream Task Performance

In this section, we declare the details about correlating the loss of selected tokens with the performance of downstream tasks. Concurrent study has explored similar methods to study the impact of scaling laws with the performance of models in downstream tasks [Gadre et al., 2024]. Our analysis here differs in that it aims to elucidate the relationship between the decrease/increase in loss for selected/unselected tokens and the model's performance on downstream tasks.

We use the average accuracy of MATH and GSM8K as the standard for measuring downstream tasks performance of model. Based on the trend of data points in Figure 7, we propose the relationship between the average accuracy of downstream tasks and selected/unselected tokens' loss,

\[Acc(\mathcal{L})=\log(a*\mathcal{L}+c) \tag{7}\]

The parameters \(a\) and \(c\) are fitted from the data. If the loss of selected tokens \(\mathcal{L}_{s}\) is used for fitting, then \(a>0\). Conversely, if the loss of unselected tokens \(\mathcal{L}_{us}\) is used for fitting, then \(a<0\). Therefore, we believe that training the model on selected tokens can effectively improve its performance on downstream tasks, while unselected tokens may have a detrimental effect on the model's performance in downstream tasks.

[MISSING_PAGE_FAIL:23]

[MISSING_PAGE_FAIL:24]

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: In the abstract and SS1, we clearly demonstrate the contribution and scope of this paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: In Appendix C, we have thoroughly discussed the limitations of our article, hoping to guide more future work. Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?Answer: [Yes] Justification: In SS2.1 and SS2.2, we elaborated on the motivation and theoretical derivation of our method, with a complete proof process in place. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We have provided detailed descriptions of the experimental setup in SS3.1 and methods in SS2.2 to ensure that our experiment can be reproduced. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: This may be temporary, and we are working hard to promote the process of open source. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: In SS3.1 and Appendix E, we clearly demonstrated various experimental settings, including hyperparameters, model settings, training settings, evaluation settings, etc. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Due to the high cost of pre-training and the significant results obtained across various settings, we do not repeat the same experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.

* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: In SS3.1, we have provided sufficient information on the computer resources needed to reproduce the experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: We guarantee that the research conducted in the paper complies with NeurIPS Code of Ethics in all aspects. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: The purpose of this paper is to improve the training process of large language models, without any negative societal impacts. Guidelines: ** The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no such risks. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: The creators or original owners of the assets used in the paper, such as code, data, and models, have been appropriately recognized, and the licenses and terms of use have been clearly mentioned and properly respected. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.

**Examples of Four Categories of Tokens**

GMAT 1: 670 Q49 V31 un GMAT 2: 710 Q50 V35 un Followers: 175 un un Kudos [7]: 890 [0], given: 235 un un Re: Mr, and Mrs Wiley, VIC[#permink] 13 Feb 2010, 01:03 un Ans A un their first child was born after J years... un un thus 1 child --> j years un un un => thus after another J years his age = J un un thus his age is J -> after 2J years and 2J after 3J years un un his present age is T which is after T years. Un thus total time after 2years will be T+2 un since aller every J year they have a child after T+2 they will have ((T+2)[J] + 1 (+1 is for the oldest) un thus A un un un right for your dreams :For all those who fear from Verbal-t is give it a fight un un Money Saved is the Money Earned in un Jo Bole So Nihaal, Sat Shri Afsah Akal un Un Gmat test review : Un 670-to-710-a-long-journey-without-destination-still-happy-141642.html un un Intern un Joined: 06 Apr 2012 un Posts: 28 un Followers: 0 un un Kudos [7]: 4 [0], given: 37 un un Re: Mr. and Mrs Wiley, VIC[#permink] 21 Nov 2012, 07:46 un jeteshsing wrote: Un Need the solution using Algebra... un un Mr. & Mrs Wiley have a child every J years. Their oldest child is now T years ago. If they have a child 2 years from now, how many children will they have in un (A) frac(T+2)[J] + 1 un un (B) T+1 un un (C) frac(J)[T] + frac(1)[T] un un (D) TJ - 1 un un (E) frac(T+J)[J] un (E) frac(T+J)[J] un (E) frac(T+J)[J] un (E) Womer's Manhattan Guide un Bumbel - would really appreciate you providing your bit on solving the original problem above algebraically. The problem and various explanations remain confusing. Should we think of it as a progression or some other way? Please share your take. Thank you. un Veritas Prep GMAT Instructor un Joined: 16 Oct 2010 un Posts: 4566 un Location: Pune, India un Followers: 1029 un un Kudos [7]: 4460 [1], given: 162 un un Re: Mr. and Mrs Wiley, VIC[#permink] 21 Nov 2012, 09:45 un 1 N CUDOS in Expert's post in jeteshsing wrote: Un Need the solution using Algebra... un un Mr. & Mrs Wiley have a child every J years. Their oldest child is now T years old. If they have a child 2 years from now, how many children will they have in total? un un (A) frac(T+2)[J] + 1 un un (B) T+1 un un (C) frac(J)[T] +frac(1)

## 10.11. The _Thesis_

Figure 12: **An example of an abnormal state of token perplexity during pretrainig process.** The tokens highlighted in orange represent tokens that were significant abnormalities during the pretraining process.

[MISSING_PAGE_FAIL:34]

### After Training 0% Checkpoint

Figure 14: **An example of dynamic token selection changes during the training process, which illustrated with five different score levels represented by deep blue, light blue, black, light orange, and dark orange. The bluer the color indicates a higher tendency for the token to be selected, while the more orange the color suggests a lower tendency for the token to be selected.**