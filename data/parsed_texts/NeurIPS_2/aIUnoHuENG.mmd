# Feature Adaptation for Sparse Linear Regression

 Jonathan A. Kelner

MIT

kelner@mit.edu. This work was supported in part by NSF Large CCF-1565235, NSF Medium CCF-1955217, and NSF TRIPODS 1740751.

Frederic Koehler

fkoehler@stanford.edu. This work was supported in part by NSF award CCF-1704417, NSF award IIS-1908774, and N. Anari's Sloan Research Fellowship

Raghu Meka

raghum@cs.ucla.edu. This work was supported in part by NSF CAREER Award CCF-1553605 and NSF Small CCF-2007682

Dhruv Rohatgi

MIT

###### Abstract

Sparse linear regression is a central problem in high-dimensional statistics. We study the correlated random design setting, where the covariates are drawn from a multivariate Gaussian \(N(0,\Sigma)\), and we seek an estimator with small excess risk.

If the true signal is \(t\)-sparse, information-theoretically, it is possible to achieve strong recovery guarantees with only \(O(t\log n)\) samples. However, computationally efficient algorithms have sample complexity linear in (some variant of) the _condition number_ of \(\Sigma\). Classical algorithms such as the Lasso can require significantly more samples than necessary even if there is only a single sparse approximate dependency among the covariates.

We provide a polynomial-time algorithm that, given \(\Sigma\), automatically adapts the Lasso to tolerate a small number of approximate dependencies. In particular, we achieve near-optimal sample complexity for constant sparsity and if \(\Sigma\) has few "outlier" eigenvalues. Our algorithm fits into a broader framework of _feature adaptation_ for sparse linear regression with ill-conditioned covariates. With this framework, we additionally provide the first polynomial-factor improvement over brute-force search for constant sparsity \(t\) and arbitrary covariance \(\Sigma\).

## 1 Introduction

Sparse linear regression is a fundamental problem in high-dimensional statistics. In a natural random design formulation of this problem, we are given \(m\) independent and identically distributed samples \((X_{i},y_{i})_{i=1}^{m}\) where each sample's covariates are drawn from an \(n\)-dimensional Gaussian random vector \(X_{i}\sim N(0,\Sigma)\), and each response is \(y_{i}=\langle X_{i},v^{*}\rangle+\xi_{i}\) for independent noise \(\xi_{i}\sim N(0,\sigma^{2})\) and a \(t\)-sparse ground truth regressor \(v^{*}\in\mathbb{R}^{n}\), where \(t\) is much smaller than \(n\). The goal1 is to output a vector \(\hat{v}\in\mathbb{R}^{n}\) for which the _excess risk_

Footnote 1: More generally, from a learning theory perspective, we could consider an arbitrary improper learner outputting a function \(\hat{f}(X_{0})\), rather than specifically learning a linear function \(\langle X_{0},\hat{v}\rangle\). At least when \(\Sigma\) is known, there is no advantage as we can always project \(\hat{f}\) onto the space of linear functions.

\[\mathbb{E}(\langle X_{0},\hat{v}\rangle-y_{0})^{2}-\sigma^{2}=(\hat{v}-v^{*})^ {\top}\Sigma(\hat{v}-v^{*})=:\|\hat{v}-v^{*}\|_{\Sigma}^{2}\]

is as small as possible, where \((X_{0},y_{0})\) is an independent sample from the same model.

Without the sparsity assumption, the number of samples needed to achieve small excess risk (say, \(O(\sigma^{2})\)) is linear in the dimension; with \(O(n)\) samples, simple and computationally efficient algorithms such as ordinary least squares achieve the statistically optimal excess risk \(O\left(\frac{\sigma^{2}n}{m}\right)\). Sparsityallows for a significant statistical improvement: ignoring computational efficiency, it is well known that there is an estimator \(\hat{v}\) with excess risk \(O(\frac{\sigma^{2}t\log n}{m})\) as long as \(m=\Omega(t\log n)\) (see e.g. [13, 33]; Theorem 4.1 in [23]).

The catch is that computing this estimator involves a brute-force search over \(\binom{n}{t}\) possibilities (i.e., the possible supports for \(v^{*}\)). At first glance, this combinatorial search may seem unavoidable if we wish to take advantage of sparsity. Indeed, similar problems are notoriously difficult: the only non-trivial algorithms for e.g., learning \(t\)-sparse parities with noise still require \(n^{\Omega(t)}\) time [29, 37]. However, it is a celebrated fact that for sparse linear regression, computationally efficient methods such as Lasso and Orthogonal Matching Pursuit can avoid this combinatorial search and still achieve very strong theoretical guarantees under conditions such as the Restricted Isometry Property (see e.g. [7, 10, 5, 4, 3, 1]). In the random design setting we consider, the Lasso is known to achieve optimal statistical rates (up to constants) when the covariance matrix \(\Sigma\) is _well-conditioned_[32, 46].

What about when \(\Sigma\) is ill-conditioned? In contrast with the statistically optimal estimator, Lasso and its cousins provably _require_ sample complexity scaling with (some variant of) the condition number of \(\Sigma\) (see e.g. Theorem 14 in [38] or Theorem 6.5 in [23]). And with a few exceptions (e.g., in some settings with special graphical structure [23]) there has been little progress on designing new efficient algorithms for sparse linear regression with ill-conditioned \(\Sigma\) (see Section 4 for further discussion). For a general covariance \(\Sigma\), no algorithm is even known that can achieve sample complexity \(f(t)\cdot n^{1-\epsilon}\) (for an arbitrary function \(f\)) without brute-force search.

A computationally efficient algorithm that approaches the optimal statistical rate for _arbitrary_\(\Sigma\) might be too much to hope for. While no computational lower bounds are known, even in restricted computational models such as the Statistical Query model,6 the related _worst-case_ problem of finding a \(t\)-sparse solution to a system of linear equations requires \(n^{\Omega(t)}\) time under standard complexity assumptions [15]. So it is plausible, though not certain, that some assumptions on \(\Sigma\) are necessary. In this work - inspired by a long tradition (in random matrix theory, statistics, graph theory, and other areas) of studying matrices with a spectrum that is split between a large "bulk" and a small number of outlier "spike" eigenvalues [28, 39, 43] - we identify a broad generalization of the standard well-conditionedness assumption, under which brute-force search can still be avoided.

Footnote 6: There are lower bounds for a family of regression estimators with coordinate-separable regularization [44] and a family of “preconditioned-Lasso” estimators [23, 24].

### Beyond well-conditioned \(\Sigma\)

Say that \(\Sigma\) has eigenvalues \(\lambda_{1}\leq\cdots\leq\lambda_{n}\), and that the sparsity \(t\) is a constant.7 Then standard bounds for Lasso require sample complexity \((\lambda_{n}/\lambda_{1})\cdot O(\log n)\). But if the covariates contain even a single approximate linear dependency, then \(\lambda_{n}/\lambda_{1}\) may be arbitrarily large. Moreover, if the dependency is sparse (e.g. two covariates are highly correlated), then there is a natural choice of \(v^{*}\) for which Lasso provably fails (see Theorem 6.5 of [23]). Indeed, this phenomenon is not just a limitation of the analysis; Lasso fails empirically as well, even for very small \(t\) (see Figure 2 in Appendix H for a simple example with \(t=3\)).

Footnote 7: Note that for moderate-sized datasets (e.g. \(n=1000\)), brute-force search is infeasible even for \(t\) as small as four or five.

Such dependencies arise in applications ranging from finance (e.g., where some pairs of stocks or ETFs may be highly correlated, and an investor may be interested in the differences) to genomic data (where functionally related genes may have highly correlated expression patterns). Two-sparse dependencies can be directly identified by looking at the covariance matrix; see Section 4 for some discussion of previous research in this direction. But as \(t\) increases, naive methods for identifying \(t\)-sparse dependencies quickly become computationally intractable. With domain knowledge, it may be possible to manually identify and correct such dependencies, but this process would also be time-consuming. Thus, we ask the following question: instead of assuming that \(\lambda_{n}/\lambda_{1}\) is bounded, suppose that there are constants \(d_{\ell}\) and \(d_{h}\) so that \(\lambda_{n-d_{h}}/\lambda_{d_{\ell}+1}\) is bounded, i.e. the spectrum of \(\Sigma\) has only \(d_{\ell}\) outliers at the low end, and only \(d_{h}\) outliers at the high end. Can we still design an algorithm that achieves sample complexity \(O(\log n)\) without resorting to brute-force search?

Main result.We give a positive answer: an algorithm for sparse linear regression that is both computationally and statistically efficient for covariance matrices with a small number of "outlier"eigenvalues. In particular, this means we can handle a few approximate dependencies among the covariates (quantified by the number of eigenvalues below a threshold). In comparison, Lasso and other classical algorithms cannot tolerate even a single sparse approximate dependency. Our main algorithmic result is the following:

**Theorem 1.1**.: _Let \(n,t,d_{\ell},d_{h},L\in\mathbb{N}\) and \(\sigma,\delta>0\). Let \(\Sigma\in\mathbb{R}^{n\times n}\) be a positive semi-definite matrix with (non-negative) eigenvalues \(\lambda_{1}\leq\cdots\leq\lambda_{n}\). Let \(v^{*}\in\mathbb{R}^{n}\) be any \(t\)-sparse vector. Let \((X_{i},y_{i})_{i=1}^{m}\) be independent with \(X_{i}\sim N(0,\Sigma)\) and \(y_{i}=\langle X_{i},v^{*}\rangle+\xi_{i}\), where \(\xi_{i}\sim N(0,\sigma^{2})\)._

_Let \(n_{\text{eff}}:=t(\lambda_{n-d_{h}}/\lambda_{d+1})\log(nL/\delta)+t^{O(t)}d_{l }+d_{h}\). Given \(\Sigma\), \(t\), \(d_{\ell}\), \(\delta\), and \((X_{i},y_{i})_{i=1}^{m}\), there is an estimator \(\hat{v}\in\mathbb{R}^{n}\) that has excess risk_

\[\|\hat{v}-v^{*}\|_{\Sigma}^{2}\leq O\left(\frac{\sigma^{2}n_{\text{eff}}L}{m} \right)+2^{-L}\cdot\|v^{*}\|_{\Sigma}^{2}\]

_with probability at least \(1-\delta\), so long as \(m\geq\Omega(n_{\text{eff}}L)\). Moreover, \(\hat{v}\) can be computed in time \(\operatorname{poly}(n)\)._

Specifically, taking \(L\sim\log(m\left\|v^{*}\right\|_{\Sigma}^{2}/\sigma^{2})\), the time complexity is dominated by \(L\) eigendecompositions and \(L\) calls to a Lasso program, for overall runtime \(\tilde{O}(n^{3})\) (see Algorithm 2). This is substantially faster than the brute-force method (which takes \(O(n^{t})\) time) even for small values of \(t\).

The excess risk decays at rate \(\tilde{O}(\sigma^{2}n_{\text{eff}}/m)\) (hiding the logarithmic factor), which is near the statistically optimal rate of \(\tilde{O}(\sigma^{2}t/m)\) so long as \(n_{\text{eff}}\) is small, i.e. \(t\) is small and only a few eigenvalues lie outside a constant-factor range. In our analysis, we prove that the standard Lasso estimator can already tolerate a few _large_ eigenvalues -- the main algorithmic innovation is needed to tolerate a few _small_ eigenvalues, which turns out to be much trickier. Notice that when \(d_{\ell}=d_{h}=0\) we recover standard Lasso guarantees up to the factor of \(L\); thus, Theorem 1.1 morally represents a generalization of classical results.

We also show how to achieve a different trade-off between time and samples, eliminating the dependence on \(d_{\ell}\) in sample complexity at the cost of larger runtime:

**Theorem 1.2**.: _In the setting of Theorem 1.1, let \(n^{\prime}_{\text{eff}}:=t(\lambda_{n-d_{h}}/\lambda_{d_{\ell}+1})\log(nL/ \delta)+t^{2}\log(t)+d_{h}\). Given \(\Sigma\), \(t\), \(d_{\ell}\), \(\delta\), and \((X_{i},y_{i})_{i=1}^{m}\), there is an estimator \(\hat{v}\in\mathbb{R}^{n}\) that has excess risk_

\[\|\hat{v}-v^{*}\|_{\Sigma}^{2}\leq O\left(\frac{\sigma^{2}n^{\prime}_{\text{ eff}}L}{m}\right)+2^{-L}\cdot\|v^{*}\|_{\Sigma}^{2}\]

_with probability at least \(1-\delta\), so long as \(m\geq\Omega(n^{\prime}_{\text{eff}}L)\). Moreover, \(\hat{v}\) can be computed in time \(\operatorname{poly}(n,m,d^{t}_{\ell},t^{t^{2}})\)._

Discussion & limitations.We discuss two limitations of the above results. First, both results incur exponential dependence on the sparsity \(t\) (in the sample complexity for Theorem 1.1, and the runtime for Theorem 1.2), which may be suboptimal. For Theorem 1.1, we remark that in practice the algorithm may not suffer this dependence (see e.g. Figure 1), and it is possible that the analysis can be tightened. For Theorem 1.2, we emphasize that the runtime is still fundamentally different than brute-force search: in particular, it's _fixed-parameter tractable_ in \(t\) and \(d_{\ell}\).

Second, both results require that \(\Sigma\) is known. Thus, they are only applicable in settings where we either have a priori knowledge, or can estimate \(\Sigma\) accurately because a large amount of unlabelled data is available. At a high level, this limitation is due to the need to compute the eigendecomposition of \(\Sigma\), which cannot be approximated from the empirical covariance of a small number of samples.

For simplicity, we have stated our results in terms of Gaussian covariates and noise, but this is not a fundamental limitation. We expect it is possible to prove similar results in the sub-Gaussian case at the cost of making the proof longer -- for instance, by building upon the techniques from [25] and related works.

Pseudocode & simulation.See Algorithm 1 for complete pseudocode of AdaptedBP(), a simplification of the method for the noiseless setting \(\sigma=0\). In Figure 1 we show that AdaptedBP() significantly outperforms standard Basis Pursuit (i.e. Lasso for noiseless data [7]) on a simple example with \(n=1000\) variables, \(d_{\ell}=10\) sparse approximate dependencies, and a ground truth regressor with sparsity \(t=13\). The covariates \(X_{1:1000}\) are all independent \(N(0,1)\) except for \(10\) disjoint triplets \(\{(X_{i},X_{i+1},X_{i+2}):i=1,4,\ldots,28\}\), each of which has joint distribution

\[X_{i}:=Z_{i};\quad X_{i+1}=Z_{i}+0.4Z_{i+1};\quad X_{i+2}=Z_{i+1}+0.4Z_{i+2}\]

where \(Z_{i},Z_{i+1},Z_{i+2}\sim N(0,1)\) are independent. The (noiseless) responses are \(y=6.25(X_{1}-X_{2})+2.5X_{3}+\frac{1}{\sqrt{10}}\sum_{i=991}^{1000}X_{i}\). See Appendix I for implementation details.

### Organization

In Section 2 we give an overview of the proofs of Theorem 1.1 and 1.2 (the complete proofs and full algorithm pseudocode are given in Appendix C). In Section 3 we discuss our other results obtained via feature adaptation. Section 4 covers related work.

## 2 Proof techniques

We obtain Theorems 1.1 and 1.2 as outcomes of a flexible algorithmic approach for tackling sparse linear regression with ill-conditioned covariates: _feature adaptation_. As a pre-processing step, adapt

Figure 1: Basis Pursuit (BP) versus Adapted BP in a simple synthetic example with \(n=1000\) covariates. The \(x\)-axis is the number of samples. The \(y\)-axis is the out-of-sample prediction error (averaged over \(10\) independent runs, and error bars indicate the standard deviation).

or augment the covariates with additional features (i.e. well-chosen linear combinations of the covariates). Then, to predict the responses, apply \(\ell_{1}\)-regularized regression (Lasso) over the new set of features rather than the original covariates. In other words, we algorithmically change the _dictionary_ (set of features) used in the Lasso regression. See Section 4 for a comparison to past approaches.

We start by explaining the goals of feature adaptation for general \(\Sigma\), and then show how we achieve those desiderata when \(\Sigma\) has few outlier eigenvalues. More precisely, the main technical difficulty is in dealing with the small eigenvalues, so in this proof overview we focus on the case where the only outliers are small eigenvalues. Complete proofs of Theorems 1.1 and 1.2 are in Appendix C.

### What makes a good dictionary: the view from weak learning

Obviously, the feature adaptation approach generalizes Lasso. Surprisingly, even though the sample complexity of the standard Lasso estimator is thoroughly understood, the basic question of whether for _every_ covariate distribution (i.e. every \(\Sigma\)) there _exists_ a good dictionary remains wide-open. To crystallize the power of feature adaptation, we introduce the following notion of a "good" dictionary. We suggest considering the simplified setting of \(\alpha\)_-weak learning_, where the goal is just to find some \(\hat{v}\) so that the predictions \(\langle X,\hat{v}\rangle\) are \(\alpha\)-correlated with the ground truth \(\langle X,v^{*}\rangle\) when \(X\sim N(0,\Sigma)\). Moreover, we focus first on the existential question (rather than the algorithmic question of finding the dictionary). We will return to the setting of Theorems 1.1 and 1.2 later. For now, in the weak learning setting, a good dictionary (when the covariate distribution is \(N(0,\Sigma)\)) is one that satisfies the following covering property, but is not too large:

**Definition 2.1**.: Let \(\Sigma\in\mathbb{R}^{n\times n}\) be a positive semi-definite matrix and let \(t,\alpha>0\). A set \(\{D_{1},\ldots,D_{N}\}\subseteq\mathbb{R}^{n}\) is a \((t,\alpha)\)-dictionary for \(\Sigma\) if for every \(t\)-sparse \(v\in\mathbb{R}^{n}\), there is some \(i\in[N]\) with

\[|\langle v,D_{i}\rangle_{\Sigma}|\geq\alpha\left\|v\right\|_{\Sigma}\left\|D_{ i}\right\|_{\Sigma},\]

where we define \(\langle x,y\rangle_{\Sigma}:=x^{\top}\Sigma y\) and \(\left\|x\right\|_{\Sigma}^{2}:=x^{\top}\Sigma x\) for any \(x,y\in\mathbb{R}^{n}\). Let \(\mathcal{N}_{t,\alpha}(\Sigma)\) be the size of the smallest \((t,\alpha)\)-dictionary.

The relevance of the covering number \(\mathcal{N}_{t,\alpha}(\Sigma)\) is quite simple: given a \((t,\alpha)\)-dictionary \(\mathcal{D}\) for \(\Sigma\), and given samples \((X_{i},y_{i})_{i=1}^{m}\), the weak learning algorithm can simply output the vector \(\hat{v}\in\mathcal{D}\) that maximizes the empirical correlation between the predictions \(\langle X_{i},\hat{v}\rangle\) and the responses \(y_{i}\). So long as there are enough samples for empirical correlations to concentrate, Definition 2.1 guarantees success. Formally, allowing for preprocessing time to compute the dictionary, \(O(\alpha)\)-weak learning is possible in time \(\mathcal{N}_{t,\alpha}(\Sigma)\cdot\mathrm{poly}(n)\), with \(O(\alpha^{-2}\log\mathcal{N}_{t,\alpha}(\Sigma))\) samples (Proposition A.5).

Hypothetically, bounding \(\mathcal{N}_{t,\alpha}(\Sigma)\) may not be _necessary_ to develop an efficient sparse linear regression algorithm. However, all assumptions on \(\Sigma\) that are currently known to enable efficient sparse linear regression also immediately imply bounds on \(\mathcal{N}_{t,\alpha}\) (see Appendix G). For example, when \(\Sigma\) is well-conditioned, the standard basis is a good dictionary of size \(n\) (Fact A.4).

In contrast, the only known bounds for arbitrary \(\Sigma\) (until the present work) are \(\mathcal{N}_{t,1/\sqrt{t}}(\Sigma)\leq t\cdot\binom{n}{t}\) (the brute-force dictionary, which includes a \(\Sigma\)-orthonormal basis for every set of \(t\) covariates) and \(\mathcal{N}_{t,1/\sqrt{n}}(\Sigma)\leq n\) (a \(\Sigma\)-orthonormal basis for all \(n\) covariates, which doesn't take advantage of sparsity and corresponds to algorithms such as Ordinary Least Squares). Thus, the following basic question - when can we improve upon these trivial bounds - seems central to understanding when brute-force search can be avoided in sparse linear regression:

**Question 2.2**.: _How large is \(\mathcal{N}_{t,\alpha}(\Sigma)\) for an arbitrary positive semi-definite \(\Sigma\in\mathbb{R}^{n\times n}\)? Are there natural families of ill-conditioned \(\Sigma\) (and functions \(f,g\)) for which \(\mathcal{N}_{t,1/f(t)}(\Sigma)\leq g(t)\cdot\mathrm{poly}(n)\)?_

### Constructing a good dictionary when \(\Sigma\) has few small eigenvalues

We now address Question 2.2 in the setting where \(\Sigma\) has a small number of eigenvalues that are much smaller than \(\lambda_{n}\). In this setting, the standard basis may not be a good dictionary. For example, if two covariates are highly correlated, their difference may not be correlated with any of them. Nonetheless, we can prove the following covering number bound:

**Theorem 2.3**.: _Let \(n,t,d\in\mathbb{N}\). Let \(\Sigma\in\mathbb{R}^{n\times n}\) be a positive semi-definite matrix with eigenvalues \(\lambda_{1}\leq\cdots\leq\lambda_{n}\). Then \(\mathcal{N}_{t,\alpha}(\Sigma)\leq t(7t)^{2t^{2}+d}t^{t}+n\), where \(\alpha=\frac{1}{7\sqrt{t}}\sqrt{\lambda_{d+1}/\lambda_{n}}\)._In particular, when \(t=O(1)\) and \(\Sigma\) is well-conditioned except for \(O(1)\) outliers \(\lambda_{1},\ldots,\lambda_{d}\), we get a linear-size dictionary just as in the case where \(\Sigma\) is well-conditioned. In fact, the desired \((t,\alpha)\)-dictionary can be constructed efficiently. Our key lemma shows that when \(\Sigma\) has few small eigenvalues, there is a small subset of covariates that "causes" all of the sparse approximate dependencies - in the sense that the \(\ell_{2}\) norm of any sparse vector, _excluding_ the mass on the subset, can be upper bounded in terms of the \(\Sigma\)-norm of the vector. Moreover, there is an efficient algorithm that finds a superset of these covariates. Formally, we prove the following:

**Lemma 2.4**.: _Let \(n,t,d\in\mathbb{N}\). Let \(\Sigma\in\mathbb{R}^{n\times n}\) be a positive semi-definite matrix with eigenvalues \(\lambda_{1}\leq\cdots\leq\lambda_{n}\). Given \(\Sigma\), \(d\), and \(t\), there is a polynomial-time algorithm_ IterativePeeling() _producing a set \(S\subseteq[n]\) with the following guarantees:_

1. _For every_ \(t\)_-sparse_ \(v\in\mathbb{R}^{n}\)_, it holds that_ \(\left\|v_{[n]\setminus S}\right\|_{2}\leq 3\lambda_{d+1}^{-1/2}\left\|v \right\|_{\Sigma}\)_._
2. \(|S|\leq(7t)^{2t+1}d\)_._

Once this set \(S\) has been found, the dictionary is simply the standard basis \(\{e_{1},\ldots,e_{n}\}\), together with a \(\Sigma\)-orthonormal basis for every set of \(t\) covariates in \(S\). By guarantee (a), we can prove that every \(t\)-sparse vector correlates with some element of this dictionary under the \(\Sigma\)-inner product. By guarantee (b), the dictionary is much smaller than the brute-force dictionary that contains a basis for all \(\binom{n}{t}\) sets of \(t\) covariates. Together, this gives an algorithmic proof for Theorem 2.3.

**Intuition for IterativePeeling().** We compute the set \(S\) via a new iterative method which leverages knowledge of the small eigenspaces of \(\Sigma\). See Algorithm 1 for the pseudocode. To compute \(S\), the algorithm IterativePeeling() first computes the orthogonal projection matrix \(P\) that projects onto the subspace spanned by the top \(n-d\) eigenvectors of \(\Sigma\). Starting with the set of coordinates that correlate with \(\ker(P)\), the procedure then iteratively grows \(S\) in such a way that at each step, a new participant of each approximate sparse dependency is discovered, but \(S\) does not become too much larger.

The intuition is as follows: as a preliminary attempt, we could identify all \(O(d)\) coordinates that correlate (with respect to the standard inner product) with the lowest \(d\) eigenspaces of \(\Sigma\). If e.g. the covariates have a sparse dependency

\[X_{1}+X_{2}=0,\]

then \(\ker\Sigma\) contains the vector \(e_{1}+e_{2}\), so the coordinates \(\{e_{1},e_{2}\}\) will be correctly discovered. Unfortunately, if \(\Sigma\) contains a more complex sparse dependency such as

\[\epsilon^{-1}(X_{1}-X_{2})-X_{3}-X_{4}=0\]

where \(\epsilon>0\) is very small, then this heuristic will discover \(\{e_{1},e_{2}\}\) but miss \(\{e_{3},e_{4}\}\). For this example, the solution is to notice that \(e_{3}\) and \(e_{4}\)_do_ correlate with the subspace spanned by \(\ker(\Sigma)\cup\{e_{1},e_{2}\}\) (which contains \(e_{3}+e_{4}\)). In general, if \(S\) is the set of coordinates discovered thus far, then by finding basis vectors that correlate with an appropriate subspace (of dimension at most \(|S|\)), we can efficiently augment \(S\) with at least one new coordinate from each \(t\)-sparse approximate dependency, without making \(S\) bigger by more than a factor of \(O(t)\). Iterating this augmentation \(t\) times therefore provably identifies all problematic coordinates.

To formalize this intuition, the following lemma will be needed to bound how much \(S\) grows at each iteration; it shows that the number of coordinates that correlate with a low-dimensional subspace is not too large (proof deferred to Appendix B):

**Lemma 2.5**.: _Let \(V\subseteq\mathbb{R}^{n}\) be a subspace with \(d:=\dim V\). For some \(\alpha>0\) define_

\[S=\left\{i\in[n]:\sup_{x\in V\setminus\{0\}}\frac{x_{i}}{\left\|x\right\|_{2} }\geq\alpha\right\}.\]

_Then \(|S|\leq d/\alpha^{2}\). Moreover, given a set of vectors that span \(V\), we can compute \(S\) in time \(\operatorname{poly}(n)\)._

We also define the set of vectors \(v\) that have unusually large norm outside a set \(S\), compared to \(\sqrt{v^{\top}Pv}\), which is the distance from \(v\) to the subspace spanned by the bottom \(d\) eigenvectors of \(\Sigma\):

**Definition 2.6**.: For any matrix \(P\in\mathbb{R}^{n\times n}\) and subset \(S\subseteq[n]\), define \(\mathcal{W}_{P,S}:=\{v\in\mathbb{R}^{n}:\left\|v_{S^{c}}\right\|_{2}>3\sqrt{v^ {\top}Pv}\}\).

We then formalize the guarantee of each iteration of IterativePeeling() as follows:

**Lemma 2.7**.: _Let \(n,t\in\mathbb{N}\) and let \(P:n\times n\) be an orthogonal projection matrix. Suppose \(\tau\geq 1\) and \(K\subseteq[n]\) satisfy_

1. \(P_{ii}\geq 1-1/(9t^{2})\) _for all_ \(i\not\in K\)_,_
2. \(|\operatorname{supp}(v)\setminus K|\leq\tau\) _for every_ \(v\in B_{0}(t)\cap\mathcal{W}_{P,K}\)_._

_Then there exists a set \(\mathcal{I}_{P}(K)\) with \(|\mathcal{I}_{P}(K)|\leq 36t^{2}|K|\) such that_

\[|\operatorname{supp}(v)\setminus(\mathcal{I}_{P}(K)\cup K)|\leq\tau-1\]

_for all \(v\in B_{0}(t)\cap\mathcal{W}_{P,K}\). Moreover, given \(P\), \(K\), and \(t\), we can compute \(\mathcal{I}_{P}(K)\) in time \(\operatorname{poly}(n)\)._

**Proof sketch.** We define the set

\[\mathcal{I}_{P}(K):=\left\{a\in[n]\setminus K:\sup_{x\in\operatorname{span}\{ Pe_{i}:i\in K\}\setminus\{0\}}\frac{|x_{a}|}{\|x\|_{2}}\geq 1/(6t)\right\}.\]

It is clear from Lemma B.2 (applied with parameters \(V:=\operatorname{span}\{Pe_{i}:i\in K\}\) and \(\alpha:=1/(6t)\)) that \(|\mathcal{I}_{P}(K)|\leq 36t^{2}|K|\), and that \(\mathcal{I}_{P}(K)\) can be computed in time \(\operatorname{poly}(n)\). It remains to show that \(|\mathcal{G}_{P}(v)\setminus(\mathcal{I}_{P}(K)\cup K)|\leq\tau-1\) for all \(v\in B_{0}(t)\).

Consider any \(v\in B_{0}(t)\cap\mathcal{W}_{P,K}\). Then \(\left\|v_{K^{c}}\right\|_{2}>3\left\|Pv\right\|_{2}\). It's sufficient to show that \(\mathcal{I}_{P}(K)\) contains some \(j\in\operatorname{supp}(v)\setminus K\), i.e. that there is some \(j\in\operatorname{supp}(v)\setminus K\) such that \(e_{j}\) correlates with \(\operatorname{span}\{P_{i}:i\in K\}\). We accomplish this by showing that \(v_{K^{c}}\) correlates with \(Pv_{K}=\sum_{i\in K}v_{i}P_{i}\).

At a high level, the reason for this is that \(v_{K^{c}}\) is close to \(Pv_{K^{c}}\) (since \(P_{i}\approx e_{i}\) for \(i\in K^{c}\)), and \(Pv=Pv_{K}+Pv_{K^{c}}\) is much smaller than \(Pv_{K^{c}}\approx v_{K^{c}}\), so \(Pv_{K}\) and \(Pv_{K^{c}}\) must be highly correlated. See Appendix B for the full proof. \(\blacksquare\)

We can now complete the proof of Lemma 2.4 by repeatedly invoking Lemma B.4.

**Proof of Lemma 2.4.** Let \(\Sigma=\sum_{i=1}^{n}\lambda_{i}u_{i}u_{i}^{\top}\) be the eigendecomposition of \(\Sigma\), and let \(P:=\sum_{i=d+1}^{n}u_{i}^{\top}\) be the projection onto the top \(n-d\) eigenspaces of \(\Sigma\). Set \(K_{t}=\{i\in[n]:P_{ii}<1-1/(9t^{2})\}\). Because \(\operatorname{tr}(P)=n-d\) and \(P_{ii}\leq 1\) for all \(i\in[n]\), it must be that \(|K_{t}|\leq 9t^{2}d\). Also, for any \(v\in B_{0}(t)\cap\mathcal{W}_{P,K_{t}}\) we have trivially by \(t\)-sparsity that \(|\operatorname{supp}(v)\setminus K_{t}|\leq t\).

Define \(K_{t-1}\) to be \(K_{t}\cup\mathcal{I}_{P}(K_{t})\) where \(\mathcal{I}_{P}(K_{t})\) is as defined in Lemma B.4; we have the guarantees that \(|K_{t-1}|\leq(1+36t^{2})|K_{t}|\) and \(|\mathcal{G}_{P}(v)\setminus K_{t}|\leq t-1\) for all \(v\in B_{0}(t)\cap\mathcal{W}_{P,K_{t}}\). Since \(K_{t-1}\supseteq K_{t}\), it holds that \(\mathcal{W}_{P,K_{t-1}}\subseteq\mathcal{W}_{P,K_{t}}\), and thus \(|\mathcal{G}_{P}(v)\setminus K_{t}|\leq t-1\) for all \(v\in B_{0}(t)\cap\mathcal{W}_{P,K_{t-1}}\). Moreover, since \(K_{t-1}\supseteq K_{t}\), it obviously holds that \(P_{ii}\geq 1-1/(9t^{2})\) for all \(i\not\in K_{t-1}\). This means we can apply Lemma B.4 with \(\tau:=t-1\) and \(K:=K_{t-1}\) and so iteratively define sets \(K_{t-2}\subseteq\dots\subseteq K_{1}\subseteq K_{0}\subseteq[n]\) in the same way. In the end, we obtain the set \(K_{0}\subseteq[n]\) with \(|K_{0}|\leq 9t^{2}d(1+36t^{2})t\) and \(\operatorname{supp}(v)\subseteq K_{0}\) for all \(v\in B_{0}(t)\cap\mathcal{W}_{P,K_{0}}\). The latter guarantee means that in fact \(B_{0}(t)\cap\mathcal{W}_{P,K_{0}}=\emptyset\). So for any \(t\)-sparse \(v\in\mathbb{R}^{n}\) it holds that

\[\left\|v_{K_{0}^{c}}\right\|_{2}\leq 3\sqrt{v^{\top}Pv}\leq 3\lambda_{d+1}^{-1/2} \sqrt{v^{\top}\Sigma v}\]

where the last inequality holds since \(\lambda_{d+1}P\preceq\Sigma\). \(\blacksquare\)

### Beyond weak learning

So far, we have sketched a proof that if \(\Sigma\) has few outlier eigenvalues, then there is an efficient algorithm to compute a good dictionary (as in Theorem 2.3). This gives an efficient \(\alpha\)-weak learning algorithm (via Proposition A.5). However, our ultimate goal is to find a regressor \(\hat{v}\) with prediction error going to \(0\) as the number of samples increases. Definition 2.1 is not strong enough to ensure this.8 However, it turns out that the dictionary constructed in Theorem 2.3 in fact satisfies a stronger guarantee9 that _is_ sufficient to achieve vanishing prediction error:

**Definition 2.8**.: Let \(\Sigma\in\mathbb{R}^{n\times n}\) be a positive semi-definite matrix and let \(t,B>0\). A set \(\{D_{1},\ldots,D_{N}\}\subseteq\mathbb{R}^{n}\) is a \((t,B)\)-\(\ell_{1}\)-representation for \(\Sigma\) if for any \(t\)-sparse \(v\in\mathbb{R}^{n}\) there is some \(\alpha\in\mathbb{R}^{N}\) with \(v=\sum_{i=1}^{N}\alpha_{i}D_{i}\) and \(\sum_{i=1}^{N}|\alpha_{i}|\cdot\|D_{i}\|_{\Sigma}\leq B\cdot\|v\|_{\Sigma}\,.\)

With this definition in hand, we can actually prove the following strengthening of Theorem 2.3:

**Lemma 2.9**.: _Let \(n,t,d\in\mathbb{N}\). Let \(\Sigma\in\mathbb{R}^{n\times n}\) be a positive semi-definite matrix with eigenvalues \(\lambda_{1}\leq\cdots\leq\lambda_{n}\). Then \(\Sigma\) has a \((t,7\sqrt{t}\sqrt{{\lambda_{n}}/{\lambda_{d+1}}})\)-\(\ell_{1}\)-representation \(\mathcal{D}\) of size at most \(n+t(7t)^{2t^{2}+t}d^{t}\). Moreover, \(\mathcal{D}\) can be computed in time \(t^{O(t^{2})}d^{t}\operatorname{poly}(n)\)._

**Proof sketch.** Let \(S\) be the output of \(\mathtt{IterativePeeling}(\Sigma,d,t)\). The dictionary \(\mathcal{D}\) consists of the standard basis, together with a \(\Sigma\)-orthogonal basis for each set of \(t\) coordinates from \(S\). The bound on \(|\mathcal{D}|\) comes from the guarantee \(|S|\leq(7t)^{2t+1}d\). For any \(t\)-sparse vector \(v\in\mathbb{R}^{n}\), we know that \(v_{S^{c}}\) is efficiently represented by the standard basis (because Theorem B.1 guarantees that \(\left\|v_{S^{c}}\right\|_{2}\leq O(\lambda_{d+1}^{-1/2}\left\|v\right\|_{\Sigma })\)), and \(v_{S}\) is efficiently represented by one of the \(\Sigma\)-orthonormal bases. See Appendix B for the full proof. \(\blacksquare\)

Why is the above guarantee useful? If each \(D_{i}\) is normalized to unit \(\Sigma\)-norm, then the condition of \((t,B)\)-\(\ell_{1}\)-representability is equivalent to \(\left\|\alpha\right\|_{1}\leq B\cdot\left\|v\right\|_{\Sigma}\). That is, with respect to the new set of features, the regressor \(\alpha\) has bounded \(\ell_{1}\) norm. Thus, if we apply the Lasso with a set of features that is a \((t,B)\)-\(\ell_{1}\)-representation for \(\Sigma\), then standard "slow rate" guarantees hold (proof in Section A):

**Proposition 2.10**.: _Let \(n,m,N,t\in\mathbb{N}\) and \(B>0\). Let \(\Sigma\in\mathbb{R}^{n\times n}\) be a positive semi-definite matrix and let \(\mathcal{D}\) be a \((t,B)\)-\(\ell_{1}\)-representation of size \(N\) for \(\Sigma\), normalized so that \(\left\|v\right\|_{\Sigma}=1\) for all \(v\in\mathcal{D}\). Fix a \(t\)-sparse vector \(v^{*}\in\mathbb{R}^{n}\), let \(X_{1},\ldots,X_{m}\sim N(0,\Sigma)\) be independent and let \(y_{i}=\langle X_{i},v^{*}\rangle+\xi_{i}\) where \(\xi_{i}\sim N(0,\sigma^{2})\). For any \(R>0\), define_

\[\hat{w}\in\operatorname*{argmin}_{w\in\mathbb{R}^{N}:\left\|w\right\|_{1}\leq BR }\left\|\mathbb{X}Dw-y\right\|_{2}^{2}\]

_where \(D\in\mathbb{R}^{n\times N}\) is the matrix with columns comprising the elements of \(\mathcal{D}\), and \(\mathbb{X}\in\mathbb{R}^{m\times n}\) is the matrix with rows \(X_{1},\ldots,X_{m}\). So long as \(m=\Omega(\log(n/\delta))\) and \(\left\|w^{*}\right\|_{\Sigma}\in[R/2,R]\), it holds with probability at least \(1-\delta\) that_

\[\left\|D\hat{w}-w^{*}\right\|_{\Sigma}^{2}=O\left(B\left\|w^{*}\right\|_{ \Sigma}\sigma\sqrt{\frac{\log(2n/\delta)}{m}}+\frac{\sigma^{2}\log(4/\delta)} {m}+\frac{B^{2}\left\|w^{*}\right\|_{\Sigma}^{2}\log(n)}{m}\right).\]

Combining Proposition 2.10 with Lemma 2.9 shows that there is an algorithm with time complexity \(t^{O(t^{2})}d^{t}\operatorname{poly}(n)\) and sample complexity \(O(\operatorname{poly}(t)(\lambda_{n}/\lambda_{d+1})\log(n)\log(d))\) for finding a regressor with squared prediction error \(o(\sigma^{2}+\left\|v^{*}\right\|_{\Sigma}^{2})\). This is a simplified version of Theorem 1.2. The full proof involves additional technical details (e.g. more careful analysis to take care of large eigenvalues, and to avoid needing an estimate \(R\) for \(\left\|w^{*}\right\|_{\Sigma}\)) but the above exposition contains the central ideas. Theorem 1.1 similarly computes the set \(S\) from Lemma 2.4 but uses it to construct a different dictionary: the standard basis, plus a \(\Sigma\)-orthonormal basis for \(S\).10 See Appendix C for the full proofs and pseudocode.

Footnote 10: More precisely, the algorithm just skips regularizing \(S\), which is morally equivalent. As it is simpler to implement, that is shown in Algorithm 1, and analyzed for the proofs.

## 3 Additional Results

We now return to Question 2.2 and ask whether there are other families of ill-conditioned \(\Sigma\) for which we can prove non-trivial bounds on \(\mathcal{N}_{t,\alpha}(\Sigma)\).

First, we ask what can be shown for _arbitrary_ covariance matrices. We prove that _every_ covariance matrix \(\Sigma\) satisfies a non-trivial bound \(\mathcal{N}_{t,1/O(t^{3/2}\log n)}(\Sigma)\leq O(n^{t-1/2})\). In fact, building on tools from computational geometry, we show the stronger result that \(\Sigma\) has a \((t,O(t^{3/2}\log n))\)-\(\ell_{1}\)-representation that of size \(O(n^{t-1/2})\), that is computable from samples in time \(\tilde{O}(n^{t-\Omega(1/t)})\) forany constant \(t>1\) (Theorem D.5). As a corollary, we provide the first sparse linear regression algorithm with time complexity that is a polynomial-factor better than brute force, and with near-optimal sample complexity, for any constant \(t\) and arbitrary \(\Sigma\) (proof in Section D):

**Theorem 3.1**.: _Let \(n,m,t,B\in\mathbb{N}\) and \(\sigma>0\), and let \(\Sigma\in\mathbb{R}^{n\times n}\) be a positive-definite matrix. Let \(w^{*}\in\mathbb{R}^{n}\) be \(t\)-sparse, and suppose \(\left\|w^{*}\right\|_{\Sigma}\in[B/2,B]\). Suppose \(m\geq\Omega(t\log n)\). Let \((X_{i},y_{i})_{i=1}^{m}\) be independent samples where \(X_{i}\sim N(0,\Sigma)\) and \(y_{i}=\langle X_{i},w^{*}\rangle+N(0,\sigma^{2})\). Then there is an \(O(m^{2}n^{t-1/2}+n^{t-\Omega(1/t)}\log^{O(t)}n)\)-time algorithm that, given \((X_{i},y_{i})_{i=1}^{m}\), \(B\), and \(\sigma^{2}\), produces an estimate \(\hat{w}\in\mathbb{R}^{n}\) satisfying, with probability \(1-o(1)\),_

\[\left\|\hat{w}-w^{*}\right\|_{\Sigma}^{2}\leq\tilde{O}\left(\frac{\sigma^{2}}{ \sqrt{m}}+\frac{\sigma\left\|w^{*}\right\|_{\Sigma}t^{3/2}}{\sqrt{m}}+\frac{ \left\|w^{*}\right\|_{\Sigma}^{2}t^{3}}{m}\right).\]

Second, one goal is to improve "sample complexity" (i.e. obtain \(\alpha\) without dependence on condition number) without paying too much in "time complexity" (i.e. retain bounds on \(\mathcal{N}_{t,\alpha}\) that are better than \(n^{t}\)). To this end, we prove that the dependence on \(\kappa\) in the correlation level (see Fact A.4) can actually be replaced by dependence on \(\kappa\) in the dictionary size (proof in Appendix E):

**Theorem 3.2**.: _Let \(n,t\in\mathbb{N}\). Let \(\Sigma\in\mathbb{R}^{n\times n}\) be a positive-definite matrix with condition number \(\kappa\). Then \(\mathcal{N}_{t,1/3^{t+1}}(\Sigma)\leq 2^{O(t^{2})}\kappa^{2t+1}\cdot n\)._

In particular, for any constant \(t=1/\epsilon\), our result shows that there is a nearly-linear size dictionary with _constant_ correlations even for covariance matrices with _polynomially-large_ condition number \(\kappa\leq n^{\epsilon/100}\). While we are not currently aware of an efficient algorithm for computing the dictionary, the above bound nonetheless raises the interesting possibility that there may be a sample-efficient and computationally-efficient weak learning algorithm under a super-constant bound on \(\kappa\).

## 4 Related work

Dealing with correlated covariates.There is considerable work on improving the performance of Lasso in situations where some clusters of covariates are highly correlated [47, 19, 2, 42, 21, 12, 27]. These methods can work well for two-sparse dependencies, but generally do not work as well for higher-order dependencies -- hence they cannot be used to prove our main result. The approach of [2] is perhaps the closest in spirit to ours. They perform agglomerative clustering of correlated covariates, orthonormalize the clusters with respect to \(\Sigma\), and apply Lasso (or solve an essentially equivalent group Lasso problem). This method fails, for example, when there is a single three-sparse dependency, and the remaining covariates have some mild correlations. Depending on the correlation threshold, their method will either aggressively merge all covariates into a single cluster, or fail to merge the dependent covariates.

Feature adaptation and preconditioning.Generalizations of Lasso via a preliminary change-of-basis (or explicitly altering the regularization term) have been studied in the past, but largely not to solve sparse linear regression per se; instead the goal has been using \(\ell_{1}\) regularization to encourage other structural properties such as piecewise continuity (e.g. in the "fused lasso", see [35, 36, 20, 8] for some more examples). An exception is recent work showing that a "sparse preconditioning" step can enable Lasso to be statistically efficient for sparse linear regression when the covariates have a certain Markovian structure [23]. Our notion of feature adaptation via dictionaries generalizes sparse preconditioning, which corresponds to choosing a non-standard basis in which \(\Sigma\) becomes well-conditioned and the sparsity of the signal is preserved.

Statistical query (SQ) model; sparse halfspaces.From the complexity standpoint, \(\mathcal{N}_{t,\alpha}(\Sigma)\) is a covering number and therefore closely corresponds to a packing number \(\mathcal{P}_{t,\alpha}(\Sigma)\) (see Section A.1 for the definition). This packing number is essentially the _(correlational) statistical dimension_, which governs the complexity of sparse linear regression with covariates from \(N(0,\Sigma)\) in the (correlational) SQ model (see e.g. [14] for exposition of this model). Whereas strong \(n^{\Omega(t)}\) SQ lower bounds are known for related problems such as sparse parities with noise [29], no non-trivial (i.e. super-linear) lower bounds are known for sparse linear regression. Relatedly, in a COLT open problem, Feldman asked whether any non-trivial bounds can be shown for the complexity of weak learning sparse halfspaces in the SQ model [11]. Our results also yield improved bounds for weakly SQ-learning sparse halfspaces over certain families of multivariate Gaussian distributions.

Improving brute-force for arbitrary \(\Sigma\).Several prior works have suggested improvements on brute-force search for variants of \(t\)-sparse linear regression [18, 16, 31, 6]. However, all of these have limitations preventing their application to the general setting we address in Theorem 3.1. Specifically, [18] requires \(\Omega(n^{t})\) preprocessing time on the covariates; [16, 31] require noiseless responses; and [6] has time complexity scaling with \(\log^{m}n\) (since our random-design setting necessitates \(m\geq\Omega(t\log n)\), their algorithm has time complexity much larger than \(n^{t}\)).

## References

* [1] Peter J Bickel, Ya'acov Ritov, Alexandre B Tsybakov, et al. Simultaneous analysis of lasso and dantzig selector. _The Annals of statistics_, 37(4):1705-1732, 2009.
* [2] Peter Buhlmann, Philipp Rutimann, Sara Van De Geer, and Cun-Hui Zhang. Correlated variables in regression: clustering and sparse estimation. _Journal of Statistical Planning and Inference_, 143(11):1835-1858, 2013.
* [3] Emmanuel Candes, Terence Tao, et al. The dantzig selector: Statistical estimation when p is much larger than n. _Annals of statistics_, 35(6):2313-2351, 2007.
* [4] Emmanuel J Candes, Justin K Romberg, and Terence Tao. Stable signal recovery from incomplete and inaccurate measurements. _Communications on Pure and Applied Mathematics: A Journal Issued by the Courant Institute of Mathematical Sciences_, 59(8):1207-1223, 2006.
* [5] Emmanuel J Candes and Terence Tao. Decoding by linear programming. _IEEE transactions on information theory_, 51(12):4203-4215, 2005.
* [6] Jean Cardinal and Aurelien Ooms. Algorithms for approximate sparse regression and nearest induced hulls. _Journal of Computational Geometry_, 13(1):377-398, 2022.
* [7] Shaobing Chen and David Donoho. Basis pursuit. In _Proceedings of 1994 28th Asilomar Conference on Signals, Systems and Computers_, volume 1, pages 41-44. IEEE, 1994.
* [8] Arnak S Dalalyan, Mohamed Hebiri, Johannes Lederer, et al. On the prediction performance of the lasso. _Bernoulli_, 23(1):552-581, 2017.
* [9] Abhimanyu Das and David Kempe. Submodular meets spectral: Greedy algorithms for subset selection, sparse approximation and dictionary selection. _arXiv preprint arXiv:1102.3975_, 2011.
* [10] David L Donoho and Philip B Stark. Uncertainty principles and signal recovery. _SIAM Journal on Applied Mathematics_, 49(3):906-931, 1989.
* [11] Vitaly Feldman. Open problem: The statistical query complexity of learning sparse halfspaces. In _Conference on Learning Theory_, pages 1283-1289. PMLR, 2014.
* [12] Mario Figueiredo and Robert Nowak. Ordered weighted l1 regularized regression with strongly correlated covariates: Theoretical aspects. In _Artificial Intelligence and Statistics_, pages 930-938. PMLR, 2016.
* [13] Dean P Foster and Edward I George. The risk inflation criterion for multiple regression. _The Annals of Statistics_, pages 1947-1975, 1994.
* [14] Surbhi Goel, Aravind Gollakota, Zhihan Jin, Sushrut Karmalkar, and Adam Klivans. Superpolynomial lower bounds for learning one-layer neural networks using gradient descent. In _International Conference on Machine Learning_, pages 3587-3596. PMLR, 2020.
* [15] Aparna Gupte and Vinod Vaikuntanathan. The fine-grained hardness of sparse linear regression. _arXiv preprint arXiv:2106.03131_, 2021.
* [16] Aparna Ajit Gupte and Kerri Lu. Fine-grained complexity of sparse linear regression. 2020.
* [17] Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2023.
* [18] Sariel Har-Peled, Piotr Indyk, and Sepideh Mahabadi. Approximate sparse linear regression. In _45th International Colloquium on Automata, Languages, and Programming (ICALP 2018)_. Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, 2018.
* [19] Jim C Huang and Nebojsa Jojic. Variable selection through correlation sifting. In _RECOMB_, volume 6577, pages 106-123. Springer, 2011.

* [20] Jan-Christian Hutter and Philippe Rigollet. Optimal rates for total variation denoising. In _Conference on Learning Theory_, pages 1115-1146. PMLR, 2016.
* [21] Jinzhu Jia, Karl Rohe, et al. Preconditioning the lasso for sign consistency. _Electronic Journal of Statistics_, 9(1):1150-1172, 2015.
* [22] Jonathan Kelner, Frederic Koehler, Raghu Meka, and Ankur Moitra. Learning some popular gaussian graphical models without condition number bounds. In _Proceedings of Neural Information Processing Systems (NeurIPS)_, 2020.
* [23] Jonathan Kelner, Frederic Koehler, Raghu Meka, and Dhruv Rohatgi. On the power of preconditioning in sparse linear regression. _62nd Annual IEEE Symposium on Foundations of Computer Science_, 2021.
* [24] Jonathan A Kelner, Frederic Koehler, Raghu Meka, and Dhruv Rohatgi. Distributional hardness against preconditioned lasso via erasure-robust designs. _arXiv preprint arXiv:2203.02824_, 2022.
* [25] Guillaume Lecue and Shahar Mendelson. Learning subgaussian classes: Upper and minimax bounds. _arXiv preprint arXiv:1305.4825_, 2013.
* [26] Yin Tat Lee, Aaron Sidford, and Sam Chiu-wai Wong. A faster cutting plane method and its implications for combinatorial and convex optimization. In _2015 IEEE 56th Annual Symposium on Foundations of Computer Science_, pages 1049-1065. IEEE, 2015.
* [27] Yuan Li, Benjamin Mark, Garvesh Raskutti, and Rebecca Willett. Graph-based regularization for regression problems with highly-correlated designs. In _2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP)_, pages 740-742. IEEE, 2018.
* [28] Shahar Mendelson et al. On the performance of kernel classes. 2003.
* [29] Elchanan Mossel, Ryan O'Donnell, and Rocco P Servedio. Learning juntas. In _Proceedings of the thirty-fifth annual ACM symposium on Theory of computing_, pages 206-212, 2003.
* [30] Wolfgang Mulzer, Huy L Nguyen, Paul Seiferth, and Yannik Stein. Approximate k-flat nearest neighbor search. In _Proceedings of the forty-seventh annual ACM symposium on Theory of Computing_, pages 783-792, 2015.
* [31] Eric Price, Sandeep Silwal, and Samson Zhou. Hardness and algorithms for robust and sparse optimization. In _International Conference on Machine Learning_, pages 17926-17944. PMLR, 2022.
* [32] Garvesh Raskutti, Martin J Wainwright, and Bin Yu. Restricted eigenvalue properties for correlated gaussian designs. _The Journal of Machine Learning Research_, 11:2241-2259, 2010.
* [33] Phillippe Rigollet and Jan-Christian Hutter. High dimensional statistics. _Lecture notes for course 18S997_, 813:814, 2015.
* [34] Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Optimistic rates for learning with a smooth loss. _arXiv preprint arXiv:1009.3896_, 2010.
* [35] Robert Tibshirani, Michael Saunders, Saharon Rosset, Ji Zhu, and Keith Knight. Sparsity and smoothness via the fused lasso. _Journal of the Royal Statistical Society: Series B (Statistical Methodology)_, 67(1):91-108, 2005.
* [36] Ryan J Tibshirani and Jonathan Taylor. The solution path of the generalized lasso. _The annals of statistics_, 39(3):1335-1371, 2011.
* [37] Gregory Valiant. Finding correlations in subquadratic time, with applications to learning parities and juntas. In _2012 IEEE 53rd Annual Symposium on Foundations of Computer Science_, pages 11-20. IEEE, 2012.
* [38] Sara Van De Geer. On tight bounds for the lasso. _Journal of Machine Learning Research_, 19:46, 2018.
* [39] Sara A Van de Geer. _Empirical Processes in M-estimation_, volume 6. Cambridge university press, 2000.
* [40] Sara A Van De Geer, Peter Buhlmann, et al. On the conditions used to prove oracle results for the lasso. _Electronic Journal of Statistics_, 3:1360-1392, 2009.
* [41] Roman Vershynin. _High-dimensional probability: An introduction with applications in data science_, volume 47. Cambridge university press, 2018.

* [42] Fabian L Wauthier, Nebojsa Jojic, and Michael I Jordan. A comparative framework for preconditioned lasso algorithms. _Advances in Neural Information Processing Systems_, 26:1061-1069, 2013.
* [43] Tong Zhang. Learning bounds for kernel regression using effective data dimensionality. _Neural Computation_, 17(9):2077-2098, 2005.
* [44] Yuchen Zhang, Martin J Wainwright, Michael I Jordan, et al. Optimal prediction for sparse linear models? lower bounds for coordinate-separable m-estimators. _Electronic Journal of Statistics_, 11(1):752-799, 2017.
* [45] Lijia Zhou, Frederic Koehler, Danica J Sutherland, and Nathan Srebro. Optimistic rates: A unifying theory for interpolation learning and regularization in linear regression. _arXiv preprint arXiv:2112.04470_, 2021.
* [46] Shuheng Zhou. Restricted eigenvalue conditions on subgaussian random matrices. _arXiv preprint arXiv:0912.4045_, 2009.
* [47] Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. _Journal of the royal statistical society: series B (statistical methodology)_, 67(2):301-320, 2005.

Preliminaries

Throughout, we use the following standard notation. For positive integers \(n,m\in\mathbb{N}\), we write \(A:m\times n\) to denote a matrix with \(m\) rows, \(n\) columns, and real-valued entries. The standard inner product on \(\mathbb{R}^{n}\) is denoted \(\langle u,v\rangle:=u^{\top}v\). For a positive semi-definite matrix \(\Sigma:n\times n\) we define the \(\Sigma\)-inner product on \(\mathbb{R}^{n}\) by \(\langle u,v\rangle_{\Sigma}:=u^{\top}\Sigma v\) and the \(\Sigma\)-norm by \(\left\lVert u\right\rVert_{\Sigma}=\sqrt{\langle u,u\rangle_{\Sigma}}\). For \(n\in\mathbb{N}\) (made clear by context) we let \(e_{1},\ldots,e_{n}\in\mathbb{R}^{n}\) be the standard basis vectors \(e_{i}(j):=\mathbb{1}\left[j=i\right]\). For a vector \(v\in\mathbb{R}^{n}\) and set \(S\subseteq[n]\) we write \(vs\) to denote the restriction of \(v\) to coordinates in \(S\). For symmetric matrices \(A,B:n\times n\) we write \(A\preceq B\) to denote that \(B-A\) is positive semi-definite.

### Covering, packing, and \(\ell_{1}\)-representability

We previously defined the covering number of \(t\)-sparse vectors with respect to a covariance matrix \(\Sigma\). We next define the packing number (i.e. correlational statistical dimension) and \(\ell_{1}\)-representability, and discuss the connections between these quantities as well as their algorithmic implications.

**Definition A.1**.: Let \(\Sigma:n\times n\) be a positive semi-definite matrix and let \(t,\alpha>0\). A set \(\{v_{1},\ldots,v_{N}\}\subseteq\mathbb{R}^{n}\) is a \((t,\alpha)\)-packing for \(\Sigma\) if every \(v_{i}\) is \(t\)-sparse, and

\[\left|\langle v_{i},v_{j}\rangle_{\Sigma}\right|<\alpha\left\lVert v_{i} \right\rVert_{\Sigma}\left\lVert v_{j}\right\rVert_{\Sigma}\]

for all \(i,j\in[N]\) with \(i\neq j\). The (correlational) statistical dimension of \(t\)-sparse vectors with maximum correlation \(\alpha\), under the \(\Sigma\)-inner product, is denoted \(\mathcal{P}_{t,\alpha}(\Sigma)\) and defined as the size of the largest \((t,\alpha)\)-packing.

We will make use of the following connections between packing, covering, and \(\ell_{1}\)-representability.

**Lemma A.2** (Covering \(\Leftrightarrow\) packing).: _For any positive semi-definite matrix \(\Sigma:n\times n\) and \(t,\alpha>0\), it holds that \((\alpha^{2}/3)\mathcal{P}_{t,\alpha^{2}/2}(\Sigma)\leq\mathcal{N}_{t,\alpha}( \Sigma)\leq\mathcal{P}_{t,\alpha}(\Sigma)\)._

Proof.: **First inequality.** Let \(\{D_{1},\ldots,D_{N}\}\) be any maximum-size \((t,\alpha^{2}/2)\)-packing. Since the \(D_{i}\)'s are all \(t\)-sparse, each must be correlated with some element of a \((t,\alpha)\)-dictionary. Thus, it suffices to show that for any \(v\in\mathbb{R}^{n}\), the set \(S(v):=\{i\in[N]:\left|\langle D_{i},v\rangle_{\Sigma}\right|\geq\alpha\left\lVert D _{i}\right\rVert_{\Sigma}\left\lVert v\right\rVert_{\Sigma}\}\) has size \(\left|S(v)\right|\leq 3/\alpha^{2}\). Indeed, for any \(i,j\in S(v)\) with \(i\neq j\), we have by the definition of a packing that

\[\left\langle D_{i}-\frac{\langle D_{i},v\rangle_{\Sigma}}{\left\lVert v \right\rVert_{\Sigma}^{2}}v,D_{j}-\frac{\langle D_{j},v\rangle_{\Sigma}}{ \left\lVert v\right\rVert_{\Sigma}^{2}}v\right\rangle_{\Sigma} =\langle D_{i},D_{j}\rangle-\frac{\langle D_{i},v\rangle_{\Sigma }\langle D_{j},v\rangle_{\Sigma}}{\left\lVert v\right\rVert_{\Sigma}^{2}}\] \[\leq-\frac{\alpha^{2}}{2}\left\lVert D_{i}\right\rVert_{\Sigma} \left\lVert D_{j}\right\rVert_{\Sigma}.\]

For each \(i\in S(v)\) define \(R_{i}=D_{i}-\langle D_{i},v\rangle_{\Sigma}v/\left\lVert v\right\rVert_{\Sigma} ^{2}\). Then

\[0\leq\left\lVert\sum_{i\in S(v)}\frac{R_{i}}{\left\lVert R_{i}\right\rVert_{ \Sigma}}\right\rVert_{\Sigma}^{2}=\left|S(v)\right|+\sum_{i,j\in S(v):i\neq j }\frac{\langle R_{i},R_{j}\rangle_{\Sigma}}{\left\lVert R_{i}\right\rVert_{ \Sigma}\left\lVert R_{j}\right\rVert_{\Sigma}}\leq\left|S(v)\right|-\left|S(v )\right|(\left|S(v)\right|-1)\cdot\frac{\alpha^{2}}{2}\]

where the last inequality uses the bound \(\left\lVert R_{i}\right\rVert_{\Sigma}\leq\left\lVert D_{i}\right\rVert_{\Sigma}\). Rearranging gives \(\left|S(v)\right|\leq 1+(2/\alpha^{2})\).

**Second inequality.** Let \(\{D_{1},\ldots,D_{N}\}\) be any maximal \((t,\alpha)\)-packing. Then for any \(t\)-sparse \(v\in\mathbb{R}^{n}\), maximality implies that there must be some \(i\in[N]\) with \(\left|\langle D_{i},v\rangle_{\Sigma}\right|\geq\alpha\left\lVert D_{i} \right\rVert_{\Sigma}\left\lVert v\right\rVert_{\Sigma}\). So \(\{D_{1},\ldots,D_{N}\}\) is also a \((t,\alpha)\)-dictionary. 

**Lemma A.3** (\(\ell_{1}\)-representation \(\Longrightarrow\) covering).: _Let \(\Sigma:n\times n\) be a positive semi-definite matrix and let \(t,B>0\). If \(\{D_{1},\ldots,D_{N}\}\subseteq\mathbb{R}^{n}\) is a \((t,B)\)-\(\ell_{1}\)-representation for \(\Sigma\), then it is also a \((t,1/B)\)-dictionary for \(\Sigma\)._

[MISSING_PAGE_EMPTY:14]

_Fix a \(t\)-sparse vector \(v^{*}\in\mathbb{R}^{n}\), let \(X_{1},\ldots,X_{m}\sim N(0,\Sigma)\) be independent and let \(y_{i}=\left\langle X_{i},v^{*}\right\rangle+\xi_{i}\) where \(\xi_{i}\sim N(0,\sigma^{2})\). For any \(R>0\), define_

\[\hat{w}\in\operatorname*{argmin}_{w\in\mathbb{R}^{N}:\left\|w\right\|_{1}\leq BR }\left\|\mathbb{X}Dw-y\right\|_{2}^{2}\]

_where \(D:n\times N\) is the matrix with columns comprising the elements of \(\mathcal{D}\), and \(\mathbb{X}:m\times n\) is the matrix with rows \(X_{1},\ldots,X_{m}\). So long as \(m=\Omega(\log(n/\delta))\) and \(R\geq\left\|v^{*}\right\|_{\Sigma}\), it holds with probability at least \(1-\delta\) that_

\[\left\|D\hat{w}-w^{*}\right\|_{\Sigma}^{2}=O\left(BR\sigma\sqrt{\frac{\log(2n/ \delta)}{m}}+\frac{\sigma^{2}\log(4/\delta)}{m}+\frac{B^{2}R^{2}\log(n)}{m} \right).\]

Proof.: By \(\ell_{1}\)-representability and normalization of \(\mathcal{D}\), there is some \(w^{*}\in\mathbb{R}^{N}\) such that \(v^{*}=Dw^{*}\) and \(\left\|w^{*}\right\|_{1}\leq B\left\|v^{*}\right\|_{\Sigma}\leq BR\). Let \(\Gamma=D^{\top}\Sigma D\). Also, by normalization, \(\max_{i}\Gamma_{ii}=1\). Thus, we can apply standard "slow rate" Lasso guarantees to the samples \((D^{\top}X_{i},y_{i})_{i=1}^{m}\) to get the claimed bound (see e.g. Theorem 14 of [22]). 

### Optimizing the Lasso in near-linear time

**Theorem A.7** (see e.g. Corollary 4 and Section 5.3 in [34]).: _Let \(n,m,B,H,T\in\mathbb{N}\) and \(\sigma>0\). Fix \(X_{1},\ldots,X_{m}\in\mathbb{R}^{n}\) with \(\left\|X_{i}\right\|_{\infty}\leq H\) for all \(i\), and fix \(w^{*}\in\mathbb{R}^{n}\) with \(\left\|w^{*}\right\|_{1}\leq B\). For \(i\in[m]\) define \(y_{i}=\left\langle X_{i},w^{*}\right\rangle+\xi_{i}\) where \(\xi_{i}\sim N(0,\sigma^{2})\) are independent random variables. Given \((X_{i},y_{i})_{i=1}^{m}\) as well as \(B\), \(T\), and \(\sigma^{2}\), there is an algorithm MirrorDescentLasso\((\left\langle X_{i},y_{i}\right\rangle_{i=1}^{m}\), \(B\), \(T\), \(\sigma^{2})\), which optimizes the Lasso objective via \(T\) iterations of mirror descent, that produces an estimate \(\hat{w}\in\mathbb{R}^{n}\) satisfying \(\left\|\hat{w}\right\|_{1}\leq B\) and, with probability \(1-o(1)\),_

\[\frac{1}{m}\left\|X\hat{w}-y\right\|_{2}^{2}\leq\frac{1}{m}\left\|Xw^{*}-y \right\|_{2}^{2}+\tilde{O}\left(\frac{H^{2}B^{2}}{T}+\sqrt{\frac{H^{2}B^{2} \sigma^{2}}{T}}\right).\]

_Moreover, the time complexity of MirrorDescentLasso() is \(\tilde{O}(nmT)\)._

**Theorem A.8**.: _Let \(n,m,B,H\in\mathbb{N}\) and \(\sigma>0\). Let \(\Sigma:n\times n\) be positive semi-definite with \(\max_{j\in[n]}\Sigma_{jj}\leq H^{2}\). Fix \(w^{*}\in\mathbb{R}^{n}\) with \(\left\|w^{*}\right\|_{1}\leq B\). Let \((X_{i},y_{i})_{i=1}^{m}\) be independent draws where \(X_{i}\sim N(0,\Sigma)\) and \(y_{i}=\left\langle X_{i},w^{*}\right\rangle+N(0,\sigma^{2})\). Then MirrorDescentLasso\((\left\langle X_{i},y_{i}\right\rangle_{i=1}^{m}\), \(B\), \(m\), \(\sigma^{2})\) computes, in time \(\tilde{O}(nm^{2})\), an estimate \(\hat{w}\) satisfying, with probability \(1-o(1)\),_

\[\left\|\hat{w}-w^{*}\right\|_{\Sigma}^{2}\leq\tilde{O}\left(\frac{\sigma^{2}}{ \sqrt{m}}+\frac{\sigma HB}{\sqrt{m}}+\frac{H^{2}B^{2}}{m}\right)\]

Proof.: Since \(\max_{j}\Sigma_{jj}\leq H\) we have that \(\max_{i}\left\|X_{i}\right\|_{\infty}\leq O(H\log n)\) with probability \(1-o(1)\). Applying Theorem A.7 with this bound and with \(T=m\), we obtain some \(\hat{w}\in\mathbb{R}^{n}\) with \(\left\|\hat{w}\right\|_{1}\leq B\) and, with probability \(1-o(1)\),

\[\frac{1}{m}\left\|X\hat{w}-y\right\|_{2}^{2}\leq\frac{1}{m}\left\|Xw^{*}-y \right\|_{2}^{2}+\epsilon\]

where \(\epsilon=\tilde{O}(H^{2}B^{2}/m)+\sqrt{H^{2}B^{2}\sigma^{2}/m})\). By \(\chi^{2}\)-concentration, we have \(\frac{1}{m}\left\|Xw^{*}-y\right\|_{2}^{2}\leq\sigma^{2}(1+O(1/\sqrt{m}))\) with probability \(1-o(1)\). Thus,

\[\left\|X\hat{w}-y\right\|_{2}\leq\left\|Xw^{*}-y\right\|_{2}+\sqrt{\epsilon m }\leq\sigma\sqrt{m}+O(\sigma m^{1/4})+\sqrt{\epsilon m}\]

and

\[\left\|X\hat{w}-y\right\|_{2}^{2}\leq\left\|Xw^{*}-y\right\|_{2}^{2}+m \epsilon\leq\sigma^{2}m+O(\sigma^{2}\sqrt{m})+\epsilon m.\]

Next, since \(\sup_{w\in\mathbb{R}^{n}:\left\|w\right\|_{1}\leq B}\langle w-w^{*},x\rangle \leq 2B\left\|x\right\|_{\infty}\leq O(HB\log n)\) with probability \(1-o(1)\) over \(x\sim N(0,\Sigma)\), we can apply Theorem C.1 to get that with probability \(1-o(1)\),

\[\left\|\hat{w}-w^{*}\right\|_{\Sigma}^{2}+\sigma^{2}\leq\frac{1+\tilde{O}(1/ \sqrt{m})}{m}(\left\|X\hat{w}-y\right\|_{2}+\tilde{O}(HB))^{2}.\]Substituting the bounds on \(\left\|X\hat{w}-y\right\|_{2}\) and \(\left\|X\hat{w}-y\right\|_{2}^{2}\) gives

\[\left\|\hat{w}-w^{*}\right\|_{\Sigma}^{2}+\sigma^{2}\leq\sigma^{2}+O(\sigma^{2}m ^{-1/2}+\epsilon)+\tilde{O}(\sigma HBm^{-1/2}+HB\sqrt{\epsilon/m})+\tilde{O}(H^ {2}B^{2}/m).\]

Substituting in the value of \(\epsilon\) and simplifying, we get

\[\left\|\hat{w}-w^{*}\right\|_{\Sigma}^{2}\leq\tilde{O}\left(\frac{\sigma^{2}}{ \sqrt{m}}+\frac{\sigma HB}{\sqrt{m}}+\frac{H^{2}B^{2}}{m}\right)\]

as claimed. 

## Appendix B Iterative Peeling

In this section we give the complete proof of Lemma 2.4, restated below as Theorem B.1, which describes the guarantees of IterativePeeling() (see Algorithm 1). This is a key ingredient in the proofs of Theorems 1.1 and 1.2. We also use it to formally prove Theorem 2.3, as well as Lemma 2.9.

**Theorem B.1**.: _Let \(n,t,d\in\mathbb{N}\). Let \(\Sigma:n\times n\) be a positive semi-definite matrix with eigenvalues \(\lambda_{1}\leq\cdots\leq\lambda_{n}\). Given \(\Sigma\), \(d\), and \(t\), there is a polynomial-time algorithm IterativePeeling() producing a set \(S\subseteq[n]\) with the following guarantees:_

* _For every_ \(t\)_-sparse_ \(v\in\mathbb{R}^{n}\)_, it holds that_ \(\left\|v_{[n]\setminus S}\right\|_{2}\leq 3\lambda_{d+1}^{-1/2}\left\|v\right\|_{\Sigma}\)_._
* \(\left|S\right|\leq(7t)^{2t+1}d\)_._

Essentially, the set \(S\) contains every coordinate \(i\in[n]\) that "participates" in an approximate sparse dependency, in the sense that there is some sparse linear combination of the covariates with small variance compared to the coefficient on \(i\). To compute \(S\), the algorithm IterativePeeling() first computes the orthogonal projection matrix \(P\) that projects onto the subspace spanned by the top \(n-d\) eigenvectors of \(\Sigma\). Starting with the set of coordinates that correlate with \(\ker(P)\), the procedure then iteratively grows \(S\) in such a way that at each step, a new participant of each approximate sparse dependency is discovered, but \(S\) does not become too much larger.

The following lemma will be needed to bound how much \(S\) grows at each iteration:

**Lemma B.2**.: _Let \(V\subseteq\mathbb{R}^{n}\) be a subspace with \(d:=\dim V\). For some \(\alpha>0\) define_

\[S=\left\{i\in[n]:\sup_{x\in V\setminus\{0\}}\frac{x_{i}}{\left\|x\right\|_{2}} \geq\alpha\right\}.\]

_Then \(\left|S\right|\leq d/\alpha^{2}\). Moreover, given a set of vectors that span \(V\), we can compute \(S\) in time \(\operatorname{poly}(n)\)._

Proof.: Let \(k:=\left|S\right|\) and without loss of generality suppose \(S=\{1,\ldots,k\}\). Define a matrix \(A\in\mathbb{R}^{n\times n}\) as follows. For \(1\leq i\leq k\) let row \(A_{i}\in V\) be some vector such that \(\left\|A_{i}\right\|_{2}=1\) and \(A_{ii}\geq\alpha\). For \(k+1\leq i\leq n\) let \(A_{i}=0\). Then \(\operatorname{tr}(A)\geq k\alpha\) and \(\left\|A\right\|_{F}=\sqrt{k}\). However, \(\operatorname{rank}(A)\leq d\), so the singular values \(\sigma_{1}\geq\sigma_{2}\geq\cdots\geq\sigma_{n}\geq 0\) of \(A\) satisfy \(\sigma_{d+1}=0\). Thus,

\[k\alpha\leq\operatorname{tr}(A)\leq\sum_{i=1}^{n}\sigma_{i}\leq\sqrt{d}\sqrt{ \sum_{i=1}^{n}\sigma_{i}^{2}}=\sqrt{d}\left\|A\right\|_{F}=\sqrt{dk}\]

where the second inequality is by e.g. Von Neumann's trace inequality, and the third inequality is by \(d\)-sparsity of the vector \(\sigma\). It follows that \(k\leq d/\alpha^{2}\) as claimed.

Let \(A\) be the matrix with columns consisting of the given spanning set for \(V\). By Gram-Schmidt, we may transform the spanning set into an orthonormal basis for \(V\), so that \(A\) has \(d\) columns, and \(A^{\top}A=I_{d}\). Fix \(i\in[n]\). Then \(\sup_{x\in V\setminus\{0\}}x_{i}/\left\|x\right\|_{2}\geq\alpha\) if and only if \((Av)_{i}^{2}-\alpha^{2}\left\|Av\right\|_{2}^{2}\geq 0\) for some nonzero \(v\in\mathbb{R}^{d}\). Equivalently, \((Av)_{i}^{2}\geq\alpha^{2}\) for some unit vector \(v\). This is possible if and only if \(\left\|A_{i}\right\|_{2}\geq\alpha\) (where \(A_{i}\) is the \(i\)-th row of \(A\)), which can be checked in polynomial time. 

For notational convenience, we also define the set \(\mathcal{W}_{P,S}\) of vectors \(v\) with unusually large norm outside the set \(S\).

**Definition B.3**.: For any matrix \(P:n\times n\) and subset \(S\subseteq[n]\), define \(\mathcal{W}_{P,S}:=\{v\in\mathbb{R}^{n}:\left\lVert v_{S^{c}}\right\rVert_{2}>3 \sqrt{v^{\top}Pv}\}\).

We then formalize the guarantee of each iteration of IterativePeeling() as follows:

**Lemma B.4**.: _Let \(n,t\in\mathbb{N}\) and let \(P:n\times n\) be an orthogonal projection matrix. Suppose \(\tau\geq 1\) and \(K\subseteq[n]\) satisfy_

1. \(P_{ii}\geq 1-1/(9t^{2})\) _for all_ \(i\not\in K\)_,_
2. \(|\operatorname{supp}(v)\setminus K|\leq\tau\) _for every_ \(v\in B_{0}(t)\cap\mathcal{W}_{P,K}\)_._

_Then there exists a set \(\mathcal{I}_{P}(K)\) with \(\left|\mathcal{I}_{P}(K)\right|\leq 36t^{2}|K|\) such that_

\[|\operatorname{supp}(v)\setminus(\mathcal{I}_{P}(K)\cup K)|\leq\tau-1\]

_for all \(v\in B_{0}(t)\cap\mathcal{W}_{P,K}\). Moreover, given \(P\), \(K\), and \(t\), we can compute \(\mathcal{I}_{P}(K)\) in time \(\operatorname{poly}(n)\)._

Proof.: We define the set

\[\mathcal{I}_{P}(K):=\left\{a\in[n]\setminus K:\sup_{x\in\operatorname{span}\{ Pe_{i}:i\in K\}\setminus\{0\}}\frac{\left|x_{a}\right|}{\left\lVert x\right\rVert _{2}}\geq 1/(6t)\right\}.\]

It is clear from Lemma B.2 (applied with parameters \(V:=\operatorname{span}\{Pe_{i}:i\in K\}\) and \(\alpha:=1/(6t)\)) that \(\left|\mathcal{I}_{P}(K)\right|\leq 36t^{2}|K|\), and that \(\mathcal{I}_{P}(K)\) can be computed in time \(\operatorname{poly}(n)\). It remains to show that \(|\operatorname{supp}(v)\setminus(\mathcal{I}_{P}(K)\cup K)|\leq\tau-1\) for all \(v\in B_{0}(t)\cap\mathcal{W}_{P,K}\).

Consider any \(v\in B_{0}(t)\cap\mathcal{W}_{P,K}\). Then \(\left\lVert v_{K^{c}}\right\rVert_{2}>3\sqrt{v^{\top}Pv}\). We have

\[\frac{\left\lVert v_{K^{c}}\right\rVert_{2}^{2}}{9}>v^{\top}Pv=\left\lVert Pv \right\rVert_{2}^{2}=\left\lVert\sum_{i=1}^{n}v_{i}P_{i}\right\rVert_{2}^{2} \tag{1}\]

where the first equality uses the fact that \(P\) is a projection matrix. We also know that

\[\left\lVert\sum_{i\in[n]\setminus K}v_{i}(P_{i}-e_{i})\right\rVert_{2}\leq \sum_{i\in[n]\setminus K}\left|v_{i}\right|\left\lVert P_{i}-e_{i}\right\rVert _{2}\leq\frac{1}{3\sqrt{t}}\left\lVert v_{K^{c}}\right\rVert_{1}\leq\frac{1} {3}\left\lVert v_{K^{c}}\right\rVert_{2} \tag{2}\]

by the triangle inequality, the bound \(\left\lVert P_{i}-e_{i}\right\rVert_{2}^{2}=(I-P)_{ii}=1-P_{ii}\leq 1/(9t)\) (since \(i\not\in K\)), and \(t\)-sparsity of \(v\). Moreover, (2) implies that

\[\left\lVert\sum_{i\in[n]\setminus K}v_{i}P_{i}\right\rVert_{2}\leq\left\lVert \sum_{i\in[n]\setminus K}v_{i}(P_{i}-e_{i})\right\rVert_{2}+\left\lVert v_{K^ {c}}\right\rVert_{2}\leq\frac{4}{3}\left\lVert v_{K^{c}}\right\rVert_{2}. \tag{3}\]

Combining (1) and (3), the triangle inequality gives

\[\left\lVert\sum_{i\in K}v_{i}P_{i}\right\rVert_{2}\leq\left\lVert\sum_{i\in[n] \setminus K}v_{i}P_{i}\right\rVert_{2}+\left\lVert\sum_{i=1}^{n}v_{i}P_{i} \right\rVert_{2}\leq\frac{5}{3}\left\lVert v_{K^{c}}\right\rVert_{2}. \tag{4}\]

Next, observe that

\[\frac{\left\lVert v_{K^{c}}\right\rVert_{2}^{2}}{3}\] (by (1)) \[\geq\left\lvert\left\langle\sum_{i=1}^{n}v_{i}P_{i},v_{K^{c}} \right\rangle\right\rvert\] (by Cauchy-Schwarz) \[\geq\left\lvert\left\langle\sum_{i\in[n]\setminus K}v_{i}P_{i},v _{K^{c}}\right\rangle\right\rvert-\left\lvert\left\langle\sum_{i\in K}v_{i}P_{i},v_{K^{c}}\right\rangle\right\rvert\] (by triangle inequality)\[\geq\left|\left\langle\sum_{i\in[n]\setminus K}v_{i}e_{i},v_{K^{c}} \right\rangle\right|-\left|\left\langle\sum_{i\in[n]\setminus K}v_{i}(P_{i}-e_{i} ),v_{K^{c}}\right\rangle\right|-\left|\left\langle\sum_{i\in K}v_{i}P_{i},v_{K ^{c}}\right\rangle\right|\] (by triangle inequality) \[\geq\left\|v_{K^{c}}\right\|_{2}^{2}-\left\|\sum_{i\in[n] \setminus K}v_{i}(P_{i}-e_{i})\right\|_{2}\left\|v_{K^{c}}\right\|_{2}-\left| \left\langle\sum_{i\in K}v_{i}P_{i},v_{K^{c}}\right\rangle\right|\] (by Cauchy-Schwarz) \[\geq\left\|v_{K^{c}}\right\|_{2}^{2}-\frac{1}{3}\left\|v_{K^{c}} \right\|_{2}^{2}-\left|\left\langle\sum_{i\in K}v_{i}P_{i},v_{K^{c}}\right\rangle\right|\] (by (2)) and hence \[\left|\left\langle\sum_{i\in K}v_{i}P_{i},v_{K^{c}}\right\rangle\right|>\frac {1}{3}\left\|v_{K^{c}}\right\|_{2}^{2}\geq\frac{1}{5}\left\|v_{K^{c}}\right\| _{2}\left\|\sum_{i\in K}v_{i}P_{i}\right\|_{2}\]

where the last inequality is by (4). On the other hand, observe that

\[\left|\left\langle\sum_{i\in K}v_{i}P_{i},v_{K^{c}}\right\rangle\right|\leq \sum_{j\in[n]\setminus K}|v_{j}|\cdot\left|\left\langle\sum_{i\in K}v_{i}P_{i},e_{j}\right\rangle\right|\leq\sqrt{t}\left\|v_{K^{c}}\right\|_{2}\max_{j\in \operatorname{supp}(v)\setminus K}\left|\left\langle\sum_{i\in K}v_{i}P_{i},e_ {j}\right\rangle\right|.\]

Hence, there is some \(j\in\operatorname{supp}(v)\setminus K\) such that

\[\left|\left\langle\sum_{i\in K}v_{i}P_{i},e_{j}\right\rangle\right|>\frac{1}{5 \sqrt{t}}\left\|\sum_{i\in K}v_{i}P_{i}\right\|_{2}.\]

So the vector \(x(v):=\sum_{i\in K}v_{i}P_{i}\in\operatorname{span}\{P_{i}:i\in K\}\) satisfies \(\left|x(v)_{j}\right|>\left\|x(v)\right\|_{2}/(5\sqrt{t})\). Moreover, \(x(v)\) is nonzero since \(\left|x(v)_{j}\right|>0\). Thus, \(j\in\mathcal{I}_{P}(K)\). Since we chose \(j\) to be in \(\operatorname{supp}(v)\setminus K\), it follows that

\[\left|\operatorname{supp}(v)\setminus(\mathcal{I}_{P}(K)\cup K)\right|\leq \left|\operatorname{supp}(v)\setminus K\right|-1\leq\tau-1\]

where the last inequality is by assumption (b) in the lemma statement. 

We can now complete the proof of Theorem B.1 by repeatedly invoking Lemma B.4 (this proof was given in Section 2.2 and is duplicated here for completeness).

Proof of Theorem B.1.: Let \(\Sigma=\sum_{i=1}^{n}\lambda_{i}u_{i}u_{i}^{\top}\) be the eigendecomposition of \(\Sigma\), and let \(P:=\sum_{i=d+1}^{n}u_{i}u_{i}^{\top}\) be the projection onto the top \(n-d\) eigenspaces of \(\Sigma\). Set \(K_{t}=\{i\in[n]:P_{ii}<1-1/(9t^{2})\}\). Because \(\operatorname{tr}(P)=n-d\) and \(P_{ii}\leq 1\) for all \(i\in[n]\), it must be that \(\left|K_{t}\right|\leq 9t^{2}d\). Also, for any \(v\in B_{0}(t)\cap\mathcal{W}_{P,K_{t}}\) we have trivially by \(t\)-sparsity that \(\left|\operatorname{supp}(v)\setminus K_{t}\right|\leq t\).

Define \(K_{t-1}\) to be \(K_{t}\cup\mathcal{I}_{P}(K_{t})\) where \(\mathcal{I}_{P}(K_{t})\) is as defined in Lemma B.4; we have the guarantees that \(\left|K_{t-1}\right|\leq(1+36t^{2})\left|K_{t}\right|\) and \(\left|\mathcal{G}_{P}(v)\setminus K_{t}\right|\leq t-1\) for all \(v\in B_{0}(t)\cap\mathcal{W}_{P,K_{t}}\). Since \(K_{t-1}\supseteq K_{t}\), it holds that \(\mathcal{W}_{P,K_{t-1}}\subseteq\mathcal{W}_{P,K_{t}}\), and thus \(\left|\mathcal{G}_{P}(v)\setminus K_{t}\right|\leq t-1\) for all \(v\in B_{0}(t)\cap\mathcal{W}_{P,K_{t-1}}\). Moreover, since \(K_{t-1}\supseteq K_{t}\), it obviously holds that \(P_{ii}\geq 1-1/(9t^{2})\) for all \(i\not\in K_{t-1}\). This means we can apply Lemma B.4 with \(\tau:=t-1\) and \(K:=K_{t-1}\) and so iteratively define sets \(K_{t-2}\subseteq\cdots\subseteq K_{1}\subseteq K_{0}=[n]\) in the same way. In the end, we obtain the set \(K_{0}\subseteq[n]\) with \(\left|K_{0}\right|\leq 9t^{2}d(1+36t^{2})^{t}\) and \(\operatorname{supp}(v)\subseteq K_{0}\) for all \(v\in B_{0}(t)\cap\mathcal{W}_{P,K_{0}}\). The latter guarantee means that in fact \(B_{0}(t)\cap\mathcal{W}_{P,K_{0}}=\emptyset\). So for any \(t\)-sparse \(v\in\mathbb{R}^{n}\) it holds that

\[\left\|v_{K_{0}^{c}}\right\|_{2}\leq 3\sqrt{v^{\top}Pv}\leq 3\lambda_{d+1}^{-1/2} \sqrt{v^{\top}\Sigma v}\]

where the last inequality holds since \(\lambda_{d+1}P\preceq\Sigma\).

**Proof of Lemma 2.9.** By Theorem B.1, there is a polynomial-time computable set \(S\subseteq[n]\) such that \(\left\|v_{S^{c}}\right\|_{2}\leq 3\sqrt{\lambda}d_{4+1}^{-1/2}\left\|v\right\|_{\Sigma}\) for all \(v\in B_{0}(t)\), and \(\left|S\right|\leq(7t)^{2t+1}d\). Let the dictionary \(\mathcal{D}\) consist of the standard basis \(\left\{e_{1},\ldots,e_{n}\right\}\) together with a \(\Sigma\)-orthogonal basis for each subspace spanned by \(t\) vectors in \(\left\{e_{i}:i\in S\right\}\). Let \(v\in\mathbb{R}^{n}\) be \(t\)-sparse. Let \(v_{S}\) denote the restriction of \(v\) to \(S\), i.e. \(v_{S}:=v-\sum_{i\in[n]\setminus S}v_{i}e_{i}\). By construction of the dictionary, there is a \(\Sigma\)-orthogonal basis for \(\left\{e_{i}:i\in S\cap\operatorname{supp}(v)\right\}\), so there are \(d_{1},\ldots,d_{t}\in\mathcal{D}\) and coefficients \(b_{d_{1}},\ldots,b_{d_{t}}\in\mathbb{R}\) with \(v_{S}=\sum_{i=1}^{t}b_{d_{i}}d_{i}\) and \(\left\langle d_{i},d_{j}\right\rangle_{\Sigma}=0\) for all \(i,j\in[t]\) with \(i\neq j\). Note that \(\left\|v_{S}\right\|_{\Sigma}^{2}=\sum_{i=1}^{t}b_{d_{i}}^{2}\left\|d_{i} \right\|_{\Sigma}^{2}\), so

\[\sum_{i=1}^{t}\left|b_{d_{i}}\right|\left\|d_{i}\right\|_{\Sigma}\leq\sqrt{t} \sqrt{\sum_{i=1}^{t}b_{d_{i}}^{2}\left\|d_{i}\right\|_{\Sigma}^{2}}=\sqrt{t} \left\|v_{S}\right\|_{\Sigma}.\]

Now, we claim that the desired coefficient vector \(\left\{\alpha_{d}:d\in\mathcal{D}\right\}\) for \(v\) is defined by \(\alpha_{d}=b_{d}+\sum_{i\in[n]\setminus S}v_{i}\mathbbm{1}[d=e_{i}]\). We can check that \(\sum_{d\in\mathcal{D}}\alpha_{d}d=\sum_{i=1}^{t}b_{d_{i}}+\sum_{i\in[n] \setminus S}v_{i}e_{i}=v\). Also,

\[\left\|v_{S}\right\|_{\Sigma} \leq\left\|v\right\|_{\Sigma}+\left\|v_{S^{c}}\right\|_{\Sigma}\] \[\leq\left\|v\right\|_{\Sigma}+\sqrt{\lambda_{n}}\left\|v_{S^{c}} \right\|_{2}\] \[\leq(1+3\sqrt{\lambda_{n}/\lambda_{d+1}})\left\|v\right\|_{\Sigma}\]

by the guarantee of set \(S\).

It follows that

\[\sum_{i=1}^{t}\left|b_{d_{i}}\right|\left\|d_{i}\right\|_{\Sigma}\leq(1+3\sqrt {\lambda_{n}/\lambda_{d+1}})\sqrt{t}\left\|v\right\|_{\Sigma}\sqrt{\lambda_{n} /\lambda_{d+1}}.\]

Thus,

\[\sum_{d\in\mathcal{D}}\left|c_{d}\right|\left\|d\right\|_{\Sigma} \leq(1+3\sqrt{\lambda_{n}/\lambda_{d+1}})\sqrt{t}\left\|v\right\| _{\Sigma}+\sum_{i\in[n]\setminus S}\left|v_{i}\right|\left\|e_{i}\right\|_{\Sigma}\] \[\leq(1+3\sqrt{\lambda_{n}/\lambda_{d+1}})\sqrt{t}\left\|v\right\| _{\Sigma}+\sqrt{t}\left\|v_{S^{c}}\right\|_{2}\sqrt{\lambda_{n}}\] \[\leq(1+3\sqrt{\lambda_{n}/\lambda_{d+1}})\sqrt{t}\left\|v\right\| _{\Sigma}+3\sqrt{t}\left\|v\right\|_{\Sigma}\sqrt{\lambda_{n}/\lambda_{d+1}}\] \[\leq 7\sqrt{t}\sqrt{\lambda_{n}/\lambda_{d

**Theorem C.1** (Theorem 1 in [45]).: _Let \(n,m\in\mathbb{N}\) and \(\epsilon,\delta,\sigma>0\). Let \(\Sigma:n\times n\) be a positive semi-definite matrix and fix \(w^{*}\in\mathbb{R}^{n}\). Let \(X:m\times n\) have i.i.d. rows \(X_{1},\ldots,X_{m}\sim N(0,\Sigma)\), and let \(y=Xw^{*}+\xi\) where \(\xi\sim N(0,\sigma^{2}I_{m})\). Let \(F:\mathbb{R}^{d}\to[0,\infty]\) be a continuous function such that_

\[\Pr_{x\sim N(0,\Sigma)}[\sup_{w\in\mathbb{R}^{n}}\left\langle w-w^{*},x\right\rangle -F(w)>0]\leq\delta.\]

_If \(m\geq 196\epsilon^{-2}\log(12/\delta)\), then with probability at least \(1-4\delta\) it holds that for all \(w\in\mathbb{R}^{d}\),_

\[\left\|w-w^{*}\right\|_{\Sigma}^{2}+\sigma^{2}\leq\frac{1+\epsilon}{m}\left( \left\|Xw-y\right\|_{2}+F(w)\right)^{2}.\]

In classical settings, e.g. (a) where \(\left\|v^{*}\right\|_{1}\) is bounded and \(\max_{i}\Sigma_{ii}\leq 1\) (see Proposition A.6) or (b) where \(\Sigma\) satisfies the compatibility condition (see Definition G.1), the above result can be applied together with the straightforward bound \(\left\langle v-v^{*},X\right\rangle\leq\left\|v-v^{*}\right\|_{1}\left\|X \right\|_{\infty}\). To prove Theorem C.2 we follow the same general recipe as (a), with several modifications.

First, since \(\max_{i}\Sigma_{ii}\) could be arbitrarily large, we need to treat the (few) large eigenspaces of \(\Sigma\) separately when bounding \(\left\langle v-v^{*},X\right\rangle\). Similarly, since Theorem B.1 only gives bounds on \(v^{*}\) for coordinates outside \(S\), we separately bound \(\left\langle(v-v^{*})_{S},X\right\rangle\) using that \(\left|S\right|\) is small. Second, to achieve the optimal rate of \(\sigma^{2}n_{\text{eff}}/m\) rather than \(\sigma^{2}\sqrt{n_{\text{eff}}/m}\), we do not directly apply Theorem C.1 to the noisy samples \((X_{i},y_{i})\); instead, we derive a modification of that result (Lemma F.7) that only invokes Theorem C.1 on the noiseless samples \((X_{i},\left\langle X_{i},v^{*}\right\rangle)\), and separately bounds the in-sample prediction error \(\left\|\mathbb{X}(\hat{v}-v^{*})\right\|_{2}\). A similar technique is used in [45] for constrained least-squares programs (see their Lemma 15); our Lemma F.7 applies to a broad family of additively regularized programs, which obviates the need to independently estimate \(\left\|v^{*}\right\|_{\Sigma}\) but otherwise achieves comparable bounds.

**Theorem C.2**.: _Let \(n,t,d_{l},d_{h},m\in\mathbb{N}\) and \(\sigma,\delta>0\). Let \(\Sigma:n\times n\) be a positive semi-definite matrix with eigenvalues \(\lambda_{1}\leq\cdots\leq\lambda_{n}\). Let \((X_{i},y_{i})_{i=1}^{m}\) be independent samples where \(X_{i}\sim N(0,\Sigma)\) and\(y_{i}=\langle X_{i},v^{*}\rangle+\xi_{i}\), for \(\xi_{i}\sim N(0,\sigma^{2})\) and a fixed \(t\)-sparse vector \(v^{*}\in\mathbb{R}^{n}\). Let \(\hat{v}\) be the output of \(\mathsf{AdaptivelyRegularizedLasso}(\Sigma,(X_{i},y_{i})_{i=1}^{m},t,d_{l},\delta)\). Let \(n_{\text{eff}}:=(7t)^{2t+1}d_{l}+d_{h}+\log(48/\delta)\) and let \(r_{\text{eff}}:=t(\lambda_{n-d_{h}}/\lambda_{d_{l}+1})\log(12n/\delta)\). There are absolute constants \(c,C>0\) so that the following holds. If \(m\geq Cn_{\text{eff}}\), then with probability at least \(1-\delta\),_

\[\|\hat{v}-v^{*}\|_{\Sigma}^{2}\leq c\left(\frac{\sigma^{2}n_{\text{eff}}}{m}+ \frac{(\sigma+\|v^{*}\|_{\Sigma})\,\|v^{*}\|_{\Sigma}\,\sqrt{r_{\text{eff}}}} {\sqrt{m}}+\frac{\|v^{*}\|_{\Sigma}^{2}\,r_{\text{eff}}}{m}\right).\]

Proof.: Define projection matrix \(P:=\sum_{i=1}^{n-d_{h}}u_{i}u_{i}^{\top}\), so that \(\operatorname{rank}(P^{\perp})=d_{h}\) and \(\lambda_{\text{max}}(P\Sigma P)\leq\lambda_{n-d_{h}}\). For any \(v\in\mathbb{R}^{n}\) and \(X\sim N(0,\Sigma)\), we can bound

\[\langle v-v^{*},X\rangle =\langle(v-v^{*})_{S^{c}},PX\rangle+\langle(v-v^{*})_{S^{c}},P^{ \perp}X\rangle+\langle(v-v^{*})_{S},X\rangle\] \[=\langle(v-v^{*})_{S^{c}},PX\rangle+\langle\Sigma^{1/2}(v-v^{*}), \Sigma^{-1/2}(P^{\perp}X)_{S^{c}}\rangle+\langle\Sigma^{1/2}(v-v^{*}),\Sigma^{- 1/2}X_{S}\rangle\] \[\leq\left\|(v-v^{*})_{S^{c}}\right\|_{1}\left\|PX\right\|_{\infty }+\left\|\Sigma^{1/2}(v-v^{*})\right\|_{2}\left(\left\|Z\right\|_{2}+\left\|W \right\|_{2}\right)\]

where \(PX\sim N(0,P\Sigma P)\), \(Z\sim N(0,\Sigma^{-1/2}(P^{\perp}\Sigma P^{\perp})_{S^{c}S^{c}}\Sigma^{-1/2})\), and \(W\sim N(0,\Sigma^{-1/2}\Sigma_{SS}\Sigma^{-1/2})\). First, since \(\max_{i}(P\Sigma P)_{ii}\leq\lambda_{\text{max}}(P\Sigma P)\leq\lambda_{n-d_{h}}\), we have the Gaussian tail bound

\[\Pr\left[\left\|PX\right\|_{\infty}>\sqrt{\lambda_{n-d_{h}}\cdot 2\log(12n/ \delta)}\right]\leq\delta/12.\]

Second, since

\[\Sigma^{-1/2}(P^{\perp}\Sigma P^{\perp})_{S^{c}S^{c}}\Sigma^{-1/2} \preceq\Sigma^{-1/2}(P^{\perp}\Sigma P^{\perp})\Sigma^{-1/2}\] (by Cauchy Interlacing Theorem) \[=P^{\perp}\] (since

\[P^{\perp}\]

 commutes with

\[\Sigma\]

)

we have that \(\left\|Z\right\|_{2}^{2}\) is stochastically dominated by \(\chi_{d_{h}}^{2}\), and thus

\[\Pr\left[\left\|Z\right\|_{2}>\sqrt{2d_{h}}\right]\leq e^{-m/4}\leq\delta/12.\]

Third, similarly, since \(\Sigma^{-1/2}\Sigma_{SS}\Sigma^{-1/2}\preceq I\) (again by Cauchy Interlacing Theorem) and also \(\operatorname{rank}(\Sigma^{-1/2}\Sigma_{SS}\Sigma^{-1/2})\leq\left|S\right|\), we have that \(\left\|W\right\|_{2}^{2}\) is stochastically dominated by \(\chi_{|S|}^{2}\), and thus

\[\Pr\left[\left\|W\right\|_{2}^{2}>\sqrt{2|S|}\right]\leq e^{-m/4}\leq\delta/12.\]

Combining the above bounds, we have that with probability at least \(1-\delta/4\) over \(X\sim N(0,\Sigma)\), for all \(v\in\mathbb{R}^{n}\),

\[\langle v-v^{*},X\rangle\leq\left\|(v-v^{*})_{S^{c}}\right\|_{1}\sqrt{\lambda_{ n-d_{h}}\cdot 2\log(12n/\delta)}+\left\|\Sigma^{1/2}(v-v^{*})\right\|_{2}(\sqrt{2 d_{h}}+\sqrt{2|S|}).\]

We can therefore apply Lemma F.7 with covariance \(\Sigma\), seminorm \(\Phi(v):=2\sqrt{2\lambda_{n-d_{h}}\log(12n/\delta)}\left\|v_{S^{c}}\right\|_{1}\), \(p:=4(d_{h}+|S|)\), ground truth \(v^{*}\), samples \((X_{i},y_{i})_{i=1}^{m}\), and failure probability \(\delta/4\). By the bound on \(|S|\) (Theorem B.1) we have \(|S|+d_{h}\leq(7t)^{2t+1}d_{l}+d_{h}\leq n_{\text{eff}}\), so it holds that \(m\geq 16p+196\log(48/\delta)\). Thus, with probability at least \(1-2\delta\), we have

\[\|\hat{v}-v^{*}\|_{\Sigma}^{2}\leq O\left(\frac{\sigma^{2}n_{\text{eff}}}{m}+ \frac{(\sigma+\|v^{*}\|_{\Sigma})\,\|v^{*}_{S^{c}}\|_{1}\,\sqrt{\lambda_{n-The limitation of AdaptivelyRegularizedLasso() is that the excess risk bound depends on \(\|v^{*}\|_{\Sigma}^{2}\) rather than just \(\sigma^{2}\). We next show that by a boosting approach, we can exponentially attenuate that dependence, essentially achieving the near-optimal rate of \(\sigma^{2}n_{\text{eff}}/m\). The key insight is that after producing an estimate \(\hat{v}\) of \(v^{*}\), we can augment the set of covariates with the feature \(\langle\mathbb{X},\hat{v}\rangle\), and try to predict the response \(y-\langle\mathbb{X},\hat{v}\rangle\), which is now a \((t+1)\)-sparse combination of the features. In standard settings, this is typically a bad idea because it introduces a sparse linear dependence. However, by the Cauchy Interlacing Theorem it increases the number of outlier eigenvalues by at most one - so our algorithms still apply. Thus, if we have enough samples that the excess risk bound in Theorem C.2 is non-trivially smaller than \(\|v^{*}\|_{\Sigma}^{2}\), then we can iteratively achieve better and better estimates up to the noise limit. This is precisely what BOAR-Lasso() does; the precise guarantees are stated in the following theorem, which completes the proof of Theorem 1.1.

**Theorem C.3**.: _Let \(n,t,d_{l},d_{h},m,L\in\mathbb{N}\) and \(\sigma,\delta>0\). Let \(\Sigma:n\times n\) be a positive semi-definite matrix with eigenvalues \(\lambda_{1}\leq\cdots\leq\lambda_{n}\). Let \((X_{i},y_{i})_{i=1}^{m}\) be independent samples where \(X_{i}\sim N(0,\Sigma)\) and \(y_{i}=\langle X_{i},v^{*}\rangle+\xi_{i}\), for \(\xi_{i}\sim N(0,\sigma^{2})\) and a fixed \(t\)-sparse vector \(v^{*}\in\mathbb{R}^{n}\)._

_Then, given \(\Sigma\), \((X_{i},y_{i})_{i=1}^{m}\), \(t\), \(d_{l}\), and \(\delta\), the algorithm BOAR-Lasso() outputs an estimator \(\hat{v}\) with the following properties._

_Let \(n_{\text{eff}}:=(7t)^{2t+1}d_{l}+d_{h}+\log(48/\delta)\) and let \(r_{\text{eff}}:=t(\lambda_{n-d_{h}}/\lambda_{d_{l}+1})\log(12n/\delta)\). There are absolute constants \(c_{0},C_{0}>0\) such that the following holds. If \(m\geq C_{0}L(n_{\text{eff}}+r_{\text{eff}})\), then with probability at least \(1-\delta\), it holds that_

\[\|\hat{v}-v^{*}\|_{\Sigma}^{2}\leq c_{0}\frac{\sigma^{2}(n_{\text{eff}}+r_{ \text{eff}})}{m/L}+2^{-L}\cdot\|v^{*}\|_{\Sigma}^{2}\,.\]

_Moreover, BOAR-Lasso() has time complexity \(\operatorname{poly}(n,m,t)\)._

Proof.: Let \((A_{0},\ldots,A_{L-1})\) be an partition of \([m]\) into \(L\) sets of size \(m/L\). The idea of the algorithm is to compute vectors \(\hat{v}^{(1)},\ldots,\hat{v}^{(L)}\) where each \(v^{(i)}\) is an estimate of \(v^{*}-\sum_{j=1}^{i-1}\hat{v}^{(j)}\). Concretely, fix some \(0\leq j\leq L-1\) and suppose that we have computed some vectors \(\hat{v}^{(1)},\ldots,\hat{v}^{(j)}\). Set \(\hat{s}^{(j)}:=\hat{v}^{(1)}+\cdots+\hat{v}^{(j)}\). Define a matrix \(\Sigma^{(j)}:(n+1)\times(n+1)\) by

\[\Sigma^{(j)}:=\begin{bmatrix}\Sigma&(\hat{s}^{(j)})^{\top}\Sigma\\ \Sigma\hat{s}^{(j)}&(\hat{s}^{(j)})^{\top}\Sigma(\hat{s}^{(j)})\end{bmatrix}.\]

Thus, for example, \(\Sigma^{(0)}\) has zeroes in the last row and last column. Now for each \(i\in A_{j}\), define \((X_{i}^{(j)},y_{i}^{(j)})\) by

\[X_{i}^{(j)} :=(X_{i},\langle X_{i},\hat{s}^{(j)}\rangle)\] \[y_{i}^{(j)} :=y_{i}-\langle X_{i},\hat{s}^{(j)}\rangle.\]

By construction, the \(m/L\) samples \((X_{i}^{(j)},y_{i}^{(j)})_{i\in A_{j}}\) are independent and distributed as \(X_{i}^{(j)}\sim N(0,\Sigma^{(j)})\) and \(y_{i}^{(j)}=\langle X_{i}^{(j)},(v^{*},-1)\rangle+\xi_{i}\). Let \(\lambda_{1}^{(j)}\leq\cdots\leq\lambda_{n+1}^{(j)}\) be the eigenvalues of \(\Sigma^{(j)}\).

Now we apply Theorem C.2 with covariance \(\Sigma^{(j)}\), samples \((X_{i}^{(j)},y_{i}^{(j)})_{i\in A_{j}}\), sparsity \(t+1\), outlier counts \(d_{l}+1\) and \(d_{h}+1\), and failure probability \(\delta/L\); let \(n_{\text{eff}}^{(j)}\) and \(r_{\text{eff}}^{(j)}\) be the induced parameters defined in that theorem statement, and let \(c,C\) be the constants. By the Cauchy Interlacing Theorem, we have \(\lambda_{d_{l}+2}^{(j)}\geq\lambda_{d_{l}+1}\) and similarly \(\lambda_{n+1-(d_{h}+1)}^{(j)}\leq\lambda_{n-d_{h}}\). Thus \(r_{\text{eff}}^{(j)}\leq 2r_{\text{eff}}\). Also \(n_{\text{eff}}^{(j)}\leq n_{\text{eff}}\). Thus, if the constant \(C_{0}\) is chosen appropriately large, then \(m/L\geq 16cr_{\text{eff}}^{(j)}\) and also \(m/L\geq Cn_{\text{eff}}^{(j)}\). Hence (by the error guarantee of Theorem C.2) with probability at least \(1-\delta/L\) we obtain a vector \(\hat{w}^{(j+1)}\) such that

\[\left\|\hat{w}^{(j+1)}-(v^{*},-1)\right\|_{\Sigma^{(j)}}^{2} \leq\frac{c\sigma^{2}n_{\text{eff}}^{(j)}}{m/L}+c\left\|(v^{*},-1) \right\|_{\Sigma^{(j)}}^{2}\sqrt{\frac{r_{\text{eff}}^{(j)}}{m/L}}+c\sigma \left\|(v^{*},-1)\right\|_{\Sigma^{(j)}}\sqrt{\frac{r_{\text{eff}}^{(j)}}{m/L}}\] \[\leq\frac{2c\sigma^{2}n_{\text{eff}}}{m/L}+\frac{\left\|(v^{*},-1 )\right\|_{\Sigma^{(j)}}^{2}}{4}+\left(\frac{\left\|(v^{*},-1)\right\|_{ \Sigma^{(j)}}^{2}}{4}+\frac{4c^{2}\sigma^{2}r_{\text{eff}}}{m/L}\right)\]\[\leq\frac{c_{0}}{2}\frac{\sigma^{2}(n_{\text{eff}}+r_{\text{eff}})}{m/L}+\frac{\| (v^{*},-1)\|_{\Sigma^{(j)}}^{2}}{2} \tag{5}\]

where the second inequality uses AM-GM to bound the third term, and the third inequality is by choosing \(c_{0}\geq 4c+8c^{2}\).

But now define \(\hat{v}^{(j+1)}:=\hat{w}_{[n]}^{(j+1)}+\hat{w}_{n+1}^{(j+1)}\hat{s}^{(j)}\). Then we observe that \(\|(v^{*},-1)\|_{\Sigma^{(j)}}^{2}=\left\|v^{*}-\hat{s}^{(j)}\right\|_{\Sigma}^ {2}\) and \(\left\|\hat{w}^{(j+1)}-(v^{*},-1)\right\|_{\Sigma^{(j)}}^{2}=\left\|\hat{v}^{( j+1)}-(v^{*}-\hat{s}^{(j)})\right\|_{\Sigma}^{2}=\left\|v^{*}-\hat{s}^{(j+1)} \right\|_{\Sigma}^{2}\) where \(\hat{s}^{(j+1)}=\hat{v}^{(1)}+\cdots+\hat{v}^{(j+1)}\). So (5) is equivalent to

\[\left\|v^{*}-\hat{s}^{(j+1)}\right\|_{\Sigma}^{2}\leq\frac{c_{0}}{2}\frac{ \sigma^{2}(n_{\text{eff}}+r_{\text{eff}})}{m/L}+\frac{1}{2}\left\|v^{*}-\hat{ s}^{(j)}\right\|_{\Sigma}^{2}.\]

Inductively, we conclude that

\[\left\|v^{*}-\hat{s}^{(L)}\right\|_{\Sigma}^{2}\leq c_{0}\frac{\sigma^{2}(n_{ \text{eff}}+r_{\text{eff}})}{m/L}+2^{-L}\left\|v^{*}\right\|_{\Sigma}^{2}\]

as desired. The time complexity (see Algorithm 2 for full pseudocode) is dominated by \(L\) eigendecompositions of \(n\times n\) Hermitian matrices (each of which takes time \(O(n^{3})\) by e.g. the QR algorithm), as well as \(L\) convex optimizations (each of which takes time \(\tilde{O}(n^{3})\) to solve to inverse-polynomial accuracy [26], which is sufficient for the correctness proof). 

### An alternative algorithm (proof of Theorem 1.2)

In this section we prove Theorem 1.2, which essentially states that the sample complexity dependence on \(d_{l}\) in BOAR-Lasso() can be removed at the cost of a time complexity depending on \(d_{l}^{t}\). See Algorithm 3 for the pseudocode of how we modify AdaptivelyRegularizedLasso(): essentially, we brute force search over all size-\(t\) subsets of the set \(S\) produced by IterativePeeling(), construct an appropriate dictionary for each of these \(\binom{|S|}{t}\) subsets, and then perform a final model selection step (with fresh samples) to pick the best dictionary/estimator. The boosting step is exactly identical to that in BOAR-Lasso().

**Lemma C.4**.: _Let \(n,t,d\in\mathbb{N}\). Let \(\Sigma:n\times n\) be a positive semi-definite matrix with eigenvalues \(\lambda_{1}\leq\cdots\leq\lambda_{n}\). Then there is a family \(\mathcal{D}\subseteq\mathbb{R}^{n\times(n+t)}\) of size \(|\mathcal{D}|\leq(7t)^{2t^{2}+t}(2d)^{t}\), consisting entirely of \(n\times(n+t)\) matrices with the form_

\[D:=\left[I_{n}\quad d_{1}\quad\ldots\quad d_{t}\right],\]

_with the following property. For any \(t\)-sparse \(v\in\mathbb{R}^{n}\), there is some \(D\in\mathcal{D}\) and \(w\in\mathbb{R}^{n+k}\) with \(v=Dw\) and_

\[\left\|w\right\|_{1}\leq\frac{7t^{1/2}}{\sqrt{\lambda_{d+1}}}\sqrt{v^{\top} \Sigma v}.\]

Proof.: Let \(u_{1},\ldots,u_{n}\in\mathbb{R}^{n}\) be the eigenvectors of \(\Sigma\) corresponding to eigenvalues \(\lambda_{1},\ldots,\lambda_{n}\) respectively, so that \(\Sigma=\sum_{i=1}^{n}\lambda_{i}u_{i}u_{i}^{\top}\). Define \(\overline{\Sigma}:=\lambda_{d+1}^{-1}\sum_{i=1}^{n}\min(\lambda_{i},\lambda_{ d+1})u_{i}u_{i}^{\top}\). Let \(S\) be the output of IterativePeeling(\(\Sigma,d_{l},t\)), and let \(\mathcal{D}:=\{D(T):T\in\binom{S}{t}\}\), where for any \(T\in\binom{S}{t}\), we let \(\{d_{1},\ldots,d_{t}\}\) be a \(\overline{\Sigma}\)-orthonormal basis for \(\operatorname{span}\{e_{i}:i\in T\}\), and let \(D(T)\) be the \(n\times(n+t)\) matrix with columns \(e_{1},\ldots,e_{n},d_{1},\ldots,d_{t}\). The bound on \(|\mathcal{D}|\) follows from Theorem B.1.

For any \(t\)-sparse \(v\in\mathbb{R}^{n}\), pick the matrix \(D\in\mathcal{D}\) indexed by any \(T\in\binom{S}{t}\) with \(S\cap\operatorname{supp}(v)\subseteq T\). Let \(d_{1},\ldots,d_{t}\in\mathbb{R}^{n}\) be the last \(t\) columns of \(D\). Then there are coefficients \(b_{1},\ldots,b_{t}\) so that we can write \(v_{S}=\sum_{i=1}^{t}b_{i}d_{i}\). Since \(d_{i}^{\top}\overline{\Sigma}d_{i^{\prime}}=\mathbb{1}\left[i=i^{\prime}\right]\) for all \(i,i^{\prime}\in[t]\), we have \(v_{S}^{\top}\overline{\Sigma}v_{S}=\sum_{i=1}^{t}b_{i}^{2}\). Hence, \(\left\|b\right\|_{1}\leq\sqrt{t}\sqrt{v_{S}^{\top}\overline{\Sigma}v_{S}}\). But we can bound

\[\sqrt{v_{S}^{\top}\overline{\Sigma}v_{S}} =\left\|\overline{\Sigma}^{1/2}v_{S}\right\|_{2}\] \[\leq\left\|\overline{\Sigma}^{1/2}v\right\|_{2}+\left\|\overline {\Sigma}^{1/2}v_{S^{c}}\right\|_{2}\] (by triangle inequality)

**Algorithm 3**Alternative algorithm to solve sparse linear regression when covariate eigenspectrum has few outliers

**Procedure AugmentedDictionaryLasso**(\(\Sigma\), \((X_{i},y_{i})_{i=1}^{m}\), \(t\), \(d_{l}\), \(\delta\))

```
Data: Covariance matrix \(\Sigma:n\times n\), samples \((X_{i},y_{i})_{i=1}^{m}\), sparsity \(t\), small eigenvalue count \(d_{l}\), failure probability \(\delta\) Result: Estimate \(\hat{v}\) of unknown sparse regressor, satisfying Theorem C.5 \(\sum_{i=1}^{n}\lambda_{i}u_{i}u_{i}^{\top}\leftarrow\text{eigendecomposition of}\)\(\Sigma\)\(S\leftarrow\text{IterativePeeling}(\Sigma,d_{l},t)\)/* See Algorithm 1 */ \(\overline{\Sigma}\leftarrow\lambda_{d_{l}}^{-1}\sum_{i=1}^{m}\min(\lambda_{i}, \lambda_{d+1})u_{i}u_{i}^{\top}\)for\(T\in\binom{S}{[t]}\)do \(d_{1}^{(T)},\ldots,d_{t}^{(T)}\leftarrow\overline{\Sigma}\)-orthogonal basis for \(\operatorname{span}\{e_{i}:i\in T\}\)\(D(T)\leftarrow\begin{bmatrix}I_{n}&d_{1}^{(T)}&\ldots&d_{t}^{(T)}\end{bmatrix}\) Compute \[\hat{w}(T) \leftarrow\operatorname*{argmin}_{w\in\mathbb{R}^{n+t}}\bigg{[} \sum_{i=1}^{m/2}\left(\langle X_{i},D(T)w\rangle-y_{1:m/2}\right)^{2}\] \[+8\lambda_{n-d}\log(8n/\delta)\left\|w\right\|_{1}^{2}+2\sqrt{2 \lambda_{n-d}\log(8n/\delta)}\left\|y_{1:m/2}\right\|_{2}\left\|w\right\|_{1}\bigg{]}\] Select best hypothesis \[\hat{T}\leftarrow\operatorname*{argmin}_{T\in\binom{S}{[t]}}\sum_{i=m/2+1}^{m }\left(\langle X_{i},D(T)\hat{w}(T)\rangle-y_{i}\right)^{2}\] return\(D(\hat{T})\hat{w}(\hat{T})\)
```

**Algorithm 4**Alternative algorithm to solve sparse linear regression when covariate eigenspectrum has few outliers

**Theorem C.5**.: _Let \(n,t,d_{l},d_{h},m\in\mathbb{N}\) and let \(\Sigma:n\times n\) be a positive semi-definite matrix with eigenvalues \(\lambda_{1}\leq\cdots\leq\lambda_{n}\). Let \((X_{i},y_{i})_{i=1}^{m}\) be independent samples where \(X_{i}\sim N(0,\Sigma)\) and \(y_{i}=\langle X_{i},v^{*}\rangle+\xi_{i}\), for \(\xi_{i}\sim N(0,\sigma^{2})\) and a fixed \(t\)-sparse vector \(v^{*}\in\mathbb{R}^{n}\). Set \(k:=t(7t)2^{t^{2}+t}d_{l}^{t}\) and let \(\mathcal{D}\) be the family of matrices (of size at most \(k\)) guaranteed by Lemma C.4._

_Let \(\delta>0\). For every \(D\in\mathcal{D}\), define_

\[\hat{w}(D)\in\operatorname*{argmin}_{w\in\mathbb{R}^{n+t}}\left\|\mathbb{X}^{( 1)}Dw-y_{1:m/2}\right\|_{2}^{2}+8\lambda_{n-d}\log(8n/\delta)\left\|w\right\|_ {1}^{2}+2\sqrt{2\lambda_{n-d}\log(8n/\delta)}\left\|y_{1:m/2}\right\|_{2}\left\| w\right\|_{1} \tag{6}\]_where \(\mathbb{X}^{(1)}:(m/2)\times n\) is the matrix with rows \(X_{1},\ldots,X_{m/2}\), and define \(\hat{v}=\hat{Dw}(\hat{D})\) where_

\[\hat{D}\in\operatorname*{argmin}_{D\in\mathcal{D}}\left\|\mathbb{X}^{(2)}D\hat{w }(D)-y_{m/2+1:m}\right\|_{2}^{2}\]

_where \(\mathbb{X}^{(2)}:(m/2)\times n\) is the matrix with rows \(X_{m/2+1},\ldots,X_{m}\)._

_Let \(n_{\text{eff}}:=t^{2}\log(t)+t\log(d_{l})+d_{h}+\log(48/\delta)\) and let \(r_{\text{eff}}:=t(\lambda_{n-d_{h}}/\lambda_{d_{l}+1})\log(8n/\delta)\). There are absolute constants \(c,C>0\) so that the following holds. If \(m\geq Cn_{\text{eff}}\), then with probability at least \(1-3\delta\) it holds that_

\[\left\|\hat{v}-v^{*}\right\|_{\Sigma}^{2}\leq c\left(\frac{\sigma^{2}n_{\text{ eff}}}{m}+\left\|v^{*}\right\|_{\Sigma}^{2}\left(\frac{r_{\text{eff}}}{m}+\sqrt{ \frac{r_{\text{eff}}}{m}}\right)+\sigma\left\|v^{*}\right\|_{\Sigma}\sqrt{ \frac{r_{\text{eff}}}{m}}\right).\]

Let \(D^{*}\in\mathcal{D}\) and \(w^{*}\in\mathbb{R}^{n+t}\) be the matrix and vector guaranteed by Lemma C.4 for the \(t\)-sparse vector \(v^{*}\). Let \(\Gamma=(D^{*})^{\top}\Sigma D^{*}\) with eigenvalues \(\gamma_{1}\leq\cdots\leq\gamma_{n+t}\). We make the following claim:

**Claim C.6**.: _With probability at least \(1-\delta/4\) over \(G\sim N(0,\Gamma)\), it holds uniformly in \(w\in\mathbb{R}^{n+t}\) that_

\[\left\langle w-w^{*},G\right\rangle\leq\left\|w-w^{*}\right\|_{1}\sqrt{ \lambda_{n-d_{h}}\cdot 2\log(8n/\delta)}+\left\|w-w^{*}\right\|_{\Gamma}\sqrt{2(d_{h} +t)}.\]

Proof.: Since \(\Sigma\) is a principal submatrix of \(\Gamma\), we have \(\gamma_{n-d_{h}}\leq\lambda_{n-d_{h}}\) (by the Cauchy Interlacing Theorem). Suppose that \(\Gamma\) has eigendecomposition \(\Gamma=\sum_{i=1}^{n+t}\gamma_{i}g_{i}g_{i}^{\top}\), and define projection matrix \(P:(n+t)\times(n+t)\) by \(P:=\sum_{i=1}^{n-d_{h}}g_{i}g_{i}^{\top}\), so that \(\operatorname{rank}(P^{\perp})=d_{h}+t\) and \(\lambda_{\text{max}}(P\Gamma P)\leq\gamma_{n-d_{h}}\leq\lambda_{n-d_{h}}\). Then for any \(w\in\mathbb{R}^{n+t}\) and \(G\sim N(0,\Gamma)\), we can bound

\[\left\langleSince \(v^{*}=D^{*}w^{*}\) and \(\left\|w^{*}\right\|_{1}\leq 7t^{1/2}\lambda_{d_{l}+1}^{-1/2}\left\|v^{*} \right\|_{\Sigma}\) (the guarantees of Lemma C.4), it follows that

\[\left\|D^{*}\hat{w}(D^{*})-v^{*}\right\|_{\Sigma}^{2}\leq O\left(\frac{\sigma^ {2}(d_{h}+t)}{m}+\frac{(\sigma+\left\|v^{*}\right\|_{\Sigma})\left\|v^{*} \right\|_{\Sigma}\sqrt{r_{\text{eff}}}}{\sqrt{m}}+\frac{\left\|v^{*}\right\|_ {\Sigma}^{2}r_{\text{eff}}}{m}\right).\]

To complete the proof of the theorem, condition on any values of \((X_{i},y_{i})_{i=1}^{m/2}\) for which the above bound holds. By applying Lemma F.2 with covariance matrix \(\Sigma\), hypothesis set \(\mathcal{W}:=\{D\hat{w}(D):D\in\mathcal{D}\}\), and samples \((X_{i},y_{i})_{i=m/2+1}^{m}\) (which are independent of \(\mathcal{W}\)), since \(m/2\geq 32\log(2|\mathcal{D}|/\delta)\), we have with probability at least \(1-2\delta\) over the samples \((X_{i},y_{i})_{i=m/2+1}^{m}\) that

\[\left\|\hat{D}\hat{w}(\hat{D})-v^{*}\right\|_{\Sigma}^{2}\leq 6\min_{D\in \mathcal{D}}\left\|D\hat{w}(D)-v^{*}\right\|_{\Sigma}^{2}+\frac{32\sigma^{2} \log(2|\mathcal{D}|/\delta)}{m}.\]

Hence, with probability at least \(1-5\delta\) we have

\[\left\|\hat{D}\hat{w}(\hat{D})-v^{*}\right\|_{\Sigma}^{2}\leq O\left(\frac{ \sigma^{2}(d_{h}+t+\log(2|\mathcal{D}|/\delta))}{m}+\frac{(\sigma+\left\|v^{*} \right\|_{\Sigma})\left\|v^{*}\right\|_{\Sigma}\sqrt{r_{\text{eff}}}}{\sqrt{ m}}+\frac{\left\|v^{*}\right\|_{\Sigma}^{2}r_{\text{eff}}}{m}\right)\]

which proves the theorem. 

We can use the above theorem (together with the previously discussed boosting approach) to get the following result, which proves Theorem 1.2.

**Theorem C.7**.: _Let \(n,t,d_{l},d_{h},m,L\in\mathbb{N}\) and \(\sigma,\delta>0\). Let \(\Sigma:n\times n\) be a positive semi-definite matrix with eigenvalues \(\lambda_{1}\leq\cdots\leq\lambda_{n}\). Let \((X_{i},y_{i})_{i=1}^{m}\) be independent samples where \(X_{i}\sim N(0,\Sigma)\) and \(y_{i}=\langle X_{i},v^{*}\rangle+\xi_{i}\), for \(\xi_{i}\sim N(0,\sigma^{2})\) and a fixed \(t\)-sparse vector \(v^{*}\in\mathbb{R}^{n}\)._

_Then, given \(\Sigma\), \((X_{i},y_{i})_{i=1}^{m}\), \(t\), \(d_{l}\), and \(\delta\), there is an estimator \(\hat{v}\) with the following properties._

_Let \(n^{\prime}_{\text{eff}}:=t^{2}\log(t)+t\log(d_{l})+d_{h}+\log(48L/\delta)\) and let \(r^{\prime}_{\text{eff}}:=t(\lambda_{n-d_{h}}/\lambda_{d_{l}+1})\log(8nL/\delta)\). There are absolute constants \(c_{0},C_{0}>0\) such that the following holds. If \(m\geq C_{0}L(n^{\prime}_{\text{eff}}+r^{\prime}_{\text{eff}})\), then with probability at least \(1-\delta\), it holds that_

\[\left\|\hat{v}-v^{*}\right\|_{\Sigma}^{2}\leq c_{0}\frac{\sigma^{2}(n^{\prime }_{\text{eff}}+r^{\prime}_{\text{eff}})}{m/L}+2^{-L}\cdot\left\|v^{*}\right\|_ {\Sigma}^{2}.\]

_Moreover, \(\hat{v}\) is computable in time \((t+1)^{O(t^{2})}(d_{l}+1)^{t+1}\cdot\operatorname{poly}(n)\)._

Proof.: Identical to that of Theorem C.3, except using Theorem C.5 instead of Theorem C.2. 

## Appendix D Faster sparse linear regression for arbitrary \(\Sigma\)

In this section we prove Theorem 3.1. The approach is via feature adaptation: in Theorem D.5, we show that any covariance matrix \(\Sigma\) has a \((t,O(t^{3/2}\log n)\cdot\ell_{1}\)-representation of size \(O(n^{t-1/2})\) that is computable in time \(n^{t-\Omega(1/t)}\log^{O(t)}n\), using \(O(t\log n)\) samples from \(N(0,\Sigma)\). The algorithm for computing this representation is described in Algorithm 4. One of the key tools is the following result from computational geometry:

**Theorem D.1** ([30]).: _Let \(n,d,k\in\mathbb{N}\) and \(\delta>0\). Given points \(p_{1},\ldots,p_{n}\in\mathbb{R}^{d}\), query dimension \(k\), and failure probability \(\delta\), there an algorithm \(\operatorname{\mathrm{DS}}((p_{1},\ldots,p_{n}),k,\delta)\) with time complexity \(n^{k+1}(\log n)^{O(k)}\operatorname{poly}(d)\log(1/\delta)\), that constructs a data structure \(\mathcal{N}\) that answers queries of the following form. Given a \(k\)-dimensional subspace \(F\subseteq\mathbb{R}^{d}\), the output \(\mathcal{N}(F)\) is some \(i^{*}\in[n]\). With probability at least \(1-\delta\), the query time complexity is \(n^{1-1/(2k)}\operatorname{poly}(d)\log(1/\delta)\), and it holds that_

\[\min_{q\in F}\left\|p_{i^{*}}-q\right\|_{2}\leq O(\log n)\cdot\min_{i\in[n]} \min_{q\in F}\left\|p_{i}-q\right\|_{2}.\]

How do we use the above theorem to efficiently construct the \(\ell_{1}\)-representation? The intuition is as follows. Let \(\mathbb{X}\) be the \(m\times n\) matrix where each row is a sample from \(N(0,\Sigma)\). Then each column is a vector \(p_{i}\) representing a particular covariate. To find the \(\ell_{1}\)-representation, it essentially suffices

[MISSING_PAGE_FAIL:27]

With this notation, we want to construct a set \(\mathcal{D}\) of size \(O(n^{t-1/2})\), consisting of \(t\)-sparse vectors, such that

\[C_{\mathcal{D}}(x)\leq\operatorname{poly}(t,\log n)\cdot\left\|\sum x_{i}p_{i} \right\|_{2}\]

for all \(t\)-sparse \(x\in\mathbb{R}^{n}\).

The construction is quite simple: divide the set \(\{p_{1},\ldots,p_{n}\}\) into \(\sqrt{n}\) equal-sized groups. For each set \(T\) of \(t-1\) vectors and each of the \(\sqrt{n}\) groups, find the closest vector in the group to the subspace spanned by \(T\) (using Theorem D.1 to achieve sublinear time complexity). Then add the difference between the vector and its projection (onto the subspace) to the dictionary. Finally, for each set of \(t\) vectors where two of the vectors lie in the same group, add an orthonormal basis for those vectors to the dictionary. See the procedure RepresentVectors() in Algorithm 5 for pseudocode.

By construction, the dictionary clearly has size \(O(n^{t-1/2})\). At a high level, the reason it satisfies the representational property is the following. Consider some \(t\)-sparse combination, such as \(p_{1}+\cdots+p_{t}\). If \(-p_{t}\) is not very close to \(p_{1}+\cdots+p_{t-1}\), then we can bound \(C(p_{1}+\cdots+p_{t})\) by \(C(p_{1}+\cdots+p_{t-1})\) and \(C(p_{t})\), which are \(O(\sqrt{t}\left\|p_{1}+\cdots+p_{t-1}\right\|_{2})\) and \(O(\sqrt{t}\left\|p_{t}\right\|_{2})\) respectively, since the dictionary contains an orthonormal basis for both terms. The only case where these bounds are not good enough is when \(\left\|p_{1}+\cdots+p_{t}\right\|_{2}\) is much smaller than \(\left\|p_{1}+\cdots+p_{t-1}\right\|_{2}\) and \(\left\|p_{t}\right\|_{2}\). In this case, \(p_{t}\) is very close to \(\operatorname{span}\{p_{1},\ldots,p_{t-1}\}\). However, in the construction we found some (potentially different) \(p_{j}\) which is just as close to \(\operatorname{span}\{p_{1},\ldots,p_{t-1}\}\), and moreover is in the same group as \(p_{t}\). Letting \(q\) be the projection of \(p_{j}\) onto \(\operatorname{span}\{p_{1},\ldots,p_{t-1}\}\), we have the crucial fact that \(\left\|p_{j}-q\right\|_{2}\) is as small as \(\left\|p_{1}+\cdots+p_{t}\right\|_{2}\).

Now, bounding \(C(p_{1}+\cdots+p_{t})\) proceeds as follows. We can subtract some appropriate (bounded) multiple of \(p_{j}-q\) from \(p_{1}+\cdots+p_{t}\) to zero out at least one of the coefficients. This residual then is a \(t\)-sparse combination of \(\{p_{1},\ldots,p_{t},p_{j}\}\) where two of the vectors \(\{p_{t},p_{j}\}\) are in the same group; thus it has small cost with respect to \(\mathcal{D}\). Moreover, \(p_{j}-q\) is contained in \(\mathcal{D}\) and thus has small cost (specifically, not much more than \(\left\|p_{j}-q\right\|_{2}\), which crucially is not much more than \(\left\|p_{1}+\cdots+p_{t}\right\|_{2}\)). It follows that \(p_{1}+\cdots+p_{t}\) has small cost.

Formalizing this argument, we start by proving one of the facts that we freely used above: that the cost function \(C\) satisfies the triangle inequality.

**Fact D.3**.: _For any \(\mathcal{D}\subseteq\mathbb{R}^{n}\) and \(x,y\in\mathbb{R}^{n}\), it holds that \(C(x+y)\leq C(x)+C(y)\)._

Proof.: For any \(\alpha,\beta\in\mathbb{R}^{\mathcal{D}}\) with \(\sum_{d}\alpha_{d}d=x\) and \(\sum_{d}\beta_{d}d=y\), the vector \(\alpha+\beta\) satisfies \(\sum_{d}(\alpha+\beta)_{d}d=x+y\). Applying the triangle inequality to \(\sum_{d}|(\alpha+\beta)_{d}|\cdot\left\|\sum_{i}d_{i}p_{i}\right\|_{2}\) completes the proof. 

We now prove the key lemma, formalizing the above intuition.

**Lemma D.4**.: _Let \(n,m,t\in\mathbb{N}\), with \(t\geq 2\), and \(\delta>0\). Fix \(p_{1},\ldots,p_{n}\in\mathbb{R}^{m}\) with \(\left\|p_{i}\right\|_{2}=1\) for all \(i\in[n]\). Let \(\mathcal{D}\) be the output of RepresentVectors(\(\{p_{1},\ldots,p_{n}\},t,\delta\)). Then \(|\mathcal{D}|=O(n^{t-1/2})\), and every element of \(\mathcal{D}\) is \(t\)-sparse. Also, with probability at least \(1-\delta\), the following guarantees hold. The time complexity of computing \(\mathcal{D}\) is \(O(n^{t-\Omega(1/t)}(\log n)^{O(t)}m^{O(1)}\log(1/\delta))\). Moreover, for every \(t\)-sparse \(x\in\mathbb{R}^{n}\) it holds that_

\[C_{\mathcal{D}}(x)\leq O(t^{3/2}\log n)\cdot\left\|\sum x_{i}p_{i}\right\|_{2}.\]

Proof.: Since the algorithm RepresentVectors() makes less than \(n^{t}\) queries to the data structures \(\mathcal{N}^{j}\), and each query has failure probability at most \(\delta^{\prime}=\delta/n^{t}\), the probability that any query fails is at most \(1-\delta\). We henceforth assume that all queries succeed, i.e. satisfy the correctness guarantee and time complexity bound stated in Theorem D.1.

**Time complexity.** We start by analyzing the time complexity of RepresentVectors(\(\{p_{1},\ldots,p_{n}\},t,\delta\)). For any fixed \(j\in[\sqrt{n}]\), the construction time of \(\mathcal{N}^{j}\) (with \(|I_{j}|=O(\sqrt{n})\) points in \(\mathbb{R}^{m}\), query dimension \(t-1\), and failure probability \(\delta/n^{t}\)) is \(O(n^{t/2}(\log n)^{O(t)}m^{O(1)}\log(1/\delta))\). We make \(\binom{n}{t-1}+|I_{j}|^{2}\binom{n}{t-2}=O(n^{t-1})\) queries to \(\mathcal{N}^{j}\), each with time complexity \(n^{1/2-1/(4(t-1))}m^{O(1)}\log(1/\delta)\). Each projection step and each orthonormalization step has time complexity \(\operatorname{poly}(t,m)\). Thus, since \(t\geq 2\), the time complexity for any fixed \(j\) is bounded by \(n^{t-1/2-1/(8t)}(\log n)^{O(t)}m^{O(1)}\log(1/\delta)\). Summing over \(j\), the overall time complexity to compute \(\mathcal{D}\) is at most \(n^{t-1/(8t)}(\log n)^{O(t)}m^{O(1)}\log(1/\delta)\) as claimed.

Correctness.The bound on \(|\mathcal{D}|\) and the fact that all elements of \(\mathcal{D}\) are \(t\)-sparse are immediate from the algorithm definition. It remains to bound \(C_{\mathcal{D}}(x)\) for \(t\)-sparse vectors \(x\). First, note that for any \((t-1)\)-sparse \(y\in\mathbb{R}^{n}\), because of step (4), the dictionary contains vectors \(\gamma^{1},\ldots,\gamma^{t-1}\) that span \(\mathrm{supp}(y)\) and satisfy \(\langle\sum_{i=1}^{n}\gamma_{i}^{k}p_{i},\sum_{i=1}^{n}\gamma_{i}^{\ell}p_{i} \rangle=0\) for all \(k\neq\ell\). Thus, letting \(\alpha_{1},\ldots,\alpha_{t-1}\in\mathbb{R}\) be such that \(y=\alpha_{1}\gamma^{1}+\cdots+\alpha_{t-1}\gamma^{t-1}\), we get

\[C_{\mathcal{D}}(y)\leq\sum_{j=1}^{t-1}|\alpha_{j}|\cdot\left\|\sum_{i=1}^{n} \gamma_{i}^{j}p_{i}\right\|_{2}\leq\sqrt{t}\sqrt{\sum_{j=1}^{t-1}\alpha_{j}^{2 }\left\|\sum_{i=1}^{n}\gamma_{i}^{j}p_{i}\right\|_{2}^{2}}=\sqrt{t}\left\|\sum _{i=1}^{n}y_{i}p_{i}\right\|_{2}. \tag{7}\]

Now fix any nonzero \(t\)-sparse \(x\in\mathbb{R}^{n}\). Fix any \(a\in\arg\max_{i\in[n]}|x_{i}|\), and let \(j\in[\sqrt{n}]\) be such that \(a\in I_{j}\). Let \(T=\mathrm{supp}(x)\setminus\{a\}\). Let \(q:=\mathrm{Proj}_{\mathrm{span}\{p_{i}:i\in T\}}\,p_{h(T,j)}\). Then by the correctness guarantee of \(\mathcal{N}^{j}\) on query \(\mathrm{span}\{p_{i}:i\in T\}\),

\[\left\|p_{h(T,j)}-q\right\|_{2}\leq O(\log n)\cdot\left\|p_{a}+\sum_{i\neq a} \frac{x_{i}}{x_{a}}p_{i}\right\|_{2}=O(\log n)\cdot\frac{\left\|\sum_{i}x_{i}p _{i}\right\|_{2}}{|x_{a}|}. \tag{8}\]

Case I.Suppose that \(\left\|p_{h(T,j)}-q\right\|_{2}\geq 1/2\). Then by (8), we have \(|x_{a}|\leq O(\log n)\cdot\left\|\sum_{i}x_{i}p_{i}\right\|_{2}\). Thus, by the triangle inequality,

\[\left\|\sum_{i\neq a}x_{i}p_{i}\right\|_{2}\leq|x_{a}|+\left\|\sum_{i}x_{i}p_ {i}\right\|_{2}\leq O(\log n)\cdot\left\|\sum_{i}x_{i}p_{i}\right\|_{2}.\]

It follows from Fact D.3 and (7) that

\[C_{\mathcal{D}}(x)\leq C_{\mathcal{D}}(x_{a}e_{a})+C_{\mathcal{D}}(x-x_{a}e_{ a})\leq\sqrt{t}|x_{a}|+\sqrt{t}\left\|\sum_{i\neq a}x_{i}p_{i}\right\|_{2} \leq O(\sqrt{t}\log n)\cdot\left\|\sum_{i}x_{i}p_{i}\right\|_{2}\]

as desired.

Case II.It remains to consider the case that \(\left\|p_{h(T,j)}-q\right\|_{2}\leq 1/2\). In this case we have \(\left\|q\right\|_{2}\geq\left\|p_{h(T,j)}\right\|_{2}-1/2\geq 1/2\). By step (3) of the algorithm, the dictionary contains some vector \(\gamma-e_{h(T,j)}\) such that \(\mathrm{supp}(\gamma)\subseteq T\) and \(q=\sum_{i\in T}\gamma_{i}p_{i}\). Fix any \(b\in\arg\max_{i}|\gamma_{i}|\). Since \(q=\sum\gamma_{i}p_{i}\) we get \(|\gamma_{b}|\geq\frac{\left\|q\right\|_{2}}{t}\geq 1/(2t)\). Now, by Fact D.3,

\[C_{\mathcal{D}}(x)\leq C_{\mathcal{D}}\left(-\frac{x_{b}}{\gamma_{b}}(e_{h(T,j )}-\gamma)\right)+C_{\mathcal{D}}\left(x+\frac{x_{b}}{\gamma_{b}}(e_{h(T,j)}- \gamma)\right).\]

By construction, \(e_{h(T,j)}-\gamma\) is an element of the dictionary, so we can bound the first term as

\[C_{\mathcal{D}}\left(-\frac{x_{b}}{\gamma_{b}}(e_{h(T,j)}-\gamma)\right) \leq\frac{|x_{b}|}{|\gamma_{b}|}\left\|\sum_{i=1}^{n}(e_{h(T,j)}- \gamma)_{i}p_{i}\right\|_{2}\] \[=\frac{|x_{b}|}{|\gamma_{b}|}\left\|p_{h(T,j)}-q\right\|_{2}\] \[\leq 2t|x_{a}|\left\|p_{h(T,j)}-q\right\|_{2}\] \[\leq O(t\log n)\left\|\sum_{i=1}^{n}x_{i}p_{i}\right\|_{2}\]

where the equality uses that \(q=\sum_{i=1}^{n}\gamma_{i}p_{i}\), the second inequality uses that \(|x_{b}|\leq|x_{a}|\) and \(|\gamma_{b}|\geq 1/(2t)\), and the final inequality uses (8).

Finally, observe that

\[z:=x+\frac{x_{b}}{\gamma_{b}}(e_{h(T,j)}-\gamma)=x_{a}e_{a}+\frac{x_{b}}{\gamma_{b} }e_{h(T,j)}+\sum_{i\in T\setminus\{a,b\}}\left(x_{i}-\frac{x_{b}\gamma_{i}}{ \gamma_{b}}\right)e_{i}\]

since the coefficients on \(e_{b}\) cancel out. Thus, \(z\) is a linear combination of two elements of \(\{p_{i}:i\in I_{j}\}\) together with \(t-2\) elements of \(\{p_{i}:i\in[n]\}\). Because of step (4) of the algorithm, the dictionary contains vectors \(\gamma^{1},\ldots,\gamma^{t}\) that span \(\operatorname{supp}(z)\) and satisfy \(\langle\sum_{i=1}^{n}\gamma_{i}^{k}p_{i},\sum_{i=1}^{n}\gamma_{i}^{\ell}p_{i} \rangle=0\) for all \(k\neq\ell\). The same argument as for (7) gives that

\[C_{\mathcal{D}}\left(x+\frac{x_{b}}{\gamma_{b}}(e_{h(T,j)}- \gamma)\right) \leq\sqrt{t}\left\|\sum_{i=1}^{n}x_{i}p_{i}+\frac{x_{b}}{\gamma_ {b}}(p_{h(T,j)}-q)\right\|_{2}\] \[\leq\sqrt{t}\left\|\sum_{i=1}^{n}x_{i}p_{i}\right\|_{2}+O(\sqrt{t }\log n)\frac{|x_{b}|}{|\gamma_{b}||x_{a}|}\left\|\sum_{i=1}^{n}x_{i}p_{i} \right\|_{2}\] \[\leq O(t^{3/2}\log n)\left\|\sum_{i=1}^{n}x_{i}p_{i}\right\|_{2}\]

where the second inequality uses the triangle inequality and (8), and the final inequality uses that \(|x_{b}|\leq|x_{a}|\) and \(|\gamma_{b}|\geq 1/(2t)\). Putting everything together, we conclude that

\[C_{\mathcal{D}}(x)\leq O(t^{3/2}\log n)\left\|\sum_{i=1}^{n}x_{i}p_{i}\right\| _{2}\]

as claimed. 

We now show that RepresentVectors() can be applied to the columns of the sample matrix to obtain a \(\ell_{1}\)-representation for \(\Sigma\) (procedure ComputeLiRepresentation() in Algorithm 5). Up to an appropriate rescaling of the covariates, Lemma D.4 immediately implies that \(\mathcal{D}\) gives a \(\ell_{1}\)-representation for the empirical covariance \(\hat{\Sigma}\). The main result then follows from concentration of \(\hat{\Sigma}\) and sparsity of the elements of the dictionary.

**Theorem D.5**.: _Let \(n,m,t\in\mathbb{N}\) and let \(\Sigma:n\times n\) be a positive-definite matrix. Suppose \(m\geq Ct\log n\) for a sufficiently large constant \(C\). Let \(X_{1},\ldots,X_{m}\sim N(0,\Sigma)\) be independent samples, and let \(\mathcal{D}\) be the output of ComputeLiRepresentation(\(\{X_{1},\ldots,X_{m}\},t\)). Then \(|\mathcal{D}|\leq O(n^{t-1/2})\), and every element of \(\mathcal{D}\) is \(t\)-sparse. Also, with probability at least \(1-e^{-\Omega(m)}\), the time complexity of the algorithm is \(O(n^{t-\Omega(1/t)}(\log n)^{O(t)}m^{O(1)})\), and \(\mathcal{D}\) is a \((t,C_{\text{ITEP}}t^{3/2}\log n)\)-\(\ell_{1}\)-representation for \(\Sigma\), for some universal constant \(C_{\text{ITEP}}\)._

Proof.: Let \(\hat{\Sigma}=\frac{1}{m}\mathbb{X}^{\top}\mathbb{X}\). Let \(\tilde{\mathcal{D}}\) denote the intermediary dictionary constructed by the algorithm using RepresentVectors(). With probability at least \(1-e^{-m}\), the successful event of Lemma D.4 holds. By standard concentration bounds (see e.g. Exercise 4.7.3 in [41]), it holds that \(\frac{1}{2}\left\|x\right\|_{\Sigma}\leq\left\|x\right\|_{\hat{\Sigma}}\leq 2 \left\|x\right\|_{\Sigma}\) for all \(t\)-sparse \(x\in\mathbb{R}^{n}\), with probability at least \(1-e^{-\Omega(m)}\). Henceforth assume that both of these events hold.

Time complexity.The time complexity of the algorithm is dominated by the call to RepresentVectors(). By the guarantee of Lemma D.4, this takes time \(O(n^{t-\Omega(1/t)}(\log n)^{O(t)}m^{O(1)})\).

Correctness.The bounds on \(|\mathcal{D}|\) and sparsity of elements of \(\mathcal{D}\) follow from identical bounds for \(\tilde{\mathcal{D}}\) (see Lemma D.4), and the fact that every element of \(\mathcal{D}\) is obtained by rescaling the coordinates of some element of \(\tilde{\mathcal{D}}\). It remains to show that \(\mathcal{D}\) is a \((t,O(t^{3/2}\log n))\)-\(\ell_{1}\) representation for \(\Sigma\).

Fix any \(t\)-sparse \(v\in\mathbb{R}^{n}\), and define \(\tilde{v}=\hat{D}v\). By the guarantee of Lemma D.4, since \(\tilde{v}\) is also \(t\)-sparse, there is some \(\alpha\in\mathbb{R}^{\tilde{\mathcal{D}}}\) such that \(\tilde{v}=\sum_{\tilde{d}\in\tilde{\mathcal{D}}}\alpha_{\tilde{d}}\tilde{d}\) and

\[\sum_{\tilde{d}\in\tilde{\mathcal{D}}}|\alpha_{\tilde{d}}|\cdot\left\|\sum_{i= 1}^{n}\tilde{d}_{i}\frac{q_{i}}{\left\|q_{i}\right\|_{2}}\right\|_{2}\leq O(t ^{3/2}\log n)\cdot\left\|\sum_{i=1}^{n}\tilde{v}_{i}\frac{q_{i}}{\left\|q_{i} \right\|_{2}}\right\|_{2}.\]But note that \(\tilde{v}_{i}=\hat{D}_{ii}v_{i}=\left\|q_{i}\right\|_{2}v_{i}\) for all \(i\). Similarly, every \(\tilde{d}\in\tilde{\mathcal{D}}\) corresponds to some \(d\in\mathcal{D}\) with \(\tilde{d}_{i}=\left\|q_{i}\right\|_{2}d_{i}\) for all \(i\). Thus, reindexing \(\alpha\) according to \(\mathcal{D}\) in the natural way, we have that \(v=\sum_{d\in\mathcal{D}}\alpha_{d}d\) and

\[\sum_{d\in\mathcal{D}}\left|\alpha_{d}\right|\cdot\left\|\sum_{i=1}^{n}d_{i}q_{ i}\right\|_{2}\leq O(t^{3/2}\log n)\cdot\left\|\sum_{i=1}^{n}v_{i}q_{i}\right\|_{2}.\]

But now let \(\hat{\Sigma}=\frac{1}{m}\mathbb{X}^{\top}\mathbb{X}\). For any \(i,j\in[n]\) we have \(\langle q_{i},q_{j}\rangle=m\hat{\Sigma}_{ii}\). Hence,

\[\left\|\sum_{i=1}^{n}v_{i}q_{i}\right\|_{2}^{2}=\sum_{i,j\in[n]}v_{i}v_{j}\hat {\Sigma}_{ij}=v^{\top}\hat{\Sigma}v\]

and similarly for \(\left\|\sum_{i=1}^{n}d_{i}q_{i}\right\|_{2}^{2}\). Thus, we get

\[\sum_{d\in\mathcal{D}}\left|\alpha_{d}\right|\cdot\left\|d\right\|_{\hat{\Sigma }}\leq O(t^{3/2}\log n)\cdot\left\|v\right\|_{\hat{\Sigma}}.\]

But as shown above, we know that \(\frac{1}{2}\left\|x\right\|_{\Sigma}\leq\left\|x\right\|_{\hat{\Sigma}}\leq 2 \left\|x\right\|_{\Sigma}\) for all \(t\)-sparse \(x\in\mathbb{R}^{n}\). Since \(v\) and all \(d\in\mathcal{D}\) are \(t\)-sparse, we conclude that

\[\sum_{d\in\mathcal{D}}\left|\alpha_{d}\right|\cdot\left\|d\right\|_{\hat{\Sigma }}\leq O(t^{3/2}\log n)\cdot\left\|v\right\|_{\Sigma}\]

as desired. 

We finally restate and prove Theorem 3.1, as a corollary of Theorem D.5 and the well-known fact that standard "slow rate" guarantees for Lasso (i.e. based on the \(\ell_{1}\) norm of the regressor) can be achieved in near-linear time (Theorem A.8). The pseudocode for the main algorithm is given in Algorithm 5.

```
1Procedure SparseLinearRegression(\(\langle X_{i},y_{i}\rangle_{i=1}^{m}\), \(t\), \(B\), \(\sigma^{2}\))
2\(\mathcal{D}\leftarrow\)ComputeLiRepresentation(\(\{X_{1},\ldots,X_{100t\log n}\},t\))
3for\(j=m/2+1,\ldots,m\)do
4for\(d\in\mathcal{D}\)do
5\(\tilde{X}_{j,d}\leftarrow\left\langle X_{j},d/\sqrt{(2/m)\sum_{i=1}^{m/2} \langle X_{i},d\rangle^{2}}\right\rangle\) /* See Theorem A.7 for definition of MirrorDescentLasso(), and Theorem D.5 for definition of \(C_{\mathsf{ITrep}}\) */
6\(\hat{\beta}\leftarrow\)MirrorDescentLasso(\(\langle\tilde{X}_{i},y_{i}\rangle_{i=m/2+1}^{m}\), \(2C_{\mathsf{ITrep}}t^{3/2}B\log(n)\), \(m/2\), \(\sigma^{2}\))
7\(\hat{w}\leftarrow\sum_{d\in\mathcal{D}}\hat{\beta}_{d}d/\sqrt{(2/m)\sum_{i=1} ^{m/2}\langle X_{i},d\rangle^{2}}\) return\(\hat{w}\)
```

**Algorithm 5**Sparse linear regression for arbitrary \(\Sigma\)

**Corollary D.6**.: _Let \(n,m,t,B\in\mathbb{N}\) and \(\sigma>0\), and let \(\Sigma:n\times n\) be a positive-definite matrix. Let \(w^{*}\in\mathbb{R}^{n}\) be \(t\)-sparse with \(\left\|w^{*}\right\|_{\Sigma}\leq B\). Suppose \(m\geq Ct\log n\) for a sufficiently large constant \(C\). Let \((X_{i},y_{i})_{i=1}^{m}\) be independent samples where \(X_{i}\sim N(0,\Sigma)\) and \(y_{i}=\langle X_{i},w^{*}\rangle+N(0,\sigma^{2})\). Then there is an \(O(m^{2}n^{t-1/2}+n^{t-1(1/t)}\log^{O(t)}n)\)-time algorithm (Algorithm 5) that, given \((X_{i},y_{i})_{i=1}^{m}\), \(t\), \(B\), \(\sigma^{2}\), produces an estimate \(\hat{w}\in\mathbb{R}^{n}\) satisfying, with probability \(1-o(1)\),_

\[\left\|\hat{w}-w^{*}\right\|_{\Sigma}^{2}\leq\tilde{O}\left(\frac{\sigma^{2}}{ \sqrt{m}}+\frac{\sigma Bt^{3/2}}{\sqrt{m}}+\frac{B^{2}t^{3}}{m}\right).\]

Proof.: By Theorem D.5 it holds with probability \(1-n^{-100t}\) that \(\mathcal{D}\) is a \((t,C_{\mathsf{ITrep}}t^{3/2}\log n)\)-\(\ell_{1}\)-representation for \(\Sigma\). Also, by standard concentration bounds (e.g. Exercise 4.7.3 in [41]), we have \(\frac{1}{2}\left\|x\right\|_{\Sigma}\leq\left\|x\right\|_{\hat{\Sigma}}\leq 2\left\|x \right\|_{\Sigma}\) for all \(t\)-sparse \(x\in\mathbb{R}^{n}\) (where \(\hat{\Sigma}=\frac{2}{m}\sum_{i=1}^{m/2}X_{i}X_{i}^{\top}\)) with probability at least \(1-\exp(-\Omega(m))\). Suppose that both of these events occur.

For each of the remaining \(m/2\) samples \(X_{j}\), compute \(\tilde{X}_{j}\in\mathbb{R}^{\mathcal{D}}\) where the entry \(\tilde{X}_{j,d}\) corresponding to \(d\in\mathcal{D}\) is \(\langle X_{j},d/\left\|d\right\|_{\hat{\Sigma}}\rangle\) (where \(\hat{\Sigma}=\frac{2}{m}\sum_{i=1}^{m/2}X_{i}X_{i}^{\top}\) is not explicitly computed; since \(d\) is sparse, both \(\langle X_{j},d\rangle\) and \(\left\|d\right\|_{\hat{\Sigma}}\) can be computed in \(\mathrm{poly}(t,m)\) time). Let \(N(0,\Gamma)\) denote the distribution of each \(\tilde{X}_{j}\). For each \(d\in\mathcal{D}\), since \(d\) is \(t\)-sparse, we have that \(\mathbb{E}_{x\sim N(0,\Sigma)}(x,d/\left\|d\right\|_{\hat{\Sigma}})^{2}=\left\| d\right\|_{\hat{\Sigma}}^{2}/\left\|d\right\|_{\hat{\Sigma}}^{2}\leq 4\). Thus, \(\Gamma_{dd}\leq 4\) for all \(d\).

Moreover, since \(w^{*}\) is \(t\)-sparse, there is some \(\alpha\in\mathbb{R}^{\mathcal{D}}\) with \(w^{*}=\sum_{d}\alpha_{d}d\) and \(\sum_{d}\left|\alpha_{d}\right|\left\|d\right\|_{\Sigma}\leq C_{\mathsf{1freq} }t^{3/2}\log(n)\cdot\left\|w^{*}\right\|_{\Sigma}\). Define \(\beta\in\mathbb{R}^{d}\) by \(\beta_{d}=\alpha_{d}\left\|d\right\|_{\hat{\Sigma}}\). Then \(w^{*}=\sum_{d}\beta_{d}d/\left\|d\right\|_{\hat{\Sigma}}\) and

\[\sum_{d}\left|\beta_{d}\right|\leq 2\cdot\sum_{d}\left|\alpha_{d}\right| \left\|d\right\|_{\Sigma}\leq 2C_{\mathsf{1freq}}t^{3/2}\log(n)\cdot\left\|w^{*} \right\|_{\Sigma}.\]

But now for any of the remaining \(m/2\) samples, we have that

\[\langle\tilde{X}_{j},\beta\rangle=\sum_{d}\langle X_{j},d/\left\|d\right\|_{ \hat{\Sigma}}\rangle\alpha_{d}\left\|d\right\|_{\hat{\Sigma}}=\langle X_{j}, \sum_{d}\alpha_{d}d\rangle=\langle X_{j},w^{*}\rangle,\]

and thus \(y-\langle\tilde{X}_{j},\beta\rangle\sim N(0,\sigma^{2})\). So we can apply Theorem A.8 to samples \((\tilde{X}_{j},y_{j})_{j=m/2+1}^{m}\) to compute an estimator \(\hat{\beta}\) satisfying

\[\left\|\hat{\beta}-\beta\right\|_{\Gamma}^{2}\leq\tilde{O}\left(\frac{\sigma^ {2}}{m}+\frac{\sigma Bt^{3/2}}{\sqrt{m}}+\frac{B^{2}t^{3}}{m}\right)\]

using that \(\left\|\beta\right\|_{1}\leq 2C_{\mathsf{1freq}}t^{3/2}\log(n)\cdot\left\|w^{*} \right\|_{\Sigma}\leq 2C_{\mathsf{1freq}}t^{3/2}B\log(n)\), and using the bound \(\max_{d}\Gamma_{dd}\leq 4\). The time complexity of this step is \(\tilde{O}(\left|\mathcal{D}\middle|m^{2}\right)=\tilde{O}(m^{2}n^{t-1/2})\). Finally, compute \(\hat{w}:=\sum_{d}\hat{\beta}_{d}d/\left\|d\right\|_{\hat{\Sigma}}\). We have that \(\left\|\hat{w}-w^{*}\right\|_{\Sigma}=\left\|\hat{\beta}-\beta\right\|_{\Gamma}\), which completes the proof. 

## Appendix E Fixed-parameter tractability in \(\kappa\) and \(t\)

In this section we prove Theorem 3.2, which shows we can achieve upper bounds on \(\mathcal{N}_{t,\alpha}(\Sigma)\) for \(\alpha\) independent of \(\kappa\) and \(n\), if we are willing to incur dependence on \(\kappa\) in the resulting bound. In fact, we actually prove an upper bound on the packing number \(\mathcal{P}_{t,\alpha}(\Sigma)\).

To achieve this, the first key idea is to consider the dual certificates for a packing. Suppose that \(v_{1},\ldots,v_{N}\) are unit vectors (in the \(\Sigma\)-norm) with \(\left|\langle v_{i},v_{j}\rangle_{\Sigma}\right|\leq\alpha\) for all \(i\neq j\). Then \(\left|\langle v_{i},\Sigma v_{i}\rangle\right|\geq\alpha^{-1}\max_{j\neq i} \left|\langle v_{j},\Sigma v_{i}\rangle\right|\), so \(\Sigma v_{i}\) certifies that any linear combination \(v_{i}=\sum_{j\neq i}x_{j}v_{j}\) must have the property that \(\left\|x\right\|_{1}\geq\alpha^{-1}\). Thus, to show that there cannot be a large packing of sparse vectors in the \(\Sigma\)-norm, it would suffice to prove that any large set of sparse vectors must have one vector that can be written as a linear combination of the remaining vectors, where the coefficient vector has small \(\ell_{1}\) norm. In fact, this would give an upper bound on \(\mathcal{N}_{t,\alpha}(\Sigma)\) for all \(\Sigma\).

We do not know if such a statement is true. However, we can prove an _approximate_ analogue. The following lemma shows that under a condition number bound on \(\Sigma\), the dual certificate argument can be generalized to require only a weaker property: that any large set of sparse vectors must have one vector that can be _approximately_ written as a linear combination of the remaining vectors, with low \(\ell_{1}\) cost. The approximation error determines how small the condition number must be:

**Lemma E.1**.: _Let \(n,N,t,T\in\mathbb{N}\) and let \(\delta>0\). Suppose that for all \(t\)-sparse vectors \(v_{1},\ldots,v_{N}\in\mathbb{R}^{n}\), there exists some \(i\in[N]\) and \(x\in\mathbb{R}^{N}\) such that \(\left\|x\right\|_{1}\leq T\) and_

\[\left\|v_{i}-\sum_{j\neq i}x_{j}v_{j}\right\|_{2}\leq\delta\cdot\max_{j\in[N]} \left\|v_{j}\right\|_{2}.\]

_Then for every positive-definite matrix \(\Sigma:n\times n\) with \(\kappa(\Sigma)<1/(4\delta^{2})\) it holds that \(\mathcal{P}_{t,1/(3T)}(\Sigma)\leq N\log_{2}\kappa(\Sigma)\)._

[MISSING_PAGE_EMPTY:33]

Proof.: For each \(J\subseteq[n]\), define \(\mathcal{S}^{J}:=\{i\in[N]:v_{ij}=1\quad\forall j\in J\}\). Since all \(v_{i}\) are nonzero, there is some \(j^{*}\in[n]\) with \(|\mathcal{S}^{\{j^{*}\}}|\geq N/n\). We iteratively construct a set \(J\subseteq[n]\) as follows. Initially, set \(J=\{j^{*}\}\). While there exists some \(a\in[n]\setminus J\) such that \(|\mathcal{S}^{J\cup\{a\}}|>(sn/N)^{1/t}|\mathcal{S}^{J}|\), update \(J\) to \(J\cup\{a\}\) (if there are multiple such \(a\), pick any one of them arbitrarily). At termination of this process, we have \(|\mathcal{S}^{J}|>0\). Since every \(v_{i}\) is \(t\)-sparse, it must be that \(|J|\leq t\). Thus, \(|\mathcal{S}^{J}|\geq(N/n)\cdot(sn/N)^{(t-1)/t}\geq s\). Set \(S:=\mathcal{S}^{J}\) and \(u:=\mathbb{1}_{J}\in\{0,1\}^{n}\). By definition of \(\mathcal{S}^{J}\), we have that \(u\preceq v_{i}\) for all \(i\in S\).

For any \(j\in J\), we have \(u_{j}=1=\frac{1}{|S|}\sum_{i\in S}v_{ij}\). For any \(j\not\in J\), we have \(u_{j}=0\) and

\[\left|\frac{1}{|S|}\sum_{i\in S}v_{ij}\right|=\frac{|\{i\in S:v_{ij}=1\}|}{|S|} =\frac{|\mathcal{S}^{J\cup\{j\}}|}{|\mathcal{S}^{J}|}\leq(sn/N)^{1/t}\]

by construction of \(J\). Thus,

\[\left\|u-\frac{1}{|S|}\sum_{i\in S}v_{i}\right\|_{\infty}\leq(sn/N)^{1/t}.\]

Additionally,

\[\left\|u-\frac{1}{|S|}\sum_{i\in S}v_{i}\right\|_{1}\leq\left\|\frac{1}{|S|} \sum_{i\in S}v_{i}\right\|_{1}\leq\frac{1}{|S|}\sum_{i\in S}\left\|v_{i}\right\| _{1}\leq t.\]

By the inequality \(\left\|x\right\|_{2}^{2}\leq\left\|x\right\|_{1}\left\|x\right\|_{\infty}\), we conclude that

\[\left\|u-\frac{1}{|S|}\sum_{i\in S}v_{i}\right\|_{2}\leq\sqrt{t(sn/N)^{1/t}}\]

as claimed. 

We now use Lemma E.2 to show that if \(N\) is sufficiently large, then at least one of the vectors \(v_{i}\) can be efficiently approximated by the rest. The proof is by induction on \(t\). As a first attempt, one might use Lemma E.2 to find some \(u\in\{0,1\}^{n}\) and some large set \(S\subseteq[N]\) such that \(u\preceq v_{i}\) for all \(i\in S\), and the average of the \(v_{i}\)'s approximates \(u\). Then, restrict to the vectors in \(S\), and induct on the \((t-1)\)-sparse residual vectors \(\{v_{i}-u:i\in S\}\). If one of the \(v_{i}-u\)'s can be efficiently approximated by the other residuals, then since \(u\) can also be efficiently approximated, we can derive an efficient approximation of \(v_{i}\) by the remaining \(v_{j}\)'s.

This doesn't quite work, since at each step of the induction the set of vectors will become smaller by a factor of roughly \(n\). However, instead of throwing away the vectors outside \(S=:S^{(1)}\) we can iteratively re-apply Lemma E.2 to get disjoint sets \(S^{(1)},S^{(2)},\ldots,S^{(m)}\), where each \(S^{(a)}\) has the same property as \(S\) (for some potentially different vector \(u^{(a)}\)). We can then induct on the residual vectors \(\cup_{a}\{v_{i}-u^{(a)}:i\in S^{(a)}\}\). This suffices to efficiently approximate some \(v_{i}\). Since we throw away fewer vectors at each step of the induction, we do not need the initial number of vectors \(N\) to be as large.

We formalize the above ideas in the following theorem.

**Theorem E.3**.: _Let \(n,N,t\in\mathbb{N}\) and let \(v_{1},\ldots,v_{N}\in\{0,1\}^{n}\) be \(t\)-sparse binary vectors. Then there is some \(i\in[N]\) and \(x\in\mathbb{R}^{N}\) such that \(\left\|x\right\|_{1}\leq 3^{t}\) and_

\[\left\|v_{i}-\sum_{j\neq i}x_{j}v_{j}\right\|_{2}\leq 4^{t}\sqrt{9t(tn/N)^{1/t}}.\]

Proof.: We induct on \(t\), observing that the case \(t=0\) is immediate. Fix \(t>0\) and \(t\)-sparse vectors \(\{v_{1},\ldots,v_{N}\}\in\{0,1\}^{n}\), and suppose that the theorem statement holds for \(t-1\). If any \(v_{i}\) is identically zero, then the claim is trivially true with \(x=0\). If \(N\leq t3^{t+1}n\) then the RHS of the desired norm bound exceeds \(4^{t}\sqrt{t}\), so the claim is trivially true with \(x=0\) and any \(i\in[N]\). Thus, we may assume that all \(v_{i}\) are nonzero, and \(N\geq t3^{t+1}n\). Applying the previous lemma with 

[MISSING_PAGE_EMPTY:35]

\[=\left\|u^{(a)}-\frac{1}{|S^{(a)}|}\sum_{r\in S^{(a)}}v_{r}\right\|_{2}+ \left\|v_{i}^{\prime}-\sum_{r\in[N]}x_{r}^{\prime}v_{r}+\sum_{b\in[m]}\sum_{r\in S ^{(b)}}\frac{1}{|S^{(b)}|}\sum_{j\in S^{(b)}}x_{j}^{\prime}v_{r}\right\|_{2}\] \[\leq\left\|u^{(a)}-\frac{1}{|S^{(a)}|}\sum_{r\in S^{(a)}}v_{r} \right\|_{2}+\left\|v_{i}^{\prime}-\sum_{r\in[N]}x_{r}^{\prime}v_{r}^{\prime} \right\|_{2}\] \[\qquad+\left\|-\sum_{b\in[m]}\sum_{r\in S^{(b)}}x_{r}^{\prime}u^{ (b)}+\sum_{b\in[m]}\sum_{r\in S^{(b)}}\frac{1}{|S^{(b)}|}\sum_{j\in S^{(b)}}x_ {j}^{\prime}v_{r}\right\|_{2}\] \[=\left\|u^{(a)}-\frac{1}{|S^{(a)}|}\sum_{r\in S^{(a)}}v_{r} \right\|_{2}+\left\|v_{i}^{\prime}-\sum_{r\in[N]}x_{r}^{\prime}v_{r}^{\prime} \right\|_{2}\] \[\qquad+\left\|\sum_{b\in[m]}\sum_{j\in S^{(b)}}x_{j}^{\prime} \left(u^{(b)}-\frac{1}{|S^{(b)}|}\sum_{r\in S^{(b)}}v_{r}\right)\right\|_{2}\]

where the first and third inequalities use that \(v_{r}=v_{r}^{\prime}+u^{(b)}\) for all \(r\in S^{(b)}\), and throughout we use that \(x_{r}=x_{r}^{\prime}=0\) for \(r\not\in S^{(1)}\cup\cdots\cup S^{(m)}\). Applying Property **(iv)**, equation (9), and the bound \(\left\|x^{\prime}\right\|_{1}\leq 3^{t-1}\), we get

\[\left\|v_{i}-\sum_{r\in[N]}x_{r}v_{r}\right\|_{2} \leq\sqrt{9t(tn/N)^{1/t}}+4^{t-1}\sqrt{9t(tn/N)^{1/t}}+3^{t-1} \sqrt{9t(tn/N)^{1/t}}\] \[\leq 3\cdot 4^{t-1}\sqrt{9t(tn/N)^{1/t}}\]

as claimed. 

However, we wanted a bound on \(v_{i}-\sum_{j\neq i}x_{j}v_{j}\), and unfortunately \(x_{i}\neq 0\). Fortunately, it is enough that \(x_{i}\) is bounded away from \(1\). Since \(x_{i}^{\prime}=0\), we have

\[|x_{i}|\leq\frac{1}{|S^{(a)}|}\sum_{j\in S^{(a)}}|x_{j}^{\prime}|+\frac{1}{|S^ {(a)}|}\leq\frac{\left\|x^{\prime}\right\|_{1}+1}{|S^{(a)}|}\leq\frac{3^{t-1}} {3^{t+1}}=\frac{1}{9}.\]

Thus, by Claim E.4,

\[\left\|v_{i}-\frac{1}{1-x_{i}}\sum_{j\neq i}x_{j}v_{j}\right\|_{2}\leq\frac{1} {1-x_{i}}\cdot 3\cdot 4^{t-1}\sqrt{9t(tn/N)^{1/t}}\leq 4^{t}\sqrt{9t(tn/N)^{1/t}}.\]

Finally, we have \(\left\|x/(1-x_{i})\right\|_{1}\leq(9/8)(2\cdot 3^{t-1}+1)\leq 3^{t}\), so \(x/(1-x_{i})\) satisfies all the desired conditions. This completes the induction. 

Finally, we extend Theorem E.3 to real-valued sparse vectors via a discretization argument.

**Lemma E.5**.: _Let \(n,N,t\in\mathbb{N}\) and let \(v_{1},\ldots,v_{N}\in\mathbb{R}^{n}\) be \(t\)-sparse vectors. Then there is some \(i\in[N]\) and \(x\in\mathbb{R}^{n}\) such that \(\left\|x\right\|_{1}\leq 3^{t}\) and_

\[\left\|v_{i}-\sum_{j\neq i}x_{j}v_{j}\right\|_{2}\leq 4^{t+2}\sqrt{t}(n/N)^{1/( 4t)}\cdot\max_{j\in[N]}\left\|v_{j}\right\|_{\infty}.\]

Proof.: Without loss of generality assume that \(\max_{j\in[N]}\left\|v_{j}\right\|_{\infty}=1\). Let \(k\in\mathbb{N}\) be fixed later. Define a map \(\varphi:[-1,1]\to\{0,1\}^{2k+1}\) by

\[\varphi(c)=\begin{cases}e_{k+1+\lfloor ck\rfloor}&\text{ if }c<0\\ e_{k+1}&\text{ if }c=0\\ e_{k+1+\lceil ck\rceil}&\text{ if }c>0\end{cases}.\]Also let \(\Phi:\mathbb{R}^{2k+1}\to\mathbb{R}\) be the linear map that sends \(\Phi e_{i}\mapsto(i-k-1)/k\) for each \(i\in[2k+1]\). Note that \(\left\lvert\Phi\varphi(c)-c\right\rvert\leq 1/k\) for all \(c\in[-1,1]\) and \(\Phi\varphi(0)=0\). Define \(\varphi^{\oplus n}:[-1,1]^{n}\to\{0,1\}^{(2k+1)n}\) by \(\varphi(c_{1},\ldots,c_{n})=(\varphi(c_{1}),\ldots,\varphi(c_{n}))\), and define \(\Phi^{\oplus n}:\{0,1\}^{(2k+1)n}\to\mathbb{R}^{n}\) by \(\Phi^{\oplus n}(x_{1},\ldots,x_{n})=(\Phi(x_{1}),\ldots,\Phi(x_{n}))\). For any \(i\in[N]\), the vector \(\varphi^{\oplus n}(v_{i})\) is \(t\)-sparse and lies in \(\{0,1\}^{(2k+1)n}\). Thus, applying Theorem E.3 gives some \(i\in[N]\) and \(x\in\mathbb{R}^{N}\) with \(\left\lVert x\right\rVert_{1}\leq 3^{t}\) and

\[\left\lVert\varphi^{\oplus n}(v_{i})-\sum_{j\neq i}x_{j}\varphi^{\oplus n}(v_{ j})\right\rVert_{2}\leq 4^{t}\sqrt{9t(tn(2k+1)/N)^{1/t}}.\]

Since \(\Phi^{\oplus n}\) is a linear map and \(\left\lVert\Phi^{\oplus n}\right\rVert_{2}=\left\lVert\Phi\right\rVert_{2}\leq \sqrt{2k+1}\), we then get

\[\left\lVert\Phi^{\oplus n}\varphi^{\oplus n}(v_{i})-\sum_{j\neq i}x_{j}\Phi^{ \oplus n}\varphi^{\oplus n}(v_{j})\right\rVert_{2}\leq 4^{t}\sqrt{9t(2k+1)(tn(2k+1)/N )^{1/t}}.\]

But now for every \(j\in[N]\), we know that

\[\left\lVert v_{j}-\Phi^{\oplus n}\varphi^{\oplus n}(v_{j})\right\rVert_{2}^{2} =\sum_{a\in\operatorname{supp}(v_{j})}(v_{ja}-\Phi\varphi(v_{ja}))^{2}\leq\frac {t}{k^{2}}.\]

We conclude that

\[\left\lVert v_{i}-\sum_{j\neq i}x_{j}v_{j}\right\rVert_{2}\leq \left\lVert\Phi^{\oplus n}\varphi^{\oplus n}(v_{i})-\sum_{j\neq i}x_{j} \Phi^{\oplus n}\varphi^{\oplus n}(v_{j})\right\rVert_{2}+\left\lVert v_{i}- \Phi^{\oplus n}\varphi^{\oplus n}(v_{i})\right\rVert_{2}\] \[\qquad+\sum_{j\neq i}\left\lvert x_{j}\right\rvert\cdot\left\lVert v _{j}-\Phi^{\oplus n}\varphi^{\oplus n}(v_{j})\right\rVert_{2}\] \[\leq 4^{t}\sqrt{9t(2k+1)(tn(2k+1)/N)^{1/t}}+(1+3^{t})\cdot\frac{ \sqrt{t}}{k}\] \[\leq(2k+1)\cdot 4^{t+1}\sqrt{t}(n/N)^{1/(2t)}+\frac{4^{t}\sqrt{t}}{k}.\]

Taking \(k=(N/n)^{1/(4t)}\) gives the claimed bound. 

Combining Lemma E.5 with Lemma E.1 lets us prove Theorem 3.2.

Proof of Theorem 3.2.: Set \(\delta:=\sqrt{1/(4\kappa)}\) and \(N=4^{4t(t+3)}t^{2t}\kappa^{2t}n\). By Lemma E.5, for any \(t\)-sparse vectors \(v_{1},\ldots,v_{N}\in\mathbb{R}^{n}\) with \(\left\lVert v_{i}\right\rVert_{2}\leq 1\) for all \(i\in[N]\), there is some \(i\in[N]\) and \(x\in\mathbb{R}^{n}\) such that \(\left\lVert x\right\rVert_{1}\leq 3^{t}\) and

\[\left\lVert v_{i}-\sum_{j\neq i}x_{j}v_{j}\right\rVert_{2}\leq 4^{t+2}\sqrt{t}(n/N )^{1/(4t)}\leq\frac{1}{4\sqrt{\kappa}}<\delta.\]

It follows from Lemma E.1 that \(\mathcal{P}_{t,1/3^{t+1}}(\Sigma)\leq N\log_{2}\kappa\). Finally, by Lemma A.2, we conclude that \(\mathcal{N}_{t,1/3^{t+1}}(\Sigma)\leq N\log_{2}\kappa\). 

## Appendix F Generalization bounds

### Finite-class model selection

**Lemma F.1**.: _Let \(n,m,n_{\text{eff}}\in\mathbb{N}\) and let \(\Sigma\) be a positive semi-definite matrix. Fix a vector \(w^{*}\in\mathbb{R}^{n}\) and a closed set \(\mathcal{W}\subseteq\mathbb{R}^{n}\) and let \((X_{i},y_{i})_{i=1}^{m}\) be independent draws \(X_{i}\sim N(0,\Sigma)\) and \(y_{i}=\langle X_{i},w^{*}\rangle+\xi_{i}\) where \(\xi_{i}\sim N(0,\sigma^{2})\). Pick_

\[\hat{w}\in\operatorname*{argmin}_{w\in\mathcal{W}}\left\lVert\mathbb{X}w-y \right\rVert_{2}^{2}\]

_where \(\mathbb{X}:\,m\times n\) is the matrix with rows \(X_{1},\ldots,X_{m}\). For any \(\epsilon,\delta\in(0,1)\), suppose that with probability at least \(1-\delta\), the following bounds hold uniformly over \(w\in\mathcal{W}\):_1. \(\left|\frac{1}{m}\left\|\mathbb{X}(w-w^{*})\right\|_{2}^{2}-\left\|w-w^{*}\right\|_ {\Sigma}^{2}\right|\leq\epsilon\left\|w-w^{*}\right\|_{\Sigma}^{2}\)__
2. \(\left|\left\langle\xi,\frac{\mathbb{X}(w-w^{*})}{\left\|\mathbb{X}(w-w^{*}) \right\|_{2}}\right\rangle\right|\leq\sigma\sqrt{n_{\text{eff}}}\)_._

_Then with probability at least \(1-\delta\) it also holds that_

\[\left\|\hat{w}-w^{*}\right\|_{\Sigma}\leq\sqrt{\frac{1+\epsilon}{1-\epsilon}} \inf_{w\in\mathcal{W}}\left\|w-w^{*}\right\|_{\Sigma}+2\sigma\sqrt{\frac{2n_{ \text{eff}}}{m}}.\]

Proof.: Consider the event in which both bounds hold. Let \(w_{\text{opt}}\in\operatorname*{argmin}_{w\in\mathcal{W}}\left\|w-w^{*} \right\|_{\Sigma}^{2}\). Then

\[\left\|\mathbb{X}(\hat{w}-w^{*})\right\|_{2}^{2} =\left\|\mathbb{X}\hat{w}-y\right\|_{2}^{2}+2\langle\xi,\mathbb{X }(\hat{w}-w^{*})\rangle-\left\|\xi\right\|_{2}^{2}\] \[\leq\left\|\mathbb{X}w_{\text{opt}}-y\right\|_{2}^{2}+2\langle\xi,\mathbb{X}(\hat{w}-w^{*})\rangle-\left\|\xi\right\|_{2}^{2}\] \[=\left\|\mathbb{X}(w_{\text{opt}}-w^{*})\right\|_{2}^{2}+2\langle \xi,\mathbb{X}(\hat{w}-w^{*})\rangle-2\langle\xi,\mathbb{X}(w_{\text{opt}}-w^{ *})\rangle\] \[\leq\left\|\mathbb{X}(w_{\text{opt}}-w^{*})\right\|_{2}^{2}+2 \left(\left\|\mathbb{X}(\hat{w}-w^{*})\right\|_{2}+\left\|\mathbb{X}(w_{\text{ opt}}-w^{*})\right\|_{2}\right)\sigma\sqrt{n_{\text{eff}}}.\]

Subtracting \(\left\|\mathbb{X}(w_{\text{opt}}-w^{*})\right\|_{2}^{2}\) from both sides and dividing by \(\left\|\mathbb{X}(\hat{w}-w^{*})\right\|_{2}+\left\|\mathbb{X}(w_{\text{opt}} -w^{*})\right\|_{2}\), we get that

\[\left\|\mathbb{X}(\hat{w}-w^{*})\right\|_{2}-\left\|\mathbb{X}(w_{\text{opt}}- w^{*})\right\|_{2}\leq 2\sigma\sqrt{n_{\text{eff}}}.\]

It follows that

\[\left\|\hat{w}-w^{*}\right\|_{\Sigma} \leq\sqrt{\frac{1}{(1-\epsilon)m}}\left\|\mathbb{X}(\hat{w}-w^{*} )\right\|_{2}\] \[\leq\sqrt{\frac{1}{(1-\epsilon)m}}\left\|\mathbb{X}(w_{\text{opt} }-w^{*})\right\|_{2}+2\sigma\sqrt{\frac{(1+\epsilon)n_{\text{eff}}}{m}}\] \[\leq\sqrt{\frac{1+\epsilon}{1-\epsilon}}\left\|w_{\text{opt}}-w^{ *}\right\|_{\Sigma}+2\sigma\sqrt{\frac{2n_{\text{eff}}}{m}}\]

as desired. 

**Lemma F.2**.: _Let \(n,m\in\mathbb{N}\) and let \(\Sigma\) be a positive semi-definite matrix. Fix a vector \(w^{*}\in\mathbb{R}^{n}\) and a finite set \(\mathcal{W}\subseteq\mathbb{R}^{n}\) and let \((X_{i},y_{i})_{i=1}^{m}\) be independent draws \(X_{i}\sim N(0,\Sigma)\) and \(y_{i}=\langle X_{i},w^{*}\rangle+\xi_{i}\) where \(\xi_{i}\sim N(0,\sigma^{2})\). Pick_

\[\hat{w}\in\operatorname*{argmin}_{w\in\mathcal{W}}\left\|\mathbb{X}w-y\right\| _{2}^{2}.\]

_For any \(\epsilon,\delta\in(0,1)\), if \(m\geq 8\epsilon^{-2}\log(2|\mathcal{W}|/\delta)\), then with probability at least \(1-2\delta\), we have_

\[\left\|\hat{w}-w^{*}\right\|_{\Sigma}\leq\sqrt{\frac{1+\epsilon}{1-\epsilon}} \inf_{w\in\mathcal{W}}\left\|w-w^{*}\right\|_{\Sigma}+4\sigma\sqrt{\frac{\log( 2|\mathcal{W}|/\delta)}{m}}.\]

Proof.: For any fixed \(w\in\mathcal{W}\), the random variables \(\langle X_{i},w-w^{*}\rangle\sim N(0,\left\|w-w^{*}\right\|_{\Sigma}^{2})\) are independent, and therefore \(\left\|\mathbb{X}(w-w^{*})\right\|_{2}^{2}\sim\left\|w-w^{*}\right\|_{\Sigma} ^{2}\chi_{m}^{2}\). It follows that for any \(\epsilon>0\),

\[\Pr\left[\left|\frac{1}{m}\left\|\mathbb{X}(w-w^{*})\right\|_{2}^{2}-\left\|w- w^{*}\right\|_{\Sigma}^{2}\right|>\epsilon\left\|w-w^{*}\right\|_{\Sigma}^{2} \right]\leq 2e^{-m\epsilon^{2}/8}.\]

By the union bound, if \(m\geq 8\epsilon^{-2}\log(2|\mathcal{W}|/\delta)\), then with probability at least \(1-\delta\) it holds that for all \(w\in\mathcal{W}\),

\[\left|\frac{1}{m}\left\|\mathbb{X}(w-w^{*})\right\|_{2}^{2}-\left\|w-w^{*} \right\|_{\Sigma}^{2}\right|\leq\epsilon\left\|w-w^{*}\right\|_{\Sigma}^{2}. \tag{10}\]

Also, for any fixed \(w\in\mathcal{W}\), conditioned on \(\mathbb{X}\), the random variable \(\langle\xi,\frac{\mathbb{X}(w-w^{*})}{\left\|\mathbb{X}(w-w^{*})\right\|_{2}}\rangle\) has distribution \(N(0,\sigma^{2})\). Thus, by a Gaussian tail bound and the union bound, we have for any \(t>0\) that

\[\Pr\left[\max_{w\in\mathcal{W}}\left|\left\langle\xi,\frac{\mathbb{X}(w-w^{*}) }{\left\|\mathbb{X}(w-w^{*})\right\|}\right\rangle\right|\geq\sigma t\right] \leq 2|\mathcal{W}|\cdot e^{-t^{2}/2}.\]In particular, with probability at least \(1-\delta\) it holds that

\[\max_{w\in\mathcal{W}}\left|\left\langle\xi,\frac{\mathbb{X}(w-w^{*})}{\left\| \mathbb{X}(w-w^{*})\right\|}\right\rangle\right|\leq\sigma\sqrt{2\log(2|\mathcal{ W}|/\delta)}. \tag{11}\]

Using (10) and (11) we apply Lemma F.1 which gives the desired bound. 

### Weak learning

**Lemma F.3**.: _Let \(n,m\in\mathbb{N}\) and \(\epsilon,\delta>0\). Let \(\Sigma:n\times n\) be a positive semi-definite matrix and let \(\mathbb{X}:m\times n\) have independent rows \(X_{1},\ldots,X_{m}\sim N(0,\Sigma)\). For any fixed \(u,v\in\mathbb{R}^{n}\), if \(m\geq 8\epsilon^{-2}\log(8/\delta)\), then it holds with probability at least \(1-\delta\) that_

\[\left|u^{\top}\left(\frac{1}{m}\mathbb{X}^{\top}\mathbb{X}-\Sigma\right)v \right|\leq 2\epsilon\left\|u\right\|_{\Sigma}\left\|v\right\|_{\Sigma}.\]

Proof.: Decompose \(u=av+w\) where \(\langle v,w\rangle_{\Sigma}=0\), so that \(a=\langle u,v\rangle_{\Sigma}/\left\|v\right\|_{\Sigma}^{2}\). Since \(\left\|\mathbb{X}v\right\|_{2}^{2}\sim\left\|v\right\|_{\Sigma}^{2}\chi_{m}^{2}\) and \(m\geq 8\epsilon^{-2}\log(4/\delta)\) it holds with probability at least \(1-\delta/2\) that

\[\left|v^{\top}\left(\frac{1}{m}\mathbb{X}^{\top}\mathbb{X}-\Sigma\right)v \right|=\left|\frac{1}{m}\sum_{i=1}^{m}\langle X_{i},v\rangle^{2}-\left\|v \right\|_{\Sigma}^{2}\right|\leq\epsilon\left\|v\right\|_{\Sigma}^{2}.\]

Next,

\[\left|w^{\top}\left(\frac{1}{m}\mathbb{X}^{\top}\mathbb{X}-\Sigma\right)v \right|=\left|\frac{1}{m}\sum_{i=1}^{m}\langle X_{i},w\rangle\langle X_{i},v \rangle\right|=\left|\frac{1}{m}\sum_{i=1}^{m}\langle Z_{i},\Sigma^{1/2}w \rangle\langle Z_{i},\Sigma^{1/2}v\rangle\right|\]

where we define independent random vectors \(Z_{1},\ldots,Z_{m}\sim N(0,I_{n})\) so that \(X_{i}=\Sigma^{1/2}Z_{i}\). Since \(m\geq 8\log(2/\delta)\), with probability at least \(1-\delta/4\) we have \(\sum_{i=1}^{m}\langle Z_{i},\Sigma^{1/2}v\rangle^{2}\leq 2m\left\|v\right\|_{ \Sigma}^{2}\). Condition on the value of this sum, and note that since \(\Sigma^{1/2}v\perp\Sigma^{1/2}w\), the random variables \(\langle Z_{i},\Sigma^{1/2}w\rangle\) are still (independent and) distributed as \(N(0,\left\|w\right\|_{\Sigma}^{2})\). Thus

\[\frac{1}{m}\sum_{i=1}^{m}\langle Z_{i},\Sigma^{1/2}w\rangle\langle Z_{i},\Sigma ^{1/2}v\rangle\sim N\left(0,\frac{1}{m^{2}}\sum_{i=1}^{m}\left\|w\right\|_{ \Sigma}^{2}\langle Z_{i},\Sigma^{1/2}v\rangle^{2}\right).\]

When the variance is at most \(2\left\|w\right\|_{\Sigma}^{2}\left\|v\right\|_{\Sigma}^{2}/m\), we have with probability at least \(1-\delta/4\) that the sum is at most \(2\left\|w\right\|_{\Sigma}\left\|v\right\|_{\Sigma}\sqrt{2\log(8/\delta)/m}\) in magnitude. So, using \(m\geq 8\epsilon^{-2}\log(8/\delta)\) it holds unconditionally with probability at least \(1-\delta/2\) that

\[\left|\frac{1}{m}\sum_{i=1}^{m}\langle Z_{i},\Sigma^{1/2}w\rangle\langle Z_{i},\Sigma^{1/2}v\rangle\right|\leq\epsilon\left\|w\right\|_{\Sigma}\left\|v \right\|_{\Sigma}.\]

In all, we have that

\[\left|u^{\top}\left(\frac{1}{m}\mathbb{X}^{\top}\mathbb{X}-\Sigma\right)v \right|\leq\left|a\right|\epsilon\left\|v\right\|_{\Sigma}^{2}+\epsilon\left\| w\right\|_{\Sigma}\left\|v\right\|_{\Sigma}\leq 2\epsilon\left\|u\right\|_{\Sigma} \left\|v\right\|_{\Sigma}\]

using that \(\left|a\right|\leq\left\|u\right\|_{\Sigma}/\left\|v\right\|_{\Sigma}\) and \(\left\|w\right\|_{\Sigma}\leq\left\|u\right\|_{\Sigma}\). 

**Lemma F.4**.: _Let \(n,m\in\mathbb{N}\) and let \(\Sigma\) be a positive semi-definite matrix. Fix a vector \(w^{*}\in\mathbb{R}^{n}\) and a finite set \(\mathcal{W}\subseteq\mathbb{R}^{n}\) and let \((X_{i},y_{i})_{i=1}^{m}\) be independent draws \(X_{i}\sim N(0,\Sigma)\) and \(y_{i}=\langle X_{i},w^{*}\rangle+\xi_{i}\) where \(\xi_{i}\sim N(0,\sigma^{2})\). Pick_

\[(\hat{w},\hat{\beta})\in\operatorname*{arg\,min}_{\begin{subarray}{c}w\in \mathcal{W}\\ \beta\in\mathcal{R}\end{subarray}}\left\|\beta\mathbb{X}w-y\right\|_{2}^{2}.\]

_Suppose \(\alpha:=\max_{w\in\mathcal{W}}\frac{(w,w^{*})_{\Sigma}}{\left\|w\right\|_{ \Sigma}\left\|w^{*}\right\|_{\Sigma}}>0\). For any \(\delta>0\), if \(m\geq C\alpha^{-2}\log(32|\mathcal{W}|/\delta)\) for a sufficiently large absolute constant \(\hat{C}\), then with probability at least \(1-\delta\),_

\[\left\|\hat{\beta}\hat{w}-w^{*}\right\|_{\Sigma}^{2}\leq(1-\alpha^{2}/4)\left\| w^{*}\right\|_{\Sigma}^{2}+\frac{400\sigma^{2}\log(4|\mathcal{W}|/\delta)}{ \alpha^{2}m}.\]Proof.: For any vectors \(u,v\in\mathbb{R}^{n}\), define \(\Delta(u,v)=u^{\top}\left(\frac{1}{m}\mathbb{X}^{\top}\mathbb{X}-\Sigma\right)v\).

**Claim F.5**.: _With probability at least \(1-\delta\), the following bounds hold uniformly over \(w\in\mathcal{W}\) and \(\beta\in\mathbb{R}\):_

1. \(\left|\left\langle\xi,\frac{\mathbb{X}(\beta w-w^{*})}{\left\|\mathbb{X}(\beta w -w^{*})\right\|_{2}}\right\rangle\right|\leq\sigma\sqrt{n_{\text{eff}}}\) _where_ \(n_{\text{eff}}:=2\log(32|\mathcal{W}|/\delta)\)_._
2. \(\left|\Delta(\beta w,w^{*})\right|\leq\frac{\alpha}{100}\left\|\beta w\right\| _{\Sigma}\left\|w^{*}\right\|_{\Sigma}\)__
3. \(\left|\Delta(\beta w,\beta w)\right|\leq\frac{\alpha}{100}\left\|\beta w\right\| _{\Sigma}^{2}.\)__

Proof of claim.: For item (1), fix \(w\in\mathcal{W}\). Let \(\Phi^{(w)}:2\times m\) be a matrix whose rows form an orthonormal basis for \(\operatorname{span}\{\mathbb{X}w,\mathbb{X}w^{*}\}\subseteq\mathbb{R}^{m}\). Then (denoting the unit Euclidean ball in \(\mathbb{R}^{2}\) by \(B_{2}\)) we have for all \(\beta\in\mathbb{R}\) that

Since \(\langle\Phi^{(w)}_{i},\xi\rangle\sim N(0,\sigma^{2})\), we have \(\Pr[\left|\langle\Phi^{(w)}_{i},\xi\rangle\right|>\sigma\sqrt{2\log(4|\mathcal{ W}|/\delta)}]\leq\delta/(4|\mathcal{W}|)\). A union bound over \(i\in[2]\) and \(w\in\mathcal{W}\) gives that condition (2) in Lemma F.1 is satisfied with probability at least \(1-\delta/2\).

For items (2) and (3), note that \(\Delta\) is bilinear, so it suffices to take \(\beta=1\). Applying Lemma F.3 and the union bound, so long as \(m\geq C\alpha^{-2}\log(32|\mathcal{W}|/\delta)\) for a sufficiently large constant \(C\), items (2) and (3) hold simultaneously with probability at least \(1-\delta/2\). 

Henceforth we assume that all of the events in the above claim hold. Let \(w_{0}\in\mathcal{W}\) be such that \(\left|\langle w_{0},w^{*}\rangle_{\Sigma}\right|=\alpha\left\|w_{0}\right\|_{ \Sigma}\left\|w^{*}\right\|_{\Sigma}\). Let \(\beta_{0}=\langle w_{0},w^{*}\rangle_{\Sigma}/\left\|w_{0}\right\|_{\Sigma}^{2}\). Then

\[\left\|\beta_{0}w_{0}-w^{*}\right\|_{\Sigma}^{2}=(1-\alpha^{2})\left\|w^{*} \right\|_{\Sigma}^{2}.\]

**Claim F.6**.: _The excess empirical risk can be bounded as_

\[\left\|\mathbb{X}(\hat{\beta}\hat{w}-w^{*})\right\|_{2}\leq\left\|\mathbb{X}( w_{0}-w^{*})\right\|_{2}+2\sigma\sqrt{n_{\text{eff}}}.\]

Proof of claim.: We have

\[\left\|\mathbb{X}(\hat{\beta}\hat{w}-w^{*})\right\|_{2}^{2} =\left\|\mathbb{X}\hat{\beta}\hat{w}-y\right\|_{2}^{2}+2\langle \xi,\mathbb{X}(\hat{\beta}\hat{w}-w^{*})\rangle-\left\|\xi\right\|_{2}^{2}\] \[\leq\left\|\mathbb{X}\beta_{0}w_{0}-y\right\|_{2}^{2}+2\langle \xi,\mathbb{X}(\hat{\beta}\hat{w}-w^{*})\rangle-\left\|\xi\right\|_{2}^{2}\] \[=\left\|\mathbb{X}(\beta_{0}w_{0}-w^{*})\right\|_{2}^{2}+2\langle \xi,\mathbb{X}(\hat{\beta}\hat{w}-w^{*})\rangle-2\langle\xi,\mathbb{X}(\beta_ {0}w_{0}-w^{*})\rangle\] \[\leq\left\|\mathbb{X}(\beta_{0}w_{0}-w^{*})\right\|_{2}^{2}+2 \left(\left\|\mathbb{X}(\hat{\beta}\hat{w}-w^{*})\right\|_{2}+\left\|\mathbb{X}( \beta_{0}w_{0}-w^{*})\right\|_{2}\right)\sigma\sqrt{n_{\text{eff}}}\]

where the last bound is by item (1) of Claim F.5. Simplifying, we get the claimed bound. 

Now we have

\[\left\|\hat{\beta}\hat{w}-w^{*}\right\|_{\Sigma}^{2} =\frac{1}{m}\left\|\mathbb{X}(\hat{\beta}\hat{w}-w^{*})\right\|_ {2}^{2}-\Delta(\hat{\beta}\hat{w}-w^{*},\hat{\beta}\hat{w}-w^{*})\] \[\leq\frac{1}{m}\left(\left\|\mathbb{X}(\beta_{0}w_{0}-w^{*}) \right\|_{2}+2\sigma\sqrt{n_{\text{eff}}}\right)^{2}-\Delta(\hat{\beta}\hat{w}-w ^{*},\hat{\beta}\hat{w}-w^{*})\] \[\leq\frac{1+\alpha^{2}/100}{m}\left\|\mathbb{X}(\beta_{0}w_{0}-w ^{*})\right\|_{2}^{2}+(1+100\alpha^{-2})\frac{\sigma^{2}n_{\text{eff}}}{m}- \Delta(\hat{\beta}\hat{w}-w^{*},\hat{\beta}\hat{w}-w^{*})\] \[=(1+\alpha^{2}/100)\left\|\beta_{0}w_{0}-w^{*}\right\|_{\Sigma}^{2 }+(1+100\alpha^{-2})\frac{\sigma^{2}n_{\text{eff}}}{m}\] \[\qquad-\Delta(\hat{\beta}\hat{w}-w^{*},\hat{\beta}\hat{w}-w^{*})+ \left(1+\frac{\alpha^{2}}{100}\right)\Delta(\beta_{0}w_{0}-w^{*},\beta_{0}w_{0 }-w^{*})\]

[MISSING_PAGE_FAIL:41]

Proof.: For notational convenience, define \(F(v):=(1/2)\Phi(v-v^{*})+\sqrt{p}\left\|v-v^{*}\right\|_{\Sigma}\). We apply the lemma's assumption twice:

* For any fixed \(\xi\), the random variable \(\mathbb{X}\xi\) has distribution \(N(0,\left\|\xi\right\|_{2}^{2}\Sigma)\). By the above claim, with probability at least \(1-\delta\) over \(\mathbb{X}\), we have \(\left\langle\xi,\mathbb{X}(v-v^{*})\right\rangle\leq\left\|\xi\right\|_{2}F(v)\) uniformly in \(v\in\mathbb{R}^{n}\). Since \(\left\|\xi\right\|_{2}^{2}\sim\sigma^{2}\chi_{m}^{2}\) and \(m\geq 8\log(2/\delta)\), it holds with probability at least \(1-\delta\) that \(\frac{1}{\sqrt{m}}\left\|\xi\right\|_{2}\leq\sqrt{2}\sigma\). Thus, with probability at least \(1-2\delta\), we have \[\left\langle\xi,\mathbb{X}(v-v^{*})\right\rangle\leq\sqrt{2m}\sigma F(v)\] (13) uniformly in \(v\in\mathbb{R}^{n}\).
* The assumption means that we can apply Theorem C.1 with (noiseless) samples \((X_{i},\left\langle X_{i},v^{*}\right\rangle)_{i=1}^{m}\) to get the following: since \(m\geq 196\log(12/\delta)\), it holds with probability at least \(1-4\delta\) over the randomness of \(\mathbb{X}\) that for all \(v\in\mathbb{R}^{n}\), \[\left\|v-v^{*}\right\|_{\Sigma}^{2}\leq\frac{2}{m}\left\|\mathbb{X}(v-v^{*}) \right\|_{2}^{2}+\frac{2}{m}F(v)^{2}.\] (14)

We also observe that the entries of \(y\) are independent and identically distributed as \(N(0,\left\|v^{*}\right\|_{\Sigma}^{2}+\sigma^{2})\), so by a \(\chi^{2}\) tail bound, since \(m\geq 32\log(2/\delta)\), it holds with probability at least \(1-\delta\) that

\[\frac{1}{m}\left\|y\right\|_{2}^{2}\in\left[\frac{1}{2}(\left\|v^{*}\right\|_{ \Sigma}^{2}+\sigma^{2}),\frac{3}{2}(\left\|v^{*}\right\|_{\Sigma}^{2}+\sigma^{ 2})\right]. \tag{15}\]

We now condition on the event (which occurs with probability at least \(1-7\delta\)) that the bounds (13), (14), and (15) all hold. Specifying (14) to \(v:=\hat{v}\), we get that

\[\frac{m}{2}\left\|\hat{v}-v^{*}\right\|_{\Sigma}^{2}\] \[\leq\left\|\mathbb{X}(\hat{v}-v^{*})\right\|_{2}^{2}+F(\hat{v})^ {2}\] \[\leq\left\|\mathbb{X}(\hat{v}-v^{*})\right\|_{2}^{2}-\left\|X\hat {v}-y\right\|_{2}^{2}-\Phi(\hat{v})^{2}-\left\|y\right\|_{2}\Phi(\hat{v})\] \[\quad\quad+\left\|\mathbb{X}v^{*}-y\right\|_{2}^{2}+\Phi(v^{*})^ {2}+\left\|y\right\|_{2}\Phi(v^{*})+F(\hat{v})^{2}\] \[=2\langle\mathbb{X}v^{*}-y,\mathbb{X}(\hat{v}-v^{*})\rangle\] \[\quad\quad-\Phi(\hat{v})^{2}-\left\|y\right\|_{2}\Phi(\hat{v})+ \Phi(v^{*})^{2}+\left\|y\right\|_{2}\Phi(v^{*})+F(\hat{v})^{2}\] \[\leq\sqrt{2m}\sigma F(\hat{v})-\Phi(\hat{v})^{2}-\left\|y\right\| _{2}\Phi(\hat{v})+\Phi(v^{*})^{2}+\left\|y\right\|_{2}\Phi(v^{*})+F(\hat{v})^{2}\]

where the first inequality is by (14), the second inequality is by optimality of \(\hat{v}\), and the third inequality is by (13). We now expand \(F(\hat{v})\) in the above expression. If \(\sqrt{2mp}\sigma\left\|\hat{v}-v^{*}\right\|_{\Sigma}\) exceeds \(\frac{m}{8}\left\|\hat{v}-v^{*}\right\|_{\Sigma}^{2}\) then the lemma immediately holds since

\[\left\|\hat{v}-v^{*}\right\|_{\Sigma}^{2}\leq\frac{128\sigma^{2}p}{m}.\]

So we may assume that in fact \(\sqrt{2mp}\sigma\left\|\hat{v}-v^{*}\right\|_{\Sigma}\leq\frac{m}{8}\left\| \hat{v}-v^{*}\right\|_{\Sigma}^{2}\). By the lemma assumptions, we also know that \(m\geq 16p\). Thus, expanding \(F(\hat{v})\) and applying these bounds,

\[\frac{m}{2}\left\|\hat{v}-v^{*}\right\|_{\Sigma}^{2} \leq\sqrt{2m}\sigma\left(\frac{1}{2}\Phi(\hat{v}-v^{*})+\sqrt{p} \left\|\hat{v}-v^{*}\right\|_{\Sigma}\right)\] \[\quad-\Phi(\hat{v})^{2}-\left\|y\right\|_{2}\Phi(\hat{v})+\Phi(v^ {*})^{2}+\left\|y\right\|_{2}\Phi(v^{*})\] \[\quad\quad+\frac{1}{2}\Phi(\hat{v}-v^{*})^{2}+2p\left\|\hat{v}-v ^{*}\right\|_{\Sigma}^{2}\] \[\leq\sqrt{\frac{m}{2}}\sigma\Phi(\hat{v}-v^{*})+\frac{m}{8}\left\| \hat{v}-v^{*}\right\|_{\Sigma}^{2}\] \[\quad\quad-\Phi(\hat{v})^{2}-\left\|y\right\|_{2}\Phi(\hat{v})+ \Phi(v^{*})^{2}+\left\|y\right\|_{2}\Phi(v^{*})\]\[\frac{m}{4}\left\|\hat{v}-v^{*}\right\|_{\Gamma}^{2}\] \[\leq\left(\sqrt{\frac{m}{2}}\sigma-\left\|y\right\|_{2}\right)\Phi( \hat{v})+\left(\sqrt{\frac{m}{2}}\sigma+\left\|y\right\|_{2}\right)\Phi(v^{*})+ 2\Phi(v^{*})^{2}\] \[\leq 2(\sigma+\left\|v^{*}\right\|_{\Sigma})\sqrt{m}\Phi(v^{*})+2 \Phi(v^{*})^{2}\]

where the last inequality uses both sides of the bound (15). 

## Appendix G Covering bounds from classical assumptions

In this section, we further motivate the definition of our covering number \(\mathcal{N}_{t,\alpha}(\Sigma)\) by showing that in all settings where efficient SLR algorithms are known, there is a straightforward _linear_ upper bound on the covering number. This lends weight to the need for stronger upper bounds on \(\mathcal{N}_{t,\alpha}\) as a stepping stone towards more efficient algorithms for sparse linear regression.

### Compatibility condition

**Definition G.1** (Compatibility Condition, see e.g. [40]).: For a positive semidefinite matrix \(\Sigma:n\times n\), \(L\geq 1\), and set \(S\subset[n]\), we say \(\Sigma\) has _\(S\)-restricted \(\ell_{1}\)-eigenvalue_

\[\phi^{2}(\Sigma,S)=\min_{w\in\mathcal{C}(S)}\frac{|S|\cdot\langle w,\Sigma w \rangle}{\|w_{S}\|_{1}^{2}}\]

where the cone \(\mathcal{C}(S)\) is defined as

\[\mathcal{C}(S)=\{w\neq 0:\|w_{S^{C}}\|_{1}\leq L\|w_{S}\|_{1}\}.\]

For \(t\in\mathbb{N}\), the \(t\)-restricted \(\ell_{1}\)-eigenvalue \(\phi^{2}(\Sigma,t)\) is the minimum over all \(S\) of size at most \(t\).

It is well-known that an upper bound on \(\frac{\max_{i}\Sigma_{i}}{\phi^{2}(\Sigma,t)}\) is sufficient for the success of Lasso (as well as nearly necessary; see e.g. the Weak Compatibility Condition defined in [23]):

**Theorem G.2** (see e.g. Corollary 5 in [45]).: _Fix \(n,m,t\in\mathbb{N}\), \(\sigma,\delta>0\), and a positive semi-definite matrix \(\Sigma:n\times n\) with \(\max_{i}\Sigma_{ii}\leq 1\). Fix a t-sparse vector \(v^{*}\in\mathbb{R}^{n}\) and let \((X_{i},y_{i})_{i=1}^{m}\) be independent samples distributed as \(X_{i}\sim N(0,\Sigma)\) and \(y_{i}=\langle X_{i},v^{*}\rangle+\xi_{i}\) where \(\xi_{i}\sim N(0,\sigma^{2})\). Define_

\[\hat{v}\in\operatorname*{argmin}_{v\in\mathbb{R}^{n}:\|v\|_{1}\leq\|v^{*}\|_{ 1}}\|\mathbb{X}v-y\|_{2}^{2}\]

_where \(\mathbb{X}:m\times n\) is the matrix with rows \(X_{1},\ldots,X_{m}\). If \(m\geq 4\phi^{2}(\Sigma,t)\cdot t\log(16n/\delta)\), then with probability at least \(1-\delta\), it holds that_

\[\|\hat{v}-v^{*}\|_{\Sigma}^{2}\leq O\left(\frac{\sigma^{2}t\log(16n/\delta)}{ \phi^{2}(\Sigma,t)m}\right).\]

**Fact G.3**.: _Let \(n,t\in\mathbb{N}\). For any positive semi-definite \(\Sigma:n\times n\) with \(\phi^{2}:=\phi^{2}(\Sigma,t)\) and \(\max_{i}\Sigma_{ii}\leq 1\), it holds that \(\mathcal{N}_{t,\phi/\sqrt{t}}(\Sigma)\leq n\)._

Proof.: The proof is essentially the same as that of Fact A.4. By Lemma A.3, it suffices to show that the standard basis is a \((t,\sqrt{t}/\phi)\)-\(\ell_{1}\)-representation for \(\Sigma\). Indeed, for any \(t\)-sparse \(v\in\mathbb{R}^{n}\), we have

\[\sum_{i=1}^{n}\left|v_{i}\right|\cdot\left\|e_{i}\right\|_{\Sigma}\leq\left\|v \right\|_{1}\cdot\max_{i}\sqrt{\Sigma_{ii}}\leq\frac{\sqrt{t}\left\|v\right\|_ {\Sigma}}{\phi}\]

as claimed.

### Submodularity ratio

**Definition G.4** (see e.g. [9]).: For a positive semi-definite matrix \(\Sigma:n\times n\) and a set \(L\subseteq[n]\) define the normalized residual covariance matrx \(\Sigma^{(L)}:n\times n\) by

\[\Sigma^{(L)}:=(D^{1/2})^{\dagger}\left(\Sigma-\Sigma_{L}^{\top}\Sigma_{LL}^{ \dagger}\Sigma_{L}\right)(D^{1/2})^{\dagger}\]

where \(D:=\operatorname{diag}\left(\Sigma-\Sigma_{L}^{\top}\Sigma_{LL}^{\dagger} \Sigma_{L}\right)\).

**Definition G.5**.: Fix a positive semi-definite matrix \(\Sigma:n\times n\), a positive integer \(t\in\mathbb{N}\), and any \(v^{*}\in\mathbb{R}^{n}\). Define the \(t\)_-submodularity ratio_ of \(\Sigma\) with respect to \(v^{*}\) by

\[\gamma_{t}(\Sigma,v^{*}):=\min_{L,S\subseteq[n]:|L|,|S|\leq t,L\cap S=\emptyset }\frac{(v^{*})^{\top}(\Sigma^{(L)})_{S}^{\top}(\Sigma^{(L)})_{S}v^{*}}{(v^{*}) ^{\top}(\Sigma^{(L)})_{S}^{\top}(\Sigma^{(L)})_{S}^{\top}(\Sigma^{(L)})_{S}^{ \top}(\Sigma^{(L)})_{S}v^{*}}.\]

In any \(t\)-sparse linear regression model with true regressor \(v^{*}\), when the above quantity \(\gamma:=\gamma_{t}(\Sigma,v^{*})\) is bounded away from zero, it can be shown that the standard Forward Regression algorithm finds some \(t\)-sparse estimate \(\hat{v}\in\mathbb{R}^{n}\) such that \(\left\lVert\hat{v}-v^{*}\right\rVert_{\Sigma}^{2}\leq e^{-\gamma}\left\lVert v^ {*}\right\rVert_{\Sigma}^{2}\) (see e.g. Theorem 3.2 in [9]; that result is for the model where the algorithm is given exact access to \(\langle v,v^{*}\rangle_{\Sigma}\) for any \(t\)-sparse \(v\in\mathbb{R}^{n}\), but analogous finite-sample bounds can be obtained with \(O(\gamma^{-O(1)}t\log(n))\) samples by applying the theorem to the empirical covariance matrix and using concentration of \(t\times t\) submatrices). A similar guarantee is also known for Orthogonal Matching Pursuit (Theorem 3.7 in [9]).

Once again, it is simple to show that the standard basis is a good dictionary for matrices with a large submodularity ratio.

**Fact G.6**.: _Let \(n,t\in\mathbb{N}\). For any positive semi-definite \(\Sigma:n\times n\) with \(\gamma:=\min_{v^{*}\in\mathbb{R}^{n}\cap B_{0}(t)}\gamma_{t}(\Sigma,v^{*})\), it holds that \(\mathcal{N}_{t,\sqrt{\gamma/t}}(\Sigma)\leq n\)._

Proof.: We show that the standard basis is a \((t,\gamma/t)\)-dictionary for \(\Sigma\). Without loss of generality assume that \(\Sigma_{ii}=1\) for all \(i\in[n]\). Then \(\Sigma^{(\emptyset)}=\Sigma\). Fix any \(t\)-sparse \(v^{*}\in\mathbb{R}^{n}\). Setting \(S:=\operatorname{supp}(v^{*})\), we have that

\[\sum_{i\in S}\langle e_{i},v^{*}\rangle_{\Sigma}^{2}=(v^{*})^{\top}\Sigma_{S}^ {\top}\Sigma_{S}v^{*}\geq\gamma(v^{*})^{\top}\Sigma_{S}^{\top}(\Sigma_{SS})^{ \dagger}\Sigma_{S}v^{*}=\gamma\left\lVert v^{*}\right\rVert_{\Sigma}^{2}\]

where the inequality is by definition of \(\gamma\), and the final equality uses that \(\Sigma_{S}v^{*}=\Sigma_{SS}(v^{*})_{S}\) (since \(v^{*}\) is supported on \(S\)). It follows that \(\max_{i\in S}\langle e_{i},v^{*}\rangle_{\Sigma}^{2}\geq(\gamma/t)\left\lVert v ^{*}\right\rVert_{\Sigma}^{2}\). Since \(\left\lVert e_{i}\right\rVert_{\Sigma}=1\) for all \(i\), we conclude that

\[\max_{i\in[n]}\frac{\left|\langle e_{i},v^{*}\rangle_{\Sigma}\right|}{\left\lVert e _{i}\right\rVert_{\Sigma}\left\lVert v^{*}\right\rVert_{\Sigma}}\geq\sqrt{ \frac{\gamma}{t}}\]

as claimed. 

### Sparse preconditioning

Recent work [23] showed that if \(\Sigma:n\times n\) is a positive definite matrix and the support of \(\Theta:=\Sigma^{-1}\) is the adjacency matrix of a graph with low _treewidth_, then there is a polynomial-time, sample-efficient algorithm for sparse linear regression with covariates drawn from \(N(0,\Sigma)\). The key to this result was a proof that such covariance matrices are _sparsely preconditioner_: i.e., there is a matrix \(S:n\times n\) such that \(\Sigma=SS^{\top}\) and \(S\) has sparse rows. We claim that this property also immediately enables succinct dictionaries.

Concretely, suppose that \(S\) has \(s\)-sparse rows. By a change-of-basis argument, any \(t\)-sparse vector in the standard basis is \(st\)-sparse in the basis \(\{(S^{\top})_{1}^{-1},\ldots,(S^{\top})_{n}^{-1}\}\). Moreover these vectors are orthonormal under \(\Sigma\). Thus, by the same argument as for Fact A.4, it's easy to see that \(\{(S^{\top})_{1}^{-1},\ldots,(S^{\top})_{n}^{-1}\}\) is a \((t,1/\sqrt{st})\)-dictionary for \(\Sigma\).

[MISSING_PAGE_FAIL:45]