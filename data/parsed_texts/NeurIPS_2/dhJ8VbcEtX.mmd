# _AQuA_: A Benchmarking Tool for Label Quality Assessment

Mononito Goswami\({}^{*}\), Vedant Sanil, Arjun Choudhry\({}^{\dagger}\),

**Arvind Srinivasan\({}^{\dagger}\)**, **Chalisa Udompanyawit**, **Artur Dubrawski**

Auton Lab, School of Computer Science

Carnegie Mellon University

{mgoswami, vsanil, arjuncho, arvindsr, cudompan, awd}@cs.cmu.edu

{www.github.com/autonlab/aqua}

MG and VS contributed equally. MG is the corresponding author.AC and AS have equal contribution.

###### Abstract

Machine learning (ML) models are only as good as the data they are trained on. But recent studies have found datasets widely used to train and evaluate ML models, e.g. _ImageNet_, to have pervasive labeling errors. Erroneous labels on the train set hurt ML models' ability to generalize, and they impact evaluation and model selection using the test set. Consequently, learning in the presence of labeling errors is an active area of research, yet this field lacks a comprehensive benchmark to evaluate these methods. Most of these methods are evaluated on a few computer vision datasets with significant variance in the experimental protocols. With such a large pool of methods and inconsistent evaluation, it is also unclear how ML practitioners can choose the right models to assess label quality in their data. To this end, we propose a benchmarking environment AQuA to rigorously evaluate methods that enable machine learning in the presence of label noise. We also introduce a design space to delineate concrete design choices of label error detection models. We hope that our proposed design space and benchmark enable practitioners to choose the right tools to improve their label quality and that our benchmark enables objective and rigorous evaluation of machine learning tools facing mislabeled data.

## 1 Introduction

A lot of machine learning (ML) research is devoted to making efficient and effective use of available data to learn accurate, high-fidelity, and interpretable models, with little to no focus on the quality of the data they are trained and evaluated on. Nonetheless, it is widely recognized that ML models are only as good as the data they rely on, i.e., the quality of data imposes practical limits to what ML models can achieve. Not only are datasets used to train ML models; they also serve as benchmarks to measure the state-of-the-art and validate theoretical findings. Thus, high quality large labeled datasets are the cornerstone of progress in supervised machine learning. However, the data is rarely free of noise, which can both manifest in the features of the data (feature noise) and in labels that categorize them (label noise). Between feature and label noise, the former has been found to be much more harmful to machine learning models [1][2][3]. To make matters worse, label noise is prevalent in popular ML benchmarks. A recent study estimated an average of at least 3.3% label errors across 10 datasets commonly used for benchmarking computer vision, natural language, and audio classification algorithms [4]. Consequently, a growing body of research is devoted to understanding the harms of label noise and to developing techniques to identify and mitigate labeling errors.

In recent years, over **50** papers have been written on this topic, including **6** surveys, yet the literature lacks a comprehensive benchmark to evaluate the available methods. The evaluation of existing methods is lacking along the following dimensions:

**Arbitrary choice of datasets and limited data modalities.** To the best of our knowledge, relevant studies have used over **40** datasets (e.g., ImageNet [5]) and their variations (e.g., Imagenette [6], ImageNet-100 [7]) for evaluation, but mostly on computer vision related tasks, with less than **15** studies using text data, **7** using tabular data and only **1** paper using time-series data.

**Arbitrary choice of classification models.** The ultimate goal of identifying labeling errors is to learn a classification model using training data with clean labels. Much like the datasets, relevant studies have used over **47** different classification architectures (e.g., ResNet [8], MobileNet [9], ResNeXt [7], BERT [11], XLM-RoBERTa [12], etc) to measure the impact of label cleaning.

**Inconsistent evaluation protocols and metrics.** Different studies conduct different experiments to measure the efficacy of their proposed methods (e.g., the accuracy of the label cleaning method, or performance of the downstream model before and after label cleaning, etc.) and use various measures of success (e.g., high accuracy, \(F_{1}\)-score, or low error rate).

With such diversity and inconsistency in the way in which these methods are evaluated, it is hard to measure the state of the art. To bridge this gap, we propose the Annotation Quality Assessment, AQuA, the _first_ benchmark framework to evaluate machine learning methods in the presence of label noise (Fig. [1]. We also elucidate the design space for such models, with the hope that it will not only foster future research on detecting labeling errors, but also enable ML practitioners to choose the appropriate label cleaning tools for their specific data and tasks. We run a large-scale experiment (> **1000** unique experiments) and make several interesting observations, demonstrating AQuA's efficacy in benchmarking machine learning models in the presence of label noise.

## 2 Background and Problem Formulation

**Sources of labeling errors.** Labeling errors can arise from automated labeling processes such as crowd-sourcing [13], programmatic weak supervision [14], and human error (e.g., due to lack of expertise or low confidence in expert assessment) [16]. Errors may also stem from idiosyncrasies

Figure 1: _Overview of the AQuA benchmark framework._AQuA comprises of datasets from **4** modalities, **4** single-label and **3** multi-annotator label noise injection methods, **4** state-of-the-art label error detection models, classification models, and several evaluation metrics beyond metrics of predictive accuracy. We are in the process of integrating several fairness, generalization, and robustness metrics into AQuA. The red and blue arrows show two example experimental pipelines for image data and time-series data, respectively.

of the annotation procedure and the corresponding guidelines themselves [11]. Finally, existing labels may also become inconsistent with prevailing knowledge due to constantly evolving problem definitions and domain knowledge leading to concept drift.

**Impact of labeling errors.** At training time, labeling errors can cripple an ML model's ability to generalize and introduce undesirable biases in its hypothesis space [19, 20]. Mislabeled training data is especially problematic for over-parameterized deep neural networks, which can achieve zero training error even on randomly-assigned labels [20]. At test time, labeling errors can lead to noisy model evaluations and invalidate common model selection strategies. In safety-critical settings, models trained, evaluated, and selected using mislabeled data can be ineffective at best and can lead to disastrous outcomes at worst. Finally, recent studies in the context of fairness have shown that naively enforcing parity constraints based on noisy labels can harm groups that are unaffected by label noise [21][22].

**Problem formulation.** Due to the far-reaching consequences that labeling errors can have on model training and evaluation, the literature has attacked multiple different but related problems, for example: (1) _label error detection_, identify which data points have erroneous labels [23], (2) _label noise estimation_, estimate the proportion of data with noisy labels [25], (3) _label noise robust learning_, learn models robust to label noise [26][27], and (4) _noise transition matrix estimation_, estimate the parameters of the noisy label generation process [28].

In this work, we focus on the **label error detection problem**, because (a) it is the most _general_ of the above problem types, i.e., with knowledge of labeling errors, we can estimate the noise rate, parameters of the noise generation process and train ML models free from label noise, (b) it provides practitioners greater visibility of issues that plague their data, and (c) allows them to directly rectify these errors.

**Label error detection problem:** Assume a dataset \(\mathcal{D}^{*}=\{(\mathbf{x}_{i},y_{i}^{*})\}_{i=1}^{N}\in(\mathcal{X}, \mathcal{Y})\), where \(\mathbf{x}_{i}\) and \(y_{i}^{*}\) denote the features and labels, respectively. In practice, we do not have access to \(\mathcal{D}^{*}\), but instead observe a noisy dataset \(\mathcal{D}=\{(\mathbf{x}_{i},y_{i})\}_{i=1}^{N}\in(\mathcal{X},\mathcal{Y}^{ \prime}\!\!_expert_ who can be queried to relabel suspicious data points [29][30]. Some other methods assume access to data points called _anchor points_, which most certainly belong to a particular class [31][32]. The number of anchor points required is generally proportional to the number of classes, and quickly becomes prohibitive for multi-class classification problems, and in more complicated noise settings [33]. Finally, a vast majority of methods assume _access to classification models_, and primarily differ in their _number_ (model-free [34], one or multiple models [35][36][37][38]), _nature of access_ (prediction-only [23] versus access to logits [24], gradients [20], etc.), and _extent of pre-training_ (no pre-training [23][24] versus large-scale pre-training e.g. large language models [39]).

**What modeling assumptions can you make?** Different studies use different assumptions on _data_ (noise structure and clusterability), _heuristics_ (model self-confidence and perceptual uncertainty), and _modeling decisions_ (whether to explicitly model the transition matrix and multi-network training). Most studies in the literature explicitly assume some form of structure in the noise present in the data [23][34][20][28]. Most early studies assumed class-dependent noise, i.e., the likelihood of error is only dependent on the latent true class, not on the data [23][23]. There is growing interest in more realistic forms of noise where the probability of error also depends on the features of a data point (instance-dependent noise) [41][42]. To this end, some recent studies have shown promising results by leveraging natural notions of similarity between data points and their labels. For example, Zhu et al. [34] assume that examples with similar features should have similar labels.

Many studies treat a trained model's low confidence that a data point belongs to its observed label as a heuristic likelihood to identify labeling errors [24][23][23]. In a similar vein, a recent study used the loss of a pre-trained large language model on each data point to identify mislabeled examples [39]. When multi-annotator labels are available, as discussed before, some studies have also used them to model the perceptual uncertainty in the annotators to identify labeling errors.

Finally, studies differ in their modeling decisions. While some explicitly estimate a data structure called the noise transition matrix, which encodes the joint probability of latent true and observed noisy labels [23][33][27], others do not [24][30][14]. Finally, there is a body of work on label noise robust learning using multiple model instances either using knowledge distillation [33][36][44] or meta-learning [38][37]. The key idea is to use a cooperative game between models to identify labeling errors and ensure that the eventually deployed model only learns from clean data.

**What outputs do you want, and what would you do with them?** All labeling error detection models identify data points that are likely to be labeling errors. With knowledge of the potentially mislabeled data points, most studies simply remove them from consideration [23][24][23]. This strategy may be practical for large datasets, where only a small fraction of data is found to be mislabeled and domain experts are unavailable for supervision. We use this strategy by default in AQuA. A smaller number of methods predict the alternate class that the data point is most likely to belong to [38][34] and even provide explanations for their predictions [30][46]. CINCER [30] is one of the few methods which not only finds labeling errors but also identifies counter-examples in the training data to serve as explanations for its suspicion. Some studies use the label predicted by these models and perform loss re-weighting or correction to learn robust classification models [27][47][48]. When domain experts are available, some studies also leverage their insight to re-label mislabeled data point [30][29].

Figure 3: _Design space of labeling error detection models to delineate concrete design choices._

## 4 Benchmark Design

### Real-world, Popular Datasets, and Downstream Classification Models

**Datasets.**AQuA currently comprises of a collection of **17** popular real-world public datasets from **4** prevalent data modalities: _image_, _text_, _time-series_ and _tabular_. To evaluate label error detection models across various practical scenarios, we carefully choose datasets with diversity in the following characteristics: (1) _classification problems_ (_e.g._, sentiment classification vs. hate speech detection), (2) _number of classes_ (binary vs multi-class classification), (3) _relative prevalence of classes_ (e.g., skewed datasets like Credit Card Fraud [33] and balanced ones like IMDb [34]), (4) _sources of annotations_ (e.g., human vs rule-based annotation), and (5) _number of annotations per example_ (e.g., CIFAR-10N labeled by 3 annotators). Table [7] summarizes the key characteristics of datasets included as a part of AQuA. In particular, to make comparison with prior work easier while maintaining diversity across practical scenarios, we try to include datasets that have been used frequently by prior work (see usage in Table [7] and preprocess them in a manner consistent with those works. We do not use any data augmentation during training. App [A.3] provides detailed descriptions of the datasets.

**Classification models.** The ultimate goal of label cleaning is to train accurate downstream classifiers, but different studies use different classification models to measure the efficacy of their proposed label cleaning methods. To provide a level playing field for all cleaning methods, we include multiple classification model architectures for each data modality. Specifically, we include ResNet-18 [33], MobileNet [34] and FastViT-T8 [69] for image datasets, all-distilroberta-v1 [70, 71] and all-MiniLM-L6-v2 [72] for text datasets, ResNet-1D, PatchTST [73] and LSTM Fully Convolutional Network [74] for time-series datasets, and TabTransformer [73] and a Multi-Layer Perceptron for tabular datasets. While choosing classification models we prioritized _performant_ methods with (1) _different architectures_ and _inductive biases_, (2) ideally _pre-trained_ using different strategies, and (3) _previously-used_ either by label cleaning methods or task-relevant papers. App [A.4] and App [A.5] provide a detailed description of classification models and their hyperparameters, respectively.

### Advanced Label Error Detection Methods

AQuA provides easy-to-use Application Programmer Interfaces (Fig. 4) for **4** state-of-the-art label error detection methods, namely Area Under Margin ranking (AUM) [74], Confident Learning [73], Contrastive and Influent Counter Example Strategy (CINCER) [80], and Model-free Label Error Detection (SimiFeat) [34]. Below, we provide a brief overview of these methods and their key ideas.

**Area Under the Margin Ranking (AUM) [74].** Given noisy data and access to the logits of a deep learning model, AUM exploits differences in training dynamics of clean and mislabeled samples to identify labeling errors. The key idea is to identify data points that do not contribute to the generalization of a model as labeling errors by leveraging the delicate tension between the label of a data point (via memorization) and its predicted label (via gradient updates), measured as the margin between the logits of a sample's assigned class and its highest unassigned class.

\begin{table}
\begin{tabular}{l|l|c c c c c c} \hline \hline \multicolumn{1}{l|}{**Modality**} & **Dataset** & **\# Train Test** & **\# Annotator/sample** & **LabelScore** & **Classification Task** & **Sample Size** & **Usage** \\ \hline \multirow{3}{*}{Image} & CIFAR-1001 & 50K / 10K & 3 & Human annotation & Object & \(22\times 23\times 3\) & **50K** \\  & CIFAR-1001 & 0 / 10K & 47-63 & Human annotation & Object & \(32\times 32\times 3\) & **50K** \\  & CIFAR-1001 & 100K & 1 & N-labeled & Deep-labeled & ImageNet & \(20\times 256\times 10\times 1\) & **50K** \\  & NoisyCereb & 2867 / 3K & 1-3C & Human expert annotation & Pneumonia & \(102\times 102\times 1\) & **50K** \\ \hline \multirow{2}{*}{Text} & BMC [71] & 25K / 25K & 1 & Human annotation & Sentiment & - & **CINCER** \\  & TextNet [73] & 100K & 1 & Human annotation & Hate speech & - & - \\ \hline \multirow{6}{*}{Tablular} & Credit Card Fraud [33] & 248K & 1 & Human annotation & Credit card fraud & 28 & **65K** \\  & Adult [33] & 48K & 1 & Risk-based extraction & Salary & 14 & **60K** \\  & Day Benefit [33] & 18 & 1 & Vision system-based annotation & Brown variety & 17 & **100K** \\  & Car Evaluation [33] & 1K & 1 & Hierarchical decision model & CM condition & 6 & **60K** \\  & Monkown [33] & 8K & 1 & - & Mahonen ethnicity & 22 & **60K** \\  & COMPAN [33] & 6K & 1 & - & Recistituent & 28 & **60K** \\ \hline \multirow{6}{*}{Tinet} & Credit Card & 7K / 10K & 1 & Hierarchical human tree & \multirow{2}{*}{Comp cover} & \multirow{2}{*}{\(46\times 1\)} & \multirow{2}{*}{-} \\  & Ensemble-Object & 49K / 7K & 1 & Human annotation & & & \\  & ShuffleNet [33] & 23K / 4K & 1 & Human expert annotation & \multirow{2}{*}{Affidnessing} & \multirow{2}{*}{\(256\times 1\)} & \multirow{2}{*}{-} \\  & PathPoint & 7K / 3K & 1 & Human annotations & Handwritten digit & \(16\times 1\) & - \\ \cline{1-1}  & WhichCab & 21K & 1 & 1 & & & \\ \hline \hline \end{tabular}
\end{table}
Table 1: **Summary of datasets.**AQuA currently includes a variety of datasets for different classification problems, varying in the number of classes, sources of annotations, and data modalities. All datasets except those marked with \(\beta\) are multi-class.

**Confident Learning (**C**on) [23]. Given noisy data, confident learning estimates a data structure called _confident joint_, which is the joint probability distribution of observed noisy and latent true labels. The key idea is to leverage a model trained on held-out data drawn from the same (or similar) distribution to predict the probability that an example \(\mathbf{x}_{i}\) belongs to its observed label \(\mathbf{y}_{i}\). A low probability is then used as a heuristic-likelihood of \(\mathbf{y}_{i}\) being a label error. The confident joint can then be used to identify labeling errors and estimate the noise rate.

**Contrastive and Influent Counter Example Strategy (**C**ncer** or **C**in) [30]. CINCER treats the problem of identifying labeling errors as a sequential decision making problem where a domain expert can be queried to relabel suspicious examples. CINCER uses the same heuristic as AUM to identify labeling errors, but also identifies counter-examples in the data to serve as explanations of the model's suspicion.

**Model-free Label Error Detection (**SimiFeat) [34].** Unlike other methods, SimiFeat does not need a (pre-)trained model to identify labeling errors. Instead, it utilizes labels of the \(k\) nearest neighbors to identify labeling errors based on the _clusterability_ assumption, _i.e._ data points with similar features should have the same true label with high probability.

There are many methods to detect labeling errors, but we choose these methods as a _starting point_ because they are recent, state-of-the-art, and have different inputs and core assumptions. While all these methods have existing public implementations, through AQuA, our goal is to create a one-stop shop for using and evaluating open-source label error detection models.

### Evaluation

There is significant variance in the ways that label cleaning methods are evaluated. To rigorously, fairly, and systematically assess these models, we unify the breadth of experimental settings through the following three dimensions of evaluation.

**Supervision.** Identifying labeling errors in practice is an _unsupervised_ problem since we do not know which data points are mislabeled. Hence, evaluating these methods is a challenging endeavor. Most studies in the literature gather noise labels either from human experts (_human-in-the-loop evaluation_) or by introducing synthetic label noise by design (_synthetic label noise_).

In human-in-the-loop evaluation, one or more human experts are asked to independently assess the true labels of data points identified as having erroneous labels [39][4]. While this is a straightforward and precise evaluation method, it is in general unscalable, expensive, time-consuming, and limited to only measuring the _precision_ of models (and not _recall_), because the experts are typically only shown data points which a model considers erroneous.

A much more common and scalable way of evaluating these methods is to introduce various kinds of synthetic label noise and measure a model's ability to detect them. There are many ways of introducing label noise, but injected noise may not always be reflective of the true noise that occurs in natural datasets, and hence identifying realistic noise injection strategies is an active area of research [33][4][59][60][71]. Moreover, model evaluation may still be noisy because there may be mislabeled examples for which our pseudo-noise labels are negative (or _correctly labeled_).

**Hypotheses.** In general, existing studies evaluate two hypotheses: (1) _cleaning labels on the train set improves the performance of the downstream classifier on the test set_, and (2) _cleaning methods can accurately identify mislabeled data on the train set_. Hypothesis 1 is practical since the primary goal of identifying labeling errors is to train accurate and unbiased classifiers. However, appropriately regularized deep learning models are known to be naturally robust to some label noise. Hence, hypothesis 2 allows researchers to directly measure the efficacy of label cleaning techniques.

Figure 4: AQuA makes identifying label issues, and evaluating new and existing label error detection models simple.

**Measures of goodness.** Different studies use different measures of predictive accuracy. While some measure error rate [24], others report the accuracy [23] or ROC-AUC [29] of their classification models. Similarly, for their cleaning methods, some studies report the \(F_{1}\) score while others report the precision or recall [23].

**More gaps in evaluation.** In addition to the lack of consistency, we believe that the experimental settings in many studies are occasionally (1) _unrealistic_, e.g., adding label noise to more than half (sometimes up to \(80\%\)) of the data points [24][23]; and (2) _uni-dimensional_, e.g., reporting only one metric of predictive performance.

AQuA's design.** To enable a realistic, multi-faceted and holistic evaluation of label error detection models, we implement **7** popular label noise injection techniques and multiple metrics of predictive performance. Specifically, for single-label datasets, we implement asymmetric [24], class-dependent [76], instance-dependent [23], and uniform [76] noise, and for datasets with labels from multiple annotators, we implement dissenting label, dissenting worker, and crowd majority [29]. In terms of metrics of predictive accuracy, we implement \(F_{1}\), accuracy, (_weighted_) precision, recall, area under ROC curve (ROC-AUC), average precision (PR-AUC), and error rate. We are in the process of implementing some other metrics beyond predictive accuracy, such as generalization [78] and robustness [79] of models. Our hope is that AQuA's _config-driven_ design will allow non-technical users to integrate it into their labeling workflows and researchers to add new models, datasets, and evaluation pipelines seamlessly. Our choice of datasets and downstream classifiers ensures that the computational complexity of running experiments is not prohibitive. Finally, we make all code, pre-trained models, and experimental logs open-source to enable rigorous and fair evaluation of models.

## 5 Experiments, Results and Discussion

\begin{table}
\begin{tabular}{c|c c c|c c c|c c c|c c c|c c c} \hline \hline \multirow{2}{*}{**Datasets**} & \multicolumn{4}{c}{**Uniform**} & \multicolumn{4}{c}{**Asymmetric**} & \multicolumn{4}{c}{**Class-dependent**} & \multicolumn{4}{c}{**Instance-dependent**} \\ \cline{2-13}  & AMR & CIN & CON & SILP & AMR & CIN & CON & SILP & AMR & CIN & CON & SILP & AMR & CIN & CON & SILP \\ \hline CIFAR-10 & 73.3 & 74.1 & 45.6 & **76.7** & 74.3 & 70.8 & 47.7 & **75.5** & 93.5 & 80.5 & 42.6 & **93.6** & 68.0 & 69.9 & 44.8 & **70.9** \\ Clothing-100K & 70.0 & 70.0 & **76.6** & 76.5 & 74.2 & 68.4 & 73.6 & **75.7** & 76.3We conduct several experiments to support AQuA's design choices and demonstrate its utility in providing a comprehensive and holistic evaluation of machine learning models in the presence of label noise.

**Experimental Setup and Hyper-Parameter Tuning.** We run experiments for all combinations of cleaning methods (AUM (AUM), confident learning (CON), CINCER (CIN) and SimiFeat (SIM), including no label cleaning (NON), noise types (_asymmetric_, _class-dependent_, _instance-dependent_ and _uniform_); for four different noise rates (0%, 2%, 10% and 40%), for a total of **2400 unique experiments**. We conduct experiments using three distinct classification architectures for image and time-series data, and two different architectures for text and tabular data. To account for class imbalance in some datasets, we report the \(F_{1}\) weighted by the support of each class. Results for all other evaluation metrics can be found in App. A.8 We also adopt critical difference diagrams [30] to succinctly represent comparisons between multiple cleaning methods and other independent variables (e.g., data modality and noise type) on multiple datasets. These diagrams represent the average ranks of methods across datasets while grouping those with insignificant difference [30]. We tuned hyper-parameters of all the classification and cleaning methods till they performed reasonably well on average on all the datasets using hyper-parameter grids used by prior work and reported in App. A.9 Finally, all our experiments were carried out on a computing cluster, with a typical machine having 128 AMD EPYC 7502 CPUs, 503 GB of RAM, and 8 NVIDIA RTX A6000 GPUs.

**Research Questions.** We aim to answer the following research questions through our experiments:

* _Which is the best cleaning method in terms of (i) its ability to identify synthetically injected label noise, and (ii) performance of the downstream classifier trained its cleaned labels?_
* _Do the rankings of cleaning methods differ across different (i) types of synthetic label noise, (ii) data modalities, and (iii) evaluation metrics (weighted \(F_{1}\) versus accuracy)?_

### Insights from Large-scale Experiments using AQuA

Tables 3, 2 and Fig. 5 report results from all our experiments aggregated by noise rate, and downstream classification models. Below we highlight some of our key findings. Due to lack of space, we defer finer grained results to App. A.8

**Best cleaning method.** Overall, we found SimiFeat (SIM) [34] to be the best cleaning method in terms of its ability to identify synthetically injected label noise, closely followed by CINCER (CIN) [30] (Fig. 5, _i_)). However, these differences shrink when evaluating cleaning methods using the performance of the downstream model trained using their cleaned labels (Fig. 5, _ii_)). Confident learning (CON) [23] consistently performed the worst among all the evaluated methods.

\begin{table}
\begin{tabular}{l|c c c c c c c c c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Datasets**} & \multicolumn{3}{c|}{**No Noise**} & \multicolumn{3}{c|}{**Disaster**} & \multicolumn{3}{c|}{**Unilative**} & \multicolumn{3}{c|}{**Aggarative**} & \multicolumn{3}{c|}{**Class-dependent**} & \multicolumn{3}{c|}{**Disaster**} & \multicolumn{3}{c}{**Disaster**} & \multicolumn{3}{c}{**Disaster**} \\ \cline{2-15}  & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} \\ \hline CIFAR-10 & **34** & 74.1 & 73.0 & 48.0 & 73.5 & 63.2 & 62.6 & **66.0** & **36.0** & 68.4 & 85.1 & **63.2** & 58.3 & **48.1** & 71.0 & **73.2** & 0.3 & 46.2 & 67.5 & 57.7 & 60.2 & **62.2** & **54.0** & 56.8 \\ Cooking-100k & **96.

**Deep learning models are inherently robust to label noise.** Perhaps unsurprisingly, we found that most downstream classifiers were reasonably robust to synthetic label noise, as can be seen from the insignificant difference between the setting where datasets were not explicitly cleaned (NON), compared to when they were cleaned using SIM, CIN and AUM. These results also illustrate the importance of measuring both hypotheses (performance of cleaning methods versus downstream models) when evaluating the performance of ML models in the presence of label noise.

**Adding label noise can sometimes improve model performance.** In the context of class-dependent or uniform noise, label noise serves as regularization to prevent models from overfitting. This phenomenon is not specific any one modality, but happens for multiple modalities, datasets, and noise types too, for example Electric Devices (time-series) under uniform noise, MIT-BIH (time-series), and Dry Bean (tabular) for class-dependent noise, in Table 16. Moreover, deep learning optimization is highly non-convex, so adding some noise might help the model reach the global minima by traversing an alternative path within the loss landscape.

**Impact of AQuA's design choices.** We found that cleaning methods perform differently for different data modalities. For instance, all cleaning methods barring CON perform on par on image datasets _(iii)_, but on tabular data _(iv)_, AUM performs significantly worse than CIN and SIM. This may be due to a variety of reasons beyond cleaning methods: size and nature of datasets, inductive biases of downstream classifier, and the quality of feature representations [34]. We also observed that some types of label noise are easier to detect than others. For example, uniform noise and asymmetric noise were the easiest to detect, cleaning methods found it much hard to detect instance and class-dependent noise _(vi)_. Finally, we noticed differences in model rankings when measuring different evaluation metrics. As an example, the difference between CIN and AUM vanishes when we measure the accuracy _(v)_ of the cleaning methods instead of their weighted \(F_{1}\)_(i)_. These findings highlight the need to evaluate label error detection methods across multiple datasets from different modalities, noise types and evaluation metrics.

## 6 Conclusion and Future Work

We propose the first benchmark designed to rigorously evaluate machine learning models in the presence of label noise. We also elucidate the design space of these methods to not only enable ML practitioners to choose the right label cleaning tool for their data, but also foster academic research on the label noise problem. We demonstrate AQuA's utility by running large-scale experiments to clean several interesting findings. We believe that, as a benchmarking toolkit, AQuA would benefit from more cleaning methods, datasets, synthetic label noise injection strategies, and evaluation metrics.

Our short-term goals include experimenting with multi-annotator label noise, measuring the impact of feature noise on time-series and image data in comparison to label noise, incorporating several metrics for model generalization, robustness and fairness, and including audio datasets. While other types of noise are beyond the scope of this work, we believe that multi-annotator, multi-class multi-label, and noise in regression problems are exciting avenues of future work, and AQuA's modular design will enable researchers to experiment with both multi-annotator and multi-class multi-label classification problems easily. We restrict ourselves to multi-class but single-label classification (as opposed to multi-label classification).

We believe that future work on label error detection should address label issues in the multi-label classification and regression settings. We believe that our work on AQuA can both harness and facilitate the development of foundation models in the two ways: (1) foundation models can be used to identify labeling errors, without explicit supervision, and (2) methods within AQuA can be use to identify labeling errors which can affect foundation model pre-training and fine-tuning. We also believe that future work should

## 7 Limitations, Biases, and Social Impacts

We acknowledge the potential adverse impact of large-scale experimentation on the environment, but believe that our publicly accessible code and experimental findings can significantly reduce resource consumption for ML practitioners in this field. Label error detection models might perpetuate existing biases and impact the fairness of models. We included the Adult dataset, that is frequently used in the fairness literature, in AQuA, to evaluate the impact of label errors on the fairness of models. Wewould also like to acknowledge that our experiments were carried without extensive hyper-parameter tuning. Moreover, hyper-parameters for cleaning methods and downstream classifiers were chosen based on model performance on the observed training set and fixed throughout the training process. We futher discuss these design choices and their limitations in Appendix A.6

## Acknowledgments and Disclosure of Funding

We would like to thank Cherie Ho and Jack H. Good for their useful comments on initial drafts of the paper. This work was partially supported by the National Institutes of Health under awards R01HL141916, 1R01NS124642-01, and 1R01DK131586-01, and by the U.S. Army Research Office and the U.S. Army Futures Command under Contract No. W911NF-20-D-0002. The content of the information does not necessarily reflect the position or the policy of the government and no official endorsement should be inferred.

## References

* Frenay and Verleysen [2013] Benoit Frenay and Michel Verleysen. Classification in the presence of label noise: a survey. _IEEE transactions on neural networks and learning systems_, 25(5):845-869, 2013.
* Zhu and Wu [2004] Xingquan Zhu and Xindong Wu. Class noise vs. attribute noise: A quantitative study. _The Artificial Intelligence Review_, 22(3):177, 2004.
* Saez et al. [2014] Jose A Saez, Mikel Galar, Julian Luengo, and Francisco Herrera. Analyzing the presence of noise in multi-class problems: alleviating its influence with the one-vs-one decomposition. _Knowledge and information systems_, 38:179-206, 2014.
* Northcutt et al. [2021] Curtis G. Northcutt, Anish Athalye, and Jonas Mueller. Pervasive label errors in test sets destabilize machine learning benchmarks. In _Proceedings of the 35th Conference on Neural Information Processing Systems Track on Datasets and Benchmarks_, December 2021.
* Russakovsky et al. [2015] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. _International Journal of Computer Vision (IJCV)_, 115(3):211-252, 2015. doi: 10.1007/s11263-015-0816-y.
* Howard [2014] Jeremy Howard. imagenette. URL [https://github.com/fastai/imagenette/](https://github.com/fastai/imagenette/).
* Gao et al. [2022] Zhengqi Gao, Fan-Keng Sun, Mingran Yang, Sucheng Ren, Zikai Xiong, Marc Engeler, Antonio Burazer, Linda Willing, Luca Daniel, and Duane S Boning. Learning from multiple annotator noisy labels via sample-wise label fusion. In _Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXIV_, pages 407-422. Springer, 2022. ISBN 978-3-031-20053-3.
* He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 770-778, 2016.
* Howard et al. [2017] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. _CoRR_, abs/1704.04861, 2017. URL [http://arxiv.org/abs/1704.04861](http://arxiv.org/abs/1704.04861)
* Xie et al. [2017] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In _2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5987-5995, 2017. doi: 10.1109/CVPR.2017.634.
* Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding, 2019.

* Conneau et al. [2020] Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzman, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics_, pages 8440-8451, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL [https://aclanthology.org/2020.acl-main.747](https://aclanthology.org/2020.acl-main.747)
* Yuen et al. [2011] Man-Ching Yuen, Irwin King, and Kwong-Sak Leung. A survey of crowdsourcing systems. In _2011 IEEE third international conference on privacy, security, risk and trust and 2011 IEEE third international conference on social computing_, pages 766-773. IEEE, 2011.
* Zhang et al. [2022] Jieyu Zhang, Cheng-Yu Hsieh, Yue Yu, Chao Zhang, and Alexander Ratner. A survey on programmatic weak supervision. _arXiv preprint arXiv:2202.05433_, 2022.
* Goswami et al. [2021] Mononito Goswami, Benedikt Boecking, and Artur Dubrawski. Weak supervision for affordable modeling of electrocardiogram data. In _AMIA Annual Symposium Proceedings_, volume 2021, page 536. American Medical Informatics Association, 2021.
* Peterson et al. [2019] Joshua C Peterson, Ruziaridh M Battleday, Thomas L Griffiths, and Olga Russakovsky. Human uncertainty makes classification more robust. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 9617-9626, Los Alamitos, CA, USA, nov 2019. IEEE Computer Society. doi: 10.1109/ICCV.2019.00971. URL [https://doi.ieeecomputersociety.org/10.1109/ICCV.2019.00971](https://doi.ieeecomputersociety.org/10.1109/ICCV.2019.00971)
* Beyer et al. [2020] Lucas Beyer, Olivier J Henaff, Alexander Kolesnikov, Xiaohua Zhai, and Aaron van den Oord. Are we done with imagenet? _arXiv preprint arXiv:2006.07159_, 2020.
* Giacobbe et al. [2021] Daniele Roberto Giacobbe, Alessio Signori, Filippo Del Puente, Sara Mora, Luca Carmisciano, Federica Briano, Antonio Vena, Lorenzo Ball, Chiara Robba, Paolo Pelosi, et al. Early detection of sepsis with machine learning techniques: a brief clinical perspective. _Frontiers in medicine_, 8:617486, 2021.
* Arpit et al. [2017] Devansh Arpit, Stanislaw Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In _International conference on machine learning_, pages 233-242. PMLR, 2017.
* Zhang et al. [2021] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. _Communications of the ACM_, 64(3):107-115, 2021.
* Wang et al. [2021] Jialu Wang, Yang Liu, and Caleb Levy. Fair classification with group-dependent label noise. In _Proceedings of the 2021 ACM conference on fairness, accountability, and transparency_, pages 526-536, 2021.
* Wu et al. [2022] Songhua Wu, Mingming Gong, Bo Han, Yang Liu, and Tongliang Liu. Fair classification with instance-dependent label noise. In _Conference on Causal Learning and Reasoning_, pages 927-943. PMLR, 2022.
* Northcutt et al. [2021] Curtis Northcutt, Lu Jiang, and Isaac Chuang. Confident learning: Estimating uncertainty in dataset labels. _Journal of Artificial Intelligence Research_, 70:1373-1411, 2021.
* Pleiss et al. [2020] Geoff Pleiss, Tianyi Zhang, Ethan Elenberg, and Kilian Q. Weinberger. Identifying mislabeled data using the area under the margin ranking. In _Proceedings of the 34th International Conference on Neural Information Processing Systems_, NIPS'20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.
* Northcutt et al. [2017] Curtis G. Northcutt, Tailin Wu, and Isaac L. Chuang. Learning with confident examples: Rank pruning for robust classification with noisy labels. In _Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence_, UAI'17. AUAI Press, 2017. URL [http://auai.org/uai2017/proceedings/papers/35.pdf](http://auai.org/uai2017/proceedings/papers/35.pdf)* [26] Scott E. Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew Rabinovich. Training deep neural networks on noisy labels with bootstrapping. In Yoshua Bengio and Yann LeCun, editors, _3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings_, 2015. URL [http://arxiv.org/abs/1412.6596](http://arxiv.org/abs/1412.6596)
* [27] Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making deep neural networks robust to label noise: A loss correction approach. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1944-1952, 2017. doi: 10.1109/CVPR.2017.240.
* [28] Xiaobo Xia, Tongliang Liu, Nannan Wang, Bo Han, Chen Gong, Gang Niu, and Masashi Sugiyama. Are anchor points really indispensable in label-noise learning? _Advances in neural information processing systems_, 32, 2019.
* [29] Melanie Bernhardt, Daniel Coelho de Castro, Ryutaro Tanno, Anton Schwaighofer, Kerem Tezcan, Miguel Monteiro, Shruthi Bannur, Matthew Lungren, Aditya Nori, Ben Glocker, Javier Alvarez-Valle, and Ozan Oktay. Active label cleaning for improved dataset quality under resource constraints. _Nature Communications_, 13, 03 2022. doi: 10.1038/s41467-022-28818-3.
* [30] Stefano Teso, Andrea Bontempelli, Fausto Giunchiglia, and Andrea Passerini. Interactive label cleaning with example-based explanations. In _Neural Information Processing Systems_, 2021.
* [31] T. Liu and D. Tao. Classification with noisy labels by importance reweighting. _IEEE Transactions on Pattern Analysis &; Machine Intelligence_, 38(03):447-461, mar 2016. ISSN 1939-3539. doi: 10.1109/TPAMI.2015.2456899.
* [32] Clayton Scott. A Rate of Convergence for Mixture Proportion Estimation, with Application to Learning from Noisy Labels. In Guy Lebanon and S. V. N. Vishwanathan, editors, _Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics_, volume 38 of _Proceedings of Machine Learning Research_, pages 838-846, San Diego, California, USA, 09-12 May 2015. PMLR. URL [https://proceedings.mlr.press/v38/scott15.html](https://proceedings.mlr.press/v38/scott15.html)
* [33] Zhaowei Zhu, Yiwen Song, and Yang Liu. Clusterability as an alternative to anchor points when learning with noisy labels. In Marina Meila and Tong Zhang, editors, _Proceedings of the 38th International Conference on Machine Learning_, volume 139 of _Proceedings of Machine Learning Research_, pages 12912-12923. PMLR, 18-24 Jul 2021. URL [https://proceedings.mlr.press/v139/zhu21e.html](https://proceedings.mlr.press/v139/zhu21e.html)
* [34] Zhaowei Zhu, Zihao Dong, and Yang Liu. Detecting corrupted labels without training a model to predict. In _International Conference on Machine Learning_, pages 27412-27427. PMLR, 2022.
* [35] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. MentorNet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In Jennifer Dy and Andreas Krause, editors, _Proceedings of the 35th International Conference on Machine Learning_, volume 80 of _Proceedings of Machine Learning Research_, pages 2304-2313. PMLR, 10-15 Jul 2018. URL [https://proceedings.mlr.press/v80/jiang18c.html](https://proceedings.mlr.press/v80/jiang18c.html)
* [36] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor W. Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In _Proceedings of the 32nd International Conference on Neural Information Processing Systems_, NIPS'18, page 8536-8546, Red Hook, NY, USA, 2018. Curran Associates Inc.
* [37] Junnan Li, Yongkang Wong, Qi Zhao, and Mohan S. Kankanhalli. Learning to learn from noisy labeled data. In _2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 5046-5054, 2019. doi: 10.1109/CVPR.2019.00519.
* [38] Guoqing Zheng, Ahmed Hassan Awadallah, and Susan Dumais. Meta label correction for noisy label learning. _Proceedings of the AAAI Conference on Artificial Intelligence_, 35(12):11053-11061, May 2021. doi: 10.1609/aaai.v35i12.17319. URL [https://ojs.aaai.org/index.php/AAAI/article/view/17319](https://ojs.aaai.org/index.php/AAAI/article/view/17319)* Chong et al. [2022] Derek Chong, Jenny Hong, and Christopher Manning. Detecting label errors by using pre-trained language models. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing_, pages 9074-9091, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. URL [https://aclanthology.org/2022/emnlp-main.618](https://aclanthology.org/2022/emnlp-main.618)
* Vahdat [2017] Arash Vahdat. Toward robustness against label noise in training deep discriminative neural networks. In _Proceedings of the 31st International Conference on Neural Information Processing Systems_, NIPS'17, page 5601-5610, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN 9781510860964.
* Xiaobo et al. [2020] Xiaobo Xia, Tongliang Liu, Bo Han, Nannan Wang, Mingming Gong, Haifeng Liu, Gang Niu, Dacheng Tao, and Masashi Sugiyama. Part-dependent label noise: Towards instance-dependent label noise. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 7597-7610. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper_files/paper/2020/file/5607fe8879e4f269e88387e8cb30b7e-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/5607fe8879e4f269e88387e8cb30b7e-Paper.pdf)
* Cheng et al. [2021] Hao Cheng, Zhaowei Zhu, Xingyu Li, Yifei Gong, Xing Sun, and Yang Liu. Learning with instance-dependent label noise: A sample sieve approach. In _International Conference on Learning Representations_, 2021.
* Javadi et al. [2021] Golara Javadi, Samareh Samadi, Sharareh Bayat, Samira Sojoudi, Antonio Hurtado, Silvia Chang, Peter Black, Parvin Mousavi, and Purang Abolmaesumi. Characterizing the uncertainty of label noise in systematic ultrasound-guided prostate biopsy. In _2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)_, pages 424-428, 2021. doi: 10.1109/ISBI48211.2021.9433765.
* Yu et al. [2019] Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama. How does disagreement help generalization against label corruption? In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 7164-7173. PMLR, 09-15 Jun 2019. URL [https://proceedings.mlr.press/v97/yu19b.html](https://proceedings.mlr.press/v97/yu19b.html).
* Bahri et al. [2020] Dara Bahri, Heinrich Jiang, and Maya Gupta. Deep k-NN for noisy labels. In Hal Daume III and Aarti Singh, editors, _Proceedings of the 37th International Conference on Machine Learning_, volume 119 of _Proceedings of Machine Learning Research_, pages 540-550. PMLR, 13-18 Jul 2020. URL [https://proceedings.mlr.press/v119/bahri20a.html](https://proceedings.mlr.press/v119/bahri20a.html)
* Desmond et al. [2020] Michael Desmond, Catherine Finegan-Dollak, Jeff Boston, and Matt Arnold. Label noise in context. In _Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations_, pages 157-186, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-demos.21. URL [https://aclanthology.org/2020.acl-demos.21](https://aclanthology.org/2020.acl-demos.21)
* Ghosh et al. [2017] Aritra Ghosh, Himanshu Kumar, and P. S. Sastry. Robust loss functions under label noise for deep neural networks. _Proceedings of the AAAI Conference on Artificial Intelligence_, 31(1), Feb. 2017. doi: 10.1609/aaai.v31i1.10894. URL [https://ojs.aaai.org/index.php/AAAI/article/view/10894](https://ojs.aaai.org/index.php/AAAI/article/view/10894)
* Thulasidasan et al. [2019] Sunil Thulasidasan, Tanmoy Bhattacharya, Jeff Bilmes, Gopinath Chennupati, and Jamal Mohd-Yusof. Combating label noise in deep learning using abstention. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, _Proceedings of the 36th International Conference on Machine Learning_, volume 97 of _Proceedings of Machine Learning Research_, pages 6234-6243. PMLR, 09-15 Jun 2019. URL [https://proceedings.mlr.press/v97/thulasidasan19a.html](https://proceedings.mlr.press/v97/thulasidasan19a.html)
* Wei et al. [2022] Jiaheng Wei, Zhaowei Zhu, Hao Cheng, Tongliang Liu, Gang Niu, and Yang Liu. Learning with noisy labels revisited: A study using real-world human annotations. In _International Conference on Learning Representations_, 2022. URL [https://openreview.net/forum?Id=TBWAGPLJZQm](https://openreview.net/forum?Id=TBWAGPLJZQm)
* Kuan and Mueller [2022] Johnson Kuan and Jonas Mueller. Model-agnostic label quality scoring to detect real-world label errors. In _ICML DataPerf Workshop_, 2022.

* Xiao et al. [2015] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy labeled data for image classification. In _2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 2691-2699, 2015. doi: 10.1109/CVPR.2015.7298885.
* Shih et al. [2019] George Shih, Carol C. Wu, Safwan S. Halabi, Marc D. Kohli, Luciano M. Prevedello, Tessa S. Cook, Arjun Sharma, Judith K. Amorosa, Veronica Arteaga, Maya Galperin-Aizenberg, Ritu R. Gill, Myrna C.B. Godoy, Stephen Hobbs, Jean Jeudy, Archana Laroia, Palmi N. Shah, Dharshan Vummidi, Kavitha Yaddanapudi, and Anouk Stein. Augmenting the national institutes of health chest radiograph dataset with expert annotations of possible pneumonia. _Radiology: Artificial Intelligence_, 1(1):e180041, 2019. doi: 10.1148/ryai.2019180041. URL [https://doi.org/10.1148/ryai.2019180041](https://doi.org/10.1148/ryai.2019180041)}.
* Maas et al. [2011] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_, pages 142-150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL [http://www.aclweb.org/anthology/P11-1015](http://www.aclweb.org/anthology/P11-1015)
* Barbieri et al. [2020] Francesco Barbieri, Jose Camacho-Collados, Luis Espinosa Anke, and Leonardo Neves. TweetEval: Unified benchmark and comparative evaluation for tweet classification. In _Findings of the Association for Computational Linguistics: EMNLP 2020_, pages 1644-1650, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.148. URL [https://aclanthology.org/2020.findings-emnlp.148](https://aclanthology.org/2020.findings-emnlp.148)
* Pozzolo et al. [2015] Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson, and Gianluca Bontempi. Calibrating probability with undersampling for unbalanced classification. In _2015 IEEE Symposium Series on Computational Intelligence_, pages 159-166, 2015. doi: 10.1109/SSCI.2015.33.
* Yue and Jha [2022] Chang Yue and Niraj K. Jha. Ctrl: Clustering training losses for label error detection, 2022.
* Salekshahrezaee et al. [2021] Zahra Salekshahrezaee, Jeffrey Leevy, and Taghi Khoshgoftaar. A reconstruction error-based framework for label noise detection. _Journal of Big Data_, 8, 04 2021. doi: 10.1186/s40537-021-00447-5.
* Dua and Graff [2017] Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL [http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)}
* Koklu and Ozkan [2020] Murat Koklu and Ilker Ali Ozkan. Multiclass classification of dry beans using computer vision and machine learning techniques. _Computers and Electronics in Agriculture_, 174:105507, 2020. ISSN 0168-1699. doi: [https://doi.org/10.1016/j.compag.2020.105507](https://doi.org/10.1016/j.compag.2020.105507). URL [https://www.sciencedirect.com/science/article/pii/S0168169919311573http://dx.doi.org/10.1016/j.compag.2020.105507](https://www.sciencedirect.com/science/article/pii/S0168169919311573http://dx.doi.org/10.1016/j.compag.2020.105507).
* Bohanec and Rajkovi [1988] Marko Bohanec and Vladislav Rajkovi. Knowledge acquisition and explanation for multi-attribute decision making. In _8th Int'l Workshop on Expert Systems and their Applications_, 1988.
* Grill et al. [2020] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi Azar, Bilal Piot, Koray Kavukcuoglu, Remi Munos, and Michal Valko. Bootstrap your own latent a new approach to self-supervised learning. In _Proceedings of the 34th International Conference on Neural Information Processing Systems_, NIPS'20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.
* Schlimmer [1987] J.S. Schlimmer. _Concept Acquisition Through Representational Adjustment (Technical Report 87-19)_. PhD thesis, Department of Information and Computer Science, University of California, Irvine, 1987.
* Larson et al. [2016] Lauren Kirchner Jeff Larson, Surya Mattu and Julia Angwin. How we analyzed the compas recidivism algorithm, 05 2016.
* Tan et al. [2017] Chang Wei Tan, Geoffrey I. Webb, and Francois Petitjean. Indexing and classifying gigabytes of time series under time warping. In _SIAM International Conference on Data Mining_, pages 1-10, 2017.

- IDEAL 2011_, pages 403-412, Berlin, Heidelberg, 2011. Springer Berlin Heidelberg. ISBN 978-3-642-23878-9.
* Moody and Mark [2001] George B. Moody and Roger G. Mark. The impact of the MIT-BIH arrhythmia database. _IEEE Engineering in Medicine and Biology Magazine_, 20:45-50, 2001.
* Alimoglu and Alpaydin [1997] F. Alimoglu and E. Alpaydin. Combining multiple representations and classifiers for pen-based handwritten digit recognition. In _Proceedings of the Fourth International Conference on Document Analysis and Recognition_, volume 2, pages 637-640 vol.2, 1997. doi: 10.1109/ICDAR.1997.620583.
* Bagnall et al. [2018] Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom, Paul Southam, and Eamonn Keogh. The UEA multivariate time series classification archive, 2018, 2018.
* Vasu et al. [2023] Pavan Kumar Anasosalu Vasu, James Gabriel, Jeff Zhu, Oncel Tuzel, and Anurag Ranjan. Fastvit: A fast hybrid vision transformer using structural reparameterization, 2023.
* Zhuang et al. [2021] Liu Zhuang, Lin Wayne, Shi Ya, and Zhao Jun. A robustly optimized BERT pre-training approach with post-training. In _Proceedings of the 20th Chinese National Conference on Computational Linguistics_, pages 1218-1227, Huhhot, China, August 2021. Chinese Information Processing Society of China. URL [https://aclanthology.org/2021.ccl-1.108](https://aclanthology.org/2021.ccl-1.108).
* Sanh et al. [2020] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter, 2020.
* Wang et al. [2020] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. In _Proceedings of the 34th International Conference on Neural Information Processing Systems_, NIPS'20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.
* Nie et al. [2023] Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. A time series is worth 64 words: Long-term forecasting with transformers. In _International Conference on Learning Representations_, 2023.
* Karim et al. [2018] Fazle Karim, Somshubra Majumdar, Houshang Darabi, and Shun Chen. LSTM fully convolutional networks for time series classification. _IEEE Access_, 6:1662-1669, 2018. doi: 10.1109/ACCESS.2017.2779939.
* Huang et al. [2020] Xin Huang, Ashish Khetan, Milan Cvitkovic, and Zohar Karnin. Tabtransformer: Tabular data modeling using contextual embeddings, 2020.
* Algan and Ulusoy [2020] Gorkem Algan and Ilkay Ulusoy. Label noise types and their effects on deep learning. _arXiv preprint arXiv:2003.10471_, 2020.
* Jiang et al. [2020] Lu Jiang, Di Huang, Mason Liu, and Weilong Yang. Beyond synthetic noise: Deep learning on controlled noisy labels. In _International conference on machine learning_, pages 4804-4815. PMLR, 2020.
* Jiang et al. [2020] Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic generalization measures and where to find them. In _8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020_. OpenReview.net, 2020. URL [https://openreview.net/forum?id=SJgIPJBFvVH](https://openreview.net/forum?id=SJgIPJBFvVH)
* Gisolfi [2021] Nicholas Gisolfi. _Model-Centric Verification of Artificial Intelligence_. PhD thesis, Carnegie Mellon University, 2021.
* Demsar [2006] Janez Demsar. Statistical comparisons of classifiers over multiple data sets. _The Journal of Machine learning research_, 7:1-30, 2006.

* Benavoli et al. [2016] Alessio Benavoli, Giorgio Corani, and Francesca Mangili. Should we really use post-hoc tests based on mean-ranks? _The Journal of Machine Learning Research_, 17(1):152-161, 2016.
* Bagnall et al. [2017] Anthony Bagnall, Jason Lines, Aaron Bostrom, James Large, and Eamonn Keogh. The great time series classification bake off: a review and experimental evaluation of recent algorithmic advances. _Data mining and knowledge discovery_, 31:606-660, 2017.
* Hedderich et al. [2021] Michael A. Hedderich, D. Zhu, and Dietrich Klakow. Analysing the noise model error for realistic noisy label data. In _AAAI Conference on Artificial Intelligence_, 2021.
* Zhu et al. [2022] Dawei Zhu, Michael A. Hedderich, Fangzhou Zhai, David Adelani, and Dietrich Klakow. Is BERT robust to label noise? a study on learning with noisy labels in text classification. In _Proceedings of the Third Workshop on Insights from Negative Results in NLP_, pages 62-67, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.insights-1.8. URL [https://aclanthology.org/2022.insights-1.8](https://aclanthology.org/2022.insights-1.8)
* Wang et al. [2019] Y. Wang, X. Ma, Z. Chen, Y. Luo, J. Yi, and J. Bailey. Symmetric cross entropy for robust learning with noisy labels. In _2019 IEEE/CVF International Conference on Computer Vision (ICCV)_, pages 322-330, Los Alamitos, CA, USA, nov 2019. IEEE Computer Society. doi: 10.1109/ICCV.2019.00041. URL [https://doi.ieeecomputersociety.org/10.1109/ICCV.2019.00041](https://doi.ieeecomputersociety.org/10.1109/ICCV.2019.00041)
* Gulshan et al. [2016] Varun Gulshan, Lily Peng, Marc Coram, Martin C. Stumpe, Derek Wu, Arunachalam Narayanaswamy, Subhashini Venugopalan, Kasumi Widner, Tom Madams, Jorge Cuadros, Ramasamy Kim, Rajiv Raman, Philip C. Nelson, Jessica L. Mega, and Dale R. Webster. Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs. _JAMA_, 316(22):2402-2410, 12 2016. ISSN 0098-7484. doi: 10.1001/jama.2016.17216. URL [https://doi.org/10.1001/jama.2016.17216](https://doi.org/10.1001/jama.2016.17216)
* Zhang et al. [2022] Jieyu Zhang, Cheng-Yu Hsieh, Yue Yu, Chao Zhang, and Alexander Ratner. A survey on programmatic weak supervision, 2022.
* Goswami et al. [2022] Mononito Goswami, Benedikt Boecking, and Artur Dubrawski. Weak supervision for affordable modeling of electrocardiogram data, 2022.
* Fawaz et al. [2019] Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber, Lhassame Idoumghar, and Pierre-Alain Muller. Deep learning for time series classification: a review. _Data mining and knowledge discovery_, 33(4):917-963, 2019.
* Friedman [1940] Milton Friedman. A comparison of alternative tests of significance for the problem of m rankings. _The Annals of Mathematical Statistics_, 11(1):86-92, 1940. ISSN 00034851. URL [http://www.jstor.org/stable/2235971](http://www.jstor.org/stable/2235971)
* Wilcoxon [1945] Frank Wilcoxon. Individual comparisons by ranking methods. _Biometrics Bulletin_, 1(6):80-83, 1945. ISSN 00994987. URL [http://www.jstor.org/stable/3001968](http://www.jstor.org/stable/3001968)
* Holm [1979] Sture Holm. A simple sequentially rejective multiple test procedure. _Scandinavian Journal of Statistics_, 6(2):65-70, 1979. ISSN 03036898, 14679469. URL [http://www.jstor.org/stable/4615733](http://www.jstor.org/stable/4615733)