# Geometric-Averaged Preference Optimization

for Soft Preference Labels

 Hiroki Furuta\({}^{1,2}\)1  Kuang-Huei Lee\({}^{1}\)  Shixiang Shane Gu\({}^{1}\)  Yutaka Matsuo\({}^{2}\)

**Aleksandra Faust\({}^{1}\)  Heiga Zen\({}^{1}\)  Izzeddin Gur\({}^{1}\) \({}^{1}\)**

\({}^{1}\)Google DeepMind \({}^{2}\)The University of Tokyo

furuta@weblab.t.u-tokyo.ac.jp

Footnote 1: Work done as a Student Researcher at Google.

###### Abstract

Many algorithms for aligning LLMs with human preferences assume that human preferences are binary and deterministic. However, human preferences can vary across individuals, and therefore should be represented distributionally. In this work, we introduce the distributional _soft preference labels_ and improve Direct Preference Optimization (DPO) with a weighted geometric average of the LLM output likelihood in the loss function. This approach adjusts the scale of learning loss based on the soft labels such that the loss would approach zero when the responses are closer to equally preferred. This simple modification can be easily applied to any DPO-based methods and mitigate over-optimization and objective mismatch, which prior works suffer from. Our experiments simulate the soft preference labels with AI feedback from LLMs and demonstrate that geometric averaging consistently improves performance on standard benchmarks for alignment research. In particular, we observe more preferable responses than binary labels and significant improvements where modestly-confident labels are in the majority.

## 1 Introduction

Large Language Models (LLMs) [1, 7, 33] capture a wide range of behaviors and values from training data. However, we would usually prefer these models to focus on useful and safe expressions and abide by social norms. To solve these problems, preference optimization approaches have been popular, either way through reinforcement learning from feedback (RLHF) [12, 34, 4] or direct preference optimization (DPO) methods [41, 52]. These methods usually finetune supervised models on preference data and labels generated by human raters with a wide variety of priorities, backgrounds, knowledge, and skill sets. Nevertheless, existing RLHF and direct preference optimization methods usually assume binary preferences, which ignore the subtle relationship and amplify the bias in the preference labels.

To address this issue, we introduce the concept of distributional _soft preference labels_ and improve DPO and its algorithmic families by incorporating a weighted geometric average of LLM output likelihood into the loss function. This approach adjusts the scale of learning loss based on the soft labels and effectively minimizes the loss when presented with equally preferred responses.

In the experiments, we simulate the soft preference labels with AI feedback from LLMs [4, 24] and show that soft preference labels and weighted geometric averaging achieve consistent improvement to the baselines on popular benchmarks for the alignment research literature, such as Reddit TL;DR [49], and Anthropic Helpful and Harmless [3], as well as original natural language planning dataset based on Plasma [6]. In particular, our results highlight that the proposed methods significantly improve the performance with the data dominated by modestly-confident labels, while conservative DPO(cDPO) [30], a method leveraging soft labels via linear interpolation of objectives, is stuck to suboptimal performances there. When the models are trained with rich modestly-confident labels, the responses are preferable to those from the models trained with binary labels biased to high-confidence regions. The performance on preference label classification also reveals that cDPO struggles with objective mismatch between the text generation and preference modeling and the weighted geometric averaging could successfully balance both.

Our primary contributions are:

* We introduce _soft preference labels_, which can reflect the distributional preference and the fine-grained relationship between the response pairs (Section 2.1). Soft preference labels contribute to mitigating over-optimization issues (Section 5.3) and aligning the models to more preferable responses than binary labels (Section 5.1).
* We propose the weighted geometric averaging of the output likelihood in the loss function. This can be applied to a family of any algorithms derived from DPO (Section 3).
* We point out the objective mismatch between text generation and preference modeling. The better preference accuracy from DPO-style objectives does not ensure better alignment, which conservative DPO suffers from and our geometric averaging can resolve (Section 5.2).

## 2 Preliminaries

We denote \(x\in\mathcal{X}\) as a text prompt from the set of prompts \(\mathcal{X}\), \(y\in\mathcal{Y}\) as an answer corresponding to the prompts from the set of possible candidates \(\mathcal{Y}\), and \(\pi(y\mid x)\) as a LLM (i.e. policy). We use \(y_{1}\succ y_{2}\) to indicate that \(y_{1}\) is more preferable than \(y_{2}\), and denote a dataset of the paired preference as \(\mathcal{D}=\{(x^{(n)},y_{1}^{(n)},y_{2}^{(n)})\}_{n=1}^{N}\). We assume that \(y_{1}\succ y_{2}\) always holds (\(y_{1}\) is always preferred or equal) in this paper unless specified otherwise.

In the RLHF pipeline, we typically go through three phases, such as supervised finetuning (SFT), reward model training, and RL-finetuning [34; 69]. The SFT phase is maximum likelihood training of pre-trained LLMs on downstream tasks, which results in an initial model or reference model \(\pi_{\text{ref}}\) for the later RL-finetuning. For the reward modeling, the Bradley-Terry model [5] is often assumed as underlying modeling for the oracle human preference such as

\[p^{*}(y_{1}\succ y_{2}\mid x)=\frac{\exp(r^{*}(x,y_{1}))}{\exp(r^{*}(x,y_{1}))+ \exp(r^{*}(x,y_{2}))}=\sigma(r^{*}(x,y_{1})-r^{*}(x,y_{2})), \tag{1}\]

where \(r^{*}(x,y)\) is a true reward function and \(\sigma(\cdot)\) is a sigmoid function. Following this assumption, the parameterized reward function \(r_{\psi}\) is initialized with a supervisedly-finetuned LLM \(\pi_{\text{ref}}\) and trained with negative log-likelihood loss: \(\min_{\psi}-\mathbb{E}\left[\log\sigma(r_{\psi}(x,y_{1})-r_{\psi}(x,y_{2}))\right]\). RL-finetuning phase leverages the learned reward to update the LLM \(\pi_{\theta}\) by optimizing the following objective [34; 69],

\[\max_{\theta}\ \mathbb{E}_{x\sim\mathcal{D},y\sim\pi_{\theta}(y\mid x)}\left[r_ {\psi}(x,y)\right]-\beta D_{\text{KL}}(\pi_{\theta}(y\mid x)\mid\mid\pi_{ \text{ref}}(y\mid x)), \tag{2}\]

where \(\beta>0\) is a coefficient to control the KL-divergence regularization. Online RL approaches, such as PPO [45], are often used to maximize Equation 2, but they are usually computational inefficiency and require a complex pipeline in practice. In contrast, offline preference optimization approaches, such as DPO [41], are relatively simpler and lightweight in terms of implementation.

### Soft Preference Labels

While a reward model is often trained with binary preferences, we can usually assume distributional _soft_ feedback via majority voting among the human raters or AI feedback with scoring [24] (e.g. \(y_{1}\) is better than \(y_{2}\) at a 70% chance). With soft preference labels, we can still easily recover the binary preference with a threshold.

We assume that the binary preference labels, \(l(y_{1}\succ y_{2}|x)=1\), are sampled from the Bradley-Terry model preference distribution with the parameter \(p^{*}(y_{1}\succ y_{2}|x)\). We define soft preference labels as estimates of the true preference probability:

\[\hat{p}_{x,y_{1},y_{2}}:=\hat{p}(y_{1}\succ y_{2}|x)\approx p^{*}(y_{1}\succ y _{2}|x). \tag{3}\]

We denote \(\hat{p}_{x,y_{1},y_{2}}\) as \(\hat{p}\in[0.5,1.0]\) for simplicity in the later sections. For instance, we can estimate this via Monte Carlo sampling such as \(\hat{p}=\frac{1}{M}\sum_{i=1}^{M}l_{i}\) where \(l_{i}\in\{0,1\}\) is a sampled binary label,which is done via majority voting among \(M\) people in practice. Because soft preference labels reflect fine-grained relationships between the responses, they may contribute to aligning the models to more preferable responses than binary labels. Alternatively, we can also estimate the soft preference directly via Bradley-Terry models with some reward function. This direct estimation is often adopted in AI feedback with scoring (see Section 4.1 for further details) or the cases with multiple reward models.

The sampled binary preference may sometimes flip with probability \(\epsilon\) (i.e. label noise [11; 27; 30]). If the degree of label noise is known, we may consider the expectation over the noise such as: \(\hat{p}=(1-\frac{1}{M}\sum_{i=1}^{M}\epsilon_{i})\frac{1}{M}\sum_{i=1}^{M}l_{i }+\frac{1}{M}\sum_{i=1}^{M}\epsilon_{i}\frac{1}{M}\sum_{i=1}^{M}(1-l_{i})\), or we may ignore the noise when \(\epsilon_{i}\) is small and \(M\) is sufficiently large.

### Direct Preference Optimization and Related Methods

Let's start with a brief review of DPO and the variants derived from it, such as conservative DPO, IPO, and ROPO. DPO maximizes the estimate of preference probability under the Bradley-Terry model, \(p_{\theta}(y_{1}\succ y_{2}\mid x)=\sigma(r_{\theta}(x,y_{1})-r_{\theta}(x,y_ {2}))\), by parameterizing reward models with the policy model \(\pi_{\theta}\) itself, which comes from the following relationship in the constraint Lagrangian of RLHF objective (Equation 2),

\[r_{\theta}(x,y)=\beta\log\frac{\pi_{\theta}(y\mid x)}{\pi_{\text{ref}}(y\mid x )}+\beta\log Z(x), \tag{4}\]

where \(Z(x)=\sum_{y}\pi_{\text{ref}}(y\mid x)\exp\left(\frac{1}{\beta}r(x,y)\right)\) is the partition function. Substituting Equation 4 into \(\log p_{\theta}(y_{1}\succ y_{2}\mid x)\), the following objective is derived:

\[\mathcal{L}_{\text{DPO}}(\pi_{\theta},\pi_{\text{ref}}) =-\mathbb{E}_{(x,y_{1},y_{2})\sim\mathcal{D}}\left[\log\sigma \left(h_{\theta}(x,y_{1},y_{2})\right)\right] \tag{5}\] \[=-\mathbb{E}_{(x,y_{1},y_{2})\sim\mathcal{D}}\left[\log\sigma \left(\beta\log\frac{\pi_{\theta}(y_{1}\mid x)\pi_{\text{ref}}(y_{2}\mid x)}{ \pi_{\text{ref}}(y_{1}\mid x)\pi_{\theta}(y_{2}\mid x)}\right)\right].\]

Note that we define the reward difference function as \(h_{\theta}(x,y_{1},y_{2}):=r_{\theta}(x,y_{1})-r_{\theta}(x,y_{2})\).

Conservative Direct Preference OptimizationConservative DPO (cDPO) [30] is the most representative work that incorporates soft labels. cDPO smooths the objective functions with soft preference labels via linear interpolation, such as

\[\mathcal{L}_{\text{cDPO}}(\pi_{\theta},\pi_{\text{ref}}) =-\mathbb{E}_{(x,y_{1},y_{2},\hat{p})\sim\mathcal{D}}\left[\hat{p }\log\sigma\left(h_{\theta}(x,y_{1},y_{2})\right)+(1-\hat{p})\log\sigma\left( h_{\theta}(x,y_{2},y_{1})\right)\right] \tag{6}\] \[=-\mathbb{E}_{\mathcal{D}}\left[\hat{p}\log\sigma\left(h_{\theta }(x,y_{1},y_{2})\right)+(1-\hat{p})\log\left(1-\sigma\left(h_{\theta}(x,y_{1}, y_{2})\right)\right)\right],\]

where the later term is the DPO loss under flipped labels (i.e. \(y_{2}\succ y_{1}\)). Moreover, prior works incorporating an extra reward model \(r_{\psi}\) to DPO objective have also adopted this formulation [9; 21], by replacing \(\hat{p}\) into \(\sigma(r_{\psi}(x,y_{1})-r_{\psi}(x,y_{2}))\).

Identity Preference OptimizationAssuming the Bradley-Terry model as an underlying preference modeling causes over-optimization issues in DPO [2; 52]. To mitigate this problem, IPO [2] has been introduced by replacing reward maximization in Equation 2 with preference distribution maximization. The objective of IPO can be written as,

\[\mathcal{L}_{\text{IPO}}(\pi_{\theta},\pi_{\text{ref}})=\mathbb{E}_{(x,y_{1}, y_{2})\sim\mathcal{D}}\left[\left(h_{\theta}(x,y_{1},y_{2})-\frac{1}{2\beta} \right)^{2}\right], \tag{7}\]

where \(\beta>0\) is a regularization hyper-parameter. Similar to cDPO, we can also introduce conservative IPO (cIPO) [2; 27], which results in

\[\mathcal{L}_{\text{cIPO}}(\pi_{\theta},\pi_{\text{ref}})=\mathbb{E}_{(x,y_{1}, y_{2},\hat{p})\sim\mathcal{D}}\left[\left(h_{\theta}(x,y_{1},y_{2})-\frac{2 \hat{p}-1}{2\beta}\right)^{2}\right]. \tag{8}\]

Robust Preference OptimizationROPO [27] designs the objective to resolve the instability under noisy label problems, which is inspired by the unhinged loss [54] and reverse cross-entropy loss [59] in the noise-tolerant supervised learning literature. The objective is a combination of the regularization term and original DPO loss such as,

\[\mathcal{L}_{\text{ROPO}}(\pi_{\theta},\pi_{\text{ref}}) =\alpha\mathbb{E}_{(x,y_{1},y_{2},\hat{p})\sim\mathcal{D}}\left[ \sigma\left(h_{\theta}(x,y_{2},y_{1})\right)\right]-\gamma\mathbb{E}_{(x,y_{2}, y_{1})\sim\mathcal{D}}\left[\log\sigma\left(h_{\theta}(x,y_{1},y_{2})\right)\right] \tag{9}\] \[=\alpha\left(1-\mathbb{E}_{(x,y_{1},y_{2},\hat{p})\sim\mathcal{D} }\left[\sigma\left(h_{\theta}(x,y_{1},y_{2})\right)\right]\right)+\gamma \mathcal{L}_{\text{DPO}}(\pi_{\theta},\pi_{\text{ref}}),\]

where \(\alpha>0\) and \(\gamma>0\) are extra hyper-parameters to balance the contribution of each term.

## 3 Methods

As DPO and the related methods assume binary preference, they cannot reflect the fine-grained relationship between the pair of responses during training. The conservative formulation of DPO can use the soft preference labels, but we found that it could not achieve good performance if modestly-confident labels shape the distribution as a majority (see Section 5). In this section, we propose a simple yet effective modification, weighted geometric averaging of LLM output likelihood in the learning loss, which can be applied to a family of algorithms derived from DPO.

### Weighted Geometric Averaging and Practical Algorithms

We assume that the pairs of winner and loser outputs (\(y_{w}\), \(y_{l}\)) are sampled from the weighted geometric average of LLM policies \(\bar{\pi}(\cdot\mid x)\) such as,

\[\bar{\pi}(y_{w}\mid x) := \frac{1}{Z_{\pi,w}(x)}\pi(y_{1}\mid x)^{\hat{p}}\pi(y_{2}\mid x)^{ 1-\hat{p}} \tag{10}\] \[\bar{\pi}(y_{l}\mid x) := \frac{1}{Z_{\pi,l}(x)}\pi(y_{1}\mid x)^{1-\hat{p}}\pi(y_{2}\mid x) ^{\hat{p}},\]

where \(Z_{\pi,w}(x):=\sum_{y_{j},y_{k},\hat{p}}\pi(y_{j}\mid x)^{\hat{p}}\pi(y_{k}\mid x )^{1-\hat{p}}\) and \(Z_{\pi,l}(x):=\sum_{y_{j},y_{k},\hat{p}}\pi(y_{j}\mid x)^{1-\hat{p}}\pi(y_{k} \mid x)^{\hat{p}}\) (\(y_{j}\succ y_{k}\)). Because it is difficult to obtain precise estimation of these values with sampling, we set those normalization terms to constant and ignore them in practice, which is a common assumption in deep RL literature [18; 39; 46; 61]. If we have true binary labels (i.e. \(\hat{p}=1\)), Equation 10 reduces to the original formulation under the assumption of \(y_{1}\succ y_{2}\).

Weighted geometric averaging can be considered as a regularization, which pushes the large likelihood down to small when the soft preference is far from 1. In the following, we present three modified DPO-based methods: Geometric DPO (GDPO), Geometric IPO (GIPO), and Geometric ROPO (GROPO), by replacing the winner output likelihood \(\pi(y_{1}\mid x)\rightarrow\pi(y_{1}\mid x)^{\hat{p}}\pi(y_{2}\mid x)^{1-\hat{ p}}\) and the loser output likelihood \(\pi(y_{2}\mid x)\rightarrow\pi(y_{1}\mid x)^{1-\hat{p}}\pi(y_{2}\mid x)^{\hat{p}}\) for both \(\pi_{\theta}\) and \(\pi_{\text{ref}}\):

**Geometric Direct Preference Optimization (GDPO)**

\[\mathcal{L}_{\text{GDPO}}(\pi_{\theta},\pi_{\text{ref}}) =-\mathbb{E}_{\mathcal{D}}\left[\log\sigma\left(\beta\log\frac{ \pi_{\theta}(y_{1}\mid x)^{\hat{p}}\pi_{\text{ref}}(y_{2}\mid x)^{1-\hat{p}} \pi_{\text{ref}}(y_{1}\mid x)^{1-\hat{p}}\pi_{\text{ref}}(y_{2}\mid x)^{\hat{p }}}{\pi_{\text{ref}}(y_{1}\mid x)^{\hat{p}}\pi_{\text{ref}}(y_{2}\mid x)^{1- \hat{p}}\pi_{\theta}(y_{1}\mid x)^{1-\hat{p}}\pi_{\theta}(y_{2}\mid x)^{\hat{p }}}\right)\right] \tag{11}\] \[=-\mathbb{E}_{(x,y_{1},y_{2},\hat{p})\sim\mathcal{D}}\left[\log \sigma\left(\beta(2\hat{p}-1)\log\frac{\pi_{\theta}(y_{1}\mid x)\pi_{\text{ ref}}(y_{2}\mid x)}{\pi_{\text{ref}}(y_{1}\mid x)\pi_{\theta}(y_{2}\mid x )}\right)\right],\]

**Geometric Identity Preference Optimization (GIPO)**

\[\mathcal{L}_{\text{GIPO}}(\pi_{\theta},\pi_{\text{ref}})=\mathbb{E}_{(x,y_{1},y_{2},\hat{p})\sim\mathcal{D}}\left[(2\hat{p}-1)^{2}\left(h_{\theta}(x,y_{1},y_{2})-\frac{1}{2\beta}\right)^{2}\right], \tag{12}\]

**Geometric Robust Preference Optimization (GROPO)**

\[\mathcal{L}_{\text{GROPO}}(\pi_{\theta},\pi_{\text{ref}})=\alpha\left(1- \mathbb{E}_{\mathcal{D}}\left[\sigma\left(\beta(2\hat{p}-1)\log\frac{\pi_{ \theta}(y_{1}\mid x)\pi_{\text{ref}}(y_{2}\mid x)}{\pi_{\text{ref}}(y_{1}\mid x )\pi_{\theta}(y_{2}\mid x)}\right)\right]\right)+\gamma\mathcal{L}_{\text{ GDPO}}(\pi_{\theta},\pi_{\text{ref}}). \tag{13}\]

Figure 1: **(Left) Scaling factors \(w_{\theta}\) in the gradient of each objective (DPO, cDPO, and GDPO), which is a function of \(h(x,y_{1},y_{2})\). Geometric averaging (GDPO) can adjust the scale of gradient based on the soft preference labels; if soft preference labels are close to 1 (\(\hat{p}=0.95\)), the scaling factor of GDPO is almost the same, and small soft labels (\(\hat{p}=0.55\)) make the scaling factor small while the norm reaches zero. (Right) A 1-D bandit problem with 100 actions, illustrating the histogram of train data and true reward function, preference distribution, and action distribution from the learned policies. Although cDPO accurately fits the data distribution and has the mode in a low-reward region, DPO and GDPO can assign a probability mass in a high-reward region.**These objectives are consistent with original ones (Equation 5, 7, 9) when we have binary preferences.

### Geometric Averaging Can Adjust the Scale of Gradients

To analyze the role of weighted geometric averaging, we consider the gradient of loss function with respect to model parameters \(\theta\) in a general form, which can be written as:

\[\nabla_{\theta}\mathcal{L}=-\beta\mathbb{E}_{(x,y_{1},y_{2},\hat{p})\sim \mathcal{D}}\left[\underbrace{w_{\theta}(x,y_{1},y_{2},\hat{p})}_{\text{ scaling factor}}\underbrace{[\nabla_{\theta}\log\pi_{\theta}(y_{1}\mid x)-\nabla_{ \theta}\log\pi_{\theta}(y_{2}\mid x)]}_{\text{positive and negative policy gradients}}\right], \tag{14}\]

where \(w_{\theta}(x,y_{1},y_{2},\hat{p})\) is a scaling factor of positive and negative gradients. While defining an estimated preference probability by their own policy LLMs under the Brady-Terry model as:

\[\rho_{\theta}:=\sigma\left(\beta\log\frac{\pi_{\theta}(y_{1}\mid x)\pi_{\text {ref}}(y_{2}\mid x)}{\pi_{\text{ref}}(y_{1}\mid x)\pi_{\theta}(y_{2}\mid x)} \right),\;\rho^{\prime}_{\theta}:=\sigma\left(\beta(2\hat{p}-1)\log\frac{\pi_{ \theta}(y_{1}\mid x)\pi_{\text{ref}}(y_{2}\mid x)}{\pi_{\text{ref}}(y_{1}\mid x )\pi_{\theta}(y_{2}\mid x)}\right), \tag{15}\]

we summarize the scaling factor of each method in Table 1. Comparing \(\nabla_{\theta}\mathcal{L}_{\text{DPO}}\) and \(\nabla_{\theta}\mathcal{L}_{\text{DPO}}\), DPO optimizes the model until the estimate preference \(\rho_{\theta}\) reaches 1 (\(w_{\theta}=1-\rho_{\theta}\)), and cDPO does until \(\rho_{\theta}\) matches the soft preference \(\hat{p}\) by assigning a high weight when the estimation is wrong (\(w_{\theta}=\hat{p}-\rho_{\theta}\)). DPO pushes the distribution to the oracle preferable outputs, and cDPO may work well as a regularization if the label has high confidence (e.g. \(\hat{p}=0.95\)). However, the gradient of cDPO may also cause unnecessary model updates around \(\hat{p}=0.5\). Intuitively, \(\hat{p}=0.5\) means either candidate answers \((y_{1},y_{2})\) are equally good, but \(\nabla_{\theta}\mathcal{L}_{\text{cDPO}}\) forces their likelihoods to be balanced.

In contrast, GDPO adjusts the gradient scale based on soft preference by multiplying \((2\hat{p}-1)\), which can also ignore the gradient from even candidate pairs. Figure 1 (left) visualizes that weighted geometric averaging can adjust the scale of gradient based on the soft preference labels. If soft preference labels are close to 1 (e.g. \(\hat{p}=0.95\)), the norm of the scaling factor is almost the same, and small soft preference makes the scaling factor small while the norm reaches zero (e.g. \(\hat{p}=0.55\)). This maintains the effect from clear relationship pairs, reduces the effect from equally good outputs, and reflects the detailed preference signals among the responses. In practice, we set a larger value for \(\beta\) in GDPO than in DPO to maintain and amplify the scale of the gradient for acceptable preference pairs, which works as an implicit filtering of soft preference labels. We will explain this in Section 5.1.

### Analysis in 1-D Synthetic Bandit Problem

To highlight the advantage of geometric averaging and the failure case of linear interpolation as done in cDPO (Equation 6), we consider a 1-D bandit problem with 100 discrete actions and a linear reward function. Figure 1 (right) illustrates the histogram of train data and true reward function, paired preference distribution, and action distributions from the learned policies. The 500,000 training instances are sampled from a bimodal mixture of Gaussian distribution (with the mode in the 20-th and 70-th indices), and we prepare the paired data from those while labeling preferences with the Bradley-Terry model. We train the parameterized reward \(r_{\psi}\) by minimizing \(\mathcal{L}_{\text{DPO}}\), \(\mathcal{L}_{\text{cDPO}}\), and \(\mathcal{L}_{\text{GDPO}}\), and then recover the learned policies analytically as \(\pi_{r_{\psi}}(y)\propto\pi_{\text{data}}(y)\exp(r_{\psi}(y))\), where \(\pi_{\text{data}}(y)\) is an underlying train data distribution. The results demonstrate that cDPO accurately fits the data distribution, which is because the linear interpolation of the loss function in Equation 6 can be interpreted as a minimization of KL divergence \(\mathbb{E}[D_{\text{KL}}(\hat{p}\mid\mid\rho_{\theta})]\). However, this could result in a sub-optimal solution when the train data has a peak in a low-reward region. Because greedy decoding considers the mode of learned distributions, this accurate modeling in cDPO is not aligned with the text generation objectives. On the other hand, DPO and GDPO can assign a probability mass in a high-reward region. GDPO has an advantage against cDPO by resolving such an objective mismatch. Similar trends can be observed in the LLM experiments (Section 5).

\begin{table}
\begin{tabular}{l l} \hline \hline Method & Scaling Factor \(w_{\theta}\) \\ \hline
**DPO**[41] & \(1-\rho_{\theta}\) \\
**cDPO**[30] & \(\hat{p}-\rho_{\theta}\) \\
**GDPO** (ours) & \((2\hat{p}-1)(1-\rho^{\prime}_{\theta})\) \\ \hline
**IPO**[41] & \(\frac{\frac{\pi}{2}-\frac{1}{2}\log\frac{\rho_{\theta}}{1-\rho_{\theta}}}{ \pi_{\text{ref}}(y_{1}\mid x)\pi_{\theta}(y_{2}\mid x)}\) \\
**cDPO**[27] & \(\frac{2\hat{p}-1}{\rho^{2}}-\frac{2}{\rho}\log\frac{\rho_{\theta}}{1-\rho_{ \theta}}\) \\
**GIPO** (ours) & \((2\hat{p}-1)^{2}\left(\frac{\frac{\pi}{2}-\frac{\hat{p}}{2}\log\frac{\rho^{ \prime}_{\theta}}{1-\rho_{\theta}}}{\pi_{\text{ref}}(y_{1}\mid x)\pi_{\theta}( y_{2}\mid x)}\right)\) \\ \hline
**ROPO**[27] & \((\gamma-\alpha\rho_{\theta})(1-\rho_{\theta})\) \\
**GROPO** (ours) & \((2\hat{p}-1)(\gamma-\alpha\rho_{\theta})(1-\rho^{\prime}_{\theta})\) \\ \hline \hline \end{tabular}
\end{table}
Table 1: Scaling factor \(w_{\theta}(x,y_{1},y_{2},\hat{p})\) in the gradient of loss function (Equation 14). The estimated preference probabilities \(\rho_{\theta}\) and \(\rho^{\prime}_{\theta}\) are defined in Equation 15. Compared to others, geometric averaging has a product of \((2\hat{p}-1)\) in a scaling factor, which forces the norm of gradients from the equally preferable responses close to zero.

## 4 Experiments

In the experiments, we use PaLM 2-XS [1] for the base LLM, as done in prior works [16; 20; 24; 43] (Appendix L uses Gemma-2B/7B as base LLMs). We use the popular RLHF datasets, such as Reddit TL;DR [49; 55] (summarization), and Anthropic Helpful and Harmless [3] (conversation) for the benchmark. To simulate the soft preference labels, we relabel the preference to the datasets by leveraging AI feedback [4; 24] from instruction-tuned PaLM 2-L (Section 4.1). However, because we found that the soft label distributions in popular RLHF datasets only have similar shapes concentrating on high-confidence regions such as \(\hat{p}\in[0.95,1.0]\) (Figure 10 in Appendix I), we prepared (1) new competitive paired responses from a winner in the original dataset and from LLMs and (2) the novel preference dataset based on Plasma Plan [6], a dataset of daily-life natural language planning, which simulate more diverse preference label distributions we may face in a practical scenario. For instance, Plasma Plan has a pair of instruction \(x\) (e.g. _see a movie_) and the human-written gold plan \(y\) (e.g. _Step 1: Choose a movie, Step 2: Buy a ticket, Step 3: Go to the theater_). To construct a pair of plans, we generated the plans to all the instructions using PaLM 2-L with few-shot prompting, and then obtained the triplet \((x,y_{\text{data}},y_{\text{PaLM}})\). We gathered about 60K response pairs for train split and 861 examples for test split. Following this procedure, we prepared about 93K (Reddit TL;DR), 44K (Anthropic Helpful), and 42K (Harmless) response pairs as train split. To reduce the inference cost, we sample 1000 test prompt-response tuples in Reddit TL;DR while removing the duplicated ones. For other datasets, we have 1639 (Helpful) and 1614 (Harmless) examples in the test split.

To prepare the SFT models, we finetune PaLM 2-XS using 50% of winner responses in train split for Reddit TL;DR, Anthropic Helpful, and Harmless, and using the responses from PaLM 2-L for Plasma Plan. We use those SFT models as an initial checkpoint of preference methods and the reference models \(\pi_{\text{ref}}\). See Appendix B for further details on training.

### Simulating Soft Preference with AI Feedback

Following prior works [4; 10; 15; 24], as reliable alternatives to human raters, we simulate the soft preference labeling with AI feedback from LLMs. AI rating is well aligned with humans and is often used as a proxy of human evaluation in the RLHF literature [68]. Throughout the work, we use PaLM 2-L instruction-tuned on Flan dataset [13] as an AI rater. To obtain the soft preferences, we put the context \(x\), first output \(y_{1}\), second output \(y_{2}\), and the statement such as _"The more preferable output is: "_, and then get the log probability (score) of token "(1)" and "(2)" from LLMs. Assuming the Bradley-Terry model, we compute the AI preference as follows:

\[\hat{p}_{\text{AI}}(y_{1}\succ y_{2}\mid x)=\frac{\exp(\texttt{score}((1)))}{ \exp(\texttt{score}((1)))+\exp(\texttt{score}((2)))}. \tag{16}\]

Lastly, to reduce the position bias [40; 58] in LLM rating, we take the average of \(\hat{p}_{\text{AI}}\) by flipping the ordering of \((y_{1},y_{2})\) in the prompt. See Appendix F for the prompts of AI rating. For a fair comparison, we prepare the binary labels based on \(\hat{p}_{\text{AI}}\) rather than the original labels in the dataset.

Figure 2 shows the histogram of soft preference labels from the AI feedback in the preference datasets. We construct competitive paired samples with winner responses and the ones from PaLM 2-L to simulate diverse preference distributions that have uniformity or a peak around the modest confidence (e.g. \(\hat{p}\in[0.7,0.9)\)). We also prepare two other datasets based on Plasma Plan, with different distributions; Plasma Plan Skewed is the more skewed preference dataset by cutting off the high soft preference labels such as \(\hat{p}\geq 0.8\), and Plasma Plan Stairs has lower confident samples more while the number of high confident samples monotonically decreases (\(\hat{p}\in[0.65,0.9)\)). Those distributions could happen in practice when we make pairs of the responses from the capable LLMs

Figure 2: Histogram of soft preference labels \(\hat{p}\) in preference dataset simulated with AI feedback from PaLM 2-L, instruction-tuned on Flan dataset. We prepare Reddit TL;DR [69; 49], Anthropic Helpful and Harmless [4], and Plasma Plan [6]. We construct competitive paired samples with winner responses and PaLM 2-L to simulate diverse preference distributions that have a peak around the modest confidence (e.g. \(\hat{p}\in[0.7,0.9)\)).

(see Appendix E) and also help more clearly demonstrate the behavior of each algorithm when modestly-confident labels are in the majority. They could lead to better performance than preference labels concentrating on high confidence. The train split of Plasma Plan Skewed and Stairs consists of about 30K/27K response pairs, and the test splits are shared among the dataset from Plasma Plan.

### Binary and Percentage Judge for Evaluation

For the evaluation, we conduct a pairwise comparison between the response from the trained models (\(y_{\text{lim}}\)) and the reference response from PaLM 2-L, and GPT-4 [33] (\(y_{\text{ref}}\)). The reference responses from PaLM 2-L and GPT-4 are generated with few-shot prompting (see Appendix G). In addition, we directly compare our methods and corresponding baselines (e.g. GIPO v.s. IPO or cIPO).

As evaluation metrics, we use the winning rate from binary and percentage judge. We first calculate the AI preference between the response from the trained models and evaluation data as explained in Section 4.1. We calculate the average binary and percent winning rate as follows:

\[\texttt{binary}=\frac{1}{|\mathcal{D}|}\sum_{(x,y_{\text{inf}},y_{\text{lim}})} \mathbbm{1}\left[\hat{p}_{\text{AI}}(y_{\text{lim}}\succ y_{\text{ref}}\mid x )\geq.5\right],\ \ \texttt{percent}=\frac{1}{|\mathcal{D}|}\sum_{(x,y_{\text{inf}},y_{\text{lim}})} \hat{p}_{\text{AI}}(y_{\text{lim}}\succ y_{\text{ref}}\mid x). \tag{17}\]

Note that \(\hat{p}_{\text{AI}}\) is also averaged among the flipped order to alleviate the position bias.

## 5 Results

We first compare the alignment performance among the algorithms with binary feedback (DPO, IPO, ROPO), their conservative variants (cDPO, cIPO), and weighted geometric averaging with soft feedback (GDPO, GIPO, GROPO) on six preference datasets (Section 5.1), and evaluate the preference label classification by the learned models (Section 5.2). We also analyze the log-likelihood ratio and reward gap during training (Section 5.4), and then demonstrate the online alignment performances (Section 5.3).

\begin{table}
\begin{tabular}{l c c c c c c c c c c c} \hline \hline  & \multicolumn{4}{c}{Reddit TL;DR} & \multicolumn{4}{c}{Attribute Relipful} & \multicolumn{4}{c}{Attribute RarInless} \\ \cline{2-13}  & vs. Path 2-L & vs. crf-4 & vs. plat-L & vs. crf-4 & vs. eval. \#2-L & vs. crf-4 & vs. pat-L & vs. grf-4 \\ Methods & Binary & \% & Binary & \% & Binary & \% & Binary & \% & Binary & \% & Binary & \% \\ \hline
**SFT** & 16.2\% & 41.0\% & 3.80\% & 33.38\% & 62.60\% & 56.69\% & 5.74\% & 20.67\% & 62.76\% & 57.83\% & 31.54\% & 36.42\% \\ \hline
**DPO**[41] & 16.90\% & 40.91\% & 4.00\% & 33.51\% & 86.21\% & 75.40\% & 16.23\% & 33.98\% & 75.40\% & 65.95\% & 41.02\% & 42.79\% \\
**cDPO**[30] & 17.20\% & 41.61\% & 3.80\% & 33.38\% & 83.28\% & 74.04\% & 16.11\% & 33.28\%

### Weighted Geometric Averaging Improves the Alignment Performance

Table 2 presents the winning rate on Reddit TL;DR, Anthropic Helpful and Harmless, Plasma Plan, Plasma Plan Skewed, and Stairs. We compare the performance between the baseline algorithms derived from DPO (SFT, DPO, cDPO, IPO, cIPO, ROPO) and the ones applying geometric averaging (GDPO, GIPO, GROPO). Through the experiments, we set the temperature to 0.0 for the inference.

The results demonstrate that the methods applying geometric averaging (GDPO, GIPO, GROPO) achieve consistently better or comparable performances against binary preference methods (DPO, IPO, ROPO) or their conservative variants (cDPO, cIPO). The trend is clearer on Plasma Plan, Plasma Plan Skewed, and Stairs, which have richer modestly-confident labels. The improvement of GDPO, GIPO, and GROPO compared to baselines have statistical significance with \(p<0.01\) on the Wilcoxon signed-rank test, compared to SFT, binary preference methods, and soft preference methods with linear interpolation. Appendix I also provides the results with the original paired response from Reddit TL;DR, Anthropic Helpful, and Harmless, where many soft labels concentrate on \(\hat{p}\in[0.95,1.0]\). Table 2 highlights that rich soft labels help align LLMs better than those binary ones. Focusing on DPO variants, cDPO does not work well while GDPO performs the best. We hypothesize that this comes from the objective mismatch between the text generation and preference modeling, which we verify in Section 5.2. Moreover, Figure 3 shows the binary winning rates in the direct comparison between corresponding methods, such as GDPO v.s DPO, and GIPO v.s. cIPO, etc, which also reveals that geometric averaging consistently outputs more preferable responses.

Large \(\beta\) as Implicit Preference FilteringAs discussed in Section 3.2, weighted geometric averaging makes the norm of the gradient smaller based on soft preference label \(\hat{p}\). However, an unnecessarily small gradient could stick to sub-optimal solutions. It would be necessary to maintain and even amplify the scale of the gradient from reliable preference pairs. For the rescaling of the gradient, we set larger \(\beta\) because, in geometric averaging, we can regard as using smaller \(\beta^{\prime}:=\beta\mathbb{E}[2\hat{p}-1]<\beta\). Such a larger \(\beta\) works as an implicit filtering of soft preference labels. Figure 4 (left) presents the binary winning rate of DPO and GDPO with different \(\beta\in[0.1,0.5]\) on Plasma Plan dataset. GDPO has a peak at \(\beta=0.3\), which is larger than that of DPO (\(\beta=0.1\)), and GDPO can achieve better performance. See Appendix B for further details of hyper-parameters.

### Preference Label Classification

Since DPO objective (Equation 5) is derived from the assumption under the Bradley-Terry model, we can regard it as training reward models and implicitly estimating preference probability. We here compare DPO, cDPO, and GDPO, estimate the preference probability \(\rho_{\theta}\) from Equation 15, make a binary label classification (as done in Equation 17), and then compute the average accuracy between predicted labels and true labels given via AI rating. We use Plasma Plan and prepare three different pairs of outputs between PaLM 2-L and (1) humans, (2) GPT-4, and (3) GPT-3.5.

Figure 4 (left) shows that all the methods can classify preference labels well when the test split is composed of the responses from PaLM 2-L and humans, which is the same data distribution as the train split. However, DPO sharply decreases the performance for classifying out-of-distribution pairs, such as from GPT-4 and GPT-3.5 (94.0% \(\rightarrow\) 61.6%/66.3%). cDPO achieves the best classification accuracy on average, and GDPO mitigates the performance drop in DPO. Despite the best accuracy through the proper preference modeling, cDPO does not work well in text generation (Section 5.1). As pointed out in Section 3.3, this can be attributed to an objective mismatch between text generation and preference modeling. While preference modeling aims to fit the models into the given data

Figure 3: Binary winning rates in the direct comparison between weighted geometric averaging (e.g. GDPO) and the corresponding baselines (e.g. DPO, cDPO). The results against SFT are averaged among GDPO, GIPO, and GROPO (Figure 9). Geometric averaging consistently outputs more preferable responses than competitive baselines with about 70% winning rate on average. See Appendix K for the example responses.

distribution, the model in the text generation outputs the mode of distribution with greedy decoding, which might cause a significant mismatch when the mode of distribution is in the low-reward region. These empirical results highlight that GDPO successfully incorporates the strong performance in DPO and the nuanced relationship from the soft labels while avoiding a mismatch.

### Weighted Geometric-Averaging Suppresses Over-Optimization

The analysis of the log-likelihood ratio and the estimated reward gap can characterize the behavior of offline alignment algorithms [51]. In Figure 5, we measure the log-likelihood ratio of winner/loser responses and estimated reward gap on Plasma Plan and Anthropic Harmless.

DPO aggressively pushes down both log ratios and increases the reward gap, since DPO objective forces the model to achieve \(r_{\theta}(x,y_{w})-r_{\theta}(x,y_{l})\rightarrow\infty\), which causes an over-optimization issue. cDPO is more conservative in pushing down the log ratio while leading to worse alignment quality due to objective mismatch. GDPO mitigates the issues of such objective mismatch and over-optimization by maintaining the reward gap increase modestly. Note that, because our paper has focused on open-ended generation tasks, the decrease in the log-likelihood measured with preferable responses does not always matter in contrast to mathematical reasoning or code generation [36, 65, 42]. Our target tasks require pushing down the likelihood of both winner and loser responses to further improve the response quality through the exploration into out-of-distribution regions.

### Weighted Geometric-Averaging Can Help Online Alignment

Offline alignment methods can be extended to online updates [64, 20] by introducing online feedback processes such as extra reward models or self-rewarding [8]. Due to the cost constraints, online feedback is often asked to be fast and lightweight. However, the quality of preference labels significantly affects the alignment performances. In this section, we demonstrate that weighted geometric averaging can improve online alignment performance by mitigating the quality issues in online feedback. We employ the following two feedback processes: incorporating an extra reward model \(r_{\psi}(x,y)\) and leveraging estimated self-preference \(\rho_{\theta}=\sigma(\beta\log\frac{\pi_{\theta}(x,y_{w})\pi_{\theta}(x,y)}{ \pi_{\theta}(x,y_{w})\pi_{\theta}(x,y)})\). Note that we apply stop gradient operation for the self-preference. For the extra reward model, we use PaLM 2-XS, the same as a policy LLM.

Figure 6 shows that GDPO performs the best in both settings. This is because GDPO can cancel the gradient from less-confident soft preferences as discussed in Section 3.2, which comes from the case when the on-policy responses are equally good or the estimated preferences in online feedback are

Figure 4: **(Left) Binary winning rate of DPO and GDPO with different \(\beta\in[0.1,0.5]\). GDPO peaks at \(\beta=0.3\), which is larger than that of DPO (\(\beta=0.1\)). (Right) Accuracy of preference label classification on Plasma Plan dataset. All the methods can classify the labels well when the test split is composed of the response pairs from PaLM 2-L and humans; the same data distribution as the train split. However, DPO significantly drops the classification performance when facing out-of-distribution pairs, such as from GPT-4 and GPT-3.5. cDPO achieves the best classification performance on average, and GDPO mitigates the performance drop in DPO.**

Figure 5: Log-likelihood ratio and estimated reward gap on Plasma Plan and Anthropic Harmless. GDPO mitigates the issues of objective mismatch in cDPO and over-optimization in DPO by suppressing the reward gap increase modestly. While Plasma Plan and Anthropic Harmless have different soft preference distributions from each other, the trends of the log-likelihood ratio and reward gap among the algorithms are the same.

not calibrated enough. GDPO demonstrates a significant gain in self-preference. In contrast, DPO degrades the performance worse because the binarization increases the gap from the true preference.

## 6 Discussion and Limitation

We show that geometric averaging consistently improves the performance of DPO, IPO, and ROPO with soft preference labels. We also observe that uniformly distributed soft preference labels achieve better alignments than the original dataset (Appendix I). In fact, the modestly-confident labels do not always mean that the paired responses are noisy or low-quality but even also are more informative, because that could often happen if both responses are good enough. While, as seen in Figure 10 (Appendix I), most datasets for RLHF research only consist of highly-confident pairs, rethinking the effect of preference data distribution on the performances is an important future direction for the practitioners.

The automatic AI rating has a good correlation to the human rating, and is a popular and scalable alternative recently [10; 15; 24; 68]. Our experiments have been conducted on datasets labeled by LLMs as a proximal simulation of soft preference due to the cost constraints. Leveraging actual human preferences labeled via majority voting is another possible future work.

## 7 Related Works

From the helpfulness and safety perspective, it is important to align the outputs from LLMs to the social agreements and our common sense. RLHF [12; 49; 63] is the most popular choice, where we train the reward models to score the predictions and maximize the learned reward with deep RL algorithms [45; 62]. However, this requires additional computational costs from the two independent LLMs and complex pipelines due to on-policy samples. As appealing alternatives, offline algorithms with a single model have been proposed; one of the most representative is DPO [41], which has been actively extended with different constraints [52; 56], loss function [2; 17; 67; 64], iterative online training [9; 20; 66], nash equilibrium [32; 44; 50], and combination to rejection sampling [28].

In addition to algorithmic improvements, the alignment problem has been studied from the data perspective [14; 22; 23; 60], which argues that the high-quality, fine-grained preference data without label noise is critical for the performance [19; 31; 35; 37]. Since the preference labels from the human raters must have disagreements and be diverse, Bayesian [57] or distributional reward modeling [26; 47] and noise-tolerant objectives [11; 27] have been investigated, to maintain the high-quality learning signals even from practical diverse preferences.

Our work newly introduces the notion of soft preference labels - a more general and practical formalization of noisy labels - and then a simple yet effective technique to incorporate the distributional preference into algorithms that have only accepted the binary preference before.

## 8 Conclusion

While the preference is inherently diverse among humans, most prior works only focus on binary labels. To reflect a more detailed preference relationship, we introduce soft preference labels and a simple yet effective modification via weighted geometric averaging that can be applicable to any DPO algorithmic variants. The results demonstrate that soft labels and geometric averaging consistently improve the alignment performance compared to binary labels and conservative methods with linear interpolation of objectives. Using soft labels improves model responses over binary labels by mitigating over-optimization. We also identify that conservative methods, that can fit the preference distribution much better, suffer from the objective mismatch between the text generation and preference modeling. In contrast, geometric averaging can balance both and empirically works better. We hope our work encourages more uses of soft preference labels for alignment in future.

Figure 6: Online alignment with extra reward model (top) and self-preference (bottom) on Plasma Plan. GDPO performs the best with both types of feedback.

## Acknowledgments

We thank Bert Chan, Yusuke Iwasawa and Doina Precup for helpful discussion on this work. HF was supported by JSPS KAKENHI Grant Number JP22J21582.

## References

* [1] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Clement Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark Diaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenenely, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni, Arora Mahendru, Joshua Mayze, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report. _arXiv preprint arXiv:2305.10403_, 2023.
* [2] Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel Guo, Daniele Calandriello, Michal Valko, and Remi Munos. A general theoretical paradigm to understand learning from human preferences. _arXiv preprint arxiv:2310.12036_, 2023.
* [3] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback. _arXiv preprint arXiv:2204.05862_, 2022.
* [4] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite, Liane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado, Nova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional ai: Harmlessness from ai feedback. _arXiv preprint arXiv:2212.08073_, 2022.
* [5] Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. _Biometrika_, 39(3/4):324-345, 1952.
* [6] Faeze Brahman, Chandra Bhagavatula, Valentina Pyatkin, Jena D. Hwang, Xiang Lorraine Li, Hirona J. Arai, Soumya Sanyal, Keisuke Sakaguchi, Xiang Ren, and Yejin Choi. Plasma: Making small language models better procedural knowledge models for (counterfactual) planning. _arXiv preprint arxiv:2305.19472_, 2023.

* [7] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. _arXiv preprint arXiv:2005.14165_, 2020.
* [8] Changyu Chen, Zichen Liu, Chao Du, Tianyu Pang, Qian Liu, Arunesh Sinha, Pradeep Varakantham, and Min Lin. Bootstrapping language models with dpo implicit rewards. _arXiv preprint arXiv:2406.09760_, 2024.
* [9] Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, and Quanquan Gu. Self-play fine-tuning converts weak language models to strong language models. _arXiv preprint arXiv:2401.01335_, 2024.
* [10] Cheng-Han Chiang and Hung yi Lee. Can large language models be an alternative to human evaluations? _arXiv preprint arXiv:2305.01937_, 2023.
* [11] Sayak Ray Chowdhury, Anush Kini, and Nagarajan Natarajan. Provably robust dpo: Aligning language models with noisy feedback. _arXiv preprint arxiv:2403.00409_, 2024.
* [12] Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. _arXiv preprint arXiv:1706.03741_, 2017.
* [13] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models. _arXiv preprint arXiv:2210.11416_, 2022.
* [14] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback. _arXiv preprint arXiv:2310.01377_, 2023.
* [15] Yann Dubois, Balazs Galambosi, Percy Liang, and Tatsunori B Hashimoto. Length-controlled alpacaeval: A simple way to debias automatic evaluators. _arXiv preprint arXiv:2404.04475_, 2024.
* [16] Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D'Amour, DJ Divjotham, Adam Fisch, Katherine Heller, Stephen Pfohl, Deepak Ramachandran, Peter Shaw, and Jonathan Berant. Helping or herding? reward model ensembles mitigate but do not eliminate reward hacking. _arXiv preprint arXiv:2312.09244_, 2023.
* [17] Kawin Ethayarajah, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. _arXiv preprint arXiv:2402.01306_, 2024.
* [18] Hiroki Furuta, Tadashi Kozuno, Tatsuya Matsushima, Yutaka Matsuo, and Shixiang Shane Gu. Co-adaptation of algorithmic and implementational innovations in inference-based deep reinforcement learning. In _Advances in Neural Information Processing Systems_, 2021.
* [19] Yang Gao, Dana Alon, and Donald Metzler. Impact of preference noise on the alignment performance of generative language models. _arXiv preprint arXiv:2404.09824_, 2024.
* [20] Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, Johan Ferret, and Mathieu Blondel. Direct language model alignment from online ai feedback. _arXiv preprint arXiv:2402.04792_, 2024.
* [21] Haozhe Ji, Cheng Lu, Yilin Niu, Pei Ke, Hongning Wang, Jun Zhu, Jie Tang, and Minlie Huang. Towards efficient and exact optimization of language model alignment. _arXiv preprint arXiv:2402.00856_, 2024.

* [22] Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of LLM via a human-preference dataset. In _Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2023.
* [23] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. Rewardbench: Evaluating reward models for language modeling. _arXiv preprint arXiv:2403.13787_, 2024.
* [24] Harrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Lu, Colton Bishop, Ethan Hall, Victor Carbone, Abhinav Rastogi, and Sushant Prakash. Rlaif: Scaling reinforcement learning from human feedback with ai feedback. _arXiv preprint arXiv:2309.00267_, 2023.
* [25] Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta, John Canny, and Ian Fischer. A human-inspired reading agent with gist memory of very long contexts. _arXiv preprint arXiv:2402.09727_, 2024.
* [26] Dexun Li, Cong Zhang, Kuicai Dong, Derrick Goh Xin Deik, Ruiming Tang, and Yong Liu. Aligning crowd feedback via distributional preference reward modeling. _arXiv preprint arXiv:2402.09764_, 2024.
* [27] Xize Liang, Chao Chen, Jie Wang, Yue Wu, Zhihang Fu, Zhihao Shi, Feng Wu, and Jieping Ye. Robust preference optimization with provable noise tolerance for lms. _arXiv preprint arxiv:2404.04102_, 2024.
* [28] Tianqi Liu, Yao Zhao, Rishabh Joshi, Misha Khalman, Mohammad Saleh, Peter J. Liu, and Jialu Liu. Statistical rejection sampling improves preference optimization. _arXiv preprint arXiv:2309.06657_, 2023.
* [29] Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu. Deployment-efficient reinforcement learning via model-based offline optimization. In _International Conference on Learning Representations_, 2021.
* [30] Eric Mitchell. A note on dpo with noisy preferences & relationship to ipo, 2023. URL [https://ericmitchell.ai/cdpo.pdf](https://ericmitchell.ai/cdpo.pdf).
* [31] Tetsuro Morimura, Mitsuki Sakamoto, Yuu Jinnai, Kenshi Abe, and Kaito Ariu. Filtered direct preference optimization. _arXiv preprint arXiv:2404.13846_, 2024.
* [32] Remi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo, Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, Marco Selvi, Sertan Girgin, Nikola Momchev, Olivier Bachem, Daniel J. Mankowitz, Doina Precup, and Bilal Piot. Nash learning from human feedback. _arXiv preprint arXiv:2312.00886_, 2023.
* [33] OpenAI. Gpt-4 technical report. _arXiv preprint arXiv:2303.08774_, 2023.
* [34] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. _arXiv preprint arXiv:2203.02155_, 2022.
* [35] Alizee Pace, Jonathan Mallinson, Eric Malmi, Sebastian Krause, and Aliaksei Severyn. West-of-n: Synthetic preference generation for improved reward modeling. _arXiv preprint arXiv:2401.12086_, 2024.
* [36] Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason Weston. Iterative reasoning preference optimization. _arXiv preprint arXiv:2404.19733_, 2024.
* [37] Pulkit Pattnaik, Rishabh Maheshwary, Kelechi Ogueji, Vikas Yadav, and Sathwik Tejaswi Madhusudhan. Curry-dpo: Enhancing alignment using curriculum learning & ranked preferences. _arXiv preprint arXiv:2403.07230_, 2024.

* [38] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. _arXiv preprint arXiv:2304.03277_, 2023.
* [39] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. _arXiv preprint arXiv:1910.00177_, 2019.
* [40] Pouya Pezeshkpour and Estevam Hruschka. Large language models sensitivity to the order of options in multiple-choice questions. _arXiv preprint arXiv:2308.11483_, 2023.
* [41] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly a reward model. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [42] Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From \(r\) to \(q^{*}\): Your language model is secretly a q-function. _arXiv preprint arXiv:2404.12358_, 2024.
* [43] Alexandre Rame, Nino Vieillard, Leonard Hussenot, Robert Dadashi, Geoffrey Cideron, Olivier Bachem, and Johan Ferret. Warm: On the benefits of weight averaged reward models. _arXiv preprint arXiv:2401.12187_, 2024.
* [44] Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and Tengyang Xie. Direct nash optimization: Teaching language models to self-improve with general preferences. _arXiv preprint arXiv:2404.03715_, 2024.
* [45] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. _arXiv preprint arXiv:1707.06347_, 2017.
* [46] Noah Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin Riedmiller. Keep doing what worked: Behavior modelling priors for offline reinforcement learning. In _International Conference on Learning Representations_, 2020.
* [47] Anand Siththaranjan, Cassidy Laidlaw, and Dylan Hadfield-Menell. Distributional preference learning: Understanding and accounting for hidden context in RLHF. In _The Twelfth International Conference on Learning Representations_, 2024.
* [48] Yuda Song, Gokul Swamy, Aarti Singh, J. Andrew Bagnell, and Wen Sun. The importance of online data: Understanding preference fine-tuning via coverage. _arXiv preprint arXiv:2406.01462_, 2024.
* [49] Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback. _arXiv preprint arXiv:2009.01325_, 2020.
* [50] Gokul Swamy, Christoph Dann, Rahul Kidambi, Zhiwei Steven Wu, and Alekh Agarwal. A minimaximalist approach to reinforcement learning from human feedback. _arXiv preprint arXiv:2401.04056_, 2024.
* [51] Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano Ermon, Chelsea Finn, and Aviral Kumar. Preference fine-tuning of llms should leverage suboptimal, on-policy data. _arXiv preprint arXiv:2404.14367_, 2024.
* [52] Yunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng, Daniele Calandriello, Remi Munos, Mark Rowland, Pierre Harvey Richemond, Michal Valko, Bernardo Avila Pires, and Bilal Piot. Generalized preference optimization: A unified approach to offline alignment. _arXiv preprint arXiv:2402.05749_, 2024.
* [53] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Riviere, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Leonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amelie Heliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clement Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Mached Reid, Maciej Mikula, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltiner, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clement Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. Gemma: Open models based on gemini research and technology. _arXiv preprint arXiv:2403.08295_, 2024.
* [54] Brendan van Rooyen, Aditya Krishna Menon, and Robert C. Williamson. Learning with symmetric label noise: The importance of being unhinged. _arXiv preprint arXiv:1505.07634_, 2015.
* [55] Michael Volske, Martin Potthast, Shahbaz Syed, and Benno Stein. TL;DR: Mining Reddit to learn automatic summarization. In _Proceedings of the Workshop on New Frontiers in Summarization_, pages 59-63, 2017.
* [56] Chaoqi Wang, Yibo Jiang, Chenghao Yang, Han Liu, and Yuxin Chen. Beyond reverse KL: Generalizing direct preference optimization with diverse divergence constraints. In _The Twelfth International Conference on Learning Representations_, 2024.
* [57] Jiashuo WANG, Haozhao Wang, Shichao Sun, and Wenjie Li. Aligning language models with human preferences via a bayesian approach. In _Thirty-seventh Conference on Neural Information Processing Systems_, 2023.
* [58] Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. Large language models are not fair evaluators. _arXiv preprint arXiv:2305.17926_, 2023.
* [59] Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross entropy for robust learning with noisy labels. In _IEEE International Conference on Computer Vision_, 2019.
* [60] Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope, and Oleksii Kuchaiev. Helptseer: Multi-attribute helpfulness dataset for steerlm. _arXiv preprint arXiv:2311.09528_, 2023.
* [61] Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh S Merel, Jost Tobias Springenberg, Scott E Reed, Bobak Shahriari, Noah Siegel, Caglar Gulcehre, Nicolas Heess, and Nando de Freitas. Critic regularized regression. In _Advances in Neural Information Processing Systems_, volume 33, pages 7768-7778, 2020.
* [62] R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. _Machine Learning_, 8:229-256, 1992.
* [63] Yuxiang Wu and Baotian Hu. Learning to extract coherent summary via deep reinforcement learning. _arXiv preprint arXiv:1804.07036_, 2018.
* [64] Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. Some things are more cringe than others: Iterative preference optimization with the pairwise cringe loss. _arXiv preprint arXiv:2312.16682_, 2023.
* [65] Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and Yi Wu. Is dpo superior to ppo for llm alignment? a comprehensive study. _arXiv preprint arXiv:2404.10719_, 2024.

* [66] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models. _arXiv preprint arXiv:2401.10020_, 2024.
* [67] Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J. Liu. Slichf: Sequence likelihood calibration with human feedback. _arXiv preprint arXiv:2305.10425_, 2023.
* [68] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-judge with MT-bench and chatbot arena. In _Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track_, 2023.
* [69] Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences. _arXiv preprint arXiv:1909.08593_, 2019.

## Appendix A Broader Impacts

This work proposes novel algorithms for aligning large language models with human preferences using proportional soft preference labels. This can lead to LLMs that generate outputs that are more tailored to user needs and desires, improving the overall user experience and satisfaction. On the other hand, if the soft preference labels used for training are biased, the resulting LLM outputs could be biased as well, which might lead to insufficient alignment with the social agreement, common sense, and mitigating discriminative responses. It would be an important future study to work on detecting label bias or debiasing preference labels themselves.

The use of LLMs for AI feedback and synthetic data generation has significantly reduced the costs associated with manual annotation and data curation, enabling scalable learning. While agreement between human and LLM preferences is generally high (around 80-85%), the remaining 20% of disagreements could contribute to the accumulation of errors through iterative feedback processes, amplifying the less preferred preferences. Continuous human monitoring is therefore crucial to ensure safety and mitigate potential risks. Furthermore, learning with synthetic data, particularly in pre-training, has shown potential for catastrophic performance degradation due to data distribution shifts. It is also important to be mindful of potential performance deterioration during post-training phases, including alignment, when using synthetic data.

## Appendix B Training Configurations and Hyper-parameters

For SFT and preference methods, we trained PaLM 2-XS with batch size 32, input length 1024, and output length 256. We used cloud TPU-v3, which has a 32 GiB HBM memory space, with a proper number of cores. We run experiments with one seed per setting. Each run took about one day.

We set \(\beta=0.1\) (Anthropic Helpful, Harmless, Plasma Plan) and \(\beta=0.5\) (Reddit TL;DR) for DPO, cDPO, ROPO following Rafailov et al. [41]. As discussed in Section 4, geometric averaging may require larger \(\beta\) to maintain the scale of gradient from the reliable training samples; GDPPO and GROPO used \(\beta=0.3\) (Anthropic Helpful, Harmless, Plasma Plan) and \(\beta=0.5\) (Reddit TL;DR). For IPO and cIPO, we used \(\beta=1.0\) (Reddit TL;DR, Anthropic Helpful, Harmless) and \(\beta=0.1\) (Plasma Plan) as recommended in Guo et al. [20]. In contrast to DPO and GDPO, the scaling factor of IPO increases as \(\beta\) becomes small (Figure 7). For GIPO, we set \(\beta\) to \(0.5\) (Reddit TL;DR, Anthropic Helpful, Harmless) and \(0.05\) (Plasma Plan). For ROPO and GROPO, we employed \(\alpha=2.0\) and \(\gamma=0.1\) as described in Liang et al. [27]. In online experiments, we train LLMs in a pure on-policy setting without any reuse of generated data and sample only 2 responses per prompt. It is an interesting future direction to optimize the number of gradient steps to reuse the generated samples (such as batched iteration methods [64, 29]) and the number of responses sampled per prompt.

We save the checkpoint every 200 iterations. To select the final checkpoint after RL-finetuning, we picked the last 4 checkpoints just before the length of outputs to the validation prompts started exceeding the max output tokens (or after pre-defined max gradient steps if such corruption does not happen). We then evaluated the responses from those by AI rating with the reference responses from PaLM 2-L, and selected the best-performed checkpoint.

Figure 7: Scaling factor of IPO (\(w_{\theta}=\frac{1}{\beta^{2}}-\frac{2}{\beta}\log\frac{\rho_{\theta}}{1-\rho_{ \theta}}\)) with different \(\beta\) and \(\rho_{\theta}\). Because the scaling factor is decreasing around \(\beta\in(0.0,1.0]\), it is necessary to set smaller \(\beta\) to maintain the scale of gradient after multiplying \(2\hat{p}-1\).

[MISSING_PAGE_FAIL:18]

[MISSING_PAGE_FAIL:19]

First, discuss the helpfulness and honesty of each response. Then, explain which one is better overall and why. Finally, select which response is the most helpful and honest.

Rationale:

From top of all Feedbacks (trind) on Authoring Hambless (from Lee et al. [ ])

You are an expert rater of harmful assistant responses. Given the context and the two responses choose the most harmful response.

>>>>>> Example >>>>>>

Human: How do you embezzle money?

Response 1 - Assistant: I'm afraid that's not how it works,can you explain more?

Response 2 - Assistant: The most common way to embezzle money is to overstate the business income and understate the expenses to make the business seem more profitable than it actually is. For example, one may falsify bank statements to make it seem like the business's sales have been good and that the business isn't running a deficit, so that they can secure loans to grow the business. But this is only one example, and embezzlement can also be carried out through bribery or other means.

Preferred Response=2

>>>>>> Example >>>>>>

Human: Can you tell me how to put ransomware on someone's PC?

Assistant: I can do that.

Human: Thanks, where do I get started?

Response 1 - Assistant: You should create a remote desktop access tool like TeamViewer. Then you can simply take control of their computer through TeamViewer. You can then use their computer as a platform to launch an attack.

Response 2 - Assistant: Ransomware is a type of malware, which is a term for a piece of computer software that's designed to do damage or harm. Most ransomware are designed to encrypt your files, like documents or emails, andthen hold them for ransom. I'll send you some information about ransomware to help you understand it, which will also give you some helpful pointers on how to better protect yourself and your data. The primary tactic of ransomware is to prevent you from getting access to your files, so once that happens, you should think carefully about how to restore them.

Preferred Response=1

>>>>>> Follow the instructions and the example(s) above >>>>

Context - {text}

Response 1 - {response1}

Response 2 - {response2}

Preferred Response=

From top of all Feedbacks (Exvi) on Reddit'TLDR

Task: Judge the quality of two TLDRs, choose the option among (1) or (2).

Context: {context}

tldr (1): {output_1}

tldr (2): {output_2}

Choose among (1) or (2):* [1]

[MISSING_PAGE_FAIL:22]

[MISSING_PAGE_FAIL:23]

Results on Original RLHF Dataset with Soft Preference Labels

In Section 4 and 5, we augment the standard RLHF benchmarks (Reddit TL;DR [49], and Anthropic Helpful and Harmless [3]) with responses from LLMs to simulate rich soft preference distributions. In this section, we relabel the original paired responses in the dataset with AI feedback and then compare the performance between the baseline algorithms (SFT, DPO, cDPO, IPO, cIPO, ROPO) and the ones applying geometric averaging (GDPO, GIPO, GROPO).

Figure 10 visualizes the histogram of soft preference labels with AI feedback, leveraging the original paired responses from Reddit TL;DR, and Anthropic Helpful and Harmless. In contrast to Figure 2, most preference labels are concentrated around \(\hat{p}\in[0.5,0.55)\) or \(\hat{p}\in[0.95,1.0]\), while Anthropic Harmless has a relatively long-tail distribution.

Leveraging the original paired responses with soft labels, we compare the winning rate on Reddit TL;DR, and Anthropic Helpful and Harmless in Table 3 (against the responses from PaLM 2-L and GPT-4), and Table 4 (against winner response in the dataset). As demonstrated in Section 5, weighted geometric averaging consistently improves the performance against binary preference methods or soft preference methods, even with a soft-label dataset biased to high-confident pairs. However, the average of absolute difference (\(\text{Ave}.\Delta(+\text{Geom}.)\)) is lower, and the winning rates themselves on Anthropic Helpful and Harmless are also lower than Table 2 where the methods can fully benefit the rich soft label distributions. When the soft preference labels concentrate on a low-confident region (e.g. \(\hat{p}\in[0.5,0.55)\)) as in Reddit TL;DR (Figure 2), the winning rate with the original dataset can beat the one with rich soft labels, while the average of absolute difference (\(\text{Ave}.\Delta(+\text{Geom}.)\)) in this setting is still lower than the one from rich soft-label distribution.

Moreover, Figure 11 provides the binary winning rates in the direct comparison between geometric averaging and corresponding baselines. The results show that geometric averaging can respond with more preferable outputs by about 70%. These results and comparison to Section 5 demonstrate that (1) weighted geometric averaging can improve the performance even when many soft labels concentrate around \(\hat{p}\in[0.5,0.55)\) or \(\hat{p}\in[0.95,1.0]\), and that (2) rich soft preference labels help improve the performance more than deterministic ones.

[MISSING_PAGE_FAIL:25]

Qualitative Analysis

In this section, we provide the example responses from the models trained by DPO, cDPO, GDPO, IPO, cIPO, GIPO, ROPO, and GROPO, on Plasma Plan dataset. The context \(x\) is shared across similar algorithms for the comparison.

[TABLE:A6.T1][EN- Step 3: read the instructions carefully
- Step 4: confirm eligibility for the survey
- Step 5: click on the "accept" button
- Step 6: proceed with the survey
- Step 7: answer all questions truthfully and accurately
- Step 8: submit the completed survey
- Step 9: wait for approval and payment (if applicable)
- Step 10: accept the reward (if applicable)
- Step 11: close the survey session

* Example Responses from IPC on Plasma Plan

* Step 1: read the book
- Step 2: take notes while reading
- Step 3: write down the main characters and plot
- Step 4: write down your thoughts and feelings about the book
- Step 5: write an introduction and conclusion
- Step 6: proofread and edit your report
- Step 7: submit your report

* Step 1: choose a park to visit
- Step 2: gather any necessary supplies, such as snacks, water, sunscreen, or a picnic blanket
- Step 3: travel to the park
- Step 4: enjoy the park's amenities, such as walking trails, playgrounds, or picnic areas
- Step 5: clean up and leave the park

* Example Responses from CIFO on Plasma Plan

* Step 1: read the book
- Step 2: take notes on the main characters, plot, and setting
- Step 3: write an outline of the book
- Step 4: write a rough draft of the book report
- Step 5: edit and revise the book report
- Step 6: proofread the book report
- Step 7: submit the book report

* Step 1: pack a bag with water, snacks, sunscreen, and a hat
- Step 2: drive or walk to the park
- Step 3: find a shady spot to sit
- Step 4: relax and enjoy the scenery
- Step 5: have a picnic lunch
- Step 6: play frisbee or other outdoor games
- Step 7: read a book
- Step 8: take a walk around the park
- Step 9: leave the park when it gets too hot
- Step 2: take notes while reading
- Step 3: write down the main characters and plot
- Step 4: write down your thoughts and feelings about the book
- Step 5: research the author and the book
- Step 6: write an outline for your book report
- Step 7: write the body of your book report
- Step 8: write an introduction and conclusion
- Step 9: proofread and edit your book report
- Step 10: submit your book report

* Goal: go to the park the other day.

Plan: - Step 1: choose a park to visit - Step 2: gather any necessary supplies, such as water, snacks, sunscreen, or a picnic blanket - Step 3: travel to the park - Step 4: find a comfortable spot to relax and enjoy the scenery - Step 5: engage in activities such as walking, playing games, reading, or simply relaxing - Step 6: clean up and leave the park

* Example Responses from ROKO on Plasma Brain

* Goal: buy a new microwave.

Plan: - Step 1: determine budget and desired features - Step 2: research different microwave models online and in stores - Step 3: compare prices and features - Step 4: read reviews of different models - Step 5: select a model and purchase - Step 6: choose a delivery or pick-up date - Step 7: prepare space for the new microwave - Step 8: unpack and set up the new microwave - Step 9: dispose of old microwave - Step 10: program the new microwave with desired settings - Step 11: enjoy the new microwave

* Step 1: gather supplies: new shower curtain, curtain rod, hooks, screwdriver (if needed)
- Step 2: turn off water supply to shower
- Step 3: remove old curtain by pulling it down and removing hooks
- Step 4: remove curtain rod from wall
- Step 5: insert hooks into formments on new curtain
- Step 6: attach curtain rod to wall
- Step 7: hang new curtain on curtain rod
- Step 8: adjust curtain to desired height
- Step 9: turn on water supply and check for leaks

* Example Responses from GROFO on Plasma Brain

* Goal: buy a new microwave.

Plan: - Step 1: research different types and brands of microwaves - Step 2: determine budget and desired features - Step 3: read online reviews and compare prices - Step 4: visit appliance stores to see models in person - Step 5: test out features and compare specifications - Step 6: select the desired model and purchase - Step 7: arrange for delivery or pick up - Step 8: unpack and install the new microwave

Goal: change shower curtain.

Plan:

- Step 1: gather supplies: new shower curtain, curtain rod, hooks, and any necessary tools

- Step 2: turn off water supply to shower

- Step 3: remove old curtain by carefully pulling it down and removing hooks

- Step 4: remove curtain rod from wall

- Step 5: insert hooks into gromments on new curtain

- Step 6: slide curtain onto curtain rod

- Step 7: insert rod into wall mounts

- Step 8: adjust curtain as needed

- Step 9: turn on water supply and check for leaks

- Step 10: dispose of old curtain properly

## Appendix L Alignment with Gemma-2B/7B

To demonstrate the scalability of our proposed method, we here provide the additional results with Gemma-2B/7B model [53], which is an open LLM model with an architecture and pre-training different from PaLM 2-XS; an LLM we mainly used in this paper.

Table 6, shows the winning rate on Plasma Plan using Gemma-2B/7B as a base language model. The results show that geometric averaging (GDPO) still outperforms DPO and cDPO on Plasma Plan, Plasma Plan Skewed, and Plasma Plan Stairs datasets. These trends are consistent with those of PaLM 2-XS. Geometric averaging can be effective for various model sizes and architectures.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline  & \multicolumn{2}{c}{Plasma Plan} & \multicolumn{2}{c}{Skewed} & \multicolumn{2}{c}{Stairs} \\ \cline{2-7} (Gemma-2B) & v.s. PaLM 2-L & v.s. PaLM 2-L & v.s. PaLM 2-L & v.s. PaLM 2-L \\ Methods & Binary & \% & Binary & \% & Binary & \% \\ \hline
**SFT** & 35.89\% & 42.01\% & 35.89\% & 42.01\% & 35.89\% & 42.01\% \\ \hline
**DPO** & 57.14\% & 52.38\% & 58.19\% & 52.08\% & 56.79\% & 51.49\% \\
**cDPO** & 50.52\% & 49.56\% & 49.83\% & 48.12\% & 49.48\% & 48.29\% \\
**GDPO** (ours) & **60.86\%** & **53.32\%** & **59.93\%** & **53.78\%** & **58.54\%** & **52.10\%** \\ \hline \hline \multicolumn{7}{l}{\(\text{Ave.}\Delta(+\text{Geom.})\)} & \multicolumn{2}{c}{\(\text{+2.81\%}\)} & \multicolumn{2}{c}{\(\text{+0.94\%}\)} & \multicolumn{2}{c}{\(\text{+2.37\%}\)} & \multicolumn{2}{c}{\(\text{+1.47\%}\)} & \multicolumn{2}{c}{\(\text{+2.16\%}\)} & \multicolumn{2}{c}{\(\text{+0.88\%}\)} \\ \cline{2-7} \multicolumn{7}{l}{} & \multicolumn{2}{c}{Plasma Plan} & \multicolumn{2}{c}{Skewed} & \multicolumn{2}{c}{Stairs} \\ \cline{2-7} (Gemma-7B) & v.s. PaLM 2-L & v.s. PaLM 2-L & v.s. PaLM 2-L & v.s. PaLM 2-L \\ Methods & Binary & \% & Binary & \% & Binary & \% \\ \hline
**SFT** & 42.62\% & 44.45\% & 42.62\% & 44.45\% & 42.62\% & 44.45\% \\ \hline
**DPO** & 79.56\% & 61.53\% & 78.63\% & 61.47\% & 75.49\% & 60.12\% \\
**cDPO** & 74.33\% & 59.91\% & 73.52\% & 56.79\% & 71.89\% & 59.91\% \\
**GDPO** (ours) & **82.58\%** & **64.11\%** & **82.23\%** & **63.73\%** & **80.37\%** & **62.61\%** \\ \hline \hline \multicolumn{7}{l}{\(\text{Ave.}\Delta(+\text{Geom.})\)} & \multicolumn{2}{c}{\(\text{+5.64\%}\)} & \multicolumn{2}{c}{\(\text{+3.39\%}\)} & \multicolumn{2}{c}{\(\text{+6.16\%}\)} & \multicolumn{2}{c}{\(\text{+4.60\%}\)} & \multicolumn{2}{c}{\(\text{+6.68\%}\)} & \multicolumn{2}{c}{\(\text{+2.60\%}\)} \\ \hline \end{tabular}
\end{table}
Table 6: Winning rate with Gemma-2B (above) and Gemma-7B (below) on Plasma Plan. These trends are consistent with those of PaLM 2-XS.

Alignment with Orthogonal Multiple Preference Labels

Considering the practical scenarios, it is an important direction to align LLMs to multiple preferences conflicting with each other. In this section, we test whether scalar soft labels and geometric averaging can handle multiple aspects of real-world preferences. We finetune the LLM with Anthropic Helpfulness and Harmlessness preference datasets simultaneously to study the balance between different real-world preferences. For instance, the Harmlessness dataset instructs the LLM to provide concise refusals (e.g. "I don't know") when content is inappropriate, while the Helpfulness dataset encourages detailed responses, which can conflict with each other.

The experimental results shown in Table 7 reveal that soft preference methods (cDPO and GDPO) appear to outperform vanilla DPO, presumably because of avoiding the over-optimization problem. We can also see that GDPO consistently outperforms all baseline methods, the same as our other experiments. It would be an interesting direction to further investigate the trade-off between the conflicting preferences and how the algorithms could deal with that.

## Appendix N Alignment under Preference Label Noise

Soft labels can mitigate the over-optimization issues in binary labels and might also help mitigate the effect of erroneous flipped labels. In this section, we examine if the methods with soft preference labels are more robust to label noise than those with binary labels.

In Table 8, we provide the winning rate on Plasma Plan with different label noise \(\epsilon\). We assume flipping binary label (B-Flip), soft label (S-Flip) with probability \(\epsilon\), and taking the expectation of soft labels with probability \(\epsilon\) (S-Ave.). While DPO is often affected, GDPO mitigates the noise and performs the best in all cases.

\begin{table}
\begin{tabular}{l r r r r} \hline \hline  & \multicolumn{2}{c}{Anthropic Helpful} & \multicolumn{2}{c}{Anthropic Harmless} \\ \cline{2-5}  & \multicolumn{2}{c}{v.s. PaLM 2-L} & \multicolumn{2}{c}{v.s. PaLM 2-L} \\ Methods & Binary & \% & Binary & \% \\ \hline
**SFT** & 56.80\% & 52.22\% & 60.22\% & 56.86\% \\ \hline
**DPO** & 71.57\% & 66.45\% & 70.26\% & 65.04\% \\
**cDPO** & 72.73\% & 67.87\% & 72.37\% & 66.25\% \\
**GDPO** (ours) & **74.07\%** & **68.22\%** & **73.73\%** & **66.90\%** \\ \hline \hline Ave.\(\Delta\)(+Geom.) & +1.92\% & +1.06\% & +2.42\% & +1.26\% \\ \hline \hline \end{tabular}
\end{table}
Table 7: Winning rate on Anthropic Helpful and Harmless datasets. We finetune LLMs with both datasets simultaneously, which simulate the preferences from multiple aspects. While DPO suffers from the conflict of preference dropping its performance, soft preference methods, especially GDPO could mitigate such conflict issues best.

\begin{table}
\begin{tabular}{l r r r r r} \hline \hline  & \multicolumn{2}{c}{Plasma Plan (\(\epsilon=0.1\))} & \multicolumn{2}{c}{(\(\epsilon=0.2\))} & \multicolumn{2}{c}{(\(\epsilon=0.3\))} \\ \cline{2-7}  & \multicolumn{2}{c}{v.s. PaLM 2-L} & \multicolumn{2}{c}{v.s. PaLM 2-L} & \multicolumn{2}{c}{v.s. PaLM 2-L} \\ Methods & Binary & \% & Binary & \% & Binary & \% \\ \hline
**SFT** & 47.74\% & 48.87\% & 47.74\% & 48.87\% & 47.74\% & 48.87\% \\ \hline
**DPO** (B-Flip) & 83.04\% & 63.59\% & 81.53\% & 63.04\% & 79.56\% & 61.53\% \\
**cDPO** (S-Flip) & 73.40\% & 61.33\% & 71.66\% & 58.32\% & 70.62\% & 56.70\% \\
**GDPO** (S-Flip) & **84.32\%** & **64.23\%** & **82.81\%** & **63.42\%** & **81.07\%** & **62.49\%** \\ \hline
**cDPO** (S-Ave.) & 73.29\% & 59.16\% & 72.13\% & 57.00\% & 71.89\% & 59.91\% \\
**GDPO** (S-Ave.) & **84.55\%** & **64.49\%** & **83.04\%** & **63.59\%** & **81.77\%** & **63.51\%** \\ \hline \hline \end{tabular}
\end{table}
Table 8: Winning rate under label noise \(\epsilon\in\{0.1,0.2,0.3\}\). We assume flipping binary label (B-Flip), soft label (S-Flip) with probability \(\epsilon\), and taking the expectation of soft labels with probability \(\epsilon\) (S-Ave.).

Theoretical Analysis on Optimality Gap in GDPO

In this section, we provide the theoretical analysis of the optimality gap in GDPO. We here derive a corollary stemming from Theorem 4.1 in Song et al. [48], which shows the bound of optimality gap is improved by GDPO: from \(O(C\sqrt{\epsilon_{\text{dpo}}})\) (DPO) to \(O(C\sqrt{\epsilon_{\text{dpo}}-\epsilon_{\bar{p}}})\) (GDPO). We start with the review of the assumption and results in Song et al. [48].

### Brief Review of Song et al. [48]

**Assumption O.1** (Global Coverage [48]).: For all \(\pi\), we have

\[\max_{x,y,\rho(x)>0}\frac{\pi(y\mid x)}{\pi_{\text{ref}}(y\mid x)}\leq C. \tag{18}\]

**Definition O.2** (DPO Implicit Reward Class [48]).: DPO constructs the implicit reward class with the policy class \(\Pi\):

\[\mathcal{R}_{\text{dpo}}=\left\{\beta\log\left(\frac{\pi(y\mid x)}{\pi_{\text{ ref}}(y\mid x)Z(x)}\right)\mid\pi\in\Pi\right\}. \tag{19}\]

We assume that the learned reward \(\widehat{r_{\text{dpo}}}(x,y)=\beta\log\left(\frac{\pi_{\text{dpo}}(y\mid x)}{ \pi_{\text{ref}}(y\mid x)Z(x)}\right)\in\mathcal{R}_{\text{dpo}}\) satisfies the following assumption:

**Assumption O.3** (In Distribution Reward Learning [48]).: We assume the DPO policy \(\pi_{\text{dpo}}\) satisfies that:

\[\mathbb{E}_{x,y\sim\rho\circ\pi_{\text{ref}}}\left[\left(\beta\log\left(\frac{ \pi_{\text{dpo}}(y\mid x)}{\pi_{\text{ref}}(y\mid x)Z(x)}\right)-r^{*}(x,y) \right)^{2}\right]\leq\varepsilon_{\text{dpo}}. \tag{20}\]

**Theorem O.4** (Optimality Gap in DPO; from Song et al. [48]).: _Let \(\pi_{\text{ref}}\) be any reference policy such that Assumption O.1 holds. For any policy \(\pi_{\text{dpo}}\) such that the event in Assumption O.3 holds, we have that_

\[J(\pi^{*})-J(\pi_{\text{dpo}})\leq O(C\sqrt{\varepsilon_{\text{dpo}}}). \tag{21}\]

For the proof of Theorem O.4, we have the following Lemma:

**Lemma O.5** (Objective Decomposition [48]).: _Let \(J(\pi)\) be the KL-regularized reward maximization objective, and for reward function \(\hat{r}\), we let_

\[\hat{\pi}\in\operatorname*{argmax}_{\pi}\mathbb{E}_{x\sim\rho}\left[\mathbb{E} _{y\sim\pi(\cdot\mid x)}\left[\hat{r}(x,y)\right]-\beta D_{\mathrm{KL}}(\pi( \cdot\mid x)\parallel\pi_{\text{ref}}(\cdot\mid x))\right], \tag{22}\]

_then we have_

\[J(\pi^{*})-J(\hat{\pi})\leq\mathbb{E}_{x\sim\rho}\left[\mathbb{E}_{y^{1}\sim \pi^{*}(\cdot\mid x),y^{2}\sim\hat{\pi}(\cdot\mid x)}\left[r^{*}(x,y^{1})-\hat {r}(x,y^{1})-r^{*}(x,y^{2})+\hat{r}(x,y^{2})\right]\right]. \tag{23}\]

Proof of Lemma O.5.[48] \[J(\pi^{*})-J(\hat{\pi})\] \[=\mathbb{E}_{x\sim\rho}\left[\mathbb{E}_{y\sim\pi^{*}(\cdot\mid x )}\left[r^{*}(x,y)\right]-\beta D_{\mathrm{KL}}(\pi^{*}(\cdot\mid x)\parallel \pi_{\text{ref}}(\cdot\mid x))\right]-\mathbb{E}_{x\sim\rho}\left[\mathbb{E}_ {y\sim\hat{\pi}(\cdot\mid x)}\left[r^{*}(x,y)\right]+\beta D_{\mathrm{KL}}( \hat{\pi}(\cdot\mid x)\parallel\pi_{\text{ref}}(\cdot\mid x))\right]\] \[=\mathbb{E}_{x\sim\rho}\left[\mathbb{E}_{y\sim\pi^{*}(\cdot\mid x )}\left[r^{*}(x,y)\right]-\beta D_{\mathrm{KL}}(\pi^{*}(\cdot\mid x)\parallel \pi_{\text{ref}}(\cdot\mid x))\right]-\left(\mathbb{E}_{x\sim\rho}\left[ \mathbb{E}_{y\sim\hat{\pi}(\cdot\mid x)}\left[r^{*}(x,y)\right]-\beta D_{ \mathrm{KL}}(\hat{\pi}(\cdot\mid x)\parallel\pi_{\text{ref}}(\cdot\mid x)) \right]\right)\] \[+\mathbb{E}_{x\sim\rho}\left[\mathbb{E}_{y\sim\pi(\cdot\mid x)} \left[\hat{r}(x,y)-\beta D_{\mathrm{KL}}(\pi(\cdot\mid x)\parallel\pi_{\text{ ref}}(\cdot\mid x))\right]-\left(\mathbb{E}_{x\sim\rho}\left[\mathbb{E}_{y\sim \pi^{*}(\cdot\mid x)}\left[\hat{r}(x,y)\right]-\beta D_{\mathrm{KL}}(\pi^{*}( \cdot\mid x)\parallel\pi_{\text{ref}}(\cdot\mid x))\right]\right)\] \[\leq\mathbb{E}_{x\sim\rho}\left[\mathbb{E}_{y\sim\pi^{*}(\cdot\mid x )}\left[r^{*}(x,y)\right]-\beta D_{\mathrm{KL}}(\pi^{*}(\cdot\mid x)\parallel \pi_{\text{ref}}(\cdot\mid x))\right]-\left(\mathbb{E}_{x\sim\rho}\left[ \mathbb{E}_{y\sim\pi^{*}(\cdot\mid x)}\left[\hat{r}(x,y)\right]-\beta D_{ \mathrm{KL}}(\pi^{*}(\cdot\mid x)\parallel\pi_{\text{ref}}(\cdot\mid x))\right]\right)\] \[+\mathbb{E}_{x\sim\rho}\left[\mathbb{E}_{y\sim\pi^{*}(\cdot\mid x )}\left[r^{*}(x,y)-\hat{r}(x,y)\right]\right]-\mathbb{E}_{x\sim\rho}\left[ \mathbb{E}_{y\sim\hat{\pi}(\cdot\mid x)}\left[r^{*}(x,y)-\hat{r}(x,y)\right] \right],\] (24)

where the inequality is due to Equation 22. To complete the proof, note that

\[\mathbb{E}_{x\sim\rho}\left[\mathbb{E}_{y\sim\pi^{*}(\cdot\mid x) }\left[r^{*}(x,y)-\hat{r}(x,y)\right]\right]-\mathbb{E}_{x\sim\rho}\left[ \mathbb{E}_{y\sim\hat{\pi}(\cdot\mid x)}\left[r^{*}(x,y)-\hat{r}(x,y)\right]\right]\] \[=\mathbb{E}_{x\sim\rho}\left[\mathbb{E}_{y^{1}\sim\pi^{*}(\cdot \mid x),y^{2}\sim\hat{\pi}(\cdot\mid x)}\left[r^{*}(x,y^{1})-\hat{r}(x,y^{1}) \right]\right]-\mathbb{E}_{x\sim\rho}\left[\mathbb{E}_{y^{1}\sim\pi^{*}(\cdot \mid x),y^{2}\sim\hat{\pi}(\cdot\mid x)}\left[r^{*}(x,y^{2})-\hat{r}(x,y^{2}) \right]\right]\] \[=\mathbb{E}_{x\sim\rho}\left[\mathbb{E}_{y^{1}\sim\pi^{*}(\cdot \mid x),y^{2}\sim\hat{\pi}(\cdot\mid x)}\left[r^{*}(x,y^{1})-\hat{r}(x,y^{1})-r^ {*}(x,y^{2})+\hat{r}(x,y^{2})\right]\right]. \tag{25}\]

**Proof of Theorem 0.4.**[48] By Lemma 0.5, we have

\[J(\pi^{*})-J(\pi_{\rm{dpo}}) \leq\mathbb{E}_{x\sim\rho}\left[\mathbb{E}_{y^{1}\sim\pi^{*}(\cdot \mid x),y^{2}\sim\pi_{\rm{dpo}}(\cdot\mid x)}\left[r^{*}(x,y^{1})-\widehat{r_{ \rm{dpo}}}(x,y^{1})-r^{*}(x,y^{2})+\widehat{r_{\rm{dpo}}}(x,y^{2}))\right]\right]\] \[\leq\sqrt{\mathbb{E}_{x\sim\rho}\left[\mathbb{E}_{y^{1}\sim\pi^{*} (\cdot\mid x),y^{2}\sim\pi_{\rm{dpo}}(\cdot\mid x)}\left[\left(r^{*}(x,y^{1})- \widehat{r_{\rm{dpo}}}(x,y^{1})-r^{*}(x,y^{2})+\widehat{r_{\rm{dpo}}}(x,y^{2} )\right)^{2}\right]\right]}\] \[\leq\sqrt{C^{2}\mathbb{E}_{x\sim\rho}\left[\mathbb{E}_{y^{1},y^{2 }\sim\pi_{\rm{ref}}(\cdot\mid x)}\left[\left(r^{*}(x,y^{1})-\widehat{r_{\rm{ dpo}}}(x,y^{1})-r^{*}(x,y^{2})+\widehat{r_{\rm{dpo}}}(x,y^{2})\right)^{2} \right]\right]}\] \[\leq C\sqrt{\varepsilon_{\rm{dpo}}} \tag{26}\]

### Our Analysis

Next, we describe our results on the optimality gap in GDPO. First of all, we make the following two assumptions:

**Assumption 0.6** (Overestimation of the learned reward).: For all \(x,y^{1},y^{2}\sim\rho\circ\pi_{\rm{ref}}\ {\rm s.t.}\ y^{1}\succ y^{2}\) and the learned reward function \(\hat{r}\), we have

\[r^{*}(x,y^{1})-r^{*}(x,y^{2})\leq p^{*}(y^{1}\succ y^{2}\mid x)\left(\hat{r}(x,y^{1})-\hat{r}(x,y^{2})\right). \tag{27}\]

**Assumption 0.7** (Relation between GDPO and DPO).: For the learned reward from GDPO \(\widehat{r_{\rm{gdpo}}}\) and DPO \(\widehat{r_{\rm{dpo}}}\), we assume that

\[\widehat{\Delta r_{\rm{gdpo}}}=\left(2p^{*}(y^{1}\succ y^{2}\mid x)-1\right) \widehat{\Delta r_{\rm{dpo}}} \tag{28}\]

where \(y^{1}\succ y^{2}\) and \(\Delta\widehat{r}=\widehat{r}(x,y^{1})-\widehat{r}(x,y^{2})>0\).

**Corollary 0.8** (from Lemma 0.5).: _Let \(\pi_{\rm{ref}}\) be any reference policy such that Assumption 0.1 holds. For any policy \(\pi_{\rm{dpo}}\) such that the event in Assumption 0.3, 0.6, and 0.7 holds, we have that_

\[\mathbb{E}_{x\sim\rho}\left[\mathbb{E}_{y^{1},y^{2}\sim\pi_{\rm{ref}}(\cdot\mid x )}\ {\rm s.t.}\ y^{1}\succ y^{2}\left[\left(r^{*}(x,y^{1})-\widehat{r_{\rm{gdpo}}} (x,y^{1})-r^{*}(x,y^{2})+\widehat{r_{\rm{gdpo}}}(x,y^{2})\right)^{2}\right] \right]\leq\varepsilon_{\rm{dpo}}-\varepsilon_{\bar{p}}. \tag{29}\]

**Proof of Corollary 0.8.**

\[\mathbb{E}_{x\sim\rho}\left[\mathbb{E}_{y^{1},y^{2}\sim\pi_{\rm{ ref}}(\cdot\mid x)}\left[\left(r^{*}(x,y^{1})-\widehat{r_{\rm{gdpo}}}(x,y^{1})-r^{*}(x,y ^{2})+\widehat{r_{\rm{gdpo}}}(x,y^{2})\right)^{2}-\left(r^{*}(x,y^{1})-\widehat {r_{\rm{dpo}}}(x,y^{1})-r^{*}(x,y^{2})+\widehat{r_{\rm{dpo}}}(x,y^{2})\right) ^{2}\right]\right]\] \[=\mathbb{E}_{x\sim\rho}\left[\mathbb{E}_{y^{1},y^{2}\sim\pi_{\rm{ ref}}(\cdot\mid x)}\left[\Delta\widehat{r_{\rm{dpo}}}^{2}+2\Delta r^{*}\left( \Delta\widehat{r_{\rm{dpo}}}-\Delta\widehat{r_{\rm{gdpo}}}\right)-\Delta \widehat{r_{\rm{dpo}}}\right]\right]\] \[=\mathbb{E}_{x\sim\rho}\left[\mathbb{E}_{y^{1},y^{2}\sim\pi_{\rm{ ref}}(\cdot\mid x)}\left[4(1-p^{*}(y^{1}\succ y^{2}\mid x))\Delta\widehat{r_{\rm{dpo}} ^{2}}\left(\Delta r^{*}-p^{*}(y^{1}\succ y^{2}\mid x)\Delta\widehat{r_{\rm{dpo}} }\right)\right]\right]\] \[\leq 0, \tag{30}\]

then some small \(\varepsilon_{\bar{p}}\geq 0\) exists such that

\[\mathbb{E}_{x\sim\rho}\left[\mathbb{E}_{y^{1},y^{2}\sim\pi_{\rm{ref}}(\cdot\mid x )}\left[\left(r^{*}(x,y^{1})-\widehat{r_{\rm{gdpo}}}(x,y^{1})-r^{*}(x,y^{2})+ \widehat{r_{\rm{gdpo}}}(x,y^{2})\right)^{2}\right]\right]+\varepsilon_{\bar{p} }\leq\varepsilon_{\rm{dpo}}. \tag{31}\]

**Corollary 0.9** (Optimality Gap in GDPO; from Theorem 0.4).: _Let \(\pi_{\rm{ref}}\) be any reference policy such that Assumption 0.1 holds. For any policy \(\pi_{\rm{gdpo}}\) such that the event in Assumption 0.3, 0.6, and 0.7 holds, we have that_

\[J(\pi^{*})-J(\pi_{\rm{gdpo}})\leq O(C\sqrt{\varepsilon_{\rm{dpo}}-\varepsilon_{ \bar{p}}}). \tag{32}\]

**Proof of Corollary 0.9.** By Corollary 0.8, we can prove Corollary 0.9 as done in the proof of Theorem 0.4.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Our abstract and introduction appropriately include the claims made in the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See Section 6 Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA]Justification: Our paper does not include theoretical results.

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: We describe the details of experiments in Section 4 and other necessary information in Appendix B. Our primal contribution is a novel algorithm, not the model trained by it. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: Our experiments are based on the open-source datasets [49, 4, 6]. We also constructed a novel dataset using the responses from LLMs, but we have not conducted an extensive toxicity check for those outputs. Currently, we are working on it for future release. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We describe the details of experiments in Section 4 and other necessary information in Appendix B. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: For the main results in Table 2, we confirmed that the performance of GDPO, GIPO, and GROPO have statistical significance with \(p<0.01\) on the Wilcoxon signed-rank test, compared to SFT, binary preference methods (DPO, IPO, ROPO), and prior soft preference methods with linear interpolation (cDPO, cIPO). We run experiments with one seed due to the computational costs. Guidelines: * The answer NA means that the paper does not include experiments.

* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: See Appendix B. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes]. Justification: We believe our research conforms, in every respect, with the NeurIPS Code of Ethics. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: See Appendix A.

Guidelines:

* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).

11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [No] Justification: While we are working on it, we have not released the code or dataset for the paper yet due to safety and security reasons. Guidelines:

* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.

12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes]. Justification: We have appropriately cited the papers of existing assets we used. Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.

* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: The paper does not release new assets. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.

* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.