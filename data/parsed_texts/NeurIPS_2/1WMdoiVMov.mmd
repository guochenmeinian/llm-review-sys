# Robust Knowledge Transfer in Tiered RL

 Jiawei Huang

Department of Computer Science

ETH Zurich

jiawei.huang@inf.ethz.ch &Niao He

Department of Computer Science

ETH Zurich

jiawei.huang@inf.ethz.ch

###### Abstract

In this paper, we study the Tiered Reinforcement Learning setting, a parallel transfer learning framework, where the goal is to transfer knowledge from the low-tier (source) task to the high-tier (target) task to reduce the exploration risk of the latter while solving the two tasks in parallel. Unlike previous work, we do not assume the low-tier and high-tier tasks share the same dynamics or reward functions, and focus on robust knowledge transfer without prior knowledge on the task similarity. We identify a natural and necessary condition called the "Optimal Value Dominance" for our objective. Under this condition, we propose novel online learning algorithms such that, for the high-tier task, it can achieve constant regret on partial states depending on the task similarity and retain near-optimal regret when the two tasks are dissimilar, while for the low-tier task, it can keep near-optimal without making sacrifice. Moreover, we further study the setting with multiple low-tier tasks, and propose a novel transfer source selection mechanism, which can ensemble the information from all low-tier tasks and allow provable benefits on a much larger state-action space.

## 1 Introduction

Comparing with individual learning from scratch, transferring knowledge from other similar tasks or side information has been proven to be an effective way to reduce the exploration risk and improve sample efficiency in Reinforcement Learning (RL). Multi-Task RL (MT-RL) [29] and Transfer RL [26; 18; 37] are two mainstream knowledge transfer frameworks; however, both are subject to limitations when dealing with real-world scenarios. MT-RL studies the setting where a set of similar tasks are solved concurrently, and the main objective is to accelerate the learning by sharing information of all tasks together. However, in practice, in many MT-RL scenarios, the tasks are not equally important and we are more interested in the performance of certain tasks. For example, in robot learning, a few robots are more valuable and hard to fix, while the others are cheaper or just simulators. Most existing works on MT-RL treat all tasks equally and focus primarily on the reduction of the total regret of all tasks as a whole [3; 8; 36; 11], with no guarantee of improving a particular task. In contrast, transfer RL distinguishes the priority of different tasks by categorizing them into source and target tasks and aims at transferring the knowledge from source tasks (or some side information like value predictors) to facilitate the learning of target tasks [21; 27; 9; 10]. However, a key assumption in transfer RL is that the source task is completely solved before the learning of the target task, and this is not always practical. For example, in some sim-to-real domain, the source task simulator may require a long time to solve [5], and in some user-interaction scenarios [13], the source and target tasks refer to different user groups and they have to be served simultaneously. In these cases, it's more reasonable to solve the source and target tasks in parallel and transfer the information immediately once available.

Recently, [13] proposed a new "parallel knowledge transfer" framework, called Tiered RL, which is promising to fill the gap. Tiered RL considers the case when a source task \(M_{\text{Lo}}\) and a target task\(M_{\text{Hi}}\) are learned in parallel, by two separate algorithms \(\text{Alg}^{\text{Lo}}\) and \(\text{Alg}^{\text{Hi}}\), and its goal is to reduce the exploration risk and regret in learning \(M_{\text{Hi}}\) by leveraging knowledge transfer from \(M_{\text{Lo}}\) to \(M_{\text{Hi}}\). [13] showed that under the strong assumption that \(M_{\text{Lo}}=M_{\text{Hi}}\), it's possible to achieve constant regret in learning \(M_{\text{Hi}}\) while keeping regret in \(M_{\text{Lo}}\) optimal in gap-dependent setting. Yet, their algorithm based on Pessimistic Value Iteration (PVI) [16] can be hardly applied when the assumption \(M_{\text{Lo}}=M_{\text{Hi}}\) breaks, given its pure exploitation nature, making their result very restrictive.

In this paper, we study the general Tiered RL setting _without prior knowledge_ about how similar the tasks are 1. The key question we would like to address is: **Can we design algorithms s.t.: (1) Regret in \(M_{\text{Lo}}\) keeps near-optimal; (2) Regret in \(M_{\text{Hi}}\) achieves provable benefits when \(M_{\text{Lo}}\) and \(M_{\text{Hi}}\) are similar while retaining near-optimal otherwise?** Note for \(\text{Alg}^{\text{Lo}}\), we expect it to achieve near-optimal regret bounds, which is reasonable since the source task is often important and our results still hold if relaxing it. As for \(\text{Alg}^{\text{Hi}}\), we expect it to be _robust_, i.e., it can adaptively exploit from \(M_{\text{Lo}}\) if it is close to \(M_{\text{Hi}}\), while avoiding negative transfer in other cases. Notably, our setting strictly generalizes [13] and is much more challenging, for balancing the exploitation from \(M_{\text{Lo}}\) and exploration from \(M_{\text{Hi}}\) without prior knowledge of task similarity. We give positive answers to the above question in this paper and demonstrate provable benefits with robust knowledge transfer for Tiered RL framework. Below, we summarize our main contributions in three aspects.

Footnote 1: We defer the detailed framework to Appx. A.1 for completeness, which is the same as Fr. 1 in [13].

**Our first contribution** is to identify essential conditions and notions about when and how our objective is achievable. We first provide a mild condition called _Optimal Value Dominance_ (OVD), and in Sec. 3, we justify its necessity to our objective by a lower bound result. Our lower bound holds even if \(M_{\text{Lo}}\) is fully known to the learner, and therefore, it also justifies the necessity of similar assumptions in previous transfer RL literatures [9; 10]. Besides, we introduce the notion of _transferable states_ to characterize states on which \(M_{\text{Hi}}\) is expected to achieve benefits by transferring knowledge from \(M_{\text{Lo}}\). We believe those findings also provide useful insights for further works.

As **our second contribution**, in Sec. 4, we propose novel algorithms for Tiered Multi-Armed Bandit (MAB) and Tiered RL, which can achieve robust parallel transfer by balancing between pessimism-based exploitation from \(M_{\text{Lo}}\) and optimism-based online learning in \(M_{\text{Hi}}\). Depending on the similarity between \(M_{\text{Lo}}\) and \(M_{\text{Hi}}\), our algorithms can enjoy constant regret on a proportion of state-action pairs or even on the entire \(M_{\text{Hi}}\) by leveraging information from \(M_{\text{Lo}}\), while timely interrupting negative transfer and retaining near-optimal regret on states dissimilar between two tasks. Moreover, in the bandit setting, our result implies a strictly improved regret bound when \(M_{\text{Hi}}=M_{\text{Lo}}\), compared with previous results under the same setting [22; 13].

Beyond the single low-tier task setting, in many real-world scenarios, it's reasonable to assume there are multiple different low-tier tasks \(\mathcal{M}_{\text{Lo}}=\{M_{\text{Lo},w}\}_{w=1}^{W}\) available. As **our third contribution**, in Sec. 5, we extend our algorithm to this setting with a new source task selection mechanism. By novel techniques, we show that, even if each \(M_{\text{Lo},w}\) may only share similarity with \(M_{\text{Hi}}\) on a small portion of states, we are able to ensemble the information from each source task together to achieve constant regret on a much larger state-action space "stitched" from the transferable state-action set in each individual \(M_{\text{Lo},w}\), at the expense of an additional \(\log W\) factor in regret. Besides, our algorithm is still robust to model difference and retains near-optimal regret in general. Although we only study the Tiered RL setting in this paper, we believe our task selection strategy can be applied to standard transfer RL setting [9; 10] when multiple (partially correct) side information or value predictors are provided, which is an interesting direction for future work. Finally, we conduct experiments in toy examples to verify our theoretical results.

### Closely Related Work

For the lack of space, we only discuss closely related work here and defer the rest to Appx. A.2. The most related to us is the Tiered RL framework [13], which was originally motivated by Tiered structure in user-oriented applications. However, they only studied the case when \(M_{\text{Hi}}=M_{\text{Lo}}\), which limits the practicability of their algorithms. Although there is a sequence of work studying parallel transfer learning in multi-agent system [25; 19], but they mainly focused on heuristic empirical algorithms and did not have theoretical guarantees.

Transfer RL [37], compared to learning from scratch, can reduces exploration risk of target task by leveraging information in similar source tasks or side information [21; 27; 9; 10]. Comparing with transfer RL setting, our parallel transfer setting has some additional challenges. Firstly, \(M_{\text{Hi}}\) can only leverage estimated value/model/optimal policy from \(M_{\text{Lo}}\) with uncertainty, which implies we need additional efforts to control failure events with non-zero probability comparing with normal transfer RL setting. Secondly, the constraints on the optimality of \(\text{Regret}_{K}(M_{\text{Lo}})\), although reasonable, restrict the transferable information because in \(M_{\text{Lo}}\) estimation uncertainty can only be controlled on those states frequently visited. Moreover, none of these previous work studies how to leverage multiple partially correct side information like what we did in Sec. 5. In MT-RL setting, the benefits of leveraging information gathered from other tasks has been observed from both empirical and theoretical works [31; 3; 8; 24; 36; 11]. But MTRL treats each task equally, and the reduction of total regret over all tasks does not directly imply benefits achieved in a particular task.

## 2 Preliminary and Problem Formulation

**Tiered Stochastic MAB and Tiered Episodic Tabular RL** In Tiered MAB setting, we consider a low-tier task \(M_{\text{Lo}}\) and a high-tier task \(M_{\text{Hi}}\) sharing the arm/action space \(\mathcal{A}=\{1,2,...,A\}\). By pulling the arm \(i\in[A]\) in \(M_{\text{Lo}}\) or \(M_{\text{Hi}}\), the agent can observe a random variable \(r_{\text{Lo}}(i)\) or \(r_{\text{Hi}}(i)\in[0,1]\). We will use \(\mu_{\text{Lo}}(i)=\mathbb{E}[r_{\text{Lo}}(i)]\) and \(\mu_{\text{Hi}}(i)=\mathbb{E}[r_{\text{Hi}}(i)]\) to denote the expected return of the \(i\)-th arm in \(M_{\text{Lo}}\) and \(M_{\text{Hi}}\), respectively, and note that it's possible that \(\mu_{\text{Lo}}(i)\neq\mu_{\text{Hi}}(i)\).

For Tiered RL, we assume that two tasks \(M_{\text{Lo}}=\{\mathcal{S},\mathcal{A},H,\mathbb{P}_{\text{Lo}},r_{\text{Lo}}\}\) and \(M_{\text{Hi}}=\{\mathcal{S},\mathcal{A},H,\mathbb{P}_{\text{Hi}},r_{\text{Hi}}\}\) share the finite state \(\mathcal{S}\) and action space \(\mathcal{A}\) across episode length \(H\) (i.e. \(\mathcal{S}_{h}=\mathcal{S},\mathcal{A}_{h}=\mathcal{A}\) for any \(h\in[H]\)), but may have different time-dependent transition and reward functions \(\mathbb{P}_{\text{Lo}}=\{\mathbb{P}_{\text{Lo},h}\}_{h=1}^{H},r_{\text{Lo}}= \{r_{\text{Lo},h}\}_{h=1}^{H}\) and \(\mathbb{P}_{\text{Hi}}=\{\mathbb{P}_{\text{Hi},h}\}_{h=1}^{H},r_{\text{Hi}}= \{r_{\text{Hi},h}\}_{h=1}^{H}\). W.l.o.g., we assume the initial state \(s_{1}\) is fixed, and the reward functions \(r_{\text{Lo}}\) and \(r_{\text{Hi}}\) are deterministic and bounded by \([0,1]\). In episodic MDPs, we study the time-dependent policy specified as \(\pi:=\{\pi_{1},...,\pi_{H}\}\) with \(\pi_{h}:\mathcal{S}_{h}\rightarrow\Delta(\mathcal{A}_{h})\) for all \(h\in[H]\), where \(\Delta(\mathcal{A}_{h})\) denotes the probability simplex over the action space. With a slight abuse of notation, when \(\pi_{h}\) is a deterministic policy, we use \(\pi_{h}:\mathcal{S}_{h}\rightarrow\mathcal{A}_{h}\) to denote the deterministic mapping. Besides, we use \(Q_{h}^{\pi}(s,a)=\mathbb{E}[\sum_{h^{\prime}=h}^{H}r_{h^{\prime}}(s_{h^{\prime }},a_{h^{\prime}})|s_{h}=s,a_{h}=a,\pi],\;V_{h}^{\pi}(s)=\mathbb{E}_{a\sim\pi} [Q_{h}^{\pi}(s,a)]\) to denote the value function for \(\pi\) at step \(h\in[H]\), and denote \(d_{h}^{\pi}(\cdot):=\Pr(s_{h}=\cdot|\pi)\) and \(d_{h}^{\pi}(\cdot,\cdot):=\Pr(s_{h}=\cdot,a_{h}=\cdot|\pi)\) as the state and state-action occupancy w.r.t. policy \(\pi\). We use \(\pi^{*}\) to denote the optimal policy, and \(h_{h}^{*},Q_{h}^{*},d_{h}^{*}\) as a short note when \(\pi=\pi^{*}\). To avoid confusion, we will specify the policy and value functions in \(M_{\text{Lo}}\) and \(M_{\text{Hi}}\) by Lo and Hi in subscription, respectively. For example, in \(M_{\text{Lo}}\) we have \(\pi_{\text{Lo}}:=\{\pi_{\text{Lo},1},...,\pi_{\text{Lo},H}\}\), \(Q_{\text{Lo},h}^{\pi_{\text{Lo}}}/V_{\text{Lo},h}^{\pi_{\text{Lo}}}\) and \(Q_{\text{Lo},h}^{*}/V_{\text{Lo},h}^{*}\), \(d_{\text{Lo},h}^{\pi_{\text{Lo}}},d_{\text{Lo},h}^{*}\), and similarly for \(M_{\text{Hi}}\).

**Gap-Dependent Setting** Throughout, we focus on gap-dependent setting [17; 23; 34; 7]. Below we introduce the notion of gap for \(M_{\text{Lo}}\) as an example, and those for \(M_{\text{Hi}}\) follows similarly. In MAB case, the gap in \(M_{\text{Lo}}\) w.r.t. arm \(i\) is defined as \(\Delta_{\text{Lo}}(i):=\max_{j\in[A]}\mu_{\text{Lo}}(j)-\mu_{\text{Lo}}(i), \forall i\in[A]\), and for tabular RL setting, we have \(\Delta_{\text{Lo}}(s_{h},a_{h}):=V_{\text{Lo},h}^{*}(s_{h})-Q_{\text{Lo},h}^{*} (s_{h},a_{h}),\forall h\in[H],s_{h}\in\mathcal{S}_{h},a_{h}\in\mathcal{A}_{h}\). We use \(\Delta_{\text{Lo},\min}\) to refer to the minimal gap such that \(\Delta(s_{h},a_{h})\geq\Delta_{\text{Lo},\min}\) for all non-optimal actions \(a_{h}\), and use \(\Delta_{\min}:=\min\{\Delta_{\text{Lo},\min},\Delta_{\text{Hi},\min}\}\) to denote the minimal gap over two tasks. In the gap-dependent setting, we assume \(\Delta_{\min}>0\).

**Knowledge Transfer from Multiple Low-Tier Tasks** In this case, we assume there are \(W>1\) different source tasks \(\mathcal{M}_{\text{Lo}}=\{M_{\text{Lo},w}\}_{w=1}^{W}\) and all the tasks share the same state and action spaces but may have different transition and reward function. We defer the extended framework for this setting to Appx. A. We specify the task index \(w\in[W]\) in sub-scription to distinguish the notation for different source tasks (e.g. \(\mathbb{P}_{\text{Lo},w,h},Q_{\text{Lo},w,h}^{(\cdot)}\)). Moreover, we define \(\Delta_{\min}:=\min\{\Delta_{\text{Lo},1,\min},...,\Delta_{\text{Lo},W,\min}, \Delta_{\text{Hi},\min}\}\). For convenience, in the rest of the paper, we use TRL-MST (Tiered RL with Multiple Source Task) as a short abbreviation for this setting.

**Performance Measure** We use Pseudo-Regret as performance measure: \(\text{Regret}_{K}(M_{\text{Lo}}):=\mathbb{E}\left[\sum_{k=1}^{K}V_{1}^{*}(s_{1})-V_{ 1}^{\pi_{\text{Lo}}^{k}}(s_{1})\right];\;\;\text{Regret}_{K}(M_{\text{Hi}}):= \mathbb{E}\left[\sum_{k=1}^{K}V_{1}^{*}(s_{1})-V_{1}^{\pi_{\text{Hi}}^{k}}(s_{ 1})\right],\) where \(K\) is the number of iterations, \(\{\pi_{\text{Lo}}^{k}\}_{k=1}^{K}\) and \(\{\pi_{\text{Hi}}^{k}\}_{k=1}^{K}\) are generated by the algorithms.

**Frequently Used Notations** We denote \([n]=\{1,2,...,n\}\). Given a transition matrix \(\mathbb{P}:\mathcal{S}\times\mathcal{A}\rightarrow\Delta(\mathcal{S})\), and a function \(V:\mathcal{S}\rightarrow\mathbb{R}\), we use \(\mathbb{P}V(s_{h},a_{h})\) as a short note of \(\mathbb{E}_{s^{\prime}\sim\mathbb{P}(\cdot|s,a)}[V(s^{\prime})]\). We will use \(i_{\text{Lo}}^{*}/i_{\text{Hi}}^{*}\) to denote the optimal arm in bandit setting, and \(\pi_{\text{Lo}}^{*}/\pi_{\text{Hi}}^{*}\) denotes the optimal policy in RL setting. In TRL-MST setting, we use \(i_{\text{Lo},w}^{*}/\pi_{\text{Lo},w}^{*}\) to distinguish different source tasks.

### Assumptions and Characterization of Transferable States

Throughout the paper, we make several assumptions. The first one is the uniqueness of the optimal policy, which is common in the literature [22; 4].

**Assumption A**.: Both \(M_{\text{Lo}}\) (or \(\{M_{\text{Lo},w}\}_{w=1}^{W}\)) and \(M_{\text{Hi}}\) have unique optimal arms/policies.

Next, we introduce a new concept called "Optimal Value Dominance" (OVD for short), which says that for each state (or at least those states reachable by optimal policy in \(M_{\text{Lo}}\)), the optimal value of \(M_{\text{Lo}}\) is an approximate overestimation for the optimal value of \(M_{\text{Hi}}\). In Sec. 3, we will use a lower bound to show such a condition is necessary to attain the robust transfer objective.

**Assumption B**.: In single source task setting, we assume \(M_{\text{Lo}}\) has Optimal Value Dominance (OVD) over \(M_{\text{Hi}}\), s.t., \(\forall h\in[H]\), for all \(s_{h}\in\mathcal{S}_{h}\) (or only for those \(s_{h}\) with \(d^{\star}_{\text{Lo},h}(s_{h})>0\)), we have: \(V^{\star}_{\text{Lo},h}(s_{h})\geq V^{\star}_{\text{Hi},h}(s_{h})-\frac{ \Delta_{\min}}{2(H+1)}\). In TRL-MST setting, we assume each \(M_{\text{Lo},w}\) has OVD over \(M_{\text{Hi}}\).2

Footnote 2: For convenience, we include the bandit setting as a special case with \(H=0\); see also Def. 2.1, 2.2 and F.1.

We remark that Assump. B is a rather mild condition that naturally holds with reward shaping. Note that since \(V^{\star}_{\text{Hi},h}(\cdot)\leq H-h\), by shifting the reward function of \(M_{\text{Lo}}\) to \(r^{\prime}_{\text{Lo},h}(\cdot,\cdot)=r_{\text{Lo},h}(\cdot,\cdot)+1\), we immediately obtain the OVD property. Even though, such a reward shift may impair the set of transferable states as we will introduce in Def. 2.2. We provide several reasonable settings in Appx. A.3 including identical model [13], small model difference, and known model difference, where Assump. B is satisfied and there exists a non-empty set of transferable states. We also point out that several existing work on transfer RL assumed something similar or even stronger [9; 10], which we defer a thorough discussion to Appx. A.2.

**Assumption C**.: The learner has access to a quantity \(\widetilde{\Delta}_{\min}\) satisfying \(0\leq\widetilde{\Delta}_{\min}\leq\Delta_{\min}\).

The final one is about the knowledge of a lower bound of \(\Delta_{\min}\), which can always be satisfied by choosing \(\widetilde{\Delta}_{\min}=0\). Nevertheless, it would be more beneficial if the learner has access to some quantity \(\widetilde{\Delta}_{\min}\) closer to \(\Delta_{\min}\) than 0. As we introduce below, the magnitude of \(\widetilde{\Delta}_{\min}\) is related to how we quantify the similarity between \(M_{\text{Lo}}\) and \(M_{\text{Hi}}\) and which states we expect to benefit from knowledge transfer. Below we focus on the single source task setting and defer the discussion for TRL-MST setting to Sec. 5.

**Definition 2.1** (\(\varepsilon\)-Close).: Task \(M_{\text{Hi}}\) is \(\varepsilon\)-close to task \(M_{\text{Lo}}\) on \(s_{h}\) at step \(h\) for some \(\varepsilon>0\), if \(V^{\star}_{\text{Lo},h}(s_{h})-V^{\star}_{\text{Hi},h}(s_{h})\leq\varepsilon\) and \(\pi^{\star}_{\text{Hi}}(s_{h})=\pi^{\star}_{\text{Lo}}(s_{h})\).

**Definition 2.2** (\(\lambda\)-Transferable States).: State \(s_{h}\) is \(\lambda\)-transferable for some \(\lambda>0\), if \(d^{\star}_{\text{Lo}}(s_{h})>\lambda\) and \(M_{\text{Hi}}\) is \(\frac{\widetilde{\Delta}_{\min}}{4(H+1)}\)-close to \(M_{\text{Lo}}\) on \(s_{h}\). The set of \(\lambda\)-transferable states at \(h\in[H]\) is denoted as \(\mathcal{Z}^{\lambda}_{h}\).

We regard \(s_{h}\) in \(M_{\text{Lo}}\) has transferable knowledge to \(M_{\text{Hi}}\), if it can be reached by optimal policy in \(M_{\text{Lo}}\) and the optimal value and action at \(s_{h}\) for two tasks are similar. Here the condition \(d^{\star}_{\text{Lo}}(s_{h})>0\) is necessary since in \(M_{\text{Lo}}\), only the states reachable by \(\pi^{\star}_{\text{Lo}}\) can be explored sufficiently by \(\text{Alg}^{\text{Lo}}\) due to its near optimal regret. Combining with Assump. B, one can observe that the value difference on transferable states are controlled by \(|V^{\star}_{\text{Lo},h}(s_{h})-V^{\star}_{\text{Hi},h}(s_{h})|=O(\frac{ \widetilde{\Delta}_{\min}}{H})\leq O(\frac{\Delta_{\min}}{H})\). As we will show in Thm. 3.2 in Sec. 3, the term \(O(\Delta_{\min})\) is indeed unimprovable if we expect robustness.

## 3 Lower Bound Results: Necessary Condition for Robust Transfer

Now we establish lower bounds that show Assump. B is necessary and how the magnitude of \(\Delta_{\min}\) restricts the robust transfer objective. The results in this section are based on two-armed Bernoulli bandits for simplicity, and the proofs are deferred to Appx. B. By extending these hard instances to RL case, there is a gap caused by the additional \(\frac{1}{H}\) in Assump. B and Def. 2.2, which comes from the requirement of our algorithm design, and we leave it to the future work.

**Justification for Assump. B** We show that if Assump. B is violated, it is impossible to have algorithms \((\text{Alg}^{\text{Lo}},\text{Alg}^{\text{Hi}})\) to simultaneously achieve constant regret when \(M_{\text{Lo}}=M_{\text{Hi}}\), while retaining sub-linear regret for all the cases regardless of the similarity between \(M_{\text{Lo}}\) and \(M_{\text{Hi}}\). Here we require constant regret on \(M_{\text{Lo}}=M_{\text{Hi}}\) since we believe it is a minimal expectation to achieve benefits in transfer whenthe two tasks are identical. Intuitively, without Assump. B, even if we know \(\mu_{\text{Lo}}(i_{\text{Lo}}^{*})=\mu_{\text{Hi}}(i_{\text{Lo}}^{*})\), we cannot ensure \(i_{\text{Lo}}^{*}\) is the optimal arm in \(M_{\text{Hi}}\). Then, if \((\text{Alg}^{\text{Lo}},\text{Alg}^{\text{Hi}})\) can achieve constant regret on \(M_{\text{Lo}}=M_{\text{Hi}}\), the algorithm must stop exploration on the arm \(i\neq i_{\text{Lo}}^{*}\) after finite steps, and thus, it suffers from linear regret in another instance of \(M_{\text{Hi}}\) where \(i_{\text{Lo}}^{*}\neq i_{\text{Hi}}^{*}\).

Moreover, Thm. 3.1 holds even if the learner has full information of \(M_{\text{Lo}}\), where the setting degenerates to normal transfer RL since there is no need to explore \(M_{\text{Lo}}\). This explains why similar assumptions to Assump. B are considered in previous transfer RL works [9; 10].

**Theorem 3.1**.: _Under the violation of Assump. B, even regardless of the optimality of \(\text{Alg}^{\text{Lo}}\), for each algorithm pair \((\text{Alg}^{\text{Lo}},\text{Alg}^{\text{Hi}})\), it cannot simultaneously (1) achieve constant regret for the case when \(M_{\text{Lo}}=M_{\text{Hi}}\) and (2) ensure sub-linear regret in all the other cases._

\(\Delta_{\min}\) in Tolerance Error is InevitableNext, we show that, if \(M_{\text{Hi}}\) and \(M_{\text{Lo}}\) are \(\Delta\)-close for some \(\Delta\geq\frac{\Delta_{\min}}{2}\), in general we cannot expect to achieve constant regret on \(M_{\text{Hi}}\) by leveraging \(M_{\text{Lo}}\) without other loss. Similar to Thm. 3.1, the main idea is to construct different instances for \(M_{\text{Hi}}\) with different optimal arms and cannot be distinguished within finite number of trials.

**Theorem 3.2**.: _[Transferable States are Restricted by \(\Delta_{\min}\)] Under Assump. B, regardless of the optimality of \(\text{Alg}^{\text{Lo}}\), given arbitrary \(\Delta_{\min}\) and arbitrary \(\Delta\in[\frac{\Delta_{\min}}{2},\Delta_{\min}]\), for each algorithm pair \((\text{Alg}^{\text{Lo}},\text{Alg}^{\text{Hi}})\), it cannot simultaneously (1) achieve constant regret for the case when \(M_{\text{Lo}}\) and \(M_{\text{Hi}}\) with minimal gap \(\Delta_{\min}\) are \(\Delta\)-close, and (2) ensure sub-linear regret in all other cases._

## 4 Robust Tiered MAB/RL with Single Source Task

In this section, we study Tiered MAB and Tiered RL when a single low-tier task \(M_{\text{Lo}}\) is available. The key challenge compared with [13] is that we do not have knowledge about whether \(M_{\text{Lo}}\) and \(M_{\text{Hi}}\) are similar or not so the pure exploitation will not work. Instead, the algorithm should be able to identify whether \(M_{\text{Lo}}\) and \(M_{\text{Hi}}\) are close enough to transfer by data collected so far, and balance between the exploration by itself and the exploitation from \(M_{\text{Lo}}\) at the same time.

To overcome the challenge, we identify a state-wise checking event, such that, under Assump. B, if \(s_{h}\) is transferable, the event is true almost all the time, and otherwise, every mistake will reduce the uncertainty so the chance the event holds is limited. By utilizing it, our algorithm can wisely switch between optimistic exploration and pessimistic exploitation and achieve robust transfer. In Sec. 4.1, we start with the MAB setting and illustrate the main idea, and in Sec. 4.2, we generalize our techniques to RL setting, and discuss how to overcome the challenges brought by state transition.

### Robust Transfer in Tiered Multi-Armed Bandits

The algorithm is provided in Alg. 1. We choose \(\text{Alg}^{\text{Lo}}\) as UCB, and \(\text{Alg}^{\text{Hi}}\) as an exploitation-or-UCB style algorithm branched by a checking event in line 7, which is the key step to avoid negative transfer.

```
1Initialize:\(\alpha>2;\ N_{\text{Lo}}^{1}(i),\ N_{\text{Hi}}^{1}(i),\ \widehat{\mu}_{\text{Lo}}^{1}(i),\ \widehat{\mu}_{\text{Hi}}^{1}(i)\gets 0,\ \forall i\in\mathcal{A};\)\(f(k):=1+16A^{2}(k+1)^{2}\)
2 Pull each arm at the beginning \(A\) iterations
3for\(k=A+1,A+2,...,K\)do
4\(\overline{\mu}_{\text{Lo}}^{k}(i)\leftarrow\widehat{\mu}_{\text{Lo}}^{k}(i)+ \sqrt{\frac{2\alpha\log f(k)}{N_{\text{Lo}}^{k}(i)}},\quad\overline{\pi}_{ \text{Lo}}^{k}\leftarrow\arg\max_{i}\overline{\mu}_{\text{Lo}}^{k}(i),\quad \pi_{\text{Lo}}^{k}\leftarrow\overline{\pi}_{\text{Lo}}^{k}.\)
5\(\underline{\mu}_{\text{Lo}}^{k}(i)\leftarrow\widehat{\mu}_{\text{Lo}}^{k}(i)- \sqrt{\frac{2\alpha\log f(k)}{N_{\text{Lo}}^{k}(i)}},\quad\overline{\pi}_{ \text{Lo}}^{k}\leftarrow\arg\max_{i}\underline{\mu}_{\text{Lo}}^{k}(i).\)
6\(\overline{\mu}_{\text{Hi}}^{k}(i)\leftarrow\widehat{\mu}_{\text{Hi}}^{k}(i)+ \sqrt{\frac{2\alpha_{\text{Lo}}\log f(k)}{N_{\text{Hi}}^{k}(i)}},\quad \overline{\pi}_{\text{Hi}}^{k}\leftarrow\arg\max_{i}\overline{\mu}_{\text{Hi}}^{k}(i).\)
7if\(\underline{\mu}_{\text{Lo}}^{k}(\underline{\pi}_{\text{Lo}}^{k})\leq\overline{\mu}_{ \text{Hi}}^{k}(\underline{\pi}_{\text{Lo}}^{k})+\varepsilon\) and \(N_{\text{Lo}}^{k}(\underline{\pi}_{\text{Lo}}^{k})>k/2\)then\(\pi_{\text{Hi}}^{k}\leftarrow\underline{\pi}_{\text{Lo}}^{k}\)else\(\pi_{\text{Hi}}^{k}\leftarrow\overline{\pi}_{\text{Hi}}^{k}.\)
8 Interact \(M_{\text{Hi}}/M_{\text{Lo}}\) by \(\pi_{\text{Hi}}^{k}/\pi_{\text{Lo}}^{k}\); Update \(N_{\text{Lo}}^{k+1}/N_{\text{Hi}}^{k+1}\) and empirical mean \(\widehat{\mu}_{\text{Lo}}^{k+1}/\widehat{\mu}_{\text{Hi}}^{k+1}\).
9 end for
```

**Algorithm 1**Robust Tiered MAB

Key Insights: Separation between Transferable and Non-Transferable CasesTo understand our checking event, we consider the following two cases: (1) \(M_{\text{Lo}}\) and \(M_{\text{Hi}}\) are \(\varepsilon\)-close, and (2)(in the rest cases we have \(i^{*}_{\text{Hi}}=i^{*}_{\text{Lo}}\) but \(\mu_{\text{Lo}}(i^{*}_{\text{Lo}})>\mu_{\text{Hi}}(i^{*}_{\text{Lo}})+\varepsilon\), so exploiting from \(M_{\text{Lo}}\) is harmless). Recall Assump. B, in Case 1, we have \(\mu_{\text{Lo}}(i^{*}_{\text{Lo}})\leq\mu_{\text{Hi}}(i^{*}_{\text{Lo}})+\varepsilon\), while in Case 2, with an appropriate choice of \(\varepsilon\) (e.g. \(\varepsilon=\frac{\tilde{\Delta}_{\min}}{4}<\frac{\Delta_{\min}}{4}\)), we have \(\mu_{\text{Lo}}(i^{*}_{\text{Lo}})\geq\mu_{\text{Hi}}(i^{*}_{\text{Ho}})- \frac{\Delta_{\min}}{2}\geq\mu_{\text{Hi}}(i^{*}_{\text{Lo}})+\varepsilon+ \frac{\Delta_{\text{B}}(i^{*}_{\text{Lo}})}{2}\), which reveals the separation between two cases. As a result, if we can construct an uncertainty-based upper bound \(\overline{\mu}^{k}_{\text{Hi}}(i^{*}_{\text{Lo}})\) for \(\mu_{\text{Hi}}(i^{*}_{\text{Lo}})\), we should expect the event \(\mathcal{E}:=\mu_{\text{Lo}}(i^{*}_{\text{Lo}})<\overline{\mu}^{k}_{\text{ Hi}}(i^{*}_{\text{Lo}})+\varepsilon\) almost always be true in Case 1, while in Case 2, everytime \(\mathcal{E}\) occurs and \(\text{Alg}^{\text{{\small fit}}}\) takes \(i^{*}_{\text{Lo}}\), the "self-correction" is triggered: the uncertainty is reduced so the estimation \(\overline{\mu}^{k}_{\text{Hi}}(i^{*}_{\text{Lo}})\) gets closer to \(\mu_{\text{Lo}}(i^{*}_{\text{Lo}})\), and because of the separation between \(\mu_{\text{Lo}}(i^{*}_{\text{Lo}})\) and \(\mu_{\text{Hi}}(i^{*}_{\text{Lo}})\), the number of times that \(\mathcal{E}\) is true is limited. The remaining issue is that we do not know \(\mu_{\text{Lo}}(i^{*}_{\text{Lo}})\) and \(i^{*}_{\text{Lo}}\), and we approximate them with LCB value \(\underline{\mu}^{k}_{\text{Lo}}(\cdot)\) and its greedy policy. The additional checking event \(N^{k}_{\text{Lo}}(\underline{\pi}^{k}_{\text{Lo}})>k/2\) is used to increase the confidence that \(\underline{\pi}^{k}_{\text{Lo}}=i^{*}_{\text{Lo}}\) once transfer, which also contributes to reducing the regret. Finally, to achieve constant regret, we use \(\alpha\) to control the total failure rate to be \(\sum_{k=1}^{+\infty}k^{-\Theta(\alpha)}=C\) for some constant \(C\). We summarize the main result in Thm. 4.1 and defer the proof to Appx. C.

**Theorem 4.1**.: _[Tiered MAB with Single Source Tasks] Under Assump. A, B and C, by running Alg. 1 with \(\varepsilon=\frac{\tilde{\Delta}_{\min}}{4}\) and \(\alpha>2\), we always have \(\text{Regret}_{K}(M_{\text{Hi}})=O(\sum_{\Delta_{\text{Hi}}(i)>0}\frac{1}{ \Delta_{\text{Hi}}(i)}\log K)\). Moreover, if \(M_{\text{Hi}}\) and \(M_{\text{Lo}}\) are \(\frac{\tilde{\Delta}_{\min}}{4}\)-close, we have: \(\text{Regret}_{K}(M_{\text{Hi}})=O(\sum_{\Delta_{\text{Hi}}(i)>0}\frac{1}{ \Delta_{\text{Hi}}(i)}\log\frac{A}{\Delta_{\min}})\)._

**Comparison with [22; 13]** As we can see, our algorithm can automatically achieve constant regret if tasks are similar while retaining near-optimal otherwise. Notably, even when \(M_{\text{Hi}}=M_{\text{Lo}}\), our regret bound \(\widetilde{O}(\sum_{\Delta_{\text{Hi}}(i)>0}\frac{1}{\Delta_{\text{Hi}}(i)})\) is strictly better than \(\widetilde{O}(\sqrt{\frac{A}{\Delta_{\min}}}\sqrt{\sum_{\Delta_{\text{Hi}}(i)> 0}\frac{1}{\Delta_{\text{Hi}}(i)}})\) in [22] and \(\widetilde{O}(\sum_{\Delta_{\text{Hi}}(i)>0}(A-i)(\frac{1}{\Delta_{\text{Hi} }(i)}-\frac{\Delta_{\text{Hi}}(i)}{\Delta_{\text{Hi}}(i-1)^{2}}))\) in [13] under the same setting.

### Robust Transfer in Tiered Tabular RL

In this section, we focus on RL setting. We provide the algorithm in Alg. 2, where we defer the details of **ModelLearning** function and the requirements for **Bonus** function to Appx. D.1. In the following, we first highlight our main result.

**Theorem 4.2**.: _[Tiered RL with Single Source Tasks] Under Assump. A, B and C, Cond. D.1 for \(\text{Alg}^{Lo}\) and Cond. D.3 for **Bonus** function, by running Alg. 2 with \(\varepsilon=\frac{\tilde{\Delta}_{\min}}{4(H+1)}\), \(\alpha>2\), an arbitrary \(\lambda>0\), we have_

\[\text{Regret}_{K}(M_{\text{Hi}})=O\Big{(}SH\sum_{h=1}^{H}\sum_{(s_{h},a_{h}) \in\mathcal{S}_{h}\times\mathcal{A}_{h}\setminus\mathcal{C}^{\lambda}_{h}}( \frac{H}{\Delta_{\min}}\wedge\frac{1}{\Delta_{\text{Hi}}(s_{h},a_{h})})\log( SAHK)\Big{)}.\]

Here the set \(\mathcal{C}^{\lambda}_{h}\subset\mathcal{S}_{h}\times\mathcal{A}_{h}\) captures the benefitable state-action pairs to be introduced later. For simplicity, in Thm. 4.2 above, we omit all constant terms independent with \(K\) that may include \(\lambda^{-1}\), \(\Delta_{\min}^{-1}\) or \(\log 1/d^{*}_{h}(\cdot)\). The complete version of Thm. 4.2 can be found in Thm. D.16. As we can see, comparing with pure online learning algorithms [23; 34; 7], in our setting, \(M_{\text{Hi}}\) only suffers non-constant regret on a subset of the state-action space. The \(SH\) factor may be further improved by choosing better **Bonus** functions than our Example D.4 given in Appx. D.1.

Different from the bandit setting, the state transition causes more challenges. In the following, we first explain the algorithm design to highlight how we overcome the difficulties, and then provide the analysis and proof sketch. Detailed proofs can be found in Appx. D.

**Technical Challenges and Algorithm Design** Similar to MAB setting, for \(M_{\text{Lo}}\) we choose an arbitrary near-optimal algorithm, and for \(M_{\text{Hi}}\) we set up a state-wise checking event \(\underline{V}^{k}_{\text{Lo},h}(\cdot)\leq\widetilde{Q}^{k}_{\text{Hi},h}(\cdot,\pi^{k}_{\text{Lo},h})+\varepsilon\) in line 15 to determine whether to exploit from \(M_{\text{Lo}}\) or not. Here \(V^{k}_{\text{Lo},h}\) and \(\widetilde{Q}^{k}_{\text{Hi},h}\) serve as lower and upper bounds for \(V^{*}_{\text{Lo}}\) and \(Q^{*}_{\text{Hi}}\), and \(\underline{V}^{k}_{\text{Lo},h}\) and \(\underline{\pi}^{k}_{\text{Lo},h}\) are constructed by Pessimistic Value Iteration [16], which can be shown to converge to \(V^{*}_{\text{Lo},h}\) and \(\pi^{*}_{\text{Lo},h}\), respectively. Similar to [13], for the choice of \(\text{Alg}^{\text{Lo}}\) and the bonus term used to construct lower/upper confidence estimation, we consider general algorithm framework under Cond. D.1 and Cond. D.3 in Appx. D.1.

To overcome challenges resulting from state transition, we make two major modifications when moving from MAB to RL setting. First of all, because of the constraint on the optimality of \(\text{Alg}^{\text{Lo}}\)we cannot expect \(M_{\text{Lo}}\) to provide useful information on those \((s_{h},a_{h})\) with \(d^{*}_{\text{Lo}}(s_{h},a_{h})=0\) since they will not be explored sufficiently. Therefore, in the checking event in line 15, we include \(\max_{a}N^{k}_{\text{Lo},h}(s_{h},a)>\Theta(\lambda k)\) as a criterion, where \(\lambda\) is a hyper-parameter chosen and input to the algorithm. Intuitively, for all \(s_{h},a_{h}\), we should expect \(N^{k}_{\text{Lo},h}(s_{h},a_{h})\approx\widetilde{O}(d^{*}_{\text{Lo}}(s_{h}, a_{h})\cdot k)\) when \(k\) is large enough. Therefore, by comparing \(N^{k}_{\text{Lo},h}\) with \(\lambda k\), we can filter out those \(s_{h}\) with \(d^{*}_{\text{Lo}}(s_{h})<\lambda\) to avoid harm from inaccurate estimation.

Secondly and more importantly, different from MAB setting, besides the error occurred at a particular step, we also need to handle the error accumulated during the back-propagation process of value iteration. In our case, this is reflected by the loss of overestimation when we incorporate selective exploitation into the optimism-based exploration framework. To see this, suppose at some \(s_{h}\), we have an overestimation on optimal value \(Q^{*}_{\text{Hi},h}\) denoted as \(\widetilde{Q}^{k}_{\text{Hi},h}\). When the checking criterion is satisfied, if we mimic the MAB setting, i.e., assign \(\pi^{k}_{\text{Lo},h}\) to \(\pi^{k}_{\text{Hi},h}\) and update value by \(\widetilde{V}^{k}_{\text{Hi},h}(s_{h})\leftarrow\widetilde{Q}^{k}_{\text{Hi},h}(s_{h},\pi^{k}_{\text{Hi},h})\), when \(\pi^{*}_{\text{Lo}}(s_{h})\neq\pi^{*}_{\text{Hi},h}(s_{h})\), \(\widetilde{V}^{k}_{\text{Hi},h}(s_{h})\) is no longer guaranteed to be an overestimation for \(V^{*}_{\text{Hi},h}(s_{h})\). As \(\widetilde{V}^{k}_{\text{Hi},h}(s_{h})\) involves in back-propagation, it will pull down the estimation value for its ancestor states, thus reducing the chance to visit \(s_{h}\) and slowing down the "self-correction process" which works well in MAB setting.

```
1Input: Ratio \(\lambda\in(0,1)\); \(\alpha>2\); Auxiliary functions Bonus and ModelLearning; Sequence of confidence level \((\delta_{k})_{k\geq 1}\) with \(\delta_{k}=1/SAHk^{\alpha}\); \(\varepsilon:=\widetilde{\Delta}_{\min}/4(H+1)\) for some \(\widetilde{\Delta}_{\min}\leq\Delta_{\min}\)
2Initialize:\(D^{0}_{\text{Lo}}/D^{0}_{\text{Hi}}\leftarrow\{\};\forall k,\;\underline{V}^ {k}_{(\cdot),H+1},\underline{Q}^{k}_{(\cdot),H+1},\widetilde{V}^{k}_{\text{ Hi},H+1},\widetilde{Q}^{k}_{\text{Hi},H+1}\gets 0\).
3for\(k=1,2,...\)do
4\(\pi_{\text{Lo}}\leftarrow\text{Alg}^{\text{Lo}}(D^{k-1}_{\text{Lo}})\);
5\(\{\widetilde{\mathbb{P}}^{k}_{\text{Lo},h}\}_{H=1}^{H}\leftarrow\textbf{ ModelLearning}(D^{k-1}_{\text{Lo}})\), \(\{b^{k}_{\text{Lo},h}\}_{h=1}^{H}\leftarrow\textbf{Bonus}(D^{k-1}_{\text{Lo}},\;\delta_{k})\).
6for\(h=H,H-1...,1\)do
7\(Q^{k}_{\text{Hi},h}(\cdot,\cdot)\leftarrow\max\{0,r_{\text{Hi},h}(\cdot,\cdot) +\widehat{\mathbb{P}}^{k}_{\text{Hi},h}\underline{V}^{k}_{\text{Lo},h+1}( \cdot,\cdot)-b^{k}_{\text{Lo},h}(\cdot,\cdot)\}\)
8\(\widetilde{Q}^{k}_{\text{Hi},h}(\cdot,\cdot)\leftarrow\min\{H,r_{\text{Hi},h} (\cdot,\cdot)+\widehat{\mathbb{P}}^{k}_{\text{Hi},h}\widetilde{V}^{k}_{\text{ Hi},h+1}(\cdot,\cdot)+b^{k}_{\text{Hi},h}(\cdot,\cdot)\}\).
9for\(s_{h}\in\mathcal{S}_{h}\)do
10if\(\underline{V}^{k}_{\text{Lo},h}(s_{h})\leq\widetilde{Q}^{k}_{\text{Hi},h}(s_{h}, \underline{\pi}^{k}_{\text{Lo},h})+\varepsilon\) and \(\max_{a}N^{k}_{\text{Lo},h}(s_{h},a)>\frac{\lambda}{3}k\)then
11\(\pi^{k}_{\text{Hi}}(s_{h})\leftarrow\arg\max_{a}N^{k}_{\text{Lo},h}(s_{h},a)\). / "Trust and Exploit" Branch
12
13 end if
14else\(\pi^{k}_{\text{Hi}}(\cdot)\leftarrow\arg\max_{a}\widetilde{Q}^{k}_{\text{Hi},h}(\cdot,a)\). / "Explore by itself" Branch ;
15\(\widetilde{V}^{k}_{\text{Hi},h}(s_{h})\leftarrow\min\{H,\widetilde{Q}^{k}_{ \text{Hi},h}(s_{h},\pi^{k}_{\text{Hi}})+\frac{1}{H}(\widetilde{Q}^{k}_{\text{ Hi},h}(s_{h},\pi^{k}_{\text{Hi}})-\underline{Q}^{\pi^{k}_{\text{Hi}}}(s_{h},\pi^{k}_{\text{Hi}}))\}\)
16\(\underline{V}^{\pi^{k}_{\text{Hi},h}}(s_{h})=Q^{\pi^{k}_{\text{Hi}}}_{\text{Hi},h}(s_{h},\pi^{k}_{\text{Hi}})\).
17
18 end if
19
20 end for
21
22 end for
23
24
```

**Algorithm 2**Robust Tiered RL

The key insight to overcome such difficulty is that, if the checking event holds yet \(\pi^{*}_{\text{Lo}}(s_{h})\neq\pi^{*}_{\text{Hi},h}(s_{h})\), the gap between \(\widetilde{Q}^{k}_{\text{Hi},h}(s_{h},\pi^{*}_{\text{Lo}})\) and \(Q^{*}_{\text{Hi},h}(s_{h},\pi^{*}_{\text{Lo}})\) should not be small, and we can show that \(\widetilde{Q}^{k}_{\text{Hi},h}(s_{h},\underline{\pi}^{k}_{\text{Lo}})\approx \widetilde{Q}^{k}_{\text{Hi},h}(s_{h},\pi^{*}_{\text{Lo}})\geq Q^{*}_{\text{ Hi},h}(s_{h},\pi^{*}_{\text{Lo}})+\Theta(\frac{H}{H+1}\Delta_{\text{Hi}}(s_{h},\pi^{*}_{ \text{Lo}}))\) with the choice of \(\varepsilon=O(\tilde{\Delta}_{\min}/H)\). Therefore, revising \(\tilde{Q}^{k}_{\text{Hi},h}(s_{h},\pi^{k}_{\text{Lo}})\) by adding \(1/H\) of the gap \(\tilde{Q}^{k}_{\text{Hi},h}(s_{h},\pi^{k}_{\text{Lo}})-Q^{*}_{\text{Hi},h}(s_{h}, \pi^{*}_{\text{Lo}})\) (line 19) is enough to guarantee the overestimation. Lastly, since \(Q^{*}_{\text{Hi},h}(s_{h},\pi^{*}_{\text{Lo}})\) in unknown, we construct an underestimation \(\underline{Q}^{\pi^{k}_{\text{Hi},h}}_{\text{Hi},h}(s_{h},\pi^{*}_{\text{Lo}})\) and use it instead. As a result, we have the following theorem, where the clip function is defined by \(\text{Clip}[x|w]:=x\cdot\mathbb{I}[x\geq w]\).

**Theorem 4.3**.: _There exists \(k_{\text{cost}}=\text{Poly}(S,A,H,\lambda^{-1},\Delta^{-1}_{\min})\), such that, for all \(k\geq k_{\text{cost}}\), on some event \(\mathcal{E}_{k}\) with \(\mathbb{P}(\mathcal{E}_{k})\leq 3\delta_{k}\), we have \(Q^{*}_{\text{Hi},h}(s_{h},a_{h})\leq\tilde{Q}^{k}_{\text{Hi},h}(s_{h},a_{h}), \;V^{*}_{\text{Hi},h}(s_{h})\leq\tilde{V}^{k}_{\text{Hi},h}(s_{h}),\forall h \in[H],\;s_{h}\in\mathcal{S}_{h},\;a_{h}\in\mathcal{A}_{h}\) and_

\[V^{*}_{\text{Hi},1}(s_{1})-V^{\pi^{k}_{\text{Hi}}}_{\text{Hi},1}(s_{1})\leq 2 e\mathbb{E}_{\pi^{k}_{\text{hi}}}\left[\sum_{h=1}^{H}\text{Clip}\left[\min\{H,4b ^{k}_{\text{Hi},h}(s_{h},a_{h})\}\middle|\frac{\Delta_{\min}}{4eH}\vee\frac{ \Delta_{\text{Hi}}(s_{h},a_{h})}{4e}\right]\right]. \tag{1}\]

Benefits of Knowledge TransferWe first take a look at \(k\geq k_{\text{cost}}\). As implied from Eq. (1), we can upper bound the regret on each \(s_{h},a_{h}\) by summing over the RHS of Eq. (1). Note that by Cond. D.3, \(b^{k}_{\text{Hi},h}(s_{h},a_{h})=O(\frac{\text{Poly}(SAH)\log k}{\sqrt{N^{k}_{ \text{Hi},h}(s_{h},a_{h})}})\) and \(\mathbb{E}[N^{k}_{\text{Hi},h}(s_{h},a_{h})]=\sum_{k^{\prime}=1}^{k-1}d^{\pi^ {\text{hi}}}_{\text{hi}}(s_{h},a_{h})\), we can establish the near-optimal regret bound with similar techniques in [23] regardless of the similarity between \(M_{\text{Hi}}\) and \(M_{\text{Lo}}\). Moreover, because of the knowledge transfer from \(M_{\text{Lo}}\), we can achieve better regret bounds for \(M_{\text{Hi}}\). In the following, we characterize three subclasses of state-action pairs, on which Alg\({}^{\text{Hi}}\) only suffers constant regret. _First of all_, for those \(s_{h}\in\mathcal{Z}^{1}_{h}\), we can expect the checking event almost always hold for arbitrary \(k\). Hence, when \(k\) is large enough, \(\pi^{k}_{\text{Hi},h}(s_{h})=\underline{\pi}^{k}_{\text{Lo},h}(s_{h})\approx \pi^{*}_{\text{Hi},h}(s_{h})\), implying Alg\({}^{\text{Hi}}\) will almost never take sub-optimal actions at \(s_{h}\) since then. We denote this first subclass as \(\mathcal{C}^{1,\lambda}_{h}:=\{(s_{h},a_{h})|s_{h}\in\mathcal{Z}^{1}_{h},a_{h} \neq\pi^{*}_{\text{Hi},h}(s_{h})\}\). _Secondly_, note that, given a state \(s_{h}\), if all possible trajectories starting from \(s_{1}\) to \(s_{h}\) have overlap with \(\mathcal{C}^{1,\lambda}_{h^{\prime}}\) for some \(h^{\prime}\in[h-1]\), when \(k\) is large enough, \(\pi^{k}_{\text{Hi}}\) will almost have no chance to reach \(s_{h}\) and will not suffer the regret at \(s_{h}\). For convenience, we define function \(\text{Block}(\{\mathcal{C}^{1,\lambda}_{h}\}_{h^{\prime}=1}^{h-1},s_{h})\) which takes True for those states described above, and takes False for the others. Then, we define the second subclass by \(\mathcal{C}^{2,\lambda}_{h}:=\{(s_{h},a_{h})|\text{Block}(\{\mathcal{C}^{1, \lambda}_{h^{\prime}}\}_{h^{\prime}=1}^{h-1},s_{h})=\texttt{True},\;s_{h}\not\in \mathcal{Z}^{\lambda}_{h},\;a_{h}\in\mathcal{A}_{h}\}\). _Finally_, for those \(s_{h},a_{h}\) with \(d^{*}_{\text{Hi}}(s_{h},a_{h})>0\), we can show \(N^{k}_{\text{Hi},h}(s_{h},a_{h})\approx d^{*}_{\text{Hi}}(s_{h},a_{h})k\). Therefore, \(b^{k}_{\text{Hi},h}(s_{h},a_{h})\propto\log k/\sqrt{N^{k}_{\text{Hi},h}(s_{h}, a_{h})}\) in Eq. (1) will decay and the clipping operator will take effect, which leads to constant regret. This third subclass is denoted by \(\mathcal{C}^{*}_{h}:=\{(s_{h},a_{h})|d^{*}_{\text{Hi}}(s_{h},a_{h})>0\}\). Based on the above discussion, we define \(\mathcal{C}^{\lambda}_{h}:=\mathcal{C}^{\lambda,1}_{h}\cup\mathcal{C}^{\lambda,2}_{h}\cup\mathcal{C}^{*}_{h}\) to be the benefitable states set in Thm. 4.2.

For \(k\leq k_{\text{cost}}\), for the lack of overestimation, we simply use \(H\) to upper bound the value gap \(V^{*}-V^{\pi^{k}_{\text{Hi}}}\). This results in a \(\text{Poly}(S,A,H,\lambda^{-1},\Delta^{-1}_{\min})\) burn-in term, which was omitted in Thm. 4.2 since it is independent with \(K\). Besides, by the definition of \(k_{\text{cost}}\) in Thm. 4.3, we can see the trade-off of choosing \(\lambda\): a smaller \(\lambda\) can enlarge \(\mathcal{C}^{\lambda}_{h}\) so we have constant regret on more state-action pairs, while it also results in the delay of overestimation by the larger \(k_{\text{cost}}\).

Constant Regret in the Entire MDPWe may expect constant regret in the entire \(M_{\text{Hi}}\) in some special cases. Note that, if \(\forall h\in[H],\forall s_{h}\) with \(d^{*}_{\text{Hi}}(s_{h})>0\), \(s_{h}\in\mathcal{Z}^{\lambda}_{h}\), we have \(\mathcal{C}^{\lambda}_{h}=\mathcal{S}_{h}\times\mathcal{A}_{h}\), \(\text{Regret}_{K}(M_{\text{Hi}})\) will be independent w.r.t. \(K\). From this perspective, if \(\lambda\) is chosen appropriately, e.g. \(\lambda\leq\min_{s_{h}}d^{*}_{\text{Hi}}(s_{h})\), we can recover the constant regret under the setting \(M_{\text{Lo}}=M_{\text{Hi}}\) in [13].

Choice of \(\lambda\)In this paper, we do not treat \(\lambda\) as a parameter to optimize. In practice, without prior knowledge about \(\max_{s_{h}}d^{*}_{\text{Lo}}(s_{h})\), one may choose \(\lambda=O(1/S)\) to ensure some chance that transferable states exist, since there exists at least some states satisfying \(d^{*}_{\text{Lo}}(s_{h})\geq 1/S\).

## 5 Robust Tiered MAB/RL with Multiple Low-Tier Tasks

Now, we focus on the case when a source task set \(\mathcal{M}_{\text{Lo}}:=\{M_{\text{Lo},w}\}_{w=1}^{W}\) is available (see Frw. 5 in Appx. A). Our objective is to achieve benefits on those states \(s_{h}\) as long as there exists some task \(w\in[W]\) such that \(M_{\text{Lo},w}\) and \(M_{\text{Hi}}\) are close on \(s_{h}\), while retaining near-optimal regret in other cases under Assump. B. The key challenge comparing with single task case is that, Alg\({}^{\text{Hi}}\) has to identify for each state which task in \(\mathcal{M}_{\text{Lo}}\) is the appropriate one to leverage. The main novelty and contribution in this section is a task selection mechanism we call _"Trust till Failure"_, which can automatically adapt to the similar task if it exists. We first highlight the main results for MAB and RL setting.

**Theorem 5.1**.: _[Tiered MAB with Multiple Source Tasks] Under Assump. A, B, and C, by running Alg. 3 with \(\mathcal{M}_{\text{Lo}}=\{M_{\text{Lo},w}\}_{w=1}^{W}\) and \(M_{\text{Lo}}\), with \(\varepsilon=\frac{\Delta_{\min}}{4}\) and \(\alpha>2\), we always have: \(\text{Regret}_{K}(M_{\text{Hi}})=O(\sum_{\Delta_{\text{Hi}}(i)>0}\frac{1}{ \Delta_{\text{Hi}}(i)}\log(WK))\). Moreover, if at least one task in \(\mathcal{M}_{\text{Lo}}\) is \(\frac{\widetilde{\Delta}_{\min}}{4}\), close to \(M_{\text{Hi}}\), we further have: \(\text{Regret}_{K}(M_{\text{Hi}})=O(\sum_{\Delta_{\text{Hi}}(i)>0}\frac{1}{ \Delta_{\text{Hi}}(i)}\log\frac{AW}{\Delta_{\min}})\)._

**Theorem 5.2**.: _[Tiered RL with Multiple Source Tasks] Under Assump. A, B, C, and Cond. D.3, F.4, by running Alg. 7 in Appx. F.1 with \(\varepsilon=\frac{\widetilde{\Delta}_{\min}}{4(H+1)}\), \(\alpha>2\) and any \(\lambda>0\), we have_

\[\text{Regret}_{K}(M_{\text{Hi}})=O\Big{(}SH\sum_{h=1}^{H}\sum_{(s_{h},a_{h}) \in\mathcal{S}_{h}\times\mathcal{A}_{h}\setminus\mathcal{C}_{h}^{\lambda,[W]} }(\frac{H}{\Delta_{\min}}\wedge\frac{1}{\Delta_{\text{Hf}}(s_{h},a_{h})})\log (SAHWK)\Big{)}.\]

```
1Initialize: \(\alpha>2;\ N_{\text{Lo}}^{1}(i),\ N_{\text{Hi}}^{1}(i),\ \widehat{\mu}_{\text{Lo}}^{1}(i),\ \widehat{\mu}_{\text{Hi}}^{1}(i)\gets 0,\ \forall i\in\mathcal{A};\,f(k):=1+16TA^{2}(k+1)^{2}\)
2 Pull each arm at the beginning \(A\) iterations. Set \(w^{A}\leftarrow\texttt{Null}\).
3for\(k=A+1,2,...,K\)do
4for\(w=1,2...,W\)do
5\(\overline{\mu}_{\text{Lo},w}^{k}(i)\leftarrow\widehat{\mu}_{\text{Lo},w}^{k}(i)+\sqrt{\frac{2\alpha\log f(k)}{N_{\text{Lo},w}(i)}}\), \(\overline{\pi}_{\text{Lo},w}^{k}\leftarrow\arg\max_{i}\overline{\mu}_{\text{Lo},w}^{k}(i)\), \(\pi_{\text{Lo},w}^{k}\leftarrow\overline{\pi}_{\text{Lo},w}^{k}\).
6\(\underline{\mu}_{\text{Lo},w}^{k}(i)\leftarrow\widehat{\mu}_{\text{Lo},w}^{k}(i)-\sqrt{\frac{2\alpha\log f(k)}{N_{\text{Lo},w}(i)}}\), \(\overline{\pi}_{\text{Lo},w}^{k}\leftarrow\arg\max_{i}\underline{\mu}_{\text{Lo},w}^{k}(i)\).
7 end for
8\(\overline{\mu}_{\text{Hi}}^{k}(i)\leftarrow\widehat{\mu}_{\text{Hi}}^{k}(i)+ \sqrt{\frac{2\alpha\log f(k)}{N_{\text{Hi}}^{k}(i)}}\), \(\overline{\pi}_{\text{Hi}}^{k}\leftarrow\arg\max_{i}\overline{\mu}_{\text{Hi}}^{k}(i)\). \(\mathcal{I}^{k}\leftarrow\{w\in[W]\underline{\mu}_{\text{Lo},w}^{k}(\underline{\pi}_{\text{Lo},w}^{k})\leq\overline{\mu}_{\text{Hi}}^{k}(\underline{\pi}_{\text{Lo},w}^{k})+\varepsilon\) and \(N_{\text{Lo},w}^{k}(\underline{\pi}_{\text{Lo},w}^{k})>k/2\}\)
9if\(\mathcal{I}^{k}=\emptyset\)then\(w^{k}\leftarrow\texttt{Null},\quad\pi_{\text{Hi}}^{k} \leftarrow\overline{\pi}_{\text{Hi}}^{k}\) ;
10else
11if\(w^{k-1}\neq\texttt{Null}\) and \(w^{k-1}\in\mathcal{I}^{k}\)then\(w^{k}\gets w^{k-1},\quad\pi_{\text{Hi}}^{k}\leftarrow\overline{\pi}_{\text{Lo},w^{k}}^{k}\) ;
12else if\(w^{k-1}\neq\texttt{Null}\) and \(\exists w\in\mathcal{I}^{k}\) such that \(\pi_{\text{Hi}}^{k-1}=\arg\max_{i}N_{\text{Lo},w}^{k}(i)\)then
13\(w^{k}\gets w,\quad\pi_{\text{Hi}}^{k}\leftarrow\underline{\pi}_{\text{Lo},w}^{k}\)
14 end if
15else\(w^{k}\sim\text{Unif}(\mathcal{I}^{k}),\quad\pi_{\text{Hi}}^{k}\leftarrow\underline{\pi}_{\text{Lo},w^{k}}^{k}\) ;
16
17 end if
18
19 Interact with \(M_{\text{Hi}}/\{M_{\text{Lo}}\}_{w=1}^{W}\) by \(\pi_{\text{Hi}}^{k}/\{\pi_{\text{Lo},w}^{k}\}_{w=1}^{W}\);
20 Update \(\{N_{\text{Lo},w}^{k+1}\}_{w=1}^{W},N_{\text{Hi}}^{k+1}\) and empirical mean \(\{\widehat{\mu}_{\text{Lo},w}^{k+1}\}_{w=1}^{W},\widehat{\mu}_{\text{Hi}}^{k+1}\) for each arm.
21
2 end for
```

**Algorithm 3**Robust Tiered MAB with Multiple Source Tasks

For the lack of space, in the following, we only analyze the bandit setting to explain the key idea of our task selection strategy. For the RL setting, we defer to Appx. F the algorithm Alg. 7, detailed version of Thm. 5.2 (Thm. D.16), defintion of transferable set \(\mathcal{C}_{h}^{\lambda,[W]}\) (Def. F.2), and technical details.

**Algorithm Design and Proof Sketch for Bandit Setting** The algorithm is provided in Alg. 3. Comparing with Alg. 1 in single task setting, the main difference is the task selection strategy from line 9 to line 17. We first examine each source task with a checking event similar to single task setting, and collect those feasible tasks passing the test to \(\mathcal{I}^{k}\). Intuitively, for those \(M_{\text{Lo},w^{*}}\) close to \(M_{\text{Lo}}\), we expect \(M_{\text{Lo},w}\in\mathcal{I}^{k}\) holds almostly for arbitrary \(k>0\), while for the other \(M_{\text{Lo},w^{\prime}}\), if it takes the position of \(w^{k}\), following \(M_{\text{Lo},w^{\prime}}\) will reduce the uncertainty and it will be ruled out from \(\mathcal{I}^{k}\), eventually. So we expect \(w^{k}\) can "escape" from dissimilar source tasks but be absorbed to the similar task if exists. Therefore, if \(\mathcal{I}^{k}\) is empty, \(\text{Alg}^{\text{Hi}}\) will do exploration by itself. Otherwise, we choose one from \(\mathcal{I}^{k}\) to transfer the action until it fails on the checking event. However, for any \(\varepsilon\) the algorithm chosen, those "marginally similar" source tasks (denoted as \(M_{\text{Lo},\widetilde{w}}\)), which are \(\varepsilon^{\prime}\)-close to \(M_{\text{Hi}}\) for some \(\varepsilon^{\prime}\) only slightly larger than \(\varepsilon\), may cause some trouble. Because the checking event will finally eliminate \(M_{\text{Lo},\bar{w}}\) since they are not \(\varepsilon\)-close, but it may occupy the position \(w^{k}\) for a long time before elimination, especially when \(\varepsilon^{\prime}\) is extremely close to \(\varepsilon\). After eliminating \(M_{\text{Lo},\bar{w}}\), Alg\({}^{\text{H}i}\) needs to re-select one from \(\mathcal{I}^{k}\). Now since other sub-optimal arms in \(M_{\text{Hi}}\) haven't been chosen for a long time and the confidence level \(\delta_{k}=O(1/k^{\alpha})\) is decreasing, \(\mathcal{I}^{k}\) will include those dissimilar tasks again, which causes difficulty to identify the true similar task. To solve this issue, once the previous trusted task fails, we give priority to the task recommending the same action as the previous one (line 14). As a result, since \(M_{\text{Lo},\bar{w}}\) and \(M_{\text{Lo},w^{*}}\) share the optimal action, after the elimination of \(M_{\text{Lo},\bar{w}}\), we can expect \(w^{k}\) to only switch among those tasks \(M_{\text{Lo},w}\) with \(\pi^{*}_{\text{Lo},w}=\pi^{*}_{\text{Hi}}\). We highlight this technical novelty to Lem. 5.3 below, and defer all the proofs to Appx. E.

**Lemma 5.3**.: _[Absorbing to Similar Task] Under Assump. A, B and C, there exists a constant \(c^{*}\), s.t., if there exists at least one \(w^{*}\in[W]\) such that \(M_{\text{Lo},w^{*}}\) is \(\frac{\bar{\Delta}_{\text{\min}}}{4}\)-close to \(M_{\text{Hi}}\), by running Alg. 3 with \(\varepsilon=\frac{\bar{\Delta}_{\text{\min}}}{4}\) and \(\alpha>2\), for any \(k\geq k^{*}:=c^{*}\frac{\alpha A}{\Delta_{\min}^{\prime}}\log\frac{\alpha AW}{ \bar{\Delta}_{\min}}\), we have \(\Pr(\pi^{k}_{\text{Hi}}\neq i^{*}_{\text{Hi}})=O(\frac{A}{k^{2\alpha-2}})\)._

## 6 Experiments

In this section, we evaluate our most representative algorithm, Alg. 7, in multiple source tasks setting.

**Experiments Setting3** We set \(S=A=3\) and \(H=5\). The details for construction of source and target tasks are defered to Appx. G. We adapt StrongEuler in [23] as online learning algorithm to solve source tasks, and use the bonus function in [23] as the bonus function in our Alg. 7. We evaluate our algorithm when \(W=0,1,2,5\), where \(W=0\) means the high-tier task \(M_{\text{Hi}}\) is simply solved by normal online learning method (StrongEuler) without any parallel knowledge transfer. We choose \(\lambda=0.3\approx 1/S\) in Alg. 7, and in the MDP instance we test, across all \(S\cdot H=15\) states, for \(W=1,2,5\), the number of transferable states would be 6, 9 and 13, respectively.

Footnote 3: Code is available at [https://github.com/jiaweihhuang/Robust-Tiered-RL](https://github.com/jiaweihhuang/Robust-Tiered-RL)

We choose iteration number \(K=1e7\), where we start the transfer since \(k=5e5\) to avoid large "burn-in" terms. As we can see, after the transfer starts, the regret in target task will suddenly increase for a while, because the target task has to make some mistakes and learn from it as a result of the model uncertainty. However, because of our algorithm design, the negative transfer will terminate after a very short period. As predicted by our theory, by adding more and more source tasks which can introduce new transferable states, the target task will suffer less and less regret.

## 7 Conclusion and Future Work

In this paper, we study how to do robust parallel transfer RL when single or multiple source tasks are available, without knowledge on models' similarity. The possible future directions include relaxing assumptions, better strategies to leveraging multiple source tasks, and identifying mild structural assumptions allowing for more aggressive transfer, and we defer to Appx. A.4 for more details.

Figure 1: **Regret in the Target Task given Multiple Source Tasks** We report the result when \(W\) source tasks are available with \(W=0,1,2,5\). The shadows indicate 96% confidence interval.

## Acknowledgments and Disclosure of Funding

The authors would like to thank Andreas Krause for valuable discussion. The work is supported by ETH research grant and Swiss National Science Foundation (SNSF) Project Funding No. 200021-207343.

## References

* Agarwal et al. [2019] Alekh Agarwal, Nan Jiang, Sham M Kakade, and Wen Sun. Reinforcement learning: Theory and algorithms. _CS Dept., UW Seattle, Seattle, WA, USA, Tech. Rep_, pages 10-4, 2019.
* Azar et al. [2017] Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforcement learning. In _International Conference on Machine Learning_, pages 263-272. PMLR, 2017.
* Brunskill and Li [2013] Emma Brunskill and Lihong Li. Sample complexity of multi-task reinforcement learning, 2013.
* Chen and Jiang [2022] Jinglin Chen and Nan Jiang. Offline reinforcement learning under value and density-ratio realizability: the power of gaps. _arXiv preprint arXiv:2203.13935_, 2022.
* Choi et al. [2021] HeeSun Choi, Cindy Crump, Christian Duriez, Asher Elmquist, Gregory Hager, David Han, Frank Heard, Jessica Hodgins, Abhinandan Jain, Frederick Leve, et al. On the use of simulation in robotics: Opportunities, challenges, and suggestions for moving forward. _Proceedings of the National Academy of Sciences_, 118(1):e1907856118, 2021.
* Dann et al. [2017] Christoph Dann, Tor Lattimore, and Emma Brunskill. Unifying pac and regret: Uniform pac bounds for episodic reinforcement learning, 2017.
* Dann et al. [2021] Christoph Dann, Teodor Vanislavov Marinov, Mehryar Mohri, and Julian Zimmert. Beyond value-function gaps: Improved instance-dependent regret bounds for episodic reinforcement learning. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, _Advances in Neural Information Processing Systems_, 2021. URL [https://openreview.net/forum?id=CUBqQMHB3dh](https://openreview.net/forum?id=CUBqQMHB3dh).
* D'Eramo et al. [2020] Carlo D'Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, Jan Peters, et al. Sharing knowledge in multi-task deep reinforcement learning. In _8th International Conference on Learning Representations, {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020_, pages 1-11. OpenReview. net, 2020.
* Golowich and Moitra [2022] Noah Golowich and Ankur Moitra. Can q-learning be improved with advice? In _Conference on Learning Theory_, pages 4548-4619. PMLR, 2022.
* Gupta et al. [2022] Abhishek Gupta, Aldo Pacchiano, Yuexiang Zhai, Sham M. Kakade, and Sergey Levine. Unpacking reward shaping: Understanding the benefits of reward engineering on sample complexity. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, _Advances in Neural Information Processing Systems_, 2022. URL [https://openreview.net/forum?id=D-X3kH-Bkpn](https://openreview.net/forum?id=D-X3kH-Bkpn).
* Hu et al. [2021] Jiachen Hu, Xiaoyu Chen, Chi Jin, Lihong Li, and Liwei Wang. Near-optimal representation learning for linear bandits and linear rl. In _International Conference on Machine Learning_, pages 4349-4358. PMLR, 2021.
* Huang et al. [2022] Jiawei Huang, Jinglin Chen, Li Zhao, Tao Qin, Nan Jiang, and Tie-Yan Liu. Towards deployment-efficient reinforcement learning: Lower bound and optimality. _arXiv preprint arXiv:2202.06450_, 2022.
* Huang et al. [2022] Jiawei Huang, Li Zhao, Tao Qin, Wei Chen, Nan Jiang, and Tie-Yan Liu. Tiered reinforcement learning: Pessimism in the face of uncertainty and constant regret. _arXiv preprint arXiv:2205.12418_, 2022.
* Jiang and Huang [2020] Nan Jiang and Jiawei Huang. Minimax value interval for off-policy evaluation and policy optimization. _Advances in Neural Information Processing Systems_, 33:2747-2758, 2020.

* [15] Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably efficient? _arXiv preprint arXiv:1807.03765_, 2018.
* [16] Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline rl? In _International Conference on Machine Learning_, pages 5084-5096. PMLR, 2021.
* [17] Tor Lattimore and Csaba Szepesvari. _Bandit algorithms_. Cambridge University Press, 2020.
* [18] Alessandro Lazaric. Transfer in reinforcement learning: a framework and a survey. In _Reinforcement Learning_, pages 143-173. Springer, 2012.
* [19] Yongyuan Liang and Bangwei Li. Parallel knowledge transfer in multi-agent reinforcement learning, 2020.
* [20] Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Provably good batch off-policy reinforcement learning without great exploration. _Advances in neural information processing systems_, 33:1264-1274, 2020.
* [21] Timothy A Mann and Yoonsuck Choe. Directed exploration in reinforcement learning with transferred knowledge. In _European Workshop on Reinforcement Learning_, pages 59-76. PMLR, 2013.
* [22] Chloe Rouyer and Yevgeny Seldin. Tsallis-inf for decoupled exploration and exploitation in multi-armed bandits. In _Conference on Learning Theory_, pages 3227-3249. PMLR, 2020.
* [23] Max Simchowitz and Kevin G Jamieson. Non-asymptotic gap-dependent regret bounds for tabular mdps. _Advances in Neural Information Processing Systems_, 32, 2019.
* [24] Shagun Sodhani, Amy Zhang, and Joelle Pineau. Multi-task reinforcement learning with context-based representations. In _International Conference on Machine Learning_, pages 9767-9779. PMLR, 2021.
* [25] Adam Taylor, Ivana Dusparic, Maxime Gueriau, and Siobhan Clarke. Parallel transfer learning in multi-agent systems: What, when and how to transfer? In _2019 International Joint Conference on Neural Networks (IJCNN)_, pages 1-8. IEEE, 2019.
* [26] Matthew E Taylor and Peter Stone. Transfer learning for reinforcement learning domains: A survey. _Journal of Machine Learning Research_, 10(7), 2009.
* [27] Volodymyr Tkachuk, Sriram Ganapathi Subramanian, and Matthew E. Taylor. The effect of q-function reuse on the total regret of tabular, model-free, reinforcement learning, 2021.
* [28] Masatoshi Uehara and Wen Sun. Pessimistic model-based offline reinforcement learning under partial coverage. _arXiv preprint arXiv:2107.06226_, 2021.
* [29] Nelson Vithayathil Varghese and Qusay H Mahmoud. A survey of multi-task deep reinforcement learning. _Electronics_, 9(9):1363, 2020.
* [30] Andrew Wagenmaker and Aldo Pacchiano. Leveraging offline data in online reinforcement learning. _arXiv preprint arXiv:2211.04974_, 2022.
* [31] Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Multi-task reinforcement learning: a hierarchical bayesian approach. In _Proceedings of the 24th international conference on Machine learning_, pages 1015-1022, 2007.
* [32] Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent pessimism for offline reinforcement learning. _Advances in neural information processing systems_, 34:6683-6694, 2021.
* [33] Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy finetuning: Bridging sample-efficient offline and online reinforcement learning. _Advances in neural information processing systems_, 34:27395-27407, 2021.

* [34] Haike Xu, Tengyu Ma, and Simon Du. Fine-grained gap-dependent bounds for tabular mdps via adaptive multi-step bootstrap. In _Conference on Learning Theory_, pages 4438-4472. PMLR, 2021.
* [35] Wenhao Zhan, Baihe Huang, Audrey Huang, Nan Jiang, and Jason Lee. Offline reinforcement learning with realizability and single-policy concentrability. In _Conference on Learning Theory_, pages 2730-2775. PMLR, 2022.
* [36] Chicheng Zhang and Zhi Wang. Provably efficient multi-task reinforcement learning with model transfer. _Advances in Neural Information Processing Systems_, 34:19771-19783, 2021.
* [37] Zhuangdi Zhu, Kaixiang Lin, and Jiayu Zhou. Transfer learning in deep reinforcement learning: A survey. _arXiv preprint arXiv:2009.07888_, 2020.