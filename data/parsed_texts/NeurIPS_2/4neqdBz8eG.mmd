# Rethinking Weight Decay for Robust Fine-Tuning of Foundation Models

Junjiao Tian

Georgia Institute of Technology

jtian73@gatech.edu

&Chengyue Huang

Georgia Institute of Technology

chuang475@gatech.edu

&Zsolt Kira

Georgia Institute of Technology

zkira@gatech.edu

###### Abstract

Modern optimizers such as AdamW, equipped with momentum and adaptive learning rate, are designed to escape local minima and explore the vast parameter space. This exploration is beneficial for finding good loss basins when training from scratch. It is not necessarily ideal when resuming from a powerful foundation model because it can lead to large deviations from the pre-trained initialization and, consequently, worse robustness and generalization. At the same time, strong regularization on all parameters can lead to under-fitting. We hypothesize that selectively regularizing the parameter space is the key to fitting and retraining the pre-trained knowledge. This paper proposes a new weight decay technique, Selective Projection Decay (SPD), that selectively imposes a strong penalty on certain layers while allowing others to change freely. Intuitively, SPD expands and contracts the parameter search space for layers with consistent and inconsistent loss reduction, respectively. Experimentally, when equipped with SPD, Adam consistently provides better in-distribution generalization and out-of-distribution robustness performance on multiple popular vision and language benchmarks. Code available at [https://github.com/GT-RIPL/Selective-Projection-Decay.git](https://github.com/GT-RIPL/Selective-Projection-Decay.git).

## 1 Introduction

Modern optimizers, such as Adam [1], LARS [2], and LAMB [3] usually include momentum and adaptive learning rates. They help optimizers avoid local minima and accelerate learning [4, 5] to explore wider parameter spaces. However, we hypothesize that this behavior is not always beneficial for fine-tuning from a _well_ pre-trained foundation model, especially when fine-tuning a few layers is already sufficient for fitting the target data [6, 7, 8, 9]. Several prior works have found that unnecessary exploration will lead to large deviation from the initialization and worse robustness [10, 11], and constraining the deviation can improve a model's generalization on in-distribution (ID) data and robustness to out-of-distribution (OOD) data [12, 13, 14]1. For example, L2-SP [13] imposes a regularization term on the distance between the current and pre-trained models. More recently, TPGM [10] and FTP [11] propose to learn different hard constraints for each layer. These new works have demonstrated impressive results on benchmarks. However, they are either difficult to tune, specialized to specific settings, or require significant computation and storage overhead. _This motivates us to ask whether a simple few-liner solution exists for this fundamental problem._We propose re-examining the existing methods and summarizing their findings to find this solution. Starting from the simplest: L2-SP [13]. Specifically, L2-SP adds an L2 regularization term to the original objective function. Formally,

\[\mathcal{L}(\theta)=\tilde{\mathcal{L}}(\theta)+\frac{\lambda}{2}\|\theta-\theta _{0}\|_{2}^{2} \tag{1}\]

where \(\theta\) denotes the model parameters, \(\theta_{0}\) the initialization, \(\tilde{\mathcal{L}}(\theta)\) the original objective function, and \(\lambda\) the hyper-parameter for regularization strength. When \(\theta_{0}=\mathbf{0}\), L2-SP reduces to an ordinary weight decay. This simple method should be effective enough to constrain the model, as our experiments show it can reduce the deviation between the fine-tuned and pre-trained models (Sec. 4.1). However, it is held back by an important design choice: the penalty is always applied to all model parameters. Our empirical results identify that a large \(\lambda\) prevents every layer from deviating too much and leads to poor fitting, while a small \(\lambda\) cannot provide enough regularization. This significantly limits the otherwise effective design (Sec. 4.1). So, _what is missing in this algorithm?_

Recent works in robust fine-tuning and parameter-efficient fine-tuning (PEFT) have shown that customizing constraints for each layer and _selectively_ choosing layers for fine-tunning can improve robustness [10; 11; 6]. Inspired by these findings, we hypothesize that selectively imposing the regularization to different layers is the key. Therefore, we propose a simple _selective_ version of L2-SP weight decay: selective projection decay (SPD). This new algorithm innovates in two aspects: a **selection condition** and a **regularization strength ratio**. The former determines when to apply regularization to a layer, and the latter determines the strength of regularization for intuitive hyperparameter tuning. Specifically, we derive the selection condition from hyper-optimization [15; 16; 17] by treating the condition as an optimizable parameter (Sec. 3.3), and the regularization strength ratio by re-writing L2-SP as a projection operation (Sec. 3.4). Intuitively, when the condition is met, the algorithm imposes _large_ regularizations on selected layers. This allows the algorithm to avoid unnecessary deviation and simultaneously fit into the fine-tuning data. We test SPD on large-scale computer vision, and NLP benchmarks with popular foundation models and test ID and OOD performance on various distribution and domain shifts. SPD achieves SOTA performance while being much simpler than other competing methods. Our contributions are:

* We propose a selective projection decay, a selective variant of the popular L2-SP/weight decay regularization methods, for robust fine-tuning of large foundation models. We show that selectivity is important to make regularization effective.
* We conduct a detailed study of ID and OOD performance on image classification and semantic segmentation with natural distribution and domain shifts. SPD improves ID and OOD performance on these benchmarks.
* We show that SPD consistently improves the performance of PEFT methods (e.g. LoRA [7] and adapters [9]) on 8 common sense reasoning language tasks with LLaMA-7B (-13B).

## 2 Related Works

**Robust Fine-Tuning with Distance Regularization.** Constraining the distance or deviation between the fine-tuned and pre-trained models has been studied in several prior works. L2-SP [13] explicitly adds an L2 norm penalty on the deviation and shows improved ID generalization for fine-tuning. MARS-SP [12] studies different forms of norms as the penalty. It shows that the Matrix Row Sum (MARS) norm can be a superior alternative to the L2 norm. These two methods impose "soft" penalties and can be less effective [18]. Instead, LCC [18] proposes constraining the deviation through direct projection on the parameters, which also enforces a hard constraint on the Lipschitz continuity of the fine-tuned model. However, LCC is hard to tune because the projection radius is not an intuitive hyper-parameter. Furthermore, using a single projection constraint for all layers is not an ideal strategy [10]. More recently, TPGM [10] proposes to automatically learn the constraints in LCC during fine-tuning, customizing a different projection radius for each layer through a bi-level optimization scheme. FTP [11] further improves the computation efficiency of TPGM by adopting hyper-optimization [15; 16; 17] in its computation. Nevertheless, FTP is still difficult to control because hyper-optimization requires a secondary optimizer with additional optimization hyper-parameters, and the learned regularization can be too strong with no intuitive way to adjust. In contrast, SPD is a much simpler and more intuitive method, which can be implemented with just a few lines of code. The superior controllability makes SPD potentially applicable to more applications.

**Parameter Efficient Fine-Tuning (PEFT).** PEFT methods such as adapters [9; 8] and LoRA [7] have been proposed to reduce training memory usage and computation complexity. Recent works have found that PEFT methods also provide good robustness because they modify fewer parameters and retain more knowledge of the pre-trained models [11]. Surgical fine-tuning [6] concludes that fine-tuning a selective few layers can improve ID generalization. These new works motivate us to re-evaluate L2-SP and weight decay, often uniformly applied to all layers. We identify that the inferior performance of the simple methods is because of this uniformity, which exhibits a strong trade-off between fitting and regularization. Other robust fine-tuning methods, such as LP-FT [19] and FLYP [20], focus on feature distortion. We will review them in the Appendix 8.1.

**Other Robust Fine-Tuning Methods.** WiSE-FT [14] discovers that linearly interpolating between the fine-tuned and pre-trained models after fine-tuning can improve out-of-distribution robustness. This demonstrates that a closer distance to the pre-trained model can improve robustness. However, it only applies to models with zero-shot capabilities. Another orthogonal line of research for robust fine-tuning focuses on feature distortion. LP-FT [19] shows that fine-tuning with a randomly initialized head layer distorts learned features. It proposes a simple two-stage method to train the head layer first and then fine-tune the entire model. FLYP [20] shows that fine-tuning a foundation model using the same objective as pre-training can better preserve the learned features. Our contribution is an optimization method to penalize the derivation between the fine-tuned and pre-trained models explicitly during fine-tuning, which is orthogonal to them.

## 3 Methods

In this section, we first provide an overview of the Selective Projection Decay (SPD) method and then describe the intuition behind SPD with a numerical example. Finally, we provide a concrete mathematical motivation for our method's algorithmic design.

### Selective Projection Decay (SPD)

**Formulation.** SPD is a regularization technique that penalizes significant deviation from the pre-trained model. We motivate the formulation from an existing method: L2-SP [13] (Eq. 1). L2-SP adds a distance penalty on the deviation between the fine-tuned and pre-trained models. The penalty is applied to all model parameters at all times. A large \(\lambda\) prevents every layer from deviating too much and empirically leads to poor fitting, while a small \(\lambda\) cannot provide enough regularization. This significantly limits the otherwise effective design. We propose a _selective_ version of this simple technique: selective projection decay (SPD). We will examine L2-SP and SPD in Alg. 1 and Alg. 2.

**Notations.** We follow the notations in prior works [1; 21]. Let \(m_{t},v_{t}\) denote the moving average of the gradient and squared gradient, \(\beta_{1},\beta_{2}\) their hyper-parameters, and \(\alpha\) the learning rate.

Alg. 1 shows the Adam optimizer with the L2-SP regularization in Eq. 1. The effects of the regularization are highlighted in blue, also shown in Eq. 2. Intuitively, the regularization leads to an interpolation-like equation2. If the product \(\lambda\alpha=1\), then \(\theta_{t}\leftarrow\theta_{0}\) and if \(\lambda\alpha=0\), then \(\theta_{t}\leftarrow\tilde{\theta}_{t}\), where \(\theta_{0}\) and \(\tilde{\theta}_{t}\) denote the initialization and the updated model _without_ regularization.

Footnote 2: Mathematically, this is not the precise formulation of L2-SP as written in Eq. 1. See the Appendix 8.3 for further discussion.

\[\theta_{t}\leftarrow\tilde{\theta}_{t}-\lambda\alpha(\tilde{\theta}_{t}-\theta _{0}) \tag{2}\]

Alg. 2 shows the proposed SPD. There are two changes compared to Alg. 1.

* a condition, \[c_{t}=-g_{t}^{\intercal}(\theta_{t-1}-\theta_{0}).\] (3)
* a new interpolation-like equation with a multiplier, \(r_{t}\), replacing the learning rate \(\alpha\), \[\theta_{t}\leftarrow\tilde{\theta}_{t}-\lambda r_{t}(\tilde{\theta}_{t}- \theta_{0}).\] (4)Compared to L2-SP, SPD only imposes a penalty when the _condition_ is met (\(c_{t}<0\)), and the strength of the penalty is controlled by a hyper-parameter \(\lambda\) and an analytical quantity _deviation ratio_\(r_{t}\), which we will introduce later.

```
Initialize\(m_{0}\gets 0\), \(v_{0}\gets 0\), \(t\gets 0\) While\(\theta_{t}\) not converged \(t\gets t+1\) \(g_{t}\leftarrow\nabla_{\theta}\tilde{\mathcal{L}}(\theta_{t-1})\) \(m_{t}\leftarrow\beta_{1}m_{t-1}+(1-\beta_{1})g_{t}\) \(v_{t}\leftarrow\beta_{2}v_{t-1}+(1-\beta_{2})g_{t}^{2}\) Bias Correction \(\widehat{m_{t}}\leftarrow\frac{m_{t}}{1-\beta_{1}^{\intercal}}\), \(\widehat{v_{t}}\leftarrow\frac{v_{t}}{1-\beta_{2}^{\intercal}}\) Update \(\tilde{\theta}_{t}\leftarrow\theta_{t-1}-\frac{\alpha\widehat{m_{t}}}{\sqrt{v_ {t}}+\epsilon}\) \(\theta_{t}\leftarrow\tilde{\theta}_{t}-\lambda\alpha(\tilde{\theta}_{t}-\theta _{0})\)
```

**Algorithm 1**Adam with L2-Regularization

### Intuition Behind SPD

**SPD prioritizes layers with consistent improvement.** SPD adds regularization on layers that meet the condition \(c_{t}<0\) to slow their growth. The condition is determined by the sign of the inner product between two vectors. One vector is the negative gradient direction \((-g_{t})\), i.e., the descent direction, and the other is the current progress direction \((\theta_{t-1}-\theta_{0})\). The inner product between them measures the _alignment_ between the _vanilla3_ update direction and the progress so far. When the inner product is positive, the current progress direction generally points to a low loss region, and following it will lead to consistent loss reduction. Conversely, if the inner product is negative, the current progress direction will likely lead to a higher loss region, indicating inconsistent improvement. In this case, SPD will impose a penalty to slow down updates for those layers. Recall that modern optimizers use momentum to escape local minima and explore wider regions. Without this penalty, the model will likely head towards the higher loss region to overcome it. SPD chooses to slow down these layers and prioritizes layers with more consistent loss reduction. We will motivate this strategy in a principled manner and validate it in our experiments.

Footnote 3: the direction w/o momentum.

```
Initialize\(m_{0}\gets 0\), \(v_{0}\gets 0\), \(t\gets 0\),\(c_{0}\gets 0\) While\(\theta_{t}\) not converged \(t\gets t+1\) \(g_{t}\leftarrow\nabla_{\theta}\tilde{\mathcal{L}}(\theta_{t-1})\) \(m_{t}\leftarrow\beta_{1}m_{t-1}+(1-\beta_{1})g_{t}\) \(v_{t}\leftarrow\beta_{2}v_{t-1}+(1-\beta_{2})g_{t}^{2}\) Bias Correction \(\widehat{m_{t}}\leftarrow\frac{m_{t}}{1-\beta_{1}^{\intercal}}\), \(\widehat{v_{t}}\leftarrow\frac{v_{t}}{1-\beta_{2}^{\intercal}}\) Update \(\tilde{\theta}_{t}\leftarrow\theta_{t-1}-\frac{\alpha\widehat{m_{t}}}{\sqrt{v_ {t}}+\epsilon}\) \(c_{t}=-g_{t}^{\intercal}(\theta_{t-1}-\theta_{0})\) If\(c_{t}<0\) : \(\theta_{t}\leftarrow\tilde{\theta}_{t}-\lambda r_{t}(\tilde{\theta}_{t}-\theta_{0})\)
```

**Algorithm 2**Adam with Selective L2-Reg.

### Deriving \(c_{t}\) from Hyper-Optimization

Previously, we explained the intuition behind SPD. Specifically, we interpreted the condition \(c_{t}\) as a measure of alignment and a test of update consistency. Nevertheless, there is a more profound reason why the quantity \(c_{t}\) is a natural choice for selective regularization. In this section, we motivate SPD from a more mathematical perspective.

**Hyper-Optimization Setup.** Hyper-optimization is a technique to optimize hyper-parameters inside an optimizer [15, 16, 17]. They treat the hyper-parameters as trainable parameters and optimize them using another gradient-based optimizer. Let's start from the vanilla Adam with L2-SP algorithm (Alg. 1) and treat the regularization strength hyper-parameter \(\lambda\) as a trainable parameter. To update \(\lambda\), we need to obtain its gradient by taking a derivative w.r.t. \(\lambda\) after applying it.

\[\nabla\lambda:=\frac{\partial\mathcal{L}(\theta_{t})}{\partial\lambda}=\frac{ \partial\mathcal{L}(\theta_{t})}{\partial\theta_{t}}^{\intercal}\frac{ \partial\theta_{t}}{\partial\lambda}=\alpha*-g_{t+1}^{\intercal}(\tilde{ \theta}_{t}-\theta_{0}). \tag{5}\]

**Selection Condition \(c_{t}\).** Intuitively, if the quantity \(\nabla\lambda\) is negative, applying the update in gradient descent will increase the value of \(\lambda\), thus increasing the regularization strength of L2-SP. Conversely, a positive quantity will decrease the regularization strength. Therefore, the \(\text{sign}(-g_{t+1}^{\intercal}(\tilde{\theta}_{t}-\theta_{0}))\) determines the change of regularization strength in the hyper-optimization of \(\lambda\). Formally, we define the condition \(c_{t}\) as,

\[c_{t}:=-g_{t+1}^{\intercal}(\theta_{t}-\theta_{0}) \tag{6}\]For memory efficiency, we use \((\theta_{t}-\theta_{0})\) instead of \((\tilde{\theta}_{t}-\theta_{0})\) because both vectors point in the same direction and won't affect the sign of \(c_{t}\). This allows us to discard \(\tilde{\theta}_{t}\). Otherwise, we need to keep an additional copy in memory. In summary, when \(c_{t}<0\), we apply a regularization for that layer as shown in Alg. 2. This calculation is done for each layer, and the regularization is selectively applied.

**Alternative Interpretation:** We just interpreted the selection condition \(c_{t}\) in SPD as a measure of consistency between the current heading direction and the gradient direction. This perspective is more valid when the algorithm has accumulated some updates, i.e., \(\|\theta_{t}-\theta_{0}\|_{2}\gg 0\), and less justified when a heading has not been established at the beginning of training. To analyze this, we discuss the behavior SPD from the perspective of _stochastic_ optimization when \(\|\theta_{t}-\theta_{0}\|_{2}\) is small at the beginning of training in the Appendix 8.1.

### Deriving \(r_{t}\) from Projection

The selection condition \(c_{t}\) determines when to apply regularization to which layers. However, one remaining question is the strength of regularization, which is not intuitive to tune. To overcome this, we introduced an analytical quantity, the deviation ratio \(r_{t}\), in Eq. 2 and Alg. 1. In this section, we will motivate it from the perspective of projection.

**L2-SP is projection.** Projection onto a norm ball is common in constrained optimization. While L2-SP is not a constrained optimization problem, its operation bears similarity to projection. Suppose we project a model \(\tilde{\theta}_{t}\) to an \(\mathcal{L}_{2}\)-norm ball with radius \(\gamma\) centered around its initialization \(\theta_{0}\). The equation of projection is the following,

\[\theta_{p}=\theta_{0}+\frac{\gamma}{\max\{\gamma,\|\tilde{\theta}_{t}-\theta_{ 0}\|_{2}\}}*(\tilde{\theta}_{t}-\theta_{0}). \tag{7}\]

Equivalently, we can rewrite the equation as,

\[\theta_{p}=\tilde{\theta}_{t}-\left(1-\frac{\gamma}{\max\{\gamma,\|\tilde{ \theta}_{t}-\theta_{0}\|_{2}\}}\right)*(\tilde{\theta}_{t}-\theta_{0}). \tag{8}\]

Now, we can equate this equation to the highlighted L2-SP equation in Eq. 2 and Alg. 1, we can see that if \(\lambda\alpha=\left(1-\frac{\gamma}{\max\{\gamma,\|\tilde{\theta}_{t}-\theta_{ 0}\|_{2}\}}\right)\), the regularization is equivalent to projection with radius \(\gamma\).

**Deviation Ratio \(r_{t}\).** This equivalence inspires us to define a deviation ratio \(r_{t}\):

\[r_{t}=\frac{\max\{0,\gamma_{t}-\gamma_{t-1}\}}{\gamma_{t}} \tag{9}\]

where \(\gamma_{t}:=\|\tilde{\theta}_{t}-\theta_{0}\|_{2}\) and \(\gamma_{t}:=\|\theta_{t-1}-\theta_{0}\|_{2}\) denote the current deviation (before regularization) and the previous deviation from the initialization \(\theta_{0}\), respectively. We use \(r_{t}\) in SPD (Alg. 2) to replace the learning rate \(\alpha\) in L2-SP (Alg. 1) to make hyper-parameter (\(\lambda\)) tuning more intuitive. Specifically, suppose the hyper-parameter \(\lambda=1\), then the regularization in SPD is:

\[\theta_{t}\leftarrow\tilde{\theta}_{t}-\frac{\max\{0,\gamma_{t}-\gamma_{t-1} \}}{\gamma_{t}}(\tilde{\theta}_{t}-\theta_{0})=\theta_{0}+\frac{\gamma_{t-1}}{ \max\{\gamma_{t-1},\gamma_{t}\}}*(\tilde{\theta}_{t}-\theta_{0}). \tag{10}\]

Intuitively, with \(\lambda=1\), the regularization in SPD is equivalent to projection with a radius equal to the previous deviation if the current deviation is larger. In summary:

* **No regularization** (\(\lambda=0\)): the projection radius is \(\|\tilde{\theta}_{t}-\theta_{0}\|_{2}\), meaning no projection.
* **Weak regularization** (\(1\geq\lambda>0\)): the projection radius lies between \(\|\tilde{\theta}_{t}-\theta_{0}\|_{2}\) and \(\|\theta_{t-1}-\theta_{0}\|_{2}\). Within this range, all layers will expand or remain unchanged.
* **Strong regularization** (\(\lambda>1\)): the projection radius lies between \(0\) and \(\|\theta_{t-1}-\theta_{0}\|_{2}\). In this range, it's possible that regularized layers can contract.

We recommend starting with \(\lambda=1\) and adjusting the strength according to the specific needs.

### Compatibility with PEFT methods.

As shown in Alg. 2, SPD retains a copy of the pre-trained model in memory. This adds additional memory requirements to the overhead of vanilla optimizers. While this is practical for moderate-sized models, as fine-tuning focuses more and more on large models, additional memory requirements become undesirable. Fortunately, in extremely large models, the prevalent fine-tuning strategy is parameter-efficient fine-tuning (PEFT), such as LoRA [7], series adapters [9], and parallel adapters [8]. SPD is naturally compatible with these methods without the additional memory. Intuitively, SPD selectively projects the current model towards the pre-trained initialization. PEFT methods generally initialize new parameters to add to the original model weights. To recover the behavior of SPD, we can instead project the new parameters towards the _origin_, equivalent to a selective version of regular weight decay, i.e., replacing \(\theta_{0}\) with \(\mathbf{0}\) in Alg. 2. Consequently, this does not require a memory copy of the pre-trained model. It consistently improves PEFT fine-tuning for large language models on common sense reasoning benchmarks in Sec. 4.4.

For example, LoRA decomposes a linear layer \(h=W_{t}x\) into two components, where \(h\in\mathbb{R}^{m\times 1}\), \(W_{t}\in\mathbb{R}^{m\times n}\) and \(x\in\mathbb{R}^{n\times 1}\) are the output, weights, and input of this layer.

\[h=W_{t}x=(W_{0}+\Delta W_{t})x\approx W_{0}x+W_{up}W_{down}x \tag{11}\]

where \(W_{0}\in\mathbb{R}^{m\times n}\), \(W_{up}\in\mathbb{R}^{m\times r}\) and \(W_{down}\in\mathbb{R}^{r\times n}\) are the pre-trained model, up-projection and down-projection matrices. If \(r\ll\min\{m,n\}\), (\(W_{up}W_{down}\)) is a low-rank approximation of \(\Delta W_{t}\). To regularize the overall deviation \(\|W_{t}-W_{0}\|_{2}\), it suffices to regularize \(\|W_{up}W_{down}\|_{2}\) to be close to zero. In this case, SPD acts as selective weight decay on \(W_{up}\) and \(W_{down}\) individually.

In summary, we propose selective projection decay (SPD) to impose strong regularization on layers during fine-tuning selectively. As shown in Fig. 1, SPD regularizes the deviation of the fine-tuned model from the pre-trained model \(\|W_{t}-W_{0}\|_{2}\) for full fine-tuning and the deviation from the origin \(\|\Delta W_{t}\|_{2}\) for PEFT fine-tuning.

## 4 Experiments

We test Selective Projection Decay on a diverse set of benchmarks, architectures, and tasks to demonstrate its effectiveness. We will test both ID generalization and OOD robustness across various domain and distribution shifts.

**Image Classification.** We first analyze the behavior of SPD on conventional image classification datasets DomainNet [22] and ImageNet [23]. We use a CLIP ViT-Base model for both experiments as the pre-trained initialization [24]. Specifically, DomainNet consists of images from several domains with 345 classes. We fine-tune on one domain and test on all domains. ImageNet is a large-scale dataset with 1000 classes. We fine-tune on ImageNet and test on ImageNet and four variants, namely ImageNet-V2 [25], ImageNet-A [26], ImageNet-R [27], and ImageNet-S [28].

**Semantic Segmentation.** We further test SPD on the PASCAL-Context semantic segmentation dataset [29]. Following prior works [30; 11], we use a Swin ViT-Tiny [31], pre-trained on ImageNet-22K, and Segformer [32] segmentation architecture. To construct the OOD datasets, we follow the popular natural robustness literature [33] and apply four representative image corruptions (fog, defocus blur, Gaussian noise, and brightness) with 5 severity each. We fine-tune on the clean segmentation data and test on clean and corrupted data.

Figure 1: **Selective Projection Decay (SPD) imposes regularization on layers selectively during fine-tuning. It regularizes \(\|W_{t}-W_{0}\|_{2}\) for full fine-tuning and \(\|\Delta W_{t}\|_{2}\) for PEFT fine-tuning.**

[MISSING_PAGE_FAIL:7]

and significantly improves OOD performance while matching the best ID performance. Under SPD, the correlation coefficient between OOD performance and deviation is \(-0.96\), which indicates a strong negative correlation between the two quantities, i.e., smaller deviation and higher OOD accuracy. This experiment shows that selective regularization is superior to uniform regularization.

**Training Details.** We use the vision transformer public repository for DEIT [46] to fine-tune all methods. We use \(\lambda=1\) for all Adam-SPD results in Tab. 1. More details are in Appendix 8.4.

### ImageNet Experiments

**SPD outperforms more complicated works on image classification.** Following the training recipe from the prior work [11], we fine-tune a CLIP ViT-Base model on ImageNet using Adam-SPD. We use the same hyper-parameters as the prior work and only adjust the regularization hyperparameter in SPD. In Tab. 3, we observe that Adam-SPD provides the best ID performance (strong ID generalization) and best average OOD performance (strong OOD robustness) on four ImageNet variants. SPD achieves a level of competitive performance with just a few lines of code. SPD's simplicity and strong performance show that selective regularization is a fundamental improvement for robust fine-tuning.

**Training Details.** For Adam-SPD, we fine-tune the model with a learning rate of \(3e-5\) and \(\lambda=1.4\). The regularization hyper-parameter is found through cross-validation, and the model with the best ID validation accuracy is taken. More details are in Appendix 8.4.

\begin{table}
\begin{tabular}{c|c c c c c c c c c c c c} \hline \hline Hyper-Parameter \(\lambda\) & 1e-1 & 1e-2 & 6e-3 & 3e-3 & 1e-3 & 6e-4 & 3e-4 & 1e-4 & 1e-5 & 1e-6 & 1e-7 & 0.0 \\ \hline Deviation & 0.03 & 0.14 & 0.18 & 0.24 & 0.34 & 0.39 & 0.46 & 0.53 & 0.58 & 0.58 & 0.58 & 0.59 \\ \hline OOD & 14.90 & 37.20 & 39.43 & 40.52 & 41.13 & 41.76 & 40.52 & 41.26 & 41.35 & 41.73 & 40.62 & 41.34 \\ \hline ID & 27.25 & 69.74 & 73.

### PASACAL Dense Semantic Segmentation

**SPD outperforms more complicated works on semantic segmentation.** The same trend is observed on semantic segmentation in Tab. 4. Again, SPD achieves the best ID generalization and OOD robustness across four different corruptions. This shows that proper regularization is not only important for achieving strong ID generalization (performance on the test set) but also for strong OOD robustness (performance on distribution shifted test sets) to domains shift (Tab. 3) and distribution shift such as natural corruptions (Tab. 4). The model fine-tuned with SPD is consistently more robust across different levels of corruption and severity.

**Training Details.** For Adam-SPD, we fine-tune the model with a learning rate of \(1e-4\) and \(\lambda=2.2\). The regularization hyper-parameter is found through cross-validation, and the model with the best ID validation accuracy is taken. More details are in Appendix 8.4.

### LLaMA PEFT Fine-Tuning Experiments

**SPD is compatible and consistently improves PEFT methods.** Previous experiments have shown that SPD imposes effective regularization for full fine-tuning. Furthermore, SPD can also improve the performance of PEFT methods. We fine-tune LLaMa-7B (-13B) models on the Commonsense-170k dataset [34]. As shown in Tab. 5, SPD consistently improves regular fine-tuning with AdamW, which uses a uniform weight decay for all tested PEFT methods. This demonstrates that selective regularization benefits full fine-tuning and PEFT fine-tuning. Combined with its simplicity, SPD can potentially improve generalization and robustness for more tasks in deep learning.

**Training Details.** We follow the training code released by a prior work [34]. We report the best performance from the original paper and compare them with Adam-SPD. More details are in Appendix 8.4.

\begin{table}
\begin{tabular}{c|c|c c c c|c c c} \hline \hline  & ID & \multicolumn{4}{c|}{OOD} & \multicolumn{4}{c}{Statistics} \\  & Clean & Fog & Defocus & Gaussian & Brightness & OOD Avg. & ID \(\Delta\) (\%) & OOD \(\Delta\) (\%) \\ \hline Vanilla FT & 66.03 & 56.72 & 38.04 & 23.21 & 58.03 & 44.00 & 0.00 & 0.00 \\ Adapter [9] & 71.85 & 69.36 & 50.94 & 37.43 & 68.26 & 56.50 & 8.82 & 28.40 \\ Birit [47] & 70.31 & 67.00 & 46.39 & 30.61 & 66.22 & 52.56 & 6.49 & 19.44 \\ L2-SP [13] & 73.47 & 69.87 & 49.20 & 39.10 & 68.61 & 56.70 & 11.27 & 28.85 \\ MARS-SP [12] & 66.24 & 56.97 & 37.29 & 21.82 & 58.27 & 43.59 & 0.32 & -0.94 \\ LLRD [48] & 72.09 & 68.13 & 46.18 & 37.28 & 66.30 & 54.47 & 9.18 & 23.79 \\ TPGM [10] & 72.56 & 69.51 & 50.88 & 38.62 & 68.82 & 56.96 & 9.89 & 29.44 \\ FTP [11] & 73.79 & 71.10 & 52.63 & 40.25 & 69.81 & 58.45 & 11.76 & 32.83 \\ \hline Adam-SPD & **74.27** & **71.74** & **53.41** & **44.17** & **70.92** & **60.06** & **12.47** & **36.50** \\ \hline \hline \end{tabular}
\end{table}
Table 4: Pascal Semantic Segmentation Results with SWIN-Tiny transformers (ImageNet21K pretrained). Performance is measured by mIoU\(\uparrow\). SPD improves OOD robustness compared to vanilla fine-tuning without regularization and L2-SP by 36.5\(\%\) and \(5.8\%\), respectively.

\begin{table}
\begin{tabular}{c|c|c|c c c c c c c|c} \hline \hline PEFT & LLM & Optimizer & BoolQ & PIQA & S1QA & HellaSwag & WinoGrande & ARC-e & ARC-e & OBOA & Avg. \\ \hline \multirow{2}{*}{Series} & LLaMA\({}_{T2B}\) & AdamW & 63.0 & 79.2 & 76.3 & 67.9 & 75.7 & 74.5 & 57.1 & 72.4 & 70.8 \\  & Adam-SPD (1.0) & **68.3** & **80.4** & **77.4** & **81.6** & **79.7** & **79.4** & **63.5** & **78.4** & **76.1** \\ \hline \multirow{2}{*}{Parallel} & LLaMA\({}_{T2B}\) & AdamW & 67.9 & 76.4 & **78.8** & 69.8 & 78.9 & 73.7 & 57.3 & 75.2 & 72.3 \\  & Adam-SPD (1.0) & **68.8** & **80.9** & 78.3 & **82.0** & **80.8** & **80.0** & **63.1** & **78.0** & **76.5** \\ \hline \multirow{2}{*}{LoRA} & LLaMA\({}_{T2B}\) & AdamW & 68.9 & 80.7 & 77.4 & 78.1 & 78.8 & 77.8 & 61.3 & 74.8 & **74.7** \\  & Adam-SPD (0.7) & **69.1** & **82.8** & **78.9** & **84.8** & **80.7** & **80.9** & **65.8** & **79.2** & **77.8** \\ \hline \multirow{2}{*}{LoRA} & LLaMA\({}_{13B}\) & AdamW & 72.1 & 83.5 & 80.5 & 80.5 & **83.7** & 82.8 & 68.3 & 82.4 & 80.5 \\  & Adam-SPD (1.2) & **72.9** & **85.6** & **80.7** & **92.0** & **83.7** & **85.6** & **71.6** & **85.6** & **82.2** \\ \hline \hline \end{tabular}
\end{table}
Table 5: Accuracy comparison of LLaMA-7B (-13B) with different adapters and optimizers on eight commonsense reasoning datasets. SPD consistently improves fine-tuning performance on multiple PEFT methods across all datasets. Note that AdamW employs uniform weight decay by default.

### Visual Question Answering (VQA) Experiments

**SPD shows competitiveness across ID, near OOD, and far OOD datasets on multimodal tasks.** Apart from uni-modal tasks, SPD outperforms other baselines on multi-modal tasks. We fine-tune PaliGemma-3B model on VQAv2 [37] dataset with LoRA. In Tab. 6, SPD improves vanilla fine-tuning and other robust fine-tuning methods, achieving best ID and average OOD performance w.r.t. distribution shifts across single modalities such as vision, question, answer and combinations of multiple modalities. We also show the performance evaluation for both near and far OOD datasets. SPD is consistently more robust under different types and degrees of distribution shifts.

**Training Details.** For Adam-SPD, we fine-tune the model with a learning rate of \(1e-3\) and \(\lambda=0.5\). The regularization hyper-parameter is found through cross-validation, and the model with the best ID validation accuracy is taken. More details are in Appendix 8.4.

## 5 Limitations

SPD is a selective regularization technique explicitly designed for fine-tuning. While it can be theoretically used for pre-training, it will likely lead to poor performance because it will hinder the training of some layers. For fine-tuning, it works well because the pre-trained foundation model is _assumed_ to be a good initialization, and only small changes in a selected few layers can lead to a good local minimum. Furthermore, the level of performance gain depends on how well the foundation models are exposed to the fine-tuning and OOD data distributions during pre-training. For example, in the DomainNet experiment (Tab. 1), fine-tuning a CLIP ViT model on any other domain does not have reasonably good OOD robustness on the Quickdraw domain. One can deduce that Quickdraw is not well represented in the pre-training data of CLIP ViT.

## 6 Conclusion

Fine-tuning differs from training from scratch because it starts from a good initialization. Therefore, effective regularization is critical to retaining the knowledge of the pre-trained foundation model while fitting a model to the target distribution. We identified that 1) regularization is necessary to keep the fine-tuned model close to its initialization and maintain robustness; 2) uniform regularization can hurt model fitting if regularization is too strong. In this paper, we proposed selective projection decay (SPD), a selective version of the popular weight decay/L2-SP regularization method. With an additional few lines of code, SPD can be integrated into existing optimizers and performs selective regularization. It demonstrates superior regularization performance on different tasks and modalities in our experiments.

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c c c} \hline \hline  & ID & \multicolumn{6}{c|}{Near OOD} & \multicolumn{6}{c}{Far OOD} \\ \cline{2-11}  & \multicolumn{3}{c|}{Vision} & \multicolumn{2}{c|}{Question} & \multicolumn{2}{c|}{Answer} & \multicolumn{2}{c|}{Multimodal} & \multicolumn{2}{c|}{Adversarial} & \\  & VQAv2 & FV-VQA & CV-VQA & VQA-Rephrasing & VQA-CP v2 & VQA-CE & A/VQA & Text/VQA & Vir-Viz & OK-VQA \\ \hline Zero-Shot & 54.42 & 63.95 & 44.72 & 50.10 & 54.29 & 30.68 & 30.46 & 14.86 & 16.84 & 28.60 \\ Vanilla FT(LoRA) & 86.29 & 94.43 & **69.36** & 78.90 & 86.21 & 71.73 & 49.82 & 42.08 & 22.92 & 48.30 \\ Linear Prob. & 78.24 & 87.83 & 63.87 & 69.61 & 78.48 & 61.66 & 42.90 & 29.61 & 18.80 & 42.27 \\ L2P-FL(LoRA) & 85.97 & 93.30 & 65.93 & 76.49 & 86.16 & 72.73 & 45.68 & 31.41 & 19.01 & 43.27 \\ WiSE-FT(LoRA) & 71.36 & 85.06 & 64.55 & 66.42 & 70.89 & 48.74 & 43.95 & 36.98 & 22.41 & 42.35 \\ Adam-SPD(LoRA) & **87.39** & **95.25** & 68.85 & **79.48** & **87.27** & **73.52** & **50.90** & **43.56** & **23.05** & **50.11** \\ \hline \hline \end{tabular}
\end{table}
Table 6: Visual Question Answering Result using PaliGemma-3B. SPD outperforms baselines across ID, near OOD and far OOD datasets using LoRA. Note that L2-SP reduces to Vinilla FT with AdamW under LoRA.

Acknowledgement

This work was supported by ONR grant N00014-18-1-2829.

## References

* [1] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.6980_, 2014.
* [2] Yang You, Igor Gitman, and Boris Ginsburg. Large batch training of convolutional networks. _arXiv preprint arXiv:1708.03888_, 2017.
* [3] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes. _arXiv preprint arXiv:1904.00962_, 2019.
* [4] Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In _International conference on machine learning_, pages 1139-1147. PMLR, 2013.
* [5] Sebastian Ruder. An overview of gradient descent optimization algorithms. _arXiv preprint arXiv:1609.04747_, 2016.
* [6] Yoonho Lee, Annie S Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, and Chelsea Finn. Surgical fine-tuning improves adaptation to distribution shifts. _arXiv preprint arXiv:2210.11466_, 2022.
* [7] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. _arXiv preprint arXiv:2106.09685_, 2021.
* [8] Shwai He, Liang Ding, Daize Dong, Miao Zhang, and Dacheng Tao. Sparse-adapter: An easy approach for improving the parameter-efficiency of adapters. _arXiv preprint arXiv:2210.04284_, 2022.
* [9] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In _International Conference on Machine Learning_, pages 2790-2799. PMLR, 2019.
* [10] Junjiao Tian, Xiaoliang Dai, Chih-Yao Ma, Zecheng He, Yen-Cheng Liu, and Zsolt Kira. Trainable projected gradient method for robust fine-tuning. _arXiv preprint arXiv:2303.10720_, 2023.
* [11] Junjiao Tian, Yen-Cheng Liu, James S Smith, and Zsolt Kira. Fast trainable projection for robust fine-tuning. _Advances in Neural Information Processing Systems_, 36, 2024.
* [12] Henry Gouk, Timothy M Hospedales, and Massimiliano Pontil. Distance-based regularisation of deep networks for fine-tuning. _ICLR_, 2021.
* [13] LI Xuhong, Yves Grandvalet, and Franck Davoine. Explicit inductive bias for transfer learning with convolutional networks. In _International Conference on Machine Learning_, pages 2825-2834. PMLR, 2018.
* [14] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust fine-tuning of zero-shot models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7959-7971, 2022.
* [15] Kartik Chandra, Audrey Xie, Jonathan Ragan-Kelley, and Erik Meijer. Gradient descent: The ultimate optimizer. _Advances in Neural Information Processing Systems_, 35:8214-8225, 2022.
* [16] Xiang Wang, Shuai Yuan, Chenwei Wu, and Rong Ge. Guarantees for tuning the step size using a learning-to-learn approach. In _International Conference on Machine Learning_, pages 10981-10990. PMLR, 2021.

* Feurer and Hutter [2019] Matthias Feurer and Frank Hutter. Hyperparameter optimization. _Automated machine learning: Methods, systems, challenges_, pages 3-33, 2019.
* Gouk et al. [2021] Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael J Cree. Regularisation of neural networks by enforcing lipschitz continuity. _Machine Learning_, 110:393-416, 2021.
* Kumar et al. [2022] Ananya Kumar et al. Fine-tuning can distort pretrained features and underperform out-of-distribution. _ICLR_, 2022.
* Goyal et al. [2022] Sachin Goyal, Ananya Kumar, Sankalp Garg, Zico Kolter, and Aditi Raghunathan. Finetune like you pretrain: Improved finetuning of zero-shot vision models. _arXiv preprint arXiv:2212.00638_, 2022.
* Zhuang et al. [2020] Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon Papademetris, and James Duncan. Adabelief optimizer: Adapting stepsizes by the belief in observed gradients. _Advances in neural information processing systems_, 33:18795-18806, 2020.
* Peng et al. [2019] Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 1406-1415, 2019.
* Deng et al. [2009] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In _2009 IEEE conference on computer vision and pattern recognition_, pages 248-255. Ieee, 2009.
* Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning_, pages 8748-8763. PMLR, 2021.
* Recht et al. [2019] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In _International Conference on Machine Learning_, pages 5389-5400. PMLR, 2019.
* Hendrycks et al. [2021] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15262-15271, 2021.
* Hendrycks et al. [2021] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 8340-8349, 2021.
* Wang et al. [2019] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by penalizing local predictive power. _Advances in Neural Information Processing Systems_, 32, 2019.
* Everingham et al. [2010] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. _IJCV_, 2010.
* Liu et al. [2022] Yen-Cheng Liu, Chih-Yao Ma, Junjiao Tian, Zijian He, and Zsolt Kira. Polyhistor: Parameter-efficient multi-task adaptation for dense vision tasks. _arXiv preprint arXiv:2210.03265_, 2022.
* Liu et al. [2021] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 10012-10022, 2021.
* Xie et al. [2021] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. _Advances in Neural Information Processing Systems_, 34:12077-12090, 2021.
* Hendrycks and Dietterich [2019] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. _arXiv preprint arXiv:1903.12261_, 2019.

* [34] Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee. Llm-adapters: An adapter family for parameter-efficient fine-tuning of large language models. _arXiv preprint arXiv:2304.01933_, 2023.
* [35] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothe Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. _arXiv preprint arXiv:2302.13971_, 2023.
* [36] Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Bosnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai. PaliGemma: A versatile 3B VLM for transfer, July 2024. arXiv:2407.07726 [cs] version: 1.
* [37] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering, May 2017. arXiv:1612.00837 [cs].
* [38] Vedika Agarwal, Rakshith Shetty, and Mario Fritz. Towards Causal VQA: Revealing and Reducing Spurious Correlations by Invariant and Covariant Semantic Editing. In _2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, pages 9687-9695, Seattle, WA, USA, June 2020. IEEE.
* [39] Meet Shah, Xinlei Chen, Marcus Rohrbach, and Devi Parikh. Cycle-Consistency for Robust Visual Question Answering, February 2019. arXiv:1902.05660 [cs].
* [40] Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering, June 2018. arXiv:1712.00377 [cs].
* [41] Corentin Dancette, Remi Cadene, Damien Teney, and Matthieu Cord. Beyond Question-Based Biases: Assessing Multimodal Shortcut Learning in Visual Question Answering, September 2021. arXiv:2104.03149 [cs].
* [42] Sasha Sheng, Amanpreet Singh, Vedanuj Goswami, Jose Alberto Lopez Magana, Wojciech Galuba, Devi Parikh, and Douwe Kiela. Human-Adversarial Visual Question Answering, June 2021. arXiv:2106.02280 [cs].
* [43] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards VQA Models That Can Read, May 2019. arXiv:1904.08920 [cs].
* [44] Jeffrey P Bigham, Chandraika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert C Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual White, and Tom Yeh. VizWiz: nearly real-time answers to visual questions.
* 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 1-5, June 2023. ISSN: 2379-190X.
* [46] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers and distillation through attention. In _International Conference on Machine Learning_, volume 139, pages 10347-10357, July 2021.
* [47] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. _arXiv preprint arXiv:2106.10199_, 2021.

* [48] Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q Weinberger, and Yoav Artzi. Revisiting few-sample bert fine-tuning. _arXiv preprint arXiv:2006.05987_, 2020.
* [49] Lukas Balles and Philipp Hennig. Dissecting adam: The sign, magnitude and variance of stochastic gradients. In _International Conference on Machine Learning_, pages 404-413. PMLR, 2018.
* [50] Guillaume Garrigos and Robert M Gower. Handbook of convergence theorems for (stochastic) gradient methods. _arXiv preprint arXiv:2301.11235_, 2023.
* [51] Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine learning. _SIAM review_, 60(2):223-311, 2018.
* [52] Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Fractalnet: Ultra-deep neural networks without residuals. _arXiv preprint arXiv:1605.07648_, 2016.
* [53] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 2818-2826, 2016.
* [54] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. _arXiv preprint arXiv:1710.09412_, 2017.
* [55] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 6023-6032, 2019.
* [56] Dongxu Li, Junnan Li, Hung Le, Guangsen Wang, Silvio Savarese, and Steven C. H. Hoi. Lavis: A library for language-vision intelligence, 2022.

Appendix

### Extended Related Works

**Other Robust Fine-Tuning Methods.** WiSE-FT [14] discovers that linearly interpolating between the fine-tuned and pre-trained models after fine-tuning can improve out-of-distribution robustness. This demonstrates that a closer distance to the pre-trained model can improve robustness. However, it only applies to models with zero-shot capabilities. Another orthogonal line of research for robust fine-tuning focuses on feature distortion. LP-FT [19] shows that fine-tuning with a randomly initialized head layer distorts learned features. It proposes a simple two-stage method to train the head layer first and then fine-tune the entire model. FLYP [20] shows that fine-tuning a foundation model using the same objective as pre-training can better preserve the learned features. Our contribution is an optimization method to penalize the derivation between the fine-tuned and pre-trained models explicitly during fine-tuning, which is orthogonal to them.

### Interpreting \(c_{t}\) as an Early Layer Selection Criterion

In previous sections, we interpreted the selection condition \(c_{t}\) in SPD as a measure of consistency between the current heading direction and the gradient direction. This perspective is more valid when the algorithm has accumulated some updates, i.e., \(\|\theta_{t}-\theta_{0}\|_{2}\gg 0\), and less justified when a heading has not been established at the beginning of training. This section discusses SPD from the perspective of _stochastic_ optimization when \(\|\theta_{t}-\theta_{0}\|_{2}\) is small at the beginning of training.

**Inner product of gradients captures gradient variance.** Modern deep learning models are trained by stochastic optimization techniques, e.g., mini-batch SGD, leading to stochasticity due to sampling. We first show that the inner product of gradients captures the variance of a sampling process. We invoke a common assumption in the convergence analysis of stochastic gradient descent [1, 49, 21]. Assuming that the stochastic gradient \(g_{t}\) is a stationary process \(\mathcal{G}\) over a short period, with a small step size, successive gradients, e.g., \(g_{t},g_{t+1}\), can be seen as samples drawn from the same distribution \(\mathcal{G}\). Given two successive draws \(g_{1}\) and \(g_{2}\), we can approximate the first and second moment of \(\mathcal{G}\).

\[\mathbb{E}\left[\|g\|^{2}\right]\approx\frac{1}{2}(\|g_{1}\|^{2}+\|g_{2}\|^{2}),\qquad\|\mathbb{E}\left[g\right]\|^{2}\approx\|\frac{1}{2}(g_{1}+g_{2})\|^{ 2}. \tag{12}\]

Define the _variation of gradients_ as \(Var(g):=\mathbb{E}\left[\|g-\bar{g}\|^{2}\right]\)[50, 51], where \(\bar{g}:=\mathbb{E}[g]\),we can show that

\[g_{1}^{\intercal}g_{2} =2\left(\frac{1}{4}\|g_{1}\|^{2}+\frac{1}{4}\|g_{2}\|^{2}+\frac{1 }{2}g_{1}^{\intercal}g_{2}\right)-\frac{1}{2}\left(\|g_{1}\|^{2}+\|g_{2}\|^{2 }\right) \tag{13}\] \[\approx\|\bar{g}\|^{2}-\left(\mathbb{E}\left[\|g\|^{2}\right]-\| \mathbb{E}\left[g\right]\|^{2}\right)=\|\bar{g}\|^{2}-Var(g)\]

**Remarks.** Eq. 13 shows that the inner product of two consecutive stochastic gradients, under certain assumptions, can be seen as the estimator for the difference between the gradient norm and the variance of gradients. When the inner product is negative, this indicates that the variance outweighs the magnitude of the gradient.

**SPD prioritizes layers with higher expected gain.** At the beginning of training, the heading direction \((\theta_{1}-\theta_{0})\) is dominated by early gradients. For example, at \(t=2\) the direction of \((\theta_{1}-\theta_{0})\) is the same as \(-g_{1}\) in Adam. The sign of \(-g_{2}^{\intercal}(\theta_{1}-\theta_{0})\) is the same as the sign of \(g_{2}^{\intercal}g_{1}\). This shows that the condition \(c_{t}\) captures the difference between gradient norm and gradient variance. With this interpretation, we show that \(c_{t}\) reflects _expected performance gain_ in stochastic optimization. To see it, we can invoke the descent lemma for SGD. For an L-smooth function \(f(W)\)[50], the descent lemma for SGD states that,

**Lemma 1**.: \[\underbrace{\mathbb{E}[f(\theta_{k+1})]-f(\theta_{k})}_{\text{Expected Performance Gain}}\leq\underbrace{-\eta_{k}(1-\frac{\eta_{k}L}{2})}_{\leq 0}\|\bar{g}_{k}\|^{2}+ \underbrace{\frac{\eta_{k}^{2}L}{2}}_{\geq 0}Var(g_{k}),\]

where \(\eta_{k}\leq\frac{2}{L}\) is the learning rate.

**Remarks.** The term on the left hand side \(\mathbb{E}[f(\theta_{k+1})]-f(\theta_{k})\) is the expected performance improvement for each step. Ideally, this should be a negative quantity. On the right-hand side, we observe that improvement depends on two quantities \(\|\bar{g}_{k}\|^{2}\) and \(Var(g_{k})\). To lower the upper bound, we want a _large_\(\|\bar{g}_{k}\|^{2}\) and a _small_\(Var(g_{k})\). According to the decoupling Eq. 13, the inner product between successive gradients approximates this proportionality. Consequently, a negative \(c_{t}\) likely indicates a higher upper bound on the expected gain, meaning a smaller improvement. Therefore, SPD will prioritize layers with potentially larger expected gains.

### Approximation in L2-SP

```
Initialize\(m_{0}\gets 0\), \(v_{0}\gets 0\), \(t\gets 0\) While\(\theta_{t}\) not converged \(t\gets t+1\) \(g_{t}\leftarrow\nabla_{\theta}\tilde{\mathcal{L}}(\theta_{t-1})\) \(m_{t}\leftarrow\beta_{1}m_{t-1}+(1-\beta_{1})g_{t}\) \(v_{t}\leftarrow\beta_{2}v_{t-1}+(1-\beta_{2})g_{t}^{2}\) Bias Correction \(\widehat{m_{t}}\leftarrow\frac{m_{t}}{1-\beta_{1}^{\intercal}}\), \(\widehat{v_{t}}\leftarrow\frac{v_{t}}{1-\beta_{2}^{\intercal}}\) Update \(\tilde{\theta}_{t}\leftarrow\theta_{t-1}-\frac{\alpha\widehat{m_{t}}}{\sqrt{ \widehat{v_{t}}+\epsilon}}\) \(\theta_{t}\leftarrow\tilde{\theta}_{t}-\lambda\alpha(\tilde{\theta}_{t}-\theta_ {0})\)
```

**Algorithm 3**(Ours) Adam with L2-Regularization

The Adam with L2-SP Regularization algorithm in the main paper is not the precise mathematical implementation of the original formulation written in Eq. 1. To see the difference, we compare ours and the accurate implementation here in Alg. 3 and Alg. 4. In our implementation, we replaced \(\theta_{t-1}-\theta_{0}\) (Alg. 4) with \(\tilde{\theta}_{t}-\theta_{0}\) (Alg. 3). This is done intentionally to improve memory efficiency when transitioning to the selective version (see Adam-SPD in Sec. 3.3). We can make the following substitution to see how this modification changes computation. Starting from our implementation,

\[\theta_{t} =\tilde{\theta}_{t}-\lambda\alpha(\tilde{\theta}_{t}-\theta_{0}) \tag{14}\] \[=\tilde{\theta}_{t}-\lambda\alpha(\theta_{t-1}-\frac{\alpha \widehat{m_{t}}}{\sqrt{\widehat{v_{t}}}+\epsilon}-\theta_{0})\] \[=\tilde{\theta}_{t}-\lambda\alpha(\theta_{t-1}-\theta_{0})+ \lambda\alpha^{2}\frac{\widehat{m_{t}}}{\sqrt{\widehat{v_{t}}}+\epsilon}.\]

We can further combine the \(\lambda\alpha^{2}\frac{\widehat{m_{t}}}{\sqrt{\widehat{v_{t}}}+\epsilon}\) into the update of \(\tilde{\theta}_{t}\). The new \(\tilde{\theta}_{t}\) is

\[\tilde{\theta

### Training Details

**DomainNet.** We use the vision transformer public repository for DEIT [46] to fine-tune all methods. Standard augmentations are used for all: weight-decay (\(0.1\)), drop-path (\(0.2\)) [52], label-smoothing (\(0.1\)) [53], Mixup (\(0.8\)) [54] and Cutmix (\(1.0\)) [55]. The learning rate is \(2e-5\) and trained for 60 epochs for Tab. 1 and 30 epochs for Tab. 2. We use \(\lambda=1\) for all Adam-SPD results in Tab. 1. We use 1 A40 GPU for each experiment.

**ImageNet.** The same procedure as the DomainNet experiment is used for training the ImageNet models. Standard augmentations are used for all: weight-decay (\(0.1\)), drop-path (\(0.2\)) [52], label-smoothing (\(0.1\)) [53], Mixup (\(0.8\)) [54] and Cutmix (\(1.0\)) [55]. We fine-tune all methods for 30 epochs and use the best hyper-parameters reported by the prior work [11]. For Adam-SPD, we fine-tune the model with a learning rate of \(3e-5\) and \(\lambda=1.4\). The regularization hyper-parameter is found through cross-validation, and the model with the best ID validation accuracy is taken. We use 2 A40 GPUs for each experiment.

**Pascal Segmentation.** We follow the training code released by a prior work [30]. We fine-tune all methods for 60 epochs and use the best hyper-parameters reported by the prior work. For Adam-SPD, we fine-tune the model with a learning rate of \(1e-4\) and \(\lambda=2.2\). The regularization hyper-parameter is found through cross-validation, and the model with the best ID validation accuracy is taken. We use 4 2080Ti GPUs for each experiment.

**Commonsense-170K.** We follow the training code released by a prior work [34]. We report the best performance from the original paper and compare them with Adam-SPD. For Adam-SPD, we fine-tune the model with an identical hyper-parameter setup as the released code and only adjust the regularization strength \(\lambda\). The regularization hyper-parameter is found through cross-validation, and the model with the best ID validation loss is taken. We use 1 A40 GPU for each experiment.

**Visual Question Answering.** We follow the LAVIS [56] public repository to fine-tune all methods. We use the PaliGemma [36] model pretrained with \(224*224\) input images and \(128\) token input/output text sequences and fine-tune with the precision of bfloat16. Standard hyper-parameters are used for all: learning rate (\(1e-3\)), weight-decay (\(1e-4\)), optimizer (AdamW), scheduler (Linear Warmup With Cosine Annealing), warm-up learning rate (\(1e-4\)), minimum learning rate (\(1e-4\)), accumulation steps (\(2\)), beam size (\(5\)). The model is trained for \(10\) epochs with a batch size of \(16\) for Tab. 6. For LoRA [7], we limit our study to only adapting the attention weights and freeze the MLP modules for parameter-efficiency, specifically apply LoRA to \(W_{q},W_{k},W_{v},W_{o}\) with \(r=8\) in Tab. 6. We use \(\lambda=0.5\) for SPD. The regularization hyper-parameter is found through cross-validation, and the model with the best ID validation accuracy is taken. We use 8 A40 GPU for each experiment.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification:

Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material?Answer: [Yes]

Justification:

Guidelines:

* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Guidelines:

* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Guidelines:

* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.

* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [No] Justification: The work does not present an obvious negative societal impact. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. * Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. * The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. * The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. * If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. * For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. * If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?Answer: [No]

Justification:

Guidelines:

* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: Guidelines:

* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.