# Motivation-Aware Session Planning over Heterogeneous Social Platforms

Anonymous Author(s)

Submission Id: 919

###### Abstract.

With the explosive growth of online service platforms, an increasing number of people and enterprises are undertaking personal and professional tasks online. In real applications such as trip planning and online marketing, planning sessions for a sequence of activities or services will enable social users to receive the optimal services, improving their experience and reducing the cost of their activities. These online platforms are heterogeneous, including different types of services with different attributes. However, the problem of session planning over heterogeneous platforms has not been studied so far. In this paper, we propose a Motivation-Aware Session Planning (MASP) framework for session planning over heterogeneous social platforms. Specifically, we first propose a novel HeterBERT model to handle the heterogeneity of items at both type and attribute levels. Then, we propose to predict user preference using the motivations behind user activities. Finally, we propose an algorithm together with its optimisations for efficient session generation. The extensive tests prove the high effectiveness and efficiency of MASP.

2018

Session planning, Heterogeneous social platform 2018

## 1. Introduction

The popularity of online service platforms has provided a vital channel for people and enterprises to undertake personal and professional activities online. Recent statistics show there are now 2.8 million active Australians on TripAdvisor and 1.5 million users on Yelp 1. Users get access to these online service platforms for various purposes such as trip planning and online purchase. These have given rise to a demand for assisting users in planning sessions of activities they wish to engage in. Popular platforms like Meituan and Google Maps provide services to numerous travellers for information on points of interest, as they offer item details and recommendations. According to EnterpriseAppsToday 2, Google Maps locates hundreds of millions of places and businesses. More than a billion people use Google Maps every month to search for destinations and check the best routes. However, these platforms only recommend a list of items based on item type or keywords. In practice, users could set up a set of activities and require a detailed travel plan. Take travel planning as an example as shown in Figure 1. The user gives a set of interested activities and the system provides a plan that contains exact items and corresponding time. Thus, designing advanced session planning solutions becomes a new research problem and is promising for improving the service quality of these platforms, and improving their user experience.

Footnote 1: [https://www.socialmedianews.com.au](https://www.socialmedianews.com.au)

Footnote 2: [https://www.socialmedianews.com.au](https://www.socialmedianews.com.au)

Session planning has contexts and objectives that are different from those for Session-Based Recommender Systems (SBRS) and Personalized Route Planning (PRP). Traditionally, SBRS [(10; 14; 16; 29; 41)] predict the next item or item session based on historical sessions. PRP methods [(5; 21; 31)] typically generate user-specific routes in response to users' queries, considering user preferences and other factors like checkpoints or distance constraints. However, in session planning, users provide multiple activity categories, resembling a set of item categories, such that the system predicts the optimal session plans for their future actions based on historical item sessions. A well-generated session should align with the user preferences and certain related constraints. Figure 1 shows examples of SBRS, PRP, and session planning. Suppose a user \(u_{i}\) named Jack recently visits 7-Eleven from home, and his profile keeps his historical activity sessions as shown in Figure 1 (a). SBRS would suggest McDonald's and ANZ Bank since his profile keeps a historical session of activities, _Home, 7-Eleven, McDonald's, ANZ Bank_. As shown in Figure 1 (b), Jack wants to travel from home to _The Flyfisher_, with two checkpoints, _IGA Hawthorn_ and _QV Melbourne_. PRP would provide several routes that pass through all the POIs, based on his historical routes, considering different modes of transport, travel time, and transportation costs. While Jack would like to go to QV Melbourne from home and wants to conduct four activities: shopping, refuelling, having lunch, and finding a parking lot as shown in Figure 1 (c), he needs the system to plan his activities. The session planning could generate a series of specific POIs, _7-Eleven, Wilson Parking, Qv Melbourne, Grill 4 QV_, for his planned activities.

This paper proposes a _Session Planning_ over _Heterogeneous social Platform_ (_SPHP_) problem, where users provide sets of activity categories and request ordered item sequences. The platforms could provide heterogeneous services (items) to users, and users can request multiple services. However, a big challenge is that wecannot predict an optimal session if only the contexts (source and destination) and objectives (service/activity categories) are provided. This is because a user may have dynamic preference for each activity category and the activity categories in a session are usually unrelated with each other. As shown in Figure 0(c), the activity categories, _refueling_ and _having lunch_, have no connections for current user session prediction. Thus, a system generated plan may not reflect user's dynamic preferences on activities and overall interests with respect to the time and distance constraints of the session. **How can we predict the optimal activity sessions without knowing the dynamic preferences of users?** Another challenge is that the item variety on platforms causes data heterogeneity at both attribute and type levels. Unlike SBRS where a session includes highly related items or user behaviours of the same type, the items within a session can vary significantly from each other in SPHP. As shown in Figure 0(c), the gas station 7-Eleven and the restaurant Grill'd QV are two different types of establishments, each with distinct services. Existing solutions for heterogeneity problem take item attributes as auxiliary information (Sendry et al., 2017) or describe each item as an attribute set (Kumar et al., 2017), which generates item embeddings with extremely high dimensionality and ignores the correlation between attributes and types. Table 1 shows three items with their attributes from Yelp. Clinic 1 and Restaurant 1 belong to different categories and have different attributes. Restaurant 1 and Restaurant 2 belong to the same category, but still have different attributes such as Alcohol and GoodForKids. **How can we model the heterogeneous items with variable and large number of attributes and types?** SPHP is a problem of global optimisation over all the services on heterogeneous platforms. Since the volume of services is huge, we have a further challenge on real-time response of the system. **How can we quickly identify the activity sessions over a huge number of heterogeneous online services?**

This paper proposes Motivation-Aware Session Planning (MASP) framework that fully exploits the driving force behind activities to predict the user preferences for SPHP. We first propose a novel HeterBERT model to capture the attribute-level and type-level heterogeneity. Then, we design a motivation-aware solution to generate motivation-aware item/user embeddings. Final plans are obtained by multi-constraints session generation.

* We propose a novel session planning over heterogeneous social platform (SPHP) problem, which generates optimal sessions for users requesting services.
* We propose a novel HeterBERT model to address the attribute-level heterogeneity. HeterBERT well handles the problem of uncertain attribute number of items and captures the type-attribute correlation in heterogeneous items.
* We design a motivation-aware prediction solution that fully exploits the motivations behind activities to capture the dynamic preferences of user for session planning.
* We propose a multi-constraints session generation algorithm together with optimisation strategies that enables effective and efficient multi-constraints session generation. The test results prove the performance of MASP.

## 2. Related Work

This research is relevant to session-based recommendation, personalized route planning and heterogeneous social media processing.

SBRS learn users' preferences from the sessions associated and generated during the consumption process. Each session includes multiple user-item interactions occurring together over a period, typically lasting for up to several hours. Conventional SBRSs (Kumar et al., 2017; Kumar et al., 2017) employ data mining or machine learning techniques to capture the dependencies embedded in sessions for recommendations. Latent representation-based SBRS (Kumar et al., 2017; Kumar et al., 2017) construct a low-dimensional latent representation for each interaction within sessions using shallow models for recommendation. Recently, DNN-based SBRSs (Kumar et al., 2017; Kumar et al., 2017; Kumar et al., 2017; Kumar et al., 2017; Kumar et al., 2017) has been popular due to their powerful capabilities to model the complex intra-session and inter-session dependencies. In (Kumar et al., 2017), a session graph and its mirror graph are constructed, and the information propagation between them is conducted with an iterative dual refinement for representation learning. GRec (Shao et al., 2017) adopts CNN with sparse kernels for item and session embeddings.SEOL (Song et al., 2017) enhances recommendation using session tokens, session segment embeddings, and temporal self-attention. KMVG (Chen et al., 2018) learns three item representations from the knowledge graph, contextual transitions in sessions, and local item-item relationships, which are merged as the final item representation. In SPHP, a session is an activity plan that includes items to interact with a user shortly. However, in SBRS, a session refers to a series of history or current user behaviours in a period. Thus, SPHP is a new research problem. The SBRS methods cannot be applied or extended for SPHP.

Route planning aims to generate the top \(k\) probable routes that satisfy a query containing the origin, destination, a set of checkpoints, a maximum time cost, etc. Conventional methods (Brockman et al., 2016; Chen et al., 2018; Chen et al., 2018) aim to find the optimal routes according to a specified objective, such as minimizing distance, time, or cost. Recent deep learning (DL)-based methods (Chen et al., 2018; Chen et al., 2018; Chen et al., 2018; Chen et al., 2018; Kumar et al., 2017) prevail in route planning since they can discover complex relationships among data. NASR+ (Kumar et al., 2017) models the observable trajectory by attention-based RNNs and estimates the future cost using position-aware graph attention

\begin{table}
\begin{tabular}{l|l} \hline
**Items** & **Attitudes** \\ \hline Clinic 1 & Accepts Insurance; By Appointment Only; Business Accepts Bitcoin. \\ \hline Restaurants 1 & Good For Mae;WH; Conf For Kids; Has TV; Restaurants Beversations; Business Pricing. \\ \hline Restaurants 2 & Restaurants Attire; Business Accepts CreditCards; Alcohol; Good For Kids; Restaurants Restrictions; Business Pricing; Risk Parting; Restaurants Delivery. \\ \hline \end{tabular}
\end{table}
Table 1. Variety of items.

networks. SpeakNav (Abadi et al., 2016) exploits BERT to extract clues from user speeches for generating routes. MaORL (Ma et al., 2017) adopts a multi-agent multi-objective reinforcement learning framework. VIAL (Zhou et al., 2018) enhances A' algorithm with a variational inference-based estimator to model the distribution of travel time between two nodes. MAC (Mikolov et al., 2013) learns the knowledge from geographical and semantic neighbours to be combined for predicting the next item. In SPHP, a user keeps implicit objectives and only gives a set of activities, while PRP methods require explicit objectives like checkpoints and transport modes. In addition, existing PRP (Zhou et al., 2018; Ma et al., 2017; Ma et al., 2017) only handle heterogeneous data from item level and interaction level, while cannot handle attribute-level heterogeneity in SPHP.

Heterogeneous social media has been handled using cross-domain and transfer learning (TL)-based methods. Cross-domain methods (Zhou et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018) address the data heterogeneity using the information from multiple domains or sources. For example, GCBAN (Zhou et al., 2018) embeds items/users and their auxiliary information into two latent spaces for each data domain. Two types of latent features are concatenated and applied to a Gaussian-based probabilistic model for recommendation. EquiTensors (Zhou et al., 2018) aligns heterogeneous datasets to a consistent spatio-temporal domain, and learns shared representations using convolutional denoising autoencoders. HetSANN (Li et al., 2018) constructs a heterogeneous user-item graph. An attention mechanism learns the node embeddings for node classification. However, HetSANN requires uniform attributes, thus inapplicable to handling the attribute-level heterogeneity in SPHP. TL-based models (Zhou et al., 2018; Li et al., 2018) exploit the knowledge gained from a previous task to generalize the model for other tasks. DJTCDR (Djumdar et al., 2018) transfers information between two types of items by dual learning. BALANCE (Li et al., 2018) downside from dynamic and heterogeneous workloads by transfer reinforcement learning. In SPHP, the number of attributes is variable and large, causing attribute-level heterogeneity. However, none of existing PRP can handle the attribute-level heterogeneity.

## 3. Problem Formulation

This section defines the concepts of item, activity, and session, and formally formulates the SPHP problem.

Definition 1 ().: _In heterogeneous social platforms, an **item** refers to a real-world entity, such as a restaurant or store, providing a type of service at a specific location and time period. An **activity** refers to a particular type of user behaviour or action such as shopping, park visits, or medical appointments. Each activity is taken by users through the platform's service. A user may request multiple services from different items within a time period._

Each item has several content attributes that describe its properties, and two contextual attributes, geographic location and opening hours. An item can have up to 33 attributes. The heterogeneous items present special characteristics in contrast to general items.

* Attribute-level heterogeneity: The number of attributes in each heterogeneous item can be large and different items may have different numbers of attributes.
* Type-level heterogeneity: A social platform includes various item types such as "restaurant" and "hotel". Different types normally share few common attributes.
* Type-attribute correlation: Attributes are associated with types. Even if some attributes from different item types share common names (e.g. "price" in "hotel" and "restaurant"), they reflect different semantic meanings.

In practice, a user may take a series of activities within a time period. A number of items, each of which is associated with an activity, are arranged in order to form a session.

Definition 2 ().: _A **session** is a service plan for a user including a series of items to be interacted by the user. Formally, a session is a series \(<\)\({}_{1}\),\({}^{\sim}\), \({}_{q}\)\(>\), where \({}_{q}\) is the \(i^{th}\) item and \(q\) is the session length._

Given a user and a set of his/her planned activities, an ideal session should contain the items that provide these service activities, satisfying the user's preference, under the context constraints. The problem of _SPHP_ is formally defined as follows:

Definition 3 ().: _Given a user, a set of activities \(\{e_{1},\cdots,e_{q}\}\), and a session score function score, SPHP aims to detect a list of sessions with the highest probability scores, satisfying the constraints below:_

* _Distance constraint: a user can only visit the items that are within the radius of this user._
* _Time constraint: each item has available time and a user can only visit the available items._

We address the problem of effective and efficient SPHP, and propose a motivation-aware session planning framework (MASP) for SPHP. Here, _motivation_ refers to the internal or external driving force, such as seeing doctor or sales promotion, that drive individuals to undertake specific actions. Motivation may stem from users' needs, desires, goals, or external stimuli, which guide user behaviours to fulfil their objectives. When planning activities, individuals typically align their behaviours with their underlying motivations to achieve their goals. Figure 2 shows MASP that mainly contains three parts: heterogeneous data model, motivation-aware preference prediction, and multi-constraints session generation.

## 4. Motivation-Aware Session Planning

Intuitively, a set of activities could imply users' motivations which lead to different strategies when selecting individual activities. Figure 3 shows two activity sets \(<\)parking, go to hospitality- and \(<\)parking, go hiking\(>\). When users are seeking medical care, they may prioritize hospitals with closer parking facilities. However,

Figure 3. Motivations behind activities.

Figure 2. Overview of MASP framework.

when heading out for outdoor activities, they might value parking lots that are more affordable. This indicates that motivations behind activities could influence users' decisions.

### Heterogeneous Data Model

We build a model to discover the heterogeneous item types with an uncertain number of correlated attributes.

#### 4.1.1. HeterBERT

Each heterogeneous item on social platforms can be described as a set of attributes, and different items can have different numbers of attributes. To represent a heterogeneous item, one-hot encoding takes the item attributes as auxiliary information (Hardt et al., 2016). Other approaches represent each item as a set of attributes (Hardt et al., 2016). With these methods, the dimensionality of an item embedding equals the number of attributes. The total number of attributes could be huge, which leads to a large size of one-hot encoding and incurs the curse of dimensionality, further incurring high memory costs for data storage and high time costs for data updates introduced by new items or attributes. In addition, all existing methods ignore the correlation between attributes and types, which fails to capture the item attributes under different types. BERT-based model is suitable for text embedding learning, which addresses the uncertain attribute number problem by the BERT padding operations. By dividing attributes into words, BERT can learn word embedding and indirectly learn attribute embedding as item representation. However, turning each attribute sentence into separate words, the BERT-based model cannot capture the word correlation in the same attribute, and thus cannot address the attribute heterogeneity of items. In addition, BERT cannot handle type-level heterogeneity or capture the type-attribute correlation since it ignores the type information. For example, the word "insurance" carries completely different meanings when it comes to hotels and clinics. We need to build a model that is robust to the heterogeneity of items at attribute and type levels and captures the type-attribute correlation.

We propose a novel HeterBERT model for heterogeneous items, as shown in Figure 4. HeterBERT advances BERT (He et al., 2016) in fourfold: (1) HeterBERT adopts our new proposed recurrence positional encoding and type encoding to keep the correlation among inner-attribute words and capture the type information respectively; (2) a new type attention layer is designed to inject type into word embedding learning, with a double threshold mechanism to enhance the correlation among inner-attribute words. (3) a C-Merge layer is proposed to capture the type-attribute correlation; (4) considering the type-level heterogeneity, a contrastive learning task is designed to learn discriminative embeddings.

**Input Embedding.** Given an item, the input embedding layer divides the item attributes into words and generates an initial embedding for each word. To address the heterogeneity, we propose a recurrence positional encoding which considers the relative position of the words within each attribute while ignoring the relative position of attributes. Figure 5 shows an example of this recurrence positional encoding. When the special token _isep_ emerges, we reset the index of tokens. The positional embedding is generated by the sine and cosine functions as in (Wang et al., 2017). By this encoding, the generated word embeddings can capture correlation among words in the same attribute, while allowing the attribute orders to be swapped for fitting the characteristics of the item attributes semantically.

To capture type information in embedding learning, intuitively, type vectors should be distinct from each other to make word embeddings discriminative from the type aspect. We propose a type encoding model as Figure 6 shows. Each type is first divided into words and the corresponding token embeddings \(E\) are integrated by linear transformation \(e=W_{c}E\) to generate intermediate type representations \(E_{C}=[e_{1},\cdots,e_{C}]\), where \(W_{c}\) is the transformation matrix. Though each type has a parameter matrix \(W_{c}\), the complexity of this type encoding model is limited, since the number of types is not large and the model architecture is shallow. Another linear transformation \(\bar{E}_{C}=WE_{C}\) is applied on \(E_{C}\) to generate the final type representation. We aim to maximise the difference among type representations and formulate the loss function as below:

\[Loss=1/Var(\bar{E}_{C}), \tag{1}\]

where \(Var(\bar{E}_{C})\) is the variance of matrix \(\bar{E}_{C}\). This loss function is supported by Theorem 1 proved in Appendix A.1:

**Theorem 1**.: _Given a set of type representations with mean value \(\mu\), the distinct difference among these representations is achieved when \(Var(\bar{E}_{C})\) is maximised._

The loss function has no regularization term, as we train this model over all types and directly use \(\bar{E}_{C}\) as the type representations, which avoids the overfitting issue. Given an item, we generate the initial embeddings by summing the corresponding token embedding (Wang et al., 2017), recurrence positional encoding, and type encoding.

**Type Attention.** Given type representations and initial word embeddings, we feed them into the transformer layer. As shown in Figure 4b (we omit softmax, scaling layers, residual connection, and normalization layers for convenience), input embeddings are projected into word-level query, key, and value matrices \(Q,K,\) and \(V\) by learned linear transformation matrices. Unlike the vanilla attention

Figure 4. HeterBERT.

Figure 5. Recurrence positional encoding.

Figure 6. Type encoding model.

layer that only feeds \(Q\) and \(K\) into the matrix multiplication layer to compute the word-level attention weights, we design C-Merge to capture the correlation among inner-attribute words, as Figure 4c shows. We merge word- and attribute-level attention weights, capturing the correlation among inner-attribute words. Specifically, given a word-level query matrix \(Q\), a key matrix \(K\), and type representation \(\tilde{E}_{C}\), we first divide \(Q\) and \(K\) into subsets and feed them into the C-Merge layer, each of which contains word embeddings to the same attribute. Given a subset, the C-Merge layer computes inner-attribute word weights by \(W_{att}=HE\), where \(E\) is the concatenation of word embeddings and the item type representation, \(H\) is the transformation matrix. Then, we can get the attribute embedding by merging word embeddings \(e_{att}=W_{att}\tilde{E}^{T}\), where \(\tilde{E}\) contains word embeddings. Especially, the C-Merge layers of each type attention layer share parameters and type representation to reduce the computation time and maintain type information in the whole embedding learning. The attribute-level query and key matrices \(Q^{\prime}\) and \(K^{\prime}\) are generated by combining all attribute embeddings from \(Q\) and \(K\) respectively. Given attribute-level \(Q^{\prime}\) and \(K^{\prime}\), we compute attribute-level attention weights by the dot product of these matrices. We select the word- and attribute-level weights larger than their thresholds to enhance the correlation among words within the same attribute. Then the selected weights are merged by weighted sum to generate the new word-level weights:

\[W^{*}=w_{T}W_{T1}+(1-w_{T})W_{T2}, \tag{2}\]

where \(W_{T1}\) and \(W_{T2}\) are filtered weights and \(w_{T}\) is a trade-off parameter. The attention layer feeds the value matrix \(V\) and the new word-level weights into the matrix multiplication layer to derive the updated word embeddings. The output word embeddings are fed into a position-wise fully connected feed-forward network with a residual connection normalization (Shi et al., 2017). The feed-forward network produces updated word embeddings for the next attention layer.

Given the output word embeddings of the last attention layer, we feed them into a C-Merge layer in HeterBERT (Figure 4a) to get attribute embeddings, injecting the type into attribute embeddings and capturing the type-attribute correlation.

**Pre-training HeterBERT.** To save the training cost, we pre-train HeterBERT by two tasks: Masked Language Model (MLM) and Contrastive Learning (CL). For MLM, we randomly select 15% of the words from all attributes, adopt a masking procedure, and predict the masked tokens, as in (Chen et al., 2018). Specifically, we use the BERT masking which replaces 80% of the selected words with _mask_ token, replaces 10% of those with a random word, and keeps the rest 10% of those unchanged. For example, given an input "Restaurant Take Out" where "Out" is the selected word, there are three masked inputs: "Restaurant Take [mask]", "Restaurant Take WEB", and "Restaurant Take Out". By applying softmax to the output embeddings at the positions of the masked tokens, the prediction results can be obtained. As in (Chen et al., 2018), we use the categorical cross-entropy loss \(L_{MLM}\).

For the second learning task, the ideal attribute embeddings should be distinct when these attributes belong to different types, due to the type-level heterogeneity. These embeddings should keep the diverse information of attributes belonging to the same type, as these attributes may describe different items from different aspects. Unlike BERT that outputs two sentences from two segments of input tokens, HeterBERT outputs \(l\) sentences to attribute embeddings, where \(l\) is the number of attributes in an item. Thus, Next Sentence Prediction (NSP) used by BERT, is improper for HeterBERT training. Other supervised learning tasks like type classification are also unsuitable since they could make attributes in an item excessively similar in feature space. We propose a C L task for training HeterBERT by using "positive" and "negative" data. Specifically, we first randomly select \(n_{p}\) pairs of attribute embeddings from the same items as "positive" pairs. Then, we derive \(n_{n}\) "negative" pairs by selecting two attribute embeddings from two items. We assume the distance of "positive" pairs is smaller than "negative" pairs. We formulate the loss function as follows:

\[L_{CL}=-ln(\sum_{p_{i}^{+}}^{n_{p}}\phi(e_{i}^{+},e_{k}^{-})-\sum_{p_{i}^{-}}^{ n_{p}}\phi(e_{j}^{+},e_{k}^{-})), \tag{3}\]

where \(\phi(\cdot)\) is cosine similarity, \(p_{i}^{+}\) is the \(i\)-th "positive" pair \((e_{j}^{+},e_{k}^{+})\), and \(p_{i}^{-}\) is the \(i\)-th "negative" pair \((e_{j}^{+},e_{k}^{-})\). With HeterBERT contrastive learning, we learn the attribute embeddings that are distinct at the type level, while keeping the diverse information of attributes.

#### 4.1.2. User Profile Construction

Each social user contains many types of data, like friendship and interaction records. We construct the profile based on her/his interacted items and friends on social platforms. Given a user \(u\), s/he should be interested in some attributes of the interacted items, and s/he should share some common interests with her/his friends. Thus, we first construct a historical attribute set \(A_{u}\) by collecting attributes from interacted items and form a neighbour attribute set \(A_{n}\) by combining historical attribute sets \(A_{u}\) of the user's friends. Then, we combine these sets as \(\overline{A}_{u}\) to reflect which attributes the user is interested in. The attribute embeddings of \(\overline{A}_{u}\) form the user profile as \(E_{u}\). Formally, a user profile is described as \(\delta\)-tuple \(U=c\)_uid_, \(l,N,A_{u},A_{n},E_{u}>\), where _uid_ is the user id; \(l\) is the history set, containing _uid_ of interacted items; \(N\) is the neighbour set, containing _uid_ of \(u\)'s friends; \(A_{u}\) is the attribute set of interacted items; \(A_{n}\) is the attribute set, combining \(A_{u}\) sets of \(u\)'s friends; \(E_{u}\) is the attribute embedding set, combining the corresponding attribute embeddings of \(A_{u}\cup A_{n}\).

### Motivation-aware Preference Prediction

We build a model that considers motivation and dynamically merges attribute embeddings of users and items.

#### 4.2.1. Activity Category Arrangement Algorithm

Given a activity set, we need to generate all the possible arrangements. However, in practice, some arrangements are not reasonable. For example, given an activity set \(\{Parking,Restaurant,Shop\}\), an arrangement \(<Restaurant,Parking,Shop>\) is unreasonable since people often park before doing other activities. Thus, we propose an Activity Category Arrangement (ACA) algorithm to select candidate category arrangements. The algorithm is detailed in Appendix A.2.

#### 4.2.2. M-Merge User Preference Prediction Model

To predict user preference, we need to represent users and items from attribute embeddings into uniform features. Existing methods (Yang et al., 2019) represent an item by merging its attributes with concatenation or weighted summing, which incurs redundancy from similar attributes and weaken the influence of key attributes. As Figure 3 shows, under different motivations, user preference could vary for the same type of items. We propose a motivation-based merge (M-Merge) layer,as shown in Figure 7, to dynamically generate user and item representations under the motivation behind an arrangement. Then, the user-item relevance is computed for candidate item list generation.

**Motivation-aware Item Representation.** Given an item, M-Merge generates an item feature by merging its attribute embeddings under the motivation behind a category arrangement. M-Merge needs to capture the influence of previous activities in item representation, due to the contextual correlation from activity series. Given a set of historical attribute embeddings \(H_{t-1}\), the current category \(c_{u_{t}}\), and attribute embeddings of the current item \(E_{u_{t}}\), M-Merge integrates attribute embeddings based on the current motivation and previous items. Specifically, given a \(c_{u_{t}}\), we sample \(N_{M}\) attribute embeddings from the corresponding attribute pool, as the motivation set \(M_{t}\in R^{N_{M}\times d}\), based on their occurrence frequency (\(N_{M}\) is set to 20 empirically3). We construct an attribute pool for each type by gathering attribute embeddings from items to that type. Given the historical attribute embedding set \(H_{t-1}\), the current motivation set \(M_{t}\), and the current item attribute embedding set \(E_{u_{t}}\), the M-Merge layer first measures how much an attribute meets another attribute from the current motivation/previous interaction by the dot product of two attribute embeddings. Extending to the whole input \(E_{u_{t}}\), we formulate two similarity matrices \(E_{u_{t}}M_{t}^{T}\) and \(E_{u_{t}}H_{t-1}^{T}\). Then, we apply \(RowSum\) on these matrices, followed by \(SoftMax\) and \(Transposition\), to generate weights \(W_{M}\)gR1\(\times N_{E}\) and \(W_{H}\)gR1\(\times N_{E}\). The integration is formulated by:

Footnote 3: [https://www.mitatnowang.com/article/48645](https://www.mitatnowang.com/article/48645)

\[e_{u_{t}}=Softmax(\omega_{0}W_{M}+(1-\omega_{0})W_{H})E_{u_{t}}, \tag{4}\]

where \(\omega_{0}\) is a trade-off parameter. We regard the integrated weights as corresponding probabilities for item attribute embeddings. Then, we repeatedly sample an attribute from \(E_{u_{t}}\) at the probability distribution and add it to \(\overline{H}_{t}\) until \(|\overline{H}_{t}|>p_{top}|E_{u_{t}}|\), where \(p_{top}\) is a parameter which controls the number of sampling. \(H_{t}\in R^{N_{M}\times d}\) is generated by combining the previous attributes \(H_{t-1}\in R^{m_{t}\times d}\) and current attributes \(\overline{H}_{t}\in R^{m_{t}\times d}\), where \(N_{H}=m_{1}+m_{2}\).

**Motivation-aware User Representation.** Similar to item representation, given a user, the model needs to capture the influence of previous activities in user representation and the current motivation. We use another M-Merge model to dynamically generate the user feature. Given a type \(c_{u_{t}}\), we first randomly select \(N_{M}\) attribute embeddings from the corresponding pool as the motivation set \(M_{t}\in R^{N_{M}\times d}\). Given a set of user attribute embeddings for previous activities \(G_{t-1}\), the current motivation \(M_{t}\), and a user profile \(E_{u_{t}}\), we then formulate two similarity matrices \(E_{u_{t}}M_{t}^{T}\) and \(E_{u_{t}}G_{t-1}^{T}\) and generate historical/current weights \(W_{M}\in R^{1\times N_{E}}\) and \(W_{G}\in R^{1\times N_{E}}\) by \(RowSum,SoftMax,\) and \(Transposition\) operations. The user feature is formulated by:

\[e_{u_{t}}=Softmax(\omega_{u}W_{M}+(1-\omega_{u})W_{G})E_{u_{t}}, \tag{5}\]

where \(\omega_{u}\) is a trade-off parameter. Like constructing current historical item attributes, we first sample attributes at the corresponding probabilities to form \(\overline{G}_{t}\in R^{n_{1}\times d}\), then combine it with previous historical user attributes \(G_{t-1}\in R^{n_{2}\times d}\) to construct current historical user attributes \(G_{t}\in R^{N_{G}\times d}\), where \(N_{G}=n_{1}+n_{2}\). The item and user representations are fed into the relevance prediction layer. Given a user feature \(e_{u_{t}}\) and an item feature \(e_{u_{t}}\), we define a user-item relevance \(r_{u_{t},p_{t}}=e_{u_{t}}e_{u_{t}^{T}}\). A larger \(r\) means a user is more likely attracted by an item. Following (Wang et al., 2019; Wang et al., 2019), we utilise AdamW as the optimiser and cross entropy as the loss function.

**Candidate Item List Generation.** Given a user \(u\) and her candidate category arrangement \(<c_{1},\cdots c_{q}>\), we compute user-item relevance scores by \(q\) steps. When \(t=1\), we first feed the user, items belonging to \(c_{1}\) and current type \(c_{1}\) into two M-Merge models. Especially, the historical attributes for an item and a user \(H_{0}\) and \(G_{0}\) are initialized as empty sets. After we achieve user and item features, we select the top-\(k_{2}\) items with high relevance scores into the candidate list \(R_{c_{1}}\) for the first activity. In addition, for each selected item, we obtain a pair of current historical attributes set \(\overline{H}_{1}\) and \(\overline{G}_{1}\). Accordingly, we derive \(H_{1}\) and \(G_{1}\) by combining the corresponding sets as the input at step \(t=2\). Extending the process to step \(t\), we generate a candidate item list \(R_{t}\). For the given arrangement, we generate a set of item lists \(R=\)\(R_{1},\cdots,R_{q}>\). The detailed algorithm ACA is shown in Appendix A.2. In practice, new items and session records could appear over platform. We dynamically and incrementally maintain the models of MASP to reflect the most recent social updates. We adopt fine-tune training strategy (Wang et al., 2019) to jointly update the HeterBERT and M-merge on the new data.

### Multi-constraints Session Generation

We propose a naive multi-constraint session generation (MCSG), and optimizations for fast candidate item generation and MCSG.

#### 4.3.1. Naive MCSG

With HeterBERT and motivation-aware preference prediction, we achieve candidate item lists for different categories under given arrangements. As Figure 2 (c) shows, for each candidate arrangement, we first regard its item lists as a \(q\)-partite graph to \(q\) independent sets. The task of generating a session can be converted into finding a path through each set sequentially. Then, our goal is to generate sessions that align with users' interests, meet their requirements for activity arrangement, and avoid breaching constraints. We evaluate each path by the user preference, correlation of adjacent items, and constraints. Given a user \(u\) and a path \(p_{u}=c_{1},\cdots,p_{q}>\), the user preference is described by the relevance score \(r_{u,u}\), the correlation is computed by \(e_{u}^{T}e_{u_{t}}\), and constraints filters improper paths. The session score is computed as follows:

\[f_{score}(p_{u})=\sum_{i=1}^{q}(r_{u,u_{t}})+\sum_{j=1}^{q-1}(e_{u_{t}}^{T}e_{u _{j+1}}). \tag{6}\]

The MCSG includes path generation and evaluation. For each item list \(\mathbf{R}^{(i)}\), we generate possible paths based on its distance and time constraints. We compute the spherical distance of adjacent items, check the distance constraint (\(rad=1\) km) and the available time of these items, as in (Chen et al., 2019; Wang et al., 2019). We evaluate each path by Eq. 6 and select top-\(K\) paths for the \(i^{th}\) candidate arrangement.

#### 4.3.2. Optimisation

We propose attribute-based candidate items generation to accelerate the item list generation and a greedy algorithm to accelerate the session generation.

Figure 7. M-Merge model.

[MISSING_PAGE_FAIL:7]

### Efficiency Evaluation

#### 5.3.1. Effect of Optimisation

We first test the effect of the proposed optimisation and report the response time in Table 3. Clearly, the combination of two optimisations achieves the best efficiency, followed by ILG optimisation. This is because ILG could quickly filter out the items that do not share attributes with a target user. On the other hand, MCSG optimisation improves efficiency slightly. This is because Greedy-MCSG balances the information loss and efficiency, leading to a slight improvement in efficiency. MASP incurs the highest time cost, which proves the importance of optimisation.

#### 5.3.2. Efficiency Comparison

We compare our MASP and MASP-OPT with SOTA methods, GNAAutoScale and ToCoSeRec, in terms of overall time cost. The the experimental results over three datasets are reported in Table 4. Clearly, our MASP-OPT is much faster than MASP, GNAAutoScale and ToCoSeRec. This is because the proposed ILG algorithm significantly reduces the candidate items for computing the relevance scores and Greedy-MCSG optimisation further accelerates the session generation. GNAAutoScale is slightly faster than MASP and ToCoSeRec because it generates sessions based only on item types, whereas MASP and ToCoSeRec consider both item types and correlations between items.

#### 5.3.3. Time Cost of Model Update

We take the model update in two ways: fully re-training the learned model over new and old data and fine-tuning the trained model over new data. We split datasets into training, validation, new data, and test sets, as in Sec 5.2.2. We use 100% of new data to fine-tune the models of MASP for Yelp, Yelp-L, and Douban. For fine-tuning-based method, the time costs for the model update are 12.2 minutes, 50.7 minutes, and 31.4 minutes, respectively. For the re-training, it requires 16 hours, 65 hours, and 39 hours. Compared to re-training, the incremental strategy improves model update efficiency by up to about 75 times. Thus the time cost of model updates in MASP is well controlled.

### Ablation Study

We conduct the ablation study to evaluate the impact of three main components of our MASP model: input embedding layer, C-Merge, double threshold mechanism, and M-Merge. Figures 9 -10 depict our ablation analysis's experimental results over three datasets.

#### 5.4.1. Input Embedding Layer

We compare the HR@K results of MASP for two settings: the full version of the proposed input embedding (Full) and the naive input embedding of the original BERT (Naive embedding). Figure 9 displays the effectiveness comparison results. With the proposed input embedding, MASP outperforms the model with naive input embedding across all K values, which indicates that our input embedding can handle the heterogeneity at attribute and type levels, thus enhancing the effectiveness of MASP.

#### 5.4.2. C-Merge Layer

We compare the HR@K results of MASP obtained under two settings: full version (Full) and None C-Merge layers (None-Merge). As shown in Figure 9, the model's performance significantly declines without the C-Merge layer. This proves that C-Merge has strong capability to effectively capture the correlation between types and items.

#### 5.4.3. Double threshold mechanism

We evaluate the effect of the double threshold mechanism by conducting tests in two settings: full version (Full) and None-Threshold. As shown in Figure 9, the model's performance slightly declines without the double threshold mechanism, indicating that the noise in the word-level and attribute-level weights are limited and can be filtered out by our mechanism.

#### 5.4.4. M-Merge

We evaluate the effect of M-Merge in motivation-aware preference prediction by conducting tests in three settings: full version of MASP (Full), only merging items' attributes (V-Merge), and only merging users' attributes (U-Merge). As shown in Figure 10, the full version of MASP outperforms other variants across all K values for all datasets. This is because M-Merge combines current and historical attributes for both users and items, enhancing the representativeness of the user and item embeddings. In addition, U-Merge consistently outperforms V-Merge, which indicates that M-Merge has a greater impact on user representations.

## 6. Conclusion

This paper proposes a new SPHP problem and a novel framework MASP for SPHP. We first propose a novel HeterBERT for heterogeneous item presentation. Then we propose a motivation-aware model for user preference prediction. Finally, we propose an MCSG algorithm together with two optimisation strategies for efficient session generation. Extensive tests over three real datasets prove that MASP outperforms the SOTA methods in terms of efficacy.

\begin{table}
\begin{tabular}{|l|c|c|c|c|} \hline Time (s) & MASP & MASP-ILG & MASP-MCSG & MASP-OPT \\ \hline Yelp & 21.2 & 4.5 & 19.5 & **3.9** \\ \hline Yelp-L & 141.8 & 45.2 & 120.5 & **33.4** \\ \hline Douban & 37.5 & 9.2 & 35.2 & **5.8** \\ \hline \end{tabular}
\end{table}
Table 3. Optimisation comparison.

Figure 10. Ablation study on M-Merge.

Figure 9. Ablation study on input embedding, C-Merge, thresholds.

## References

* (1)
* Bao et al. (2021) Lei Bi, Juan Cao, Guohui Li, Nguyen Quoc Viet Hung, Christian S. Jensen, and Bolong Zheng. 2021. Speelaw: A Voice-based Navigation System via Route Description Language Understanding. In _ICDE 2020-2692_.
* Chen et al. (2021) Di Chen, Ye Yuan, Wenjin Du, Yurong Cheng, and Guoren Wang. 2021. Online Route Planning over Time-Dependent Road Networks. In _ICDE_. 325-335.
* Chen et al. (2023) Qian Chen, Zhiqiang Guo, Jianjun Li, and Guohui Li. 2023. Knowledge-enhanced Multi-View Graph Neural Networks for Session-based Recommendation. In _SIGIR_. 352-361.
* Cheng et al. (2020) Yuorong Cheng, Doying Li, Xiangnan Zhou, Ye Yuan, Guoren Wang, and Lei Chen. 2020. Real-Time Cross Online Matching in Spatial Crowdsourcing. In _ICDE_. 1-12.
* Das et al. (2015) Jian Das, Bin Yang, Chenjuan Guo, and Zhaining Ding. 2015. Personalized route recommendation using big trajectory data. In _ICDE_. 543-554.
* Zhang et al. (2020) Yizhou Zhang, Hang Hangqing Guo, Linying Jiang, Xingwei Wang, Xiaoxiao Xu, Qinghui Sun, and Ling Li. 2020. Microsoft-Augmenting Data to Uniform Sequences by Time Intervals for Effective Recommendation. _IEEE Trans. Knowl. Data Eng._ 36, 6 (2020), 2686-2700.
* Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Berton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In _NAACL-HLT_. 4171-4186.
* Feng et al. (2015) Shanthan Feng, Quota Li, Ying Zeng, Cao Cong, Yeow Meng, and Qian Yuan. 2015. Personalized Ranking Metric Embedding for Next New PO Recommendations. In _IJCAI_. 2069-2075.
* Feng et al. (2022) Matthias Feng, Jian L Esmaek, Prasit Weichert, and Jure Leskovec. 2022. Granu-Totas: Scalable and aggressive graph neural networks via historical embeddings. In _ICML_. 3294-3304.
* Guo et al. (2019) Lei Gu, Hongbin Yin, Qiuyong Wang, Tong Chen, Alexander Zhou, and Nguyen Quoc Viet Hung. 2019. Streaming Session-based Recommendation. In _SIGKDD_. 1596-1597.
* He et al. (2018) Jia He, Rui Liu, Fuhen Zhuang, Fen Lin, Cheng Niu, and Qing He. 2018. A General Cross-Domain Recommendation Framework via Bayesian Neural Network. In _ICDM_. 1001-1006.
* Hong et al. (2020) Huiling Hong, Hantao Guo, Yucheng Lin, Xiaoging Yang, Zang Li, and Jeping Ye. 2020. An Attention-Based Graph Neural Network for Heterogeneous Structural Learning. In _AAAI_. 4123-4139.
* Hong et al. (2021) Liang Hu, Longbing Cao, Shoubin Wang, Guandong Xu, Jlan Cao, and Zhiping Gao. 2021. Diversifying Personal Recommendation with User-session Context. In _IJCAI_. 1858-1864.
* Jagat et al. (2023) Akshay Jagat, Nikit Gupta, Sachin Farfade, and Prakash Mandayam Conar. 2023. AttributeT: Session-based Product Attribute Recommendation with BERT. In _SIGIR_. 3214-325.
* James and Ludwig (2017) Dietmar James and Mike Ludwig. 2017. When Recurrent Neural Networks meet the Neighborhood for Session-Based Recommendation. In _RecSys_. 306-310.
* Kershpernger et al. (2022) Barik Kershpernger, Olivier Spangers, and Sebastian Schiele. 2022. Serende-Low-Latency Session-Based Recommendation in a Commerce at Scale. In _SIGMOD_. 158-159.
* Lai et al. (2022) Shi Lai, Heli Meng, Fan Zhang, Chenliang Li, Bin Wang, and Aixin Sun. 2022. An Attribute-Driven Mirror Graph Network for Session-based Recommendation. In _SIGIR_. 1674-1683.
* Li et al. (2015) Guoliang Li, Han He, Dong Deng, and Jian Li. 2015. Efficient similarity join and search on multi-attribute data. In _SIGMOD_. 1137-1151.
* Li and Tuzhilin (2020) Pan Li and Alexander Tuzhilin. 2020. DirectDB: Deep Dual Transfer Cross Domain Recommendation. In _WSDM_. 20: 331-339.
* Liang et al. (2016) Dawen Liang, Jan Albosa, Laurent Charlin, and David M. Blei. 2016. Factorization Meets in the Lambda: Beginning Starfix Factorization with Item Co-occurrence. In _RecSys_. 95-96.
* Lin et al. (2020) Hao Lin, Jingdong Tian, Yanizi Fu, Jingbo Zhou, Xinjiang Lu, and Hui Xiong. 2020. Multi-Modal Transformation Recommendation with Unified Route Representations Unit Learning. _Proc. VLDB Endow_. 14, 3 (2020), 342-345.
* Lin et al. (2022) Hao Lin, Yongjun Tong, Jiahong Han, Panpan Zhang, Xinjiang Lu, and Hui Xiong. 2022. Incorporating Multi-Source Urban Data for Personalized and Context-Aware Multi-Modal Transportation Recommendation. _IEEE Trans. Knowl. Data Eng._ 34, 2 (2022), 725-735.
* Long et al. (2023) Jing Long, Tong Chen, Quoc Viet Hung Nguyen, Guandong Xu, Kai Zheng, and Yonghui Feng. 2023. Model-Agnostic Decentralized Collaborative Learning for On-Device OI Recommendation. In _SIGIR_. 423-432.
* Nikolov et al. (2012) Sepulok Nikolov, Paras Sukhatur, Salimyaraman Somanov, Senhui Basu Roy, Adam Bienkowski, Matthew Maeschke, Kristina R. Pattigat, and David Sidedi. 2012. Cooperative Route Planning Framework for Multiple Distributed Access in Maritime Applications. In _SIGMOD_. 1518-1527.
* Christensen Petersen et al. (2019) Niklas Christoffer Petersen, Filipc Rodrigues, and Francisco Cimara Pereira. 2019. Multi-output two latent time prediction with convolutional LSTM neural network. _Expert Syst. Appl._ 120 (2019), 426-435.
* Sathleid and Nandi (2019) Ritesh Sathleid and Armah Nandi. 2019. Visual Segmentation for Information Extraction from Heterogeneous Visually Rich Documents. In _SIGMOD_. 247-262.
* Seel et al. (2022) Jinseok Jamie Seel, Youngko Ko, and Sang-goo Lee. 2022. Exploiting Session Information in BERT-based Session-aware Sequential Recommendation. In _SIGIR_. 2639-2644.
* Shan et al. (2016) Ying Shan, T Ryan Hoens, Jian Jiao, Huijing Wang, Dong Yu, and JC Mao. 2016. Deep crossing: Web-scale modeling without manually crafted combinatorial features. In _SIGMOD_. 255-262.
* Song et al. (2016) Yang Song, Ali Mamdouh Elishay, and Xiaodong He. 2016. Multi-rate deep learning for temporal recommendation. In _SIGIR_. 509-912.
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In _NeurIPS_. 5096-6018.
* Wang et al. (2022) Jingyuan Wang, Ning Wu, and Wayne Xin Zhao. 2022. Personalized Route Recommendation With Neural Network Enhanced Search Algorithm. _IEEE Trans. Knowl. Data Eng._ 34, 12 (2022), 5910-5924.
* Wang et al. (2020) Zijia Wang, Huqan Li, Chen Jin, Zhiqiang Bao, Guoliang Li, and Tianqing Wang. 2020. Leveraging Dynamic and Heterogeneous Worldload Knowledge to Boost the Performance of Index Advisors. _WCLDB Endow._ 17, 7 (2020), 1642-1654.
* Wang and Shen (2023) Zikui Wang and Yangyan Shen. 2023. Incremental Learning for Multi-Interest Sequential Recommendation. In _ICDE_. 1071-1083.
* Wu et al. (2019) Shu Wu, Yuyuan Tang, Yanqiu Zhu, Liang Wang, Xing Xie, and Tienia Tan. 2019. Session-Based Recommendation with Graph Neural Networks. In _AAAI_. 346-353.
* Xu et al. (2020) Mirui Xu, Jajie Xu, Rui Zhou, Jianxin Li, Kai Zheng, Pengpeng Zhao, and Chengfei Li. 2020. Empowering A' Algorithm With Neuralized Variational Heuristics for Fastnet Recommendation. _IEEE Trans. Knowl. Data Eng._ 35, 10 (2020), 10011-10023.
* Yan and Howe (2021) An Yan and Bill Howe. 2021. EquTrans: Learning Fair Integrations of Heterogeneous Urban Data. In _SIGMOD_. ACM, 2338-2347.
* Yin et al. (2016) Hongqi Yan, Zhiting Hu, Xiaofang Zhou, Hao Wang, Kai Zheng, Nguyen Quoc Viet Hung, and Shaixia Wosha Sadig. 2016. Discovering interpretable geo-social communities for user behavior prediction. In _ICDE_. 942-953.
* Yu et al. (2021) Junliang Yu, Hongqi Yan, Jundong Li, Qiangying Wang, Jseong Quoc Viet Hung, and Xiaofang Zhang. 2021. Self-Supervised Multi-Channel Hypergraph Convolutional Network for Social Recommendation. In _WWW_. 407. ACM, 1032-1242.
* Yuan et al. (2020) Fang Yuan, Xiangmin He, Heckandun Zhang, Guibang Guo, Jian Xiong, Zhenhao Xu, and Yilin Xiong. 2020. Future Data Helps Training: Modeling Future Contests for Session-based Recommendation. In _WWW_. 305-313.
* Yuan et al. (2022) Haitao Yuan, Guoliang Li, and Zhifeng Zheng. 2022. Route Travel Time Estimation on A Road Network Reviate Heterogeneousity, Proximity, Periodicity and Proximity. _WCLDB Endow._ 16, 3 (2022), 395-405.
* Yuan et al. (2022) Jiahao Yuan, Wendi J. Dell Zhang, Jiawei Pan, and Xiaolong Wang. 2022. Micro-Behavior Encoding for Session-based Recommendation. In _ICDE_. 2886-2899.
* Tong et al. (2018) Yuxiang Zeng, Yongxin Tong, Yuqiang Song, and Lien Chen. 2018. The Simpler The Better: An Indexing Approach for Shared-Route Planning Queries. _Proc. VLDB Endow._ 13, 13 (2020), 3517-3530.
* Zhou et al. (2020) Xiangmin Zhou, Rosin Lumbantoram, Yongli Ren, Lei Chen, Xiaochun Yang, and Be Shao. 2020. Dynamic Bi-layer Graph Learning for Context-aware Sequential Recommendation. _Trans. Recoms. Syst._ 2, 2 (2020), 141-1423.

[MISSING_PAGE_FAIL:10]

primary time cost is \(O(m^{\prime}*q*k_{1})\), where \(m^{\prime}\ll m\). With ILG, we can significantly reduce the computation cost. In addition, this process can be performed offline, leading to less time cost in online processing.

**Input:**\(A_{ii}\): attribute set from the user interaction history.

\(A_{ci}\): set of attributes belonging to type \(c_{i}\).

**Output:**\(S_{i}\): candidate items.

\(1.Au_{ci}\gets A_{ii}\wedge A_{ci}\);

\(2.\)\(AM\leftarrow\) Sample from \(A_{ci}\);

\(3.\)\(Au_{ci}\gets A_{ac}\wedge A_{i}\);

\(4.\)for\(\forall\)\(\forall_{0}\in V_{ci}\);

\(5.\)if\(A_{ii}\cap A_{ac}\neq\emptyset\):\(S_{i}\gets u_{i}\);

\(6.\)Return\(S_{i}\).

### Greedy-MCSG

Given a split line at the \(r\)-th row, paths generated in the top part have higher approximate scores since each column of \(R^{(t)}\) is sorted in descending order. We remove the items with smaller relevance scores from \(R^{(t)}\), reshaping the matrix from \(R^{k*q}\) to \(R^{N_{r}*q}\), where \(N_{r}\) is the number of remaining items in \(R^{(t)}\). Then, we apply MCSG on the remaining item set \(\overline{\mathcal{R}}\) to fast generate candidate sessions. We analyze the Yelp dataset by computing the ratio (\(Rat_{fa}\)) of falsely removed candidates to the final sessions to be returned. As reported in Table 5, at \(N_{r}=1.25K\) where \(K\) is the number of returned sessions in the final result, the \(Rat_{fa}\) value is close to 0. Thus, we set \(N_{r}\) to 1.25K for a good trade-off between the information loss and efficiency of MCSG.

### Detailed Experiment Setup

**Datasets Details.** Details of these datasets are shown in Table 6.

**Evaluation Metrics.** The effectiveness of models is evaluated in terms of Hit Ratio (HR@K) and Spearman's rank correlation. HR@K is computed by: HR@K \(=\frac{\#HT\cap RK}{\#R}\), where \(\#HT\)@K is the number of relevant sessions in top-\(K\) returned ones and \(\#\)R is the number of all relevant sessions. In SPH, we use Spearman's rank correlation to measure the "distance" between the generated category arrangement and the ground-truth category arrangement, which is computed as: \(\rho=1-\frac{\epsilon}{n(n^{2}-1)}\), where \(n\) is the length of arrangement and \(d_{i}\) is the difference between the two ranks of each category. We evaluate the efficiency of MASP by the response time of the session generation.

**Implementation Details.** All tests are conducted on an Intel i5, 2.30GHz processor machine with 16 GB RAM and 4 GB NVIDIA GTX 1050Ti graphics card. The source code and datasets are available 6.

Footnote 6: [https://anonymous.atopen.science/r/CODING-87D2/](https://anonymous.atopen.science/r/CODING-87D2/)

### Effect of Dataset Size

We consider the effect of dataset size on the session planning efficiency. Specifically, we divide each dataset into 5 folds and conduct session planning on them separately. As shown in Figure 13, MASP-OPT is much faster than other methods at different updated data sizes. In addition, as the updated data size increases, the time cost of MASP-OPT increases smoothly while those of the other three methods across proportionally with the updated data size. Clearly, the time cost increase speed of MASP-OPT is much lower than those of the other models with the dataset size increase. Thus MASP-OPT is most scalable in terms of updated data size.

### Parameter Tuning

#### a.8.1. Effect of \(\mathbf{v_{T}}\)

We test the optimal trade-off parameter \(\mathbf{v_{T}}\) in Eq. 2 by conducting session planning over three datasets with varying \(\mathbf{v_{T}}\) from 0 to 1. As shown in Figure 14 (a), as \(\mathbf{v_{T}}\) increases, the HR@20 result of our MASP increases first and declines after an optimal \(\mathbf{v_{T}}\) value for all datasets. This proves both word-level and attribute-level correlations affect the effectiveness of MASP. An optimal trade-off is achieved at \(\mathbf{v_{T}}\)=0.6. Thus, we set the default \(\mathbf{v_{T}}\) to 0.6.

#### a.8.2. Effect of Word-level Noise Filtering Threshold \(th_{11}\) in HeterBert

We evaluate the effect of word-level noise filtering threshold \(th_{11}\) in HeterBert to the effectiveness of MASP. We test MASP at different \(th_{11}\) values over three datasets in terms of HR@20. In the test, the \(th_{11}\) value varies from 0 to 1. Figure 14 (b) reports the HR@20 values at different \(th_{11}\). Clearly, the MASP model's performance increases first, achieving the best HR@20 when \(th_{11}=0.1\) for all datasets. This is because more word-level noises are filtered out, thus the word-level correction among cleaned words can be better captured in the HeterBert model, leading to higher effectiveness of MASP. With the further increase of \(th_{11}\), the HR@20 result declines. This is because a large \(th_{11}\) threshold incorrectly filters out some word-level word correlation, which causes the information loss. Thus, we set the default \(th_{11}\) to 0.1.

\begin{table}
\begin{tabular}{|c|c|c|c|c|} \hline Statistics & \# Users & \# Items & \# Interaction \\ \hline Yelp & 32,124 & 23,013 & 24,103 \\ \hline Yelp-L & 162,721 & 150,345 & 219,545 \\ \hline Douban & 12,040 & 34,883 & 86,350 \\ \hline \end{tabular}
\end{table}
Table 6. Statistics of datasets.

Figure 12. ILG algorithm.

Figure 13. Effect of updated data size.

#### a.8.3. Effect of Attribute-level Noise Filtering Threshold \(th_{2}\) in HeterBert

We test the effect of \(th_{2}\) on the effectiveness of MASP by reporting the HR@20 of MASP at different \(th_{2}\) ranging from 0 to 1. As shown in Figure 14 (c), the effectiveness of the MASP model increases with \(th_{2}\) increasing from 0 to 0.1, and then drops after the optimal value \(th_{2}=0.1\) for all datasets. This is because applying the double filtering mechanism well filters out the attribute-level noises, which enables the HeterBert model to well capture the word correlations between different cleaned attributes. Meanwhile, extremely filtering after the optimal \(th_{2}\) could filter out some normal attributes, which removes some attribute-level word correlations. Thus, we set the default \(th_{2}\) to 0.1 to balance the quality of attributes and the information loss in HeterBert.

#### a.8.4. Effect of \(w_{w}\)

We evaluate the effect of the trade-off parameter \(w_{w}\) in Eq. 4 on the effectiveness of MASP by varying it from 0 to 1 across all datasets. Figure 14 (d) shows that the HR@20 result increases with \(w_{w}\) and reaches the optimal values at \(w_{w}\) =0.8 for all datasets. The HR@20 result drops with the further increase of \(w_{w}\) after \(w_{w}\) =0.8. This proves that both current attributes and historical attributes of items affect the effectiveness of MASP. We set the default \(w_{w}\) to 0.8.

#### a.8.5. Effect of \(w_{w}\)

We evaluate the effect of the trade-off parameter \(w_{w}\) in Eq. 5 by varying \(w_{w}\) from 0 to 1. The test results are reported in Figure 14 (e). Clearly, the HR@20 result rises with the \(w_{w}\) increasing, reaches the best values at 0.7 and drops after the best values. This proves that both current attributes and historical attributes of users affect the effectiveness of MASP. Thus, we set the default \(w_{w}\) to 0.7.

#### a.8.6. Effect of Sample Proportion \(p_{rop}\)

We evaluate the effect of the sample proportion \(p_{rop}\) by searching it from 0 to 1. As shown in Figure 14 (f), as the increase of \(p_{rop}\), the HR@20 values increase, reach their peaks at 0.6 for Yelp and Yelp-L, and 0.5 for Douban, and drop with the further increase of \(p_{rop}\). This indicates that selectively maintaining the current motivations can better represent items, improving the model's performance. Thus, we set the default \(p_{rop}\) to 0.6 for Yelp and Yelp-L, and 0.5 for Douban.

### Case Study

We conduct a case study to demonstrate the advantages of MASP. We randomly select one user from the Yelp dataset. The user visited a coffee shop (Starbucks), a spa (I touch Day Spa), and a hotel (La Quinta Hotel) in order as the ground truth. Correspondingly, our MASP generates a session plan \(\subset\)Starbucks, I touch Day Spa, Days Inn & Suites- which is represented in blue. MASP accurately predicts two items and provides one hotel that is quite similar in properties, such as location and star rating (in blue arrow). In Table 7, we further report the top-1 arrangement by MASP and the SOTAs on the Yelp data. The user demands a plan for activities (hotel, spa, coffee roasters). First, our ACA model accurately arranges the order of activities. Then, MASP predicts two items correctly while the predictions of GNNAutoScale and TiCoSeRec are completely wrong. This is because GNNAutoScale never considers the inter-item correlation while TiCoSeRec generates a session based on the user's historical records, which is not suitable for handling new trips irrelevant to the user history. Note that "Starbucks" and "I Touch Day Spa" seem to be irrelative but share the same attribute "DogsAllowed", which indicates that MASP can well capture the correlation of heterogeneous items.

\begin{table}
\begin{tabular}{|c|l|l|} \hline Method & Plan & 1380 \\ \hline Ground-truth & Starbucks, I Touch Day Spa, La Quinta Hotel & 1371 \\ \hline MASP & **Starbucks.I Touch Day Spa**, Days Inn \& Suites & 1372 \\ \hline GNNAutoScale & McDonalds, Elegant Nail, My Place Hotel & 1373 \\ \hline TiCoSeRec & Dutch Bros Coffee, Tipsy Nails, Holiday Inn Express \& Suites & 1374 \\ \hline \end{tabular}
\end{table}
Table 7. Case study: ground-truth and generated plans.

Figure 14. Parameters vs. the effectiveness of MASP.