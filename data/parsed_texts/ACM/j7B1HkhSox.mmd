P4GCN: Vertical Federated Social Recommendation with Privacy-Preserving Two-Party Graph Convolution Networks

Anonymous Author(s)

###### Abstract.

In recent years, graph neural networks (GNNs) have been commonly utilized for social recommendation systems. However, real-world scenarios often present challenges related to user privacy and business constraints, inhibiting direct access to valuable social information from other platforms. While many existing methods have tackled matrix factorization-based social recommendations without direct social data access, developing GNN-based federated social recommendation models under similar conditions remains largely unexplored. To address this issue, we propose a novel vertical federated social recommendation method leveraging privacy-preserving two-party graph convolution networks (P4GCN) to enhance recommendation accuracy without requiring direct access to sensitive social information. First, we introduce a Sandwich-Encryption module to ensure comprehensive data privacy during the collaborative computing process. Second, we provide a thorough theoretical analysis of the privacy guarantees, considering the participation of both curious and honest parties. Extensive experiments on four real-world datasets demonstrate that P4GCN outperforms state-of-the-art methods in terms of recommendation accuracy.

Social Recommendation, Federated Learning, Graph Neuron Network +
Footnote †: ccs: Information and Communication Networks

+
Footnote †: ccs: Information and Communication Networks

+
Footnote †: ccs: Information and Communication Networks

+
Footnote †: ccs: Information and Communication Networks

+
Footnote †: ccs: Information and Communication Networks

+
Footnote †: ccs: Information and Communication Networks

+
Footnote †: ccs: Information and Communication Networks

+
Footnote †: ccs: Information and Communication Networks

+
Footnote †: ccs: Information and Communication Networks

+
Footnote †: ccs: Information and Communication Networks

+
Footnote †: ccs: Information and Communication Networks

+
Footnote †: ccs: Information and Communication Networks

+
Footnote †: ccs: Information and Communication Networks

+
Footnote †: ccs: Information and Communication Networks

## 1. Introduction

Graph neural networks (GNNs) (Ganin and Lempitsky, 2015; Ganin and Lempitsky, 2015) are a class of deep learning models specifically designed to handle graph-structured data, including various scenarios such as social networks (Ganin and Lempitsky, 2015; Ganin and Lempitsky, 2015), finance and insurance technology (Ganin and Lempitsky, 2015; Ganin and Lempitsky, 2015), etc. By harnessing the capabilities of GNNs, social recommendation systems can gain an in-depth understanding of the intricate dynamics and social influence factors that shape users' preferences, leading to improved recommendation accuracy. For example, an insurance company could utilize social relationships extracted from a social network platform by a GNN model to enhance the accuracy of personalized product recommendations (i.e., insurance marketing). However, in real-world scenarios, privacy and business concerns often hinder direct access to private information possessed by aforementioned social platforms. Consequently, the integration of privacy-preserving technologies, such as federated learning (Ganin and Lempitsky, 2015), secure multi-party computation (Ganin and Lempitsky, 2015), homomorphic encryption (Ganin and Lempitsky, 2015), and differential privacy (Ganin and Lempitsky, 2015), into social recommendation tasks has attracted significant attention from both academia and industries.

Recent works mainly enable the recommender to collaboratively train matrix factorization (Krizhevsky et al., 2012) based recommendation models without accessing the social data owned by other platforms(Krizhevsky et al., 2012; Hinton et al., 2012). (Krizhevsky et al., 2012) proposed the secure social MF to utilize the social data as the regularization term when optimizing the model. Further, (Krizhevsky et al., 2012) significantly reduces both the computation and communication costs of the secure social matrix factorization by designing a new secure multi-party computation protocol. However, these solutions cannot be applied to training GNN models, because the computation processes involved in training GNN models are typically more complex compared to MF-based methods. For example, in GNN models, the aggregation of features from different users on the social graph involves multiplying the aggregated results with additional parameter matrices. In contrast, MF-based methods focus on reducing the distances between neighbors' embeddings based on the social data, without the need for additional parameters. In addition, the formulations used in the forward and backward processes of GNN models are much more complex than those of MF-based methods. Consequently, it is essential to develop a secure social recommendation protocol tailored explicitly to enhance the optimization of GNN models.

Figure 1. The example of vertical federated social recommendation with inaccessible social data.

To address the aforementioned challenges, we propose a novel _vertical federated Social recommendation with Privacy-Preserving Party-to-Party Graph Convolution Networks_ (**P4GCN**) to improve the social recommendation system without direct access to the social data. In our approach, we first introduce the _Sandwich-Encryption_ module, which ensures data privacy throughout the collaborative computing process. We then provide a theoretical analysis of the security guarantees under the assumption that all participating parties are curious and honest. Finally, extensive experiments are conducted on three real-world datasets, and results demonstrate that our proposed P4GCN outperforms state-of-the-art methods in terms of both recommendation accuracy and communication efficiency.

The main contributions of this study can be summarized as follows:

* We propose P4GCN, a novel method for implementing vertical federated social recommendation with theoretical guarantees. Unlike previous works that assume the availability of social data, we focus on leveraging GNN to enhance recommendation systems with fully unavailable social data in a privacy-preserving manner.
* We introduce the sandwich encryption module, which guarantees data privacy during model training by employing a combination of homomorphic encryption and differential privacy. We provide theoretical guarantees to support its effectiveness.
* Experimental results conducted on four real-world datasets illustrate the enhancements in performance and efficiency. Furthermore, we evaluate the impact of the privacy budget on the utility of the model.

## 2. Related Works

### Social recommendation

Existing social recommendation methods have adopted various architectures according to their goals and achieved outstanding results (Kumar et al., 2017). For instance, many SocialRS methods employ the graph attention neural network (GANN) (Bahdanau et al., 2014) to differentiate each user's preference for items or each user's influence on their social friends. Some other methods (Kumar et al., 2017; Li et al., 2017; Li et al., 2017; Li et al., 2017; Li et al., 2017) use the graph recurrent neural networks (GRNN) (Shen et al., 2016; Li et al., 2017) to model the sequential behaviors of users. However, these centralized methods cannot be directly applied when the social data is inaccessible.

### Federated recommendation

There are mainly two types of works addressing recommendation systems in FL. The first type is User-level horizontal FL. FedMF (Li et al., 2017) safely train a matrix factorization model for horizontal users. FedGNN (Li et al., 2017) captures high-order user-item interactions. FedSoG (Li et al., 2017) leverages social information to further improve model performance. The second type is Enterprise-level vertical FL which considers training a model with separated records kept by different companies. To promise data security in this case, techniques such as differential privacy(Li et al., 2017) and homomorphic encryption(Li et al., 2017), are widely used. (Li et al., 2017) uses random projection and ternary quantization mechanisms to achieve outstanding results in privacy-preserving. However, these works failed to construct the social recommendation model when the social data is unavailable. To address this issue, SeSore(Li et al., 2017) protects social information while utilizing the social data to regularize the model. (Kumar et al., 2017) proposed two secure computation protocols to further improve the training efficiency. Although these works can be applied to matrix factorization models, the GNN-based models have not been considered in this case.

## 3. Problem Formulation

In this section, we first introduce the notations we used, and then we give the formal definition of our problem. Let \(U=\{u_{i}\},u_{i}\in\mathbb{N}\) denote the user set and \(V=\{u_{i}\},u_{i}\in\mathbb{N}\) denote the item set, where the number of users is \(N_{U}=|U|\) users and the number of items is \(N_{V}=|V|\). There are two companies \(\mathcal{P}_{1},\mathcal{P}_{2}\) that own different parts of the user and item data. \(\mathcal{P}_{1}\) owns the user set \(U\) and the item set \(V\) with the interactions between users and items \(\mathcal{R}=\{(u_{i},u_{j},y_{k})\}\), where each \(r_{k}\in\mathbb{R}\) is a scalar that describes the \(k\)th interaction in \(\mathcal{R}\). \(\mathcal{P}_{2}\) owns the same user set \(U\) and their social data (i.e. user-user interactions) \(\mathcal{S}=\{(u_{i},u_{j},y_{k})\}\), where \(s_{k}\in\mathbb{R}\) denotes the \(k\)th interaction in \(\mathcal{S}\).

\(\mathcal{P}_{1}\) and \(\mathcal{P}_{2}\) collaboratively train a social recommendation GNN-based model \(\mathbf{f}_{\mathbf{\theta}}\) that predicts the rating \(\hat{r}_{u_{i}u_{j}}\equiv f(U,V,\mathcal{R}_{train},\mathcal{S})\) of the user \(u\) assigning to the item \(g_{j}\). We minimize the mean square errors (i.e. MSE) (Li et al., 2017) between the predictions and the targets to optimize the model parameters \(\mathbf{\theta}\):

\[\min_{\mathbf{\theta}}\mathcal{L}(\mathbf{\theta};U,V,\mathcal{R}_{train},\mathbf{S})= \frac{1}{|\mathcal{R}_{train}|}\sum_{(u_{i},g_{j},f_{\mathbf{\theta}})\in\mathcal{ R}_{train}}\|r_{k}-\hat{r}_{u_{i}g_{j}}\|^{2}\]

Since all the computation can be done by \(\mathcal{P}_{1}\) itself except for the GNN layers for the social aggregation, we focus on protecting data privacy when computing the results of the social aggregation layer. Particularly, we consider the most classical GNN operator, Graph Convolution (GC), as the social aggregation operator in our model. Given a social-aggregation GC operator \(G(\mathbf{X},\mathbf{A},\mathbf{\theta}_{GC})\), \(\mathcal{P}_{1}\) should realize message passing mechanism of user features \(\mathbf{X}\in\mathbb{R}^{N\times d}\) over the users' social graph \(\mathbf{A}\in\{a_{ij}\}_{N\times N},a_{ij}\in\{0,1\}\) (i.e. the adjacent matrix) as below:

**Forward.**

\[\tilde{\mathbf{L}}_{sym}=\mathbf{D}^{-\frac{1}{2}}(\mathbf{A}+\mathbf{I}) \mathbf{D}^{-\frac{1}{2}},\mathbf{D}=\text{diag}([1+\sum_{j}a_{1j},...,1+\sum _{j}a_{Nj}]) \tag{1}\]

\[\mathbf{Z}=\sigma(\mathbf{Y}+\mathbf{1}\mathbf{b}^{\top}),\mathbf{Y}=\tilde{ \mathbf{L}}_{sym}\mathbf{X}\mathbf{W} \tag{2}\]

**Backward.**

\[\frac{\partial\mathcal{L}}{\partial\mathbf{X}}=\frac{\partial\mathcal{L}}{ \partial\mathbf{Y}}\frac{\partial\mathbf{Y}}{\partial\mathbf{X}}=\tilde{ \mathbf{L}}_{sym}\frac{\partial\mathcal{L}}{\partial\mathbf{Y}}\mathbf{W}^{ \top},\frac{\partial\mathcal{L}}{\partial\mathbf{W}}=\frac{\partial\mathcal{L }}{\partial\mathbf{Y}}\frac{\partial\mathbf{Y}}{\partial\mathbf{W}}=\mathbf{X}^ {\top}\tilde{\mathbf{L}}_{sym}^{\top}\frac{\partial\mathcal{L}}{\partial \mathbf{Y}} \tag{3}\]

where the parameters of graph convolution are \(\mathbf{\theta}_{GC}=[\mathbf{W};\mathbf{b}],\mathbf{W}\in\mathbb{R}^{d_{im}\times d _{our}},\mathbf{b}\in\mathbb{R}^{d_{our}}\). We consider safely computing the two processes under the limitation that data privacy should be bi-directionally protected for these processes, where the parties cannot have access to another one's data (i.e. \(\mathcal{P}_{1}\) cannot infer the adjacent matrix \(\mathbf{A}\) and \(\mathcal{P}_{2}\) cannot infer the node features \(\mathbf{X}\) during computation). We follow (Li et al., 2017) to assume that all the parties are honest and curious. Different from works that consider each party to own user-user and user-item interactions partially, we attempt to apply GNNmodules to the **social-data-fully-inaccessible** vertical federated social recommendation.

## 4. Methodology

### Motivation

After social aggregation in Eq.(1) and Eq.(2), \(\mathcal{P}_{1}\) obtains the output Y for further computation of the loss \(\mathcal{L}\). To optimize the model, \(\mathcal{P}_{1}\) uses \(\frac{\mathcal{P}_{1}}{2\mathcal{P}}\) to compute the derivate of node features \(\frac{\mathcal{P}_{1}}{2\mathcal{P}}\). We notice that a key computation paradigm, multiplying three matrices, repeatedly appears in both forward and backward processes. Further, if we let the parameter matrix \(\mathbf{W}\) be kept by \(\mathcal{P}_{2}\) that owns \(\bar{\mathbf{L}}_{sym}\), the matrices on both sides and the matrix at the middle for each equation will be kept by different parties. In addition, the left-side result of each equation will be only needed by the one that owns the middle matrix. This observation motivates us to consider such a problem

_Given the matrices \(\mathbf{L}\in\mathbb{R}^{p\times q}\mathbf{N}\in\mathbb{R}^{r\times s}\) owned by the party \(p_{1}\) and the matrix \(\mathbf{M}\in\mathbb{R}^{q\times r}\) owned by the party \(p_{2}\), how can we design an algorithm to satisfy the two requirements below_

* _the party \(p_{2}\) obtains the multiplication \(\mathbf{J}=\mathbf{LMN}\) without exposing \(\mathbf{M}\) to the party \(p_{1}\)._
* _the party \(p_{2}\) cannot infer \(\mathbf{L}\) and \(\mathbf{N}\) from \(\mathbf{J}\) and \(\mathbf{M}\)._

As long as the above problem is solved, the computing processes of a graph convolution operator can be done without leaking data privacy. Therefore, we now focus on how to find a solution to this problem with the theoretical guarantee of privacy-preserving.

### Sandwich encryption

#### 4.2.1. Solution to \(R1\)

For the first requirement, each time there is a need to compute \(\mathbf{J}=\mathbf{LMN}\), the party first \(p_{2}\) encrypts the matrix \(\mathbf{M}\) with the public key \(\mathcal{P}_{pub,2}\) by simply using _Homomorphic Encryption_ (e.g. Paillier (2007)). Then, the ciphertext \([\mathbf{M}]_{\mathcal{P}_{pub,2}}\) is sent to the party \(p_{1}\) to compute \([\mathbf{J}]_{\mathcal{P}_{pub,2}}=\mathbf{L}[\mathbf{M}]_{\mathcal{P}_{pub,2}}\mathbf{N}\), and the result is returned to \(p_{2}\). By decrypting the result with the private key \(\mathcal{P}_{pub,2}\), \(p_{2}\) can know \(\mathbf{J}\) without leaking \(\mathbf{M}\) to \(p_{1}\).

#### 4.2.2. Solution to \(R2\)

Now we discuss how to protect privacy for \(\mathbf{L}\) and \(\mathbf{M}\).

#### Database-level protection

Since \(p_{2}\) doesn't know the exact values of both the two side matrices, it brings significant challenges for \(p_{2}\) to steal information about them from \(\mathbf{J}\) and \(\mathbf{M}\). To better illustrate this, we take an example where all variables of the equation \(j=lmn\) are scalars, and we can thus infer that \(j/m=ln\), which indicates there are infinite combinations of \(l\) and \(n\) for any given \(j\neq 0,m\neq 0\). For the matrix case, we illustrate the protection on the database level through Theorem 1.

**Theorem 4.1**.: _Given \(\mathbf{J}=\mathbf{LMN}\) where all matrices are not zero matrices, there exists infinite combinations of \(\mathbf{N}^{\prime}\neq\mathbf{N},\mathbf{L}^{\prime}\neq\mathbf{L}\) such that \(\mathbf{J}=\mathbf{L}^{\prime}\mathbf{MN}^{\prime}\)._

Proof.: See Appendix A.2. 

Therefore, without knowing \(\mathbf{L}\) (or \(\mathbf{N}\)), \(p_{2}\) cannot fully recover \(\mathbf{N}\) (or \(\mathbf{L}\)), leading to the _database-level_ privacy protection. However, this barrier fails to protect the privacy of the two-side matrices at the element level. For example, if there are only two users' embeddings in \(\mathbf{M}\in\mathbb{R}^{2\times d_{\mathbf{M}}}\) and one of the two embeddings happens to be zero, we can easily infer whether the two users have social interactions from the result \(\mathbf{J}\in\mathbb{R}^{2\times d_{\mathbf{M}^{\prime}}}\) by recognizing whether the aggregated embeddings corresponding to the zero embedding are still zero.

#### Element-level protection

To further enhance privacy protection for the two-side matrices at the element level, we introduce differential privacy (DP) noise (Berg et al., 2017) to the computed result \(\mathbf{J}\). DP offers participants in a database the compelling assurance that information from datasets is virtually indistinguishable whether or not someone's personal data is included. Since the object to be protected can be of high dimension, we leverage the advanced matrix-level DP

Figure 2. The framework of the _Sandwich Encryption_

mechanism, aMGM, introduced by (Zhou et al., 2017; Zhang et al., 2018) to enhance the utility of the computation.

**Definition 4.2** (analytic Matrix Gaussian Mechanism (Zhou et al., 2017)).: For a function \(f(\mathbf{X})\in\mathbb{R}^{m\times n}\) and a matrix variate \(\mathbf{Z}\sim\mathcal{MN}_{m,n}(\mathbf{0},\Sigma_{1},\Sigma_{2})\), the analytic Matrix Gaussian Mechanism is defined as

\[\text{aMGM}(f(\mathbf{X}))=f(\mathbf{X})+\mathbf{Z} \tag{4}\]

where \(\mathcal{MN}_{m,n}(\mathbf{0},\Sigma_{1},\Sigma_{2})\) denotes matrix gaussian distribution.

**Definition 4.3** (Matrix Gaussian Distribution(Zhou et al., 2017)).: The probability density function for the \(m\times n\) matrix-valued random variable \(\mathbf{Z}\) which follows the matrix Gaussian distribution \(\mathcal{MN}_{m,n}(\mathbf{M},\Sigma_{1},\Sigma_{2})\) is

\[\Pr(\mathbf{Z}|\mathbf{M},\Sigma_{1},\Sigma_{2})=\frac{\exp\frac{1}{2}\| \mathbf{U}^{-1}(\mathbf{Z}-\mathbf{M})\mathbf{V}^{-\top}\|_{F}^{2}}{(2\pi)^{ mn/2}|\Sigma_{2}|^{n/2}|\Sigma_{1}|^{m/2}} \tag{5}\]

where \(\mathbf{U}\in\mathbb{R}^{m\times m},\mathbf{V}\in\mathbb{R}^{n\times n}\) are invertible matrices and \(\mathbf{U}\mathbf{U}^{\top}=\Sigma_{1},\mathbf{V}\mathbf{V}^{\top}=\Sigma_{2} \mid\cdot\mid\) is the matrix determinant and \(\mathbf{M}\in\mathbb{R}^{m\times n},\Sigma_{1}\in\mathbb{R}^{m\times m},\Sigma _{2}\in\mathbb{R}^{n\times n}\) are respectively the mean, row-covariance, column-covariance matrices.

The privacy protection is guaranteed by Lemma 4.4

**Lemma 4.4** (DP of aMGM (Zhou et al., 2017)).: _For a query function \(f\), aMGM satisfies \((\epsilon,\delta)-DP\), iff_

\[\frac{s_{2}(f)}{b}\leq\sigma_{m}(\mathbf{U})\sigma_{n}(\mathbf{V}) \tag{6}\]

_where \(b\) is decided by \((\epsilon,\delta)\) and \(s_{2}(f)\) is the \(L_{2}\)-sensitivity, \(\sigma_{m}(\mathbf{U})\) and \(\sigma_{n}(\mathbf{V})\) are respectively the smallest singular values of \(\mathbf{U}\) and \(\mathbf{V}\)._

The general procedure of the _Sandwich Encryption_ is listed in Algorithm.1. The encryption process is like making a sandwich where the two pieces of bread are corresponding to the two-side matrices and the middle matrix is the meat in the sandwich as shown in Figure 2. By properly pre-processing the materials, the data privacy of each material can be preserved. While we apply DP to enhance privacy protection, how to preserve the utility of these computing processes as much as possible still brings non-trivial challenges. To this end, we design the Privacy-Preserving Two-Party Graph Convolution Network (P4GCN) to enhance the utility of the model while applying DP.

### P4gcn

#### 4.3.1. Architecture

Overview.The architecture of P4GCN is as shown in Figure 3. During each training iteration, \(\mathcal{P}_{1}\) first locally aggregates the user features \(\mathbf{X}_{user}^{(0)}\) and the item features \(\mathbf{X}_{item}^{(0)}\) by the backend (e.g., LightGCN(Chen et al., 2019)) into embeddings \(\mathbf{X}_{user}^{(1)}\) and \(\mathbf{X}_{item}^{(1)}\). Then, \(\mathcal{P}_{1}\) uses Algo.1 to collaboratively compute the user social embeddings that are aggregated on the social data by the GCN layer with \(\mathcal{P}_{2}\). After obtaining the user social embeddings \(\mathbf{X}_{user}^{(2)}\), \(\mathcal{P}_{1}\) uses the fusion layer to aggregate \(\mathbf{X}_{user}^{(1)}\) and \(\mathbf{X}_{user}^{(2)}\) together to construct the new

Figure 3. The training workflow of the proposed P4GCN

user embeddings \(\mathbf{X}_{user}^{(3)}\). Finally, both \(\mathbf{X}_{user}^{(3)}\) and \(\mathbf{X}_{item}^{(1)}\) are input into the decoder to obtain the predictions to compute the loss. The backward computation of the social GCN layer is also protected by Algo.1.

Fusion LayerThe fusion layer is designed for two reasons. For one thing, the DP mechanism may bring too much noise that leads to the degradation of the model performance. For another thing, the social information of different users may not consistently improve the model's performance but harm it. Therefore, we design the fusion layer to adaptively extract useful information by reweighing the inputs. Concretely, the fusion layer allocates weights to each activation in each user's embeddings by a two-layer MLP with a softmax function and position-wisely fuses them. This introduces a chance for the party \(\mathcal{P}_{1}\) to avoid the collaboration significantly reducing local model performance.

#### 4.3.2. Privacy-Preserved Social Aggregation

We analyze the sensitivity of the graph convolution and then apply aMGM to its computing processes.

ForwardDuring aggregation, the user \(i\)'s social embedding is specified by \(\mathbf{x}_{i}^{(2)}=\mathbf{X}_{user,i}^{(2)}=\mathbf{l}\mathbf{X}\mathbf{W} _{i}\mathbf{l}_{i}=\tilde{\mathbf{L}}_{sym,i}\), which can be independently computed without queries on other users' social embeddings. Therefore, we focus on the computing sub-process \(f_{i}(\mathbf{l},\mathbf{X}\mathbf{W})\) to protect user-level privacy (i.e., the social interaction between any two users). Given two adjacent social databases \(\mathbf{A}\) and \(\mathbf{A}^{\prime}\) whose elements are the same except one (e.g., \(||\mathbf{A}-\mathbf{A}^{\prime}||_{F}=1\)), the \(L_{2}\)-sensitivity of each \(f_{i},\forall i\in[N]\) is bounded by

\[s_{2}(f)=\max_{A,A^{\prime}}\|f_{i}^{\prime}\mathbf{X}\mathbf{W}-\mathbf{l}_{ i}\mathbf{X}\mathbf{W}\|_{F}\leq||\mathbf{X}\|_{F}\|\mathbf{W}\|_{FSI}(i) \tag{8}\]

\[s_{l}(i)=\left\{\begin{array}{ll}(\frac{1}{2}+\frac{1}{c_{0}}c_{0})^{1/2},&,\mathbf{a}_{i}=\mathbf{0}\\ (\frac{1}{||\mathbf{a}||_{i}^{2}+||\mathbf{a}||_{i}}c_{i}+\frac{1}{||\mathbf{a} ||_{i}}c_{0})^{1/2},&,\text{else}\end{array}\right. \tag{9}\]

where \(c_{i}=\sum_{j=1}^{N}\frac{a_{i}\neq i(i+j)}{||\mathbf{a}_{i}||_{i}+1}\leq||a_{i }||_{1}+1,c_{0}=\max_{j}\frac{1}{||\mathbf{a}_{j}||_{i}+1}\leq 1,s_{l}(i) \leq 2\) always hold for all users. Then, we respectively clip \(\mathbf{X}\) and \(\mathbf{W}\) by \(\max(1,\frac{||\mathbf{L}||_{F}}{c_{0}})\) to bound the sensitivity \(s_{2}(f)\leq C^{2}s_{l}(i)\) (e.g., the coefficient \(C=1\) in experiments) before computation and finally rescale the computed result by the inverse scale factor. We empirically scale \(\mathbf{L}_{sym}\) with a factor \(\frac{1}{N}\) in practice. We detail the derivation of the sensitivity term in Appendix A.1.

Backward for node featuresThe backward process for node features \(f_{i}^{\text{back}}\) is \(\eta\frac{\partial\mathcal{L}}{\partial\mathbf{x}_{i}^{(2)}}=\mathbf{l}_{i}( \eta\frac{\partial\mathcal{L}}{\partial\mathbf{V}})\mathbf{W}^{\top}\). We bound the sensitivity of \(f_{i}^{\text{back}}\) like forward process \(f_{i}\), leading to the same bound

\[s_{2}(f_{i}^{\text{back}})\leq C^{2}s_{l}(i) \tag{10}\]

Backward for model parametersThe backward process for model parameters is \(\frac{\partial\mathcal{L}}{\partial\mathbf{W}}=\mathbf{X}^{\top}\tilde{\mathbf{ L}}_{sym}^{\top}\frac{\partial\mathcal{L}}{\partial\mathbf{V}}\). We notice that the actual function sensitivity can be significantly influenced by the Frobenius norms of all the three matrices that scale with the user number \(N\), leading to large noise added to the computed result. Therefore, we seek for an alternative to this computing process by splitting \(\mathbf{W}=\mathbf{W}_{p_{1}}\mathbf{W}_{p_{1}},\mathbf{W}_{p_{2}}\in\mathbb{R} ^{d_{in}\times d_{in}},\mathbf{W}_{p_{1}}\in\mathbb{R}^{d_{in}\times d_{out}}\) and freeze \(\mathbf{W}_{p_{1}}\) that is kept by the party \(\mathcal{P}_{2}\) without updating it. We initialize \(\mathbf{W}_{p_{2}}\) by normal distribution to approximate full rank and then clip it only once before training starts. The parameter \(\mathbf{W}_{p_{1}}\) is updated by \(\mathcal{P}_{1}\) without any communication to \(\mathcal{P}_{2}\) since components in \(\frac{\partial\mathcal{L}}{\partial\mathbf{W}_{p_{1}}}=(\tilde{\mathbf{L}}_{sym }\mathbf{X}\mathbf{W}_{p_{1}})^{\top}\frac{\partial\mathcal{L}}{\partial \mathbf{V}}\) are already known by \(\mathcal{P}_{1}\).

PrivacyWe independently apply aMGM mechanism to each user's social embedding based on its sensitivity bound (e.g., Eq.(4.3.2) and Eq.(9)). Eq.(8) suggests that the more social relations one user owns, the smaller sensitivity its computing process is, resulting in less noise being injected into the intermediates of this user. The total privacy cost can be estimated by the maximum privacy cost among users according to the parallel composition theorem (Shen et al., 2017). We follow (Shen et al., 2017) to accumulate privacy costs across iterations based on the privacy loss distribution of aMGM in Lemma.4.5.

**Lemma 4.5**.: _[Privacy Loss of aMGM(Shen et al., 2017)] The privacy loss variable of aMGM follows gaussian distribution \(\mathcal{N}(\eta,2\eta)\) and \(\eta\) is given by \(\eta=\frac{\|\mathbf{U}^{-1}(f(X)-f(X^{\prime}))\mathbf{V}^{-1}\|_{F}^{2}}{2}\)._

#### Efficiency

Batch-wise optimizationWe now show how to optimize the model in a batch-wise manner for efficiency. The full batch training will bring large communication and computation costs (e.g., frequently encrypting large matrices and transmitting the expanded ciphertext). To tackle this issue, for a batch of records \(\{(u_{k_{i}},v_{k_{i}},P_{k_{i}})\}||_{k=1}^{|B|}\), we denote the users in the current batch as \(\mathcal{B}\in\mathbb{R}^{|\mathcal{B}|\times|N},|\mathcal{B}|\leq|B|\). Then, the corresponding computing process is

\[\mathbf{Y}_{B}=(\mathcal{B}\tilde{\mathbf{L}}_{sym})\mathbf{X}\mathbf{W}_{i} \frac{\partial\mathcal{L}}{\partial\mathbf{X}_{B}}=(\mathcal{B}\tilde{\mathbf{ L}}_{sym}\mathbf{S}^{\top})\frac{\partial\mathcal{L}}{\partial\mathbf{Y}_{B}}\mathbf{W}^{\top} \tag{11}\]

In this way, the party \(\mathcal{P}_{2}\) can store the full ciphertext \([\mathbf{X}]\) that will be only encrypted once and batch-wisely update it by \(\eta\)\([\frac{\partial\mathcal{L}}{\partial\mathbf{Y}_{B}}]\). Unlike full batch training, the embeddings of users out of the batch cannot be updated. Otherwise, the social interactions will be easily exposed to the recommender.

CommunicationThe communication cost lies in the transmission of the encrypted middle matrices (i.e. \(\mathbf{X}\), \(\frac{\partial\mathcal{L}}{\partial\mathbf{X}_{B}}\)) and the results (i.e. \(\mathbf{Y}_{B},\frac{\partial\mathcal{L}}{\partial\mathbf{X}_{B}}\)). Since \(\mathbf{X}\) is only encrypted and transmitted once, the total communication cost is \(\mathcal{O}(Nd+TBd)\) over iterations \(T\) where \(\frac{\partial\mathcal{L}}{\partial\mathbf{Y}_{B}},\mathbf{B}\in\mathbb{R}^{| \mathcal{B}|\times d_{out}}\), \(\frac{\partial\mathcal{L}}{\partial\mathbf{X}_{B}}\in\mathbb{R}^{|\mathcal{B}| \times d_{in}}\) and \(d=\max{(d_{in},d_{out})}\).

## 5. Evaluation

### Experimental Setting

\begin{table}
\begin{tabular}{l|l l l l} \hline \hline
**Dataset** & **CiaoDVD** & **FilmTrust** & **Douban** & **Epinions** \\ \hline
**Users** & 7375 & 1508 & 3000 & 22158 & 5258 \\
**Items** & 99746 & 2071 & 3000 & 296277 & 5258 \\
**Ratings** & 278483 & 35497 & 136891 & 728517 & 529 \\
**Social Links** & 111781 & 1853 & 7765 & 355364 & 528 \\
**Density\({}_{Rating}\)** & 0.0379\% & 1.1366\% & 1.5210\% & 0.0110\% & 522 \\
**Density\({}_{Link}\)** & 0.2055\% & 0.0815\% & 0.0863\% & 0.0723\% & 533 \\ \hline \hline \end{tabular}
\end{table}
Table 1. Dataset statistics _Datasets._ We use four social recommendation datasets to validate the effectiveness of the proposed method: Filmtrust (Shi et al., 2018), CiaoDVD (Shi et al., 2018), Douban (Shi et al., 2018), and Epinions (Shi et al., 2018). Specifically, we set the social data owned by \(\mathcal{P}_{2}\) and other data owned by \(\mathcal{P}_{1}\). We show the statistics of the datasets in Table 1.

_Implementation._ All our experiments are implemented on a Ubuntu 16.04.6 server with 64 GB memory, 4 Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz, 4 NVidia(R) 3090 GPUs, and PyTorch 1.10.1.

_Baselines._ We compare P4GCN with two types of baselines. The first type contains traditional methods without using social data. These methods are concluded as follows

* **PMF(Huang et al., 2019)** is a classic matrix factorization model that only uses rating data on \(\mathcal{P}_{1}\).
* **NeuMF(Shi et al., 2018)** is a neuron-network-based matrix factorization method that has superior performance against traditional MF methods.
* **GCN(Shi et al., 2018)** is a classic convolutional graph neural network that only uses rating data on \(\mathcal{P}_{1}\).
* **LightGCN(Shi et al., 2018)** improves the convolutional graph neural network by reducing the parameters and aggregating the activations of different layers.
* **FeSoGe**-(Shi et al., 2018) removes the social aggregation module from the original version that requires social links to be stored together with user features, which will break our fundamental assumption of inaccessible social data. We compare FeSog with fully available data in Sec. 5.7

The second type contains methods that safely use social data to make social recommendations:

* **SeSoRec(Shi et al., 2018)** tries to solve the privacy-preserving cross-platform social recommendation problem, but suffers from excessive and efficiency problems.
* **SeSeNet(Shi et al., 2018)** is the state-of-the-art method that solves the safety problem and improves the efficiency within the scope of matrix factorization on the basis of **SeSoRec**.
* **P4GCN** (ours) is set to satisfy \((\epsilon,\delta)\)-DP guarantee (e.g., \(\epsilon\) depends on the dataset) and **P4GCN\({}^{*}\)** corresponds to the ideal case without injecting DP noise.

_Hyper-parameters._ We fix the embedding dimensions \(k=64\) of the model for all the datasets. We tune the learning rate \(\eta\in\{1e-3,1e-2,1e-1,1,10,100,1000\}\) and batch size \(|B|\in\{64,256,512,1024,2048,4096\}\), full to achieve each method's optimal results. We respectively limit the privacy budgets of P4GCN by \(\epsilon=\{15,10,10,10,0,3.0\}\) and \(\delta=1e-4\) across datasets in columns of Table 2 (i.e., FilmTrust, CiaoDvd, Douban, and Filmtrust). The hyper-parameter \(\beta_{\text{P4GCN}}\) is tuned on \(\{0.01,0.05,0.1,0.5,1.0,10.0,100.0\}\) and both \(\lambda_{\text{SeSoRec}}\) and \(\lambda_{\text{SSRec}}\) are tuned on \((1e-4,1e-3,1e-2,1e-1)\).

_Metrics._ We follow previous works (Chen et al., 2019) to use Root Mean Square Error (RMSE) and Mean Absolute Error (MAE) as the evaluation metrics of model performance.

### Model performance

From Table 2, we find that: (1) P4GCN\({}^{*}\) without DP consistently improves both MAE and RMSE metrics over all the baselines on the first three datasets (i.e., FilmTrust, CiaoDVD, and Douban) and achieves competitive results (e.g., RMSE= 1.0642, MAE=0.8186) against others' optimal results (e.g., RMSE\({}_{\text{LightGCN}}=1.0746\) and MAE\({}_{\text{NeNeMF}}=0.8020\)). (2) Our proposed Sandwich Encryption Module can well preserve the final model performance over four datasets given proper privacy budgets, which achieves the optimal or second optimal results over 87.5% columns. (3) P4GCN exhibits superior performance to traditional matrix-decomposition-based social recommendation (e.g., SeSoRec and S3Rec), especially on datasets of large-scale (e.g., CiaoDVD with 7375 clients and Epinions with 22158 clients). We attribute this enhancement to the adaption of GNN which has a stronger representation ability than the traditional matrix-decomposition-based model in recommendation.

\begin{table}
\begin{tabular}{c|c|c c|c c|c c|c c} \hline \multirow{2}{*}{**Method**} & \multicolumn{2}{c|}{**FilmTrust**} & \multicolumn{2}{c|}{**CiaoDVD**} & \multicolumn{2}{c|}{**Douban**} & \multicolumn{2}{c}{**Epinions**} \\ \cline{3-10}  & & RMSE & MAE & RMSE & MAE & RMSE & MAE & RMSE & MAE \\ \hline \multirow{5}{*}{**Local**} & PMF & 0.8007 & 0.6106 & 1.2245 & 0.9651 & 0.8361 & 0.6300 & 1.2487 & 0.9721 & 648 \\ \cline{2-10}  & NeuMF & 0.8287 & 0.6319 & 1.1842 & 0.8839 & 0.7894 & 0.6222 & 1.1285 & **0.8020** & 648 \\ \cline{2-10}  & GCN & 0.8765 & 0.6796 & 1.1076 & 0.8383 & 0.7989 & 0.6346 & 1.1513 & 0.8177 & 648 \\ \cline{2-10}  & LightGCN & 0.7960 & 0.6079 & 1.1186 & 0.8396 & 0.7892 & 0.6209 & 1.0746 & 0.8412 & 649 \\ \cline{2-10}  & FeSeg\({}^{-}\) & 0.8029 & 0.6118 & 1.2314 & 0.9741 & 0.8331 & 0.6498 & 1.2171 & 0.9530 & 651 \\ \hline \multirow{5}{*}{**Social**} & SeSoRec & 0.8009 & 0.6106 & 1.1988 & 0.9635 & 0.8171 & 0.6316 & 1.2131 & 0.9598 & 652 \\ \cline{2-10}  & S3Rec & 0.8009 & 0.6106 & 1.1988 & 0.9635 & 0.8171 & 0.6316 & 1.2131 & 0.9598 & 653 \\ \cline{1-1} \cline{2-10}  & P4GCN & 0.7929 & 0.6059 & **1.0776** & **0.8224** & 0.7672 & **0.6023** & 1.0744 & 0.8272 & 655 \\ \cline{1-1} \cline{2-10}  & P4GCN\({}^{*}\) & **0.7905** & **0.6032** & 1.0803 & 0.8225 & **0.7670** & 0.6035 & **1.0642** & 0.8186 \\ \hline \end{tabular}
\end{table}
Table 2. Comparison results of different models in terms of model accuracy (in RMSE and MAE). The optimal (second optimal) result of each column is bolded (underlined).

### Impact of privacy budget \(\epsilon\)

_Privacy Budget._ We investigate the impact of privacy budget \(\epsilon\) on our proposed method in Figure 4, where the red dashed line corresponds to results without leveraging social data and the green dashed line corresponds to the ideal results without adding DP noise. First, as the privacy budget grows properly, P4GCN introduces non-trivial improvements over the results without using social information (e.g., the bars below the red dashed lines). Second, our proposed privacy-preserving mechanism can well preserve the performance of the ideal case without adding DP noise (e.g., the green dashed lines), which confirms the effectiveness of our P4GCN in leveraging social data to enhance existing recommendation systems.

_Ablation on the fusion layer._ We further demonstrate the effectiveness of the fusion layer integrated into P4GCN by directly averaging the user social embeddings (e.g., scaled by \(\beta\)) and the original user embeddings for comparison. As shown in Figure 4, P4GCN will suffer performance degradation after removing the fusion layer across different datasets, where most of the yellow bars are higher than the blue ones under the same privacy budget \(\epsilon\). In addition, P4GCN w.o. the fusion layer failed to approximate the ideal performance even though the privacy budget is relatively large (e.g., \(\epsilon=10.0\) in CiaoDVD), while the version w. Fusion did. This suggests the excellent ability of the fusion layer to aggregate the social information into the user features. Further, P4GCN with the fusion layer also shows a better tolerance to the low privacy budget than the one without using the fusion layer. For example, P4GCN w.o. the fusion layer will harm the original recommendation system on FilmTrust when \(\epsilon=10.0\) and Douban when \(\epsilon=5.0\), while the usage of the fusion layer decreases the minimal effective privacy budget. These results confirm the effectiveness of the proposed fusion layer in both handling DP-noise and fusing social information.

### Integrate To Existing Methods

We show that existing local recommendation methods (e.g., PMF and GCN) without considering social data can benefit from our proposed P4Layer on FilmTrust and CiaoDVD in Table 3, which suggests that companies can improve their local recommendation system by leveraging our proposed P4GCN in a plug-in manner. The parameters of differential privacy are consistent with the settings in Table 2.

### Impact of hyper-parameter \(\beta\)

We study the impact of the choice of hyper-parameter \(\beta\) on the model performance in Figure 5. We denote P4GCN without adding DP noise as the ideal case (e.g., the red notations). The figure shows that the optimal value of \(\beta\) is always larger than 0 across all the datasets, indicating that the recommendation system can consistently benefit from social information integrated by our P4GCN regardless of differential privacy. In addition, the DP noise lowers the optimal degree of leveraging social information (e.g., the blue

Figure 4. The model performance RMSE and MAE of P4GCN w/w.o. fusion layer v.s. privacy budget \(\epsilon\).

\begin{table}
\begin{tabular}{c|c|c c c c} \hline \hline \multicolumn{2}{c|}{\multirow{2}{*}{**Method**}} & \multicolumn{2}{c}{**FilmTrust**} & \multicolumn{2}{c}{**CiaoDVD**} \\  & RMSE & MAE & RMSE & MAE \\ \hline \multirow{4}{*}{**PMF**} & **original** & 0.8007 & 0.6106 & 1.2245 & 0.9651 & 783 \\  & **+P4Layer\&DP** & **0.7997** & 0.6112 & 1.2163 & 0.9648 & 784 \\  & **+P4Layer-Ideal** & **0.7997** & **0.6105** & **1.2125** & **0.9642** & 786 \\ \hline \multirow{4}{*}{**GCN**} & **original** & 0.8765 & 0.6796 & 1.1709 & 0.8731 & 778 \\  & **+P4Layer\&DP** & 0.8569 & 0.6606 & **1.1388** & 0.8766 & 788 \\  & **+P4Layer-Ideal** & **0.8506** & **0.6486** & 1.1414 & **0.8598** & 790 \\ \hline \hline \end{tabular}
\end{table}
Table 3. The improvement on model performance by integrating P4Layer to existing methods.

star never appears on the left of the red star) since the aggregation efficiency can be degraded by the noise. We also notice that a large value of \(\beta\) will lead to the degradation of the model performance, which suggests the choice of \(\beta\) should be very careful in practice. We consider how to efficiently and adaptively decide effective \(\beta\) as our future works.

### Communication cost

We list the communication costs of P4GCN and another communication-efficient VFL social recommendation method (i.e., S3Rec (He et al., 2016)) in Table 4. We report the communication costs under fixed parameter settings (e.g., 3th-5th columns) and the practical settings used in Table 2 (e.g., the last columns). P4GCN causes nearly 2.2\(\times\) costs than S3Rec when the epoch number and batch size are fixed on three datasets (i.e., FilmTrust, CiaoDVD, and Douban), and P4GCN saves \(\frac{2}{3}\) costs on Epinions. Although S3Rec exhibits lower communication amounts than P4GCN under fixed settings, P4GCN can achieve competitive communication efficiency when each method runs until reaching its optimal results. We also plan to further improve the communication efficiency of P4GCN in our future works.

### Comparison with FeSog w. social data

We finally compare our method with FeSog-Ideal which can directly access the full social data to verify the advantage of P4GCN in enhancing recommendation systems with social data. As shown in Figure 6, integrating social data can slightly improve model performance in FeSeg when the social data is fully available in most cases (e.g., CiaoDVD, Douban, and Epinions). However, FeSeg-Ideal failed to leverage social data to enhance performance in FilmTrust. We attribute this to the weak connection between social information and recommendations in FilmTrust, where S3Rec/SeSoRec also suffers similar failure and the improvement of P4GCN is also limited. Further, our P4GCN dominates FeSeg in terms of RMSE and MAE across all the datasets regardless of the availability of social data to FeSeg and the usage of differential privacy, which confirms the advantage of P4GCN in federated social recommendation.

## 6. Conclusion

This paper addresses the development of GNN-based models for a secure social recommendation. We present P4GCN, a novel vertical federated social recommendation approach designed to enhance recommendation accuracy when dealing with inaccessible social data. P4GCN incorporates a sandwich-encryption module, which guarantees comprehensive data privacy during collaborative computing. Experimental results on four datasets demonstrate that P4GCN outperforms state-of-the-art methods in terms of recommendation accuracy. We are considering leveraging other formats of graph information like LLM guidance, and knowledge graph, by P4GCN to enhance recommendation systems in our future works.

\begin{table}
\begin{tabular}{c|c|c c c c} \hline
**Dataset** & **Method** & **B–64** & **B–1024** & **B–4096** & **Prac.** \\ \hline \multirow{2}{*}{**FilmTrust**} & P4GCN & 10.70 & 10.68 & 3.81 & 61.77 \\  & S3Rec & 5.48 & 5.47 & 1.78 & 118.33 \\ \hline \multirow{2}{*}{**CiaoDVD**} & P4GCN & 21.88 & 21.88 & 21.74 & 21.88 \\  & S3Rec & 15.01 & 15.01 & 14.88 & 21.00 \\ \hline \multirow{2}{*}{**Douban**} & P4GCN & 42.28 & 42.22 & 31.44 & 82.18 \\  & S3Rec & 19.23 & 19.20 & 14.01 & 33.70 \\ \hline \multirow{2}{*}{**Epinions**} & P4GCN & 394.44 & 394.44 & 394.24 & 716.38 \\  & S3Rec & 1160.98 & 1160.96 & 1160.09 & 2785.83 \\ \hline \end{tabular}
\end{table}
Table 4. Communication costs (GB) under the fixed epoch \(E=5\) with varying batch sizes (e.g., 64, 1024, and 4096) and the practical cost in our experiments in Table 2 (e.g., the last column)

Figure 5. The impact of social aggregation degree \(\beta\) v.s. model performance (i.e. MAE)

Figure 6. The model performance of FeSeg and P4GCN across datasets where smaller areas are better. Each metric is divided by its corresponding maximum value for a clear view.

## References

* (1) Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. _arXiv preprint arXiv:1609.02097_, 2016.
* (2) Peter Velickovic, Guillem Cucurull, Arana Casanova, Adriana Romero, Pietro Le, and Yoshua Bengio. Graph attention networks. _arXiv preprint arXiv:1710.10983_, 2017.
* (3) Rex Ying, Ruining He, Kaifeng Chen, Yong Elsombatchai, William L Hamilton, and Jure Leskovec. Graph convolutional neural networks for web-scale recommender systems. In _Proceedings of the 24th ACM SIGKDD international conference on Knowledge discovery & data mining_, pages 794-803, 2018.
* (4) Wenjun Fan, Yao Ming, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. Graph neural networks for social recommendation. In _The world wide web conference_, pages 417-426, 2019.
* (5) Mark Weber, Giacomo Domeniconi, Jie Chen, Daniel Karl I Weidde, Claudio Bellei, Tom Robinson, and Charles Lileserson. Anti-monetary modeling in biometrics: Experimenting with graph convolutional networks for financial forensics. _arXiv preprint arXiv:1908.02591_, 2019.
* (6) Ziqin Liu, Chacheng Chen, Longfei Li, Jun Zhou, Xiaolong Li, Le Song, and Yuan Qi. Genipati. Graph neural networks with adaptive receptive paths. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 33, pages 4424-4431, 2019.
* (7) Brendan McMahan, Eder Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In _Artificial intelligence and statistics_, pages 1273-1282. PMLR, 2017.
* (8) Andrew C Yao. Protocols for secure computations. In _23rd annual symposium on foundations of computer science (1982)_, pages 160-164. IEEE, 1982.
* (9) Ronald I. Rivest, Len Adleman, Michael L Dertoumess, et al. On data banks and privacy homomorphisms. _Foundations of secure computation_, (411):169-180, 1978.
* (10) Cynthia Dwork. Differential privacy. In _Automata, Languages and Programming: 33rd International Colloquium, ICALP 2006, Vote, Italy, July 10-14, 2006. Proceedings, Part II_, pages 1-12. Springer, 2006.
* (11) Andriy Mnih and Rusk S Mnichlindero. Probabilistic matrix factorization. _Advances in neural information processing systems_, 20, 2007.
* (12) Chaochao Chen, Liang Li, Binghu Wei, Cheng Hong, Li Wang, and Jun Zhou. Secure social recommendation based on secret sharing. In _ECAI 2020_, pages 506-512. ICS Press, 2020.
* (13) Jimmin Chiu, Chacheng Chen, Linguian Lyu, Carl Yang, and Wang Li. Exploiting data sparsity in secure cross-platform social recommendation. _Advances in Neural Information Processing Systems_, 34(1):10524-10534, 2021.
* (14) Kartik Sharma, Yoon-Chang Lee, Siyuan Nambi, Aditya Salim, Shik Shah, Sang-Woo Kim, and Siriu Kumar. A survey of graph neural networks for social recommender systems. _arXiv preprint arXiv:2212.02248_, 2022.
* (15) Pan Gu, Yuqiang Han, Wei Gao, Guandong Xu, and Jian Wu. Enhancing session-based social recommendation through item gap with embedding and contextual friendship modeling. _Neurocomputing_, 49:190-202, 2021.
* (16) Kankin Narang, Yelong Song, Alessandro Schwing, and Hari Sundaram. Tuserc: fusing user and item homophily modeling with temporal recommender systems. _Data Mining and Knowledge Discovery_, 35:837-862, 2021.
* (17) Yong Niu, Xing Xing, Mingdong Liu, Qingyu Han, and Zhichun Jia. Multi-preference social recommendation of users based on graph neural network. In _2021 International Conference on Intelligent Computing, Automation and Applications (ICAA)_, pages 109-194. IEEE, 2021.
* (18) Hongyi Sun, Liliu, and Raping Chen. Social recommendation based on graph neural networks. In _2020 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communication_, ser. Social Computing & Networking (ESPA/HICDM)/Social/SocialCom/StatisticCom), pages 489-496. IEEE, 2020.
* (19) Dengcheng Yan, Tianyi Tang, Wenxin Xie, Yuwen Zhang, and Qiang He. Session-based social and dependency-aware software recommendation. _Applied Soft Computing_, 111:1680-1682, 2022.
* (20) Nanyun Peng, Hoifang Poon, Chris Quirk, Kristina Tothuwan, and Wen-tau Yih. Cross-sentence-ranr-r activation constraint with graph humans. _Transactions of the Association for Computational Linguistics_, 5:101-115, 2017.
* (21) Victoria Zayats and Mari O'Gendorf. Conversation modeling on reddit using a graph-structured lstm. _Transactions of the Association for Computational Linguistics_, 6:211-132, 2018.
* (22) Di Chiu, Yang Dong, Kai Chen, and Qiang Yang. Secure federated matrix factorization. _IEEE Intelligent Systems_, 36(3):11-20, 2020.
* (23) Cukhan Wu, Pingzhua Wu, Yang Chen, Yongfeng Huang, and Xing Xie. Fedgms: Federated graph neural network for privacy-preserving recommendation. _arXiv preprint arXiv:1204.04293_, 2021.
* (24) Zhiwei Liu, Liangwei Zhang, Zijin Fan, Hao Peng, and Philip S Yu. Federated social recommendation with graph neural network. _ACM Transactions on Intelligent Systems and Technology (TIST)_, 13(4):1-24, 2022.
* (25) Chaochao Chen, Jun Zhou, Longfei Zheng, Huiwen Wu, Lingjuan Lyu, Jia Wu, Bingzhe Wu, Ziqin Lu, Li Wang, and Xiaolin Zheng. Vertically federated graph neural network for privacy-preserving node classification. _arXiv preprint arXiv:2005.1993_, 2020.
* (26) Xiang Ni, Xiaolong Xu, Linguian Lyu, Changhua Meng, and Weiqiang Wang. A vertical federated learning framework for graph convolutional network. _arXiv preprint arXiv:2105.15793_, 2021.
* (27) Peihua Mai and Yan Pang. Vertical federated graph neural network for recommender system. _arXiv preprint arXiv:2103.05765_, 2023.
* (28) Neily Faraju, Rosario Gemameo, Tiberla Jafarshikh, and William E Sheith. Homomorphic secret sharing from pallier encryption. In _Provable Security 11th International Conference, Procekoc 2017, Xi'an, China, October 23-25, 2017, Proceedings_, 11 pages 381-399. Springer, 2017.
* (29) Minsen Du, Xiang Yue, Sherman S Chu, Tianjua Wang, Chenyu Huang, and Kun Du. Dp-forum: Fine-tuning and inference on mongae models with differential privacy in forward pass. In _Proceedings of the 2023 ACM SIGSAC Conference on Computer and Communications Security_, pages 2645-2679, 2023.
* (30) Jungang Tang, Lyu Xiong, Weiqing Li, Wei Liu, and Xinbing Wang. Improved matrix gaussian mechanism for differential privacy. 2021.
* (31) Xiangzhan He, Kun Deng, Xiang Wang, Yan Li, Yonglong Zhang, and Meng Wang. Lightgcn: Simplifying and powering graph convolution network for recommendation. In _Proceedings of the 46th International ACM SIGIR conference on research and development in Information Retrieval_, pages 639-648, 2020.
* (32) Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. _Foundations and Trends in Theoretical Computer Science_, 9(5):421-407, 2014.
* (33) G. Gao, J. Zhang, and N. Yorke-Smith. A novel bayesian similarity measure for recommender systems. In _Proceedings of the 23rd International Joint Conference on Artificial Intelligence (IJCAI)_, pages 2691-2625, 2015.
* (34) G. Gao, J. Zhang, D. Talman, and N. Yorke-Smith. Eaf: An extended trust antecedents framework for trust prediction. In _Proceedings of the 2014 International Conference on Advances in Social Networks Analysis and Mining (ASONAM)_, pages 540-547, 2014.
* (35) Federico Monti, Michael Bronstein, and Xavier Bresson. Geometric matrix completion with recurrent multi-graph neural networks. _Advances in neural information processing systems_, 30, 2017.
* (36) Paola Massa and Paolo Massa. Trust-aware recommender systems. In _Proceedings of the 2007 ACM conference on Recommender systems_, pages 17-24, 2007.
* (37) Xiangzen He, Liu Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chiu. Neural collaborative filtering. In _Proceedings of the 26th international conference on world wide web_, pages 173-182, 2017.
* (38) Joan Bruna, Wojciken Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally connected networks on graphs. _arXiv preprint arXiv:1312.6208_, 2013.
* (39) Pascal Pallier. Public-key cryptosystems based on composite degree residuosity classes. In _Advances in Cryptology-EUROCRY symposium: International Conference on the Theory and Application of Cryptographic Techniques Prague, Czech Republic, May 2-6, 1999 Proceedings 18_, pages 223-258. Springer, 1999.
* (40)

## Appendix A Derivations

### The derivation of the upper bounds of \(\ell_{2}\) sensitivity

We denote the adjacent databases by \(\mathbf{A}\) and \(\mathbf{A}^{\prime}\) where \(\mathbf{A}^{\prime}_{\mathit{EM}}=1-A_{\mathit{km}}\). And other elements of the two matrix are the same. The \(k\)th row in the \(\hat{\mathbf{l}}_{\mathit{sigm}}\) of \(\mathbf{A}\) is \(\mathbf{l}_{\mathit{k}}\) (e.g., \(\mathbf{l}^{\prime}_{\mathit{k}}\) for \(\mathbf{A}^{\prime}\)). Letting \(d_{j}=\sqrt{\|\mathbf{a}_{j}\|_{1}}\), then we have

\[\|l^{\prime}_{k}-l_{k}\|_{2}^{2}=\|\left[\frac{a_{kj}}{d_{k}d_{j}},\cdots,\frac{a_{km}}{d_{k}d_{m}},\cdots\right]-\left[\frac{a_{kj}}{d^{\prime} _{k}d_{j}},\cdots,\frac{1-a_{km}}{d^{\prime}_{k}d_{m}},\cdots\right]\|_{2}^{2}\] \[=\|\frac{a_{k}-d^{\prime}_{k}}{d_{k}d^{\prime}_{k}}\left[\frac{a_ {kj}}{d_{j}},\cdots,\frac{1-a_{km}}{d_{m}}\frac{d_{k}}{d_{k}-d^{\prime}_{k}}- \frac{a_{km}}{d_{m}}\frac{d^{\prime}_{k}}{d_{k}-d^{\prime}_{k}},\cdots\right]\| _{2}^{2}\] \[=(\frac{d_{k}-d^{\prime}_{k}}{d_{k}d^{\prime}_{k}})^{2}\left(\sum _{j=1}^{N}\frac{a_{kj}^{2}}{\|\mathbf{a}_{j}\|_{1}}-\frac{a_{km}^{2}}{d_{m}^{2 }}+\frac{\left((1-a_{km})d_{k}-a_{km}d^{\prime}_{k}\right)^{2}}{d_{m}^{2}(d_{k }-d^{\prime}_{k})^{2}}\right)\] \[=(\frac{d_{k}-d^{\prime}_{k}}{d_{k}d^{\prime}_{k}})^{2}\left(\sum _{j=1}^{N}\frac{a_{kj}}{\|\mathbf{a}_{j}\|_{1}}+\frac{\left((1-a_{km})d_{k}-a_ {km}d^{\prime}_{k}\right)^{2}-a_{km}^{2}(d_{k}-d^{\prime}_{k})^{2}}{\|\mathbf{a }_{m}\|_{1}(d_{k}-d^{\prime}_{k})^{2}}\right)\] \[=(\frac{d_{k}-d^{\prime}_{k}}{d_{k}d^{\prime}_{k}})^{2}\left(\sum _{j=1}^{N}\frac{a_{kj}}{\|\mathbf{a}_{j}\|_{1}}+\frac{\left(\frac{(1-a_{km})d_ {k}-a_{km}d^{\prime}_{k}}{d_{k}-d^{\prime}_{k}}\right)^{2}-a_{km}^{2}}{\| \mathbf{a}_{m}\|_{1}}\right)\] \[\leq(\frac{d_{k}-d^{\prime}_{k}}{d_{k}d^{\prime}_{k}})^{2}(\| \mathbf{a}_{k}\|_{1}+\frac{\|\mathbf{a}_{k}\|_{1}+a_{km}(1-2a_{km})}{\| \mathbf{a}_{m}\|_{1}})\] \[\leq\frac{1}{\|\mathbf{a}_{k}\|_{1}^{2}+\|\mathbf{a}_{k}\|_{1}}c _{k}+\frac{1}{\|\mathbf{a}_{k}\|_{1}}c_{o} \tag{11}\]

where \(c_{k}=\sum_{j=1}^{N}\frac{a_{kj}}{\|\mathbf{a}_{j}\|_{1}}\leq\|\mathbf{a}_{k} \|_{1},c_{o}=\max_{m}\frac{1}{\|\mathbf{a}_{m}\|_{1}+1}\leq 1\). Then, we can obtain Eq.4.3.2 by replacing \(d_{k}\) with its definition.

### Proof of Theorem 4.1

**Theorem A.1**.: _Given \(\mathbf{J}=\mathbf{L}\mathbf{M}\mathbf{N}\) where all matrices are not zero matrices, there exists infinite combinations of \(\mathbf{N}^{\prime}\neq\mathbf{N},\mathbf{L}^{\prime}\neq\mathbf{L}\mathbf{N}\) such that \(\mathbf{J}=\mathbf{L}^{\prime}\mathbf{M}\mathbf{N}^{\prime}\)._

Proof.: Given \(\mathbf{J}=\mathbf{L}\mathbf{M},\mathbf{L}\in\mathbb{R}^{p\times q},\mathbf{M} \in\mathbb{R}^{q\times r},\mathbf{N}\in\mathbb{R}^{r\times s}\), we have

\[rank(\mathbf{L}\mathbf{M})=rank([\mathbf{L}\mathbf{M};\mathbf{J}]) \tag{12}\]

Now we consider the equation

\[(\mathbf{L}^{\prime}\mathbf{M})\mathbf{X}=\mathbf{J},\mathbf{X}\in\mathbb{R}^ {r\times s},\mathbf{L}\neq\mathbf{L}^{\prime} \tag{13}\]

As long as equation (13) is solvable, then we can directly set \(\mathbf{N}^{\prime}\) to be the solver \(\mathbf{X}\), leading to the establishment of \(\mathbf{J}=\mathbf{L}^{\prime}\mathbf{M}\mathbf{N}^{\prime}\). Therefore, to make the equation (13) solvable, we must establish the following equation

\[rank(\mathbf{L}^{\prime}\mathbf{M})=rank([\mathbf{L}^{\prime}\mathbf{M}; \mathbf{J}])\]

Without loss of generality, we denote \(\mathbf{L}^{\prime}=\mathbf{L}+\Delta\mathbf{L}\). We now introduce a way to choose \(\mathbf{L}^{\prime}\) without changing \(rank([\mathbf{L}^{\prime}\mathbf{M}])\).

\[\mathbf{L}^{\prime}\mathbf{M}=\mathbf{L}\mathbf{M}+\Delta\mathbf{L}\mathbf{M} \tag{14}\]

By setting \(\Delta\mathbf{L}\) as

(15)

we can obtain that

(16)

The influence of \(\Delta\mathbf{Z}\) on the rank can be easily eliminated by setting a small enough value of \(\delta_{11}\). In this way, the rank of \(\mathbf{Z}=\mathbf{L}\mathbf{M}\) is preserved as

(17)

from which we can immediately infer that there exists at least a solver \(\mathbf{X}\) such that \(\mathbf{L}^{\prime}\mathbf{M}=\mathbf{J}\). Note that the choice of the position of value changing is not necessary to be specified to (1,1) and the number of changes is also not limited, there will thus be an infinite number of \(\Delta\mathbf{L}\) that can be the alternative one, leading to the infinite number of combinations of \(\mathbf{L}^{\prime},\mathbf{N}^{\prime}\). The distance between \(\mathbf{L}^{\prime}\) and \(\mathbf{L}\) can be arbitrarily decided by choosing \(\mathbf{L}^{\prime}\gets r\mathbf{L}^{\prime},\mathbf{N}^{\prime} \leftarrow\frac{1}{N}\mathbf{N}^{\prime},r\in\mathbb{R}\) and \(r\neq 0\)

## Appendix B The architecture of P4GCN

The architecture of P4GCN is shown in Table 5. During each iteration, the party \(\mathcal{P}_{1}\) first inputs the batch data (e.g. the batched users' features \(\mathbf{X}^{(0)}_{\mathit{user,B}}\) and the items' features \(\mathbf{X}^{(0)}_{\mathit{item}}\)) and the user-item graph into the local aggregation GC layer to obtain \(\mathbf{X}^{(1)}_{\mathit{user,B}}\) and \(\mathbf{X}^{(1)}_{\mathit{item}}\). Then, \(\mathcal{P}_{1}\) uses sandwich encryption to make the social aggregation on users' features with \(\mathcal{P}_{2}\) to obtain \(\mathbf{X}^{(2)}_{\mathit{user,B}}\). \(\mathcal{P}_{1}\) further fuses the two types of users' embeddings together by the fusion layer. Concretely, for each user \(u_{i}\) in the current batch, its fusion of embeddings is \(\mathbf{x}^{(3)}_{u_{i}}=[\mathbf{x}^{(1)\top}_{\mathit{user,B}}|\mathbf{x}^{(2) \top}_{\mathit{user,B}u_{i}}]^{\top}\odot(\mathbf{W}_{\mathit{fusion,us}}[ \mathbf{x}^{(1)\top}_{\mathit{user,B}u_{i}}]^{\top})\in\mathbb{R}^{2d}\). Finally, both the items' embeddings \(\mathbf{X}^{(1)}_{\mathit{item}}\) and the users' embeddings \(\mathbf{X}^{(3)}_{\mathit{user,B}}=[\mathbf{x}^{(3)}_{u_{1}},\ldots,\mathbf{x}^{(3 )}_{\mathit{NB}}]\) will be input into the decoder to predict the rating \(\hat{f}_{u,0}=4*sigmoid(Relu\left([\mathbf{x}^{(3)\top}_{\mathit{user,B}}| \mathbf{x}^{(1)}_{\mathit{item,B}}|\mathbf{W}_{\mathit{mlp}1}\right)\mathbf{W}_{ \mathit{mlp}2}]\)

The LightGCN used in our experiments shares the same architecture as our P4GCN but without the fusion layer. We directly cat the users' embeddings \(\mathbf{X}^{(1)}_{\mathit{user,B}}\) and the items' embeddings \(\mathbf{X}^{(1)}_{\mathit{item}}\)

\begin{table}
\begin{tabular}{l|l} \hline
**LayerName** & **Parameter** \\ \hline Local Agg. Weight & - \\ Social Agg. Weight & \(\mathbf{W}_{1}\in\mathbb{R}^{d\times d}\) and \(\mathbf{W}_{2}\in\mathbb{R}^{d\times d}\) \\ Fusion Layer & \(\mathbf{W}_{\mathit{fusion1}}\in\mathbb{R}^{2d\times 2d},\mathbf{W}_{\mathit{fusion2}}\in\mathbb{R}^{2d\times 2d}\) \\ Decoder & - \\ \\ \hline \end{tabular}
\end{table}
Table 5. Parameters of layers in P4GCNtogether and then input them into the two-layer decoder to obtain the prediction.

## Appendix C Homomorphic encryption

### Paillier algorithm

Paillier is a public-key cryptosystem that supports additive homomorphism [39]. The main steps of the Paillier algorithm are key generation, encryption, and decryption.

Key generationFirst randomly selects two large prime numbers \(p\) and \(q\) that satisfy the formula \(gcd(pq,(p-1)(q-1))=1\), and \(p\), \(q\) are equal in length. Then we calculate \(n=pq\) and \(\lambda=lcm(p-1,q-1)\). Second, randomly selection of integer \(g\in Z_{n^{2}}^{*}\) and define function \(L\) as \(L(x)=\frac{x-1}{n}\) and calculate \(\mu=\left(L\left(g^{\lambda}\bmod n^{2}\right)\right)^{-1}\bmod n\). Finally, we get private key \((n,g)\) and public key \((\lambda,\mu)\).

EncryptionFirst input the plaintext \(m\) satisfies \(0\leq m\leq n\). Then choose a random number \(r\) that satisfies \(r\in Z_{n}^{*}\). Finally, we calculate the ciphertext as \(c=g^{m}r^{n}\bmod n^{2}\).

DeryptionInput ciphertext \(c\) that satisfies \(c\in Z_{n^{2}}^{*}\), and then calculate the plaintext message as \(m=L\left(c^{\lambda}\bmod n^{2}\right)\cdot\mu\bmod n\)

## Appendix D Limitation and broader impact

This work introduces a way to leverage user's social data to improve the recommendation system on the company view. One limitation lies in that we only discuss the method on GCN operator. And we plan to extend this work to other operators like graph attention as our future work.