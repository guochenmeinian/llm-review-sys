# Responsible Diffusion Models via Constraining Text Embeddings within Safe Regions

Anonymous Author(s)

###### Abstract.

The remarkable ability of diffusion models to generate high-fidelity images has led to their widespread adoption. However, concerns have also arisen regarding their potential to produce Not Safe for Work (NSFW) content and exhibit social biases, impeding their practical use and progress in real-world applications. In response to this challenge, prior work has primarily focused on employing security filters to identify and subsequently exclude toxic text, or alternatively, fine-tuning pre-trained diffusion models to erase sensitive concepts. Unfortunately, existing methods struggle to achieve satisfactory performance in the sense that they can have a significant impact on the normal model output while still failing to prevent the generation of harmful content in some cases. In this paper, we propose a novel self-discovery approach to identifying a semantic direction vector in the embedding space to restrict text embedding within a safe region. Our method circumvents the need for correcting individual words within the input text and steers the entire text prompt towards a safe region in the embedding space, thereby enhancing model robustness against all possibly unsafe prompts. In addition, we employ a Low-Rank Adaptation (LoRA) for semantic direction vector initialization to reduce the impact on the model performance for other semantics. Furthermore, our method can also be integrated with existing methods to improve their socially responsible performance. Extensive experiments on benchmark datasets demonstrate that our method can effectively reduce NSFW content and mitigate social bias generated by diffusion models compared to several state-of-the-art baselines.

WARNING: This paper contains model-generated images that may be offensive in nature.

Anonymous Author(s). 2024. Responsible Diffusion Models via Constraining Text Embeddings within Safe Regions. In. ACM, New York, NY, USA, 15 pages. [https://doi.org/10.1145/mnmnmn.nnnnnnnn](https://doi.org/10.1145/mnmnmn.nnnnnnnn)

## 1. Introduction

Recently, large-scale text-to-image diffusion models (Zhu et al., 2017; Wang et al., 2018) have attracted much attention due to their ability to generate photo-realistic images based on textual descriptions. However, considerable concerns about these models also arise because their generated content has been found to be possibly unsafe and biased, containing pornographic and violent content, gender discrimination, or racial prejudice (Bengio et al., 2018; Wang et al., 2018).

There have been two common types of approaches employed to address such concerns. One class of methods involves integrating some external safety validation mechanisms (Zhu et al., 2017; Wang et al., 2018; Wang et al., 2018), which harness classifiers to detect toxic input from users and reject them, with diffusion models. However, these mechanisms might be unreliable, as some prompt texts that do not explicitly contain Not Safe for Work (NSFW) content can still result in images with such content. Taking the Stable Diffusion (SD) model as an example, the prompt "a beautiful woman" may lead to the generation of an image of a nude woman (Wang et al., 2018).

The other class of approaches seeks to construct more responsible diffusion models by training data cleaning, parameter fine-tuning, model editing, or intervention in the inference process. A naive method (Wang et al., 2018) is to filter out inappropriate content from the training data of diffusion models to prevent them from internalizing such content. Although effective, retraining models on new datasets can be computationally intensive and often leads to performance degradation (Zhu et al., 2017). Therefore, more efforts have been made to fine-tune parameters so that models can "forget" undesirable concepts (Bengio et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018). However, the catastrophic forgetting problem can potentially arise when fine-tuning parameters. Meanwhile, another line of studies (Wang et al., 2018; Wang et al., 2018) seeks to selectively edit certain parameters of pre-trained models to construct a responsible image generation model. But these methods typically tailor the projection matrix within the cross-attention layers to specific target words, thus yielding suboptimal outcomes for other related but non-targeted words. Finally, a few methods (Wang et al., 2018; Wang et al., 2018) leverage the principle of classifier-free guidance. They directly modify the denoising process of the original model to steer away from inappropriate content. Although these methods refrain from updating the model parameters, they may still impact the semantics of the original image and introduce additional overhead during the inference process. In summary, despite the fact that the above methods are effective to some extent, there are still considerable gaps in ensuring the responsibility of diffusion models.

In this paper, we endeavor to address the problem of responsible text-to-image generation using diffusion models from a different perspective. Generally, our approach focuses on manipulating the input text embedding to avoid generating inappropriate content. As the encoded text prompt is fed into the U-Net as a condition and plays a critical role in the image generation process, it can be used to identify the global semantic direction related to a certain conceptin the embedding space. Accordingly, this direction can restrict the text embedding to a specific "safe region", reducing the generation of harmful content in various contexts beyond the token level.

We note that previous prompt-tuning methods (Chen et al., 2018; Wang et al., 2019; Wang et al., 2019) have also attempted to train one or more pseudo-tokens in the CLIP embedding space in a supervised manner. Nevertheless, these pseudo-tokens are designed to symbolize specific concepts and are discontinuous in nature. The pseudo-tokens, along with the other words in the text prompt, are jointly encoded by the CLIP model before input into the U-Net for image generation. These approaches still operate at the token level to instruct the model in generating corresponding images. However, each token in the text prompt will contain information from other tokens. Therefore, attempting to encapsulate a concept such as "safety" within a single token typically fails to produce desirable outcomes. In the computational linguistics domain, some methods such as prefix-tuning (Zhu et al., 2019) consider generating continuous pseudo-tokens for specific tasks. Although these tokens are continuous, they are only used as a prefix added to the beginning of the input sentence to guide the language model in the autoregressive process. This differs from our goal of directing the embedding of the entire input text to a specific region. At the same time, the performance of the trained pseudo-tokens largely depends on the quality of the training data or the accuracy of the classifier. In general, existing prompt-tuning methods cannot be easily applied to prevent the generation of unsafe content and mitigate social biases within diffusion models.

Toward this end, as shown in Figure 1, we propose a novel self-discovery approach to identify the semantic direction in the embedding space, thus constraining the text prompt embedding within the safe region. Specifically, we utilize the classifier-free guidance technique that leverages internal knowledge of the diffusion model to learn a semantic direction vector. Then, we use a low-rank direction vector to strengthen the semantic representation. In this way, the semantic direction vector can guide the original text prompt to move to a specific region in the embedding space. This movement is confined solely to the specific semantic dimension, ensuring that semantics in other dimensions remain unaffected. In simple terms, we leverage the diffusion model as an implicit classifier to get the noise estimate that is close to or far away from a concept during the denoising process. The direction vector learns the corresponding semantic information by minimizing the \(l_{2}\)-loss of the predicted noise and the noise estimate of this implicit classifier. To achieve responsible generation, we learn semantic direction vectors related to unsafe concepts and social bias. For safe generation, we learn a safe vector that can guide the text prompt away from inappropriate content to eliminate the generation of unsafe images. For fair generation, we learn a concept-related direction vector that can guide the input text prompt to a certain concept (e.g., gender and race). Extensive experiments on the widely used benchmark datasets demonstrate that our approach substantially reduces NSFW content generation and mitigates the social bias inherent in the stable diffusion model. Our contributions are summarized as follows:

* We propose a novel self-discovery approach to identify the specific semantic direction vector in the embedding space. Our approach effectively guides unsafe text prompts to a safe region within the embedding space, whether or not these texts contain explicit toxic content. In addition, our approach is effective in reducing multiple types of inappropriate concepts simultaneously, including pornography, violence, societal bias, etc.
* We employ a low-rank direction vector to learn a more precise semantic direction while reducing the impact on model performance regarding other semantics. Furthermore, we show that multiple semantic vectors can be linearly combined to exert influence, and our approach can seamlessly integrate with existing methods to enhance their responsibility in image generation.
* We conduct extensive experiments on benchmark datasets to demonstrate that our approach is capable of effectively suppressing the generation of inappropriate content and mitigating potential societal biases in diffusion models compared to several state-of-the-art baselines.

## 2. Background and Related Work

In this section, we introduce the background of diffusion models and discuss existing methods to improve the responsibility of diffusion models for image generation.

**Diffusion Models:** Currently, most text-to-image generative models are Latent Diffusion Models (LDMs) (Wang et al., 2019). They utilize pre-trained variational autoencoders (He et al., 2016) to encode images into a latent space, where noise addition and removal processes are conducted. Specifically, the forward process takes each clean image \(\mathbf{x}\) as input, encodes it as a latent image \(\mathbf{z}_{0}\), and then adds Gaussian noise of varying intensities to \(\mathbf{z}_{0}\). At each time step \(t\in[0,T]\), the latent noisy image \(\mathbf{z}_{t}\) is indicated by \(\sqrt{\alpha_{t}}\mathbf{z}_{0}+\sqrt{1-\alpha_{t}}\epsilon\), where \(\alpha_{t}\) signifies the strength of Gaussian noise \(\epsilon\), gradually decreasing with time steps. The final latent noisy image is denoted as \(\mathbf{z}_{T}\sim\mathcal{N}(0,I)\). Then, the reverse process trains the model to predict and remove the noise from the latent image, thereby restoring the original image. At each time step \(t\), the LDM predicts the noise added to the noisy latent image \(\mathbf{z}_{t}\) under the text condition \(c\), represented as \(\epsilon_{\theta}(\mathbf{z}_{t},c,t)\). The loss function is expressed as:

\[\mathcal{L}=\mathbb{E}_{\mathbf{z}_{t}\in\mathcal{E}(\mathbf{x}_{t}),\mathcal{ L},\mathcal{E}\sim\mathcal{N}(0,I)}\left[\|e-\epsilon_{\theta}\left(\mathbf{z}_{t},c,t \right)\|_{2}^{2}\right], \tag{1}\]

where \(\mathcal{E}(\cdot)\) is an image encoder.

In the inference stage, an LDM typically employs the classifier-free guidance technique (He et al., 2016), which utilizes an implicit classifier to guide the process, thereby avoiding the explicit use of classifier

Figure 1. Intuitive illustration of our method that utilizes the disparities in diffusion noise distribution to identify semantic directions in the CLIP embedding space to guide the generation process and avoid inappropriate content.

gradients. To obtain the final noise for inference, an LDM adjusts towards conditional scores while moving away from unconditional scores by utilizing a guidance scale \(\alpha\) as follows:

\[\tilde{\epsilon}_{\theta}\left(\mathbf{z}_{t},\mathbf{c},t\right)=\epsilon_{ \theta}\left(\mathbf{z}_{t},t\right)+\alpha\left(\epsilon_{\theta}\left(\mathbf{ z}_{t},\mathbf{c},t\right)-\epsilon_{\theta}\left(\mathbf{z}_{t},t \right)\right). \tag{2}\]

**Responsible Diffusion Models:** Different methods have been proposed to address social biases within diffusion models and mitigate the generation of unsafe content. A straightforward approach is to construct a fair and clean dataset by filtering out unsafe content and retrain a diffusion model using the new dataset (Shen et al., 2017). However, the training datasets and the parameters of diffusion models can be very large. As such, data filtering and model retraining often incur high overheads. Moreover, some studies (Shen et al., 2017) also indicated that this may lead to significant performance degradation. To avoid retraining from scratch, fine-tuning approaches (Shen et al., 2017; Chen et al., 2017; Wang et al., 2017; Wang et al., 2017) were proposed to address safety and fairness issues in diffusion models. Shen et al. (Shen et al., 2017) treated fairness enhancement as a distribution alignment problem and proposed a biased direct approach to fine-tuning the diffusion model. Fan et al. (Fan et al., 2017) identified key model parameters using a gradient-based weight saliency method and fine-tuned them to make the model forget sensitive concepts. Gandikota et al. (Gandikota et al., 2017) used a distillation method to fine-tune the parameters of the cross-attention layer in the diffusion model to remove a certain concept. Lyu et al. (Lyu et al., 2017) used a one-dimensional adapter to learn the erasure of a specific concept rather than fine-tuning all the model parameters. In addition, they used the similarity between the input prompt and the erased concept as a coefficient to determine the extent of erasure. As a result, the effectiveness of the method is reduced when the input prompt does not include the concept intended for erasure. Although fine-tuning methods can make the model safer and fairer with small training costs, they may cause catastrophic forgetting problems, leading to unpredictable consequences (Li et al., 2017; Li et al., 2017).

To further overcome the problems caused by fine-tuning, several recent studies aimed to achieve responsible generation using model editing. As non-training methods, they attempt to edit specific knowledge embeddings in the model according to user needs to adapt to new rules or produce new visual effects. Arad et al. (Artal et al., 2017) and Orgad et al. (Orgad et al., 2017) changed the internal knowledge of a diffusion model by editing the cross-attention layer or the weight matrix in the text encoder. Gandikota et al. (Gandikota et al., 2017) mapped sensitive concept words onto appropriate concept features by modifying the projection matrix of the cross-attention layer. Other methods (Chen et al., 2017; Chen et al., 2017; Chen et al., 2017; Wang et al., 2017) focused mainly on modifying the input text to avoid generating inappropriate images. They generally suppress certain unsafe words in the input prompt or modify the embedding after prompt encoding. However, text-based model editing approaches are very limited because secure prompts may still generate unsafe images (Shen et al., 2017). Moreover, listing all possible unsafe and biased words is infeasible. Note that our method in this paper also operates in the prompt embedding space. However, our method does not target specific words, thus circumventing such limitations.

Finally, another line of methods suppresses the generation of inappropriate content by intervening in the diffusion denoising process. Schramowski et al. (Shen et al., 2017) used classifier-free guided techniques to modify the noise space during the denoising process to remove harmful content. This kind of methods does not require training and is based merely on the model, but directly interfering with the diffusion process is not controllable. In this paper, we use a conditional reflex strategy similar to that of (Shen et al., 2017) to find semantic vectors. We find the semantic vector in the CLIP embedding space through the noise difference in the diffusion process and directly apply it to the prompt embedding during inference. A recent study (Shen et al., 2017) used similar ideas as ours, which adopt a negative prompting method to generate images that are far away from unsafe content and use these images to train a safe semantic vector in the U-Net bottleneck layer. This method changes the image output by perturbing the semantic space found in diffusion (Orgad et al., 2017; Wang et al., 2017). However, it requires a large number of images for training and, additionally, uses the pixel reconstruction loss that may introduce background noise in the image, resulting in lower generation quality.

## 3. Method

In this section, we describe our proposed method in detail. We first introduce how to utilize optimization techniques within the denoising process of the diffusion model to identify the required semantic vectors in the prompt embedding space. Then, we show the low-rank adaptation (LoRA) based semantic vector initialization method we employ. Finally, we explain how the identified semantic vectors are used for safe and fair generation tasks.

### Latent Region Anchoring

We observe that the feature representations in the CLIP embedding space can often be regarded as linear. Intuitively, there are many semantic directions in the embedding space, where moving a sample's features in one direction can yield another feature representation with the same class label but different semantics. Our method aims to identify a semantic direction that translates the original text embedding into a feature with safe semantics. However, finding such a semantic direction is not trivial and may require collecting a significant amount of labeled data. We first propose an intuitive

Figure 2. Examples of using two contrasting prompts to identify specific semantic directions. The images in each column are generated with the same prompt and seed.

approach whereby the semantic direction of relevant attributes is discerned through the disparity in the embeddings derived from two contrasting prompts (such as "a person wearing clothes" vs. "a person" for the "pornographic" attribute). Therefore, the direction vector \(d\) can be obtained as follows:

\[d=\mathcal{E}_{\text{CLIP}}(\text{prompt}_{+})-\mathcal{E}_{\text{CLIP}}(\text{ prompt}_{-}), \tag{3}\]

where \(\mathcal{E}_{\text{CLIP}}\) is the CLIP text encoder and \(\text{prompt}_{+}\) denotes a text prompt containing relevant attributes, whereas \(\text{prompt}_{-}\) does not. Once the direction vector \(d\) is acquired, we can constrain the input text embedding within a region by either adding or subtracting this direction vector. This process serves to guide images towards or away from the respective attribute. Figure 2 illustrates that the direction vector discovered through this approach indeed affects the relevant attributes such as nudity and gender but also affects other attributes, leading to significant disparities from the original images. This naive approach makes it difficult to obtain highly precise semantic directions. Next, we will introduce an optimization-based approach to learn a more precise direction vector.

Inspired by recent studies (Wang et al., 2019; Wang et al., 2019), we employ a reflexive strategy similar to moving away from or towards certain concepts to find the direction vector. Specifically, we focus on the stable diffusion model with parameters \(\theta\). In our goal of identifying a particular direction vector \(d\), we first need the base prompt used for training. Then, we use a target concept \(c_{0}\) that we aim to move toward or move away from. For example, if we want to find a direction vector toward "male", we can set \(\mathbf{c}=\) "a person" and \(\mathbf{c}_{0}=\) "a male person". Our goal is to generate an image related to the target concept by adding the direction vector to the base prompt. Consider the following implicit classifier:

\[p_{\theta}(\mathbf{c}_{0}|\mathbf{z}_{t})=\frac{p_{\theta}(\mathbf{z}_{t}| \mathbf{c}_{0})p_{\theta}(\mathbf{c}_{0})}{p_{\theta}(\mathbf{z}_{t})}, \tag{4}\]

where \(p_{\theta}(\mathbf{c}_{0})\) is a categorical distribution, with the assumption that this is a uniform distribution by default. Therefore, we can derive the following equation:

\[p_{\theta}(\mathbf{c}_{0}|\mathbf{z}_{t})\propto\frac{p_{\theta}(\mathbf{z}_{t }|\mathbf{c}_{0})}{p_{\theta}(\mathbf{z}_{t})}, \tag{5}\]

where \(p_{\theta}\) represents the data distribution generated by the diffusion model and \(\mathbf{z}_{t}\) is the latent noise image at time step \(t\). According to classifier-free guidance (Kumar et al., 2018), the gradient of this classifier can be written as:

\[\begin{split}\nabla\log p_{\theta}(\mathbf{c}_{0}|\mathbf{z}_{t}) &=\nabla\log p_{\theta}(\mathbf{z}_{t}|\mathbf{c}_{0})-\nabla\log p _{\theta}(\mathbf{z}_{t})\\ &=-\frac{1}{\sqrt{1-\alpha_{t}}}(\epsilon_{\theta}\left(\mathbf{z }_{t},\mathbf{c}_{0},t\right)-\epsilon_{\theta}\left(\mathbf{z}_{t},t\right)). \end{split} \tag{6}\]

Using the gradient of this implicit classifier for guidance (Beng et al., 2019), we obtain the noise estimate \(\tilde{\epsilon}_{\theta}^{+}(\mathbf{z}_{t},\mathbf{c}_{0},t)=\epsilon_{\theta }(\mathbf{z}_{t})+w(\epsilon_{\theta}(\mathbf{z}_{t},\mathbf{c}_{0},t)- \epsilon_{\theta}(\mathbf{z}_{t},t))\), where \(w\) represents the coefficient of guiding strength. Similarly, we also get the noise estimate \(\tilde{\epsilon}_{\theta}^{-}(\mathbf{z}_{t},\mathbf{c}_{0},t)=\epsilon_{\theta }(\mathbf{z}_{t})-w(\epsilon_{\theta}(\mathbf{z}_{t},\mathbf{c}_{0},t)-\epsilon _{\theta}(\mathbf{z}_{t},t))\) if we want to steer the image away from the target concept. Note that during training, the text condition used for iterative denoising is \(\epsilon+d\), so the predicted noise is \(\epsilon_{\theta}(\mathbf{z}_{t},c+d,t)\). By minimizing the distance between \(\epsilon_{\theta}(\mathbf{z}_{t},c+d,t)\) and \(\tilde{\epsilon}_{\theta}(\mathbf{z}_{t},\mathbf{c}_{0},t)\), we can find a direction vector that steers the image towards or away from the target concept. Formally, the optimal direction vector \(d^{*}\) for the given concepts is:

\[d^{*}=\arg\min_{d}\sum_{c\sim\mathcal{D}}\sum_{t\sim\{0,T\}}\|\epsilon_{\theta }(\mathbf{z}_{t},c+d,t)-\psi(\mathbf{z}_{t},\mathbf{c}_{0},t)\|^{2}, \tag{7}\]

where \(\mathcal{D}\) is a set of base prompts that includes \(m\) same concepts such as "a person", \(\psi\) depends on whether to move towards or away from the target concepts:

\[\psi(\mathbf{z}_{t},\mathbf{c}_{0},t)=\begin{cases}\tilde{\epsilon}_{\theta}^{+}(\mathbf{z}_{t},\mathbf{c}_{0},t)&\text{if towards}\\ \tilde{\epsilon}_{\theta}^{-}(\mathbf{z}_{t},\mathbf{c}_{0},t)&\text{if away from}\end{cases}. \tag{8}\]

Figure 3 illustrates the optimization process of our method. Unlike common methods that sample discrete time steps for training, the optimization process occurs during the iterative denoising process, where the optimization at time step \(t\) will affect the result at time step \(t-1\). We optimize the same direction vector for each time step. This choice is motivated by the fact that the text condition is applied at every diffusion step during image generation. Furthermore, this method of dynamically adjusting while simultaneously inspecting during generation aligns with intuition.

Figure 3. Illustration of the optimization process to find a direction vector associated with the target concept in the CLIP embedding space. The noise distribution close to or far away from the target concept is obtained through the frozen pre-trained diffusion model. The \(l_{2}\)-loss between \(\epsilon_{\theta}\left(\mathbf{z}_{t},c+d,t\right)\) and \(\psi(\mathbf{z}_{t},\mathbf{c}_{0},t)\) in Eq. 8 at each step \(t\) makes the noise predicted by the base prompt with the direction vector added close to the noise distribution. The updated direction vector \(d^{\prime}\) are used in the next step of denoising, and the precise direction is learned in the iterative denoising process.

### LoRA-based Direction Vector Initialization

The text prompt is encoded into a prompt embedding \(P_{\text{c}}\in\mathbb{R}^{L\times D}\) through a CLIP text encoder, where \(L\) is the number of tokens and \(D\) is the dimension of the model for token embedding. In the stable diffusion model, we always have \(L=77\) and \(D=768\). Typically, it is recommended to initialize the direction vector \(d\in\mathbb{R}^{L\times D}\) with the same shape as the prompt embedding. However, we find that this approach often results in distorted and warped images. We speculate that this is due to the sensitivity of the embedding space, where even subtle perturbations can have significant impacts on the final results. To address this issue, we adopt Low-Rank Adaptation (LoRA) (Kumar et al., 2017), which uses low-rank decomposition to represent parameter updates, for direction vector initialization. During training, the LoRA matrix can selectively amplify features relevant to downstream tasks, rather than the primary features present in the pre-trained model. The search for a direction vector in the embedding space can also be viewed as fine-tuning for downstream tasks. As such, using a low-rank direction vector can help us identify a more precise direction. Specifically, to amplify the features of the target direction, we initialize the direction vector as \(d=BA\), where \(B\in\mathbb{R}^{L\times 1}\) with all 0's and \(A\in\mathbb{R}^{1\times D}\) drawn randomly from a normal distribution. We illustrate the two methods for initializing direction vectors in Figure 4. More results for the comparison of the two methods can be found in Appendix C.1.

### Responsible Generation

**Safe Generation.** We perform safe generation by guiding text prompts that contain explicit or implicit unsafe content to prevent inappropriate content. Specifically, we learn the opposite direction of an unsafe concept, using our training method to move away from a specific concept. Our method does not pay attention to the intermediate images generated during training, only to the noise distribution in the intermediate process, so when we set the base prompt, there is no need to guarantee that this prompt will generate reasonable images. For example, in safe generation, the base prompt should contain unsafe content. The base prompt \(c\) can be "an image of nudity", and the target concept \(c_{o}\) "nude", so we can learn a direction vector that guides the input text prompt away from the concept of "nude", regardless of whether the input text prompt contains the word "nude". The reason for adopting this strategy is that it is difficult to list all the opposites of "nude", such as "dressed", "wearing clothes", and others.

After the training process, we keep the inference process unchanged and only add the direction vector we learned to the embedding \(P_{\text{c}}\) after the text prompt encoding, i.e., \(P_{\text{c}}\gets P_{\text{c}}+\beta d\), where \(d\) refers to the direction away from unsafe concepts, such as the opposite of "nude", and \(\beta\) is a guidance strength coefficient.

**Fair Generation.** We perform fair generation by learning a direction vector close to a specific attribute to debias the text prompt. A text prompt contains specific words that may unintentionally create biased associations. We should generate images with uniformly distributed attributes for a given text prompt. For example, for the prompt "doctor", we want to generate a male doctor or a female doctor with equal probability. Therefore, we learn a direction vector close to each sensitive attribute in order to generate an image that contains a person with the attribute.

At the beginning of the inference phase, we select a vector with the same probability and add it to the prompt embedding. For example, the "male" and "female" direction vectors are chosen with equal probability, denoted as \(P_{\text{c}}\gets P_{\text{c}}+d\), where \(D=\{d_{1},\ldots,d_{k}\}\) is the set of direction vectors, each for a distinct attribute, and \(d\) is drawn from a uniform distribution on \(D\). In this way, the number of generated images with different attributes should be equal by expectation, e.g., an equal number of male and female doctors.

## 4. Experiments

In this section, we conduct extensive experiments on benchmark datasets to demonstrate the effectiveness of our method for safe and fair generation tasks using the stable diffusion model. We also perform transferability, ablation, and case studies for our method. Our code is published anonymously at [https://anonymous.4open.science/r/Responsible-Diffusion-21C2/](https://anonymous.4open.science/r/Responsible-Diffusion-21C2/).

### Safe Generation

**Setup.** For safe generation, our aim is to learn a direction vector in the embedding space and add it to the text prompt embedding to suppress the generation of inappropriate content. We used Stable Diffusion (SD) v1.4 as the base model and set the denoise steps to 50. Following a series of unsafe concepts defined in (Zhu et al., 2017), we set the target concept \(c_{\text{o}}\) as "hate, harassment, violence, suffering, humiliation, harm, suicide, sexual, nudity, bodily fluids, blood, obscene gestures, illegal activity", drug use, theft, vandalism, weapons, child abuse, brutality, cruelty" and the base prompt \(c\) as "an image of hate, harassment, violence,...". Some prior work such as (Kumar et al., 2017) needs to collect related images for training but abstract concepts like "hate" pose difficulties in finding their opposite images. In contrast, our method directly obtains a safe direction vector away from multiple dangerous concepts.

**Baselines.** We use ESD (Zhu et al., 2017), SLD (Zhu et al., 2017), SPM (Zhu et al., 2017) and IntDiff (Kumar et al., 2017) as baselines in our experiments. We also compare with the original SD and the Negative Prompt technique in the SD. The same target concept \(c_{o}\) is used for these methods. More details about the implementation of our method and these baselines are included in Appendix A.1.

Figure 4. Illustration of two methods for direction vector initialization. The images on the top and bottom are generated using two different vectors; the images in the middle are generated without direction vectors.

**Datasets and Evaluation Metrics.** We use the I2P benchmark (Liu et al., 2019) for evaluation. I2P has been widely used to evaluate the safety of text-to-image generative models. It contains 4,703 inappropriate prompts from real-world user input. We also used the red teaming tool, Ring-A-Bell (Spiegel et al., 2017), to generate two sets of adversarial prompts related to 'hudity' and 'violence', consisting of 95 and 250 prompts, respectively. This method utilizes a pre-trained text encoder to generate adversarial prompts by leveraging relative text semantics and a genetic algorithm. We use NudeNet (Brock et al., 2018) and Q16 (Wang et al., 2019) as inappropriate image classifiers. Following previous studies (Zhu et al., 2019; Liu et al., 2019), an image is considered inappropriate if either of the two classifiers reports a positive prediction. We generate one image per prompt, and all the methods use the same seed for each prompt.

**Results.** Figure 5 illustrates that the "safe" direction vector by our method effectively guides the generation of safe images across various prompt categories, including sexual, horror, hate, and others. Meanwhile, the overall harmony of the image is still maintained, indicating that our method effectively constrains the image within a safe region in the latent space rather than forcefully altering its semantics. Table 1 shows that the safe direction vectors our method learns effectively suppress the generation of inappropriate content. Compared with baselines, our method achieves the best results on all categories of unsafe prompts except the category 'hate', where our method is second-best. Table 2 shows the results of various methods on adversarial prompts. In contrast to other methods, our approach demonstrates superior performance against adversarial

\begin{table}
\begin{tabular}{c|c c c c c c c|c} \hline \hline
**Method** & **Harassment** & **Hate** & **Illegal** & **Self-harm** & **Sexual** & **Shocking** & **Violence** & **Overall** \\ \hline Original SD & 0.32 & 0.45 & 0.35 & 0.42 & 0.37 & 0.50 & 0.42 & 0.40 \\ Negative Prompt & 0.17 & 0.17 & 0.15 & 0.17 & 0.14 & 0.31 & 0.23 & 0.19 \\ SD (Liu et al., 2019) & 0.21 & 0.19 & 0.16 & 0.16 & 0.17 & 0.28 & 0.21 & 0.19 \\ ESD (Spiegel et al., 2017) & **0.14** & **0.13** & 0.16 & 0.19 & 0.14 & 0.25 & 0.24 & 0.18 \\ IntDiff (Zhu et al., 2019) & 0.25 & 0.38 & 0.27 & 0.30 & 0.19 & 0.42 & 0.33 & 0.29 \\ SPM (Liu et al., 2019) & 0.25 & 0.31 & 0.29 & 0.38 & 0.32 & 0.41 & 0.37 & 0.34 \\ \hline Ours (†) & **0.14 (+0.00)** & 0.17 (+0.04) & **0.11 (-0.04)** & **0.09 (-0.07)** & **0.08 (-0.06)** & **0.18 (-0.07)** & **0.15 (-0.06)** & **0.12 (-0.06)** \\ \hline \hline \end{tabular}
\end{table}
Table 1. Results on the I2P benchmark dataset. Here, each number represents the ratio of images that are detected as containing inappropriate content by NudeNet and Q16. The lower the ratio, the better the performance of the method. Each column indicates the performance of each method in suppressing inappropriate content generation on a certain category of unsafe prompts. The best and second-best results in each category are highlighted in bold and underline fonts, respectively.

Figure 5. Illustration of our method and baselines for reducing inappropriate content in image generation. Each column contains the images generated by different methods with the same prompt (from the I2P benchmark) and random seed.

attacks. This is because other methods typically isolate a specific concept or simply filter out certain words, but adversarial prompts often contain uncommon characters while embedding implicit associations with unsafe concepts, leading to poor performance in such methods. Our method, however, constrains the adversarial prompt within a relatively safe region through a safe semantic direction. Additional examples can be found in Appendix C.4.

### Fair Generation

**Setup**. For fair generation, we learned a direction vector for each sensitive attribute:'male' and 'female' in gender and 'black', 'white', and 'Asian' in race. We set the base prompt \(c\) as "a person" and the target concept \(c_{o}\) as "a [mask] person", where [mask] is filled with the corresponding attribute, such as'male' and 'black'. In the inference stage, we used the process in Section 3.3 to sample a direction vector.

**Baselines.** We used UCE (Kohn et al., 2017) and IntDiff (Zhu et al., 2017) as baselines in the experiments. Since DebiasVL (Chen et al., 2018) and Concept Algebra (Zhu et al., 2017) can only debias binary attributes, we exclude them from the evaluation. More details about the implementation of our method and these baselines are included in Appendix A.2.

**Datasets and Evaluation Metrics.** We used the Winobias (Zhu et al., 2017) benchmark for fairness evaluation. Winobias contains 36 professions known to have social biases. Following IntDiff, we set up two sets of templates to generate images, such as "a photo of a [profession]" and "a photo of a successful [profession]". The latter set of templates is more challenging for debiasing methods, as "successful" is known to incur greater biases. For each set of templates, 150 images are generated for each profession. Then, we used the pre-trained CLIP classifier to predict the attributes of an image. Finally, we measured the balance of different attributes in the generated images using the deviation ratio \(\Delta=\max_{c\in C}\frac{|N_{c}|-1/C|}{1-1/C}\), where \(C\) is the number of attributes of a social group, \(N\) is the number of generated images, and \(N_{c}\) represents the number of images predicted to have an attribute \(c\).

**Results.** Figure 6 compares the original SD with our method for gender fairness. We find that our direction vector can guide the model to generate images with a balanced gender distribution, but the original SD model cannot. As shown in Table 3, our method greatly alleviates the social bias manifested by the original SD and shows better performance than baselines. We randomly select 6 professions from 36 professions in the Winobias benchmark in Table 3, where our method performs the best or the second best in most cases. The average deviation ratio of our method is always the lowest, indicating that its generated images are closer to a uniform distribution in sensitive attributes. Since our direction vector acts directly on the CLIP embedding space without targeting specific words, it can achieve good results on most prompt templates without performance degradation. In contrast, as UCE debiases different profession nouns individually, it lacks generalizability in terms of templates and shows significant performance decreases.

### Transferability

Next, we verify whether our direction vectors are transferable. The I2P benchmark is still used to evaluate whether the direction vector we learned in the original SD can be used directly in other approaches to improve their effectiveness. Table 4 shows that our direction vectors can significantly enhance the performance of existing methods. We transfer the direction vectors to ESD and SLD in a training-free manner. For other diffusion models, such as SDXL (Zhu et al., 2017), which utilizes two text encoders and concatenates their outputs to form the final result, directly applying the direction vector obtained from the original SD is difficult. Therefore, we adjusted the shape of the low-rank direction vector and retrained it to fit the SDXL setup. The results still indicate that our method is effective in models like SDXL that use multiple encoders, successfully identifying specific safe regions within a more complex latent space. In summary, our method demonstrates strong transferability for both existing fine-tuning approaches based on the original SD and for models with different architectures.

### Combination of Direction Vectors

Experiments on safe generation have shown that our method can learn multiple concepts simultaneously. We further show that it can also combine multiple single-concept vectors. After learning

\begin{table}
\begin{tabular}{c|c c c c c c c} \hline \hline
**Concept** & SD & Neg. Prompt & SLD & ESD & IntDiff & SPM & Ours (\({}^{\prime}\)) \\ \hline nudity & 0.947 & 0.947 & 0.968 & 0.537 & 0.968 & 0.653 & **0.316** \\ violence & 0.976 & 0.812 & 0.828 & 0.740 & 0.924 & 0.720 & **0.116** \\ \hline \hline \end{tabular}
\end{table}
Table 2. Results on the adversarial prompts produced by Ring-A-Bell. Here, each value represents the ratio of images classified as inappropriate out of all generated images.

Figure 6. Comparison of SD and our method for gender fairness when generating eight “photos of a doctor”.

Figure 7. Illustration of the linear combination of multiple direction vectors.

the direction vectors for different single concepts, such as "male" and "black", these vectors can be linearly combined and added to the text prompt embedding as \(P_{e}\gets P_{e}+\sum_{i=1}^{k}\beta_{i}d\). The results of linear combinations are shown in Figure 7. Using the linear combination method, the model can be guided to generate images with multiple attributes simultaneously. As seen from the last line, the superposition of multiple direction vectors may weaken the effect of each vector. For example, the impact of the "safe" vector on the original text prompt becomes weaker as the number of superimposed vectors increases.

### Image Fidelity and Text Alignment

Finally, we evaluate the impact of our method on the quality of the generated images and the fidelity to the original text prompt. The FID score (Golov et al., 2013) is used to evaluate the fidelity of the generated images by comparing them to real images. The CLIP score (Golov et al., 2013) measures the semantic alignment between images and input text. The COCO-30K dataset is used for evaluation, with one image generated per prompt. As shown in Table 5, the quality and text alignment of the images generated using our method on COCO-30k are at the same level as the original SD. This also shows from another aspect that the semantic direction we learn is accurate and less relevant to other semantics.

## 5. Conclusion

In this paper, we approach the problem of responsible generation using diffusion models from a new perspective and propose a novel self-discovery approach to find the specific semantic direction vector in the embedding space. Unlike previous methods that train pseudo-tokens to represent a certain concept, our method learns a direction vector in the embedding space for concept representation. This direction vector can perform fine-grained continuous control of text embedding on specific semantics, thereby constraining the text embeddings within a safe region. Since our method only guides the text embedding along specific directions, it will not affect other semantics, thus hardly impacting the quality of the generated images. Our method can be applied to the responsible generation of diffusion models, including safe and fair generation tasks. Extensive experiments have demonstrated the effectiveness and superiority of our method, which greatly reduces the generation of harmful content and mitigates social bias in diffusion models.

\begin{table}
\begin{tabular}{c c c|c c|c c} \hline \hline
**Category** & **SLD** & **SLD+** & **ESD** & **IntDiff** & **SPM** & **Ours (\(\ast\))** \\ \hline \hline FID (↓) & 14.30 & 18.22 & 17.34 & 15.87 & 14.77 & 15.13 \\ CLIP (↑) & 0.2626 & 0.2543 & 0.2381 & 0.2632 & 0.2581 & 0.2588 \\ \hline \hline \end{tabular}
\end{table}
Table 4. Transferability results on the I2P benchmark dataset, where "SLD+/ESD+/SDXL+" represents the integration of our method with SLD/ESD/SDXL, respectively.

\begin{table}
\begin{tabular}{c c|c c c c c c|c} \hline \hline
**Dataset** & **Method** & **Designer** & **Farmer** & **Cook** & **Hairdresser** & **Librarian** & **Writer** & **Winobias (Vinobias, 2018)** \\ \hline \multirow{4}{*}{Gender+} & SD & 0.46 & 0.97 & 0.13 & 0.77 & 0.84 & 0.60 & 0.68 \\  & UCE & 0.40 & 0.32 & 0.24 & 0.49 & 0.50 & **0.13** & 0.26 \\  & IntDiff & **0.05** & 0.28 & 0.25 & 0.65 & 0.21 & 0.14 & 0.22 \\  & Ours (↑) & 0.18 (+0.13) & **0.12 (-0.16)** & **0.11 (-0.02)** & **0.32 (-0.17)** & **0.12 (-0.09)** & 0.18 (+0.05) & **0.19 (+0.03)** \\ \hline \multirow{4}{*}{Gender+} & SD & 0.40 & 0.96 & 0.28 & 0.79 & 0.84 & 0.32 & 0.71 \\  & UCE & 0.56 & 0.52 & 0.19 & 0.57 & 0.54 & 0.11 & 0.46 \\  & IntDiff & 0.13 & **0.02** & **0.04** & 0.84 & **0.08** & 0.08 & 0.23 \\  & Ours (↑) & **0.08 (-0.05)** & 0.07 (+0.05) & 0.11 (+0.07) & **0.37 (-0.20)** & 0.09 (+0.01) & **0 (-0.08)** & **0.16 (+0.07)** \\ \hline \multirow{4}{*}{Race} & SD & 0.42 & 0.58 & 0.35 & 0.63 & 0.78 & 0.82 & 0.55 \\  & UCE & **0.11** & 0.49 & 0.19 & 0.49 & 0.60 & 0.64 & 0.30 \\  & IntDiff & 0.36 & 0.27 & 0.41 & 0.43 & 0.33 & 0.41 & 0.31 \\  & Ours (↑) & 0.23 (-0.12) & **0.05 (-0.22)** & **0.03 (-0.16)** & **0.22 (-0.21)** & **0.07 (-0.26)** & **0.05 (-0.36)** & **0.13 (-0.18)** \\ \hline \multirow{4}{*}{Race+} & SD & 0.38 & 0.34 & 0.29 & 0.49 & 0.86 & 0.74 & 0.54 \\  & UCE & 0.16 & 0.46 & 0.30 & 0.68 & 0.67 & 0.83 & 0.38 \\  & IntDiff & 0.36 & 0.26 & **0.08** & 0.37 & 0.32 & 0.24 & 0.29 \\  & Ours (↑) & **0.10 (-0.06)** & **0.18 (-0.08)** & 0.09 (+0.01) & **0.14 (-0.23)** & **0.13 (-0.19)** & **0.04 (-0.20)** & **0.14 (-0.15)** \\ \hline \hline \end{tabular}
\end{table}
Table 3. Results for fair generation measured by the deviation ratio \(\Delta\in[0,1]\), where a lower value indicates fairer results. Here, “Gender/Race” uses a normal template to generate images, and “Gender+/Race+” uses an extended template to generate images. Each column represents the deviation ratios of different methods for each profession and “Winobias” presents the average results for all professions. The best and second-best results in each setting are also highlighted in bold and underline fonts.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline
**Method** & **SD** & **SLD** & **ESD** & **IntDiff** & **SPM** & **Ours (\(\ast\))** \\ \hline FID (↓) & 14.30 & 18.22 & 17.34 & 15.87 & 14.77 & 15.13 \\ CLIP (↑) & 0.2626 & 0.2543 & 0.2381 & 0.2632 & 0.2581 & 0.2588 \\ \hline \hline \end{tabular}
\end{table}
Table 5. Results of FID and CLIP scores on the COCO-30K dataset.

## References

* (1)
* Arad et al. (2024) Dana Arad, Hadas Orgad, and Yonatan Belinkov. 2024. ReFACT: Updating Text-to-Image Models by Editing the Text Encoder. In _Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)_. Association for Computational Linguistics, 2537-2558.
* Belongi (2022) Pranewell Belongi. 2022. NudeNet: lightweight Nudity detection. Retrieved October 14, 2024 from [https://github.com/nodal-tech/NudeNet](https://github.com/nodal-tech/NudeNet)
* Brock et al. (2023) Manuel Brock, Frederic Bertolini, Dominik Hinterderl, Lukas Struppek, Patrick Schmarowski, and Kristin Kersting Kersting. 2023. GSGR: Instructions Text-to-Image Models using Semantic Guidance. _Advances in Neural Information Processing Systems 36_ (2023), 25365-25389.
* Brads et al. (2023) Manuel Brock, Felix Friddis, Patrick Schmarowski, and Kristin Kersting. 2023. Mitigating Inappropriateness in Image Generation: Can there be Value in Reflecting the World's Uplizers' Association? 2023,13898 [cs.CV].
* Chuang et al. (2023) Ching-Yao Chuang, Yuru Jampani, Yuanzhen Li, Antonio Torralba, and Stefanie Zeydas. 2023. Debiasing Vision-Language Models via Biased Prompts. arXiv:2302.00070 [cs.LG].
* Dhariwal and Nichol (2021) Pavlish Dhariwal and Alexander Nichol. 2021. Diffusion Models Beat GANs on Syngles. _Advances in Neural Information Processing Systems_ 34 (2021), 8780-8794.
* Pan et al. (2024) Chongyu Pan, Jianchao Liu, Yihang Zhang, Eric Weng, Dennis Wei, and Sijia Liu. 2024. SalUn: Empowering Machine Unlearning via Gradient-based Weight Saliency in Both Image Classification and Generation. In _The Twelfth International Conference on Learning Representations_. Oertzheim, November, 31 pages.
* Chai et al. (2023) Rinon Gal, Yuval Ashok, Yuval Atzmon, Patashnik, Antil Hanim Berman, Gal Chechik, and Daniel Cohen. 2023. An Image is Worth On the Word: Personalizing Text-to-Image Generation using Textual Inversion. In _The Eleventh International Conference on Learning Representations_. OpenReviewer, 31 pages.
* Gandikova et al. (2023) Rohit Gandikova, Joanna Materzynska, John Pietro-Sakmann, and David Bau. 2023. Erasings Concepts from Diffusion Models. In _2023 IEEE/CVF International Conference on Computer Vision_. ICCV, 2426-2436.
* Gandikova et al. (2024) Rohit Gandikova, Hadsa Orgad, Yonatan Belinkov, Joanna Materzynska, and David Bau. 2024. Unified Concept Editing in Diffusion Models. In _2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)_. IEEE, 5099-5108.
* Hong and Yoshi (2023) Akin Hong and Haroh Yoshi. 2023. Selective Amesis: A Continual Learning Approach to Forgetting in Deep Generative Models. _Advances in Neural Information Processing Systems 36_ (2023), 1770-1798.
* Hessel et al. (2021) Jack Hessel, Ari Holtmann, Maxwell Forbes, Ronn Le Bras, and Yejin Choi. 2021. CIPSeco: A Reference Face-Free Evaluation Metric for Image Captioning. In _Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing_. Association for Computational Linguistics, 7514-7528.
* Heusel et al. (2017) Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernard Nessler, and Sepp Hochreiter. 2017. GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. _Advances in Neural Information Processing Systems 30_ (2017), 6266-6367.
* Ho and Salimans (2022) Jonathan Ho and Tim Salimans. 2022. Classifier-Free Diffusion Guidance. arXiv:2207.12598 [cs.LG].
* Hu et al. (2022) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanhui Li, Skean Wang, Lin Wang, and Weihin Chen. 2022. LORA-Low-Rank Adaptation of Large Image Models. In _The Twelfth International Conference on Learning Representations_. OpenReviewer, 1st pages.
* Kim et al. (2023) Emji Kim, Swoon Kim, Chaohun Shin, and Sungroh Yoon. 2023. De-stereotyping Text-to-Image Models through Prompt Tuning. In _ICML 2023 Workshop on Deployment Challenges for Generation of OpenReviewer_, 1st pages.
* Kim et al. (2022) Sanghyun Kim, Swoon Jung, Ralime Klem, Monosueh Choi, Jinwoo Shin, and Julbe Lee. 2022. Towards Safe Self-Distillation of Internet-Scale Text-to-Image Diffusion Models. In _ICML 2023 Workshop on Deployment Challenges for Generative AI_. OpenReviewer, 17 pages.
* Kingma and Welling (2014) Diederik P. Kingma and Max Welling. 2014. Auto-Encoding Variational Bayes. In _2nd International Conference on Learning Representations_. OpenReviewer, 14 pages. [https://arxiv.org/abs/1312.6114](https://arxiv.org/abs/1312.6114)
* Kurnit et al. (2023) Nugur Kurnit, Bingfang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, and Jun-Yan Zhu. 2023. Ablating Concepts in Text-to-Image Diffusion Models. In _2023 IEEE/CVF International Conference on Computer Vision (ICCV)_. IEEE, 2023-20245.
* Kwon et al. (2023) Ming Kwon, Jesseok Jeong, and Yonming. 2023. Diffusion Models Already Have A Semantic Latent Space. In _The Eleventh International Conference on Learning Representations_. OpenReviewer, 35 pages.
* Li et al. (2024) Hang Li, Chengjhi Shen, Phillip Torr, Volker Tresp, and Jindong Gu. 2024. Self-disovering interpretable diffusion latent directions for responsible text-to-image generation. In _2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_. IEEE, 2024-2016.
* Li and Liang (2021) Xiang Lis Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous Prompts for Generation. In _Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_. Association for Computational Linguistics, 4582-4597.
* Lyu et al. (2024) Mengyeo Lyu, Yuhong Tang, Haiwen Hong, Hui Chen, Xuan Jin, Yuan He, Hui Xue, Jungong Han, and Guiguang Ding. 2024. One-Dimensional Adapter to Rule Them: ML Concepts, Diffusion Models and Erasing Applications. In _2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_. IEEE, 7559-7568.
* Maldjourney (2022) Midjourney. Retrieved October 14, 2024 from [https://www.miljourney.com](https://www.miljourney.com).
* Ni et al. (2024) Murheng Ni, Chenfei Wu, Xiaodong Wang, Shengming Yin, Lijuan Wang, Zhenheng Liu, and Nan Duan. 2024. ORES: Open-vocabulary Responsible Visual Synthesis. _Proceedings of the AAAI Conference on Artificial Intelligence_, 38, 1992, 24738-24815.
* Xu et al. (2023) Zixuan N, Longhui Wei, Jaeheng Li, Xiang Tang, Taeing Zhang, and Qi Tian. 2023. Depermination-Tuning: Usang Semantic Grid: Mailed Unventral Concepts from Stable Diffusion. In _Proceedings of the 31st ACM International Conference on Multimedia (MM '23)_. Association for Computing Machinery, New York, NY, USA, 8900-8909.
* Nichol et al. (2022) Alexander Qunin Nachol, Pafulla Dhariwal, Aditya Ransch, Pranewa Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. 2022. GLDR: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models. In _Proceedings of the 39th International Conference on Machine Learning_. PMLR, 16784-16804.
* What you need to know. Retrieved October 14, 2024 from [https://www.aazeshvh.com/blog/stable-diffusion-1v2-2-what-you-need-to-know/](https://www.aazeshvh.com/blog/stable-diffusion-1v2-2-what-you-need-to-know/)
* Orgad et al. (2022) Hidas Orgad, Bajuar Kawan, and Yonatan Belinkov. 2022. Editing Implicit Assumptions in Text-to-Image Diffusion Models. In _2022 IEEE/CVF International Conference on Computer Vision (ICCV)_, 7803-7038.
* Park et al. (2013) Yong-Hyun Park, Mingxu Kwon, Jeong Choi, Junhybo Jo, and Youngjung Uh. 2013. Understanding the Latent Space of Diffusion Models through the Lems of Examination Geometry. _Advances in Neural Information Processing Systems_ 36 (2013), 2219-2142.
* Podd et al. (2024) Dusin Podd, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dodson, Jonas Miller, Joel Perona, and Robin Rombach. 2024. SDX: Improving Latent Diffusion Models for High-Resolution Image Synthesis. In _The Twelfth International Conference on Learning Representations_. OpenReviewer, 15 pages.
* Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Case Chu, and Mark Chen. 2022. Hierarchical Text-Conditional Image Generation with CLIP Latents. arXiv:2204.04025 [cs.CV].
* Rando et al. (2022) Javier Rando, Daniel Paloka, David Lindner, Lennit Hein, and Florian Tramie. 2022. Red-Training the Stable Diffusion Style Filter. arXiv:2202.04460 [cs.AI].
* Roth et al. (2022) Robin Roth, Andreas Blattmann, Dominik Lorenz, Frank Szeker, and Bjorn Ommer. 2022. High-Resolution Image Synthesis with Latent Diffusion Models. In _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_. IEEE, 10674-10685.
* Rui et al. (2023) Nathani Rui, Yuanzhen Li, Yuanu Jampani, Yael Prich, Michael Rubinstein, and Kirk Aperman. 2023. DreamBook: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation. In _2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_. IEEE, 22500-22510.
* Schramowski et al. (2023) Patrick Schramowski, Manuel Brack, Bjorn Deisenroth, and Kristian Kersting. 2023. Safe Latent Diffusion: Mitigating Inappropriate Differentiation in Diffusion Models. In _2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_. IEEE, 22522-2253.
* Schmarowski et al. (2022) Patrick Schramowski, Christopher Tauchmann, and Kristin Kersting. 2022. Can Machines Help Us Answering Question 16 in Databases, and In Turn Reflecting on Imprint Content?. In _Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (Fact '22)_. Association for Computing Machinery, New York, NY, USA, 1350-1361.
* Shen et al. (2024) Msohn Shen, Chao Tanya Wang, Min Lin, Yongdong Wang, and Mohan S. 2024. Self-Kankanhalli. 2024. Fine-Tenting Text-to-Image Diffusion Models for Fairness. In _The Twelfth International Conference on Learning Representations_. OpenReviewer, 4, pages.
* Tsai et al. (2024) Yu-In Tai, Chiu-Yi Hsu, Chilin Xie, Chi-Hun Lim, Jia-Yu Chu, Bean Li, Yin-Yu Chen, Chiu-Yi Hsu, and Chun-Ying Huang. 2024. Ring-A-Bell How Reliable are Concept Removal Methods For Diffusion Models?. In _The Twelfth International Conference on Learning Representations_. OpenReviewer, 12 pages.
* Wang et al. (2023) Zhao Wang, Lin Cui, Jeffrey Negrega, and Victor Veitch. 2023. Concept Algebra for (Gover Science-Based) Text-Controlled Generative Models. arXiv:2302.00095 [cs.CL].
* Xiang et al. (2023) Eric Zhang, Kai Wang, Xingqian Xu, Zhaoyang Wang, and Humphrey Shi. 2023. Forget-Meet: Learning to Forget in Text-to-Image Diffusion Models. arXiv:2303.17501 [cs.CV].
* Zhao et al. (2018) Jieyu Zhao, Tianli Wang, Mark Tatsuk, Vicente Ordonez, and Kai-Wei Chang. 2018. Gender Bias in Conference Resolution: Evaluation and Debiasing Methods. In _Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)_. Association for Computational Linguistics, 15-20.

## Appendix A Implementation Details

### Safe Generation

**Implementation of Our Method**. We configured the shape of the low-rank direction vector according to the text encoder settings of the original SD v1.4, specifically setting it to \(77\times 1\) and \(1\times 768\). For SDXL, the final feature dimension output by its two encoders is \(2.048\), so the shape of the low-rank direction vector is set to \(77\times 1\) and \(1\times 2048\). During training, we used stochastic gradient descent for optimization, with the Adam optimizer and a learning rate of \(0.05\). The training dataset contains \(60\) base prompts: "An image of hate, harassment, violence, suffering, humiliation, harm, suicide, sexual, nudity, bodily fluids, blood, obscene gestures, illegal activity, drug use, theft, vandalism, weapons, child abuse, brutality, cruelty" with target concepts "hate, harassment, violence, suffering, humiliation, harm, suicide, sexual, nudity, bodily fluids, blood, obscene gestures, illegal activity, drug use, theft, vandalism, weapons, child abuse, brutality, cruelty".

**Baselines.** We benchmark our method against the following state-of-the-art baselines.

* The **Negative Prompt** technique replaces the unconditional estimate in the classifier-free guidance with a conditional estimate based on an unsafe concept, thus reducing the generation of inappropriate content.
* Similarly, **SLD**(Wang et al., 2017) leverages classifier-free guidance by using three noise predictions, which shifts the unconditional estimate towards the prompt condition estimate while steering it away from the unsafe concept condition estimate.
* **ESD**(Chen et al., 2018) employs a frozen diffusion model as a teacher model to align the probability distribution of the target concept in the student model with the probability distribution of an empty string, thereby achieving concept forgetting within the student model.
* **IntDiff**(Wang et al., 2017) identifies a directional vector corresponding to a specific attribute in the latent space of the diffusion model using a self-discovery method on images with the corresponding attribute.
* **SPM**(Wang et al., 2017) trains an adapter to replace the fine-tuning of all model parameters to forget a target concept, with the training objective being the alignment of the target concept with an anchor concept in the latent space.

For ESD, we utilized the open-source code and parameters in the original paper to remove NSFW concept during training. The concepts to be erased were set as "hate, harassment, violence, suffering, humiliation, harm, suicide, sexual, nudity, bodily fluids, blood, obscene gestures, illegal activity, drug use, theft, vandalism, weapons, child abuse, brutality, cruelty". During training, we only updated the parameters of the cross-attention layers in the UNet. For SLD and IntDiff, we use the parameters of SLD-Medium to generate images in SLD and use the code and weights published by the original paper (Wang et al., 2017). For SPM, we trained a separate adapter for each of the aforementioned unsafe concepts and then injected all of them into the diffusion model.

**Evaluation Setting.** The prompts in the I2P may not contain explicit toxic words, but they can guide the model to generate images of inappropriate content, including seven categories such as self-harm, illegal activity, sexual content, harassment, and violence. We used Ring-A-Bell to generate two adversarial prompt sets from the I2P dataset. For nudity, we selected \(95\) nudity-related prompts, all of which had a probability of generating nudity greater than \(50\%\). For the concept of violence, we selected \(250\) prompts where the probability of generating nudity was below \(50\%\), but the probability of generating inappropriate images exceeded \(50\%\). These prompts were then used to generate adversarial prompts on Ring-A-Bell. Based on the best results reported in the paper, for nudity, we set Ring-A-Bell with \(K=16\) and \(\eta=3\). For violence, we chose \(K=77\) and \(\eta=5.5\). In the NudeNet evaluation, the image is considered a nude image only if it detects nudity in specific sensitive body parts, including "BUTTOCS_EXPOSED", "FEMALE_BREAST_EXPOSED", "FEMALE_GENITAILA_EXPOSED", "MALF_GENITAILA_EXPOSED", and "ANUS_EXPOSED".

### Fair Generation

**Implementation of Our Method**. In the fair generation task, we use an objective function close to a concept to learn the direction vectors and learn a concept separately. The training dataset only contains \(10\) base prompts such as "an image of a person". We use different sensitive attributes as the target concept, such as "a male person". These direction vectors can guide the model to generate the image of the corresponding attribute. Similarly to IntDiff (Wang et al., 2017), the direction vector we get here is a global direction vector and does not target a particular profession alone. Note that UCE (Wang et al., 2017) needs to edit the parameters for each profession.

**Baselines.** UCE (Wang et al., 2017) is an additional baseline for fair generation. UCE proposes a model editing method tailored for linear projection layers, which identifies new model weights to map the original concept to a new target value in a training-free manner. For IntDiff, we used their released pre-trained weights for each attribute to generate images. For UCE, we updated the parameters of the cross-attention layers in the UNet for \(36\) different professions according to the debias settings outlined in the paper.

**Evaluation Setting.** The prompt templates for generating images were as follows: "a photo of a [profession], "an image of a [profession]", "a headshot of a [profession], "a picture of a [profession], "a photo of a successful [profession], "an image of a successful [profession], "a neutral of a successful [profession], and "a picture of a successful [profession]." The pre-trained CLIP classifier predicts the attributes by comparing the similarity between the text embedding and the image embedding. We set up some text templates such as "a photo of [label]" to complete the classification. The "label" is an attribute to be predicted, such as "male person" and "black person".

### Computational Costs

Our approach consists of two stages: training and inference. During the training stage, we optimize only a small subset of parameters within the LoRA matrix. For example, in the SD v1.4 model, the number of parameters in the direction vector is \(77+768=845\). During training, we freeze the gradients of the text encoder and use the gradient information from the UNet to update the directionvector. Taking the training of a "safe" vector as an example, our training dataset contains 60 prompts. We perform optimization at every diffusion step, resulting in a total of 3000 steps. Using our experimental setup, the training process on an NVIDIA A100 80G GPU took about 15 minutes. This time cost is a fraction--sometimes even as little as 10% or less--of the time required by fine-tuning methods.

Once we identify a suitable direction vector, it can be directly embedded in the model. During inference, we simply add the direction vector to the encoded prompt embedding without introducing any additional computational cost. In other words, our method facilitates responsible diffusion generation with marginal additional costs, requiring only about ten to fifteen minutes of training.

## Appendix B Hyper-parameter Setting

**Warm-up Step.** The text condition dynamically interacts with the image through cross-attention layers in the U-Net, guiding the generation of images that align with the input text. Based on our observations, during the denoising process, the initial outputs typically consist of the outlines of the image, gradually evolving to incorporate finer details of shape and color.

Taking a common example of 50 steps of generation, we recommend adding a guiding direction vector at \(t=15\) to start the guide. This approach maximally preserves the primary structure of the original image, while guiding the overall image details toward specific semantic directions. In addition, this approach minimizes the impact of prompts that are not related to the intended direction of the guiding. For example, the movement of "an apple" along the "safe dimension " will not affect the generation of the image. It should be noted that although we recommend adding a direction vector when \(t=15\), as we mentioned earlier, we optimize the same direction vector for each time step, so we can choose to add it at any time step. However, we do not recommend guiding after more than 2/3 of the total diffusion steps because by that time the image has been formed, and further guidance will not work anymore.

**Guidance Scale \(\beta\).** The value of \(\beta\) determines the extent to which the direction vector influences the original text embedding. The larger the value, the more the original text embedding is shifted towards a specific region in the latent space. In our experiments, we consistently use \(\beta=1\).

## Appendix C Additional Experiments

### Effect of Direction Vector Initialization Method

In this subsection, we investigate the generative performance between low-rank direction vectors and standard direction vectors. We trained a standard direction vector using the same settings as those in the safe generation experiment and evaluated it on the I2P benchmark. Additionally, we selected 3,000 images from the COCO-30k dataset to test the FID and CLIP scores.

Table 6 quantitatively shows that the performance of the standard direction vector in reducing unsafe content is inferior to that of the low-rank direction. In addition, there is a noticeable decrease in image fidelity. Figure 4 visually illustrates that images generated without the low-rank direction vector exhibit a certain degree of distortion, although the overall semantics of the images are preserved. This indicates that the semantic direction identified using the standard direction vector is not precise and significantly impacts the quality of the generated images.

### Effect of Warm-up Step

As mentioned above, the earlier the guidance is applied, the greater the influence of the direction vector on the generated images. However, starting too early can significantly impact the semantics of the generated images and may also affect the image quality. Conversely, if the guidance is applied too late, it becomes ineffective as

Figure 8. Visual results for the two direction vector initialization methods on the I2P and COCO-3K datasets. We observe that using the standard direction vector can also reduce the generation of inappropriate content to some extent; however, the overall harmony of the images is compromised, resulting in less refined visuals.

the primary details of the image would already have been generated. Figure 9 illustrates the impact of starting the guide at different steps on the generated images. In the experiments, a total of 50 diffusion steps were used, using the direction vector obtained from the safe generation task. We observe that when the total number of diffusion steps is set to 50, initiating the guidance at the 15th step yields the best overall results. When the warm-up step is too small, the semantic content of the generated images undergoes significant changes. Furthermore, we find that different prompts exhibit varying degrees of sensitivity to the warm-up step. This variability arises because, in high-dimensional feature space, the distances between some prompt embeddings and specific safe regions differ. Therefore, it is necessary to make trade-offs based on the actual circumstances.

### Fine-grained Control

Unlike methods that fine-tune the model parameters, the direction vectors we learn are flexible. It can also achieve fine-grained control over the impact of the original text prompt by adjusting the strength coefficient. Figure 10 shows that as the strength coefficient

\begin{table}
\begin{tabular}{c|c c c} \hline \hline
**Method** & I2P ratio (↓) & FID (↓) & CLIP (↑) \\ \hline Standard & 0.14 & 35.67 & 0.2487 \\ LoRA-based & 0.12 & 33.18 & 0.2545 \\ \hline \hline \end{tabular}
\end{table}
Table 6. Results for the ablation studies on using the standard and the low-rank direction vectors. The I2P ratios represent the average value across all categories, while the FID and CLIP scores are calculated using a subset of COCO-30K, which contains 3,000 images. The low-rank direction vector outperforms the standard one across all three metrics.

Figure 9. Visual results for different warm-up steps. The results show that when the warm-up step is less than 10, the semantic content of the images is compromised; when the warm-up step exceeds 25, the guidance has little to no effect. Furthermore, the sensitivity of different prompts varies.

[MISSING_PAGE_FAIL:13]

Figure 11. Visualization of images generated by our method and baselines for adversarial prompts produced by Ring-A-Bell. The first row shows the prompts used to generate the images. Unsafe images have been censored with mosaics and blurring.

Figure 12. Visualization results of applying our method to SDXL. The first row shows the prompts used to generate the images. Our method effectively reduces the generation of violent and gory content while preserving the semantics of generated images.