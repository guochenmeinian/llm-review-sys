[MISSING_PAGE_FAIL:1]

## 2. Related Work

### Knowledge Tracing

Researchers have explored different modes of KT conduction. Existing KT methods can be divided into two types: probabilistic or logistic model-based traditional methods and deep learning-based methods. Probabilistic model-based methods generally define a student's knowledge states as a binary variable and use _Hidden Markov Model_ to estimate the student's conceptual mastery level, and the representatives include BKT (Brocker, 1977) and its variants (Kumar et al., 2017). Logistic model-based methods mainly estimate student performance by usually learning a logistic function, based on different factors in some students who solve the same set of problems, and the representatives include _Performance Factor Analysis_(Kumar et al., 2017) and _Learning Factor Analysis_(Brocker, 1977). Differently, deep learning-based KT methods leverage various neural network techniques to solve the sequential prediction task of the student answering exercises for tracing the student's knowledge states, which are commonly implicit in the hidden states of models. The representatives include the RNN-based method DKT (DKT, 1971), memory-augmented methods (e.g., DKVMN (Kumar et al., 2017)), attention mechanism-based methods (Kumar et al., 2017; Kumar et al., 2017), transformer-based methods (Kumar et al., 2017; Kumar et al., 2017) and graph neural network-based methods (Kumar et al., 2017; Kumar et al., 2017; Kumar et al., 2017).

Among them, some KT methods not only focus on designing novel network architectures but also try to solve some intrinsic difficulties in KT. For example, CL4KT (Kumar et al., 2017) and CMKT (Kumar et al., 2017) aim to address the student-exercise interaction sparseness problem; ATKT (Kumar et al., 2017) and DLKT (Kumar et al., 2017) pursue to improve model generalization performance; LPKT (Kumar et al., 2017), HawkesKT (Kumar et al., 2017) and CT-NCM (Kumar et al., 2017) attempt to model the forgetting behaviors of students during the learning process; DTransformer (Kumar et al., 2017) was proposed to obtain stable knowledge state estimation and tracing, instead of only improving the prediction performance, by inventing a new training paradigm. It can be observed that many intrinsic difficulties (including sparseness, forgetting, stable tracing, and so on) in KT have been well solved, but how to overcome the influence caused by the abnormal conditions that occur among students during the online learning process has been less explored. The abnormal conditions may arise from low data quality and abnormal student behaviors, which is ubiquitous in online learning system and will affect the accuracy and interpretability of KT tasks, and thus it is urgent for us to develop corresponding KT methods to solve this difficulty.

### Anomaly Detection

Anomaly detection is an important research topic with broad application prospects. For example, in the recommendation system, there are certain abnormal behaviors in the user's click sequence (such as clicking on a product that he does not like), which will affect the recommendation of the next item for the user (Kumar et al., 2017; Kumar et al., 2017). In industry, researchers detect whether abnormalities occur in the sensors to improve production efficiency (Kumar et al., 2017; Kumar et al., 2017; Kumar et al., 2017). Anomaly detection has been applied for various types of data. Here we focus on anomaly detection for time series data. These existing researches can be divided into prediction-based methods (Kumar et al., 2017; Kumar et al., 2017) and reconstruction-based methods (Kumar et al., 2017; Kumar et al., 2017). Prediction-based models utilize advanced

Figure 1. The illustrative examples of a sequence of interactions for a student learning online and the corresponding diagnosed knowledge states. The record comprises 9 learning interactions, spanning 6 exercises and encompassing 5 knowledge concepts.

Figure 2. The impact of different proportions of noise data on the DKT method (DKT, 1971) in the ASSISTment12 dataset. The range of change in knowledge proficiency denotes the maximum rate of change in each student’s mastery level for different knowledge concepts within a learning process.

machine learning components to predict the future variable performance based on the historical time series through modeling the spatiotemporal correlation between variables in time series data.
* The abnormality is detected through prediction probabilities. In order to improve the accuracy of abnormality detection, a variety of discriminant models attempt to better learn the complex relationship between variables to enhance the prediction performance. For example, _Deng and Hooi_(Deng and Hooi, 2017) proposed a graph neural network based prediction model to capture complex inter-sensor relationships to detect and explain anomalies that deviate from these relationships. _Zhao et al._(Zhao et al., 2019) combined feature-oriented graph attention network (GAT) and time-oriented GAT to handle spatial dependence and temporal dependence in predicting. Reconstruction-based methods pursue precise representations of the entire time series data for data reconstruction, and detect anomalies according to the difficulty of reconstruction. To be specific, it is more difficult to reconstruct abnormal data and less difficult to reconstruct normal data. Therefore, this category pursues to learn robust and accurate representations of input data for reconstructing input data. For example, _Li et al._(Li et al., 2019) used the generative adversarial network (GAN) framework with long short-term memory (LSTM) as the basic unit to accurately reconstruct input data by considering the entire set of variables concurrently. In the literature (Zhu et al., 2019), the proposed OmniAnomaly uses stochastic recurrent neural networks (RNN) to find robust representations for multivariate time series. _Aubieter et al._(Aubieter et al., 2019) proposed an AutoEncoder architecture with adversarial learning inspired by GANs. Recent work (Abadi et al., 2016) exploits spectral analysis of latent representations and produces simultaneous representations of multivariate data. However, to the best of our knowledge, no researchers have head-on addressed the anomaly issue in knowledge tracing tasks.

## 3. Problem Definition

In this section, we formally define the problem of knowledge tracing (KT). Suppose there is a set of \(N\) students, \(\mathcal{S}=\{s_{1},s_{2},\ldots,s_{N}\}\), a set of \(M\) exercises, \(\mathcal{E}=\{e_{1},e_{2},\ldots,e_{M}\}\), and a set of \(\mathcal{C}\) knowledge concepts, \(\mathcal{K}=\{k_{1},k_{2},\ldots,k_{C}\}\). Each exercise is associated with specific knowledge concepts and the \(Q\)-matrix \(Q=\{q_{ij}\in\{0,1\}\}^{M\times C}\) is utilized to indicate the relationship between exercises and knowledge concepts, where \(q_{ij}=1\) if exercise \(e_{i}\) involves concept \(k_{j}\) and \(q_{ij}=0\) otherwise. For the exercise-solving sequence for each student during the learning process, we denote it with \(\mathcal{R}=\{(e_{k_{1}},k_{1}),(e_{2},k_{2},r_{2}),\ldots,(e_{T},k_{T},r_{T})\}\), where the triplet \((e_{t},k_{T})\) is the \(t\)-th learning iteration behavior, and \(e_{t}\in\mathcal{E}\), \(k_{t}\in\mathcal{K},r_{t}\in\{0,1\}\) represent the answered question, the related knowledge concept and the response result, respectively.

**Problem Definition.**_Given students' learning sequence \(\mathcal{R}=\{(e_{1},k_{1},r_{1}),(e_{2},k_{2},r_{2}),\ldots,(e_{T},k_{T},r_{T})\}\), the KT task aims to monitor students' evolving knowledge state during the learning process and predict their future performance at the next time step \(T+1\), which can be further applied to individualized students' learning scheme and maximize their learning efficiency._

## 4. Methodology

In this section, we initially provide an overall overview of our proposed framework **HD-KT** (short for **Hy**brid learning interactions **D**enoising **K**nowledge **T**racing). Subsequently, we explore each component of the model with a detailed explanation.

**Overview.** Our HD-KT model innovatively introduces the measurement of anomalous factors during the students' learning processes, effectively achieving robust knowledge tracing through the implementation of the hybrid learning interaction denoising strategy. As shown in Figure 3, the overall architecture of HD-KT consists of four main components, including the embedding layer, the knowledge state-guided anomaly detector, the student profile-guided anomaly detector, and the KTM adaptor. Specifically, by taking learning sequence, the embedding layer first outputs the vectorized representation of students, exercises and concepts. In the first detector, a knowledge concept-aware sequential variational autoencoder is designed to reconstruct the proficiency distribution of students with the dimension of knowledge concepts. Meanwhile, we leverage an effective attention mechanism with modeling students' long-term characteristics to explore anomalous interactions in the student profile-guided anomaly detector. In particular, both of these signals modeling different aspects of anomaly perception are jointly exploited to denoise and refine the sequential exercising behaviors of learners. Finally, the KTM adaptor that allows the integration of different KT models is conducted to realize the prediction of the future response performance of students.

### Embedding Layer

As is well known, the learning process of students is inherently intricate, characterized by students progressively engaging with exercises and continually enhancing their cognitive abilities (Dong et al., 2019; Zhai et al., 2019). In HD-KT, to effectively model the response interaction behaviors during the learning process of students, we consider the following elements: students, exercises, concepts, answers, and knowledge status. We define the basic unit of the learning process as the triplet _exercise-concept-response_ and construct an embedding layer to encode them with trainable parameter matrices. Specifically, for the \(t\)-th exercising behavior \((e_{t},k_{t},r_{t})\) of student \(s\), we transform them into the corresponding embedded representations by multiplying their one-hot vectors with the parameter matrices:

\[\mathbf{x}_{e_{t}}=\mathbf{e}_{i}\mathbf{W}^{E},\ \mathbf{x}_{a_{t}}=\mathbf{a}_{t} \mathbf{W}^{A}, \tag{1}\]

where \(\mathbf{e}_{t}\in\mathbb{R}^{M}\) and \(\mathbf{a}_{t}\in\mathbb{R}^{2C}\) denote the one-hot vector of the exercise and the response interaction, respectively; \(\mathbf{x}_{e_{t}}\in\mathbb{R}^{d_{e}}\) and \(\mathbf{x}_{a_{t}}\in\mathbb{R}^{d_{a}}\) stand for their embeddings representations; \(\mathbf{W}^{E}\in\mathbb{R}^{M\times d_{e}}\) and \(\mathbf{W}^{A}\in\mathbb{R}^{2C\times d_{a}}\) denote the trainable weight matrices; \(d_{e}\) and \(d_{a}\) are corresponding dimensions. In particular, \(\mathbf{a}_{t}\) here is the response interaction vector representing the knowledge performance, which is obtained by combining the knowledge concept \(c_{t}\) and the answer \(r_{t}\):

\[\mathbf{a}_{t,i}=\begin{cases}1,i=k_{t}+C\cdot r_{t}\\ 0,otherwise\end{cases} \tag{2}\]

Furthermore, we introduce an adaptable embedding representation \(\mathbf{x}_{a}=\mathbf{s}\mathbf{W}^{S}\) for student \(s\) to delineate its profile, which supports the consistency of knowledge evolution, thus facilitating the exploration of the learning trajectory, where \(\mathbf{s}\in\mathbb{R}^{N}\) denotes the one-hot vector of student \(s\), \(\mathbf{x}_{s}\in\mathbb{R}^{d_{s}}\) is the global student profile, \(W^{S}\in\mathbb{R}^{N\times d_{a}}\) denotes the trainable weight matrix, and \(d_{s}\) is the corresponding embedding size. Finally, to effectively model each of the student's learning behaviors, with reference to (Zhu et al., 2019), we acquire the learning embedding by fusing the exercise representation and the knowledge performance representation together and employinga multi-layer perceptron (MLP) as follows:

\[\mathbf{x}_{t}=[\mathbf{x}_{\mathbf{c}_{t}}|\mathbf{x}_{a_{t}}|\mathbf{W}_{1}+ \mathbf{b}_{1}, \tag{3}\]

where \(\|\) denotes the operation of concatenating, \(\mathbf{W}_{1}\in\mathbb{R}^{(d_{e}+d_{a})\times d}\) is the weight matrix, \(\mathbf{b}_{1}\in\mathbb{R}^{d}\) is the bias term, \(d\) is the dimension. As a result, we get the representation of the learning sequence of student \(s\): \(\mathbf{X}_{s}=[\mathbf{x}_{1},\mathbf{x}_{2},\ldots,\mathbf{x}_{T}]\in\mathbb{ R}^{T\times d}\).

### Knowledge State-Guided Anomaly Detector

The consistency and gradual progression of competence growth are recognized as inherent characteristics of the student's learning process [38, 47]. Nonetheless, in real and intricate learning environments, anomalous signals can manifest due to external influences, e.g., a student correctly answers multiple questions that he has not genuinely mastered, potentially due to cheating, or a highly proficient student may inaccurately respond to straightforward exercises due to carelessness, among other possibilities. Therefore, in this part, we develop a knowledge state-guided anomaly detector to explore the anomalous signals thus enabling more effective modeling and diagnosing of student learning behaviors.

Firstly, to proficiently exploit the sequential learning behaviors of students and capture dependencies in the contextual knowledge states, the encoded bidirectional long short-term memory network (Bi-LSTM) [13] is utilized to model and process the embedded learning sequence representation \(\mathbf{X}^{s}\) as follows:

\[\mathbf{\hat{H}}_{s}^{L},\mathbf{\hat{H}}_{s}^{R}=Bi\_LSTM(\mathbf{X}_{s}, \mathbf{\Theta}_{1}), \tag{4}\]

where \(\mathbf{\hat{H}}_{s}^{L},\mathbf{\hat{H}}_{s}^{R}\in\mathbb{R}^{T\times d_{e}}\) represent the bidirectional intermediate hidden states, respectively, \(\mathbf{H}_{s}=[\mathbf{h}_{1},\mathbf{h}_{2},\ldots,\mathbf{h}_{T}]\in\mathbb{ R}^{T\times d_{e}}\) denotes the knowledge state matrix, \(Bi\_LSTM(\cdot)\) refers to the Bi-LSTM network architecture, \(\mathbf{\Theta}_{1}\) is the corresponding trainable parameterset, and \(\oplus\) stands for the element-wise addition operator. After obtaining the student knowledge states, inspired by [23], we contemplate utilizing a _Variational Autoencoder_ (VAE[11]) to reconstruct the temporal evolving competencies for capturing anomalous signals during the learning process. Specifically, we model the latent variable \(\mathbf{\hat{H}}_{s}\) to adhere to a Gaussian distribution for deriving more robust embedding as follows:

\[\mathbf{\hat{H}}_{s}\sim\mathcal{N}(\boldsymbol{\mu},\boldsymbol{\sigma}^{2}), \ \boldsymbol{\mu}=MLP_{\mu}(\mathbf{H}_{s}),\boldsymbol{\sigma}=MLP_{\sigma}( \mathbf{H}_{s}), \tag{5}\]

where \(\mathbf{\hat{H}}_{s}=[\mathbf{\hat{h}}_{1},\mathbf{\hat{h}}_{2},\ldots,\mathbf{ \hat{h}}_{T}]\in\mathbb{R}^{T\times d_{e}}\) represents the reconstructed sequential competency level consisting of the knowledge state at each time step, and both \(MLP_{\mu}(\cdot)\) and \(MLP_{\sigma}(\cdot)\) are two trainable MLP networks for learning the distribution parameters. After getting the reconstructed knowledge state sequence, we can calculate the completed reconstruction loss as follows:

\[\mathcal{L}^{Rec}=\frac{1}{T}\sum_{t=1}^{T}(\mathbf{\hat{h}}_{t}- \mathbf{h}_{t})^{2}+\mathcal{L}^{kl},\] \[\mathcal{L}^{kl}=\sum_{1\leq t\leq T}\boldsymbol{\mu}_{t}^{2}+ \boldsymbol{\sigma}_{t}^{2}-log(\boldsymbol{\sigma}_{t}). \tag{6}\]

After acquiring the reconstructed student knowledge state, intuitively, we can capture the inconsistency by integrating it with the student's initial knowledge level and inputting this combined information into the fully connected layers. Nevertheless, the minimization of the reconstruction loss can make it challenging to discern the distinctions between the aforementioned representations. Inspired by previous works [22, 52], we endeavor to leverage a convolutional neural network (CNN) to enhance the detection capacity for capturing disparities among distinct representations of the same dimension. Specifically, we concatenate the original embedding of the knowledge state with the decoded representation at each moment and utilize a convolution operator to preserve dimensional information by:

\[\boldsymbol{\alpha}_{t}=\sigma(\mathbf{C}_{t}\mathbf{W}_{2}),\] \[\mathbf{C}_{t}=Conv([\mathbf{\hat{h}}_{t}||\mathbf{h}_{t}],\mathbf{ \Theta}_{2}), \tag{7}\]

Figure 3. The overall framework of the proposed HD-KT.

where \(\mathbf{C}_{t}\in\mathbb{R}^{d_{s}}\) is the output of the convolution layer, \(\textit{conv}(\cdot)\) is a two-dimensional convolution operation with a filter size of 2\(\times\)1 and a stride of 1, \(\Theta_{2}\) is the trainable parameter of each channel, \(\mathbf{W}_{2}\in\mathbb{R}^{d_{s}\times 2}\) is the trainable parameter matrix. Notably, \(\mathbf{\alpha}_{t}\in\mathbb{R}^{2}\) denotes the relation vector, where the first dimension represents the consistency between \(\hat{\mathbf{h}}_{t}\) and \(\mathbf{h}_{t}\), while the second dimension refers to the inconsistency. Therefore, the scores can be treated as a binary distribution (i.e., consistency vs. inconsistency). To generate binary values (i.e., 0 vs. 1) and facilitate gradient back-propagation, we utilize a Gumbel-Softmax function (Gumbel and Softmax, 2017; Gumbel and Softmax, 2017; Gumbel and Softmax, 2017) to support the learning of model via:

\[\hat{\mathbf{\alpha}}_{t}=\textit{Gumbel-Softmax}(\mathbf{\alpha}_{t},\tau), \tag{8}\]

where \(\hat{\mathbf{\alpha}}_{t}\in\mathbb{R}^{2}\) denotes whether the changes in student's knowledge status is stable, \(g_{t}\) is i.i.d sampled from the Gumbel distribution as noise disturber, _Gumbel-Softmax_(\(\cdot\)) denotes the Gumbel-Softmax function and \(\tau>0\) is the temperature parameter that controls the selection distribution. When \(\tau\to 0\), \(\hat{\mathbf{\alpha}}_{t}\) approximates a one-hot vector (i.e. hard selection). When \(\tau\to\infty\), \(\hat{\mathbf{\alpha}}_{t}\) approximates a uniform distribution. When \(\tau\to 1\), the Gumbel-Softmax function is the same as the general Softmax function.

### Student Profile-Guided Anomaly Detector

Due to the unique attributes of each student within the learning process, even when subjected to the same learning experience, differential learning outcomes and memory retention may be observed. We contend that this phenomenon imparts crucial insights into sequence denoising, specifically, the fact that abnormal learning behaviors frequently exhibit substantial deviations from the individual characteristics of students throughout the learning process. Therefore, we design a student profile-guided anomaly detection module to explore the asymptotic smoothness of the student's evolving competency. Specifically, we develop an attention module as the discriminator to detect inconsistency between the learning status and the student profile, which utilizes the student representation as a query vector and assign different attention weight to each learning encoding within the learning sequence:

\[\mathbf{\beta}_{t}=\sigma(\textit{tanh}([\mathbf{x}_{t}|\hat{\mathbf{h}}_{t}]\mathbf{ W}_{3}+\mathbf{h}_{s}\mathbf{W}_{4})\mathbf{W}_{5}), \tag{9}\]

where \(\mathbf{\beta}_{t}\in\mathbb{R}^{2}\) is the \(t\)-th attention vector, \(\mathbf{W}_{3}\in\mathbb{R}^{2d_{s}\times d}\), \(\mathbf{W}_{4}\in\mathbb{R}^{d_{s}\times d}\) and \(\mathbf{W}_{5}\in\mathbb{R}^{d_{s}\times 2}\) are the trainable parameter matrix, and \(\sigma(\cdot)\) and \(\textit{tanh}(\cdot)\) denote the sigmoid and tanh activation function, respectively. Notably, the first dimension of \(\mathbf{\beta}_{t}\) represents the consistency between student response performance and student learning profile, as well as the second dimension denotes the inconsistency. Therefore, the scores can be viewed as binary distributions (i.e., consistency vs. inconsistency), and then we leverage a similar process to generate binary value for \(\mathbf{\beta}_{t}\) via:

\[\hat{\mathbf{\beta}}_{t}=\textit{Gumbel-Softmax}(\mathbf{\beta}_{t},\tau),\] \[=\frac{\textit{exp}(\log(\mathbf{\beta}_{t,i})+g_{i})/\tau}{\sum_{j=0 }^{1}\textit{exp}(\log(\mathbf{\beta}_{t,i})+g_{j})/\tau}, \tag{10}\]

where \(\hat{\mathbf{\beta}}_{t}\in\mathbb{R}^{2}\) denotes the predicted anomalous vector about the knowledge state of student \(s\), and \(\tau\) is the same temperature parameter used in formula Eq. (8) to tune the learned distribution from the Gumbel-Softmax function.

### KTM Adaptor

With the previously mentioned anomaly detectors, the proposed HD-KT model enables to detect noise components within the sequence based on signals derived from the knowledge state and student profile levels, which involves labeling a response as noise when it exhibits inconsistency with the respective student attributes or the amalgamated knowledge state. Nevertheless, in practical applications, the false positives may be introduced, leading to the inadvertent exclusion of valuable information essential for predicting student performance. Hence, we advocate the development of a more stringent criterion for the elimination of anomalous learning interaction, aimed at retaining solely dependable noise-free data while preserving valuable information. An instance is categorized as noise only when incongruities are concurrently identified in both signals, typically adhering to the principle of consensus. Formally, we generate noise-free sequences from the input sequential learning behaviors of individual KTM via the following steps:

\[\mathbf{X}_{s}^{+}=[p_{1}\mathbf{x}_{1},p_{2}\mathbf{x}_{2},\dots,p_{T} \mathbf{x}_{T}], \tag{11}\]

where \(p_{t}\in\{0,1\}\) indicates whether an learning interaction is noisy (i.e., \(p_{t}=0\)) or not, \(a_{t}\) and \(b_{t}\) denote the second dimension scalar of above mentioned \(\mathbf{\alpha}_{t}\) and \(\mathbf{\beta}_{t}\), respectively. Note that we apply the denoised signal to the embedding representation of learning sequence \(\mathbf{X}_{s}\) to support the gradient backpropagation. Particularly, we design a KTM adaptor to adapt our proposed HD-KT framework for the integration into various mainstream knowledge tracing model for predicting the feature response performance of students, and we formalize as follows:

\[\hat{y}=\text{KTM}(\mathbf{X}_{s}^{+}), \tag{13}\]

where KTM is a basic knowledge tracing model (e.g., DKT, LPKT, etc.), which takes the denoised learning sequence representation \(\mathbf{X}_{s}^{+}\) as input, and outputs the predicted future performance \(\hat{y}\).

### Model Optimization

In the training phase, we mainly evaluate the performance of the predicted student's responses in the interaction sequences. Similar to (Zhu et al., 2017; Zhu et al., 2017), the binary cross entropy loss function between the predicted value \(\hat{y}_{t}\) of student \(s\) at time step \(t\) and the ground truth \(r_{t}\) is utilized, as follows:

\[\mathcal{L}^{Pre}=-\sum_{t=1}^{T}\left(r_{t}\log(\hat{y}_{t})+(1-r_{t})\log(1- \hat{y}_{t})\right). \tag{14}\]

where \(\mathcal{L}^{Pre}\) represents the prediction loss. Meanwhile, we also introduce the reconstruct loss to enhance the stability of parameter training of the anomaly detector according to Eq. (6), and build the final training loss as follows:

\[\mathcal{L}=\frac{1}{|\mathcal{S}|}\sum_{s\in\mathcal{S}}(\mathcal{L}_{s}^{Pre} +\mathcal{L}_{s}^{Rec})+\lambda\|\Theta\|_{2}^{2}, \tag{15}\]

where \(\lambda\) represents the hyperparameter of \(L_{2}\) regularization strength, and \(\Theta\) is the set of all model parameters. The objective function was minimized using Adam optimizer (Kingma and Ba, 2015) on mini-batches. More details of settings are specified in the part of experiments.

## 5. Experiment

In this section, we conduct a series of experiments using four real-world benchmark datasets to validate the efficacy of our proposed model. We aim to address the subsequent research questions:

* **RQ1**: Can our proposed HD-KT framework effectively enhance the performance and robustness of the existing KT models?
* **RQ2**: What benefit does each component of the proposed HD-KT model offer?
* **RQ3**: Does our approach facilitate the analysis of question quality and enable student clustering based on learning behaviors?

### Experimental Setting

#### 5.1.1. Datasets

In this paper, we conducted our experiments on four public benchmark datasets, i.e., ASSISTment12, ASSISTment17, Slepemapy.cz, and Junyi. The ASSISTment12 dataset, referenced in (Brockman et al., 2016), was collected from the ASSISTments online tutoring system and encompasses student activity data for the academic year 2012-2013. ASSISTment17 (Shen et al., 2016) was released during the ASSISTments Longitudinal Data Mining Competition in 2017. The dataset Slepemapy.cz (Shen et al., 2016) originates from an online adaptive system, i.e., slepemapy.cz, for practicing geography. The Junyi dataset (Brockman et al., 2016) was collected from the Junyi Academy, an E-learning platform established in 2012. To optimize calculation efficiency, we followed (Shen et al., 2016) to set the maximum sequence length to 50 and truncate the learning sequences exceeding this length into multiple sub-sequences. To ensure reasonableness, we screened out the sequences with lengths less than 5. The statistics of four datasets are shown in Table 1.

#### 5.1.2. Evaluation Metrics

We employed both accuracy (ACC) and the area under the receiver operating characteristics curve (AUC) as metrics to assess the efficacy of various methods in predicting the binary outcomes of future student responses to exercises.

#### 5.1.3. Baseline Methods

To validate that our proposed HD-KT framework can significantly enhance the performance of different KT models, we selected three representative KT models as the backbone, including DKT, HawkesKT, and LPKT. The details are displayed as follows:

* **DKT**(Shen et al., 2016) pioneered the use of Recurrent Neural Networks (RNNs) to model students' knowledge states, inferring current exercise performance from past learning records. In our implementation, we employed the LSTM architecture.
* **HawkesKT**(Shen et al., 2016) posits that students' proficiency in each knowledge concept is influenced not only by prior interactions with that concept but also by other relevant concepts, termed as cross-effects among knowledge concepts. HawkesKT employs collaborative filtering and matrix factorization to discover the temporal cross-effects between different concepts.
* **LPKT**(Shen et al., 2016) distinguishes students' absorption of knowledge and forgetting of knowledge during the learning process through specially designed learning gates and forgetting gates respectively. The state undergoes intermittent updates via a straightforward weighted blend of both learning and forgetting factors.

We applied our framework to these models, resulting in three variants named HD-DKT, HD-HawkesKT, and HD-LPKT. Additionally, we selected two representative anomaly information section methods in the field of time series and sequential recommendation to serve as the baselines, including:

* **DSAN**(Shen et al., 2016), known as the dual sparse attention network, is designed to pinpoint items in a recommendation system that diverge from the user's anticipated preferences by assigning unique weights to each item in the sequence. In our experiment, we treated students and exercises as target item and interactive items, respectively. By integrating DSAN with various KT models, we leveraged the unique weights within DSAN to detect anomalous data.
* **GDN**(Shen et al., 2016), known as the graph deviation network, is a prediction-based multivariate temporal anomaly detection method leveraging graph attention (GAT) to capture the relationships within each feature of the time series data. In our experiments, we treat the sequential knowledge states on different concepts as the multivariate time series.

Moreover, we compared our HD-KT with the state-of-the-art robust KT model, that is,

* **DTransformer**(Shen et al., 2016), which introduces a unique transformer-based architecture combined with a novel training paradigm to achieve consistent and reliable knowledge state tracing.

#### 5.1.4. Implementation Details

In our experiment, we performed 5-fold cross-validation. Specifically, the 80% of the learning sequences are split as the training set (70%) and the validation set (10%), while the rest 20% are used as the test set. We faithfully implemented DKT, HawkesKT and LPKT based on their original papers. To be specific, if parameters were consistent across various datasets in the original paper, we retained them as described (e.g., all parameters for HawkesKT). However, if the sensitivity to datasets was indicated, we performed parameter tuning on the validation set, adhering to the value ranges specified in the original works (e.g., parameters for DKT). We performed hyperparameter tuning for each KTM combined with HD based on the validation set. We searched the embedding size in (Krizhevsky et al., 2012; He et al., 2012; He et al., 2012; Zhang et al., 2013), hidden size in (Krizhevsky et al., 2012; He et al., 2012; He et al., 2012; He et al., 2012), and dropout rate in \([0,0.1,0.2,0.25]\). We used the Adam algorithm (Kingma and Ba, 2014) as the optimizer. All experiments were implemented with PyTorch by Python and conducted with GeForce RTX4090 GPU.

### Overall Performance (RQ1)

To verify the effectiveness of our HD-KT framework, we conducted future students' performance prediction experiments in the above four datasets. Table 2 shows the experimental results of the proposed HD-KT implemented in three KTMs compared with the baselines. First, it is clear that integrating our HD-KT framework to filter out the anomalous learning interaction has resulted in marked improvements in the performance of various KT models on all the

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Dataset & \#Students & \#Concepts & \#Exercises & \#Interactions \\ \hline ASSISTment17 & 25.3k & 245 & 50.9k & 2,621.3k \\ ASSISTment17 & 1.7k & 102 & 3.2k & 942.8k \\ Slepemapy.cz & 81.7k & 1,458 & 2.9k & 9,786.5k \\ Junyi & 175.4k & 40 & 0.7k & 25,670.2k \\ \hline \hline \end{tabular}
\end{table}
Table 1. Statistics of all datasets.

[MISSING_PAGE_FAIL:7]

original dataset. Additionally, the right side of Figure 5 presents the proportion of noise data correctly identified by our HD-LPKT after introducing varying percentages of noise data into each student's learning sequence. It can be observed that even added 2% noise data, our framework can still accurately capture approximately 70% of them. Moreover, as the amount of noise data increases, the effective detection rate of our model gradually rises. This is because the more noise introduced, the more volatile the student's knowledge state becomes, making it easier for our model to detect.

#### 5.4.2. One Case Study of KT on Junyi Dataset

In this case study, we showcased the results of knowledge tracing for student #14's learning sequence in the Junyi dataset using both HD-LPKT and LPKT. The results are presented in Figure 6. We can observe that our HD-LPKT model identified the second interaction with exercise #26 as anomalous, leading to the HD-LPKT and LPKT models diagnosing the student's mastery level of the knowledge concept "\(\epsilon_{1}\): Isoseles triangle" as 0.86 and 0.63, respectively. Subsequently, we found that for future answer predictions related to exercise #18, which is associated with the knowledge concept \(c_{1}\), our HD-LPKT model could predict accurately, while LPKT could not. Moreover, for the knowledge concept "\(\epsilon_{6}\): Square", which potentially relates to \(c_{1}\), our model also predicts the student's future performance more effectively. This validates that our HD-KT framework can robustly diagnose students' knowledge states by removing anomalous data from learning interactions.

#### 5.4.3. Exercise Quality Analysis

In online learning systems, an important task is to evaluate the quality of exercises, since high-quality exercises can more precisely track the students' knowledge states. Our method enables the detection of anomalous learning interactions within the data, facilitating an analysis of the proportion of anomalies across various exercises during data interaction. Figure 7 illustrates the distribution of exercise across different anomaly proportions in ASSISTment17. This result can serve as a basis for exercise quality analysis, whereby exercises detected with a higher anomaly rate can be revisited and reviewed by domain experts.

#### 5.4.4. Learning Behavior-based Student Clustering

As previously mentioned, some anomalous interactions in a student's learning sequence result from their learning behavior, such as carelessness. Here, we identify student groups with similar learning behaviors by analyzing the detected anomalous interactions from our HD-KT. Specifically, we first computed the proportion of detected anomalous interactions per student, for each knowledge concept, relative to all interactions associated with that concept. Subsequently, we utilized these proportions as feature vectors, representing potential anomalous behaviors of students across various knowledge concepts. These vectors were visualized after dimensionality reduction using t-distributed stochastic neighbor embedding (t-SNE). As shown in Figure 8, students with similar anomalous behavior are grouped into distinct clusters. These separated student groups assist educators in identifying representative student behavior patterns, enabling the creation of more tailored online learning experiences.

## 6. Conclusion

In this paper, we proposed a novel framework, termed **HD-KT**, to enhance the robustness of existing knowledge tracing (**KT**) methodologies with Hybrid learning interactions **D**enoising approach. In HD-KT, two detectors for anomalous learning interactions (namely knowledge state-guided anomaly detector and student profile-guided anomaly detector) were specially designed. More specifically, in the first detection module, a sequential autoencoder was designed to identify anomalous learning interactions by detecting atypical student knowledge states. In the second module, an attention mechanism was incorporated by modeling a student's long-term profile to capture irregular interactions. Extensive experiments validate the significant advantages of our HD-KT from multiple aspects. On the one hand, HD-KT markedly boosts both the robustness and accuracy of prevailing KT models. On the other hand, the HD-KT can facilitate exercise quality analysis and learning behavior-based student clustering.

Figure 8. Student clustering based on the proportion of detected anomalous interactions, wherein we sampled 1000 students in ASSISTment12. We used K-means to cluster the students and marked them with different colors accordingly.

Figure 6. A case study of HD-LPKT. This experiment shows the knowledge proficiency radar chart of the student with ID #14 in the Junyi data set using LPKT or HD-LPKT.

Figure 7. Distribution of anomalous proportions for exercises in ASSISTment17’s learning interactions.

## References

* (1)
* Abdolmal et al. (2021) Ahmed Abdolmal, Zhuanghu Liu, and Tomer Lancewicki. 2021. Practical approach to asynchronous multifarious time series anomaly detection and localization. In _Proceedings of the 27th ACM SIGIR conference on knowledge discovery & data mining_, 2485-2494.
* Aulbert et al. (2020) Julien Aulbert, Pietro Michaili, Frederic Guyard, Sebastien Marti, and Maria A Zulugao. 2020. Usual: unsupervised anomaly detection on multivariate time series. In _Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery & data mining_, 3395-3404.
* Borghesi et al. (2019) Andrea Borghesi, Andrea Bartolini, Michelle Lombardi, Michela Milano, and Luca Danielli. 2019. Anomaly detection using autoencoders in high performance computing systems. In _Proceedings of the AAAI Conference on artificial intelligence_, vol. 60. Vol. 33, 4928-4933.
* Cao et al. (2006) Hao Cao, Kenneth Koedinger, and Brian Junker. 2006. Learning factors analysis-a general method for cognitive model evaluation and improvement. In _Proceedings of 2006 International Conference on Intelligent Tutoring Systems_. Springer, 164-175.
* Chang et al. (2015) Huw-Shuan Chang, Hwai-Jung Hsu, and Kun-Ta Chen. 2015. Modeling escape relationships in e-learning a unified approach. In _EDM_, 532-535.
* Corbett and Anderson (1994) Albert T Corbett and John R Anderson. 1994. Knowledge tracing: modeling the acquisition of procedural knowledge. _User modeling and user-adapted interaction_, 4, 253-278.
* Deng and Hooi (2021) Ailin Deng and Bryan Hooi. 2021. Graph neural network-based anomaly detection in multivariate time series. In _Proceedings of the AAAI conference on artificial intelligence_ number V. Vol. 3, 4027-4035.
* Feng et al. (2009) Mingyu Feng, Ned Hellerman, and Kenneth Zoelinger. 2009. Addressing the assessment challenge with an online system that tutos as it assesses. _User modeling and user-adapted interaction_, 19, 82-86.
* Ghosh et al. (2020) Antitra Ghosh, Neil Hefferman, and Andrew S Lan. 2020. Context-aware attentive knowledge tracing. In _Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining_, 2330-2339.
* Guo et al. (2021) Xiaoguen Guo, Zhiqi Huang, Edo Gao, Mingyu Shang, Maojin Shu, and Jun Sun. 2021. Enhancing knowledge tracing via adversarial training. In _Proceedings of the 29th ACM international Conference on Multimedia_, 367-375.
* Higgins et al. (2020) Irina Higgins, Lucia Matthey, Arla Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Michael, and Alexander Lerchner. 2020. Beta-vae-learning basic visual concepts with a constrained variational framework. In _International conference on learning representations_.
* Huang et al. (2003) Shayan Huang, Zhao Liu, Xiangyu Zhao, Weiqi Luo, and Jian Weng. 2003. Toward robust knowledge tracing models via k-sparse attention. In _Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval_, 2441-2445.
* Huang et al. (2015) Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional lstm-eff models for sequence tagging. _arXiv preprint arXiv:1508.01991_.
* Kister et al. (2017) Tanja Kister, Severin Klingle, Alexander G Schwing, and Markus Gross. 2017. Dynamic bayesian networks for student modeling. _IEEE Transactions on Learning Technologies_, 1, 45:60-626.
* Ke et al. (2022) Fuetai Ke, Weiguang Wang, Wicong Tan, Lan Du, Yuan Jin, Yujian Huang, and Hongliu Yin. 2022. Infist: a hierarchical transformer model for session-aware knowledge tracing. _arXiv preprint arXiv:2212.17329_.
* Kille et al. (2016) Muhammad Kille, Robert V Lindsey, and Michael C Meurer. [n. d.]. How deep is knowledge tracing in _Educational Data Mining_, 94.
* Kim et al. (2023) Dae-Em Kim, Changdi Hong, and two Iytun Hwan. 2023. Efficient transformer-based knowledge tracing for a personalized language education application. In _Proceedings of the Tenth ACM Conference on Learning Sociology_, 336-340.
* Kingma and Ba (2014) Diederik P Kingma and Jimmy Ba. 2014. Adam: a method for stochastic optimization. _arXiv preprint arXiv:1412.6080_.
* Kingma and Ba (2014) Diederik P Kingma and Jimmy Ba. 2014. Adam: a method for stochastic optimization. _arXiv preprint arXiv:1412.6080_.
* Kingma and Ba (2022) Worseng Deng, Jaeyou Chun, Yongmin Lee, Kyoungpoo Park, and Sungrae Park. 2022. Contrastive learning for knowledge tracing. In _Proceedings of the ACM Workshop Conference on_, 2223, 2303-2358.
* Li et al. (2019) Dami L, Duchene Chen, Baibong Jin, Lei Shi, Jonathan Gu, and See-Kiong Ng. 2019. Mad-gpus: multivariate anomaly detection for time series data with generative adversarial networks. In _International conference on artificial neural networks_. Springer, 703-716.
* Fan et al. (2021) Zewen Li, Fan Liu, Wenjie Yang, Shoohang Peng, and Jun Zhou. 2021. A survey of convolutional neural networks: analysis, applications, and prospects. _IEEE transactions on neural networks and learning systems_.
* Lin et al. (2020) Shuyu Lin, Ronald Clark, Robert Birke, Sandrine Schlichorn, Niki Trigoni, and Stephen Roberts. 2020. Anomaly detection for the series using wse-lstm hybrid model. In _ICASSP 2020:3020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, dec 322-326.
* Liu et al. (2021) Qi Liu, Shuanghong Shen, Zhenya Huang, Enhong Chen, and Yonghee Zheng. 2021. A survey of knowledge tracing. _arXiv preprint arXiv:2105.15106_.
* Lu et al. (2022) Yu Lu, Penghe Chen, Yang Pan, and Vincent W Zheng. 2022. Cmkl: concept map driven knowledge tracing. _IEEE Transactions on Learning Technologies_, 15, 4:67-480.
* Ma et al. (2022) Haiping Ma, Jingyuan Wang, Henghua Zhu, Xin Xia, Haifeng Zhang, Xingyi Zhang, and Lei Zhang. 2022. Reconsigning cognitive modeling with knowledge forgetting: a continuous time-aware neural network approach. In _Proceedings of the 31st International Joint Conference on Artificial Intelligence_, 2174-2181.
* Nguyen (2015) Yun Nguyen. 2015. The effectiveness of online learning beyond a significant difference and future horizons. _MEIDT Journal of online learning and tracking_, 11, 2, 309-319.
* Ossiannisson (2020) Ebba Ossiannisson. 2020. Sustainability: special issue" the futures of education in the global context: sustainable distance education. _Sustainability of (2020)_.
* Pandey and Kappis (2019) Shaimi Pandey and George Kappis. 2019. A self-ethnic model for knowledge tracing. _arXiv preprint arXiv:1907.06857_.
* Pang et al. (2019) Gunsmag Pang, Chunhua Shen, and Anton van den Hengel. 2019. Deep anomaly detection with deviation networks. In _Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining_, 353-362.
* Papoulaek et al. (2016) Jan Papoulaek, Rudek Palacio, and Vital Szanki. 2016. Adaptive geography practice data. _Journal of Learning Analytics_, 3, 2, 317-321.
* Palikyan et al. (2018) Thamaporn Palikyan, Neil T Tefferman, and Ryan S Baker. 2018. Assistments longitudinal data mining competition 2017: a practice. In _Proceedings of the Workshop on Scientific Findings from the ASSISTments Digital Data Competition, International Conference on Educational Data Mining_.
* Pavlio et al. (2009) Philip Pavlio, Jr. Hao Cen, and Kenneth R Kaedinger. 2009. Performance factors analysis-a new alternative to knowledge tracing. Online Submission.
* Pelanek (2017) Rudek Pelanek. 2017. Bayesian knowledge tracing, logistic models, and beyond: an overview of learner modeling techniques. _User Modeling and User-Adapted Interaction_, 27, 313-350.
* Piech et al. (2019) Chris Piech, Jonathan Bussean, Jonathan Huang, Surya Gangali, Mehran Sahami, Leonidas G Gullus, and Jascha Sohl-Dickstein. 2019. Deep knowledge tracing. _Advances in neural information processing systems_, 28.
* Romero and Ventura (2010) Cristobal Romero and Sebastian Ventura. 2010. Educational data mining: a review of the state of the art. _IEEE Transactions on Systems, Man, and Cybernetics, Part C_. _Graphics and retrieval_, 60, 64:618-618.
* Shen et al. (2021) Lifeng Shen, Zhenghong Yu, Qinxi Ma, and James T Kwok. 2021. Time series anomaly detection with multiresolution ensemble decoding. In _Proceedings of the AAAI Conference on Artificial Intelligence_ number I. Vol. 3, 9567-9575.
* Shen et al. (2021) Shuanghong Shen, Qi Liu, Enhong Chen, Zheng Huang, Wei Huang, Yu Yu, and Shiliu Wang. 2021. Learning process-consistent knowledge tracing. In _Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining_, 1452-1460.
* Song et al. (2021) Xiangyu Song, Jianxin Li, Qi Lei, Wei Zhao, Yunliang Chen, and Ajun Mian. 2021. Blei-a joint cognitive learning based knowledge tracing. _Knowledge-Based Systems_, 241, 108274.
* Sun et al. (2021) Xiangyu Song, Jianxin Li, Yifu Tang, Taige Zhao, Yunliang Chen, and Ziyu Guan. 2021. Jit: a joint graph convolutional network based deep knowledge tracing. _Information Science_, 350, 510-523.
* Su et al. (2019) Tai Su, Youqian Zhao, Chenhao Niu, Rong Liu, Wei Sun, and Dan Pei. 2019. Robust anomaly detection for multivariate time series through stochastic recurrent neural network. In _Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining_, 2828-2837.
* Wang et al. (2021) Chenyang Wang, Weiqiu Ma, Min Zhang, Chuanchang Lv, Fengyuan Wan, Huijie Liu, Tao Zhang, Yiqin Liu, and Shaoping Ma. 2021. Temporal cross-effects in knowledge tracing. In _Proceedings of the 14th ACM International Conference on Web Search and Data Mining_, 517-525.
* Wang et al. (2019) Qiuyong Wang, Hongchi Yin, Hao Wang, Quce Viet Hung Nguyen, Zi Huang, and Lizhen Cui. 2019. Enhancing collaborative filtering with generative augmentation. In _Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, 548-555.
* Wu et al. (2017) Lei Wu, Yong Ge, Li Liu, Hang Chen, Richang Hong, Junping Du, and Meng Wang. 2017. Modeling the evolution of users' preferences and social links in social networking reviews. _IEEE Transactions on Knowledge and Data Engineering_, 29, 6, 1240-1253.
* Wu et al. (2021) Siqing Wu, Ying Li, Davi Zhang, Yang Zhou, and Zhenghui Ma. 2021. Topic-scale: generating commonsense knowledge-aware dialogue responses towards the recommended topic fact. In _Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence_, 3766-3772.
* Yang et al. (2021) Yang Yang, Jian Shen, Yannu Qu, Yunfei Liu, Keong Wang, Yuanqing Zhu, Wenjian Zhang, and Yong Yu. 2021. Gait: a graph-based interaction model for knowledge tracing. In _Machine Learning and Knowledge Discovery in Databases European Conference_. ICML PKDD 2020, Ghent, Belgium, September 14-18, 2020.
* Yeung and Yeung (2018) Chun-ku Yeung and Chi-Yan Yeung. 2018. Addressing two problems in deep knowledge tracing via prediction-consistent regularization. In _Proceedings of the fifth annual ACM conference on learning at scale_, 1-10.
* Yin et al. (2023) Yu Yin, Le Dai, Zhenyang Huang, Shanqhong Shen, Fei Wang, Qi Liu, Enhong Chen, and Xin Li. 2023. Tracing knowledge instead of patterns: stableknowledge tracing with diagnostic transformer. In _Proceedings of the ACM Web Conference 2013_, pages 835-846.
* [49] Junliang Yu, Min Gao, Hongzhi Yin, Jundong Li, Chongming Gao, and Qiuyong Wang. 2019. Generating reliable friends via adversarial training to improve social recommendation. In _2019 IEEE international conference on data mining (ICDM)_. IEEE, 768-777.
* [50] Jiahao Yuan, Zhan Song, Mingyong Sun, Xiaoting Wang, and Wayne Xin Zhao. 2021. Dual sparse attention network for session-based recommendation. In _Proceedings of the AAAI conference on artificial intelligence_. Vol. 35, 4635-4643.
* [51] Michael V Yatelosh, Kenneth R Kaedinger, and Geoffrey J Gordon. 2013. Individualized bayesian knowledge tracing models. In _Artificial Intelligence in Education: 16th International Conference, AIID 2013, Knights, TN, USA, July 9-13, 2013. Proceedings_. 16. Springer, 171-180.
* [52] Sergey Zagoruyko and Nikos Komodakis. 2016. Paying more attention to attention: improving the performance of convolutional neural networks via attention transfer. _arXiv preprint arXiv:1612.02938_.
* [53] Jian Zhang, Xingxi Shi, Irwin King, and Dit-Yan Yeung. 2017. Dynamic key-value memory networks for knowledge tracing. In _Proceedings of the 26th international conference on World Wide Web_. 765-774.
* [54] Hang Zhao et al. 2020. Multivariate time-series anomaly detection via graph attention network. In _2020 IEEE International Conference on Data Mining (ICDM)_. IEEE, 841-850.
* [55] Kun Zhou, Hui Yu, Wayne Xin Zhao, and Ji-Rong Wen. 2022. Filter-enhanced limb is all you need for sequential recommendation. In _Proceedings of the ACM web conference_. 2022, 2388-2399.
* [56]