[MISSING_PAGE_FAIL:1]

or approximation methods (Selvin and Komodakis, 2017; Komodakis et al., 2018). However, as demonstrated by relevance-based word embedding (Selvin and Komodakis, 2017), relevance information cannot simply be captured by models trained with syntactic, semantic, and proximity based objectives. And since generative retrieval models conduct optimization with fixed document IDs, inappropriate initial construction of document IDs leads to a bottleneck inherently influencing the effectiveness of generative retrieval models. We address this issue by presenting a novel pre-training phase for initial document ID construction. Here, we transform the encoder-decoder generative retrieval model to a special dense retrieval model, with a relevance-based objective trained on the target task. The trained document representations are then decomposed into multiple vectors using residual quantization (RQ) (Choi et al., 2016; Chen et al., 2016) that has proven to be a successful approximation for relevance-based representations.

We conduct experiments on standard large-scale information retrieval benchmarks, including MSAMRCO (Chen et al., 2016) and TREC 2019-20 Deep Learning Track data (Chen et al., 2016; Chen et al., 2016), The retrieval collection consists of 8.8 million passages. Our approach achieves substantial improvements compared to state-of-the-art generative retrieval models in all settings. For example, our RIPOR framework2 outperforms the best performing generative retrieval model by 30.5% in terms of MRR@10 on MSMARCO. In most settings, our model also shows better performance compared to popular dense retrieval models, such as DPR (Komodakis et al., 2018), ANCE (Selvin and Komodakis, 2017), MarginMSE (Komodakis et al., 2018), and TAS-B (Komodakis et al., 2018). Therefore, this paper sets an important milestone in generative retrieval research by demonstrating, for the first time, the feasibility of developing generative retrieval models that perform effectively at scale, and paying the path towards their implementation in real-world applications. To foster research in this area, we open-source our implementation and release the learned model parameters.3

Footnote 2: RIPOR stands for relevance-based identifiers for prefix-oriented ranking.

## 2. Introduction to Generative Ir

In generative document retrieval, each document is symbolized by a unique identifier, known as document ID or _DocID_ for short. Pre-trained encoder-decoder models, such as TS (Selvin and Komodakis, 2017), are employed to generate a list of document IDs in response to a given query. Let \(M\) represent a generative retrieval model that represents a document \(d\) using the document ID \(c_{d}=[c_{1}^{d},c_{2}^{d},\dots,c_{L}^{d}]\) of length \(L\). Various methods are applied to the DocID construction (Chen et al., 2016; Komodakis et al., 2018; Komodakis et al., 2018). For instance, DSI (Selvin and Komodakis, 2017) employs the hierarchical k-means over the document embeddings obtained from the pre-trained BERT model (Devlin et al., 2018). Once the tree is built, each root-to-leaf path is used as a unique document ID.

As depicted in Figure 1, \(M\) is trained to generate document IDs autoregressively for any given query \(q\), meaning that it generates each DocID token \(c_{i}^{d}\) conditioned on previously generated tokens, denoted by \(c_{ci^{d}}^{d}\). Therefore, the model generates a conditional hidden representation for the \(t^{\text{th}}\) DocID token as follows:

\[\mathbf{h}_{t}^{d}=\text{Decoder}(c_{<i}^{d};\text{Encoder}(q))\in\mathbb{R}^{ D}.\]

where \(c_{<i}^{d}=[c_{1}^{d},c_{2}^{d},\dots,c_{i^{d}-1}^{d}]\) is fed to the decoder as its input and the encoded query vector is used to compute cross-attentions to the decoder. In generative retrieval, each DocID token is associated with a \(D\)-dimensional representation. Let \(\mathbf{E}_{i}\in\mathbb{R}^{V\times D}\) denotes a token embedding table for each position \(i\) in the DocID sequence, where \(V\) is the vocabulary size for DocID tokens, i.e., the number of distinct tokens for representing document IDs. Therefore, the representation associated with each DocID token \(c_{i}^{d}\) is represented as \(\mathbf{E}_{i}[c_{i}^{d}]\in\mathbb{R}^{D}\). Note that the DocID token embedding matrices are distinct, thus \(\mathbf{E}_{i}\neq\mathbf{E}_{j}:\forall i\neq j\).

Inspired by seq2seq models(Chen et al., 2016; Komodakis et al., 2018; Komodakis et al., 2018), existing generative retrieval models estimate relevance scores based on log-conditional probability as follows:

\[S(q,c_{d}) =\log p([c_{1}^{d},c_{2}^{d},\dots,c_{L}^{d}]|q)\] \[=\sum_{i=1}^{L}\log p(c_{i}^{d}|q,c_{ci}^{d})\] \[=\sum_{i=1}^{L}\left[\text{LogSoftmax}(\mathbf{E}_{i}\cdot \mathbf{h}_{1}^{d})[c_{i}^{d}]\right]\]

where \(S(q,c_{d})\) denotes the scoring function for a query-document pair. In this paper, we instead adopt a conditional to fine-tuning with a relevance-based objective. Upon training, RIPOR employs Residual Quantization (RQ) (Goodfellow et al., 2014) to derive a unique identifier for each document. Subsequently, following Pradeep et al. (Pradeep et al., 2017), Wang et al. (Wang et al., 2019), Zhuang et al. (Zhang et al., 2019), we leverage a seq2seq pre-training approach for pre-training the model using pseudo queries generated from the documents. Next, we introduce a novel rank-oriented fine-tuning procedure for refining the parameters of model \(M\). In the next two sections, we elucidate the motivations and methodologies behind the two major novel components in RIPOR: prefix-oriented ranking optimization and relevance-based document ID construction. A detailed description of the entire optimization pipeline in presented in Section 3.3.

### Prefix-Oriented Ranking Optimization

State-of-the-art generative retrieval models, such as LTRGR (Zhuang et al., 2019), adopt a learning-to-rank loss for optimization. The objective is to ensure that \(S(q,c_{d^{+}})>S(q,c_{d^{-}})\) for a training triplet of query \(q\), relevant document \(d^{+}\) and irrelevant document \(d^{-}\). We posit that this modeling is not optimal. A primary oversight is the intrinsic nature of beam search that _sequentially_ decodes document ID tokens from left to right. Solely focusing on pairwise ranking for a full-length document ID does not guarantee that relevant documents can survive the beam search eliminations in earlier decoding steps. Therefore, we aim at developing a model that produce accurate scoring at every decoding step. Formally, we desire to satisfy the following criterion: \(S^{i}_{\text{prefix}}(q,c_{d^{+}})\geq S^{i}_{\text{prefix}}(q,c_{d^{-}}),\ \ \forall i\in[1,L]\), where \(S^{i}_{\text{prefix}}(q,d)\) denotes the relevance score produced by the generative retrieval model for the first \(i\) tokens in the document ID: \([c^{d}_{1},c^{d}_{2},\ldots,c^{d}_{i}]\).

_Margin Decomposed Pairwise Loss_. Taking inspiration from MarginMSE (Margin, 1979), a pairwise loss for knowledge distillation as follows:

\[\mathcal{L}(q,d^{+},d^{-})=\left(S(q,d^{+})-S(q,d^{-})-T_{(q,d^{+},d^{-})} \right)^{2},\]

where \(T_{(q,d^{+},d^{-})}\) denotes the golden margin, commonly predicted by a teacher model derived from a cross-encoder (Wen et al., 2017). Prior research (Margin, 1979; Zhang et al., 2019) reveals that this loss function often outperforms other pairwise losses (Wang et al., 2019) by addressing data sparsity issues in large-scale retrieval benchmark (Pradeep et al., 2017), utilizing pseudo-labels for unlabeled query-document pairs.

For generative retrieval, we extend the MarginMSE loss by modeling pairwise ranking between prefixes of \(c_{d^{+}}\) and \(c_{d^{-}}\) for each decoding step \(i\):

\[\mathcal{L}^{i}_{\text{rank}}(q,c_{d^{+}},c_{d^{-}})=\left(S^{i}_{\text{prefix} }(q,c_{d^{+}})-S^{i}_{\text{prefix}}(q,c_{d^{-}})-\alpha_{i}T_{(q,d^{+},d^{-} )}\right)^{2}.\]

Here, at each step \(i\) we re-weight the golden margin by multiplying with \(\alpha_{i}\), which is a weight we assign to each prefix position. The reason for this decision is that we emphasize on the early decoding steps of the document IDs. With this motivation, \(\alpha_{i}\) should be a monotonically increasing concave function w.r.t. \(i\). Formally, \(\alpha_{i}\) values should satisfy the following constraint: \(\alpha_{i}-\alpha_{i-1}\geq\alpha_{i+1}-\alpha_{i}\) for every \(i\). In our experiments, we use \(\alpha_{i}=\frac{1}{2}(1-\frac{\beta}{1})\), where \(Z=1-\frac{\beta}{1}\) is a normalization factor and \(\beta\) is a constant hyper-parameter. We leave the exploration of other concave functions to future work. For efficiency reasons, we only do prefix-oriented optimization for \(i=4,8,16,32\) and thus set \(\beta=2\). This concave formulation of \(\alpha_{i}\) emphasises larger sub-margins in early steps, ensuring for any query \(q\) that \(S^{i}_{\text{prefix}}(q,c_{d^{+}})\) surpasses \(S^{i}_{\text{prefix}}(q,c_{d^{-}})\). Moreover, as \(\alpha_{i}=1\), the predicted margin for full-length DocID sequences aligns with the real margin, maintaining the fidelity of ranking knowledge.

_Progressive Training_. To better learn representations aligned with the left-to-right decoding characteristic of the beam search, we draw inspiration from curriculum learning (Bishop, 2006; Krizhevsky, 2009; Hinton et al., 2012; He et al., 2015) and implement a progressive training strategy. The training process is initialized with the shortest prefix. This allows the model to first focus on basic sequence representations and build adequate capacity for the subsequent stages. As the training advances, the scope is systematically extended to the longer prefixes, culminating in training on the full-length sequence with length \(L\).

Figure 2. The overview of the RIPOR framework

During training on longer prefixes, we empirically found that the model tends to overlook previously acquired knowledge related to shorter prefixes. To mitigate this catastrophic forgetting issue, we employ multi-objective learning at each time step to ensure the retention of knowledge acquired in earlier stages. Given the training data \(\mathcal{D}=\{(q_{j},d^{+}_{j},d^{-}_{j},T_{(q_{j},d^{+}_{j},d^{-}_{j})})\}_{j=1}^{|\mathcal{D}|}\), we use the following multi-objective loss function:

\[\sum_{(q,d^{+}_{j},d^{+}_{j})\in\mathcal{D}}\ \underbrace{(\underbrace{L^{t}_{\text{rank}}(q,d^{+}_{j},d^{-}_{j})}_{(1)}+\underbrace{\sum_{k=1}^{i-1}\mathcal{L}^{k}_{\text{rank}}(q,d^{+}_{j},d^{-}_{j})}_{(2)})}_{(2)}\]

In this loss function, term (1) is responsible for acquiring the pairwise rankings specific to the current step \(i\), while term (2) ensures the model retains the ranking knowledge from previous prefixes. As mentioned earlier, for efficiency reasons, without loss of generality, we only repeat this training process for \(i=4,8,16,32\).

### Relevance-Based DocID Construction

Generative retrieval models predominantly adopt a two-step optimization approach. First, they initialize the document IDs by employing various methods such as hierarchical k-means (Zhou et al., 2017; Wang et al., 2018) or discriminative textual descriptions extracted from documents (Bahdanau et al., 2014; Wang et al., 2018). In the subsequent step, they optimize the model leveraging either cross-entropy loss (Bahdanau et al., 2014; Wang et al., 2018) or learning-to-rank loss (Wang et al., 2018), with fixed DocIDs obtained in the first step. Given that the DocIDs remain immutable in this phase, they potentially become a significant bottleneck, influencing the overall efficacy of generative retrieval models.

We argue that the design of DocIDs is crucial in two specific ways: First, it must ensure the documents with inherent similarity possess correspondingly similar DocIDs. Second, due to the characteristics of beam search for decoding in generative retrieval, these DocIDs should encapsulate a hierarchical structure. Notably, the conception of similarity in this context is nuanced; it is tied intricately to specific queries and deviates from standard linguistic similarities observed in natural language processing. Addressing these challenges, we introduce a relevance-based method for initializing DocIDs. This approach is crafted to encapsulate both the query-document relevance nuances and the necessary hierarchical structure, ensuring effective performance in generative retrieval tasks.

#### Generative retrieval model as dense encoder

To capture the relevance-based similarities among documents, we design an optimization process inspired by dense retrieval models, but by utilizing the encoder-decoder architecture in \(M\). Specifically, we input document content into the encoder and a special start token as input to the decoder. The document representation is then derived from the first contextualized output embedding of the decoder:

\[\mathbf{d}=\text{Decoder}(s_{0};\text{Encoder}(d))\in\mathbb{R}^{D}.\]

Where \(s_{0}\) is the start token. Adopting a similar approach for queries, we determine their representations. To optimize model \(M\), we employ the MarginMSE loss (Maggi et al., 2017) with multi-stage negative sampling introduced in Sec 3.3.1 in details.

#### Residual Quantization

Hierarchical k-means, which is used in (Zhou et al., 2017; Wang et al., 2018; Wang et al., 2018) for document ID construction, does not explicitly minimize the distortion error between original and approximated representations. As highlighted by Ge et al. (2017), there is a notable inverse correlation between information retrieval metrics like MAP and the distortion error, particularly for large-scale datasets. Motivated by this observation, we adopt quantization-based techniques (Bahdanau et al., 2014; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018) explicitly designed to minimize this distortion error. Among a myriad of quantization algorithms, we select Residual Quantization (RQ) (Bahdanau et al., 2014) (Bahdanau et al., 2014) due to its inherent advantages. Specifically, (1) its recursive procedure captures the hierarchical document structure, aligning with the beam search strategy inherent to generative retrieval, and (2) compared to methods like product quantization (PQ) (Maggi et al., 2017; Wang et al., 2018), it requires a shorter length of DocID to achieve a strong performance, leading to memory and time savings during inference. Using \(M\) as our dense encoder, we calculate the representation \(\mathbf{d}\) for each document \(d\). Subsequently, employing RQ, we optimize the token embedding table \(\{\mathbf{E}_{i}\}_{i=1}^{L}\) to determine the optimal DocID \(c_{d}=[c^{d}_{1},\ldots,c^{d}_{L}]\) for every document \(d\). Upon optimization, each \(\mathbf{d}\) can be approximated using a sequence of token embeddings as:

\[\mathbf{d}\approx\sum_{i=1}^{L}\mathbf{E}_{i}[c^{d}_{i}].\]

The trained model \(M\) alongside the embedding tables \(\{\mathbf{E}_{i}\}_{i=1}^{L}\) will serve as the initial weights for subsequent optimization phases within generative retrieval.

### Optimization Details

Our optimization process can be delineated into three distinct phases: (1) DocID initialization (2) Seq2seq Pre-training, and (3) Rank-oriented Fine-tuning.

#### 3.3.1. DocID initialization

As described in Section 3.2, we treat \(M\) as a dense encoder. To optimize the dense encoder \(M\), we use the recent advance of multi-stage training strategy (Wang et al., 2018). Here's the tailed steps of the multi-stage training: In the initial stage, we use BM25 (Shen et al., 2016) to sample the top \(K\) (We choose \(K=100\) in our work) documents for each query and train the model using the MarginMSE (Maggi et al., 2017) loss function. Once the model is trained, we obtain the dense representation \(\mathbf{d}\) from our model \(M\) for each document and store them in an index. For each query \(q\), we apply nearest neighbor search to retrieve the top \(K\) documents. Then, we train the model using the same loss function with the retrieved documents. After training, we then apply residual quantization (RQ) to obtain the DocID for each document. The trained model is denoted as \(M^{0}\), and the embedding tables \(\{\mathbf{E}_{i}\}_{i=1}^{L}\) will be used as the initial weights for the next phase.

#### 3.3.2. Seq2seq Pre-training

To equip our model \(M\) with a comprehensive understanding of the corpus, we incorporate a seq2seq pre-training phase. Instead of using the document \(d\) as input and predicting its corresponding semantic tokens \([c^{d}_{1},\ldots,c^{d}_{L}]\), we align with prior work (Wang et al., 2018) and utilize pseudo queries associated with each document as input proxies for DocID prediction. Specifically, by leveraging the doc2query model (Chen et al., 2019), we generate \(N_{pseudo}\) pseudo queries for every document. We then optimize the model usinga cross-entropy loss, with the tokens from the relevant DocIDs serving as the ground-truth labels. We denote the trained model in this phase as \(M^{1}\).

#### 3.3.3. Rank-oriented fine-tuning

To optimize our model, we leverage the pairwise loss as described in Sec 3.1. Literature suggests the pivotal roles of negative sampling (Zhu et al., 2017) and the quality of the supervision signal (Zhu et al., 2017; Li et al., 2018; Li et al., 2019) in enhancing the performance of ranking models. Following this, we incorporate a multi-stage training strategy (Zhu et al., 2017; Li et al., 2019) to incrementally enhance the model's capacity and extract improved negatives for subsequent stages.

Initial Fine-tuningThis stage is primarily geared towards further preparing the generative retrieval model for the ranking task and sourcing high-quality negative samples for ensuing stages. Utilizing the model \(M^{0}\) from Sec 3.3.1 as a dense encoder, we index each document via its embedded representation. We apply the Nearest Neighborhood search to retrieve the top 100 documents. The training data \(\mathcal{D}^{R}\) can be constructed based on the negative samples and ground-truth query-document positive pairs. Unlike our approach in subsequent stages, we directly utilize the full-sequence ranking loss \(\mathcal{L}^{L}_{rank}\). Starting from \(M^{1}\) as an initial model, after training, the model is represented as \(M^{2}\). This is intentional, as the primary objective here is to curate quality negative samples for later stages rather than perfecting the model.

Prefix-Oriented Ranking OptimizationGiven a query \(q\), we deploy beam search on the model \(M^{2}\) to retrieve the top 100 DocIDs, each of which is mapped back to corresponding documents. The documents serve as an augmented source of negative samples, and we subsequently construct a training set \(\mathcal{D}^{B}\) in a manner analogous to the previous section. The comprehensive training set for this stage consolidates data both from the Nearest Neighborhood Search and Beam Search, represented as \(\mathcal{D}=\mathcal{D}^{R}\cup\mathcal{D}^{B}\). To optimize the model, we utilize the progressive training described in Section 3.1. For each optimization step \(i\), we employ the multi-objective loss function described in Section 3.1. After training, the model is denoted as \(M^{3}\).

Self-Negative Fine-tuningTo enhance the model's effectiveness, we employ beam search on the most recently optimized model \(M^{3}\) to establish a training dataset \(\mathcal{D}^{B}_{self}\). Then the model is trained on the same multi-objective loss function in the full-length setting (\(i=L\)), and denoted as \(M^{4}\).

## 4. Experiments

### Experiments Settings

#### 4.1.1. Dataset

We assess our information retrieval models on the MSMARCO dataset (Chen et al., 2019), comprising 8.8M passages and 532K training queries with shallow annotations (averaging about 1.1 relevant passages per query). We evaluate our models using three datasets: (1) MSMARCO-Dev, with 7K queries and shallow annotations, (2) TREC DL 2019: the passage retrieval dataset used in 2019 TREC Deep Learning Track (Chen et al., 2019) with 43 queries and (3) TREC DL 2020: the passage retrieval dataset of TREC Deep Learning Track 2020 (Chen et al., 2019) with 54 queries. For evaluation, we report recall@10 for all datasets, as well as the official metric MRR@10 for the MSMARCO-Dev set and NDCG@10 for the TREC DL 2019 and 2020.

#### 4.1.2. **Implementation Details**

We employ the pre-trained T5-base (Zhu et al., 2017) as the backbone for our generative retrieval model. For DocID initialization, we adopt the residual quantization (RQ) implementation from Faiss (2018). The length of DocID \(L\) is 32 and the table size \(V\) is 256. For Seq2seq pre-training, the dco2query model (Chen et al., 2019) with t5-large as the backbone generates 10 pseudo queries for each document. For progressive training, we sample 4 prefixes with lengths \(4,8,16,32\). The optimization is done using Adam (Kingma and Ba, 2015), featuring linear scheduling and a warmup ratio of 4.5% of total learning steps. For DocID initialization and rank-oriented fine-tuning phases, we set the learning rate as 0.0001 with 120 epochs and batch size of 64 For Seq2seq pre-training, we set the learning rate as 0.001 with 250,000 steps and batch size of 256 We conducted all the experiments using 8 A100 GPUs.

#### 4.1.3. **Baselines**

We select a wide range of document retrieval models from generative retrieval to dense retrieval as the baselines for comparison:

* **DSI**(Zhu et al., 2017): DSI is one of the earliest generative retrieval models that apply the hierarchical k-means over document representations obtained from pre-trained BERT for DocID construction. The model utilizes cross-entropy loss for fine-tuning on the retrieval task.
* **DSI-QG**(Zhu et al., 2017): DSI-QG generates pseudo queries for each document and uses them as the augmented data for training.
* **NCI-QG**(Zhu et al., 2017): NCI-QG invents a prefix-aware weight-adaptive decoder architecture to capture position information of document identifiers, and like DSI-QG, uses the doc2query model for data augmentation.
* **SEAL**(Chen et al., 2019): SEAL employs document n-grams as identifiers, applying the FM-index to ensure valid document identifiers are decoded in response to specific queries.
* **MINDER**(Zhu et al., 2017): An extension of SEAL, MINDER constructs document identifiers from multiple document views, such as titles, pseudo-queries, and n-grams.
* **LTRGRG**(Zhu et al., 2017): LTRGR utilizes multi-view document identifiers, akin to MINDER, but shifts the loss function to a pairwise-based learning-to-rank algorithm.
* **BM25**(Zhu et al., 2017): the simple yet effective bag-of-word retrieval model that uses term-frequency, inverse document frequency, and document length for computing the relevant scores
* **DPR**(Zhu et al., 2017): DPR is a dual-encoder based dense retrieval models. It incorporates the in-batch negative and BM25 negatives for training.
* **ANCE**(Zhu et al., 2017): ANCE selects hard training negatives from the entire corpus by using an asynchronously updated ANN index.
* **MarginMSE**(Zhu et al., 2017): MarginMSE develops a distinctive loss function based on the knowledge distillation. It aims to minimize the discrepancy between the predicted margin from dense retrieval models and the golden margin from the teacher model.
* **TAS-B**(Zhu et al., 2017): Building upon MarginMSE, TAS-B designs a topic-aware sampling algorithm to enhance the model's effectiveness.

### Experiment Results

#### Main Results

We report the performance of RIPOR and other baselines MSMARCO in Table 1. First, most generative retrieval models, including DSI, DSI-QG, NCI-QG, SEAL, and MINER, consistently lag behind BM25 across all three evaluation sets. In contrast, the LTRGR model, which incorporates a learning-to-rank algorithm, manages to surpass BM25. These observations underscore the importance of integrating learning-to-rank methodologies when designing generative retrieval models. Second, our proposed RIPOR consistently outperforms other generative retrieval baselines, demonstrating a significant advantage. Notably, when compared to the top-performing baseline LTRGR, RIPOR achieves a 30.5% improvement in MRR@10 on the MSMARCO Dev set and a remarkable 94% enhancement in NDCG@10 on the TREC-20 test set. Third, our RIPOR can obtain comparable results to state-of-the-art dense retrieval models, particularly in precision-oriented metrics. For instance, compared to ANCE, our model achieves a 16% improvement in terms of MRR@10 on MSMARCO Dev and a 6.6% improvement on the two TREC DL evaluation sets in total NDCG@104. Additionally, we provide the experimental results on the small-scale dataset MSMARCO-1M, in line with previous work (Wang et al., 2018). These results can be found in Appendix Table 4.

#### Ablation Studies

We conduct a thorough ablation studies on the MSMARCO dataset to investigate the impact of each component in RIPOR. We report our study in Table 2.

Beginning with Row 1, we can see the significance of incorporating prefix-oriented ranking optimization. The absence of this optimization results in a pronounced 19% degradation in MRR@10. Without employing the optimization approach, the model fails to explicitly ensure that every prefix of relevant DocIDs receives higher scores than those of relevant DocIDs in response to a query. This increases the risk of discarding these relevant DocIDs in the early steps of beam search, which, in turn, negatively impacts information performance.

\begin{table}
\begin{tabular}{l|c c|c c|c c} \hline \hline  & \multicolumn{3}{c|}{**MSMARCO Dev**} & \multicolumn{2}{c|}{**TREC DL 2019**} & \multicolumn{2}{c}{**TREC DL 2020**} \\
347 & MRR@10 & Recall@10 & NDCG@10 & Recall@10 & NDCG@10 & Recall@10 \\ \hline
**Generative Retrieval** & & & & & & \\ DSI &.045 &.138 &.163 &.076 &.150 &.070 & 648 \\ DSI-QG &.105 &.292 &.320 &.138 &.328 &.120 & 649 \\ NCI-QG &.153 &.352 &.403 &.167 &.394 &.159 & 650 \\ SEAL &.127 & - & - & - & - & - & - \\ MNDER &.186 &.383 &.506 &.201 &.392 &.144 & 651 \\ LTRGR &.255 &.531 & - & - & - & - & 652 \\
**RIPOR (ours)** & **.333\({}^{*}\)** & **.562\({}^{*}\)** & **.628\({}^{*}\)** & **.205\({}^{*}\)** & **.631\({}^{*}\)** & **.191\({}^{*}\)** & 654 \\ \hline
**Sparse and Dense Retrieval Models (For Reference)** & & & & & & \\ BM25 &.185\({}^{\triangledown}\) &.381\({}^{\triangledown}\) &.512\({}^{\triangledown}\) &.178\({}^{\triangledown}\) &.477\({}^{\triangledown}\) &.164\({}^{\triangledown}\) & 658 \\ DPR &.287\({}^{\triangledown}\) &.539\({}^{\triangledown}\) &.588\({}^{\triangledown}\) &.195\({}^{\triangledown}\) &.581\({}^{\triangledown}\) &.182\({}^{\triangledown}\) & 657 \\ ANCE &.301\({}^{\triangledown}\) &.545\({}^{\triangledown}\) &.600\({}^{\triangledown}\) &.262\({}^{\triangledown}\) &.587\({}^{\triangledown}\) &.174\({}^{\triangledown}\) & 658 \\ MarginMSE &.312\({}^{\triangledown}\) &.552\({}^{\triangledown}\) &.634\({}^{\triangle}\) &.250\({}^{\triangle}\) &.614\({}^{\triangledown}\) &.193 & 659 \\ TAS-B &.323\({}^{\triangledown}\) &.557\({}^{\triangledown}\) &.629 &.200 &.633 &.227\({}^{\triangle}\) & 660 \\ \hline \hline \end{tabular}
\end{table}
Table 1. Experimental results on MSMARCO and TREC Deep Learning Track Data. Highest generative retrieval performances are boldfaced. Superscript \(*\) denotes statistically significant improvement compared to all generative retrieval baselines. Superscripts \({}^{\triangle}\) and \({}^{\triangledown}\) denote significantly higher and lower performance compared to RIPOR. (t-test with Bonferroni correction, p_value \(<\) 0.01). For dense retrieval models, HNSW (Herbst et al., 2017) index is used for ANN search.

\begin{table}
\begin{tabular}{l|c c} \hline \hline  & \multicolumn{3}{c}{Extra} \\ L \(\times\) V & MRR@10 & Recall@10 & Param.(M) & 667 \\ \hline
32 × 256 &.333 &.562 & 6.29 & 648 \\
16 × 512 &.307 &.520 & 6.29 & 649 \\
8 × 1024 &.306 &.535 & 6.29 & 670 \\
4 × 2048 &.273 &.493 & 6.29 & 671 \\
16 × 1024 &.324 &.554 & 12.58 & 672 \\
8 × 2048 &.319 &.550 & 12.58 & 673 \\
4 × 4096 &.291 &.528 & 12.58 & 674 \\ \hline \hline \end{tabular}
\end{table}
Table 3. The retrieval performance for various DocID combinations on MSMARCO Dev set.

In Row 2, we infer the significance of incorporating multi-objective learning within the prefix optimization. This inclusion results in a further improvement of 5% in MRR@10. The enhancement can be credited to the approach's efficacy in mitigating the forgetting issue encountered during the progressive training's latter stages. Notably, this methodology introduces only a minimal addition to the loss computation, ensuring that there is no increase in computational overhead during training.

Row 3 reports the results for RIPOR when self-negative fine-tuning is not used in the final training stage. Incorporating this strategy yields a 2.5% enhancement in MRR@10 and a 3.5% boost in Recall@10. This improvement stems primarily from the fact that hard negative samples would increase the efficacy of the retrieval model as shown in previous dense retrieval models(Wang et al., 2019). By strategically leveraging these hard negative samples, we bolster the model's capability, ensuring relevant DocIDs consistently be ranked higher than potential high-scoring hard negatives, which subsequently elevates the model's overall effectiveness.

From Row 4, we note that by integrating seq2seq pre-training, RIPOR achieves a 4% improvement in MRR@10. This method allows the model to encapsulate document information across the entire corpus, mirroring the indexing phase in dense retrieval models, and subsequently driving the observed performance improvement.

From Row 5, when we treat the generative retrieval model as a dense encoder and instead use the sentence-T5 (Wang et al., 2019) to derive the hidden representation for each document, a substantial performance degradation would happen, with a 73% drop in MRR@10, for instance. The rationale behind this decline is that sentence-T5, being pre-trained on NLP tasks, is not optimized to discern query-dependent semantic similarities between documents. Leveraging it to initialize the DocIDs disrupts the inherent semantic linkages among documents in relation to queries.

Finally, in Row 6, substituting RQ with PQ results in a substantial performance decline, evidenced by a 197% decrease in MRR@10. While PQ is recognized as a potent quantization algorithm in the dense retrieval domain, our results suggest its unsuitability for generative retrieval. This limitation may stem from PQ's inability to encapsulate the hierarchical structure among documents, an attribute that has been shown to be crucial in generative retrieval, especially when employing beam search.

### Analysis and Discussion

#### 4.3.1. The impact of DocID combination

The configuration of the Document Identifier (DocID), specifically its length \(L\) and vocabulary size \(V\), influences the effectiveness of model \(M\). We examine this relationship by evaluating various performance metrics on the MSMARCO Dev set, as detailed in Table 3. Firstly, when holding the extra parameters constant (quantified by \(L\times V\times D\)), we observe that an elongation in DocID length \(L\) corresponds to enhanced performance in both MRR@10 and Recall@10. Secondly, while maintaining a fixed DocID length \(L\) and incrementing the vocabulary size \(V\), there's a noticeable improvement in performance metrics. For instance, when \(L=16\), increasing the vocabulary size from 512 to 1024 leads to the 5.5% improvement in MRR@10.

#### 4.3.2. The quality of document approximated representation

In Section 3.2, we emphasized the importance of the relevance-based

Figure 3. Clusters of the relevant documents to 20 queries sampled from TREC DL. The color indicates the query ID.

document similarities, in influencing model performance. To prove that our model can capture these signals. We randomly selected 20 queries from TREC DL 19 and TREC DL 20, along with their corresponding relevant documents. We utilize the approximated vector representation \(\hat{\mathbf{d}}=\sum_{i=1}^{L}\mathbf{E}_{i}[c_{i}^{d}]\) and apply T-SNE (Maaten and Hinton, 2008) to project the approximated representations of each document into a 2D space for visualization. We studied clustering quality for different prefix lengths, specifically \(L=1,2,4,8,16\), and 32, as illustrated in Figure 3. First, when \(L\geq 8\), those documents with the same relevant query are located in their corresponding cluster nicely, which indicates that our RIPOR effectively draws relevant documents near while distancing the irrelevant ones. Second, the clustering quality is progressively improved when \(L\) increases. This might be because when \(L\) increases, the distance between approximated vector \(\hat{\mathbf{d}}\) and original vector \(\mathbf{d}\) diminishes, enabling the approximation to capture finer-grained ranking information.

#### 4.3.3. The influence of prefix-length

The prefix length plays a pivotal role in the RIPOR framework due to its influence on the distortion error between the original and approximated vectors. While Section 4.3.2 provides a qualitative perspective on its effects in terms of document similarities in a low-dimensional space, this section delves into its quantitative impact on retrieval performance, as depicted in Figure 4. Referring to the left figure, which displays different DocID combinations from RIPOR, several trends emerge. First, as the prefix length \(L\) grows, there's a consistent improvement in performance. Second, the rate of this performance gain is more pronounced for shorter prefix lengths, since we observe that the boost is more substantial when \(L\leq 8\) than when \(L>8\). Third, given an equal prefix length, variants with a larger vocabulary size tend to perform better. From the right figure, which contrasts RIPOR with three other selected variants from the ablation study in Section 4.3.3: First, excluding the prefix-oriented optimization invariably results in reduced performance. Second, the performance curve of the "replace with sentence-T5" variant emphasizes the critical role of DocID initialization. Its subpar performance suggests that not employing relevance-based DocID initialization is detrimental, rendering it less effective than the variant excluding prefix-oriented optimization. Third, Product Quantization (PQ) seems less compatible with generative retrieval, given its stagnant performance for \(L\geq 8\). This stagnation might be due to PQ's shortcomings in capturing the hierarchical nuances among documents, subsequently impacting the benefits drawn from longer prefix lengths.

## 5. Related Work

Pre-trained language models (LMs) (Hinton et al., 2015; Salimans et al., 2015; Salimans et al., 2015; Salimans et al., 2015) have become foundational in the field of information retrieval (IR). The integration of these LMs into neural IR models can be broadly categorized into four main streams:

Neural Sparse Retrieval models, inspired by conventional bag-of-words approaches like TF-IDF (Wang et al., 2016) and BM25 (Wang et al., 2016), adapt BERT to re-weight subwords, thereby enhancing IR performance. To maintain the sparsity of high-dimensional vectors, they utilize L1 (Yang et al., 2016) or Flop (Hinton et al., 2015) regularizers. This characteristic sparsity allows them to be incorporated into fast search frameworks based on the inverted index (Wang et al., 2016).

Re-ranking with LMs is another approach where LMs serve as re-rankers (Wang et al., 2016; Salimans et al., 2015). By feeding a concatenated query and document, these models produce a relevance score. Despite their often superior performance, they are better suited for document re-ranking due to efficiency constraints, rather than retrieval.

Dense Retrieval models, a more recent advancement, are dual-encoder-based (Hinton et al., 2015; Salimans et al., 2015; Salimans et al., 2015; Salimans et al., 2015; Salimans et al., 2015; Salimans et al., 2015; Salimans et al., 2015). Notably, they have exhibited standout performance on large-scale datasets (Sutskever et al., 2016; Salimans et al., 2015). These models, typically leveraging BERT, encode each document and query into dense representations. For efficient retrieval, they employ approximated nearest neighbor search (ANN) (Wang et al., 2016; Salimans et al., 2015).

Lastly, the Generative Retrieval paradigm is an innovative approach drawing inspiration from successful generative LMs (Hinton et al., 2015; Salimans et al., 2015; Salimans et al., 2015). In this paradigm, models like T5 are treated as retrievers. Each document is mapped to a distinct sequence, often denoted as a DocID. At inference, given a specific query, a constrained beam search (Sutskever et al., 2016; Salimans et al., 2015) retrieves a list of the most probable DocIDs.

## 6. Conclusions and Future Work

We introduced the RIPOR framework, designed to generalize generative retrieval models for large-scale datasets. We employ a novel prefix-oriented ranking optimization method to harness the sequential nature of DocID generation. By viewing generative retrieval as a dense encoder, we fine-tune it for the target task, and apply RQ for DocID construction. Our experimental results demonstrate that this DocID construction captures the relevance-based similarity among documents, thereby improving the effectiveness of the IR task. Looking ahead, we aim to further optimize the model's efficiency and integrate the framework into other knowledge-intensive NLP tasks, such as Open-domain QA.

Figure 4. The retrieval performance for different prefix lengths in MSMARO Dev.

## References

* (1)
* Babenko and Lempitsky (2014) Artem Babenko and Victor S. Lempitsky. 2014. Additive Quantitation for Extreme Vector Compression. 2014 IEEE Conference on Computer Vision and Pattern Recognition. (2014), pp. 931-938. [https://apcmi.semantichokorny.org/CrystalD125563225](https://apcmi.semantichokorny.org/CrystalD125563225)
* Bengio et al. (2009) Yoshua Bengio, Jerome Louradox, Roman Collector, and Jason Weston. 2009. Curriculum learning. In _International Conference on Machine Learning_. [https://apismarinstitucable.org/CrystalD873064](https://apismarinstitucable.org/CrystalD873064).
* Bevilacqua et al. (2022) Michele Bevilacqua, Giuseppe Ottavania, Patrick Lewis, Wen ten Yih, Sebastian Rebel, and Fabio Petron. 2022. Autoregressive Search Engines: Generating Substrings as Document Identifiers. _ArXiv_ abs/2204.10628 (2022). [https://api.scannicheskoharp.org/CrystalD82483629](https://api.scannicheskoharp.org/CrystalD82483629)
* Crambo et al. (2016) Daniel Fernando Crambo, Tim Nguyen, Miros Reusenye, Xia Song, Jianfeng Gao, Saurabhav Tiwary, Rangan Majumder, I. Deng, and Blackar Mitra. 2016. MS MARCO: A Human Generated Machine Reading Comprehension Dataset. _ArXiv_ abs/1611.04288 (2016). [https://apismarinstitucable.org/CrystalD1258917](https://apismarinstitucable.org/CrystalD1258917)
* Cao et al. (2020) Nicola De Cao, Ganitre Losev, Sebastian Rebel, and Fabio Petron. 2020. Autoregressive Unity Retrieval. _ArXiv_ abs/2010.09004 (2020). [https://api.semantichokorny.org/CrystalD2252277](https://api.semantichokorny.org/CrystalD2252277)
* 11273. [https://apscannicheskoharp.org/CrystalD87377420](https://apscannicheskoharp.org/CrystalD87377420)
* Cheriton (2019) David R Cheriton. 2019. From Adequacy to decTTITITguys. [https://api.csmnistochikro.org/CrystalD861257](https://api.csmnistochikro.org/CrystalD861257)
* Chung et al. (2020) Hyung Won Chung, Le Hou, S. Longrere, Barret Zoph, Yi Tay, William Fedus, Eric L, Axelni Wang, Mostola Dehghani, Suddhartha Brahma, Albert Webson, Shixia-ang Shawe Qin, Zhiwen Dai, Marie Simeq, Xingyuan Chen, Akanahak Chowdhury, Daha Valter, Sharan Narang, Gaurav Mishra, Adnan Wei, Vincent Zhao, Yimping Huang, Andrew M. Dai, Hongkun Yu, Siye Petrov, Elf Mati Ishai, Chi Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling Instruction-Functured Language Models. _ArXiv_ abs/2210.11416 (2022). [https://apscannicheskoharp.org/CrystalD8315854](https://apscannicheskoharp.org/CrystalD8315854)
* Caswell et al. (2019) Nick Caswell, Bhaskar Mitra, Emine Filmas, and Daniel Campos. 2019. Overview of the TEC 2015 Deep Learning Track. In _TREC_.
* Cravwell et al. (2021) Nick Cravwell, Bhaskar Mitra, Emine Filmas, Daniel Fernando Campos, and Ellen M. Voorhees. 2021. Overview of the TREC 2020 Deep Learning Track. _ArXiv_ abs/2102.07662 (2021). [https://apscannicheskoharp.org/CrystalD225273158](https://apscannicheskoharp.org/CrystalD225273158)
* Detrin et al. (2013) Jacob Detrin, Ming-Wei Chang, Keznet L and Kristina Toutanova. 2013. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In _Neurth American Chapter of the Association for Computational Linguistics_. [https://api.semantichokorny.org/CrystalD82569799](https://api.semantichokorny.org/CrystalD82569799)
* Formal et al. (2021) Thibault Formal, C. Lassance, Benjamin Piworsarski, and Stephane Clinchant. 2021. SPLAD: x-Space Lexical and Expansion Model for Information Retrieval. _ArXiv_ abs/2100.10082 (2021). [https://apscannicheskoharp.org/CrystalD825785150](https://apscannicheskoharp.org/CrystalD825785150)
* Formal et al. (2021) Thibault Formal, Benjamin Piworsarski, and Stephane Clinchant. 2021. SPLAD: Sparse Lexical and Expansion Model for First Stage Ranking. _Proceedings of the 4th International ACM SIGIR Conference on Research and Development in Information Retrieval_ (2021). [https://apscannicheskoharp.org/CrystalD245792467](https://apscannicheskoharp.org/CrystalD245792467)
* Ge et al. (2014) Tiexheng Ge, Kaiming He, Qifa Ke, and Jian Sun. 2014. Optimized Product Quantization. _IEEE Transactions on Pattern Analysis and Machine Intelligence_ 36 (2014), 744-755. [https://apscannicheskoharp.org/CrystalD835221706](https://apscannicheskoharp.org/CrystalD835221706)
* Guin et al. (2020) Keith Guin, Kenton Lee, Zoran Wang, Panpong Pasupat, and Mingxiong Chang. 2020. Retrieval augmented Language model pre-training. In _International conference on machine learning_. PMLR, 3929-3938.
* Hofstatter et al. (2020) Sebastian Hofstatter, Sophia Althammer, Michael Schroder, Mete Serkan, and Alan Hanbury. 2020. Improving Efficient Neural Ranking Models with Cross-Architecture Knowledge Distillation. _ArXiv_ abs/2010.02666 (2020). [https://api.semantichokorny.org/CrystalD2214104](https://api.semantichokorny.org/CrystalD2214104)
* Hofstatter et al. (2021) Sebastian Hofstatter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy J. Lin, and Allan Hanbury. 2021. Efficiently Teaching an Effective Device Perview with Balanced Topic Aware Sampling. _Proceedings of the 4th International ACM SIGIR Conference on Research and Development in Information Retrieval_ (2021). [https://apscannicheskoharp.org/CrystalD23237106](https://apscannicheskoharp.org/CrystalD23237106)
* Johnson et al. (2019) Jeff Johnson, Matthias Douze, and Herve Jeyon. 2019. Billion-see similarity search with GPUs. _IEEE Transactions on Big Data_ 7 (2019), 535-547.
* Karpukhin et al. (2017) Vladimir Karpukhin, Batlas Oguz, Sewon Min, Patrick Lewis, Leekli Yu, Sergey Edunov, Diane Chen, and Ivan Yu. 2017. Dense Query-Dependent Question Answering. In _Conference on Empirical Methods in Natural Language Processing_. [https://api.semantichokorny.org/CrystalD215373187](https://api.semantichokorny.org/CrystalD215373187)
* Rathb and Zahata (2020) O. Rathb and Matei A. Zahata. 2020. GENBERT: Efficient and Effective Usage Search via Contextualized Late Interaction over BERT. _Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval_ (2020). [https://apscannicheskoharp.org/CrystalD2156532323](https://apscannicheskoharp.org/CrystalD2156532323)
* Kingma and Ba (2014) Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization. _CoRR_ abs/1412.6980 (2014). [https://apscannicheskohar.org/CrystalD21458454](https://apscannicheskohar.org/CrystalD21458454)
* Robertson and Walker (1907) Stephen E. Robertson and Hugo Yang. 1907. On relevance weights with little relevance information. In _Annual International ACM SIGIR Conference on Research and Development in Information Retrieval_. [https://apscannicheskohar.org/CrystalD215373187](https://apscannicheskohar.org/CrystalD215373187)
* Robertson and Zaragoza (2009) Yury Mallor and Dmitri A. Yashim. 2010. Efficient and Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs. _IEEE Transactions on Pattern Analysis and Machine Intelligence_ 42 (2010), 824-836. [https://apscannicheskohar.org/CrystalD9358979](https://apscannicheskohar.org/CrystalD9358979)
* Mathiesen et al. (2017) Tambei Mathiesen, Aviol Pierce, Chao Schum, and John Schulman. 2017. Teacher-Student Curriculum Learning. _IEEE Transactions on Neural Networks and Learning Systems_ 11 (2017), 3732-3740. [https://apscannicheskohar.org/CrystalD8432994](https://apscannicheskohar.org/CrystalD8432994)
* Mathiesen et al. (2022) Sanlet V Mathiesen, Jiaqi Gupta, Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Jingtao Rao, Marc Najita, Emma Strubell, and Donald Metzler. 2022. Dist++. Updating Transformer Memory with New Documents. _ArXiv_ abs/2212.09744 (2022). [https://apscannicheskohar.org/CrystalD25484290](https://apscannicheskohar.org/CrystalD25484290)
* N. Ni et al. (2021) Jianno N. Ni, Gustavo Hernandez Abergo, Noah Constant, Ji Ma, Keith B. Hall, Daniel Matthew Cer, and Yarifei. 2021. Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-Text Models. _ArXiv_ abs/2108.08877 (2021). [https://apscannicheskohar.org/CrystalD237206203](https://apscannicheskohar.org/CrystalD237206203)
* Nogueira and Ng (2019) Rodrigo Nogueira and Ng Ng (2019) Raulipos Nogueira and Ng Ng (2019). Passage Re-ranking with BERT. _ArXiv_ abs/1901.04085 (2019). [https://api.semantichokorny.org/CrystalD80046692](https://api.semantichokorny.org/CrystalD80046692)
* Ouyang et al. (20222) Long Ouyang, Jeff Wu, Jia Jiang, Diago Almeida, Carroll L. Winningworth, Pamela Mishkin, Chong Zhang, Sandhuani Agarwal, Karafan Sharma, Alex Ray John Schulman, Jacob Hilton, Fraser Kelchu, Luke J. Miller, Maddis Simas, Amanda Askell, Peter Welinder, Paul Francis Christos, Janhee, and Ryan J. Love. 2022. Training language model to follow instructions with human feedback. _ArXiv_ abs/2202.024505 (2022). [http://apscannicheskohar.org/CrystalD2464009](http://apscannicheskohar.org/CrystalD2464009)
* Faderer et al. (2023) Ronta Pederer, Kai Hui, Gupta, Admin Daniel Lelmes, Hongkui Zhuang, Ima Dondat Metrie, and Vish Q Tran. 2023. How Does Generative Retail Scale to Millions of Passage? _ArXiv_ abs/2301.114823 (2023). [https://apscannicheskohar.org/CrystalD238822999](https://apscannicheskohar.org/CrystalD238822999)
* Qu et al. (2020) Yingqi Qu, Yuchen Jing, Jin Lu, Kai Liu, Ruiyong Ren, Xin Zhao, Daxiang Deng, Hua Wu, and Haiting Wang. 2020. RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering. In _Neurth American Chapter of the Association for Computational Linguistics_. [https://api.semantichokorny.org/CrystalD23815627](https://api.semantichokorny.org/CrystalD23815627)
* Raffel et al. (2019) Colin Raffel, Nann M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matzen, Yang Zhou, Wei Xu, and Peter J. Liu. 2019. Exploring the Limits of Tramer Learning with a Unified Text-to-Text Transformer. _J. Mach. Learn. Res._ 21 (2019), 1401-1407. [https://apscannicheskohar.org/CrystalD201538007](https://apscannicheskohar.org/CrystalD201538007)
* Raju et al. (2023) Shashank Raju, Nikh Mehta, Aimin Singh, Raghunandam H. Kehavam, Trang Hieu W. Lukens, Heldm Helm, Heng T. Tay, Vish Q Tran, Jonah Somat, Maciej Kula, Ed H. Chi, and Maeswaran Sahinhau. 2023. Rec-ommender Systems with Generative Retrieval. _ArXiv_ abs/2305.05065 (2023). [https://apscannicheskohar.org/CrystalD25856454](https://apscannicheskohar.org/CrystalD25856454)
* Robertson and Walker (1907) Stephen E. Robertson and Steve Walker. 1907. On relevance weights with little relevance information. In _Annual International ACM SIGIR Conference on Research and Development in Information Retrieval_. [https://apscannicheskohar.org/CrystalD14629071](https://apscannicheskohar.org/CrystalD14629071)
* Robertson and Zaragoza (2009) Chrispern E. Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance Framework: RM2S and Beyond. _Found. Trends Inf. Rec._ 3 (2009), 333-389. [https://api-semantichokorny.org/CrystalD178704](https://api-semantichokorny.org/CrystalD178704)
* Salton and McGill (1983) Gerard Salton and Michael McGill. 1983. Introduction to Modern Information Retrieval. [https://api.semantichokorny.org/CrystalD5685115](https://api.semantichokorny.org/CrystalD5685115)
* Sunh et al. (2019) Victor Sunh, Lysandre O. Ismailen Chaumond, and Thomas Wolf. 2019. Dis-tilBERT: a distilled version of BERT: smaller, faster, cheaper and lighter. _ArXiv_ abs/1910.01108 (2019). [https://apscannicheskohar.org/CrystalD20362072](https://apscannicheskohar.org/CrystalD20362072)
* Sutskever et al. (2014) Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to Sequence Learning with Neural Networks. _ArXiv_ abs/1409.3215 (2014). [https://api.semantichokorny.org/CrystalD24609](https://api.semantichokorny.org/CrystalD24609)
* Yang et al. (2021) Jingtao Yang, Wei Zhang, and Jian Sun. 2021. Accurate Text-to-Text Matching. In _2021 IEEE Conference on_. IEEE, 2021. IEEE, 2021. IEEE, 2021. IEEE, 2021.

semanticscholar.org/CorpusID7-961699
* Yin et al. (2022) Yiu Yin, Wun D Han, Mostafa Dehghani, Jianmo N, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Liu, Zhe Zhao, Jiaqi Gupta, Tal Schuster, William W. Cohen, and Daniel Mettke. 2022. Transformer Memory as a Differentiable Search Index. _ArXiv abs/2202.06991_ (2022). [https://api.semanticscholar.org/CorpusID2-26863488](https://api.semanticscholar.org/CorpusID2-26863488)
* Thoppulin et al. (2022) Ronald Thoppulin, Daniel De Freitas, Jamie Hall, Noam Shaweer, Apoor Kuleszarehula, Heng-Teng Cheng, Alicia Taylor, Noelia Hastie, Yule Du, et al. 2022. Landup Language models for dialog applications. _arXiv preprint arXiv:2202.08293_ (2022).
* van der Maaten and Hinton (2008) Laurens van der Maaten and Geoffrey E. Hinton. 2008. Visualizing Data using k-SNE. _Journal of Machine Learning Research_ 9 (2008), 2579-2605. [https://api.semanticscholar.org/CorpusID8-3855042](https://api.semanticscholar.org/CorpusID8-3855042)
* Wang et al. (2014) Jianfeng Wang, Jingdong Wang, Jingkuan Song, Xin-Shun Xu, Heng Tao Shen, and Shipeng Li. 2014. Optimized Cartesian K-Means. _IEEE Transactions on Knowledge and Data Engineering_ 27 (2014), 180-192. [https://api.semanticscholar.org/CorpusID140726](https://api.semanticscholar.org/CorpusID140726)
* Wang et al. (2022) Yujing Wang, Ying Hou, Hong Wang, Zhixing Ma, Shihun Wu, Hao Sun, Qi Chen, Yong Xia, Chengmin Chi, Guisui-Shouz Zhao, Zhen Liu, Xing Xie, Hao Sun, Weiwei Deng, Qi Zhang, and Xiao Tang. 2022. A Neural Corpus Indexer for Document Retrieval. _ArXiv abs/2202.06723_ (2022). [https://api.semanticscholar.org/CorpusID249935549](https://api.semanticscholar.org/CorpusID249935549)
* Xiong et al. (2020) Lee Xiong, Chenyan Xiong, Ye Li, Kwak-Fang Tang, Jialin Liu, Paul Bennett, Junial Ahmed, and Arnold Chervitz. 2020. Approximate Nearest Neighbor Negative Contrastive Learning for Dense Text Retrieval. _ArXiv abs/2007.300808_ (2020). [https://api.semanticscholar.org/CorpusID2202032524](https://api.semanticscholar.org/CorpusID2202032524)
* Zamani and Croft (2017) Hamed Zamani and W. Bruce Croft. 2017. Relevance-Based Word Embedding. In _Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval_ (Shijshu, Tokyo, Japan) (SIGIR '17): 505-514.
* Zamani et al. (2018) Hamed Zamani, Mostafa Dehghani, W. Bruce Croft, Enik G. Learned-Miller, and J. Kamps. 2018. From Neural Re-Ranking to Neural Ranking: Learning a Sparse Representation for Inverted Indexing. _Proceedings of the 27th ACM International Conference on Information and Knowledge Management_ (2018). [https://api.semanticscholar.org/CorpusID2522988](https://api.semanticscholar.org/CorpusID2522988)
* Zeng et al. (2022) Hansi Zeng, Hamed Zamani, and Vishnu Vinay. 2022. Curriculum Learning for Dense Retrieval Distillation. _Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval_ (2022). [https://api.semanticscholar.org/CorpusID28482670](https://api.semanticscholar.org/CorpusID28482670)
* Zh et al. (2011) Jingtao Zh, Jixia Mao, Yiqun Liu, Jiafeng Guo, M. Zhang, and Shaoping Ma. 2011. Optimizing Dense Retrieval Model Training with Hard Negatives. _Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval_ (2011). [https://api.semanticscholar.org/CorpusID23288984](https://api.semanticscholar.org/CorpusID23288984)
* Zhu et al. (2022) Wijin Zhu, Jing Tuo Zhu, Zhibong Dou, Ledell Yu Wu, Petitian Zhang, and Ji rong. 2022. Ultron: An Ultimate Reflewer on Corpus with a Model-based Indexer. _ArXiv abs/2008.09257_ (2022). [https://api.semanticscholar.org/CorpusID251710621](https://api.semanticscholar.org/CorpusID251710621)
* Zhang et al. (2022) Honglei Zhang, Zhen Qin, Bolf Jagersman, Kai Liu, Jiu Ma, Jing Lu, Jianmo N, Xuanhui Wang, and Michel Benardynsky. 2022. RankTPS: Fine-Tuning T5 for Text Ranking with Ranking Losses. _Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval_ (2022). [https://api.semanticscholar.org/CorpusID252993059](https://api.semanticscholar.org/CorpusID252993059)
* Zhuang et al. (2022) Shengyao Zhuang, Houxing Ren, Linjin Shen, Jian Pei, Ming Gong, G. Zuccon, and Daxin Jiang. 2022. Bridging Gap Between Matching and Retrieval for Differentiable Search Index with Query Generation. _ArXiv abs/2206.10128_ (2022). [https://api.semanticscholar.org/CorpusID249890267](https://api.semanticscholar.org/CorpusID249890267)

[MISSING_PAGE_FAIL:11]