# Local Centrality Minimization with Quality Guarantees

Anonymous Author(s)

###### Abstract.

Centrality measures, quantifying the importance of vertices or edges, play a fundamental role in network analysis. To date, triggered by some positive approximability results, a large body of work has been devoted to studying centrality maximization, where the goal is to maximize the centrality score of a target vertex by manipulating the structure of a given network. On the other hand, due to the lack of such results, only very little attention has been paid to centrality minimization, despite its practical usefulness.

In this study, we introduce a novel optimization model for local centrality minimization, where the manipulation is allowed only around the target vertex. We prove the NP-hardness of our model and that the most intuitive greedy algorithm has a quite limited performance in terms of approximation ratio. Then we design two effective approximation algorithms: The first algorithm is a highly-scalable algorithm that has an approximation ratio unachievable by the greedy algorithm, while the second algorithm is a bicriteria approximation algorithm that solves a continuous relaxation based on the Lovasz extension, using a projected subgradient method. To the best of our knowledge, ours are the first polynomial-time algorithms with provable approximation guarantees for centrality minimization. Experiments using a variety of real-world networks demonstrate the effectiveness of our proposed algorithms: Our first algorithm is applicable to million-scale graphs and obtains much better solutions than those of scalable baselines, while our second algorithm is rather strong against adversarial instances.

**ACM Reference Format:**

Anonymous Author(s). 2018. Local Centrality Minimization with Quality Guarantees. In _Proceedings of Make sure to enter the correct conference title from your rights confirmation email (Conference acronym 'XX')_. ACM, New York, NY, USA, 13 pages. [https://doi.org/XXXXXXXXXXXX](https://doi.org/XXXXXXXXXXXX)

+
Footnote †: copyright: none

+
Footnote †: copyright: none

+
Footnote †: copyright: none

+
Footnote †: copyright: none

+
Footnote †: copyright: none

+
Footnote †: copyright: none

+
Footnote †: copyright: none

## 1. Introduction

Among the many analytical tools that social network analysis (Shen et al., 2015) borrowed from graph theory, centrality measures play a fundamental role in a wide variety of analyses (Gardner et al., 2016). Centrality, which quantifies the _importance_ of vertices or edges only based on the graph structure, has found many applications, including, e.g., identification of important users or connections in social networks, community detection (Shen et al., 2015), anomaly detection (Zhu et al., 2016), to name a few.

_Local centrality minimization_ is the problem of removing a few existing edges around a target vertex, so as to minimize its centrality score. A direct application can be found in the context of reducing the visibility, or influence, of a targeted harmful user in a social network, without explicitly blocking the user account. The edges to be removed, identified by local centrality minimization, are the most important edges for the centrality (i.e., visibility or influence) of the target vertex. In this regard, another direct application is to keep satisfying influential users so that they are engaged in the platform. The degree of satisfaction of such influencers often depends on how actually influential they are in the platform, i.e., how much their content is consumed by the network. Local centrality minimization can be used for revealing the most important connections between the influencers and their followers, which are key in contributing to their visibility.

While a large body of work has been devoted to studying _centrality maximization_(Gardner et al., 2016), much less attention has been paid to centrality minimization (see Section 2 for a brief literature survey). Generally speaking, the goal of centrality maximization is to maximize the centrality score of a target vertex by adding a limited number of edges to the network. In many reasonable optimization models, the objective function becomes monotone and submodular (Gardner et al., 2016), and thus a simple greedy algorithm admits a \((1-1/e)\)-approximation (Zhou et al., 2016). This positive result makes the basis of various studies on centrality maximization (Bouvier et al., 2015; Goyal et al., 2016; Goyal et al., 2016; Goyal et al., 2016; Goyal et al., 2016).

The lack of positive approximation results has instead limited the attention on the centrality minimization problem, especially in its local variant. Waniek et al. (Waniek et al., 2016) introduced several optimization models for local centrality minimization under some specific objectives and constraints, investigated the computational complexity of their models, and devised some algorithms. Later, Waniek et al. (Waniek et al., 2016) investigated local centrality minimization from a game-theoretic point of view. However, the work by Waniek et al. (Waniek et al., 2016; Waniek et al., 2016) proposed only (exponential-time) exact algorithms and heuristics.

In this paper, we study the local centrality minimization problem, adopting the most well-established centrality measures called the _harmonic centrality_(Gardner et al., 2016), which quantifies the importance of vertices based on the level of reachability from the other vertices. The harmonic centrality is known as an effective alternative to the closeness centrality (Bouvier et al., 2015), which was employed in Waniek et al. (Waniek et al., 2016), in the sense that unlike the closeness centrality, it is well-defined even in the case where a graph is not strongly connected.

Boldi and Vigna (Bouvier et al., 2015) showed that among all the known centrality measures, only the harmonic centrality satisfies all the desirable axioms, namely the size axiom, density axiom, and score monotonicity axiom. Recently, Murai and Yoshida (Murai and Yoshida, 2016) theoretically and empirically demonstrated that among well-known centrality measures, the harmonic centrality is most stable (thus reliable) against the uncertainty of a given graph.

### Paper contributions and roadmap

In this paper, we introduce a novel optimization model for local centrality minimization, where the harmonic centrality is employed as an objective function. Specifically, in our model, given a directed graph \(G=(V,A)\), a target vertex \(v\in V\), and a budget \(b\in\mathbb{Z}_{>0}\)we aim to find a set of incoming edges of \(v\) with size (no greater than) \(b\) whose removal minimizes the harmonic centrality score of \(v\), denoted by \(h_{G}(v)\) (to be defined in Section 3).

For our optimization model, we first analyze the computational complexity. Specifically, we show that our model is NP-hard even on a very limited graph class (i.e., acyclic graphs), by constructing a polynomial-time reduction from the minimum \(k\)-union problem. Furthermore, we prove that the most intuitive greedy algorithm, which iteratively removes an incoming edge of \(v\) that maximally decreases the objective value, cannot achieve an approximation ratio of \(o(|V|)\), while any reasonable algorithm has an approximation ratio of \(O(|V|)\). This negative result motivates the design of algorithms that exploit the characteristics of our model.

We design two polynomial-time approximation algorithms. The first algorithm is a highly-scalable algorithm that has an approximation ratio of \(\sqrt{2h_{G}(v)}\). We stress that as \(\sqrt{2h_{G}(v)}=O(\sqrt[4]{|V|})=o(|V|)\), this approximation ratio is unachievable by the above greedy algorithm. Our algorithm first sorts the incoming neighbors of the target vertex \(v\) in the decreasing order of their harmonic centrality scores on a slightly modified graph, and then removes \(b\) incoming edges from the top-\(b\) vertices in the sorted list. To prove the approximation ratio, we scrutinize the relationship between the harmonic centrality scores of the target vertex and its incoming neighbors. In the end, we also prove the tightness of our analysis of the approximation ratio.

The second algorithm is a polynomial-time algorithm that has a bicriteria approximation ratio of \((\frac{1}{4},(\frac{1}{1-\alpha},e))\) for any \(\alpha\in(0,1)\) and \(e>0\). That is, the algorithm finds a subset of incoming edges of the target vertex \(v\) with size at most \(b/\alpha\) but attains the objective value at most the original optimal value times \(\frac{1}{1-\alpha}\) plus \(e\). Therefore, the algorithm approximates the original optimal value while violating the budget constraint to some bounded extent. To design the algorithm, we first introduce a continuous relaxation of our model. To this end, we use the well-known extension of set functions, called the Lovasz extension (Lovasz, 1965). An important fact is that the objective function of our model is submodular, which guarantees that its Lovasz extension is (not necessarily differentiable but) convex. Therefore, we can solve the relaxation (with an arbitrarily small error) using a projected subgradient method (Baudoin, 1994). Once we get a fractional solution, we apply a simple probabilistic procedure and obtain a subset of incoming edges of the target vertex \(v\).

Finally, our experiments on a variety of real-world networks show that our first algorithm is applicable to million-scale graphs and obtains much better solutions than those of scalable baselines, while our second algorithm is strong against adversarial instances.

In summary, our contributions are as follows:

* We study the _local harmonic centrality minimization problem_: We prove that it is NP-hard even on acyclic graphs, its objective function is submodular, and the most intuitive greedy algorithm cannot achieve \(o(|V|)\)-approximation (Section 3).
* We devise a highly-scalable algorithm with an approximation ratio of \(\sqrt{2h_{G}(v)}\), unachievable by the greedy algorithm.
* We then devise a bicriteria approximation algorithm that solves a continuous relaxation based on the Lovasz extension, using a projected subgradient method (Sections 5 and 6).

To the best of our knowledge, ours are the first polynomial-time algorithms with provable approximation guarantees for centrality minimization.

## 2. Related Work

In this section, we review related literature about centrality minimization and maximization, submodular minimization, and other relevant applications in social networks analysis.

**Centrality minimization**. The most related to the present paper is the work on centrality minimization by Waniek et al. (Waniek et al., 2016; Waniek et al., 2016) whose goal is to provide a methodology that contributes to hiding individuals in social networks from centrality-based network analysis algorithms.

More specifically, Waniek et al. (Waniek et al., 2016) introduced the following optimization model: Given a directed graph \(G=(V,A)\), a target vertex \(v\in V\), a budget \(b\in\mathbb{Z}_{>0}\), and a set \(M\) of possible single-edge modifications, we are asked to select at most \(b\) actions in \(M\) so as to minimize the centrality score of \(v\) while satisfying some constraint on the influence of some vertices in the graph. As a centrality measure, they considered the degree centrality, closeness centrality, and betweenness centrality. They showed that except for the degree centrality case, the above model is NP-hard even when the constraint on the influence of vertices is ignored, and devised simple heuristics.

Waniek et al. (Waniek et al., 2016) investigated local centrality minimization from a game-theoretic point of view. As a tool to analyze their game, they studied the following optimization model: Given a directed graph \(G=(V,A)\), a target vertex \(v\in V\), a hiding parameter \(\delta\), and a set \(M\) as above, we are asked to find a minimal subset of \(M\) guaranteeing that there are at least \(\delta\) vertices having a centrality score greater than that of \(\sigma\). As a centrality measure, they again considered the above three. They showed that the model is \(2\)-approximable for the degree centrality but is inapproximable within any logarithmic factor for the other two. Note that the above approximation is just for the size of the output rather than the ranking of the centrality score of \(v\) (or the centrality score of \(v\)).

Veremyev et al. (Veremyev et al., 2016) studied a global centrality minimization problem: Given an undirected graph \(G=(V,E)\) with cost \(c_{e}\in\mathbb{R}_{\geq 0}\) for each \(e\in E\), a target vertex subset \(S\subseteq V\), and a budget \(b\in\mathbb{Z}_{>0}\), we are asked to find \(F\subseteq E\) whose removal minimizes the centrality score of the target vertex subset \(S\) subject to the budget constraint \(\sum_{e\in F}c_{e}\leq b\). The centrality score of a vertex subset is defined as a generalization of the centrality score of a vertex. As a centrality measure, they considered a quite general one, based on distance between vertices, which includes the harmonic centrality as a special case. They proved that the above model is NP-hard for any centrality measure included in the above, and as a by-product of the analysis, they also mentioned the NP-hardness of its local variant, which coincides with (the undirected-graph counterpart of) our proposed model. In the present paper, by focusing on the harmonic centrality, we prove that our model is NP-hard even on a very limited graph class (i.e., acyclic graphs). On a positive side, they presented an exact algorithm based on mathematical programming and greedy heuristics. Very recently, Liu et al. (Liu et al., 2019) addressed another global centrality minimization problem, where the objective function is a centrality measure called the information centrality and the connectivity of the resulting graph is guaranteed.

**Centrality maximization.** Centrality maximization has more actively been studied in the literature (e.g., (Bahmalkar et al., 2017; Goyal et al., 2017; Goyal et al., 2018; Goyal et al., 2019; Goyal et al., 2019)), where the most related to ours is due to Crescenzi et al. (Goyal et al., 2017). They introduced the harmonic centrality maximization problem, where given a directed graph \(G=(V,A)\), a target vertex \(v\in V\), and a budget \(b\in\mathbb{Z}_{>0}\), we are asked to insert at most \(b\) incoming edges of \(v\) so as to maximize the harmonic centrality score of \(v\). Our proposed optimization model can be seen as a minimization counterpart of their problem. They proved that the problem is APX-hard, but devised a polynomial-time (\(1-1/\epsilon\))-approximation algorithm based on the submodularity of the objective function. Finally, we note that there is another class of problems also called centrality maximization, where the goal is to find \(S\subseteq V\) that has the maximum group centrality score (Bahmalkar et al., 2017; Goyal et al., 2018; Goyal et al., 2019; Goyal et al., 2019; Goyal et al., 2019; Goyal et al., 2019; Goyal et al., 2019; Goyal et al., 2019; Goyal et al., 2019), which is less relevant to the present paper.

**Submodular minimization.** Submodular minimization is one of the most well-studied problem classes in combinatorial optimization. Among the literature, the most related work is due to Svitkina and Fleischer (Svitkina and Fleischer, 2017). They stated that a polynomial-time (\(\frac{1}{\epsilon^{2}}\), \(\frac{1}{1-\epsilon^{2}}\))-bicriteria approximation algorithm for submodular minimization with a cardinality upper bound (and thus for our proposed model) is possible for any \(\alpha\in(0,1)\), using techniques in Hayrapetyan et al. (Hayapetyan et al., 2017). However, Hayapetyan et al. (Hayapetyan et al., 2017) did not consider the problem: They addressed the problem called the minimum-size bounded-capacity cut, where the function in the constraint instead of the objective function is submodular. Therefore, the above statement is not trivial even our proposed model should be handled in a formal way. Lovasz extension has actively been used for developing novel network analysis algorithms (e.g., (Zhou et al., 2019; Goyal et al., 2019; Goyal et al., 2019)).

**Applications.** Reducing the visibility or influence of target users in social networks has been studied in the context of influence minimization (Goyal et al., 2019; Goyal et al., 2019; Goyal et al., 2019). All of existing studies are based on some influence diffusion models such as the independent cascade model (Goyal et al., 2019; Goyal et al., 2019) and the linear threshold model (Goyal et al., 2019). Unlike those, our model does not assume any influence diffusion model, but is just based on the network structure. Very recently, Fabbri et al. (Fabbri et al., 2019) and Couette et al. (Couette et al., 2019) addressed the problem of reducing the exposure to harmful contents in social media networks.

On the other hand, identifying the users and/or connections that play a key role for user engagement in social networks has also attracted much attention. Bhawalkar et al. (Bhawalkar et al., 2019) initiated this kind of study from an optimization perspective. They invented a model that aims to find a group of users whose permanent use of the service guarantees user engagement as much as possible, and designed polynomial-time algorithms for some cases. Later, Zhang et al. (Zhang et al., 2019) and Zhu et al. (Zhu et al., 2020) introduced variants of the above model, and devised intuitive heuristics.

## 3. Problem Formulation and Characterization

In this section, we mathematically formulate our problem (Problem 1), and prove its NP-hardness (Theorem 1) and the submodularity of the objective function (Theorem 2). Finally, we show the quite limited performance of the greedy algorithm (Theorem 3).

Let \(G=(V,A)\) be a directed graph (or _digraph_ for short). Throughout the paper, we assume that digraphs are simple, that is, there exist neither self-loops nor multiple edges. For \(F\subseteq A\), we define \(G\setminus F\) as the subgraph of \(G\) that is constructed by removing all edges in \(F\) from \(G\), i.e., \(G\setminus F=(V,A\setminus F)\). For \(v\in V\), we denote by \(\rho(v)\) the set of incoming edges of \(v\), i.e., \(\rho(v)=\{(u,v)\in A\mid u\in V\}\). For \(v\in V\), let \(h_{G}(v)\) be the harmonic centrality score of \(v\) on a digraph \(G\), i.e.,

\[h_{G}(v)=\sum_{u\in V\setminus\{v\}}\frac{1}{d_{G}(u,v)},\]

where \(d_{G}(u,v)\) is the (shortest-path) distance from \(u\in V\) to \(v\in V\) on \(G\), and by convention, \(d_{G}(u,v)=\infty\) when \(v\) is not reachable from \(u\). Note that, contrarily to other centrality measures, even in the case where a digraph is not strongly connected, the harmonic centrality is still well-defined (assuming by convention that \(1/\infty=0\)). Intuitively, the harmonic centrality quantifies the importance of a given vertex \(v\) based on the level of reachability from the other vertices. The problem we tackle in this paper is formalized as follows:

**Problem 1** (Local harmonic centrality minimization).: _Given a digraph \(G=(V,A)\), a target vertex \(v\in V\), and a budget \(b\in\mathbb{Z}_{>0}\), we are asked to find \(F\subseteq\rho(v)\) with \(|F|\leq b\) whose removal minimizes the harmonic centrality of \(v\in V\), i.e., \(f_{(G,b)}(F)=h_{G\setminus F}(v)\)._

By constructing a polynomial-time reduction from the NP-hard optimization problem called the _minimum \(k\)-union_, we can prove the following. The proof can be found in Appendix A.1.

**Theorem 1**.: _Problem 1 is NP-hard even on acyclic graphs._

We next show that the objective function \(f_{(G,\rho)}\) of Problem 1 is submodular, which helps us design our bicriteria approximation algorithm in Section 5. Let \(S\) be a finite set. A set function \(f\colon 2^{S}\to\mathbb{R}\) is said to be _submodular_ if for any \(X,Y\subseteq S\), it holds that

\[f(X)+f(Y)\geq f(X\cup Y)+f(X\cap Y).\]

We prove the following in Appendix A.2:

**Theorem 2**.: _For any \(G=(V,A)\) and \(v\in V\), the objective function \(f_{(G,\rho)}\) of Problem 1 is submodular._

Finally, we prove that the most intuitive greedy algorithm has a quite limited performance in terms of the approximation ratio. Specifically, we consider the algorithm that iteratively removes an incoming edge of the target vertex \(v\) that maximally decreases the harmonic centrality score of \(v\), until it exhausts the budget. For reference, the pseudo-code is given in Algorithm 4 in Appendix A.3. This algorithm runs in \(O(b|\rho(v)|(|V|+|A|))\) time. Note that, unlike many submodular maximization algorithms, the _lazy evaluation_ technique (Goyal et al., 2019) cannot be used to obtain a practically efficient implementation. The proof of the following is available in Appendix A.4.

**Theorem 3**.: _The greedy algorithm has no approximation ratio of \(o(|V|)\) for Problem 1, while any algorithm that outputs \(F\subseteq\rho(v)\) with \(|F|=b\) has an approximation ratio of \(O(|V|)\)._

## 4. Scalable Approximation Algorithm

In this section, we present a highly-scalable \(\sqrt{2h_{G}(v)}\)-approximation algorithm for Problem 1.

### Algorithm

Let \(N_{\text{in}}(o)\) be the set of incoming neighbors of \(o\), i.e., \(N_{\text{in}}(o)=\{w\in V\mid(w,o)\in\rho(o)\}\). The intuition behind our algorithm is quite simple: As long as there exists a vertex \(w\in N_{\text{in}}(o)\) that has a large harmonic centrality score, so does the target vertex \(o\). This means that it is urgent to remove incoming edges of \(o\) that come from vertices having large harmonic centrality scores. Note that our algorithm and analysis consider the harmonic centrality scores on \(G\setminus\rho(o)\) rather than \(G\); this is essential to obtain our approximation ratio. Specifically, our algorithm first sorts the elements of \(N_{\text{in}}(o)\) as \((w_{1},\ldots,w_{|\rho(o)|})\) so that \(h_{G\setminus\rho(o)}(w_{1})\geq\cdots\geq h_{G\setminus\rho(o)}(w_{|\rho(o)|})\) and just returns \(\{(w_{1},o),\ldots,(w_{|\rho(o)|})\}\). For reference, the entire procedure is described in Algorithm 1.

The algorithm is highly scalable. Indeed, the time complexity of Algorithm 1 is dominated by the part of computing the harmonic centrality scores of vertices in \(N_{\text{in}}(o)\), which just takes \(O(|\rho(o)|(|V|+|A|))\) time. Therefore, the algorithm is asymptotically \(b\) times faster than the greedy algorithm (Algorithm 4).

### Analysis

From now on, we analyze the approximation ratio of Algorithm 1. The following lemma demonstrates that the optimal value can be lower bounded using the maximum harmonic centrality score over the remaining incoming neighbors of \(o\) in the resulting graph:

**Lemma 1**.: _Let \(F^{*}\) be an optimal solution to Problem 1 and \(N^{*}\) the vertex subset corresponding to \(F^{*}\), i.e., \(N^{*}=\{w\in V\mid(w,o)\in F^{*}\}\). Then it holds that_

\[f_{(G,o)}(F^{*})\geq\frac{1}{2}\left(\max_{w\in N_{\text{in}}(o) \setminus N^{*}}h_{G\setminus\rho(o)}(w)+1\right).\]

Proof.: For any \(w\in N_{\text{in}}(o)\setminus N^{*}\), we have

\[f_{(G,o)}(F^{*})=h_{G\setminus F^{*}}(o)=\sum_{w\in V\setminus \{o\}}\frac{1}{d_{G\setminus F^{*}}(u,w)}\] \[\geq\sum_{w\in V\setminus\{o\}}\frac{1}{d_{G\setminus F^{*}}(u,w )+1}=1+\sum_{w\in V\setminus\{w,o\}}\frac{1}{d_{G\setminus F^{*}}(u,w)+1}\] \[\geq 1+\frac{1}{2}\sum_{w\in V\setminus\{w,o\}}\frac{1}{d_{G \setminus F}(u,w)}\geq 2+\frac{1}{2}\sum_{w\in V\setminus\{w\}}\frac{1}{d_{G \setminus F^{*}}(u,w)}\] \[=\frac{1}{2}\left(h_{G\setminus F^{*}}(w)+1\right)\geq\frac{1}{2 }\left(h_{G\setminus\rho(o)}(w)+1\right),\]

where the first inequality follows from the triangle inequality of distance \(d_{G\setminus F^{*}}\), the third equality follows from \(d_{G\setminus F^{*}}(w,w)=0\), and the second inequality follows from the fact that the addition of \(1\) to the denominator makes it at most twice the original. The arbitrariness of the choice of \(w\in N_{\text{in}}(o)\setminus N^{*}\) derives the statement. 

On the other hand, the next lemma upper bounds the objective value of the output of Algorithm 1 using the harmonic centrality scores of the remaining incoming neighbors of \(v\) in the resulting graph:

**Lemma 2**.: _Let \(F_{\text{ALG}}\) be the output of Algorithm 1 and \(N_{\text{ALG}}\) the vertex subset corresponding to \(F_{\text{ALG}}\), i.e., \(N_{\text{ALG}}=\{w\in V\mid(w,o)\in F_{\text{ALG}}\}\). Then we have_

\[f_{(G,o)}(F_{\text{ALG}})\leq(|\rho(o)|-b)+\sum_{w\in N_{\text{in}}(o)\setminus N _{\text{ALG}}}h_{G\setminus\rho(o)}(w).\]

Proof.: On digraph \(G\setminus F_{\text{ALG}}\), any \(u\in V\setminus\{v\}\) satisfies either (i) there exists no (shortest) path from \(u\) to \(v\) or (ii) there exists \(w(u)\in N_{\text{in}}(o)\setminus N_{\text{ALG}}\) that is contained in a shortest path from \(u\) to \(v\), i.e., \(d_{G\setminus F_{\text{ALG}}}(u,w)=d_{G\setminus F_{\text{ALG}}}(u,w(u))+1\). Let \(V^{\prime}\subseteq V\setminus\{v\}\) be the subset of vertices that satisfy the condition (ii). Then we have

\[f_{(G,o)}(F_{\text{ALG}}) =h_{G\setminus F_{\text{ALG}}}(o)=\sum_{u\in V\setminus\{v\}} \frac{1}{d_{G\setminus F_{\text{ALG}}}(u,v)}\] \[=\sum_{w\in V\setminus\{v\}}\frac{1}{d_{G\setminus F_{\text{ALG}}} (u,w(u))+1}. \tag{1}\]

We see that the shortest path corresponding to \(d_{G\setminus F_{\text{ALG}}}(u,w(u))\) does not contain \(o\) (and thus any edge in \(\rho(o)\setminus F_{\text{ALG}}\)). Otherwise there would exist \(w^{\prime}(u)\in N_{\text{in}}(o)\setminus N_{\text{ALG}}\) satisfying that \(d_{G\setminus F_{\text{ALG}}}(u,w^{\prime}(u))<d_{G\setminus F_{\text{ALG}}}(u, w(u))\), which contradicts the fact that \(w(u)\) is contained in a shortest path from \(u\) to \(v\) on \(G\setminus F_{\text{ALG}}\). Hence, we have

\[d_{G\setminus F_{\text{ALG}}}(u,w(u))=d_{G\setminus\rho(o)}(u,w(u)).\]

Combining this with the equality (1), we have

\[f_{(G,o)}(F_{\text{ALG}})=\sum_{u\in V^{\prime}\setminus\{v\}} \frac{1}{d_{G\setminus\rho(o)}(u,w(u))+1}\] \[=(|\rho(o)|-b)+\sum_{u\in V^{\prime}\setminus\{v\}}\sum_{(N_{ \text{in}}(o)\setminus N_{\text{ALG}})}\frac{1}{d_{G\setminus\rho(o)}(u,w(u))+1}\] \[<(|\rho(o)|-b)+\sum_{u\in V^{\prime}\setminus\{v\}}\sum_{(N_{ \text{in}}(o)\setminus N_{\text{ALG}})}\frac{1}{d_{G\setminus\rho(o)}(u,w(u))}\] \[\leq(|\rho(o)|-b)+\sum_{w\in N_{\text{in}}(o)\setminus N_{\text{ALG}} }h_{G\setminus\rho(o)}(w),\]

where the last inequality holds by the fact that any term \(\frac{1}{d_{G\setminus\rho(o)}(u,w(u))}\) in the summation of the left-hand-side appears as a term in \(h_{G\setminus\rho(o)}(w)\) for appropriate \(w=w(u)\) in the right-hand-side. Therefore, we have the lemma. 

We are now ready to prove our main theorem:

**Theorem 4**.: _Algorithm 1 is a \(2(|\rho(o)|-b)\)-approximation algorithm for Problem 1._

Proof.: Here we use the notation that appeared in Lemmas 1 and 2. By the behavior of Algorithm 1, we have

\[\max_{w\in N_{\text{in}}(o)\setminus N_{\text{ALG}}}h_{G\setminus \rho(o)}(w)\leq\max_{w\in N_{\text{in}}(o)\setminus N^{*}}h_{G\setminus\rho(o )}(w).\]Using Lemmas 1 and 2 together with this inequality, we have

\[f_{G,\Theta}(F_{\text{ALG}})\leq(|\rho(v)|-b)+\sum_{w\in N_{\text{ in}}(\Theta)\setminus N_{\text{ALG}}}h_{G\setminus\rho(\Theta)}(w)\] \[\leq(|\rho(v)|-b)\left(1+\max_{w\in N_{\text{in}}(\Theta)\setminus N _{\text{ALG}}}h_{G\setminus\rho(\Theta)}(w)\right)\] \[\leq(|\rho(v)|-b)\left(1+\max_{w\in N_{\text{in}}(\Theta)\setminus N _{\text{}}}h_{G\setminus\rho(\Theta)}(w)\right)\] \[\leq(|\rho(v)|-b)\left(1+2f_{(G,\Theta)}(F^{*})-1\right)=2(|\rho(v )|-b)f_{(G,\rho)}(F^{*}),\]

which completes the proof. 

Based on the theorem, we obtain the desired approximation ratio:

**Corollary 1**.: _Algorithm 1 is a \(\sqrt{2h_{G}(o)}\)-approximation algorithm for Problem 1._

For any instance that satisfies \(b=|\rho(v)|\), Algorithm 1 outputs the trivial optimal solution (i.e., \(\rho(v)\)). Therefore, in what follows, we focus only on the instances with \(b<|\rho(v)|\). Obviously the output of any algorithm for Problem 1 has an objective value at most \(h_{G}(v)\). On the other hand, the optimal value is at least \(|\rho(v)|-b\) because in the resulting digraph, there are still \(|\rho(v)|-b\) incoming neighbors of \(v\), each of which contributes exactly 1 to the objective value. Therefore, any algorithm (including Algorithm 1) for Problem 1 has an approximation ratio of \(\frac{h_{G}(v)}{|\rho(v)|-b}\). By combining this with Theorem 4, the approximation ratio of Algorithm 1 can be improved to

\[\min\left\{2(|\rho(v)|-b),\ \frac{h_{G}(v)}{|\rho(v)|-b}\right\}\leq\sqrt{2h_{G}(v)},\]

which presents the statement. 

It should be remarked that as \(\sqrt{2h_{G}(v)}\leq\sqrt{2(|V|-1)}=o(|V|)\), the approximation ratio is unachievable by the greedy algorithm. Finally, we conclude this section by showing that the analysis of the approximation ratio is tight up to a constant factor. The proof is available in Appendix B.1.

**Theorem 5**.: _Algorithm 1 has no approximation ratio of \(o\left(\sqrt{h_{G}(v)}\right)\)._

## 5. Bicriteria Approximation Algorithm

In this section, we present a polynomial-time \((\frac{1}{\varepsilon},(\frac{1}{1-\varepsilon},\varepsilon))\)-bicriteria approximation algorithm (\(\alpha\in(0,1)\) and \(\varepsilon>0\)) for Problem 1. Our algorithm first solves a continuous relaxation of the problem and then applies a simple probabilistic procedure to the fractional solution to obtain the output.

### Continuous relaxation

To obtain a continuous relaxation of Problem 1, we consider the well-known extension of set functions, called the Lovasz extension (Lovasz, 1977). For our objective function \(f_{(G,\rho)}\), the Lovasz extension \(\widehat{f}_{(G,\rho)}\): \([0,1]^{\rho(v)}\rightarrow\mathbb{R}\) is defined in the following way: Let \(\rho(v)=\{\varepsilon_{1},\ldots,\varepsilon_{|\rho(v)|}\}\). For \(x\in[0,1]^{\rho(v)}\), we relabel the elements of \(\rho(v)\) so that \(x_{x_{1}}\geq x_{v_{2}}\geq\cdots\geq x_{v_{|\rho(v)|}}\), and construct a sequence of subsets \(\emptyset=X_{0}\subset X_{1}\subset\cdots\subset X_{|\rho(v)|}=\rho(v)\), where \(X_{i}=\{\varepsilon_{1},\ldots,\varepsilon_{i}\}\) for \(i=1,\ldots,|\rho(v)|\). Based on these, we define the value of \(\widehat{f}_{(G,\rho)}(x)\) as follows:

\[\widehat{f}_{(G,\rho)}(x)=(1-x_{\text{e}_{1}})f_{(G,\rho)}( \emptyset)\] \[+\sum_{i=1}^{|\rho(v)|-1}(x_{\text{e}_{i+1}}-x_{\text{e}_{i+1}})f _{(G,\rho)}(X_{i})+x_{v_{|\rho(v)|}}f_{(G,\rho)}(\rho(v)).\]

Observe that for any \(F\subseteq\rho(v)\), it holds that \(\widehat{f}_{(G,\rho)}(1_{F})=f_{(G,\rho)}(F)\), where \(1_{F}\) is an indicator vector of \(F\), taking \(1\) if \(e\in F\) and \(0\) otherwise. Therefore, \(\widehat{f}_{(G,\rho)}\) is indeed an extension of \(f_{(G,\rho)}\).

The Lovasz extension can be defined on any (not necessarily submodular) set function. The Lovasz extension is always continuous but not necessarily differentiable. An important fact is that the Lovasz extension is convex if and only if the original set function is submodular (Lovasz, 1977). Therefore, by Theorem 2, the Lovasz extension \(\widehat{f}_{(G,\rho)}\) of \(f_{(G,\rho)}\) is convex.

Using \(\widehat{f}_{(G,\rho)}\), we introduce our continuous relaxation as follows:

\[\text{Relaxation: minimize }\widehat{f}_{(G,\rho)}(x)\] \[\text{subject to }\|x\|_{1}\leq b,\ x\in[0,1]^{\rho(v)}.\]

For convenience, we denote by \(C\) the feasible region of the problem, i.e.,

\[C\coloneqq\left\{x\in\mathbb{P}^{\rho(v)}:\|x\|_{1}\leq b\ \text{ and }\ x\in[0,1]^{\rho(v)}\right\}.\]

From the above, we see that Relaxation is a non-smooth convex programming problem. We will present an algorithm for Relaxation and its convergence result in Section 6. In the remainder of this section, we assume that for \(\epsilon^{\prime}=(1-\alpha)\epsilon\), we can compute, in polynomial time, an \(\epsilon^{\prime}\)-additive approximate solution for Relaxation, i.e., a feasible solution for Relaxation that has an objective value at most the optimal value plus \(\epsilon^{\prime}\).

### Algorithm

Let \(\mathbf{x}^{*}\in[0,1]^{\rho(v)}\) be an \(\epsilon^{\prime}\)-additive approximate solution for Relaxation, where \(\epsilon^{\prime}=(1-\alpha)\epsilon\). Then our algorithm picks \(p\in[\alpha,1]\) uniformly at random and just returns \(\{e\in\rho(v)\mid x_{e}^{*}\geq p\}\). For reference, the entire procedure is summarized in Algorithm 2.

```
Input :\(G=(V,A)\), \(v\in V\), and \(b\in\mathbb{Z}_{>0}\) Output :\(F\subseteq\rho(v)\)
1\(\epsilon^{\prime}\leftarrow(1-\alpha)\epsilon\);
2 Solve Relaxation (using Algorithm 3 in Section 6) and obtain its \(\epsilon^{\prime}\)-additive approximate solution \(\mathbf{x}^{*}\in[0,1]^{\rho(v)}\);
3 Pick \(p\in[\alpha,1]\) uniformly at random;
4return\(\{e\in\rho(v)\mid x_{e}^{*}\geq p\}\);
```

**Algorithm 2**\((\frac{1}{1-\alpha},(\frac{1}{1-\alpha},\epsilon))\)-bicriteria approximation algorithm for Problem 1

### Analysis

The following theorem gives the bicriteria approximation ratio of Algorithm 2:```
Input :\(\mathbf{x}_{0}\in C\) and some stopping condition Output :\(\mathbf{x}\in C\)
1\(t\gets 0\);
2whilethe stopping condition is not satisfieddo
3 Pick a stepsize \(\eta_{t}>0\) and a subgradient \(\widehat{f}_{(G,\mathbf{x})}(\mathbf{x}_{t})\) of \(\widehat{f}_{(G,\mathbf{x})}\) at \(\mathbf{x}_{t}\);
4\(x_{t+1}\leftarrow\operatorname*{proj}_{C}\left(\mathbf{x}_{t}-\eta_{t}\cdot\widehat{ f}^{\prime}(\mathbf{x}_{t})\right)\) and \(t\gets t+1\);
5return\(\mathbf{x}_{t}\);
```

**Algorithm 3**Projected subgradient method for Relaxation

**Theorem 6**.: _For any \(\alpha\in(0,1)\) and \(\epsilon>0\), Algorithm 2 is a polynomial-time \((\frac{1}{\alpha},(\frac{1}{1-\alpha},\epsilon))\)-bicriteria approximation algorithm for Problem 1._

Proof.: Let \(F\subseteq\rho(v)\) be the output of Algorithm 2. The approximation ratio with respect to the size of \(F\) can be evaluated as follows:

\[\operatorname{E}[|F|]=\frac{1}{\alpha}\cdot\operatorname{E}\left[\sum_{e\in F} \alpha\right]\leq\frac{1}{\alpha}\cdot\operatorname{E}\left[\sum_{e\in F}x_{e }^{*}\right]\leq\frac{1}{\alpha}\sum_{e\in\rho(v)}x_{e}^{*}\leq\frac{b}{ \alpha},\]

where the first inequality follows from \(\alpha\leq p\leq x_{e}^{*}\) for any \(\epsilon\in F\), the second inequality follows from the nonnegativity of \(\mathbf{x}^{*}\), and the third inequality follows from the first constraint in Relaxation.

Next we analyse the approximation ratio with respect to the quality of \(F\). Let \(F^{*}\) be an optimal solution to Problem 1. As Relaxation is indeed a relaxation of Problem 1 and \(x^{*}\) is its \(\epsilon^{*}\)-additive approximate solution, we have \(\widehat{f}_{(G,\mathbf{x})}(\mathbf{x}^{*})\leq f_{(G,\mathbf{x})}(F^{*})+\epsilon^{\prime}\). For convenience, we define \(x_{0}^{*}=1\) for an imaginary element \(\epsilon_{0}\). Let \(\ell\) be the maximum number that satisfies \(x_{e}^{*}\geq\alpha\). Then we have

\[\operatorname{E}[f_{(G,\mathbf{\rho})}(F)]=\frac{\sum_{i=0}^{\ell-1}(x _{e_{i}}^{*}-x_{e_{i+1}}^{*})f_{(G,\mathbf{\rho})}(X_{i})+(x_{e_{i}}^{*}-\alpha)f_ {(G,\mathbf{\rho})}(X_{\ell})}{1-\alpha}\] \[\leq\frac{\sum_{i=0}^{\lfloor\rho(v)\rfloor-1}(x_{e_{i}}^{*}-x_{e _{i+1}}^{*})f_{(G,\mathbf{\rho})}(X_{i})+x_{e_{i+\rho(v)}}^{*}f_{(G,\mathbf{\rho})}( \rho(v))}{1-\alpha}\] \[=\frac{\widehat{f}_{(G,\mathbf{\rho})}(\mathbf{x}^{*})}{1-\alpha}\leq \frac{f_{(G,\mathbf{\rho})}(F^{*})+\epsilon^{\prime}}{1-\alpha}=\frac{f_{(G,\mathbf{ \rho})}(F^{*})}{1-\alpha}+\epsilon,\]

where the first equality follows from the random choice of \(p\) and the first inequality follows from the monotonicity of elements in \(\mathbf{x}^{*}\) and the nonnegativity of \(f_{(G,\mathbf{\rho})}\). Therefore, we have the theorem. 

## 6. Solving Relaxation

In this section, we present our algorithm for solving Relaxation.

### Algorithm

Specifically, we design a projected subgradient method for Relaxation. The algorithm is an iterative method, where each iteration consists of two parts, i.e., the subgradient computation part and the projection computation part. The pseudo-code is given in Algorithm 3. All the details will be given later. The sequence generated by the algorithm is \(\{\mathbf{x}_{t}\}_{t\geq 0}\), while the sequence of function values generated by the algorithm is \(\{\widehat{f}_{(G,\mathbf{\rho})}(\mathbf{x}^{t})\}_{t\geq 0}\). As the sequence of function values is not necessarily monotone, we are also interested in the sequence of best-achieved function values at or before \(t\)-th iteration, which is defined as

\[\widehat{f}_{\text{best}}^{(\ell)}=\min_{t=0,1,\ldots,\ell}\widehat{f}_{(G,\bm {\rho})}(\mathbf{x}_{t}).\]

**Subgradient computation.** From the definition of \(\widehat{f}_{(G,\mathbf{\rho})}\), a subgradient \(\widehat{f}_{(G,\mathbf{\rho})}\) at \(\mathbf{x}_{t}\in C\) is given by

\[\widehat{f}_{(G,\mathbf{\rho})}^{\prime}(\mathbf{x}_{t})=\sum_{i=1}^{\lfloor\rho(v) \rfloor}\left(f_{(G,\mathbf{\rho})}(X_{i})-f_{(G,\mathbf{\rho})}(X_{i-1})\right)\mathbf{u} _{e_{i}}, \tag{2}\]

where \(\mathbf{u}_{e_{i}}\) is the \(\lfloor\rho(v)\rfloor\)-dimensional vector that takes \(1\) in the element corresponding to \(e_{i}\) and \(0\) elsewhere. To compute the subgradient \(\widehat{f}_{(G,\mathbf{\rho})}^{\prime}(\mathbf{x}_{t})\), we need to sort the entries of \(\mathbf{x}_{t}\) and compute \(f_{(G,\mathbf{\rho})}(X_{t})\) for all \(i=0,1,\ldots,\lfloor\rho(v)\rfloor\), which takes \(O(\lfloor\rho(v)\rfloor(|V|+|A|))\) time.

**Projection computation.** For a given \(\mathbf{x}\in\mathbb{R}^{\rho(v)}\), it is not trivial how to compute the projection of \(\mathbf{x}\) onto \(C\) because \(C\) is the intersection of the two sets \(\{\mathbf{x}\in\mathbb{R}^{\rho(v)}\mid\|\mathbf{x}\|_{1}\leq\mathbf{b}\}\) and \(\{\mathbf{x}\in\mathbb{R}^{\rho(v)}\mid\mathbf{x}\in[0,1]^{\rho(v)}\}\). For simplicity, define \(\operatorname{Box}[\mathbf{0},\mathbf{1}]=\{\mathbf{x}\in\mathbb{R}^{\rho(v)}\mid\mathbf{x}\in[ 0,1]^{\rho(v)}\}\). Let \(\operatorname{proj}_{\text{Box}}[\mathbf{1}](\mathbf{x})\) be the projection of \(\mathbf{x}\) onto \(\operatorname{Box}[\mathbf{0},\mathbf{1}]\). Then by Lemma 6.26 in Beck (Beck, 2015), we have

\[\operatorname{proj}_{\text{Box}}[\mathbf{0},\mathbf{1}](\mathbf{x})=(\min\{\max\{\mathbf{x}_{ \mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\bm}}}}}}}}}}}}}}} }\}},1\})_{\in\in\mathcal{P}(\mathbf{0})}.\]

Using this projection, we can give the projection of \(\mathbf{x}\) onto \(C\) as follows:

**Fact 1** (A special case of Example 6.32 in Beck (Beck, 2015)).: _Let \(\operatorname{proj}_{C}(\mathbf{x})\) be the projection of \(\mathbf{x}\in\mathbb{R}^{\rho(v)}\) onto \(C\). Then we have_

\[\operatorname{proj}_{\text{C}}(\mathbf{x})=\begin{cases}\operatorname{proj}_{ \text{Box}}[\mathbf{0},\mathbf{1}](\mathbf{x})&\text{if }\|\operatorname{proj}_{\text{Box}}[\mathbf{0},\mathbf{1}](\mathbf{x})\|_{1}\leq b,\\ \operatorname{proj}_{\text{Box}}[\mathbf{0},\mathbf{1}](\mathbf{x}-\lambda^{*}\mathbf{1})& \text{otherwise},\end{cases}\]

_where \(\lambda^{*}\) is any positive root of the nonincreasing function \(\varphi(\lambda)=\|\operatorname{proj}_{\text{Box}}[\mathbf{0},\mathbf{1}](\mathbf{x}-\lambda 1)\|_{1}-b\)._

In practice, we can compute the value of \(\lambda^{*}\) for \(\mathbf{x}:=\mathbf{x}_{t}-\eta_{t}\cdot\widehat{f}_{(G,\mathbf{\rho})}(\mathbf{x}_{t})\) using binary search. Assume that the stepsize \(\eta_{t}>0\) is no greater than \(1\) for any iteration \(t=0,1,\ldots\), which is indeed the case of ours (specified later). As initial lower and upper bounds on \(\lambda^{*}\), we can use \(0\) and \(\max_{e\in\rho(v)}x_{e}\), respectively. From the fact that \(\mathbf{x}_{t}\) is always contained in \(C\) and the definition of the subgradient (2), we see that

\[\max_{e\in\rho(v)}x_{e} \leq\max_{e\in\rho(v)}x_{e}(\mathbf{x})+\eta_{t}\cdot\max_{i=1,\ldots, \lfloor\rho(v)\rfloor-1}f_{(G,\mathbf{\rho})}(X_{i-1})\] \[\leq 1+f_{(G,\mathbf{\rho})}(\mathbf{\theta})\leq|V|.\]

where \(\mathbf{x}_{t}(\mathbf{\epsilon})\) is the element of \(\mathbf{x}_{t}\) corresponding to \(\mathbf{\epsilon}\). Therefore, the binary search finds \(\lambda^{*}\) in \(O(\lfloor\rho(v)\rfloor\log(|V|/\delta))\) time with an additive error of \(\delta>0\). Note that any polynomial-time algorithm cannot recognize an additive error of \(o(2^{-|V|^{*}})\) for constant \(c\), due to its bit complexity. Hence, if we set \(\delta=O(2^{-|V|^{*}})\), we can assume that the projection is exact, and the time complexity is still polynomial.

### Convergence result

Let \(L_{\widehat{f}_{(G,\mathbf{\rho})}}=\widehat{f}_{(G,\mathbf{\rho})}(\mathbf{0})\) (\(=f_{(G,\mathbf{\rho})}(\mathbf{\theta})\)). Based on the convergence result of the projected subgradient method in Beck (Beck, 2015), which is reviewed in Appendix C.1, we present the convergence result of Algorithm 3 as follows:

**Theorem 7**.: _Let \(\Theta\) be an upper bound on the half-squared diameter of \(C\), i.e., \(\Theta\geq\max_{\mathbf{x},\mathbf{y}\in\mathbbm{C}}\frac{1}{2}\|\mathbf{x}-\mathbf{y}\|^{2}\). Determine the stepsize \(\eta_{t}\) (\(t=0,1,\ldots\)) as \(\eta_{t}=\frac{\sqrt{2\Theta}}{L_{\widehat{f}_{(G,\Theta)}}}\sqrt{t+1}\). Let \(\widehat{f}^{\ast}\) be the optimal value of Relaxation. Then for all \(t\geq 2\), it holds that_

\[\widehat{f}^{(t)}_{\rm best}-\widehat{f}^{\ast}\leq\frac{2(1+\log 3)L_{ \widehat{f}_{(G,\Theta)}}\sqrt{2\Theta}}{\sqrt{t+2}}.\]

The proof can be found in Appendix C.2. By this theorem and the above discussion of the time complexity, the following is almost straightforward:

**Corollary 2**.: _Let \(\epsilon^{\prime}>0\). Set the stopping condition of Algorithm 3 as follows:_

\[t\geq\left(\frac{2(1+\log 3)L_{\widehat{f}_{(G,\Theta)}}\sqrt{2\Theta}}{ \epsilon^{\prime}}\right)^{2}-2.\]

_Then, Algorithm 3 outputs, in polynomial time, an \(\epsilon^{\prime}\)-additive approximate solution for Relaxation._

## 7. Experimental Evaluation

In this section, we thoroughly evaluate the performance of our proposed algorithms (i.e., Algorithms 1 and 2) using various real-world networks.

### Setup

**Instances.** Table 1 lists real-world digraphs on which our experiments were conducted. All graphs were collected from the webpage of the KONECT Project1. Note that self-loops and multiple edges were removed so that the graphs are made simple. For each graph, we randomly chose 20 vertices as target vertices among those having the in-degree at least 100. The last column of Table 1 gives the statistics of the in-degrees of the target vertices, i.e., the maximum, average, and minimum in-degrees. For each graph and each target vertex \(v\), we vary the budget \(b\) in \(\{\lfloor\frac{1}{4}|\rho(v)|\rfloor,\lfloor\frac{1}{2}|\rho(v)|\rfloor,\lfloor \frac{3}{4}|\rho(v)|\rfloor\}\).

Footnote 1: [http://konect.cc/](http://konect.cc/)

**Baselines.** We employ the following baseline methods:

* Empty: This algorithm just outputs the empty set, thus presenting an upper bound on the objective function value (i.e., \(h_{G}(v)\)) of any feasible solution.
* Random: This algorithm randomly chooses \(b\) edges from \(\rho(v)\). For each instance, this algorithm is run 100 times and the average objective value is reported.
* Degree: This algorithm sorts the elements of \(N_{\rm in}(v)\) as \((w_{1},\ldots w_{|\rho(v)|})\) so that \(|\rho(w_{1})|\geq\cdots\geq|\rho(w_{|\rho(v)|})|\) and just returns \(\{(w_{1},v),\ldots,(w_{b},v)\}\).
* Greedy: Execute the greedy algorithm (Algorithm 4).

**Machine specs and code.** All experiments were conducted on Mac mini with Apple M1 Chip and 16 GB. All codes were written in Python 3.9.

### Performance of algorithms

Here we evaluate the performance of our algorithms. To this end, we run the algorithms together with the baselines for all graphs and budgets. For each graph and each budget, if the algorithm tested does not terminate within 1,200 seconds for the target vertex having the largest in-degree, we do no longer run the algorithm for the graph and the other budgets. Note that Algorithm 3 (in Algorithm 2) is run with stopping condition \(t\geq 1000\) for scalability and initial solution \(\mathbf{x}_{0}=\mathbf{0}\).

The quality of solutions of the algorithms except for Algorithm 2 is illustrated in Figure 1. Due to space limitations, only the results for the budget \(b=\lfloor\frac{1}{4}|\rho(v)|\rfloor\) are presented here. Although the trend is similar, the results for the other budget settings \(b=\lfloor\frac{1}{4}|\rho(v)|\rfloor,\lfloor\frac{3}{4}|\rho(v)|\rfloor\) are given in Appendix D.1. As the solutions of Algorithm 2 may violate the budget constraint, it is unfair to compare those with the others in the same plots; thus, the solutions are evaluated later. In the plots in Figure 1, once we fix a value in the vertical axis, we can observe the cumulative number of solutions (i.e., targets) that attain the harmonic centrality score at most the fixed value. Therefore, we can say that an algorithm drawing a lower line has a better performance.

\begin{table}
\begin{tabular}{l c c c c} \hline Name & \(|V|\) & \(|A|\) & Stat. of in-degrees of targets \\ \hline \multirow{2}{*}{
\begin{tabular}{} \end{tabular} } & 1,224 & 19,022 & (337, 158, 30, 101) \\  & 1,224 & 3,430 & (274, 143, 454, 104) \\  & 1,224 & 3,430 & (361, 152, 50, 100) \\  & 46,591 & 84,379 & (174, 123, 56, 100) \\  & 384,654 & 1,744, & (495, 180, 70, 106) \\  & 1,138,494 & 94,229 & (311, 273, 40, 106) \\  & 1,056,266 & 1,485,859 & (104, 920, 201, 11) \\  & soc-pbcc-relationships & 1,632,803 & 30,622,564 & (316, 157, 35, 101) \\ \hline \end{tabular}
\end{table}
Table 1. Real-world graphs used in our experiments.

\begin{table}
\begin{tabular}{l c c c c} \hline Name & \(b\) & Greedy & Algorithm 1 & Algorithm 2 & 737 \\ \hline \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & \(\lfloor\frac{1}{4}|\rho(v)|\rfloor\) & 4.53 & 0.10 & 323,55 & 738 \\  & \(\lfloor\frac{1}{4}|\rho(v)|\rfloor\) & 7.79 & 0.10 & 333,48 & 779 \\  & \(\lfloor\frac{1}{4}|\rho(v)|\rfloor\) & 9.75 & 0.10 & 332,48 & 760 \\ \hline \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & \(\lfloor\frac{1}{4}|\rho(v)|\rfloor\) & 4.40 & 0.12 & 393,72 & 761 \\  & \(\lfloor\frac{1}{4}|\rho(v)|\rfloor\) & 7.59 & 0.13 & 462,1 & 762 \\  & \(\lfloor\frac{1}{4}|\rho(v)|\rfloor\) & 9.55 & 0.13 & 404,52 & 763 \\ \hline \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & \(\lfloor\frac{1}{4}|\rho(v)|\rfloor\) & 6.17 & 0.15 & 482,83 & 763 \\  & \(\lfloor\frac{1}{4}|\rho(v)|\rfloor\) & 10.39 & 0.15 & 502,63 & 765 \\  & \(\lfloor\frac{1}{4}|\rho(v)|\rfloor\) & 13.33 & 0.15 & 489,92 & 765 \\ \hline \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & \(\lfloor\frac{1}{4}|\rho(v)|\rfloor\) & 4.05 & 0.14 & – & 766 \\  & \(\lfloor\frac{1}{4}|\rho(v)|\rfloor\) & 7.03 & 0.14 & – & 767 \\  & \(\lfloor\frac{1}{4}|\rho(v)|\rfloor\) & 8.90 & 0.14 & – & 768 \\ \hline \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & \(\lfloor\frac{1}{4}|\rho(v)|\rfloor\) & – & 4.58 & – & 769 \\  & \(\lfloor\frac{1}{4}|\rho(v)|\rfloor\) & – & 4.57 & – & 770 \\  & \(\lfloor\frac{1}{4}|\rho(v)|\rfloor\) & – & 4.57 & – & 770 \\ \hline \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & \(\lfloor\frac{1}{4}|\rho(v)|\rfloor\) & – & 130,56 & – & 771 \\  & \(\lfloor\frac{1}{4}|\rho(v)|\rfloor\) & – & 130,54 & – & 772 \\  & \(\lfloor\frac{1}{4}|\rho(v)|\rfloor\) & – & 132,19 & – & 773 \\ \hline \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & \(\lfloor\frac{1}{4}|\rho(v)|\rfloor\) & – & 188,84 & – & 774 \\  & \(\lfloor\frac{1}{4}|\rho(v)|\rfloor\) & – & 187,92 & – & 773 \\  & \(\lfloor\frac{1}{4}|\rho(v)|\rfloor\) & – & 187,68 & – & 776 \\ \hline \multirow{2}{*}{\begin{tabular}{} \end{tabular} } & \(\lfloor\frac{1}{4}|\rho(v)|\rfloor\) & – & 402,24 & – & 777 \\  & \(\lfloor\frac{1}{4}|\rho(v)|\rfloor\) & – & 392,63 & – & 778 \\ \hline \multirow{2}{*}{
\begin{tabular}{} \end{tabular} } & \(\lfloor\frac{1}{4}|\rho(v)|\rfloor\) & – & 512,05 & – & 778 \\  & \(\lfloor\frac{1}{4}|\rho(v)|\rfloor\) & – & 512,05 & – & 778 \\ \hline \end{tabular}
\end{table}
Table 2. Computation time (seconds) of the algorithms tested.

As can be seen, Algorithm 1 outperforms the baselines. Indeed, Algorithm 1 is applicable to all graphs tested, thanks to its high scalability, and the quality of solutions is much better than that of the scalable baselines, i.e., Random and Degree. Most interestingly, for the graph citeseer, Algorithm 1 succeeds in reducing the harmonic centrality scores of all target vertices to the values relatively close to 0, although Degree fails to have centrality scores less than 9,000 for five target vertices. Note that this result does not contradict the fact that even an optimal solution has the harmonic centrality score no less than \(|\rho(\sigma)|-b\): The minimum objective value attained by Algorithm 1 is 54, rather than 0. For small graphs, Greedy performs slightly better than Algorithm 1; however, due to its heavy computation burden, Greedy is not applicable to citeseer and the larger graphs.

The detailed report of the computation time is found in Table 2, where the average computation time over all target vertices is reported. Note that the results for Random and Degree are omitted because Random is obviously quite fast and Degree records 0.00 seconds for all graphs and budgets. We remark that the computation time of Greedy grows roughly proportionally to the budget \(b\), but that of Algorithm 1 remains almost the same for all settings of \(b\).

Finally, the quality of solutions of Algorithm 2 (with \(\alpha=\frac{1}{3},\frac{1}{2}\)), averaged over the target vertices, is reported in Table 3, where each of the objective value and the size of the solutions is a relative value compared with that of the solutions of Algorithm 1. As the rounding procedure of the algorithm contains randomness, we run the procedure 100 times and report the average value. As can be seen, Algorithm 2 returns very small solutions that even do not exhaust the budget. However, this result does not contradict the guarantee given in Theorem 6; the objective value is not much worse than that of Algorithm 1 and the (expected) size of solutions is just upper bounded in Theorem 6.

To further examine the performance of Algorithm 2, we run it on the instance that appeared in the proof of Theorem 5, where we set \(k=50\) (and \(b=50\)). Note that it is theoretically guaranteed that Algorithm 1 outputs a poor solution to the instance. On the other hand, Algorithm 2 with \(\alpha=\frac{3}{4}\) always obtains the optimal solution among 100 routing trials. Hence, we see that the algorithm is rather strong against adversarial instances, verifying the effectiveness of the theoretical approximation guarantee of Algorithm 2.

## 8. Conclusion

In this study, we have introduced a novel optimization model for local centrality minimization and designed two effective approximation algorithms. The first algorithm (Algorithm 1) is a highly-scalable \(\sqrt{2h_{G}(\sigma)}\)-approximation algorithm. We stress that this approximation ratio is unachievable by the most intuitive greedy algorithm (Algorithm 4). The second algorithm (Algorithm 2) is a polynomial-time \(\frac{1}{\sigma}(\frac{1}{1-\sigma},e)\)-biteriria approximation algorithm. To the best of our knowledge, ours are the first polynomial-time algorithms with provable approximation guarantees for centrality minimization. Experiments using a variety of real-world networks demonstrate the effectiveness of our proposed algorithms.

Our work opens up several interesting problems. Can we design a polynomial-time algorithm that has an approximation ratio better than that of Algorithm 1 or a bicriteria approximation ratio better than that of Algorithm 2? Another interesting direction is to study Problem 1 with a more capable setting. For example, it would be valuable to have a target vertex subset (rather than a single target vertex) and aim to minimize its group harmonic centrality score, as in the literature on global centrality minimization (Shenzhen et al., 2016).

Figure 1. Quality of solutions of the algorithms (except for Algorithm 2) with \(b=\lfloor\frac{1}{2}|\rho(\sigma)|\rfloor\).

## References

* Approximation and engineering. In _Proceedings of LENIX 21_, pages 154-168, 2021.
* (11) E. Angiraman, A. van der Grinten, A. Bojchevski, D. Zugner, S. Gunnemann, and H. Meyerhenke. Group centrally maximizing for large-scale graphs. In _Proceedings of LALENIX '20_, pages 56-69, 2020.
* (12) A. Beck. _First-Order Methods in Optimization_. MOS-SIAM Series on Optimization. SIAM, 2017.
* (13) E. Bergamini, P. Crescent, G. D'Angelo, H. Meyerhenke, L. Severini, and Y. Vchis. Improving the betweenness centrality of a node by adding links. _ACM Journal of Experimental Algorithmics_, 23:1-32, 2018.
* (14) K. Hawalhurst, J. Kleinberg, E. Levi, P. Roughgarden, and A. Sharma. Preventing unravelling in social networks: The anchored \(k\)-one problem. _SIAM Journal on Discrete Mathematics_, 203(14):1452-1475, 2015.
* (15) P. Boldi and V. Vigna. Axioms for centrality. _Internet Mathematics_, 10(3-4):222-262, 2014.
* (16) M. Castanlo, C. Catalano, G. Como, and F. Fagnani. On a centrality maximization game. _PAC-PapersOnLine_, 53(2):2844-2869, 2020.
* (17) C. Chen, W. Wang, and X. Wang. Efficient maximum closeness centrality group identification. In _Proceedings of the 27th Australasian Database Conference_, pages 43-55, 2016.
* (18) C. Cupertle, S. Neumann, and A. Gionis. Reducing exposure to harmful content via graph rewiring. In _Proceedings of KDD '23_, pages 332-334, 2023.
* (19) P. Crescent, G. D'Angelo, L. Severini, and Y. Vchis. Greedy improving our own closeness centrality in a network. _ACM Transactions on Knowledge Discovery from Data_, 11(1):1-32, 2016.
* (20) G. D'Angelo, H. Olsen, and L. Severini. Coverage centrality maximization in undirected networks. In _Proceedings of AAAI '18_, pages 501-508, 2019.
* (21) K. Das, S. Samanta, and M. Pal. Study on centrality measures in social networks: A survey. _Social Network Analysis and Mining_, 8:11, 2018.
* (22) F. Fabbri, Y. Wang, F. Bennich, C. Castillo, and M. Mathioudakis. Rewiring what-to-shot-net recommendations to reduce radicalization pathways. In _Proceedings of The Web Conference '22_, page 2719-2728, 2022.
* (23) S. Fujishige. _Submodular Functions and Optimization_, volume 58 of _Annals of Discrete Mathematics_. Elsevier, 2005.
* (24) J. Goldenberg, B. Libai, and A. Muller. Talk of the network: A complex systems look at the underlying process of word-of-mouth. _Marketing Letters_, 12:211-223, 2001.
* (25) J. Goldenberg, B. Libai, and E. Muller. Using complex systems analysis to advance marketing theory development: Modeling heterogeneity effects on new product growth through stochastic cellular automata. _Academy of Marketing Science Review_, 50(3):18, 2001.
* (26) A. Haynapetyan, D. Kempe, M. Pal, and Z. Svikkina. Unbalanced graph cuts. In _Proceedings of ESA '15_, pages 191-202, 2005.
* (27) V. Iukadin, D. Erdos, R. Terzi, and A. Bestvors. A framework for the evaluation and management of network centrality. In _Proceedings of SIAM '12_, pages 427-438, 2012.
* (28) D. Kempe, J. Kleinberg, and E. Tardos. Maximizing the spread of influence through a social network. In _Proceedings of KDD '03_, pages 137-146, 2003.
* (29) E. K. Bahl, R. Dilkina, and L. Song. Cuttingedge: Influence minimization in networks. In _Proceedings of Workshop on Frontiers of Network Analysis: Methods, Models, and Applications_, pages 1-13, 2013.
* (30) D. Knoke and S. Yang. _Social Network Analysis_, volume 154 of _Quantitative Applications in the Social Sciences_. SAGE Publications, 2020.
* (31) A. Konar and N. N. Shirotopakis. Exploring the subgraph density-size trade-off via the Lovasz criterion. In _Proceedings of WSDM '21_, pages 743-751, 2021.
* (32) A. Konar and N. D. Shirotopakis. The triangle-densest-k-subgraph problem: Hardness, Lovasz extension, and application to document summarization. In _Proceedings of AAAI A '22_, pages 4075-4082, 2022.
* (33) H. Li, R. Peng, L. Shan, Y. Yi, and Z. Zhang. Current flow group closeness centrality for complex networks? In _Proceedings of the Web Conference '18_, pages 961-971, 2019.
* (34) C. Lin, X. Zhou, A. N. Zebnikan, and Z. Zhang. A fast algorithm for moderating critical nodes via the removal of the network. In _IEEE Transactions on Knowledge and Data Engineering_, pages 1-14, 2023. In press.
* (35) L. Lovasz. Submodular functions and convexity. In A. Bachem, B. Korte, and M. Grotschel, editors, _Mathematical Programming: The State of the Art_, pages 235-257. Springer, 1983.
* (36) A. Malmoody, C. T. Touvrakis, and E. Ugfal. Scalable betweenness centrality maximization via sampling. In _Proceedings of KDD '16_, pages 1765-1773, 2016.
* (37) A. Malvana and M. Armacle. Many-objective optimization for anomaly detection on multi-layer complex integration networks. _Applied Sciences_, 11(9):0005, 2021.
* (38) S. Medya, A. Silva, A. Singh, P. Basu, and A. Svanni. Group centrality maximization via network design. In _Proceedings of SDM '18_, pages 126-134, 2018.
* (39) M. Minoux. Accelerated greedy algorithms for maximizing submodular set functions. In _Proceedings of the 8th IFIP Conference on Optimization Techniques_, pages 234-243, 2005.
* (40) S. Mumtaz and X. Wang. Identifying top-k influential nodes in networks. In _Proceedings of CIKM'17_, pages 2291-2222, 2017.
* (41) S. Murat and Y. Yoshida. Sensitivity analysis of centralities on unweighted networks. In _Proceedings of The Web Conference '19_, pages 1332-1342, 2019.
* (42) G. L. Nemhauser and I. A. Volker. Best algorithms for approximating the maximum of a submodular set function. _Mathematics of Operations Research_, 3(9):107-138, 1978.
* (43) M. E. J. Newman and M. Girvan. Finding and evaluating community structure in networks. _Physical Review E_, 69:026113, 2004.
* (44) I. Pellegrini. Efficient centrality maximization with Rademacher averages. In _Proceedings of KDD '23_, pages 1872-1884, 2023.
* (45) S. Rangapuram, T. Bilkier, and M. Heint. Towards realistic team formation in social networks based on densest subgraphs. In _Proceedings of WWW '13_, pages 1077-1108, 2013.
* (46) Z. Svitlins and I. Plesicher. Submodular approximation: Sampling-based algorithms and lower bounds. _SIAM Journal on Computing_, 40(6):1715-1737, 2011.
* (47) A. Veremberg, O. A. Przobyere, and E. L. Psailaila. Finding critical links for closeness centrality. _INFORMS Journal on Computing_, 31(3):367-389, 2019.
* (48) S. A. Vittelen. Privacy: a machine learning view. _IEEE Transactions on Knowledge and Data Engineering_, 16(9):939-948, 2004.
* (49) X. Wang, K. Deng, J. Li, J. Xu, C. S. Jensen, and X. Yang. Efficient targeted influence minimization in big social networks. _World Wide Web_, 2(4):2333-234, 2018.
* (50) M. Waniek, T. P. Michalik, M. J. Wooldridge, and T. Rahwan. Finding individuals and communities in a social network. _Nature Human Behaviour_, 2(3):391, 2018.
* (51) M. Waniek, J. Wooldridge, K. Zhou, Y. Vochevski, T. P. Michalik, and T. Rahwan. Hiding from centrality measures: A Stackelberg game perspective. _IEEE Transactions on Knowledge and Data Engineering_, 2023. In press.
* (52) L. Yang, Z. Li, and A. Gia. Influence minimization in linear threshold networks. _Automatica_, 106:10-16, 2019.
* (53) F. Zhang, Z. Zhang, L. Qin, W. Zhang, and X. Lin. Finding critical users for social network engagement: The collapsed score problem. In _Proceedings of AAAI '17_, 2017.
* (54) P. Zhao, Y. Li, H. Xu, Z. Wu, Y. Xu, and J. C. Lai. Measuring and maximizing influence via random walk in social activity networks. In _Proceedings of DASHA '17_, pages 323-338, 2017.
* (55) W. Zhu, C. Chen, X. Wang, and X. Lin. K-core minimization: An edge manipulation approach. In _Proceedings of CIKM '18_, pages 1667-1670, 2018.

## Appendix A Omitted contents from section 3

### Proof of Theorem 1

To prove the theorem, we formally introduce the following problem:

**Problem 2** (Minimum \(k\)-union).: _Given a ground set \(U=\{e_{1},\ldots,e_{n}\}\), a set system \(\mathcal{S}=\{S_{1},\ldots,S_{m}\}\subseteq 2^{U}\), and \(k\in\mathbb{Z}_{>0}\), we are asked to find \(J\subseteq\{1,\ldots,m\}\) with \(|J|=k\) that minimizes \(\bigcup_{j\in J}S_{j}|\). Without loss of generality, it can be assumed that \(\bigcup_{j\in\{1,\ldots,m\}}S_{j}=U\)._

**Fact 2** (Theorem 1 in Vinterbo (2018)).: _Problem 2 is NP-hard._

Proof of Theorem 1.: We construct a polynomial-time reduction from Problem 2 to Problem 1. Take an arbitrary instance of Problem 2: \(U=\{e_{1},\ldots,e_{n}\}\), \(\mathcal{S}=\{S_{1},\ldots,S_{m}\}\subseteq 2^{U}\), and \(k\in\mathbb{Z}_{>0}\). We make an instance (i.e., a gadget) of Problem 1, i.e., \(G=(V,A)\), \(v\in V\), and \(b\in\mathbb{Z}_{>0}\), as follows:

* \(G=(V,A)\) such that
* \(A=\{(v_{j},v_{0})\mid j=1,\ldots,m\}\cup\{(v_{e_{j}},v_{S_{j}})\mid e_{i}\in S _{j},\,i=1,\ldots,n,\,j=1,\ldots,m\}\);
* \(v=v_{0}\);
* \(b=m-k\).

Clearly, \(G\) is acyclic. Let \(F^{*}\subseteq\rho(v_{0})\) (\(=\{(v_{S_{j}},v_{0})\mid j=1,\ldots,m\}\)) be an optimal solution to this gadget, and let \(J^{*}\subseteq\{1,\ldots,m\}\) be the index set that satisfies \(F^{*}=\{(v_{S_{j}},v_{0})\mid j\neq J^{*}\}\). We see that \(|J^{*}|=b\). Let \(\overline{J^{*}}=\{1,\ldots,m\}\setminus J^{*}\). Then, the objective value of \(F^{*}\), i.e., the harmonic centrality of \(o_{0}\) after the removal of \(F^{*}\), can be evaluated as \((m-b)+\frac{1}{2}\left\|\bigcup_{j\in\overline{J^{*}}}S_{j}\right\|\). Therefore, as \(F^{*}\) is optimal, \(\left\|\bigcup_{j\in\overline{J^{*}}}S_{j}\right\|\) is minimized. Noticing that \(|\overline{J^{*}}|=m-b=m-(m-k)=k\), we see that \(\overline{J^{*}}\) is an optimal solution to the instance of Problem 2. 

### Proof of Theorem 2

Before proving the theorem, we mention a fundamental fact of the submodularity. It is well known that the submodularity is equivalent to the diminishing marginal return property (Geseren et al., 2015), which can be written as follows: For any \(X\subset Y\subseteq S\) and any \(p\in S\setminus Y\), it holds that

\[f(X\cup\{p\})-f(X)\geq f(Y\cup\{p\})-f(Y).\]

Proof of Theorem 2.: Let \(G=(V,A)\) be any digraph and \(v\in V\) be any vertex. From the above fact, it suffices to show that for any \(E\subset F\subseteq\rho(v)\) and \(e\in\rho(v)\setminus F\),

\[f_{(G,v)}(E\cup\{e\})-f_{(G,v)}(E)\geq f_{(G,v)}(F\cup\{e\})-f_{(G,v)}(F)\]

holds. From the definition of \(f_{(G,v)}\), we have

\[f_{(G,v)}(E\cup\{e\})-f_{(G,v)}(E)\] \[=h_{G\setminus(E\cup\{e\})}(e)-h_{G,v}(E)\] \[=\sum_{u\in V\setminus\{o\}}\left(\frac{1}{d_{G\setminus(E\cup\{e \})}(u,v)}-\frac{1}{d_{G\setminus E}(u,v)}\right).\]

For each \(e\in\rho(v)\), we define

\[U_{G}(v,e) =\{u\in V\setminus\{o\}\mid\] \[\text{there exists a path from }u\text{ to }v\text{ on }G\text{ and }\] \[\text{all shortest paths from }u\text{ to }v\text{ on }G\text{ contain }e\}.\]

Noticing that \(d_{G\setminus(E\cup\{e\})}(u,v)=d_{G\setminus E}(u,v)\) for all \(u\in V\setminus U_{G\setminus E}(v,e)\), we have

\[\sum_{u\in V\setminus\{e\}}\left(\frac{1}{d_{G\setminus(E\cup\{e\} )}(u,v)}-\frac{1}{d_{G\setminus E}(u,v)}\right)\] \[=\sum_{u\in U_{G\setminus E}(u,v)}\left(\frac{1}{d_{G\setminus(E \cup\{e\})}(u,v)}-\frac{1}{d_{G\setminus E}(u,v)}\right).\]

Applying a similar argument, we also have

\[f_{(G,v)}(F\cup\{e\})-f_{(G,v)}(F)\] \[=\sum_{u\in U_{G\setminus E}(v,e)}\left(\frac{1}{d_{G\setminus(E \cup\{e\})}(u,v)}-\frac{1}{d_{G\setminus E}(u,v)}\right).\]

Combining the above two equalities, we have

\[\left(f_{(G,v)}(E\cup\{e\})-f_{(G,v)}(E)\right)-\left(f_{(G,v)}(F \cup\{e\})-f_{(G,v)}(F)\right)\] \[=\sum_{u\in U_{G\setminus E}(u,v)}\left(\frac{1}{d_{G\setminus(E \cup\{e\})}(u,v)}-\frac{1}{d_{G\setminus E}(u,v)}\right)\] \[\geq\sum_{u\in U_{G\setminus E}(u,v)}\left(\left(\frac{1}{d_{G \setminus(E\cup\{e\})}(u,v)}-\frac{1}{d_{G\setminus E}(u,v)}\right)\right.\] \[\left.\hskip 14.226378pt-\left(\frac{1}{d_{G\setminus(E\cup\{e\} )}(u,v)}-\frac{1}{d_{G\setminus E}(u,v)}\right)\right)\] \[=\sum_{u\in U_{G\setminus E}(u,v)}\left(\frac{1}{d_{G\setminus(E \cup\{e\})}(u,v)}-\frac{1}{d_{G\setminus(E\cup\{e\})}(u,v)}\right)\geq 0,\]

where the first inequality follows from \(U_{G\setminus E}(u,v)\subseteq U_{G\setminus E}(v,e)\) and \(d_{G\setminus(F\cup\{e\})}(u,v)\geq d_{G\setminus(F\cup\{e\})}(u,v)\) for any \(u\in V\), and the second equality follows from \(d_{G\setminus E}(u,v)=d_{G\setminus F}(u,v)\) for any \(u\in U_{G\setminus E}(v,e)\). Therefore, we have the theorem. 

We wish to remark that the above theorem can also be proved using the fact that the objective function of the harmonic centrality maximization problem introduced in Crescenzi et al. (2018) is submodular. However, if we employed such an approach, the notation would be more complex. Therefore, we proved it from scratch, based on the definition of the submodularity.

### Pseudo-code of the greedy algorithm

See Algorithm 4.

```
Input :\(G=(V,A)\), \(v\in V\), and \(b\in\mathbb{Z}_{>0}\) Output :\(F\subseteq\rho(v)\) with \(|F|\leq b\)
1\(F\leftarrow\emptyset\);
2while\(|F|<b\)do
3 Find \(e\in\operatorname{argmin}(f_{(G,v)}(e)\mid e\in\rho(v))\);
4\(F\gets F\cup\{e\}\) and \(G\gets G\setminus\{e\}\);
5return\(F\);
```

**Algorithm 4**Greedy algorithm for Problem 1

### Proof of Theorem 3

Proof.: We first show that any algorithm that outputs \(F\subseteq\rho(\mathbf{\varepsilon})\) with \(|F|=b\) has an approximation ratio of \(O(|V|)\). Let \(G=(V,A)\), \(\mathbf{\varepsilon}\in V,\mathbf{\varepsilon}\in\mathbb{Z}_{>0}\) be an arbitrary instance of Problem 1. If the optimal value is equal to \(0\), then \(b=|\rho(\mathbf{\varepsilon})|\) holds, which means that the above algorithm can also achieve the optimal value. If the optimal value is greater than \(0\), then it is at least \(1\), while any algorithm can achieve the objective value at most \(h_{G}(\mathbf{\varepsilon})\leq|V|-1\). Therefore, the above algorithm has an approximation ratio of \(O(|V|)\).

Next we show that the greedy algorithm has no approximation ratio of \(\sigma(|V|)\). To this end, for any integer \(k\geq 2\), we construct an instance of Problem 1 as follows: Let \(G=(V,A)\) be a digraph. The vertex set \(V\) consists of two vertices \(n_{L},o_{L}\), and size-\(k\) subsets \(N_{R}=\{n_{R}^{1},\ldots,n_{R}^{k}\}\) and \(O_{R}=\{o_{R}^{1},\ldots,o_{R}^{k}\}\), in addition to one vertex \(v^{\prime}\). The edge set \(A\) is defined as the union of the following three sets:

\[\{(n_{L},v^{\prime}),(n_{R}^{i},v^{\prime})\mid i=1,\ldots,k\},\] \[\{(o_{L},n_{L})\},\] \[\{(o_{L}^{i},n_{R}^{j})\mid i=1,\ldots,k,\;j=1,\ldots,k\}.\]

Then we employ \(v^{\prime}\) as target vertex \(v\) and \(k\) as budget \(b\).

From now on, we analyze the performance of Algorithm 4 for this instance. In the first iteration, the removal of \((n_{L},v)\) decreases the objective value by \(3/2\), while the removal of any incoming edge from \(N_{R}\) decreases the objective value by \(1\). Therefore, the algorithm removes \((n_{L},v)\). In the later iterations, the removal of any edge decreases the objective value by \(1\), and thus the algorithm removes \(k-1\) edges from \(N_{R}\) to \(v\) and then terminates. We see that the objective value of the output is \(1+k/2\). On the other hand, consider the following feasible (actually optimal) solution to the instance, i.e., all \(k\) edges from \(N_{R}\) to \(v\). Then the objective value of this solution is \(1+1/2\). Thus, the approximation ratio of the algorithm is lower bounded by

\[\frac{1+k/2}{1+1/2}=\Omega(|V|),\]

meaning that Algorithm 4 has no approximation ratio of \(o(|V|)\) for Problem 1. 

## Appendix B Omitted contents from Section 4

### Proof of Theorem 5

Proof.: For any integer \(k\geq 2\), we construct an instance of Problem 1 as follows: Let \(G=(V,A)\) be a digraph. The vertex set \(V\) consists of size-\(k\) subsets \(N_{L}=\{n_{L}^{1},\ldots,n_{L}^{k}\}\), \(N_{R}=\{n_{R}^{1},\ldots,n_{R}^{k}\}\), \(O_{R}=\{o_{R}^{1},\ldots,o_{R}^{k}\}\) and size-\((k(k-1))\) subset \(O_{L}\subseteq\{o_{L}^{1},\ldots,o_{L}^{k(k-1)}\}\), in addition to one vertex \(v^{\prime}\). The edge set \(A\) is defined as the union of the following three sets:

\[\{(n_{L}^{i},v^{\prime}),(n_{R}^{i},v^{\prime})\mid i=1,\ldots,k\},\] \[\{(o_{L}^{i},n_{L}^{j})\mid i=1,\ldots,k(-1),\;j=1,\ldots,k,\;[i /(k-1)]=j\},\] \[\{(o_{R}^{i},n_{R}^{j})\mid i=1,\ldots,k,\;j=1,\ldots,k\}.\]

Then we employ \(v^{\prime}\) as target vertex \(v\) and \(k\) as budget \(b\).

From now on, we analyze the performance of Algorithm 1 for this instance. For any \(n_{L}^{i}\in N_{L}\) (\(i=1,\ldots,k\)), we have \(h_{G(\rho(\mathbf{\varepsilon}))}(n_{L}^{i})=k-1\). For any \(n_{R}^{i}\in N_{R}\) (\(i=1,\ldots,k\)), we have \(h_{G(\rho(\mathbf{\varepsilon}))}(n_{R}^{i})=k\). Therefore, the output of Algorithm 1 is the set of all \(k\) edges from \(N_{R}\) to \(v\), which has an objective value of \(k+k(k-1)/2\). On the other hand, consider the following feasible (actually optimal) solution to the instance, i.e., the set of all \(k\) edges from \(N_{L}\) to \(v\). Then the objective value of this solution is \(k+k/2\). Thus, the approximation ratio of the algorithm is lower bounded by

\[\frac{k+k(k-1)/2}{k+k/2}=\frac{1+(k-1)/2}{1+1/2}=\Omega\left(\sqrt{|V|}\right),\]

meaning that Algorithm 1 has no approximation ratio of \(o\left(\sqrt{|V|}\right)\) for Problem 1. Noticing that \(h_{G}(\mathbf{\varepsilon})\leq|V|-1\), we have the statement. 

```
Input :\(\mathbf{x}_{0}\in C\) and some stopping condition Output :\(\mathbf{x}\in C\)
1whilethe stopping condition is not satisfieddo
21 Pick stepsize \(\eta_{I}>0\) and subgradient \(f^{\prime}(\mathbf{x}_{I})\) of \(f^{\prime}\) at \(\mathbf{x}_{I}\);
223\(\mathbf{x}_{I+1}\leftarrow\text{proj}_{C}(\mathbf{x}_{I}-\eta_{I}\cdot f^{\prime}(\mathbf{x }_{I}))\) and \(t\gets t+1\);
23
24return\(\mathbf{x}_{I}\);
```

**Algorithm 5**Projected subgradient method

## Appendix C Omitted contents from Section 6

### Convergence result in Beck [3]

For self-containedness, we review the convergence result of the projected subgradient method in Beck [3], on which the convergence result of Algorithm 3 is based. Note that for simplicity, the description is appropriately specialized to our setting: For example, we consider the objective function \(f\) on \(\mathbb{R}^{n}\) rather than a general vector space.

Let \(f\colon\mathbb{R}^{n}\to(-\infty,\infty]\) and \(C\subseteq\mathbb{R}^{n}\). Consider the following problem:

\[\begin{array}{ll}(\mathbb{P})\text{:~{}minimize}&f(\mathbf{x})\\ \text{subject to}&\mathbf{x}\in C.\end{array}\]

**Assumption 1** (Assumption 8.7 in Beck [3]).: _The following hold:_

1. \(f\colon\mathbb{R}^{n}\to(-\infty,\infty]\) _is proper closed and convex;_
2. \(C\subseteq\mathbb{R}^{n}\) _is nonempty closed and convex;_
3. _The interior of the (effective) domain of_ \(f\) _contains_ \(C\)_;_
4. _The set of optimal solutions is nonempty, and the optimal value is denoted by_ \(f^{*}\)_._

We consider the projected subgradient method for (P), which is summarized in Algorithm 5. Note that we denote by \(\text{proj}_{C}(\mathbf{x})\) the projection of \(\mathbf{x}\in\mathbb{R}^{n}\) onto \(C\). The sequence generated by the algorithm is \(\{\mathbf{x}_{I}\}_{I\geq 0}\), while the sequence of function values generated by the algorithm is \(\{f(\mathbf{x}^{\prime})\}_{I\geq 0}\). As the sequence of function values is not necessarily monotone, we are also interested in the sequence of best-achieved function values at or before \(\ell\)-th iteration, which is defined as

\[f^{\ell}_{\text{best}}=\min_{t=0,1,\ldots,d}f(\mathbf{x}_{I}).\]

If \(C\) is compact, then there exists a constant \(L_{f}>0\) for which \(\|f^{\prime}(\mathbf{x})\|\leq L_{f}\) for all \(f^{\prime}(\mathbf{x})\in\partial f(\mathbf{x})\) and \(\mathbf{x}\in C\), where \(\partial f(\mathbf{x})\) isthe subdifferential of \(f\) at \(\mathbf{x}\). Based on this, we have the following convergence result of Algorithm 5:

**Fact 3** (A special case of Theorem 8.30 in Beck [3]).: _Suppose that Assumption 1 holds and assume that \(C\) is compact. Let \(\Theta\) be an upper bound on the half-squared diameter of \(C\), i.e., \(\Theta\geq\max_{\mathbf{x},\mathbf{y}\in\mathcal{C}}\frac{1}{2}\|\mathbf{x}-\mathbf{y}\|^{2}\). Determine the stepsize \(\eta_{t}\) (\(t=0,1,\ldots\)) as \(\eta_{t}=\frac{\sqrt{2\Theta}}{L_{f}\sqrt{t+1}}\). Then for all \(t\geq 2\), it holds that_

\[f^{t}_{\rm best}-f^{*}\leq\frac{2(1+\log 3)L_{f}\sqrt{2\Theta}}{\sqrt{t+2}}.\]

### Proof of Theorem 7

Proof.: The proof is almost straightforward from Fact 3 in Appendix C.1. By defining \(\widehat{f}_{(G,\Theta)}(\mathbf{x})=\infty\) for any \(\mathbf{x}\in\mathbb{R}^{\rho(\mathbf{y})}\setminus C\), we have an extended real-valued function \(\widehat{f}_{(G,\Theta)}\colon\mathbb{R}^{\rho(\mathbf{x})}\to(-\infty,\infty]\). Then Relaxation becomes a special case of (P) in Appendix C.1. Let us confirm the validity of Assumption 1 in Appendix C.1 for Relaxation:

1. From the definition of \(\widehat{f}_{(G,\Theta)}\) (and \(f_{(G,\Theta)}\)), \(\widehat{f}_{(G,\Theta)}\) does not attain \(-\infty\) and \(\widehat{f}(\mathbf{x})<\infty\) for any \(\mathbf{x}\in C\neq\emptyset\), which means that \(\widehat{f}_{(G,\Theta)}\) is proper. As \(\widehat{f}_{(G,\Theta)}\) is continuous over \(C\) and the (effective) domain of \(\widehat{f}_{(G,\Theta)}\) (i.e., \(C\)) is closed, by Theorem 2.8 in Beck [3], \(\widehat{f}_{(G,\Theta)}\) is closed. As mentioned above, \(\widehat{f}_{(G,\Theta)}\) is convex by the submodularity of \(f_{(G,\Theta)}\).
2. This is trivial.
3. As the boundary of \(C\) is not contained in the interior of the (effective) domain of \(\widehat{f}_{(G,\Theta)}\), this does not hold. However, according to the analysis in Beck [3], this assumption is used only for guaranteeing the subdifferentiability of the objective function over the feasible region (i.e., the subdifferentiability of \(\widehat{f}_{(G,\Theta)}\) over \(C\) in our case), which is clearly valid.
4. From the boundedness of \(C\) and the piecewise-linearity of \(\widehat{f}_{(G,\Theta)}\), we see that the set of optimal solutions to Relaxation is nonempty.

Finally, \(C\) is obviously compact. Therefore, we have the theorem. 

## Appendix D Omitted contents from Section 7

### Counterparts of Figure 1

See Figures 2 and 3.

Figure 3. Quality of solutions of the algorithms (except for Algorithm 2) with \(b=\lfloor\frac{3}{4}\rfloor\rho(\nu)\lfloor\rfloor\).

Figure 2. Quality of solutions of the algorithms (except for Algorithm 2) with \(b=\lfloor\frac{1}{4}\rfloor\rho(\nu)\lfloor\rfloor\).