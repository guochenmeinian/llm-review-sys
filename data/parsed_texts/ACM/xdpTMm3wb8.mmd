# Accurate Cold-start Bundle Recommendation via Popularity-based Coalescence and Curriculum Heating

Anonymous Author(s)

###### Abstract.

How can we accurately recommend cold-start bundles to users? The cold-start problem in bundle recommendation is crucial in practical scenarios since new bundles are continuously created on the Web for various marketing purposes. Despite its importance, existing methods for cold-start item recommendation are not readily applicable to bundles. They depend overly on historical information, even for less popular bundles, failing to address the primary challenge of the highly skewed distribution of bundle interactions. In this work, we propose CoHeat (Popularity-based Coalescence and Curriculum Heating), an accurate approach for cold-start bundle recommendation. CoHeat first represents users and bundles through graph-based views, capturing collaborative information effectively. It then tackles the highly skewed distribution of bundle interactions by incorporating both historical and affiliation information based on the bundle's popularity when estimating the user-bundle relationship. Furthermore, it effectively learns latent representations by exploiting curriculum learning and contrastive learning. CoHeat demonstrates superior performance in cold-start bundle recommendation, achieving up to 193% higher nDCG@20 compared to the best competitor.

bundle recommendation; curriculum learning; contrastive learning +
Footnote †: ccs: Information systems Recommender systems

+
Footnote †: ccs: Information systems Recommender systems

+
Footnote †: ccs: Information systems Recommender systems

+
Footnote †: ccs: Information systems Recommender systems

+
Footnote †: ccs: Information systems Recommender systems

+
Footnote †: ccs: Information systems Recommender systems

+
Footnote †: ccs: Information systems Recommender systems

+
Footnote †: ccs: Information systems Recommender systems

## 1. Introduction

_How can we accurately recommend cold-start bundles to users?_

Bundle recommendation has garnered significant attention in both academia and industry since it enables providers to offer items to users with one-stop convenience (Zhou et al., 2017). In particular, recommending new bundles to users (i.e. cold-start bundle recommendation) has become crucial with the Web's evolution as new bundles are constantly created on the Web for various marketing purposes (Han et al., 2018).

In recent years, bundle recommendation has seen advancements through matrix factorization-based approaches (Chen et al., 2018; Chen et al., 2018; Liu et al., 2018) and graph learning-based approaches (Chen et al., 2018; Liu et al., 2018; Liu et al., 2018). However, they have been developed for a warm-start setting, where all bundles already possess historical interactions with users. Consequently, their efficacy diminishes in cold-start scenarios, where certain bundles are devoid of historical interactions. This is because warm-start methods rely highly on historical information of user-bundle interactions to discern collaborative signals between users and bundles.

On the other hand, the cold-start problem (Zhou et al., 2018) in item recommendation has been extensively studied, with a focus on aligning behavior representations with content representations. For instance, generative methods have aimed to model the generation of item behavior representations using mean squared error (Zhou et al., 2018; Liu et al., 2018), metric learning (Zhou et al., 2018), or adversarial loss (Chen et al., 2018). Dropout-based methods (Zhou et al., 2018; Liu et al., 2018) have aimed to bolster robustness to behavior information by randomly dropping the behavior embedding in the training phase. More recently, contrastive learning-based methods (Zhou et al., 2018; Liu et al., 2018) have shown superior performance by reducing the discrepancy between the distributions of behavior and content information of items. However, the existing methods for cold-start item recommendation fail to achieve high performance in bundle recommendation because they lack the ability to effectively leverage the user-item historical interactions when representing bundles. Furthermore, none of the existing works have explicitly considered the skewed distribution of user-bundle interactions which is a pivotal aspect in bundle recommendation as shown in Figure 0(a). For unpopular bundles, aligning behavior representations from insufficient historical information with content representations amplifies inherent biases and makes it difficult to learn meaningful representations; this results in sacrificing the performance on a warm-start setting to improve the performance on a cold-start setting (see Figure 2).

Figure 1. (a) Extremely skewed distribution of bundle interactions in real-world datasets (data statistics are summarized in Table 1). (b-c) For an unpopular bundle, user-bundle view provides insufficient information while user-item view provides sufficient information.

In this paper, we propose CoHeat (Popularity-based Coalescence and Curriculum Heating), an accurate method for cold-start bundle recommendation. CoHeat constructs representations of users and bundles using two distinct graph-based views: user-bundle view and user-item view. The user-bundle view is grounded in historical interactions between users and bundles, whereas the user-item view is rooted in bundle affiliations and historical interactions between users and items. To handle the extremely skewed distribution as shown in Figure 1a, CoHeat strategically leverages both views in its predictions, emphasizing user-item view for less popular bundles since they provide richer information than the sparse user-bundle view, as shown in Figures 1b and 1c. In addition, to effectively learn the user-item view representations which are fully used for cold-start bundles, CoHeat exploits a curriculum learning approach that gradually shifts the training focus from the user-bundle view to the user-item view. CoHeat further exploits a contrastive learning approach to align the representations of the two views effectively.

Our contributions are summarized as follows:

* **Problem.** To our knowledge, this is the first work that tackles the cold-start problem in bundle recommendation, a challenging problem of significant impact in real-world scenarios.
* **Method.** We propose CoHeat, an accurate method for cold-start bundle recommendation. CoHeat effectively learns user and bundle representations by considering the extremely skewed interactions to accurately recommend cold-start bundles based on their affiliations.
* **Experiments.** We experimentally show that CoHeat provides the state-of-the-art performance achieving up to 193% higher nDCG@20 compared to the best competitor in cold-start bundle recommendation while maintaining competitive performance in warm-start scenarios. (see Figure 2 and Table 2).

The rest of this paper is organized as follows. In section 2, we introduce the problem definition and preliminaries of CoHeat. We then propose CoHeat in Section 3, and present the experimental results in Section 4. We explain the related works in Section 5, and conclude in Section 6. The code and datasets are available at [https://github.com/ColdBundle/CoHeat](https://github.com/ColdBundle/CoHeat).

## 2. Preliminaries

### Problem Definition

The problem of cold-start bundle recommendation is defined as follows. Let \(\mathcal{U}\), \(\mathcal{B}\), and \(\mathcal{I}\) be the sets of users, bundles, and items, respectively. Among the bundles, \(\mathcal{B}_{w}\subset\mathcal{B}\) refers to the warm-start bundles that have at least one historical interaction with users, while \(\mathcal{B}_{e}=\mathcal{B}\setminus\mathcal{B}_{w}\) represents the cold-start bundles that lack any historical interaction with users. The observed user-bundle interactions, user-item interactions, and bundle-item affiliations are defined respectively as \(\mathcal{X}=\{(u,b)|u\in\mathcal{U},b\in\mathcal{B}_{w}\}\), \(\mathcal{Y}=\{(u,i)|u\in\mathcal{U},i\in\mathcal{I}\}\), and \(\mathcal{Z}=\{(b,i)|b\in\mathcal{B},i\in\mathcal{I}\}\). Given \(\{\mathcal{X},\mathcal{Y},\mathcal{Z}\}\), our goal is to recommend \(k\) bundles from \(\mathcal{B}\) to each user \(u\in\mathcal{U}\). Note that the given interactions are observed only for warm bundles but the objective includes recommending also cold bundles to users.

The central challenge in cold-start bundle recommendation, compared to traditional bundle recommendation, lies in accurately predicting the relationship between a user \(u\in\mathcal{U}\) and a cold-start bundle \(b\in\mathcal{B}_{e}\) in the absence of any historical interactions of \(b\). Hence, the crux of addressing the problem is to effectively estimate the representations of cold-start bundles using their affiliation information.

Bundles are compositions of multiple items, each having distinct interactions with users. This contrasts sharply with traditional cold-start item recommendations where contents are often represented as independent entities like texts or images. Moreover, user-bundle interactions exhibit a pronounced skewness, far more than typical user-item interactions. These make the cold-start bundle recommendation uniquely challenging compared to the cold-start item recommendation.

### Curriculum Learning

Curriculum learning, inspired by human learning, structures training from simpler to more complex tasks, unlike standard approaches that randomize task order (Brockman et al., 2017; Chen et al., 2018). Its effectiveness has been proven in various domains, including computer vision (Zhu et al., 2019; Zhang et al., 2019), natural language processing (Gulwani et al., 2017; Chen et al., 2018), robotics (Gulwani et al., 2017; Chen et al., 2018), and recommender systems (Chen et al., 2018; Chen et al., 2018).

In this work, we harness curriculum learning to enhance the learning process of user-bundle relationships. We initiate with a

Figure 2. Performance comparison of CoHeat with competitors on three real-world datasets: Youshu, NetEase, and iFashion. The performance is evaluated through Recall@20 for all experiments. We mark cold-start methods as orange, and warm-start methods as red. The cold-start methods typically sacrifice warm setting performance to excel in cold settings. The warm-start methods show poor performance in cold settings. CoHeat demonstrates superior performance over existing methods in both cold and warm settings, with a notable advantage in outperforming competitors.

focus on the more straightforward user-bundle view embeddings and then progressively shift attention to the intricate user-item view embeddings. This strategy stems from the ease of learning user-bundle view embeddings, which directly capture collaborative signals from historical interactions. In contrast, user-item view embeddings are more complicated due to their dependence on the representations of affiliated items.

### Contrastive Learning

Contrastive learning aims to derive meaningful embeddings by distinguishing between similar and dissimilar data samples. This approach has consistently demonstrated superior performance across a range of research fields, including computer vision (He et al., 2016; He et al., 2017; He et al., 2018), natural language processing (He et al., 2016; He et al., 2017), and recommender systems (He et al., 2018; He et al., 2018). In bundle recommendation, CrossCBR (He et al., 2018) has utilized InfoNCE (He et al., 2018) as a contrastive learning function to regularize embeddings of users and bundles between the user-bundle and user-item views. However, its approach of aligning the two views without differentiation in prediction can be limiting, especially in cold-start scenarios where the user-bundle view is sparse.

In this work, we enhance the application of contrastive learning in bundle recommendation. Instead of treating the two views equally, we dynamically adjust their weights based on bundle popularity. This facilitates the transfer of information from a more informative view to the counterpart, enabling effective recommendations for both cold and warm bundles. Furthermore, we leverage the alignment and uniformity loss (He et al., 2017), which has been demonstrated to outperform InfoNCE in various applications (He et al., 2018; He et al., 2018; He et al., 2018). This loss function directly optimizes the foundational principles of contrastive learning, ensuring more robust and meaningful embeddings.

## 3. Proposed Method

### Overview

We address the following challenges to achieve high performance in cold-start bundle recommendation.

* **Handling highly skewed interactions.** Previous works depend overly on user-bundle view representations, which are unreliable if bundles have sparse interactions. How can we effectively learn the representations from highly skewed interactions?
* **Effectively learning user-item view representations.** Despite the ample information provided by the user-item view, multiple items in a bundle complicate the learning of these representations. How can we effectively learn the user-item view representations?
* **Bridging the gap between two view representations.** Aligning user-bundle and user-item views is crucial, as we estimate future interactions of cold bundles using only their affiliations. How can we effectively reconcile these two view representations?

To address these challenges, we propose CoHeat (Popularity-based Coalescence and Curriculum Heating) with the following main ideas.

* **Popularity-based coalescence.** For the score between users and bundles, we propose the coalescence of two view scores, with less popular bundles relying more on user-item view scores and less on user-bundle view scores.
* **Curriculum heating.** We propose a curriculum learning approach that focuses initially on training representations using the user-bundle view, gradually shifting the focus to the user-item view.
* **Representation alignment and uniformity.** We exploit a representation alignment and uniformity approach to effectively reconcile the user-bundle view and user-item view representations.

Figure 3 depicts the schematic illustration of CoHeat. Given user-bundle interactions, user-item interactions, and bundle-item affiliations, CoHeat forms two graph-based views. Then, it predicts user-bundle scores by coalescing scores from both views based on bundle popularity. During training, CoHeat prioritizes user-bundle view initially, transitioning progressively to user-item view via

Figure 3. Overview of CoHeat (see Section 3 for details).

curriculum heating. CoHeat also exploits alignment and uniformity loss to regularize both views.

### Two Graph-based Views

The objective of bundle recommendation is to estimate the relationship between users and bundles by learning their latent representations. We utilize graph-based representations of users and bundles to fully exploit the given user-bundle interactions, user-item interactions, and bundle-item affiliations. We construct user-bundle view and user-item view graphs and use LightGCN (Li et al., 2017) to obtain embeddings of users and bundles (Li et al., 2018).

**User-bundle view representation and score.** In user-bundle view, we aim to capture the behavior signal between users and bundles. Specifically, we construct a bipartite graph using user-bundle interactions, and propagate the historical information using a LightGCN. The \(k\)-th layer of the LightGCN is computed as follows:

\[\mathbf{h}_{u}^{(k)}=\sum_{b\in\mathcal{N}_{u}}\frac{1}{\sqrt{|\mathcal{N}_{u} |}\sqrt{|\mathcal{N}_{b}|}}\mathbf{h}_{b}^{(k)},\mathbf{h}_{b}^{(k)}=\sum_{u\in \mathcal{N}_{b}}\frac{1}{\sqrt{|\mathcal{N}_{b}|}\sqrt{|\mathcal{N}_{u}|}} \mathbf{h}_{u}^{(k-1)}, \tag{1}\]

where \(\mathbf{h}_{u}^{(k)},\mathbf{h}_{b}^{(k)}\in\mathbb{R}^{d}\) are the embeddings of user \(u\) and bundle \(b\) at \(k\)-th layer, respectively; \(\mathcal{N}_{u}\) and \(\mathcal{N}_{b}\) are the sets of user \(u\)'s neighbors and bundle \(\mathbf{b}_{i}^{(0)},\mathbf{h}_{b}^{(0)}\in\mathbb{R}^{d}\) are randomly initialized before the training of the model. We obtain the user-bundle view representations of user \(u\) and bundle \(b\) by aggregating the embeddings from all layers with a weighting approach that places greater emphasis on the lower layers as follows:

\[\mathbf{h}_{u}=\sum_{k=0}^{K}\frac{1}{k+1}\mathbf{h}_{u}^{(k)},\mathbf{h}_{b}= \sum_{k=0}^{K}\frac{1}{k+1}\mathbf{h}_{b}^{(k)}, \tag{2}\]

where \(\mathbf{h}_{u},\mathbf{h}_{b}\in\mathbb{R}^{d}\) are the user-bundle view embeddings of user \(u\) and bundle \(b\), respectively; \(K\) denotes the last layer. Finally, the user-bundle view score between user \(u\) and bundle \(b\) is defined as \(h_{u}=\mathbf{h}_{u}^{T}\mathbf{h}_{b}\).

**User-item view representation and score.** In user-item view, we aim to learn the relationship between users and bundles from the perspective of item affiliations. Specifically, we construct a bipartite graph using user-item interactions, and propagate the historical information using another LightGCN. Then, we obtain bundle representations by aggregating the affiliated items' representations. The \(k\)-th layer of the LightGCN is computed as follows:

\[\mathbf{a}_{u}^{(k)}=\sum_{i\in\mathcal{N}_{u}}\frac{1}{\sqrt{|\mathcal{N}_{u} |}\sqrt{|\mathcal{N}_{i}|}}\mathbf{a}_{i}^{(k-1)},\mathbf{a}_{i}^{(k)}=\sum_{u \in\mathcal{N}_{i}}\frac{1}{\sqrt{|\mathcal{N}_{i}|}\sqrt{|\mathcal{N}_{u}|}} \mathbf{a}_{u}^{(k-1)}, \tag{3}\]

where \(\mathbf{a}_{u}^{(k)},\mathbf{a}_{i}^{(k)}\in\mathbb{R}^{d}\) are the embeddings of user \(u\) and item \(i\) at \(k\)-th layer, respectively; \(\mathcal{N}_{u}^{(k)}\) and \(\mathcal{N}_{i}\) are the sets of user \(u\)'s neighbors and item \(i\)'s neighbors in the user-item graph, respectively. \(\mathbf{a}_{u}^{(0)},\mathbf{a}_{i}^{(0)}\in\mathbb{R}^{d}\) are randomly initialized before the training. We obtain the user-item view representations of user \(u\) and item \(i\) by aggregating the embeddings from all layers with a weighting approach as follows:

\[\mathbf{a}_{u}=\sum_{k=0}^{K}\frac{1}{k+1}\mathbf{a}_{u}^{(k)},\mathbf{a}_{i}= \sum_{k=0}^{K}\frac{1}{k+1}\mathbf{a}_{i}^{(k)}, \tag{4}\]

where \(\mathbf{a}_{u},\mathbf{a}_{i}\in\mathbb{R}^{d}\) are the user-item view embeddings of user \(u\) and section \(i\), respectively; \(K\) indicates the last layer. We then obtain the user-item view representations of bundle \(b\) by an average pooling as \(\mathbf{a}_{b}=\frac{1}{|\mathcal{N}_{u}|}\sum_{i\in\mathcal{N}_{u}}\mathbf{a}_ {i}\), where \(\mathcal{N}_{b}^{\prime}\) is the set of bundle \(b\)'s affiliated items. Finally, the user-item view score between user \(u\) and bundle \(b\) is defined as \(a_{u}=\mathbf{a}_{u}^{T}\mathbf{a}_{b}\).

### Popularity-based Coalescence

For recommending bundles to users, our objective is to estimate the final score \(\hat{y}_{ub}\in\mathbb{R}\) between user \(u\) and bundle \(b\) using scores \(h_{ub}\) and \(a_{ub}\), derived from the two distinct views. However, real-world datasets present an inherent challenge of handling the extremely skewed distribution of interactions between users and bundles, as illustrated in Figure 0(a). While both views are informative, many unpopular bundles are underrepresented in the user-bundle view due to the insufficient interactions as illustrated in Figure 0(b). In contrast, they are often sufficiently represented in the user-item view, as depicted in Figure 0(c). A uniform weighting strategy for both views, as in CrossCBR, risks amplifying biases inherent to the user-bundle view, especially for the unpopular bundles. This predicament is further exacerbated for cold-start bundles devoid of interactions in user-bundle view.

To deal with this challenge, we propose two desired properties for the user-bundle relationship score \(\hat{y}_{ub}\).

**Property 1** (User-bundle view influence mitigation): The influence of user-bundle view score should be mitigated as a bundle's interaction number decreases, i.e. \(\frac{\partial\hat{y}_{ub}}{\partial h_{ub}}<\frac{\partial\hat{y}_{ub}}{ \partial h_{ub}}\) if \(n_{b}<n_{b^{\prime}}\) where \(n_{b}\) is the number of user interactions of bundle \(b\).

**Property 2** (User-item view influence amplification): The influence of user-item view score should be amplified as a bundle's interaction number decreases, i.e. \(\frac{\partial\hat{y}_{ub}}{\partial h_{ub}}>\frac{\partial\hat{y}_{ub}}{ \partial h_{ub}}\) if \(n_{b}<n_{b^{\prime}}\) where \(n_{b}\) is the number of user interactions of bundle \(b\).

Properties 1 and 2 are crucial in achieving a balanced interplay between the user-bundle view and user-item view scores based on bundle popularities. Specifically, they ensure a heightened emphasis on the user-item view over the user-bundle view for less popular bundles.

We propose the user-bundle relationship score \(\hat{y}_{ub}\) that satisfies the two desired properties by weighting the two scores \(h_{ub}\) and \(a_{ub}\) based on bundle popularies as follows:

\[\hat{y}_{ub}=\gamma_{b}h_{ub}+(1-\gamma_{b})a_{ub}, \tag{5}\]

where \(\gamma_{b}\in[0,1]\), which is defined in the next subsection, denotes a weighting coefficient such that \(\gamma_{b}>\gamma_{b^{\prime}}\) if \(n_{b}>n_{b^{\prime}}\). A smaller value of \(\gamma_{b}\) (i.e. a smaller value of \(\eta_{b}\)) ensures that the score \(\hat{y}_{ub}\) is predominantly influenced by the user-item view score \(a_{ub}\). We show in Lemmas 3.1 and 3.2 that Equation (5) satisfies all the desired properties.

**Lemma 3.1**.: _Equation (5) satisfies Property 1._

Proof.: \(\frac{\partial\hat{x}_{ub}}{\partial h_{ub}}=\gamma_{b}\). Thus, \(\frac{\partial\hat{y}_{ub}}{\partial h_{ub}}<\frac{\partial\hat{y}_{ub^{\prime}}}{ \partial h_{ub^{\prime}}}\) if \(n_{b}<n_{b^{\prime}}\) because \(\gamma_{b}<\gamma_{b^{\prime}}\). 

**Lemma 3.2**.: _Equation (5) satisfies Property 2._

Proof.: \(\frac{\partial\hat{x}_{ub}}{\partial h_{ub}}=1-\gamma_{b}\). Thus, \(\frac{\partial\hat{y}_{ub}}{\partial h_{ub}}>\frac{\partial\hat{y}_{ub^{\prime}}}{ \partial a_{ub^{\prime}}}\) if \(n_{b}<n_{b^{\prime}}\) because \(1-\gamma_{b}>1-\gamma_{b^{\prime}}\). 

### Curriculum Heating

Despite the ample information provided by the user-item view, multiple items in a bundle complicate the learning of user-item representations. This difficulty arises because accurate representation of a bundle necessitates well-represented embeddings of its all affiliated items, and each item further requires well-represented embeddings of the connected users. On the other side, the user-bundle view representation is relatively straightforward to learn. This simplicity arises because we encapsulate each bundle's historical characteristics into a single embedding rather than understanding the intricate composition of the bundle.

Hence, we propose to focus initially on learning easier view representations and gradually shift the focus to learning harder view representations. Thus, we modify Equation (5) by exploiting a curriculum learning approach that focuses initially on training user-bundle view representations, and gradually shifts the focus to the user-item view representations as follows:

\[\hat{y}_{ub}^{(t)}=\gamma_{b}^{(t)}h_{ub}+(1-\gamma_{b}^{(t)})a_{ub}, \tag{6}\]

where \(\hat{y}_{ub}^{(t)}\in\mathbb{R}\) is the estimated relationship score between user \(u\) and bundle \(b\) at epoch \(t\). \(\gamma_{b}^{(t)}\in\mathbb{R}\) is defined as \(\gamma_{b}^{(t)}=\tanh\left(\frac{n_{b}}{\hat{y}(t)}\right)\), where \(n_{b}\) is the number of interactions of bundle \(b\), and \(\psi^{(t)}>0\) is the temperature at epoch \(t\). Note that \(\gamma_{b}^{(t)}\) lies within the interval \([0,1]\) because \(\frac{n_{b}}{\hat{y}(t)}\geq 0\). Then, we incrementally raise the temperature \(\psi^{(t)}\) up to the maximum temperature as follows:

\[\hat{y}^{(t)}=\epsilon^{t/T},t:0\to T, \tag{7}\]

where \(t,T\in\mathbb{R}\) are the current and the maximum epochs of the training process, and \(\epsilon>1\) is the hyperparameter of the maximum temperature. In the initial epochs of training, \(\gamma_{b}^{(t)}\) is large since \(t\) is small. As a result, the score \(\hat{y}_{ub}^{(t)}\) relies more heavily on \(h_{ub}\) than \(a_{ub}\). However, as the training progresses and \(t\) increases, \(\gamma_{b}^{(t)}\) diminishes, shifting the emphasis from \(h_{ub}\) to \(a_{ub}\). This heating mechanism is applied to all bundles regardless of their popularity. Furthermore, we show in Lemmas 3.3 and 3.4 that Equation (6) still satisfies the two desired properties.

**Lemma 3.3**.: _Equation (6) satisfies Property 1._

Proof.: \(\frac{\partial y_{ub}^{(t)}}{\partial h_{ub}}=\tanh\left(\frac{n_{b}}{\hat{y}(t )}\right)\). Thus, \(\frac{\partial\hat{y}_{ub}^{(t)}}{\partial h_{ub}}<\frac{\partial\hat{y}_{ub}^{ (t)}}{\partial h_{ub}}\) if \(n_{b}<n_{b^{\prime}}\) because \(\hat{y}^{(t)}\) is the same for all bundles at epoch \(t\) and \(tanh(\cdot)\) is an increasing function. 

**Lemma 3.4**.: _Equation (6) satisfies Property 2._

Proof.: \(\frac{\partial\hat{y}_{ub}^{(t)}}{\partial u_{ub}}=1-\tanh\left(\frac{n_{b}}{ \hat{y}(t)}\right)\). Thus, \(\frac{\partial\hat{y}_{ub}^{(t)}}{\partial u_{ub}}>\frac{\partial\hat{y}_{ub}^{ (t)}}{\partial u_{ub}}\) if \(n_{b}<n_{b^{\prime}}\) because \(\hat{y}^{(t)}\) is the same for all bundles at epoch \(t\) and \(1-tanh(\cdot)\) is a decreasing function. 

### Representation Alignment and Uniformity

While the user-bundle view and user-item view are crafted to capture distinct representations, aligning the two views is essential, especially when predicting future interactions of cold bundles solely based on user-item view representations. Moreover, aligning two views facilitates knowledge transfer between the two views. This is essential because we gradually change the learning focus of views by curriculum heating, and the alignment helps the success of curriculum heating by effectively transferring knowledge from a view with richer knowledge to the opposite view. To achieve this, we exploit a contrastive learning-based approach that reconciles the two views. Specifically, we use the alignment and uniformity loss (Spiegel et al., 2015) as a regularization for the representations of the two views. We firstly \(l_{2}\)-normalize the embeddings of the two views as follows:

\[\hat{\mathbf{h}}_{u}=\frac{\mathbf{h}_{u}}{\|\mathbf{h}_{u}\|_{2}},\hat{ \mathbf{a}}_{u}=\frac{\mathbf{a}_{u}}{\|\mathbf{a}_{u}\|_{2}},\hat{\mathbf{h}} _{b}=\frac{\mathbf{h}_{b}}{\|\mathbf{h}_{b}\|_{2}},\hat{\mathbf{a}}_{b}=\frac {\mathbf{a}_{b}}{\|\mathbf{a}_{b}\|_{2}}, \tag{8}\]

where \(\mathbf{h}_{u},\mathbf{h}_{b}\in\mathbb{R}^{d}\) are user-bundle view representations of user \(u\) and bundle \(b\), respectively; \(\mathbf{a}_{u},\mathbf{a}_{b}\in\mathbb{R}^{d}\) are user-item view representations of user \(u\) and bundle \(b\), respectively. Then, we define an alignment loss as follows:

\[l_{align}=\underset{u\sim p_{user}}{\mathbb{E}}\quad\|\hat{\mathbf{h}}_{u}- \hat{\mathbf{a}}_{u}\|_{2}^{2}+\underset{b\sim p_{bundle}}{\mathbb{E}}\quad \|\hat{\mathbf{h}}_{b}-\hat{\mathbf{a}}_{b}\|_{2}^{2} \tag{9}\]

where \(p_{user}\) and \(p_{bundle}\) are the distributions of users and bundles, respectively. The alignment loss makes the embeddings of the two views close to each other for each user and bundle. We also define a uniformity loss as follows:

\[l_{uniform}=\log\underset{u,u^{\prime}\sim p_{user}}{\mathbb{E}}e^{-2\| \hat{\mathbf{h}}_{u}-\hat{\mathbf{h}}_{u^{\prime}}\|_{2}^{2}}+\log\underset{u,u ^{\prime}\sim p_{user}}{\mathbb{E}}e^{-2\|\hat{\mathbf{h}}_{u}-\hat{\mathbf{ h}}_{u^{\prime}}\|_{2}^{2}} \tag{10}\]

where \(u^{\prime}\) and \(b^{\prime}\) denote a user and a bundle distinct from \(u\) and \(b\), respectively. The uniformity loss ensures distinct representations for different users (or bundles) by scattering them across the space. Finally, we define the contrastive loss for the two views as follows:

\[\mathcal{L}_{AU}=l_{align}+l_{uniform}. \tag{11}\]

### Objective Function and Training

To effectively learn the user-bundle relationship, we utilize Bayesian Personalize Ranking (BPR) loss (Zhou et al., 2017), which is the most widely used loss owing to its powerfulness, as follows:

\[\mathcal{L}_{BPR}^{(t)}=\underset{(u\not{b}^{\prime},\not{b}^{\prime})-p_{data}} {\mathbb{E}}-\ln\sigma(\hat{y}_{ub^{\prime}}^{(t)}-\hat{y}_{ub^{\prime}}^{(t)}), \tag{12}\]

where \(p_{data}\) is the data distribution of user-bundle interactions, with \(u\) denoting a user, \(b^{+}\) indicating a positive bundle, and \(b^{-}\) representing a negative bundle. We define the final objective function as follows:

\[\mathcal{L}^{(t)}=\mathcal{L}_{BPR}^{(t)}+\lambda_{1}\mathcal{L}_{AU}+\lambda_{2 }\|\Theta\|_{2}, \tag{13}\]

where \(\lambda_{1},\lambda_{2}\in\mathbb{R}\) are balancing hyperparameters for the terms, and \(\Theta\) denotes trainable parameters of CoHeAr. For the distributions \(p_{user}\) and \(p_{bundle}\), we use in-batch sampling which selects samples from the training batch of \(p_{data}\) rather than the entire dataset. This approach has empirically demonstrated to mitigate the training bias in prior studies (Zhou et al., 2017; Wang et al., 2018). All the parameters are optimized in an end-to-end manner through the optimization. We also adopt an edge dropout (Zhou et al., 2017; Wang et al., 2018) while training to enhance the performance robustness.

### Discussion of CoHeat

The core of CoHeat lies in its ability to dynamically adjust the weights of two distinct views, setting it apart from previous methods such as CrossCBR (Zhu et al., 2017). This dynamic adjustment is pivotal for achieving superior performance in the cold-start bundle recommendation.

Through the popularity-based coalescence, CoHeat dynamically adjusts the weight \(\gamma_{b}^{(t)}\) in Equation (6) to effectively harness the more informative view. For instance, when a bundle \(b\) is popular, the influence of user-bundle view is bolstered with a large \(\gamma_{b}^{(t)}\) because the bundle is rich of knowledge in this view. The knowledge then gets transferred to the user-item view by the alignment and uniformity loss, as depicted in Figure 4 (a). Conversely, for a less popular bundle, the influence of user-item view is amplified with a small \(\gamma_{b}^{(t)}\), transferring the learned knowledge to the user-bundle view, as shown in Figure 4 (b). This strategy contrasts with CrossCBR, which may inadvertently amplify the underrepresented knowledge of unpopular bundles due to its uniform weighting strategy.

Additionally, the curriculum heating of CoHeat further adjusts the weight \(\gamma_{b}^{(t)}\) throughout the learning process. As the epochs progress, \(\gamma_{b}^{(t)}\) diminishes (transitioning from Figure 4 (a) to Figure 4 (b)), ensuring both views are thoroughly utilized during training. This dynamic exchange of knowledge between two views results in CoHeat's superior performance in both cold and warm settings, owing to the wealth of knowledge each view offers. This strategy is distinct from CrossCBR since the uniform weights for both views may lead to suboptimal results, especially in cold-start scenarios where the user-bundle view is sparse. Moreover, the curriculum heating strategically focuses on the easier view first, gradually shifting its attention to the more challenging view as the learning progresses. This helps a smoother and more effective knowledge transfer between the views.

## 4. Experiments

In this section, we perform experiments to answer the following questions.

* **Comparison with cold-start methods.** Does CoHeat show superior performance in comparison to other cold-start methods in bundle recommendation?
* **Comparison with warm-start methods.** Does CoHeat show similar performance in warm-start bundle recommendation compared with baselines, although CoHeat is a cold-start bundle recommendation method?
* **Comparison by cold bundle ratio.** Does the performance difference between CoHeat and baseline increase as the cold bundle ratio increases?
* **Ablation study.** How do the main ideas of CoHeat affect the performance?
* **Effect of the maximum temperature.** How does the maximum temperature \(\epsilon\), the critical hyperparameter, affect the performance of CoHeat?

### Experimental Setup

**Datasets.** We use three real-world bundle recommendation datasets as summarized in Table 1. Youshu (2017) comprises bundles of books sourced from a book review site; NetEase (2018) features bundles of music tracks from a cloud music service; 'Fashion (2018) consists of bundles of fashion items from an outlet sales platform.

**Baseline cold-start methods.** We compare CoHeat with existing cold-start item recommendation methods because they can be easily adapted for bundle recommendation by considering bundle-item affiliations as content information. DropoutNet (Zhu et al., 2017) is a robustness-based method with a dropout operation. CB2CF (Chen et al., 2018) and Heater (2018) are constraint-based methods that regularize the alignment. GAR (Chen et al., 2018) is a generative method with two variants GAR-CF and GAR-GNN. CVAR (2018) is another generative method with a conditional decoder. CLCRec (Zhu et al., 2017) and CCFCRec (Zhu et al., 2017) are contrastive learning-based methods. We omit other competitors such as DUIF (Chen et al., 2018), MTPR (Chen et al., 2018), and NFM (Chen et al., 2018) because CLCRec and CCFCRec outperform them by a large margin on their extensive experiments. We also omit other generative competitors such as DeepMusic (Zhu et al., 2017) and MetaEmb (Zhu et al., 2017) because GAR (Chen et al., 2018) greatly outperforms them on its experiments. We use bundle-item multi-hot vectors as their content information.

**Baseline warm-start methods.** We also compare CoHeat with previous warm-start recommendation methods. MFBPR (Zhu et al., 2017) and LightGCN (Zhu et al., 2017) are item recommendation methods with the modelings of matrix factorization and graph learning, respectively. SGL (Zhu et al., 2017), SimGCL (Zhu et al., 2017), and LightGCL (Chen et al., 2018) are the improved methods of item recommendation with contrastive learning approaches. DAM (Chen et al., 2018) is a bundle recommendation method with the modeling of matrix factorization. BundleNet (Chen et al., 2018), BGCN (Chen et al., 2018; Chen et al., 2018), and CrossCBR (Zhu et al., 2017) are other bundle recommendation methods with the modeling of graph learning.

**Evaluation metrics.** We use Recall@\(k\) and nDCG@\(k\) metrics as in previous works (Zhu et al., 2017; Zhu et al., 2017). Recall@\(k\) measures the proportion of relevant items in the top-\(k\) list, while nDCG@\(k\) weighs items by their rank. We set \(k\) to 20. In tables, bold and underlined values indicate the best and second-best results, respectively.

\begin{table}
\begin{tabular}{l|r r r r r r r} \hline \hline
**Dataset** & **Users** & **Bundles** & **Items** & **User-bundle (dens.)** & **User-item (dens.)** & **Bundle-item (dens.)** & **Avg. size of bundle** \\ \hline Youshu\({}^{1}\) & 8,039 & 4,771 & 32,770 & 51,377 (0.13\%) & 138,515 (0.05\%) & 176,667 (0.11\%) & 37.03 \\ NetEase\({}^{1}\) & 18,528 & 22,864 & 123,628 & 302,303 (0.07\%) & 1,128,065 (0.05\%) & 1,778,838 (0.06\%) & 77.80 \\ iFashion\({}^{1}\) & 53,897 & 27,694 & 42,563 & 1,679,708 (0.11\%) & 2,290,645 (0.10\%) & 106,916 (0.01\%) & 3.86 \\ \hline \hline \end{tabular}

* [https://github.com/myshapt/CrossCBR](https://github.com/myshapt/CrossCBR)

\end{table}
Table 1. Summary of three real-world datasets where “dens.” denotes the density of a matrix.

Figure 4. Learning mechanism of CoHeat (see Section 3.7 for details).

**Experimental process.** We conduct experiments in warm-start, cold-start, and all-bundle scenarios as in previous works (Shi et al., 2019). For the warm-start scenario, interactions are split into 7:1:2 subsets for training, validation, and testing. In the cold-start scenario, bundles are split in 7:1:2 ratio. In the all-bundle scenario, interactions are split in 7:1:2 ratio with a half for warm-start and the other half for cold-start bundles. We report the best Recall@20 and nDCG@20 within 100 epochs, averaged over three runs.

**Hyperparameters.** We utilize the baselines with their official implementations and use their reported best hyperparameters. We implement CoHeat with PyTorch. We set the dimensionality \(d\) of node embeddings as 64. The other hyperparameters are grid-searched: the learning rate in (0.001, 0.0001, 0.00001), \(\lambda_{1}\) in {0.1, 0.2, 0.5, 1.0}, \(\lambda_{2}\) in {0.00004, 0.0001, 0.0004, 0.001}, \(K\) in {1, 2}, and the maximum temperature \(\epsilon\) in {10\({}^{1}\), 10\({}^{2}\), 10\({}^{3}\), 10\({}^{4}\), 10\({}^{5}\), 10\({}^{6}\)}.

### Comparison with Cold-start Methods (Q1)

In Table 2, we compare CoHeat with baseline cold-start methods. The results show that CoHeat consistently surpasses the baselines across all datasets and settings, verifying its superiority. Notably, CoHeat achieves 193% higher nDCG@20 compared to CCFRce, the best competitor, on the iFashion dataset in the all-bundle scenario. The superiority of CoHeat over other cold-start methods stems from the following two key aspects. First, CoHeat adaptively harnesses collaborative information of each affiliated item in a bundle through the user-item view. This approach diverges from existing cold-start methods, which fall short in utilizing user-item interactions for bundle affiliations. Second, CoHeat explicitly addresses the pronounced skewness in user-bundle interactions through the proposed popularity-based coalescence. The results reveal the importance of tackling the inherent biases in distributions with extreme skewness, such as user-bundle interactions.

### Comparison with Warm-start Methods (Q2)

Table 3 compares CoHeat with baseline warm-start methods in the warm-start scenario. Even though CoHeat is primarily designed for cold-start bundle recommendation, it surpasses all the baselines in the warm-start scenario as well. This indicates CoHeat effectively learns representations from both user-bundle and user-item views by dynamically adjusting the weights of two views in training. For the baselines, the performance improves when contrastive learning is used as exemplified in SGL, SimGCL, LightGCL, and CrossCBR. Additionally, graph-based models such as LightGCN, SGL, SimGCL, LightGCL, BCN, and CrossCBR typically excel over other non-graph-based models. In light of these observations, CoHeat strategically exploits a graph-based modeling approach and harnesses the power of contrastive learning. This makes CoHeat robustly achieve the highest performance across diverse scenarios.

### Comparison by Cold Bundle Ratio (Q3)

In Figure 5, we compare the performance between CoHeat and CrossCBR on NetEase and iFashion datasets under varying cold bundle ratios in test datasets. We focus on investigating the performance disparity as conditions become increasingly colder, despite their analogous performance in warm settings, as shown in Table 3. The figure reveals a pronounced performance disparity between

\begin{table}
\begin{tabular}{l|c c c|c c c|c c c|c c c|c c c|c c c} \hline \hline \multirow{2}{*}{**Model**} & \multicolumn{4}{c|}{**Youshui**} & \multicolumn{4}{c|}{**NetEase**} & \multicolumn{4}{c}{**iFashion**} \\  & \multicolumn{4}{c|}{Recall@20} & \multicolumn{4}{c|}{nDCG@20} & \multicolumn{4}{c|}{Recall@20} & \multicolumn{4}{c}{nDCG@20} & \multicolumn{4}{c}{Recall@20} & \multicolumn{4}{c}{nDCG@20} \\  & _Cold_ & _Warm_ & _All_ & _Cold_ & _Warm_ & _All_ & _Cold_ & _Warm_ & _All_ & _Cold_ & _Warm_ & _All_ & _Cold_ & _Warm_ & _All_ & _Cold_ & _Warm_ & _All_ & _Cold_ & _Warm_ & _All_ & _Total_ \\ \hline DropoutNet (Shi et al., 2019) &.0022 &.0336 &.0148 &.0007 &.0153 &.0055 &.0028 &.0154 &.0046 &.0015 &.0078 &.0024 &.0099 &.0600 &.0039 &.0008 &.0045 &.0027 &.798 \\ CB2CF (Chen et al., 2019) &.0012 &.0258 &.0028 &.0007 &.0208 &.0021 &.0016 &.0049 &.0027 &.0066 &.0027 &.0014 &.0009 &.0057 &.0066 &.0006 &.0043 &.0048 &.790 \\ Heater (Chen et al., 2019) &.0016 &.1753 &.0541 &.0007 &.0826 &.0286 &.0021 &.0125 &.0102 &.0102 &.0106 &.0064 &.0054 &.0015 &.0217 &.0123 &.0010 &.0151 &.0083 &.761 \\ GAR-CF (Chen et al., 2019) &.0015 &.1688 &.0529 &.0011 &.0726 &.0317 &.0100 &.0063 &.0041 &.0005 &.0035 &.0008 &.0013 &.0203 &.0090 &.0013 &.0143 &.0055 &.762 \\ GAR-GNN (Chen et al., 2019) &.0013 &.0385 &.0358 &.0006 &.0569 &.0178 &.0096 &.0056 &.0027 &.0003 &.0030 &.0012 &.0055 &.0172 &.0126 &.0030 &.0167 &.0087 &.781 \\ CVAR (Chen et al., 2019) &.0089 &.0582 &.0829 &.0002 &.0112 &.0533 &.0002 &.0308 &.0156 &.0001 &.0154 &.0007 &.0220 &.0125 &.0004 &.0152 &.0084 &.741 \\ CLRec (Shi et al., 2019) &.0137 &.0626 &.0367 &.0087 &.0317 &.0194 &.0136 &.0407 &.0259 &.0075 &.0215 &.0138 &.0053 &.0203 &.0126 &.0043 &.0135 &.0085 \\ CCFRce (Shi et al., 2019) &.0044 &.1554 &.0702 &.0022 &.0798 &.0425 &.0007 &.0265 &.0130 &.0004 &.0128 &.0068 &.0005 &.0439 &.0252 &.0003 &.0304 &.0172 &.766 \\ \hline
**CoHeat (ours)** & **.0183** & **.2804** & **.1247** & **.0105** & **.1646** & **.0883** & **.0191** & **.0874** & **.0453** & **.0093** & **.0455** & **.0264** & **.0170** & **.1156** & **.0658** & **.0096** & **.0876** & **.0504** & **.0504** \\ \hline \hline \end{tabular}
\end{table}
Table 2. Performance comparison of CoHeat and baseline cold-start methods on three real-world datasets.

Figure 5. Performance comparison by cold bundle ratio.

### Ablation Study (Q4)

Table 4 provides an ablation study that compares CoHeat with its four variants CoHeat-_PC_, CoHeat-_CH-Ant_, CoHeat-_CH-Fix_, and CoHeat-_AU_. This study is conducted in the cold-start scenario, which is the primary focus of our work. In CoHeat-_PC_, we remove the influence of popularity-based coalescence by setting the value of \(\gamma_{p}^{(t)}\) in Equation (6) to a constant 0.5. For CoHeat-_CH-Ant_, we exploit an anti-curriculum learning strategy. The temperature in Equation (7) is defined as \(t:T\to 0\), initiating the learning process with the user-item view and gradually shifting the focus to the user-bundle view. For CoHeat-_CH-Fix_, we remove the effect of curriculum learning by setting the value of the \(\psi^{(t)}\) in Equation (7) to the fixed maximum temperature \(\epsilon\) regardless of epochs. For CoHeat-_AU_, we omit \(\mathcal{L}_{AU}\) from Equation (13), thereby excluding the contrastive learning between the two views. As shown in the table, CoHeat consistently outperforms all the variants, which verifies all the main ideas help improve the performance. In particular, CoHeat-_PC_ shows a severe performance drop, justifying the importance of satisfying Properties 1 and 2 when addressing the extreme skewness inherent in cold-start bundle recommendation.

### Effect of the Maximum Temperature (Q5)

The maximum temperature \(\epsilon\) in Equation (7) is the most influential hyperparameter of CoHeat since it directly affects both popularity-based coalescence and curriculum heating. Accordingly, we analyze the influence of \(\epsilon\) in cold-start scenario on real-world datasets, as depicted in Figure 6. As shown in the figure, CoHeat shows low performance for the extreme low temperature because the representations of user-item view are not sufficiently learned. For the extreme high temperature, the performance degrades because the speed of the curriculum is too fast to fully learn the representation of the two views. As a result, we set \(\epsilon\) to \(10^{4}\) for all datasets since it shows the best performance.

## 5. Related Works

**Bundle recommendation.** Our work focuses on the cold-start problem in bundle recommendation. Previous works can be categorized based on their modeling structures: matrix factorization-based models (Chen et al., 2018; Chen et al., 2019; Chen et al., 2020) and graph learning-based models (Chen et al., 2018; Chen et al., 2019; Chen et al., 2020; Chen et al., 2020). Such methods operate under the assumption that all bundles have historical interactions, which makes them ill-suited for tackling the cold-start problem. However, in real-world scenarios, new bundles are introduced daily, leading to an inherent cold-start challenge. Our work addresses this significant yet overlooked issue, recognizing its potential impact on the field.

**Cold-start recommendation.** The cold-start problem, a long-standing challenge in recommender systems, focuses on recommending cold-start items with which users have not yet interacted. Existing works are primarily categorized into content-based methods (Zhou et al., 2019; Chen et al., 2020), generative methods (Chen et al., 2018; Chen et al., 2020; Chen et al., 2020; Chen et al., 2020), dropout-based methods (Chen et al., 2018; Chen et al., 2020; Chen et al., 2020), meta-learning methods (Chen et al., 2020; Chen et al., 2020), and constraint-based methods (Chen et al., 2020; Chen et al., 2020; Chen et al., 2020). However, these methods are designed for item recommendation where contents are often represented as independent entities such as bag-of-words vectors, texts, or images. Moreover, such prior works have not explicitly addressed the highly skewed distribution of interactions, a critical aspect in bundle recommendation. Thus, our work excels over these methods in cold-start bundle recommendation by effectively harnessing intricate bundle affiliations from the user-item view and addressing the skewed distribution during training.

## 6. Conclusion

We propose CoHeat, an accurate method for cold-start bundle recommendation. CoHeat strategically leverages user-bundle and user-item views to handle the extremely skewed distribution of bundle interactions. By emphasizing the user-item view for less popular bundles, CoHeat effectively captures richer information than the often sparse user-bundle view. The incorporation of curriculum learning further enhances the learning process, starting with the simpler user-bundle view embeddings and gradually transitioning to the more intricate user-item view embeddings. In addition, the contrastive learning of CoHeat bolsters the learning of representations from the two views facilitating effective knowledge transfer from the richer to the sparser view. Extensive experiments show that CoHeat provides the state-of-the-art performance in cold-start bundle recommendation, achieving up to 193% higher nDCG@20 compared to the best competitor.

\begin{table}
\begin{tabular}{l|c c|c c|c c} \hline \hline  & \multicolumn{2}{c|}{**Youshu**} & \multicolumn{2}{c|}{**NetEase**} & \multicolumn{2}{c}{**iFashion**} \\
**Model** & Recall & nDCG & Recall & nDCG & Recall & nDCG \\  & @20 & @20 & @20 & @20 & @20 & @20 \\ \hline CoHeat-_PC_ &.000 &.000 &.000 &.000 &.000 &.000 &.000 \\ CoHeat-_CH-Ant_ &.017 &.0087 &.0176 &.0087 &.0164 &.0093 \\ CoHeat-_CH-Fix_ &.0180 &.0092 &.0182 &.0090 &.0164 &.0092 \\ CoHeat-_AU_ &.0069 &.0031 &.0029 &.0013 &.0013 &.0005 \\ \hline
**CoHeat (ours)** & **.0183** & **.0105** & **.0191** & **.0093** & **.0170** & **.0096** \\ \hline \hline \end{tabular}
\end{table}
Table 4. Ablation study of CoHeat in cold-start scenario which is our main target.

Figure 6. Effect of the maximum temperature \(\epsilon\).

## References

* (1)
* Barkan et al. (2019) Oren Barkan, Noam Koenigstein, Eyjon Yoger, and Ori Katz. 2019. CB2CF: a neural multilayer content-to-chimative filtering model for completely cold latent recommendations. In _IEEE/RSJ_.
* Bengio et al. (2009) Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In _ICML_.
* Cai et al. (2023) Xuheng Cai, Chao Huang, Lianghao Xia, and Xalun Ren. 2023. LightGCL: Simple Yet Effective Graph Contrastive Learning for Recommendation. In _ICLR_.
* Cao et al. (2017) Da Cao, Ligang Xie, Xingwen Li, Xiaochui Wei, Shumin Zhu, and Tat-Seng Chua. 2017. Embedding Factorization Models for Jointly Recommending Items and User Generated Links. In _SIGIR_.
* Chen et al. (2019) Dong-Kyu Chen, Jin-Soo Kang, Sang-Wook Kim, and Jaeho Choi. 2019. Rating Augmentation with Generative Adversarial Networks towards Accurate Collaborative Filtering. In _WWW_.
* Cheng et al. (2020) Jianxin Cheng, Chao Gao, Xiangnan He, Deyeng Jin, and Yong Li. 2020. Bundle Recommendation with Graph Convolutional Networks. In _SIGIR_.
* Chang et al. (2023) Jianxin Chang, Chen Gao, Xiangnan He, Deyeng Jin, and Yong Li. 2023. Bundle Recommendation and Generation With Graph Neural Networks. _IEEE Trans. Knowl. Data Eng._ (2023).
* Cheng et al. (2020) Hong Chen, Yudong Chen, Xin Wang, Ruobing Xie, Hui Wang, Peng Xia, and Wenwu Zhu. 2020. Curriculum Disentangled Recommendation with Noisy Multi-feedback. In _NeurIPS_.
* Chen et al. (2019) Hao Chen, Zefan Wang, Feiran Huang, Xiao Huang, Yue Xu, Yushu Lin, Peng He, and Zhoujun Li. 2019. Generative Adversarial Framework for Cold-Start Item Recommendation. In _SIGIR_.
* Chen et al. (2019) Ling Chen, Yang Liu, Xiangnan He, Lianli Gao, and Zibin Zheng. 2019. Matching User with Item Set: Collaborative Rundle Recommendation with Deep Attention Network. In _IJCAI_.
* Chen et al. (2020) Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020. A Simple Framework for Contrastive Learning of Visual Representations. In _ICML_.
* Chen et al. (2019) Wen Chen, Piqi Huang, Jianxin Xu, Guo Chen, Guo Fei Sun, Chao Li, Andrea Fulder, Ranu Zhao, and Bingqing Zhao. 2019. Pco: Personalized Outfit Generation for Fashion Recommendation at Aliqa Fashion. In _KDD_.
* Cheng et al. (2021) Yudong Chen, Yan Wang, Menghao Zhu, Zhen Zhang, Shengyu Tang, and Wenwu Zhu. 2021. Curriculum Meta-Learning for Next POI Recommendation. In _KDD_.
* Deng et al. (2020) Qinli Deng, Kai Wang, Minghao Zhao, Zheme Qian, Ruan Wu, Jianqiong Tao, Chunjing Fan, and Liang Chen. 2020. Personalized Recommendation in Online Games. In _CIKM_.
* Du et al. (2020) Xiaowu Du, Xiang Wang, Xiangnan He, Zechao Li, Jinhui Tang, and Tat-Seng Chen. 2020. How to Learn Item Representation for Cold-Start Multimedia Recommendation?. In _MM_.
* Yang et al. (2021) Tianyu Guo, Xingxheng Ma, and Danqi Chen. 2021. SimCSE: Simple Contrastive Learning of Sentence Embeddings. In _EMNLP_.
* Gong et al. (2015) Xue Gong, Tianwang Zhang, Jingwen Bian, and Tat-Seng Chua. 2015. Learning Image and User Features for Recommendation in Social Networks. In _ICCV_.
* Gong et al. (2021) Yunta Gong, Cao Liu, Jairhen Yuan, Fang Jiang, Yuanliang Chen, Guanghu Wan, Jianqiong Chen, Raiyao Niu, and Houang Wang. 2021. Density-based Dynamic Curriculum Learning for Intent Detection. In _CIKM_.
* He and Chen (2017) Xiangnan He and Tat-Seng Chen. 2017. Neural Factorization Machines for Sparse Predictive Analytics. In _SIGIR_.
* He et al. (2020) Xiangnan He, Yuan Deng, Xiang Wang, Yan Li, Yong-Dong Zhang, and Meng Wang. 2020. LightGCN: Simplifying and Porcinegr Graph Convolution Network for Recommendation. In _SIGIR_.
* He et al. (2020) Zhengfu He, Chaoqiu Gu, Kai Xu, and Kaijie Wu. 2020. Automatic Curriculum Generation by Hierarchical Reinforcement Learning. In _ICONIP_.
* Ma et al. (2022) Yunshan Ma, Yingjie Liu, An Zhang, Xiang Wang, and Tat-Seng Chua. 2022. GCNCB: Cross-view Contrastive Learning for Rundle Recommendation. In _KDD_.
* Ma and Biess (2022) Buisymin Maucha and Armin Biess. 2022. Curriculum learning with Hindsight Experience Repley for sequential object manipulation tasks. _Neural Networks_ (2022).
* Pan et al. (2019) Feiyang Pan, Shuokai Li, Xiang Ao, Pingzhong Tang, and Qing He. 2019. Warm Up Cold-start Advertisements: Improving CTR Predictions via Learning to Learn HR Embeddings. In _SIGIR_.
* Pathak et al. (2017) Apurua Pathak, Kathieie Gupta, and Julian J. McAuley. 2017. Generating and Personalizing Bundle Recommendations on Stream. In _SIGIR_.
* Rendle et al. (2009) Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR: Bayesian Personalized Ranking from Implicit Feedback. In _UAI_.
* Robinson et al. (2018) Joshua David Robinson, Ching-Yao Chuang, Surri Sen, and Stefanie Jegelka. 2018. Contrastive Learning with Hard Negative Samples. In _ICLR_.
* Ischi et al. (2002) Andrew I. Ischi, Alexandria Popeschi, Jie H. Ungar, and David M. Pennock. 2002. Methods and metrics for cold-start recommendations. In _SIGIR_.
* Shi et al. (2018) Shaoyu Shi, Min Zhang, Xintong Yu, Tongfeng Zhang, Bin Luo, Yiqun Liu, and Shuqing Ma. 2018. Adaptive Feature Sampling for Recommendation with Missing Content Feature Values. In _CIKM_.
* Sun et al. (2020) Changfeng Sun, Han Liu, Meng Liu, Zhaoxun Ren, Tian Gan, and Liqiang Nie. 2020. LARA: Attribute-to-feature Adversarial Learning for New-item Recommendations. In _WSD_.
* van den Oord et al. (2013) Aaron van den Oord, Sander Dieleman, and Benjamin Schrauwen. 2013. Deep content-based music recommendation. In _NIPS_.
* van den Oord et al. (2018) Aaron van den Oord, Yabe Li, and Oriol Vinyals. 2018. Representation Learning with Contrastive Predictive Coding. _CoRR_ (2018).
* Varakk et al. (2017) Manar Varakk, Arvind Thiagarajan, Corndad Miranda, Joshua Bretman, and Hong Larochelle. 2017. A Meta-Learning Perspective on Cold-Start Recommendations for Items. In _NIPS_.
* Volkovs et al. (2017) Maksims Volkovs, Guang Wei Yu, and Tomi Fontanen. 2017. DropoutNet: Addressing Cold Start in Recommender Systems. In _NIPS_.
* Wang et al. (2022) Chenyang Wang, Yuanqing Fu, Weihui Jia, Min Zhang, Chong Chen, Yiqun Liu, and Shaying Ma. 2022. Towards Representation Alignment and Uniformity in Collaborative Filtering. In _KDD_.
* Wang et al. (2015) Hao Wang, Nalayan Wang, and Det-Yan Yeung. 2015. Collaborative Deep Learning for Recommender Systems. In _KDD_.
* Wang and Joo (2020) Tongzhou Wang and Phillip Isola. 2020. Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere. In _ICML_.
* Wang et al. (2002) Xin Wang, Yudong Chen, and Wenwu Zhu. 2022. A Survey on Curriculum Learning. _IEEE Trans. Pattern Anal. Mach. Intell._ (2022).
* Wei et al. (2021) Yiwei Wei, Xiang Wang, Qi Li, Liqiang Nie, Yan Li, Xuamping Li, and Tat-Seng Chua. 2021. Contrastive Learning for Cold-Start Recommendation. In _MM_.
* Wang et al. (2021) Jiancan Wu, Xiang Wang, Fuil Peng, Xiangnan He, Liang Chen, Jianyuan Lian, and Xing Xie. 2021. Self-supervised Graph Learning for Recommendation. In _SIGIR_.
* Xia et al. (2022) Jun Xia, Liang Wu, Jinta Chen, Boehen Hu, and Stan Z. Li. 2022. SimGRAC: A Simple Framework for Graph Contrastive Learning without Data Augmentation. In _WWW_.
* Xu et al. (2021) Yuanzun Xu, Ramel Li, Siri Wang, Fuhue Zhang, Wei Wu, and Weiran Xu. 2021. CoNEXT: A Contrastive Framework for Self-Agent Services Representation Transfer. In _ACL_.
* Jiang et al. (1989) Junliang Jiang, Hongxin Li, Xin Xia, Tong Chen, Linchen Cui, and Qucvcu Fung. 1989. Nguyen. 2022. Age Graph Momentum Accessary: Simple Graph Contrastive Learning for Recommendation. In _SIGIR_.
* Xu et al. (2020) Quig Yu, Duki Bozhi, Giei, and Nyukohara Aizawa. 2020. Multi-task Curriculum Framework for Open-Set Semi-supervised Learning. In _ECCV_.
* Zhao et al. (2021) Runke Zhuo, Xuebo Liu, Derek F. Wong, and Idzik S. Chao. 2021. Meta-Curriculum Learning for Domain Adaptation in Neural Machine Translation. In _AAAI_.
* Zhang et al. (2021) Bowen Zhang, Yidong Wang, Wenxin Hou, Hao Wu, Jiadong Wang, Manuba Okumura, and Takahiro Shinoaki. 2021. FlexMatch: Boosting Semi-Supervised Learning with Curriculum Pseudo Labeling. In _NeurIPS_.
* Zhu et al. (2016) Yuleheng Zhuo, Nicholas Jing, Yeun He, Lian Xing Xie, and Wei-Ying Ma. 2016. Collaborative Knowledge Base Embeddings for Recommender Systems. In _KDD_.
* Zhao et al. (2022) Xu Zhao, Yiq Ren, Ying Du, Suehende Zhang, and Nun Wang. 2022. Improving Item Cold-start Recommendation via Model-agnostic Conditional Variational Autoencoder. In _SIGIR_.
* Zhou et al. (2018) Chang Zhou, Jianwei Ma, Jianwei Zhang, Jingreen Zhou, and Hongxia Yang. 2021. Contrastive Learning for Debiased Candidate Generation in Large-Scale Recommender Systems. In _KDD_.
* Zhou et al. (2018) Zidhui Zhou, Lilin Zhang, and Ning Yang. 2021. Contrastive Collaborative Filtering for Cold-Start Item Recommendation. In _The Web Conference_.
* Zhu et al. (2020) Ziwei Zhu, Shahi Safi, Parsa Saadatpamah, and James Caverlee. 2020. Recommendation for New Users and New Items via Randomized Training and Mixture-of-Experts Transformation. In _SIGIR_.