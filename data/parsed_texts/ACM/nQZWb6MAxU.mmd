# Fast and Accurate Fair \(k\)-Center Clustering in Doubling Metrics

Anonymous Author(s)

###### Abstract.

We study the classic \(k\)-center clustering problem under the additional constraint that each cluster should be _fair_. In this setting, each point is marked with one or more _colors_, which can be used to model protected attributes (e.g., gender or ethnicity). A cluster is deemed _fair_ if, for every color, the fraction of its points marked with that color is within some prespecified range. We present a coreset-based approach to fair \(k\)-center clustering for general metric spaces which attains almost the best approximation quality of the current state of the art solutions, while featuring running times which can be orders of magnitude faster for large datasets of low doubling dimension. We devise sequential, streaming and MapReduce implementations of our approach and conduct a thourough experimental analysis to provide evidence of their practicality, scalability, and effectiveness.

**ACM Reference Format:**

Anonymous Author(s). 2023. Fast and Accurate Fair \(k\)-Center Clustering in Doubling Metrics. In _Proceedings of ACM Conference (Conference'17)_. ACM, New York, NY, USA, 11 pages. [https://doi.org/10.1145/nnnmnm.nnnmnm](https://doi.org/10.1145/nnnmnm.nnnmnm)

## 1. Introduction

Clustering, in its many variants, is a fundamental primitive in unsupervised learning and data analysis, aiming at grouping points according to some notion of similarity. In the most common setting, the input to clustering is a set of points \(S\) from a metric space \((M,d)\), where \(d:M\times M\rightarrow\mathbb{R}_{0}^{+}\) is a distance function, modeling dissimilarity (Kang and Yang, 2017). A popular variant is \(k\)-clustering, which requires to select a set of \(k\) centers and to build an assignment of each input point to one of the \(k\) centers while minimizing some cost, which is a function of the distances between points and centers. Different cost functions define different clustering objectives to be minimized. This paper focuses on the popular _\(k\)-center clustering_ problem (\(k\)-center problem, for short), which aims at minimizing the maximum distance between a point and its assigned center.

A very natural assignment strategy for \(k\)-center associates each point with its closest center (Kang and Yang, 2017). Imagine, however, that each point is a representation of some features of individuals, and that clustering implies decisions that may impact individual livelihoods. In this scenario, the decisions being made, i.e., the clustering, should not have a _disproportionate_ effect on the people involved. For instance, people from a particular protected group cannot be segregated in a single cluster. This intuition is captured by the notion of _disparate impact_(Kang and Yang, 2017): people in different protected classes should not experience disproportionately different outcomes. Blindly ignoring protected attributes, however, is no solution (Kang and Yang, 2017): correlated features (e.g., height which correlates with biological sex) can leak information about the protected attributes and may influence the clustering, leading to _unfair_ solutions. This suggests that to achieve fairness in the clustering we need to explicitly take into account protected attributes when assigning points to centers.

The study of fair \(k\)-clustering under the disparate impact notion has been initiated by Chierichetti et al. (Chierichetti et al., 2017) and generalized in subsequent works (Chierichetti et al., 2017; Chen et al., 2017; Li et al., 2017). Each point is assigned one or more colors to model the protected attributes, and the clustering has to be built so that in each cluster the fraction of points of each color is within a color-specific range. For instance, if the input set has half blue points and half red points, each cluster could be required to have roughly half blue points and half red points. State of the art approaches to fair clustering with multiple colors are based on Linear Programming, which limits their scalability to large datasets. Coresets are an effective way of dealing with scalability issues for big data analytics (Li et al., 2017). A coreset is a compact representation of a large instance on which computationally demanding (e.g., LP-based) algorithms can be run to efficiently obtain good solutions for the whole instance. For fair clustering in the big data setting, coresets have been recently used in (Chen et al., 2017) to reduce the size of the linear programs, yielding a 2-pass streaming algorithm and a 2-round MapReduce/MPC algorithm, attaining, respectively, \((7+\varepsilon)\) and 9 approximations.

### Our contribution

In this paper, we present an improved coreset-based strategy for fair \(k\)-center clustering of multi-colored points in general metrics, whose accuracy/performance tradeoffs are analyzed in terms of the doubling dimension of the data set. We devise implementations of our strategy in the sequential, streaming and MapReduce/MPC frameworks, yielding the following contributions, where \(S\), \(\Gamma\), and \(D\) represent, respectively, the input dataset, the set of colors, and the doubling dimension of \(S\).

* A sequential algorithm for fair \(k\)-center which attains a \((3+\varepsilon)\) approximation, and whose running time is linear in (Chen et al., 2017) for constant \(k\), \(|\Gamma|\), \(\varepsilon\), and \(D\). (See Theorem 4.5 for the general statement.)
* A \(2\)-pass streaming algorithm for fair \(k\)-center which attains a \((3+\varepsilon)\) approximation and requires working memory which, for constant \(k\), \(|\Gamma|\), \(\varepsilon\), and \(D\), is \(O\left(\log(d_{max}/d_{min})\right)\), where \(d_{min}\) and \(d_{max}\) are, respectively, the minimum and maximum pairwise distance in \(S\). (See Theorem 5.1 for the general statement.)
* A \(5\)-round MapReduce/MPC algorithm for fair \(k\)-center which attains a \((3+\varepsilon)\) approximation and requires a local memory which, for constant \(k\), \(|\Gamma|\), \(\varepsilon\), and \(D\), is \(O\left(\max\{|S|/p,p\}\right)\), when \(p\) processors are used. (See Theorem 6.1 for the general statement.)As in (Bera et al., 2017; Bera et al., 2017), all of the above algorithms return solutions where the color distribution in each cluster complies with the fairness constraints within a modest additive violation of \(4\Delta+3\), where \(\Delta\leq|\Gamma|\) is the maximum number of colors per point.

We implemented and ran our algorithms on real-world datasets, scaling up to 16 million points, to assess the effectiveness and scalability of our coreset-based strategy. The experiments show that our algorithms return solutions whose quality is comparable to the best attained by state-of-the-art algorithms but exhibit significantly better performance.

The main novelty of our approach is that it adapts (obliviously) to the dimensionality of the input dataset, becoming extremely accurate (abating considerably the approximation ratios of (Bera et al., 2017)), and time and space efficient for low-dimensional datasets, in all computational settings. Also, our experiments provide evidence of its practicality.

**Structure of the paper.** The rest of the paper is organized as follows. Section 2 summarizes the relevant related work. Section 3 formally defines the problem and states some basic technical facts. Sections 4, 5, and 6 describe and analyze, respectively, our sequential, streaming and MapReduce algorithms. Finally, our experimental results are reported in Section 7. For space limitations, some technical details are reported in an appendix.

## 2. Related Work

For space limitations, in this section we limit our literature review to the fair \(k\)-center clustering problem, in which a fair assignment to \(k\) cluster centers has to be built while minimizing the maximum distance of a point from its assigned center. For a survey of other fair clustering objective functions and notions, we refer the interested reader to the recent tutorial1 offered at _AAAI 2022_.

Footnote 1: [https://www.fairclustering.com/](https://www.fairclustering.com/)

In the pioneering work by Chierichetti et al. (Chierichetti et al., 2013), each point of the input is colored either red or blue, and a feasible solution is an assignment that preserves the balance of colors in each cluster, i.e., the ratio of blue to red points in each cluster must be the same as the ratio in the input dataset. The authors provide a combinatorial algorithm yielding a 3 approximation for the \(k\)-center objective.

The main limitation of their approach, however, is that it is limited to the case of a single binary protected attribute. An extension to the case where the protected attribute can face one out of many colors has been devised by Rosner and Schmidt (Rosner and Schmidt, 2013), who provide a 14-approximation algorithm.

The notion of balance has been generalized by Bercea et al. in (Bercea et al., 2017). In their work, the ratio of each color in each cluster is allowed to take values within user-specified color-specific ranges. The paper proposes approaches based on Linear Programming, obtaining a 5 approximation for fair \(k\)-center with exact preservation of the ratios, and a bicriteria 3 approximation that incurs a small violation of the fairness constraints. The main drawback of their approach is that it generates linear programs with a number of variables _quadratic_ in the size of the input set.

Bera et al. (Bera et al., 2017) provide a further extension to the fairness notion, allowing each point to have multiple colors (thus supporting the notion of multiple protected attributes). As in (Bercea et al., 2017), the balance of each individual color in a cluster is then required to be within pre-specified color-specific ranges. The paper proposes a two-step approach valid for all \(k\)-clustering problems with a \(\mathcal{L}_{p}\) norm objective (thus including \(k\)-center, \(k\)-median, and \(k\)-means, among the others), where the centers are first identified using an _unfair_ approximation algorithm for the unconstrained \(k\)-clustering objective and then the assignment of points to centers is obtained using an LP-based technique. For \(k\)-center, their approach leads to a 3 approximation2, with an additive \(4\Delta+3\) violation of the fairness constraints, where \(\Delta\) is the maximum number of colors of a point. Importantly, this approach reduces the number of variables in the LP program to \(O(k\cdot n)\), where \(n\) is the input size.

Footnote 2: In fact, in (Bercea et al., 2017) a approximation is claimed, but a careful reading of the proof reveals that, for the \(k\)-center objective, the approximation factor can be brought down to 3.

Coreset-based streaming and MapReduce/MPC instantiations of the aforementioned strategy are presented in (Bera et al., 2017). In both cases, the approach still relies on first determining a good set of unfair centers, together with the determination of a weighted summary of the input set upon which a variant of the LP introduced in (Bera et al., 2017) is solved to identify a suitable assignment of points to centers. The resulting algorithms achieve a 2-pass \(7+\varepsilon\) approximation in the Streaming setting, and a 2-round 9 approximation for the MPC.

Ahmadian et al. (Ahmadian et al., 2017) study a different \(k\)-center variant, where there is an upper bound \(\alpha\) to the ratio of each color in each cluster, but there are no lower constraints on the ratios. They devise a 3-approximate LP-based solution to the problem, and they also provide a combinatorial 12-approximation algorithm for the special case of \(\alpha=0.5\).

The goal of reducing the size of the LP used to build the fair assignment of points is further pursued by Harb and Lam (Harb and Lam, 2017), that are thus able to achieve the same approximation factors as in (Bera et al., 2017) while being considerably faster in practice.

## 3. Preliminaries

This section formally defines the problems studied in this paper, and states some important technical facts. Consider a metric space \((M,d)\). We will analyze the performance of our algorithms in terms of the dimensionality of the input set \(S\subseteq M\) which, for general metric spaces, can be captured by the notion of _doubling dimension_, reviewed below.

For any \(p\in S\) and \(r>0\), let the _ball of radius \(r\) centered at \(p\)_, denoted as \(B(p,r)\subseteq S\), be the subset of all points of \(S\) at distance at most \(r\) from \(p\). Then, the _doubling dimension_ of \(S\) is the minimum value \(D\) such that, for all \(p\in S\), any ball \(B(p,r)\) is contained in the union of at most \(2^{D}\) balls of radius \(r/2\) centered at points of \(S\). The notion of doubling dimension has been used extensively for a variety of applications (e.g., see (Bera et al., 2017; Bera et al., 2017; Bera et al., 2017; Bera et al., 2017) and references therein).

Given an input set \(S\), we assume that each point \(x\in S\) is colored with a _color combination_ of at most \(\Delta\) colors out of a set of colors \(\Gamma\).3 With \(S_{\ell}\subseteq S\) we denote the set of points whose color combination contains \(\ell\in\Gamma\). For \(x\in S\) we use \(col(x)\subseteq\Gamma\) to denote its color combination, and define \(C_{S}\subseteq\Omega^{\Gamma}\) to be the family of all color combinations associated with at least one point in \(S\).

Footnote 3: This allows to model the setting of multiple sensitive attributes.

A \(k\)-clustering of a set \(S\) is a pair \((C,\phi)\) where \(C\subseteq S\) is the set of _centers_, and \(\phi:S\to C\) is the _assignment function_ that maps each point of \(S\) to a center. The \(k\)-center cost, also called _radius_, of a \(k\)-clustering \((C,\phi)\) is the largest distance between a point and its assigned center:

\[r_{C,\phi}(S)=\max_{x\in S}d(x,\phi(x))\]

Given a set of centers, the standard, color-oblivious way of building a clustering is by assigning each point to its closest center (with ties broken arbitrarily). Let this assignment function be denoted by \(\phi_{unf}(\cdot)\), where \(unf\) stands for _unfair_, and let \(OPT_{unf}(S,k)\) be the minimum radius of any \(k\)-clustering of \(S\) under \(\phi_{unf}(\cdot)\). (We will omit \(S\) and \(k\) when clear from context.)

For the unfair \(k\)-center problem, the classic \(O(kn)\)-time algorithm by Gonzalez (2001) provides a 2 approximation. The algorithm, which we refer to as GMM (Greedy Minimum Maximum) implements the following simple greedy strategy. The set of centers is initialized with an arbitrary point. Then, the next center is selected to be a point at maximum distance from all previously selected centers. The procedure is repeated until there are \(k\) centers.

Our analysis will make use of the following result, which was proved in (Bauer et al., 2011, Lemma 1).

Lemma 3.1 ().: _Let \(X\subseteq S\). For a given \(k\), let \(T_{X}\) be set of \(k\) centers computed by GMM on \(X\). We have_

\[r_{T_{X},\phi_{unf}}(X)\leq 2\cdot OPT_{unf}(S,k)\]

Clearly, the aforementioned standard assignment function might lead to unfair results, in the sense that different clusters might exhibit different proportions of points with the same color. This motivates the following additional constraint. A clustering \((C,\phi)\) for \(S\) is called _fair_ if, for each \(\ell\in\Gamma\), for given parameters \(\beta_{\ell}\leq\alpha_{\ell}:\)

\[\beta_{\ell}\leq\frac{|\{x\in S:\phi(x)=c_{i}\}|}{|\{x\in S:\phi(x)=c_{i}\}|} \leq\alpha_{\ell}\quad\forall c_{i}\in C.\]

In other words, fairness requires that the fraction of points whose color combination includes color \(\ell\) is between parameters \(\beta_{\ell}\) and \(\alpha_{\ell}\) in every cluster. Our algorithms will enforce this notion of fairness within some (small) tolerance. More precisely, as in (Bauer et al., 2011) we say that a clustering \((C,\phi)\) for \(S\) is _fair with additive violation \(\lambda\)_ if for every \(c_{i}\in C\) and \(\ell\in\Gamma\),

\[\beta_{\ell}\cdot|\{x\in S:\phi(x)=c_{i}\}|-\lambda\leq|\{x\in S:\phi(x)=c_{i}\}|\]

and

\[|\{x\in S_{\ell}:\phi(x)=c_{i}\}|\leq\alpha_{\ell}\cdot|\{x\in S:\phi(x)=c_{i} \}|+\lambda.\]

Note that the fairness conditions are stated for each color _independently_. This means that a point with multiple colors will be involved in multiple fairness constraints. An alternative approach would be that of considering every color combination in \(C_{S}\) as a new, different color and enforcing a fairness constraint for each of these new colors. Clearly, this simpler approach can be modeled as the case of a single color per point. It is important to observe that no fair clustering may exist for a given multicolored pointsset \(S\) under a certain set of fairness constraints.

An optimal fair clustering is a fair clustering which minimizes the radius, denoted as \(OPT_{fair}(S,k)\). (In case no fair clustering exists, we set \(OPT_{fair}(S,k)=+\infty\).) The following basic fact trivially holds.

Fact 1.: _For a given set \(S\) and any fairness constraint, we have_

\[OPT_{unf}(S,k)\leq OPT_{fair}(S,k)\]

### Big-data models of computation

In the MapReduce model (Kipf and Welling, 2017), an algorithm executes in a sequence of _parallel_ rounds. In each round a multiset \(X\) of _key-value_ pairs is transformed in a new multiset \(Y\) by means of a _mapper_ function, followed by the application of a _reducer_ function to obtain a final multiset \(Z\). Crucially, the local memory available to each _mapper_ and _reducer_ is limited by a parameter \(M_{L}\), whereas the _aggregate_ memory across all mappers and reducers is limited by parameter \(M_{A}\). An algorithm in this model strives to minimize the number of rounds while complying with the memory limits.

We emphasize that our MapReduce algorithms admit a straightforward porting to the MPC model (Bauer et al., 2011), maintaining the same round and space complexity. Hence, all the results in this paper stated for MapReduce hold identically for the MPC model.

## 4. Sequential algorithm

This section describes our sequential coreset-based algorithm for fair k-center clustering. The input consists of a set of colored points \(S\), the number of clusters \(k\), the fairness constraints, represented by the two vectors \(\alpha=\{\alpha_{\ell}:\ell\in\Gamma\}\) and \(\beta=\{\beta_{\ell}:\ell\in\Gamma\}\), and an accuracy parameter \(\varepsilon\in(0,1)\). The algorithm executes three main steps. In the first step, a small coreset \(T\) of colored and weighted points is computed from \(S\), so that each \(x\in S\) has a _proxy_\(\pi(x)\in T\) with the same color combination, and each \(t\in T\) carries a weight denoting the number of original points for which it acts as a proxy. In the second step, a solution \(C\subseteq T\) consisting of \(k\) centers is computed running GMM on \(T\), and a skeleton of the final clustering is computed by suitably distributing the weights of the coreset points among the centers. Finally, in the third step, the skeleton is turned into the final clustering. The pseudocode for this high-level structure of the algorithm is depicted in Algorithm 1. The three steps and the procedures that they use are described in detail in the following subsections.

```
Input: Set of points \(S\), parameters \(k\), \(\alpha\), \(\beta\) and \(\varepsilon\) Output: Set of centers \(C\), assignment function \(\phi\) /* Step 1 */ \((T,\pi)\leftarrow\textsc{CoresetConstruction}(S,k,\varepsilon)\); /* Step 2 */ \(C\leftarrow\textsc{GMM}(T,k)\); \(\hat{\phi}\leftarrow\textsc{WeightDistribution}(T,C,\alpha,\beta)\); /* Step 3 */ \(\phi\leftarrow\textsc{FinalAssignment}(\hat{\phi},S,T,C)\); return\((C,\phi)\);
```

**Algorithm 1**Sequential Fair k-Center Clustering

### Step 1: Coreset construction

We build the weighted coreset \(T\) as follows (see Algorithm 2 for the pseudocode). First we run \(k\) iterations of GMM on \(S\) to determine a set of \(k\) centers, which we denote as \(T^{k}\), and compute the radius \(r_{T^{k},\phi_{unf}}\). Then, we continue to run GMM until the first iteration \(\tau\geq k\) such that

\[r_{T^{k},\phi_{unf}}\leq(\varepsilon/6)\cdot r_{T^{k},\phi_{unf}}.\]
```
Input: Set of points \(S\), parameters \(k\) and \(\varepsilon\)
21 /* Identify coreset points */ \(T\leftarrow\{\)an arbitrary point of \(S\}\); while\(|T|<k\)do\(T\gets T\cup\{\arg\max_{x\in S}d(x,T)\}\) ; \(r_{k}\leftarrow\max_{x\in S}d(x,T)\); while\(\max_{x\in S}d(x,T)>(\varepsilon/6)r_{k}\)do\(T\gets T\cup\{\arg\max_{x\in S}d(x,T)\}\); /* Build the proxy function and weights */ for\(t\in T\)do Build \(|C_{S}|\) copies of \(t\) with distinct color combinations; Set the weight \(w(t)\) of each copy to \(0\); for\(x\in S\)do\(t^{\prime}\leftarrow\arg\min_{t\in T\cap(t)\leq\cup(x)}d(x,t)\) ; \(w(t^{\prime})\gets w(t^{\prime})+1\); \(\pi\left(x\right)\gets t^{\prime}\); return\(T,w,\pi\);
```

**Algorithm 2**CoresetConstruction

Now, for each point \(t\in T^{r}\) we create \(|C_{S}|\) copies, each colored with a distinct color combination in \(C_{S}\). The resulting set of \(|C_{S}|\cdot|T^{r}|\) copies will be our coreset \(T\). Then, we determine a proxy function \(\pi:S\to T\) which assigns to each point \(x\in S\) the closest coreet point of the same color combination, namely

\[\pi(x)=\operatorname*{arg\,min}_{t\in T\;:\;\text{co}(t)\preceq\text{co}(x)}d( x,t)\qquad\forall x\in S.\]

Also, for each coreset point \(t\in T\) we compute a weight \(w(t)\), corresponding to the number of points of \(S\) for which \(t\) is a proxy :

\[w(t)=\left|\{x\in S:\pi(x)=t\}\right|.\]

Points of \(T\) with zero weight (i.e. which are the proxy of no input point) are simply discarded. Observe that all the points proxied by the same coreset point \(t\) have the same color combination.

The following lemma upper bounds the distance between each input point from its representative in \(T\).

**Lemma 4.1**.: _Let \(T\) be the coreset constructed above for the set \(S\), and let \(\pi\) be the associated proxy function. Then, for each \(x\in S\) we have:_

\[d(x,\pi(x))\leq(\varepsilon/3)\cdot OPT_{\text{unrf}}\]

Proof.: We have that

\[r_{T^{r},\phi_{\text{unrf}}}\leq(\varepsilon/6)\cdot r_{T^{k}, \phi_{\text{unrf}}}\] \[\leq(\varepsilon/3)\cdot OPT_{\text{unrf}}\]

where the first inequality holds by construction, and the second by Lemma 3.1. 

We now bound the size of the coreset.

**Lemma 4.2**.: _If \(S\) has doubling dimension \(D\), then_

\[|T|\leq|C_{S}|\cdot k\cdot(12/\varepsilon)^{D}\]

Proof.: We first prove an upper bound on the number \(\tau\) of iterations needed by GMM to obtain a radius

\[r_{T^{r},\phi_{\text{unrf}}}\leq(\varepsilon/6)\cdot r_{T^{k},\phi_{\text{unrf }}}.\]

Consider the unfair \(k\)-center clustering induced by \(T^{k}\) using \(\phi_{\text{unrf}}\), whose radius is \(r_{T^{k},\phi_{\text{unrf}}}\). By the doubling dimension property, each of the \(k\) clusters can be covered using at most \((12/\varepsilon)^{D}\) balls of radius \(\leq(\varepsilon/12)r_{T^{k},\phi_{\text{unrf}}}\), for a total of at most \(h=k\cdot(12/\varepsilon)^{D}\) balls.

Consider now the execution of \(h\) iterations of GMM on \(S\), with \(T^{h}\) being the set of centers and \(x\in S\) being the point farthest from any center in \(T^{h}\). It is easy to see that GMM ensures that any two points in \(T^{h}\cup\{x\}\) are at distance at least \(r_{T^{h},\phi_{\text{unrf}}}\) from one another. Since two of these points must fall into one of the \(h\) balls mentioned above, by the triangle inequality we have that

\[r_{T^{h},\phi_{\text{unrf}}}\leq 2(\varepsilon/12)\cdot r_{T^{k},\phi_{\text{unrf }}}=(\varepsilon/6)\cdot r_{T^{k},\phi_{\text{unrf}}}\]

Hence, we are guaranteed that after running \(h\) iterations of GMM we find a set of points meeting the stopping condition, which implies \(\tau\leq h\). The lemma follows by noting that each point in \(T^{h}\) is replicated at most \(|C_{S}|\) times in \(T\). 

### Step 2: creating the clustering skeleton

Recall that coreset \(T\) computed in Step 1 is such that each \(t\in T\) represents \(w(t)\) points of \(S\) with the same color combination, which, by virtue of Lemma 4.1, are rather close to \(t\). In Step 2, our algorithm first computes a set \(C\) of \(k\) centers by running GMM on \(T\), and then invokes a procedure called WeightDistribution, described below, to distribute the weight of each \(t\in T\) among one or more centers of \(C\), so to minimize the maximum distance between coreset points and one of the centers receiving their weights (which we will refer to as the _radius of the distribution_) while, at the same time, enforcing the fairness constraints. This distribution will be modeled through a weight assignment function \(\hat{\phi}:T\times C\to N\) which will provide a skeleton of the final clustering and will be used in Step 3 to extract the assignment function \(\phi\).

To achieve the aforementioned weight distribution, we make use of a weighted version of the Frequency Distributor LP of Harb and Shan (1999). Let \(R\) be a guess on the radius of the distribution and consider the power set \(2^{C}\) of the set of centers \(C\). For each color combination \(L\in C_{S}\) and each subset of centers \(C^{\prime}\in 2^{C}\), define \(JC_{\cdot,L,R}\) as the set of points with color combination \(L\) that are within distance \(R\) from all and only the points of \(C^{\prime}\), namely

\[\begin{array}{rcl}J_{C^{\prime},L,R}&=&\{t\in T:\text{co}(t)=L\wedge d(t,c) \leq R\,\forall c\in C^{\prime}\wedge\\ &&\wedge d(t,\overline{c})>R\,\forall\,\overline{c}\,\overline{c}\,\overline{ c}^{\prime}\}\end{array}\]

Each \(J_{C^{\prime},L,R}\) is referred to as a _joiner_ in (1999). For a joiner \(J\), we introduce the following notation: \(C_{J}\) denotes the subset of centers defining \(J\), \(\text{co}(J)\) denotes the color combination common to all of its points and

\[w(J)=\sum_{t\in J}w(t)\]

denotes the total weight carried by the points of the joiner. Let \(\mathcal{J}(R)\) be the set of joiners obtained for the guess \(R\) and observe that they define a partition of \(T\). For every \(\ell\in\Gamma\) we also define

\[\mathcal{J}(R)_{\ell}=\{J\in\mathcal{J}(R):\ell\in\text{co}(J)\}\]

The crucial observation is that a joiner \(J\in\mathcal{J}(R)\) acts as a _super point_ in the sense that the weight of any of its points can be indifferently distributed to _any_ center in \(J_{C}\). Thus, the weight distribution can be computed at the joiner level rather than the coreset point level. To this purpose, the following linear program is defined which uses a variable \(z_{J,c}\) for every joiner \(J\) and center \(c\in C_{J}\).

**LP-WPD**(\(\mathcal{J}(R),C\))

(1) \[z_{J,c}\geq 0 J\in\mathcal{J}(R),c\in C_{J}\] \[\sum_{c\in C_{J}}z_{J,c}= w(J) \forall J\in\mathcal{J}(R)\] (2) \[\beta_{\ell}\sum_{J\in\mathcal{J}(R)}z_{J,c}\leq\sum_{J^{\prime} \in\mathcal{J}(R)_{\ell}}z_{J^{\prime},c} \forall c\in C,\ell\in\Gamma\] (3) \[\sum_{J^{\prime}\in\mathcal{J}(R)_{\ell}}z_{J^{\prime},c}\leq \alpha_{\ell}\sum_{J\in\mathcal{J}(R)}z_{J,c} \forall c\in C,\ell\in\Gamma\] (4)

Condition (2) ensures that all the weight is assigned to some center, whereas Conditions (3) and (4) encode the fairness constraints. Note that the fairness constraints (3) and (4) are defined cluster-wise, and can be specified through variables \(z_{J,c}\) which exploit the aggregation of the points defined by the joiners.

By construction, in a feasible solution to the above LP the nonzero \(z_{J,c}\)'s define an association between joiners and centers, such that for every coreset point \(t\) belonging to some joiner \(J\) and any center \(c\) with \(z_{J,c}>0\), we have \(d(t,c)\leq R\). Clearly, for small values of \(R\) no feasible solution may exist. The following lemma provides a crucial lower bound to values \(R\) which yield feasible solutions.

**Lemma 4.3**.: _Suppose that the weighted coreset \(T\) computed for \(S\) features a proxy function \(\pi\) such that \(d(x,\pi\left(x\right))\leq\epsilon/3\), for every \(x\in S\). Then, for \(R\geq(3+(2/3)\epsilon)OPT_{fair}\), the linear program above has a feasible solution._

Proof.: Consider the set of centers \(C\) selected by GMM, and the optimal fair clustering \((C^{\star},\phi^{\star})\) of cost \(OPT_{fair}\). We will show that we can distribute the weight of points in \(T\) to points in \(C\) within distance \(R\) so that the constraints of the linear program are satisfied.

For each point \(x\in S\), consider its optimal fair center \(\phi^{\star}\left(x\right)\), and let \(\text{nrst}\left(\pi\left(\phi^{\star}\left(x\right)\right)\right)\) be the center of \(C\) nearest to \(\pi\left(\phi^{\star}\left(x\right)\right)\). By Lemma 3.1, we have that \(d(\text{nrst}\left(\pi\left(\phi^{\star}\left(x\right)\right)\right),\pi\left( \phi^{\star}\left(x\right)\right)\leq 2OPT_{unf}\). Figure 1 depicts all the points involved, along with relevant bounds on their distances. Let \(J\in\mathcal{J}\) be the joiner such that \(\pi\left(x\right)\in J\). By following the chain of inequalities of Figure 1, we have that \(d(\pi\left(x\right),\text{nrst}\left(\pi\left(\phi^{\star}\left(x\right) \right)\right))\leq R\), hence \(\text{nrst}\left(\pi\left(\phi^{\star}\left(x\right)\right)\right)\in C_{J}\). We determine values for the variables of \(\text{LP-WPD}(\mathcal{J}(R),C)\) by "moving" one unit of weight from \(\pi\left(x\right)\) to the variable \(z_{J,\text{nrst}\left(\pi\left(\phi^{\star}\left(x\right)\right)\right)}\). After processing all points in \(S\), it is immediate to see that the group of constraints (2) is satisfied.

As for the fairness constraints (3) and (4), for \(c\in C^{\star}\), let \(C^{\star}(c)=\{x\in S:\phi^{\star}\left(x\right)=c\}\) be the optimal cluster centered in \(c\). Similarly, let \(C^{\star}_{\ell}(c)=\{x\in S:\phi^{\star}\left(x\right)=c\}\) for \(c\in C^{\star}\), \(\ell\in\Gamma\) be the set of points of color \(f\) assigned to the cluster centered in \(c\). Clearly, each optimal cluster \(C^{\star}(c)\) must respect the fairness constraints, i.e.

\[\beta_{\ell}\leq\frac{|C^{\star}_{\ell}(c)|}{|C^{\star}(c)|}\leq\alpha_{\ell} \tag{5}\]

for each \(c\in C^{\star}\) and \(\ell\in\Gamma\). Now, for each \(c\in C\) let \(N(c)=\left\{e^{\star}\in C^{\star}:\text{nrst}\left(\pi\left(c^{\star}\right) \right)=c\right\}\) be the set of optimal centers for which \(c\) is the closest center in \(C\) to their proxy in the coreset. By the weight assignment procedure described above, we have that any center \(c\in C\) is assigned a weight equal to the number of points in \(\cup_{\ell^{\prime}\in N(c)}C^{\star}(c^{\prime})\). Therefore we have that for any color \(\ell\in\Gamma\) and any center \(c\in C\)

\[\beta_{\ell}\leq\frac{\sum_{c^{\star}\in N(c)}|C^{\star}_{\ell}(c^{\prime})|}{ \sum_{c^{\star}\in N(c)}|C^{\star}(c^{\prime})|}\leq\alpha_{\ell}\]

by Fact 2 (in the Appendix) and Inequality (5), which proves that the set of constraints (3) and (4) are also satisfied. 

In Step 2, after computing the centers \(C\) we run Procedure WeightDistribution which performs the following operations (see Algorithm 3 for the pseudocode). First it computes and sorts the \(|T|k\) distances between the coreset points and the centers, and then performs a binary search over these distances to identify the smallest value \(R\) such that the LP-WFD yields a feasible solution \(Z_{LP-WFD}=\{z_{J,c}\ :\ \text{$J\in\mathcal{J}(R)\wedge c\in C$}\}\). Note that this solution may be fractional. In order to derive an integral weight assignment to the centers, we run Procedure CoRESTAsign. The procedure first transforms \(Z_{LP-WFD}\) into an integral solution \(Z_{LP-WFD}^{int}=\{z_{J,c}^{int}\ :\ J\in\mathcal{J}(R)\wedge c\in C\}\), by using the iterative rounding procedure presented in (Bartlett and Barthelemy, 2009), and then derives the weight assignment function \(\hat{\phi}\) by distributing the weight of the coreset points of each joiner \(J\) among the centers, as specified by the \(z_{J,c}^{int}\)'s. The rounding introduces a mere additive violation of the fairness constraints, as stated in the following lemma. For space limitations, the details of Procedure CoRESTAsign and the proof of the lemma are moved to Appendix B.

**Lemma 4.4**.: _Procedure CoRESTAsign returns a weight distribution function \(\hat{\phi}:T\times C\to\mathbb{N}\) such that for every color \(\ell\in\Gamma\) and every center \(c\in C\)_

\[\beta_{\ell}\cdot\sum_{t\in T}\hat{\phi}(t,c)-(4\Delta+3)\leq\sum_{t\in T_{\ell }}\hat{\phi}(t,c)\leq\alpha_{\ell}\cdot\sum_{t\in T}\hat{\phi}(t,c)+(4\Delta+3),\]

_where \(T_{\ell}\) is the subset of coreset points whose color combination contains \(\ell\)._

### Step 3: Final assignment

In the last step, the algorithm uses the weight assignment function \(\hat{\phi}\) computed in Step 2 to compute the final assignment function \(\phi\)

Figure 1.

between the original points of \(S\) and the centers in \(C\). Observe that, by construction, \(\hat{\phi}\) ensures that for each \(t\in T\)

\[\sum_{c\in C}\hat{\phi}(t,c)=w(t)=\left|\{x\in S:\pi(x)=t\}\right|.\]

Therefore, we can compute \(\phi\) through a sequential scan of \(S\), where for each \(x\in S\) an arbitrary center \(c\) with \(\hat{\phi}(\pi(x),c)>0\) is chosen, and \(\phi(x)\) is set equal to \(c\) while \(\hat{\phi}(\pi(x),c)\) is decreased by \(1\). The pseudocode is depicted in Algorithm 4.

```
Input: Weighted coreset \(T\), set of centers \(C\), parameters \(a\), \(\beta\) Output: Assignment \(\hat{\phi}:T\times C\rightarrow\mathbb{N}\) \(\Lambda\leftarrow\) Sorted list of distances \(d(t,c),\ \forall t\in T,c\in C\);  Do binary search on \(\Lambda\) to find the smallest \(R\) such that \(\text{LP-WFD}(\mathcal{J}(R),C)\) yields a feasible solution \(z^{*}=\{z_{Jc}:J\in\mathcal{J}(R),c\in C_{J}\}\): \(\hat{\phi}\leftarrow\textsc{CorrestAssion}(T,\mathcal{J}(R),C,z^{*})\);
``` Input: Sets \(S,T\) and \(C\), and assignment \(\hat{\phi}:T\times C\rightarrow\mathbb{N}\) Output: Assignment \(\hat{\phi}:S\to C\) for\(x\in S\)do \(t\leftarrow\pi(x)\); \(c\leftarrow\text{arbitrary}\ e\in C:\hat{\phi}(t,c)>0\); \(\hat{\phi}(x)\gets c\); \(\hat{\phi}(t,c)\leftarrow\hat{\phi}(t,c)-1\); return\(\phi\); ```

**Algorithm 4**FinalAssignment

### Putting all pieces together

The following theorem concludes the analysis.

**Theorem 4.5**.: _For an input set \(S\) of doubling dimension \(D\), the above sequential algorithm returns a \((3+\epsilon)\)-approximation to the optimum fair k-center clustering, with an additive violation \(\leq 4\Delta+3\) of the fairness constraints. For fixed values of \(R\), \(D\), and \(|\Gamma|\), the algorithm requires linear time in the input set size._

Proof.: The compliance with the fairness constraints is an immediate consequence of Lemma 4.4 and the derivation of \(\phi\) from \(\hat{\phi}\). As for the radius, the above assignment procedure, combined with the result of Lemma 4.3, ensures that for every \(x\in S\)

\[d\left(x,\phi(x)\right) \leq d\left(x,\pi\left(x\right)\right)+\max_{e\in C,z^{*}\in C,z^{* }\in C,z^{*}>0}d\left(\pi\left(x\right),c\right)\] \[\leq(\epsilon/3)OPT_{\textit{fair}}+(3+(2\epsilon)/3)\ OPT_{\textit {fair}}\] \[\leq(3+\epsilon)OPT_{\textit{fair}}.\]

The running time of the algorithm is dominated by the run of GMM to identify \(T\), the cost of solving \(O\left(\log(|T|k)\right)\) instances of \(\textbf{LP-WFD}(\mathcal{J}(R),C)\), and the cost of computing the final assignment. By Lemma 4.2 we have that \(|T|\leq|C_{S}|\cdot k\cdot(12/\epsilon)^{D}\), and by adapting the analysis in (Hardt et al., 2016), we have that each \(\textbf{LP-WFD}(\mathcal{J}(R),C)\) entails \(O\left(k\cdot\min\{2^{k}|C_{S}|,|T|\}\right)\) variables and \(O\left(k\left(|T|+\min\{2^{k}|C_{S}|,|T|\}\right)\right)\) constraints. Thus, since \(|C_{S}|\leq 2^{|\Gamma|}\), for fixed values of \(k\), \(D\), and \(|\Gamma|\), the algorithm exhibits only linear dependence in \(|S|\). 

It is important to remark that our algorithm attains an approximation factor that can be made arbitrarily close to the one of (Bauer, 2016) but, for wide ranges of the involved parameters, reduces dramatically the size of the linear programs required to compute the solution, which dominate by far the computation costs.

## 5. Streaming Algorithm

In this section, we describe a 2-pass streaming implementation of the sequential algorithm (Algorithm 1). We now regard the input \(S\) as a stream of points. The first pass constructs the weighted coreset \(T\) and, at the end of the pass, Step 2 of Algorithm 1, whose space requirements are independent of the stream size, is performed as is, returning the weight distribution function \(\hat{\phi}\). Then, in the second pass the final assignment \(\phi\) is computed.

The coreset construction requires the knowledge of the smallest and largest pairwise distances in the stream (or suitable approximations), denoted respectively as \(d_{\textit{min}}\) and \(d_{\textit{max}}\).4. The first pass runs in parallel several _instances_ for geometric guesses \(R\) of the optimal radius of \(OPT_{\textit{fair}}(S,k)\), namely \(R=2^{j}d_{\textit{min}}\), with \(0\leq j\leq\lceil\log_{2}(d_{\textit{max}}/d_{\textit{min}})\rceil\). Let \(S_{i}\) be the set of the first \(i\) points of \(S\). For \(i\geq 1\), each instance maintains two sets of points:

Footnote 4: A similar assumption was needed in the streaming algorithm by (Bauer, 2016). The assumption can be removed by introducing an extra pass (details will be provided in the full version).

* A set \(C_{R}\) of up to \(k+1\) points with \(d(x,C_{R})\leq 2R,\forall x\in S_{i}\),
* A set \(T_{R}\) of weighted points with \(d(x,T_{R})\leq\frac{1}{2}R,\forall x\in S_{i}\).
* \(C_{R}\) is used to detect when the guess \(R\) is too small, while \(T_{R}\) is the candidate coreset. For each point \(x\) in the stream, if \(d(x,C_{R})>2R\), then \(x\) is added to \(C_{R}\). In case the size of \(C_{R}\) exceeds \(k\), this instance fails because the guess \(R\) is too small. Otherwise, \(x\) is processed as follows. If \(d(x,T_{R})>\frac{1}{2}R\), then we add \(x\) to \(T_{R}\) with weight \(1\), and also add \(|C_{S}|-1\) copies of \(x\) to \(T_{R}\), with the other color combinations from \(C_{S}\) and weights \(0\). If instead \(d(x,T)\leq\frac{1}{2}R\), then we take the point \(t\in T_{R}\) which arrived the earliest (rather than the closest), and such that \(col(t)=col(x)\) and \(d(x,t)\leq\frac{1}{2}R\), and increase \(w(t)\) by one, thus making \(t\) proxy of \(x\). It is important to remark that we do not store the proxy function explicitly, since it would require linear memory. By using the earliest valid coreset point as the proxy, the proxy function can be reconstructed on the fly, a fact that will be used in the second pass of the algorithm. We select the output \((C_{R},T_{R},R)\) of the non-failing instance associated with the smallest guess \(R\). The pseudocode for the first pass is depicted as Algorithm 5

As mentioned above, at the end of the first pass, Step 2 of Algorithm 1 is run on \(T\) to compute the weight distribution function \(\hat{\phi}\) based on \((C_{R},T_{R})\). In the second pass, the final assignment \(\phi\) is computed using the naturally streamlined algorithm FinalAssignment (Algorithm 4) with the only difference that, for every \(x\in S\), its proxy \(\pi(x)\) is obtained as the earliest coreset point \(t\) such that \(col(t)=col(x)\) and \(d(x,t)\leq(\epsilon/12)R\). We have:

**Theorem 5.1**.: _For an input stream \(S\) of doubling dimension \(D\), the above 2-pass algorithm returns a \((3+\epsilon)\) approximation to the optimum fair k-center clustering, with an additive violation \(\leq 4\Delta+3\) of the fairness constraints using working memory \(O\left(k\cdot|C_{\mathcal{S}}|\left((24/\varepsilon)^{D}\log(d_{\max}/d_{min})+| \Gamma|\min(2^{k},k\cdot(24/\varepsilon)^{D})\right)\right)\)

Proof.: First, we prove that the set \(C_{R}\) returned by Algorithm 5 provides a 4 approximation to the unfair k-center problem. Consider the smallest integer \(j\) such that \(d_{min}\cdot 2^{j-1}<r_{k}^{*}\leq d_{min}\cdot 2^{j}\), where \(r_{k}^{*}\) is the radius of an optimal solution to unfair k-center on the stream \(S\), and define \(\hat{R}=2^{j}\). Clearly, \(\hat{R}\leq 2r_{k}^{*}\) Observe that the instance associated to guess \(\hat{R}\) indeed terminates successfully, since the points put in \(C_{\hat{R}}\) must necessarily belong to different optimal unfair clusters. Also, at the end of the stream, we will have that for each \(x\in S\), \(d(x,C_{\hat{R}})\leq 2\hat{R}\leq 4\cdot r_{k}^{*}\). Therefore, Algorithm 5 will return a triple \((C_{R},T_{R},R)\) with \(\leq\hat{R}\leq 4\cdot r_{k}^{*}\). Consider now coreset \(T_{R}\). For each \(x\in S\), we have

\[d(x,T_{R})\leq(\varepsilon/12)R\leq(\varepsilon/12)4r_{k}^{*}\leq(\varepsilon /3)OPT_{\textit{unf}},\]

hence \(T_{R}\) has the same quality of the coreset computed by the sequential algorithm, and the approximation guarantee exhibited by the final solution can thus be argued similarly.

Let us now bound the working memory required by the streaming algorithm. By virtue of the doubling dimension property, for every instance associated with a generic guess \(R\) and until the instance is non-failed, each of the \(\leq k\) clusters of radius \(2R\) induced by \(C_{\mathcal{R}}\) can be covered by using at most \((24/\varepsilon)^{D}\) clusters of radius \(\varepsilon R/12\), and each subset may contribute \(|C_{\mathcal{S}}|\) coreset points to \(T_{R}\). Also, in the first pass, we have \(\log(d_{\max}/d_{min})\) instances of the algorithm running in parallel. The bound on the working memory is a consequence of the bounds on the \(|T_{R}|\)'s and on the size of the linear programs executed at the end of the first pass. 

## 6. Mapreduce algorithm

In this section, we adapt the sequential strategy presented in the Section 4 to the distributed setting, devising the following 5-round MapReduce algorithm. In the first round, the input \(S\) is partitioned arbitrarily across the \(p\) workers, and worker \(i\) extracts a subset \(T_{i}\) of points by executing the first two while-loops of Algorithm 2 on its partition, using accuracy parameter \(\varepsilon/2\) rather than \(\varepsilon\). In the second round, the \(T_{i}\)'s are gathered in a single worker, and their union, say \(T^{\prime}\), is further processed through the first two while-loops of Algorithm 2, using again accuracy \(\varepsilon/2\), to extract a subset \(T^{\prime\prime}\subset T^{\prime}\). In the third round, a copy of \(T^{\prime\prime}\) is sent to each worker, which makes \(|C_{\mathcal{S}}|\) copies of each \(t\in T^{\prime\prime}\) and computes their weights with respect to the points of \(S\) in its partition, as specified in the last two for-loops of Algorithm 2. In the fourth round, the final coreset \(T\) is built by gather all copies of the points of \(T^{\prime\prime}\) created by the different workers in a single worker, and coalescing the \(p\) like-colored copies of each \(t\in T^{\prime\prime}\) by adding up their weights. This produces the final weighted coreset \(T\), on which Step 2 of Algorithm 1 is run sequentially to compute the set \(C\) of centers and \(p\) projections \(\hat{\phi}_{l},i\in[p]\), of the weight distribution function \(\hat{\phi}\), relative to the \(p\) partitions of \(S\). The final assignment is then built in the fifth round, by sending to the \(i\)-th worker the projection \(\hat{\phi}_{l}\), so that procedure FinalAssignment can be applied independently within its partition. The following theorem, whose proof is deferred to Appendix C for lack of space, summarizes the accuracy-space tradeoffs featured by the above algorithm.

**Theorem 6.1**.: _For an input set \(S\) of doubling dimension \(D\), the above 5-round MapReduce algorithm returns a \((3+\varepsilon)\) approximation to the optimum fair k-center clustering, with an additive violation \(\leq 4\Delta+3\) of the fairness constraints, using local memory_

\[M_{L}=O\left(\max\left\{\frac{|S|}{p},k|C_{\mathcal{S}}|\left(p\left(24/ \varepsilon\right)^{D}+|\Gamma|\min\left\{2^{k},k\left(24/\varepsilon\right) ^{D}\right\}\right)\right\}\right)\]

## 7. Experiments

Our experiments aim at: **(a)** comparing the performance of different algorithms for different values of \(k\) in terms of radius and running time; **(b)** verifying the influence of the coreset size on the quality of the approximation; **(c)** demonstrating the efficiency of the streaming and MapReduce approaches on large datasets. We compare our approach against the following baselines: unfarm, the classic GMM algorithm (Gardner et al., 2011), which returns the unfair clustering radius that we use as a reference point; Bera-et-al, the algorithm from (Bera-et-al, 2017); KFC, the algorithm of (Bera-et-al-stream and Bera-et-al-MR the streaming and MapReduce algorithms from (Bera-et-al, 2017), respectively. We devised best-effort implementations of all of the above algorithms, always improving on the running time of the original ones while maintaining the same accuracy, but for KFC, for which we used the author's code. We experiment with the same datasets used by

\begin{table}
\begin{tabular}{l r r r r r r} \hline \hline dataset & \multicolumn{1}{c}{n} & \multicolumn{1}{c}{d} & \multicolumn{1}{c}{\(|\Gamma|\)} & \multicolumn{1}{c}{dataset} & \multicolumn{1}{c}{n} & \multicolumn{1}{c}{d} & \multicolumn{1}{c}{\(|\Gamma|\)} & \multicolumn{1}{c}{77} \\ \hline lmdata & 16 07 906 & 8 & 18 & adult & 32 561 & 5 & 7 & 78 \\ census1990 & 245 285 & 66 & 8 & creditcard & 30 000 & 14 & 7 & 789 \\ athlete & 206 165 & 3 & 2 & bank & 4521 & 9 & 3 & 780 \\ diabetes & 89 782 & 9 & 5 & victorian & 4 500 & 10 & 45 & 761 \\ area & 35 385 & 8 & 4 & reuter\_50 & 50 & 2 500 & 10 & 50 \\ \hline \hline \end{tabular}
\end{table}
Table 1. Datasets used in the experimental evaluation.

previous works (Bera-et-al-stream, 2017; 2017; 2017), whose features are given in Table 1. Due to space constraints, in this section we report only on the four largest ones, providing the results for the others in Appendix D, along with details about our experimental setup. As in (Bera-et-al-stream, 2017), we set strict fairness constraints: \(\beta_{l}=r_{l}(1-\delta)\) and \(\alpha_{l}=r_{l}/(1-\delta)\), for \(r_{l}=|S_{l}|/|S|\) and \(\delta=0.01\). For our algorithms, rather than governing the coreset size indirectly through \(\epsilon\), we fix it directly as a multiple of \(k\), allowing for more interpretable results. Our source code is publicly available (see [https://anonymous.4open.science/r/fair-clustering-C8EF/](https://anonymous.4open.science/r/fair-clustering-C8EF/))

Sequential settingFigure 2 reports the radius and the running time of different sequential algorithms for \(k=2^{t}\), \(i\in[1,6]\). unear always has the smallest radius and the fastest running time, as expected. The best fair clustering radius is up to 15 times larger (hnda) than the unfair clustering radius. Notably, while an unfair clustering sees its radius constantly decreasing with \(k\), for some datasets (hnda, census1990) the fair radius tends to remain constant as \(k\) becomes larger. We observed that in these cases the fairness constraints encourage the assignment of the majority of points to a few (\(\ll k\)) large clusters, whose radius remains large irrespective of the value of \(k\). Two instances of our algorithm, dubbed coreset, were run with coreset sizes \(k\) and \(32k\). As expected, using a larger coreset gives a clustering with a smaller radius, which becomes comparable (at most 1.39 times larger) to the one attained by KFC and Bera-et-al-Note: Noteeably, the slight increase in the radius is compensated by the significantly faster execution time (Figure 2, bottom), even with a coreset of size \(32k\). Indeed, in our experiments, Bera-et-al timed out after one hour on census1990 and hnda, and on athlete for large \(k\), whereas KFC timed out on hnda, with 16 million points. In contrast, our coreset-based algorithm completed just in under 5 minutes for \(k=64\) and a coreset of size \(32k\).

StreamingWe compare our algorithm (coreset-stream) with Bera-et-al-stream (2017) for different amounts of memory allowed to both algorithms, and for \(k=32\). For coreset-stream, larger memory implies that each of the \(\log_{2}d_{max}/d_{min}\) instances of the algorithm builds a larger coreset, whereas for Bera-et-al-stream, larger memory implies that a smaller \(\epsilon\) is used, hence more parallel instances are run, each building a \(k\)-clustering. Both implementations feature the same level of optimization. Figure 3 reports the running time and the radius achieved by both algorithms on the two largest datasets of the testbed. The dashed lines, used for reference, mark the best running time and radius attainable by the sequential fair algorithms. We observe that for comparable memory usage, our coreset-stream algorithm runs faster than Bera-et-al-stream. As for the radius, coreset-stream provides a radius closer to the best radius found by sequential algorithms. The figure highlights the fundamental tradeoff of our coreset construction: larger coresets allow for better approximations. Interestingly, for small memories, both algorithms are faster than the fastest sequential one. This is due both to the low aspect ratio \(d_{max}/d_{min}\) (\(\approx 56K\) for hnda, \(\approx 43\) for census1990) and to the streaming clustering strategy which may require less than \(n\) distance computations per center.

MapReduceWe compare our algorithm (coreset-MR) with Bera-et-al-MR (2017) for different numbers of processors and \(k=32\). Figure 4 reports the results in terms of time and radius. The line in the time plots marks the total running time, whereas the shaded area represents the time required to solve the linear program on the pointset created by each algorithm. As already noted in (2017), the running time of Bera-et-al-MR increases with the number of processors because it is dominated by the time to solve the linear program (shaded red area) whose size increases with the number of processors, thus annulling scalability. Conversely, in coreset-MR the size of the linear program is independent of the number of processors: in fact, the blue shaded area marks a constant running time for all processor counts. Consequently coreset-MR features good scalability. As for the radius, both approaches provide solutions of comparable quality.

Figure 4. MapReduce algorithms performance vs. parallelism: time (left) and radius (right).

Figure 3. Streaming algorithms performance vs. memory (log scale): time (left, log scale) and radius (right).

Figure 2. Radius (top) and running time (bottom, in log scale) of different algorithms vs. \(k\) (in logarithmic scale). Missing points are for timed-out runs.

## References

* (1)
* Amadian et al. (2019) Sara Amadian, Alessandro Epsato, Ravi Kumar, and Mohammad Mabhian. 2019. Clustering without Over-Representation. In _Proc. KDD_. ACM, 267-275.
* Bean et al. (2017) Paul Bean, Parenakes Koufis, and Dan Suciu. 2017. Communication Steps for Parallel Query Processing. _J. ACM_ 64, 6 (2017), 404-405.
* Bean et al. (2019) Susan K Bean, Deepamach Chakrabarty, Nicolas Flores, and Maryam Neishabhani. 2019. Far Algorithms for Clustering. In _Proc. NeurIPS_. 4955-4966.
* Berg et al. (2022) Suman K. Berg, Sramanthank Das, Suanyam Galhotra, and Saguir Schulz. 2022. Fair \(\sim\)Center Clustering in a Single-Reducing and Streaming Settings. In _Proc. WWW_. ACM, 1414-1422.
* Greena et al. (2019) Ioana Greena, Martin Grossi, Samir Khuller, Aounon Kumar, Clemens Roemer, Daniel R Schmidt, and Melanie Schmidt. 2019. On the Cost of Essentially Fair Clustering. In _Proc. APPROX-RANDM (JIFcs, Vol. 145)_. Schloss Dagstuhl-Leibniz-Zentrum fur Informatik. 18:1-1822.
* Ceccardo et al. (2019) Matteo Ceccardo, Andrea Pietaracarpina, and Gepping Pucci. 2019. Solving k-center Clustering (with outliers) in MapReduce and Streaming. _almost as Accurately as Sequentially_. Proc. IJDB Eng. 12, 7 (2019), 766-778.
* Ceccardo et al. (2016) Matteo Ceccardo, Andrea Pietaracarpina, Gepping Pucci, and Eli Upfal. 2016. A Practical Parallel Algorithm for Dameter Approximation of Massive Weighted Graphs. In _Proc. CVPR_. IEEE, 12:1-21.
* Ceccardo et al. (2017) Matteo Ceccardo, Andrea Pietaracarpina, Gepping Pucci, and Eli Upfal. 2017. MapReduce and Streaming Algorithms for Diversity Maximization in Metric Spaces of Bounded Dumbling Dimension. _Proc. VLDB Endow._ 10, 5 (2017), 409-480.
* Chierichetti et al. (2017) Flavio Chierichetti, Ravi Kumar, Silvio Lattanzi, and Sergei Vassilvitskii. 2017. Fair Clustering Through Parallels. In _Proc. VLDB_. 5029-5037.
* Dwork et al. (2012) Cynthia Dwork, Moritz Hardt, Toniann Visser, Omer Reingold, and Richard S. Zemel. 2012. Fairness through awareness. In _Proc. ICTS_. ACM, 214-226.
* Feldman et al. (2014) Michael Feldman, Sorelle A. Friedler, John Moeller, Carlos Stellegger, and Suresh Venkatasubramanian. [n. d.]. Certifying and Removing Disparate Impact. In _Proc. KDD_. ACM, 292-268.
* Gonzalez (1985) Teofilo F. Gonzalez. 1985. Clustering to Minimize the Maximum Intercluster Distance. _Theor. Comput. Sci._ 38 (1985), 230-366.
* Golthek et al. (2014) Lee-Ad Golthek, Aryh Kontorovich, and Robert Krauthgamer. 2014. Efficient Classification for Metric Data. _IEEE Trans. Information Theory_ 60, 9 (2014), 5059-5759.
* Harb and Lam (2020) Elizabeth Harb and Ho Sham Lam. 2020. RFC: A Scalable Approximation Algorithm for \(\mathsf{MSLSC}\)-center Fair Clustering. In _Proc. NeurIPS_.
* Kumar et al. (2023) S. Im R. Kumar, S. Lattanzi, N. Moskey, and Vassilvitskii. 2023. Massive Parallel Computation: Algorithms and Applications. In _Foundations and Trends in Optimization_. Vol. 5. NOW Publishers, 30-47.
* Jorgensen-Houri et al. (2021) Jeff Jorgensen-Houri, Robert Krebsberg, and Derek Dreyer. 2021. Safe systems programming in Rust. _Commun. ACM_ 64, 4 (2021), 144-152.
* Pellizzoni et al. (2020) Paolo Pellizzoni, Andrea Pietaracarpina, and Gepping Pucci. 2020. Dimensionality-adaptive k-center in sliding windows. In _Proc. DSAA_. IEEE, 197-206.
* Pietaracarpina et al. (2012) Andrea Pietaracarpina, Gepping Pucci, Matteo Hendrich, Francesco Silvetti, and Eli Upfal. 2012. Space-round tradeoffs for MapReduce computations. In _Proc. ICS_. ACM, 235-244.
* Leibniz-Zentrum fur Informatik, Vol. 96:1-964.
* Xu and Tian (2015) Dongkuan Xu and Yingjie Tian. 2015. A Comprehensive Survey of Clustering Algorithms. _Annals of Data Science_ 2 (2015), 165-193.
* (70)
* Toth et al. (2019)```
/* Round the solution */ \(z_{J,c}^{int}\leftarrow\lfloor z_{J,c}\rfloor\)\(\forall J\in\mathcal{J}\); Define \(\overline{w}\), \(Z_{c}\), and \(Z_{c,t}\) as in equations (6), (7), and (8); Construct LP-RES; while\(\exists J\in\mathcal{J}(R):\sum_{c\in C}z_{J,c}\neq\overline{w}(J)\)do \(\overline{z}\leftarrow\) solution to LP-RES; foreach\(\overline{z}_{J,c}=0\)do  Remove \(\overline{z}_{J,c}\) from LP-RES foreach\(\overline{z}_{J,c}=1\)do  Remove \(\overline{z}_{J,c}\) from LP-RES; \(z_{J,c}^{int}\leftarrow\overline{z}_{J,c}+1\);  Decrease by \(1\)\(Z_{c}\) and \(Z_{c,t}\); foreach\(c\in C\)do if\(|\{J\in\mathcal{J}:0<\overline{z}_{J,c}<1\}|\leq 3\)then  Remove from constraints (11) involving \(c\); foreach\(c\in C,\ell\in\Gamma\)do if\(|\{J^{\prime}\in\mathcal{J}:0<\overline{z}_{J^{\prime},c}<1\}|\leq 3\)then  Remove from RES constraints (12) involving \(c\);
```

**Algorithm 6**CoresetAssign

Algorithm 6 shows the implementation of this iterative rounding procedure. The last step of Algorithm 6 builds the weight assignment function. By construction we have that for each center, the total weight assigned to \(c\in C\) by means of the function \(\dot{\phi}\) is

\[\sum_{t\in\Gamma}\dot{\phi}\left(t,c\right)=\sum_{J\in\mathcal{J}}z_{J,c}^{int} \qquad\forall c\in C\]

and similarly, for any given color \(c\in\Gamma\)

\[\sum_{t^{\prime}\in\Gamma_{\mathcal{I}}}\dot{\phi}\left(t^{\prime},c\right)= \sum_{J^{\prime}\in\mathcal{J}_{c}}z_{J^{\prime},c}^{int}\qquad\forall c\in C,\forall\ell\in\Gamma\]

Therefore, we have that Inequality (13) holds for the weight distribution function as well:

\[\beta_{\ell}\sum_{t\in\Gamma}\dot{\phi}\left(t,c\right)-4\Delta-3\leq\sum_{t^ {\prime}\in\Gamma_{\mathcal{I}}}\dot{\phi}\left(t^{\prime},c\right)\leq a_{ \ell}\sum_{t\in\Gamma}\dot{\phi}\left(t,c\right)+4\Delta+3\]

and thus Lemma 4.4 follows.

## Appendix C Proof of Theorem 6.1

For \(i\in[\rho]\), let \(S_{i}\subseteq S\) be the subset of the input set \(S\) assigned to the \(i\)-th worker, and let \(\pi_{i}:S_{i}\to\Gamma_{\mathcal{I}}\) be the proxy function associated to the coreset \(T_{i}\subseteq S_{i}\) extracted locally at each worker. Letting \(T_{i}^{k}\subseteq T_{i}\) be the first \(k\) centers computed by GMM on \(S_{i}\), by Lemma 3.1 we have that \(r_{T_{i}^{k},\phi_{unf}}\leq 2OPT_{unf}\), whence \(r_{T_{i},\phi_{unf}}\leq\left((\epsilon/2)/6\right)\cdot 2OPT_{unf}\leq \left(\epsilon/6\right)\cdot OPT_{unf}\). Recall that \(T^{\prime}=\cup_{1\leq i\leq p}T_{i}\). It follows that for each \(x\in S\):

\[d(x,T^{\prime})\leq(\epsilon/6)OPT_{unf}.\]

The argument can now be repeated identically with respect to the extraction of the coreset \(T^{\prime\prime}\) from \(T^{\prime}\). Thus, we have that for each \(t\in T^{\prime}\), \(d(t,T^{\prime\prime})\leq(\epsilon/6)\cdot OPT_{unf}\). Consider now the final coreset \(T\) computed in the fourth round, and, for each \(x\in S\) let \(\pi^{\prime}(x)\) be the point in \(T^{\prime}\) closest to \(x\). We have that

\[d(x,T)\leq d(x,\pi^{\prime}(x))+d(\pi^{\prime}(x),T)\leq(\epsilon/3)OPT_{unf}.\]

Observe that coreset \(T\) satisfies the hypotheses of Lemma 4.3, which in combination with Theorem 4.5 ensures the approximation factor.

For what concerns the bound on the local space, the local space requirements per round is as follows: \(O\left(|S|/\rho\right)\) for Round 1, \(O\left(k\rho(24/\epsilon)^{D}\right)\) for Round 2, \(\max\{|S|/\rho,k(24/\epsilon)^{D}|C_{S}|\}\) for Round 3, \(\max\{|K|_{S}|\rho(24/\epsilon)^{D},k|T||C_{S}|\min\{2^{k},k(24/\epsilon)^{D}\}\}\) for Round 4, and \(\max\{|S|/\rho,k^{2}(24/\epsilon)^{D}\}\) for Round 5. The bound on the local space follows by maximizing over the space requirements of each round.

## Appendix D Additional Experiments

In this appendix we report the experiments and the information omitted from the main paper for space reasons.

### Experimental setup

We implement all the algorithms using Python 3.11.4, leveraging the implementations provided by Harb and Lam (2018). Performance-sensitive parts such as the coreset construction (in the sequential, streaming, and MapReduce settings) are implemented using Rust 1.70.0 for efficiency, and made available to the Python code via Python bindings. Rust provides a similar level of control to C++, and thus allows to write code that is much more efficient than the Python equivalent (Krishna et al., 2018).

We use cplex 22.1.1.0 to solve the linear programs.

The streaming and MapReduce implementations of the algorithms of (Brandt et al., 2018) do not appear to be publicly available. We therefore re-implemented them using Rust, applying the same optimizations as on our own code.

All the code used to carry out the experimental evaluation is publicly available5. Furthermore, the code repository provides information to download and preprocess all the datasets.

Footnote 5: [https://anonymous.atopen.science/r/fair-clustering-C&EF/](https://anonymous.atopen.science/r/fair-clustering-C&EF/)

### Experiments

Figure 5 reports the results for the experiment described in paragraph _Sequential setting_ of Section 7 for all the datasets reported in Table 1. Panels in the Figure are arranged by decreasing size of the corresponding dataset.

The same takeaways discussed in the main paper apply: increasing the coreset size improves the quality of the radius of the clustering found by coreset; the running time is faster than the baselines; the solution quality of coreset is comparable with the baselines. We note that on smaller datasets the gap with the baselines in terms of running time is less marked. This is not surprising, since for small datasets the linear programs used by the baselines are small enough to allow a fast solution. We stress, however, that our coreset construction scales to large instances.

As for the streaming and MapReduce settings, we remark that in Section 7 we report results on the two largest datasets in the testbed. Given that the other datasets are comparatively very small, they do not provide any meaningful insight on the behavior of the MapReduce and Streaming algorithms.

Figure 5. Performance of sequential algorithms in terms of radius (top two rows of plots) and running time (bottom two rows of plots, in logarithmic scale) against \(k\) (in logarithmic scale)