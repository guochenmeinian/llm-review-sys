# An Interpretable Answer Scoring Framework

Omar Alonso

Amazon

omralon@amazon.com

Preetam Prabhu Srikar

Dammu

University of Washington

preetams@uw.edu

Diji Yang

University of California Santa Cruz

dyang39@ucsc.edu

###### Abstract.

In this new LLM-world where users can ask any natural language question, the focus is on the generation of answers with reliable information while satisfying the original intent. LLMs are known to generate multiple versions of answers for the same question, some of which may be better than others. Identifying the most suitable response that adequately addresses the question is non-trivial. In order to tackle this problem, we propose an interpretable scoring system that considers three aspects of an answer: knowledge, content, and structure. We provide an answer quality score method that is explainable and can be a key signal to determining a good answer.

## 1. Introduction

The information-seeking process assumes an interaction cycle that includes the identification of an information need, a specification of such need in a query, the inspection of results, and if needed, a reformulation of the query (Sutton, 2012). Users may use a search engine, an intelligent assistant, or a ChatGPT-like system (Kumar et al., 2017) to accomplish a specific task, but the cognitive model stays the same. The underlying system can provide better services, but from the user's perspective, the overall information access procedure remains constant.

Part of the search process involves assessing how relevant the results are according to the original intent (Sutton, 2012). There are many factors that contribute to relevance criteria and degrees of relevance. Instead of providing a link to a web page or document that contains relevant bits, LLMs-based systems provide an answer that synthesizes all the relevant information (Bauer et al., 2017). The verbosity of such models can, unfortunately, lead to statements that are incorrect and, therefore, produce wrong responses (Kumar et al., 2017).

We propose an explicit and interpretable scoring system for the answer generated by the LLM, thereby providing a measure for the reliability of the LLM response. To derive such a score, we look at three main characteristics of an answer: _knowledge_, _structure_, and _content_ and present the overall architecture of our proposed system in Figure 1. Specifically, the _knowledge_ component evaluates the statements or information present in the answer against a knowledge graph (KG) for reliability. After identifying the entities present in a statement, all relevant entries are fetched from a suitable KG. We then evaluate if the corresponding statement is attributable or contradictory with respect to the retrieved entities by querying an LLM. Adjustable hyper-parameters are used to penalize contradictory statements and award correct statements. The _structure component_ focuses on the gap between the ideal and generated answers. We first define a feature collection to match the features of the ideal answer for each question intent. We utilize embedding alignment techniques to capture the shared attributes between question-answer pairs, which are represented by the output distribution of the intent classifier. The final structure score is jointly determined by the feature collection and the learnable embedding projection. The _content component_ is engineered to align important entity details from the question with those articulated in the answer. This alignment serves as a gauge for how effectively the answer mirrors the question. By identifying and comparing the key entities present in the question and the answer, we gauge the extent to which the answer addresses the essence of the inquiry.

The Interpretability is essential for building information retrieval and knowledge management systems (Bauer et al., 2017); on the other hand, in the era of large-scale language models (LLM), providing the interpretable score for the LLM generated answer is also crucial for the evaluation of LLMs' capabilities (Bauer et al., 2017) as well as for providing meaningful feedback or reward for the model training (Kumar et al., 2017; Wang et al., 2018). Therefore, we believe that defining reliable automatic scoring systems is a step in the right direction.

## 2. Answering Scoring System

Our work is based on identifying specific answer properties that provide signals and aggregate them into a final score. In general, we use knowledge graphs to compute the Knowledge scores (Sec. 2.1), question intent analyzing (Bauer et al., 2017) to assist with the design of the Structure score (Sec. 2.2), and entity-based answer analyzing (Kumar et al., 2017) to drive the Content score (Sec. 2.3). As shown in the Figure 1, the overall scoring pipeline considered the above three scores and functions in a fully automatic manner. This modularity is by design as it allows

Figure 1. Interpretable answer scoring architecture.

us to improve a specific component. Some of the proposed features are related to previous work on aboutiness axioms (Becker et al., 2017; Chen et al., 2018).

### Knowledge Score

In addressing the knowledge limitations in LLMs, the use of information stored in KGs as an external source is beneficial. KGs are adaptable and suitable for handling evolving datasets, such as those in e-commerce. A KG organizes properties and relations as triplets and offers a clear way to organize information, facilitating efficient retrieval. The use of KGs reduces the need for direct one-to-one mapping of reference documents, enhancing the information processing task and potentially reducing errors associated with direct mapping methods.

#### 2.1.1. Verifiable Entities Score (VE-score)

Quantifies the ratio of entities that are verifiable by the KG.

\[VE\_Score=|text\_entities\cap KG\_entities|/|text\_entities| \tag{1}\]

where \(text\_entities\) = entities identified in the text and \(KG\_entities\) = entities present in the KG. The _VE-Score_ measures the ratio of verifiable entities generated by the LLM. This penalizes the LLM when it hallucinates new entities that are invalid.

#### 2.1.2. Entities Relevance Score (ER-Score)

Quantifies the relevance of the verifiable entities.

\[\begin{split} ER\_Score=&\sum_{n=1}^{n}Jaccard(ques,entity \_desc_{i})\\ &+cosine\_similarity(e\_ques,e\_entity\_desc_{i})\end{split} \tag{2}\]

where each question has \(n\) KG entities, and

* \(entity\_desc_{i}\) = description of \(i^{th}\) entity obtained from KG
* \(e\_ques\) = text embedding vector of the question
* \(e\_entity\_desc_{i}\) = text embedding vector of the \(i^{th}\) entity

#### 2.1.3. KG-based Attribution Score (KG-attrScore)

The proposed attribution score presents several advancements over previous attribution methods (Zhu et al., 2017; Chen et al., 2018). Firstly, it offers a more granular approach by enabling sentence-level evaluation. This allows for a detailed analysis of the text, providing a finer understanding of how each sentence contributes to the overall context. Secondly, our approach is more comprehensive. Unlike previous methods that evaluate content against a single document, this approach utilizes a KG as the reference point. This broadens the scope of the evaluation, incorporating a wider array of information and relationships present within the KG, thus offering a more holistic assessment. Lastly, the attribution score is more dynamic due to its integration with the KG. As the KG is updated, the attribution score automatically adjusts to these changes. This ensures that the evaluation remains current and reflects the latest information and trends, enhancing its relevance and accuracy over time.

Given a natural language query \(q\), an answer \(a\), and a set of triplets \(SoT\), the scoring function \(f\) takes (q, a, SoT) as input and outputs a decision along with the rationale. Specifically,

* \(\text{f(q, a, SoT)}=1\) (Attributable) + Rationale
* \(\text{f(q, a, SoT)}=0\) (Extrapolatory) + Rationale
* \(\text{f(q, a, SoT)}=-1\) (Contradictory) + Rationale

In operationalizing the objective function, we employ an LLM Query. LLMs have shown proficiency in Natural Language Inference (NLI) tasks (Becker et al., 2017; Chen et al., 2018; Chen et al., 2018; Chen et al., 2018), which forms the backbone of this approach.

The NLI task (Chen et al., 2018) involves assessing the relationship between a 'premise' and a 'hypothesis', which in our case corresponds to the set of triplets and Q&A pair. The goal is to determine if the hypothesis is true (attributable), false (contradiction), or cannot be determined (extrapolatory) based on the given premise. This process requires analyzing the semantic relationship between the premise and the hypothesis, a task for which LLMs are particularly well-suited due to their advanced capabilities in language understanding. The prompt for performing this operation is shown in Figure 2.

In our experiment, we utilized the Flan-T5 XXL, with a generation temperature set to 0.2, to ensure controlled and precise outputs. Although Flan-T5 XXL was employed in our implementation, the approach is flexible and can be adapted to use other models as needed. This adaptability allows the framework to be tailored to different domains and answer types.

```
SELECT?subject?predicate?object WHERE{wd:{subject_qid}?predicatewd:{object_qid}. BIND(wd:{subject_qid}AS?subject). BIND(wd:{object_qid}AS?object). } SPARQLQueryforretrievingtripletsfromWikidata
```

We utilize a SPARQL query to retrieve relevant triplets from the Wikidata KG (KrishnamurthyThe hyperparameters \(\alpha\), \(\beta\), and \(\gamma\) can be fine-tuned to align with desirable answer characteristics and domain requirements. This flexibility allows the K-score to adapt to a variety of contexts and use cases. In our experiments, we assigned equal weightage to all three parameters (\(\alpha=\beta=\gamma\approx 0.3\)), which balanced the influence of each sub-score.

### Structure Score

_Design._ Lehnert (Lehnert, 1998) highlights the importance of understanding question intent in question and answer systems to satisfy users' information needs. Even if both questions are about the information of the same entities, the different question intents result in two types of answer structures. Bolotova et al. (Bolotova et al., 2018) states that with a well-defined question intent taxonomy, the answers that satisfy one intent are expected to have a specific (language) structure. Thus, Bolotova et al. (Bolotova et al., 2018) proposes a comprehensive taxonomy, NFQA, and the expected structure of answers. Specifically, NFQA includes six categories for non-factoid questions: instruction, reason, evidence-based, comparison, experience, and debate. However, NFQA lacks the fine-grained feature of how each intent is different from others, which limits the reliability and interpretability of the taxonomy. Moreover, it is hard to build an automatic scoring system based on the coarse-grained question description. Building on top of the NFQA framework, we have developed a specialized set of features tailored to our use case. As shown in our scoring pipeline, highlighted by a red line in Figure 1, encompasses both the fine-tuning of an intent classifier and the subsequent feature scoring. Given the domain-specific nature of the classifier, it undergoes fine-tuning within our target domain to ensure its efficacy. Upon successful training, the classifier's intent predictions could be used to guide the feature-scoring function. Table 2 shows the feature set to differentiate between various intents. We design a corresponding function for each feature to map feature representations to scores, thereby differentiating question intents and their respective answer structures. Practically, the feature set includes question length, which indicates complexity and the level of detail required for responses; tense analysis, which is closely connected to the question intent and helps in understanding the temporal context of inquiries; keyword extraction, enabling the identification of the primary subject matter of questions for efficient classification; pronoun detection, providing insights into the type of information sought, such as factual details or explanations; and consideration of the expected answer structure, such as lists or paragraphs, to match the format preferences of users, thereby enhancing precision in classification. By incorporating these features, we can effectively analyze and classify questions based on their underlying intents, facilitating more accurate and tailored responses to user inquiries.

_Implementation._ Following the standard rule-based syntactic parsing works (Selvin et al., 2017), the above-mentioned functions are mainly supported by NLTK (The Natural Language Toolkit) (Brockman et al., 2017). Specifically, we first applied dependency parsing and POS (Part-of-Speech) tagging over the answer sentence compliance with the Penn Treebank (Peswaran et al., 2017). Then, the length can be easily captured by counting the token, while the tense and pronoun can be decided by the predicate and its POS tag. The format of the answer is detected by the special tokens, e.g., a step-by-step list requires a newline token at the end

\begin{table}
\begin{tabular}{|p{34.1pt}|p{142.3pt}|p{142.3pt}|p{142.3pt}|} \hline
**Sent.\#** & **Prediction** & **Relevant triplets** & **Rationale** & **Identified Wiki entities** \\ \hline
1 & Supports & [(‘Barack Obama’, ‘position held’, ‘President States’)] & The triplet (‘Barack Obama’, ‘position held’, ‘President States’) supports the statement that Bards Obama is a former President of the United States. & Q76, Q30, Q305178, Q1169 \\ \hline
2 & Supports & [(‘Barack Obama’, ‘ethnic group’, ‘African Americans’)] & The triplet [‘Barack Obama’, ‘ethnic group’, ‘African Americans’] supports the statement that he is the first African American to have held the office. However, the triplets provided do not directly support the statement about his progressive policies and leadership. & Q484275, Q76, Q1156854, Q49085, Q12823105 \\ \hline
3 & Supports & [(‘Patient Protection and Affordable Care Act’, ‘signatory’, ‘Barack Obama’)] & The triplet [‘Patient Protection and Affordable Care Act’, ‘signatory’, ‘Barack Obama’] supports the statement that Obama enacted landmark legislation such as the Affordable Care Act. However, it does not provide direct support for the other legislation mentioned or the issues addressed by Obama during his presidency. & Q14471, Q118157, Q76, Q5152897, Q32518, Q11771944 \\ \hline
4 & Neutral & NA & NA & Q11471, Q118157, Q76, Q5152897, Q32518, Q11771944 \\ \hline \end{tabular}
\end{table}
Table 1. Examples of KG Attribution Score (KG-AttrScore). KG is Wikidata. Sentence number (first column) refers to the respective sentence in Figure 3. Retrieved triplets are not displayed due to space restrictions.

of each step description. Finally, inspired by NFQA (Chen et al., 2019), we also manually define the keywords for each question intent and use exact match to provide the keyword overlap score.

### Content Score

The Content Score, \(C_{score}\), quantifies the relevance and completeness of an answer by measuring the overlap of entities that are important to the question's context. Specifically, for question-answering tasks that utilize Wikipedia data, these important entities are referred to as _wikiEntities_. The score is computed as the ratio of important entities found in the model-generated answer (\(E\)) to the total number of important entities in the ground-truth answer (\(E_{total}\)), represented mathematically as:

\[C_{score}=\frac{|E\cap E_{total}|}{|E_{total}|} \tag{4}\]

This scoring mechanism emphasizes the model's ability to identify and include all relevant entities in its response, ensuring that the answer is not only correct but also comprehensive. For instance, as depicted in Figure 5, when comparing three products--iPhone 13, Pixel 6, and Galaxy S23, captured by their respective wikiEntities Q108118280, Q108939091, and Q115127699--the Content Score is calculated based on the entity overlap. As the answer includes only two out of the three entities, the recall of important entities is 2/3, indicating that the response falls short of fully addressing the comparison request.

### Final Quality Score

The overall scoring equation is shown as Equation 5, where the Quality score \(SCK\) is the weighted sum of \(S\), \(C\), and \(K\) scores.

\[SCK=\alpha*S_{score}+\beta*C_{score}+\gamma*K_{score} \tag{5}\]

Since question intent faithfully reflects what kind of answer the user expects, the value of the weight factors are designed intent-wise, i.e., the structure of the expected answers for different intent has varying feature weight. For example, Factoid intent is more concerned with matching knowledge (K score) than with the structure of the answer (S score); however, Experience intent should focus more on the common entity (C score) than on some piece of knowledge extracted from the KG. In this work, we present our heuristic-designed weight factors and the rationale behind them in Table 3. In the case that label data is available, the value of weight factors could also be learnable parameters.

\begin{table}
\begin{tabular}{|l|l|l|l|l|l|} \hline
**Category** & **Length** & **Tense** & **Keyword** & **Pronoun** & **Format** \\ \hline Instruction & short to moderate & present; future & steps; follows & 2nd & step-by-step list \\ \hline Reason & short to moderate & present; past & because; due to; since & 3rd, 2nd & sentences short paragraph \\ \hline Evidence-based & moderate to long & past & found; indicate & 3rd & paragraph \\ \hline Comparison & moderate & present; past & different; compared to & 3rd & side-by-side list \\ \hline Experience & moderate to long & past & noticed; felt & 1st & paragraphs \\ \hline Debate & moderate to long & present; past & believe; think; & 1st, 3rd & point-counterpoint paragraphs \\ \hline \end{tabular}
\end{table}
Table 2. Feature collection of NFQA taxonomy.

Figure 4. Structure Scoring Pipeline

Figure 5. LLM answer compares two of the three products. The underline highlights the Wikientities found in the text.

\begin{table}
\begin{tabular}{|l|l|l|l|l|} \hline
**Intent** & \(\alpha\) & \(\beta\) & \(\gamma\) & **Reason** \\ \hline Instruction & 0.5 & 0.2 & 0.3 & Answer structure \textgreater{} KG matching \textgreater{} entity mentioning \\ \hline Reason & 0.5 & 0.3 & 0.2 & keyword from feature collection is useful \\ \hline Evidence-based & 0.2 & 0.3 & 0.5 & No salient feature from structure, Knowledge more important \\ \hline Comparison & 0.5 & 0.3 & 0.2 & Comparison structure is easy to capture \\ \hline Experience & 0.4 & 0.5 & 0.1 & Entity matching \textgreater{} structure features \textgreater{} KG matching \\ \hline Debate & 0.4 & 0.4 & 0.2 & Can hardly be grounded by KG \\ \hline Factoid & 0.1 & 0.1 & 0.8 & KG matching \textgreater{} others \\ \hline \end{tabular}
\end{table}
Table 3. Heuristic design for weight factors. The Reason column includes the rationale behind the design, and the Greater-than sign means that the former score is more important than the latter score under the current intent.

## 3. Experiments and Results

The system is evaluated on 1K question-answer pairs, all of which were randomly selected from the YahooQA dataset1, a corpus containing Non-factoid question-answering pairs. To the best of our knowledge, there is no available data set of LLM answers, so we use YahooQA as a human baseline to test our framework. To illustrate, we present three representative examples in Table 4. The structure of the first question's answer aligns well with the Comparison intent, resulting in a high structure score. Additionally, mentioning both key entities contributes to a high content score. Although not all information can be grounded, the weight factor defined for Comparison intent in Table 3 ensures that decent Structure (S) and Content (C) scores lead to an acceptable final quality score. In the second example, despite the absence of the key entity "router", the answer's overall quality score remains moderately high due to the emphasis on knowledge groundedness, which is crucial for Factoid questions. The last example with a 0.1 quality score indicates a poor response to the question, characterized by irrelevant content and a lack of substantial information. The average SCK scores distribution for the 1K per intent category is presented in Table 5.

Footnote 1: [https://webscope.sandbox.yahoo.com/catalog.php?datatype=ikddi-67](https://webscope.sandbox.yahoo.com/catalog.php?datatype=ikddi-67)

## 4. Limitations

In the proposed framework, we assume that the KG contains sufficient information to evaluate a given answer. In practice, this may not always be the case; therefore, we should consider the knowledge coverage of the KG. Effective routing to the most relevant KG for a given context could significantly improve the relevance of the information retrieved. This indicates a need for further development in linking LLM queries to the most appropriate KGs. In order to support complex relationships, path prediction using knowledge graph embedding models (Wang et al., 2019; Wang et al., 2019) or path ranking algorithms (Wang et al., 2019; Wang et al., 2019) could be used. Additionally, the final quality score computed is based on hand-tuned parameters, which may lead to sub-optimal results. We expect to apply deep neural networks in the future to facilitate weight tuning. More sophisticated feature designs could be tried.

## 5. Conclusion and Future Work

This work presents an automatic scoring system for answer quality that is reliable, interpretable, and faithful. Our scoring system is a meaningful attempt at combating the LLM hallucination problem. Our method opens up opportunities for subsequent applications like candidate answer ranking, results filtering, and more.

Future work involves extending testing to a wider array of LLM-generated answers and diverse standard QA datasets to better assess the scalability and robustness of our scoring system. We also plan to fine-tune the integration of LLMs with knowledge graphs, aiming to enhance the precision of evidence attribution and expand quantitative studies on benchmark results for a more comprehensive evaluation of the model's performance. By understanding the rationale behind LLM outputs at a more granular level, we expect to gain insights into the decision-making process of these models, potentially leading to more sophisticated applications.

## References

* (1)
* A. A. Ting Bai, Z. Cao, Y. Chang, J. Chen, Z. Chen, Z. Cheng, S. Dong, Z. Dou, F. Feng, S. Gao, J. Guo, X. He, T. Lan, C. Li, Y. Liu, Z. Lyu, W. Ma, J. Ma, Z. Ren, P. Ren, Z. Zhang, W. Wang, Y. Wang, J. Rong, W. Zhu, W. Xin, Y. Xu, D. Yu, Z. Yin, P. Zhang, F. Zhang, W. Zhang, M. Zhang, and X. Zhu (2023)Information retrieval meets large language models: A strategic report from Chinese IR community. AI Open4. External Links: Link Cited by: SS1, SS2.
* A. Anand, L. Lyu, M. Idahl, Y. Wang, J. Wallat, and Z. Zhang (2022)Explainable information retrieval: a survey. CoRRabs/2211.02405. External Links: Link, 2111.02405 Cited by: SS1, SS2.
* S. Bird and E. Loper (2004)NLTK: the natural language toolkit. In Proceedings of the ACL Interactive Poster and Demonstration Sessions, pp. 214-217. External Links: Link Cited by: SS2.
* V. Bolotova, Y. Blinov, F. Scholer, W. Bruce Croft, and M. Sanderson (2022)A non-factoid question-answering taxonomy. In Proc. of SIGIR, pp. 1196-1207. Cited by: SS1, SS2.
*

\begin{table}
\begin{tabular}{|p{113.8pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}|p{113.8pt}|} \hline
**Question** & **Answer** & **Intent** & **S** & **C** & **K** & **SCK** \\ \hline What is the difference between soups and stews? & Stews are normally chunkier w/ larger more inexpensive cuts of meat since u simmer i on low heat for a long time. Most are made with a tomato base. while soups on the other hand don’t require large cuts of meat and are perfect for a quick meal w/ little time and ingredients involved compared to stews. & Comparison & 1 & 1 & 0.42 & **0.88** \\ \hline What’s the application area for router? & The application area would be anywhere you want to connect two networks together. At home, connecting your local network to the internet network, definitely at businesses & Factoid & 0.25 & 0.2 & 0.74 & **0.63** \\ \hline How many liquor stores in Houston? & Not enough if you can still formulate questions. :P ton? & Experience & 0.0 & 0.0 & 0.10 & **0.01** \\ \hline \end{tabular}
\end{table}
Table 4. Three representative examples from our scoring system. SCK is the final answer quality score.

\begin{table}
\begin{tabular}{|l|c|} \hline
**Intent** & **Avg. SCK** \\ \hline Comparison & 0.39 \\ \hline Debate & 0.31 \\ \hline Evidence-based & 0.24 \\ \hline Experience & 0.20 \\ \hline Factoid & 0.29 \\ \hline Instruction & 0.33 \\ \hline Reason & 0.33 \\ \hline \end{tabular}
\end{table}
Table 5. Average SCK score per intent for 1K data set.

* Bondarenko et al. (2022) Alexander Bondarenko, Maik Frole, Jan Heinrich Reimer, Bruno Stein, Michael Volke, and Matthias Hagen. 2022. Axiomatic Retrieval Experimentation with ii, axioms. In _Proc. of SIGIR_. ACM, 3131-3140.
* Bruza and Huibers (1994) Peter Bruza and Theo W. C. Huibers. 1994. Investigating Aboutness Axioms using Information Fields. In _Proc. of SIGIR_. 112-121.
* Chang et al. (2024) Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaije Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2024. A survey on evaluation of large language models. _ACM Transactions on Intelligent Systems Technology_ 15, 3 (2024), 1-45.
* Chowdhury et al. (2022) Aakanka Chowdhury, Sharan Narang, Jacob Devlin, Maarten Bosma, Gureav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language with pathways. _arXiv preprint arXiv:2204.02311_ (2022).
* Dagan et al. (2010) Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth. 2010. Recognizing Textual Entailment: Rational, Evaluation And Approaches. _Journal of Natural Language Engineering_ 4 (January 2010).
* Huang et al. (2023) Lei Huang, Weiqing Yu, Weihao Meng, Zhong Zhong, Zhangyin Feng, Haotian Wang, Qiangdong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. 2023. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. _arXiv preprint arXiv:2311.05232_ (2023).
* Lan et al. (2019) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019. Albert: A lite bert for self-supervised learning of language representations. _arXiv preprint arXiv:1909.11942_ (2019).
* Li and Cohen (2010) N Lu and William W Cohen. 2010. Relational retrieval using a combination of path-constrained random walks. _Machine learning_ 81 (2010), 53-67.
* Luo et al. (2011) Ni Luo, Tom Mitchell, and William Cohen. 2011. Random walk inference and learning in a large scale knowledge base. In _Proceedings of the 2011 conference on empirical methods in natural language processing_. 529-539.
* Liembert (1997) Wendy G. Liembert. 1997. A Conceptful Theory of Question Answering. In _Proc. of IJCAI_. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 158-164.
* Longger et al. (2021) Shaye Longger, Kartik Perisich, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh. 2021. Entity-based knowledge conflicts in question answering. _arXiv preprint arXiv:2109.08628_ (2021).
* Marcus et al. (1993) Mitchell P. Marcus, Beattie Santorini, and Mary Ann Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. _Computational Linguistics_ 19, 2 (1993), 313-330.
* Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mikhkin, Chong Zhang, Sandhimi Agarwal, Katarina Slima, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. _Advances in Neural Information Processing Systems_ 35 (2022), 27730-27744.
* Mnid et al. (2021) Colin Mnid, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Li. 2021. Exploring the limits of transfer learning with a unified text-to-text transformer. _The Journal of Machine Learning Research_ 11, 2 (2020), 5485-5551.
* Rashkin et al. (2023) Hannah Rashkin, Vitaly Nikolaev, Matthew Lamm, Lora Aroyo, Michael Collins, Dipanjan Das, Slav Petrov, Gaurav Singh Tomar, Iulia Turc, and David Reitter. 2023. Measuring attribution in natural language generation models. _Computational Linguistics_ (2023), 1-64.
* Rossi et al. (2021) Andrea Rossi, Denilson Barbosa, Donalella Firmani, Antonio Matinata, and Paolo Merialdo. 2021. Knowledge graph embedding for link prediction: A comparative analysis. _ACM Transactions on Knowledge Discovery from Data (TKDD)_ 15, 2 (2021), 1-49.
* Yandecik and Kotvitsch (2014) Denny Yandecik and Markus Kotvitsch. 2014. Wikidata: a free collaborative knowledgebase. _Commun. ACM_ 57, 10 (2014), 78-85.
* Wang et al. (2017) Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. 2017. Knowledge graph embedding: A survey of approaches and applications. _IEEE Transactions on Knowledge and Data Engineering_ 29, 12 (2017), 2274-2743.
* Wang et al. (2021) Sinong Wang, Han Fang, Madian Khabas, Hamzi Mao, and Hao Ma. 2021. Entailment as few-shot learner. _arXiv preprint arXiv:2104.14060_ (2021).
* Yang et al. (2024) Diip Yang, Kezhen Chen, Jimnane Rao, Xiaoyang Guo, Yawen Zhang, Jie Yang, and Yi Zhang. 2024. Tackling vision language tasks through learning inner monologs. In _Proceedings of the AAAI Conference on Artificial Intelligence_, Vol. 38. 19350-19358.
* Yue et al. (2023) Xiang Yue, Bohli Wang, Kai Zhang, Ziru Chen, Yu Su, and Huan Sun. 2023. Automatic evaluation of attribution by large language models. _arXiv preprint arXiv:2305.06311_ (2023).
* Zhai (2020) ChengXiang Zhai. 2020. Interactive Information Retrieval: Models, Algorithms, and Evaluation. In _Proceedings of the 40rd International ACM SIGIR Conference on Research and Development in Information Retrieval_ (Virtual Event, China) _(SIGIR '20)_. Association for Computing Machinery, New York, NY, USA, 2444-2447. [https://doi.org/10.1145/397271.39401424](https://doi.org/10.1145/397271.39401424)
* Zhang et al. (2019) Hongfei Zhang, Xia Song, Chenyan Xiong, Corby Rosset, Paul N Bennett, Nick Craswell, and Saurabh Tiwary. 2019. Generic intent representation in web search. In _Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval_. 65-74.
* Zhang (2020) Meishan Zhang. 2020. A survey of syntactic-semantic parsing based on constituent and dependency structures. _Science China Technological Sciences_ 63, 10 (2020), 1898-1920.