# MixRec: Individual and Collective Mixing Empowers Data Augmentation for Recommender Systems

Anonymous Author(s)

Anonymous Author(s)

###### Abstract.

The core of the modern recommender systems lies in learning high-quality embedding representations of users and items to investigate their positional relations in the feature space. Unfortunately, data sparsity caused by difficult-to-access interaction data severely limits the effectiveness of recommender systems. Faced with such a dilemma, various types of self-supervised learning methods have been introduced into recommender systems in an attempt to alleviate the data sparsity through distribution modeling or data augmentation. However, most data augmentation relies on elaborate manual design, which is not only not universal, but the bloated and redundant augmentation process may significantly slow down model training progress. To tackle these limitations, we propose a novel Dual Mixing-based Recommendation Framework (MixRec) to empower data augmentation as we wish. Specifically, we propose individual mixing and collective mixing, respectively. The former aims to provide a new positive sample that is unique to the target (user or item) and to make the pair-wise recommendation loss benefit from it, while the latter aims to portray a new sample that contains group properties in a batch. The two mentioned mixing mechanisms allow for data augmentation with only one parameter that does not need to be set multiple times and can be done in linear time complexity. Besides, we propose the dual-mixing contrastive learning to maximize the utilization of these new-constructed samples to enhance the consistency between pairs of positive samples. Experimental results on four real-world datasets demonstrate the effectiveness of MixRec in terms of recommendation performance, training efficiency, sparsity resistance, and usability.

recommender system, collaborative filtering, data augmentation, self-supervised learning +
Footnote †: ccs: Information systems Recommender Systems

+
Footnote †: ccs: Information systems Recommender Systems

+
Footnote †: ccs: Information systems Recommender Systems

+
Footnote †: ccs: Information systems Recommender Systems

+
Footnote †: ccs: Information systems Recommender Systems

+
Footnote †: ccs: Information systems Recommender Systems

+
Footnote †: ccs: Information systems Recommender Systems

## 1. Introduction

As an integral part of modern Internet platforms, recommender systems have garnered significant attention (Kang et al., 2016; Chen et al., 2017). The wisdom of the crowd lies at the core of recommender systems, which leverage group behavior to filter out irrelevant information (Kang et al., 2016; Chen et al., 2017). With the rise of representation learning, the development of recommender systems has kept pace, with various neural networks being employed to model high-quality embeddings for users and items (Kang et al., 2016). As a data-driven scientific application, recommender systems now rely on interaction data more than ever. However, for various reasons, it is often challenging for individual platforms to obtain comprehensive user data, leaving recommender systems frequently dealing with data sparsity (Kang et al., 2016; Chen et al., 2017).

It is against this backdrop that many researchers have sought to introduce Self-supervised learning (SSL) (Kang et al., 2016) into recommender systems to mitigate the data sparsity problem. This contains two paradigms, Generative and Contrastive. Specifically, generative models (Goodfellow et al., 2014; Goodfellow et al., 2014) based on probabilistic modeling can reconstruct historical interactions, while the more easily implemented contrastive learning (Chen et al., 2017) (CL) has made significant strides in combating data sparsity through data augmentation. Through data augmentation, contrastive learning maximizes mutual information from different views of the same sample, providing additional supervision signals for the recommendation task (Kang et al., 2016). Consequently, research is increasingly focusing on exploring different types of data augmentation (Zhu et al., 2017). Early explorations concentrated on augmenting graph (Zhu et al., 2017), where the graph is derived from user-item interactions (Kang et al., 2016). Researcher constructs different views of the original input by randomly masking nodes or edges (Bogurovsky et al., 2016). Subsequently, a new line of thought has emerged, focusing on achieving augmentation in the feature space. Examples include introducing noise for representations (Kang et al., 2016; Wang et al., 2016), or finding semantic neighbors through clustering (Kang et al., 2016; Wang et al., 2016), attention (Kang et al., 2016; Chen et al., 2017), or hierarchical mechanisms (Chen et al., 2017).

Despite the impressive progress of these novel methods, we argue that existing research still faces several limitations. Firstly, much of CL-based methods tends to rely on manually crafted augmentation. Examples include the need to precisely control the ratio of censored raw data (Kang et al., 2016) or carefully add noise to raw data or representations to avoid over-corrupting the original semantic information (Wang et al., 2016). More critically, to leverage more sophisticated augmentation strategies without overly disrupting recommendation, many recent methods introduce more hyper-parameters to balance multi-tasking (Wang et al., 2016; Wang et al., 2016; Wang et al., 2016). Given the wide variability of data structures, researchers are often required to test with multiple hyper-parameter sets for different application scenarios. This not only increases the cost of trial and error, but also raises concerns about improving subsequent research. Although some methods adopt an alternative approach by achieving data augmentation through repeated sampling (Kang et al., 2016; Wang et al., 2016), this strategy significantly increases warm-up and introduces unquantifiable sampling bias (Chen et al., 2017; Wang et al., 2016).

Secondly, the data augmentation strategies proposed in many existing works substantially increase the time costs associated with model training and inference. In early research, commonly used data augmentation methods are often accompanied by multiple encodings and propagations to obtain different perspectives of the same input (Zhu et al., 2017; Wang et al., 2018). And in more recent research, many studies introduce the paradigm of learnable (or adaptive) augmentation (Zhu et al., 2017; Wang et al., 2018), as well as unique feature-lookup mechanisms (Zhu et al., 2017; Wang et al., 2018) to identify suitable new views in the semantic space. In practice, these strategies typically lead to an exponential increase in model training and inference time, as multiple iterations of the base recommendation model's encoder are needed to construct representations of various views. As noted in previous pioneering work (Zhu et al., 2017), the time complexity of the model after introducing graph augmentation-based CL is 3.7 times greater than that of the base model.

The final and most significant limitation is that many CL-based methods do not utilize these new samples efficiently. Specifically, the broad approach aims to enhance the consistency of a pair of positive samples while maintaining uniformity among the negative samples (Bahdanau et al., 2014). While effective, this approach is insufficient for fully utilizing each sample, particularly as it neglects to explore the varying characteristics of individual samples. If we intend to introduce a larger number of samples, we may encounter some challenges. In other words, the approach lacks scalability. In a nutshell, although data augmentation is essential for data-sparse recommendation scenarios, existing augmentation strategies often struggle with the dilemma. This prevents us from achieving desired augmentations, thereby limiting the overall recommendation performance. Therefore, we further investigate whether the following objectives can be achieved simultaneously:

* How can we more easily implement data augmentation with minimal hyper-parameter tuning?
* How can we achieve data augmentation without significantly increasing model complexity?
* How can new samples obtained through data augmentation be utilized more effectively to enhance recommendation?

To tackle the above limitations, we propose a novel Dual Mixing-based Recommendation Framework (**MixRec**) for recommender system. MixRec does not rely on any external strategies to construct new views of the original sample, instead using the samples themselves. Specifically, inspired by the mixing mechanism (Zhu et al., 2017), we intend to create mixes of several samples. Based on this assumption, we propose the **Individual Mixing** and **Collective Mixing**, respectively. Individual mixing aims to create new views that are more closely aligned with the original sample while also incorporating information from other samples. Collective mixing, on the other hand, emphasizes a more holistic perspective, _i.e._, the wisdom of the crowd. Such mixing process is a convex combination, which can be achieved with linear complexity and requires only simple controls to create new samples. In our experiments, we also find that the mixing process is hyper-parameter-agnostic and exhibits strong generalization properties. In addition, we propose the **Dual Mixing Contrastive Learning**, which, for the first time, maximizes the consistency of positive samples from dual mixing perspectives. This approach fully leverages all available new samples to enhance the supervision signals for the recommendation task. The major contributions of this paper are summarized as follows:

* We propose a recommendation framework MixRec, which contains two lightweight data augmentation strategies, individual mixing and collective mixing for recommendation task.
* We further propose dual-mixing contrastive learning, which maximizes the consistency of positive sample pairs from two perspectives while utilizing more negative samples to provide additional supervision signals for the recommendation task.
* We conduct comparison experiments with twenty related baseline methods across four real datasets to thoroughly validate the effectiveness of the proposed MixRec.

## 2. Methodology

### Problem Formulation

Without loss of generality, a recommendation scenario contains \(M\) users (\(\mathcal{U}=\{u_{1},u_{2},...,u_{M}\}\)) and \(N\) items (\(\mathcal{U}=\{i_{1},i_{2},...,i_{N}\}\)), and interactions between them (Chen et al., 2019). Based on existing work (Chen et al., 2019; Wang et al., 2018), historical interactions are typically stored as an interaction matrix \(\mathbf{R}\in\mathbb{R}^{M\times N}\), where if there is an observed interaction between user \(u\) and item \(i\), we then have \(R_{ui}=1\). The task of the recommender system aims to learn the prediction function \(f_{\Theta}(u,j)\) as a means of predicting user \(u\)'s preference score \(\hat{R}_{uj}\) for the item \(j\) that have not been interacted with, where \(\Theta\) is the set of trainable model parameter. For general recommendation scenarios, the only trainable parameters are the initial embedding representations for users \(\{\mathbf{e}_{u}^{(0)}\}\) and items \(\{\mathbf{e}_{i}^{(0)}\}\). Based on the above definition, we propose the dual mixing-based recommendation framework MixRec, as shown in Fig. 1.

### Interaction Graph Encoding

In the context of a recommender system, the user-item interaction graph (Wang et al., 2018) is a natural way to represent the complex relationships between users and items. Specifically, the user-item interaction graph is denoted as \(\mathcal{G}=\mathcal{C}\ \mathcal{U}\cup\mathcal{I},\mathcal{E}>\), where \(\mathcal{U}\), \(\mathcal{I}\), and \(\mathcal{E}\) are the set of users, the set of items, and the interactions between users and items (\(\mathcal{E}_{ui}=\mathcal{E}_{iu}=R_{ui}\)). In recent studies, graph convolutional networks (GCNs) (Golov et al., 2013) are widely used to model interaction graph, which aggregate information from a node's neighbors, allowing each node to incorporate contextual information and leads to more informative representations. Given any user node \(u\in\mathcal{U}\), the user embedding update process for an arbitrary layer is shown below:

\[\mathbf{e}_{u}^{(l)}=\text{AGG}(\mathbf{e}_{u}^{(l-1)},\{\mathbf{e}_{i}^{(l-1 )}:i\in\mathcal{N}_{u}\}), \tag{1}\]

where \(\mathbf{e}_{u}^{(l)}\in\mathbb{R}^{d}\) is the embedding of user \(u\) on the \(l\)-th layer, \(d\) is the embedding size, \(\mathcal{N}_{u}\) is the first-order neighbor set of user \(u\), and \(\text{AGG}(\cdot)\) is a manually defined aggregation function. The item side has a similar definition. In general, \(\text{AGG}\) functions have multiple implementation strategies. Considering that general recommendation only uses user and item ID as inputs, we adopt the widely used lightweight graph convolution (Chen et al., 2019) to encode user \(u\in\mathcal{U}\) and item \(i\in\mathcal{I}\) on the interaction graph:

\[\mathbf{e}_{u}^{(l)}=\sum_{i\in\mathcal{N}_{u}}p_{ui}\mathbf{e}_{i}^{(l-1)}; \quad\mathbf{e}_{i}^{(l)}=\sum_{u\in\mathcal{N}_{i}}p_{ui}\mathbf{e}_{u}^{(l-1 )}, \tag{2}\]where \(p_{ui}=1/\sqrt{|\mathcal{N}_{u}||\mathcal{N}_{i}|}\) is the graph Laplacian norm (Golov et al., 2013; Golov et al., 2013). Compared to vanilla GCN (Golov et al., 2013), Eq. 2 does not rely on feature transformation, and the process of information aggregation is done only by linear combination of neighbors, which is more in line with the semantic-free feature of implicit feedback (Golov et al., 2013; Golov et al., 2013). After \(L\) layers of propagation, we construct usable embeddings for downstream tasks by addition (Golov et al., 2013): \(\mathbf{e}_{u}=\sum_{l=1}^{L}\mathbf{e}_{u}^{(l)}\) and \(\mathbf{e}_{i}=\sum_{l=1}^{L}\mathbf{e}_{i}^{(l)}\).

### Dual-Mixing for Data Augmentation

Although user and item representations obtained by graph structure encoding can already be directly used in the downstream recommendation task, a serious problem is the data sparsity issue as mentioned in the Introduction section (Golov et al., 2013; Golov et al., 2013). Extremely sparse implicit feedback may not provide sufficient supervision signals, leading to suboptimal learned embeddings (Golov et al., 2013). Therefore, data augmentation is especially important for recommendation task. Given that most existing data augmentation strategies for self-supervised contrastive learning suffer from high complexity and a lack of flexibility, this dilemma prompts us to reconsider and rediscover a proven augmentation strategy. Our goal is to generate multiple new views as we wish, without relying on complex augmentation rules or time-consuming repetitive sampling. Building on this, we propose the **Dual-Mixing** for data augmentation, which incorporates both individual and collective mixing.

#### Individual Mixing

We first consider any individual (user \(u\) or item \(i\)) in a sampled mini-batch. By graph encoding, we have obtained their corresponding embedding representations: \(\mathbf{e}_{u}\) and \(\mathbf{e}_{i}\). Inspired by (Golov et al., 2013; Golov et al., 2013), we construct new samples by linearly combining the original embedding pairs. Specifically, **individual mixing** creates synthetic training samples by linearly interpolating between two random samples from a sampled mini-batch:

\[\mathbf{e}_{u}^{im}=\beta_{u}\cdot\mathbf{e}_{u}+(1-\beta_{u})\cdot\mathbf{e}_ {u}^{dis},\quad\mathbf{e}_{i}^{im}=\beta_{i}\cdot\mathbf{e}_{i}+(1-\beta_{i}) \cdot\mathbf{e}_{i}^{dis}, \tag{3}\]

where \(\beta\) is a mixing coefficient drawn from a symmetric Beta distribution \(Beta(\alpha,\alpha)\) with shape parameter \(\alpha\in(0,\infty)\) (For simplicity, we define \(\alpha=\alpha_{u}=\alpha_{i}\)), and \(\mathbf{e}_{u}^{dis}\) is the embedding at the corresponding position of user \(u\) after the order of user embeddings in a batch has been disrupted (_i.e._, the embedding of any other user within the same batch).

Unlike traditional data augmentation, which is commonly used in contrastive learning (Wang et al., 2016; Wang et al., 2016), individual mixing does not require multiple instances of graph encoding or repeated sampling. More importantly, we can generate new views on demand by simply tweaking the shape parameter \(\alpha\), while embedding \(\mathbf{e}_{u}^{dis}\) is automatically obtained through a disordered operation. In practice, we empirically set \(\alpha\) to 0.1, which eliminates the need for extensive manual effort in individual mixing. In addition, by controlling for a smaller \(\alpha\), we constrain the sampled beta \(\beta\) to yield larger values, enabling the newly generated sample \(\mathbf{e}_{u}^{im}\) to retain as many properties of the original sample \(\mathbf{e}_{u}\) as possible. This aligns with the invariance of contrastive learning (Golov et al., 2013), treating these new sample as positive of the original view.

#### Collective Mixing

With individual mixing, we can easily generate new views for user \(u\) or item \(i\). These new views retain information from most of the original views, allowing us to align them in feature space by maximizing mutual information. However, we note that the essence of recommender systems lies in the wisdom of the crowd (Golov et al., 2013; Golov et al., 2013), highlighting the importance of considering inter-user relationships. Therefore, we further propose **collective mixing**, building upon individual mixing, to create new views that incorporate group information for each user. Specifically, collective mixing generates new examples by forming convex combinations of pairs of examples from the entire batch:

\[\mathbf{e}_{u}^{em}=\beta_{1}\mathbf{e}_{1}+\beta_{2}\mathbf{e}_{2}+\cdots+ \beta_{|\mathcal{B}|}\mathbf{e}_{|\mathcal{B}|},\quad\text{s.t.}\sum_{\alpha =1}^{|\mathcal{B}|}\theta_{o}=1.0, \tag{4}\]

Figure 1. The complete information flow of the proposed MixRec. MixRec contains several phases of graph encoding, dual-mixing, dual-mixing contrastive learning, and multi-task learning for recommendation.

Figure 2. Example of construction process for three new views of user node \(u_{1}\) (batch size \(|\mathcal{B}|=3\)).

where \(\mathbf{e}_{u}^{cm}\) is the new view for user \(u\) generated through collective mixing, with a similar definition for the item side. \(\{\beta_{1},\beta_{2},...,\beta_{|\mathcal{B}|}\}\) is a set of coefficients for convex combination, which is sampled from a multivariate Dirichlet distribution (Bishop, 2006):

\[\{\beta_{1},\beta_{2},...,\beta_{|\mathcal{B}|}\}\sim Dirichlet(\alpha_{1}, \alpha_{2},...,\alpha_{|\mathcal{B}|}), \tag{5}\]

where \(\{a_{l}:i\in\mathcal{B}\}\) is a set of positive real numbers to parameterize, and \(\mathcal{B}\) is a sampled mini batch. The new sample \(\mathbf{e}_{u}^{cm}\) contains more information from other users in the same batch than the individual mixing. The new samples obtained from collective mixing contain more information from other users than individual mixing.

We now sort out the proposed dual-mixing, and the process of constructing new views is presented in Fig. 2. For individual mixing (Fig. 2(b)), the new sample \(im\) is a linear interpolation of the original inputs \(u_{1}\) and \(u_{2}\). For collective mixing (Fig. 2(c)), the new sample \(em\) produced by the convex combination always lies within the convex hull of the original inputs \(\{u_{1},u_{2},u_{3}\}\). Due to the influence of multiple sets of shape parameters, the \(cm\) is affected by the influence from a wider range of users, which makes it extremely limited although it contains information from the original sample \(u_{1}\). Therefore, the sample \(im\) obtained from individual mixing we regard as a **positive sample** of the original view \(u_{1}\), while the sample \(cm\) produced by collective mixing is a **hard negative sample** of the original view \(u_{1}\). And the sample \(dis\) (_i.e._, \(u_{2}\)) obtained by disorder is naturally a **easy negative sample** of the original view \(u_{1}\).

### Dual-Mixing Contrastive Learning

Taking the user side as an example, through the proposed dual-mixing, we obtain two new views of user \(u\) (_i.e._, \(\mathbf{e}_{u}^{im}\) and \(\mathbf{e}_{u}^{cm}\)), along with one additional view \(\mathbf{e}_{u}^{dis}\) of the other users derived from disorder. For user \(u\), we have a total of four different views, as outlined in Table 1. It is now time to consider how to utilize these new views to provide additional supervision signals for the main recommendation task. A reasonable approach is to apply widely-used infoNCE (Bishop, 2006; Li et al., 2017) to maximize the mutual information between positive samples:

\[\mathcal{L}_{u}^{\text{cl}}=\frac{\exp(s(\mathbf{e}_{u}^{\prime},\mathbf{e}_{u}^{\prime \prime})/\tau)}{\sum_{\mathbf{e}\in\mathcal{U}}t^{\text{exp}}(s(\mathbf{e}_{u}^{ \prime\prime},\mathbf{e}_{u}^{\prime\prime})/\tau)}, \tag{6}\]

where \(\mathbf{e}_{u}^{\prime}\) and \(\mathbf{e}_{u}^{\prime\prime}\) are additional views of user \(u\) via data augmentation, \(s(\cdot,\cdot)\) is the cosine similarity function, and \(\tau\) is a temperature parameter. The contrastive loss essentially encourages the alignment of positive views while pushing the positive view \(\mathbf{e}_{u}^{\prime}\) away from the negative sample view \(\mathbf{e}_{u}^{\prime\prime}\), thereby making the feature space more uniform (Zhou et al., 2017; Wang et al., 2018). However, we argue that the supervision signals provided by vanilla contrastive learning are insufficient, as it fails to account for the uniformity across a wider range of sample pairs. In addition, the standard infoNCE loss overlooks the original view, leading to inconsistency between the auxiliary task and the main recommendation task. Therefore, we shift our focus and leverage the multiple views constructed earlier to propose the **dual-mixing contrastive learning**. Given user \(u\), we first define the positive mixing contrastive loss:

\[\mathcal{L}_{u}^{\text{pos}}=-\text{log}\frac{\exp(s(\mathbf{e}_{u},\mathbf{e}_{u}^{ im})/\tau)}{\sum_{\mathbf{e}\in\mathcal{U}}t^{\text{(exp}}(s(\mathbf{e}_{u},\mathbf{e}_{u}^{ dis})/\tau)+\exp(s(\mathbf{e}_{u},\mathbf{e}_{u}^{cm})/\tau)]}. \tag{7}\]

In contrast to the original infoNCE, the above equation directly utilizes the original view as an anchor node to align the main task with the auxiliary task. In addition, we expand the number of negative sample pairs by contrasting the original view with multiple negative samples, further optimizing the uniformity of the entire feature space. To fully leverage the efficacy of these views, we next present the negative mixing contrastive loss as a counterpart:

\[\mathcal{L}_{u}^{\text{neg}}=-\text{log}\frac{\exp(s(\mathbf{e}_{u}^{dis},\mathbf{e}_ {u}^{im})/\tau)}{\sum_{\mathbf{e}\in\mathcal{U}}t^{\text{(exp}(s(\mathbf{e}_{u}^{dis}, \mathbf{e}_{u})/\tau)+\exp(s(\mathbf{e}_{u}^{dis},\mathbf{e}_{u}^{cm})/\tau))}}. \tag{8}\]

In this step, we boldly employ the negative view \(\mathbf{e}_{u}^{cm}\) at the corresponding position, obtained through order perturbation, as the anchor node while maintaining the roles of the other views. This design aims to effectively measure the correlation between these views, thereby providing additional supervision signals for the recommendation task. Note that the positive sample \(\mathbf{e}_{u}^{im}\) is derived from mixing the two anchor nodes \(\mathbf{e}_{u}\) and \(\mathbf{e}_{u}^{dis}\). Consequently, both contrastive losses include a measure of the alignment between \(\mathbf{e}_{u}^{im}\) with the two anchor nodes. The similarity of the positive sample \(\mathbf{e}_{u}^{im}\) to the two anchor nodes is governed by the mixing coefficient \(\beta_{u}\). Therefore, we similarly utilize \(\beta_{u}\) to determine the weight of the two contrastive losses described above:

\[\mathcal{L}_{\text{user}}=\sum_{u\in\mathcal{U}}\beta_{u}\cdot\mathcal{L}_{u}^ {\text{pos}}+(1-\beta_{u})\cdot\mathcal{L}_{u}^{\text{neg}}, \tag{9}\]

where \(\beta_{u}\) is the mixing coefficient used for individual mixing in Eq. 3. For the item side, we have a similar definition:

\[\mathcal{L}_{\text{item}}=\sum_{i\in I}\beta_{i}\cdot\mathcal{L}_{i}^{\text{pos }}+(1-\beta_{i})\cdot\mathcal{L}_{i}^{\text{neg}}. \tag{10}\]

### Multi-task Learning

The optimization of MixRec involves two components: the recommendation task and the auxiliary task. For the main recommendation task, we take the widely applied BPR loss (Kang et al., 2017) to make more variability between positive and negative samples:

\[\mathcal{L}_{\text{BPR}}^{\text{pos}}=\sum_{<u,>\in\mathcal{O}^{\prime},<u,j>\in \mathcal{O}^{-}}-\text{ln}\sigma(\mathbf{e}_{u}^{\top}\mathbf{e}_{i}-\mathbf{e}_{u}^{\top} \mathbf{e}_{j}), \tag{11}\]

where \(\sigma\) is the sigmoid function. \(i\in\mathcal{I}\) is an item that user \(u\in\mathcal{U}\) has interacted with, and \(j\in I\) is any uninteracted one, and both of them are sampled from a uniform distribution (Chen et al., 2019). \(\mathcal{O}^{+}\) and \(\mathcal{O}^{-}\) are the observed and unobserved interaction sets, respectively.

As mentioned earlier, the original interaction data is highly sparse, making it infeasible to achieve satisfactory results by relying solely on the BPR loss described above. Therefore, we further utilize the previously mentioned individual mixing to construct additional negative samples for each user: \(\mathbf{e}_{j}^{im}=\beta_{i}\cdot\mathbf{e}_{j}+(1-\beta_{i})\cdot\mathbf{e}_{j}^{dis}\), where

\begin{table}
\begin{tabular}{c|c} \hline
**Notation** & **Explanation** \\ \hline \hline \(\mathbf{e}_{u}^{im}\) & The original view of user \(u\) \\ \hline \(\mathbf{e}_{u}^{im}\) & The new view of user \(u\) by individual mixing \\ \(\mathbf{e}_{u}^{im}\) & The new view of user \(u\) by collective mixing \\ \(\mathbf{e}_{u}^{dis}\) & The new view of user \(u\) by disorder in a batch \\ \hline \end{tabular}
\end{table}
Table 1. Notation and explanation of the original and other new views of the user \(u\).

item \(j\) is the original negative sample for user \(u\), and \(\mathbf{\varepsilon}_{j}^{dis}\) is constructed by disorder. Thus, within a mini-batch, we can construct multiple new negative samples for user \(u\), resulting in the set \(\mathbf{O}_{u}^{-}\). Subsequently, we compute the pairwise ranking loss for user \(u\) based on these new negative samples:

\[\mathcal{L}_{\text{BPR}}^{\text{neg}}=\sum_{c_{u},b>c\,O^{*}}-\text{ln}\sigma (\mathbf{\varepsilon}_{u}^{\top}\mathbf{\varepsilon}_{b}-\sum_{j\in\mathbf{O}_{u}}\mathbf{ \varepsilon}_{u}^{\top}\mathbf{\varepsilon}_{j}^{im}). \tag{12}\]

Compared to the original BPR loss, the equation above further considers the distance relationship between positive sample \(i\) and multiple constructed negative samples \(\mathbf{O}_{u}^{-}\), which encourages item \(i\) to remain closer to user \(u\) in the feature space. Similarly, we balance the two BPR losses through a linear combination:

\[\mathcal{L}_{\text{main}}=\beta_{i}\cdot\mathcal{L}_{\text{BPR}}^{\text{pos}} +(1-\beta_{i})\cdot\mathcal{L}_{\text{BPR}}^{\text{neg}}. \tag{13}\]

Finally, to integrate the recommendation task with the auxiliary task, we employ a multi-task joint training strategy for optimization. The complete optimization objective of MixRec is defined as follows:

\[\mathcal{L}_{\text{MixRec}}=\mathcal{L}_{\text{main}}+\lambda_{1}\cdot(\mathcal{ L}_{\text{user}}+\mathcal{L}_{\text{item}})+\lambda_{2}\cdot\left\|\mathbf{\varepsilon} \right\|_{2}^{2}, \tag{14}\]

where \(\lambda_{1}\) and \(\lambda_{2}\) are hyper-parameters to trade off the magnitude of losses, and \(\Theta=\mathbf{E}^{(0)}\) is the set of trainable model parameter.

### Time Complexity

In this section, we analyze the time complexity of MixRec. Specifically, we set the number of nodes and edges of the interaction graph \(\mathbf{\mathcal{G}}\) to be \(|\mathcal{V}|\) and \(|\mathcal{E}|\), respectively. \(L\) is the number of GCN layers, \(d\) is the embedding size, and \(|\mathbf{\mathcal{B}}|\) is the batch size. Next, we present the key components that contribute to the time complexity:

* **Interaction Graph Encoding:** The time complexity of this component is in line with mainstream methods since we adopt the design of the classical LightGCN (Chen et al., 2017). Therefore, the time complexity of this component is \(O(2|\mathcal{E}|\mathcal{L}d)\).
* **Dual-Mixing:** We introduce individual and collective mixing for data augmentation. Recalling Eqs. 3 and 4, this component does not significantly increase the time complexity, as the mixing operation involves only an addition of embeddings rather than matrix multiplication
* **Dual-Mixing Contrastive Learning:** In this component, we need to compute the contrastive loss on the user and item side, respectively. Therefore, the time complexity of this component is \(O(4(|\mathbf{\mathcal{B}}|d+2|\mathbf{\mathcal{B}}|^{2}d))\).
* **Recommendation Losses:** We adopt the widely used BPR loss (Zhu et al., 2019) to optimize the recommendation task. In addition, we additionally introduced mixed negative samples. Therefore, the time complexity of this component is \(O(2|\mathbf{\mathcal{B}}|d+|\mathbf{\mathcal{B}}|^{2}d)\).

In practice, the time complexity of MixRec comes mainly from interaction graph encoding since the batch size \(|\mathbf{\mathcal{B}}|\) is much smaller than the interaction scale \(|\mathcal{E}|\). As a result, the actual complexity of MixRec is slightly higher than that of LightGCN due to the additional computation of losses; however, it remains significantly lower than other methods based on self-supervised contrastive learning or multiple sampling. This is primarily because MixRec requires only linear time complexity for data augmentation, avoiding the need to perform graph encoding multiple times. Additionally, MixRec does not require extra time for sampling multiple negative samples.

## 3. Experiments

In this section, we perform experiments on four real-world datasets to validate our proposed MixRec compared with state-of-the-art recommendation methods.

### Experimental Settings

#### 3.1.1. **Datasets**

To validate the effectiveness of MixRec, we adopt four widely used recommendation datasets: Douban-Book (Zhu et al., 2019; Wang et al., 2020), Yelp (Chen et al., 2017; Wang et al., 2020), Amazon-Book (Chen et al., 2017; Wang et al., 2020), and Tmall (Chen et al., 2019; Wang et al., 2020), which are varied in field, scale, and sparsity level. Detailed statistics for the four datasets are presented in Table 2. For fair comparison, preprocessing of all datasets remains consistent with previous studies (Chen et al., 2017; Wang et al., 2020). Specifically, all explicit feedback is forced to be converted to implicit feedback (binary values). Items that a user has interacted with are considered positive samples, while all other items are considered negative samples that can be sampled for that user.

#### 3.1.2. **Baselines**

To validate the effectiveness of MixRec, we choose the following state-of-the-art recommendation methods for comparison experiment:

* **Factorization-based method:** MF (Zhu et al., 2019).
* **Generative methods:** Mult-VAE (He et al., 2016), CVGA (Wang et al., 2020), and DiffRec (Wang et al., 2020).
* **GCN-based methods:** NGCF (Wang et al., 2020), LightGCN (Chen et al., 2017), IMP-GCN (Chen et al., 2017), MixGCF (Chen et al., 2017), and CAGCN* (Chen et al., 2017).
* **SSL-based methods:** SGL-ED (Wang et al., 2020), NCL (Chen et al., 2019), DirectAU (Wang et al., 2020), SimGL (Wang et al., 2020), GraphAU (Chen et al., 2017), CGL (Chen et al., 2017), VGCL (Wang et al., 2020), LightGCL (Chen et al., 2017), SCCF (Wang et al., 2020), RecDCL (Wang et al., 2020), and BIGCF (Wang et al., 2020).

#### 3.1.3. **Hyperparameter Settings**

We implement MixRec in PyTorch1. For a fair comparison, we adopt an experimental setup consistent with previous works (Chen et al., 2017; Wang et al., 2020). Specifically, the embedding size and batch size of all models are set to 64 (excluding Mult-VAE (He et al., 2016), DiffRec (Wang et al., 2020), and RecDCL (Wang et al., 2020)) and 2048, respectively. For all graph-based methods, the number of network layers was set to 3 (Chen et al., 2017) (excluding IMP-GCN (Chen et al., 2017)). The default optimizer is Adam (Kingma and Ba, 2014), and initialization is done via the Xavier method (Chen et al., 2017). We follow the suggested settings in the authors' original papers and use a grid search to choose the optimum hyperparameters for all baselines. For MixRec, we empirically set the temperature coefficient \(\tau\) to be 0.2. The weight of contrastive learning \(\lambda_{1}\) is set in the range of {0.01, 0.5, 0.1, 0.2,..., 2.0}, and the weight of \(L_{2}\) regularization \(\lambda_{2}\) is set in 1e\({}^{-4}\) by default. The default setting for the shape parameter \(\alpha\) is 0.1. To assess the performance of Top-N recommendation, we employ two commonly used evaluation metrics: Recall@N and NNDCG@N (N=20 by default), which are computed by the all-ranking (Chen et al., 2017; Wang et al., 2020; Wang et al., 2020).

\begin{table}
\begin{tabular}{l|c|c|c|c} \hline
**Dataset** & **\#Users** & **\#Items** & **\#Interactions** & **Sparsity** \\ \hline
**Douban-Book** & 13,024 & 22,347 & 792,062 & 99.72\% \\
**Yelp** & 31,668 & 38,048 & 1,561,406 & 99.87\% \\
**Tmall** & 47,939 & 41,390 & 2,357,450 & 99.88\% \\
**Amazon-Book** & 52,643 & 91,599 & 2,984,108 & 99.94\% \\ \hline \end{tabular}
\end{table}
Table 2. Statistics of the datasets.

### Performance Comparisons

#### Overall Comparisons

Table 3 shows the performance of MixRec and all baseline methods on four datasets. MixRec achieves the best recommendation performance over all baselines on all datasets. Quantitatively, MixRec improves over the best baselines _w.r.t._ Recall@20 by 1.78%, 5.05%, 1.81%, and 2.13% on Yelp, Amazon-Book, Tmall, and Douban-Book datasets, respectively. The experimental results demonstrate the effectiveness and generalization of MixRec. We attribute the performance improvement to the proposed individual and collective mixing, which effectively achieves data augmentation and alleviates the data sparsity problem faced by recommender systems.

MixGCF (Chen et al., 2019), another recommendation model utilizing the mixing mechanism, achieves better recommendation performance than MF and LightGCN, further demonstrating the superiority of the mixing mechanism. However, MixGCF merely samples multiple negative instances for calculating the BPR loss, which not only introduces sampling bias but also fails to provide additional supervision signals for the recommendation task. Compared to all contrastive learning-based methods, MixRec consistently outperforms them. This can be attributed to the fact that traditional contrastive learning methods do not fully leverage both positive and negative samples. In contrast, MixRec introduces dual-mixing contrastive learning, which effectively evaluates the role of various negative samples. Moreover, MixRec avoids the need for multiple graph encodings, ensuring its time complexity remains relatively low.

\begin{table}
\begin{tabular}{l|c c c|c c c} \hline \hline  & \multicolumn{3}{c|}{**Tmall**} & \multicolumn{3}{c}{**Amazon-Book**} \\ \cline{2-7}  & T/E & epochs & runtime & T/E & epochs & runtime \\ \hline \hline LightGCN & 49.1s & 286 & 3h50m & 54.7s & 423 & 6h26m \\ IMP-GCN & 224.5s & 220 & 13h43m & 357.2s & 260 & 25h48m \\ MixGCF & 180.3s & 114 & ShtQm & 202.5s & 89 & 5h \\ \hline SimGCL & 132.4s & 24 & 53m & 167.5s & 21 & 58m \\ CGCL & 105.5s & 76 & 2h14m & 147.3s & 59 & 2h25m \\ BIGCF & 56.7s & 40 & 38m & 71.1s & 42 & 50m \\ \hline
**MixRec-1** & 32.4s & 34 & **18m** & 35.3s & 26 & **15m** \\
**MixRec-3** & 56.1s & 26 & **24m** & 61.5s & 19 & **19m** \\ \hline \hline \end{tabular}
\end{table}
Table 4. Efficiency comparison on Tmall and Amazon-Book datasets _w.r.t._ time/epoch (T/E), number of epochs, and total runtime (measured in seconds (s), minutes (m), hours (h)).

Figure 3. Training curves of LightGCN (best), SimGCL and MixRec on (a) Tmall and (b) Amazon-Book datasets.

\begin{table}
\begin{tabular}{l|l|c c c|c c c|c c} \hline \hline \multirow{2}{*}{**Model Name**} & \multicolumn{3}{c|}{**Yelp**} & \multicolumn{3}{c|}{**Amazon-Book**} & \multicolumn{3}{c|}{**Tmall**} & \multicolumn{3}{c}{**Douban-Book**} \\ \cline{2-11}  & \multicolumn{1}{c|}{} & Recall@20 & NDCG@20 & Recall@20 & NDCG@20 & Recall@20 & NDCG@20 & Recall@20 & NDCG@20 \\ \hline \hline MF (Wang et al., 2019) & UAI’09 & 0.0539 & 0.0439 & 0.0308 & 0.0239 & 0.0547 & 0.0400 & 0.1292 & 0.1147 \\ \hline Mult-VAE (He et al., 2016) & WWW’18 & 0.0584 & 0.0450 & 0.0407 & 0.0315 & 0.0740 & 0.0552 & 0.1670 & 0.1604 \\ CVGA (Wang et al., 2019) & TOIS’23 & 0.0694 & 0.0571 & 0.0492 & 0.0379 & 0.0854 & 0.0648 & 0.1736 & 0.1650 \\ DiffRec (Wang et al., 2019) & SIGIR’23 & 0.0665 & 0.0556 & 0.0514 & 0.0418 & 0.0792 & 0.0612 & 0.1619 & 0.1661 \\ \hline NGCF (Wang et al., 2019) & SIGIR’19 & 0.0560 & 0.0456 & 0.0342 & 0.0261 & 0.0629 & 0.0465 & 0.1376 & 0.1215 \\ LightGCN (Chen et al., 2019) & SIGIR’20 & 0.0639 & 0.0525 & 0.0411 & 0.0315 & 0.0711 & 0.0530 & 0.1504 & 0.1404 \\ MixGCF (Chen et al., 2019) & KDD’21 & 0.0713 & 0.0589 & 0.0485 & 0.0378 & 0.0813 & 0.0611 & 0.1731 & 0.1685 \\ IMP-GCN (Chen et al., 2019) & WWW’21 & 0.0653 & 0.0531 & 0.0460 & 0.0357 & 0.0729 & 0.0539 & 0.1725 & 0.1604 \\ CAGCN* (Chen et al., 2019) & WWW’23 & 0.0711 & 0.0590 & 0.0506 & 0.0400 & 0.0783 & 0.0581 & 0.1704 & 0.1667 \\ \hline SGL-ED (Wang et al., 2019) & SIGIR’21 & 0.0675 & 0.0555 & 0.0478 & 0.0379 & 0.0738 & 0.0556 & 0.1633 & 0.1585 \\ NCL (He et al., 2016) & WWW’22 & 0.0685 & 0.0577 & 0.0481 & 0.0373 & 0.0750 & 0.0553 & 0.1647 & 0.1539 \\ DirectAU (Wang et al., 2019) & KDD’22 & 0.0703 & 0.0583 & 0.0506 & 0.0406 & 0.0752 & 0.0576 & 0.1660 & 0.1568 \\ SimGCL (Wang et al., 2019) & SIGIR’22 & 0.0721 & 0.0601 & 0.0515 & 0.0414 & 0.0884 & 0.0674 & 0.1728 & 0.1671 \\ GraphAU (Chen et al., 2019) & SIGIR’23 & 0.0691 & 0.0574 & 0.0508 & 0.0403 & 0.0840 & 0.0625 & 0.1699 & 0.1633 \\ CGCL (Chen et al., 2019) & SIGIR’23 & 0.0694 & 0.0561 & 0.0482 & 0.0375 & 0.0861 & 0.0650 & 0.1741 & 0.1667 \\ VGGL (Wang et al., 2019) & SIGIR’23 & 0.0715 & 0.0587 & 0.0506 & 0.0401 & 0.0880 & 0.0670 & 0.1733 & 0.1689 \\ LightGCL (Wang et al., 2019) & ICLR’23 & 0.0692 & 0.0571 & 0.0506 & 0.0397 & 0.0833 & 0.0637 & 0.1570 & 0.1455 \\ SCCF (Wang et al., 2019) & KDD’24 & 0.0701 & 0.0580 & 0.0491 & 0.0399 & 0.0772 & 0.0580 & 0.1711 & 0.1639 \\ RecDCL (Wang et al., 2019) & WWW’24 & 0.0690 & 0.0560 & 0.0510 & 0.0405 & 0.0853 & 0.0632 & 0.1664 & 0.1526 \\ BIGCF (Wang et al., 2019) & SIGIR’24 & 0.0729 & 0.0602 & 0.0500 & 0.0398 & 0.0876 & 0.0664 & 0.1741 & 0.1682 \\ \hline
**MixRec (Ours)** & & **0.0740*** & **0.0612*** & **0.0541*** & **0.0433*** & **0.0900*** & **0.0686*** & **0.1778*** & **0.1712*** \\ \(p\)-values & & 5.21e-6 & 3.97e-5 & 2.07e-7 & 9.14e-6 & 4.75e-5 & 1.25e-5 & 9.82e-6 & 4.44e-5 \\ \hline \hline \end{tabular}
\end{table}
Table 3. Overall performance comparisons on Yelp, Amazon-Book, Tmall, and Douban-Book datasets. The results of MixRec are bolded, whereas the best baseline is _underlined_. * denotes that the improvement is significant with a \(p\)-value < 0.001 based on a two-tailed paired t-test. Part of the results are duplicated from original papers for consistency.

#### 3.2.2. Comparisons w.r.t. Efficiency

In this section, we present a comparison of the training time of MixRec (\(L=1\) and \(L=3\)) with baseline methods on the two largest datasets, Tmall and Amazon-Book, as shown in Table 4. As can be seen, while MixRec-3's single training time is only slightly higher than LightGCN (Chen et al., 2019) due to the additional loss computations, it is still significantly lower than other methods. MixRec-1, on the other hand, takes even less time than LightGCN. Besides, methods like LightGCN are constrained by sparse interaction data, requiring hundreds of iterations to achieve convergence. In contrast, MixRec converges in far fewer iterations, resulting in a significantly reduced overall training time.

We further compare the training process of MixRec with the best-performing baseline method SimGCL (Zhu et al., 2019) on Tmall and Amazon-Book datasets, as shown in Fig. 3. Compared to SimGCL, MixRec's training process is more stable, as evidenced by the absence of the performance drop seen in the early stages. We attribute this stability to MixRec's ability to strike a better balance between alignment and uniformity, preventing early excessive uniformity from disrupting the effective distribution of the feature space (Zhu et al., 2019).

#### 3.2.3. Comparisons w.r.t. Data Sparsity

In this section, we study the sparsity resistance of MixRec with the classical method LightGCN and the two well-performing baseline methods SimGCL and BIGCF on the two sparsest datasets, Tmall and Amazon-Book datasets. We take a generalized approach by dividing the interaction data from the training set evenly into four user groups (\(U_{1},U_{2},U_{3},U_{4}\)) based on the scale of the interactions. Specifically, \(U1\) has the fewest interactions per user, indicating that it is the sparsest user group. And so on, \(U4\) is the most active engaging user group. We train on the full training set and test each user group individually, and the experimental results are shown in Fig. 4.

MixRec achieves noticeable performance gains across all sparse groups, further demonstrating its effectiveness. Focusing on the sparsest group \(U1\), the improvement rates on the two datasets are 12% and 10%, respectively. We attribute this performance improvement primarily to the proposed individual and collective mixing strategies. These not only significantly increase the number of samples but also provide additional supervision signals for main recommendation task through dual-mixing contrastive learning.

#### 3.2.4. Comparisons w.r.t. GCN Layers

Table 5 provides comparisons of the effectiveness of MixRec and other methods with various GCN layer settings. It should be noted that MixRec with only one layer outperforms even SimGCL (Zhu et al., 2019) and BIGCF (Zhu et al., 2019) with three layers, which shows MixRec can effectively mine user-item relationships without ever-reliance on high-order graph structures, making it effective in reducing training costs (MixRec-1 in Table 4).

### In-depth Studies of MixRec

#### 3.3.1. Ablation Studies

We construct a series of variants to verify the validity of each module in MixRec:

* MixRec\({}_{\text{w/o DMCL (user)}}\): remove the Dual-Mixing Contrastive Learning on the user side (Eq. 9);
* MixRec\({}_{\text{w/o DMCL (item)}}\): remove the Dual-Mixing Contrastive Learning on the item side (Eq. 10);
* MixRec\({}_{\text{w/o DM}}\): remove individual mixing (Eq. 3), and modify the positive sample to be the anchor node itself;
* MixRec\({}_{\text{w/o DM}}\): remove collective mixing (Eq. 4).

The experimental results for all variants with MixRec on four datasets are shown in Table 6. It is obvious that removing any of the modules resulted in varying degrees of performance degradation for MixRec, demonstrating the effectiveness of the various modules. Regarding the DMCL modules on both the user and item sides, the most significant performance degradation occurs when these modules are removed, indicating that relying solely on the main recommendation task is insufficient for modeling high-quality user and item embeddings. Focusing on the other three modules that utilize data augmentation, the observed performance decline further underscores the importance of data augmentation in mitigating the data sparsity problem.

#### 3.3.2. Hyperparameter Sensitivities

Most of MixRec's parameters are kept at the default settings (see Section 3.1.3 for details). Here we focus on two parameters \(\lambda_{1}\) and \(\alpha\). Their performance variations on four datasets are shown in Fig. 5.

For the weight of contrastive losses, \(\lambda_{1}\) shows different trends on four datasets, which is mainly due to the characteristics of the datasets themselves. For dense datasets, \(\lambda_{1}\) usually takes smaller values (_e.g._, Douban-Book); for sparse datasets, \(\lambda_{1}\) takes larger values (_e.g._, Tmall and Amazon-Book). The optimal values on Amazon-Book, Douban-book, Tmall, and Yelp datasets are 1.0, 0.1, 0.6 and 0.3 respectively.

For the shape parameter \(\alpha\), all datasets then show consistency. Specifically, in all cases, \(\alpha=0.1\) fetches the best recommendation performance. In other cases, too large a \(\alpha\) will significantly degrade performance, indicating that the newly constructed sample

Figure 4. Sparsity tests on (a) Tmall and (b) Amazon-Book datasets. The \(x\)-axis shows user groups and proportions.

\begin{table}
\begin{tabular}{c|l|r r|r r} \hline \multirow{2}{*}{\# **Layers**} & \multicolumn{3}{c|}{**Tmall**} & \multicolumn{3}{c}{**Amazon-Book**} \\ \cline{3-6}  & \multicolumn{3}{c|}{Recall} & \multicolumn{1}{c|}{NDCG} & \multicolumn{1}{c}{Recall} & \multicolumn{1}{c}{NDCG} & \multicolumn{1}{c}{788} \\ \hline \hline \multirow{4}{*}{\(L=1\)} & SimGCL (Zhu et al., 2019) & 0.0834 & 0.0635 & 0.0453 & 0.0358 & 779 \\  & BIGCF (Zhu et al., 2019) & 0.0851 & 0.0648 & 0.0466 & 0.0360 & 760 \\  & **MixRec** & **0.0890** & **0.0681** & **0.0533** & **0.0429** & 761 \\ \hline \multirow{4}{*}{\(L=2\)} & SimGCL (Zhu et al., 2019) & 0.0867 & 0.0665 & 0.0507 & 0.0405 & 762 \\  & BIGCF (Zhu et al., 2019) & 0.0865 & 0.0660 & 0.0493 & 0.0401 & 763 \\  & **MixRec** & **0.0896** & **0.0684** & **0.0535** & **0.0431** & 764 \\ \hline \multirow{4}{*}{\(L=3\)} & SimGCL (Zhu et al., 2019) & 0.0884 & 0.0674 & 0.0515 & 0.0414 & 765 \\  & BIGCF (Zhu et al., 2019) & 0.0876 & 0.0664 & 0.0500 & 0.0398 & 766 \\ \cline{1-1}  & **MixRec** & **0.0900** & **0.0686** & **0.0541** & **0.0433** & 767 \\ \hline \end{tabular}
\end{table}
Table 5. Performance comparisons between MixRec and other baseline methods _w.r.t._ number of GCN layer \(L\).

is too corrupted to be considered a positive sample of the original view. Since \(\alpha\) shows consistency across multiple datasets, we can set it to 0.1 by default without additional adjustment. Given that the shape parameter \(\alpha=0.1\), the number of GCN layers \(L=3\), the temperature coefficient \(\tau=0.2\), and the regularization coefficient \(\lambda=1\text{e}^{-4}\) are kept at their default values regardless of the datasets, **the only hyper-parameter MixRec can adjust is the contrastive coefficient \(\lambda_{1}\)**.

## 4. Related Work

**General Recommendation** General recommendation is a branch of recommender systems that focuses solely on user-item interactions (Chen et al., 2019). In this context, implicit feedback is widely used due to its easy accessibility (Chen et al., 2019). However, improving the accuracy of general recommender system is challenging because of the sparsity of implicit feedback and the lack of semantic richness without auxiliary feature information (Kang et al., 2019; Wang et al., 2019). Early work focuses on matrix factorization (Chen et al., 2019), which eventually led to the introduction of neural networks to significantly enhance the model's learning capacity and generalization (Chen et al., 2019). With the rise of graph neural networks, researchers have begun abstracting historical interactions into bipartite graph to model high-order relationships between users and items (Zhou et al., 2019; Wang et al., 2019). From the graph perspective, traditional vector-based and neural network-based approaches can only capture first-order interactions, which limits their recommendation performance. Among these novel efforts, LightGCN (Chen et al., 2019) has been widely adopted in subsequent research due to its ease of deployment and has replaced matrix factorization as a foundational model. The success of graph-based methods is evident not only in general recommendation scenario but also in other recommendation branches, including social recommendation (Zhou et al., 2019), knowledge graph recommendation (Zhou et al., 2019), and multimodal recommendation (Chen et al., 2019), etc.

**Self-supervised Learning for Recommendation** Despite its considerable growth and a series of achievements, general recommendation still suffers from the sparsity of interaction data. Therefore, self-supervised learning is introduced into recommender systems to alleviate the data sparsity problem by constructing auxiliary tasks to provide additional supervision signals for the main recommendation task. In general, self-supervised learning can be broadly categorized into generative and contrastive models (He et al., 2016). The former aims to model the distribution of the user community and reconstruct the complete interaction at a probabilistic level, as seen in models like Mult-VAE (He et al., 2016) and DiffRec (He et al., 2016). The latter leverages data augmentation techniques to maximize mutual information between different views of the same sample. Earlier work focuses on modifying the original input data (Chen et al., 2019), such as in SGL (Zhou et al., 2019), which randomly masks edges or nodes on the interaction graph. More recent work has shifted its focus toward finding new views within the feature space (Zhou et al., 2019). Examples include introducing noise for representations (Wang et al., 2019; Wang et al., 2019), or finding semantic neighbors through clustering (Zhou et al., 2019; Wang et al., 2019), attention (Kang et al., 2019; Wang et al., 2019), or hierarchical mechanisms (Zhou et al., 2019). Studies like DirectAU (Wang et al., 2019; Wang et al., 2019; Wang et al., 2019) revisit contrastive learning from the perspectives of alignment and uniformity (Zhou et al., 2019). SCCF (Wang et al., 2019), on the other hand, seeks to integrate graph convolution and contrastive learning into a unified framework. However, existing data augmentation strategies often suffer from high complexity and lack flexibility. More importantly, they typically measure the mutual information of a sample pair from only one perspective, failing to fully utilize the newly generated samples. MixRec is contrary to the design philosophy of pioneering works. Specifically, MixRec not only constructs richer new views with linear complexity but also maximizes the use of these samples through dual-mixing, enhancing its ability to support recommendation.

## 5. Conclusion

In this paper, we revisited the data sparsity problem entrenched in general recommendation and presented MixRec, an end-to-end dual mixing recommendation framework, which contains individual mixing and collective mixing for data augmentation. Specifically, individual mixing aims to construct new samples that are unique to the original inputs, while collective mixing considers the overall group perspective, creating new samples that represent the collective behavior of all users. Furthermore, we proposed dual-mixing contrastive learning to fully leverage all available sample pairs, maximizing the supervision signals provided for the recommendation task. we conducted extensive experiments on four real-world datasets and verified the effectiveness of MixRec.

\begin{table}
\begin{tabular}{l|c c|c c|c c|c c}  & \multicolumn{2}{c|}{**Yelp**} & \multicolumn{2}{c|}{**Amazon-Book**} & \multicolumn{2}{c|}{**Tmall**} & \multicolumn{2}{c}{**Douban-Book**} \\ \cline{2-9}  & Recall@20 & NDCG@20 & Recall@20 & NDCG@20 & Recall@20 & NDCG@20 & Recall@20 & NDCG@20 \\ \hline \hline w/o DMCL (user) (Eq. 9) & 0.0676 & 0.0556 & 0.0484 & 0.0385 & 0.0826 & 0.0630 & 0.1566 & 0.1451 \\ w/o DMCL (item) (Eq. 10) & 0.0652 & 0.0540 & 0.0436 & 0.0342 & 0.0815 & 0.0608 & 0.1593 & 0.1473 \\ w/o IM (Eq. 3) & 0.0731 & 0.0605 & 0.0504 & 0.0403 & 0.0872 & 0.0665 & 0.1716 & 0.1628 \\ w/o DM (Eq. 4) & 0.0721 & 0.0597 & 0.0511 & 0.0409 & 0.0868 & 0.0664 & 0.1720 & 0.1639 \\ \hline MixRec & **0.0740** & **0.0612** & **0.0541** & **0.0433** & **0.0900** & **0.0686** & **0.1778** & **0.1712** \\ \hline \end{tabular}
\end{table}
Table 6. Ablation studies of MixRec on Yelp, Amazon-Book, Tmall, and Douban-Book datasets.

Figure 5. Hyper-parameter Sensitivities for (a) the weight of loss \(\lambda_{1}\) and (b) shape parameter \(\alpha\) on four datasets.

## References

* (1)
* Bai et al. (2024) Haoyue Bai, Le Wu, Min Hou, Xiaomiao Cai, Zhuanghuang He, Yuyang Zhou, Richang Hong, and Meng Wang. 2024. Multimodality Invariant Learning for Multimedia-Based New Item Recommendation. In _Proceedings of the 42th International ACM SIGIR Conference on Research and Development in Information Retrieval_. 677-686.
* Cai et al. (2023) Xuheng Cai, Chao Huang, Lianghao Xia, and Xabin Ren. 2023. LightGCL: Simple Yet Effective Graph Contrastive Learning for Recommendation. In _The Eleventh International Conference on Learning Representations (ICLR)_.
* Chen et al. (2026) Jiawei Chen, Hanle Dong, Xiang Wang, Fei Feng Wang, and Xiangnan Zhou. 2026. Bias and dellaison in recommender system: A survey and future directions. _ACM Transactions on Information Systems_ 41, 3 (2026), 1-39.
* Chen et al. (2020) Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A Simple Framework for Contrastive Learning of Visual Representations. In _International Conference on Machine Learning (ICML)_. 1597-1607.
* Cilor and Bengio (2010) Xavier Cilor and Yoshua Bengio. 2010. Understanding the Difficulty of Training Deep Feedforward Neural Networks. In _Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics (ICAB)_. 249-256.
* He et al. (2023) Wei He, Guohuo Sun, Jinhui Lu, and Xin Suie Fang. 2023. Candidate-aware Graph Contrastive Learning for Recommendation. In _Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval_. 1670-179.
* He et al. (2020) Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. LightGCN: Simplifying and Powering Graph Convolution Network for Recommendation. In _Proceedings of the 14th International ACM SIGIR Conference on Research and Development in Information Retrieval_. 639-648.
* Li et al. (2020) Xiangnan He, Lizi Liao, Zhuang Zhang, Liqiang Nie, Xiu Hu, and Tat-Seng Chen. 2020. Nextril Collaborative Filtering. In _Proceedings of the 26th International Conference on World Wide Web (WWW)_. 173-182.
* Huang et al. (2021) Tinglin Huang, Yuhao Dong, Ming Ding, Zhen Yang, Wenzheng Feng, Xinyu Wang, and Jeng Tang. 2021. Migged: An improved training method for graph neural network-based recommender systems. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_. 665-674.
* Kim et al. (2020) Sangryun Kim, Gilun Lee, Sangmin Bae, and Se-Young Yun. 2020. Misco: Mix-up contrastive learning for visual representation. _arXiv preprint arXiv:2010.06300_ (2020).
* Kingma and Ba (2014) Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method for Stochastic Optimization. _arXiv preprint arXiv:1412.6980_ (2014).
* Kipf and Welling (2017) Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with Graph Convolutional Networks. In _International Conference on Learning Representations_. Toulon, France.
* Li et al. (2024) Honglong Li, Lei Sang, Yi Zhang, and Yven Zhang. 2024. SimCN: Simple Contrast-enhanced Network for CTR Prediction. In _Proceedings of the 28th ACM International Conference on Multimedia_.
* Li et al. (2022) Shang Li, Xiang Wang, An Zhang, Tingxin Wu, Xiangnan He, and Tat-Seng Chua. 2022. Lei Invariant Random Discovery Inspire Graph Contrastive Learning In _International Conference on Machine Learning_. PMLR, 3052-3065.
* Liang et al. (2018) Dawen Liang, Rahul G Krishnan, Matthew D Hoffman, and Tony Jebara. 2018. Variational Autoencoder for Collaborative Filtering. In _Proceedings of the 2018 World Wide Web Conference (WWW)_. 689-698.
* Lin et al. (2022) Zihun Lin, Changxin Tian, Yongguo Hou, and Wayne Xin Zhao. 2022. Improving graph collaborative filtering with neighborhood-enriched contrastive learning. In _Proceedings of the ACM SIGIR Conference on_. 2232-2329.
* Liu et al. (2021) Fan Liu, Zhiyong Cheng, Lei Zhu, Zan Gao, and Liqiang Nie. 2021. Interest-aware Message-Passing GCN for Recommendation. In _Proceedings of the Web Conference_. 2021, 1296-1305.
* Liu et al. (2021) Xiao Liu, Yanjin Zhang, Zheyu Luo, Liqin Li, Mian Zhouy Wang, Jing Zhang, and jie Tang. 2021. Self-supervised learning: Generative contrastive. IEEE Transactions on Knowledge and Data Engineering_ 35, 1 (2021), 857-876.
* Ren et al. (2023) Xunli Ren, Lianghao Xia, Jiashu Zhao, Davey Yin, and Chao Huang. 2023. Disentangled contrastive filtering. In _Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval_. 1137-1146.
* Rendle et al. (2009) Steffen Rendle, Christoly Freudenthaler, Zong Gantner, and Lars Schmidt-Thieme. 2009. IRPR: Bayesian Personalized Ranking from Implicit Feedback. In _Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (IUI)_. 452-461.
* Ricci et al. (2011) Francesco Ricci, Luc Rolnick, and Bracha Shapira. 2011. Introduction to Recommender Systems Handbook. Springer, 1-35.
* Wang et al. (2020) Chenyang Wang, Yuanqing Yu, Weihli Ma, Min Zhang, Chong Chen, Yiqun Liu, and Shaoging Ma. 2020. Towards representation alignment and uniformity in collaborative filtering. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_. 1816-1825.
* Wang and Lin (2021) Feng Wang and Hauping Lin. 2021. Understanding the Behaviour of Contrastive Loss. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_. 2495-2504.
* Wang and Isola (2020) Tongzhou Wang and Phillip Isola. 2020. Understanding Contrastive Representation Learning through Alignment and Uniformity of Divergence. In _International Conference on Machine Learning (ICML)_. 9929-993.
* Wang et al. (2021) Wenjie Wang, Yiyan Xu, Fei Feng, Xinyu Lin, Xiangnan He, and Tat-Seng Chua. 2021. Diffusion Recommender Model. In _Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval_. 832-841.
* Wang et al. (2019) Xiang Wang, Xiangnan He, Meng Wang, Full Feng, and Tat-Seng Chua. 2019. Neural Graph Collaborative Filtering. In _Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval_. 165-174.
* Wang et al. (2021) Xiang Wang, Tinglin Huang, Dingxiang Wang, Yancheng Yuan, Zhenguang Lin, and Tai-Seng Chua. 2021. Learning intents behind interactions with knowledge graph for recommendation. In _Proceedings of the Web Conference 2021, 2021, 878-887.
* Wang et al. (2023) Tu Wang, Tuying Zhao, Yi Zhang, and Tyler Derr. 2023. Collaboration-aware graph convolutional network for recommender systems. In _Proceedings of the ACM Web Conference_. 2023, 91-101.
* Wang et al. (2021) Jiancan Wu, Xiang Wang, Yuli Feng, Xiangnan He, Liang Chen, Jianxuan Li, and Xing Xie. 2021. Self-Supervised Graph Learning for Recommendation. In _Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval_. 1117-1126.
* Wang et al. (2018) Yiang Wu, Le Zhang, Fengnan Ma, Tingyu Zhou, Weihli Ma, and Jian-Yan Nie. 2021. Unifying Graph Convolution and Contrastive Learning in Collaborative Filtering. In _Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_. 3425-3436.
* Wang et al. (2021) Liangyu Wang, Zhiwei Liu, Chen Wang, Mingdai Yang, Xiaolong Liu, Jing Ma, and Philip S. Yu. 2021. Graph-based alignment and uniformity for recommendation. In _Proceedings of the 32nd ACM International Conference on Information and Knowledge Management_. 495-499.
* Yang et al. (2009) Yonghui Yang, Zhengwei Wu, Le Wu, Kun Zhang, Richang Hong, Zhiqiang Zhang, Jun Zhou, and Meng Wang. 2009. Kinetics-Contractive Graph Learning for Recommendation. In _Proceedings of the 46th international ACM SIGIR Conference on Research and Development in Information Retrieval_. 1117-1126.
* Ying et al. (2018) Rex Ying, Ruining He, Kafeng Chen, Yong Boshkund, William L Hamilton, and Jure Leskovec. 2018. Graph Convolutional Neural Networks for Web-Scale Recommender Systems. In _Proceedings of the 42th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_. 974-986.
* Yu et al. (2018) Yuning Yu, Tianlong Chen, Yongzhou Su, Ting Chen, Zhangyang Wang, and Yang Shen. 2020. Graph Contrastive Learning with Applications. In _Advances in Neural Information Processing Systems (NeurIPS)_. 5812-5823.
* Yu et al. (2022) Juliang Yu, Hongqi Yin, Xin Xia, Tong Chen, Linheu Cui, and Quco Viet Hung Nguyen. 2022. Age graph augmentation necessary? simple graph contrastive learning for recommendation. In _Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval_. 1294-1305.
* Yu et al. (2023) Junliang Yu, Hongqi Yin, Xin Xia, Tong Chen, Jundong Li, and Zi Huang. 2023. Self-supervised learning for recommender systems: A survey. _IEEE Transactions on Knowledge and Data Engineering_ 36, 1 (2023), 3355-3355.
* Zhang et al. (2023) An Zhang, Leheng Sheng, Zhiko Cai, Xiang Wang, and Tat-Seng Chua. 2023. Empowering Collaborative Filtering with Prietopled Adversarial Contrastive Loss. In _Thirty-seventh Conference on Neural Information Processing Systems_.
* Zhang et al. (2024) Dan Zhang, Yangliao Geng, Wenwen Geng, Zhuang Qiu, Zhiyu Chen, Xing Tang, Ying Shan, Yuanjiao Ding, and Jeng Tang. 2024. ReckCL: Dual Contrastive Learning for Recommendation. In _Proceedings of the ACM on Web Conference_. 2024, 3655-3666.
* Zhang et al. (2018) Hongqi Zhang, Moustapha Cisse, Yuan N Dauphin, and David Lopez-Paz. 2018. mmixpv: Beyond Empirical Risk Minimization. In _International Conference on Learning Representations_.
* Zhang et al. (2024) Yi Zhang, Lei Sang, and Yuwen Zhang. 2024. Exploring the individuality and collectivity of intends behind interactions for graph collaborative filtering. In _Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval_. 1253-1262.
* Zhang et al. (2023) Yi Zhang, Yiwen Zhang, Dengcheng Yan, Shuiguang Deng, and Yun Yang. 2023. Revisiting Graph-based Recommender Systems from the Perspective of Variational Auto-encoder. _ACM Transactions on Information Systems (TOIS)_ 41, 3 (2023), 1-28.
* Zhang et al. (2024) Yi Zhang, Yiwen Zhang, Yuchuan Zhao, Shuiguang Deng, and Yun Yang. 2024. Dual Variational Graph Reconstruction Learning for Social Recommendation. _IEEE Transactions on Knowledge and Data Engineering_ 36, 11 (2024), 6002-6015.
* Zheng et al. (2021) Yu Zheng, Chen Gao, Xiang Li, Xiangnan He, Yong Li, and Dengcheng. 2021. Disentangling user interest and conformity for recommendation with causal embedding. In _Proceedings of the Web Conference 2021, 2808-2991.