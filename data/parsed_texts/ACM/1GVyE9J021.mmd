[MISSING_PAGE_EMPTY:1]

domains. Therefore, the main challenge of DA in EM is that we need to explicitly model the domain-private features so that the final common features contain as few domain-private properties as possible.

To address this challenge, we propose a new DA framework for EM called Matching Feature Separation Network (MFSN). As shown in Figure 1(c), MFSN can learn better common matching features by explicitly modeling the private matching features of each domain. We conduct various experiments on twelve transferring EM tasks, and the results indicate the effectiveness of our proposed model. The main contributions of this paper are as follows:

1. We propose a framework called Matching Feature Separation Network (MFSN). MFSN explicitly models the private and common matching features in the source and target domains by three encoders and transfers only the knowledge in common matching features.
2. We propose an enhanced variant of MFSN called Feature Representation and Separation Enhanced MFSN (MFSN-FRSE). Compared with MFSN, MFSN-FRSE has better feature representation and separation capabilities.
3. We evaluate the effectiveness of MFSN and MFSN-FRSE on twelve transferring EM tasks (six similar domain tasks and six different domain tasks). Our experimental results show that the MFSN-FRSE is approximately 7% higher in F1 score on average than the previous SOTA methods. Then, we verify the effectiveness of each module in MFSN and MFSN-FRSE by ablation study. Finally, we explore the optimal strategy of key modules in MFSN and MFSN-FRSE by detail tests.

The rest of this paper is organized as follows. Section 2 introduces the related work about EM, transfer learning, and transferring EM. Section 3 first defines the EM task and DA in EM, and then proposes a framework for DA in EM. Section 4 reports a series of comparative experiments. Section 6 concludes the paper.

## 2 Related Work

Early works on EM are devoted to designing various matching rules [2]. However, these methods lack universality, as no matching rules are suitable for all datasets. In recent years, DL-based EM methods have been extensively studied and have achieved the SOTA results. DeepMatcher [1] proposes a DL-based EM framework, which includes four solutions with varying representational power: SIF, RNN, Attention, and Hybrid. Ditto [5] applies pre-trained LMs to EM tasks, which achieves the SOTA results and reduces the number of training data. DL-based EM methods can automatically generate more expressive features and satisfy the end-to-end needs of real-world applications. However, DL-based methods still need a large amount of training data to achieve satisfying results.

Transfer learning (TL) refers to transferring knowledge learned in an old domain to a new domain by utilizing similarities between data, tasks, or models [12]. In the EM literature, only a few studies have focused on TL. Kirielle et al. [18] propose an instance-based method, called TransER. This method first selects source instances with similar features and neighborhoods to the target domain instances. Then, it uses selected source instances to train a classifier that can work on the target domain. However, this method uses attribute-based similarities to generate features, so it is only applicable to the structured dataset and can't be applied to the unstructured dataset. Tu et al. [10] propose a DA framework for EM, called DADER. This framework systematically explores the design space and compares different choices of DA for ER. Some methods in DADER achieve the current SOTA results. However, these methods in DADER directly learn common features through statistics metrics or adversarial training. By disregarding the explicit modeling of domain-private features, it can't be guaranteed that the final common features contain as few domain-private properties as possible. Sun et al. [19] propose a DSN-based method called VAER-DSN. VAER-DSN uses gated recurrent units (GRU) and variational auto-encoders (VAE) as the basic components to learn the private and common features. However, the pre-trained LMs are not considered as the underlying models. As mentioned earlier, they have general natural language comprehension capabilities, therefore they can be a good starting point to help the models quickly adapt to a new task. Thus, in the next section, we will utilize the pre-trained LMs to solve the DA in EM.

## 3 Matching Feature Separation Network for Domain Adaptation in Entity Matching

### Task Definitions

Entity matching (EM) aims to determine whether two records from different data sources refer to the same entity [1]. Let \(D_{1}\) and \(D_{2}\) be two collections of entity records with multiple attributes. Each record \(r_{1}\in D_{1}(\text{or}\,r_{2}\in D_{2})\) is a set of key-value pairs \(\{attr_{i},val_{i}\}_{1\leq i\leq k}\), where \(attr_{i}\) and \(val_{i}\) denote the \(i\)-th attribute name and attribute value respectively. EM aims to determine whether \(r_{1}\) and \(r_{2}\) refer to the same real-world entity or not. A typical EM pipeline consists of two steps: blocking and matching. The blocking step generates a candidate set \(Cnd\subset D_{1}\times D_{2}\) with a high recall by removing unnecessary comparisons. The subsequent matching step only needs to determine whether the candidate pair \((r_{1},r_{2})\in Cnd\) match or not. The DL-based EM method first defines an EM model \(\mathcal{M}\),which takes the candidate pair \((r_{1},r_{2})\) as input and outputs a prediction \(\mathcal{Y}\).

\[\mathcal{Y}=\mathcal{M}(r_{1},r_{2}) \tag{1}\]

Figure 1: Domain adaptation (DA) for entity matching (EM).

[MISSING_PAGE_EMPTY:3]

#### 3.3.2 Similarity Loss

The similarity loss \(L_{\text{sim}}\) can effectively measure the discrepancy between the feature distributions of \(MF_{C}^{S}\) and \(MF_{C}^{T}\).

\[L_{\text{sim}}=\text{Distribution\_Discrepancy}(MF_{C}^{S},MF_{C}^{T}) \tag{7}\]

By continuously minimizing \(L_{\text{sim}}\), the distributions of \(MF_{C}^{S}\) and \(MF_{C}^{T}\) are becoming more and more similar. In this paper, we propose three methods to implement \(L_{\text{sim}}\), denoted as MMD loss [13], CORAL loss [14], and GRL loss [15].

_MMD Loss_. The MMD loss is based on Maximum Mean Discrepancy (MMD) [13]. Briefly, MMD maps the distributions into the reproducing kernel Hilbert space (RKHS) by using a kernel function. Then MMD takes the distance between the mean embeddings of the two distributions in the RKHS as the discrepancy between them:

\[\begin{split} L_{\text{sim}}^{\text{MMD}}&=\sup_{ \|\varphi\|_{H^{S}}\leq 1}\left\|\mathbb{E}_{MF_{C}^{S}\leftarrow\text{Enc}_{C}(D^{ S})}\middle[\varphi(MF_{C}^{S})\right]-\\ &\mathbb{E}_{MF_{C}^{T}\leftarrow\text{Enc}_{C}(D^{T})}\middle[ \varphi(MF_{C}^{T})]\right\|_{H}^{2}\end{split} \tag{8}\]

\(\varphi(\cdot)\) represents a kernel function that maps \(MF_{C}^{S}\) (\(MF_{C}^{T}\)) to an RKHS, and \(\|\varphi\|_{H}\leq 1\) defines a set of functions in the unit ball of RKHS. \(L_{\text{sim}}^{\text{MMD}}=0\) if and only if the distributions of \(MF_{C}^{S}\) and \(MF_{C}^{T}\) are the same.

_CORAL Loss_. The CORAL loss is based on CORrelation Alignment [14], which is a particular case of \(k\)-order where \(k=2\). It measures the discrepancy between two distributions by computing the difference between their covariance matrices (second-order statistics). The CORAL loss is defined as Equation (9).

\[L_{\text{sim}}^{\text{Coral}}=\frac{1}{4d^{2}}\left\|\text{cov}\big{(}\mathbf{ MF}_{C}^{S}\big{)}-\text{cov}(\mathbf{MF}_{C}^{T})\right\|_{F}^{2} \tag{9}\]

\(\mathbf{MF}_{C}^{S}(\mathbf{MF}_{C}^{T})\in\mathbb{R}^{n\times d}\) is the common matching feature matrix of the source (target) domain, where \(d\) is the dimensionality of \(MF_{C}\) and \(n\) is the number of source (target) samples. The function cov(\(\cdot\)) is used to compute the covariance matrix for a given matching features matrix. \(\|\cdot\|_{F}^{2}\) denotes the squared matrix Frobenius norm.

_GRL Loss_. The GRL loss is based on adversarial training [15]. We first introduce a Discriminator to determine whether the \(MF_{C}\) are from the source or target domain, as shown in Equation (10). Then, the Discriminator and Enc\({}_{C}\) are trained in an adversarial manner: the Enc\({}_{C}\) aims to maximize the domain classification error, while the Discriminator aims to minimize it. The loss function is shown in Equation (11), and the adversarial training is implemented by adding a gradient reversal layer (GRL) [15] between the Enc\({}_{C}\) and Discriminator:

\[d=\text{Discriminator }(MF_{C}) \tag{10}\]

\[L_{\text{sim}}^{\text{GRL}}=\sum_{i=0}^{N_{S}+N_{T}}\left\{d_{i}\log\hat{d}_{i }+(1-d_{i})\log(1-\hat{d}_{i})\right\} \tag{11}\]

\(N_{S}\) and \(N_{T}\) denote numbers of common matching features in the source and target domains, respectively, and \(d_{i}\in\{0,1\}\) denotes the domain label for \(MF_{C}\).

#### 3.3.3 Difference Loss

The difference loss [16] can help the private and shared encoders to encode different aspects of the input. Given a source or target domain candidate pair (\(r_{1}\),\(r_{2}\)), we first use the corresponding private and shared encoder to obtain \(MF_{P}\) and \(MF_{C}\). The difference loss is defined as Equation (12).

\[L_{\text{diff}}=\left\|MF_{P}\top MF_{C}\right\|_{F}^{2} \tag{12}\]

\(\|\cdot\|_{F}^{2}\) denotes the squared matrix Frobenius norm. Minimizing \(L_{\text{diff}}\) can help the shared and private encoders generate mutually orthogonal features.

### Decoder for Reconstruction

The decoder Dec uses both common and private matching features to reconstruct the original input pairs. As shown in Figure 2, given a source or target domain candidate pair (\(r_{1}\),\(r_{2}\)), we first use the corresponding private and shared encoder to obtain \(MF_{P}\) and \(MF_{C}\), respectively. Then \(MF_{P}\) and \(MF_{C}\) are summed up and fed into the Dec for decoding:

\[(\hat{r}_{1},\hat{r}_{2})=\text{Dec}(MF_{C}+MF_{P}) \tag{13}\]

We evaluate the effectiveness of the two encoders by comparing the discrepancy between \((\hat{r}_{1},\hat{r}_{2})\) and \((r_{1},r_{2})\). Finally, we use the discrepancy as the reconstruction loss to further optimize the model. The reconstruction loss is defined as:

\[\begin{split} L_{\text{recon}}&=\text{Record\_Discrepancy} \big{(}(\hat{r}_{1},\hat{r}_{2}),(r_{1},r_{2})\big{)}\\ &=\sum_{i=1}^{m}\text{CE}(\hat{t}_{i},\hat{t}_{i})\end{split} \tag{14}\]

The function CE(\(\cdot\)) is the Cross-Entropy loss function. The \(t_{i}\) denotes the \(i\)-th token in candidate pairs sequence S(\(r_{1}\),\(r_{2}\)) which is obtained from \((r_{1},r_{2})\) by Equation (3). The \(\hat{t}_{i}\) denotes the \(i\)-th token in S(\(\hat{r}_{1}\),\(\hat{r}_{2}\)), and \(m\) denotes the length of S(\(r_{1}\),\(r_{2}\)).

For the choice of decoder architecture, we can't directly use the vanilla Transformer-Decoder architecture [21]. As shown in Equation (15), if the encoder only provides a single vector \(MF\) for decoding, the decoder's CrossAttention layer always outputs the same value for any query vector \(q\), which comes from the previous masked self-attention layer.

\[MF^{T}\mathbf{W}_{V}=\text{CrossAttention}(q,MF^{T}\mathbf{W}_{K},MF^{T} \mathbf{W}_{V}) \tag{15}\]

\(\mathbf{W}_{K}\) and \(\mathbf{W}_{V}\) are key and value matrices. To avoid the above situation, we add a gate mechanism [22] to the vanilla Transformer-Decoder: Suppose \(q_{t}\) is the \(t\)-th query vector, and the output \(o_{t}\) of the CrossAttention layer is:

\[o_{t}=\sigma(q_{t}\mathbf{W}_{1}+MF\mathbf{W}_{2})\bigodot MF^{T}\mathbf{W}_{V} \tag{16}\]

The function \(\sigma(\cdot)\) is the sigmoid activation function, \(\mathbf{W}_{1}\) and \(\mathbf{W}_{2}\) are two learnable parameter matrices, \(\mathbf{W}_{V}\) is value matrix, and \(\bigodot\) denotes Hadamard product.

### Entity Matching Classifier and Objective Function

EM classifier can make matching decisions based on \(MF_{C}\).

Matching Feature Separation Network for Domain Adaptation in Entity Matching

\[\mathcal{Y}=\text{Classifier}(MF_{C}) \tag{17}\]

We utilize the \(MF_{C}^{\mathcal{S}}\) to train an EM classifier, while the EM loss is defined as Equation (18).

\[L_{EM}=\sum_{l=0}^{N}y_{i}\log\mathcal{Y}_{l}+(1-y_{i})\log(1-\mathcal{Y}_{l}) \tag{18}\]

\(N\) denotes the number of candidate pairs in the source domain, \(\mathcal{Y}_{i}\) denotes the label for the \(i\)-th pairs, and \(\mathcal{Y}_{l}\) is the prediction made by EM classifier for it.

The final objective function consists of EM loss, reconstruction loss, difference loss, and similarity loss, as shown in Equation (19). Our training goal is to minimize this objective function.

\[L=L_{EM}+aL_{\text{recon}}+\beta L_{\text{diff}}+\gamma L_{\text{sim}} \tag{19}\]

\(L_{\text{EM}}\) comes from Section 3.5, \(L_{\text{recon}}\) comes from Section 3.4, \(L_{\text{diff}}\) and \(L_{\text{sim}}\) comes from Section 3.3.

### Feature Representation and Separation Enhancement

In this section, we propose an enhanced variant called Feature Representation and Separation Enhanced MFSN (MFSN-FRSE). First of all, to enhance the feature representation capability, we propose an enhanced encoder to obtain the hidden representation of all tokens. Then, the difference loss is computed in a "token-by-token" manner, the similarity loss is computed with the help of DomAtt. Lastly, the decoder takes all tokens' private and common features as input to reconstruct the original input pairs. In MFSN-FRSE, we mainly improve the encoder and decoder modules, while the other modules are the same as MFSN.

#### 3.6.1 Enhanced Encoder

Recall that the encoder in MFSN simply uses the hidden representation of [CLS] as \(MF\), which is suitable for classification tasks. However, TL tasks often require a higher feature representation capability of the model. Therefore, to improve the feature representation capability, we propose an enhanced encoder EEnc. Specifically, it uses BERT to obtain the hidden representations of all tokens in (\(r_{1},r_{2}\)), and combine them to form a hidden representation matrix \(\mathbf{H}\):

\[\mathbf{H}=\text{EEnc}(r_{1},r_{2}) \tag{20}\]

The hidden representation matrix \(\mathbf{H}\in\mathbb{R}^{m\times d}\), \(m\) is the number of tokens in candidate pairs sequence \(\text{S}(r_{1},r_{2})\) obtained from (\(r_{1},r_{2}\)) by Equation (3), and \(d\) is the dimensionality of the BERT's output. As shown in Figure 3 and Figure 4, we next use a token sequence-based method to compute the difference loss and similarity loss.

#### Enhanced Similarity Loss

The enhanced similarity loss can help the enhanced shared encoder EEnc\({}_{C}\) learn similar common features from the source and target domain. We introduce a pooling layer Pool to efficiently compute the similarity loss between two hidden representation matrices. As shown in Figure 3, we first obtain the common hidden representation matrices \(\mathbf{H}_{C}^{S}\) and \(\mathbf{H}_{C}^{T}\) of (\(r_{1}^{S},r_{2}^{S}\)) and (\(r_{1}^{T},r_{2}^{T}\)) by the EEnc\({}_{C}\). Next, we use the Pool to learn a domain-aware matching feature vector from the \(\mathbf{H}_{C}^{S}\) and \(\mathbf{H}_{C}^{T}\), respectively.

\[h_{C}^{S},h_{C}^{T}=\text{Pool}(\mathbf{H}_{C}^{S}),\text{Pool}(\mathbf{H}_{C }^{T}) \tag{21}\]

The similarity loss of \(h_{C}^{S}\) and \(h_{C}^{T}\) is then computed to ensure that their feature distributions are similar. The similarity loss between \(h_{C}^{S}\) and \(h_{C}^{T}\) can be computed in three ways described in Section 3.3.

However, conventional pooling strategies (such as using special tokens or averaging operations) can't accurately capture the domain information of the sequence. So, we propose a DomAtt mechanism, which is based on self-Attention [21]. As shown in Equation (22)-(23), the DomAtt takes a hidden representation matrix as the key and the value, while a learnable domain-shared vector \(a\) as the query.

\[h_{C}^{T}=\text{softmax}\left(\frac{a^{T}\mathbf{W}^{Q}(\mathbf{H}_{C}^{T} \mathbf{W}^{K})^{T}}{\sqrt{d}}\right)\mathbf{H}_{C}^{T}\mathbf{W}^{V} \tag{22}\]

\[h_{C}^{S}=\text{softmax}\left(\frac{a^{S}\mathbf{W}^{Q}(\mathbf{H}_{C}^{T} \mathbf{W}^{K})^{T}}{\sqrt{d}}\right)\mathbf{H}_{C}^{S}\mathbf{W}^{V} \tag{23}\]

The learnable parameter matrices \(\mathbf{W}^{Q},\mathbf{W}^{K},\mathbf{W}^{V}\in\mathbb{R}^{d\times d}\), and \(d\) is the dimensionality of the input features. For the technical details of the self-attention mechanism, please refer to [21].

#### Enhanced Difference Loss

The enhanced difference loss can help the enhanced private encoder EEnc\({}_{P}\) and enhanced

Figure 4. The way to compute the enhanced difference loss, in MFSN-FRSE.

Figure 3. The way to compute the enhanced similarity loss, in MFSN-FRSE. \(a^{S}\) and \(a^{T}\) are two learnable domain-shared vectors in the source and target domains, respectively.

shared encoder \(\text{EEnc}_{C}\) to encode different aspects of the input. As shown in Figure 4, Given a source or target domain candidate pair \((r_{1},r_{2})\), we first obtain the private hidden representation matrix \(\mathbf{H}_{C}\) and common hidden representation matrix \(\mathbf{H}_{p}\), respectively. Then, we adopt a token-by-token manner to compute the difference loss:

\[L_{\text{diff}}^{\text{enh}}=\sum_{i=1}^{m}\|\mathbf{H}_{C}[i]^{\top}\mathbf{H }_{p}[i]\|_{2}^{2} \tag{24}\]

\(\mathbf{H}_{C}[i]\) (\(\mathbf{H}_{p}[i]\)) denotes the \(i\)-th row of the hidden representation matrix \(\mathbf{H}_{C}\) (\(\mathbf{H}_{p}\)), which corresponds to the hidden representation of the \(i\)-th token in the S(\(r_{1},r_{2}\)) obtained from (\(r_{1},r_{2}\)) by Equation (3). \(m\) denotes the length of S(\(r_{1},r_{2}\)).

#### 3.6.2 Enhanced Decoder

Decoder Dec takes \(\mathbf{H}_{c}\) and \(\mathbf{H}_{p}\) as inputs to reconstruct the original input candidate pairs. As mentioned before, it can help the \(\text{EEnc}_{C}\) and \(\text{EEnc}_{p}\) to learn more effective features. Specifically, given a source or target domain candidate pair \((r_{1},r_{2})\), we first use the corresponding \(\text{EEnc}_{C}\) and \(\text{EEnc}_{p}\) to obtain the \(\mathbf{H}_{p}\) and \(\mathbf{H}_{c}\). Then the \(\mathbf{H}_{p}\) and \(\mathbf{H}_{c}\) are summed up and fed into the Dec for decoding:

\[(\hat{r}_{1},\hat{r}_{2})=\text{Dec}(\mathbf{H}_{p}+\mathbf{H}_{C}) \tag{25}\]

The reconstruction loss can be computed using Equation (14) in Section 3.4. For the architecture of Dec, we use a single layer of Transformer-Decoder.

## 4 Experimental Evaluation

### Experiment Setup

#### 4.1.1 Datasets

Table 1 displays the statistical information of all datasets used in the experiment. The first nine datasets are obtained from DeepMatcher [1] and cover a wide range of domains such as products, citations, and restaurants. The latter four datasets are obtained from the WDC product dataset [23]. This dataset collects data from multiple ecommerce sites and categorizes them into four categories: computers, watches, shoes, and cameras. Each category has 1,100 labeled candidate pairs. Finally, we denote a DA task by \(D^{S}\to D^{T}\), where \(D^{S}\) is the source dataset and \(D^{T}\) is the target dataset.

#### 4.1.2 Baselines

To demonstrate the effectiveness of our model, we set following baselines:

_NoDA._ NoDA uses the pre-trained LMs as an encoder to learn matching features from the input candidate pair. Then, it uses a classifier to make matching decisions based on the learned features. Notice that NoDA doesn't use any DA.

_VAER-DSN_[19]. VAER-DSN is based on the DSN model. VAER-DSN uses GRU and VAE as the basic components to learn the private and common features. The classifier in VAER-DSN can make matching decisions by using the common features. The decoder can reconstruct the learned features back to the encoder input.

_DADER_[10]. DADER is a famous framework of DA for EM, and describes six representative methods: MMD, K-order, GRL, InvGAN, InvGAN+KD, and ED. The experimental results show that: 1) the methods based on pre-trained LMs often achieve the best results. 2) the results of ED are even worse than NoDA in most cases. Therefore, we use MMD, K-order, GRL, InvGAN, and InvGAN+KD as the baselines in our experiments, and all five methods adopt the pre-trained LMs.

In the subsequent experiments, we use MFN-basic to represent the basic model introduced in Section 3.2-3.5. MFN-basic-MMD, MFN-basic-CORAL, and MFN-basic-GRL represent MFN-basic with three different similarity losses: MMD loss, CORAL loss, and GRL loss, respectively. Similarly, MFN-FRSE is used to represent the enhanced variant introduced in Section3.6. MFN-FRSE-MMD, MFN-FRSE-CORAL, and MFN-FRSE-GRL represent MFN-FRSE with three different similarity losses, respectively.

#### 4.1.3 Evaluation Metric and Experiment Settings

Following most EM works [1, 5, 10], we use precision, recall, and F1 as the evaluation metrics. Specifically, precision = [TP] / ([TP] + |FP]), recall = |TP] / ([TP] + |FN]), and F1 = 2 \(\cdot\) precision \(\cdot\) recall / (precision + recall). TP denotes true positives, FP denotes false positives, FP denotes false negatives, and |\(\cdot\)| denotes the cardinal number of a set.

All experiments are implemented using Python. In all methods, the batch size is set to 32. The pre-trained LMs are uniformly using "distilbert-base-uncased" [8]. We used a server with NVIDIA GeForce RTX 3090 GPU for the experiments All experiments are repeated three times, and the average results are reported.

### Main Results

The effect of DA may depend on the discrepancy levels between the source and target datasets, so we classify the tasks into two categories: similar domain tasks (selecting source and target domain datasets from the same domain, e.g., \(\text{FZ}\to\text{DZY}\)) and different domain tasks (selecting source and target datasets from different domains, e.g., \(\text{B2}\to\text{FZ}\)). The experimental results are shown in Table 2.

From the overall perspective, the best method is MFN-FRSE-MMD, followed by MFN-FRSE-GRL. They significantly outperform NoDA in both task settings, which shows the effectiveness of our proposed method. Among all methods, VAER-DSN usually achieves the worst results. The main reason is

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Dataset & Domain & \#Pairs & \#Matches & \#Attrs \\ \hline Walmart-Amazon (WA) & Product & 10,242 & 962 & 5 \\ Alt-Buy (AB) & Product & 9,575 & 1,028 & 3 \\ DBLP-Scholar (DS) & Citation & 28,707 & 5,347 & 4 \\ DBLP-ACM (DA) & Citation & 12,363 & 2,220 & 4 \\ Fodor-Zagats (FZ) & Restaurant & 946 & 110 & 6 \\ Zomato-Yeb (DZY) & Restaurant & 994 & 214 & 3 \\ Times-Amazon (IA) & Music & 532 & 132 & 8 \\ RottenTomotes-IMDB & Movies & 600 & 190 & 3 \\ Books2 (B2) & Books & 394 & 92 & 9 \\ WDC-Computers (CO) & Product & 1,100 & 300 & 2 \\ WDC-Cameras (CA) & Product & 1,100 & 300 & 2 \\ WDC-Watches(WT) & Product & 1,100 & 300 & 2 \\ WDC-Shoes (SH) & Product & 1,100 & 300 & 2 \\ \hline \hline \end{tabular}
\end{table}
Table 1: Datasets used in our experiments. #Pairs, #Matches, and #Attrs represent the numbers of entity pairs, matching pairs, and attributes, respectively.

that VAER-DSN doesn't use the Transformer-Encoder-based pre-trained LMs as its underlying model. As mentioned before, these pre-trained LMs not only have powerful feature representation capabilities but also have general comprehension capabilities. Therefore, for DA in EM, it is necessary to use Transformer-Encoder-based pre-trained LMs as the underlying model.

Compared with DADER, MFSN-FRSE-MMD have higher average F1 scores in both similar and different domain tasks. Specifically, in similar domain tasks, MFSN-FRSE-MMD shows an average improvement of 2.08%. In different domain tasks, the improvement is even more significant at 8.42%. This indicates that MFSN-basic and MFSN-FRSE have more advantages in different domain tasks. Compared with similar domain tasks, the different domain tasks may have more domain-private features. Methods in DADER don't explicitly model or process these domain-private features. As a result, the learned domain-invariant features are not clean (may contain some domain-private features), which may affect the models' performances. MFSN-basic and MFSN-FRSE can separate the private features from the common features. Thus, our proposed models can achieve better DA performances.

We can find that the performance of MFSN-FRSE is generally better than MFSN-basic. In some cases, MFSN is even worse than NoDA. The possible reason is that the encoder in MFSN-basic has limited feature representation capability. This assumption can be confirmed by subsequent ablation studies. At last, the average F1 score of MMD loss is better than that of both CORAL loss and GRL loss. Therefore, we chose the MMD loss as the default similarity loss for subsequent experiments.

### Visualization Analysis of Transferring Effect

To further analyze the effect of DA, we use t-SNE (Maaten and Hinton, 2008) to map the matching features learned from the source and target datasets into a two-dimensional space. Due to the limitation of space, we only show three representative cases. The top of Figure 5 shows the matching feature distribution obtained by NoDA, and the bottom shows the matching feature distribution obtained by MFSN-FRSE. Compared with NoDA, MFSN-FRSE generates more similar feature distributions for the source and target domain, which help the entity matching classifier make correct predictions on the target dataset.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c c c c} \hline \hline  & & \multicolumn{6}{c|}{Similar domain tasks} & \multicolumn{6}{c}{Difference domain tasks} \\  & F2 & D2Y & SH & CA & CO & WA & Avg- & B2 & B2 & RI & RI & IA & IA & Avg- & Avg- \\  & l & l & l & l & l & l & sim & l & l & l & l & l & l & l & dif & all \\  & D2Y & F2 & CA & CO & WA & SH & F2 & D2Y & WA1 & AB & DA & DS & & & & \\ \hline NoDA & 6.27 & 82.87 & 50.75 & 69.13 & 72.85 & 65.77 & 57.84 & 49.51 & 29.55 & 18.28 & 19.34 & 74.42 & 56.15 & 41.21 & 49.52 \\ \hline VAER-ISN & 32.59 & 15.89 & 37.31 & 44.55 & 43.01 & 46.14 & 35.58 & 26.94 & 40.47 & 19.24 & 20.50 & 52.57 & 42.71 & 33.74 & 35.16 \\ \hline  & MMD & 53.19 & 66.98 & 64.03 & 70.08 & 74.76 & 68.77 & 66.30 & 24.51 & 13.31 & 18.85 & 22.61 & 91.52 & 85.22 & 42.67 & 54.49 \\  & CORAL & 75.37 & 75.91 & 64.51 & 69.30 & 68.44 & 70.67 & 70.70 & 74.27 & 54.23 & 19.81 & 19.72 & 79.33 & 58.41 & 50.96 & 60.83 \\ DADER & GRL & 23.98 & 83.16 & 59.34 & 70.45 & 69.13 & 63.43 & 61.58 & 62.57 & 46.18 & **34.03** & **28.46** & 85.64 & 70.24 & 54.52 & 58.05 \\  & ImGAN & 28.68 & 91.10 & 57.73 & 68.60 & 67.86 & 67.10 & 63.51 & 63.25 & 44.16 & 23.83 & 21.62 & 85.97 & 69.95 & 51.46 & 57.49 \\  & ImGAN+KD & 19.18 & 90.24 & 64.61 & 68.51 & 25.01 & 71.91 & 64.99 & 62.70 & 36.49 & 25.56 & 23.27 & 87.35 & 71.75 & 51.19 & 58.09 \\ \hline  & MMD & 46.74 & 80.91 & 62.93 & 72.43 & 74.67 & 70.08 & 67.96 & 69.12 & 42.76 & 22.06 & 25.32 & 91.52 & **86.63** & 56.24 & 62.10 \\ MESN-basic & CORAL & 68.47 & 53.20 & 64.18 & 70.30 & 72.25 & 68.49 & 66.15 & 52.65 & 51.04 & 20.11 & 22.29 & 89.05 & 78.47 & 52.27 & 59.21 \\  & GRL & 29.06 & 78.27 & 62.30 & 72.32 & 72.01 & 73.05 & 64.58 & 84.45 & 37.47 & 29.20 & 25.56 & **91.98** & 76.20 & 57.48 & 60.99 \\ \hline  & MMD & 58.58 & **91.67** & 67.24 & 70.16 & 76.44 & 72.56 & **72.78** & **90.00** & **60.40** & 25.01 & 25.95 & 90.44 & 85.86 & **62.94** & **67.86** \\ MFSN-FRSE & CORAL & **86.12** & 58.54 & 66.05 & 68.50 & 74.09 & 67.60 & 70.15 & 51.94 & 56.70 & 28.08 & 25.26 & 87.91 & 78.40 & 54.72 & 62.43 \\  & GRL & 50.67 & 87.54 & **68.04** & **72.95** & **78.03** & **73.61** & 71.81 & 87.72 & 50.84 & 30.95 & 25.93 & 91.10 & 82.80 & 61.56 & 66.68 \\ \hline \hline \end{tabular}
\end{table}
Table 2. F1 score on similar and different domain tasks. Bold, single underline, and double underline indicate the best, second, and third values, respectively. “Avg-sim” represents the average for tasks in six similar domains. “Avg-all” represents the overall average across all 12 tasks.

Figure 5. DA Visualization of MFSN-FRSE. Distributions of source (red) and target (blue) are much closer in MFSN-FRSE than NoDA.

### Ablation Study

Next, we analyze the effectiveness of difference loss, similarity loss, and decoder by ablation study. The results are shown in Table 3-4.

We can see that if the decoder is removed, the MFSN-FRSE's performance will decrease. This indicates that the decoder can help the model to learn more effective features. However, if the decoder is removed, the MFSN-basic's performance will increase. The main reason is that the encoders in MFSN-basic have limited representation capability. These encoders only use the hidden representation of [CLS] as the matching feature, which can't simultaneously contain the information for matching decisions and reconstruction tasks.

In most tasks, if the difference loss is removed, the MFSN-FRSE's performance will also decrease. Introducing the difference loss can encourage the model to separate the private features from the common feature, leading to better DA performance. However, in RI \(\rightarrow\) WA1 and RI \(\rightarrow\) AB, if the difference loss is removed, the performance of MFSN-FRSE will increase. The main reason is that these two tasks have too few common matching features between the source and target domains. That is not enough to make a correct matching decision. We will try to tackle this issue in our future work.

In most tasks, if the similarity loss is removed, the model's performance will decrease significantly. Introducing the similarity loss can ensure the distributions of the source and target common matching features are similar. Therefore, the entity matching classifier can make correct decisions in both the source and target domains. We can also see that on the WDC dataset, introducing the similarity loss doesn't bring a great improvement. The possible reason is that the data distribution between the different WDC datasets is very similar [10].

To explore the optimal strategies for key modules in MFSN and MFSN-FRSE, we conduct a series of detailed tests in the appendix. The results show that the optimal similarity loss is the MMD loss, and the optimal pooling layer is the DomAttion mechanism. For the reconstruction and computing difference loss, the optimal strategy is to use the hidden representation of all tokens.

## 5 Conclusion

We propose a framework for DA in EM called **Matching Feature Separation Network** (MFSN). Briefly, MFSN achieves good DA performance by explicitly modeling domain-specific features. It utilizes three Pre-LMs based encoders to learn the private and common matching features of the source and target domains. The difference loss can make the common and private matching features mutually orthogonal. The similarity loss can make the distributions of the both common matching features are similar. We also propose an enhanced variant called **F**eature **R**epresentation and **S**eparation **E**nhanced MFSN (MFSN-FRSE). Compared with MFSN, it has better feature representation and separation capabilities. It utilizes three enhanced encoders to learn more expressive private and common hidden representation matrices of both domains. Then, the difference loss is computed in a "token-by-token" manner, and the similarity loss is computed with the help of DomAtt. The experiment results show that our framework outperforms the previous SOTA methods. we verify the effectiveness of each module by ablation study. Finally, we explore the optimal strategy of each module by detailed tests.

## Acknowledgments

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & complete & \begin{tabular}{c} w/o \\ decoder \\ \end{tabular} & \begin{tabular}{c} w/o \\ difference \\ \end{tabular} & 
\begin{tabular}{c} w/o \\ similarity \\ \end{tabular} \\ \hline B2-FZ & **90.00** & 79.90 & 84.24 & 45.19 \\ B2-D2Y & **60.40** & 50.11 & 37.94 & 56.84 \\ RI-WA1 & 25.01 & 23.23 & **28.08** & 19.57 \\ RI-AB & 25.95 & 25.31 & **26.22** & 21.28 \\ IA-DA & **90.44** & 90.34 & 89.53 & 63.64 \\ IA-DS & **84.86** & 84.83 & 84.02 & 57.74 \\ FZ-D2Y & **58.58** & 57.02 & 56.32 & 2.05 \\ DZY-FZ & **91.67** & 90.24 & 88.35 & 84.52 \\ SH-CA & **67.24** & 64.30 & 66.71 & 57.04 \\ CA-CO & 70.16 & **73.22** & 70.19 & 70.22 \\ CO-WA & 76.44 & 73.31 & 73.29 & **77.71** \\ WA-SH & **72.56** & 70.33 & 70.90 & 70.17 \\ \hline average & **67.78** & 65.18 & 64.65 & 52.16 \\ \hline \hline \end{tabular}
\end{table}
Table 4: Ablation test for MFSN-FRSE. Complete represents the complete model. **W/o** decoder, **w/o** difference, and **w/o** similarity represent removing the decoder, difference loss, and similarity loss, respectively. **Bold indicates the best value**.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline  & complete & \begin{tabular}{c} w/o \\ decoder \\ \end{tabular} & \begin{tabular}{c} w/o \\ difference \\ \end{tabular} & 
\begin{tabular}{c} w/o \\ similarity \\ \end{tabular} \\ \hline B2-FZ & 69.12 & 67.69 & 65.37 & **72.07** \\ B2-D2Y & 42.76 & **53.10** & 34.08 & 45.29 \\ RI-WA1 & 22.06 & 18.24 & **23.93** & 23.06 \\ RI-AB & **25.32** & 24.41 & 22.57 & 23.78 \\ IA-DA & 91.52 & 92.32 & **92.81** & 73.08 \\ IA-DS & 86.63 & **87.16** & 87.05 & 66.19 \\ FZ-D2Y & 47.74 & **55.88** & 54.20 & 52.43 \\ DZY-FZ & 80.91 & 80.99 & **86.33** & 71.78 \\ SH-CA & 62.93 & 63.47 & **66.85** & 63.96 \\ CA-CO & **72.43** & 70.88 & 69.92 & 70.33 \\ CO-WA & 74.67 & 72.31 & 72.76 & **74.70** \\ WA-SH & 70.08 & 67.77 & **70.84** & 70.25 \\ \hline average & 62.18 & **62.85** & 62.23 & 58.91 \\ \hline \hline \end{tabular}
\end{table}
Table 3: Ablation test for MFSN-basic. Complete represents the complete model. **W/o** decoder, **w/o** difference, and **w/o** similarity represent removing the decoder, difference loss, and similarity loss, respectively. **Bold indicates the best value**.

## References

* S. Mudgal, H. Li, T. Rekatsinas, A. Doan, Y. Park, G. Krishnan, R. Deep, E. Arcuate, and V. Raghavendra (2018)Deep learning for entity matching: a design space exploration. In Proceedings of the 2018 International Conference on Management of Data (SIGMOD '18), pp. 38-91. External Links: ISBN 978-1-5386-2045-3, Link, Document Cited by: SS1.
* W. Fan, H. Gao, X. Jia, J. Li, and S. Ma (2011)Dynamic constraints for record matching. The VLDB Journal 20 (August 2011), pp. 495-520. External Links: Document, Link, [https://doi.org/10.1007/s00778-1-012-026-6](https://doi.org/10.1007/s00778-1-012-026-6) Cited by: SS1.
* A. Doan, P. Konda, P. Suganthan G. C., Y. Govind, D. Paulsen, K. Chandrasekhar, P. Martius, and M. Christie (2020)MapELI: toward building ecosystems of entity matching solutions. Commun. ACM63 (8). External Links: ISSN 0018-0702, Link, Document Cited by: SS1.
* M. Bruben, S. Thirumuruganathan, S. Joy, M. Ouzzani, and N. Tang (2018)Distributed representations of tuples for entity resolution. Proc. VLDB Endow.11 (11). External Links: ISSN 1454-1477, Link, Document Cited by: SS1.
* Y. Li, J. Li, Y. Suhara, J. Wang, W. Hirota, and W. Chievo (2021)Deep entity matching: challenges and opportunities. J. Data and Information Quality13 (1), pp. 17 pages. External Links: ISSN 1454-1477, Link, Document Cited by: SS1.
* U. Brunner and K. Stockinger (2020)Entity matching with transformers: are architectures- a step forward in data integration. In 23rd international Conference on Extending Database Technology, Copenhagen, 30 March-2 April 2020, pp. 463-473. Cited by: SS1.
* J. Devlin, M. Chang, K. Lee, and K. Toutanova (2018)Bert: pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805. Cited by: SS1.
* V. Sankl, I. Debut, J. Chaumond, and T. Wolf (2019)DistillBERT: a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108. Cited by: SS1.
* Y. Liu, M. Ott, N. Agoval, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov (2019)Roberta: a robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. Cited by: SS1.
* J. Tu, J. Fan, N. Tang, P. Wang, C. Chai, G. Li, R. Fan, and X. Du (2022)Domain adaptation for deep entity resolution. In Proceedings of the 2022 International Conference on Management of Data, SIGMOD '22, New York, NY, USA, pp. 443-457. External Links: ISBN 978-1-4503-6949-6, Link, Document Cited by: SS1.
* M. Trabelsi, J. Heflin, and J. Cao (2022)DAME: domain adaptation for matching entities. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM '22), New York, NY, USA, pp. 1016-1024. External Links: ISBN 978-1-4503-6949-6, Link, Document Cited by: SS1.
* S. Jain Pan and Q. Yang (2009)A survey on transfer learning. IEEE Transactions on knowledge and data engineering22 (10), pp. 1345-1359. Cited by: SS1.
* E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell (2014)Deep domain confusion: maximizing for domain invariance. arXiv preprint arXiv:1412.3474. Cited by: SS1.
* B. Sun and K. Saenko (2016)Deep coral: correlation alignment for deep domain adaptation. In Computer Vision-ECCV 2016 Workshops: Amsterdam, The Netherlands, October 8-10 and 15-16, Proceedings, Part III, pp. 443-450. Cited by: SS1.
* Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V. Lempitsky (2016)Domain-adversarial training of neural networks. J. Mach. Learn. Res.17 (1). External Links: ISSN 1454-1477, Link, Document Cited by: SS1.
* L. Gondelman, J. Kastberg Hinrichsen, M. Pereira, A. Timany, and L. Birkedal (2023)Verifying reliable network components in a distributed separation look with dependent separation protocols. Proc. ACM Program. Lang. 7 (ICFP). External Links: ISSN 0018-0702, Link, Document Cited by: SS1.
* S. Cui, X. Jin, S. Wang, Y. He, and Q. Huang (2020)Heuristic domain adaptation. Advances in Neural Information Processing Systems33 (20), pp. 7571-7583. Cited by: SS1.
* N. Kirfelie, P. Christen, and T. Ranbaduge (2022)TransER: homogeneous transfer learning for entity resolution. In EDBT, pp. 2-118. Cited by: SS1.
* S. Cui, X. Li, S. D. Nie, T., and others (2023)Domain separation network based entity resolution transferring method. Journal of Human University2 (2), pp. 86-94. Cited by: SS1.
* B. Li, Y. Miao, Y. Wang, Y. Sun, and W. Wang (2021)Improving the efficiency and effectiveness for bert-based entity resolution. In Proceedings of the AAAI Conference on Artificial Intelligence, pp. 13226-13233. Cited by: SS1.
* S. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, and I. Polosukhin (2017)Attention is all you need. In Advances in Neural Information Processing Systems 30, pp. 5998-6008. Cited by: SS1.
* A. Primpeki, R. Peeters, and C. Bizer (2019)The wdc training dataset and gold standard for large-scale product matching. In Companion Proceedings of the 2019 World Wide Web Conference (WWW '19), New York, NY, USA, pp. 381-386. External Links: ISBN 978-1-4503-6949-6, Link, Document Cited by: SS1.
* I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio (2020)Generative adversarial networks. Commun. ACM63 (11). External Links: ISSN 0018-0702, Link, Document Cited by: SS1.
* L. van der Maaten and G. Hinton (2008)Visualizing data using t-sne. Journal of machine learning research9 (Nov), pp. 2579--2605. Cited by: SS1.

## Appendix A Appendix

To explore the optimal strategies for key modules in MFSN and MFSN-FRSE, we conduct a series of detailed tests.

### Detailed test of similarity loss

Figure 6 and Figure 7 show the results of MFSN-basic and MFSN-FRSE using MMD loss, CoRAL loss, GRL loss, and without similarity loss. We can observe that the GRL-loss and MMD-loss have a strong generality, while the CORAL loss lacks flexibility. This is because it measures the discrepancy between two distributions only by their difference in the second-order statistics (a.k.a., the covariance). Therefore, it is only suitable for few datasets. For example, CORAL loss can effectively measure the discrepancy between Foods-Zagats (FZ) and Zomato-Yelp (DZY). However, it can't accurately compute the discrepancy between Books2 (B2) and Foods-Zagats (FZ). The adversarial training-based methods (e.g., GRL) learn a function that can reasonably compute the distribution differences according to given data examples, so it has a strong generality [24]. Equation (8) shows that MMD usually predefines multiple kernel functions \(\rho(\cdot)\) to measure the discrepancy between the source and target domains. In other words, MMD can measure the discrepancy from multiple perspectives, so it also has a strong generality.

### Detailed test of pooling strategy in MFSN-FRSE

To explore the optimal choice of pooling layer in MFSN-FRSE, we test the DomAtt and three different conventional pooling strategies. The results are shown in Figure 8. SelfAtt indicates using self-Attention for pooling: the hidden representation of [CLS] is the query, and the hidden representations of all tokens are the key and the value. CLS indicates taking the [CLS]'s hidden representation as the result of pooling. Mean indicates taking the average of all tokens' hidden representations as the result of pooling. From the overall results, DomAtt usually achieves better results. Compared with other methods, it can learn critical domain features from the input based on a domain-shared vector \(a\). Therefore, DomAtt is more suitable for computing the similarity loss between two token sequences.

### Detailed test of Decoder

To explore the optimal reconstruction strategy, we test the decoder to adopt three different strategies for reconstruction: only using the hidden representation of [CLS] (denoted as dec(cls)), using the hidden representation of all tokens except for [CLS] (denoted as dec(seq)), and using the hidden representation of all tokens (denoted as dec(all)). The results are shown in Figure 9 and Figure 10. Overall, the optimal reconstruction strategy for the decoder is dec(all). The results also show that dec(all) is better than dec(seq). This indicates that

Figure 8. The detailed test for pooling strategy. DomAtt, SelfAtt, Mean, and CLS indicate different pooling strategies.

Figure 10. The detailed test for the decoder in MFSN-FRSE. Dec(cls), dec(seq), and dec(all) represent the input strategies of the decoder. They correspond to only the [CLS] token, all tokens except for the [CLS] token, and all tokens in the sequence, respectively.

Figure 6. The detailed test of similarity loss in MFSN-basic. W/o sim represents removing the similarity loss.

Figure 7. The detailed test of similarity loss in MFSN-FRSE. W/o sim represents removing the similarity loss.

Figure 9. The detailed test for the decoder in MFSN-basic. Dec(cls), dec(seq), and dec(all) represent the input strategies of the decoder. They correspond to only the [CLS] token, all tokens except for the [CLS] token, and all tokens in the sequence, respectively.

[CLS] contains the semantic information of the whole record pair sequence, which is helpful for the reconstruction task. However, the effect of dec(cls) is often the worst, which indicates that the semantic information in [CLS] is relatively limited. Relying on [CLS] alone is not enough to reconstruct the whole sequence.

### Detailed test of Difference Loss

Finally, we test three different strategies for computing the difference loss: only using the hidden representation of [CLS] (denoted as dif(cls)), using the hidden representations of all tokens except for [CLS] (denoted as dif(seq)), and using the hidden representations of all tokens (denoted as dif(all)). The results are shown in Figure 11 and Figure 12. We can observe that dif(all) is usually the optimal strategy, followed by dif(cls). This also indicates that the hidden representation of [CLS] can encoder the overall features of the record pairs sequence. Therefore, the model can achieve feature separation by computing the difference loss between the hidden representations of [CLS]. On this basis, introducing all the tokens in the sequence can further improve the feature separation ability of the model.

Figure 11: The detailed test for the difference loss in MFSN-basic. dif(cls), dif(seq), and dif(all) represent three different strategies to calculate the difference loss. They correspond to only the [CLS] token, all tokens except for the [CLS] token, and all tokens in the sequence, respectively.

Figure 12: The detailed test for the difference loss in MFSN-FRSE. dif(cls), dif(seq), and dif(all) represent three different strategies to calculate the difference loss. They correspond to only the [CLS] token, all tokens except for the [CLS] token, and all tokens in the sequence, respectively.