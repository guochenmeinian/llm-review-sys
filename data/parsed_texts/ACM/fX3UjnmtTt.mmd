[MISSING_PAGE_FAIL:1]

GSSL methods (Zhou et al., 2017; Wang et al., 2018; Zhang et al., 2019; Zhang et al., 2019) often fail to fully leverage the complementary nature of structural and positional information, which hinders their ability to differentiate non-isomorphic graphs with similar local attributes but different global topologies.

Building on these insights, we develop a framework that fundamentally reimagings graph representation learning by innovating both GNN architecture and the self-supervised learning process. Our goal is to significantly enhance the expressiveness and representational capacity of GSSL. In distinguishing non-isomorphic graphs with similar local structures but different global topologies.

To this end, we focus on two main components:

_GenHopNet_ GNN: A novel GNN architecture designed to capture complex structural information beyond immediate neighborhoods. It implements a k-hop message-passing scheme that expands the receptive field of each node, allowing the model to capture long-range dependencies and global structural information.

Structural and Positional Aware Self-Supervised Learning: A new self-supervised learning framework that preserves and uses crucial topological information by incorporating both structural and positional information into learning. It overcomes limitations of methods that focus solely on final graph representations.

**Contributions.** Below we summarize our main contributions:

1. We introduce _GenHopNet_, a GNN framework that implements a k-hop message-passing aggregation scheme and surpasses the expressiveness of the WL test.
2. We propose a structural- and positional-aware GSSL framework, namely _StructPosGSSL_, for GNN pre-training, enabling the learning of representations invariant to specific structural and feature augmentations while preserving topological and positional information.
3. With extensive experiments on both real-world and synthetic datasets we demonstrate that our _StructPosGSSL_ achieves superior performance on most graph classification benchmarks.

## 2. Related Work

GNNs are a class of neural networks designed to effectively process and represent graph-structured data. Since the development of the previous GNN models, various adaptations have emerged, including GCN (Kipf and Welling, 2016), GAT (Vaswani et al., 2017), GraphSAGE (Hamilton et al., 2017), and GIN (Vaswani et al., 2017), among others. These models aim to learn distinguishing representations of graphs based on their data labels. However, annotating graph data, such as identifying categories of biochemical molecules, often requires specialized expertise, making it challenging to obtain large-scale labeled graph datasets (Wang et al., 2018). This challenge highlights a key limitation of supervised graph representation learning.

Contrastive Learning (CL) stands out as a highly effective self-supervised technique embedding unlabeled data (Zhou et al., 2017). By bringing similar examples closer together and pushing dissimilar ones apart, CL methods--including SimCLR (He et al., 2016), MoCo (Kipf and Welling, 2016), BYOL (Kipf and Welling, 2016), MetAug (Wang et al., 2018), and Barlow Twins (2018)--have demonstrated remarkable success in the realm of computer vision (Zhou et al., 2017; Wang et al., 2018).

**Graph Self-Supervised Learning (GSSL)** is a promising technique for learning representations of graph-structured data without requiring labeled examples, making it especially effective for graph classification tasks. To date, many GSSLs with unique strategies have been proposed to enhance graph classification. These methods build on the strengths of GNNs and CL techniques (Zhou et al., 2017; Wang et al., 2018; Zhang et al., 2019).

A key focus of GSSL is the development of effective graph augmentation strategies. For instance, GraphCL (Wang et al., 2018) introduces perturbation invariance and proposes various graph augmentations, such as node dropping, edge perturbation, attribute masking, and subgraph extraction. Recognizing the limitations of using complete graphs, Sub-Con (Shi et al., 2019) advocates for subgraph sampling as a more effective method for capturing structural information. To improve the semantic depth of sampled subgraphs, MICRO-Graph (Kipf and Welling, 2016) proposes generating informative subgraphs by learning graph motifs. Furthermore, the process of selecting suitable graph augmentations can be time-consuming and labor-intensive; JoAO (Wang et al., 2018) addresses this by introducing a bi-level optimization framework that automates the selection of data augmentations tailored to specific graph data. RGCL (Wang et al., 2018) argues that random destruction of graph properties during augmentation can lead to a loss of critical semantic information and proposes a rationale-aware approach for graph augmentation. Additionally, SPAN (Zhou et al., 2017) introduces a spectral perspective for guiding topology augmentation, noting that previous work has largely concentrated on spatial domain augmentation. To address the neglect of hierarchical structures in existing GSSL methods, HGCL (Wang et al., 2018) proposes Hierarchical GSSL, which integrates node-level CL, graph-level CL, and mutual CL components. Another important aspect of GSSL is the process of negative sampling; BGRL (Wang et al., 2018) simplifies this process by eliminating the need for constructing negative samples, allowing it to scale efficiently to large graphs. To mitigate sampling bias, PGCL (Wang et al., 2018) introduces a negative sampling strategy based on semantic clustering. In contrast to existing GSSL methods, our approach enhances both global and local structural understanding. It ensures that global graph representations effectively capture complex topological similarities and differences, while local node embeddings are refined to preserve detailed structural and positional nuances. By incorporating structural and positional awareness through invariance, variance, and covariance across node features, our method improves the ability to distinguish between isomorphic and non-isomorphic graphs. This ensures that both global graph structure and local node characteristics are robustly represented and aligned.

**Enhancing GNN Expressiveness.** A substantial amount of effort has been devoted to enhancing the expressive power of GNNs beyond the 1-WL1. This pursuit arises from the need to capture more intricate graph structures and relationships to address complex real-world problems effectively. Broadly, there are four primary directions which GNNs can extend beyond the 1-WL level: (1) A number of studies have introduced higher-order variants of GNNs, demonstrating comparable expressiveness to k-WL with \(k\geq 3\)(Kipf and Welling, 2016). As an example, k-order graph networks, introduced by (Wang et al., 2018), offer expressiveness that is similar to a set-based variation of k-WL. (Wang et al., 2018) introduced a 2-order graph network that maintains expressive power similar to 3-WL. Furthermore, (Wang et al., 2018) introduced a localized variant of k-WL, focusing solely on a subset of vertices within a neighborhood. Nevertheless, using these expressive GNNs presents challenges due to their intrinsic computational demands and intricate architecture. In addition, some studies aimed to integrate inductive biases on isomorphism counting w.r.t predefined topological attributes such as triangles, cliques, and cycles [7, 36, 38]. These efforts similar to the traditional graph kernels, as outlined by [63]. However, the task of predefining topological characteristics needs specialised knowledge in the respective domain, a resource that is frequently not easily accessible. (3) In a different vein, there has been a recent surge in studies exploring into the notion of enhancing GNNs through the augmenting of node identifiers or stochastic features. For example, [54] introduced an approach that preserves a node's local context through the manipulation of node identifiers in a permutation-equivariant fashion. [65] developed ID-GNNs, incorporating vertex identity information in their design. [11] and [41] assigned one-hot identifiers to nodes, drawing inspiration from the principles of relational pooling. In a similar vein, [46] enriched the representational capability of GNNs by incorporating a random feature for each node. There are some other approaches modify the MPNN framework or incorporate additional heuristics to enhance their expressiveness [6, 8, 57]. (4) Some works inject positional encoding (PE) as initial node features because nodes in a graph lack inherent positional information. Canonical index PE can be assigned to the nodes in a graph. However, the model must be trained on all possible index permutations, or sampling must be employed [41]. Another direction for PE in graphs is using Laplacian Eigenvectors [14, 15], as they establish a meaningful local coordinate system while maintaining the global structure of the graph. [16] proposed a PE scheme (RWPE) based on random-walk diffusion to initialize the positional representations of nodes. These positional encoding methods such as Laplacian positional encoding [14] or RWPE [16] have a significant limitation in that they usually fail to quantify the structural similarity between nodes and their surrounding neighborhoods. Nonetheless, while these techniques have demonstrated their expressivity to go beyond 1-WL. However, it remains uncertain what further attributes they can encompass beyond the scope of 1-WL.

Despite these limitations, our method offers notable advantages. _GenHopNet_ enjoys greater expressive power than the 1-WL test, providing improved node and graph-level distinction by accounting for both local and global graph structures through closed walk counts and positional information. Additionally, by incorporating edge centrality measures to enrich message-passing, _StructPosGSSL_ enhances the model's ability to differentiate various types of connections, making it strictly more expressive than Subgraph MPNNs [12, 65, 71] in distinguishing certain non-isomorphic graphs.

## 3. An Expressive and Generalizable k-hop Message Passing Framework

In this section, we introduce the Expressive and Generalizable Message-Passing (EGMP) framework, designed to incorporate learnable local structural information through an aggregation method that leverages the k-hop neighborhood without the need for explicit extraction of local substructure patterns. We provide a theoretical analysis demonstrating how k-hop GNNs within this framework can achieve greater expressiveness than 1-WL.

Let \(G=(V,E,\mathbf{A})\) be an undirected graph with a set of nodes \(V\) and a set of edge \(E\), where \(|V|=m\), \(|E|=e\), and \(\mathbf{A}\in\mathbb{R}^{m\times m}\) is the adjacency matrix. Nodes are associated with a feature matrix \(\mathbf{X}\in\mathbb{R}^{m\times x}\) with \(z\) features for each node. Let \(\mathbf{L}=\mathbf{D}-\mathbf{A}\) be a Laplacian matrix, where a diagonal matrix \(\mathbf{D}\in\mathbb{R}^{m\times m}\), and \(\mathbf{D}_{ii}=\sum_{j}\mathbf{A}_{j}\). Let be a real symmetric matrix and diagonalizable as \(\mathbf{L}=\mathbf{U}\mathbf{U}^{H}\). Here, \(\mathbf{U}=\{u_{i}\}_{j=1}^{m}\in\mathbb{R}^{m}\) are orthogonal eigenvectors, \(\Lambda=diag\left(\{1,\ldots,\ldots,m\}\right)\in\mathbb{R}^{m\times m}\) are real eigenvalues, and \(\mathbf{U}^{H}\) is a hermitian transpose of \(\mathbf{U}\).

Let \(\{\mathbf{\cdot}\}\) represent a multiset. Let \(\bar{\mathbf{A}}^{k}=\left(\bar{\mathbf{A}}^{k}_{\mathit{out}}\right)_{ \mathit{out}\in V}\) where \(\bar{\mathbf{A}}^{k}_{\mathit{out}}=\frac{\bar{\mathbf{A}}^{k}_{\mathit{out}}} {\sum_{i\in N(v)\neq\mathbf{A}^{k}_{\mathit{in}}}\mathbf{A}^{k}_{\mathit{in}}}\) refers to a normalized value of \(\mathbf{A}^{k}_{\mathit{out}}\), and \(\mathbf{X}\in\mathbb{R}^{m\times x}\) be the matrix of input feature vectors with each \(\mathbf{x}_{v}\in\mathbb{R}^{x}\) corresponding to each vertex \(v\in V\). We indicate the feature vector of vertex \(v\) at the \(\mathbf{t}^{\text{th}}\) layer as \(\mathbf{h}^{(t)}_{v}\) and set \(\mathbf{h}^{(0)}_{v}=\mathbf{x}_{v}\). Then, the definition of the

Figure 1. (a) A high-level overview of the GSSL model architecture of _StructPosGSSL_ (\(G\) is an input graph and \(G^{\prime},G^{\prime}\) are two augmented views). Our design comprises three main components: (i) a structural encoder (SE) that generates structural embeddings (\(h_{SE}\) and \(h^{\prime}_{SE}\)) for nodes based on their local structural properties; (ii) a positional encoder (PE) that generates positional embeddings (\(h_{PE}\) and \(h^{\prime}_{PE}\)) for nodes; and (iii) a pooling layer that aggregates the node representations to generate the final graph representation. Moreover, \(\mathit{MLP}_{\mathit{\theta}}\) and \(\mathit{MLP}_{\mathit{\theta}}\) are two shared projection heads for node representations and graph representations, respectively. (b) The real-world graph structures of two molecules, Decalin and Bicyclopentyl. While standard Graph SSL frameworks cannot distinguish between these molecular structures, our model successfully differentiates them.

(+1)th layer in an aggregation scheme is given as:

\[\mathbf{M}_{e}^{(t)} =\text{A}\text{G}\text{c}^{E}\Big{(}\{(\mathbf{A}_{\text{out}}, \mathbf{h}_{u}^{(t)},e_{\text{out}}^{b},e_{\text{out}}^{c})|u\in\mathcal{N}(v) \}\Big{)}, \tag{2}\] \[\mathbf{M}_{u}^{(t)} =\text{A}\text{G}\text{c}^{N}\Big{(}\{(\mathbf{\hat{A}}_{\text{out }}^{k},\mathbf{h}_{u}^{(t)})|u\in\mathcal{N}^{k}(v)\}\Big{)};k\geq 2,\] (3) \[\mathbf{M}_{v}^{(t)} =\text{A}\text{c}^{G}\Big{(}\{\mathbf{\hat{A}}_{\text{out }}^{k},\mathbf{h}_{v}^{(t)}\}\Big{)};k\geq 2,\] (4) \[\mathbf{h}_{v}^{(t+1)} =\text{Combine}\Big{(}\mathbf{h}_{v}^{(t)},\mathbf{M}_{e}^{(t)},\mathbf{M}_{u}^{(t)},\mathbf{M}_{v}^{(t)}\Big{)}. \tag{1}\]

The above equations define a process for aggregating messages in our GNN, where:

1. \(\text{A}\text{G}\text{c}^{E}(\cdot)\) in Eq. 1 computes the edge-level aggregated message \(M_{e}^{(t)}\) for vertex \(v\) based on the node features and edge attributes of its neighbors \(u\in\mathcal{N}(v)\) within the 1-hop neighborhood;
2. \(\text{A}\text{c}\text{c}^{N}(\cdot)\) in Eq. 2 aggregates a normalized message \(M_{u}^{(t)}\) from the \(k\)-hop neighbors (\(k\geq 2\)) of vertex \(v\) weighting their contributions by the normalized adjacency matrix \(\mathbf{\hat{A}}_{\text{out}}^{k}\);
3. \(\text{A}\text{c}\text{c}^{I}\) in Eq. 3 calculates the self-message \(\mathbf{M}_{v}^{(t)}\) for vertex \(v\) using its own adjacency information and features, focusing on capturing closed-walks of length up to \(k\) (\(k\geq 2\)) that return to node \(v\) itself;
4. Eq. 4 combines the current feature vector \(\mathbf{h}_{v}^{(t)}\) of vertex \(v\) with the aggregated messages \(\mathbf{M}_{e}^{(t)},\mathbf{M}_{u}^{(t)}\), and \(\mathbf{M}_{v}^{(t)}\) to update final node representation of \(v\) for the next layer.

We use the above set of equations to compute the graph's topological information. For the positional information, we only use Eq. 1 by replacing node features \(\mathbf{h}_{u}^{(t)}\) with positional features \(\mathbf{h}_{u,pos}^{(t)}\).

In more detail, \(\mathbf{M}_{u}^{(t)}\) is a message aggregated from neighbors of node \(v\), using their normalized coefficients \(\mathbf{\hat{A}}_{\text{out}}^{k}\), while \(\mathbf{M}_{v}^{(t)}\) is the adjusted message from node \(v\) to itself, considering walk lengths up to k-hop. The diagonal elements of \(\mathbf{A}^{k}\), namely \(\mathbf{A}_{\text{out}}^{k}\), count the number of closed walks of length \(k\) that start and end at the same node \(v\). Such a mechanism highlights the importance of node \(v\) within its local topology and its role in the connectivity of the graph over multiple hops. This should exhibit the following properties:

1. **Closed Walks and Connectivity:** The power \(\mathbf{A}^{k}\) enumerates all possible walks of length \(k\) in the graph. By examining \(\mathbf{A}_{\text{out}}^{k}\), one can infer how _connected_ or _central_ a node is with respect to walks of length \(k\). \(\mathbf{Tr}(\mathbf{A}^{k})\) counts the total number of closed walks of length \(k\) starting and ending at the same vertex. This measure gives a quantitative sense of the graph's connectivity:
2. **Local Connectivity:** High numbers of shorter closed walks (smaller \(k\)) indicate strong local connectivity. This is useful for understanding how tightly knit individual neighborhoods are within the graph.
3. **Global Connectivity:** As \(k\) increases, the nature of the closed walks provides insights into the global connectivity and the presence of cycles within the graph. Any cycle in a graph is a closed walk; however, not all closed walks are cycles as cycles have the additional constraint of not repeating vertices or edges except the starting/ending vertex.
4. **Isomorphic Invariant:** This simple trick facilitates the creation of unique node representations by ensuring that each node possesses a distinct k-hop topological neighborhood, provided that k is sufficiently large. The sum \(\sum_{k}\mathbf{A}_{\text{in}}^{k}\) across different powers \(k\) reflects the number of closed walks of varying lengths starting and ending at node \(v\). For instance, the equality \(\sum_{k}\mathbf{A}_{\text{out}}^{k}=\sum_{k}\mathbf{A}_{\text{out}^{k}}^{k}\), holds if nodes \(v\) and \(v^{\prime}\) are k-hop isomorphic, implying that they share identical local connectivity patterns up to k-hops. This characteristic serves as a powerful tool for identifying and distinguishing nodes based on their structural roles within the network.

Understanding cycle and closed-walk isomorphisms is essential for elucidating the structural roles of nodes within a graph, revealing both local and global connectivity patterns. Cycle isomorphism highlights nodes that engage in similar closed-loop interactions, while closed-walk isomorphism provides insights into broader connectivity by capturing indirect relationships that contribute to the overall network topology. The definitions of cycle and closed-walk isomorphisms formalize these concepts, emphasizing their significance in analyzing graph structures.

**Definition 1**.: _Cycle Isomorphism (\(\simeq_{Cycle}\)): Two nodes \(v\) and \(v^{\prime}\) are cycle isomorphic if the sum of the powers of \(\mathbf{A}\) over \(k\) (i.e., \(\sum_{k}\mathbf{A}_{\text{in}}^{k}\) and \(\sum_{k}\mathbf{A}_{\text{out}^{k}}^{k}\)) considers only closed walks that are also cycles (i.e., walks that do not repeat any vertices or edges except the starting/ending vertex)._

**Definition 2**.: _Closed Walk Isomorphism (\(\simeq_{ClosedWalk}\)): Two nodes \(v\) and \(v^{\prime}\) are closed walk isomorphic if the sum of the powers of \(\mathbf{A}\) over \(k\) (i.e., \(\sum_{k}\mathbf{A}_{\text{in}}^{k}\) and \(\sum_{k}\mathbf{A}_{\text{out}^{k}}^{k}\)) considers all possible closed walks (i.e., walks that start and end at the same vertex, but may repeat vertices and edges)._

**Theorem 1**.: _The following statement is true: (a) If \(\sum_{k}\mathbf{A}_{\text{out}}^{k}\simeq_{Cycle}\)\(\sum_{k}\mathbf{A}_{\text{out}^{k}\text{out}^{\prime}}\), then \(\sum_{k}\mathbf{A}_{\text{in}}^{k}\simeq_{ClosedWalk}\)\(\sum_{k}\mathbf{A}_{\text{out}^{k}\text{out}^{\prime}}\); but not vice versa._

Going back ot Eq. 1, \(\mathbf{M}_{v}^{(t)}\) is a message aggregation function that considers the 1-hop neighborhood, including their edge connections and edge attributes. We enrich the 1-hop neighborhood aggregation by injecting three different centrality based edge attributes \(\mathbf{e}_{\text{out}}^{c}\), between given pair of nodes: (1) Edge Betweenness (EB), (2) Edge Closeness (EC), and (3) Edge Clustering Coefficients (ECC) as additional features (Shen et al., 2016). These attributes are designed to augment the distinguishing capabilities of the model, enabling it to better differentiate between various types of connections and facilitating nuanced understanding and processing of edge-related information in graphs. This should exhibit the following properties:

Figure 2. A high-level overview of different closed-walks \(k=2,\ldots,4\), where the blue node represents the source node. For \(k=2\), the walk can traverse between nodes multiple times, forming a non-cyclic path. For \(k=3\) and \(k=4\), the walks can capture closed cycles of length 3 and 4, respectively, effectively identifying cyclic structures in the graph.

[MISSING_PAGE_FAIL:5]

and space complexities of _GenHopNet_ are \(O(tezd)\) and \(O(e)\), respectively, where \(e\) denotes the number of edges in the graph, \(t\) represents the number of layers, and \(z\) and \(d\) correspond to the dimensions of the input and output feature vectors.

## 5. Graph Self-Supervised Learning Framework

In this section, we introduce Structural and Positional GSSL (_StructPosGSSL_), a new class of graph self-supervised learning framework based on structural and positional information within graphs.

### Data Augmentation for Graph

The goal of data augmentation is to produce consistent, identity-preserving positive samples of a specific graph. In this work, we use two main types of augmentation strategies: structural augmentation and feature augmentation (Wang et al., 2017). In structural augmentation, three distinct strategies are considered: (1) Subgraph Induction by Random Walks (RWS), (2) Node Dropping (ND), and (3) Edge Dropping (ED). For feature augmentation, we employ three different approaches: (1) Feature Dropout (FD), (2) Feature Masking (FM), and (3) Edge Attribute Masking (EAM). In our work, we generate different augmented graphs from a single input graph \((\mathbf{A},\mathbf{X})\), resulting in two correlated views, namely \((\hat{\mathbf{A}}^{\ell},\hat{\mathbf{X}}^{\ell})\) and \((\hat{\mathbf{A}}^{\ell^{\prime}},\hat{\mathbf{X}}^{\ell^{\prime}})\).

### Expressive/Generalizable Graph Encoders

For each augmented view, we process it through two distinct GNN encoders. One encoder is dedicated to structural encoding via the proposed _GenHopNet_. Alongside structural features in the _GenHopNet_, we employ another encoder for capturing positional information. We initiate the positional feature vectors using Laplacian eigenvectors, as outlined in (Shen et al., 2017). This second encoder, also a GNN, applies Eq.1 with the Combine function, i.e., \(\mathbf{h}_{\mathbf{e},\mathbf{pos}}^{(t+1)}=\text{Combine}\Big{(}\mathbf{h} _{\mathbf{e},\mathbf{pos}}^{(t)},\mathbf{h}_{\mathbf{e}^{\prime}}^{(t)}\Big{)}\), and leverages the spectral properties of the graph Laplacian. This approach targets the smallest non-trivial eigenvalues to derive meaningful positional encodings that accurately reflect the structural roles of nodes within the graph. By focusing on the spectral characteristics of the Laplacian, this encoding strategy effectively captures both the local connectivity of nodes and the broader topology of the graph, significantly enhancing the model's capability to understand and manage complex graph structures. Each encoder outputs node representations and a final graph representation for augmented views. We then pass them through another shared projection head (an MLP) to obtain the final structural and positional representations for both nodes and graphs. Next, we concatenate the structural features with positional features for node representations and graph representations separately, ensuring a comprehensive integration of both structural and positional data. To facilitate the end-to-end training of encoders and generate comprehensive node and graph representations that are independent of specific downstream tasks, we implement a loss function that merges NT-Xent (Kumar et al., 2017) and refined VICReg (Chen et al., 2019).

### Training Pipeline

In the pursuit of effective self-supervised learning frameworks, the quality of learned representations hinges critically on the choice of loss functions. To enable end-to-end training of the encoders and to develop robust graph and node representations that are independent of downstream tasks, we employ the NT-Xent (Kumar et al., 2017) loss to learn the graph representations and use the refined VICReg (Chen et al., 2019) loss as a regularization term to learn the node representations. The NT-Xent loss, which maximizes discriminative power between positive and negative samples of graph representations, while VICReg's regularization terms that ensure a balanced and non-redundant spread of node features across dimensions to reduce the representational collapse. This integration stabilizes training in self-supervised setup and enriches graph representations for downstream tasks.

The NT-Xent loss, is defined by the formula:

\[L_{N}(\mathbf{z}^{\prime},\mathbf{z}^{\prime})=-log\frac{exp(sim(\mathbf{z}^{ \prime},\mathbf{z}^{\prime})/\tau)}{\sum_{k=1}^{2m}\mathbf{l}_{\{k+r^{\prime} \}}exp(sim(\mathbf{z}^{\prime},\mathbf{z}^{k})/\tau)}, \tag{6}\]

where \(\mathbf{z}^{\prime},\mathbf{z}^{\prime}\in\mathbb{R}^{d}\) are graph representations for two augmented views, with \(\tilde{d}\) denoting the embedding size of each graph augmented view in the dataset. The parameter \(\tau\) is the temperature scaling parameter, and \(\mathbf{l}_{\{k+i\}}\) is an indicator function that is \(\mathbf{i}\) if \(k\neq i\) and \(0\) otherwise. Here \(sim(\mathbf{z}^{\prime},\mathbf{z}^{\prime})\) is a similarity function, typically the cosine similarity, between two vectors \(\mathbf{z}^{\prime}\) and \(\mathbf{z}^{\prime}\).

We combine the refined VICReg loss with the contrastive NT-Xent loss to create a unified loss function that maximizes mutual information between graph embeddings while preserving structural and positional node alignment. This unified approach not only aligns embeddings for isomorphic node pairs but also maintains diversity and enhances the model's topological discriminative power. By enabling the model to differentiate between nodes with subtle structural or positional differences, this method is crucial for accurately identifying both isomorphic and non-isomorphic node pairs. In this work, the refined VICReg loss is specifically adapted for correspondence node alignment to improve isomorphic graph representation learning in a self-supervised setting.

The refined VICReg loss is given by:

\[L_{V}(\mathbf{h}^{\ell^{\prime}},\mathbf{h}^{\ell^{\prime}})= \lambda_{inv}\cdot L_{inv}(\mathbf{h}^{\ell^{\prime}},\mathbf{h}^{\ell^{ \prime}})+\lambda_{outer}\cdot L_{outer}(\mathbf{h}^{\ell^{\prime}},\mathbf{h }^{\ell^{\prime}})+\] \[\lambda_{coo}\cdot L_{coo}(\mathbf{h}^{\ell^{\prime}},\mathbf{h}^{ \ell^{\prime}}) \tag{7}\]

where \(\lambda_{inv},\lambda_{coo}\) and \(\lambda_{coo}\) are weighting factors for the invariance, variance, and covariance components, respectively. The Invariance Loss \(L_{inv}(\mathbf{h}^{\ell^{\prime}},\mathbf{h}^{\ell^{\prime}})=\frac{1}{m} \sum_{i}||\mathbf{h}^{\ell^{\prime}}_{i}-\mathbf{h}^{\ell^{\prime}}_{i}||_{2}^ {2}\), where \(\mathbf{h}^{\ell^{\prime}},\mathbf{h}^{\ell^{\prime}}\in\mathbb{R}^{m\times d}\) represent the node representations of the two augmented views. The Variance Loss \(L_{coo}(\mathbf{h}^{\ell^{\prime}},\mathbf{h}^{\prime^{\prime}})=\frac{1}{2} \sum_{j=1}^{d}\left|max(0,\gamma-std(\mathbf{h}^{\ell^{\prime}}_{j},e))-max(0, \gamma-std(\mathbf{h}^{\ell^{\prime}}_{j},e))\right|\). The Covariance Loss \(L_{coo}(\mathbf{h}^{\ell^{\prime}},\mathbf{h}^{\ell^{\prime}})=\frac{1}{d} \sum_{i\neq j}\left|\left[\mathbf{C}(\mathbf{h}^{\ell^{\prime}})\right]_{2}^ {2}-\left[\mathbf{C}(\mathbf{h}^{\ell^{\prime}})\right]_{2}^{2}\right|\) where \(\mathbf{C}(\mathbf{h}^{\ell^{\prime}})=\frac{1}{m-1}\sum_{i=1}^{m}(\mathbf{h} ^{\ell^{\prime}}_{i}-\mathbf{h}^{\ell^{\prime}})(\mathbf{h}^{\ell^{\prime}}_{i} -\mathbf{h}^{\ell^{\prime}})^{T}\), with \(\mathbf{h}^{\ell^{\prime}}=\frac{1}{m}\sum_{i=1}^{m}\mathbf{h}^{\ell^{\prime}}_{i}\) denoting the mean over feature vectors. The Invariance Loss captures the inherent structure of the graph, maintaining node representations'

\begin{table}
\begin{tabular}{l c c} \hline \hline Method & CSL & SR25 \\ \hline GCN & \(10.0\pm 0.0\) & \(6.6\pm 0.0\) \\ GIN & \(10.0\pm 0.0\) & \(6.6\pm 0.0\) \\
3WLGNN & \(97.8\pm 10.9\) & - \\
3-GCN & \(95.7\pm 14.8\) & \(6.6\pm 0.0\) \\ GCN-RNI & \(16.0\pm 0.0\) & \(6.6\pm 0.0\) \\ \hline StructPosGSSL-SA & \(\mathbf{98.6}\pm\mathbf{2.8}\) & \(\mathbf{100.0}\pm\mathbf{0.0}\) \\ StructPosGSSL-FA & \(\mathbf{98.3}\pm\mathbf{2.5}\) & \(\mathbf{100.0}\pm\mathbf{0.0}\) \\ \hline \hline \end{tabular}
\end{table}
Table 1. Classification accuracy (%) on the test set for CSL and SR25 datasets.

structural and positional properties despite augmentation. The Variance Loss helps distinguish non-isomorphic nodes, enhancing the model's ability to capture structural differences. Lastly, the Covariance Loss ensures that node embeddings reflect diverse structural and positional aspects, improving the expressiveness and discriminative power of the representations.

The total combined loss for integrating NT-Xent and refined VICReg is simply the sum of these two losses:

\[L_{T}(\mathbf{z}^{\prime},\mathbf{z}^{\prime},\mathbf{h}^{\prime},\mathbf{h}^{ \prime})=L_{N}(\mathbf{z}^{\prime},\mathbf{z}^{\prime})+\mu\cdot L_{V}(\mathbf{ h}^{\prime},\mathbf{h}^{\prime}), \tag{8}\]

where \(\mu\) is the weighting factor for VICReg loss. This unified loss function effectively integrates isomorphism preservation with representation expressiveness and diversity.

Theorem 3 ().: StructPosGSSL2 _is more expressive than subgraph MPNNs in distinguishing certain non-isomorphic graphs._

Footnote 2: The code is available at: [https://anonymous.depen.science/r/StructPosGSSL-73D0](https://anonymous.depen.science/r/StructPosGSSL-73D0)

## 6. Numerical Experiments

In this section, we evaluate our self-supervised learning framework on graph classification benchmark tasks. The results from our models are statistically significant with a 95% confidence level. We evaluate _StructPosGSSL_ on graph classification benchmark tasks and compare their performance with leading baselines to address the following questions:

1. How effective are _StructPosGSSL_ in small graph classification task based on empirical performance?
2. How effective are _StructPosGSSL_ in large graph classification tasks based on empirical performance?
3. How effective is _StructPosGSSL_ for isomorphism testing in synthetic graph classification tasks?
4. How do structural and positional encodings impact overall performance?

In this following sections, we analyze the experimental results to address the four previously mentioned questions.

### Experiments on Small Graphs

We use eight datasets from two categories: (1) bioinformatics datasets: MUTAG, PTC-MR, NCI1, and PROTEINS (Gil et al., 2018; Wang et al., 2019; Wang et al., 2019; Wang et al., 2019); (2) social network datasets: IMDB-B, IMDB-M, COLLAB and RDT-MSK (Gil et al., 2018).

We compare our method against fourteen baseline approaches: (1) Graph kernel methods: WI. subtree kernel (WL) (Walner et al., 2018), WL-OA (Wang et al., 2019), RetGK (Wang et al., 2019), P-WL (Wang et al., 2019), and WL-PM (Wang et al., 2019); (2) GNN-based methods: PATCHY-SAN (Wang et al., 2019), DGCNN (Wang et al., 2019), CAPSGNN (Wang et al., 2019), and GIN (Wang et al., 2019); (3) Unsupervised methods: InfoGraph (Wang et al., 2019), GraphCL (Wang et al., 2019), MVGRL (Wang et al., 2019), AutoGCL (Wang et al., 2019), and JOAO (Wang et al., 2019).

Specifically, the _GenHopNet_ encoder models are initially trained in an unsupervised manner, and the resulting embeddings are then input into a linear classifier to accommodate the labeled data. Then, for fair comparison, we execute our method using ten random splits (Wang et al., 2019) and utilize the 10-fold cross-validation method and present the best mean accuracy (%) along with the standard deviation. The results are presented in table 2 and 3. We have two settings: (1) StructPosGSSL-SA, which considers structure augmentation, and (2) StructPosGSSL-FA, which considers feature augmentation. In both settings, we employ the Adam optimizer (Kingma and Ba, 2014), with hidden dimension of 128, weight decay is 0.0003, a 2-layer MLP with batch normalization, 100 epochs, positional encoding dimension of 6, a dropout rate of 0.5, and a temperature scaling parameter \(\tau\) of 0.10.

We choose a batch size from \((32,64,128,256)\) and a number of hops \(k\in\{2,3,4,5,6\}\). We use \(\lambda_{inn}=1,\lambda_{var}=3,\lambda_{coa}=2\), and \(\mu=0.5\) for MUTAG and \(\lambda_{inn}=1,\lambda_{var}=23,\lambda_{coa}=23\), and \(\mu=0.0003\) for PTC-MR, and \(\lambda_{inn}=1,\lambda_{var}=24,\lambda_{coa}=24\), and \(\mu=0.005\) for the remaining datasets. The readout function, as described in (Wang et al., 2019), is utilized, which involves concatenating representations from all layers to derive a final graph representation.

To address **Q1**, in Tables 2 & 3, StructPosGSSL outperforms the best baseline by 0.4% (PATCHY-SAN), 1.1% (CapsGNN), 1.1% (WL-OA, MVGRL), 0.5% (WL-PM), and 0.2% (GIN) on the datasets MUTAG, PTC-MR, IMDB-B, IMDB-M, and RDTMSK, respectively.

These gains are a reflection of the inherent characteristics of the datasets. Graphs with smaller diameters, _i.e._, IMDB-B, IMDB-M, PTC-MR, and MUTAG, feature nodes that are closer together, promoting localized interactions that enable GNNs to capture both local and global information effectively, even with few message-passing steps. In such cases, structural encoding with closed walks is particularly beneficial, as it differentiates local structures by capturing repeated node interactions and identifying cycles. Conversely, datasets such as NCI1 and COLLAB, with larger diameters, present increased structural complexity, making it challenging to capture patterns using closed walks alone due to the difficulty of accounting for distant node interactions with limited local information.

### Experiments on Large Graphs

We utilize five large graph datasets from the Open Graph Benchmark (OGB) (Kumar et al., 2018), comprising one molecular graph dataset (ogbg-moltoxcast, ogbg-moltox21, ogbg-moltbox) and one protein-protein association network (ogbgbg-ppa). We compare our approach with the following methods that have reported results on the aforementioned OGB datasets: GIN and GIN+VN (Kumar et al., 2018), GSN (Wang et al., 2019), ID-GNNs (Wang et al., 2019), Deep LRP (Wang et al., 2019), GraphSNN (Wang et al., 2019), DS-GNN (EGO+), (Wang et al., 2019), DSS-GNN (EGO+) (Wang et al., 2019), and POLICY-LEARN (Chen et al., 2019).

For large graph datasets, we adopt the same experimental framework as outlined in (Kumar et al., 2018). Our evaluation process is divided into two distinct learning phases. In the initial phase, the models are trained in a self-supervised fashion using only node features and graph structure without any label data. Subsequently, in the second phase, the representations generated by the GNN encoders during the first phase are fixed in place and employed to train, validate, and test the models using a straightforward linear classifier.

We utilize the Adam optimizer with a learning rate of 0.001, a batch size of 32, dropout of 0.5, positional encoding dimension of 6, and run training for 100 epochs across all datasets. We use a 2-layer MLP with a hidden dimension of 200 and a temperature scaling parameter \(\tau\) of 0.10 for both settings. We choose \(\lambda_{inn}=1,\lambda_{var}=24,\lambda_{coa}=24\), and \(\mu=0.005\) for all datasets. The classification accuracy results are presented in Table 4.

To address **Q2**, in Table 4, StructPosGSSL consistently outperforms all the baseline methods across all OGB graphs listed. StructPosGSSL surpasses best results of existing GNNs by 0.5% (POLICY-LEARN), 0.55% (DSS-GNN (EGO+)), 0.32% (GIN+VN), 0.24% (GraphSNN), and 0.42% (GIN+VN) on the datasets ogbg-moltoxcast, ogbg-moltox21, ogbg-moltiv, ogbg-ppa, and ogbg-moltox, respectively.

[MISSING_PAGE_FAIL:8]

## References

* [1]A. Bordes, J. Ponce, and Y. Lecun (2022) Vicency: variance-covariance regularization for self-supervised learning. In ICLR 2022-International Conference on Learning Representation, Cited by: SS1, SS2.
* [2]B. Devlacqua, F. Frasca, D. Lim, B. Srinivasan, C. Cai, G. Balamurugan, M. M. Bronstein, and H. Maron (2022) Equivariant subgraph aggregation networks. In International Conference on Learning Representations, Cited by: SS1, SS2.
* [3]B. Devlacqua, F. Frasca, S. Zafeiriou, and M. M. Bronstein (2022) Improving graph neural network expression via subgraph isomorphism counting. IEEE Transactions on Pattern Analysis and Machine Intelligence. Cited by: SS1, SS2.
* [4]C. Bodnar, F. Frasca, Y. Wang, N. Offer, G. F. Montufaro, P. Lio, and M. Bronstein (2021) Weisfeiler and leblum gc topologist: message passing simplicial networks. In International Conference on Machine Learning, pp. 1026-1037. Cited by: SS1, SS2.
* [5]G. Bouritsas, F. Frasca, S. Zafeiriou, and M. M. Bronstein (2022) Improving graph neural network expression via subgraph isomorphism counting. IEEE Transactions on Pattern Analysis and Machine Intelligence. Cited by: SS1, SS2.
* [6]C. Boulatiux, F. Frasca, S. Zafeiriou, and M. M. Bronstein (2022) Improving graph neural network expression via subgraph isomorphism counting. IEEE Transactions on Pattern Analysis and Machine Intelligence. Cited by: SS1, SS2.
* [7]T. Chen, S. Kornblith, M. Norouzi, and G. Hinton (2021) A simple framework for contrastive learning of visual representations. In International conference on machine learning, pp. 1597-1607. Cited by: SS1, SS2.
* [8]Z. Chen, L. Chen, S. Villar, and J. Bruna (2020) Can graph neural networks count substructures?. Advances in neural information processing systems. Cited by: SS1, SS2.
* [9]L. Cotta, C. Morris, and R. Ribeiro (2021) Reconstruction for powerful graph representations. Advances in Neural Information Processing Systems34, pp. 1713-1726. Cited by: SS1, SS2.
* [10]A. K. Debnath, R. L. Lopez de Commodre, G. Debnath, A. J. Shusterman, and C. Hansch (1992) Structure-activity relationship of mutagenic aromatic and heteroaromatic nitric compounds. correlation with molecular orbital energies and hydrophobicity. Journal of medicinal chemistry34 (2), pp. 786-797. Cited by: SS1, SS2.
* [11]V. P. Dwivedi and X. Bresen (2021) A generalization of transformer networks to graphs. In AAAI Workshop on Deep Learning on Graph: Methods and Applications, Cited by: SS1, SS2.
* [12]V. P. Dwivedi, A. T. Laur, T. Laurent, Y. Bengio, and X. Bresen (2021) Graph neural networks with learnable structural and positional representations. In International Conference on Learning Representations, Cited by: SS1, SS2.
* [13]J. Feng, Y. Chen, E. Li, A. Sarkar, and M. Zhang (2022) How powerful are k-hop message passing graph neural networks. Advances in Neural Information Processing Systems35, pp. 37-476. Cited by: SS1, SS2.
* [14]J. Gull, F. Strub, F. Alche, C. Tallee, P. Richemond, E. Buchukaya, C. Doersch, B. Avila Pires, Z. Guo, M. Gheshaghi Azar, et al. (2021) Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems32, pp. 1211-1224. Cited by: SS1, SS2.
* [15]W. Hamilton, Z. Ying, and J. Leskovec (2017) Inductive representation learning on graphs. Advances in neural information processing systems30. Cited by: SS1, SS2.
* [16]W. Li, H. Yan, Y. Jin, Y. Jin, Y. Zhang, T. Zhang, and Y. Zhu (2020) Sub-graph contrast for scalable self-supervised graph representation learning. In 2020 IEEE international conference on data mining (ICDM), pp. 222-231. Cited by: SS1, SS2.
* [17]W. Li, J. Zhang, Y. Guo, Z. Liu, Q. Long, Z. Qiao, Y. Qin, J. Shen, F. Sun, Z. Xiao, et al. (2024) A comprehensive survey on deep graph representation learning. Neural Networks, pp. 106207. Cited by: SS1, SS2.
* [18]W. Li, Y. Guo, X. Luo, Y. Wang, H. Yuan, H. Zhong, and M. Zhang (2023) Unsupervised graph-level representation learning with hierarchical contrasts. Neural Networks258, pp. 359-368. Cited by: SS1, SS2.
* [19]D. P. Kingma and J. Ba (2015) Adam: a method for stochastic optimization. In International Conference on Learning Representations (ICLR), Cited by: SS2.
* [20]T. N. Kipf and M. Welling (2016) Semi-supervised classification with graph convolutional networks. In International Conference on Learning Representations, Cited by: SS1, SS2.
* [21]N. M. Krieg, P. Gisard, and R. Wilson (2016) On valid optimal assignment kernels and applications to graph classification. In Advances in Neural Information Processing Systems (NeurIPS), pp. 1623-1631. Cited by: SS1, SS2.
* [22]J. Li, W. Qiang, C. Zheng, B. Su, and Y. Xiong (2021) Contrastive learning via lfeat feature augmentation. In International Conference on Machine Learning, pp. 1294-12978. Cited by: SS1, SS2.
* [23]S. Li, X. Wang, A. Zhang, Y. Wu, X. He, and T. S. Chua (2021) Let invariant rationale discovery in magic graph contrastive learning. In International conference on machine learning, pp. 1305-1305. Cited by: SS1, SS2.
* [24]L. Lin, J. Chen, and H. Wang (2023) Spectral augmentation for self-supervised learning on graphs. In The Eleventh International Conference on Learning Representations, Cited by: SS1, SS2.
* [25]S. Lin, C. Liu, P. Zhou, Z. Yu, H. Wang, B. Zhao, Y. Zheng, L. Lin, E. Xing, and X. Liang (2022) Prototypical graph contrastive learning. IEEE transactions on network and learning systems35 (2), pp. 2747-2758. Cited by: SS1, SS2.
* [26]X. Lin, H. Pan, M. He, Y. Song, X. Jiang, and L. Shang (2020) Neural subgraph isomorphism counting. In ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (SIGKDD), pp. 1959-1960. Cited by: SS1, SS2.
* [27]M. M. Ren-Hunan, H. Scrivainen, and Y. Lipman (2019) Provably powerful graph networks. Advances in Neural Information Processing Systems (NeurIPS). Cited by: SS1, SS2.
* [28]F. Monti, K. Oness, and M. M. Bronstein (2018) Motilent: a multi-based graph convolutional network for directed graphs. In 2018 IEEE Data Science Workshop (DSW), pp. 225-228. Cited by: SS1, SS2.
* [29]C. Morris, M. Ritzert, M. Frey, W. L. Hamilton, J. E. Lenssen, G. Rattan, and M. Grube (2019) Weisfeiler and leblum gc topologist: a multi-layer deep neural network. In AAAI Conference on Artificial Intelligence (AAAI), pp. 4602-4607. Cited by: SS1, SS2.
* [30]M. Nieppert, M. Ahmed, and K. Kutzkov (2016) Learning convolutional neural networks for graphs. In International conference on machine learning, pp. 2014-2023. Cited by: SS1, SS2.
* [31]O. Niederhuane, P. Mellaianos, and N. Varigiannis (2017) Matching node embeddings for graph similarity. In Proceedings of the AAAI conference on Artificial Intelligence, Vol. 31. Cited by: SS1, SS2.
* [32]S. Niess, P. Mellaianos, and N. Varigiannis (2017) Matching node embeddings for graph similarity. In Proceedings of the AAAI conference on Artificial Intelligence, Vol. 31. Cited by: SS1, SS2.
* [33]S. Niess, A. T. Lin, J. H. Zhang, and Y. Zhang (2020) InfoGraph: unsupervised and semi-supervised graph-level representation learning via mutual information maximization. In International Conference on Learning Representations, Cited by: SS1, SS2.
* [34]S. Niess, A. T. Lin, J. H. Zhang, Y. Wang, H. Yuan, H. Zhong, and M. Zhang (2020) Unsupervised graph convolutional networks for graph classification. In International Conference on Learning Representations, Cited by: SS1, SS2.
* [35]S. Niess, A. T. Lin, J. H. Zhang, Y. Jin, Y. Jin, Y. Jiang, and Y. Zhu (2020) Sub-graph contrast for scalable self-supervised graph representation learning. In 2020 IEEE international conference on data mining (ICDM), pp. 222-231. Cited by: SS1, SS2.
* [36]W. Yu, Z. Fang, Y. Gu, Z. Liu, Q. Long, Z. Qiao, Y. Qin, J. Shen, F. Sun, Z. Xiao, et al. (2024) A comprehensive survey on deep graph representation learning. Neural Networks, pp. 106207. Cited by: SS1, SS2.
* [37]W. Yu, Z. Yang, Y. Jin, and Y. Zhang (2020) A survey on deep learning approach leveraging neural networks. In International Conference on Learning Representations, Cited by: SS1, SS2.
* [38]W. Yu, Z. Yang, Y. Jin, Y. Zhang, H. Ren, B. Liu, M. C. Clark, and J. Leskovec (2020) Open graph benchmark: datasets for machine learning on graphs. Advances in Neural Information Processing Systems (NeurIPS). Cited by: SS1, SS2.
* [39]W. Yu, Z. Yang, Y. Jin, Y. Zhang, Y. Zhang, and Y. Zhu (2020) Sub-graph contrast for scalable self-supervised graph representation learning. In 2020 IEEE international conference on data mining (ICDM), pp. 222-2215. Cited by: SS1, SS2.
* [40]W. Yu, Z. Yang, Y. Jin, Y. Zhang, Y. Zhang, and Y. Zhu (2020) Sub-graph contrast for scalable self-supervised graph representation learning. In 2020 IEEE international conference on data mining (ICDM), pp. 222-2215. Cited by: SS1, SS2.
* [41]W. Yu, Z. Yang, Y. Jin, Y. Wang, D. G. Torr, P. Lio, and M. Bronstein (2021) Improving graph neural network expression via subgraph isomorphism counting. arXiv preprint arXiv:2108.0225. Cited by: SS1, SS2.
* [42]W. Yu, Z. Yang, Y. Jin, Y. Zhang, Y. Zhang, and Y. Zhu (2020) Sub-graph contrast for scalable self-supervised graph representation learning. In 2020 IEEE international conference on data mining (ICDM), pp. 222-2215. Cited by: SS1, SS2.
* [43]W. Yu, Z. Yang, Y. Jin, Y. Zhang, Y. Zhang, Y. Jin, Y. Jin, S. Feng, Y. Sun, Z. Xiao, et al. (2020) A comprehensive survey on deep graph representation learning. Neural Networks, pp. 106207. Cited by: SS1, SS2.
* [44]W. Yu, Z. Yang, Y. Jin, Y. Zhang, Y. Jin, Y. Zhang, Y. Jin, Y. Sun, and Y. Zhu (2020) A comprehensive survey on deep graph representation learning. Neural Networks, pp. 106207. Cited by: SS1, SS2.
* [45]W. Yu, Z. Yang, Y. Jin, Y. Zhang, H. Yan, H. Zhang, and M. Zhang (2020) Unsupervised graph-level representation learning with hierarchical contrasts. Neural Networks258, pp. 359-368. Cited by: SS1, SS2.
* [46]W. Yu, Z. Yang, Y. Jin, Y. Zhang, Y. Jin, and Y. Zhang (2020) Unsupervised graph-level representation learning via mutual information maximization. In International Conference on Learning Representations, Cited by: SS1, SS2.
* [47]W. Yu, Z. Yang, Y. Jin, Y. Zhang, Y. Jin, Y. Zhang, and Y. Zhu (2020) Sub-graph contrast for scalable self-supervised graph representation learning. In 2020 IEEE international conference on data mining (ICDM), pp. 222-2215. Cited by: SS1, SS2.
* [48]W. Yu, Z. Yang, Y. Jin, Y. Zhang, Y. Jin, Y. Jin, Y. Shen, F. Sun, Z. Xiao, et al. (2020) A comprehensive survey on deep graph representation learning. Neural Networks, pp. 106207. Cited by: SS1, SS2.
* [49]W. Yu, Z. Yang, Y. Jin, Y. Zhang, Q. Long, Z. Qiao, Y. Qin, J. Shen, F. Sun, Z. Xiao, et al. (2024) A comprehensive survey on deep graph representation learning. Neural Networks, pp. 106207. Cited by: SS1, SS2.
* [50]W. Yu, Z. Yang, Y. Jin, Y. Zhang, Y. Jin, Y. Zhang, Y. Jin, Y. Shen, F. Sun, Z. Xiao, et al. (2020) A comprehensive survey on deep graph representation learning. Neural Networks, pp. 106207. Cited by: SS1, SS2.
* [51]W. Yu, Z. Yang, Y. Jin, Y. Zhang, Y. Jin, Y. Zhang, Y. Jin, Y. Jin, Y. Yang, Y. Jin, Y. Yang, et al. (2020) A comprehensive survey on deep graph representation learning. Neural Networks, pp. 106207. Cited by: SS1, SS2.
* [52]W. Yu, Z. Yang, Y. Jin, Y. Zhang, Y. Jin, Y. Zhang, Y. Jin, Y. Yang, Y. Jin, Y. Yang, et al. (2020) A comprehensive self-supervised graph representation learning. In 2020 IEEE international conference on data mining (ICDM), pp. 222-2215. Cited by: SS1, SS2.
* [53]W. Yu,* [54] C. Vignac, A. Loukas, and P. Frossard. Building powerful and equivariant graph neural networks with structural message-passing. In _Advances in Neural Information Processing Systems (NeurIPS)_ 2020.
* [55] N. Walz, A. A. Watson, and G. Karypis. Comparison of descriptor spaces for chemical compound retrieval and classification. _Knowledge and Information Systems_, 14(3):347-375, 2008.
* [56] J. Wang, M. Li, H. Wang, and Y. Pan. Identification of essential proteins based on edge clustering coefficient. _IEEE/ACM Transactions on Computational Biology and Bioinformatics_, 9(4):1070-1080, 2011.
* [57] A. Wiesinghe and Q. Wang. A new perspective on" how graph neural networks go beyond twelfie-lehman?. In _International Conference on Learning Representations_, 2012.
* [58] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip. A comprehensive survey on graph neural networks. _IEEE transactions on neural networks and learning systems_, 3(3):34-42, 2020.
* [59] Z. Wu, Y. Xiong, S. X. Yu, and D. Lin. Unsupervised feature learning via non-parametric instance discrimination. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 3733-3742, 2018.
* [60] Z. Xingyi and L. Chen. Capsule graph neural network. In _International conference on learning representations_, 2018.
* [61] D. Xu, W. Cheng, D. Lau, H. Chen, and X. Zhang. Infogcl: Information-aware graph contrastive learning. _Advances in Neural Information Processing Systems_, 34:3041-30425, 2021.
* [62] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural networks? In _International Conference on Learning Representations_, 2018.
* [63] P. Tsamardig and S. Vishwanathan. Deep graph kernels. In _Proceedings of the 21th ACM SIGKDD international conference on Knowledge discovery and data mining_, pages 1365-1374, 2015.
* [64] Y. Yin, Q. Wang, S. Huang, H. Xiong, and X. Zhang. Autopad: Automated graph contrastive learning via learnable view generators. In _Proceedings of the AAAI conference on artificial intelligence_, volume 36, pages 892-890, 2022.
* [65] J. You, J. Gomes-Soldan, R. Ying, and J. Leskovec. Identity-aware graph neural networks. In _AAAI Conference on Artificial Intelligence (AAAI)_, 2021.
* [66] Y. You, T. Chen, Y. Shen, and Z. Wang. Graph contrastive learning automated. In _International Conference on Machine Learning_, pages 12212-1232. PMLR, 2021.
* [67] Y. You, T. Chen, Y. Sui, T. Chen, Z. Wang, and Y. Shen. Graph contrastive learning with augmentations. _Advances in neural information processing systems_, 35:3518-3528, 2020.
* [68] Y. You, T. Chen, Z. Wang, and Y. Shen. L2-scar-wise and learned efficient training of graph convolutional networks. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 2127-2135, 2020.
* [69] J. Zbontar, L. Jing, M. Yao, L. Cun, and J. Desy. Badow twins: Self-supervised learning via redundancy reduction. In _International conference on machine learning_, pages 1230-1230. PMLR, 2021.
* [70] M. Zhang, Z. Cui, M. Neumann, and Y. Chen. An end-to-end deep learning architecture for graph classification. In _Proceedings of the AAAI conference on artificial intelligence_, volume 28, 2018.
* [71] M. Zhang and P. L. Needed graph neural networks. _Advances in Neural Information Processing Systems_, 34:1573-1574, 2021.
* [72] S. Zhang, Z. Hu, A. Subramonian, and Y. Sun. Motif-driven contrastive learning of graph representations. _arXiv preprint arXiv:2012.12533_, 2020.
* [73] Z. Zhang, P. Cui, and W. Zhu. Deep learning on graphs. _A survey. IEEE Transactions on Knowledge and Data Engineering_, 34(12):249-270, 2020.
* [74] Z. Zhang, M. Wang, Y. Xiang, Y. Huang, and A. Nehorai. Refig: Graph kernels based on return probabilities of random walks. _Advances in Neural Information Processing Systems_, 31, 2018.
* [75] Y. Zhu, Y. Xu, D. Lin, and S. Wu. An empirical study of graph contrastive learning. In _Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Brand)_, 2021.
* [76] Y. Zhu, Y. Xu, F. Yu, Q. Liu, S. Wu, and L. Wang. Graph contrastive learning with adaptive augmentation. In _Proceedings of the web conference 2021_, pages 2069-2080, 2021.

## Appendix A Appendix

### Laplacian Eigenvectors for Positional Encoding

Positional features should ideally differentiate nodes that are far apart in the graph while ensuring that nearby nodes have similar features. We use graph Laplacian eigenvectors as node positional features because they have fewer ambiguities and more accurately represent distances between nodes [15, 51]. Laplacian eigenvectors can embed graphs into Euclidean space, providing a meaningful local coordinate system while preserving the global graph structure. They are mathematically defined by the factorization of the graph Laplacian matrix as \(\mathbf{L}=\mathbf{U}\mathbf{\Lambda}\mathbf{U}^{H}\), where \(\mathbf{U}=\{\mathbf{u}_{i}\}_{i=1}^{m}\in\mathbb{R}^{m}\) are orthogonal eigenvectors, \(\mathbf{\Lambda}=diag\left([\mathbf{\lambda}_{1},\ldots,\mathbf{\lambda}_{m}] \right)\in\mathbb{R}^{n\times m}\) are real eigenvalues, and \(\mathbf{U}^{H}\) is a hermitian transpose of \(U\). After normalizing to unit length, eigenvectors are defined up to a factor of \(\pm 1\), leading to random sign flips during training. In our experiments, we employ the \(p\) smallest non-trivial eigenvectors, with \(p\) specified for each experiment. The initial positional encoding vector for each node is computed beforehand and assigned as node attributes during dataset creation.

### Proofs of Lemmas and Theorems

**Theorem 1** The following statement is true: (a) If \(\sum_{k}\mathbf{A}_{\mathbf{v}^{\prime}\in\mathcal{C}\mathit{v}\mathit{ele}}^{k}\), then \(\sum_{k}\mathbf{A}_{\mathbf{v}^{\prime}\neq\mathbf{v}^{\prime}}^{k}\), but not vice versa.

Proof.: The implication \(\sum_{k}\mathbf{A}_{\mathbf{v}^{\prime}\in\mathcal{C}\mathit{v}\mathit{ele}}^{k}\)\(\sum_{k}\mathbf{A}_{\mathbf{v}^{\prime}\neq\mathbf{v}}^{k}\)\(\Rightarrow\)

\(\sum_{k}\mathbf{A}_{\mathbf{v}^{\prime}\in\mathcal{C}\mathit{loved}\mathit{walk}}^{k}\)\(\sum_{k}\mathbf{A}_{\mathbf{v}^{\prime}\neq\mathbf{v}^{\prime}}^{k}\) is true because every cycle is a closed walk, but not every closed walk is a cycle. Thus, if two nodes are isomorphic with respect to cycles, they must also be isomorphic with respect to closed walks. The reverse implication \(\sum_{k}\mathbf{A}_{\mathbf{v}^{\prime}\in\mathcal{C}\mathit{loved}\mathit{ walk}}^{k}\)\(\sum_{k}\mathbf{A}_{\mathbf{v}^{\prime}\neq\mathbf{v}}^{k}\)\(\Rightarrow\)\(\sum_{k}\mathbf{A}_{\mathbf{v}^{\prime}\neq\mathbf{v}}^{k}\) is not true because closed walks can include walks that repeat vertices or edges, which do not qualify as cycles. 

**Theorem 2** Let S represent a GNN with an aggregation scheme \(\Phi\) delineated by Eq. 1-Eq. 4. 5 exceeds the expressiveness of 1-WL in identifying non-isomorphic graphs, provided that S operates over a sufficient number of hops, where \(k>1\), and also meets the following criteria:

1. \(\Phi\left(\mathbf{h}_{o}^{(t)},\big{\{}(\mathbf{A}_{\mathbf{v}u},\mathbf{h}_{o}^{(t )},\mathbf{e}_{\mathbf{u}u}^{b},\mathbf{e}_{\mathbf{u}u}^{c})\mathbf{u}\in \mathcal{N}(v)\big{\}},\big{\{}(\mathbf{A}_{\mathbf{v}u}^{k},\mathbf{h}_{o}^{(t )})\big{\}}|u\in\mathcal{N}^{k}(v)\big{\}},\big{\{}(\mathbf{A}_{\mathbf{v}u}^{k}, \mathbf{h}_{o}^{(t)})\big{\}}\right)\) is injective;
2. The graph-level readout function of \(S\) is injective.

Proof.: For the proof, we proceed in two steps. First, we assume the existence of two graphs \(G_{1}\) and \(G_{2}\) that are distinguishable by \(1\)-WL but indistinguishable by \(S\), and we demonstrate a contradiction. We consider the iterations of 1-WL from 1 to \(k\), where \(k\) is the number of hops. If 1-WL distinguishes \(G_{1}\) and \(G_{2}\) using the information up to the \(k\)-th iteration but \(S\) cannot, it implies the existence of k-hop local neighborhood subgraphs \(\mathcal{G}_{i}\) and \(\mathcal{G}_{j}\) with different multisets of \(\mathbf{H}\in\mathcal{H}\), \(\mathbf{W}_{1}\in\mathcal{W}_{1}\), \(\mathbf{W}_{2}\in\mathcal{W}_{2}\), \(\mathbf{W}_{3}\in\mathcal{W}_{3}\). However, by the injectiveness property of \(\Phi\), \(S\) should yield different tuple \((\mathbf{H},\mathbf{W}_{1},\mathbf{W}_{2},\mathbf{W}_{3})\) for \(\mathcal{G}_{i}\) and \(\mathcal{G}_{j}\), contradicting the assumption. In the second step, we prove the existence of at least two graphsdistinguishable by \(S\) but indistinguishable by 1-WL. This step involves providing specific examples of such graphs, illustrating \(S\)'s enhanced expressiveness compared to 1-WL. By completing these steps, we establish the validity of Theorem A.2, confirming that under the specified conditions, \(S\) indeed surpasses the expressiveness of 1-WL in identifying non-isomorphic graphs. 

**Lemma 1** Given two distinct pairs of multisets \(\mathbf{H}_{1},\mathbf{H}_{1}^{\prime}\in\mathcal{H}\), \(\mathbf{W}_{1},\mathbf{W}_{1}^{\prime}\in\mathbf{W}_{2},\mathbf{W}_{2}^{\prime} \in\mathcal{W}_{2},\mathbf{W}_{3},\mathbf{W}_{3}^{\prime}\in\mathcal{W}_{3}\), there exists a function \(f\) such that the aggregation function \(\pi(\mathbf{h}_{\text{o}},\mathbf{H}_{1},\mathbf{W}_{1},\mathbf{W}_{2},\mathbf{W }_{3})\) and \(\pi(\mathbf{h}_{\text{o}^{\prime}}^{\prime},\mathbf{H}_{1}^{\prime},\mathbf{W }_{1}^{\prime},\mathbf{W}_{2}^{\prime},\mathbf{W}_{3}^{\prime})\) defined as \(\pi(\mathbf{h}_{\text{o}},\mathbf{H}_{1},\mathbf{W}_{1},\mathbf{W}_{2},\mathbf{ W}_{3})=\sum_{\text{h}\in\mathbf{H}_{1},\mathbf{w}_{1}\in\mathbf{W}_{1}}f( \mathbf{h},\mathbf{w}_{1})+\sum_{\text{h}\in\mathbf{H}_{1},\mathbf{w}_{2}\in \mathbf{W}_{2}}f(\mathbf{h},\mathbf{w}_{2})\) and \(\pi(\mathbf{h}_{\text{o}^{\prime}}^{\prime},\mathbf{H}_{1}^{\prime},\mathbf{W }_{1}^{\prime},\mathbf{W}_{2}^{\prime},\mathbf{W}_{3}^{\prime})=\sum_{\text{h} \in\mathbf{H}_{1}^{\prime},\mathbf{w}_{1}\in\mathbf{W}_{1}^{\prime}}f(\mathbf{ h},\mathbf{w}_{1})+\sum_{\text{h}\in\mathbf{H}_{1}^{\prime},\mathbf{w}_{1}\in\mathbf{W}_{1}^{\prime}}f(\mathbf{h},\mathbf{w}_{1})+\sum_{\text{h}\in\mathbf{H}_{1}^{\prime},\mathbf{w}_{2}\in\mathbf{W}_{2}^{\prime}}f(\mathbf{h},\mathbf{w}_{2})\) are unique, respectively._

Proof.: Since \(\mathcal{H}\), \(\mathcal{W}_{1}\), \(\mathcal{W}_{2}\), and \(\mathcal{W}_{3}\) are countable, there must exist four functions \(\psi_{1}:\mathcal{H}\rightarrow\mathbb{N}_{odd}\) mapping \(\text{h}\in\mathcal{H}\) to odd natural numbers and \(\psi_{2},\psi_{3}\), and \(\psi_{4}\) mapping elements from \(\mathcal{W}_{1}\), \(\mathcal{W}_{2}\), and \(\mathcal{W}_{3}\) to even natural numbers, respectively. For any pair of multisets \((\mathbf{h}_{\text{o}},\mathbf{H}_{1},\mathbf{W}_{1},\mathbf{W}_{2},\mathbf{W }_{3})\), given that the cardinalities of \(\mathbf{H}_{1},\mathbf{W}_{1},\mathbf{W}_{2}\), and \(\mathbf{W}_{3}\) are bounded, there must be a natural number \(N\) such that \(|\mathbf{H}_{1}|<N\), \(|\mathbf{W}_{1}|<N\), \(|\mathbf{W}_{2}|<N\), and \(|\mathbf{W}_{3}|<N\). Consider a prime number \(p>4N\). Define the function \(f\) such that \(f(\mathbf{h},\mathbf{w}_{1},\mathbf{w}_{2},\mathbf{w}_{3})=p-\psi_{1}(\mathbf{h})+p-\psi_{2}(\mathbf{w}_{1})+p-\psi_{3}(\mathbf{w}_{2})+p-\psi_{4}(\mathbf{w}_{3})\). Then, the aggregation functions \(\pi(\mathbf{h}_{\text{o}},\mathbf{H}_{1},\mathbf{W}_{1},\mathbf{W}_{2},\mathbf{W }_{3})\) and \(\pi(\mathbf{h}_{\text{o}^{\prime}}^{\prime},\mathbf{H}_{1}^{\prime},\mathbf{W }_{2}^{\prime},\mathbf{W}_{3}^{\prime})\) are unique for each distinct pair of multisets because the sum of these functions will be unique for distinct pairs of multisets by the properties of prime numbers and the unique mappings \(\psi_{1},\psi_{2},\psi_{3}\), and \(\psi_{4}\). 

**Lemma 2** Expanding upon Lemma 1, we introduce an extended aggregation function \(\pi(\mathbf{h}_{\text{o}},\mathbf{H}_{1},\mathbf{W}_{1},\mathbf{W}_{2},\mathbf{W }_{3})\), which incorporates the feature vector of the central node \(h_{\text{o}}\) and the multisets \(\mathbf{H}\in\mathcal{H}\), \(\mathbf{W}_{1}\in\mathcal{W}_{1},\mathbf{W}_{2}\in\mathcal{W}_{2}\), and \(\mathbf{W}_{3}\in\mathcal{W}_{3}\). There exists a function \(f\) such that \(\pi(\mathbf{h}_{\text{o}},\mathbf{H}_{1},\mathbf{W}_{1},\mathbf{W}_{2},\mathbf{W }_{3})=(1+e)f(\mathbf{h}_{\text{o}})+\sum_{\text{h}\in\mathbf{H}_{1},\mathbf{w}_{1}}f(\mathbf{h},\mathbf{w}_{1})+\sum_{\text{h}}\left(f(\mathbf{h}_{\text{o}},\mathbf{w}_{3})+\sum_{\text{h}\in\mathbf{H}_{1},\mathbf{w}_{2}}f(\mathbf{h},\mathbf

**Theorem 3** (StructPosGSSL is more expressive than subgraph MPNNs in distinguishing certain non-isomorphic graphs.

To prove Theorem 3, we consider the two non-isomorphic graphs \(G_{1}\) and \(G_{2}\) shown in Figure 4. Let \(v\in G_{1}\) and \(v^{\prime}\in G_{2}\) be the middle nodes in each graph. In the case of Subgraph MPNNs (e.g., [12, 65, 71]), the aggregation function over the neighborhoods of \(v\) and \(v^{\prime}\) fails to differentiate between the two nodes. This is because Subgraph MPNNs rely on local subgraphs, and the structural features and neighborhood-based information are symmetric for \(v\) and \(v^{\prime}\).

Let us first consider the structural encoder (i.e., GenHopNet) with only closed-walk information up to \(k=3\), without EB attributes \(e^{c}_{uv}\). In this case, the node representations for \(v\) and \(v^{\prime}\) generated by the structural encoder using closed-walks are identical, since \(\{(A_{uv},h^{(t)}_{u,v},e^{b}_{uv})|u\in N(v)\}=\{(A_{v^{\prime}u^{\prime}},h^ {(t)}_{u^{\prime}v^{\prime}}),e^{b}_{uv^{\prime}v^{\prime}}\}|u^{\prime}\in N(v^ {\prime})\}\), \(\{(\tilde{A}^{c}_{uv},h^{(t)}_{u^{\prime}v})|u\in N^{c}(v)\}=\{(\tilde{A}^{c}_{ uv^{\prime}u^{\prime}},h^{(t)}_{u^{\prime}v})|u^{\prime}\in N^{c}(v^ {\prime})\}\), and \(\{(\tilde{A}^{c}_{uv},h^{(t)}_{u^{\prime}v})\}=\{(\tilde{A}^{c}_{v^{\prime}u^{ \prime}},h^{(t)}_{u^{\prime}v})\}\). This indicates that the closed-walk information alone is insufficient to distinguish these non-isomorphic graphs. Now, we show how _StructPosGSSL_, when enhanced with positional encodings and \(e^{c}_{uv}\), can differentiate between the non-isomorphic graphs \(G_{1}\) and \(G_{2}\). Let \(h^{(t)}_{u,pos}\) be the positional encoding of node \(u\). When positional encodings are combined with \(e^{c}_{uv}\), the aggregation function can distinguish these non-isomorphic graph pairs. Using Lemmas 1 and 2, we know that the aggregation function is still injective when positional encodings and \(e^{c}_{uv}\) are included. Thus, for the middle nodes of each graph, we have \(\{(A_{uv},h^{(t)}_{u,pos},e^{b}_{uv},e^{c}_{uv})|u\in N(v)\}\neq\{(A_{v^{\prime}u ^{\prime}},h^{(t)}_{u^{\prime}v^{\prime}},e^{b}_{uv^{\prime}v^{\prime}},e^{c}_ {u^{\prime}v^{\prime}})|u^{\prime}\in N(v^{\prime})\}\). Therefore, the _StructPosGSSL_ with positional encodings and EB attributes yields different representations for \(v\) and \(v^{\prime}\), even though they were previously indistinguishable. 

### Ablation Analysis of Loss Function

To showcase the effectiveness of each element in the loss function, we perform an ablation study on the following variants:

* NoVICReg: This variant excludes the VICReg regularization term from the overall loss.
* Inv: This variant keeps only the Invariance term in the VICReg regularization term.
* Var: This variant keeps only the Variance term in the VICReg regularization term.
* Cov: This variant keeps only the Covariance term in the VICReg regularization term.

We conducted the ablation study on the _StructPosGSSL-SA_ variant. The results shown in Table 6 indicate that the Invariance, Variance, and Covariance terms are crucial to the performance. Specifically, the covariance term has the greatest impact on performance, whereas the invariance term has the least effect across all datasets, as detailed in Table 6. Specifically, as shown in Table 6, performance on graphs decreases by 3.5% to 5.1% with the NT-Xent+NoVICReg loss function, by 2.4% to 4.1% with NT-Xent+Inv, by 1.7% to 3.0% with NT-Xent+Var, and by 1.0% to 1.9% with NT-Xent+Cov, compared to the combined NT-Xent+VICReg loss function.

### Comparison under Different \(\mu\)

To evaluate the impact of the regularization term \(\mu\) on the performance of our _StructPosGSSL_ framework, we conduct experiments by evaluating _StructPosGSSL-SA_ across six datasets (MUTAG, PTC-MR, PROTEINS, IMDB-B, IMDB-M, and RDT5K), using varying values for the regularization term \(\mu=\{0.1,0.2,\ldots,0.9\}\). For this experimental setup, we use the same hyperparameter configuration for each dataset as described in section 6.1. Figure 5 represents the experimental results. In our experiments, we observed that setting \(\mu\) either too low or too high leads to suboptimal performance. To achieve better results, it is essential to select an intermediate value for \(\mu\), as this provides a balance that optimizes the _StructPosGSSL_ performance.

Figure 4. A pair of non-isomorphic graphs where the colored square box on each node represents the feature representations derived from closed-walk information. The two middle nodes (colored gray) in graphs \(G_{1}\) and \(G_{2}\) cannot be distinguished using only closed-walk information (up to \(k=3\)), as they receive the same representation (colored blue). However, by incorporating positional information along with EB attributes, we can successfully distinguish these nodes.

Figure 5. Accuracy (%) of StructPosGSSL-SA under different \(\mu\) values.