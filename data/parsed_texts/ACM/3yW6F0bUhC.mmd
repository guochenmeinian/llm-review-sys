List-aware Reranking-Truncation Joint Model for Search and Retrieval-augmented Generation

Anonymous Author(s)

###### Abstract.

The results of information retrieval (IR) are usually presented in the form of a ranked list of candidate documents, such as web search for humans and retrieval-augmented paradigm for large language models (LLMs). List-aware retrieval aims to capture the list-level contextual features to return a better list, mainly including reranking and truncation. Reranking finely re-scores the documents in the list. Truncation dynamically determines the cut-off point of the ranked list to achieve the trade-off between overall relevance and avoiding misinformation from irrelevant documents. Previous studies treat them as two separate tasks and model them separately. However, the separation is not optimal. First, it is hard to share information between the two tasks. Specifically, reranking can provide fine-grained relevance information for truncation, while truncation can provide utility requirement for reranking. Second, the separate pipeline usually meets the error accumulation problem, where the small error from the reranking stage can largely affect the truncation stage. To solve these problems, we propose a Reranking-Truncation joint model (GenRT) that can perform the two tasks concurrently. GenRT integrates reranking and truncation via generative paradigm based on encoder-decoder architecture. We also design the novel loss functions for joint optimization to make the model learn both tasks. Sharing parameters by the joint model is conducive to making full use of the common modeling information of the two tasks. Besides, the two tasks are performed concurrently and co-optimized to solve the error accumulation problem between separate stages. Experiments on public learning-to-rank benchmarks and open-domain Q&A tasks show that our method achieves SOTA performance on both reranking and truncation tasks for web search and retrieval-augmented LLMs. To the best of our knowledge, this is the first work that discusses list-aware retrieval (esp. truncation task) in retrieval-augmented LLMs.

Reranking, Truncation, Retrieval-augmented LLMs +
Footnote †: isbn: 978-xxxxx-xxxx-x-xYY/MM.

+
Footnote †: isbn: 978-xxxxx-xxxx-xYY/MM.

+
Footnote †: isbn: 978-xxxxx-xxxx-xYY/MM.

+
Footnote †: isbn: 978-xxxxx-xxxx-xYY/MM.

+
Footnote †: isbn: 978-xxxxx-xxxx-xYY/MM.

+
Footnote †: isbn: 978-xxxxx-xxxx-xYY/MM.

+
Footnote †: isbn: 978-xxxxx-xxxx-xYY/MM.

+
Footnote †: isbn: 978-xxxxx-xxxx-xYY/MM.

+
Footnote †: isbn: 978-xxxxx-xxxx-xYY/MM.

+
Footnote †: isbn: 978-xxxxx-xxxx-xYY/MM.

+
Footnote †: isbn: 978-xxxxx-xxxx-xYY/MM.

+
Footnote †: isbn: 978-xxxxx-xxxx-xYY/MM.

## 1. Introduction

In information retrieval (IR), even though the ranking methods based on probability ranking principle (Rapidakis et al., 2016) (PRP) that assumes the relevance of each document is modeled independently for a query have been widely used (Kumar et al., 2016; Krizhevsky et al., 2012; Lee et al., 2012), many studies have shown that users' feedback of the retrieval result is based on the entire returned list (Krizhevsky et al., 2012; Lee et al., 2012; Lee et al., 2012). In web search, humans usually compare multiple documents in the list before clicking. In retrieval-augmented paradigm for LLMs, LLMs process the documents in the list via self-attention (Krizhevsky et al., 2012) and select the information for generation (Krizhevsky et al., 2012; Lee et al., 2012; Lee et al., 2012). It has been proven that the performance of the retrieval-augmented LLMs is affected by the length of the retrieved list and the arrangement of documents within the list provided in the prompt (Krizhevsky et al., 2012).

Therefore, list-aware retrieval models (Krizhevsky et al., 2012) are proposed as the post-processing stage of IR, which are used to capture the list-level contextual features. List-aware retrieval mainly includes reranking and truncation. Reranking exploits list-level contextual features to re-score each document. Truncation dynamically determines the cut-off point of the list to achieve the optimal trade-off between overall relevance and weeding out irrelevant documents, which is meaningful to improve retrieval efficiency and avoid misinformation (Krizhevsky et al., 2012; Lee et al., 2012). Truncation is important for the domains that need users to use the high cost to judge the relevance of documents (Krizhevsky et al., 2012). It is also important for retrieval-augmented LLMs. Because the

Figure 1. Problems of the separation of reranking and truncation. Separate pipeline leads to the error accumulation problem between two stages and the loss of relevant documents. Similar reranking but results in different truncations.

Figure 2. Comparison with previous methods.

performance of LLMs fluctuates with the number of retrieved documents, while for different prompts, the suitable number of retrieved documents changes dynamically. Blindly increasing the number of retrieved documents will not always improve performance, but will affect the efficiency of LLMs and introduce noise (Zhu et al., 2017).

Previous methods model reranking and truncation separately (Bishop, 2006; Lee et al., 2016; Wang et al., 2017; Wang et al., 2017), first rerank and then truncate. Although LeCut (Luo et al., 2017) exchanges features between the two models in training, it still treats them as separate models and stages during inference. This leads to several problems. **First**, these two tasks are interdependent but the separation makes it hard to exploit the information shared between them. Document relevance modeled by reranking can provide an important basis for truncation. Trade-off characterization of the relevance and position of documents in the list modeled by truncation provides important contextual interaction information for reranking. **Second**, the separate pipeline usually meets the error accumulation problem, where the error from the reranking stage can affect the truncation stage largely, which cannot be directly optimized during training. There are two factors that cause this. **1)** Inconsistent document relevance judgment in two separate stages. As shown in list (b) of Figure 1, the reranking model mistakenly thinks that \(B\) and \(E\) are highly relevant, but the truncation model thinks they are irrelevant and thus truncates the list at top-ranked position. **2)** Reranking and truncation have different concerns to ranking list. Truncation is more sensitive to the ranking performance of the top-ranked documents because once too many irrelevant documents appear at the top of list, it causes the list to be truncated at these documents to lose relevant documents that are ranked behind it. But reranking focuses on the overall ranking performance of the entire list and is not sensitive to the case that irrelevant documents appear at the top of list. Figure 1 shows that although (a) and (b) have similar reranking performance (0.89 and 0.90), two irrelevant top-ranked documents (\(B\) and \(E\)) of (b) result in the worse result of reranking-truncation pipeline than (a) (19.4 \(<\) 25.8).

To solve the above problems, it is necessary to get a joint model to perform them concurrently. However, there are several challenges. First, how to make the two tasks share the modeling information effectively (**C1**). Second, reranking is a process of dynamically changing the ranking list. However, the truncation decision needs to be based on a static list, how to perform them concurrently (**C2**). Third, how to design loss functions for joint learning (**C3**).

In this paper, we propose a Reranking-Truncation joint model via sequence generation called GenRT (shown in Figure 2). The input of GenRT is a ranked list, and GenRT can perform reranking and truncation concurrently to directly output the final list that has been reranked and truncated. Specifically, to address **C1**, we design the global dependency encoder to provide global list-level contextual features within ranked lists that can be shared by reranking and truncation. To address **C2**, different from the mainstream ranking model that ranks documents by estimating the score of documents, GenRT outputs the final reranked list step by step in the paradigm of sequence generation. At each time step, the document at the current ranking position is selected according to the previous state and the current candidate set, and the local optimal truncation decision is made at the same time. Truncation is transformed into a binary classification task based on the forward and backward sequential information of the dynamic list at each step. Sequence generation paradigm records the forward information and we also introduce the local backward window to provide the backward information. In this way, our model can combine dynamic reranking with static truncation. To address **C3**, we design step-adaptive attention loss and step-by-step lambda loss and combine them as the objective function for reranking. We introduce the reward augmented maximum likelihood (RAML (Shen et al., 2017)) to design the RAML-based soft criterion as the loss function for truncation at each step.

To sum up, our contributions are: (1) We point out the problem of separating reranking and truncation in list-aware retrieval and propose that these two tasks can be concurrently done with a joint model. (2) We propose the novel model, inference paradigm, and loss function to jointly optimize and perform reranking and truncation on only one model. (3) Experimental results on public learning-to-rank benchmarks and open-domain Question-answer tasks show that our method achieves state-of-the-art performance on both reranking and truncation tasks for web search and retrieval-augmented LLMs. The code will be released on GitHub.

## 2. Related Work

### Reranking in List-aware Retrieval

Reranking in list-aware retrieval exploits list-level contextual features to re-score and rank each document in the list. DLCM (Bishop, 2006) uses recurrent neural network (RNN) to encode the contextual information of the list and reranks the documents. GSF (Bishop, 2006) proposes a multivariate scoring function framework to score the document affected by other documents. SetRank (Shen et al., 2017) employs multi-head self-attention to capture interaction information within the list. PRM (Shen et al., 2017) optimizes personalized recommendations by capturing user-personalized information. IRGPR (Zhu et al., 2017) employs GNN to capture the relationship between candidate items. DASALC (Shen et al., 2017) further explores neural IR models from data augmentation perspective. SRGA (Shen et al., 2017) proposes a scope-aware reranking model with gated attention. MIR (Shen et al., 2017) considers the dynamic interaction between the user behavior and the candidate set. (Shen et al., 2017; Wang et al., 2017; Wang et al., 2017; Wang et al., 2017) exploit counterfactual signals for re-score. Different from them, we use the sequence generation method to directly generate a reranked list. Although Seq2Slate (Bishop, 2006) and Globalrerank (Shen et al., 2017) also use the similar generative method, they do not satisfy the permutation-invariant (Shen et al., 2017). The most prominent difference between previous studies is that our method can not only be applied to single reranking task but also perform reranking and truncation concurrently.

### Truncation in List-aware Retrieval

Truncation aims to determine the best cut-off point for the input ranked list to achieve the optimal trade-off between overall relevance and weeding out irrelevant documents. Recently, some work uses machine learning methods to solve the truncation problem. (Chen et al., 2018) investigate machine learning approaches for learning dynamic cut-offs within cascade-style IR systems. BiCut (Li et al., 2019) leverages bidirectional LSTM to find the best truncation point. Choppy (Choppy, 2018) uses transformer to model the input ranked list. AttnCut (Wang et al., 2017) uses RAML (Shen et al., 2017) to make the model optimize user-defined metric directly and smoothly. LeCut (Luo et al., 2017) passes the relevance information of the ranking model to the truncation model during training time and iteratively trains between ranking and truncation. Differentfrom them, we focus on jointly modeling truncation and reranking. We transform truncation into a step-by-step binary classification task and leverage the paradigm of sequence generation to combine dynamic reranking with static truncation and design binary classification soft criterion as the optimization object for truncation.

## 3. Method

The overall architecture of GenRT is shown in Figure 3. GenRT aims to jointly model reranking and truncation by a shared model and perform these two tasks concurrently during the inference. The most critical challenge to achieve this is that reranking dynamically changes the ranked list, while the truncation decision needs to be based on a static list. To address the challenge, GenRT adopts encoder-decoder architecture consisting of a global dependency encoder and a sequential dependency decoder. Global dependency encoder is used to capture the global list-level contextual features of the input list by multi-head self-attention (MHSA) (Wang et al., 2019), which can be shared by the reranking and truncation. Sequential dependency decoder generates the final list step-by-step with decreasing relevance and makes truncation decision at each step based on the bidirectional sequential information. This sequence generation paradigm combines dynamic reranking with static truncation, which can address the critical challenge. Details are introduced below.

### Global Dependency Encoder

Global dependency encoder captures the list-level contextual features within the input list. As for the input of the encoder, each document in the input list is represented as an embedding. As shown in Figure 4, given a query \(g\), a document list \(D=\{d_{1},d_{2},...,d_{N}\}\), and initial ranking score list \(L=\{l_{1},l_{2},...,l_{N}\}\) obtained from previous ranking stage (e.g. retrieval), input embedding for document \(d_{i}\) is:

\[\mathbf{u}_{d_{i}}=f(q,d_{i},l_{i}),\mathbf{u}_{d_{i}}\in\mathbb{R}^{Z},i\in[1,N]. \tag{1}\]

\(N\) is the number of documents in the input list, \(f\) is used to fuse the features of \(q\), \(d_{i}\) and the initial ranking feature \(l_{i}\). Specifically, for the feature-based ranking tasks such as MSLR (input data is the learning-to-rank feature), we follow (Brockman et al., 2017; Chen et al., 2018) to use the traditional learning-to-rank method to extract the features (matching, pagerank, etc) between \(q\) and \(d_{i}\) as described in SetRank (Wang et al., 2019) and concatenate the features with the ranking score \(l_{i}\) to obtain \(\mathbf{u}_{d_{i}}\). For the text-based ranking tasks such as Natural Questions (Zhou et al., 2019) (input data is text), we use the output embedding of [CLS] token in the interaction-based ranking model as the representation (\(C(q,d_{i})\)) for \(q\) and \(d_{i}\). The score \(l_{i}\) is mapped to a learnable position embedding \(\mathbf{I}_{\mathbf{p}_{i}}\in\mathbb{R}^{Z}\) according to its rank in \(L\). Then, \(C(q,d_{i})\) and \(\mathbf{I}_{\mathbf{p}_{i}}\) are added element-wise to obtain \(\mathbf{u}_{d_{i}}\). The embeddings corresponding to the documents in the list are concatenated to get \(U\in\mathbb{R}^{N\times Z}\), and the matrix of vectors for the input list \(L\) can be represented as:

\[\mathbf{U}=[\mathbf{u}_{d_{i}},\mathbf{u}_{d_{i}},...,\mathbf{u}_{d_{N}}]^{T}. \tag{2}\]

Transfer layer is used to map \(\mathbf{U}\) to the vector space of multi-head self-attention (MHSA) (Wang et al., 2019) and align the dimensions:

\[\mathbf{X}=Swish(\text{MLP(U)}),\mathbf{X}\in\mathbb{R}^{N\times E}, \tag{3}\]

where MLP is multilayer perceptron, \(Swish\) is the activation function that is shown to have stronger generalization (Wang et al., 2019), \(E\) is the dimension of MHSA. In order to retain the feature of the document itself while capturing the list-level contextual features, we adopt

Figure 4. (a) Global Dependency Encoder and (b) Sequential Dependency Decoder at T-th step.

Figure 3. Overview of GenRT. Global dependency encoder captures the features of the initial list. Sequential dependency decoder generates the final list step-by-step with decreasing relevance and concurrently makes truncation decision.

the residual connection method as:

\[\mathbf{O}=\mathbf{X}+\text{MHSA}(\text{LN}(\mathbf{X})), \tag{4}\]

where \(\mathbf{O}=[\mathbf{o}_{d_{1}},\mathbf{o}_{d_{2}},...,\mathbf{o}_{d_{N}}]^{T}\) is the output of global dependency encoder. \(\mathbf{o}_{d_{i}}\) contains the list-level contextual features in the ranked list and the input feature of \(d_{i}\), which can be shared by reranking and truncation. LN is layer normalization (Bishop, 2006).

### Sequential Dependency Decoder

Sequential dependency decoder follows the paradigm of generating sequences to generate the final list step-by-step with decreasing relevance and makes truncation decision at each step based on the bidirectional sequential information. In Figure 4, we show the operation of the decoder at the \(T\)-\(th\) step. Multi-head self-attention captures the interaction information of the document sequence from 1 to \(T-1\) steps of the reranked list. Previously generated documents from 1 to \(T-1\) steps serve as sequential dependency information to facilitate the selection of subsequent relevant documents (Han et al., 2016; Wang et al., 2016). Cross ranking FFN and dynamic ranking module determine the dynamic ranking list and select the best output document at each step. The ordinal number of the step is the rank of its output document in the final reranked list. At the same time, the truncation module makes truncation decision based on the bidirectional sequential information obtained by the dynamic ranking module and local backward window at each step.

We describe the operation mechanism of the decoder at the \(T\)-\(th\) step in detail. Given an input document list \(D^{\prime}=\{r_{1}^{1},r_{1}^{2},...,r_{T_{1}^{-1}}^{T-1}\}\) (\(r_{1}^{t}\) is the output document at step \(t\)) from 1 to \(T-1\) (the documents that have been reranked), the representation vectors \(\mathbf{U}^{{}^{\prime}}\) and \(\mathbf{X}^{{}^{\prime}}\) can be obtained according to Equ.(1)(2)(3) with the same parameters. MHSA is used to capture the list-level contextual features of the reranked list from 1 to \(T-1\) and it can be used as the sequential dependency for the current step:

\[\text{MHSA}(\mathbf{X}^{{}^{\prime}})\rightarrow[\mathbf{m}_{r_{1}^{t}}^{*}, \mathbf{m}_{r_{1}^{t}}^{*},...,\mathbf{m}_{r_{T}^{t-1}}]^{T}.\]

Cross ranking FFN estimates the generation score for each document to select the best output document at the current generating step, which is the core module of the decoder. The input of this module comes from decoder and encoder. Specifically, the input from the decoder side is \(\mathbf{m}_{r_{1}^{T-1}}^{*}\) from MHSA(\(\mathbf{X}^{{}^{\prime}}\)), which is the sequential dependency information at \(T\)-\(th\) step. Expand \(\mathbf{m}_{r_{1}^{T-1}}\in\mathbb{R}^{E}\) to the matrix \(\mathbf{M}\in\mathbb{R}^{N\times E}\), each row of \(\mathbf{M}\) is \(\mathbf{m}_{r_{1}^{T-1}}\). The input from the encoder side is \(\mathbf{I}\in\mathbb{R}^{N\times E}\) that can be obtained by latent cross (Han et al., 2016; Wang et al., 2016):

\[\mathbf{I}=(1+\text{MLP}(\mathbf{O}))\odot\text{FFN-Swish}(\mathbf{U}), \tag{5}\]

where \(\mathbf{U}\) and \(\mathbf{O}\) are obtained from Equ. (2) and (4) respectively, MLP is multilayer perceptron, FFN-Swish is the block of MLP and Swish, \(\odot\) is the element-wise multiplication operator. \(\mathbf{I}\) is the embedding matrix of the candidate document set at the current step. \(\mathbf{M}\) is the sequential dependency matrix of the current step and is used to interact with the embedding of each document in \(\mathbf{I}\) to get the score. Specifically, \(\mathbf{I}\) and \(\mathbf{M}\) are concatenated and processed by a row-wise FFN (rFFN) to get the predicted score of each document in the candidate set at the current step:

\[\mathbf{S}=\text{rFFN}(\text{Concat}(\mathbf{I},\mathbf{M})),\mathbf{S}\in \mathbb{R}^{N}. \tag{6}\]

Dynamic ranking module masks the documents in steps 1 to \(T-1\) (avoid selecting duplicate documents) and ranks the remaining candidate documents according to \(\mathbf{S}\) in descending order to get the ranking list \(R=\{r_{1}^{T},r_{2}^{T},...,r_{N-T+1}^{T}\}\) at the current step. The generated document at current step is the Top-1 element in this list (i.e., \(r_{1}^{T}\)). Document generation is finished and the next is truncation.

Previous truncation models need to be performed on a static list. However, reranking is a process of dynamically changing the ranking list, which is the challenge for the joint model to perform reranking and truncation concurrently (\(\mathbf{C2}\) in Section 1). To address this challenge, we transform truncation into a binary classification task at each step. Truncation module aggregates forward and backward information of the current generated document (\(r_{1}^{T}\)) to make truncation decision. Specifically, the module records the embedding sequence of the selected documents (\(D\)) in \(\mathbf{O}\) (Equ. 4) from 1 to T-1 as the forward information:

\[\mathbf{F}=[\mathbf{o}_{r_{1}^{t}},\mathbf{o}_{r_{1}^{t}},...,\mathbf{o}_{r_{T }^{t-1}}]^{T},\]

which is the sequence of the documents that precede the document output at the current step (\(r_{1}^{T}\)) in the reranked list. Reranking-Truncation joint model has to complete the truncation decision when reranking. However, the reranked list is generated step by step with decreasing relevance, when outputting the current document \(r_{1}^{T}\), the model cannot capture the backward information of the documents ranked behind \(r_{1}^{T}\) in the final reranked list. To address it, local backward window is proposed to select \(\beta\) documents behind \(r_{1}^{T}\) in the ranking list \(R\) at the current step and gets the corresponding embedding sequence from \(\mathbf{O}\).

\[\mathbf{B}=[\mathbf{o}_{r_{2}^{T}},\mathbf{o}_{r_{3}^{T}},...,\mathbf{o}_{r_{ \beta+1}^{T}}]^{T}.\]

The reason why only \(\beta\) documents are selected is that \(R\) is only a local ranking list of the current step and cannot represent the global result of reranking, selecting all the remaining documents will introduce noise. Embedding sequences \(\mathbf{F},\mathbf{o}_{r_{1}^{T}}\), and \(\mathbf{B}\) are concatenated to \(\mathbf{G}=\text{Concat}(\mathbf{F},\mathbf{o}_{r_{1}^{T}},\mathbf{B})\) as the input of truncation module.

To distinguish between forward and backward information and the position of document embedding in \(\mathbf{G}\), we introduce relative position encoding into MHSA like T5 (Han et al., 2016) for truncation module. Specifically, attention calculation with relative position encoding for the \(a\)-\(th\) and \(b\)-\(th\) vectors of the input is \(\mathbf{H}_{a}\mathbf{W}^{0}(\mathbf{H}_{b}\mathbf{W}^{K})^{T}+pos_{a,b}\), where \(\mathbf{W}^{Q},\mathbf{W}^{K}\) are matrices in MHSA, \(\mathbf{H}_{a}\) and \(\mathbf{H}_{b}\) are embeddings of input, \(pos_{a,b}=bucket(a-b)\), _bucket_ is a bucketing function. We call this module MHSA\(pos\), which is used to aggregate the bidirectional sequential information at the current step and make the truncation decision at \(r_{1}^{T}\). The result of the truncation decision for step \(T\) (\(r_{1}^{T}\)) can be obtained by:

\[\text{MHSA}_{pos}(\mathbf{G}) \rightarrow[\mathbf{j}_{r_{1}^{t}}^{*},...,\mathbf{j}_{r_{1}^{T}} ^{*},\mathbf{j}_{r_{1}^{T}}^{*},...,\mathbf{j}_{r_{\beta+1}^{T}}^{*}]^{T},\] \[\mathbf{P}=\text{Softmax}(\text{MLP}(\mathbf{j}_{r_{1}^{T}})), \mathbf{P}\in\mathbb{R}^{2}, \tag{7}\]

\(\mathbf{P}=[p_{0},p_{1}]\) is a binary probability distribution representing the probability of truncating or not at the current step (i.e., at \(r_{1}^{T}\)). If the decision is truncating, GenRT directly returns the documents generated at steps 1 to \(T\) as the final reranked and truncated list, if not, the model continues to execute until the decision is truncating or the reranking of all documents is completed.

[MISSING_PAGE_FAIL:5]

relevance of documents. In later epochs, the model learns to rerank and truncate alternately in batches. When the model learns to rerank, the parameters of truncation module are fixed and \(\mathcal{L}_{R}\) is the objective. When the model learns to truncate, the parameters of cross ranking FFN are fixed and \(\mathcal{L}_{T}\) is the objective.

In inference, the reranked list is generated step by step and the ordinal number of the step is the rank of the document in final list. At each generation step, the model selects the best output document at current position based on the global and sequential dependency and makes the truncation decision concurrently. The final generated sequence is the reranked and truncated list with decreasing relevance. GenRT can be applied to scenarios that only require reranking or truncation. When the IR system does not need truncation, the truncation result can be not considered directly. When the IR system does not need reranking, the scoring matrix \(\mathbf{S}\) at each step can be obtained from the input ranked list. For the scenarios that only require reranking, we propose an acceleration strategy that balances latency and accuracy. In training, it follows the generation paradigm step by step as described above. In inference, it directly uses the trainable vector \(start\) as the sequential dependency and uses the scoring matrix \(\mathbf{S}\) at the first step as the reranking results without generating sequence.

## 4. Experiments

### Experiment Settings

_Datasets_. Datasets in our experiments can be divided into two categories: (1) Learning-to-rank public benchmarks for web search including Microsoft LETOR 30K (MSLR30K) (Sidney et al., 2016), Yahoo! LETOR set (1Yahoo!)2 and Istella LETOR (Istella)3. These three datasets are collected from real search engines and they are feature-based. Each sample in these datasets is a feature vector, and the label has five-level relevance annotation from 0 (irrelevant) to 4 (perfectly relevant). (2) Open-domain Question-Answering datasets including Natural Questions (Xu et al., 2019) and TriviaQA (Xu et al., 2019). These two datasets are text-based and the label has 2-level relevance annotation from 0 (irrelevant) to 1 (relevant). They are used to measure the reranking and truncation models on retrieval-augmented LLMs.

Footnote 2: [http://learningformakchallenge.yahoo.com](http://learningformakchallenge.yahoo.com)

Footnote 3: [http://blog.istella.it/ststella-learning-to-rank-dataset/](http://blog.istella.it/ststella-learning-to-rank-dataset/)

_Baslines and Evaluation Metrics_. We select the SOTA models for reranking and truncation respectively as the baselines. For reranking, we select the following SOTA list-aware reranking models: **GlobalRerank**(Sidney et al., 2016), **DLCM**(Chen et al., 2019), **Seq2Slate**(Chen et al., 2019), **GSF**(Chen et al., 2019), **PRM**(Sidney et al., 2016), **SetRank**(Sidney et al., 2016), **CRUM**(Sidney et al., 2016), **SRGA**(Sidney et al., 2016). DASALC (Sidney et al., 2016) is not compared because it is a much larger model (50 times the number of parameters of GenRT). Methods based on gradient boosting tree are not considered because they can not be applied to text-based data. For truncation, **BiCut**(Sidney et al., 2016), **Choppy**(Chen et al., 2019), **AttnCut**(Sidney et al., 2016) and **LeCut**(LeCun et al., 2019) are selected as the baselines. We also introduce **Fixed-\(x\)** that truncates the given ranking list at the fixed position \(x\). Some retrieval-augmented works such as FID, REALM and Retro (Chen et al., 2019; Chen et al., 2019) are not considered. Because they need to train language models. Our work only focuses on the training of the IR models and does not require the training of language models, which can be flexibly compatible with black-box LLMs.

For the metrics on three web search datasets, Normalized Discounted Cumulative Gain (NDCG) (Li et al., 2019), Expected Reciprocal Rank (ERR) (Chen et al., 2019) and Mean Average Precision (MAP) are used to evaluate the performance of reranking (Chen et al., 2019; Li et al., 2019). Following (Sidney et al., 2016), TDCG (Equ.(10)) are used to evaluate the performance of truncation. \(\gamma\) in TDCG (Equ. (10)) means that if the label is 0, outputs -4, if the label is 1, outputs -2, otherwise (2, 3, 4) outputs the label itself.

For the metrics on retrieval-augmented LLMs, improving the performance of LLMs is the ultimate goal of the retrieval system, so we use the accuracy of LLMs in answering open-domain questions as the evaluation metric. Since the labels in open-domain QA datasets are binary categories, \(\gamma\) in TDCG (Equ. (10)) is set that if the label is 0, outputs -1, if the label is 1, outputs 1.

_Implementation_. MHSA for global dependency encoder has 2 blocks and for sequential dependency decoder has 1 block. Each block has 8 heads and 256 hidden units. Compared with SetRank (6 blocks), our method has fewer parameters, which alleviates the inference overhead caused by generative paradigm to some extent. The shapes of transfer layer, \(FFN\) and \(rFFN\) are \([Z,256]\), \([Z,256]\) and \([512,32,1]\) respectively where \(Z\) is the dimension of the input feature vector. The interaction-based ranker model for retrieval-augmented LLMs is _bert-base_(Li et al., 2019). The \(\beta\) of local backward window is set as 4. The hyperparameter \(\eta\) used to balance the two reranking loss is set as 0.1. In training, the batch size is 16 and Adam (Kingma and Ba, 2015) with learning rate \(10^{-5}\) is used to optimize the loss. We implement our model in PyTorch. Our method has the following implementations: **GenRT**: Jointly trained Reranking-Truncation model. **GenRT\({}_{fast}\)**: Same training method as **GenRT** but uses the acceleration strategy that directly uses the scoring matrix \(\mathbf{S}\) at the first step as the reranking results in inference. **GenRT**: **w/o \(\mathbf{T}\)**: Only learns to rerank. **GenRT**: **w/o \(\mathbf{R}\)**: Only learns to truncate.

### Performance on Web Search

This section evaluates the performance of GenRT and baselines on three learning-to-rank benchmarks collected from web search engine. Specifically, we follow the settings in SetRank (Li et al., 2019) and DLCM (Chen et al., 2019) that use LambdaMart implemented by RankLib to retrieve the top 40 documents for each query as the input ranked lists. The lists are used as the input for list-aware reranking models.

_List-aware Reranking Performance._ Table 1 shows that compared with the baselines, GenRT achieves the best list-aware reranking performance on three IR benchmark datasets. In the training, GenRT jointly learns to rerank and truncate end-to-end, in the inference, we decouple the two tasks (i.e., do not truncate the list) and use NDCG to evaluate its reranking performance. GenRT is better than GenRT-w/o T shows that joint modeling of reranking and truncation facilitates the sharing of contextual information and improves the performance of reranking. GenRT-w/o T outperforming most baselines (except SRGA on Yahoo!) indicates the effectiveness of integrating global and sequential dependency to represent the list-level contextual features and the sequence generation paradigm. GenRT\({}_{fast}\) is an acceleration strategy and outperforms most baselines with the faster inference than SetRank, which indicates that GenRT is a highly flexible model that achieves improvements for efficiency and performance in single reranking scenario.

List-aware Truncation PerformanceTable 2 shows the truncation performance of GenRT and previous SOTA models. As GenRT is the best reranker demonstrated in Table 1, we use its reranking results as the input for the previous baselines models to achieve the fair comparison. The experimental results indicate that Reranking-Truncation joint model GenRT gets the best truncation performance. Specifically, GenRT outperforms GenRT- w/o R demonstrates the positive effect of joint modeling on truncation. GenRT- w/o R working better than previous SOTA models indicates the positive effects of integrating global and sequential information to make fine-grained truncation decision step by step and using RAML-based local binary classification soft criterion as the objective functions. In addition, our method outperforming LeCut+JOTA (a method that jointly trains reranking and truncation but still treats them as separate models and stages) shows the effectiveness of integrating the two tasks into a joint model and performing them concurrently.

### Performance on Retrieval-augmented LLMs

This section evaluates the performance of GenRT and baselines on open-doma QA datasets under the retrieval-augmented LLMs settings. We use Wikipedia passage-collection provided by (Zhu et al., 2019) as the corpus, use Contriever (2018) and interaction-based rank (Chen et al., 2019) to perform retrieval-ranking pipeline to get top 40 passages as the input ranked list for list-aware reranking models. We use _gpt-3.5-turbo-16k_ as the LLM. We provide the returned passage list from IR system to LLM in prompt and let LLM answer questions. We use EM (Yang et al., 2019) to count the accuracy of LLM answering questions by referring to the returned passage list.

List-aware Reranking PerformanceTable 3 shows that the list reranked by GenRT can help LLM achieve better performance than retrieval-ranking pipeline and other list-aware reranking baselines4. The reason is that our method better reranks the passages in list to make relevant passages appear more at the top of the ranked list (higher Recall@5 and Recall@10). It means relevant information appears more at the start of the text input to LLM. There has been study (Zhu et al., 2019) proves that LLMs prefer to exploit information at the start of the input text for generation, which can support our conclusion.

Footnote 4: Since most of the list-aware reranking models in the baselines are designed for feature-based data, we only reproduce SetRank and Seg2State that are suitable for text-based data (open-domain QA) and perform well in Table 1.

List-aware Truncation PerformanceTable 4 shows that our method beats all baselines in balancing the number of passages in the retrieved list and the performance of LLMs. Specifically, as GenRT is the best reranker demonstrated in Table 1 and 3, we use its reranking results as the input for all tr

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c c c} \hline \hline  & \multicolumn{4}{c}{MSLR 30K} & \multicolumn{4}{c}{Yahoo!} & \multicolumn{4}{c}{Istella} \\ \cline{2-13} \multicolumn{1}{c}{\multirow{-2}{*}{Rearker}} & \multicolumn{2}{c}{NDCG} & \multicolumn{2}{c}{ERR} & \multicolumn{2}{c}{MAP} & \multicolumn{2}{c}{NDCG} & \multicolumn{2}{c}{ERR} & \multicolumn{2}{c}{MAP} & \multicolumn{2}{c}{NDCG} & \multicolumn{2}{c}{ERR} & \multicolumn{2}{c}{MAP} \\ \cline{2-13}  & @5 & @10 & @5 & @10 & - & @5 & @10 & @5 & @10 & - & @5 & @10 & @5 & @10 & - \\ \hline GlobalRearak & 0.4501 & 0.4689 & 0.3445 & 0.3617 & 0.5110 & 0.6983 & 0.7425 & 0.4375 & 0.4514 & 0.7101 & 0.6190 & 0.6679 & 0.7001 & 0.7101 & 0.6840 & 7681 \\ DLCM & 0.4500 & 0.4690 & 0.3440 & 0.3620 & 0.5109 & 0.6990 & 0.7430 & 0.4380 & 0.4517 & 0.7105 & 0.6194 & 0.6680 & 0.7005 & 0.7104 & 0.6843 & 762 \\ Seq2State & 0.4533 & 0.4701 & 0.3473 & 0.3685 & 0.5170 & 0.6993 & 0.7438 & 0.4385 & 0.4523 & 0.7143 & 0.6201 & 0.6693 & 0.7012 & 0.7114 & 0.6843 & 763 \\ GSF & 0.4151 & 0.4374 & 0.3215 & 0.3479 & 0.5073 & 0.6838 & 0.7316 & 0.4273 & 0.4405 & 0.7092 & 0.5968 & 0.6508 & 0.6882 & 0.7015 & 0.6805 & 764 \\ PRM & 0.435 & 0.4620 & 0.3402 & 0.3550 & 0.5112 & 0.7072 & 0.7500 & 0.4390 & 0.4528 & 0.7147 & 0.6189 & 0.6605 & 0.6901 & 0.7080 & 0.6842 & 765 \\ SetRank & 0.4515 & 0.4696 & 0.3458 & 0.3632 & 0.5143 & 0.7029 & 0.7453 & 0.4380 & 0.4525 & 0.7140 & 0.6345 & 0.6834 & 0.7103 & 0.7273 & 0.6995 & 766 \\ CRUM & 0.4603 & 0.4812 & 0.3523 & 0.3745 & 0.5171 & 0.7078 & 0.7486 & 0.4397 & 0.4532 & 0.7150 & 0.6417 & 0.6902 & 0.7245 & 0.7401 & 0.7084 & 767 \\ SRGA & 0.4449 & 0.4672 & 0.3420 & 0.3619 & 0.5120 & 0.7079 & 0.7502 & 0.4400 & 0.4541 & 0.7159 & 0.6235 & 0.6713 & 0.7039 & 0.7120 & 0.6877 & 768 \\ GenRT & **0.4757\({}^{\dagger}\) & **0.4919\({}^{\dagger}\) & **0.3623\({}^{\dagger}\)** & **0.3805\({}^{\dagger}\)** & **0.5200\({}^{\dagger}\)** & **0.7085\({}^{\dagger}\)** & **0.7505\({}^{\dagger}\)** & **0.4408\({}^{\dagger}\)** & **0.4550\({}^{\dagger}\)** & **0.4717\({}^{\dagger}\)** & **0.635\({}^{\dagger}\)** & **0.7032\({}^{\dagger}\)** & **0.7418\({}^{\dagger}\)** & **0.7479\({}^{\dagger}\)** & **0.7329\({}^{\dagger}\)** & 769 \\ GenRT-w/o T & 0.4662 & 0.4878 & 0.3507 & 0.3760 & 0.5170 & 0.7068 & 0.7492 & 0.4392 & 0.4539 & 0.7145 & 0.6520 & 0.7018 & 0.7398 & 0.7450 & 0.7253 & 770 \\ GenRT\({}_{fast}\) & 0.4698 & 0.4891 & 0.3542 & 0.3780 & 0.5179 & 0.7072 & 0.7498 & 0.4394 & 0.4540 & 0.7161 & 0.6529 & 0.7023 & 0.7405 & 0.7462 & 0.7275 & 771 \\ \hline \hline \end{tabular}
\end{table}
Table 1. Performance of different reranking models on three learning-to-rank benchmark datasets (bold: best; underline: runner-up; \(\uparrow\): results with significant performance improvement with p-value \(\leq 0.05\) in T-test compared with baselines).

\begin{table}
\begin{tabular}{c c c c c} \hline \hline \multirow{2}{*}{Rearker} & \multirow{2}{*}{Truncation} & \multicolumn{2}{c}{NQ} & \multicolumn{2}{c}{TriviaQA} & \multirow{2}{*}{778} \\ \cline{2-2} \cline{4-5} \multirow{-2}{*}{Seq2State} & \multicolumn{2}{c}{R@5} & \multicolumn{2}{c}{Acc. (LLM)} & \multicolumn{2}{c}{R@5} & \multicolumn{2}{c}{R@10} & \multicolumn{2}{c}{Acc. (LLM)} \\ \hline - & 59.470 & 77.31 & 57.63 & 68.22 & 68.20 & 780 \\ Seq2State & 60.175 & 77.50 & 58.05 & 68.90 & 85.47 & 63.55 \\ SetRank & 60.02 & 77.48 & 57.92 & 68.74 & 85.32 & 63.29 \\ GenRT & **60.78\({}^{\dagger}\)** & **77.63\({}^{\dagger}\)** & **58.79** & **70.01\({}^{\dagger}\)** & **85.70\({}^{\dagger}\)** & **64.37** \\ \hline \hline \end{tabular}
\end{table}
Table 3. Reranking on retrieval-augmented LLMs. The input lists for list-aware reranker are retrieved by Contriever and ranked by interaction-based ranker. Acc. is for LLM.

[MISSING_PAGE_FAIL:8]

## References

* (1)
* Ai et al. (2018) Qingyao Ai, Keping Bi, Jiafeng Guo, and W. Bruce Croft. 2018. Learning a Deep Lishore Context Model for Nathing Refinement. In _Proceedings of the 2018 Conference on SIGIR_. ACM, 135-144.
* Qinyao et al. (2019) Qingyao Ai, Xuanhui Wang, Sebastian Bruch, Nadu McDonald, Michael Bender-May, and Marc Najorki. 2019. Learning Groupwise Multivariate Scoring Functions Using Deep Neural Networks. In _Proceedings of the 2019 Conference on ICTIR_. ACM, 85-92.
* Ji et al. (2016) Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. 2016. Layer Normalizations (GMR) arXiv:1607.04506 (2016). arXiv:1607.06450
* Bahri et al. (2020) Dharan Bahri, Yi Tay, the Zheng, Donald Metzler, and Andrew Tomkins. 2020. Chopy: Cut Transformer for Ranked List Truncation. In _Proceedings of the 2020 Conference on SIGIR_. Jimmy Training, Y. Chang, X. Cheng, J. Zugny, Y. Suraan Murdock, Ji-Hong Wen, and Yiyun Liu (Eds.). ACM, 1513-1516.
* Belkin et al. (2018) Ivan Belkin, Sayali Kulkarni, Sagar Singh, Craig Boullet, Elad Haui-Hsin Chi, Elad Eksam, Yiyun Liu, and Marc Najorki. 2018. SegLate: Re-ranking and Salate Optimization with RNNs. _CoRR_ abs/1810.02019 (2018). arXiv:1810.02019 [http://arxiv.org/abs/1810.02019](http://arxiv.org/abs/1810.02019)
* Beutel et al. (2018) Alex Beutel, Paul Covington, Sagar Jain, Can Xu, Jia Li, Vince Gatto, and Ed H. Ch. 2018. Latent Cross-Maps Using to Context in Recurrent Recommender Systems. In _Proceedings of the 2018 Conference on Neural_. ACM, 64-54.
* Borgwardt et al. (2021) Sebastian Borgwardt, Arthur Mensch, Jordan Hoffmann, et al. 2021. Improving language models by retraining from millions of Tokens. _CoRR_ abs/2112.04426 (2021). arXiv:2112.04426 [https://arxiv.org/abs/2112.04426](https://arxiv.org/abs/2112.04426)
* Chapelle et al. (2009) Olivier Chapelle, Donald Metzler, Zu Jiang, and Pierre Grinspan. 2009. Expected Reciprocal Rank for Graded Relevance. In _Proceedings of the 18th ACM Conference on Information and Knowledge Management_ (Hong Kong, China) _(CIKM '09)_. 621-630. [https://doi.org/10.1145/16459365.1644053](https://doi.org/10.1145/16459365.1644053)
* Chen et al. (2020) Dongmei Chen, Sheng Zhang, Xin Zhang, and Keijing Yang. 2020. Cross-Lingual Passage Re-Ranking With Alignment Augmented Multilingual BERT. _IEEE Access_ (2020). 223232-2323363.
* Chen et al. (2022) Jianwei Chen, Bruguang Zhang, Zhifeng Guo, Yixing Fan, and Xueqi Cheng. 2022. GENE: Generative Evidence Retrieval for Text Verification. In _Proceedings of the 2022 Conference on SIGIR (SIGIR '22)_. 2184-2189. [https://doi.org/10.1145/3477495.3533227](https://doi.org/10.1145/3477495.3533227)
* Culpepper et al. (2016) J.S. Chase Culpepper, Charles L. A. Clarke, and Jimmy Lin. 2016. Dynamic Cut-off Prediction in Multi-Stage Retrieval Systems. In _Proceedings of the 21st Annualization Document Computing Symposium (Catille)_, vol. 374, ACM, 495-508.
* Cho et al. (2016) Yongfei Cho, Jiacunof the 2021 Conference on AAAI_. AAAI Press, 4453-4461.
* [44] Yunya Xi, Weiwen Liu, Xinyi Bai, Ruiming Tang, Wennan Zhang, Qing Liu, Xuxiang He, and Yong Yu. 2021. Context-aware Remaining with Utility Maximization for Recommendation. _ArXiv_ (2020).
* [45] Yunya Xi, Weiwen Liu, Jieaming Zhu, Xiong Zhao, Xinyi Dai, Ruiming Tang, Wennan Zhang, Rui Zhang, and Yong Yu. 2022. Multi-Level Interaction Reranking with User Behavior History. In _Proceedings of the 2022 Conference on SIGIR_. Vol. abs/2209.03930.
* [46] Shichen Xu, Liang Pang, Huawei Shen, Xueqi Cheng, and Tat-Sei Chang. 2023. Search in-the-Chain Towards Accurate, Credible and Trucable Large Language Models for Knowledge-intensive Tasks. arXiv:2304.14732 [cs.CL].
* [47] Ziying Yang. 2017. Relevance Judgments. Pelecommers, Scores and Ties. In _Proceedings of the 2017 Conference on SIGIR_. ACM, 1373. [https://doi.org/10.1145/30737136.3084154](https://doi.org/10.1145/30737136.3084154)
* [48] Zitien Yilmaz, Manish Verma, Nick Craswell, Filip Radlinski, and Peter Bailey. 2014. Relevance and Effort: An Analysis of Document Utility. In _Proceedings of the 2014 Conference on CIKM_. ACM, 91-100. [http://doi.org/10.1145/2661829.2661953](http://doi.org/10.1145/2661829.2661953)
* [49] Tao Zhang, Wenwu Ou, and Zhirong Wang. 2018. Globally Optimized Mutual Influence Aware Ranking in E-Commerce Search. In _Proceedings of the 2018 Conference on IJCAI_. ijcairog, 3725-3731.

## Appendix A Appendix

### Details of Datasets

Table 6 shows the details of feature-based datasets for web search. Each sample in these datasets is a feature vector extracted by traditional learning-to-rank method such as matching, BM25 and pagerank.

### Relevance Capturing in Truncation

We explore the ability of truncation models to capture the relevance of documents. The relevance of the documents at the truncation point (i.e, the first truncated document) can reflect the ability of the truncation model to understand document relevance. On the one hand, if the document at the truncation point is high-relevant, it means that the truncation model misunderstands the relevance of the document and truncates the high-relevant document, leading to the negative impact on the metric (i.e. DCG). On the other hand, if the document at the truncation point is low-relevant, it at least proves that the truncation model can distinguish low-relevant documents. Figure 6 shows the relevance distribution of the documents at the truncation point obtained from different models. The result shows that the documents at the truncation point obtained from GenRT have lower relevance, which indicates that GenRT has the strongest ability to capture the relevance of documents in truncation compared with the other baselines. This performance comes from joint modeling of reranking and truncation, which enables the truncation to take full advantage of the modeling information of document relevance in reranking. While separation between reranking and truncation makes the two cannot share information well.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline Dataset & \begin{tabular}{c} \#Queries \\ Train \\ \end{tabular} & \begin{tabular}{c} \#Queries \\ Test \\ \end{tabular} & \begin{tabular}{c} \#Doc \\ Train \\ \end{tabular} & \begin{tabular}{c} \#Doc \\ Test \\ \end{tabular} & 
\begin{tabular}{c} \#Feature \\ Test \\ \end{tabular} \\ \hline MSLR30K & 18,919 & 6,306 & 2,270k & 753k & 136 \\ Yahoo! & 19,944 & 6,983 & 473k & 165k & 700 \\ Istella & 20,317 & 9,799 & 7,325k & 3,129k & 201 \\ \hline \hline \end{tabular}
\end{table}
Table 6. Details of feature-based datasets for web search.

Figure 6. Relevance distribution of the first truncated documents. X-axis is the relevance. Y-axis represents the distribution of the first truncated document with the corresponding relevance out of all first truncated documents. For low-relevant (0 and 1) documents, the higher the value the better, for high-relevant (2, 3 and 4) documents, the lower the value the better.