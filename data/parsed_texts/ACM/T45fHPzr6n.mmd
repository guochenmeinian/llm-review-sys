[MISSING_PAGE_FAIL:1]

network, REACT decomposes its weights into the sum of two components: meta weights, which are meta-trained to form a solid foundation of general knowledge and shared globally, and adaptive weights, which are the residual component fine-tuned to specific distributions. We leverage a hypernetwork (Zhu et al., 2017) to generate adaptive weights based on data and contextual information. Intuitively, the hypernetwork maps its inputs onto a low-dimensional manifold within the parameter space (Selvin et al., 2016; Wang et al., 2017). This mapping positions adaptive weights for similar contexts and data patterns close to each other, enabling knowledge transfer across different distributions. During training, REACT optimizes the meta weights and the hypernetwork alternately through meta-learning on subsets sampled according to underlying shifts. At inference, the adaptive weights are fine-tuned from the prediction given by the trained hypernetwork, while the meta weights are fixed, preserving the generalizability of the model (Selvin et al., 2016; Wang et al., 2017; Wang et al., 2017; Wang et al., 2017).

We theoretically analyze the convergence of REACT on linear models, showing the parameters converge at a linear rate characterized by the eigenvalues of the sample matrices and other hyperparameters in REACT. Our framework is model-agnostic, broadly applicable to various neural networks and loss functions. We evaluate REACT on three datasets with different backbone models. Compared to models without adaptation, REACT improves the AUROC by 14.85% with few fine-tuning efforts (e.g., update 1 to 10 gradient steps on 10 to 100 samples). Ablation studies and sensitivity analyses show that REACT is robust to variations in the number of samples, the number of fine-tuning steps, and contamination in training data. We further showcase the capability of REACT for parameter-efficient fine-tuning, achieving 5.75% higher AUROC with 94.3% fewer parameters updated compared to full fine-tuning, highlighting its efficiency. Our contributions are as follows:

* We study the problem of fast model adaptation under distribution shift in threat detection, focusing on a practical yet challenging scenario where labels are unavailable and only limited data from new distributions are observed.
* We introduce REACT, a novel adaptation framework using a few unlabeled data and contextual insights. REACT decomposes model weights into meta and adaptive components and updates them through meta-learning alternately. It employs a hypernetwork to generate adaptive weights based on data and contexts, enabling knowledge transfer across distributions.
* We establish the convergence rate of REACT through theoretical analysis. Moreover, we conduct extensive evaluations on multiple model architectures and datasets, demonstrating that REACT consistently outperforms various state-of-the-art methods.

## 2. Related Work

**Threat Detection.** Threat detection (Chen et al., 2016; Wang et al., 2017; Wang et al., 2017; Wang et al., 2017) aims to identify security risks in systems and networks, such as insider threat (Wang et al., 2017; Wang et al., 2017), intrusion attacks (Wang et al., 2017), malware (Wang et al., 2017). Typically, the ground truths for benign and malicious activities are not available, as they require user reports or inspections by domain experts. As a result, threat detection follows unsupervised or semi-supervised approaches in anomaly detection based on different assumptions of data distribution (Chen et al., 2016; Wang et al., 2017). These methods assume the majority of data belong to a "normal" class, while anomalies deviate from this norm, e.g., lying in low-density regions (Deep Gaussian Mixture Model(Wang et al., 2017)), being far from normal data clusters (DeepSVDD (Wang et al., 2017)), or showing high reconstruction errors from the latent space of normal data (AutoEncoder (Chen et al., 2016)). These methods are designed for static environments and are not robust to distribution shifts. Changes in data distributions can significantly degrade model performance.

**Distribution Shifts in General Machine Learning.** Distribution shift means the distributions of the training and testing data are different, leading to poor model generalization to unseen data (Wang et al., 2017). To address the challenge, adaptation methods have been proposed (Wang et al., 2017; Wang et al., 2017; Wang et al., 2017). We focus on works designed for unsupervised or semi-supervised scenarios due to the data specificity in threat detection. Unsupervised domain adaptation (Chen et al., 2016) is a closely related topic, which adapts models to target domains that have no labeled data. Methods include invariant representation learning (Wang et al., 2017), prototype-oriented conditional transport (Wang et al., 2017), contrastive pre-training (Wang et al., 2017). However, these methods assume the availability of labels from source data to guide adaptation, falling short in threat detection where labels in source domains are also unavailable.

**Model Adaptation in Threat Detection.** Distribution shifts are observed in threat detection, such as malware detection (Wang et al., 2017), network intrusion detection (Wang et al., 2017; Wang et al., 2017), and log anomaly detection (Wang et al., 2017). Traditional supervised approaches (Chen et al., 2016; Wang et al., 2017; Wang et al., 2017) require extensive labeling, making them impractical for real-world deployment. Recent efforts have recognized this limitation and have been exploring adaptation approaches without relying on labels. Unsupervised domain adaptation methods, like learning domain-invariant representations (Chen et al., 2016) have been extended. However, they typically require simultaneous training on source and target domains, making them less suitable for emerging domains. Test-time adaptation, such as batch normalization updates (Wang et al., 2017), energy-based models (Wang et al., 2017), and trend estimation (Wang et al., 2017), updates models during inference without gradient descent. Though efficient, they are limited to minor shifts (Wang et al., 2017) or sequential shifts that display continuous patterns (Wang et al., 2017). To address more severe and random shift, meta-learning (Wang et al., 2017; Wang et al., 2017) is a promising approach that trains a meta model on a variety of learning tasks, enabling adaptation to new distributions with a small amount of data. Prior works have applied meta-learning to graph neural networks (Wang et al., 2017) and autoencoders (Wang et al., 2017) for few-shot detection, and introduced prototype-oriented optimal transport for adapting models to new multivariate time-series (Wang et al., 2017). However, these methods fine-tune models solely on limited data from new distributions, leading to variations in adaptation performance. In contrast, our method considers contextual information about shifts to understand correlations among distributions and transfer knowledge, improving adaptability.

Figure 1. Illustration of our problem setting.

## 3. Preliminaries

### Problem Definition

Distribution shifts in threat detection involve changes in the probability distribution of data over time or across domains (e.g., users, services). These shifts can affect the marginal feature distribution \(\mathcal{P}(x)\), the conditional distribution \(\mathcal{P}(y|x)\), or both. Consider a threat detection model \(f(\cdot;\theta)\) trained on a dataset \(\mathbf{D}=\{x_{i}\}_{i=1}^{N}\) drawn from a distribution \(\mathcal{P}\). The dataset \(\mathbf{D}\) is _unlabeled_ and is dominated by samples from the benign class. The model parameters \(\theta\) are optimized by minimizing a loss function \(\mathcal{L}\). Common choices for \(\mathcal{L}\) include reconstruction loss in autoencoders or contrastive loss in self-supervised learning. The objective is: \(\theta^{*}=\arg\min_{\theta}\mathbb{E}_{\mathbf{x}\sim\mathcal{P}}\mathcal{L}( f(x;\theta))\).

Our goal is to develop an adaptation method that updates model parameters to \(\theta^{*}\) using a few examples from the new distribution \(\mathcal{P}^{\prime}\). The observed dataset \(\mathbf{D}^{\prime}\) from distribution \(\mathcal{P}^{\prime}\) is unlabeled, and its size is small \(|\mathbf{D}^{\prime}|=k\ll|\mathbf{D}|\).

### Meta-Learning

Meta-learning trains models that can quickly adapt to new tasks using only a few examples. A task \(\mathcal{T}_{i}\) is defined as an independent learning problem with a dataset following a specific distribution \(\mathcal{P}_{i}\) and a learning objective which is to find the optimal parameters \(\theta^{*}_{i}\) that minimize the expected loss \(\theta^{*}_{i}=\arg\min_{\theta}\mathbb{E}_{\mathbf{x}\sim\mathcal{P}_{i}} \mathcal{L}(f(x;\theta))\). The dataset \(\mathbf{D}_{i}\) for task \(\mathcal{T}_{i}\) is divided into a support \(\mathbf{D}^{i}_{\text{support}}\) and a query set \(\mathbf{D}^{i}_{\text{query}}\). \(\mathbf{D}^{i}_{\text{support}}\) is used to fine-tune the model to learn task-specific parameters for \(\mathcal{T}_{i}\), while \(\mathbf{D}^{i}_{\text{query}}\) evaluates how well the model generalizes the learned task-specific knowledge.

One of the most representative algorithm is Model-Agnostic Meta-Learning (MAML) (Srivastava et al., 2014), which optimizes the initial model parameters \(\theta\) so that after fine-tuning, the model performs well across various tasks, minimizing the average loss. For each task \(\mathcal{T}_{i}\), the parameters are fine-tuned using the support set \(\mathbf{D}^{i}_{\text{support}}\): \(\theta^{*}_{i}=\arg\min_{\mathbf{x}\in\mathbf{D}^{i}_{\text{support}}}\mathcal{ L}(f(x;\theta))\). The weight initialization \(\theta\) is optimized using the query sets:

\[\theta^{*}=\arg\min_{\theta}\sum_{\mathcal{T}_{i}}\sum_{x\in\mathbf{D}^{i}_{ \text{query}}}\mathcal{L}(f(x;\theta^{*}_{i})).\]

During inference, the model is fine-tuned on a few samples from the new distribution, and then applied to all testing samples.

### HyperNetwork

A hypernetwork (Srivastava et al., 2014) is a neural network that predicts the weights of another neural network (i.e., target network). By training a single hypernetwork to predict weights across multiple tasks rather than optimizing each one independently, hypernetworks offer a parameter-efficient solution for model adaptation. It has shown effective in improving learning efficiency through parameter sharing (Bahdanau et al., 2015; Gulrajani et al., 2016; Gulrajani et al., 2017; Chen et al., 2018; Li et al., 2019). Let \(h\) represent the hypernetwork with parameters \(\phi\), and let \(f\) denote the target network. Given a representation \(\mathbf{V}_{\mathbf{i}}\) for describing task \(\mathcal{T}_{i}\), the hypernetwork generates the model weights \(\theta_{i}=h(\mathbf{V}_{\mathbf{i}};\phi)\), which are loaded into \(f\) for the downstream task. Given multiple tasks \(\mathcal{T}_{i}\) and the corresponding task representations \(\mathbf{V}_{i}\), the learning objective is to optimize the hypernetwork's parameters \(\phi\) to minimize the loss \(\mathcal{L}\) across these tasks:

\[\phi^{*}=\arg\min_{\phi}\sum_{\mathcal{T}_{i}}\sum_{x\in\mathbf{D}_{i}} \mathcal{L}[f(x;h(\mathbf{V}_{i};\phi))].\]

```
Input: Task distribution \(\mathcal{P}(\mathcal{T})\), target network \(f\), hypernetwork \(h\) Output: Meta weights \(\theta_{\text{meta}}\), hypernetwork weights \(\phi\)
1 Initialize model weights \(\theta_{\text{meta}}\) and \(\phi\);
2whilenot convergeddo// Update meta weights.
3 Sample a set of tasks \(\{\mathcal{T}_{i}\}_{i=1}^{M}\sim\mathcal{P}(\mathcal{T})\);
4foreach task\(\mathcal{T}_{i}\)do
5 Form support set \(\mathbf{D}^{i}_{\text{support}}\) and query set \(\mathbf{D}^{i}_{\text{query}}\) and extract contextual information \(c_{i}\);
6 Generate adaptive weights: \(\theta^{i}_{\text{adapt}}=h(\mathbf{D}^{i}_{\text{support}};c_{i};\phi)\);
7 Fine-tune \(\theta^{i}_{\text{adapt}}\) on \(\mathbf{D}^{i}_{\text{support}}\) following Eq. 1;
8 Update \(\theta_{\text{meta}}\) following Eq. 2;
9 // Update hypernetwork.
10 Sample a set of tasks \(\{\mathcal{T}_{i}\}_{f=1}^{M}\sim\mathcal{P}(\mathcal{T})\);
11foreach task\(\mathcal{T}_{f}\)do
12 Form support set \(\mathbf{D}^{j}_{\text{support}}\) and query set \(\mathbf{D}^{j}_{\text{query}}\), and extract contextual information \(c_{j}\);
13 Generate adaptive weights: \(\theta^{j}_{\text{adapt}}=h(\mathbf{D}^{j}_{\text{support}};c_{j};\phi)\);
14 Update \(\phi\) following Eq. 3;
15
16 end while
```

**Algorithm 1**Training Procedure of REACT

## 4. The REACT Framework

We approach the problem from both meta-training and fine-tuning perspectives. Through meta-training, the model establishes a strong, generalizable foundation that can be applied to most scenarios. Then, through fine-tuning, the model weights are slightly adjusted for specific shifts. We propose a framework that decomposes model weights into two components and alternately optimizes them to address both perspectives. Algorithm 1 provides the pseudo-code.

### Weight Decomposition

Given a neural network, we decompose its weights into two complementary components: meta weights \(\theta_{\text{meta}}\) and adaptive weights \(\theta_{\text{adapt}}\). Meta weights capture global patterns that are common across different distributions, representing the core knowledge acquired during meta-learning. Adaptive weights, on the other hand, serve as a small "residual" component that allows the model to be fine-tuned to the unique characteristics of specific data distributions, while still leveraging the global patterns encoded in the meta weights. The full model weights are then formed by adding the two components together: \(\theta=\theta_{\text{meta}}+\theta_{\text{adapt}}\). By applying a small residual update to the pretrained meta weights, the model can adapt to new distributions without overwriting the essential pretrained knowledge.

### Residual-Adaptive Weight Generation with Hypernetwork

We incorporate a hypernetwork to generate adaptive weights based on data characteristics and contextual information. The architecture of the hypernetwork is presented in Figure 2.

**Data Encoding.** Our hypernetwork includes a data encoder that processes data from the support set to produce feature representations. These representations are averaged and passed through a series of linear layers, with each layer producing the weights for a corresponding layer in the target network.

**Context Encoding.** To enhance the hypernetwork's ability to handle varying distributions, we integrate the contextual information \(\epsilon_{i}\) about distribution \(\mathcal{P}_{l}\) as an additional input. This context offers semantic insights into the shifts and helps capture similarities across distributions. We incorporate a context encoder in the hypernetwork to transform contexts into embeddings, which are then added to the data representations for weight generation. The choice of context depends on the type of shift. For example, we use time information for temporal shifts, with positional encoding (Zhu et al., 2017) generating embeddings. Details about context modeling for different tasks can be found in Section 6.1.

### Alternating Optimization

We design an alternating optimization scheme to iteratively update the meta weights and the hypernetwork. This approach balances the learning dynamics and prevents mutual interference between the two components. Figure 3 illustrates the process.

**Task Sampling for Meta Learning.** To let the model learn how to adapt to new distributions, we first create a diverse set of tasks that reflect the expected variations in the application. We sample tasks from training set by simulating the underlying data shifts. For instance, if the goal of adaptation is to address distribution shifts over time, the data can be grouped according to temporal factors such as day or month, with each time period forming a separate task. If the focus is on handling shifts across different users, the data can be grouped by users, with each user forming a task.

**Update of Meta Component.** Let \(\theta^{i}_{\text{adapt}}\) denote the adaptive weights of task \(\mathcal{T}_{l}\). In each iteration, we begin by updating the meta weights. We sample a set of tasks \(\{\mathcal{T}_{l}\}_{i=1}^{M}\) and contextual information \(\{c_{l}\}_{i=1}^{M}\). The hypernetwork \(h(\cdot;\phi)\) is fixed and used to generate adaptive weights, \(\theta^{i}_{\text{adapt}}=h(\mathbf{D}^{i}_{\text{support}},c_{l};\phi)\). The generated adaptive weights are then fine-tuned to derive the optimal model weight \(\theta^{i*}_{\text{adapt}}\) for task \(\mathcal{T}_{l}\) by minimizing the empirical loss over the support set \(\mathbf{D}^{i}_{\text{support}}\):

\[\theta^{l*}_{\text{adapt}}=\operatorname*{argmin}_{\theta_{\text{adapt}}}\sum \limits_{x\in\mathbf{D}^{i}_{\text{support}}}\mathcal{L}(f(x;\theta_{\text{ meta}},\theta_{\text{adapt}})). \tag{1}\]

We then fix these fine-tuned adaptive weights and update the meta model by minimizing the loss on the query set \(\mathbf{D}^{i}_{\text{query}}\). Let \(\eta_{\text{meta}}\) be the learning rate for updating meta weights. The update of meta weight after one gradient step is as follows:

\[\theta_{\text{meta}}\leftarrow\theta_{\text{meta}}-\eta_{\text{meta}}\nabla_{ \theta_{\text{meta}}}\sum\limits_{\mathcal{T}_{l}\times\mathbf{D}^{i}_{\text{ query}}}\mathcal{L}(f(x;\theta_{\text{meta}},\theta^{i*}_{\text{adapt}})). \tag{2}\]

**Update of Hypernetwork.** Next, we sample another set of tasks \(\{\mathcal{T}_{j}\}_{i=1}^{M}\), fix the meta weights learned in the previous step, and update the hypernetwork using the query sets. Let \(\eta_{h}\) be the learning rate for updating the hypernetwork. The weight update of hypernetwork after one gradient step is as follows:

\[\phi\leftarrow\phi-\eta_{h}\nabla_{\phi}\sum\limits_{\mathcal{T}_{j}}\sum \limits_{x\in\mathbf{D}^{i}_{\text{query}}}\mathcal{L}(f(x;\theta_{\text{meta} },h(\mathbf{D}^{j}_{\text{support}},c_{j};\phi))). \tag{3}\]

**Regularization.** We apply L2 regularization to the adaptive weights generated by the hypernetwork, encouraging them to act as residuals to the globally shared meta weights. The query loss for optimizing the hypernetwork, denoted as \(L^{i}_{\text{query}}\), is combined with the regularization as \(\mathcal{L}=\mathcal{L}^{i}_{\text{query}}+\lambda\|\theta^{i}_{\text{adapt}}\| _{2}^{2}\), where \(\lambda\) is the hyperparameter to control the regularization strength.

### Adapting to New Distributions

When doing inference on a new distribution \(\mathcal{P}_{j}\), the meta weights and the hypernetwork are fixed. This ensures the pre-trained knowledge are not "forgotten" during fine-tuning (Golov et al., 2013; He et al., 2016; Li et al., 2017), preserving generalizability of the model. A small number of support data \(\mathbf{D}^{j}_{\text{support}}\) from \(\mathcal{P}_{j}\) along with its contextual information \(c_{j}\) are fed into the hypernetwork to predict the adaptive weights \(\theta^{j}_{\text{adapt}}=h(\mathbf{D}^{j}_{\text{support}},c_{j};\phi)\). With this initialization, the adaptive weights are then fine-tuned on \(\mathbf{D}^{j}_{\text{support}}\) following Equation 1. Finally, the two parts of the weights are merged by summing them as

Figure 3. Alternating optimization in REACT. In each training iteration, we sample a set of tasks to update the meta weights, then sample another set to train the hypernetwork.

Figure 2. Architecture of the proposed hypernetwork.

if there is only one target network. The merged weights are used for inference on data from the new distribution \(\mathcal{P}_{j}\).

## 5. Analysis

We provide convergence analysis of REACT on linear models. Let \(X^{l}\) be the matrix whose rows are the samples from the dataset of task \(i\in\{1,...,M\}\), i.e., \(\mathbf{D}_{l}\). The data matrix \(X^{l}\) can be split into support set \(X^{l}_{g}\) and query set \(X^{l}_{q}\). We assume that the relevant datasets are sampled at the beginning of the algorithm. Given linear model1

Footnote 1: We note that we abuse the notation and set \(h(X,c_{i};\phi)=h(X,\phi)\), i.e., the context information is part of the input data

\[h(X;\phi)=X\phi,\quad f(X;\theta_{\text{meta}},\theta_{\text{adapt}})=X(\theta_{ \text{meta}}+\theta_{\text{adapt}}), \tag{4}\]

Theorem 1 provides convergence guarantees for REACT.

Theorem 1.: _Consider REACT on the linear model in (4) with \(Eq.(1)\) being solved exactly. Let \(X^{l}_{g}\) and \(X^{l}_{q}\) satisfy \((X^{l}_{g})^{\top}X^{l}_{g}=(X^{l}_{q})^{\top}X^{l}_{q}=\sigma_{l}l\) for each task \(i\in\{1,..,M\}\), where \(\sigma_{l}\) are the variances and \(l\) is the identity matrix. Learning rates are chosen as \(\eta_{\text{meta}}<1/\sum_{l=1}^{M}\sigma_{l}/(\sigma_{l}+\lambda)\) and \(\eta_{h}<1/\max\left(\sum_{j=1}^{n_{0}}\sigma_{j}(\sigma_{j}+\lambda),\left\| \mathbf{X}_{g}\right\|\right)\) where \(\mathbf{X}_{g}=\sum_{l=1}^{M}\sigma_{j}(X^{l}_{g})^{\top}\). Then, for any \(\varepsilon>0\), there exists_

\[K=\mathcal{O}\left(\log_{1/\rho_{\text{meta}}}(1/\varepsilon)+\log_{1/\rho_{h} }(1/\varepsilon)\right)\]

_for \(\rho_{\text{meta}}=1-\eta_{\text{meta}}\sum_{l=1}^{M}\sigma_{l}/(\sigma_{l}+\lambda)\) and \(\rho_{h}=1-\eta_{h}\sum_{j=1}^{M}\sigma_{j}(\sigma_{j}+\lambda)\) such that the \(K\)-iteration of Algorithm 1 satisfies_

\[\|\theta^{K}-\theta^{*}\|\leq\varepsilon,\qquad\text{and}\qquad\|\theta^{K}- \phi^{*}\|\leq\varepsilon,\]

_where \(\theta^{*}\) and \(\phi^{*}\) are stationary points of the algorithm._

The proof is provided in Appendix A.2. Our results suggest that \(\theta_{\text{meta}}\) and \(\phi\) converge to stationary points at a linear rate which can be characterized based on the eigenvalues of the sample matrices in each task and hyperparameters considered in REACT.

## 6. Experiments

### Experiment Setup

**Datasets and Backbone Models.** Our evaluation focuses on two key applications in cybersecurity, network intrusion detection and malware detection, and targets both temporal and domain shifts. REACT is compatible with various neural network architectures. To assess its performance across different models, we employ three representative architectures, AutoEncoder (AE) (Abadi et al., 2016), DeepVDD (DSVDD) (Selvin et al., 2017), and GOAD (Chen et al., 2018), paired with the following datasets:

* **AnoShift**(Zhu et al., 2018) is a benchmark for network intrusion detection under distribution shifts. It collects traffic logs from a university network between 2006 and 2015. Data shifts occur over time due to reasons such as user behavior changes and software updates. Each sample has 15 features (9 numerical and 6 categorical) and a label of whether it is an attack. We use the train-test split provided by the dataset, including training subsets from 2006 to 2010 and test subsets from 2015. Each month is regarded as a separate task. AutoEncoder is used as the backbone model.
* **Malware**(Zhu et al., 2018) contains executables collected between 2010 and 2014 from VirusShare2, an online malware analysis platform. Data shifts happen along time. Each executable has 482 counting features and a risk score (ranging from 0 to 1) indicating the probability of it being malware. These risk scores are converted to binary labels using thresholding, with executables labeled as malicious if the score is greater than 0.6 and benign if the score is less than 0.4 (Zhu et al., 2018). Following previous work (Zhu et al., 2018), the dataset is split into training data from 2011 to 2013, validation data from 2010, and testing data from 2014. Each month is treated as a separate task. DeepSVDD is used as the backbone model.
* **NSL-KDD**(Zhu et al., 2018) is another dataset for evaluating network intrusion detection. Each sample has 40 attributes describing the network traffic, with 6 categorical and 34 numerical features. We simulate domain shifts by grouping data according to services (e.g., HTTP, Telnet) and randomly assigning half of the services as training tasks and the other half as test tasks. We use the official train-test split provided by the dataset and remove services not selected for the respective splits. Besides, services with fewer than 20 benign samples are excluded to ensure sufficient unseen data for testing. GOAD is used as the backbone model. We sample the datasets to form a 10% ratio of threat samples for both training and testing. In Section 6.4, we vary this ratio to test the robustness of REACT to the contamination of training data. For the NSL-KDD dataset, since GOAD is a semi-supervised method that trains only on benign data, we remove attack samples from the training set. Table 1 summarizes the statistics and configurations of the datasets and backbone models in the experiments. Further details on the backbone models are provided in Appendix A.1.

Footnote 2: [https://virussbare.com/](https://virussbare.com/)

**Baselines.** We compare REACT with unsupervised methods from the anomaly detection benchmark (Zhu et al., 2018), including linear and statistical models: **ECOD**(Zhu et al., 2018), **COPOD**(Zhu et al., 2018), **OCSVM**(Zhu et al., 2018); distance- and proximity-based methods: **LOF**(Zhu et al., 2018), **KNN**(Zhu et al., 2018); ensemble methods: **LODA**(Zhu et al., 2018), **Floresf**(Zhu et al., 2018); and neural networks: **AE**(Abadi et al., 2016), **DSVDD**(Selvin et al., 2017), **LUNAR**(Zhu et al., 2018). These methods assume static environments and do not account for distribution shifts. In addition, we compare REACT with training from scratch, fine-tuning strategies, and state-of-the-art model adaptation methods. Brief descriptions are as follows:

* **w/o adaptation**: The model is trained on the training data and directly tested on each test task. This serves as the pretrained model, denoted as \(\mathcal{A}\).
* **Train-from-scratch**: For each task, a model is trained from scratch using \(k\) samples and is used for evaluation.
* **Fine-tuning**: For each task, the model is fine-tuned using \(k\) samples from the task based on the pretrained model \(\mathcal{A}\).
* **Continual Learning**: Starting from \(\mathcal{A}\), we sequentially fine-tune the latest updated model using \(k\) samples from each task.
* **Experience Replay (ER)**(Zhu et al., 2018) is a method to mitigate catastrophic forgetting in continual learning. We maintain a memory buffer to store historical data. In each fine-tuning iteration, we

\begin{table}
\begin{tabular}{l c c c c c} \hline \hline Dataset & \# Train/Test & \# Train/Test tasks & \(k\) & Shift by & Model \\ \hline Anoshift & 1.3M / 1.8M & 50 / 110 & 100 & Time & AutoEncoder \\ NSL-KDD & 28K / 6K & 8 / 6 & 10 & Service & GOAD \\ Malware & 15K / 5K & 36 / 12 & 10 & Time & DeepSVDD \\ \hline \hline \end{tabular}
\end{table}
Table 1. Experiment configurations and dataset statistics after preprocessing.

sample a batch from this buffer and compute its loss. This loss is then weighted and combined with the loss from the new batch.
* **ACR**[40] is a zero-shot adaptation which adopts meta-learning to train the model and update the batch normalization layers with the batch statistics during inference. We add batch normalization layer after each linear or convolutional layer in the model.
* **OC-MAML**[20] is a few-shot one-class classification method. It extends MAML by modifying the episodic data sampling strategy by forming one-class support sets to optimize the meta model.

**Training and Adaptation Configurations.** The size of support data \(k\) during training and adaptation is set based on the data quantity, with \(k=100\) for Anoshift and \(k=10\) for Malware and NSL-KDD. The size of query data varies proportionally, with 1000 for Anoshift and 100 for Malware and NSL-KDD. In each meta-training iteration, we sample \(M=5\) tasks for Malware and NSL-KDD, and \(M=1\) for Anoshift. The number of fine-tuning epochs \(E\) is determined by the convergence rate of the learning task, with \(E=10\) epochs for Anoshift and Malware, and \(E=1\) for NSL-KDD due to its faster convergence. Section 6.4 provide sensitivity analyses on \(k\) and \(E\) to assess the robustness of our model.

**Choices of Contexts.** For Anoshift and Malware whose shifts occur along time, we use time index as the context, which is modeled by positional encoding [67] to generate contextual embedding for each task. For NSL-KDD dataset whose shifts occur across services, we first feed these services names to a large language model with the prompt "please briefly describe each of these web services, including the normal and anomalous patterns". Then, we use Sentence Transformer3 to generate embeddings for the descriptions.

Footnote 3: [https://huggingface.co/sentence-transformers](https://huggingface.co/sentence-transformers)

**Evaluation Metrics.** For each test task, we adapt the model and evaluate its performance using AUROC and AUPR scores. All experiments are repeated for five times with the same set of random seeds, and the results are averaged across all test tasks and runs.

### Main Results and Analysis

The results are presented in Table 2, where the left sub-table shows the performance of static methods and the right one focuses on fine-tuning and adaptation methods across three backbone models. The static methods (left table) generally show lower performance than models with adaptation (right table), highlighting the negative impact of distribution shifts on model performance. The right table also includes the performance of train-from-scratch using all data from each individual test task (in grey). When sufficient data is available from the new tasks, training a model from scratch yields better performance than using a pretrained model without adaptation. When comparing the models trained from scratch, fine-tuning, and continual learning, it is shown that the pretrained model \(\mathcal{H}\) can be a poor initialization for shifted distributions, e.g., Anoshift. We also observe that experience replay slightly improves performance compared to continual learning without any strategy to prevent catastrophic forgetting. However, this improvement is limited.

REACT consistently outperforms the baselines and even surpasses the model trained from scratch using all data from individual tasks on two datasets. This is because REACT adapts from a model meta-trained on a larger and more diverse training set than each individual task, providing a stronger foundation for adaptation. Furthermore, the training data in Anoshift and Malware contains noise (10% threat ratio). Training a model on all data from an individual task increases the likelihood of exposing the model to many threat samples within that distribution, which, due to the specific training objectives of AutoEncoder and DSVDD, may cause the models to mistakenly learn malicious patterns as benign ones. In contrast, REACT is less prone to such overfitting as it utilizes the meta model and only updates the adaptive weights on a small set of new data. We note that on NSL-KDD, GOAD achieves high scores when trained from scratch using all data since it is trained on benign data only, but such training is impractical in the real world. When compared to other baselines using the same \(k\) samples from new tasks, REACT achieves the highest scores. Among state-of-the-art methods, ACR, which performs test-time adaptation, shows relatively lower performance as it does not apply gradient updates during inference, limiting its adaptation ability. OC-MAML achieves the second-best performance, demonstrating the strength of meta-learning. However, REACT outperforms OC-MAML by weight decomposition and incorporating a hypernetwork for contextual tuning. These

\begin{table}
\begin{tabular}{l|c c|c c|c c} \hline \multirow{2}{*}{Method} & \multirow{2}{*}{\(\mathbf{D}_{i}^{\prime}\)} & \multicolumn{2}{c|}{Anoshift} & \multicolumn{2}{c|}{NSL-KDD} \\ \cline{3-7}  & & AUROC & AUPR & AUROC & AUPR & AUROC & AUPR \\ \hline \hline \multirow{2}{*}{KNN [54]} & 0.6714 & 0.4062 & 0.2732 & 0.1040 & 0.5323 & 0.2956 \\  & LOF [9] & 0.6107 & 0.2873 & 0.2781 & 0.1101 & 0.4150 & 0.1827 \\  & OCSVM [57] & 0.6903 & 0.3157 & 0.3880 & 0.1190 & 0.6649 & 0.3492 \\  & Forest [46] & 0.6658 & 0.2830 & 0.2660 & 0.0722 & 0.7809 & 0.4798 \\  & LODA [52] & 0.5723 & 0.2111 & 0.5190 & 0.1368 & 0.5207 & 0.2532 \\  & AE [1] & 0.7110 & 0.3204 & 0.3789 & 0.1156 & 0.6057 & 0.2678 \\  & DSVDD [56] & 0.7716 & 0.3895 & 0.5165 & 0.1644 & 0.6006 & 0.2848 \\  & COPOD [44] & 0.7664 & 0.3831 & 0.4450 & 0.1102 & 0.7849 & 0.4471 \\  & ECOD [45] & 0.7461 & 0.3727 & 0.5403 & 0.1390 & 0.8100 & 0.4706 \\  & LluxAR [22] & 0.4449 & 0.2450 & 0.2719 & 0.0880 & 0.5243 & 0.2350 \\ \hline \end{tabular}
\end{table}
Table 2. Main experiment results (averaged over 5 runs). The left sub-table reports the performance of static methods, while the right focuses on fine-tuning and adaptation across three backbone models. REACT consistently outperforms all other methods. \(|\mathbf{D}_{i}^{\prime}|\) denotes the number of samples observed from each test task for fine-tuning or training from scratch.

designs help maintain generalizability and enhance adaptability beyond meta-learning alone.

### Ablation Studies

We crafted five ablated versions of REACT by systematically removing each key component: (1) We remove the hypernetwork and perform meta-learning on the meta weights only, denoted as **w/o hypernetwork**. (2) We disable fine-tuning during inference and use the merged weights from meta weights and the hypernetwork's prediction to do the inference directly, denoted as **w/o fine-tuning**. (3) We remove the use of context and only provide the support data for hypernetwork, denoted as **w/o context**. (4) We replace the context with randomly-generated embeddings, denoted as **w/ random context**. (5) We remove the regularization term on the hypernetwork's prediction, denoted as **w/o regularization**.

The results in Table 3 show that every component in REACT contributes to performance improvement. Integrating the hypernetwork has a significant impact, as contextual information complements limited new data and facilitates knowledge transfer across distributions. Fine-tuning and regularization also have notable impacts. REACT w/o fine-tuning shows competitive performance on AnoShift and Malware compared to the baselines, indicating its potential for zero-shot adaptation. However, with just a few gradient updates, performance can be largely improved. Besides, adding regularization ensures the adaptive weights predicted by the hypernetwork do not overpower the full model, maintaining model generalizability. REACT w/o context learns distribution patterns solely from the support data, which is less effective than incorporating contexts since the support data is limited and might not provide sufficient insights. REACT w/ random context can recognize new tasks as the random context indicates whether the task has been seen during training, thus mitigating overfitting. Therefore, it performs slightly better than w/o context. However, these random contexts do not provide task-specific knowledge to capture meaningful patterns. With additional information about tasks, REACT can model similarity among distributions more effectively.

### Sensitivity Analyses

**Number of Support Samples.** We vary the number of support samples \(k\) for each task from 5 to 100 and compare REACT with the fine-tuning baseline. Figure 4 shows that REACT consistently outperforms the fine-tuning baseline across all datasets. This demonstrates REACT's robustness in data-scarce scenarios and highlights its ability to efficiently leverage available data for fast adaptation.

**Number of Fine-Tuning Epochs.** We vary the number of fine-tuning epochs for each new task from 1 to 10 and compare the performance of REACT with the fine-tuning baseline. Figure 5 shows that REACT outperforms the baseline in all settings. The improvement from additional fine-tuning epochs is less significant in the Malware and NSL-KDD datasets, as these two datasets are simpler and the model is able to adapt to them with fewer epochs.

\begin{table}
\begin{tabular}{l|c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{3}{c|}{Malware} & \multicolumn{3}{c}{AnoShift} \\ \cline{2-7}  & 1\% & 5\% & 10\% & 20\% & 1\% & 5\% & 10\% & 20\% \\ \hline w/o adaptation (\(\mathcal{A}\)) & 0.506 & 0.513 & 0.517 & 0.567 & 0.764 & 0.753 & 0.711 & 0.634 \\ train-from-scratch & 0.366 & 0.366 & 0.356 & 0.377 & 0.818 & 0.791 & 0.740 & 0.740 \\ fine-tuning & 0.559 & 0.545 & 0.568 & 0.580 & 0.812 & 0.765 & 0.704 & 0.605 \\ continual learning & 0.562 & 0.559 & 0.588 & 0.590 & 0.683 & 0.572 & 0.609 & 0.453 \\ ER & 0.582 & 0.585 & 0.602 & 0.602 & 0.734 & 0.669 & 0.614 & 0.577 \\ ACR & 0.544 & 0.570 & 0.580 & 0.570 & 0.785 & 0.774 & 0.763 & 0.773 \\ OC-MAML & 0.683 & 0.688 & 0.678 & 0.687 & 0.827 & 0.803 & 0.777 & 0.755 & 796 \\ \hline REACT (ours) & **0.725** & **0.738** & **0.725** & **0.719** & **0.832** & **0.813** & **0.823** & **0.775** \\ \hline \hline \end{tabular}
\end{table}
Table 4. Sensitivity analysis: AUROC scores across different contamination levels.

Figure 4. Sensitivity analysis: number of support samples (\(k\)).

Figure 5. Sensitivity analysis: number of fine-tuning epochs.

**Contamination on Training Data.** We evaluate the robustness of our system against contamination in the training data when applying to AutoEncoder and DeepSVDD models on Anoshift and Malware respectively--both unsupervised methods. We note that GOAD is a semi-supervised method trained solely on benign data (as applied to the NSL-KDD dataset) thus the evaluation is trivial to it. We fix the number of benign samples while varying the ratio of threat samples from 1% to 20%. Table 4 shows the AUROC scores. REACT consistently achieves higher AUROC scores across different contamination rates than the fine-tuning baseline, showing that it is robust to noise in training data.

### Parameter-Efficient Fine-Tuning

Our framework supports parameter-efficient fine-tuning, which is especially useful when working with large models. By incorporating adaptive weights into only a subset of the model's parameters and having the hypernetwork predict this subset of weights, we can reduce the number of parameters to be fine-tuned. We conducted experiments using an AutoEncoder on the Anoshift dataset to showcase REACT's ability in parameter-efficient fine-tuning. Specifically, we predicted adaptive weights for either the two symmetric linear layers closest to the input (denoted as **REACT-Inner**) or those closest to the latent representations (denoted as **REACT-Outer**). Full fine-tuning of REACT is denoted as **REACT-Full**. The results are shown in Figure 6. Both methods achieve better performance compared to the baselines, although they slightly underperformed compared to REACT-Full which fine-tunes all layers. Notably, REACT-Inner achieved a 5.75% higher AUC while updating 94.3% fewer parameters compared to conventional full fine-tuning, highlighting its efficiency.

### Case Study

To understand how well REACT leverages contextual information, we analyze the weights generated by the trained hypernetworks. Specifically, we compare the adaptive biases of the last layer in the AutoEncoder for Anoshift across different months and calculate their cosine similarities. The results are presented in Figure 7 A, with warmer colors indicating higher similarity. The high similarities around the diagonal indicate the weights generated for each month are similar to those of nearby months. This suggests that REACT effectively captures the temporal dynamics and smoothly adapts model weights over time. As a reference for how data shifts, we follow the analyses in (Kumar et al., 2019) to calculate distances between data subsets of each year. Specifically, we measure the Jeffrey's Divergence (Kohn, 1999) averaged over categorical features and the Optimal Transport Dataset Distance (OTDD) (Bishop, 2006) across all features. As shown in Figure 7 B, data from adjacent years exhibit smaller distances (in red). Besides, it presents block patterns where data from 2006-2010, 2011-2013, and 2014-2015 are more similar within their respective groups than with other years. This temporal shift corresponds with trends in weight similarity over time. The observations also hint at the potential for detecting shifts, a research question actively discussed in the literature (Kumar et al., 2019; Wang et al., 2020; Wang et al., 2020)--by monitoring deviations in the hypernetwork's predictions compared to prior tasks, we may identify moments where shifts occur.

## 7. Conclusions

Our work sheds light on how to approach the distribution shift problem--from both meta-learning and fine-tuning perspectives. We propose a novel framework, REACT, that decomposes the weights of a neural network into the sum of meta and adaptive components, following a meta-learning paradigm to train the components. By integrating a hypernetwork to generate adaptive weights, REACT enables knowledge sharing and adjusts weights for new distributions with minimal fine-tuning effort. The framework is model-agnostic, generally applicable to arbitrary neural networks. It works effectively with unlabeled and imbalanced data, making it broadly applicable to various threat detection models and objectives.

While focused on cybersecurity, the principles and methods developed in our research can be adapted to other fields facing similar distribution shift challenges, such as finance (Kumar et al., 2019; Wang et al., 2020; Wang et al., 2020) and healthcare (Wang et al., 2020; Wang et al., 2020). Our study provides insights for studies in the general machine learning community, fostering a more comprehensive understanding of adaptation and fine-tuning by showcasing applications in cybersecurity. One of the future directions is to incorporate a lightweight mechanism for updating the meta model within our framework. A potential solution could involve applying aggregation of the predicted adaptive weights into the meta model. This approach could enhance the framework's ability to continuously adapt to evolving distributions, especially for scenarios with significant distribution shifts over long period of time.

Figure 6. Results of parameter-efficient fine-tuning. Both REACT-Inner and REACT-Outer outperform the baselines.

Figure 7. Case Study. The adaptive weights generated for each month are similar to those of nearby months, reflecting the data shift pattern.

REACT: Residual-Adaptive Contextual Tuning for Fast Model Adaptation in Threat Detection

## References

* C. Aggarwal and C. C. Aggarwal (2017)An introduction to outlier analysis. Springer. Cited by: SS1.
* V. Allah, O. Tov, R. Mokady, R. Gal, and A. Bermano (2022)Hyperstyle: stylegan inversion with hypernetworks for real image editing. In Proceedings of the IEEE/CVF conference on computer Vision and pattern recognition, pp. 18511-18521. Cited by: SS1.
* D. Alvarez-Melis and N. Tsai (2020)Geometric dataset datasets via optimal transport. Advances in Neural Information Processing Systems33, pp. 21428-21439. Cited by: SS1.
* F. Barbero, F. Pendlebury, F. Pierazzi, and L. Cavallaro (2022)Transcoming transcend: revisiting malware classification in the presence of concept drift. In 2022 IEEE Symposium on Security and Privacy (SP), pp. 805-823. Cited by: SS1.
* S. Ben-David, J. Bilzer, K. Crammer, A. Kulesza, F. Pereira, and J. Wortman Vaughan (2010)A theory of learning from different domains. Machine learning79, pp. 151-175. Cited by: SS1.
* S. Ben-David, D. Baker, K. Crammer, A. Kulesza, F. Pereira, and J. Wortman Vaughan (2010)A theory of learning from different domains. Machine learning79, pp. 151-175. Cited by: SS1.
* S. Ben-David, T. M. Debben, J. Cheney, and P. Valcheev (2021)A rule mining-based advanced persistent threats detection system. arXiv preprint arXiv:2105.10053. Cited by: SS1.
* I. Bergman and V. M. Debben (2020)Classification-based anomaly detection for general data. arXiv preprint arXiv:2005.02359. Cited by: SS1.
* J. Boett, D. M. Mester, X. Giroi-Nieto, and A. G. Ioannidis (2024)Hyper-listart classification for-labart data. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 88, pp. 1114-1123. Cited by: SS1.
* M. M. He, R. Kriegel, R. T. Ng, and J. Sander (2000)LOF: identifying density-based local outliers. In Proceedings of the 2000 ACM SIGMOD international conference on Management of data, pp. 93-104. Cited by: SS1.
* J. Carvalho, M. Zhang, R. Geiger, C. Cortin, and J. M. Buhmann (2023)Invariant anomaly detection under distributed shifts: a causal perspective. Advances in Neural Information Processing Systems36, pp. 255-264. Cited by: SS1.
* S. Chananpaviya, I. Reddy Tamura, et al. (2024)Augmented memory replay-based continual learning approaches for network intrusion detection. Advances in Neural Information Processing Systems36, pp.. Cited by: SS1.
* A. Chaudhry, M. Bahlou, M. Elhoseiny, T. Ask, et al. (2019)On tiny episodic memories in continual learning. arXiv preprint arXiv:1902.10486. Cited by: SS1.
* Y. Kumar Chauhan, J. Zhou, P. Lu, S. M. Mabei, and D. A. Clifton (2023)A brief review of hypernetworks in deep learning. arXiv preprint arXiv:2308.06055. Cited by: SS1.
* S. Chen, Y. Hsu, T. Cui, W. Che, T. Liu, and X. Yu (2020)Real and learn: fine-tuning deep pretrained language models with less forgetting. arXiv preprint arXiv:2004.12651. Cited by: SS1.
* H. Ding, L. Chen, S. Li, Y. Bai, P. Zhou, and Z. Qu (2024)Divide, conquer, and coalesce: meta-parallel graph neural network for lof intrusion detection at scale. In Proceedings of the ACM on Web Conference 2024, pp. 1656-1667. Cited by: SS1.
* K. Ding, Q. Zhou, H. Tong, and H. Liu (2021)Few-shot network anomaly detection via cross-network meta-learning. In Proceedings of the Web Conference 2021, pp. 2448-2456. Cited by: SS1.
* M. Daggi, E. Burceanu, E. Halle, A. Manolache, and P. Brad (2022)AnoSHA: a distribution shift benchmark for unsupervised anomaly detection. Neural Information Processing Systems34, pp.. Cited by: SS1.
* A. Farashah, S. Vogel, K. Raheed, and H. R. Arabnia (2021)A brief review of domain adaptation. Advances in data science and information engineering proceedings from ICDAR 2020, pp. 877-894. Cited by: SS1.
* C. Finn, P. Abbeel, and S. Levine (2017)Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning, pp. 1126-1135. Cited by: SS1.
* A. Fridkin, D. Krompaul, H. Kopken, and V. Tresp (2021)Few-shot one-class classification via meta-learning. In Proceedings of the AAAI conference on artificial intelligence, Vol. 35, pp. 7448-7456. Cited by: SS1.
* I. Gibbs and E. Candes (2021)Adaptive conformal inference under discrete diffusion shift. Advances in Neural Information Processing Systems34, pp. 1660-1672. Cited by: SS1.
* A. Goodge, B. Hooi, S. Ng, and W. S. Ng (2022)Ikmar: unifying legal outlier detection methods via graph neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 66, pp. 6375-6387. Cited by: SS1.
* A. Getton, A. Smola, J. Huang, M. Schmittfull, K. Borgwardt, and B. Scholkopf (2008)Couraging shift by kernel mean matching. Cited by: SS1.
* S. Gui, X. Li, and S. Ji (2024)Active test-time adaptation: theoretical analyses and an algorithm. arXiv preprint arXiv:2404.0504. Cited by: SS1.
* Y. Guo, C. Hu, and Y. Yang (2023)Predict the future from the past? on the temporal data distribution shift in financial sentiment classifications. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 1029-1038. Cited by: SS1.
* D. Han, Z. Wang, W. Chen, K. Wang, R. Yu, S. Wang, H. Zhang, Z. Wang, M. Jin, J. Yang, et al. (2023)Anomaly detection in the open world: normality shift detection, explanation, and adaptation. In NDSS, Cited by: SS1.
* S. Han, X. Hu, H. Huang, M. Jiang, and Y. Zhao (2022)Adventech: anomaly detection benchmark. Advances in Neural Information Processing Systems35, pp. 32142-32159. Cited by: SS1.
* Z. Huang, M. Liang, S. Zhang, and L. Lin (2014)AJUNS: attention-inspired numerical solving for trained data scenarios. In Forty-first International Conference on Machine Learning, pp.. Cited by: SS1.
* N. A. Huynh, W. Ke, B. S. Arjanshi, et al. (2017)A new adaptive learning algorithm and its application to online malware detection. In Discovery Science 20th International Conference, DS 2017, Kyoto, Japan, October 15-17, 2017, Proceedings, pp. 18-32. Cited by: SS1.
* H. Jeffreys (1946)An invariant form for the prior probability in estimation problems. Proceedings of the Royal Society of London. Series A. Mathematical and Physical Sciences186, pp. 455-461. Cited by: SS1.
* X. Ji, H. Choi, O. Sokolsky, and L. Lee (2023)Incremental anomaly detection with parameter-to the internet model things. In Proceedings of the 8th ACM Conference on Internet of Things Design and Implementation, pp. 327-339. Cited by: SS1.
* J. Li, S. Cai, B. Chin Ooi, P. Wang, and Y. Xiong (2023)Robust and transferable log-based anomaly detection. Proceedings of the ACM on Management of Data1, pp. 1-26. Cited by: SS1.
* R. Jordaney, K. Sharaf, S. K. Das, Z. Wang, D. Papini, I. Noertulova, and L. Cavallaro (2017)Transcend: detecting concept drift in malware classification models. In 2016 USENIX security symposium (USENIX security 17), pp. 625-642. Cited by: SS1.
* D. Kim, S. Park, and J. Choo (2024)When model meets new normals: test-time adaptation for unsupervised time-series anomaly detection. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38, pp. 13113-13121. Cited by: SS1.
* L. Klamer, T. P. J. Rudner, M. Retulinger, T. Schindler, G. M. Morris, C. Dee, and Y. Wei (2023)RecSysmover under over: meta-shift with domain-informed prior distributions over functions. In International Conference on Machine Learning, pp. 17176-17197. Cited by: SS1.
* S. Kullinski, S. Bagchi, and D. Inouye (2015)Feature shift detection: localizing which features three shifted via conditional distribution tests. ArXivabs/1507.06929. Cited by: SS1.
* A. Kungsi, T. Iwata, H. Takahashi, and Y. Fujisawa (2023)Meta-learning for robust anomaly detection. In International Conference on Artificial Intelligence and Statistics, pp. 675-691. Cited by: SS1.
* Y. LeCun, K. Kwakcuoglu, and C. Farabet (2010)Convolutional networks and applications in vision. In Proceedings of 2010 IEEE International Symposium on Circuits and Systems, pp. 253-256. Cited by: SS1.
* A. Li, C. Qiu, M. Kloft, P. Smyth, M. Rudolph, and S. Mand (2032)Zero-shot anomaly detection via batch normalization. Advances in Neural Information Processing Systems36, pp.. Cited by: SS1.
* H. Li, L. Ding, M. Fang, and D. Tao (2024)Revisiting catastrophic forgetting in large language model tuning. arXiv preprint arXiv:2406.08482. Cited by: SS1.
* H. Li, S. Zhou, H. Yuan, X. Lu, C. Gao, and S. Chen (2021)Robust modular malware detection against adversarial example attacks. In Proceedings of the Web Conference 2021, pp. 3603-3612. Cited by: SS1.
* Y. Li, W. Chen, B. Yang, L. Tang, and M. Zhou (2023)Prototype-oriented unsupervised anomaly detection for multivariate time series. In International Conference on Machine Learning, pp. 19407-19424. Cited by: SS1.
* Z. Li, Y. Zhao, X. Botin, C. Tonescu, and X. Zhou (2020)COVID: copula-based outlier detection. In 2020 IEEE international conference on data mining (ICDM), pp. 1181-1123. Cited by: SS1.
* Z. Li, Y. Zhao, X. Hu, N. Botta, C. Ionescu, and G. H. Chen (2022)Exact: unsupervised outlier detection using empirical cumulative distribution functions. IEEE Transactions on Knowledge and Data Engineering35, pp. 12181-12193. Cited by: SS1.
* F. Tony Liu, K. Ming Ting, and Z. Zhou (2008)Isolation forest. In 2008 IEEE international conference on data mining, pp. 413-422. Cited by: SS1.
* I. Liu, C. D. Ye, C. Chen, D. Zhang, and Y. Xing (2018)Anomaly-based mixed rhetor dataset containing deep autoencoders. In 2018 IEEE international conference on data mining workshops (ICDM), pp. 39-48. Cited by: SS1.
* X. Lin, L. Yao, P. Xing, H. Oh, G. El Fakhri, J. Kang, J. Woo, et al. (2022)Deep unsupervised domain adaptation: a review of recent advances and perspectives. APSIPA Transactions on Signal and Information Processing11, pp. 1-1. Cited by: SS1.

* [106] Rabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa Dehghani, and James Henderson. 2021. Parameter-efficient multi-task fine-tuning for transformers via shared hypernetworks. _arXiv preprint arXiv:2110.04878_ (2021).
* [107] Yuwen Mao, Taobo Liang, Nan Duan, Haobo Wang, Kai Wang, Lu Chen, and Yunjun Gao. 2022. Less-forcing Multi-based fine-tuning. _Advances in Neural Information Processing Systems 3_ (2022), 14917-14928.
* [108] Feqargs Pendlebury, Fabio Peraori, Roberto Jordanes, Johannes Kinder, and Lorenzo Cavallaro. 2019. [TESSARCLT]: Eliminating experimental bias in malware classification across space and time. In _28th USENIX security symposium (USENIX Security 19)_. 729-746.
* [109] Torusi Penny. 2016. Lots: Lightweight on-line detector of anomalies. _Machine Learning_ 102 (2016), 275-304.
* [110] Stephan Rabanster, Stephan Gunnemann, and Zachary Lipton. 2019. Failing loudly: An empirical study of methods for detecting dataset shift. _Advances in Neural Information Processing Systems 32_ (2019).
* [111] Stefano Ramaswamy, Rajee Nataghi, and Kiryoseh Shim. 2000. Efficient algorithms for mining outliers from large data sets. In _Proceedings of the 2000 ACM SIGMOD international conference on Management of data_ (2002), 4642-4738.
* [112] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster R-CNN: towards real-time object detection with region proposal networks. In _Proceedings of the 28th International Conference on Neural Information Processing Systems Volume 1_ (Montreal, OAN) (1975), MIT Press, Cambridge, MA, USA, 91-99.
* [113] Lukas Ruf. Robert Vanderbilt, Nico Gemertz, Lucas Dresde, Sioubh Amedar Siddiqui, Alexander Binder, Emmanuel Millett, and Marisa Kloft. 2018. Deep one-class classification. In _International conference on machine learning_. PMLR, 4393-4402.
* [114] Bernhard Scholkopf, John C Platt, Shawe-Taylor, Alex J Smola, and Robert C Williamson. 2001. Estimating the support of a high-dimensional distribution. _Neural computation_ 13, 7 (2001), 1443-1471.
* [115] Jessica Montroll, Natalie Harris, Sammi Koejko, Ibrahim M Alabdulmohain, Eva Schindler, Krista Opall-Omg: Alexander Brown, Subhapti Roy, Diana Mincu, Christina Chen, et al. 2022. Diagnosing failures of fairness transfer across distribution shift in real-world medical settings. _Advances in Neural Information Processing Systems 35_ (2022), 19004-19318.
* [116] Aviv Shamsim, Aviv Nayon, Ethan Fetaya, and Gal Chechik. 2021. Personalized federated learning using hypernetworks. In _International Conference on Machine Learning_. PMLR, 989-9502.
* [117] Karchick Shen, Robbin M Jones, Ananya Kumar, Sang Michael Xie, Jeff Z HacChen, Tengyu Ma, and Percy Liang. 2022. Connect, not collapse: Explaining contrastive learning for unsupervised domain adaptation. In _International conference on machine learning_. PMLR, 19847-1987.
* [118] Peter Stojancy, Zijian Li, Mingdong Gong, Ricich Cui, Jianme Cucchiari, and Kun Zhang. 2021. Domain adaptation with invariant representation learning: What transformations to learn? _Advances in Neural Information Processing Systems_ 34 (2021), 21791-21803.
* [119] Zacharian Sutton, Peter Willett, and Yaakov Bar-Shalom. 2018. Modeling and detection of evolving threats using random finite set statistics. In _2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_. IEEE, 4319-4323.
* [120] Korwar Tanvikuni, Xinjie Fan, Huangjie Zheng, Shujian Zhang, Hao Zhang, Bo Chen, and Mingyuan Zhou. 2021. A prototype-oriented framework for unsupervised domain adaptation. _Advances in Neural Information Processing Systems_ 34 (2021), 1719-17208.
* [121] Mahboul Tavallae, Eksehunh Bagheri, Wei Lu, and Ali A Ghorbani. 2009. A detailed analysis of the KDD CUP 94 data set. In _2009 IEEE symposium on computational intelligence for security and algebra applications_. IEEE, 1-6.
* [122] Sebastian Thrun and Lorien Pratt. 1998. Learning to learn: Introduction and overview. In _Learning to learn_. Springer, 3-17.
* [123] Cheng-Hao Tu, Hong-You Chen, Zhede Mui, Jike Zhong, Vardan Palushy, Trays Berger-Wolf, Song Gao, Charles Stewart, Yu Su, and Wei-Lin Harry Chao. 2004. Holistic transfer: towards non-miscriptive fine-tuning with partial target data. _Advances in Neural Information Processing Systems_ 36 (2024).
* [124] A Vaswani. 2017. Attention is all you need. _Advances in Neural Information Processing Systems_ (2017).
* [125] Chen Wang, Zwei Fan, Liangyuei Yang, Mingliqi Yang, Xiaolong Liu, Zhiwei Lin, and Philip Yu. 2020. Pre-Training with Transferable Attention for Addressing Market Shifts in Cross-Market Sequential Recommendation. In _Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_. 2970-2978.
* [126] Hao Wang. 2024. Improving Neural Network Generalization on Data-limited Regression with Doubly-Robust Boosting. In _Proceedings of the AAAI Conference on Artificial Intelligence_, Vol. 30, 2821-2829.
* [127] Ze Wang, Yipin Zhou, Rui Wang, Tsung-Yu Lin, Ashish Shah, and Ser Nam Lim. 2022. Few-shot fast-adaptive anomaly detection. _Advances in Neural Information Processing Systems_ 35 (2022), 4957-4970.
* [128] Carrett Wilson and Diane J Cook. 2000. A survey of unsupervised deep domain adaptation. _ACM Transactions on Intelligent Systems and Technology (TIST)_ 11, 5 (2020), 1-46.
* [129] Shuo Yang, Xinran Zheng, Jinze Li, Jingfeng Xu, Xingjun Wang, and Edith CH Ngai. 2024. ReCLN: Concept Drift Adaptation with Representation Enhancement for Network Intrusion Detection. In _Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, 3818-3828.
* [130] Shuhan Yuan, Panpan Zheng, Xintao Wu, and Hanghang Tong. 2020. Few-shot instead filter detection. In _Proceedings of the 29th ACM international conference on Information & Knowledge Management_. 2289-2292.
* [131] Hai Zhang, Chunwei Wu, Guifeng Cao, Hailing Wang, and Wenming Cao. 2024. Hyperfilter: Achieving Both Authenticity and Cross-Domain Capability in Image Editing via Hypertext. In _Proceedings of the AAAI Conference on Artificial Intelligence_, Vol. 38, 7051-7059.
* [132] Jiajun Zhang, Shuheng Li, Liyuei Huang, Jian Wang, Xiaohuan Fu, Denhi Hong, Rajesh K Gupta, and Jingbo Shang. 2020. How Few-shot Improved Fine-Grained Learning in Resource-Skewed Edge Computing Environments. In _Proceedings of the ACM on Web Conference_. 2020. 2976-2985.
* [133] Jannan Zhang, Xinyang Zhang, Dehhi Hong, Rajesh K Gupta, and Jingbo Shang. 2023. Minimally Supervised Contextual Inference from Human Mobility: An Iterative Collaborative Distillation Framework. In _2021_. 2450-2458.
* [134] Yong Zhong, Hongtao Lu, Xiaodong Liu, Fan Rao, Weizen Shen, and Chongquan Li. 2022. Deep generative modeling on limited data with regularization by nontransferable pre-trained models. _arXiv preprint arXiv:2206.14133_ (2022).
* [135] Bo Zong, Qi Song, Martin Rennigang Min, Wei Cheng, Cristian Lumenen, Doshi Cho, and Haifeng Chen. 2018. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. In _International conference on learning representations_.

## Appendix A Appendix

### Backbone Models and Implementation Details

* **AutoEncoder (AE) [(1)]:** is an unsupervised model trained to reconstruct the input through an encoder-decoder structure. The key idea is that, threat samples, appearing less frequently, tend to have larger reconstruction losses, making them distinguishable by observing the loss. We implement the AutoEncoder with four linear layers followed by ReLU activation. These layers project the data into [(64; 32; 64)]-dimensional features and finally map the features to the original data dimension. Cross-entropy loss is applied to categorical features, and mean-square error is used for numerical features.
* **DeepSVDD (DSVDD) [(56)]:** is an unsupervised model which encodes data into feature representations and measures their distances from a learnable center. The encoder is a multi-layer perception (MLP) consists of two linear layers with ReLU activation, mapping data to representations of dimension [(64; 32)]. Similar to the AutoEncoder, threat samples tend to have larger distances from the center. The smooth L1 loss [(55)] is used to measure the distances for its robustness against outliers.
* **GOAD [(7)]:** is a semi-supervised model that applies multiple transformations to the input data and uses a convolutional neural network (CNN) [(39)] to extract feature representations. We implement a 5-layer CNN with kernel size of 1. The loss function has two components: a center triplet loss, which measures the distance between the learned representations and their mean, and a cross-entropy loss for predicting which transformation was applied to the data.

### Convergence Analysis On REACT

In this section, we provide convergence analysis of REACT on linear models under the premise of Theorem 1. That the models \(h\) and \(f\) admit the form (\(\mathbf{\upmu}\)), the adaptive weights are updated by exactly solving Eq. 1 and relevant datasets are sampled at the beginning of the algorithm and fixed throughout the iterations.

Based on Eq. (4), we consider the following objective function in the analysis.

\[\mathcal{L}(f(X;\theta_{\text{meta}},\theta_{\text{adapt}}))=\frac{1}{2}\|f(X; \theta_{\text{meta}},\theta_{\text{adapt}})-Y\|^{2}+\frac{\lambda}{2}\|\theta_ {\text{adapt}}\|^{2},\]

which consists of a mean squared error and a L2 regularization for the adaptive weights (see Section 4.2). \(Y\) is the target associated with the input data in the loss function. It can have different forms according to the underlying target model. For example, it can be the input data for reconstruction loss, center of samples for methods like DeepSVDD, or labels of samples in cases of supervised or semi-supervised learning.

Without loss of generality we are going to set \(\theta\in\mathbb{R}^{d_{1}}\) and \(\phi\in\mathbb{R}^{d_{2}}\) for some \(d_{1},d_{2}>0\). Notice that this assumption can be generalized by considering vectorization of the matrix product and hence our results in this proof can easily be extended to more generic output spaces. We also note that the assumption that the datasets have uncorrelated constant variance, i.e. \((X^{i})^{\top}(X^{i})=\sigma_{i}I\) is to make the computations in the proof easier. The results in the proof can be relaxed to bounded norm, i.e. \(\|X^{i}\|_{2}\leq\sigma_{i}\) where \(\|.\|_{2}\) is L-2 norm on the matrix space.

We restate the theorem here.

**Theorem 1**.: _Consider REACT on the linear model in (4) with Eq. (1) being solved exactly. Let \(X^{i}_{1}\) and \(X^{i}_{2}\) satisfy \((X^{i}_{1})^{\top}X^{i}_{2}=(X^{i}_{1})^{\top}X^{i}_{2}=\sigma_{i}I\) for each task \(i\in\{1,..,M\}\), where \(\sigma_{i}\) are the variances and \(I\) is the identity matrix. Learning rates are chosen as \(\eta_{\text{meta}}<1/\sum_{i=1}^{M}\sigma_{i}\lambda/(\sigma_{i}+\lambda)\) and \(\eta_{\text{k}}<1/\max\left(\sum_{j=1}^{M}\sigma_{j}(j_{j}+\lambda),\|\mathbf{ X}_{\text{k}}\|\right)\) where \(\mathbf{X}_{\text{k}}=\sum_{j=1}^{M}\sigma_{j}(X^{i}_{j})^{\top}\). Then, for any \(\varepsilon>0\), there exists_

\[K=\mathcal{O}\left(\log_{1/\rho_{\text{meta}}}(1/\varepsilon)+\log_{1/\rho_{ \text{k}}}(1/\varepsilon)\right)\]

_for \(\rho_{\text{meta}}=1-\eta_{\text{meta}}\sum_{i=1}^{M}\sigma_{i}\lambda/( \sigma_{i}+\lambda)\) and \(\rho_{\text{k}}=1-\eta_{\text{k}}\sum_{j=1}^{M}\sigma_{j}(\sigma_{j}+\lambda)\) such that the \(K\)-iteration of Algorithm 1 satisfies_

\[\|\theta^{K}-\theta^{*}\|\leq\varepsilon,\qquad\text{and}\qquad\|\theta^{K}- \phi^{*}\|\leq\varepsilon,\]

_where \(\theta^{*}\) and \(\phi^{*}\) are stationary points of the algorithm._

Proof.: We prove the result following the steps in Algorithm 1. Let \(\theta_{\text{adapt}}^{i,k}\) be the fine-tuned adaptive weights of task \(i\) computed by REACT algorithm at \(k\)-th iteration and similarly, \(\phi^{k}\) denote the hypernetwork parameters, and \(\theta_{\text{meta}}^{k}\) be the meta weights at iteration \(k\) of REACT.

**Task fine-tuning:** The exact intermediate updates defined in (1) can be rewritten as follows.

\[\theta_{\text{adapt}}^{i,k+1}=\operatorname*{argmin}_{\theta}\frac{1}{2}\|X^{ i}_{s}(\theta_{\text{meta}}^{k}+\theta)-Y^{i}_{s}\|^{2}+\frac{\lambda}{2}\| \theta\|^{2}\]

Setting gradient to zero, we have

\[\mathbf{0}=(X^{i}_{s})^{\top}X^{i}_{s}(\theta_{\text{adapt}}^{i,k+1}+\theta_{\text {meta}}^{k})-(X^{i}_{s})^{\top}Y^{i}_{s}+\lambda\theta_{\text{adapt}}^{i,k+1}\]

where \(\mathbf{0}\) is the vector of all zeros. This implies

\[\theta_{\text{adapt}}^{i,k+1}=\frac{1}{\sigma_{i}+\lambda}(X^{i}_{s})^{\top}Y^ {i}_{s}-\frac{\sigma_{i}}{\sigma_{i}+\lambda}\theta_{\text{meta}}^{k}, \tag{5}\]

where we used the fact that \((X^{i}_{s})^{\top}X^{i}_{s}=\sigma_{i}I\).

**Meta weight update:** Next, we consider the gradient update of meta weight in (2). The gradient with respect to \(\theta_{\text{meta}}\) is

\[\nabla_{\theta_{\text{meta}}}\sum_{x\in\Omega_{\text{meta}}^{i}} \mathcal{L}\left.\left(f(x;\theta_{\text{meta}},\theta_{\text{adapt}}^{i,k+1}) \right)\right|_{\theta_{\text{meta}}^{k}}^{k} \tag{6}\] \[=(X^{i}_{q})^{\top}X^{i}_{q}\left(\theta_{\text{meta}}^{k}+\theta _{\text{adapt}}^{i,k+1}\right)-(X^{i}_{q})^{\top}Y^{i}_{q}\] \[=\sigma_{i}\left(\frac{\lambda}{\sigma_{i}+\lambda}\theta_{\text{ meta}}^{k}+\frac{1}{\sigma_{i}+\lambda}(X^{i}_{s})^{\top}Y^{i}_{s}\right)-(X^{i}_{q})^{ \top}Y^{i}_{q},\]where the last equality is given by (5) and the assumption in data covariance matrix. Therefore, the gradient update step is

\[\theta_{\text{meta}}^{k+1} =\theta_{\text{meta}}^{k}-\eta_{\text{meta}}\sum_{i=1}^{M}\sum_{x \in\mathbbm{D}_{\text{query}}^{\prime}}\nabla_{\theta_{\text{meta}}}\mathcal{L} \left(f(x;\theta_{\text{meta}}^{k};\theta_{\text{adapt}}^{i,k+1})\right)\] \[=\theta_{\text{meta}}^{k}-\eta_{\text{meta}}\sum_{i=1}^{M}\sigma_{ i}\left(\frac{\lambda}{\sigma_{i}+\lambda}\theta_{\text{meta}}^{k}+\frac{1}{ \sigma_{i}+\lambda}(X_{s}^{i})^{\top}Y_{s}^{i}\right)-(X_{q}^{i})^{\top}Y_{q}^ {i}\] \[=\left(1-\eta_{\text{meta}}\sum_{i=1}^{M}\frac{\sigma_{i}\lambda}{ \sigma_{i}+\lambda}\right)\theta_{\text{meta}}^{k}\] \[\quad-\eta_{\text{meta}}\sum_{i=1}^{M}\frac{\sigma_{i}}{\sigma_{i} +\lambda}(X_{s}^{i})^{\top}Y_{s}^{i}-(X_{q}^{i})^{\top}Y_{q}^{i}\]

Let us introduce \(\rho_{\text{meta}}=1-\eta_{\text{meta}}\sum_{i=1}^{M}\sigma_{i}\lambda/(\sigma _{i}+\lambda)\), and choose learning rate \(0<\eta_{\text{meta}}<1/\Sigma_{\text{meta}}^{M}\sigma_{i}/(\sigma_{i}+\lambda)\) so that \(0<\rho_{\text{meta}}<1\).

The stationary point \(\theta_{\text{meta}}^{*}\) should satisfy

\[\theta_{\text{meta}}^{*} =\left(1-\eta_{\text{meta}}\sum_{i=1}^{M}\frac{\sigma_{i}\lambda} {\sigma_{i}+\lambda}\right)\theta_{\text{meta}}^{*}\] \[\quad-\eta_{\text{meta}}\sum_{i=1}^{M}\frac{\sigma_{i}}{\sigma_{i} +\lambda}(X_{s}^{i})^{\top}Y_{s}^{i}-(X_{q}^{i})^{\top}Y_{q}^{i}.\]

Thus, we obtain

\[(\theta_{\text{meta}}^{k+1}-\theta_{\text{meta}}^{*})=\rho_{\text{meta}}(\theta _{\text{meta}}^{k}-\theta_{\text{meta}}^{*})\]

yielding.

\[\|\theta_{\text{meta}}^{k+1}-\theta_{\text{meta}}^{*}\|\leq\rho_{\text{meta}} \|\theta_{\text{meta}}^{k}-\theta_{\text{meta}}^{*}\|\leq\cdots\leq\rho_{ \text{meta}}^{k+1}\|\theta_{\text{meta}}^{0}-\theta_{\text{meta}}^{*}\|. \tag{6}\]

**Hypernetwork update:** Lastly, the gradient of the objective function update with respect to \(\phi^{k}\) is

\[\nabla_{\phi}\sum_{x\in\mathbbm{D}_{\text{query}}^{\prime}}\mathcal{L}\left(f( x;\theta_{\text{meta}}^{k+1},h(X_{s}^{j};\phi))\right)\Big{|}_{\phi^{k}}\]

Thus, the update (3) can be written as

\[\phi^{k+1} =\phi^{k}-\eta_{h}\sum_{j=1}^{n_{h}}\sum_{x\in\mathbbm{D}_{\text{ query}}^{\prime}}\nabla_{\phi}\mathcal{L}\left(f(x;\theta_{\text{meta}}^{k+1},h(X_{s}^{j}; \phi))\right)\] \[=(1-\eta_{h}\sum_{j=1}^{M}\sigma_{j}(\sigma_{j}+\lambda))\phi^{k}\] \[\quad-\eta_{h}\left(\sum_{i=1}^{M}\sigma_{j}(X_{s}^{j})^{\top} \right)\theta_{\text{meta}}^{k+1}\pi_{h}\sum_{i=1}^{M}\left(X_{q}^{j}X_{s}^{ j}\right)^{\top}Y_{q}^{j}\]

where the last equality follows from \((X_{q}^{j})^{\top}X_{q}^{j}=(X_{s}^{j})^{\top}X_{s}^{j}=\sigma_{j}I\).

Notice that the choice of learning rate \(\eta_{h}\) implies \(0<\eta_{h}<1/\max\left(\Sigma_{j=1}^{n_{h}}\sigma_{j}(\sigma_{j}+\lambda),\| \mathbf{X}_{s}\|\right)\) so that the rate \(\rho_{h}\) and \(\mathbf{X}_{s}\) satisfy \(0<\rho_{h}<1\) and \(0<\eta_{h}\|\mathbf{X}_{s}\|<1\). On the other hand, the stationary points \(\phi^{*}\) and \(\theta^{*}\) satisfy

\[\phi^{*}=\rho_{h}\phi^{*}-\eta_{h}\mathbf{X}_{s}\theta_{\text{meta}}^{*}+\eta_ {h}\sum_{i=1}^{M}\left(X_{q}^{j}X_{s}^{j}\right)^{\top}Y_{q}^{j}\]

yielding

\[\phi^{k+1}-\phi^{*}=\rho_{h}(\phi^{k}-\phi^{*})-\eta_{h}\mathbf{X}_{s}(\theta_ {\text{meta}}^{k+1}-\theta_{\text{meta}}^{*})\]

and

\[\|\phi^{k+1}-\phi^{*}\|\leq\rho_{h}\|\phi^{k}-\phi^{*}\|+\eta_{h}\|\mathbf{X}_{s }\|\|\theta_{\text{meta}}^{k+1}-\theta_{\text{meta}}^{*}\| \tag{7}\]

**Convergence:** With (6), we can show that for \(k\geq K_{\text{meta}}=\log_{1/\rho_{m}}(1/c)+\log_{1/\rho_{m}}(\|\theta_{\text{ meta}}^{0}-\theta_{\text{meta}}^{*}\|)\), we have

\[\|\theta_{\text{meta}}^{k}-\theta_{\text{meta}}^{*}\|\leq\varepsilon.\]

Similarly, from (7), we get

\[\|\phi^{k}-\phi^{*}\| \leq\rho_{h}\|\phi^{k-1}-\phi^{*}\|+\eta_{h}\|\mathbf{X}_{s}\|\| \theta_{\text{meta}}^{k}-\theta_{\text{meta}}^{*}\|\] \[\leq\rho_{h}\|\phi^{k-1}-\phi^{*}\|+\eta_{h}\|\mathbf{X}_{s}\|\| \theta_{\text{meta}}^{k}\|\theta_{\text{meta}}^{k}-\theta_{\text{meta}}^{*}\|\] \[\leq\rho_{h}^{2}\|\phi^{k-2}-\phi^{*}\|+\eta_{h}\|\mathbf{X}_{s}\| \|\theta_{\text{meta}}^{0}-\theta_{\text{meta}}^{*}\|\Big{[}\rho_{\text{meta}}^{k}+ \rho_{h}\rho_{\text{meta}}^{k-1}\Big{]}\] \[\quad\cdots\] \[\leq\rho_{h}^{k}\|\phi^{0}-\phi^{*}\|+\frac{\rho_{\text{meta}}\eta_{h }\|\mathbf{X}_{s}\|\|\theta_{\text{meta}}^{0}-\theta_{\text{meta}}^{*}\|}{\rho_{ \text{meta}}-\rho_{h}}(\rho_{\text{meta}}^{k}-\rho_{h}^{k})^{\frac{1366}{13