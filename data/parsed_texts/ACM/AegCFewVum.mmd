# M\({}^{2}\)-VLP: Enhancing Multilingual Vision-Language Pre-Training via Multi-Grained Alignment

Anonymous Author(s)

###### Abstract.

Recently, multilingual Vision-Language Pre-training (mVLP) has shown remarkable progress in learning joint representations across different modalities and languages. However, most existing methods learn semantic alignment at a coarse-grained level and fail to capture fine-grained correlations between different languages and modalities. To address this, we propose a **M**ulti-grained **M**ultilingual **V**ision-Language Pre-training (M\({}^{2}\)-VLP) model, which aims to learn cross-lingual cross-modal alignment at different semantic granular levels. In cross-lingual interaction, the model learns the global alignment of parallel sentence pairs and the word-level correlations. In cross-modal interaction, the model aligns images with captions and image regions with corresponding words. To integrate the cross-lingual and cross-modal alignment above, we propose a unified multi-grained contrastive learning paradigm. Under zero-shot cross-lingual and fine-tuned multilingual settings, extensive experiments on vision-language downstream tasks across twenty languages demonstrate the effectiveness of M\({}^{2}\)-VLP over competitive contrastive models. The anonymous code is available in [https://anonymous.4open.science/r/M2-VLP-ANNO-27BA](https://anonymous.4open.science/r/M2-VLP-ANNO-27BA).

**CCS Concepts - Computing methodologies Natural language processing.**

## 1. Introduction

Vision-Language Pre-training (VLP) research plays a pivotal role in shaping the future landscape of the internet, as it greatly enhances the intelligence of web applications (e.g., search engines [42] and recommendation [45]). Typically, VLP models learn cross-modal joint representations from large-scale image-text pairs during the pre-training stage, which are then fine-tuned on downstream vision-language tasks, such as visual question answering, image captioning, and image-text retrieval [7]. While most works are based on English, recent studies on multilingual VLP (mVLP) have attempted to overcome the language barrier by extending VLP models to multilingual scenarios. By aligning the representations from different modalities and languages, mVLP models achieve promising results on various cross-lingual cross-modal downstream tasks[6, 30, 34, 52].

However, most existing mVLP methods only learn cross-lingual and cross-modal alignment at a coarse-grained level, ignoring the critical role of explicit fine-grained semantic alignment. As illustrated in Figure 1 (a, b), previous methods can be categorized mainly into two paradigms: (a) The translation-based methods [6, 34, 52] extend English image-text pairs to other languages using translation engines, subsequently performing VLP on the generated multilingual data. (b) The unified modeling methods [30] aim to

Figure 1. An illustration of the differences between existing mVLP methods and M\({}^{2}\)-VLP. Most existing methods align cross-lingual cross-modal representations at a coarse-grained level, while M\({}^{2}\)-VLP performs multi-grained aligning.

align cross-modal image-text pairs and cross-lingual translation pairs within a unified framework. Despite their advances, the pre-training paradigms of these models are limited to coarse-grained level, lacking explicit supervision to capture fine-grained correlations between different languages and modalities.

In this paper, we explore cross-lingual cross-modal alignment from the perspective of human learning. When we learn a new language, the initial step is to memorize the vocabulary and align the vocabulary with our native language. Similarly, when young children learn to recognize images, they are first taught to associate the objects depicted in the image with the corresponding concepts. These observations suggest that fine-grained token-token and region-token aligning signals may be effective in understanding detailed correlations in both cross-lingual and cross-modal interaction. Inspired by this, we introduce a Multi-grained Multilingual Vision-Language Pre-training model, namely M\({}^{2}\)-VLP. As shown in Figure 1 (c), the model integrates four aligning tasks with different granular levels through a unified multi-grained contrastive learning strategy. In cross-modal contrastive, M\({}^{2}\)-VLP aligns global images with English captions at a coarse-grained level, and image regions with corresponding English phrases at a fine-grained level. In cross-lingual contrastive, the model simultaneously learns sentence-level and token-level alignment using parallel translation pairs. Through a unified formulation for different data streams, the model achieves multi-grained cross-lingual cross-modal cascading alignment without any multilingual image-text pairs. To further improve cross-lingual and cross-modal interactions, M\({}^{2}\)-VLP performs three Masked Language Modeling (MLM) tasks, including Vision MLM (VMLM), cross-lingual MLM (xMLM), and Translation Language Modeling (TLM).

Under zero-shot cross-lingual and fine-tuned multilingual settings, we demonstrate the effectiveness of M\({}^{2}\)-VLP across a broad range of downstream vision-language tasks, including visually-grounded natural language inference, visual question answering, visual reasoning, and image-text retrieval. Our experiments cover a set of 20 target languages from diverse language families. Experimental results show that M\({}^{2}\)-VLP significantly outperforms competitive mVLP models with an averaged improvement of 2.9% and 10.6% on cross-lingual vision-language understanding and retrieval tasks in the IGLUE benchmark. Visualization analysis of the proposed model further demonstrates its ability to perform fine-grained cross-modal and cross-lingual interactions.

Our contributions can be summarized as follows:

* We present M\({}^{2}\)-VLP, the first known effort to explicitly learn multi-grained cross-lingual and cross-modal aligning in multilingual vision-language pre-training.
* A unified contrastive learning strategy is proposed to learn Vision-to-English and English-to-X languages alignment in a multi-grained manner. With English serving as the aligning bridge, it achieves cross-lingual cross-modal aligning without any multilingual image-text data.
* Extensive experiments on 7 vision-language tasks across 20 languages demonstrate the effectiveness of M\({}^{2}\)-VLP. Further analysis is conducted to show the fine-grained aligning ability across different modals and languages.

## 2. Related Works

### Multilingual Language Models

Recent studies (Kang et al., 2018; Liu et al., 2019; Liu et al., 2019) have demonstrated the effectiveness of multilingual pre-trained language models on various downstream tasks. Multilingual BERT (Liu et al., 2019) is the first work to extend the monolingual pre-training to the multilingual setting by performing masked language modeling (MLM) on large-scale multilingual corpora. XLM (Liu et al., 2019) introduced translation language modeling to achieve better cross-lingual alignment, while XLM-R (Liu et al., 2019) further enlarged the model size and training corpora. Based on these models, several methods have been proposed to enhance multilingual representation through different perspectives. InfoXLM (Liu et al., 2019) introduces sentence-level contrastive loss, aiming to maximize the mutual information between translation pairs. HICTL (Wang et al., 2019) bridges the semantic discrepancy across languages through hierarchical contrastive learning. UniPrompt (Li et al., 2019) introduces a language-agnostic prompting model to alleviate the effort of designing multilingual prompt templates for different languages. MLM-GC (Chen et al., 2019) leverages the global co-occurrence information from multilingual corpora to enhance semantic alignment. EMMA-X (Liu et al., 2019) integrates cross-lingual representation learning with semantic relation prediction within an expectation-maximization framework. In addition, several studies focus on improving language-agnostic representations (Chen et al., 2019) and mitigating the influence of linguistic discrepancy (Liu et al., 2019) between the source and target languages.

### Vision-Language Pre-training

Based on the way of integrating features from vision and language, VLP models generally fall into two categories: dual-encoder and fusion-encoder architecture.

The dual-encoder model (Wang et al., 2019; Wang et al., 2019) consists of an image encoder and a text encoder to encode images and text separately. Then, it adopts straightforward methods such as shallow attention layer (Wang et al., 2019) or dot product (Wang et al., 2019; Wang et al., 2019) to model the interaction between different modalities. However, the simple interaction is not enough to handle tasks that require complex reasoning, such as visual reasoning and visual question answering (Kang et al., 2018).

The fusion-encoder model takes text embeddings and image features as input and employs a deep fusion encoder to learn vision-language interaction. VisualBERT (Devlin et al., 2019) and VL-BERT (Wang et al., 2019) implicitly align elements of an input text and associated image regions with self-attention. Instead of simply using image-text pair, OSCAR (Wang et al., 2019) incorporates object detection tags within the image to enhance the fusion encoder to better align different modalities. ALBEF (Wang et al., 2019) utilizes a text and a vision encoder to independently learn intra-modal interaction, followed by a fusion-encoder with cross-attention for cross-modal interaction. BEIT-3 (Wang et al., 2019) proposes a multiway transformer by performing masked data modeling on images, texts, and image-text pairs. Besides, some approaches leverage the pre-trained Large Language Models (LLM) to enhance VLP. For example, Flamingo (Chen et al., 2019) aligns the vision encoder and LLM using a perceiver resampler, which shows remarkable few-shot performance. BLIP-2 (Wang et al., 2019) bridges the modality gap with a lightweight querying transformer. LLVA (Wang et al., 2019) connects the visual encoder of CLIP (Wang et al., 2019) with the language decoder Vicuna (Wang et al., 2019), and conducts end-to-end fine-tuning on generated instructional vision-languagedataset. Due to the scarcity of large-scale multilingual image-text data, most VLP methods mainly focus on English.

### Multilingual Vision-Language Pre-training

As multilingual language models advance rapidly, some research attempts to explore universal representations across multiple languages and modalities. MURAL (MURAL, 2017) extends the ALIGN (Liang et al., 2017) model to multilingual settings by applying multi-task contrastive learning on image-text pairs and translation pairs. To construct multilingual image-text pairs, M-CLIP (Chen et al., 2019) and UC2 (Zhou et al., 2019) extend English-only datasets via machine translation, and leverage the generated datasets for vision-language alignment. As these methods highly depend on the quality of the translation engine, some research has attempted alternative paradigms. RC3(Zhou et al., 2019) applies regularized contrastive learning that constrains the representation proximity of weakly-aligned multilingual vision-language inputs. M3P (Zhou et al., 2019) enforces explicit alignment through a code-switching strategy, replacing English words in image-text pairs with their synonyms in the target languages. Li et al. (Li et al., 2019) proposes a weakly supervised framework to effectively unify cross-lingual and cross-modal pre-training, achieving remarkable results on various cross-lingual vision-language tasks.

## 3. Methodology

In this section, we present a multi-grained multilingual framework M2-VLP. We first briefly introduce the three types of data streams used for pre-training in section 3.1. Then we describe the model architecture and pre-training objectives in section 3.2 and 3.3 respectively.

### Data Stream

We use three data streams: multilingual text stream, parallel text stream, and monolingual image-text stream. Different from most previous methods, our approach does not rely on machine translation engines or multilingual image-text datasets. Details regarding the three data streams are as follows:

Multilingual Text StreamTo learn cross-lingual modeling, we use multilingual text stream as model input. Given \(N\) languages \(\{L_{i}\}_{i=1}^{N}\), we construct a multilingual text dataset \(D_{M}=\cup_{i=1}^{N}\{x_{j}^{L_{i}}\}_{j=1}^{N}\), where \(N_{L_{i}}\) denotes the number of training sentences in language \(L_{i}\) and \(x_{j}^{L_{i}}\) is the \(j\)-th sentence in language \(L_{i}\). Due to significant differences in data size for different languages, sentences are sampled according to a multinomial distribution (Liang et al., 2017). This sampling increases the proportion of low-resource languages and alleviates the training bias towards high-resource languages.

Parallel Text StreamTo learn cross-lingual semantic alignment, we apply parallel text stream. Given \(N\) languages \(\{L_{i}\}_{i=1}^{N}\), we construct an English-X parallel text dataset \(D_{T}=\cup_{i=1}^{N}\{x_{j}^{L_{i}},y_{j}^{L_{i}}\}_{j=1}^{N}\), where \(\{(x_{j}^{E_{m}},y_{j}^{L_{j}})\}\) is the \(j\)-th English-\(L_{i}\) translation pair and \(N_{L_{i}}^{\prime}\) denotes the number of translation pairs in \(L_{i}\).

**Monolingual Image-Text Stream** To learn cross-modal modeling, we use monolingual image-text stream. This data stream consists of two parts: coarse-grained image-caption data and fine-grained region-token data. We denote image-caption data as \(D_{I}=\{(j_{r},x_{j}^{E_{m}})\}_{j=1}^{N}\), where \((I_{j},x_{j}^{E_{m}})\) represents an image-caption pair and \(N_{I}\) is the number of image-caption samples. For images in \(D_{I}\), we also construct fine-grained region-token dataset \(D_{R}=\{(R_{j},x_{j}^{E_{m}})\}_{j=1}^{N_{R}}\), where \((R_{j},x_{j}^{E_{m}})\) denotes an region-token pair and \(N_{R}\) is the number of region-token samples. Note that the "token" here can not only be a word, but also textual phrases that describe a specific region of the image. Specifically, we utilize the existing image annotations of object detection and region description to form region-token pairs. Based on region and object coordinate labels, we cut the original images to obtain region \(R_{j}\). \(x_{j}^{E_{m}}\) for objects are original object labels. If an object annotation contains additional attributes (e.g., color or shape), we concatenate the attributes with the original labels as the region description. \(x_{j}^{E_{m}}\) for regions are phrases that describe the specific regions in original images. Furthermore, we augment \(D_{I}\) and \(D_{R}\) with a code-switching strategy, in which the model randomly replaces tokens in image-text pairs with their target language synonyms.

### Model Architecture

As illustrated in Figure 2, we construct a two-steam framework that contains a vision encoder to learn visual features, a text encoder to learn textual features, and a fusion encoder to learn cross-modal interactions. The text and vision encoders adopt standard Transformer (Vaswani et al., 2017) architecture with \(N_{V}\) and \(N_{T}\) layers respectively.

Specifically, texts and images are fed into the corresponding uni-modal encoders to perform intra-modal interaction. Since our method is based on multi-grained aligning, "image" can refer to a global image or a specific region, and "text" can refer to a complete caption or a text phrase describing the corresponding region. For text input, we feed it to the text encoder and get the text representation \(T=\{t_{cls},t_{1},...,t_{n}\}\). For image input, the vision encoder transforms images into fixed-size \(m\) patches to get image representation \(I=\{i_{cls},i_{1},...,i_{m}\}\). Similar to language models (Vaswani et al., 2017), a special token [CLS] is prepended to the image patches, serving as the representation of the global image.

After that, image and text representations are fed into the fusion encoder. To learn the alignment between different modalities and languages, we follow (Li et al., 2019) to adopt a pluggable cross-attention layer in the fusion module, which allows different routines for different data streams. For image-text stream \(D_{I}\) and \(D_{R}\), the cross-attention layers are activated to learn cross-modal interaction. Mathematically, the cross-attention between text feature \(x_{t}\) and image feature \(x_{i}\) can be denoted as

\[\begin{split} q=x_{t}\mathbf{W}_{q},\quad k=x_{i}\mathbf{W}_{k}, \quad v=x_{t}\mathbf{W}_{v},\\ \text{Attention}(q,k,v)=softmax(\frac{qk^{T}}{\sqrt{d_{k}}})\cdot v,\end{split} \tag{1}\]

where \(\mathbf{W}_{q}\in\mathbb{R}^{d_{m}\times d_{q}}\), \(\mathbf{W}_{k}\in\mathbb{R}^{d_{m}\times d_{k}}\), and \(\mathbf{W}_{v}\in\mathbb{R}^{d_{m}\times d_{v}}\) are learnable weight matrices, and \(d_{m}\), \(q_{d}\), \(k_{d}\), and \(d_{v}\) are dimensions of the input embedding, query, key, and value vectors. For parallel text stream \(D_{T}\), the cross-attention layers are activated to learn cross-language interaction. The cross-attention query, key, and value vectors of parallel text input (\(x_{p},x_{q}\)) can be formulated as

\[q=x_{p}\mathbf{W}_{q},\quad k=x_{q}\mathbf{W}_{k},\quad v=x_{q}\mathbf{W}_{v}. \tag{2}\]For unpaired text stream \(D_{M}\), the cross-attention layers are skipped. Different from previous methods that learn cross-modal and intra-modal modeling in a single self-attention layer, the pluggable cross-attention modules can learn extensive interaction and semantic alignment.

### Pre-training Objectives

We optimize M2-VLP by employing two types of pre-training objectives. On top of the uni-modal encoders, the model performs multi-grained contrastive learning. On top of the fusion encoder, the model is trained by masked language modeling and vision-language matching. Our goal is to achieve Vision-to-X languages alignment by learning multi-grained Vision-to-English alignments and English-to-X languages alignments. In this way, the model learns cascading alignments between Vision-English-X languages, with English serving as the aligning bridge.

#### Multi-grained contrastive learning

To cascadingly align unimodal representations before fusion, we propose a unified multi-grained contrastive learning to simultaneously align both of the English-to-Vision and English-to-X languages. For a batch of \(N\) pairs \(\{(A_{i},B_{i})\}_{N}^{N}\), \(B_{i}\) is the positive sample for \(A_{i}\), and the other \(N-1\) samples within the batch are negative samples. Specifically, \((A_{i},B_{i})\) refers to an image-text pair or a translation text pair. For each pair, we calculate the softmax-normalized \(A\)-to-\(B\) similarity as:

\[\mathbf{P}_{n}^{A2B}(A)=\frac{exp(sim(A,B_{n})/\tau)}{\sum_{n=1}^{N}exp(sim(A,B_{n })/\tau)}. \tag{3}\]

Similarly, \(B\)-to-\(A\) similarity is:

\[\mathbf{P}_{n}^{B2A}(B)=\frac{exp(sim(B,A_{n})/\tau)}{\sum_{n=1}^{N}exp(sim(B,A_{n })/\tau)}, \tag{4}\]

where \(\tau\) is a learnable temperature parameter and the function \(sim(\cdot)\) calculate the cosine similarity between \(A\) and \(B\). For the \(i\)-th pair, let \(\mathbf{y}^{A2B}(A)\) and \(\mathbf{y}^{B2A}(B)\) denote the ground-truth one-hot similarity, where negative pairs have a probability of 0 and the positive pair has a probability of 1. The contrastive loss of pair \((A,B)\) is defined as the cross-entropy \(H\) between \(\mathbf{p}\) and \(\mathbf{y}\):

\[\mathcal{L}_{CL}=\frac{1}{2}\mathbb{E}_{(A,B)\sim D_{L,\mathbf{y}T}}[ H(\mathbf{y}^{A2B}(A),\mathbf{p}^{A2B}(A))+\] \[H(\mathbf{y}^{B2A}(B),\mathbf{p}^{B2A}(B))]. \tag{5}\]

In multi-grained contrastive, pair \((A_{i},B_{i})\) can be four types of data inputs. In the coarse-grained level, pair \((A_{i},B_{i})\) can be an embedded vector of an image-caption pair (from data stream \(D_{T}\)) or a textual translation pair (from data stream \(D_{T}\)). In fine-grained level, \((A_{i},B_{i})\) can be embedded vector of a region-token pair (from data stream \(D_{R}\)) or a token-token pair (from word align matrix \(M\) of data stream \(D_{T}\)).

As shown on the right side of Figure 2, since no labeled word pairs are accessible, we utilize unsupervised word aligner (FastAlign(Fan et al., 2017)) to construct word align matrix \(\mathcal{M}\) for translation pairs. Specifically,

Figure 2. Illustration of the proposed framework. M2-VLP consists of vision, text, and fusion encoders. Modules with the same color share the same parameters. We use cuboids with different colors to highlight the data flow within the model. \(\mathcal{L}\) with different subscripts denotes different pre-training losses.

for every translation pair \((x,y)\) in data stream \(D_{T}\), the element \(\mathcal{M}_{i,j}\) of word align matrix \(\mathcal{M}\) in \(i\)-th row and \(j\)-th column is defined as:

\[\mathcal{M}_{i,j}=\begin{cases}1,&\text{if word $w_{i}^{x}$ is synonym of $w_{j}^{y}$},\\ 0,&\text{otherwise},\end{cases} \tag{6}\]

where \(w_{i}^{x}\) is the \(i\)-th word of sentence \(x\) and \(w_{j}^{y}\) is the \(j\)-th word of sentence \(y\). With the word align matrix \(M\), the model is able to construct positive and negative samples in token-token contrastive learning.

Corresponding to the four types of multi-grained input data above, the contrastive loss functions can be denoted as \(\mathcal{L}_{ICC}\) (image-caption contrastive), \(\mathcal{L}_{RTC}\) (region-token contrastive), \(\mathcal{L}_{SLC}\) (sentence-level contrastive), and \(\mathcal{L}_{TLC}\) (token-level contrastive) respectively. Therefore, the overall objective function of multi-grained contrastive learning is:

\[\mathcal{L}_{MLC}=\lambda(\mathcal{L}_{ICC}+\mathcal{L}_{SLC})+(1-\lambda)( \mathcal{L}_{RTC}+\mathcal{L}_{TLC}), \tag{7}\]

where \(\lambda\) is a scale hyper-parameter to balance coarse-grained contrastive and fine-grained contrastive.

#### Masked language modeling

Masked language modeling (MLM) is a simple but effective self-learning paradigm that has been proven in multiple domains (Chen et al., 2018; Chen et al., 2018). As shown in Figure 2, we design three variants of MLM: Vision MLM (VMLM) on image-caption pairs in \(D_{I}\) and region-token pairs in \(D_{R}\), cross-lingual MLM (xMLM) on multilingual texts in \(D_{M}\), translation language modeling (TLM) on translation sentence pairs in \(D_{T}\).

In VMLM, the model predicts the masked tokens based on both the image and unmasked tokens from texts. During the prediction process, the model needs to understand the masked word associated features of the image, which helps to learn cross-modal interaction. xMLM predicts the masked tokens in the multilingual texts. Previous studies (Chen et al., 2018; Chen et al., 2018; Chen et al., 2018) have indicated that xMLM training helps encode different languages into the shared embedding space, improving the model's ability in multilingual modeling and cross-language transfer. In TLM, the model can attend to both the English sentence and its translation to predict a masked token, thereby encouraging the alignment of representations across different languages.

In all three tasks, we randomly mask out the input tokens with a probability of 15%. Mathematically, we denote a masked text as \(\hat{T}\), a masked parallel text pair as \((\hat{T}_{\hat{X}},\hat{T}_{\hat{Y}})\),and a masked image-text pair as \((\hat{T}_{\hat{I}},I)\). The complete MLM pre-training loss \(\mathcal{L}_{MLM}\) can be obtained by summing up the \(\mathcal{L}_{\mathcal{K}MLM}\), \(\mathcal{L}_{TLM}\), and \(\mathcal{L}_{VMLM}\) together:

\[\mathcal{L}_{MLM} =\mathbb{E}_{\hat{T}-D_{R}}[H(\mathbf{y}^{mlm}(\hat{T}_{\hat{X}},\hat{T }_{\hat{Y}}),\mathbf{p}^{mlm}(\hat{T}_{\hat{X}},\hat{T}_{\hat{Y}}))]\] \[+\mathbb{E}_{(\hat{T}_{\hat{I}},I)-D_{R}}[H(\mathbf{y}^{mlm}(\hat{T} _{\hat{I}},I),\mathbf{p}^{mlm}(\hat{T}_{\hat{I}},I))], \tag{8}\]

where \(\mathbf{p}^{mlm}\) is the predicted probability for a masked token, and \(\mathbf{y}^{mlm}\) is a one-hot vocabulary distribution where the ground-truth token has a probability of 1.

#### Vision-language matching

In the vision-language matching (VLM) task, the model predicts whether a pair of an image and a text sequence is matched. In practice, the image and its corresponding caption are regarded as positive pairs, whereas the remaining examples in the batch are considered as negatives. To predict the matching probability \(\mathbf{p}^{mlm}\), the model utilizes the output embedding of the [CLS] token from the fusion encoder as the joint representation of the image-text pair \((V,T)\). The VLM loss can be denoted as:

\[\mathcal{L}_{VLM}=\mathbb{E}_{(V,T)-D_{LR}}[H(\mathbf{y}^{mlm}(V,T),\mathbf{p}^{mlm}(V,T))], \tag{9}\]

where the ground-truth label is represented as a one-hot vector \(\mathbf{y}^{mlm}\).

Finally, the overall pre-training objective of M\({}^{2}\)-VLP is defined as:

\[\mathcal{L}=\mathcal{L}_{MLC}+\mathcal{L}_{MLM}+\mathcal{L}_{VLM} \tag{10}\]

## 4. Experiment

### Pre-training Datasets

In pre-training stage, we consider 21 languages, including English, to cover all target languages in downstream datasets. To construct \(D_{M}\), a subset of 400M multilingual sentences is sampled from the open-source dataset CC-100 (Wang et al., 2017), which is collected from the CommonCraw11 dump. As for \(D_{T}\), we use 20M English-centric parallel sentences from WikiMatrix (Wang et al., 2017). The specific language distribution in \(D_{M}\) and \(D_{T}\) is given in Appendix A.1. The data used to construct \(D_{I}\) and \(D_{R}\) is detailed in Table 1. From Conceptual Captions (Wang et al., 2017), MSCOCO (Wang et al., 2017), and Visual Genome (Ghosh et al., 2017), we use 3.9M image-caption pairs for \(D_{I}\), along with 6.1M region-token pairs for \(D_{R}\).

Footnote 1: [https://commoncraw1.org/](https://commoncraw1.org/)

### Pre-training Settings

For transformer modules, we adopt a base hidden size of 768 along with 12 heads for both self-attention and cross-attention. Following (Wang et al., 2017), we set \(N_{T}=N_{F}=6\) and \(N_{V}=12\). The vision encoder is initialized with (Wang et al., 2017). The text encoder is initialized with the first six layers of XLMR (Chen et al., 2018), while the fusion encoder is initialized with the last six layers. Due to the absence of cross-attention modules in the XLMR model, we initialize the cross-attention layers with parameters of self-attention.

We optimize the model using the Adam optimizer for 240K steps. Each training batch consists of 512 image-text pairs, 512 region-token pairs, 2048 translation pairs, and 2048 multilingual sentences. The learning rate is scheduled with a linear decay with 24K warmup steps, where the peak learning rate is set as \(1e-4\). The tokenizer of the XLMR is employed to tokenize the text. The maximum text sequence length of sentences in \(D_{M}\), \(D_{T}\), \(D_{I}\), and \(D_{R}\) are set to 64, 50, 35, and 35 respectively. The model is pre-trained at image resolution of \(256\times 256\) using \(16\times 16\) patch size. More details are given in Appendix A.2.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Dataset & Images & Captions & Objects & Regions \\ \hline MSCOCO & 0.12M & 0.55M & 0.45M & \(-\) \\ Conceptual Captions & 3.33M & 3.33M & \(-\) & \(-\) \\ Visual Genome & 0.10M & \(-\) & 2.0M & 3.7M \\ \hline \hline \end{tabular}
\end{table}
Table 1. Statistics of annotations in image-text datasets.

### Downstream Tasks

We conduct comprehensive experiments to evaluate the proposed model across two settings:

**Zero-shot cross-lingual transfer in vision-language tasks**: To assess the cross-lingual transferability of the proposed model in vision-language tasks, we evaluate M\({}^{2}\)-VLP on a recently released IGLUE (Chen et al., 2020) benchmark. For all tasks in this scenario, we follow a zero-shot cross-lingual transfer setting in which the model is trained only in English and evaluated directly in other languages. The five tasks of the IGLUE benchmark include:

* **XVNLI** Cross-lingual Visual Natural Language Inference (XVNLI) (Chen et al., 2020) requires the model to predict if a text-hypothesis 'entails', 'contradicts', or 'is 'neutral' to an image-premise. The dataset is collected by combining SNLI (Chen et al., 2020) with its multi-modal (Zhu et al., 2020) and cross-lingual (Chen et al., 2020) counterparts.
* **xGQA** The Cross-lingual Grounded Question Answering (xGQA) task (Zhu et al., 2020) is collected by manually translating the GQA validation set into 7 languages, while training data are sourced from the English training set of GQA. It requires a model to answer several types of structured questions about an image.
* **MaRVL** The Multicultural Reasoning over Vision and Language (MaRVL) dataset (Wang et al., 2019) requires the model to determine whether a textual description is true or false about a pair of images. The task involves comparing two visual representations and reasoning about the information in the textual description.
* **xFlickr&CO** The dataset (Chen et al., 2020) is collected by combining 1000 images from Flickr30K (Kumar et al., 2019) and MSCOCO (Wang et al., 2019) respectively. It is a retrieval task with two subtasks: image-to-text retrieval (TR) and text-to-image retrieval (IR). The image captions encompass a total of eight languages, including English.
* **WIT** Wikipedia-based Image Text dataset (WIT) (Kumar et al., 2019) is collected from Wikipedia in 108 languages. Similar to xFlickr&CO, the tasks consist of retrieving the correct image given a text (IR) and vice versa (TR). The English training set consists of 500K captions, and the evaluation set comprises 10 languages, each containing at least 500 image-text pairs.

**Multilingual fine-tuning in vision-language tasks**: To evaluate the multilingual ability of the proposed model, we conduct experiments on image-text retrieval tasks. We consider two settings, one is to train and test each language independently, and the other is to combine all languages into a unified training set and test on different languages. Following (Wang et al., 2019), We use multilingual extensions of MSCOCO (Wang et al., 2019) and Multi30K (Chen et al., 2020), both of which offer multilingual annotations for training.

* **Multi30K** This dataset extends Flickr30K (Kumar et al., 2019) from English to German, French and Czech. It consists of 31,783 images, each paired with five captions in English and German, and one caption in French and Czech. We follow the data split in (Wang et al., 2019).
* **MSCOCO** This dataset contains 123,287 images and provides five captions per image in English. STARI (Kumar et al., 2019) extends the original MSCOCO with 820K Japanese captions. Moreover, COCO-CN (Chen et al., 2020) extends MSCOCO with Chinese captions for nearly 20K images. For English and Japanese, we follow the data split in (Wang et al., 2019). As for Chinese, we use the original COCO-CN split.

In fine-tuning stage, we follow the task-specific hyper-parameters in the baseline model (Wang et al., 2019). We report accuracy for XVNLI, xGQA, and MaRVL. For retrieval tasks, we use mean Recall (mR) as our evaluation metric, which is an averaged score of R@1, R@5, and R@10 on image-to-text retrieval and text-to-image retrieval tasks.

### Baseline Models

We compare our model with recent competitive multilingual VLP models: mUNITER (Wang et al., 2019), xUNITER (Wang et al., 2019), UC\({}^{2}\)(Wang et al., 2019), M\({}^{2}\)(Wang et al., 2019), and WS-mVLP (Wang et al., 2019). Among the baselines, mUNITER and xUNITER, which are initialized from mBERT and XLM-R respectively, employ the UNITER architecture and are pre-trained with MLM on both cross-lingual texts and English image-text pairs. M\({}^{2}\)P implements explicit alignment by employing a code-switching strategy, replacing English words in image-text pairs with their respective synonyms in the target languages. UC\({}^{2}\) utilizes translation engines to translate existing English-only image-text pairs into other five

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c c c} \hline \hline  & \multicolumn{2}{c}{VNLI} & \multicolumn{2}{c}{VQA} & \multicolumn{2}{c}{Reasoning} & \multicolumn{4}{c}{Retrieval} \\ \cline{2-13}  & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} \\ \cline{2-13} Model & \multicolumn{2}{c}{XVNLI} & \multicolumn{2}{c}{xGQA} & \multicolumn{2}{c}{MaRVL} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} & \multicolumn{2}{c}{} \\ \cline{2-13}  & avg & en & avg & en & avg & en & avg & en & avg & en & avg & en & avg & en & avg & en \\ \hline mUNITER & 53.7 & 76.4 & 10.0 & 54.7 & 53.7 & 71.9 & 8.1 & 44.5 & 8.9 & 40.9 & 9.2 & 19.9 & 10.4 & 22.3 \\ xUNITER & 58.5 & 75.8 & 21.7 & 54.8 & 54.6 & 71.6 & 14.0 & 38.5 & 13.5 & 32.1 & 8.7 & 16.7 & 9.8 & 18.5 \\ UC\({}^{2}\) & 62.1 & 76.4 & 29.4 & 55.2 & 57.3 & 70.6 & 20.3 & 37.4 & 17.9 & 34.6 & 7.8 & 17.9 & 9.1 & 19.7 \\ M\({}^{2}\)P & 58.3 & 76.9 & 28.2 & 53.8 & 56.0 & 68.2 & 12.9 & 31.4 & 11.9 & 24.6 & 8.1 & 15.5 & 10.0 & 15.3 \\ WS-mVLP & 69.5 & 79.7 & 42.1 & 57.4 & 62.1 & 75.3 & 59.8 & 86.6 & 58.7 & **91.7** & 36.3 & 56.0 & 36.6 & 56.2 \\ Ours & **71.1** & **80.3** & **46.6** & **58.0** & **64.6** & **80.3** & **71.4** & **91.9** & **70.7** & 84.5 & **46.6** & **71.2** & **45.1** & **71.0** \\ \hline \hline \end{tabular}
\end{table}
Table 2. Results of zero-shot cross-lingual transfer on IGLUE benchmark. The en indicates the results on the English test sets, while avg represents the average results for other target languages. IR and TR refer to image retrieval and text retrieval, respectively.

languages and a visual-conditioned translation language modeling objective is introduced. WS-mVLP unifies cross-lingual and cross-modal pre-training within a weakly supervised framework, achieving remarkable results on the IGLUe benchmark. The experimental results for the baseline models are sourced from (Wang et al., 2019).

### Main Results

In order to comprehensively evaluate the multilingual performance of the model, we conduct experiments under the settings of zero-shot cross-lingual transfer and multilingual fine-tuning:

**Cross-lingual Transfer** As shown in Table 2, M\({}^{2}\)-VLP achieves superior zero-shot cross-lingual performance across various vision-language tasks. Specifically, compared to the best baseline WS-mVLP, our model achieves an average accuracy improvement of 6.9% on zero-shot cross-lingual vison-language understanding tasks including XVNLI, xGQA, and MaRVL. For retrieval tasks, M\({}^{2}\)-VLP outperforms all compared models by a substantial margin in cross-lingual settings. On the English test sets, our method exhibits the best performance on all tasks except for TR in xFlickr&CO. These results suggest that our multi-grained vision-language pre-training can learn better alignment between image and multilingual texts.

**Multilingual Fine-tune** As shown in Table 3, we conduct experiments on the multilingual extensions of Flickr30K and MSCOCO under three settings, which are zero-shot English-only fine-tuning, target language fine-tuning, and all-language fine-tuning. Compared to the baseline models, M\({}^{2}\)-VLP shows improvement across different languages. Under the setting of all-language fine-tuning, the improvement is not prominent. In contrast, our model exhibits more improvements under the cross-lingual transfer setting, possibly attributed to the additional fine-grained alignment between different languages.

### Ablation Study

We conduct ablation studies to investigate the effect of different components in the proposed model. We discuss the following three model variants:

* w/o _TLC_. This variant removes the token-level contrastive objective \(\mathcal{L}_{TLC}\), which means that the model aligns cross-lingual texts with only sentence-level contrastive.
* w/o _RTC_. In this setting, we remove region-token contrastive objective \(\mathcal{L}_{RTC}\), which means the model only performs coarse-grained cross-modal aligning with image-caption contrastive.
* w/o _CS_. This variant removes code-switching augmentation module (in section 3.1) for \(D_{I}\) and \(D_{R}\).

The results are shown in table 4. In general, all three modules have contributed to the model performance. Firstly, TLC showed significant improvement in xGQA and WIT tasks, indicating their sensitivity to cross-lingual fine-grained alignment. Secondly, RTC demonstrates more noticeable improvement on the XVNLI and xFlickr&CO datasets, indicating that token-region alignment may be more important for these two tasks. Surprisingly, the CS module has contributed the most to performance improvement. Previous studies have demonstrated that randomly replacing English words with their synonyms is a simple yet effective method that can promote word-level cross-lingual alignment. Compared to retrieval tasks, CS has shown greater improvement in XVNLI, xGQA, and MaRVL tasks, achieving an average improvement of 2.6%. These results demonstrate the importance of the proposed fine-grained alignment training for cross-lingual vision-language tasks.

### Visualization

#### Cross-attention Visualization

To obtain an intuitive comprehension of the proposed model, we use Grad-CAM (Wang et al., 2019), a commonly used "visual explanation" toolkit, to generate cross-attention location maps for the last layer of the fusion encoder. As shown in Figure 3, words from different languages that convey the same meaning can activate to corresponding regions in the image. It indicates that our model can effectively transfer the cross-modal alignment knowledge learned from English to other languages. More examples of cross-attention visualization are shown in Appendix B.2.

#### Cross-lingual Aligning

To explore whether our model tends to learn fine-grained cross-lingual alignment, we employ t-SNE (Maaten and Hinton, 2008) to visualize the distances between representations of translation word pairs. As Figure 4 shows, we sample 15 English-X translation word

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline \multirow{2}{*}{Model} & \multirow{2}{*}{XVNLI} & \multirow{2}{*}{xGQA} & \multirow{2}{*}{MaRVL} & \multicolumn{3}{c}{xFlickr\&CO} & \multicolumn{1}{c}{WIT} \\  & & & & IR & TR & IR & TR \\ \hline Ours & **71.1** & **46.6** & **64.6** & **71.4** & **70.7** & **46.6** & **45.1** \\ \hline \(w/o\) _TLC_ & 70.7 & 45.8 & 64.4 & 71.3 & 69.5 & 45.4 & 43.3 \\ \(w/o\) _RTC_ & 70.1 & 46.4 & 64.1 & 70.3 & 68.1 & 45.6 & 43.7 \\ \(w/o\) _CS_ & _69.7_ & 42.0 & 62.8 & 69.5 & 69.1 & 46.2 & 43.7 \\ \hline \hline \end{tabular}
\end{table}
Table 4. Results of ablation studies. TLC is the abbreviation for token-level contrastive, RTC for region-token contrastive, and CS for code-switching.

\begin{table}
\begin{tabular}{l c c c c c c c} \hline \hline \multirow{2}{*}{Model} & \multicolumn{6}{c}{Flickr30K} & \multicolumn{3}{c}{MSCOCO} \\ \cline{2-8}  & en & de & fr & cs & en & zh & ja \\ \hline \multicolumn{8}{c}{English only Finetune} \\ \hline UC2\({}^{2}\) & 87.2 & 74.9 & 74.0 & 67.9 & 88.1 & 82.0 & 71.7 \\ M\({}^{3}\)P & 87.4 & 58.5 & 46.0 & 36.8 & 88.6 & 53.8 & 56.0 \\ WS-mVLP & 94.9 & 84.4 & 86.1 & 77.2 & 89.6 & 83.3 & 73.1 \\ Ours & **95.3** & **88.6** & **90.3** & **85.9** & **90.7** & **88.9** & **83.3** \\ \hline \multicolumn{8}{c}{Single Language Finetune} \\ \hline UC2\({}^{2}\) & 87.2 & 83.8 & 77.6 & 74.2 & 88.1 & 84.9 & 87.3 \\ M\({}^{3}\)P & 87.4 & 82.1 & 67.3 & 65.0 & 88.6 & 75.8 & 80.1 \\ WS-mVLP & 94.9 & 92.5 & 92.4 & 91.0 & 89.6 & **92.5** & 90.4 \\ Ours & **95.3** & **93.2** & **92.6** & **91.6** & **90.7** & 91.9 & **91.2** \\ \hline \multicolumn{8}{c}{All-Language Finetune} \\ \hline UC2\({}^{2}\) & 88.2 & 84.5 & 83.9 & 81.2 & 88.1 & 89.8 & 87.5 \\ M\({}^{3}\)P & 87.7 & 82.7 & 73.9 & 72.2 & 88.7 & 86.2 & 87.9 \\ WS-mVLP & **95.3** & 93.6 & **93.8** & 92.4 & 90.4 & 92.6 & 90.0 \\ Ours & 95.2 & **94.0** & **93.8** & **92.8** & **90.8** & **92.7** & **91.6** \\ \hline \hline \end{tabular}
\end{table}
Table 3. Results of multilingual image-text retrieval on Flickr30K and MSCOCO.

pairs (excluding stop words) from \(D_{l}\) for four low-resource target languages. The hidden states of the last transformer layer in the fusion encoder serve as the word representations. Figure 4(a) shows the t-SNE visualization of our model without fine-grained aligning (without \(\mathcal{L}_{RTC}\) and \(\mathcal{L}_{TLC}\))). We observe that only a few word pairs are properly aligned and words from the same language are more likely to gather in the hidden space. As for fully trained M\({}^{2}\)VLP in Figure 4(b), even in the case of low-resource languages, the results show that they achieve significant token-level semantic alignment with English, which leads to better performance in cross-lingual transfer. The cross-lingual aligning visualization of other languages is presented in Appendix B.3.

## 5. Conclusion

In this paper, we propose M\({}^{2}\)-VLP to perform multi-grained multilingual vision-language pre-training. Our approach focuses on enhancing fine-grained alignment in two dimensions: cross-lingual and cross-model. Specifically, we introduce two novel training objectives, RTC and TLC, to enhance semantic alignment between image regions and corresponding phrases, as well as words in different languages. Then we perform multi-grained multilingual vision-language pre-training by unifying training paradigms from different granular levels. Experimental results demonstrate that M\({}^{2}\)-VLP outperforms previous state-of-the-art models on various cross-lingual vision-language benchmarks.

Figure 4. The tSNE visualization of the word representations. Each word pair is connected by a grey dotted line.

Figure 3. Grad-CAM visualization of the cross-attention between regions and corresponding words across different languages.

M\({}^{3}\)-M-P. Enhancing Multilingual Vision-Language Pre-Training via Multi-Grained Alignment

## References

* (1)
* Agic and Schliuter (2018) Zeilhao Agic and Natalie Schliuter. 2018. Baselines and Test Data for Cross-Lingual Inference in LREC.
* Ai and Yang (2023) Xi Ai and Bin Fang. 2023. Multilingual Pre-training with Self-supervision from Global Co-occurrence Information. In _ACL Findings_. 7526-7543.
* Alayrac et al. (2022) Jana-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antonio Michel, Iain Barr, Yana Hasson, Karol Lene, Arthur Mensch, Katherine Millican, et al. 2022. Flamingo: a Visual language model for few-shot learning. _NeurIPS_ (2022), 2376-2376.
* Bowman et al. (2015) Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In _EMNLP_. 632-642.
* Bugliarello et al. (2020) Emmanuel Bugliarello, Fangyu Lua Jons Pfeifer, Siva Reddy, Desmond Elliott, Elkornda Maria Ponti, and Ivan Vulic. 2020. ZOLUE: A benchmark for Transfer Learning across Molecules, Tasks, and Languages. In _ICLR_. 2370-2392.
* Carlsson et al. (2022) Fredrik Carlsson, Philipp Eisen, Rachidanti, and Magnus Sahlgren. 2022. Cross-lingual Multilingual CLIP. In _IREC_. 6488-6458.
* Chen et al. (2023) Feelong Chen, Duchene Zhang, Mingzhan Han, Xinyi Chen, Jing Shi, Shuang Xu, and Bo Xu. 2023. VLP: A Survey on Vision-language Pre-training. _Int. J. Autom. Comput._ (2023), 38-36.
* Choi et al. (2020) Zewen Choi, Li Dong, Furu Wei, Nanya, Sakhaim Singhal, Verhaul Wang, Xia Song, Kazun Tang, Meq, Hyean Huang, and Ming Zhou. 2020. InfoM: An Information-Theoretic Framework for Cross-Lingual Language Model Pre-Training. In _NAACL_. 3576-3588.
* Cheonni and Shutova (2020) Rochelle Cheonni and Ekaterina Shutova. 2020. What does it mean to be language-agnostic? probing multilingual sentence encoders for typological properties. _arXiv:2008.12820_ (2020).
* Conneau et al. (2020) Aleisa Conneau, Karilay Khandour, Namon Goyal, Vishav Chaudhary, Guillaume Wherack, Francisco Guzman, Eduard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised Cross-lingual Representation Learning at Scale. In _ACL_. 8440-851.
* Conneau and Lample (2019) Alexis Conneau and Guillaume Lample. 2019. Cross-lingual Language Model Pretraining. In _NeurIPS_. 7057-7067.
* Choluk et al. (2020) Eizih D Choluk, Barret Zopf, Jonathon Shlens, and Quoc V Le. 2020. Raduabsorbent: Practical automated data augmentation with a reduced search space. In _CVPR_. 702-703.
* Devlin et al. (2019) Jacob Devlin, MingWei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In _NAACL_. 4117-4166.
* Dyer et al. (2013) Chris Dyer, Victor Chahmens, and Noah A Smith. 2013. A simple, fast, and effective reparameterization of B1M model. _22_. In _NAACL_. 674-648.
* Elliott et al. (2016) Desmond Elliott, Sidiela Frank, Kail Sima, and Lucia Specia. 2016. Multi30K: Multilingual English-German Image Description. In _ACL_. 70-74.
* Guo et al. (2021) Ping Guo, Xiangwei Wang, Wei Yu, Hao Wangong Yang, Jingheng Li, Fei Huang, and Jun Xie. 2021. EMMA-A: An EM-like Multilingual Pre-training Algorithm for Cross-lingual Representation Learning. In _NeurIPS_.
* Huang et al. (2022) Lianthe Huang, Shuming Ma, Dongdong Zhang, Furu Wei, and Houfeng Wang. 2022. Zero-shot Cross-lingual Transfer of Promo-based Tuning with a Unified Multilingual Prompt. In _EMNLP_. 1148-1157.
* Jain et al. (2021) Aasih Jain, Mandy Guo, Krishna Srinivasan, Ting Chen, Sacha Kudugunta, Chao Jia, Tintel King, and Jason Shalidho. 2021. Muar: multimodal, multitask retrieval across languages. _arXiv:21010.01825_ (2021).
* Ji et al. (2021) Yatzi Ji, Hongcheng Tu, Jing Jiang, Weiqi Kong, Chengfei Cai, Wenbue Zhao, Hongqi Wang, Yuqi Yang, and Wei Lua. 2021. Seging What You Miss: Vision-Language Pre-training with Semantic Completion Learning. In _CVPR_. 6789-6798.
* Jin et al. (2021) Chao Ji, Tintel King, Yeia Xia, Yitang Chen, Zarana Paekh, Hieu Wang, Quoc V. L., Yankun Sunqi, and L. Tomi Derugu. 2021. Scaling Up Visual and Vision Language Representation Learning With Noisy Text Supervision. In _ICML_. 4904-4916.
* Kaputhy and Fei-Feileci (2017) Andrej Karpathy and Li Fei-Feileci. 2017. Deep Visual-Semantic Alignments for Generating Image Descriptions. _IEEE Trans. Pattern Anal. Mach. Intell._ (2017), 664-676.
* Krishna et al. (2017) Ranjay Krishna, Kue Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Idija Li, David A. Shanna, Michael S. Bernstein, and Li Fei-Fei. 2017. Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations. _Int. J. Comput. Vis._ (2017), 32-37.
* Lauchner et al. (2020) Anne Lauchner, Vink Ravindhar, Ivan Vulic, and Goren Glavas. 2020. From Zero to Hero: On the Limitations of Zero-Shot Language Transfer with Multilingual Transformers. In _EMNLP_. 4483-4499.
* Lee et al. (2018) Kuang-Hieu Lee, Xi Chen, Gang Hua, Houdong Hu, and Xiaodong He. 2018. Stacked cross attention for image-text matching. In _ECCV_. 201-216.
* Dongxu et al. (2021) Junnan L. Dongxu, Liibio Savarese, and Steven Hoi. 2021. Skip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In _ICML_. 19730-19742.
* Li et al. (2021) Junnan Li, Ramprasath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong fori. 2021. Align before fine: Vision and language representation learning with momentum distillation. _NeurIPS_ (2021), 9044-9075.
* Li et al. (2019) Linnan Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2019. VisualBERT: A simple and performant baseline for vision and language. _arXiv:1908.05527_ (2019).
* Li et al. (2019) Xiong Li, Chaoqi Xu, Xiaoxan Wang, Weiyu Lan, Zhengxiong Jia, Gang Yang, and Jeping Xu. 2019. COCO: COM for Cross-Lingual Image Tagging, Captioning, and Retrieval. _IEEE Trans. Multim._ (2019), 2347-2360.
* Li et al. (2020) Xiujun Li, Yin, Chunyuan Li, Pengzhou Zhang, Xiaodong Hu, Lei Zhang, Lijun Wang, Houdong Hu, Li Dong, Furu Wei, Yipin Choi, and Jianfeng Gao. 2020. Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks. In _ECCV_. 112-137.
* Li et al. (2019) Zejun Li, Zhhao Fan, Jingjing Chen, Qi Zhang, Xuanqing Huang, and Zhongyu Wei. 2020. Unifying Cross-Lingual and Cross-Modal Modeling Towards Weakly Supervised Multilingual Vision-Language Pre-training. In _ACL_. 5939-5958.
* Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Hort Dufle, and C. Lawrence Zitnick. 2014. Microsoft COCO: Common Objects in Context. In _ECCV_. 704-725.
* Liu et al. (2021) Fangyu Liu, Emmanuel Rehgile, Eduarda Martin Puri, Siva Reddy, Nigel Collier, and Desmond Elliott. 2021. Visually Grounded Resources across Languages and Cultures. In _EMNLP_. 10467-10485.
* Liu et al. (2020) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2020. Visual instruction tuning. _NeurIPS_ 7058 (2020).
* Liu et al. (2021) Maheeng Liu, Haoyang Huang, Lin Su, Edward Cui, Taroon Bharti, Lipian Wang, Dongdong Zhang, and Nun Sun. 2021. M3p: Learning Universal Representations via Multitask Multilingual Multimodal Pre-Training. In _CVPR_. 3977-3986.
* Pfeiffer et al. (2020) Jonas Pfeiffer, Gregor Gele, Abhiaros Kamath, Jan-Martin G. Seita, Stefan Roth, Ivan Vulic, and Iryna Sutveyn. 2020. AGOX-Lingual Visual Question Answering. In _ACL_. Findings, 2497-2511.
* Radford et al. (2007) Alec Radford, Jong Wei, Chris Linkey, Aditya Ramesh, Gabriel Cohn, Sandhini Agarwal, Grishah Sastry, Amanda Askul, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2007. Learning Transferable Visual Models from Natural Language Supervision. In _ICML_. 8748-8763.
* Schwenk et al. (2021) Holger Schwenk, Vishav Chaudhary, Shuso Sun, Hongyi Gong, and Francesco Guzzoni. 2021. Wikidata: Mining 30st Parallel sentences in 160 Language Pairs from Wikipedia. In _EACL_. 1351-1361.
* Sekarni et al. (2020) Ramprasath, Bekarnios Vedaldi, Alshok Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2020. Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization. _Int. J. Comput. Vis._ (2020), 336-359.
* Sharma et al. (2018) Piyush Sharma, Nam Ding, Sebastian Goodman, and Raulu Soricut. 2018. Conceptual Dial Captions: A Cleaned Hypermed. Image All-text Dataset For Automatic Image Captioning. In _ACL_. 2556-2565.
* Srivastava et al. (2021) Krishna Srivastava, Karathik Raman, Jiacen Chen, Michael Bendersky, and Marc Najork. 2021. WHT: Wi-Viologi-scale Image Text Dataset for Multimodal Multi-lingual Machine Learning. In _SIGIR_. 4263-4249.
* Su et al. (2020) Wei, Suo, Zhilin Tang, Yue Cao, Bin Li, Lervei Lu, Furu Wei, and Jifeng Dai. 2020. VL-BERT: Pre-training of Generic Visual-Linguistic Representations. In _ICLR_.
* Sun et al. (2020) Guodong Sun, Yue Bai, Xueying Yang, Yi Fang, Yun Fu, and Zhiqiang Luo. 2020. Aligning Out-Distribution We Images and Graph Semantics via Evidential Learning. In _WWW_. 2271-2281.
* Van der Maaten and Hinton (2008) Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE. _Journal of machine learning research_ 9, 11 (2008).
* Wang et al. (2023) Wenhui Wang, Hangbo Bai, Li Dong, Jolan Brock, Zhiliang Peng, Guang Liu, Kai Aggarwal, Ouvats Khan Mohammed, Sakhaim Singhal, Subhojiri Sont, et al. 2023. Imags: a foreign language: Bet training for vision and vision-language tasks. In _CVPR_. 1975-1986.
* Wei et al. (2024) Wei Wei, Jishin Tang, Lianghao Xia, Yangjin Jiang, and Chao Huang. 2024. PromptMM: Multi-Modal Knowledge Distillation for Recommendation with Prompt-Tuning. In _WWW_. 3217-3228.
* Wei et al. (2021) Xiangpeng Wei, Rongjeng Wang, Yue Fu, Hixi Lieng, Hao Fu, and Weihua Luo. 2021. On Learning Universal Representations Across Language. In _ICLR_.
* Wemack et al. (2020) Guillaume Wemack, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francois Gurzani, Arnaud Joulin, and Edward Grave. 2020. GCNC: Meta-Training. Highly Quadrual Datasets from Web Crawl Data. In _LREC_. 4003-4012.
* Yoshikawa et al. (2017) Yugo Yoshikawa, Yurita Shigeto, and Akikane Takeshi. 2017. STATH Captions: Constructing a Large-Scale Japanese Language Caption Dataset. In _ACL_. 417-421.
* Young et al. (2014) Peter Young, Alice Lai, Michal Hochsch, and Julia Hochschreiter. 2014. From image descriptions to visual denotations. New scholarly metrics for semantic inference 

[MISSING_PAGE_FAIL:10]

[MISSING_PAGE_FAIL:11]

[MISSING_PAGE_FAIL:12]