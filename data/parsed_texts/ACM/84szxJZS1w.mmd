# Graph Anomaly Detection with Bi-level Optimization

Anonymous Author(s)

###### Abstract.

Graph anomaly detection (GAD) has various applications in finance, healthcare, and security. Graph Neural Networks (GNNs) are now the primary method for GAD, treating it as a task of semi-supervised node classification (normal vs. anomalous). However, most traditional GNNs aggregate and average embeddings from all neighbors, without considering their labels, which can hinder detecting actual anomalies. To address this issue, previous methods try to selectively aggregate neighbors. However, the same selection strategy is applied regardless of normal and anomalous classes, which does not fully solve this issue. This study discovers that nodes with different classes yet similar neighbor label distributions (NLD) tend to have opposing loss curves, which we term it as "loss rivalry". By introducing Contextual Stochastic Block (CSBM) and defining _NLD distance_, we explain this phenomenon theoretically and propose a **Bi**-level **o**ptimization **G**raph **N**eural **N**etwork (BioGNN), based on these observations. In a nutshell, the lower level of BioGNN segregates nodes based on their classes and NLD, while the upper level trains the anomaly detector using separation outcomes. Our experiments demonstrate that BioGNN outperforms state-of-the-art methods and effectively mitigates "loss rivalry". Codes are available at [https://anonymous.4open.science/r/BioGNN-12B4](https://anonymous.4open.science/r/BioGNN-12B4).

Graph Neural Networks, Anomaly Detection, Bi-level Optimization +
Footnote †: copyright: none

+
Footnote †: copyright: none

+
Footnote †: copyright: none

+
Footnote †: copyright: none

+
Footnote †: copyright: none

+
Footnote †: copyright: none

+
Footnote †: copyright: none

+
Footnote †: copyright: none

+
Footnote †: copyright: none

## 1. Introduction

Graph anomaly detection (GAD) is a learning-to-detect task. The objective is to differentiate anomalies from normal ones, assuming that the anomalies are generated from a distinct distribution that diverges from the normal nodes (Song et al., 2018). As demonstrated by (Song et al., 2018), GAD has various real-world applications including detecting spam reviews in user-rating-product graphs (Song et al., 2018), finding misinformation and fake news in social networks (Song et al., 2018), and identifying fraud in financial transaction graphs (Song et al., 2018; Wang et al., 2018).

A primary method is to consider GAD as a _semi-supervised node classification_ problem, where the edges play a crucial role. By examining the edges, we can divide an ego node's neighbors into two groups: (1) homophilous neighbors that have the same labels as the ego node, and (2) heterophilous neighbors whose labels are different from the ego node's label. For instance, in the case of an anomaly ego node, its interactions with anomaly neighbors display homophily, while its anomaly-normal edges demonstrate heterophily. Both homophily and heterophily are prevalent in nature. In transaction networks, fraudsters have heterophilous connections with their customers, while their connections with accomplices are homophilous.

From the standpoint of neighbor relationships, we can briefly describe the primary graph neural networks (GNNs)-based GAD solutions and their limitations as follows:

* Early studies (Song et al., 2018; Wang et al., 2018) aggregate over all neighbors without considering the impact of homophily and heterophily. That is, the representation of each node blindly aggregate the information from all neighbors, without discriminating the neighbor relationships. However, this approach can be disadvantageous to GAD as anomalies are more likely to be hidden among a large number of normal neighbors. Blindly aggregating information can dilute the suspiciousness of anomalies with normal signals, making them less discernible (Song et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018).
* To address the above-mentioned problem, recent studies (Bogu et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018) draw inspiration from graph signal processing (GSP). They suggest that a low-pass filter may not be optimal for all graphs. Instead, they manipulate eigenvalues of the normalized graph Laplacian to amplify some frequency information and weaken others. However, these studies optimize node representations as a whole, without addressing differences in their distribution regarding neighbor labels. For instance, as shown in Figure 1, a normal node shares the same neighbors as an anomaly. Our analysis in SS3.2 reveals that nodes of different classes with the same neighbors retain rather different frequency components. While emphasizing a single frequency band can improve learning for some nodes, it can hinder the learning of others.

Thus, it is crucial to understand the impact of neighbor label distribution (NLD) on detector behavior. We introduce and reveal the phenomenon of "loss rivalry". Surprisingly, we observe opposite loss curves for anomalies and normal nodes holding similar NLDs.

Figure 1. The ego normal node and anomaly (marked in red circle) have comparable neighborhood label distributions (NLD). The probability of neighbor labels being 0 or 1 is denoted by \(p_{c}\) and \(q_{c}\), where the subscript represents the class label. For instance, \(p_{1}\) denotes the probability of a normal neighbor for anomalies.

These are separately highlighted around the maxima and minima of the curves in Figure 2. Our analysis emphasizes the importance of using distinct aggregation mechanisms for nodes with different classes but similar NLD.

Based on this finding, we propose a bi-level optimization model in SS4, named BioGNN. Specifically, it consists of two key components. The first component is a mask generator that separates nodes into mutually exclusive sets based on their classes and NLD. The second component contains two well-designed GNN encoders that adopt different mechanisms to learn node representations separately. In SS3.1, we define the NLD distance based on the Contextual Stochastic Block Model (CSBM) and verify its direct proportion to representation expressiveness. Due to the proved superiority of adaptive filters in heterophilic graphs(Chen et al., 2017; Wang et al., 2018; Wang et al., 2018), we approach the problem in the spectral domain. Specifically, we first explain the feasibility of acquiring NLD given the ego graph of a node in the spectral domain in SS3.2. Then, we distill the NLD of nodes from filter performance through the bi-level optimization process, as spectral filter performance depends on the concentration of spectral label distribution (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018). In a nutshell, BioGNN distinguishes nodes with similar NLD but likely belong to different classes and feeds them into separate filters to prevent "loss rivalry". Our code is available at [https://anonymous.4open.science/r/BioGNN-12B4](https://anonymous.4open.science/r/BioGNN-12B4).

**Our contributions.** (1) We reveal the "loss rivalry" phenomenon, where nodes belonging to different classes but with similar NLD tend to have opposite loss curves, which can negatively impact model convergence. (2) We provide theoretical explanations regarding the importance of NLD and the benefits of using polynomial-based spectral filtering methods to capture the NLD of nodes. (3) We propose a novel bi-level optimization framework to address the problem, and the effectiveness of the proposed method is verified through experiments.

## 2. Preliminaries and Notations

In GAD, anomalous and normal nodes can be modeled as an attributed graph \(\mathcal{G}=(\mathcal{V},\mathcal{E},\mathbf{X})\), where \(\mathcal{V}\) represents the set of anomalous and normal nodes, \(\mathcal{E}\) denotes edges, and \(\mathbf{X}\) is the attribute matrix. The objective of GAD is to identify anomalous nodes by learning from the attributes and structure of the graph. In SS3.1, we will discuss the impact of NLD on GAD and demonstrate the superiority of spectral filtering in addressing this issue. Therefore, we introduce basic knowledge of graph spectral filtering in this section.

**Graph-based Anomaly Detection.** A primary approach for GAD is to frame it as a semi-supervised node classification task (Wang et al., 2018). The goal is to train a predictive GNN model \(g\) that achieves minimal error in approaching the ground truth \(\mathbf{Y}_{test}\) for unobserved nodes \(\mathcal{V}_{test}\) given observed nodes \(\mathcal{V}_{train}\), where \(\mathcal{V}_{train}\cup\mathcal{V}_{test}=\mathcal{V}\) and \(\mathcal{V}_{train}\cap\mathcal{V}_{test}=\emptyset\):

\[g(\mathcal{G},\mathbf{Y}_{train})\rightarrow\hat{\mathbf{Y}}_{test}. \tag{1}\]

Note that GAD is an imbalanced classification problem, which often results in similar NLD for normal nodes and anomalies: anomalies in the graph are rare, hence both normal nodes and anomalies are surrounded by numerous normal nodes.

**Graph Spectral Filtering.** Let \(\mathbf{A}\) be the adjacency matrix, and \(\mathcal{L}\) be the graph Laplacian, which can be expressed as \(\mathbf{D}-\mathbf{A}\) or as \(\mathbf{I}-\mathbf{D}^{-1/2}\mathbf{A}\mathbf{D}^{-1/2}\) (symmetric normalized), where \(\mathbf{I}\) is the identity matrix, and \(\mathbf{D}\) is the diagonal degree matrix. \(\mathcal{L}\) is positive semi-definite and symmetric, so it has an eigen-decomposition \(\mathcal{L}=\mathbf{U}\mathbf{A}\mathbf{U}^{T}\), where \(\mathbf{A}=\{\lambda_{1},\cdots,\lambda_{N}\}\) are eigenvalues, and \(\mathbf{U}=\{\mathbf{u}_{1},\cdots,\mathbf{u}_{N}\}\) are corresponding unit eigenvectors (Wang et al., 2018). Assuming \(\mathbf{X}=[\mathbf{x}_{1},\cdots,\mathbf{x}_{N}]\) is a graph signal, we call the spectrum \(\mathbf{U}^{T}\mathbf{X}\) the graph Fourier transform of the signal \(\mathbf{X}\)(Wang et al., 2018; Wang et al., 2018). In graph signal processing (GSP), the frequency is associated with \(\mathbf{A}\). Therefore, the goal of spectral methods is to identify a response function \(g(\cdot)\) on \(\mathbf{\Lambda}\) to learn the graph representation \(\mathbf{Z}\)(Wang et al., 2018):

\[\mathbf{Z}=g(\mathcal{L})\mathbf{X}=\mathbf{U}[g(\mathbf{\Lambda})\odot( \mathbf{U}^{T}\mathbf{X})]=\mathbf{U}g(\mathbf{\Lambda})\mathbf{U}^{T}\mathbf{X}. \tag{2}\]

## 3. Theoretical Analysis

In this section, we introduce the Contextual Stochastic Block Model (CSBM), a widely used model for describing node feature formation. Based on CSBM, we define the NLD distance and verify its direct proportion to representation expressiveness. Furthermore, because adaptive filters have been shown to perform better in heterophilic graphs, we explore the feasibility of expressing NLD in the spectral domain to facilitate further study in later sections.

Figure 2. Illustration of the ‘loss rivalry’ phenomenon in YelpChi and Amazon Datasets with BWGNN (Wang et al., 2018). From the same-color circles around the maxima and minima, we observe that the two loss curves in the same dataset are opposite along the epochs. The curves are plotted

### Impact of NLD on Node classification

GNNs are widely used to learn node representations in networks, as they can capture graph topological and structural information effectively. However, GNNs distinguish nodes by averaging the node features of their neighborhood (Shi et al., 2018). Therefore, it is intuitive that the neighbor label distribution has a significant impact on GNN performance. To analyze NLD from a graph generation perspective, we introduce the Contextual Stochastic Block Model (CSBM) (Shi et al., 2018). CSBM is a random graph generative model commonly used to measure the expressiveness of GNNs (Shi et al., 2018).

**CSBM**. The Contextual Stochastic Block Model (CSBM) makes the following assumptions for an attributed graph \(\mathbf{\mathcal{G}}\): (1) For a central node \(u\) with label \(c\in\{0,1\}\), the labels of its neighbors are independently sampled from a fixed distribution \(\mathcal{D}_{c}\sim\mathit{Bern}(p_{c})\). \(p_{c}\) denotes the sampling probability of class \(c\), and the sampling process continues until the number of neighbors matches the degree of node \(u\). In this work, we refer to the distribution \(\mathcal{D}_{c}\) as the **neighborhood label distribution (NLD)**. (2) Anomalies and normal nodes have distinct node feature distributions, namely \(\mathcal{F}_{c}\).

For simplicity, we define the NLD distance as follows:

**Definition 3.1** (Neighborhood Label Distribution Distance): Given a graph \(\mathbf{\mathcal{G}}\) with label vector \(\mathbf{y}\), the neighborhood label distribution distance between nodes \(u\) and \(v\) is:

\[d(u,v)=dis(\mathcal{D}_{u_{c}}(u),\mathcal{D}_{\mathcal{D}_{c}}(v)), \tag{3}\]

where \(dis(\cdot,\cdot)\) measures the difference between distribution vectors, such as cosine distance or Euclidean distance; \(u_{c}\) and \(v_{c}\) denote the class of nodes \(u\) and \(v\), respectively.

In this work, we focus on the binary GAD classification problem, hence \(\mathcal{D}_{c}=\{\mathcal{D}_{0}=[p_{0},q_{0}],\mathcal{D}_{1}=[p_{1},q_{1}]\}\), where the symbol definitions are shown in Table 1. Furthermore, following previous works (Gulraj et al., 2017; Chen et al., 2018; Li et al., 2018), we suppose that \(\mathcal{F}_{c}\) are two Gaussian distributions of \(n\) variables, \(i.e.,x_{0}\sim N_{n}(\mu_{0},\alpha^{2}),x_{1}\sim N_{n}(\mu_{1},\alpha^{2})\). This problem setting leads us to the following proposition, which indicates the expressive power of GNNs.

**Proposition 3.1**: Given a graph \(\mathbf{\mathcal{G}}=(\mathcal{V},\mathbf{\mathcal{E}},\{\mathcal{F}_{c}\},\{\mathcal{ D}_{c}\})\), the distance between the means of the class-wise hidden representations is proportional to their NLD distance.

**Remark.** The detailed proof can be found in Appendix A.1. This proposition shows that the expressive power of the representation depends on the neighborhood label distribution. Specifically, for nodes \(u\) and \(v\) in different classes, a vanilla 2-layer GCN has the following distance between their hidden representations:

\[||\mu_{u}-\mu_{v}||_{2}=\frac{[d(u,v)]^{2}}{2}\cdot||\mu_{1}-\mu_{0}||_{2}, \tag{4}\]

where \(\mu_{u}\) and \(\mu_{0}\) are the mean values of the learned representations of nodes \(u\) and \(v\). Similarly, for spectral methods, whose general polynomial approximation form can be written as \(\sum_{k}\alpha_{k}\tilde{L}^{k}\mathbf{X}\)(Shi et al., 2018), we can achieve a much larger NLD distance with a second-order polynomial:

\[||\mu_{u}-\mu_{v}||_{2}=[1+\frac{[d(u,v)]}{\sqrt{2}}+\frac{[d(u,v)]^{2}}{2}] \cdot||\mu_{1}-\mu_{0}||_{2}. \tag{5}\]

The larger the distance \(||\mu_{u}-\mu_{v}||_{2}\), the more expressive the representation and the better capability of the downstream linear detector. From (4) and (5), we observe two things: (1) the minimum value of \(||\mu_{u}-\mu_{v}||_{2}\) is achieved when \(d(u,v)\approx 0\); (2) using second-order polynomial graph filtering can improve the ability to distinguish between nodes, especially when the NLD of nodes from different classes are similar. This finding aligns with previous research (Shi et al., 2018; Li et al., 2018) in this area.

### NLD in the Spectral Domain

The NLD of anomalous and normal nodes in four benchmark datasets is statistically reported in Table 2. We observe that the NLD for nodes from different classes are similar, especially in YelpChi and Amazon datasets. Our analysis justifies the need to filter out anomalies sharing similar neighborhood labels with normal nodes, so that the distribution of the remaining anomalies can be distinguished from that of normal nodes. Proposition 3.1 suggests that spectral methods are more effective. Therefore, we aim to address the problem in the spectral domain. To begin with, we express NLD in the spectral domain by bridging the gap between it and frequency. Specifically, we fragment a graph into a set of ego-graphs (Shi et al., 2018) and define the spectral label distribution as follows:

**Definition 3.2**: (Spectral Label Energy Distribution): Given an ego node \(u\) and its one-hop neighbor set \(\mathcal{N}_{u}\) with size \(N\), the spectral label energy distribution at \(\lambda_{k}\) is:

\[f_{k}(\mathbf{y},\mathcal{L})=\alpha_{k}^{2}/\sum_{n=1}^{N}\alpha_{k}^{2}, \tag{6}\]

where \(f\) is a probability distribution with \(\sum_{k=1}^{N}f_{k}=1\), \(\mathcal{L}\) is the Laplacian matrix of the ego-graph, and (\(x\)) denotes the ego-graph spectrum of the one-hot label vector \(\mathbf{y}\). Since \(\alpha_{k}=\mathbf{u}_{k}^{T}\mathbf{y}\), \(f_{k}(\mathbf{y},\mathcal{L})\) measures the weight of \(\mathbf{u}_{k}\) in y, a larger \(f_{k}\) indicates that the spectral label distribution concentrates more on \(\lambda_{k}\). With Definition 3.2, we now show the relationship between \(f(\mathbf{y},\mathcal{L})\) and NLD.

**Proposition 3.2**: For a binary classification problem, the expectation of the spectral label energy distribution \(\mathbb{E}[f(\mathbf{y},\mathcal{L})]\) is positively associated with the NLD of the node. Specifically:

\[\mathbb{E}[f(\mathbf{y},\mathcal{L})]=\begin{cases}\frac{|\mathbf{ \mathcal{E}}|\cdot(1-p_{0})}{N}&y=0,\\ \frac{|\mathbf{\mathcal{E}}|\cdot p_{1}}{N}&y=1.\end{cases} \tag{7}\]

**Remark.** The detailed proof can be found in Appendix A.2. Proposition 3.2 indicates that capturing the difference in spectral label distribution is equivalent to measuring the similarity between NLDs. Furthermore, the proposition elucidates that different nodes with

\begin{table}
\begin{tabular}{c|c} \hline \hline Symbol & Definition (Probability of) \\ \hline \(p_{1}\) & normal neighbor for anomalies \\ \hline \(q_{1}\) & anomaly neighbor for anomalies \\ \hline \(p_{0}\) & normal neighbor for normal nodes \\ \hline \(q_{0}\) & anomaly neighbor for normal nodes \\ \hline \hline \end{tabular}
\end{table}
Table 1. NLD Symbol Definition.

similar NLD retain rather different frequency components. Based on this finding, separating nodes whose spectral label distributions are different could bring two benefits: (1) separate nodes in the same class but have different NLDs; (2) separate nodes in different classes but have similar NLDs. Both of these benefits alleviate the "loss rivalry" phenomenon and help with the convergence of GNNs.

### Validation on Real-World Graphs

To verify the correctness of our theoretical findings, we report the F1-macro and AUC performance of some general methods (triangle marker) (Zhou et al., 2017; Wang et al., 2018; Wang et al., 2018) and some polynomial spectral methods (Zhou et al., 2017; Wang et al., 2018; Wang et al., 2018) (star marker) in Figure 3. We make two observations: (1) As shown in Table 2, the NLD distance between the two classes is 0.0762 and 0.6462 for YelpChi and T-Finance, respectively. From Figure 3, we observe that most methods achieve better results on T-Finance than on YelpChi, demonstrating the importance of NLD. Moreover, the performance gap between models on YelpChi and Amazon is much larger than that on T-Finance. This suggests that we can achieve decent performance with less powerful models on datasets with larger NLD distances. Our finding supports the notion that NLD can influence the expressive power of the GNN model, and separating nodes with specific NLDs can improve the performance of the GNN model. (2) Spectral methods outperform spatial methods by a large margin. These tailored heterophilic filters further support our argument for the superiority of addressing the problem in the spectral domain.

## 4. Methodology

Guided by the analysis in SS3.1, we advocate for the necessity of treating nodes with distinct spectral label distributions separately. In this section, we introduce our **bi-**level optimization graph neural network BioGNN. To begin with, we introduce the learning objectives in SS4.1 and present the parameterization process in SS4.2. In SS4.4, we validate the effectiveness of the framework on golden-separated graphs.

### The Learning Objectives

To start with, we introduce Lemma 4.1 which is widely agreed upon in the literature (Zhou et al., 2017; Wang et al., 2018; Wang et al., 2018):

**Lemma 4.1** The prediction performance of a spectral filter is better when the spectral label energy distribution concentrates more on the pass band of the filter.

Building on Lemma 4.1, we could identify nodes according to the performance of different spectral filters through bi-level optimization. As shown in Figure 4, our learning objective is twofold: (1) Optimize the encoders \(\{\Phi(\cdot),\varphi_{1}(\cdot),\varphi_{2}(\cdot)\}\) to maximize the probability of correctly classifying nodes separated by \(\theta(\cdot),(2)\) Optimize the encoder \(\theta(\cdot)\) which predicts the NLD of nodes and separate nodes to two sets. We set all the encoders as MLP with learnable parameters. Concretely, the learning objective of BioGNN is defined as follows:

\[\min_{\varphi_{1},\Phi,M_{1}}\mathcal{R}(\Phi(g_{1}(\mathcal{L}) \varphi_{1}(M_{1}\circ\mathbf{X})),\mathbf{Y})\] \[+\min_{\varphi_{2},\Phi,M_{2}}\mathcal{R}(\Phi(g_{2}(\mathcal{L}) \varphi_{2}(M_{2}\circ\mathbf{X})),\mathbf{Y}),\] \[s.t.\quad M_{1}+M_{2}=\mathbf{1}, \tag{8}\]

where \(M_{1}\) and \(M_{2}\) are hard masks given by learnable encoder \(\theta(\cdot)\), \(\mathbf{1}\) is an all-one vector, \(g_{1}(L)\) and \(g_{2}(L)\) are spectral filters, and \(\circ\) denotes the element-wise multiplication.

### Instantiation of BioGNN

Given the two-fold objective, we propose to parameterize the encoder \(\theta(\cdot)\) and \(\{\Phi(\cdot),\varphi_{1}(\cdot),\varphi_{2}(\cdot)\}\).

_Parameterizing \(\theta(\cdot)\)._ The encoder \(\theta(\cdot)\) serves as a separator that predicts the NLD of nodes and feeds nodes into different branches of filters. Consequently, to obtain informative input for \(\theta\), we employ a label-wise message passing layer (Golov et al., 2013) which aggregates the labeled neighbors of the nodes label-wise. Concretely, for node \(u\), the aggregated feature \(h_{u,c}\) for class \(c\):

\[h_{u,c}=\frac{1}{|\mathcal{N}_{l,c}(u)|}\sum_{\omega\in\mathcal{N}_{l,c}(u)}x_ {0}, \tag{9}\]

where \(\mathcal{N}_{l,c}(u)\) is the set of neighbors labeled with \(c\). When there are no labeled neighbors belonging to class \(c\), we assign a zero

\begin{table}
\begin{tabular}{c|c c c|c c c c} \hline \multirow{2}{*}{Dataset} & \multicolumn{3}{c|}{Statistics} & \multicolumn{3}{c}{Neighbor Label Distribution (NLD)} \\  & \# Nodes & \# Edges & \# Features & \(p_{0}\) & \(q_{0}\) & \(p_{1}\) & \(q_{1}\) & Distance \\ \hline YelpChi & 11,944 & 4,398,392 & 25 & 0.8683 & 0.1317 & 0.8144 & 0.1856 & 0.0762 \\ Amazon & 45,954 & 3,846,979 & 32 & 0.9766 & 0.0234 & 0.9254 & 0.0746 & 0.0724 \\ T-Finance & 39,357 & 21,222,543 & 10 & 0.9850 & 0.0150 & 0.5280 & 0.4720 & 0.6462 \\ T-Social & 5,781,065 & 73,105,508 & 10 & 0.7634 & 0.2366 & 0.9161 & 0.0839 & 0.2159 \\ \hline \end{tabular}
\end{table}
Table 2. Summary of the dataset statistics and the neighbor label distributions.

Figure 3. The influence of NLD on model performance.

embedding to \(h_{u,\mathcal{C}}\). Then we set

\[M_{1}(u)=\operatorname*{argmax}(\operatorname{MLP}_{\theta}([X_{u};h_{u,0};h_{u,1 }])). \tag{10}\]

To ensure smoothed and well-defined gradients \(\frac{\partial y}{\partial\theta}\), we apply a straight-through (ST) gradient estimator (Brocker, 1991) to make the model differentiable. Note that BioGNN is trained in an iterative fashion, the encoders \(\{\Phi(\cdot),\varphi_{1}(\cdot),\varphi_{2}(\cdot)\}\) are fixed as \(\{\Phi^{*}(\cdot),\varphi_{1}^{*}(\cdot),\varphi_{2}^{*}(\cdot)\}\), the objective function in this phase is:

\[\min_{M_{1}}\mathcal{R}(\Phi^{*}(g_{1}(\mathcal{L})\varphi_{1}^{* }(M_{1}\circ\mathbf{X})),\mathbf{Y})\] \[+\min_{M_{2}}\mathcal{R}(\Phi^{*}(g_{2}(\mathcal{L})\varphi_{2}^{ *}(M_{2}\circ\mathbf{X})),\mathbf{Y})\] \[s.t.\quad M_{1}+M_{2}=\mathbf{1}. \tag{11}\]

_Parameterizing \(\{\Phi(\cdot),\varphi_{1}(\cdot),\varphi_{2}(\cdot)\}\)_. These three encoders serve as a predictor that assigns labels to input nodes. As we aim to distinguish between different spectral label distributions, which are closely related to the performance of filters with corresponding band-pass, we adopt low-pass and high-pass filters as \(g_{1}(L)\) and \(g_{2}(L)\), respectively. Here, we choose to use two branches and leave the multi-branch framework for future work. Therefore, the functions of \(M_{1}\) and \(M_{2}\) become the masking of nodes with high-frequency and low-frequency ego-graphs, respectively. In this iterative training phase, we freeze the masks as \(M_{1}^{*}\) and \(1-M_{1}^{*}\), and set the objective function as:

\[\min_{\Phi,\varphi_{1}}\mathcal{R}(\Phi(g_{1}(\mathcal{L})\varphi _{1}(M_{1}^{*}\circ\mathbf{X})),\mathbf{Y})\] \[+\min_{\Phi,\varphi_{2}}\mathcal{R}(\Phi(g_{2}(\mathcal{L}) \varphi_{2}((1-M_{1}^{*})\circ\mathbf{X})),\mathbf{Y}). \tag{12}\]

A similar training process has also been used in graph contrastive learning (Zhu et al., 2017). For the choice of \(g_{1}(\mathcal{L})\) and \(g_{2}(\mathcal{L})\), we adopt Bernstein polynomial-based filters (Zhu et al., 2017; Wang et al., 2018) for their convenience to decompose low-pass and high-pass filters:

\[g(\mathcal{L})=\frac{1}{2}U\beta_{\alpha,\beta}(\Lambda)U^{T}=\frac{(\mathcal{ L}/2)^{\alpha}(I-\mathcal{L}/2)^{\beta}}{2\int_{0}^{1}\alpha^{-1}(1-t)^{\beta-1} \mathrm{d}t}, \tag{13}\]

where \(\beta_{\alpha,\beta}\) is the standard beta distribution parameterized by \(\alpha\) and \(\beta\). When \(\alpha\to 0\), we acquire \(g(\mathcal{L})\) as a low-pass filter; similarly, \(g(\mathcal{L})\) acts as a high-pass filter when \(\beta\to 0\). For the choices of \(\alpha\) and \(\beta\) on the specific benchmark and more training details, please refer to Appendix B.1 and B.2.

### Initialization of BioGNN

To embrace a more stable process of the bi-level optimization, we initialize the encoders before iterative training.

**Initialization of \(\theta(\cdot)\)**. \(\theta(\cdot)\) is initialized in a supervised fashion, where the supervision signal is obtained by counting the labeled inter-class neighbors:

\[Y_{sep}(u)=round(\frac{1}{|\mathcal{N}_{L}(u)|}\sum_{0\in\mathcal{N}_{L}(u)}| \{y_{u}\neq y_{0}\}|), \tag{14}\]

then the cross-entropy is minimized:

\[\min_{\theta}-[Y_{sep}\circ log(\theta(\mathbf{X}))+(1-Y_{sep})\circ log(1- \theta(\mathbf{X}))]. \tag{15}\]

Note that in our experiments, although the supervision signal are calculated with ego-graphs, the input data is a **complete graph** rather than ego-graphs extracted from a larger graph. Each node in the complete graph connects directly to all other nodes, ensuring that all interactions are considered during the learning process. As nodes with high-frequency ego-graph are rare, to shield the separator from predicting all nodes as low-frequency ego nodes, we regularize the ratio of two sets of nodes by enforcing the following constraint: we treat \(\mathbf{Y}_{sep}\) as the optimal known mask, and one term \(\mathbf{Y}_{sep}-\theta(\mathbf{X})\) is added to the objective. The final objective is:

\[\min_{\theta}-[Y_{sep}\circ log(\theta(\mathbf{X}))+(1-Y_{sep})\circ log(1- \theta(\mathbf{X}))]+r(\mathbf{Y}_{sep}-\theta(\mathbf{X})). \tag{16}\]

**Initialization of \(\{\Phi(\cdot),\varphi_{1}(\cdot),\varphi_{2}(\cdot)\}\).** In this phase, we treat \(Y_{sep}\) as the optimal known mask:

\[\min_{\Phi,\varphi_{1}}\mathcal{R}(\Phi(g_{1}(\mathcal{L})\varphi _{1}(Y_{sep}\circ\mathbf{X}),\mathbf{Y})\] \[+\min_{\Phi,\varphi_{2}}\mathcal{R}(\Phi(g_{2}(\mathcal{L})\varphi _{2}((1-Y_{sep})\circ\mathbf{X}),\mathbf{Y}). \tag{17}\]
can alleviate the problem and boost the performance of GAD. We discovered that the training order is significant in achieving better performance. Training nodes with high-frequency ego-graphs before those with low-frequency ones leads to better results. One possible reason for this is the shared linear classifier \(\Phi\) between the two branches. Embeddings learned from the high-pass filter are noisier, and a classifier that performs well on noisy embeddings would most likely perform well on the whole dataset (LeCun et al., 2017). We consider this to be an intriguing discovery, yet leaving a comprehensive theoretical examination for future research.

## 5. Experiment

In this section, we conduct experiments on four benchmarks and report the results of our models as well as some state-of-the-art baselines to demonstrate the effectiveness of BioGNN.

### Experimental Setup

**Datasets.** Following previous works (Zhu et al., 2018; Zhang et al., 2019), we conduct experiments on four datasets introduced in Table 2. For more details about the datasets, please refer to Appendix B.3.

**Baselines.** Our baselines can be roughly categorized into three groups. The first group includes general methods, such as **MLP**, **GCN**(LeCun et al., 2017), **GAT**(LeCun et al., 2017), **ChebyNet**(Zhu et al., 2018), **GWNN**(Wang et al., 2019), and **JKNet**(Wang et al., 2019). As our focus is GAD, the second group considers tailored GAD methods including **CAREGNN**(Zhu et al., 2018), **PCGNN**(Wang et al., 2019), **GDN**(Wang et al., 2019), and **BWGNN**(Wang et al., 2019). The third group includes methods that consider neighbor labels, such as **H2GCN**(Wang et al., 2019), **GPRGNN**(Chen et al., 2019), and **MixHop**(Chen et al., 2019):

* **GCN**(Wang et al., 2019): GCN is a traditional graph convolutional network in spectral space.
* **GAT**(LeCun et al., 2017): GAT leverages masked self-attentional layers to weight the neighbors.
* **ChebyNet**(Zhu et al., 2018): ChebyNet generalizes CNN to graph data in the context of spectral graph theory.
* **GWNN**(Wang et al., 2019)1: GWNN leverages graph wavelet transform to address the shortcomings of spectral graph CNN methods that depend on graph Fourier transform. Footnote 1: [https://github.com/bendekrozumberscaki/GraphWaveletNewtalkNetwork](https://github.com/bendekrozumberscaki/GraphWaveletNewtalkNetwork)
* **JKNet**(Wang et al., 2019): The jumping-knowledge network which concatenates or max-pooling the hidden representations. Footnote 1: [https://github.com/PonderL7/PC-GNN](https://github.com/PonderL7/PC-GNN)
* **Care-GNN**(Zhu et al., 2018)2: Care-GNN is a camouflage-resistant graph neural network that adaptively samples neighbors according to the feature similarity, and the optimal sampling ratio is found through an RL module. Footnote 2: [https://github.com/CfNN](https://github.com/CfNN)(Wang et al., 2019)3: PC-GNN consists of two modules "pick" and "choose", and maintains a balanced label frequency around fraudsters by downsampling and upsampling. Footnote 3: [https://github.com/PonderL7/PC-GNN](https://github.com/PonderL7/PC-GNN)
* **H2GCN**(Wang et al., 2019)4: H2GCN is a tailored heterophily GNN which identifies three useful designs. Footnote 4: [https://github.com/CfNN](https://github.com/CfNN)(Wang et al., 2019)5: BWGNN is a spectral filter addressing the "right-shift" phenomenon in anomaly detection. Footnote 5: [https://github.com/jawnet/Roethinking-Anomaly-Detection](https://github.com/jawnet/Roethinking-Anomaly-Detection)
* **GDN**(Wang et al., 2019)6: GDN deals with heterophily by leveraging constraints on original node features. Footnote 6: [https://github.com/jankingjunath/](https://github.com/jankingjunath/)
* **MixHop**(Chen et al., 2019)7: MixHop repeatedly mixes feature representations of neighbors at various distances to learn relationships. Footnote 7: [https://github.com/jankingjunath/](https://github.com/jankingjunath/)
* **GPRGNN**(Chen et al., 2019)8: GPR-GNN learns a polynomial filter by directly performing gradient descent on the polynomial coefficients. Footnote 8: [https://github.com/jankingjunath/](https://github.com/jankingjunath/)

Footnote 8: [https://github.com/jankingjunath/](https://github.com/jankingjunath/)

Footnote 8: [https://github.com/jankingjunath/](https://github.com/jankingjunath/)

### Performance Comparison

The main results are reported in Table 3. Note that we search for the best threshold to achieve the best F1-macro in validation for all methods. In general, BioGNN achieves the best F1-macro score in all datasets, empirically verifying that it has a larger distance between predictions and the decision boundary, benefiting from measuring the NLD distance. For AUC, BioGNN did not achieve the best score in T-Social. We suppose the reason is that T-social has a complex frequency composition since the best performance is achieved when the frequency order is high according to BWGNN (Wang et al., 2019). We believe this issue could be alleviated if multi-branch filters are adopted, which we leave for future work. Furthermore, some methods could achieve high AUC while maintaining a low F1-Macro, indicating that the instances can be distinguished but hold tightly in the space.

Figure 5. Golden separated loss and real loss curves on YelpChi.

In such cases, it is hard to identify a classification threshold, which we consider unstable.

H2GCN, MixHop, and GPRGNN are three state-of-the-art spectral heterophilous GNNs that shed light on the relationship between the ego node and neighbor labels. We observe that they consistently outperform other groups of methods, including some tailored GAD methods. We ascribe this large performance gap to two reasons: (1) the harmfulness of heterophily where vast normal neighborhoods attenuate the suspiciousness of the anomalies; (2) the superiority of spectral filters to distinguish nodes with different NLD. However, they optimize the node representations as a whole, while BioGNN outperforms these methods, especially in FI-Macro, where the improvement ranges from 2.7% to 25.8%. This supports our analysis that different class nodes with similar NLD should be treated separately to alleviate "loss rivalry". Furthermore, among the tailored GNN methods (CAREGNN, PCGNN, GDN, BWGNN, and BioGNN), BWGNN and BioGNN are polynomial-based filters that perform better than others, further suggesting that spectral filtering is more promising in GAD.

In several datasets, MLP outperforms some GNN-based methods, indicating that blindly mixing neighbors can sometimes degrade the prediction performance. Therefore, structural information should be used with care, especially when the neighborhood label distributions for nodes are complex.

### Analysis of BioGNN

In this section, we take a closer look in BioGNN. We first verify the smoothness of the BioGNN loss curve to demonstrate its effectiveness in alleviating "loss rivalry". Then we plot the distribution of the separated nodes to elucidate that our model can successfully discriminate nodes with different NLD and set them apart. Making it more clear, we visualize some high-frequency ego-graphs.

#### Loss Rivalry Addressing

To answer the question of whether BioGNN can alleviate the "loss rivalry", we plot the training loss of BioGNN in Figure 4(b). Similar to Section 4.4, two separate sets of nodes are trained in a specific order: high-frequency nodes are trained first, followed by low-frequency nodes. Comparing Figure 2, 4(a), and 4(b), we find that the smoothness of BioGNN's training curve lies between golden-separate and mixed training, indicating that the new framework is effective in alleviating "loss rivalry" and improves the overall performance of GAD.

#### Distribution of the separated nodes

The core of BioGNN is node separation. To further validate its effectiveness, we report the

\begin{table}
\begin{tabular}{c|c c|c c|c c|c c} \hline \hline Dataset & \multicolumn{2}{c|}{YelpChi} & \multicolumn{2}{c|}{Amazon} & \multicolumn{2}{c|}{T-Finance} & \multicolumn{2}{c}{T-Social} \\ Metric & F1-Macro & AUC & F1-Macro & AUC & F1-Macro & AUC & F1-Macro & AUC \\ \hline MLP & 0.4614 & 0.7366 & 0.9010 & 0.9082 & 0.4883 & 0.8609 & 0.4406 & 0.4923 \\ GCN & 0.5157 & 0.5413 & 0.5098 & 0.5083 & 0.5254 & 0.8203 & 0.6550 & 0.7012 \\ GAT & 0.4614 & 0.5459 & 0.5675 & 0.7731 & 0.8816 & 0.9388 & 0.4921 & 0.4923 \\ ChebyNet & 0.4608 & 0.6216 & 0.8070 & 0.9187 & 0.8017 & 0.8001 & & OOM \\ GWNN & 0.4608 & 0.6246 & 0.4822 & 0.9319 & 0.4883 & 0.9670 & & OOM \\ JKNet & 0.5805 & 0.7736 & 0.8270 & 0.8970 & 0.8971 & 0.9554 & 0.4923 & 0.7226 \\ \hline CAREGNN & 0.5015 & 0.7300 & 0.6313 & 0.8832 & 0.7261 & 0.9105 & 0.4868 & 0.7939 \\ PCGNN & 0.6925 & 0.8118 & 0.8367 & 0.9555 & 0.4462 & 0.9200 & 0.4536 & 0.8917 \\ GDN & 0.7545 & 0.8904 & 0.9068 & 0.9709 & 0.8474 & 0.9462 & 0.7401 & 0.9287 \\ BWGNN & 0.7568 & **0.8967** & 0.9204 & 0.9706 & 0.8899 & 0.9599 & 0.7494 & 0.9275 \\ \hline H2GCN & 0.6575 & 0.8406 & 0.9213 & 0.9693 & 0.8824 & 0.9553 & OOM & OOM \\ MixHop & 0.6534 & 0.8796 & 0.8093 & 0.9723 & 0.4880 & 0.9569 & 0.6471 & 0.9597 \\ GPRGNN & 0.6423 & 0.8355 & 0.8059 & 0.9358 & 0.8507 & 0.9462 & 0.5976 & **0.9622** \\ BioGNN & **0.7606** & 0.8947 & **0.9462** & **0.9766** & **0.9059** & **0.9670** & **0.8140** & 0.9325 \\ \hline \hline \end{tabular}
\end{table}
Table 3. Performance Results. The best results are in boldface, and the 2nd-best are underlined.

Figure 6. The NLD distance between two separated sets of nodes.

empirical histogram of the NLD in four benchmarks in Figure 6. The x-axis represents the edge homophily, which explicitly represents the NLD around the ego node. The y-axis denotes the density, and the distribution curves are shown in dashed lines. From Figure 6, we observe that the two histograms seldom overlap, and the mean of two curves maintains a separable distance, demonstrating that BioGNN successfully sets the nodes apart.

**Visualization.** To show the results in an intuitive way, we report the ego-graph of some nodes in Figure 7. These nodes are assigned to the high-pass filter by \(\theta(\cdot)\). As observed from the figure where color denotes the class of the nodes, the ego node (red-circled) has more inter-class neighbors compared to the nodes assigned to the low-pass filter. This finding provides support for Equation 7 and verifies the effectiveness of our novel framework. More visualizations are in Appendix C.

**Time complexity analysis.** The time complexity of BioGNN is \(O(C|\mathcal{E}|)\), where \(C\) represents a constant and \(|\mathcal{E}|\) denotes the number of edges in the graph. This is due to the fact that the BernNet-based filter is a polynomial function that can be computed recursively, as explained in (Gardner et al., 2017).

## 6. Related Work

In this section, we introduce some static GAD networks and polynomial-based spectral GNNs.

### Static Graph Anomaly Detection

On static attributed graphs, GNN-based semi-supervised learning methods are widely adopted. For example, GraphUCB (Grabh et al., 2019) adopts contextual multi-armed bandit technology, and transforms graph anomaly detection into a decision-making problem; DCI (Zhu et al., 2019) decouples representation learning and classification with the self-supervised learning task. Recent methods realize the necessity of leveraging multi-relation graphs into GAD. EdGars (Zhu et al., 2019) and GraphConsis (Zhu et al., 2019) construct a single homo-graph with multiple relations. Likewise, Semi-GNN (Zhu et al., 2019), CARE-GNN (Grabh et al., 2019), and PC-GNN (Zhu et al., 2019) construct multiple homo-graphs based on node relations. In addition, some works discover that heterophily should be addressed properly in GAD. Semi-GNN and IHGAT (Grabh et al., 2019) employ hierarchical attention mechanisms for interpretable prediction, while based on camouflage behaviors and imbalanced problems, CARE-GNN, PC-GNN, and AO-GNN (Zhu et al., 2019) prune edges adaptively according to neighbor distribution. GDN (Grabh et al., 2019) and H2 -FDetector (Zhu et al., 2019) adopt different strategies for anomalies and normal nodes.

### Graph Spectral Filtering

Spectral GNNs simulate filters with different passbands in the spectral domain, enabling GNNs to work on both homophilic and heterophilic graphs (Zhu et al., 2019). GPRGNN (Grabh et al., 2019) adaptively learns the Generalized PageRank weights, regardless of whether the node labels are homophilic or heterophilic. FSGNN (Zhu et al., 2019) designs a feature selection graph neural network. FAGCN (Zhu et al., 2019) adaptively fuses different signals in the process of message passing by employing a self-gating mechanism. BernNet (Zhu et al., 2019) expresses the filtering operation with Bernstein polynomials. BWGNN (Gardner et al., 2017) observes the "right-shift" phenomenon and designs a band-pass filter to aggregate different frequency signals simultaneously. AdaGNN (Grabh et al., 2019) captures the varying importance of different frequency components to alleviate over-smoothing problem. AMNet (Bahdanau et al., 2015) aims to capture both low-frequency and high-frequency signals, and adaptively combine signals of different frequencies. GHRN (Grabh et al., 2019) design an edge indicator to distinguish homophilous and heterophilous edges.

## 7. Limitation and Conclusion

**Limitation.** Although we propose a novel network that treats nodes separately, it has some limitations. Our work only separates the nodes into two sets, and we hope to extend it to more fine-grained multi-branch neural networks in the future. Furthermore, our theoretical result largely relies on CSBM's assumptions; hence our model may fail in some cases where the graph generation process doesn't follow these assumptions.

**Conclusion.** This work starts with "loss rivalry", expressing the phenomenon that some nodes tend to have opposite loss curves from others. We argue that it is caused by the mixed training of different class nodes with similar NLD. Furthermore, we discover that spectral filters are superior in addressing the problem. To this end, we propose BioGNN, which essentially discriminates nodes that share similar NLD but are likely to be in different classes and feeds them into different filters to prevent "loss rivalry".

## References

* (1)
* Abu-El-Haija et al. (2019) Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina Lerman, Havy Haertuymyan, Greg Ver Steeg, and Aram Galstyan. 2019. Michigan: Higher-order graph convolutional architectures via sparsified neighborhood mining. In _ICML_. 21-29.
*

Figure 7. The ego-graph of some yellow-circled ego nodes classified as high-frequency by BioGNN in YelpChi. The anomalies are represented in red, while normals are represented in blue.

* Bengio et al. (2013) Yoshua Bengio, Nicholas Leonard, and Aaron C. Courville. 2013. Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation. _CoRR_ abs/1308.3432 (2013).
* Bo et al. (2021) Deyu Bo, Xiao Wang, Chuan Shi, and Huawei Shen. 2021. Beyond low-frequency information in graph convolutional networks. In _AAAI_. 3950-3957.
* Cai et al. (2022) Ziwei Cai, Sui Fu, Yang Yang, Shiliang Pu, Jiangsu Xu, Huagong Cai, and Weihao Jing. 2022. Can An abnormality be Detected by Graph Neural Networks?. In _IJCAI_. 1945-1951.
* Cai et al. (2022) Ziwei Cai, Sui Fu, Yang Yang, Shiliang Pu, Jiangsu Xu, Huagyu Cai, and Weihao Jiang. 2022. Can An abnormality be Detected by Graph Neural Networks?. In _IJCAI_. 23-29.
* Chapurovs and Musco (2022) Sadhunis Chapurovs and Cameron Musco. 2022. Simplified Graph Convolution with Heteronymity. In _NeurIPS_.
* Chen et al. (2020) Ming Chen, Zhewei Wei, Zengfeng Huang, Belin Ding, and Yaliang Li. 2020. Simple and Deep Graph Convolutional Networks. In _ICML_. 2735-2735.
* Chen et al. (2020) Zhiqian Chen, Fangjian Chen, Lei Zhang, Taoan J, Raison Fu, Liang Zhao, Feng Chen, and Chang-Tien Lu. 2020. Bridging the Gap between Spatial and Spectral Domains: A Survey on Graph Neural Networks. _CoRR_ abs/2002.18670 (2020).
* Chen et al. (2022) Zhixian Chen, Tengfei Liu, and Yang Wang. 2022. When Does 3. Spectral Graph Neural Network fail in Node Classification? _CoRR_ abs/2202.07902 (2022).
* Cheng et al. (2021) Li Cheng, Roncheng Guo, Kai Shu, and Huan Liu. 2021. Causal understanding of fine clues dissemination on social media. In _KDD_. 148-157.
* Chen et al. (2020) Eli Chen, Jianhao Peng, Fan Li, and Qijie Mellorio. 2020. Adaptive Universal Generalized PageRank Graph Neural Network. In _ICLR_.
* Dai et al. (2021) Espan Dai, Ziizeng Guo, and Sabang Wang. 2021. Label-Wise Message Passing Graph Neural Network on Heterographic Graphs. _CoRR_ abs/2110.08128 (2021).
* Defferrant et al. (2016) Michael Defferrant, Xavier Bresson, and Pierre Vandergheynst. 2016. Convolutional Neural Networks on Graphs with Part Localized Spectral Filtering. In _NIPS_. 3837-3845.
* Dehpande et al. (2018) Nath Dehpande, Subhabrata Sen, Andrea Montanari, and Elchanan Mossel. 2018. Contextual stochastic block models. In _NeurIPS_, Vol. 31.
* Deshpande et al. (2018) Nath Deshpande, Subhabrata Sen, Andrea Montanari, and Elchanan Mossel. 2018. Contextual Stochastic Block Models. In _NeurIPS_. 8599-8602.
* Ding et al. (2019) Kaixie Ding, Jundong Li, and Hanjun Li. 2019. Interactive anomaly detection on attributed networks. In _WSDM_. 357-365.
* Dong et al. (2021) Yushun Dong, Kaixie Ding, Brian Jialani, Shuiwang Ji, and Jundong Li. 2021. Adding: Graph neural networks with adaptive frequency response filter. In _CVPR_. 392-401.
* Duo et al. (2020) Yingdong Dou, Zhiwei Liu, Li Sun, Yulong Deng, Hao Peng, and Philip S Yu. 2020. Enhancing graph neural network-based hand detectors against cambiotized fraudsters. In _CIKM_. 315-324.
* Gao et al. (2022) Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huanin Feng, and Yongdong Zhang. 2022. Addressing Heterophily in Graph Anomaly Detection: A Perspective of Graph Spectrum. In _WWW_. ACM, 1528-1538.
* Gao et al. (2023) Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huanin Feng, and Yongdong Zhang. 2023. Alleviating Structural Distribution Shift in Graph Anomaly Detection. In _WSDM_.
* Gao et al. (2023) Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huanin Feng, and Yongdong Zhang. 2023. Alleviating Structural Distribution Shift in Graph Anomaly Detection. In _WSDM_.
* Hawkins (1980) Douglas M Hawkins. 1980. _Identification of outliers_. Vol. 11. Springer.
* He et al. (2021) Mingwei He, Zhewei Wei, Zengfeng Huang, and Hongfeng Su. 2021. BernNet: Learning Arbitrary Graph Spectral Filters via Bernstein Approximation. In _NeurIPS_. 14239-14251.
* Hu et al. (2022) Weihua Hu, Rafail Cao, Kesar Huang, Edward W. Huang, Karrilka Sabbian, and Jure Leskovec. 2022. Tunst-Dv: A Training Strategy for Improving Generalization of Graph Neural Networks. _CoRR_ abs/2210.14843 (2022).
* Huang et al. (2022) Mengqi Huang, Yang Liu, Xiang Ao, Kuan Li, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing He. 2022. Co-oriented Graph Neural Network for Fraud Detection. In _WWW_. 3111-312.
* Kipf and Welling (2017) Thomas N. Kipf and Max Welling. 2017. Semi-Supervised Classification with Graph Convolutional Networks. In _ICLR_.
* Lei et al. (2022) Runlin Lei, Zhen Wang, Yaliang Li, Boilin Ding, and Zhewei Wei. 2022. EvenNet: Ignoring Odd-Hop Neighbors Improves Robustness of Graph Neural Networks. In _NeurIPS_.
* Cai et al. (2021) Can Tai, Li Sun, Xiang Ao, Jinghua Feng, Qing He, and Hao Yang. 2021. Interfusion-aware heterogeneous graph attention networks for fraud transactions detection. In _KDD_. 3280-3288.
* Liu et al. (2022) Kay Jan, Yinglong Dou, Yue Zhao, Xueying Ding, Xiyang Hu, Ruitong Zhang, Kaixie Ding, Canyu Chen, Hao Peng, Kai Shu, et al. 2022. BOND: Benchmarking Unsupervised Outlier Node Detection on Static Attributed Graphs. In _NeurIPS Datasets and Benchmarks_. Track.
* Liu et al. (2021) Yang Liu, Xiang Ao, Zoli Qiu, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing He. 2021. Pick an choose: a GN-based imbalanced learning approach for fraud detection. In _WWW_. 3168-3177.
* Liu et al. (2020) Zhiwei Liu, Yinglong Dou, Philip S Yu, Yutong Deng, and Hao Peng. 2020. Alleviating the inconsistency problem of applying graph neural network to fraud detection. In _SIGIR_. 1569-1572.
* Ma et al. (2021) Xiaoxiao Ma, Jia Wu, Shan Xue, Jian Yang, Chuan Zhou, Quan Z Sheng, Hui Xiong, and Leman Akoglu. 2021. A comprehensive survey on graph anomaly detection with deep learning. _TKDE_ (2021).
* Ma et al. (2022) Yao Ma, Xiaorui Liu, Neil Shah, and Jiliang Tang. 2022. Is homophily a necessity for graph neural networks?. In _ICLR_.
* Mauryu et al. (2021) Sunil Kumar Mauryu, Xin Liu, and Tuuyoshi Murata. 2021. Improving Graph Neural Networks with Simple Architecture Design. _CoRR_ abs/2102.05763 (2021).
* Moulay and Leskovec (2013) Julian John Moulay and Jure Leskovec. 2013. From amateurs to connoisseur: modeling the evolution of user expertise through online reviews. In _WWW_. 897-908.
* Rayman and Akoglu (2016) Shebuti Rayman and Leman Akoglu. 2016. Collective opinion spam detection using active inference. In _SDM_. 504-508.
* Shi et al. (2022) Fengzhgo Shi, Yannu Cao, Yannim Shang, Yuchen Zhou, Chuan Zhou, and Jia Wu. 2022. H2-PIectectect: A GNN-based Hard Detector with Homophilic and Heterophilic Connections. In _WWW_. 1188-1194.
* Shi et al. (2022) Fengzhgo Shi, Tsan Cao, Yannim Shang, Yuchen Zhou, Chuan Zhou, and Jia Wu. 2022. H2-PIectectect: A CNN-based Hard Detector with Homophilic and Heterophilic Connections. In _WWW_. 1486-1494.
* Shen et al. (2021) Suheel Suheel, Jun Li, Qing Hao, and Jennifer Neville. 2021. Adversarial Graph Augmentation to Improve Graph Contexts Learning. In _NeurIPS_. 15929-1593.
* Tang et al. (2021) Jianheng Tang, Jiqin Li, Ziqi Gao, and Jia Li. 2021. Rethinking Graph Neural Networks for Anomaly Detection. In _ICML_. 2017-2106.
* Velickovic et al. (2018) Felickovic, Guillen Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Joshua Bengio. 2018. Graph Attention Networks. In _ICLR_.
* Wang et al. (2019) Daixin Wang, Zabrin Liu, Peng Cui, Quanhi Jia, Zhen Wang, Yanming Fang, Quan Yu, Jun Zhou, Shuang Yang, and Yuan Q. 2019. A semi-supervised graph attentive network for financial fraud detection. In _ICML_. 590-607.
* Wang et al. (2008) Jianyu Wang, Rui Wen, Ming Wu, Yu Huang, and Jian-Xin. 2008. Fldgars: Fruuder detection via graph convolutional networks in online app review system. In _WWW_. (2008) 310-316.
* Wang and Zhang (2002) Xiyun Wang and Nathan Zhang. 2022. How Powerful are Spectral Graph Neural Networks. In _KM_. 23341-23362.
* Yang et al. (2012) Yanlong Wang, Jingzhang Shao, Guo, Honghi Yin, Cuiping Li, and Hong Chen. 2021. Decoupling representation learning and classification for gpn-based anomaly detection. In _SIGIR_. 1293-1248.
* Wu et al. (2018) Qian Wu, Hengxin Zhang, Junchi Yan, and David Wipf. 2022. Handling Distribution Shifts on Graphs: An Invariance Perspective. In _ICLR_.
* Shen et al. (2019) Bingbing Xu, Huawei Shen, Qi, Cao Yau, Yongqi Qiu, and Xueqi Cheng. 2019. Graph Wavelet Neural Network. In _ICLR_ (Punct) OpenReview.net.
* Xu et al. (2019) Keyhua Xu, Weihua Hu, Jue Leskovec, and Stefaniejekola. 2019. How Powerful are Graph Neural Networks?. In _ICLR_.
* Xu et al. (2018) Keyulu Xu, Chenghao Li, Donghao Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie Jegelka. 2018. Representation Learning on Graphs with Jumping Knowledge Networks. In _ICML_. 5449-5458.
* Zhu et al. (2018) Jong Zhu, Yiqin Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Dami Kostra. 2020. Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs. In _NeurIPS_.

## Appendix A Proofs

In this section, the proofs of propositions are listed.

### Proof of Proposition 1

Proof.: In the spectral domain, the hidden representation of the spectral filter can be expressed as:

\[H=\sum_{k}a_{k}\hat{L}^{k}\mathbf{X}=\sum_{k}a_{k}(\mathbf{I}-\mathbf{D}^{-1/2} \mathbf{D}^{-1/2})^{k}\mathbf{X} \tag{18}\]

Taking the second-order spectral filter as an example,

\[H^{2}=a_{0}\mathbf{X}+a_{1}(\mathbf{I}-\mathbf{D}^{-1/2}\mathbf{D}^{-1/2}) \mathbf{X}+a_{2}(\mathbf{I}-\mathbf{D}^{-1/2}\mathbf{D}^{-1/2})^{2}\mathbf{X} \tag{19}\]

The representation of node \(i\) is given as:

\[h_{i} =a_{0}x_{i}+a_{1}(x_{i}-\frac{1}{deg(x_{i})}\sum_{j\in N_{i}}x_{j })+ \tag{20}\] \[a_{2}(x_{i}-2\frac{1}{deg(x_{i})}\sum_{j\in N_{i}}x_{j}+\frac{1} {deg(x_{i})}\sum_{j\in N_{i}}\frac{1}{deg(x_{j})}\sum_{k\in N_{j}}x_{k})\] \[=(a_{0}+a_{1}+a_{2})x_{i}-\frac{a_{1}+2a_{2}}{deg(x_{i})}\sum_{j \in N_{i}}x_{j}\] \[+\frac{a_{2}}{deg(x_{i})}\sum_{j\in N_{i}}\frac{1}{deg(x_{j})} \sum_{k\in N_{j}}x_{k}\]

Here we only focus on the aggregation process, hence the non-linear activation is ignored. Furthermore, to simplify the calculation and the analysis, we set \(a_{0}\) as \(1\), \(a_{1}\) as \(-1\), and \(a_{2}\) as \(1\). In this case the coefficients or the numerators of the coefficients equal to \(1\). Suppose \(u\) and \(v\) are nodes with different labels (\(i\),\(e\), anomalies and normal nodes), along with their NLD as \(\mathcal{D}_{u}=[p_{1},q_{1}]\) and \(\mathcal{D}_{v}=[p_{0},q_{0}]\), where \(p_{1}+q_{1}=p_{0}+q_{0}=1\). From previous analysis, we assume \(x_{u}\sim N(\mu_{1},\mathbf{I})\) and \(x_{v}\sim N(\mu_{0},\mathbf{I})\), hence we know \(h_{u}\) and \(h_{v}\) should obey Gaussian distribution, whose mean can be acquired as:

\[\mu_{u} =\mu_{1}-(p_{1}\mu_{0}+q_{1}\mu_{1})+p_{1}(p_{0}\mu_{0}+q_{0}\mu_{ 1})+q_{1}(p_{1}\mu_{0}+q_{1}\mu_{1}) \tag{21}\] \[=\mu_{1}+p_{1}(p_{0}\mu_{0}+q_{0}\mu_{1}-p_{1}\mu_{0}-q_{1}\mu_{1})\] \[=\mu_{1}+p_{1}[(p_{0}-p_{1})\mu_{0}+(q_{0}-q_{1})\mu_{1}]\] \[\mu_{0} =\mu_{0}-q_{0}[(p_{0}-p_{1})\mu_{0}+(q_{0}-q_{1})\mu_{1}]\]

Hence the distance between the mean of these two distributions is:

\[||\mu_{u}-\mu_{0}||_{2} =||\mu_{1}-\mu_{0}||_{2}+(p_{1}+q_{0})||(p_{0}-p_{1})\mu_{0}+(q_{0 }-q_{1})\mu_{1}||_{2} \tag{22}\] \[=||\mu_{1}-\mu_{0}||_{2}+(1+q_{0}-q_{1})\cdot|q_{0}-q_{1}|\cdot|| \mu_{1}-\mu_{0}||_{2}\] \[=[1+|q_{0}-q_{1}|+|(p_{0}-p_{1})(q_{0}-q_{1})|]\cdot||\mu_{1}-\mu _{0}||_{2}\]

Similarly, since \(|q_{0}-q_{1}|=|p_{0}-p_{1}|\), we have:

\[||\mu_{u}-\mu_{0}||_{2}=[1+|p_{0}-p_{1}|+|(p_{0}-p_{1})(q_{0}-q_{1})|]\cdot|| \mu_{1}-\mu_{0}||_{2} \tag{23}\]

In our paper, we adopt Euclidean distance between vectors as NLD:

\[d(u,v)=\sqrt{(p_{0}-p_{1})^{2}+(q_{0}-q_{1})^{2}} \tag{24}\]

Joining equations (23) and (24), we can rewrite the distance between distribution mean values as:

\[||\mu_{u}-\mu_{0}||_{2}=[1+\frac{[d(u,v)]}{\sqrt{2}}+\frac{[d(u,v)]^{2}}{2}] \cdot||\mu_{1}-\mu_{0}||_{2}. \tag{25}\]

Likewise, the mean values of hidden representation given by a \(2\)-layer vanilla GCN are:

\[\mu_{u} =p_{1}(p_{0}\mu_{0}+p_{1}\mu_{1})+q_{1}(p_{1}\mu_{0}+p_{0}\mu_{1}) \tag{26}\] \[=\mu_{0}+p_{1}^{2}(\mu_{1}-\mu_{0})+q_{1}p_{0}(\mu_{1}-\mu_{0})\] \[\mu_{0} =p_{0}(p_{0}\mu_{0}+p_{1}\mu_{1})+q_{0}(p_{1}\mu_{0}+p_{0}\mu_{1})\] \[=\mu_{0}+p_{1}p_{0}(\mu_{1}-\mu_{0})+qqp_{0}(\mu_{1}-\mu_{0})\]

Hence we have the distance between them:

\[||\mu_{u}-\mu_{0}||_{2}=p_{1}\cdot|p_{1}-p_{0}|\cdot||\mu_{1}-\mu_{0}||_{2}-p_ {0}\cdot|q_{1}-qq|\cdot||\mu_{1}-\mu_{0}||_{2} \tag{27}\]

Since \(|p_{1}-p_{0}|=|1-q_{1}-(1-q_{0})|=|q_{0}-q_{1}|\)

\[||\mu_{u}-\mu_{0}||_{2}=|(p_{0}-p_{1})(q_{0}-q_{1})|\cdot||\mu_{1}-\mu_{0}||_{2} \tag{28}\]

Joining Equations (24) and (28), we can rewrite the distance as:

\[||\mu_{u}-\mu_{0}||_{2}=\frac{[d(u,v)]^{2}}{2}\cdot||\mu_{1}-\mu_{0}||_{2}, \tag{29}\]

Finish the proof.

### Proof of Proposition 2

The Rayleigh quotient is widely adopted as the smoothness index which plays the role of frequency in classical spectral analysis. Here we adopt this metric to bridge two variables. Specifically, the Rayleigh quotient of the one-hot label vector \(\mathbf{y}\) is:

\[E[\mathbf{y}] =\mathbf{y}^{T}\mathbf{\mathcal{L}}\mathbf{y}=\mathbf{y}^{T} \mathbf{\mathbf{D}}\mathbf{y}-\mathbf{y}^{T}\mathbf{\mathbf{A}}\mathbf{y}= \sum_{l=1}^{N}d_{l}\mathbf{y}_{l}^{2}-\sum_{l,j=1}^{N}\mathbf{y}_{l}\mathbf{y}_ {l}\mathbf{\mathbf{y}}_{l}\mathbf{\mathbf{y}}_{l} \tag{30}\] \[=\frac{1}{2}(\sum_{i=1}^{N}d_{l}\mathbf{y}_{l}^{2}-2\sum_{i,j=1}^ {N}\mathbf{y}_{i}\mathbf{y}_{j}\mathbf{\mathbf{A}}_{ij}+\sum_{j=1}^{N}d_{j} \mathbf{y}_{j}^{2})\] \[=\frac{1}{2}(\sum_{i,j\})\in\mathcal{E}(\mathbf{y}_{i}-\mathbf{y} _{j})^{2}\] \[=\sum_{(i,j)\in\mathcal{E}}\mathbb{I}(\mathbf{y}_{i}\neq\mathbf{y} _{j})\] \[=|\mathcal{E}|\cdot(1-h(\mathcal{G}))\]

On the other hand, the Rayleigh quotient can also be acquired as:

\[E[\mathbf{y}] =\mathbf{y}^{T}\mathbf{U}\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{ \mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf \mathbf \mathbf \mathbf{\mathbf \mathbf{\mathbf \mathbfmathbfmathbfmathbfmathbf{\mathbf{\mathbf      \mathbfmathbfmathbfmathbfmathbfmathbfmathbfmathbfmathbf{\mathbf{\mathbf{\mathbfmathbf{\mathbf{\mathbf{\mathbfmathbf{\mathbfmathbf{\mathbfmathbf{\mathbfmathbf\mathbf{\mathbfmathbf\mathbf\mathbf\mathbf\mathbf\mathbf\,\mathbf.}}} \mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf, \mathbf{\mathbf,\mathbf,\mathbf{\mathbf,\mathbf,\mathbf,\mathbf,\mathbf,\mathbf,\mathbf,\mathbf,\mathbf,\mathbfNote that \(\mathcal{G}\) is the ego-graph of the node, hence \(1-h(\mathcal{G})\) is the ratio of inter-class edges, which is \(q_{0}\) for negative nodes and \(p_{1}\) for positive nodes. Finish the proof.

## Appendix B Reproducibility

In this section, some details for reproducibility are listed.

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline Dataset & YelpChi & Amazon & T-Finance & T-Social \\ \hline \(\alpha\) & \{**0**,1,2\} & **[0,1,2]** & **[0,1,2]** & **[0,1,2]** \\ \(\beta\) & \{**0**,1,2\} & **[0,1,2]** & **[0,1,2]** & **[0,1,2]** \\ \hline Learning Rate (\(lr\)) for \(\theta\) & \{1e-3, 5e-3, **1e-2**\} & \multirow{2}{*}{128} & \multirow{2}{*}{128} \\ Learning Rate (\(lr\)) for \(\Phi\) & \{1e-3, 5e-3, **1e-2**\} & \multirow{2}{*}{128} & \multirow{2}{*}{128} \\ Learning Rate (\(lr\)) for \(\varphi_{1}\) and \(\varphi_{2}\) & \{**1e-3**, 5e-3, 1e-2**\} & \multirow{2}{*}{128} \\ weight decay for \(\Phi\) & \& 1e-3 & \\ \hline \end{tabular}
\end{table}
Table 4. Model Hyperparameters and their search ranges

Figure 8. More training curves of BioGNN.

Figure 10. The ego-graph of some yellow-circled ego nodes classified as low-frequency by BioGNN.

Figure 9. The ego-graph of some yellow-circled ego nodes classified as high-frequency by BioGNN in Amazon.

### Model Hyperparameters

According to [40], a Bernstein Polynomial-Based filter is parameterized by \(\alpha\) and \(\beta\). The choice of \(\alpha\) and \(\beta\) on datasets are presented in Table 4. In addition, some basic learning hyperparameters are reported.

### Datasets

The YelpChi dataset [36] focus on detecting anomalous recommendations from Yelp.com. The Amazon dataset [35] includes product reviews under the Musical Instruments category from Amazon.com. Both of the datasets have three relations, hence we treat them as multi-relation graphs. T-Social and T-Finance [40] are two large-scale datasets released recently. The T-Finance dataset aims to detect anomalous accounts in a transaction network where the nodes are annotated as anomaly if they are likely fraud, money laundering and online gambling. The nodes are accounts with 10-dimension features whereas the edges connecting them denote they have transaction records. The T-social dataset aims to detect human-annotated anomaly accounts in a social network. The node annotations and features are the same as T-Finance, whereas the edges connecting the nodes denote they maintain the friendship for more than 3 months.

## Appendix C Limited Label and Anomalies

In the real-world case, the percentage of the anomaly is usually quite low, even less than 5% or even 1%; also human annotation is expensive which leads to limited label information. Hence we have conducted additional experiments to address this concern and provide insights into the performance in such scenarios in Table 5.

**Limited Label Information:** We experimented with reduced labeled data to 1%. Despite the reduced amount of labeled information, our proposed method still achieved good performance, demonstrating its ability to effectively leverage limited label information for accurate detection.

**Small Percentage of Abnormal Nodes:** We also examined the performance when the dataset contained a small percentage of abnormal nodes. In this scenario, our proposed method maintained a high level of f1 and auc in detecting the abnormal instances, even amidst the imbalanced class distribution.

**Small Percentage of Abnormal Nodes with limited label information:** In this case, the performance of the proposed method drops a little due to the inaccurate prediction of the NLD of the nodes.

\begin{table}
\begin{tabular}{l r r r r}  & **BWGNN(F1-Macro \%)** & **BWGNN(AUC \%)** & **BioGNN(F1-Macro \%)** & **BioGNN(AUC \%)** & **1337** \\ \hline Yelp (anomaly=5\%, training=40\%) & 76.44 & 89.67 & 74.71 & 88.17 & 1389 \\ Yelp(anomaly=14.53\%, training=1\%) & 67.02 & 79.65 & 67.12 & 80.20 & 1389 \\ Yelp (anomaly=5\%, training=1\%) & 66.27 & 78.49 & 64.93 & 74.57 & 1381 \\ Amazon (anomaly=5\%, training=40\%) & 91.20 & 96.55 & 90.98 & 96.60 & 1382 \\ Amazon (anomaly=6.87\%, training=1\%) & 90.69 & 91.24 & 84.36 & 92.46 & 1383 \\ Amazon (anomaly=5\%, training=5\%) & 89.69 & 94.20 & 86.90 & 93.60 & 1384 \\ Tfinance (anomaly=4.58\%, training=1\%) & 84.89 & 91.15 & 83.14 & 92.53 & 1386 \\ TSocial (anomaly=3.01\%, training=1\%) & 75.93 & 88.06 & 83.07 & 93.75 & 1386 \\ \hline \end{tabular}
\end{table}
Table 5. Performance with limited label information or (and) small percentage of abnormal nodes