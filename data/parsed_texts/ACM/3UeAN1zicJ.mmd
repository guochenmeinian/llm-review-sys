[MISSING_PAGE_FAIL:1]

keywords and highlight their token entropy in UE. Kuhn et al. [32] cluster answers with similar semantics and aggregate their generation entropy to eliminate lexical influence. These studies primarily concentrate on enhancing UE by exploiting model-intrinsic information such as token probabilities, relational coherence, and semantic consistency, while often overlooking the impact of the background or scenario in which the QA is presented.

Intuitively, when a query is expressed in a different background or conversational scenario (e.g., debate or casual conversation), the human perception of the answer's uncertainty varies.1 Given that LLMs easily capture strong style biases unrelated to factual content during training [59], we argue that similar to humans, the perception of LLMs' uncertainty may be heavily influenced by the scenario-related noise (e.g., language style, wording, and syntax). In nearly all real-world situations, users care more about how confident the LLM is about the semantics of their query, such as the meaning, facts, or knowledge involved. They are less concerned about the model's uncertainty of the conversational scenario or speaking background. Therefore, alleviating the impact of scenario backgrounds in UE and focusing mainly on the contained semantics can lead to a more desirable estimation of the LLM's uncertainty for real-world applications.

Footnote 1: For the question "Is drinking eight glasses of water a day necessary for good health?", humans may be less certain about their answer when in the scenario of an academic symposium than in a daily chir-chat.

In this paper, we propose a plug-and-play scenario-independent framework to augment unsupervised UE for LLM, which disentangles the semantic information and scenario-specific information via factor analysis and focuses on semantic information for UE while ignoring scenario-specific noises2. The plug-and-play design means that our method is adaptive to almost all existing UE methods, including black-box LLMs (i.e. GPT-4) based UE methods, since our method only uses the traditional UE's outputs instead of accessing its model structure. Specifically, given any existing UE model, to capture that model's output distribution in various scenarios, we design a scenario-specific sampling approach for a given QA pair to generate multiple paraphrases containing the shared common semantics while differing in their scenario backgrounds (see the demonstration of our intuition in Figure 1). Subsequently, we propose a factor analysis (FA) model to disentangle the given UE model's outputs into a combination of multiple latent factors, representing the contribution of the common semantics and scenario-related noise. By solving the FA model, we decompose the impact of the most influential latent factor to approximate the uncertainty that originates from the common semantics, thus achieving a scenario-independent UE. Extensive empirical evidence verifies the effectiveness and robustness of our framework.

Footnote 2: Our code is available at: anonymous.4open.science/vWWW551.

Our contributions are as follows: (1) To the best of our knowledge, we are the first to consider scenario information to enhance uncertainty estimation for detecting the hallucinations in LLMs; (2) We model scenario-independent uncertainty estimation with factor analysis and disentangle the semantic information from scenario-related noise to improve UE; (3) Our method achieves SOTA performance across multiple datasets and model families. Empirical evidence verifies the effectiveness of our proposed approach.

## 2. Related Work

### Reference-based Hallucination Detection

Reference-based hallucination detection uses external references (e.g., Documents [7], knowledge graph [44]) to compare the generated outputs from LLMs with known facts, thus determining the presence of hallucinations. Huo et al. [23] combine the question with the LLM's generated answer to retrieve supporting evidence from a corpus, thereby improving the detection of hallucinations in their responses. Sansford et al. [44] introduce GraphEval and leverage knowledge graphs to provide well-structured, interpretable evaluations and corrections for hallucination detection. Min et al. [40] introduce FACTSCORE, a fine-grained evaluation metric for factual precision in long-form text generation by decomposing text into atomic facts and assessing their veracity against reliable knowledge sources. Chern et al. [10] propose a versatile framework for detecting factual errors across multiple tasks and domains by leveraging external tools, such as Google Search. These approaches rely on high-quality external references to ensure that the generated outputs are aligned with verified facts.

### Hallucination Detection via Uncertainty Estimation

UE assesses the likelihood that the LLM-generated content is factual without relying on external references, which can be categorized into two types based on the need for supervision [32, 36].

#### Supervised Approach for Uncertainty Estimation

Supervised approaches train the LLM to generate a confidence score alongside its responses [1, 56] or train an additional detector [8, 9, 11, 48] to recognize hallucinations in the LLM's output. Zhang et al. [57] introduce an automated approach for creating synthetic data to train hallucination detectors, enhancing detection accuracy and latency without manual annotations. Ji et al. [26] propose an iterative self-training framework that progressively scales hallucination annotation datasets, thereby improving the accuracy of hallucination annotators in LLMs. Chen et al. [8] suggest training a discriminator

Figure 1. The idea of our method. Black dashed lines in both subfigures indicate the UE scores obtained with (right) and without (left) applying our method. (A) The traditional scenario-dependent UE method contains both semantic information (the blue area) and various kinds of noise (the gray area). (B) Our method extracts the shared common semantic information across diverse scenarios, thus alleviating the influence of scenario-related noise (the green and yellow area) on UE.

on bilingual QA datasets to more effectively detect hallucinations in LLMs' generated answers. These supervised approaches require additional training data and computational resources. They are also sensitive to distribution shifts.

#### 2.2.2. Unsupervised Approach for Uncertainty Estimation

Unsupervised methods do not require additional training and typically rely on internal signals from the LLM (Chen et al., 2017; Chen et al., 2018; Chen et al., 2019; Zhang et al., 2019; Zhang et al., 2019) to estimate the certainty of its outputs. Researchers explore various strategies using entropy (Zhu et al., 2019), similarity (Zhu et al., 2019; Zhang et al., 2019), semantic features (Chen et al., 2017; Chen et al., 2019; Zhang et al., 2019), and information from logits or hidden states (Chen et al., 2017; Chen et al., 2019; Zhang et al., 2019; Zhang et al., 2019) to derive uncertainty metrics (Zhu et al., 2019). Vazmhentsev et al. (2019) propose modeling the conditional dependency between multiple generation steps, adjusting the current generation step's uncertainty based on the previous step's uncertainty. Chuang et al. (2019) identify contextual hallucinations by analyzing the ratio of attention weights between context and generated tokens. Da et al. (2019) introduce a directional entailment graph and claim-level response augmentation for quantifying uncertainty, considering both semantic and logical directional information. Chen et al. (2017) leverage the internal states of LLMs to detect hallucinations using an EigenScore metric from the sentence embeddings of multiple responses. When dealing with black-box models, whose internal signals are inaccessible, researchers approximate these metrics by sampling multiple outputs from the LLMs (Zhu et al., 2019; Zhu et al., 2019; Zhu et al., 2019). Manakul et al. (2019) evaluate factual consistency among multiple sampled outputs. Zhang et al. (2019) enhance hallucination detection performance in commonsense QA by checking response consistency across different models.

Aside from UE, researchers also try to detect hallucination by observing LLM behavior (Sun et al., 2019; Zhang et al., 2019). Sun et al. (2019) propose a Markov Chain-based multi-agent debate framework to improve hallucination detection accuracy. Cohen et al. (2017) leverage interactions between two LLMs where an "examiner" LLM questions an "examiner" LLM, aiming to unveil inconsistencies of responses through multi-round conversations. While some previous work also employs query paraphrasing (Zhu et al., 2019), they overlook the influence of scenario information on the hallucination detection results.

Our approach belongs to unsupervised UE methods. We explicitly consider diverse scenario information during paraphrasing and construct a factor analysis model to eliminate scenario-related noise, thus achieving a more accurate uncertainty estimation.

## 3. Method

### Overview

We first construct a unified framework to identify the impact of scenario information on uncertainty estimation (UE) methods and then implement our algorithm. Our framework comprises two modules (see Figure 2). First, we perform scenario-specific paraphrase sampling of the query, using a scenario-dependent UE method to obtain scenario-dependent UE scores. Then, we propose a factor analysis (FA) model to achieve scenario-independent UE by disentangling and decomposing the contribution of common semantics from scenario-specific noise.

### Plug-and-play Scenario-independent Uncertainty Estimation Framework

We propose a scenario-independent UE framework to identify the impact of scenario information on traditional UE and then mitigate its effect. Traditional UE methods \(g(x,y)\) predict the probability of an LLM's answer \(y\) being correct to the query \(x\). However, when using any lexical representation \(x\) to express the semantic \(s\) of a query, the choice of lexical resources in \(x\) is inherently influenced by scenario information \(c\): \(x\sim p(x\mid c,s)\). Here, \(c\) is associated with the background in which \(x\) is posed (e.g., social media, academic conferences, etc.). Note that users typically care about the uncertainty of the semantic \(s\) rather than the uncertainty of \(c\). However, existing (scenario-dependent) UE suffers from the noise brought by this scenario-related noise (we observe that scenario information has a significant impact on existing UE algorithms in Sec. 4.3).

Therefore, we propose a plug-and-play framework \(G(C,g(x,y))\) to achieve scenario-independent UE, where \(C=\{c_{1},\ldots,c_{m}\}\) denotes a set of different scenarios. We first employ a scoring function for an LLM-based task (e.g. QA) that evaluates the quality of the generated response \(f(\cdot,\cdot):\mathcal{Y}\times\mathcal{Y}\rightarrow[0,1]\). For each pair of \((x,y)\), the evaluation function rates the response with the score \(f(y_{true},y)\), where \(y\) is the LLM response for the query \(x\) and \(y_{true}\) is the ground truth. A larger score represents a more reliable answer. \(G\) approximates the value of \(f(y^{\text{true}},y)\) without relying on the ground truth \(y^{\text{true}}\), as shown in Eq. 1. The goal of \(G\) is to minimize the prediction error relative to the true evaluation function, considering the scenarios \(C\) and UE's output \(g(x,y)\):

\[G(C,g(x,y))\approx f(y^{\text{true}},y), \tag{1}\]

\[\min_{G}\mathbb{E}_{(x,y,y^{\text{true}})\sim\mathcal{D}}\left\|G(C,g(x,y))-f( y^{\text{true}},y)\right\|_{2}^{2}. \tag{2}\]

Our framework \(G\) can build upon any scenario-dependent UE method in a plug-and-play fashion because it is agnostic to the implementation of \(g(x,y)\), relying solely on its output. \(G\) considers multiple possible scenarios for a given QA pair to remove the scenario-related noise and achieve a more reliable UE. We realize \(G\) is in Sec. 3.3 and Sec. 3.4.

### Scenario-specific Sampling for Query Paraphrases

We conduct a scenario-specific sampling to diversify the scenario distribution among paraphrases while retaining the query's semantics. Each paraphrased sentence corresponds to each scenario. Specifically, we instruct GPT-40-mini (He et al., 2017) to generate various scenario-specific paraphrases of the query. First, we pre-define \(m\) diverse scenarios based on various real-world situations, ensuring a significant divergence between them, such as chatting on social media and reporting at academic conferences, etc. To ensure that each synthesized paraphrase \(\hat{x}_{i}\) retains the semantic meaning of the original query \(x\) while only altering information related to the corresponding scenario \(c_{i}\), we explicitly constrain this in the instruction:

\[\hat{x}_{i}\sim P_{LLM}(\hat{x}_{i}|x,c_{i}),\quad\forall i\in\{1,2,\ldots,m\} \tag{3}\]Our instruction template is: _You need to rewrite the provided sentences into the scenario: [SCENARIO]. Here are three examples: [EXAMPLES]. You need to paraphrase this sentence: [SENTENCE]. "[TEXAMPLES]"_ represents the few-shot examples and response format we show, "[SENTENCE]" represents the given query that needs to be paraphrased, and "[SCENARIO]" is the scenario for paraphrasing.

Finally, for each sample \((x,y)\) in the dataset \(\mathcal{D}=\{(x_{i},y_{i}^{\text{true}})\mid i=1,2,\ldots,N\}\), we instruct the LLM to paraphrase the input query \(x\) based on each scenario, resulting in a set of scenario-specific paraphrases \(\hat{X}=\{\hat{x}_{1},\ldots,\hat{x}_{m}\}\). By maintaining the semantics during scenario-specific sampling, we ensure that the underlying semantics are the core of the information they have in common. Besides, by diversifying the scenario backgrounds, we reduce the similarity of different types of scenario-related noise information among paraphrases, thereby making it easier to separate them from the core semantic information (see demonstration in Fig 1).

### Disentanglement of Common Semantic via Factor Analysis

Based on factor analysis, our method disentangles the common semantics and scenario-related noise information within UE scores. Factor analysis is a statistical technique that identifies underlying latent variables or factors. These factors explain the relationships among observed variables by expressing them as linear combinations of multiple latent variables. As we maintain the same semantics while making the noise more dissimilar in scenario-specific sampling, the contribution of the shared semantics in the common factor is larger than that of common noise. Therefore, it is natural to assume that these paraphrases' most significant common factor is their shared semantics. Consequently, by disentangling the proportion of the most influential factor in uncertainty scores from others, we can distill UE to concentrate on semantics.

Specifically, our algorithm consists of three stages. We first conduct scenario-dependent UE for paraphrases obtained in Sec. 3.3 and then disentangle the UE scores into multiple latent factors. Finally, we decompose the contribution of semantic information from scenario-related noise to achieve scenario-independent UE.

1. **Uncertainty estimation for paraphrases.** We first feed the query \(x\) into the target LLM for an answer \(y\). We then apply a scenario-dependent (traditional) UE algorithm \(g(x,y)\) for each group of scenario-specific paraphrases \(\{(\hat{x}_{i},y)|\hat{x}_{i}\in\hat{X}\}\), to obtain their corresponding UE scores, which we organize into a vector: \(u=[g(\hat{x}_{1},y),\ldots,g(\hat{x}_{m},y)]^{T}\). This process allows us to compile a sample matrix \(U_{m\times n}=[u_{1},\ldots,u_{n}]\), where \(n\) is the number of data samples and \(m\) is the number of scenarios.
2. **Disentanglement of common factors.** We construct a factor analysis model \(U=A_{m\times m}F_{m\times n}\) to disentangle the uncertainty scores into a linear combination of a series of common factors. We denote \(F=[f_{1},\ldots,f_{m}]^{T}\), where each \(f_{i}\) describes a common factor's scores across \(n\) samples. The columns of loading matrix \(A\) represents the loading weights of common factors, and their weighted sum yields the uncertainty scores \(U\) under different scenarios. The larger the weights in \(A\), the more its corresponding factor contributes to UE. It consists of the following two steps:
3. **Calculating the loading matrix \(A\).** We first consider the UE scores across these \(m\) scenarios as the observed variables \(U\) and compute its covariance matrix: \[R_{m\times m}=\frac{1}{n-1}UU^{T},\] (3)

Figure 2. The overview of our framework. For a query \(x\) with an answer \(y\) given by the target model, GPT-4 paraphrases \(x\) considering 3 different scenarios, resulting in QA pairs \((\hat{X},y)\). Given any scenario-dependent UE algorithm \(g\), our framework disentangles its output sample matrix \(U\) into multiple common factors (the row vectors of \(F\)). Then, we decompose the contribution of common semantic information (the blue row vector) from noise information (green and yellow vectors) to produce the final UE score.

which measures the pairwise correlation between each scenario. Inspired by the PCA algorithm that reduces the dimensionality of the original data using eigen decomposition, we perform spectral decomposition (Spiegel et al., 2016) on the sample covariance matrix \(R\) to obtain the loading matrix \(A\): \(R=A_{m\times m}A_{m\times m}^{T}\), where \(A=[\sqrt{A_{1}}\epsilon_{1},\ldots,\sqrt{A_{m}}\epsilon_{m}]\) (eigenvalues \(\lambda s\) are arranged in descending order corresponding to the \(m\) common factors from most to least influential). The advantage of using the FA model is that it allows us to preserve the correlation information among UE scores from different perspectives while ensuring that different factors are orthogonal. Besides, FA helps rank the contribution of obtained factors, with larger eigenvalues in \(A\) indicating greater importance of corresponding factors in \(F\), thereby extracting the most significant information from \(U\), i.e. common semantics.
* **Calculating the common factors matrix \(F\).** Since the loading matrix \(A\) is invertible (full rank), we derive the common factor matrix \(F\) according to our FA model \(U=AF\) as follows: \[F=A^{-1}U.\] (4) In this way, we map the original UE scores to common factors of varying importance based on their eigenvalues. The value of each \(f\) quantifies the amount of common information across all scenarios from different perspectives. The larger the eigenvalue corresponding to \(f\), the more critical \(f\) is in describing the shared information among paraphrases.
* **Decomposition of common semantic information.** We consider the common factor \(f_{1}\) (i.e. the first row vector in \(F\)), corresponding to the largest eigenvalue, as representing the shared information across all sampled paraphrases for UE. Since the eigenvalue quantifies the contribution of common factors, and common semantics is the most significant factor affecting different paraphrases, \(f_{1}\) can be used to measure the contribution of common semantics to the UE scores. Below, we decompose the common semantic information based on \(f_{1}\) as shown in Eq.5. Specifically, we decompose the matrix \(U\) into a product of column vectors of \(A\) and row vectors of \(F\). Further, We consider \(U\) to comprise two components: (1) the product of common semantic information \(f_{1}\) with its corresponding loading weights \(\sqrt{A_{1}}\epsilon_{1}\) that quantifies the proportion of \(f_{1}\) under different scenarios; (2) the non-semantic noise information \(\Phi\) that is unhelpful in UE. \[U =\sum_{i}^{m}A_{(i,i)}F_{(i,i)}\] \[=A_{(i,1)}F_{(i,i)}+A_{(i,2m)}F_{(2m,i)}\] \[=\sqrt{A_{1}}\epsilon_{1}f_{1}^{T}+\Phi.\] (5) In this way, we derive the scenario-independent UE score \(f_{1}\), which mainly considers the semantic information for each sample, to evaluate the quality of the model's responses.

Our framework can be applied in a plug-and-play manner to any unsupervised UE method. In our framework, we first apply scenario-specific sampling on the query across diverse scenarios (Sec.3.3). Next, we estimate the uncertainty for these paraphrases with arbitrary scenario-dependent UE algorithms in a plug-and-play fashion (the first step in Sec.3.4). Based on factor analysis, we then disentangle the UE scores across different scenarios into a combination of multiple common factors with varying weights (the second step in Sec.3.4). Finally, we decompose the uncertainty information related to semantics (the one with the largest eigenvalue) from scenario-related noise information (the final step in Sec.3.4). This ultimately achieves scenario-independent UE.

## 4. Experiments

### Experimental Setting

**Implementation Details.** We conduct our experiments on a single A100 40GB GPU, using LLaMA-3 (LeCun et al., 2015) 8B model and Qwen (Qwen, 2019) models (0.5B, 1.5B, and 7B) as the target L1Ms for uncertainty estimation. We set the number of scenarios \(m\) as 6 and ask GPT-to-mini to paraphrase the query based on the following scenarios: Chat, Academic, Research, Report, Podcast, Informal. See detailed explanations for each scenario in App. E.1. In our experiments, we use greedy decoding for the target L1M to generate the most probable answers and compare them with the gold answers to evaluate their correctness. For different UE applied in our method, we keep other hyperparameters consistent with those in their original papers. In inference, we apply 5-shot prompting for the target L1M to answer the given question. We select the first 5 samples from each dataset as demonstrations, which are not included in the testing set. See details in App. F.

**Dataset.** We apply our method on two datasets: EntityQuestions (Yang et al., 2015) and TriviaQA (Yang et al., 2015), to evaluate its performance in enhancing UE on QA tasks. EntityQuestions is a closed-book QA dataset comprising approximately 221k QA pairs, with the test set consisting of around 22k samples. The dataset consists of 24 sub-sets covering various knowledge types, such as places of birth, city locations, and music genres. The gold answers are typically unique and consist of a single word or phrase3. We evaluate our method on the test set. TriviaQA is an open-book QA dataset primarily derived from Wikipedia and the Web, containing about 95k samples. Following Kuhn et al. (Kuhn et al., 2019) and Welble et al. (Welble et al., 2019), we evaluate our approach on the validation set with approximately 17k samples. The dataset typically contains gold answers in the form of a word, phrase, or short sentence, along with aliases that are also considered correct4.

Footnote 3: For example, for question “Who is Birgit Rosengren married to?”, the answer is “Elaf Abile”.

Footnote 4: For example, for the question “In which country was the inventor of the machine gun Hiram Maxim bom?”, gold answers include “America”, “the US?, and “the USA?”, “Jungging@nce.com/mercost/dibert-large-mall

**Metric.** We adopt the widely adopted **AUROC**(Zhou et al., 2019; Zhang et al., 2019) to evaluate the performance of our method in measuring L1M uncertainty. See its calculation details in App. A. Following the common practice (Zhou et al., 2019; Zhang et al., 2019; Zhang et al., 2019), we apply a DeBERTa-based (Liang et al., 2019) natural language inference (NLI) model5 to evaluate whether the L1M response aligns with the gold answer. It categorizes the relationship between two sentences as either entailment, neutrality, or contradiction. We consider an L1M response to align with the gold answer if it entails the gold answer and vice versa.

Footnote 5: For example, for question “Who is Birgit Rosengren married to?”, the answer is “Elaf Abile”.

**Baseline.** Our baselines consist of two parts. The first part consists of existing scenario-dependent UE algorithms that calculate scenario-dependent UE scores. The second part contains baselines we use to integrate the obtained UE scores. we compare our method with the second part to demonstrate its performance.

#### Scenario-dependent UE Baselines

We apply our method to the following diverse scenario-dependent UE algorithms to evaluate our effectiveness in augmenting their UE reliability. (1) Inspired by Kadavath et al. (2019), **P (True)** directly asks the LLM to output the probability of whether the QA pair is true. (2) Following perplexity-based approaches, **Answer PPL**(Kadavath et al., 2019) estimates the model's uncertainty on the query by calculating the perplexity of the response generated by the LLM. In addition, **PE-UE**(Kadavath et al., 2019) (predict entropy) samples multiple times to estimate the average perplexity instead of calculating by the greedy decoding response like PPL ("unnormalized" means not normalizing based on the token length of the output answer). (3) Following Kuhn et al. (2018), **SE-UE** (semantic entropy) clusters the LLM's responses and calculates the probability of each cluster for UE ("unnormalized" means not normalizing based on the token length of the output answer). (4) Following Zhao et al. (2018), we use **Self-Detect** to verify our method is also suitable for black-box UE methods. It estimates the uncertainty by considering the diversity of responses.

#### Integrated Baselines

We compare our method with other baselines by integrating UE scores from scenario-dependent methods across different scenarios to validate our method's efficiency. (1) **Mean.** Inspired by Kadavath et al. (2019), we calculate the mean of uncertainty scores for different scenarios for a given scenario-dependent UE algorithm. (2) **PPL Re-weighting.** Inspired by Zhang and Wu (2019), we calculate the final UE score by re-weighting and summing the UE scores of different styles based on the perplexity of the query under each scenario. (3) Inspired by Chan et al. (2018), the patterns extracted by FA may not necessarily be the most helpful for UE (Yang et al., 2019; Wang et al., 2019). Thus, we compare our method with two similar dimensionality reduction algorithms for pattern extraction: 1) Multidimensional Scaling (**MDS**) (Sandvikumar et al., 2019) aims to extract the Euclidean distances among all samples and 2) Isometric Mapping (**Isomap**) (Wang et al., 2019), which preserves the geodesic distances (curved surface distances) information.

### Overall Performance

We analyze the effectiveness of our method in LLM uncertainty estimation for QA tasks by comparing it with multiple baselines on LLaMA 3 8B model (Han et al., 2018). In addition, we evaluate Self-Detect's performance through GPT-40-mini (Krishnan et al., 2019) to verify that our method also applies to black-box LLMs. First, we use GPT-40-mini (Krishnan et al., 2019) to generate six additional queries in different scenarios. Then, we select the answer generated by the target LLM and use the NLI model to evaluate their correctness. Subsequently, we estimate the UE scores for paraphrases with various scenario-dependent UE baselines outlined in Sec. 4.1. Finally, we integrate the UE scores using our method alongside other baselines in Sec. 4.1 to obtain the final UE scores and evaluate their performance using AUROC. Table 1 shows the integration performance of different scenario-dependent UE methods under diverse scenarios using various integration methods.

Mean directly uses the average of UE scores for all scenarios, resulting in the worst performance, except for the baselines related to dimensionality reduction. It suggests that averaging UE scores across different scenarios does not effectively eliminate noise and then reveal the common semantics. Answer PPL uses query perplexities to re-weight UE scores. It shows a significant improvement compared to Mean, suggesting that perplexity can evaluate noise intensity by measuring sentence fluency. However, Answer PPL only measures individual sentences' perplexity while failing to adequately analyze the correlations among queries across different scenarios. This insufficiency in removing noise may account for its relative underperformance compared to our method.

Our method disentangles semantic information from scenario-related noise via FA and achieves the SOTA performance across all evaluation datasets and baselines (see examples in App. E). MDS and Isomap aim to reduce the UE scores across all scenarios to one dimension by minimizing changes in Euclidean and geodesic distances, respectively. Both methods yield the poorest results across almost all baselines, indicating that without a proper perspective, merely reducing dimensions is insufficient to derive semantic information to help UE. In addition, we also observe that Isomap performs significantly better than MDS in several baselines, which suggests that different scenarios of UE scores may exhibit manifold characteristics rather than linear correlations in high-dimensional space, which is counterintuitive. Furthermore, our method demonstrates more remarkable improvements in baselines that utilize the

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \multirow{2}{*}{Dataset} & \multirow{2}{*}{Method} & \multicolumn{6}{c}{**UE for White-box LLM**} & \multicolumn{2}{c}{**UE for Black-box LLM**} \\ \cline{3-10}  & & **Answer** & **P (True)** & **SE-UE** & **SE-UE** & **SE-UE** & **PE-UE** & **PE-UE** & **PE-UE** \\  & & **PPL** & **P (True)** & **SE-UE** & **(unnormalized)** & **PE-UE** & **(unnormalized)** & **Self-Detect** \\ \hline \multirow{50}{*}{EntityQuestions} & **Mean** & 0.779 & 0.705 & 0.787 & 0.783 & 0.789 & 0.825 & 0.668 & \\  & **PPL Re-weighting** & 0.787 & 0.743 & 0.825 & 0.821 & 0.797 & 0.835 & 0.726 & \\  & **MDS** & 0.549 & 0.575 & 0.512 & 0.585 & 0.519 & 0.529 & 0.514 & \\  & **Isomap** & 0.646 & 0.675 & 0.665 & 0.713 & 0.606 & 0.784 & 0.776 & \\  & **Our** & **0.797** & **0.778** & **0.842** & **0.839** & **0.807** & **0.844** & **0.836** & \\ \hline \multirow{50}{*}{TriviaQA} & **Mean** & 0.826 & 0.659 & 0.787 & 0.785 & 0.829 & 0.824 & 0.711 & \\  & **PPL Re-weighting** & 0.832 & 0.700 & 0.868 & 0.867 & 0.835 & 0.827 & 0.736 & \\ \cline{1-1}  & **MDS** & 0.558 & 0.534 & 0.519 & 0.532 & 0.565 & 0.532 & 0.537 & \\ \cline{1-1}  & **Isomap** & 0.547 & 0.527 & 0.710 & 0.535 & 0.561 & 0.715 & 0.822 & \\ \cline{1-1}  & **Our** & **0.846** & **0.762** & **0.884** & **0.882** & **0.848** & **0.837** & **0.836** & \\ \hline \end{tabular}
\end{table}
Table 1. The performance (AUROC) of different integration strategies with various scenario-dependent baselines. The second row shows the scenario-dependent baselines we use to calculate the UE score, and the second column displays the integrated baselines we compare with ours, the results in bold indicate the best-performing strategies. Our improvements are significant under the t-test with \(p<0.01\) (see details in App. C).

diversity of answers compared to others with approximately the same AUROC score. Specifically, SE-UE shows a more significant enhancement than both Answer PPL and PE-UE, while Self-Detect exhibits notable improvements over P (True).

### Ablation Study

We conduct an ablation study on our proposed method to verify each component's importance in improving uncertainty estimation accuracy (as shown in Table 2). - Scenario asks the model to paraphrase the original query under the same scenario and achieves suboptimal results in our experiment. Without the constraint of the scenario, the restated sentences are highly similar. Consequently, FA may erroneously interpret common noise as common semantics, thus risking the pollution of the distilled common semantics by such noise. Paraphrasing the original query into different scenarios reduces common noise and thus effectively enhances performance. In - FA, we randomly select a scenario's UE score for each test case instead of using the factor analysis model to estimate the common semantics. Compared to our full method, it integrates arbitrary scenario data and exhibits subpar performance. It indicates that scenario information harms UE but can be eliminated by our factor analysis model, thereby improving performance. In - Denoising, we select the common factors without the largest eigenvalue, which means that the patterns related to scenario noise are the major contributors to the final UE score. This approach receives the poorest results among all model variants, with AUROC values consistently ranging between 0.5 and 0.54. It indicates that these non-semantic common factors almost randomly distinguish between reliable and unreliable LLM responses. It aligns with our expectation of noise behavior and our belief that, unlike common semantic information, scenario-related noise information does not aid UE.

### Analysis of the Selection of Common Factor

We analyze the correlation between common factors and scenario-dependent UE score to verify that the common factor corresponding to the largest eigenvalue can approximate the contribution of semantics while the other factors are noise. We use box plots to demonstrate the correlation coefficients according to the order of eigenvalue size. As Figure 3 shows, the first common factor correlates strongly with the original UE results across multiple scenarios, with correlation coefficients generally exceeding 0.8. This suggests it captures the most prevalent information among paraphrases, i.e., the common semantics (refer to Sec. 3). Excluding the factor corresponding to the largest eigenvalue, the Pearson correlation coefficients of the remaining factors with the original scenario-dependent UE scores are mostly under 0.4, which indicates a weak correlation between non-first common factors and the original data. The characteristics of the remaining factors align with scenario noise, echoing the findings in Sec. 4.3 (the poor performance of - Denoising) that the common factors corresponding to noise could hardly distinguish between correct and incorrect answers.

### Uncertainty Estimation with Different Numbers of Scenarios

To study the impact of the number of scenarios on our method, we adjust the number of additional scenarios \(m\) in our framework from 1 to 6 and conduct experiments following the steps in Sec 4.2. As in Figure 4, the improvement of our method on all scenario-dependent UE algorithms exhibits a positive correlation with the number of scenarios. The improvement is particularly noticeable in cases with few scenario samples, where adding just one or two scenarios can substantially enhance performance.

Figure 3. The correlation between the common factors with each scenario-dependent UE baseline. The horizontal axis represents the descending order of eigenvalue ranks, while the vertical axis displays the correlation value.

### Analysis of Scenario-related Noise

As the latent factors do not correspond to our pre-defined scenarios one-to-one, we use K-Means to cluster noise-related factors and analyze their patterns. Specifically, we normalize the loading weights (the columns of \(A\)) for each factor which denote each scenario's influence, and apply the K-Means algorithm (Krishna et al., 2017) to group factors into 3 clusters (see the reasons for our choice of cluster number in App. D). We demonstrate the average loading weights for each scenario in each cluster in Figure 5.

The loading weights indicate the significance of noise in each scenario so that we can infer their causes through their distribution. For Cluster 1, Chat and Informal scenarios exhibit strong positive correlations. In contrast, all formal scenarios show negative correlations, suggesting that the presence or absence of formality may be a major noise source. In Cluster 2, informal scenarios generally have a mild impact, while Research and Report display a strong opposite impact, indicating that the noise may primarily arise from variations of written styles within formal expressions. In contrast, formal scenarios in Cluster 3 have low weights and informal scenarios strongly affect UE in mixed directions. It implies that differences within informal scenarios are the main reason for noise in Cluster 3. Overall, the clustering of noise reveals that scenario noise originates from the presence or absence of formality, writing styles within formal expressions, and differences in informal scenarios.

### Analysis of Robustness Across Model Families and Parameter Scales

We conduct experiments on the Owen2 model family to verify our method's robustness across different model families and parameter scales (see results in Table 3). We enhance scenario-dependent UE baselines with our method on LLM with 0.5B, 1.5B, and 7B parameters respectively. Our approach significantly improves across all model scales and scenario-dependent UE baselines, demonstrating its strong generalization ability.

## 5. Conclusion

We propose a plug-and-play unsupervised method to enhance uncertainty estimation (UE) by eliminating scenario-related noise and focusing on semantic information. We perform scenario-specific sampling, which rephrases each query into various stylistic paraphrases, capturing a range of expressions for the same underlying semantics. We then design a factor analysis model to decompose the original UE scores into multiple latent factors. By isolating the most significant factor, we disentangle the uncertainty caused by common semantics from scenario-related noise. Experiments on multiple models and datasets shows the effectiveness of our method, improving the reliability of existing UE methods.

Figure 4. Our performance on two datasets increases with the number of additional scenarios. We change the number of additional scenarios and measure the changes in our method’s performance. The horizontal axis shows the number of additional scenarios, and the vertical axis shows AUROC.

Figure 5. Average noise weight in clusters. We conduct K-means clustering on loading weights and average them in different clusters. The horizontal axis shows the cluster ID, and the vertical axis shows the average noise weight for each scenario in different clusters. The degree of formality or informality of a scenario increases in proportion to the intensity of blue or red, respectively.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & \multirow{2}{*}{Model Size} & **Answer** & **P (True)** & **SE-UE** & \begin{tabular}{c} **SE-UE** \\ **(unnormalized)** \\ \end{tabular} & **PE-UE** & 
\begin{tabular}{c} **PE-UE** \\ **(unnormalized)** \\ \end{tabular} \\ \hline \multirow{3}{*}{EntityQuestions} & 0.5B & 0.742 (+0.013) & 0.807 (+0.078) & 0.835 (+0.038) & 0.817 (+0.041) & 0.762 (+0.013) & 0.804 (+0.010) \\  & 1.5B & 0.780 (+0.015) & 0.761 (+0.060) & 0.841 (+0.065) & 0.805 (+0.075) & 0.781 (+0.030) & 0.763 (+0.046) \\  & 7B & 0.777 (+0.010) & 0.824 (+0.070) & 0.862 (+0.049) & 0.843 (+0.055) & 0.785 (+0.022) & 0.806 (+0.043) \\ \hline \multirow{3}{*}{TriviaQA} & 0.5B & 0.666 (+0.013) & 0.578 (+0.037) & 0.773 (+0.059) & 0.741 (+0.063) & 0.657 (+0.020) & 0.630 (+0.024) \\  & 1.5B & 0.578 (+0.005) & 0.617 (+0.022) & 0.685 (+0.044) & 0.667 (+0.044) & 0.578 (+0.010) & 0.555 (+0.010) \\  & 7B & 0.834 (+0.020) & 0.821 (+0.079) & 0.885 (+0.048) & 0.883 (+0.054) & 0.830 (+0.031) & 0.786 (+0.037) \\ \hline \hline \end{tabular}
\end{table}
Table 3. The performance of Owen2 models with different parameter scales. The numbers in parentheses indicate the improvement of our method over the Mean baseline.

## References

* (1)
* Amayvelas et al. (2020) Alfonso Amayvelas, Kyle Wong, Liangming Pan, Wenhu Chen, and William Yang Wang. 2020. Knowledge of Knowledge: Exploiting Known-Unknown Uncertainty with Large Language Models. In _Findings of ACL_, Jun-Fei Ku, Andrea Martins, and Vivek Sikiramur (Eds.). Association for Computational Linguistics, Bangkok, Thailand, and virtual meeting. [https://doi.org/10.18653/v1/2024a](https://doi.org/10.18653/v1/2024a) findings-sd-383
* Bai et al. (2019) Junze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenhu Ge, Yuen Fu, Feng Huang, Suyuan Liu, Jo, Mei Li, Luyungang Jin, Luin Liu, Deying Liu, Gao Li, Mengyang Lu, Feng Huang, Lien Liang, Juanna N, Rui Ren, Xianglong Ren, Yuanheng Ren, Chaunqi Tang, Yanlong Tai, Jianlong Tu, Peng Wang, Xingjie Wang, Wei Wang, Shengguang Wang, Wei Zhang, Yu, Xu Fang, Hao Yang, Jian Yang, Shu Shuang Tang, Yang Luo, Zhuang Yu, Hangyu Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhern Zhang, Cheng Zhou, Jingzen Zhou, Xueqhan Zhou, and Tianhang Zhu. 2020. Open Technical Report. arXiv:2009.16009.160015. [https://arxiv.org/abs/2009.16009](https://arxiv.org/abs/2009.16009)
* Bartlett (1997) M. S. Bartlett. 1997. Properties of Subjective and Statistical Tests. _Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences_ 160, 901 (1997), 268-282. [http://www.jstor.org/stable/960438](http://www.jstor.org/stable/960438)
* Chan et al. (2015) Tsung-Han Chan, Kui Jia, Shenghua Gao, Jiwen Lu, Zizan Zeng, and Yi Ma. 2015. PCANet: A Simple Deep Learning Baseline for Image Classification? _IEEE transactions on image processing_ 12, 24 (2015), 5017-5032.
* Chen et al. (2020) Chao Chen, Kai Luo, Zie Chen, Yi Ma, Yue Wu, Mingyuan Zhou, Zhang Fu, and Jieping Ye. 2020. INSIDE: LI-IM: Internal States Beta from the Power of Hallatation Detection. In _ICLR_. [https://openreview.net/forum?id=2j1an20ba](https://openreview.net/forum?id=2j1an20ba)
* Chen and Shao (2024) Caroyu Chen and Kai Niu. 2024. Can LI-M: Concerted Misinformation Re-Detected? In _The Twelfth International Conference on Learning Representations_. [https://openreview.net/forum?id=2024b-cm2017](https://openreview.net/forum?id=2024b-cm2017)
* Chen et al. (2024) Jifan Chen, Grace Kim, Animudhir, Stefan, Greg Durrett, and Eunsol Choi. 2024. Complex Claim Verification with Evidence Retrieved in the Wild. In _NAACL_, Kevin Duh, Helen Gomez, and Steven Bethard (Eds.). Association for Computational Linguistics, Mexico City, Mexico, [https://doi.org/10.18653/v1/2024a-mac-196](https://doi.org/10.18653/v1/2024a-mac-196)
* Chen et al. (2023) Yuyan Chen, Qiang Fu, Yixen Yuan, Zhihao Wen, Ge Fan, Dayching Liu, Donghui Zhang, Zhexu Li, and Yanghua Xiao. 2023. Hallacification Detection: Robustly Discerving Reliable Answers in Large Language Models. In _Proceedings of the 33rd ACM International Conference on Information and Knowledge Management_ (Hinghamton, United Kingdom) (CIKM). Association for Computing Machinery, New York, NY, USA, 2545-2555. [https://doi.org/10.1145/357890861689](https://doi.org/10.1145/357890861689)
* Cheng et al. (2024) Xiaoxone Cheng, Junyi Li, Wayne Xin Zhao, Hongbin Zhang, Fuheng Zhang, Di Jin Zhang, Kun Qi, and Ji-Rong Wen. 2024. Small Agent Can Also Rock: Enpowering Small Language Models as Hallatation Detector. [https://arxiv.org/abs/2406.11277](https://arxiv.org/abs/2406.11277)
* Chen et al. (2023) J-Chun Chen, Steffi Chen, Shiq Chen, Weizie Yuan, Kehua Feng, Chunting Zhou, Junxin He, Graham Neubig, Pengfei Liu, et al. 2023. FactFact: Factuality Detection in Generative AI-A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios. arXiv preprint arXiv:2301.155823 (2023).
* Chuang et al. (2024) Yun-Sung Chuang, Linib Qiu, Cheng-Yi Hsieh, Raniyi Krishna, Yoon Kim, and James Glass. 2024. Lookback: Lems: Detecting and Mitigating Confucental Hallacinations in Large Language Models Using Only Attention Maps. _arXiv preprint arXiv:2407.00710_ (2024).
* Cohen et al. (2023) Boi Cohen, Mary Hamri, Nor Geva, and Amir Globerson. 2023. LM vs LM: Detecting Factual Errors via Cross Examination. In _EMNLP_, Houla Boumon, Jun Pinno, and Kaila Bali (Eds.). Association for Computational Linguistics, Singapore. [https://doi.org/10.18653/v1/2023-emnlp-rnn.778](https://doi.org/10.18653/v1/2023-emnlp-rnn.778)
* Dingchao et al. (2020) Lingchao Dingchao Dingchao Dingchao Dingchao Dingchaoo Dingchaoo Dragt, Alexander Leng, and Hui Wei. 2020. LIMI Uncertainty Quantification through Incremental Entangle Graph and Claim Level Response Augmentation. arXiv:2009.00945 (2020). [https://arxiv.org/abs/2009.00945](https://arxiv.org/abs/2009.00945)
* Duan et al. (2024) Jinhao Duan, Hao Cheng, Shiq Wang, Zachary, Chuan Wang, Renjing Xu, and Bradley Kailhu Xu. 2024. Shifting Attention to Relevance: Towards the Predictive Uncertainty Quantification of Free-Form Large Language Models. In _ACL_. 5050-5063.
* Dubey et al. (2017) Abhimany Dubey, Abhime Joshi, Abhime Pandey, Abhishek Kadian, Abhimei, and Ah-Dahlei. Lukas Leitman, Akhilar Matin, Abhimei, Amy Yang, Angola Fan, Anirudhir Goyal, Anthony Harbou, Abo Yang, Archi Mitra, Archie Sravanzum, Artem Koreva, Arthur Hinswar, Arun Rao, Aaron Zhang, Aurelien Georgosu, Arun Spatran, Baptiste Roziee, Behrmann Byron, Binh Tang, Bobie Chern, Charlotte Cavalcott, Chavehaya Nayak, Chio E. Gixia, Mirza, Chris McCombell, Christian Keller, Christophe Tuquet, Chunyang Wu, Couine Wang, Gristian Canton Ferrer, Cyrus Van Bikolulu, Danim Altosian, Daniel Song, Damille Pirtz, Danny Liakhti, David Eishou, Dhruv Chowdhury, Dhruv Mahjani, Diego Garcia-Olm, Diego Perino, Ibarpreu Hughes, Egor Iakomkin, Ethab Alikanday, Elina Lobanov, Emily Dinan, Eric Michel Smith, Utip Edunovic, and Kenneth Zhang. Gabriel Synagure, Gabriele, Georgia Lewis Anderson, Graeme Nail, Nail, Gregoire Mihai, Guan Pang, Guillem Cucurrell, Hanley Nguyen, Hamah Koreva, He Xu, Hugo Touytron, Ilgam Zarveg, Imand Arteira Barra, label Kloumann, Itham Misra, Ivan Primov, Jadre Coper, Jaeon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Joet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenny Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecca Yu, Joanna Botto, Spejs, Yongboo Park, Joseph Brocca, Joshua Johnston, Joshua Sauce, Jintang Jia, Kaivan Vasudevan, Alexei Alyuk, Karifayu Upusan, Kate Pluwak, Ke Li, Kenneth Henfield, Kevin Stone, Khalid El-Auria, Karithia Iyek, Kethiaiie Ng, Kitize Mikik, Kureley Chix, Kunah Bhalla, Laurence Pantula-Yeury, Laurensen, Laurence Chen, Liang Tan, Linjie, Lewis Loustin Martin, Lavish Madan, Labo Mola, Lukas Bleeker, Lukas Landmark, Luke de Oliveira, Madeirair, Munzal Khayuki, Manuel Pasupat, Mamat Singh, Manohar Paluri, Marcin Karakis, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melvin Schmidt, Mike Lewis, Milek Lewis, Milek Nayak, Nikolov Zagobeyev, Nikolov Zagobeyev, Niklind Carter, Olivier Duchene, Qour Celebi, Patrick Aksay, Pengchiora Zhang, Pengwei Li, Erett Vasic, Peter Weng, Prajwal Bhargava, Parikh Dari, Jem Foxen Krishnan, Philip Smith Koum, Pus Xu, Ping, Qing, Qixinga Dong, Raguvin Srinivasan, Raj Gangampathy, Ramon Calder, Ricott Silveira, Cabbert Siqueira, Robert Raleiman, Robin Tardini, Roman Saurevich, Roman Schroter, Roman Spinrad, Roman Taylor, Rusu Sink, Rui Hou, Rusu Wang, Sagar Saghar, Kishin Sinan, Chuan Chenahbasp, Samiy Singh, Sean Bell, Soubiran Kim, Sergey Eshovin, Shoham, Shoham Nie, Sharan Narang, Sharath Rapapathy, Sheng Shen, Shengyu Sun, Shmut Bisokale, Shina Zhang, Simon Vanhlemken, Soupiya Matu, Spencer Wilburn, Siene Sotil, Stephane Collot, Suchin Cortiau, Gurishang, Sydney Brodov, Timar Herman, Tara Fowler, Tarek Sheanda, Thomas Georgiou, Thomas Salidon, Tobias Speichakher, Mehrozy, Tong Xux, Qiwei Chen, Yoshiyuki Oh, Yoshi-Yuepita, Yogesh Ramamantam, Viktor Kerker, Vincent Gongler, Virginie Do, Yeh Vu-Egti, Vladu Petrov, Weirciew, Mveni Cheh, Kwang Kwang Weiyn, P.W. Withers, Merer, Xavier Martinet, Xiaodong Wang, Xiaoqing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xueqwen Swang, Yaele Gidasliang, Yahui Garr, Yanme Babaei, Yi Wen, Yuen Yuen Song, Yuchen Zhang, Yueli Yu, Yuen Yuen Song, Yuchen Zhang, Yueli Yuen Yuen, Yuen Yuen, Yuen Yuen Yuen, Yuen Yuen Yuen, Yuen Yuen Yuen, Yuen Yuen Yuen, Yuen Yuen, Yuen Yuen Yuen, Yuen Yuen Yuen, Yuen Yuen, Yuen Yuen, Yuen Yuen, Yuen Yuen, Yuen Yuen, Yuen Yuen, Sharan Ramaswamy, Shuan Lindsay, Shuan Lindsay, Shuan Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shixu Shuang Zhang, Shuang Zhang, Shuang Zhang, Shuang Zhang, Shuang Zhang, Shuang Zhang, Shuang Zhang, Sheng Wang, Szehein Agarwal, Soji sujoyigbe, Sounith Chintala, Stephanie Nau, Stephane Chen, Steve Agarwal, Sojie Salvatore,

[MISSING_PAGE_FAIL:10]

[MISSING_PAGE_FAIL:11]

relatively fewer clusters while guaranteeing the reliability of the clustering results. Consequently, we set the cluster number to 3 as the silhouette coefficient's growth began to decelerate to achieve a trade-off between K-means performance explainability.

## Appendix E Case Study

### Scenarios Interpret

We provide specific cases to illustrate the meaning of each scenario in Table 6.

### Cases of AUROC Change

As shown in Figure 7, we select a strong baseline (SE-UE) and a weak baseline (P (True)) to demonstrate the changes in the ROC

\begin{table}
\begin{tabular}{c c c c c c c c c c} \hline \hline Dataset & \multicolumn{2}{c}{**Answer**} & \multicolumn{2}{c}{**Example**} & \multicolumn{2}{c}{**Description**} & \multicolumn{2}{c}{**SE-UE**} & \multicolumn{2}{c}{**PE-UE**} & \multicolumn{2}{c}{**PE-UE**} & \multicolumn{2}{c}{**Self-Detect**} & \multicolumn{2}{c}{1380} \\
**Original** & \multicolumn{2}{c}{Which country was The Locker Room created in?} & \multicolumn{2}{c}{The original expression of the dataset} & \multicolumn{2}{c}{Do you know which country was The Locker Room created in?} & \multicolumn{2}{c}{The scenario of daily chat} & \multicolumn{2}{c}{1354} \\
**Academic** & \multicolumn{2}{c}{Please elaborate on the country that the Locker Room was created.} & \multicolumn{2}{c}{The scenario of academic writing} & \multicolumn{2}{c}{1358} \\
**Research** & \multicolumn{2}{c}{Which geographical region is associated with the creation of The Locker Room?} & \multicolumn{2}{c}{The scenario of academic discussion} & \multicolumn{2}{c}{1356} \\
**Report** & \multicolumn{2}{c}{The Locker Room was created in which country?} & \multicolumn{2}{c}{The scenario of written report} & \multicolumn{2}{c}{The scenario of chat in podcast} & \multicolumn{2}{c}{1359} \\
**Podcast** & \multicolumn{2}{c}{Can you tell me the country where The Locker Room was founded?} & \multicolumn{2}{c}{The scenario of chat in podcast} & \multicolumn{2}{c}{1358} \\
**Informal** & \multicolumn{2}{c}{What country is home to The Locker Room?} & \multicolumn{2}{c}{The scenario of informal chat} & \multicolumn{2}{c}{1359} \\

[MISSING_PAGE_FAIL:13]

[MISSING_PAGE_FAIL:14]