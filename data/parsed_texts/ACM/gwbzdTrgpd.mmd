# HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous Graph Neural Networks

Anonymous Author(s)

###### Abstract.

Graphs have emerged as a natural choice to represent and analyze the intricate patterns and rich information of the Web, enabling applications such as online page classification and social recommendation. The prevailing "_pre-train, fine-tune_" paradigm has been widely adopted in graph machine learning tasks, particularly in scenarios with limited labeled nodes. However, this approach often exhibits a misalignment between the training objectives of pre-text tasks and those of downstream tasks. This gap can result in the "negative transfer" problem, wherein the knowledge gained from pre-training adversely affects performance in the downstream tasks. The surge in prompt-based learning within Natural Language Processing (NLP) suggests the potential of adapting a "_pre-train, prompt_" paradigm to graphs as an alternative. However, existing graph prompting techniques are tailored to homogeneous graphs, neglecting the inherent heterogeneity of Web graphs. To bridge this gap, we propose HetGPT, a general post-training prompting framework to improve the predictive performance of pre-trained heterogeneous graph neural networks (HGNNs). The key is the design of a novel prompting function that integrates a virtual class prompt and a heterogeneous feature prompt, with the aim to reformulate downstream tasks to mirror pretext tasks. Moreover, HetGPT introduces a multi-view neighborhood aggregation mechanism, capturing the complex neighborhood structure in heterogeneous graphs. Extensive experiments on three benchmark datasets demonstrate HetGPT's capability to enhance the performance of state-of-the-art HGNNs on semi-supervised node classification.

## 1. Introduction

The Web, an ever-expanding digital universe, has transformed into an unparalleled data warehouse. Within this intricate web of data, encompassing diverse entities and patterns, graphs have risen as an intuitive representation to encapsulate and examine the Web's multifaceted content, such as academic articles (Bahdanau et al., 2014), social media interactions (Bahdanau et al., 2014), chemical molecules (Bahdanau et al., 2014), and online grocery items (Shen et al., 2015). In light of this, graph neural networks (GNNs) have emerged as the state of the art for graph representation learning, which enables a wide range of web-centric applications such as online page classification (Shen et al., 2016), social recommendation (Bahdanau et al., 2014), pandemic trends forecasting (Shen et al., 2016), and dynamic link prediction (Shen et al., 2016; Wang et al., 2016).

A primary challenge in traditional supervised graph machine learning is its heavy reliance on labeled data. Given the magnitude and complexity of the Web, obtaining annotations can be costly and often results in data of low quality. To address this limitation, the "_pre-train, fine-tune_" paradigm has been widely adopted, where GNNs are initially pre-trained with some self-supervised pretext tasks and are then fine-tuned with labeled data for specific downstream tasks. Yet, this paradigm faces the following challenges:

* (**C1**) Fine-tuning methods often overlook the inherent gap between the training objectives of the pretext and the downstream task. For example, while graph pre-training may utilize binary edge classification to draw topologically proximal node embeddings closer, the core of a downstream node classification task would be to ensure nodes with the same class cluster closely. Such misalignment makes the transferred node embeddings suboptimal for downstream tasks, _i.e._, negative transfer (Wang et al., 2016; Wang et al., 2016). The challenge arises: _how to reformulate the downstream node classification task to better align with the contrastive pretext task?_
* (**C2**) In semi-supervised node classification, there often exists a scarcity of labeled nodes. This limitation can cause fine-tuned networks to highly overfit these sparse (Shen et al., 2016) or potentially imbalanced (Shen et al., 2016) nodes, compromising their ability to generalize to new and unlabeled nodes. The challenge arises: _how to capture and generalize the intricate characteristics of each class in the embedding space to mitigate this overfitting?_
* (**C3**) Given the typically large scale of pre-trained GNNs, the attempt to recalibrate all their parameters during the fine-tuning phase can considerably slow down the rate of training convergence. The challenge arises: _how to introduce only a small number of trainable parameters in the fine-tuning stage while keeping the parameters of the pre-trained network unchanged?_

One potential solution that could partially address these challenges is to adapt the "_pre-train, prompt_" paradigm from natural language processing (NLP) to the graph domain. In NLP, prompt-based learning has effectively generalized pre-trained language models across diverse tasks. For example, a sentiment classification task like "_The WebConf will take place in the scenic city of Singapore in 2024_" can be reframed by appending a specific textual prompt "_1 feel so [MASK] to end. It is highly likely that a language model pre-trained on next word prediction will predict "[MASK]" as "_excited_" instead of "_frustrated_", without necessitating extensive fine-tuning. With this methodology, certain downstream tasks can be seamlessly aligned with the pre-training objectives. While few prior work (Bahdanau et al., 2014; Wang et al., 2016; Wang et al., 2016; Wang et al., 2016) has delayed into crafting various prompting templates for graphs, their emphasis remains strictly on homogeneous graphs. This narrow focus underscores the last challenge inherent to the heterogeneous graph structures typical of the Web:

* (**C4**) Homogeneous graph prompting techniques typically rely on the pre-trained node embeddings of the target node or the aggregation of its immediate neighbors' embeddings for downstream node classification, which ignores the intricate neighborhood structure inherent to heterogeneous graphs. The challenge arises: _how to leverage the complex heterogeneous neighborhood structure of a node to yield more reliable classification decisions?_

To comprehensively address all four aforementioned challenges, we propose HetGPT, a general post-training prompting framework tailored for heterogeneous graphs. Represented by the acronymHeterogeneous Graph Prompt Tuning, HetGPT serves as an auxiliary system for HGNNs that have undergone constrastive pre-training. At the core of HetGPT is a novel _graph prompting function_ that reformulates the downstream node classification task to align closely with the pretext contrastive task. We begin with the the _virtual class prompt_, which generalizes the intricate characteristics of each class in the embedding space. Then we introduce the _heterogeneous feature prompt_, which acts as a task-specific augmentation to the input graph. This prompt is injected into the feature space and the prompted node features are then passed through the pre-trained HGNN, with all parameters in a frozen state. Furthermore, a _multi-view neighborhood aggregation_ mechanism, that encapsulates the complexities of the heterogeneous neighborhood structure, is applied to the target node, generating a node token for classification. Finally, Pairwise similarity comparisons are performed between the node token and the class tokens derived from the virtual class prompt via the contrastive learning objectives established during pre-training, which effectively simulates the process of deriving a classification decision. In summary, our main contributions include:
* To the best of our knowledge, this is the first attempt to adapt the "_pre-train_, _prompt_" paradigm to heterogeneous graphs.
* We propose HetGPT, a general post-training prompting framework tailored for heterogeneous graphs. By coherently integrating a virtual class prompt, a heterogeneous feature prompt, and a multi-view neighborhood aggregation mechanism, it elegantly bridges the objective gap between pre-training and downstream tasks on heterogeneous graphs.
* Extensive experiments on three benchmark datasets demonstrate HetGPT's capability to enhance the performance of state-of-the-art HGNNs on semi-supervised node classification.

## 2. Related Work

**Heterogeneous graph neural networks.** Recently, there has been a surge in the development of heterogeneous graph neural networks (HGNNs) designed to learn node representations on heterogeneous graphs (Gardner et al., 2017; Wang et al., 2018; Wang et al., 2019). For example, HAN (Wang et al., 2019) introduces hierarchical attention to learn the node-level and semantic-level structures. MAGNN (Chen et al., 2019) incorporates intermediate nodes along metapaths to encapsulate the rich semantic information inherent in heterogeneous graphs. HetGNN (Wang et al., 2019) employs random walk to sample node neighbors and utilizes LSTM to fuse heterogeneous features. HGT (Han et al., 2019) adopts a transformer-based architecture tailored for web-scale heterogeneous graphs. However, a shared challenge across these models is their dependency on high-quality labeled data for training. In real-world scenarios, obtaining such labeled data can be resource-intensive and sometimes impractical. This has triggered numerous studies to explore pre-training techniques for heterogeneous graphs as an alternative to traditional supervised learning.

**Heterogeneous graph pre-training.** Pre-training techniques have gained significant attention in heterogeneous graph machine learning, especially under the scenario with limited labeled nodes (Gardner et al., 2017; Wang et al., 2019). Heterogeneous graphs, with their complex types of nodes and edges, require specialized pre-training strategies. These can be broadly categorized into generative and contrastive methods. Generative learning in heterogeneous graphs primarily focuses on reconstructing masked segments of the input graph, either in terms of the underlying graph structures or specific node attributes (Chen et al., 2019; Wang et al., 2019; Wang et al., 2019). On the other hand, contrastive learning on heterogeneous graphs aims to refine node representations by magnifying the mutual information of positive pairs while diminishing that of negative pairs. Specifically, representations generated from the same data instance form a positive pair, while those from different instances constitute a negative pair. Some methods emphasizes contrasting node-level representations (Gardner et al., 2017; Wang et al., 2019; Wang et al., 2019; Wang et al., 2019), while another direction contrasts node-level representations with graph-level representations (Gardner et al., 2017; Wang et al., 2019; Wang et al., 2019). In general, the efficacy of contrastive methods surpasses that of generative ones (Wang et al., 2019), making them the default pre-training strategies adopted in this paper.

**Prompt-based learning on graphs.** The recent trend in Natural Language Processing (NLP) has seen a shift from traditional fine-tuning of pre-trained language models (LMs) to a new paradigm: "_pre-train_, _prompt_" (Chen et al., 2019). Instead of fine-tuning LMs through task-specific objective functions, this paradigm reformulates downstream tasks to resemble pre-training tasks by incorporating textual prompts to input texts. This not only bridges the gap between pre-training and downstream tasks but also instigates further research integrating prompting with pre-trained graph neural networks (Wang et al., 2019). For example, GPPT (Wang et al., 2019) and GraphPrompt (Gardner et al., 2017) introduce prompt templates to align the pretext task of link prediction with downstream classification. GPF (Chen et al., 2019) and VNT-GPPE (Wang et al., 2019) employ learnable perturbations to the input graph, modulating pre-trained node representations for downstream tasks. However, all these techniques cater exclusively to homogeneous graphs, overlooking the distinct complexities inherent to the heterogeneity in real-world systems.

## 3. Preliminaries

**Definition 1: Heterogeneous graph.** A heterogeneous graph is defined as \(\mathcal{G}=(\mathcal{V},\mathcal{E})\), where \(\mathcal{V}\) is the set of nodes and \(\mathcal{E}\) is the set of edges. It is associated with a node type mapping function \(\phi:\mathcal{V}\rightarrow\mathcal{A}\) and an edge type mapping function \(\varphi:\mathcal{E}\rightarrow\mathcal{R}\). \(\mathcal{A}\) and \(\mathcal{R}\) denote the node type set and edge type set, respectively. For heterogeneous graphs, we require \(|\mathcal{A}|+|\mathcal{R}|>2\). Let \(\mathcal{X}=\{\mathcal{X}_{A}\mid A\in\mathcal{A}\}\) be the set of all node feature matrices for different node types. Specifically, \(X_{A}\in\mathbb{R}^{|\mathcal{X}_{A}|\times\mathcal{A}_{A}}\) is the feature matrix where each row corresponds to a feature vector \(\mathbf{x}_{A}^{\text{f}}\) of node \(i\) of type \(A\). All nodes of type \(A\) share the same feature dimension \(d_{A}\), and nodes of different types can have different feature dimensions.

Figure 1(a) illustrates an example heterogeneous graph with three types of nodes: author (A), paper (P), and subject (S), as well as two types of edges: "write" and "belong to".

**Definition 2: Network schema.** The network schema is defined as \(\mathcal{S}=(\mathcal{A},\mathcal{R})\), which can be seen as a meta template for a heterogeneous graph \(\mathcal{G}\). Specifically, network schema is a graph defined over the set of node types \(\mathcal{A}\), with edges representing relations from the set of edge types \(\mathcal{R}\).

Figure 1(b) presents the network schema for a heterogeneous graph. As per the network schema, we learn that a paper is written by an author and that a paper belongs to a subject.

**Definition 3: Metapath.** A metapath \(P\) is a path defined by a pattern of node and edge types, denoted as \(A_{1}\xrightarrow{R_{1}}A_{2}\xrightarrow{R_{2}}\cdots\xrightarrow{R_{1}}A_{ \text{f+1}}\) (abbreviated as \(A_{1}A_{2}\cdots A_{\text{f+1}}\)), where \(A_{i}\in\mathcal{A}\) and \(R_{i}\in\mathcal{R}\).

Figure 1(c) shows two metapaths for a heterogeneous graph: "PAP" represents that two papers are written by the same author, while "PSP" indicates that two papers share the same subject.

**Definition 4: Semi-supervised node classification.** Given a heterogeneous graph \(\mathcal{G}=\{\mathcal{V},\mathcal{E}\}\) with node features \(\mathcal{X}\), we aim to predict the labels of the target node set \(\mathcal{V}_{T}\) of type \(T\in\mathcal{M}\). Each target node \(v\in\mathcal{V}_{T}\) corresponds to a class label \(v_{e}\in\mathcal{Y}\). Under the semi-supervised learning setting, while the node labels in the labeled set \(\mathcal{V}_{L}\subset\mathcal{V}_{T}\) are provided, our objective is to predict the labels for nodes in the unlabeled set \(\mathcal{V}_{U}=\mathcal{V}_{T}\setminus\mathcal{V}_{L}\).

**Definition 5: Pre-train, fine-tune.** We introduce the "_pre-train, fine-tune_" paradigm for heterogeneous graphs. During the pre-training stage, an encoder \(f_{\theta}\) parameterized by \(\theta\) maps each node \(v\in\mathcal{V}\) to a low-dimensional representation \(\mathbf{h}_{v}\in\mathbb{R}^{d}\). Typically, \(f_{\theta}\) is an HGN that takes a heterogeneous graph \(\mathcal{G}=\{\mathcal{V},\mathcal{E}\}\) and its node features \(\mathcal{X}\) as inputs. For each target node \(v\in\mathcal{V}_{T}\), we construct its positive \(\mathcal{P}_{v}\) and negative sample sets \(\mathcal{N}_{v}\) for contrastive learning. The contrastive head \(g_{\psi}\), parameterized by \(\psi\), discriminates the representations between positive and negative pairs. The pre-training objective can be formulated as:

\[\theta^{*},\psi^{*}=\operatorname*{arg\,min}_{\mathcal{G},\psi}\mathcal{L}_{ conq}\left(g_{\psi},f_{\theta},\mathcal{V}_{T},\mathcal{P},\mathcal{N}\right), \tag{1}\]

where \(\mathcal{L}_{con}\) denotes the contrastive loss. Both \(\mathcal{P}=\{\mathcal{P}_{v}\mid v\in\mathcal{V}_{T}\}\) and \(\mathcal{N}=\{\mathcal{N}_{v}\mid v\in\mathcal{V}_{T}\}\) can be nodes or graphs. They may be direct augmentations or distinct views of the corresponding data instances, contingent on the contrastive learning techniques employed.

In the fine-tuning stage, a prediction head \(h_{\eta}\), parameterized by \(\eta\), is employed to optimize the learned representations for the downstream node classification task. Given a set of labeled target nodes \(\mathcal{V}_{L}\) and their corresponding label set \(\mathcal{Y}\), the fine-tuning objective can be formulated as:

\[\theta^{**},\eta^{*}=\operatorname*{arg\,min}_{\mathcal{G},\eta}\mathcal{L}_{ sup}\left(h_{\eta},f_{\theta^{*}},\mathcal{V}_{L},\mathcal{Y}\right), \tag{2}\]

where \(\mathcal{L}_{sup}\) is the supervised loss. Notably, the parameters \(\theta\) are initialized with those obtained from the pre-training stage, \(\theta^{*}\).

## 4. Method

In this section, we introduce HetGPT, a novel graph prompting technique specifically designed for heterogeneous graphs, to address the four challenges outlined in Section 1. In particular, HetGPT consists of the following key components: (1) _prompting function design_; (2) _virtual class prompt_; (3) _heterogeneous feature prompt_; (4) _multi-view neighborhood aggregation_; (5) _prompt-based learning and inference_. The overall framework of HetGPT is shown in Figure 2.

### Prompting Function Design (C1)

Traditional fine-tuning approaches typically append an additional prediction head and a supervised loss for downstream tasks, as depicted in Equation 2. In contrast, HetGPT pivots towards leveraging and tuning prompts specifically designed for node classification.

In prompt-based learning for NLP, a prompting function employs a pre-defined template to modify the textual input, ensuring its alignment with the input format used during pre-training. Meanwhile, within graph-based pre-training, contrastive learning has overshadowed generative learning, especially in heterogeneous graphs (Han et al., 2018; Wang et al., 2019; Wang et al., 2019), as it offers broader applicability and harnesses overlapping task subspaces, which are optimal for knowledge transfer. Therefore, these findings motivate us to reformulate the downstream node classification task to align with contrastive approaches. Subsequently, a good design of graph prompting function becomes pivotal in matching these contrastive pre-training strategies.

Central to graph contrastive learning is the endeavor to maximize mutual information between node-node or node-graph pairs. In light of this, we propose a graph prompting function, denoted as \(l(\cdot)\). This function transforms an input node \(v\) into a pairwise template that encompasses a node token \(\mathbf{z}_{v}\) and a class token \(\mathbf{q}_{c}\):

\[l(v)=[\mathbf{z}_{v},\mathbf{q}_{c}]. \tag{3}\]

Within the framework, \(\mathbf{q}_{c}\) represents a trainable embedding for class \(c\) in the downstream node classification task, as explained in Section 4.2. Concurrently, \(\mathbf{z}_{v}\) denotes the latent representation of node \(v\), derived from the pre-trained HGNN, which will be further discussed in Section 4.3 and Section 4.4.

### Virtual Class Prompt (C2)

Instead of relying solely on direct class labels, we propose the concept of a virtual class prompt, a paradigm shift from traditional node classification. Serving as a dynamic proxy for each class, the prompt bridges the gap between the abstract representation of nodes and the concrete class labels they are affiliated with. By leveraging the virtual class prompt, we aim to reformulate downstream node classification as a series of mutual information calculation tasks, thereby refining the granularity and adaptability of the classification predictions. This section delves into the design and intricacies of the virtual class prompt, illustrating how it can be seamlessly integrated into the broader contrastive pre-training framework.

#### 4.2.1. Class tokens.

We introduce class tokens, the building blocks of the virtual class prompt, which serve as representative symbols for each specific class. Distinct from discrete class labels, these tokens can capture intricate class-specific semantics, providing a richer context for node classification. We formally define the set of class tokens, denoted as \(\mathcal{Q}\), as follows:

\[\mathcal{Q}=\{\mathbf{q}_{1},\mathbf{q}_{2},\ldots,\mathbf{q}_{C}\}, \tag{4}\]

Figure 1. A example of a heterogeneous graph.

where \(C\) is the total number of classes in \(\mathbf{y}\). Each token \(\mathbf{q}_{c}\in\mathbb{R}^{d}\) is a trainable vector and shares the same embedding dimension \(d\) with the node representations from the pre-trained network \(f_{\theta^{*}}\).

#### 4.2.2. Prompt initialization

Effective initialization of class tokens facilitates a smooth knowledge transfer from pre-trained heterogeneous graphs to the downstream node classification. We initialize each class token, \(\mathbf{q}_{c}\), by computing the mean of embeddings for labeled nodes that belong to the respective class. Formally,

\[\mathbf{q}_{c}=\frac{1}{N_{c}}\sum_{\begin{subarray}{c}\mathbf{q}\in\mathbf{Y}_{b}\\ \mathbf{y}_{c}=c\end{subarray}}\mathbf{h}_{\mathbf{0}},\quad\forall c\in\{1,2,\ldots,C\}, \tag{5}\]

where \(N_{c}\) denotes the number of nodes with class \(c\) in the labeled set \(\mathbf{Y}_{L}\), and \(\mathbf{h}_{\mathbf{0}}\) represents the pre-trained embedding of node \(v\). This initialization aligns each class token with the prevalent patterns of its respective class, enabling efficient prompt tuning afterward.

### Heterogeneous Feature Prompt (C3)

Inspired by recent progress with visual prompts in the vision domain (Bengio et al., 2017; Chen et al., 2017), we propose a heterogeneous feature prompt. This approach incorporates a small amount of trainable parameters directly into the feature space of the heterogeneous graph \(\mathbf{\beta}\). Throughout the training phase of the downstream task, the parameters of the pre-trained network \(f_{\theta^{*}}\) remain unchanged. The key insight behind this feature prompt lies in its ability to act as task-specific augmentations to the original graph. It implicitly taifors the pre-trained node representations for an effective and efficient transfer of the learned knowledge from pre-training to the downstream task.

Prompting techniques fundamentally revolve around the idea of augmenting the input data to better align with the pretext objectives. This makes the design of a graph-level transformation an important factor for the efficacy of prompting. To illustrate, let's consider a homogeneous graph \(\mathbf{\beta}\) with its adjacency matrix \(\mathbf{A}\) and node feature matrix \(\mathbf{X}\). We introduce \(t_{\mathcal{E}}\), a graph-level transformation function parameterized by \(\xi\), such as changing node features, adding or removing edges, _etc._ Prior research (Chen et al., 2017; Wang et al., 2017) has proved that for any transformation function \(t_{\mathcal{E}}\), there always exists a corresponding feature prompt \(\mathbf{p}^{*}\) that satisfies the following property:

\[f_{\theta^{*}}(\mathbf{A},\mathbf{X}+\mathbf{p}^{*})\equiv f_{\theta^{*}}(t_{\mathcal{E}}( \mathbf{A},\mathbf{X}))+O_{\mathbf{\beta}\theta}, \tag{6}\]

where \(O_{\mathbf{\beta}\theta}\) represents the deviation between the node representations from the graph that's augmented by \(t_{\mathcal{E}}\) and the graph that's prompted by \(\mathbf{p}^{*}\). This discrepancy is primarily contingent on the quality of the learned prompt \(\mathbf{p}^{*}\) as the parameters \(\theta^{*}\) of the pre-trained model are fixed. This perspective further implies the feasibility and significance of crafting an effective feature prompt within the graph's input space, which emulates the impact of learning a specialized augmentation function tailored for downstream tasks.

However, in heterogeneous graphs, nodes exhibit diverse attributes based on their types, and each type has unique dimensionalities and underlying semantic meanings. Take a citation network for instance: while paper nodes have features represented by word embeddings derived from their abstracts, author nodes utilize one-hot encoding as features. Given this heterogeneity, the approach used in homogeneous graph prompting methods may not be effective or yield optimal results when applied to heterogeneous graphs, as it uniformly augments node features for all node types via a single and all-encompassing feature prompt.

Figure 2. Overview of the HetGPT architecture: Initially, an HCNN is pre-trained alongside a contrastive head using a contrastive learning objective, after which their parameters are frozen. Following this, a _heterogeneous feature prompt_ (Sec. 4.3) is injected into the input graphâ€™s feature space. These prompted node features are then processed by the pre-trained HCNN, producing the prompted node embeddings. Next, a _multi-view neighborhood aggregation_ mechanism (Sec. 4.4) captures both local and global heterogeneous neighborhood information of the target node, generating a node token. Finally, pairwise similarity comparisons are performed between this node token and class tokens derived from the _virtual class prompt_ (Sec. 4.2) via the same contrastive learning objective from pre-training. _As an illustrative example of employing HetGPT for node classification: consider a target node \(P_{2}\) associated with class 1, its positive samples during prompt tuning are constructed using the class token of class 1, while negative samples are drawn from class tokens of classes 2 and 3 (_i.e.,_ all remaining classes).

#### 4.3.1. Type-specific feature tokens

To address the above challenge, we introduce type-specific feature tokens, which are a set of designated tokens that align with the diverse input features inherent to each node type. Given the diversity in scales and structures across various graphs, equating the number of feature tokens to the node count is often sub-optimal. This inefficiency is especially obvious in large-scale graphs, as this design demands extensive storage due to its \(O(|\mathcal{V}|)\) learnable parameters. In light of this, for each node type, we employ a feature prompt consisting of a limited set of independent basis vectors of size \(K\), _i.e._, \(f_{K}^{A}\in\mathbb{R}^{d_{A}}\), with \(d_{A}\) as the feature dimension associated with node type \(A\in\mathcal{H}\):

\[\mathcal{F}=\left\{\mathcal{F}_{A}\mid A\in\mathcal{A}\right\},\qquad\quad \mathcal{F}_{A}=\left\{f_{1}^{A},f_{2}^{A},\ldots,f_{K}^{A}\right\}, \tag{7}\]

where \(K\) is a hyperparameter and its value can be adjusted based on the specific dataset in use.

#### 4.3.2. Prompted node features

For each node \(i\) of type \(A\in\mathcal{A}\), its node feature vector \(\mathbf{x}_{i}^{A}\) is augmented by a linear combination of feature token \(f_{k}^{A}\) through an attention mechanism, where the attention weights are denoted by \(\mathsf{w}_{i,k}^{A}\). Consequently, the prompted node feature vector evolves as:

\[\tilde{\mathbf{x}}_{i}^{A}=\mathbf{x}_{i}^{A}+\sum_{k=1}^{K}\mathsf{w}_{i,k}^ {A}\cdot f_{k}^{A}, \tag{8}\]

\[\mathsf{w}_{i,k}^{A}=\frac{\exp\left(\sigma\left((f_{j}^{A})^{\top}\cdot \mathbf{x}_{i}^{A}\right)\right)}{\sum_{j=1}^{K}\exp\left(\sigma\left((f_{j}^ {A})^{\top}\cdot\mathbf{x}_{i}^{A}\right)\right)}, \tag{9}\]

where \(\sigma(\cdot)\) represents a non-linear activation function. Subsequently, we utilize these prompted node features, represented as \(\tilde{\mathcal{X}}\), together with the heterogeneous graph, \(\mathcal{G}\). They are then passed through the pre-trained HGNN \(f_{0}\)- during the prompt tuning phase to obtain a prompted node embedding matrix \(\tilde{\mathbf{H}}\):

\[\tilde{\mathbf{H}}=f_{0}\cdot(\mathcal{G},\tilde{\mathcal{X}})\in\mathbb{R}^{| \mathcal{V}|\times d}. \tag{10}\]

### Multi-View Neighborhood Aggregation (C4)

In prompt-based learning for homogeneous graphs, the node token \(\mathbf{z}_{0}\) in Equation 3 for a given node \(v\in\mathcal{V}\) is directly equated to \(\mathbf{h}_{v}\), which is the embedding generated by the pre-trained network \(f_{0}\)(Han et al., 2017). Alternatively, it can also be derived from an aggregation of the embeddings of its immediate neighboring nodes (Han et al., 2017). However, in heterogeneous graphs, such aggregations are complicated due to the inherent heterogeneity of neighboring structures. For example, given a target node with the type "paper", connections can be established either with other "paper" nodes through different metapaths (_e.g._, PAP, PSP) or with nodes of varied types (_i.e._, author or subject) based on the network schema. Furthermore, it is also vital to leverage the prompted pre-trained node embeddings \(\tilde{\mathbf{H}}\) (as detailed in Section 4.3) in the aggregation. Taking all these into consideration, we introduce a multi-view neighborhood aggregation mechanism. This strategy incorporates both type-based and metapath-based neighbors, ensuring a comprehensive representation that captures both local (_i.e._, network schema) and global (_i.e._, metapath) patterns.

#### 4.4.1. Type-based aggregation

Based on the network schema outlined in Definition 2, a target node \(i\in\mathcal{V}_{T}\) can directly connect to \(M\) different node types \(\{A_{1},A_{2},\ldots,A_{M}\}\). Given the variability in contributions from different nodes of the same type to node \(i\) and the diverse influence from various types of neighbors, we utilize a two-level attention mechanism (Han et al., 2017) to aggregate the local information of node \(i\). For the first level, the information \(R_{i}^{A_{m}}\) is fused from the neighbor set \(\mathcal{N}_{i}^{A_{m}}\) for node \(i\) using node attention:

\[\mathbf{h}_{i}^{A_{m}}=\sigma\left(\sum_{\{i\in\mathcal{N}_{i}^{A_{m}}\cup\{i\}}} a_{i,j}^{A_{m}}\cdot\tilde{\mathbf{h}}_{j}\right), \tag{11}\]

\[\alpha_{i,j}^{A_{m}}=\frac{\exp\left(\sigma\left(\mathbf{a}_{A_{m}}^{\top} \cdot[\tilde{\mathbf{h}}_{i}||\tilde{\mathbf{h}}_{j}]\right)\right)}{\sum_{k\in \mathcal{N}_{i}^{A_{m}}\cup\{i\}}\exp\left(\sigma\left(\mathbf{a}_{A_{m}}^{ \top}\cdot[\tilde{\mathbf{h}}_{i}||\tilde{\mathbf{h}}_{k}]\right)\right)}, \tag{12}\]

where \(\sigma(\cdot)\) is a non-linear activation function, \(\|\) denotes concatenation, and \(\mathbf{a}_{A_{m}}\in\mathbb{R}^{2d\times 1}\) is the node attention vector shared across all nodes of type \(A_{m}\). For the second level, the type-based embedding of node \(i\), denoted as \(\mathbf{z}_{1}^{T}\), is derived by synthesizing all type representations \(\{R_{i}^{A_{i}},\mathbf{h}_{i}^{A_{2}},\ldots,\mathbf{h}_{i}^{A_{M}}\}\) through semantic attention:

\[\mathbf{z}_{i}^{T}=\sum_{i=1}^{M}\beta_{A_{m}}\cdot\mathbf{h}_{i}^{A_ {m}},\quad\beta_{A_{m}}=\frac{\exp(w_{A_{m}})}{\sum_{k=1}^{M}\exp(w_{A_{k}})}, \tag{14}\] \[w_{A_{m}}=\frac{1}{|\mathcal{V}_{T}|}\sum_{i\in\mathcal{V}_{T}} \mathbf{a}_{\text{TP}}^{\top}\cdot\tanh(\mathbf{W}_{\text{TP}}\cdot\mathbf{h}_{i}^{A_ {m}}+\mathbf{b}_{\text{TP}}), \tag{13}\]

where \(\mathbf{\alpha}_{\text{TP}}\in\mathbb{R}^{d\times 1}\) is the type-based semantic attention vector shared across all node types, \(\mathbf{W}_{\text{TP}}\in\mathbb{R}^{d\times d}\) is the weight matrix, and \(\mathbf{b}_{\text{TP}}\in\mathbb{R}^{d\times 1}\) is the bias vector.

#### 4.4.2. Metapath-based aggregation

In contrast to type-based aggregation, metapath-based aggregation provides a perspective to capture global information of a target node \(i\in\mathcal{V}_{T}\). This is attributed to the nature of metapaths, which encompass connections that are at least two hops away. Given a set of defined metapaths \(\{P_{1},P_{2},\ldots,P_{N}\}\), the information from neighbors of node \(i\) connected through metapath \(P_{n}\) is aggregated via node attention:

\[\mathbf{h}_{i}^{P_{n}}=\sigma\left(\sum_{i\in\mathcal{N}_{i}^{P_{n}} \cup\{i\}}a_{i,j}^{P_{n}}\cdot\tilde{\mathbf{h}}_{i}\right), \tag{15}\]

\[\sigma_{i,j}^{P_{n}}=\frac{\exp\left(\sigma\left(\mathbf{a}_{P_{n}}^{\top} \cdot[\tilde{\mathbf{h}}_{i}||\tilde{\mathbf{h}}_{j}]\right)\right)}{\sum_{k\in \mathcal{N}_{i}^{P_{n}}\cup\{i\}}\exp\left(\sigma\left(\mathbf{a}_{P_{n}}^{\top} \cdot[\tilde{\mathbf{h}}_{i}||\tilde{\mathbf{h}}_{k}]\right)\right)}, \tag{16}\]

where \(\mathbf{a}_{P_{n}}\in\mathbb{R}^{2d\times 1}\) is the node attention vector shared across all nodes connected through metapath \(P_{n}\). To compile the global structural information from various metapaths, we fuse the node embeddings \(\{\mathbf{h}_{i}^{P_{1}},\mathbf{h}_{i}^{P_{2}},\ldots,\mathbf{h}_{i}^{P_{N}}\}\) derived from each metapath into a single embedding using semantic attention:

\[\mathbf{z}_{i}^{\text{MP}}=\sum_{i=1}^{N}\beta_{P_{n}}\cdot\mathbf{h}_{i}^{P_{n}}, \quad\beta_{P_{n}}=\frac{\exp(w_{P_{n}})}{\sum_{k=1}^{N}\exp(w_{P_{k}})}, \tag{18}\] \[w_{P_{n}}=\frac{1}{|\mathcal{V}_{T}|}\sum_{i\in\mathcal{V}_{T}} \mathbf{a}_{\text{MP}}^{\top}\cdot\tanh(\mathbf{W}_{\text{MP}}\cdot\mathbf{h}_{i}^{P_{n}}+ \mathbf{b}_{\text{MP}}), \tag{17}\]

where \(\mathbf{a}_{\text{MP}}\in\mathbb{R}^{d\times 1}\) is the metapath-based semantic- attention vector shared across all metapaths, \(\mathbf{W}_{\text{MP}}\in\mathbb{R}^{d\times d}\) is the weight matrix,and \(b_{\text{MP}}\in\mathbb{R}^{d\times 1}\) is the bias vector. Integrating the information from both aggregation views, we obtain the final node token, \(z_{i}\), by concatenating the type-based and the metapath-based embedding:

\[z_{i}=\sigma\left(W[z_{i}^{\text{MP}}|z_{i}^{\text{TP}}]+\mathbf{b}\right), \tag{19}\]

where \(\sigma(\cdot)\) is a non-linear activation function, \(\mathbf{W}\in\mathbb{R}^{2d\times d}\) is the weight matrix, and \(\mathbf{b}\in\mathbb{R}^{d\times 1}\) is the bias vector.

### Prompt-Based Learning and Inference

Building upon our prompt design detailed in the preceding sections, we present a comprehensive overview of the prompt-based learning and inference process for semi-supervised node classification. This methodology encompasses three primary stages: (1) _prompt addition_, (2) _prompt tuning_, and (3) _prompt-assisted prediction_.

#### 4.5.1. Prompt addition

Based on the graph prompting function \(l(\cdot)\) outlined in Equation (3), we parameterize it using the trainable virtual class prompt \(\mathbf{Q}\) and the heterogeneous feature prompt \(\mathcal{F}\). To ensure compatibility during the contrastive loss calculation, which we detail later, we use a single-layer Multilayer Perceptron (MLP) to project both \(z_{\text{p}}\) and \(\mathbf{q}_{c}\), onto the same embedding space. Formally:

\[z_{o}^{\prime}=\text{MLP}(z_{o}),\quad\quad\mathbf{q}_{c}^{\prime}=\text{MLP}( \mathbf{q}_{c}),\quad l_{Q,\mathcal{F}}(v)=[z_{o}^{\prime},\mathbf{q}_{c}^{\prime}]. \tag{20}\]

#### 4.5.2. Prompt tuning

Our prompt design allows us to reuse the contrastive head from Equation 1 for downstream node classification without introducing a new prediction head. Thus, the original positive \(\mathcal{P}_{o}\) and negative samples \(\mathcal{N}_{o}\) of a labeled node \(v\in\mathcal{V}_{\text{L}}\) used during pre-training are replaced with the virtual class prompt corresponding to its given class label \(y_{o}\).

\[\mathcal{P}_{o}=\left\{\mathbf{q}_{y_{o}}\right\},\quad\quad\quad\mathcal{N}_{o}= \mathbf{Q}\setminus\left\{\mathbf{q}_{y_{o}}\right\}, \tag{21}\]

Consistent with the contrastive pre-training phase, we employ the InfoNCE (Zhou et al., 2017) loss to replace the supervised classification loss \(\mathcal{L}_{sup}\):

\[\mathcal{L}_{\mathbf{con}}=-\sum_{v\in\mathcal{V}_{\text{L}}}\log\left(\frac{\exp (\text{sim}(z_{o}^{\prime},\mathbf{q}_{y_{o}}^{\prime})/\tau)}{\sum_{c=1}^{C}\exp (\text{sim}(z_{o}^{\prime},\mathbf{q}_{c}^{\prime})/\tau)}\right). \tag{22}\]

Here, \(\text{sim}(\cdot)\) denotes a similarity function between two vectors, and \(\tau\) denotes a temperature hyperparameter. To obtain the optimal prompts, we utilize the following prompt tuning objective:

\[\mathbf{Q}^{*},\mathcal{F}^{*}=\operatorname*{arg\,min}_{\mathcal{Q},\mathcal{F}} \mathcal{L}_{\text{con}}\left(g_{(\mathcal{V}^{*},f_{\mathcal{V}},l_{Q, \mathcal{F}},\mathcal{V}_{\text{L}})+\lambda\mathcal{L}_{\text{orth}}}. \tag{23}\]

where \(\lambda\) is a regularization hyperparameter. The orthogonal regularization (Bishop, 2006) loss \(\mathcal{L}_{orth}\) is defined to ensure the label tokens in the virtual class prompt remain orthogonal during prompt tuning, fostering diversified representations of different classes:

\[\mathcal{L}_{orth}=\left\|\mathbf{Q}\mathbf{Q}^{\top}-\mathbf{I}\right\|_{F}^{2}, \tag{24}\]

where \(\mathbf{Q}=[\mathbf{q}_{1},\mathbf{q}_{2},\dots,\mathbf{q}_{C}]^{\top}\in\mathbb{R}^{C\times d}\) is the matrix form of the virtual class prompt \(\mathbf{Q}\), and \(\mathbf{I}\in\mathbb{R}^{C\times C}\) is an identity matrix.

#### 4.5.3. Prompt-assisted prediction

During the inference phase, for an unlabeled target node \(v\in\mathcal{V}_{U}\), the predicted probability of node \(v\) belonging to class \(c\) is given by:

\[P(y_{v}=c)=\frac{\exp(\text{sim}(z_{o}^{\prime},q_{c}^{\prime}))}{\sum_{k=1}^ {C}\exp(\text{sim}(z_{o}^{\prime},\mathbf{q}_{k}^{\prime}))}. \tag{25}\]

This equation computes the similarity between the projected node token \(\mathbf{z}_{o}^{\prime}\) and each projected class token \(\mathbf{q}_{c}^{\prime}\), using the softmax function to obtain class probabilities. The class with the maximum likelihood for node \(v\) is designated as the predicted class \(\hat{y}_{o}\):

\[\hat{y}_{o}=\operatorname*{arg\,max}_{c}P(y_{v}=c), \tag{26}\]

## 5. Experiments

In this section, we conduct a thorough evaluation of our proposed HetGPT to address the following research questions:

* (**RQ1**) Can HetGPT improve the performance of pre-trained heterogeneous graph neural networks on the semi-supervised node classification task?
* (**RQ2**) How does HetGPT perform under different settings, _i.e._, ablated models and hyperparameters?
* (**RQ3**) How does the prompt tuning efficiency of HetGPT compare to its fine-tuning counterpart?
* (**RQ4**) How interpretable is the learned prompt in HetGPT?

### Experiment Settings

#### 5.1.1. Datasets

We evaluate our methods using three benchmark datasets: ACM (Yang et al., 2017), DBLP (Chen et al., 2019), and IMDB (Chen et al., 2019). Detailed statistics and descriptions of these datasets can be found in Table 1. For the semi-supervised node classification task, we randomly select 1, 5, 20, 40, or 60 labeled nodes per class as our training set. Additionally, we set aside 1,000 nodes for validation and another 1,000 nodes for testing. Our evaluation metrics include Macro-F1 and Micro-F1.

#### 5.1.2. Baseline models

We compare our approach against methods belonging to three different categories:

* **Supervised HGNNs:** HAN (Yang et al., 2017), HGT (Huang et al., 2017), MAGNN (Chen et al., 2019);
* **HGNNs with "_pre-train, fine-tune_":*
* **Generative**:**: HGMAE (Yang et al., 2017);
* **Contrastive (our focus):** DMGI (Zhou et al., 2017),HeCo (Yang et al., 2017),HDM (Huang et al., 2017);
* **GNNs with"_pre-train, prompt_":** GPPT (Chen et al., 2019).

#### 5.1.3. Implementation details

For the homogeneous method GPPT, we evaluate using all the metapaths and present the results with the best performance. Regarding the parameters of other baselines, we adhere to the configuration specified in their original papers.

In our HetGPT model, the heterogeneous feature prompt is initialized using Kaiming initialization (Kaiming, 1999). During the prompt tuning phase, we employ the Adam optimizer (Kingma and Ba, 2014) and search within a

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline \hline
**Dataset** & **\# Nodes** & **\# Edges** & **Metapaths** & **\# Classes** \\ \hline \multirow{3}{*}{ACM} & Paper: 4.019 & \multirow{3}{*}{P-A: 13,407} & \multirow{3}{*}{PAP} & \multirow{3}{*}{3} \\  & Author: 7.167 & & & P-A: 4,019 & \\ \cline{1-1} \cline{2-5}  & Subject: 6.0 & & & \\ \hline \multirow{3}{*}{DBLP} & Author: 4.057 & \multirow{3}{*}{P-A: 13,9,645} & \multirow{3}{*}{APCA} & \multirow{3}{*}{4} \\  & Paper: 14.328 & & P-T: 85,810 & \\  & Term: 7,723 & & P-C: 14,328 & \\ \cline{1-1}  & Conference: 20 & & \\ \hline \multirow{3}{*}{IMDB} & Movie: 4.278 & \multirow{3}{*}{M-D: 4,278} & \multirow{3}{*}{M-M: 12,828} & \multirow{3}{*}{MMD} & \multirow{3}{*}{3} \\  & Director: 2.081 & & M-A: 12,828 & \\ \cline{1-1}  & Actor: 5,257 & & & \\ \hline \hline \end{tabular}
\end{table}
Table 1. Detailed statistics of the benchmark datasets. Underlined node types are the target nodes for classification.

[MISSING_PAGE_FAIL:7]

as the most pivotal component, indicated by the significant performance drop when it's absent. This degradation mainly stems from the overfitting issue linked to the negative transfer problem, especially when labeled nodes are sparse. The virtual class prompt directly addresses this issue by generalizing the intricate characteristics of each class within the embedding space.

#### 5.3.2. Hyper-parameter sensitivity

We evaluate the sensitivity of HetGPT to its primary hyperparameter: the number of basis feature tokens \(K\) in Equation (7). As depicted in Figure 4, even a really small value of \(K\) (_i.e._, 5 for ACM, 20 for DBLP, and 5 for IMDB) can lead to satisfactory node classification performance. This suggests that the prompt tuning effectively optimizes performance without the need to introduce an extensive number of new parameters.

### Prompt Tuning Efficiency Analysis (RQ3)

Our HetGPT, encompassing the virtual class prompt and the heterogeneous feature prompt, adds only a few new trainable parameters (_i.e._, comparable to a shallow MLP). Concurrently, the parameters of the pre-trained HGNNs and the contrastive head remain unchanged during the entire prompt tuning phase. Figure 5 illustrates that HetGPT converges notably faster than its traditional "_pre-train_, _fine-tune_" counterpart, both recalibrating the parameters of the pre-trained HGNNs and introducing a new prediction head. This further demonstrates the efficiency benefits of our proposed framework, allowing for effective training with minimal tuning iterations.

### Interpretability Analysis (RQ4)

To gain a clear understanding of how the design of the virtual class prompt facilitates effective node classification without relying on the traditional classification paradigm, we employ a t-SNE plot to visualize the node representations and the learned virtual class prompt on ACM and DBLP, as shown in Figure 6. Within this visualization, nodes are depicted as colored circles, while the class tokens from the learned virtual class prompt are denoted by colored stars. Each color represents a unique class label. Notably, the embeddings of these class tokens are positioned in close vicinity to clusters of node embeddings sharing the same class label. This immediate spatial proximity between a node and its respective class token validates the efficacy of similarity measures inherited from the contrastive pretext for the downstream node classification task. This observation further reinforces the rationale behind our node classification approach using the virtual class prompt, _i.e._, a node is labeled as the class that its embedding is most closely aligned with.

## 6. Conclusion

In this paper, we propose HetGPT, a general post-training prompting framework to improve the node classification performance of pre-trained heterogeneous graph neural networks. Recognizing the prevalent issue of misalignment between the objectives of pretext and downstream tasks, we craft a novel prompting function that integrates a virtual class prompt and a heterogeneous feature prompt. Furthermore, our framework incorporates a multi-view neighborhood aggregation mechanism to capture the complex neighborhood structure in heterogeneous graphs. Extensive experiments on three benchmark datasets demonstrate the effectiveness of HetGPT. For future work, we are interested in exploring the potential of prompting methods in tackling the class-imbalance problem on graphs or broadening the applicability of our framework to diverse graph tasks, such as link prediction and graph classification.

Figure 4. Performance of HetGPT with the different number of basis feature vectors on ACM, DBLP, and IMDB.

Figure 5. Comparison of training losses over epochs between HetGPT and its fine-tuning counterpart on DBLP and IMDB.

Figure 3. Ablation study of HetGPT on ACM and IMDB.

Figure 6. Visualization of the learned node tokens and class tokens in virtual class prompt on ACM and DBLP.

## References

* (1)
* Bahng et al. (2022) Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola. 2022. Exploring visual prompts for adapting large-scale models. _arXiv:2203.17274_ (2022).
* Brock et al. (2016) Andrew Brock, Theodore Lim, James M Richie, and Nick Weston. 2016. Neural photo editing with introspective adversarial networks. _arXiv:1609.01903_ (2016).
* Cao et al. (2021) Yuwei Cao, Hao Peng, Jia Wu, Yinglong Dou, Jianxin Li, and Philip S Yu. 2021. Knowledge-preserving incremental social event detection via heterogeneous gems. In _TheWebConf_.
* Yan et al. (2019) Wenqi Yan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiang Tang, and Dawei Yin. 2019. Graph neural networks for social recommendation. In _TheWebConf_.
* Yang et al. (2022) Taoran Yang, Yunchao Zhang, Yang Yang, Chunping Wang, and Li Chen. 2022. Universal Temporal Tuning for Graph Neural Networks. _arXiv:2202.18202_ (2022).
* Fang et al. (2022) Yang Fang, Xiang Zhao, rifan Chen, Widdong Xiao, and Maarten de Rijke. 2022. rsf-HR: Pre-Training for Heterogeneous Information Networks. _IEEE Transactions on Knowledge and Data Engineering_ (2022).
* Fu et al. (2020) Xinyu Fu, Juani Zhang, Ziqiao Meng, and Irwin King. 2020. Magm: Meta-with aggregated graph neural network for heterogeneous graph embedding. In _TheWebConf_.
* Guo et al. (2023) Zhichun Guo, Kehan Guo, Bohao Yan, Yiujun Tian, Robin M Cy, Yong Ma, and Olivet Vissang Zhang. Wei Wang. Chuxu Zhang, et al. 2023. Graph-based molecular representation learning. In _IJCAI_.
* He et al. (2015) Kaiming He, Xiangyu Zhang, Shuoting Ren, and Jian Sun. 2015. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In _ICCV_.
* Hu et al. (2020) Zilin Hu, Yuxiao Dong, Kunsan Wang, Kai-Wei Chang, and Yithou Sun. 2020. Gpt-gan: Generative pre-training of graph neural networks. In _KDD_.
* Hu et al. (2020) Zilin Hu, Yuxiao Dong, Kunsan Wang, and Yithou Sun. 2020. Heterogeneous graph transformer. In _TheWebConf_.
* Liu et al. (2022) Menglin Liu, Luming Tang, Zhou Chen, Chengihe Carlie, Serge Belongie, Bharath Hariharan, and Ser-Num Lim. 2022. Visual Prompt Tuning. _arXiv:2203.12119_ (2022).
* Jiang et al. (2021) Yanxiang Jiang, Tianrui Jia, Yuan Fang, Chuan Shi, Zhe Lin, and Hui Wang. 2021. Pre-training on large-scale heterogeneous graph. In _KDD_.
* Jiang et al. (2021) Xunqiang Jiang, Tianrui Li, Yuan Fang, and Chun Shu. 2021. Contrastive pre-training of CNNs on heterogeneous graphs. In _CHN_.
* Jing et al. (2021) Baoyu Jing, Chunxong Park, and Hanghang Tong. 2021. Hdml: High-order deep multiplex infomax in _WWW_.
* Kingma and Ba (2014) Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. In _ICLR_.
* Liu et al. (2023) Pengfei Liu, Weihao Yuan, Jilin Fu, Zhenghao Jiang, Hiroaki Hayashi, and Graham Neubig. 2023. 20re-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. _Comput. Surveys_ (2023).
* Liu et al. (2022) Yizin Liu, Ming Jin, Shirui Pan, Chuan Zhou, Yu Zheng, Feng Xia, and S Yu Philip. 2022. Graph self-supervised learning: A survey. _IEEE Transactions on Knowledge and Data Engineering_ (2022).
* Liu et al. (2023) Zemin Liu, Xingfeng Yu, Yuan Fang, and Ximing Zhang. 2023. GPFP: Graph Pre-training and Prompt Tuning to Generative Graph Neural Networks. In _WWW_.
* Lv et al. (2012) Qingsong Lv, Ming Ding, Qiang Liu, Yuxiao Chen, Wernheng Feng, Siming He, Chung Zhou, Jingqiu Jiang, Yixiao Tong, and Fang. 2012. Are we really making much progress? revisiting, benchmarking and refining heterogeneous graph neural networks. In _KDD_.
* Ma et al. (2022) Yihong Ma, Patrick Gerald, Yijun Tian, Zhichun Guo, and Nitesh V Chawla. 2022. Hierarchical spatio-temporal graph neural networks for pandemic forecasting. In _CIKM_.
* Ma et al. (2023) Yihong Ma, Yijun Tian, Nuno Monir, and Nitesh V Chawla. 2023. Class-Imbalanced Learning on Graphs: A Survey. _arXiv:2304.06020_ (2023).
* van den Oord et al. (2018) Aaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. _arXiv:1807.03788_ (2018).
* Park et al. (2020) Chayoung Park, Donghyun Kim, Jiwei Han, and Hrungyu Yu. 2020. Unsupervised attributed multiplex network embedding. In _AAAI_.
* Qi and Davison (2009) Xiaogang Qi and Brian D Davison. 2009. Web page classification: Features and algorithms. _ACM computing surveys (CSUR)_ (2009).
* Ren et al. (2019) Yuxiang Ren, Bo Lu, Chao Huang, Peng Li, Jefong Bo, and Jiawei Zhang. 2019. Heterogeneous deep graph infomax. _arXiv:1911.08583_ (2019).
* Sun et al. (2022) Mingchen Sun, Kaiqiong Zhou, Xin He, Ying Wang, and Xin Wang. 2022. GPFT: Graph Pre-training and Prompt Tuning to Generalize Graph Neural Networks. In _KDD_.
* Sun et al. (2023) Xianglong Sun, Hong Cheng, Jia Li, Bo Lin, and Jihong Guan. 2023. All in One: Multi-Task Prompting for Graph Neural Networks. In _KDD_.
* Tan et al. (2013) Zhen Tan, Ruocheng Guo, Kazie Ding, and Huan Liu. 2013. Virtual Node Tuning for Few-shot Model Classification. In _KDD_.
* Tian et al. (2009) Yijun Tian, Kaiwen Duan, Chunxing Zhang, Chuxu Zhang, and Nitesh V Chawla. 2003. Heterogeneous graph masked autoencoders. In _AAAI_.
* Tian et al. (2022) Yijun Tian, Xinxong Zhang, Zichuan Gao, Xiaog Ma, Ronald Metoyer, and Nitesh V Chawla. 2022. RecipeNet: Multi-modal recipe representation learning with graph neural networks. In _IJCAI_.
* Wang et al. (2021) Xiang Wang, Zichuan Zhang, Yihong Ma, Tong Zhao, Tianwen Jiang, Nitesh V Chawla, and Meng Jiang Jiang. 2021. Modeling co-evolution of attributed and structural information in graph space. _The Transactions on Knowledge and Data Engineering_ (2021).
* Wang et al. (2007) Daheu Wang, Zihhan Zhang, Yihong Ma, Tong Zhao, Tianwen Jiang, Nitesh V Chawla, and Meng Jiang. 2007. Learning attribute-structure co-evolutions in dynamic graphs. In _DAC_.
* Wang et al. (2021) Liyuan Wang, Mingtian Zhang, Zhongfan Jia, Qian Li, Chenghao Bao, Kaiqiong Ma, Jun Zhang, and Yihong. 2021. Afec: Active forgetting of negative transfer in continual learning. In _NeurIPS_.
* Wang et al. (2022) Xiao Wang, Deyu Bo, Chuan Shi, Shaohua Fan, Tanding Ye, and S Yu Philip. 2022. A survey on heterogeneous graph embedding methods, techniques, applications and sources. _IEEE Transactions on Big Data_ (2021).
* Wang et al. (2019) Xiao Wang, Houye Ji, Chun Shi, Bai Wang, Tanding Ye, Peng Cui, and Philip S Yu. 2019. Heterogeneous graph attention network. In _WebConf_.
* Wang et al. (2021) Xiao Wang, Nian Liu, Hui Han, and Chuan Shi. 2021. Self-supervised heterogeneous graph neural network with co-contrastive learning. In _KDD_.
* Shu et al. (2023) Zihhao Shu, Yuan Fang, Yihun Liu, Tang Guo, and Shiqi Hao. 2023. Voucher Aluse Detection with Prompt-based Fine-tuning on Graph Neural Networks. In _CIKM_.
* Xie et al. (2022) Yachen Xie, Zhao Xu, Jingtan Zhang, Zhengyang Wang, and Shuiwang Ji. 2022. Self-supervised learning of graph neural networks: A unified review. _IEEE transactions on pattern analysis and machine intelligence_ (2022).
* Yang et al. (2020) Carl Yang, Yuxin Xiao, Yuqiang Zhang, Yithou Sun, and Jiawei Han. 2020. Heterogeneous network representation learning: A unified framework with survey and benchmark. _IEEE Transactions on Knowledge and Data Engineering_ (2020).
* Yang et al. (2022) Yanqing Yang, Ziyu Cui, Zhen Wang, Wei Zhao, Cai, Weiqiang Lu, and Jian-hua Huang. 2022. Self-supervised heterogeneous graph pre-training based on structural clustering. In _NeurIPS_.
* Zhang et al. (2019) Chunx Zhang, Donglin Song, Chao Huang, Ananthram Swami, and Nitesh V Chawla. 2019. Heterogeneous graph neural network. In _KDD_.
* Zhang et al. (2022) Wen Zhang, Lingfei Liu, Ziqiao Zhang, and Dongrui Wu. 2022. A survey on negative transfer. _IEEE/CAA Journal of Automatica Sinica_ (2022).
* Zhao et al. (2008) Jianan Zhao, Xiao Wang, Chuan Shi, Zekuan Liu, and Yanfang Ye. 2008. Network schema preserving heterogeneous information network embedding. In _IJCAI_.