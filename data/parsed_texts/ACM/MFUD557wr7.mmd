# LLMCloudHunter: Harnessing LLMs for Automated Extraction of Detection Rules from Cloud-Based CTI

Anonymous Author(s)

###### Abstract.

As the number and sophistication of cyber attacks have increased, threat hunting has become a critical aspect of active security, enabling proactive detection and mitigation of threats before they cause significant harm. Open-source cyber threat intelligence (OSTCI) is a valuable resource for threat hunters, however, it often comes in unstructured formats that require further manual analysis. Previous studies aimed at automating OSCTI analysis are limited since (1) they failed to provide actionable outputs, (2) they did not take advantage of images present in OSCTI sources, and (3) they focused on on-premises environments, overlooking the growing importance of cloud environments. To address these gaps, we propose LLMCloudHunter, a novel framework that leverages large language models (LLMs) to automatically generate generic-signature detection rule candidates from textual and visual OSCTI data. We evaluated the quality of the rules generated by the proposed framework using 20 annotated real-world cloud threat reports. The results show that our framework achieved a precision of 83% and recall of 99% for the task of accurately extracting API calls made by the threat actor and a precision of 99% with a recall of 97% for IoCs. Additionally, 99.18% of the generated detection rule candidates were successfully compiled and converted into _Splunk_ queries.

C Cyber threat intelligence (CTI), Large language model (LLM), Threat hunting, Cloud, Sigma rules +
Footnote †: copyright: none

+
Footnote †: copyright: none

+
Footnote †: copyright: none

+
Footnote †: copyright: none

+
Footnote †: copyright: none

## 1. Introduction

The rapid evolution of technology, digitization, and application development has been accompanied by an increase in the number of cyberattacks (Shen et al., 2017), raising concerns about the security risks associated with these advancements. In the face of these concerns, organizations have adopted dynamic defensive strategies in addition to the traditional reactive measures employed (Shen et al., 2017). One such strategy is threat hunting, a proactive approach aimed at searching for and mitigating undetected threats in a network or system (Shen et al., 2017). Threat hunters try to minimize the damage caused by threat actors by shortening the time window between intrusion and discovery (Bauer et al., 2017). In their comprehensive survey, Nour et al. (Shen et al., 2017) stated that the threat hunting methodology consists of three main principles: (1) formulating and testing hypotheses about the threat actor and their actions; (2) utilizing existing information for an intelligence-driven investigation; and (3) leveraging data analysis techniques and machine learning algorithms to effectively handle vast amounts of data.

The second principle involves collecting and analyzing publicly available information about potential and active threats from blogs, forums, and other digital sources. Open-source cyber threat intelligence (OSCTI) is one of the most commonly used sources of information among security personnel according to the SANS 2023 CTI survey (Shen et al., 2017). However, various challenges arise when using OSCTI. The first and main challenge is that OSCTI often comes in non-uniform and unstructured formats, such as text and images, rather than more actionable information/data (e.g., detection rules) (Shen et al., 2017). As a result, manual analysis by human experts is required to derive meaningful and actionable insights (Shen et al., 2017). Another challenge is the increasing amount of available information (i.e., CTIs), necessitating the automation of OSCTI analysis (Shen et al., 2017).

Previous studies on threat hunting introduced various methodologies, some of which incorporated natural language processing (NLP) techniques, to automate the extraction and enrichment of information from OSCTI textual data. However, the methods presented in these studies suffer from three main limitations: (1) they provide structured but limited insights, such as identified entities and their relationships or attack techniques, necessitating further processing to generate actionable outputs; an exception is the approach presented by Gao et al. (Gao et al., 2018), in which the authors developed proprietary, non-standard graph-based queries using static rules (regexes) that require substantial customization for application with standard tools and on-premises environments; (2) these studies, including the work of Gao et al., do not take advantage of visual components, such as images, which may be present in OSCTI data; and (3) many of these methodologies were primarily developed for on-premise environments, limiting their effectiveness and relevance in cloud-centric environments.

Cloud computing has become an integral component in the modern enterprise landscape, valued for its scalability, cost-effectiveness, and flexibility (Shen et al., 2017). It employs a shared responsibility model for security, in which both the provider and the consumer play roles in securing cloud infrastructure and cloud-delivered applications (Bauer et al., 2017). This model presents unique challenges in threat hunting, as traditional security methodologies often fall short in addressing the dynamic and distributed nature of cloud environments (Shen et al., 2017). Among these challenges is the fact that in some cloud technologies (e.g., serverless), access to data for threat hunting is limited to application-level logs (APIs, storage access, etc.), and important infrastructure- (system)-level data (e.g., virtual machines and network) can only be accessed by the cloud provider (Shen et al., 2017). This is exacerbated by the fact that the exploitation of cloud-based threat intelligence has not yet reached maturity. The work of Fengrui and Du (Fengrui and Du, 2018) is the only study that extends beyond on-premise OSCTI, however rather than providing actionable output, their framework extracts MITE ATT&CK tactics, techniques, and procedures (TTPs) (Bauer et al., 2017). These gaps highlight the need for innovative OSCTI analysis approaches suited to the unique security challenges of cloud environments; such challenges can be addressed by integrating OSCTI analysis results within practical, actionable security measures (Fengrui and Du, 2018).

In this paper, we present LLMCloudHunter, a novel framework that leverages pretrained large language models (LLMs) to generate detection rule candidates from unstructured OSCTIs automatically. LLMCloudHunter generates _Sigma_ rule (Shen et al., 2017) candidates from both textual and visual cyber threat information, using an innovative,automated data extraction and processing framework that leverages LLMs and employs various techniques to address their limitations (e.g., unstructured output and hallucinations).

_Sigma_ rules, provided in a generic and open signature format written in YAML, enable the creation and sharing of detection methods across security information and event management (SIEM) systems. Fig. 1 presents our LLM pipeline for _Sigma_ candidate generation; as can be seen, textual and visual OSCTI data is processed first, converting it into semi-structured paragraphs in the preprocessing phase. It then extracts API calls (that are unique entities to threat hunting in cloud environments) and MITRE ATT&CK TTPs from the paragraphs and generates initial _Sigma_ candidates (in the Paragraph-Level phase). Finally, it consolidates the candidates from all paragraphs, verifies their syntactic and logical correctness, eliminates duplication, and enriches them with identified indicators of compromise (IoCs) (in the OSCTI-Level phase). An example of a _Sigma_ rule generated by LLMCloudHunter is illustrated in Listing 1, with a demonstration of its generation process in Appendix C.

We evaluated the efficacy and precision of the _Sigma_ candidates generated using 20 cloud-related OSCTI sources that we identified. The evaluation was performed using common entity and relationship extraction metrics, and the results were validated against a ground truth carefully defined by our research team. Additionally, we introduced a set of criteria specifically designed to test each _Sigma_ candidate's functionality in the operational context of OSCTI. This evaluation ensures that the rules generated not only meet syntactic standards but are also operationally effective in addressing the dynamic and complex nature of cloud-based cyber threats. We also conducted an ablation study, systematically removing components of the framework to pinpoint their individual contributions to LLMCloudHunter's overall efficacy. The results show that our framework achieved a precision of 83% and recall of 99% for the task of accurately extracting threat actors' API calls, and a precision of 97% with a recall of 97% for IoCs. Moreover, 99.18% of the generated _Sigma_ candidates were successfully converted into _Splunk_ queries. In terms of overall performance, i.e., including the extraction of API calls, IoCs, MITRE ATT&CK TTPs, and request parameters, our framework achieved 85% and 88% precision and recall, respectively.

To summarize, the main contributions of this paper are: (1) _A novel LLM-based framework for the automatic generation of Sigma candidates from unstructured OSCTI_, which integrates both textual and visual information. While our framework focuses on cloud environments, it can be adapted for use with on-premise-related CTL LLMCloudHunter utilizes a pretrained LLM, thus providing flexibility in updating the underlying LLM, and does not require "heavy" model training. (2) _An annotated dataset_ (used for the evaluation of our framework) consisting of 20 cloud-related OSCTI posts, complete with entities and their relationships, as well as _Sigma_ rules. (3) _Insights on the application of LLMs for complex NLP tasks in the field of cybersecurity_, pertaining to prompt engineering techniques and the effective use of models' features and parameters. (4) _A comprehensive evaluation_ that assesses the accuracy and correctness of the _Sigma_ candidates generated. (5) _We make both our code and cloud CTI dataset available to the research community_ on GitHub.1

Footnote 1: To preserve anonymity, the code and dataset will be available upon paper acceptance.

```
title:AccesstoTerraformFilefromMaliciousIPs description:Detectsrequestsforterraform.tfstatefile fromknownmaliciousIPs.ThisfilecontainssensitiveInfrastructureinformationandsecrets,indicatingpotentialcompromiseorunauthorizedaccess.refreferences:-[https://sysdig.com/blog/cloud-breach-terraform-data-theft/https://docs.aws.amazon.com/Amazon53/latest/API/API.GetObject.htmlauthor](https://sysdig.com/blog/cloud-breach-terraform-data-theft/https://docs.aws.amazon.com/Amazon53/latest/API/API.GetObject.htmlauthor):{LMCloudHunter} tags:-attack.collection-attack.t1530 logsource:-product:aws-service:cloudtrail-detection:selection_event:event:eventName:GetObject requestParameters.key:terraform.tfstate selection_ip_address:sourceIPAddress:-88.239.148.221 -45.9.148.221 -45.9.148.221 -45.9.249.58 condition:selection_eventandselection_ip_address:falsepositive:- AutomatedCI/CDpipelineoperations - DevOps engineersmanuallyrunningTerraform commandslevel:high
```

Listing 1: A Sigma rule generated by LLMCloudHunter.

## 2. Related Work

In this section, we provide a brief overview of recent studies focused on analyzing unstructured OSCTI analysis. A detailed description of related work is provided in Appendix A.

Earlier works have extensively utilized NLP techniques for OSCTI analysis (Becker et al., 2016; Krizhevsky et al., 2012; Krizhevsky et al., 2012; Krizhevsky et al., 2012). These methods leveraged advanced NLP models to extract actionable insights from OSCTI text. However, to adapt these models to the cyber threat domain, a significant amount of preprocessing and fine-tuning is required. While the approach implemented by TTPDrill (Krizhevsky et al., 2012) and THREATRAPTOR (Luo et al., 2017) reduces the need for extensive model training, it is not flexible, and significant customization is needed for use in cloud environments. This is due to fundamental differences in terminology and data types between traditional on-premise environments and cloud environments, as well as the dynamic nature of cloud architectures, which continuously evolve with new services and configurations.

The introduction of LLMs has led to a paradigm shift in OSCTI processing, with research demonstrating their ability to extract meaningful and structured data from OSCTI text. Utilizing GPT-3.5, Purba and Chu (Purba and Chu, 2019) and Sircausano et al. (Sircausano et al., 2019) addressed tasks ranging from the extraction of IoCs to the generation of structured CTI format (e.g., STIX), respectively, while Liu and Zhan (Liu and Zhan, 2019) applied ChatGPT to construct graphical representations of OSCTI data. Hu et al. (Hu et al., 2019) and Fengrui and Du (Fengrui and Du, 2019) expanded upon these capabilities by utilizing both pretrained and fine-tuned LLM models. They employed GPT-3.5 and ChatGPT for data annotation and augmentation, respectively, to prepare datasets for fine-tuning the LLaMA2-7B model. Hu et al. (Hu et al., 2019) applied the fine-tuned LLaMA2-7B to construct knowledge graphs, while Fengrui and Du (2018) focused on TTP classification. In this research, we are the first to develop an end-to-end framework based on a pretrained LLM, demonstrating the potential of LLMs in processing OSTI and generating actionable _Sigma_ rules. Moreover, our framework integrates visual analysis capabilities, expanding the scope of OSTI analysis beyond previous text-centric methodologies. By leveraging pretrained LLMs, we avoid the need for rule-based methods or training customized models with dedicated datasets. Our framework also focuses on generating rules for cloud environments, which has not been addressed before.

In terms of OSTCI datasets, in contrast to prior studies that used semi-structured and on-premise-related datasets, we use 20 unstructured, publicly available _cloud-based_ posts and reports sourced from various publishers. These OSTCI reports, which describe AWS cloud incidents, were systematically annotated by our research team to develop a robust ground truth for development and evaluation.

Previous studies produced a variety of outputs with different levels of utility and applicability. This includes extracting IoCs (Zhou et al., 2017; Zhao et al., 2018), TTPs (Zhou et al., 2018), and structured representations using the STX format (Zhou et al., 2018; Zhao et al., 2018). More advanced approaches were used to create threat behaviour graphs (Zhou et al., 2018; Zhao et al., 2018) and knowledge graphs (Zhou et al., 2018; Zhao et al., 2018; Zhao et al., 2018; Zhao et al., 2018; Zhao et al., 2018; Zhao et al., 2018). While the approaches highlighted above provide valuable contextual information, further processing is required to transform the representations into actionable defense mechanisms. To address this, in their study, Gao et al. presented a framework for converting OSCTI data into a threat behavior graph and associated domain-specific queries. The detection rule candidates generated by LLMCloudHunter, however, are in the known open-source _Sigma_ structure. This widely used generic signature format is inherently suitable for integration in various application environments and SIEMs. By capturing the entities, relations, IoCs, and TTPs identified in OSTCI, LLMCloudHunter translates threat intelligence into applicative _Sigma_ candidates.

## 3. Proposed Method

In this section, we present our proposed framework, LLMCloudHunter, and how it leverages OpenAI's GPT-4o (Zhou et al., 2018) model to process cloud-based OSCTs and generate _Sigma_ candidates. LLMCloudHunter's pipeline (see Fig. 1) consists of three main phases: _Preprocessing_, _Paragraph-Level Processing_, and _OSCTI-Level Processing_; these phases are described in the subsections that follow.

**Relevant Entities for Threat Hunting in Cloud Environments.** The atomic units in cloud application logs are cloud API calls, which describe system and application activities that potentially provide traces of threat behavior. An example of an API call may be the _GetFunction_ action, which requests information about a function. Therefore, the information used to generate _Sigma_ candidates for threat hunting in cloud environments includes entities such as IP addresses and user agents, similar to on-premise environments, as well as API calls that are unique to cloud environments.

We differentiate between primary (essential) entities and contextual entities. Primary entities are required for the correct execution of generated _Sigma_ candidates in SIEM systems. A mistake in extracting a relationship that includes a primary entity will result in incorrect "hunting" activity. Primary entities in cloud environments include API calls (e.g., _GetFunction_) as well as the request parameters of that API call (e.g., _requestParameters_functionName: respondUser_), IoCs (including IP addresses and user agents), log source (e.g., _AWS CloudTrail_), and event source (e.g., _lambda.amzonaws.com_). Contextual entities do not impact the correctness of the detection rule logic; however, they provide additional contextual information to the threat hunter, making the investigation of a case more efficient. Contextual entities include the title and description of the _Sigma_ rule, TTPs, false positives, and criticality level.

### OSTCI Preprocessing

OSTI varies in terms of the type and format, depending on the publishing platform, the author, the nature of the collected information, and its intended purpose. Due to this lack of uniformity, preliminary steps must be performed to standardize the format. Such steps enable the data to be automatically and effectively handled by subsequent processing components. The preprocessing converts the HTML content into a structured markdown format, which has been shown to improve LLM task performance (Zhou et al., 2018). Additionally, our framework uniquely handles image extraction, classification, and transcription-a novel approach compared to related works.

**Downloader and Parser.** The automated OSCTI preprocessing phase begins by downloading and parsing the OSCTI HTML code (A in Fig. 1), using web scraping and processing tools such as Selenium (Sen et al., 2017) and BeautifulSoup (Suo et al., 2018), followed by additional reformatting techniques (e.g., regex) to ensure a valid OSCTI markdown output. By examining the web page elements, LLMCloudHunter pinpoints the beginning and end of the relevant content, excluding irrelevant elements (such as sidebars and advertisements). In the next step, these HTML layout elements are converted into a unified markdown based on the following guidelines: (1) Preserve spacing to separate content types such as paragraphs and code sections, maintaining their original layout. (2) Mark headings (h1, h2, etc.) to maintain the hierarchical structure of the original HTML content. (3) Parse HTML code encompassing tables and nested lists to preserve their structural properties. For example, a tab character is employed in lists to signify nested items, whereas in tables, the '|' symbol is used to demarcate columns. (4) Identify and embed image URLs as placeholders within the text, positioning them according to their original placement in the report.

After converting the HTML into a markdown, we employed a targeted approach to exclude non-essential content (including headings, subheadings, and the corresponding paragraphs). Such content is identified by indicative keywords that suggest repetitive and redundant information. Examples of this type of content include overviews, recommendations, and concluding paragraphs. For instance, if a'recommendations' paragraph appears under an h2 heading, we remove the paragraph and any subsequent content until the next h2 (or h1) heading is encountered, as recommendations are not part of the attack description and often include marketing content. This approach effectively removes non-essential or duplicated content nested under the identified headings. The filtered version of the output is then passed on to the next component in the framework. The full output, which includes all content, will be used in the _OSCTI-Level Processing_ phase.

**Image Analysys.** Continuing with the _Preprocessing_ phase, each image is first classified by the _Image Classifier_(B in Fig. 1) using a _classification prompt_ as either an informative image (e.g., screenshots, charts, diagrams, and tables containing information related to the OSCTI content) or non-informative one(e.g., decorative art, advertisements, logos, or generic symbols). The prompt includes the text of the paragraph in which the image is located in the OSCTI as context to assist the LLM in determining its classification. Along with the classification, we requested the LLM to explain the image classification to facilitate human validation during testing. If an image is classified as informative, it is then passed to _Image Transcript_ (C in Fig. 1). It is processed using a _transcription prompt_ to extract and convert its content into the most appropriate markdown format (e.g., lists and code). The extracted text is integrated into the OSCTI formatted text in its original location, preserving the report's context/flow and enhancing it with critical details, such as API calls and IoCs. By adopting this comprehensive image processing approach, the framework increases the accuracy of extracted information and introduces a novel method in OSCTI analysis (See ablation study 4). Unlike previous works, which have overlooked the potential value of visual data, our framework integrates relevant images into the analytical pipeline, ensuring that no critical information is missed. The image classification and transcription prompts are provided in Appendix F.

### Paragraph-Level Processing

After preprocessing the OSCTI, the next phase in the LLMcloudHunter framework is _Paragraph-Level Processing_. In this phase, LLMcloudHunter first identifies key entities: API calls, MITRE ATT&CK TTPs, and threat event criticality levels. These entities are then used to enrich the format paragraphs, from which LLMCloudHunter generates initial _Sigma_ candidates. To perform these complex tasks, LLMs require carefully defined steps of accurate information extraction and effective data linkage. Our experiments showed that segmenting the OSCTI text into manageable chunks (i.e., paragraphs) enhances the efficiency of the tasks involved in _Sigma_ candidate generation. This approach aligns with the natural structure of writing, organizing information into semantically distinct paragraphs, which narrows the model's focus and minimizes errors. Additionally, we leverage parallelization by processing these paragraphs concurrently to boost processing speed significantly.

**API Call Extractor.** The _Paragraph-Level Processing_ phase starts with the _API Call Extractor_ (D in Fig. 1), which analyzes paragraphs from the OSCTI formatted text that were generated in the previous phase and extracts both explicitly mentioned and implicitly referred API calls in each paragraph (this process is depicted in the flowchart presented in Appendix F). To improve the model's output reliability, mitigate hallucinations (e.g., referencing nonexistent events), and prevent the omission of API calls, we incorporate a majority voting mechanism to ensure higher accuracy and confidence in identifying and extracting relevant API calls.

The operational flow begins with the _explicit API call extractor_, where a dedicated prompt instructs the LLM to extract all explicitly mentioned API calls in the paragraph. This operation is executed \(N_{explicit}\) times, with API calls that exceed the \(T_{explicit}\) threshold selected for subsequent analysis. Only paragraphs containing API calls that meet the \(T_{explicit}\) are kept; the rest are discarded.

Then, paragraphs that are found to contain explicit API calls undergo more nuanced extraction by the _Implicit API Call Extractor_. In this step, we utilized the LLM to perform a deeper analysis to infer API calls suggested indirectly by the OSCTI author. For example, operational descriptions such as performing a _sync action_ on an S3 bucket should be mapped to the _ListBuckets_ and _GetObject_ API calls. Due to the complexity of identifying these implicit API calls, this step is executed \(N_{implicit}\) times, where \(N_{implicit}\) is set to twice the number of \(N_{\text{explicit}}\) iterations performed. Similar to the explicit API call extraction process, paragraphs are analyzed for implicit API calls that meet the \(T_{implicit}\) threshold. However, paragraphs without any implicit API calls are not discarded, as they still have some value due to their explicit API call content.

**TTP Extractor.** This component (E in Fig. 1) analyzes the extracted API calls, mapping them to cloud-based MITRE ATT&CK tactics, techniques, and sub-techniques. It utilizes a detailed prompt, which includes mapping cloud tactics to techniques and techniques to sub-techniques (in JSON format), along with illustrative examples of effective and ineffective mappings. This integrated approach not only enhances the accuracy of TTP assignments but also safeguards against model hallucinations. Each API call is evaluated in its specific context to assign the most precise and relevant TTPs. While these TTPs do not directly alter the detection logic of the _Sigma_ candidates, they play a critical role in understanding the structure of the attack and classifying its various stages.

**Criticality Classifier.** This component (F in Fig. 1) estimates the severity of each _Sigma_ candidate. It uses a single prompt, which includes the paragraph markdown along with the extracted API calls and TTPs, to classify API calls into appropriate criticality levels based on their context. The prompt guides the LLM by providing examples (zero-shot learning), helping emphasize each API call's potential impact, malicious use, and monitoring importance.

**Rule Generator.** The last component in the _Paragraph-Level Processing_ phase (G in Fig. 1) receives as input a list of identified API calls, their criticality, and corresponding TTP assignments, bundled

Figure 1. Overview of the LLMCloudHunter framework.

with the paragraph markdown. The LLM processes this enriched input using the _Rule Generator_ prompt (the full prompt is provided in Appendix P). This prompt defines the LLM's role as a cybersecurity analysis tool that specializes in generating _Sigma_ rules from OSCTI text. This approach aims to leverage extracted AWS API calls to enrich paragraphs and transform them into _Sigma_ candidates. This, in turn, enables the detection of similar activities or patterns in log files. The generation prompt includes several important instructions:
* Each API call provided (along with its TTPs) must be included in the _Sigma_ candidates, but not more than once, to avoid the omission of important details and duplications.
* Paying attention to small details is extremely important as they can improve the detection specificity of the _Sigma_ candidates.
* _Sigma_ candidates with the same attack patterns and stages (i.e., their TTPs) should be merged and vice versa.
* _Sigma_ candidates must align with the specific terminology and functionality of AWS environments to ensure relevance.
* The output (i.e., LLM response) is required to be in a uniform and interpretable format. We used JSON format since it is a built-in feature available through the OpenAI API (Cai et al., 2019).
* **Rule Validator.** Once _Sigma_ candidates are generated, a validation function is applied to ensure that the output complies with the _Sigma_ standard structure (YAML). This function is denoted as _Valid_ in Fig. 1, and is executed by each component that produces rules using LLM. This validation process involves sanitizing too specific or extraneous fields, such as _errorcode_, _errormessage_, and explicit resource names, to enhance the applicability of the rules. It also reformats the syntax to ensure the validity of _-keyvalue_- pairs and verifies metadata, including author names, reference URLs, and dates. This function safeguards the integrity and consistency of the _Sigma_ candidates by eliminating redundant attributes and correcting structural flaws.

### OSCTI-Level Processing

The final phase in the LLMCloudHunter framework aggregates _Sigma_ candidates generated from individual paragraphs to produce a consolidated and optimized set of detection rules, enabling holistic processing and enrichment. It takes the collected _Sigma_ candidates from all processed paragraphs and outputs a final, optimized set free of redundancies and enriched with IoCs.

**Rule Optimizer.** The first component (H in Fig. 1) in the OSCTI-_Level Processing_ phase is designed to improve _Sigma_ candidates' detection logic. In this component, the LLM processes the validated _Sigma_ candidates concurrently to enhance the speed and efficiency of the optimization process. A designated prompt, along with optimization examples, guides the LLM to ensure that the detection criteria are clear and aligned with their intended purpose. The optimization process includes the following tasks:

* merges _selection_ fields that match identical detection criteria, i.e., those sharing the same filtering logic. For example, consider the _Sigma_ rule in Listing 1, which detects access to a certain file from malicious IP addresses. Assume this _Sigma_ rule includes another _selection_ field with the same event source, event name, and request parameter (_s3.amazonaws.com_, _GetObject_, and _terraform.tfstate_, respectively) but adds an additional request parameter: _requestParameters.bucket: Stark_. When performing the unification task, the _Rule Optimizer_ combines these two _selection_ fields into a single _selection_ that encompasses all relevant fields: _eventSource_, _eventName_, _requestParameters.key_, and _requestParameters.bucket_. This unification ensures that the rules are comprehensive and free of redundancy by merging overlapping criteria while preserving their original integrity.
* Splits disjoint _selection_ fields that share some detection criteria but have misaligned logic. For example, consider the _Sigma_ rule in Listing 1. Assume that the initial _Sigma_ rule incorrectly included two additional unrelated fields: _eventSource: iam.amazonaws.com_ and _eventName: PutUserPolicy_ in the same existing _selection_ field. The _Rule Optimizer_ would recognize that these fields are unrelated to the original detection logic and would separate them into a new _selection_ field. Then, it would update the _condition_ field to search for either the first _selection_ or the newly created second _selection_. This separation ensures the rule remains accurate and logically consistent by distinguishing between different detection criteria.

```
0:A set of _Sigma_ candidates \(\mathit{osciRules}\)
0:Modified \(\mathit{osciRules}\)
0:\(\mathit{osciRPLS}\leftarrow\mathit{ExtracPLR}(\mathit{osciRules})\)
0:for each \(\mathit{osciRIP}\in\mathit{osciR APIs}\)do
0:\(\mathit{commonRules}\leftarrow\) GetCommonRules(\(\mathit{osciRules},\mathit{osciRIP}\))
0:\(\mathit{selectedRule}\leftarrow\) RuleSelectGordonRules(\(\mathit{osciRIP}\))
0:\(\mathit{rulesToAJust}\gets\mathit{commonRules}-\mathit{selectedRule}\)
0:for each \(\mathit{ruleToAJust}\in\mathit{rulesToAJust}\)do
0:\(\mathit{ruleAPIs}\leftarrow\) ExtractAPIs(\(\mathit{ruleToAJust}\))
0:if\([\mathit{ruleAPIs}]=1\)then
0:\(\mathit{osciRules}\gets\mathit{osciRules}-\mathit{ruleToAJust}\)
0:else
0:\(\mathit{APICallRemover(ruleToAJust,osciRIP)}\)
0:endif
0:endfor
0:endfor
0:endfor
```

**Algorithm 1**Rule Deduplicator.

**Rule Selector.** This component (J in Fig. 1) refines the _Sigma_ candidate set by selecting the most suitable rule among those containing the same API call. It uses prompts to evaluate the specificity and context of each rule, prioritizing those with detailed criteria directly linked to the API call. If multiple rules are equally specific, the context (the paragraph of which they have been generated) is used to make the final selection.

**API Call Remover.** Following the _Rule Selector's_ identification of the best rule, the _API Call Remover_ (K in Fig. 1) edits other rules containing the same API call. It systematically preserves each rule's structure while removing the redundant API call. If a rule solely depends on the API call being removed, it is discarded entirely.

**Rule Deduplicator.** Working with the _Rule Selector_ and _API Call Remover_, the _Rule Deduplicator_ (I in Fig. 1) finalizes the _Sigma_ candidate set by eliminating overlaps as the depicted in Algorithm 1. It maps event names to rule indices and retains only the most comprehensive rule for each detection scenario, resulting in a precise and non-overlapping set of _Sigma_ candidates.

**IoC Extractor.** This component (L in Fig. 1) parses OSCTI texts to identify and extract explicit IoCs, notably IP addresses and user agents pertinent to AWS CloudTrail logs. Its input is the full mark-down of the OSCTI created by the _Downloader and Parser_, along with an instruction prompt. This prompt guides the LLM to focus on paragraphs typically containing IoCs (e.g., conclusion, findings, or IoC sections). Additionally, the LLM is instructed to ensure that all IoCs are identified and to convert obfuscated IP addresses and user agents to standardized formats.

**IoC Enhancer.** Following the extraction of IoCs by the _IoC Extractor_, this component (M in Fig. 1) integrates the extracted IoCs into all _Sigma_ candidates, enhancing their detection capabilities while maintaining flexibility for analysts. The IoCs (IP addresses and user agents) associated with the threat actor are added to each _Sigma_ candidate as optional detection filters. The _IoC Enhancer_ introduces new selection fields for each type of IoC. For instance, when an IP address is extracted (_198.51.100.1_), the _selection_ioc_ip_ field is added: _selection_ioc_ip: sourceIPAddress: 1985.1100.1_. Similarly, when a user agent is extracted (_Mozilla/5.0_), the _selection_ioc_ua_ field is introduced: _selection_ioc_ua: userAgent(contains: Mozilla/5.0_)._

The _[contains_ operator is used to improve string matching flexibility, allowing for variations (e.g., different versions). After adding these IoC selections, the _IoC Enhancer_ updates the _condition_ field of each _Sigma_ candidate to include the IoCs as optional criteria. If the original condition was: _condition: selection_, it is modified to: _selection_ioc_ip_or selection_ioc_ua_). This ensures that an event must meet the original detection criteria (e.g., specific API calls and event sources) and either the IP address or user agent IoC. By integrating IoCs in this way, the rules become more accurate in detecting activities associated with the threat actor. Importantly, since the IoCs are added as optional filters, analysts can easily adjust the rules to suit their needs. If the IoCs lead to false positives or become irrelevant, analysts can remove or modify these conditions without altering the core detection logic. This approach maintains transparency of information passed from OSCTI to the _Sigma_ rules while ensuring the _Sigma_ candidates remain adaptable for various use cases.

## 4. Evaluation

In this section, we describe the creation of an annotated benchmark dataset and present the methodology and metrics used to evaluate the efficacy and accuracy of the Sigma candidates generated by LLMCloudHunter. We present the results of our evaluation, which also includes an ablation study in which we analyze the impact of each of the framework's components on the overall performance.

### Dataset

We collected 20 cloud environment OSCTIs published by different vendors. Table 6 in Appendix provides a description of the OSCTIs, including the number of images, token sizes, number of API calls, and their technical complexity. To establish the dataset's ground truth, a team of threat hunting and cloud security experts thoroughly analyzed each OSCTI's content. The team (1) identified and extracted the entities described in the OSCTI and (2) identified the relevant inter-entity relationships essential for creating coherent and meaningful _Sigma_ candidates. The list of extracted entities and inter-entity relationships is provided in Table 1. We categorized the entities and relationships into two main groups:

### Evaluation Metrics

We evaluated our framework's performance using a comprehensive set of metrics designed to assess both the extraction of entities and relationships from OSCTIs and the functionality of the generated _Sigma_ candidates.

**Entity and Relationship Extraction Metrics:** We utilized common entity and relationship extraction metrics, as done in prior studies (Brockman et al., 2016; Chen et al., 2016; Chen et al., 2016; Chen et al., 2016; Chen et al., 2016; Chen et al., 2016; Chen et al., 2016; Chen et al., 2016; Chen et al., 2016; Chen et al., 2016; Chen et al., 2016), to assess our framework's performance, validating the results against the ground truth defined by our research team. The metrics used to assess LLMCloudHunter's performance in extracting and identifying the entities and inter-entity relationships in the OSCTI are the precision (P), recall (R), and F1 score (F1) weighted by the total number of entities/relationships of each type, denoted as '\(\mathit{i}\)' (since each OSCTI has a different number of entities/relationships). By calculating these metrics separately for each entity and relationship type, we can pinpoint areas of strength and identify opportunities for improvement.

To evaluate the functionality, logical validity, and relevance of the _Sigma_ candidates generated by LLMCloudHunter, we defined the following criteria. These metrics were calculated by our research team for each _Sigma_ candidate generated:

* Assesses whether the generated _Sigma_ candidates are syntactically correct and properly formatted, ensuring that a given rule is operational in a SIEM system. We used _Sigma_ CLI (Chen et al., 2016) for compilation and conversion into query languages (e.g., Splunk).
* Focuses on the correctness of the _condition_ fields, which specify the relationship between various _selection_ fields.
* Measures the accuracy of the _level_ field of each _Sigma_ candidate, which represents the level of importance and urgency of the rule.
* Evaluates whether the _title_, _description_, and _falsepositives_ fields accurately reflect the rule's intended purpose and context.

\begin{table}
\begin{tabular}{|c|c|} \hline
**Entity** & **Relationship** \\ \hline \multicolumn{2}{|c|}{**Detection Entities and Relationships**} \\ \hline API Call & Detection Field Name \(\leftrightarrow\) Detection Entity Log Source & API Call \(\leftrightarrow\) Log Source \\ API Source & API Call \(\leftrightarrow\) API Source \\ IoC & API Call \(\leftrightarrow\) IoC \\ Other & API Call \(\leftrightarrow\) Other \\ \hline \multicolumn{2}{|c|}{**MITRE ATT\&C Ext Entities and Relationships**} \\ \hline Technique & API Call \(\leftrightarrow\) Technique Sub-Technique & \multicolumn{1}{c|}{} \\ \hline \end{tabular}
\end{table}
Table 1. Entity types and relationships.

### Results

The results (averaged over all evaluated OSCTIs) of our entity and relationship extraction evaluation are presented in Tables 2 and 3, respectively (detailed results are provided in Appendix B).

**Detection.** We consider API calls and IoCs to be the most important entities for generating practical and relevant _Sigma_ candidates. For these two entity types, LLMCloudHunter achieved a weighted precision of 83% with a recall of 99% for the API calls and a precision of 99% with a recall of 97% for the IoCs. In the 'Other' entity category, which includes various entities (e.g., request parameters and IP address), resulted in precision and recall values of 75% and 61%, respectively. The relationship extraction results, which represent LLMCloudHunter's ability to interrelate detection entities to the appropriate fields in _Sigma_ rules, achieved an F1 score of 96% for the _Detection Field Name_\(\leftrightarrow\)_Detection Entity_ relationship.

**Informative (MITRE Tags).** For the extraction of MTRE ATT&CK TTPs, which is known to be a challenging task (Kumar et al., 2017), LLMCloudHunter achieved an F1 score of 74% for technique and 81% for sub-technique. Since each technique and sub-technique directly maps to one or more known tactics, this entity becomes redundant. For instance, _Cloud Service Discovery (T1526)_' maps to the _Discovery_' tactic, illustrating how tactics can be directly inferred from techniques, rendering the explicit identification of tactics redundant. These results are notable compared to similar works; for instance, Daniel et al. (2017) reported a highest F1 score of 0.49 in MITRE tags extraction. The relationship identification results, which represent LLMCloudHunter's ability to interrelate the detection entities to the relevant key in the _Sigma_ candidates (_Detection Field Name_\(\leftrightarrow\)_Detection Entity_), achieved an F1-score of 96%. Regarding the extraction of MITRE ATT&CK TTPs, LLMCloudHunter achieved an F1 score of 74% for Techniques and 81% for Sub-Techniques, with notably high recall rates of 82% and 90%, respectively. The precision was impacted due to LLMCloudHunter generating more _Sigma_ candidates than the ground truth, leading to the creation of additional, more specific tags. This increase in the number of tags stems from LLMCloudHunter's strategy to extract all the threat actor actions, resulting in a higher number of API Calls and, thus, a higher number of false positives when compared to the ground truth, thus lowering the precision. Similarly, in the relationship extraction task, the low precision for MITRE-related relationships can be attributed to the model associating more specific Techniques and Sub-Techniques with the API Calls, which were not always present in the ground truth. While this affects the precision metric, the high recall indicates that LLMCloudHunter successfully captures the relevant TTPs, providing valuable context for threat detection.

In summary, LLMCloudHunter demonstrates strong performance in extracting and identifying key entities and their relationships within OSCTI. While the framework was shown to excel in handling API calls, IoCs, and request parameters, achieving high precision and recall for this, it faces challenges with MITRE ATT&CK TTPs, which impacts the overall performance but does not affect the detection capabilities of the _Sigma_ candidates generated.

The results of our _Sigma_ candidate evaluation are presented in Table 4. Out of 260 generated candidates, an impressive 99.18% were syntactically correct and operational, showcasing high syntax correctness. The detection condition accuracy was equally noteworthy, with all but one candidate correctly specifying the logical relationships between selection fields, resulting in an accuracy rate exceeding 99%. While the criticality accuracy varied between 75% and 100% across different OSCTs--with an average of approximately 88% -- this suggests that LLMCloudHunter generally assigns appropriate importance levels, though there is room for improvement in aligning more closely with expert assessments. Lastly, the descriptive metadata alignment was exceptional, with most OSCTIs scoring above 95%, demonstrating that LLMCloudHunter effectively generates titles, descriptions, and false positive information that accurately reflect each rule's intended purpose and context.

**Ablation Study Results.** We conducted an ablation study to better understand the impact of LLMCloudHunter's components on its performance. We created three variations of LLMCloudHunter by systematically removing key components and evaluating the performance of each variant. Table 11 in Appendix D summarizes

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|} \hline \multirow{2}{*}{**OSCT ID**} & \multirow{2}{*}{\(\mathbf{\mu_{\text{Rules}}}\)} & \multirow{2}{*}{Executability} & \multirow{2}{*}{\begin{tabular}{c} **Condition** \\ **Field** \\ **Accuracy** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **Criticality** \\ **Accuracy** \\ \end{tabular} } & \multirow{2}{*}{\begin{tabular}{c} **Descriptive** \\ **Relative** \\ **Accuracy** \\ \end{tabular} } & \multirow{2}{*}{
\begin{tabular}{c} **Testing** \\ **Accuracy** \\ \end{tabular} } \\ \cline{1-1} \cline{5-7}

[MISSING_PAGE_POST]

*Weighted Avg.** & 15 & 99.185 & 100.005 & 83.185 & 97.500 & 805 \\ \hline \end{tabular}
\end{table}
Table 4. Sigma candidate evaluation results.

\begin{table}
\begin{tabular}{|c|l|c|c|c|} \hline \multirow{2}{*}{**Entity**} & \multirow{2}{*}{\(\mathbf{\mu}\)} & \multirow{2}{*}{\(\mathbf{P}\)} & \multirow{2}{*}{\(\mathbf{R}\)} & \multirow{2}{*}{\(\mathbf{F1}\)} \\ \hline \multirow{4}{*}{**Detection**} & Field Name & 8.20 & 0.85 & 0.85 & 0.85 \\ \cline{2-5}  & API Call & 18.75 & 0.83 & **0.99** & **0.90** \\ \cline{2-5}  & IoC & 9.50 & 0.99 & 0.97 & **0.98** \\ \cline{2-5}  & Log Source & 2.00 & 1.00 & 1.00 & 1.00 \\ \cline{2-5}  & Other & 3.45 & 0.75 & 0.61 & 0.67 \\ \hline \hline
**MITRE** & Technique & 6.25 & 0.67 & **0.82** & 0.74 \\ \hline
**ATT&8CK** & Sub-Technique & 3.00 & 0.73 & **0.90** & 0.81 \\ \hline \end{tabular}
\end{table}
Table 2. Entity extraction results.

\begin{table}
\begin{tabular}{|c|l|c|c|c|c|} \hline \multirow{2}{*}{**OSCT ID**} & \multirow{2}{*}{\(\mathbf{\mu_{\text{Rules}}}\)} & \multirow{2}{*}{\(\mathbf{Executability}\)} & \multirow{2}{*}{\(\mathbf{Condition}\)} & \multirow{2}{*}{\(\mathbf{Criticality}\)} & \multirow{2}{*}{\begin{tabular}{c} **Descriptive** \\ **Accuracy** \\ \end{tabular} } & \multirow{2}{*}{\(\mathbf{Accuracy}\)} & \multirow{2}{*}{\(\mathbf{Accuracy}\)} & \multirow{2}{*}{
\begin{tabular}{c} **Descriptive** \\ **Relative** \\ **Accuracy** \\ \end{tabular} } \\ \hline
1 & 10 & 9 (905) & 9 (905) & 87.505 & 93.755 & 789 \\ \hline
2 & 15 & 15 (10005) & 15 (10005) & 90.005 & 93.005 & 790 \\ \hline
3 & 15 & 15 (10005) & 15 (10005) & 83.335 & 90.005 & 790 \\ \hline
4 & 9 & 9 (10005) & 9 (9003) & 83.335 & 100.005 & 791 \\ \hline
5 & 18 & 18 (10005) & 18 (1005) & 86.115 & 100.005 & 792 \\ \hline
6 & 14 & 14 (10005) & 14 (1005) & 92.865 & 100.005 & 792 \\ \hline
7 & 7 & 7 (10005) & 7 (10005) & 83.715 & 100.005 & 793 \\ \hline
8 & 9 & 9 (10005) & 9 (10005) & 83.335 & 100.005 & 794 \\ \hline
9 & 4 & 4 (10005) & 4 (1005) & 75.005 & 87.305 & 795 \\ \hline
10 & 15 & 15 (1005) & 15 (1005) & 96.455 & 100.005 & 796 \\ \hlinethe different configurations used in the ablation study. The BlindHunter variation evaluates the impact of the image processing by _Image Classifier_ and _Image Transcript_. The NoAPHunter variation is designed to evaluate the impact of the _API Call Extractor_ and _TTP Classifier_ components (D and F in Fig. 1, respectively); the UnoptimizedHunter variation aims to evaluate the _Rule Optimizer_ component (H in Fig. 1), and the CritLessHunter is used variation evaluates the impact of the _Criticality Classifier_ component (F in Fig. 1). When the _Criticality Classifier_ was omitted (CritLessHunter variation), we observed minimal impact on entity extraction metrics. However, this component is vital for assigning appropriate threat levels and aiding in the prioritization of _Sigma_ candidates. Table 12 in Appendix D presents the results for each of the variations in the previously evaluated entity and relationship identification tasks.

The results obtained with the BlindHunter variation show a 7% decrease in the F1 score for the API Call entity extraction task, with the recall dropping to 82%. Additionally, the weighted average precision and recall for _Detection Field Name_\(\leftrightarrow\)_Detection Entity_ relationship identification were reduced by 17% and 21%, respectively. This significant reduction in accuracy, especially in extraction coverage (API Calls), highlights the importance of the _Image Classifier_ and _Image Transcript_ components in extracting information from images that may not be available elsewhere.

The NoAPHunter variation, with the _API Call Extractor_ and _TTP Extractor_ components removed, resulted in significantly worse performance compared to the other variations. For the task of entity extraction, we observed a 22% drop in the average precision and a 7% drop in the average recall. Performance on the relationship extraction metrics was even more affected, with a 42% reduction in the average precision and a 14% reduction in the average recall.

These findings highlight the importance of dedicated components for entity extraction, such as the _API Call Extractor_ and _TTP Classifier_, which allow the model to focus on accurate extraction before rule generation. Specifically, the _API Call Extractor_ and _TTP Extractor_ components proved essential to LLMCloudHunter's overall performance. In contrast, less dramatic differences in the performance were seen with the UnoptimizedHunter variation, which assesses the impact of omitting the _Rule Optimizer_ component. In the relationship extraction task, there was a 17% reduction in average precision and a 9% decrease in average recall. Although these declines are not as great as those seen in the previous variation in terms of API Call extraction, the decrease in the relationship identification indicates that syntax and executability will be affected.

To summarize, the ablation study highlights the essential roles of the _Image Classifier_, _Image Transcript_, _API Call Extractor_, and _TTP Extractor_ components in maintaining high precision and recall in both entity and relationship extraction tasks. The _Rule Optimizer_ also plays a valuable role, though its impact is less pronounced compared to the other components.

## 5. Discussion

Our experiments highlighted the effectiveness of various techniques applied throughout LLMCloudHunter's pipeline. These techniques, along with the purpose and specific settings for each component, are summarized in Table 13 in Appendix E and described below:

**Majority Rule in Entity Extraction Using LLMs.** We used a majority voting mechanism in the _API Call Extractor_ to address LLM inconsistencies and hallucinations. While identical extraction requests generally produced similar results, occasional variations may occur due to the LLM's generative nature. To ensure accuracy, only API calls meeting a set majority threshold were retained. We experimented with the number of runs and threshold size to balance runtime, cost, and accuracy. This approach effectively reduced erroneous results in ambiguous cases.

**Structured Response Format.** For each LLM request, we use the JSON output format LLM via the request setting (Zhou et al., 2017). This structured format enables automatic validation and processing. It also allows direct access to values without additional post-processing.

**LLM Temperature Settings.** The temperature setting of an LLM influences the creativity and randomness of its outputs, and its values range between zero and two (Zhou et al., 2017). By adjusting the temperature for different tasks, we can improve the results. For example, in the _API Call Extractor_ component, where extracting the information accurately is crucial, we use a low temperature of zero to ensure more accurate responses. In contrast, for the _Rule Generator_ component, we set the temperature to 0.7 to allow the model to generate conditions for _Sigma_ rules, which require some 'creativity'.

**Leveraging the Few-shot Learning Technique.** Providing instructions and input-output examples can significantly improve model performance (Zhou et al., 2017; Wang et al., 2018). By dividing the OSCTI analysis into smaller tasks, we provided specific instructions for each. Using few-shot learning with a small number of examples further enhanced the model's ability to generate accurate outputs.

**Parallel LLM Requests.** We leveraged independent LLM prompts to perform parallel execution, resulting in improved speed and efficiency. We identified two key scenarios where parallel requests were particularly beneficial. First, in preprocessing, we translated all images into text simultaneously, accelerating this step. Second, in paragraph-level processing, we processed each paragraph in parallel, reducing overall processing time by threefold. This approach reduces the runtime and improves scalability for larger datasets, allowing for more efficient handling of extensive text corpora.

**Limitations.** Using a commercial LLM model (OpenAI's GPT-40), known for its performance (Han et al., 2017; Chen et al., 2018), adds a cost factor that needs to be considered (approximately 25 cents per OSCTI). In addition, while we used pretrained LLMs, fine-tuning open-source models, may have an advantage in performing specific tasks correctly.

## 6. Conclusions and Future Work

In this paper, we presented LLMCloudHunter, an end-to-end framework that analyzes textual and visual OSCTI using a pretrained LLM model when provided a URL. Our framework offers significant flexibility by allowing easy updates to newer and improved models without the need for fine-tuning, and it demonstrates scalability by running independently across multiple OSCTI images and paragraphs. By using the _Sigma_ format, LLMCloudHunter's output can be seamlessly integrated into existing SIEM systems. Future work can focus on extending LLMCloudHunter to on-premise environments, increasing its applicability in diverse organizational settings and environments. Additionally, we plan to enhance our framework by equipping it with playbook automation capabilities, which will improve its ability to mitigate detected threats and provide more robust support for threat hunters.

## References

* (1)
* (2024) 2024. AITRACK Matrix for Enterprise. [https://attack.mitre.org/](https://attack.mitre.org/). Accessed: 2024-05-14.
* (2024) 2024. Sigma Command Line Interface. [https://github.com/SigmatIQ/sigma-cli/](https://github.com/SigmatIQ/sigma-cli/). Accessed: 2024-05-27.
* (2025) Susan Ahmadi. 2024. Systematic Literature Review on Cloud Computing Security: Threats and Mitigation Strategies. _International Journal of Information Security_ 15, 6 (2020), 148-167.
* (2026) Kaban Ahmed, Syed Khalidoon Khurshid, and Sadifan Hina. 2024. CyberEntRel: Joint extraction of cyber utilities and relations using deep learning. _Computers & Security_ 136 (2020), 103579.
* (2027) Anita Kikrokawa Akash Sharma, Said Sestrehpalli. 2024. Analysis: GPT-4.0 vs GPT-4.0.0. [https://www.ulm.kiba/display-gpt-do-vs-gpt-4-turbo/](https://www.ulm.kiba/display-gpt-do-vs-gpt-4-turbo/). Accessed: 2024-05-27.
* (2028) Md Tanwani Alam, Djoland Bhusai, Yousipa Park, and Nabili Rastiqi. 2023. Looking beyond ICs: Automatically extracting attack patterns from external CTI. In _Proceedings of the 28th International Symposium on Research in Attacks, Intrusions and Defense_, 92-108.
* (2029) Masumi Arafino, Sidharib Rajakshani, Linjq Jaldon, Zahra Jadidi, Shastan Pal, Ernest Root, and Nagaman Venkatasuban. 2022. Design and development of automated threat hunting in industrial control systems. In _2022 IEEE International Conference on Pervasive Computing and Communications Workshops and other Affiliated Events (PerCom Workshops)_. IEEE, 618-623.
* (2020) AttackQ. 2022. What is the Pyramid of Pian? [https://www.attackq.com/glossary/pyramid-of-pian](https://www.attackq.com/glossary/pyramid-of-pian). Accessed: 2024-05-27.
* (2021) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvul Neelakantan, Franz Stayar, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. _Advances in neural information processing systems_ 33 (2020), 1877-1901.
* (2021) Nun Daniel, Tomkian Klauser, Anton Booga, Ariel Dyabak, and Rami Punais. 2023. Labeling NDS files with MTET AT. ACM Techniques Using ChatTGP. In _European Symposium on Research in Computer Security_. Springer, 76-91.
* (2021) Yu Fengyi and Yanhui Du. 2024. Few-shot Learning of TTP's Classification Using Large Language Models. (2020).
* (2022) Shia Tsuji, Nothaka Kawaguchi, Tomohio Shigeto, and Nabili Rayane Yamachi. 2022. Cyber-information extraction from unstructured led of i-sources with nontechnical tools. In _International Workshop on Security_. Springer, 85-104.
* (2022) Peng Guo, Kai Shao, Xiaoyuan Liu, Xusheng Xiao, Zheng Qin, Fenyun Xu, Prateek Mittal, Sunjeev R Kulkarni, and Dyan Song. 2021. Enabling efficient cyber threat hunting with cyber threat intelligence. In _2021 IEEE 37th International Conference on Data Engineering (ICDE)_. IEEE, 193-194.
* (2022) Yuejin Hu, Fuxi Zou, Jia Han, Sunjie Han, Sunjie Wang, Zoc Jin, Ting-Treat Intelligence Knowledge Graph Construction Utilizing Large Image Model. Available at SSRN 467385 (2023).
* (2021) Ghaith Humaru, Ehsi Al-Shaer, Mohiuddin Ahmed, Bill Chu, and Xi Niu. 2017. TPDH: Automatic and accurate extraction of threat actions from unstructured text of i-sources. In _Proceedings of the 33rd annual computer security applications_. IEEE, 103-115.
* (2024) IBM. 2024. What is threat hunting? [https://www.ibm.com/qrade/threat-hunting](https://www.ibm.com/qrade/threat-hunting). Accessed: 2024-05-08.
* (2025) Ramprasser Kaur, Duran Gabrijelic, and Tomat Klebunfar. 2023. Artificial intelligence for cybersecurity: Literature review and future research directions. _Information Fusion_ (2023), 1304.
* (2026) Hanbum Ko, Hongjun Yang, Schui Han, Sungwoong Kim, Sunghun Lim, and Rodrigo Hormanzad. 2026. Finding in the Gaps: LLL-Based Structured Data Generation from Semi-Structured Scientific Data. In _ICML 2020 AI for Science Workshop_.
* (2027) Jin Liu, Junjie Yan, Junjie Yifong He, Xuwen Wang, Zhengwei Jiang, Peim Yang, and Ning Li. 2022. TriCT: An actionable cyber threat intelligence discovery system via trigger-enhanced neural network. _Cybersecurity_ 5, 1 (2022), 8.
* (2028) Iguchi and Liyei Zhou. 2028. Constructing Knowledge Graph from Cyber Threat Intelligence Using Large Language Model. In _2023 IEEE International Conference on Big Data Engineering_. IEEE, 156-21.
* (2024) Baiyu Muthukkada. 2024. Selenium. [https://selenium-python.readthedocs.io/](https://selenium-python.readthedocs.io/). Accessed: 2024-05-12.
* (2029) Bobular Nour, Mohan Pourzandi, and Mourad Debbabi. 2023. A survey on threat hunting in enterprise networks. _IEEE Communications Surveys & Tutorials_ (2023).
* (2024) OpenAI. 2024. How should i set the temperature parameter? [https://platform.openai.com/dois/guise/text-generation/how-should-i-get-the-temperature-parameter-](https://platform.openai.com/dois/guise/text-generation/how-should-i-get-the-temperature-parameter-). Accessed: 2024-05-12.
* (2024) OpenAI. 2024. JSON mode. [https://platform.openai.com/docs/guides/text-generation/json-mode](https://platform.openai.com/docs/guides/text-generation/json-mode). Accessed: 2024-05-12.
* (2024) OpenAI. 2024. Models. [https://platform.openai.com/docs/models](https://platform.openai.com/docs/models). Accessed: 2024-05-12.
* (2025) Long Quynq, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Miikkin, Chong Zhang, Sandimi Agarwal, Katarina Stara, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. _Advances in neural information processing systems_ 35 (2022), 2730-2774.
* (2026) Merve Ozkan-Oazy, Erdal Alin, Omer Aalin, Schafatin Kosmolny, Teodor Iliev, Ivaylo Steynov, and Ivan Belov. 2026. A Comprehensive Survey: Evaluating the Efficiency of Artificial Intelligence and Machine Learning Techniques on Cyber Security Solutions. _IEEE Access_ (2024).
* (2027) Youngia Park and Tenseinee. 2022. Full-Suck Information Extraction System for Cyberneurality Intelligence. In _Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing Industry Track_. 531-539.
* (2028) Moumiha Das Pursha and Bill Chu. 2023. Extracting Actionable Cyber Threat Intelligence from Twitter Stream. In _2023 IEEE International Conference on Intelligence and Security Informatics_ (DSE). IEEE, 1-6.
* (2029) Md Rayham Rahman, Rearan Mahdivar Hezawa, and Laurie Williams. 2023. What are the attackers doing now? Automating cyberthreat intelligence extraction from text on page with the changing threat landscape: A survey. _Comput. Survey_ 55, 12 (2023), 1-36.
* (2020) Md Rayham Rahman, Rearan Mahdivar-Hezawa, and Laurie Williams. 2020. A literature review on mining cyberthreat intelligence from unstructured texts. In _2020 International Conference on Data Mining Workshops (ICDMW)_. IEEE, 516-525.
* (2021) Kent Ramchand, Mohan Baruwal Chehretti, and Ryszard Kowalczyk. 2021. Enterprise adoption of cloud computing with application portfolio profiling and application portfolio assessment. _Journal of Cloud Computing_ 10, 1 (2021), 1.
* (2020) Leonard Richardson. 2020. Beautiful Soup. [https://www.ccrummy.com/software/](https://www.ccrummy.com/software/). BeautifulSoup. Accessed: 2024-05-12.
* (2021) SANS. 2023. SANS 2023 CTTSurvey: Keeping Up with a Changing Threat Landscape. [https://www.sans.org/white-papers/2023-cti-survey-keeping-up/chang-threat-laans/](https://www.sans.org/white-papers/2023-cti-survey-keeping-up/chang-threat-laans/).
* (2021) hiyu Sarhan and Marco Soral. 2021. Open-X: An open cyber threat intelligence knowledge graph. _Knowledge-Based Systems_ 233 (2021), 107524.
* (2021) Kiavash Satwal, Rigel Gjom, and Vm Venkatakrishnan. 2021. Extractor: Extracting attack behavior from threat reports. In _2021 IEEE European Symposium on Security and Privacy (EuroSys)_. IEEE, 596-515.
* (2022) Taneey Satyapun, Francis Ferraro Ferraro, and Tim Finin. 2020. Csafering cybersecurity event information text. In _Proceedings of the AAAI conference on artificial intelligence_, Vol. 34, 8749-8757.
* (2023) SigmatIQ. 2024. About Sigma. [https://api.ajio/doc/guide/about.html](https://api.ajio/doc/guide/about.html).
* (2023) Gisneros Sircasma, Dougla Sarutovic, Roberto Gonzalez, Manikantan Srinivasan, Sivicana Kamnati, Watar Takahashi, Masaru Kawuki, Takahu Nakamura, and Roberto Riffica. 2023. Time for ATCH: Automated Analysis of Cyber Threat Intelligence in the Wild. _arXiv preprint arXiv:2301.10242_ (2024).
* (2024) Pontus Steenbero, Sampo Pysolsan, Goran Topic, Tomoko Ohta, Sophia Amaniolu, and Jinhui Tsuji. 2021. REAT: a web-based tool for NLP-assisted classification. In _Proceedings of the Demonstrations at the 31th Conference of the European Chapter of the Association for Computational Linguistics_. 192-107.
* (2025) Hamed Tabricchi and Marian Kuchuki Rafanjani. 2020. A survey on security challenges in cloud computing: issues, threats, and solutions. _The journal of hypercomputing_ 76, 12 (2020), 4093-5952.
* (2026) Damian A Tambur, Marco Migliorini, and Elashelu Di Nitto. 2020. Cloud applications monitoring: An industrial study. _Information and Software Technology_ 127 (2020), 106376.
* (2027) Jaiyu Wang, Pengran Mo, Weihui Ma, Peijie Sun, Min Zhang, and Jian-Yun Nie. 2024. A User-Centric Benchmark for Evaluating Large Language Models. _arXiv preprint arXiv:2404.13940_ (2020).

## Appendix A Related Work

In this section, we provide an overview of recent studies focused on unstructured OSCT1 analysis (also summarized in Table 5).

**OSCTI Analysis Techniques.** The development of efficient threat hunting mechanisms that leverage OSCTI has resulted in a wide range of research methodologies, each using different approaches to analyze and interpret OSCTI data. Within each OSCTI, key information (e.g., Lo or TTPS) is often implicit and requires the use of a different extraction approach.

NLP techniques have been utilized extensively for OSCTI analysis in methods including: Casie (2017), Extractor (2016), Open-CyKG (2018), SecE (2018), and CyberEntRel (2019). These methods leveraged advanced NLP models (e.g., BiLSTM, BERT, RoBERTa) to extract actionable insights from OSCTI text. However, to adapt these models to the cyber threat domain, a significant amount of preprocessing and fine-tuning is required. TTPDrill (2019) and THREATRAPTOR (2019)

[MISSING_PAGE_FAIL:10]

were systematically annotated by our research team to develop a robust ground truth for development and evaluation.

**Extractions and Outputs.** Previous studies produced a variety of outputs with different levels of utility and applicability. Liu et al. [19] and Purba and Chu [29] focused on extracting IoCs, while Fengrui and Du [11] extracted TTPs. The studies presenting TTP-Drill [15], Cyber [12], and aCTIon [39] converted unstructured OSCTIs into structured representations using the STNC format, which facilitates the systematic sharing and analysis of threat information. A more advanced approach was used in Extractor [37] and ThreatRaptor [13], in which threat behavior graphs are created; and in Casie [37], Open-CyKG [35], SecIE [28], LADER [6], aCTIon [20], LLM-TIKG [14], CyberEntRel [4], in which knowledge graphs are generated. Both approaches interrelate entities with associated actions and artifacts (i.e., IoCs and TTPs), providing structured insights into attack strategies through graph-based representations. While the approaches highlighted above provide valuable contextual information, further processing is required to transform the representations into actionable defense mechanisms.

To address this, in their study, Gao et al. presented a framework for converting OSCTI data into a threat behavior graph and associated domain-specific queries. Both frameworks go beyond simply identifying and contextualizing threat data, by developing operational detection rules and queries. The detection rule candidates generated by LLMcloudHunter, however, are in the known open-source _Sigma_ structure. This widely used generic signature format is inherently suitable for integration in various application environments and SIEMs. By capturing the entities, relations, IoCs, and TTPs identified in OSCTI, LLMcloudHunter translates threat intelligence into applicative _Sigma_ candidates.

## Appendix B OSCTI sources used in our research

Table 6 presents the list of OSCTI sources used in the development and evaluation of LLMCloudHunter. For each source, we provide the number of images included, the number of tokens (which serve as input to the LLM), the number of API calls, and our rating of the OSCTTs technical complexity. The complete results of the entity and relationship extraction are presented in the following tables: detection entities and relationships in Tables 7 and 8, and MITRE ATT&CK entities and relationships in Tables 9 and 10.

[MISSING_PAGE_FAIL:12]

## Appendix F Prompts

In this section, we provide the various prompts utilized throughout our proposed method in the LLMCloudHunter framework. The prompts associated with each component are described below:

* **API Call Extractor** (D in Fig. 1): This component extracts explicit and implicit API calls from the dataset. The methodology employed in this component is illustrated in Fig. 4. The explicit and implicit API call extraction prompts are shown in Figures 5 and 6, respectively. Additionally, the prompts used for the image classification and transcription sub-components are provided in Figures 7 and 8.
* **TTP Classifier** (E in Fig. 1): The prompt used for classifying threat tactics, techniques, and procedures (TTPs) is detailed in Figure 9.
* **Criticality Classifier** (F in Fig. 1): The prompt used to evaluate the criticality of specific elements is shown in Figure 10.
* **Rule Generator** (G in Fig. 1): The prompt used for generating _Sigma_ rules is provided in Figure 11.
* **Rule Optimizer** (H in Fig. 1): The prompt used for the rule optimization process is outlined in Figure 12.
* **Rule Selector** (J in Fig. 1): The prompt for selecting the most suitable rules is shown in Figure 13.

\begin{table}
\begin{tabular}{|c|c|c|c|c|c|c|c|} \hline
**OSCTI** & \multirow{2}{*}{**OSCTI Name**} & \multirow{2}{*}{**\#Images**} & \multicolumn{2}{c|}{**\#Tokens**} & \multirow{2}{*}{**\#API**} & \multirow{2}{*}{**Technical Complexity**} & \multirow{2}{*}{**1612**} \\
**ID** & & & & **No Images** & & **Images** & **Calls** & **Complexity** \\ \hline
1 & \multirow{3}{*}{1} & Anatomy of an Attack: & \multirow{3}{*}{1254} & \multirow{3}{*}{1511} & \multirow{3}{*}{11} & \multirow{3}{*}{High} & \multirow{3}{*}{High} & \multirow{3}{*}{1649} \\
1 & & Exposed keys to Crypto Mining & & & & & & & \\ \hline
197 & \multirow{3}{*}{2} & Behind the scenes in the Expel SOC: & \multirow{3}{*}{7} & \multirow{3}{*}{3136} & \multirow{3}{*}{4892} & \multirow{3}{*}{11} & \multirow{3}{*}{Medium} & \multirow{3}{*}{1649} \\
198 & & Alert-to-fix in AWS & & & & & & & \\ \hline
199 & \multirow{3}{*}{3} & Bling Libra’s Tactical Evolution: & \multirow{3}{*}{20} & \multirow{3}{*}{6414} & \multirow{3}{*}{11391} & \multirow{3}{*}{20} & \multirow{3}{*}{High} & \multirow{3}{*}{1649} \\
400 & & The Threat Actor Group Behind ShinyHunters Ransomware & & & & & & & \\ \cline{4-8}
401 & & CloudKeys in the Air: & \multirow{3}{*}{10} & \multirow{3}{*}{5792} & \multirow{3}{*}{10884} & \multirow{3}{*}{21} & \multirow{3}{*}{Low} & \multirow{3}{*}{1649} \\
402 & & Tracking Malicious Operations of Exposed IAM Keys & & & & & & \\ \hline
403 & \multirow{3}{*}{5} & Compromised Cloud Compute Credentials: & \multirow{3}{*}{1} & \multirow{3}{*}{2448} & \multirow{3}{*}{2718} & \multirow{3}{*}{51} & \multirow{3}{*}{Low} & \multirow{3}{*}{1649} \\
404 & & Case Studies From the Wild (Case 1) & & & & & & \\ \cline{4-8}
405 & & Detecting AI resource-hijacking & \multirow{3}{*}{4} & \multirow{3}{*}{2952} & \multirow{3}{*}{4078} & \multirow{3}{*}{22} & \multirow{3}{*}{Medium} & \multirow{3}{*}{1649} \\
406 & & with Composite Alerts & & & & & & \\ \cline{4-8}
407 & & Finding evil in AWS: & \multirow{3}{*}{7} & \multirow{3}{*}{2852} & \multirow{3}{*}{3814} & \multirow{3}{*}{11} & \multirow{3}{*}{Medium} & \multirow{3}{*}{1649} \\
408 & & A key pair to remember & & & & & & \\ \cline{4-8}
409 & & Incident report: From CLI to console, & \multirow{3}{*}{1} & \multirow{3}{*}{2326} & \multirow{3}{*}{3504} & \multirow{3}{*}{11} & \multirow{3}{*}{Medium} & \multirow{3}{*}{1649} \\
401 & & chasing an attacker in AWS & & & & & & \\ \hline
401 & \multirow{3}{*}{9} & Incident report: & \multirow{3}{*}{4} & \multirow{3}{*}{1984} & \multirow{3}{*}{3998} & \multirow{3}{*}{7} & \multirow{3}{*}{Medium} & \multirow{3}{*}{1649} \\
402 & & stolen AWS access keys & & & & & & \\ \hline
10 & LUCR-3: Scattered Spider & \multirow{3}{*}{2} & \multirow{3}{*}{3666} & \multirow{3}{*}{4143} & \multirow{3}{*}{20} & \multirow{3}{*}{Low} & \multirow{3}{*}{1649} \\
403 & & Getting SaaS-\(\gamma\) in the Cloud & & & & & & \\ \hline
11 & Ransomware & \multirow{3}{*}{7} & \multirow{3}{*}{4743} & \multirow{3}{*}{5931} & \multirow{3}{*}{17} & \multirow{3}{*}{High} & \multirow{3}{*}{1649} \\
404 & & in the cloud & & & & & & \\ \hline
12 & SCARLETEEL: Operation leveraging Terraform, & \multirow{3}{*}{12} & \multirow{3}{*}{3671} & \multirow{3}{*}{9764} & \multirow{3}{*}{26} & \multirow{3}{*}{Medium} & \multirow{3}{*}{1649} \\
405 & Kubernetes, and AWS for data theft & & & & & & & \\ \hline
13 & Tales from the cloud trenches: & \multirow{3}{*}{2} & \multirow{3}{*}{4784} & \multirow{3}{*}{5209} & \multirow{3}{*}{23} & \multirow{3}{*}{Medium} & \multirow{3}{*}{1649} \\
14 & Amazon ECS is the new EC2 for crypto mining & & & & & & & \\ \hline
14 & Tales from the cloud trenches: Raidding for & \multirow{3}{*}{2} & \multirow{3}{*}{2027} & \multirow{3}{*}{2310} & \multirow{3}{*}{9} & \multirow{3}{*}{Medium} & \multirow{3}{*}{1649} \\
42 & AWS vaults, buckets and secrets & & & & & & & \\ \hline
15 & Tales from the cloud trenches: Using AWS CloudTrail & \multirow{3}{*}{8} & \multirow{3}{*}{3602} & \multirow{3}{*}{5187} & \multirow{3}{*}{6} & \multirow{3}{*}{Medium} & \multirow{3}{*}{1649} \\
15 & to identify malicious activity and spot phishing campaign & & & & & & & \\ \cline{4-8}
16 & The curious case of & \multirow{3}{*}{31} & \multirow{3}{*}{7541} & \multirow{3}{*}{14465} & \multirow{3}{*}{60} & \multirow{3}{*}{Medium} & \multirow{3}{*}{1649} \\
16 & DangerDev@pprotomail.me & & & & & & & \\ \cline{4-8}
17 & Two real-life examples of why limiting permissions works: & \multirow{3}{*}{0} & \multirow{3}{*}{2160} & \multirow{3}{*}{2160} & \multirow{3}{*}{9} & \multirow{3}{*}{Low} & \multirow{3}{*}{1649} \\
17 & Lessons from AWS CIRT (Case 1) & & & & & & & \\ \hline
18 & Two real-life examples of why limiting permissions works: & \multirow{3}{*}{0} & \multirow{3}{*}{2059} & \multirow{3}{*}{7} & \multirow{3}{*}{Low} & \multirow{3}{*}{1649} \\  & Lessons from AWS CIRT (Case 2) & & & & & & & \\ \cline{4-8}
19 & Ummasking GUI-Vi: & & & & & & \\ \cline{4-8}
19 & Financially Motivated Cloud Threat Actor & \multirow{3}{*}{7} & \multirow{3}{*}{7604} & \multirow{3}{*}{9018} & \multirow{3}{*}{13} & \multirow{3}{*}{High} & \multirow{3}{*}{1649} \\
20 & When a Zero Day and Access Keys collide in the Cloud: & \multirow{3}{*}{6} & \multirow{3}{*}{4922} & \multirow{3}{*}{5743} & \multirow{3}{*}{20} & \multirow{3}{*}{High} & \multirow{3}{*}{1649} \\  & Responding to the SugarCRM Zero-Day Vulnerability & & & & & & & & \\ \hline \end{tabular}
\end{table}
Table 6. OSCTI sources used in our research.

[MISSING_PAGE_EMPTY:14]

[MISSING_PAGE_FAIL:15]

[MISSING_PAGE_FAIL:16]

Figure 4. Threat actors’ API call extraction process.

Figure 5. Explicit API Call Extraction Prompt

[MISSING_PAGE_FAIL:18]

[MISSING_PAGE_FAIL:19]

[MISSING_PAGE_FAIL:20]

[MISSING_PAGE_FAIL:21]

[MISSING_PAGE_FAIL:22]