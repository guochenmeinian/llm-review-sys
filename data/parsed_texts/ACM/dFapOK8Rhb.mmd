# Exploiting Language Power for Time Series Forecasting with Exogenous Variables

Anonymous Author(s)

###### Abstract

The World Wide Web thrives on intelligent services that depend heavily on accurate time series forecasting to navigate dynamic and evolving environments. Due to the partially-observed nature of real world, exclusively focusing on the target of interest, so-called _endogenous variables_, is insufficient for accurate forecasting, especially in web systems that are susceptible to external influences. Thus, utilizing _exogenous variables_ to harness external information, i.e., forecasting with exogenous variable (FEV), is imperative. Nevertheless, as the external environment is complex and ever-evolving, inadequately capturing external influences can even lead to learning spurious correlations and invalid prediction. Fortunately, recent studies have demonstrated that large language models (LLMs) exhibit exceptional recognition capabilities across open real-world systems, including a deep understanding of exogenous environments. However, it is difficult to directly apply LLMs for FEV due to challenges of task activation, exogenous knowledge extraction, and feature space alignment. In this work, we devise ExoLLM, an LLM-driven method to sufficiently utilize Exogenous variables for time series forecasting. We begin by Meta-task Instruction to activate the knowledge transfer of LLM from natural language processing to FEV. To comprehensively understand the intricate and hierarchical influences of exogenous variables, we propose Multi-grained Prompts, encompassing diverse external influences, including natural attributes, trend correlations, and period relationships between two types of variables. Additionally, a Dual TS-Text Attention is devised to bridge the feature gap between text and numeric data in LLM. Evaluation on real-world datasets demonstrates ExoLLM's superiority in exploiting exogenous information for forecasting with open-world language knowledge. Code is available at [https://anonymous.4open.science/r/ExoLLM](https://anonymous.4open.science/r/ExoLLM).

## CCS Concepts

* **Computing methodologies Artificial intelligence**.

## 1 Introduction

The World Wide Web, as a continuously and ever-changing physical system, heavily depends on the ability to forecast and respond to shifting patterns and user behaviors [11, 12, 15, 28]. Time series forecasting is essential to modern web technologies, utilizing historical data to anticipate future web patterns and trends [20, 25, 41]. Its predictive accuracy not only enhances user experience but also drives the development of intelligent web services, ranging from personalized content recommendations [27] and web economics modeling [5] to microservice log analysis [9]. These capabilities position time series forecasting as a cornerstone in creating adaptive, data-driven web platforms [23, 37].

Recently, deep models have achieved promising progress in time series forecasting [3, 17], with most of them focusing exclusively on the target of interest, known as _endogenous variables_, to make predictions [21, 36, 44]. This approach often ignores the influence of _exogenous variables_ from the external environment. **Exogenous variables refer to observable data within a system that are not the target variable being predicted.** As shown in Figure 1 (a), the variations within web page views (endogenous variable) are often influenced by exogenous variables, such as traffic flow, hospitalization rate, and societal events [35]. Thus, given the complex and changing physical system [48], incorporating exogenous factors, i.e., forecasting with exogenous variables (FEV) is becoming prevalent and indispensable [24].

Generally, the core of FEV is to effectively model the influence of exogenous variables on endogenous variable [4, 18, 19, 30]. Recent research in FEV proposes using attention among observed numerical exogenous series and endogenous series to capture this inherent relationship [24, 35]. Nevertheless, due to the _Intricate influences and interactions_ from external environment, relying solely on time series modality is insufficient for capturing these external influences: **(1) Multi-grained temporal dependencies**[14]. The external influences and interactions from exogenous variables is multi-grained, such as periodicity and trends, which can be reflected by various aspects including complex human behaviors and living habits [46]. It is difficult to model such changing and diverse impact only by observed numeric [20], highlighting the necessity of thoroughly learning multi-grained temporal features to effectively model these intricate patterns [14]. **(2) Spurious correlation**[10]. Noise and interventions in current data can lead to learning biased external influence, thereby affecting the accuracy of forecasting results [34]. For example, traffic flows are positively correlated with exogenous weather variables, but mandatory controls can lead to less traffic even when the weather is good, resulting in spurious association that may be learned by models. Without any external knowledge from real world, a high prediction uncertainty tends to be inevitable [47].

Consequently, designing more intelligent and robust FEV framework that enable models to effectively understand the intricateexternal influence and avoid spurious correlation is in demand. Fortunately, with rapid development of large language models (LLMs) (Bahdanau et al., 2015), there have been more opportunities to leverage the vast language knowledge to comprehend external influence on endogenous variables. Through extensive training on large-scale text corpora, pre-trained LLMs have extensively acquired knowledge of multi-grained correlation between two types of variables. Intuitively, empowering FEV with these full-scale external knowledge can significantly enhance forecasting accuracy (Zhu et al., 2017). Nevertheless, as shown in Figure 1 (b), considering distinct task differences between NLP and time series forecasting (Bahdanau et al., 2015; Zhu et al., 2017), and distant data gap between discrete text and continues numeric (Bahdanau et al., 2015), employing LLMs to FEV faces several urgent challenges: **(1) Task activation.** How to construct task instruction to fully activate the potential of LLMs in FEV, enabling the knowledge transfer across tasks. **(2) Full-scale language-driven knowledge acquirement.** Given an LLM-based solution, how to devise effective and comprehensive prompts to acquire hierarchical and sufficient knowledge from exogenous variables. **(3) Feature space alignment.** Given the solution is concerned with two data modalities of both numerical and text data, how to construct a feasible encoding-decoding strategy to ensure the alignment between text space and time series space.

In this work, we devise ExoLLM to forecast with Exogenous variables using LLM, capturing diverse and changing external influences from exogenous variables with language-based knowledge. Technically, we elaborately craft domain-specific Meta-task Instructions to guide LLMs to process FEV tasks in different data domains. Subsequently, we establish Multi-grained Prompts to dynamically capture the natural attributes, periodic associations, trend correlations, and other granular external influence of exogenous variables, thereby adaptive transferring the dynamic auxiliary information into knowledge that can be understood by ExoLLM. Additionally, we design the Dual Time series-Text Attention Attention (DT\({}^{2}\)Attention) to mitigate data discrepancies during time series encoding and feature decoding, respectively. Comprehensive evaluation demonstrates that LLM can even act as an effective few-shot and zero-shot FEV learners when adopted through our elaborate design, outperforming specialized forecasting models. Our meticulous design enables LLMs to function even as a proficient few-shot and zero-shot FEV learner, surpassing specialized forecasting models in terms of effectiveness, as demonstrated by the comprehensive evaluation. Our contributions can be summarized as follows:

* Given the complex and evolving external environment of real-world system, i.e., web service, traffic, electricity and weather, we introduce LLMs to maximally explore the auxiliary information of exogenous variables.
* We propose ExoLLM, the first LLM-based forecasting model to accomplish FEV: 1) To fully exploit the potential of LLM in FEV, we elaborately design Meta-task Instruction and Multi-grained Prompt, realizing the pre-trained knowledge transfer from NLP to FEV and integrate dynamic context information into knowledge of time-series domain. 2) To deal with the distant data gap between discrete text and continues numeric, we design modality-aware encoding and decoding mechanisms, i.e., DT\({}^{2}\)Attention, to achieve aligned feature before and after LLM encoding.
* ExoLLM demonstrates outstanding predictive performance across various real scenarios, including long-term, short-term, few-shot, and zero-shot forecasting. Quantitatively, ExoLLM outperforms 10 state-of-the-art models for long-term forecasting, achieving top-1 performance in 51 settings and top-2 in 5 settings out of a total of 56 settings. In addition, ExoLLM reduces MAE by an average of 4.1%, 5.2%, and 4.5% in short-term, few-shot, and zero-shot forecasting tasks, respectively.

## 2. Related Work

### Forecasting with Exogenous Variables

In practical forecasting scenarios, the utilization of exogenous variables as auxiliary information for forecasting endogenous variables is more prevalent. Previous research has explored statistical methods such as ARIMAX (Zhu et al., 2017) and SARIMAX (Zhu et al., 2017), which understand relationships between exogenous and endogenous series along with auto-regression. Additionally, deep learning models like

Figure 1. (a) Illustration of Knowledge Reserve from Pre-trained LLM: The extensive pre-trained text data endows LLMs with the potential to understand intricate influence of exogenous variables on web page views. (b) Huge Gaps in Feature Space and Tasks: Text embeddings and time series features are usually mapped to different feature spaces, and it is challenging to fine-tune text-generation pre-trained LLM for FEV.

NBEATSx (Xiong et al., 2017) and TiDE (Xiong et al., 2017) argue that forecasting models can leverage future values of exogenous variables during the forecasting endogenous variables. Notably, TimeXer (Xiong et al., 2017) introduces external information into transformer architectures through well-designed embedding strategies to effectively incorporate external information into segmented representations of endogenous variables, accommodating temporal lags or missing data records. However, these approaches rely on establishing auxiliary information only based on numeric correlation between exogenous and endogenous series. In contrast, ExoLLM has the capability to extract multi-grained effects of exogenous variables on endogenous ones as auxiliary information from extensive world knowledge, thereby holding significant potential for enhancing accuracy and generalization in FEV.

### LLM-based Forecasting

The recent emergence of LLMs has opened up new possibilities for time series forecasting (Xiong et al., 2017; Zhang et al., 2017). GPT4TS (Xiong et al., 2017) utilizes a pre-trained language model without updating its self-attention and feedforward layers. The model undergoes fine-tuning and evaluation across various time series analysis tasks, demonstrating comparable or state-of-the-art performance by leveraging knowledge transfer from natural language pre-training. LLM4TS (Bahdanau et al., 2014) adopts a two-stage fine-tuning approach on the LLM to fully leverage time series data. TimelLM (Tai et al., 2016) introduces the concept of text prototypes and reprograms time series based on these prototypes to align them more naturally with language models. Tempo (Chen et al., 2016) decomposes the trend, seasonality, and residual components of time series while dynamically selecting prompts to address comprehension challenges for LLMs. UniTime (Xiong et al., 2017) proposes a language-empowered unified model to efficiently capture knowledge from cross-domain time series data. With their extensive knowledge base, LLMs exhibit tremendous potential in time series forecasting. However, as shown in Table 1, there has been no prior research exploiting LLM for forecasting with exogenous variables (FEV) to enhance the prediction accuracy. To address this gap, we propose ExoLLM which harnesses the power of language to capture the influence of exogenous variables on endogenous variables.

## 3. Problem Definition

In forecasting with exogenous variables, there is a historical endogenous series \(\mathbf{X}\in\mathbb{R}^{1\times L}\) and its associated exogenous information \(\mathbf{E}\), where \(L\) is look-back window size. Concretely, \(\mathbf{E}\in\mathbb{R}^{M\times L}\) comprises multiple exogenous variables \(\{\mathbb{E}^{(1)},\mathbb{E}^{(2)},\ldots,\mathbb{E}^{(M)}\}\), where \(M\) is the variable num and \(\mathbb{E}^{(m)}\in\mathbb{R}^{1\times L}\) is the \(m\)-th exogenous series. Our goal is to learn a forecasting model \(\ell\) (\(\cdot\)), which predicts the future \(T\) time steps of endogenous series \(\widehat{\mathbf{X}}\in\mathbb{R}^{1\times T}\), based on its historical observation \(\mathbf{X}\) and the exogenous variables \(\mathbf{E}\).

## 4. Methodology

The detailed framework of ExoLLM is illustrated in Figure 2. Firstly, the Meta-task Instruction (MTI) and Multi-grained Prompt (MGP) text are embedded using frozen large language model to get uniform size embedding. Then, exogenous and endogenous series will be tokenized by shared Temporal-property preserved Tokenizer (TPT) to preserve temporal properties. Furthermore, a mainly frozen pre-trained LLM is utilized to integrate exogenous knowledge into endogenous token. It's worth noting that a Dual TS-Text Attention (DT\({}^{2}\)Attention) is devised to align TS-Text feature space before and after LLM encoding, which enables the model to aware of specific modality. The output endogenous token wil be finally mapped to the future time series by a lightweight forecasting head.

### Language-driven Exogenous Knowledge Utilization

#### Meta-task Instruction

To activate the knowledge transfer of LLM from nature language processing (NLP) to FEV, it is necessary to construct task instructions as guidance. As illustrated in Figure 3, the meta-task instruction comprises three key elements: (1) Overall description and analysis of dataset, offering explicit domain identification information to the model. (2) Brief summary of endogenous and exogenous variables, facilitating model to discern the source of each variables. (3) Introduction to the FEV task, fully activating LLM to accomplishing forecasting task with exogenous variables. We aim to activate the LLM's FEV capability in different domains through carefully designed meta-task instructions.

#### Multi-grained Prompt

To comprehensively understand the external environment of Entire systems, we need to consider not only

\begin{table}
\begin{tabular}{c c c c c c c c c} \hline \hline \multirow{2}{*}{Method} & **ExoLLM** & **AntiTimes** & **TimeLLM** & **LLM4TS** & **UnTime** & **LLM4Time** & **TEST** & **TEMPO** & **GPT4TS** \\  & **(Ours)** & (2024) & (2023) & (2023) & (2024) & (2023) & (2023) & (2023) & (2023) \\ \hline Exogenous Variables & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ & ✗ \\ Multimodal & ✓ & ✓ & ✓ & ✗ & ✗ & ✓ & ✓ & ✗ \\ Feature Alignment & ✓ & ✗ & ✓ & ✗ & ✗ & ✗ & ✗ & ✗ \\ \hline \hline \end{tabular}
\end{table}
Table 1. Comparison between prior LLM-based time series forecasting models and ExoLLM.

Figure 2. Overall architecture of ExoLLM, which consists of Dual TS-Text Attention and pre-trained LLM to sufficiently exploit exogenous variables in FEV.

the apparent data correlation between numerical exogenous and endogenous variables, but also the natural properties, constant relationships, sequential trends, period influences, stability, and other multicolver factors. Therefore, we design multi-grained prompts (MGP) to exploit the LLM's comprehensive knowledge of the world to a diversified understanding of a specific environment. As shown in Table 2, the multi-grained prompt mainly consists of two elements: (1) Revealing the natural attribute of exogenous variables and their essential correlation with endogenous variables, endowing the model with prior knowledge of external environment. (2) Describing the dynamic characteristics of exogenous endogenous series in term of trends, period, stability, and noise intensity, enabling the model to consider dynamic external influences. Intuitively, MGP not only deepens the LLM's understanding of exogenous variables, but also enhances the LLM's perception of the external invisible environment.

#### Uniform-scale Text Encoding

After constructing the meta-task instruction and multi-grained prompt, the next step involves encoding the text to obtain embeddings of uniform dimensions. To integrate these text with adequate language knowledge, we use a pre-trained LLM to encode these text descriptions. Since the text length of each prompt is different, we design an ingenious method to obtain the same embedding size. Particularly, we add a special token <_EOS_> at the end of the prompt. Since all the previous tokens are visible to <_EOS_> throughout the causal attention in LLM, the embedding of <_EOS_> could represent the entire text. The text encoding process is given by:

\[\mathbf{PT} = \text{SelectLast}(\text{LLM}(\text{TD}, \text{<}EOS\text{>})), \tag{1}\]

where SelectLast(\(\cdot\)) denotes selecting the embedding of the last <_EOS_> token, LLM(\(\cdot\)) represents encoding part of large language model, \(\mathbf{TD} = \{t_{\text{dask}},td_{\text{exo}}^{(1)},td_{\text{exo}}^{(2)},...,td_{\text{ exo}}^{(M)},td_{\text{end}}\}\) is text description set of Meta-task Instruction and Multi-grained Prompt. \(\mathbf{PT} = \{p_{\text{task}},p_{\text{exo}}^{(1)},p_{\text{exo}}^{(2)},...,p_{\text{ exo}}^{(M)},p_{\text{end}}\}\) represents the uniform-scale text embeddings of TD, where \(p_{\text{task}} \in \mathbb{R}^{1\times D}\) is the embedding of meta-task instruction, \(p_{\text{exo}}^{(i)} \in \mathbb{R}^{k\times D}\) is the \(i\)-th exogenous prompt embedding set, \(p_{\text{end}} \in \mathbb{R}^{k\times D}\) is endogenous prompt embedding set, \(k\) is multi-grained prompt number of one-type variable and \(D\) is the uniform hidden dimension.

### Temporal-property Preserved Tokenizer

To facilitate LLM's understanding of the different types of variable series, we need to compress each series into a single token. Recent studies (Zhou et al., 2017) use a linear layer to embed the entire time series as a token. However, this embedding approach neglects the temporal properties of data, resulting in the model's incomplete understanding of the relationships between exogenous and endogenous series. Therefore, we devise a Temporal-property Preserved Tokenizer (TPT) to obtain tokens reserving the temporal characteristics. Firstly, we partition the exogenous variables \(\mathbf{E}\) and endogenous variables \(\mathbf{X}\) into non-overlapping patches to enhance the local semantics at each time step (Zhou et al., 2017), resulting in \(\mathbf{P}_{\text{end}} \in \mathbb{R}^{1\times N\times P}\) and \(\mathbf{P}_{\text{exo}} \in \mathbb{R}^{M\times N\times P}\), where \(P\) is patch length, and \(N = \frac{1}{p}\) is the corresponding numbers of patches. To compress the temporal representations, TPT employs Self-Attention to learn temporal interactions among patches and selects the the last patch as the output:

\[\mathbf{TK}_{*}^{time} = \text{SelectLast}(\text{Self-Attn}(\text{PE} + \mathbf{P}_{*})), \tag{2}\]

where Self-Attn(\(\cdot\)) denotes self-attention applied to time series, PE represents the position embedding, SelectLast(\(\cdot\)) denotes the operation of selecting the last patch, \(\mathbf{P}_{*}\) is patched exogenous or endogenous series and \(\mathbf{TK}_{end}^{time}\) is the corresponding token. Selecting the last patch as the token representation of the entire series is justified by two reasons. (1) It interacts with all preceding patches through attention, thus possessing sequence-level temporal information; (2) It is closest to the future sequence, providing crucial near-term information. Finally, we obtain exogenous tokens \(\mathbf{TK}_{\text{exo}}^{time} \in \mathbb{R}^{M\times D}\) and endogenous token \(\mathbf{TK}_{end}^{time} \in \mathbb{R}^{1\times D}\) in the time series feature space.

### Knowledge-retained LLM Encoder

Understanding the exogenous impact on endogenous variables is crucial for time series forecasting. We utilize meta-task instruction along with tokenized exogenous and endogenous variables to LLMs to fully exploit the prior knowledge in LLMs, thereby forming enhanced representations of endogenous token:

\[\mathbf{TK}_{end}^{llm} = \text{LLM}((pt_{task},\mathbf{TK}_{\text{exo}},\mathbf{TK}_{end})), \tag{3}\]

where LLM(\(\cdot\)) denotes the encoder part of LLM. Each variable is treated as a token, and exogenous and endogenous variables are concatenated in a fixed order to form a "sentence" in a fixed order, like \([pt_{task},\mathbf{TK}_{\text{exo}}^{(1)},\mathbf{TK}_{\text{exo}}^{(2)},..., \mathbf{TK}_{\text{exo}}^{(M)},\mathbf{TK}_{\text{end}}]\). Following (Zhou et al., 2017), we freeze the positional embedding layers and self-attention blocks in LLM to retain majority of learned knowledge from language pre-training. Ultimately, we obtain an exogenous variable enhanced representation of endogenous token, \(\mathbf{TK}_{end}^{llm} \in \mathbb{R}^{1\times D}\), which encapsulates rich information from prior exogenous knowledge.

### Feature Alignment with Dual TS-Text Attention

Given that LLM is pre-trained on discrete textual data and lack exposure to continuous numerical values, directly inputting tokens \(\mathbf{TK}_{\text{exo}}^{time}\) and \(\mathbf{TK}_{end}^{time}\) in time series featrue space into LLMs would increase the difficulties in understanding never-seen modality, thus resulting in degraded predictive performance. Besides, the output token from LLM in text space is difficult to decode into future series. Thus, a Dual TS-Text Attention (DT\({}^{2}\)Attention) is devised to align ts-text feature space before and after LLM encoder, respectively.

Figure 3. Example of Meta-task Instruction and headlines of Multi-grained Prompt in ETTh1.

#### TS-Text Attention

Intuitively, there should be a certain distinction between exogenous and endogenous tokens to avoid over-smoothing representation among different types of tokens, and absorb certain prior external knowledge to enhance the LLM's encoding ability. Thus, a TS-Text Attention is designed to achieve: 1) Mapping tokens from time series feature space to text feature space; 2) Distinguishing between endogenous and exogenous Tokens. Specifically, for any type of token, TS-Text Attention designs its Query as token in time series space, while the Key and Value are its corresponding multi-grained prompt. Then, we perform Cross Attention to align tokens:

\[\mathbf{TK}_{\epsilon}^{text}=\text{Cross-Attn}(\mathbf{TK}_{\epsilon}^{time}, \mathbf{PT}_{\epsilon},\mathbf{PT}_{\epsilon}). \tag{4}\]

where \(\mathbf{TK}_{\epsilon}^{time}\in\mathbb{R}^{D}\) is the exogenous/endogenous token in time series space, \(\mathbf{PT}_{\epsilon}\in\mathbb{R}^{k\times D}\) is this variable's corresponding multi-grained prompt, \(\mathbf{TK}_{\epsilon}^{text}\in\mathbb{R}^{D}\) is the mapped token in text space and will be input into LLM in Eq (3).

#### Text-TS Attention

Denote \(\mathbf{TK}_{end}^{\textit{llm}}\in\mathbb{R}^{1\times D}\) as the endogenous token encoded by LLM encoder in Eq (3). The \(\mathbf{TK}_{end}^{\textit{llm}}\) remains in the text space, directly decode \(\mathbf{TK}_{\epsilon}^{\textit{llm}}\) for forecasting faces the challenge of converting textual semantics into time series. Thus, we use Text-TS Attention to alleviate such problem, decoding \(\mathbf{TK}_{\textit{end}}^{\textit{llm}}\) into time series space based on the temporal information of exogenous series. This can be expressed as:

\[\mathbf{TK}_{\textit{dec}}^{\textit{llm}}=\text{Cross-Attn}(\mathbf{TK}_{ \textit{end}}^{\textit{llm}}.\mathbf{TK}_{\textit{exco}}^{time},\mathbf{TK}_{ \textit{exco}}^{time}), \tag{5}\]

where \(\mathbf{TK}_{\textit{exco}}^{\textit{llm}}\) represents exogenous variables in time series space, \(\mathbf{TK}_{\textit{dec}}^{\textit{llm}}\in\mathbb{R}^{1\times D}\) is the decoded endogenous token. Through this approach, we can better utilize the representation capability of LLMs and combine exogenous series to enhance the endogenous forecasting.

### Lightweight Forecasting Head

Considering the richness of the encoded token and maximumly preserving exogenous information by LLMs, a simple linear layer is employed to transform \(\mathbf{TK}_{\textit{dec}}^{\textit{llm}}\) for forecasting:

\[\widehat{\mathbf{X}}=\text{Linear}(\mathbf{TK}_{\textit{dec}}^{\textit{llm}}). \tag{6}\]

where \(\widehat{\mathbf{X}}\in\mathbb{R}^{1\times T}\) is the future endogenous series.

## 5. Experiments

### Dataset and Experimental Settings

To verify the model's effectiveness, we extensively evaluate our proposed ExoLLM on a diverse range of FEV scenarios, including long-term, short-term, few-shot and zero-shot task.

#### Datasets and Experimental Setups

To completely evaluate the FEV capability of ExoLLM, we conduct experiments on \(12\) real-world datasets. These datasets are collected from web and especially the exogenous factors retrieved from are in the formation of language. In particular, seven well-established public long-term datasets from different domains, and five short-term datasets in electricity price are involved in our FEV experiments. The endogenous and exogenous variables of each dataset are summarized in detail in Appendix A.1. For short-term forecasting datasets, the input length is set as \(168\) and prediction length is \(24\). For long-term forecasting datasets, the input length is set as \(96\) and prediction length varies \(\{96,192,336,720\}\). More implementation details can be found at Appendix A.2.

#### Baselines

We compare ExoLLM with \(10\) baselines, which comprise the state-of-the-art forecasting methods, including LLM-based model: LLM4TS [(2)], GPT4TS [(45)], TimeLLM [(14)], Transformer-based model: TimeXer [(35)], TextTST [(26)],ITransformer [(21)], Transformer [(43)], AutoFormer [(39)], CNN-based model: S:ICNet [(19)], and Linear-based model: T1DE [(6)]. Among these models, TimeXer and

\begin{table}
\begin{tabular}{c c c} \hline \hline Characteristics & Prompts & \\ \hline \multirow{4}{*}{Nature Attribute} & 1 & This Exogenous variable is High UeLess Load, representing external load that is inefficiently utilized. \\  & 2 & Exogenous High UeLess Load indicates a potential inefficiency in the system’s external load handling. \\  & 3 & Exogenous High UeLess Load can lead to increased energy consumption without corresponding output. \\  & 4 & Exogenous High UeLess Load might suggest that the system is operating under suboptimal external conditions. \\ \hline \multirow{3}{*}{Trend} & 3 & Exogenous High UeLess Load series shows an overall upward trend \\  & 4 & Exogenous High UeLess Load series initially rises and then declines. \\  & 5 & Exogenous High UeLess Load series inhibits an overall declining trend \\  & 6 & Exogenous High UeLess Load series initially declines and then rises \\ \hline \multirow{4}{*}{Period} & 4 & Exogenous High UeLess Load series has no apparent periodicity. \\  & 5 & Exogenous High UeLess Load series exhibits shorter periodicity and higher frequency \\  & 6 & Exogenous High UeLess Load series displays clear periodicity. \\  & 7 & Exogenous High UeLess Load series exhibits relatively longer periodicity. \\ \hline \multirow{4}{*}{Stability} & 6 & Exogenous High UeLess Load series undergoes significant instability over all the time. \\  & 6 & Exogenous High UeLess Load series remains relatively stable while minimal fluctuations \\  & 6 & Exogenous High UeLess Load series experiences occasional bouts of volatility, interspersed with periods of relative calm. \\  & 6 & Exogenous High UeLess Load series shows consistent stability, with values remaining close to a steady mean. \\ \hline \multirow{4}{*}{Noise Intensity} & 6 & Exogenous High UeLess Load series is subject to very strong noise interference \\  & 6 & Exogenous High UeLess Load series has a low signal-to-noise ratio, where noise significantly affects the clarity of the underlying data. \\  & 6 & Exogenous High UeLess Load series experiences moderate noise, partially obscuring the underlying pattern. \\  & 6 & Exogenous High UeLess Load series is not influenced by any noise interference. \\ \hline \hline \end{tabular}
\end{table}
Table 2. An example of Multi-grained Prompt of one variable in ETH1. Orange is chosen from exogenous and endogenous. Green is the variable name. Blue is prior knowledge about the variable’s nature attribute. Black is the fixed template.

[MISSING_PAGE_FAIL:6]

### Efficiency Analysis

We have compared ExoLLM with other LLM-based and linear-based methods in term of running time, and the results are provided in Table 7. As demonstrated, ExoLLM significantly reduces computational costs since it does not require repetitive text encoding during training. It saves considerable computational time compared to TimeLLM and is even comparable to Dlinear. We will discuss the computational efficiency of ExoLLM from theoretical perspectives: (a)The LLM used for frozen text embeddings does not participate in forward computation or backpropagation during each training iteration. For a given dataset, its MTI and MGP components are fixed, meaning their embeddings can be precomputed and stored on disk. During training, these embeddings only need to be loaded into memory, resulting in zero additional training time for this part. (b) The TPT (Time Patch Tokenization) used for encoding time-series features reduces the sequence length by a factor of \(P\) (where \(P\) is the patch length). This reduces the theoretical time complexity from \(O(L^{2})\) to \(O(\frac{2}{P})\). Thus, ExoLLM is designed with a strong emphasis on resource efficiency, making it more computationally economical in practice.

### Ablation Study

We conduct ablation studies by removing each module from ExoLLM on six datasets. **w/o MGP** removes Multi-grained Prompt (MGP). **w/o MTI** removes Meta-task Instruction (MTI). **w/o DT\({}^{2}\)A** removes Dual TS-text Attention (DT\({}^{2}\)Attention) for feature space alignment. **w/o TPT** replaces Temporal-property Preserved Tokenizer (TPT), which could preserve temporal properties for each token, with a linear layerr. We analyze the results shown in Table 8. The observations are listed as follows: Obs.1) Removing MGP and MTI results in the most significant decrease in prediction metrics, 77 emphasizing their strong ability in activating LLM in FEV. Obs.2) DT\({}^{2}\)Attention also significantly improves the model performance, demonstrating the importance of feature space alignment. Obs.3) TST constantly promotes the forecasting accuracy, suggesting that reserving temporal properties in each token is needed.

### Exogenous Scale Analysis

Real-world time series often encounter challenges such as the absence of crucial exogenous data. In this section, we employ random masking to simulate these scenarios and further investigate the forecasting performance. As illustrated in Figure 4 (a) and (b), We

\begin{table}
\begin{tabular}{c|c

[MISSING_PAGE_FAIL:8]

## References

* (1)
* Cao et al. (2023) Defu Cao, Furoong Jia, Sercan O Arik, Tomas Pfister, Yixiang Zheng, Wen Ye, and Yan Liu. Tempo: "Toronto-based generative pre-trained transformer for time series forecasting." _arXiv preprint arXiv:2310.08498_, 2023.
* Chang et al. (2023) Ching Chang, Wen-Chih Peng, and Tian-T Gu. Chen. "Lithx-two-stage fine-tuning for time-series forecasting with pre-trained films." _arXiv preprint arXiv:2308.08469_, 2023.
* Chen et al. (2024) Feiyi Chen, Zhen Qin, Mengbin Zhou, Yingying Zhang, Shuiguang Deng, Lanting Fan, Guanhong Fang, and Qingsong Wen. Lara: A light and anti-overflowing retraining approach for unsupervised time series anomaly detection. In _Proceedings of the ACM on Web Conference 2024_, pages 4138-4149, 2024.
* Chen and Billings (1990) Sheng Chen and Steve A Billings. Representations of non-linear systems: the narma method. _International journal of control_, 49(10):1032-1032, 1990.
* Colcari et al. (2024) Zimato Colcari, Federico Cimus, Giannarco De Francisci Morales, and Michele Starnini. Navigating multidimensionalologies with reddit's political compass: Economic conflict and social affinity. In _Proceedings of the ACM on Web Conference 2024_, pages 2582-2593, 2024.
* Das et al. (2023) Ablimanyu Das, Weihao Kong, Andrew Leach, Shan K Mathur, Rajat Sen, and Rose Yu. Long-term forecasting with tide: Time-series dense encoder. _Transactions on Machine Learning Research_, 2023.
* Ehaymakers et al. (2019) Kevin Ehaymakers, How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings. _arXiv preprint arXiv:1908.00512_, 2019.
* Gruver et al. (2023) Nate Gruver, Marc Fini, Shikai Qin, and Andrew Gordon Wilson. Large language models are zero-shot time series forecasts. _arXiv preprint arXiv:2310.07820_, 2023.
* Guo et al. (2024) Yunda Guo, Jialke Gue, Pameng Guo, Yungjen Chi, Tao Li, Mengbin Shi, Yang Tu, and Jian Onyang. Pass Prediction auto-scaling system for large-scale enterprise web applications. In _Proceedings of the ACM on Web Conference 2024_, pages 2727-2755, 2024.
* Ding et al. (2003) Brian Diao, Shuai Tu, Shuai Tu, and Shuai Tu. "Inverse prediction" _Understanding Statistics: Statistical Issues in Psychology, Education, and the Social Sciences_, 2(2):125-132, 2003.
* Jin et al. (2022) Shieo Yun Jin, Jieboen Lee, Miyao J, Seungj Kooi, Hsiang Jeon, Jihyeon Jiyeon, Jiyeon Kim, and Noeonga Park. Exit: Extrapolation and interpolation-based neural controlled differential equations for time-series classification and forecasting. In _Proceedings of the ACM Web Conference 2022_, pages 3102-3112, 2022.
* Jiang et al. (2023) Berthe Jiang, Zhoxuan Wang, Yudong Tao, Chuang Yang, Xuan Song, Ryvonke Shibasaki, Siba-Ching Chen, and Mei-Ling Shyu. Learning social meta-knowledge for nonrotating human mobility in disaster. In _Proceedings of the ACM Web Conference 2023_, pages 2655-2665, 2023.
* Ding et al. (2023) Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Duan, James Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuang-Liang Li, Shuiu Pan, et al. Time-Time Time series forecasting by reprogramming large language models. _arXiv preprint arXiv:2302.04170_, 2023.
* Jin et al. (2024) Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Y Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuang-Tuan Li, Shuai Pan, et al. Timing-Lithx: Time series forecasting by reprogramming large language models. In _International Conference on Learning Representations (ICLR)_, 2024.
* Kamarishi et al. (2022) Harsharayan Kamarishi, Linghui Kong, Alexander Rodriguez, Chao Zhang, and B Aditya Prakash. Camml: Calibrated and accurate multi-view time-series forecasting. In _Proceedings of the ACM Web Conference 2022_, pages 3174-3185, 2022.
* Lago et al. (2021) Jews Lago, Greggore Marcjasz, Bart De Schutter, and Rafal Weron. Forecasting day-ahead electricity prices: A review of state-of-the-art algorithms, best practices and an open-access benchmark. _Applied Energy_, 2978:116003, 2021.
* Li et al. (2024) Zhichen Li, Huan Li, Dalin Zhang, Yan Zhao, Mukun Qian, and Christian S Jensen. Exaol: Efficient-effective unsupervised state detection for multi-variate time series. In _Proceedings of the ACM on Web Conference 2024_, pages 3901-3921, 2024.
* Lin et al. (2022) Bryan Lin, Sercan O Arak, Nicolas Loeff, and Tomas Pfister. Temporal fusion transformers for interpretable multi-horizon time series forecasting. _International Journal of Forecasting_, 37(1):4718-4763, 2022.
* Liu et al. (2022) Minhao Liu, Aling Zeng, Musi Chen, Zhijian Xu, Qixia Lai, Lingma Ma, and Qiang Xu. Scinet: Time series modeling and forecasting with sample convolution and interaction. _Advances in Neural Information Processing Systems_, 35:S1816-S282, 2022.
* Liu et al. (2024) Xu Liu, Junfeng Hu, Yuan Li, Shirlie Diao, Yuxuan Liang, Bryan Hoel, and Roger Zimmermann. Ultimate A language-empowered unified model for cross-domain time series forecasting. In _Proceedings of the ACM on Web Conference 2024_, pages 4095-4106, 2024.
* Lu et al. (2021) Yong Lu, Tengge Hu, Haon Zhang, Haixu Wu, Shiyu Wang, Junhao Ma, and Mingheng Long. Transformer: Inverted transformer as effective for time series forecasting. In _The Twelfth International Conference on Learning Representations_, 2024.
* Liu et al. (2024) Yong Liu, Guo Qin, Xiangdong Huang, Jianmin Wang, and Mingxheng Long. Automotressive time series forensactivates via large language models. _arXiv preprint arXiv:2402.02370_, 2024.
* Long et al. (2020) Qingqing Long, Zheng Fang, Chen Fang, Chong Chen, Pengfei Wang, and Yuanzhou Zhou. Unveiling delay effects in traffic forecasting: A perspective from spatial-temporal delay differential equations. In _Proceedings of the ACM on Web Conference 2024_, pages 1035-1044, 2020.
* Lu et al. (2021) Jecheng Lu, Xu Han, Yan Sun, and Shihuo Yang. Cats: Enhancing multivariate time series forecasting by constructing auxiliary time series as exogenous variables. _arXiv preprint arXiv:2010.06173_, 2021.
* Miyake et al. (2021) Kentaro Miyake, Hiroyoshi Ito, Christos Paloutsos, Hirotomo Matsumoto, and Auyuki Morishima. Network: Social network forecasting using multi-agent reinforcement learning with interpretable features. In _Proceedings of the ACM on Web Conference 2024_, pages 2524-2521, 2024.
* Ng et al. (2020) Yong Nae, Nam H Nguyen, Pharavoe Sintosh, and Jayant Kalngmann. A time series is worth 64 words: Long-term forecasting, with transformers. In _International Conference on Learning Representations_, 2020.
* Ning et al. (2022) Wentao Ning, Reynold Cheng, Xiao Yan, Ben Kao, Nan Huo, Nur Al Hasan Hidar, and Tao D Tang. Debiasing recommendation with personal popularity. In _Proceedings of the ACM on Web Conference 2022_, pages 3806-3809, 2024.
* Obata et al. (2020) Kobei Obata, Koki Kawabata, Yasuike Matsubara, and Yasushi Sakurai. Dynamic multi-network mining of tensor time series. In _Proceedings of the ACM on Web Conference 2020_, pages 4117-4127, 2020.
* Olivares et al. (2008) Kita O Olivares, Cristian Challa, Greggore Marcjasz, Rafal Weron, and Arthur Dukawski. Neural basis expansion analysis with exogenous variables: Forecasting electricity prices with hebates. _International Journal of Forecasting_, 39(2):884-800, 2008.
* Oreshkin et al. (2019) Boris N Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-bents: Neural basis expansion analysis for interpretable time series forecasting. _arXiv preprint arXiv:1905.10237_, 2019.
* Sun et al. (2007) Chenxi Sun, Yulang Li, Hongyun Li, and Sheanda Hong. Text prototype 1007 aligned embedding to activate m2s ability for time series. _arXiv preprint arXiv:2008.08241_, 2020.
* Tournovi et al. (2012) Hugo Tournovi, Thibaut Lavril, Gautier Iacard, Xavier Martinet, Marie-Anne Latour, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama Open and efficient foundation language models. _arXiv preprint arXiv:2102.10971_, 2023.
* Vapropoulos et al. (2016) Stylianos I Vapropoulos, G Chioulias, Evagologos G Kardakos, Christos K Simoglu, and Anastasio G Bakiriakis. Comparison of arimax, arimax and un-based models for short-term generation forecasting. In _2016 IEEE international energy conference (IEEE/INCON)_, pages 1-6, IEEE, 2016.
* Wang et al. (2003) Biawu Wang, Yudong Zhang, Jiahao Shi, Pengkun Wang, Xu Wang, Lei Bai, and Yang Wang. Knowledge expansion and consolidation for continual traffic prediction with expanding graphs. _IEEE Transactions on Intelligent Transportation Systems_, 2023.
* Wang et al. (2017) Yuxuan Wang, Haixu Wu, Jiauxuo Dong, Yong Liu, Yunhong Qiu, Haoran Zhang, Jianmin Wang, and Mingxeng Long. Timney: Improving transformers for time series forecasting with exogenous variables. _arXiv preprint arXiv:2007.19724_, 2020.
* Wang et al. (2024) Zezin Wang, Changhua Pei, Minghua Ma, Xin Wang, Zhixuan Li, Dan Pei, Saravan Rajamohan, Dongjian Zhang, Qingwei, Haiming Zhang, and A. Revisiting vae for unsupervised time series anomaly detection: A frequency perspective. In _Proceedings of the ACM on Web Conference 2024_, pages 3096-3105, 2024.
* Wei et al. (2023) Wei, Chen Huang, Lianghao Xu, and Chunx Zhang. Multi-modal self-supervised learning for recommendation. In _Proceedings of the ACM Web Conference 2023_, pages 7908-790, 2023.
* Williams (2001) Billy M Williams. Multivariate vehicular traffic flow prediction: evaluation of arimax modeling. _Transportation Research Record_, 177(14):194-200, 2001.
* Wu et al. (2021) Haixu Wu, Jianxin Yu, Jianxin Wang, and Mingxeng Long. Autodformer: Decomposition transformers with auto-correlation for long-term series forecasting. _Advances in Neural Information Processing Systems_, 34:2249-2230, 2021.
* Wang et al. (2011) Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingxeng. Timney: Temporal 2d-variation modeling for general time series analysis. In _International Conference on Learning Representations_, 2021.
* Xu et al. (2013) Wentao Xu, Weiqing Liu, Chang Xu, Jiang Bian, Jian Yin, and Tie-Yan Liu. Best: Relational event-driven stock trend forecasting. In _Proceedings of the web conference 2013_, pages 1-10, 2013.
* Zhang et al. (2021) Susan Zhang, Stephen Roller, Namao Qazi, Mikael Artless, Moyen Chen, Shuahui Chen, Christopher Dewan, Mona Diah, Xin Li, Xi Yuotti Lin, et al. Opt: Open pre-trained transformer language models. _arXiv preprint arXiv:2202.01068_, 2022.
* Zhang and Vanhais (2018) Yunhao Zhang and Junich Vanhais. Transformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting. In _International Conference on Learning Representations_, 2023.
* Zhou et al. (2021) Haoyu Zhou, Shanghang Zhang, Jiegi Feng, Shuai Zhang, Jianxin Liu, Hui Xiang, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In _Proceedings of the AAAI conference on artificial intelligence_, volume 35, pages 11106-11115, 2021.
* Zhou et al. (2023) Tian Zhou, Peisong Niu, Xue Wang, Liang Sun, and Song Jin. One fits all: Power general time series analysis by pretrained lm. 2023.

* [46] Zhengyang Zhou, Yang Wang, Xike Xie, Lianliang Chen, and Hengchang Liu. Riskorder: A mimitext evelyutivate traffic accident forecasting framework. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 1258-1265, 2020.
* [47] Zhengyang Zhou, Yang Wang, Xike Xie, Lei Qiao, and Yuntao Li. Stunnet: Understanding uncertainty in spatiotemporal collective human mobility. In _Proceedings of the Web Conference 2012_, pages 1868-1879, 2021.
* [48] Zhengyang Zhou, Qike Huang, Kuo Yang, Kun Wang, Xu Wang, Yudong Zhang, Yuxuan Liang, and Yang Wang. Maintaining the status quo: Capturing invariant relations for end spatiotemporal learning. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, KDD '29, page 3603-3614, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9789480701030, doi: 10.1145/358035.3599421. URL [https://doi.org/10.1145/358035.3599421](https://doi.org/10.1145/358035.3599421).
* [49] Zhengyang Zhou, Yang Wang, Xike Xie, Lianliang Chen, and Hengchang Liu. Riskorder: A mimitext evelyutivate traffic accident forecasting framework. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 1258-1265, 2020.
* [50] Zhengyang Zhou, Yang Wang, Xike Xie, Lei Qiao, and Yuntao Li. Stunnet: Understanding uncertainty in spatiotemporal collective human mobility. In _Proceedings of the Web Conference 2012_, pages 1868-1879, 2021.
* [51] Zhengyang Zhou, Qike Huang, Kuo Yang, Kun Wang, Xu Wang, Yudong Zhang, Yuxuan Liang, and Yang Wang. Maintaining the status quo: Capturing invariant relations for end spatiotemporal learning. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, KDD '29, page 3603-3614, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9789480701030, doi: 10.1145/358035.3599421. URL [https://doi.org/10.1145/358035.3599421](https://doi.org/10.1145/358035.3599421).
* [52] Zhengyang Zhou, Yang Wang, Xike Xie, Lianliang Chen, and Hengchang Liu. Riskorder: A mimitext evelyutivate traffic accident forecasting framework. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 1258-1265, 2020.
* [53] Zhengyang Zhou, Yang Wang, Xike Xie, Lei Qiao, and Yuntao Li. Stunnet: Understanding uncertainty in spatiotemporal collective human mobility. In _Proceedings of the Web Conference 2012_, pages 1868-1879, 2021.
* [54] Zhengyang Zhou, Qike Huang, Kuo Yang, Kun Wang, Xu Wang, Yudong Zhang, Yuxuan Liang, and Yang Wang. Maintaining the status quo: Capturing invariant relations for end spatiotemporal learning. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, KDD '29, page 3603-3614, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9789480701030, doi: 10.1145/358035.3599421. URL [https://doi.org/10.1145/358035.3599421](https://doi.org/10.1145/358035.3599421).
* [55] Zhengyang Zhou, Yang Wang, Xike Xie, Lianliang Chen, and Hengchang Liu. Riskorder: A mimitext evelyutivate traffic accident forecasting framework. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 1258-1265, 2020.
* [56] Zhengyang Zhou, Yang Wang, Xike Xie, Lei Qiao, and Yuntao Li. Stunnet: Understanding uncertainty in spatiotemporal collective human mobility. In _Proceedings of the Web Conference 2012_, pages 1868-1879, 2021.
* [57] Zhengyang Zhou, Qike Huang, Kuo Yang, Kun Wang, Xu Wang, Yudong Zhang, Yuxuan Liang, and Yang Wang. Maintaining the status quo: Capturing invariant relations for end spatiotemporal learning. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, KDD '29, page 3603-3614, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9789480701030, doi: 10.1145/358035.3599421. URL [https://doi.org/10.1145/358035.3599421](https://doi.org/10.1145/358035.3599421).
* [58] Zhengyang Zhou, Yang Wang, Xike Xie, Lianliang Chen, and Hengchang Liu. Riskorder: A mimitext evelyutivate traffic accident forecasting framework. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 1258-1265, 2020.
* [59] Zhengyang Zhou, Yang Wang, Xike Xie, Lei Qiao, and Yuntao Li. Stunnet: Understanding uncertainty in spatiotemporal collective human mobility. In _Proceedings of the Web Conference 2012_, pages 1868-1879, 2021.
* [60] Zhengyang Zhou, Qike Huang, Kuo Yang, Kun Wang, Xu Wang, Yudong Zhang, Yuxuan Liang, and Yang Wang. Maintaining the status quo: Capturing invariant relations for end spatiotemporal learning. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, KDD '29, page 3603-3614, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9789480701030, doi: 10.1145/358035.3599421. URL [https://doi.org/10.1145/358035.3599421](https://doi.org/10.1145/358035.3599421).
* [61] Zhengyang Zhou, Yang Wang, Xike Xie, Lianliang Chen, and Hengchang Liu. Riskorder: A mimitext evelyutivate traffic accident forecasting framework. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 1258-1265, 2020.
* [62] Zhengyang Zhou, Yang Wang, Xike Xie, Lei Qiao, and Yuntao Li. Stunnet: Understanding uncertainty in spatiotemporal collective human mobility. In _Proceedings of the Web Conference 2012_, pages 1868-1879, 2021.
* [63] Zhengyang Zhou, Qike Huang, Kuo Yang, Kun Wang, Xu Wang, Yudong Zhang, Yuxuan Liang, and Yang Wang. Maintaining the status quo: Capturing invariant relations for end spatiotemporal learning. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, KDD '29, page 3603-3614, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9789480701030, doi: 10.1145/358035.3599421. URL [https://doi.org/10.1145/358035.3599421](https://doi.org/10.1145/358035.3599421).
* [64] Zhengyang Zhou, Yang Wang, Xike Xie, Lianliang Chen, and Hengchang Liu. Riskorder: A mimitext evelyutivate traffic accident forecasting framework. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 1258-1265, 2020.
* [65] Zhengyang Zhou, Yang Wang, Xike Xie, Lei Qiao, and Yuntao Li. Stunnet: Understanding uncertainty in spatiotemporal collective human mobility. In _Proceedings of the Web Conference 2012_, pages 1868-1879, 2021.
* [66] Zhengyang Zhou, Qike Huang, Kuo Yang, Kun Wang, Xu Wang, Yudong Zhang, Yuxuan Liang, and Yang Wang. Maintaining the status quo: Capturing invariant relations for end spatiotemporal learning. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, KDD '29, page 3603-3614, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9789480701030, doi: 10.1145/358035.3599421. URL [https://doi.org/10.1145/358035.3599421](https://doi.org/10.1145/358035.3599421).
* [67] Zhengyang Zhou, Yang Wang, Xike Xie, Lianliang Chen, and Hengchang Liu. Riskorder: A mimitext evelyutivate traffic accident forecasting framework. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 1258-1265, 2020.
* [68] Zhengyang Zhou, Yang Wang, Xike Xie, Lei Qiao, and Yuntao Li. Stunnet: Understanding uncertainty in spatiotemporal collective human mobility. In _Proceedings of the Web Conference 2012_, pages 1868-1879, 2021.
* [69] Zhengyang Zhou, Qike Huang, Kuo Yang, Kun Wang, Xu Wang, Yudong Zhang, Yuxuan Liang, and Yang Wang. Maintaining the status quo: Capturing invariant relations for end spatiotemporal learning. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, KDD '29, page 3603-3614, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9789480701030, doi: 10.1145/358035.3599421. URL [https://doi.org/10.1145/358035.3599421](https://doi.org/10.1145/358035.3599421).
* [70] Zhengyang Zhou, Qike Huang, Kuo Yang, Kun Wang, Xu Wang, Yudong Zhang, Yuxuan Liang, and Yang Wang. Maintaining the status quo: Capturing invariant relations for end spatiotemporal learning. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, KDD '29, page 3603-3614, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9789480701030, doi: 10.1145/358035.3599421. URL [https://doi.org/10.1145/358035.3599421](https://doi.org/10.1145/358035.3599421).
* [71] Zhengyang Zhou, Yang Wang, Xike Xie, Lianliang Chen, and Hengchang Liu. Riskorder: A mimitext evely traffic accident forecasting framework. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 1258-1265, 2020.
* [72] Zhengyang Zhou, Yang Wang, Xike Xie, Lei Qiao, and Yuntao Li. Stunnet: Understanding uncertainty in spatiotemporal collective human mobility. In _Proceedings of the Web Conference 2012_, pages 1868-1879, 2021.
* [73] Zhengyang Zhou, Qike Huang, Kuo Yang, Kun Wang, Xu Wang, Yudong Zhang, Yuxuan Liang, and Yang Wang. Maintaining the status quo: Capturing invariant relations for end spatiotemporal learning. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, KDD '29, page 3603-3614, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9789480701030, doi: 10.1145/358035.3599421. URL [https://doi.org/10.1145/358035.3599421](https://doi.org/10.1145/358035.3599421).
* [74] Zhengyang Zhou, Yang Wang, Xike Xie, Lianliang Chen, and Hengchang Liu. Riskorder: A mimitext evelyutivate traffic accident forecasting framework. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 1258-1265, 2020.
* [75] Zhengyang Zhou, Yang Wang, Xike Xie, Lianliang Chen, and Hengchang Liu. Riskorder: A mimitext evely traffic accident forecasting framework. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 1258-1265, 2020.
* [76] Zhengyang Zhou, Yang Wang, Xike Xie, Lianliang Chen, and Hengchang Liu. Riskorder: A mimitext evely traffic accident forecasting framework. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 1258-1265, 2020.
* [777] Zhengyang Zhou, Yang Wang, Xike Xie, Lianliang Chen, and Hengchang Liu. Riskorder: A mimitext evely traffic accident forecasting framework. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 1258-1265, 2020.
* [78] Zhengyang Zhou, Yang Wang, Xike Xie, Lianliang Chen, and Hengchang Liu. Riskorder: A mimitext evely traffic accident forecasting framework. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 1258-1265, 2020.
* [79] Zhengyang Zhou, Yang Wang, Xike Xie, Lianliang Chen, and Hengchang Liu. Riskorder: A mimitext evely traffic accident forecasting framework. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 1258-1265, 2020.
* [80] Zhengyang Zhou, Yang Wang, Xike Xie, Lianliang Chen, and Hengchang Liu. Riskorder: A mimitext evely traffic accident forecasting framework. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 1258-1265, 2020.
* [81] Zhengyang Zhou, Yang Wang, Xike Xie, Lianliang Chen, and Hengchang Liu. Riskorder: A mimitext evely traffic accident forecasting framework. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 1258-1265, 2020.
* [82] Zhengyang Zhou, Yang Wang, Xike Xie, Lianliang Chen, and Hengchang Liu. Riskorder: A mimitext evely traffic accident forecasting framework. In _Proceedings of the AAAI conference on artificial intelligence_, volume 34, pages 1258-

## Appendix A Experimental Details

### Dataset Descriptions

Seven real-world datasets are used in long-term FEV to evaluate ExoLLM, including: (1) **ETT**[44] consists of two hourly-level datasets (ETTh) and two 15minute-level datasets (ETTm). Each of them contains seven oil and load features of electricity transformers from July 2016 to July 2018. The endogenous variable is the oil temperature and the exogenous variables are 6 power load features. (2) **Weather**[44] includes 21 indicators of weather, such as air temperature, and humidity. Its data is recorded every 10 min for 2020 in Germany. In our experiment, we use the Wet Bulb factor as the endogenous variable to be predicted and the other indicators as exogenous variables. (3) **ECL**[39] contains hourly electricity consumption (in Kwh) of 321 clients from 2012 to 2014. The electricity consumption of the last client is token as an endogenous variable and other clients as exogenous variables. (4) **Traffic**[40] describes hourly road occupancy rates measured by 862 sensors on San Francisco Bay area freeways from 2015 to 2016. Te measurement of the last sensor is token as an endogenous variable and others as exogenous variables.

Five short-term datasets in term of electricity price [16] is used in short-term FEV, including: (1) **NP** represents The Nord Pool electricity market, recording the hourly electricity price, and corresponding grid load and wind power forecast from 2013-01-01 to 2018-12-24. (2) **PJM** represents the Pennsylvania-New Jersey-Maryland market, which contains the zonal electricity price in the Commonwealth Edison (COMED), and corresponding System load and COMED load forecast from 2013-01-01 to 2018-12-24. (3) **BE** represents Belgium's electricity market, recording the hourly electricity price, load forecast in Belgium, and generation forecast in France from 2011-01-09 to 2016-12-31. (4) **FR** represents the electricity market in France, recording the hourly prices, and corresponding load and generation forecast from 2012-01-09 to 2017-12-31. (5) **DE** represents the German electricity market, recording the hourly prices, the zonal load forecast in the TSO Amprion zone, and the wind and solar generation forecasts from 2012-01-09 to 2017-12-31.

### Implementation Details

All experiments are conducted using PyTorch on 2 NVIDIA H100 PCIe 80GB GPUs. We try using models such as LLaMa-7B [32], GPT-2 [7],OPT-1.3B [42], as large language models and find that their effects are in little difference. Considering the lightweight of time series forecasting and avoid data leakage issue, GPT-2 is finally selected as the backbone of LLM in the reported results. We utilize the ADAM optimizer with L2 loss for model optimization, adjusting the batch size from 32 to 512 to maximize GPU memory utilization. Grid search is performed for learning rates, exploring values in [1e-2, 1e-3, 5e-3, 5e-4] corresponding to different datasets. Early stop of training occurs if the validation loss did not decrease for 10 consecutive rounds. The patch length is set as 8 across all datasets. The max training epoch number is set as 50. For a fair comparison, we set the input length to 96 for long-term forecasting and 168 for short-term forecasting, aligning with all baseline models.

## Appendix B Key Differences and Advantages of ExoLLM

In fact, ExoLLM is fundamentally different from other LLM-based methods. **1. Different Task Focus:** ExoLLM specifically focuses on leveraging exogenous information to enhance the predictability of the target variables. It introduces innovative components such as MGP and MTL, which significantly improve the LLM's understanding of exogenous variables, rather than merely relying on a single prompt to guide LLM. **2. Resource Efficiency:** Unlike other large language models that require LLM involvement in forward computation for text encoding. ExoLLM allows the embedding from MGP to be pre-computed and stored, significantly reducing computational time. As shown in Table 2 of the supplementary one-page PDF, ExoLLM's computational time is significantly lower, significantly shorter than other TimeLLM and competitive to other linear-based models. **3. Different Objective:** The goal of ExoLLM is to generalize across various real-world forecasting scenarios by leveraging open-world knowledge (i.e., superior insights into exogenous variables) to improve predictive performance in different contexts, a capability not shared by other LLM-based methods. **4. Appropriate Encoding:** Other LLM-based models often lack fine-grained, targeted designs for encoding different data modalities, risking suboptimal data utilization. In contrast, ExoLLM employs a tailored TFT to tokenize sequences while preserving temporal features. Additionally, the DT2Attention mechanism enables dual alignment between temporal and textual modalities during both LLM encoding and decoding, enhancing data usability--an area where other approaches fall short.

Our ExoLLM provides a paradigm to activate the power of LLM that is encompassed with textualized knowledge in open world to help better understand structural data of time-series. This can be potentially extended to more general structural and tabular data learning on various domains.

## Appendix C Necessity of Language Power

We demonstrated the importance of using LLMs for FEV from both a theoretical and practical perspective. Theoretical justification: LLMs provide a crucial foundation for leveraging open-world knowledge to understand the impact of exogenous variables on endogenous variables. During pre-training, LLMs acquire extensive linguistic knowledge, which includes essential prior knowledge. This knowledge can be embedded into the prediction process, enhancing the model's understanding of the dynamic system. Experimental Results: In the ablation study presented in Section 5.4,

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \hline Dataset & Office & ExoLDMertification & ExoLDMertification & ExoLDMertification & ExoLDMertification \\ \hline \hline \multirow{2}{*}{Baseline} & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \cline{2-6}  & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \hline \multirow{2}{*}{Baseline} & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \cline{2-6}  & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \hline \multirow{2}{*}{Baseline} & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \cline{2-6}  & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \hline \multirow{2}{*}{Baseline} & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \cline{2-6}  & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \hline \multirow{2}{*}{Baseline} & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \cline{2-6}  & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \hline \multirow{2}{*}{Baseline} & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \cline{2-6}  & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \hline \multirow{2}{*}{Baseline} & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \cline{2-6}  & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \hline \multirow{2}{*}{Baseline} & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \cline{2-6}  & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \hline \multirow{2}{*}{Baseline} & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \cline{2-6}  & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \cline{2-6}  & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \hline \multirow{2}{*}{Baseline} & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \cline{2-6}  & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \hline \multirow{2}{*}{Baseline} & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \cline{2-6}  & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \cline{2-6}  & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \hline \multirow{2}{*}{Baseline} & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \cline{2-6}  & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \cline{2-6}  & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \hline \multirow{2}{*}{Baseline} & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \cline{2-6}  & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \cline{2-6}  & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \hline \multirow{2}{*}{Baseline} & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \cline{2-6}  & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \cline{2-6}  & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \hline \multirow{2}{*}{Baseline} & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \cline{2-6}  & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \cline{2-6}  & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) & \(\Delta\) \\ \hline \multirow{2}{*}{Baseline} & \(\Delta\)we examine the effects of removing MGP and MTI to assess the contribution of textual information to FEV prediction. The results showed that removing MGP and MTI led to the most significant decrease in prediction metrics, highlighting their critical role in activating LLMs for FEV.

## Appendix D Data Integrity and Leakage Prevention

We consider much during the initial design of our model, particularly ensuring that ExoLLM is free from data leakage. (1)No Data Leakage in Practice: As outlined in A.2 Implementation Details, ExoLLM uses GPT-2 as its backbone. GPT-2 was trained on WebText, a dataset created by OpenAI using text from Reddit posts that linked to highly-rated external content prior to 2018. The datasets used in our study were all released after 2019, meaning they have in overlap with the WebText corpus. Thus, there is no potential for data leakage in principle. (2) Testing Results Confirm No Data Leakage: To further verify the absence of data leakage, we tested GPT-2 with queries about the datasets used in our study. The model was unable to provide any relevant descriptions of these datasets, indicating that the pre-trained GPT-2 has not encountered this data before. This confirms that data leakage is not an issue. Our contribution fundamentally lies in leveraging LLMs to utilize existing knowledge about open world, enabling more practical time series forecasting. Experimental results demonstrate the effectiveness of ExoLLM on datasets where no data leakage is present.

## Appendix E Full Results

The results presented in Tables 2, 3, and 4 highlight the predictive advantages of ExoLLM in utilizing exogenous variables. ExoLLM demonstrates strong adaptability across few-shot, long-term, and zero-shot forecasting tasks by effectively capturing multi-grained temporal dependencies. Its ability to maintain high performance even under limited (few-shot) or unseen (zero-shot) data conditions underscores the model's flexibility and generalization capabilities, making it highly applicable to dynamic, real-world scenarios.

[MISSING_PAGE_EMPTY:13]

[MISSING_PAGE_EMPTY:14]

[MISSING_PAGE_EMPTY:15]