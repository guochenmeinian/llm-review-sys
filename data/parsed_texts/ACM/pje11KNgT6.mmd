# Cluster Aware Graph Anomaly Detection

Anonymous Author(s)

###### Abstract.

Graph anomaly detection has gained significant attention across various domains, particularly in critical applications like fraud detection in e-commerce platforms and insider threat detection in cybersecurity. Usually, these data are composed of multiple types (e.g., user information and transaction records for financial data), thus exhibiting view heterogeneity. However, in the era of big data, the heterogeneity of views and the lack of label information pose substantial challenges to traditional approaches. Existing unsupervised graph anomaly detection methods often struggle with high-dimensionality issues, rely on strong assumptions about graph structures or fail to handle complex multi-view graphs. To address these challenges, we propose a cluster aware multi-view graph anomaly detection method, called CARE. Our approach captures both local and global node affinities by augmenting the graph's adjacency matrix with the pseudo-label (i.e., soft membership assignments) without any strong assumption about the graph. To mitigate potential biases from the pseudo-label, we introduce a similarity-guided loss. Theoretically, we show that the proposed similarity-guided loss is a variant of contrastive learning loss, and we present how this loss alleviates the bias introduced by pseudo-label with the connection to graph spectral clustering. Experimental results on several datasets demonstrate the effectiveness and efficiency of our proposed framework. Specifically, CARE outperforms the second-best competitors by more than 39% on the Amazon dataset with respect to AUPR and 18.7% on the YelpChi dataset with respect to AUROC. The code of our method is available at the anonymous GitHub link: [https://anonymous.4open.science/r/CARE-demo-1C7F](https://anonymous.4open.science/r/CARE-demo-1C7F).

## 1. Introduction

Graph-based anomaly detection has been an important research area across diverse domains for decades, particularly within high-impact applications, such as fraud detection within e-commerce platforms (Shen et al., 2017; Wang et al., 2018; Wang et al., 2018) and insider threat detection in the cybersecurity domain (Bengio et al., 2017; Wang et al., 2018; Wang et al., 2018). For instance, in the realm of e-commerce, leveraging a graph-based anomaly detection algorithm proves invaluable for identifying fraudulent sellers by analyzing the properties (i.e., attributes) and connections (i.e., structure) among users (Wang et al., 2018). Similarly, in the context of insider threat detection, constructing a graph based on users' activities allows investigators to discern anomalous users in the organization by exploring the substructure of the graph (Wang et al., 2018).

In the era of big data, the collected data often exhibit heterogeneous views (e. g., various data) and lack labeled data. For example, in the e-commerce platform, multiple types of data can be collected for heterogeneous graph construction, including the user's shopping history, search trends, and product ratings (Wang et al., 2018); in credit card fraud detection, the data are composed of both cardholder information and transaction records (e. g., online purchase records) (Wang et al., 2018). However, obtaining labels is often impractical due to the expense associated with labeling services and the demand for domain-specific expertise in discerning malicious patterns (Wang et al., 2018). This highlights the need for innovative approaches that can deal with the intricacies of heterogeneous and unlabeled datasets.

Until then, many unsupervised anomaly detection methods (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018) have been proposed and they can be categorized into two branches, including feature reconstruction methods and self-supervised learning methods. The feature reconstruction approaches focus on minimizing the reconstruction error of node attributes or structures (Bengio et al., 2017; Wang et al., 2018; Wang et al., 2018). However, these feature reconstruction based methods tend to suffer from the curse of high dimensionality (Wang et al., 2018), especially for citation network where the word occurrence is extracted as the node attributes. Another direction is the self-supervised learning methods (Bengio et al., 2017; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018), which aim to design a proxy task related to anomaly detection, whereas these methods tend to have strong assumptions regarding the graph structure, thus only performing well on some graphs with certain structures. For instance, TAM (Wang et al., 2018) holds the _one-class homophily assumption_ that normal nodes tend to have much stronger affinity with each other than with the abnormal nodes and the authors propose to maximize the local node similarity. However, TAM fails to consider a situation where the normal nodes and their normal neighbors might come from different classes, indicating the distinct features for these connected normal nodes. A general limitation for both branches is that many of these methods (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018) are designed for single-view graphs, suffering from the presence of multiple views.

To address these limitations, we propose a unified Cluster AwAE graph anomaly detection method to identify anomalous nodes in multi-view graphs, named CARE. In this work, we propose to capture both local and global node affinity and design an anomaly score function to assigning higher anomaly scores to nodes that are less similar to their neighbors based on both node attributes and structural similarity. Since the raw adjacency matrix in the graph only contains the local node affinity information, we measure the global node affinity by leveraging the pseudo-label (i.e., soft membership assignments) to augment the original adjacency matrix without any strong assumption about the graph. To reduce the potential bias introduced by the pseudo-label during the optimization, we propose a similarity-guided loss that utilizes the soft assignment to build a similarity map to help the model learn robust representations. Theoretically, we analyze that the proposed similarity-guided loss is a variant of the contrastive loss, and we present how the proposed regularization mitigates the potential bias by connecting it withgraph spectral clustering. Our main contributions are summarized below:

* A novel self-supervised framework for detecting anomalies in multi-view graphs.
* A novel similarity-guided contrastive loss for learning graph contextual information and its theoretical analysis showing the connection to graph spectral clustering.
* Theoretical analysis showing the negative impact of other types of contrastive loss.
* Experimental results on six datasets demonstrating the effectiveness and efficiency of the proposed framework.

The rest of this paper is organized as follows. After briefly reviewing the related work in Section 2, we then introduce a multi-view graph anomaly detection framework in Section 3. Next, we conduct the systematic evaluation of the proposed framework on several datasets in Section 4 before we conclude the paper in Section 5.

## 2. Related Work

In this section, we briefly review the related work on clustering and anomaly detection.

### Clustering

In the past decades, clustering methods gradually evolved from traditional shallow methods, (e. g., Non-Negative Matrix Factorization (NMF) methods (He et al., 2016; Li et al., 2017; Li et al., 2018) and spectral clustering methods (Li et al., 2017; Li et al., 2018; Li et al., 2019)), to deep learning-based methods (e. g., deep NMF (Wang et al., 2018; Li et al., 2019), graph neural network clustering methods (Li et al., 2017; Li et al., 2018; Li et al., 2019), autoencoder-based clustering methods (Li et al., 2017; Li et al., 2018)). For example, the authors of Li et al. (2019) propose an NMF-based clustering method that models the intrinsic geometrical structure of the data by assuming that several neighboring points should be close in the low dimensional subspace. The work in (He et al., 2016) presents a semi-NMF clustering method by taking advantage of the mutual reinforcement between data reduction and clustering tasks. Additionally, Li et al. (2019) extends shallow NMF to a deep Semi-NMF for multi-view clustering by learning the hierarchical structure of multi-view data and maximizing the mutual information of each pair of views. Different from these methods, this paper first follows the idea of the graph pooling method (Wang et al., 2018) to get the soft assignment, aiming to capture the global node affinity information and regularize the learned representations by the similarity-guided contrastive loss.

### Anomaly Detection

Anomaly detection has been studied for decades (Li et al., 2017; Li et al., 2018; Li et al., 2019; Li et al., 2019). The increasing demand in many domains, such as financial fraud detection, anomaly detection in cybersecurity, etc., has attracted many researchers' attention, and a variety of outstanding algorithms have been proposed, ranging from shallow algorithms (Li et al., 2017; Li et al., 2018; Li et al., 2019; Li et al., 2019; Li et al., 2019) to deep models (He et al., 2016; Li et al., 2019; Li et al., 2019; Li et al., 2019; Li et al., 2019). To name a few, (Li et al., 2019) presents an algorithm named Outlier Pursuit, which projects the raw data to the low-dimensional subspace and identifies the corrupted points with PCA; (Li et al., 2019) encodes observed co-occurrence in different events into a hidden space and utilizes the weighted pairwise interactions of different entity types to define the event probability. AnoGAN (He et al., 2016) is a generative adversarial network (GAN) based anomaly detection method, which utilizes the generator and the discriminator to capture the normal patterns while the anomalies are detected based on the residual score and the discrimination score. HCNA-A (Li et al., 2019) designs a self-supervised learning loss by forcing the prediction of the shortest path length between pairs of nodes. ComGA (Li et al., 2019) proposes a community-aware attributed graph anomaly detection method to detect community structure of the graph. CONAD (Li et al., 2019) integrates human knowledge of different anomaly types via data augmentation and introduces a contrastive learning-based method for graph anomaly detection. TAM (Li et al., 2019) proposes a scoring measure by assigning large score to the nodes that are less affiliated with their neighbors and introduce truncated affinity maximization to reduce the bias during the optimization. In contrast, this paper proposes to capture both local and global node affinity information, and we propose similarity-guided contrastive learning loss to learn robust representations and to mitigate potential bias.

## 3. Proposed CARE Framework

In this section, we present our proposed framework, CARE, for multi-view graph anomaly detection. We begin by defining the notation and then introduce cluster-aware node affinity learning alongside the similarity-guided contrastive learning loss. Next, we outline the overall objective function and the inference process for detecting anomalies. Finally, we analyze the limitations of using weakly supervised contrastive loss in our method.

### Notation

Throughout this paper, we use regular letters to denote scalars (e. g., \(a\)), boldface lowercase letters to denote vectors (e. g., \(\mathbf{x}\)), and boldface uppercase letters for matrices (e. g., \(\mathbf{A}\)). Given an undirected graph \(\mathcal{G}=(V,\mathbf{E}^{1},...,\mathbf{E}^{o},\mathbf{X}^{1},...,\mathbf{X}^{n})\), our objective is to identify anomalous nodes in the graph, where \(\mathbf{e}\) represents the number of views, \(\mathbf{V}\) consists of \(n\) vertices, \(\mathbf{E}^{o}\) consists of \(m^{o}\) edges, \(\mathbf{X}^{o}\in\mathbb{R}^{n\times d_{o}}\) denotes the feature matrix of the \(o\)-th view and \(d_{o}\) is the feature dimension. For clarity, we denote \(u_{i}\) as node \(i,\mathbf{x}_{i}^{o}\in\mathbb{R}^{d_{o}}\) as the node attributes of \(u_{i}\) for the \(o\)-th view, \(\mathbf{h}_{i}\in\mathbb{R}^{d}\) as the embedding of node \(u_{i}\) by any type of GNNs and \(d\) is the feature dimensionality of the hidden representation \(\mathbf{H}^{o}\in\mathbb{R}^{n\times d^{o}}\) is the node embedding matrix. We let \(\mathbf{A}^{o}\in\mathbb{R}^{n\times n}\) denote the adjacency matrix of the \(o\)-th view where \(\mathbf{A}_{ij}^{o}=1\) iff node \(u_{i}\) and node \(u_{j}\) are connected, \(\mathbf{D}^{o}\in\mathbb{R}^{n\times n}\) denotes the diagonal matrix of vertex degrees for the \(o\)-th view, and \(\mathbf{I}\in\mathbb{R}^{n\times n}\) denotes the identity matrix. The symbols are summarized in Table 1.

### Cluster-Aware Node Affinity Learning

Many self-supervised learning methods (Li et al., 2017; Li et al., 2018; Li et al., 2019; Li et al., 2019; Li et al., 2019; Li et al., 2019) aim to design a proxy task relevant to the anomaly detection. One branch (Li et al., 2019; Li et al., 2019) is to maximize the local node similarity for a multi-view graph as follows:

\[\mathcal{L}_{1} =\sum_{a=1}^{p}\sum_{u_{i}}\frac{1}{|\mathbf{N}^{a}(t)|}\sum_{u_{j}\in \mathbf{N}^{a}(t)}\operatorname{sim}(\mathbf{h}_{i}^{a},\mathbf{h}_{j}^{a})\] \[=\sum_{a=1}^{p}\sum_{u_{i}}\sum_{u_{j}}\frac{1}{\mathbf{D}_{i}^{a}} \mathbf{A}_{ij}^{a}\cdot\operatorname{sim}(\mathbf{h}_{i}^{a},\mathbf{h}_{j}^{a}) \tag{1}\]where \(\sin(\mathbf{h}_{i}^{a},\mathbf{h}_{j}^{a})=\frac{\mathbf{h}_{i}^{a}(\mathbf{h}_{j}^{a})^{T}}{|\mathbf{h }_{i}^{a}||\mathbf{h}_{j}^{a}|}\) measures the similarity of node embedding between the node \(u_{i}\) and its neighbor \(u_{j}\) based on the \(a\)-th view of the graph, \(\mathbf{N}^{a}(i)\) denotes the neighbors of the node \(u_{i}\) and \(\mathbf{D}_{i}^{a}\) is the degree of the node \(u_{i}\) for the \(a\)-th view. However, these methods typically rely on _one-class homophily assumption_, which posits that normal nodes tend to exhibit strong affinities with each other, while the affinities among abnormal nodes are significantly weaker. This assumption is overly restrictive, as it focuses exclusively on extracting local node affinities while neglecting global node affinities.

A naive solution to relax this constraint is to incorporate high-order information by extending the local \(1\)-hop neighbors to \(k\)-hop neighbors. However, this approach presents two main issues. First, including high-order neighbors inevitably introduces more abnormal nodes during the node affinity maximization process, resulting in a sub-optimal solution. Second, it overlooks a crucial scenario where normal anchor nodes and their neighbors may belong to different classes, indicating that these connected normal nodes may have distinct features. Since both normal neighbors from different classes and abnormal neighbors possess features that differ from those of normal anchor nodes, incorporating these distinct features in node affinity learning decreases the likelihood of detecting abnormal nodes. Furthermore, adding high-order neighbors in node affinity learning also increases the probability of including neighbors from different classes, exacerbating the problem. To address this issue, we propose incorporating label information into the local node affinity maximization as follows:

\[\mathcal{L}_{2}=\sum_{a=1}^{u}\sum_{u_{i}}\sum_{u_{j}}\frac{1}{\sum_{u_{j}}(\bm {A}_{ij}^{a}+\mathbf{S}_{ij}^{a})}(\mathbf{A}_{ij}^{a}+\mathbf{S}_{ij}^{a})\cdot\sin(\mathbf{h }_{i}^{a},\mathbf{h}_{j}^{a}) \tag{2}\]

where \(S_{ij}=1\) if node \(u_{i}\) and node \(u_{j}\) belong to the same class and \(S_{ij}=0\) otherwise. Compared to \(\mathcal{L}_{1}\), \(\mathcal{L}_{2}\) further encodes the label information in the node affinity learning. However, label information is often unavailable due to the high costs of labeling services and the rapid growth of new data. To address this issue, we propose replacing the unavailable label information with pseudo-labels derived from a graph clustering method. Following the idea of differential graph pooling (Wang et al., 2017), we employ a one-layer Graph Convolutional Network (GCN) (Shi et al., 2016) with a softmax activation function to model soft membership assignments as follows:

\[\mathbf{M}^{a} =\text{GCN}^{a}(\mathbf{A}^{a},\mathbf{X}^{a},\mathbf{W}^{a})\] \[\tilde{\mathbf{M}} =\sum_{a=1}^{b}\sum_{a=1}^{a}\mathbf{M}^{a} \tag{3}\]

where \(\mathbf{W}^{a}\in\mathbb{R}^{d_{a}\times c}\) is the weight matrix of GCN for the \(a\)-th view and \(c\) is the number of clusters and we aggregate soft membership assignments from all views to obtain \(\tilde{\mathbf{M}}\). We then augment the adjacency matrix by incorporating the clustering results as follows:

\[\tilde{\mathbf{A}}=(1-a)\tilde{\mathbf{A}}+\alpha\tilde{\mathbf{M}}\tilde{\mathbf{M}}^{T} \tag{4}\]

where \(\tilde{\mathbf{A}}=\frac{1}{5}\sum_{a=1}^{a}\mathbf{A}^{a}\) and \(\alpha\in[0,1]\) is a hyper-parameter balancing the importance between raw adjacency matrix and the similarity of the soft membership assignments. Our goal is to maximize the cluster-aware node affinity as follows:

\[\mathcal{L}_{A}(u_{i})=\sum_{u_{j}}\frac{1}{\tilde{D}_{i}}\tilde{\mathbf{A}}_{ij} \cdot\sin(\tilde{\mathbf{h}}_{i},\tilde{\mathbf{h}}_{j})+\sum_{a=1}^{a}||\mathbf{h}_{i}^{ a}-\tilde{\mathbf{h}}_{i}||_{2} \tag{5}\]

where \(\tilde{D}_{i}=\sum_{u_{j}}\tilde{A}_{ij}\) measures the degree of node \(u_{i}\) in the new adjacency matrix and \(\tilde{\mathbf{h}}_{i}=\frac{1}{5}\sum_{a=1}^{a}\mathbf{h}_{i}^{a}\) is the average node representation. The second term enforces consistency in node embeddings across all views. However, optimization using Eq. 5 may be significantly biased by low-quality soft membership assignments at the early stages. To address this issue, we introduce the similarity-guided graph contrastive regularization.

\begin{table}
\begin{tabular}{|c|c|} \hline
**Symbols** & Definition \\ \hline \(\mathbf{V}\) & The set of vertices \\ \hline \(\mathbf{E}^{a}\) & The set of edges for the \(a\)-th view \\ \hline \(\mathbf{A}^{a}\) & The \(a\)-th view adjacency matrix \\ \hline \(\mathbf{X}^{a}\) & The \(a\)-th view node attribute matrix \\ \hline \(\mathbf{v}(n)\) & The number of views (nodes) \\ \hline \(\mathbf{D}^{a}\) & The degree matrix for the \(a\)-th view \\ \hline \(\mathbf{H}^{a}\) & The the \(a\)-th view node representations \\ \hline \(\mathbf{A}\) & The adjacency matrix augmented by cluster similarity \\ \hline \(\mathbf{M}^{a}\) & The soft membership matrix for the \(a\)-th view \\ \hline \(\mathbf{H}\) & The average node representations \\ \hline \(\mathbf{M}\) & The average soft membership matrix \\ \hline \(\mathbf{A}\) & The normalized augmented adjacency matrix \\ \hline \end{tabular}
\end{table}
Table 1. Definition of Symbols

Figure 1. The overview of CARE. It first extracts the global node affinity based on the soft assignment by graph clustering method, and then combines the global node affinity and local node affinity together. Similarity-guided graph contrastive loss is then introduced to mitigate the potential bias.

### Similarity-guided Graph Contrastive Regularization

To mitigate potential bias introduced by low-quality soft membership assignments, we propose a similarity-guided graph contrastive loss that minimizes the difference between the similarity of the soft assignment and the similarity of node representations for any pair of nodes. This is formulated as follows:

\[\mathcal{L}_{2}=\min_{\mathbf{H}}||\mathbf{\hat{M}}\mathbf{\hat{M}}^{T}-\mathbf{\hat{H}}\mathbf{ \hat{H}}^{T}||_{F}^{2} \tag{6}\]

where \(\mathbf{\hat{M}}\) is the soft assignment matrix computed in Eq. 3. \(\mathcal{L}_{2}\) aims to learn the hidden representations such that the representations of node \(u_{l}\) and node \(u_{j}\) are expected to be close in the latent space if their soft membership assignments are similar. Following the idea of many existing methods restoring graph structure along with the node attributes of the graph (Bishop, 2006; He et al., 2016; He et al., 2016; Zhang et al., 2017), we propose to take the graph topological structure into consideration, reformulating the similarity-guided graph contrastive loss as:

\[\mathcal{L}_{C}=\min_{\mathbf{H}}||\mathbf{\hat{A}}-\mathbf{\hat{H}}\mathbf{\hat{H}}^{T}||_{F}^ {2} \tag{7}\]

where \(\mathbf{\hat{A}}=\hat{D}^{-1/2}\mathbf{\hat{A}}\hat{D}^{-1/2}\) is the normalized augmented adjacency matrix, and \(\mathbf{\hat{D}}\in\mathbb{R}^{n\times n}\) is the diagonal matrix with \(\mathbf{\hat{D}}_{ii}=\sum_{u_{j}}\mathbf{\hat{A}}_{ij}\).

**Theoretical Justification.** We aim to provide a theoretical analysis of how the proposed similarity-guided graph contrastive regularization mitigates potential bias by connecting it with graph spectral clustering. Before delving into the direct analysis of bias mitigation, we first demonstrate that \(\mathcal{L}_{C}\) functions as a contrastive learning loss.

**Lemma 3.1**.: _(Similarity-guided Graph Contrastive Loss) Let \(\mathbf{\hat{M}}\) be the output of a one-layer graph neural network defined in Eq. 3. Then, we have_

\[\mathcal{L}_{C}=\mathcal{L}_{f}+C \tag{8}\]

_where \(\mathcal{L}_{f}=-\sum_{i=1}^{n}\sum_{j=1}^{n}\log\frac{\exp(2\mathbf{\hat{A}}_{ij} \mathbf{\hat{h}}_{i}\mathbf{\hat{h}}_{j}^{T})}{\Pi_{u_{i}=1}^{n}\exp((h_{i}\mathbf{\hat{h} }_{i}^{T})^{2})^{1/n}}\) is a graph contrastive loss and \(C\) is a constant._

**See proof in Appendix A.1.**

**Remark:** Compared to traditional contrastive learning losses, the denominator of \(\mathcal{L}_{f}\) in Lemma 3.1 is the product of the exponential similarity between two node embeddings rather than a summation. The weakly supervised contrastive methods (Wang et al., 2017; Wang et al., 2017) impose a constraint that forces a given pair of nodes to form a positive/negative pair based on their likelihood of being assigned to the same cluster (further discussion can be found in Subsection 3.5). Unlike these "discrete" formulations regarding positive and negative pairs, our approach is in a continuous form. In \(\mathcal{L}_{f}\), \(\mathbf{\hat{S}}_{ij}=\mathbf{\hat{M}}_{i}\mathbf{\hat{M}}_{j}^{T}\) can be interpreted as the similarity measurement of a node pair \((u_{i},u_{j})\) in terms of soft assignment, guiding the similarity of the representations of two nodes in the latent space. If we ignore the influence of the adjacency matrix in \(\mathbf{\hat{A}}=\hat{D}^{-1/2}\mathbf{\hat{A}}\mathbf{\hat{D}}^{-1/2}=\hat{D}^{-1/2}(a \mathbf{\hat{M}}\mathbf{\hat{M}}^{T}+(1-a)^{\frac{1}{2}}\sum_{a=1}^{n}\mathbf{\hat{D}}^{-1 /2})\mathbf{\hat{D}}^{-1/2}\) by setting \(\alpha\) to be 1 (i. e., \(\mathbf{\hat{A}}=\hat{D}^{-1/2}\mathbf{\hat{M}}\mathbf{\hat{M}}^{T}\mathbf{\hat{D}}^{-1/2}\)), it would be interesting to see that when two nodes are sampled from two distant clusters, \(\mathbf{\hat{A}}_{ij}\approx 0\) and \(\log\frac{\exp(2\mathbf{\hat{S}}_{ij}\mathbf{\hat{h}}_{j}^{T})}{\Pi_{u_{i}}^{n}\exp(( \mathbf{\hat{h}}_{i}\mathbf{\hat{h}}_{j}^{T})^{\frac{1}{n}})^{\frac{1}{n}}}=0\) for this pair of nodes. Notice that for the node pair \((u_{i},u_{j})\) with high confidence being assigned to the same cluster, the weight \(\mathbf{\hat{S}}_{ij}\) is larger than the weight \(\mathbf{\hat{S}}_{ik}\) for the uncertain node pair \((u_{i},u_{k})\), where the node \(u_{k}\) has low confidence to be assigned to the same cluster as \(u_{i}\). By reducing the weight for these unreliable positive pairs, the negative impact of uncertain pseudo-labels can be alleviated.

Next, we aim to clarify our proposed contrastive loss by demonstrating the connection between \(\mathcal{L}_{C}\) and graph spectral clustering.

**Lemma 3.2**.: _(Graph Contrastive Spectral Clustering) Let \(\mathbf{\hat{M}}\) be the output of a one-layer graph neural network defined in Eq. 3 and \(\mathbf{\hat{h}}_{i}\) and \(\mathbf{\hat{h}}_{j}\) be unit vectors. Then, minimizing \(\mathcal{L}_{C}\) is equivalent to minimizing the following loss function:_

\[\min\mathcal{L}_{C}=\min[2Tr(\mathbf{H}^{T}\mathbf{L}\mathbf{H})+R(\mathbf{\hat{H}})] \tag{9}\]

_where \(\mathbf{L}=\mathbf{I}-\mathbf{\hat{A}}\) can be considered as the normalized graph Laplacian, \(\mathbf{I}\) is the identity matrix and \(R(\mathbf{\hat{H}})=||\mathbf{\hat{H}}\mathbf{\hat{H}}^{T}||_{F}^{2}\) is the regularization term._

**See proof in Appendix A.2.**

**Remark:** Based on Lemma 3.2, \(\mathcal{L}_{C}\) can be considered as the graph spectral clustering. Graph spectral clustering (Wang et al., 2017) aims to find clusters that minimize connections between different clusters while maximizing the connections within each cluster. Traditional graph spectral clustering (Chen et al., 2017) aims to find the embedding \(\mathbf{\hat{H}}\) such that \(Tr(\mathbf{H}^{T}\mathbf{L}^{T}\mathbf{\hat{H}})\) is minimized, where \(\mathbf{L}^{\prime}\) is the normalized graph Laplacian. The first term of Eq. 9 is similar to the objective function in traditional graph spectral clustering, but we enhance it by incorporating the similarity measurement \(\mathbf{\hat{S}}_{ij}=\mathbf{\hat{M}}_{i}\mathbf{\hat{M}}_{j}^{T}\) into the normalized graph Laplacian. By including the similarity measurement in the graph spectral clustering, we reinforce \(\mathcal{L}_{C}\) to mitigate the bias introduced by the clustering method defined in Eq. 3. It is important to note that in Lemma 3.2, the constraint that \(\mathbf{\hat{h}}_{i}\) and \(\mathbf{\hat{h}}_{j}\) are unit vectors is a common practice in many existing works (Wang et al., 2017; Wang et al., 2017; Wang et al., 2017). This constraint can be easily implemented through normalization, i. e., \(\mathbf{\hat{h}}_{i}=\frac{\mathbf{\hat{h}}_{i}}{||\mathbf{\hat{h}}_{i}||_{2}}\).

### Objective Function and Inference

Now, we are ready to introduce the overall objective function:

\[\min J=-\sum_{\mathbf{\hat{u}}_{i}}\mathcal{L}_{A}(u_{i})+\lambda\mathcal{L}_{C} \tag{10}\]

where \(\mathcal{L}_{A}\) is the cluster-aware node affinity loss and \(\mathcal{L}_{C}\) is the similarity-guided graph contrastive loss. \(\lambda\) is a constant parameter balancing these two terms. During the inference stage, we directly use the cluster-aware node affinity loss \(\mathcal{L}_{A}\) as the abnormal score:

\[score_{i}=-\mathcal{L}_{A}(u_{i}) \tag{11}\]

### Why can't weakly supervised contrastive learning loss be used as a regularization?

One effective remedy to reduce uncertainty and learn high-quality representations in an unsupervised setting is through contrastive learning loss, which has demonstrated significant performance improvements in representation quality (Wang et al., 2017; Wang et al., 2017; Wang et al., 2017). However, an existing study (Wang et al., 2017) has theoretically proved that simply applying vanilla contrastive learning loss (i. e., InfoNC (Wang et al., 2017)) can easily lead to the suboptimal solution. Similarly, according to (Wang et al., 2017), both normaland abnormal nodes are uniformly distributed in the unit hypersphere in the latent space by minimizing the vanilla contrastive learning loss, which leads to worse performance for the anomaly detection task. To address this issue, many weakly supervised contrastive losses (Wang et al., 2019; Wang et al., 2019) are proposed by incorporating the semantic information, such as the clustering results, into a contrastive regularization term as follows:

\[\mathcal{L}_{3}=-\sum_{i=1}^{n}\sum_{j\in C(i),j\neq i}\log\frac{\text{sim}( \hat{\mathbf{h}}_{i},\hat{\mathbf{h}}_{j})}{\text{sim}(\hat{\mathbf{h}}_{i},\hat{\mathbf{h}}_{j })+\sum_{k\in C(i)}\text{sim}(\hat{\mathbf{h}}_{i},\hat{\mathbf{h}}_{k})} \tag{12}\]

where \(\hat{\mathbf{h}}_{i}=\sum_{\sigma=1}^{n}\mathbf{h}_{i}^{T}\) is the representation for node \(u_{i}\) aggregated over all views, \(\text{sim}(\hat{\mathbf{h}}_{i},\hat{\mathbf{h}}_{j})=\exp(\mathbf{h}_{i}\hat{\mathbf{h}}_{j}^ {T}/\tau)\) and \(\tau\) is the temperature. \(j\in C(i)\) means that node \(u_{j}\) and node \(u_{i}\) are assigned into the same cluster or form a positive pair, while \(k\notin C(i)\) means that node \(u_{k}\) and node \(u_{i}\) are assigned into two different clusters, resulting in a negative pair. The intuition of the above equation is that if two nodes are from the same cluster, they should be close in the latent space by maximizing their similarity. However, we find out that the construction of the positive and negative pairs in Eq. 12 heavily relies on the quality of the soft assignment, while directly converting the soft assignment to the binary membership inevitably introduces bias/noise during the training phase. This bias will be amplified further in the node affinity learning as we mentioned in Subsection 3.2. (We also validate this in the ablation study in Subsection 4.3.) Here, we theoretically analyze that including this bias in the weakly supervised contrastive loss defined in Eq. 12 leads to suboptimal solution. Formally, we first define what is a true positive pair and a false positive pair respectively, and then introduce Theorem 3.4 to show the issue in Eq. 12.

**Definition 3.3**.: Given a sample \(\mathbf{x}_{i}\), we say \((\mathbf{x}_{i},\mathbf{x}_{j})\) is a true positive pair (or a false negative pair), if their optimal representations satisfy \(\exp(\mathbf{h}_{i}\mathbf{h}_{j}^{T}/\tau)>1\) for a small positive value \(\tau\). Similarly, we say \((\mathbf{x}_{i},\mathbf{x}_{k})\) is a false positive pair (or a true negative pair), if their optimal representations satisfy \(\exp(\mathbf{h}_{i}\mathbf{h}_{k}^{T}/\tau)\approx 0\) for a small positive value \(\tau\).

**Theorem 3.4**.: _Given the contrastive learning loss function \(\mathcal{L}_{3}\), if there exists one false positive sample in the batch during training, the contrastive learning loss will lead to a sub-optimal solution._

**See proof in Appendix A.3**.

## 4. Experimental Results

In this section, we demonstrate the performance of our proposed framework in terms of both effectiveness and efficiency by comparing it with state-of-the-art methods.

### Experiment Setup

#### 4.1.1. Datasets

We evaluate the performance of our proposed framework on six datasets for both single-view and multi-view graph anomaly detection scenarios, including the Insider Threat Test (CERT) (Chen et al., 2019), DBLP (Yang et al., 2019), IMDB (Yang et al., 2019), BlogCatalog (Yang et al., 2019), Amazon (Yang et al., 2019) and YelpChi (Ye et al., 2019) datasets. Among these datasets, CERT, IMDB, and DBLP are multi-view graphs, while BlogCatalog, Amazon, and YelpChi are single-view graphs. CERT, Amazon, and YelpChi are real-world datasets, whereas IMDB, DBLP, and BlogCatalog are semi-synthetic graphs. (See Appendix B.1 for the details of generating anomalous nodes.) Specifically, the CERT dataset is a collection of synthetic insider threat test datasets that provides both synthetic background data and data from synthetic malicious actors. This dataset does not include a feature matrix, so we use node2vec (Wang et al., 2019) to extract two feature matrices as two views. IMDB is a movie network, where each node corresponds to a movie, and two adjacency matrices indicate whether two movies share the same actor or director. DBLP is a citation network, where each node corresponds to an academic research paper, and two adjacency matrices indicate whether two papers share the same authors or if one paper cites another. Amazon is a review network, where each node represents a product in the musical instruments category, and its attributes are extracted from product reviews. Similarly, the Yelp dataset contains hotel and restaurant reviews, either filtered (spam) or recommended (legitimate) by Yelp (Yang et al., 2019). The statistics of these graphs are summarized in Table 2.

#### 4.1.2. Experiment Setting

The neural network structure of the proposed framework is GCN (Kipf and Welling, 2017). The hyper-parameters \(\alpha\) and \(\lambda\) for each dataset are specified in Appendix B.2. In all experiments, we set the initial learning rate to be 1e-5, the hidden feature dimension to be 128 and use Adam (Kingma and Ba, 2014) as the optimizer. The similarity function \(\text{sim}(a,b)\) is defined as \(\text{sim}(a,b)=\exp(\frac{a+b^{T}}{\|a\|\|p\|})\). We use TAM as the backbone of our method to capture local node affinity. The number of GCN layers is set to 2. The experiments are performed on a Windows machine with a 24GB RTX 4090 GPU. The code of our framework can be found in the anonymous GitHub link *.

Footnote *: [https://anonymous.4open.science/r/CARIE-demo-1CCF](https://anonymous.4open.science/r/CARIE-demo-1CCF)

#### 4.1.3. Evaluation Metrics

Following (Yang et al., 2019; Yang et al., 2019), all methods are evaluated based on Area Under the Receiver Operating Characteristic Curve (AUROC) and Area Under the Precision-Recall Curve (AUPRC). Higher AUROC/AUPRC indicates better performance. All of the experiments are repeated five times with different random seeds and the mean and standard deviation are reported.

#### 4.1.4. Baseline Methods

In our experiments, we compare our proposed framework CARE with state-of-the-art methods in the following two settings.

For _single-view graphs_, we compare CARE with the following eight baseline methods: (1). **ANOMALOUS**(Wang et al., 2019): a shallow method, jointly conducting attribute selection and anomaly detection as a \(\mathbf{z}\) whole based on CUR decomposition and residual analysis; (2). **Dominant**(Chen et al., 2019): a graph auto-encoder-based deep neural network model for graph anomaly detection, which encodes both the topological structure and node attributes to node embedding; (3). **CoLA**(Wang et al., 2019): a contrastive self-supervised graph anomaly detection method by

\begin{table}
\begin{tabular}{c c c c c} \hline Name & \(|V|\) & \(|E|^{1}\) & \(|E^{2}|\) & \# Anomalies \\ \hline BlogCatalog & 5,196 & 171,743 & - & 298 \\ \hline Amazon & 10,244 & 175,608 & - & 445 \\ \hline YelpChi & 24,741 & 49,315 & - & 597 \\ \hline CERT & 1,000 & 24,213 & 22,467 & 70 \\ \hline IMDB & 4,780 & 1,811,262 & 419,883 & 334 \\ \hline DBLP & 4,057 & 299,499 & 520,440 & 283 \\ \hline \end{tabular}
\end{table}
Table 2. Statistics of the datasets, including the number of nodes, anomalies, and edges for two views.

exploiting the local information; (4). **SLGAD**(Zhou et al., 2017): an unsupervised framework for outlier detection based on unlabeled in-distribution data, which uses contrastive learning loss as a regularization; (5). **HCM-A**(Zhou et al., 2017): a self-supervised learning by forcing the prediction of the shortest path length between pairs of nodes; (6). **ComGA**(Zhou et al., 2017): a community-aware attributed graph anomaly detection framework; (7). **CONAD**(Zhou et al., 2017): a contrastive learning-based graph anomaly detection method; (8). **TAM**(Zhou et al., 2017): an unsupervised anomaly method, proposing a scoring measure by assigning large score to the nodes that are less affiliated with their neighbors.

For _multi-view graphs_, we compare CARE with the following five baseline methods: (1). **MLRA**(Zhou et al., 2017): a multi-view non-negative matrix factorization-based method for anomaly detection, which performs cross-view low-rank analysis for revealing the intrinsic structures of data; (2). **NSNMF**(Zhou et al., 2017): an NMF based method, incorporating the neighborhood structural similarity information into the NMF framework to improve the anomaly detection performance; (3). **SRLSP**(Zhou et al., 2017): a multi-view detection method based on the local similarity relation and data reconstruction; (4). **NCMOD**(Zhou et al., 2017): an auto-encoder-based multi-view anomaly detection method, which proposes neighborhood consensus networks to encode graph neighborhood information; (5) We also report the performance of **TAM**(Zhou et al., 2017) on these multi-view graphs by averaging the adjacency matrix or concatenating two feature matrices together. We omit the comparison with other baseline methods since TAM outperforms them.

### Experimental Analysis

#### 4.2.1. Multi-view Graph Scenario

In this subsection, we evaluate the performance of CARE by comparing it with five baseline methods on three multi-view graphs, i. e., CERT, IMDB and DBLP datasets. The experimental results with respect to AUROC and AUPBC are presented in Table 3. We observe the following: (i) MLRA and NSNMF exhibit the poor performance among all baseline methods across most datasets. This can be attributed to their design, which is optimized for independent and identically distributed data (e.g., image data). Thus, these two methods struggle to capture graph topological information, resulting in lower performance in both AUPPRC and AUROC. (ii) In contrast, NCMOD incorporates graph topological information into the learned representation through its proposed neighborhood consensus networks, thus achieving the second-best performance on the IMDB and DBLP datasets. (iii) TAM suffers from the inability to handle multi-view graphs, performing worse than several self-supervised learning methods designed for multi-view graphs (e.g., NCMOD). (iv) Our proposed method outperforms all baselines in terms of AUROC and AUPRC across all datasets. Notably, graph-based anomaly detection methods designed for single views (e.g., TAM) filter in the presence of view heterogeneity as simply concatenating input features from multiple views distorts the feature space, resulting in significant performance challenges. In contrast, our method excels by encoding both local and global node affinities in the learned representation and mitigating biases through the proposed similarity-guided graph contrastive loss.

#### 4.2.2. Single-view Graph Scenario

Next, we evaluate the performance of CARE by comparing it to eight baseline methods on three single-view graphs, i. e., BlogCatalog, Amazon and YelpChi datasets. The experimental results with respect to AUROC and AUPRC are presented in Table 4. We have the following observations: (i) TAM outperforms most baseline methods across three single-view graphs. We attribute its great performance to the design of normal structure-preserved graph truncation to remove edges connecting normal and abnormal nodes. (ii) While TAM excels on BlogCatalog, CARE still ranks as a close second, showcasing its robustness in handling single-view graphs. (iii) CARE proves to be a highly competitive approach, demonstrating superior performance on the Amazon and YelpChi datasets. Notably, CARE surpasses TAM, the second-best method, by over 15.9% in AUROC and 39.3% in AUPRC on the Amazon dataset. Similarly, on the YelpChi dataset, CARE improves AUROC by over 18.7% compared to TAM. The ability of CARE to capture both local and global node affinity information in the learned representations enables it to perform effectively on the Amazon and YelpChi datasets. By leveraging its similarity-guided graph contrastive loss, CARE enhances representation learning, allowing it to better distinguish between normal and anomalous nodes. Overall, these results emphasize that while TAM leverages truncated affinity maximization techniques to tailor the raw adjacency matrix, CARE offers a more flexible and powerful approach by incorporating global and local affinities, making it a highly effective method in the single-view graph anomaly detection scenario. Its ability to significantly outperform other baselines on challenging datasets like Amazon and YelpChi demonstrates the effectiveness of the proposed method.

### Ablation Study

In this subsection, we conduct an ablation study to demonstrate the necessity of each component of CARE and validate the effectiveness of similarity-guided graph contrastive loss over vanilla and weakly supervised contrastive loss. Specifically, CARE-A removes the similarity measurement of soft membership and replaces \(\mathcal{L}_{A}\) with \(\mathcal{L}_{2}\). CARE-G refers to a variant of our proposed method by removing similarity-guided contrastive learning loss. CARE-InfoNCE replaces the similarity-guide contrastive loss with the vanilla contrastive loss while CARE-WSC substitutes it with the weakly supervised loss as shown in Eq. 12. The experimental results with respect to AUROC on the BlogCatalog, Amazon, DBLP, and IMDB datasets are presented in Table 5. Our observations are as follows:

* **Global Node Affinity**: CARE shows slight improvements on Amazon and BlogCatalog compared to CARE-A. However, excluding the global node affinity matrix (i. e., the similarity map of the soft membership) in CARE-A results in significant performance drops of approximately 40% on DBLP and 56% on IMDB, highlighting the importance of capturing global affinities in certain datasets.
* **Similarity-Guided Loss**: Removing the similarity-guided graph contrastive loss (i.e., CARE-G) leads to an average AUROC performance drop of 15% across the four datasets, underscoring the critical role of this regularization in mitigating bias and improving performance.
* **Contrastive Loss Substitutions**: Replacing the similarity-guided loss with either the vanilla contrastive loss (i.e., CARE-InfoNCE) or the weakly supervised contrastive loss 

[MISSING_PAGE_FAIL:7]

proposed method prefers a small number of clusters. By reducing the number of clusters, our method tends to generalize better by capturing larger, more meaningful patterns in the data instead of overfitting to noise.

### Efficiency Analysis

**Time Complexity.** In this subsection, we analyze the time complexity of our proposed CARE. Assume that the graph has \(n\) nodes, the input feature dimension is \(d\), the hidden feature dimension is \(f\), the number of nodes is \(n\), the number of edges is \(|E|\), and the number of clusters is \(k\). For ease of explanation, we only consider the 1-layer case. Following (Brocker et al., 2017), the time complexity of computing the soft membership matrix using GCN is \(O(ndk+n|E|k+n)\). The complexity of the GCN to capture the hidden representation \(\mathbf{h}\in O(|E|d+ndf)\) with sparse computation. The complexity of the similarity-guided contrastive learning loss is \(O(n^{2}f)\). However, in the experiments, we can use the sampling strategy to sample \(p(p<<n)\) nodes and thus the complexity can be reduced to \(O(\rho^{2}f)\). The total complexity of CARE is \(O(n(kd+fd+|E|k+1)+|E|d+p^{2}f)\).

**Running Time Analysis.** Next, we experiment on the YelpChi dataset to show the efficiency of our proposed CARE by changing the number of nodes and the number of layers. The reason why we only provide the efficiency analysis regarding the training/running time vs the number of nodes on the YelpChi dataset is that we can manually increase the number of nodes from 500 to 15,000 on this dataset. For the other datasets (e.g., IMDB, BlogCatalog, Amazon, DBLP), the number of nodes is less than 15,000 (see Table 2 for details) and we cannot get the running time if the number of nodes is set to be a value larger than 15,000. Thus, we do not provide the efficiency analysis on small datasets. In the first experiment, we fix the number of layers to be 1, the total number of iterations to be 10,000 and adjust the number of nodes by randomly selecting \(k\) nodes, where \(k\in[500,1000,2000,5000,10000,15000]\). The experimental result is shown in Figure 4 (a). By observation, we find that the running time is quadratic to the number of nodes. In the second experiment, we fixed the number of nodes to be 2000, the total number of iterations to be 10,000, and adjusted the number of GCN layers from 1 layer to 4 layers. The experimental result is shown in Figure 4 (b). We observe that the running time is almost linear with respect to the number of layers.

## 5. Conclusion

In this paper, we proposed a novel self-supervised framework for anomaly detection in multi-view graphs, addressing key limitations of existing methods. By capturing both local and global node affinities and leveraging a similarity-guided contrastive loss, our approach effectively identifies anomalous nodes without relying on strong structural assumptions. The proposed method not only augments the graph structure with learned soft membership assignments but also mitigates the negative impact of low-quality assignments through a robust loss function, theoretically connected to graph spectral clustering. Our experimental results on six datasets demonstrate the superior performance of the proposed framework in detecting anomalies in complex, multi-view graphs.

Figure 4. Efficiency analysis on the YelpChi dataset

Figure 3. The number of clusters v.s. AUROC on four datasets.

Figure 2. \(\alpha\), \(\log(\lambda)\) v.s. AUROC on four datasets.

* [201] Charu C. Aggarwal and Philip S. Yu. 2001. Outlier Detection for High Dimensional Data. In _Proceedings of the 2001 ACM SIGKDD International Conference on Management of Data, Santa Barbara, CA, USA, May 21-24, 2001_. ACM, 37-46.
* [202] Imita Luzel, Xin Ren, Huxing W. Acharya, and Yu Ding. 2021. Neighborhood Structure Assisted Non-negative Matrix Factorization and Its Application in Unsupervised Point-wise Anomaly Detection. _J. Mach. Learn. Res._ 22 (2021), 341-363.
* ACCV 2018
- 14th Asian Conference on Computer Vision, Pitch, Australia, December 24-26, 2018, Revised Selected Papers, Part III_. vol. 1135. Springer, 622-637.
* [204] Kita Allah, Lashar Labiod, and Mohamed M. 2017. A Semi-MM-CCA Unified Framework for Data Clustering. _IEEE Trans. Knowl. Data Eng._ 29 (2), 1 (2017), 2-16.
* [205] Francis R. Bach and Michael I. Jordan. 2003. Learning Spectral Clustering in _Advances in Neural Information Processing Systems 16 Neural Information Processing Systems 15th Symposium on_. _NeurIPS 2003, December 8-13, 2003, Vancouver and Whiskey, British Columbia, Canada, 2017_. MIT Press, 305-322.
* [206] Mikhail Belkin and Partha Nyvgi. 2003. Laplacian Eigenmaps for Dimensionality Reduction and Data Representation. _Neural Comput._ 15, 6 (2003), 1375-1396.
* [207] Rinkel Blakely, Jack Lanchantini, and Jurjun G. 2002. Image and space complexity of graph convolutional networks. _Accessed on:_. Ore. Hi 3 (2021).
* [208] Deyu Do, Xiao Wang, Chuan Shi, Micki Zhu, Emine Liu, and Peng Cui. 2020. Structural Deep Clustering Network. In _WWW '20: The Web Conference 2020, Taipei, Taiwan, July 24-26, 2020_. ACM, IW3EC, 1400-1410.
* [209] Ojner Brickx, Juan Liu, Boyle, Jiangjiang Shen, Achay Path, Richard Chow, Eugene Hart, and Nicolas Duchennagel. 2022. Proactive Insider Threat Detection through Graph Learning and Psychological Context. In _2022 IEEE Symposium on Security and Privacy Workshops, San Francisco, CA, USA, May 24-25, 2022_. IEEE Computer Society, 142-149.
* [210] Hammamed Candes, Xiaodong Li, Yi Ma, and John Wright. 2011. Robust principal component analysis? _Journal of the ACM (JACM)_ 58, 3 (2011), 1-37.
* [211] Bo Chen, Jing Zhang, Xiaokang Zhang, Yuxiao Deng, Jian Song, Peng Zhang, Kaibo Xu, Evgory Khaimanov, and Jie Tang. 2023. GCCAD: Graph Contrastive Coding for Anomaly Detection. _IEEE Trans. Knowl. Data Eng._ 35, 8 (2023), 8037-8051.
* [212] Ting Chen, Lu-An Tang, Yizhou Sun, Zhenghang Chen, and Kai Zhang. 2016. Entity Embedding-Based Anomaly Detection for Heterogeneous Categorical Events. In _Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016, New York, NY, USA, 9-15 July 2016_. IJCAI/AAAI Press, 1396-1403.
* [213] Li Cheng, Yijie Wang, and Xinrui Liu. 2021. Neighborhood Consensus Networks for Unsupervised Multi-view Online Detection. In _Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Virtual Event, February 2-9, 2021_. AAAI Press, 7099-71068.
* [214] Kaie Ding, Jundong Li, Rohit Bhannabal, and Huan Liu. 2019. Deep Anomaly Detection on Attributed Networks. In _Proceedings of the 2019 SIAM International Conference on Data Mining, SDM 2019, Calgary, Alberta, Canada, May 2-4, 2019_. SIAM, 594-602.
* [215] Kamara Ghasech Dizaji, Anirahossein Heraud, Cheng Deng, Weidong Cai, and Heng Huang. 2017. Deep Clustering via Joint Convolutional Autoencoder Embedding and Relative Entropy Minimization. In _IEEE International Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017_. IEEE Computer Society, 5747-5756.
* [216] Xiangyu Dong, Xingyi Zhang, Yami Sun, Lei Chen, Mingyuan Yuan, and Shou Wang. 2020. SmoothRNN: Smoothing-based GNN for Unsupervised Node Anomaly Detection. _CoRR_ abs/2005.17525 (2020).
* [217] David I. Donoho et al. 2001. High-dimensional data analysis: The curses and blossings of dimensionality. _AISR math challenges_, 1, 2000 (2000), 32.
* [218] Yinqeng Dou, Zhiwei Liu, Li Sun, Yitong Deng, Hao Peng, and Philip S. Yu. 2020. Enhancing Graph Neural Network-based Frauders against Cam-outfied Frauders. In _CIKM 'The 2020 ACM International Conference on Information and Knowledge Management, Virtual Event, Ireland, October 19-23, 2020_. Mathieu & Aquin, Stefan Dietter, Claudia Hauff, Edward Curry, and Philippe Cudre-Manurous (Eds.). ACM, 315-324.
* [219] William Eberle, Jeffrey Graves, and Lawrence Holder. 2010. Instruct threat detection using a graph-based approach. _Journal of Applied Security Research_ 6, 1 (2010), 32-81.
* [220] Joulian Glaser and Brian Lindauer. 2013. Bridging the Gap: A Pragmatic Approach to Generating Inside Threat Data. In _2013 IEEE Symposium on Security and Privacy Workshops, San Francisco, CA, USA, May 23-24, 2013_. IEEE Computer Society, 98-104.
* [221] Aditya Grover and Jure Leskovec. 2016. node2vec: Scalable feature learning for networks. In _Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining_. 855-864.
* [222] Frank E. Grinibb. 1960. Procedures for detecting outlying obervation in samples. _Technometrics_ 11, 1 (1969), 1-21.
* [223] Xiaohong Guan, Wei Wang, and Xiangliang Zhang. 2009. Fast intrusion detection based on an-negative matrix factorization model. _J. Netw. Comput. Appl._ 32, 1 (2009), 31-44.
* [224] Xifeng Guo, Xinwang Liu, En Zhu, and Jianping Yin. 2017. Deep Clustering with Convolutional Autoencoders. In _Neural Information Processing: 29th International Conference, ICONY 2017, Guangzhou, China, November 14-18, 2017_. Proceedings, 997-101, Vol. 1055, Springer, 373-382.
* [225] Jeff Z. Hacob, Chen Qin Wei, Andrei Gaidon, and Tengyu Mu. 2021. Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss. In _Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, NeurIPS 2021, December 6-14, 2021_. virtual 500-5011.
* [226] Bryan Hooi, Flynn Ah Song, Alex Beutel, Neil Shah, Kitayo Shin, and Christos Faloutsos. 2016. FRADMAR: Bounding Graph Fraud in the Face of Camphor. In _Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016_. ACM, 895-904.
* [227] Tianjin Huang, Yulong Pei, Vladu Menkovski, and Mikola Pechenizkiy. 2022. Hop-Conn Based Self-supervised Anomaly Detection on Attributed Networks. _SIGKDD International Conference on Knowledge Discovery in Databases: European Conference, ECM, PKDD 2022, Greenblo, France, September 19-23, 2022, Proceedings, Part I (Lecture Notes in Computer Science, 10. 1315)_. Massih-IEEE Animi, Siphane Can, Asia Fischer, Tias Guns, Petra Kjoskiy Novak, and Grigorios Tsoumakas (Eds.). Springer, 225-241.
* [228] Jianpu Jiang, Jianping Chen, Tianhao Gu, Kim-Kewang Raymond Choo, Chao Liu, Min Yu, Weiqing Huang, and Prasant Mohapatra. 2019. Anomaly Detection with Graph Convolutional Networks for Insider Threat and Fraud Detection. In _2019 IEEE Military Communications Conference, ACM 2019, NeurIPS, 14-20, Newslerm 12-10, 2019-114.
* [229] Petra Jilkova and Petra Kekaikova. 2021. Digital consumer behaviour and commerce trends during the COVID-19 crisis. _International Advances in Economic Research_ 1, 2 (2021), 83-85.
* 5, 2021_. Gianluca Democrat, Guido Diacon, J., Shane Culpepper, Zi Huang, and Hanghang Tong (Eds.). ACM, 312-3126.
* [231] Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In _3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings_.
* [232] Thomas K. Kipf and Max Welling. 2017. Semi-Supervised Classification with Graph Convolutional Networks. In _3th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings_. OpenReview.net.
* [233] Srijan Kumar, Xikun Zhang, and Jure Leskovec. 2019. Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks. In _Proceedings of the 28th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2019, Anchorage, AK, USA, August 4-8, 2019_. Akur, Teredesai, Vipin Kumar, Jing Li, Romer Radules, Emiram Terai, and George Pisheri. 2018. ACM, 1269-1278.
* [234] Chun-Liang Li, Khyuk Sohn, Jinming Yoon, and Tomas Pisheri. 2021. CutPaste: Self-Supervised Learning for Anomaly Detection and Localization. In _IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, June 19-25, 2021_. Computer Vision Foundation (2002), 1664-9647.
* [235] Jianbo Li, Leheng Zheng, Yada Zhu, and Jingrui He. 2021. Outlier Impact Characterization for Time Series Data. In _Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Third Joint Track Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, IAAI 2022, The Eleventh Symposium on Educational Advances in Artificial Intelligence, IAAI 2022, The Eleventh Symposium on Educational Advances in Artificial Intelligence, IAAI 2022, The Eleventh Symposium on Educational Advances in Artificial Intelligence, IAAI 2022, The Eleventh Symposium on Educational Advances in Artificial Intelligence

[MISSING_PAGE_FAIL:10]

## Appendix A Proof

### Proof for Lemma 3.1

**Lemma 3.1**: _(Similarity-guided Graph Contrastive Loss) Let \(\bar{\mathbf{M}}\) be the output of a one-layer graph neural network defined in Eq. 3. Then, \(\mathcal{L}_{C}\) is equivalent to the following loss function:_

\[\mathcal{L}_{C}=\mathcal{L}_{f}+C\]

_where \(\mathcal{L}_{f}=-\sum_{i=1}^{n}\sum_{j=1}^{n}\log\frac{\exp(2\hat{\mathbf{A}}_{ij} \hat{\mathbf{h}}_{i}\hat{\mathbf{h}}_{j}^{T})}{\prod_{i=1}^{n}\exp((\hat{\mathbf{h}}_{i}\hat {\mathbf{h}}_{i}^{T})^{2})^{i}n}\) is a graph contrastive loss and \(C\) is a constant._

Proof.: \[\min_{H}\mathcal{L}_{C} =\min_{H}||\hat{\mathbf{A}}-\bar{\mathbf{H}}\bar{\mathbf{H}}^{T}||_{F}^{2}\] \[=\min_{\bar{H}}\sum_{i=1}^{n}\sum_{j=1}^{n}(\hat{\mathbf{A}}_{ij}- \bar{\mathbf{h}}_{i}\hat{\mathbf{h}}_{j}^{T})^{2}\] \[=\min_{H}\sum_{i=1}^{n}\sum_{j=1}^{n}(\hat{\mathbf{A}}_{ij}^{2}-2\hat {\mathbf{A}}_{ij}\hat{\mathbf{h}}_{i}\hat{\mathbf{h}}_{j}^{T}+(\hat{\mathbf{h}}_{i}\hat{\mathbf{h} }_{j}^{T})^{2})\]

Notice that \(\bar{\mathbf{M}}\) is independent of \(\bar{\mathbf{H}}\). When we fix the parameter \(\bar{\mathbf{M}}\) to update \(\bar{\mathbf{H}}\), then \(\hat{\mathbf{A}}_{ij}^{T}\) can be considered as a constant in this optimization problem. Thus, we have

\[\min_{\bar{H}}\mathcal{L}_{C} =\min_{\bar{H}}\sum_{i=1}^{n}\sum_{j=1}^{n}(-2\hat{\mathbf{A}}_{ij} \bar{\mathbf{h}}_{i}\hat{\mathbf{h}}_{j}^{T}+(\bar{\mathbf{h}}_{i}\bar{\mathbf{h}}_{j}^{T})^{ 2})+C\] \[=\min_{\bar{H}}\sum_{i=1}^{n}\sum_{j=1}^{n}(-2\hat{\mathbf{A}}_{ij} \bar{\mathbf{h}}_{i}\hat{\mathbf{h}}_{j}^{T}+\frac{1}{n}\sum_{k=1}^{n}(\bar{\mathbf{h}}_{i }\hat{\mathbf{h}}_{k}^{T})^{2})+C\] \[=\min_{\bar{H}}-\sum_{i=1}^{n}\sum_{j=1}^{n}\log\frac{\exp(2\hat{ \mathbf{A}}_{ij}\bar{\mathbf{h}}_{i}\hat{\mathbf{h}}_{j}^{T})}{\prod_{k=1}^{n}\exp((\hat{ \mathbf{h}}_{i}\hat{\mathbf{h}}_{k}^{T})^{2})^{\frac{1}{2}}}+C\] \[=\min_{\bar{H}}\mathcal{L}_{f}+C\]

where \(\mathcal{L}_{f}=-\sum_{i=1}^{n}\sum_{j=1}^{n}\log\frac{\exp(2\hat{\mathbf{A}}_{ij} \bar{\mathbf{h}}_{i}\hat{\mathbf{h}}_{j}^{T})}{\prod_{i=1}^{n}\exp((\hat{\mathbf{h}}_{i} \hat{\mathbf{h}}_{k}^{T})^{2})^{i}n}\). Thus, we have \(\mathcal{L}_{C}=\mathcal{L}_{f}+C\), which completes the proof. 

### Proof for Lemma 3.2

**Lemma 3.2**: _(Graph Contrastive Spectral Clustering) Let \(\bar{\mathbf{M}}\) be the output of a one-layer graph neural network defined in Eq. 3 and \(\bar{\mathbf{h}}_{i}\) and \(\bar{\mathbf{h}}_{j}\) be unit vectors. Then, minimizing \(\mathcal{L}_{C}\) is equivalent to minimizing the following loss function:_

\[\min\mathcal{L}_{C}=\min[2Tr(\bar{\mathbf{H}}^{T}\mathbf{L}\bar{\mathbf{H}})+R(\bar{\mathbf{H}} )]\]

_where \(\mathbf{L}=\mathbf{I}-\bar{\mathbf{A}}\) can be considered as the normalized graph Laplacian, \(\mathbf{I}\) is the identity matrix and \(R(\bar{\mathbf{H}})=||\bar{\mathbf{H}}\bar{\mathbf{H}}^{T}||_{F}^{2}\) is the regularization term._

Proof.: Based on the proof in Lemma 3.1, we have

\[\min\mathcal{L}_{C} =\sum_{i=1}^{n}\sum_{j=1}^{n}(-2\hat{\mathbf{A}}_{ij}\bar{\mathbf{h}}_{i} \hat{\mathbf{h}}_{j}^{T}+(\hat{\mathbf{h}}_{i}\hat{\mathbf{h}}_{j}^{T})^{2}) \tag{14}\] \[=(\sum_{i=1}^{n}\sum_{j=1}^{n}(2\hat{\mathbf{A}}_{ij}-2\hat{\mathbf{A}}_{ ij}-2\hat{\mathbf{A}}_{ij}\bar{\mathbf{h}}_{i}\hat{\mathbf{h}}_{j}^{T}+(\hat{\mathbf{h}}_{i}\hat{ \mathbf{h}}_{j}^{T})^{2})\]

Notice that \(\bar{\mathbf{A}}=\tilde{D}^{-1/2}(\alpha\bar{\mathbf{M}}\bar{\mathbf{M}}^{T}+(1-\alpha) \frac{1}{\alpha}\sum_{\alpha=1}^{n}A^{\alpha})\tilde{D}^{-1/2}\) and \(\tilde{D}_{ii}=\sum_{j}(\alpha\bar{\mathbf{M}}_{i}\bar{\mathbf{M}}_{j}^{T}+(1-\alpha) \frac{1}{\alpha}\sum_{\alpha=1}^{n}A_{ij}^{T})\). We have \(\sum_{j=1}^{n}\hat{\mathbf{A}}_{ij}=1\) and thus \(\sum_{i=1}^{n}\sum_{j=1}^{n}2\hat{\mathbf{A}}_{ij}\) is a constant, which can be ignored in this optimization problem. Since \(\bar{\mathbf{h}}_{i}\) and \(\bar{\mathbf{h}}_{j}\) are unit vectors, we have

\[\min\mathcal{L}_{C} =\sum_{i=1}^{n}\sum_{j=1}^{n}(2\hat{\mathbf{A}}_{ij}-2\hat{\mathbf{A}}_{ ij}\bar{\mathbf{h}}_{i}\hat{\mathbf{h}}_{j}^{T}+(\hat{\mathbf{h}}_{i}\hat{\mathbf{h}}_{j}^{T})^{2}) \tag{15}\] \[=\sum_{i=1}^{n}\sum_{j=1}^{n}\hat{\mathbf{A}}_{ij}||\bar{\mathbf{h}}_{i} ||_{2}^{2}+\sum_{i=1}^{n}\sum_{j=1}^{n}\hat{\mathbf{A}}_{ij}||\bar{\mathbf{h}}_{j}||_{ 2}^{2}-\sum_{i=1}^{n}\sum_{j=1}^{n}2\hat{\mathbf{A}}_{ij}\bar{\mathbf{h}}_{i}\hat{\mathbf{h} }_{j}^{T}\] \[+\sum_{i=1}^{n}\sum_{j=1}^{n}(\bar{\mathbf{h}}_{i}\hat{\mathbf{h}}_{j}^{T})^ {2}\] \[=\sum_{i=1}^{n}I_{ii}||\bar{\mathbf{h}}_{i}||_{2}^{2}+\sum_{j=1}^{n}I_{ jj}||\bar{\mathbf{h}}_{j}||_{2}^{2}-\sum_{i=1}^{n}\sum_{j=1}^{n}2\hat{\mathbf{A}}_{ij}\bar{\mathbf{h} }_{i}\hat{\mathbf{h}}_{j}^{T}\] \[+\sum_{i=1}^{n}\sum_{j=1}^{n}(\bar{\mathbf{h}}_{i}\hat{\mathbf{h}}_{j}^{T})^ {2}\] \[=2Tr(\bar{\mathbf{H}}^{T}\mathbf{L}\bar{\mathbf{H}})+R(\bar{\mathbf{H}})\]

where \(\mathbf{L}=\mathbf{I}-\bar{\mathbf{A}}\) can be considered as the normalized graph Laplacian, \(\mathbf{I}\) is the identity matrix and \(R(\bar{\mathbf{H}})=||\bar{\mathbf{H}}\bar{\mathbf{H}}^{T}||_{F}^{2}\), which completes the proof. 

### Proof for theorem 3.4

**Definition 3.3**: _Given a sample \(\mathbf{x}_{i}\), we say \((\mathbf{x}_{i},\mathbf{x}_{j})\) is a false negative pair (or a true positive pair), if their optimal representations satisfy \(\exp(\hat{\mathbf{h}}_{i}\bar{\mathbf{h}}_{j}^{T}/r)>1\) for a small positive value \(r\). Similarly, we say \((\mathbf{x}_{i},\mathbf{x}_{k})\) is a true negative pair (or a false positive pair), if their optimal representations satisfy \(\exp(\hat{\mathbf{h}}_{i}\hat{\mathbf{h}}_{k}^{T}/r)\approx 0\) for a small positive value \(r\)._

**Theorem 3.4**: _Given the contrastive learning loss function \(\mathcal{L}_{3}\), if there exists one false positive sample in the batch during training, the contrastive learning loss will lead to a sub-optimal solution._

Proof.: We can rewrite \(\mathcal{L}_{3}\) as follows:

\[\mathcal{L}_{3} =\sum_{i}\sum_{j\in C(i),j\neq i}[\log(\frac{\exp(\hat{\mathbf{h}}_{i} \hat{\mathbf{h}}_{j}^{T}/r)+\sum_{k\in C(i)}\exp(\bar{\mathbf{h}}_{i}\hat{\mathbf{h}}_{k}^{T} /r)}{\exp(\hat{\mathbf{h}}_{i}\hat{\mathbf{h}}_{j}^{T}/r)})]\] (16) \[=\sum_{i}\sum_{j\in C(i),j\neq i}[\log(\exp(\bar{\mathbf{h}}_{i}\hat{\mathbf{ h}}_{j}^{T}/r)+\sum_{k\in C(i)}\exp(\bar{\mathbf{h}}_{i}\hat{\mathbf{h}}_{k}^{T}/r))-\hat{\mathbf{h}}_{i} \hat{\mathbf{h}}_{j}^{T}/r]\] \[=\sum_{j\in C(i),j\neq i}[\log(\exp(\bar{\mathbf{h}}_{i}\hat{\mathbf{h}}_{j}^{T}/r)+ \sum_{k\in C(i)}\exp(\bar{\mathbf{h}}_{i}\hat{\mathbf{h}}_{k}^{T}/r))-\hat{\mathbf{h}}_{i} \hat{\mathbf{h}}_{j}^{T}/r]\] \[+\sum_{i\neq 1}\sum_{j\in C(i),j\neq i}[\log(\exp(\bar{\mathbf{h}}_{i}\hat{\mathbf{ h}}_{j}^{T}/r)+\sum_{k\in C(i)}\exp(\bar{\mathbf{h}}_{i}\hat{\mathbf{h}}_{k}^{T}/r))-\hat{\mathbf{h}}_{i} \hat{\mathbf{h}}

\[\frac{\partial\mathcal{L}_{3}}{\partial\bar{\mathbf{k}}_{1}} =\frac{1}{\tau}\sum_{j\in C(1),j\neq 1}[\frac{\exp(\bar{\mathbf{k}}_{1} \bar{\mathbf{k}}_{j}^{T}/\tau)\bar{\mathbf{k}}_{j}+\sum_{k\in C(1)}\exp(\bar{\mathbf{k}}_{1 }\bar{\mathbf{k}}_{k}^{T}/\tau)\bar{\mathbf{k}}_{k}}{\exp(\bar{\mathbf{k}}_{1}\bar{\mathbf{k}}_ {j}^{T}/\tau)+\sum_{k\in C(1)}\exp(\bar{\mathbf{k}}_{1}\bar{\mathbf{k}}_{k}^{T}/\tau)}- \bar{\mathbf{k}}_{j}]\] \[+\frac{1}{\tau}\sum_{i\neq 1,\in C(1)}[\frac{\exp(\bar{\mathbf{k}}_{i} \bar{\mathbf{k}}_{1}^{T}/\tau)\bar{\mathbf{k}}_{i}}{\exp(\bar{\mathbf{k}}_{i}\bar{\mathbf{k}}_ {1}^{T}/\tau)+\sum_{k\in C(1)}\exp(\bar{\mathbf{k}}_{i}\bar{\mathbf{k}}_{k}^{T}/\tau)}- \bar{\mathbf{k}}_{i}]\] \[=\frac{1}{\tau}\sum_{j\in C(1),j\neq 1}[\frac{\sum_{k\in C(1)}\exp( \bar{\mathbf{k}}_{1}\bar{\mathbf{k}}_{1}^{T}/\tau)\bar{\mathbf{k}}_{k}-\sum_{k\in C(1)} \exp(\bar{\mathbf{k}}_{1}\bar{\mathbf{k}}_{k}^{T}/\tau)\bar{\mathbf{k}}_{j}]}{\exp(\bar{\mathbf{ k}}_{1}\bar{\mathbf{k}}_{j}^{T}/\tau)+\sum_{k\in C(1)}\exp(\bar{\mathbf{k}}_{i}\bar{\mathbf{k}}_{ k}^{T}/\tau)}\] \[+\frac{1}{\tau}\sum_{i\neq 1,\in C(1)}[\frac{-\sum_{k\in C(1)} \exp(\bar{\mathbf{k}}_{i}\bar{\mathbf{k}}_{k}^{T}/\tau)\bar{\mathbf{k}}_{i}}{\exp(\bar{\mathbf{ k}}_{i}\bar{\mathbf{k}}_{1}^{T}/\tau)+\sum_{k\in C(1)}\exp(\bar{\mathbf{k}}_{i}\bar{\mathbf{k}}_{ k}^{T}/\tau)}]\]

Setting the gradient to 0, we have

\[\sum_{j\in C(1),j\neq 1}[\frac{\sum_{k\in C(1)}[\exp(\bar{\mathbf{k}}_{1 }\bar{\mathbf{k}}_{k}^{T}/\tau)\bar{\mathbf{k}}_{j}-\exp(\bar{\mathbf{k}}_{1}\bar{\mathbf{k}}_ {k}^{T}/\bar{\mathbf{k}}_{k}]}{\exp(\bar{\mathbf{k}}_{1}\bar{\mathbf{k}}_{1}^{T}/\tau)+ \sum_{k\in C(1)}\exp(\bar{\mathbf{k}}_{1}\bar{\mathbf{k}}_{k}^{T}/\tau)}]\] \[+\sum_{i\neq 1,\in C(1)}[\frac{\sum_{k\in C(1)}\exp(\bar{\mathbf{k}}_{i} \bar{\mathbf{k}}_{k}^{T}/\tau)\bar{\mathbf{k}}_{i}}{\exp(\bar{\mathbf{k}}_{i}\bar{\mathbf{k}}_ {1}^{T}/\tau)+\sum_{k\in C(1)}\exp(\bar{\mathbf{k}}_{i}\bar{\mathbf{k}}_{k}^{T}/\tau)}]=0 \tag{15}\]

As \((\mathbf{x}_{i},\mathbf{x}_{1})\) is a true positive pair, both \((\mathbf{x}_{i},\mathbf{x}_{k})\) and \((\mathbf{x}_{1},\mathbf{x}_{k})\) are true negative pairs. According to Definition 3.3, \(\exp(\bar{\mathbf{k}}_{i}\bar{\mathbf{k}}_{k}^{T}/\tau)\approx 0\) and \(\exp(\bar{\mathbf{k}}_{1}\bar{\mathbf{k}}_{k}^{T}/\tau)\approx 0\) for some positive small values \(\tau\) and both two terms of Eq. 15 is 0. Thus, Eq. 15 holds.

Next, we want to show that if there exists one false positive sample, we reach a contradiction. Assuming that \(\mathbf{x}_{1}\) is one false positive sample of \(\mathbf{x}_{i}\) in the batch, then both \((\mathbf{x}_{1},\mathbf{x}_{j})\) and \((\mathbf{x}_{i},\mathbf{x}_{1})\) are false positive pairs and \((\mathbf{x}_{1},\mathbf{x}_{k})\) is a false negative pair for some \(k\) (e. g., \(\mathbf{x}_{1}\) and \(\mathbf{x}_{k}\) are from the same cluster for some \(k\)). Similarly, as \((\mathbf{x}_{i},\mathbf{x}_{k})\) is a true negative pair for all \(k\), \(\exp(\bar{\mathbf{k}}_{i}\bar{\mathbf{k}}_{k}^{T}/\tau)\approx 0\) and the second term of Eq. 15 is approximately 0. Therefore, we have

\[\sum_{j\in C(1),j\neq 1}\frac{\sum_{k\in C(1)}[\exp(\bar{\mathbf{k}}_{i} \bar{\mathbf{k}}_{k}^{T}/\tau)\bar{\mathbf{k}}_{j}-\exp(\bar{\mathbf{k}}_{i}\bar{\mathbf{k}}_{k} ^{T}/\tau)\bar{\mathbf{k}}_{j}]}{\exp(\bar{\mathbf{k}}_{i}\bar{\mathbf{k}}_{j}^{T}/\tau)+ \sum_{k\in C(1)}\exp(\bar{\mathbf{k}}_{i}\bar{\mathbf{k}}_{k}^{T}/\tau)}+0=0\] \[\sum_{j\in C(1),j\neq 1}\sum_{k\in C(1)}[\exp(\bar{\mathbf{k}}_{i} \bar{\mathbf{k}}_{k}^{T}/\tau)\bar{\mathbf{k}}_{j}-\exp(\bar{\mathbf{k}}_{1}\bar{\mathbf{k}}_{k }^{T}/\tau)\bar{\mathbf{k}}_{k}]=0 \tag{16}\]

We multiply Eq. 16 by \(\bar{\mathbf{k}}_{i}^{T}\), where \((\mathbf{x}_{i},\mathbf{x}_{j})\) is a true positive pair for any \(j\) (i. e., both \(\mathbf{x}_{i}\) and \(\mathbf{x}_{j}\) are from the same cluster) and \((\mathbf{x}_{i},\mathbf{x}_{k})\) is a true negative pair for any \(k\). Then, we have

\[\sum_{j\in C(1),j\neq 1}\sum_{k\in C(1)}[\exp(\bar{\mathbf{k}}_{i} \bar{\mathbf{k}}_{k}^{T}/\tau)\bar{\mathbf{k}}_{j}\bar{\mathbf{k}}_{i}^{T}\] \[+\exp(\bar{\mathbf{k}}_{i}\bar{\mathbf{k}}_{k}^{T}/\tau)(-\bar{\mathbf{k}}_{i }\bar{\mathbf{k}}_{i}^{T})]=0 \tag{17}\]

Since \((\mathbf{x}_{i},\mathbf{x}_{j})\) is a true positive pair and \((\mathbf{x}_{i},\mathbf{x}_{k})\) is a true negative pair, we have \(\exp(\bar{\mathbf{k}}_{j}\bar{\mathbf{k}}_{i}^{T}/\tau)>1\) and \(\exp(\bar{\mathbf{k}}_{k}\bar{\mathbf{k}}_{i}^{T}/\tau)\approx 0\), which means that \(\bar{\mathbf{k}}_{j}\bar{\mathbf{k}}_{i}^{T}>0\) and \(\bar{\mathbf{k}}_{k}\bar{\mathbf{k}}_{i}^{T}<0\). Therefore, both two terms of Eq. 17 are non-negative and Eq. 17 holds if and only if \(\exp(\bar{\mathbf{k}}_{1}\bar{\mathbf{k}}_{k}^{T}/\tau)\approx 0\) for any \(k\notin C(1)\) (i. e., \((\mathbf{x}_{1},\mathbf{x}_{k})\) is a true negative pair for any \(k\notin C(1)\)).

If \((\mathbf{x}_{1},\mathbf{x}_{k})\) is a true negative pair for any \(k\notin C(1)\), then \((\mathbf{x}_{1},\mathbf{x}_{i})\) has to be a true positive pair, and it contradicts our assumption that \(\mathbf{x}_{1}\) is a false positive sample of \(\mathbf{x}_{1}\). Therefore, we reach a contradiction and we could not get the optimal solution for \(\bar{\mathbf{k}}_{1}\), which completes the proof. 

## Appendix B Experiments

In this section, we show the details of generating anomalous node for the semi-supervised datasets, including IMDB and DBLP. We also show the hyper-parameter specification for reproducing the experimental results.

### Anomalous node generation

We follow the published works [14] to generate anomalous nodes by perturbing the topological structure or node attributes of an attributed network. To perturb the topological structure of an attributed network, we adopt the method introduced by [14] to generate some small cliques as in many real-world scenarios. A small clique is a typical anomalous substructure due to larger node degrees than normal nodes. By [1], after we specify the clique size as \(m\), we randomly select \(m\) nodes from the network and then make those nodes fully connected. Then all the \(m\) nodes in the clique are regarded as anomalies. In addition to the injection of structural anomalies, we adopt another attribute perturbation schema introduced by [14] to generate anomalies from an attribute perspective. For each selected node \(u_{i}\), we randomly pick another \(k\) nodes and select node \(u_{j}\) whose attributes deviate the most from node \(u_{j}\) among the \(k\) nodes by maximizing the Euclidean distance \(||x_{i}-x_{j}||^{2}\). Afterward, we then change the attributes \(x_{i}\) of node \(u_{i}\) to \(x_{j}\).

### Reproducibility

All of the real-world data sets are publicly available. The experiments are performed on a Windows machine with a 24GB RTX 4090 GPU. We use TAM as the backbone of our method to capture the local node affinity. We set the number of clusters to be 10, \(\lambda=0.1\) and \(\alpha=0.8\) for the CERT dataset. For the IMDB dataset, we set the number of clusters to be 10, the value of \(\lambda=1\) and \(\alpha=0.8\). For the DBLP dataset, we set the number of clusters to be 10, the value of \(\lambda=0.01\) and \(\alpha=0.8\). For the BlogCatalog dataset, we set the number of clusters to be 5, \(\lambda=0.01\) and \(\alpha=0.01\). For the Amazon dataset, we set the number of clusters to be 10, \(\lambda=1\) and \(\alpha=0.8\). For the Yelp dataset, we set the number of clusters to be 10, \(\lambda=1\) and \(\alpha=0.8\).

For the Yelp dataset, we set the number of clusters to be 10, \(\lambda=1\) and \(\alpha=0.8\).

### The \(m\)-dimensional case

We first consider the case of \(m=0.1\) and \(m=0.