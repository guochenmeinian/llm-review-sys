[MISSING_PAGE_EMPTY:1]

gets a higher ex-ante expected utility, i.e., the expected utility before agents know their types. In a Bayesian \(k\)-strong equilibrium, no group with at most \(k\) agents can deviate from the equilibrium strategy so that every group member gets a higher expected utility conditioned on every type.

The difference between the two solution concepts is how agents calculated their expected utilities when contemplating whether a deviation is beneficial. We interpret this difference as different attitudes of agents towards deviations. In the ex-ante Bayesian \(k\)-strong equilibrium, a group of agents deviates once the deviation is profitable in the ex-ante expectation. In the Bayesian \(k\)-strong equilibrium, an agent is assumed to be more risk-averse towards deviation and will deviate only when the deviation is profitable conditioned on every possible type. Proposition 1 shows that an ex-ante Bayesian \(k\)-strong equilibrium implies a Bayesian \(k\)-strong equilibrium.

Our technical contributions lie in the study of the collusion problem in peer prediction mechanisms, as a representative of group strategic behavior in Bayesian games, with our solution concepts. We exactly characterize the group sizes \(k\) where the truthful reporting in the peer prediction mechanism by (Brockman, 1983) is an ex-ante Bayesian \(k\)-strong equilibrium (Theorem 1) and a Bayesian \(k\)-strong equilibrium (Theorem 2), respectively. In each case, we show a threshold so that group sizes below this threshold cannot benefit by deviating while group sizes above this threshold can. In general, these thresholds are different for the two types of equilibria we consider. Our thresholds are characterized by the parameters of the game, including the number of agents, common prior, and the scoring adopted by the mechanism. Our result implies that our equilibria parameterized by \(k\) are natural criteria to evaluate the robustness against collusion for a peer prediction mechanism. If truth-telling is an equilibrium with a larger \(k\), the mechanism is more robust against collusion. In the application of the peer prediction mechanism, the scoring rule and the mechanism that maximizes the threshold could be chosen to prevent a wider range of collusion.

We also discuss two other possible scenarios where our solution concept may apply. In the voting scenario where voters only have partial information about the alternatives, it is known that strong equilibria with unlimited coalition sizes may fail to exist when there is a sufficiently large group of voters whose preferences are not aligned with the rest of the voters (Gil et al., 2017). However, the sizes of the deviating groups are typically large in those non-equilibria. Given that it is unlikely for large numbers of voters to collaborate in large elections, it is therefore appealing to study equilibria with bounded deviating groups and obtain more informative results. In the private Blotto game (Blotto, 2018), social media users with noisy information choose to annotate for/against one of the multiple posts. Agents aim to maximize the overall influence of their type on the posts. Our notion interpolates the centralized Colonel Blotto game and the decentralized private Blotto game. The parameter \(k\) becomes an evaluation to characterize scenarios where agents have different centralization levels, where a higher \(k\) represents a higher ability for agents to coordinate and for their type.

### Related Work

Previous work studies group strategic behavior in Bayesian games under different scenarios. Hahn and Yannelis (Hahn and Yannelis, 1992) and Safronov (Safronov, 1992) study coalitional implementation problems under an exchange economy with a strong equilibrium. Ichishi and Idzik (Ichiishi and Idzik, 1993) and Ichishi and Yamazaki (Ichiishi and Yamazaki, 1994) propose the Bayesian strong equilibrium and study its relationship with cooperative game theory. Schoenebeck and Tao (Schoenebeck and Tao, 1992), Han et al. (Han et al., 2007), and Deng et al. (Deng et al., 2017) adopt an approximated version of strong equilibrium to study information aggregation and voting with incomplete information. Nevertheless, none of these works characterizes group strategic behaviors with a bounded size of the group. Guo and Yannelis (Guo and Yannelis, 1995) proposed a coalitional interim equilibrium in which the set of admissible coalitions can be arbitrarily exogenously given. Their solution concept covers a wider range of admissible coalitions than our paper. However, truthful reporting is such an equilibrium only when agents are also coalitionally truthful when they know the report of all other agents, which does not hold for most peer prediction mechanisms. In our setting, truthful reporting fails to be such an equilibrium even with a constant coalition size under mild assumptions (Appendix E). Abraham et al. (Abraham et al., 2018) proposes a \(k\)-coalitional equilibrium where the deviators are allowed to arbitrarily share private information, which may not be applied to many real-world scenarios. For example, the organizer can randomly assign tasks or set limited response periods to prevent agents from arbitrary communication. Moreover, truthful reporting also fails to be such an equilibrium even with a constant coalition size, as signal sharing updates the deviators' beliefs and drives them to different strategies. Our ex-ante Bayesian \(k\)-strong equilibrium is related to the equilibrium in (Gil et al., 2017; Hohn and Yannelis, 1992; O'Connor and Ott, 2017), and our Bayesian \(k\)-strong equilibrium is an extension of the Bayesian strong equilibrium in (Ichiishi and Yamazaki, 1994). In the game with complete information, Aumann (Aumann, 1994) propose the strong Nash equilibrium in which no group of agents has an incentive to deviate. The strong Nash equilibrium (and its variants) has been applied to study group strategic behavior in many scenarios such as congesting game (Hahn and Yannelis, 1992; O'Connor and Ott, 2017; O'Connor and Ott, 2017), voting (Gil et al., 2017; O'Connor and Ott, 2017; O'Connor and Ott, 2017), and Markov game (Gil et al., 2017; O'Connor and Ott, 2017). Abraham et al. (Abraham et al., 2018) studies \(k\)-coalitional strategic behavior under games with complete information. However, a strong Nash equilibrium does not apply to Bayesian games where the information is incomplete.

Our paper is also related to studying the collusion problem in peer prediction mechanisms. Because the appropriate theoretical definitions have not been available, collusion has not been studied explicitly in theoretical peer prediction work. However, many works touch on related concepts. Intuitively, equilibrium selection is related to collusion because agents can coordinate to choose an equilibrium that is bad for the mechanism. Gao et al. (Gao et al., 2017) empirically showed this to be a problem, while Gao et al. (Gao et al., 2017) shows that agents may also coordinate on a low-effort signal. The problem of equilibrium selection is exacerbated by the inevitable existence of uninformative equilibria (Schoenebeck and Tao, 1992; O'Connor and Ott, 2017; Han et al., 2007). Many papers address the problem by developing mechanisms where truthful reporting is more profitable than uninformative collusions (Gil et al., 2017; O'Connor and Ott, 2017; O'Connor and Ott, 2017; O'Connor and Ott, 2017; O'Connor and Ott, 2017; O'Connor and Ott, 2017). More powerfully, works have shown that the truthful equilibrium has the highest possible payments either among all equilibrium (O'Connor and Ott, 2017) or even among all strategies profiles (O'Connor and Ott, 2017; O'Connor and Ott, 2017; O'Connor and Ott, 2017). However, all the latter results consider multi-task peer prediction, while no single-task peer prediction mechanisms have been discovered to have the same merit. Moreover, none of these works study the collusion problem from the perspective of strong equilibrium. Schoenebeck et al. (Schoenebeck et al., 2016) studied a more extreme case where the goal was to design peer prediction mechanisms that are robust against an adversary that controls a constant fraction of the nodes. The present work is different because the deviating groups are required to be strategic and not purely malicious.

Several works study collusion using simulations and measuring how many agents must deviate before truth-telling fails to be the best response for the remaining agents (Ghosh et al., 2016; Snoek et al., 2016) or so that certain dynamics fail to converge back to truth-telling (Schoenebeck et al., 2016). This shows that while the problem is interesting, the theoretical tools available for prior work were insufficient.

## 2. Preliminaries

For an integer \(n\), let \([n]\) denote the set \(\{1,2,\cdots,n\}\). For a finite set \(A\), let \(|A|\) be the number of elements in \(A\), and \(\Delta_{A}\) denote the set of all distributions on \(A\).

_Proper Scoring Rule._ Given a finite set \(\mathcal{S}\), a scoring rule \(PS:\mathcal{S}\times\Delta_{\mathcal{S}}\rightarrow\mathbb{R}\) maps an element \(s\in\mathcal{S}\) and a distribution \(\mathbf{q}\) on \(\mathcal{S}\) to a score. A scoring rule \(PS\) is _proper_ if for any distributions \(\mathbf{q}_{1}\) and \(\mathbf{q}_{2}\), \(\mathbb{E}_{\mathbf{s}\rightarrow\mathbf{q}_{1}}[PS(s,\mathbf{q}_{1})] \geq\mathbb{E}_{\mathbf{s}\rightarrow\mathbf{q}_{1}}[PS(s,\mathbf{q}_{2})]\) and _strictly proper_ if the equality holds only at \(\mathbf{q}_{1}=\mathbf{q}_{2}\).

**Example 2**.: _Given a distribution \(\mathbf{q}\) on a finite set \(\mathcal{S}\), let \(q(s)\) be the probability of \(s\in\mathcal{S}\) in \(\mathbf{q}\). The log score rule \(PS_{1}(s,\mathbf{q})=\log(q(s))\). The Brier/quadratic scoring rule \(PS_{B}(s,\mathbf{q})=2\cdot q(s)-\mathbf{q}\cdot\mathbf{q}\). Both the log scoring rule and the Brier scoring rule are strictly proper._

### Bayesian Game Model

A Bayesian game \(\mathcal{I}=([n],(\mathcal{A}_{l})_{l\in[n]},(\mathcal{S})_{l\in[n]},(v_{l})_{ l\in[n]},\mathbf{q})\) is defined by the following components.

* The set of agents \([n]\).
* For each agent \(i\), \(\mathcal{A}_{l}\) is the set of available actions of \(i\). The action profile \(A=(a_{1},a_{2},\cdots,a_{n})\) is the vector of actions of all the agents.
* For each agent \(i\), \(\mathcal{S}_{i}\) is the set of possible types of agent \(i\). The type characterizes the private information agent \(i\) holds, and the agent can only observe his/her type in the game. The type vector \(S=(s_{1},s_{2},\cdots,s_{n})\) is the vector of types of all agents.
* For each agent \(i\), \(v_{l}:\mathcal{S}_{l}\times\mathcal{A}_{1}\times\cdots\times\mathcal{A}_{n} \rightarrow\mathbb{R}\) is \(i\)'s utility function that maps \(i\)'s type and the action of all the agents to \(i\)'s utility.
* A _common prior_ that the types of the agents follow is a joint distribution \(\mathbf{q}\). For a signal \(s_{i}\) of agent \(i\), we use \(q(s_{i})\) to denote the marginal prior probability that \(i\)'s signal is \(s_{i}\). We assume that \(q(s_{i})>0\) for any \(i\) and any \(s_{i}\in\mathcal{S}_{i}\).

For each agent \(i\), a (mixed) strategy \(\sigma_{i}:\mathcal{S}_{i}\rightarrow\Delta_{\mathcal{N}_{i}}\) maps \(i\) private signal to a distribution on his/her actions. A strategy profile \(\Sigma=(\sigma_{l})_{l\in[n]}\) is a vector of the strategies of all the agents.

Given a strategy profile \(\Sigma\), the _ex-ante_ expected utility of agent \(i\) is

\[u_{i}(\Sigma)=\mathbb{E}_{S\rightarrow\mathbf{q}}\boxtimes_{A}[v_{l}(s_{i},a_ {1},\cdots,a_{n})\mid\Sigma].\]

Similarly, given a strategy profile \(\Sigma\) and a type \(s_{i}\), the _interim_ expected utility of agent \(i\) conditioned on his/her type being \(s_{i}\) is

\[u_{i}(\Sigma\mid s_{i})=\mathbb{E}_{S\rightarrow\mathbf{q}_{-i}\cdot\mathbf{q} _{-i}}\boxtimes_{A}[v_{l}(s_{i},a_{1},\cdots,a_{n})\mid\Sigma],\]

where \(S_{-i}\) is the type vector of all agents except for agent \(i\), and \(\mathbf{q}_{-i|s_{i}}\) is the joint distribution on \(S_{-i}\) conditioned on agent \(i\)'s signal being \(s_{i}\).

### (Ex-ante) Bayesian \(k\)-Strong Equilibrium

In this paper, we focus on agents that coordinate for strategic behaviors before they know their types. This assumption relates to various constraints in real-world scenarios that prevent agents from discussions after knowing their types.

**Example 3**.: _Consider the online crowdsourcing group in Example 1. The website requires workers to make an immediate report after seeing the task so that workers cannot communicate with each other after they know their types. (For example, workers have to submit the report in 30 seconds to reflect their intuition.) However, workers may collude on the same report before seeing the task._

Both equilibria share the same high-level form: there does not exist a group of \(k\) agents and a deviating strategy such that all the deviators' expected utility in the deviation is as good as the equilibrium strategy profile and at least one deviator's expected utility strictly increases. The difference lies in the expected utility. Ex-ante Bayesian \(k\)-strong equilibrium adopts ex-ante expected utility, while Bayesian \(k\)-strong equilibrium adopts interim expected utility on every type.

**Definition 1** (ex-ante Bayesian \(k\)-strong equilibrium).: Given an integer \(k\geq 1\), a strategy profile \(\Sigma\) is an ex-ante Bayesian \(k\)-strong equilibrium (\(k\)-ESSE) if there does not exist a group of agent \(D\) with \(|D|\leq k\) and a different strategy profile \(\Sigma^{\prime}=(\sigma^{\prime}_{1})\) such that

1. for all agent \(i\notin D\), \(\sigma^{\prime}_{i}=\sigma_{i}\);
2. for all \(i\in D\), \(u_{i}(\Sigma^{\prime})\geq u_{i}(\Sigma)\);
3. there exists an \(i\in D\) such that \(u_{i}(\Sigma^{\prime})>u_{i}(\Sigma)\).

**Definition 2** (Bayesian \(k\)-strong equilibrium).: Given an integer \(k\geq 1\), a strategy profile \(\Sigma\) is a Bayesian \(k\)-strong equilibrium (\(k\)-BSE) if there does not exist a group of agent \(D\) with \(|D|\leq k\) and a different strategy profile \(\Sigma^{\prime}=(\sigma^{\prime}_{i})\) such that

1. for all agent \(i\notin D\), \(\sigma^{\prime}_{i}=\sigma_{i}\);
2. for every \(i\in D\) and every \(s_{i}\in\mathcal{S}_{i}\), \(u_{i}(\Sigma^{\prime}\mid s_{i})\geq u_{i}(\Sigma\mid s_{i})\);
3. there exist an \(i\in D\) and an \(s_{i}\in\mathcal{S}_{i}\) such that \(u_{i}(\Sigma^{\prime}\mid s_{i})>u_{i}(\Sigma\mid s_{i})\).

In both solution concepts, if such a deviating group \(D\) and a strategy profile \(\Sigma^{\prime}\) exist, we say that the deviation succeeds.

When \(k=1\), both ex-ante Bayesian \(1\)-strong equilibrium and Bayesian \(1\)-strong equilibrium are equivalent to the Bayesian Nash equilibrium (Snoek, 2016). (See Appendix B.) However, the two solution concepts are not equivalent for larger \(k\). Example 5 illustrates a scenario in the peer prediction mechanism where the same deviation succeeds under the ex-ante Bayesian \(k\)-strong equilibrium but fails under the Bayesian \(k\)-strong equilibrium.

We interpret the difference between the two solution concepts as different attitudes of agents towards deviations. Agents are assumed to be more risk-averse towards deviations under Bayesian\(k\)-strong equilibrium, as they will deviate only when the deviation brings them higher interim expected utility conditioned on every type. On the other hand, agents under the ex-ante Bayesian \(k\)-strong equilibrium will deviate once their ex-ante expected utility increases. Proposition 1 supports our interpretation by revealing that an ex-ante Bayesian \(k\)-strong equilibrium implies a Bayesian \(k\)-strong equilibrium.

**Proposition 1**.: _For every strategy profile \(\Sigma\) and every \(1\leq k\leq n\), if \(\Sigma\) is an ex-ante Bayesian \(k\)-strong equilibrium, then \(\Sigma\) is a Bayesian \(k\)-strong equilibrium._

Proof.: Suppose \(\Sigma^{\prime}\) is an arbitrary deviating profile from \(\Sigma\) with no more than \(k\) deviators, and \(i\) is an arbitrary deviator in \(\Sigma^{\prime}\). Since \(\Sigma\) is an ex-ante Bayesian \(k\)-strong equilibrium, then \(u_{i}(\Sigma^{\prime})\leq u_{i}(\Sigma)\). By the law of total probability, \(u_{i}(\Sigma)=\sum_{i\in\mathcal{S}_{i}}\mathbf{q}(s_{i})\cdot u_{i}(\Sigma \mid s_{i})\). Therefore, one of the following must hold: (1) for all \(s\in\mathcal{S}_{i}\), \(u_{i}(\Sigma^{\prime}\mid s_{i})=u_{i}(\Sigma\mid s_{i})\), or (2) there exists \(s\in\mathcal{S}_{i}\), \(u_{i}(\Sigma^{\prime}\mid s_{i})<u_{i}(\Sigma\mid s_{i})\). In either case, the deviation fails. Therefore, \(\Sigma\) is a Bayesian \(k\)-strong equilibrium. 

### Peer Prediction Mechanism

In a peer prediction mechanism, each agent receives a private signal in \(\mathcal{S}=\{\ell,h\}\) and reports it to the mechanism. All the agents share the same type set \(\mathcal{S}_{i}=\mathcal{S}\) and action set \(\mathcal{A}_{i}=\mathcal{S}\).

\(\mathbf{q}\) is the common prior joint distribution of the signals. Let \(\Psi_{i}\) denote the random variable of agent \(i\)'s private signal. We assume that the common prior \(\mathbf{q}\) is symmetric -- for any permutation \(\pi\) on \([n]\), \(\mathbf{q}(\Psi_{1}=s_{1},\Psi_{2}=s_{2},\cdots,\Psi_{n}=s_{n})=q(\Psi_{1}=s_{ \pi(1)},\Psi_{2}=s_{\pi(2)},\cdots,\Psi_{n}=s_{\pi(n)})\).

\(q(s)\) is the prior marginal belief that an agent has signal \(s\), and \(q(s\mid s^{\prime})\) be the posterior belief of an agent with private signal \(s^{\prime}\) on another agent having signal \(s\). We also define \(\mathbf{q}_{s}=q(\cdot\mid s)\) be the marginal distribution on \(\mathcal{S}\) conditioned on \(s\). We assume that an agent with \(h\) signal has a higher estimation than an agent with \(\ell\) signal on the probability that another agent has \(h\) signal, i.e., \(q(h\mid h)>q(h\mid\ell)\). We also assume that any pair of signals is not fully correlated, which is \(q(h\mid\ell)>0\) and \(q(\ell\mid h)>0\).

We adopt a modified version of the peer prediction mechanism (Shen et al., 2017) characterized by a (strictly) proper scoring rule \(PS\). The mechanism compares the report of agent \(i\), denoted by \(a_{i}\), with the reports of all other agents. For each agent \(j\) with report \(a_{j}\), the reward \(i\) gains from comparison with \(j\)'s report is \(R_{i}(a_{j})=PS(a_{j},\mathbf{q}_{a_{i}})\). The utility of agent \(i\) is the average reward from each \(j\).

\[\alpha_{i}(s_{i},A)=\frac{1}{n-1}\sum_{j\in[n],j\neq i}R_{i}(a_{j}).\]

**Remark 1**.: _In the original mechanism in (Shen et al., 2017), the reward of an agent \(i\) is \(R_{i}(a_{j})\), where \(j\) is chosen uniformly at random from all other agents. We derandomize the mechanism so that it fits better into the Bayesian game framework while the expected utility of an agent is unchanged._

**Example 4**.: _Suppose \(n=100\). For the common prior, the prior belief \(q(h)=2/3\), and \(q(\ell)=1/3\). The posterior belief \(q(h\mid h)=0.8\) and \(q(\ell\mid\ell)=0.6\). Suppose the Brier scoring rule is applied to the peer prediction mechanism. Consider an agent \(i\) with report \(a_{i}=h\). Then, \(i\)'s reward from a peer \(j\) with report \(a_{j}=h\) is \(R_{i}(a_{j})=PS_{B}(h,\mathbf{q}_{h})=2\cdot q(h\mid h)-q(h\mid h)^{2}-q(\ell \mid h)^{2}=0.92\). Similarly, \(i\)' reward from another peer \(j^{\prime}\) with report \(a_{j^{\prime}}=\ell\) is \(PS_{B}(\ell,\mathbf{q}_{h})=-0.28\)._

A (mixed) strategy \(\sigma:\mathcal{S}_{i}\rightarrow\Delta_{i,\Psi_{i}}\) maps an agent's type to a distribution on his/her action. A strategy profile \(\Sigma=(\sigma_{i})_{i\in[n]}\) is a vector of the strategies of all the agents. An agent is _truthful_ if he/she always truthfully reports his/her private signal. Let \(\sigma^{*}\) be the truthful strategy and \(\Sigma^{*}\) be the strategy profile where all agents are truthful. We also represent a strategy in the form \(\sigma=(\beta_{\ell},\beta_{h})\in[0,1]^{2}\), where \(\beta_{\ell}\) and \(\beta_{h}\) are the probability that an agent playing \(\sigma\) reports \(h\) conditioned on his/her signal begin \(\ell\) and \(h\), respectively. The truthful strategy \(\sigma^{*}=(0,1)\).

Given the strategy profile \(\Sigma\), the ex-ante expected utility of an agent \(i\) is

\[u_{i}(\Sigma)=\frac{1}{n-1}\sum_{j\in[n],j\neq i}\mathbb{E}_{s_{i}\sim\mathbf{ q}_{i}\sim\sigma_{i}(s_{i})}\mathbb{E}_{s_{j}\sim\mathbf{q}_{i}\sim\sigma_{j}(s_{j})}R_{i}(a_{j}).\]

Given a strategy profile \(\Sigma\) and a type \(s_{i}\), the interim expected utility of an agent \(i\) conditioned on his/her type being \(s_{i}\) is

\[u_{i}(\Sigma\mid s_{i})=\frac{1}{n-1}\sum_{j\in[n],j\neq i}\mathbb{E}_{a_{i} \sim\sigma_{i}(s_{i})}\mathbb{E}_{s_{j}\sim\mathbf{q}_{i}\sim\sigma_{j}(s_{j})}R _{i}(a_{j}).\]

**Example 5**.: _We follow the setting in example 4. Let \(\Sigma^{*}\) be the profile where all agents report truthfully. Let \(D\) be a group containing \(k=40\) agents and \(\Sigma^{\prime}\) be the profile where all deviators report \(h\)._

_For truthful reporting, consider an agent \(i\) and his/her peer \(j\). The probability that both \(i\) and \(j\) receive (and report) signal \(h\) is \(q(h)\cdot q(h\mid h)=2/3*0.8=0.533\), and \(i\) will be reward \(PS(h,\mathbf{q}_{h})=0.92\). Other probabilities can be calculated similarly. Adding on the expectation of different pairs of signals, we can calculate the ex-ante expected utility of \(i\) in truthful reporting: \(u_{i}(\Sigma^{*})=\sum_{s_{i},s_{j}\in\{\ell,h\}}q(s_{i})\cdot q(s_{j}\mid s_{i}) \cdot PS(s_{j},\mathbf{q}_{s_{i}})=0.627\)._

_Now we consider the expected utility of a deviator \(i\) deviating profile \(\Sigma^{*}\). Since all the deviators always report \(h\), the expected reward \(i\) gets from a truthful reporter, \(i\)'s expected reward is \(q(h)\cdot PS(h,\mathbf{q}_{h})+q(\ell)\cdot PS(\mathbf{q}_{h})=0.52\). Among all the other agents, \(k-1=39\) agents are deviators, and \(n-k=60\) agents are truthful reporters. Therefore, \(i\)'s expected utility on \(\Sigma^{*}\) is \(u_{i}(\Sigma^{*})=0.682>u_{i}(\Sigma^{*})\). Therefore, the deviation succeeds under the ex-ante Bayesian \(k\)-strong equilibrium._

_However, the deviating fails under the Bayesian \(k\)-strong equilibrium. The truthful expected utility conditioned on \(i\)'s signal is \(\ell\) is \(u_{i}(\Sigma^{*}\mid\ell)=\sum_{s_{j}\in\{\ell,h\}}q(s_{j}\mid\ell)\cdot PS(s_{j}, \mathbf{q}_{\ell})=0.52\). On the other hand, when agents deviate to \(\Sigma^{\prime}\), \(i\)'s reward from a truthful agents becomes \(\sum_{s_{j}\in\{\ell,h\}}q(s_{j}\mid\ell)\cdot PS(s_{j},\mathbf{q}_{h})=0.2\). Therefore, \(i\)'s interim expected utility \(u_{i}(\Sigma^{*}\mid\ell)=0.484<u_{i}(\Sigma^{*}\mid\ell)\)._

## 3. Dichotomies on Equilibria

Our theoretical results focus focus on the collusive behavior in the peer prediction mechanisms. While the mechanism is known to be prone to collusions, Shnayder and Parkes (Shnayder and Parkes, 2016) empirically shows that there is a lower bound for collusion to be profitable. With our new solution concepts, our theoretical results specify the exact threshold. For both equilibria, we find the largest group size \(k_{B}\) (\(k_{B}\), respectively) such that truthful reporting is an equilibrium. Moreover, for any \(k\) larger than \(k_{E}\) (\(k_{B}\), respectively), truthful reporting fails to be an equilibrium. We first present the result of the ex-ante Bayesian \(k\)-strong equilibrium.

**Theorem 1**.: _In the peer prediction mechanism, for any \(n\geq 2\) and any strictly proper scoring rule PS, truthful reporting \(\Sigma^{*}\) is an ex-ante Bayesian \(k_{E}\)-strong equilibrium, where_

\[k_{E}^{h}=\begin{cases}\frac{\left((n-1)\Sigma_{g_{k}}(PS(s_{\mathbf{d}_{k}}))- PS(s_{\mathbf{d}_{k}})\right)}{PS(h_{\mathbf{d}_{k}})-PS(s_{\mathbf{d}_{k}})}\end{cases}+1&\text{if }PS(h_{\mathbf{d}_{k}})>PS(t,\mathbf{q}_{h})\\ \\ \frac{\text{otherwise}}{k_{E}^{\ell}}=\begin{cases}\frac{\left((n-1)\Sigma_{g_{k}}( pS(s_{\mathbf{d}_{k}}))-PS(s_{\mathbf{d}_{k}})\right)}{PS(s_{\mathbf{d}_{k}})-PS(s_{\mathbf{d}_{k}})}\right]}{n}+1&\text{if }PS(t,\mathbf{q}_{j})>PS(h,\mathbf{q}_{j})\\ \\ \frac{\text{otherwise}}{k_{E}}=\min(k_{E}^{h},k_{E}^{\ell},n).\end{cases}\]

_For all \(n\geq k>k_{E}\), truthful reporting is NOT an ex-ante Bayesian \(k\)-strong equilibrium._

While a proof sketch is presented below, here we give a brief explanation of the thresholds. \(k_{E}^{h}\) and \(k_{E}^{\ell}\) are characterized by comparing the ex-ante expected utility of a deviator between truthful reporting and all the deviators always report \(h\) (\(\ell\), respectively). Take \(k_{E}^{h}\) as an example. The numerator \(\mathbb{E}_{g-\mathbf{q}_{j}}[PS(s,\mathbf{q}_{j})-PS(s,\mathbf{q}_{j})]\) is proportional to the loss that the deviator suffers in his expected rewards from the truthful reporters in switching from truthful reporting to always reporting \(h\). The denominator \(PS(h,\mathbf{q}_{h})-PS(t,\mathbf{q}_{h})\) is proportional to the amount that, for a deviator, the expected reward gain from other deviators exceeds the expected reward loss from truthful reporting. If \(PS(h,\mathbf{q}_{h})-PS(t,\mathbf{q}_{h})<0\), the extra gain never compensates for the loss, so the deviation cannot succeed for any \(k\leq n\). Otherwise, a group size of \(k>k_{E}^{h}\) is required for the deviation to succeed.

**Example 6**.: _We calculate the threshold for ex-ante Bayesian \(k\)-strong equilibrium for the instance in Example 4. For \(k_{E}^{h}\), the numerator equals to \(q(h\mid)\cdot(0.28-0.92)+q(\ell\mid)\cdot(0.68+0.28)=0.32\). The denominator, according to the Brier scoring rule, equals to \(PS(h,\mathbf{q}_{h})-PS(t,\mathbf{q}_{h})=2\cdot(q(h\mid h)-q(\ell\mid h))=1.2\). Therefore, \(k_{E}^{h}=\lfloor\frac{4}{15}\cdot(n-1)\rfloor+1\). Similarly, we calculate that \(k_{E}^{\ell}=\lfloor\frac{1}{5}\cdot(n-1)\rfloor+1\). Therefore, when \(n=100\), a deviation group needs at least \(\lfloor\frac{4}{15}\times 99\rfloor+1=27\) deviators to succeed. This aligns with Example 5, where a 40-agent group succeeds._

Similarly, Theorem 2 characterizes the threshold under Bayesian \(k\)-strong equilibrium.

**Theorem 2**.: _In the peer prediction mechanism, there exists an no such that for every \(n\geq n_{0}\) and any strictly proper scoring rule PS, truthful reporting \(\Sigma^{*}\) is a Bayesian \(k\)-strong equilibrium in peer prediction, where_

\[k_{B}^{h}=\begin{cases}\left[\frac{(n-1)\Sigma_{g_{k}}(PS(s_{\mathbf{d}_{k}})- PS(s_{\mathbf{d}_{k}}))}{q(\ell\mid)-PS(s_{\mathbf{d}_{k}})}\right]&\text{if }PS(h,\mathbf{q}_{h})>PS(t,\mathbf{q}_{h})\\ \\ n&\text{otherwise}\end{cases}\]

\[k_{B}^{\ell}=\begin{cases}\left[\frac{(n-1)\Sigma_{g_{k}}(PS(s_{\mathbf{d}_{k}}) -PS(s_{\mathbf{d}_{k}}))}{q(h\mid h)-PS(s_{\mathbf{d}_{k}})}\right]&\text{if }PS(t,\mathbf{q}_{j})>PS(h,\mathbf{q}_{j})\\ \\ n&\text{otherwise}\end{cases}\]

\[k_{B}=\min(k_{B}^{h},k_{B}^{\ell},n).\]

_For all \(n\geq k>k_{B}\), truthful reporting is NOT a Bayesian \(k\)-strong equilibrium._

The lower bound \(n_{0}\) on \(n\) is characterized by the common prior \(q\) and the scoring rule \(PS\) and is independent of \(n\). The explicit expression on \(n_{0}\) is in Appendix D.

The thresholds for the Bayesian \(k\)-strong equilibrium \(k_{B}^{h}\) and \(k_{B}^{\ell}\) are larger than those for the ex-ante Bayesian \(k\)-strong equilibrium \(k_{B}^{h}\) and \(k_{E}^{\ell}\) respectively. This is because, for example, \(k_{B}^{h}\) is characterized by comparing the interim utility of a deviator conditioned on signal \(\ell\) between truthful reporting and all deviators reporting \(h\). In ex-ante, the deviator \(i\) has a probability of \(q(\ell)\) to report untruthfully and suffer a loss on expected reward from truthful reporters. When \(i\) has a private signal \(\ell\), such probability becomes \(1\). Therefore, the deviator suffers more loss in the interim expected utility than in the ex-ante expected utility in the expected reward from truthful reporters. On the other hand, \(i\) gets the same extra gain in the reward from other deviators as in the ex-ante expected utility. Therefore, a larger group is needed to make the deviation succeed.

**Example 7**.: _We calculate the threshold for Bayesian \(k\)-strong equilibrium for the instance in Example 4. For \(k_{B}^{h}\), the numerator equals to \(0.32\). The denominator is multiplied by \(q(\ell\mid)=0.6\) compared with \(k_{E}^{h}\) and equals to \(1.2\times 0.6=0.72\). Therefore, \(k_{E}^{h}=\lceil\frac{4}{3}\cdot(n-1)\rceil\). Similarly, we calculate that \(k_{E}^{\ell}=\lceil n-1\rceil\). Therefore, when \(n=100\), a deviation group needs at least \(45\) deviators to succeed. This also aligns with Example 5, where a 40-agent group fails in deviation._

Theorem 1 and 2 imply that the ex-ante Bayesian \(k\)-strong equilibrium and the Bayesian \(k\)-strong equilibrium are natural criteria to evaluate the robustness against collusion for a peer prediction mechanism. If truth-telling is an equilibrium with a larger \(k\), the mechanism is more robust against collusion. If an information collector aims to prevent collusion in a peer prediction task, he/she could carefully select the mechanism and the scoring rule to maximize the threshold \(k\) under which truth-telling becomes an equilibrium.

**Example 8**.: _If we change the scoring rule from the Brier scoring rule Example 4 to the lag scoring rule with base \(\ell\) in and follow the calculation in Example 6, we have \(k_{E}=\lfloor 0.275(n-1)\rfloor+1\). When \(n=100\), a group of at least 28 agents is needed to perform a successful deviation. Therefore, the lag scoring rule is more robust than the Brier scoring rule in this instance._

### Proof Sketch of Theorem 1

The proof consists of two steps. In Step 1, \(k_{E}\) is characterized by comparing the ex-ante expected utility of a deviator when every agent reports truthfully and when all \(k\) deviators always report \(h\) (and always report \(\ell\), respectively). The two deviations bring a deviator higher expected utility if and only if \(k>k_{E}\). In Step 2, we show that for any \(k\leq k_{E}\) and any deviating strategy profile \(\Sigma^{\prime}\), the average expected utility among all the deviators when \(\Sigma^{\prime}\) is played will not exceed the expected utility when every agent reports truthfully. Therefore, either no deviators have strictly increasing expected utility or some deviators have strictly decreasing utility after deviation, and the deviation cannot succeed. The full proof is in Appendix C.

**Step 1: determine \(k_{E}\).** We show how \(k_{E}^{h}\) is determined by comparing truthful reporting strategy profile \(\Sigma^{*}\) and the deviating strategy profile \(\Sigma\) where all \(k\) deviators always report \(h\), i.e., \(\sigma=(1,1)\). The reasoning for \(k_{E}^{i}\) is similar. The condition that a deviator \(i\) is willing to deviate is \(u_{i}(\Sigma)>u_{i}(\Sigma^{*})\). The inequality should be strict because all deviators have equal expected utility in \(\Sigma\).

\(u_{i}(\Sigma)\) can be viewed as a linear combination of the expected utility \(i\) gets from the truthful agents, denoted by \(u_{i}(\Sigma\mid\text{truthful})\), and the expected utility \(i\) gets from other deviators, denoted by \(u_{i}(\Sigma\mid\text{deviation})\). In \(\Sigma\), there are \(n-k\) truthful reporters and \(k-1\) deviators other than \(i\). Therefore, \(u_{i}(\Sigma)=\frac{n-k}{n-1}\). \(u_{i}(\Sigma\mid\text{truthful})+\frac{k-1}{n-1}\cdot u_{i}(\Sigma\mid\text{deviation})\). Let \(\Delta u_{d}=u_{i}(\Sigma^{*})-u_{i}(\Sigma\mid\text{deviation})\), and \(\Delta u_{d}=u_{i}(\Sigma^{*})-u_{i}(\Sigma\mid\text{truthful})\). Then \(u_{i}(\Sigma)>u_{i}(\Sigma^{*})\) is equivalent to \(\frac{k-1}{n-1}\cdot\Delta u_{d}+\frac{n-k}{n-1}\cdot\Delta u_{d}<0\).

The ex-ante expected reward of deviator \(i\) from truthful reporters can be divided into two parts, one conditioned on \(i\)'s private signal being \(h\), the other on \(i\)'s signal being \(\ell\). When \(i\)'s signal is \(h\), \(i\) reports \(h\) both in \(\Sigma^{*}\) and in \(\Sigma\), and the expected rewards from truthful reporters in this part are the same. When \(i\)'s signal is \(\ell\), \(i\) reports \(\ell\) in \(\Sigma^{*}\) and \(h\) in \(\Sigma\), and the expected rewards make a difference. Therefore, \(\Delta u_{\ell}=q(\ell)\cdot\mathbb{E}_{q\sim q}\{PS(s,\mathbf{q}_{\ell})-PS(s, \mathbf{q}_{\ell})\}\). According to the properness of \(PS\), \(\Delta u_{\ell}>0\). Therefore, when \(\Delta u_{\ell}>\Delta u_{d}\), \(u_{i}(\Sigma)>u_{i}(\Sigma^{*})\) is equivalent to

\[k>\frac{\Delta u_{\ell}}{\Delta u_{\ell}-\Delta u_{d}}\cdot(n-1)+1.\]

When \(\Delta u_{\ell}\leq\Delta u_{d}\), the condition does not hold for any \(k\), and the deviation will never succeed.

From the calculation, \(\Delta u_{\ell}-\Delta u_{d}=q(\ell)\cdot(PS(h,\mathbf{q}_{h})-PS(\ell, \mathbf{q}_{h}))\). Therefore, when \(PS(h,\mathbf{q}_{h})>PS(\ell,\mathbf{q}_{h})\), \(\Delta u_{\ell}>\Delta u_{d}\), and \(u_{i}(\Sigma)>u_{i}(\Sigma^{*})\) is equivalent to

\[k>\frac{\mathbb{E}_{q\sim q_{\ell}}[PS(s,\mathbf{q}_{\ell})-PS(s,\mathbf{q}_{h })]}{PS(h,\mathbf{q}_{h})-PS(\ell,\mathbf{q}_{h})}\cdot(n-1)+1.\]

And when \(PS(h,\mathbf{q}_{h})\leq PS(t,\mathbf{q}_{h})\), \(\Delta u_{\ell}\leq\Delta u_{d}\), and \(u_{i}(\Sigma)>u_{i}(\Sigma^{*})\) does not hold for any \(k\). This is how \(k_{E}^{i}\) is determined. \(k_{E}^{i}\) is determined in a similar reasoning.

**Step 2: Deviations cannot succeed for \(k\leq k_{E}\)**. For \(k=1\), the statement holds from the truthfulness of the mechanism. Suppose \(2\leq k\leq k_{E}\), and \(\Sigma\) be an arbitrary deviating strategy. Let \(u(\Sigma^{*})\) be the expected utility of truthful reporting, which is equal for all agents. For each deviator \(i\), let \(\sigma_{i}=(\beta_{\ell_{\ell}}^{i}\beta_{\ell_{\ell}}^{i})\) denote \(i\)'s strategy in \(\Sigma\). We show that \(\frac{1}{k}\sum_{i\in D}u_{i}(\Sigma)\leq u(\Sigma^{*})\). Therefore, either there exists some deviator \(i\) such that \(u(\Sigma)<u(\Sigma^{*})\), or for all the deviator \(i\) there is \(u_{i}(\Sigma)=u(\Sigma^{*})\). In either case, the deviation fails.

Now let \(\hat{\sigma}=(\hat{\rho}_{\ell},\hat{\beta}_{h})=\frac{1}{k}\sum_{i\in D}\sigma _{i}\) be the average of the deviator's strategies, and \(\Sigma\) be the strategy profile where all agents in \(D\) plays \(\hat{\sigma}\) and all other agents report truthfully. \(u_{i}(\hat{\Sigma})\) is equal among all the deviators \(i\) due to symmetricity (and denoted by \(u(\Sigma)\)). We first show that \(\frac{1}{k}\sum_{i\in D}u_{i}(\Sigma\mid h)\leq u(\Sigma)\) (the average expected utility of deviators playing \(\Sigma\) will not exceed the expected utility when each deviator plays \(\hat{\sigma}\)) and then that \(u(\Sigma)\leq u(\Sigma^{*})\) (the expected utility that each deviator play \(\hat{\sigma}\) will not exceed the truthful expected utility).

To show \(\frac{1}{k}\sum_{i\in D}u_{i}(\Sigma)\leq u(\hat{\Sigma})\), we compare the expected reward from truthful agents and deviators separately. For a deviator \(i\), \(u_{i}(\Sigma\mid\text{truthful})\) is independent of the strategy of other deviators and is linear on \(\beta_{\ell}\) and \(\beta_{h}\). Therefore, \(\frac{1}{k}\sum_{i\in D}u_{i}(\Sigma\mid\text{truthful})=u(\Sigma\mid\text{ truthful})\).

For the deviator's part, \(u_{i}(\Sigma\mid\text{deviation})\) is the average of \(i\)'s expected reward from comparing the report with all other deviators \(j\in D\). Such expected reward is linear on \(j\)'s strategy given a fixed \(i\)'s strategy and linear on \(i\)'s strategy given a fixed \(j\)'s strategy. Therefore, the average expected reward from agents with different strategies equals to the reward from a peer playing the average strategy, and \(u_{i}(\Sigma\mid\text{deviation})\) equals to \(i\)'s expected reward from an agent playing the average strategy \(\hat{\sigma}\) minus a share of \(i\)'s expected reward from an agent playing \(\hat{\sigma}_{i}\). Given a strategy \(\sigma=(\beta_{\ell},\beta_{h})\), let \(f(\beta_{\ell},\beta_{h})\) be the expected reward of an agent playing \(\sigma\) from another agents also playing \(\sigma\). Then

\[\frac{1}{k}\sum_{i\in D}u_{i}(\Sigma\mid\text{deviation})=\frac{k}{k-1}f(\beta _{\ell},\beta_{h})-\frac{1}{(k-1)k}\sum_{i\in D}f(\beta_{\ell}^{i},\beta_{h} ^{i}).\]

It turns out that \(f\) is a convex function. Therefore, \(\frac{1}{k}\sum_{i\in D}u_{i}(\Sigma\mid\text{deviation})\leq f(\beta_{\ell}, \beta_{h})=u(\Sigma\mid\text{deviation})\). Combining the truthful part and the deviator part, we show that \(\frac{1}{k}\sum_{i\in D}u_{i}(\Sigma)\leq u(\Sigma)\).

Finally, we show that \(u(\hat{\Sigma})\leq u(\Sigma^{*})\). Note that \(u(\hat{\Sigma})\) can be viewed as a convex function on \(\beta_{\ell}\) and \(\beta_{h}\). This is because \(u(\hat{\Sigma}\mid\text{truthful})\) is linear on \(\hat{\sigma}\), and \(u(\hat{\Sigma}\mid\text{deviation})=f(\beta_{\ell},\beta_{h})\) is convex on \(\hat{\beta}_{\ell}\) and \(\hat{\beta}_{h}\). Therefore, it is sufficient to show that \(u(\hat{\Sigma})\leq u(\Sigma^{*})\) on the four corner cases of \(\hat{\sigma}\): truthful reporting: \(\hat{\sigma}=(0,1)\), always reporting \(\hat{\sigma}\): \(\hat{\sigma}=(1,0)\), and always tell a lie \(\hat{\sigma}=(1,0)\). When \(\hat{\sigma}=(0,1)\), all the deviator also report truthfully, and \(\hat{\Sigma}=\Sigma^{*}\). For \(\hat{\sigma}=(0,0)\) and \(\hat{\sigma}=(1,1)\), \(k\leq k_{E}\) guarantees that \(u(\hat{\Sigma})\leq u(\Sigma^{*})\). Finally, when \(\hat{\sigma}=(1,0)\), similar reasoning to Step 1 shows that such deviation cannot succeed. 

### Proof Sketch of Theorem 2.

The steps of the proof resemble the steps of the proof of Theorem 1, yet the techniques are different. In Step 1, we determine \(k_{B}\) by comparing the interim expected utilities of a deviator when every agent reports truthfully and when all \(k\) deviators always report \(h\) (\(\ell\), respectively). In Step 2, we show that for any \(k\leq k_{E}\) and any deviating strategy profile \(\hat{\Sigma}\) where all the deviators play the same strategy \(\hat{\sigma}\), the expected utility of a deviator on \(\hat{\Sigma}\) will not exceed the expected utility when every agent reports truthfully. In Step 3, we show that for sufficiently large \(n\), any \(k\leq k_{B}\), and any deviating strategy profile \(\Sigma\), there exists a deviator whose expected utility is strictly smaller than the expected utility when every agent reports truthfully. The full proof is in Appendix D.

The main technical difficulty lies in Step 2 and Step 3. Let \(u(\hat{\Sigma}\mid h)\) and \(u(\hat{\Sigma}\mid\ell)\) be the interim expected utility of a deviator conditioned on his/her signal being \(h\) and \(\ell\), respectively, when \(\hat{\Sigma}\) is played. \(u(\hat{\Sigma}\mid h)\) and \(u(\hat{\Sigma}\mid\ell)\) can still be viewed as functions on \(\hat{\rho}_{\ell}\) and \(\hat{\beta}_{h}\). However, unlike the ex-ante \(u(\hat{\Sigma})\), they are not convex. Therefore, we cannot get \(\frac{1}{k}\sum_{i\in D}u_{i}(\Sigma\mid h)\leq u(\hat{\Sigma}\mid h)\) or \(u(\hat{\Sigma}\mid h)\leq u(\Sigma^{*})\) (or the \(\ell\) side) directly from similar reasoning with those in Theorem 1.

**In Step 2**, we instead show that for any \(\hat{\Sigma}\), either \(u(\hat{\Sigma}\mid h)\leq u(\Sigma^{*}\mid h)\) or \(u(\hat{\Sigma}\mid\ell)\leq u(\hat{\Sigma}^{*}\mid h)\) holds. Although \(u(\hat{\Sigma}\mid h)\) is not convex, the convexity (or linearity) still holds in certain directions. Here we slightly abuse the notation to write \(u(\hat{\Sigma}\mid h)\) as \(u(\hat{\rho}_{\ell},\hat{\beta}_{h}\mid h)\). The following properties hold. (1) When \(\hat{\rho}_{\ell}\) is fixed, \(u(

[MISSING_PAGE_FAIL:7]

preferences, we mean that all voters' utilities for alternative \(\mathcal{A}\) are higher in world state \(X\) than in world state \(Y\) and their utilities for alternative \(\mathcal{B}\) are higher in world state \(Y\) than in world state \(X\). That is, all voters' preferences are aligned in that they agree \(X\) "corresponds to" \(\mathcal{A}\) and \(Y\) "corresponds to" \(\mathcal{B}\), although the extent the voters' preferences are aligned with this correspondence can be different and due to which voters can be classified into three types:

* the "left-wing voters" who always prefer \(\mathcal{A}\): \(o(\mathcal{A},X)>o(\mathcal{A},Y)>o(\mathcal{B},Y)>o(\mathcal{B},X)\)
* the "right-wing voters" who always prefer \(\mathcal{B}\): \(o(\mathcal{B},Y)>o(\mathcal{B},X)>o(\mathcal{A},Y)\)
* "swing voters" who prefer the alternative corresponding to the world state: \(o(\mathcal{A},X)>o(\mathcal{B},X)\) and \(o(\mathcal{B},Y)>o(\mathcal{A},Y)\) where \(o(a,s)\) denotes the utility for alternative \(a\in\{\mathcal{A},\mathcal{B}\}\) given the actual world state \(s\in\{X,Y\}\).

The story is much more complicated with general utilities \(o(\cdot,\cdot)\) that are not necessarily aligned. In Deng et al. (2017), it is proved that strong Bayes Nash equilibria may not exist even with only two types of voters with antagonistic preferences. In particular, "good" equilibria that identify the majority-favored alternative only exist when the voters from one type significantly outnumber the voters from the other type. When the population sizes of the two types of voters are close, Deng et al. (2017) show that no strong Bayes Nash equilibrium exists.

However, only strong equilibria with unrestrictive group sizes are considered in the above-mentioned work. A typical deviation group in a strategy profile consists of all voters of the same type, and the existence of this kind of large deviation group prevents many strategy profiles from being equilibria. When considering more practical scenarios with bounded coalition sizes, more "good" equilibria are attainable. Given the large size of deviation groups in the non-equilibria found in Deng et al. (2017), it is likely there is an interpolation between the deviation group size \(k\) and the distribution of voters from different types where "good" equilibria exist. This provides a fine-grained structure to the problem compared with the "all-or-nothing" result in Deng et al. (2017).

Strong equilibria are even less likely to exist for more general utilities. It is appealing to apply our new equilibrium concepts with bounded deviating group sizes to characterize voters' strategic behaviors and obtain more positive and fine-grained results. We believe this is a challenging yet exciting future research direction.

### Private Blotto Game

Private Blotto game (Deng et al., 2017) is a decentralized variation of the classic Colonel Blotto game (Deng et al., 2017). It is proposed in order to model the conflict in the crowdsourcing social media annotation. For example, the Community Notes on X.com (Zen et al., 2018) allows users to vote for/against posts to identify misinformation and toxic speech with the wisdom of the crowd.

**Example 9**.: _Suppose there are \(n\) platform users and \(m\) posts on a topic (for example, whether restrictions should be made for the COVID pandemic). Users obtain different private information from different sources, which can be generally categorized into two types, pros and cons. Each user simultaneously chooses and labels one post based on their type. The labels on each post will eventually determine the influence on the readers. A post with more supporters spreads more widely, and a post with more opponents will be announced as misinformation. Each user aims to maximize the influence of their type and plays the game strategically. What will be a stable status in such a scenario?_

The traditional Colonel Blotto game models this scenario as a centralized game, where two opposite "colonels" (for example, campaign groups) control all the users. In the Private Blotto game, on the other hand, users make their own decisions on where to deploy. This better simulates the modern social media environment where a central coordinator is generally lacking.

**Definition 3** (Private Blotto game.).: \(n\) agents are competing over \(m\) items. Each agent has a type (_pro_ or _con_). Every agent (simultaneously) chooses exactly one item to label. The outcome of each item is determined by some outcome function. The disutility of each agent is the distance from the agent's type to each item's outcome.

The results in the Private Blotto game (Deng et al., 2017) appear to heavily rely on the complete lack of coordination, which is also not entirely realistic. While a central coordinator is lacking, an agent can still locally coordinate with a few others. This allows (small) strategic groups and local campaigns to emerge in real-world scenarios. Moreover, these settings nearly always lack complete information and might be more faithfully modeled by agents receiving different information about various topics.

In this setting, our new solution concept of (ex-ante) Bayesian \(k\)-strong equilibrium seamlessly interpolates between these two extremes of complete centralization and complete decentralization. The bound \(k\) can characterize how well-organized the agents are. When \(k=1\), agents are fully decentralized. A larger \(k\) characterizes scenarios where agents coordinate with friends, neighborhoods, or campaigns on relevant issues. Finally, when \(k\) is large enough, agents can be viewed as commanded by two opposite centralized "colonels", and the game becomes closer to the traditional Colonel Blotto game. Moreover, our definition will also naturally extend to the setting where agents have more than two sides (for example, different political factions that are more or less aligned) and the scenario where the agent's utilities are related to an underlying ground truth rather than peer partisanship.

## References

* Abraham et al. (2006) Itai Abraham, Danny Dolev, Rica Gonen, and Joe Halpern. 2006. Distributed computing meets game theory: robust mechanisms for rational secret sharing and multiparty computation. In _Proceedings of the Twenty-Fifth Annual ACM Symposium on Principles of Distributed Computing (Denver, Colorado, USA)_. Association for Computing Machinery, New York, NY, USA, 53-62.
* Abraham et al. (2008) Itai Abraham, Danny Dolev, and Joseph T. Halpern. 2008. Lower Bounds on Implementing Robust and Resilient Relations. In _Proceedings of Cryptography_, Ran Canetti (Ed.). Springer Berlin Heidelberg, Berlin, Heidelberg, 302-319.
* Acharya (2016) Arulkany. 2016. Information aggregation failure in a model of social mobility. _Games and Economic Behavior_ 100 (2016), 257-272.
* Angeli et al. (2018) S N Angeli, Altscham Milan, and Lucas Sage. 2018. Adverse selection in distributive politics. _Available at SSRN 3357905_ (2018).
* Baum (1959) Robert J. Baum. 1959. Acceptable points in general cooperative n-person games. In _Annals of Mathematics Study_ 40. Contributions to the Theory of Games, Vol. IV. Princeton University Press, 287-324.
* Anstein-Smith and Shields (1996) David Anstein-Smith and Jeffrey S. Shields. 1996. Information aggregation, rationality, and the Condorcet jury theorem. _American political science review_ 90, 1 (1996), 34-5.
* Barbera et al. (2001) Salvador Barbera, Michael Maschler, and Jonathan Shalev. 2001. Voting for voters: a model of electoral evolution. _Games and Economic Behavior_ 37, 1 (2001), 40-78.
* Bhattacharya (2013) Sourav Bhattacharya. 2013. Preference monotonicity and information aggregation on elections. _Geometrika_ 81, 3 (2013), 1229-1247.

[MISSING_PAGE_FAIL:9]

## Appendix A Properties of proper scoring rule.

For a scoring rule \(PS\), we define \(\bar{FS}:\Delta_{S}\times\Delta_{S}\rightarrow\mathbb{R}\) such that for two distributions \(\mathbf{q}_{1}\) and \(\mathbf{q}_{2}\), \(\bar{FS}(\mathbf{q}_{1};\mathbf{q}_{2})=\mathbb{E}_{s\sim\mathbf{q}_{1}}[PS( \mathbf{s},\mathbf{q}_{2})]\).

Proper scoring rules have the following properties.

**Theorem 3**.: _[_31_]__A scoring rule \(PS\) is (strictly) proper if and only if there exists a (strictly) convex function \(G:\Delta_{S}\rightarrow\mathbb{R}\), such that for any \(\mathbf{q}\), \(G(\mathbf{q})=\bar{FS}(\mathbf{q};\mathbf{q})\), and \(PS(s,\mathbf{q})=G(\mathbf{q})+dG(\mathbf{q})(\delta_{s}-\mathbf{q})\), where \(dG\) is a subgradient of \(G\), and \(\delta_{s}\) is the distribution that putting probability 1 on \(s\)._

We use this property to prove the following Lemma.

**Lemma 3**.: _Given the assumptions, the following inequalities hold. (1) \(PS(h,\mathbf{q}_{h})>PS(h,\mathbf{q}_{\ell})\). (2) \(PS(\ell,\mathbf{q}_{\ell})>PS(\ell,\mathbf{q}_{h})\)._

Proof of Lemma 3.: Suppose \(G\) and \(dG\) are the convex function and the subgradient satisfying Theorem 3 regarding \(PS\). Then we have

\[PS(h,\mathbf{q}_{h}) =G(\mathbf{q}_{h})+dG(\mathbf{q}_{h})\cdot(\delta_{h}-\mathbf{q}_{ h}).\] \[PS(h,\mathbf{q}_{\ell}) =G(\mathbf{q}_{\ell})+dG(\mathbf{q}_{\ell})\cdot(\delta_{h}- \mathbf{q}_{\ell}).\]

\(\delta_{h}\) is the distribution on \(\mathcal{S}\) that putting probability 1 on \(h\).

Since \(dG\) is a subgradient of \(G\), it satisfies \(G(g)\geq G(x)+dG(x)\cdot(y-x)\) for any \(x,y\in\Delta_{S}\). Consequently, \((dG(g)-dG(x))\cdot(y-x)\geq 0\) for any \(x,y\in\Delta_{S}\). Then we have

\[PS(h,\mathbf{q}_{h})-PS(h,\mathbf{q}_{\ell}) =G(\mathbf{q}_{h})-G(\mathbf{q}_{\ell})+dG(\mathbf{q}_{h})\cdot( \delta_{h}-\mathbf{q}_{h})-dG(\mathbf{q}_{\ell})\cdot(\delta_{h}-\mathbf{q}_{ \ell})\] \[\geq dG(\mathbf{q}_{\ell})\cdot(\delta_{h}-\mathbf{q}_{\ell})+dG( \mathbf{q}_{h})\cdot(\delta_{h}-\mathbf{q}_{h})-dG(\mathbf{q}_{\ell})\cdot( \delta_{h}-\mathbf{q}_{\ell})\] \[=(dG(\mathbf{q}_{h})-dG(\mathbf{q}_{\ell}))\cdot(\delta_{h}- \mathbf{q}_{h}).\]

Note that \((\delta_{h}-\mathbf{q}_{h})(h)=1-q(h\mid h)=q(\ell\mid h)\), and \((\delta_{h}-\mathbf{q}_{h})(\ell)=-q(\ell\mid h)\). On the other hand, \((\mathbf{q}_{h}-\mathbf{q}_{\ell})(h)=-(\mathbf{q}_{h}-\mathbf{q}_{\ell})(\ell )=q(h\mid h)-q(h\mid\ell)>0\). Therefore,

\[PS(h,\mathbf{q}_{h})-PS(h,\mathbf{q}_{\ell}) =(dG(\mathbf{q}_{h})-dG(\mathbf{q}_{\ell}))\cdot(\delta_{h}- \mathbf{q}_{h})\] \[=\frac{q(\ell\mid h)}{q(h\mid h)-q(h\mid\ell)}\cdot(dG(\mathbf{q }_{h})-dG(\mathbf{q}_{\ell}))\cdot(\mathbf{q}_{h}-\mathbf{q}_{\ell})\] \[\geq 0,\]

With similar reasoning we have \(PS(\ell,\mathbf{q}_{\ell})-PS(\ell,\mathbf{q}_{h})\geq 0\). Then note that since \(PS\) is a strictly proper scoring rule,

\[\bar{FS}(\mathbf{q}_{h};\mathbf{q}_{h})-\bar{FS}(\mathbf{q}_{h}, \mathbf{q}_{\ell})\] \[=q(h\mid h)\cdot(PS(h,\mathbf{q}_{h})-PS(h,\mathbf{q}_{\ell}))+ q(\ell\mid h)\cdot(PS(\ell,\mathbf{q}_{h})-PS(\ell,\mathbf{q}_{\ell}))\] \[>0.\]

Therefore, \(PS(h,\mathbf{q}_{h})-PS(h,\mathbf{q}_{\ell})>0\). The proof of (2) follows a similar reasoning as (1). 

## Appendix B Equivalence with Bayesian Nash Equilibrium

In this section, we show that when \(k=1\), both ex-ante Bayesian \(k\)-strong equilibrium and Bayesian \(k\)-strong equilibrium are equivalent to the classical Bayesian Nash Equilibrium [30].

**Definition 4** (Bayesian Nash equilibrium).: A strategy profile \(\Sigma=(\sigma_{i})_{i\in[n]}\) is a Bayesian Nash equilibrium (BNE) if for every agent \(i\), every \(i\)'s strategy \(\sigma^{\prime}_{i}\), and every type \(s_{i}\in\mathcal{S}_{i}\), \(u_{i}(\Sigma\mid s_{i})\geq u_{i}((\sigma^{\prime}_{i},\Sigma_{-i})\mid s_{i})\), where \(\Sigma_{-i}\) is the strategies all other agents play in \(\Sigma\).

The following propositions shows the equivalence among solution concepts.

**Proposition 2**.: _If a strategy profile \(\Sigma\) is a Bayesian Nash equilibrium, then \(\Sigma\) is an ex-ante Bayesian \(1\)-strong equilibrium._

Proof.: Suppose \(\Sigma\) is a Bayesian Nash equilibrium, then for every agent \(i\), every \(i\)'s strategy \(\sigma^{\prime}_{i}\), and every type \(s_{i}\in\mathcal{S}_{i}\), \(u_{i}(\Sigma\mid s_{i})\geq u_{i}((\sigma^{\prime}_{i},\Sigma_{-i})\mid s_{i})\). Then from the law of total probability, adding up all the types in \(\mathcal{S}\), \(u_{i}(\Sigma)\geq u_{i}((\sigma^{\prime}_{i},\Sigma_{-i}))\). This implies that \(\Sigma\) is an ex-ante Bayesian \(1\)-strong equilibrium. 

**Proposition 3**.: _If a strategy profile \(\Sigma\) is a Bayesian \(1\)-strong equilibrium, then \(\Sigma\) is a Bayesian Nash equilibrium._

Proof.: Suppose \(\Sigma\) is NOT a Bayesian Nash equilibrium, and for agent \(i\), strategy \(\sigma^{\prime}_{i}\), and type \(s_{i}\). \(u_{i}(\Sigma\mid s_{i})<u_{i}((\sigma^{\prime}_{i},\Sigma_{-i})\mid s_{i})\). Now consider the strategy \(\sigma^{\prime\prime}_{i}\) such that for all \(s^{\prime}_{i}\in\mathcal{S}_{i}\) and \(s^{\prime}_{i}\neq s_{i}\), \(\sigma^{\prime\prime}_{i}(s^{\prime})=\sigma_{i}(s^{\prime})\), and \(\sigma^{\prime\prime}_{i}(s_{i})=\sigma^{\prime}_{i}(s)\). Then we have \(u_{i}(\Sigma\mid s_{i})<u_{i}((\sigma^{\prime\prime}_{i},\Sigma_{-i})\mid s_{i})\) and \(u_{i}(\Sigma\mid s_{i})=u_{i}((\sigma^{\prime}_{i},\Sigma_{-i})\mid s_{i})\) for all \(s^{\prime}_{i}\in\mathcal{S}_{i}\) and \(s^{\prime}_{i}\neq s_{i}\). This implies that \(\Sigma\) is a Bayesian \(1\)-strong equilibrium. 

Proposition 2, Proposition 3, and Proposition 1 when \(k=1\) form a cycle of equivalence.

## Appendix C Proof of Theorem 1

The proof consists of two steps. In Step 1, we characterize \(k_{E}\) by comparing the ex-ante expected utility of a deviator when every agent reports truthfully and when all \(k\) deviators always report \(h\) (and always report \(\ell\), respectively). At least one of the two deviations brings a deviator higher expected utility if and only if \(k>k_{E}\). In Step 2, we show that for any \(k\leq k_{B}\) and any deviating strategy profile \(\Sigma^{\prime}\), the average expected utility among all the deviators when \(\Sigma^{\prime}\) is played will not exceed the expected utility when every agent reports truthfully.

Therefore, there exists a deviator whose expected utility will decrease after the deviation, and the deviation cannot succeed.

**Step 1: characterizing \(k_{E}\).**

Consider a deviating group \(D\) of \(k\) agents. In the deviating strategy profile \(\Sigma\), all the deviators always report \(h\), i.e. \(\sigma=(1,1)\). (The reasoning for all deviators reporting \(\ell\) will be similar.) We fix an arbitrary deviator \(i\in D\) and characterize the condition of \(k\) such that \(u_{i}(\Sigma)>u_{i}(\Sigma^{*})\). Due to the symmetricity of the strategy profile, this implies that the expected utility of every deviator is higher in deviation than in truth-telling, and the deviation is successful.

To compare the expected utilities, we divide \(u_{i}(\Sigma)\). One part is the average expected utility from all other deviators \(j\in D\setminus i\), denoted by \(u_{i}(\Sigma\mid\text{deviator})\). The other part is the average expected utility from all truthful agents \(j\in[n]\setminus D\), denoted by \(u_{i}(\Sigma\mid\text{truthful})\).

\[u_{i}(\Sigma) =\frac{1}{n-1}\left(\sum_{j\in D\setminus i}\mathbb{E}[R_{i}( \alpha_{j})]+\sum_{j\in[n]\setminus D}\mathbb{E}[R_{i}(\alpha_{j})]\right) \tag{128}\] \[=\frac{k-1}{n-1}\cdot u_{i}(\Sigma\mid\text{deviator})+\frac{n-k} {n-1}\cdot u_{i}(\Sigma\mid\text{truthful}).\]

With the truthfulness of the peer prediction mechanism, \(u_{i}(\Sigma\mid\text{truthfully})\) is maximized when \(i\) reports truthfully, and \(i\) cannot increase his/her expected utility by deviation in this part. Therefore, agent \(i\) should gain a higher expected utility in the deviation part.

Let \(\Delta u_{d}=u_{i}(\Sigma^{*})-u_{i}(\Sigma\mid\text{deviator})\), and \(\Delta u_{t}=u_{i}(\Sigma^{*})-u_{i}(\Sigma\mid\text{truthful})\). Our goal is to find the condition on \(k\) such that

\[\frac{k-1}{n-1}\cdot\Delta u_{d}+\frac{n-k}{n-1}\cdot\Delta u_{t}<0.\]

Note that when in \(\Sigma\) all agents \(i\) have equal expected utility. Therefore, the inequality should be strict. When \(\Delta u_{t}>\Delta u_{d}\), this is equivalent to

\[k>\frac{\Delta u_{t}}{\Delta u_{t}-\Delta u_{d}}\cdot(n-1)+1.\]

And when \(\Delta u_{t}\leq\Delta u_{d}\), the condition does not hold for any \(k\), and the deviation will never succeed.

We first calculate the truthful expected utility. Note that when everyone plays the same strategy, the expected utility equals the expectation on \(R_{i}(\alpha_{j})\).

\[u_{i}(\Sigma^{*}) =q(h)\cdot(q(h\mid h)\cdot PS(h,\mathbf{q_{h}})+q(\ell\mid h) \cdot PS(\ell,\mathbf{q_{h}})) \tag{129}\] \[\quad+q(\ell)\cdot(q(h\mid h)\cdot PS(h,\mathbf{q_{f}})+q(\ell \mid h)\cdot PS(\ell,\mathbf{q_{f}})).\]

Then we calculate \(u_{i}(\Sigma\mid\text{truthful})\).

\[u_{i}(\Sigma\mid\text{truthful}) =q(h)\cdot(q(h\mid h)\cdot PS(h,\mathbf{q_{h}})+q(\ell\mid h) \cdot PS(\ell,\mathbf{q_{h}})) \tag{130}\] \[\quad+q(\ell)\cdot(q(h\mid h)\cdot PS(h,\mathbf{q_{h}})+q(\ell \mid h)\cdot PS(\ell,\mathbf{q_{h}})).\]

Therefore, the first part of the difference is

\[\Delta u_{t} =q(\ell)\cdot(q(h\mid\ell)\cdot(PS(h,\mathbf{q_{f}})-PS(h, \mathbf{q_{h}}))+q(\ell\mid\ell)\cdot(PS(\ell,\mathbf{q_{f}})-PS(\ell,\mathbf{ q_{h}}))) \tag{131}\] \[=q(\ell)\cdot\mathbb{E}_{\mathbf{s}-\mathbf{q_{f}}}[PS(\mathbf{s },\mathbf{q_{f}})-PS(\mathbf{s},\mathbf{q_{h}})]\]

From the property of the strict proper scoring rule, we know that \(\Delta u_{t}>0\).

And when \(j\) is a deviator always reporting, the utility of \(i\) will always be \(PS(h,\mathbf{q_{h}})\). Therefore, the second part of the difference is

\[\Delta u_{d} =q(\ell)\cdot(q(h\mid\ell)\cdot(PS(h,\mathbf{q_{f}})-PS(h, \mathbf{q_{h}}))+q(\ell\mid h)\cdot(PS(\ell,\mathbf{q_{f}})-PS(h,\mathbf{q_{h}} ))) \tag{132}\] \[\quad+q(h)\cdot q(\ell\mid h)\cdot(PS(\ell,\mathbf{q_{h}})-PS(h, \mathbf{q_{h}})).\]

And

\[\Delta u_{t}-\Delta u_{d} =q(\ell)\cdot q(\ell\mid\ell)(PS(h,\mathbf{q_{h}})-PS(\ell,\mathbf{ q_{h}}))-q(h)\cdot q(\ell\mid h)(PS(\ell,\mathbf{q_{h}})-PS(h,\mathbf{q_{h}})) \tag{133}\] \[=q(\ell)\cdot(PS(h,\mathbf{q_{h}})-PS(\ell,\mathbf{q_{h}})).\]

Therefore, when \(PS(h,\mathbf{q_{h}})-PS(\ell,\mathbf{q_{h}})>0\), the condition for the deviation to succeed is \(k>\frac{\mathbb{E}_{\mathbf{s}-\mathbf{q_{f}}}[PS(\mathbf{s},\mathbf{q_{f}})-PS( \mathbf{s},\mathbf{q_{f}})]}{PS(h,\mathbf{q_{h}})-PS(\ell,\mathbf{q_{h}})}\cdot( n-1)+1\). And when \(PS(h,\mathbf{q_{h}})-PS(\ell,\mathbf{q_{h}})\leq 0\), the deviation will never succeed. This is how \(k_{E}^{h}\) is defined.

Similarly, for deviation where all deviators always report \(\ell\), the condition for the deviation to succeed is \(k>\frac{\mathbb{E}_{\mathbf{s}-\mathbf{q_{f}}}[PS(\mathbf{s},\mathbf{q_{f}})-PS( \mathbf{s},\mathbf{q_{f}})]}{PS(\ell,\mathbf{q_{f}})-PS(h,\mathbf{q_{f}})}\cdot 1\) when \(PS(\ell,\mathbf{q_{f}})-PS(h,\mathbf{q_{f}})>0\), and the deviation can never succeed when \(PS(\ell,\mathbf{q_{f}})-PS(h,\mathbf{q_{f}})\leq 0\). This is how \(k_{E}^{h}\) is defined.

Also, note that by Lemma 3, at least one of \(PS(h,\mathbf{q_{h}})-PS(\ell,\mathbf{q_{h}})>0\) and \(PS(\ell,\mathbf{q_{f}})-PS(h,\mathbf{q_{f}})>0\) holds.

Therefore, for all \(k\leq k_{E}\), both deviations cannot succeed.

**Step 2: Equilibrium holds for \(k\leq k_{E}\).**

We fix an arbitrary \(2\leq k\leq k_{E}\). (For \(k=1\), the ex-ante Bayesian 1-strong equilibrium is equivalent to BNE, which is guaranteed by the truthfulness of the peer prediction mechanism.) Let \(\Sigma\) be the deviating strategy and \(\sigma_{1}=(\beta^{i}_{P},\beta^{i}_{B})\) be the strategy agent \(i\) plays in \(\Sigma\). \(u_{i}(\Sigma\mid\text{truthful})\) and \(u_{i}(\Sigma\mid\text{deviator})\) still denote the expected utility \(i\) gain from truthful agents and deviators, respectively. In this part, we consider the average on the expected utility of all agents \(i\in D\). Let \(\bar{\sigma}=(\tilde{\beta}_{\ell},\tilde{\beta}_{h})=\frac{1}{k}\sum_{i\in D} \sigma_{b}\) be the average strategy on all deviators.

For the truthful side, we have

\[u_{i}(\Sigma\mid\text{truthful}) =\frac{1}{n-k}\sum_{j\in[n]\setminus D}\mathbb{E}_{\mathbf{s}_{j }\sim\mathbf{q}_{i}\sim\sigma_{i}(\mathbf{s}_{i})}\mathbb{E}_{\mathbf{s}_{j} \sim\mathbf{q}_{i}}PS(s_{j},\mathbf{q}_{i})\] \[=\mathbb{E}_{\mathbf{s}_{i}\sim\mathbf{q}_{i}\sim\sigma_{i}( \mathbf{s}_{i})}\mathbb{E}_{\mathbf{s}_{j}\sim\mathbf{q}_{i}}PS(s_{j},\mathbf{ q}_{i})\]

Let

\[\bar{u}(\Sigma\mid\text{truthful}) =\frac{1}{k}\sum_{i\in D}u_{i}(\Sigma\mid\text{truthful})\] \[=\mathbb{E}_{\mathbf{s}\sim\mathbf{q},\mathbf{a}\sim\sigma( \mathbf{s})}\mathbb{E}_{\mathbf{s}_{j}\sim\mathbf{q}_{i}}PS(s_{j},\mathbf{q}_{ i}).\]

The equation comes from the fact that \(\beta^{j}_{h}\) and \(\beta^{j}_{\ell}\) are linear in the expected utility. Note that when everyone reports truthfully, everyone has equal expected utility, i.e. \(\bar{u}(\Sigma^{*})=u_{i}(\Sigma^{*})\). Then the difference between truthful reporting and deviation is a function of \(\tilde{\beta}_{\ell}\) and \(\tilde{\beta}_{h}\).

\[\Delta u_{\ell}(\tilde{\beta}_{\ell},\tilde{\beta}_{h})\] \[=\bar{u}(\Sigma^{*})-\bar{u}(\Sigma\mid\text{truthful})\] \[=q(h)\cdot(1-\tilde{\beta}_{h})\cdot(q(h\mid h)\cdot(PS(h, \mathbf{q}_{h})-PS(h,\mathbf{q}_{i}))+q(\ell\mid h)\cdot(PS(\ell,\mathbf{q}_{ h})-PS(\ell,\mathbf{q}_{i})))\] \[\quad+q(\ell)\cdot\tilde{\beta}_{\ell}\cdot(q(h\mid\ell)\cdot(PS (h,\mathbf{q}_{i})-PS(h,\mathbf{q}_{h}))+q(\ell\mid\ell)\cdot(PS(\ell,\mathbf{ q}_{i})-PS(\ell,\mathbf{q}_{h}))).\]

\(\Delta u_{\ell}(\tilde{\beta}_{\ell},\tilde{\beta}_{h})\geq 0\) and the equation holds when \(\tilde{\beta}_{h}=1\) and \(\tilde{\beta}_{\ell}=0\) is guaranteed by the property of the strict proper scoring rule.

For the deviator side, we have

\[u_{i}(\Sigma\mid\text{deviator})=\frac{1}{k-1}\sum_{j\in D\setminus\{i\}} \mathbb{E}_{\mathbf{s}_{j}\sim\mathbf{q}_{i}\sim\sigma_{i}(\mathbf{s}_{i})} \mathbb{E}_{\mathbf{s}_{j}\sim\mathbf{q}_{i}\sim\sigma_{j}(\mathbf{s}_{j})} PS(a_{j},\mathbf{q}_{a_{i}}).\]

And \(\bar{u}(\Sigma\mid\text{deviator})=\frac{1}{k}\sum_{i\in D}u_{i}(\Sigma\mid \text{deviator})\).

To better characterize \(\bar{u}(\Sigma\mid\text{deviator})\), we find an upper bound parameterized only by \(\bar{\sigma}\). Note that

\[u_{i}(\Sigma\mid\text{deviator}) =\frac{k}{k-1}\mathbb{E}_{\mathbf{s}_{j}\sim\mathbf{q}_{i}\sim \sigma_{j}(\mathbf{s}_{i})}\mathbb{E}_{\mathbf{s}_{j}\sim\mathbf{q}_{i}\sim \sigma_{j}(\mathbf{s})}PS(a,\mathbf{q}_{a_{i}})\] \[\quad-\frac{1}{k-1}\mathbb{E}_{\mathbf{s}_{j}\sim\mathbf{q}_{i} \sim\sigma_{i}(\mathbf{s}_{i})}\mathbb{E}_{\mathbf{s}_{j}\sim\mathbf{q}_{i} \sim\sigma_{j}(\mathbf{s})}PS(a,\mathbf{q}_{a_{i}}).\]

The first term can be viewed as adding an independent deviator that plays the same strategy \(\sigma_{i}\) as \(i\) into the deviator set \(D\). Then the average can be represented as

\[\bar{u}(\Sigma\mid\text{deviator}) =\frac{k}{k-1}\mathbb{E}_{\mathbf{s}^{\prime}\sim\mathbf{q}_{i} \sim\sigma(\mathbf{s}^{\prime})}\mathbb{E}_{\mathbf{s}_{j}\sim\mathbf{q}_{i} \sim\sigma_{j}(\mathbf{s})}PS(a,\mathbf{q}_{a^{\prime}})\] \[\quad-\frac{1}{(k-1)k}\sum_{i\in D}\mathbb{E}_{\mathbf{s}_{j} \sim\mathbf{q}_{i}\sim\sigma_{i}(\mathbf{s}_{i})}\mathbb{E}_{\mathbf{s}_{j}\sim \mathbf{q}_{i}\sim\sigma_{j}(\mathbf{s})}PS(a,\mathbf{q}_{a^{\prime}}).\]

Let \(f:[0,1]^{2}\rightarrow\mathbb{R}\). For a strategy \(\sigma=(\beta_{\ell},\beta_{h})\), let

\[f(\beta_{\ell},\beta_{h})=\mathbb{E}_{\mathbf{s}^{\prime}\sim\mathbf{q}_{i}^{ \prime}\sim\sigma(\mathbf{s}^{\prime})}\mathbb{E}_{\mathbf{s}\sim\mathbf{q}_{i} \sim\sigma(\mathbf{s})}PS(a,\mathbf{q}_{a^{\prime}}).\]

Then we can represent \(\bar{u}(\Sigma\mid\text{deviator})\) in the form of \(f\).

\[\bar{u}(\Sigma\mid\text{deviator})=\frac{k}{k-1}f(\beta_{\ell},\tilde{\beta}_ {h})-\frac{1}{(k-1)k}\sum_{i\in D}f(\beta^{i}_{\ell},\tilde{\beta}^{i}_{h}).\]

**Claim 1**.: \(f(\beta_{\ell},\beta_{h})\) _is convex on \([0,1]^{2}\)._Proof.: Note that

\[\frac{\partial^{2}f}{\partial\beta_{\ell}^{2}} =2q(\ell)\cdot q(\ell\mid\ell)\cdot(PS(h,\mathbf{q}_{h})+PS(\ell, \mathbf{q}_{\ell})-PS(h,\mathbf{q}_{\ell})-PS(\ell,\mathbf{q}_{h})), \tag{164}\] \[\frac{\partial^{2}f}{\partial\beta_{h}^{2}} =2q(h)\cdot q(h\mid h)\cdot(PS(h,\mathbf{q}_{h})+PS(\ell,\mathbf{ q}_{\ell})-PS(h,\mathbf{q}_{\ell})-PS(\ell,\mathbf{q}_{h})),\] (165) \[\frac{\partial^{2}f}{\partial\beta_{\ell}\partial\beta_{h}} =(q(\ell)\cdot q(h\mid\ell)+q(h)\cdot q(\ell\mid h))(PS(h,\mathbf{ q}_{h})+PS(\ell,\mathbf{q}_{\ell})-PS(h,\mathbf{q}_{\ell})-PS(\ell,\mathbf{q}_{h})),\] (166) \[\frac{\partial^{2}f}{\partial\beta_{h}\partial\beta_{\ell}} =(q(\ell)\cdot q(h\mid\ell)+q(h)\cdot q(\ell\mid h))(PS(h, \mathbf{q}_{h})+PS(\ell,\mathbf{q}_{\ell})-PS(h,\mathbf{q}_{\ell})-PS(\ell, \mathbf{q}_{h})). \tag{167}\]

Let

\[H=\begin{cases}2q(\ell)\cdot q(\ell\mid\ell)&q(\ell)\cdot q(h\mid\ell)+q(h) \cdot q(\ell\mid h)\\ q(\ell)\cdot q(h\mid\ell)+q(h)\cdot q(\ell\mid h)&2q(h)\cdot q(h\mid h)\end{cases}. \tag{168}\]

Then the Hermitian matrix of \(f\) is

\[H(f)=(PS(h,\mathbf{q}_{h})+PS(\ell,\mathbf{q}_{\ell})-PS(h,\mathbf{q}_{\ell})- PS(\ell,\mathbf{q}_{h}))\cdot H. \tag{169}\]

To show the convexity of \(f\), it is sufficient to show that \(H(f)\) is positive semi-definite. From Lemma 3 we know that \((PS(h,\mathbf{q}_{h})+PS(\ell,\mathbf{q}_{\ell})-PS(h,\mathbf{q}_{\ell})-PS( \ell,\mathbf{q}_{h}))>0\). Therefore, it is sufficient to show that \(H\) is positive semidefinite. We leverage the following lemma.

**Lemma 4**.: _[_44_, (7.6.12)]__A real symmetric matrix \(A\) is positive semidefinite if and only if all principal minors of \(A\) are non-negative._

Therefore, it is sufficient to show that all principal minors of \(H\) are non-negative.

First, \(|H_{1\times 1}|=2q(\ell)\cdot q(\ell\mid\ell)>0\), and \(|H_{2\times 2}|=2q(h)\cdot q(h\mid h)>0\). Note that \(q(\ell)\cdot q(h\mid\ell)=q(h)\cdot q(\ell\mid h)\). Therefore,

\[|H|=4q(h)\cdot q(\ell)\cdot(q(\ell\mid\ell)\cdot q(h\mid h)-q(h\mid\ell)\cdot q (\ell\mid h))>0. \tag{170}\]

Therefore, the Hessian matrix of \(f\) is positive semidefinite. Consequently, \(f\) is convex. 

By the convexity, \(\frac{1}{L}\sum_{i\in D}f(\beta_{\ell}^{i},\beta_{h}^{i})\geq f(\bar{\beta}_{ \ell},\bar{\beta}_{h})\). Therefore, \(\bar{u}(\Sigma\mid\text{deviator})\leq f(\bar{\beta}_{\ell},\bar{\beta}_{h})\), and the equality holds if all the agents \(i\in D\) plays the same strategy.

We use \(f(\bar{\beta}_{\ell},\bar{\beta}_{h})\) as an upper bound of \(\bar{u}(\Sigma\mid\text{deviator})\). Let \(\Delta u_{d}(\bar{\beta}_{\ell},\bar{\beta}_{h})=\bar{u}(\Sigma^{*})-\bar{u}( \Sigma\mid\text{deviator})\), and \(\Delta u_{d}^{\prime}(\bar{\beta}_{\ell},\bar{\beta}_{h})=\bar{u}(\Sigma^{*}) -f(\bar{\beta}_{\ell},\bar{\beta}_{h})\). Then \(\Delta u_{d}(\bar{\beta}_{\ell},\bar{\beta}_{h})\geq\Delta u_{d}^{\prime}(\bar {\beta}_{\ell},\bar{\beta}_{h})\) always holds.

Now we are ready to show that truthful reporting \(\Sigma^{*}\) is more profitable than any deviation \(\Sigma\) for \(k\leq k_{E}\).

Let \(\Delta u(\bar{\beta}_{\ell},\bar{\beta}_{h})=\frac{k-1}{k-1}\cdot\Delta u_{d}^{ \prime}(\bar{\beta}_{\ell},\bar{\beta}_{h})+\frac{n-k}{n-1}\cdot\Delta u_{d}( \bar{\beta}_{\ell},\bar{\beta}_{h})\). Then it's sufficient to show that \(\Delta u(\bar{\beta}_{\ell},\bar{\beta}_{h})\geq 0\) on any \((\bar{\beta}_{\ell},\bar{\beta}_{h})\in[0,1]^{2}\).

First, notice that \(\Delta u(\bar{\beta}_{\ell},\bar{\beta}_{h})\) is a concave function. This is because \(\Delta u_{d}(\bar{\beta}_{\ell},\bar{\beta}_{h})\) is linear on \(\bar{\beta}_{\ell}\) and \(\bar{\beta}_{h}\), and \(\Delta u_{d}^{\prime}(\bar{\beta}_{\ell},\bar{\beta}_{h})\) is a concave function according to Claim 1. Therefore, it is sufficient to show that \(\Delta u(\bar{\beta}_{\ell},\bar{\beta}_{h})\geq 0\) on any \((\bar{\beta}_{\ell},\bar{\beta}_{h})\in\{0,1\}^{2}\), i.e. the corner points. Note that \(\partial=(\bar{\beta}_{\ell},\bar{\beta}_{h})\) is the average strategy of all the deviators. \(\bar{\beta}_{\ell}\) (\(\bar{\beta}_{h}\), respectively) equals to \(0\) (or \(1\)) means that for all \(i\in D\), \(\beta_{\ell}^{i}\) (\(\bar{\beta}_{h}\), respectively) equals to \(0\) (or \(1\), respectively). Therefore, in the corner points, \(\Delta u_{d}^{\prime}=\Delta u_{d}\).

When \(\bar{\beta}_{\ell}=0\) and \(\bar{\beta}_{h}=1\), \(\Sigma=\Sigma^{*}\), and all the deviators also report truthfully. In this case, \(\Delta u_{\ell}=\Delta u_{d}^{\prime}=0\) since the two strategies are the same. Therefore, \(\Delta u(0,1)=0\).

When \(\bar{\beta}_{\ell}=\bar{\beta}_{h}=1\), all the deviators always report \(h\). In Step 1 we have shown that such deviation cannot succeed for any \(k\leq k_{E}\). Therefore, \(\Delta u(1,1)\geq 0\).

When \(\bar{\beta}_{\ell}=\bar{\beta}_{h}=0\), all the deviators always report \(\ell\). In Step 1 we have shown that such deviation cannot succeed for any \(k\leq k_{E}\).

Therefore, \(\Delta u(0,0)\geq 0\).

And when \(\bar{\beta}_{\ell}=1\) and \(\bar{\beta}_{h}=0\), all the deviators always tell a lie. We follow a similar reasoning as in step 1. First, we have

\[\Delta u_{\ell}(1,0) =q(h)\cdot(q(h\mid h)\cdot(PS(h,\mathbf{q}_{h})-PS(h,\mathbf{q}_{ \ell}))+q(\ell\mid h)\cdot(PS(\ell,\mathbf{q}_{h})-PS(\ell,\mathbf{q}_{\ell}))) \tag{169}\] \[+q(\ell)\cdot(q(h\mid\ell)\cdot(PS(h,\mathbf{q}_{\ell})-PS(h, \mathbf{q}_{h}))+q(\ell\mid\ell)\cdot(PS(h,\mathbf{q}_{\ell})-PS(\ell, \mathbf{q}_{h})))\] \[= q(h)\cdot\mathbb{E}_{\mathbf{s}-\mathbf{q}_{h}}[PS(s,\mathbf{q} _{h})-PS(s,\mathbf{q}_{\ell})]+q(\ell)\cdot\mathbb{E}_{\mathbf{s}-\mathbf{q}_{ \ell}}[PS(s,\mathbf{q}_{\ell})-PS(s,\mathbf{q}_{h})]. \tag{160}\]

\(\Delta u_{\ell}(1,0)\geq 0\) is guaranteed by the property of the proper scoring rule.

And

\[\Delta u_{\ell}(1,0)-\Delta u_{d}^{\prime}(1,0) \tag{161}\] \[=q(h)\cdot(q(h\mid h)\cdot(PS(\ell,\mathbf{q}_{\ell})-PS(h, \mathbf{q}_{\ell}))+q(\ell\mid h)\cdot(PS(h,\mathbf{q}_{\ell})-PS(\ell, \mathbf{q}_{\ell})))\] (162) \[+q(\ell)\cdot(q(h\mid\ell)\cdot(PS(t,\mathbf{q}_{h})-PS(h,\mathbf{q }_{h}))+q(\ell\mid\ell)\cdot(PS(h,\mathbf{q}_{h})-PS(\ell,\mathbf{q}_{h})))\] (163) \[=q(h)\cdot(q(h\mid h)-q(\ell\mid h))\cdot(PS(t,\mathbf{q}_{\ell})-PS(h, \mathbf{q}_{\ell}))\] (164) \[+q(\ell)\cdot(q(\ell\mid\ell)-q(h\mid\ell))\cdot(PS(h,\mathbf{q}_{h})-PS( t,\mathbf{q}_{h}))). \tag{165}\]If \(\Delta u_{t}(1,0)-\Delta u_{d}^{\prime}(1,0)\leq 0\), \(\Delta u(1,0)\geq 0\) for every \(k\). If \(\Delta u_{t}(1,0)-\Delta u_{d}^{\prime}(1,0)>0\), \(\Delta u(1,0)<0\) if and only if

\[k>\frac{q(h):\mathbb{E}_{u_{k}}[PS(s,\mathbf{q}_{h})-PS(s,\mathbf{q}_{h})]+q( t):\mathbb{E}_{u_{k}}[PS(s,\mathbf{q}_{h})-PS(s,\mathbf{q}_{h})]}{q(h):(PS(s, \mathbf{q}_{h})-PS(s,\mathbf{q}_{h}))+q(t):(q(t)\cdot q(t)\cdot q(h)\cdot(PS(s, \mathbf{q}_{h})-PS(s,\mathbf{q}_{h}))+1}.\]

Denote this threshold as \(k^{\prime}\). We claim that \(k^{\prime}\geq k_{E}\), and therefore \(\Delta u(1,0)\geq 0\) for all \(k\leq k_{E}\). We consider the following three cases.

1. \(PS(\ell,\mathbf{q}_{\ell})-PS(h,\mathbf{q}_{\ell})>0\) and \(PS(h,\mathbf{q}_{h})-PS(\ell,\mathbf{q}_{h})>0\). In this case, \(k^{\prime}\) is between \(k_{E}^{th}\) and \(k_{E^{\prime}}^{\prime}\), and cannot be smaller than \(k_{E}\).
2. \(PS(\ell,\mathbf{q}_{\ell})-PS(h,\mathbf{q}_{\ell})\leq 0\) but \(PS(h,\mathbf{q}_{h})-PS(\ell,\mathbf{q}_{h})>0\). If \(q(h\mid h)\geq q(t\mid h)\), the first term in the denominator is non-positive, and it's not hard to verify that \(k^{\prime}>k_{E}\). If \(q(h\mid h)<q(t\mid h)\), note that we have \(PS(h,\mathbf{q}_{h})>PS(h,\mathbf{q}_{\ell})\geq PS(\ell,\mathbf{q}_{\ell})>PS( \ell,\mathbf{q}_{h})\) according to Lemma 3. Therefore, the denominator is non more than \(q(\ell(t)\cdot q(t\mid t)-q(h\mid h))\cdot PS(h,\mathbf{q}_{h})-PS(\ell,\mathbf{ q}_{h})\), which is smaller than \(q(t)\) times the denominator in the \(k_{E}^{th}\). On the other hand, the nominator in \(k^{\prime}\) is larger than \(q(t)\) times the nominator in \(k_{E}\). Therefore, \(k^{\prime}\geq k_{E}\).
3. \(PS(\ell,\mathbf{q}_{\ell})-PS(h,\mathbf{q}_{\ell})>0\) but \(PS(h,\mathbf{q}_{h})-PS(\ell,\mathbf{q}_{h})\leq 0\). This case follows the second case due to symmetricity.

Therefore, \(\Delta u(1,0)\geq 0\) for all \(k\leq k_{E}\).

Therefore, we prove that for any \(k\leq k_{E}\) and any strategy profile, the average (ex-ante) expected utility of the deviators will not exceed the expected utility when all the agents report truthfully. Therefore, one of the following cases occurs.

1. There exists an agent \(i\) such that \(i^{\prime}\)s expected utility in deviation is strictly lower than in \(\Sigma^{*}\). Therefore, the second condition of the deviating group is violated.
2. All the agents have exactly the same expected utility in deviation and \(\Sigma^{*}\). The third condition of a deviating group to have an agent strictly better off is violated.

Therefore, any deviation cannot succeed, and truth-telling \(\Sigma^{*}\) is an ex-ante Bayesian \(k_{E}\)-strong equilibrium.

## Appendix D Proof of Theorem 2

The steps of the proof resemble the steps of the proof of Theorem 1, yet the techniques are different. In Step 1, we determine \(k_{B}\) by comparing the interim expected utility of a deviator conditioned on his/her signal being \(h\) and \(\ell\) respectively when every agent reports truthfully and when all \(k\) deviators always report \(h\) (and always report \(\ell\), respectively). In Step 2, we show that for any \(k\leq k_{E}\) and any deviating strategy profile \(\tilde{\Sigma}\) where all the deviators play the same strategy \(\tilde{\sigma}\), the average expected utility among all the deviators when \(\tilde{\Sigma}\) is played will not exceed the expected utility when every agent reports truthfully. In Step 3, we show that for sufficiently large \(n\), any \(k\leq k_{B}\), and any deviating strategy profile \(\Sigma\), there exists a deviator whose expected utility is strictly smaller than the expected utility when every agent reports truthfully.

Let \(\overline{\Delta PS}\) be the largest difference in the positional scoring rule. Let \(\Delta h=PS(h,\mathbf{q}_{h})-PS(h,\mathbf{q}_{\ell})\), and \(\Delta\ell=PS(\ell,\mathbf{q}_{\ell})-q(\ell,\mathbf{q}_{h})\). From Lemma 3, we have \(\Delta h>0\) and \(\Delta\ell>0\). We first explicitly give the lower bound of \(n\):

1. \(\hat{h}_{h}=\frac{\overline{\Delta PS}:(\Delta h\wedge\Delta\ell+PS(\ell, \mathbf{q}_{\ell})-PS(h,\mathbf{q}_{\ell}))}{(n-1):(\Delta h\wedge\Delta\ell): \mathbb{E}_{u_{k}}[PS(s,\mathbf{q}_{h})-PS(s,\mathbf{q}_{\ell})]}<\frac{1}{2}\).
2. If \(PS(h,\mathbf{q}_{h})>PS(\ell,\mathbf{q}_{\ell})\), then \(\hat{h}_{h}\leq\frac{\overline{\Delta}(h,\mathbf{q}_{h})-PS(\ell,\mathbf{q}_{h})}{ \Delta h+\Delta\ell}\),
3. \(\hat{h}_{h}\leq\frac{\mathbb{E}_{u_{k}}[PS(s,\mathbf{q}_{h})-PS(s,\mathbf{q}_{ \ell})]}{(n-1):(\Delta h\wedge\Delta\ell)}\),
4. \(\hat{h}_{l}=\frac{\overline{\Delta PS}:(\Delta h\wedge\Delta\ell+PS(h, \mathbf{q}_{h}))-PS(\ell,\mathbf{q}_{h})]}{(n-1):(\Delta h\wedge\Delta\ell): \mathbb{E}_{u_{l}}[PS(s,\mathbf{q}_{\ell})-PS(s,\mathbf{q}_{\ell})]}<\frac{1}{4}\).
5. If \(PS(\ell,\mathbf{q}_{\ell})>PS(\ell,\mathbf{q}_{\ell})\), then \(\hat{h}_{h}\leq\frac{PS(s,\mathbf{q}_{\ell})-PS(h,\mathbf{q}_{\ell})}{ \Delta h+\Delta\ell}\),
6. \(\hat{h}_{h}\leq\frac{\mathbb{E}_{u_{k}}[PS(s,\mathbf{q}_{\ell})-PS(s,\mathbf{q}_ {\ell})]}{q(\ell(\ell):\Delta h\wedge\Delta\ell)}\).

**Step 1: characterizing \(k_{B}\).**

Consider a deviating group \(D\) of \(k\) agents. In the deviating strategy profile \(\Sigma\), all the deviators always report \(h\), i.e. \(\sigma=(1,1)\). We fix an arbitrary deviator \(i\in D\). In the interim setting, the condition for the deviation is successful is \(u_{i}(\Sigma\mid h)\geq u_{i}(\Sigma^{*}\mid h)\) and \(u_{i}(\Sigma\mid\ell)\geq u_{i}(\Sigma^{*}\mid\ell)\) hold, and at least one of the inequality is strict.

Similar to the ex-ante's proof, let \(u_{i}(\Sigma\mid s_{i},\text{deviator})\)\(u_{i}(\Sigma\mid s_{i},\text{truthful})\) be the average expected utility from all other deviators (truthful agents, respectively) conditioned on \(i\)'s signal being \(s_{i}\). And let \(\Delta u_{|s_{ii}}=u_{i}(\Sigma^{*}\mid s_{i})-u_{i}(\Sigma\mid s_{i},\text{deviator})\) and \(\Delta u_{t|s_{ii}}=u_{i}(\Sigma^{*}\mid s_{i})-u_{i}(\Sigma\mid s_{i},\text{ truthful})\).

For expected utility on \(\Sigma^{*}\), we have

\[u_{i}(\Sigma^{*}\mid h)=q(h\mid h)\cdot PS(h,\mathbf{q}_{h})+q( \ell\mid h)\cdot PS(\ell,\mathbf{q}_{h}),\] \[u_{i}(\Sigma^{*}\mid\ell)=q(h\mid\ell)\cdot PS(h,\mathbf{q}_{\ell} )+q(\ell\mid\ell)\cdot PS(\ell,\mathbf{q}_{\ell}).\]

And for expected utility of \(\Sigma\), we have

\[u_{i}(\Sigma\mid h,\text{truthful})=q(h\mid h)\cdot PS(h,\mathbf{q}_{h})+q( \ell\mid h)\cdot PS(\ell,\mathbf{q}_{h})\] \[u_{i}(\Sigma\mid\ell,\text{truthful})=q(h\mid\ell)\cdot PS(h, \mathbf{q}_{h})+q(\ell\mid\ell)\cdot PS(\ell,\mathbf{q}_{h}).\]

Therefore, \(u_{i}(\Sigma^{*}\mid h)=u_{i}(\Sigma\mid h,\text{truthful})\), and \(\Delta u_{t|h}=0\). Also for the \(\ell\) side, \(\Delta u_{t|\ell}=\mathbb{E}_{u_{k}-\mathbf{q}_{\ell}}[PS(s,\mathbf{q}_{\ell})-PS(s, \mathbf{q}_{h})]>0\).

[MISSING_PAGE_FAIL:15]

_._
3. _Let_ \(\alpha_{\ell}=\frac{q(\ell|\ell)}{q(h|\ell)}\)_, and let_ \(b_{\ell}\geq 0\) _be a constant. When fixing_ \(\beta_{h}=(b_{\ell}-\alpha_{\ell}\cdot\beta_{\ell})\)_, then_ 1799__ _(1800)_ \[\frac{\partial q^{\ell}(\beta_{r},(b_{\ell}-\alpha_{\ell}\cdot\beta_{ \ell}))}{\partial\beta_{\ell}}=\frac{\partial f^{\ell}(\beta_{r}^{\prime}(\beta_ {r},(b_{\ell}-\alpha_{\ell}\cdot\beta_{\ell})))}{\partial\beta_{\ell}^{\ell}}\] \[= b_{\ell}\cdot q(h\mid\ell)(PS(h,\mathbf{q}_{h})-PS(h,\mathbf{q}_{ \ell}))+(1-b_{\ell}\cdot q(h\mid\ell))(PS(\ell,\mathbf{q}_{h})-PS(\ell,\mathbf{ q}_{\ell})).\] _Specifically, when_ \(b_{\ell}=1\)_,_ \(\frac{\partial f^{\ell}(\beta_{r},(b_{\ell}-\alpha_{\ell}\cdot\beta_{\ell}))}{ \partial\beta_{\ell}^{\ell}}=\mathbb{E}_{\mathbf{x}\smallsetminus\mathbf{q}_{ \ell}}[PS(\mathbf{s},\mathbf{q}_{h})-PS(\mathbf{s},\mathbf{q}_{\ell})]<0\)_._

Now we start to characterize the deviation. We fix an arbitrary \(k\). Let \(\hat{\sigma}=(\hat{\beta}_{\ell},\hat{\beta}_{h})\) be the strategy on all deviators. Since all the deviators play the same strategy, they receive the same expected utility conditioned on the same signal.

The expected utility of deviator with private signal \(h\) conditioned on his/her peer \(j\) is a truthful agent is \(u(\hat{\beta}_{r},\hat{\beta}_{h}\mid h,\text{truthful})=f^{h}(\hat{\beta}_{h},(0,1))\), and that conditioned on \(j\) is also a deviator is \(u(\hat{\beta}_{r},\hat{\beta}_{h}\mid h,\text{tev})=f^{h}(\hat{\beta}_{h},( \hat{\beta}_{\ell},\hat{\beta}_{h}))=g^{h}(\hat{\beta}_{r},\hat{\beta}_{h})\). Similarly, for the \(\ell\) side we have \(u(\hat{\beta}_{\ell},\hat{\beta}_{h}\mid\ell,\text{truthful})=f^{\ell}(\hat{ \beta}_{r},(0,1))\) and \(u(\hat{\beta}_{r},\hat{\beta}_{h}\mid\ell,\text{tev})=f^{\ell}(\hat{\beta}_{r},(\hat{\beta}_{h}))=g^{l}(\hat{\beta}_{r},\hat{\beta}_{h})\). Therefore, the expected reward of deviation can be represented by the following function.

\[u(\hat{\beta}_{\ell},\hat{\beta}_{h}\mid h) =\frac{n-k}{n-1}\cdot u(\hat{\beta}_{\ell},\hat{\beta}_{h}\mid h, \text{truthful})+\frac{k-1}{n-1}\cdot u(\hat{\beta}_{\ell},\hat{\beta}_{h} \mid h,\text{deviator}),\] \[u(\hat{\beta}_{\ell},\hat{\beta}_{h}\mid\ell) =\frac{n-k}{n-1}\cdot u(\hat{\beta}_{\ell},\hat{\beta}_{h}\mid\ell,\text{truthful})+\frac{k-1}{n-1}\cdot u(\hat{\beta}_{r},\hat{\beta}_{h}\mid \ell,\text{deviator}).\]

Now we show that for any \(\hat{\sigma}=(\hat{\beta}_{r},\hat{\beta}_{h})\in[0,1]^{2}\), either \(u(\hat{\beta}_{r},\hat{\beta}_{h}\mid h)\leq u(\Sigma^{*}\mid h)\) or \(u(\hat{\beta}_{r},\hat{\beta}_{h}\mid\ell)\leq u(\Sigma^{*}\mid\ell)\).

**Lemma 1**.: _For any \((\hat{\beta}_{\ell},\hat{\beta}_{h}\mid h)\in\mathbb{R}^{2}\) satisfying (1) \(\hat{\beta}_{\ell}\geq 0\), (2) \(\hat{\beta}_{h}\geq 0\), and (3) \(\hat{\beta}_{h}+\frac{q(\ell|h)}{q(h|h)}\cdot\hat{\beta}_{\ell}\leq 1\), it always holds that \(u(\hat{\beta}_{r},\hat{\beta}_{h}\mid h)\leq u(\Sigma^{*}\mid h)\), and the equality holds only when \(\hat{\beta}_{\ell}=0\) and \(\hat{\beta}_{h}=1\)._

Proof of Lemma 1.: The proof proceeds in three steps.

First, we show that \(u(0,\beta_{h}\mid h)\leq u(\Sigma^{*}\mid h)\) for any \(\hat{\beta}_{h}\). This holds for the following three reasons. First, \(u(0,1\mid h)=u(\Sigma^{*}\mid h)\), as in this case all the deviators report truthfully and no deviation happens. Second, \(u(0,0\mid h)<u(\Sigma^{*}\mid h)\) is guaranteed by \(k\leq k_{B}\). Finally, since \(u(\hat{\beta}_{\ell},\hat{\beta}_{h}\mid h,\text{truthful})\) is linear and \(u(\hat{\beta}_{\ell},\hat{\beta}_{h}\mid h,\text{tev})\) is strictly convex on \(\hat{\beta}_{h}\), \(u(0,\hat{\beta}_{h}\mid h)\) is also convex on \(\hat{\beta}_{h}\). The convexity bound the expected utility for every \(0<\hat{\beta}_{h}<1\).

Second, we show that \(u(\hat{\beta}_{\ell},\hat{\beta}_{h}\mid h)\leq u(\Sigma^{*}\mid h)\) when \(\hat{\beta}_{\ell}=\frac{q(h|h)}{q(\ell|h)}(1-\hat{\beta}_{h})\) for any \(\hat{\beta}_{h}\in[0,1]\). This is because the derivative

\[\frac{\partial u(\frac{q(h|h)}{q(\ell|h)}(1-\hat{\beta}_{h}),\hat {\beta}_{h}\mid h)}{\partial\hat{\beta}_{h}} =\frac{n-k}{n-1}\cdot\frac{\partial f^{h}(\hat{\beta}_{h},(0,1))} {\partial\hat{\beta}_{h}}+\frac{k-1}{n-1}\cdot\frac{\partial g^{h}(\frac{q(h|h)} {q(\ell|h)}(1-\hat{\beta}_{h}),\hat{\beta}_{h})}{\partial\hat{\beta}_{h}}\] \[=\mathbb{E}_{\mathbf{x}\smallsetminus\mathbf{q}_{h}}[PS( \mathbf{s},\mathbf{q}_{h})-PS(\mathbf{s},\mathbf{q}_{\ell})]\] \[>0.\]

Therefore, for any \((\hat{\beta}_{\ell},\hat{\beta}_{h})\) with \(\hat{\beta}_{h}<1\) and \(\hat{\beta}_{\ell}=\frac{q(h|h)}{q(\ell|h)}(1-\hat{\beta}_{h})\), \(u(\hat{\beta}_{\ell},\hat{\beta}_{h}\mid h)<u(0,1\mid h)=u(\Sigma^{*}\mid h)\).

Finally, we extend the result to any \((\hat{\beta}_{\ell},\hat{\beta}_{h})\) in the area. For any \(\hat{\beta}_{h}\in[0,1)\), we have shown that \(u(0,\hat{\beta}_{h}\mid h)\leq u(\Sigma^{*}\mid h)\) and \(u(\frac{q(h|h)}{q(\ell|h)}(1-\hat{\beta}_{h}),\hat{\beta}_{h}\mid h)<u(\Sigma^{*} \mid h)\). Then by the linearity of \(u(\hat{\beta}_{\ell},\hat{\beta}_{h}\mid h)\) on \(\hat{\beta}_{\ell}\), \(u(\hat{\beta}_{\ell},\hat{\beta}_{h}\mid h)<u(\Sigma^{*}\mid h)\) for any \(\hat{\beta}_{\ell}\in(0,\frac{q(h|h)}{q(\ell|h)}(1-\hat{\beta}_{h}))\), which finishes the proof. 

Similarly, for \(\ell\) side, we have

**Lemma 2**.: _For any \((\hat{\beta}_{\ell},\hat{\beta}_{h})\in\mathbb{R}^{2}\) satisfying (1) \(\hat{\beta}_{\ell}\leq 1\), (2) \(\hat{\beta}_{h}\leq 1\), and (3) \(\hat{\beta}_{h}+\frac{q(\ell|\ell)}{q(h|h)}\cdot\hat{\beta}_{\ell}\geq 1\), it always holds that \(u(\hat{\beta}_{r},\hat{\beta}_{h}\mid\ell)\leq u(\Sigma^{*}\mid\ell)\), and the equality holds only when \(\hat{\beta}_{\ell}=0\) and \(\hat{\beta}_{h}=1\)._

Proof of Lemma 2.: The proof follows the proof of Lemma 1. First, \(u(\hat{\beta}_{\ell},1\mid\ell)<u(\Sigma^{*}\mid\ell)\) for any \(\hat{\beta}_{\ell}\in[0,1]\) due to the convexity of \(u(\hat{\beta}_{\ell},\hat{\beta}_{h}\mid\ell)\) on \(\hat{\beta}_{\ell}\). Secondly, \(u(\hat{\beta}_{\ell},\hat{\beta}_{h}\mid\ell)<u(\Sigma^{*}\mid\ell)\) when \(\hat{\beta}_{h}+\frac{q(\ell|\ell)}{q(h|\ell)}

Note that for any pair of \((\beta_{\ell},\beta_{h})\in[0,1]^{2}\), at least one of \(\beta_{h}+\frac{q(\ell|h)}{q(h|h)}\cdot\beta_{\ell}\leq 1\) and \(\beta_{h}+\frac{q(\ell|\ell)}{q(h|h)}\cdot\beta_{\ell}\geq 1\) holds. This comes from \(q(h\mid h)>q(h\mid\ell)\) and \(q(\ell\mid\ell)>q(\ell\mid h)\). Therefore, the two triangle areas cover the whole \([0,1]^{2}\), and we can apply either Lemma 1 or 2 to show that the deviation cannot succeed for any \(\Sigma\).

**Step 3: General deviation cannot succeed for \(k\leq k_{B}\).**

Let \(\overline{\Delta PS}\) be the largest difference in the positional scoring rule. Let \(\Delta h=PS(h,\mathbf{q}_{h})-PS(h,\mathbf{q}_{\ell})\), and \(\Delta\ell=PS(\ell,\mathbf{q}_{\ell})-q(\ell,\mathbf{q}_{h})\). From Lemma 3, we have \(\Delta h>0\) and \(\Delta\ell>0\). Then we explicitly give the lower bound of \(n\):

1. \(\beta_{h}=\frac{4\overline{\Delta PS}\cdot(\Delta h+\Delta\ell PS(h,\mathbf{q }_{\ell})-PS(h,\mathbf{q}_{\ell}))}{(n-1)\cdot(\Delta h+\Delta\ell)\cdot\Sigma _{n-\Delta\ell}PS((s,\mathbf{q}_{h})-q(s,\mathbf{q}_{\ell}))}<\frac{1}{4}\),
2. If \(PS(h,\mathbf{q}_{h})>PS(\ell,\mathbf{q}_{h})\), then \(\beta_{h}\leq\frac{PS(h,\mathbf{q}_{h})-PS(\ell,\mathbf{q}_{h})}{4h\times \Delta\ell}\),
3. \(\beta_{h}\leq\frac{8\cdot\mathbf{q}_{h}-PS(s,\mathbf{q}_{h})-PS(s,\mathbf{q}_{ h}))}{(4h\times \Delta\ell)\cdot\Sigma_{n-\Delta\ell}PS(h,\mathbf{q}_{\ell})-q(s,\mathbf{q}_{h}) }<\frac{1}{4}\),
4. If \(PS(\ell,\mathbf{q}_{\ell})>PS(h,\mathbf{q}_{\ell})\), then \(\beta_{h}\leq\frac{PS(\ell,\mathbf{q}_{\ell})-PS(h,\mathbf{q}_{\ell})}{4h\times \Delta\ell}\),
5. \(\beta_{h}\leq\frac{8\cdot\mathbf{q}_{h}-PS(\ell,\mathbf{q}_{h})-4(s,\mathbf{q}_ {h})}{q(\ell/(\ell)\cdot(\Delta h+\Delta\ell)\cdot\Sigma_{n-\Delta\ell})}\).

Step 3 proceeds as follows. First, when deviators play asymmetrically, we compare the worst expected utility among the agent with the expected utility when all deviators play their 'average' strategy \(\hat{\sigma}=\sum_{i\in D}\frac{1}{k}\sigma_{i}\). We show that the reward of the worst agent cannot be better than the reward of the average strategy by \(O(\frac{\overline{\Delta PS}}{n-1})\). Then we show that the reward of the average strategy is no less than the truthful reward by \(\Theta(\frac{\overline{\Delta PS}}{n-1})\) only when \(\hat{\sigma}\) is close to \((0,1)\) or \((0,0)\). Finally, we deal with the corner cases and show that in this case, the worst agent cannot be better than the truthful reward. We will give the proof on the \(h\) side (or specifically, conditioned on an agent having private signal \(h\)). The \(\ell\) side follows similar reasoning.

Now let \(\hat{\sigma}=(\beta_{\ell},\beta_{h})\) be the _average strategy_ of all the deviators, i.e. \(\hat{\sigma}=\frac{1}{k}\sum_{j\in D}\sigma_{j}\). And \(\sigma_{i}=(\beta_{\ell}^{i},\beta_{h}^{i})\) be the strategy of a deviator \(i\in D\). We represent the expected utility of \(i\) with \(f^{h},f^{t},g^{h},\) and \(g^{t}\).

The expected utility of \(i\) conditioned on his/her peer \(j\) is a truthful agent is \(u(\beta_{\ell}^{i},\beta_{h}^{i}\mid h,\text{truthful})=f^{h}(\beta_{\ell}^{i},(0,1))\). and that conditioned on \(j\) is also a deviator is

\[u(\beta_{\ell}^{i},\beta_{h}^{i}\mid h,\text{deviator})=\frac{1}{k-1}\sum_{j \in D,j\neq i}f(\beta_{h}^{i},(\beta_{\ell}^{j},\beta_{h}^{j})).\]

An important observation is that for a fixed \(\beta_{h}^{t},f^{h}(\beta_{h}^{t},(\beta_{\ell},\beta_{h}))\) is linear on \(\beta_{\ell}\) and \(\beta_{h}\). Therefore,

\[u(\beta_{\ell}^{i},\beta_{h}^{i}\mid h,\text{deviator}) =\frac{k}{k-1}f(\beta_{h}^{i},(\beta_{\ell},\beta_{h}^{i}))-\frac {1}{k-1}f(\beta_{h}^{i},(\beta_{\ell}^{i},\beta_{h}^{i})) \tag{198}\] \[=f(\beta_{h}^{i},(\beta_{\ell},\beta_{h}^{i}))+\frac{1}{k-1}(f( \beta_{h}^{i},(\beta_{\ell},\beta_{h}^{i}))-f(\beta_{h}^{i},(\beta_{\ell}^{i}, \beta_{h}^{i}))).\]

Adding two parts together, we have

\[u(\beta_{\ell}^{i},\beta_{h}^{i}\mid h)=\frac{n-k}{n-1}\cdot u(\beta_{\ell}^{i },\beta_{h}^{i}\mid h,\text{truthful})+\frac{k-1}{n-1}\cdot u(\beta_{\ell}^{i}, \beta_{h}^{i}\mid h,\text{deviator}).\]

Then we consider the difference of agent \(i\)'s utility between when all the deviators play the average strategy \(\hat{\sigma}\) and when the deviators play differently with \(i\) playing \(\sigma_{i}\).

\[u(\beta_{\ell}^{i},\beta_{h}^{i}\mid h)-u(\beta_{\ell},\beta_{h} \mid h)= \frac{n-k}{n-1}\cdot(f^{h}(\beta_{h}^{i},(0,1))-f^{h}(\beta_{h},(0,1))) \tag{199}\] \[+\frac{k-1}{n-1}(f(\beta_{h}^{i},(\beta_{\ell},\beta_{h}))-f(\beta _{h},(\beta_{\ell},\beta_{h})))\] (190) \[+\frac{k-1}{n-1}\cdot\frac{1}{k-1}(f(\beta_{h}^{i},(\beta_{\ell}, \beta_{h}))-f(\beta_{h}^{i},(\beta_{\ell}^{i},\beta_{h}^{i})))\] (191) \[= \frac{n-k}{n-1}\cdot f^{h}(\beta_{h}^{i}-\beta_{h},(0,1))+\frac{k -1}{n-1}f(\beta_{h}^{i}-\beta_{h},(\beta_{\ell},\beta_{h}))\] (192) \[+\frac{1}{n-1}(f(\beta_{h}^{i},(\beta_{\ell},\beta_{h}))-f(\beta _{h}^{i},(\beta_{\ell}^{i},\beta_{h}^{i}))).\]

The second equality comes from the fact that \(f^{h}(\beta_{h}^{t},(\beta_{\ell},\beta_{h}))\) is linear on \(\beta_{h}^{t}\) for any fixed \((\beta_{\ell},\beta_{h})\). Given a fixed \(\hat{\sigma}=(\beta_{\ell},\beta_{h})\), the term

\[\frac{n-k}{n-1}\cdot f^{h}(\beta_{h}^{t}-\beta_{h},(0,1))+\frac{k-1}{n-1}f(\beta _{h}^{t}-\beta_{h},(\beta_{\ell},\beta_{h})) \tag{193}\]equals to \(0\) when \(\beta_{h}^{l}=\beta_{h}\) and is linear on \(\beta_{h}^{l}\). Therefore, in at least on of \(\beta_{h}^{l}\leq\beta_{h}\) or \(\beta_{l}^{l}\leq\beta_{l}\), the term will be no larger than zero.

On the other hand, the third term \(\frac{1}{n-1}(f(\beta_{h}^{l},(\beta_{l},\beta_{h}))-f(\beta_{h}^{l},(\beta_{l} ^{l},\beta_{h}^{l})))\leq\frac{\overline{\Delta PS}}{n-1}\). Therefore, there exists a deviator \(i\) such that \(u(\beta_{F}^{l}\beta_{h}^{l}\mid\beta_{h}^{l})=u(\tilde{\beta}_{r},\tilde{\beta }_{h}\mid h)\leq\frac{\overline{\Delta PS}}{n-1}\).

Then we show that for sufficiently large \(n\), for all \((\tilde{\beta}_{k},\tilde{\beta}_{h})\) not close to \((0,1)\) or \((0,0)\), \(u(\Sigma^{*}\mid h)-u(\beta_{k},\tilde{\beta}_{h}\mid h)>\frac{\overline{ \Delta PS}}{n-1}\).

**Lemma 5**.: _Let \(\Delta u(\tilde{\beta}_{r},\tilde{\beta}_{h}\mid h)=u(\Sigma^{*}\mid h)-u( \tilde{\beta}_{r},\tilde{\beta}_{h}\mid h)\). Then for any \(\tilde{\beta}_{h}\in[0,1]\) and \(\tilde{\beta}_{r}\in[0,\frac{q(h\mid h)}{q(\tilde{r}\mid h)}\cdot(1-\tilde{ \beta}_{h})]\) (i.e., the range in Lemma 1), \(\Delta u(\tilde{\beta}_{r},\tilde{\beta}_{h}\mid h)\leq\frac{\overline{\Delta PS }}{n-1}\) only if one of the following two holds: (1) \(\tilde{\beta}_{h}\geq 1-\tilde{b}_{h}\), or (2) \(\tilde{\beta}_{h}\leq\tilde{b}_{h}\) and \(\tilde{\beta}_{r}\leq\frac{q(h\mid h)}{q(\tilde{r}\mid h)}\cdot\tilde{b}_{h}\), where_

\[\tilde{b}_{h}=\frac{4\overline{\Delta PS}\cdot(\Delta h+\Delta t+PS(t,\mathbf{q }_{t})-PS(h,\mathbf{q}_{t}))}{(n-1)\cdot(\Delta h+\Delta t)\cdot\mathbb{E}_{ \mathbf{s}-\mathbf{q}_{t}}[PS(s,\mathbf{q}_{h})-q(s,\mathbf{q}_{t})]}.\]

We first consider the case when \(\tilde{\beta}_{r}=0\). Note that \(\Delta u(0,\beta_{h}\mid h)\) is a quadratic function of \(\tilde{\beta}_{h}\) satisfying (1) \(\Delta u(0,1\mid h)=0\), (2) \(\frac{\partial^{2}\Delta u(\tilde{\beta}_{h}\mid h)}{\partial\beta_{h}^{2}}=- \frac{2(k-1)}{n-1}\cdot q(h\mid h)\cdot(\Delta h+\Delta t)<0\), and (3) another root other than \(1\), denoted by \(\beta_{h}^{\mu\prime}\), satisfies \(\beta_{h}^{\mu\prime}\leq 0\). If (3) does not hold, we will have \(\Delta u(0,\mid h)<0\), which is a contradiction.

According to the property of the quadratic function, \(\Delta u(\tilde{\beta}_{r},\tilde{\beta}_{h}\mid h)\) is maximized at \(\frac{1+\beta_{h}^{\mu\prime}}{2}=\)

\[\frac{(k-1)\cdot(\Delta t+q(h\mid h)\cdot(PS(t,\mathbf{q}_{t})-PS(h,\mathbf{q} _{t}))+(n-k)\cdot(q(h\mid h)\cdot(-\Delta h)+q(\ell\mid h)\cdot\Delta t)}{2(k -1)\cdot(q(h\mid h)\cdot(\Delta h+\Delta t))}\]

with value

\[\frac{k-1}{4(n-1)}\cdot q(h\mid h)\cdot(\Delta h+\Delta t)\cdot(1-\beta_{h}^ {\mu\prime})^{2}\]

\[\geq\frac{k-1}{4(n-1)}\cdot q(h\mid h)\cdot(\Delta h+\Delta t).\]

We consider three different cases

Firstly, when \(PS(t,\mathbf{q}_{t})-PS(h,\mathbf{q}_{t})\leq 0\). In this case, we show that \(u(0,0\mid h)\) is faraway from \(u(\Sigma^{*}\mid h)\). Therefore, \(u(\Sigma^{*}\mid h)-u(0,\tilde{\beta}_{h}\mid h)\leq\frac{\overline{\Delta PS}} {n-1}\) only if \(\tilde{\beta}_{h}\) is close to \(1\). Note that in this case,

\[u(0,0\mid h,\text{deviator}) =PS(t,\mathbf{q}_{t})\] \[\leq q(h\mid h)\cdot q(\ell,\mathbf{q}_{h})+q(\ell\mid h)\cdot PS( \ell,\mathbf{q}_{t}).\]

Therefore,

\[\Delta u(0,0\mid h) =u(\Sigma^{*}\mid h)-u(0,0\mid h)\] \[\geq q(h\mid h)\cdot\Delta h-q(\ell\mid h)\cdot\Delta t\] \[=\mathbb{E}_{\mathbf{s}-\mathbf{q}_{t}}[PS(s,\mathbf{q}_{h})-q(s, \mathbf{q}_{t})]\] \[>0.\]

Then, give that \(\Delta u(0,\tilde{\beta}_{h}\mid h)\) is convex on \(\tilde{\beta}_{h}\), for all \(\tilde{\beta}_{h}\in[0,1]\),

\[\Delta u(0,\tilde{\beta}_{h}\mid h) \geq(1-\tilde{\beta}_{h})\cdot\mathbb{E}_{\mathbf{s}-\mathbf{q}_{t }}[PS(s,\mathbf{q}_{h})-q(s,\mathbf{q}_{t})].\]

Therefore, \(\Delta u(0,\tilde{\beta}_{h}\mid h)>\frac{\overline{\Delta PS}}{n-1}\) for any \(0\leq\tilde{\beta}_{h}<1-\frac{\overline{\Delta PS}}{(n-1)\cdot\mathbb{E}_{ \mathbf{s}-\mathbf{q}_{t}}[PS(s,\mathbf{q}_{t})-q(s,\mathbf{q}_{t})]}\).

Secondly, when \(PS(t,\mathbf{q}_{t})-PS(h,\mathbf{q}_{t})\geq 0\) and \(\frac{1+\beta_{h}^{\mu\prime}}{2}\leq 0\), we still prove that \(u(0,0\mid h)\) is faraway from \(u(\Sigma^{*}\mid h)\). Note that when \(\frac{1+\beta_{h}^{\mu\prime}}{2}\leq 0\), the deviating group size \(k\) must satisfy

\[k\leq\frac{\mathbb{E}_{\mathbf{s}-\mathbf{q}_{t}}[PS(s,\mathbf{q}_{t})-q(s, \mathbf{q}_{t})]}{q(h\mid h)\cdot(\Delta h+\Delta t+PS(t,\mathbf{q}_{t})-PS(h, \mathbf{q}_{t}))}\cdot(n-1)+1.\]Then,

\[\Delta u(0,0\mid h) =\frac{1}{n-1}((k-1)\cdot(u(0,0\mid h,\text{truthful})-u(0,0\mid h, \text{deviator}))\] \[\quad+(n-1)\cdot(u(\Sigma^{*}\mid h)-u(0,0\mid h,\text{truthful})))\] \[\geq\frac{1}{q(h\mid h)(\Delta h+\Delta t+PS(t,\mathbf{q}_{f})-PS( h,\mathbf{q}_{f}))}\] \[\quad\cdot(-\mathbb{E}_{\mathbf{s}\sim\mathbf{q}_{k}}[PS(s, \mathbf{q}_{h})-q(s,\mathbf{q}_{f})]\cdot q(h\mid h)\cdot(PS(t,\mathbf{q}_{f}) -PS(h,\mathbf{q}_{f}))\] \[\quad+q(h\mid h)\cdot(\Delta h+\Delta t+PS(t,\mathbf{q}_{f})-PS( h,\mathbf{q}_{f}))\cdot\mathbb{E}_{\mathbf{s}\sim\mathbf{q}_{h}}[PS(s,\mathbf{q}_{h})-q(s, \mathbf{q}_{f})])\] \[=\frac{(\Delta h+\Delta t)\cdot\mathbb{E}_{\mathbf{s}\sim\mathbf{ q}_{h}}[PS(s,\mathbf{q}_{h})-q(s,\mathbf{q}_{f})]}{(\Delta h+\Delta t+PS(t, \mathbf{q}_{f})-PS(h,\mathbf{q}_{f}))}\] \[>0. \tag{219}\]

Therefore, \(\Delta u(0,\beta_{h}\mid h)>\frac{\Delta PS}{n-1}\) for any \(0\leq\beta_{h}<1-\frac{\Delta PS}{(n-1)\cdot(\Delta h+\Delta t+PS(t,\mathbf{q} _{f})-PS(h,\mathbf{q}_{f}))}{(n-1)\cdot(\Delta h+\Delta t)\cdot\mathbb{E}_{ \mathbf{s}\sim\mathbf{q}_{h}}[PS(s,\mathbf{q}_{h})-q(s,\mathbf{q}_{f})]}\).

Thirdly, when \(PS(t,\mathbf{q}_{f})-PS(h,\mathbf{q}_{f})\geq 0\) and \(\frac{1+\mathbf{q}_{f}^{\prime\prime}}{2}>0\), the group size \(k\) must satisfy

\[k>\frac{\mathbb{E}_{\mathbf{s}\sim\mathbf{q}_{h}}[PS(\mathbf{q}_{h})-q(s, \mathbf{q}_{f})]}{q(h\mid h)\cdot(\Delta h+\Delta t+PS(t,\mathbf{q}_{f})-PS(h, \mathbf{q}_{f}))}\cdot(n-1)+1.\]

Therefore, For any \(\beta_{h}\in[\frac{1+\mathbf{q}_{f}^{\prime\prime}}{2},1]\),

\[\Delta u(0,\beta_{h}\mid h) \geq(1-\beta_{h})\cdot\frac{k-1}{4(n-1)}\cdot q(h\mid h)\cdot( \Delta h+\Delta t)\] \[\geq(1-\beta_{h})\cdot\frac{\mathbb{E}_{\mathbf{s}\sim\mathbf{q} _{h}}[PS(s,\mathbf{q}_{h})-q(s,\mathbf{q}_{f})]\cdot(\Delta h+\Delta t)}{4( \Delta h+\Delta t+PS(t,\mathbf{q}_{f})-PS(h,\mathbf{q}_{f}))}\]

Similarly, for any \(\beta_{h}\in[0,\frac{1+\mathbf{q}_{f}^{\prime\prime}}{2}]\),

\[\Delta u(0,\beta_{h}\mid h)\geq\beta_{h}\cdot\frac{\mathbb{E}_{\mathbf{s}\sim \mathbf{q}_{h}}[PS(s,\mathbf{q}_{h})-q(s,\mathbf{q}_{f})]\cdot(\Delta h+ \Delta t)}{4(\Delta h+\Delta t+PS(t,\mathbf{q}_{f})-PS(h,\mathbf{q}_{f}))}.\]

Therefore, \(\Delta u(0,\beta_{h}\mid h)>\frac{\Delta PS}{n-1}\) for any \(\beta_{h}<\beta_{h}<1-\beta_{h}\), where

\[\tilde{b}_{h}=\frac{4\Delta PS\cdot(\Delta h+\Delta t+PS(t,\mathbf{q}_{f})-PS (h,\mathbf{q}_{f}))}{(n-1)\cdot(\Delta h+\Delta t)\cdot\mathbb{E}_{\mathbf{s} \sim\mathbf{q}_{h}}[PS(s,\mathbf{q}_{h})-q(s,\mathbf{q}_{f})]}.\]

Then we consider when \(\beta_{f}=\frac{q(h\mid h)}{q(t\mid h)}\cdot(1-\beta_{h})\). From Lemma 1,

\[\frac{\partial u(\frac{q(h\mid h)}{q(t\mid h)}(1-\beta_{h}),\beta_{h}\mid h)}{ \partial\beta_{h}}=\mathbb{E}_{\mathbf{s}\sim\mathbf{q}_{h}}[PS(s,\mathbf{q}_ {h})-PS(s,\mathbf{q}_{f})]>0.\]

Therefore, for any \(0\leq\beta_{h}<1-\frac{\Delta PS}{(n-1)\cdot\mathbb{E}_{\mathbf{s}\sim\mathbf{ q}_{h}}[PS(s,\mathbf{q}_{h})-q(s,\mathbf{q}_{f})]}\), \(\Delta u(\frac{q(h\mid h)}{q(t\mid h)}\cdot(1-\beta_{h}),\beta_{h}\mid h)> \frac{\Delta PS}{n-1}\).

Then, by the linearity of \(\Delta u(\beta_{f},\beta_{h}\mid h)\) on \(\beta_{f}\), we know that for any \(\beta_{h}<\beta_{h}<1-\beta_{h}\) and any \(0\leq\beta_{f}<\frac{q(h\mid h)}{q(t\mid h)}\cdot(1-\beta_{h}),\Delta u(\beta_ {f},\beta_{h}\mid h)>\frac{\Delta PS}{n-1}\), the threshold comes out that \(\tilde{b}_{h}\) is the largest among all the threshold.

Then we consider \(0\leq\beta_{h}\leq b_{h}\). We have

\[\Delta u(0,\beta_{h}\mid h)\geq\beta_{h}\cdot\frac{\mathbb{E}_{\mathbf{s}\sim \mathbf{q}_{h}}[PS(s,\mathbf{q}_{h})-q(s,\mathbf{q}_{f})]\cdot(\Delta h+\Delta t)}{4( \Delta h+\Delta t+PS(t,\mathbf{q}_{f})-PS(h,\mathbf{q}_{f}))}\]

and

\[\Delta u(\frac{q(h\mid h)}{q(t\mid h)}\cdot(1-\beta_{h}),\beta_{h}\mid h) =(1-\beta_{h})\cdot\mathbb{E}_{\mathbf{s}\sim\mathbf{q}_{h}}[PS(s, \mathbf{q}_{h})-PS(s,\mathbf{q}_{f})]\] \[\geq(1-\beta_{h})\cdot\frac{\mathbb{E}_{\mathbf{s}\sim\mathbf{q} _{h}}[PS(s,\mathbf{q}_{h})-q(s,\mathbf{q}_{f})]\cdot(\Delta h+\Delta t)}{4( \Delta h+\Delta t+PS(t,\mathbf{q}_{f})-PS(h,\mathbf{q}_{f}))}.\]

Therefore, for any \(0\leq\beta_{f}\leq\frac{q(h\mid h)}{q(t\mid h)}\cdot(1-\beta_{h})\),\[\Delta u(\hat{p}_{\ell},\hat{p}_{h}\mid h) =\left(1-\frac{\hat{\beta}_{\ell}\cdot q(\ell\mid h)}{q(h\mid h) \cdot(1-\hat{\beta}_{h})}\right)\cdot\Delta u(0,\hat{p}_{h}\mid h) \tag{228}\] \[\quad+\frac{\hat{p}_{\ell}\cdot q(\ell\mid h)}{q(h\mid h)\cdot(1- \hat{p}_{h})}\cdot\Delta u\big{(}\frac{q(h\mid h)}{q(\ell\mid h)}\cdot(1-\hat{ \beta}_{h}),\hat{p}_{h}\mid h\big{)}\] (229) \[\geq\left(\hat{\beta}_{h}+\hat{p}_{\ell}\cdot\frac{q(\ell\mid h) \cdot(1-2\hat{p}_{h})}{q(h\mid h)\cdot(1-\hat{p}_{h})}\right)\cdot\frac{ \mathbb{E}_{q-\Phi_{h}}[PS(s,q_{h})-q(s,\mathbf{q}_{s})]\cdot(\Delta h+ \Delta\ell)}{4(\Delta h+\Delta\ell+PS(\ell,\mathbf{q}_{s})-PS(h,\mathbf{q}_{s }))}\] (230) \[\geq(\hat{\beta}_{h}+\frac{q(\ell\mid h)}{q(h\mid h)}\cdot\hat{p}_ {\ell}\cdot(1-2\hat{p}_{h}))\cdot\frac{\mathbb{E}_{q-\Phi_{h}}[PS(s,q_{h})-q(s,\mathbf{q}_{s})]\cdot(\Delta h+\Delta\ell)}{4(\Delta h+\Delta\ell+PS(\ell, \mathbf{q}_{s})-PS(h,\mathbf{q}_{s}))}. \tag{231}\]

Therefore, for

\[\hat{p}_{\ell}>\frac{q(h\mid h)}{q(\ell\mid h)\cdot(1-2\hat{p}_{h})}\cdot( \hat{p}_{h}-\hat{\beta}_{h}), \tag{232}\]

\(\Delta u(\hat{p}_{\ell},\hat{p}_{h}\mid h)>\frac{\overline{\Delta\Phi}S}{n-1}\). Given that \(\hat{b}_{h}<\frac{1}{2}\), the RHS is maximized at \(\hat{p}_{h}=0\). Therefore, for any \(0\leq\hat{\beta}_{h}\leq\hat{b}_{h}\) and any \(\hat{p}_{\ell}>\frac{q(h\mid h)}{q(\ell\mid h)}\cdot\hat{b}_{h}\), \(\overline{\Delta\Phi}_{h}\mid h)>\frac{\overline{\Delta\Phi}S}{n-1}\).

So far we have proved Lemma 5, which ends the second part.

In the third part, for the area close to \((0,0)\) or \((0,1)\), i.e. \((1)\)\(\hat{\beta}_{h}\geq 1-\hat{b}_{h}\), and \((2)\)\(\hat{p}_{h}\leq\hat{b}_{h}\) and \(\hat{p}_{\ell}\leq\frac{q(h\mid h)}{q(\ell\mid h)}\cdot\hat{b}_{h}\), we show that we can always find a deviator \(i\) such that \(u(\beta_{\ell}^{i},\beta_{h}^{i}\mid h)<u(\Sigma^{*}\mid h)\). Let

\[\hat{p}_{h} =\frac{1}{k-1}\sum_{j\in\overline{D}_{j}\neq i}\beta_{h}^{j}= \hat{p}_{h}+\frac{1}{k-1}(\hat{p}_{h}-\beta_{h}^{i}) \tag{233}\] \[\hat{p}_{\ell} =\frac{1}{k-1}\sum_{j\in\overline{D}_{j}\neq i}\beta_{h}^{j}= \hat{p}_{\ell}+\frac{1}{k-1}(\hat{p}_{\ell}-\beta_{\ell}^{i}) \tag{234}\]

be the average strategy of all the deviators other than \(i\). It is satisfied that \((\hat{p}_{\ell},\hat{p}_{h})\in[0,1]^{2}\).

We pick a deviator \(i\) and characterize the \(k\) such that \(u(\beta_{\ell}^{i},\beta_{h}^{i}\mid h)\geq u(\Sigma^{*}\mid h)\). More precisely,

\[k\geq\frac{u(\Sigma^{*}\mid h)-u(\beta_{\ell}^{i},\beta_{h}^{i}\mid h,\text{ truthful})}{\left(u(\Sigma^{*}\mid h)-u(\beta_{\ell}^{i},\beta_{h}^{i}\mid h,\text{ truthful})\right)-\left(u(\Sigma^{*}\mid h)-u(\beta_{\ell}^{i},\beta_{h}^{i}\mid h,\text{ deviator})\right)}\cdot(n-1)+1 \tag{235}\]

when the denominator \((u(\Sigma^{*}\mid h)-u(\beta_{\ell}^{i},\beta_{h}^{i}\mid h,\text{ truthful}))-(u(\Sigma^{*}\mid h)-u(\beta_{\ell}^{i},\beta_{h}^{i}\mid h,\text{ deviator}))>0\) or \(k\) does not exist when the denominator equals to or is less than \(0\). We will show that this \(k>k_{B}\) in both corner cases.

The numerator of RHS is

\[u(\Sigma^{*}\mid h)-u(\beta_{\ell}^{i},\beta_{h}^{i}\mid h,\text{ truthful})=(1-\beta_{h}^{i})\cdot\mathbb{E}_{3-\Phi_{h}}[PS(s,\mathbf{q}_{h})-PS(s, \mathbf{q}_{\ell})]. \tag{236}\]

The denominator is

\[u(\beta_{\ell}^{i},\beta_{h}^{i}\mid h,\text{ deviator})-u(\beta_{\ell}^{i},\beta_{h}^{i}\mid h,\text{ truthful}) \tag{237}\] \[=\beta_{h}^{i}\cdot((q(h\mid h)\cdot(\hat{\beta}_{h}-1)+q(\ell \mid h)\cdot\hat{p}_{\ell})\cdot PS(h,\mathbf{q}_{h})\] (238) \[\quad+(q(h\mid h)\cdot(1-\hat{p}_{h})-q(\ell\mid h)\cdot\hat{p}_{ \ell})\cdot PS(t,\mathbf{q}_{h}))\] (239) \[=(1-\beta_{h}^{i})\cdot((q(h\mid h)\cdot(\hat{\beta}_{h}-1)+q(\ell \mid h)\cdot\hat{p}_{\ell})\cdot PS(h,\mathbf{q}_{\ell})\] (240) \[\quad+(q(h\mid h)\cdot(1-\hat{p}_{h})-q(\ell\mid h)\cdot\hat{p}_{ \ell})\cdot PS(t,\mathbf{q}_{\ell}))\] (241) \[=(q(h\mid h)\cdot(1-\hat{p}_{h})-q(\ell\mid h)\cdot\hat{p}_{\ell})\] (242) \[\quad-(\beta_{h}^{i}\cdot(PS(t,\mathbf{q}_{h})-PS(h,\mathbf{q}_{h }))+(1-\beta_{h}^{i})\cdot(PS(t,\mathbf{q}_{\ell})-PS(h,\mathbf{q}_{\ell}))). \tag{243}\]

We first consider \(\hat{p}_{h}\leq\hat{b}_{h}\) and \(\hat{p}_{\ell}\leq\frac{q(h\mid h)}{q(\ell\mid h)}\cdot\hat{b}_{h}\). In this case, we pick a deviator \(i\) such that \(\beta_{h}^{i}\leq\hat{p}_{h}\).

Firstly, there must be \((q(h\mid h)\cdot(1-\hat{p}_{h})-q(\ell\mid h)\cdot\hat{p}_{\ell})>0\) for all sufficiently large \(n\). This is because \(\hat{p}_{h}\leq\hat{b}_{h}=\Theta(\frac{\overline{\Delta\Phi}S}{n-1})\) and \(\hat{p}_{\ell}\leq\frac{q(h\mid h)}{q(\ell\mid h)}\cdot\hat{b}_{h}\). Moreover, \(\hat{p}_{h}\leq 2\hat{p}_{h}\) and \(\hat{p}_{\ell}\leq 2\hat{p}_{\ell}\) by the property of the average. Therefore, for sufficiently large \(n\) such that \(\hat{b}_{h}<\frac{1}{4}\), \((q(h\mid h)\cdot(1-\hat{p}_{h})-q(\ell\mid h)\cdot\hat{p}_{\ell})>0\).

If \(PS(t,\mathbf{q}_{\ell})-PS(h,\mathbf{q}_{\ell})\leq 0\), there must be \((PS(t,\mathbf{q}_{h})-PS(h,\mathbf{q}_{h}))<0\). In this case \(u(\beta_{\ell}^{i},\beta_{h}^{i}\mid h,\text{deviator})-u(\beta_{\ell}^{i}, \beta_{h}^{i}\mid h,\text{truthful})<0\), and for any \(k\geq 2\), \(i\)'s reward will be strictly lower than the truthful reward.

Suppose \(PS(\ell,\mathbf{q}_{\ell})-PS(h,\mathbf{q}_{\ell})>0\). In this case, the condition for \(u(\beta_{\ell}^{l},\beta_{h}^{l}\mid h)\geq u(\Sigma^{*}\mid h)\) is equivalent to \(k-1\geq 2599\)

\[\frac{(1-\beta_{h}^{l})-\Sigma_{\mathbf{q}_{\ell}}[PS(\mathbf{q}_{h},\mathbf{q} _{h})-PS(\mathbf{q}_{\ell})]}{(q(h\mid h)\cdot(1-\beta_{h})-q(\ell\mid h)\cdot \beta_{\ell})\cdot(\beta_{h}^{l}-PS(\mathbf{q}_{h},\mathbf{q}_{h}))+(1-\beta_{ h}^{l})\cdot(PS(\ell,\mathbf{q}_{\ell})-PS(h,\mathbf{q}_{\ell}))}\cdot(n-1). \tag{280}\]

We show this lower bound is larger than \(k_{B}\). Recall that

\[k_{B}\leq\frac{\Sigma_{\mathbf{s}-\mathbf{q}_{h}}[PS(\mathbf{s},\mathbf{q}_{h })-PS(\mathbf{s},\mathbf{q}_{\ell})]}{q(h\mid h)\cdot(PS(\ell,\mathbf{q})-PS( h,\mathbf{q}_{\ell}))}\cdot(n-1)+1 \tag{281}\]

Therefore, it is sufficient to show that

\[\frac{(1-\beta_{h}^{l})\cdot q(h\mid h)\cdot(PS(\mathbf{c},\mathbf{q}_{h}))}{ (q(h\mid h)\cdot(1-\beta_{h})-q(\ell\mid h)\cdot\beta_{\ell})\cdot(\beta_{h}^ {l}-PS(h,\mathbf{q}_{h}))+(1-\beta_{h}^{l})\cdot(PS(\ell,\mathbf{q}_{\ell})-PS (h,\mathbf{q}_{\ell}))}>1. \tag{282}\]

Firstly, given that \(\beta_{h}^{l}\leq\hat{\beta}_{h}\), there is \(\beta_{h}^{l}\leq\hat{\beta}_{h}\). Therefore,

\[(q(h\mid h)\cdot(1-\hat{\beta}_{h})-q(\ell\mid h)\cdot\hat{\beta}_{\ell}) \leq q(h\mid h)\cdot(1-\hat{\beta}_{h}) \tag{283}\] \[\leq(1-\beta_{h}^{l})\cdot q(h\mid h). \tag{284}\]

Secondly, by Lemma 3 we have \(PS(\ell,\mathbf{q}_{h})-PS(h,\mathbf{q}_{h})<PS(\ell,\mathbf{q}_{\ell})-PS(h, \mathbf{q}_{\ell})\). Therefore, \(\beta_{h}^{l}\cdot(PS(\ell,\mathbf{q}_{h})-PS(h,\mathbf{q}_{h}))+(1-\beta_{h}^ {l})\cdot(PS(\ell,\mathbf{q}_{\ell})-PS(h,\mathbf{q}_{\ell}))\leq PS(\ell, \mathbf{q}_{\ell})-PS(h,\mathbf{q}_{\ell})\).

By combining two parts, we show that every part in the denominator is smaller than the corresponding part in the nominator. Therefore, the threshold for \(i\)'s reward exceeds the truthful reward \(k\geq k_{B}\), and the equality holds only when \(\beta_{h}^{l}=\hat{\beta}_{h}=\hat{\beta}_{\ell}=0\). When the equality holds, all other deviators play (0, 0). If \(\beta_{\ell}^{l}=0\), the case is covered by Step 2. Otherwise, we consider a different deviator \(i\). Then the threshold for the new \(i\) will be strictly larger than \(k_{B}\). Therefore, for all \(k\leq k_{B}\), \(i\)'s reward is strictly lower than the truthful reward.

We then consider the second area \(\hat{\beta}_{h}\geq 1-\hat{b}_{h}\). When \(PS(h,\mathbf{q}_{h})\leq PS(\ell,\mathbf{q}_{h})\), we we pick an \(i\) such that \(\beta_{h}^{l}\leq\hat{\beta}_{h}\) and compare \(i\)'s reward with the truthful reward. In this case, \(0\leq PS(\ell,\mathbf{q}_{h})-PS(h,\mathbf{q}_{h})<PS(\ell,\mathbf{q}_{\ell})- PS(h,\mathbf{q}_{\ell})\). If \(q(h\mid h)\cdot(1-\hat{\beta}_{h})-q(\ell\mid h)\cdot\hat{\beta}_{\ell}\leq 0\), the denominator is non-positive, and for any \(k\geq 2\), \(i\)'s reward cannot exceed the truthful reward. If \(q(h\mid h)\cdot(1-\hat{\beta}_{h})-q(\ell\mid h)\cdot\hat{\beta}_{\ell}>0\), the denominator is positive. Following similar reasoning for \((\hat{\beta}_{h},\hat{\beta}_{h})\) close to \((0,0)\) shows that the threshold \(k>k_{B}\).

Otherwise, when \(PS(h,\mathbf{q}_{h})>PS(\ell,\mathbf{q}_{h})\), we compare \(i\)'s reward with the reward of average strategy \((\hat{\beta}_{\ell},\hat{\beta}_{h})\). Recall that

\[u(\beta_{\ell}^{l},\beta_{h}^{l}\mid h)-u(\hat{\beta}_{\ell},\hat{ \beta}_{h}\mid h) \tag{285}\] \[=\frac{n-k}{n-1}\cdot f^{h}(\beta_{h}^{l}-\hat{\beta}_{h},(0,1))+ \frac{k-1}{n-1}f(\beta_{h}^{l}-\hat{\beta}_{h},(\hat{\beta}_{\ell},\hat{\beta}_ {h}))\] (286) \[\quad+\frac{1}{n-1}(f(\beta_{h}^{l},(\hat{\beta}_{\ell},\hat{\beta }_{h}))-f(\beta_{h}^{l},(\hat{\beta}_{h}^{l})))\] (287) \[=(\beta_{h}^{l}-\hat{\beta}_{h})\cdot(\Sigma_{\mathbf{s}-\mathbf{ q}_{h}}[PS(s,\mathbf{q}_{h})-PS(s,\mathbf{q}_{\ell})]\] (288) \[\quad-\frac{k-1}{n-1}\cdot(q(h\mid h)\cdot(1-\hat{\beta}_{h})-q( \ell\mid h)\cdot\hat{\beta}_{\ell})\cdot(\Delta h+\Delta\ell))\] (289) \[\quad+\frac{1}{n-1}\cdot(q(h\mid h)\cdot(\hat{\beta}_{h}-\beta_{h }^{l})+q(\ell\mid h)\cdot(\hat{\beta}_{\ell}-\beta_{\ell}^{l}))\] (290) \[\quad\cdot(\beta_{h}^{l}\cdot(PS(h,\mathbf{q}_{h})-PS(\ell, \mathbf{q}_{h}))+(1-\beta_{h}^{l})\cdot(PS(h,\mathbf{q}_{\ell})-PS(\ell,\mathbf{q }_{\ell}))). \tag{291}\]

We will assume that \(n\) is sufficiently large so that \(((1-\hat{b}_{h})\cdot(PS(h,\mathbf{q}_{h})-PS(\ell,\mathbf{q}_{h}))+\hat{b}_{h} \cdot(PS(h,\mathbf{q}_{\ell})-PS(\ell,\mathbf{q}_{\ell}))>0\). (Recall that \(\hat{b}_{h}=\Theta(\frac{1}{n-1})\)).

Note that for the second line, \((q(h\mid h)\cdot(1-\hat{\beta}_{h})-q(\ell\mid h)\cdot\hat{\beta}_{\ell})\cdot( \Delta h+\Delta\ell)\leq q(h\mid h)\cdot\hat{b}_{h}\cdot(\Delta h+\Delta\ell)\). Therefore, for sufficiently large \(n\),

\[(\beta_{h}^{l}-\hat{\beta}_{h})\cdot(\Sigma_{\mathbf{s}-\mathbf{q}_{h}}[PS(s, \mathbf{q}_{h})-PS(s,\mathbf{q}_{\ell})] \tag{292}\] \[\quad-\frac{k-1}{n-1}\cdot(q(h\mid h)\cdot(1-\hat{\beta}_{h})-q( \ell\mid h)\cdot\hat{\beta}_{\ell})\cdot(\Delta h+\Delta\ell))\geq 0. \tag{293}\]

Let

\[m_{1} =(\mathbb{B}_{\mathbf{s}-\mathbf{q}_{h}}[PS(s,\mathbf{q}_{h})-PS(s, \mathbf{q}_{\ell})] \tag{294}\] \[\quad-\frac{k-1}{n-1}\cdot(q(h\mid h)\cdot(1-\hat{\beta}_{h})-q( \ell\mid h)\cdot\hat{\beta}_{\ell})\cdot(\Delta h+\Delta\ell))\] (295) \[m_{2}(\beta_{h}^{l}) =(\beta_{h}^{l}\cdot(PS(h,\mathbf{q}_{h})-PS(\ell,\mathbf{q}_{h}))+(1 -\beta_{h}^{l})\cdot(PS(h,\mathbf{q}_{\ell})-PS(\ell,\mathbf{q}_{\ell}))). \tag{296}\]Then

\[u(\beta_{r}^{i},\beta_{h}^{i}\mid h)-u(\beta_{r},\beta_{h}\mid h) \tag{249}\] \[=(\beta_{h}^{i}-\beta_{h})\cdot m_{1}+\frac{1}{n-1}\cdot(q(h\mid h) \cdot(\beta_{h}-\beta_{h}^{i})+q(\ell\mid h)\cdot(\beta_{\ell}-\beta_{\ell}^{i} ))\cdot m_{2}(\beta_{h}^{i})\] (250) \[=(\beta_{h}^{i}-\beta_{h})\cdot(m_{1}-\frac{1}{n-1}\cdot q(h\mid h) \cdot m_{2}(\beta_{h}^{i}))+\frac{1}{n-1}\cdot q(\ell\mid h)\cdot(\beta_{\ell} -\beta_{\ell}^{i})\cdot m_{2}(\beta_{h}^{i}). \tag{251}\]

Note that \(m_{1}>0\) and \(\frac{\partial m_{2}}{\partial\beta_{h}^{i}}=\Delta h+\Delta t>0\).

If there exists a deviator \(i\) such that \(\beta_{h}^{i}\leq\beta_{h}\) and \(u(\beta_{r}^{i},\beta_{h}^{i}\mid h)-u(\beta_{\ell},\beta_{h}\mid h)<0\), we just pick this \(i\). Otherwise, if all deviators \(j\) with \(\beta_{h}^{j}\leq\beta_{h}\) has \(u(\beta_{r}^{i},\beta_{h}^{i}\mid h)-u(\beta_{r},\beta_{h}\mid h)\geq 0\), then the range of \(j\)'s strategy \((\beta_{r}^{i},\beta_{h}^{j})\) satisfies

\[(\beta_{h}^{j}-\beta_{h})\cdot(m_{1}-\frac{1}{n-1}\cdot q(h\mid h)\cdot m_{2} (\beta_{h}^{j}))+\frac{1}{n-1}\cdot q(\ell\mid h)\cdot(\beta_{\ell}-\beta_{\ell }^{j})\cdot m_{2}(\beta_{h}^{j})\geq 0 \tag{252}\]

This directly implies that \(\beta_{\ell}^{i}<\beta_{\ell}\) for any \(j\) with \(\beta_{h}^{j}\leq\beta_{h}\).

Now we pick another deviator \(i\) such that (1) \(\beta_{h}^{i}\geq\beta_{h}\) and (2) for some deviator \(j\) with \(\beta_{h}^{j}<\beta_{h}\), \((\beta_{h}^{i}-\beta_{h})(\beta_{\ell}-\beta_{\ell}^{i})\leq(\beta_{\ell}^{i}- \beta_{\ell})(\beta_{h}-\beta_{h}^{j})\). If such \(i\) does not exist, then for any \(i\) with \(\beta_{h}^{i}>\beta_{h}\) and any \(j\) with \(\beta_{h}^{i}\leq\beta_{h}\), there is \((\beta_{h}^{i}-\beta_{h})(\beta_{\ell}-\beta_{\ell}^{i})>(\beta_{\ell}^{i}- \beta_{\ell})(\beta_{h}-\beta_{h}^{j})\). Then,

\[0 =\sum_{i\in D,\beta_{h}^{i}>\beta_{h}}(\beta_{h}^{i}-\beta_{h})+ \sum_{j\in D,\beta_{h}^{i}\leq\beta_{h}}(\beta_{h}^{j}-\beta_{h}) \tag{253}\] \[>\sum_{i\in D,\beta_{h}^{i}>\beta_{h}}\frac{\beta_{\ell}^{i}-\beta _{\ell}}{\sum_{j\in D,\beta_{h}^{i}\leq\beta_{h}}(\beta_{h}^{j}-\beta_{\ell})} \cdot\sum_{j\in D,\beta_{h}^{i}\leq\beta_{h}}(\beta_{h}^{j}-\beta_{h})+\sum_{j \in D,\beta_{h}^{i}\leq\beta_{h}}(\beta_{h}^{j}-\beta_{h})\] (254) \[=\frac{\sum_{j\in D,\beta_{h}^{i}\leq\beta_{h}}(\beta_{h}^{j}- \beta_{h})}{\sum_{j\in D,\beta_{h}^{i}\leq\beta_{h}}(\beta_{h}^{j}-\beta_{h})} \cdot\left(\sum_{i\in D,\beta_{h}^{i}>\beta_{h}}(\beta_{\ell}^{i}-\beta_{\ell} )+\sum_{j\in D,\beta_{h}^{i}\leq\beta_{h}}(\beta_{h}^{j}-\beta_{\ell})\right)\] (255) \[=0, \tag{256}\]

which is a contradiction. Therefore, the deviator \(i\) we pick always exists.

Now we compare \(i\)'s reward with the reward of the average strategy. Note that since \(\beta_{h}^{i}>\beta_{h}\geq\beta_{h}^{j},m_{2}(\beta_{h}^{i})>m_{2}(\beta_{h}^ {j})\).

\[u(\beta_{r}^{i}\beta_{h}^{i}\mid h)-u(\beta_{r},\beta_{h}\mid h) \tag{257}\] \[=(\beta_{h}^{i}-\beta_{h})\cdot(m_{1}-\frac{1}{n-1}\cdot q(h\mid h )\cdot m_{2}(\beta_{h}^{i}))+\frac{1}{n-1}\cdot q(\ell\mid h)\cdot(\beta_{ \ell}-\beta_{\ell}^{i})\cdot m_{2}(\beta_{h}^{i})\] \[<(\beta_{h}^{i}-\beta_{h})\cdot(m_{1}-\frac{1}{n-1}\cdot q(h\mid h )\cdot m_{2}(\beta_{h}^{j}))+\frac{1}{n-1}\cdot q(\ell\mid h)\cdot(\beta_{\ell} -\beta_{\ell}^{i})\cdot m_{2}(\beta_{h}^{j})\] (258) \[\leq\frac{\beta_{h}^{i}-\beta_{\ell}}{\beta_{\ell}-\beta_{\ell}^{i }}(\beta_{h}-\beta_{h}^{j})\cdot(m_{1}-\frac{1}{n-1}\cdot q(h\mid h)\cdot m_{2 }(\beta_{h}^{j}))+\frac{1}{n-1}\cdot q(\ell\mid h)\cdot(\beta_{\ell}-\beta_{ \ell}^{i})\cdot m_{2}(\beta_{h}^{j})\] (259) \[=-\frac{\beta_{\ell}^{i}-\beta_{\ell}}{\beta_{\ell}-\beta_{\ell}^{i }}\left((\beta_{h}^{i}-\beta_{h})(m_{1}-\frac{1}{n-1}q(h\mid h)\cdot m_{2}( \beta_{h}^{j}))+\frac{1}{n-1}q(\ell\mid h)\cdot(\beta_{\ell}-\beta_{\ell}^{i} )\cdot m_{2}(\beta_{h}^{j})\right)\] (260) \[\leq 0. \tag{261}\]

Therefore, we find an \(i\) such that \(u(\beta_{\ell}^{i},\beta_{h}^{i}\mid h)<u(\beta_{\ell},\beta_{h}\mid h)\leq u( \Sigma^{*}\mid h)\).

Consequently, for any \(n\) satisfying:

1. \(\beta_{h}=\frac{\frac{\partial\mathcal{D}S}{(\Delta h+\Delta t+PS(\Delta h)-PS( \Delta h))}}{(n-1)\cdot(\Delta h+\Delta t)\cdot\Sigma_{h}-u(\beta_{\ell}, \beta_{\ell})}<\frac{1}{n}\),
2. If \(PS(h,\mathbf{q}_{h})>PS(\mathbf{q}_{h})\), then \(\beta_{h}\leq\frac{PS(h,\mathbf{q}_{h})-PS(\mathbf{r}_{h})}{\Delta h+\Delta t}\),
3. \(\beta_{h}\leq\frac{\frac{\partial\mathcal{D}S}{(\Delta h+\Delta t+PS(\Delta h)-PS( \Delta h))}}{\Delta h+\Delta t}\),

for any deviation with no more than \(k_{B}\) deviators and the average strategy in the area of Lemma 1, there exists a deviator \(i\) with private signal \(h\) whose reward is strictly worse than the truthful reward. Therefore, such deviation cannot succeed.

Similarly, for the \(t\) side, for any \(n\) such that

1. \(\beta_{l}=\frac{\frac{\partial\mathcal{D}S}{(\Delta h+\Delta t+PS(h,\mathbf{q}_{h}) -PS(\Delta h))}}{(n-1)\cdot(\Delta h+\Delta t)\cdot\Sigma_{h}-u(\beta_{\ell}, \beta_{\ell})-q(\Delta_{h},\beta_{\ell})}<\frac{1}{4}\),
2. If \(PS(t,\mathbf{q}_{h})>PS(\mathbf{q}_{h})\), then \(\beta_{h}\leq\frac{PS(t,\mathbf{q}_{h})-PS(\Delta h)}{\Delta h+\Delta t}\),3. \(\hat{b}_{h}\leq\frac{\mathbb{E}_{n-k_{B}}[PS(s_{h},q_{h})-q(s_{h},q_{h})]}{q(f(f( \cdot)\Delta h\cdot\Delta t))}\),
4. for any deviation with no more than \(k_{B}\) deviators and the average strategy in the area of Lemma 2, there exists a deviator \(i\) with private signal \(\ell\) whose reward is strictly worse than the truthful reward. Therefore, such deviation cannot succeed.

Therefore, truthful reporting is an Bayesian \(k_{B}\)-strong equilibrium.

### Truthful Reporting is not a coalitional interim equilibrium

In this section, we introduce the coalitional interim equilibrium in [26].

**Definition 5**.: Given the set of all admissible deviating groups \(\mathcal{D}\), a strategy profile \(\Sigma\) is an interim \(\mathcal{D}\) equilibrium if there does not exist a group of agent \(D\in\mathcal{D}\), a set of types \(s_{D}=(s_{i})_{i\in D}\), and a different strategy profile \(\Sigma^{\prime}=(\sigma^{\prime}_{i})\) such that

1. for all agent \(i\not\in D\), \(\sigma^{\prime}_{i}=\sigma_{i}\);
2. for all \(i\in D\), \(u_{i}(\Sigma^{\prime}\mid s_{D})>u_{i}(\Sigma\mid s_{D})\),

where \(u_{i}(\Sigma\mid s_{D})\) is \(i\)'s expected utility conditioned on he/she knows the types of all the deviators in \(D\).

When \(\mathcal{D}=\{(i\mid i\in[n])\}\) contains only singletons, interim \(\mathcal{D}\) equilibrium is exactly the Bayesian Nash equilibrium. On the other hand, we show that for any \(\mathcal{D}\) containing a group of at least two agents, truthful reporting fails to be an interim \(\mathcal{D}\) equilibrium.

**Proposition 4**.: _In the peer prediction mechanism, assume \(PS(h,q_{h})>PS(\ell,q_{h})\) and \(PS(\ell,q_{r})>PS(h,q_{r})\). Then for any constant \(d\geq 2\) any \(\mathcal{D}\) such that there exists a \(D\in\mathcal{D}\) with \(|D|=d\), and for all sufficiently large \(n\), truthful reporting is NOT an interim \(\mathcal{D}\) equilibrium._

Proof.: Let \(D\in\mathcal{D}\) such that \(|D|=d\) be a deviate group. Suppose there are \(d_{1}>0\) agents with signal \(h\) and \(d_{2}>0\) agents with signal \(\ell\).

\(d_{1}+d_{2}=|D|\).

Now consider the expected utility when every agent reports truthfully. For agent \(i\) with signal \(h\), \(i\)'s reward from other deviators is \(\frac{d_{i}-1}{n-1}\cdot PS(h,q_{h})+\frac{d_{2}}{n-1}\cdot PS(\ell,q_{h})\). And \(i\)'s reward from truthful reporter is \(\frac{n-|D|}{n-1}\cdot(q(h\mid s_{D})\cdot PS(\ell,q_{h}))\). Similarly, with agent \(i\) with signal \(\ell\), \(i\)'s reward from other deviators is \(\frac{d_{i}}{n-1}\cdot PS(h,q_{r})+\frac{d_{2}-1}{n-1}\cdot PS(\ell,q_{r})\). And \(i\)'s reward from truthful reporter is \(\frac{n-|D|}{n-1}\cdot(q(h\mid s_{D})\cdot PS(h,q_{r})+q(\ell\mid s_{D})\cdot PS (\ell,q_{r}))\).

Now we consider the deviating strategy. If \(q(h\mid s_{D})\cdot PS(h,q_{h})+q(\ell\mid s_{D})\cdot PS(\ell,q_{h})>q(h\mid s _{D})\cdot PS(h,q_{r})+q(\ell\mid s_{D})\cdot PS(\ell,q_{r})\), then all the deviators report \(h\). If \(q(h\mid s_{D})\cdot PS(h,q_{h})+q(\ell\mid s_{D})\cdot PS(\ell,q_{h})<q(h\mid s _{D})\cdot PS(h,q_{r})+q(\ell\mid s_{D})\cdot PS(\ell,q_{r})\), then all the deviators report \(\ell\). If \(q(h\mid s_{D})\cdot PS(h,q_{h})+q(\ell\mid s_{D})\cdot PS(\ell,q_{h})=q(h\mid s _{D})\cdot PS(h,q_{r})+q(\ell\mid s_{D})\cdot PS(\ell,q_{r})\), all the deviators report \(h\) if \(PS(h,q_{h})\geq PS(\ell,q_{r})\) and report \(\ell\) otherwise. We show that in this case, the deviation succeeds.

**Case 1**. Suppose \(q(h\mid s_{D})\cdot PS(h,q_{h})+q(\ell\mid s_{D})\cdot PS(\ell,q_{h})>q(h\mid s _{D})\cdot PS(h,q_{r})+q(\ell\mid s_{D})\cdot PS(\ell,q_{r})\). We consider the changes on the expected utility after the deviators switch from truthful reporting to the deviating strategy. Then for agents with signal \(h\), the expected utility from the truthful reporters is unchanged, and the expected utility from other deviators becomes \(\frac{d_{i}-1}{n-1}PS(h,q_{h})\), which has been strictly increased. For agents with signal \(\ell\), the expected utility from the truthful reporters strictly increases by a constant factor, while the changes in expected utility from other deviators is \(\Theta(\frac{d}{h})=\frac{1}{h}\). Therefore, the sufficiently large \(n\), the expected utility for agents with signal \(\ell\) also strictly increases.

**Case 2** follows similar reasoning to Case 1.

**Case 3**. Suppose \(q(h\mid s_{D})\cdot PS(h,q_{h})+q(\ell\mid s_{D})\cdot PS(\ell,q_{h})=q(h\mid s _{D})\cdot PS(h,q_{r})+q(\ell\mid s_{D})\cdot PS(\ell,q_{r})\) In this case, for both type of agents, the expected utility from truthful reporters is unchanged, and the expected utility from other deviators strictly increases.

Therefore, we show that in all cases, there exists a group of agents in \(\mathcal{D}\) wish to deviate. Therefore, truthful reporting is not an interim \(\mathcal{D}\) equilibrium.