InfoMAE: Pairing-Efficient Cross-Modal Alignment with Informational Masked Autoencoders for IoT Signals

Anonymous Author(s)

###### Abstract.

Standard multimodal self-supervised learning (SSL) algorithms regard cross-modal synchronization as implicit supervisory labels during pretraining, thus posing high requirements on the scale and quality of multimodal samples. These constraints significantly limit the performance of sensing intelligence in IoT applications, where the heterogeneity and the non-interpretability of time-series signals result in abundant unimodal data but scarce high-quality multimodal pairs. This paper proposes InfoMAE, a cross-modal alignment framework that tackles the challenge of multimodal pair efficiency under the SSL setting by facilitating efficient cross-modal alignment of pretrained unimodal representations. InfoMAE achieves _efficient cross-modal alignment_ with _limited data pairs_ through a novel information theory-inspired formulation that simultaneously addresses distribution-level and instance-level alignment. Extensive experiments on two real-world IoT applications are performed to evaluate InfoMAE's pairing efficiency to bridge pretrained unimodal models into a cohesive joint multimodal model. InfoMAE enhances downstream multimodal tasks by over 60% with significantly improved multimodal pairing efficiency. It also improves unimodal task accuracy by an average of 22% 1.

Footnote 1: We promise to release the source code upon the paperâ€™s acceptance.

**ACM Reference Format:**

Anonymous Author(s). 2024. InfoMAE: Pairing-Efficient Cross-Modal Alignment with Informational Masked Autoencoders for IoT Signals. In. ACM, New York, NY, USA, 16 pages. [https://doi.org/10.1145/nnnnnn.nnnn](https://doi.org/10.1145/nnnnnn.nnnn)

## 1. Introduction

Multimodal Self-Supervised Learning (SSL) algorithms, although achieving unprecedented performance in extensive sensing applications [11, 12, 40, 52, 53], present unique data challenges rarely encountered with unimodal SSL or vision-language domains due to the complexity in acquiring high-quality multimodal pair for IoT signals. The inherent properties of sensory data common in sensing applications result in abundant unimodal signals but scarce multimodal pairs. First, sensory modalities have heterogeneous properties, such as sampling rate, timestamp, or duration, that increase the likelihood of capturing asynchronous events. Consequently, standard IoT multimodal datasets require manual calibration to reduce temporal misalignments or to synchronize between the time-series signals. [40, 57, 62]. Second, raw IoT signals often lack intuitive interpretability. Unlike images or text, where visual features can be easily matched to textual captions, capturing useful signatures between sensing modalities like vibration or frequency waves is challenging. Preprocessing and calibrating these signals requires modality-specific domain knowledge or technical expertise, which is labor-intensive and susceptible to operational errors. Finally, IoT sensors are subject to varying deployment conditions, leading to sparse and noisy data [36]. Each modality can be independently affected by the deployment conditions or environmental factors. For instance, a loud noise source might significantly impact acoustic sensors while having minimal effect on seismic signals. This heterogeneity often results in poor-quality multimodal pairs that are uncorrelated with each other or incomplete datasets with significant gaps and missing data points. These factors contribute to significant challenges in IoT data collection. As IoT networks scale in quantity and the number of modalities, acquiring large-scale, high-quality multimodal pairs becomes increasingly time-consuming, error-prone, and less reliable. The limited multimodal pairs with potential misalignments can introduce uninformative false positive pairs [9, 49], polluting the multimodal feature patterns extracted by the pretrained encoders.

Despite these challenges, most existing multimodal SSL frameworks [1, 35, 48, 56] rely heavily on massive multimodal pairs to learn robust joint representations during the pretraining, but their capability could degrade significantly with insufficient synchronized pairs [44, 72]. On the other hand, independently pretraining each modality on their unimodal data and directly concatenating misaligned modality features for finetuning fails to capture cross-modal interactions that are critical to downstream multimodal tasks [28, 69]. Instead, we observe that with limited multimodal pairs, we can effectively convert independently trained unimodal encoders into a coherent model that sustains strong generalizability

Figure 1. Comparison of supervised learning, self-supervised learning, and pair-efficient self-supervised learning.

in multimodal tasks. We refer to this process as _pair-efficient SSL_. The relation of pair-efficient SSL for multimodal data compared to standard SSL draws an analogy to the evolvement of SSL compared to supervised learning, as visualized in Figure 1. In supervised learning, manual labels serve as supervision to train encoders for mapping inputs to task-specific labels. Its performance depends heavily on the quantity and quality of human annotations. Self-supervised learning (SSL) mitigates label scarcity by first designating proxy labels from the data properties to learn general semantics with massive unlabeled data, then calibrating the pretrained model to a downstream task with minimal human annotations. Similarly, in multimodal SSL contexts, cross-modal alignment acts as a special form of "supervision", where point-to-point modality correspondence is utilized to identify semantically meaningful and consistent sensory information. Taking another step forward, pair-efficient SSL takes advantage of abundant unimodal data for "independent pretraining", followed by "cross-modal finetuning" with limited multimodal pairs to align unimodal models into a cohesive multimodal model.

In this paper, we propose InfoMAE, a cross-modal learning framework designed to enhance the alignment of unimodal representations using a limited number of multimodal pairs. The key idea behind InfoMAE is to enforce alignment across modalities at both the _distribution_ and _instance_ levels. Existing contrastive learning frameworks adopt point-to-point alignment to map samples across different modalities to a proximate joint representation (Selak and Komodakis, 2016; Liu et al., 2017; Wang et al., 2018; Wang et al., 2019). These approaches focus on aligning individual samples, essentially viewing alignment as a local optimization problem that aims to minimize the geometric distances between corresponding samples in the representation space. However, such instance-level approaches face significant challenges with limited multimodal pairs, as they may overfit to the specific pairs available and result in poor generalization with pariring biases. These hinder capturing complex cross-modal relationships, especially when the multimodal pairs are sparse and unevenly distributed. In contrast, InfoMAE takes a more holistic approach by emphasizing _distribution-level_ alignment, considering the overall information content of the limited multimodal pairs rather than only focusing on the individual samples. We present a comprehensive analysis of distribution alignment and propose an _information theory-based approach_ to formally define the distribution alignment problem in the factorized information space. We formulate this as a differential learning objective to construct (i) shared joint representations as a compact common variable across modalities capable of performing any multimodal task and (ii) private representations holding implicit modality-specific information independent of shared representations. InfoMAE alleviates the strict requirement of exact multimodal sample pairs and can better accommodate potential misalignments in data collection or temporal synchronization, improving the representations learned even with a small-scale multimodal pair.

We extensively evaluate InfoMAE across various combinations of pretrained unimodal domains. InfoMAE achieves exceptional performance gain compared to the standard multimodal SSL paradigm under limited multimodal pairs and outperforms existing works when aligning the unimodal representations. Individual unimodal encoders, in return, can also benefit from the representational structures with improved downstream performance. Additionally, as the number of multimodal pairs scale, InfoMAE also demonstrates versatility as a standard multimodal SSL framework, achieving SOTA performance across real-world IoT applications.

## 2. Analysis of Cross-Modal Alignment

### Notation

Consider \(M\) sets of unsynchronized modality data \(\mathcal{X}=\{X_{i}\}_{i\in M}\), where each set \(X_{i}\) contains unlabeled samples of fixed-length windows partitioned from the time-series signals of the \(i\)-th modality. Let \(N_{i}=|X_{i}|\) denote the size of each modality set.

For the \(j\)-th sample of modality set \(i\), we apply Short-Time Fourier Transform (STFT) to obtain its time-frequency representation, \(\mathbf{x}_{ij}\in\mathbb{R}^{C_{i}\times I\times S_{i}}\), where \(C_{i}\) is the number of input channels, \(I\) is the number of time intervals within a sample window, and \(S_{i}\) is the spectrum length in the frequency domain. We have a set of modality encoders \(\mathcal{E}=\{E_{1},E_{2},\ldots,E_{M}\}\) to extract the modality embeddings of each sample and a set of modality decoders \(\mathcal{D}=\{D_{1},D_{2},\ldots,D_{M}\}\) to map the samples from the embedding space back to the time-frequency domain \(\hat{\mathcal{X}}=\{\hat{X_{i}}\}_{i\in M}\) as a part of the reconstruction process. Additionally, there is a set of multimodal data \(\mathcal{X}^{s}=\{X_{i}^{s}\}_{i\in M^{s}}\) consisting of a subset of modalities \(M^{s}\subseteq M\), where samples across the modalities are synchronized in time and have equal sizes \(|X_{1}^{s}|=\cdots=|X_{M^{s}}^{s}|\). Note that each synchronized data of modality \(i\) can also be a subset of the unsynchronized unimodal, set such that \(X_{i}^{s}\subseteq X_{i}\), as any synchronized multimodal data is inherently unsynchronized when considered independently. Finally, we have a set of labeled data for supervised learning and finetuning on a much smaller scale, where each sample has a corresponding label \(y_{j}\) for each downstream task.

### Problem Definition

Prior multimodal SSL practices require large-scale, fully synchronized multimodal sets \(\mathcal{X}^{s}\) to learn joint multimodal representations that perform well in downstream tasks. However, these approaches often overlook two challenges: (i) _Insufficient multimodal data_: When \(|\mathcal{X}^{s}|\) is small, existing methods struggle to learn effective joint representations, and (ii) _Unutilized unimodal data_. The abundance of available unimodal data is often neglected. In IoT applications, the scale of synchronized multimodal sets can be significantly limited due to signal heterogeneities, temporal misalignment, or domain variances, which result in incomplete modalities. This results in more available unimodal data than synchronized multimodal data (\(|X_{i}|\geq|X_{i}^{s}|\)). However, this abundant unimodal data is excluded from existing multimodal SSL pretraining techniques. To better utilize unimodal data, our problem falls under the SSL setting with unimodal pretrained models and limited multimodal pairs, which consists of two stages:

_Independent Unimodal Pretraining:_ For each independent modality data \(X_{i}\), we train a corresponding unimodal encoder \(E_{i}\). The goal is to learn a _holistic unimodal representation_ that maximizes downstream unimodal performance after finetuning. Since modality sets \(X_{i}\) are independent, this pretraining is not constrained by the number of synchronized pairs and can, therefore, fully leverage the abundant unimodal data.

_Efficient Cross-Modal Alignment:_ Given a set of synchronized modalities data \(\mathcal{X}^{s}\) of \(M^{s}\subseteq M\) modalities, we aim to align the pretrained encoders efficiently. This alignment projects unimodal representations into joint representations that maximize the downstream multimodal performance after finetuning. The scale of the multimodal alignment should be significantly smaller than the unimodal pretraining \(|X_{i}^{s}|\ll|X_{i}|\). In contrast to prior multimodal SSL works focusing on learning robust joint representations on large-scale multimodal data, this work aims to improve the _data efficiency_ of learning robust joint representations given only limited multimodal pairs.

### Factorization & Distributional Alignment

This section analyzes multimodal representation factorization in the information space and demonstrates how it enables distribution-level alignment of unimodal representations.

#### 2.3.1. Connection between Factorization and Cross-modal Alignment

In aligning multimodal representations, prior approaches often rely on contrastive learning to minimize the _modality gap_(Zhou et al., 2017) by pulling representations of different modalities from the same sample closer together while pushing representations from different samples further apart. However, due to the inherent heterogeneity, each modality contains unique, modality-specific information, and enforcing perfect alignment across modalities could potentially hurt the performance in multimodal downstream tasks (Zhou et al., 2017). To address these challenges, recent works (Zhou et al., 2017; Zhou et al., 2017; Zhou et al., 2017) have proposed factorizing modality representations into shared and private subspaces. It preserves both common and modality-specific information and allows for the alignment of shared representations while maintaining independent private representations for downstream tasks. However, these works operate on _instance-level alignment_, and it remains unclear whether this is sufficient when only limited modality pairs are available for learning. The scarcity of paired samples introduces the risk of biased sampling, potentially misleading the alignment process. With this in mind, we analyze a different approach that factorizes the representation in the information space and enforces _distribution-level_ alignment to capture a more comprehensive correlation between modalities by _emphasizing their information content rather than just their geometric proximity_. The intuition behind this is that instead of individual sample pairs, we aim to align modalities by the global structure (as shown in Figure 2). When the multimodal pairs are scarce, the distributional alignment aims to be _resilient to sampling biases_ and capture meaningful cross-modal relationships.

#### 2.3.2. Distributional Alignment through Information-theory based Factorization

We now formally define the factorization problem in the information space. Without loss of generality, we state the definitions for two modalities, \(\mathcal{X}=\{X_{1},X_{2}\}\), but they can be generalized to more modalities.

First, we are interested in constructing a compact random variable \(U\) (shared representation) that can perform any task that can be achieved using \(X_{1}\) separately and \(X_{2}\) separately. Formally, we define a sufficient common variable as follows.

**Definition 2.1**.: (Sufficient Common Variable) \(U\) is defined as the sufficient common variable between \(X_{1},X_{2}\) if and only if \(U=s_{1}(X_{1})=s_{2}(X_{2})\) for some \(s_{1},s_{2}\), and

\[(\forall f_{1},f_{2})\Big{(}[f_{1}(X_{1})=f_{2}(X_{2})]\implies[(\exists f)f(U) =f_{1}(X_{1})=f_{2}(X_{2})]\Big{)}, \tag{1}\]

namely, any common (shared) function between \(X_{1},X_{2}\) can be computed using \(U\). Building on the sufficient common variable, we define the shared representation to be the most compact form of \(U\) with the minimized entropy to ensure that \(U\) captures only the essential shared features across modalities.

**Definition 2.2**.: (Shared Representation) We refer to a sufficient common variable \(U\) with minimal entropy \(H(U)\) as the shared representation.

However, it is not clear how to find a sufficient common variable or a shared representation. We show that an approximation of the shared representation can be obtained by solving the following optimization problem, and later in Section 3, we propose the differentiable loss objectives with proof provided in Appendix A.

\[\begin{split}\text{minimize}& H(U)\quad\text{s.t.} \ X_{1}\perp\!\!\!\perp X_{2}\mid U,\\ &\text{and}\ (\exists s_{1},s_{2})\ U=s_{1}(X_{1})=s_{2}(X_{2})\end{split} \tag{2}\]

The conditional independence in Equation 2 enforces a form of distributional alignment, ensuring that given the shared representation \(U\) is the most compact aligned representation such that \(X_{1},X_{2}\) provide no additional information about each other.

Moreover, we define the private representations \(V_{1},V_{2}\) between \(X_{1},X_{2}\) as follows.

**Definition 2.3**.: (Private Representation) \(V_{1},V_{2}\) is the private representation of \(X_{1},X_{2}\) if they have minimal entropy among the random variables satisfying: \(V_{1}=p_{1}(X_{1}),V_{2}=p_{2}(X_{2})\) for some \(p_{1},p_{2}\) and there exist functions \(g_{1},g_{2}\) such that \(X_{1}=g_{1}(V_{1},U),X_{2}=g_{2}(V_{2},U)\), where \(U\) is the shared representation.

Similarly, we look for approximate representations. In particular, we replace equalities with a distance constraint \(d\), and independence is replaced by small mutual information. In Section 3, we discuss the detailed implementation of a differentiable loss function to find the approximate representations.

## 3. InfoMAE

This section introduces InfoMAE, a novel cross-modal alignment framework that efficiently aligns unimodal representations at the distribution and instance levels. We provide a detailed overview of InfoMAE's cross-modal alignment module in Figure 3.

Figure 2. An illustration of instance-level vs. distribution-level Cross-Modal Alignment

### Unimodal Pretraining

Unlike standard multimodal SSL that pretrains on synchronized multimodal pairs, we first initiate _unimodal pretraining_ on large-scale unsynchronized unimodal data. In the first stage, we pretrain each encoder \(E_{i}\) independently on unimodal data \(X_{i}\) with MAE, which applies mask reconstruction defined as the following for each modality \(i\in M\):

\[\mathcal{L}_{i}^{\text{unimodal}}=||\hat{X_{i}}-X_{i}||^{2}\mid\hat{X_{i}}=D_{i }(E_{i}(X_{i})). \tag{3}\]

The pretrained unimodal encoders \(E_{i}\) extract a generalized representation for each modality \(M_{i}\). However, they do not guarantee information compatibility between modalities when used together in the downstream tasks. In the following sections, we present InfoMAE's different components (as illustrated in Figure 4) to calibrate the encoders to _explicitly align_ the modalities in both the distribution-level and instance-level with only a limited amount of multimodal pair \(\mathcal{X}^{s}\).

### Distribution-level Alignment

We begin with the differentiable objective function that we optimize to obtain the (approximate) shared (\(U\)) and private representations (\(V\)) defined in Section 2.3.2. To extract \(U\) that is a function of both \(X_{1},X_{2}\), we equivalently extract \(U_{1}=F_{1}^{\text{shared}}(E_{1}(X_{1})),U_{2}=F_{2}^{\text{shared}}(E_{2}(X_ {2}))\), where \(F_{1},F_{2}\) are 2-layer MLP projectors that maps the general representation into shared and private representations, and enforce a constraint that \(U_{1}=U_{2}\). Similarly, we extract \(V_{1}=F_{1}^{\text{private}}(E_{1}(X_{1})),V_{2}=F_{2}^{\text{private}}(E_{2}( X_{2}))\). We use \(\mathcal{U}=\{U_{1},U_{2}\}\) and \(\mathcal{V}=\{V_{1},V_{2}\}\) for the extracted shared and private representations, respectively.

#### 3.2.1. Shared Representation

As described in Section 2, we aim to find the shared representation \(U\) that solves the optimization problem in Definition (2.2). However, due to the difficulty of the optimization problem 2 and the possibility that a shared representation does not exist, we instead approximate the shared representation by minimizing the following objective

Footnote 2: The optimization problem in Definition (2.2) is non-convex with a possibly infinite number of variables.

\[\begin{split}\mathcal{L}_{\text{info}}^{\text{shared}}=& ad(U_{1},U_{2})+\beta(H(U_{1})+H(U_{2}))\\ &+I(X_{1};X_{2}\mid U_{1})+I(X_{1};X_{2}\mid U_{2}),\end{split} \tag{4}\]

where \(a\) and \(\beta\) are the hyperparameters controlling the weight of each term, and \(d(\cdot)\) is a distance measure. The first two terms in the loss function aim to find \(U_{1}=U_{2}\) with minimal entropy, while the last two terms aim to impose conditional independence of \(X_{1},X_{2}\) given \(U_{1}\) or \(U_{2}\). We would like to note that the entropy and conditional mutual information listed in Eq. (4) are not easy to compute or differentiate. To alleviate this, we reduce these terms into probabilistic density functions below:

\[\begin{split}\mathcal{L}_{\text{info}}^{\text{shared}}=& \alpha d(U_{1},U_{2})+\sum_{l=1}^{2}\mathbb{E}_{X_{1},X_{2},U_{l}}\left[\log \frac{p\mathbb{X}_{X_{1},X_{2},U_{l}}}{p\mathbb{X}_{l}p\mathbb{X}_{l}p\mathbb{ X}_{l}}\right.\\ &\left.+(1-\beta)\log\frac{p\mathbb{X}_{X_{1},U_{l}}}{p\mathbb{X }_{l}p\mathbb{X}_{l}}+\log\frac{p\mathbb{X}_{X_{1},U_{l}}}{p\mathbb{X}_{l}-p \mathbb{X}_{l}}\right].\end{split} \tag{5}\]

Due to the space limit, we leave the detailed proof and discussion in Appendix A. To further enhance the differentiability of Eq. (5) by avoiding directly computing the probabilistic density ((_e.g._, \(\log\frac{p\mathbb{X}_{X_{1},U_{1}}}{p\mathbb{X}_{l}p\mathbb{X}_{l}p\mathbb{ X}_{l}p\mathbb{X}_{l}p\mathbb{X}_{l}p\mathbb{X}_{l}p\mathbb{X}_{l}p\mathbb{X}_{l}p\mathbb{X}_{l}p\mathbb{X}_{l}}\)), we follow [(31; 50; 60)] and utilize the _density-ratio trick_ to train a discriminator \(\mathcal{R}\), which given \(X_{1},X_{2},U\), outputs

Figure 4. Key learning objectives of InfoMAE Cross-Modal Alignment.

Figure 3. Overview of InfoMAEâ€™s alignment in the information space. InfoMAE adopts an information theory-inspired objective to align the factorized representations. Best viewed in color:

[MISSING_PAGE_FAIL:5]

#### 4.1.2. Datasets

Our experiments focus on two real-world applications: Moving Object Detection (MOD) and Human Activity Recognition (HAR). The MOD application contains vibration-based datasets using seismic and acoustic sensors. The HAR application consists of publicly released IMU sensor datasets (accelerometer, gyroscope, and magnetometer) collected from many human subjects performing various daily activities. To evaluate cross-modal alignment, we simulate a practical scenario where the pretrained domains differ significantly to reflect the diverse signals across different IoT application domains. Under this setting, we have unsynchronized unimodal data from different domains: MOD consists of data from three separately collected domains (M, G, T), each with different targets, terrains, and environmental conditions. HAR consists of data from two publicly released datasets (RealWorld-HAR [62] and PAMAP2 [57]). We pretrain unimodal encoders with only the unimodal data from different domains and then use a limited amount of synchronized multimodal pairs for cross-modal alignment and downstream finetuning. For joint pretraining, we pretrain on the massive available synchronized multimodal pairs. We summarize the data used in Table 1 and describe these applications and domains in more detail in Appendix B.

#### 4.1.3. Baselines

We extensively evaluate InfoMAE with different SSL baselines including unimodal contrastive (SimCLR[7], MoCo[8]), multimodal (CMC[63], GMC[55], FOCAL [40]) contrastive, temporal contrastive (TNC[64], TSTCC[15]), and MAE based frameworks (MAE[24], CAV-MAE[18]). We describe these baselines in more detail in Appendix F.

### Cross-Modal Alignment Evaluation

#### 4.2.1. Cross-Modal Alignment on MOD

We evaluate InfoMAE against prior CL works [7, 15, 40, 56, 63, 64] on cross-modal alignment with various combinations of unimodal models pretrained with different domains. We align the encoders with a small scale of multimodal pairs (5% of the unimodal data scale) and an even smaller subset of labeled multimodal pairs from domain M for finetuning. This application involves two modalities (seismic and acoustic). Therefore we represent the domains of the unimodal representations with two letters (_e.g._, \(T_{\text{Self}}||G_{\text{Aco}}\) means aligning seismic encoder pretrained on domain T and acoustic encoder pretrained on domain G).

In addition to the prior CL baselines, we also show the performance for direct concatenation of the pretrained unimodal representations without any alignment and for Joint Multimodal Pretraining on the same amount of synchronized multimodal pairs. We present the accuracy and F1-score after finetuning in Table 2, InfoMAE consistently outperforms the unimodal concatenation by a significant margin since direct concatenation fails to exploit cross-modal correspondence. CMC and other unimodal SSL frameworks even have negative impacts compared to direct concatenation, indicating that unimodal objectives or simply aligning the multimodal representations without considering the modality discrepancy could hurt the downstream performance. InfoMAE also achieves better results than FOCAL and GMC, underscoring the benefits of enforcing distribution-level alignment over instance-level alignment in downstream tasks with limited multimodal data. When the same amount of multimodal data is used for Joint Multimodal Pretraining, the significant gap between the aligned unimodal models and the joint pretrained multimodal model suggests the feasibility of transferring pretrained unimodal representations to multimodal representations with only limited (5%) synchronized multimodal data. It is noteworthy that some domain combinations ( e.g., GT, TT, TG) do not even overlap with data from the alignment and finetuning set (MOD).

#### 4.2.2. Cross-Modal Alignment on HAR

Besides MOD application, we also evaluate InfoMAE on HAR applications. In contrast to MOD evaluation, which aligns unimodal encoders pretrained on different domains, we analyze how additional unsynchronized data from the same domains could assist the downstream performance given the limited number of multimodal pairs. Here, we independently pretrain all unimodal encoders on unsynchronized IMU data from either PAMAP2, RealWorld-HAR, or Combined, which is the concatenation of the former two. Then, we use a small portion of the synchronized multimodal data pairs from PAMAP2 for cross-modal alignment and downstream finetuning. We present the results in Table 4. InfoMAE consistently achieves the best performance, with an average of 4.09% and 5.16% improvements in accuracy and the

\begin{table}
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|c} Framework & \multicolumn{3}{c|}{Aligned Domains} & \multicolumn{3}{c|}{\(T_{\text{Self}}\parallel M_{\text{Aco}}\)} & \multicolumn{3}{c|}{\(G_{\text{Self}}\parallel T_{\text{Aco}}\)} & \multicolumn{3}{c|}{\(T_{\text{Self}}\parallel T_{\text{Aco}}\)} & \multicolumn{3}{c|}{\(G_{\text{Self}}\parallel M_{\text{Aco}}\)} & \multicolumn{3}{c|}{\(T_{\text{Self}}\parallel G_{\text{Aco}}\)} \\ \cline{2-13}  & Joint & Modal & & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 \\
847 & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} & \multicolumn{3}{c|}{} \\ \hline Unimodal Concat & âœ— & âœ— & 0.6731 & 0.6699 & 0.5392 & 0.5281 & 0.4454 & 0.4366 & 0.7247 & 0.7217 & 0.6584 & 0.6543 & 0.6648 & 0.6543 & 0.666 \\ \hline CMC [63] & âœ— & âœ“ & 0.6792 & 0.6702 & 0.4313 & 0.4356 & 0.4173 & 0.4032 & 0.6919 & 0.6877 & 0.6497 & 0.6335 & 0.6479 & 0.6335 & 0.647 \\ FOCAL [40] & âœ— & âœ“ & 0.7462 & 0.7432 & 0.6249 & 0.6249 & 0.5613 & 0.5579 & 0.7549 & 0.7527 & 0.7194 & 0.7160 & 0.649 \\ GMC [55] & âœ— & âœ“ & 0.7354 & 0.7317 & 0.6591 & 0.6523 & 0.4756 & 0.4720 & 0.8044 & 0.8053 & 0.7247 & 0.7211 & 0.649 \\ SimCLR [6] & âœ— & âœ“ & 0.3061 & 0.2742 & 0.2873 & 0.2609 & 0.2974 & 0.2758 & 0.2981 & 0.2698 & 0.2800 & 0.2308 & 0.2638 & 0.

F1-score compared to the best-performing baseline, FOCAL. The improvement is most significant in aligning unimodal encoders pretrained on RealWorld-HAR, which completely differs from the alignment set (PAMAP2). This further demonstrates InfoMAE's robustness as an alignment framework with a limited amount of multimodal pairs, reflecting its superior ability to utilize the unimodal data better even when they are from different domains.

### Unimodal Evaluation with Cross-Modal Alignment

We analyze how incorporating the multimodal correspondences into each unimodal encoder after alignment could benefit the downstream tasks. Figure 5 shows the accuracy for seismic and acoustic modalities before and after cross-modal alignment in the MOD application. With limited multimodal pairs, the pretrained unimodal encoders could gain the most significant performance improvements with InfoMAE. This emphasizes the InfoMAE's superior efficiency in enforcing cross-modal correspondence to each modality to improve their downstream performance, with only a few multimodal pairs required. With InfoMAE, the aligned unimodal model can generate the most holistic representations through distributional alignment compared to geometric alignment (CMC, FOCAL).

### Multimodal Pairing Efficiency

We also evaluate InfoMAE's alignment performance at varying amounts of multimodal data for MOD application in Table 3. We align both encoders pretrained from domain M (MM) and compare them to standard joint pretraining with different ratio of multimodal data. Additionally, we provide supervised training results on the same amount of labeled multimodal data used for finetuning. InfoMAE consistently achieves superior multimodal data efficiency, with minimal degradation as we reduce the number of multimodal data. In general, InfoMAE has an average of 3.42% gain over the highest-performing baselines and over 60% compared to joint model pretraining, which performs poorly in the absence of multimodal data. Note that the joint pretraining even performs worse than the supervised approach with only 5% of multimodal data, indicating the standard self-supervised pretraining fails to learn effective representations with an insufficient amount of synchronized multimodal data. In contrast, the two-stage learning paradigm of InfoMAE leveraging widely available unsynchronized unimodal data could effectively mitigate this problem.

### Standard Multimodal Pretraining on Large-scale Synchronized Dataset

While InfoMAE excels as an efficient cross-modal alignment framework under limited pairs, it also demonstrates remarkable flexibility as a standard multimodal SSL framework. We evaluate InfoMAE

\begin{table}
\begin{tabular}{c|c c|c c|c c} \hline \hline \multirow{2}{*}{Multimodal Data} & \multicolumn{2}{c|}{Supervised} & Joint Pretrain & CMC & GMC & FOCAL & **InfoMAE** \\ \cline{2-9}  & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 \\ \hline
5\% & & & 0.3329 & 0.3039 & 0.7087 & 0.6989 & 0.8614 & 0.8616 & 0.8694 & 0.8668 & **0.8828** & **0.8808** \\
15\% & & & 0.6142 & 0.6104 & 0.8111 & 0.8062 & 0.8781 & 0.8753 & 0.8727 & **0.9049** & **0.9028** & **7.681** \\
25\% & & & 0.7071 & 0.7938 & 0.8433 & 0.8372 & 0.8774 & 0.8758 & 0.8848 & 0.8831 & **0.9290** & **0.9270** \\
50\% & & & 0.8942 & 0.8920 & 0.8754 & 0.8724 & 0.8948 & 0.8938 & 0.9009 & 0.8994 & **0.9377** & **0.9367** \\ \hline \hline \end{tabular}
\end{table}
Table 3. Alignment performance (MM) with different multimodal pair ratios from MOD.

\begin{table}
\begin{tabular}{c|c c|c c|c c} \hline \hline \multirow{2}{*}{\begin{tabular}{c} Unimodal \\ Pretrain Domain \\ \end{tabular} } & \multirow{2}{*}{Combined} & \multirow{2}{*}{PAMAP2} & \multirow{2}{*}{RealWorld-HAR} \\ \cline{1-1} \cline{5-7} \multirow{2}{*}{
\begin{tabular}{c} Multimodal \\ Alignment Domain \\ \end{tabular} } & & & & & & \\ \hline Metric & Acc & F1 & Acc & F1 & Acc & F1 \\ \hline Concat & 0.7843 & 0.7000 & 0.7763 & 0.6210 & 0.5675 & 0.4187 \\ CMC & 0.7334 & 0.6508 & 0.7285 & 0.6788 & 0.7010 & 0.5956 \\ FOCAL & 0.7922 & 0.7129 & 0.7354 & 0.6327 & 0.7643 & 0.6243 \\ GMC & 0.7314 & 0.5915 & 0.7344 & 0.5869 & 0.7414 & 0.5816 \\ SimCLR & 0.7299 & 0.6190 & 0.7075 & 0.5426 & 0.7225 & 0.5581 \\ TNC & 0.5431 & 0.4080 & 0.5889 & 0.4824 & 0.6378 & 0.5167 \\ TSTCC & 0.7299 & 0.6003 & 0.7065 & 0.5773 & 0.7354 & 0.5864 \\ \hline
**InfoMAE** & **0.8261** & **0.7303** & **0.8117** & **0.7175** & **0.7912** & **0.6901** \\ \hline \hline \end{tabular}
\end{table}
Table 4. Linear Probing performance of HAR on PAMAP2 by aligning pretrained unimodal encoders.

Figure 5. Unimodal linear probing accuracy of MOD with and without cross-modal alignment.

\begin{table}
\begin{tabular}{c|c c|c c} \hline \hline \multirow{2}{*}{Domain} & \multicolumn{2}{c|}{**Domain G**} & \multicolumn{2}{c}{**Domain T**} \\ \hline Frameworks & Acc & F1 & Acc & F1 \\ \hline CMC [(8)] & 0.7924 & 0.7897 & 0.6791 & 0.6776 & 770 \\ FOCAL [(40)] & 0.9137 & 0.9111 & 0.8156 & 0.8130 & 771 \\ GMC [(55)] & 0.7986 & 0.7947 & 0.3457 & 0.3387 & 772 \\ MoCo [(8)] & 0.8719 & 0.8688 & 0.7500 & 0.7483 & 773 \\ SimCLR [(6)] & 0.8418 & 0.8386 & 0.7288 & 0.7207 & 773 \\ TNC [(64)] & 0.6916 & 0.6797 & 0.5680 & 0.5625 & 778 \\ TSTCC [(15)] & 0.7080 & 0.7004 & 0.5804 & 0.5766 & 777 \\ MAE [(24)] & 0.6708 & 0.6642 & 0.4421 & 0.4365 & 778 \\ CAV-MAE [(18)] & 0.5507 & 0.5282 & 0.3457 & 0.3387 & 779 \\ \hline
**InfoMAE** & **0.9196** & **0.9186** & **0.8546** & **0.8535** & 780 \\ \hline \hline \end{tabular}
\end{table}
Table 5. Performance of Joint Pretraining on MOD (seismic and acoustic) dataset and then finetuned on unseen domains.

against prior state-of-the-art works on Joint Multimodal Pretraining using abundant multimodal pairs, as shown in Table 5. We use synchronized, unlabeled multimodal data from the MOD dataset to pretrain backbone encoders. Then we freeze the pretrained encoders and perform linear probing using labeled multimodal data from domains \(G\) and \(T\), as described in Section 4.1. InfoMAE consistently outperforms the MAE-based framework and achieves better performance than other contrastive baselines. We leave more evaluation on Joint Multimodal Pretraining across four real-world datasets to Appendix G. Prior works, primarily designed for joint multimodal pretraining, often struggle with limited multimodal pairs and show significant performance degradation. In contrast, InfoMAE not only improves multimodal pairing efficiency but maintains high performance with minimal performance degradation.

### Ablation Studies

Finally, we study how each module of InfoMAE contributes to its performance through ablation studies. We evaluate four variants of InfoMAE by removing temporal, shared, private, and augmentation components in Table 6. The absence of either shared or private components leads to a significant degradation, implying the significance of factorized representation for cross-modal alignment. The drop in performance after removing temporal locality constraints also indicates the importance of learning temporal correspondence for time-series signals. Without temporal locality, the learned representations lose crucial temporal correspondence and can significantly compromise the ability to learn multimodal correspondences on top of the unimodal representations. Conversely, InfoMAE without augmentations does not significantly reduce the performance, demonstrating its robustness toward augmentation choices, in contrast to many contrastive learning frameworks that require careful selection of augmentations to avoid representational collapses.

## 5. Related Works

**Self-Supervised Multimodal Learning.** Self-supervised learning (SSL) techniques, such as Contrastive Learning (CL) and masked reconstructions, have achieved significant success in visual, textual, and time-series representation learning (Chen et al., 2017; Chen et al., 2018; Chen et al., 2019; Chen et al., 2019; Chen et al., 2020; Chen et al., 2021; Chen et al., 2020; Chen et al., 2021). Masked reconstruction learns informative representations by reconstructing masked inputs (Chen et al., 2019; Chen et al., 2019; Chen et al., 2020; Chen et al., 2021), with various masking strategies explored (Chen et al., 2019; Chen et al., 2020; Chen et al., 2021), and extended to time-frequency spectrograms (Chen et al., 2020; Chen et al., 2021) and videos (Chen et al., 2020; Chen et al., 2021). Multimodal representation learning has become increasingly important with diverse applications (Chen et al., 2019; Chen et al., 2021; Chen et al., 2021; Chen et al., 2021). Recent works leverage CL to learn correspondences between modalities (Chen et al., 2021; Chen et al., 2021; Chen et al., 2021; Chen et al., 2021; Chen et al., 2021), and others pretrain unified encoders for multimodal representations (Chen et al., 2021; Chen et al., 2021). Factorized Multimodal Learning (Chen et al., 2021; Chen et al., 2021; Chen et al., 2021) further decouples multimodal learning by acknowledging both modality-specific and modality-shared information. FOCCAL (Chen et al., 2021) proposed contrastive learning objectives to learn shared and private representation in the orthogonal space. FactorizedCL (Chen et al., 2021) separates the shared and private space based on their relevance to the downstream tasks. Some works (Chen et al., 2021; Chen et al., 2021) combine CL with MAE to capture cross-modal correspondence. Yet, these works minimize the geometric modality gap to learn cross-modal correspondences and rely on massive amounts of multimodal data for joint multimodal pretraining. In contrast, InfoMAE minimizes the information modality gap to further enhance the downstream performance. In reducing multimodal data pairs for training, many works (Chen et al., 2021; Chen et al., 2021; Chen et al., 2021) propose to impute missing modality pairs through feature generations. Wang _et al._(Wang et al., 2021) proposes using CL to align multimodal encoders through an anchor modality yet still overlooking unimodal data. In contrast, InfoMAE minimizes the reliance on multimodal data by taking advantage of a large amount of unimodal data.

**Multimodal Information Theory.** There has been a long history of exploring common information between random variables in information theory (Chen et al., 2021; Chen et al., 2021; Chen et al., 2021), and it is still an active research field (Chen et al., 2021; Chen et al., 2021; Chen et al., 2021; Chen et al., 2021). However, it remains challenging to compute the common information in practical applications. Kleinmanl _et al._(Kleinmanl et al., 2021) combines Variational Autoencoders with Gacs-Korner Common Information. Mai _et al._(Mai et al., 2021) proposes to measure the information redundancy for multimodal data. However, they do not explicitly consider the unique information for factorization. InfoMAE adopts the informational factorization considering both private and shared information to construct a joint representation in a task-agnostic manner rather than extracting task-related information like (Chen et al., 2021).

## 6. Discussion & Conclusion

We proposed InfoMAE, a pairing-efficient multi-stage SSL paradigm for multimodal IoT sensing. It first pretrains independent modality encoders on large-scale unimodal data sets. Then, it leverages a novel information theory-based optimization to achieve distributional cross-modal alignment with only limited multimodal pairs. Extensive evaluations compared to standard multimodal SSL frameworks demonstrated the superior efficiency and effectiveness of InfoMAE across multiple real-world IoT applications. We believe it opens new opportunities for developing more data-efficient and qualitative self-supervised multimodal models. In the Appendix, we provide more implementation details and evaluations.

\begin{table}
\begin{tabular}{c|c c|c c|c c|c c|c c} \hline \hline \multirow{2}{*}{Frameworks} & \multicolumn{2}{c|}{TM} & \multicolumn{2}{c|}{GT} & \multicolumn{2}{c|}{TT} & \multicolumn{2}{c|}{GM} & \multicolumn{2}{c}{TG} \\ \cline{2-11}  & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 \\ \hline noTemp & 0.6946 & 0.6902 & 0.5881 & 0.5884 & 0.5044 & 0.4888 & 0.7435 & 0.7432 & 0.6651 & 0.6570 \\ noShared & 0.7683 & 0.7595 & 0.6504 & 0.6515 & 0.5298 & 0.5232 & 0.8125 & 0.8116 & 0.7395 & 0.7351 \\ noPrivate & 0.5479 & 0.4732 & 0.4180 & 0.3402 & 0.2873 & 0.1812 & 0.6259 & 0.5519 & 0.5399 & 0.5519 \\ noAug & 0.7863 & 0.7823 & 0.6973 & 0.6967 & 0.5881 & 0.5868 & 0.2332 & 0.8252 & 0.7924 & 0.7879 \\ \hline
**InfoMAE** & **0.7950** & **0.7929** & **0.6986** & **0.7007** & **0.5928** & **0.5908** & **0.8326** & **0.8324** & **0.8326** & **0.8324** \\ \hline \hline \end{tabular}
\end{table}
Table 6. Ablation Results of InfoMAE Cross-Modal Alignment.

## References

* (1)
* (2) Human Alwassel, Dhruv Mahajan, Bruno Korbar, Lorenzo Torresani, Bernard Ghanem, and D Tran. Self-supervised learning by cross-modal audio-video clustering. _Advances in Neural Information Processing Systems_, 33:9758-9770, 2020.
* (3) Wiele Gedara Charminda Bandara, Naman Patel, Ali Gholami, Mehdi Nikkhah, Motil Agrawal, and Vishal M Patel. Adame: Adaptive masking for efficient spatiotemporal learning with masked autoencoders. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 14507-14517, 2023.
* (4) Danny Oade Boa et al. Eye-based emotion recognition. _The Influence of Visual and Auditory Stimuli_, 5(6):1-7, 2006.
* (5) Tom Brown, Benjamin Mann, Nick Ryder, Melane Sukhbahi, Jared D Kaplan, Prafulla Dhariwal, Arvish Nealalainen, Franz Wayan, Girish Sastry, Amanda Askell, Samhhi Agarwal, Ariel Herbert-Voss, Greeden Kuegger, Tom Tenighan, Rewon Child, Aditya Kamas, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eugenie Matzus Liuen, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Darba Arnold. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and R. H. Lin, editors, _Advances in Neural Information Processing Systems_, volume 33, pages 1877-1901. Curran Associates, Inc., 2020.
* (6) Mathilde Caron, Hugo Tuorvo, Ikhan Mira, Trevor Egyoun, Julien Murali, Piotr Bignonwiki, and Armand Joulin. Emerging properties in self-supervised vision transformers. In _Proceedings of the IEEE/CVF international conference on computer vision_, pages 9650-9660, 2021.
* (7) Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _International Conference on Machine Learning (ICML)_, 2020.
* (8) Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In _International Conference on Machine Learning (ICML)_, 2020.
* (9) Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision Transformers. In _IEEE/CVF International Conference on Computer Vision (CVPR)_, 2021.
* (10) Cling-Taho Chang, R Devon Hjola, Xin Wang, Vibhu Vineet, Neel Joshi, Antonio Torralba, Stefanie Jegelink, and Yale Song. Robust contrastive learning against noisy views. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 16670-16681, 2022.
* (11) Thomas M Cover. _Elements of information theory_. John Wiley & Sons, 1999.
* (12) Shobheh Deldari, Hue Xue, Angj Saeed, Daniel V Smith, and Flora D S. Salim. Coors: Cross modality contrastive learning for sensor data. _Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies_, 6(3):1-28, 2022.
* (13) Joseph DuPfeuch, Chao Liu, Yiyue Luo, Michael Foshey, Yunxha Li, Antonio Torralba, Wojciech Matuski, and Daniela Rus. Actionenzene: A multimodal dataset and recording framework for human activities using wearable sensors in a kitchen environment. _Advances in Neural Information Processing Systems_, 35:13800-13813, 2022.
* (14) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.04845_, 2018.
* (15) Aleyev Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dive Weissenborn, Michael Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 166 trade word: Transformers for image recognition at scale. In _International Conference on Learning Representations_, 2020.
* (16) Emadokefiec Hidec, Mohamed Ragh, Zarephua Chen, Min Wu, Chee Keong Kwoh, Xiaoli Li, and Cuntai Guan. Time-series representation learning via temporal and contextual contrasting. In _Thirtieth International Joint Conference on Artificial Intelligence (IJCAI)_, 2021.
* (17) Emadokefiec Eblee, Mohamed Ragh, Zhenghua Chen, Min Wu, Chee Keong Kwoh, Xiaoli Li, and Cuntai Guan. Self-supervised contrastive representation learning for semi-supervised time-series classification. _arXiv preprint arXiv:2208.08616_, 2022.
* (18) Piotr Gacs and J. Korner. Common information is far less than mutual information. _Problems of Control and Information Theory_, 2, 01 1973.
* (19) Yuan Gong, Andrew Roudfachoelhache, Alexander H Liu, David Harvath, Leonid Karlinos, Hildec Kuehne, and James G. Glass. Contrastive audio-visual masked autoencoders. In _The Eleventh International Conference on Learning Representations_, 2022.
* (20) Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallee, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Aviis Perras, Zhaohan Guo, Mohammad Cheishali Ray, et al. Bootstrap your own latent-a new approach to self-supervised learning. _Advances in neural information processing systems_, 33:2217-2218, 2020.
* (21) Agrim Gupta, Jiajun Wu, Jao Deng, and Fei-Fei Li. Siamese masked autoencoders. _Advances in Neural Information Processing Systems_, 36, 2024.
* (22) Osama A Hanna, Xinlin Li, Suhas Diggavi, and Christina Fragouli. Common information dimension. In _2023 IEEE International Symposium on Information Theory (ISIT)_, pages 406-411. IEEE, 2023.
* (23) Oanna A Hanna, Xinlin Li, Suhas Diggavi, and Christina Fragouli. On the relation between the common information dimension and upper common information. In _2024 IEEE International Symposium on Information Theory (ISIT)_, IEEE, 2024.
* (24) Oanna A Hanna, Xinlin Li, Christina Fragouli, and Suhas Diggavi. One weak the dependency in distributed detection. In _2022 IEEE International Symposium on Information Theory (ISIT)_, pages 2720-2725 IEEE, 2022.
* (25) Kaiming He, Xinlei Chen, Saining Xie, Yang Li, Piotr Bollitz, and Ross Girshick. Masked autoencoders are scalable visual learners. In _IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_, 2022.
* (26) Wise-Wise Jiang and Glass. Disentangling by partitioning: A representation learning framework for multimodal sensory data. _arXiv preprint arXiv:1805.11264_, 2018.
* (27) Ronghang Hu and Ammoyeri Singh. Unit: Multimodal multitask learning with a unified transformer. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1439-1449, 2012.
* (28) Po-Yao Huang, Hu Xu, Juncheng Li, Alexei Baewalik, Michael Auli, Wojciech Galkina, Horan Mater, and Christoph Feichtenhofer. Masked autoencoders that listen. _Advances in Neural Information Processing Systems_, 35:28708-28720, 2022.
* (29) Zhenyu Huang, Guocheng Niu, Xiao Li, Wenkong Ding, Xinyan Xia, Hua Wu, and Siip Peng. Learning with noisy correspondence for cross-modal matching. _Advances in Neural Information Processing Systems_, 34:29406-29419, 2021.
* (30) Qian Jiang, Changyou Chen, Han Zhao, Liqun Chen, Qing Peng, Song Chuh Tran, Yi Xu, Belinda Zeng, and Trishul Chilmichi. Understanding and constructing latent mobility structures in multi-modal representation learning. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 7661-7671, 2020.
* (31) Danish Karra, Tomoyoshi Kimura, Liu Shengzhong, Li Jiuyang, Li Daongxin, Wang Tianshi, Wang Ruijie, Chen Yizhuo, Hu Yigeng, and Abdelzaher Tarek. Freegome: Frequency-aware masked autoencoder for multi-modal total sensing. In _The World Wide Web Conference_, 2024.
* (32) Huynink Kim and Andriy Mnih. Disentangling by factorising. In _International Conference on Machine Learning (ICML)_, 2018.
* (33) Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In _International Conference on Learning Representations (ICLR)_, 2015.
* (34) Michael Kleinman, Alessandro Achille, Stefano Soatto, and Jonathan Kao. Gauss-houcer common information variational autoencoder. _Advances in Neural Information Processing Systems_, 36, 2024.
* (35) Lingqing Kong, Martin Q Ma, Guangyi Chen, Eric P Xing, Yuejie Chi, Louish-Philippe Moreoey, and Kara Zhang. Understanding masked autoencoders via hierarchical latent variable models. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 798-7928, 2023.
* (36) Bruno Korbar, Du Tran, and Lorenzo Torresani. Cooperative learning of audio and video models from self-supervised synchronization. _Advances in Neural Information Processing Systems_, 31, 2018.
* (37) Jynyas Li, Yihuo Chen, Tomoyoshi Kimura, Tianshi Wang, Ruijie Wang, Denishara, Yizong Jin, Li Yu, Wali A Hanady, Abel Souza, et al. A desic: A content-centric platform for edge ai training and orchestration. In _2024 33rd International Conference on Computer Communications and Networks (ICCCN)_, pages 1-9. IEEE, 2024.
* (38) Paul P. Liang, Zhiluo Deng, Martin Q Ma, James Y Zou, Louis-Philippe Moreoey, and Rudus Salakhutdinov. Factorized contrastive learning: Going beyond multi-view redundancy. _Advances in Neural Information Processing Systems_, 36, 2023.
* (39) Paul P. Liang, Yiwei Lyu, Xiang Fan, Zefian Wu, Yun Cheng, Jason Wu, Leslie Chen, Peter Wu, Michel A Lee, Take Zhu, et al. Multibench: Multiscale benchmarks for multimodal representation learning.
* (40) Yetterin Weixin Zhang, Yorogu Kwon, Serena Yeung, and James Y. Mostafi. The gap: Understanding the modality gap in multi-modal contrastive representation learning. _Advances in Neural Information Processing Systems_, 35:17612-17625, 2022.
* (41) Shengzhong Liu, Tomoyoshi Kimura, Dongxin Liu, Ruijie Wang, Jinyang Li, Shus Diggavi, Mani Srivastava, and Tarek Abdelzaher. Focal: Contrastive learning for multimodal-free-sense sensing signals in federated orthogonal latent space. _Advances in Neural Information Processing Systems_, 36, 2023.
* (42) Zei-Ta, Yungiqi, Lin Yeo, Han Han, Yiwei Yu, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In _IEEE/CVF International Conference on Computer Vision (CVPR)_, 2021.
* (43) Jha Luchliov and Frank Hutter. SGD: Stochastic gradient descent with warm restarts. In _International Conference on Learning Representations (ICLR)_, 2017.
* (44) Jing Luchliov and Frank Hutter. Decoupled weight decay regularization. In _International Conference on Learning Representations (ICLR)_, 2019.
* (45) Mengmeng Ma, Jian Ren, Long Zhao, David Testigngie, and X Peng. Are multimodal transformers robust to training modality? In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 18177-18186, 2022.

* [45] Mengmeng Ma, Jian Ren, Long Zhao, Sergey Tulyakov, Cathy Wu, and Xi Peng. Snit: Multimodal learning with severely missing modality. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 35, pages 2302-2310, 2021.
* [46] Sijie Mai, Ying Zeng, and Haifeng Hu. Multimodal information bottleneck: Learning minimal sufficient unimodal and multimodal representations. _IEEE Transactions on Multimedia_, 2022.
* [47] David Mitarli, Roman Bachmann, Quzhan Kar, Terence Yeo, Mingfei Gao, Afifi-shi DiDehghan, and Amir Zamir. Gm: Massively multimodal masked modeling. _Advances in Neural Information Processing Systems_, 36, 2024.
* [48] Pedro Morgado, V Li, and Luan Nousconcelos. Learning representations from audio-visual spatial alignment. _Advances in Neural Information Processing Systems_, 34:3743-3744, 2020.
* [49] Pedro Morgado, Maria Maria, and Vascoencos. Robust audio-visual instance discrimination. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 1392-1395, 2021.
* [50] XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals and the likelihood ratio by convex risk minimization. _IEEE Transactions on Information Theory_, 56(11):5847-5861, 2010.
* [51] Daixie Niimuni, Dude Taketouch, Yasunori Oishi, Noboru Harada, and Kunio Kashino. Masked spectrogram modeling using mixed autoencoders for learning general-purpose audio representation. In Joseph Turian, Bjorn W. Scholler, Dorren Herrmann, Katina Kirchholt, Paolo Garcia Perera, and Philippe Ealing. Editors, _IEEE/ACM: Intelligent Evaluation of Audio Representations (NeurIPS 242 Conference)_, pages 1-24. Retrieved, volume 166 of _Proceedings of Machine Learning Research_, pages 1-24. PMLR, 13-14 Dec 2022.
* [52] Xiaomin Ouyang, Xin Shuai, Jiaya Zhou, Ivy Wang Shi, Zhiyuan Xie, Guoliang Xing, and Jianwei Huang. Cosme: Contrastive fusion learning with small data for multimodal human activity recognition. In _International Conference on Mobile Computing And Networking (MobiCom)_, 2022.
* [53] Robert J Piechocki, Xiaoyang Wang, and Mohammad J Bocus. Multimodal sensor fusion in the latent representation space. _Scientific Reports_, 13(1):2005, 2023.
* [54] Nicolas Pelekhovski, Elisabeth Wetter, Johnen Overestel, Jialao Dae, Carolinas Wabby, Joshim Labollah, and Natasq Salado. Comit: Contrastive multimodal image representation for registration. _Advances in neural information processing systems_, 33:18443-1844, 2020.
* [55] Petra Pokhakar, Miguel Vasco, Hang Yin, Francisco S Mela, Ana Paiva, and Danica Kragic. Geometric multimodal contrastive representation learning. In _International Conference on Machine Learning (ICML)_, 2022.
* [56] Alex Radford, Jong Wook Kim, Chris Halliday, Adayo Ramesh, Gabriel Goh, Sandhini Agarwal, Gmith Saaty, Amanda Ashieh, Paolo Calk, et al. Learning transferable visual models from natural language supervision. In _International Conference on Machine Learning (ICML)_, 2021.
* [57] Aritha Reiss and Inder Stricker. Introducing a new benchmarked dataset for activity monitoring. In _International Symposium on Wearable Computers (ISWC)_, 2012.
* [58] Philip Schmidt, Aritha Reiss, Robert Durichen, Claus Marberger, and Kristof Van Laerhoven. Introducing weap, a multimodal dataset for wearable stress and affect detection. In _ICML 2018_, pages 406-408. ACM, 2018.
* [59] Sheng Shen, Limin Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhenwei Luo, and Kurt Keutzer. How much can clip benefit revision and-language tasks? In _International Conference on Learning Representations_, 2021.
* [60] Masahi Sohyama, Taju Suzuki, and Tadifumi Kamaroni. Density ratio matching under the bayesian approach: a unified framework of density-ratio estimation. _Annals of the Institute of Statistical Mathematics_, 6:1009-104, 2012.
* [61] Ericken Sala and Michael Caspar. The gray-yware network and wyner's common information for gaussian sources. _IEEE Transactions on Information Theory_, 6(6):2369-1384, 2022.
* [62] Timo Styller and Heiner Stuckenschmidt. On-body localization of wearable devices: An investigation of position-aware activity recognition. In _IEEE International Conference on Pervasive Computing and Communications (PerCom)_, 2016.
* [63] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In _European Conference on Computer Vision (ECCV)_, 2020.
* [64] Sam Tonekohan, Danny Eytan, and Anna G G Gheberg. Unsupervised representation learning for image retrieval with temporal neighborhood coding. In _International Conference on Learning Representations (ICLR)_, 2021.
* [65] Zhan Tong, Yliong Song, Weng Wang, and Limin Wang. Videome: Masked autoencoders are data-efficient learners for self-supervised video pre-training. _Advances in neural information processing systems_, 35:10078-10093, 2022.
* [66] Lian Tran, Xiaoming Liu, Jiaya Zhou, and Jong Ina. Missing modalities imputation via cascaded residual autoencoder. In _Proceedings of the IEEE conference on computer vision and pattern recognition_, pages 1405-1414, 2017.
* [67] Daniel J Troiseth, Sigurd Lokse, Robert jensen, and Michael Kampffmeyer. Reconsidering representation alignment for multi-view clustering. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 1255-1265, 2021.
* [68] Yao-Tung Huber Tsai, Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency, and Ruani Sakhahdinov. Learning factorized multimodal representations. In _International Conference on Learning Representations_, 2018.
* [69] Gaurav Verma, Estaban Gunesh Dickaee, and Tanya Guha. Learning affective correspondence between music and image. In _ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_, pages 3975-3979. IEEE, 2019.
* [70] Hu Wang, Yuanhong Chen, Congbo Ma, Joide Avery, Louise Hull, and Gastavo Carneiro. Multi-modal learning with missing modality via shared-specific feature modeling. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_, pages 15878-1587, 2023.
* [71] Yuxing Wang, Jiapeng Wang, Bin Chen, Ziyun Zeng, and Shu-Tao Xia. Contrastive masked autoencoders for self-supervised video hashing. In _Proceedings of the AAAI Conference on Artificial Intelligence_, volume 37, pages 2733-2741, 2023.
* [72] Zhan Wang, Yang Zhao, Haifeng Huang, Jiageng Liu, Axiono Yin, Li Tang, Linyuan Li, Yongqi Wang, Zhang, and Zhao Zhao. Connecting multi-modal contrastive representations. _Advances in Neural Information Processing Systems_, 36:2209-2214, 2023.
* [73] Aaron Wyner. The common information of two dependent random variables. _IEEE Transactions on Information Theory_, 12(2):163-179, 1975.
* [74] Zhendi Xie, Zheng Yao, Cao, Yutong Liu, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Smmmin: A simple framework for masked image modeling. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_, pages 963-963, 2022.
* [75] Yuxing Yang, Xin Liu, Jiang Wu, Skiu Borca, Dina Katabi, Ming-Zher Poh, and Daniel McDuff. Supernet: Simple self-supervised learning of periodic targets. In _International Conference on Learning Representations_, 2023.
* [76] Lei Yu, Vincent F?T'an et al. Common information, none stability, and their extensions. _Foundations and Trends(r) in Communications and Information Theory_, 19(2):109-329, 2022.
* [77] Zhihua Yue, Yihang Wang, Junyoung Duan, Tianmeng Tang, Congrui Huang, Yunhai Tong, and Rixiong Xu. Tsi: Towards universal representation of time series. In _AAAI Conference on Artificial Intelligence (AAAI)_, 2022.
* [78] Qiang Yiqi, Wei Wang, and Yisei Wang. How mask matters: Towards theoretical understanding of masked autoencoders. _Advances in Neural Information Processing Systems_, 35:22172-22173, 2022.
* [79] Xiang Zhang, Ziyuan Zhao, Theodores Tsiligbaridis, and Marina Zitnik. Self-supervised contrastive pre-training for time series via time-frequency consistency. In _Neural Information Processing Systems (NeurIPS)_, 2022.
* [80] Zhuo Zheng, Aliong Ma, Lingwei Zhang, and Yanfei Zhong. Deep multisensor learning for missing-modality all-weather mapping. _ISPRS Journal of Photogrammetry and Remote Sensing_, 17:2454-264, 2021.
* [81] Mohammadreza Zolfaghri, Yi Zhu, Peter Gehler, and Thomas Brox. Cross-modal contrastive learning for multi-modal video representations. In _Proceedings of the IEEE/CVF International Conference on Computer Vision_, pages 1450-1459, 2021.

[MISSING_PAGE_FAIL:11]

## Appendix B Datasets

This section describes the cross-modal alignment and joint multimodal pretraining evaluation datasets. We have two real-world IoT applications: Moving Object Detection (MOD) and Human Activity Recognition (HAR).

### Cross-modal Alignment Datasets

#### b.1.1. Moving Object Detection

We have seismic and acoustic signals describing different vehicles on three different domains. For simplicity, we use one letter to represent each domain. Table 7 provides the statistical values of each domain, and we provide detailed descriptions below:

**Domain M** is a publicly released (Doria et al., 2018) moving object detection dataset consisting of seismic and acoustic data from 7 different moving vehicles, recorded at three different distances and four different speeds. The sensor nodes, RaspberryShake, include a microphone array sampled at 16,000Hz and a geophone sampled at 100Hz. This dataset contains three types of downstream tasks - vehicle classification, distance classification, and speed classification. For cross-validation, data collected from three sensor nodes are used for training, and data from a separate node is used for testing.

**Domain G** contains a self-collected dataset on state park grounds near an outdoor research facility. Four sensor nodes, each featuring a geophone and a microphone array, were deployed to collect seismic and acoustic vibration signals from nearby objects at 200Hz and 16000Hz, respectively. Both seismic and acoustic signals were downsampled by half to match the signals from MOD. Four targets were chosen during the data collection, and each navigated the neighborhood near the sensors in some arbitrary order. Targets involved were (1) Polaris' 9 off-road vehicle, (ii) a Warthog4 all-terrain unmanned ground robot, (iii) a Husky unmanned outdoor field robot5, and (iv) a standard civilian truck. Data collection spanned over two days with different on-site noises observed, including but not limited to human interference (talking and walking near the sensors), environmental disturbances, background noises, etc.

Footnote 4: [https://www.polaris.com/](https://www.polaris.com/)

Footnote 5: [https://clearaphrobotics.com/warthog-unmanned-ground-vehicle-robot/](https://clearaphrobotics.com/warthog-unmanned-ground-vehicle-robot/)

**Domain T** contains seismic and acoustic vibration signals with a similar setup as MOD but involves different targets and scenes. This set contains data collected from a paved parking lot, unpaved trails, and gravel roads within a park. Vibration signals of 2 standard-size SUVs from different manufacturers, one lightweight sports car, and one muscle car were recorded. For each scene, we collected one hour of data for each vehicle. We use the first 50 minutes for training and the last 10 minutes for validation and testing.

#### b.1.2. Human Activity Recognition

Unlike the MOD application, where we used data from different domains for unimodal pretraining, we leveraged two different HAR datasets for unimodal pretraining and cross-modal alignment to evaluate the scenario in which IMU data has high degrees of heterogeneity.

**RealWorld-HAR (Wang et al., 2019)** is a public dataset that utilizes accelerometer, gyroscope, magnetometer, and light signals sampled at 50Hz. It includes data from 15 subjects performing eight common human activities: climbing stairs down and up, jumping, lying, standing, sitting, running/jogging, and walking. We used the data collected from devices positioned at the subjects' waists. For our experiments, we randomly selected ten subjects for training, 2 for validation, and 3 for testing.

**PAMAP2 (Wang et al., 2019)** contains inertial data from 18 human daily activities, such as walking, cycling, and playing soccer, performed by nine subjects. The dataset includes 9,611 instances, with data captured using inertial measurement units (IMUs) placed on the chest, the wrist of the dominant arm, and the dominant side's ankle. However, our experiment only utilized data collected from the wrist. Each data contains a 3-axis accelerometer, gyroscope, and magnetometer signal at a sampling rate of 100Hz. Seven random subjects are used for training, and two subjects for testing.

**Combined** is a concatenated dataset of RealWord-HAR and PAMAP2. Since PAMAP2 does not contain any light signals, we drop the light modality and only use the accelerometer, gyroscope, and magnetometer for evaluation.

\begin{table}
\begin{tabular}{c|c|c c|c c|c c c c} Dataset & Applications & Classes & Modality Sampling Rate & Sample Length & Interval (Overlap) & \#Pretrain Samples & Alignment & \#Alignment Samples & \#Sintetune Samples & 1337 \\ Domain M & MOD & 7 & acoustic (8kHz), seismic (100Hz) & 2 sec & 0.2 sec (0\%) & 39,609 & âœ“ & 1981 & 734 & 139 \\ Domain G & MOD & 4 & acoustic (8kHz), seismic (100Hz) & 2 sec & 0.2 sec (0\%) & 35,168 & âœ— & - & - & 1340 \\ Domain T & MOD & 4 & acoustic (8kHz), seismic (100Hz) & 2 sec & 0.2 sec (0\%) & 43,819 & âœ— & - & - & 1341 \\ \hline PAMAP2 & HAR & 18 & acc, gyro, mag (all 100Hz) & 2 sec & 0.4 sec (50\%) & 9,611 & âœ“ & 4805 & 961 & 1342 \\ RealWorld-HAR & HAR & 8 & acc, gyro, mag (all 50Hz) & 5 sec & 1 sec (50\%) & 12,887 & âœ— & - & - & 1343 \\ \hline \end{tabular}
\end{table}
Table 7. Statistical Summaries of Individual Domains

### Joint Multimodal Pretraining

In addition to pair-efficient SSL, we separately evaluate standard multimodal SSL, pretraining on large-scale multimodal data. We describe each dataset in more detail below and summarize the statistics in Table 8

**Moving Object Detection (MOD) [40]** is the superset of the domains we used in cross-modal alignment evaluation. We pretrain the encoders on domain M and then report the finetune performance in domain G and domain T.

**Acoustic-Seismic Identification Data Set (ACIDS)** is an additional multimodal dataset collected using two synchronized acoustic and seismic sensor systems (16-bit analog-to-digital converter operating at 1025 Hz) with more than 270 individual data runs, each featuring a singular ground vehicle type. The dataset captures signals from 9 distinct ground vehicles operating under 3 different environmental conditions. These targets were recorded at a constant speed from 5km/h to 40km/h as they navigated through the sensor systems, passing the sensors between 25 and 100 meters. The acoustic data is processed with a low-pass filtration at 400 Hz using a 6th-order filter to obviate spectral aliasing and a high-pass filtration at 25 Hz with a 1st-order filter to mitigate environmental noise (_e.g._, wind). The collected data runs were randomly divided into an 8:1:1 ratio for training, validation, and testing.

**RealWorld-HAR [62]** is the same dataset we used for cross-modal alignment evaluation. See B.1.2 for the detailed description.

**PAMAP2 [57]** is the same dataset we used for cross-modal alignment evaluation. See B.1.2 for the detailed description.

## Appendix C Data Preprocessing

We partition the time-series data into segments of uniform length. Each segment is then subdivided into intervals that may or may not overlap. We apply the Fourier transform to the signal in each interval to derive its spectral content, thereby retaining both temporal and spectral characteristics. The resultant spectrograms are subsequently imputed into our designed feature encoders. We have established a suite of data augmentation techniques applicable to the time domain pre-Fourier transform and the frequency domain post-Fourier transform. Each sample is subjected to a randomly chosen augmentation, which could be from either domain. Moreover, to enhance the stochastic nature of data augmentation within multimodal frameworks, we assign a fifty percent chance for each modality to undergo the randomly selected augmentation process.

## Appendix D Backbone

**SWIN-Transformer[41]**. SWIN-Transformer is a variant of the Vision Transformer [14] designed for images. We have adapted the SWIN-Transformer to process time-frequency spectrogram inputs. The time-frequency spectrogram input from each modality is first segmented into non-overlapping patches of embedding vectors via a convolutional layer. The model then extracts features through blocks of layer, each consisting of self-attention layers computed within the local windows. A shifting window mechanism is applied to increase the perceptual field at a much lower computational cost and allows the model to capture global information. The patches are downsampled at the end of each block by merging adjacent patches to double the feature channels. Separate SWIN-Transformer encoders are used to extract features from each sensory input modality. For supervised learning, additional self-attention layers are used to fuse the features to combine information across the various modalities. In cases where the learning framework operates at the modality level, the model bypasses the cross-modal fusion stages and computes pretraining losses directly on the features extracted from each modality. SWIN-Transformer is also used as the decoder for Masked Autoencoders. Instead of downsampling after each block, we expand the patches through linear layers at the beginning of each block to mirror the encoding steps. The decoded features at the end are then projected to match the dimension of patched input spectrograms for reconstruction loss. The backbone configurations of the datasets used in this paper for both encoding and decoding are detailed in Table 9.

## Appendix E Experiment and Implementation Details

**Training**. We specify the configurations used for InfoMAE during Joint Multimodal Pretraining, InfoMAE two-stage pretraining, and finetuning in Table 10. We randomly sample a batch of sequences of 4 consecutive samples (a total of 256 samples) during

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline Dataset & MOD & ACIDS & RealWorld-HAR & PAMAP2 \\ \hline Dropout Ratio & 0.2 & 0.2 & 0.2 & 0.2 & 1645 \\ Patch Size & audi: [1, 40], sei: [1,1] & [1, 8] & [1, 2] & [1, 2] \\ Window Size & [3, 3] & [2,4] & [3, 3] & [3, 5] \\ Encoder Block Num & [2, 2, 4] & [2, 2, 4] & [2, 2, 2] & [2, 2, 2] \\ Encoder Block Channels & [64, 128, 256] & [64, 128, 256] & [32, 64, 128] & [32, 64, 128] \\ Head Num & 4 & 4 & 4 & 4 \\ Encoder Fusion Channel & 256 & 256 & 128 & 128 \\ Encoder Fusion Head Num & 4 & 4 & 4 & 4 \\ Encoder Fusion Block & 2 & 2 & 2 & 2 \\ FC Dim & 512 & 512 & 256 & 128 & 1483 \\ Factorization Dimension & 128 & 128 & 128 & 128 & 1464 \\ Decoder Block Num & [2, 2] & [2, 2] & [2, 2] & [2, 2] & [140] \\ Decoder Block Channels & [128, 64] & [128, 64] & [64, 32] & [64, 32] & [140] \\ \hline \hline \end{tabular}
\end{table}
Table 9. Encoder & Decoder configurations.

pretraining. We jointly optimize the backbone encoders and decoders with AdamW (Chen et al., 2019) optimizer and Cosine scheduler (Chen et al., 2019). The model configurations for both the encoder and decoder are summarized in Table 9. Additionally, when training InfoMAE, we jointly train discriminators for density-ratio estimations (Zhu et al., 2019; Wang et al., 2019; Wang et al., 2019). We apply convolution blocks to map the time-frequency sample into a one-dimensional embedding to match the input dimension \(X_{1}\) with their shared and private representations \(V_{1},U_{1}\). Then, the discriminator estimates their density ratio through a 5-layer MLP. The exact discriminator configuration is provided in Table 11. We follow standard practice (Zhu et al., 2019) in training the discriminators with warmup epochs and training periods. The detailed training configurations for the discriminators are presented in Table 12.

**Computation.** We conducted our experiments on NVIDIA RTX 4090 GPUs with 24GB memory. The training time varies from a few minutes for finetuning to 2 days for Joint Multimodal Pretraining.

**Implementations.** We build our models on top of the open-source implementations (Chen et al., 2019; Wang et al., 2019; Wang et al., 2019) using Pytorch 2.0.1. We will also release our code upon acceptance.

## Appendix F Baselines

We describe all the baselines used in the evaluation below.

**Supervised:** The supervised approach trains with full supervision for each task to learn a mapping from the input to task-specific labels. Labels are used to update the entire model which includes both the encoder and the classification layer.

**SimCLR**(Chen et al., 2019) presents a simple contrastive framework for contrastive learning in visual perception tasks. In our implementation, each batch is randomly sampled. During the pretraining phase, we apply random augmentations to each sample to create two distinct views. The goal of these augmentations is to pull the transformed version of the same sample closer in the feature space while pushing the representations of other samples further apart. We treat other samples within the same minibatch (2N - 2) as the negative pairs. Consequently, variant views of an identical sample form positive pairs, whereas views derived from separate samples are treated as negative pairs.

**MoCoV3**(Chen et al., 2019) involves a query encoder denoted as \(f_{q}\) and a key momentum encoder denoted as \(f_{k}\). Both encoders have the same backbone network followed by a projection head. The query encoder \(f_{q}\) includes an additional projection head at the end. During pretraining, MoCoV3 relies on randomly augmented views of input samples to learn transformation invariant features. For each sample, it generates a query vector \(q\) using \(f_{q}\) and a key vector \(k\) using \(f_{k}\). The objective is to maximize the agreement between positive encoded query-key pairs. Specifically, the positive key \(k^{+}\) is encoded from the same sample as the query \(q\), while the negative keys \(k^{-}\) are encoded from other samples within the same mini-batch. This encourages the model to learn meaningful representations by contrasting positive and negative pairs. Additionally, MoCoV3 employs random batch sampling during training, and the key momentum encoder \(f_{k}\) is gradually updated using a query momentum with the query encoder \(f_{q}\).

**CMC**(Chen et al., 2019) represents a novel approach in contrastive learning that emphasizes the utilization of multiview data. CMC extracts meaningful representations by treating different modalities of the

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline \hline  & Joint Multimodal Pretraining & Unimodal Pretraining & Cross-Modal Alignment & Finetuning \\ \hline Optimizer & AdamW (Chen et al., 2019) & AdamW (Chen et al., 2019) & AdamW (Chen et al., 2019) & Adam (Chen et al., 2019) \\ Weight Decay & 0.05 & 0.05 & 0.05 & 0.05 \\ Start Learning Rate (LR) & 0.0001 & 0.0001 & 0.0001 & 0.01 \\ LR Scheduler & Cosine & Cosine & Cosine & Step \\ LR Decay & 0.2 & 0.2 & 0.2 & 0.2 \\ LR Period & 500 & 500 & 100 & 50 \\ \hline \multirow{2}{*}{Epochs} & MOD, ACIDS: 2500 & \multirow{2}{*}{2500} & \multirow{2}{*}{500} & \multirow{2}{*}{200} \\ Batch Size & & 256 & 256 & 256 & 128 \\ \hline \hline \end{tabular}
\end{table}
Table 10. Training configurations.

\begin{table}
\begin{tabular}{c|c c c c} \hline \hline Dataset & MOD & ACIDS & RealWorld-HAR & PAMAP2 \\ \hline Dropout Ratio & 0.2 & 0.2 & 0.2 & 0.2 \\ Mod Conv Kernel & aud: [1, 5], sei: [1, 3] & [1, 4] & [1, 3] & [1, 5] \\
827 & Mod Conv Channel & 128 & 128 & 128 & 64 \\ Mod Conv Layers & 5 & 6 & 4 & 1860 \\ MLP Layers & 4 & 4 & 4 & 4 & 1871 \\ Activation Function & LeakyReLU & LeakyReLU & LeakyReLU & LeakyReLU & LeakyReLU \\ \hline \hline \end{tabular}
\end{table}
Table 11. Discriminator configurations.

data as different views, minimizing the geometric gap between synchronized feature representations from these different modalities. During the training process, each mini-batch is processed by the backbone model to obtain modality representation. The framework then focuses on maximizing the similarity of representations from identical samples across modalities, while simultaneously treating dissimilar modality representations from distinct samples as negative pairs. The cumulative loss from all modality pairings is computed as the final loss term.

**GMC**[55] presents a multimodal contrastive loss function designed to align embeddings from various modalities geometrically to the joint embeddings. This framework operates on the principle of random batch sampling and augmentation similar to other contrastive learning baselines. In addition to modality-specific encoders, GMC has an additional joint encoder that processes inputs from all modalities concurrently. To facilitate the alignment of the joint embedding with the individual modality embeddings, an extra linear layer maps the joint embedding into the same dimensional space. Subsequently, a unified projection head is utilized to project both the individual and joint modality embeddings before the loss computation. GMC aims to maximize the similarity of individual modality embeddings (single view) to the joint embedding (global view).

**FOCAL**[40] is a recent contrastive learning framework designed for multimodal time-series signals. FOCAL acknowledges that each modality contains information shared across different modalities and information unique to each modality. FOCAL follows CMC in learning the shared representations and SimCLR for private representation. To avoid entanglement of the factorized representations, FOCAL minimizes the cosine similarity between the shared and private subspaces as well as between the private subspaces, creating an orthogonal latent space. In addition, FOCAL proposes a temporal ranking constraint to learn time-series locality.

**TNC**[64] is a self-supervised learning framework that captures time series representations through a debiased contrastive objective, distinguishing between temporally close and distant samples. TNC defines neighboring samples as those within the same sequence, sharing similar timestamps, and non-neighboring samples as those from different sequences. A discriminator is employed to predict the likelihood of each sample and its neighbors being in the same temporal window, to maximize the similarity of neighboring samples while minimizing that of non-neighboring ones.

**TS-TCC**[16] is a contrastive learning framework that robustly captures time series representations. It achieves this by combining cross-view predictions and contrasting both temporal and contextual information. In practice, TS-TCC randomly groups multiple sequences into mini-batches. For each sample, it generates two views through random augmentations. Context vectors are extracted from all sample representations up to a given timestamp within the sequence using an autoregressive model. These context vectors are then used to predict future timestamps in the other view. The framework simultaneously addresses both temporal alignment and context awareness, enhancing its ability to discern meaningful patterns in time series data.

**MMAE**[24] is a variant of Masked Autoencoders (MAE) with additional fusion modules for multimodal learning. It incorporates an encoder-decoder architecture and achieves SOTA performance on multiple vision tasks. Unlike contrastive learning, MAE does not depend heavily on random augmentations. During the pretraining, we randomly mask a significant portion (_i.e._, 75%) of each modality input. Instead of dropping the masked patches as in the original MAE paper, we replace them with 0 values to ensure consistent dimensions. A separate encoder and decoder are used for each modality. After encoding, we concatenate the modality features and then use separate MLP projection layers to get the fused modality embeddings before decoding to learn cross-modal information. Then a separate projector is used to map the fused embedding back to each modality embedding. Finally, the modality decoder reconstructs the modality input from the projected modality embeddings. The overall objective is to minimize the mean squared error (MSE) between the masked portion of the original modality patches and the reconstructed modality patches.

**CAV-MAE**[18] is a self-supervised learning framework building on top of both contrastive learning and MAE to learn audio-visual representations. It extends the capabilities of the traditional Masked Autoencoder (MAE) to process and learn from both audio and visual inputs simultaneously. CAV-MAE first extracts modality embedding through individual modality encoder. Then, it applies a joint encoder to extract joint embedding as well as encode separate modality embedding. The joint embedding is decoded for reconstruction, while the encoded modality embeddings are treated as positive pairs for contrastive learning.

## Appendix G Additional Evaluation

### Joint Multimodal Pretraining

Although InfoMAE is primarily designed for learning settings where the multimodal pairs are scarce, InfoMAE also demonstrates strong flexibility and generalization as a standard multimodal SSL framework when abundant multimodal pairs are available. We present

Figure 6. Joint Multimodal Pretraining compared with previous joint pretraining SSL frameworks on four datasets.

additional finetuning performance after joint multimodal pretraining in Figure 6 with many SOTA multimodal SSL frameworks across the four real-world IoT datasets we described. InfoMAE significantly exceeds the MAE-based SSL framework and achieves comparable or superior performance to the contrastive learning baselines. It is noteworthy that other baselines are mainly designed for joint multimodal pretraining. InfoMAE is a universal framework for cross-modal alignment that achieves comparable performance as multimodal SSL with few sacrifices.

### Ablation Studies on Joint Multimodal Pretraining

In addition to ablating InfoMAE for cross-alignment, we conduct ablation studies on Joint Multimodal Pretraining, evaluating different variants of InfoMAE when abundant multimodal data is available for pretraining. The results across the four datasets are presented in Table 13. Consistent with the results in Section 4.6, removing either shared or private representations hurts the performance of InfoMAE, indicating that both modality-shared and modality-exclusive information contribute positively to downstream tasks. Removing augmentations has minor impacts. However, InfoMAE experiences the most significant degradation when the temporal locality is absent, primarily due to that a generalized learning objective is required for Information Formulation to learn meaningful factorizations on time-series signals.

\begin{table}
\begin{tabular}{c|c c|c c|c c|c c} \hline \hline Dataset & \multicolumn{2}{c|}{MOD} & \multicolumn{2}{c|}{ACIDS} & \multicolumn{2}{c|}{RealWorld-HAR} & \multicolumn{2}{c}{PAMAP2} \\ \hline Frameworks & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 \\ \hline woTemp & 0.8734 & 0.8724 & 0.8808 & 0.7154 & 0.8442 & 0.8394 & 0.6948 & 0.6279 & 1864 \\ woShared & 0.9531 & 0.9518 & 0.8845 & 0.7435 & 0.8771 & 0.8843 & 0.8095 & 0.7686 & 1865 \\ woPrivate & 0.9082 & 0.9066 & 0.8562 & 0.7174 & 0.9100 & 0.9179 & 0.8080 & 0.7680 & 1860 \\ woAugmentation & 0.9538 & 0.9532 & 0.9101 & 0.7275 & 0.9106 & 0.9180 & 0.8163 & 0.7903 & 1869 \\ \hline InfoMAE & 0.9826 & 0.9819 & 0.9356 & 0.8101 & 0.9411 & 0.9462 & 0.8478 & 0.8319 & 1869 \\ \hline \hline \end{tabular}
\end{table}
Table 13. Ablation Results of InfoMAE.