# E2Usd: Efficient-yet-effective Unsupervised State Detection for Multivariate Time Series

Anonymous Author(s)

###### Abstract.

Cyber-physical system sensors emit multivariate time series (MTS) that monitor physical system processes. Such time series generally capture unknown numbers of states, each with a different duration, that correspond to specific conditions, e.g., "walking" or "running" in human-activity monitoring. Unsupervised identification of such states facilitates storage and processing in subsequent data analyses, as well as enhances result interpretability. Existing state-detection proposals face three challenges. First, they introduce substantial computational overhead, rendering them impractical in resource-constrained or streaming settings. Second, although state-of-the-art (SOTA) proposals employ contrastive learning for representation, insufficient attention to false negatives hampers model convergence and accuracy. Third, SOTA proposals predominantly only emphasize offline non-streaming deployment, we highlight an urgent need to optimize online streaming scenarios. We propose E2Usd that enables efficient-yet-accurate unsupervised MTS state detection. E2Usd exploits a Fast Fourier Transform-based Time Series Comressor (fftCompress) and a Decomposed Dual-view Embedding Module (ddEM) that together encode input MTS at low computational overhead. Additionally, we propose a False Negative Cancellation Contrastive Learning method (fncCleanXing) to counteract the effects of false negatives and to achieve more cluster-friendly embedding spaces. To reduce computational overhead further in streaming settings, we introduce Adaptive Threshold Detection (ddATD). Comprehensive experiments with six baselines and six datasets offer evidence that E2Usd is capable of SOTA accuracy at significantly reduced computational overhead. Our code is available at [http://bit.ly/3rMFJVv](http://bit.ly/3rMFJVv).

## 1. Introduction

In Cyber-Physical Systems (CPSs) (Stein et al., 2016; Krizhevsky et al., 2012; Krizhevsky et al., 2012), sensors monitor physical processes continuously, generating streams of Multivariate Time Series (MTS) data. This raw data, often complex and devoid of immediate interpretability, requires human labors to discern underlying "states" that correspond to specific conditions. For instance, consider an MTS corresponding to a dance routine as depicted in Fig. 1. The MTS is collected using four accelerometers situated in the dancer's arms and legs, capturing transitions between states that can be labeled "walk", "run", "jump", "kick", and "left hop". The aim of _state detection_ is to segment the MTS into a sequence of concise segments and assign each segment a state. Segments that share similar characteristics should be assigned the same state. In Fig. 1, the first and last segments exhibit similar fluctuations and are consequently assigned the same state, "walk".

Supervised state detection (Stein et al., 2016; Krizhevsky et al., 2012; Krizhevsky et al., 2012) requires known segments of an MTS and their labels, which are often not available. Thus, there has been a growing interest in _unsupervised state detection_ (USD) (Stein et al., 2016; Krizhevsky et al., 2012; Krizhevsky et al., 2012; Krizhevsky et al., 2012; Krizhevsky et al., 2012). USD is capable of identifying distinct states in an MTS directly, without relying on known segments and their labels. Once their USD process is completed, minimal human intervention is needed to assign a semantic label to each detected state. As depicted in Fig. 1, USD often uses clustering to associate each time step with a sufficiently similar already seen state or a new state if no similar state has been seen. Consecutive time steps with the same state are merged to form a segment. In the dance example, developers are not required to know the number and types of dance states beforehand. Instead, they can identify and label the dance states based on the USD results. This level of flexibility is particularly attractive in open-ended detection tasks, e.g., as found in cyber-attacks (Bartack et al., 2016), web application behavior (Krizhevsky et al., 2012), and beyond.

A commonly used USD approach (Krizhevsky et al., 2012; Krizhevsky et al., 2012; Krizhevsky et al., 2012), as depicted in Fig. 1, involves two stages. An initial encoding stage projects input data, acquired using a sliding window, into a latent embedding; then, a clustering stage identifies the state of this latent embedding. By moving the sliding window as data arrives, it is possible to support real-time detection of states. While recent advances in deep learning (DL) have improved the initial MTS data encoding (Krizhevsky et al., 2012; Krizhevsky et al., 2012), DL-based USD methods excel at capturing intricate MTS features, thus enhancing the subsequent clustering process. However, three main challenges remain.

**C1 (Resource-Intensive Architectures).** The intricate architectures, particularly those of DL-based MTS encoders (Krizhevsky et al., 2012; Krizhevsky et al., 2012), incur substantial computational and storage overheads. This precludes the deployment of such USD models on devices with limited resources, which occur frequently in practice.

**C2 (False Negative Sampling of Unsupervised Contrastive Learning).** State-of-the-art (SOTA) learning proposals for MTS encoders are rooted in unsupervised contrastive learning (Krizhevsky et al., 2012) and aim to maximize the similarity between similar samples (from consecutive windows) and to minimize the similarity between dissimilar ones (from distant windows). However, this approach can be prone to false negative sampling due to its idealized assumption that distant windows have distinct states. Ensuring the robustness of unsupervised contrastive learning at forming a clustering-friendly embedding space is an important concern.

Figure 1. An example of unsupervised state detection on MTS.

**C3 (Suboptimal Methodology on Streaming Scenarios)**. While current studies (Zhu et al., 2017; Wang et al., 2018; Wang et al., 2019) focus primarily on offline, non-streaming USD, there is a critical need for optimization for online deployments. Unconditional invocation of a USD model for all windowed MTS data can cause redundant clustering computations. Thus, more efficient strategies for streaming use are needed.

In this study, we present E2Usd, an efficient-yet-effective model addressing these challenges in unsupervised MTS state detection. To tackle **C1**, E2Usd includes a compact embedding method featuring two key strategies. First, it utilizes a Fast Fourier Transform-based Time Series Compressor (fftCompress) to selectively retain essential frequency components, discarding noisy ones. This reduces the computational overhead of subsequent operations. Second, to strike a balance between feature extraction capacity and model simplicity, E2Usd advocates a return to the original nature of a time series by decomposing compressed MTS into trend and seasonal components followed by a simple but sufficiently effective dual-view DL embedding module (named ddEM). This eliminates reliance on complex end-to-end DL architectures.

To tackle **C2**, we propose a False Negative Cancellation Contrastive Learning method (fnccLearning) tailored to mitigate false negative sampling in SOTA contrastive learning for USD. fnccLearning introduces a novel negative sampling scheme, where selecting genuinely false negatives is carried out by taking into account both the trend and seasonal similarities between paired samples. Moreover, instead of considering individual windowed samples, we take a holistic approach by harnessing groups of consecutive samples for similarity computation in the negative aspect, thereby ensuring consistent embedding within the same state. This unique treatment is reflected in the overall fnccLearning loss.

To tackle **C3**, we present Adaptive Threshold Detection (adaTD), which aims to reduce the number of clustering operations in online USD by first assessing the similarity between the currently windowed MTS data and the data in the preceding window and then deciding whether to perform clustering on the current windowed MTS data. A customized adaptive threshold, based on a simple and effective similarity metric is proposed to determine the similarity sufficiency. In experiments, E2Usd achieves the best accuracy while using only 4% of the total and 1% of the trainable parameters when compared to the SOTA method, while also achieving the lowest processing time among all competitors.

The primary contributions are as follows.

* We propose a compact MTS embedding method, comprising (i)fftCompress for retaining essential temporal information while mitigating noise for simplified time series representation and (ii) noEM, which enables dual-view embedding of trend and seasonal components in MTS, effectively integrating traditional and modern methodologies (Section 3.1).
* We propose fnccLearning, aimed at mitigating the likelihood of false negatives in unsupervised contrastive learning for MTS state detection. This is achieved by a unique treatment of potential negative pairs exhibiting the lowest similarities (Section 3.2).
* We devise the adATD scheme tailored for streaming USD. By comparing the current windowed MTS data to the preceding window, adATD reduces clustering operations based on an adaptive similarity threshold (Section 3.3).
* We study E2Usd on six datasets while considering six baselines, providing evidence of SOTA accuracy and substantial computational costs reduction. We also provide evidence of practical applicability by deploying E2Usd on an STM32 MCU (Section 4). Besides, Section 2 provides necessary background information, Section 5 reviews related work, and Section 6 concludes the paper.

## 2. Preliminaries

### Unsupervised State Detection for MTS

Definition 1 (Multivariate Time Series, MTS).: _A multivariate time series (MTS), denoted by \(\mathbf{X}\), is an ordered sequence of sensory observations:_

\[\mathbf{X}=\{\mathbf{x}_{i}\}_{i=1}^{\mathsf{T}},\quad\mathbf{x}_{i}\in\mathbb{R}^{\mathsf{ N}}, \tag{1}\]

_where \(\mathbf{x}_{i}\) is the observation at the \(i\)-th time step; the parameters \(\mathsf{N}\) and \(\mathsf{T}\) are the MTS dimensionality and the length of the MTS, respectively. A segment of \(\mathbf{X}\in\mathbb{R}^{\mathsf{N}\mathsf{C}}\) spanning time steps \(i\) to \(j\) is denoted as \(\mathbf{X}_{i:j}\in\mathbb{R}^{\mathsf{N}\mathsf{C}\left(j-1\right)}\)._

Definition 2 (State in MTS).: _A state acts as a concise representation of the underlying condition associated with an MTS segment._

States are discernible due to their unique internal features, such as recurring patterns or consistent statistical behaviors (Ghosh et al., 2017; Wang et al., 2018; Wang et al., 2019). Referring to the dance routine example in Fig. 1, states might correspond to different dance movements, each of which exhibits a unique pattern: "walk" with rhythmic variations, "jump" with intense spikes, "hop" with recurrent bursts, and "run" with higher frequency and intensity than "walk". Accurate identification of these specific patterns (states) is crucial to understanding the underlying process captured by an MTS and to enable downstream applications like urban monitoring (Brockman et al., 2018) and healthcare (Wang et al., 2019).

Definition 3 (Unsupervised State Detection, USD).: _Given an MTS \(\mathbf{X}\) of length \(\mathsf{T}\), the process of unsupervised state detection (USD) aims to assign each observation \(x_{i}\in\mathbf{X}\) at state index, \(s_{i}\), without any training data and a set of predefined states. This process ultimately yields a state sequence \(\mathbf{s}=\{s_{i}\}_{i=1}^{\mathsf{T}}\), where \(s_{i}\in\mathbb{R}^{+}\) identifies a specific state detected by the USD process._

Note that the number of distinct states found by the USD process, Distinct(\(\mathbf{s}\)), is unknown prior to the start of the process.

We proceed to introduce the **classical USD pipeline**(Wang et al., 2018; Wang et al., 2019; Wang et al., 2019). In general, USD utilizes a _sliding window_, i.e., MTS data is processed by the USD system per window along the time dimension. In Fig. 1, a sliding window of size \(\mathsf{P}\) and step size \(\mathsf{B}\) traverses the MTS. Let \(\mathbf{W}_{t}=\mathbf{X}_{t-\mathsf{P}:t}\) be the current window of the MTS. This window is processed by the **MTS encoder** to obtain an embedding \(\mathbf{x}_{t}\) in a latent space. This embedding is input to a **clustering model** that deduces its state index \(\mathbf{s}_{t}\in\{\mathbb{R}^{+}\}^{\mathsf{P}}\), which is then assigned to the \(\mathsf{P}\) time steps in window \(\mathbf{W}_{t}\in\mathbb{R}^{\mathsf{N}\mathsf{C}\mathsf{P}}\).

As the sliding window moves at step size \(\mathsf{B}\), each time step eventually has \(\lfloor\frac{\mathsf{P}}{\mathsf{B}}\rfloor\) state indexes determined by the USD process1. To reconcile these sets of state indexes and ensure smoother transitions, the final state index for each time step is determined through majority voting (Wang et al., 2018; Wang et al., 2019; Wang et al., 2019).

Conventional DL-based USD methods (Zhou et al., 2017; Zhang et al., 2018) employ intricate neural networks and take the raw MTS data as input directly. To enhance efficiency, we incorporate a Fast Fourier Transform (FFT)-based time series compressor (see Section 3.1.1). Below, we provide a brief overview of FFT.

### FFT in Time Series Analysis

FFT (Zhou et al., 2017) is fundamental to signal processing. Engineered for optimal computational efficiency, FFT calculates the Discrete Fourier Transform of numerical sequences. This capability is essential for identifying frequency components in a time series, enabling noise reduction and compression. Further, this capability supports our goal of reducing the computational overhead of the USD process, since this overhead is correlated with the length of the MTS. Integrating FFT offers a promising avenue for enhancing both efficiency and accuracy.

Specifically, we utilize real-valued FFT. Crafted as a variant of FFT, it transforms an MTS window \(\mathbf{W}\) with P time steps into \(K=(\lfloor p\rfloor/2+1)\) frequency components, represented as a matrix \(\mathbf{Q}\in\mathbb{R}^{\text{lock}}\). Conversely, the inverse real-valued FFT converts \(\mathbf{Q}\) back to a new time-domain representation, denoted by \(\hat{\mathbf{W}}\).

The computation of real-valued FFT and the inverse real-valued FFT are formulated in Equations 2 and 3, respectively.

\[q_{k}\leftarrow\sum_{j=0}^{k-1}\left(x_{p}\cos\left(-\frac{2\pi kp}{p}\right)+ \gamma\sin\left(-\frac{2\pi kp}{p}\right)\right),k=0,\ldots,K-1 \tag{2}\]

\[\hat{x}_{p}\leftarrow\sum_{j=0}^{k-1}\left(q_{k}\cos\left(\frac{2\pi kp}{p} \right)-q_{k}\sin\left(\frac{2\pi kp}{p}\right)\right),p=0,\ldots,P-1 \tag{3}\]

Here, \(\mathbf{q}_{k}\) is the \(k\)-th (\(0\leq k<\text{K}\)) frequency component of the real-valued FFT result \(\mathbf{Q}\), and \(\hat{x}_{p}\) is the \(p\)-th (\(0\leq p<\text{P}\)) time step's data of the inverse real-valued FFT result \(\hat{\mathbf{W}}\).

## 3. Key Techniques of E2USD

E2US follows the classical USD pipeline (Section 2.1), encompassing MTS embedding and clustering. In particular, E2USp utilizes the Dirichlet Process Gaussian Mixture Model (DPGMM) (Brockman, 1988) for clustering. Below, we present the key innovations in E2USp. E2USp first applies a compact embedding procedure to the input MTS to ease subsequent neural computations (Section 3.1). In the compact embedding procedure, E2USp also incorporates a novel False Negative Cancellation Contrastive Learning method (Section 3.2) to ensure or improve the effectiveness of the learned embeddings. Finally, E2USp employs an Adaptive Threshold Detection (adaTD) scheme for improved applicability in online streaming (Section 3.3).

### Compact Embedding of MTS

Contemporary DL-based embedding methods often utilize complex end-to-end networks (Zhou et al., 2017; Zhang et al., 2018; Zhang et al., 2018; Zhang et al., 2018), potentially overlooking the rich domain-specific knowledge in traditional feature engineering. To generate concise yet informative embeddings for input MTS, we advocate for revisiting fundamental principles in data analysis and signal processing, particularly in frequency domain analysis (Zhou et al., 2017) and time series decomposition (Zhou et al., 2017). Our studies indicate that these approaches can yield effective embedding outcomes while using simpler techniques. As shown in Fig. 2, our compact embedding comprises an FFT-based Time Series Compressor (fftCompress) and a Decomposed Dual-view Embedding Module (h0EM), detailed in Section 3.1.1 and Section 3.1.2, respectively.

#### 3.1.1. FftCompars

In practical applications, sensor designs often incorporate over-sampling rates to ensure comprehensive information capture. In this sense, maintaining a sparse representation of time series is reasonable and beneficial for retaining essential information and reducing noise, ultimately simplifying subsequent temporal feature extraction. Frequency domain analysis, primarily using FFT (Zhou et al., 2017; Zhang et al., 2018; Zhang et al., 2018; Zhang et al., 2018), is a well-established technique to achieve this goal. However, it remains challenging to effectively identify the set of distinct active frequency components for FFT analysis across different tasks. To address this aspect, we introduce fftCompars, which encompasses three steps.

**(1) Real-valued FFT.** Given a window \(\mathbf{W}_{t}=X_{t-P,t}\in\mathbb{R}^{\text{lock},p}\), this step transforms \(\mathbf{W}_{t}\) into its frequency domain representation \(\mathbf{Q}\in\mathbb{R}^{\text{lock}}\), using Equation 2. Expressing the input signal in the frequency domain is crucial for our goal of compressing time series data. In the frequency domain, we can identify and prioritize the most significant frequency components, thus achieving compression by emphasizing salient features and discarding less critical ones.

**(2) Energy-based Frequency Compressor.** As the core of fftCompars, this step dynamically selects and retains _active frequencies_ from the frequency domain based on the cumulative energy observed across all the MTS dimensions. Energy in signal processing quantifies the strength (magnitude) of a signal's frequency components. Typically, low energy implies reduced strength and activation. A possible solution is to select discrete frequencies, rather than forming a continuous band. However, two important factors need to be considered: first, research has shown that noise is often concentrated at the extreme frequencies (Zhang et al., 2018); second, active frequencies tend to cluster around a central range. Skipping frequencies could potentially lead to the omission of crucial information. Considering these factors, we choose to utilize a continuous frequency band. This decision is especially effective at removing both high- and low-frequency noise, which improves the overall data representation. Specifically, given a frequency-domain representation \(\mathbf{Q}\in\mathbb{R}^{\text{lock}}\), we compute for its **cumulative energy \(\mathbf{e}\in\mathbb{R}^{\text{lock}}\)**(Zhou et al., 2017), where the \(k\)-th component \(\mathbf{e}_{k}\) corresponds to a viable starting frequency \(k\) (\(0\leq k<\text{K}\)) and is computed as follows:

\[\mathbf{e}_{k}=\sum_{i=k}^{k+0-1}\sum_{n=0}^{N-1}(Q_{n,i})^{2}. \tag{4}\]

Here, Q (\(<\text{K}\)) is a predefined frequency bandwidth; N is dimensionality of the MTS; and \(Q_{n,i}\), a scalar, is the amplitude of the \(i\)-th

Figure 2. Compact embedding of the input MTS.

frequency component of the \(n\)-th dimension. We then identify the starting position \(k^{\prime}\in(0,\kappa)\) that yields the maximum cumulative energy over the given bandwidth \(Q\). Subsequently, we cut off \(Q\) consecutive frequency components starting from the \(k^{\prime}\)-th frequency component of \(Q\):

\[Q^{\prime}=Q[0:N:k^{\prime}:k^{\prime}+Q]. \tag{5}\]

As a result, \(Q^{\prime}\) encompasses \(Q\) consecutive frequency components that capture the most pronounced energy contributions.

**(3) Inverse Real-valued FFT.** The compressed frequency domain representation \(Q^{\prime}\in\mathbb{R}^{N\times Q}\) is then transformed back to its time domain using Equation 3. This yields a compressed MTS \(\hat{\mathbf{W}}\in\mathbb{R}^{N\times P^{\prime}}\), where \(P^{\prime}=2\times(0-1)\). As illustrated in the right part of Fig. 9, for an original time series of length \(P=480\), the energy-based frequency compressor retains a frequency bandwidth of \(Q=41\). This results in a compressed time series of length \(P^{\prime}=80\). Through fftCompress, the MTS is compressed from \(P\) to \(P^{\prime}\) in the temporal dimension. This leads to a significant reduction in computational overhead for the subsequent feature extraction while preserving essential signal attributes and reducing noise.

A detailed assessment of the energy-based frequency compressor's impact, along with a comprehensive parameter sensitivity analysis related to \(Q\), can be found in the Appendix [(1)].

#### 3.1.2. doEM

Many DL-based approaches employ computationally intensive modules for MTS feature extraction [(26; 34; 24; 36)]. While effective, these complex structures may capture redundantly features that can be obtained efficiently using lightweight, traditional tools, thus incurring unnecessary computational costs. Time series decomposition is a widely used technique for extracting essential components like trend and seasonality. Recognizing its significance, we introduce the Decomposed Dual-view Embedding Module (_d_e_DEM), which features an innovative and lightweight architecture that seamlessly combines time series decomposition with a subsequent lightweight dual-view neural embedding module. It facilitates the embedding of both trend and seasonality in MTS by leveraging the strengths of both traditional and modern approaches.

**(1) Decomposition of Compressed MTS**. Studies [(13; 42; 14)] show that time series data can be broken down into trend, seasonal, and residual (noise) components. In our approach, we exclude the residual component, as the prior fftCompress step has eliminated noise. Thus, with the compressed MTS \(\hat{\mathbf{w}}\in\mathbb{R}^{N\times P^{\prime}}\), we employ a proven method, the _moving average_ scheme [(42)], for decomposing it into trend and seasonal components. This approach is well recognized for its effectiveness in this context.

* The **trend component \(\mathbf{t}\mathbf{c}\in\mathbb{R}^{N\times P^{\prime}}\)** is calculated using a moving average kernel of size \(\kappa\), which is odd, as follows: (6) \[\mathbf{t}\mathbf{c}[n,t]=\frac{1}{\kappa}\sum_{i:-(\kappa-(1)/2)}^{(\kappa-(\kappa-1 )/2)}\hat{\mathbf{w}}[n,t+\|.\]
* The **seasonal component \(\mathbf{s}\mathbf{c}\in\mathbb{R}^{N\times P^{\prime}}\)** is obtained by subtracting the trend component \(\mathbf{t}\mathbf{c}\) from \(\hat{\mathbf{w}}\), formally, \(\mathbf{s}\mathbf{c}=\hat{\mathbf{w}}-\mathbf{t}\mathbf{c}\).

**(2) Dual-view Embedding**. Referring to Fig. 2, after decomposing the signals into two distinct views, they undergo embedding using lightweight networks. Both trend and seasonal views share identical embedding structures (1D convolution + max pooling + linear embedding). The embeddings resulting from both views are fused to create a compact MTS embedding.

Feature Projection Layer (no training). The trend and seasonal components are projected into high-dimensional latent spaces using 1D convolution:

\[\mathbf{h}^{\text{T}}=\text{Conv1D}(\mathbf{t}\mathbf{c})\ \ \mathbf{h}^{\text{S}}=\text{Conv1D}(\mathbf{s}\mathbf{c}). \tag{7}\]

This step employs convolution feature maps to provide varying perspectives on the signals. Notably, this projection module does not require training. It has been proven effective in multiple MTS classification studies [(15; 14; 30)]. The subsequent layers are trained to extract features from each view. Additionally, a detailed parameter sensitivity analysis regarding the dimensionality of these two latent spaces can be found in the Appendix [(1)].

Linear Embedding Layer (trainable). A max pooling layer is used to reduce dimensionality, thus enhancing computational efficiency while highlighting the most informative features. Then, a linear layer is applied to effectuate the embedding:

\[\mathbf{z}^{\text{T}}=\text{ReLU}(\text{Linear}(\text{MaxPooling1D}(\mathbf{h}^{ \text{T}});\Theta^{\text{T}})), \tag{8}\]

\[\mathbf{z}^{\text{S}}=\text{ReLU}(\text{Linear}(\text{MaxPooling1D}(\mathbf{h}^{ \text{S}});\Theta^{\text{S}})), \tag{9}\]

where \(\mathbf{z}^{\text{T}}\in\mathbb{R}^{0}\) and \(\mathbf{z}^{\text{S}}\in\mathbb{R}^{0}\) are the trend and seasonal embeddings of size \(D\), respectively. Trainable parameters \(\Theta^{\text{T}}\) and \(\Theta^{\text{S}}\) for \(\text{Linear}(\cdot)\) encompass weights and biases.

Fusion Layer (trainable). The embeddings from both views are concatenated and further transformed to generate the final MTS embedding \(\mathbf{z}\in\mathbb{R}^{0}\), providing a comprehensive representation that captures inter-view relationships:

\[\mathbf{z}=\text{Linear}(\text{Concat}(\mathbf{z}^{\text{T}},\mathbf{z}^{\text{S}});\Theta), \tag{10}\]

where \(\Theta\) is the corresponding trainable parameters. For a detailed sensitivity analysis regarding the final embedding size \(D\), refer to the Appendix [(1)].

By employing fftCompress and dDEM, we avoid intricate neural networks and their computational demands. In the following section, we introduce an innovative contrastive learning scheme to ensure that the compact embedding structure preserves crucial information effectively.

### False Negative Cancellation Contrastive Learning for Effective Embedding

As a technique used widely in the embedding stage of USD, contrastive learning [(11)] maximizes the similarity between similar samples (so-called positive pairs), while minimizing it for dissimilar ones (so-called negative pairs). However, **false negative sampling** is a common issue: current approaches [(34; 17; 36)] involve randomly sampling \(U\) distinct window groups from an MTS, each having \(\forall\) consecutive windows (see Fig. 3) through a sliding window. With this setup, each group is assumed to represent a unique state, with positive sample pairs always being from the same group and negative sample pairs always being from different groups. However, this setup can lead to a problem, where different groups inadvertently share the same state, causing samples from these groups to be incorrectly regarded as negative pairs. In Fig. 3, the blue and green windowed samples, which come from different groups, are incorrectly classified as negative pairs. In reality, they belong to the same state "walk" and should be considered as positive pairs.

We thus propose fnrcLearning (False Negative Cancellation Contrastive Learning), a novel approach that addresses this issue with a unique similarity-based negative sampling scheme. By considering seasonal and trend embeddings, it can sample those genuinely dissimilar _negative pairs_ from the groups with low similarity, enhancing MTS embedding effectiveness for USD. We proceed to present the new sampling scheme, followed by the overall fnccLearning loss.

**(1) Similarity-based Negative Sampling.** This scheme ensures the selection of genuinely dissimilar negative pairs via a similarity-based approach, evaluating seasonal and trend similarities for each pair and retaining the least similar pairs. The process involves three main steps:

Listing Possible Negative Pairs. Let U be the number of randomly sampled window groups, each of which contains V consecutive windows. A comprehensive set, \(\mathcal{J}\), is compiled encompassing all conceivable pair combinations from a total of U groups, resulting in \(\mathsf{U}\times(\mathsf{U}-1)/2\) possible negative pairs.

Computing Similarities for Each Pair. For each pair \((i,j)\in\mathcal{J}\), we compute seasonal \((\mathsf{sim}^{\mathsf{S}}_{i,j})\) and trend \((\mathsf{sim}^{\mathsf{T}}_{i,j})\) similarities using dot products, capturing both seasonal patterns and evolving trends:

\[\mathrm{sim}^{\mathsf{S}}_{i,j}=(\mathbf{d}_{i}^{\mathsf{S}})^{\top}\cdot\mathbf{d}_{ j}^{\mathsf{S}},\quad\mathsf{sim}^{\mathsf{T}}_{i,j}=(\mathbf{d}_{i}^{\mathsf{T}})^{ \top}\cdot\mathbf{d}_{j}^{\mathsf{T}}, \tag{11}\]

where \(\mathbf{d}_{i}^{\mathsf{S}}\) (_resp._\(\mathbf{d}_{i}^{\mathsf{T}}\)) represent the centroid (i.e., average embedding) of the seasonal embeddings \(\mathbf{z}^{\mathsf{T}}\) (_resp._ trend embeddings \(\mathbf{z}^{\mathsf{T}}\)) of all V consecutive windowed samples in the \(i\)-th window group.

The comprehensive similarity, \(\mathrm{sim}^{\mathsf{O}}_{i,j}\), for each pair \((i,j)\) is finally computed as the product of the trend and seasonal similarities:

\[\mathrm{sim}^{\mathsf{O}}_{i,j}=\mathrm{sim}^{\mathsf{T}}_{i,j}\cdot\mathrm{ sim}^{\mathsf{S}}_{i,j} \tag{12}\]

Filtering True False Negative Pairs. The \([\lambda\times|\mathcal{J}|]\) least similar pairs based on \(\mathrm{sim}^{\mathsf{O}}_{i,j}\), with \(\lambda\) as a fraction parameter in \((0,1]\) (set to 0.5 by default), are selected to form the set \(\mathcal{J}^{\prime}\) of negative pairs.

**(2) fnrcLearning Loss.** The following negative loss \(\mathcal{L}_{\text{neg}}\) aims to minimize the similarity between negative pairs from \(\mathcal{J}^{\prime}\):

\[\mathcal{L}_{\text{neg}}=\frac{1}{|\mathcal{J}^{\prime}|}\sum_{(i,j)\in \mathcal{J}^{\prime}}-\log(\text{Sigmoid}(-\mathbf{d}_{i}^{\mathsf{T}}\cdot\mathbf{d} _{j})), \tag{13}\]

where \(\mathbf{d}_{i}\) refers to the centroid of the final embedding \(\mathbf{z}\) of all V consecutive windowed samples in the \(i\)-th window group.

In Fig. 3, it is assumed that consecutive windowed samples from the same group (indicated by the same color) share a uniform state. Accordingly, the positive loss \(\mathcal{L}_{\text{pos}}\) aims to maximize the similarity between their embeddings:

\[\mathcal{L}_{\text{pos}}=1/\mathsf{N}\sum_{k=0}^{\mathsf{U}-1}\sum_{j=0}^{ \mathsf{V}-1}\sum_{j=0,j<l}^{\mathsf{V}-1}-\log(\text{Sigmoid}((\mathbf{z}_{k,l})^ {\top}\cdot(\mathbf{z}_{k,j}))), \tag{14}\]

where \(\mathbf{z}_{k,l}\) represents the embedding of the \(i\)-th (\(0\leq i<\mathsf{V}\)) window from the \(k\)-th (\(0\leq k<\mathsf{U}\)) group, and \(\mathsf{M}=\frac{\mathsf{U}(\mathsf{V}\mathsf{V}\mathsf{V}(\mathsf{V}-1)}{2}\) is used for normalization.

Ultimately, the fnrcLearning aims to enhance the similarity among positive pairs while diminishing the similarity among negative pairs. Thus, the fnrcLearning loss is defined as the sum of its positive and negative loss components:

\[\mathcal{L}_{\text{FNCC}}=\mathcal{L}_{\text{pos}}+\mathcal{L}_{\text{neg}} \tag{15}\]

The two hyperparameters, U and V, directly impact the loss computation, and the evaluation of their impact has been provided in the Appendix (Becker et al., 2017).

### Streaming USD with Adaptive Threshold

Upon obtaining the embedding of each windowed sample, a clustering algorithm, typically the Dirichlet Process Gaussian Mixture Model (DPGMM) (Becker et al., 2017), is traditionally employed. However, current methodologies often do not take into account the challenges of real-world online streaming. In such scenarios, states tend to persist, making clustering unnecessary for successive samples. To avoid redundant clustering, we propose the Adaptive Threshold Detection (adATD) mechanism that defers clustering until a new sample shows low similarity (controlled by an adaptive threshold) to the previous one. This ensures that clustering is invoked only when needed, enhancing USD efficiency. Fig. 4 illustrates adATD's computational savings compared to the classical 'Always Clustering Detection" (acD) mechanism.

The adATD process is detailed in Algorithm 1. Specifically, to gauge the temporal consistency between the current sample and its predecessor, we employ the dot product to quantify the similarity between their respective embeddings, \(z_{\text{pre}}\) and \(\mathbf{z}_{t}\):

\[\mathrm{sim}(\mathbf{z}_{\text{pre}},\mathbf{z}_{t})=\mathbf{z}_{\text{pre}}^{\top}\cdot\mathbf{ z}_{t}. \tag{16}\]

Then, this similarity value is compared to an adaptive threshold \(\tau\). If the similarity is below this threshold, this indicates a likely state transition, triggering clustering to identify the new state \(s_{t}\).

\[s_{t}\leftarrow\begin{cases}s_{\text{pre}}&\text{if}\ \mathrm{sim}(\mathbf{z}_{\text{pre}},\mathbf{z}_{t})\geq\tau\\ \mathrm{Cluster}(\mathbf{z}_{t})&\text{otherwise}\end{cases} \tag{17}\]

The overall adATD process is outlined in Algorithm 1, where the threshold \(\tau\) adapts to the context of streaming MTS. Its tuning is controlled by the computed similarity with two _scaling factors_\(\delta_{i}\) and \(\delta_{\tau}\). When the similarity exceeds \(\tau\), it implies that the state remains unchanged, prompting an increase in the threshold by the scaling factor \(\delta_{i}\) (line 10 in Algorithm 1), as the growing probability of a state transition. Conversely, if the similarity falls below the threshold, we hypothesize a state transition, prompting a clustering

Figure 4. Saved overhead.

Figure 3. Overview of \(\mathcal{L}_{\text{FNCC}}\). Same-color windows belong to the same group. The blue and green groups denote a pair of _false negatives_; these are random selections labeled as negative pairs, although they share the same state.

operation for confirmation (line 12). Upon verification (line 13), \(\tau\) is raised to act conservatively against the state transition (line 15). If the transition is deemed false, the threshold is decreased to counter overestimation from an excessively high threshold (line 16), and a higher value of the scaling factor \(\delta_{r}\) is selected to ensure a rapid response to incorrect state transition hypotheses. Concurrently, a reduced value for \(\delta_{l}\) is preferred, particularly considering the probability of an impending state transition rises. Parameter sensitivity study of \(\delta_{r}\) and \(\delta_{l}\) is reported in the Appendix (A).

In general, and CamTD seamlessly incorporates a cost-effective similarity metric tailored for streaming USD, significantly reducing redundant clustering operations. Empirical evaluation in Section 4.4 confirms the efficacy of adaTD.

```
1:MTS stream \(X\), threshold \(\tau\), and scaling factors \(\delta_{r}\) and \(\delta_{l}\)
2:State \(s_{\text{f}}\) for continuous time step \(t\) on \(X\)
3:\(W_{0}\gets X_{0:P-1}\)\(\triangleright\) sliding window sampling
4:\(z_{0}\leftarrow\) CompactEmbedding(\(W_{0}\))\(\triangleright\) see Section 3.1
5:\(s_{0}\leftarrow\) Clustering(\(z_{0}\))\(\triangleright\) DPGMM
6:\(z_{\text{pre}},z_{\text{pre}}\gets z_{0},s_{0}\)\(\triangleright\) initialize state
7:while obtaining updated \(\mathbf{x}_{t}\) from \(X\)do
8:\(W_{t}\gets X_{t\cdot p+1:t}\)\(\triangleright\) sliding window sampling
9:\(z_{t}\leftarrow\) CompactEmbedding(\(W_{t}\))
10:if\(\text{sim}(z_{\text{pre}},\mathbf{z}_{t})\geq\tau\)then
11:\(s_{t}\gets s_{\text{pre}}\)\(\triangleright\) keep current state
12:\(\tau\leftarrow\tau\times(1+\delta_{l})\)\(\triangleright\) increase \(\tau\)
13:else
14:\(s_{t}\leftarrow\) Clustering(\(\mathbf{z}_{t}\))\(\triangleright\) acquire new state
15:if\(s_{t}\neq s_{\text{pre}}\)then\(\triangleright\) verify state transition
16:\(z_{\text{pre}},z_{\text{pre}}\gets z_{t},s_{t}\)\(\triangleright\) update state
17:\(\tau\leftarrow\tau\times(1+\delta_{l})\)\(\triangleright\) increase \(\tau\)
18:else\(\tau\leftarrow\tau\times(1-\delta_{r})\)\(\triangleright\) decrease \(\tau\)
19:
20:endwhile
```

**Algorithm 1** Adaptive Threshold Detection (adaTD)

## 4. Experiments

### Experimental Settings

The entire codebase, datasets, hyperparameter settings, and instructions are available at [http://bit.ly/3rMFJVv](http://bit.ly/3rMFJVv). We trained the DL models on a server with an NVIDIA Quadro RTX 8000 GPU. For model inference, we employed an Intel Xeon Gold 5215 CPU (2.50GHz). Additionally, we carried out a case study of MCU deployment using an STM32H747 device (Ball et al., 2017). Further implementation details can be found in the Appendix (A).

**Baselines**. The following baselines are introduced. Baselines 1-3 employ the USD pipeline outlined in Section 2.1, while the remaining ones do not. (1) HVGH (Hoffmann et al., 2016) employs a variational autoencoder for encoding MTS windows and utilizes the Hierarchical Dirichlet Process (HDP) for clustering. (2) TICC (Hoffmann et al., 2016) uses a correlation network for encoding and adopts Toeplitz inverse covariance-based clustering. (3) Time2State(Tieleman et al., 2016) employs a Temporal Convolutional Network to encode MTS windows and utilizes the DPGMM for clustering (as does E2Usb). (4) Autofalt(Auer et al., 2017) applies the Minimum Description Length principle to segment the MTS and recursively models each segment with the Hidden Markov Model. (5) ClaSPTS_KMeans(Hoffmann et al., 2016) identifies change points in an MTS using multiple binary classifiers and employs KMeans(Krishnan et al., 2017) for segment clustering. (6) HDP_HSMM(Krishnan et al., 2017) is a Bayesian non-parametric extension of the Hidden Semi-Markov Model that uses HDP to estimate the number of states.

**Datasets**. For evaluations, we employ six datasets used in previous studies (Auer et al., 2017; Tieleman et al., 2016). These include one synthetic dataset, Synthetic(Tieleman et al., 2016), and five real-world datasets from diverse fields: MoCap, ActRecTut, PAMAP2, and UscHad track various human activities (Ball et al., 2017; Auer et al., 2017; Auer et al., 2017; Auer et al., 2017), and UcrSeg covers MTS from applications such as insect research, robotics, and energy(Auer et al., 2017). Among these, PAMAP2 exhibits the largest MTS lengths (ranging from 253k to 408k), while UcrSeg has the shortest (varying between 2k and 40k). UscHad features the largest number of states (12 in total), whereas UcrSeg has the fewest, with up to 3 states. Notably, UcrSeg stands out as a univariate times series dataset. A detailed description of the datasets is available in the Appendix (A).

**Metrics**. We use the Adjusted Rand Index (ARI) and Normalized Mutual Information (NMI) to assess the detection accuracy, as does prior research(Tieleman et al., 2016). ARI quantifies the instance-wise consistency between predicted and ground truth clusters by emphasizing clustering granularity, while NMI measures the shared information between ground truth and model clusterings. Furthermore, for evaluating detection efficiency, we present the Processing Time (PT), which records the time in seconds required by each method to process USD over a specific length of an MTS.

### Overall Comparison

An overall comparison between E2Usb and the baselines across the six datasets is listed in Table 1. The comparison is conducted on an Intel CPU over the STM device due to higher computational demands for the baselines that surpass the STM device's capacity. E2Usb consistently exhibits top-tier performance, achieving the best or near-best ARI and NMI scores. Notably, E2Usb also exhibits superior efficiency, as evidenced by consistently having the shortest processing time (PT)2. Among the baselines, Time2State manifests high performance at ARI, NMI, and PT for most datasets, positioning itself as the strongest competitor to E2Usb. Later in this section, a more detailed comparison between E2Usb and Time2State is provided. Autofalt excels in the MoCap dataset--notable for its shortest average MTS length--achieving the highest ARI and NMI scores, albeit with longer PT. However, it struggles on other datasets, particularly as it is unable to process the PAMAP2 dataset (marked as 'N/A' in Table 1), which has the longest average MTS length. ClaSPTS_KMeans achieves the top ARI and NMI scores on the univariate UcrSeg dataset. However, its performance is not competitive on other datasets of MTS.

Footnote 2: Here, PT denotes the processing time in seconds for each method applied to a medium-sized MTS (40k length) from the given dataset.

Efficiency is crucial in real-world applications, especially with large sequences or variable data volumes. Thus, we conduct experiments over sequence lengths ranging from 40k to 400k, using the Synthetic dataset. As illustrated in Fig. 5 (a), E2Usb exhibits by far the lowest overall processing times throughout the tested range of sequence lengths, from 0.52s for 40k to 1.7s for 400k. This renders E2Usb ideally suited for real-world scenarios demanding swift responses and the capacity to adapt to varying data volumes.

**E2Ust****vs.****Time2State****.** We compare with the current SOTA model Time2State on the Synthetic dataset. Both methods utilize DL-based encoders and share the same clustering model, directing our focus primarily on the encoder component. As depicted in Table 2, E2Ust outperforms Time2State significantly in terms of computational and storage efficiency. To be precise, E2Ust requires roughly 30 times fewer total parameters, 120 times fewer trainable parameters, and reduces Multiply-ACCumulate (MACC) counts3 by an impressive 83.5 times. Moreover, its peak memory usage is about a factor of 229 times smaller. All in all, these statistics provide evidence of E2Ust's strength in resource-constrained scenarios.

Footnote 3: The MACC count refers to the aggregate of multiply-accumulate operations in a given algorithm, commonly used as a metric for computational complexity in DL.

### Component Study of E2Ust

This section evaluates the effectiveness and efficiency of E2Ust's proposed components. As shown in Section 4.2, E2Ust exhibits relatively high consistency between API and NMI scores. Due to space limit, we thus focus on reporting the ARI scores. The corresponding NMI results are available in the Appendix [(1)].

#### 4.3.1. Encoder

We compare our encoder (denoted as E2Usd) with two variants: one without fftCompress (E2Ust w/o FFT) and the other without Trend-Seasonal Decomposition of dEdEM (E2Usd w/o TSD). Besides, we include widely used MTS encoders LSTM [(17)] and TCN [(36)] for comparison. To maintain fairness, we only substitute the encoder component of E2Ust with these alternatives, while keeping all the other settings unchanged.

We first examine the efficiency of all encoders using the setting described in Section 4.2. Fig. 5 (b) reveals that E2Ust consistently offers top-tier temporal efficiency, starting at a 0.52s PT for a 40k sequence and only slightly increasing to 1.71s for a 400k sequence. When comparing E2Ust to its variants, we observe that introducing TSD marginally increases PT but significantly boosts accuracy (explained later). Conversely, integrating fftCompress leads to a notable reduction in PT. Further, LSTM and TCN increase the processing time substantially, with LSTM at 39.46s and TCN at 10.51s for processing a 400k sequence.

Referring to the accuracy results reported in Fig. 6 (a), E2Usd consistently outperforms its competitors across all datasets. Notably, the inclusion of fftCompress does not compromise accuracy, owing to its noise reduction capability, while TSD significantly enhances accuracy (see E2Ustb vs. E2Ustb w/o TSD). When juxtaposed with LSTM and TCN, E2Usd also demonstrates better performance. One of the distinct advantages of E2Usd is its ability to clearly extract valid frequency and period trend information, which is crucial for accurate clustering. While LSTM and TCN have their merits, their black-box nature makes it uncertain whether they can effectively capture this information as reliably as the trend-seasonal decomposition feature of E2Ust.

#### 4.3.2. FneckLearning Loss

We compare our proposed \(\mathcal{L}_{\text{FNCC}}\) with SOTA loss functions, including Temporal Neighborhood Coding (TNC) [(34)], Contrastive Predictive Coding (CPC) [(24)], and Latent State Encoding (LSE) [(36)]. Besides, we include a variant of \(\mathcal{L}_{\text{FNCC}}\), \(\mathcal{L}_{\text{FNCC},\text{SE}}\), by constructing negative pairs using Samples' Embeddings (SE), rather than employing the centroids (i.e., average embeddings) of these groups. As emphasized in existing studies, these losses are encoder-agnostic [(24; 36; 34)]. To ensure fairness, we only replace the loss function of E2Ustb, following established research conventions [(36)]. This setup ensures that any performance differences stem solely from the inherent qualities of the loss functions themselves, rather than variations in encoder architectures.

Fig. 6 (b) shows that \(\mathcal{L}_{\text{FNCC}}\) consistently outperforms baselines on all datasets.A key factor contributing to this robust performance

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{3}{c}{Synthetic} & \multicolumn{3}{c}{MacCap} & \multicolumn{3}{c}{ActRecTut} & \multicolumn{3}{c}{PAMAP2} & \multicolumn{3}{c}{UscMed} & \multicolumn{3}{c}{UcrSeg} & \multicolumn{3}{c}{756} \\ \cline{2-15}  & PT & ARI & NMI & PT & AH & NMI & PT & ARI & NMI & PT & AH & NMI & PT & AH & NMI & PT & AH & NMI & 737 \\ \hline TIVGR [(27)] & 27.53 & 0.8809 & 0.1406 & 25.98 & 0.0500 & 0.1523 & 26.17 & 0.0881 & 0.2088 & 24.06 & 0.0023 & 0.074 & 25.42 & 0.0778 & 0.183 & 26.74 & 0.0638 & 0.1451 & 738 \\ HDP JESM [(31)] & 51.24 & 0.6491 & 0.7795 & 35.48 & 0.5590 & 0.7230 & 56.57 & 0.6644 & 0.6473 & 52.10 & 0.2882 & 0.5378 & 0.43 & 0.6789 & 0.438 & 0.1625 & 0.2574 & 779 \\ TTC [(20)] & 20.55 & 0.8422 & 0.7489 & 2.169 & 0.7218 & 0.7218 & 0.7322 & 22.41 & 0.7320 & 0.7246 & 29.44 & 0.3040 & 0.5955 & 21.51 & 0.3927 & 0.7043 & 19.29 & 0.2325 & 0.2158 & 798 \\ ATWPTA [(25)] & 73.80 & 0.8713 & 0.1077 & 7.569 & **0.8057** & **0.8289** & 22.960 & 0.6148 & **N.8** & **N.

is \(\mathcal{L}_{\text{FNCC}}\)'s effectiveness in minimizing false negatives, which leads to a cluster-friendly latent embedding space and enhances accuracy. Note that incorporating \(\mathcal{L}_{\text{FNCC}}\) does not compromise the efficiency, as trained DL models remain loss-agnostic.

### Efficacy of adaTD

We have empirically evaluated the performance of the Adaptive Threshold Detection (adaTD) algorithm when applied to the processing of streaming MTS data using the Synthetic dataset. We simulate streaming scenarios by continuously feeding MTS data to the model. Our assessment involved a comparison with two baseline detection schemes: "Always Clustering Detection" (acD) and "Static Threshold Detection" (STD), with sTD(\(\tau\)) representing detection based on a fixed threshold value \(\tau\).

As depicted in Fig. 7 (a) and (b), adaTD demonstrates a well-balanced trade-off between accuracy and efficiency. While acD achieves slightly higher accuracy, adaTD excels significantly in terms of efficiency. Furthermore, adaTD surpasses sTD across a range of static thresholds in terms of accuracy while requiring fewer clustering operations and maintaining competitive processing time. Notably, when increasing the static thresholds in sTD (i.e., \(\tau\) ranging from 0.2 to 0.8), the detection accuracy improves and approaches that of adaTD at \(\tau=0.8\), However, this increase in \(\tau\) is accompanied by a decrease in efficiency, and even at \(\tau=0.4\), it still falls short of matching the efficiency of adaTD. This observation highlights the superior adaptability of adaTD to variations in the similarity across different states, thus ensuring efficient and accurate detection.

### Case Study on Resource-limited MCU

To assess the viability of deploying EQuS on edge devices, we conducted experiments using a commodity STMS2HT47 MCU on the Synthetic dataset. Significantly, this device could not accommodate other baseline methods due to their high demands on computation and memory. The results reveal the following operational metrics during the operational phase. The Flash memory consumption amounts to **63.72 KB**, a mere 3.11% of the available 2 MB. In terms of RAM, it uses **73.27 KB**, representing a modest 7.16% of the overall 11 MB capacity, signifying efficient memory utilization. Moreover, the latency for processing each sample is **44.95 ms**, which equates to a detection frequency close to 20 Hz. When assuming a state persists for 10 sampling intervals, E2Usd is capable of handling streaming USD scenarios below 200 Hz, encompassing a wide range of practical applications. These results constitute strong evidence of the efficient resource utilization of E2USD, underscoring its applicability in resource-limited scenarios.

## 5. Related Work

**Unsupervised State Detection for MTS**. Broadly, USD for MTS can be categorized into two groups: those that follow the two-stage pipeline outlined in Section 2.1 and those that deviate from it. Research in the former category typically places its focus on the MTS embedding stage. A notable example is HVGH (Zhou et al., 2017), which employs a variational autoencoder for MTS encoding and the Hierarchical Dirichlet Process for clustering. Besides, TICC (Zhou et al., 2017) proposes a novel correlation network based on Toeplitz inverse covariance for MTS embedding. Recently, Time2State(Zhou et al., 2017) introduced contrastive learning to enhance the learning of the embedding module, but it faces challenges of computational overhead and false negative samples, highlighting the need for efficient models for resource-constrained devices like MCUs. In contrast to the two-stage pipeline, methods like AutoPdlat(Zhou et al., 2017), HDP_HSMM (Zhou et al., 2017), and ClapTS_KMeans(Zhou et al., 2017) adhere to a one-stage framework but encounter significant scalability and stability issues (see Fig. 5 (a)), rendering them unsuitable for online usage.

**Compact Unsupervised Representation Learning for MTS**. While numerous DL studies have explored unsupervised representation learning for MTS data, the majority of current research has concentrated on innovating intricate structures to enhance representation effectiveness (Zhou et al., 2017; Zhou et al., 2017; Zhou et al., 2017) but has not considered developing compact models for this purpose. There are also studies dedicated to compact DL models for MTS, offering techniques that can be adapted for unsupervised MTS representation learning. For example, the recent LightCTS (Zhou et al., 2017) introduces compact architectures and operators for MTS forecasting. Similarly, LightTS(Zhou et al., 2017) employs adaptive ensemble distillation to achieve a compact architecture for MTS classification. However, these studies tend to exclusively explore DL approaches, overlooking the traditional methods that have been developed over the years, which often exhibit a higher level of compactness compared to DL structures.

Recognizing this gap, EQuS aims to take into account the nature of MTS data, bridging traditional MTS representation techniques and DL methods. The method leverages an FFT-based approach to obtain a sparse representation of MTS data and then employs a decomposed dual-view embedding module, which integrates classical time series decomposition and a lightweight DL model to produce the final embedding. This offers a promising avenue for compact unsupervised MTS representation learning.

## 6. Conclusion and Future Work

In this study, we present E2USD, an efficient-yet-effective method for unsupervised MTS state detection. An extensive empirical study offers detailed insight into the properties of the components of E2USD, including fftCompress, ddem, and frecLearning. Overall, E2USD achieves state-of-the-art accuracy and efficiency in diverse scenarios. The incorporation of an Adaptive Threshold Detection (adaTD) enables a harmonious balance between accuracy and computational requirements, positioning E2USD as the best choice for streaming state detection. As we move forward, we aim to investigate the false positive cases in frecLearning and explore wider real-world applications for E2USD.

Figure 7. Comparative analysis of adaTD with acD and sTD.

## References

* (1)
* (2023)
* (2023) 2023. Appendix of E2USD. [http://bit.ly/3rdBqNp](http://bit.ly/3rdBqNp). (Accessed Oct 2023)
* (2023)
* (2023) 2023. Microcontrollers and microprocessors, SYIndirectelectronics. [https://www.st.com/microcontrollers-microprocessors/3indirect-series.html](https://www.st.com/microcontrollers-microprocessors/3indirect-series.html). (Accessed Oct 2023)
* (2023) 2023. SYM32Cube.AI. SYMnicoelectronics. [https://www.st.com/content/st_com/en/cuspaigns/stm32cube-aih-abltml](https://www.st.com/content/st_com/en/cuspaigns/stm32cube-aih-abltml) (Accessed Oct 2023)
* (2023) Md Ashidim Mohdadi and Zeenan Belemus. 2019. Intelligent traffic congestion classification system using artificial neural. In _WWW_. 110-116.
* (2024) David M Else and Michael I Jordan. 2006. Variational inference for Dirichlet processes mixtures. (2006).
* (2025) E. Ozan Brigham and RE Morrow. 1967. The fast Fourier transform. _IEEE Spectrum_ 41, 12 (1967), 63-70.
* (2026) Andreas Bulling, Ulf Blanke, and Bernt Schiele. 2014. A tutorial on human activity recognition using body-worn inertial sensors. _Computers_ 46, 3 (2021), 331-333.333-3333. [https://doi.org/10.1145/2490621](https://doi.org/10.1145/2490621)
* (2027) David Campos, Muao Zhang, Bin Yuang Kim, Chenjuan Guo, and Christian S Jensen. 2023. LightPS: Lightweight time series classification with adaptive ensemble distillation. _SIGMOD_ 1, 2 (2020), 1-27.
* (2028) Marta Catillo, Antonio Pecchia, Massimiano Rako, and Unburlot Villano. 2021. Supervisibility of the role of public intrusion datasets: A replication study of DDS network traffic data. _Computers_ 108 (2021), 102431.
* (2029) Manosone Casa-Stoks and Daniel Massicone. 2020. Windowing compensation in Fourier based surrogate analysis and application to EEG signal classification. _IEEE T&M_ 71 (2022), 1-11.
* (2029) Ting Chen, Smarni Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive learning of visual representations. In _ICLR_. 1995-1967.
* (2030) Zhi Chen, Jiang Duan, Li Kang, and Guoping Qiu. 2022. Supervised anomaly detection via conditional generative adversarial network and ensemble active learning. _IEEE T&M_ 65, 6 (2022), 781-798.
* (2031) Robert E Cleveland, William S Cleveland, Jean E MacRae, and Junr Terpenning. 1990. STL: A seasonal trend decomposition. _J. Opt. Stat._ 6 (1990), 3-73.
* (2032) Angus Dempster, Francois Fotileau, and Geoffrey T Webb. 2020. ROCKET: exceptionally fast and accurate time series classification using random convolutional kernels. _INDM_ 35, 3 (2020), 1454-1495.
* (2033) Angus Dempster, Daniel F Schmidt, and Geoffrey T Webb. 2021. Minitocket: a survey for fast (global) deterministic transform for time series classification. In _SIGKDD_. 248-257.
* (2034) Arik Emshaus, Patrick Schafer, and Ulf Leser. 2023. ChSr2 parameter-free time series segmentation. _Data Mining and Knowledge Discovery_ 37, 3 (2023), 1262-1300.
* (2035) Jean-Yves Franceschi, Aymeric Brasilevett, and Martin Jaggi. 2019. Unsupervised scalable representation learning for multivariate time series. In _NeurIPS_, Vol. 32.
* (2036) Shaghayesh Gharghabi, Yifie Ding, Chris Chiahei Sheik, Kenny Kamgar, and Lindahl Ulmarova. and Eamonn Keogh. 2017. Matrix profile VII: domain agnostic online semantic segmentation at superhuman performance levels. In _ICDM_. 117-126.
* (2037) Nico Gornitz, Marius Kloff, Konrad Breck, and Ulf Bredddi. 2013. Toward supervised anomaly detection. _JMLR_ 63 (2013), 235-262.
* (2038) David Hallase, Sesar Vet, Stephen Boyd, and Jure Leskovec. 2017. Toeplitz inverse covariance-based clustering of multivariate time series data. In _SIGKDD_. 215-223.
* (2039) Hirawya Jayathikka, Chandra Krivts, and Rich Wolski. 2017. Performance monitoring and root cause analysis for cloud-hosted web applications. In _WWW_. 499-478.
* (2040) Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In _ICLR_.
* (2041) Zhichen Lai, Dalia Zhang, Huan Li, Christian S Jensen, Hua Lu, and Yan Zhao. 2023. LightCPTS: A lightweight framework for correlated time series forecasting. In _SIGMOD_, Vol. 1. 1-26.
* (2042) Sandy Love, Peter G-Connor, and Bastian Veling. 2019. Putting an end to end-end: Gradient-based learning of representations. In _NeurIPS_, Vol. 32.
* (2043) Yassiko Matsubara, Yasushi Sakurai, and Christos Faloutsos. 2014. Autopalk: Automatic mining of co-voting time sequences. In _SIGMOD_. 192-204.
* (2044) Matsuoshi Nagano, Tomohi Nakamura, Takayuki Nagai, Dachi Mochihashi, Ichiro Kobayashi, and Muratame Takano. 2019. HVG: unsupervised segmentation for high-dimensional time series using deep neural compression and statistical generative model. _Front. Robot. AI_ (2019), 115.
* (2045) Guanong Pang, Anton van den Hengel, Chunhua Shen, and Longbing Cao. 2021. Toward deep supervised anomaly detection: Reinforcement learning from partially labeled anomaly data. In _SIGMOD_. 1288-1008.
* (2046) Attia Reis. 2012. PAM22: Physical Activity Monitoring. UCI Machine Learning Repository. DOI: [https://doi.org/10.24323/CSNW2H](https://doi.org/10.24323/CSNW2H)
* (2016) Yasushi Sakurai, Yasusiko Matsubara, and Christos Faloutsos. 2016. Mining big time-series data on the web. In _WWW_. 1029-1032.
* (2047) Hoijt Salehinejad and Shalhrokh Valuee. 2022. Litchar: lightweight human activity recognition from wifi signals with random convolution kernels. In _ICASSP_. 408-4072.
* (2016) D Sundararajan. 2016. _Discrete wavelet transform: a signal processing approach_.
* (2016) Romain Travenard, Johann Fouini, Giles Vandervalle, Felix Divo, Guillaume Andron, Chetter Holt, Marie Payne, Roman Turchak, Marc Rudworn, Kuhul Kalar, et al. 2020. "Meam: a machine learning toolkit for time series data. _JMLR_ 21, 1 (2020), 4686-4691.
* (2048) Yee Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei. 2006. Hierarchical artificial processes. _J. Am. Stat. Assoc._ 101, 676 (2006), 1566-1581.
* (2020) Samo Teukohui, Danny Flynn, and Anna Golenberg. 2020. Unsupervised representation learning for time series with temporal neighborhood coding. In _ICLR_.
* (2020) Kevin Vaat, Maria Pa Santi, Gregory Faaat, and Jean-Jacques Leaspe. 2020. Human activity discovery and recognition using probabilistic finite-state automata. _IEEE T&M_ 17, 4 (2020), 2085-2095.
* (2032) Chengyang Wang, Kai Wu, Tongqing Zhou, and Zhiping Cao. 2022. TimeState: an unsupervised framework for inferring the latent states in time series data. In _SIGMOD_, Vol. 1. 1-18.
* (2031) Chengyang Wang, Kai Wu, Tongqing Zhou, Guang Yu, and Zhiping Cai. 2021. Tsagen: synthetic time series generation for kpi anomaly detection. _IEEE TNRM_ 19, 1 (2021), 1306-145.
* (2033) Qiong Wen, Jingfan Gao, Xiaomin Song, Liang Sun, Huan Xu, and Shenghuo Zhu. 2019. RobustSIT: A robust seasonal-trend decomposition algorithm for long time series. In _AAAI_, Vol. 33, 5409-5416.
* (2037) Mike West. 1997. Time series decomposition. _Biometrika_ 84, 2 (1997), 489-494.
* (2038) Makoto Tamara, Alsiao Kimura, Faroichi Naya, and Hiroshi Sawada. 2013. Clause-point detection with feature selection in high-dimensional time-series data. In _ICML_.
* (2039) Shucukoo Ma, Anilao Wang, Wenjun Jiang, Yiran Zhao, Huajie Shao, Shenghuo Liu, Dongxin Liu, Jinyang Li, Tianshi Wang, Shaohan Hu, et al. 2019. Sifnets: Learning sensing signals from the time-frequency perspective with short-time fourier neural networks. In _WWW_. 1292-2020.
* (2049) Aiming Zeng, Muxi Chen, Li Zhang, and Qiang Xu. 2003. Are transformers effective for time series forecasting?. In _AAAI_, Vol. 37. 11211-11218.
* (2050) Mi Zhang and Alexander A Sawchuk. 2012. USC-HAD: A daily activity dataset for ubiquitous activity recognition using wearable sensors. In _UEDQ_. 1036-1043.
* (2051) Xiang Zhang, Ziyuan Zhao, Theodore Tsidigaridas, and Mariana Zitnik. 2022. Self-supervised contrastive pre-training for time series via time-frequency consistency. _NeurIPS_ 35 (2022), 3988-4003.
* (2052) Yichi Zhang, Guisheng Yin, and Yuxin Dong. 2023. Contrastive learning with frequency-domain interest trends for sequential recommendation. In _RecSys_. 1041-150.
* (2053) Tian Zhou, Ziyiang Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin. 2022. Felderer: Frequency enhanced decomposed transformer for long-term series forecasting. In _ICLR_. 27268-27226.

## 7. Appendix

### Dataset Description

We use five real-world datasets and one synthetic dataset for comprehensive evaluations:

* Synthetic [36]: It is a synthetic dataset, generated by the MTS generator TSAGen [37].
* MoCap [25]: Derived from the CMU motion capture repository. In this dataset, every motion is represented as a sequence of hundreds of frames.
* ActRecTut [7]: This dataset involves two participants performing hand movements with height gestures in daily life and 3D gestures while playing tennis.
* PAMAP [28]: Covering both basic (e.g., walking, sitting) and composite human activities (e.g., soccer), this dataset features data from eight individuals.
* UscHad [43]: This dataset encompasses 12 distinct human activities such as jumping and running, recorded for 14 individuals.
* UcrSeg [18]: This dataset encompasses diverse sources, including medical fields, insect studies, robotics, and power demand data.

A summary of the statistics is provided in Table 3. Specifically, the varying range denoted as \(x-y\) for the number of states, i.e., \(\pi\)(State), implies that an individual time series within the MTS can encompass as few as \(x\) states and as many as \(y\) states. The same applies to the length and state duration.

### Implementation Details

Experiments were conducted on a server with an NVIDIA Quadro RTX 8000 GPU and an Intel Xeon Gold 5215 CPU (2.50GHz). For fftCompress in Section 3.1.1, the frequency bandwidth \(\Omega\) is set to 33 (see Equation (4) and Equation (5)). For ddEm in Section 3.1.2, \(\kappa\) in Equation (6) is set to 5, the dimension of the intermediate embeddings \(\boldsymbol{h}^{\text{T}}\) and \(\boldsymbol{h}^{\text{S}}\) (see Equation (7)), denoted as \(\mathbb{C}\), is set to 80 by default, the random convolution kernel for Conv1D in Equation (7) is set to 3, and the dimension of the final embedding \(z\) in Equation (7), denoted as \(\mathbb{D}\) is set to 4. The fmcLearning method used \(\mathbb{U}=20\) groups of windows (each with \(\mathbb{V}=4\) neighboring windows) and a fraction threshold \(\lambda\) of 0.5. Sliding window sizes were 128, 256, or 512, dataset-dependent, with step size \(\mathbb{B}=50\). Adam optimizer [22] was used with a learning rate of 0.003 for 20 epochs. Lastly, ddTD was configured with scaling factors \(\delta_{l}=0.08\) and \(\delta_{r}=0.1\) and initiated the threshold \(\tau\) at a value of 1. The sensitivity of various key parameters, including \(\mathbb{U}\), \(\mathbb{V}\), \(\mathbb{Q}\), \(\mathbb{D}\), \(\mathbb{C}\), \(\delta_{l}\), and \(\delta_{r}\) are reported later in this appendix.

The MCU deployment uses an STM32H747 device [2] with a 480 MHz Arm Cortex-M7 core, 2 MB Flash memory, and 1 MB RAM, as presented in Fig. 8. The E2Usn model was converted to ONNX format and translated to C code with X-CUBE-AI [3]. The C code was compiled using an ARM-specific version of GCC to create an executable binary.

### Impact Assessment of the Energy-based Frequency Compressor

To verify the effectiveness of the Energy-based Frequency Compressor (EFC), we conduct fftCompress on the UcrSeg dataset using various bandwidth values, denoted as \(\mathbb{Q}=\{120,60,40\}\). Referring to Fig. 9, the top row showcases the original and reconstructed waveforms within the native time domain. The middle row displays their corresponding amplitude spectra, while the bottom row exhibits the compressed waveforms. The original data is represented in blue, the reconstructed versions in orange, and the compressed versions in green.

Upon reverting the filtered frequency components to the original time domain, we observe _minimal distortion_. Specifically, the Mean Absolute Percentage Error (MAPE) is less than 5%, even though we retain only a sixth of the original frequency domain representation.

### Additional NMI Results for Component Study

In Fig. 10, we present the NMI comparisons for both encoders and losses. We note that the trends in NMIs are basically consistent with the corresponding ARIs shown in Fig. 6. This observation further substantiates the effectiveness of our proposed encoder and loss components.

\begin{table}
\begin{tabular}{l c c c c} \hline \hline
**Dataset** & **\#(MTS)** & **\#(State)** & **\#(Variate)** & **Length (\(\boldsymbol{h}\))** & **State Duration (\(\boldsymbol{h}^{\text{T}}\))** \\ \hline Synthetic & 100 & 5 & 4 & 9-32-37 & 0.1-39 \\ MoCap & 9 & 5-8 & 4 & 4-16-16 & 0.4-20 \\ _ActRecTut_ & 2 & 6 & 10 & 31.4-32-6 & 0.02-5.1 \\ PAMAP2 & 10 & 11 & 9 & 235-408 & 2.0-03 \\ UscHad & 70 & 12 & 6 & 25.4-56-3 & 0.6-13.5 \\ UcrSeg & 32 & 2-3 & 1 & 2-40 & 1-25 \\ \hline \hline \end{tabular}
\end{table}
Table 3. Statistics of the datasets.

Figure 8. The STM32H747 device for model deployment.

Figure 9. Impact assessment of the Energy-based Frequency Compressor on the performance of fftCompress.

### Parameter Sensitivity Study

We conduct a comprehensive parameter analysis using the ActRecTut dataset, focusing on assessing how these key parameters affect the ARI and NMI.

#### 7.5.1. Impact of \(\mathsf{U}\) and \(\mathsf{V}\) in Negative Sampling

These two parameters jointly contribute to the computation of \(\mathcal{L}_{\mathsf{FNCC}}\). More specifically, \(\mathsf{U}\) designates the number of distinct window groups, whereas \(\mathsf{V}\) defines the number of consecutive windows within each group. As shown in Fig. 11, altering \(\mathsf{V}\) does not consistently influence ARI. This unexpected result could be attributed to larger \(\mathsf{V}\) values capturing broader temporal scores, thereby introducing false positives due to state transitions. This implies that while increasing \(\mathsf{V}\) appears to be beneficial for capturing more data, it may inadvertently degrade performance. Conversely, ARI remains stable across a range of \(\mathsf{U}\) values, highlighting the robustness of E2Usd.

#### 7.5.2. Impact of Frequency Bandwidth \(\mathsf{Q}\) in FftCompress

This parameter \(\mathsf{Q}\) serves as the size of the frequency bandwidth of Energy-based Frequency Compression (EFC) on the fftCompress. It has a direct bearing on the fftCompress's compression rate. Fig. 12 exhibits two key trends. Lower \(\mathsf{Q}\) values compromise ARI and NMI due to aggressive data compression, causing the loss of essential information. On the other hand, elevating \(\mathsf{Q}\) leads to performance plateaus or minor reductions, likely because of the introduction of noise.

#### 7.5.3. Impact of Intermediate Embedding Size \(\mathsf{C}\) in \(\mathsf{d\textsc{eem}M}\)

As illustrated in Fig. 13, enlarging the latent space dimensionality \(\mathsf{C}\) generally boosts both ARI and NMI, peaking at \(\mathsf{C}=80\). Further increases in \(\mathsf{C}\) result in diminishing returns and even minor performance setbacks. Thus, by default, we set \(\mathsf{C}\) to \(80\) for all the experiments.

#### 7.5.4. Impact of Final Embedding Size \(\mathsf{D}\) in \(\mathsf{d\textsc{eem}M}\)

As demonstrated in Fig. 14, increasing the embedding size \(\mathsf{D}\) typically enhances ARI and NMI metrics, with a peak performance observed at \(\mathsf{D}=4\). Beyond this value, the benefits of enlarging \(\mathsf{D}\) decrease, and there may even be slight performance deterioration. By default, we set \(\mathsf{D}=4\) in E2Usp.

#### 7.5.5. Impact of \(\delta_{i}\) and \(\delta_{r}\) in \(\mathsf{a\textsc{ad}T}\)

In E2Usd, the adaptability of \(\mathsf{a\textsc{ad}T}\) stems from its ability to adjust the threshold \(\tau\) based on the model's response, which is directly influenced by the \(\frac{\delta_{i}}{\delta_{r}}\) ratio. For our evaluation, we set \(\delta_{r}=0.1\) and adjust the \(\frac{\delta_{i}}{\delta_{r}}\) ratio within a range of \(0.1\) to \(1\).

As shown in Fig. 15, the effectiveness of \(\mathsf{a\textsc{ad}T}\) improves incrementally with an increase in the \(\frac{\delta_{i}}{\delta_{r}}\) ratio. Remarkably, it means parity with the conventional "Always Clustering Detection" (AcD) approach when \(\frac{\delta_{i}}{\delta_{r}}=0.8\). This is achieved while executing notably fewer clustering operations and maintaining a reduced average processing time per window.