# Beyond Labels and Topics: Discovering Causal Relationships in Neural Topic Modeling

Anonymous Author(s)

###### Abstract.

Topic models that can take advantage of labels are broadly used in identifying interpretable topics from textual data. However, existing topic models tend to merely view labels as names of topic clusters or as categories of texts, thereby neglecting the potential causal relationships between supervised information and latent topics, as well as within these elements themselves. In this paper, we focus on uncovering possible causal relationships both between and within the supervised information and latent topics to better understand the mechanisms behind the emergence of the topics and the labels. To this end, we propose Causal Relationship-Aware Neural Topic Model (CRNTM), a novel neural topic model that can automatically uncover interpretable causal relationships between and within supervised information and latent topics, while concurrently discovering high-quality topics. In CRNTM, both supervised information and latent topics are treated as nodes, with the causal relationships represented as directed edges in a Directed Acyclic Graph (DAG). A Structural Causal Model (SCM) is employed to model the DAG. Experiments are conducted on three public corpora with different types of labels. Experimental results show that the discovered causal relationships are both reliable and interpretable, and the learned topics are of high quality comparing with seven start-of-the-art topic model baselines.

Causal Relationships Discovery, Neural Topic Model, Structural Causal Model +
Footnote †: ccs: Information systems Web mining Web mining

+
Footnote †: ccs: Information systems Web mining Web mining

+
Footnote †: ccs: Information systems Web mining Web mining

+
Footnote †: ccs: Information systems Web mining Web mining

+
Footnote †: ccs: Information systems Web mining Web mining

+
Footnote †: ccs: Information systems Web mining Web mining

+
Footnote †: ccs: Information systems Web mining Web mining

+
Footnote †: ccs: Information systems Web mining Web mining

## 1. Introduction

Topic modeling is a family of text mining techniques aimed at automatically discovering representative and semantically interpretable topics from textual data (Song et al., 2016; Yang et al., 2017). Topic models are wildly used in a variety of AI tasks, including web mining and information retrieval (Yang et al., 2017; Yang et al., 2017). In recent years, the incorporation of supervised information has gained attraction in topic modeling. The supervised information mainly contains semantic labels and text categories. For the first branch, topic models typically map each semantic label to a specific topic or a cluster of topics. The semantics of these labels then serve to guide the interpretability and relevance of the discovered topics (Yang et al., 2017; Yang et al., 2017; Yang et al., 2017). Parallel to this, another significant branch of topic models views the supervised information as categories or outcomes of texts. This perspective has been instrumental in advancing several natural language processing (NLP) tasks, such as document classification (Yang et al., 2017; Yang et al., 2017).

However, despite the ability of these models to exploit supervised information to enhance the discovery of semantically related topics, most existing topic models ignore the complex relationships between supervised information and latent topics, as well as within these elements themselves. To alleviate the problem, in this paper, we propose a novel neural topic model that can jointly extract the latent topics and discover potential causal relationships between supervised information and the latent topics. Causal relationships in textural data and its supervised information are widespread in the real-world. The reason why we choose "causal relationship" to model the complex relationships mentioned above is that compared to the common relationships in topic modeling, such as correlation (Yang et al., 2017; Yang et al., 2017; Yang et al., 2017) and hierarchical relationships (Yang et al., 2017; Yang et al., 2017), the applicable scenarios of causal relationships are wider and the relationships that can be described are more comprehensive.

Figure. 1 shows a toy instance of the causal relationships between the supervised information and the latent topics, as well as within these elements themselves. The left side of Figure. 1 displays the discovered latent topics with top-5 topic words in topic modeling, while the right side denotes some manually annotated labels. Causal relationships are represented as the directed edges. Taking

Figure 1. Examples of the causal relationships (directed edges) between the manually annotated labels and the latent topics (nodes), as well as within these elements themselves.

the node _"Topic Modeling"_ as an example, the topic _"Topic Modeling"_ is influenced not only by the topic _"Computer Science"_, but also by the label _"Machine Learning"_ and _"Computation and Language"_. Recognizing the causal relationships is crucial for understanding the mechanisms behind the emergence of certain topics within the context of specific labels. However, the causal relationships between the supervised information and the latent topics are rarely modeled by existing topic models. Furthermore, the causal relationships cannot be replaced by correlation or hierarchical relationships without missing semantic information.

In order to automatically discover the causal relationships discussed above, in this paper, we propose a novel neural topic model, called Causal Relationship-Aware Neural Topic Model (CRNTM). CRNTM takes the textual data and the supervised information as inputs and jointly learns the causal relationship between and within the supervised information and the latent topics. Specifically, CRNTM is built upon the variational autoencoder (VAE) framework with a Dirichlet distribution prior (Goodfellow et al., 2014). It encodes the input texts and the supervised signals into low-dimensional latent topical representations and then learns the inherent causal relationships both between and within the supervised information and latent topics. To jointly learn the three types of causal relationships, we consider both the latent topics of documents and the supervised information as nodes in a Directed Acyclic Graph (DAG), and the causal relationships can be represented as the directed edges. We adopt a Structural Causal Model (SCM) (Golovin et al., 2013; Kingma and Welling, 2014; Kingma and Welling, 2014; Kingma and Welling, 2014) to learn the causal relationship DAG. SCM is a type of strategy to model causal relationships between variables in causal inference based on the theory of structural equation modeling (SEM). By integrating SCM as a causal relationship learning module, the model can discover the inherent causal relationships both between and within the supervised information and latent topics, and generate the causality enhanced representations of the variables. We further introduce some regularization functions to optimize the causal relationship matrix and make it conform to the properties of causal relationships, including the directed acyclicity of the DAG, the information transmission of parent-child nodes in causal relationships, and the counterfactual regularization.

We conduct experiments on three public corpora from the real-world with different types of supervised information, such as age ratings, document categories and annotated tags. We compare the discovered topics with seven start-of-the-art topic model baselines in terms of topic coherence, topic uniqueness and topic quality. The experimental results show that the topics learned by our proposed CRNTM are of high quality. Furthermore, we visualize the discovered causal relationships between the variables and show their reliability and interpretability.

Our main contributions can be summarized as follows:

* We propose a novel neural topic model that can capture the potential relationships between supervised information (i.e. labels) and latent topics, as well as within these elements themselves, simultaneously.
* We employ the Structural Causal Model to jointly model the causal relationships between supervised information and the latent topics, since causality is widespread and covers most situations in practice.

## 2. Related Work

In this section, we briefly review state-of-the-art neural topic models, along with a discussion on the interplay between supervised information and topics in topic modeling.

### Neural Topic Models

Topic modeling is wildly used in automatically uncovering representative topics from corpora (Kumar et al., 2017; Chen et al., 2018; Chen et al., 2019; Chen et al., 2019). In recently years, neural topic modeling (Kumar et al., 2017; Kingma and Welling, 2014; Kingma and Welling, 2014) has attracted much attention thanks to the development of deep learning.

ProdLDA is the first autoencoding variational Bayes (AEVB) (Kingma and Welling, 2014) based topic model, which uses a Laplace approximation to represent the Dirichlet distribution prior. Gaussian Softmax (Sofman, 1999) uses the Gaussian Softmax distribution to parameterize the latent multinomial topic proportion of each document. W-LDA (Kohn, 1999) introduces Wasserstein autoencoders (WAE) (Shi et al., 2016) to topic modeling, allowing topic proportions to follow the Dirichlet prior. Sparse Dirichlet variational autoencoder (DVAE Sparse) (Goodfellow et al., 2014) implements the rejection sampling variational inference (RSU) as the reparameterization function of the Dirichlet distribution prior in VAE based neural topic modeling. TAN-NTM (Shi et al., 2016) uses an LSTM to extract contextual information and an attention mechanism to identify words relevant to each topic in topic modeling. Coordinated Topic Modeling (CTM) (Chen et al., 2018) uses a set of well-defined topics as prior knowledge for easily understandable representation. NSEM-GMITM (Kumar et al., 2017) enhances hierarchical topic modeling by incorporating a Gaussian mixture prior for improved sparse data handling, and explicitly representing both hierarchical and symmetrical topic relationships through dependency matrices and nonlinear structural equations.

Most recently, the incorporation of pre-trained language models into topic modeling has provided contextualized semantic embedding. Examples include embedded topic model (ETM) (Kumar et al., 2017). CombinedTM (Chen et al., 2018), enhanced guided LDA model (Shi et al., 2016), BERT-Flow-VAE (Devlin et al., 2018) and BERTopic (Devlin et al., 2018).

### Supervised Information and Topic Models

The integration of supervised information into topic modeling, which blends structured, labeled data with the unsupervised extraction of latent topics, is an increasingly pivotal area of study and has the potential to greatly enhance our understanding and utilization of text corpora. In topic modeling, supervised information are usually utilized to guide the semantic structure and enrich the quality of the topics leading to improved model performance (Shi et al., 2016; Kolesnikov et al., 2017). Existing topic models that can leverage supervised information can be broadly divided into two types: 1) Supervised information is used as a guidance of the semantic of the topics (or a subset of topics), representative models include Labeled LDA (LLDA) (Zhou et al., 2016), Partially LDA (PLDA) (Zhou et al., 2016), Topic Attention Model (Shi et al., 2016), JoSH (He et al., 2016) and supervised BERTopic (Devlin et al., 2018). 2) Supervised information, typically in the form of category labels, is correlated with the topic proportion vectors of each document (Chen et al., 2018; Devlin et al., 2018). Models such as SCHOLAR (Chen et al., 2018) offer flexible incorporation of metadata into neural topic modeling. HIMECat (Himing et al., 2018) is a neural topic model that can integrate the label hierarchy, metadata, and text signals for document categorization under weak supervision. HSTM (Henderson et al., 2018) is designed to model the structure in text concurrently capturing heterogeneity in the relationship between text and outcomes of documents.

However, none of these models take into consideration the causal relationships between supervised information and latent topics. To the best of our knowledge, our work is the first one that can uncover potential causal relationships between the supervised information and latent topics, and jointly model the causal relationships within these elements themselves.

## 3. Causal Relationship-Aware Neural Topic Model

This section provides an in-depth introduction of the proposed CRNTM and its learning strategy. The objective of CRNTM is to jointly discover interpretable causal relationships between and within the supervised information and latent topical representations from the corpus and manual labels. CRNTM is built upon the variational autoencoder (VAE) framework (Goodfellow et al., 2014; Kingma and Welling, 2016). Initially, the model encodes the input texts and the supervised information into low-dimensional latent topical representations based on the Dirichlet distribution prior. Subsequently, the model endeavors to identify the causal relationships between and within the learned latent topical representations and the supervised signals to enrich the topical embeddings with causality via a Structural Causal Model (SCM). An illustration of the model architecture is provided in Figure. 2, showcasing the process from latent topical representation learning to causal relationship identification.

### Latent Topical Representation Learning

To capture the latent topical representation of a document alongside its supervised information, we use a two-phase representation learning method. First, a pre-encoding phase learns topical representations that encapsulate the essential semantic information within the input texts. Then, we jointly encode the latent variables of the documents and the supervised information. This approach provides a comprehensive understanding of both the latent topics and the supervised information inherent in the document.

#### 3.1.1. Pre-encoding for the documents

Assume the input corpus contains a set of \(D\) documents, denoted as \(\{x_{d}\}_{d=1}^{D}\), where \(x_{d}\in\mathbb{R}^{V}\) represented as a vector in a \(V\)-dimensional space using a bag-of-words model (BoW), with \(V\) being the size of the vocabulary. Each document \(x_{d}\) is associated with a set of supervised information, denoted as \(l_{d}\).

Different from most VAE based neural topic models (Bishop, 2006; Kingma and Welling, 2016; Kingma and Welling, 2016), where the variational distribution are drawn from the Gaussian distribution, in this paper, we use the Dirichlet distribution as a prior for the latent topical representations. The input document \(x_{d}\) is transformed into the Dirichlet parameter \(\alpha\), which can be represented as:

\[\lambda=MLP(x_{d}),\] \[\alpha=\log(1+\epsilon^{2}), \tag{1}\]

where \(MLP(\cdot)\) denotes a multilayer perceptron layer, which can transfer the input \(x_{d}\in\mathbb{R}^{V}\) to latent topical variables, \(\lambda\in\mathbb{R}^{K}\). \(K\) denotes the number of the latent topics. \(\alpha\) is the Softplus function of \(\lambda\) for smoothness.

Then, we can draw a topic proportion \(\theta_{d}\) from a Dirichlet distribution parameterized by \(\alpha\). Since the Dirichlet distribution does not support non-central differentiable reparameterization, we adopt the proposal function of a rejection sampler as the reparameterization function based on the rejection sampling variational inference (RSVI) (Goodfellow et al., 2014; Kingma and Welling, 2016). In RSVI, a complex or unknown probability distribution (referred to as the target distribution), denoted as \(q(z;\alpha)\), can be sampled from an easier-to-sample proposal distribution, denoted as \(r(z;\alpha)\), with a constant accept rate \(M_{\alpha}\):

\[q(z;\alpha)\leq M_{\alpha}r(z;\alpha). \tag{2}\]

As discussed in (Chen et al., 2017), a Dirichlet distribution with parameter vector \(\alpha\) can be sampled from independent Gamma distributions with the same parameter \(\alpha\). Therefore, the latent topical representation \(z\) drawn from the Dirichlet distribution with parameter \(\alpha\), i.e. \(z\sim Dirichlet(\alpha)\), can be simulated from the distribution with Gamma-distributed random variables:

\[\tilde{z}_{d,k}\sim\Gamma(\tilde{a}_{k},1),k=1,...,K, \tag{3}\]

Then latent topical representation \(z\) can be computed through the simulated latent topical representation \(\tilde{z}\):

\[z_{1:K}=\frac{1}{\sum_{k}\tilde{z}_{d,k}}(\tilde{z}_{d,1},...,\tilde{z}_{d,K}) ^{\top}\sim Dirichlet(\alpha_{1:K}). \tag{4}\]

For Gamma distribution, there exists an efficient rejection sampler (Goodfellow et al., 2014):

\[z=h_{\Gamma}(\epsilon,\alpha)=(\alpha-\frac{1}{3})(1+\frac{\epsilon}{\sqrt{q_ {\alpha}-3}})^{3},\epsilon\sim s(\epsilon)\coloneqq\mathcal{N}(0,1), \tag{5}\]

where \(\epsilon\) is the accepted sample in the rejection sampler.

Since the rejection sampler has higher acceptance rates for higher values of the parameter \(\alpha\) in the Gamma distribution, we use a shape augmentation trick following the idea in RSVI (Goodfellow et al., 2014) to solve the problem. Suppose \(B\) is a positive integer. Then, \(z\) can be expressed as: \(z=\tilde{z}\prod_{i=1}^{B_{1}}u_{i}^{\frac{i+1}{2}}\), \(\tilde{z}\sim\Gamma(\alpha+B,1)\) and the uniform random variable \(u_{i}\)\(i.i.d.\)\(U[0,1]\). Therefore, the above rejection sampling Eq. (3) can be redefined as \(\tilde{z}\sim\Gamma(\alpha+B,1)\), and the shape augmented Eq. (5) can be redefined as:

\[z=h_{\Gamma}(\epsilon,\alpha,B)=(\alpha+B-\frac{1}{3})(1+\frac{\epsilon}{\sqrt {9(\alpha+B)-3}})^{3}. \tag{6}\]

#### 3.1.2. Joint encoding for both texts and supervised information

To leverage the supervised information inherent in the textual data and further explore causal relationships, we propose a joint encoding of the supervised information and latent variables of the documents.

In our model, the supervised information can take various forms such as labels or categories associated with the texts. The model imposes no restrictions on the type of supervised signals, allowing for great flexibility. These signals could include both quantifiable (numeric) and non-quantifiable (non-numeric) information. For example, quantifiable information can be converted into real numbers, while non-quantifiable information could be represented as binary variables. This versatility allows a wide range of data types to be incorporated into our model.

Specifically, we concatenate the latent topical variables of document \(d\), denoted as \(\alpha\), with the associated supervised information \(l_{d}\), and encode them in a manner similar to the pre-encoding phase section 3.1.1:

\[\lambda^{*} =MLP(Contact(\alpha;l_{d}))), \tag{7}\] \[\alpha^{*} =\log(1+\epsilon^{2^{*}}),\] (8) \[z^{*} =h_{\Gamma}(\epsilon^{*},\alpha^{*},B) :=(\alpha^{*}+B-\frac{1}{3})(1+\frac{\epsilon}{\sqrt{9(\alpha^{*} +B)}-3})^{3}, \tag{9}\]

where \(l_{d}\in\mathbb{R}^{S}\) denotes the supervised signals of document \(d\), and \(S\) is the number of supervised signals. The comprehensive representation \(z^{*}\) contains both the latent topic information from the text and the supervised information.

### Causal Relationship Learning

We utilize a Structural Causal Model (SCM) (Sandvikumar et al., 2017) to learn the causal relationships between and within the latent representations and the supervised signals. The causal relationships between the variables can be modeled via the weighted adjacency matrix of the Directed Acyclic Graph (DAG), denoted as \(A\). In CRSTM, both the latent topics and the supervised variables are considered as nodes in the causal relationship DAG. We arrange the topics and supervised variables as the first few nodes and the last few nodes in the DAG respectively. Then, the causal relationships between the latent topics, and between the supervised variables, can be represented by the upper left and lower right parts of the DAG's weighted adjacency matrix, respectively. Meanwhile, the causal relationships between topics and supervised variables can be represented by the remaining parts of the adjacency matrix. In this way, we can use the weighted adjacency matrix to clearly represent the above three types of causal relationships in supervised information and latent topics. The dimension of the weighted adjacency matrix of the DAG equals the total number of latent topics (\(K\)) and the supervised signals (\(S\)), i.e. \(A\in\mathbb{R}^{(K+S)\times(K+S)}\). According to structural causal learning (Sandvikumar et al., 2017), we have the following linear structural equation model (SEM):

\[C_{d}^{*}=A^{\Gamma}C_{d}^{*}+z_{d}^{\star}=(I-A^{\Gamma})^{-1}z_{d}^{\star}, \tag{10}\]

where \(C_{d}^{*}\in\mathbb{R}^{(K+S)\times H}\) is the causal representation of document \(d\), which denotes the causal relationships enhanced representations. \(H\) is the dimension of the causal representations. \(z_{d}^{\star}=t(z_{d}^{\star})\), where \(t(\cdot)\) is a linear transformation layer, and \(z_{d}^{\star}\in\mathbb{R}^{(K+S)\times H}\) is an extension of the latent topical representation, \(z_{d}^{\star}\), to make it contain more semantic information.

In order to enhance the directionality in causal relationships, the following constraints should be concerned: the causal representation of a node i) is not allowed to contain information from its non-parent nodes; and ii) ought to incorporate the representation information of its parent nodes to ensure the information transformation from parents to children. Therefore, we adopt the Mask Layer (Ross et al., 2017; Wang et al., 2017) in CRNTM to implement the above two constraints:

\[C_{d,i}=g_{i}(A_{i}\circ C_{d}^{*})+z_{d,i}^{\frac{\star}{2}}, \tag{11}\]

where \(A_{i}\) denotes the \(i^{th}\) column in the weighted causal adjacency matrix \(A\), \(C_{d,i}\) is the masked latent causal representation of the \(i^{th}\) node (topic or supervised signal) in document \(d\), and \(\circ\) is the element-wise multiplication. The function \(g_{i}(\cdot)\) is a mild nonlinear function for input variables to do self reconstruction.

To better understand Eq. (11), we can consider the following extreme cases.

* If \(A_{j,i}>0\) in matrix \(A\), the \(j^{th}\) node is a parent of the \(i^{th}\) node. Then, \([A_{i}\circ C_{d}]_{j}\neq\emptyset\), and Eq. (11) can be expressed as a function of \(C_{d,i}\): \(C_{d,i}=G_{i}(C_{d,j}^{*})\).
* If \(A_{j,i}=0\), \([A_{i}\circ C_{d}^{*}]_{j}\equiv\emptyset\Rightarrow\forall G_{i}(\cdot),C_{ d,i}\neq G_{i}(C_{d,j}^{*})\).

The topic proportion \(\theta_{d}\) can be computed through the causal topical representation \(C_{d}\):

\[\theta_{d}=\text{softmax}(f(C_{d})), \tag{12}\]

where \(f(\cdot)\) is a linear transformation layer, and \(\theta_{d}\in\mathbb{R}^{K}\).

In the decoding step, we reconstruct the original input documents with the topic proportion \(\theta_{d}\) and the topic distribution

Figure 2. Network structure of CRNTM. The pre-encoding phase ingests textual data \(x\) and learns to represent the essential semantic information as topical representations. The joint encoding phase combines the latent topical variables with the supervised information and encodes them into the same semantic embedding space. The causal relationship learning module uncovers potential causal relationships both between and within the latent topics and the supervised signals and enriches the representations with causality via a DAG. See more details in section 3.

\(\beta\in\mathbb{R}^{K\times V}\). The reconstruction of a word \(x_{d,n}\) in the text \(x_{d}\) can be modeled as \(x_{d,n}\sim Mult(\text{softmax}(\theta_{d}\cdot\beta))\).

### Learning and Inference

Our proposed CRNTM takes the documents \(\{x_{d}\}_{d=1}^{D}\) and the associated supervised signals \(\{l_{d}\}_{d=1}^{D}\) as inputs to discover the causal relationships between and within the learned latent topical representations and the supervised signals. The generative model of document \(d\) in CRNTM can be written as:

\[\begin{split}\text{E}_{\text{q}(D)}&\big{[}\sum_{d= 1}^{D}\log p(x_{d}^{1},x_{d}^{2}|u_{d})\big{]}\\ =&\text{E}_{\text{q}(D)}\big{[}\sum_{d=1}^{D}\log p(x _{d}^{1})\big{]}+\text{E}_{\text{q}(D)}[\sum_{d=1}^{D}\log p(x_{d}^{2}|u_{d}) ]\\ \geq&\mathcal{L}_{D_{\text{pre}}}+\mathcal{L}_{D_{ joint}},\end{split} \tag{13}\]

where \(x_{d}^{1}\) and \(x_{d}^{2}\) are two independent copy of \(x_{d}\), which are used in the reconstruction in the the pre-encoding phase and the joint encoding phase, respectively. \(\mathcal{L}_{D_{\text{pre}}}\) denotes the evidence lower bound (ELBO) of the pre-encoding phase, and \(\mathcal{L}_{D_{joint}}\) is the ELBO of the joint encoding phase.

\[\begin{split}\mathcal{L}_{D_{\text{pre}}}&=\text{E} _{\text{q}(D)}\big{[}\text{E}_{\text{q}(x|x_{d})}\big{[}\sum_{n=1}^{N}\log p(x _{d,n}|x_{d})\big{]}\\ &\qquad\qquad\qquad\qquad\qquad\qquad-\mathcal{D}_{KL}(q(x_{d}|x_ {d})\|p(x_{d}))\big{]},\end{split} \tag{14}\]

where \(D_{KL}(\cdot\|\cdot)\) denotes the KL divergence.

According to RSVI, the distribution of the accepted sample \(\epsilon\), \(\pi(\epsilon;\phi)\), can be obtained by marginalizing over a uniform variable \(u\) of the rejection sampler:

\[\pi(\epsilon;\alpha,B)=\int\pi(\epsilon,u;\alpha,B)du=s(\epsilon)\frac{q(h_{T} (\epsilon;\alpha,B))}{r(h_{T}(\epsilon;\alpha,B))}, \tag{15}\]

where \(r(\cdot)\) is the proposal function for the rejection sampler.

Therefore, Eq. (14) can be written as:

\[\begin{split}\mathcal{L}_{D_{\text{pre}}}&=\text{E} _{\text{q}(D)}\big{[}\text{E}_{\pi(\epsilon;\alpha,B)}\big{[}\sum_{n=1}^{N} \log p(x_{d,n}|h_{T}(\epsilon;\alpha,B))\big{]}\\ &\qquad\qquad\qquad\qquad+\text{E}_{\pi(\epsilon;\alpha,B)}\big{[} \log\frac{p(h_{T}(\epsilon;\alpha,B))}{q(h_{T}(\epsilon;\alpha,B)|x_{d})} \big{]}\big{]},\end{split} \tag{16}\]

For the joint encoding phase:

\[\begin{split}\mathcal{L}_{D_{joint}}&=\text{E}_{ \text{q}(D)}\big{[}\text{E}_{\text{q}(C_{d}|x_{d},d_{1})}\big{[}\sum_{n=1}^{N} \log p(x_{d,n}|C_{d})\big{]}\\ &\qquad\qquad\qquad-D_{KL}(q(C_{d}|x_{d},d_{1})\|p(C_{d}|d_{)}) \big{]}.\end{split} \tag{17}\]

Moreover, to optimize the mask parameters in the Mask Layer, we need to minimize the following equation according to Eq. (11) (Hardt et al., 2017):

\[\begin{split}\mathcal{L}_{m}&=\text{E}(\sum_{l=1}^ {K+S}\|C_{l}-g_{l}(A_{l}\circ C)\|^{2}).\end{split} \tag{18}\]

Furthermore, the discovered causal adjacency matrix \(A\) is supposed to be a DAG. On the basis of DAG-GNN (Gerlach et al., 2017), the causal adjacency matrix \(A\) should satisfy the following condition:

\[\begin{split}\text{For any }\rho>0,\text{ the graph is acyclic if and only if:}\\ H(A)&\equiv tr((I+\rho A\circ A)^{(K+S)})-(K+S)=0. \end{split} \tag{19}\]

Since we assume that the latent topical representation and the supervised signals are causally related in this work, we further introduce a counterfactual regularization item following (Gerlach et al., 2017) to ensure the convincingeness of the learned causal DAG structure. It is a commonsense that in a causal relationship, changing the cause leads to a change in the effect, but changing the effect does not lead to a change in the cause. Therefore, for the nodes in the causal DAG, the following equations holds true:

\[\begin{split}\text{causal direction}&:l_{l}\to C_{j} \Rightarrow C(l_{l})\neq C(do(l_{l})),\\ \text{anti-causal direction}&:l_{n}\gets C_{m}\Rightarrow C (l_{n})=C(do(l_{n})),\end{split} \tag{20}\]

where \(\rightarrow\) and \(\leftarrow\) represent the direction of the causal relationship, and \(do(\cdot)\) denotes the do-operation where we set \(l_{l}\neq do(l_{l})\).

Specifically, we further train a binary classifier \(\mathbf{D}\) to distinguish the causal counterfactuals and the anti-causal counterfactuals. The counterfactual representation contrastive regularizer can be written as:

\[\begin{split}\mathcal{L}_{do}&=\text{E}[\text{E}_{ \Omega^{+}}(\mathbf{D}(C(do(l_{l}))))+\text{E}_{\Omega^{-}}(1-\mathbf{D}(C(do(l _{n}))))],\end{split} \tag{21}\]

where \(\Omega^{+}=\{l_{i}|l_{i}\in parents(C_{j}),C_{j}\in\mathbb{C},l_{i}\in\mathbb{I} \},\Omega^{-}=\{l_{n}|l_{n}\in Children(C_{m}),C_{m}\in\mathbb{C},l_{n}\in \mathbb{I}\}\).

To sum up, considering the above Eq. (16), Eq. (17), Eq. (19), Eq. (18), and Eq. (21), we get the overall loss function of the proposed model:

\[\begin{split}\mathcal{L}=-\mathcal{L}_{D_{\text{pre}}}-\mathcal{L} _{D_{joint}}+H(A)+\mathcal{L}_{m}+\mathcal{L}_{do}.\end{split} \tag{22}\]

## 4. Experiments

### Experimental Setup

#### Corpora

We conduct experiments on three public corpora, including _Russian books1_(Kirch and _Kirch_, _Axiw2_ and _StackSample3_). For each corpus, we tokenize the lowereck documents, remove the nltk stop words, and then perform the stemming step based on the nltk SnowballSterner tool4, respectively. The statistics of corpora are listed in Table 1.

Footnote 1: [https://www.kaggle.com/datasets/okdamoexkaya/fiction-corpus-for-agbased-text-classification](https://www.kaggle.com/datasets/okdamoexkaya/fiction-corpus-for-agbased-text-classification)

Footnote 2: [https://www.kaggle.com/datasets/Cornell-University/arxiv](https://www.kaggle.com/datasets/Cornell-University/arxiv)

Footnote 3: [https://www.kaggle.com/datasets/tackover/low-tacksample](https://www.kaggle.com/datasets/tackover/low-tacksample)

Footnote 4: [https://www.kirch.org](https://www.kirch.org)

#### Baselines and Experimental Settings

We compare the proposed model with seven state-of-the-art neural topic models, including GSM5(Hardt et al., 2017), SCHOLAR6(Kirch and _Kirch_, 2017), DVAE7(Dong et al., 2017), HIMECat8(Kirch and _Kirch_, 2017),

\begin{table}
\begin{tabular}{l c c c c} \hline \hline Corpora & Label type & Label & Train & Test & Voc \\ \hline \hline \begin{tabular}{l} _Russian books_ \\ _ArXiv_ \\ _StackSample_ \\ \end{tabular} & \begin{tabular}{l} age rating, genre \\ discipline category \\ \end{tabular} & \begin{tabular}{l} 37 \\ 20 \\ 20 \\ \end{tabular} & \begin{tabular}{l} 4,492 \\ 228,866 \\ 821,724 \\ \end{tabular} & \begin{tabular}{l} 1,000 \\ 10,000 \\ 821,724 \\ \end{tabular} & \begin{tabular}{l} 10,000 \\ 10,000 \\ 821,724 \\ \end{tabular} & 
\begin{tabular}{l} 10,000 \\ 10,000 \\ \end{tabular} \\ \hline \hline \end{tabular}
\end{table}
Table 1. The statistics of the corpora.

supervised BERTopic9[(12)], HSTM10[(29)] and NSEM-GMITM11[(9)].

Footnote 9: [https://github.com/MaartenGr/BERTopic](https://github.com/MaartenGr/BERTopic)

Footnote 10: [https://github.com/dsridhar91/hstim](https://github.com/dsridhar91/hstim)

Footnote 11: [https://github.com/mbathwry/NSEM-GMITM](https://github.com/mbathwry/NSEM-GMITM)

We conduct experiments using a variable topic number of 20 and 50 for the baseline models and our proposed model across each of the three corpora. Following DVAE [(7)], we set the hidden units of our proposed model to 100, and a dropout rate of 0.25 is implemented. The Dirichlet prior is set to 0.01 and the shape augmentation parameter \(B\) is set to 10 both as per the DVAE source code. We set the dimension of the causal topical representation \(H\) to \(\{1,2,4,8,16,32,64,128\}\), and choose \(H=2\) for _Russian books_, \(H=32\) for _ArXiv_ and \(H=128\) for _StackSample_ according to the quality of the learned topics on each training set. We initialize the learning rate at 0.001 and set the batch size to 256. The Adam optimizer is employed to train our model, and the model's performance is monitored on a validation set, employing an early stopping strategy if no improvement is observed over 30 epochs. The parameter settings of the baseline models are kept consistent with those detailed in their respective original papers.

### Evaluation on Topic Quality

In topic modeling, the quality of the learned topics (TQ) [(21)] is typically assessed from two perspectives: topic coherence (TC) [(25)] and topic uniqueness (TU) [(21)]. Topic coherence can measure the semantic similarity between the top words within the same topic. In this paper, we use the normalized pointwise mutual information (NPMI) based topic coherence12. The topic coherence score for topic \(k\) with top \(N\) words can be computed by:

Footnote 12: [https://github.com/jhau/topic_interpretability](https://github.com/jhau/topic_interpretability)

\[TC(k)=\sum_{j=2}^{N}\sum_{l=1}^{j-1}\frac{\log P(\psi_{i}\psi_{j})}{-\log P( \psi_{i},\psi_{j})}. \tag{23}\]

On the other hand, topic uniqueness reflects the discriminative power of a topic in relation to others, indicating the extent to which a topic captures unique aspects of the corpus that are not covered by other topics. TU can be computed by:

\[TU=\frac{1}{K\cdot N}\sum_{k=1}^{K}\sum_{n=1}^{N}\frac{1}{cnt(n,k)}, \tag{24}\]

where \(cnt(n,k)\) is the total number of times the \(n^{th}\) word in topic \(k\) appears in the top words across all the topics. The quantitative measure of topic quality is calculated as the product of these two factors: \(TQ=TC\times TU\). This approach ensures that high-quality topics are both semantically meaningful and distinct from each other.

In the experiments, we compute the mean value of each metric over top-5 and top-10 topical words in the discovered topics. A

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{Model} & \multicolumn{4}{c}{_Russian books_} & \multicolumn{4}{c}{_ArXiv_} & \multicolumn{4}{c}{_StackSample_} \\ \cline{2-13}  & \multicolumn{3}{c}{\(K=20\)} & \multicolumn{3}{c}{\(K=50\)} & \multicolumn{3}{c}{\(K=20\)} & \multicolumn{3}{c}{\(K=50\)} & \multicolumn{3}{c}{\(K=20\)} & \multicolumn{3}{c}{\(K=20\)} & \multicolumn{3}{c}{\(K=20\)} & \multicolumn{3}{c}{\(K=50\)} \\ \cline{2-13}  & TC & TU & TQ & TC & TU & TQ & TC & TU & TQ & TC & TU & TQ & TC & TU & TQ & TC & TU & TQ \\ \hline GSM & 0.200 & 0.325 & 0.065 & 0.172 & 0.271 & 0.047 & 0.143 & 0.630 & 0.090 & 0.154 & 0.563 & 0.087 & 0.190 & 0.400 & 0.076 & 0.162 & 0.396 & 0.064 \\ SCHOLAR & 0.262 & 0.875 & 0.229 & 0.215 & 0.652 & 0.140 & 0.105 & 0.950 & 0.100 & 0.134 & **0.933** & 0.125 & 0.185 & 0.972 & 0.180 & 0.212 & **0.923** & 0.196 \\ DVAE & 0.333 & 0.940 & 0.313 & 0.316 & 0.712 & 0.225 & 0.301 & **0.998** & 0.300 & **0.305** & 0.897 & **0.318** & 0.462 & 0.958 & 0.443 & 0.853 & 0.374 \\ HIMECat & 0.160 & 0.951 & 0.152 & - & - & - & 0.206 & 0.980 & 0.202 & - & - & - & 0.311 & **0.990** & 0.308 & - & - & - & - \\ BERTopic & 0.134 & 0.752 & 0.101 & - & - & - & - & 0.249 & 0.782 & 0.195 & - & - & - & 0.274 & 0.793 & 0.217 & - & - & - \\ HSTM & 0.054 & **0.995** & 0.054 & 0.044 & **0.981** & 0.043 & 0.015 & 0.748 & 0.011 & 0.018 & 0.540 & 0.010 & 0.034 & 0.75 & 0.026 & 0.026 & 0.666 & 0.017 \\ NSEM-GMITM & 0.196 & 0.838 & 0.164 & 0.198 & 0.768 & 0.152 & 0.026 & 0.765 & 0.020 & 0.038 & 0.738 & 0.028 & 0.110 & 0.695 & 0.076 & 0.112 & 0.687 & 0.077 \\ CRNTM & **0.351** & 0.920 & **0.323** & **0.328** & 0.709 & **0.233** & **0.361** & **0.998** & **0.360** & 0.339 & 0.910 & 0.308 & **0.503** & 0.978 & **0.492** & **0.501** & 0.846 & **0.424** \\ \hline \hline \end{tabular}
\end{table}
Table 2. A comparison of the topic coherence (TC), topic unique (TU) and topic quality (TQ). We compute the mean value of each metric over top-5 and top-10 topical words, and higher value represents better performance. The best results are in bold. See more details in section 4.2.

\begin{table}
\begin{tabular}{c c c c c c c c c c c c c c} \hline \hline solar activity & astrochemistry & galactic composition & hardware & telecommunication & astronomy & particle physics & carcinology & W-Fi & NLP & 699 \\ \hline CMEs & desorption & GCS & GPUs & multiple-input & Jupiter & seesaw & tumor & backhaul & multilingual & 600 \\ rope & Deterium & Gyr & FPGA & precoding & close-in & B-L & lung & downlink & cross-lingual & 601 \\ CME & gas-phase & metal-rich & FPGA & MIMO & planet & teleggenesis & breast & uplink & monolingual & 602 \\ reconnection & photodissociation & FeH & CPUs & multiple-output & TESS & vector-like & cancer & NOMA & low-resource & 602 \\ corona & CH3OH & bulge & HPC & beamform & super-earth & NNSM & liver & D2D & bilingual & 603 \\ footprint & HCN & gas-rich & CUDA & CISM & externalaker & CP-Even & malignant & QoS & NMT & 604 \\ magnetogram & isotopologues & galactocentric & Intel & downlink & G & MSSM & module & offload & BERT & 604 \\ cryogenic & r-process & mass-to-light ratio & Nvidia & OFDM & Neptune & system & prostate & relay & corpora & 604 \\ Alfvén & prestellar & Alpha/Fe & CPU & CSI & semi-major & neutrino & lesion & 5G & BLEU & 606 \\ Hinode & H2 & globular & NISQ & multi-antenna & semimajor & R-Parity & histology & caching & PLMs & 607 \\ \hline
0.50 & 0.34 & 0.40 & 0.41 & 0.54 & 0.42 & 0.37 & 0.48 & 0.39 & 0.46 & & 608 \\ \hline \hline \end{tabular}
\end{table}
Table 3. Examples of the top-10 words per topic and the corresponding topic coherence (TC) values on _ArXiv_.

higher value indicates better results for all the above three metrics. The experimental results are shown in Table 2. The missing value for HIMECat and BERTopic is attributable to the fact that the number of topics in these models is inherently linked to the total number of supervision signals. Therefore, we adjust the topic count to match the label count of the corresponding corpora.

The results demonstrate that our proposed CRNTM outperforms other models in most cases, particularly concerning two crucial metrics - topic coherence and topic quality. Despite a slightly lower topic uniqueness compared to SCHOLAR, HIMECat and HSTM in some cases, the topic coherence and overall metric topic quality of CRNTM significantly exceed those of these three models. Actually, topic uniqueness typically becomes a valuable reference primarily when the topics generated by the model are semantically meaningful. Hence, TU is more meaningful when the topics are both unique and coherent, underlining the importance of balancing these two metrics in topic modeling.

CRNTM outperforms all other baselines on topic quality metric, with the exception of DVAE in some cases. Compared to DVAE, our model achieves better results in terms of topic coherence, topic uniqueness, and topic quality, except some occasional cases where it falls slightly short. This indicates that our model can effectively learn high-quality topics. Additionally, our model is capable of discovering causal relationships between supervised information and latent topics, an achievement that other baselines fail to accomplish. This unique capability further enhances the robustness and interpretability of our model, providing an essential tool for deeper understanding in topic modeling.

We further display the top-10 words of some example topics learned from _ArXiv_. The topic names in the first line are manually assigned based on the topical words. For ease of presentation and comprehension, the top words have been lemmatized from the stem forms in the second line. The real numbers below the words represent topic coherence values. These examples demonstrate the semantic coherence and interpretability of the topics discovered by our proposed model.

### Causal Relationships between Supervision Information and Latent Topics

We demonstrate the discovered causal relationships between the supervised information and the latent topics to show the ability of our model in causal relationship discovery. We extract the causal relationships from the learned weighted adjacency matrix by using a thresholding value 0.3 to rule out cycle-inducing edges following NOTEARS (Song et al., 2017), since a small threshold suffices to rule out cycle-inducing edges. Figure. 4 is the learned weighted adjacent matrix of the causal relationship DAG on _ArXiv_ with 20 topics. The first 20 nodes are the learned latent topics, and the last 20 ones are the supervised variables. In the adjacent matrix, the causal relationships between different topics, between supervised information, and between supervised information and the topics are denoted with different colors. Figure. 4 confirms that our model is capable of simultaneously constructing causal relationship between supervised signals and topics, as well as within each of them individually.

Figure 4. The weighted adjacent matrix of the causal relationship DAG on _ArXiv_ with 20 topics. See more details in section 4.3.

Figure 3. Examples of the causal relationships between and within the supervised information and the latent topics. The topic 770 names are manually assigned based on the top-5 words. The spatial relative positions between different variables are merely for illustrative purposes, and bear no relation to the causal relationships between the variables. In other words, these relationships do not form a hierarchical structure. See more details in section 4.3.

To further demonstrate the learned causal relationships, we select several discipline category (supervised information) and topics from the DAG on _ArXiv_ with 50 topics shown in Figure. 3. From the examples, we can see that the discovered causal relationships are credible and interpretable to a certain extent. For example, for the causal relationship between the supervised information and the latent topics, the causal chain from the topic _"quantum interactions"_ to the label _"cond-mat"_ reflects the impact of quantum effects on the microscopic state of matter. The topic _"photonics technology"_ \(\rightarrow\) the label _"cond-mat"_ is reasonable because new photonics technology can be used to study and control the macroscopic state of matter. For the causal relationships between topics, the causal chain from the topic _"solar activity"_ to the topic _"particle cosmology"_ reveals that it may be reasonable for solar activity to affect particle behavior and distribution in the universe, because solar activity produces a large number of high-energy particles and radiation, which can affect particle cosmology. The topic _"galactic composition"_ \(\rightarrow\) the topic _"particle cosmology"_ shows the effect of the composition of the galaxy on particle cosmology, for the composition of the galaxy can affect the behavior and distribution of particles within it. Furthermore, the proposed model can also uncover causal relationships between supervised signals, such as causal relationships among label _"hep-ex"_, _"math-ph"_ and _"physics"_.

### Ablation Study

To study the contribution of each component of our model, we consider the following three types of components:

* The DAG: The restricted condition on the directed acyclic nature of DAGs (\(H(A)\)); the counterfactual regularization in causal relationships (\(\mathcal{L}_{do}\)); and the Mask Layer on causal structure (\(\mathcal{L}_{m}\)).
* The prior distribution: the prior distribution of the document vectors, Gaussian distribution or Dirichlet distribution.
* The encoding phase: whether to use the pre-encoding phase.

We ablate different components in nine cases: without the Mask Layer loss (\(\#2\) in Table 4); without the condition on the directed acyclicity (\(\#3\)); without the counterfactual regularization regularization (\(\#4\)); without directed acyclicity and counterfactual (\(\#5\)); without the above three parts (\(\#6\)); without the DAG network structure and its related loss, and the model degrade into the baseline DVAE (\(\#7\)), without the pre-encoding phase (\(\#8\)); replace the Dirichlet distribution with Gaussian distribution (\(\#9\)); replace the Dirichlet distribution with Gaussian distribution and without the DAG network structure, and the model degrade into the baseline GSM (\(\#10\)).

Table 4 shows the topic quality results of the ablation study experiment on _StackSample_ under 50 topics. The complete CRNTM model achieves the best performance across most metrics, and achieves the best overall topic quality. The removal of each part of optimizing the DAG leads to a noticeable drop in the performance. This indicates that our assumption of a directed acyclic causal relationship existing between the supervised information and the latent topics is reasonable. Incorporating the directed acyclic causal relationship in topic modeling can effectively enhance the topic discovering capability of the model and guide the model to better understand and capture the underlying structure of the corpus, leading to more accurate and robust topic modeling. Moreover, the prior distribution of the document vectors is confirmed to be a important role in discovering interpretable topics, for models under the Dirichlet distribution outperform than that under the Gaussian distribution.

Furthermore, the model without the pre-encoding phase demonstrates a significant reduction in topic coherence compared to the complete model. This indicates the effectiveness of our model's pre-encoding phase, which is capable of mapping the crucial semantic information from the input documents to the latent topic space to provide ample semantic information in discovering causal relationships. The pre-encoding phase ensures that the VAE framework can strike a balance between learning the semantic information of the latent topics and the causal relationship structure of the topics. This allows the model to capture the intricate relationships between topics and their semantic, leading to a more coherent and interpretable topic model. In summary, each component of CRNTM contributes significantly to its performance.

## 5. Conclusion

In this paper, we undertake an exploration into the causal relationships between and within the supervised information and latent topics in neural topic modeling. We propose Causal Relationship-Aware Neural Topic Model (CRNTM), a novel approach designed to automatically unravel significant causal relationships in supervised information and latent topics, while concurrently discovering high-quality topics, thereby enhancing the overall interpretability of the model. We conceptualize these causal relationships as directed edges within a Directed Acyclic Graph (DAG), treating both supervised information and latent topics as nodes. We employ a Structural Causal Model (SCM) to imbue the representations of the supervised information and the latent topics with causality, modeling these interactions within the causal relationship DAG. The experimental results confirm the reliability and interpretability of the causal relationships uncovered. Moreover, they underscore the high quality of the learned topics.

\begin{table}
\begin{tabular}{l l r r r r r r} \hline \hline \multirow{2}{*}{\#} & \multirow{2}{*}{Model} & \multicolumn{4}{c}{\(K=20\)} & \multicolumn{4}{c}{\(K=50\)} \\ \cline{3-8}  & & & TC & TU & TQ & TC & TU & TQ \\ \hline
1 & CRNTM & **.503** & **.978** & **.492** & **.501** & **.846** & **.424** & \\
2 & w/o \(\mathcal{L}_{m}\) & 4.95 &.972 &.481 &.463 &.872 &.404 & \\
3 & w/o \(H(A)\) & 4.26 &.912 &.389 &.444 &.872 &.387 & \\
4 & w/o \(\mathcal{L}_{do}\) & 4.90 &.953 &.467 &.404 &.831 &.336 & \\
5 & w/o \((H(A)+\mathcal{L}_{do})\) & 4.65 &.925 &.430 &.458 &.894 &.409 & \\
6 & w/o \((\mathcal{L}_{m}+H(A)+\mathcal{L}_{do})\) & 4.41 &.940 &.415 &.454 &.884 &.401 & \\
7 & w/o DAG (DVAE) & 4.62 &.958 &.443 &.438 &.853 &.374 & \\
8 & w/o pre-encoding phase & 3.62 &.962 &.348 &.340 &.957 &.325 & \\
9 & \(\mathcal{N}(\cdot)\)+DAG &.307 & **.978** &.300 &.246 &.942 &.232 & \\
10 & GSM &.190 &.400 &.076 &.162 &.396 &.064 & \\ \hline \hline \end{tabular}
\end{table}
Table 4. A comparison results of the ablation experiments on _StackSample_ under 50 topics. See more details in section 4.4.

[MISSING_PAGE_FAIL:9]