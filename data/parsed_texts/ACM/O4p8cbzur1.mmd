Low Mileage, High Fidelity: Evaluating Hypergraph Expansion Methods by Quantifying the Information Loss

Anonymous Author(s)

###### Abstract.

Hypergraphs are typically used for solving downstream tasks in two steps: expanding a hypergraph into a conventional graph, known as the _hypergraph expansion_, and conducting machine learning methods on the expanded graph. Depending on how hypergraph expansion is performed, certain information of the original hypergraph may be lost, which negatively affects the accuracy of downstream tasks.

If the amount of information loss can be measured, one can select the best hypergraph expansion procedure and target a better downstream performance. To this end, we propose a novel framework, named the MILEAGE, to evaluate hypergraph expansion methods by measuring their degree of information loss. MILEAGE employs the following four steps: (1) expanding a hypergraph; (2) performing the unsupervised representation learning on the expanded graph; (3) reconstructing a hypergraph based on vector representations obtained; and (4) measuring the MILEAGE-score (_i.e._, mileage) by comparing the reconstructed and the original hypergraphs. To demonstrate the usefulness of MILEAGE, we conduct experiments via downstream tasks on three levels (_i.e._, node, hyperedge, and hypergraph): node classification, hyperedge prediction, and hypergraph classification on eight real-world hypergraph datasets. We observe that the average and minimum Pearson correlation coefficient between the mileage of expanded graphs and the performance of the downstream task are -0.871 and -0.904, respectively. The results validate that information loss through hypergraph expansion has a negative impact on downstream tasks and MILEAGE can effectively evaluate hypergraph expansion methods through the information loss and recommend a new method that resolves the problems of existing ones.

hypergraph, hypergraph expansion, information loss +
Footnote †: ccs: Computing methodologies Machine learning

+
Footnote †: ccs: Computing methodologies Machine learning

+
Footnote †: ccs: Computing methodologies Machine learning

+
Footnote †: ccs: Computing methodologies Machine learning

+
Footnote †: ccs: Computing methodologies Machine learning

+
Footnote †: ccs: Computing methodologies Machine learning

+
Footnote †: ccs: Computing methodologies Machine learning

+
Footnote †: ccs: Computing methodologies Machine learning

+
Footnote †: ccs: Computing methodologies Machine learning

+
Footnote †: ccs: Computing methodologies Machine learning

+
Footnote †: ccs: Computing methodologies Machine learning

## 1. Introduction

_Graphs_, typically defined as a set of nodes connected by edges, are ubiquitous in representing structural information in a variety of domains, ranging from social networks in the Web to molecule structures in biochemistry (Gardner et al., 2012; Kessler et al., 2013; Kessler et al., 2013; Kessler et al., 2013). However, graphs have limits in representing real-world relationships that are not always _pairwise_. Indeed, in the real-world, there are often _tuplewise relationships_ that involve more than two objects (Gardner et al., 2012; Kessler et al., 2013). For instance, research papers can be published through the collaboration of more than two co-authors (Kessler et al., 2013); group communications are increasingly common comparing to 1-on-1 communications in online platforms (Brockman et al., 2013; Kessler et al., 2013); and more than two items are often purchased together by users (Gardner et al., 2012) in e-business platforms. Since edges in a conventional graph (a.k.a., graph) encode only _pairwise relationships between two nodes_, they cannot readily represent these tuplewise relationships (Gardner et al., 2012; Kessler et al., 2013; Kessler et al., 2013).

A _hypergraph_ is a generalization of a graph, consisting of nodes and hyperedges (Gardner et al., 2012; Kessler et al., 2013). A _hyperedge_ associates an arbitrary number of nodes, and it enables a hypergraph to represent tuplewise relationships (Gardner et al., 2012; Kessler et al., 2013). Figure 1 shows an example of representing co-authorship information of three papers (Figure 1-(a)) in a hypergraph (Figure 1-(b)) and in a graph (Figure 1-(c)). In the case of a hypergraph, a set of authors who collaborated on the same paper are grouped in a single hyperedge (Brockman et al., 2012), which is clearly shown as a circle in Figure 1-(b). As a result, in the hypergraph, we can correctly infer who collaborated together on a paper (Gardner et al., 2012; Kessler et al., 2013). On the other hand, using a graph, _any possible pair_ of authors who collaborated on any paper are connected by an edge (Kessler et al., 2013), as shown in Figure 1-(c). Since an edge indicates a _pairwise_ co-authorship, in the graph, we can _only_ know whether a pair of authors have collaborated or not (or how frequently they collaborate) but cannot accurately identify who else has collaborated with both of them on a paper.

Thanks to the expressive power of hypergraphs, there have been many attempts to utilize hypergraphs for solving various downstream tasks such as recommendation (Gardner et al., 2012; Kessler et al., 2013), node classification (Gardner et al., 2012; Kessler et al., 2013); Kessler et al. (2013), community detection (Kessler et al., 2013; Kessler et al., 2013); Kessler et al. (2013), and hyperedge prediction (Gardner et al., 2012; Kessler et al., 2013). Hypergraph mining has often been validated to provide better results, compared to those utilizing graphs. Most of these attempts first expand a hypergraph into a graph, which is named as the _hypergraph expansion_(Kessler et al., 2013), and then conduct machine learning or deep learning on the expanded graphs (Kessler et al., 2013; Kessler et al., 2013; Kessler et al., 2013; Kessler et al., 2013; Kessler et al., 2013).

Figure 1. Co-authorship as a hypergraph and a graph.

The main reason for doing hypergraph expansion is that there are a wide variety of mature algorithms and tools proposed for handling graphs [14; 26; 34; 42] while models particularly designed for hypergraphs are scarce [11; 37]. As a result, while much attention has been put on how to process the extended graph, there has been rare investigation on which hypergraph expansion is more reasonable.

Indeed, there are at least four approaches to hypergraph expansion in literature: (1) _clique expansion_ (in short, **C**) [35], (2) _star expansion_ (in short, **S**) [2], (3) _multi-level decomposition_ (in short, **M**) [11], and (4) _time expansion_ (in short, **L**) [44]. In addition, although not explicitly proposed as hypergraph expansion methods, combining the clique and star expansion methods (in short, **CS**) can be a possible hypergraph expansion method.

Downstream tasks are typically built upon one of the above hypergraph expansion methods [7; 8; 12; 18; 21; 39; 44; 46]. However, these studies tend to choose a hypergraph expansion method _without undergoing comparative analysis_, unlike what they do for selecting the machine learning methods for the expanded graphs, which are _carefully evaluated_ before deployment. Unfortunately, there is not a clear answer about which hypergraph expansion method is superior and there is also lack of empirical results that systematically compare them. To fill this gap, this paper presents a comprehensive and comparative analysis of hypergraph expansion methods.

We approach this problem by the observation that in hypergraph expansion, every tuplewise relationship is transformed into a set of pairwise relationships. Through this process, certain information encoded by the original hypergraph may have been lost, which may affect the accuracy of downstream tasks negatively. In this paper, we formally define such a problem as the _information loss_ in hypergraph expansion. We further categorize the information loss in the hypergraph expansion into the following three types:

* **Non-recoverability:** a problem one cannot recover the original hypergraph precisely from its expanded graph.
* **Tie-weakening:** a problem where the tie strength between nodes belonging to the same hyperedge becomes weaker than that in the expanded graph.
* **Multi-cloning:** a problem where a single node in the original hypergraph is represented by multiple nodes in its expanded graph

When conducting a downstream task, if we can employ a hypergraph expansion method that provides an expanded graph with the _least information loss_, it is expected to provide a higher accuracy in the downstream task. However, to the best of our knowledge, there is _no current way to measure the degree of the information loss_ quantitatively. This raises a new research question: how can we quantify the information loss through different hypergraph expansion methods?

To answer this question, we start with an intuition as follows. Given a hypergraph, denoted as \(H_{o}\), and an expanded graph, denoted as \(G_{1}\), expanded via a hypergraph expansion method, it is crucial for \(G_{1}\) to preserve the topology characteristics of \(H_{o}\) as much as possible [44]. _Unsupervised representation learning_ (URL) aims to represent nodes in a graph as vectors in a low-dimensional embedding space, where it is essential for these vectors to preserve the underlying topological characteristics of the graph [15; 41]. If we conduct URL on \(G_{1}\) and subsequently reconstruct a hypergraph, denoted as \(H_{r}\), by using the vectors thus obtained, we can quantify the difference between \(H_{r}\) and \(H_{o}\), denoted as \(M_{1}\). Moreover, if we repeat the same process with another expanded graph \(G_{2}\) obtained by a different hypergraph expansion method and quantify the difference \(M_{2}\) in the same way, the difference between \(M_{1}\) and \(M_{2}\) primarily stems from the difference in the two hypergraph expansion methods.

Based on this intuition, in this paper, we propose a novel framework to evaluate hypergraph expansion methods by measuring the information loss in expanded graphs, named the MILEAGE (_Measuring the Information Loss of Expanded graphs via reconstructing the hypErgraph_). It consists of the following four steps: (1) expanding a hypergraph into an expanded graph; (2) performing URL on the expanded graph; (3) reconstructing a hypergraph based on the vectors obtained at step (2); and (4) measuring the MILEAGE-score based on the similarity between two sets of hyperedges, one from the reconstructed hypergraph and the other from the original hypergraph.

In this paper, we validate MILEAGE via extensive experiments with eight real-world hypergraphs. If MILEAGE is appropriately designed to evaluate the hypergraph expansion methods through the information loss, we expect to observe a strong _(negative) correlation_ between the MILEAGE-score (or mileage thereafter) of the expanded graph and the accuracy of a downstream task conducted on this expanded graph. To validate this claim, we conduct downstream tasks at three-levels (_i.e._, node, hyperedge, and hypergraph) including node classification, hyperedge prediction, and hypergraph classification. We observe that the average and minimum Pearson correlation coefficient (PCC) [17] between the mileage of the extended graphs and the accuracy of downstream tasks are -0.871 and -0.904 (_i.e._, fairly high), respectively, across all tasks on all hypergraphs, which indicates MILEAGE is well designed to evaluate the hypergraph expansion methods in terms of the information loss.

We then evaluate the goodness of existing hypergraph expansion methods via MILEAGE, which is interpretable (_i.e._, can be explained in terms of the information loss), general (_i.e._, doesn't depend on particular URL methods or datasets), easy to compute, and indicative of the accuracy of downstream tasks. We observe that **CS** generates expanded graphs with the least mileage compared to other hypergraph expansion methods; furthermore, employing **CS** leads to the highest accuracy in all the downstream tasks.

The contributions of this paper are summarized as follows:

* **Problems.** We define three information loss problems (_i.e._, non-recoverability, tie-weakening, and multi-cloning) appearing in the hypergraph expansion and show that they negatively affect the accuracy of downstream tasks.
* **Novel framework.** We propose a new framework, the MILEAGE, to evaluate hypergraph expansion methods by measuring the degree of the information loss occurring in hypergraph expansion.
* **Extensive evaluation.** Through extensive experiments using eight real-world hypergraph datasets, we first validate the effectiveness of MILEAGE and evaluate the goodness of existing hypergraph expansion methods via MILEAGE. To the best of our knowledge, this is the first paperwork to provide a comprehensive and comparative analysis of hypergraph expansion methods.
* **Recommendation.** Through the information loss analysis and evaluations, we are able to recommend a new and better hypergraph expansion method, a combination of the clique and star expansion (**CS**), which leads to the lowest mileage in expanded graphs and the best performance in downstream tasks.

## 2. Related Work

### Hypergraph Expansion Methods

In the literature, four hypergraph expansion methods have been proposed: (1) _clique expansion_(35); (2) _star expansion_(2); (3) _multi-level decomposition_(11); and (4) _line expansion_(44).

Clique expansion (in short, **C**), as shown in Figure 2-(a), expands a hypergraph into a graph(35), where a node corresponds to a node in the hypergraph and an edge does a pair of nodes belonging to the same hyperedge in the hypergraph. As a result, each hyperedge in the hypergraph is represented as a _clique structure_ in the graph.

Star expansion (in short, **S**), as shown in Figure 2-(b), expands a hypergraph into a bipartite graph(2). In a bipartite graph, a node in one side corresponds to a node in the hypergraph, _i.e._, we refer to this node as an _n-node_, and a node in the other side corresponds to a hyperedge in the hypergraph, _i.e._, we refer to this node as an _h-node_; an edge between an _n-_node and an _h-_node represents the relationship between a node and its belonging hyperedge in the hypergraph.

Multi-level decomposition (in short, **M**), as shown in Figure 2-(d), expands a hypergraph into \(m\) (decomposed) graphs(2). In the level-_i_ (decomposed) graph, a node corresponds to a possible combination of \(i\) nodes belonging to the same hyperedge, and an edge between nodes indicates the two nodes come from the same hyperedge in the hypergraph. The \(m\) is the largest size of a hyperedge in the hypergraph. The level-1 (decomposed) graph is equivalent to the graph obtained by the clique expansion.

Line expansion (in short, **L**), as shown in Figure 2-(e), expands a hypergraph into a _line graph_(44). In a line graph, a node (_i.e._, line node) represents a pair of a node and a hyperedge that the node belongs to and an edge between two line nodes represents (1) the two line nodes have the same hyperedge or (2) they have the same node in the hypergraph.

In addition, although not explicitly proposed in the literature, we can consider a possible hypergraph expansion method by combining **C** and **S** (in short, **CS**). **CS** expands a hypergraph into a heterogeneous graph, as shown in Figure 2-(c), that is a combination of a graph and a bipartite graph obtained by the clique expansion and the star expansion, respectively. In the heterogeneous graph, a node corresponds to a node (_i.e._, an _n_-node) or a hyperedge (_i.e._, an _h_-node) in the hypergraph. There are two types of edges: (1) one connects a pair of nodes belonging to the same hyperedge in the hypergraph (_i.e._, an edge between _n_-nodes) and (2) the other connects the relationship between a node and its belonging hyperedge in the hypergraph (_i.e._, an edge between an _n_-node and an _h_-node).

### Hypergraph Learning for Downstream Tasks

There have been many attempts to propose the methods that utilize hypergraphs in solving downstream tasks such as recommendation(18; 46), node classification(12; 44), community detection(9; 38), and hyperedge prediction(21; 37; 47). They commonly proceed in two steps: (1) to conduct the hypergraph expansion and (2) to employ machine learning or deep learning methods on the graph obtained by the hypergraph expansion.

These methods were devised after careful consideration on which machine learning (_e.g._, Louvain(4) or graph cut(10)) or deep learning(_e.g._, GCN(23) or self-attention(40)) methods to be employed for a specific downstream task. However, there has been little discussion regarding the selection of hypergraph expansion methods to be used. If a better hypergraph expansion method is chosen after careful analysis of existing hypergraph expansion methods, it is expected that the accuracy of downstream tasks can be further improved. To this end, this paper aims to conduct a comprehensive analysis of hypergraph expansion methods.

## 3. Information Loss in Hypergraph Expansion

When a hypergraph is expanded into a graph through the hypergraph expansion, the information of tuplewise relationships that were explicitly represented in the hypergraph may not be fully preserved. This occurs because the hypergraph represents tuplewise relationships by using hyperedges, whereas the graph represents the tuplewise relationships by only using edges, which are _pairwise_ in nature. As a result, the explicit representation of tuplewise relationships in the hypergraph may not be fully preserved in the graph, leading to degradation of the downstream task accuracy. In this paper, we define such a problem as _information loss_. In this section, we present the information loss problem associated with each hypergraph expansion method introduced in Section 2.

**Clique expansion (C).** As mentioned in Section 2, **C** represents a hyperedge in a hypergraph as a _clique structure_ in a graph, which is the simplest and most intuitive method for representing tuplewise relationships by using pairwise relationships. Due to this simplistic representation, however, **C** cannot fully preserve the precise information in tuplewise relationships (_i.e._, the hyperedge information in the original hypergraph). This is because every clique structure in the graph can be considered as a hyperedge, regardless of whether it exists due to a hyperedge in the original hypergraph or not.

For example, we see that three nodes \(v_{a}\), \(v_{b}\), and \(v_{c}\) form a hyperedge in Figure 2-(f). However, in the graph obtained by **C** as shown in Figure 2-(a), it is difficult to distinguish whether the three nodes \(v_{a}\), \(v_{b}\), and \(v_{c}\) form a single hyperedge or three pairs of nodes \(v_{a}\) and \(v_{b}\), \(v_{b}\) and \(v_{c}\), and \(v_{a}\) and \(v_{c}\) form separate three hyperedges. Therefore, if we want to recover the original hypergraph from the graph, various hypergraphs, including the original hypergraph, can be considered (Figure 2-(a)). In this paper, we define an information loss problem that can not recover the original hypergraph precisely from its expanded graph as _non-recoverability_.

To achieve high accuracy in downstream tasks, it is crucial to learn the precise information in tuplewise relationships in a hypergraph(7). However, the graph obtained by **C** loses some information in hyperedges, making it difficult to learn precise information in the tuplewise relationships. As a result, this information loss could lead to a degradation in the accuracy of downstream tasks.

**Star expansion (S).** Unlike the graph obtained by **C**, the bipartite graph obtained by **S** represents the relationships between nodes and hyperedges, rather than the relationships between nodes. Therefore, the bipartite graph precisely preserves the tuplewise relationships in the hypergraph, indicating that **S** does not suffer from the problem of non-recoverability.

However, we note that there are no edges between nodes of the same side (_i.e._, edges between _n_-nodes or _h_-nodes) in a bipartitegraph. Therefore, while the original hypergraph represents relationships among nodes _directly_, the bipartite graph represents the relationships among _n_-nodes _indirectly_ through _h_-nodes. As a result, there is a problem where the tightness of relationships between nodes in the bipartite graph is weaker than that in the original hypergraph.

For example, in Figure 2-(f)-(f), two nodes \(v_{a}\) and \(v_{b}\) have a direct relationship within the blue hyperedge. However, when this hypergraph is expanded into a bipartite graph by **S**, the relationship between the two _n_-nodes needs to be inferred through the _h_-node \(h_{1}\) as shown in Figure 2-(b)-(f) and Figure 2-(b)-(f). In this paper, we define an information loss problem where the tie between nodes in an expanded graph becomes weaker, compared to that in its original graph, as _tie-weakening_.

In order to provide a higher accuracy in a downstream task, hypergraph-based methods learn relationships between nodes by employing the following intuition (Zhu et al., 2017): the nodes belonging to the same hyperedge have strong relationships and the nodes belonging to different hyperedges have weak relationships. However, in the bipartite graph, the relationships between _n_-nodes belonging to the same hyperedge are _indirectly_ represented through the _h_-node. Consequently, the relationships between _n_-nodes in the bipartite graph are represented weaker, compared to the actual connections in the original hypergraph, thereby likely to affect the accuracy of downstream tasks.

**Multi-level decomposition (M).****M** theoretically preserves all tuple-wise relationships in the hypergraph by setting the number of levels (_m_) as the largest size of a hyperedge. However, setting \(m\) in this way requires to manage a large number of graphs, resulting in significant requirements of storage space and computation time. Therefore, \(m\) is usually set as much smaller than the largest size of hyperedges in practice. For example, in (Garfinkel et al., 2018), the largest size of hyperedges is 9,705,709 in a real-world hypergraph used in experiments, but \(m\) is set to only 4 for experimental purposes. In such cases, **M** suffers from non-recoverability. However, this method does not suffer from the problem of tie-weakening. This is because, as already mentioned in Section 2, the level-1 (decomposed) graph is equivalent to the graph obtained by **C**, and it demonstrates that this decomposed graph represents the direct relationships among the nodes in a hypergraph.

We note that, due to the use of multiple decomposed graphs to preserve hyperedge information precisely, a single node in the hypergraph is represented as multiple nodes in decomposed graphs. For example, in Figure 2-(f), the node \(v_{d}\) is represented in decomposed graphs as follows: in level-1, it is represented as \(v_{d}\) itself; in level-2, it is represented as \(v_{ad}\) and \(v_{cd}\); and in level-3, it is represented as \(v_{rad}\) (Figure 2-(d)). To conduct downstream tasks on decomposed graphs, it is essential to merge the information of multiple nodes in decomposed graphs for representing their corresponding node in the hypergraph. At this time, depending on merging strategies, the merged information may be inappropriate to represent the node in the original hypergraph, which may negatively affect the accuracy of downstream tasks. In this paper, we define such an information loss problem as _multi-cloning_.

**Line expansion (L).** As mentioned in Section 2, a _line node_ in the line graph represents a pair of a hyperedge and a node in an original hypergraph. Therefore, the line graph preserves all tuplewise relationships, which indicates that **L** does not suffer from non-recoverability. Moreover, the line nodes corresponding to the nodes belonging to the same hyperedge are directly connected by edges. This connectivity preserves the tie among the nodes in the same hyperedge; therefore, tie-weakening does not occur in **L**. However, similar to **M**, a single node in the hypergraph can be represented as multiple line nodes in the line graph as shown in Figure 2-(e). Therefore, **L** suffers from the problem of multi-cloning.

**Combining Clique and Star Expansions (CS).** In the case of **CS**, the problem of non-recoverability is addressed by incorporating

Figure 2. Three types of information losses in hypergraph expansion methods.

S, and the problem of tie-weakening is addressed by incorporating **C**. Therefore, this method is free from both problems of non-recoverability and tightness-weakening. In addition, since the nodes in the hypergraph are not represented by multiple nodes in the expanded graph, there is no multi-cloning as well.

Here, we provide a summary of the three information loss problems associated with each expansion method in Table 1.

## 4. MILEAGE: The Proposed Framework

If we employ a hypergraph expansion method that provides an expanded graph with a smaller information loss in conducting a downstream task, it can be expected to provide more-accurate results. Unfortunately, there is no such work that aims to measure the degree of the information loss of expanded graphs. This motivates us to propose a framework, named the MILEAGE, to evaluate the hypergraph expansion methods.

**Overview.** MILEAGE evaluates hypergraph expansion methods by measuring the mileage (_i.e._, the degree of the information loss) based on the following intuition. A graph (say, \(G_{1}\)) expanded by a hypergraph expansion method from a hypergraph (say, \(H_{o}\)) should preserve the topology of \(H_{o}\) as much as possible [44]. _Unsupervised representation learning_ (URL) aims to represent nodes in a graph as vectors in a low-dimensional embedding space by leveraging the topology of the graph; it is essential for these vectors to preserve the underlying topological properties of the graph [15, 41]. If we conduct URL on \(G_{1}\) and reconstruct a hypergraph (say, \(H_{r_{1}}\)) based on the vectors obtained by URL, we can quantify the mileage (\(M_{1}\) of \(G_{1}\)) by simply measuring the difference between \(H_{r_{1}}\) and \(H_{o}\).

We note that the difference between \(H_{r_{1}}\) and \(H_{o}\) can be attributed not only to the information loss obtained through the _hypergraph expansion_ but also to other factors, such as _URL_ methods and _hypergraph reconstruction_ methods. Thus, it is difficult to say that the information loss solely contributes to \(M_{1}\). However, if we repeat the same process with another expanded graph \(G_{2}\) obtained by a different hypergraph expansion method and measure the information loss \(M_{2}\) of \(G_{2}\) in the same way, the difference between \(M_{1}\) and \(M_{2}\) primarily stems from the difference in the two hypergraph expansion methods employed. Therefore, we can evaluate hypergraph expansion methods _by quantifying_ the information loss.

Figure 3 illustrates an example of the overall process of MILEAGE to quantify the information loss of an expanded graph obtained by **S** from a given hypergraph based on the above intuition. It consists of four components: (1) _hypergraph expansion_, (2) _node representation_, (3) _hypergraph reconstruction_, and (4) _MILEAGE-score computation_. We describe each component in detail in the rest of this section and provide the time complexity analysis of MILEAGE in Appendix A.1.

**Hypergraph expansion.**_Hypergraph expansion_ (Figure 3-(a)) is a component that transforms a given hypergraph into an expanded graph, selecting one of the five hypergraph expansion methods (_i.e._, **C**, **S**, **M**, **L**, or **CS**) mentioned in Section 2.

**Node representation.**_Node representation_ (Figure 3-(b)) is a component that represents nodes of the expanded graph into vectors in a low-dimensional space by leveraging the topological characteristics of the graph. We note that _our MILEAGE is agnostic to URL methods_; thus, any URL method can be employed. In this paper, we employ DGI [41], which is a widely used URL method and provides the best result in our case.1

Footnote 1: The issues of selecting URLs and negative sampling methods are described in detail in EQ2 and EQ4 in Section 5.2, respectively.

**Hypergraph reconstruction.**_Hypergraph reconstruction_ (Figure 3-(c)) is a component that reconstructs a hypergraph based on the vectors obtained from node representation. In this paper, we conduct hypergraph reconstruction in the same manner as done in the previous work [37, 47], based on the vectors obtained from node representation. This approach first conducts hyperedge prediction to determine a set of hyperedges whose number is equal to the number of the original hyperedges. Then, it reconstructs a hypergraph where the predicted hyperedges are considered as its hyperedges.

To facilitate hyperedge prediction in a practical sense, it is necessary to have a set of candidate hyperedges for determining the final hyperedges [21, 37, 47]. To this end, we begin by constructing a set of candidate hyperedges composed of existing hyperedges (_i.e._, negative positive hyperedges) and non-existing hyperedges (_i.e._, negative hyperedges). To generate negative hyperedges, we employ _Clique Negative Sampling_[21, 31]2, which is commonly used in negative sampling, and generate negative hyperedges whose number is equal to the number (\(h\)) of positive hyperedges. This sampling method selects a random hyperedge from positive hyperedges and replaces a random constituent node in the hyperedge with a random node that is adjacent to all other constituent nodes but belongs to different hyperedges.

Footnote 2: The issues of selecting URLs and negative sampling methods are described in detail in EQ2 and EQ4 in Section 5.2, respectively.

Afterward, to determine \(h\) hyperedges among the \(2h\) candidate hyperedges that are most likely to be true positive hyperedges, we measure the degree of positiveness for all \(2h\) hyperedges by using the prediction approach below. Based on the measured scores, we choose the top-\(h\) hyperedges as positive hyperedges and regard them as actual hyperedges in the reconstructed hypergraph. To achieve this, we employ two prediction approaches: (1) _heuristic-based prediction_ and (2) _model-based prediction_. Both approaches rely on the intuition that the vectors of nodes belonging to the same hyperedge should be close to each other (_i.e._, similar) in the embedding space, while the vectors of the nodes belonging to different hyperedges should be distant (_i.e._, dissimilar) in the embedding space [21, 37, 47]. The heuristic-based prediction, which does not require a separate training process, measures the degree of positiveness by computing the average similarity between all pairs of nodes belonging to the same hyperedge. In this paper, we use the dot product as a similarity measure: _i.e._, the similarity between two nodes is measured by taking the dot product of their respective vectors. In contrast to the heuristic-based prediction, the model-based prediction first trains a prediction model by using positive and negative hyperedges and determines the degree of positiveness of a given hyperedge via the prediction model [21, 37, 47]. In this paper, we simply use multilayer perceptron (MLP) [13] as a prediction model.

\begin{table}
\begin{tabular}{c||c c c c c}  & C & S & M & L & CS \\ \hline
**Non-recoverability** & ✗ & & ✗ & & \\
**Tie-weakening** & & ✗ & & & \\
**Multi-cloning** & & & ✗ & ✗ & \\ \hline \end{tabular}
\end{table}
Table 1. Summary of information loss problems associated with hypergraph expansion methods

**MILEAGE-score computation.**_MILEAGE-score computation_ (Figure 3-(d)) is a component that quantifies the information loss of the expanded graph by comparing the reconstructed and original hypergraphs. In this paper, we regard the degree of mismatch between the hyperedges in the reconstructed hypergraph and those of the original hypergraph as the mileage; we measure the degree of mismatch based on _precision_[17]. Given the original hypergraph \(H_{o}\)=(\(V,HE_{o}\)) and the reconstructed hypergraph \(H_{r}\)=(\(V,HE_{r}\)), where \(V\) indicates a set of nodes and \(HE_{o}\) and \(HE_{r}\) indicate sets of hyperedges of \(H_{o}\) and \(H_{r}\), respectively, the mileage is computed as shown in the following equation:

\[\begin{split}\textit{mileage}\left(H_{o},H_{r}\right)& =1-\textit{precision}\\ &=1-\frac{\left|HE_{o}\cap H_{r}\right|}{\left|HE_{r}\right|}.\end{split} \tag{1}\]

A smaller mileage of the expanded graph indicates that the reconstructed hypergraph from the expanded graph is more similar to the original hypergraph. This suggests that the expanded graph has a smaller information loss.

## 5 Evaluation

In this section, we first validate the usefulness of the MILEAGE and evaluate the goodness of existing hypergraph expansion methods via extensive experiments. Our experiments are designed by aiming to answer the following evaluation questions (EQs):

* **EQ1:** Are the _information loss_ of the expanded graph and the accuracy of downstream tasks correlated?
* **EQ2:** Is MILEAGE agnostic to URL methods?
* **EQ3:** Which hypergraph expansion method provides the least information loss in its expanded graph?
* **EQ4:** Is MILEAGE agnostic to negative sampling methods?
* **EQ5:** How do different values of parameters of MILEAGE influence the correlation between the information loss and the accuracy of downstream tasks?

### Experimental Settings

We conduct the experiments on a server equipped with Intel i9-9900K, 5006GB SSD, 64GB memory, GeForce RTX 2070, and Linux 5.4.0-53. For reproducibility, we provide our detailed settings such as parameters of MILEAGE in Appendix A.2.

**Datasets.** We use eight datasets of real-world hypergraph datasets prevalent in four distinct domains (_i.e._, graph labels): Senate [9], House [9], Primary [9], High [9], Enron [3, 16], Eu [3], Substances [3], and Classes [3]. We preprocess each hypergraph to remove the hyperedges with a size of \(1\) (_i.e._, the number of nodes in a hyperedge is \(1\)) and duplicate hyperedges, following [25]. We summarize their basic statistics of the datasets in Table 2 and provide their detailed descriptions in Appendix A.3.

**Target downstream tasks.** In order to demonstrate the usefulness of MILEAGE regardless of downstream tasks, we conduct downstream tasks at three-levels (_i.e._, node, hyperedge, and hypergraph) such as node classification, hyperedge prediction, and hypergraph classification. The procedure for each downstream task is as follows. First, we construct \(10\) different training-test splits for each dataset. At this time, the ratio of training to test is set to \(8\):\(2\). Then, for each split, we (\(1\)) train a classifier model; (\(2\)) predict the class labels (_i.e._, labels of nodes, positive/negative of hyperedges, and domains of hyperedgnds) for the test set via the model; and (\(3\)) measure the accuracy of the prediction results. Finally, the final accuracy is computed by averaging the accuracies of all splits. In order to show consistent results, for each downstream task, we use four well-known classifier models, _i.e._, logistic regression (in short, LR) [24], random forest (in short, RF) [5], support vector machine (in short, SVM) [30], and multilayer perceptron (in short, MLP) [13].

### Experimental Results

**EQ1 and EQ2: Correlation between the information loss and the downstream task accuracy.** If the information loss actually affects the accuracy of downstream tasks, the accuracy is expected to decrease as the information loss increases. Moreover, if MILEAGE is properly designed to evaluate the hypergraph expansion methods through the information loss, there should appear a clear (_negative_) _correlation_ between the mileage of expanded graphs and the accuracy of downstream tasks conducted on these graphs.

To investigate this claim, for each dataset, we first measure the mileage of each expanded graph via MILEAGE and then conduct the three downstream tasks on the expanded graph with measuring the accuracy. Next, we rank the expansion methods in the mileage and the accuracy in descending order and compute the Pearson correlation coefficient (PCC) [17] between the rankings in the mileage and the accuracy of the downstream task. At this time, to demonstrate the _agnostic nature_ of MILEAGE to URL methods, we use four URL methods: (1) Deepwalk [32], (2) Node2vec [15], (3) DGI [39], and (4) BGRL [36].

\begin{table}
\begin{tabular}{c||c c c c} \hline \hline
**Dataset** & **Domain** & **\begin{tabular}{} \end{tabular}** & **\begin{tabular}{} \end{tabular}** & **
\begin{tabular}{} \end{tabular}** \\ \hline \hline
**\begin{tabular}{} \end{tabular}** & \begin{tabular}{} \end{tabular}** & \begin{tabular}{} \end{tabular}** & 
\begin{tabular}{} \end{tabular}** \\ \hline \hline
**\begin{tabular}{} \end{tabular}** & \begin{tabular}{} \end{tabular}** & \begin{tabular}{} \end{tabular}** & 
\begin{tabular}{} \end{tabular}** \\ \hline \hline
**\begin{tabular}{} \end{tabular}** & \begin{tabular}{} \end{tabular}** & \begin{tabular}{} \end{tabular}** & \begin{tabular}{} \end{tabular}** & 
\begin{tabular}{} \end{tabular}** \\ \hline \hline
**\begin{tabular}{} \end{tabular}** & \begin{tabular}{} \end{tabular}** & \begin{tabular}{} \end{tabular}** & \begin{tabular}{} \end{tabular}** & 
\begin{tabular}{} \end{tabular}** \\ \hline \hline
**\begin{tabular}{} \end{tabular}** & \begin{tabular}{} \end{tabular}** & \begin{tabular}{} \end{tabular}** & 
\begin{tabular}{} \end{tabular}** \\ \hline \hline \end{tabular}
\end{table}
Table 2: Summary of the datasets used in the experiments

Figure 3: Overview of the MILEAGE.

In the cases of **L** and **M**, as already mentioned in Section 3, these methods expand a single node in a hypergraph into multiple nodes in their expanded graph. Therefore, it is essential to aggregate the vectors of multiple nodes in decomposed graphs for the vector of its corresponding node in the hypergraph to conduct downstream tasks. To conduct hyperedge prediction, in the case of **L**, following (Kumar et al., 2017), we average the vectors of multiple nodes as a vector of a corresponding node in the hypergraph. Similarly, in the case of **M**, we average the vectors of multiple nodes as a vector of a corresponding node for each level, taking the averaged vectors in _all_ levels for the final vector of the corresponding node.

In the case of node classification, we note that only four hypergraphs have node labels among all the hypergraphs. Therefore, we conduct a node classification task on these four hypergraphs only. In the case of hypergraph classification, since we have only eight hypergraphs, the amount of data for model training is insufficient. To address this issue, we sample 100 sub-hypergraphs whose sizes are 10%, 15%, and 20% of the total nodes from each hypergraph via _forest fire_(Grover and Leskovec, 2017). Furthermore, to conduct hypergraph classification, we need a vector for a hypergraph. Since we only have vectors of nodes, following (Kumar et al., 2017), we compute a vector of each graph by averaging of the vectors of all nodes in the hypergraph.

Table 3 shows PCC between the mileage and the accuracy of a downstream task. The rows correspond to two variants of MILEAGE in terms of hyperedge prediction methods (_i.e._, model-based and heuristic-based) in hypergraph reconstruction. A row is subdivided into four categories, indicating the URL methods used for node representation. The columns correspond to downstream tasks. A column is subdivided into four categories, indicating the classifiers used for a downstream task.

We summarize the results shown in Tables 3 as follows. First, we observe that there is a _strong (negative) correlation_ between the mileage and the downstream task accuracy in all cases regardless of variants of MILEAGE, classifiers, and downstream tasks. More specifically, the average and minimum PCCs are -0.871 and -0.904, respectively, which are very high. Second, we observe strong (negative) correlations in all URL methods; DGI exhibits the strongest (negative) correlation among the four URL methods. Third, in all URL methods, we observe that the model-based prediction shows a slightly stronger correlation than the heuristic-based one.

Through the results, we have validated that (1) the information loss problem we have defined has a truly negative impact on the accuracy of downstream tasks, (2) MILEAGE is _agnostic_ to URL methods, and (3) MILEAGE appropriately evaluates the hypergraph expansion methods. Moreover, we have confirmed that MILEAGE using model-based prediction with DGI provides the strongest (negative) correlation among all possible variants of MILEAGE; therefore, we use it as our MILEAGE in the subsequent experiments.

**EQ3: Comparison of hypergraph expansion methods.** Next, we evaluate the goodness of existing hypergraph expansion methods in terms of the information loss via MILEAGE. Tables 4 and 5 show the mileage of expanded graphs obtained via MILEAGE and the accuracy in a downstream task using DGI as a URL method and MLP as a classifier on expanded graphs, respectively.

We summarize the results shown in Tables 4 and 5 as follows. First, **M** provides expanded graphs that provide the _highest_ mileage (_i.e._, highest information loss). Among hypergraph expansion methods, **M** is the only one that has two kinds of information loss problems (_i.e._, non-recoverability and multi-cloning). Therefore, it appears to have the lowest performance in terms of reconstructing the hypergraph. Second, **CS** provides expanded graphs with the _least_ mileage. We conjecture that the mileage of **CS** primarily stems from in URL and hypergraph reconstruction, as **CS** does not exhibit any information loss problems during the hypergraph expansion process. Third, we observe that the trend of downstream task accuracy is opposite to that of the mileage. This demonstrates once again that there is a clear (negative) correlation between the mileage and the accuracy.

Through the results, we conclude that (1) **CS** provides the best expanded graphs in terms of a mileage and (2) it is possible to estimate the performance superiority of the downstream task on expanded graphs, based on the superiority of the mileage obtained by MILEAGE.

**EQ4: Sensitivity of MILEAGE according to negative sampling methods in hypergraph reconstruction.** Next, we analyze the change of the mileage according to different negative sampling methods in MILEAGE. For negative sampling methods, we employ the following three methods (Kumar et al., 2017; Wang et al., 2017): (1) _Sized Negative Sampling_ (SNS), (2) _Motif Negative Sampling_ (MNS), and (3) _Clique Negative Sampling_. Figure 4 shows the mileage of expanded graphs in (a)

\begin{table}
\begin{tabular}{c c||c c c c|c c c c|c c c c|c} \hline  & & & \multicolumn{3}{c|}{**Node Classification**} & \multicolumn{4}{c|}{**Hyperedge Prediction**} & \multicolumn{4}{c|}{**Hypergraph Classification**} & \multirow{2}{*}{**Avg.**} \\ \cline{3-14}  & & **LR** & **RF** & **SVM** & **MLP** & & **LR** & **RF** & **SVM** & **MLP** & & **LR** & **RF** & **SVM** & **MLP** & \\ \hline \hline \multirow{8}{*}{**MNS**} & Deepwalk & -0.798 & -0.821 & -0.869 & -0.861 & -0.804 & -0.889 & -0.798 & -0.869 & -0.905 & -0.885 & -0.824 & -0.865 & -0.849 & 759 \\  & Nodehave & -0.865 & -0.854 & -0.881 & -0.905 & -0.932 & -0.801 & -0.915 & -0.903 & -0.911 & -0.896 & -0.843 & -0.854 & -0.880 & 760 \\  & DGI & -0.820 & -0.886 & -0.923 & -0.951 & -0.865 & -0.919 & -0.826 & -0.968 & -0.913 & -0.923 & -0.884 & -0.971 & -0.904 & 761 \\  & BGRL & -0.818 & -0.864 & -0.902 & -0.896 & -0.845 & -0.901 & -0.809 & -0.946 & -0.912 & -0.901 & -0.885 & -0.890 & -0.882 & 762 \\  & Average & -0.825 & -0.856 & -0.893 & -0.903 & -0.862 & -0.881 & -0.837 & -0.921 & -0.910 & -0.901 & -0.859 & -0.895 & -0.879 & 763 \\  & Deepwalk & -0.802 & -0.815 & -0.861 & -0.856 & -0.805 & -0.887 & -0.785 & -0.862 & -0.902 & -0.883 & -0.821 & -0.871 & -0.845 & 765 \\  & Nodehave & -0.835 & -0.880 & -0.885 & -0.753 & -0.875 & -0.817 & -0.800 & -0.922 & -0.841 & -0.863 & -0.872 & -0.891 & -0.853 & 766 \\  & DGI & -0.819 & -0.881 & -0.819 & -0.918 & -0.913 & -0.934 & -0.902 & -0.894 & -0.873 & -0.832 & -0.855 & -0.912 & -0.895 & 769 \\  & BGRL & -0.812 & -0.866 & -0.894 & -0.887 & -0.831 & -0.897 & -0.795 & -0.934 & -0.893 & -0.817 & -0.807 & -0.881 & -0.860 & 767 \\  & Average & -0.817 & -0.860 & -0.890 & -0.852 & -0.856 & -0.883 & -0.821 & -0.903 & -0.877 & -0.874 & -0.839 & -0.889 & -0.863 & 769 \\  & **Overall average** & -0.821 & -0.858 & -0.892 & -0.876 & -0.859 & -0.882 & -0.829 & -0.911 & -0.894 & -0.888 & -0.849 & -0.892 & -0.871 & 770 \\ \hline \end{tabular}
\end{table}
Table 3. PCC between rankings in information loss obtained by MILEAGE and downstream task accuracy 

[MISSING_PAGE_EMPTY:8]

* [90] A. Aggaru, 2018. Deep Learning Using Rectified Linear Units (relu). _arXiv preprint arXiv:1803.08375_ (2018).
* [91] S. Agarwal, R. Branson, and S. Belongie. 2006. Higher Order Learning with Graphs. In _Proceedings of International Conference on Machine Learning (ICML)_, 17-24.
* [92] J. Benson, R. Abbe, M. Schaub, A. Jadhaite, and J. Kleinberg, 2018. Simplicial Closure and Higher-Order Link Prediction. _Proceedings of National Academy of Sciences USA_ 113 (48), 21221-11230.
* [93] V. Blondel, J. Guillaume, R. Lambiotte, and E. Lefebvre. 2008. Fast Unfolding of Communities in Image Networks. _Journal of Statistical Mechanics: Theory and Experiment_ 2008, 10 (2008), P10008.
* [94] L. Breiman. 2001. Random Forests. _Machine learning_ 45 (2001), 5-32.
* [95] S. Chen, E. Xie, C. Bo, R. Chen, D. Liang, and P. Luo. 2022. Cyclenp: A Mlp-like Architecture for Dense Prediction. In _Proceedings of International Conference on Learning Representations (ICLR)_.
* [96] E. Chino, C. Pan, J. Peng, and O. Milenkovic. 2012. You are All-Set: A Multiset Function Framework for Hypergraph Neural Networks. In _Proceedings of International Conference on Learning Representations (ICLR)_.
* [97] E. Chino, C. Pan, J. Peng, and O. Milenkovic. 2014. You are All-Set: A Multi-set Function Framework for Hypergraph Neural Networks. In _Proceedings of International Conference on Learning Representations (ICLR)_.
* [98] P. Chorbov, N. Vidt, and A. Benson. 2021. Generative Hypergraph Clustering: From Blockmodels to Modularity. _Science Advances_ 7, 28 (2021), 1-13.
* [99] U. Dhillon, Y. Guan, and R. Kulis. 2007. Weighted Graph CNN without Egroscue a Multi-Approach. _IEEE Transactions on Pattern Analysis and Machine Intelligence_ 29, 11 (2007), 1944-1957.
* [100] M. Do, S. Yoon, B. Hooi, and K. Shin. 2020. Structural Patterns and Generative Models of Real-World Hypergraphs. In _Proceedings of ACM International Conference on Knowledge Discovery and Data Mining (KDD)_, 176-186.
* [101] Y. Feng, H. You, Z. Zhang, R., and J. Gao. 2019. Hypergraph Neural Networks. In _Proceedings of AAAI Conference on Artificial Intelligence (AAAI)_, 355-356.
* [102] M. Gardner and S. Dorfin. 1998. Artificial Neural Networks (the Multilayer Perceptron)--A Review of Applications in the Atmospheric Sciences. _Atmospheric environment_ 32, 1-15 (1998), 267-2636.
* [103] P. Goyal and E. Ferrara. 2018. Graph Embedding Techniques, Applications, and Performance: A Survey. _Knowledge-Based Systems_ 151 (2018), 78-94.
* [104] G. Grover and L. Leskovec. 2016. node2vec: Scalable Feature Learning for Networks. In _Proceedings of ACM International Conference on Knowledge Discovery and Data Mining (KDD)_, 835-864.
* [105] Yin H. A. Benson, I. Leskovec, and D. Gleich. 2017. Local Higher-Order Graph Clustering. In _Proceedings of ACM International Conference on Knowledge Discovery and Data Mining (KDD)_, 555-564.
* [106] J. Han, J. Pei, and M. Kamber. 2011. _Data Mining: Concepts and Techniques_. Morgan Kaufmann, Waltham, Massachusetts.
* [107] Z. Han, X. Zheng, C. Chen, W. Cheng, and Y. Yao. 2023. Intra and Inter Domain Hypergraph Convolutional Network for Cross-Domain Recommendation. In _Proceedings of ACM Web Conference (WWW)_, 449-459.
* [108] X. He, K. Deng, X. Wang, Y. Li, Y. Zhang, and M. Wang. 2020. Lightgcn: Simplifying and Wovering Graph Convolution Network for Recommendation. In _Proceedings of International ACM Conference on Research and Development in Information Retrieval (SIGIR)_, 839-848.
* [109] R. Henderson, D. Clevert, and F. Montanari. 2021. Improving Molecular Graph Neural Network Explainability with Orthonormalization and Induced Sparsity. In _Proceedings of International Conference on Machine Learning (ICML)_, 4203-4213.
* [110] H. Hwang, S. Lee, C. Park, and K. Shin. 2022. AHP: Learning to Negative Sample Representations. In _Proceedings of International ACM Conference on Research and Development in Information Retrieval (SIGIR)_, 2237-2242.
* [111] Y. Kang, W. Lee, Y. Lee, K. Hate, and S. Kim. 2021. Adversarial Learning of Balanced Triangles for Accurate Community Detection on Signed Networks. In _Proceedings of IEEE International Conference on Data Mining (ICDM)_, 1150-1155.
* [112] T. Kipf and M. Welling. 2017. Semi-Supervised Classification with Graph Convolutional Networks. In _Proceedings of International Conference on Learning Representations (ICLR)_.
* [113] D. Kientham, L. Dietz, M. Gail, M. Klein, and M. Klein. 2002. _Logistic Regression_. Springer.
* [114] Y. Kook, J. Ko, and K. Shin. 2020. Evolution of Real-World Hypergraphs: Patterns and Models without Oracles. In _Proceedings of ACM International Conference on Information and Knowledge Management (CIKM)_, 272-281.
* [115] A. Kumar, S. Singh, K. Singh, and B. Biswas. 2020. Link Prediction Techniques, Applications, and Performance: A survey. _Physica A: Statistical Mechanics and its Applications_ 553 (2020), 124289.
* [116] Y. Lee, N. Seo, K. Han, and S. Kim. 2020. ASINE: Adversarial Signed Network Embedding. In _Proceedings of International ACM Conference on Research and Development in Information Retrieval (SIGIR)_, 609-618.
* [117] J. Leskovec and C. Faloutsos. 2006. Sampling From Large Graphs. In _Proceedings of ACM International Conference on Knowledge Discovery and Data Mining (KDD)_, 613-636.
* [118] M. Newman. 2014. Coauthorship Networks and Patterns of Scientific Collaboration. _Proceedings of National Academy of Sciences USA_ 101 (2014), 5200-5205.
* [119] W. Noble. 2006. What is a Support Vector Machine? _Nature biotechnology_ 24, 98 (2006), 1565-1567.
* [120] P. Patil, G. Sharma, and M. Murty. 2020. Negative Sampling for Hyperlink Prediction in Networks. In _Proceedings of Pacific-Asia Conference on Knowledge Discovery and Data Mining (PANDED)_, 607-619.
* [121] B. Pocuzzi, R. Al-Ruba, and S. Srian. 2021. _Deepwalk: Online Learning of Social Representations. In _Proceedings of ACM International Conference on Knowledge Discovery and Data Mining (KDD)_, 701-710.
* [122] R. Rossi and N. Ahmed. 2015. The Network Data Repository with Interactive Graph Analytics (Vivsantiation). In _Proceedings of AAAI Conference on Artificial Intelligence (AAAI)_, 4292-4293.
* [123] X. Su, S. Xue, E. Liu, J. Yu, J. Yang, C. Wu, H. Chen, S. Nepal, D. Jin, Q. Shen, and P. Yu. 2022. A Comprehensive Survey on Community Detection with Deep Learning. _IEEE Transactions on Neural Networks and Learning Systems_ (2022).
* [124] L. Sun, S. Ji, and J. Ye. 2008. Spectral Learning for Multi-Label Classification. In _Proceedings of ACM International Conference on Knowledge Discovery and Data Mining (KDD)_, 608-676.
* [125] S. Thadore, C. Tallec, M. Azar, M. Azahou, E. Dyer, R. Munos, P. Velickovic, and M. Vilko. 2021. Large-scale Representation Learning on Graphs via Bootstrapping. (2021).
* [126] K. Tu, P. Cu, X. Wang, F. Wang, and W. Zhu. 2018. Structural Deep Embedding for Hyper-Networks. In _Proceedings of AAAI Conference on Artificial Intelligence (AAAI)_, 426-433.
* [127] Nate Veldt. 2023. Cu-Matching Games for Generalized Hypergraph Ratio Cuts. In _Proceedings of ACM Web Conference (WWW)_, 694-704.
* [128] N. Veldt, A. Benson, and J. Kleinberg. 2020. Minimizing Localized Ratio Cut Objectives in Hypergraphs. In _Proceedings of ACM International Conference on Knowledge Discovery and Data Mining (KDD)_, 1708-1718.
* [129] P. Velickovic, G. Coauthari, A. Cassonaro, A. Romero, P. Li, and Y. Bengio. 2017. Graph Attention Networks. _arXiv preprint arXiv:1710.10003_ (2017).
* [130] P. Velickovic, W. Fedas, W. Hamilton, P. Lu, Y. Bengio, and R. Hjem. 2018. Deep Graph Infomax. In _Proceedings of International Conference on Learning Representations (ICLR)_.
* [131] S. Wang, L. Hu, Y. Wang, X. He, Q. Sheng, M. Orgun, L. Cao, F. Ricci, and P. Yu. 2021. Graph Learning Based Recommender Systems: A Review. _arXiv preprint arXiv:2106.01303_ (2021).
* [132] J. Wu, S. Li, J. Li, Y. Pan, and K. Xu. 2022. A Simple yet Effective Method for Graph Classification. In _Proceedings of International Joint Conference on Artificial Intelligence (IJCAI)_, 3580-3586.
* [133] C. Yang, R. Wang, S. Yao, and T. Abdelzaher. 2022. Semi-Supervised Hypergraph Node Classification on Hypergraph Line Expansion. In _Proceedings of ACM International Conference on Information and Knowledge Management (CIEM)_, 2325-2361.
* [134] J. Yang and J. Leskovec. 2012. Defining and Evaluating Network Communities Based on Ground-Truth. In _Proceedings of IEEE International Conference on Data Mining (KDD)_, 745-754.
* [135] J. Yu, H. Yin, J. Li, Q. Wang, N. Hung, and X. Zhang. 2021. Self-Supervised Multi-Channel Hypergraph Convolutional Network for Social Recommendation. In _Proceedings of ACM Web Conference (WWW)_, 413-424.
* [136] R. Zhang, Y. Zou, and J. Ma. 2020. Hyper-SAGN: A Self-Attention Based Graph Neural Network for Hypergraphs. In _Proceedings of International Conference on Learning Representations (ICLR)_.
* [137] C. Zheng, X. Fan, C. Wang, and J. Q. 2020. Defining and Evaluating Network Communities Based on Ground-Truth. In _Proceedings of AAAI Conference on Artificial Intelligence (AAAI)_, 1234-1241.
* [138] D. Zhou, J. Huang, and B. Scholkopf. 2006. Learning with Hypergraphs: Clustering, Classification, and Embedding. In _Proceedings of Conference on Neural Information Processing Systems (NIPS)_, 1633-1640.

[MISSING_PAGE_EMPTY:10]