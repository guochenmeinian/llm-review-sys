# Online Bidding under RoS Constraints

without Knowing the Value

Anonymous Author(s)

###### Abstract.

We consider the problem of bidding in online advertising, where an advertiser aims to maximize value while adhering to budget and Return-on-Spend (RoS) constraints. Unlike prior work that assumes knowledge of the value generated by winning each impression (e.g., conversions), we address the more realistic setting where the advertiser must simultaneously learn the optimal bidding strategy and the value of each impression opportunity. This introduces a challenging exploration-exploitation dilemma: the advertiser must balance exploring different bids to estimate impression values with exploiting current knowledge to bid effectively. To address this, we propose a novel Upper Confidence Bound (UCB)-style algorithm that carefully manages this trade-off. Via a rigorous theoretical analysis, we prove that our algorithm achieves \(\widetilde{O}(\sqrt{T\log(|\mathcal{B}|T)})\) regret and constraint violation, where \(T\) is the number of bidding rounds and \(\mathcal{B}\) is the domain of possible bids. This establishes the first optimal regret and constraint violation bounds for bidding in the online setting with unknown impression values. Moreover, our algorithm is computationally efficient and simple to implement. We validate our theoretical findings through experiments on synthetic data, demonstrating that our algorithm exhibits strong empirical performance compared to existing approaches.

online bidding, Return-on-Spend, constrained bandits, UCB +
Footnote †: ccs: Applied computing Online auctions

+
Footnote †: ccs: Applied computing Online auctions

+
Footnote †: ccs: Applied computing Online auctions

+
Footnote †: ccs: Applied computing Online auctions

+
Footnote †: ccs: Applied computing Online auctions

+
Footnote †: ccs: Applied computing Online auctions

+
Footnote †: ccs: Applied computing Online auctions

+
Footnote †: ccs: Applied computing Online auctions

+
Footnote †: ccs: Applied computing Online auctions

+
Footnote †: ccs: Applied computing Online auctions

+
Footnote †: ccs: Applied computing Online auctions

+
Footnote †: ccs: Applied computing Online auctions

## 1. Introduction

Online advertising, a multi-billion dollar industry, relies on real-time auctions to connect advertisers with users. These auctions, triggered by user queries or website visits, allow advertisers to bid for advertising slots, such as prominent placements on search engine results pages or in social media feeds. Advertisers aim to maximize their returns, measured in conversions or other relevant metrics, by carefully determining their bids while adhering to budget constraints and desired return-on-spend (RoS) targets. To achieve this, a wide array of bidding strategies have been developed, leveraging techniques from optimization, online learning, and game theory to maximize advertiser utility (Brocker, 1983; Kahneman, 1984; Kahneman, 1984; Kahneman, 1985; Kahneman, 1986; Kahneman, 1987).

Despite the sophistication of these strategies, many rely on the assumption that perfect knowledge of the value an impression generates is available to the advertiser beforehand. In reality, however, advertisers frequently face uncertainty about the true value of an ad impression, especially when dealing with new ad campaigns or evolving user preferences (cf. Section 2 for more details). In this work, we consider the practical scenario in which the value of an impression is _unknown_ a priori and focus on developing bidding strategies that simultaneously learn the value of ad impressions as well as maximize the realized value of the advertiser.

Specifically, we study the problem of bidding for a single advertiser subject to total budget and RoS constraints. The budget constraint limits the total expenditure, while the RoS constraint ensures that the ratio of total value to total spend meets a predefined target, thus effectively capturing performance goals like target cost-per-acquisition (ICPA) and target return-on-ad-spend (tROAS), widely used in real-world advertising campaigns.1

Footnote 1: See Google ads support page and Meta business help center for examples.

We consider a stochastic setting where search queries and associated auctions arise dynamically. In this setting, the competing bids and the value of winning an auction are assumed to be sampled independently and identically distributed (i.i.d.) from an unknown distribution. In each round, the advertiser submits a bid without knowing the query's value beforehand. Upon bid submission, the auction mechanism determines the winner and price, with the value being revealed only if the advertiser wins. Our goal is to design an online bidding algorithm that maximizes the bidder's value over the entire horizon, while respecting the RoS and budget constraints.

### Our Main Result

We evaluate our algorithm's performance via the notion of _regret_ (Equation (5)), which quantifies the difference between its expected cumulative value and that achieved by an oracle, which possesses complete knowledge of the underlying competing bid and value distributions and employs a fixed strategy optimized for maximum cumulative value. Our main result now follows.

**Theorem** (Informal; see Theorem 3.3).: _We propose an algorithm (Algorithm 3) designed for value maximization in online advertising auctions with return-on-spend (RoS) and budget constraints, without any prior knowledge of the values associated with incoming user queries. In the stochastic setting described earlier, with an online horizon of length \(T\), our algorithm provably achieves \(O(\sqrt{T\log(|\mathcal{B}|T)/V})\) regret for the objective of value maximization and \(O(\sqrt{T\log(|\mathcal{B}|T)/V})\)violation of the RoS constraint and \(O(\sqrt{T\log(|\mathcal{B}||T)})\) violation of the budget constraint. Here, \(\mathcal{B}\) is the domain of possible discrete bids and \(V\) is the maximum per-round value achieved by the above oracle._

Comparison to prior workTo the best of our knowledge, ours is the first algorithm to achieve near-optimal regret and constraint violation bounds in this setting _without_ knowledge of item values. This significantly extends recent work, which crucially assumes that the values are known to the bidder before bidding (Gardner et al., 2017; Gardner et al., 2018; Gardner et al., 2019). While (Gardner et al., 2019) also addresses the setting with unknown values, their regret and constraint violation bounds incur a dependence of \(O(\sqrt{|\mathcal{B}|})\), which we improve to a _logarithmic_ dependence on \(|\mathcal{B}|\). Additionally, (Gardner et al., 2019) requires assuming the existence of a Slater point (a strictly feasible solution (Gardner et al., 2019)), which limits the generality of their approach. Although (Gardner et al., 2019) removes this assumption, it still incurs a \(O(\sqrt{|\mathcal{B}|})\) dependence in the regret bounds, which is exponentially weaker than the logarithmic dependence achieved by our bound. In contrast to prior works, our work replaces the assumption of a Slater point with a milder condition that \(V\) is bounded away from zero (see Section 1.2 for a detailed discussion). Furthermore, based on the lower bounds established by Achdou et al. (Achdou et al., 2018) for utility maximization (without RoS constraints) under second-price auctions, we believe that a dependence on \(V\) is unavoidable.

Our core strengthsA key strength of our algorithm and analysis is its simplicity. Unlike prior work that predominantly adopts a primal-dual approach -- requiring intricate analysis of dual variables and assumptions like Slater's condition -- we completely eliminate these restrictive (and often impractical) assumptions, making our approach both stronger and more general. Furthermore, by employing the upper confidence bound (UCB) framework, our method becomes easier to analyze (Sections 1.2 and 3) and simpler to implement, requiring very few hyper-parameters (cf. Section 4).

Computational aspectsA key challenge in our algorithm lies in efficiently estimating the arm recommended by the UCB. This estimation requires solving, in each round, a complex non-convex problem (Problem 3.5). We address this challenge by providing a computationally efficient technique that runs in \(O(|\mathcal{B}|^{3})\) time. This technique leverages the key insight that both the allocation and payment functions in standard auctions are monotonically increasing, which holds for both truthful auctions (e.g., second-price) and non-truthful auctions (e.g., first-price, all-pay) (Steiner and Gardner, 2018).

### Key Technical Contributions

The problem of constrained reward-maximization may naturally be cast as one maximizing the "price-adjusted reward" (i.e., the reward minus a penalty on the constraint violation, with the penalty weighted by the dual variable associated with the constraint). This is the approach that has been widely adopted by much of the past work (Gardner et al., 2017; Gardner et al., 2018; Gardner et al., 2019; Gardner et al., 2019). The key idea is that as a constraint approaches violation, the corresponding dual variable grows large, signaling the need to bid conservatively in the next round; conversely, when previous bids create a buffer in the constraint, the dual variable shrinks, encouraging more aggressive bidding in the subsequent round.

However, in this primal-dual approach, one must assume the existence of a Slater point -- an action which _strictly_ satisfies the expected constraints. As dual variable magnitudes are bounded by \(O(1/\kappa)\) (cf. (Gardner et al., 2019, Theorem 8.42)), where \(\kappa\) is the minimum constraint slack of the Slater point, primal-dual methods can incur \(O(1/\kappa)\) regret and constraint violation. In online bidding with RoS and budget constraints, bids with \(\kappa\approx 0\) are common (e.g., when competing bids are narrowly concentrated). Consequently, algorithms based on the primal-dual approach will fundamentally incur large regret and constraint violation. We circumvent this shortcoming by introducing a UCB-style algorithm.

In a typical UCB-style algorithm (see, for example, (Gardner et al., 2018, Chapter 7)), the idea is to create confidence sets for the unknown rewards and select the arm with the highest upper confidence bound. Our primary insight is to extend this principle to the constrained setting inherent in online bidding. In particular, we maintain appropriate confidence sets for both the constraints and rewards. Our algorithm then selects the bid that maximizes the reward while satisfying the constraints based on these confidence sets. This approach entirely eliminates the need for a Slater point (and, hence, avoids a regret dependence of \(1/\kappa\)) and addresses the problem even when the value is unknown. Finally, as noted earlier, finding this reward-maximizing bid is a highly nonconvex optimization problem. By utilizing the structure inherent to autobidding, we derive a provably efficient solution that is also easy to implement (Lemma 3.2).

### Related Work

Our problem falls under the broader umbrella of bandit optimization under long-term constraints and has witnessed a long line of work by various research communities e.g. Agrawal and Devanur (Agrawal and Devanur, 2018), Badanidiyuru et al. (Balseiro et al., 2018), Balseiro et al. (Castigoni et al., 2018), Castigoni et al. (Gao et al., 2018), Gao et al. (Zimmorien et al., 2018), Immorlica et al. (Mahdavi et al., 2018; Mannor et al., 2018), Mannor et al. (2018), Yu et al. (Yu and Neely, 2018).

Most of these works study the _budget/packing constraint_, e.g., Devanur et al. (Devanur et al., 2018) obtain the optimal \(O(\sqrt{T})\)-regret under linear objective and constraints, Agrawal and Devanur (Agrawal and Devanur, 2018) generalize it to nonlinear objectives, and Balseiro et al. (Balseiro et al., 2018) generalize it to nonlinear budget constraints. The RoS constraint we study differs fundamentally from the packing constraint studied in these works as well as in (Balseiro et al., 2018; Gardner et al., 2018). There also exist papers that study a variant of our problem with a constraint class more general than ours (e.g., Agrawal and Devanur (Agrawal and Devanur, 2018), Castiglioni et al. (Castiglioni et al., 2018)); however, their guarantees for our problem are not as strong as ours, as we elaborate next.

For example, Castiglioni et al. (Castiglioni et al., 2018) use a primal-dual framework for regret minimization with bandit feedback, which, when adapted to our bidding problem under the RoS constraint, achieves \(\widetilde{O}(T^{3/4})\) regret with \(\widetilde{O}(T^{3/4})\) constraint violation. Another crucial difference from our setting is that we do _not_ know the values of the bids, whereas (Gardner et al., 2019) (when adapted to this problem) does. Their bounds improve to our \(\widetilde{O}(\sqrt{T})\) bounds under a'strictly feasible' assumption; however, we require no such assumptions to get these bounds. In follow-up work, again with bandit feedback, Bernasconi et al. (Bernasconi et al., 2018) study "Best of Both Worlds"-type algorithms for constrained regret minimization without the Slater point assumption. This work is closest to ours, but with the regret and constraint violation bounds suffering from an \(O(\sqrt{|\mathcal{B}|})\) dependence (just like (Gardner et al., 2019)), which can be substantial in practical settings; our work, in contrast, achieves an \(O(\log(|\mathcal{B}|))\) dependence. This difference in the bidding setting arises because their observational model does not account for the specifics of allocation and pricing functions. We empirically compare these algorithms against ours over synthetically generated bidding instances (cf. Section 4 for details).

Another example is the work of Agrawal and Devanur [3], which considers general online optimization with convex constraints. This work uses black-box low-regret methods with a strongly convex regularizer over the dual space. A sub-linear regret bound is attainable only when the dual space is well-bounded (e.g., a scaled simplex) or when the dual variable can be projected onto such a space without incurring too much additional regret. This canonical approach proves difficult for the RoS constraint, which can incur poor problem-specific parameters in generic guarantees. Hence, this technique cannot give sub-linear regret for the RoS constraint.

A recent line of work studies our bidding problem under both budget and RoS constraints; however, in each of these papers, the underlying assumption is that the bidder _knows_ the value before submitting its bid. For example, Feng et al. [26] provide \(\tilde{O}(T^{1/2})\) regret and almost-sure constraint satisfaction using a primal-dual algorithm. The work of Lucier et al. [38], also in this setting, additionally obtains vanishing regret in the adversarial setting and provides aggregate guarantees on the resulting expected liquid welfare when multiple autobidders all deploy their algorithm. Other closely related works include those of Golrezaei et al. [39] and Celli et al. [19], the latter also considering multiple different constraints. However, as noted earlier, all of these require knowing the value.

The following works study regret minimization with the bidder _not_ knowing the value. Weed et al. [49] study this for second-price auctions, and Achdotio et al. [1] and Feng et al. [27] study this for general auctions, proving \(O(\sqrt{T})\) regret bounds, with the former in a stochastic setting and the latter in an adversarial one. However, we note that all these works focus only on the unconstrained setting and maximize the utility, which is defined as the difference between the received value and the paid price.

A closely related line of work studies bandit optimization under long-term constraints [28; 37; 45; 53]. The works of Liu et al. [37] and Gangrade et al. [28] study this for linear bandits with long-term linear constraints. Liu et al. [37] use a primal-dual approach to provide \(\tilde{O}(d\sqrt{T})\) rates (where \(d\) is the problem dimension), but require the existence of a Slater point. Gangrade et al. [28] avoid the need for Slater points, by maintaining doubly optimistic constraints and reward estimates, and obtain \(\tilde{O}(d\sqrt{T})\) rates. Both these works, when specialized to autobidding, incur linear dependence on \(|\mathcal{B}|\) in the regret. This problem was also studied for kernelized bandits in [53], but their algorithm requires knowledge of a lower bound on the Slater slack. A different, but related, problem of satisfying constraints in each round is studied in [45]. This work shows that knowledge of a "safe" action is necessary for per-round constraint satisfaction and obtains \(O(\sqrt{T})\) regret under this assumption.

Finally, the related problem of learning to bid in repeated auctions has been explored in both academia and industry, e.g. Badanidiyuru et al. [8], Borgs et al. [16], Feng et al. [27], Han et al. [31], Nedelec et al. [42], Noti and Syrgkanis [44], Weed et al. [49]. These works abstract the problem of learning to bid as one of contextual bandits, but do not incorporate constraints into them. Beyond these, there has been some work on bidding under budget constraints, e.g., Ai et al. [4], Balseiro and Gur [12]. However, these papers focus on utility-maximizing agents with at most one constraint.

## 2. Preliminaries

We consider an auction with multiple bidders and study the online bidding problem from the perspective of a single learner (bidder). \(A\) each time step \(t\), nature generates an ad query associated with a value \(v_{t}\in[0,1]\) and an auction mechanism \((x_{t},p_{t})\). The auction mechanism is determined by the allocation and payment functions:

* _Allocation function_, \(x_{t}:\mathcal{B}\rightarrow[0,1]\), which specifies the probability of winning the auction for a given bid. We define \(x_{t}(\ \cdot\ )\coloneqq x(\ \cdot\,B_{t}^{\mathbb{G}})\), where \(\mathcal{B}\) is a finite subset of \(\mathbb{R}_{\geq 0}\) with \(0\in\mathcal{B}\) (i.e., the bidder can submit a bid of zero), and \(B_{t}^{\mathbb{G}}\) denotes the vector of bids of the other bidders at time step \(t\). Observe that the allocation probability depends not only on the learner's bid but also on the bids of other participants.
* _Payment function_, \(p_{t}:\mathcal{B}\rightarrow[0,1]\), which determines the payment required when the auction is won. Similar to the allocation function, we define \(p_{t}(\ \cdot\ )\coloneqq p(\ \cdot\,B_{t}^{\mathbb{G}})\). We assume that the payment is zero when the allocation is zero and is always at most the submitted bid. This ensures that the bidder never pays more than their bid, a standard assumption in auctions.

We use the shorthand \(q_{t}(b)\coloneqq x_{t}(b)\cdot p_{t}(b)\) to denote the price paid for a bid \(b\). A key distinction of our model from those in prior works [1; 26] is that we do not assume the auctions to be truthful. Instead, all we require is that the functions \(x_{t}(\ \cdot\ )\) and \(p_{t}(\ \cdot\ )\) be _monotonic_, a property satisfied by many popular auctions, including first-price, second-price, and all-pay auctions [33].

Another important point of departure from previous work is that, at each time step \(t\), the value \(v_{t}\) is _unknown_ to the learner before submitting a bid. This model reflects the uncertainty inherent in many online advertising scenarios. The learner decides its bid \(b_{t}\) based on all the information obtained so far. After submitting its bid, the learner observes the outcome from the auction mechanism, i.e., \(x_{t}(\ \cdot\ )\) and \(p_{t}(\ \cdot\ )\). If the bidder wins, then the auction mechanism also reveals the value \(v_{t}\). For a bid \(b\) with value \(v\), allocation function \(x(\ \cdot\ )\), and payment function \(p(\ \cdot\ )\), the _realized_ value and _paid_ price are \(\cdot x(b)\) and \(\circ\ \cdot p(b)\), respectively.

This setting of unknown value is common in several practical online bidding environments. Examples include advertisers who participate infrequently in auctions, new ad campaigns with uncertain performance, or scenarios where the value of an advertisement is influenced by multiple factors, such as clicks, conversions, brand awareness, and customer lifetime value. Even in autobidding systems [22], where machine learning models predict clicks and conversions to inform bidding algorithms, these predictions often capture only partial information about the true value and can be inaccurate, especially for new or infrequently shown advertisements.

Similar to prior works on online bidding [14; 18; 46], we assume a stochastic setting where the auction environment is governed by an underlying probability distribution. Specifically, for all \(t\in[T]\), the tuple \(y_{t}\coloneqq(v_{t},x_{t},p_{t})\) is drawn independently and identically (i.i.d.) from an unknown distribution \(\mathcal{P}\). This implies that the sequence of \(T\) samples, denoted by \(\overrightarrow{\gamma}\coloneqq\{\tau_{1},\tau_{2},\ldots,\tau_{T}\}\), follows the product distribution \(\mathcal{P}^{T}\). This induces the expectations \(\overrightarrow{v}=\mathbb{E}[v_{t}]\), \(\overrightarrow{q}(b)\coloneqq\mathbb{E}[x_{t}(b)p_{t}(b)]\), and \(\overrightarrow{x}(b)\coloneqq\mathbb{E}[x_{t}(b)]\) for any bid \(b\in\mathcal{B}\).

We design online bidding algorithms to maximize the learner's total realized value subject to RoS and budget constraints. Formally, this optimization problem is given by

\[\begin{array}{ll}\underset{b_{t}:t=1,\cdots,T}{\text{maximize}}&\sum_{t=1}^{T}q_ {t}\cdot x_{t}(b_{t})\\ \text{subject to}&\text{RoS}\cdot\sum_{t=1}^{T}q_{t}(b_{t})\leq\sum_{t=1}^{T}q_ {t}\cdot x_{t}(b_{t}),\\ &\sum_{t=1}^{T}q_{t}(b_{t})\leq\rho T,\end{array} \tag{2.1}\]

where RoS \(>0\) is the target ratio of the RoS bidder and \(\rho T\) the total budget, with \(\rho>0\) (assumed a fixed constant) measuring the limit of the average expenditure over \(T\) rounds (ad queries). Throughout the paper we assume without loss of generality2 that \(\text{RoS}=1\).

Footnote 2: For any RoS \(\neq 1\), we can scale the values to be \(\eta_{t}=\text{RoS}\cdot\eta_{t}\).

Analysis setup.We use the notions of regret and constraint violation to measure the performance of our algorithm. To define regret, we first define the reward collected by our algorithm (\(\text{"}\text{"}\text{"}\text{"}\text{"}\text{"}\text{"}\text{"}\)) for a sequence of requests \(\overrightarrow{Y}\) over a time horizon \(T\) as

\[\text{Reward}(\text{Alg},\overrightarrow{Y})\coloneqq\sum_{t=1}^{T}\eta_{t} \cdot x_{t}(b_{t}). \tag{2.2}\]

To define the benchmark against which we measure the regret of Alg, we consider the following linear program (LP):

\[\begin{array}{ll}\underset{w\in\Delta_{[\mathcal{B}]}}{\text{maximize}}& \sum_{b\in\mathcal{B}}w(b)\cdot\overline{\vartheta}\cdot\overline{x}(b)\\ \text{subject to}&\sum_{b\in\mathcal{B}}w(b)\cdot\overline{q}(b)\leq\sum_{b\in \mathcal{B}}w(b)\cdot\overline{\vartheta}\cdot\overline{x}(b),\\ &\sum_{b\in\mathcal{B}}w(b)\cdot\overline{q}(b)\leq\rho.\end{array} \tag{2.3}\]

and let us denote the value of this LP as \(V\) and its optimizer as \(w_{LP}^{*}\). Here \(\Delta_{|\mathcal{B}|}\) is the set of all probability distributions over \(\mathcal{B}\). We define our benchmark to be:

\[\text{Reward}(\text{Opt})\coloneqq\sum_{t=1}^{T}\sum_{b\in\mathcal{B}}w_{LP}^{ *}(b)\cdot\overline{\vartheta}\cdot\overline{x}(b)=T\cdot V. \tag{2.4}\]

Thus, we are comparing against an algorithm that has knowledge of \(\overline{\vartheta}\), \(\overline{x}\), and \(\overline{\vartheta}\), and plays a bid sampled from \(w_{LP}^{*}\) for each of the \(T\) rounds. This is a commonly used benchmark in the stochastic setting (Gelman and Boyd, 2004; Boyd, 2004). These definitions lead to the following definition of regret of Alg in this setup:

\[\text{Regret}(\text{Alg},\mathcal{P}^{T})\coloneqq\text{Reward}(\text{Opt})- \mathbb{E}_{\overrightarrow{Y}\to\mathcal{P}^{T}}\left[\text{Reward}(\text{ Alg},\overrightarrow{Y})\right]. \tag{2.5}\]

We remark that Reward is defined for some specific input sequence, whereas Regret is defined with respect to a distribution. Additionally, we define budget and RoS constraint violations as \(\sum_{t=1}^{T}q_{t}(b_{t})-\rho T\) and \(\sum_{t=1}^{T}(q_{t}(b_{t})-\eta_{t}\cdot x_{t}(b_{t}))\), respectively.

## 3. UCB-RoS

In this section, we solve the online bidding problem formalized in Problem 2.1 by designing a novel UCB-style algorithm (presented in Algorithm 3). Our approach draws inspiration from the UCB technique widely used in the bandit literature (Bradley, 1998).

We rely on the principle of 'optimism in the face of uncertainty'. At each time step, our algorithm maintains confidence sets for the unknown parameters of the problem, namely the allocation function, the pricing function, and the value distribution. It then selects the bid that maximizes the expected reward within these confidence sets. These confidence intervals are carefully designed to satisfy two key properties: (1) they contain the true expected values with high probability, and (2) they shrink as more data is collected, reflecting increasing confidence in the estimates.

As mentioned earlier, finding the reward-maximizing bid within these confidence intervals is challenging. This optimization problem is inherently non-convex and can be computationally intractable in general. However, by exploiting the specific structure of typical auctions, we derive a simple and efficient solution to this problem, as detailed in Lemma 3.2. We now expand upon these ideas.

Recalling our setup, after submitting bid \(b_{t}\), the bidder obtains the allocation \(x_{t}(\ \cdot\ )\) and price function \(p_{t}(\ \cdot\ )\). Additionally, if the bid is won, then it also obtains its value \(q_{t}\). Let \(N_{t}\) denote the number of times the user wins the bid in the first \(t\) rounds. Then, the algorithm at time step \(t\) updates its sample estimators for the allocation, pricing functions, and value in the following way:

\[\widetilde{x}_{t}(\ \cdot\ )\coloneqq\sum_{s=1}^{t}\frac{x_{s}(\ \cdot\ )\ }{t}, \quad\widetilde{q}_{t}(\ \cdot\ )\coloneqq\sum_{s=1}^{t}\frac{x_{s}(\ \cdot\ )\ }{t}, \quad\widetilde{\eta}_{t}\coloneqq\sum_{s=1}^{N_{t}}\frac{p_{s}}{N_{t}}. \tag{3.1}\]

We remark that the first two estimators are functions defined on \(\mathcal{B}\) and taking values in \([0,1]\), while the value estimator takes real values built only from the subset of samples in which the algorithm wins the bid. Next, we describe our construction of confidence intervals around these estimators, in which we later show (Lemma 3.1) the true expected quantities lie with high probability.

Constructing confidence sets.For every \(t\in[T]\), the algorithm constructs confidence sets centered around the sample estimators defined in Equation (3.1). To introduce these constructions, we first let \(\mathcal{M}\) denote the set of all non-decreasing functions \(f\) on \(\mathcal{B}\), taking values in \([0,1]\). Then, these confidence sets are defined as:

\[\begin{split} C^{\widetilde{x}_{t}}&\coloneqq\left\{ f\in\mathcal{M}\ \mid|f(b)-\widetilde{x}_{t}(b)|\leq\sqrt{\frac{\log(2|\mathcal{B}|T)}{2t}},\, \forall b\in\mathcal{B}\right\},\\ C^{\widetilde{q}_{t}}&\coloneqq\left\{f\in\mathcal{M} \mid|f(b)-\widetilde{q}_{t}(b)|\leq\sqrt{\frac{\log(2|\mathcal{B}|T)}{2t}}, \forall b\in\mathcal{B}\right\},\\ C^{\widetilde{\eta}_{t}}&\coloneqq\left\{\flat\in[0,1] \mid|\flat-\widetilde{\eta}_{t}|\leq\sqrt{\frac{\log(2T)}{2N_{t}}}\right\}. \end{split} \tag{3.2}\]

Interestingly, the confidence set \(C^{\widetilde{q}_{t}}\) does not require its constituent functions to be of the form \(x\cdot p\), rather only that they are clustered around \(\widetilde{q}\). This generality proves crucial in Lemma 3.2. These confidence sets have been constructed to ensure that for all bids, the expectations of the true allocation functions \(\overline{x}(\ \cdot\ )\), pricing functions \(\overline{\vartheta}(\ \cdot\ )\), and values \(\overline{s}\) fall, with high probability, within their respective confidence sets. More precisely, we have the following.

**Lemma 3.1**.: _With probability at least \(1-\frac{1}{T}\), for every \(t\in[T]\) it holds that \(\overline{x}(\ \cdot\ )\in C^{\widetilde{x}_{t}}\), \(\overline{q}(\ \cdot\ )\in C^{\widetilde{q}_{t}}\), and \(\overline{v}\in C^{\widetilde{\eta}_{t}}\), where \(C^{\widetilde{x}_{t}}\), \(C^{\widetilde{q}_{t}}\), and \(C^{\widetilde{\eta}_{t}}\) are as defined in Equation (3.2)._

Proof.As a result of the imposed ranges on \(x_{t}\) and \(p_{t}\), we infer that for each bid \(b\in\mathcal{B}\), the allocation function \(x_{t}(\ \cdot\ )\) and the pricing function \(q_{t}(\ \cdot\ )\) satisfy the bounded difference property, i.e., for any \((x_{t},p_{t})\) and \((x_{t}^{\prime},p_{t}^{\prime})\),

\[|x_{t}(b)-x_{t}^{\prime}(b)|\leq 1 \tag{3.3}\]

[MISSING_PAGE_FAIL:5]

This then implies that for any _fixed_ choice of \(q\) and \(v\), we have

\[\sum_{b\in\mathcal{B}}w(b)\cdot v\cdot x^{1}(b) \leq\sum_{b\in\mathcal{B}}w(b)\cdot v\cdot x^{0}(b),\,\forall b\in \mathcal{B} \tag{3.6}\] \[S(x^{1},q,v) \leq S(x^{0},q,v).\]

Then, combining (3.6) with the definition of \(f\) implies that

\[f(x^{1},q,v)\leq f(x^{0},q,v).\]

We can then infer that, for any fixed choice of \(v\) and \(q\), the maximizer of Problem 3.5 is the function that chooses the upper confidence bound of the current confidence set \(\mathcal{C}^{\tilde{\alpha}_{t}}\), i.e.,

\[x^{*}_{t}(\,\cdot\,)=\min\left\{1,\,\widetilde{x}_{t}(\,\cdot\,)+\sqrt{\frac{ 1}{2t}\log(2|\mathcal{B}|T)}\right\}\]

An analogous argument can be applied to show that \(v^{*}_{t}\) is:

\[v^{*}_{t}=\min\left\{1,\,\widetilde{v}_{t}+\sqrt{\frac{1}{2N_{t}}\log(2T)}\right\}\]

and that \(q^{*}_{t}\) is given by the lower confidence bound of the set \(\mathcal{C}^{\tilde{\alpha}_{t}}\):

\[q^{*}_{t}(\,\cdot\,)=\max\left\{0,\,\widetilde{q}_{t}(\,\cdot\,)-\sqrt{\frac{ 1}{2t}\log(2|\mathcal{B}|T)}\right\}.\]

The form of \(w^{*}_{t}\) is obtained by plugging back into Problem 3.5 the explicit form of \(x^{*}_{t}\), \(\widetilde{q}^{*}_{t}\), and \(v^{*}_{t}\) obtained above. 

Since \(w^{*}_{t}\) is a solution to an LP, one can explicitly compute it with at most \(O(|\mathcal{B}|^{3})\) computational effort [21].

_Regret and constraint violation bound._ Our main result below guarantees an \(\widetilde{O}(\sqrt{T})\) regret and constraint violation bound. We call the event when the concentration results in Lemma 3.1 and Lemma B.1 hold as _clean execution_ and note that it occurs with probability at least \(1-\frac{3}{T}\).

**Theorem 3.3**.: _Consider the online bidding problem described in Section 2. Let \(V\) be the value of the LP defined in Equation (2.3). For any time horizon \(T\), Algorithm 3 suffers the following regret bound in expectation:_

\[\mathbb{E}\left[\mathrm{Regret}(\mathrm{Alg},\mathcal{P}^{T})\right]=O\left( \max\left\{\sqrt{\frac{T\log(|\mathcal{B}|T)}{V}},\frac{\log(|\mathcal{B}|T)}{ V^{2}}\right\}\right),\]

_where regret is as defined in Equation (2.5). Further, the violation of the RoS and budget constraint is, in expectation, at most \(O\left(\sqrt{\frac{T\log(|\mathcal{B}|T)}{V}}\right)\) and \(O(\sqrt{T\log(|\mathcal{B}|T)})\), respectively._

Proof.: First, observe that under clean execution, we have

\[|x^{*}_{t}(t)-\overline{x}(b)|\leq\sqrt{\frac{2}{\tau}\log(2|\mathcal{B}|T)}.\]

Combining this result with Lemma B.1, and utilizing the fact that \(w^{*}_{s}\) is a probability distribution over bids, gives us

\[\left|N_{t}-\sum_{s=1}^{t}\sum_{b\in\mathcal{B}}w^{*}_{s}(b)x^{*}_{s}(b)\right| \leq\frac{2\log(T)}{V}+\frac{V\cdot t}{2}+\sum_{s=1}^{t}\sqrt{\frac{2\log(2| \mathcal{B}|T)}{s}}. \tag{3.7}\]

Under clean execution, \(\overline{x}(\,\cdot\,)\in\mathcal{C}^{\tilde{\alpha}_{t}}\), \(\overline{q}(\,\cdot\,)\in\mathcal{C}^{\tilde{\alpha}_{t}}\), \(\overline{v}\in\mathcal{C}^{\tilde{\alpha}_{t}}\) for each \(t\) and hence \((\overline{x}(\,\cdot\,),\overline{q}(\,\cdot\,),\overline{v},w^{*}_{\mathrm{ LP}})\) is a feasible point for Problem 3.5. Combining this observation with the optimality of \((x^{*}_{s},q^{*}_{s},q^{*}_{s},w^{*}_{s})\), we get that

\[V\leq\sum_{b\in\mathcal{B}}w^{*}_{s}(b)x^{*}_{s}(b)x^{*}_{s}(b)n^{*}_{s}\]

for each \(s\leq t\). Noting that \(s^{*}_{s}\leq 1\), and summing over \(s\), we have that \(t\cdot V\leq\sum_{s=1}^{t}\sum_{b\in\mathcal{B}}w^{*}_{s}(b)x^{*}_{s}(b)\). Using this in (3.7), we get:

\[N_{t}\geq\frac{t\cdot V}{2}-\frac{2\log(T)}{V}-\sqrt{t\log(|\mathcal{B}|T)}. \tag{3.8}\]

Hence,

\[N_{t}\geq\frac{V\cdot t}{3},\quad\forall t\geq\frac{24\log(|\mathcal{B}|T)}{V ^{2}}. \tag{3.9}\]

Consider the "per-round regret"

\[r_{t}=\sum_{b\in\mathcal{B}}w^{*}_{\mathrm{LP}}(b)\cdot\overline{v}\cdot \overline{x}(b)-\sum_{b\in\mathcal{B}}w^{*}_{t}(b)\cdot\overline{v}\cdot \overline{x}(b).\]

We then have:

\[r_{t} \leq\sum_{b\in\mathcal{B}}w^{*}_{t}(b)\cdot v^{*}_{t}\cdot x^{*}_{ t}(b)-\sum_{b\in\mathcal{B}}w^{*}_{t}(b)\cdot\overline{v}\cdot\overline{x}(b) \tag{3.10}\] \[=\sum_{b\in\mathcal{B}}w^{*}_{t}(b)\cdot\left[v^{*}_{t}\cdot(x^ {*}_{t}(b)-\overline{x}(b))+\overline{x}(b)(\sigma^{*}_{t}-\overline{v})\right]\] \[\leq\sqrt{\frac{1}{2t}\log(2|\mathcal{B}|T)}+\sqrt{\frac{3}{2tV} \log(2|\mathcal{B}|T)}\] \[\leq O\left(\sqrt{\frac{1}{tV}\log(|\mathcal{B}|T)}\right),\]

where the first step is by clean execution, Lemma 3.1, and the optimality, for Problem 3.5, of \(v^{*}_{t}\), \(x^{*}_{t}\), and \(w^{*}_{t}\), all of which lie in the confidence intervals given by Lemma 3.1. In the third step we utilize the (3.9) and Lemma 3.1. Next, by definition of \(r_{t}\), we note that \(\sum_{t=1}^{T}r_{t}\) may equivalently be expressed as below:

\[\sum_{t=1}^{T}r_{t}=\mathrm{Reward}(\mathrm{Opt})-\sum_{t=1}^{T}\mathbb{E}_{t-1}[ v_{t}\cdot x_{t}(b_{t})],\]

where \(\mathbb{E}_{t}[\,\cdot\,]\) is the conditional expectation. Computing the expectation over the randomness in the entire sequence of inputs gives:

\[\mathbb{E}_{\overline{j}\sim\mathcal{P}^{T}}\left[\sum_{t=1}^{T}r_{t}\right] =\mathrm{Reward}(\mathrm{Opt})-\mathbb{E}_{\overline{j}\sim \mathcal{P}^{T}}\left[\mathrm{Reward}(\mathrm{Alg},\overline{Y})\right]\] \[=\mathrm{Regret}(\mathrm{Alg},\mathcal{P}^{T}).\]

Because we have a bound on \(\sum_{t=1}^{T}r_{t}\) under clean execution, which holds with a probability at least \(1-\frac{1}{T}\), we can bound the regret as:

\[\mathrm{Regret}(\mathrm{Alg},\mathcal{P}^{T}) \leq\sum_{t=1}^{T}O\left(\sqrt{\frac{1}{Vt}\log(|\mathcal{B}|T)} \right)\left(1-\frac{3}{T}\right)+2T\cdot\frac{3}{T}\] \[\leq O\left(\sqrt{\frac{T\log(|\mathcal{B}|T)}{V}}\right).\]

This completes the proof of the regret bound. We now proceed to bound the violation of the budget constraint under clean execution. To this end, we consider the following expression:

\[\sum_{b\in\mathcal{B}}w^{*}_{t}(b)\cdot\overline{q}(b)=\sum_{b\in\mathcal{B}}w^{* }_{t}(b)\cdot(\overline{q}(b)-q^{*}_{t}(b))+\sum_{b\in\mathcal{B}}w^{*}_{t}(b) \cdot q^{*}_{t}(b). \tag{3.11}\]By clean execution and Lemma 3.1, we have

\[\sum_{t=1}^{T}\sum_{b\in\mathcal{B}}w_{t}^{*}(b)\cdot(\overline{q}(b) -q_{t}^{*}(b)) \leq\sum_{t=1}^{T}\sqrt{\frac{\log(2|\mathcal{B}|T)}{2t}}\] \[\leq O\left(\sqrt{T\log(|\mathcal{B}|T)}\right). \tag{3.12}\]

Next, since \(q_{t}^{*}\) satisfies the per round constraint, we have

\[\sum_{t=1}^{T}\sum_{b\in\mathcal{B}}w_{t}^{*}(b)\cdot q_{t}^{*}(b)\leq\rho T. \tag{3.13}\]

Plugging Inequalities (3.12) and (3.13) into Equation (3.11) yields

\[\sum_{t=1}^{T}\sum_{b\in\mathcal{B}}w_{t}^{*}(b)\cdot\overline{q}(b)\leq O( \sqrt{T\log(|\mathcal{B}|T)})+\rho T. \tag{3.14}\]

The expression on the left-hand side of Inequality (3.14) may be expressed as \(\sum_{t=1}^{T}\sum_{b\in\mathcal{B}}w_{t}^{*}(b)\cdot\overline{q}(b)=\sum_{t= 1}^{T}\mathbb{E}_{t-1}\left[q_{t}(b_{t})\right]\). The expected budget violation may then be bounded as follows:

\[\mathbb{E}_{\overline{T}\sim\mathcal{P}^{T}}\left[\sum_{t=1}^{T}q _{t}(b_{t})-\rho T\right] \leq O(\sqrt{T\log(|\mathcal{B}|T)})\left(1-\frac{3}{T}\right)+ \frac{3\rho T}{T}\] \[\leq O(\sqrt{T\log(|\mathcal{B}|T)}).\]

This concludes the proof of the bound on the total budget violation. To prove our bound on the RoS constraint violation, we apply a similar analysis, which we state here for completeness. Consider again Equation (3.11). Then, Inequality (3.12) holds again, due to clean execution. Continuing the analysis, we have

\[\sum_{t=1}^{T}\sum_{b\in\mathcal{B}}w_{t}^{*}(b)q_{t}^{*}(b)\leq\sum_{t=1}^{T} \sum_{b\in\mathcal{B}}w_{t}^{*}(b)x_{t}^{*}(b)v_{t}^{*},\]

because of optimality of \(v_{t}^{*}\), \(w_{t}^{*}\), \(x_{t}^{*}\), and \(q_{t}^{*}\) for Problem 3.5 (from Lemma 3.2). Repeating, on the right-hand side above, the steps from Inequality (3.10), we get the following bound:

\[\sum_{t=1}^{T}\sum_{b\in\mathcal{B}}w_{t}^{*}(b)\cdot q_{t}^{*}(b)\] \[\leq O\left(\sqrt{\frac{T\log(|\mathcal{B}|T)}{V}}\right)+\sum_{ t=1}^{T}\sum_{b\in\mathcal{B}}w_{t}^{*}(b)\cdot\overline{v}\cdot\overline{x}(b). \tag{3.15}\]

From Inequality (3.15), we can obtain the following bound:

\[\mathbb{E}_{\overline{T}\sim\mathcal{P}^{T}}\left[\sum_{t=1}^{T} q_{t}(b_{t})-\sum_{t=1}^{T}\mathbb{E}_{t}\cdot x_{t}(b_{t})\right]\] \[\leq O\left(\sqrt{\frac{T\log(|\mathcal{B}|T)}{V}}\right)\left(1 -\frac{3}{T}\right)+\frac{6T}{T}\] \[\leq O\left(\sqrt{\frac{T\log(|\mathcal{B}|T)}{V}}\right).\]

This concludes the proof of the RoS constraint violation bound in expectation and therefore finishes the proof of the lemma. 

Observe that we have exhibit a logarithmic dependence on \(|\mathcal{B}|\). This is in contrast with existing algorithms for this problem, which suffer from a \(\sqrt{|\mathcal{B}|}\) dependence. Moreover, ignoring the dependence on \(\mathcal{B}\), our algorithm achieves \(\widetilde{O}(\sqrt{T/V})\) regret and constraint violation bounds. In contrast, primal-dual approaches yield \(\widetilde{O}(\sqrt{T/\kappa})\) bounds (where \(\kappa\) is the Slater slack) (Gardner, 2017). In many practical scenarios, \(\kappa\) can be very close to zero, while \(V\) remains bounded away from zero (see Appendix D). Consequently, our approach provides significantly stronger guarantees in cases where primal-dual methods may suffer from large regret and constraint violations. Furthermore, we believe that a dependence on \(V\) is unavoidable. This conjecture is supported by the lower bound established in (Barbieri et al., 2017) for online bidding with unknown value (albeit without RoS constraints), which depends on a quantity proportional to \(1/V\).

_Remark 3.1_ (Extension to linear bandits).: While our focus is on online bidding, our algorithm and analysis can be extended to the more general setting of stochastic linear bandits with linear long-term stochastic constraints (see Appendix E for results). The regret and constraint violation bounds in this case avoid the Slater slack \(\kappa\), thus improving on the existing primal-dual algorithms.

We now strengthen the in-expectation regret and constraint violation bounds of Theorem 3.3 by providing high-probability guarantees. These stronger bounds are obtained by leveraging Azuma's inequality (Fact A.3) to bound the deviations of the key quantities from their expectations

**Theorem 3.4** (High Probability bounds).: _Given i.i.d. inputs from a distribution \(\mathcal{P}\) over a time horizon \(T\) to Algorithm 3, with a probability at least \(1-\frac{5}{T}\), we have that:_

\[\mathrm{Reward}(\mathrm{Opt})-\mathrm{Reward}(\mathrm{Alg}.\, \overline{V})\leq O\left(\sqrt{\frac{T\log(|\mathcal{B}|T)}{V}}\right),\] \[\sum_{t=1}^{T}q_{t}(b_{t})\leq\rho T+O(\sqrt{T\log(|\mathcal{B}|T )}),\] \[\sum_{t=1}^{T}q_{t}(b_{t})\leq\sum_{t=1}^{T}n_{t}\cdot x_{t}(b_{t} )+O\left(\sqrt{\frac{T\log(|\mathcal{B}|T)}{V}}\right).\]

Theorem 3.4 demonstrates that, with high probability, the sample pathwise constraint violation of Algorithm 3 is bounded by \(O\left(\sqrt{\frac{T\log(|\mathcal{B}|T)}{V}}\right)\). This strengthens the in-expectation bounds from Theorem 3.3 (see Appendix C for the proof).

## 4. Experiments

We empirically study the performance of UCB-RoS and compare it with other existing approaches on synthetically generated datasets. We create synthetic problems where \(v_{t}\), \(x_{t}(\,\cdot\,)\), \(q_{t}(\,\cdot\,)\), and \(B_{t}^{\mathsf{G}}\) are sampled i.i.d. from specified distributions. Given a pre-specified mean value \(\overline{v}_{t}\), the values \(v_{t}\) are sampled i.i.d. from a corresponding beta distribution with shape parameters \((105,10\cdot(1-\overline{v}))\). The bidding set \(\mathcal{B}\) is assumed to be a uniformly spaced grid over \([0,1]\) with grid size of \(1/|\mathcal{B}|\). The competing bid distribution of \(B_{t}^{\mathsf{G}}\) is a discrete distribution over \(\mathcal{B}\). Finally, the type of auction is also given as input. We allow for two types of auctions \(-\) first-price and second-price auctions. The distributions of \(x_{t}(\,\cdot\,)\) and \(q_{t}(\,\cdot\,)\) are fixed with these inputs of \(B_{t}^{\mathsf{G}}\), \(v_{t}\), and the auction type. We compare the performance of UCB-RoS against the approaches in (Gardner, 2017; Gardner, 2017).

The work of Castiglioni et al. (Castiglioni et al., 2018) suggests a meta algorithmic game between a primal regret minimizing algorithm and a dual algorithm that minimizes the constraint violation. In our implementation of their algorithm, we choose the primal regret minimizer to be Exp3.P.1, as given in Auer et al. (Auer et al., 2016, Section 6), and the full information dual minimizer to be the DS-OMD algorithm, introduced in Fang et al. (Eang et al., 2015, Section 6). The work of Bernasconi et al. (Bernasconi et al., 2018) weights the constraint violation in a time decaying fashion and uses the primal regret minimizer EXP-IX of Neu (Steiner, 2018).

We create a bidding instance with the parameters in Table 1 along with \(w_{\text{LP}}^{\text{s}}\) and \(V\) of the benchmark (2.3).

Figure 1(a), Figure 1(b) shows the distribution of the competing bids (\(B_{\text{f}}^{\text{E}}\)). This distribution has a mode at the bid \(b=0.333\). Figure 1(c) plots expected pricing \(\overline{q}\) ( ) and realized value \(\overline{x}\)( ) \(\overline{y}\) as function of the bids. The bids with value and budget curves above the pricing curve are the feasible bids that satisfy the budget and RoS constraint in expectation. For the instance in Table 1, we see that all bids are feasible, and hence, the optimal \(w_{\text{LP}}^{\text{s}}\) is \(\delta_{1}(b)\). Thus, for this optimal allocation, the budget and RoS constraints are exactly satisfied. This suggests that both the constraints can be binding.

The experimental results are shown in Figure 3 for different horizons up to \(2\times 10^{5}\). Our algorithm, UCB-RoS (depicted in yellow), has a much smaller regret than those of Castiglioni et al. (2018) (in green) and (Bernasconi et al., 2018) (in blue). This primarily reflects our improved dependence on \(|\mathcal{B}|\). The two baselines have much smaller constraint violations than UCB-RoS, which suggests that they each achieve a lower constraint violation at the cost of incurring near-linear regret. This near-linear regret for the chosen horizons is due to their worse dependence on \(|\mathcal{B}|\). Hence, these baselines achieve sublinear regret only over much larger horizons. In contrast, UCB-RoS achieves a much better trade-off between regret and constraint violation.

We remark that the algorithms in the works of Castiglioni et al. (Castiglioni et al., 2018) and Bernasconi et al. (Bernasconi et al., 2018) were designed for _both_ adversarial and stochastic rewards. Often, such algorithms are outperformed by algorithms designed for specific stochastic setting.

## 5. Conclusion and Future Work

In this paper, we studied online bidding with RoS and budget constraints when the value of an impression is unknown a priori. We developed a novel UCB-style algorithm that achieves near-optimal regret and constraint violation bounds without relying on restrictive assumptions like the existence of a Slater point. Our algorithm is not only theoretically sound but also computationally efficient. This work opens up several exciting avenues for future research. One direction is to extend our approach to more complex settings, such as those with multiple advertisers. Another promising direction is to consider adversarial environments where the competing bids or impression values are chosen adversarially. Finally, it would be valuable to develop variants of our algorithm that can incorporate contextual information into the decision-making process. We believe that our work takes a significant step towards developing more robust and effective bidding algorithms for online advertising.

## References

* (1)
* Achdokou et al. (2021) Juliette Achdokou, Olivier Cappe, and Aurelien Garivier. 2021. Efficient Algorithms for Stochastic Repeated Second-priceuctions. In _Algorithmic Learning Theory & Its Application 2021, Virtual Conference, HybridWorks (Proceedings of Machine Learning Research)_, Vitaly Feldman, Katrina Ligett, and Siran Sabato (Eds.), Vol. 132. PMLR, 99-150. [http://proceedings.mlr.press/v132/Audokou21a.html](http://proceedings.mlr.press/v132/Audokou21a.html).
* Agarwal et al. (2019) Gang Agarwal, Ashurikumar Badanadhyram, and Aranayk Mehta. 2019. Auto-bidding with constraints. In _International Conference on Web and Internet Economics_. Springer, 17-30.
* Agrawal and Devanur (2014) Shipra Agrawal and Nikhil P Devanur. 2014. Fast algorithms for online stochastic convex programming. In _Proceedings of the twenty-sixth annual ACM-SIAM symposium on Discrete algorithms_. SIAM, 1405-1424.

\begin{table}
\begin{tabular}{c c} \hline
**Parameter** & **Value** \\ \hline \(\mathcal{B}\) & \(\{0,0.33,0.66,1\}\) \\ \(\rho\) & \(0.4\) \\ \(\overline{y}\) & \(0.4\) \\ \(\overline{x}\) & \(0.4\) \\ \(\text{Auction type}\) & Second Price \\ Value distribution & \(\text{Beta}\{10\overline{x},10(1-\overline{y})\}\) \\ \(w_{\text{LP}}^{\text{s}}\) & \([0,\,0,\,0,\,1]\) \\ \(V\) & \(0.4\) \\ \hline \end{tabular}
\end{table}
Table 1. Table with parameter and benchmark values.

Figure 2. Comparison between UCB-RoS (in yellow), Castiglioni et al. (Castiglioni et al., 2018) (in green), and Bernasconi et al. (Bernasconi et al., 2018) (in blue).

Online bidding under RoS Constraints without Knowing the Value
* [89] Rui Ai, Chang Wang, Chenchen Li, Jianhua Zhang, Wenhan Huang, and Xiaotie Deng. 2022. No-regret Learning in Repeated First-Price Auctions with Budget Constraints.
* [90] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. 2002. Finite-time Analysis of the Multiarmed Bandit Problem. _Mach. Learn._ 47, 2-3 (2002). [https://doi.org/10.1023/A:10388970352](https://doi.org/10.1023/A:10388970352)
* [91] Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. 2002. The nonstochastic multiarmed bandit problem. _SIAM journal on computing_ 32, 1 (2002), 48-77.
* [92] Moshe Babaif, Richard Coles, Joseph Martiline, Nicole Immortalte, and Brendan Lucier. 2020. Non-Ques-Linear Agents in Quasi-Linear Mechanisms. In _12th Innovations in Theoretical Computer Science Conference (TCS 2021)_. Schloss Dagstuhl-Leibniz-Zentrum fur Informatik.
* [93] Ashwin-Kumar Bahadilyavur, Zhe Feng, and Guru Guruganneh. 2021. Learning to Bid in Contextual First Price Auctions. _CoRR_ abs/2109.03173 (2021). arXiv:2009.03173
* [94] Ashwin-Kumar Bahadilyavur, Robert Kleinberg, and Aleksandre Slivelins. 2018. Bandits with knapapcasts. _Journal of the ACM (JACM)_ 65, 3 (2018), 1-55.
* [95] Santiago Balseiro, Ralfano Lu, and Vahro Mirok. 2020. Dual Mirror Descent for Online Allocation Problems. In _Proceedings of the 37th International Conference on Machine Learning (Proceedings of Machine Learning Research)_. Hal Daume III and Aarti Singh (Eds.), Vol. 119. PMLR, 613-628.
* [96] Santiago Balseiro, Yuan Deng, Jieting Mao, Vahub S Mirokani, and Song Zhou. 2021. The landscape of auto-bliding auctions: Value versus utility maximization. In _Proceedings of the 22nd ACM Conference on Economics and Computation_. 132-133.
* [97] Santiago Balseiro and Vonatan Gur. 2019. Learning in repeated auctions with budget-s: Report minimization and equilibrium. _Management Science_ 65, 9 (2019), 3952-3964.
* [98] Amir Beck. 2017. _First-order methods in optimization_. SIAM.
* [99] Martin Bernasconi, Matteo Catsigliand, Andrea Celli, and Federico Fusco. 2024. Beyond Primal-Dual Methods in Bandits with Stochastic and Adversarial Constraints. _arXiv preprint arXiv:2503.16118_ (2020).
* [100] Blackwell. 1997. Large deviations for martingales. _Festchrift for Lavien Le Cam: Research Papers in Probability and Statistics_ (1997), 89-91.
* [101] Christian Borgs, Jennifer Chayes, Nicole Immortalte, Kamal Jain, Omid Eleanari, and Mohammad Mahdian. 2007. Dynamics of bid optimization in online advertisement auctions. In _Proceedings of the 16th international conference on World Wide Web_. 531-540.
* [102] Stephen P. Boyd and Lieven Vandenberghe. 2014. Convex Optimization. Cambridge University Press. [https://doi.org/10.1017/CBO97851180441](https://doi.org/10.1017/CBO97851180441)
* [103] Matteo Catsigliand, Andrea Celli, Alberto Marchesi, Guilla Romano, and Nicola Gatti. 2022. A Unifying Framework for Online Optimization with Long-Term Constraints. _arXiv preprint arXiv:2209.04540_ (2022).
* [104] Andrea Celli, Riccardo Colin-Bialek, Christian Korot, and Eric Spedlosa. 2022. The Parity Ray Regularization for Pacing in Action Markets. In _Proceedings of the ACM Web Conference 2022 (WWW '22)_. Association for Computing Machinery, New York, NY, USA, 162-172.
* [105] Fan Chung and Linyuan Lu. 2006. Concentration inequalities and martingale inequalities: a survey. _Internet Mathematics_ 3, 1 (2006), 79-127.
* [106] Michael N. Cohen, Yin Tat Lee, and Zhao Song. 2021. Solving linear programs in the current matrix multiplication time. _Journal of the ACM (JACM)_ 68, 1 (2021), 1-39.
* [107] Yuan Deng, Negin Golrezaei, Patrick Jaillet, Jason Cheek Nam Liang, and Vahab Mirrokni. 2024. Individual Welfare Guarantees in the Antibudding World with Machine-learned Advice. In _Proceedings of the ACM on Web Conference_. 2024, 267-275.
* [108] Yuan Deng, Jieming Mao, Vahub Mirrokni, and Song Zuo. 2021. Towards efficient auctions in an auto-bliding world. In _Proceedings of the Web Conference 2021_. 3969-3973.
* [109] Nikki M. E. Devaux, Kamal Jain, Balasubramanian Sivan, and Christopher A. Willems. 2019. Near Optimal Online Algorithms and Fast Approximation Algorithms for Resource Allocation Problems. _J. ACM_ 66, 1 (2019). [https://doi.org/10.1145/23284177](https://doi.org/10.1145/23284177)
* [110] Huang Fang, Nicholas JA Harvey, Victor S Portella, and Michael P. Friedlander. 2022. Online mirror descent and dual averaging: keeping pace in the dynamic case. _Journal of Machine Learning Research_ 23, 121 (2021), -138.
* [111] Zhe Feng, Swoit Padmanabhan, and Di Wang. 2023. Online bidding Algorithms for Return-on-Spend Constrained Advertisers. In _Proceedings of the ACM Web Conference 2023_. 3530-3560.
* [112] Zhe Feng, Chara Fodlanta, and Vasilis Syrgelans. 2018. Learning to bid Without Knowing your Value. In _Proceedings of the 2018 ACM Conference on Economics and Computation, Ithaca, NY, USA, June 28-29, Rio Paso, California, and Rakesh Vohra (Eds.)_. ACM, 506-522. [https://doi.org/10.1145/3219166.3219208](https://doi.org/10.1145/3219166.3219208)
* [113] Aditya Gangnagar, Tianrui Chen, and Venkatesh Saligrama. 2024. Safe Linear Bandits over Unknown Polytopes. In _The Thirty Seventh Annual Conference on Learning Theory_. PMLR, 1755-1759.
* [114] Yuan Gao, Kaiyu Yang, Yuanlong Chen, Min Liu, and Noureddine El Karousi. 2022. Bidding Agent Design in the LinkedIn Ad Markertbelger. _arXiv preprint arXiv:2202.12272_ (2022).
* [115] Neglien Golrezaei, Patrick Jaillet, Jason Cheek Nam Liang, and Vahab Mirrokni. 2021. Bidding and Pricing in Budget and ROI Constrained Markets. _arXiv preprint arXiv:2107.07725_ (2021).
* [116] Tsaipan Han, Zhengyuan Zhou, Aaron Flores, Erik Ordentlich, and Tsachy Weissman. 2020. Learning to Bid Optimally and Efficiently in Adversarial First-price auctions. _CoRR_ abs/2007.04568 (2020). arXiv:2007.04568
* [117] Nicole Immortalte, Karthik A Shivaramaraman, Robert Schapire, and Aleksandre Slivelins. 2019. Adversarial bandits with knapcasts. In _2020 IEEE 20th Annual Symposium on Foundations of Computer Science (FOCS)_. IEEE, 202-219.
* [118] V. Krishna. 2002. _Anton Theory_. Elsevier Science.
* [119] Vijay Krishna. 2009. _Anton theory_. Academic press.
* [120] Tae Lattinore and Caba Szepwari. 2020. _Bandit algorithms_. Cambridge University Press.
* [121] Kuang-Chih Lee, Ali Jalali, and Ali Dasdan. 2013. Real time bid optimization with smooth budget delivery in online advertising. In _Proceedings of the seventh international workshop on data mining for online advertising_. 1-9.
* [122] Xin Liu, Bin Li Pengyi, and Lijia Yang. 2021. An efficient pessimistic optimistic algorithm for stochastic linear bandits with general constraints. _Advances in Neural Information Processing Systems_ 34 (2021), 20475-20408.
* [123] Brendan Lucier, Sarath Pattalikashakis Shivanis, and Mengiotou Zhang. 2020. Autobability with budget and on constraints: Efficiency, regret, and pacing dynamics. In _The Thirty Seventh Annual Conference on Learning Theory_. PMLR, 3642-3643.
* [124] Mehrdad Mahdavi, Rong Jin, and Tanbao Yang. 2012. Trading regret for efficiency: online convex optimization with long term constraints. _The Journal of Machine Learning Research_ 13, 1 (2012), 2503-2528.
* [125] Mehrdad Mahdavi, Tianbao Yang, and Hong Jin. 2013. Stochastic convex optimization with multiple objectives. _Advances in neural information processing systems_ 26 (2013).
* [126] Shie Mamou, John N Tsitsiklis, and Jia Yuan Yu. 2009. Online Learning with Sample Path Constraints. _Journal of Machine Learning Research_ 10, 3 (2009).
* [127] Thomas Neddee, Clement Calvanueva, Nourdeniz Li, and Yuim Vianey. 2022. Learning in Repeated Auctions. _Foundations and Trends(r) in Machine Learning_ 15, 3 (2022), 176-348. [https://doi.org/10.1561/20000077](https://doi.org/10.1561/20000077)
* [128] Gengby Nue. 2015. Explore no more: Improved high-probability regret bounds for non-stochastic bandits. _Advances in Neural Information Processing Systems_ 28 (2015).
* [129] Gai Noti and Vasilis Syrgelans. 2021. Bid Prediction in Repeated Auctions with Learning. In _Proceedings of the Web Conference 2021 (WWW '21)_. Association for Computing Machinery, New York, NY, USA, 3953-3963.
* [130] Aldo Pacchan, Mohammad Ghivamzadeh, and Peter Bartlett. 2024. Contextual Bandits with Ships: Some Constraints. _arXiv preprint arXiv:2408.0808_ (2024).
* [131] Aleksandre Slivelins, Karthik A Shivarajaraman, and Dylan J Foster. 2023. Contextual bandits with packing and covering constraints: A modular lagrangian approach via expression. In _The Thirty Sixth Annual Conference on Learning Theory_. PMLR, 6433-6465.
* [132] Rotem Stram, Rani Abboud, Alex Shaf, Oren Sombi, Ariel Ravi, and Yair. Karen. 2021. Miquel, Aude: Budget Pricing System for Performance Optimization in Online Advertising. In _Companion Proceedings of the ACM on Web Conference 2020_. 433-442.
* [133] Roman Vershynin. 2018. _High-dimensional probability: An introduction with applications of data science_. Vol. 7. Cambridge university press.
* [134] Juntao Ween, Vijay Perchet, and Philippe Rigollet. 2016. Online learning in repeated auctions. In _Conference on Learning Theory_. PMLR, 1562-1583.
* [135] Hao Yu, Michael Neyb, and Xiaohun Wei. 2017. Online convex optimization with stochastic constraints. _Advances in Neural Information Processing Systems_ 30 (2017).
* [136] Hao Yu and Michael J Neely. 2020. A Low Complexity Algorithm with \(O(\sqrt{T})\). Regret and \(O(1)\) Constraint Validating for Online Convex Optimization with Long Term Constraints. _Journal of Machine Learning Research_ 11, (2009), 1-24.
* [137] Jun Zhao, Guang Qiu, Ziyui Guan, Wei Zhao, and Xiaotie He. 2018. Deep reinforcement learning for sponsored search-real-time bidding. In _Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining_. 1021-1030.
* [138] Xingyu Zhou and Bo Ji. 2022. On kernelized multi-armed bandits with constraints. _Advances in neural information processing systems_ 35 (2022), 14-26.

## Appendix A Standard Concentration Results

We now state some well-known concentration inequalities that we use in our proofs.

[MISSING_PAGE_FAIL:10]

1. Loss observations and cost observations: \[f_{t}(x_{t})=\langle f,x_{t}\rangle+\epsilon_{t,x},\quad g_{t,i}(x)=\langle g_{i},x _{t}\rangle+\epsilon_{i,t,x},\] where \(\epsilon\) are 1-sub gaussian noise.
2. \(\|f\|_{2}\leq B\) and \(\|g_{t}\|_{2}\leq B\). Here, \(f_{i}\) are in \(\mathbb{R}^{d}\).
3. Action set \(X\) is a compact convex set of \(\mathbb{R}^{d}\).

The agent pulls an arm \(x_{t}\) at time \(t\) and receives a noisy reward \(f_{t}(x_{t})\) and \(i\in[m]\) constraint values \(g_{t,i}(x_{t})\). The goal is to minimize regret with respect to the stationary benchmark

\[x_{\text{OPT}}=\operatorname*{argmax}_{x\in X}(f,x)\] subject to \[(g_{t},x)\leq 0,\quad\forall i\in[m],\]

while trying to ensure the constraint violation \(-\sum_{t=1}^{T}g_{t,i}(x_{t})\) is sublinear. The regret is formally defined as

\[\text{Regret}=T\cdot\langle f,x_{\text{OPT}}\rangle-\mathbb{E}\left[\sum_{t=1} ^{T}f_{t}(x_{t})\right].\]

### UCB-based Algorithm

We next describe various aspects of the UCB-style algorithm.

_OLS estimators_: The algorithm maintains a set of Ordinary Least Squares (OLS) estimators for \(f,g_{t}\). At time \(t\) we set the OLS estimators to be:

\[\widehat{f}_{t} \coloneqq(\lambda I+\sum_{s=1}^{t}x_{s}x_{s}^{T})^{-1}\sum_{s=1}^ {t}f_{s}(x_{s})x_{s} \tag{128}\] \[\widehat{g}_{t} \coloneqq(\lambda I+\sum_{s=1}^{t}x_{s}x_{s}^{T})^{-1}\sum_{s=1}^ {t}g_{s,i}(x_{s})x_{s}.\]

These OLS estimators satisfy concentration inequalities. Let \(V_{t}=\lambda I+\sum_{s=1}^{t}x_{s}x_{s}^{T}\), then we have that with probability \(1-\delta\):

\[\|\widehat{f}-f\|_{V_{t}}\leq\sqrt{d\log\left(1+\frac{dB^{2}/ \lambda}{\delta}\right)}+\lambda^{1/2}B \tag{129}\] \[\|\widehat{g}_{t,t}-g_{t}\|_{V_{t}}\leq\sqrt{d\log\left(1+\frac{ dB^{2}/\lambda}{\delta}\right)}+\lambda^{1/2}B,\]

which can be derived using subgaussian concentration in a manner similar to Theorem 20.5 (Srivastava et al., 2016).

_Confidence sets_: Based on the above concentration inequalities one can derive confidence ellipsoids:

\[C_{f,t}\coloneqq\left\{f^{\prime}\left\|\|\widehat{f}_{t}-f\|_{V_{t}}\leq\beta _{t}\right\},\quad C_{g,t}\coloneqq\left\{g^{\prime}\left\|\|\widehat{g}_{t,t }-g_{t}\|_{V_{t}}\leq\beta_{t}\right\},\right.\right.\]

such that, with high probability, \(f,g_{t}\) lie in these sets. Here, we have \(\beta_{t}=\sqrt{d\log\left(1+\frac{dB^{2}/\lambda}{\delta}\right)}+\lambda^{1/ 2}B\).

_Algorithm:_ For each time \(t\), the algorithm repeats the following three steps sequentially:

1. Action \(x_{t}\) is chosen as follows: \[x_{t+1}=\operatorname*{arg\,min}_{x\in X}\quad\min_{f^{\prime}\in C_{f,t}} \quad\min_{\begin{subarray}{c}g_{t}(x)\leq\beta_{t}\\ g_{t}(x)\leq g_{t}\end{subarray}}\langle f^{\prime},x\rangle.\] (E.1)
2. Observe the noisy rewards \(f_{t}(x_{t})\) and the constraints \(g_{t,i}(x_{t})\).

Update the OLS estimators and \(V_{t}\) to incorporate the new data.
3. Create confidence sets based on updated \(V_{t}\) and OLS estimators.

_Remark E.1_.: The setting of linear bandits with linear constraints that we consider here Equation (E.1) can, in general, be computationally very hard to solve without further structure in the problem.

### Regret and Constraint Violation Analysis

A regret and constraint violation bound may be derived by largely following the template of Theorem 3.3. The difference from the bidding problem is that, in this case, we do not have to derive concentration bounds for quantities like \(N_{t}\).

_Regret analysis_: Assume that the minimizers in Equation (E.1) are \(\overline{f}_{t}\) and \(\overline{g}_{t}\), respectively. Further, we assume it holds with probability \(1-\delta_{0}\) (this can be done by choosing \(\delta=\frac{\delta_{0}}{(m+1)T}\) and using union bound over \(i,t\)) that \(f,g_{i}\) always belong in their respective confidence sets for all time \(t\in[T]\). Then, by definition of the UCB choice, we have:

\[\langle f,x\rangle\geq\langle\overline{f}_{t},x_{t}\rangle.\]

Figure 4. Comparison between UCB-RoS (in yellow), Castiglioni et al. (2018) (in green), and Bernasconi et al. (2014) (in blue).

Figure 3. Figures (a), (b) show the distribution over bids for the competing bidders. Figure (c) shows the expected pricing, expected value and budget curves over the bids.

Hence, we have that:

\[r_{t} =\langle f,x_{t}\rangle-\langle f,x\rangle\] \[\leq\langle f,x_{t}\rangle-\langle\overline{f}_{t},x_{t}\rangle\] \[\leq\|f-\overline{f}_{t-1}\|_{V_{t-1}}\|x_{t}\|_{V_{t-1}^{-1}}+\| \overline{f}_{t}-\overline{f}_{t-1}\|_{V_{t-1}}\|x_{t}\|_{V_{t-1}^{-1}}\] \[\leq 2\beta_{t-1}\|x_{t}\|_{V_{t-1}^{-1}}.\]

Since \(\beta_{t}\) is a increasing sequence and using the elliptical potential bound with log determinant (Theorem 19.3 in [35]), we have that:

\[\text{Regret}=\sum_{s=1}^{T}r_{s} \leq\sqrt{T\sum_{s=1}^{T}r_{s}^{2}}\] \[\leq\sqrt{16T\beta_{T}^{2}}\log((\det(V_{T})/\det(V_{0})).\]

_Constraint violations._ We analyze the constraint violation next. Let \(r_{s,i}=\langle g_{i},x_{s}\rangle\). Then, we have that:

\[r_{t,i} \leq\langle g_{i},x_{s}\rangle-\langle\overline{g}_{it},x_{t}\rangle\] \[\leq\|g_{i}-\overline{g}_{it-1}\|_{V_{t-1}}\|x_{t}\|_{V_{t-1}^{- 1}}+\|\overline{g}_{it}-\overline{g}_{it-1}\|_{V_{t-1}}\|x_{t}\|_{V_{t-1}^{-1}}\] \[\leq 2\beta_{t-1}\|x_{t}\|_{V_{t-1}^{-1}}.\]

Thus we get the following bound in the manner as before

\[\sum_{t=1}^{T}r_{t,i}\leq\sqrt{16T\beta_{T}^{2}}\log((\det(V_{T})/\det(V_{0})).\]

Further, \(\log((\det(V_{T})/\det(V_{0}))\leq d\log\big{(}1+TB^{2}/d\lambda)\). 

This gives a proof sketch for the following theorem.

**Theorem**.: _The UCB-based algorithm in Appendix E.2, with \(\lambda=\theta(1)\) and \(\delta=1/T\) in \(\beta_{t}\), has a regret bound of_

\[\text{Regret}\leq\widetilde{\mathcal{O}}(dB\sqrt{T})\]

_and constraint violation bounds of \(\widetilde{\mathcal{O}}(dB\sqrt{T})\) in expectation._