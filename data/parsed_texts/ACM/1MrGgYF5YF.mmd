# FG-CIBGC: A Unified Framework for Fine-Grained and Class-Incremental Behavior Graph Classification

Anonymous Author(s)

###### Abstract.

Learning-based Behavior Graph Classification (BGC) has been widely adopted in Internet infrastructure for partitioning and identifying similar behavior graphs. However, the research communities realize significant limitations when deploying existing proposals in real-world scenarios. The challenges are mainly concerned with (i) fine-grained emerging behavior graphs, and (ii) incremental model adaptations. To tackle these problems, we propose to (i) mine semantics in multi-source logs using Large Language Models (LLMs) under In-Context Learning (ICL), and (ii) bridge the gap between Out-Of-Distribution (OOD) detection and class-incremental graph learning. Based on the above core ideas, we develop the first unified framework termed as Fine-Grained and Class-Incremental Behavior Graph Classification (**FG-CIBGC**). It consists of two novel modules, i.e., gPartition and gAdapt, that are used for partitioning fine-grained graphs and performing unknown class detection and adaptation, respectively. To validate the efficacy of FG-CIBGC, we introduce a new benchmark, comprising a new 4,992-graph, 32-class dataset generated from 8 attack scenarios, as well as a novel Edge Intersection over Union (EIoU) metric for evaluation. Extensive experiments demonstrate FG-CIBGC's superior performance on fine-grained and class-incremental BGC tasks, as well as its ability to generate fine-grained behavior graphs that facilitate downstream tasks. The code and dataset are available at: [https://anonymous.4open.science/r/FG-CIBGC-70BC/README.md](https://anonymous.4open.science/r/FG-CIBGC-70BC/README.md).

**CCS Concepts - Mathematics of computing \(\rightarrow\) Graph algorithms; - Networks \(\rightarrow\) Network security.**

## 1. Introduction

Sophisticated attacks increasingly threaten the global internet infrastructure. As graph offers an ideal representation for security investigation (Hernandez et al., 2017), analysts often transform audit logs into a large, unified graph containing numerous operations. However, navigating and investigating the large-scale graph presents a non-trivial challenge of heavy analysis workload (Kirchhoff et al., 2017). Behavior Graph Classification (BGC) addresses this challenge by partitioning the large graph into a set of smaller behavior graphs and subsequently classifying them, enabling analysts to focus on a few representative behaviors. BGC has emerged as an indispensable technique for various security investigation domains (Kirchhoff et al., 2017), including Host Intrusion Detection Systems (HIDSs), vulnerability detection, _etc_.

Existing solutions on BGC task can be categorized into three types: pattern-based (Zhu et al., 2017; Wang et al., 2017), rule-based (Zhu et al., 2017; Wang et al., 2017; Wang et al., 2017; Wang et al., 2017), and learning-based (Zhu et al., 2017; Wang et al., 2017). The former two rely on static patterns and expert knowledge, demanding heavy manual effort. Learning-based methods have addressed this limitation by leveraging machine learning models. While it sounds promising, the research communities have uncovered a series of limitations when implementing the learning-based approaches in real-world scenarios. By summarizing those issues in Fig. 1, we recognize the following two main challenges.

**(i) Fine-Grained Emerging Behavior Graphs.** Prior learning-based proposals rely solely on coarse-grained behavior graphs. That is to say, while these methods can ascertain whether a behavior graph is related to a specific service (e.g., "apache"), they lack the granularity to differentiate between distinct operations that service, such as distinguishing "apache processing request 1" from "apache processing request 2". Partitioning discrete operations from large graphs generated by audit logs remains a formidable challenge, as existing approaches consolidate all operations on a given object into a single graph, limiting their ability to differentiate distinct service operations. Yet fine-grained labels are pivotal for analysts to comprehend service activities and deploy effective countermeasures. Existing coarse-grained BGC approaches present a significant _semantic gap_ between graph identification and actionable intelligence. A more granular scheme capable of automatically distinguishing distinct service operations would substantially enhance the understanding of attack vectors and facilitate targeted

Figure 1. An illustration of fine-grained and class-incremental behavior graph classification task. This task faces both _fine-grained emerging behavior graphs_ and _incremental model adaptations_ challenges, leading to performance degradation of state-of-the-art baselines.

defenses. _Consequently, the primary challenge lies in developing a methodology for fine-grained behavior graph partition._

**(ii) Incremental Model Adaptations.** In real-world scenarios, behavior graphs evolve in an incremental manner, presenting the requirement of _class increments. Class increments_ refer to emerging novel classes that should be incrementally updated into the model to become known classes subsequently. In production environments, continuous service updates introduce novel behavior graph classes unknown to analysts (also known as the _open-world_ issue). Actively detecting and attaching new classes to a model's knowledge base without _catastrophic forgetting_ is a significant challenge. Despite the importance, graph-level class-incremental learning with novel class detection remains a largely unexplored area.

In this paper, we propose the first unified framework called Fine-Grained and Class-Incremental Behavior Graph Classification (**FG-CIBGC**), aiming to enable two vital abilities, i.e., the fine-grained behavior graph partitioning and incremental model updates with novel class detection. At the high level, FG-CIBGC is designed with two novel components named _gPartition_ and _gAdapt_. **First**, gPartition processes multi-source logs by leveraging Large Language Models (LLM) under In-Context Learning (ICL) paradigm to correlate semantically similar logs, forming _behavior units_. Each _behavior unit_ corresponds to a single operation performed by a service application and is subsequently converted into a compact behavior graph. **Second**, gAdapt is responsible for utilizing Out-Of-Distribution (OOD) detection to identify unknown classes, assigns fine-grained labels to both known and unknown classes of behavior graphs, and incrementally updates the model accordingly. **Finally**, existing BGC benchmarks suffer from insufficiencies in both completeness (lacking multi-source logs) and diversity, as well as the absence of metrics tailored to the fine-grained requirements of BGC tasks. To address these issues, we introduce a new benchmark comprising 8 attack scenarios and 3 log types, amounting to 4,992 graphs across 32 classes. Additionally, we propose a novel metric, Edge Intersection over Union (EIoU), to fully evaluate FG-CIBGC.

In summary, this paper makes three key contributions:

* Through analysis of current learning-based behavior graph classification proposals, we identify two critical challenges that impede their deployment. To tackle these challenges, we propose FG-CIBGC, the first unified framework for fine-grained and class-incremental behavior graph classification.
* We design two novel components (i.e. gPartition and gAdapt) for FG-CIBGC, thereby realizing the fine-grained behavior graph identification and incremental model update with novel class detection simultaneously.
* We construct a new benchmark that satisfies both _completeness_ and _diversity_, featuring 3 log types, 4,992 graphs across 32 classes, as well as a new EIoU metric for fine-grained evaluation. Extensive experiments demonstrate the superiority of FG-CIBGC.

## 2. Related Work

**Behavior Graph Classification.** Existing BGC methods can be divided into three categories: **(i) pattern-based methods**(Kumar et al., 2017; Wang et al., 2018) mine graph patterns from behaviors of interest and use them as templates to identify similar behaviors; **(ii) rule-based methods**(Kumar et al., 2017; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018) match audit events against a knowledge store of rules that describe behaviors; **(iii) learning-based methods**(Kumar et al., 2017; Wang et al., 2018) utilize machine learning models to represent behavior graphs as vectors, enabling identifying of semantically similar behaviors. Compared with prior work, we pioneer the exploration of class-incremental BGC task, thereby demonstrating greater competence in real-world scenarios. Furthermore, we are the first to produce fine-grained behavior graphs matching operations in services.

**LLMs-based Log Processing.** In recent years, with the increase in model sizes and richer training corpora, LLMs have notably grown in power. Given the vast pretraining datasets that encompass logging-related data, LLMs possess immense potential for log processing tasks. Existing research has explored the application of large language models across a wide range of log-related tasks, including **log parsing**(Kumar et al., 2017; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018), **vulnerability detection**(Wang et al., 2018; Wang et al., 2018) and **anomaly detection**(Wang et al., 2018; Wang et al., 2018; Wang et al., 2018). Compared with prior works, we uniquely explore the capabilities of large language models in correlating multi-source log data. Furthermore, we have designed a novel type-position-aware prompt format to enable more effective in-context inference of log correlation using the LLMs.

**Class-Incremental Graph Learning.** Recently, class-incremental graph learning has garnered growing attention owing to its broad applications (Chen et al., 2018; Wang et al., 2018), with existing works in this domain generally falling into three primary categories: **regularization-based**(Kumar et al., 2017), **architecture-based**(Kumar et al., 2017), and **replay-based**(Kumar et al., 2017; Wang et al., 2018) methods. Crucially, existing methods have largely assumed that all data comes from a predefined set of classes known to humans. However, this assumption does not hold for behavior graph classification task in real-world scenarios, where previously unknown classes may emerge during the learning process. Compared with prior works, we are the first to bridge the gap between Out-of-Distribution (OOD) detection methods and class-incremental graph learning, thereby handling _class increments_ in real-world scenarios.

## 3. Problem Definition

Our goal is to incrementally identify and classify semantically similar behavior graphs within a stream of multi-source logs. Given that fine-grained and class-incremental behavior graph classification involves two critical challenges, we provide precise definitions for each of these respective aspects.

**Fine-Grained Emerging Behavior Graphs.** Given a prior dataset \(\mathcal{D}_{tra}\) comprising multi-source logs (e.g., audit, application, and network logs), we first extract multiple fine-grained behavior graphs \(\mathcal{G}_{tra}\) from the dataset. The term _fine-grained behavior graph_ denotes a representation where each behavior graph precisely specifies an operation of a service. Each behavior graph \(G_{i}\in\mathcal{D}_{tra}\) corresponds a category label \(y_{i}\in\mathcal{Y}_{tra}\), where \(\mathcal{Y}_{tra}=\{y_{tra}^{1},y_{tra}^{2},\cdots,y_{tra}^{n}\}\) where \(n\) refers to the number of known classes. And we use \(\mathcal{D}_{tra}\) as the training set to fit the model \(\mathcal{M}\). When deploying the model \(\mathcal{M}\) in practice, it will encounter the open-world test set \(\mathcal{D}_{t}\) at stage \(t\), which includes: (i) samples whose ground-truth labels are present in the training set \(\mathcal{D}_{tra}\); and (ii) instances of emerging unknown classes \(\{y_{1}^{1},y_{t}^{2},\cdots,y_{tra}^{m}\}\), where \(m\) denotes the number of unknown classes, which is unknown to us a priori.

**Incremental Model Adaptations.** The _class increments_ represent an inherent challenge in class-incremental learning, and it distinguishes our approach from existing techniques. We assume there is no prior knowledge when suffering unprecedented behavior graphs in practice. When a novel class of behavior graphs emerges, we aim to proactively detect it and seamlessly incorporate it into the knowledge base, thereby strengthening the model's classification performance. Crucially, the updated model should be able to accurately identify previously unseen classes of behavior graphs in the future. This is a formidable challenge that eludes the capabilities of existing approaches.

## 4. Proposed Method

In this section, we present the proposed Fine-Grained and Class-Incremental Behavior Graph Classification (**FG-CIBGC**).

### Overview

As shown in Fig. 2, the proposed FGCIBGC consists of two novel modules: gPartition and gAdapt. **First**, gPartition is proposed to learn the correlation of multi-source logs in a prompt and finish the log correlation process. gPartition builds on the in-context learning paradigm with the large language models (LIMs). The derived correlation results, referred to as _behavior units_, are then used to partition the audit logs into fine-grained behavior graphs, which are subsequently reduced to achieve greater compactness. **Second**, gAdapt is proposed to proactively detect both known and unknown classes, classify them accurately, and correspondingly update the model in an incremental fashion. This is especially achieved through the synergistic combination of Out-Of-Distribution (OOD) detection and class-incremental graph learning.

### Partition: Fine-Grained Graph Partition

**Challenges Analysis.** Audit logs alone contain only low-level information about system activities, lacking the necessary knowledge to partition them into fine-grained operations. To accurately capture operations, the research communities observe that introducing logs with high-level semantics offers a more promising solution (Shen et al., 2017; Wang et al., 2018). Specifically, application logs of services are designed to record each

Figure 2. The main framework of the proposed FG-CIBGC.

operation and its attributes, while network logs explicitly track the corresponding network sessions incurred by each operation.

A vanilla method would be to leverage existing log correlation approaches to correlate these diverse log sources, allowing audit logs belonging to the same operations to be used in generating fine-grained behavior graphs. However, prior log correlation techniques rely on static correlation rules and overlook the potential for different logs to describe the same operation. This renders the use of logs with high-level semantics to partition audit logs into fine-grained behavior graphs a _fundamentally_ new task.

**Rationale Behind gPartition.** To address this challenge of multi-source log correlation, we resort to the In-Context Learning (ICL) paradigm of large language models (LLMs), as LLMs have demonstrated superiority in log processing tasks. ICL with LLMs not only enables mining of the latent semantic correlations across multi-source logs, but also bolsters the advantages of rapid deployment and easy interactivity without large tuning costs. Additionally, we propose a novel type-position-aware prompt template specifically designed for log correlation tasks, coupled with a warmup mechanism that enhances the ICL capabilities of LLMs.

**Model Backbone.** The performance of the large language model is a key factor in the success of ICL. Considering that log messages are semi-structured sentences that are mainly composed of natural language descriptions (i.e., log template) [(53)], we chose GPT-3.5 [(5)], an LLM that is pre-trained on an extremely large amount of semantic information from the open-source corpus, as the backbone for gPartition. Recent large language models, including GPT-3.5, have demonstrated in-context learning (ICL) capabilities, motivating its use as the backbone for gPartition. As gPartition utilizes the LLM in a black-box manner, the backbone model can be replaced as long as the relevant API is accessible.

**Prompt Strategy.** Prompt strategy is the most significant part of ICL. To preprocess the influx of multi-source logs before prompting, we first parse the logs into standardized forms. After that, we merge these parsed logs in chronological order and segment them into batches of 400 entries. This approach considers that logs representing the same operation are typically temporally close, and a service's related operations do not correlate with an excessive number of logs.

Before designing the prompt template, an essential question arises: _How should we model log correlation task to facilitate understanding by LLMs?_ To enable LLMs to perform the _"which-correlates-with-which"_ task while adhering to the _"one query, one inference"_ principle, we explicitly represent log types and position information, transforming the problem into a compact format. Specifically, we have observed that position closeness is a crucial factor in log correlation, and different types of logs contain various fields that serve distinct roles in the correlating process. To address this, we insert two special tokens, namely <type> and <lineF>, at each newline to indicate the log type and line ID, respectively, and subsequently flatten the log sequences into a linear format. Additionally, the instruction necessitates that the model provide the <type>|<lineF> pairs to form a _behavior unit_, where each _behavior unit_ represents the complete set of multi-source logs generated by a single service operation. This type-position-aware prompt format ensures both efficiency and accuracy in inference, as illustrated in Fig. 3.

It is important to note that the selection of examples in the prompt significantly impacts the downstream task performance of LLMs under the ICL paradigm. In this study, we utilize KATE [(24)] for in-context example augmented selection. Due to page limit, the details of the selection algorithm are provided in the Appendix.

**Warmup Strategy.** Given that the model's ICL abilities can be enhanced through a warmup phase prior to ICL inference, we employ the following warmup process: gPartition first randomly selects 800 samples from the validation set to serve as prompt queries for the warmup. For each query, gPartition employs the aforementioned selection algorithm to identify the eight most similar samples in the training set as prompt examples, appending their ground-truth labels. These are then combined with fixed instructions to form a complete prompt. Subsequently, all prompts will be submitted to GPT-3.5 for parameter tuning in batches.

**Behavior Graph Generation.** Prior works showed the conversion of unstructured logs into a unified graph. Having obtained the behavior units, we implement an audit log parser to transform the audit logs within the same behavior units into a behavior graph, where each edge represents an audit log. This allows the audit logs to be partitioned into multiple small-scale behavior graphs. _However, not all audit logs belonging to the same operation can be incorporated into a behavior unit simply through log correlation._ For instance, system configuration activities of an operation may leave no traces in high-level semantics. To tackle this issue, we design a novel heuristic search algorithm to capture operations comprehensively. The overall workflow is shown in Alg 2.

**Graph Reduction.** Background noisy events resulting from the inherent low-level nature of auditing mechanism are massive, and they do not impact BGC task according to prior works [(10; 17)]. To reduce noisy events, we employ a few existing graph reduction algorithms including LogGC [(20)], CPR [(43)] and NodeMerge [(38)].

Figure 3. The prompt templates omit insignificant log details for simplicity. Each prompt contains several labeled examples and one query. The last example in the prompt is the most similar to the query, whereas the first example is the least similar.

### gAdapt: Incremental Model Adaptation

**Challenges Analysis.** A vanilla design would be to directly apply an existing class-incremental graph learning method to implement gAdapt. However, when deploying existing proposals in real-world scenarios, the research community faces _class increments_ challenge that renders a straightforward design impractical. This is because the unknown class labels are inherently unpredictable, as the set of system operations is continuously expanding. Yet, the majority of existing methods tend to overlook this _open-world_ problem.

In order to detect unknown classes, Out-Of-Distribution (OOD) Detection is showing promising potential recently. Therefore, a straightforward method is to use existing graph OOD detection method in an unsupervised way. However, they only finish _partial_ of task requirements, that is, they only tell if a given behavior graph is OOD, but can not assign label to them so making it incapable of attaching new classes to the model's knowledge base. Besides, they do not consider the challenge of determining a threshold to decide if a datapoint is OOD.

**Rationale Behind gAdapt.** To address the aforementioned challenges, we implement gAdapt based on the idea that bridging the gap between OOD detection and class-incremental learning. We adopt a disentangled graph encoder as the model backbone, as graph formation typically follows a complex relational process, and such disentangled representation learning has shown promising results. Additionally, we employ replay-based methods for model updates. Upon receiving new samples, an OOD Detector generates OOD scores to identify _class increments_. The OOD samples are then clustered to obtain new-class labels, and used to update the model, alongside the structure incremental samples.

**Features Extraction.** We propose a novel strategy to extract node features for behavior graphs. In our analysis, we identified three distinct types of nodes within each behavior graph: processes, files, and sockets. Given the heterogeneous nature of these nodes, their respective feature vectors comprise different elements. For process nodes, we utilize [process_name, p_id, exe_name] as the feature set. File nodes are characterized by [file_name, inode, file_type], while socket nodes are represented by [ip, port, socket_type]. To encode all textual components within these feature sets, we employ FastText (Chen et al., 2017), a library for efficient learning of text representations.

**Outlier Detection.** We choose GOOD-D (Zhu et al., 2018) as basic OOD Detector for its ability to detecting OOD graphs without using ground-truth labels. It is worth noting that OOD samples are often noisy. In real-world deployment, manual labeling is often required to enable custom configuration and adapt the model to new OOD samples (Hardt et al., 2016). However, this incurs nontrivial annotation cost.

With Weakly-supervised Relevance Feedback, we propose a method to overcome the challenge of custom configuration and human labeling. We introduce the hyperparameter \(q\), which is a domain-interpretable value of the expected ratio of new OOD behavior graphs in the next time step. Specifically, we first apply the OOD detector to all inputs in the dataset \(\mathcal{D}_{l}\) and compute their OOD scores \(S(G_{1}),\ldots,S(G_{n})\). These scores are sorted in ascending order, resulting in a permutation \(\pi\), \(S(G_{\pi(1)}),\ldots,S(G_{\pi(n)})\). Subsequently, we allow analysts to assign pseudo labels to the datapoints by selecting a domain-specific value for the hyperparameter \(q\), e.g. 0.05. The analysts label the top \(q\) percent of the datapoints as OOD and the lower \(1-q\) percent as ID, receiving a labeled dataset for OOD detection feedback, i.e. \(G_{\pi(1)}\ldots G_{\pi(n)}\) is labeled with \(0_{\pi(1)},\ldots,0_{\pi(\lfloor\pi(1-q)\rfloor)},1_{\pi(\lceil\lfloor\pi(1-q) \rfloor)},-1_{\pi(n)}\). With this labeled dataset, we fine-tune the OOD detector. The introduced hyperparameter \(q\) represents the ratio of OOD scores in the domain, which is not only an interpretable number but can also be determined with the help of prior knowledge without extensive tuning.

**Class Annotation.** For new-class samples, we utilize a K-Means (Zhu et al., 2018) based clustering algorithm for class annotation due to its efficiency. Given the uncertainty in the number of unknown classes, naive K-Means, which requires a predefined cluster count, is not applicable. This prompts us to consider whether reference values exist for determining \(K\). Fortunately, we observe that the lower bound of \(K\) is defined by the distinct types of application logs, as limitations in logging tools and attack complexity prevent the capture of all execution details, leading similar logs to potentially reflect different behaviors. Thus, the number of different application log type can serve as the reference value \(R_{k}\). Our goal is to find the optimal \(R_{k}\) close to the reference value, evaluating the effectiveness using the Silhouette Score. The process is demonstrated in Alg 1.

```
Input: Behavior graphs \(G_{i}\) and behavior units \(U_{i}\). Output: Behavior Clusters \(C_{1},C_{2},C_{3}...C_{n_{c}}\).
1\(T\leftarrow\emptyset,s\leftarrow\emptyset\);
2for\(l_{app,i}\gets U_{i}\)do
3\(\Gamma_{l_{app}}\leftarrow\) Log templates of application logs in \(U_{i}\);
4\(n_{ref}\leftarrow\) Get_Reference_Value(\(T\));
5for\(i\gets n_{ref}\) to \(3n_{ref}\)do
6\(\Delta_{n_{ref}}\leftarrow\) Get_Silhouette_Score(\(G_{i},n_{ref}\))
7\(n_{c}\leftarrow\) choose_Optimal(\(s\))
8\(C_{1},C_{2},C_{3}...C_{n_{c}}\leftarrow\) Minibatch_Kmeans(\(G_{i},n_{c}\));
9 Return \(C_{1},C_{2},C_{3}...C_{n_{c}}\).
```

**Algorithm 1**Kmeans with parameter selection

**Model Update.** We choose DisenGCN (Miyato et al., 2017) as the backbone graph encoder considering its effectiveness and efficiency. Besides, to enhance model adaptability to structural and class increments, we adopt a replay-based incremental learning strategy utilizing class prototypes. Specifically, after stage \(t-1\), the old model has learned the optimal parameters \(\Theta_{old}\). By feeding \(\mathcal{D}_{t-1}\) into \(\mathcal{M}^{old}\), we obtain old class embeddings. To overcome catastrophic forgetting, we construct class prototypes \(\mathcal{N}(\mu_{i},\sigma_{i})\) to approximate \(\mathcal{D}_{t-1}\), where \(\sigma_{i}\) is the diagonal covariance. This reduces memory cost while preserving key information, as disentangled embeddings have most variance along the diagonal. For robustness, we use only correctly predicted samples to estimate \(\mu_{i},\sigma_{i}\).

In addition, since we only save the prototypes of the old data rather than the raw data, the old saved prototypes may not be available when the backbone is training on new data, i.e., the saved prototypes cannot represent the current positions of the old data in the embedding space. Therefore, when training the backbone \(\mathcal{M}\) at stage \(t\), we need to limit the shift of the old prototypes in the embedding space to ensure their availability. Thus, we add a loss to distill the knowledge of the old backbone \(\mathcal{M}_{old}\).

\[\mathcal{L}_{kd}:=\mathbb{E}_{(\mathcal{G},\mathcal{Y})\sim\mathcal{D}_{t}}\left[ \left|\mathcal{M}_{\Theta std}(\mathcal{G})-\mathcal{M}_{\Theta}(\mathcal{G}) \right|\right]\right]. \tag{1}\]

Besides, we use Prototype Augmentation (PA) [34] strategy to enhance the incremental learning backbone. Let \(P_{t}\) denote the class prototypes before stage \(t\) and \(f_{PA}\) be the classifier after adding the classification heads of virtual classes. The loss function over old data is calculated by:

\[\mathcal{L}_{old}:=\mathbb{E}_{(\mathcal{G},\mathcal{Y})\sim\mathcal{D}_{t}} \left[\mathcal{L}(f_{PA}(\mathcal{P},\mathcal{Y}))\right]. \tag{2}\]

In addition, we use the following equation to calculate the classification loss on the new data:

\[\mathcal{L}_{cls,PA}:=\mathbb{E}_{(\mathcal{G},\mathcal{Y})\sim\mathcal{D}_{t}} \left[\mathcal{L}(f_{PA}(\left[\mathcal{M}_{\Theta}(\mathcal{G})\right]_{PA}, \mathcal{Y}))\right], \tag{3}\]

where \(\left|\mathcal{M}_{\Theta}(\mathcal{G})\right|\)\(p_{A}\) represents the embeddings obtained by using \(\mathcal{M}_{\Theta}(\mathcal{G})\) after the Prototype Augmentation step. Finally, we have the total loss function as follows:

\[\mathcal{L}=\mathcal{L}_{cls,PA}+\alpha*\mathcal{L}_{old}+\beta*\mathcal{L}_{ kd}, \tag{4}\]

where \(\alpha\), \(\beta\) are used to balance \(\mathcal{L}_{cls,PA}\), \(\mathcal{L}_{old}\) and \(\mathcal{L}_{kd}\).

## 5. Experiment Setup

In this section, we introduce the experimental setup, including the datasets, baselines, evaluation metrics and implementation details.

### Datasets

In order to support the thorough evaluation of FG-CIBGC, the dataset should have the following properties:

* **Completeness of Log Sources.** The dataset should offer complete log sources including application logs and network logs. Without them, behavior graph identification relies either on a static knowledge base or search within a pre-obtained graph, both of which are impossible in a class-incremental setting.
* **Diversity of Behavior Types.** The dataset should contain a diverse range of system behaviors. Limited types of behavior graphs restrict thorough evaluation.

Open-source datasets like DARPA Trace (Chen et al., 2017) and StreamSpot (Kumar et al., 2018) lack application and network logs _completely_. Moreover, they also fail to cover diverse operations in Internet Infrastructure. Given the above limitations, we construct a new behavior dataset that satisfies both properties, featuring 3 log types and 4,406 graphs across 31 classes. The statistics of the dataset is shown in Table 1. Due to page limit, more details are presented in the Appendix.

### Baselines

To facilitate a comprehensive evaluation, we compare the proposed FG-CIBGC framework with existing methods from two key perspectives: (**i) Behavior Graph Classification Performance. (**ii) Efficacy in Attack Investigation.**

**(i) Behavior Graph Classification Performance.** In terms of behavior graph classification task, we use 12 existing methods as baselines, covering state-of-the-art methods in the behavior graph classification landscape. (**i) Three behavior graph classification methods: **Tgeminer**(Wang et al., 2017), **Watson**(Wang et al., 2017) and **DepComm**(Wang et al., 2017)(ii) Six class-incremental incremental learning methods: **EWC**(Liu et al., 2019), **LwF**(Wang et al., 2019), **GEM**(Wang et al., 2019), **TWP**(Kumar et al., 2018), **CPCA**(Wang et al., 2017) and **Fine-Tuning**(Chen et al., 2017). (**iii) Three graph-level dynamic graph learning methods: **tdGraphEmbed**(Chen et al., 2017), **GraphERT**(Chen et al., 2017) and **TP-GNN**(Kumar et al., 2018).

**(ii) Efficacy in Attack Investigation.** The fine-grained behavior graph classification task is inherently designed to facilitate downstream applications. Thus, we analyze whether the generated behavior graphs can benefit downstream tasks. Specifically, we select attack investigation as the representative downstream task, given its practical significance. In brief, the attack investigation task aims to identify attack-related edges within a given behavior graph. We use **Watson**(Wang et al., 2017) and **DepComm**(Wang et al., 2017) which generate coarse-grained behavior graphs as baselines. Besides, we employ **DepImpact**(Chen et al., 2017) as the baseline for attack investigation.

### Evaluation Metrics

Metrics such as F1-Score (F1) have been adopted in prior studies to evaluate the behavior graph classification task. Following the convention, we use F1 as evaluation metric to conduct the experiments on behavior graph classification. However, none of these metrics take into account the fine-grained requirements of behavior graph classification. Considering such fine-grained characteristics could enable a fair comparison between coarse-grained and fine-grained behavior graph classification methods. In this regard, borrowing the idea from the MIoU (Mean Intersection over Union) metric in semantic segmentation, we propose a new metric called EIoU (Edge Intersection over Union). The EIoU metric enables a comprehensive evaluation of different methods by capturing fine-grained requirements. Specifically, EIoU reframes the graph classification problem as an edge-level classification task, where the classification of a graph into a specific category corresponds to the assignment of its edges to that category. By applying matching criteria to the

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline
**Scenarios** & **Attack Cases** & **\#Graph** & **\#Class** & **Avg of Node** & **Avg of Edge** & **Avg of Node** & **Avg of Edge** & **\#Audit** & **\#App** & **\#Net** \\ \hline Apache & Data Leakage & 502 & 3 & 10.01 & 46.02 & 5.10 & 16.99 & 11.7MB & 43KB & 216KB \\ IM-1 & Data Leakage & 1,040 & 4 & 88.12 & 401.69 & 28.12 & 102.54 & 1.25GB & 52.1MB & 6.93GB \\ \hline Vim & Unsafe Action & 125 & 3 & 28.6 & 1,256.00 & 14.6 & 52.32 & 432MB & 275KB & - \\ Redis & Unsafe Action & 201 & 2 & 14.00 & 451.19 & 8.23 & 66.54 & 63.78MB & 17KB & 987KB \\ Pgsql & Unsafe Action & 512 & 9 & 30.27 & 145.20 & 17.75 & 54.30 & 48.5MB & 360KB & 65.2MB \\ ProFTPd & Unsafe Action & 1,001 & 3 & 11.13 & 179.90 & 8.34 & 29.01 & 112.2MB & 95KB & 2.5MB \\ IM-2 & Unsafe Action & 1,040 & 4 & 65.68 & 1,360.25 & 35.68 & 114.96 & 796.8MB & 8.9MB & 3.94GB \\ \hline Nginx & Misconfiguration & 1,001 & 4 & 5.14 & 17.93 & 3.00 & 10.51 & 9.8MB & 105KB & 133KB \\ \hline \hline \end{tabular}
\end{table}
Table 1. Overview of dataset for FG-CIBGC evaluation. We implement 8 attack scenarios based on their detailed reports of real-world APT campaigns (Wang et al., 2017). This dataset consists of 4,992 graphs which can be categorized into 32 classes.

ground truth at each fine-grained category, we construct a confusion matrix delineating True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN). With these definitions in place, Intersection over Union (IoU) can be formulated by \(\text{IoU}=\frac{|TP_{e}|}{|FP_{e}|+|FP_{e}|+|FN_{e}|}\), where \(|TP_{e}|\), \(|FP_{e}|\), and \(|FP_{e}|\) respectively stand for the number of edge-level TP, FP, and FN. For the EIoU metric used in behavior graph classification, the matching criterion is determined by the accurate edge-level prediction corresponding to the ground truth fine-grained labels. The EIoU is calculated as:

\[\text{EIoU}=\sum_{i=0}^{k}\text{IoU}_{i} \tag{5}\]

where IoU represents the IoU of fine-grained class \(i\) and \(k+1\) is the total number of fine-grained classes in the evaluated dataset.

To quantify the attack investigation performance, we treat the task as an edge-level binary classification problem, as attack investigation inherently aims to identify attack-related edges. Consequently, we compute the Accuracy (Acc) and F1-Score (F1) to evaluate the attack investigation tasks.

### Implementation Details

We prototype FG-CIBGC in 42K lines of Python code. The proposed model is implemented by PyTorch 2.1 framework on Ubuntu 22.04, and all the evaluations are conducted on NVIDIA GeForce RTX 3090 card. For a fair comparison, we tune the hyper-parameters of the base Class-Incremental learning model using grid-search: learning rate \(lr\in\{0.005,0.001,0.01\}\), batch size \(b\in\{512,1024,2048\}\), embedding dimension \(d\in\{32,64,128,256\}\). We set \(C_{k}:C_{u}\) (representing the number of known and unknown classes) \(=9:1\).

## 6. Results and Analysis

In this section, we conduct experiments regarding behavior graph classification performance, ablation study, efficacy in attack investigation and hyper-parameter sensitivity to validate the proposed FG-CIBGC. Due to the page limit, we have to move additional results, including but not limited to more ablation study results and the associated time analysis to the Appendix.

### Behavior Graph Classification Performance

In this section, we compare the behavior graph classification performance of FG-CIBGC with the constructed baselines. The results are shown in Table 2. By observing the experimental results, we can have the following observations:

(1) Coarse-grained behavior graph-based baselines generally exhibit relatively low EIoU performance. Watson performs an adapted DFS on every single data object found in the KG, except for libraries that do not reflect the roots of user-intended goals. DepComm identifies process-centric communities. They all fail to find operations centered around data/process objects, leading to undesirable performance. Among all coarse-grained behavior graph-based methods, Tgminer emerges as the top performer in terms of the EIoU metric. This can be attributed to its strategic focus on finding frequent patterns, rather than centering around data or processes, which sets it apart from other methods. In contrast, FG-CIBGC excels by leveraging more comprehensive information from diverse sources, resulting in accuracy in identifying behavior boundaries.

(2) FG-CIBGC demonstrates significant superiority over class-incremental graph learning baselines. These baselines struggle to accommodate scenarios with unknown new classes, and thus fail to adapt effectively. Furthermore, graph-level dynamic graph learning baselines lag behind class-incremental learning methods, as they lack the ability to adapt to known new classes.

(3) FG-CIBGC outperforms baselines across all datasets, achieving an average improvement of 4.89% in EIoU and 6.82% in F1-Score

\begin{table}
\begin{tabular}{c|c c|c c|c c|c c|c c|c c|c c|c c} \hline \multirow{2}{*}{Method} & \multicolumn{3}{c|}{Apache} & \multicolumn{3}{c|}{IM-1} & \multicolumn{3}{c|}{IM-2} & \multicolumn{3}{c|}{Vim} & \multicolumn{3}{c|}{Redis} & \multicolumn{3}{c|}{Pgsql} & \multicolumn{3}{c}{ProFIPd} & \multicolumn{3}{c}{Nginx} \\  & EIoU & F1 & EIoU & F1 & EIoU & F1 & EIoU & F1 & EIoU & F1 & EIoU & F1 & EIoU & F1 & EIoU & F1 & EIoU & F1 & EIoU & F1 \\ \hline Tgminer & 48.82 & 65.15 & 59.64 & 66.33 & 52.63 & 67.67 & 58.72 & 60.13 & 64.21 & 79.06 & 51.72 & 66.43 & 59.44 & 59.75 & 53.23 & 65.91 & 78.1 \\ Watson & 40.24 & 69.23 & 42.62 & 56.34 & 39.67 & 66.30 & 47.09 & 56.79 & 62.35 & 82.65 & 48.27 & 61.16 & 41.22 & 52.25 & 44.74 & 54.86 & 76.22 \\ DepComm & 41.09 & 70.54 & 51.39 & 75.99 & 43.61 & 72.51 & 54.80 & 72.84 & 60.71 & 80.80 & 52.75 & 70.45 & 57.70 & 67.91 & 57.56 & 76.46 \\ \hline Fine-Tuning(+) & 50.84 & 71.08 & 52.63 & 76.35 & 55.93 & 74.94 & 56.05 & 73.75 & 64.23 & 78.97 & 56.24 & 73.27 & 54.38 & 68.21 & 57.41 & 78.92 & 74.38 \\ EWC(+) & 55.81 & 74.95 & 54.60 & 78.02 & 61.90 & 80.74 & 64.10 & 82.42 & 69.20 & 85.15 & 68.49 & 87.32 & 67.43 & 84.34 & 63.62 & 80.84 \\ LwF(+) & 62.32 & 83.22 & 58.46 & 77.83 & 59.75 & 76.85 & 62.80 & 76.10 & 64.21 & 85.84 & 57.29 & 81.61 & 66.46 & 87.26 & 63.54 & 82.23 & 76.48 \\ GEM(+) & 58.74 & 76.83 & 58.98 & 74.98 & 59.86 & 76.93 & 68.20 & 82.15 & 64.53 & 84.43 & 58.67 & 81.26 & 57.72 & 81.03 & 64.27 & 83.53 & 76.19 \\ TWP(+) & 69.42 & 86.88 & 65.64 & 85.55 & 67.04 & 66.57 & 72.08 & 84.43 & 70.12 & 85.43 & 50.74 & 76.95 & 70.16 & 84.36 & 61.64 & 85.89 \\ CPCA(+) & 67.08 & 86.42 & 59.94 & 81.91 & 67.63 & 78.84 & 71.27 & 83.26 & 70.29 & 78.24 & 67.93 & 85.54 & 71.09 & 87.82 & 71.60 & 87.60 \\ \hline tddGraphEmbed(+) & 56.43 & 75.12 & 46.43 & 60.78 & 48.25 & 67.37 & 56.26 & 73.81 & 59.52 & 79.03 & 53.68 & 72.62 & 51.48 & 67.63 & 46.48 & 61.54 & 77.71 \\ GraphERT(+) & 67.62 & 77.71 & 52.36 & 68.44 & 58.69 & 78.23 & 65.71 & 75.32 & 64.52 & 78.39 & 64.15 & 79.42 & 62.08 & 77.89 & 55.37 & 75.32 & 77.11 \\ TP-GNN(+) & 62.32 & 78.56 & 55.32 & 71.84 & 57.36 & 77.93 & 64.08 & 74.26 & 63.12 & 76.21 & 59.34 & 73.45 & 53.48 & 72.09 & 60.02 & 79.84 & 77.22 \\ \hline
**Ours** & **74.62** & **91.62** & **70.35** & **90.26** & **73.93** & **93.92** & **76.08** & **96.07** & **78.24** & **98.32** & **74.56** & **93.08** & **74.28** & **92.73** & **73.31** & **93.27** & 77.33 \\ \hline \end{tabular}
\end{table}
Table 2. Comparison results (EIoU & F and F1 %) of fine-grained class-incremental behavior graph classification task across all datasets. ”(+)” indicates that the input to this baseline is the fine-grained behavior graphs generated by gPartition. The best results are shown in bold type and the runner-ups are underlined.

\begin{table}
\begin{tabular}{c||c c|c c|c c} \hline \multirow{2}{*}{Method} & \multicolumn{3}{c|}{Apache} & \multicolumn{3}{c|}{IM-1} & \multicolumn{3}{c}{IM-2} \\  & EIoU & F1 & EIoU & F1 & EIoU & F1 & EIoU & F1 \\ \hline Baseline & 43.95 & 71.93 & 54.36 & 73.21 & 48.56 & 77.21 \\ Baseline-T & 50.95 & 77.93 & 55.36 & 76.21 & 65.56 & 82.21 \\ Baseline-C & 67.08 & 86.42 & 59.44 & 81.91 & 67.63 & 87.84 \\ \hline
**FG-CIBGC** & **74.62** & **91.62** & **70.35** & **90.26** & **73.93** & **93.92** \\ \hline \end{tabular}
\end{table}
Table 3. Ablation study results. The best results are shown in bold type and the runner-ups are underlined.

compared to the baselines. The superiority is largely attributed to the combination of the innovative gPartition and gAdapt components. Therefore, FG-CIBGC excels in fine-grained and class-incremental behavior graph classification task.

### Ablation Study

The proposed FG-CIBGC framework contains two major components. We conduct an ablation study on 3 representative datasets to further verify their effectiveness. Specifically, 4 combinations of key modules are compared in the ablation study as follows:

* **Baseline:** For this variant, we employ Tginner to identify behavior graphs, and leverage CPCA to perform incremental graph classification.
* **Baseline-T:** For this variant, we substitute the gPartition component of FG-CIBGC with the behavior graph generation algorithm of Tginner.
* **Baseline-C:** For this variant, we replace the gAdapt component of FG-CIBGC with the class-incremental learning baseline CPCA.
* **FG-CIBGC:** This variant is the proposed FG-CIBGC model. As shown in Table 3, we can draw the following conclusions:

(1) The Baseline' performs the worst due to its inability to generate behavior graphs matching service operations. Furthermore, it lacks an efficient strategy to detect known classes.

(2) Incorporating gPartition or gAdapt into the 'Baseline' markedly enhances its performance, highlighting the necessity of generating fine-grained behavior graphs and combining OOD detection with class-incremental learning.

(3) The two proposed modules achieve stable and effective performance on different datasets. FG-CIBGC leverages the advantages of its modules to achieve significant performance gains.

### Efficacy In Attack Investigation

In this section, we seek to ascertain whether the proposed FG-CIBGC can indeed generate fine-grained behavior graph classification results that are effective in facilitating the downstream attack investigation task. We utilize **Depl Impact** to identify critical components in a unified graph derived from raw audit logs. FG-CIBGC and baselines partition the unified graph and classify the resulting behavior subgraphs, which guide forward and backward causality analysis. As illustrated in Fig. 4, it is evident that FG-CIBGC demonstrates the best performance in fine-grained behavior graph generation, thereby optimally facilitating attack investigation.

### Hyper-Parameter Sensitivity

In this section, we perform hyper-parameter sensitivity analysis on 3 representative datasets to investigate the impact of \(\alpha\) and \(\beta\) on FG-CIBGC by conducting a grid search for their optimal values. Initially, we set \(\beta\) = 0.3 and vary \(\alpha\), followed by fixing \(\alpha\) = 1 while varying \(\beta\). The experimental results are illustrated in the Fig. 5. Overall, FG-CIBGC maintains solid performance with different parameter settings. The optimal performance is observed when \(\alpha\) = 3 and \(\beta\) = 0.1, indicating its strongest capability under this setting.

## 7. Conclusion

This paper presents FG-CIBGC, the first unified framework for fine-grained and class- incremental behavior graph classification. FG-CIBGC comprises two novel modules: gPartition for fine-grained graph partitioning, and gAdapt for unknown class detection and adaptation. To validate its efficacy, we introduce a novel benchmark. This benchmark includes a new dataset of 4,992 graphs across 32 classes, derived from 8 attack scenarios. It also features a novel Edge Intersection over Union (EIoU) evaluation metric. Extensive experiments demonstrate FG-CIBGC's superior performance on fine-grained and class-incremental behavior graph classification tasks. Furthermore, FG-CIBGC has the ability to generate fine-grained behavior graphs that facilitate downstream applications.

Figure 4. Experimental results regarding effect on attack investigation task.

Figure 5. Hyper-parameter sensitivity analysis results for different values of hyper-parameters \(\alpha\) and \(\beta\) on Apache, IM-1 and IM-2 datasets.

## References

* (1)
* Belaidey et al. (2023) Moran Belaidey, Gilad Katz, Lior Rokach, Ulid Singer, and Kira Radinsky. 2023. GraphMET: Transformer-based Temporal Dynamic Graph Embedding. In _Proceedings of the 23rd ACM International Conference on Information and Knowledge Management_. 68-77.
* Belaidey et al. (2020) Moran Belaidey, Lior Rokach, Gilad Katz, Lio Guy, and Kira Radinsky. 2020. IdigraphenNet: Temporal dynamic graph-level embedding. In _Proceedings of the 29th ACM International Conference on Information & Knowledge Management_. 55-64.
* Belaidey and Popescu (2019) Edela Belaidey and Adrian Popescu. 2019. 12nm: Class incremental learning with dual memory. In _Proceedings of the IEEE/CVF international conference on computer vision_. 583-592.
* Bogianowski et al. (2017) Piotr Bogianowski, Eduard Grave, Armand Joulin, and Tomas Mikolov. 2017. Exriching word vectors with subword information. _Transactions of the association for computational linguistics_ 5 (2017), 135-146.
* Brown et al. (2021) Tom Brown, Benjamin Mint, Nicky Rudiger, Melanie Subbaiah, Jared D Kaplan, Prafulla Dhariwal, Arvell Neelakantan, Pranav Sharma, Girish Sastry, Amanda Askell, Sandhimi Agarwal, Arvell Hebert-Voss, Gerdene Krugner, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Szpete, Matthew Livan, Scott Gray, Benjamin Chess, Jack Clark, Christopher Bremer, Sam McCandlish, Lifke Torya Sutskever, and Emily Arnold. 2020. Language Models at Few-Shot Learners. In _Advances in Neural Information Processing Systems_. 33, 1877-1991.
* DARPA (2014) DARPA. 2014. Transparent Computing Engagement 3 Data Release. [https://github.com/darpa-20/Transparent-Computing/blob/master/README-EMD-2](https://github.com/darpa-20/Transparent-Computing/blob/master/README-EMD-2).
* Derman et al. (2021) Angel Emam, Mehdi Gupta, Mohan Sridhar, and Soni Cherniore. 2021. Continual learning of knowledge graph embeddings. _IEEE Robotics and Automation Letters 6_, 2 (2021), 1128-1155.
* Fang et al. (2022) Pengcheng Fang, Peng Gao, Changlin Lin, Erman Ayday, Kangkook Jee, Ting Wang, Yantang Fennyy, Ebo Zhao, and Nuebing Xiao. 2022. Back-Propagating System Dependency Imperfect for Attack Investigation. In _USENIX Security Symposium_. 2021-2618.
* Farula et al. (2020) Anna Farula, Vida Ghavrani, Masoud Makrechki, Shathyrar Rahnamyan, Sanna Alwidian, and Akramani Azim. 2020. Log Anomaly Detection by Leveraging LIM-Based Parsing and Embedding with Attention Mechanism. In _2020 IEEE Canadian Conference on Electrical and Computer Engineering_ (CCECE). IEEE, 859-863.
* Gao et al. (2016) Peng Gao, Xuheng Xiao, Ding Li, Zhibin Liu, Kangkook Jee, Zhengyu Wu, Chang Huan Sunper, Kulfaneri, and Patrick Mittal. 2016. SASQ: A stream-based query system for Real-Time abnormal system behavior detection. In _USENIX Security Symposium_. 693-664.
* Han et al. (2023) Dongqi Han, Zhiliang Wang, Wenjie Chen, Kai Wang, Rui Yu, Su Wang, Han Zhang, Zhilin Wang, Minghui Jin, Jiahui Yang, et al. 2023. Anomaly Detection in the Open World: Normality Shift Detection, Explanation, and Adaptation. In _Proceedings of the Network and Distributed System Security Symposium_.
* Hasan et al. (2020) Waih UI Hasan, Adam Bates, and Daniel Marino. 2020. Tactical provenance analysis for endpoint detection and response systems. In _IEEE Symposium on Security and Privacy_. 1172-1189.
* Hasan et al. (2019) Waih UI Hasan, Shengjin Gao, Ding Li, Zhengchang Chen, Kangkook Jee, Zhibin Liu, and Adam Bates. 2019. Nodoze: Computing threat alert fatigue with automated provenance triage. In _Proceedings of the Network and Distributed System Security Symposium_.
* Mnih et al. (2017) Md Nahid Hossein, Sadegh M Majzedi, Junuo Yang, Bijhanue Eshte, Rigel Gjemoon, R Sekar, Scott Sidler, and VV Venkatakrishnan. 2017. SLUTH: Real-time attack scenario reconstruction from COTS audit data. In _USENIX Security Symposium_. 487-504.
* Hosuani et al. (2020) Md Nahid Hosuani, Sanna Sheikh, and R Sekar. 2020. Combating dependence explosion in forensic analysis using alternative tag propagation semantics. In _IEEE Symposium on Security and Privacy_. 1139-1155.
* Imam et al. (2023) Muhammad Adil Imam, Yintang Chen, Akul Goyal, Jason Lin, Joran Mink, Noor Michael, Sneh Gur, Adam Bates, and Waih UI Hassan. 2023. 2023. Click: History is a vast early warning system: Auditing the provenance of system intrusions. In _IEEE Symposium on Security and Privacy_. 2620-2638.
* Hill et al. (2022) Muhammad Adil Imam, Akul Goyal, Jason Lin, Aaron Mink, Noor Michael, Sneh Gur, Adam Bates, and Wil U Hassan. 2022. FAST: Striking: A Bargain between Forensic Auditing's Security and Throughput. In _Proceedings of the Computer Security Applications Conference_. 813-826.
* Jiang et al. (2021) Zahlan Jiang, Jiyang Liu, Zhuang Chen, Yichen Li, Junjie Huang, Yintong Huo, Piatila Eshte, Jianfan Gu, and Michael R. Lyu. 2021. MLAC: Log parsing using LIMs with adaptive parsing cache. _Proceedings of the ACM on Software Engineering_. 1, 7851 (2021), 137-160.
* Kirkpatrick et al. (2017) James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Qun, Tiago Ramabh, Agresiades Grasbas-Barwinidis, et al. 2017. Overcoming catastrophic forgetting in neural networks. _Proceedings of the National Academy of Sciences_ 114, 13 (2017), 3521-3526.
* Lee et al. (2013) Kyu Hyung Lee, Xiangyu Zhang, and Dongyan Xu. 2013. Logge: garbage collecting audit log. In _Proceedings of the ACM SIGSAC Conference On Computer & Communications Security_. 1005-1016.
* Li and Hoiem (2017) Zhizhong Li and Derek Hoiem. 2017. Learning without forgetting. _IEEE Transactions on Pattern Analysis and Machine Intelligence_ 40, 12 (2017), 2935-2947.
* Liu et al. (2020) Huihui Liu, Yiding Yang, and Xiachuo Wang. 2020. Overcoming catastrophic forgetting in graph neural networks. In _Proceedings of the AAAI conference on artificial intelligence_, Vol. 35, 3653-3661.
* Liu et al. (2024) Jie Liu, Jianon Liu, Kaiqi Zhao, Yuani Tang, and Wu Chen. 2024. TP-CNN: Continuous Dynamic Graph Neural Network for Graph Classification. In _2024 IEEE 44th International Conference on Data Engineering (ICDE)_. IEEE, 2848-2861.
* Liu et al. (2021) Jiexiong Liu, Dinghan Shen, Yizhe Zhang, BilDi, Lenhwee Carlin, and Weizhn Chen. 2021. What Makes Good In-Context Examples for GP? _37th ArXiv preprint arXiv:2107.08040_ (2021).
* Liu et al. (2023) Yin Liu, Kaiqi Zhao, Jiwan Lin, and Shirui Pan. 2023. Good-d: On unsupervised graph out-of-distribution detection. In _Proceedings of the Network ACM International Conference on Web Search and Data Mining_. 339-347.
* Liu et al. (2024) Yifun Liu, Shimin Tao, Weibin Meng, Jiregy Wang, Wenbing Ma, Yuhang Chen, Yangqing Zhao, Hao Yang, and Varui Jiang. 2024. Interceptable online log analysis using large language models with prompt database. In _Proceedings of the 22Nd IEEE/ACM International Conference on Program Comprehension_. 35-46.
* Lopez-Paz and Rinano (2017) David Lopez-Paz and Marc'Aurelio Ramos. 2017. Gradient episodic memory for continual learning. _Advances in Neural Information Processing Systems_ 39 (2017).
* Ma et al. (2019) Jianxin Ma, Peng Cui, Kun Kuang, Xin Wang, and Wenyan Zhu. 2019. Disent-length graph convolutional networks. In _International conference on machine learning_. PMLR, 4212-4221.
* Monrooz et al. (2016) Emad Monrooz, Sadegh M Majzedi, and Lennan Akoglu. 2016. Fast memory-efficient anomaly detection in streaming heterogeneous graphs. In _Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_. 1035-1044.
* Milgireath et al. (2019) Sadegh M Majzedi, Briham Ehute, Rigel Gjemoon, and VV Venkatakrishnan. 2019. Poetter: Aligning attack behavior with kernel audit records for cyber threat hunting. In _Proceedings of the ACM SIGSAC Conference On Computer and Communications Security_. 1795-1812.
* Milgireath et al. (2019) Shilgireath M Majzedi, Rigel Gjemoon, Birham Ehute, Ramachandran Sekar, and V Venkatakrishnan. 2019. Holmes: real-time get detection through correlation of suspicious information flows. In _IEEE Symposium on Security and Privacy_. 1137-1152.
* Pan et al. (2024) Jonathan Pan, Wong Weng Xiang, and Yuan Yiki. 2024. RAGLog Log Anomaly Detection using Retrieval Augmented Generation. In _2024 IEEE World Forum on Public Safety Technology (WIPTSET)_. IEEE, 169-174.
* Rei et al. (2016) Keiqi, R Zhonghu Gu, Brendan Salindranzoglu, Shioging Ma, Fei Wang, Zhiwei Zhang, Luo Si, Xiangyu Zhang, and Dongyan Xu. 2016. Heretule: Attack story reconstruction via community discovery on correlated log graph. In _Proceedings of the Conference on Computer Security_. 358-359.
* Ren et al. (2023) Yifun Ren, Li, Ko Dong, Li Hui, Xue Zhao Li, and Shugging Zhou. 2023. Inter-mental graph classification by class prototype construction and augmentation. In _Proceedings of the 33rd ACM International Conference on Information and Knowledge Management_. 2316-245.
* Roy et al. (2024) Dreyer Roy, Xuchong Zhang, Raish Shue, Chetan Bansal, Pedro Las-Casas, Rodrigo Fonseca, and Sarava Rijambachan. 2024. Exploring In-based agents for root cause analysis. In _Companion Proceedings of the 33rd ACM International Conference on the Foundations of Software Engineering_. 208-219.
* Sculley (2010) David Sculley. 2010. Web-scale z-a machine. In _Proceedings of the International Conference on World Wide Web_. 11777-118.
* Sun et al. (2024) Shiwen Sun, Yintong Huo, Yunxin, Yuichi Li, Jun Li, and Zihong Zheng. 2024. Face i5vyness: An Ill-agent Yous-based two-stage strategy to localize configuration errors via tags. In _Proceedings of the 33rd ACM SIGSOFT International Symposium on Software Testing and Analysis_. 13-25.
* Tang et al. (2018) Yutao Tang, Dung Li, Zhichun Li, Jun Zhang, Kangkook Jee, Xuheng Xiao, Zhenyu Wu, Junghunen Rhee, Fengyuan Xu, and Qun Li. 2018. Nodemberg: Tensorlab base classifier data extraction for the big data causality analysis. In _Proceedings of the ACM SIGSAC Conference on Computer and Communications Security_. 1324-1337.
* Xu et al. (2014) Junjieq Xu, Ziting Cui, Yuan Zhao, Xu Zhang, Shilin He, Pinja He, Ligoni Li, Yu Kang, Qingyuen Lin, Yinguang Dang, et al. 2014. Onto: Automatic Logging via LLLM and In-Context Learning. In _Proceedings of the 46th IEEE/ACM International Conference on Software Engineering_. 1-12.
* Xu et al. (2024) Junjieq Xu, Rixinha Tang, Yintong Huo, Chenyang Zhang, and Prujha He. 2024. 108th: DoLog Parsing with Prompt Enhanced In-Context Learning. In _Proceedings of the IEEE/ACM on Multimedia Conference on Software Engineering_. 1-12.
* Xu et al. (2020) Yu, Xinglong Zhang, Wei Guo, Huifeng Guo, Ruiming Tang, and Mark Coates. 2020. GraphNet: Graph structure aware incremental learning for recommender systems. In _Proceedings of the 29th ACM International Conference on Information & Knowledge Management_. 2661-2668.
* Xu et al. (2022) Zhiqiang Xu, Pengcheng Fang, Changlin Liu, Xuheng Yao, and Dan Meng. 2022. Deepcomm: Graph summarization on system audit log for attack investigation. In _2022 IEEE Symposium on Security and Privacy (SP)_. IEEE, 540-557.

[MISSING_PAGE_FAIL:10]

inclusion of file management, communication, and content creation tasks allows us to assess the performance of our technique in contexts that go beyond typical web applications.

This comprehensive evaluation strategy enables us to thoroughly verify that our method can achieve strong results across a wide spectrum of application domains, rather than being limited to a narrow set of web-centric scenarios. The breadth of the tested scenarios strengthens the practical significance and impact of our contributions.

We have selected applications that are critical for building the Internet services.

1. Apache Vulnerability : CVE-2021-41773 Apache is an open-source, cross-platform web server software and stands as one of the most widely-used choices for Internet services on the web today. It is developed and maintained by the Apache Software Foundation. Apache, renowned for stability, security, and efficiency, supports various operating systems including Windows, Linux, etc. It provides rich features and flexible configuration options, making it suitable for building various types of websites, including static websites, dynamic websites, and web applications. Apache supports a variety of programming languages and technologies, including PHP, Python, Perl, CGI, and more, empowering developers to effortlessly craft robust web applications. Moreover, Apache supports advanced features like virtual hosting, SSL/TLS encryption, URL rewriting, etc., catering to diverse requirements in website construction and operation.
2. Apache HTTPd Server 2.4.49 version introduces a new function with a path traversal vulnerability, but it needs to be combined with the traversal directory configuration "Require all granted". Attackers can exploit this vulnerability to achieve path traversal, read arbitrary files, or execute bash commands in httpd programs configured with CGI, thereby gaining the opportunity to control the server and access the root directory to read the files inside.
3. Apache-Pgsql Vulnerability : CVE-2019-9193 PostgreSQL is one of the most popular database systems today. It is the most commonly used database on Mac OSX systems, but it also provides versions for Windows and Linux operating systems. (Metasploit on Kali uses the PostgreSQL database.)
4. The vulnerability is caused by a feature of PostgreSQL that allows specific users to execute arbitrary code within the PostgreSQL environment. This feature is enabled by default in PostgreSQL versions 9.3-11.2. Starting from version 9.3, PostgreSQL implemented a new feature called COPY TO/FROM PROGRAM, which allows superusers and users to execute arbitrary operating system commands. In this dataset, we set up a web service using Apache and PostgreSQL version 9.3. We utilized the vulnerability to perform normal database operations and attack operations. We collected application logs, audit logs, and network logs from Apache and PostgreSQL.
5. IM-1 Vulnerability : CVE-2016-3714 ImageMagick is a widely used image processing program that many vendors use for tasks such as resizing, cropping, watermarking, and format conversion. However, researchers have discovered that when a user inputs an image containing'malformed content', it can trigger a command injection vulnerability. One of the most serious vulnerabilities is CVE-2016-3714, which allows remote code execution. This vulnerability affects version 6.9.3-9 and all versions prior. ImageMagick has a feature called delegate, which is used to call external libraries to handle files. The process of calling external libraries uses the system's system command, which is the cause of this vulnerability.
6. IM-2 Vulnerability : CVE-2022-44268 In ImageMagick versions prior to 7.1.0-51, there is a feature in the code that handles PNG files. This feature can lead to the reading of arbitrary files on the current operating system when converting images, and then outputting the contents of those files into the image content.
7. In the above two datasets of ImageMagick, we constructed a web application using Apache and ImageMagick, and collected logs from multiple sources.
8. Nginx Vulnerability : Path Traversal Nginx is a high-performance open-source web server and reverse proxy server known for its exceptional performance and high reliability. Nginx uses an event-driven architecture and asynchronous non-blocking processing to handle a large number of concurrent connections, making it perform well under high loads. Nginx also offers flexible configuration options and rich features, making it suitable for various types of web services, including serving static content, dynamic content, and reverse proxying. Due to its low resource consumption, high stability, ease of configuration, and scalability, Nginx has become the preferred server software for many websites and applications. In Nginx, when configuring an alias using the alias directive, forgetting to include a trailing slash () (i.e., using '/files'

\begin{table}
\begin{tabular}{c|c c|c c|c c|c c|c c|c c|c c|c c} \hline \hline \multirow{2}{*}{Method} & \multicolumn{3}{c|}{Apache} & \multicolumn{3}{c|}{IM-1} & \multicolumn{3}{c|}{IM-2} & \multicolumn{3}{c|}{Vim} & \multicolumn{3}{c|}{Redis} & \multicolumn{3}{c|}{Pgsql} & \multicolumn{3}{c|}{ProFTPd} & \multicolumn{3}{c}{Nginx} \\  & EloU & F1 & EloU & F1 & EloU & F1 & EloU & F1 & EloU & F1 & EloU & F1 & EloU & F1 & EloU & F1 \\ \hline Codex & 68.52 & 82.15 & 58.25 & 84.33 & 50.03 & 84.67 & 56.01 & 59.13 & 63.21 & 89.06 & 60.34 & 86.43 & 59.44 & 59.75 & 70.23 & 85.91 \\ GPT-3 & 60.24 & 68.23 & 59.23 & 86.34 & 49.78 & 89.32 & 67.39 & 86.25 & 69.35 & 89.65 & 68.27 & 81.16 & 70.22 & 88.65 & 64.74 & 88.39 \\ ILaMa-2 & 51.09 & 85.54 & 51.39 & 80.25 & 48.61 & 76.51 & 60.80 & 84.85 & 66.21 & 86.80 & 62.75 & 71.95 & 61.70 & 67.91 & 57.56 & 80.46 \\ \hline
**Ours** & **74.62** & **91.62** & **70.35** & **90.26** & **73.93** & **93.92** & **76.08** & **96.07** & **78.24** & **98.32** & **74.56** & **93.08** & **74.28** & **92.73** & **73.31** & **93.27** \\ \hline \hline \end{tabular}
\end{table}
Table 4. Ablation study results (EloU % and F1 %) of different LIM Backbones. The best results are shown in bold type and the runner-ups are underlined.

instead of "/files/") can result in a directory traversal vulnerability. The original purpose of this location block was to allow users to access files under the /home directory.
* (6) ProFTPD Vulnerability : CVE-2019-12815
* ProFTPD is an open-source, highly configurable FTP server software that supports multiple operating systems, including Linux, Unix, and Windows. It offers rich features and flexible configuration options to meet various FTP server requirements. ProFTPD is known for its good performance and security, supporting virtual users, restricting user permissions, logging, and SSL/TLS encrypted transmission for data security. Easy to install and configure, ProFTPD is suitable for networks of all sizes and is a popular choice for many organizations and individuals as an FTP server software.
* There is a vulnerability in ProFTPD <= 1.3.6 that allows arbitrary file copying. This vulnerability is due to the custom SITE CPFR and SITE CPTO commands in the mod_copy module not properly checking read/write permissions. An attacker can exploit this vulnerability to copy any file on the FTP server without permission.
* (7) Redis Vulnerability : CVE-2022-0543
* Redis is an open-source in-memory database that can also be used as a cache and message broker. It supports various data structures such as strings, hashes, lists, sets, and sorted sets, providing rich commands and flexible configuration options. Redis offers high performance, persistence, replication, clustering, and more, making it versatile across various applications. As an efficient key-value store and caching solution, Redis is widely used in the Internet and big data fields. Its simplicity and ease of use allow developers to quickly build high-performance applications and provide reliable data storage and access services in distributed environments.
* Redis has a vulnerability where, after a user connects to Redis, they can execute Lua scripts using the eval command.
* However, these scripts are run in a sandbox, and under normal circumstances, cannot execute commands or read files.
* Some distributions, such as Debian, Ubuntu, and CentOS, patch the original software with additional packages. For example, Debian's patch for Redis includes an include statement.
* Unfortunately, in Debian and Ubuntu's packaging of Redis, a package object was inadvertently left in the Lua sandbox. Attackers can exploit this object to load functions from the libhua dynamic link library (DLL) and escape the sandbox to execute arbitrary commands. By utilizing the package.loadlib function in the Lua sandbox to load functions from /usr/lib/x86_64-linux-gnu/libhua5.1.s.o., an attacker can gain access to the iob library and use it to execute commands.
* (8) Vim Vulnerability : CVE-2019-12735
* Vim is a powerful text editor widely used on Unix and Unlike systems. It boasts advanced features such as syntax highlighting, code folding, auto-completion, multi-level undo/redo, and macro recording. Vim supports various modes of operation, including insert mode, command mode, and visual mode, making editing more efficient. Vim also supports a variety of plugins and scripts to extend its functionality. Due to its high degree of customization and powerful features, Vim is favored by many developers and system administrators as their preferred editor.
* CVE-2019-12735 is a vulnerability in Vim versions before 8.1.1365 and Neovim versions before 0.3.6. It allows remote attackers to execute arbitrary OS commands via the source! command in a modeling, as demonstrated by execute in Vim, and assert_fails or nivm_input in Neovim.

### Data Collection

For each dataset, the data collection process involves gathering application logs, network logs, and audit logs. We employ Linux Auditd, a widely used tool for recording audit logs (Li et al., 2019), along with built-in logs from Internet services for application logs, while capturing network logs with shark. It should be noted that log collection configurations must be adjusted to capture comprehensive fields, as default settings only gather a limited set, resulting in incompleteness. In addition, we outline the specific collection methods for these three types of logs (Lewis et al., 2019; Li et al., 2019; Li et al., 2019).

1. Application Logs Different kinds of applications employ distinct logging mechanisms tailored to their specific needs. Application logs record important events with application-specific semantics pertaining to the application's behavior, errors, and performance. To collect application logs, we conducted research on the optional configurations of various application logs, aiming to record all log fields that are useful for the experiment to the fullest extent. We focused on fields such as IP addresses, port numbers, payloads, and fields that can reveal the type of event. This enabled us to achieve full collection of application logs.

We have configured the application logs for Apache and PostgreSQL with special configurations, while the rest of the application logs use default configurations. Apache:

```
1$LogFormat"%h%l%r!"%s!"%s!"%o%a%a%p%p"combined
3$CustomLog/var/log/apache2/access.log
4combined Postgresql:
1# = Where to Log =!log_destination='stderr,csvlog'
2. Network Logs Network logs are records of network traffic, detailing communication between devices. They contain valuable information such as source and destination IP addresses, ports, protocols, and timestamps.
3. Thank, a command-line network protocol analyzer, is a powerful tool for capturing and analyzing network logs. It can capture live traffic from a network interface or read saved capture files. Thank's filtering capabilities allow analysts to focus on specific traffic of interest, making it easier to identify patterns or anomalies. Additionally, Tshark can output captured data in various formats for further analysis or integration with other tools. Overall, Tshark is a versatile tool for analyzing network logs and gaining insights into network activity. Therefore, we collect network logs containing all fields using tshark, saving and reading them in JSON format.

We have applied a uniform tshark configuration to all network logs as follows:

```
1$fshark-n-rtest.pcap-Tfields-E
2headers-eframe.number
3-eframe.time
4-eip.src-etep.srcport
5-eip.dst-etep.dstport
6-eip.proto-frame.len
7-e_ws.col.Info
8-eframe.interface_name
9-eframe.interface_description
10-eframe.encap_type
11-eframe.offset_shift
12-eframe.time_epoch-eframe.time_delta
13-eframe.time_delta_displayed
14-eframe.time_relative
15-eframe.cap_len-eeth.dst-eeth.src
16-eeth.type-ehtp.response.version
17-ehttp.response.code
18-ehttp.response.code.desc
19-ehttp.response.phrase-ehttp.server
20-ehttp.response.line
21-ehttp.content_encoding
22-ehttp.content_length
23-ehttp.connection-ehtp.content_type
24-ehttp.response_number
25-ehtp.time
26-ehtp.request_in
27-ehtp.response_for.uri
28-ehtp.file_data
29-edata-text-lines
30>out.tsv
```
3
43Audit Logs are records that provide a detailed account of system and application activity, helping organizations track access, changes, and other events for security and compliance purposes. Audid is the user-space component of the Linux Auditing System, responsible for writing audit records to the disk. It monitors various system calls and generates audit logs based on pre-defined rules.
44Audit allows administrators to configure what events to monitor and how to handle them. It can log events such as file access, process execution, user authentication, and more. The audit logs produced by Audid are stored in a binary format and can be viewed using the ausearch or aureport commands.

audit provides detailed information about system activity, helping administrators detect unauthorized access attempts, track system changes, and investigate security incidents. It is a critical component of a comprehensive security monitoring strategy for Linux systems, offering insights into system behavior and helping ensure compliance with security policies and regulations.

We use Audit to record the system calls related to processes and files involved in the experiment. Then, we manually analyze the data to remove redundant system call information.

For audit logs, we have applied special configurations to Pgsql, IM-1, and IM-2 datasets, while the rest of the datasets use default configurations.

Apache-Pgsql:
1=D
2=b8192
3=f1
4=backlog_wait_time0
5=always,exit-Sall-Fexe=/usr/local/
6pgsql/bin/postres-kpgsql_audit
7=w/etc/passwd-prwxa-kpasswd_audit
8=always,exit-Sall-Fexe=/usr/sbin/
9apache2-kapache_audit
10=always,exit-Farch=b64-Sbind
11=always,exit-Sread,write,open

[MISSING_PAGE_FAIL:14]

that we apply a few existing graph reduction algorithms to the generated behavior graphs. If we remove the graph reduction methods, the time costs will be significantly reduced. The results are shown in Fig. 10.

In the comparison of time cost with different backbone models, we conducted experiments using various backbones. Notably, our proposed method is able to strike a balance between time efficiency and performance.

## Appendix H Code File

The code of our system is placed in the **code** directory of this material and detailed instructions for experiments are shown in **code/README.pdf** file.

### Python Environment Setup With Conda

Our code is written in Python3.10.8 with cuda 12.1 and pytorch 2.1.0 on Ubuntu 22.04.

\begin{table}
\begin{tabular}{p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt} p{56.9pt}} \hline \hline
**Rule Type** & **Rule Source** & **Rule Target** & **Fields Required** & **Description** & **1688** \\ \hline Event Division & Network Log & Network Log & IP, Port, Time Range & In a period of time, the traffic between two (IP, Port) is considered as a connection. & 1706 \\ \hline Event Division & Apache Log & Apache Log & IP, URL & A single visit to a specific URL is considered as a separate event. & 1706 \\ \hline Event Division & PotptregSQL Log & PotptregSQL Log & IP, Port, PID & A single transaction of a database operation is considered as a separate event. & 1706 \\ \hline Event Division & Redis Log & Redis Log & IP, Port, Action & A single database operation is considered as a separate event. & 1706 \\ \hline Event Division & ImageMagick Log & ImageMagick Log & IP, Port, PID & A single operation on the image is considered as a separate event. & 1707 \\ \hline Event Division & Nginx Log & Nginx Log & IP, URL & A single visit to a specific URL is considered as a separate event. & 1708 \\ \hline Event Division & ProHpd Log & ProHpd Log & IP, Port, Filename & A single file transfer between two IP addresses is considered as a separate event. & 1711 \\ \hline Event Division & Vim Log & Vim Log & Filename, Action & A single operation on the file is considered as a separate event. & 1712 \\ \hline Direct Correlation & Network Log & Apache Log & IP, PORT,Time Range,URL & Associate logs based on the resources accessed, IP port, and time range. & 1713 \\ \hline Direct Correlation & Network Log & PotptregSQL Log & IP, PORT,Time Range,HTTP Filedata & Find database-related Network logs through HTTP file data, time, IP, and port. & 1713 \\ \hline Direct Correlation & Network Log & Redis Log & IP, PORT,Time Range,HTTP Filedata & Find database-related Network logs through file data, time, IP, and port. & 1713 \\ \hline Direct Correlation & Network Log & ImageMagick Log & IP, PORT,PID, Time Range & Find imageMagick-related Network logs through time, IP, port and PID. & 1712 \\ \hline Direct Correlation & Network Log & Nginx Log & IP, PORT,Time Range,URL & Associate logs based on the resources accessed, IP port, and time range. & 1712 \\ \hline Direct Correlation & Network Log & ProHpd Log & IP, PORT,Time Range,Filename & Associate logs based on the resources transferred, IP port, and time range. & 1714 \\ \hline Indirect Equivalence & Apache Log & PostgreSQL Log & No Field Required & PostgreSQL Log and Apache Log relate with each other by relating with any the same Network Log. & 1715 \\ \hline Indirect Equivalence & Apache Log & ImageMagick Log & No Field Required & ImageMagick Log and Apache Log relate with each other by relating with any the same Network Log. & 1716 \\ \hline \hline \end{tabular}
\end{table}
Table 6. A complete List of Labeling Rules

\begin{table}
\begin{tabular}{p{56.9pt} p{56.9pt} p{56.9pt}} \hline \hline
**Seenarios** & **Vulnerability Description** & **Description** \\ \hline Apache & CVE-2021-41773 & Vulnerability allows attackers to gain control of the server and access sensitive files. & 1688 \\ IM-1 & CVE-2016-3714 & The vulnerability exists because of the insufficient filtering for the file names passed to a system() call. & 1688 \\ \hline Vim & CVE-2019-12735 & Vulnerability allows remote attackers to execute arbitrary OS commands via the source! command & 1689 \\ Redis & CVE-2022-0543 & Vulnerability allows remote attackers to escape the sandbox to execute arbitrary commands. & 1689 \\ Pgsql & CVE-2019-9193 & Vulnerability allows specific users to execute arbitrary code within the PostgreSQL environment. & 1691 \\ ProFTPd & CVE-2019-12815 & There is a vulnerability in ProFTPd \(<\) = 1.3.6 that allows arbitrary file copying. & 1692 \\ IM-2 & CVE-2022-44268 & Vulnerability leads to the reading of arbitrary files on the current operating system when converting images & 1693 \\ \hline Nginx & Path Traversal & Forgetting to include a trailing slash can result in a directory traversal vulnerability. & 1694 \\ \hline \hline \end{tabular}
\end{table}
Table 5. Overview of Dataset for FG-CIBGC Evaluation.

install anaconda: [https://repo.anaconda.com/archive/index.html](https://repo.anaconda.com/archive/index.html).
* install torch-scatter 2.1.2+pt21cu121 with the whl file downloaded from here.
* $condc create --name FG-CIBGC
* $condactivateFG-CIBGC
* $pipinstall -rrequirements.txt

### Dataset

Our full dataset's compressed file size is around 2.3GB. Due to space constraints, we are only providing a sample dataset (Apache) here.

### Directory

We present a brief introduction about the directories.

### Workflow

In this section, we introduce the workflow of the overall project.

#### h.4.1. Parse

The "hlogs_parse.py" file in this directory serves as the entry point for all log preprocessing and parsing. This part is responsible for parsing audit logs, application logs, and network logs, and it generates associated JSON files for subsequent correlation with high-level behavior units and audit logs. See the following command :

1. $ cdParse
2. $ pythonhlogs_parse.py
3. --datastename=$dataset

#### h.4.2. Embedding

The code in this directory accomplishes two main tasks. Firstly, "run.py" correlates behavior units with audit logs, ultimately generating fine-grained behavior graphs. Secondly, "run.py" executes graph embedding. See the following command :

1. $ cd Embedding
2. $ pythonrun.py --dataset=$dataset
3. --kg=$algorithm

#### h.4.3. Classification

The code in this directory aims to produce classification results. See the following command:

1. $ cdClassification
2. $ pythonrun.py --dataset=$dataset
3. --classification=$classification

#### h.4.4. Evaluate

The code in this directory produces evaluation results. See the following command :

1. $ cdTools
2. $ python3evaluate.py --dataset$dataset
3. --output.txt

### Reproducibility

Use bash.sh to reproduce the results of performance comparison.

1. $ bash bash.sh

\begin{table}
\begin{tabular}{c|c c|c c|c c|c c|c c|c c|c c|c c} \hline \multirow{2}{*}{Method} & \multicolumn{3}{c|}{Apache} & \multicolumn{3}{c|}{IM-1} & \multicolumn{3}{c|}{IM-2} & \multicolumn{3}{c|}{Vim} & \multicolumn{3}{c|}{Redis} & \multicolumn{3}{c|}{Pgsql} & \multicolumn{3}{c}{ProFTPd} & \multicolumn{3}{c}{Nginx} \\  & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 & Acc & F1 \\ \hline Tgminer & 68.93 & 65.15 & 69.56 & 66.33 & 62.63 & 67.67 & 69.79 & 60.13 & 74.53 & 79.06 & 62.09 & 66.43 & 69.56 & 59.75 & 63.59 & 65.91 & 1885 \\ Watson & 71.54 & 69.23 & 63.59 & 56.34 & 58.43 & 66.30 & 69.49 & 56.79 & 81.69 & 82.65 & 69.43 & 61.16 & 61.93 & 52.25 & 65.84 & 54.86 & 1868 \\ DepComm & 71.63 & 70.54 & 73.49 & 75.99 & 74.60 & 72.51 & 75.89 & 72.84 & 80.71 & 80.80 & 72.75 & 70.45 & 78.91 & 67.91 & 78.76 & 76.46 \\ \hline Fine-Tuning(+) & 73.44 & 71.08 & 73.33 & 76.35 & 76.95 & 74.94 & 76.35 & 73.75 & 85.12 & 78.97 & 75.93 & 73.27 & 72.98 & 68.21 & 78.45 & 78.92 \\ EWC(+) & 76.23 & 74.95 & 74.22 & 78.02 & 80.93 & 80.74 & 83.72 & 82.42 & 84.54 & 85.15 & 99.56 & 87.32 & 87.10 & 84.34 & 83.26 & 80.84 \\ LwF(+) & 82.78 & 73.22 & 75.84 & 77.83 & 79.56 & 78.85 & 81.89 & 76.10 & 83.90 & 85.84 & 76.94 & 81.61 & 86.66 & 87.26 & 83.97 & 82.23 & 1810 \\ GEM(+) & 79.34 & 76.83 & 74.28 & 74.98 & 78.39 & 76.93 & 87.46 & 82.15 & 85.06 & 84.43 & 79.45 & 81.26 & 78.12 & 81.03 & 84.57 & 83.53 & 1811 \\ TWP(+) & 89.54 & 86.88 & 86.73 & 85.55 & 87.31 & 86.57 & 88.14 & 84.43 & 85.37 & 85.43 & 71.14 & 76.95 & 80.93 & 85.46 & 81.22 & 85.89 & 1822 \\ CPCA(+) & 86.82 & 86.42 & 79.91 & 81.91 & 88.93 & 87.84 & 86.27 & 83.26 & 88.39 & 87.24 & 87.59 & 85.54 & 89.90 & 87.82 & 87.69 & 87.60 \\ \hline tdGraphEmbed(+) & 77.03 & 75.12 & 66.41 & 60.78 & 68.32 & 67.37 & 78.41 & 73.81 & 79.45 & 79.03 & 73.56 & 72.62 & 73.08 & 67.63 & 65.98 & 61.54 \\ GraphERT(+) & 86.96 & 77.71 & 72.59 & 68.44 & 78.79 & 78.23 & 85.91 & 75.32 & 84.69 & 78.39 & 85.02 & 79.32 & 82.14 & 77.89 & 75.63 & 75.32 & 1818 \\ TP-GNN(+) & 81.53 & 78.56 & 76.29 & 71.84 & 77.59 & 77.93 & 84.68 & 74.26 & 82.67 & 76.21 & 78.94 & 73.45 & 73.77 & 72.09 & 81.63 & 79.84 \\ \hline
**Ours** & **95.19** & **91.62** & **91.26** & **90.26** & **94.12** & **93.92** & **96.13** & **96.07** & **98.65** & **98.32** & **94.73** & **90.88** & **94.39** & **92.73** & **93.63** & **93.27** \\ \hline \end{tabular}
\end{table}
Table 7. Comparison Results (Acc % and F1 %) of Class-Incremental Behavior Graph Classification Task Across Datasets. “(+)” indicates that the input to this baseline is the fine-grained behavior graphs generated by gPartition. The best results are shown in bold type and the runner-ups are underlined.

Figure 10. The time overhead comparison in terms of behavior graph partitioning without graph reduction.

Use basemode_grid_search.sh to reproduce the results of grid search.
* $ bash basemode_grid_search.sh

## Appendix A Further Explanation of Implementation

It is important to note that the labels for coarse-grained classification differ from those for fine-grained classification. When comparing classification performance, we assess the F1 score of coarse-grained classification using coarse-grained labels, while fine-grained classification is evaluated using fine-grained labels. However, when evaluating the EIoU (Edge Intersection over Union), a single coarse-grained label may correspond to multiple fine-grained labels. For example, coarse-grained label 0 corresponds to fine-grained labels 0, 1, and 2. In this case, when evaluating the EIoU, we adhere to the principle of using fine-grained labels. Consequently, the classifications made by the coarse-grained classification method for the fine-grained labels 1 and 2 are considered incorrect. We select GOOD-D [25] as the baseline OOD detector, as it is the sole open-source unsupervised graph-level OOD detection method available. We leave the exploration of alternative OOD detection techniques for future work.

## Appendix B KATE Algorithm Description

In FG-CIBGC, we choose KATE, a simple \(k\)NN-based sampling algorithm that does not involve much computational overhead in practice, for in-context example augmentation. Specifically, we begin by embedding all parsed log batch candidates \(x_{i}\) from training data into vector representations \(v_{i}\). Then, for each vectorized query \(a_{q}\), we calculate the similarity metric \(d(v_{q},v_{i})\) between it and all candidates, outputting the top-8 results as examples. Note that in our implementation, the similarity metric \(d(v,a_{i})\) represents the cosine distance as shown in Eq. 6.

\[d(v_{q},v_{i})=\cos(q_{q},v_{i})=\frac{v_{q}\cdot v_{i}}{\|q_{q}\|_{2}\|v_{i} \|_{2}}. \tag{6}\]

Moreover, some studies have also shown that the permutation of different examples in the context can also affect the performance of ICL seriously. For example, Zhao [50] pointed out that the model's prediction for a query tends to be biased towards the closest example (i.e., recency bias), which means if the example closest to the query in the prompt is similar enough to the query, the model's prediction for the query will tend towards the results closest to the query (i.e., obtaining the correct prediction according to the nearest example's label supervision). Therefore, we choose to directly use the similarity measure \(d\) obtained in the previous step to arrange these examples in ascending order, so that the example closest to the query is most similar to the query.

## References

* [1] S. B.