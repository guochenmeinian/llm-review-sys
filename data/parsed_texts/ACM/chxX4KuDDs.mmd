# SCOOT: SLO-Oriented Performance Tuning for LLM Inference Engines

Anonymous Author(s)

###### Abstract.

As large language models (LLMs) are gaining increasing popularity across a wide range of web applications, it is of great importance to optimize service-level objectives (SLOs) for LLM inference services to enhance user satisfaction and improve the competitiveness of cloud vendors. In this paper, we observe that adjusting the parameters of LLM inference engines can improve service performance, and the optimal parameter configurations of different services are different. Therefore, we propose SCOOT, an automatic performance tuning system to optimize SLOs for each LLM inference service by tuning the parameters of the inference engine. SCOOT jointly exploits single-objective and multiple-objective Bayesian optimization (BO) techniques to handle various optimization objectives via exploration and exploitation. Moreover, SCOOT prunes the search space with known constraints and adopts a random forest to learn hidden constraints during the tuning process to mitigate invalid exploration. To improve the tuning efficiency, SCOOT utilizes the parallel suggestion to accelerate the tuning process. Extensive experiments demonstrate that SCOOT considerably outperforms existing tuning techniques in SLO optimization while greatly improving the tuning efficiency. SCOOT is universally applicable to various LLM inference engines and is easily expandable to new parameters. Currently, SCOOT has already been implemented in the production environment of a leading international technology company.

SLOs, 2022

## 1. Introduction

With the impressive capabilities demonstrated by large language models (LLMs) across various web applications (Bogesian et al., 2016), cloud vendors such as Alibaba Cloud and AWS have started offering services of LLM deployment and inference (Bogesian et al., 2016; 2016). Customers (e.g., developers and providers of web applications) can deploy specified LLMs on their exclusive computing resources. Through the application programming interface (API) provided by the cloud vendor, customers' requests can be continuously served by the deployed LLMs.

These LLM inference services run LLM instances with advanced inference engines such as vLLM (Bogesian et al., 2016) and TensorRT-LLM (Bogesian et al., 2016), which are equipped with cutting-edge technologies, such as continuous batching (Bogesian et al., 2016), paged attention (Bogesian et al., 2016), and chunked prefill (Bogesian et al., 2016). These techniques can accelerate inference and improve throughput, thus delivering a high service performance to customers.

To ensure service performance, customers always agree on service level objectives (SLOs) with cloud vendors. SLOs are defined as a series of performance metric constraints, such as requiring that the 95th percentile latency of requests be less than 1 second. If cloud vendors violate these SLOs, they not only have to compensate customers but also face reputational damage. Therefore, appropriately setting SLOs is critical for cloud vendors. They should optimize SLOs (e.g., guarantee a lower tail latency) to improve customer satisfaction while ensuring that there won't be SLO violations.

Typically, cloud vendors stress test services with high request rates and set the performance metrics achieved under the stress testing as SLOs. This ensures that SLOs won't be violated even under heavy workload scenarios. By improving the service performance under stress testing, cloud vendors can deliver better SLOs to customers. In this paper, we observe that adjusting the parameters of LLM inference engines has great potential to improve service performance and the optimal parameter configurations of various LLM inference services are different. Therefore, we stand in the shoes of cloud vendors and optimize SLOs for each LLM inference service by tuning the parameters of LLM inference engines under stress testing. **The relevance of this paper to the systems and infrastructure for web is elaborated in Appendix A.1**.

Numerous performance tuning methods have been proposed and applied across various fields (Bogesian et al., 2016), but they all fall short of efficiency and optimality for tuning LLM inference engines. Methods like random sampling and meta-heuristic algorithms, such as Monte Carlo sampling (Gillespie et al., 2016) and genetic algorithms (Gillespie et al., 2016) lack efficiency as they fail to fully utilize historical information. Besides, heuristic searches rely on expert knowledge and elaborate pre-profiling to model relationships between parameters and performance. Nevertheless, with the rapid evolution of technologies in LLM inference engines, the parameters' numbers and ranges are frequently updated, which makes the modeled parameter-performance relationship outdated and the designed heuristic search methods inapplicable. Additionally, learning-based approaches such as reinforcement learning (RL) (Gillespie et al., 2016) and Bayesian optimization (BO) (Gillespie et al., 2016) have also been widely exploited in performance tuning. They can effectively leverage historical information and tune parameters automatically withoutprior knowledge. However, RL requires a time-consuming training process to obtain a well-behaved tuning agent, while existing BO solutions fail to address the following three challenges.

**Challenge 1: Various Optimization Objectives.** Customers seek to enhance different performance metrics depending on their application requirements, such as improving request throughput for the periodically invoked offline recommendation application, reducing request tail latency for the online classification application, and minimizing time-to-first-token (TTFT) and time-per-out-token (TPOT) simultaneously for interactive applications such as chatbot, requiring the tuner to have the capability of handling both single-objective and multi-objective optimization problems.

**Challenge 2: Complex Known and Hidden Constraints.** Some parameters of inference engines depend on the settings of other parameters, thus causing constraints on the search space. For example, for vLLM, max-num-batched-tokens must be greater than or equal to max-num-seqs. We refer to these as known constraints, which can be provided to the tuner ahead of time. Besides, given a specific service, certain parameter combinations can lead to inference engine crashes during the stress testing. In this paper, these infeasible parameter combinations are referred to as hidden constraints. Different inference services have different hidden constraints that are initially unknown and must be learned throughout the tuning process. Specifically, for some certain services, vLLM often crushes due to a timeout error during the stress testing when scheduler-delay-factor is set to a large value.

**Challenge 3: High Evaluation Overhead.** Learning-based tuners always learn the correlation between the service performance and the engine's parameter settings by evaluating various parameter configurations. As stress testing an inference service takes about 5 to 10 minutes, even with only 30 evaluations for tuning, the total time spent tuning a single service can range from 2.5 to 5 hours. Since the increasing popularity of LLMs leads to the deployment of a large number of LLM inference services, the cumulative time required for tuning these services is prohibitive.

To tackle these challenges, we propose SCOOT, a **S**ervi**C**e-level Objective Oriented performance **T**uning system, which automatically tune parameters of LLM inference engines to optimize SLOs for LLM inference services. We first propose a general formulation of the inference engine tuning problem to accommodate various optimization objectives and complex constraints, and SCOOT can resolve the problem with BO, where single-objective BO (SOBO) and multi-objective BO (MOBO) (Kumar et al., 2017; Wang et al., 2018; Wang et al., 2018) are respectively employed to search optimized parameter configurations for single-objective and multi-objective optimization scenarios, thus addressing challenge 1. Since constraint violations result in invalid observations caused by engine crashes, which greatly hurts the tuning efficiency, SCOOT prunes the search space with known constraints and exploits a random forest to learn hidden constraints during the tuning process, thus mitigating challenge 2. To resolve challenge 3, SCOOT employs the parallel suggestion technique to recommend multiple parameter configurations each time for simultaneous evaluation, thus fully utilizing idle computing resources to speed up tuning. SCOOT can support various inference engines and is easily expandable to new parameters. It's currently in use at Company-X1.

Footnote 1: Company-X is the anonymous name of a technology company with billions of users.

We conduct extensive experiments with various LLMs and different types and numbers of GPUs under request traces collected from various LLM-based web applications at Company-X. The results show that SCOOT can speed up the tuning process and significantly optimize SLOs, improving the request throughput by up to 68.3%, reducing the request tail latency by up to 40.6%, and reducing the TFT and TPOT by up to 99.8% and 61.0%, respectively, compared to the default parameter configuration and existing tuning methods. The main contribution of this paper is summarized as follows.

* To the best of our knowledge, this is the first study that introduces performance tuning into the field of LLM serving, and we uncover the significance of tuning LLM inference engines with real-world request traces.
* We propose a general formulation of the inference engine tuning problem that accommodates various optimization objectives and constraints, and we design SCOOT to solve the problem by intelligently searching optimized parameter configurations with BO.
* Random forest regression is employed by SCOOT to learn hidden constraints during the tuning process to avoid invalid explorations, while the parallel suggestion technique is adopted to significantly improve the tuning efficiency using additional computing resources.
* Extensive experiments are conducted to confirm the superiority of SCOOT in terms of both the optimality and efficiency for tuning LLM inference engines under various LLMs, computing resources, and request traces.

## 2. Background and Motivation

**Background: parameters of the LLM inference engine.** To provide flexibility of use, inference engines expose many parameters. For LLM, these parameters include boolean variables such as enable-chunked-prefill that can enable the chunked prefill technique, integer and float variables such as max-num-seqs and scheduler-delay-factor that can change the request scheduling strategy, and enumeration variables such as block-size that can change the memory allocation policy. In this paper, we focus on tuning parameters that do not affect the accuracy of LLMs. Therefore, we do not consider parameters related to model compression, such as quantization. Besides, although speculative decoding has been theoretically proven not to hurt LLM accuracy (Kumar et al., 2017), it still affects the LLMs' generation results, making it inapplicable for certain applications. Hence, we also do not tune parameters related to speculative decoding. In this paper, we choose vLLM as the inference engine, and the parameters to be tuned are listed in Table 1, which constructs a huge search space of billions of configuration points.

\begin{table}
\begin{tabular}{c|c|c} Configuration Parameter & Type & Range \\ \hline \hline tensor-parallel & Integer & [1, 4 GPUs] \\ \hline max-num-seqs & Integer & [64, 8192] \\ \hline max-num-batched-tokens & Integer & [64, 8192] \\ \hline block-size & Enumeration & [8, 16, 32] \\ \hline scheduler-delay-factor & Float & [0, 2] \\ \hline enable-chunked-prefill & Boolean & \{True, False\} \\ \hline enable-prefix-caching & Boolean & \{True, False\} \\ \hline disable-custom-all-reduce & Boolean & \{True, False\} \\ \hline use-v2-block-manager & Boolean & \{True, False\} \\ \hline \end{tabular}
\end{table}
Table 1. PARAMETERS TO TUNE.

[MISSING_PAGE_FAIL:3]

### SCOOT Workflow

The workflow of SCOOT is depicted in Fig. 3, which consists of nine key steps to find optimized parameter configurations. Customers define their optimization objectives, and SCOOT \(\bullet\) gathers their request traces that include both input text of requests and output text generated by the LLM. Then, SCOOT leverages Sobol sequence-based Quasi-Monte Carlo (Selvin, 2015) to uniformly \(\bullet\) sample configurations across the search space, where the number of samples matches the search space's dimensionality. Then, SCOOT runs the inference engine with each sampled configuration and stress tests the inference service to obtain initial observations. These observations of configuration-performance pairs are leveraged to \(\bullet\) build a random forest and \(\bullet\) construct a surrogate model to learn the \(POF(\cdot)\) and predict the probability distribution of each optimization objective, respectively. Subsequently, acquisition functions are exploited to \(\bullet\) assess configurations according to the predicted results. Based on the assessment, a solver \(\bullet\) suggests multiple configurations in parallel while adhering to the known constraints and \(\bullet\) ensuring hidden constraints using \(POF(\cdot)\) learned by the random forest. Lastly, the inference engine \(\bullet\) is started with each suggested parameter configuration, and the stress testing is conducted to obtain new observations to refine the random forest and the surrogate model. Steps \(\bullet\)-\(\bullet\) run iteratively, and the tuning process stops until the number of observations reaches a given threshold.

In the workflow, the LLM, GPU number, and GPU type used for tuning are the same as the inference service owned by the customer.

### Bayesian Optimization-based Solution

BO is a theoretically grounded method for finding the optimum of black-box functions. It can explore the complex multi-dimensional search space efficiently and intelligently (Kang et al., 2017), which is suitable for solving problems with expensive evaluation overhead. BO leverages a surrogate model to approximate the objective function and use acquisition functions to assess configuration points for suggesting.

SCOOT leverages SOBO to maximize throughput and minimize tail latency for non-interactive offline and online applications, respectively. For interactive applications, it exploits MOBO to minimize TIFT and TPOT simultaneously by finding a set of parameter configurations representing the Pareto frontier that denotes the optimal trade-offs between TIFT and TPOT. We do not linearly combine TIFT and TPOT as a single optimization objective and solve it with SOBO because TIFT can be hundreds or even thousands of times larger than TPOT. Hence, it is difficult to assign weights to TIFT and TPOT to make a good trade-off. Besides, TIFT and TPOT are always two conflicting optimization objectives as illustrated in Appendix A.2.

### Surrogate Model

The surrogate model of BO predicts the objective function \(f(\mathbf{x})\) based on observations. It models \(f(\mathbf{x})\) for a given configuration \(\mathbf{x}\) as a random variable and predicts its probability distribution. In the context of LLM inference engine tuning, \(f(\mathbf{x})\) can represent the objective functions of request throughput \(T(\mathbf{x})\), request tail latency \(L(\mathbf{x})\), average TIFT \(\Phi(\mathbf{x})\), and average TPOT \(\Theta(\mathbf{x})\).

SCOOT uses the Gaussian process (GP) as the surrogate model. Given a parameter configuration \(\mathbf{x}\), GP assumes that the probability distribution of \(f(\mathbf{x})\) follows a Gaussian distribution whose mean \(\mu(f(\mathbf{x}))\) and the variance \(\sigma^{2}(f(\mathbf{x}))\) are respectively computed by

\[\mu(f(\mathbf{x})) =k(\mathbf{x},\mathbf{X})(k(\mathbf{X},\mathbf{X})+\tau^{2}l)^{-1}\mathbf{y}, \tag{4}\] \[\sigma^{2}(f(\mathbf{x})) =k(\mathbf{x},\mathbf{x})-k(\mathbf{x},\mathbf{X})(k(\mathbf{X},\mathbf{X})+\tau^{2}l)^{- 1}k(\mathbf{X},\mathbf{x}), \tag{5}\]

where \(\mathbf{X}\) represents the previously evaluated configurations, \(\mathbf{y}\) denotes the corresponding observed objective values, \(\tau^{2}\) is the level of white noise, and \(k(\mathbf{x},\mathbf{x}^{\prime})\) is the covariance function that quantifies the similarity between input points \(\mathbf{x}\) and \(\mathbf{x}^{\prime}\) for inferring the relationships between their objective function values. SCOOT employs the Matern kernel (\(\frac{2}{2}\)) with input wrapping (Kang et al., 2017) as the covariance function due to its capability to balance the smoothness and flexibility when modeling unknown functions. SCOOT utilizes maximum likelihood estimation (Selvin, 2015) to learn \(\tau^{2}\) during tuning.

For SOBO, a single-output GP is leveraged to predict the objective function. For MOBO, SCOOT adopts a multi-output GP by considering each output to be independent. During the tuning process, the prediction accuracy of the GP model is continuously improved as more observations are collected.

### Acquisition Function

The acquisition function of BO assesses parameter configurations in the search space, which calculates a score for each configuration point \(\mathbf{x}\) according to the surrogate model's predicted mean \(\mu(f(\mathbf{x}))\) (indicating the expected performance) and variance \(\sigma^{2}(f(\mathbf{x}))\) (representing uncertainty). Parameter configurations with high scores are more likely to be suggested for evaluation. Different acquisition functions balance exploration (visiting areas with high uncertainty) and exploitation (intensively searching areas with good known objective values) in different manners.

#### 4.4.1. **SOBO Acquisition Function**

For SOBO, commonly used acquisition functions include upper confidence bound (UCB), probability of improvement (PI), and expected improvement (EI). UCB incorporates both mean and variance into the score and prioritizes the point with a high balance of exploration and exploitation through a trade-off parameter \(\beta\), which is expressed by

\[UCB(\mathbf{x})=\mu(f(\mathbf{x}))+\beta\cdot\sigma(f(\mathbf{x})), \tag{6}\]

Figure 3. SCOOT workflow. SCOOT leverages BO to find optimized parameter configurations via exploration and exploitation.

where \(\sigma(f(\mathbf{x}))=\sqrt{\sigma^{2}(f(\mathbf{x}))}\) represents the standard deviation.

PI prioritizes the point that is likely to yield an improvement over the current known best observation, which is expressed by

\[PI(\mathbf{x})=P(f(\mathbf{x})+\xi>f(\mathbf{x}^{+})), \tag{7}\]

where \(\mathbf{x}^{+}\) is the point that has the largest objective function value known so far, and \(\xi\) is leveraged to encourage exploration.

EI not only considers the probability of objective improvement but also the magnitude of the expected improvement, which is computed by

\[EI(\mathbf{x})=\mathbb{E}(\max(0,f(\mathbf{x})+\xi-f(\mathbf{x}^{+}))), \tag{8}\]

where \(\mathbf{x}^{+}\) represents the best observed configuration point as well, and \(\xi\) is also a parameter used to encourage exploration.

#### 4.4.2. **MOBO Acquisition Function**

For MOBO, expected hypervolume improvement (EHVI) is widely adopted as the acquisition function to identify promising parameter configurations that denote optimal trade-offs between multiple optimization objectives. EHVI assesses a new configuration point based on the potential improvement it might bring to the hypervolume of the existing solution set, which is calculated by

\[EHVI(\mathbf{x})=\mathbb{E}(\max(0,HV(\mathbf{\mathcal{Y}}\cup\{f(\mathbf{x})\})-HV(\mathbf{ \mathcal{Y}}))), \tag{9}\]

where \(HV(\cdot)\) is the hypervolume function and \(\mathbf{\mathcal{Y}}\) denotes the objective values of previously evaluated configurations.

The hypervolume function \(HV(\cdot)\) is calculated by measuring the volume of space enclosed between the Pareto frontier and a reference point \(\mathbf{r}\), where \(\mathbf{r}\) is typically a lower bound point for all objective functions. When there are only two optimization objectives, \(HV(\cdot)\) is calculated by summing the areas of rectangles formed between each point on the Pareto frontier and \(\mathbf{r}\). Figure 4 presents how the two-dimensional \(HV(\cdot)\) is computed.

A larger \(HV(\cdot)\) value indicates a Pareto front that covers a larger objective space, offering a broader range of promising solutions. By maximizing the EVHI, the search process can be guided towards configurations that can improve the overall quality and diversity of the Pareto frontier, thus optimizing multiple objectives simultaneously and providing more optional configurations for customers.

### Configuration Suggestion

#### 4.5.1. **Sobo Suggestion**

For SOBO, a common practice is to select a specific acquisition function and suggest parameter configurations that maximize the acquisition function. However, when it comes to real-world applications, the selected acquisition function might work sub-optimally for the tuning task, and the best acquisition function is challenging to identify beforehand (Kumar et al., 2017).

To mitigate this issue, SCOOT employs multi-objective acquisition function ensemble (MACE) (Kumar et al., 2017). MACE uses three acquisition functions, UCB, PI, and EI, to assess configuration points and runs a solver to find out the Pareto frontier that denotes the optimal trade-offs of the three acquisition functions. Suggested configurations are randomly selected in the Pareto frontier. With MACE, SCOOT can avoid using sub-optimal acquisition functions all the time, thereby improving the quality of suggested parameter configurations.

During the tuning process, for UCB, \(\beta\) is dynamically adjusted, and in the \(t\)th tuning iteration, \(\beta\) is calculated by \(2log(\frac{t^{2}\mathbf{\pi}}{60})\), where \(\mathbf{\pi}=\frac{1}{t^{2}}\)(Kumar et al., 2017). For EI and PI, \(\xi\) is set to a fixed value of 0.0001.

#### 4.5.2. **MoboS Suggestion**

For MOBO, SCOOT leverages the EHVI as the acquisition function and runs a solver to suggest parameter configurations by maximizing EHVI. The default configuration of the inference engine is chosen as the reference point \(\mathbf{r}\).

#### 4.5.3. **Known Constraint Assurance**

For both SOBO and MOBO, in the process of solvers solving optimization problems, the solution space is pruned by known constraints.

### Parallel Configuration Suggestion

With the popularity of LLMs, an increasing number of LLM inference services are being deployed, and tuning them incurs substantial time overhead. To mitigate this problem, SCOOT suggests multiple parameter configurations at a time and utilizes multiple sets of computing resources to conduct stress testing in parallel.

Given the parallelism degree \(k\), for SOBO, SCOOT randomly selects \(k\) points from the Pareto frontier of the three acquisition functions for suggesting. For MOBO, the top-\(k\) points with the largest EHVI are suggested. Thus, tuning can be accelerated by a factor of \(k\) when the total observation number is fixed.

Although the parallel suggestion requires additional computing resources to evaluate configurations in parallel, many computing resources in production clusters are often idle during off-peak hours such as nights and weekends, which can be conveniently used for tuning LLM inference engines.

### Random Forest-based POF Learning

To handle hidden constraints, a common practice is to assign penalty objective values to the infeasible parameter configurations that cause engine crashes. However, it is challenging to set appropriate penalty objective values. Besides, penalty objective values often hurt the surrogate model (Shen et al., 2015). Therefore, SCOOT leverages random forest regression to learn the \(POF(\cdot)\) and handle hidden constraints by restricting the solver to adhere to Eq. (3) in the process of configuration suggestion. In this way, SCOOT can substantially reduce invalid observations without affecting the surrogate model.

For the hidden constraint expressed in Eq. (3), the feasibility probability threshold \(\Delta\) is critical. If \(\Delta\) is fixed, setting it too high may result in repeatedly suggesting configuration points near already observed feasible configurations, potentially missing out on other superior configuration points. Conversely, setting \(\Delta\) too low may lead to suggestions of infeasible configurations. Hence, SCOOT dynamically adjusts the threshold \(\Delta\) during the tuning process.

The dynamic adjustment process of \(\Delta\) can be found in Algorithm 1, where we can observe that when infeasible parameter configurations are suggested, \(\Delta\) is increased to reduce the likelihood of

Figure 4. Illustration of two-dimensional HV. \(f_{1}\) and \(f_{2}\) are two objective functions and \(\mathbf{r}\) is the reference point. (a) Blue area represents the HV of the existing solution set. (b) Yellow area depicts the HV improvement after adding \(\mathbf{y}_{4}\).

further suggesting infeasible configurations. Besides, after continuously suggesting feasible configurations five times, \(\Delta\) is decreased to allow the discovery of potential superior configurations. Additionally, during the tuning process, the random forest is continuously refined using the latest observations to enhance prediction accuracy, ensuring that hidden constraints are intelligently followed without compromising the quality of performance tuning.

Since known and hidden constraints pure the solution space, solvers may be unable to give a sufficient number of solutions during the configuration suggestion. In such scenarios, SCOOT randomly samples configuration points with Sobol sequence-based Quasi-Monte Carlo to make up for the shortfall.

### Ensuring SLO Robustness

Through extensive experiments, we find that when the performance tuning is conducted under different request arrival orders, the best parameter configurations are almost the same, but the corresponding optimized SLOs vary. Thus, after performance tuning, we conduct stress testing multiple times again using the best configuration, varying request arrival orders, and select the worst-case objectives as the SLOs to ensure SLO Robustness. Finally, we summarize the overall tuning procedure of SCOOT in Algorithm 1.

```
Input:N: total observation number; \(k\): parallelism degree; // Initialize observations
1\(\mathcal{O}\leftarrow\) UNIFORM_SAMPLE_AND_EVALUATE()
2\(RF\leftarrow\) TRAIN(\(\mathcal{O}\)) // Train random forest regression
3\(\Delta\gets 0.5\); \(c\gets 0\); \(v\leftarrow\) 0.05
4while\(|\mathcal{O}|<N\)do
5// Suggest \(k\) configurations
6\(\{x_{1},...,x_{k}\}\leftarrow\) SUGGEST(\(k\), \(\mathcal{O}\), \(RF\), \(\Delta\)) // Collect performance metrics
7\(\{y_{1},...,y_{k}\}\leftarrow\) EVALUATE(\(\{x_{1},...,x_{k}\}\)) // Adjust feasibility threshold
8ifthere are invalid \(y_{1}\) in \(\{y_{1},...,y_{k}\}\)then
9\(\Delta\leftarrow\) min(\(0.75\), max(\(0.5\), \(\Delta+v\)))
10\(\leftarrow\) 0// Clear feasible suggestion count
11else
12\(c\gets c+k\)
13if\(c\geq 5\)then
14\(\Delta\leftarrow\) max(\(0.25\), \(\Delta-v\))
15\(c\gets c-5\)
16 // Update observations
17\(\mathcal{O}\leftarrow\) UNION(\(\mathcal{O}\), \(\{x_{1},...,x_{k}\}\), \(\{y_{1},...,y_{k}\}\))
18\(RF\leftarrow\) TRAIN(\(\mathcal{O}\)) // Refine Random Forest
19
20
21 end if
22
23 end while
```

**Algorithm 1**SCOOT: BO-based Performance Tuning

### Experiment Setup

We implement SCOOT on top of HEBO (Kumar et al., 2017), where the random forest regression is implemented using the sklearn (Pedregosa et al., 2011) library. Request traces are collected from four LLM inference services at Company-X, including applications of text-to-SQL (**SQL**), chatbot (**BOT**), classification (**CLS**), and recommendation (**REC**). Since the computational load of an LLM inference request depends on its input and output lengths, to enhance the robustness of optimized SLOs, SCOOT uses the most loaded 50% of requests for stress testing, where the input and output lengths are presented in Fig. 5. For SQL and BOT, TTFT and TPOT are optimization objectives at the same time. For CLS and REC, 95th percentile latency and request throughput are utilized as optimization objectives, respectively.

Multiple sets of computing resources are utilized in experiments, including **2A10**, **4A10**, **2A100**, and **4A100**, where A10 represents the NVIDIA A10 24GB GPU, A100 denotes the NVIDIA A100 80GB GPU, and the integer before the GPU type indicate the number of GPUs. For each set of computing resources, 256GB CPU memory and one 2.90GHz 32-core Intel(R) Xeon(R) CPU are equipped. Besides, A100 GPUs are connected over NVLink, and A10 GPUs are connected over PCIE. We use LLAMA2-7B and LLAMA2-13B (Pedregosa et al., 2011) in experiments because the model architecture of LLAMA is representative and fine-tuned 7B and 13B LLMs can fulfill the requirements for most applications in practice. We adopt vLLM as the inference engine to tune due to its high usability and popularity.

In each configuration evaluation, when the inference engine is started with the suggested configuration, we send requests from the request trace for 100 seconds and the request arrival times are generated using Poisson distribution with various request rates. For A10, the request rates for SQL, BOT, CLS, and REC are 5,5, 10, and 15, respectively. For A100, the request rates of these request traces are twice that of A10. The request rate is set according to the actual workload of the applications as well as considering the computing capability and GPU memory capacity of GPUs.

We compare SCOOT with three baselines, including random sampling (**RD**), genetic algorithm (**GA**), and Vanilla BO (**VBO**). Detailed baseline description is provided in Appendix A.3.

### SLO Optimization

In the experiments of SLO optimization, we limit the total observation number to 30 and set the suggestion parallelism degree (PD) of SCOOT to one. The experimental results are presented in Figures 6, 7, 8, 9, 10, and 11, where we not only present the SLO improvement under each set of computing resources but also the average SLO improvement across various computing resources. The bar represents the SLO improvement compared to the default parameter configuration, and the **higher the bars, the better**. In addition, \(\star\) represents that no better configuration than the default configuration is found.

For BOT and SQL applications, TTFT and TPOT are optimized simultaneously, and a Pareto frontier is resolved. We choose SLOs

Figure 5.: Probability density function (PDF) of the request input and output lengths for four application request traces.

[MISSING_PAGE_FAIL:7]

intelligently handles hidden constraints without affecting the GP-based surrogate model. Besides, by dynamically adjusting the POF threshold \(\Delta\), SCOOT can explore the region near the infeasible configurations after continuous exploration of feasible configurations so as to fully explore the solution space.

### Ablation Studies

We conduct experiments to confirm the effectiveness of the random forest (RF)-based learning of hidden constraints under 2A100 and 4A100. The average SLO optimization results across various computing resources are presented in Fig. 12. Since requests from the CLS application seldom cause engine crashes, not applying RF-based POF learning has little impact on SCOOT's performance. However, since BOT application requests cause many hidden constraints, RF-based POF learning can greatly enhance SLO optimization.

### Tuning Efficiency

We conduct experiments to tune the CSL and BOT applications under 2A100 with different parallelism degrees (PDs). In the experiments, the number of observations is also limited to 30 when PD is set to 2. Figures 12(a) and 12(b) show the SCOOT's performance in SLO optimization under various PDs, while Fig. 12(c) presents the total tuning time under various PDs.

Figures 12(a) and 12(b) show that when PD is set to 2, using the parallel suggestion achieves the same performance in SLO optimization as not using it. Besides, as presented in Fig. 12(c), when PD is set to 2, the parallel suggestion technique significantly reduces the total tuning time, nearly cutting it in half, which demonstrates that the parallel suggestion can effectively accelerate the tuning process by PD times. Furthermore, Figure 12(c) shows that compared with the time of configuration evaluation (i.e., stress testing), the time required for configuration suggestion is negligible, which validates the efficiency of applying BO to tune LLM inference engines.

We also increase PD and conduct experiments. However, we find that when PD is set to a large value, such as greater than 4, SCOOT's performance of SLO optimization is compromised, and increasing the total observation number can mitigate this issue. In the future, we will explore the best number of observations under different PDs to find the most cost-effective acceleration solution.

### Solution Quality

For multi-objective optimization, we evaluate the quality of solutions from two perspectives. First, the optimality of solutions, that is, whether the optimized parameter configurations lie on the Pareto frontier of TTPT and TPOT. Second, we focus on the diversity of solutions, that is, whether the range of optimized parameter configurations on the Pareto frontier is sufficiently broad to offer customers promising optional configurations to select based on their TTFT and TPOT requirements. We illustrate the resolved Pareto frontier of SCOOT and other baselines in Appendix A.4, which confirms the superiority of SCOOT in both the optimality and diversity of SLO optimization in multi-objective situations.

## 6. Related Works

### Advanced LLM Inference Techniques

Existing inference engines such as vLLM and TensorRT-LLM support a variety of advanced inference techniques to improve the speed and throughput of LLM inference, including a series of kernel-level optimizations such as continuous batching (Chen et al., 2017), paged attention (Chen et al., 2017), flash attention (Chen et al., 2017; Wang et al., 2018), and chunked prefill (Chen et al., 2018). Besides, prefix caching (Wang et al., 2018) is employed by inference engines to reuse computed key and value tensors of requests' common prefix texts to reduce redundant memory allocation and computation for newly arrived requests, which can significantly improve the serving efficiency when requests share a long common prefix. Moreover, speculative decoding (Chen et al., 2017; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018) is also adopted by inference engines to accelerate LLM inference, where an efficient draft model is leveraged to generate tokens, and the LLM occasionally refines the draft.

Some of these technologies are effective in specific scenarios and have been implemented as configurable parameters of inference engines. However, how to combine these technologies to fully exploit the capabilities of inference engines has not been adequately studied, a gap that this study aims to fill.

### Automatic Performance Tuning Approaches

Performance tuning techniques have been widely applied across a wide variety of fields, including database tuning (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018), Spark configuration tuning (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018), compiler optimization (Wang et al., 2018), and tuning of web-relevant applications (Wang et al., 2018; Wang et al., 2018). The vast majority of these performance tuning studies adopt BO to find optimized parameter configurations since BO is theoretically grounded and can efficiently learn the relationship between performance and parameters from evaluations, thus intelligently tuning parameters for performance improvement. Because of these advantages, BO is also employed in resource allocation studies (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018).

To the best of our knowledge, previous studies have not focused on tuning LLM inference engines and can not address all the three unique challenges faced in inference engine tuning.

## 7. Conclusion and Future Works

In this paper, we propose SCOOT, a BO-based automatic inference engine tuning system, to optimize SLOs for LLM inference services. SCOOT leverages both SOBO and MOBO to handle various optimization objectives and utilizes random forest regression to learn hidden constraints during the tuning process. Additionally, SCOOT exploits the parallel suggestion to accelerate tuning with additional computing resources. Experimental results show that SCOOT can effectively speed up tuning and significantly optimize SLOs, improving the request throughput by up to 68.3%, reducing the request tail latency by up to 40.6%, and reducing the TTFT and TPOT by up to 99.8% and 61.0%, respectively. In the future, we will attempt to utilize meta-learning approaches to further accelerate the tuning process for new LLM inference services using the knowledge previously learned from tuning other inference services.

Figure 12. Tuning efficiency of the parallel suggestion.

## References

* (1)
* (2) Bonan Min, Hayley Ross, Eller Salem, et al. Recent advances in natural language processing via large pre-trained language models: A survey. _ACM Computing Surveys_, 56(2):40-420, 2023.
* (3) Alisha cloud bailium platform. [https://www.asylum.com/product/bailian](https://www.asylum.com/product/bailian), 2024.
* (4) says sagmenter. [https://aws.amazon.com/sagemaker/](https://aws.amazon.com/sagemaker/), 2024.
* (5) vilm. [https://github.com/vilm-project/vilm](https://github.com/vilm-project/vilm), 2024.
* (6) Tensorflow11m. [https://github.com/vilm/TensorFlow-FLAM](https://github.com/vilm/TensorFlow-FLAM), 2024.
* (7) Gyepne G-In-V, Joao Sompong, Geoo-Woo Kim, et al. Occa: A distributed serving system for transformer-based generative models. In _10th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)_, pages 521-538, 2022.
* (8) Woosuk Karou, Zhaohan Li, Styun Zhuang, et al. Efficient memory management for large language model serving with applications. In _Proceedings of the 29th Symposium on Operating Systems Principles_, pages 611-626, 2023.
* (9) Amey Agrawal, Nikita Kedia, Ashish Panwar, Jayashine Mohan, et al. Taming throughput-latency tradeoff in l1m inference with spatial-serve. In _18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)_, pages 117-134, 2024.
* (10) Fredoolds Herodotou, Yuxing Chen, and Jiaheng Lu. A survey on automatic parameter tuning for big data processing systems. _ACM Computing Surveys_, 5((3):1-37, 2020).
* (11) Tito Homme de Melo and Guin Bayrakan. Monte carlo sampling-based methods for stochastic optimization. Surveys in Operations Research and Management Science, 19(1):56-85, 2014.
* (12) Soumah Kotech, Sumit Singh Chauhan, and Vijay Kumar. A review on genetic algorithm past, present and future. _Multimedia tools and applications_, pp. 880:8126, 2021.
* (13) Shihu Padakandla. A survey of reinforcement learning algorithms for dynamically varying environments. _ACM Computing Surveys_ (CSUR), 54(6):1-25, 2021.
* (14) Xiku Wang, Yaochu Jin, Sebastian Schmitt, and Markus Ollhofer. Recent advances in bayesian optimization. _ACM Computing Surveys_, 55(1):1-36, 2023.
* (15) Naman Khan, David E Goldberg, and Martin Pelizan. Multi-objective bayesian optimization algorithm. In _Proceedings of the 4th Annual Conference on Genetic and Evolutionary Computation_, pages 648-644, 2002.
* (16) Kaifeng Yang, Michael Esmarchi, Andre Dett, and Thomas Black. Multi-objective bayesian global optimization using expected hypercube improvement gradient. _Soomr and evolutionary computation_, 44:454-456, 2019.
* (17) Samuel Daubton, Maximilian Balanduk, and Eytan Rakicby. Differentiable expected hypervolume improvement for parallel multi-objective bayesian optimization. _Advances in Neural Information Processing Systems_, 38:981-9864, 2020.
* (18) Yann Levitathan, Marian Kalainen, and Yosi Matias. Fast inference from transformers via speculative modeling. In _International Conference on Machine Learning_, pages 19274-19286, PMLR, 2023.
* (19) Russell T Calkins. Monte carlo and quasi-monte carlo methods. _Acta numerica_, 7:4-19, 1998.
* (20) Jasper Spacek, Kevin Swersky, Rick Zemel, and Ryan Adams. Input warping for bayesian optimization of non-stationary functions. In _International conference on machine learning_, pages 1674-1682. PMLR, 2014.
* (21) Matthias Seger. Gaussian processes for machine learning. _International journal of neural systems_, 4(12):649-106, 2004.
* (22) Alexander I. Coureux-Rivers, Wenlong Lyu, Rasul Tutunov, et al. Ebbot: Pushing the limits of sample-efficient hyper-parameter optimization. _Journal of Artificial Intelligence Research_, 7:1269-1349, 2022.
* (23) Shulan Zhang, Yang Yang, Chang Li, et al. An efficient batch-constrained bayesian optimization approach for analog circuit synthesis via multiobjective acquisition ensemble. _IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems_, 41(1):1-24, 2021.
* (24) Niranjan Srinivas, Andreas Krause, Shum Kakade, and Matthias Seger. Gaussian process optimization in the bandit setting to no regret and experimental design. In _Proceedings of the 27th International Conference on Machine Learning_, pages 1915-1022, 2010.
* (25) Erik Onm Hellsten, Artur Souza, Johannes Lenfers, et al. Bacc: A fast and portable bayesian compiler optimization framework. In _Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems_, Volume 4, pages 197-142, 2023.
* (26) Fabian Pedregosa, Gali Varoquaux, Alexandre Gramfort, et al. Scikit-learn: Machine learning in python. _The Journal of machine learning research_, 1:2825-2830, 2011.
* (27) Hong Tuorun, Louis Martin, Kevin Stone, et al. Lluma: 2: Open foundation and fine-tuned chat models. 2023.
* (28) Tien Dan, Tom Pu, Stefano Ermon, et al. Flashattention: Fast and memory-efficient exact attention with so-awareness. _Advances in Neural Information Processing Systems_, 35:16344-16359, 2022.
* (29) Tiu Das. Flashattention-2: Faster attention with better parallelism and work partitioning. In _The Twelfth International Conference on Learning Representations_, 2023.
* (30) Nikhil Jha and Kevin Wang. Improving large language model throughput with efficient long-term memory management.
* (31) Heming Xia, Tao Ge, Peiyu Wang, et al. Speculative decoding: Exploiting speculative execution for accelerating ecn/2seq generation. In _Findings of the Association for Computational Linguistics: EMNLP 2023_, pages 3909- 3925, 2023.
* (32) Xupeng Miao, Gabriele Olanz, Zhihao Zhang, et al. Specifier: Accelerating large language model serving with tree-based speculative inference and verification. In _Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems_, Volume 3, pages 932-949, 2024.
* (33) Schoon Kim, Kartikiey Mangalam, Suhong Moon, et al. Speculative decoding with big little decoder. _Advances in Neural Information Processing Systems_, 36, 2024.
* (34) Zieng Sun, Amanda Theertha Suresh, Jae Hun Ro, et al. Spectr: Fast speculative decoding via optimal transport. _Advances in Neural Information Processing Systems_, 36, 2024.
* (35) Xinyi Zhang, Hong Wu, Zhao Chang, et al. Returne: Resource oriented tuning based by model-learning for cloud databases. In _Proceedings of the 2021 international conference on management of data_, pages 1202-2114, 2021.
* (36) Jale Luo, Yibo Wang, Yufei Li, et al. Optranet: A manual-reading database tuning system via gpt-guided bayesian optimization. 2023.
* (37) Tatumo Yang, Wen Liu, Wangjen Peng, et al. Volunteer: Automated performance tuning for vector data management systems. In _2024 20th International Conference on Data Engineering (ICDE)_, pages 4357-4369, 2024.
* (38) Anastasio Gouranis, Georgia Kroga, and Raben others Tous. Dynamic configuration of partitioning in spark applications. _IEEE Transactions on Parallel and Distributed Systems_, 27(18):1931-1940, 2021.
* (39) Juri Kelsey, Lucian Caruia, Thomas Pasquiger, et al. To tune or not to tune? in search of optimal configurations for data analytics. In _Proceedings of the 28th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, pages 2494-2504, 2020.
* (40) Yang Li, Huaijun Jiang, Yu Shen, et al. Towards general and efficient online training for spark. _Proceedings of the VLDB Endowment_, 16(12):3570-3583, 2023.
* (41) Yu Shen, Xinyang Ren, Yupeng, Li Hanjun Zhang, Huaijun Jiang, Yu Ding, Yang Li, Wentao Zhang, and Bin Cui. Rover: An online spark still running service via generalized transfer learning. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, pages 6800-6812, 2023.
* (42) Valentin Dalibard, Michael Schaeuchmidt, and Alex Yoskoi. Bost: Building auto-tunes with structured bayesian optimization. In _Proceedings of the 26th International Conference on World Wide Web_, pages 479-488, 2017.
* (43) Dan Li and Evangelos Karoubas. Bayesian optimization for optimizing retrieval systems. In _Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining_, pages 360-368, 2018.
* (44) Tirthak Patel and Devvas Triant. Efficient and on-aware co-located of multiple latency-critical jobs for warehouse sales computers. In _2020 IEEE International Symposium on High Performance Computer Architecture (HPCA)_, pages 193-206. IEEE, 2020.
* (45) Rohan Rao, Tyrutuk Patel, and Devvas Triant. Sutor: efficient and far resource partitioning by sacrificing short-term benefits for long-term gains. In _2021 ACM 21st 28th Annual International Symposium on Computer Architecture (ISCA)_, pages 292-305. IEEE, 2021.
* (46) Tianquo Wang, Roching Chen, Yuen Li, Xiaoguang Liu, and Gang Wang. Co-tuner: A hierarchical learning framework for coordinately optimizing resource partitioning and parameter tuning. In _Proceedings of the 52nd International Conference on Parallel Processing_, pages 317-326, 2023.
* (47) Chao Zhang, Shiwei Wu, Haoxin Zhang, Tong Xu, Yan Gao, Yue Hu, and Enhong Chen. Notellam: A retiemble large language model for reinforcement. In _Companion Prohibition of the ACM on Web Conference_, 2022, pages 170-179, 2024.
* (48) Xubin Ren, Wei Liang Xiao, Lixin Sun, Siu Cheng, Junfeng Wang, Dawei Yin, and Chao Huang. Representation learning with large language models for recommendation. In _Proceedings of the ACM on Web Conference_, pages 3464-3475, 2024.
* (49) Prateek Sanchezti, Kamalakar Karalpabel, and Kavira Vemuri. Llhu driven web profile extraction for identical machines. In _Companion Proedings of the ACM on Web Conference_, 2020, pages 1616-1625, 2024.
* (50) Xueguang Ma, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. Fine-tuning l1m for multi-stage text retrieval. In _Proceedings of the 7th International ACM SIGIR Conference on Research and Development in Information Retrieval_, pages 2421-2425, 2024.
* (51) Marc Selmack, Nari Nakashima, and Stephen Bataill. Genetic algorithms for mixed discrete/continuous optimization in multidisciplinary design. In _7th AIAA/SIG/IASSPM Symposium on Multidisciplinary Analysis and Optimization_, page 4771, 1998.
* (52) Kashymany Deb, Amiri Pratap, Sameer Agrawal, and TAMT Meyarivan. A fast and difficult multiobjective genetic algorithm: Ngo-ii. _IEEE transactions on evolutionary computation_, 6(2):182-197, 2002.

[MISSING_PAGE_FAIL:10]