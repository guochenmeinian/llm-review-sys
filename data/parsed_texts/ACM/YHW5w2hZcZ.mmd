Sherlock: Towards Multi-scene Video Abnormal Event Extraction and Localization via a Global-local Spatial-sensitive LLM

Anonymous Author(s)

###### Abstract.

In the literature, prior studies on Video Anomaly Detection (VAD) mainly focus on detecting whether each video frame is abnormal or not in the video, which largely ignore the structured video semantic information (i.e., what, when, and where does the abnormal event happen), though this structured information could be employed to construct a more precise and efficient system for abnormal event monitoring and retrieval. With this in mind, we propose a new chat-paradigm **M**ulti-scene **V**ideo **A**honormal **E**vent Extraction and Localization (M-VAE) task, aiming to extract the abnormal event quadruples (i.e., subject, event type, object, scene) and localize such event. Further, this paper believes that this new task faces two key challenges, i.e., global-local spatial modeling and global-local spatial balancing. To this end, this paper proposes a Global-local Spatial-sensitive Large Language Model (LLM) named Sherlock, i.e., acting like _Sherlock Holmes_ to track down the criminal events, for his M-VAE task. Specifically, this model designs a Global-local Spatial-enhanced MoE (GSM) module and a Spatial Imbalance Regulator (SIR) to address the above two challenges respectively. Extensive experiments on our constructed M-VAE instruction dataset show the significant advantages of Sherlock over several advanced Video-LLMs. This justifies the importance of global-local spatial information for the M-VAE task and the effectiveness of Sherlock in capturing such information.

Multi-scene Video, Video Abnormal Event, Spatial-sensitive LLM 1
Footnote 1: Relevance to the Web: M-VAE task belongs to _Development of structured data_ topic of _Semantic and Knowledge_ track, which aims to extract and locate quadruples from web videos (like YouTube and Tik Tok), making web content more accessible through video quick retrieval.

## 1. Introduction

Video Understanding is a foundational task in artificial intelligence, which focuses on analyzing and interpreting the content of videos to enable various applications, including video classification, activity recognition, and scene understanding [42, 66, 67]. As a critical branch of video understanding, **Video Anomaly Detection** (VAD) [22], which aims to automatically detect abnormal videos, has garnered significant research attention due to its wide range of applications in criminal activity detection and disaster response [63]. Prior studies on VAD mainly focus on detecting whether each video frame is abnormal or not in the video [22, 31, 43, 63]. However, these studies overlook targeting at determining the underlying video semantic structure, i.e., "_what is the abnormal type, where they have occurred, which people or things are involved_" with a given video.

Motivated by these, this paper proposes a novel **M**ulti-scene **V**ideo **A**hormal **E**vent **E**xtraction and **Localization** (M-VAE) task1, aiming at localizing abnormal events (i.e., starting and ending times of the anomaly) and extracting event quadruples (i.e. [subject of the event, event type, object of the event, scene of the event]) through a chat paradigm. Take an example of _Street_ scene in Figure 1 (a), within 23s to 25s, a man bends down and pries the lock, then drives away from the street and the abnormal event quadruple is [_people_, _steal_, _car_, _street_]. Different scene (i.e., Residence scene) is also shown in Figure 1 (b). Within 15s to 17s, a man vandalizes a sculpture at one's residence and the quadruple is [_people_, _Vandalism_, _Sculpture_, _Residence_]. This structured processing for abnormal videos can significantly improve the practicality and efficiency of video anomaly localization systems. In fields such as real-time abnormal event monitoring that require high reliability and precision monitoring, using such structured processing can quickly search and screen for

Figure 1. (a) and (b) illustrate two surveillance video examples for our M-VAE task and Sherlock model in two scenes (**Street** and **Residence**). Sherlock precisely generates the abnormal event quadruples and their corresponding timestamps. (c) presents a circular ratio diagram illustrating different spatial information. From (c), we observe that the global spatial information and the local spatial information (i.e., action, object relation, and background) in our M-VAE dataset are imbalanced.

the required abnormal elements, which provides more convenient and intuitive evidence for further processing. Therefore, it is worthwhile to address this new task. Nevertheless, we believe that this new task faces two key challenges.

For one thing, it is challenging to model the global-local spatial information (named global-local spatial modeling challenge). Existing video understanding models (Sanchez et al., 2016; Wang et al., 2017; Wang et al., 2018) mainly focus on modeling general global information. However, local spatial information in our M-VAE task is often crucial compared to general global information, which are highly discriminative and essential for precise identification. Taking Figure 1 (a) as an example, the local spatial information, such as action (bend down), object relations (\(<\)man, near, car-), and background (street), can help better identify abnormal events. However, those local spatial information (e.g., actions, object relations, backgrounds) have different heterogeneous representations (i.e., different model structures and encoders). Therefore, a single, fixed-capacity transformer-based model, often makes it difficult to capture those critical local spatial information in videos. Recently, the Mixture of Expert (MoE) (Wang et al., 2017; Wang et al., 2018) paradigm has demonstrated scalability in multi-modal heterogeneous representation fusion tasks (Wang et al., 2017; Wang et al., 2018; Wang et al., 2018). Inspired by this, a well-behaved model for our task should adopt the MoE paradigm to not only consider global spatial information but also emphasize the importance of local spatial information.

For another, a straightforward approach is to employ a basic Mixture of Expert (MoE) mechanism (Wang et al., 2017; Wang et al., 2018; Wang et al., 2018) to treat global spatial information (i.e., general representations of videos) and local spatial information (e.g., actions) as the global expert and local experts for integrating those information. However, the data imbalance issue among local spatial information may lead to the basic MoE experts being biased towards the more frequently occurring spatial information in the dataset. The statistics in Figure 1 (c) can illustrate this imbalance. Certain frequently appearing local information (i.e., action at 45%), can lead to higher weight for the corresponding expert. However, in Figure 1 (a), the object relations information, with the smallest proportion (25%), but is the most discriminative for extracting and localizing _Theft_ events. More seriously, global spatial information is the most frequent and our preliminary experiments in Figure 7 (a) reveal global expert is often more thoroughly trained and often have the highest weights. Therefore, a better-behaved MoE expert fusion mechanism should mitigate this data imbalance (named global-local spatial balancing challenge), ensuring all experts are sufficiently trained to highlight their importance.

To tackle above challenges, we propose a Global-local Spatial-sensitive LLM named Sherlock, i.e., acting like _Sherlock Holmes_ to track down criminal events, for M-VAE. Specifically, this model designs a Global-local Spatial-enhanced MoE (GSM) module to address the global-local spatial modeling challenge, which includes four spatial experts to extract spatial information and an expert gate to weigh global and local spatial information. Furthermore, this model designs a Spatial Imbalance Regulator (SIR) to address the global-local spatial balancing challenge, which includes a Gated Spatial Balancing Loss (GSB) to further balance global and local experts. Particularly, we construct a M-VAE instruction dataset to better evaluate the effectiveness of our model. Detailed experiments show Sherlock can effectively extract and localize abnormal events and surpass advanced Video-LLMs in multiple evaluation metrics.

## 2. Related Work

\(\bullet\)**Video Anomaly Detection.** Video Understanding is a rapidly evolving research field which encompasses several tasks, including video grounding (Wang et al., 2017; Wang et al., 2018; Wang et al., 2018), spatial-temporal detection (Wang et al., 2018) and so on. As an important branch of video understanding, previous studies on Video Anomaly Detection (VAD) can be categorized into unsupervised, weakly-supervised, and fully-supervised categories. Unsupervised approaches focus on leveraging reconstruction techniques to identify anomalies (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018). Weakly-supervised methods have shown promising results in identifying abnormal frames (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018). Fully-supervised methods are scarce due to the expensive frame-level annotations required (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018). Different from the above studies, our Sherlock model aims to target at determining the underlying video semantic structure, providing a structured quadruple that goes beyond previous methods, facilitating the rapid detection and early warning of abnormal events in real-time.

\(\bullet\)**Event Extraction** (EE) focuses on extracting structured information from given types of information. Traditional EE methods mainly extract from text documents (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018). Recently, many studies (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018) generate similar event structures from visual image data. Different from all the above studies, we are the first to focus on extracting the abnormal event from videos and constructing a quadruple dataset, incorporating information from multiple spatial information, enriching the task of event extraction, and making it more practical for real-world applications.

\(\bullet\)**Scene Recognition** is a fundamental task applied in remote sensing (Wang et al., 2018; Wang et al., 2018) and autonomous driving (Wang et al., 2018). Traditional methods rely on hand-crafted features for extracting visual attributes (Wang et al., 2018; Wang et al., 2018). Recently, ARCNet (Wang et al., 2018) and CapsNet (Wang et al., 2018) reinforcement, aim to locate important regions. Others, like using CapsNet in (Chen et al., 2018) and FACNN (Wang et al., 2018), focus on modeling global context. SCViT (Wang et al., 2018) and KPR combine fine-grained information. Recently, many studies (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018) utilize LLMs to solve the illusion problem. Different from the above studies, we introduce scene classification into our M-VAE task and integrate scenes into event quadruples, greatly improving the applicability of our M-VAE task in the real world.

\(\bullet\)**Video-oriented Large Language Models.** The rise of Chat-GPT (Chen et al., 2018) has stimulated the prosperity of Video Large Language Models which can be categorized into four major types: firstly, Video Chat (Sanchez et al., 2016) and Video LLaMA (Wang et al., 2018), which utilize BLIP-2 (Wang et al., 2018) and Q-Former to map visual representations onto Vicuna; secondly, models like Video ChatGPT (Wang et al., 2018), Otter (Wang et al., 2018), Valley (Wang et al., 2018), mPLUG-Owl (Wang et al., 2018), and Chat-UniVi (Wang et al., 2018), which leverage CLIP (Wang et al., 2018) to encode visual features; thirdly, PandapT (Chen et al., 2018), which adopts Image-Bind (Wang et al., 2018) as its core architecture for video understanding; and fourthly, VideoLLaVA (Wang et al., 2018), which aligns image and video features into a linguistic feature space using LanguageBind (Wang et al., 2018). Recently, a few studies (Wang et al., 2018; Wang et al., 2018) consider incorporating spatial information in models. Besides, some studies (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018) introduce the concept of MoE into LLMs, but they only focus on efficiency, without considering the balance between different information. Different from all the above studies, we design a new Sherlock model, to address our M-VAE task, which includes a Global-local Spatial-enhanced MoE module and a Spatial Imbalance Regulator to address the challenges of global-local modeling and balancing.

## 3. Our Sherlock Model

In this paper, we propose a Sherlock model to address the M-VAE task. Figure 2 illustrates the framework of Sherlock, which is composed of two core components (i.e., the Global-local Spatial-enhanced MoE (GSM) module (sec 3.1) for the global-local spatial modeling challenge and the Spatial Imbalance Regulator (SIR) (sec 3.2) for the global and local spatial balancing challenge). Subsequently, we present our training strategies to enhance the ability of understanding spatial information (sec 3.3).

**Backbone**. We choose Video-L1aVA2(Huang et al., 2019) and its visual encoder LanguageBind(Wang et al., 2019) as the core framework. Video-L1aVA, which is optimized with a mixed dataset of images and videos, demonstrates leading performance across most image and video benchmarks. We employ Video-L1aVA as the backbone to explore the potential of Video-L1a in extracting and localizing abnormal events.

Footnote 2: [https://github.com/PKU-YuanGroupVideo-L1aVA.git](https://github.com/PKU-YuanGroupVideo-L1aVA.git)

**Task Formulation.** Given a video \(V\) for \(M\) frames, each frame is labeled with 1 or 0, where 1 and 0 represent whether this frame conveys an abnormal event. The goal of M-VAE is to interactively generate the quadruple (_sub_, _type_, _obj_, _see_) for each event along with the corresponding timestamp \(sta\) and \(end\), where \(sub\), \(type\), \(obj\), \(see\), \(sta\) and \(end\) are the subject, event type, object, scene, start time and end time of the abnormal event. As shown in Figure 1 (a), a man steals a car at street from 23s to 25s. Therefore, the output of our M-VAE task is {23s, 25s, (_people, steal, car, street_}).

### Global-local Spatial-enhanced MoE Module

As shown in Figure 2, we design a Global-local Spatial-enhanced MoE (GSM) Module for the global-local spatial modeling challenge. Inspired by Mixture-of-Experts (MoE) (MoE, 2019), we design three Local Spatial Experts (i.e., Local Action Expert, Local Object Relation Expert and Local Background Expert) and a Global Spatial Expert to extract spatial information, detailed as follows.

**Local Spatial Experts** contain three local spatial experts (i.e., action, object relation, and background), detailed as follows.

\(\bullet\)**Local Action Expert (Action Expert, AE)**. We leverage HigherHRNet(Wang et al., 2019), a well-adopted bottom-up human pose estimation network to extract local spatial action information. HigherHRNet can generate local spatial action tokens \(\mathbf{T_{a}}=(t_{a}^{\mathbf{r}},...,t_{a}^{\mathbf{r}},...,t_{m}^{\mathbf{r }})\), and each token consists of 17 human joint nodes for each individual in every frame of a video sequence. Here, \(t\) denotes the \(i\)-th frame. Next, we apply Action Graph Attention to integrate \(\mathbf{T_{a}}\) with the video tokens \(\mathbf{T_{o}}=\{t_{a}^{\mathbf{r}},...,t_{a}^{\mathbf{r}},...,t_{m}^{\mathbf{r }}\}\) generated by the Video Encoder in Video-L1aVs. We start by calculating the attention weights \(\alpha_{kj}\) for each node \(e_{k}\) in \(t_{i}^{\mathbf{r}}\) relative to its neighboring node \(e_{j}\):

\[\boldsymbol{\alpha_{kj}}=\text{softmax}\left(\frac{(\mathbf{W_{a}}\mathbf{h_{k }})\cdot(\mathbf{W_{a}}\mathbf{h_{j}})}{\sqrt{d}}\right) \tag{1}\]

where \(h_{k}\) and \(h_{j}\) is the features of \(e_{k}\) and \(e_{j}\) respectively. \(\mathbf{W_{a}}\) denote the learnable weight matrix, and \(d\) is the feature dimension. Then we aggregate the feature \(\tilde{h}_{k}\) of node \(e_{k}\): \(\tilde{h}_{k}=\sum_{j\in\mathcal{N}(e_{k})}\alpha_{kj}\cdot h_{j}\), where \(\mathcal{N}(e_{k})\) is the neighboring nodes of \(e_{k}\). Finally the feature of

Figure 2. The overall framework of Sherlock. It consists of a Global-local Spatial-enhanced MoE (GSM) Module and a Spatial Imbalance Regulator (SIR). The SIR exerts a direct influence on the output weights of the expert gate. W SIR or W/o SIR means with or without Spatial Imbalance Regulator.

\(e_{k}\) is calculated by \(h_{k}^{\prime}=\text{ReLU}(\mathbf{W_{k}}[\hat{h}_{k},h_{k}])\), where \(\mathbf{W_{a}}\) donates the weight matrix and \([\hat{h}_{k},h_{k}]\) is the concatenation of \(\hat{h}_{k}\) and \(h_{k}\).

After graph attention operation, we enhance \(\mathbf{T_{a}}\) using the attention mechanism with query \(\mathbf{Q}_{w}\), key \(\mathbf{K_{a}}\), and value \(\mathbf{V_{a}}\) calculation to obtain final action tokens: \(\mathbf{T_{a}^{\prime}}=\text{softmax}(\mathbf{Q}_{w}^{T}\cdot\mathbf{K_{a}}) \cdot\mathbf{V_{a}}\).

\(\bullet\)**Local Object Relation Expert (Object Relation Expert, ORE)**. We leverage ReITR (Kumar et al., 2017), a well-studied one-stage object relation graph generation method to extract local spatial object relation information. ReITR can generate an object relation token \(t_{t}^{\theta}=(R_{i},E_{i})\), which represents the object relation graph of the \(i\)-th frame. Here, \(R_{i}=\left(\left\{e_{i,1},b_{i,1}\right\},...\left(e_{i,k},b_{i,k}\right\}\right)\) is a set of \(k\) detected objects, with class e and corresponding bounding box \(b\). The set \(E_{i}=\left\{e_{i,p},r_{i,(pq)},c_{i,q}\right\}\) consists of the directed edges in the graph, representing two directional edges from \(c_{i,p}\) to \(r_{i,(p,q)}\) and from \(r_{i,(p,q)}\) to \(c_{i,q}\), where \(r_{i,(p,q)}\) denotes a relationship category. For example, an object might be represented as (_man_, \(\leftarrow\)_0.36, 0.24, 0.75, 1.625), and an edge as (_man_, _near_, _car_). Subsequently, we apply object-aware masking with Masked Graph Transformer Networks (MaskGTN) to fully utilize object relations. We mask irrelevant object parts based on the bounding box information, and aggregate information from neighbors using a graph transformer layer (GT). Given an input graph of region classes and edges, MaskGTN computes updated vectors for each region and edge. Assuming we use \(L\) layers of GT, with \(\mathbf{H^{(\ell)}}\) representing the features of the \(\ell\)-th layer, the final forward propagation is defined as follows:

\[\mathbf{H^{(\ell+1)}}=\sigma\left(\sqrt{\tilde{\mathbf{D}}}\cdot\tilde{\mathbf{ A}}\cdot\sqrt{\tilde{\mathbf{D}}}\cdot\mathbf{H^{(\ell)}}\cdot\mathbf{W^{(\ell)}}\right) \tag{2}\]

where \(\sigma\) is the activation function on the graph. \(\tilde{\mathbf{A}}\) is the adjacency matrix of the object-relation graph, derived from \(E_{i}\), and \(\tilde{\mathbf{D}}\) is its degree matrix, with \(\tilde{\mathbf{D}}_{ii}=\sum_{i}\tilde{\mathbf{A}}_{ij}\). \(\mathbf{W^{(\ell)}}\) is a trainable weight matrix.

\(\bullet\)**Local Background Expert (Background Expert, BE)**. We leverage SAM2 (Shen et al., 2017), an advanced model for visual segmentation, to extract local spatial background information from videos. SAM2 can generate a background image for each frame of video. Then we leverage InternVit (Chen et al., 2018) to encode local spatial background information which is a large vision encoder extending the parameters of vision transformer (VIT) (Chen et al., 2018) to \(\Theta\), formally represented as:

\[\mathbf{T_{b}}=\text{InternVit}\left(\text{SAM2}\left(v_{i}\right)\right) \tag{3}\]

where \(v_{i}\) is the \(i\)-th frame of video \(V\). This process results in the local spatial background tokens \(\mathbf{T_{b}}=\{t_{1}^{\theta},...,t_{b}^{\theta},...,t_{m}^{\theta}\}\) for the entire video sequence, with \(n\) representing the total number of frames.

**Global Spatial Expert** has a comprehensive understanding of the training data. Collaborate with local spatial experts to bring specialization and generalization capabilities to M-VAE tasks.

\(\bullet\)**Global Spatial Expert (Global Expert, GE)**. The weight assigned to the global spatial expert complements that of the local spatial experts. Consequently, the local spatial experts acquire specialized skills for specific tasks, whereas the global spatial expert develops a comprehensive understanding of the entire training corpus. The collaboration between these two types of experts provides both specialization and generalization for our M-VAE task. In this way, we leverage LanguageBind (Yang et al., 2019) in Video-LLaVA (Yang et al., 2019), which inherits the ViT-L/14 structure from CLIP and as equipped with powerful and universal visual encoding capabilities to extract global spatial information for our task. We subsequently leverage a pre-trained FFN layer by (Yang et al., 2019) to align the dimension with other spatial information, formally represented as:

\[\mathbf{T_{\boldsymbol{g}}}=\text{FFN}\left(\text{LanguageBind }\left(v_{i}\right)\right) \tag{4}\]

where \(v_{i}\) is the \(i\)-th frame of video \(V\). This process yields the full set of global tokens \(\mathbf{T_{\boldsymbol{g}}}=\{t_{1}^{\theta},...,t_{m}^{\theta},...,t_{m}^{ \theta}\}\) for the entire video sequence, with \(n\) representing the total number of frames.

After designing four experts, we ensure that the four Spatial Experts can dynamically adjust the weights of the four heterogeneous types of spatial information inspired by Mixture-of-Experts (MoE) (Shen et al., 2017). As shown in Figure 2, unlike methods that embed several FFNs within LLMs, our GSM put four experts outside the LLMs to adjust weights for global and local spatial information. Based on this, we introduce a dynamic Expert Gate (EG) (Shen et al., 2017), which controls the contribution of each expert by calculating gating weights as a soft gate. Finally, the output \(\mathbf{O}\) of GSM, based on four spatial experts and EG, is formally represented as:

\[\boldsymbol{g}=\text{softmax}\left(\mathbf{W_{g}}\cdot\sum_{i=1}^{N}\left( \mathbf{S_{i}}\right)\right) \tag{5}\]

\[\mathbf{O}=\text{LayerNorm}\left(\sum_{i=1}^{N}\left(g_{i}\cdot\mathbf{S_{i}} \right)\right) \tag{6}\]

where LayerNorm (\(\cdot\)) indicates layer normalization (Chen et al., 2018). \(g_{i}\) (the \(i\)-th entry in \(\boldsymbol{g}\)) represents the weight of the \(i\)-th expert. \(\mathbf{S_{i}}\) represents the outputs of the \(i\)-th Spatial expert. \(N\) is the total number of spatial expert, and \(\mathbf{W_{g}}\) being the trainable weight matrix.

### Spatial Imbalance Regulator

After modeling the spatial information, we design a Spatial Imbalance Regulator (SIR) including a Gated Spatial Balancing Loss (GSB) for the global-local spatial balancing challenge, detailed as follows.

**Gated Spatial Balancing (GSB) Loss**. Previous researches employ a basic Mixture of Experts (MoE) (Shen et al., 2017; Chen et al., 2018) to model global and local spatial information. When faced with an imbalance between these two types of information, the weights assigned to experts tend to be biased toward those that appear more frequently. As shown in Figure 1 (c), there are the most spatial elements (_e.g._, _People_) related to local spatial action information in event quadruple. This implies that performance will deteriorate when faced with real-world data that is not processed by an action expert (_e.g._, _object relations_). More seriously, as shown in Figure 1 (c), global information holds significant weight in all data, which will lead to excessive training of global experts and weaken the abilities of local experts with lower weights. This imbalance phenomenon will greatly affect the performance of our model. Based on this, we should keep the weights of all spatial experts not too different and achieve the optimal state of relative balance where every expert is fully trained. Inspired by MoELORA (Yang et al., 2019), we propose a Gated Spatial Balancing (GSB) Loss to balance spatial weights, as follows:

\[\mathcal{L}_{\text{gate}}=\left(\frac{1}{N_{\text{local}}}\sum_{i=1}^{N_{\text{ local}}}-\log\left(g_{i}\right)\right)-\log\left(g_{\text{global}}\right) \tag{7}\]where \(N_{\text{local}}\) is the number of local expert. \(g_{\text{global}}\) is the weight of global expert. The first term of Eq.(7) is balancing between local experts, and the second term is balancing between local and global experts. The weights of four experts have already balanced when the loss is optimized to a minimum. This regulation achieves a better balance among all experts, reducing the impact of data imbalance, which effectively addresses the global-local balancing challenge. Finally, the overall loss of Sherlock can be represented as:

\[\mathcal{L}=\mathcal{L}_{\mathcal{D}}+\alpha*\mathcal{L}_{\text{gate}} \tag{8}\]

where \(\alpha\) is the hyper-parameter that controls the strength of \(\mathcal{L}_{\text{gate}}\), and \(\mathcal{L}_{\mathcal{D}}\) is the next-token prediction loss of Video-LLMs.

### Training Strategies for Sherlock

In order to enhance the ability of understanding spatial information, we design a two-stage training process. Stage 1 is to enhance the ability of understanding spatial information and Stage 2 is to address the M-VAE task, detailed as follows.

**Stage 1. Pre-Tuning for spatial understanding.** As shown in Figure 2, we first pre-tune Video-LLaV using four high-quality datasets. We aim for Video-LLaVA to have a good spatial understanding ability. Specifically, we selected four high-quality datasets: HumanML3D (Huang et al., 2019), Ref-I4 (Wang et al., 2019), RSI-CB (Wang et al., 2019), and COCO-Caption (Wang et al., 2019), as described in sec 4.1. For each pre-tuning dataset, we enable this dataset to understand corresponding spatial information.

**Stage 2. Instruction Tuning for M-VAE task.** We aim to enable the model to localize abnormal events and extract quadruples through the chat paradigm. We construct an instruction tuning dataset described in sec 4.1 and instruct the pre-tuned Video-LLaVA to _Extract quadruples and localize abnormal events. The quadruple includes subject, event type, object, and scene in abnormal events_. The instruction will undergo text embedding to obtain the textual tokens \(\mathbf{T}_{\mathbf{t}}\). Finally, the input of the LLM is "**O** from Eq.(5) + \(\mathbf{T}_{\mathbf{t}}\)".

## 4. Experimental Settings

### Instruction Data Construction

The training pipeline of Sherlock contains two stages. As shown in Figure 3, for each stage, we construct the corresponding instruction dataset for better tuning.

**For Stage 1.** We construct a special understanding dataset based on Ref-I4 (Chen et al., 2019), HumanML3D (Huang et al., 2019), RSI-CB (Wang et al., 2019) and COCO (Wang et al., 2019). Specifically, we manually design an instruction for each type of spatial information, for instance: **Instruction**: "_Judge the action of the characters in the image. Describe the image region_-_objs in the image_. _Judge the background of the image. Describe the image_". As HumanML3D has 25K videos with an average duration of 1 second, and we take 8 frames per second. For the data balance, we randomly select 20K images or frames from each dataset.

**For Stage 2.** We construct an M-VAE instruction dataset based on CUVA (Huang et al., 2019), which primarily consists of surveillance videos, with an average duration of **80** seconds per video. As this dataset includes five detailed video Q-A tasks (i.e., timestamp, classification, reason, result, and description tasks), it is highly beneficial for constructing our M-VAE dataset. **1)** For abnormal event quadruples, constructing quadruples involves two steps. **First**, we collect answers from the reason, result, and description tasks in CUVA for each video. Subsequently, we construct initial quadruples through ChatGPT (Wang et al., 2019) based on the answers to these tasks, with the instruction: "_Please extract the subject, object, and scene of the event based on the responses below_". **Second**, we create multiple candidate sets for subjects, objects, and scenes in quadruple. Specifically, **for subjects and objects elements**, we manually construct a set of around 40 for subjects and objects and filter elements based on this set. **For event types elements**, we adopt the 11 categories (i.e., Fighting, Animals, Water, Vandalism, Accidents, Robbery, Theft, Pedestrian, Fire, Violations, and Forbidden) from CUVA as the event types. **For scenes elements**, we assign two annotators to classify scenes for each abnormal event. If they cannot reach an agreement, an expert will make the final decision to ensure annotation quality. The _Kappa_ consistency check value of the annotation is 0.87. **2)** For localization task, we use the timestamp in the CUVA as labels for localization. Furthermore, we adhere to the split of CUVA for training and inference videos and take 8 frames per second, resulting in **800K** frames from 1K videos and each video contains **1.68** abnormal event on average. The statistics of the number of events and the duration in seconds (s) of events for each scene are shown in Table 1. Finally, we obtain our M-VAE instruction dataset. Our instruction for the M-VAE task is: _"Generate a quadruple and localize an abnormal event in the video. The quadruple includes subject, event type, object, and scene in abnormal events."_. Figure 1 (c) and Figure 4 show the top 20 quadruple elements, revealing the spatial imbalance.

### Baselines

In this paper, we select several advanced Video-LLMs as baselines which are introduced as follows. **VideoChat**(Wang et al., 2019) employs Q-Former (Wang et al., 2019) to map visual representations to Vicuna (Huang et al., 2019). **VideoChatGPT**(Wang et al., 2019) integrates LLMs with CLIP (Wang et al., 2019) for video representations. **Valley**(Wang et al., 2019) employs a temporal modeling module to bridge

\begin{table}
\begin{tabular}{c

[MISSING_PAGE_FAIL:6]

### Contributions of Each Key Component

In order to further investigate the contributions of different modules of **Sherlock**, we conduct an ablation study on our **Sherlock** model. As shown in Table 2, w/o AE, w/o GRE, w/o GE, w/o GE, and w/o pre-tuning represent without four Spatial Experts, Expert Gate, and pre-tuning stage in see 3.2 respectively.

**Effectiveness Study of Global and Local Spatial Expert.** From Table 2, we can see that: The performance of **w/o AE**, **w/o GRE**, **w/o BE** and **w/o GE** degrades in all metrics, with an average decrease of 7.54 (_p_-value < 0.01), 7.57 (_p_-value < 0.01), 4.37 (_p_-value < 0.01), and 5.68 (_p_-value < 0.01) in FNRs, F2, average mapp@toU, and average event extraction metrics. This confirms the importance of global and local spatial information in extracting and localizing abnormal events, and our **Sherlock** model can better model those information well.

**Effectiveness Study of Spatial Imbalance Regulator.** From Table 2, we can see that: **1)** Compared with **Sherlock**, **w/o GE** shows poorer performance in all metrics, with a decrease of FNRs, F2, average mapp@toU, and average extraction performance by 15.34 (_p_-value < 0.01), 16.52 (_p_-value < 0.01), 8.62 (_p_-value < 0.01) and 10.36 (_p_-value < 0.01), respectively. This demonstrates the effectiveness of GSM in global-local spatial modeling and encourages us to consider handling heterogeneity issues between spatial information in the manner of Mo. **2)** From Table 2, we can see that compared to performance of **w/o SIG**, the performance of **w/o MG** is poorer, with FNRs, F2, average mapp@toU, and average event extraction metrics decreasing by 1.94 (_p_-value < 0.05), 3.9 (_p_-value < 0.05), 1.13 (_p_-value < 0.05) and 4.84 (_p_-value < 0.05), respectively. This further demonstrates the effectiveness of \(\mathcal{L}_{\text{gate}}\) in global-local spatial balancing and encourages us to consider using SIR to better balance spatial information. **3)** In addition, we record the weights of four spatial experts after training in Figure 6 and Figure 7 (_a_). We can see that the weights of all experts have been relatively balanced, and each expert has demonstrated outstanding professional abilities when facing different types of abnormal videos.

**Effectiveness Study of Pre-tuning.** From Table 2, we can see that **w/o pre-tuning**, the performance is inferior to **Sherlock**. FNRs, F2, average mapp@toU, and average event extraction metrics have decreased by 17.63 (_p_-value < 0.01), 16.95 (_p_-value < 0.01), 10.92 (_p_-value < 0.01) and 11.48 (_p_-value < 0.01), respectively. This further

\begin{table}
\begin{tabular}{c|c c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Models**} & \multicolumn{4}{c|}{**Anomaly Location**} & \multicolumn{4}{c}{**Anomaly Cls.**} \\ \cline{2-9}  & & \multicolumn{3}{c|}{mAP@toU} & \multicolumn{3}{c|}{Average} & \multicolumn{3}{c|}{FNRs} & \multicolumn{1}{c}{F2} & \multicolumn{1}{c}{782} \\  & 0.1 & 0.2 & 0.3 & & & & & & & & & & & & & & & & & & & & & \\ \hline BiConvLSTM[(21)] & 52.74 & 37.31 & 31.12 & 40.39 & 68.05 & 44.48 & 784 & 784 & 784 & 784 & 784 & 784 & 784justifies the effectiveness of pre-tuning, as well as encourages us to use more high-quality datasets to enhance the spatial understanding ability of Video-LLMs before instruction-tuning.

### Convergence Analysis and Practical Assessment for Sherlock

In order to analyze the convergence of Sherlock, we record the loss of baseline Video-LLMs, Sherlock, and its variant without specific components over various training steps during the experiment. The results are shown in Figure 5 and we can see that: **1) Sherlock** demonstrates the fastest convergence compared to other Video-LLMs. At the convergence point, the loss of Sherlock is 1.05, while Video-LLAv is 2.06. This underscores the high efficiency of Sherlock over other advanced Video-LLMs, which hints at the potential of Sherlock for quicker training steps and less resource utilization. **2) Sherlock** demonstrates the fastest convergence compared to its variant without specific components in Figure 5. This justifies that the four types of spatial information along with GSM and SIR can accelerate the convergence process, which further encourages us to consider the spatial information in the M-VAE task.

To assess practicality, we analyze the FNRs of Sherlock for each scene. As shown in Table 3, we can observe that in every scene, Sherlock outperforms other Video-LLMs. This indicates that the possibility of misclassifying abnormal events as normal events is minimized, thereby demonstrating the importance of global and local spatial modeling of Sherlock. We also analyze the average inference time in seconds for a one-minute video. As shown in Figure 7 (b), Sherlock does not perform much differently from the other models in terms of inference time. This is reasonable, as some studies confirm that the MoE architecture can improve efficiency (Golovne et al., 2013; He et al., 2016). This suggests that introducing more information along with a MoE module for the M-VAE task does not increase the inference time and Sherlock can maintain good inference efficiency.

### Compared with Advanced Non-LLM Models on Public Dataset

In order to more comprehensively evaluate the effectiveness of Sherlock, we compare our **Sherlock** model with other advanced non-LLM models (Golovne et al., 2013; He et al., 2016; He et al., 2016; He et al., 2016) on traditional anomaly localization and anomaly classification task based on publicly available CUVA datasets (He et al., 2016). Specifically, we need Sherlock to determine whether each second of the video is abnormal or not without generating quadruples. As shown in Table 4, non-LLM models not only underperform relative to other Video-LLMs presented in Table 4 but also significantly inferior to our Sherlock model. This further demonstrates the importance of the global and local spatial information we proposed for the M-VAE task.

### Qualitative Analysis for Sherlock

As shown in Figure 8, we visualize and compare **Sherlock** with other Video-LLMs. We randomly select two samples from our dataset and ask these models to _Analyze the following video and localize the timestamp and extract the quadruple of the abnormal events_. From the figure, we can see that: **1)** Accurately localizing abnormal events and extracting correct quadruples is a huge challenge. For instance, example 2 captures a segment from 9s to 15s, where identifying the collision of the truck at road is particularly challenging, **2)** Compared with other advanced Video-LLMs, **Sherlock** shows excellent performance in localizing abnormal events. In example 1, **Sherlock** outperforms other models in terms of prediction accuracy. In example 2, it outperforms PandaGPT in terms of accuracy and can generate a correct quadruple. This further demonstrates the effectiveness of **Sherlock** in precisely extracting and localizing abnormal events in video segments.

## 6. Conclusion

In this paper, we firstly propose a new M-VAE task and a constructed M-VAE instruction dataset, making a significant contribution to future research on abnormal events. Secondly, we propose a Global-local Spatial-sensitive LLM named Sherlock to assist in localizing and extracting abnormal event quadruples, providing decision-makers with more intuitive and comprehensive information support. This model includes a Global-local Spatial-enhanced MoE module and Spatial Imbalance Regular to model and balance spatial information. In the end, our experimental results demonstrate the outstanding performance of Sherlock. In future work, we hope to consider the relationships between events and enrich our tasks with event inference to improve the performance of extraction. In addition, we also hope to improve the interpretability of our model by providing explanations for each abnormal event.

Figure 8. Two Visualized samples to compare Sherlock with other Video-LLMs.

## References

* (1)
* (2) Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. _arXiv preprint arXiv:1607.04659_ (2016).
* (3) Lin Bai, Qingmin Liu, Cuiling Li, Zhen Ye, Meng Hui, and Xiuping Jia. 2022. Remote sensing image scene classification using multiscale feature fusion covariance network with coarse convolution. _IEEE Transactions on Geoscience and Remote Sensing_ (2022), 1-14.
* (4) Antoine Bosseluti, Junitri Chen, David Scott Warren, Hannanh Hajishirzi, and Yieu Choi. 2016. Learning Prototypical Event Structure from Photo Albums. In _Proceedings of ACL 2016_.
* (5) Jerum Chen, Fangmin Wei, Jingjing Zhao, Siabe Song, Bohunai Wu, Zhuowuan Pong, S.-H. Gary Chan, and Hongyang Zhang. 2004. Revisiting Referring Expression Comprehension Evaluation in the Era of Large Multimodal Models. _CoRR_ abs/2204.18066 (2020).
* (6) Xiangming Chen, Cho-Jui Hsieh, and Boqing Gong. 2022. When Vision Transform-Cut Superform Rankets with Post-training of Strong Data Attenuations. In _Proceedings of ACL 2022_.
* (7) Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Mayan Meng, Qinglong Zhang, Xilin Zhou, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qu, and Jing Li. 2023. Intersvm: Scaling by Vision Foundation Models and Align for Generic Visual-Ignostic Tasks. _CoRR_ abs/2212.41283 (2023).
* (8) Bowen Cheng, Bin Xiao, Jingdong Wang, Honghua Shih, Thomas T. Huang, and Lei Zhang. 2020. HighNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation. In _Proceedings of CVPR 2020_. 5385-5394.
* (9) Gong Cheng, Peichong Zhou, Junwei Han, Lei Guo, and Jongang Han. 2015. Auto-encoder-based model-level visual dictionary learning for scene classification using very high resolution remote sensing images. _IET Computer Vision_ 9 (2015), 639-647.
* (10) Ming Cheng, Kunqing Cai, and Ming Li. 2020. RWP-2004. An Open Large Scale Video Database for Violence Detection. In _Proceedings of ICRA 2020_. 4183-4190.
* (11) Wei-Lin Chiang, Zhouhan Li, Zili Jin, Ying Sheng, Zhanghao Wu, Hao Zhang, Liamin Zheng, Syuan Zhang, Yonghao Zhuang, Joseph E Gonzalez, Ioni Sotka, and Eric Yung. 2023. Vicuna: An Open-Source Chattopressing GPT+ with 90% ChatGPT Quality. [https://mysg.org/blog/2023-03-30-vicuna/](https://mysg.org/blog/2023-03-30-vicuna/).
* (12) Yuen Cong, Michael Ying Yang, and Bodo Rosenthal. 2023. REff-Relation Transformer for Scene Graph Generation. _IEEE Trans. Pattern Anal. Mach. Intell._ 45, 02031, 11169-11183.
* (13) Jing Deng, Nicheng Zhang, Jianbin Xie, Guodhan Nan, Jiyang Zhang, Junruxi Xu, Hangyu Liu, Sicong Leng, Jianping Li, Hebe Fan, Dixiu Huang, Jing Feng, Lani Chen, Can Zhang, Xuhuan Li, Hao Zhao, Jianping Chen, and Jianfeng Tao. 2024. Uncovering What, Why and How: A Comprehensive Benchmark for Causation Understanding of Video Anomaly. _CoRR_ abs/2405.00181 (2024).
* (14) Jia-Chang Feng, Fa-Ting Hong, and Wei-Shi Zheng. 2021. MIST: Multiple Instance Self-Training Framework for Video Anomaly Detection. In _Proceedings of CVPR 2021_. 14009-14014.
* (15) Guilettem Garcia-Cobo and Juan C. SanMiguel. 2023. Human skeletons and change detection for efficient video detection in surveillance videos. _Comput. Vis. Image Underst._ 233 (2023).
* (16) John Giftur, John Garcia, Carl Desrosch, and Andrew Zisserman. 2019. Video Action Transformer Network. In _Proceedings of CVPR 2019_. 244-253.
* (17) Rohit Giftur, Alaeidi El-Noby Zhang, Li Mann, Siabh Kim, Jusan Vasudev Alwala, Armand Joulin, and Manu Miros. 2023. ImagenIDi: The Embedding Space to Bind Them All. In _Proceedings of CVPR 2023_. 15180-15190.
* (18) Dong Gong, Lingqiuo Liu, Yong Li, Buddhaily Sathas, Moussa Reda Mansour, Svetina Venkatesh, and Yuan Sun. 2019. Generating Normality to Detect Anomaly: Memory-Augmented Deep Attractor for Unsupervised Anomaly Detection. In _Proceedings of ICCV 2019_. 1705-1714.
* (19) Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. 2022. Generating Diverse and Natural 3D Human Actions From Text. In _Proceedings of CVPR 2019_. 5142-5151.
* (20) Huiwen Guo, Xinyu Wu, Naman Li, Ruiqing Fu, Guoyuan Liang, and Wei Peng. 2019. Anomaly detection and localization for object scenes using short-term trajectories. In _Proceedings of RODS 2019_. 245-249.
* (21) Jianfeng Han, Kaistong Gong, Yiyun Zhang, Jingqi Wang, Kaigeng Zhang, Dahua Lin, Yu Qiao, Peng Gao, and Gangyu Xue. 2023. OneLM: One Framework to Align All Modalities with Language. _CoRR_ abs/2312.03700 (2023).
* (22) Kathosopher Sanjitha Davis Lawr Hansen, Alex Prystuk Kotliusky. 2019. Bidirectional Convolutional LSTM for the Detection of Violence in Videos. In _Proceedings of ECCV 2019_. 280-296.
* (23) Mahmudul Hassan, Jonghyun Choi, Jan Neumann, Amit K. Roy-Chowdhury, and Larry S. Davis. 2018. Learning Temporal Regularization by Video Sequences. In _Proceedings of CVPR 2019_. 733-742.
* (24) Yu Hong, Jianfeng Zhang, Bin Ma, Jian-Min Yao, Guodong Zhou, and Qioming Zhu. 2011. Using Cross-Finity Inference to Improve Event Extraction. In _Proceedings of ACL 2011_. 1127-1136.
* (25) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weihu Chen. 2021. ZAGL: Low-Rank Adaptation of Large Image Models. In _Proceedings of ICLR 2022_.
* (26) Robert A. Jacobs, Michael I. Jordan, Steven J. Novlan, and Geoffrey E. Hinton. 1991. Adaptive Mixtures of Local Experts. _Neural Comput._ 3, 1 (1991), 79-87.
* (27) Gagan Jin, Nidi Hegde, Aditya Kuupati, Arslan Nagrani, Syamul Broad, Practical Jain, Anurag Arnah, and Sajoyu Satwal. 2024. Mixture of Nested Experts: Adaptive Processing of Visual Tochers. _CoRR_ abs/2407.1985 (2024).
* (28) Heng J and Ralph Grishman. 2008. Refining Event Extraction through Cross-Document Inference. In _Proceedings of ACL 2008_. 254-262.
* (29) Peng Jin, Ryuichi Takabuoh, Caiyan Zhang, Xiaochun Cao, and Li Yuan. 2024. Chat-Univ: Unified Visual Representation Empowered Large Language Models with Image and Video Understanding. _CoRR_ abs/2311.08046 (2023).
* (30) Bingxin, Katona Otsukhoy, Shengyu Huang, Nando Metzger, Rodrigo Cave, David, and Konrad Schindler. 2024. Reproving Diffusion-Based Image Generation for Monocular Depth Estimation. In _Proceedings of CVPR 2024_. 9492-9502.
* (31) Alexander Krillov, Eric Mittmann, Nikhia Ravi, Huzi Maha, Chao Eidlund, Laura Gustafson, Tie Xu, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lu, Piotr Dollar, and Ross B. Girshick. 2023. Segment Anything. In _Proceedings of ICCV 2023_. 3992-4003.
* (32) Federico Landi, Cees G. M. Snoek, and Rita Cucchiara. 2019. Anomaly Locality in Video Surveillance. _CoRR_ abs/1901.1364 (2019).
* (33) Li, Yingjang Liu, Lin Liao, Tat-Seng Chen, and Donghong Ji. 2023. DaS:A 20: Reachable of Conversational Aspect-based Sentiment Quadruple Analysis. In _Proceedings of ACL 2023_. 13449-13467.
* (34) Bo Li, Yunzhan Zhang, Liangyu Chen, Jinghao Wang, Jingyang Yang, and Zirwei Liu. 2020. Ofer: A Multi-Modal Model with In-Context Instruction Tuning. _CoRR_ abs/2005.03726 (2022).
* (35) Haifeng Li, Xin Duo, Chao Tao, Zhixiang Wu, Jie Chen, Jian Peng, Min Deng, and Ling Zhao. 2020. RBS-CB: A Large-Scale Remote Sensing Image Classification Benchmark Using Crowdsourced Data. _Sensors_ 20 (2020).
* (36) Jian Li, Dongxin Li, Siuiyaue Sariew, and Steven C. H. Hoi. 2023. RILIP-2: Bootstrapping Language-Image Pre-training with First Promega Images Encoders and Large Language Models. In _Proceedings of ACL 2023_. 13939-13942.
* (37) Kunchang Li, Yinan He, Yi Wang, Yizhao Li, Wenhai Wang, Ping Luo, Yali Wang, Lianin Wang, and Yin Qiao. 2023. VideoChat: Chat-Centric Video Understanding. _CoRR_ abs/2306.05355 (2023).
* (38) Qi Li, Peng Ji, and Liang Huang. 2020. Joint Event Extraction via Structured Prediction with Global Features. In _Proceedings of ACL 2013_. 73-82.
* (39) Shao Li, Fang Liu, and Licheng Jiao. 2022. Self-Training Multi-Seguring Learning with Transformer for Weakly Supervised Video Anomaly Detection. In _Proceedings of AAAI 2020_. 1395-1403.
* (40) Shaha Liao and Ralph Grishman. 2010. Using Document Level Cross Event Inference to Improve Event. In _Proceedings of ACL 2020_. 789-797.
* (41) Bin Lin, Yang Ye, Bin Zhu, Jiasci Cui, Muann Ning, Peng Jin, and Li Yuan. 2023. Video-LAVA: Learning Uniteal Visual Representation by Alignment Before Projection. _CoRR_ abs/2311.11092 (2023).
* (42) Tang-Yi Lin, Michael Jiang, Sergey L. Bolang, James Pirs, Petros Penna Devan Raman, Piotr Dollar, and Cavendere Zitnick. 2014. Microsoft COCO: Common Objects in Context. In _Proceedings of ICCV 2014_. 740-755.
* (43) Zhang Lin, Chaolat Tan, Jian-Yang He, Zhi Jin, Tianci Xie, and Wei-Shi Zheng. 2023. Collaborative Static and Dynamic Vision-Language Streams for Spatio-Temporal Video Grounding. In _Proceedings of CVPR 2023_. 23100-23109.
* (44) Kun Liu, Yue Han, Xiaofeng Zhu, Yuanxing Zhu, Jiangu Zhang, Yu Ren, Yong Tian, and Yejeng Zheng. 2023. MOELRA: An MOE-based Parameter Efficient Fine-Tuning Method for Multi-Task Medical Applications. _CoRR_ abs/2310.18959 (2023).
* (45) Insolubio and Frank Hutter. 2019. Decoupled Weight Decay Regularization. In _Proceedings of KCLR 2019_.
* (46) Cevin L. Rangy Krishna, Michael S. Bernstein, and Li Fei-Fei. 2016. Visual Relationship Detection with Language Priors. In _Proceedings of ECCV 2016_. 852-869.
* (47) Cevin L, Jianping Shi, and Jiaya Jia. 2013. Abnormal Event Detection at 150 FPS in MATLAB. In _Proceedings of ICCV 2018_. 2720-2727.
* (48) Xiaoping Lu, Jing Sun, and Xianglong Zheng. 2019. A feature aggregation convolutional neural network for remote sensing scene classification. _IEEE Transactions on Geoscience and Remote Sensing_ 57 (2019), 7894-7906.
* (49) Rujita Liu, Zizuo Zhao, Minu Jiang, Junwei Dong, Mingqiu Qin, Pengheng Lu, Tao Wang, and Zhongyu. 2023. Valley: Video Assistant with Large Language Benchmarked Subir. _CoRR_ abs/2306.0727 (2023).
* (50) Fengyuan Lv, Wenjun Wu, Yanfei Zhang, Fang Du, and Liangpei Zhang. 2022. SCVVL: A spatial-channel feature preserving vision transformer for remote sensing image scene classification. _IEEE Transactions on Geoscience and Remote Sensing_ (2022), 1-12.
* (51) Muhammad Maxa, Hancoma Abdul Rashed, Salman H. Khan, and Fahnd Shahbaz Khan. 2023. Video-ChatGPT: Towards Detailed Video Understanding via LargeVision and Language Models. _CoRR_ abs/2306.65424 (2023).
* [106] Diket Mathieu, Zhenzh Li, Peng Gao, Xuelong Zhang, and Pengfeng Xiao. 2024. Links: [http://www.tensorflow.com/content/early/2024/02/024/024](http://www.tensorflow.com/content/early/2024/02/024/024).
* [107] Diket Mathieu, Zhenzh Li, Peng Gao, Xuelong Zhang, and Pengfeng Xiao. 2024. Links: [http://www.tensorflow.com/content/early/2024/02/024/024](http://www.tensorflow.com/content/early/2024/02/024/024).
* [108] Yuan Ning, Bin Liu, Yiqia Xie, Bin Lin, Jiaxi Cui, Lian Yuan, Dongdong Chen, and Li Yuan. 2023. Video-based A Comprehensive Benchmark and Toolkit for Evaluating Video-based Large Language Models. _CoRR_ abs/2312.16163 (2023).
* [109] OpenAI. 2023. "GT-T-T technical Report. _CoRR_ abs/2307.0747 (2023).
* [110] Chao Pang, Jiang Wu, Jiayu Li, Yi Liu, Jianxing Sun, Weihai L. Xingang, Wenqiu Wang, Liangxing Feng, Cai-Sou Xie, et al. 2024. HESVLM: Towards Hybrid and Honest Remote Sensing Engine Vision Language Model. _arXiv preprint arXiv:2402.03123_ (2024).
* [111] Jean Pougareser, Carlos Riquelme Ruiz, Basil Mustafa, and Neil Houlsby. 2024. From Sparse to Soft Mixtures of Experts. In _Proceedings of ICTE 2024_.
* [112] Alex Bedford, Jonker Ritch, Chris Halliday, Aditya Barnett, Gabriel Golub, Sandhuji Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In _Proceedings of ICML 2021_. 8748-8763.
* [113] Jianfeng Ren, Xudong Jiang, and Junsong Yuan. 2015. Learning LBP structure by maximizing the conditional mutual information. _Pattern Recognition_ 48 (2015), 3180-3190.
* [114] Zhiqi Song, Ann Isie, Stephanie M. Strasser, Tom Rieze, Justin Mott, Joe Ellis, Jonathan Wright, Seth Kulick, Neville Bryant, and Xiao Yi. 2015. From Light to Rich ERN: Annotation of Entities, Relations, and Events. In _Proceedings of EVENTS 2015_. 89-98.
* [115] Jiayu Su, Paris Her, Erik Clemens, Edwin E. Yaz, Susan C. Schneider, and Henry Medenov. 2022. Violence Detection using 3D Convolutional Neural Networks. In _Proceedings of WASSA 2022_. 1-8.
* [116] Yiuxan Yu, Tian Lan, Huayang Li, Jia Liu, Xu Wang, and Deng Cai. 2023. PandaGP: The Model to Instruction-Tother Than All. _CoRR_ abs/2305.16355 (2023).
* [117] Yukun Su, Guosheng Lin, Jin-Hui Zhu, and Qingyao Wu. 2020. Human Intersection Learning on SD Selection Point Clouds for Video Violence Recognition. In _Proceedings of ECCV 2020_. 74-90.
* [118] Waqas Sultani, Chen Chen, and Maharak Shah. 2018. Real-World Anomaly Detection in Surveillance Videos. In _Proceedings of CVPR 2018_. 6479-6488.
* [119] Yu Tai, Guanqeng Pang, Yuanhong Chen, Rayinqiu Singh, John W. Weyian, and Gustave Carneiro. 2021. Weakly-supervised Video Anomaly Detection with Robust Temporal Feature Magnitude Learning. In _Proceedings of ICCV 2021_. 4955-4966.
* [120] Qi Wang, Shaoteng Liu, Jocelyn Chausset, and Xuelong Li. 2018. Scene classification with recurrent attention of VIR remote sensing images. _IEEE Transactions on Geoscience and Remote Sensing_ 57 (2018), 1155-1167.
* [121] Seyed Talal Wainis, Muhammad Naseer, Salman Khan, Ming-Hsuan Yang, and Fahad Shabha Khan. 2024. Video-Groundings: Towards Open-Volabulary Spatio-Temporal Video Grounding. _CoRR_ abs/2401.00901 (2024).
* [122] Jiannan Yun, Yi Jiang, Bin Jin, Huachui Liu, Zhenhua Yuan, and Ping Luo. 2023. Unified+: Segment Every Reference Object in Spatial and Temporal Spaces. _arXiv preprint arXiv:2312.5175_ (2023).
* [123] Peng Wu, Jing Liu, Yuiqia Xie, Jinxiang Shao, Zhuoyang Wu, and Zhiwei Yang. 2020. Not only Look, But Also Listen: Learning Multimodal Violence Detection Under Suck supervision. In _Proceedings of ECCV 2020_. 322-339.
* [124] Peng Wu, Xuerong Zhou, Guanqeng Pang, Liangxing Zhou, Qingyao Yuan, Peng Wang, and Yanming Zhang. 2024. MaCLIP: Adapting Vision-Language Models for Weakly Supervised Video Anomaly Detection. In _Proceedings of AAAI 2023_. 6074-6082.
* [125] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. 2010. SUN database: Large-scale scene recognition from abbery to no. In _Proceedings of CVPR 2010_. 3485-3492.
* [126] Dan Xu, Yan Yan, Elisa Ricci, and Nicu Sebe. 2017. Detecting anomalous events in videos by learning deep representations of appearance and motion. _Comput. Vis. Image Understanding_ 15 (2017), 117-127.
* [127] Zhixuan Xu, Chongdai Gao, Zixuan Liu, Gang Yang, Chenrui Te, Haoohuo Zheng, Huoyu Zhou, Weibun Feng, Dehang Wang, Tianyi Chen, Zhobilang Yu, and Lin Shao. 2024. ManifFoundation Model for General-Purpose Robotic Manipulation of Contact Synthesis with Arbitrary Objects and Robots. _CoRR_ (2020).
* [128] Zhiwei Yang, Jing Liu, Zhaoyang Wu, Feng Wu, and Xiaotao Liu. 2023. Video Event Restoration Based on Keyframes for Video Anomaly Detection. In _Proceedings of CVPR 2020_. 14592-14601.
* [129] Qinghao Ye, Haiyang Xu, Guahui Zhao, Yuhao Ye, Ming Yan, Yiyong Zhou, Junyang Wang, Anwen Hu, Pengfeng Shi, Tayu Sish, Chenling Li, Yuanchang Xu, Hehong Chen, Junfeng Zian, Qian Q, Ji Zhang, and Fei Huang. 2023. mPL10G-Owl: Modularization Empowers Large Language Models with Multimodality. _CoRR_ abs/2304.14176 (2023).
* [130] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. _Trans. Assoc. Comput. Linguistics_ 2 (2014), 67-78.
* [131] Yang Zhan, Zhitong Xiong, and Yuan Yuan. 2024. Skeypscript: Unifying remote sensing vision-language tasks via instruction training with large language model. _arXiv preprint arXiv:2001.09121_ (2020).
* [132] Hanwang Zhang, Zawilin Kyaw, Shih-Fu Chang, and Tat-Seng Chua. 2017. Visual Transition Embedding Network for Visual Relation Detection. In _Proceedings of CVPR 2017_. 3107-3115.
* [133] Hanwang Zhang, Zawilin Kyaw, Jiayuang Fu, and Shih-Fu Chang. 2017. WFC: Weakly Supervised Visual Relation Detection via Parallel Pairwise R-FCN. In _Proceedings of CVPR 2017_. 4243-4251.
* [134] Hang Zhang, Xin Li, and Liangqiong Zheng. 2023. Video-L1AM: An Instruction-Tuned Audio-Visual Language Model for Video Understanding. In _Proceedings of EMNLP 2021_. 545-553.
* [135] Huan Zhang, Xiaohuo Xu, Xiang Wang, Jialong Zuo, Chuchu Han, Xiaonan Huang, Changxin Gao, Yuehuan Wang, and Nong Sang. 2024. Holmes-VAD: Towards Unbiased and Explainable Video Anomaly Detection via Multi-modal LIM. _CoRR_ abs/2406.12235 (2024).
* [136] Wei Zhang, Ping Tang, and Liyan Zhao. 2019. Remote sensing image scene classification using CNN-Capsule: _Remote Sensing_ 11 (2019), 494.
* [137] Zhiqiong Zhang and Jufeng Tang. 2022. Temporal Sentiment Localization: Listen and Look in Untrimated Videos. In _Proceedings of MM 2022_. 199-208.
* [138] Jia-Xing Zhong, Naman Li, Weijie Kong, Shan Liu, Thomas H. Li, and Ge Li. 2019. Graph Convolutional Label Noise Cleaner: Train a Plug-Andy Action Classifier for Anomaly Detection. In _Proceedings of CVPR 2019_. 1237-1246.
* [139] Bin Liu, Bin Liu, Muan Ning, Yang Han, Jiasui Cui, Hong Wang, Yuan Zhang, Weihang Jiang, Junru Zhang, Zhongwei Li, Caiyuan Zhang, Li Wei Lu, and Li Yuan. 2024. LanguageBinL: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment. In _Proceedings of ICLR 2024_.