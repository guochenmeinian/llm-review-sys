# Aggregate to Adapt: Node-Centric Aggregation for Multi-Source-Free Graph Domain Adaptation

Anonymous Author(s)

###### Abstract.

Unsupervised graph domain adaptation (UGDA) focuses on transferring knowledge from labeled source graph to unlabeled target graph under domain discrepancies. Most existing UGDA methods are designed to adapt information from a single source domain, which cannot effectively exploit the complementary knowledge from multiple source domains. Furthermore, their assumptions that the labeled source graphs are accessible throughout the training procedure might not be practical due to privacy, regulation, and storage concerns. In this paper, we investigate multi-source-free unsupervised graph domain adaptation, i.e., exploring knowledge adaptation from multiple source domains to the unlabeled target domain without utilizing labeled source graphs but relying solely on source pre-trained models. Unlike previous multi-source domain adaptation approaches that aggregate predictions at model level, we introduce a novel model named GraphATA which conducts adaptation at node granularity. Specifically, we parameterize each node with its own graph convolutional matrix by automatically aggregating weight matrices from multiple source models according to its local context, thus realizing dynamic adaptation over graph structured data. We also demonstrate the capability of GraphATA to generalize to both model-centric and layer-centric methods. Comprehensive experiments on various public datasets show that our GraphATA can consistently surpass recent state-of-the-art baselines with different gains. Our source codes and datasets are available at [https://anonymous.4open.science/r/GraphATA-CoDR](https://anonymous.4open.science/r/GraphATA-CoDR).

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

+
Footnote †: ccs: Information and machine learning

## 1. Introduction

Web data is inherently complex, characterized by diverse entities and intricate relationships, making it challenging to mine meaningful insights. Graph algorithms play a pivotal role in numerous web applications, enabling more efficient representation [3, 49], analysis [5], and decision-making [13, 16], etc. While graph neural networks (GNNs) [15, 20, 46, 53] have achieved remarkable success across diverse tasks including node classification [20, 46], traffic forecasting [60, 61], molecular property prediction [22, 42] and web-scale recommendation [11, 56], these GNN models exhibit substantial performance deterioration when applied to graphs with domain discrepancies [51]. To mitigate this gap and eliminate the need for label annotations, unsupervised graph domain adaptation [50, 51, 58] has been proposed to adapt the model by transferring knowledge from labeled source graph to unlabeled target graph. Existing graph domain adaptation approaches either learn domain invariant representations via adversarial training [40, 51] or explicitly minimize the domain distribution discrepancy [41, 50] to improve their generalization capability.

However, the above mentioned methods assume that the knowledge is specifically transferred from a single labeled source domain to an unlabeled target domain. Whereas, in the real world scenarios, data are often collected from multiple domains, which provides a range of complementary knowledge from different perspectives. This could significantly benefit target domains that do not strictly align with any single available source domain. For example, social networks might come from different countries and platforms with linguistic diversity. If the source networks are popular for a particular language like English or Spanish and the target network involves a mix of different languages, the adaptation can be tailored to target distribution by aggregating knowledge from multiple sources. To this end, Multi-Source Domain Adaptation (MSDA) [14, 37, 63] is introduced to learn from multiple source domains, allowing it to obtain complementary knowledge from various source domains and making it more resilient to domain shifts.

Unfortunately, recent MSDA approaches require labeled source data during the adaptation procedure, which might be impractical due to privacy as well as security concerns, especially when source data containing sensitive information, e.g., financial transactions [24] and medical diagnosis [9], etc. Therefore, it is imperative to investigate Multi-Source-Free Domain Adaptation (MSFDA) by relying solely on source pre-trained models without access to any labeled source data [1, 10, 39]. A simple yet straightforward solution for addressing MSDFA is to employ existing single-source-free domain adaptation methods [25, 27, 54] to adapt each source model individually, then the predictions from different source models are averaged to generate the final prediction. Nonetheless, it ignores the transferability of different source domains, since they may contribute differently to the target domain.

There are some recent studies that automatically assign weights to source predictions [1, 10, 39], where a larger value indicates higher transferability. Nevertheless, these aforementioned methods are designed for independent and identically distributed (i.e., iid) data, while the existence of non-iid graph-structured data poses great challenges to MSFDA that remain unexplored. In graphs, nodes are interconnected with each other through edges, formingcomplicated graph structure. Existing model-centric adaptation approaches, which learn a weight for each model, might not be adequate to capture the complementary semantics encoded by each model, leading to inaccurate combination of predictions. The main reason is that different nodes are associated with distinct local neighborhoods, thus globally aggregating source model predictions ignores the fine-grained node level disparity. For instance, source models are trained on two different social networks, e.g., one's connections emphasizing shared interests and the other one's links indicating geographical proximity. Then, we want to adapt these source models to classify node in target network, where neighboring connections might arise from shared interests as well as geographical proximity. As shown in Figure 1, the combination of model level predictions fails to adapt to different local patterns in the target network and results in sub-optimal performance. No matter how the predictions are merged, the outcome remains inaccurate because the individual predictions themselves are flawed. Thus, more devotion is required to effectively handle the graph domain adaptation task with fine-grained information.

To address the aforementioned key challenges, we propose a novel framework named GraphATA (_Aggregate To Adapt_), _which performs node-centric adaptation through dynamically parameterizing each node with a unique graph convolutional matrix_. Instead of globally aggregating source model predictions, we conduct fine-grained adaptation by taking each node's local context information into consideration. At each layer, we generate a personalized graph convolutional matrix for each node by automatically aggregating source models' weight matrices based on its local neighborhood. Therefore, different nodes could have distinct optimal weight matrices, which is flexible to adapt to diverse patterns. Furthermore, sparse constraints are employed to filter out irrelevant information, since not all the source models are useful during the adaptation procedure. We have carried out extensive experiments including node as well as graph classification, and the experimental results demonstrate that our proposed GraphATA outperforms recent state-of-the-art baselines over widely used datasets.

In summary, the main contributions of this paper are as follows:

* To the best of our knowledge, we are the first to investigate the problem of multi-source-free unsupervised graph domain adaptation, which is a practical yet unexplored setting within the graph neural network community.
* We propose a node-centric adaptation framework that parameterizes each node with a personalized graph convolutional matrix according to its local context information, which enables a more generalizable model.
* Extensive experimental results show that GraphATA could achieve state-of-the-art performance across various public datasets with thorough ablation studies further validating the effectiveness of our node-centric adaptation.

## 2. Related Work

**Graph Neural Networks.** With the remarkable success in various graph related tasks, graph neural networks have drawn continuous attention in both academic and industrial communities. Different types of graph neural networks have been designed following the message passing paradigm, which can be categorized into spectral methods (Golov et al., 2013; He et al., 2016; He et al., 2016) and spatial methods (He et al., 2016; He et al., 2016; He et al., 2016). Among them, GCN (He et al., 2016) performs convolution by approximating the Chebyshev polynomial (He et al., 2016) using its truncated first-order graph filter. GAT (He et al., 2016) utilizes an attention mechanism to learn different weights for dynamically aggregating node's neighborhood representations. GraphSAGE (He et al., 2016) introduces an inductive framework that generates representations by sampling and aggregating local representations. For more details, please refer to comprehensive surveys on graph neural networks (He et al., 2016; He et al., 2016). Despite their success, the performance of GNNs depends on high-quality labeled data, which can be challenging for graph-structured data. To address this issue, adapting models trained on label-rich source domains to unlabeled target domains has emerged as a promising solution.

**Unsupervised Domain Adaptation.** The goal of domain adaptation is to transfer knowledge from labeled source domains to unlabeled target domains. One key challenge lies in how to mitigate the domain shifts between source and target domains. To reduce the distribution discrepancy, most methods focus on learning domain invariant representations, which involve either explicit or implicit constraints. For example, some works (Zhu et al., 2017; He et al., 2016) employ maximum mean discrepancy or central moment discrepancy to explicitly minimize the distance between source and target distributions. Other studies (He et al., 2016; He et al., 2016) utilize adversarial training to make the domain discriminator unable to differentiate source and target representations. Recently, there have been endeavors dedicated to unsupervised domain adaptation for non-iid graph-structured data. Particularly, UDAGCN (Wang et al., 2017) follows the adversarial training framework to learn domain invariant representations on graphs. GRADE (Zhu et al., 2017) introduces the metric of graph subtree discrepancy to minimize the distribution shift between source and target graphs. SpecReg (Zhu et al., 2017) designs spectral regularization for theory-grounded graph domain adaptation. Liu et al. (Liu et al., 2017) proposes an edge re-weighting strategy to reduce the conditional structure shift. Mao et al. (Mao et al., 2017) preserves target graph structural proximity and Zhang et al. (Zhang et al., 2017) conducts collaborative adaptation in the scenario of single source-free graph domain adaptation. However, these methods cannot address the multi-source-free graph domain adaptation problem since they require labeled data or are unable to adapt complementary knowledge from multiple source domains.

**Multi-Source-Free Domain Adaptation.** MSFDA extends domain adaptation by transferring knowledge from multiple source

Figure 1. A toy example, where GNN 1 excels in modeling shared interests, whereas GNN 2 is good at capturing geographical proximity. If node B has mixed connection types, simply combining the predictions from GNN 1 and GNN 2 is ineffective, as neither of the source pre-trained GNNs performs well in this scenario.

pre-trained models without accessing any source domain data. To capture the relationship among different source domains, various domain weighting strategies are utilized to estimate the contribution of each source domain to the target domain, including uniform weights, wasserstein distance-based weights and source domain accuracy-based weights (Zhu et al., 2017; Wang et al., 2018; Wang et al., 2019; Wang et al., 2019). Due to the absence of source data, the above strategies are not applicable in the MSFDA setting. Towards this end, DECISION (Abbott et al., 2018) and CAiDA (Cai et al., 2019) aggregate multiple source model predictions and construct pseudo labels for model adaptation. Shen et al. (Shen et al., 2019) propose to balance the bias-variance trade-off through domain aggregation, selective pseudo-labeling and joint feature alignment. Nonetheless, all these models are designed for independent and identically distributed data, which are not suitable for non-iid graph structured data. Moreover, aggregating model level predictions is insufficient to capture the highly diverse graph patterns, since the global weights cannot adequately reflect the importance of each node's local context. In contrast, our model performs adaptation at node granularity with aggregating weight matrices from multiple source models according to its local context.

## 3. Problem Statement

**Notations and Problem Definition.** In multi-source-free unsupervised graph domain adaptation, the goal is to jointly adapt multiple source pre-trained graph neural network models to a target graph without any labels. In this paper, we focus on adapting classification models with \(K\) categories. Formally, let \(\mathcal{G}=(\mathcal{V},\mathcal{E},\mathbf{X})\) denote the unlabeled target graph, where \(\mathcal{V}\) and \(\mathcal{E}\) are the node set and edge set respectively. \(\mathbf{X}\in\mathbb{R}^{n\times d}\) indicates the node feature matrix, with \(n\) representing the number of nodes and \(d\) denoting the dimension of node features. Given a set of source pre-trained GNN models \(\{\Phi_{1},\Phi_{2},\cdots,\Phi_{m}\}\), where the \(i\)-th model is trained using the graph from \(i\)-th source domain, we decompose each source model \(\Phi_{i}\) into two basic components, i.e., the feature extractor \(\phi_{i}:\mathcal{G}\rightarrow\mathbb{R}^{n\times d}\) encoding graph \(\mathcal{G}\) into node representation space and the classifier \(\psi_{i}:\mathbb{R}^{n\times d}\rightarrow\mathbb{R}^{n\times d}\sigma\)\(\mathbb{R}^{K}\) projecting node or graph representations into corresponding class labels. Hence, the source model \(\Phi_{i}\) can be expressed as \(\Phi_{i}=\phi_{i}\circ\psi_{i}\). Our ultimate problem can be reformulated as follows:

_Given an source trained graph neural network models \(\{\Phi_{1},\cdots,\Phi_{m}\}\) and an unlabeled graph \(\mathcal{G}\) (node level task) or a set of unlabeled graphs \(\{\mathcal{G}_{1},\cdots,\mathcal{G}_{n}\}\) (graph level task) from target domain, our goal is to build a target model \(\Phi_{t}\) that aggregates knowledge from multiple source models to achieve accurate predictions in target domain under distribution shifts._

**Message Passing GNN Revisiting.** Most GNNs adopt the message passing framework (Golov et al., 2013; Wang et al., 2018; Wang et al., 2019), where each node iteratively aggregates representations from its local neighborhood. Specifically, the node \(v\)'s representation at layer \(l\) can be calculated as follows:

\[\mathbf{h}_{v}^{l}=\sigma(\text{AGo}(\{\mathbf{h}_{v}^{l-1}\}\cup\{\mathbf{h} _{u}^{l-1},\forall u\in\mathcal{N}(v)\})\cdot\mathbf{W}^{l}), \tag{1}\]

where \(\sigma(\cdot)\) is the activation function and \(\text{AGo}(\cdot)\) represents the permutation-invariant aggregation function that aggregates message from its neighbors \(\mathcal{N}(v)\). \(\mathbf{W}^{l}\) denotes the convolutional matrix at layer \(l\). The aggregation process in mainstream GNNs can be generalized as a weighted summation. For example, GCN (Kipf and Welling, 2017) aggregates neighborhood representations using fixed weights inversely proportional to node degrees. GraphSAGE (Golov et al., 2013) utilizes a mean pooling aggregator, while GAT (Wang et al., 2019) employs an attention mechanism for learnable weighted aggregation. For graph classification task, we simply use global mean pooling and max pooling to assemble all the node representations in the graph. Advanced techniques like hierarchical graph pooling can also be utilized in this scenario (Wang et al., 2019).

## 4. The Proposed GraphATA Model

Figure 2 provides a comparison between existing model-centric methods and our proposed node-centric framework. _Specifically, model-centric adaptation approaches allocate a weight to each model, implying that all the nodes in the target graph share the same weight within each model._ Hence, it fails to reflect the unique characteristic of each individual target node, since the same model may exhibit varying capabilities when encoding different nodes. _In contrast, our node-centric adaptation framework GraphATA takes node disparity into consideration and parameterizes a unique convolutional matrix for each node to achieve fine-grained personalized adaptation._ Particularly, each node derives its own convolutional matrix by automatically aggregating matrices from multiple source GNN models based on its local neighborhood, which results in more generalizable model. Subsequently, we will elaborate the details of the proposed modules.

**Node Neighborhood Disparity.** We start by investigating the local context of each node within the graphs. Different nodes typically exhibit diverse structural patterns as they are not uniformly distributed across the graph. To characterize this property, we conduct a thorough examination of the node's homophilic and heterophilic patterns through the lens of node homophily ratio, which is a widely adopted metric that quantifies the proportion of a node's neighbors having the same class label (Zhu et al., 2017; Wang et al., 2019; Wang et al., 2019). It is formally defined as follows:

\[h_{o}=\frac{|\{u\in\mathcal{N}(v):y_{u}=y_{v}\}|}{|\mathcal{N}(v)|}, \tag{2}\]

where \(\mathcal{N}(v)\) represents node \(v\)'s neighbors set and \(y_{v}\) indicates the class label for node \(v\). Figure 3 demonstrates the node homophily ratio distributions on three social graphs from Twitch datasets (Section 5.1). We can observe that _(1) all three graphs manifest a mixture of homophilic as well as heterophilic patterns; (2) the patterns' distributions vary significantly across different graphs_. Thus, existing model-centric methods (Abbott et al., 2018; Cai et al., 2019) overlook each node's neighborhood disparity and the allocated weights might be sub-optimal. The above observations motivate us to perform fine-grained node-centric adaptation.

**Node-Centric Adaptation.** In the above investigation, we recognize the necessity of adapting to the local context of each individual node. To achieve this goal, we propose to assign distinct matrices to different nodes by aggregating convolutional matrices from the source pre-trained models, rather than aggregating model predictions. Specifically, different pre-trained models in the source domains have encapsulated different semantic information, which demonstrate varying capabilities in encoding the local context of each target node. For each node \(o\), we utilize a straightforward yet effective way to represent its local contextual information at layer \(l\) as follows:

\[\mathbf{c}_{\rho}^{l}=\text{Mean}((\mathbf{h}_{u}^{l-1},\forall u\in\mathcal{N}(v) )), \tag{3}\]

where we adopt mean operation to pool its neighbor's representation \(\mathbf{h}_{u}^{l-1}\) from previous layer and \(\mathbf{c}_{\rho}^{l}\in\mathbb{R}^{d_{l-1}}\). More alternative options are presented at Appendix E.

After having obtained the local context \(\mathbf{c}_{\rho}^{l}\), we generate a personalized graph convolutional matrix for node \(v\) as follows:

\[\mathbf{W}_{v}^{l}=\sum_{i=1}^{m}\alpha_{ui}^{l}\mathbf{A}(\mathbf{c}_{\rho}^{l })\mathbf{W}_{i}^{l}+\lambda\mathbf{W}_{g}^{l}, \tag{4}\]

where \(\mathbf{W}_{i}^{l}\in\mathbb{R}^{d_{l-1}\times d_{l}}\) represents the convolutional matrix from the \(i\)-th source pre-trained GNN model at layer \(l\) and \(\mathbf{A}(\mathbf{c}_{\rho}^{l})\) is the \(d_{l-1}\times d_{l-1}\) diagonal matrix with its elements setting as \(\mathbf{c}_{\rho}^{l}\). The attentive coefficient \(\alpha_{ui}\) characterizes the importance of each source domain model when adapting to node \(v\). We further incorporate a global parameter \(\mathbf{W}_{g}^{l}\) shared by all the nodes in the \(l\)-th layer to capture the global general patterns and \(\lambda\) is a trade-off parameter. Therefore, our derived personalized \(\mathbf{W}_{v}^{l}\) considers not only local but also global aspects of the graph, making it more adaptable to different types of distribution shifts.

**Sparse Attention Selection.** In Equation (4), although \(\mathbf{W}_{v}^{l}\) automatically aggregates the convolutional matrices from multiple source models according to its local context, we posit that not all the source domain models are useful, which is known as "negative transfer" (Chen et al., 2017; Wang et al., 2018). To combat this issue, we aim to filter out detrimental models and preserve a sparse mixture of effective models via attention coefficients. Particularly, we utilize a shared linear transformation parametrized by \(\mathbf{a}^{l}\in\mathbb{R}^{d_{l}}\) to quantify the trustworthy and reliability of each model when adapting to the target node's local contextual information \(\mathbf{c}_{\rho}^{l}\). At each layer, the attention score can be calculated as follows:

\[\alpha_{ui}=\text{Attention}(\mathbf{a},\mathbf{c}_{\rho},\mathbf{W}_{i})= \mathbf{a}^{\top}(\mathbf{W}_{i}^{\top}\mathbf{c}_{\rho}), \tag{5}\]

where the superscripts are omitted for simplicity. Additionally, we normalize the scores to ensure that \(\alpha_{ui}\in[0,1]\) and \(\sum_{i=1}^{m}\alpha_{ui}=1\). One commonly utilized approach is to employ the softmax function; however, it always produces non-zero values, which fails to yield the desired selective results.

Inspired by recent successes on sparse activation functions, we choose to adopt the sparsemax function (Wang et al., 2018), which preserves the crucial properties of the softmax function and generates sparse distributions. It projects the input onto the probability simplex as follows:

\[\text{sparsemax}(\mathbf{\alpha})=\operatorname*{arg\,min}_{\mathbf{x}\in\mathbf{\alpha}^{ m-1}}\|\mathbf{x}-\mathbf{\alpha}\|^{2}, \tag{6}\]

where the simplex \(\mathbf{\alpha}^{m-1}=\{\mathbf{x}\in\mathbb{R}^{m}|\mathbf{1}^{\top}\mathbf{x}=1,\mathbf{x}\geq 0\}\). Its closed-form solution can be formulated as follows:

\[\text{sparsemax}_{i}(\mathbf{\alpha})=\{\mathbf{a}_{i}-\tau(\mathbf{\alpha})\}_{+}, \tag{7}\]

where \([x]_{+}=\max\{0,x\}\) and \(\tau(\cdot)\) is the threshold function that satisfies \(\sum_{j}\{\sigma_{j}-\tau(\mathbf{\alpha})\}_{+}=1\). To compute \(\tau(\mathbf{\alpha})\), we first sort \(\mathbf{\alpha}\) in descending order: \(\alpha_{1}\geq\alpha_{2}\geq\cdots\geq\alpha_{m}\), and define \(\eta=\max\{1\leq j\leq m|\sigma_{j}>\frac{1}{2}(\sum_{i=1}^{j}\alpha_{i}-1)\}\). Then, we have \(\tau(\mathbf{\alpha})=\frac{\sum_{i=1}^{m}\alpha_{i}-1}{\eta}\). The sparsemax function truncates the values below the threshold to zero and shifts the remaining values by this threshold. Detailed proof is provided in Appendix A.

**Model Optimization.** To optimize the model's parameters, we leverage predictions from the nearest neighbors to generate pseudo labels. For the stability of the learning procedure, we maintain

Figure 3. Node homophily ratio distributions.

Figure 2. An illustrative comparison between existing model-centric methods and our proposed node-centric framework. (a) The target prediction is the weighted combination of source models’ predictions. (b) GraphATA performs fine-grained adaptation by considering each node’s unique characteristic. The grey box with dash lines shows the personalized convolutional matrix for each node at layer \(l\).

a target representation bank \(\mathcal{R}=[\hat{\mathbf{h}}_{1},\cdots,\hat{\mathbf{h}}_{n}]\) and a prediction bank \(\mathcal{P}=[\hat{\mathbf{p}}_{1},\cdots,\hat{\mathbf{p}}_{n}]\) through a momentum updating manner as follows:

\[\hat{\mathbf{h}}_{i}=(1-\gamma)\hat{\mathbf{h}}_{i}+\mathbf{y}\mathbf{h}_{i},\ \hat{\mathbf{p}}_{i}=(1-\gamma)\hat{\mathbf{p}}_{i}+\gamma\mathbf{p}_{i}, \tag{8}\]

where \(\gamma\) denotes the smoothing parameter setting as 0.9 by default. \(\mathbf{h}_{i}\in\mathbb{R}^{d}\) and \(\mathbf{p}_{i}\in\mathbb{R}^{K}\) are the outputs of feature extractor \(\phi_{t}\) and classifier \(\psi_{t}\), respectively. Then, for each target representation \(\mathbf{h}_{i}\), we extract \(r\) nearest neighbors from representation memory bank \(\mathcal{R}\) according to their cosine similarities. With the nearest neighborhood information, the pseudo label distribution of sample \(i\) can be obtained by aggregating the predicted class distributions of these nearest neighbors in memory bank \(\mathcal{P}\) as follows:

\[\hat{\mathbf{y}}_{i}=\mathbb{1}[\arg\max_{k}(\frac{1}{|\mathcal{S}(i)|}\sum_{ j\in\mathcal{S}(i)}\hat{\mathbf{p}}_{j})], \tag{9}\]

where \(\mathbb{1}[\cdot]\) represents the one-hot transformation function and \(\mathcal{S}(i)\) is a set of \(r\) nearest neighbors' indices for sample \(i\). Thus, we could update the model's parameters by minimizing the cross entropy loss between the generated pseudo labels and the predicted class distributions as follows:

\[\mathcal{L}_{cls}=-\frac{1}{n}\sum_{i=1}^{n}\sum_{k=1}^{K}\hat{\mathbf{y}}_{i, k}\text{log}(\mathbf{p}_{i,k}). \tag{10}\]

Additionally, we further encourage the prediction to be individually certain and globally diverse (Hendricks et al., 2017) to avoid the degenerated prediction. Therefore, we minimize the entropy for each individual sample while maximizing the entropy for each class, which is expressed as follows:

\[\mathcal{L}_{reg}=(\frac{1}{n}\sum_{i=1}^{n}\mathcal{H}(\mathbf{p}_{i})]- \mathcal{H}(\frac{1}{n}\sum_{i=1}^{n}\mathbf{p}_{i}). \tag{11}\]

Among them, \(\mathcal{H}(\mathbf{p}_{i})=-\sum_{k=1}^{K}\hat{\mathbf{p}}_{i,k}\text{log}( \mathbf{p}_{i,k})\) denotes the entropy function. Finally, we can obtain the overall objective function as follows and the training procedure is presented in Algorithm 1:

\[\mathcal{L}=\mathcal{L}_{cls}+\mathcal{L}_{reg}. \tag{12}\]

**Model Analysis.** We discuss how our GraphATA generalizes to existing state-of-the-art methods. _(1) Relation with layer-centric approaches._ In particular, when setting \(\mathbf{c}_{0}=1\) and \(\mathbf{W}_{d}=\mathbf{0}\), we suppress the awareness of each node's local contextual information, thus every node will have the same matrix expressed as \(\mathbf{W}_{1,\cdots,n}^{1}=\sum_{k=1}^{n}\alpha_{k}\mathbf{W}_{i}^{k}\) within each layer. It essentially performs a weighted combination of the node representations propagated at each layer. Taking GCN (Kipf and Welling, 2017) as an example, we have \(\mathbf{H}^{I}=\sigma(\hat{\mathbf{A}}\mathbf{H}^{-1}\sum_{i=1}^{n}\alpha_{i} \mathbf{W}_{i}^{I})=\sum_{i=1}^{n}\alpha_{i}\sigma(\hat{\mathbf{A}}\mathbf{H} ^{-1}\mathbf{W}_{i}^{I})\), where \(\sigma(\cdot)\) is the ReLU activation function and \(\hat{\mathbf{A}}\) indicates the normalized adjacent matrix with self-loops. _(2) Relation with model-centric methods._ If we further restrict the information aggregation to the last layer \(L\), i.e., the allocated weights are only employed for aggregating the predictions from each model and there is no information fusion in the intermediate layers, the simplified GraphATA degenerates to model-centric methods. _In summary, the design of GraphATA enjoys various benefits by taking each node's local context into consideration, and the current layer-centric as well as model-centric approaches are its special cases._

**Complexity Analysis.** Suppose that we have a graph with \(n\) nodes and \(e\) edges, the node representation dimension is set as \(d\) and the graph neural network has \(L\) layers. Calculating the local contextual information in each layer has the cost of \(\mathcal{O}(ed)\). Then, if we have \(m\) source models from different domains, the time complexity of generating sparse attention weights for each node is \(\mathcal{O}(md+m\text{log}(m))\). Over \(L\) layers, the feature encoder has the time complexity of \(\mathcal{O}(Lnd^{2}+Led)\). Computing pseudo labels involves obtaining nearest neighbors, which takes \(\mathcal{O}(d\text{log}(n))\) using k-d tree. Thus, the overall time complexity of our model falls within the same range as that of vanilla graph neural network. Detailed comparisons with baselines are presented at Appendix D.

## 5. Experiments

### Datasets

To fully validate the effectiveness of our proposed GraphATA, we perform node and graph classification tasks from various domains. The summary of dataset statistics is presented in Table 1 and the details are described as follows:

_CSBM_ is a synthetic dataset, which is composed of four graphs generated by a 4-class contextual stochastic block model (Chen et al., 2016). Each class contains 2,000 nodes in each graph. To synthesize different conditional structural shift, we fix the intra-class edge probability \(p\) and vary the inter-class probability \(q\) to generate different graphs. The node attributes are sampled from multivariate normal distributions with different mean vectors. The detailed process is described in Appendix B.

_Twitc1_(Wang et al., 2016) consists of six social networks from different regions, i.e., Germany (DE), England (EN), Spain (ES), France (F), Portugal (P) and Russia (R). The nodes represent users, while the edges denote their friendships. We construct node attributes from various factors such as users' gaming activities, preferences, geographic locations and streaming habits, etc. All the nodes are classified into two categories based on whether they use explicit language.

_Citation_(Zhu et al., 2016) contains three research paper citation networks from different platforms and time periods. Particularly, DBLPv7 (D) is extracted from the DBLP database spanning the years 2004 to 2008; ACMv9 (A) comprises papers from ACM database between years 2000 and 2010; Citation1 (C) is derived from MAG database prior to the year 2008. We categorize each paper into one of the five classes, i.e., DB, AI, CV, IS and Networking.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Datasets & \#Nodes & \#Edges & \#Feat & \#Class \\ \hline CSBM & 8,000\(-\)8,000 & 607,699\(-\)752,776 & 128 & 4 \\ Twitc1 & 1,912\(-\)9,498 & 31,299\(-\)153,138 & 3,170 & 2 \\ Citation & 5,484\(-\)9,360 & 8,117\(-\)15,556 & 6,775 & 5 \\ \hline Proteins & -39.00 & -72.82 & 4 & 2 \\ Mutagenicity & -30.32 & -30.77 & 14 & 2 \\ Frankenstein & \(\sim\)16.90 & \(\sim\)17.88 & 780 & 2 \\ \hline \hline \end{tabular}
\end{table}
Table 1. Dataset Statistics.

For graph classification task, we utilize three widely adopted datasets from TUMatasets2(TUMatasets, 2019), i.e., _Proteins_, _Mutagenicity_ and _Frankenstein_. To differentiate the distribution shifts in the datasets, we partition each dataset into four disjoint groups based on their density. Specifically, we first sort all the graphs in each dataset by their density in an ascending order, and then divide them into four equally disjoint groups.

Footnote 2: [https://chrsmrrs.github.io/datasets/docs/datasets/](https://chrsmrrs.github.io/datasets/docs/datasets/)

### Baselines

We compare our proposed GraphATA with four groups of baselines including _source-needed_, _no-adaptation_, _single-source-free_ and _multi-source-free_ domain adaptation methods. The detailed introduction is elaborated as follows:

_Source-needed_. Approaches in this category leverage the labeled source domains' samples to explicitly address the distribution shifts. We consider two multi-source-needed models MDAN (Zhu et al., 2017), M\({}^{3}\)SDA (Zhu et al., 2017) and three single-source-needed models UDGCN (Zhu et al., 2017), GRADE (Zhu et al., 2017) and SpecReg (Zhu et al., 2017) as our baselines. For single-source-needed methods, we merge all the samples from different source domains into a single unified source domain.

_No-adaptation_. This group of baselines include widely used graph neural network models like GCN (Kipf and Welling, 2016), GraphSAGE (Hamilton et al., 2017), GAT (Vaswani et al., 2017) and GIN (Zhu et al., 2017). The model is trained on each labeled source domain and then directly evaluated on the target domain. We output final predictions by taking an average of soft predictions from all the source models.

_Single-source-free_. We also extend existing single-source-free models including SHOT (Zhu et al., 2017), BNM (Zhu et al., 2017), ATDOC (Zhu et al., 2017), NRC (Zhu et al., 2017), JMDS (Zhu et al., 2017), GTrans (Xu et al., 2017), SOGA (Zhu et al., 2017), GraphCTA (Zhu et al., 2017) and TPDS (Zhu et al., 2017) to work in the scenario of multi-source-free domain adaptation. To achieve this goal, we utilize ensemble averaging to integrate soft predictions from all adapted source models.

_Multi-source-free_. Methods in this classes are recent state-of-the-art multi-source-free domain adaptation baselines such as DECISION (Aguilar et al., 2017), CAIDA (Aguilar et al., 2017) and MSFDA (Zhu et al., 2017). They automatically assign suitable weights to each model's predictions for final predictions. These models are originally designed for i.i.d images and we replace its backbone to adapt them for graph structured data.

**Experimental Settings**. Following recent works (Zhu et al., 2017; Zhu et al., 2017), we randomly partition the samples in each source domain into training set (80%), validation set (10%) and test set (10%), respectively. The source model is first trained using the training set and its hyperparameters are fine-tuned on the validation set. Then, we conduct sanity check on the test set to ensure that it is well-trained on the labeled source domain. The final performance is evaluated on the entire target domain. For baselines, we employ the source codes released by the authors and utilize the same graph neural network backbone with identical layers. The node representation dimension is set as 128 for node classification and 64 for graph classification tasks. We implement our proposed GraphATA with Pytorch Geometric (He et al., 2016) and the parameters are optimized with Adam (Kingma and Ba, 2015). The optimal learning rate and weight decay are searched in the set of \(\{0.1,0.01,0.001,1e^{-4}\}\). We keep the smoothing parameter \(\gamma\) for memory banks as 0.9 by default and search the trader-off hyperparameter \(\lambda\) within the range of \([0,1]\). More implementation details are given at Appendix C.

### Results and Analyses

We show the results of node classification and graph classification in Table 2. Additional experiments on large-scale datasets are presented in Table 8 and Table 9 in Appendix E. All the experiments are repeated 5 times and we report the mean accuracy with standard deviation. In summary, we have the following key observations.

First, our proposed GraphATA surpasses all the baselines across various adaptation tasks with different margins. For instance, we achieve 12.20% average relative gains in the scenario of A,D\(\rightarrow\)C compared with the naive no-adaptation method GCN. This implies that simply taking an average of the source model predictions cannot obtain satisfied performance, which is because different domains might contribute differently to the target domain. Therefore, it is important to aggregate information from multiple source domains with suitable weights. We also notice that GAT exhibits relatively poorer performance within this group. The reason can be attributed to the distribution shifts between source and target domains, as the optimal attention weights in source domains become less suitable for the target domain.

Second, when further compared with source-need methods that utilize labeled source data during the training process, our model can still outperform them by significant margins. Meanwhile, single-source-needed baselines, which consolidate all the samples into one large source domain, occasionally beat approaches specifically designed for multi-source domain adaptation like MDAN and M\({}^{3}\)SDA in several settings. It justifies the necessity of aggregating rich information from multiple source domains, since they may contain complementary information for the target domain.

Third, all the multi-source extensions of single-source-free models demonstrate superior performance compared with non-adapted graph neural networks, even though they utilize the same ensemble strategy. These results suggest that domain adaptation is a promising way to mitigate the distribution shifts across different domains. However, there does not exist a clear winner that consistently outperforms the others within this category, because taking average of the predictions might be sub-optimal in some scenarios.

Finally, our GraphATA consistently exceeds the strongest baselines tailored for multi-source-free domain adaptation, such as DECISION, CAiDA and MSFDA. This stems from the fact that our model conducts fine-grained adaptation by comprehensively capturing each node's local context information, while existing model-centric approaches only aggregate information at the prediction level and overlook the fine-grained information. In challenging datasets with significant domain shifts, such as the Frankenstein and Protein datasets, traditional multi-source-free models struggle due to the likelihood of negative transfer. In contrast, our proposed GraphATA could filter out irrelevant models and reduce the impact of negative transfer, leading to more precise adaptation.

### Ablation Studies

**The Effect of Different Modules.** To fully investigate the contribution of each component in our proposed GraphATA model,we conduct a series of ablation studies on citation datasets. We first show the rationality and effectiveness of utilizing sparse attention to selectively aggregate convolutional matrices from multiple source pre-trained models. When replacing the sparsemax function with softmax function in Eq. (6) (termed as GraphATA\({}_{\text{softmax}}\)), its performance degrades 4.46% and 2.04% in Table 3, respectively. This indicates that not all of the source models are useful, and filtering out irrelevant information would be beneficial. We further degenerate Eq. (4) to model-centric method (restricting the information aggregation to the last layer) and layer-centric method (setting \(\mathbf{c}_{0}=1,\mathbf{W}_{g}=0\)), which are denoted as GraphATA\({}_{\text{MC}}\) and GraphATA\({}_{\text{MC}}\). When compared with GraphATA, their performance decreases about 2.93% \(\sim\) 4.26%. This justifies the advantages of conducting fine-grained node-centric adaptation. More ablation studies can be found at Appendix E.

**Hyperparameter Analysis and Attention Visualization.** We also show the impacts of several key hyper-parameters in Figure 4(a) and Figure 4(b). Particularly, when setting number of layers \(L=2\) and \(\lambda=0.2\), our model could always obtain the satisfied performance. To demonstrate the uniqueness of each node's graph convolutional matrices, we randomly sample a graph with 11 nodes and 11

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline \multicolumn{8}{c}{Node Classification} & \multicolumn{8}{c}{Graph Classification} \\ \hline \hline \multicolumn{2}{c}{CSBM} & \multicolumn{3}{c}{Twitch} & \multicolumn{3}{c}{Citation} & \multicolumn{3}{c}{} \\ \cline{2-10}  & C1,C2,C3 \textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitilde{}\textasciitildeasciitilde{

[MISSING_PAGE_FAIL:8]

## References

* (1)
* Ahmed et al. (2021) Skr Minaj Ahmed, Dirgit S Raychaudhuri, Siqoy Paul, Samuel Oymak, and Amit K Roy-Chowdhury. 2021. Unsupervised multi-source domain adaptation without access to source data. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_. 10103-1012.
* Bianchi et al. (2021) Filippo Maria Bianchi, Daniel Gattardi, Lorenzo Livi, and Cesare Alippi. 2021. Graph neural networks with convolutional arms filters. _IEEE transactions on pattern analysis and machine intelligence_ 44, 7 (2021), 3496-3507.
* Brockschmidt (2020) Marc Brockschmidt. 2020. Gim-film: Graph neural networks with feature-wise linear modulation. In _International Conference on Machine Learning_. PMLR, 1144-1152.
* Cao et al. (2018) Zhangie Cao, Minghong Long, Jianmin Wang, and Michael J Jordan. 2018. Partial transfer learning with selective adversarial networks. In _Proceedings of the IEEE conference on computer vision and pattern recognition_. 2724-2732.
* Cui et al. (2022) Hejie Cui, Wei Das, Yuang Zhu, Xuan Anin Antonio Codem Chen, Joubas Lauleine, Liang Liang, Hao Tang, Hejie Cao, and Carl Yang. 2022. Reinbug: a benchmark for brain network analysis with graph neural networks. _IEEE transactions on medical imaging_ 24, 2 (2022), 495-506.
* Cui et al. (2020) Shinahao Cui, Shihui Wang, Jinhao Zhuo, Liang Li, Qingming Huang, and Qi Tian. 2020. Towards discriminability and diversity: Batch nuclear-norm maximization under label insufficient situations. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_. 3941-3950.
* Deferraet et al. (2016) Michael Deferraet, Xavier Flesson, and Pierre Vandergheynst. 2016. Convolutional neural networks on graphs with fast localized spectral filtering. _Advances in neural information processing systems_ 29 (2016).
* Deshpande et al. (2018) Yush Deshpande, Shuhua Sen, Andrea Montanari, and Elchanan Mossel. 2018. Contextual stochastic block models. _Advances in Neural Information Processing Systems_ 31 (2018).
* Dong et al. (2020) Jiahua Dong, Yang Cong, Gan Sun, Bineng Zhong, and Xiaowei Xu. 2020. What can be transferred: Unsupervised domain adaptation for endoscope lessons segmentation. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_. 6023-6032.
* Dong et al. (2021) Jiahua Dong, Zhen Fang, Anjin Liu, Gan Sun, and Tongliang Liu. 2021. Confident anchor-induced multi-source free domain adaptation. _Advances in Neural Information Processing Systems_ 30 (2021), 2848-2860.
* Fan et al. (2019) Wenqi Fan, Yao Ma, Qing Li, Yuan He, Eric Zhao, Jiliang Tang, and Dawei Yin. 2019. Graph neural networks for social recommendation. In _The world wide web_. 417-426.
* Fey and Lenssen (2019) Matthias Fey and Jan Eric Lenssen. 2019. Fast graph representation learning with PyTorch Geometric. _arXiv preprint arXiv:1908.02282_ (2019).
* Qiu et al. (2022) Shawn Qiu, Xiao Wang, Chuan Shi, and Ding Yao. 2022. Self-supervised Graph Neural Networks for Multi-behavior Recommendation. In _IJCAI_. 2025-2058.
* Gao et al. (2018) Jing Gao, Darsh Jish Shah, and Regina Barrahy. 2018. Multi-source domain adaptation with mixture of experts. _arXiv preprint arXiv:1809.02256_ (2018).
* Hamilton et al. (2017) Will Hamilton, Zhito Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. _Advances in neural information processing systems_ 30 (2017).
* He et al. (2020) Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng Wang. 2020. Lightgcn: Simplifying and powering graph convolution network for recommendation. In _Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval_. 639-648.
* Hoffman et al. (2018) Judy Hoffman, Eric Tzeng, Tensuya Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, and Trevor Darrell. 2018. Cycada: Cycle-consistent adversarial domain adaptation. In _International conference on machine learning_. PMLR, 1398-1301.
* Ji et al. (1998) Wei Jin, Tong Zhao, Jiayuan Ding, Yoran Liu, Jiliang Tang, and Neil Shah. 2022. Empowering graph representation learning with test-time graph transformation. _arXiv preprint arXiv:1902.035105_ (2022).
* Kingma and Ba (2014) Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. _arXiv preprint arXiv:1412.09804_ (2014).
* Kipf and Welling (2016) Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph convolutional networks. _arXiv preprint arXiv:1609.02070_ (2016).
* Lee et al. (2022) Jonghyun Lee, Dahjun Zhang, Junbo Yang, Yin, and Sunghyun Yoon. 2022. Confidence score for source-the-unsupervised domain adaptation. In _International Conference on Machine Learning_. PMLR, 12365-12377.
* Li et al. (2022) Han Li, Dan Zhao, and Jianyang Zeng. 2022. KVGT: knowledge-guided pre-training of graph transformer for molecular property prediction. In _Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_. 857-867.
* Li et al. (2021) Xiang Li, Renyu Zhu, Yao Cheng, Calius Shan, Siguang Luo, Dongheng Li, and Weiming Qian. 2021. Finding global homophily in graph neural networks when meeting heterophily. In _International Conference on Machine Learning_. PMLR, 13242-13256.
* Li et al. (2021) Zijian Li, Ruichu Cai, Hong Wei, Meizanne Winslett, Tom ZJ Fu, Boyan Xu, Xiaoyen Yang, and Zhenjie Zhang. 2021. Causal mechanism transfer network for time series domain adaptation in mechanical systems. _ACM Transactions on Intelligent Systems and Technology (TIST)_ 12, 2 (2021), 1-21.
* Liang et al. (2020) Jian Liang, Dapeng Hu, and Jiashi Feng. 2020. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In _International conference on machine learning_. PMLR, 6028-6039.
* Liang et al. (2021) Jian Liang, Dapeng Hu, and Jiashi Feng. 2021. Domain adaptation with auxiliary target domain-oriented classifier. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_. 1663-16642.
* Ling et al. (2021) Jian Ling, Zhengyu Hu, Yunbo Wang, Ran He, and Jiashi Feng. 2021. Source data-absent unsupervised domain adaptation through hypothesis transfer and labeling transfer. _IEEE Transactions on Pattern Analysis and Machine Intelligence_ 41, 11 (2021), 8602-8617.
* Liu et al. (2023) Shihu Liu, Tianchun Li, Yongbin Feng, Nhan Tran, Han Zhao, Qiang Qiu, and Pan Li. 2023. Structured re-weighting improves graph domain adaptation. In _International Conference on Machine Learning_. PMLR, 21795-21793.
* Liu et al. (2021) Zemin Liu, Yuan Fang, Chenghao Liu, and Steven CH Hoi. 2021. Node-wise localization of graph neural networks. _arXiv preprint arXiv:2110.03282_ (2021).
* Long et al. (2015) Menglong Long, Yue Cao, Jianmin Wang, and Michael Jordan. 2015. Learning transferable features with deep adaptation networks. In _International conference on machine learning_. PMLR, 971-915.
* Long et al. (2018) Mingzheng Long, Zhanjie Cao, Jianmin Wang, and Michael I Jordan. 2018. Conditional adversarial domain adaptation. _Advances in neural information processing systems_ 31 (2018).
* Ma et al. (2023) Haitao Ma, Zhixia Chen, Wei Jin, Haoyu Han, Yao Ma, Tong Zhao, Neil Shah, and Liang Tang. 2023. Demyensifying structural disparity in Graph Neural Networks. Can One Size Fit All? _arXiv preprint arXiv:2306.01323_ (2023).
* Mao et al. (2021) Haitao Mao, Lan Du, Weiqi Zheng, Qiang Fu, Zili Li, Xu Chen, Shi Han, and Donghui Zhang. 2021. Source free unsupervised graph domain adaptation. _arXiv preprint arXiv:2112.0953_ (2021).
* Marius and Attuallo (2016) Andre Marius and Ramon Attuallo. 2016. From softmax to sparsemax: A sparse model of attention and multi-label classification. In _International conference on machine learning_. PMLR, 1614-1623.
* Morris et al. (2009) Christopher Morris, Nils M Krieg, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion Neumann. 2009. Tudaatar: A collection of benchmark datasets for learning with graph. _arXiv preprint arXiv:2007.06638_ (2020).
* Pei et al. (2020) Hongbin Pei, Bingke Wei, Eichen Chen-Chuan Chang, Yu Lei, and Jo Yang. 2020. Geom: Generic graph convolutional networks. _arXiv preprint arXiv:2007.05287_ (2020).
* Peng et al. (1983) Xingchao Peng, Qimxu Bai, Xide Xia, Zijun Huang, Kate Soreba, and Bo Wang. 2019. Moment matching for multi-source domain adaptation. In _Proceedings of the IEEE/CVF international conference on computer vision_. 1406-1415.
* Rozenkeck et al. (2019) Benedek Rozenkeck, Carl Allen, and Ralf Sankar. 2019. Multi-scale Attributed Node Embedding. _arXiv preprint arXiv:1908.13021_ (2019).
* Shen et al. (2020) Mahao Shen, Yuheng Bu, and Gregory W Wornell. 2020. On balancing bias and variance in unsupervised multi-source-free domain adaptation. In _International Conference on Machine Learning_. PMLR, 9076-9091.
* Shen et al. (2020) Xiao Shen, Qimxu Dai, Hui-Fu Chang, and Kui-Sae Choi. 2020. Adversarial deep network embedding for cross-network node classification. In _Proceedings of the AAAI conference on artificial intelligence_. 2991-2991.
* Sun et al. (2020) Xiao Shen, Qimxu Dai, Xiao, Fu-Fu Liang, and Kui-Sae Choi. 2020. Network together: Node classification via cross-network deep network embedding. _IEEE Transactions on Neural Networks and Learning Systems_ 32, 35 (2020), 1935-1948.
* Stitak et al. (2022) Hames Stitak, Dominique Beaint, Gabriele Corso, Prudeco Tossou, Christian Dalalegh, Stephan Gunnemann, and Pietro Li.o. 2022. 3dlman improves gms for molecular property prediction. In _International Conference on Machine Learning_. PMLR, 2027-20502.
* Tang et al. (2020) Jie Tang, Jing Zhang, Lumin Yao, Juanzi Li, Li Zhang, and Zhong Su. 2020. Architecture: extraction and mining of academic social networks. In _Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining_. 90-908.
* Tang et al. (2010) Song Tang, An Chung, Fabian Zhang, Xatian Zhu, Mao Ye, and Changhui Zhang. 2010. 204. Source-free domain adaptation via target prediction distribution searching. _International journal of computer vision_ 33, 2 (2010), 654-672.
* Van der Maaten and Hinton (2018) Laurens Van der Maaten and Geoffrey Hinton. 2018. Visualizing data using t-SNE. _Journal of machine learning research_ 9, 11 (2018).
* Velickovic et al. (2017) Peter Velickovic, Guillem Cucurull, Arathua Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks. _arXiv preprint arXiv:1710.0031_ (2017).
* Wang et al. (2019) Haotian Wang, Weiqing Yang, Zhipeng Lin, and Yue Yu. 2019. TMDA: Task-specific multi-source domain adaptation via clustering embedded adversarial training. In _2019 IEEE International Conference on Data Mining (ICDM)_. IEEE, 1372-1377.
* Wang et al. (2019) Zirui Wang, Zihang Dai, Barnabias Piccas, and Jaime Carbonell. 2019. Characterizing and avoiding negative transfer. In _Proceedings of the IEEE/CVF conference on computer vision and pattern recognition_. 11295-11302.
* Wu et al. (2019) Felix Wu, Amani Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. 2019. Simplifying graph convolutional networks. In _International conference on machine learning_. PMLR, 6861-6871.

* Wu et al. (2023) Jun Wu, Jingrui He, and Elizabeth Ainsworth. 2023. Non-iid transfer learning on graphs. In _Proceedings of the AAAI Conference on Artificial Intelligence_. 10342-10350.
* Wu et al. (2020) Man Wu, Shirui Pan, Chuan Zhou, Xiaoyian Chang, and Xingquan Zhu. 2020. Unsupervised domain adaptive graph convolutional networks. In _Proceedings of The Web Conference_. 2020. 1457-1467.
* Wu et al. (2019) Zengzhou Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqiu Zhang, and Yiu Philip. 2020. A comprehensive survey on graph neural networks. _IEEE transactions on neural networks and learning systems_ 32. 1 (2020), 4-24.
* Xu et al. (2018) Keyulu Xu, Weiliua Hu, Juei Lee, and Sefrina Leggla. 2018. How powerful are graph neural networks? _arXiv preprint arXiv:1810.00265_ (2018).
* Yang et al. (2021) Shiqi Yang, Joost van de Weijer, Lisa Herru, Shaqiang-Jing Fu, et al. 2021. Exploiting the intrinsic neighborhood structure for source-free domain adaptation. _Advances in neural information processing systems_ 34 (2021), 29939-29045.
* Yin et al. (2023) Ning Yin, Li Shen, Mengqiu Wang, Long Lan, Zeryu Xu, Cheng Chen, Xin-Sheng Hua, and Xiao Luo. 2023. CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification. _arXiv preprint arXiv:2306.04979_ (2023).
* Ying et al. (2018) Rex Ying, Ruining He, Kaifeng Chen, Peng Khohnachukai, William L Hamilton, and Yue Leskovec. 2018. Graph convolutional neural networks for web-scale recommender systems. In _Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining_. 974-983.
* Ying et al. (2018) Zhifuo Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. 2018. Hierarchical graph representation learning with differentiable pooling. _Advances in neural information processing systems_ 33 (2018).
* You et al. (2022) Yuning You, Tianlong Chen, Zhangyang Wang, and Yang Shen. 2022. Graph domain adaptation via theory-grounded spectral regularization. In _The Eleventh International Conference on Learning Representations_.
* Zellinger et al. (2017) Werner Zellinger, Thomas Gruhner, Edwin L Taylor, Thomas Nachtshager, and Susanne Sangmeyer-Platt. 2017. Central moment discrepancy (em) for domain-invariant representation learning. _arXiv preprint arXiv:1702.00811_ (2017).
* Zhang et al. (2020) Qi Zhang, Jianlong Chang, Gaofeng Zhang, Shiming Xiang, and Chunhong Pan. 2020. Spatio-temporal graph structure learning for traffic forecasting. In _Proceedings of the AAAI conference on artificial intelligence_. 1177-1185.
* Zhang et al. (2021) Xyeng Zhang, Chao Huang, Yong Xu, Lianghao Xia, Peng Dai, Lofeng Bo, Junbo Zhang, and Yu Zheng. 2021. Traffic flow forecasting with spatial-temporal graph diffusion network. In _Proceedings of the AAAI conference on artificial intelligence_. 15081-15155.
* Zhang et al. (2021) Zhen Zhang, Mehan Liu, Anhui Wang, Hongyang Cheng, Zhao Li, Jiajun Bu, and Bingqheng He. 2021. Callbackre to Adapt: Source-Free Graph Domain Adaptation via Bi-directional Adaptation. In _Proceedings of the ACM on Web Conference_. 2024 64-645.
* Zhao et al. (2018) Han Zhao, Shanghang Zhang, Guanhang Wu, Jose MF Moura, Joao P Costeira, and Geoffrey J Gordon. 2018. Adversarial multiple source domain adaptation. _Advances in neural information processing systems_ 31 (2018).
* Zhao et al. (2020) Sicheng Zhao, Guangshi Wang, Shanghang Zhang, Yang Gu, Yasuan Li, Zhichao Song, Pengfei Xu, Ranboo Hu, Hua Chai, and Kurt Keutzer. 2020. Multi-source distilling domain adaptation. In _Proceedings of the AAAI Conference on Artificial Intelligence_. 12975-12983.
* Zhou et al. (2022) Yiu Zhou, Haixu Hu, Zheng Xu, Huang, Shaifeng Hao, Dengga Li, and Junmin Zhao. 2022. Graph neural networks: Taxonomy, advances, and trends. _ACM Transactions on Intelligent Systems and Technology (TIST)_ 1, 2022, 1-54.
* Zhu et al. (2019) Yongzhou Zhu, Yuzhen Zhang, and Dengqing Wang. 2019. Aligning domain-specific distribution and classifier for cross-domain classification from multiple sources. In _Proceedings of the AAAI conference on artificial intelligence_. 5995-5996.

## Appendix A Proof for Equation (7)

To summarize, \(\text{sparsemax}(\cdot)\) considers the euclidean projection of the input vector \(\mathbf{\alpha}\) onto the probability simplex, which can be defined as the following optimization problem:

\[\operatorname*{arg\,min}_{\mathbf{x}\in\mathbf{\alpha}\,\,m-1}\|\mathbf{x}-\mathbf{\alpha}\|^{2 },\ s.t.,\ 1^{\mathsf{T}}\mathbf{x}=1,\ x\geq\mathbf{0}. \tag{13}\]

Then, the Lagrangian of the optimization problem in Eq. (13) is:

\[\mathcal{L}(\mathbf{x},\mathbf{\mu},\omega)=\frac{1}{2}\|\mathbf{x}-\mathbf{\alpha}\|^{2}-\mathbf{ \mu}^{\mathsf{T}}\mathbf{x}+\omega(\mathbf{1}^{\mathsf{T}}\mathbf{x}-1). \tag{14}\]

The optimal \((\mathbf{x}^{*},\mathbf{\mu}^{*},\omega^{*})\) must satisfy the following Karush-Kuhn-Tucker conditions:

\[\mathbf{x}^{*}-\mathbf{\alpha}-\mathbf{\mu}^{*}+\omega^{*}\mathbf{1}=0, \tag{15}\]

\[\mathbf{1}^{\mathsf{T}}\mathbf{x}^{*}=1,\ x^{*}\geq 0,\ \mathbf{\mu}^{*}\geq 0, \tag{16}\]

\[x_{i}^{*}\mu_{i}^{*}=0,\ \forall i\in\{1,\cdots,m\}. \tag{17}\]

If for \(\forall i\in\{1,\cdots,m\}\), we have \(x_{i}^{*}>0\), then from Eq. (17) we must satisfy \(\mu_{i}^{*}=0\). Thus, from Eq. (15), we can get \(x_{i}^{*}=\alpha_{i}-\omega^{*}\). Let \(S(\mathbf{\alpha})=\{j\in\{1,\cdots,m\}|x_{j}^{*}>0\}\). From Eq. (16), we obtain \(\sum_{j\in S(\mathbf{\alpha})}(\alpha_{j}-\omega^{*})=1\), which yields \(\omega^{*}=\tau(\mathbf{\alpha})\) in Eq. (7). Again, from Eq. (17), we have that \(\mu_{i}^{*}>0\) implies \(x_{i}^{*}=0\), which from Eq. (15) implies \(\mu_{i}^{*}=\omega^{*}-\sigma_{i}^{*}\geq 0\), i.e., \(x_{i}^{*}\leq\omega^{*}\) for \(i\notin S(\mathbf{\alpha})\). That's to say, if the element \(\alpha_{i}\) less than threshold \(\omega^{*}\), then \(\mu_{i}^{*}\) will larger than \(0\), and output \(x_{i}\) must be reset to \(0\) to to satisfy Eq. (17). Thus, we can generate the sparse values and have the property of sum-to-one.

## Appendix B Datasets Details

In this section, we present the detailed information for experimental datasets including data processing and dataset splits. Specifically, we employ three types of graphs for the node classification task, which involve synthetic, social and citation networks. For synthetic datasets, contextual stochastic block models with different intra-class probability \(p\) and inter-class probability \(q\) are utilized to synthesize varying degrees of conditional structural shifts. Particularly, we fix intra-class probability \(p=0.04\) and vary inter-class probability \(q\) from \(\{0.012,0.014,0.016,0.018\}\) to generate C1, C2, C3 and C4. As for node attributes, we construct a multivariate normal distribution for each class, where the mean vectors are set as \(\{-2.0,-2/3,2/3,2\}\) with 128-dimension for each class and the covariance matrix is fixed as identity matrix. Then, each class's node attributes are sampled from those multivariate normal distributions. For Twitch datasets, we use their default splits, and different sub-datasets are constructed from distinct regions, which encompasses domain level distribution shifts across different datasets. For Citation dataset, the graphs are extracted from different platforms and periods, thus it contains both domain level and temporal level distribution shifts. For graph classification task, we employ three TUMatases: Proteins, Mutagenicity, and Frankenstein, partitioning each dataset into four equally sized disjoint groups based on density shifts. The detailed information is presented in Table 1.

**Ethical Use of Data and Informed Consent**. All of our datasets are synthetic or publicly available, and do not involve human participants and subjects.

## Appendix C Implementation Details

### Running Environment

Our experiments are conducted on a Linux server with 2 AMD EPYC 7543 CPU@2.80GHz, 512G RAM and one NVIDIA A100-SXM4-80GB GPU. The proposed model is implemented with Pytorch 1.13.1 in Python 3.8 using Pytorch Geometric 2.4.0.

### Compared Baselines

We present the detailed running configures for all the baselines as follows. Since some of the methods are initially devised for i.i.d image datasets, we modify their backbone to suit non-iid graph-structured data. The node represention dimension is set as 128 for node classification task and 64 for graph classification task, respectively.
* **MDAN.3** We use the source code released by the authors and replace its backbone with GCN. MDAN utilizes \(K\) domain classifiers to implicitly align source and target representations via adversarial training. We use MDAN's SoftMax version by setting the mode as dynamic and \(\mu,\gamma\) are set as 0.01 and 10.0, respectively.
* **M\({}^{3}\)SDA.4** The codes are from the author's github page and we use GCN as the backbone. M\({}^{3}\)SDA employs moment matching to explicitly match source and target representations. We adopt 5-order moment matching in the experiments.
* **UDAGCN5**, **GRADE6** and **SpecReg7** are three baselines designed for single source need graph domain adaptation. To adapt them to the scenario of multiple sources, we consolidate all the source domains into a single unified source domain. For UDAGCN, when generating PPMI matrix, we set the random walk length as 5 and each node is repeated 40 times. For SpecReg, the gamma smooth is set as 0.01.

Footnote 5: [https://github.com/hanzhaozhoni/MDAN](https://github.com/hanzhaozhoni/MDAN)

Footnote 6: [https://github.com/viniduan-zhangGroup/MSSDA](https://github.com/viniduan-zhangGroup/MSSDA)

#### c.2.2. No-adaptation GNNs

* **GCN, SAGE, GAT, GIN.8** We use the implementations provided by Pytorch Geometric. The number of layers \(L\) is set as 2. For GAT, we set number of attention heads as 1 and concat is initialized with False. For GIN, we utilize one MLP layer as the injective multiset function and the learning parameter \(\epsilon\) is set as True. Finally, ReLU is used as the activation function and we fix the dropout ratio as 0.1 across different models.

Footnote 8: [https://github.com/hanzhaozhoni/MDAN](https://github.com/hanzhaozhoni/MDAN)

Footnote 9: [https://github.com/viniduan-zhangGroup/MSSDA](https://github.com/viniduan-zhangGroup/MSSDA)

#### c.2.3. Single-source-free

* **SHOT9**, **BNM10**, **ATDOC11**, **NRC12**, **JMDS13** and **TPDS14** are six methods devised for single-source-free domain adaptation in the field of computer vision, thus we replace their backbones with GCN. In the context of multi-source-free domain adaptation, we normalize the soft predictions from multiple sources by taking their average. For ATDOC, the nearest centroid version is used. For NRC, we fix both neighbor size and neighbor of neighbor size as 3.

Footnote 10: [https://github.com/jwatml/GRADE](https://github.com/jwatml/GRADE)

Footnote 11: [https://github.com/Shen-Lab/CbA-SpecReg](https://github.com/Shen-Lab/CbA-SpecReg)

Footnote 12: [https://github.com/viniduan-zhangGroup/MSSDA](https://github.com/viniduan-zhangGroup/MSSDA)

Footnote 13: [https://github.com/jwatml/GRADE](https://github.com/jwatml/GRADE)

Footnote 14: [https://github.com/jwatml/GRADE](https://github.com/jwatml/GRADE)

Footnote 15: [https://github.com/viniduan-zhangGroup/MSSDA](https://github.com/viniduan-zhangGroup/MSSDA)

Footnote 16: [https://github.com/jwatml/LRC](https://github.com/jwatml/LRC)

Footnote 17: [https://github.com/jwatml/GRADE](https://github.com/jwatml/GRADE)

Footnote 18: [https://github.com/jwatml/GRADE](https://github.com/jwatml/GRADE)

Footnote 19: [https://github.com/Shen-Lab/CbA-SpecReg](https://github.com/Shen-Lab/CbA-SpecReg)

Footnote 20: [https://github.com/jwatml/GRADE](https://github.com/jwatml/GRADE)

Footnote 21: [https://github.com/jwatml/GRADE](https://github.com/jwatml/GRADE)

Footnote 22: [https://github.com/tim-learn/SHOT](https://github.com/tim-learn/SHOT)

Footnote 23: [https://github.com/jwatml/GRADE](https://github.com/jwatml/GRADE)

Footnote 24: [https://github.com/jwatml/GRADE](https://github.com/jwatml/GRADE)

Footnote 25: [https://github.com/jwatml/GRADE](https://github.com/jwatml/GRADE)

Footnote 26: [https://github.com/jwatml/GRADE](https://github.com/jwatml/GRADE)

Footnote 27: [https://github.com/cslanghpan/CbA](https://github.com/cslanghpan/CbA)

#### c.2.4. Multi-source-free.

* **DECISION18**, **CAiDA19** and **MSFDA20** are three recent multi-source-free domain adaptation models that are particularly designed for i.i.d image classification task. We replace their backbone with GCN as other baselines. For CAiDA, we use cosine similarity as the distance function to construct confident anchors. For MSFDA, we reuse their default hyperparameters.

Footnote 20: [https://github.com/cslanghpan/CbA](https://github.com/cslanghpan/CbA)

Footnote 21: [https://github.com/cslanghpan/CbA](https://github.com/cslanghpan/CbA)

## Appendix D Time and Space Complexity Comparisons

We compare our proposed model with DECISION [1], a representative model-centric multi-source-free domain adaptation method. Specifically, given a graph with \(n\) nodes and \(\epsilon\) edges, the node representation dimension is set as \(d\) and GNN has \(L\) layers. \(K\) is the number of categories. Then, if we have \(m\) source models from different domains, the feature encoder has the time complexity of \(O(Lmnd^{2}+Lmed)\). The time complexity of calculating cluster centroids for \(m\) models is \(O(mmd)\). Computing the pseudo-label of each sample by assigning it to its nearest cluster centroid has a time complexity of \(O(mnKd)\) for \(m\) models. Thus, the overall time complexity is \(O(Lmnd^{2}+Lmed+md+mnKd)\). Meanwhile, to store \(m\) models' predictions and node representations, it requires a space complexity of \(O(mnd+mnK)\). The time complexity of our proposed GraphATA has been discussed in the end of Section 4. For ease of comparison, we present the time and space complexity of DECISION and GraphATA in Table 6. Among them, \(|G|\) refers to the model parameters of the Graph Neural Network (GNN), while \(|S|\) represents the additional parameters of our model, such as \(W_{g}^{l}\) and \(a^{l}\). Our model utilizes aggregated weight matrices to perform graph convolution once, while DECISION performs graph convolution \(m\) times and then aggregates their predictions. Similarly, GraphATA maintains one representation bank and one prediction bank, while DECISION stores them \(m\) times. Thus, GraphATA has lower time and space complexity compared with DECISION. It's worth noting that other baselines like CAiDA [10] and MSFDA [39] exhibit higher time complexity compared to DECISION, because they utilize more complicated strategies to generate pseudo-labels. In summary, our model demonstrates the lowest time and space complexity among the compared multi-source-free methods.

Figure 6. Hyper-parameter sensitivity analysis.

### More Ablation Studies and Experiments

**Hyperparameter Analyses.** We present the sensitivities of node representation dimension and number of nearest neighbors in Figure 6. Particularly, node dimension \(d=128\) and \(r=40\), our model could always obtain the satisfied performance. As we can see, accuracy improves consistently as the node representation dimension increases from \(32\) to \(128\), then slightly drops at \(256\). For number of nearest neighbors, the accuracy remains relatively stable, fluctuating between \(75\%\) and \(80\%\), with a slight dip at \(20\) neighbors, then recovering toward \(40\) neighbors. This is because too few nearest neighbors could not provide sufficient supervision information, while too many nearest neighbors might introduce noises into the generation process. While there are fluctuations, they remain within a reasonable range, as our sensitivity analysis covering a wide range of nearest neighbor values.

**The Effect of Different Aggregation Strategies for Local Contexts.** The mean operation is chosen for its simplicity in aggregating information from neighboring nodes. It provides a straightforward way to capture the average characteristics of the neighborhood, which is widely adopted in many graph-based learning models. While it is true that the mean operation can sometimes result in similar \(\mathbf{c}\) values for nodes with different types of neighbors, our ablation studies demonstrate that our model maintains high performance when compared to other aggregation strategies such as max, min and sum, as shown in the Table 7. These results highlight the robustness of our mean aggregation approach despite its simplicity.

**Additional Experimental Results.** We conduct experiments on a larger citation dataset **ogbn-arxiv** for node classification task, which contains \(169,343\) nodes and \(1,166,243\) edges from \(40\) classes. The dataset is chronologically divided into five groups according to the publication years of the papers. _We construct three source graphs encompassing papers published before 2011, during the periods 2011-2014 and 2014-2016, while the two target graphs are derived from the periods 2016-2018 and 2018-2020._ The results are presented in Table 8. As demonstrated in the table, our proposed GraphATA consistently

\begin{table}
\begin{tabular}{l c c} \hline \hline  & DECISION & GraphATA \\ \hline Time & \(O(Lmmd^{2}+Lmed+mnd+mnkCd)\) & \(O(Lnd^{2}+Led+mnd+mlog(m)+dndlog(n))\) \\ Space & \(O(m|G|+mnd+mnk)\) & \(O(m|G|+nd+nK+|S|)\) \\ \hline
**Algorithm 1:** Pseudocode of GraphATA & & \\ \hline
**Input** :\(m\) source pretrained graph neural network models \(\{\Phi_{1},\cdots,\Phi_{m}\}\) and an unlabelled target graph \(\mathcal{G}=(\mathbf{A},\mathbf{X})\) or a set of & & \\   unlabeled graphs \(\{\Phi_{1},\cdots,\mathcal{G}_{n}\}\) & & \\   Output :\(model\)\(\Phi_{t}\) for target graph classification & & \\   Load model weights from source pretrained models \(\{\mathbf{W}_{1}^{l},\cdots,\mathbf{W}_{m}^{l}\}_{l=1,\cdots,L}\) & & \\  **while** _not converged or not reached the maximum epochs_ & & \\  **for** _layer \(l\gets 1\)_**to** \(L\)**do** & & \\   Compute personalized graph convolutional matrix for each node \(v\) via Eq. (4) & & \\   Perform graph convolution at layer \(l\) & & \\   Compute node representations \(\mathbf{Z}\in\mathbb{R}^{n\times k}\) and predictions \(\mathbf{P}\in\mathbb{R}^{n\times C}\) with \(\Phi_{t}\) & & \\   Calculate loss \(\mathcal{L}\) according to Eq. (12) and update model \(\Phi_{t}\)’s parameters & & \\   Update memory banks \(\mathcal{R}\) and \(\mathcal{P}\) via a momentum manner & & \\   Compute predictions \(\mathbf{Y}=\Phi_{t}(\mathbf{A},\mathbf{X})\) with optimized model \(\Phi_{t}\) & & \\ \hline \hline \end{tabular}
\end{table}
Table 6. Comparisons of time and space complexity.

\begin{table}
\begin{tabular}{l|c c} \hline \hline Methods & 2016-2018 & 2018-2020 \\ \hline GCN [(20)] & 55.57\(\pm\)0.09 & 53.03\(\pm\)0.16 & 1839 \\ SOGA [(33)] & 58.10\(\pm\)0.23 & 52.28\(\pm\)0.12 & 1822 \\ DECISION [(1)] & 59.53\(\pm\)0.17 & 57.55\(\pm\)0.34 & 1839 \\ CAiDA [(10)] & 58.42\(\pm\)0.14 & 56.19\(\pm\)0.38 & 1846 \\ MSFDA [(39)] & 61.78\(\pm\)0.87 & 59.91\(\pm\)0.20 & 1869 \\ GraphATA & **63.55\(\pm\)0.94** & **60.85\(\pm\)0.13** & 1869 \\ \hline \hline \end{tabular}
\end{table}
Table 7. GraphATA results with different local contexts.

\begin{table}
\begin{tabular}{l|c c} \hline \hline Methods & T1,T2,T4\(\rightarrow\)T3 & T1,T2,T3\(\rightarrow\)T4 \\ \hline DECISION [(1)] & 31.90\(\pm\)0.61 & 19.40\(\pm\)0.23 \\ CAiDA [(10)] & 40.23\(\pm\)0.39 & 17.05\(\pm\)0.77 & 1873 \\ MSFDA [(39)] & 39.46\(\pm\)0.59 & 19.04\(\pm\)1.46 & 1874 \\ GraphATA & **43.08\(\pm\)0.77** & **22.49\(\pm\)0.34** & 1875 \\ \hline \hline \end{tabular}
\end{table}
Table 8. GraphATA results on ogbn-arxiv datasets.

\begin{table}
\begin{tabular}{l c c} \hline \hline Methods & T1,T2,T4\(\rightarrow\)T3 & T1,T2,T3\(\rightarrow\)T4 \\ \hline DECISION [(1)] & 31.90\(\pm\)0.61 & 19.40\(\pm\)0.23 \\ CAiDA [(10)] & 40.23\(\pm\)0.39 & 17.05\(\pm\)0.77 & 1873 \\ MSFDA [(39)] & 39.46\(\pm\)0.59 & 19.04\(\pm\)1.46 & 1874 \\ GraphATA & **43.08\(\pm\)0.77** & **22.49\(\pm\)0.34** & 1875 \\ \hline \hline \end{tabular}
\end{table}
Table 9. GraphATA results on TRIANGLE datasets.

exhibits effective performance across various adaptation scenarios within this large-scale citation dataset.

We also choose the **TRIANGLE** dataset, a large-scale dataset from TUDataset for graph classification task, which consists of 45000 graphs across 10 classes. Then, we partition it into four equally size disjoint groups based on density shift (i.e., T1,T2,T3,T4) and compare our model with 3 recent multi-source-free baselines. The results are demonstrated in Table 9. As shown in the table, our proposed model consistently demonstrates strong performance across a range of adaptation scenarios within this large-scale dataset.

Finally, Table 10 presents all the adaptation results on social datasets and TUDatasets, which further verify the effectiveness of our proposed GraphATA under different settings.

**GPU Consumption and Training Time per Iteration.** We measured the GPU consumption and training time per epoch for our proposed GraphATA and compared it with representative methods as follows. GCN is trained independently on each labeled source domain and then directly evaluated on the target domain without any adaptation process. Therefore, we do not report the GPU consumption and training time for GCN. We also do not report the results of single-source-free models, as they require adapting each of the \(m\) source models to the target domain individually. Instead, we focus on reporting the adaptation time for source-needed as well as multi-source-free models to provide a meaningful comparison of their performance in adapting to the target domain. The results are shown in Table 11 and Table 12, which is consistent with our analyses in previous sections.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline Models & C1,C2,C4 & C1,C3,C4 & C2,C3,C4 & P1,P2,P4 & P1,P3,P4 & P2,P3,P4 \\  & \(\rightarrow\) C3 & \(\rightarrow\) C2 & \(\rightarrow\) C1 & \(\rightarrow\) P3 & \(\rightarrow\) P2 & \(\rightarrow\)P1 \\ \hline MDAN & 89.65\(\pm\)0.15 & 91.88\(\pm\)0.31 & 91.33\(\pm\)0.28 & 67.45\(\pm\)0.17 & **77.66\(\pm\)0.18** & 54.10\(\pm\)0.25 \\ GCN & 86.14\(\pm\)0.16 & 87