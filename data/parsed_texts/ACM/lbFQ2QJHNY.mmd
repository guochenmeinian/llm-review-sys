# Boosting Graph Convolution with Disparity-induced Structural Refinement

Anonymous Author(s)

Submission Id: 11

###### Abstract.

Graph Neural Networks (GNNs) have expressed remarkable capability in processing graph-structured data. Recent studies have found that most GNNs rely on the homophily assumption of graphs, leading to unsatisfactory performance on heterophilous graphs. While certain methods have been developed to address heterophilous links, they lack more precise estimation of high-order relationships between nodes. This could result in the aggregation of excessive interference information during message propagation, thus degrading the representation ability of learned features. In this work, we propose a Disparity-induced Structural Refinement (DSR) method that enables adaptive and selective message propagation in GNN, to enhance representation learning in heterophilous graphs. We theoretically analyze the necessity of structural refinement during message passing grounded in the derivation of error bound for node classification. To this end, we design a disparity score that combines both features and structural information at the node level, reflecting the connectivity degree of hopping neighbor nodes. Based on the disparity score, we can adjust the aggregation of neighbor nodes, thereby mitigating the impact of irrelevant information during message passing. Experimental results demonstrate that our method achieves competitive performance, mostly outperforming advanced methods on both homophilous and heterophilous datasets.

Graph neural network, homophily and heterophily, structural learning, message passing. +
Footnote †: ccs: Computing methodologies Neural networks

+
Footnote †: ccs: Computing methodologies Neural networks

+
Footnote †: ccs: Computing methodologies Neural networks

+
Footnote †: ccs: Computing methodologies Neural networks

+
Footnote †: ccs: Computing methodologies Neural networks

+
Footnote †: ccs: Computing methodologies Neural networks

+
Footnote †: ccs: Computing methodologies Neural networks

+
Footnote †: ccs: Computing methodologies Neural networks

+
Footnote †: ccs: Computing methodologies Neural networks

+
Footnote †: ccs: Computing methodologies Neural networks

+
Footnote †: ccs: Computing methodologies Neural networks

+
Footnote †: ccs: Computing methodologies Neural networks

+
Footnote †: ccs: Computing methodologies Neural networks

+
Footnote †: ccs: Computing methodologies Neural networks

+
Footnote †: ccs: Computing methodologies Neural networks

+
Footnote †: ccs: Computing methodologies Neural networks

+
Footnote †: ccs: Computing methodologies Neural networks

+
Footnote †: ccs: Computing methodologies Neural networks

+
Footnote †: ccs: Computing methodologies Neural networks

## 1. Introduction

Graph-structured data are prevalent in the real world, exemplified by social networks and molecular structures. To effectively address such non-Euclidean data, Graph Neural Networks (GNNs) have emerged as powerful tools, extensively applied across various domains, including traffic prediction (Hamilton et al., 2017; Kipf and Welling, 2016; Hamilton et al., 2017), molecular exploration (Hamilton et al., 2017; Kipf and Welling, 2016; Hamilton et al., 2017), classification and clustering (Hamilton et al., 2017; Kipf and Welling, 2016; Hamilton et al., 2017) and others (Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017). As a pivotal stage of GNNs, message passing transforms and disseminates information through the graph's topology, significantly enhancing the expressiveness of learned feature. Most GNNs (Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017) are designed under the homophily assumption, where nodes with similar labels or features tend to be connected. However, real-world applications frequently involve highly heterophilous graphs, such as in the case of amino acids of different types forming connections. Consequently, many previous GNNs proposed for homophilous networks, such as GEGCN (Golov et al., 2013), JKNet (Wang et al., 2016) and APPNP (Hamilton et al., 2017), struggle to effectively capture heterophily, resulting in an unsatisfactory performance on heterophilous networks.

Real-world graphs typically contain both homophilous and heterophilous edges. A graph is considered homophilous when the former outnumber the latter; otherwise, it is viewed as heterophilous. Recent studies have revealed that the smoothing operation inherent in GNNs can generate similar node features for nodes with different labels, when applied to graphs with heterophily (Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017). To mitigate the negative impact of this issue on node classification tasks, various designs have been developed to enhance the discriminative capabilities of GNNs in heterophilous scenarios. One typical strategy is to construct augmented graphs by introducing additional semantics to prevent nodes from different classes from adopting similar representations. For instance, Huang et al. (Huang et al., 2017) utilized known edge labels to identify other links, thus facilitating message passing by removing all heterophilous edges. Pei et al. (Pei et al., 2018) redefined graph convolution by utilizing geometric relationships in the latent space. These methods primarily focus on the detrimental effects of heterophilous connections. However, they often overlook the potential advantages of effectively identifying and leveraging heterophilous edges.

Another established approach involves learning signed edges to cluster similar nodes while repelling dissimilar ones, which reloates edges and facilitates message passing adopting the whole graph topology. In this context, homophilous relationships are assigned positive signs, whereas heterophilous connections are designated negative signs. For instance, graph attention functions were used to compute signed edges such that node representations were better learned (Hamilton et al., 2017; Hamilton et al., 2017). To better define signed edges, (Hamilton et al., 2017; Hamilton et al., 2017) designed both low-pass and high-pass filters to differentiate between various connections. In graphs with high heterophily, direct neighbors often exhibit greater heterophily than multi-hop neighbors. Consequently, these algorithms aggregate high-order information by applying signed convolutional filters multiple times, effectively utilizing edges with assigned signs and weights for information propagation and fusion. Nevertheless, these methods encounter several limitations in the context of heterophilous graphs: i) They solely rely on node features to infer node relationships, neglecting structure information, which can easily establish inaccurate estimation; ii) The interplay between the discriminative capacityof models and the nature of homophilous/heterophilous graphs remains unclear.

To address the issues aforementioned, we propose a Disparity-induced Structural Refinement (DSR) framework with the integration integrated with GNN, named DSR-GNN, aimed at enhancing node representations in heterophilous graphs. Grounded in the theory of error bound for node classification, we first conduct a theoretical analysis of the factors affecting the model's capacity to handle heterophilous graphs, underscoring the necessity of exploring refined graph structures. Our proposed architecture integrates two collaborative steps: assessing node relationships and performing message passing on refined graphs. In the initial step, we evaluate high-order node relationships in graphs by calculating a disparity score that combines distances of aggregated features and differences in homophily ratios. Subsequently, the score drives the construction of layer-wise adjacency edges by removing links with significant disparity. This refinement process ensures that message passing is conducted on graphs with minimized interference from irrelevant high-order information. Notably, the updated node representations from message passing can, in turn, update the disparity score. Together, these two collaborative steps facilitate the attainment of more discriminative node representations.

Our contributions can be summarized in three aspects:

1. We propose a disparity-induced structural refinement framework, theoretically dissecting the relationship between model capacity and homo/heterophilous ratios, to enhance representation learning in heterophilous graphs.
2. We propose a disparity score that integrates both features and structural information at the node level, facilitating structural refinement and mitigating the impact of irrelevant information during message passing.
3. Extensive experiments demonstrate that the proposed model achieves state-of-the-art performance on heterophilous graphs and competitive accuracy on homophilous networks.

**Overview.** In the remainder of this paper, we first introduce the primary preliminaries used in the paper in Section 2. Following this, Section 3 analyzes the theoretical background of our research, and Section 4 presents our framework DSR-GNN. Finally, we conduct extensive experiments in Section 5 and conclude our work in Section 6.

## 2. Preliminaries

### Notations

Given an undirected graph \(\mathcal{G}(V,E)\) with \(N\) nodes (\(\{v_{i}\in V|_{i=1}^{N}\}\)) and \(e\) edges, where \(V=V_{\text{lab}}\cup V_{\text{unlab}}\) with labeled node set \(V_{\text{lab}}\) and unlabeled node set \(V_{\text{unlab}}\), and \(e_{ij}\in E\) denotes the edge between the \(i\)-th and \(j\)-th nodes. The topological relationships among nodes are expressed as \(\mathbf{A}\in\mathbb{R}^{N\times N}\), where \(A_{ij}=1\) if nodes \(i\) and \(j\) are connected, \(0\) otherwise. Moreover, \(\bar{\mathbf{A}}=\mathbf{A}+\mathbf{I}\) stands for \(\mathbf{A}\) with added self-loops, while \(\bar{\mathbf{A}}=\bar{\mathbf{D}}^{-1/2}\bar{\mathbf{A}}^{-1/2}\) denotes the symmetric normalized adjacency matrix. Note that the renormalization trick on the adjacency matrix is used to prevent gradient explosion. Here, \(\hat{\mathbf{D}}\) is the diagonal degree matrix, where \(\hat{D}_{ii}=\sum_{j=1}^{N}\hat{A}_{ij}\). \(\mathbf{X}\in\mathbb{R}^{N\times d}\) indicates node features, in which \(\mathbf{x}_{i}\) with \(d\)-dimensions is the feature vector of the \(i\)-th node. Among the \(N\) nodes, \(N_{\text{lab}}\) nodes are labeled, with their labels captured in the ground truth matrix \(\mathbf{Y}\in\mathbb{R}^{N_{\text{lab}}\times C}\), where \(C\) is the number of classes, and each row \(\mathbf{y}_{i}\) of \(\mathbf{Y}\) is a one-hot vector representing the label of node \(v_{i}\).

### Node-level Homophily and Heterophily

Given a set of nodes with labels, the homophily ratio of each node calculates the tendency of the node to have the same label as its neighbors. Considering node \(v_{i}\), we assume its neighbor set as \(\mathcal{N}_{i}\), then the homophily ratio of node \(v_{i}\) is defined as: \(h_{i}^{+}=\frac{|(\mathbf{Y}_{i}\mathbf{y}_{i})|_{2}\sigma(\mathbf{X}_{i})|}{| \mathcal{N}_{i}|}\). \(h_{i}^{+}\) ranges in \([0,1]\), with values close to \(1\) indicating high homophily (or low heterophily) and values close to \(0\) indicating the opposite. Corresponding, the heterophily ratio \(h_{i}^{-}=1-h_{i}^{+}\). Therefore, the node-level homophily in the graph \(\mathcal{G}\) can be measured by \(\mathcal{H}(\mathcal{G})=\frac{\sum_{i=1}^{N}h_{i}^{+}}{N}\). Many previous works have explored heterophily using the above node-level homophily metric and have proposed various approaches, such as signed edges, to reduce the impact of confusing information brought by non-similar neighbors (Bang et al., 2017; Wang et al., 2018).

### Graph Neural Network for Semi-supervised Classification

The core of GNN is the message passing, which collects the neighborhood information to update node representations. Consider a GNN with \(L\) layers, where the output of the \(l\)-th layer is given by: \(\mathbf{h}_{i}^{(l)}=\sigma(\text{Aggregate}(\{\mathbf{h}_{j}^{(l-1)}|\hat{A} _{ij}=1)})\mathbf{\Theta}^{(l)})\). Here, \(\mathbf{\Theta}^{(l)}\) is the trainable parameter matrix of the \(l\)-th layer and \(\sigma(\cdot)\) indicates the ReLU(\(\cdot\) or Softmax(\(\cdot\)) activation function. After gaining the final representation \(\mathbf{H}^{(L)}\), the cross-entropy loss consisted of \(\mathbf{H}^{(L)}\) and \(\mathbf{Y}\) is attained: \(\mathcal{L}_{ce}=-\sum_{i\in\Omega}\sum_{j=1}^{C}V_{ij}\ln(H_{ij}^{(L)})\). Here, \(\Omega\) is the set of labeled samples. To simplify the model parameter, some models (Bang et al., 2017; Wang et al., 2018) firstly use the fully connected neural network on the feature matrix \(\mathbf{X}\) to generate the hidden state features \(\mathbf{H}^{(0)}\) and then propagate them via the message passing. Their updating rule can be defined as: \(\mathbf{H}^{(l)}=\sigma(\overline{\mathbf{A}}\mathbf{H}^{(l-1)}+\mathbf{H}^{( 0)}),\mathbf{H}^{(0)}=\Phi_{\theta}(\mathbf{X})\), where \(\theta\) is the parameter set of the neural network \(\Phi\).

## 3. Theoretical Disparity Analysis

Classic graph convolution methods typically assume that nodes belonging to the same class are more likely to be connected, which fails to hold in heterophilous graphs. To investigate the factors affecting the model's ability to differentiate between nodes, we derive an error bound for node classification, which can boost the design of effective message-passing for heterophilous graphs. Theoretically, drawing inspiration from PAC-Bayes analysis (Bang et al., 2017; Wang et al., 2018), we delineate the key assumptions and definitions related to graph data and classifiers, followed by a thorough derivation of the error bound applicable to any unlabeled nodes.

Definition 1 ().: _Let's define a \(L\)-layer GNN classifier \(f\), for node \(v_{i}\) the prediction score is \(f_{i}(\mathbf{X},\mathcal{G})=f(g_{i}(\mathbf{X},\mathcal{G});\mathbf{\Theta}^{(1 )},\mathbf{\Theta}^{(2)},\cdots,\mathbf{\Theta}^{(L)})\), where \(g\) denotes a feature aggregation function and \(f\) is a ReLU-activated \(L\)-layer MLP with learnable parameters \(\{\mathbf{\Theta}^{(l)}\}_{l=1}^{L}\). We assume that the maximum number of hidden units across all layers is \(b\)._

**Definition 2**.: _For any node \(v_{i}\), the distance of aggregated features from it to other node \(v_{j}\) is defined as_

\[\epsilon_{ij}=\|g_{i}(\mathbf{X},\mathcal{G})-g_{j}(\mathbf{X},\mathcal{G})\|_{ 2}. \tag{1}\]

**Definition 3**.: _Given a labeled node \(v_{j}\in V_{lab}\) with label \(y_{j}\), there exists a margin \(\gamma\geq 0\) satisfing_

\[f_{j}(\mathbf{X},\mathcal{G})\{y_{j}\}\leq\gamma+\max_{c\neq y_{i}}f_{j}( \mathbf{X},\mathcal{G})\{c\}, \tag{2}\]

_where \(f_{j}(\mathbf{X},\mathcal{G})\{\cdot|\cdot\}\) is to take an element of the predicted probability vector (w.r.t classifier)._

**Definition 4**.: _The expected loss \(\mathcal{L}_{i}^{\gamma}(f)\) of the classifier \(f\) on \(v_{i}\) for a margin \(\gamma\) and any distribution \(\mathcal{D}\) is defined as (Zhu et al., 2017; 2018):_

\[\mathcal{L}_{i}^{\gamma}(f):=\mathbb{P}_{n\sim\mathcal{D}}\Big{[}f_{i}( \mathbf{X},\mathcal{G})\{y_{i}\}\leq\gamma+\max_{c\neq y_{i}}f_{i}(\mathbf{X},\mathcal{G})\{c\}\Big{]}. \tag{3}\]

_The empirical loss is denoted as \(\hat{\mathcal{L}}_{i}^{\gamma}(f)\) that is the empirical estimate of the expected loss._

According to the above definitions, the error bound for semi-supervised node classification is illustrated as below. It aims to bound the expected loss \(\mathcal{L}_{i}^{0}\) of classifier on the unlabeled node \(v_{i}\) for a margin \(\gamma\) in Here, the empirical loss on the labeled node \(v_{j}\) for a margin \(\gamma\) is denoted as \(\hat{\mathcal{L}}_{j}^{\gamma}\).

**Theorem 1** (Error Bound for Unlabeled Node Classification).: _Let \(f\) be a classifier in the classifier family \(\mathcal{F}\) with learnable \(\{\mathbf{\Theta}^{(1)}\}_{l=1}^{L}\) that conform with the normal distribution, then for any unlabeled node \(v_{i}\) and \(\gamma\geq 0\), we have_

\[\mathcal{L}_{i}^{0}(f)\leq\hat{\mathcal{L}}_{j}^{\gamma}(f)+ \mathcal{O}\Big{(}\frac{C\rho}{\sqrt{2\pi}\sigma}(\epsilon_{ij}+\rho|h_{i}^{ \gamma}-h_{j}^{\gamma}|)\] \[+\frac{\sum_{l=1}^{L}\|\mathbf{\Theta}^{(l)}\|_{F}^{2}}{\sigma^{2 }}\Big{)}, \tag{4}\]

_where \(\sigma=\min\Big{(}\frac{(\gamma/\delta\epsilon_{ij})^{1/L}}{\sqrt{2b(1+\text{ in}(2bL))}},\frac{\gamma}{s4Lb_{i}\rho^{k-\sqrt{b}\ln(4bL)}}\Big{)},B_{i}=\|g_{i}( \mathbf{X},\mathcal{G})\|_{2},\)\(h_{i}^{+}\) denotes the homophily ratio of node \(v_{i}\) and \(\rho\) is original feature separability of nodes._

Proof.: The proof is deferred to **Appendix**. 

This theorem elucidates that the primary factors influencing the error bound are the distance of aggregated feature \(\epsilon_{ij}=\|g_{i}(\mathbf{X},\mathcal{G})-g_{j}(\mathbf{X},\mathcal{G})\|_{ 2}\) and the disparity in homophily ratios1: \(|h_{i}^{+}-h_{j}^{\gamma}|\). Conventional GNN methods primarily emphasize minimizing the distance of aggregated feature to enhance representation learning, often neglecting the significance of homophily ratios, which fundamentally reflect the underlying graph structure.

Footnote 1: \(|h_{i}^{+}-h_{j}^{\gamma}|=|h_{i}^{\gamma}-h_{j}^{\gamma}|,\) as \(h_{i}^{+}+h_{i}^{\gamma}=1\).

**Remark 1**.: _In previous studies, two key aspects of debate have emerged regarding heterophily (conversely homophily) in graph convolution. One perspective asserts that heterophily is detrimental to message passing, as connections between nodes of different classes can lead to mixed features, resulting in indistinguishable node representations (Ashman et al., 2017; 2018). Another viewpoint posits that heterophily edges can be advantageous, as they not only enhance the differentiation of inter-class information but also facilitate long-distance message passing (Bahman et al., 2017; 2018). Different from them, according to Theorem 1, we should consider the **disparity** of structure (\(|h_{i}^{k}-h_{j}^{\gamma}|\)) and feature (\(\epsilon_{ij}\)) between nodes during message passing to balance advantages and disadvantages of heterophilous links, rather than simply adjusting heterophily/homophily._

To this end, we attempt to devise an effective structural adjustment strategy that leverages the disparity of homophily ratios as well as the distance of aggregated features. This strategy aims to reduce error bounds and enhance the discriminative capacity of the model. The central idea is to refine graph structures to mitigate the influence of irrelevant high-order information while facilitating more meaningful message passing, thereby improving the model's discernibility.

## 4. Disparity-Induced Structural Refinement

In this section, we present the disparity-induced structural refinement method, designed for integration with graph neural network, inspired by the insights from Theorem 1. This method consists of three critical steps: evaluating edge signs, computing the disparity score, and adjusting message propagation. We finally aggregate the node representations updated across all refined graphs to obtain the final predicted results.

### Assign Homo/Heterophile Edges

In order to estimate the node-level homophily ratio, it is essential to annotate the homophily and heterophily properties of the \(k\)-hop neighboring nodes surrounding a given node. It sometimes aligns with the concept of signed edges, which could enhance the purity of neighbor information gathered during message aggregation. Specifically, we assign positive signs to edges connecting nodes of the same class (i.e., homophilous edges) and negative signs to those linking nodes of distinct categories (i.e., heterophilous edges), By doing so, the use of signed edges allows the model better capture graph structure, thereby improving discrimination between nodes belonging to distinct classes. Incorrectly assigning a negative sign to a homophilous edge or a positive sign to a heterophilous edge can not only hinder model performance but may also lead to degradation, as demonstrated in GGCN (2018). Therefore, accurately matching signs to edges is paramount. To address this, we propose a pre-training process that enhances the accuracy of signed edges, effectively mitigating the influence of noise in the raw data, rather than merely relying on the cosine similarity of the original node features as utilized in (2018).

Concretely, we learn a way of generating signed edges from the training set, leveraging the set of labeled nodes. Let \(E_{\text{lab}}\) denotes the set of edges just that exist solely between labeled samples. We can then define a signed matrix \(\mathbf{W}\in\mathbb{R}^{N\times N}\) restricted to the labeled edges during the training phase, with elements drawn from the set \(\{-1,0,1\}\). Formally,

\[W_{ij}=\left\{\begin{array}{ll}1,&\text{if }v_{i},v_{j}\in V_{\text{lab}} \&\text{\& }e_{ij}\in E_{\text{lab}}\&\text{\& }\mathbf{y}_{i}=\mathbf{y}_{j},\\ -1,&\text{if }v_{i},v_{j}\in V_{\text{lab}}\&\text{\& }e_{ij}\in E_{\text{lab}}\&\text{\& }\mathbf{y}_{i}\neq\mathbf{y}_{j},\\ 0,&\text{otherwise}.\end{array}\right. \tag{5}\]

Eq. (5) indicates the true signed edges for the training phase. In order to learn a prediction model of signed edges, we concatenate the representations of two connected nodes to form the feature of the corresponding edge. Formally, the feature of the edge connectingnodes \(v_{i}\) and \(v_{j}\) is denoted as \([\mathbf{x}_{i}||\mathbf{x}_{j}]\), where \(||\) represents vector concatenation. To predict the sign of each edge, we input these edge features into a multi-layer perceptron (MLP) as follows,

\[\widetilde{W}_{ij}\leftarrow\text{sgn}(\text{Tanh}(\text{MLP}(\{\mathbf{x}_{i} ||\mathbf{x}_{j}\}))), \tag{6}\]

where "sgn" is the sign function, "Tanh" is the hyperbolic tangent activation function that maps values to the range [-1, 1]. We optimize the MLP through gradient backpropagation on the Mean Squared Error (MSE) loss, defined as: \(\mathcal{L}_{mse}=\frac{1}{|\mathbf{x}_{i}|}\sum_{(v_{i},v_{j})\in\mathbf{E}_ {\text{lab}}}(W_{ij}-\widetilde{W}_{ij})^{2}\). Based on the learnt prediction model, we can generate a signed matrix \(\widetilde{\mathbf{W}}\), whose elements are whether predicted signs of unsigned edges (in testing) or true signed edges (in training).

**Remark 2**.: _The pre-training procedure of edge assignment described above utilizes existing training edges, whereby labeled samples are interconnected. In the absence of such conditions, our model can proceed without pre-training, instead estimating edge signs based on the similarity between nodes. Following the prediction of edge signs using \(\widetilde{\mathbf{W}}\), we can gain the final representations through a step-wise integration of various high-order neighbor signals._

### Compute Disparity Scores

Building on Theorem 1, we conclude that the classification error is primarily influenced by the distance between aggregated features and the disparity in homophily ratios. To address this, we incorporate these two critical factors into a unified disparity score, the calculation of which is detailed in this subsection.

After learning the signed matrix \(\widetilde{\mathbf{W}}\) with Eq. (6), the \(k\)-hop homophily ratio of node \(v_{i}\) is defined as

\[h_{i}^{(k)}=\frac{|\{v_{j}|v_{j}\in\mathcal{N}_{i}^{(k)},\widetilde{W}_{ij}^{( k)}>0\}|}{|\mathcal{N}_{i}^{(k)}|}, \tag{7}\]

where the superscript \((k)\) denotes the \(k\)-hop neighbors2. Hereby, we can compute the \(l\)-hop disparity score between node \(v_{i}\) and its neighbor \(v_{j}\) as follows,

Footnote 2: Different hopping levels utilize distinct sign matrices.

\[S_{ij}^{(l)}=\|\mathbf{h}_{i}^{(l-1)}-\mathbf{h}_{j}^{(l-1)}\|_{2}+|\mathbf{h} _{i}^{(l)}-\mathbf{h}_{j}^{(l)}|. \tag{8}\]

The first term represents the aggregated-feature distance with \(\mathbf{h}_{i}^{(l)}=\text{ReLU}\left(g(\{\mathbf{h}_{j}^{(l-1)}|v_{j}\in \mathcal{N}_{i}^{(l)}\})\right)\), where \(g(\cdot)\) is an aggregation function. The second term encapsulates the disparity in homophily ratios, reflecting the differences of neighbor substructure surrounding nodes \(v_{i}\) and \(v_{j}\). The score reflects the disparity between a given node and its the \(l\)-hop neighbor nodes, both in terms of feature and structure spaces, thereby serving to guide message propagation along accurate paths for obtaining discriminative node representations.

### Adjust Message Propagation

According to disparity scores from Eq. (8), we adjust the aggregated neighboring nodes to mitigate the latent noise from surrounding neighbor information. Formally, for a given node \(v_{i}\), the aggregated nodes in the \(l\)-th layer are defined as,

\[\mathcal{A}_{i}^{(l)} \leftarrow\{v_{j}|v_{j}\in\mathcal{N}_{i}^{(l)}\wedge S_{ij}^{(l )}\leq\tau_{i}^{(l)}\},\] \[\text{s.t.}, \tau_{i}^{(l)}=\frac{1}{|\mathcal{N}_{i}^{(l)}|}\sum_{v_{j}\in \mathcal{N}_{i}^{(l)}}S_{ij}^{(l)}, \tag{9}\]

where \(\tau_{i}^{(l)}\) represents the average score between node \(v_{i}\) and its \(l\)-order neighbors. The construction of \(\{\mathcal{A}_{i}^{(l)}\}_{l=1}^{L}\) is guided by disparity scores, preserving high-order neighbors that exhibit minimal differences to avoid the influence of irrelevant high-order information. Thus, given a set \(\mathcal{A}_{i}^{(l)}\) containing neighbors of node \(v_{i}\) to be aggregated in the \(l\)-th layer, we can perform message passing during graph convolution.

The message aggregation for the \(l\)-th layer is defined as follows,

\[\mathbf{h}_{i}^{(l)}=\] \[\text{gCov}(\sum_{v_{j}\in\{v_{1}\}\cup\mathcal{A}_{i}^{(l)}} \widetilde{W}_{ij}^{(l)}(D_{ii}^{(l)}D_{jj}^{(l)})^{-1/2}\mathbf{h}_{j}^{(l-1) },\mathbf{\Theta}_{1}), \tag{10}\]

where \(l=1,\cdots,L\), \(\mathbf{h}_{i}^{(0)}\) is gained using a fully-connected neural network with parameter \(\mathbf{\Theta}_{1}\in\mathbb{R}^{d\times m}\) on \(\mathbf{x}_{i}\), and "gCov" denotes a conventional graph convolution layer followed by a ReLU activation function. Here, \(D_{ii}^{(l)}\) represents the degree of node \(i\), calculated from the aggregation neighbor structure \(\mathcal{A}^{(l)}\) for the normalization purpose. Note that message aggregation is applied solely to the refined graph structures \(\mathcal{A}^{(l)}\) besides the node itself. Furthermore, the representation \(\mathbf{h}_{i}^{(l)}\), obtained by aggregating messages from \(\mathcal{A}_{i}^{(l)}\), can iteratively update the disparity scores used in the (\(l\)+1)-th layer.

Through multi-layer message propagation, the final output can be derived by aggregating features from all layers as:

\[\hat{y}_{i}=\text{Softmax}\left(\left(\sum_{l=0}^{L}\lambda_{i}\mathbf{h}_{i} ^{(l)}\right)\mathbf{\Theta}_{2}\right), \tag{11}\]

where \(\lambda_{l}\) is a learnable parameter that indicates the importance of features from each layer, and \(\mathbf{\Theta}_{2}\in\mathbb{R}^{m\times c}\) is a weight matrix optimized for predicting class scores. The final outputs are then combined with the labels from the training data to compute the cross-entropy loss, facilitating model optimization.

In conclusion, we begin by learning a signed matrix that reflects the homophily and heterophily of links, utilizing edges between labeled samples. Subsequently, based on the node-level homophily ratios provided by the signed matrix and the aggregated features obtained by graph convolution, we compute the disparity score revealing high-order relationships between nodes. The disparity score explores key links from both node features and structures to obtain improved graph structures. Message propagation and aggregation are then performed on these refined graphs to derive discriminative node representations. Network parameters are optimized through backpropagation of the cross-entropy loss consisted of the final output and the labels from the training data. Algorithm 1 summarizes the updating process of variables. The network is implemented in Pytorch and uses GPU acceleration to boost training efficiency.

### Connect to Other Methods

**DSR-GNN Vs. DropEdge**. DropEdge (Shi et al., 2019) constructs an augmented adjacency matrix by randomly removing partial edges, and the matrix is shared by all layers. Its strategy is sample and intuitive, but it doesn't fully leverage the inherent data structure of the graph. Compared to DropEdge, DSR-GNN uses the disparity score to guide the sampling procedure, ensuring that layer-wise adjacency matrices more accurately capture high-order relationships between nodes.

**DSR-GNN Vs. GPR-GNN**. Both GPR-GNN (Chen et al., 2019) and DSR-GNN adopt weighted summation to generate the final node representations. Differently, the key component of GPR-GNN lies in exploring learnable weights to adapt to the homophily or heterophily patterns within the graph; while DSR-GNN focuses on leveraging signed edges and structural refinement techniques. These schemes balance the topological propagation abilities with the inherent heterophily in the graph, resulting in more discriminative node representations.

**DSR-GNN Vs. GGCN**. GGCN (Shi et al., 2019) proposes two edge correction strategies based on the theoretical analysis, including structure-based and feature-based methods. The former rescales edge weights to satisfy the required node degree conditions; while DSR-GNN emphasizes exploring node-level high-order homophily structures. Moreover, its feature-based method leverages cosine similarity to gain edge signs, but DSR-GNN adopts a pre-training scheme to predict signs for unknown edges.

## 5. Experiments

In this section, we construct a series of experiments to assess the effectiveness of DSR-GNN. Our model is implemented in PyTorch on a workstation with AMD Ryzen 9 5900X CPU (3.70GHz), 64GB RAM and RTX 3090GPU (24GB caches). We answer several key questions via experiments:

* **Q1:** How does DSR-GNN perform on both homophilous and heterophilous datasets?
* **Q2:** What is the impact of signed edges on model performance in heterophilous and homophilous graphs?
* **Q3:** How can we empirically verify the influence of structural refinement driven by the disparity score on performance?
* **Q4:** In what ways does the refined graph differ structurally from the original graph?
* **Q5:** How does high-order information affect the final representations learned by DSR-GNN?

### Experimental Setups

Datasets. To validate the performance of DSR-GNN, we use three homophilous datasets: Cora, Citeseer and Pubmed (Yang et al., 2019), which are citation networks where nodes represent publications and edges correspond to citation links. Additionally, we test on four heterophilous datasets: Texas, Wisconsin, Cornell, and Actor (Yao et al., 2019). In the Texas, Wisconsin, and Cornell datasets, nodes represent webpages, and edges denote hyperlinks between them. For the Actor dataset, each node represents an actor, with edges indicating co-occurrence on the same Wikipedia page. A summary of the dataset statistics is provided in Table 1.

Competitors. We compare DSR-GNN against 13 algorithms: 1) A baseline: 2-layer MLP; 2) Two classic GNN models: vanilla GCN (He et al., 2016) and GAT (Vaswani et al., 2017); 3) Two recent models performing well on homophilous graphs: GCNII (Chen et al., 2019) and GCNet (Vaswani et al., 2017); 4) Seven advanced models designed specifically to handle heterophily: FACON (Chen et al., 2019), H\({}^{2}\)GCN (Chen et al., 2019), GPR-GNN (Chen et al., 2019), GGGN (Shi et al., 2019), ACM-GCN (Yao et al., 2019), LRGNN(Yao et al., 2019) and PCNet (Chen et al., 2019).

Experimental Settings. We exploit accuracy (ACC) as the evaluation metric to measure the model's performance in correctly classifying samples. For all datasets, we randomly split training/validation/testing samples into 48%/32%/20% of all samples. For heterophilous datasets, the learning rate, weight decay, dropout rate and number of hidden units are set to 0.01, 5e-4, 0.1 and 128, respectively. For homophilous graphs, the configurations are largely analogous, with the exception that the weight decay is set to 0. Each experiment is performed 10 times, and the mean and standard deviation are recorded. Our code is available at **[https://anonymous.4open.science](https://anonymous.4open.science) /r/DSR-GNN-5876**.

### (Q1) Classification Results on Benchmark Datasets

Table 2 presents a comparison of test accuracy across all algorithms on real-world datasets with different homophily levels. From this table, we draw the following observations:

* DSR-GNN obtains the optimal and suboptimal performance on most datasets, particularly on heterophilous networks. On homophilous datasets, DSR-GNN maintains its competitive performance, which is within 1% of the best model.
* MLP is a solid baseline for handling heterophilous graphs, outperforming models with implicit homophilous assumptions, such as GCNII and GCNet. This observation underscores that message passing over indistinguishable edges can negatively impact performance.

* The highest and second-highest results are achieved by models specifically designed to handle heterophilous graphs, suggesting that effectively harnessing heterophilous links can significantly improve model performance. Notably, DSR-GNN surpasses these models by successfully balancing propagation capabilities with the inherent heterophily of the graph.

### (Q2 & Q3) Ablation Study

To demonstrate the impact of signed edges on model performance, we evaluate the role of different adjacency matrices. We first construct JGNN, a variant of DSR-GNN that omits structural refinement. In Figure 1, "with Ori. Adj." denotes JGNN utilizing the original adjacency matrix without signed edges. "with Cos." and "with Pre." indicate JGNN using signs generated by the cosine similarity and the proposed pre-training method, respectively. The figure highlights several key points: 1) We note that JGNN with Pre. achieves superior performance across most datasets, particularly on heterophilous graphs. 2) Assigning signs to edges helps the model to distinguish neighbors, significantly enhancing the model's discriminative power. 3) JGNN with Pre. substantially outperforms JGNN using cosine similarity on both homophilous and heterophilous graphs. The suboptimal performance of JGNN with cosine similarity can be attributed to inaccuracies in sign prediction caused by noise in the raw data. Moreover, we observe that predicting edge signs using similarity shows only marginal improvements over the original adjacency matrix, and in some cases, even leads to performance degradation. This indicates that incorrectly assigning a negative/positive sign to a homophilous/heterophilous edge can adversely affect model performance.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Datasets & \#Nodes & \#Edges & \#Features & \#Classes & \#Training/Testing/Validation \\ \hline Citeseer & 3,327 & 4,676 & 3,703 & 7 & 1,597/1,065/665 & 642 \\ Cora & 2,708 & 5,278 & 1,433 & 6 & 1,300/867/541 & 64 \\ Pubmed & 19,717 & 44,327 & 500 & 3 & 9,464/6,309/3,944 & 64 \\ Texas & 183 & 295 & 1,703 & 5 & 885/59/36 & 64 \\ Wisconsin & 251 & 466 & 1,703 & 5 & 121/80/50 & 64 \\ Cornell & 183 & 280 & 1,703 & 5 & 121/80/50 & 64 \\ Actor & 7,600 & 26,752 & 931 & 5 & 3,648/2,432/1,520 & 64 \\ \hline \hline \end{tabular}
\end{table}
Table 1. Benchmark dataset statics.

\begin{table}
\begin{tabular}{c|c c c c c c} \hline \hline Methods/Datasets & Citeseer & Cora & Pubmed & Texas & Wisconsin & Cornell & Actor \\ \hline MLP & 74.02\(\pm\)1.90 & 75.69\(\pm\)2.00 & 87.16\(\pm\)0.37 & 80.81\(\pm\)4.75 & 85.29\(\pm\)3.31 & 81.89\(\pm\)6.40 & 36.53\(\pm\)0.70 \\ \hline GCN & 76.50\(\pm\)1.36 & 86.98\(\pm\)1.27 & 88.42\(\pm\)0.50 & 55.14\(\pm\)5.16 & 51.76\(\pm\)3.06 & 60.54\(\pm\)5.30 & 27.32\(\pm\)1.10 \\ GAT & 76.55\(\pm\)1.23 & 87.30\(\pm\)1.10 & 86.33\(\pm\)0.48 & 52.16\(\pm\)6.63 & 49.41\(\pm\)4.09 & 61.89\(\pm\)5.05 & 27.44\(\pm\)0.89 \\ \hline GCNII & 77.33\(\pm\)1.48 & 88.37\(\pm\)1.25 & 90.15\(\pm\)0.43 & 77.57\(\pm\)3.83 & 80.39\(\pm\)3.40 & 77.86\(\pm\)3.79 & 37.44\(\pm\)1.30 \\ GCNet & 74.29\(\pm\)0.50 & 86.13\(\pm\)0.38 & 86.29\(\pm\)0.09 & 72.54\(\pm\)1.66 & 66.75\(\pm\)2.81 & 73.56\(\pm\)3.95 & 27.66\(\pm\)0.20 \\ \hline FAGCN & 74.01\(\pm\)1.85 & 86.34\(\pm\)0.67 & 76.57\(\pm\)1.88 & 77.56\(\pm\)6.11 & 79.41\(\pm\)6.55 & 78.64\(\pm\)5.47 & 34.85\(\pm\)1.61 \\ H\({}^{2}\)GCN & 77.11\(\pm\)1.57 & 87.87\(\pm\)1.20 & 89.49\(\pm\)0.38 & 84.86\(\pm\)7.23 & 87.65\(\pm\)4.98 & 82.70\(\pm\)5.28 & 35.70\(\pm\)1.00 \\ GPR-GNN & 77.13\(\pm\)1.67 & 87.95\(\pm\)1.18 & 87.54\(\pm\)0.38 & 78.38\(\pm\)3.46 & 82.94\(\pm\)4.21 & 80.27\(\pm\)8.11 & 34.63\(\pm\)1.22 \\ GCNG & 77.14\(\pm\)1.45 & 87.95\(\pm\)1.05 & 89.15\(\pm\)0.37 & 84.86\(\pm\)4.55 & 86.86\(\pm\)3.29 & 85.68\(\pm\)6.63 & 37.54\(\pm\)1.56 \\ ACM-GCN & 77.32\(\pm\)1.70 & 87.91\(\pm\)0.95 & 90.00\(\pm\)0.52 & 87.84\(\pm\)4.40 & 88.43\(\pm\)3.22 & 85.14\(\pm\)6.07 & 36.28\(\pm\)1.09 \\ LRGNN & 77.53\(\pm\)1.31 & 88.33\(\pm\)0.89 & **90.24\(\pm\)0.64** & 90.27\(\pm\)4.49 & 88.23\(\pm\)3.54 & 86.48\(\pm\)5.65 & 37.34\(\pm\)1.78 \\ PCNet & 77.50\(\pm\)1.06 & 88.41\(\pm\)0.66 & 89.51\(\pm\)0.28 & 88.11\(\pm\)2.17 & 88.63\(\pm\)2.75 & 82.61\(\pm\)2.70 & **37.80\(\pm\)0.64** \\
**DSR-GNN** & **78.38\(\pm\)0.81** & **88.64\(\pm\)0.61** & 89.58\(\pm\)0.15 & **92.61\(\pm\)2.98** & **90.60\(\pm\)1.80** & **90.50\(\pm\)2.79** & 37.57\(\pm\)0.81 \\ \hline \hline \end{tabular}
\end{table}
Table 2. Node classification results on real-world datasets: Mean accuracy (%) \(\pm\) Standard deviation (%). Best performance is highlighted in bold, and runner-up accuracy is highlighted in underline.

Figure 1. Performance of JGNN using various adjacency matrices on homophilous and heterophilous datasets, where JGNN is DSR-GNN w/o structural refinement.

To validate the role of disparity-induced structural refinement, we conduct an ablation study of each focal component, as displayed in Table 3. When DSR-GNN solely uses the signed adjacency matrix obtained by the pre-training procedure for message passing, performance declines, particularly on heterophilous graphs. However, this variant still outperforms other competitors on some datasets (e.g., Texas) due to the effective pre-training scheme. Subsequently, we incorporate structural refinement through random edge dropping, which positively impacts the model but still leaves room for further enhancement. Observations reveal that eliminating some heterophilous links to rationally balance graph heterophily with graph topology can optimize the embedding generated by DSR-GNN. Moreover, the disparity score with only the aggregated-feature distance provides minimal benefits, as it considers feature-level relationships but neglects structural information. In brief, superior accuracy is obtained by the model combining three components. It is worth noting in the table that on homophilous datasets Cora and Citeseer, due to clear connections between nodes of the same class, signed edges assisting nodes to distinguish inter-class information have allowed the model to achieve comparable performance.

### (Q4) Comparison of Original and Refined Graphs

To highlight the differences between the refined and original graphs, we compare heterophily ratios of various datasets across distinct layers, as shown in Table 4. The data reveal that the heterophily ratio fluctuates on most datasets rather than showing a consistent decline, which indicates that DSR-GNN accomplishes refinement based on high-order disparity score instead of merely removing heterophilous edges. Meanwhile, as shown in Table 3, the performance achieved by the original graph is suboptimal compared to that of the refined graph, which may remove homophilous edges. These phenomenons illustrate that the influence of homo./hete. links on performance is not strictly positive/negative. The proposed structural refinement scheme effectively balances both types of links, thereby mitigating the adverse effects of extraneous high-order information.

Moreover, to intuitively compare the original and refined graphs, Figure 2 visualizes the graph structures used by DSR-GNN in the 4th layers on the Texas, Cornell and Wisconsin datasets, respectively. Initially, it is evident that heterophilous links outnumber homophilous links in these datasets. However, due to their pronounced heterophily, the number of heterophilous links is notably reduced after the refinement process.

### (Q5) Visualization of Layer-wise Weights \(\{\lambda_{l}\}_{l=0}^{L}\)

To intuitively understand the impact of high-order information on the gained representations, we visualize the learned weights \(\{\lambda_{l}\}_{l=0}^{4}\) of DSR-GNN with four convolutional layers on several datasets, as illustrated in Figure 3. From this figure, we observe that for three heterophilous graphs (Texas, Cornell and Wisconsin), the weights assigned to neighbors decrease as the number of hops increases. Although the structural refinement operation allows the model to aggregate high-order neighbors with minimal disparity scores, they

Figure 2. Visualizations of the original graph and the refined graph used in 4th layer on the (a) Texas, (b) Cornell and (c) Wisconsin datasets, respectively. Here, blue/red lines indicate heterophilous/homophilous links, and the red circles highlight areas where significant changes occur between them.

\begin{table}
\begin{tabular}{c|c c c|c||c|c c c|c|c} \hline \multirow{2}{*}{Datasets} & Structural refinement & Aggregated-feature distance & Homophily difference & ACC & Datasets & Structural refinement & Aggregated-feature distance & Homophily difference & ACC & 737 \\ \hline \multirow{4}{*}{Citeseer} & & & & & **78.62±0.54** & & & & & 88.62±0.83 & 798 \\  & ✓ & & & & 77.44±0.88 & & ✓ & & & 87.31±0.59 & 760 \\  & ✓ & ✓ & & & 78.30±0.88 & & ✓ & ✓ & & 88.31±0.49 & 761 \\  & ✓ & ✓ & ✓ & & 78.38±0.81 & & ✓ & ✓ & ✓ & **88.64±0.61** & 762 \\ \hline \multirow{4}{*}{Texas} & & & & & 89.50±3.30 & & & & & 89.20±1.60 & 763 \\  & ✓ & & & & 90.75±3.30 & & ✓ & & & 88.20±1.40 & 764 \\  & ✓ & ✓ & & & 92.89±3.56 & & ✓ & ✓ & & 89.60±1.50 & 765 \\  & ✓ & ✓ & ✓ & & **92.61±2.98** & & ✓ & ✓ & ✓ & **90.60±1.80** & 766 \\ \hline \multirow{4}{*}{Cornell} & & & & & & 88.94±3.61 & & & & 36.06±0.83 & 767 \\  & ✓ & & & & 90.78±3.77 & & & & & 36.01±0.94 & 768 \\  & ✓ & ✓ & & 90.00±2.79 & & ✓ & ✓ & ✓ & 36.92±0.79 & 769 \\  & ✓ & ✓ & ✓ & **90.50±2.79** & & ✓ & ✓ & ✓ & ✓ & **37.57±0.81** & 7provide less information and thus receive lower weight. Notably, the significance of 3-hop and 4-hop interactions is quite similar, suggesting that the model effectively mitigates the incorporation of noise as the number of hops increases. For the homophilous graph Cora, the weights assigned to each hop are more balanced, indicating a strong feature similarity between nodes. This characteristic facilitates consistent message passing across layers.

Moreover, the loss values of DSR-GNN across seven datasets during the training process are depicted in Figure 4. Notably, the losses decrease significantly throughout the training epochs, followed by a gradual stabilization. This phenomenon underscores that the model is effective and can achieve stable state through continuous optimization.

## 6. Conclusion

In this paper, we proposed a novel framework that integrated a Disparity-induced Structural Refinement (DSR) scheme with Graph Neural Network (GNN), termed DSR-GNN, to enhance representation learning on heterophilous graphs. The model incorporated two collaborative steps to optimize message propagation and fusion. In specific, the initial step designed a disparity score, derived from the theory of error bound for node classification, to evaluate high-order relationships between nodes based on both features and structure information. The score derived the construction of layer-wise edges by eliminating links with significant disparity, thereby minimizing the impact of irrelevant high-order information during message passing. Meanwhile, the gained node representations can optimize the disparity score in return. Extensive experiments of the proposed model on both heterophilous and homophilous datasets demonstrated that DSR-GNN outperformed existing methods, showcasing its effectiveness in handling heterophilous links.

\begin{table}
\begin{tabular}{c c c c c c c c} \hline \hline Datasets/Hete. ratio & Citeseer & Cora & Pubmed & Texas & Wisconsin & Cornell & Actor & 878 \\ \hline \(\mathcal{H}^{-}(\mathbf{G})\) & 0.2609 & 0.1900 & 0.1976 & 0.8923 & 0.8039 & 0.6946 & 0.7812 & 878 \\ \(\mathcal{H}^{-}(\mathbf{G}^{(2)})\) & 0.2883 & 0.1648 & 0.2139 & 0.8684 & 0.8497 & 0.6810 & 0.7612 & 877 \\ \(\mathcal{H}^{-}(\mathbf{G}^{(3)})\) & 0.2908 & 0.1746 & 0.2143 & 0.8776 & 0.8507 & 0.6781 & 0.7606 & 879 \\ \(\mathcal{H}^{-}(\mathbf{G}^{(4)})\) & 0.2906 & 0.1758 & 0.2140 & 0.8585 & 0.8231 & 0.6735 & 0.7624 & 880 \\ \hline \hline \end{tabular}
\end{table}
Table 4. Hetrophy ratio of various graphs varies across different layers, where the heterophily ratio in the \(l\)-th refined graph \(\mathbf{G}^{(l)}\) is \(\mathcal{H}^{-}(\mathbf{G}^{(l)})=1-\frac{\sum_{i=1}^{N}h_{i}^{(l)}}{N}\).

Figure 4. Loss curves during the training procedure of DSR-GNN on seven datasets.

Figure 3. Visualizations of learnable layer-wise weights \(\{\lambda_{l}\}_{l=0}^{4}\) of 4-layer DSR-GNN, where the vertical axis represents the number of epochs.

## References

* (1)
* Achten et al. (2024) Sonny Achten, Francesco Tomin, Panagiotis Patrions, and Johan AK Suykens. 2024. Unsupervised Neighborhood Propagation Kernel Layers for Semi-supervised Node Classification. In _Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence_. 10764-10774.
* Bo et al. (2021) Joseph Bo, Xiao Wang, Chuan Si, and Jiawei Shen. 2021. Beyond Low-frequency Information in Graph Convolutional Networks. In _Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence_. 3950-3957.
* Chen et al. (2020) Ming Chen, Zhewei Wei, Zengfei Huang, Robin Ding, and Yaliang Li. 2020. Simple and Deep Graph Convolutional Networks. In _Proceedings of the Thirty-First Seventh International Conference on Machine Learning_. 1725-1735.
* Chen et al. (2021) Eli Chen, Jianhao Peng, Fan Li, and Qijben Miklenovic. 2021. Adaptive Unsupervised Graph-Based Graph Neural Network. In _Proceedings of the Ninth International Conference on Learning Representations_.
* DiCrete et al. (2022) Eugen DiCrete, George Deliglmath, and Arnaud Douce. 2022. Wide stochastic networks: Gaussian linear and PAC-Bayesian training. In _International Conference on Algorithmic Learning Theory_. 447-470.
* Ding et al. (2024) Huan Ding, Liang Cheng, Shengheng Li, Yang Bai, Pan Zhou, and Zhe Qu. 2024. Divide, Conquer, and Coalese. Meta Parallel Graph Neural Network for IoT Intrusion Detection at Scale. In _Proceedings of the ACM on Web Conference_. 1656-1667.
* Du et al. (2022) Dan Du, Xiaochuo Shi, Qiang Fu, Xiaoqiu Ma, Hengyu Liu, Shih Han, and Dongmint Zhang. 2022. GMR-CNN: Gated R-Normal Graph Neural Networks for Modeling Both Homophily and Heterophily. In _Proceedings of the ACM on Web Conference_. 1550-1558.
* Han et al. (2024) Jingfan Han, Weiqiu Zhang, Hao Liu, Tao Tao, Naiqiang Tan, and Hui Xiong. 2024. BigST: Linear Complexity Spatio-Temporal Graph Neural Network for Traffic Forecasting on Large-Scale Road Networks. _Proceedings of the VLDB Endowment_ (2024), 1081-1090.
* Huang et al. (2024) Jiucheng Huang, Jing Li, Huanqiang, Xu Chen, and Acong Zhang. 2024. Revisiting the Role of Heterophily in Graph Representation Learning: An Edge Classification Perspective. _ACM Transactions on Knowledge Discovery from Dat_ 18, 1 (2024), 1313-1317.
* Huang et al. (2023) Kexi Huang, Ying Jin, Emmanuel Candes, and Jure Leskovec. 2023. Uncertainty Quantification over Graph with Conformalized Graph Neural Networks. In _Advances in Neural Information Processing Systems_. 26699-26721.
* Huang et al. (2024) Kevin Huang, Ying Jin, Emmanuel Candes, and Jure Leskovec. 2024. Uncertainty quantification over graph with conformalized graph neural networks. In _Advances in Neural Information Processing Systems_. 1-23.
* Jiang et al. (2023) Jiawei Jiang, Chengjali Han, Wayne Xin Zhao, and Jingyuan Wang. 2023. Polarform: Propagation delay-aware dynamic long-range transformer for traffic flow prediction. In _Proceedings of the Thirty-Seventh AAAI conference on artificial intelligence_. 3465-3473.
* Ji et al. (2023) Wei Ji, Zequn Liu, Yifang Qin, Bin Feng, Chen Wang, Zhihui Guo, Xiao Luo, and Ming Zhang. 2023. Few-shot molecular property prediction via hierarchically structured learning on graphs. _Neural Networks_ 163 (2023), 122-131.
* Kipf and Welling (2017) Thomas N. Kipf and Max Welling. 2017. Semi-supervised classification with graph convolutional networks. In _Proceedings of the Fifth International Conference on Learning Representations_. 1-13.
* Kippera et al. (2019) Johannes Kippera, Alessandro Broderski, and Stephan Gunnemann. 2019. Pre-ordering dirt from Propagate: Graph Neural Networks meet Personalized PageRank. In _Proceedings of the Seventh International Conference on Learning Representations_. 1-15.
* Lai et al. (2020) Kwei-Heng Lai, Daochen Zha, Kaixiong Zhou, and Xia Hu. 2020. Policy-gmx: Aggregation optimization for graph neural networks. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_. 464-471.
* Li et al. (2024) Bingheng Li, Erlin Pan, and Zhao Kang. 2024. PC-Conv: Unifying Homophily and Heterophily with Two-Fold Filtering. In _Proceedings of the Thirty-Eighth AAAI Conference on Artificial Intelligence_. 3457-3445.
* Liang et al. (2023) Xingdong Liang, Xiangjing Hu, Zengliu Xu, Zixing Song, and Irwin King. 2023. Predicting Global Label Relationship Matrix for Graph Neural Networks under Heterophily. In _Advances in Neural Information Processing Systems_. 1-13.
* Liao et al. (2023) Ningyi Luo, Siguang Luo, Xiang Li, and Jeming Shi. 2023. LD2: Scalable Heterogeneous Graph Neural Network with Decoupled Embeddings. In _Advances in Neural Information Processing Systems_. 1-13.
* Liu et al. (2023) Yujing Liu, Zongqiu Min, Zhengui Ma, Guogin Wen, Junbo Ma, Guangyuan Lu, and Xiaofeng Zhu. 2023. Multi-teacher Self-training for Semi-supervised Node Classification with Noisy Labels. In _Proceedings of the Thirty-First ACM International Conference on Multimedia_. 2968-2954.
* Lin et al. (2024) Jielong Lin, Zhihao Wu, Loving Zhong, Zhaohang Chen, Hong Zhao, and Shiping Wang. 2024. Generative essential graph convolutional network for multi-view semi-supervised classification. _IEEE Transactions on Multimedia_ (2024), 1-13.
* Lu et al. (2021) Siue Lu, Chengjie Hua, Qincheng Li, Jaya Zita, Mingdee Zhao, Shuyuan Zhang, Xiao-Yue Chang, and Doina Precup. 2021. Is heterophily: a real inphil-marc for graph neural networks to do node classification? _arXiv preprint arXiv:2109.05047_ (2021), 1-27.
* Luo et al. (2024) Xiao Luo, Yintong Zhao, Yiqiang Qin, Wei Ju, and Ming Zhang. 2024. Towards Semi-Supervised Universal Graph Classification. _IEEE Transactions on Knowledge and Data Engineering_ 36, 1 (2024), 146-148.
* Li et al. (2023) Li, Kaihai Song, Qiang Ye, and Guangtian Tian. 2023. Semi-supervised node classification via fine-grained graph auxiliary augmentation learning. _Pattern Recognition_ 137 (2023), 109301.
* Ma et al. (2021) Jaiq Ma, Junwei Deng, and Qiaozhu Mei. 2021. Subgroup Generalization and Fairness of Graph Neural Networks. In _Advances in Neural Information Processing Systems_. 1086-1061.
* Ma et al. (2023) Hulan Ma, Zhikai Chen, Wei Jin, Haoyu Han, Yao Ma, Tong Zhao, Neil Shah, and Jiliang Tang. 2023. Demystifying Structural Disparity in Graph Neural Networks: Can One Sire Fit All?. In _Advances in Neural Information Processing Systems_. 1-55.
* McLintzer (2003) David A McLintzer. 2003. Simplified PAC-Bayesian Margin Bounds. In _Learning Theory and Kernel Machines_. 16th Annual Conference on Theory and 7th Kernel Workshop, 203-215.
* Nyshamkumar et al. (2018) Behnam Nyshamkumar, Srinadh H Shojangmuld, and Nathan Srebro. 2018. A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks. In _Proceedings of the 6th International Conference on Learning Representations_. 1-9.
* Pei et al. (2020) Hongbin Pei, Bingheit Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. 2020. Gestion-GCN: Geometric Graph Convolutional Networks. In _Proceedings of the Eighth International Conference on Learning Representations_. 1-12.
* Yoon et al. (2020) Yuq. Yoon, Wenkihang Huang, Tingyang Xu, and Junhua Huang. 2020. DropF: MegToward Deep Graph Convolutional Networks on Node Classification. In _Proceedings of the 8th International Conference on Learning Representations_. 1-17.
* Yan et al. (2008) Pruhiyan Pei, Galileo Namatu, Mustafa Mbigi, Liei Cetore, Brian Gallagher, and Tan Eliassi-Rad. 2008. Collective Classification in Network Data. _AI Mag_ 29, 308), 93-106.
* Shi et al. (2022) Yan Shi, Ji-Niong Cai, Yoli Shavit, Tai-Jiang Mu, Wensen Feng, and Kai Zhang. 2022. ClusterCNN: Cluster-Based Coarse-To-Fine Graph Neural Network for Efficient Feature Matching. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_. 12517-12536.
* Tropp (2015) Joaoi T. Tropp. 2015. An Introduction to Matrix Concentration Inequalities. _Found. Trends Mach. Learn. 8_ : 2 (2015), 1-230.
* Velickovic et al. (2018) Petar Velickovic, Guillem Cucurull, Arunta Castanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2018. Graph Attention Networks. In _Proceedings of the Sixth International Conference on Learning Representations_. 1-12.
* Zhang et al. (2024) Hanchen Zhang, Jean Koldar, Shengchuo Liu, Jian Zhang, Joan Leskovec, and Qi Li. 2024. Evaluating self-supervised learning for molecular graph embeddings. In _Advances in Neural Information Processing Systems_. 1-33.
* Wu et al. (2024) Zhihua Wu, Zhaohang Chen, Shide Du, Sujia Huang, and Shingu Wang. 2024. Graph Convolutional Network with elastic topology. _Pattern Recognition_ 151 (2024), 110564.
* Xu et al. (2024) Jiaxu Xu, Lecheng Zhang, Xiao Zhu, Yue Liu, Zhangyang Gao, Bozhen Hu, Cheng Tan, Jingqiang Zheng, Syuan Li, and San Zili. 2024. Understanding the limitations of deep models for molecular property prediction: Insights and solutions. In _Advances in Neural Information Processing Systems_. 1-19.
* Xu et al. (2018) Keyulu Xu, Chenghao Li, Yonglong Tian, Tomin Donohoue Sonok, Ken-ichi Kawarabayashi, and Stefanie Jogleink. 2018. Representation Learning on Graphs with Jumping Knowledge Networks. In _Proceedings of the 35th International Conference on Machine Learning_. 5449-5458.
* Yan et al. (2002) Yujun Yan, Milad Hashemi, Kevin Swersky, Yaoqing Yang, and Dansi Koutra. 2002. Two Sides of the Same Coin: Heterophily and Overemoothing in Graph Convolutional Neural Networks. In _IEEE International Conference on Data Mining_. 1287-1292.
* Yang et al. (2001) Liang Yang, Mengzhe Li Lyuang Liu, Bingxin Niu, Chuan Wang, Xiaohuan Cao, and Yuanfang Guo. 2021. Diverse Message Passing for Attribute with Heterophily. In _Advances in Neural Information Processing Systems_. 5713-6713.
* Zhang et al. (2023) Houtan Zhang, Z. Tu, Xi Xiao, Qing Li, Francesco Merzoldo, Xiapu Luo, and Qiuxin Li. 2023. TFE-CNN: A Temporal Fusion Encoder Using Graph Neural Networks for Fine-grained Encrypted Traffic Classification. In _Proceedings of the ACM Web Conference_. 2023, 2026-2075.
* Zhu et al. (2008) Jong Zhu, Yiqun Yan, Lingqiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. 2008. Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs. In _Advances in Neural Information Processing Systems_.

## Appendix A Appendix

### Proofs

Definition 5.: _Let's define a \(L\)-layer GNN classifier \(f\), for node \(v_{i}\), the prediction score is \(f_{i}(\mathbf{X},\mathcal{G})=f(g_{i}(\mathbf{X},\mathcal{G});\mathbf{\Theta}^{( 1)},\mathbf{\Theta}^{(2)},\cdots,\mathbf{\Theta}^{(L)})\), where \(g\) denotes a feature aggregation function and \(f\) is a ReLU-activated \(L\)-layer MLP with learnable parameters \(\{\mathbf{\Theta}^{(l)}\}_{l=1}^{L}\). We assume that the maximum number of hidden units across all layers is \(b\)._

Definition 6.: _For any node \(v_{i}\), the distance of aggregated features from it to other node \(v_{j}\) is defined as_

\[\epsilon_{ij}=\|g_{i}(\mathbf{X},\mathcal{G})-g_{j}(\mathbf{X},\mathcal{G})\| _{2}. \tag{12}\]

Definition 7.: _Given a labeled node \(v_{j}\in V_{\text{tab}}\) with label \(y_{j}\), there exists a margin \(\gamma\geq 0\) satisfying_

\[f_{j}(\mathbf{X},\mathcal{G})[y_{j}]\leq\gamma+\max_{c\neq y_{j}}f_{j}( \mathbf{X},\mathcal{G})[\epsilon], \tag{13}\]

_where \(f_{j}(\mathbf{X},\mathcal{G})[\cdot]\) is to take an element of the predicted probability vector (w.r.t classifier)._

Definition 8.: _The expected loss \(\mathcal{L}_{i}^{\gamma}(f)\) of the classifier \(f\) on \(v_{i}\) for a margin \(\gamma\) and any distribution \(\mathcal{D}\) is defined as (Zadeh, 2018; 2018):_

\[\mathcal{L}_{i}^{\gamma}(f)=\mathbb{P}_{\mathbb{H}\sim\mathcal{D}}\Big{[}f_{i }(\mathbf{X},\mathcal{G})[y_{i}]\leq\gamma+\max_{c\neq y_{i}}f_{i}(\mathbf{X}, \mathcal{G})[\epsilon]\Big{]}. \tag{14}\]

The empirical loss is denoted as \(\mathcal{L}_{i}^{\gamma}(f)\) that is the empirical estimate of the expected loss.

Assumption 1.: _Let \(P\) be a distribution on the classifier family \(\mathcal{F}\), defined by sampling the vectorized MLP parameters from \(\mathcal{N}\left(0,\sigma^{2}l\right)\) for some \(\sigma^{2}\leq\frac{\left(\gamma/8\epsilon_{ij}\right)^{2/L}}{2b(\epsilon_{i} \pm 2BL)}\)._

Lemma 1.: _(Lemma 2 in (Zadeh, 2018)) With assumptions (1) A balance class distribution with \(\mathbf{P}(\mathbf{Y}=1)=\mathbf{P}(\mathbf{Y}=0)\) and (2) Aggregated feature distribution shares the same variance \(\epsilon\). When nodes \(v_{i}\) and \(v_{j}\) have the same aggregated features \(|\mathbf{f}_{i}-\mathbf{f}_{j}|=\epsilon_{ij}\), we can have:_

\[\begin{split}&\big{|}\mathbf{P}_{1}\left(y_{i}=c_{1}\mid\mathbf{f} _{i}\right)-\mathbf{P}_{2}\left(y_{j}=c_{1}\mid\mathbf{f}_{j}\right)\big{|} \leq\\ &\frac{\rho}{\sqrt{2\pi}}(\epsilon_{ij}+\rho\left\|h_{i}^{*}-h_{j} ^{*}\right\|),\end{split} \tag{15}\]

_where \(\rho=\|\mathbf{f}_{1}-\mathbf{r}_{2}\|\) is original feature separability of nodes, \(\mathbf{P}_{1}\) and \(\mathbf{P}_{2}\) are the conditional probability and \(h_{i}^{*}\) denotes the node-level homophily ratio of node \(v_{i}\). Specifically, the node features follow the Gaussian distribution: \(\mathbf{x}_{l}\sim N(\mathbf{\mu}_{1},\mathbf{l})\) for \(i\in V_{\text{tab}}\) and \(\mathbf{x}_{i}\sim N(\mathbf{\mu}_{2},\mathbf{l})\) for \(i\notin V_{\text{tab}}\)._

Theorem 2.: _(Node Pair Generalization of Deterministic Classifiers (Zadeh, 2018)) Let \(\widetilde{f}\) be any classifier in \(\mathcal{F}\). For any node \(v_{i}\), for any \(\lambda>0\) and \(\gamma\geq 0\), for any 'prior' distribution \(P\) on \(\mathcal{F}\) that is independent of the training data \(a_{j}\), with probability at least \(1-\delta\) over the sample of \(y_{j}\), for any \(Q\) on \(\mathcal{F}\) such that \(\Pr_{f\sim Q}\left(\left\|\widetilde{f}_{i}(X,G)-\widetilde{f}_{i}(X,G)\right\| _{\infty}<\frac{\gamma}{\delta}\right)>\frac{1}{2}\), we have_

\[\begin{split}&\mathcal{L}_{i}^{0}(\widetilde{f})\leq\widetilde {\mathcal{L}}_{j}^{\gamma}(\widetilde{f})+\frac{1}{4}\Big{(}2\left(D_{\text{ KL}}(Q\|P)+1\right)+\ln\frac{1}{\delta}+\frac{\lambda^{2}}{4}\\ &\qquad\qquad\qquad+\ln\mathbb{E}_{f\sim P}e^{\lambda(\mathcal{L }_{i}^{\gamma/4}(f)-\mathcal{L}_{j}^{\gamma/2}(f))}\Big{)}\end{split} \tag{16}\]

Lemma 2.: _Suppose an \(L\)-layer GNN classifier \(f\) is associated with model parameters \(\mathbf{\Theta}^{(1)},\ldots,\mathbf{\Theta}^{(L)}\). Define \(T_{f}:=\max_{l=1,\ldots,L}\|\mathbf{\Theta}^{(l)}\|_{2}\). For any node \(v_{i}\) and \(\gamma\geq 0\), \(f\epsilon_{ij}T_{f}^{L}\leq\frac{\gamma}{\delta}\), then_

\[\begin{split}&\mathcal{L}_{i}^{\gamma/2}(f)-\mathcal{L}_{j}^{ \gamma}(f)\leq\frac{C\rho}{\sqrt{2\pi}\sigma}(\epsilon_{ij}+|h_{i}^{*}-h_{j} ^{*}|\rho).\end{split} \tag{17}\]

Proof.: We denote \(f_{i}\) as \(f_{i}(\mathbf{X},\mathcal{G})\) and \(\eta_{c}(i)\) as \(Pr(y_{i}=c|g_{i}(\mathbf{X},\mathcal{G}))\). Following the above analysis, we have

\[\begin{split}&\mathcal{L}_{i}^{\gamma/2}(f)-\mathcal{L}_{j}^{ \gamma}(f)\\ =&\mathbb{E}_{\text{tab}}\mathbb{E}_{\text{tab}}\mathbb{E} ^{\gamma/2}(f_{i},y_{i})-\mathbb{E}_{\text{tab}}\mathbb{E}_{\text{tab}}\mathcal{ L}^{\gamma}(f_{j},y_{j})\\ =&\sum_{c=1}^{C}\eta_{c}(i)\mathcal{L}^{\gamma/2}(f_{c} )-\sum_{c=1}^{C}Pr(y_{j}=c)\mathcal{L}^{\gamma}(f_{j},c)\\ =&\sum_{c=1}^{C}\Big{(}\eta_{c}(i)\mathcal{L}^{\gamma/2 }(f_{i},c)-\eta_{c}(j)\mathcal{L}^{\gamma}(f_{j},c)\Big{)}\\ =&\sum_{c=1}^{C}\Big{(}\eta_{c}(i)(\mathcal{L}^{ \gamma/2}(f_{i},c)-\mathcal{L}^{\gamma}(f_{j},c))\\ &\quad+(\eta_{c}(i)-\eta_{c}(j))\mathcal{L}^{\gamma}(f_{j},c) \Big{)}\\ \leq&\sum_{c=1}^{C}\Big{(}(\mathcal{L}^{\gamma/2}(f_{i},c)-\mathcal{L}^{\gamma}(f_{j},c))+(\eta_{c}(i)-\eta_{c}(j))\Big{)}.\end{split} \tag{18}\]

According to Lemma 1, we have

\[\eta_{c}(i)-\eta_{c}(j)\leq\frac{\rho}{\sqrt{2\pi}\sigma}(\epsilon_{ij}+|h_{i}^{ *}-h_{j}^{*}|\rho). \tag{19}\]

Moreover, we have

\[\|f_{i}-f_{j}\|_{\infty}\leq\frac{Y}{4}. \tag{20}\]

Therefore, we can rewrite it as follows

\[f_{i}(\mathbf{X},\mathcal{G})\left[y_{i}\right]-f_{j}(\mathbf{X},\mathcal{G}) \left[y_{j}\right]\leq\frac{Y}{4}. \tag{21}\]

According to the definition of Expected Margin Loss, we have

\[\mathcal{L}^{\gamma/2}(f_{i},c)\leq\mathcal{L}^{\gamma}(f_{j},c), \tag{22}\]

Consequently, the original bound can be scaled as

\[\mathcal{L}_{i}^{\gamma/2}(f)-\mathcal{L}_{j}^{\gamma}(f)\leq\frac{C\rho}{\sqrt {2\pi}\sigma}(\epsilon_{ij}+|h_{i}^{*}-h_{j}^{*}|\rho), \tag{23}\]

which completing the proof. 

Lemma 3.: _For any node \(v_{i}\), any \(\lambda>0\) and \(\gamma\geq 0\), assume the 'prior' \(P\) on \(\mathcal{F}\) is defined by sampling the vectorized parameters from \(\mathcal{N}\left(0,\sigma^{2}l\right)\) for some \(\sigma^{2}\leq\frac{\left(\gamma/8\sigma\right)^{2/L}}{2b(\epsilon_{i}\pm 2BL)}\). We have_

\[\begin{split}&\ln\mathbb{E}_{f\sim P}e^{\lambda(\mathcal{L}_{i}^{ \gamma/4}(f)-\mathcal{L}_{j}^{\gamma/2}(f))}\leq\ln 3+\frac{C\rho}{\sqrt{2\pi}\sigma}( \epsilon_{ij}+|h_{i}^{*}-h_{j}^{*}|\rho).\end{split} \tag{24}\]

Proof.: Under the condition in Lemma 2, we can split the classifier's space into two regimes. (a): \(Pr(\epsilon_{ij}T_{f}^{L}\leq\frac{Y}{8})\) and (b): \(Pr(\epsilon_{ij}T_{f}^{L}>\frac{Y}{8})\).

Firstly, by Lemma 2, we have \(e^{\lambda(\mathcal{L}_{i}^{\gamma/4}(f)-\mathcal{L}_{j}^{\gamma/2}(f))}\leq \epsilon^{\frac{2C}{\gamma/8\sigma\sigma}(\epsilon_{ij}+|h_{i}^{*}-h_{j}^{*}|)^{ \gamma/3}}\) for any \(\epsilon_{ij}T_{f}^{L}\leq\frac{Y}{8}\). Then, for \(\epsilon_{ij}T_{f}^{L}>\frac{Y}{8}\), according to Assumption 3 in (Zadeh, 2018), with probability at least \(1-e\),

(25) \[e^{\lambda\left(\mathcal{L}_{i}^{\gamma/4}(f)-\mathcal{L}_{j}^{\gamma/2}(f) \right)}\leq\epsilon^{\frac{2+\lambda\frac{C\rho}{\lambda\sigma\sigma}}(\epsilon_{ ij}+|h_{i}^{*}-h_{j}^{*}|

[MISSING_PAGE_EMPTY:11]