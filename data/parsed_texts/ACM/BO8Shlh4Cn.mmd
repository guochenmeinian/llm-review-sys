[MISSING_PAGE_FAIL:1]

in the second sub-figure) are unable to capture these heterogeneous patterns effectively. This observation naturally leads to the idea of utilizing conditional VAE to map data into distinct Gaussian spaces by considering the timestamp as a condition. Unfortunately, as illustrated in the fourth sub-figure (Time CVAE (Zhu et al., 2018)), the results are unsatisfactory, which we will further discuss below.

**Challenge 2: Capturing detailed trends.** Reconstructing monotonous patterns (_i.e._, trends) might appear straightforward at first glance. However, upon a closer examination of the local window (highlighted in a blue rectangle \(\mathbb{O}\) in Figure 1 and magnified in Figure 2(a), it becomes evident that existing methods fail to restore detailed patterns within this time frame. In Figure 2(a), the two green lines initially overestimate the ground truth (purple curve) but subsequently underestimate it for the remainder of the window. This is primarily because existing methods aim to minimize the overall reconstruction error without focusing on "point-to-point" dependencies, _e.g._, the precise upward and downward ranges following a specific point. This omission results in fluctuating reconstruction outcomes (as seen in the second sub-figure). Although CNN attempts to model point-to-point dependencies within the window, it still produces coarse-grained fluctuations (visible in the third sub-figure in Figure 1). The reason of unsatisfactory result of CNN-CVAE lies in Figure 2(b). Upon converting the curves reconstructed by various methods (Figure 2(b)), it becomes evident that the primary cause of these phenomena is the **absence of some frequencies** (smaller amplitude of certain frequencies) in existing methods, hindering the reconstruction of detailed patterns. This observation logically suggests the possibility of employing frequency as the conditional factor in a Conditional Variational Autoencoder (CVAE). Nonetheless, employing frequency as the condition in CVAE presented a new challenge.

**Challenge 3: Large number of sub-frequencies make the signal in condition noisy and difficult to use.** Directly converting the entire window into the frequency-domain results in numerous sub-frequencies, adding noise and obstructing effective VAE-based reconstruction. To address these challenges, we sub-divide the entire window into smaller ones and propose a **target attention** method to select the most useful sub-window frequencies.

In this paper, we introduce a novel unsupervised anomaly detection algorithm, named FCVAE (Frequency-enhanced Conditional Variational AutoEncoder). Different from current VAE-based anomaly detection methods, FCVAE innovatively incorporates both global and local frequency information to guide the encoding-decoding procedures, that both heterogeneous periodic and detailed trend patterns can be effectively captured. This in turn enables more accurate anomaly detection. Our paper's contributions can be summarized as follows:

* Our analysis of the widely-used VAE model for anomaly detection reveals that existing VAE-based models fail to capture both heterogeneous periodic patterns and detailed trend patterns. We attribute this failure to the missing of some frequency-domain information, which current methods fail to reconstruct.
* Our study systematically improves the long-standing VAE by focusing on frequency. Our proposed FCVAE makes the VAE-based approach the state-of-the-art in anomaly detection once more. This is significant because VAE-based methods can inherently handle mixed anomaly-normal training data, while prediction-based methods cannot.
* Evaluations demonstrate that our FCVAE substantially surpasses state-of-the-art methods (0%-40% on public datasets and 10% in a **real-world web system** in terms of F1 score).

Figure 1. Comparison of four KPI reconstruction methods presented in our paper, highlighting anomalies in red \(\mathbb{A}\). The green shade \(\mathbb{A}\) represents the difference between the reconstructed values and the original values, the red shade \(\mathbb{A}\) represents a long period, and the blue ellipse \(\mathbb{A}\) indicates peaks and valleys that are not properly reconstructed, the blue rectangle \(\mathbb{A}\) will be magnified in Figure 2 for detailed comparison.

Figure 2. A detailed view of the region enclosed by a blue rectangle \(\mathbb{O}\) in Figure 1, where the shaded area represents the value range before applying a sliding window average.

Comprehensive ablation studies provide an in-depth analysis of the model, revealing the reasons behind its superior performance.

The replication package for this paper, including all our data, source code, and documentation, is publicly available online at **[https://anonymous.4open.science/r/FCVAE](https://anonymous.4open.science/r/FCVAE)**.

## 2. Preliminaries

### Problem Statement

To facilitate comprehension, we employ the notation established by (Zhou et al., 2017). Given a UTS data \(\mathbf{x}=[x_{0},x_{1},x_{2},\cdots,x_{L}]\) and label series \(\mathbf{L}=[l_{0},l_{1},l_{2},\cdots,l_{L}]\), where \(x_{i}\in\mathbb{R}\), \(l_{i}\in\{0,1\}\), and \(t\in\mathbb{N}\). \(\mathbf{x}\) represents the entire time series data array, while \(x_{i}\) signifies the metric value at time \(t\). \(\mathbf{L}\) denotes the label of time series \(\mathbf{x}\). We define the UTS anomaly detection task as follows:

_Given a UTS\(\mathbf{x}=[x_{0},x_{1},x_{2},\cdots,x_{L}]\), the objective of UTS anomaly detection is to utilize the data \([x_{0},x_{1},\cdots,x_{i-1}]\) preceding each point \(x_{i}\) to predict \(l_{i}\). Based on the value of \(l_{i}\), we can determine whether \(x_{i}\) is an anomaly or not._

### VAEs and CVAEs

VAE is composed of an encoder \(q_{\phi}(\mathbf{z}|\mathbf{x})\) and a decoder \(p_{\theta}(\mathbf{z}|\mathbf{x})\). VAE can be trained by using the reparameterization trick. SGVB (Zhu et al., 2017) is a commonly used training method for VAE because of its simplicity and effectiveness. It maximizes the evidence lower bound (ELBO) to simultaneously train the reconstruction and generation capabilities of VAE. The ELBO is defined in (1).

\[\mathcal{L}=\mathbb{R}_{q_{\phi}(\mathbf{z}|\mathbf{x})}\left[\log p_{\theta}( \mathbf{x}|\mathbf{z})+\log p_{\theta}(\mathbf{z}|\mathbf{x})-\log q_{\phi}( \mathbf{z}|\mathbf{x})\right] \tag{1}\]

DONUT (Zhu et al., 2017) proposed the modified ELBO (M-ELBO) to weaken the impact of abnormal and missing data in the window on the reconstruction. M-ELBO is defined in (2), \(\alpha_{w}\) is defined as an indicator, where \(\alpha_{w}=1\) indicates \(x_{w}\) being not anomalous or missing, and \(\alpha_{w}=0\) otherwise. \(\beta\) is defined as \((\sum_{w=1}^{W}\alpha_{w})/W\).

\[\mathcal{L}=\mathbb{R}_{q_{\phi}(\mathbf{z}|\mathbf{x})}\left[\sum_{w=1}^{W} \alpha_{w}\log p_{\theta}(\mathbf{x}|\mathbf{z})+\beta\log p_{\theta}(\mathbf{ z}|\mathbf{x})-\log q_{\phi}(\mathbf{z}|\mathbf{x})\right] \tag{2}\]

The overall structure of CVAE (Zhu et al., 2017) is similar to VAE, and it combines conditional generative models with VAE to achieve stronger control over the generated data. The training objective of CVAE is defined as (3), where \(\mathbf{c}\) is the condition, similar to that of VAE.

\[\mathcal{L}=\mathbb{R}_{q_{\phi}(\mathbf{z}|\mathbf{x})}\left[\log p_{\theta}( \mathbf{x}|\mathbf{x},\mathbf{c})+\log p_{\theta}(\mathbf{z})-\log q_{\phi}( \mathbf{z}|\mathbf{x},\mathbf{c})\right] \tag{3}\]

## 3. Methodology

### Framework Overview

The proposed algorithm for anomaly detection is illustrated in Figure 3 and comprises three main components: data preprocessing, training, and testing. Given a data point \(x_{l}\), as defined in the problem statement, its state can only be evaluated based on its preceding points \([x_{0},x_{1},\cdots,x_{L}]\). To maintain model consistency, a sliding window method is employed, where a window of \(W\) consecutive points, \([x_{t-W+1},x_{t-W+2},\cdots,x_{L}]\), is utilized to determine if \(x_{l}\) is anomalous. Following sliding window and data preprocessing, a batch of data is input into the FCVAE model for offline training, which will be presented in detail later. Subsequently, the trained model is transferred to the online test module for testing and computing the anomaly score.

### Data Preprocessing

Data preprocessing encompasses standardization, filling missing and anomaly points, and the newly introduced method of **data augmentation**. The efficacy of data standardization and filling missing and anomaly points has been substantiated in prior studies (Zhu et al., 2017; Zhu et al., 2017; Zhu et al., 2017). Therefore, we directly incorporate these techniques into our approach.

Previous data augmentation methods (Zhu et al., 2017; Zhu et al., 2017; Zhu et al., 2017) often added normal samples, such as variations of data from the time domain or frequency domain. However, for our method, we train the model by incorporating all the time series from the dataset together, which provides sufficient pattern diversity. Furthermore, FCVAE has the ability to extract pattern information due to the addition of frequency information, so it can handle new patterns well. Nonetheless, even with the introduction of frequency information, anomalies are often challenging to be effectively addressed. For the model to learn how to handle anomalies, we primarily focus on abnormal data augmentation. In time series data, anomalies are mostly manifested as pattern mutations or value mutations (shown in Figure 4), so our data augmentation mainly targets to these two aspects. The augmentation on the pattern mutation is generated by combining two windows from different curves, with the junction acting as the anomaly. Value mutation refers to changing some points in the window to randomly assigned abnormal values. With the augmented

Figure 4. Examples of the two most frequent anomalies, where the red shaded area denotes the abnormal segments.

Figure 3. Overall Framework.

anomaly data, M-ELBO in CVAE, which will be introduced in detail later, can perform well even in an unsupervised setting without true labels.

### Network Architecture

The proposed FCVAE model is illustrated in Figure 5. It comprises three main components: encoder, decoder, and a condition extraction block that includes a global frequency information extraction module (GFM) and a local frequency information extraction module (LFM). Equation (4) illustrates how our model works.

\[\begin{split}&\mu,\sigma=\text{Encoder}(\mathbf{x},\text{LFM}( \mathbf{x}),\text{GFM}(\mathbf{x}))\\ &\mathbf{z}=\text{Sample}(\mu,\sigma)\\ &\mu,\sigma_{k}=\text{Decoder}(\mathbf{x},\text{LFM}(\mathbf{x}), \text{GFM}(\mathbf{x}))\end{split} \tag{4}\]

#### 3.3.1. Gfm

The GFM module (Figure 7) extracts the global frequency information using the FFT transformation (\(\mathcal{F}\)). However, not all frequency information is useful. The frequencies resulted from the noise and anomalies in the time series data appear as long tails in the frequency domain. Therefore, we employ a linear layer after the FFT to filter out the useful frequency information that can represent the current window pattern. Moreover, we incorporate a dropout layer following Fedformer (Fedformer, 2016) to enhance the model's ability to learn the missing frequency information.

The \(f_{global}\in\mathbb{R}^{1\times d}\) is calculated as (5), where d is the embedding dimension of the global frequency information and \(\mathcal{F}\) means FFT.

\[f_{global}=\text{Dropout}(\text{Dense}(\mathcal{F}(\mathbf{x}))) \tag{5}\]

#### 3.3.2. Lfm

The attention mechanism (Shen et al., 2017) has been widely adopted in time series data processing due to its ability to dynamically process dependencies between different time steps and focus on important ones. Target attention, which is developed based on attention, is widely used in the field of recommendation (Chen et al., 2017). Specifically, target attention can weigh the features of the target domain, leading to more accurate domain adaptation.

The GFM module extracts the frequency information from the entire window, proving to be effective in reconstructing the data within the whole window. However, we use a window to detect whether the last point is abnormal, which poses a challenge because the GFM module does not provide sufficient attention to the last point. This can result in a situation where the reconstruction is satisfactory for part of the window but not for another part, especially when changes in system services lead to the concept drift in the time series data. Even in the absence of concept drift, GFM cannot capture local changes as it extracts the average frequency information from the entire window; hence, the reconstruction of the last key point may be unsatisfactory. Nonetheless, as previously mentioned, target attention can effectively address this issue, as it captures the frequency information of the entire window while paying a greater attention to the latest time point. Therefore, we propose the LFM that incorporates the target attention.

As depicted in Figure 6, the LFM module operates by sliding the entire window \(\mathbf{x}\) to obtain several small windows \(\mathbf{x}_{\mathbf{x}_{\mathbf{y}}}\). Subsequently, FFT and frequency information extraction are applied to each small window. The most recent small window is used as the query \(Q\) because it contains the last point that we want to detect. The remaining small windows are utilized as keys \(K\) and values \(V\) for target attention. Finally, a linear layer is employed to facilitate the model in learning to extract the most important and useful part of the local frequency information, and dropout is also applied to enhance the model's ability to reconstruct some of the local frequency information like GFM.

Figure 5. FCVAE Model Architecture.

Figure 6. Architecture of LFM.

Figure 7. Architecture of GFM.

\[\begin{split} x_{w}=\text{SlidingWindow}(x)\\ Q=\text{Select}(\text{Dense}(\mathcal{F}(x_{w})))\\ K,V=\text{Dense}(\mathcal{F}(x_{w}))\\ f_{local}=\text{Dropout}(\text{FeedFawful}((\sigma(Q\cdot K^{+}) \cdot V))\end{split} \tag{6}\]

The calculation of \(f_{local}\in\mathbb{R}^{1\times d}\) in LFM is given by (6), where \(d\) is the embedding dimension of the local frequency information, which is the same as that of GFM. Here, \(x_{w}\in\mathbb{R}^{n\times k}\) represents a group of small windows extracted from the original window, where \(k\) is the dimension of the small windows and \(n\) is the number of small windows. The Select function is employed to select the latest window as the query \(Q\) and the Dense function means dense neural network. The softmax function \(\sigma\) is used to calculate the attention weights for the small windows.

### Training and Testing

The training process of FCVAE incorporates three key technologies: CVAE-based modified evidence lower bound (CM-ELBO), missing data injection, as well as the newly proposed **masking the last point**. As shown in (7), **CM-ELBO** is obtained by applying M-ELBO (Zhu et al., 2017) to CVAE. Missing data injection (Zhu et al., 2017; Zhu et al., 2017) is a commonly used technique in VAE that we directly apply. We observed that an anomalous point in time series data manifests as an outlier value in the time domain. However, when the data are transformed into the frequency domain, all frequency information is shifted, leading to a challenge. The impact of this issue will be amplified when the last point is abnormal, as we specifically aim to detect whether the last point is abnormal given the whole window. While we use the frequency enhancement method and frequency selection to mitigate this problem to some extent, we mask the last point as zero during the extraction of the frequency condition to address this issue further.

\[\mathcal{L}=\mathbb{E}_{\text{log}_{\theta_{0}}(\text{axis})\bigcap\limits_{i =1}^{W}\alpha_{w}\text{log}_{\theta}(x_{w}|\mathbf{x}_{i},\mathbf{c}_{i})+ \text{log}_{\theta}\text{log}_{\theta}(x)-\text{log}_{\theta}(x|\mathbf{x}_{i },\mathbf{c}_{i})]} \tag{7}\]

While testing, FCVAE adopts the Markov Chain Monte Carlo (MCMC)-based missing imputation algorithm proposed in (Zhu et al., 2017) and applied in (Zhu et al., 2017) to mitigate the impact of missing data. Since our goal is to detect the last point of a window, the last point is set to missing for MCMC to obtain a normal value. This also allows for a better adaptation to the last point mask mentioned earlier. FCVAE further utilizes reconstruction probabilities as anomaly scores, which are defined in equation (8).

\[\text{AnomalyScore}=-\mathbb{E}_{\text{log}_{\theta}(\text{Max})}[\text{log}_{ \theta}(\text{axis},\mathbf{c})] \tag{8}\]

## 4. Experiments

### Experiment Settings

#### 4.1.1. Datasets

To evaluate the effectiveness of our proposed algorithm, we conducted experiments on four datasets. **Yahoo**(Ashar et al., 2017) is an open data set for anomaly detection released by Yahoo lab. **KPI**(Zhu et al., 2017) KPI is collected from five large Internet companies (Sougo, eBay, Baidu, Tencent, and Ali). **WSD**(Ashar et al., 2017) Web service dataset (WSD) contains real-world KPIs collected from three top-tier Internet companies, Baidu, Sogou, and eBay, providing large-scale Web services. **NAB**(Baidu et al., 2017) The Numenta Anomaly Benchmark (NAB) is an open dataset created by Numenta company for evaluating the performance of time series anomaly detection algorithms.

#### 4.1.2. Baseline Methods

To benchmark our model FCVAE against existing methods, we chose the following approaches for evaluation: SPOT (Zhu et al., 2017), SRCNN (Zhu et al., 2017), TED (Zhu et al., 2017), DONUT (Zhu et al., 2017), Informer (Zhu et al., 2017), Anomaly-Transformer (Zhu et al., 2017), AnoTransfer (Zhu et al., 2017), VQRAE (Zhu et al., 2017). SPOT represents a traditional statistical method rooted in extreme value theory. SRCNN and TFAD are supervised methods relying on high-quality labels. Donut, VQRAE, and AnoTransfer are unsupervised reconstruction-based methods utilizing VAE for normal value reconstruction. Informer is an unsupervised prediction-based method that endeavors to predict normal values using an attention mechanism. Anomaly-Transformer is an unsupervised anomaly detection method leveraging the transformer architecture. It posits that normal data exhibit stronger correlations with distant data.

#### 4.1.3. Evaluation Metrics

In practical applications, operators tend to be less concerned with point-wise anomaly detection, i.e., whether each individual point is classified as anomalous or not, and focus more on detecting continuous anomalous segments in time series data. Moreover, due to the substantial impact of anomalous segments, operators aim to identify such segments as early as possible. To address these requirements, we adopt two metrics, best F1 and delay F1, which are based on the works of DONUT (Zhu et al., 2017) and SRCNN (Zhu et al., 2017), respectively.

Best F1 is obtained by traversing all possible thresholds for anomaly scores, and subsequently applying a point adjustment strategy to the prediction in order to compute the F1 score. Delay F1 is similar to best F1 but employs a delay point adjustment strategy to transform the prediction. The adjustment strategies are illustrated in Figure 8, with a delay set to 1 as an example. The detector misses the second anomalous segment because it takes two-time intervals to detect this segment, exceeding the maximum delay threshold we established. We configure the delay for all datasets to be 7, except for Yahoo, where it is set to 3, and NAB, where it is set to 150. This is because the anomaly segments in Yahoo are very short, while in NAB, they are typically much longer, often spanning several hundred data points.

#### 4.1.4. Implementation Details

To guarantee the widespread applicability, all the experiments described below were conducted under entirely unsupervised conditions, without employing any actual labels (all labels are set to zero). For consistency across all methods, we trained a single model for all curves within a dataset. Regarding hyperparameters, we conducted a grid search to identify the most effective parameters for different datasets. Additionally, we later evaluated the sensitivity of these parameters to ensure robust performance.

Figure 8. Illustration of the adjustment strategy.

### Overall Performance

The performance of FCVAE and baseline methods across the four datasets is depicted in Table 1. Our method surpasses all baselines on four datasets regarding best F1 by 6.45%, 0.98%, 14.14% and 0.31%. In terms of delay F1, our method outperforms all baselines on four datasets by 4.98%, 1.58%, 38.68% and 0.65%.

The performance of various baseline methods on the datasets exhibits considerable variation. For instance, SPOT (Song et al., 2017) does not excel on most datasets, as it erroneously treats extreme values as anomalies, whereas anomalies are not always manifested as such. SRCNN (Song et al., 2017) is a reasonably proficient classifier, yet its performance falls short compared to most other models. This underscores the fact that implicitly extracting abnormal features is challenging. Informer (Song et al., 2018) outperforms most other baselines across different datasets, as many anomalies exhibit notable value jumps, and prediction-based methods can effectively manage this situation. However, it struggles with anomalies induced by frequency changes. Anomaly-Transformer (Song et al., 2018) attains commendable results on most datasets in terms of best F1 but demonstrates a low delay F1. It detects anomalies based on the relationships with nearby points, and only when the anomalous point is relatively central within the window can it easily capture the correlation. Conversely, TAD (Tam et al., 2018) achieves favorable results on various datasets but exhibits a certain delay in detection.

Moreover, our method surpasses DONUT (Song et al., 2018) and VQRAE (Song et al., 2018) in terms of reconstruction-based methods. Although VQRAE (Song et al., 2018) introduces numerous modifications to the VAE, employing RNN to capture temporal relationships, our method still outperforms it. This finding implies that for UTS anomaly detection, it is imperative to incorporate only key information while avoiding the model with superfluous data.

### Different Types of Conditions in CVAE

In this context, we conduct experiments under identical settings to evaluate different types of conditions. The chosen conditions encompass information potentially useful for time series anomaly detection within the scope of our understanding, including timestamps (Song et al., 2018), time domain information, and frequency domain information. To ensure consistency, we apply the same operation on the time domain information as we do on the frequency domain information.

As illustrated in Figure 9(a), the performance of employing the frequency information as a condition surpasses that using the timestamp or time domain information. This can be readily comprehended, as timestamps carry limited information and typically require one-hot encoding, resulting in sparse data representation. Time domain information is already incorporated in VAE, and utilizing it as a condition may lead to redundant information without significantly benefiting the reconstruction. Conversely, **frequency information, as a valuable and complementary prior, rendering it a more effective condition for anomaly detection.**

### Frequency VAE and FACVAE

Is CVAE the optimal strategy for harnessing the frequency information in anomaly detection? In this study, we compare FCVAE with an improved frequency-based VAE (FVAE) model, in which the frequency information is integrated into VAE along with the input to reconstruct the original time series. As depicted in Figure 9(b), FCVAE surpasses FVAE. This outcome can be attributed to two primary reasons. Firstly, CVAE, due to its unique architecture that incorporates conditional information, intrinsically outperforms VAE in numerous applications. Secondly, FVAE does not fully exploit frequency information. Although it incorporates this additional information, it still lacks efficient utilization in practice, particularly in the decoder. Consequently, **the CVAE that incorporates the frequency information as a condition represents the most effective structure known to date.**

### GFM and LFM

We propose GFM and LFM to extract global and local frequency information, respectively. However, do these two modules achieve our intended effects through their designs? Additionally, it is worth noting that GFM and LFM may overlap to some degree. Thus, we would like to determine if combining the two can further enhance the performance.

We conduct experiments and the results are depicted in Figure 9(c). It can be observed that, across the four datasets, employing either LFM or GFM in FCVAE outperforms the VAE model under

\begin{table}
\begin{tabular}{c|c c|c c|c c|c c}  & \multicolumn{2}{c|}{**Yahoo**} & \multicolumn{2}{c|}{**KPI**} & \multicolumn{2}{c|}{**WSD**} & \multicolumn{2}{c}{**NAB**} \\
**Method** & **Best F1** & **Delay F1** & **Best F1** & **Delay F1** & **Best F1** & **Delay F1** & **Best F1** & **Delay F1** \\
**SPOT**(Song et al., 2017) & 0.417 & 0.417 & 0.360 & 0.143 & 0.472 & 0.237 & 0.829 & 0.829 \\
**SRCNN**(Song et al., 2017) & 0.251 & 0.198 & 0.786 & 0.678 & 0.170 & 0.053 & 0.828 & 0.575 \\
**DONUT**(Song et al., 2018) & 0.215 & 0.215 & 0.454 & 0.364 & 0.224 & 0.158 & 0.935 & 0.797 \\
**VQRAE**(Song et al., 2018) & 0.510 & 0.492 & 0.272 & 0.137 & 0.312 & 0.103 & 0.933 & 0.893 \\
**Anotransfer**(Song et al., 2018) & 0.567 & 0.496 & 0.685 & 0.461 & 0.674 & 0.379 & 0.965 & 0.871 \\
**Informer**(Song et al., 2018) & 0.707 & 0.671 & **0.918** & **0.822** & 0.557 & 0.393 & **0.973** & 0.892 \\
**TFAD**(Tam et al., 2018) & **0.805** & **0.802** & 0.752 & 0.680 & 0.628 & **0.455** & 0.734 & 0.248 \\
**Anomaly-Transformer**(Song et al., 2018) & 0.274 & 0.029 & 0.868 & 0.346 & **0.728** & 0.137 & 0.971 & **0.911** \\
**FCVAE** & **0.857** & **0.842** & **0.927** & **0.835** & **0.831** & **0.631** & **0.976** & **0.917** \\ \end{tabular}
\end{table}
Table 1. Performance on test data.

identical conditions of other settings except for NAB, where the frequent oscillation of data results in inconsistency between the information extracted from GFM and the data value of the current time. For all datasets, when both LFM and GFM modules are utilized concurrently, they synergistically enhance each other, resulting in superior performance. Consequently, **both global and local frequency information play a crucial role in detecting anomalies.**

### Attention Mechanism

It is crucial to discern whether the enhancement in LFM stems from the reduced window size or the attention mechanism. Thus, we perform experiments by excluding the attention operation from LFM while keeping GFM unaltered. Specifically, we utilized frequency information either from the latest small window in LFM (Latest) or from the average pooling of frequency information across all small windows in LFM (Average Pooling).

The findings in Figure 9(d) demonstrate that without attention, it is impossible to attain the original performance of FCVAE since it is not feasible to determine the specific weight of each small window in advance. However, **the attention mechanism effectively addresses this issue by assigning higher weights to more informative windows.**

We present a comprehensive explanation of the attention mechanism in LFM using a case. A specific data segment, denoted by the black dashed box in Figure 10(b), is selected and all small windows produced by LFM's sliding window module are transformed into the frequency domain to obtain their spectra. As illustrated in Figure 10(a), the 5-th (green) and the 8-th (red) windows exhibit the highest similarity, where the 8-th window serves as the query \(Q\) for our attention. Upon examining Figure 10(b), it can be observed that the heat value of the 5-th window is the highest, which corresponds with the findings in Figure 10(a). Furthermore, we observe from Figure 10(b) that the weight changes of LFM attention have a certain temporal gradient. This is easily understood since adjacent windows are similar.

### Key Techniques in Framework

In this section, we evaluate the effectiveness of our novel data augmentation technique, masking the last point, and the application of CM-ELBO on four distinct datasets. The results are presented in Table 2. Based on the results, it is clear that CM-ELBO plays the most crucial role in most datasets, which aligns with our expectations. This is because it can tolerate abnormal or missing data to a certain extent. Furthermore, masking the last point has a substantial impact on the results, as when an anomaly occurs at the last point of the window, it affects the entire frequency information. Effectively masking this point resolves the issue and improves the detection accuracy. Data augmentation, on the other hand, introduces some artificial anomalies to boost the performance of CM-ELBO, particularly in unsupervised settings.

### Parameter Sensitivity

Our architecture incorporates several techniques, each with its own set of parameters. The stability of a model to different parameters

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Variants & Yahoo & KPI & WSD & NAB \\ \hline w/o data augment & 0.841 & 0.825 & 0.626 & 0.904 \\ w/o mask last point & 0.835 & 0.830 & 0.534 & 0.877 \\ w/o CM-ELBO & 0.690 & 0.757 & 0.435 & 0.897 \\
**FCVAE** & **0.842** & **0.855** & **0.631** & **0.917** \\ \hline \hline \end{tabular}
\end{table}
Table 2. Delay F1 of different settings.

Figure 10. A example of how attention mechanism works in LFM.

Figure 9. Delay F1 score of different settings.

is an important aspect to consider, and therefore we test the sensitivity of our model parameters on two datasets, KPI and WSD. We examine four aspects: the dimension of the condition, the window size, the proportion of missing data injection, and the proportion of data augmentation. The results, shown in Figure 11, indicate that our model can achieve stable and excellent results under different parameter settings.

## 5. Production Impact and Efficiency

Our FCVAE approach has been incorporated as a crucial component in a large-scale cloud system that caters to millions of users globally. The system generates billions of time series data points on a daily basis. The FCVAE detects anomalies in the cloud system, with the primary goal of identifying any potential regressions in the system that may indicate the occurrence of an incident.

Table 3 presents the online performance improvement achieved by employing FCVAE over a period of one year. The experiments were conducted on a 24GB memory 3090 GPU. The results demonstrate substantial enhancements in both Best F1 and Delay F1 compared to the legacy detector. This underscores the effectiveness and robustness of our proposed method. Furthermore, our model is lightweight and highly efficient, capable of processing over 1000 data points within 1 second. This far exceeds the speed at which the system generates new temporal points.

## 6. Related Work

**Traditional statistical methods**[(28; 31; 36; 41; 52)] are widely used in time series anomaly detection because of their great advantages in time series data processing. For example, [(33)] find the high frequency abnormal part of data through FFT [(41)] and verify it twice. Twitter [(40)] uses STL [(6)] to detect anomaly points. SPOT [(38)] considers that some extreme values are abnormal, therefore, detects them through Extreme Value Theory [(8)].

**Supervised methods**[(19; 27; 34)] mostly learn the features of anomalies and identify them through classifiers based on the features learned. Oppertice [(27)] efficiently combines the results of many detectors through random forest. SRCNN [(34)] build a classifier through spectral residual [(12)] and CNN. Some methods [(3; 49)] obtain pseudo-labels through data augmentation to enhance the learning ability.

**Unsupervised methods** are mainly divided into reconstruction-based and prediction-based methods. Reconstruction-based methods [(5; 17; 23; 25; 46)] learn low-dimensional representations and reconstruct the "normal patterns" of data and detect anomalies according to reconstruction error. DONUT [(46)] proposed the modified ELBO to enhance the capability of VAE. Buzz [(5)] is the first to propose a deep generative model. ACVAE [(25)] adds active learning and contrastive learning. Prediction-based methods [(15; 55)] try to predict the normal values of metrics based on historical data and detect anomalies according to the prediction error. LSTM [(15)] proposes to use the LSTM model to predict normal values. Informer [(55)] changes the relevant mechanism of self attention. In recent years, transformer-based methods have been widely proposed. Anomaly-Transformer [(47)] detect anomalies by comparing Kullback-Leible (KL) divergence between two distributions. In recent years, some methods [(44; 48; 56)] have begun to solve some practical problems from the frequency domain. Moerover, many transfer learning methods have been proposed in recent years [(10; 23; 50; 51)] since they are very fast.

## 7. Conclusion

Our paper presents a novel unsupervised method for detecting anomalies in UTS, termed FCVAE. At the model level, we introduce the frequency domain information as a condition to work with CVAE. To capture the frequency information more accurately, we propose utilizing both GFM and LFM to concurrently capture the features from global and local frequency domains, and employing the target attention to more effectively extract the local information. At the architecture level, we propose several new key technologies, including CM-ELBO, the augmentation of anomalous data and masking the last point when extracting the frequency information during training. These innovations further improves the detection accuracy and efficiency. We carry out experiments on four dataset and an online cloud system to evaluate our approach's accuracy, and comprehensive ablation experiments to demonstrate the effectiveness of each module.

Figure 11. Delay F1 score of different settings.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline Baseline & \multicolumn{2}{c}{FCVAE} & Improvement & Inference efficiency \\ F1 & F1* & F1 & F1* & F1 & F1* & [points/second] \\ \hline
0.66 & 0.63 & 0.73 & 0.69 & 10.9\% & 11.1\% & 1195.7 \\ \hline \hline \end{tabular}
\end{table}
Table 3. Online performance of FCVAE in production compared to legacy detector. F1 represents Best F1, F1* represents Delay F1.

## References

* (1)
* WSD dataset [2014] [n. d.]. WSD dataset. Available: [https://github.com/antoraster/AnoTransfer-data/](https://github.com/antoraster/AnoTransfer-data/).
* (2)
* Yahoo dataset [2019] [n. d.]. Yahoo dataset. Available: [https://webscope.sandbox.yahoo.com/](https://webscope.sandbox.yahoo.com/).
* Curranos et al. [2021] Chris Curranos, Francoise-Xavier Aubert, Valentin Flunkert, and Jan Gasthaus. 2021. Neural contextual anomaly detection for time series. _arXiv preprint arXiv:2107.07702_ (2021).
* Chen et al. [2021] Qiwei Chen, Changhua Pei, Shanshan Lv, Chao Li, Junfeng Ge, and Wenwen Ou. 2021. End-to-end user behavior retrieval in click-through retrepetition model. _arXiv preprint arXiv:2106.04608_ (2021).
* Chen et al. [2019] Wenxiao Chen, Haoxuan Xu, Zeyan Li, Dan Fei, Jie Chen, Honglin Qiao, Yang Feng, and Zhongang Wang. 2019. Unsupervised anomaly detection for intricate kigis via adversarial training for k. In _IEEE INFOCOM 2019-IEEE Conference on Computer Communications_. IEEE, 1891-1899.
* Cleveland et al. [1990] Robert R Cleveland, William C Cleveland, Jan E McLrae, and James Terpenning. 1990. STL: A seasonal-resolution and decomposition. _3.01_. State of ( 1990).
* Tang et al. [2021] Liang Tang, Tao Liu, Chang Liu, Bo Jiang, Yanwei Lin, Zhen Xu, and Zhi-Liang Zhou. 2021. SPFAX: State and Dynamic Federated VME for Anomaly Detection of Multivariate CDN KPIs. In _Proceedings of the Web Conference 2021_ (Julbjams, Slovenia) (WWW '21). Association for Computing Machinery, New York, NY, USA, 307-3086. [https://doi.org/10.1145/342381340031](https://doi.org/10.1145/342381340031)
* Hasan and Ferreira [2006] Lea Hasan and A Ferreira. 2006. Extreme Time Theory: an Introduction Springer Science Business Media. LLC. New York (2006).
* Deldart et al. [2021] Shioheh Deldart, Daniel V. Smith, Hao Xue, and Flora D. Salin. 2021. Time Series Change Point Detection with Self-Supervised Contrastive Predictive Coding. In _Proceedings of the Web Conference_ (2021) (Julbjams, Slovenia) (WWW '21). Association for Computing Machinery, New York, NY, USA, 3124-3135. [https://doi.org/10.1145/342381349003](https://doi.org/10.1145/342381349003)
* Duan et al. [2019] Xiaofan Duan, Ningliang Chen, and YongSheng Xie. 2019. Intelligent detection of large-scale MRI systems anomaly based on transfer learning. In _Big Data '1999 Conference for ICDC'1999_. 2019, Wohua, China, September 26-30, 2019, Proceedings.
* Ginemann et al. [2014] Nikon Ginemann, Stephan Gunnemann, and Christos Faloutsos. 2014. Robust Multivariate Autoregression for Anomaly Detection in Dynamic Product Ratings. In _Proceedings of the 23rd International Conference on World Wide Web_ (Seoul, Korea) (WWW '14). Association for Computing Machinery, New York, NY, USA, 361-379. [https://doi.org/10.1145/34584568.2568008](https://doi.org/10.1145/34584568.2568008)
* Hou and Zhang [2007] Xiaofan Hou and Liang Zhang. 2007. Saliency detection: A spectral residual approach. In _2007 IEEE Conference on computer vision and pattern recognition_. Ieee, 1-8.
* Huang et al. [2019] Sheng Huang, Donglin Wang, Xuehan Wu, and Ao Tang. 2019. Dsanet: Dual self-attention network for multivariate time series forecasting. In _Proceedings of the 28th ACM international conference on information and knowledge management_. 2129-2132.
* Huang et al. [2022] Tao Huang, Pengfei Chen, and Ruping Li. 2022. A Semi-Supervised VME Based Active Anomaly Detection Framework in Multivariate Time Series for Online Systems. In _Proceedings of the ACM Web Conference_ 2022 (Virtual Event, Lyon, France) (WWW '22). Association for Computing Machinery, New York, NY, USA, 1797-1806. [https://doi.org/10.1145/3458457.3311984](https://doi.org/10.1145/3458457.3311984)
* Hamilton et al. [2013] Kyle Hamilton, Valentina Constantin, Christopher Laporte, Ian Colwell, and Tom Soderstrom. 2013. Detecting spacecraft anomalies using lstms and nonparametric dynamic thresholding. In _Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining_. 387-395.
* Kamar et al. [2022] Harishwaran Kamar, Lingxia Kong, Alexander Rodriguez, Chao Zhang, and B Adiyas Prakash. 2022. CAMA: Calibrated and Accurate Multi-View Time-Series Forecasting. In _Proceedings of the ACM Web Conference 2022 Virtual Event_. Lyon, France) (WWW '22). Association for Computing Machinery, New York, NY, USA, 3174-3185. [https://doi.org/10.1145/345847.3512037](https://doi.org/10.1145/345847.3512037)
* Kieu et al. [2022] Ting Kieu, Bin Yang, Chenjuan Guo, Razvan-Gabriel Cirstea, Yan Zhao, Vale Song, and Christian S Jensen. 2022. Anomaly detection in time series with robust variational quasi-recurrent sentences. In _2022 IEEE 38th International Conference on Data Engineering (ICDE)_. IEEE, 1342-1354.
* Kingma and Welling [2013] Deheriek P Kingma and Max Welling. 2013. Auto-encoding variational bayes. _arXiv preprint arXiv:1312.6114_ (2013).
* Laptev et al. [2015] Nikoly Laptev, Saeed Amitzadeh, and Ian Fint. 2015. Generic and scalable framework for automated time-series anomaly detection. In _Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining_. 1393-1397.
* Lavin and Abhand [2015] Alexander Lavin and Sabbia Abhand. 2015. Evaluating real-time anomaly detection algorithms-the-Numsten anomaly benchmark. In _2015 IEEE 14th international conference on machine learning and applications (ICMLA)_. IEEE, 38-44.
* Le Guennec et al. [2016] Arthur Le Guennec, Simon Mallowski, and Roman Tavenard. 2016. Data augmentation for time series classification using convolutional neural networks. In _ICML/PKDD workshop on advanced analytics and learning on temporal data_.
* Li et al. [2021] Jia Li, Sihini Di, Yangwen Shen, and Lei Chen. 2021. HuEW: a fast and effective unsupervised framework for time-series anomaly detection. In _Proceedings of the 14th ACM International Conference on Web Search and Data Mining_. 824-832.
* Li et al. [2018] Zeyan Li, Wenxiao Chen, and Dan Pei. 2018. Robust and unsupervised lpi anomaly detection based on conditional variational autoencoder. In _2018 IEEE 57th International Performance Computing and Communications Conference (IPCCC)_. IEEE, 1-9.
* Li et al. [2022] Zeyan Li, Nenewu Zhao, Shengjin Zhang, Yongqian Sun, Pengfei Chen, Xidao Wen, Minghua Ma, and Dan Pei. 2022. Constructing Large-Scale Real-World Benchmark Datasets for ADDs. _arXiv preprint arXiv:2202.08938_ (2022).
* Li et al. [2022] Zhihui Li, Yuqian Zhao, Hiong Geng, Zhuang Zhang, Hanming Wang, Wenxiao Chen, Huaqi Jiang, Anirhee Vaidya, Liangfei Su, and Dan Pei. 2022. Situation-Aware Multivariate Time Series Anomaly Detection Through Active Learning and Contrast VAE-Based Models in Large Distributed Systems. _IEEE Journal on Selected Areas in Communications_. 2022, doi: 10.1202/IJDE.2022.2766-2765.
* Liu et al. [2021] Zhihui Li, Youjun Zhao, Jiaqi Han, Xia, Sui Jin, Xiaoo Wen, and Dan Pei. 2021. Multivariate time series anomaly detection and interpretation using hierarchical infre-metric and temporal embedding. In _Proceedings of the 22th ACM SIGKDD conference on Knowledge discovery & data mining_. 3220-3230.
* Liu et al. [2015] Dapeng Liu, Yuqian Zhao, Haowen Xu, Yongqian Sun, Pei Xiao, and Xiaoqi Jin. 2015. Objective: Towards practical and automatic anomaly detection through machine learning. In _Proceedings of the 2015 internet conference on pattern recognition_. 211-224.
* Lu and Chowrani [2008] Wei Lu and Ali Chowrani. 2008. Network anomaly detection based on wavelet analysis. _EURASIP Journal on Advances in Signal Processing_ 2008 (2008), 1-16.
* Liao et al. [2023] Xiaofeng Liao, Xiaoyang Tang, and Pietro Lio. 2023. GAT-DNS: Multivariate Time Series Prediction Model Based on Graph Attention Network. In _Companion Proceedings of the ACM Web Conference 2023_ (Anustin, TX, USA) (WWW '23). Association for Computing Machinery, New York, NY, USA, 127-131. [https://doi.org/10.1145/3458473.358735829](https://doi.org/10.1145/3458473.358735829)
* Ma et al. [2012] Minghua Ma, Shengjin Zhang, Cheng Chen, Jin Xu, Husehei Li, Yongliang Liu, Xiaohui Nie, Bo Zhou, Yong Wang, and Dan Pei. 2012. [Jump-Shattering Multi-trivariate Time Series Anomaly Detection for Online Service Systems. In _2021 USENIX Annual Technical Conference (USENIX ATC 21)_. 413-426.
* Ma et al. [2011] Aiyu Mahaifar, Ziliu Gao, L Wang, Yongfan Tartes, Timo, Zhang, Jeanne Emmons, Bintunity, and Mark Soderici. 2011. Rapid detection of maintenance induced changes in service performance. In _Proceedings of the Seventh Conference on Emerging Networking EXperiments and Technologies_. 1-12.
* Ovadia et al. [1982] Oded Ovadia, Oren Elisha, and Elad Tom-Tov. 2002. Detection of Infectious Disease Outbreak in Search Engine Time Series Using Non-Specific Syndrome Surveillance with Effect-Size Filtering. In _Companion Proceedings of the Web Conference_. 2021 Virtual Event, Lyon, France) (WWW '22). Association for Computing Machinery, New York, NY, USA, 3242-3293.
* Rasheed et al. [2009] Faraz Rashed, Peter Peng, Beda Alajaji, and Jon Rohae. 2009. Fourier transform based spatial outlier mining. In _Intelligent Data Engineering and Automated Learning-IDEA 2009. 10th International Conference, Bugros, Spain, September 23-24, 2009, Proceedings. 10 Springer, 317-254.
* Ren et al. [2019] Hanheng Ren, Bixiong Xu, Yuting Wang, Chao Yi, Congrui Huang, Xiaoya Kon, Tony Xing, Mao Tzeng, Tong, and Qixing Zhang. 2019. Time-series anomaly detection service at microsoft. In _Proceedings of the 2019 ACM SIGKDD international conference on knowledge discovery data mining_. 3009-3017.
* Rezende et al. [2014] Daniam Rezende, Shikar Mohammadi, and Daun Naturita. 2014. Stochastic backpropagation and approximate inference in deep generative models. In _International conference on machine learning_. PMLR, 1278-1286.
* Rosner [1985] Bernard Rosner. 1985. Percentage points for a generalized ESD many-outlier procedure. _Technometrics_ 25, 2 (1985), 105-172.
* Shen et al. [2008] Lifeng Shen, Zhuoong Li, and James Kwok. 2008. Timeless anomaly detection using temporal hierarchical one-class network. _Advances in Neural Information Processing Systems_ 33 (2020), 13016-13026.
* Siffer et al. [2017] Alon Siffer, Pierre-Ashain Poque, Alexandre Termier, and Christine Largouet. 2017. Anomaly detection in streams with extreme value theory. In _Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_. 1067-1075.
* Sohn et al. [2011] Kihyu Sohn, Hongluk Lee, and Xinchen Yan. 2015. Learning structured output representation using deep conditional generative models. _Advances in neural information processing systems_ 28 (2015).
* Valsa et al. [2014] Owen Valsa, Jordan Hochbaum, and Arun Kegiaraki. 2014. A novel technique for long-term anomaly detection in the cloud. In _eighth USENIX workshop on hot topics in cloud computing (HotCloud 14)_.
* Van Loan [1992] Charles Van Loan. 1992. _Computational frameworks for the fast Fourier transform_. SIAM.
* Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Us Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. _Advances in neural information processing systems_ 97 (2017).
* Wen et al. [2020] Qingsong Wen, Liang Sun, Fan Yang, Xiaomin Song, Jingbun Gao, Xue Wang, and Hanu Xu. 2020. Time series data augmentation for deep learning: A survey. _arXiv preprint arXiv:2002.1248_ (2020).
* Wu et al. [2022] Haizu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long. 2022. TimesNet: Temporal 20-Variant Modeling for General Time Series Analysis. In _International Conference on Learning Representations_.

* Xie et al. (2012) Sibong Xie, Guan Wang, Shuyang Lin, and Philip S. Yu. 2012. Review Spam Detection via Time Series Pattern Discovery. In _Proceedings of the 21st International Conference on World Wide Web_ (Lyon, New York, NY, USA, 635-636. 1000). Association for Computing Machinery, New York, NY, USA, 635-636. [https://doi.org/10.1145/2178002.2181864](https://doi.org/10.1145/2178002.2181864)
* Xu et al. (2016) Haoyen Xu, Wenxing Chen, Nengwen Zhao, Zeyan Li, Jiahao Bu, Zhihan Li, Ying Lin, Youjun Zhao, Dan Pei, Yiueng Feng, et al. 2016. Unsupervised anomaly detection via variation and unco-encoder for seasonal lpiis in web applications. In _Proceedings of the 2016 world wide web conference_. 187-196.
* Xia et al. (2022) Jiehui Xia, Hsiao Wu, Jianmin Wang, and Mingheng Long. 2022. Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy. In _International Conference on Learning Representations_. [https://openreview.net/forum?id=Lq50493049](https://openreview.net/forum?id=Lq50493049)
* Tokkampan et al. (2020) Umapork Tokkampan, Sakmoong Chunkamon, Abbe Moshowitwa, and Eiji Hayashi. 2020. Anomaly detection using variational autoencoder with spectrum analysis for time series data. In _2020 Joint 9th International Conference on Informatics, Electronics & Vision (ICEV) and 2020 4th International Conference on Imaging, Vision & Pattern Recognition (iot/ITIP)_. IEEE, 1-6.
* Zhang et al. (2022) Chaoli Zhang, Tim Zhou, Qingyang Wen, and Liang Sun. 2022. TREAD: A Decompetition Time Series Anomaly Detection Architecture with Time-Frequency Analysis. In _Proceedings of the 31st ACM International Conference on Information & Knowledge-Management_. 2497-2507.
* Zhang et al. (2021) Shengmia Zhang, Zhenyu Zhong, Dongwen Li, Qiliang Fan, Yongqian Sun, Man Zhu, Yuxhi Zhang, Dan Pei, Jiyan Sun, Yinlong Liu, et al. 2021. Efficient kpi anomaly detection through transfer learning for large-scale web services. _IEEE Journal on Selected Areas in Communications_ 40, 8 (2021), 2440-2455.
* Zhang et al. (2018) Xu Zhang, Qingwei Lin, Yong Xu, Si Qin, Hongyu Zhang, Bo Qiao, Yingqong Tang, Xingheng Yang, Qian Cheng, Murali Chintalapati, et al. 2018. Cross-dataset Time Series Anomaly Detection for Cloud Systems. In _USENIX Annual Technical Conference_. 1063-1076.
* Zhang et al. (2005) Yin Zhang, Zhiqi Ge, Albert Greenberg, and Matthew Roughan. 2005. Network amonggraphy. In _Proceedings of the 5th ACM SIGCOMM conference on Internet Measurement_. 30-30.
* Zhao et al. (2022) Chenyu Zhao, Minghua Ma, Zhenyu Zhong, Shenglui Zhang, Zhiyuan Tan, Xiao Xiong, Lafa Lu, Jiayi Feng, Yongqian Sun, Yuzhi Zhang, et al. 2023. Robust Multimodal Failure Detection for Microservice Systems. _arXiv preprint arXiv:2205.18985_ (2022).
* Zhang et al. (2019) Shengzhen Zhao, Jingxu Zhu, Yong Wang, Minghua Ma, Wenchi Zhang, Daepe Liu, Ming Zhang, and Dan Pei. 2019. Automatic and generic periodicity adaptation for lpi: anomaly detection. _IEEE Transactions on Network and Service Management_ 16, 3 (2019), 1170-1183.
* Zhong et al. (2021) Haoyi Zhong, Shanghang Zhang, Jegp Peng, Shuai Zhang, Jianmin Li, Hui Xiong, and Wanxai Zhang. 2021. Informer: Beyond efficient transformer for long sequence time-series forecasting. In _Proceedings of the AAAI conference on artificial intelligence_. 1106-1115.
* Zhou et al. (2017) Tian Zhou, Zxqing Ma, Qingqong Wen, Xue Wang, Liang Sun, and Rong Jin. 2022. FedFormer: Frequency enhanced decomposed transformer for long-term series forecasting. In _International Conference on Machine Learning_. PMLR, 27268-27286.