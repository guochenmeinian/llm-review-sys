[MISSING_PAGE_FAIL:1]

the user-item interaction bipartite graph to group "similar" entities based on their interaction patterns, thus generating user/item bucket assignments that better align with the structure of the data when reducing embedding tables. Our choice of modularity-based bipartite graph clustering is motivated by two key factors: first, we demonstrate that based on a random walk interpretation of the modularity maximization objective (Bahdanau et al., 2015), GraphHash can be regarded as a coarser yet more efficient way to perform the smoothing over the embeddings offered by message-passing, providing a solid foundation for our method. Second, the broad availability of efficient modularity optimization algorithms, such as the Louvain method (Bouvain et al., 2016) that employs local greedy heuristics, enables GraphHash to scale effectively to large-scale user-item interaction graphs. As a result, our approach provides a computationally efficient and easily implementable solution for model reduction in large-scale recommender systems by leveraging the user-item interaction graph.

Despite its simplicity in implementation (Algorithm 1), GraphHash achieves substantial performance improvements. It introduces a novel way of utilizing graph information during preprocessing, serving as a scalable and practical alternative to message-passing. This makes GraphHash particularly advantageous for industrial applications where computational and parameter efficiency, combined with ease of implementation, are crucial.

**Our contributions can be summarized as follows:**

* Theoretically, we demonstrate that modularity-based clustering offers a coarser yet more efficient alternative to the smoothing effect of message-passing on embeddings.
* Building on this theoretical insight, we introduce GraphHash, the first graph-based method to effectively utilize the user-item interaction graph for reducing embedding table size. Our approach employs modularity-based bipartite graph clustering, tailored for scalability in large graphs, and acts as a simple, plug-and-play solution for ID hashing in recommender systems. This combination of efficient and ease of implementation makes GraphHash a practical and powerful tool for improving RecSys performance.
* We conduct extensive evaluations against diverse hashing baselines, showing GraphHash's superior performance in both retrieval and click-through-rate (CTR) prediction tasks1. On average, with fewer parameters, GraphHash outperforms the strongest baseline by 101.52% in recall and 88.33% in NDCG for retrieval, while achieving a 2.9% improvement in LogLoss and a 0.2% gain in AUC for CTR.

Footnote 1: The implementation of GraphHash along with all baselines and backbones are available at [https://anonymous.topen.science/](https://anonymous.topen.science/); GraphHash2024-IBD8.
* Through comprehensive ablation studies across a wide range of experimental settings, we empirically validate our theoretical insights and reveal key findings on the robustness and sensitivity of different design choices in our approach. These results highlight the adaptability and reliability of GraphHash across varying conditions, paving the way for future optimization and refinement in graph-based model reduction.

## 2. Related Work

_Hashing Techniques in RecSys._ Embedding tables, where each row stores the embedding for a user or item, require substantial memory due to the vast number of entities in online platforms. A simple yet effective way to reduce the size of these tables is through the "hashing trick," which randomly hashes unique IDs into a smaller set of values using operations like modulo (Shi et al., 2017). Although this approach inevitably leads to collisions, its simplicity has made it widely used in practice. To mitigate collisions, methods such as double hashing and incorporating frequency information have been shown to be important for enhancing model performance (Shi et al., 2017; Wang et al., 2018). Nonetheless, most prior reduction techniques have focused on ID- or feature-based heuristics, overlooking the user-item interaction information. In this work, we introduce the first graph-based approach for embedding table reduction, integrating interaction information with efficient bipartite graph clustering.

_Graph Clustering._ Graph clustering is a fundamental technique for dimensionality reduction and has been applied in numerous real-world tasks, including more recent ones such as mining higher-order relational data (Shi et al., 2017), and retrieval-augmented generation in large language models (Shi et al., 2017). Common classes of graph clustering include spectral clustering (Shi et al., 2017; Wang et al., 2018), local graph clustering (Bahdanau et al., 2015) and flow-based clustering (Shi et al., 2017). In this work, we choose modularity maximization (Bahdanau et al., 2015; Wang et al., 2018; Wang et al., 2018) as the clustering objective, due to its underlying connection with message-passing and computational efficiency, making it well-suited for large-scale graphs in RecSys.

_Graph Learning Beyond Message-Passing._ Graph learning has emerged as a powerful framework for processing relational data (Shi et al., 2017; Wang et al., 2018; Wang et al., 2018), with most models following the message-passing paradigm (Shi et al., 2017), under which node embeddings are computed by recursively aggregating information from all neighboring nodes. However, such a way of integrating the graph in the forward pass also introduces practical challenges, such as scalability with large graphs and oversmoothing, where increasing model depth soon leads to degrading performance (Wang et al., 2018; Wang et al., 2018). These challenges drive the need for alternative graph learning paradigms. Broadly, existing graph learning methods can be categorized by their use of graphs during preprocessing, training, or inference (Wang et al., 2018). RecSys, traditional collaborative filtering and GNN-based methods use the graph during training (Shi et al., 2017; Wang et al., 2018; Wang et al., 2018), while recent methods like TAG-CF leverage it during test-time inference. (Wang et al., 2018). Our approach, GraphHash, introduces a novel use of graph structure during preprocessing.

## 3. Method: GraphHash

In this section, we outline the road map leading to our proposed method, GraphHash. To provide context, we begin with a brief overview of representation learning in deep RecSys, where recommendations are generated by interacting user and item embeddings, capturing the essential relationships between entities. This background helps illustrate how bipartite graph structures naturally emerge from user-item interactions in tasks such as retrieval and CTR prediction. We then introduce modularity-based graph clustering, the foundation of our method, which aims to group similar entities based on interaction patterns. Finally, we explore the connection between the modularity maximization objective and message-passing techniques, offering deeper theoretical insights into the mechanics underlying GraphHash.

### Embedding in Deep RecSys

Recommendations are generated by "interacting" user and item embeddings, typically through computing the dot product between corresponding rows of the user and item embedding matrices (Hendricks et al., 2016). In deep RecSys, these embeddings are learned representations that map high-dimensional data--such as user preferences or item characteristics--into lower-dimensional vectors, capturing the essential relationships between users and items. These representations are stored in embedding tables, with separate tables for users and items.

The embeddings play a central role in two key tasks: retrieval and click-through rate (CTR) prediction. For retrieval, the system suggests relevant items by comparing the similarity between user and item embeddings, while CTR prediction estimates the likelihood of user engagement with a specific item. Both tasks rely heavily on modeling user-item interactions, often represented as a bipartite graph, where users and items form the nodes, and interactions such as clicks, purchases, or ratings form the edges (Song et al., 2016; Wang et al., 2017; Wang et al., 2017).

With the huge number of entities in online platforms, embedding tables modern recommender systems can easily take hundreds of GB of memory footprint. While commonly adopted hashing tricks (Hendricks et al., 2016; Wang et al., 2017) can effectively reduce the number of rows, the undesired collisions would negatively affect the recommendation accuracy. Therefore, aside from mitigating collisions by mapping entities to a larger set (Wang et al., 2017), another effective solution would be to map "similar" entities--those with similar interaction patterns--into the same embedding (Hendricks et al., 2016). Graph clustering, which effectively groups entities based on their interaction patterns, can help achieve this, reducing the impact of collisions while preserving recommendation quality.

### Modularity-based Graph Clustering

To implement our clustering approach, we rely on modularity, a widely used objective for graph clustering (Bahdanau and Belkin, 2014; Chen et al., 2015; Wang et al., 2016; Wang et al., 2017; Wang et al., 2017; Wang et al., 2017). Modularity-based clustering groups similar entities based on the density of their connections, ensuring that densely connected entities share the same embedding. Specifically, for clustering the user-item bipartite graph, we adopt the modularity definition for bipartite graphs proposed in (Bahdanau and Belkin, 2014): given the set of users \(\mathcal{H}\subset\mathbb{N}\) and set of items \(\mathcal{I}\subset\mathbb{N}\), the adjacency matrix \(A\in\mathbb{R}^{|\mathcal{H}|\times|\mathcal{I}|}\) of the user-item bipartite graph \(\mathcal{G}(\mathcal{H},\mathcal{I},\mathcal{E})\), which encodes the set of user-item interaction pairs \(\mathcal{E}\), is defined as

\[A_{ui}=\begin{cases}1&(u,i)\in\mathcal{E}\\ 0&otherwise\end{cases}.\]

Then _modularity_ of a cluster assignements \(\mathcal{P}\) for the bipartite graph \(\mathcal{G}\) is defined as

\[Q=\frac{1}{m}\sum_{\mathcal{C}\in\mathcal{P}}\sum_{u,i\in\mathcal{C}}\left(A_{ ui}-\frac{k_{u}d_{i}}{m}\right),\]

where \(m=|\mathcal{E}|\) is the number of edges in \(\mathcal{G}\), \(k_{u}=\sum_{j}A_{uj}\) is the degree of user \(u\), and \(d_{i}=\sum_{j}A_{ji}\) is the degree of item \(i\). The optimal cluster assignments \(\mathcal{P}^{*}\) in terms of modularity is then found by maximizing \(Q\).

Directly optimizing modularity is NP-hard (Chen et al., 2015). In practice, optimal partitions can be found by modularity optimization algorithms. One of the most popular and state-of-the-art modularity optimization method is the Louvain method (Chen et al., 2015), which is based on greedy heuristics and enables efficient clustering even on graphs with billions of nodes (Chen et al., 2015).2 Denote the algorithm as \(\mathcal{A}\), then the clustering assignment of node \(x\) is given by \(\mathcal{A}(x)\).

Footnote 2: In the implementation of our method, we make use of the function provided in the scikit-network library (Pedregosa et al., 2011).

```
""" ID hashing """ louvain = Louvain(resolution-resolution) louvain.fit(train_data, force_bipartite=True) user_hashed_id = map_to_consec_int(louvain.labels_row_) item_hashed_id = map_to_consec_int(louvain.labels_col_) """ build model with hashed user/item ID vocab """ user_vocab = np.unique(user_clusters) item_vocab = np.unique(user_clusters) model = MFRetriever(user_vocab, item_vocab, emb_dim) """ model training """ for user_id, item_id in train_data: pos_score = model(user_hashed_id[user_id], item_hashed_id[item_id])... # negative sampling, BPR loss, etc
```

**Algorithm 1** Example GraphHash implementation with Louvain

### GraphHash

With the clustering assignments obtained through modularity-based graph clustering, we can now extend this approach to define the hashing mechanism of GraphHash. Given the set of users \(\mathcal{U}\subset\mathbb{N}\) and items \(\mathcal{I}\subset\mathbb{N}\), a hash function \(\mathcal{H}\) assigns these IDs to a smaller set of buckets, \(\mathcal{B}\subset\mathbb{N}\). In this setup, users or items within the same bucket will share the same embedding in the corresponding embedding table.

The clustering assignments provided by the modularity optimization algorithm \(\mathcal{A}\) offer a natural way to define these bucket assignments. By leveraging the dense connections between users and items--reflecting similar behaviors or preferences--we can improve recommendation quality while maintaining the memory budget. Formally, the bucket assignments are derived from the cluster assignments \(\mathcal{A}(\mathcal{U})\) and \(\mathcal{A}(\mathcal{I})\). To ensure consistent and ordered assignments, a relabeling function \(\ell\) maps the clusters to consecutive integers based on the order of their appearance in \(\mathcal{A}(\mathcal{U})\) and \(\mathcal{A}(\mathcal{I})\). GraphHash can then be defined as:

\[\texttt{GraphHash}(x)=\ell(\mathcal{A}(x)),\quad\forall x\in\mathcal{U}, \mathcal{I}. \tag{1}\]

Algorithm 1 gives an example pseudocode for implementing GraphHash with the Louvain algorithm on a matrix factorization retriever. This approach requires minimal changes to existing code and can be easily integrated in a plug-and-play manner into any recommender model that uses embedding tables.

While graph clustering differs from traditional hash functions in various ways, one key property of regular hash functions that GraphHash shares is that given the user-item interaction graph, it is deterministic when \(\mathcal{A}\) is the Louvain algorithm:

**Proposition 3.1**.: _Given \(\mathcal{G}(\mathcal{U},\mathcal{I},\mathcal{E})\), where \(\mathcal{U},\mathcal{I}\) are finite subsets of \(\mathbb{N}\), and \(\mathcal{A}\) is the Louvain algorithm. Then GraphHash\((\cdot):\mathcal{U},\mathcal{I}\rightarrow\{1,2,...,|\mathcal{P}^{*}|\}\) is a deterministic function._The proof can be found in Appendix A. As such, one advantage of GraphHash is that it behaves like a regular hash function, making it easy to integrate with existing techniques such as double hashing, which was proposed to reduce the collision rate between embeddings of different entities during hashing (Ghosh et al., 2017). By using a regular random hash function \(\mathcal{H}\) and GraphHash, we derive a natural variant of our method to improve collision mitigation, which we referred as DoubleGraphHash:

\[\texttt{DoubleGraphHash}(x)=(\mathcal{H}(x),\texttt{GraphHash}(x)),\forall x \in\mathcal{U},\mathcal{I}. \tag{2}\]

Similarly as discussed in (Ghosh et al., 2017), the combination of \(\mathcal{H}\) and GraphHash can be viewed as an approximation of a hashing into a set of larger cardinality to mitigate collisions between embeddings of different entities when reducing the number of rows in a embedding table.

### Why Modularity? A Random Walk Perspective

While various graph clustering methods exist, the use of modularity-based graph clustering as an alternative to hashing has a fundamental, albeit implicit, connection with message-passing techniques that have proven effective in RecSys models (Ghosh et al., 2017; Ghosh et al., 2017). The link becomes apparent when considering the random walk interpretation of modularity (Ghosh et al., 2017; Ghosh et al., 2017): under modularity, an optimal clustering assignment is one where a random walker is the most likely to remain within its starting cluster compared to chance. Based on this criterion, modularity can be rewritten in the following expressions:

\[Q=\sum_{\mathcal{C}\in\mathcal{P}}\sum_{u,i\in\mathcal{C}}\left(\frac{A_{ui}} {d_{i}}\frac{d_{i}}{m}-\frac{k_{u}d_{i}}{m^{2}}\right)=\sum_{\mathcal{C}\in \mathcal{P}}\sum_{u,i\in\mathcal{C}}\left(\frac{A_{iu}}{k_{u}}\frac{k_{u}}{m} -\frac{k_{u}d_{i}}{m^{2}}\right).\]

Essentially, modularity \(Q\) computes the probability of starting in a cluster \(\mathcal{C}\), and still being in a cluster \(\mathcal{C}\) after one step of unbiased random walk minus the probability that two independent random walkers are in \(\mathcal{C}\), evaluated at large-time asymptotic.

On the other hand, one iteration of message-passing can be written as

\[X^{\prime}_{\mathcal{U}}=D_{\mathcal{U}}^{-1/2}A^{\mathrm{D}-1/2}X_{\mathcal{I }},\qquad X^{\prime}_{\mathcal{I}}=D_{\mathcal{I}}^{-1/2}A^{\mathrm{T}}D_{ \mathcal{U}}^{-1/2}X_{\mathcal{U}}, \tag{3}\]

where \(X_{\mathcal{U}},X_{\mathcal{I}}\) are the user and item embeddings, respectively, and \(D_{\mathcal{U}},D_{\mathcal{I}}\) are the diagonal degree matrices for users and items, respectively. This operation essentially recursively smoothes a node's embedding with the embeddings of its neighboring nodes (Ghosh et al., 2017; Ghosh et al., 2017). From a random walk perspective, one can directly interpret message-passing in (3) as a random walker starting from each root node, then update the root node's embedding with the embeddings of other nodes in the reachable neighborhood, weighted by the corresponding unbiased random walk transition probabilities \(D^{-1}A\) (up to a left and right matrix transformation at both ends): \(X^{\prime}=D^{1/2}(D^{-1}A)D^{-1/2}X\). However, a natural question to pose for the message-passing process is: when should the random walker stop, i.e., which neighbors should each node use for smoothing? The number of message-passing layers in a model directly affects the smoothness of the embeddings, which in turn impacts downstream task performance. Yet message-passing methods such as LightGCN treat it as a hyperparameter requiring manual tuning on a case-by-case basis to achieve optimal results (Ghosh et al., 2017).

Under this random walk interpretation of modularity, GraphHash can be seen as a coarser but more efficient way to perform smoothing over the graph, similar to iterative message-passing. There are two key differences: 1) rather than being set as a hyperparameter, the random walk's stopping point is now automatically determined by maximizing modularity so that the probability of staying in the starting cluster is maximized; 2) graphHash fully smooths node embeddings within the same cluster, while message-passing gradually smooths embeddings through the iterative process in (3). Although this approach sacrifices some granularity in node embeddings compared to iterative message-passing, this trade-off allows for greater computational efficiency: GraphHash simplifies the process by fully smoothing node embeddings within the same cluster in a single step, rather than iteratively computing them over multiple layers.

## 4. Research Questions

We are interested in investigating the following aspects of GraphHash:

1. How does hashing based on the graph information compared with pure ID-based or feature-based hashing methods?
2. Is the graph information more beneficial to power or tail users?
3. How would the training objective affect the model performance with hashing?
4. How would hashing based on the graph information help if the backbone model also uses the graph information?
5. How would different graph clustering objectives affect the model performance?

## 5. Evaluation of GraphHash's Effectiveness

In this section, we validate the effectiveness of our proposed GraphHash, and answer **RQ1** and **RQ2** above.

### Experimental Setup

We benchmark all hashing methods for embedding table reduction on two key recommendation tasks: context-free top-k retrieval and context-aware click-through-rate (CTR) prediction. Here, context-free means that models do not use any additional feature information other than the IDs of users or items, whereas context-aware models utilize complimentary contextual features in addition to the user or item IDs (Ghosh et al., 2017). Due to the nature of our method, we select publicly available datasets where user ID and item ID are explicitly available. Namely, Gowalla (Ghosh et al., 2017), Yelpa018 and AmazonBook (Xie et al., 2018) for retrieval, and Frappe (Ghosh et al., 2017), MovieLens-1M, and MovieLens-20M (Ghosh et al., 2017) for CTR. Further details on datasets are provided in Appendix B.

#### 5.1.1. Backbones

We use matrix factorization (MF) (Ghosh et al., 2017), Neural Matrix Factorization (NeuMF) (Krishnamurthy et al., 2017), LightGCN (Ghosh et al., 2017), and MF+DirectAU (DAU) loss (Luo et al., 2018) as backbones for the retrieval task, where the first three are trained with the Bayesian Personalized Ranking (BPR) loss (Wang et al., 2019); we use WideDeep (Chen et al., 2019), DLRM (Ghosh et al., 2017), and DCNv2 (Ghosh et al., 2017), all trained with binary cross entropy loss (LogLoss) for the CTR task.

#### 5.1.2. Baselines

We evaluate GraphHash against the following baseline hashing methods:

* **Random:** we apply modulo operation to IDs.
* **Frequency (Ghosh et al., 2017; Ghosh et al., 2017)**:** we allocate half the number of buckets to individual users/items with the highest frequencies in the training data, and apply random hashing to the rest.

* **Double**(Roth et al., 2018): we apply two hash functions to IDs and generate two hash codes for each entity and sum the corresponding entries in the embedding table up as the embedding for the entity.
* **Double frequency**(Roth et al., 2018): similar to frequency, we allocate half the number of buckets to individual users/items that have the highest frequencies in the training data. We then apply double hashing to the rest of the entities.
* **LSH**: we apply locally sensitive hashing (LSH) to user/item features, if features are available.
* **LSH-structure**: we treat one-hop neighbor patterns in the user-item interaction graph as the features (\(\mathcal{A}\) as the feature matrix for users and \(\mathcal{A}^{\top}\) as the feature matrix for items) and apply LSH hashing. This can been seen as an alternative way to use the graph structure for user/item bucket assignments.

For reference, we also include the results of models without hashing (**full**).

We implemented all the backbones, baselines, and our approaches with PyTorch. For a fair comparison, all the implementations were identical across all the models except for the hashing component and the resulting embedding table. More details about the experimental setup and training can be found in Appendix C.

### Performance Comparison (RQ1)

#### 5.2.1. Performance in retrieval task

Table 1 reports the mean and standard deviation of the standard retrieval evaluation metrics, Recall@20 and NDCG@20 (in percentage), over 5 independent runs using the best hyperparameters. We see that our proposed method GraphHash achieves the best performance across datasets and backbones, and the improvements over the strongest baseline are substantial, with an average of 101.52% increase in Recall@20 and 88.33% in NDCG@20.

The only exception occurs in the Yelp2018 dataset when employing MF+DirectAU loss as the backbone, where our method slightly underperforms compared to double frequency hashing. Notably, all hashing methods exhibit a significant performance drop when transitioning from BPR loss to DirectAU loss. We conduct a thorough examination of this phenomenon in Section 6.1, where we present a detailed ablation study on the DirectAU loss function and its impact on the model performance.

#### 5.2.2. Performance in CTR task

Table 2 reports the mean and standard deviation of the standard CTR evaluation metrics, LogLoss and AUC (Area Under the ROC Curve), over 5 independent runs using the best hyperparameters. Unlike the case for the retrieval task, our proposed method GraphHash does not perform as ideal. We then further consider a variant of our method, DoubleGraphHash in (2), which combines GraphHash with another random hashing function based on the double hashing technique (Roth et al., 2018) to mitigate collisions. We see that DoubleGraphHash achieves much better performance than GraphHash on CTR tasks and in fact, the best performance across datasets and backbones.

#### 5.2.3. The impact of collisions on retrieval vs. CTR performance

Comparing the results for retrieval and CTR tasks, we make the following observations: 1) GraphHash performs the best in the retrieval task but falls short in the CTR task; 2) DoubleGraphhash, which incorporates double hashing, is the top performer for the CTR task; and 3) while double hashing methods underperform in the retrieval task, they are much stronger baselines for CTR. These findings suggest that pure user-item interaction information is more directly beneficial for retrieval, where collision is less of an issue. In contrast, for the CTR task, collision avoidance techniques are essential for improving performance.

### User Subgroup Evaluation (RQ2)

Next, we examine model performance across different user groups, categorized by their frequency percentile in the training data. For the retrieval task, we aggregate the metrics within each degree subgroup. For the CTR task, we divide the clicks in the test set based on the user subgroup that generated the click. Figures 1 and 3 showcase the results for the retrieval and CTR tasks, respectively.

For the retrieval task, incorporating frequency information generally benefits power users, regardless of the backbone model used. In contrast, GraphHash achieves more balanced performance across all user groups, closely resembling the trend of models without hashing. For the CTR task, all methods, including those without hashing, tend to perform better for clicks generated by power users. Notably, DoubleGraphHash, which delivers the best overall performance, also performs better for power users than for tail users. These observations suggest the fundamental differences between the retrieval and CTR tasks. Nevertheless, the user-item interaction graph benefits model performance in both tasks, with different variants of the method optimizing for their specific characteristics.

## 6. Method Analysis

In this section, we conduct further ablation studies investigating various aspects of our approach, providing deeper insights into its function, robustness and adaptability across different scenarios.

### The Impact of Training Objective (RQ3)

For the retrieval task, we have considered two popular loss functions when training the backbone MF model: the BPR loss and the DirectAU loss. As shown in Table 1, while DirectAU performs better on MF models without hashing, the performance drops significantly for all hashing-based methods when switching from BPR to DirectAU. This finding aligns with recent results in the literature, suggesting that DirectAU may not be compatible with hashing-based methods (Zhu et al., 2018).

To further investigate this phenomenon, we conduct an additional set of experiments with varying values of \(\gamma\) in \([0.25,0.5,1,2,5]\)3, the strength of the uniformity term in the DirectAU loss, and compare the model performance without hashing, with double frequency hashing (the strongest baseline), and with GraphHash in terms of Recall@20 on Gowalla and Yelp2018. The results in Figure 2 show that while both the model without hashing and GraphHash are quite robust to changes in \(\gamma\), there exists a specific sweet spot for the value of \(\gamma\) under double frequency hashing. The corresponding results in NDCG@20 can be found in Appendix D and exhibit similar trends. This indicates that although hashing methods may generally be less compatible with DirectAU than with BPR (Table 1), GraphHash, by leveraging graph information, is more robust to the choice of \(\gamma\).

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

We further investigate whether the user/item clusters found by GraphHash, in which the embeddings of different users/items would be fully smoothed, corresponds to good candidates found by learning when the model has enough capacity, i.e. without hashing. Here, we measure the average within cluster smoothness, normalized by the number of entities in each cluster, for a cluster assignment \(\mathcal{C}\) on user embeddings \(X_{\mathcal{U}}\) by

\[\mathcal{S}(X_{\mathcal{U}},C)=\frac{1}{|\mathcal{U}|}\sum_{u \in\mathcal{U}}\sum_{u^{\prime}\in C(u)}\frac{\|X_{u}-X_{u^{\prime}}\|_{2}^{2} }{|C(u)|}\,,\]

and similarly for item embeddings \(X_{I}\) by \(\mathcal{S}(X_{I},C)\).

We compare the cluster assignments \(\mathcal{C}\) given by GraphHash, against the two-hop neighborhood for each node, which corresponds to the neighborhoods for smoothing when applying two message-passing layers. The results computed on the embeddings of MF and LightGCN without hashing on Gowalla and Yelp2018 are shown in Table 3. We make the following two observations: 1) of graphHash indeed finds a better candidate neighborhood to perform complete smoothing, as compared to the \(2\)-hop neighborhood for each node. This also makes sense as message-passing would not completely smooth the embeddings within the \(2\)-hop neighborhood for each node. 2) Compared to the ones in MF, the embeddings in LightGCN are less smooth, since message-passing would perform further smoothing over them.

### The Impact of Clustering Objective (RQ5)

As discussed in Section 3.4, the choice of the modularity objective for clustering is based on its theoretical connection to message-passing and its computational efficiency in practice. In this section, we study how the clustering objective affects the model performance in terms of accuracy and efficiency.

#### 6.4.1 Modularity-based clustering at varying resolution

In Section 3.4, we see that the modularity objective has a one-step random walk interpretation and a generalized modularity extends this to varying walk lengths (Grandola et al., 2016; Zhang et al., 2017). Such a generalized objective in practice is achieved through a resolution hyperparameter in the Louvain algorithm (Algorithm 1) (Bouillet et al., 2016), which essentially controls the length of the random walk. Higher resolution values correspond to shorter random walks, resulting in more clusters, smaller cluster sizes, and thus larger embedding tables. Table 4 shows the retrieval task performance of GraphHash under different resolution values, along with a comparison to double frequency hashing (the strongest baseline).

From Table 4 we can observe that GraphHash consistently outperforms double frequency hashing at all resolution levels for the retrieval task. Moreover, while increasing resolution and thus embedding table size) improves performance for both of aphdish and the baseline with the MF backbone, the resolution value has little effect when using GraphHash with the LightGCN backbone, unlike double frequency hashing which is more sensitive. A similar set of experiments for the CTR task can be found in Appendix D, where DoubleGraphHash consistently outperforms double frequency hashing across different resolution levels.

#### 6.4.2. Other types of clustering objective

We also compare to other types of bipartite graph clustering methods, such as the spectral bipartite graph co-clustering proposed in (Grandola et al., 2016)4. The results are presented in Table 5 for the retrieval task on Gowalla. We see that while the spectral co-clustering method slightly outperforms GraphHash in retrieval, the cost is at the clustering time, where spectral co-clustering requires \(>\)170x more time on Gowalla, making it inefficient, if not non-applicable to large-scale graphs. A similar set of experimental results for the CTR task, can be found in Appendix D, where DoubleGraphHash outperforms its spectral variant where the graph clustering component is replaced with spectral co-clustering, in addition to requiring much less clustering time.

Footnote 4: We use the implementation provided in the scikit-learn library.

## 7. Conclusion

In this paper, we introduce GraphHash, a novel embedding table reduction method utilizing modularity-based bipartite graph clustering to generate user/item bucket assignments. GraphHash is an efficient alternative to message-passing by using the graph during preprocessing. Empirical evaluation shows the superior performance of GraphHash and its variant in both retrieval and CTR tasks, as well as the robustness of its design choices under various settings. Building upon the promising results of this new graph-based approach, future work could explore how to incorporate the frequency information with graph clustering to better leverage this crucial information (Grandola et al., 2016; Zhang et al., 2017), and how to adapt GraphHash to the OOV setting (Grandola et al., 2016).

\begin{table}
\begin{tabular}{c c|c c c c c c} \hline \hline \multirow{2}{*}{resolution} & \multicolumn{4}{c|}{GraphHash} & \multicolumn{3}{c}{double frequency} \\ \cline{3-8}  & \(\tau\) & \multicolumn{1}{c}{p} & Recall (\(\dagger\)) & NDCG (\(\dagger\)) & \(\tau\) & \multicolumn{1}{c}{p} & Recall (\(\dagger\)) & NDCG (\(\dagger\)) \\ \hline \multirow{3}{*}{MF} & 50 & 0.223M & **7.296** & **3.798** & 0.216M & 3.964 & 1.809 & 1.809 \\  & 100 & 0.432M & **8.894** & **4.008** & 0.363M & 3.253 & 2.045 & 2.045 \\  & 200 & 0.742M & **5.933** & **5.448** & 0.743M & 3.927 & 2.544 & 877 \\  & 400 & 1.140M & **0.733** & **6.023** & 1.140M & 4.521 & 2.964 & 878 \\ \hline \multirow{3}{*}{LightGCN} & 50 & 0.212M & **15.365** & **7.966** & 0.216M & 5.105 & 3.453 & 879 \\  & 100 & 0.432M & **15.783** & **9.590** & 0.463M & 7.131 & 6.078 & 809 \\  & 200 & 0.742M & **15.388** & **9.641** & 0.748M & 10.096 & 6.509 & 400 \\  & 400 & 1.140M & **15.289** & **9.531** & 1.140M & 11.698 & 7.563 & 881 \\ \hline \hline \end{tabular}
\end{table}
Table 4. Impact of resolution in modularity clustering objective on model performance in retrieval. Graphhash consistently outperforms double frequency hashing across all resolution levels considered.

\begin{table}
\begin{tabular}{c c|c c c c} \hline \hline  & & \multicolumn{2}{c|}{Gowalla} & \multicolumn{2}{c}{Yelp2018} \\ \cline{3-6}  & & \(\mathcal{S}(X_{\mathcal{U}},C)\) & \(\mathcal{S}(X_{\mathcal{U}},C)\) & \(\mathcal{S}(X_{\mathcal{U}},C)\) & \(\mathcal{S}(X_{\mathcal{U}},C)\) \\ \hline \multirow{2}{*}{MF} & 2-hop & 15.046 & 12.345 & 3.987 & 3.718 \\  & \(\sigma\)raphHash & 8.324 & 7.628 & 1.848 & 1.592 \\ \hline \multirow{2}{*}{LightGCN} & 2-hop & 70.653 & 46.458 & 86.977 & 68.900 \\  & \(\sigma\)raphHash & 35.462 & 28.080 & 49.712 & 42.4239 \\ \hline \hline \end{tabular}
\end{table}
Table 3. Average within cluster smoothness found by learning in full models without hashing. Compared to the 2-hop neighborhood for each node, GraphHash finds a better candidate neighborhood to perform complete smoothing.

[MISSING_PAGE_FAIL:9]

A Proof of Proposition 3.1

Note that given a graph, the procedures in the Louvain algorithm (Courant and Leskovec, 2015) iterate through the nodes in the order by their indices and thus outputs deterministic clusters (the randomness in its actual implementation in scikit-network, exactly comes from shuffling the node indices at the beginning). Then by putting the users/item cluster assignments in order indexed by their unique IDs, the re-labelling function \(\ell\) guarantees that GraphHash is a deterministic function. 

Note.Empirically, we observe that the cluster assignments given by the Louvain algorithm are quite stable even with different random seeds.

## Appendix B Datasets

In this section, we provide detailed description for datasets used in the experiments.

Data Splitting.For the context-free top-k retrieval task, we consider the following three datasets: Gowalla (2018), Yelp2018 and AmazonBook (AmazonBook, 2018). For each dataset, we adopt a random split of 80%/10%/10% for training, validation and testing (Zhu et al., 2019). For the context-aware CTR task, we consider the following three datasets: Frappe (Courant and Leskovec, 2015), Movielle-Lens-1M and MovieLens-20M (Movelle-Lens-1M and MovieLens-20M, 2018). For Frappe, we use the split provided in RecZoo5, where the data are divided into 70%/20%/10% for training, validation and testing (AmazonBook, 2018). For MovieLens-1M and MovieLens-20M, we adopt a random split of 80%/10%/10% (Movelle-Lens-1M and MovieLens-20M, 2018).

Preprocessing.To avoid out-of-vocabulary (OOV) IDs, which is outside the scope of this work, we preprocess each dataset to satisfy the transductive setting where all users and items in the validation and test sets appear during training. For Movielle-Lens-20M, we use the movie's genres and the user's top-15 tags as the feature information. To make MovieLens-1M and MovieLens-20M suitable for the CTR task, we follow the procedures in (Movelle-Lens-1M and MovieLens-20M suitable for the CTR task, we follow the procedures in (Movelle-Lens-1M and MovieLens-20M suitable for the CTR task, we also follow the procedures in (Movelle-Lens-1M and MovieLens-20M, 2018)) such that all the ratings smaller than 3 are normalized to be 0s, all the ratings greater than 3 to be 1s, and rating 3 are removed.

Table 6 and Table 7 summarize the statistics of each dataset used in the retrieval tasks and CTR tasks, respectively.

resolution values are shown in Table 8. For comparison, we also report the performance of double frequency hashing (the strongest baseline), for which the backbone models have roughly similar but strictly larger size compared to the one used for DoubleGraphHash.

In Table 8, we observe that for the CTR task, DoubleGraphHash consistently outperforms double frequency hashing across different resolution levels, similar to the case for the retrieval task reported in Table 4 in the main text.

#### d.3.2. Other types of clustering objective

We compare the results obtained under modularity-based clustering to spectral bipartite graph co-clustering proposed in [15]. The results are presented in Table 9 for the CTR task on Frappe. DoubleGraphHash outperforms its spectral variant, where the clustering component is replaced with spectral co-clustering, while only requiring less than 1/9 of the clustering time of the latter.

\begin{table}
\begin{tabular}{l c|c c c|c c c} \hline \multirow{2}{*}{resolution} & \multicolumn{5}{c|}{DoubleGraphHash} & \multicolumn{4}{c}{double frequency} \\ \cline{2-9}  & \#Param & LogLoss (L) & AUC (L) & \#Param & LogLoss (L) & AUC (L) \\ \hline \multirow{3}{*}{DLBM} & 3 & 0.140M & **0.215** & **0.971** & 0.144M & 0.263 & 0.968 \\  & 5 & 0.152M & **0.222** & **0.972** & 0.153M & 0.296 & 0.969 \\  & 10 & 0.171M & **0.259** & **0.975** & 0.172M & 0.306 & 0.922 \\  & 20 & 0.186M & **0.208** & 0.972 & 0.188M & 0.293 & **0.973** \\ \hline \multirow{3}{*}{IDCN2} & 3 & 0.986M & **0.194** & **0.972** & 0.989M & 0.219 & 0.966 \\  & 5 & 0.098M & **0.194** & **0.972** & 1.001M & 0.208 & 0.970 \\  & 10 & 0.117M & **0.188** & **0.972** & 1.020M & 0.208 & 0.972 \\  & 20 & 1.032M & **0.187** & **0.972** & 1.034M & 0.201 & 0.971 \\ \hline \end{tabular}
\end{table}
Table 8. Impact of resolution in modality clustering objectives on model performance in CTR task. DoubleGraphHash consistently outperforms double frequency hashing across various resolutions and corresponding embedding table reduction levels.

Figure 6. The impact of LightGCN’s depth on the performance of different hashing methods. GraphHash consistently outperforms random hashing. In particular, GraphHash without any additional message-passing layers, performs roughly equal to random hashing with one or two message-passing layers, where the performance in the latter model can be sorely attributed to pure message-passing.