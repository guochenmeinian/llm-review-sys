[MISSING_PAGE_FAIL:1]

model, we propose the Framework of dfffusion approxi**M**ation (FIM) to estimate diffusion parameters by reconstructing the propagations from cascade data, thereby inferring the underlying network structures. Subsequently, to achieve the desired efficiency for influence estimation, we propose the sampling technique _shortest diffusion time of set_ (SDTS). We analyze the network inference error of FIM and quantify the effect of such errors on the influence estimation theoretically. Our experiments on synthetic and real-world datasets demonstrate that (i) the FIM approach significantly improves the state-of-the-art learning ability on network inference, and (ii) it significantly boosts the efficiency of influence estimation, scaling to realistic datasets. In particular, our method is the only one able to tackle real-world networks with \(12,677\) nodes with thousands of cascades, while offering superior performance.

In a nutshell, our contributions can be summarized as follows.

* We develop a model for continuous-time diffusion and design a scalable and effective framework (FIM) to address network inference and influence estimation.
* We systematically analyze the approximation errors of FIM for both network inference and influence estimation.
* We enhance the scalability of influence estimation, by a sampling technique to estimate the shortest diffusion time of sets.
* We run experiments on synthetic and real-world data, which confirms the superior scalability and effectiveness of FIM.

## 2. Preliminary

### Diffusion Networks

**Notations.** We use calligraphic fonts, bold uppercase letters, and bold lowercase letters to represent sets (e.g., \(\mathcal{N}\)), matrices (e.g., \(\mathbf{A}\)), and vectors (e.g., e) respectively. The \(i\)-th row (resp. column) of matrix \(\mathbf{A}\) is denoted by \(\mathbf{A}[i,\cdot]\) (resp. \(\mathbf{A}[\cdot,i]\)). For ease of exposition, node \(u\) can indicate the row (resp. column) associated with \(u\) in the matrix, e.g., \(\mathbf{A}[u,\cdot]\) (resp. \(\mathbf{A}[\cdot,u]\)). Frequently used notations are summarized in Table 1.

Let \(\mathbf{G}=(\mathcal{V},\mathcal{E})\) be a directed graph with nodes \(\mathcal{V}\) and edges \(\mathcal{E}\), and let \(n=|\mathcal{V}|\) and \(m=|\mathcal{E}|\). We assume that the diffusion process along edge \((u,v)\in\mathcal{E}\) from \(u\) to \(v\) is determined by a diffusion probability density function (PDF) \(p(\lambda_{uw},t)=\begin{cases}\lambda_{uw}\text{e}^{-\lambda_{uw}t}&\text{if }t\geq 0\\ 0&\text{if }t<0\end{cases}\)

where \(\lambda_{uw}\in[0,\infty)\) is the _diffusion rate_ determining the strength of node \(u\) influencing \(v\) and \(t\) is the diffusion delay. For example, Figure 1 illustrates the expanding tendency of a diffusion PDF with \(\lambda_{uw}=2\) with time. Accordingly, the cumulative distribution function (CDF) \(F(\lambda_{uw},t)=\int_{0}^{t}p(\lambda_{uw},t^{\prime})\text{d}t^{\prime}=1- \text{e}^{-\lambda_{uw}t}\) (\(t\geq 0\)) is the probability that node \(u\) succeeds in influencing \(v\) along edge \((u,v)\) by time \(t\). Let \(\mathbf{A}\) be the _adjacency parameter_ matrix of \(\mathbf{G}\) with \(\mathbf{A}[u,v]=\lambda_{uw}\) if \((u,v)\in\mathcal{E}\) and \(\mathbf{A}[u,v]=0\) otherwise, and let \(N_{u}\) be the direct _incoming_ neighbor set of \(u\) with _in-degree_\(d_{u}=|N_{u}|\).

### Continuous-time Diffusion Model

The independent cascade (IC) and linear threshold (LT) models (Zhou et al., 2017) are well-adopted discrete-time models for vanilla IM. Yet discrete-time models have a synchronization property by nature and have been shown empirically to have limitations for modeling diffusion processes in reality (Zhou et al., 2017; Wang et al., 2018; Wang et al., 2019; Wang et al., 2020). Therefore, we adopt here the _continuous-time independent cascade_ (CIC) model for diffusion between users (Zhou et al., 2017; Wang et al., 2018; Wang et al., 2020; Wang et al., 2020). Given a time window \(T>0\) and a source set \(\mathcal{S}\subset\mathcal{V}\), the influence from \(\mathcal{S}\) under the CIC model stochastically spreads as follows.

At time \(t=0\), all nodes \(u\in\mathcal{S}\) are activated. When a node \(u\) is activated at time \(t_{u}\), it attempts to influence each of its outgoing and inactive neighbors \(v\) by following the diffusion function \(p(\lambda_{uw},t-u_{u})\). Each node \(v\) is activated by its incoming neighbors _independently_. Once activated, \(v\) remains active and tries to influence its outgoing neighbors. A diffusion process reaches a fixed point ultimately and ends at time \(T\). In what follows, To avoid clutter in notations, we do not specify the source set \(\mathcal{S}\) nor the time window \(T\) (if applicable) when these are clear from the context.

A diffusion process originating at the source (seed) set \(\mathcal{S}\) and ending by time \(T\), which underwent as just described, is denoted as a _cascade_\(\epsilon:=\{t_{u}:\forall u\in\mathcal{V}\}\in\mathbb{F}_{u_{0}^{2}}^{n}\) where \(t_{u}\in[0,T]\cup\{\infty\}\) is the activation time of node \(u\). In particular, \(t_{u}:=\infty\) if node \(u\) is not activated within the time window \(T\). In applications, for previously recorded cascades, the source set \(\mathcal{S}\) may not be given explicitly but can be easily inferred from any cascade \(\epsilon\) as \(\mathcal{S}=\{u:\forall t_{u}\in\mathbf{c},u_{u}=0\}\); \(\mathcal{S}\) may be a singleton. For diffusions initialized by \(\mathcal{S}\), let \(\mathcal{C}\) be the set of all the known cascades from \(\mathcal{S}\) within the time window \(T\). Given a cascade \(\epsilon\) and time stamp \(t\in[0,T]\), let \(\mathbf{h}_{t,u}\in\mathbb{F}_{u_{0}^{2}}^{n}\) denote the _observation_ of \(\mathbf{c}\) at time \(t\), i.e., \(\mathbf{h}_{t,u}=t_{u}\) for \(t_{u}\in\mathbf{c}\) if \(t_{u}\leq t\); otherwise \(\mathbf{h}_{t,u}:=\infty\). Therefore, \(\mathbf{h}_{t}\) is the snapshot of cascade \(\epsilon\) that \(t\). In general, let the variable \(\mathcal{H}_{t}\) be the observation (random vector) of cascades randomly sampled from \(\mathcal{C}\). To quantify the probability of each node being activated by time \(t\), the states of nodes during diffusion at time \(t\) are recorded. Specifically, given an observation \(\mathbf{h}_{t}\), let \(\phi_{t}\in\{0,1\}^{n}\) be the corresponding _state_ vector of nodes in \(\mathcal{V}\); i.e., \(\phi_{t,o}=1\) if \(\mathbf{h}_{t,o}\leq t\) and \(\phi_{t,o}=0\) otherwise. Accordingly, let \(\phi_{t}\) be the corresponding random state vector of \(\mathcal{H}_{t}\). Therefore, \(E[\phi_{t}]=E_{t,o}-C[\phi_{t}]\) indicates the probability vector of nodes in \(\mathcal{V}\) activated by time \(t\).

\begin{table}
\begin{tabular}{c|l} \hline
**Notation** & **Description** \\ \hline \(\mathbf{G}=(\mathcal{V},\mathcal{E})\) & a network with node set \(\mathcal{V}\) and edge set \(\mathcal{E}\) \\ \hline \(n,m\) & the number of nodes and the number edges in \(\mathbf{G}\) \\ \(\mathcal{N}_{u},d_{u}\) & the incoming neighbor set of node \(u\) and its indegree \\ \hline \(\mathbf{A}\) & the adjacency parameter matrix of \(\mathbf{G}\) \\ \hline \(\mathbf{c},C\) & a cascade recording the activation times for nodes in \(\mathcal{V}\) and the set of all known cascades \\ \hline \(\mathbf{h}_{t},\mathcal{H}_{t}\) & the specific observation of cascade \(\mathbf{c}\) and the observation of a random cascade from \(C\) at time \(t\) \\ \hline \(\phi_{t},\Phi_{t}\) & the specific node state in observation \(\mathbf{h}_{t}\) and the random node state in random observation \(\mathcal{H}_{t}\) \\ \hline \(\gamma_{t},\Gamma_{t}\) & the diffusion rates of nodes in cascade \(\mathbf{c}\) and the expected diffusion rates of nodes at time \(t\) \\ \hline \(\odot\) & the Hadamard product \\ \hline \hline \end{tabular}
\end{table}
Table 1. Frequently used notationsFor example, Figure 2 illustrates a cascade within time \(T\), which is \(\mathbf{c}=\{t_{0},t_{1},\cdots,t_{7},t_{8}\}\), with \(0=t_{0}<t_{1}<t_{3}<t_{4}<t_{5}<T\) and \(t_{\{2,6,7,8\}}=\infty\). By taking an observation at some time moment \(t^{*}\in(t_{4},t_{5})\), we have \(0=\mathbf{h}_{t^{*},0}<\mathbf{h}_{t^{*},1}<\mathbf{h}_{t^{*},3}<\mathbf{h}_{t^ {*},4}<t^{*}\) and \(\mathbf{h}_{t^{*},\{2,5,6,7,8\}}=\infty\). Correspondingly, we have \(\phi_{t^{*},\{0,1,3\}}=1\) and \(\phi_{t^{*},\{2,5,6,7,8\}}=0\).

**Definition 1** (Network Inference).: _Let \(\mathbf{G}=(\mathcal{V},\mathcal{E})\) be a diffusion network with a known node set \(\mathcal{V}\), \([\mathcal{V}]=n\), an unknown edge set \(\mathcal{E}\), and an unknown associated adjacency parameter matrix \(\mathbf{A}\in\mathbb{R}^{n\times n}\). Given a set of cascades \(\mathcal{C}\), network inference seeks to estimate the adjacency parameter matrix \(\mathbf{A}\) based on \(\mathcal{C}\), thereby inferring the underlying edge set \(\mathcal{E}\) of \(\mathcal{G}\)._

Definition 2 (Influence Estimation).: _Consider a diffusion network \(\mathbf{G}=(\mathcal{V},\mathcal{E})\) with node set \(\mathcal{V}\), edge set \(\mathcal{E}\), and the associated adjacency parameter matrix \(\mathbf{A}\). Given a set of seed nodes \(\mathcal{S}\subset\mathcal{V}\) and a time window \(T\), influence estimation aims to estimate the expected number of nodes influenced by \(\mathcal{S}\) within the time window \(T\) under the continuous-time independent cascade (CIC) diffusion model._

## 3. Diffusion Framework

In this section, we develop the framework to model _continuous-time diffusions_ in a diffusion medium.

### Conditional Diffusion Rate

During one diffusion process, an inactive node can have multiple active incoming neighbors, thus possibly being influenced by them simultaneously. The parameterization of the cumulative impact conditioned on a given observation is determined by the joint diffusion rates of its active neighbors. In this context, \(\gamma_{t}\in\mathbb{R}^{n}\) denotes the _conditional diffusion rate_, for which the following holds.

**Lemma 1**.: _Given an observation \(\mathbf{h}_{t}\) with \(\phi_{t,n}=0\) for node \(v\in\mathcal{V}\), we have_

\[\gamma_{t,v}=\sum_{u\in\mathcal{N}_{v}}(\lambda_{uv}\cdot\phi_{t,u}). \tag{1}\]

Let \(\Gamma_{t}\) be the _expected_ conditional diffusion rate over the randomness of observations at time \(t\), i.e., \(\Gamma_{t}=\sum_{u\sim\mathcal{C}}\Gamma_{t}\cdot p(\mathbf{h}_{t})\) where \(p(\mathbf{h}_{t})\) is the probability of observation \(\mathbf{h}_{t}\). By Equation (1), we have

\[\Gamma_{t}=\mathbb{E}[(1-\Phi_{t})\circ\mathbf{A}^{\top}\Phi_{t}\mid\mathcal{ H}_{t}], \tag{2}\]

where \(\mathbb{1}=\{1\}^{n}\) is an \(n\)-dimension all-ones vector and \(\odot\) is the Hadamard product.

### Continuous-time Diffusion Propagation

As described in Section 2.2, influence from source sets spreads along edges to other nodes as a continuous-time cascade. Seeing such influence propagations as temporal evolving dynamics, a continuous-time diffusion among nodes of a network is essentially a _continuous-time dynamical system_ (CDS), succinctly defined as follows.

**Definition 3** (Continuous-time Dynamical System (Durham, 2005; 2015)).: _A continuous-time dynamical system consists of a phase space \(\mathcal{X}\) and a transformation map \(\Omega:(t,\mathcal{X})\rightarrow\mathcal{X}\) where \(t\in\mathbb{R}^{n}_{0}\) is the time._

Accordingly, we apply CDS to diffusion networks and formalize the concept of _continuous-time diffusion propagation_ (CDP).

**Definition 4** (Continuous-time Diffusion Propagation).: _Consider a diffusion network \(\mathbf{G}=(\mathcal{V},\mathcal{E})\) associated with adjacency parameter matrix \(\mathbf{A}\), source set \(\mathcal{S}\subseteq\mathcal{V}\), and time window \(T\). The continuous-time diffusion propagation starting from \(\mathcal{S}\) by time \(T\) is defined as a tuple \((\mathbf{G},\mathcal{X}_{T},\Omega)\), where \(\mathcal{X}_{T}\) is the state space of \(\mathcal{V}\) and \(\Omega\) is the state transition function, which advances a state \(\Phi_{t}\in\mathcal{X}_{T}\) to \(\Phi_{t+t}\) for an infinitesimal interval \(t\to 0^{+}\) with respect to graph \(\mathbf{G}\)._

The foundation of CDP is the state transition function \(\Omega\), which quantifies the evolution of node states over time. In particular, function \(\Omega\) computes node states in the future time \(t+\tau\) solely upon states at the current time \(t\). To capture the transition within \(\tau\), we first establish the _ordinary differential equation_ (ODE) of \(\Omega\). Consequently, CDP can be formulated as follows.

**Theorem 1**.: _Let \(\mathcal{S}\) be a given source set and \(\mathcal{H}_{t}\) be the random observation variable at time \(t\). Consider the infinitesimal interval \(\tau\to 0^{+}\). The continuous-time diffusion propagation for all \(t\geq 0\) is defined as_

\[\mathbb{E}[\{\Phi_{t+\tau}-\Phi_{t}\mid\mathcal{H}_{t}\}=\tau \Gamma_{t}, \tag{3}\]

_where \(\circ\) denotes the Hadamard product and \(\mathbb{1}_{\mathcal{S}}\in\{0,1\}^{n}\) is the indicator vector, with \(\mathbb{1}_{0}=1\) for \(v\in\mathcal{S}\) and \(\mathbb{1}_{u}=0\) otherwise._

### Framework of Continuous-time Diffusion

Note that \(\Gamma_{t}\) and \(\mathbb{E}[\Phi_{t}]\) at time \(t=0\) can be simply known from Equation (4) and Equation (5) respectively, once the source set \(\mathcal{S}\) is given. Furthermore, Equation (3) shows that the exact value of \(\mathbb{E}[\Phi_{t+t}]\) within interval \(\tau\to 0^{+}\) is derived from \(\mathbb{E}[\Phi_{t}]\). Instead, by relaxing the constraint from \(\tau\to 0^{+}\) to \(z>0\) for a small interval \(\varepsilon\), we are able to acquire an approximation of the future state \(\mathbb{E}[\Phi_{t+\varepsilon}]\).

Figure 1. Diffusion function \(p(2,t)\).

Figure 2. Illustration of a diffusion cascade.

For this, we resort to approximate \(\mathbb{E}[\Phi_{t\varepsilon}]\) based upon \(\mathbb{E}[\Phi_{t}]\) and \(\Gamma_{t}\), after which we then estimate \(\Gamma_{t\varepsilon}\), i.e.,

(6) \[\mathbb{E}[\Phi_{t\varepsilon}\mid\mathcal{H}_{t}] \approx\mathbb{E}[\Phi_{t}\mid\mathcal{H}_{t}]+\varepsilon\Gamma_{t},\] (7) \[\Gamma_{t\varepsilon} \approx\mathbb{E}[(1-\Phi_{t\varepsilon})\otimes\mathbf{A}^{ \mathsf{T}}\Phi_{t\varepsilon}\mid\mathcal{H}_{t}].\] (8)

Based on this analysis, we propose our framework FIM (Framework of diffusion approximation) to approximate \(\{\mathbb{E}[\Phi_{\varepsilon}],\mathbb{E}[\Phi_{2\varepsilon}],\ldots, \mathbb{E}[\Phi_{T}]\}\) progressively.

Approximation by \(\varepsilon\) instead of intuitively incurs an approximation error at each iteration. Let \(\xi(t,\varepsilon)\in\mathbb{R}^{n}\) be the corresponding approximation error of \(\mathbb{E}[\Phi_{t\varepsilon}]\) given observation \(\mathcal{H}_{t}\), i.e., \(\xi(t,\varepsilon)=\mathbb{E}[\Phi_{t}-\Phi_{t\varepsilon}\mid\mathcal{H}_{ t}]+\varepsilon\Gamma_{t}\). To quantify \(\xi(t,\varepsilon)\), we derive the following theorem.

**Theorem 2**.: _Given time \(t\in[0,T)\) and time interval \(t\in(0,T-t]\), the approximation error \(\xi(t,\varepsilon)\) at \(t\) is_

\[\xi(t,\varepsilon)=\varepsilon\Gamma_{t}+\exp(-\varepsilon\Gamma_{t})-1. \tag{9}\]

As shown, \(\xi(t,\varepsilon)\) is therefore monotonically increasing when \(\varepsilon\geq 0\) and \(\xi(t,\varepsilon)=0\) if \(\varepsilon=0\).

**Complexity.** The time complexity is dominated by the calculation of \(\Gamma\) in Equation (4), with a cost of \(O(m)\), where \(m\) is the number of non-zero elements in \(\mathbf{A}\). Thus the total time complexity of the approximation is \(O(\frac{1}{T}(m+n))\).

## 4. Network inference and influence estimation

FIM assumes that the adjacency parameter matrix \(\mathbf{A}\) is known. However, this assumption rarely holds, as \(\mathbf{A}\) is typically unknown in reality. In this section, we aim to learn the parameter matrix \(\mathbf{A}\) by leveraging FIM to model the cascade data and then infer the underlying network structure. Afterwards, we develop a sampling technique for continuous-time influence estimation.

### Network Inference

As introduced in Section 2.2, a cascade \(\mathbf{c}\) within time \(T\) records the activation time \(t_{\theta}\) of each node \(v\in\mathcal{V}\). Given cascade \(\mathbf{c}\), we can restore its diffusion process by generating a series of diffusion observations \(\mathbf{h}_{t}\) and node states \(\phi_{t}\). In particular, given a small interval \(\varepsilon\), we construct \(\mathbf{h}_{k\varepsilon}\) and \(\phi_{k\varepsilon}\) for \(k\in\{0,1,\cdots,\lfloor T/\varepsilon\rfloor\}\) by comparing \(t_{\theta}\) with \(k\varepsilon\) for each node \(v\). Specifically, we set \(\mathbf{h}_{k\varepsilon,v}=t_{\theta}\) and \(\phi_{k\varepsilon,v}=1\) if \(t_{\theta}\leq k\), else \(\mathbf{h}_{k\varepsilon,v}=\infty\) and \(\phi_{k\varepsilon,v}=0\) for \(\forall v\in\mathcal{V}\).

With observation \(\mathbf{h}_{(k-1)\varepsilon}\) and state \(\phi_{(k-1)\varepsilon}\) at \(t=(k-1)\varepsilon\), the expected state \(\mathbb{E}[\Phi_{k\varepsilon}\mid\mathbf{h}_{(k-1)\varepsilon}]\) conditioned on \(\mathbf{h}_{(k-1)\varepsilon}\) can be estimated by FIM, after which the corresponding _binary cross-entropy_ (BCE) loss \(\delta_{k\varepsilon}\) over all nodes is calculated as

\[\begin{split}\delta_{k\varepsilon}&=\sum_{v\in \mathcal{V}}\phi_{k\varepsilon,v}\log\mathbb{E}[\Phi_{k\varepsilon,v}\mid \mathbf{h}_{(k-1)\varepsilon}]\\ &+(1-\phi_{k\varepsilon,v})\log(1-\mathbb{E}[\Phi_{k\varepsilon,v }\mid\mathbf{h}_{(k-1)\varepsilon}]).\end{split} \tag{10}\]

By simulating the complete cascade \(\mathbf{c}\), the total loss \(\ell(\mathbf{c})\) of \(\mathbf{c}\) is \(\ell(\mathbf{c})=\sum_{k=1}^{\lfloor T/\varepsilon\rfloor}\delta_{k\varepsilon} +\ell_{T}\). Formally, the procedure of FIM simulating cascade \(\mathbf{c}\) is presented in Algorithm 1.

```
Input: Cascade batch \(\mathcal{C}_{\text{B}}\), time interval \(\varepsilon\), parameter matrix \(\mathbf{A}\) Output: Loss \(\ell(\mathcal{C}_{\text{B}})\)
1for cascade \(\mathbf{c}\in\mathcal{C}_{\text{B}}\)do
2 Initialize observation \(\mathbf{h}_{0}\) and state \(\phi_{0}\) according to \(\mathbf{c}\);
3for\(k\gets 1\) to \(\lfloor T/\varepsilon\rfloor\)do
4\(Y_{(k-1)\varepsilon}=(1-\phi_{(k-1)\varepsilon})\otimes\mathbf{A}^{\top}\phi_{ (k-1)\varepsilon}\) ;
5\(\mathbb{E}[\Phi_{k\varepsilon}\mid\mathbf{h}_{(k-1)\varepsilon}]\leftarrow\phi_{ (k-1)\varepsilon}+\epsilon Y_{(k-1)\varepsilon}\);
6
7 Obtain \(\mathbf{h}_{k\varepsilon}\) and state \(\phi_{k\varepsilon}\) from \(\mathbf{c}\);
8 Calculate \(\delta_{k\varepsilon}\) as Equation (9);
9\(\varepsilon\gets T-k\varepsilon\);
10 Calculate \(\ell_{T}\) conditioned on \(\{\gamma_{T/\varepsilon\rfloor\varepsilon}\}\) by following the procedure from Line 4 to Line 7;
11\(\ell(\mathbf{c})=\sum_{k=1}^{\lfloor T/\varepsilon\rfloor}\delta_{k\varepsilon}+\ell_ {T}\);
12
13\(\ell(\mathcal{C}_{\text{B}})\leftarrow\frac{1}{\lfloor\mathcal{C}_{\text{B}} \rfloor}\sum_{\varepsilon\in\mathcal{C}_{\text{B}}}\ell(\mathbf{c})\);
14return\(\ell(\mathcal{C}_{\text{B}})\);
```

**Algorithm 1**Diffusion approximation by FIM

Once the parameter matrix \(\mathbf{A}\) is estimated, the underlying network structure can be inferred accordingly. By following the literature (Ashburn and Boyd, 1995; Boyd, 1995; Boyd, 1995; Boyd, 1995), an empirical threshold \(\lambda^{\kappa}\) can be set such that an edge \((u,v)\) exists if \(\mathbf{A}[u,v]\geq\lambda^{\kappa}\). (More details in Appendix A.2).

### Influence Estimation

Given a graph \(\mathbf{G}=(\mathcal{V},\mathcal{E})\) with the adjacency parameter matrix \(\mathbf{A}\), a source set \(\mathcal{S}\subseteq\mathcal{V}\), and a time window \(T\), let \(I(T,\mathcal{S})\) be the number of activated nodes in a cascade \(c\) sampled from \(\mathcal{C}(T,\mathcal{S})\). The influence estimation problem is to compute the expected number of nodes \(\mathbb{E}[I(T,\mathcal{S})]\) influenced by \(\mathcal{S}\) within the time window \(T\).

It is intuitive that a node \(u\) is influenced by the source node \(s\in\mathcal{S}\) that has the shortest time (path) to \(u\) under the CIC model, where the edges are weighted by their transmission times, which is known as the _shortest path property_ in the literature (Brockman, 1995; Boyd, 1995; Boyd, 1995). By consequence, one instance of \(I(T,\mathcal{S})\) can be calculated as follows. By sampling the diffusion time \(t_{u,v}\) of each edge \((u,v)\in\mathcal{E}\) according to the diffusion function \(p(u_{uv},t),I(T,\mathcal{S})\) is the number of nodes reachable by \(\mathcal{S}\) within time \(T\). Existing methods (Brockman, 1995; Boyd, 1995) first identify one activation set for each source node \(s\in\mathcal{S}\) and then take the union over all the source nodes to calculate \(I(T,\mathcal{S})\).

[MISSING_PAGE_FAIL:5]

**Limitations of the baseline methods.** We start by reporting that, on our reasonably powerful server1, both NMF and NetRate are _out of memory_ (OOM) on the larger datasets (Weibo and Twitter) and thus their performance is not available on them. Furthermore, NetRate could not finish in less than 12 hours on MemeTracker and the synthetic datasets, except the smallest one (HR). We believe this is due to the joint effect of the node count, cascade count, and cascade sizes. We can therefore conclude from these outcomes that NetRate- especially with the official Matlab implementation - is not efficient for large networks2.

Footnote 1: We refer to Appendix A.2.2 for the details on the serverâ€™s configurations.

**Parameter learning on HR and MemeTracker.** In a first experiment, we compare FIM's parameter learning performance on (i) the only dataset common to all three methods (HR), and (ii) the only real-world dataset common to both NMF and FIM (MemeTracker). (We defer a similar comparison between FIM and NMF, on the other six synthetic datasets, to the next experiment, where they are complemented by effectiveness scores on network inference based on the inferred parameters.) Table 3 presents the BCE loss results - as defined in Equation (9) - for parameter learning from cascades with a varying time window \(T\) and time interval \(\varepsilon\). We can notice that FIM outperforms NMF by achieving slightly smaller BCE loss on both datasets. Furthermore, FIM consistently achieves notably smaller BCE loss compared to NetRate on HR, across all settings. These findings provide good evidence of FIM's superior ability to learn a high-quality parameter matrix \(\mathbf{A}\).

Furthermore, Figures 3 and 4 plot the training time of FIM and NMF, with varying time window values \(T\) and time intervals \(\varepsilon\), on HR and MemeTracker respectively. As displayed, FIM runs notably faster than NMF, with speedups up to \(3\times\sim 4\times\). Moreover, the efficiency advantage becomes more obvious as the data grows.

**CIM based evaluation on the learned parameter matrix.** Following the previous experiment, we further assess of quality of each inferred matrix \(\mathbf{A}\) from the perspective of _continuous influence maximization (CIM)_, based on _test cascades_ (unseen during training), as follows.

Recall that CIM algorithms are able to identify the seed set \(\mathcal{S}\) with the largest expected spread \(\mathbb{E}[\mathcal{S}]\) in a given diffusion network (as per \(\mathbf{A}\)). In our setting, following the convention (Brockman et al., 2016; Chen et al., 2016), we can obtain the _ground-truth_ spread (i.e., quality) of a source set \(\mathcal{S}\) as follows: randomly sample from the test cascades one for each node in \(\mathcal{S}\), and take the union of the nodes from the sampled cascades, leading to a single estimate of \(\mathbb{E}[\mathcal{S}]\); repeat this process 1000 times to obtain the average over estimates as \(\mathcal{S}\)'s ground-truth spread. Therefore, generically, when a state-of-the-art CIM algorithm selects a seed set \(\mathcal{S}\) in the network defined by an estimated parameter matrix \(\mathbf{A}\), we can assess the quality of \(\mathcal{S}\) and, by design, also the accuracy of \(\mathbf{A}\) itself. _A larger ground-truth spread means a higher estimation quality for the parameter matrix \(\mathbf{A}\)_. (A more detailed justification for this CIM experiment can be found in Appendix A.2.2.)

As the HR dataset is unsuitable for this CIM experiment, we used Core2048 instead. This is because HR's cascades contain multiple source nodes (see the dataset's description in Appendix A.2.1), hence ground-truth spread cannot be obtained as described. Nevertheless, we describe in Appendix A.3 an influence estimation (IE) experiment on HR, designed for multi-source cascades, to assess the quality of the estimated \(\mathbf{A}\) by FIM, NMF, and NetRate.

We applied the state-of-the-art CIM algorithm (Shen et al., 2016) on the estimated \(\mathbf{A}\) of both MemeTracker and \(Core2048\), selecting seed sets \(\mathcal{S}\) with sizes from \(|\mathcal{S}|=4\) to \(|\mathcal{S}|=10\). Table 4 shows the spread

\begin{table}
\begin{tabular}{c|c c c c c c c c} \hline \hline \multirow{2}{*}{**Dataset**} & \multirow{2}{*}{**Method**} & \multicolumn{6}{c}{Seed Set Size \(|\mathcal{S}|\)} \\  & & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 68 \\ \hline \multirow{2}{*}{Meme} & FIM & 65.85 & 75.18 & 88.33 & 106.88 & 113.90 & 127.11 & 136.67 \\  & NMF & 53.14 & 64.78 & 70.66 & 76.10 & 83.39 & 88.23 & 94.95 \\ \hline \multirow{2}{*}{Core2048} & FIM & 517.12 & 576.19 & 559.19 & 634.83 & 636.26 & 639.38 & 723.88 \\  & NMF & 442.25 & 506.17 & 476.44 & 570.90 & 591.80 & 637.40 & 567.28 \\ \hline \hline \end{tabular}
\end{table}
Table 4. Influence maximization on Meme.

\begin{table}
\begin{tabular}{c|c c c c c c c c} \hline \hline \multirow{2}{*}{**Dataset**} & \multirow{2}{*}{**HR**} & \multicolumn{3}{c}{**Hier\({}_{1024}\) / Hier\({}_{2048}\)**} & \multicolumn{3}{c}{**Core\({}_{1024}\) / Core\({}_{2048}\)**} & \multicolumn{3}{c}{**Rand\({}_{1024}\) / Rand\({}_{2048}\)**} & \multicolumn{3}{c}{**MemeTracker**} & \multicolumn{1}{c}{**Weibo**} & \multicolumn{1}{c}{**Twitter**} \\
**\#Nodes** & 128 & 1024 / 2048 & 1024 / 2048 & 1024 / 2048 & 1024 / 2048 & 498 & 8,190 & 12,677 & 642 \\ \hline \multirow{2}{*}{**\#Cascades**} & 10,000 & 20,000 / 10,000 & 20,000 / 10,000 & 20,000 / 10,000 & 8,304 & 43,365 & 3,461 & 642 \\ \hline \hline \end{tabular}
\end{table}
Table 2. Dataset details.

Figure 3. Running time on HR.

results for seed sets selected based on the estimated \(\mathbf{A}\) of these two datasets, by FIM and NMF respectively. We can observe that the spread values obtained based on FIM's estimated \(\mathbf{A}\) are significantly larger than those based on NMF's output. This observation further supports the high learning ability of FIM over NMF.

**Network inference on synthetic networks.** Recall that, once the parameter matrix is learned, we can infer a most likely diffusion topology by applying a predefined threshold \(\lambda^{*}\) to the estimated \(\mathbf{A}\), i.e., \(\mathbf{A}[u,v]\geq\lambda^{*}\) indicating the existence of the edge \((u,v)\). In this experiment, we assess the network inference performance of FIM and NMF, on the \(6\) synthetic datasets, for which the ground-truth networks are available. Here, we fixed the time window \(T=10\) and the time interval \(\varepsilon=1.0\). When an estimated parameter matrix \(\mathbf{A}\) is obtained, we set the threshold \(\lambda^{*}=0.01\) to determine the existence of an edge.

We use the _F1-score_ metric to measure the performance on predicting the existence of edges, and the results are presented in Table 5. We can observe that the F1-scores of FIM are significantly higher compared to those of NMF, especially on Hier1024 and Core1024. For a complete perspective, besides the F1-score, we also provide the BCE loss of parameter learning on those synthetic datasets. Once again, FIM achieves consistently a smaller BCE loss than NMF, for all the synthetic datasets.

Finally, in this same setting (\(T=10\) and \(\varepsilon=1.0\)), Figure 5 compares the training time of FIM and NMF on the synthetic datasets. Similar to the results of Figures 3 and 4, FIM outperforms NMF significantly (e.g., speedup of up to \(8.24\times\) on Hier2048).

### Influence estimation on inferred networks

**Setting and baselines.** By fixing the underlying inferred networks, we can evaluate the performance of FIM (Section 4.2) for influence estimation (IE), against the baselines ConTinEst (Chen et al., 2018) and NMF (Kang et al., 2018).

We randomly generate a series of source sets \(\mathcal{S}\) with size \(|\mathcal{S}|\in\{4,5,6,7,8,9,10\}\) from the test cascades. We then compute the _mean absolute error_ (MAE) between the spread estimated by each method and the ground-truth, denoted by _IE MAE_, as well as the influence estimation time. For a fair comparison, we use the same sample size for FIM and ConTinEst (no sampling in NMF).

**Results.** Due to space limitations, we chose to present the results of IE on the inferred networks from MemeTracker and Core2048, since we observed similar results across the other synthetic datasets. From the results on IE MA (Figure 6) we can conclude FIM achieves the smallest MAE on both datasets, regardless of the source set size. The IE MAEs of NMF are notably larger than those of the competitors on MemeTracker, while ConTinEst performs clearly the worst on Core2048. This observation also indicates the robustness of FIM across diverse datasets.

Figure 7 compares the influence estimation times for the three tested methods. We can observe that FIM is _orders of magnitude_ faster than NMF and ConTinEst on both datasets, which confirms the gains by our proposed sampling technique SDTS (Section 4.2). In particular for NMF, its running time consists of the model loading time, i.e., loading the pre-trained model for IE, and the estimation time, while the former obviously dominates the latter, as observed in our experiment.

### Scalability evaluation on large networks

**Larger real-world datasets.** To further explore the scalability of FIM, we employ two large real-world datasets, i.e., Weibo and Twitter, containing \(8,190\) and \(12,677\) nodes respectively, for continuous-time network inference. To the best of our knowledge, Twitter is the largest real-world dataset tested in the literature for this problem.

Table 6 presents the BCE loss under various time window values \(T\) and time intervals \(\varepsilon\), while Figure 8 plots the corresponding running time of FIM. It is worth pointing out that the BCE loss of FIM on Twitter is surprisingly low (at most \(0.01\)). Regarding the running time on the two datasets, FIM completes the training phase within \(30\) seconds for the largest time window \(T=15\) and around \(35\) seconds for the smallest \(\varepsilon=0.5\). These results not only validate the practical interest of FIM for network inference from cascades,

\begin{table}
\begin{tabular}{l|c c c c c c c c c}  & **Method** & \multicolumn{4}{c|}{**Time window \(T\) (s)**} & \multicolumn{4}{c}{**Time interval \(\varepsilon\)**} \\  & 5 & 8 & 10 & 12 & 15 & 0.5 & 1.0 & 1.5 & 2.0 \\ \hline Weibo & FIM & 0.69 & 1.07 & 1.79 & 1.93 & 2.25 & 2.65 & 1.79 & 0.82 & 0.77 & 758 \\ \hline Twitter & FIM & 0.003 & 0.006 & 0.008 & 0.01 & 0.014 & 0.015 & 0.008 & 0.005 & 0.005 \\ \hline \end{tabular}
\end{table}
Table 6. BCE loss on Weibo and Twitter.

Figure 5. Running time on synthetic datasets.

Figure 6. Influence estimation MAE.

but are also promising indicators that our method could scale to even larger datasets.

**Larger synthetic datasets.** To further investigate the efficiency gains of FIM over the baseline ConTinEst, for influence estimation, we also generated a synthetic dataset, \(\text{Core}_{4096}\), with \(4096\) nodes and \(10000\) cascades. The corresponding MAE and running time of the two tested methods are in Table 7. (Recall that both NetRate and NMF are OOM in our computing environment for networks of this scale). First, we observe the IE MAEs of FIM are consistently smaller than those of ConTinEst, with a difference ranging from \(11.70\%\) to \(33.98\%\), which is in line with the results of Figure 6.

Moreover, FIM clearly outperforms ConTinEst in terms of running time. In particular, the speedup of FIM over the state-of-the-art ConTinEst is up to \(100-120\times\), so two orders of magnitude faster. These observations, along with the results in Figure 7, further support the scalability of our approach.

## 6. Related Work

**Network Inference.** Gomez-Rodriguez et al. (2018) establish a generative probabilistic model to calculate the likelihood of cascade data. They devise the NetIng approach to infer the network connectivity, by a submodular maximization, aiming to maximize the cascades' likelihood. Similarly, Gomez-Rodriguez et al. (2018) set a conditional likelihood of transmission with parameter \(a_{i,j}\) for each pair of nodes \(i,j\) and derive the corresponding survival and hazard functions to express the likelihood of a cascade. They propose the algorithm NetRate to maximize the likelihood of cascades, by optimizing the pairwise parameter \(\alpha_{i,j}\), which is the indicator of existence for edge (\(i,j\)). To capture heterogeneous influence among nodes (instead of following a fixed, parametric form), Du et al. (2019) adopt a linear combination of multiple parameterized kernel methods to approximate the hazard function for the cascade likelihood maximization. This approach is shown to be more expressive than previous models. Later, Gomez-Rodriguez et al. (2018) develop a more general additive and multiplicative risk model by adopting survival theory. As a result, the network inference problem is solved via convex optimization. We did not include the methods of Du et al. (2019), Gomez-Rodriguez et al. (2018) in our experimental comparison, as we were not able to obtain their implementation.

**Influence Estimation.** Du et al. (2019) explores the influence estimation problem when the underlying network and transmission parameters are accessible. They point out that the set of influenced nodes is tractable through the shortest-path property. Based on this finding, they devise a novel size estimation method, by using a randomized sampling method from (D'Angelo et al., 2018) and develop the ConTinEst algorithm for influence estimation. Later, Du et al. (2019) further study the estimation of transmission parameters when only the network and cascade data are available. They propose InfuLearner to learn the diffusion function - using a convex combination of random basis functions - directly from cascade data, by maximizing the likelihood. However, InfuLearner requires knowledge of the cause (source node) of each activation in the cascades. Instead of focusing on the global influence of a seed set, Qiu et al. (2019) aims to predict the local social influence of each individual user. To this end, they design an end-to-end framework called DeepIng to learn latent representations, by incorporating both the network topology and user-specific features. Recently, He et al. (2018) adopt neural mean-field dynamics to design NMF to approximate continuous-time diffusion processes. As shown by our experiments, our model FIM outperforms NMF in terms of both efficiency and accuracy, for the problems of network inference and influence estimation.

## 7. Conclusion

In this paper, we revisit the problems of network inference and influence estimation from continuous-time diffusion cascades. We propose the framework FIM, with the objective of improving upon both the learning ability and the scalability of existing methods. To this end, we first propose a continuous-time dynamical system to model diffusion processes, and build our framework FIM for network inference based on it. Furthermore, we improve upon the influence estimation by proposing a new sampling technique. We analyze the approximation error of FIM for network inference and the effect thereof on influence estimation. Comprehensive experimental results demonstrate the state-of-the-art performance of FIM for both network inference and influence estimation, as well as its superior scalability and applicability to real-world datasets. As a future work, we intend to extend and adapt FIM to different diffusion functions, besides the exponential one.

\begin{table}
\begin{tabular}{l|l|c c c c c c c} \hline \hline  & \multirow{2}{*}{**Method**} & \multicolumn{6}{c}{Scod Set Set \(|S|\)} \\  & & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ \hline \multirow{2}{*}{FIM} & FIM & 34.682 & 571.38 & 355.36 & 253.58 & 299.41 & 290.72 & 300.26 \\  & ConTinEst & 401.92 & 433.66 & 402.42 & 384.46 & 406.90 & 362.82 & 353.24 \\ \hline \multirow{2}{*}{KT (rec)} & FIM & 19.22 & 20.87 & 21.42 & 19.13 & 20.19 & 20.98 & 19.93 \\  & ConTinEst & 2172.89 & 2219.13 & 2189.17 & 2271.28 & 2291.80 & 2156.48 & 2210.29 \\ \hline \hline \end{tabular}
\end{table}
Table 7. IE MAE and running time (RT) on \(\text{Core}_{4096}\).

Figure 8. Running time of FIM on Weibo and Twitter.

Figure 7. Influence estimation times.

## References

* (1)
* Altieri et al. (2020) Nick Altieri, Rebecca L. Barter, James Duncan, Raav Dwivedi, Karl Kumbier, Xiao Li, Robert Netzberg, Brian Prakt, Chanish Singh, Yan Shao Yu, et al. 2020. Cuning A copolymer-19 data repository and forecasting country-level death counts in the United States. _arXiv preprint arXiv:2005.00528_ (2020).
* Omer et al. (2020) Serena Omer Arik, Chun-Liang Li, Jinsung Yoon, Rajarshi Sinha, Arkady Epshteyn, Long T. Le, Vikas Meteon, Shashank Singh, Leyon Zhang, Martin Nicholter, Yush Sontinka, Hokun Nakhci, Eliash Kisiel, and Tomas Pfister. 2020. Interpretable Sequence Learning for CoV-19 Forecasting. In _NeurIPS_.
* Bogani and Staotras (2002) Maria Bogani and Romualdo Pastor-Satorras. 2020. Epidemic spreading in correlated complex networks. _Physical Review E_. 64, 4 (2002).
* Brin and Stuck (2002) Michael Brin and Garrett Stuck. 2002. _Introduction to dynamical systems_. Cambridge university press.
* Clauset et al. (2008) Aaron Clauset, Cristiogber Moore, and Mark EJ Newman. 2008. Hierarchical structure and the prediction of missing links in networks. _Nature_ 453, 7191 (2008), 98-101.
* Cohen (1997) Edith Cohen. 1997. Size-Estimation Framework with Applications to Transitive Closure and Readability. _J. Comput. Syst. Sci._ 55, 3 (1997), 441-453.
* Dijkstra (1959) E Dijkstra. 1959. A note on two problems in connection with graphs. _Numer. Math._ 1 (1959), 269-271.
* Du et al. (2014) Nam Du, Ying Liang, Maria-Florina Balcan, and Le Song. 2014. Influence Function Learning in Information Diffusion Networks. In _ICML_, Vol. 32. 2016-2024.
* Du et al. (2013) Nan Du, Le Song, Manuel Gomez-Rodriguez, and Hongyuan Zha. 2013. Scalable Influence Estimation in Continuous-Time Diffusion Networks. In _NIPS_. 3147-3155.
* Du et al. (2012) Nan Du, Le Song, Alexander J. Smola, and Ming Yuan. 2012. Learning Networks of Heterogeneous Influence. In _NIPS_. 2789-2797.
* Gomez-Rodriguez et al. (2011) Manuel Gomez-Rodriguez, David Balduzzi, and Bernhard Scholkopf. 2011. Uncovering the Temporal Dynamics of Diffusion Networks. In _ICML_. 561-568.
* Gomez-Rodriguez et al. (2010) Manuel Gomez-Rodriguez, Jure Leskovec, and Andreas Krause. 2010. Inferring networks of diffusion and influence. In _SIGKDD_. 1019-1028.
* Gomez-Rodriguez et al. (2013) Manuel Gomez-Rodriguez, Jure Leskovec, and Bernhard Scholkopf. 2013. Modeling information Propagation with Survival Theory. In _ICML_, Vol. 28. 666-674.
* Gomez-Rodriguez and Scholkopf (2012) Manuel Gomez-Rodriguez and Bernhard Scholkopf. 2012. Influence Maximization in Continuous Time Diffusion Networks. In _ICML_.
* Gomez-Rodriguez et al. (2016) Manuel Gomez-Rodriguez and Bernhard Scholkopf. 2016. Submodular Inference of Diffusion Networks from Multiple Trees. In _ICML_.
* Gomez-Rodriguez et al. (2016) Manuel Gomez-Rodriguez, Le Song, Nan Du, Hongyuan Zha, and Bernhard Scholkopf. 2016. Influence Estimation and Maximization in Continuous-Time Diffusion Networks. _ACM Trans. Inf. Syst._ 34, 2 (2016), 91-93.
* Grant and Boyd (2014) Michael Grant and Stephen Boyd. 2014. CVX: Matlab software for disciplined convex programming, version 21.
* He et al. (2020) Shuhan He, Hongyuan Zha, and Xiaojing Ye. 2020. Network Diffusions via Neural Mean-Field Dynamics. In _NeurIPS_.
* Hoch and Lerman (2014) Nathan O Hoch and Kristina Lerman. 2014. The simple rules of social contagion. _Scientific reports_ 1, 4 (2014), 4343.
* Hoffding (1963) Wassily Hoffding, 1963. Probability Inequalities for Sums of Bounded Random Variables. _J. Amer. Statist. Assoc._ (1963), 13-30.
* Iwata et al. (2013) Tomoharu Iwata, Janor Shah, and Zoubin Ghahramani. 2013. Discovering latent influence in online social activities via shared cascade poisson processes. In _SIGKDD_. 266-274.
* Katok and Hasselblatt (1997) Amide Katok and Boris Hasselblatt. 1997. _Introduction to the modern theory of dynamical systems_. Cambridge university press.
* Kempe et al. (2003) David Kempe, Jon M. Kleinberg, and Farwa Todorov. 2003. Maximizing the spread of influence through a social network. In _SIGKDD_. 137-146.
* Kamp et al. (2006) David Kamp, Jon M. Kleinberg, and Kay Tarlow. 1996. Individual Nodes in a Diffusion Model for Social Networks. In _ACLP_. Vol. 3508. 1127-1138.
* Kossinets and Watts (2006) Gu Georgi Kossinets and Duncan J Watts. 2006. Empirical analysis of an evolving social network. _Science_ 311, 5777 (2006), 89-90.
* Leskovec et al. (2009) Theor Leskovec, Lars Beckstrom, and Jon M. Kleinberg. 2009. Meme-tracking and the dynamics of the news cycle. In _SIGKDD_. 497-506.
* Leskovec et al. (2010) Jure Leskovec, Deepayan Chakrabarti, M. Kleinberg, Christos Faloutsos, and Zoubin Ghahramani. 2010. Kronecker Graphs: An Approach to Modeling Networks. _J. Math. Learn. Res._ 71 (2010), 985-1012.
* Leskovec et al. (2008) Jure Leskovec, Kevin J. Lang, Austin Daupuis, and Michael W. Mahoney. 2008. Statistical properties of community structure in large social and information networks. In _WWW 2008. 695-704.
* Moreno and A Makse (2015) Flawara Moreno and Herran A Makse. 2015. Influence maximization in complex networks through optimal percolation. _Nature_ 524, 7563 (2015), 65-68.
* Myers and Leskovec (2010) Seth A. Myers and Dave Leskovec. 2010. On the Connectivity of Latent Social Network Inference. In _NIPS_. 1741-1749.
* Narasimani et al. (2015) Harishima Narasimani, David C. Pakes, and Yaron Singer. 2015. Learnability of Influence in Networks. In _NIPS_. 3186-3194.
* Newman (2018) Mark Newman. 2018. Networks. Oxford university press.
* Qin et al. (2018) Jezhong Qin, Jian Tang, Hao Ma, Yuxiao Dong, Ruansan Wang, and Jie Tang. 2018. DeepInf: Social Influence Prediction with Deep Learning. In _SIGKDD_. 2110-2119.
* Snedan et al. (2017) Meghdad Snedan, Moein Khaligh, Nahid Azimi-Tafrielis, CR Jahri, and Marcel Anselon. 2017. Memory effects on epidemic evolution: The susceptible-infected-recovered epidemic model. _Physical Review E_. 95, 2 (2017), 022409.
* Saito et al. (2010) Kazumi Saito, Masahiko Kimura, Kouroun Ohara, and Hiroshi Motoh. 2010. Selecting Information Diffusion Models over Social Networks for Behavioral Analysis. In _PLENIX_. Vol. 623. 180-195.
* Yee et al. (2011) Greg Yee Yee, Samuel R. Choudh, and Kristina Lerman. 2011. What Stop Social Epidemic? In _ICWSM_.
* Tang et al. (2015) Youze Tang, Yanchen Shi, and Xiaokui Xiao. 2015. Influence Maximization in Near-Linear Time A Martingale Approach. In _SIGMOD_. ACM.
* Wortmann et al. (2008) Jennifer Wortmann, Osoi. Visualizing and the diffusion of trends on social networks. _Technical Reports_ (CSS) (2008), 880.
* Xie et al. (2015) Miao Xie, Qunusong Yang, Qing Wang, Gao Cong, and Gerard Melo. 2015. Dynaffirure: A dynamic diffusion model for continuous time constrained influence maximization. In _AAAI_. Vol. 29.
* Yang and Leskovec (2010) Jawon Yang and Jure Leskovec. 2010. Modeling information diffusion in implicit networks. In _ICDM_. IEEE, 599-608.
* Zhang et al. (2013) Jing Zhang, Shao Liu, Jie Tang, Ting Chen, and Juan I. 2013. Social Influence Lookify for Modeling Reviewing Behaviors. In _IJCAI_. 2761-2767.
* Zhang et al. (2016) Zhijian Zhang, Hong Wu, Kun Yue, Jin Li, and Weiyi Lu. 2016. Influence Maximization for Cascade Model with Diffusion Decay in Social Networks. In _ICICTSE_, Vol. 623. 418-427.

## Appendix A Appendix

### Proofs

Proof of the Lemma 1.: For an observation \(\mathbf{h}_{t}\), let \(\mathcal{N}_{\theta}^{a}=\{u\mid\phi_{t,u}=1\forall u\in\mathcal{N}_{\theta}\}\) be the set of active neighbors of node \(v\) at time \(t\). Let \(t_{l}\) be the time when the neighbor \(u_{l}\in\mathcal{N}_{\theta}^{a}\) succeeds in activating node \(v\) individually conditioned on \(\mathbf{h}_{t}\) for \(i\in\{1,2,\ldots,|\mathcal{N}_{\theta}^{a}|\}\). Given a time interval \(\tau\),

\[\Pr\{\phi_{t\tau,v}=1\mid\mathbf{h}_{t}\}\] \[=1-\Pr\big{[}\bigcup_{u_{l}\in\mathcal{N}_{\theta}^{a}}\{t_{l}>t +\tau\mid t_{l}>t\}\big{]}\] \[=1-\Pr\big{[}\bigcup_{u_{l}\in\mathcal{N}_{\theta}^{a}}\{t_{l}>t +\tau,t_{l}>t\mid t_{l}>t\}\big{]}\] \[=1-\prod_{u_{l}\in\mathcal{N}_{\theta}^{a}}\frac{e^{-\lambda_{u_{l }u_{l}}t}}{e^{-\lambda_{u_{l}u_{l}}t}}\] \[=1-\prod_{u_{l}\in\mathcal{N}_{\theta}^{a}}e^{-\lambda_{u_{l}u_{ l}}t}\] \[=1-e^{-\tau\sum_{u_{l}\in\mathcal{N}_{\theta}^{a}}\lambda_{u_{l}u _{l}}},\]

When \(\tau\to 0^{+}\) (approaches \(0\) from the positive side) we have \(\gamma_{t,v}=\sum_{u\in\mathcal{N}_{\theta}}\lambda_{u_{l}u_{l}}\phi_{t,u}\). 

Proof of Lemma 2.: Notice that the probability for a random node \(v\in\mathcal{V}\) being inactive at time \(t\) is \(1-\mathbb{E}[\mathbf{q}_{t,v}\mid\mathcal{H}_{t}]\). According to Lemma 1, we have

\[\Gamma_{t,v}=\mathbb{E}\big{[}(1-\Phi_{t,v})\sum_{u\in\mathcal{N}_{\theta}} \lambda_{u_{l}u_{l}}\cdot\Phi_{t,u}\mid\mathcal{H}_{t}\big{]}.\]

For node set \(\mathcal{V}\), it is straightforward to obtain Equation (2), which completes the proof. 

Proof of Theorem 1.: Equation (4) is proved in Lemma 2 and Equation (5) is the initial state given set \(\mathcal{S}\). In what follows, we then prove Equation (3). Before that, we first establish the following ordinary differential equation.

\[\frac{\mathrm{d}\mathbb{E}[\Phi_{t}]\mathcal{H}_{t}]}{\mathrm{d}t}=\Gamma_{t} \tag{11}\]

First, for any \(v\in V\), we have

\[\frac{\mathrm{d}\mathbb{E}[\Phi_{t,v}]\mathcal{H}_{t}]}{\mathrm{d}t}\] \[=\lim_{\tau\to 0^{+}}\frac{\mathbb{E}[\Phi_{t,v}\gets \Phi_{t}]\mathcal{H}_{t}]}{\tau}\] \[=\lim_{\tau\to 0^{+}}\frac{\Pr[t_{v}\in[t,t+\tau]\lfloor\mathbf{g} \sigma(t)\rfloor}{\tau}\] \[=\lim_{\tau\to 0^{+}}\frac{\Pr[t_{v}\in[t,t+\tau]\lfloor\mathbf{g} \sigma(t)\rfloor}{\tau\lfloor\mathbf{g}\sigma(t)\rfloor}\] \[=\lim_{\tau\to 0^{+}}\frac{\Pr[t_{v}\in[t,t+\tau]\lfloor\mathbf{g} \sigma(t)\rfloor}{\Pr[t_{v}\in[t,0]\setminus\mathcal{H}_{t}]}\] \[=\lim_{\tau\to 0^{+}}\frac{\Pr[t_{v}\in[t,t+\tau]\lfloor\mathbf{g} \sigma(t)\rfloor}{\Pr[t_{v}\in[t,t+\tau]\lfloor\mathbf{g}\sigma(t)\rfloor}\] \[=\lim_{\tau\to 0^{+}}\frac{\Pr[t_{v}\in[t,t+\tau]\lfloor\mathbf{g} \sigma(t)\rfloor}{\Pr[t_{v}\in[t,t+\tau]\lfloor\mathbf{g}\sigma(t)\rfloor}\] \[=\lim_{\tau\to 0^{+}}\frac{\Pr[t_{v}\in[t,t+\tau]\lfloor\mathbf{g} \sigma(t)\rfloor}{\tau\lfloor\mathbf{g}\sigma(t)\rfloor+\tau}\] \[=\lim_{\tau\to 0^{+}}\frac{\Gamma_{t,v}\in[t,t+\tau]\cdot\tau}{e^{- \tau\sum_{u_{l}\in\mathcal{N}_{\theta}^{a}}\tau}} \tag{12}\]

According to the Euler method, we could derive Equation (3) from Equation (11). 

Proof of Theorem 2.: Given observation \(\mathcal{H}_{t_{0}}\) at time \(t_{0}\) and \(v\in\mathcal{V}\), we have

\[\mathbb{E}[\Phi_{t_{0}t+\tau,0}\mid\mathcal{H}_{t_{0}}]\] \[=\mathbb{E}[\Phi_{t_{0},v}\mid\mathcal{H}_{t_{0}}]+\sum_{l=1}^{ \infty}\frac{\mathbb{E}[\Phi_{t_{0},v}^{(l_{0})}|\mathcal{H}_{t_{0}}]}{t!}e^{t}\]

where \(\Phi_{t,v}^{(l)}\) is the \(i\)-th derivative of \(\Phi_{t,v}\). Since we have \(\mathbb{E}[\Phi_{t,v}^{(1)}\mid\mathcal{H}_{t_{0}}]=\Gamma_{t_{0},v}\mathrm{e}^ {-\Gamma_{t_{0},v}(t-t_{0})}\). In this regard, we have

\[\mathbb{E}[\Phi_{t,v}^{(l)}\mid\mathcal{H}_{t_{0}}]=(-1)^{+1}\Gamma_{t_{0},v}^{ t_{0}}e^{-\Gamma_{t_{0},v}(t-t_{0})}.\]

Therefore, we have

\[\xi_{v}(t_{0},v)=\varepsilon\Gamma_{t_{0},v}=\sum_{l=1}^{\infty} \frac{\mathbb{E}[\Phi_{t_{0},v}^{(l_{0})}|\mathcal{H}_{t_{0}}]}{t!}e^{l}\] \[=\varepsilon\Gamma_{t_{0},v}+\sum_{l=1}^{\infty}\frac{(-\varepsilon \Gamma_{t_{0},v})^{l}}{t!}\] \[=\varepsilon\Gamma_{t_{0},v}+\sum_{l=0}^{\infty}\frac{(-\varepsilon \Gamma_{t_{0},v})^{l}}{t!}-1\] \[=\varepsilon\Gamma_{t_{0},v}+\mathrm{e}^{-\varepsilon\Gamma_{t_{0},v} }-1\]

Lemma 4 (Hoeffding Inequality [20]).: _Let \(x_{i}\) be an independent bounded random variable such that for each \(1\leq i\leq\theta\), \(x_{i}\in[a_{i},b_{i}]\). Let \(X=\frac{1}{2}\sum_{i=1}^{\theta}x_{i}\). Given \(\eta>0\), then_

\[\Pr[|X-\mathbb{E}[X]|\geq\eta]\leq 2\mathrm{e}^{-\frac{2\theta^{2}v^{2}}{\sum_{i=1}^{ \theta}x_{i}(b_{i}-a_{i})^{2}}}.\]

Proof of Theorem 3.: Given seed set \(\mathcal{S}\) and diffusion rate vector \(\rho^{*}\), let \(\mathcal{O}\) be the sampling space of instance \(\mathcal{G}\) of \(\mathrm{G}\). For instance \(\mathcal{G}\in\mathcal{O}\), let \(\ell\) be the shortest path in \(\mathcal{G}\) from certain source node \(u_{0}\in\mathcal{S}\) to node \(u_{k}\in\mathcal{V}\) within time \(T^{*}\), i.e., \(\ell=\{(u_{l},u_{l+1})\mid i\in\{0,1,\ldots,k-1\}\}\). Let \(\lambda_{i}^{*}\in\rho^{*}\) be the diffusion rate and \(t_{i}^{*}\) be the sampled diffusion time for edge \((u_{i},u_{l+1})\). For estimation \(\rho\), the corresponding diffusion rate and diffusion time are \(\lambda_{i}\) and \(t_{i}\) respectively. We consider two cases.

**Case one:**\(\lambda_{i}\in[\lambda_{i}^{*},\lambda_{i}^{*}+e]\). In this case, \(t_{i}\) can be lower bounded by samplings from \(\min(t_{i}^{*},\rho(\epsilon,t))\). Thus we have

\[\mathbb{E}[t_{i}] \geq\int_{0}^{t_{i}^{*}}t\rho(\epsilon,t)\mathrm{d}t+\left(1-\int_ {0}^{t_{i}^{*}}\rho(\epsilon,t)\mathrm{d}t\right)t_{i}^{*}\] \[=\int_{0}^{t_{i}^{*}}t\cdot\mathrm{e}^{-\epsilon\mathrm{d}t}+ \left(1-\int_{0}^{t_{i}^{*}}\mathrm{e}^{-\epsilon\mathrm{d}t}\right)t_{i}^{*}\] \[=-t_{i}^{*}\mathrm{e}^{-\epsilon\ell t_{i}^{*}}-\frac{\epsilon^{ -\epsilon^{-\epsilon^{\prime}}}}{\epsilon}+t_{i}^{*}\mathrm{e}^{-\epsilon t_{i}^{*}}\epsilon\] \[=\frac{1-\epsilon^{-\epsilon^{\prime}}}{\epsilon}\]

Since \(\frac{\mathbb{E}[t_{i}]}{t_{i}^{*}}\geq\frac{1-\epsilon^{-\epsilon^{\prime}}}{ \epsilon t_{i}^{*}}\geq\frac{1-\mathrm{e}^{-\epsilon^{\prime}}}{\epsilon T^{*}}\), we have \(\frac{\mathbb{E}[T]}{T^{*}}\geq\frac{1-\mathrm{e}^{-\epsilon^{\prime}}}{\epsilon T^{*}}\).

**Case two:**\(\lambda_{i}\in[\lambda_{i}^{*}-\epsilon,\lambda_{i}^{*}]\). The probability \(p_{i}\) that \(u_{i}\) reaches \(u_{i+1}\) in instance \(\mathcal{G}\) is \(p_{i}=\int_{0}^{t_{i}^{*}}p(\lambda_{i}^{*},t)\mathrm{d}t\). Therefore, for estimation \(\lambda_{i}\) and \(t_{i}\), it requires \(\int_{0}^{t_{i}^{*}}p(\lambda_{i}^{*},t)\mathrm{d}t=\int_{0}^{\mathbb{E}[t_{i}]}p( \lambda_{i},t)\mathrm{d}t\). It is easy to obtain that \(\frac{\mathbb{E}[t_{i}]}{t_{i}^{*}}\geq\frac{\lambda_{i}^{*}}{\lambda_{i}^{*}+ \epsilon}\leq\frac{\min(\rho^{*})}{\min(\rho^{*})-\epsilon}=\frac{\epsilon}{ \epsilon<1}\), which completes the proof.

### Other Experimental Details

#### a.2.1. Publicly available datasets

**MemeTracker**.: MemeTracker (Maee et al., 2018) is a dataset aiming to track the diffusion of memes, e.g., frequent quotes or phrases, in online websites and blogs over time. Specifically, it collects a million news stories and blog posts and then selects the most frequent quotes and phrases as "memes". By regarding each meme as an information item and each URL or blogs as a user, it tracks the diffusion of memes.

**Sina Weibo**.: Sina Weibo (Sina Weibo, 2018) is a major micro-blogging platform in China. With an underlying follower - followee network, Weibo users can share news or comments with timestamps and their followers can repost the content. By tracking the diffusion of each post, we have continuous-time cascades.

**Twitter**.: Similar to Weibo, Twitter (Welton, 2018) is the most popular micro-blogging application. With an underlying follower-followee network, each tweet with URLs is taken as a node and all tweets with URLs posted during October 2010 are collected in Twitter dataset which contains the complete tweeting trace of each post during that period.

**Pre-processing**.: Following the methodology of previous works (Sina and Weibo, 2018; Weibo, 2018), we only keep the nodes that are the most involved in diffusions, in order to refine the quality of the cascade data for network inference and influence estimation. Specifically, a node is seen as actively participating in diffusions if it appears in at least 5 cascades. We remove the cascades that do not contain such nodes.

**Cascades**.: One cascade e consists of activation timestamps of all the participating nodes. Specifically, cascades for the above three real-world datasets were collected online. Cascades for the HR dataset are provided by (Welton, 2018) and the cascades of the 6 synthetic datasets were generated by simulating the diffusion process. One particularity is that the HR cascades may contain multiple source nodes, i.e., several nodes with active times equal to 0, while the cascades of all the other datasets can only have one source node.

#### a.2.2. Experimental setting

**Computing environment**.: All experiments are conducted on a machine with an NVIDIA RTX A5000 GPU (24GB memory), AMD EPYC CPU (1.50 GHz), and 500 GB of RAM.

**Source code of FIM and baselines.** During the open review phase of the paper, we will make the source code of FIM publicly accessible through an anonymous GitHub link. For baseline methods, we downloaded their implementation from the official releases, i.e., NetRate3, ConTinEst4, and NMF5.

Footnote 4: [https://github.com/Networks-Learning/netrate](https://github.com/Networks-Learning/netrate)

Footnote 5: [https://dmann.github.io/DcSina/Data/NMF5-2013.html](https://dmann.github.io/DcSina/Data/NMF5-2013.html)

Footnote 6: [https://github.com/Shuhahfa/net-mf](https://github.com/Shuhahfa/net-mf)

**Ground-truth spread.** By following the convention (Brockman et al., 2018; Welton, 2018), the ground-truth spread of a source set \(\mathcal{S}\) is evaluated as follows. Given a source set \(\mathcal{S}\), we first randomly sample from the test cascades a cascade for each node in \(\mathcal{S}\), and then take the union of the nodes from the sampled cascades, leading to one estimate of \(\mathbb{E}[\mathcal{S}]\). We repeat this process 1000 times and compute the average over estimates as the ground-truth spread.

**Motivation and rationale behind IM experiments.** Besides the binary cross-entropy (BCE) loss, we designed an additional experiment, for CIM, to further verify the quality of the estimated parameter matrix \(\mathbf{A}\). The rationale is the following. For the sake of the example, let \(\mathbf{A}_{\text{FIM}}\) and \(\mathbf{A}_{\text{NMF}}\) be the estimated parameter matrices by FIM and NMF respectively, and let \(\mathbf{A}^{*}\) be the true parameter matrix (unknown). W.l.o.g, let \(S_{\text{FIM}}\) and \(S_{\text{NMF}}\) be the seed sets of the same sizes selected by the state-of-the-art CIM algorithm on \(\mathbf{A}_{\text{FIM}}\) and \(\mathbf{A}_{\text{NMF}}\) respectively, and \(\mathcal{S}^{*}\) be the unknown optimal seed set (i.e., with the largest expected spread) on \(\mathbf{A}^{*}\) (were it known). For any selected seed set \(\mathcal{S}\), its _ground-truth spread_\(\mathbb{E}[I(\mathcal{S})]\) on \(\mathbf{A}^{*}\) can be estimated using the testing cascades, as described previously. If we observe that \(\mathbb{E}[I(S_{\text{NMF}})]<\mathbb{E}[I(S_{\text{FIM}})]\), this means \(\mathbb{E}[I(S_{\text{FIM}})]\) is closer to \(\mathbb{E}[I(\mathcal{S}^{*})]\) since \(\mathcal{S}^{*}\) has by definition the largest expected spread. This indicates that \(\mathbf{A}_{\text{FIM}}\) is closer to the optimal \(\mathbf{A}^{*}\), hence represents further proof for the higher quality in the estimation of \(\mathbf{A}\).

### Additional Experimental Results

**Evaluation of NetRate for parameter learning.** We assess in this experiment NetRate's performance on HR dataset, from the perspective of influence spread. Since HR's cascades have multiple source nodes, we cannot directly use the ground-truth spread estimation. As a workaround, we use the validation and test cascades (2000 cascades) as follows. We take the node set of each cascade \(\mathbf{c}\) within the time window \(T=10\) as the ground-truth spread of the source set \(\mathcal{S}\). Subsequently, we run FIM, NMF, and NetRate for influence estimation (IE) based on their own estimated \(\mathbf{A}\) and then compare the IE absolute error of each cascade. Figure 9 plots the IE absolute error in non-decreasing order, by the three tested methods. As shown, FIM achieves notably smaller absolute error than the other two methods. In particular, the mean absolute error (MAE) for FIM, NMF, and NetRate are \(12.42\), \(28.86\), and \(17.90\), respectively. In other words, the MAE of FIM is significantly lower than the one of both NMF and NetRate, representing only \(43.05\) and \(69.43\)% of their respective values. This result further confirms the performance of FIM on network inference.

**Additional IM experimental results on MemeTracker.** Recall that Table 4 in Section 5.1 presents the influence maximization results of FIM and NMF on the MemeTracker data, with a time window \(T=10\) and a time interval \(\epsilon=1\). For a comprehensive evaluation, we present here additional results with two more \((T,\epsilon)\) pairs, \((T=10,\epsilon=0.5)\) and \((T=15,\epsilon=1.0)\) in Table 8. Overall,

Figure 9. IE absolute error on HR.

seed sets selected from the estimated \(\mathbf{A}\) by \(\mathsf{FIM}\) lead clearly to larger influence spread than those of \(\mathsf{NMF}\). Compared with the results for the combination (\(T=10,\varepsilon=1.0\)), the spread of \(\mathsf{FIM}\) in the (\(T=10,\varepsilon=0.5\)) setting increases slightly, as expected, since smaller \(\varepsilon\) means smaller estimation error, as proved in Theorem 2. Furthermore, we can observe that the spread of \(\mathsf{FIM}\) in the setting (\(T=15,\varepsilon=1.0\)) decreases, especially when \(|\mathcal{S}|=\{7,8,9,10\}\). This peculiar outcome is likely due to the insufficient number of cascades, which leads to an insufficiently precise estimation of the ground-truth influence spread.

**NetRate reproduction of experimental results.** Recall that NetRate approaches network inference from continuous-time cascades as a convex optimization problem and utilizes CVX as the problem solver [17]. Specifically, NetRate creates an individual convex problem for each node. The number of variables of each convex problem is linear in the number of nodes and cascades, as well as in the size of each cascade. Therefore, as the number of nodes increases, more new convex problems are generated. As the number of cascades increases, more variables are introduced in the objective functions. Therefore, both facets contribute to a very high computational time on reasonably small problem instances.

As a sanity check for our experimental setting, following the experimental framework described in [11], we reproduce here the experiments of NetRate (Figure 3(c) in [11]) and present the reproduced results in Figure 10. Specifically, Figure 10(a) shows our reproduction of NetRate's average running time to infer transmission rates for all incoming edges to a node against network size (number of nodes), compared with the original results in Figure 10(b). The high similarity between these two sub-figures confirms the fidelity of our reproduction and testing setting for NetRate. Moreover, we further investigate NetRate's performance in Figures 11(a) and (b), on the datasets \(Rand_{1024}\) and \(Rand_{2048}\), depending on the number of cascades. The results reveal once more that NetRate's computational time increases drastically as the input size increases.

Figure 11. The Running time on different cascades of NetRate.

\begin{table}
\begin{tabular}{c|c|c c c c c c c} \hline \hline \multirow{2}{*}{**Parameters \(T\) and \(\varepsilon\)**} & \multirow{2}{*}{**Method**} & \multicolumn{6}{c}{Seed Set Size \(|\mathcal{S}|\)} & \multirow{2}{*}{1350} \\  & & & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\ \hline \multirow{2}{*}{\(T=10\), \(\varepsilon=1.0\)} & \(\mathsf{FIM}\) & 65.85 & 75.18 & 88.33 & 108.88 & 113.90 & 127.11 & 136.67 \\  & \(\mathsf{NMF}\) & 53.14 & 64.78 & 70.66 & 76.10 & 83.39 & 88.23 & 94.95 \\ \hline \multirow{2}{*}{\(T=10\), \(\varepsilon=0.5\)} & \(\mathsf{FIM}\) & 69.53 & 76.88 & 92.70 & 101.79 & 122.58 & 135.04 & 144.66 \\  & \(\mathsf{NMF}\) & 47.24 & 53.40 & 68.88 & 75.09 & 68.99 & 87.20 & 80.74 \\ \hline \multirow{2}{*}{\(T=15\), \(\varepsilon=1.0\)} & \(\mathsf{FIM}\) & 67.59 & 72.85 & 88.88 & 96.37 & 103.35 & 107.71 & 119.41 \\  & \(\mathsf{NMF}\) & 61.00 & 68.60 & 76.82 & 86.56 & 84.32 & 91.26 & 95.74 \\ \hline \hline \end{tabular}
\end{table}
Table 8. Additional influence maximization on \(\mathsf{MemeTracker}\).

Figure 10. The reproduction of running time of NetRate.