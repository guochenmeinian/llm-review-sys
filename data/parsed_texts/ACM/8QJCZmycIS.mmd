# Policy-Guided Causal State Representation for Offline Reinforcement Learning Recommendation

Anonymous Author(s)

###### Abstract.

In offline reinforcement learning-based recommender systems (RLRS), learning effective state representations is crucial for capturing user preferences that directly impact long-term rewards. However, raw state representations often contain high-dimensional, noisy information and components that are not causally relevant to the reward. Additionally, missing transitions in offline data make it challenging to accurately identify features that are most relevant to user satisfaction. To address these challenges, we propose Policy-Guided Causal Representation (PGCR), a novel two-stage framework for causal feature selection and state representation learning in offline RLRS. In the first stage, we learn a causal feature selection policy that generates modified states by isolating and retaining only the causally relevant components (CRCs) while altering irrelevant components. This policy is guided by a reward function based on the Wasserstein distance, which measures the causal effect of state components on the reward and encourages the preservation of CRCs that directly influence user interests. In the second stage, we train an encoder to learn compact state representations by minimizing the mean squared error (MSE) loss between the latent representations of the original and modified states, ensuring that the representations focus on CRCs and filter out irrelevant variations. We provide a theoretical analysis proving the identifiability of causal effects from interventions, validating the ability of PGCR to isolate critical state components for decision-making. Extensive experiments demonstrate that PGCR significantly improves recommendation performance, confirming its effectiveness for offline RL-based recommender systems.

Offline Reinforcement Learning, Recommendation, Causal State Representation +
Footnote †: ccs: Information and Control

+
Footnote †: ccs: Information and Control

+
Footnote †: ccs: Information and Control

+
Footnote †: ccs: Information and Control

+
Footnote †: ccs: Information and Control

+
Footnote †: ccs: Information and Control

## 1. Introduction

Reinforcement Learning (RL) has emerged as a powerful approach for developing recommender systems (RS), where the objective is to sequentially learn a policy that maximizes long-term rewards, typically measured by user satisfaction or engagement. Unlike traditional recommendation methods that primarily aim to optimize immediate rewards, RLRS focuses on learning a recommendation strategy that adapts to user preferences over time (Brockman et al., 2016). This allows RLRS to dynamically update recommendations based on user feedback, aiming to improve long-term outcomes and enhance user experiences.

However, deploying RL in recommender systems poses significant challenges. Traditional RLRS rely on continuous user interaction to learn and adapt their policies, which may be impractical in many real-world applications due to concerns such as exploration risks, privacy issues, and computational costs (Brockman et al., 2016). To address these challenges, offline RL-based recommender systems have been proposed, where the goal is to learn optimal recommendation policies from a fixed dataset of historical user interactions without further online data collection. This offline setting leverages existing data to refine and optimize recommendations, but it also introduces some challenges.

A critical aspect in offline RLRS is learning efficient state representations (Brockman et al., 2016; Bockman et al., 2016). In the offline setting, the agent must learn solely from historical data without additional interactions, making the challenges of high-dimensional and noisy state representations more pronounced. The state space, which includes information about user interactions, context, and preferences, is fundamental for deciding actions (i.e., recommendations). However, raw state representations are often complex and may contain components that are not causally relevant to the reward.

Recent advances in representation learning in RL have focused on extracting abstract features from high-dimensional data to enhance the efficiency and performance of RL algorithms (Kirkpatrick et al., 2017; Sutton and Barto, 2018). However, these challenges are compounded in the context of offline RLRS due to the static nature of the data and the inability to interact with the environment. Techniques such as those developed by Zhang et al. (2018), which use the bisimulation metric to learn representations that ignore task-irrelevant information, may encounter challenges when applied directly to offline settings. In particular, missing transitions in the offline dataset can particularly impair the effectiveness of the bisimulation principle, resulting in inaccurate state representation and poor estimation (Zhu et al., 2018). Moreover, the complexity and high-dimensional nature of user data in offline RLRS require isolating the components that are causally relevant to the reward, rather than merely compressing the state space. Thus, there is a need for targeted techniques that emphasize causally critical state components within the constraints of offline learning.

To address these challenges, we propose a policy-guided approach for causal feature selection and state representation learning. Our approach is designed to use a policy to generate intervened states that isolate and retain only the causally relevant components (CRCs). By focusing on the features that directly impact user satisfaction, this method enables the state representation to concentrate on the most informative components, reducing noise and irrelevant variations. Additionally, by creating targeted interventions, this approach augments offline datasets, enhancing the learning of state representations even with finite datasets.

We introduce a method called PGCR (Policy-Guided Causal Representation), which operates in two stages. In the first stage, we learn a causal feature selection policy that generates modified states, retaining the CRCs and modifying the causally irrelevant components (CIRCs). We quantify the causal effect of the state components on the reward, which reflects user feedback, by using the Wasserstein distance between the original and modified reward distributions. This metric effectively measures the distributionalchange caused by the interventions, and we use it to design a reward function that encourages the retention of CRCs while altering CIRCs. Furthermore, we provide theoretical analysis on the identifiability of the causal effects resulting from these interventions. In the second stage, we leverage the learned causal feature selection policy to guide the training of a state representation encoder. Given a pair consisting of an original state and its modified counterpart generated by the causal feature selection policy, the encoder is trained to produce latent representations that preserve only the CRCs. Specifically, we minimize the mean squared error (MSE) loss between the latent representations of the original and modified states, encouraging the encoder to ignore irrelevant variations and focus on causally meaningful features. This process allows the encoder to map states into a latent space where only the information necessary for optimal decision-making is preserved.

Our contributions are as follows:

* We propose PGCR, a novel two-stage framework for offline RL-based recommender systems. In the first stage, we learn a causal feature selection policy to generate modified states that retain causally relevant components. In the second stage, we train an encoder to learn state representations concentrating on these components.
* We design a reward function based on the Wasserstein distance to guide the causal feature selection policy in identifying and retaining the state components that directly influence user interests.
* We provide a theoretical analysis proving the identifiability of causal effects from interventions, ensuring that our method isolates the components of the state critical for decision-making.
* Extensive experiments demonstrate the effectiveness of PGCR in improving recommendation performance in offline RL-based recommender systems.

## 2. Preliminaries

### Offline RL-Based Recommender Systems

Offline Reinforcement Learning (RL) in Recommender Systems (RS) aims to optimize decision-making by learning solely from historical user interaction data within the framework of a Markov Decision Process (MDP). The MDP is represented by the tuple \(\langle\mathcal{S},\mathcal{A},\mathcal{R},\mathcal{P},\gamma\rangle\), where:

* \(\mathcal{S}\) represents the state space, encompassing user data, historical interactions, item characteristics, and contextual factors.
* \(\mathcal{A}\) denotes the action space, which includes all candidate items available for recommendation.
* \(\mathcal{R}:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}\) defines the reward function, based on user feedback such as clicks, ratings, or engagement metrics.
* \(\mathcal{P}\) describes the transition probabilities, governing the dynamics of state transitions.
* \(\gamma\) is the discount factor, used to balance immediate and future rewards.

Unlike online RL, the agent does not interact with the environment in real-time but must infer the optimal policy solely from historical data. In this MDP setup, the agent (RS) learns from a fixed dataset \(\mathcal{D}\) of interactions collected by a behavior policy. Each entry in this dataset consists of a state \(s_{t}\), an action \(a_{t}\) taken by the behavior policy, the resulting reward \(r_{t}\), and the next state \(s_{t+1}\). The primary goal of the RS agent is to learn a policy \(\pi:\mathcal{S}\rightarrow\mathcal{A}\) that maximizes the cumulative discounted return, thereby ensuring the long-term effectiveness of the recommendations provided to the user.

### Causal Models

Causal models provide a structured way to represent and analyze the causal relationships among a set of variables. Consider a finite set of random variables denoted by \(\mathbf{X}=\{X_{1},X_{2},\ldots,X_{n}\}\), each associated with an index in \(\mathbf{V}=\{1,2,\ldots,n\}\). These variables have a joint distribution \(P_{\mathbf{X}}\) and a joint density function \(p(\mathbf{x})\). A causal graphical model is represented by a Directed Acyclic Graph (DAG) \(\mathcal{G}=(\mathbf{V},\mathcal{E})\), where \(\mathbf{V}\) is the set of nodes, each corresponding to one of the variables in \(\mathbf{X}\) and \(\mathcal{E}\) is the set of directed edges between the nodes, indicating direct causal influences.

**Definition 2.1** (Structural Causal Model).: A Structural Causal Model (SCM) \(M=(\mathbf{S},P_{\mathbf{U}})\) associated with a DAG \(\mathcal{G}\) consists of a set \(\mathbf{S}\) of structural equations:

\[X_{i}=f_{i}(\mathbf{PA}_{i},U_{i}),\quad i=1,2,\ldots,n,\]

where \(\mathbf{PA}_{i}\subseteq\mathbf{X}\setminus\{X_{i}\}\) denotes the set of parent variables (direct causes) of \(X_{i}\) in the graph \(\mathcal{G}\). \(U_{i}\) represents the exogenous (noise) variables, accounting for unobserved factors, and \(\mathbf{U}=\{U_{1},\ldots,U_{n}\}\) is the set of all such variables. A joint distribution \(P_{\mathbf{U}}\) over the noise variables \(\mathbf{U}\), assumed to be jointly independent.

Each structural function \(f_{i}\) specifies how \(X_{i}\) is generated from its parents \(\mathbf{PA}_{i}\) and the noise term \(U_{i}\). The combination of the structural equations \(\mathbf{S}\) and the distribution \(P_{\mathbf{U}}\) induces a joint distribution \(P_{\mathbf{X}}\) over the endogenous variables \(\mathbf{X}\).

**Definition 2.2** (Intervention).: An intervention in an SCM \(M\) is an operation that modifies one or more of the structural equations in \(\mathbf{S}\). Specifically, suppose we replace the structural equation for variable \(X_{j}\) with a new equation:

\[X_{j}=f_{j}(\widehat{\mathbf{PA}}_{j},U_{j}).\]

This results in a new SCM \(\widehat{\mathcal{M}}\), reflecting the intervention on \(X_{j}\). The corresponding distribution changes from the observational distribution \(P_{\mathbf{X}}^{M}\) to the interventional distribution \(P_{\mathbf{X}}^{M}\), expressed as:

\[P_{\mathbf{X}}^{M}=P_{\mathbf{X}}^{M\mathbf{I}\cdot do(X_{j}=f_{j}(\widehat{ \mathbf{PA}}_{j},U_{j}))},\]

where the \(do\)-operator \(do(X_{j}=\hat{f}_{j}(\widehat{\mathbf{PA}}_{j},U_{j}))\) denotes the intervention that replaces the structural equation for \(X_{j}\).

## 3. Methodology

### Problem Formulation

To learn a policy that identifies the causally relevant components in the state, we first represent the MDP from a causal modeling perspective. Assuming there are no unobserved confounders, the SCMs for the MDP can be formulated using deterministic equations augmented with exogenous noise variables to capture stochasticity, as shown in Figure 1 (a):

[MISSING_PAGE_FAIL:3]

After the causal feature selection policy intervenes on the action \(a_{t}\), setting it to a specific value \(a_{t}^{T}\), the environment transitions to a new state \(s^{T}\). This intervened state \(s^{T}\) is expected to preserve only the causally relevant components of the original state \(s_{t}\), while any causally irrelevant CIRCs are modified or filtered out.

Since the CRCs are the parts of \(s_{t}\) that have a significant causal impact on rewards, we regard the new state \(s^{T}\), induced by the intervention on \(a_{t}\), as an effective intervention on \(s_{t}\) in the original tuple \(\{s_{t},a_{t},s_{t+1},r_{t}\}\). By comparing the rewards obtained before and after the intervention, we can evaluate the causal effect of the original state \(s_{t}\) on the reward \(r_{t}\), isolating the impact of the causally relevant components.

Formally, following Pearl's rules of \(do\)-calculus (Pearl, 2009), as outlined in Appendix A, the causal effect of \(s_{t}\) on \(r_{t}\) is given by the formula:

\[p^{\mathcal{M}_{t},do(s_{t}:=s^{T})}\left(r_{t}\right)\] \[=\sum_{a_{t}}\int_{\eta_{t}}P(r_{t}\mid do(s_{t}:=s^{T}),a_{t}, \eta_{t})\,P(\eta_{t})\,P(a_{t}\mid do(s_{t}:=s^{T}))\,d\eta_{t}\] \[=\sum_{a_{t}}\int_{\eta_{t}}P(r_{t}\mid s_{t}:=s^{T},a_{t},\eta_{ t})\,P(\eta_{t})\,P(a_{t}\mid s_{t}=s^{T})\,d\eta_{t}\] \[=\mathbb{E}_{a_{t},\eta_{t}}\left[P(r_{t}\mid s_{t}:=s^{T},a_{t}, \eta_{t})\right]. \tag{3}\]

If the intervened probability distribution of the reward is similar to the original distribution, substituting \(s_{t}\) with \(s^{T}\) has a minor causal effect on the reward. This indicates that the causally CRCs of \(s_{t}\) that significantly influence learning the user's interest have been retained. To quantify this effect, we measure the distance between the two probability distributions of the reward before and after the intervention. Inspired by bisimulation for state abstraction (Bhattacharya et al., 2017), we adopt the first-order Wasserstein distance to measure how the intervened reward probability distribution \(p^{\mathcal{M}_{t},do(s_{t}:=s^{T})}(r_{t})\) differs from the original distribution \(p^{\mathcal{M}_{t}}(r_{t})\):

\[W_{1}\left(p^{\mathcal{M}_{t},do(s_{t}:=s^{T})}\left(r_{t}\right),\,p^{ \mathcal{M}}(r_{t})\right)=\inf_{Y\in\{Y^{T},p^{\mathcal{M}}\}}\int_{\mathcal{ R}\times\mathcal{R}}|r-r^{\prime}|\,dy(r,r^{\prime}), \tag{4}\]

where \(\Gamma\left(p^{T},p^{\mathcal{M}}\right)\) is the set of all joint distributions \(\gamma(r,r^{\prime})\) with marginals \(p^{\mathcal{M}_{t},do(s_{t}:=s^{T})}\left(r_{t}\right)\) and \(p^{\mathcal{M}}(r_{t})\). A small Wasserstein distance indicates that the intervention on the state \(s_{t}\) has a negligible effect on the reward distribution, suggesting that the components altered by the intervention are causally irrelevant to the reward. Conversely, a large Wasserstein distance implies that the intervention significantly changes the reward distribution, highlighting the causal relevance of the components modified in the state.

By evaluating the Wasserstein distance between the original and intervened reward distributions, we can quantify the causal effect of the state components on the reward. This measurement not only guides the causal feature selection policy in identifying and retaining the causally relevant components in the state but also serves as a crucial guide for the agent's learning process. To operationalize this measurement within the agent's learning, we introduce an effective reward function defined as:

\[r_{t}=\exp\left(-\lambda\,W_{1}\left(p^{\mathcal{M}_{t},do(s_{t}:=s^{T})}(r_{ t}),\,p^{\mathcal{M}}(r_{t})\right)\right), \tag{5}\]

where \(\lambda\in(0,1]\) is a scaling parameter that controls the sensitivity of the reward to changes in the Wasserstein distance.

By maximizing this reward, the agent is incentivized to select actions that minimize the Wasserstein distance between the intervened and original reward distributions. This encourages the agent to choose actions that retain the causally relevant components of the state, effectively filtering out causally irrelevant features. Consequently, the agent adjusts its policy to focus on the essential causal elements.

### Policy-Guided State Representation

Having identified the CRCs of the state through our causal feature selection policy, we proceed to learn a state representation that effectively captures these essential components. The objective is to encode the current state \(s_{t}\) and its intervened counterpart \(s_{t}^{T}\) into a latent space where only the CRCs are preserved, and the CIRCs are minimized or disregarded. To achieve this, we employ an encoder trained using mean squared error (MSE) loss, which focuses on aligning the representations of \(s_{t}\) and \(s_{t}^{T}\) by minimizing the differences in their latent representations.

By using the causal feature selection policy to generate modified states \(s_{t}^{T}\), which share the same CRCs but differ in CIRCs compared to the original state \(s_{t}\), we provide the encoder with pairs of states that should be mapped to similar latent representations. The MSE loss between the latent representations of \(s_{t}\) and \(s_{t}^{T}\) encourages the encoder to focus on the CRCs and ignore the CIRCs. Moreover, generating modified states \(s_{t}^{T}\) through interventions allows us to augment the dataset, addressing the issue of missing transitions commonly encountered in offline recommender systems.

Practically, we design an encoder network \(\phi\) that processes the input states and outputs their latent representations:

\[z_{t}=\phi(s_{t}),\quad z_{t}^{T}=\phi(s_{t}^{T}). \tag{6}\]

We train the encoder by minimizing the mean squared error (MSE) loss between the latent representations of \(s_{t}\) and \(s_{t}^{T}\):

\[J=\|\phi(s_{t})-\phi(s_{t}^{T})\|_{2}^{2}. \tag{7}\]

This loss function encourages the encoder to focus on the CRCs by reducing the differences in the latent representations of \(s_{t}\) and \(s_{t}^{T}\), which differ only in their CIRCs.

**Proposition 2** (Optimal Policy Based on Latent State Representation).: Let \(s_{t}\in\mathcal{S}\) be the full state at time \(t\), and let \(G=\sum_{k=0}^{\infty}\gamma^{k}r_{t+k}\) be the expected discounted return. Let \(\phi:\mathcal{S}\rightarrow\mathcal{Z}\) be an encoder that maps \(s_{t}\) to a latent state representation \(z_{t}=\phi(s_{t})\in\mathcal{Z}\), capturing the causally relevant components. Suppose for \(z_{t}\), we have:

* \(r_{t}\perp s_{t}\mid z_{t},a_{t}\).
* For all \(s_{t-1},s_{t-1}^{o}\in\mathcal{S}\) with \(\phi(s_{t-1})=\phi(s_{t-1}^{o})\), \(p(\phi(s_{t})\mid s_{t-1}^{o})=p(\phi(s_{t})\mid s_{t-1}^{o})\).

Then the optimal policy \(\pi_{\text{opt}}\) depends only on the latent state representation \(z_{t}\), and not on the full state \(s_{t}\). That is, there exists

\[\pi_{\text{opt}}\in\arg\max_{\pi}\mathbb{E}[G],\]such that

\[\pi_{\text{opt}}(a_{t}\mid s_{t-1})=\pi_{\text{opt}}(a_{t}\mid s_{t-1}^{o})\quad \forall s_{t-1},s_{t-1}^{o}:\phi(s_{t-1})=\phi(s_{t-1}^{o}).\]

The proof of Proposition 2 is given in Appendix C. This proposition shows that using the encoder \(\phi(s_{t})\) as a means of simplifying the state is theoretically justified. The encoder learns to isolate the CRCs from the full state, ensuring that the resulting latent representation \(z_{t}\) contains all information needed for decision-making. This supports the approach of training an encoder to map states into a latent space that focuses on the essential causal features.

```
Input: Initial parameters \(\theta_{t^{\prime}}\), \(\theta_{\theta^{c}}\); replay buffer \(D_{\text{c}}\); reward buffers \(R\), \(\hat{R}\)forepisode = 1 to \(E\)dofor\(t=1\)to\(T\)do  Expert observes state \(s_{t}\), executes action \(a_{t}\), and stores reward \(r_{t}\) in \(R\);  Causal agent intervenes with action \(a_{t}^{T}\) and obtains modified state \(s_{t}^{T}\);  Expert observes \(s_{t}^{T}\), executes action \(a_{t}\), and stores reward \(\hat{r}_{t}\) in \(\hat{R}\).  Calculate reward \(r\) based on the reward function ; // See Eq. (5)  Store transition \((s_{t},a_{t}^{T},s_{t}^{T},r)\) in replay buffer \(D_{\text{c}}\);  Sample minibatch from \(D_{\text{c}}\) and update parameters \(\theta_{t^{\prime}}\), \(\theta_{\phi^{c}}\);

#### 3.3.1. Learning of Causal Feature Selection Policy

The causal feature selection policy is trained by leveraging the reward function in Equation (5). The objective is to design interventions that retain the CRCs while minimizing changes to the reward distribution, thereby preserving the essential components influencing user satisfaction. The algorithm for learning the causal feature selection policy is provided in Algorithm 1.

A one-step illustration of the training process is depicted in Figure 1 (c). The causal feature selection policy is trained with the assistance of a pre-trained expert policy, which uses external knowledge to obtain both the observational and intervened reward distributions. The expert policy can be learned using any RL-based algorithm, and the causal feature selection policy can follow a similar approach.

During training, the expert policy interacts with the environment to collect tuples of the form \((s_{t},a_{t},r_{t},s_{t+1})\), where \(r_{t}\) contributes to the observational reward distribution. Simultaneously, the causal feature selection policy observes the state \(s_{t}\) and intervenes on the action to generate a modified state \(s_{t}^{T}\). This modified state \(s_{t}^{T}\) is treated as an intervention on the original tuple's state. The expert policy then observes \(s_{t}^{T}\) and executes the original action \(a_{t}\), thereby obtaining an intervened reward, which is used to construct the intervened reward distribution.

By maximizing the reward in Equation (5), the causal feature selection policy is incentivized to produce modified states \(s_{t}^{T}\) that yield reward distributions similar to the original. This similarity indicates that the CRCs are effectively retained while the CIRCs are altered, ensuring that the modified states preserve the key causal components.

#### 3.3.2. Integrated Learning Process

In the offline RL setting, we integrate the causal feature selection policy with the training of both the state representation encoder and the recommendation policy, as depicted in Figure 1 (d). Given a current state \(s_{t}\) from the offline dataset, the causal feature selection policy generates a modified state \(s_{t}^{T}\) that retains only the CRCs. The state pair \((s_{t},s_{t}^{T})\) is then used to train the encoder network \(\phi\), which processes the input states and outputs their latent representations.

The encoder is trained by minimizing the loss defined in Equation (6), which encourages it to focus on the CRCs by reducing the differences in the latent representations of the state pairs, which differ only in their CIRCs. Consequently, the encoder learns to map states into a latent space where only the causally relevant features are preserved, effectively filtering out irrelevant variations.

The recommendation policy \(\pi_{\text{Re}}\) is subsequently trained using the latent representations \(z_{t}\) as inputs. Because the encoder prioritizes the CRCs, the recommendation policy is equipped to make decisions based on the most pertinent information influencing user satisfaction. The full algorithm for the integrated learning process is presented in Algorithm 2.
``` Input: Offline dataset \(\mathcal{D}\); causal policy \(\pi_{\text{C}}\); encoder \(\phi\) with parameters initial \(\theta\); recommendation policy \(\pi_{\text{Re}}\) with initial parameters \(\phi\); learning rate \(\alpha\)foreachtraining epoch \(\mathcal{B}\) from \(\mathcal{D}\)do // Generate Modified State Using Causal Feature Selection foreach\((s_{t},a_{t},r_{t},s_{t+1})\in\mathcal{B}\)do  Generate modified state \(s_{t}^{T}=\pi_{\text{C}}(s_{t})\); // Train Encoder Using MSE Loss Encode states: \(z_{t}=\phi(s_{t})\), \(z_{t}^{T}=\phi(s_{t}^{T})\);  Compute MSE loss: \(\mathcal{L}_{\text{encoder}}=\|z_{t}-z_{t}^{T}\|_{2}^{2}\);  Update encoder parameters: \(\theta=\theta-\alpha\nabla_{\theta}\mathcal{L}_{\text{encoder}}\); // Train Recommendation Policy Using Latent Representations Update policy parameters \(\phi\) with offline RL algorithm; ```

**Algorithm 2**Integrated Learning Process

## 4. Experiments

In this section, we begin by performing experiments on an online simulator and recommendation datasets to highlight the remarkable performance of our methods. We then conduct an ablation study to demonstrate the effectiveness of the causal-indispensable state representation.

### Experimental Setup

We introduce the experimental settings with regard to environments and state-of-the-art RL methods.

#### 4.1.1. Recommendation Environments

For offline evaluation, we use the following benchmark datasets:

* **MovieLens-1M1**: These datasets, derived from the Movielens website, feature user ratings of movies. The ratings are on a 5-star scale, with each user providing at least 20 ratings. Movies and users are characterized by 23 and 5 features, respectively.

Footnote 1: [https://grouplens.org/datasets/movielens/](https://grouplens.org/datasets/movielens/)
* **Coat**(Zhou et al., 2018); is a widely used dataset that is proposed for product recommendation.
* **YahooR3**: a music recommendation dataset that proposed by (Zhou et al., 2018).
* **KuaiRec**: a video recommendation dataset that proposed by (Li et al., 2018) which is fully-observable.
* **KuaiRand**: a video recommendation dataset similar to KuaiRec but with a randomly exposed mechanism (Li et al., 2018).

When converting them into the RL environments, we use the GRU as the state encoder for those offline datasets. In addition to those offline datasets, we also conduct experiments on an online simulation platform - VirtualTB (Zhou et al., 2018).

#### 4.1.2. Baseline

Since limited work focuses on causal state representation learning for offline RLRS, we selected the traditional RL algorithm as the baseline. In concurrent work, CIDS (Vaswani et al., 2017) proposes using conditional mutual information to isolate crucial state variables. The key difference between our work and CIDs is that CIDs is tailored for online RLRS, focusing primarily on the causal relationship between action and state. In contrast, our work addresses offline RLRS, incorporating the reward into the framework to train a policy that guides the learning of state representations. In our experiments, we employ the following algorithms as the baseline:

* **Deep Deterministic Policy Gradient (DDPG) (Krishna et al., 2017)**: An off-policy method suitable for environments with continuous action spaces, employing a target policy network for action computation.
* **Soft Actor-Critic (SAC) (Goodfellow et al., 2014)**: An off-policy maximum entropy Deep RL approach, optimizing a stochastic policy with clipped double-Q method and entropy regularization.
* **Twin Delayed DDPG (TD3) (Dwork et al., 2016)**: An enhancement over DDPG, incorporating dual Q-functions, less frequent policy updates, and noise addition to target actions.

To evaluate the performance of the proposed PGCR, we have plugged the PGCR into those mentioned baselines to evaluate the performance.

#### 4.1.3. Evaluation Measures

Following the previous work (Zhou et al., 2018), we will use the cumulative reward, average reward and interaction length as the main evaluation metric for those mentioned offline datasets. For VirtualTB, we use the embedded CTR as the main evaluation metric.

### Implementation Details

In our experiments, we first need to train the causal agent to conduct the intervention and thus generate the intervened state. The offline demonstration is required to train the causal agent. We use a DDPG algorithm to conduct the process to obtain the offline demonstrations for various datasets. The algorithm is trained for 100,000 timesteps, and we save the policy with the best performance during the evaluation stage. The saved policy will be used to generate the offline demonstrations. For the training of our proposed method, we set the learning rate to \(10^{-4}\) for the actor-network and \(10^{-3}\) for the critic network. The discount factor \(\gamma\) is set to 0.95, and we use a soft target update rate \(\tau\) of 0.001. The hidden size of the network is set to 128, and the replay buffer size is set to \(10^{6}\).

For those baselines, we are using the standard hyper-parameters settings from the Tianshou2.

Footnote 2: [https://github.com/thu-ml/transform](https://github.com/thu-ml/transform)

### Overall Results

The results in Table 1 show that PGCR, a causal state representation learning method, significantly enhances state representation in reinforcement learning algorithms. Across different datasets, the PGCR-enhanced versions of standard algorithms (DDPG, SAC, TD3) demonstrate consistent improvements in cumulative and average rewards. This suggests that PGCR effectively strengthens the algorithms' ability to learn better state representations, leading to more informed decision-making and improved policy performance.

Moreover, the enhanced state representation provided by PGCR does not adversely affect the interaction length, which remains stable or slightly increases, indicating efficient learning processes. Additionally, the relatively low variance in the results for PGCR-enhanced methods further emphasizes their stability and reliability across different environments. These findings highlight the effectiveness of PGCR in boosting the overall learning and performance of reinforcement learning models by focusing on improved causal state representations.

### Ablation Study

In this section, we aim to investigate the impact of the proposed causal agent on the final performance. To do this, we replaced the

Figure 2. The 1-step CTR performance in the VirtualTaobao simulation is presented as the mean with error bars.

causal agent with a randomly sampled state. We denote the model without the causal agent as "-C."

Table 2 presents a comparison between the performance of PGCR, the proposed causal state representation learning method, and its variant, PGCR-C, which excludes the causal agent. Across all datasets and reinforcement learning algorithms (DDPG, SAC, TD3), PGCR consistently outperforms PGCR-C in terms of cumulative and average rewards. This highlights the importance and effectiveness of incorporating the causal agent within the PGCR framework, suggesting that the causal state representation significantly enhances the learning process, leading to better policy decisions and improved overall performance.

Regarding interaction length, the differences between PGCR and PGCR-C are generally minor, indicating that the causal agent does not significantly change the duration of interactions but rather improves the quality of decisions during those interactions. The consistent improvements in both cumulative and average rewards across various settings demonstrate that the causal aspect of PGCR is crucial for achieving optimal performance in reinforcement learning tasks. These results underscore the value of the causal state representation in capturing the underlying structure of the environment, enhancing the algorithm's ability to learn and adapt effectively.

### Hyper-parameter Study

In this section, we investigate how the reward balance parameter \(\lambda\) in Equation (5) influences the final performance. To account for computational costs, this study is conducted using an online simulation platform, with the results presented in Figure 4. We observe that all three models--PGCR-DDPG, PGCR-SAC, and PGCR-TD3--are highly sensitive to the value of \(\lambda\). Each model achieves peak performance in terms of CTR around a \(\lambda\) range of 0.1 to 0.2, suggesting that this range is optimal for maximizing the CTR across the models. However, as \(\lambda\) increases beyond 0.2, there is a noticeable decline in performance for all models, with PGCR-DDPG experiencing the most significant drop.

\begin{table}
\begin{tabular}{c|c c c|c c c}  & & MovieLens-1M & & & Coat & & 758 \\  & Cumulative Reward & Average Reward & Interaction Length & Cumulative Reward & Average Reward & Interaction Length & 768 \\ \hline DDPG & \(9.3706\pm 4.49\) & \(3.0329\pm 1.44\) & \(3.11\pm 0.02\) & \(16.3348\pm 7.23\) & \(2.3277\pm 1.03\) & \(7.02\pm 0.03\) & 768 \\ PGCR-DDPG & \(\mathbf{13.0722\pm 3.55}\) & \(\mathbf{4.0587\pm 1.10}\) & \(\mathbf{3.22\pm 0.03}\) & \(\mathbf{19.4281\pm 4.01}\) & \(2.7675\pm 0.57\) & \(7.02\pm 0.05\) & 768 \\ SAC & \(10.2424\pm 3.66\) & \(2.8852\pm 1.03\) & \(3.55\pm 0.03\) & \(17.5432\pm 7.22\) & \(2.4231\pm 1.00\) & \(7.24\pm 0.02\) & 762 \\ PGCR-SAC & \(\mathbf{13.4522\pm 3.77}\) & \(4.4544\pm 1.25\) & \(\mathbf{3.02\pm 0.05}\) & \(20.4272\pm 4.70\) & \(2.7164\pm 0.63\) & \(7.52\pm 0.10\) & 763 \\ TD3 & \(10.1620\pm 4.90\) & \(2.9410\pm 1.42\) & \(3.45\pm 0.02\) & \(16.3232\pm 7.02\) & \(2.3542\pm 1.01\) & \(6.93\pm 0.03\) & 764 \\ PGCR-TD3 & \(\mathbf{14.1281\pm 5.21}\) & \(3.4375\pm 1.27\) & \(\mathbf{4.11\pm 0.02}\) & \(\mathbf{19.1192\pm 3.81}\) & \(\mathbf{2.5323\pm 0.50}\) & \(7.55\pm 0.11\) & 765 \\ \hline \hline  & \multicolumn{4}{c}{KuaiRec} & \multicolumn{4}{c}{KuaiRand} & \multicolumn{1}{c}{764} \\  & Cumulative Reward & Average Reward & Interaction Length & Cumulative Reward & Average Reward & Interaction Length & 767 \\ \hline DDPG & \(9.2155\pm 4.05\) & \(1.0192\pm 0.45\) & \(9.04\pm 0.04\) & \(1.4232\pm 0.51\) & \(0.3287\pm 0.12\) & \(4.33\pm 0.03\) & 768 \\ PGCR-DDPG & \(\mathbf{14.2254\pm 4.87}\) & \(1.5948\pm 0.55\) & \(8.92\pm 0.04\) & \(2.0334\pm 0.65\) & \(0.3657\pm 0.10\) & \(5.56\pm 0.03\) & 769 \\ SAC & \(10.5235\pm 3.92\) & \(1.1693\pm 0.44\) & \(9.00\pm 0.10\) & \(1.8272\pm 0.55\) & \(0.3500\pm 0.11\) & \(5.22\pm 0.04\) & 770 \\ PGCR-SAC & \(\mathbf{15.3726\pm 4.02}\) & \(\mathbf{1.8588\pm 0.49}\) & \(\mathbf{8.27\pm 0.04}\) & \(\mathbf{2.4421\pm 0.23}\) & \(\mathbf{0.4531\pm 0.05}\) & \(5.39\pm 0.04\) & 771 \\ TD3 & \(\mathbf{7.8179\pm 3.25}\) & \(0.8610\pm 0.36\) & \(9.09\pm 0.04\) & \(1.5083\pm 0.40\) & \(0.3010\pm 0.08\) & \(5.01\pm 0.05\) & 772 \\ PGCR-TD3 & \(\mathbf{14.0021\pm 4.90}\) & \(\mathbf{1.5203\pm 0.53}\) & \(\mathbf{9.21\pm 0.03}\) & \(\mathbf{2.0001\pm 0.34}\) & \(\mathbf{0.3992\pm 0.07}\) & \(\mathbf{5.01\pm 0.02}\) & 773 \\ \hline \hline  & \multicolumn{4}{c}{KuaiRec} & \multicolumn{4}{c}{KuaiRand} & \multicolumn{1}{c}{764} \\  & Cumulative Reward & Average Reward & Interaction Length & Cumulative Reward & Average Reward & Interaction Length & 767 \\ \hline DDPG & \(9.2155\pm 4.05\) & \(1.0192\pm 0.45\) & \(9.04\pm 0.04\) & \(1.4232\pm 0.51\) & \(0.3287\pm 0.12\) & \(4.33\pm 0.03\) & 768 \\ PGCR-DDPG & \(\mathbf{14.2254\pm 4.87}\) & \(1.5948\pm 0.55\) & \(8.92\pm 0.04\) & \(2.0334\pm 0.65\) & \(0.3657\pm 0.10\) & \(5.56\pm 0.03\) & 769 \\ SAC & \(10.5235\pm 3.92\) & \(1.1693\pm 0.44\) & \(9.00\pm 0.10\) & \(1.8272\pm 0.55\

## 5. Related Work

**RL-based Recommender Systems** model the recommendation process as a Markov Decision Process (MDP), leveraging deep learning to estimate value functions and handle the high dimensionality of MDPs (Hoffman, 2015). Chen et al. (2016) proposed InvRec, which uses inverse reinforcement learning to infer rewards directly from user behavior, enhancing policy learning accuracy. Recent efforts have focused on offline RLRS. Wang et al. (2018) introduced CDT4Rec, which incorporates a causal mechanism for reward estimation and uses transformer architectures to improve offline RL-based recommendations. Additionally, Chen et al. (2018) enhanced this line of research by developing a max-entropy exploration strategy to improve the decision transformer's ability to "stitch" together diverse sequences of user actions, addressing a key limitation in offline RLRS. Gao et al. (2018) developed a counterfactual exploration strategy designed to mitigate the Matthew effect, which refers to the disparity in learning from uneven distributions of user data.

**Causal Recommendation**. The recommendation domain has recently seen significant advancements through the integration of causal inference techniques, which help address biases in training data. For example, Zhang et al. (2019) tackled the prevalent issue of popularity bias by introducing a causal inference paradigm that adjusts recommendation scores through targeted interventions. Similarly, Li et al. (2019) proposed a unified multi-task learning approach to eliminate hidden confounding effects, incorporating a small number of unbiased ratings from a causal perspective. Counterfactual reasoning has also gained traction in recommender systems. Chen et al. (2018) developed a causal augmentation technique to enhance exploration in RL-based recommender systems (RLRS) by focusing on causally relevant aspects of user interactions. Wang et al. (2018) introduced a method to generate counterfactual user interactions based on a causal view of MDP for data augmentation. In a related vein, Li et al. (2019) explored personalized incentive policy learning through an individualized counterfactual perspective. Further studies have focused on the use of causal interventions. Wang et al. (2018) proposed CausalInt, a method inspired by causal interventions to address challenges in multi-scenario recommendation. Additionally, He et al. (2018) tackled the confounding feature issue in recommendation by leveraging causal intervention techniques. These efforts collectively demonstrate the growing importance of causal inference and intervention in improving recommendation performance and addressing biases.

## 6. Conclusion

In this work, we introduced Policy-Guided Causal Representation (PGCR), a framework designed to enhance state representation learning in offline RL-based recommender systems. By using a causal feature selection policy to isolate the causally relevant components (CRCs) and training an encoder to focus on these components, PGCR effectively improves recommendation performance while mitigating noise and irrelevant features in the state space. Extensive experiments demonstrate the benefits of our approach, confirming its effectiveness in offline RL settings.

For future work, we plan to explore the extension of PGCR to more complex, multi-agent environments where user preferences may dynamically change over time.

\begin{table}
\begin{tabular}{c|c c c|c c c}  & \multicolumn{3}{c|}{MovieLens-1M} & \multicolumn{3}{c}{Coat} & \multicolumn{1}{c}{\multirow{2}{*}{\begin{tabular}{c} \(\mathcal{F}_{373}\) \\ Interaction Length \\ \end{tabular} }} \\  & Cumulative Reward & Average Reward & Interaction Length & Cumulative Reward & Average Reward & Interaction Length \\ \hline PGCR-DDPG & 13.0722 ± 3.55 & 4.0587 ± 1.10 & 3.22 ± 0.03 & 19.4281 ± 4.01 & 2.7675 ± 0.57 & 7.02 ± 0.05 & 873 \\ PGCR-C-DDPG & 9.9271 ± 4.02 & 3.1022 ± 1.26 & 3.20 ± 0.03 & 17.0237 ± 6.55 & 2.3611 ± 0.91 & 7.21 ± 0.04 & 873 \\ \hline PGGR-SAC & 13.4522 ± 3.77 & 4.4544 ± 1.25 & 3.02 ± 0.05 & 20.4272 ± 4.70 & 2.7164 ± 0.63 & 7.52 ± 0.10 & 873 \\ PGCR-C-SAC & 11.0238 ± 3.44 & 2.7491 ± 0.86 & 4.01 ± 0.05 & 18.1253 ± 7.02 & 2.5209 ± 0.98 & 7.19 ± 0.03 & 873 \\ \hline PGGR-TD3 & 14.1281 ± 5.21 & 3.4375 ± 1.27 & 4.11 ± 0.02 & 19.1192 ± 3.81 & 2.5323 ± 0.50 & 7.55 ± 0.11 & 879 \\ PGCR-C-TD3 & 11.0261 ± 4.45 & 3.4349 ± 1.39 & 3.21 ± 0.03 & 17.0221 ± 6.42 & 2.3907 ± 0.91 & 7.12 ± 0.04 & 88 \\ \hline \multirow{4}{*}{\begin{tabular}{c} \(\mathcal{F}_{373}\) \\ Interaction Length \\ \end{tabular} } & \multicolumn{3}{c|}{KuaILan} & \multicolumn{1}{c}{\multirow{2}{*}{
\begin{tabular}{c} \(\mathcal{F}_{373}\) \\ Interaction Length \\ \end{tabular} }} \\  & Cumulative Reward & Average Reward & Interaction Length & Cumulative Reward & Average Reward & Interaction Length \\ \hline PGGR-DDPG & 14.2254 ± 4.87 & 1.5948 ± 0.55 & 8.92 ± 0.04 & 2.0334 ± 0.65 & 0.3657 ± 0.10 & 5.56 ± 0.03 & 833 \\ PGCR-C-DDPG & 10.4222 ± 4.19 & 1.1087 ± 0.45 & 9.40 ± 0.07 & 1.6410 ± 0.62 & 0.3447 ± 0.13 & 4.76 ± 0.04 & 884 \\ \hline PGGR-SAC & 15.3726 ± 4.02 & 1.8588 ± 0.49 & 8.27 ± 0.04 & 2.4421 ± 0.23 & 0.4531 ± 0.05 & 5.39 ± 0.04 & 835 \\ PGGR-C-SAC & 11.2890 ± 4.11 & 1.2858 ± 0.47 & 8.78 ± 0.11 & 2.0316 ± 0.41 & 0.3999 ± 0.08 & 5.08 ± 0.05 & 886 \\ \hline PGGR-TD3 & 14.0021 ± 4.90 & 1.5203 ± 0.53 & 9.21 ± 0.03 & 2.0001 ± 0.34 & 0.3992 ± 0.07 & 5.01 ± 0.02 & 887 \\ PGGR-C-TD3 & 8.1247 ± 3.01 & 0.8793 ± 0.33 & 9.24 ± 0.08 & 1.6218 ± 0.36 & 0.2998 ± 0.07 & 5.41 ± 0.03 & 888 \\ \hline \end{tabular}
\end{table}
Table 2. Ablation Study

Figure 4. Hyper Parameter Study in VirtualTB
## References

* M. Afsar, T. Crump, and B. Far (2022)Reinforcement learning based recommender system: a survey. Comput. Surveys 55 (7), pp. 1-38. Cited by: SS1.
* X. Chen, S. Wang, J. McAuley, D. Jannach, and L. Yao (2024)On the opportunities and challenges of offline reinforcement learning for recommender systems. ACM Transactions on Information Systems24 (6), pp. 1-26. Cited by: SS1.
* X. Chen, S. Wang, L. Qi, Y. Li, and L. Yao (2023)Intrinsically motivated reinforcement learning based recommendation with counterfactual data augmentation. World Wide Web55 (2), pp. 3333-3347. Cited by: SS1.
* X. Chen, S. Wang, and L. Yao (2024)Maximum-entropy regularized decision transformation with reward: establishing for dynamic recommendation. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, SIGCHI '24, New York, NY, USA, pp. 376-384. External Links: ISBN 978145036660, Link, Document Cited by: SS1.
* X. Chen, L. Yao, B. Chen, and L. Yao (2023)Deep reinforcement learning in recommender systems: a survey and new perspectives. Knowledge-Based Systems264, pp. 110335. External Links: ISSN 0018-9219, Link, Document Cited by: SS1.
* X. Chen, L. Yao, A. Yuan, X. Xue, and L. Zhu (2021)Generative inverse deep reinforcement learning for online recommendation. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pp. 201-210. Cited by: SS1.
* N. Ferns, P. Pannagaden, and D. Precup (2011)Bisimulation metrics for continuous markov decision processes. SIAM J. Comput.40 (6), pp. 1662-1714. Cited by: SS1.
* S. Fujimoto, H. Hebe, and D. Meger (2018)Addressing function approximation error in actor-critic methods. In International conference on machine learning, pp. 1587-1596. Cited by: SS1.
* C. Gao, S. Huang, J. Chen, Y. Zhang, R. Li, P. Jiang, S. Wang, Z. Zhang, and X. He (2023)Alleviating matthew effect of offline reinforcement learning in interactive recommendation. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 238-248. Cited by: SS1.
* C. Gao, S. Li, Y. Zhang, J. Chen, B. Li, P. Jiang, X. He, J. Yao, and T. Chua (2022)KauLife: a fully-observed dataset and insights for evaluating recommender systems. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management, pp. 540-550. Cited by: SS1.
* C. Gao, S. Li, Y. Zhang, J. Chen, B. Li, W. Lei, P. Jiang, and X. He (2022)KauBand: an unbiased sequential recommendation dataset with randomly exposed videos. In Proceedings of the 31st ACM International Conference on Information and Knowledge Management, pp. 953-957. Cited by: SS1.
* T. Marzuolo, A. Zhou, P. Abbeel, and S. Levine (2018)Soft actor-critic: off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, pp. 1861-1870. Cited by: SS1.
* X. He, Y. Zhang, F. Feng, C. Song, L. Yi, G. Ling, and Y. Zhang (2023)Addressing confounding feature noise for causal recommendation. ACM Transactions on Information Systems41 (3), pp. 1-23. Cited by: SS1.
* B. Huang, C. Liu, L. Yao, S. Li, Y. Zhang, Z. Liu, and P. Cui (2022)When should be given incentives? counterfactual optimal treatment regimes learning for recommendation. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 1235-1247. Cited by: SS1.
* T. J. Li, Y. Zhang, D. Yi, Y. Li, and L. Yao (2023)Intrinsically motivated reinforcement learning based recommendation with counterfactual data augmentation. World Wide Web55 (2), pp. 3333-3347. Cited by: SS1.
* T. Li, Y. Zhang, D. Yi, Y. Li, and L. Yao (2024)Multi-modal reinforcement learning for control: an overview. Neural Networks108 (1), pp. 379-392. Cited by: SS1.
* H. Li, K. Wu, C. Zheng, Y. Xiao, H. Wang, Z. Feng, Y. Fu, X. He, and P. Wu (2024)Removing hidden confounding in recommendation: a unified multi-task learning approach. Advances in Neural Information Processing Systems36. Cited by: SS1.
* H. Li, C. Zheng, P. Wu, K. Kuang, Y. Liu, and P. Cui (2023)Who should be given incentives? counterfactual optimal treatment regimes learning for recommendation. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 1235-1247. Cited by: SS1.
* T. J. Li, Y. Zhang, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tsatsi, D. Silver, and D. Wierstra (2015)Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971. Cited by: SS1.
* T. Ma, L. Yao, and L. Yao (2020)Learning and ambiguity in interactive recommender systems. In Proceedings of the ninth international conference on Electronic commerce, pp. 73-84. Cited by: SS1.
* J. Pearl (2009)Causality. Cambridge university press. Cited by: SS1.
* J. Peters, D. J. Janning, and B. Schloep (2017)Elements of causal inference: foundations and learning algorithms. The MIT Press. Cited by: SS1.
* T. Schmiedl, A. Swaminathan, A. Singh, N. Chandak, and T. Joachims (2016)Recommendations as treatments: debiasing learning and evaluation. In international conference on machine learning, pp. 1670-1679. Cited by: SS1.
* J. Singh, S. Yang, H. Qin, D. Shi-Yong Chen, and A. Zeng (2019)Virtual-tabacq: virtualizing real-world online retail environment for reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 3, pp. 4902-4909. Cited by: SS1.
* S. Wang, X. Chen, D. Jannach, and L. Yao (2023)Causal decision transformer for recommender systems via offline reinforcement learning. In Proceedings of the 46th International ACM SIGIR conference on Research and Development in Information Retrieval, pp. 1599-1608. Cited by: SS1.
* S. Wang, X. Chen, J. McAuley, S. Cripps, and L. Yao (2023)Who-and-agnostic counterfactual policy synthesis for deep reinforcement learning-based recommendation. IEEE Transactions on Neural Networks and Learning Systems. Cited by: SS1.
* Y. Wang, H. Guo, B. Chen, W. Li, Z. Liu, Q. Zhang, T. He, L. Yao, and L. Yao (2022)Causality-inspired interpretable intention for multi-sensor recommendation. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 4009-4099. Cited by: SS1.
* Y. Wang, C. Gao, J. Chen, H. Tang, Y. Sun, Q. Chen, W. Liu, and M. Zavigna (2022)ExaIPL/HCC: an easy-to-use library for reinforcement learning based recommender systems. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 977-987. Cited by: SS1.
* Y. Wang, H. Guo, B. Chen, W. Li, Z. Liu, Q. Zhang, T. He, L. Yao, and L. Yao (2022)Causality-inspired interpretable intention for multi-sensor recommendation. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 4009-4099. Cited by: SS1.
* Y. Wang, C. Gao, J. Chen, H. Tang, Y. Sun, Q. Chen, W. Liu, and M. Zavigna (2022)ExaIPL/HCC: an easy-to-use library for reinforcement learning based recommender systems. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 977-987. Cited by: SS1.
* H. Yang, X. Li, L. Zhang, Y. Liu, B. Sun, R. Islam, B. He, J. C. Lee, and R. Larochelle (2024)Understanding and addressing the pitfalls of biomimulin based representations in offline reinforcement learning. Advances in Neural Information Processing Systems36. Cited by: SS1.
* A. Zhang, R. Thomas McAllister, R. Calandra, Y. Gal, and S. Levine (2021)Learning invariant representations for reinforcement learning without reconstruction. In International Conference on Learning Representations, pp. 1002-1057. Cited by: SS1.
* Y. Zhang, F. Feng, X. He, T. Wei, C. Song, G. Ling, and Y. Zhang (2021)Causal intervention for leveraging popularity bias in recommendation. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 11-20. Cited by: SS1.
* Y. Zhang, F. Feng, Y. He, Y. Wu, K. Kuang, Y. Liu, and P. Cui (2023)Who should be given incentives? counterfactual optimal treatment regimes learning for recommendation. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 1235-1247. Cited by: SS1.
* T. Zhang, F. Feng, Y. He, Y. Liu, and P. Cui (2023)Who should be given incentives? counterfactual optimal treatment regimes learning for recommendation. In Proceedings of the 31st ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 1235-1247. Cited by: SS1.
* T. Zhang, F. Feng, Y. He, Y. Sun, and L. Yao (2023)Causal intervention for leveraging popularity bias in recommendation. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 11-20. Cited by: SS1.

## Appendix A Definitions in Causality

Here, we briefly introduce some fundamental definitions [21, 22] that are used throughout this paper to present and prove our methodology.

**Definition A.1** (d-Separation [22]).: In a Directed Acyclic Graph (DAG) \(\mathcal{G}\), a path between two nodes, denoted as \(i_{n}\) and \(i_{m}\), is considered blocked by a set \(\mathbf{S}\) if:

1. Neither \(i_{n}\) nor \(i_{m}\) are included in \(\mathbf{S}\), and
2. There exists a node \(i_{k}\) on the path such that either: 1. \(i_{k}\in\mathbf{S}\) and the connections around \(i_{k}\) follow one of the forms: \(i_{k-1}\to i_{k}\to i_{k+1}\), \(i_{k-1}\gets i_{k}\gets i_{k+1}\), or \(i_{k-1}\gets i_{k}\to i_{k+1}\), or 2. \(i_{k}\) is a collider (i.e., \(i_{k-1}\to i_{k}\gets i_{k+1}\)), none of its descendants are included in \(\mathbf{S}\), and \(i_{k}\) itself is not part of \(\mathbf{S}\).

**Definition A.2** (Valid adjustment set [21]).: Consider an SCM \(\mathcal{M}\) over nodes \(\mathbf{V}\) and let \(Y\not\in\mathbf{P}\mathbf{A}_{X}\) (otherwise we have \(p^{\mathcal{M},\text{do}(X\!=\!x)}(y)=p^{\mathcal{M}}(y)\)). We call a set \(\mathbf{Z}\subseteq\mathbf{Y}\setminus\{X,Y\}\) a valid adjustment set for the ordered pair \((X,Y)\) if

\[p^{\mathcal{M},\text{do}(X\!=\!x)}(y)=\sum_{\mathbf{z}}p^{\mathcal{M}}(y\mid x,\mathbf{z})p^{\mathcal{M}}(\mathbf{z}).\]

Here, the sum (which could also be an integral) is over the range of \(\mathbf{Z}\), that is, over all values \(\mathbf{z}\) that \(\mathbf{Z}\) can take.

**Definition A.3** (Back-Door Criterion [21]).: A set of variables \(\mathbf{Z}\) satisfies the back-door criterion relative to an ordered pair of variables \((X_{i},X_{j})\) in a Directed Acyclic Graph (DAG) \(\mathcal{G}\) if:

1. No node in \(\mathbf{Z}\) is a descendant of \(X_{i}\); and
2. \(\mathbf{Z}\) blocks every path between \(X_{i}\) and \(X_{j}\) that contains an arrow into \(X_{i}\).

Similarly, if \(\mathbf{X}\) and \(\mathbf{Y}\) are two disjoint subsets of nodes in \(\mathcal{G}\), then \(\mathbf{Z}\) is said to satisfy the back-door criterion relative to \((\mathbf{X},\mathbf{Y})\) if it satisfies the criterion relative to any pair \((X_{i},X_{j})\) such that \(X_{i}\in\mathbf{X}\) and \(X_{j}\in\mathbf{Y}\).

The name "back-door" refers to condition (ii), which requires that only paths with arrows pointing at \(X_{i}\) be blocked; these paths can be viewed as entering \(X_{i}\) through the "back door."

**Definition A.4** (Back-Door Adjustment [21]).: If a set of variables \(\mathbf{Z}\) satisfies the back-door criterion relative to \((\mathbf{X},\mathbf{Y})\), then the causal effect of \(\mathbf{X}\) on \(\mathbf{Y}\) is identifiable and is given by the formula:

\[p^{\mathcal{M},\text{do}(X\!=\!x)}(y)=\sum_{\mathbf{z}}p(y\mid x,\mathbf{z})p( \mathbf{z}).\]

**Definition A.5** (Do-Calculus [21]).: Again, consider an SCM over variables \(\mathbf{V}\). Let us call an intervention distribution \(p^{\mathcal{M},\text{do}(X\!=\!x)}(y)\)_identifiable_ if it can be computed from the observational distribution and the graph structure. Given a graph \(\mathcal{G}\) and disjoint subsets \(\mathbf{X},\mathbf{Y},\mathbf{Z},\mathbf{W}\), we have the following:

1. **Insertion/deletion of observations**: \[p^{\mathcal{M},\text{do}(X\!=\!x)}(y\mid\mathbf{z},\mathbf{w})=p^{\mathcal{M}, \text{do}(X\!=\!x)}(y\mid\mathbf{w})\] if \(\mathbf{Y}\) and \(\mathbf{Z}\) are d-separated by \(\mathbf{X},\mathbf{W}\) in a graph where incoming edges into \(\mathbf{X}\) have been removed.
2. **Action/observation exchange**: \[p^{\mathcal{M},\text{do}(X\!=\!x,\mathbf{Z}\!=\!z)}(y\mid\mathbf{w})=p^{ \mathcal{M},\text{do}(X\!=\!x)}(y\mid\mathbf{z},\mathbf{w})\] if \(\mathbf{Y}\) and \(\mathbf{Z}\) are d-separated by \(\mathbf{X},\mathbf{W}\) in a graph where incoming edges into \(\mathbf{X}\) and outgoing edges from \(\mathbf{Z}\) have been removed.
3. **Insertion/deletion of actions**: \[p^{\mathcal{M},\text{do}(X\!=\!x,\mathbf{Z}\!=\!z)}(y\mid\mathbf{w})=p^{ \mathcal{M},\text{do}(X\!=\!x)}(y\mid\mathbf{w})\] if \(\mathbf{Y}\) and \(\mathbf{Z}\) are d-separated by \(\mathbf{X},\mathbf{W}\) in a graph where incoming edges into \(\mathbf{X}\) and \(\mathbf{Z}\) (or \(\mathbf{W}\)) have been removed. Here, \(\mathbf{Z}(\mathbf{W})\) is the subset of nodes in \(\mathbf{Z}\) that are not ancestors of any node in \(\mathbf{W}\) in a graph obtained from \(\mathcal{G}\) after removing all edges into \(\mathbf{X}\).

## Appendix B Proof of Proposition 1

Proof.: We are given a Structural Causal Model (SCM) that follows the relationships in Equation (7):

\[s_{t+1}=f_{P}(s_{t},a_{t},\epsilon_{t+1}),\quad a_{t}=\pi_{t}(s_{t},\eta_{t}), \quad r_{t}=f_{R}(s_{t},a_{t}), \tag{7}\]

where the state transition function \(f_{P}\) determines the next state \(s_{t+1}\) based on the current state \(s_{t}\), action \(a_{t}\), and exogenous noise \(\epsilon_{t+1}\). The policy function \(\pi_{t}\) selects the action \(a_{t}\) given the current state \(s_{t}\) and exogenous noise \(\eta_{t}\). The reward function \(f_{R}\) assigns a reward \(r_{t}\) based on the current state \(s_{t}\) and action \(a_{t}\).

We aim to show that \(s_{t}\) satisfies the back-door criterion relative to the pair \((a_{t},s_{t+1})\), allowing us to identify the causal effect of \(a_{t}\) on \(s_{t+1}\).

**Step 1: Verify the Back-Door Criterion Conditions.** According to Pearl's back-door criterion, for the causal effect of \(a_{t}\) on \(s_{t+1}\) to identifiable, the following conditions must be met:

* No Descendants of \(a_{t}\) in \(s_{t}\): From the given SCM, there are no directed edges from \(a_{t}\) to \(s_{t}\). This means \(s_{t}\) is not a descendant of \(a_{t}\), satisfying the first condition of the back-door criterion.
* Blocking Paths with Arrows into \(a_{t}\): Any back-door path between \(a_{t}\) and \(s_{t+1}\) that contains an arrow into \(a_{t}\) must be blocked by \(s_{t}\). In the given SCM, all paths that contain an arrow into \(a_{t}\) are blocked by \(s_{t}\). Specifically, since \(s_{t}\to a_{t}\), the node \(s_{t}\) acts as a "blocker" for any indirect influence from \(a_{t}\) to \(s_{t+1}\) via other variables.

Thus, \(s_{t}\) satisfies the back-door criterion relative to \((a_{t},s_{t+1})\).

**Step 2: Identifiability of the Causal Effect.** Since \(s_{t}\) satisfies the back-door criterion, the causal effect of \(a_{t}\) on \(s_{t+1}\) is identifiable, meaning we can compute the effect of intervening on \(a_{t}\) on \(s_{t+1}\) using observational data.

**Step 3: Derivation of the Intervention Formula.** Using the back-door adjustment, the probability distribution of the state \(s_{t+1}\) after an intervention \(do(a_{t}=a_{t}^{T})\) can be computed as:

\[p^{\mathcal{M},\text{do}(a_{t}=a_{t}^{T})}(s_{t+1})\] \[=\sum_{k_{t}}\int_{\epsilon_{t+1}}P(s_{t+1}\mid do(a_{t}),s_{t}, \epsilon_{t+1})\,P(\epsilon_{t+1})\,P(s_{t}\mid do(a_{t}))\,d\epsilon_{t+1}.\]

Since \(P(s_{t}\mid do(a_{t}))=P(s_{t})\), we simplify the expression:

[MISSING_PAGE_FAIL:11]