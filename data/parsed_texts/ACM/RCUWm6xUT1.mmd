# Full Stage Learning to Rank: A Unified Framework for Multi-Stage Systems

Anonymous Author(s)

###### Abstract.

The Probability Ranking Principle (PRP) has been considered as the foundational standard in the design of information retrieval (IR) systems. The principle requires an IR module's returned list of results to be ranked with respect to the underlying user interests, so as to maximize the results' utility. Nevertheless, we point out that it is inappropriate to indiscriminately apply PRP through every stage of a contemporary IR system. Such systems contain multiple stages (e.g., retrieval, pre-ranking, ranking, and re-ranking stages, as examined in this paper). The _selection bias_ inherent in the model of each stage significantly influences the results that are ultimately presented to users. To address this issue, we propose an improved ranking principle for multi-stage systems, namely the Generalized Probability Ranking Principle (GPRP), to emphasize both the selection bias in each stage of the system pipeline as well as the underlying interest of users. We realize GPRP via a unified algorithmic framework named Full Stage Learning to Rank. Our core idea is to first estimate the selection bias in the subsequent stages and then learn a ranking model that best compiles with the downstream modules' selection bias so as to deliver its top ranked results to the final ranked list in the system's output. We performed extensive experiment evaluations of our developed Full Stage Learning to Rank solution, using both simulations and online A/B tests in one of the leading short-video recommendation platforms. The algorithm is proved to be effective in both retrieval and ranking stages. Since deployed, the algorithm has brought consistent and significant performance gain to the platform.

Multi-stage systems, retrieval, ranking, learning to rank +
Footnote †: isbn: 978-1-4503-XXXX-XXXXXX

+
Footnote †: isbn: 978-1-4503-XXXX-XXXX

+
Footnote †: isbn: 978-1-4503-XXXX-XXXX

+
Footnote †: isbn: 978-1-4503-XXXX-XXXX

+
Footnote †: isbn: 978-1-4503-XXXX-XXXX

+
Footnote †: isbn: 978-1-4503-XXXX-XXXX

+
Footnote †: isbn: 978-1-4503-XXXX-XXXX

+
Footnote †: isbn: 978-1-4503-XXXX-XXXX

+
Footnote †: isbn: 978-1-4503-XXXX-XXXX

+
Footnote †: isbn: 978-1-4503-XXXX-XXXX

## 1. Introduction

Information retrieval (IR) systems widely influence numerous aspects of our daily lives, from information search to entertainment choices and travel decisions. The significant impact has gathered extensive research interest and gained substantial attention since its inception (Shen et al., 2015). Central to this research area is the precise inference of users' underlying information needs, so as to present the most relevant information to the users, a concept that proved to be optimal by the Probability Ranking Principle (PRP) (Shannon, 1958).

Early year's IR research only concerned single-stage systems, for example using BM25 (Song et al., 2015) or a logistic regression model (Krishnamurthy et al., 2016) to rank results against a given query or user. Propelled by the emergency of new demands and development of technologies, modern IR systems nowadays are equipped with multiple stages, such as retrieval and ranking stages employed in YouTube (Bengio et al., 2015), the recall, first round ranking, and second round ranking stages in Yahoo search (Yao et al., 2016), and the retrieval, pre-ranking, ranking and re-ranking stages in KuaiShou (Yao et al., 2016). No matter it is manually crafted rules, such as the BM25 scoring function, or data-driven rankers, such as learning to rank models (Krishnamurthy et al., 2016), in each component of these multi-stage IR systems, PRP is still the design principle, i.e., each stage is supposed to present the most relevant results for the next stage's further processing.

However, the optimality of PRP strongly depends on the assumption that the top-ranked result by the algorithm will eventually be presented to the end users. This is only true in a single-stage IR system, but no longer holds in multi-stage systems. For example, at the retrieval stage (typically an early stage), even if we know the underlying preferences of users on each result in the whole candidate set and return the most relevant ones to the later stage, there is a possibility that some or even all of them are filtrated eventually because the _selection bias_ of subsequent stages can deviate from user's underlying interests and promote sub-optimal results along the way until reaching the end users 1.

Footnote 1: In a commercial system, there can be tens of different ranking algorithms employed in each stage of the pipeline, making the selection bias inevitable.

The fundamental reason for the _selection bias_ resides in the misalignment between how the ranking models in each stage are learned and how they are used in a multi-stage system. Typically, the training data for each stage is constructed by treating the results from this stage and finally preferred by the end users as positive and the rest from this stage as negative. Sub-sampling can be devised when the stage returns a large number of results (e.g., more than thousands), but the end users can only interact with a few (e.g., less than ten). However, relevant results from the intermediate stage but eventually filtered by later stages cannot be differentiated from those truly irrelevant ones, and are often mistakenly treated as negatives. This phenomenon is analogous to position bias in user feedback (Bengio et al., 2015; Krishnamurthy et al., 2016), where non-clicked results do not suggest irrelevance, as they could also not be examined by a user. In a multi-stage system, the selection bias is introduced by the subsequent components in an IR pipeline, which is more implicit and dynamic. Hence, those well-known solutions for correcting position bias (Bengio et al., 2015; Krishnamurthy et al., 2016; Krishnamurthy et al., 2016; Yao et al., 2016) do not apply to this problem.

Moreover, the operation and management of multi-stage systems in industry practice further exacerbates the selection bias. Due to the system's high complexity, each stage or even a particular algorithm at a single stage is often managed by a different team. When improving an algorithm or choosing which algorithm todeploy, each team can only access and control data at its specific stage. However, these intermediate algorithmic decisions are only informed by online A/B tests on the final results.

In this paper, we propose an improved version of PRP named the Generalized Probability Ranking Principle (GPRP) for multi-stage IR systems. GPRP extends PRP by explicitly modeling the ranking preferences from both the end users and subsequent stages. GPRP degrades to PRP when there only one stage is present or it functions at the final stage of the system. At non-final stages, the expected ranking utility achieved by GPRP is always an upper bound of the expected utility achieved by PRP, which can also be proved to be a max-min approximation of GPRP. However, it is difficult to precisely fulfill GPRP in practice due to computational complexity. Under mild assumptions about a multi-stage IR system, we develop an efficient and effective algorithmic framework named Full Stage Learning to Rank (FS-LTR) to realize GPRP approximately. The core idea of FS-LTR is to define preferential treatments on the exposure data collected in each stage of the system as well as users' feedback for the final ranking and fit an LTR algorithm with the preferential labels to estimate the most effective ranking for each particular stage. FS-LTR can be seamlessly applied to any stage in the entire pipeline. Extensive experiments on both a simulated environment and online A/B testing in one of the world's largest short-video recommendation platforms validate the effectiveness of our framework. To the best of our knowledge, we are the first to study ranking principles for multi-stage IR systems holistically and to design a universally efficient algorithmic framework with theoretical foundations.

## 2. Related Work

**Retrieval Stage:** Retrieval is the first stage of the recommendation system, which needs to recall users' interested items from a large candidate pool. There are various different retrieval algorithms including basic dual-encoder model (Bishop, 2006; Krizhevsky et al., 2012; He et al., 2015), tree-based deep models (Zhu et al., 2017; He et al., 2016), and recent generative retrieval (Zhu et al., 2017; He et al., 2017). In-batch softmax loss and Bayesian Pairwise Loss (BPR) are two frequently used losses in common retrieval algorithms, and training data mainly contain items with positive feedback and random items sampled from the whole candidate set.

**General Ranking Stage:** This part can be more carefully divided to three (or even more) stages, including pre-ranking (He et al., 2016; He et al., 2016; He et al., 2016; He et al., 2016; He et al., 2016), ranking (He et al., 2016; He et al., 2016; He et al., 2016; He et al., 2016), and re-ranking (He et al., 2016; He et al., 2016; He et al., 2016; He et al., 2016). The difference between models in the three stages mainly lies in their model capacity and training method, either in a pointwise, pairwise, or listwise manner.

**Unbiased Learning:** The goal of unbiased learning is to eliminate the exposure bias introduced by training data collected from recommendation systems and learn unbiased ground-truth interests of users. one main idea of unbiased learning is based on the Inverse Propensity Score (IPS) estimator (Zhu et al., 2017), which is used either at the retrieval stage (He et al., 2016) or re-ranking stage (He et al., 2016; He et al., 2016).

## 3. Preliminary of Multi-Stage Systems

Without loss of generality, we consider a multi-stage system with four stages: retrieval (stage 1, from all candidates to \(c_{1}\) items), pre-ranking (stage 2, from \(c_{1}\) items to \(c_{2}\) items), ranking (stage 3, from \(c_{2}\) items to \(c_{3}\) items), and re-ranking (stage 4, from \(c_{3}\) items to \(c_{4}\) items), where \(c_{1},c_{2},c_{3},c_{4}\) are predefined values like 10000/500/50/6 respectively. Note the output of one stage is exactly the input candidate set of the next stage. Finally, \(c_{4}\) items outputted by the re-ranking stage are exposed to a user and receive corresponding user feedback. Given a candidate item set \(A\), a real-valued function \(f(u,v)\) and a positive integer \(c\), the \(\textbf{Topk}(A,f(u,v),c)\) operator returns the top \(c\) items in the item set \(A\) in the descending order of \(f(u,v)\).

Denote \(u\in\mathbb{R}^{d}\) as the user (or query) representation, and \(v\in\mathcal{I}\) as the item representation, where \(\mathcal{I}\) is the whole candidate set. Once an item \(v\) is exposed to a user \(u\), we observe corresponding Bernoulli feedback \(\Upsilon(u,v)\in\{0,1\}\), which is sampled from the underlying interest \(r(u,v)\in[0,1]\), thus \(\mathbf{E}[\Upsilon(u,v)]=r(u,v)\).

Under the above four-stage system, given a user \(u\), denote the corresponding candidate set and output item set of stage \(is\)\(I^{i-1}(u)\) and \(I^{i}(u)\) respectively, where \(i\in[4]:=\{1,2,3,4\}\) and \(I^{0}\equiv\mathcal{I}\), \(|I^{i}(u)|=c_{i}\). For the clarity of our notations, we will use \(I^{i}\) instead of \(I^{i}(u)\) when there is no ambiguity. Given candidate set \(I^{i-1}\) at stage \(i\), suppose item \(v\in I^{i-1}\) is selected into the output item set \(I^{i}\) with probability \(p^{i}(u,|I^{i-1})\) and denote \(O^{i}(u,|I^{i-1})\) as the corresponding observed Bernoulli random variable. Thus \(\mathbf{E}[O^{i}(u,v|I^{i-1})]=p^{i}(u,v|I^{i-1})\). Here, we want to emphasize that \(p^{i}(u,v|I^{i-1})\) depends on \(I^{i-1}\), which means the probability of the same item that enters the next stage is also influenced by other candidate items in the same stage and reveals the combinatorial complexity of real-world systems.

## 4. Generalized Probability Ranking Principle

Before we introduce Generalized Probability Ranking Principle (GPRP), we recap the Probability Ranking Principle (PRP) in classical information retrieval (Zhu et al., 2017; He et al., 2016) first:

_Probability Ranking Principle (PRP): If an Information Retrieval system's response to each request is a ranking of items in the collections in order of decreasing probability of usefulness to the user who submitted the request, then the overall effectiveness of the system to its users will be the best that is obtainable on the basis of the data._

The principle is intuitive and proved from the view of traditional measure of effectiveness and decision theory under certain assumptions (Zhu et al., 2017). Suppose we know the underlying interest function \(r(u,v)\) in advance for every request and the system has only one stage (using the same notation as previous section that the system needs to return \(c_{4}\) items to each request), according to PRP, then \(c_{4}\) items returned by the system should be:

\[\operatorname*{argmax}_{\{u_{k}\in\mathcal{I}\}|k\in[c_{4}]\}\sum_{k=1}^{c_{4} }r(u,v_{k}) \tag{1}\]

Suppose we are in the 4-stage system described before, and we need improve a particular stage \(i\in\{1,2,3,4\}\) but have no control on other stages. If we still use PRP in stage \(i\), we should output:

\[\mathcal{I}^{i,\textit{PRP}}\coloneqq\operatorname*{argmax}_{I^{i}\subset I^{ i-1}}\sum_{v\in\mathcal{I}^{i}}r(u,v) \tag{2}\]

Hence, nearly all previous studies in information retrieval systems focus on how to learn the underlying interest function \(r(u,v)\) from logged exposure data, for either retrieval stage (Bishop, 2006; Krizhevsky et al., 2012; He et al., 2015; He et al., 2016) or ranking stage (He et al., 2016; He et al., 2016). Furthermore, since logged exposure data is affected by selection bias of the system which may lead to biased

[MISSING_PAGE_FAIL:3]

The observation is obvious. For example, when subsequent stages' output candidate items to the next stage are completely random, then the optimal candidate set should be chosen according to the underlying interest of users. However, the completely random strategy is very dangerous in an online environment which can hurt users' experience on the platform, and the strategy in each stage is often some learned models from logged data.

**Observation 2**.: _If the selection bias of subsequent stages \(p^{i+1,A}(u,v)\) is monotonically increasing with respect to the underlying interest \(r(u,v)\), then \(\bar{I}^{i,\text{GRPBP}}=\bar{I}^{i,\text{PRBP}}\)._

Above observation is also straightforward, since \(p^{i+1,A}(u,v)\) is monotonically increasing with \(r(u,v)\), which implies the ranking of items in terms of \(p^{i+1,A}(u,v)(u,v)\) is the same as the ranking by \(r(u,v)\), thus \(\bar{I}^{i,\text{GRPBP}}\) equals \(\bar{I}^{i,\text{PRBP}}\). However, the assumption in observation 2 is too strict, as it requires the preference of each stage to exactly coincide with the interest of users in every request, which is impractical considering the existence of learning error of any algorithm in any stage.

Now, given the difference between objective functions (7) and (2), the key question is then how to solve equation (7) efficiently. A straightforward method is to learn \(p^{i+1,A}(u,v)\) and \(r(u,v)\) separately, where each term in \(p^{i+1,A}(u,v)\) can be learned by any supervised algorithm on corresponding data and \(r(u,v)\) term can be learned by any existing algorithm especially unbiased learning algorithms (Bishop, 2006; Bishop, 2006; Bishop, 2006; Bishop, 2006; Bishop, 2006). Having learned \(p^{i+1,A}(u,v)\) and \(r(u,v)\), we can sort items in \(\bar{I}^{i-1}\) according to \(p^{i+1,A}(u,v)r(u,v)\) and output the top \(\varepsilon_{i}\) items as \(\bar{I}^{i,\text{GRPBP}}\). However, this method is also inefficient and may be ineffective, as we need to learn several models which causes additional computational burden and accumulated learning error. Besides, in the retrieval stage, usually we use Approximate Nearest Neighbor (ANN) for fast inference, which does not support the operation in equation (7). To address the above issue, we propose a unified algorithmic framework named Full Stage Learning to Rank.

## 5. Full Stage Learning to Rank

According to objective function (7), we need to sort items according to \(p^{i+1,A}(u,v)r(u,v)\). Instead of learning each term independently, we directly learn the product \(w^{i+1,A}(u,v)=p^{i+1,A}(u,v)r(u,v)\) to avoid issues mentioned in previous section. Apparently \(0^{i+1,A}(u,v)Y(u,v)\) is a realization which can be observed as \(w^{i+1,A}(u,v)\) in practical systems. As \(\mathbb{E}[O^{i+1,A}(u,v)Y(u,v)]=w^{i+1,A}(u,v)\), we obtain our first efficient unbiased algorithm for learning objective function (7) of GPRP under Assumption 1: we collect data after stages \(i-1\) with label 0, except the exposed and clicked data with label 1, then learning a supervised model with such labeled data which is then used online at stage \(i\).

Though this algorithm is simple and easy to implement, it treats all non-exposed items equally, which may omit the abundant information of the data. Intuitively, items which enter stage \(i+1\) may be better than items filtrated by stage \(i\), thus we hope to make full use of the information of collected data to learn the selection bias and user's interest better.

Before introducing our new algorithmic framework, we make another assumption about the multi-stage system that is essential for later analysis:

**Assumption 2**.: _For two observed items \(u_{i},u_{i+1}\) in two subsequent stages \(i\) and \(i+1\) (i.e. \(v_{i}\in\bar{I}^{i},v_{i}\notin\bar{I}^{i+1},v_{i+1}\in\bar{I}^{i+1}\)), where \(i\in[3]\), the ratio between user's interests of these two items is bounded in a constant interval, which is less than the ratio of selecting probability between them, that is_

\[\frac{1}{a}\leqslant\frac{r(u,v_{i+1})}{r(u,v_{i})}\leqslant a\leqslant\frac{p ^{i+1:A}(u,v_{i+1})}{p^{i+1:A}(u,v_{i})} \tag{8}\]

_where \(a\geqslant 1\) is a constant._

The intuition behind Assumption 2 is that each stage of the system can be seen as a cluster of items depending on the degree of user's interest in some sense. Apparently, for an information retrieval system with good performance, user's interest for items in higher stages may be stronger than interest for items in lower stages with high probability. What's more, the interest of users with respect to two items in subsequent stages are close to each other (i.e. \(\frac{r(u,v_{i+1})}{r(u,v_{i})}\in[\frac{1}{a},a]\)). Note this condition does not require the selection bias of the system is exactly the same with user's underlying interest, and allows the learning error of system to some degree, which is much weaker than the monotone assumption of multi-stage systems used in Observation 2 and in line with practical situation. See more discussion about this assumption in next section, where we collect real-world online data from one of largest short-video platforms to verify it in an approximate way and the result implies Assumption 2 is relatively mild in practical multi-stage recommendation systems.

For \(t\)-th request of the system, we collect a series of data at all stages \(\{(u^{t},v_{k}^{t},S_{k}^{t},\bar{V}_{k}^{t})|k\in[M]\}\), where \(M\) is the number of collected data in each request and \(S_{k}^{t}\in[4]\) is an observed random variable representing the stage of item \(v_{k}^{t}\). \(\bar{V}_{k}\in\{0,1,NA\}\) here represents the feedback of \((u,v_{k})\) pair, where \(\bar{V}_{k}=Y_{k}\) when \(S_{k}=4\), otherwise \(\bar{V}_{k}=NA\), since we can only observe the true feedback \(Y_{k}(u,v_{k})\) when the item is exposed to the user.

With these data collected from full stages of different requests, the main technique of our method is to relabel each user-item pair \((u,v,S,\bar{Y})\) in collected data set by the following rule:

\[L(u,v)=\begin{cases}z_{0}&\text{if}\,S(u,v)=0\\ z_{i}&\text{if}\,S(u,v)=i,\text{for}\,i<4\\ z_{i}&\text{if}\,S(u,v)=4,\text{and}\,Y(u,v)=0\\ z_{5}&\text{if}\,S(u,v)=4,\text{and}\,Y(u,v)=1\end{cases} \tag{9}\]

where \(z_{0}\leq z_{1}\leq\cdots\leq z_{5}\) are six non-negative numbers.

The core idea behind this relabeling technique is intuitive, which distinguish the difference among non-exposed items and makes full use of them. Items which are returned to users and receive positive feedback apparently are the most important ones, since they have passes the examination of selection bias of subsequent stages and enjoys users interest. For those items that have been retrieved but not shown to users, entering into next stage of the system is more difficult compared with being liked by users. Therefore, if an item enters into a higher stage of the system, it is more important compared with items in lower stages. In fact, under Assumption 1 and 2, one can prove above relabeling technique coincides with GPRP as stated in the following theorem, which give us a theoretical support of our simple relabeling technique.

**Theorem 1**.: _Suppose Assumption 1 and 2 hold in multi-stage systems, for any collected request data \(\{(u,v_{k},S_{k},\bar{V}_{k})|k\in[M]\}\), after relabeling via equation (9), we obtain new labels \(L(u,v_{k})\) for each data respectively, the a ranking of items in decreasing order of \(L(u,v_{k})\) implies the ranking by term \(w^{i+1,A}(u,v_{k})\) for items in different stages. As a special case, when \(z_{0}=z_{1}=\cdots=z_{4}=0\) and \(z_{5}=1\), the conclusion still holds even without Assumption 2._

According to Theorem 1, now we can use the new label \(L(u,v)\) as an approximate substitution of \(w^{i+1,4}(u,v)\), and this new relabeling also allows efficient learning. Once we can estimate \(L(u,v)\) or has the ability of ranking items which is the same as ranking by \(L(u,v)\), then it nearly matches GPRP, since \(I^{i,OPRP}\) is obtained by ranking \(w^{i+1,4}(u,v)\) in decreasing order according to equation (7) under Assumption 1. Thus, we can use any supervised or general ranking algorithm to reach an efficiently learned model that coincides with GPRP. In detail, having obtained the collected relabeled data \(\{(u^{\prime},v_{k}^{I},L_{k}^{I})|k\in[M]\}|t\in[N]\}\), where \(N\) is the total number of collected requests, we can use any supervised learning or Learning to Rank (LTR) algorithm to learn a model with the goal of correctly ranking items \(\{(u^{\prime},v_{k}^{I},L_{k}^{I})|k\in[M]\}\) in each request, for example Lambda Rank (Bahdanau et al., 2015; Chen et al., 2015).

At stage \(i\), since we only care about the selection bias of stages after \(i\) and users' underlying interest, the collected data at stage \(i\) and before stage \(i\) seems useless. However, from the view of consistency between training and serving, it will be better to use them duration training. For example when serving at retrieval stage, we use the learned model to predict scores for all items. Therefore, it will help a lot to add some random samples from the candidate pool during training. The same reasoning applies at other stages. What's more, we can also add the collected data at any other previous stage duration training, which can be regarded as an auxiliary training task and may help learn the model better. Note using data in previous stages doesn't influence the near consistency between our algorithm and GPRP.

The final algorithmic framework including training and inference is given in Algorithm 1. Since we need collect data from full stages of a system and use LTR algorithm as our backbone, we name our algorithmic framework as Full Stage Learning to Rank, FS-LTR in short. When \(S_{k}^{t}=0\) in Algorithm 1, it means the item \(v_{k}^{t}\) is randomly sampled from \(T\).

As discussed above, by choosing different \(j\) and appropriate models, Algorithm 1 can be used either in retrieval, pre-ranking, ranking or re-ranking. For example, in retrieval stage, we can choose \(j=0\) which includes random negative items from the whole candidate set and dual-encoder models which represents users and items as vectors and supports ANN for fast inference. Similarly in pre-ranking/ranking stage, we can choose suitable \(j\) and dual-encoder model or complex dnn models. In re-ranking stage, Algorithm 1 degrades to backbone LTR algorithm if only using exposed data, and becomes a new LTR algorithm with auxiliary tasks which uses data in previous stages.

```
1Training:
2Input: Initialized model parameter \(\theta\), manually defined labels \(\{z_{i}|i\in[0,5]\}\) and original data-set \(\{(u^{\prime},v_{k}^{I},v_{k}^{I})|k\in[M],S_{k}^{t}\geq j\}|t\in[N]\}\), where \(j\in[0,i+1]\) is a hyper-parameter. Output:Model \(\{(u,v|\theta)\}\)
3While\(\theta\) not convergeddo
4 sample a batch of requests from \([N]\)
5
6 relabel each user-item pair by the equation (9)
7 update model parameters by chosen LTR algorithm
8return\(\theta\)
9
10Serving in stage \(i\):
11Input: Candidate set \(T^{i-1}\) in a request of user \(u\), learned model parameters \(\theta\)
12Output:\(T^{i}\)
13\(I^{i}\coloneqq\mathbf{Top}(T^{i-1},\hat{l}(u,v|\theta),c_{i})\)
14return\(T^{i}\)
```

**Algorithm 1** Full Stage Learning to Rank

## 6. Discussion About Assumptions

As mentioned in Section 4, Assumption 1 is too strong to be true in real complex multi-stage systems, but it is acceptable if this assumption could be satisfied in some degrees, and we explain its reasonableness from four viewpoints:

1. The simplification from a combinatorial system into point-wise system is quite common method in similar situation, like in re-ranking (Hardt et al., 2016).
2. Most of previous work about unbiased learning (Bahdanau et al., 2015) are based on estimating point-wise exposure probability to selection exposure bias of the system and then learn the underlying interests, which also rely on Assumption 1 implicitly.
3. Though the probability \(p^{i}(u,v|T^{i-1})\) varies with input candidate set \(T^{i-1}\), we may consider the general performance of each user-item pair, i.e. \(p^{i}(u,v)=\mathbb{E}[p^{i}(u,v|T^{i-1})]\) as an approximation to \(p^{i}(u,v|T^{i-1})\), where the expectation is over some distribution of \(T^{i-1}\) (for example the distribution of \(T^{i-1}\) of current system).
4. We only use Assumption 1 to induce efficient and practical algorithm. In experimental section, we find our algorithm still works when this assumption does not hold either in simulated experiments or online A/B testing.

Assumption 2 is also critical for FS-LTR. However, it is impossible to collect real \(r(u,v)\) and \(p^{i}(u,v)\) in real systems, as we cannot present the same item twice for a user in short-video platform which implies we can only observe one realization \(Y(u,v)\) of underlying Bernoulli distribution with probability \(r(u,v)\). What's more, items at non-final stages of the system are not exposed to users, so we cannot even observe their realizations of users' underlying interest. Besides, practical system is very complex, there does not exist any true \(p^{i}(u,v)\) in real situation.

One possible approach to solving above difficulty is to learn approximate models about \(r(u,v)\) and \(\{p^{i}(u,v)|i\in[4]\}\) respectively. However, this approach highly depends on the learning performance which is also hard to have some guarantee, because of the discrepancy between learning space and inference space. Therefore, we adopt an approximate verification approach.

In detail, we collect items at different stages in previous request (there are 5 stages in our system), and then force these items to be exposed to corresponding users directly, which don't need to enter the multi-stage system to avoid its selection bias. Now we can receive ground-truth feedback of items at different stages. Table 1 shows the average performance of CTR (Click Through Rate) at different stages of such collected data. We can see average posterior CTR is very close between consecutive stages, and the ratio between them is also in a small interval \([\frac{1}{2},2]\). Thus we know users' interest for all items which have entered the system since at least on average, which implies the inequality \(\frac{1}{a}\leq\frac{r(u_{i}u_{i1})}{r(u_{i1}u_{i1})}\leq a\) of equation (8) may be true in real-world data \(a\) may be 2 in our case. Besides, the inequality about term \(\frac{\rho^{i+1}(u_{i1}u_{i1})}{p^{i+1}(u_{i1}u_{i1})}\) in equation (8) means an item that could be selected to the next stage is relatively more difficult than user whether likes it compared with other candidate items in subsequent stages. In practical system where \(c_{1}/c_{2}/c_{3}/c_{4}/c_{5}=6000/3000/500/120/10\), the selecting probability from stage 2 to 3 is roughly \(\frac{500}{5000}=\frac{1}{6}\) on average, which means the ratio of selecting probabilities between positive (i.e. select by stage 2 into stage 3) and negative item (i.e. filtrated by stage 2) in this stage is roughly 6 on average, hence the ratio of exposure probability between these two items after subsequent stages could be even larger than 6, which may be greater than \(\frac{r(u_{i1}u_{i1})}{r(u_{i1}u_{i1})}\) with high probability. Therefore, Assumption 2 may be mild enough to be satisfied in a real environment, at least in a short-video platform.

## 7. Practical Implementation

In this section, we present our implementation details of FS-LTR in real-world online multi-stage recommendation systems, including data collection, training, and serving.

**Data Collection:** Collecting data in full stages of a recommendation system online is a very challenging engineering task, since the number of these data far exceeds the magnitude of the exposure data in each request. For example, it could be 10000 versus 6 in real system, hence it is impractical to collect all of them considering the cost of storage and communication bandwidth. In practice, for each request, we randomly sample 40 negative items in retrieval and pre-ranking stage respectively, and record all items (around 400) in ranking stage with corresponding labels representing the final stage of items.

**Training:** We use lambda rank to train our model in retrieval stage with \(z_{i}=i\). To reduce training cost, we only use a subset of collected data in each request. The higher stage the data belongs to, the more we use it, since data in higher stages is more important and hard to be learned. We also substitute the cross-entropy loss in lambda rank with margin loss, as well as trying different labels (for example, 0, 1, 2, 3, 4, 6) to enhance the learning of hard samples. Though having some improvements in offline evaluation, it doesn't lead to online gain. What's more, to speed up training, one could use pairwise LTR instead of lambda rank, which avoids expensive sort operation during training but with only mild damage in online performance. Finally, it is also possible to use softmax loss with soft labels \(l(u,v)\) to further speed up training.

**Serving:** At retrieval stage, to enhance the diversity of returned results in each request, we add some noise to the top representation vector of users, which also fulfills some mild exploration. The same trick could be used in other stages too.

## 8. Experiments

In this section, we conduct both offline experiments and online A/B testing to answer the following research questions:

* How can we simulate a multi-stage system with an offline dataset for evaluating our proposed method? (Sec 8.1)
* What is the effectiveness of FS-LTR on the offline simulated multi-stage recommendation system? (Sec 8.2)
* How does each component or hyperparameter in Full Stage Learning to Rank affect the performance? (Sec 8.3.2)
* How does FS-LTR perform in online environment? (Sec 8.4)

### Offline Simulation of Multi-Stage Pipeline

In this section, we introduce our offline simulation of the multi-stage recommendation systems from three perspectives: dataset, multi-stage simulation, and training data.

#### 8.1.1. Dataset

Different stages of a multi-stage recommendation have an exponential magnitude difference in the number of candidates, thereby the first stage needs an enormous amount of candidates. However, most of the datasets in recommendation systems are highly sparse, which leads to a limited number of available reasonable candidates. Even if we can train a model to capture the latent preference of the user, without the ground truth of user-item interaction, we still cannot determine whether the user prefers the item. The original sparse ground truth may be more sparse after passing the multi-stage pipeline.

To build a convincing simulation on the multi-stage recommendation pipeline, we utilize the fully-observed dataset **KuaiRec**(Kui et al., 2018) as our base dataset. KuaiRec is a real-world dataset collected from the recommendation logs of a video-sharing platform. "Fully Observed" means that there are almost no missing values in the user-item interaction matrix, allowing an enormous amount of available candidates with ground truth for simulating the multi-stage recommendation pipeline. There are two user-item interaction matrices in KuaiRec, named _small matrix_ and _big matrix_. The statistics of the two matrices are listed in Table 3. Both of these two matrix are more dense than most of the recommendation datasets. Moreover, rich side features are provided for each user and item in the KuaiRec dataset, enabling training a good ranking model for simulating the multi-stage pipeline. All of the user and item in _small matrix_ also occur in the _big matrix_, but interactions in _small matrix_ and _big matrix_ are excluded from each other.

#### 8.1.2. Multi-Stage Simulation

We consider building a multi-stage pipeline containing three parts: retrieval, prerank, and rank. Each item must pass all these three stages to be exposed to the user. In each stage, a learned model will score each candidate item, and items with relatively high scores can be passed to the next stage. We assume that

* **Retrieval candidate pool** contains all of the items in _big matrix_(~11000).

\begin{table}
\begin{tabular}{c|c c c c c}  & Stage 1 & Stage 2 & Stage 3 & Stage 4 & Stage 5 \\ \hline _Posterior CTR_ & 0.29 & 0.33 & 0.34 & 0.49 & 0.56 \\ \hline \end{tabular}
\end{table}
Table 1. Posterior CTR of Items at Different Stages.

\begin{table}
\begin{tabular}{l|c c c c}  & \#User & \#Item & \#Interaction & Density \\ \hline _small matrix_ & 1,411 & 3,327 & 4,676,570 & 99.6\% \\ _big matrix_ & 7,176 & 10,728 & 12,530,806 & 16.3\% \\ \end{tabular}
\end{table}
Table 3. Statistics of KuaiRec dataset.

\(\bullet\) **Prerank candidate pool** of each user contains all of the items exposed to the user in both _big matrix_ and _small matrix_(-3500).

Since we hope the learned models have a relatively strong ability to predict users' interests, we do not mind using more data to train the prerank / rank model. We use the _big matrix_ with user features and item features to train the prerank / rank model. We select the two-tower DNN for the prerank model and the single-tower DNN for the rank model. Details can be viewed in Appendix A.4. All of the prerank candidates can be scored with the prerank model and the rank model, enabling us to simulate the multi-stage pipeline.

For the convenience of later training and evaluation, we model the whole multi-stage pipeline as a static request. In a static request, the following steps are executed in sequence:

1. For each user, 1,000 items are randomly selected from the prerank candidate pools as the **simulated prerank candidates**.
2. For each user, the top 200 items with highest prerank scores in simulated prerank candidates are selected as the **simulated rank candidates**, while the bottom 200 items with lowest scores in simulated prerank candidates are regarded as the **simulated prerank negative samples**.
3. For each user, the top 50 items with highest rank scores in simulated rank candidates are selected as the **simulated exposed candidates**, while the bottom 50 items with lowest scores in simulated rank candidates are regarded as the **simulated rank negative samples**.
4. For each user, items with positive labels are regarded as the **simulated exposed positive samples**, while items with negative labels are regarded as the **simulated exposed negative samples**.

Multiple request procedures can be repeatedly simulated to generate different multi-stage recommendation training samples. For evaluation, requests for validation or tests are separated in advance in order to avoid label leakage.

#### 8.1.3. Data Preparation for Training and Evaluation

Most of the recommendation models are training on the exposed samples, e.g., the retrieval models are trained on the exposed positive samples with randomly sampled negative samples, the rank models are trained on the exposed samples. In our simulated multi-stage recommendation pipeline, models should be trained on the simulated exposed samples. From this perspective, the prerank / rank model in the simulated multi-stage pipeline is not aligned with the real settings. However, it is acceptable since we do not mind the simulated rank model is more powerful in the simulated multi-stage pipeline. The simulated exposed samples are partially randomly generated from the simulated multi-stage pipeline, which may cause difficulty in splitting the dataset for training, validation, and testing. In order to avoid label leakage, we follow the following steps to generate simulated training data:

1. Generate a static request based on the simulated prerank / rank model and the original prerank candidate pool. The simulated exposed positive samples are reserved as positive samples in the test set, and removed from the original prerank candidate pool.
2. Generate a static request based on the simulated prerank / rank model and the prerank candidate pool without the positive samples in the test set. The newly simulated exposed positive samples are reserved as positive samples in the validation set, and removed from the prerank candidate pool.
3. Generate multiple static requests based on the simulated prerank / rank model and the prerank candidate pool without the positive samples in the validation and test set. Samples of each request can be utilized as training samples.

Complete separation of the train, validation, and test set is accomplished by the steps above. In our request-based training setting, the training request is set to a random request. Exposed samples and multi-stage samples are randomly sampled from the randomly selected request to build multiple pair-wise or single list-wise optimization objectives.

### Performance Comparison

#### 8.2.1. Implementation Details

We utilize the **MF**(Wang et al., 2017) as the model structure for all methods. We implement all of the offline experiments with Pytorch 1.13 in Python 3.8. We set the same value for basic hyperparameters. The size of the embedding is set to 64, the batch size is set to 128. I.2 normalization is applied to the embedding during training and inference. We use the Adam optimizer with a learning rate of 0.001 for training for all methods.

#### 8.2.2. Baseline Methods

We select multiple widely used optimization objectives as baseline methods for our experiments:

* **BPR**(Wang et al., 2017) (Beyesian Personalized Ranking). An objective models the posterior preference probability of a user-item pair based on a single sampling negative user-item pair. The default number of negative samples is 5.

\begin{table}
\begin{tabular}{c|c|c c c c|c c c c} \hline \hline Stage & Method & HR@20 & NDCG@20 & R@20 & P@20 & HR@50 & NDCG@50 & R@50 & P@50 \\ \hline \multirow{4}{*}{Retr} & BPR & 15.77(\(\pm\)0.97) & 5.50(\(\pm\)0.53) & 0.55(\(\pm\)0.03) & 0.88(\(\pm\)0.05) & 35.59(\(\pm\)1.85) & 9.46(\(\pm\)0.62) & 1.41(\(\pm\)0.07) & 0.91(\(\pm\)0.04) \\  & SSM & 17.32(\(\pm\)1.34) & 6.02(\(\pm\)0.50) & 0.59(\(\pm\)0.05) & 0.96(\(\pm\)0.08) & 37.88(\(\pm\)0.86) & 10.16(\(\pm\)0.42) & 1.53(\(\pm\)0.05) & 0.99(\(\pm\)0.03) \\ \cline{2-11}  & F-SRN & 18.14(\(\pm\)0.49) & 6.41(\(\pm\)0.25) & 0.63(\(\pm\)0.03) & 1.03(\(\pm\)0.03) & 38.60(\(\pm\)0.73) & 10.52(\(\pm\)0.19) & 1.61(\(\pm\)0.05) & 1.04(\(\pm\)0.03) \\  & FS-LR & 18.83(\(\pm\)0.58) & 6.69(\(\pm\)0.12)* & 0.66(\(\pm\)0.02) & 1.06(\(\pm\)0.03) & 40.75(\(\pm\)0.33)* & 11.07(\(\pm\)0.08) & 1.72(\(\pm\)0.02)* & 1.10(\(\pm\)0.01)* \\ \hline \multirow{2}{*}{PR} & BCE & 14.40(\(\pm\)1.10) & 4.93 (\(\pm\)0.35) & 0.48(\(\pm\)0.04) & 0.76(\(\pm\)0.06) & 32.57(\(\pm\)1.68) & 8.54(\(\pm\)0.50) & 1.24 (\(\pm\)0.09) & 0.79(\(\pm\)0.06) \\  & FS-LR & 19.56(\(\pm\)1.18)* & 6.74(\(\pm\)0.48)* & 0.70(\(\pm\)0.05)* & 1.13(\(\pm\)0.07)* & 40.16(\(\pm\)0.72)* & 10.95(\(\pm\)0.39)* & 1.79(\(\pm\)0.04)* & 1.15(\(\pm\)0.02)* \\ \hline \hline \end{tabular}
\end{table}
Table 2. Performance comparison on our simulated multi-stage pipeline. Bold numbers represent the best results. All the numbers in the table are percentage numbers with %’ omitted. All experiments are repeated 5 times to calculate the mean and standard deviation. We conduct an unpaired t-test of Full Stage Learning to Rank and the best baseline and the improvement of Full Stage Learning to Rank is significant with \(\rho\leq 0.05\) for all values with \({}^{**}\).

* **SSM[8, 15]** (Sampled-SoftMax). An objective models the preference probability of user-item pairs based on multiple sampling negative user-item pairs. The default number of negative samples is 5.
* **BCE** (Binary Cross Entropy). An objective directly optimizes the preference probability of the user-item pair.

BPR and SSM are common baseline methods for retrieval, and negative user-item pairs for them are sampled from the global candidate pool. BCE is adopted as the baseline method for our offline experiment on prerank stage.

#### 8.2.3. Evaluation Metrics

We adopt four widely used metrics for evaluation of our methods, i.e., **R** (Recall), **P** (Precision), **HR** (HitRate), and **NDCG** (Normalized Discounted Cumulative Gain). The metrics are computed on the top 20/50 matched items. As mentioned above, the different stage has a different candidate pool for evaluation. The prerank stage has a relatively smaller candidate pool than retrieval. However, metrics only have a very limited difference between the retrieval and prerank stage for our imperfect simulation on a multi-stage recommendation system.

#### 8.2.4. Overall Performance

Performance comparison is listed in Table 2. For baseline methods and our FS-ITR methods in retrieval, the negative sample size for each training instance is set to 5 to eliminate the effect of the negative sample size. Two variants of our FS-LTR, FS-RN(RankNet) and FS-LR(LambdaRank) outperform significantly to the baseline methods in the retrieval stage. Our FS-LR method also shows an advantage over the commonly used BCE in the prerank stage.

### Ablation Study

Now we conduct experiments to further understand the effect of multi-stage negative samples and negative sample size in FS-LTR.

#### 8.3.1. Effectiveness of multi-stage samples

The complete FS-LTR in retrieval requires user-item pairs from full stages in recommendation systems. We remove user-item pairs from different stages to verify the effectiveness of each stage sample. Results are shown in Table 6. The re-labeled label is also adapted to the number of stages left in the training samples. Removing the stage-negative samples causes the most degradation in model performance, showing that negative samples in a multi-stage recommendation system play a key role in FS-LTR.

#### 8.3.2. The number of samples from different stages

We show the performance of FS-LR on the number of samples from different stages in Table 5. Results show that more negative sample sizes may have a positive effect on model performance. The ratio of different stage negative sample sizes is also critical for model performance.

### Online A/B Testing

We used our Algorithm 1 at the retrieval stage on one of the largest short-video platforms with implementation details described in Section 7. The A/B test lasted for six days and had influenced over 20 million users in the experiment group, reaching a significant improvement compared with the base group which already had some strong baselines like TDM [42], Multi-Interest Retrieval [37], Comi-Rec Retrieval [3] etc. As shown in Table 4, our approach achieves significant gains in many engagement metrics. What's more, our retrieval algorithm has the highest reveal ratio (16%, where the second highest reveal ratio is 6%) of any other retrieval algorithm online, which coincides with the GPRP.

## 9. Conclusions and Future Work

We believe our work opens a new direction of research in multi-stage IR systems, which needs to take both the selection bias in multiple stages of the system and users' underlying interest into consideration. We proved the effectiveness of this solution framework in both offline experiments and online A/B test. There are several important future directions which worth indepth exploration. First, we have taken initial efforts aim to decipher the behavior of multi-stage systems, and a more comprehensive understanding will help us design more efficient and effective ranking algorithms. Second, our focus has been primarily on the alignment between our learning objective and GPRP, thus calling for the need to examine the generalization performance throughout the entire learning processes. Third, it is worthwhile to further customize the general framework for specific stages of the system for better performance. Lastly, exploring optimal solutions to handle multiple or all system stages, rather than just one, remains imperative.

\begin{table}
\begin{tabular}{l|c c c c} \hline \hline  & HR@50 & NDCG@50 & R@50 & P@50 \\ \hline w/o _exposed neg_ & 37.29 & 10.03 & 1.52 & 0.98 \\ w/o _stage neg_ & 35.52 & 9.72 & 1.44 & 0.93 \\ w/o _rank neg_ & 39.40 & 10.74 & 1.63 & 1.04 \\ w/o _prerank neg_ & 38.31 & 10.16 & 1.56 & 1.01 \\ \hline FS-LR & 40.75 & 11.07 & 1.72 & 1.10 \\ \hline \hline \end{tabular}
\end{table}
Table 6. Ablation study on different stage examples.

\begin{table}
\begin{tabular}{l|c c c c c c c} \hline App Usage Time Per User & Real-show & Click & Like & Follow & Forward & Comment & Watch Time \\ \hline +0.12\% & +0.50\% & +0.69\% & +0.40\% & 0.74\% & +0.94\% & 1.08\% & +0.18\% \\ \hline \end{tabular}
\end{table}
Table 4. Online A/B testing results.

\begin{table}
\begin{tabular}{l|c c c c} \hline \hline Negative Sampling & HR@50 & NDCG@50 & R@50 & P@50 \\ \hline
1,1,1 & 39.12 & 10.45 & 1.68 & 1.07 \\
1,2,1 & 40.71 & 10.73 & **1.73** & **1.10** \\
2,1,1 & 39.67 & 10.71 & 1.68 & 1.07 \\
1,1,2 & **40.75** & **11.07** & 1.72 & 1.10 \\
1,1,3 & 39.24 & 10.73 & 1.63 & 1.05 \\
1,2,3 & 39.71 & 11.00 & 1.69 & 1.09 \\ \hline \hline \end{tabular}
\end{table}
Table 5. Study on the number of different stage examples. The ‘x,y,z’ in the Negative Sampling column denotes the number of rank negative samples, prerank negative samples, and global negative samples in one training instance, respectively. Exposed negative sample size is set to 1 for all methods.

## References

* (1)
* Burges et al. (2006) Christopher Burges, Robert Ragno, and Quoc Le. 2006. Learning to rank with nonsmooth cut functions. _Advances in neural information processing systems_ 19 (2006).
* Chortijver (2010) Christopher JC Burges. 2010. From ramacher to lambdrank to lambdamart: An overview. _Learning_ 11, 23-531 (2010), 2010.
* Chen et al. (2020) Yukuo Chen, Jianwei Zhang, Xu Zhou, Chang Zhang, Hongzhi Yang, and Jie Tang. 2020. Controllable Multi-Interest Framework for Recommendation. In _Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, 2042-2951.
* Chang et al. (2023) Jianmin Chang, Chenbin Zhang, Zhiyi Fu, Xiaowue Zang, Lin Guan, Jing Lu, Yiqun Hui, Duwei Leng, Yannu Niu, Yong Song, et al. 2023. TNNF: Two-stage Interest Network for Lifelong Deep Learning: Modularity in CT Prediction at Kausliou. _arXiv preprint arXiv:2302.02325_ (2023).
* Chang et al. (2023) Jianwei Chang, Chenbin Zhang, Yiqun Hui, Dewei Leng, Yannu Niu, Yang Song, and Kun Gai. 2023. Ppaint: Parameter and embedding personalized network for infusing with personalized prior information. In _Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining_, 3795-3804.
* Chen et al. (2019) Minmin Chen, Alex Beutel, Paul Couvington, Sagan Jain, Francisco Belletti, and Ed H Chi. 2019. Ppaint: Poly-poly correction for a REINFORCE recommender system. In _Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining_, 456-464.
* Covington et al. (2016) Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep Neural Networks for YouTube Recommendations. In _Proceedings of the 10th ACM Conference on Recommender Systems_. New York, NY, USA.
* Covington et al. (2016) Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for youtube recommendations. In _Proceedings of the 10th ACM conference on recommender systems_. 191-198.
* Creswell et al. (2008) Nick Creswell, Omao Zoeter, Michael Taylor, and Bill Ramsey. 2008. An experimental comparison of click position-bias models. In _Proceedings of the 2008 international conference on Web search and data mining_, 87-94.
* Feng et al. (2021) Yifeng Feng, Binbin Hu, Yong Pei, San Qingpu Liu, and Wenru Ou. 2021. GRB: Generative Rezunk Network for Context-wise Recommendation. _arXiv preprint arXiv:2104.00806_ (2021).
* Gao et al. (2022) Chongming Gao, Shiban Li, Wenqiang Lei, Jaweei Chen, Biao Li, Peng Jiang, Xiangen He, Jiaxu Mao, and Tat-Seng Chua. 2022. Kausliece: A Fully-Observed Dataset and Insights for Evaluating Recommender Systems. In _Proceedings of the 31st ACM International Conference on Information & Knowledge Management (Atlanta, GA, USA)_ (2021) 250-2505. [https://doi.org/10.1451/31803.3857220](https://doi.org/10.1451/31803.3857220)
* Ccyg (1994) Frederic Ccyg. 1994. Inferring probability of relevance using the method of logistic regression. In _SIGIR'94 Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, organised by Dublin City University_. Springer, 22-321.
* Yin et al. (2019) Zimin Yin, Yang Wang, Qin Peng, and Hang Li. 2019. Unbiased lambdamart: an unbiased pairwise learning-to-rank algorithm. In _The World Wide Web Conference_. 2829-2836.
* Le et al. (2019) Eugene Le, Vhan Jain, Jing Wang, Sammit Narvekar, Ritesh Agarwal, Rui Wu, Teng-Tie Cheng, Fisher Chandra, and Craig Boulier. 2019. SLATEQ: a tractable decomposition for reinforcement learning with recommendation sets. In _Proceedings of the 28th International Joint Conference on Artificial Intelligence_, pages 2592-2599.
* Jean et al. (2014) Sebastien Jean, Kyunghyun Cho, Roland Manezliewicz, and Yoshua Bengio. 2014. On using very large vector vocabulary for neural machine translation. _arXiv preprint arXiv:1412.2007_ (2014).
* Jia et al. (2021) Yiling Jia, Huazhong Wang, Stephen Guo, and Hongning Wang. 2021. Pairrank: Online pairwise learning to rank by divide-and-conquer. In _Proceedings of the World Conference_. 2021, 146-157.
* Joachims et al. (2017) Thorsten Joachims, Laura Grabs, Bing Fan, Helen Hendrickson, and Geri Gay. 2017. Accurately interpreting clickthrough data as implicit feedback. In _Acm Sigir Forum_, Vol. 51. Acm New York, NY, USA, 4-11.
* Joachims et al. (2017) Thorsten Joachims, Adith Swaminathan, and Tobias Schnabel. 2017. Unbiased learning-to-rank with biased feedback. In _Proceedings of the tenth ACM international conference on web search and data mining_, 781-789.
* Kang and McAlloy (2018) Wm-Cheng Kang and Julian McAlloy. 2018. 5d: "Gattentive sequential recommendation. In _2018 IEEE International Conference on Data Mining (ICDM)_. IEEE, 197-206.
* Koren et al. (2009) Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender systems. _Computer_ 42, 8 (2009), 30-37.
* Li et al. (2011) Chao Li, Zhiyuan Liu, Mengming Wu, Yuch Xu, Huan Zhao, Pipei Huang, Guoliang Kang, Qiwei Chen, and Li, and Du Li. 2011. Multi-Interest Network with Dynamic Routing for Recommendation at Tamil. In _Proceedings of the 28th ACM International Conference on Information and Knowledge Management_ (Belling, China) (ICDM '19). Association for Computing Machinery, New York, NY, USA, 2615-2623. [https://doi.org/10.1145/3357834.3357184](https://doi.org/10.1145/3357834.3357184)
* Liu et al. (2017) Slichen Liu, Lei Xie, Wenruu, and Lan Jose M. 2017. Cascade ranking for operational co-commerce search. In _Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, 1557-1565.
* Liu et al. (2009) Tie-Yan Liu et al. 2009. Learning to rank for information retrieval. _Foundations and Trends(r) in Information Retrieval_ 3, 3 (2009), 253-331.
* Pei et al. (2019) Changha Pei, Yihang Zhang, Pei Sun, Xiao Liu, Hanxiao Sun, Jian Wu, Peng Jiang, Jandeng Ge, Wenruu Ou, et al. 2019. Personalized re-ranking for recommendation. In _Proceedings of the 13th ACM conference on recommender systems_. 3-11.
* Qi et al. (2020) Qi Pi, Guoni Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, Xiaoqiang Zhu, and Kun Gai. 2020. Search-based user interest modeling with lifelong sequential behavior data for click-through rate prediction. In _Proceedings of the 29th ACM International Conference on Information & Knowledge Management_, 2020-2025.
* Rendle et al. (2012) Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2012. iPR: Bayesian personalized ranking from implicit feedback. _arXiv preprint arXiv:1205.1281_ (2012).
* Robertson et al. (2009) Stephen Robertson, Hugo Zangoza, et al. 2009. The probabilistic relevance framework: RMLS and beyond. _Foundations and Trends(r) in Information Retrieval_ 3, 4 (2009), 333-389.
* Robertson (1977) Stephen E Robertson. 1977. The probability ranking principle in IR. _Journal of documentation33_ 3, 4 (1977), 294-304.
* Rubin (1994) Donald B Rubin. 1994. Estimating causal effects of treatments in randomized and nonrandomized tables. _Journal of educational Psychology_ 46, 13 (1974), 688.
* Saito (2020) Yuta Saito. 2020. Unbiased pairwise learning from biased implicit feedback. In _Proceedings of the 20th ACM SIGIR on International Conference on Theory of Information Retrieval_ 5-12.
* Singhal et al. (2020) Amit Singhal et al. 2020. Modern information retrieval: A brief overview. _IEEE Data Eng. Bull._ 24, 4 (2020), 35-43.
* Tay et al. (2008) T Tay, Yish Tran, Monthia Dehghani, Jhaun N, Dara Bahri, Harh Mehta, Zime Qiu, Kai Hui, Zhe Zhao, Jiri Prakash Gupta, Li Schuster, William W. Cohen, and Donald Metzler. 2020. Transformer memory as a Differentiable Search Index. In _NeurIPS_. [http://papers.nips.cc/paper/2022/hash/](http://papers.nips.cc/paper/2022/hash/)
* Wang et al. (2022) Yiqing Wang, Yingyan Hou, Haonan Wang, Ziming Ma, Shihui Wu, Qi Chen, Yuqing Xie, Chenghui Chi, Guoliang Zhou, Zheng Liu, Xing Xie, Hu, Weiwei Deng, Qi Zhang, and Mao Yang. 2022. A Neural Corpus for Documen Retrieval. In _Advances in Neural Information Processing Systems_, K. Xopya, S. Mohamed, A. Agarwal, I. Belgrave, K. Cho, and A. Oh (2018). Vol. 35. Curran Associates, Inc., 25600-25614. [https://proceedings.neurips.cc/paper/files/paper/4164501563579816868204413-13-Paper-Conference.pdf](https://proceedings.neurips.cc/paper/files/paper/4164501563579816868204413-13-Paper-Conference.pdf)
* Wang et al. (2020) Zike Wang, Liqin Zhao, Byley Zou, Guoni Zhou, Xiaoqiang Zhu, and Kun Gai. 2020. Cold: Towards the next generation of pre-ranking system. _arXiv preprint arXiv:2007.14122_ (2020).
* Wechsler and Schabide (2000) Martin Wechsler and Peter Schabide. 2000. The probability ranking principle revisited. _Information Retrieval_ 3 (2000), 277-287.
* Yi et al. (2016) Daeyu Yi, Yuxing Hu, Jiang Tang, Timo Li, Maityanzi Zhou, Hua Ouyang, Chun Chen, Yanghong Kang, Hongzhi Deng, Chizakui, Nikola, et al. 2016. Ranking relevance in video search. In _Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining_, 332-332.
* Zhang et al. (2023) Yuan Zhang, Xu Dong, Weiqie Ding, Hao Li, Feng Jiang, and Kun Gai. 2023. Divide and commer: Towards Better Embedding-based Retrieval for Recurrent Textual Systems From a Multi-Task Perspective. _arXiv preprint arXiv:2002.00285_ (2023).
* Zhou et al. (2018) Chang Zhou, Jianxin Ma, Jianwei Zhang, Jingwen Zhou, and Hongzhi Yang. 2018. Contrastive learning for delayed candidate generation in large-scale recommender systems. In _Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining_, 385-3995.
* Zhou et al. (2019) Guoni Zhou, Nao M, Ying Fan, Qi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai. 2019. Deep interest evolution network for click-through rate prediction. In _Proceedings of the AAAI conference on artificial intelligence_, Vol. 33. 5941-5948.
* Zhou et al. (2018) Qunqi Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yu, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In _Proceedings of the 20th ACM SIGKDD international conference on knowledge discovery & data mining_, 1059-1068.
* Zhou et al. (2019) Han Zhou, Daqing Chang, Xue Peng, Xiang Zhang, Jiang Li, Jie Tang, Hai, Li, Sun Xu, and Kun Gai. 2019. Joint Optimization of the Travel Index and Deep Model for Recommender Systems. Curran Associates Inc., Inc., Red Hook, NJ, USA.
* Zhu et al. (2018) Han Zhu, Xiang Li, Peng Zhang, Guoliang Li, Jie Tang, Hai, and Kun Gai. 2018. Learning Tree-Based Deep Model for Recommender Systems. In _Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_ (London, United Kingdom) (KDD '18). Association for Computing Machinery, New York, NY, USA, 1079-1088. [https://doi.org/10.1145/3219836](https://doi.org/10.1145/3219836)

[MISSING_PAGE_FAIL:10]