# Team formation amidst conflicts

Anonymous Author(s)

###### Abstract.

In this work, we formulate the problem of _team formation amidst conflicts_. The goal is to assign individuals to tasks, with given capacities, taking into account individuals' task preferences and the conflicts between them. Using dependent rounding schemes as our main toolbox, we provide efficient approximation algorithms.

Our framework is extremely versatile and can model many different real-world scenarios as they arise in educational settings and human-resource management. We test and deploy our algorithms on real-world datasets and we show that our algorithms find assignments that are better than those found by natural baselines. In the educational setting we also show how our assignments are far better than those done manually by human experts. In the human-resource management application we show how our assignments increase the diversity of teams. Finally, using a synthetic dataset we demonstrate that our algorithms scale very well in practice.

team formation, conflicts, task assignment, diversity +
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

+
Footnote †: isbn: 978-1-4503-XXXX-X/18/06

## 1. Introduction

In large project-based classes instructors often need to create _teams_ of students and assign them to a finite number of projects they have available. Students are happy if they are in a team with friends and they work on a project they like. Additionally, student teams are efficient if there are no time conflicts between the team members; i.e., conflicts that stem from their class schedule. Traditionally, such assignments are done in an adhoc manner or manually by some admin who can spend several days on the task.

Motivated by such applications in the education domain, we formally define the above problem as a combinatorial optimization problem. For this, we assume two inputs: the _preference graph_ and the _conflict graph_. The former captures the preferences of users to projects; this is a bipartite graph with edge weights that are proportional to how much a student likes a project. The latter captures the conflicts between students, i.e., there is a (weighted) edge between two students if they are incompatible. Our goal is to find an assignment of students to projects such that every student is assigned to one project and every project is not assigned more students than its capacity. The objective is to maximize sum of the weights of the edges of the preference graph that participate in the assignment and and the sum of the weights of the conflict edges across the formed teams. Students assigned to the same project form a team so we call this problem the Team Formation amidst Conflicts (TFC) problem and we show it is NP-hard.

In this paper, we present an algorithmic framework for approximating TFC. This framework, consists of two steps: first, our objective is replaced by a concave objective - which can be optimized in polynomial time and produce a fractional solution. Then, the fractional solution is rounded, using dependent-rounding techniques (Gardner, 1999; Gardner, 1999). For our framework to work we need the original objective and its concave relaxation to match one integral inputs.

This general framework is not new. In fact, it is inspired by the _Max-k-Cut with given part sizes_ problem (Gardner, 1999). In fact, our problem is identical to the _Max-k-Cut with given part sizes_, except for the fact that we also have an additional linear term in our objective. We show that their approximation algorithm can be applied to our problem. Our contribution is a much more efficient randomized algorithm with better approximation ratio on expectation.

To the best of our knowledge the dependent-rounding techniques we use here, such as pipage and randomized pipage, have not been widely used in practical applications; they primarily stem from work in theoretical computer science (Gardner, 1999; Gardner, 1999). We see the deployment of these techniques in practice as a contribution by itself.

Using real, anonymized data, from large classes 1, we demonstrate that our algorithms work extremely well in practice. In our experiments, we show that the solutions we obtain are much better compared to the manual solutions produced by a course admin across different dimensions and metrics.

Footnote 1: We obtained an IRB exemption to use the anonymized version of this data.

Our problem formulation is general and goes beyond educational settings. For example, we can use our framework in human-resource management in order to increase the diversity of departments in companies; depending on the dimension across which we want to diversify we can appropriately define the conflict graph. In our experiments, we show how to achieve gender diversity in a company's departments using this idea.

The generality of our framework calls for efficient algorithms. Part of our contribution is a set of speedup techniques that allow us to apply our approximation algorithms to reasonably large data. In our experimental evaluation we demonstrate that these techniques work extremely well in practice.

**Discussion:** We note here that it was a design decision from our part to consider the conflict graph and maximize conflicts across teams (instead of friends within teams). We believe that this choice gives us greater modeling flexibility to apply our model to a variety of settings. For example, the conflict graph better models time conflicts among collaborators as well as diversity constraints.

## 2. Related Work

To the best of our knowledge, we are the first to define and approximate the TFC problem. However, our work is related to works in _team formation_, in the data-mining literature, as well as other _assignment_ and _clustering_ problems, in the theoretical computer science literature. We review these works below.

**Team formation:** In terms of application, our work belongs to the team-formation literature (Becker et al., 2010; Chen et al., 2011; Chen et al., 2012; Zhang et al., 2013; Zhang et al., 2014; Zhang et al., 2015). Most of these works consider the problem of assigning groups of individuals to tasks (one group per task) such that the tasks are completed and some objective (usually related to the well-functioning of the team or the well-being of the individuals) is optimized. Our problem is a partitioning problem and as such is much more complicated than problems of finding a good team for each available task. Additionally, in many of the existing works the objective function has a well-defined structure (e.g., is monotone and submodular, concave). Our objective does not have such a structure and while this gives us modeling power, optimizing it requires more advanced techniques.

**Partitioning problems:** The _Max-k-Cut with given part sizes_ problem (Chen et al., 2011) served as an inspiration for our model. In fact, our \(1/2\)-approximation algorithm is a very close variant of the algorithm presented there. However, our objective function is slightly different from the one defined by Ageev et al. - due to an additional linear term. This allows us to design algorithms tailored to our problem, which achieve better approximation ratios under certain assumptions. Additionally, we focused on developing scalable algorithms as the running time of the algorithm proposed by Ageev et al. was not a computationally feasible approach.

**Clustering problems:** One can view our problem as a clustering problem with capacity constraints (Ageev et al., 2010; Ageev et al., 2011). Our model, however, is quite different from these works both in the objective function and the constraints.

**Assignment problems:** Our problem can be viewed as a generalization of the _weighted assignment_ problem (Ageev et al., 2011), where the goal is to assign individuals to tasks taking into account the task preferences of individuals. In our problem, apart from task preferences, we also have a graph capturing the relationships (or conflicts) between individuals. This additional structure increases the complexity of the problem significantly. Our problem is also related to the famous _stable marriage_(Grover and Leskovec, 2007) problem and its variants (Grover and Leskovec, 2007; Leskovec, 2007). However, we don't look for a stable matching. Instead, our goal is to optimize an objective function capturing the overall satisfaction of individuals.

**The metric-labeling problem**: A minimization version of our problem is the _metric-labeling_ problem (Mikolov et al., 2013), where the goal is to assign one of \(k\) labels to each node (i.e., partition the nodes). Every assignment incurs assignment costs (based on the choice of label for each node) and separation costs (based on the choice of labels for "related" nodes). In the capacitated version of metric-labeling (Becker et al., 2010) we are also given a capacity for each partition. The main disadvantage of the algorithm developed for this version (Becker et al., 2010) is that the capacity constraints are violated by a multiplicative factor. Also, the algorithm only works for labels with uniform capacities. Finally, the approximation factor of the proposed algorithm depends on the number of labels (i.e. tasks). Defining TFC as a maximization problem allows us to overcome all of the above disadvantages.

## 3. Problem Definition

In this section, we provide the necessary notation and we formally define the problem we solve in this paper.

**Notation:** Throughout the paper, we assume that we are given a weighted (undirected) graph \(G=(V,E_{G},w)\) with \(w:E_{G}\rightarrow\mathbb{R}_{\geq 0}\). More specifically, each node \(e\in V\) corresponds to an individual; the weight \(w_{uw}\) of an edge \((u,v)\in E_{G}\) captures the degree of conflict between individuals \(u\) and \(v\). We call graph \(G\) the _conflict graph_.

In addition to the conflict graph \(G\), we also assume a preference graph \(R\), which is a _bipartite graph_, i.e., \(R=(V,T,E_{R},c)\). The one side of the graph corresponds to individuals \((V)\), the other side to items or tasks \(T\). The edges \((E_{R})\) capture the preferences of individuals to projects. More specifically \(c:V\times T\rightarrow\mathbb{R}_{\geq 0}\) is a _preference_ function, where \(c_{wt}\) captures the satisfaction of individual \(v\in V\) when assigned to task \(t\in T\). Without loss of generality we assume that \(0\leq c_{wt}\leq 1\).

Throughout, we assume that each individual \(v\in V\) is assigned to exactly one task and that each task \(t\in T\) has _capacity_\(p_{t}\), which is task-specific.

**The Team Formation amidst Conflicts problem:** Given the above, our goal is to assign individuals to tasks such that the overall satisfaction of individuals is maximized; the satisfaction of each individual is measured by how much they like the task they are assigned to and the lack of conflicts with the other individuals assigned to the same task. We capture this intuition formally in the form of a (quadratic) program. For this, we define binary variables \(x_{wt}\) such that \(x_{wt}=1\) if individual \(v\) is assigned to task \(t\) and \(x_{wt}=0\) otherwise. Thus, our goal is the following:

\[\max F(\mathbf{x})=\lambda\sum_{v\in V}\sum_{t\in T}c_{wt}x_{wt}+ \sum_{(u,v)\in E_{G}}w_{uw}(1-\sum_{t\in T}x_{wt}x_{wt})\] \[\text{s.t.} \sum_{t\in T}x_{wt}=1,v\in V \tag{2}\] \[\sum_{v\in V}x_{wt}\leq p_{t},t\in T\] (3) \[x_{wt}\in\{0,1\},v\in V,t\in T \tag{1}\]

We call the problem captured by the above program Team Formation amidst Conflicts or TFC for short. The linear term of the objective captures the satisfaction of assigning individuals to tasks and we call it the _task satisfaction term_: \(F_{R}=\sum_{v\in V}\sum_{t\in T}c_{wt}x_{wt}\). The quadratic term captures conflicts in the following sense. The objective increases by \(w_{uw}\) whenever there is conflict between individuals \(u\) and \(v\) and they are assigned to different tasks. We call this term the _social satisfaction term_, i.e., \(F_{G}=\sum_{(u,v)\in E_{G}}w_{uw}(1-\sum_{t\in T}x_{wt}x_{wt})\); this term models Max-k-Cut with given sizes of parts (Chen et al., 2011).

As far as the constraints are concerned: the first constraint enforces that every individual is assigned to exactly one task while the second constraint enforces that we assign at most \(p_{t}\) individuals to task \(t\in T\); \(p_{t}\) is the capacity of tasks. Observe that our problem as represented above is a quadratic program with integer constraints and the objective function \(F\) is non-convex. This observation hints that the problem may be computationally hard. In fact, we have the following result regarding the hardness of TFC:

[MISSING_PAGE_FAIL:3]

**Approximation guarantees:** Following the analysis of Ageev et al. ((2017)) we can show that Pipage is an \(1/2\)-approximation algorithm for the TFC problem. Thus we have:

Theorem 1 ((2017)).: _The_ Pipage _algorithm is an \(\frac{1}{2}\)-approximation algorithm for the TFC problem._

For completeness we present this proof in Appendix A.1

**Running time:** The overall complexity of Pipage consists of the running time of a gradient-ascent algorithm that finds a fractional solution \(\mathbf{y}^{*}\) to the Relaxed-TFC problem plus the running time of the latter is \(O\left((\mathcal{T}_{F}+|V|+|T|)\left|E_{\mathbf{y}^{*}}\right|\right)\), where \(\mathcal{T}_{F}\) is the time required to evaluate the function \(F\) and \(E_{\mathbf{y}^{*}}\) is the number of fractional components of the initial solution \(\mathbf{y}^{*}\). This is because each of the \(E_{\mathbf{y}^{*}}\) steps of pipage rounding requires time \(O\left(\mathcal{T}_{F}+|V|+|T|\right)\) since we run a Depth-First-Search and two evaluations of \(F\).

### Randomized \(\frac{3}{4}\) - approximation algorithm

Here, we present a \(\frac{3}{4}\)-approximation algorithm for TFC. We call this algorithm RPipage, because we use the randomized pipage rounding in order to instantiate the Relax-Round algorithm.

**The \(L_{2}\) concave relaxation:**

\[L_{2}(\mathbf{x})=\lambda\sum_{\mathbf{z}\in V}\sum_{\mathbf{f}\in T}c_{ \mathbf{f}\mathbf{x}\mathbf{f}}-\mathbf{w}(E_{G})+\sum_{(u,v)\in E_{G}}\sum_{ t\in T}w_{uv}\min(1,x_{u}+x_{\mathbf{v}\mathbf{f}}).\]

For \(L_{2}\) we have the following:

Proposition 2.: \(L_{2}\) _satisfies Properties 1 and 2._

The proof of Proposition 2 is given in Appendix A.2.

**Randomized pipage rounding** Here, we briefly present the randomized pipage scheme originally proposed by Gandhi ((2017)). Randomized pipage rounding proceeds in iterations, just like (deterministic) pipage rounding. If \(\mathbf{y}\) is the current fractional solution of the rounding algorithm, we calculate \(\mathbf{y}_{1}\) and \(\mathbf{y}_{2}\) (same as in pipage rounding) and then we probabilistically set \(y^{\prime}\) equal to either \(\mathbf{y}_{1}\) or \(\mathbf{y}_{2}\). For more details we refer the reader to Appendix C.2.

**Approximation guarantees:** In order to prove the \(\frac{3}{4}\)-approximation ratio of RPipage for TFC we need the following Lemma:

Lemma 2 ((2017)).: _If we use \(\Xi\) to denote the randomized pipage algorithm that rounds a fractional solution \(\mathbf{y}\) to an integral solution \(\mathbf{x}\), i.e. \(\Xi(\mathbf{y})=\mathbf{x}\), then \(\Xi\) satisfies the following properties:_

* \(\mathbb{E}_{\Xi}[\mathbf{x}]=\mathbf{y}\)__
* \(\mathbb{E}_{\Xi}[(1-x_{\mathbf{u}\mathbf{f}})(1-x_{\mathbf{u}\mathbf{f}})] \leq(1-y_{\mathbf{u}\mathbf{f}})(1-y_{\mathbf{u}\mathbf{f}})\)_, for all_ \(u,v\in V\) _and_ \(t\in T\)__

The proof of this lemma is due to Chekuri et al. ((2017)), and thus omitted. The most important consequence of Lemma 2 is the following proposition, the proof of which is given in Appendix A.3

Proposition 3.: _Under Assumption 1, for all \(\mathbf{x},\mathbf{y}\) such that \(\mathbf{x}=\Xi(\mathbf{y})\) and \(\Xi\) being the randomized pipage rounding, we have that:_

\[\mathbb{E}_{\Xi}[L(\mathbf{x})]\geq\frac{3}{4}L(\mathbf{y})\]

Now, let \(\mathbf{y}^{*}\) be the optimal fractional solution of the Relaxed-TFC problem with objective \(L_{2}\) and \(\Xi(\mathbf{y}^{*})=\mathbf{x}^{*}\), with \(\Xi\) being the randomized pipage rounding scheme. Also, let \(\mathbf{x}_{\mathsf{int}}\) be the optimal solution of the integral problem TFC. Then, it holds that:

\[F(\mathbf{x}^{*})=L_{2}(\mathbf{x}^{*})\geq\frac{3}{4}L_{2}(\mathbf{y}^{*}) \geq\frac{3}{4}F(\mathbf{x}_{\mathsf{int}}).\]

Thus, we have the following theorem:

Theorem 2.: _Under Assumption 1,_ RPipage _is a \(\frac{3}{4}\)-randomized approximation algorithm for the TFC problem._

**Running time:** The overall complexity of RPipage consists of the running time of a gradient-ascent algorithm that finds a fractional solution \(\mathbf{y}^{*}\) to the Relaxed-TFC problem with objective \(L_{2}\) plus the running time of the randomized pipage rounding scheme, which is \(O((|T|+|V|)|T|V|)\); assuming that the number of tasks \(|T|<|V|\), this becomes \(O(|T||V|^{2})\). In contrast to deterministic pipage rounding, observe that randomized pipage rounding does not require evaluating the objective function. This results in a significant computational speed-up.

**Discussion:** In the future, it would be interesting to examine if _swap rounding_(Brandenburg, 2017), can be used in place of randomized pipage rounding and whether such a scheme can lead to more efficient algorithms. We leave this as an open problem.

### Tuning the hyperparameter \(\lambda\)

In order for Theorem 2 to hold, we need to make the following assumption:

Assumption 1. (Balancing Assumption) _Consider a feasible fractional solution \(\mathbf{y}\). We assume that the following holds_

\[\lambda\sum_{v\in V}\sum_{t\in T}c_{\mathbf{u}\mathbf{f}}-\mathbf{w}(E_{G})\geq 0\]

\[\lambda\geq\frac{\mathbf{w}(E_{G})}{\sum_{u\in V}\sum_{t\in T}c_{\mathbf{u} \mathbf{f}}},\]

_where \(\mathbf{w}(E_{G})=\sum_{(u,v)\in E_{G}}w_{uw}\). If we also assume that \(0\leq c_{\mathbf{u}\mathbf{f}}\leq 1\) for all \(v\in V\) and \(t\in T\), then we have_

\[\lambda\geq\frac{\mathbf{w}(E_{G})}{|V|}=\frac{d_{\mathbf{u}\mathbf{f}}}{2},\]

_where \(d_{\mathbf{u}\mathbf{f}}\) is the average degree of the nodes in the conflict graph \(G\) and we used the fact that \(\sum_{t\in T}s_{\mathbf{u}\mathbf{f}}=1,\forall v\in V\)._

The above assumption provides a way to tune the balancing parameter \(\lambda\). In practice, we do the following: we introduce the _balancing factor_\(\alpha\in\mathbb{R}_{>0}\) and we set \(\lambda\) to be \(\lambda=\alpha\times\frac{d_{\mathbf{u}\mathbf{f}}}{2}\). In practice, we tune \(\alpha\) as follows: for different values of \(\alpha\) we evaluate the task and the social satisfaction terms \(\left(F_{R}^{(\alpha)},F_{G}^{(\alpha)}\right)\). Then, we pick the value of \(\alpha\) that gives the desired balance between the two terms.

## 5. Computational Speedups

We discuss here a few methods we use in order to speedup our algorithms. All heuristics we discuss here can be applied to both Pipage as well as RPipage.

**Converting convex to linear programs:** The algorithms we developed in Section 4 are based on the fact that the Relaxed-TFC problem with objective functions \(L_{1}\) and \(L_{2}\) is a concave problem with linear constraints and it can be solved in polynomial time via an application of gradient ascent. In fact, we show that there is away to rewrite the Relaxed-TFC problems with objectives \(L_{1}\) and \(L_{2}\) as linear programs, by adding some extra variables.

For the Relaxed-TFC problem with objective \(L_{1}\), this can be done as follows: first, we substitute the term \(\min\left(1,\min_{t}\left(2-x_{ut}+x_{ot}\right)\right)\) with the new variable \(x_{uw}\) and the objective becomes:

\[L_{1}(\mathbf{x})=\lambda\sum_{v\in V}\sum_{t\in T}c_{vt}x_{vt}+\sum_{(u,v)\in E _{G}}w_{uw}z_{uw}.\]

Then we also add the constraints \(z_{uw}\leq 1\) and \(z_{uw}\leq 2-x_{ut}-x_{gt}\), \(t\in T\). The full linear program is given in Appendix A.4

The corresponding linearization of the \(L_{2}\) objective can be done as follows: we substitute the term \(\min(1,x_{ut}+x_{ot})\) with a new variable \(x_{uut}\) such that:

\[L_{2}(\mathbf{x})=\lambda\sum_{v\in V}\sum_{t\in T}c_{vt}x_{vt}-w(E_{G})+\sum_{ (u,v)\in E_{G}}\sum_{t\in T}w_{uw}x_{uut}.\]

We also add the constraints \(x_{uut}\leq 1\) and \(x_{uut}\leq x_{ut}+x_{gt}\). The complete linear program is given in Appendix A.5.

The advantage of converting the convex problems into linear is that solving a linear program is much more efficient than solving a convex program with linear constraints. In practice, using the Gurobi solver we obtained speedups up to 500x (see Table 4).

**Sparsification:** When the task capacities are small and the conflict graph is dense, a heuristic, we named Sparsify, that works well in practice is randomly removing conflict edges. That is, we keep each conflict edge with a certain probability \(p\). Otherwise, with probability \((1-p)\) we discard the edge. This greatly reduces the number of terms we need to evaluate \(F_{G}\) in our objective resulting in computational speedups when optimizing the fractional relaxation.

Note that when \(p=1\), we don't alter the objective. As \(p\) decreases, we remove more conflict edges, resulting in computational speedups, although our fractional solution might not be optimal. Selecting \(p\) is problem specific. A rule of thumb is that the denser the conflict graph, the lower we can set \(p\).

**Compact:** Our intuition, but also our real-world datasets (see Section 6.2 and Appendix D.2), reveal that our data have the following pattern: the complement of the conflict graph, i.e., the friend graph, consists of relatively small densely-connected communities with similar task preferences. Intuitively, for two individuals \(u,v\) that belong in the same community and have similar preferences we would expect that the vectors of \(x_{ut}\)'s and \(x_{ot}\)'s will be similar for all \(t\in T\). Taking this to the extreme: individuals \(u,v\) with the same neighbors in the conflict graph \(G_{G}\) and the same preferences for tasks in \(T\) should have identical values \(x_{ut}\), \(x_{vt}\).

Formally, this is captured in the following theorem, which is proved in Appendix A.6:

**Theorem 3**.: _Consider two individuals \(u,v\in V\) which have identical neighbors in \(G_{G}\) (i.e.,\((u,w)\in E_{G}\Leftrightarrow(v,w)\in E_{G}\)) and have identical project preferences (i.e., \(c_{ut}=c_{vt},\forall t\in T\)). Then, there exists an optimal solution \(\mathbf{y}\) of Relaxed-TFC such that \(x_{ut}=x_{ut},\forall t\in T\)._

Motivated by the above theorem we define the Compact algorithm. On a high-level the idea is to compact densely-connected subgraphs into supernodes. Note that the supernodes need not be nodes that have identical neighborhood in \(G\); after all, it may be unreasonable to assume that this will happen in practice. However, using a graph-partitioning algorithm (e.g., spectral clustering (Krishnamurthy et al., 2017), finding dense components (Bartlett and Barthelemy, 2017)) we can partition the original set of nodes into supernodes with similar neighborhoods. Let \(S\) be the set of supernodes, which is a partition of the original set of nodes \(V\). Then, we create a conflict graph between supernodes; the number of conflict edges between two supernodes \(A\) and \(B\) is approximately \(|A|\times|B|\) (almost every node of \(A\) is in conflict with every node of \(B\)). Thus, we set in this new conflict graph we set the weight of edge \((A,B)\) to be \(w_{AB}=|A|\times|B|\) (assuming that each edge of the original graph has unit weight). The next step is to solve the following _compact_ Relaxed-TFC problem:

\[\max L(\mathbf{x})\] (10) s.t. \[\sum_{t\in T}x_{gt}=1,v\in S \tag{11}\] \[\sum_{v\in S}|v|x_{vt}\leq p_{t},t\in T\] (12) \[0\leq x_{gt}\leq 1,\forall v\in S,\forall t\in T. \tag{13}\]

where we use \(|v|\) to denote the number of simple nodes in the supernode \(v\).

Then, we unroll the solution to obtain a fractional solution for the original graph. That is, for each \(v\in V\) we set \(x_{vt}=x_{St}\), where \(S\) is the supernode \(v\) belongs to. Finally, we round the fractional solution to obtain an integral solution. Depending on whether we use \(L_{1}\) or \(L_{2}\) as our objective, we then round the fractional solution using Pipage or RPipage respectively.

## 6. Experiments

In this section, we evaluate our framework using both real-world as well as synthetic datasets. The experiments prove the effectiveness and efficiency of our algorithms as well as the versatility of our model to encompass many different real-world scenarios involving assignment problems with conflicts.

### Baselines and Setup

**Baselines:** For our experiments we use the following baselines:

QuadraticThis is the optimal algorithm, i.e., the algorithm that solves the original TFC problem as expressed in Equations (1)-(4). We use an off-the-shelf solver to implement Quadratic, but even though this is a powerful solver, we can run Quadratic only for small datasets since the solver is asked to optimize a non-convex quadratic function subject to integral constraints.

GreedyThe Greedy algorithm sequentially assigns an individual to the best team (i.e., the team that maximizes the objective function \(F\)) given that the constraints are satisfied. The algorithm terminates when all individuals are assigned. We refer the reader to Appendix B for a detailed analysis of Greedy.

Random: This is an algorithm that randomly assigns each individual to a team until all individuals are assigned.

Manual: This is the _manual_ assignment of individuals to tasks as made by a human expert, which is available only in some datasets.

**Experimental setup:** Our experimental setup is descried in Appendix D.1. Unless otherwise stated we use the "linearization" speedup presented in Section 5.

### Datasets

For our experiments, we use three different types of data: (a) data of preferences of students with respect to projects and collaborators in educational settings, (b) data from the Bureau of Labor statistics that concern employees and their assignment to company departments and, finally, (c) a synthetic dataset that we use to test the scalability of our algorithms and the speedups described in Section 5. We describe these datasets below. A summary of the characteristics of each dataset is shown in Table 1.

**Education data:** This is data coming from courses in a US institution 2. In the classes we considered, there were a number of projects (with fixed capacities) available to students and each student filled in a form with their project preferences (each student ranked the projects from best to worst) and their preferences with respect to other students they want to work with in the same project. We have data from four such classes: _Class-A_, _Class-B_, _Class-C_ and _Class-D_. These datasets do not contain a conflict graph between students, but instead a _friend_ graph (indicative such graphs are shown in Appendix D.2), i.e. two students that have an edge between them are friends and want to work together. We define the conflict graph as the complement of the friend graph. We assign unit weight to each edge of the conflict graph.

Footnote 2: IRB exception was obtained in order to use an anonymized version of the data.

Next, we have to construct the preference graph by assigning weight \(c_{u,p}\) for each student \(u\) and project \(t\). Let \(rank_{u}(t)\in[|T|]\) be the rank of project \(t\) in student's \(u\) preference list (1 is the best, \(|T|\) is the worst). We considered the following functions:

* inverse (_Inversely_: \(c_{u,t}=\frac{1}{rank_{u}(t)}\)
* linear-normalized (_LinNorm_): \(c_{u,t}=\frac{|T|-rank_{u}(t)+1}{|T|}\)

**Employee data:** Based on statistics from the _U.S. Bureau of Labor Statistics_3 for 2022, we built a dataset of employees in an company. Specifically, we created a company with 4000 employees and four departments: IT, Sales, HR, PR. Each department has 1000 employees. IT and Sales departments are male dominated while HR and PR are female dominated. The distribution of males and females in each department are according to the data in the _Management occupations_ section of the _U.S. Bureau of Labor Statistics_. In our experiments, we add conflict edges between all male employees, since our objective is to distribute the male employees more evenly. Equivalently, we could have added conflict edges between all females. Generally, depending on the diversity goal, one can add conflicts judiciously to guide the diversification process. For employee preferences, we set \(c_{u,t}=1\), if \(t\) is the original department of \(u\), otherwise \(c_{u,t}=0\). We assume that with probability 1% an employee is suitable for switching to a new department. For those employees we set \(c_{u,t}=1\), where \(t\) is the department to which employee \(u\) may switch.

Footnote 3: lblg.gov/cps/cpsatt11.htm

**Synthetic data:** We also created a synthetic dataset (_Synth-TF_) to test the speedups we discussed in Section 5. For this we created \(|V|=1000\) individuals. The conflict graph is defined as the complement of the following friend graph: the friend graph is a planted partition graph where each partition has 100 nodes connected with probability 0.99. Edges across partitions are added with probability \(10^{-5}\). For each partition we select a primary project \(t\) and set \(c_{u,t}=1\) for all nodes \(v\) in the partition. Next, for each node \(v\) we choose uniformly at random a project \(t^{\prime}\) and set \(c_{u,t^{\prime}}=1\).

### Forming teams in education settings

In this section, we evaluate the quantitative and qualitative performance of our algorithms using the education datasets.

**Quantitative performance of our algorithms:** We evaluate the qualitative performance of our algorithms using the _approximation ratio_ AR, i.e. the ratio of the objective function evaluated at the solution and the optimal value.

Figure 1 shows the approximation ratios achieved by our algorithms and the different baselines. For all datasets we used the _LinNorm_ project preference function. The results for the _Inverse_ preference function are presented in Appendix D.3). The results demonstrate that our algorithms have approximation ratio very close to 1.

Interestingly, and despite the fact that in Appendix B we show that the worst-case approximation ratio of Greedy is unbounded, in practice Greedy has AR score very close to 1. We conjecture that this is true due to the correlation between students' friendships and preferences. In fact, we believe that one can bound the approximation ratio of Greedy under such correlation patterns; we leave this as future work.

Somewhat surprisingly, the performance of Random is quite good, although not comparable to the other algorithms. This can be explained by the high density of the conflict graph (or the sparseness of the friend graph) and the fact that the capacities of projects are small relative to the number of individuals. Since the conflict graph is almost a complete graph, small random teams inevitably have large number of conflict edges between them. That said, our analysis demonstrates that the solution given by Random has poor qualitative characteristics.

Figure 1. Education data; approximation ratio of the different algorithms. For all datasets we used the _Inverse_ project-preference function and \(\alpha=10\).

For these experiments we used \(\alpha=10\) for all datasets, since our primary goal was to assign students to projects they like. Assigning students with their friends was a secondary objective according to the course instructors. We chose the value of \(\alpha\) following the grid-search procedure of Section 4.3; the details are given in Appendix D.4. Note that by our selection of \(\alpha\), the balancing assumption holds and thus the approximation guarantees of RPipage hold.

**Qualitative performance of our algorithms:** In applications such as the assignment of students to teams, what matters is not just the value of the objective function, but the per-student satisfaction. In this section, we analyze the solutions provided by our algorithms and our baselines and compare those with the _manual_ solution provided by a domain expert who tries to find an empirically good assignment based on the input data.

In order to evaluate the quality of the results, we compute the following metrics. Given a solution \(\mathbf{x}\) to our problem we define \(r(\mathbf{o},\mathbf{x})\) to be the rank\({}_{\mathcal{G}}(t)\), where \(x_{\mathcal{G}}=1\), i.e. \(r(\mathbf{o},\mathbf{x})\) is the rank - according to \(v\) - of the task to which \(v\) was assigned. Then, we define the \(\mathcal{M}\)-preference metric to be:

\[\mathcal{M}Q_{R}(\mathbf{x})=\mathcal{M}(r(\mathbf{o},\mathbf{x})\mid\mathbf{ o}\in\mathcal{V}).\]

In the above equation, \(\mathcal{M}\) can be substituted by max or \(avg\) and the preference metric corresponds to the maximum and average ranking of the projects assigned to students; note that the minimum is not used as it is identical across algorithms and does not provide any insight. Intuitively, the lower the value of \(\mathcal{M}Q_{R}(\mathbf{x}_{\mathcal{G}})\) for a solution provided by an algorithm \(\mathcal{H}\), the better the algorithm.

Similarly, we define \(\mathcal{M}Q_{G}(\mathbf{x})\) to be the max or \(avg\) number of friends (non-conflicts) assigned to students in \(v\). In this case, the larger the value \(\mathcal{M}Q_{G}(\mathbf{x}_{\mathcal{H}})\) for a solution provided by an algorithm \(\mathcal{H}\), the better the algorithm.

Table 2 shows that students got a better project on average when using the Quadratic,Pipage and RPipage algorithms compared to the Manual assignment. Table 5 shows that students got the same (or almost the same) average number of friends using the Quadratic, Pipage and RPipage algorithms as in the manual assignment.

### Forming teams of employees

In this section, we use the _Company_ dataset in order to evaluate our algorithms' ability to form diverse teams of employees.

**Quantitative performance of our algorithms:** Figure 2 shows the approximation ratios of our algorithms and baselines for the _Company_ dataset and for \(\alpha=1,2,3,4\). Somewhat surprisingly, for \(\alpha=1\) our algorithms have a approximation ratio close to \(0.8\), while Greedy is almost optimal. Random has significantly lower approximation ratio than the other algorithms; most of the times less than \(0.5\). In general, all other algorithms have approximation ratio close to \(1\). It is interesting to observe that in this dataset Greedy performs almost optimally for all choices of \(\alpha\); despite the fact that its worst case approximation factor is unbounded (see Appendix B).

As we discussed before, we believe that the reason for this is the correlation between conflicts and task preferences.

Figure 3. Employee data; Diversity per department before (1\({}^{\mathbf{st}}\) row) and after (2\({}^{\mathbf{nd}}\) row) we run the quadratic algorithm (\(\alpha=2\)), \(8\%\) of the employees changed department. The average male-female percentage gap decreased from \(35\%\) to \(26\%\).

Figure 2. Employee data; approximation ratio of the different algorithms for \(\alpha=1,2,3,4\).

**Qualitative performance of our algorithms:** In this dataset, we evaluate the qualitative performance of our framework by showing how the optimal solution to our problem (obtained by Quadratic), affected the diversification of the teams. Figure 3 demonstrates exactly this. While only 8% of the employees changed department, the average male-female percentage gap decreased from 35% to 26%. Varying the value of \(\alpha\) we can control this balance. Specifically, if we decrease the value of \(\alpha\) the number of people who change department increases, while the average male-female gap decreases. Hyperparameter tuning for the _Company_ dataset is further discussed in Appendix D.7.

### Evaluating the speedup techniques

In this section, we use the _Synth-TF_ dataset to demonstrate the speedups obtained by the different techniques we discussed in Section 5. As we discussed, solving the TFC problem as described in Equations (5)-(8) using a convex solver does not scale up. Thus, we apply to this original problem sparsification and then run the convex solver; we call this algorithm Sparsity-Concave. For the Sparsity algorithm we used \(p=0.01\), i.e., we kept only 1% of the edges of the conflict graph. Alternatively, we transform the problem into a problem with a linear objective by adding auxiliary variables and constraints (as discussed in Section 5) and run the RPipage algorithm. We call this algorithm Linear. Another algorithm we use is Sparsity-Linear, which combines sparsification and linearization. Finally, we also combine Compact with Linear to obtain the Compact-Linear algorithm. Note that for the implementation of Compact we use the spectral clustering algorithm ((21)) available in scikit-learn 4.

Footnote 4: scikit-learn.org/stable/modules/generated/sklearn.chuster.SpectraKlustering.html

Figure 4 shows the approximation ratios of the above heuristics and Table 4 the running times of the same heuristics on the _Synth-TF_ dataset. For this experiment, we set the value of \(\alpha=10\).

Although all algorithms perform almost optimally, speed-ups vary. First, trying to directly optimize the Concave relaxation results in a time-out of our solver. Using Sparsity before optimizing the Concave relaxation renders the problem solvable in 1000 seconds. Using Linear algorithm yields an extra 3x speedup. Finally, combining Linear with either Sparsity or Compact further reduces the running time down to \(2-3\) seconds which is a 100x speedup. In total, we managed to reduce the time from 1095 seconds using Sparsity-Concave to \(2-3\) seconds combining Linear with one of Sparsity or Compact.

## 7. Conclusions

Motivated by the need to form teams of students in large project-based classes we defined the TFC problem and showed that (a) it is NP-hard and that (b) it is closely related to Max-\(k\)-Cut with given part sizes (Bartlett and Schober, 2009). For TFC, we designed a new efficient randomized approximation algorithm and practical methods for speeding it up. We applied our algorithms to real-world datasets and demonstrated their efficacy across different dimensions.

In the future, we want to further explore possible speedups for our algorithm and also formally investigate the extremely good performance of greedy in practice - despite its unbounded worst-case approximation ratio.

\begin{table}
\begin{tabular}{c c c} \hline \hline Algorithm & Time (seconds) & \\ \hline Concave & time out & \\ Sparsity-Concave & 1095 & \\ Linear & 342 & \\ Sparsity-Linear & 2 & \\ Compact-Linear & 3 & \\ \hline \hline \end{tabular}
\end{table}
Table 4. Running time (seconds) for the speed-ups

Figure 4. _Synth-TF_ dataset; approximation ratios of the speedups; We used \(\alpha=10\).

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Dataset & Algorithm & max\(Q_{G}\) & avg\(Q_{G}\) & _std_ \\ \hline \multirow{4}{*}{\begin{tabular}{} \end{tabular} } & Quadratic & 4 & 0.65 & 0.78 \\  & Pipage & 2 & 0.1 & 0.34 \\  & RPipage & 1 & 0.08 & 0.26 \\  & Greedy & 3 & 0.65 & 0.74 \\  & Random & 1 & 0.4 & 0.49 \\  & Manual & 3 & 0.63 & 0.63 \\ \hline \multirow{4}{*}{\begin{tabular}{} \end{tabular} } & Quadratic & 2 & 0.64 & 0.77 \\  & Pipage & 2 & 0.64 & 0.77 \\  & RPipage & 2 & 0.57 & 0.78 \\  & Greedy & 3 & 0.79 & 1.08 \\  & Random & 2 & 1.5 & 0.87 \\  & Manual & 2 & 0.64 & 0.67 \\ \hline \multirow{4}{*}{\begin{tabular}{} \end{tabular} } & Quadratic & 2 & 0.54 & 0.57 \\  & Pipage & 1 & 0.38 & 0.49 \\  & RPipage & 1 & 0.38 & 0.49 \\  & Greedy & 2 & 0.69 & 0.67 \\  & Random & 1 & 0.54 & 0.5 \\  & Manual & 2 & 0.69 & 0.54 \\ \hline \multirow{4}{*}{
\begin{tabular}{} \end{tabular} } & Quadratic & 3 & 0.43 & 0.72 \\  & Pipage & 1 & 0.29 & 0.45 \\  & RPipage & 1 & 0.23 & 0.42 \\  & Greedy & 2 & 0.38 & 0.54 \\  & Random & 0 & 0.0 & 0.0 \\ \hline \hline \end{tabular}
\end{table}
Table 3. Education data; \(\mathcal{M}Q_{G}(x_{\mathcal{A}})\) for \(\mathcal{M}=\{\textit{max},\textit{avg}\}\) of number of friends per student and \(\mathcal{A}\) being all algorithms; we also report _std_ - the standard deviation for \(\textit{avg}\)\(Q_{G}\); _Inverse_ project preference function and \(\alpha=10\).

## References

* (1)
* Agger and Nsridenko (2004) Alexander A Agger and Maxim I. Vriddenko. 2004. Figure rounding: A new method of constructing algorithms with proven performance guarantee. _Journal of Combinatorial Optimization_ (2004), 307-328.
* Angamoropoulos et al. (2010) Aris Angamoropoulos, Luca Recht, Carlos Castillo, Aristides Gionis, and Stefan Lozenid. 2010. Power in tiny-forming teams in large-scale community systems. In _Proceedings of the 19th ACM international conference on Information and knowledge management_, pages 509-608.
* Angamoropoulos et al. (2018) Aris Angamoropoulos, Carlos Castillo, Adriano Fazzone, Stefano Leonardi, and Evimaria Terzi. 2018. Algorithms for hiring and outsourcing in the online labor market. In _Proceedings of the 2018 ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_, 1109-1118.
* Andrews et al. (2011) Matthew Andrews, Mohammad Highajaghy, Howard Kauff, and Ankur Mehta. 2011. Coordinated metric labeling. In _Proceedings of the Twenty-Second Annual ACM-SIAM Symposium on Discrete Algorithms_. SIAM, 976-995.
* Boyd and Vandenberghe (2004) Stephen Boyd and Lieven Vandenberghe. 2004. Convex optimization: Cambridge university press.
* Chatzikar (2000) Moses Chatzikar. 2000. Greedy approximation algorithms for finding dense components in a graph. In _International workshop on approximation algorithms for combinatorial optimization_. Springer, 84-95.
* Chekuri et al. (2010) Chandra Chekuri, Jan Vondrak, and Rice Zenkusen. 2010. Dependent randomized rounding via exchange properties of combinatorial structures. In _2010 IEEE SIAM Annual Symposium on Foundations of Computer Science_. IEEE, 575-584.
* Gale and Shapley (1982) David Gale and Lloyd S Shapley. 1982. College admissions and the stability of marriage. _The American Mathematical Monthly_ 61, 10 (1982), 9-15.
* Gandhi et al. (2002) Rajar Gandhi, Samir Khuller, Srinivasan Parthasarathy, and Arvind Srinivasan. 2002. Dependent rounding in bipartite graphs. In _The 3rd Annual IEEE Symposium on Foundations of Computer Science_. IEEE, 2002. Proceedings. IEEE, 323-332.
* Coemans and Williamson (1994) Michel M Coemans and David W Williamson. 1994. New 3-approximation algorithms for the maximum satisfiability problem. _SIAM Journal on Discrete Mathematics_ 7, 4 (1994), 656-666.
* Canfield and Irving (1989) Dan Canfield and Robert W Irving. 1989. _The stable marriage problem: structure and algorithmic_. MIT press.
* Karpat et al. (2013) Mehdi Karpat, Mottae Zhhyat, and Ajim An. 2013. Finding affordable and collaborative teams from a network of experts. In _Proceedings of the 2013 SIAM international conference on data mining_. SIAM, 587-595.
* Karpat (2010) Richard M Karpat. 2010. _Reducibility among combinatorial problems_. Springer.
* Kleinberg and Tardos (2002) Jon Kleinberg and Tardos Tardos. 2002. Approximation algorithms for classification problems with pairwise relationships: Metric labeling and Markov random fields. _Journal of the ACM (JACM)_ 49, 5 (2002), 416-439.
* Kuhn (1955) Harold W Kuhn. 1955. The Hungarian method for the assignment problem. _Naval research logistics quarterly_ 1, 2 (1953), 83-121.
* Lappas et al. (2009) Theodore Lappas, Kun Lin, and Evimaria Terzi. 2009. Finding a team of experts in social networks. In _Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining_, 467-476.
* Majumder et al. (2012) Anirin Majumder, Samik Datta, and KVM Naidu. 2012. Capricuted team formation problem on social networks. In _Proceedings of the 18th ACM SIGKDD international conference on knowledge discovery and data mining_, 1005-1013.
* McVitie and Wilson (1971) David C McVitie and Leslie E Wilson. 1971. The stable marriage problem. _Commun. ACM_ 14, 7 (1971), 486-490.
* Negreiros and Palmao (2006) Marcon Negreiros and Augusto Palmao. 2006. The capacitated context clustering problem. _Computers & operations research_ 33, 6 (2006), 1639-1663.
* Vondrakiere and Zemini (2003) Karam Vondrakiere and Florini T Zemini. 2003. Balancing Task Coverage and Expert Workload: Team Formulation. In _Proceedings of the 2003 SIAM International Conference on Data Mining_ (SDM), 640-648.
* Van Luburg (2007) Ulrike Von Luburg. 2007. A tutorial on spectral clustering. _Statistics and computing_ 17 (2007), 395-416.
* Zhu et al. (2010) Sunthui Zhu, Dingding Wang, and Tao Li. 2010. Data clustering with size constraints. _Knowledge-Based Systems_ 23, 8 (2010), 883-889.

## Appendix A Proofs

### Proof of Theorem 1

We begin with the following lemma:

**Lemma 3**.: _Let \(\mathbf{y}\) be a (fractional) feasible solution to the TTC-1 problem. Then, for every \(u,v\in V\) we have:_

\[1-\sum_{t\in T}y_{uu}y_{qt}\geq\frac{1}{2}\min\left(1,\min_{t\in T}(2-y_{uu}-y_ {qt})\right)\]

Proof.: Let \(z_{uu}=\min\left(1,\min_{t}(2-y_{uu}-y_{qt})\right)\). Define

\[t^{\prime}=\arg\max_{t\in T}(y_{qt}+y_{qt})\]

and

\[q_{uu}=y_{u\prime}+y_{qt^{\prime}}. \tag{14}\]

Then,

\[z_{uu}=\min(1,2-q_{uu}). \tag{15}\]

Also, it holds that

\[\sum_{t\in T}y_{ut}+y_{qt}=\sum_{t\in T}y_{ut}+\sum_{t\in T}y_{qt}=1+1=2 \tag{16}\]

as well as the arithmetic-geometric mean inequality which says that for any two positive numbers \(a,b\):

\[\left(\frac{a+b}{2}\right)^{2}\geq ab. \tag{17}\]

Thus, we have:

\[1-\sum_{t\in T}y_{ut}y_{qt} =1-y_{ut^{\prime}}y_{qt^{\prime}}-\sum_{t\in T}y_{ut}y_{qt}\] \[\geq 1-\left(\frac{y_{ut^{\prime}}+y_{qt^{\prime}}}{2}\right)^{2}- \sum_{t\neq t^{\prime}}\left(\frac{y_{ut}+y_{qt}}{2}\right)^{2}\] (using 17) \[\geq 1-\left(\frac{y_{ut}}{2}\right)^{2}-\left(\frac{x_{t\neq t^{ \prime}}y_{ut}+y_{qt}}{2}\right)^{2}\] ((14) and convexity) \[=1-\left(\frac{y_{ut}}{2}\right)^{2}-\left(\frac{2-(y_{ut^{\prime}}+y_ {qt^{\prime}})}{2}\right)^{2}\] (using (16)) \[=1-\left(\frac{y_{ut}}{2}\right)^{2}-\left(1-\frac{q_{uu}}{2} \right)^{2}\] (using (14)) \[=q_{uu}-\frac{q_{uu}^{2}}{2}.\]

_Case 1. \(1\leq q_{uu}\leq 2\). Then, by Eq. (15), \(z_{uu}=2-q_{uu}\), and

\[1-\sum_{t\in T}y_{ut}y_{qt}\geq q_{uu}-\frac{q_{uu}^{2}}{2}\geq\frac{1}{2}z_{uu}. \tag{18}\]

_Case 2. \(0\leq q_{uu}\leq 1\). Then, by Eq. (15), \(z_{uu}=1\). By the assumption of this case, for every \(t\) it holds that_

\[0\leq y_{ut}+y_{qt}\leq 1. \tag{19}\]

Using the arithmetic-geometric mean inequality (Eq. (17)), we have

\[1-\sum_{t\in T}y_{ut}y_{qt} \geq 1-\sum_{t\in T}\left(\frac{y_{ut}+y_{qt}}{2}\right)^{2}\] \[=1-\frac{1}{t\in T}(y_{ut}+y_{qt})^{2}\] \[\geq 1-\frac{1}{t}\sum_{t\in T}(y_{ut}+y_{qt})\] (using (16)) \[=1-\frac{1}{4}2\] \[=\frac{1}{2}z_{uu}\] \[\square\]

A corollary of Lemma 3 is the following:

**Corollary 1**.: _If \(\mathbf{y}\) is a (fractional) feasible solution to problem Relaxed-TFC with objective \(L_{1}\), then_

\[F(\mathbf{y})\geq\frac{1}{2}L_{1}(\mathbf{y}).\]Let \(F^{*}\) be the value of the optimal (integral) solution to TFC. If \(y^{*}\) is the optimal (fractional) solution to Relaxed-TFC with objective \(L_{1}\), it holds that \(L_{1}(y^{*})\geq F^{*}\). From Corollary 1 we have that \(F(y^{*})\geq\frac{1}{2}L_{1}(y^{*})\). Using pipage rounding we can round the fractional solution \(y^{*}\) to an integral solution \(\bar{\kappa}\) such that \(F(\bar{\kappa})\geq F(y^{*})\). Thus,

\[F(\bar{\kappa})\geq F(y^{*})\geq\frac{1}{2}L_{1}(y^{*})\geq\frac{1}{2}F^{*}.\]

### Proof or Proposition 2

**Property 1** To prove concavity it suffices to see that \(L_{2}(\mathbf{x})\) is the sum of concave functions. Note that \(\min(1,x_{ut}+x_{ot})\) is concave.

**Property 2**: For the proof of Property 2, we need the following lemmas:

Lemma 4.: _For all \(x,y\in\{0,1\}\) it holds that_

\[1-(1-x)(1-y)=x+y-xy=\min(1,x+y).\]

The proof of this lemma consists of checking equality for all combinations of integral values of \(x\) and \(y\).

Lemma 5 ((10)).: _For all \(x,y\in[0,1]\) it holds that_

\[1-(1-x)(1-y)\geq\frac{3}{4}\min(1,x+y).\]

Now we are ready to prove the following:

Proposition 4.: _For all integral \(\mathbf{x}\) it holds that \(F(\mathbf{x})=L_{2}(\mathbf{x})\)._

Proof.: It holds that

\[\sum_{(u,v)\in E}w_{uu}(1-\sum_{t\in T}x_{ut}x_{ot})=\] \[w(E)-\sum_{(u,v)\in E}\sum_{t\in T}w_{uu}x_{ut}x_{ot}=\] \[w(E)+\sum_{(u,v)\in E}\sum_{t\in T}w_{uu}\min(1,x_{ut}+x_{ot})\] \[-\sum_{(u,v)\in E}\sum_{t\in T}w_{uu}(x_{ut}+x_{ot})=\] \[w(E)+\sum_{(u,v)\in E}\sum_{t\in T}w_{uu}\min(1,x_{ut}+x_{ot})-2 w(E)=\] \[\sum_{(u,v)\in E}\sum_{t\in T}w_{uu}\min(1,x_{ut}+x_{ot})-w(E),\]

where in the second equality we used lemma 4.

Using the above we have that

\[F(\mathbf{x}) =\lambda\sum_{v\in V}\sum_{t\in T}c_{vt}x_{ot}+\sum_{(u,v)\in E}w _{uuv}(1-\sum_{t\in T}x_{ut}x_{ot})\] \[=\sum_{(u,v)\in E}\sum_{t\in T}w_{uu}\min(1,x_{ut}+x_{ot})+\lambda \sum_{v\in V}\sum_{t\in T}c_{vt}x_{ot}-w(E)\] \[=L_{2}(\mathbf{x}).\]

### Proof of Proposition 3

Proof.: Let \(y\) be an optimal solution of the fractional relaxation of TFC such that it does not hold that \(y_{ut}=y_{ot},\forall t\in T\). Since \(\sum_{t}y_{ut}=\sum_{t}y_{ot}=1\), there exist \(t_{1},t_{2}\in T\) such that \(y_{ut_{1}}\neq y_{ot}\).

and \(y_{ut_{1}}\neq y_{ut_{2}}\). Let \(\bar{\mathbf{y}}\) be the "symmetrical" solution where we have replaced \(u\) with \(v\) and vice versa. That is,

* \(\bar{y}_{ut}=y_{ot}\), \(t\in T\)
* \(\bar{y}_{ut}=y_{ut}\), \(t\in T\)
* \(\bar{y}_{ut^{\prime}}=y_{ut^{\prime}}\), \(t\in T,(u,v^{\prime})\in E\)
* \(\bar{y}_{ut^{\prime}}=y_{ut^{\prime}}\), \(t\in T,(u,v^{\prime})\in E\)
* \(\bar{y}_{ut^{\prime}}=y_{ut^{\prime}}\), \(t\in T,(u,v^{\prime})\in E\)

For the last two equalities note that since nodes \(u\) and \(v\) are _symmetrical_ it holds that \((u,v^{\prime})\in E\Leftrightarrow(u,v^{\prime})\in E\). Then, \(\mathbf{x}=(y+\bar{\mathbf{y}})/2\) is an optimal feasible solution where \(x_{ut}=x_{ot},\forall t\in T\). To see that note that the constraints are linear and thus any convex combination of feasible solutions is also feasible. Moreover, the objective function is linear and thus any convex combination of optimal solutions is also optimal. Finally, concluding our proof observe that

\[x_{ut_{1}}=x_{ot_{1}}=\frac{y_{ut_{1}}+y_{ot_{1}}}{2}\]

and

\[x_{ut_{2}}=x_{ot_{2}}=\frac{y_{ut_{2}}+y_{ot_{2}}}{2}\]

## Appendix B Analysis of the Greedy Algorithm

**Unbounded approximation ratio of Greedy:** We have the following result in terms of the performance of greedy with respect to our objective function:

**Proposition 5**.: Greedy _has unbounded approximation ratio._

Proof.: Consider the following instance of our problem. \(V=\{u,v,z\}\), with task preferences \(c_{ut_{1}}=1-\epsilon\), \(c_{ut_{2}}=\epsilon\) and all other preferences equal to \(0\). \(T=\{t_{1},t_{2}\}\), with capacities \(p_{t_{1}}=1\) and \(p_{t_{2}}=2\). The conflict graph consists of the edge \((v,z)\) with weight \(w_{\text{infer}}=W\). For the objective function assume that \(\lambda=1\). Running the greedy yields the assignment: \(u\) assigned to \(t_{1}\) and \(v,z\) are assigned to \(t_{2}\). The optimal assignment is \(z\) assigned to \(t_{1}\) and \(u,v\) to \(t_{2}\). The approximation ratio is:

\[\text{AR}=\frac{(1-\epsilon)+\epsilon}{W+\epsilon}\leq\frac{1}{W}.\]

As \(W\rightarrow\infty\), the approximation ratio \(\text{AR}\to 0\). 

**Running time of Greedy:** The complexity of Greedy is \(O(|V|^{2}|T|\overline{f}_{F})\), where \(\overline{f}_{F}\) is the cost of calculating \(F\). We have \(|V|\) iterations in total, since at each iteration we assign one individual to a team. Each iteration costs \(|V||T|\overline{f}_{F}\), since we calculate the change in the objective function when considering the addition of each remaining individual to each team which is not full.

## Appendix C Dependent Rounding Schemes

### Pipage rounding

In this section we give a description of the pipage rounding algorithm. For a more detailed analysis we refer the reader to the original paper (Brandt et al., 2017). Pipage rounding is an iterative algorithm; at each iteration the current fractional solution \(\mathbf{y}\) is transformed into a new solution \(\mathbf{y}^{\prime}\) with smaller number of non-integral components. Throughout, we will assume that any solution \(\mathbf{y}\) is associated with the bipartite graph \(H_{\mathbf{y}}=(V,T,E_{\mathbf{y}})\), where the nodes on the one side correspond to individuals, the nodes on the other side to tasks and there is an edge \(e(o,t)\) for every pair \((o,t)\) with \(v\in V\) and \(t\in T\) if and only if \(y_{ut}\in(0,1)\), i.e., \(y_{ot}\) is fractional.

Let \(\mathbf{y}\) be a current solution satisfying the constraints of the program and \(H_{\mathbf{y}}\) the corresponding bipartite graph. If \(H_{\mathbf{y}}\) contains cycles, then set \(C\) to be this cycle. Otherwise, set \(C\) to be a path whose endpoints have degree \(1\). Since \(H_{\mathbf{y}}\) is bipartite, in both bases \(C\) may be uniquely expressed as the union of two matchings \(M_{1}\) and \(M_{2}\). Given this, define a new solution \(\mathbf{y}(e,C)\) as follows:

* if \(e\in E_{\mathbf{y}}\setminus C\), then \(y_{e}(e,C)=y_{e}\).
* Otherwise, \(y_{e}(e,C)=y_{e}+\epsilon,e\in M_{1}\) and \(y_{e}(e,C)=y_{e}-\epsilon,e\in M_{2}\).

For the above, set

\[\epsilon_{1}=\min\{e>0:(\exists e\in M_{1}:y_{e}+\epsilon=1)\vee(\exists e\in M _{2}:y_{e}-\epsilon=0)\}\]

and

\[e_{2}=\min\{e>0:(\exists e\in M_{1}:y_{e}-\epsilon=0)\vee(\exists e\in M_{2}:y _{e}+\epsilon=1)\}.\]

Let \(\mathbf{y}_{1}=\mathbf{y}(-\epsilon_{1},C)\) and \(\mathbf{y}_{2}=\mathbf{y}(e_{2},C)\). Set \(\mathbf{y}^{\prime}=\mathbf{y}_{1}\), if \(F(\mathbf{y}_{1})>F(\mathbf{y}_{2})\), and \(\mathbf{y}^{\prime}=\mathbf{y}_{2}\) otherwise. Note that \(\mathbf{y}^{\prime}\) has smaller number of fractional components than \(\mathbf{y}\) and, thus, Pipage terminates after at most \(|E_{\mathbf{y}^{\prime}}|\) iterations, i.e., as many as the number of fractional values in the \(\mathbf{y}^{\prime}\) vector output by the optimization algorithm. The following theorem states that \(\mathbf{y}^{\prime}\) satisfies the following constraints:

**Theorem 4** (Brandt et al., 2017).: _Consider performing pipage rounding starting from the fractional solution \(\mathbf{y}\). Let \(\mathbf{x}\) be the integral solution produced when Pipage terminates. Then,_

\[\lfloor\sum_{e(o,t)\in\mathcal{E}(v)}y_{ot}\rfloor\leq\sum_{e(o,t)\in\mathcal{E }(v)}x_{ot}\leq\lfloor\sum_{e(u,t)\in\mathcal{E}(v)}y_{ot}\rfloor+1,\]

_where for every \(v\in V\)\(\mathcal{E}(v)\) is the set of edges in the preference graph \(R\), that are incident to \(v\)._

Since all \(p_{t}\)'s are integers (see Eq. 3), the above theorem implies that \(\mathbf{x}\) is a feasible solution.

### Randomized pipage rounding

Here, we briefly present the randomized pipage scheme originally proposed by Gandhi (Gandhi, 2017) adapted to our problem. Randomized pipage rounding proceeds in iterations, just like (deterministic) pipage rounding. If \(\mathbf{y}\) is the fractional solution at the current iteration of the rounding algorithm, we update \(\mathbf{y}\) as follows:

If \(\epsilon\in E_{\mathbf{y}}\setminus C\), then \(y_{e}(e,C)=y_{e}\). If \(\epsilon\in C\), then \(\mathbf{y}^{\prime}=\mathbf{y}_{1}\), with probability \(\epsilon_{2}/(\epsilon_{1}+\epsilon_{2})\). Otherwise, with probability \(\epsilon_{1}/(\epsilon_{1}+\epsilon_{2})\), \(\mathbf{y}^{\prime}=\mathbf{y}_{2}\). Note \(C\), \(\epsilon_{1}\), \(\epsilon_{2}\), \(\mathbf{y}_{1}\) and \(\mathbf{y}_{2}\) are the same as the ones defined in the description of pipage rounding. As the number of fractional elements of \(\mathbf{y}\) decrease in every iteration, randomized pipage rounding terminates after at most \(O(|E_{\mathbf{y}^{\prime}}|)\) iterations, where \(\mathbf{y}^{*}\) is the solution to the Relaxed-TFC problem with objective \(L_{1}\).

## Appendix D Experiments

### Experimental setup

All of the experiments were run on a machine with an Intel(R) Xeon(R) Gold 6242 CPU @ 2.80GHz and 16GB memory. All of our code is written in Python 3.6.8. For linear and quadratic optimization we used Gurobi 5. For optimizing concave functions we used 

[MISSING_PAGE_FAIL:12]

### Qualitative results for education datasets using the _LinNorm_ project preference function

Tables 6 and 7 contain the qualitative results for the _LinNorm_ project preference function. Our algorithms are again comparable or better

Figure 8. Friend graph colored using the optimal team assignment (Quadratic) for _Class-B_. On top of nodes are the anonymized student ids.

Figure 10. Education data; hyperparameter \(\alpha\) tuning. We used the _LinNorm_ project preference function.

Figure 9. Education data; approximation ratios for education datasets. For all datasets we used the _LinNorm_ project preference function and \(\alpha=10\).

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Dataset & Algorithm & max\(Q_{G}\) & avg\(Q_{G}\) & \(std\) \\ \hline \multirow{6}{*}{Supervised} & Quadratic & 4 & 0.65 & 0.78 \\  & Pipage & 2 & 0.1 & 0.34 \\  & RPipage & 1 & 0.08 & 0.26 \\  & Greedy & 3 & 0.65 & 0.74 \\  & Random & 1 & 0.4 & 0.49 \\  & Manual & 3 & 0.63 & 0.63 \\ \hline \multirow{6}{*}{Supervised} & Quadratic & 2 & 0.64 & 0.77 \\  & Pipage & 2 & 0.64 & 0.77 \\  & RPipage & 2 & 0.57 & 0.78 \\  & Greedy & 3 & 0.79 & 1.08 \\  & Random & 2 & 1.5 & 0.87 \\  & Manual & 2 & 0.64 & 0.67 \\ \hline \multirow{6}{*}{Supervised} & Quadratic & 2 & 0.54 & 0.57 \\  & Pipage & 1 & 0.38 & 0.49 \\  & RPipage & 1 & 0.38 & 0.49 \\  & Greedy & 2 & 0.69 & 0.67 \\  & Random & 1 & 0.54 & 0.5 \\  & Manual & 2 & 0.69 & 0.54 \\ \hline \multirow{6}{*}{Supervised} & Quadratic & 3 & 0.43 & 0.72 \\  & Pipage & 1 & 0.29 & 0.45 \\  & RPipage & 1 & 0.23 & 0.42 \\  & Greedy & 2 & 0.38 & 0.54 \\  & Random & 0 & 0.0 & 0.0 \\ \hline \hline \end{tabular}
\end{table}
Table 5. Education data; \(\mathcal{M}Q_{G}(\mathbf{x}_{\mathcal{R}})\) for \(\mathcal{M}=\{\textit{max},\textit{avg}\}\) of number of friends per student and \(\mathcal{A}\) being all algorithms; we also report \(std\) - the standard deviation for \(\textit{avg}Q\). For all datasets we used the _Inverse_ project preference function and \(\alpha=10\).

than the baselines. An observation is that using the _LinNorm_ project preference function does not make a significance difference in the qualitative results. Note again that our main focus on the education datasets was assigning students to projects they like. Assigning them with friends was a secondary goal.

\begin{table}
\begin{tabular}{c c c c c} \hline \hline Dataset & Algorithm & max\(Q_{R}\) & avg\(Q_{R}\) & \(std\) \\ \hline \multirow{6}{*}{\(\mathcal{M}=\{\textit{max},\textit{avg}\}\) of assigned project preferences and \(\mathcal{A}\) being all algorithms; we also report _std_ - the standard deviation for \(avgQ_{R}\). For all datasets we used the _LinNorm_ project preference function and \(\alpha=10\). & 1989 \\ \hline \hline \end{tabular}
\end{table}
Table 6: Education data; \(\mathcal{M}Q_{R}(\mathbf{x},\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{ \mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{\mathbf{ \mathbf{ \mathbf{ \mathbf{               }}}}}}}}}}}}})\) for \(\mathcal{M}=\textit{max}\), _avg_ of assigned project preferences and \(\mathcal{A}\) being all algorithms; we also report _std_ - the standard deviation for \(avgQ_{R}\). For all datasets we used the _LinNorm_ project preference function and \(\alpha=10\).

Figure 12: Employee data; Hyperparameter \(\alpha\) tuning for the _Company_ dataset.

Figure 13: Employee data; Controlling the balance between the fraction of people who changed department _PER_ and average gender gap per department _AVG-GAP_ using \(\alpha\).

### Hyperparameter tuning for the _Company_ dataset

Figure 13 shows the trade-off between the percentage of people who changed department and the average percentage of the male-female gap per department. Note that for values of \(\alpha\) between 0.5 and 2 there is a plateau. That is, the average gender gap per department remains almost constant although more employees change department. After \(\alpha\) drops below 0.5 the average male-female gap drops significantly, but the percentage of people who change departments grows very fast. According to the above plot a reasonable choice for \(\alpha\) is \(\alpha=2\).

Figure 12 shows the trade-off between the task and social satisfaction terms.