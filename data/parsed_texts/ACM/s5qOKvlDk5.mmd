Prompt-enhanced Federated Content Representation Learning

for Cross-domain Recommendation

Anonymous Author(s)

###### Abstract.

Cross-domain Recommendation (CDR) as one of the effective techniques in alleviating the data sparsity issues has been widely studied in recent years. However, previous works may cause domain privacy leakage since they necessitate the aggregation of diverse domain data into a centralized server during the training process. Though several studies have conducted privacy preserving CDR via Federated Learning (FL), they still have the following limitations: 1) They need to upload users' personal information to the central server, posing the risk of leaking user privacy. 2) Existing federated methods mainly rely on atomic item IDs to represent items, which prevents them from modeling items in a unified feature space, increasing the challenge of knowledge transfer among domains. 3) They are all based on the premise of knowing overlapped users between domains, which proves impractical in real-world applications. To address the above limitations, we focus on Privacy-preserving Cross-domain Recommendation (PCDR) and propose PFRC as our solution. For Limitation 1, we develop a FL schema by exclusively utilizing users' interactions with local clients and devising a Local Differential Privacy (LDP) method for gradient encryption. For Limitation 2, we model items in a universal feature space by their description texts. For Limitation 3, we initially learn federated content representations, harnessing the generality of natural language to establish bridges between domains. Subsequently, we craft two prompt fine-tuning strategies to tailor the pre-trained model to the target domain. Extensive experiments conducted on two real-world datasets consistently demonstrate the superiority of our PFRC method compared to the SOTA approaches.

Cross-domain Recommendation, Content Representation, Federated Learning +
Footnote †: ccs: Information systems Recommender systems

+
Footnote †: ccs: Information systems Recommender systems

+
Footnote †: ccs: Information systems Recommender systems

+
Footnote †: ccs: Information systems Recommender systems

+
Footnote †: ccs: Information systems Recommender systems

+
Footnote †: ccs: Information systems Recommender systems

## 1. Introduction

Cross-domain Recommendation (CDR) is a key area of recommender systems that aims to improve recommendation quality by leveraging knowledge from multiple domains. It relies on shared patterns and latent correlations, extending recommendations beyond single domains. Techniques such as domain adaptation and transfer learning play a vital role in information migration [7, 8, 17], and have achieved great success in attaining distinguished recommendations. However, a primary limitation of existing CDR methods is that they may leak domain privacy because of the centralized training schema. Direct data aggregation proves infeasible due to safeguards protecting trade secrets, exemplified by regulations such as the General Data Protection Regulation (GDPR) [36]. Furthermore, prior studies grapple with another limitation by aligning domains through direct utilization of users' identity information, under the assumption of overlap. This approach is also unfeasible in many real-world CDR scenarios, chiefly due to user privacy concerns. For instance, users registering for personal services on one platform typically harbor reservations about exposing their identity to other platforms.

Recently, several studies have been focused on conducting privacy preserving in CDR tasks [1, 2, 23]. For instance, Mai et al. [23] propose a federated GNN-based recommender system with random projection to prevent de-anonymization attacks, and a ternary quantization strategy to avoid user privacy leakage. However, previous methods solving PCDR still suffer from the following limitations: 1) They need to upload users' personal information, such as user embedding or user-related model parameters, to the central server. Despite the encryption of user information, there remains a potential for divulging users' private data, as achieving a balance between information encryption and method efficacy is challenging. 2) Existing federated methods predominantly rely on atomic IDs for item modeling, making it challenging to learn unified item representations. The uniqueness of items in different domains and the non-ID nature of data distributions pose difficulties in modeling items within a unified feature space. This limitation impedes the acquisition of universal item representations and hampers knowledge transfer across domains. 3) These methods are premised on the assumption of knowing overlapping users across domains, enabling domain alignment and CDR. However, this assumption carries the risk of serious privacy breaches, as it necessitates the use of users' identity information. Furthermore, identifying common users between domains can expose individuals to de-anonymization attacks, as organizations may exploit this information to infer user preferences in other domains.

To address these limitations, we target Domain-level Privacy-preserving Cross-domain Sequential Recommendation (DPCSR) and propose a Prompt-enhanced Federated Content Representation (PFRC) paradigm as our solution. We consider the sequential characteristic in PCDR since it is a common practice to organize users' behaviors into sequences. Specifically, to mitigate **Limitation 1**, we propose a federated content representation learning schema, treating domains as clients, with a central server responsible for parameter updates (FedAvg (Zhu et al., 2017) is applied). In PPCR, the gradients related to content representations can be shared across domains (the LDP strategy is also applied), while the user-related gradients are strictly prohibited to prevent privacy leakage. To deal with **Limitation 2**, we model items as language representations (i.e., _semantic ID_) by the associated description text of them so as to learn universal item representations, where the natural language plays the role of a general semantic bridging different domains. Compared with _atomic item ID_, _semantic ID_ enables us to represent different domain items in the same semantic space and simultaneously avoids the huge memory and storage footprint caused by the huge number of items in modern recommender systems. To tackle **Limitation 3**, we initially pre-train the federated content representations to fuse non-overlapped domains by leveraging the generality of natural languages, where a global _code embedding table_ under a universal semantic space is learned. Subsequently, to adapt the pre-learned knowledge to specific domains, we fine-tune the pre-trained content representations and model parameters with two kinds of prompting strategies.

The main contributions of this work can be summarized as:

* We target DPCSR and solve it by proposing PPCR, where a federated content representation learning schema and a prompt-enhanced fine-tuning paradigm are developed for domain transfer under the non-overlapping scenario.
* We model items in different domains as vector-quantified representations on the basis of their associated description texts, so as to unify them in the same semantic space.
* We develop a federated content representation learning framework for PCDR in the non-overlapping scenario by leveraging the generality of natural languages.
* We design two prompting strategies, namely full prompting, and light prompting, to adapt the pre-learned domain knowledge to the target domain.
* We conduct extensive experiments on two real-world datasets, and the experimental results consistently demonstrate the superiority of PPCR compared with other SOTA methods.

## 2. Related Work

### Federated Cross-domain Recommendation

Existing Federated Cross-domain Recommendation (FCDR) studies can be categorized into Cross-Silo Federated Recommendation (CSFR) and Cross-User Federated Recommendation (CUFR) methods according to the nature of clients. CSFR refers to FCDR among organizations and focus on preserving domain-level privacy (Bengio et al., 2017; Chen et al., 2018; Chen et al., 2019; Wang et al., 2019; Wang et al., 2019). For example, Wan et al. (Wang et al., 2019) devise a privacy-preserving double distillation framework (FedPDD) for CSFR to solve the limited overlapping user issue, which exploits a double distillation strategy to learn both explicit and implicit knowledge, and an offline training schema to prevent privacy leakage. In CUFR, each user is served as a client and tends to conduct user-level privacy preserving by FCDR (Wang et al., 2019; Wang et al., 2019). For instance, Yan et al. (Yan et al., 2019) train a general recommendation model on each user's personal device to avoid the leakage of user privacy and devise an embedding transformation mechanism on the server side for knowledge transfer. However, existing studies mainly rely on all or part of the overlapped users for CDR, and cannot be applied to scenarios in which users are non-overlapped across domains. Our solution falls into the CSFR category and tends to solve DPCSR by proposing a FL framework with federated content representations.

### Recommendation with Item Text

Recently, several recommendation methods have been focused on leveraging the content information to represent items to explore the generality of natural languages. Depending on whether text representation is directly used for recommendations, existing studies can be categorized into text representation (Huang et al., 2019; Wang et al., 2019; Wang et al., 2019; Wang et al., 2019) and code representation-based methods (Huang et al., 2019; Wang et al., 2019). For example, Hou et al. (Huang et al., 2019) design a text representation-based method in universal sequence representation approach (UnisRec) by utilizing a Pre-trained Language Models (PLM), where the semantic item encoding is obtained and participates in sentence modeling. But this kind of method is too strict in binding item text and its representation, causing the model to pay much attention to the text features. To address this, Hou et al. (Huang et al., 2019) first convert the text of the item into a series of distinct indices called "item codes", and then learn these Vector-Quantized (VO) item representations by engaging with these codes. However, the above methods are all based on a centralized training schema, and none of them consider the generality of contents in helping FCDR.

### Recommendation with Prompt tuning

Prompt tuning, initially introduced in the field of NLP, involves designing specific input and output formats to guide PLM in performing specific tasks. Recent researchers have also used prompt tuning to solve the cold-start (Wang et al., 2019; Wang et al., 2019; Wang et al., 2019) and cross-domain (Wu et al., 2019; Wang et al., 2019) issues in recommender systems. For example, Wu et al. (Wu et al., 2019) devise a personalized prompt-based recommendation framework for cold-start recommendation, which builds a soft prompt via a prompt generator based on user profiles, and enables a sufficient training via prompt-oriented contrastive learning. Wang et al. (Wang et al., 2019) propose a prompt-enhanced paradigm for multi-target CDR, where a unified recommendation model is first pre-trained using data from all the domains, then the prompt tuning process is conducted to capture the distinctions among various domains and users. Though the prompt tuning methods have been widely studied, they are mainly utilized for domain adaption or zero-shot issues, and few of them focus on solving the FCDR task, which is one of the main purposes of this work.

## 3. Methodologies

### Preliminaries

Suppose we have two domains A and B. Let \(\mathcal{U}^{A}=\{u_{1}^{A},u_{2}^{A},\ldots,u_{m_{A}}^{A}\}\) and \(\mathcal{U}^{B}=\{u_{1}^{B},u_{2}^{B},\ldots,u_{m_{B}}^{B}\}\) be the user sets, \(\mathcal{A}=\{A_{1},A_{2},\ldots,A_{M_{A}}\}\) and \(\mathcal{B}=\{B_{1},B_{2},\ldots,B_{M_{B}}\}\) be the item sets in domains A and B, respectively, where \(m_{A}\), \(m_{B}\), \(M_{A}\) and \(M_{B}\) are the corresponding user number and item number in each domain. Each item \(A_{i}\in\mathcal{A}\) (or \(B_{j}\in\mathcal{B}\)) is identified by a unique item ID and associated with a description text (such as the product title, introduction, and brand).

Users can express their preferences by interacting with specific items. Take user \(u_{i}^{A}\) as an example, we record her sequential behaviors on items in domain A as \(\mathcal{S}_{i}^{A}=\{A_{1},A_{2},\ldots,A_{j},\ldots\}\). The description text of item \(A_{i}\) is denoted as \(\mathcal{T}_{i}=\{w_{i},w_{2},\ldots,w_{c}\}\), where \(w_{j}\) is the content word in natural languages, and \(c\) is the truncated length of item text. To represent items in a unified feature space, we share the language vocabulary in both domains. Compared with other ID-based traditional recommendation methods (Hendricks et al., 2014; Li et al., 2016; Wang et al., 2017), we only take item IDs as auxiliary information, and they will not be used for domain knowledge transfer. Instead, we represent items by deriving generalizable ID-agnostic representations from their description texts. Moreover, although users may simultaneously interact with items in multiple domains or platforms, we do not align them between domains, since it may compromise users' privacy. That is, we assume users and items are entirely non-overlapped in our setting. In this work, we tend to preserve domain privacy in a federated training schema and transfer domain knowledge by a pre-train & prompt learning paradigm with the help of the generalized content representations.

### Overview of PFCR

**Motivation.** To conduct PCD, we resort to FL by viewing domains as clients and local data can only be utilized within clients. Moreover, to prevent attackers from inferring user identities from uploaded public information, we only share the item-related gradients with the protection of LDP. To transfer domain information under the non-overlapping and privacy-preserving scenario, we first embed domain information into the distributed item representation in the pre-training stage, and then adapt the prompts in the fine-tuning stage, so as to meet the specific distribution of the target domain.

The system architecture of PFCR in the pre-training stage is shown in Fig. 1, which is a FL process that consists of Vector-Quantified Item Representation (VQIR), Sequence Encoder (SE), and Federated Content Representation (FCR). VQIR aims at representing items in different domains in the same semantic space. It is the foundation of modeling users' cross-domain personal interests. By unifying domains into the same language feature space, we are able to effectively integrate domain information (see Section 3.3). But as we focus on PCD, we do not follow the traditional centralized training schema. On the contrary, we devise a framework with the help of the federated content representations (see Section 3.4). In SE, we apply a transformer-style neural network to learn users' sequential interests in each client (see Section 3.4.1). The overall framework of PFCR in the fine-tuning stage is shown in Fig. 2. Two prompting strategies, i.e., full prompting and light prompting, are developed in this phase. In the full prompting strategy (as shown in Fig. 2 (a)), we explore the domain prompts and user prompts for fine-tuning, while in the light prompt learning (as shown in Fig. 2 (b)), only the domain prompts are reserved. In our design, the domain prompts are shared by all the users in the same domain, and the user prompts are related to specific users.

### Vector-Quantified Item Representation

As natural language is a universal way to represent items in different domains, it is intuitive to leverage the generality of natural language texts to bridge domain gaps, since similar items will have similar contents even if they are not in the same domain. To model the common semantic information across different domains, we unify items in the same semantic feature space and take the learned text encodings via PLMs as universal item representations. But such a "_text_\(\sim\)_representation_" paradigm (Hendricks et al., 2014; Li et al., 2016) is too tight in binding

Figure 1. The system architecture of PFCR in the pre-training stage. In PFCR, the _code embedding table_ is pre-learned federally. The orange color within it denotes the index embeddings shared by all the clients, and the blue color indicates the index embedding that only appears in the current client.

item text and their representations, making the recommender might overemphasize the effect of text features. Moreover, the text encodings from different domains cannot naturally align in a unified semantic space. To address the above challenges, we exploit a "_text_ - \(\infty\)_ode - \(\varepsilon\)presentation_" schema (Hang et al., 2017; Zhang et al., 2017), which first maps item text into a vector of discrete indices (called _item code_), and then employs these indices to lookup the _code embedding table_ for deriving item representations. But different from existing studies that learn it in a centralized server, we tend to develop a FL schema for privacy-preserving purposes (the details can be seen in Section 3.4). In this work, we deem the description texts of items as public data and the users' interaction behaviors as privacy information.

#### 3.3.1. Discrete Item Code Leaning

To obtain the discrete codes of items, we first encode their description texts into text encodings via PLMs to leverage the generality of natural language text. Then, we map the text encodings into discrete codes based on the optimized product quantization method (the Product Quantization (PQ) (Hang et al., 2017) algorithm is utilized).

For item text encoding, we utilize the widely used BERT model (Devlin et al., 2019) as the text encoder (the Huggingface model is exploited1). Specifically, for a given item \(i\), we first insert a [CLS] token at the beginning of its description text \(\mathcal{T}_{i}=\{w_{1},\ldots,w_{c}\}\) and subsequently feed it into BERT to obtain its textual encoding:

Footnote 1: [https://huggingface.co/bert-base-uncased](https://huggingface.co/bert-base-uncased)

\[\mathbf{x}_{i}=\text{BERT}(\{[CLS];w_{1}\ldots;w_{c}\}]), \tag{1}\]

where \([:]\) represents the concatenation operation, \(\mathbf{x}_{i}\in\mathbb{R}^{d_{W}}\) is the representation of the given text, which is defined as the final hidden vector of the special input token [CLS].

To map the text encoding \(\mathbf{x}_{i}\) to discrete codes, the PQ method is applied. PQ defines \(D\) sets of vectors, within which each vector corresponds to \(M_{c}\) centroid embeddings with dimension \(d_{W}/D\). Let \(\mathbf{a}_{k,j}\in\mathbb{R}^{d_{W}/D}\) be the \(j\)-th centroid embedding for the \(k\)-th vector set. In the PQ method, the text encoding vector \(\mathbf{x}_{i}\) is first split into \(D\) sub-vectors \(\mathbf{x}_{i}=[\mathbf{x}_{i,1}\ldots;\mathbf{x}_{i,D}]\). Then, for the \(k\)-th sub-vector of \(\mathbf{x}_{i}\), PQ selects the index of its nearest centroid embedding from the corresponding set to generate the discrete code of \(\mathbf{x}_{i,j}\). The selected index of \(\mathbf{x}_{i,k}\) can be defined as:

\[\mathbf{c}_{i,k}=\arg\min_{j}\|\mathbf{x}_{i,k}-\mathbf{a}_{k,j}\|^{2}\in\{1,2,\ldots,M_{c }\}, \tag{2}\]

where \(\mathbf{c}_{i,k}\) is \(k\)-th dimension of the discrete code vector for item \(i\).

#### 3.3.2. Item Code Representation

Given the discrete item codes (e.g., \((c_{i1},\ldots,c_{i,D})\) of item \(i\)), we can derive item representations by directly performing lookup operation on a _code embedding table_ with average pooling.

**Code Embedding Table.** Let \(\mathbf{E}\in\mathbb{R}^{D\times M_{c}\times d_{W}}\) be the global _code embedding table_, where \(d_{0}\) denotes the dimension of the item embedding. There are \(D\) code embedding matrices within \(\mathbf{E}\), and each of them \(\mathbf{E}^{(k)}\in\mathbb{R}^{M_{c}\times d_{W}}\) is shared by the discrete codes of all the items (even if they are not in the same domain). This characteristic allows us to align different domains and embed the common domain information into item embeddings. Moreover, as we share the code embedding among all domains, we can represent items in the same code space, which forms the prerequisite for our subsequent FL endeavors. It is worth noting that we need to let the _code embedding table_ in all the clients and servers have the same initialization value to ensure they have the same update direction.

**Lookup Operation.** By performing the lookup operation on \(\mathbf{E}\), the code embeddings for item \(i\) can be denoted as \(\{\mathbf{e}_{1,\mathbf{c}_{i1}},\ldots,\mathbf{e}_{D,\mathbf{c}_{i,D}}\}\), where \(\mathbf{e}_{k,\mathbf{c}_{i,k}}\in\mathbb{R}^{d_{W}}\) is the \(c_{i,k}\)-th row of matrix \(\mathbf{E}^{(k)}\).

Then, we can arrive at the final item representation of item \(i\) by conducting the average pooling on the code embeddings:

\[\mathbf{v}_{i}=\text{Pool}\left(\left[\mathbf{e}_{1,\mathbf{c}_{i1}};\ldots;\mathbf{e}_{D,\mathbf{ c}_{i,D}}\right]\right), \tag{3}\]

where \(\mathbf{v}_{i}\in\mathbb{R}^{d_{W}}\) is the final item representation, and \(\text{Pool}(\cdot):\mathbb{R}^{D\times d_{W}}\rightarrow\mathbb{R}^{d_{W}}\) is the mean pooling method on the \(D\) dimension.

### Federated Content Representation

Since we focus on the PCDR task, we do not follow the traditional centralized training schema for item representation learning. Instead, we resort to FL, where domains are viewed as clients, and the privacy of user data is strictly utilized only on local clients. To this end, a federated content representation learning paradigm is devised, which involves local training, uploading gradient, and gradient aggregation and synchronization.

#### 3.4.1. Local Training

To learn users' sequential interests, we first feed items' VQ representations \(\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots,\mathbf{v}_{l},\ldots,\mathbf{v}_{n}\}\) to a transformer-style sequence encoder.

**Sequence Encoder.** It mainly consists of a multi-head self-attention layer (called MH) and a position-aware feed-forward neural network (called FFN) to model items' sequential dependencies. More formally, for each input item representation \(\mathbf{v}_{i}\), we first add it to the corresponding position embedding \(\mathbf{p}_{j}\) (\(j\) is the position of item \(i\) in the sequence).

\[\mathbf{h}_{j}^{0}=\mathbf{v}_{i}+\mathbf{p}_{j}. \tag{4}\]

Then, we feed \(\mathbf{h}_{j}^{0}\) to MH (Shen et al., 2017) and FFN (Shen et al., 2017) to conduct non-linear transformations. The encoding process is defined as follows:

\[\mathbf{H}^{l}=[\mathbf{h}_{0}^{l};\ldots;\mathbf{h}_{n}^{l}], \tag{5}\]

\[\mathbf{H}^{l+1}=\text{FFN}\left(\text{MH}\left(\mathbf{H}^{l}\right)\right),l\in\{1,2,\ldots,L\}, \tag{6}\]

where \(\mathbf{H}^{l}\in\mathbb{R}^{n\times d_{W}}\) denotes the hidden representation of each sequence in the \(l\)-th layer, \(L\) is the total layer number. We take the hidden state \(\mathbf{h}_{i}^{A}=\mathbf{h}_{n}^{L}\) at the \(n\)-th position as the sequence representation (\(\mathbf{S}_{A}\) is the input sequence in domain A).

**Optimization Objective.** Given the input sequence \(\mathbf{S}_{i}^{A}\), we define the next item prediction probability in domain A as follows:

\[P(A_{i+1}|\mathbf{S}_{i}^{A})=\text{Softmax}(\mathbf{h}_{i}^{A}\cdot\mathbf{V}_{\mathcal{A}}), \tag{7}\]

where \(\mathbf{V}_{\mathcal{A}}\) is the representation of all the items in domain A.

We exploit the cross-entropy loss on all the domains as the local training objective:

\[L_{A}=-\frac{1}{|\mathbf{S}_{A}|}\sum_{\mathbf{S}_{i}^{A}\in\mathcal{S}_{A}}\log P(A_{i+ 1}|\mathbf{S}_{i}^{A}), \tag{8}\]

where \(\mathcal{S}_{A}\) is the training set in domain A.

#### 3.4.2. Gradient Uploading and the Encryption Strategy

To enhance the local training process and enable the item representations to embed cross-domain information, we need to leverage the user preference information, such as users' interactions, in other domains. But as the privacy leakage concerns, we do not directly upload the user interaction data to the central server. Instead, we only upload model parameters' gradients for aggregation (the details can seen in Section 3.4.3). Then, the accumulated gradients will be passed back to clients to let them engage in the local training. In this distributed learning way, we can embed domain knowledge into pre-trained models, with which we can further conduct CDR.

However, as user-related gradients also have privacy leakage issues (attackers can obtain privacy features from model parameters or gradient through attach methods such as DLG (Shen et al., 2017)),we only upload item-related gradients in each client (i.e., the code _embedding table_) to the server for accumulation, and prohibit all the user-related parameters. The uploaded gradients of the _code embedding table_ are represented by \(g_{A}\) and \(g_{B}\) in domains \(A\) and \(B\), respectively.

**Encryption Strategy.** To prevent malicious actors from intercepting these gradients and then using them to infer item information, we further devise a LDP encryption method on these gradients. Traditional LDP methods (Han et al., 2017; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018) mainly add Gaussian or Laplace noise to the gradients. But as they may significantly distort the gradient direction, few recent methods exploit the quantization (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018) or randomized response (Chen et al., 2019; Wang et al., 2018) methods. However, they can only solve the problem of third-party attacks or untrustworthy partners, since only a single LDP solution is applied. To simultaneously consider both attack types, we propose a composite LDP method on \(g_{A}\) and \(g_{B}\), which consist of a quantization and a randomized response component.

_Quantization._ This component aims to map gradients to a finite number of discrete values to avoid third-party attacks, as the attackers cannot restore the gradient values without knowing the mapping method, even if they can intercept the uploaded gradients. For each element of \(g_{i}^{A}\) in client A (take domain A as an example), we first clip \(g_{i}^{A}\) to a certain range \([-\tau,\tau]\) according to its threshold:

\[g_{i}^{A}=\text{clamp}(g_{i}^{A},-\tau,\tau), \tag{9}\]

where \(\tau\) is the gradient threshold. Note that, we do not follow previous methods (Wang et al., 2018; Wang et al., 2018) that clip gradients to \([0,1]\), as gradients usually have richer values, and put them into a smaller interval will cause many gradient values to go to 0.

Then, we scale \(g_{i}^{A}\) by the following mapping function:

\[g_{i}^{A}=\text{round}(\frac{g_{i}^{A}+\tau}{s}), \tag{10}\]

where round(\(\cdot\)) means rounding each element to its nearest integer. \(q_{i}^{A}\in\{0,1,\dots,b-1\}\) is the quantized gradient element. \(s=\frac{2\tau}{b}\) is the scaling factor, \(b=2^{k}\) is the number of quantization buckets, \(k\) is the number of quantization bits.

_Randomized response._ To enable our method can also protect against attacks from untrusted partners, the randomized response method (Wang et al., 2018) is further applied. This method achieves protection by introducing more uncertainty via randomly flipping or displacing the quantized gradient so that its value can be randomly perturbed. The random noise adding process is defined as:

\[r_{i}^{A}=(q_{i}^{A}+\text{noise})\gamma b, \tag{11}\]

where noise \(\sim\text{Uniform}(0,b)\) is the random noise that follows an Uniform distribution.

Then, to proceed with the flip operation, we generate a flip variable following the Bernoulli distribution. It determines whether the data should undergo the flip operation:

\[c_{i}^{A}\sim\text{Bernoulli}(p),p=\frac{e^{\epsilon}+1}{e^{\epsilon}+2},q= \frac{1}{e^{\epsilon}+2}, \tag{12}\]

where \(p\) and \(q=1-p\) are the probability parameters, \(\epsilon\) is the privacy parameter. Once \(c_{i}^{A}\) is obtained, the permutation operation is performed on the noisy gradient \(r_{i}^{A}\):

\[r_{i}^{A}=(r_{i}^{A}-c_{i}^{A}\cdot\text{noise})\gamma b. \tag{13}\]

#### 3.4.3. Gradient Aggregation and Synchronization

To learn the global _code embedding table_ from distributed clients, the gradient aggregation operation is then applied on the server side. However, due to the LDP encryption method, the aggregated gradients may encompass inherent uncertainties and deviation. Therefore, the server needs to undertake further rectification, decode, and reconcile steps on the encrypted gradients.

To ensure each element in the gradient can be processed in the same way, we start by flattening the gradients from both clients, followed by a concatenation operation:

\[\mathbf{r}=\text{flatten}(r^{A})\oplus\text{flatten}(r^{B}), \tag{14}\]

where \(\mathbf{r}\in\mathbb{R}^{n_{e}\times d_{f}}\) is the flattened gradient, \(n_{e}\) is the client number, \(d_{f}=M_{e}\times d_{f}\) is the dimension of \(\mathbf{r}\), \(\oplus\) denotes the concatenation operation. \(\mathbf{r}^{A}=\{\mathbf{r}_{1}^{A},\mathbf{r}_{2}^{A},\dots,\mathbf{r}_{M_{A}}^{A}\}\) and \(\mathbf{r}^{B}=\{\mathbf{r}_{1}^{B},\mathbf{r}_{2}^{B},\dots,\mathbf{r}_{M_{B}}^{B}\}\) are the gradients of \(\mathbf{E}\) in clients A and B, respectively. Subsequently, to perform scaling and normalization operations during the denoising and recovery process of the gradients, we construct a constant matrix \(\mathbf{E}^{c}\) as follows:

\[\mathbf{E}^{c}=[(p-q)]_{i=1}^{n_{e}\times d_{f}}, \tag{15}\]

where \(\mathbf{E}^{c}\in\mathbb{R}^{2\times d_{f}}\) is a constant matrix with the same shape as \(\mathbf{r}\). \(p\) and \(q\) are employed to introduce the probabilities of inversion and permutation operations. By multiplying this constant matrix with the gradients after random response, each element is effectively subjected to an inverse operation during the decoding and correction process, consequently restoring the denoised gradient information.

We utilize FedAVG to aggregate the gradients, and determine the aggregation weight based on the ratio of the client data to the total data:

\[w_{i}=\frac{m_{i}}{\sum\limits_{i=1}^{m_{i}}m_{i}}. \tag{16}\]

The rectification and aggregation process can then be expressed as:

\[\mathbf{g}=\sum\limits_{i=1}^{m_{e}}\mathbf{r}_{i}\cdot\mathbf{E}_{i}^{c}\cdot w_{i}. \tag{17}\]

After that, we reshape the gradients back to their original dimensions and then decode them back to their previous range:

\[\mathbf{g}=\text{reshape}(\mathbf{g})\cdot s-\tau. \tag{18}\]Finally, we utilize the decoded gradients to update the embedding within the server, which can be defined as:

\[E\leftarrow E-\alpha\cdot\mathbf{g}. \tag{19}\]

We synchronize this updated embedding to all clients, followed by repeating the aforementioned training process until the pre-training phase convergences. Note that during the initialization phase of FL, we've ensured that the initial random parameters of the embedding on the server match those of the clients. As a result, the updated embedding is valid at this point.

### Domain-adaptive Prompting Paradigm

To retrieve the domain knowledge from pre-trained models, we further fine-tune the federated content representation through domain-adaptive prompts, i.e., full prompt and light prompt learning, to enhance CDR.

#### 3.5.1. The Full Prompting Schema

In this prompting paradigm, we exploit two kinds of soft prompts, i.e., domain prompt and user prompt, for domain adaption.

**Domain Prompt.** This is to extract the common preferences shared by all the users within each domain, which consists of the prompt context words and a domain prompt encoder. Suppose we have \(d_{W}\) context words in the domain prompt \(P_{\text{domain}}\in\mathbb{R}^{d_{W}\times d_{V}}\) (we set \(d_{W}\) as the batch size for the convenience of calculation). Then, we encode it by a multi-head attention layer (called MA). But different from the vanilla self-attention, we take the sequence embedding \(\mathbf{h}\) obtained by the pre-trained model as the queries. This encoding process can be defined as:

\[\text{head}_{i}=\text{Attention}(\mathbf{h}_{\text{d}}\mathbf{W}_{i}^{Q},\mathbf{p}_ {\text{domain}}\mathbf{W}_{i}^{K},\mathbf{p}_{\text{domain}}\mathbf{W}_{i}^{V}), \tag{20}\]

where \(n_{h}\) denotes the number of heads, \(\mathbf{W}_{i}^{Q},\mathbf{W}_{i}^{V},\mathbf{W}_{i}^{V}\in\mathbb{R}^{d_{V} \times d_{V}/n_{h}}\), and \(\mathbf{W}^{O}\in\mathbb{R}^{d_{W}\times d_{V}}\) are learnable parameters, \(\mathbf{h}_{d}\) is the representation of \(d_{W}\) sequences.

**User Prompt.** This is to model users' personal preferences in each domain. We represent it by the sequences of original item IDs, since they are unique to each user and can as supplementary information to user preferences (they are specific to each domain, and do not need to have cross-domain information). Then, we encode it (\(\mathbf{P}_{\text{user}}\)) by a transformer-style user prompt encoder(called UPE), which is defined as:

\[P_{\text{user}}=\text{UPE}(\mathbf{S}). \tag{21}\]

To this end, we concatenate domain prompt, user prompt, and sequence embedding, followed by a fully connected work, to make predictions:

\[\mathbf{h}_{\text{full}}=\mathbf{W}^{C}[P_{\text{domain}}\oplus\mathbf{P}_{\text{user }}\oplus\mathbf{h}_{S}]+b^{C}, \tag{22}\]

where \(\mathbf{W}^{C}:\mathbb{R}^{3d_{W}}\rightarrow\mathbb{R}^{d_{W}}\) represents the weight matrix.

#### 3.5.2. The Light Prompting Paradigm

To reduce the storage and computational costs in the fine-tuning process, we further consider a light prompting paradigm by removing the item-level features (i.e., the user prompt) and concatenation layer. The final sequence representation in this schema is:

\[\mathbf{h}_{\text{light}}=\mathbf{p}_{\text{domain}}+\mathbf{h}_{S}. \tag{23}\]

**Learning Objective.** For both paradigms, we minimize the following cross-entropy loss for learning optimal prompts (take domain A as an example):

\[L_{\text{A}}=-\frac{1}{|\mathbf{S}_{A}|}\sum_{\mathbf{S}_{i}^{A}\in\mathcal{S}_{A}}\log P (A_{i+1}|\mathcal{S}_{i}^{A}), \tag{24}\]

where \(P(A_{i+1}|\mathcal{S}_{i}^{A})=\text{Softmax}(\mathbf{h}_{\text{full}}(\mathbf{h}_{ \text{light}})\cdot\mathbf{V}_{i\mathcal{N}})\) is the probability of predicting the next item \(A_{i+1}\). The two-stage optimization algorithm is shown in Appendix B.

## 4. Experimental Setup

### Research Questions

We fully evaluate our PFCR method by answering the following research questions:

1. How does PFCR perform compared with the state-of-the-art baselines?
2. How do the key components of PFCR, i.e., Vector-Quantified Item Representation (VQIR), Federated Content Representation (FCR), and Domain-adaptive Prompting (DP), contribute to the performance of PFCR?
3. How do different federated learning algorithms affect PFCR?
4. What are the impacts of the key hyper-parameters on the performance of PFCR?

### Datasets and Evaluation Protocols

We conduct experiments on two pairs of domains, i.e., "_Office-Arts_", and "_OnlineRetail-Pantry_", in Amazon2 and OnlineRetail3 to evaluate our PFCR method. To conduct CDR, we select the Office and Arts domains in Amazon as our learning objective (i.e., the "_Office-Arts_" dataset). To further evaluate PFCR on the cross-platform scenario, we select Pantry as one domain and the data from OnlineRetail

Figure 2. The system architecture of PFCR in the prompt-tuning stage. The components with yellow color are the prompts to be fine-tuned.

[MISSING_PAGE_FAIL:7]

benefit of conducting CDR, and our method can effectively transfer domain knowledge in the non-overlapping scenario. 3) The cross-domain methods (i.e., CCDR and RecGURU) do not show impressive improvement over the single domains methods, denoting that non-overlapping CDR is a challenging task since there is no direct information to align domains. Our methods outperform CDR methods, demonstrating the usefulness of the contents in modeling the generality of domain information.

## 6. Model Analysis

### Ablation Studies (RQ2)

To show the importance of different model components, we further conduct ablation studies by comparing with the following variations of PPCR: 1) PPCR-VFD: This method excludes the VQIR, FCR, and DP components from PPCR. 2) PPCR-FD: This method removes the FCR and DP modules from PPCR. 3) PPCR-D: This method detaches the DP module from PPCR. 4) PPCR-F: This model does not use the FCR component in PPCR. The results of the ablation studies are shown in Table 2, from which we can observe that: 1) PPCR still has the best performance over its other variations. The gap between PPCR and PPCR-VFD indicates the importance of the components (i.e., VQIR, FCR, and DP) in learning the federated content presentations. 2) PPCR-VF performs better than PPCR-VFD, indicating the effectiveness of encoding items by semantic contents. 3) PPCR has a better performance than PPCR-D, showing the usefulness of conducting prompt learning in the DPCSR task. 4) The gap between PPCR and PPCR-F, indicating the importance of our FL strategy.

### Impacts of Different Federated Learning Strategies (RQ3)

To show the impact of different FL strategies, we further compare FedAVG (utilized in our PPCR method) with the following methods: FedProx (Zhou et al., 2017), Scaffold (Wang et al., 2018), and PopCode. PopCode is the method that only aggregates the code gradients with high frequencies in clients (i.e., popular codes). The experimental results are shown in Table 3, from which we can conclude that: 1) FedAVG outperforms FedProx and Scaffold, demonstrating the importance of simultaneously learning the common and domain-specific features of the code embeddings. Paying more attention to the common knowledge across domains (i.e., FedProx and Scaffold) cannot achieve better results. 2) FedAVG performs better than PopCode, indicating the benefit of aggregating all the codes. Only updating a subset of gradients in each client will distort the learning direction of the _code embedding table_, which results in sub-optimal results. 3) Beyond performance, we notice that FedProx and Scaffold upload more parameters than FedAVG, and have heavier computational costs.

### Impact of Hyper-parameters (RQ4)

We explore the impact of three key hyper-parameters, i.e., the number of communication round \(t\) in FL, the privacy parameter \(\epsilon\) in LDP, and the number of quantified buckets \(b\) in the quantization component of LDP. The experimental results are reported in Fig 3. Due to the space limitation, we only show the results on _'OnlineRetail-Pantry'_, and similar results are achieved on _'Office-Arts_'. From Fig 3, we can observe that: 1) \(t\) significantly impacts the performance of PPCR. The value of \(t\) is not that the bigger the better. A higher value of \(t\) will result in excessive updates, leading to the performance decline. 2) PPCR does not have a definite chaining trend as \(\epsilon\) changes since the randomized response method is different from Laplace or Gaussian noise that is added on all the gradients, while ours are only on part of them. 3) A proper value of \(b\) is important to PPCR. An over-big value of \(b\) will map the gradients to an overlarge range and will result in a performance decline. Similarly, an over-small \(b\) will limit the gradients in an over-small range, and let the gradients have less representational ability.

## 7. Conclusions

In this work, we target DPCSR and propose a PPCR paradigm as our solution with a two-stage training schema (the pre-training and prompt tuning stages). The pre-training phase is dedicated to achieving domain fusion and privacy preservation by harnessing the generality inherent in natural languages. Within this phase, we introduce a federated content representation learning method. The prompt tuning phase is geared towards adapting the pre-learned domain knowledge to the target domain, thereby enhancing CDR. To achieve this, we have devised two prompt learning techniques: the full prompt and light prompt learning methodologies. The experimental results on two real-world datasets demonstrate the superiority of our PPCR method and the effectiveness of our federated content representation learning in solving the non-overlapped PCDR tasks.

Figure 3. Impact of the hyper-parameters \(t,e,b\) on the _OnlineRetail-Pantry dataset_.

\begin{table}
\begin{tabular}{l c c c c|c c c c} \hline \hline  & \multicolumn{4}{c|}{**Office**} & \multicolumn{4}{c}{**Arts**} \\ \cline{2-9}
**Methods** & \multicolumn{2}{c|}{Recall} & \multicolumn{2}{c|}{NDCG} & \multicolumn{2}{c}{Recall} & \multicolumn{2}{c}{NDCG} \\ \hline \hline  & @10 & @50 & @10 & @50 & @10 & @50 & @10 & @50 \\ \hline \hline \multirow{2}{*}{10} & FullProx (Zhou et al., 2017) & 0.1197 & 0.1912 & 0.0731 & 0.0907 & 0.1132 & **0.2179** & 0.0353 & 0.0862 \\  & Scaffold (Wang et al., 2018) & 0.1146 & 0.1393 & 0.0742 & 0.0938 & 0.1115 & 0.3098 & **0.0036** & **0.0907** \\  & Footnote code & 0.1209 & 0.1937 & 0.0742 & 0.0900 & 0.1132 & 0.2533 & 0.0267 & 0.0350 \\ \hline \hline \multirow{2}{*}{**10**} & **FdAvG** & **0.213** & **0.1943** & **0.0771** & **0.0900** & **0.1146** & 0.2776 & 0.0638 & 0.0862 \\ \hline \hline \end{tabular}
\end{table}
Table 3. Results of different federated learning strategies on the _Office-Arts_ dataset.

## References

* (1)
* Bonawitz et al. (2017) Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marceloen, H Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. 2017. Practical secure aggregation for privacy-preserving machine learning. In _proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security_. 1175-1191.
* Chen et al. (2022) Chaohao Chen, Huiwen Wu, Jiajie Su, Lingjuan Lyu, Xiaolin Zheng, and Li Wang. 2022. Differential private knowledge transfer for privacy-preserving cross-domain recommendation. In _Proceedings of the ACM Web Conference 2022_. 11455-1463.
* Chen et al. (2023) Gaede Chen, Xinghua Zhang, Yijun Su, Yantong Lai, Ji Xiang, Junbo Zhang, and Yu Zheng. 2023. Win-Win: A Privacy-Preserving Federated Framework for Dual-Target Cross-Domain Recommendation. In _Proceedings of the AAAI Conference on Artificial Intelligence_, vol. 37, 4149-4156.
* Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristin Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. _arXiv preprint arXiv:1810.00808_ (2018).
* Ding et al. (2021) Hao Ding, Yifei Ma, Anoop Deeura, Yuyang Wang, and Hao Wang. 2021. Zero-shot recommender systems. _arXiv preprint arXiv:2105.05318_ (2021).
* Du et al. (2021) Yongqiu Du, Deyun Zhou, Yu Xie, Jao Shi, and Maong Gong. 2021. Federated matrix factorization for privacy-preserving recommender systems. _Applied Soft Computing_ 111 (2021), 10700.
* Guo et al. (2023) Lei Guo, Hao Liu, Lei Zhu, Wei Guan, and Zhiyong Cheng. 2023. DA-DAN: A Dual Adversarial Domain Algorithm Network for Unsupervised Non-overlapping Cross-domain Recommendation. _ACM Transactions on Information Systems_ (2023).
* Guo et al. (2021) Lei Guo, Li Tang, Tong Chen, Lei Zhu, Quoc Viet Hung Nguyen, and Hongqin Yin. 2021. DA-GCN: A domain-aware attentive graph convolution network for shared-account cross-domain sequential recommendation. _arXiv preprint arXiv:2105.03300_ (2021).
* Guo et al. (2023) Lei Guo, Chunxing Wang, Xinhua Wang, Lei Zhu, and Hongqin Yin. 2023. Automated Prompting for Non-overlapping Cross-domain Sequential Recommendation. _arXiv preprint arXiv:2304.04218_ (2023).
* Hallasi et al. (2015) Balazs Hallasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk. 2015. Session-based recommendations with recurrent neural networks. _arXiv preprint arXiv:1511.06990_ (2015).
* Hou et al. (2023) Yupeng Hou, Zhuzhao He, Julian McAuley, and Wayne Xin Zhao. 2023. Learning vector-quantized item representation for transferable sequential recommenders. In _Proceedings of the ACM Web Conference_. 2023. 1162-1171.
* Hou et al. (2022) Yupeng Hou, Shantle Liu, Wayne Xin Zhao, Yailen Li, Boh Ding, and J-Rong Wen. 2022. Towards universal sequence representation learning for recommender systems. In _Proceedings of the 2021 ACM SIGKDD Conference on Knowledge Discovery and Data Mining_. 585-593.
* Jogou et al. (2010) Herve Jogou, Matthijs Douze, and Cordelia Schmid. 2010. Product quantization for nearest neighbor search. _IEEE transactions on pattern analysis and machine intelligence_ 35, 13 (2010), 117-128.
* Kairouz et al. (2017) Peter Kairouz, Ziyu Liu, and Thomas Steinle. 2017. The distributed discrete gaussian mechanism for federated learning with secure aggregation. In _International Conference on Machine Learning_. PMLR, 2017-2126.
* Kang and McAuley (2018) Wang Cheng Kang and Julian McAuley. 2018. Self-attentive sequential recommendation. In _2018 IEEE international conference on data mining (ICDM)_. IEEE, 197-206.
* Karimireddy et al. (2020) Sai Pranetk Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and Amanda Theerthi Suresh. 2020. Scaffold: Stochastic controlled averaging for federated learning. In _International conference on machine learning_. PMLR, 5132-5143.
* Li et al. (2023) Chenghui Li, Yuanhua Xie, Chiemyun Yu, Ho Hu, Zang Li, Guoqiang Shu, Xiaohu and Di Ni. 2023. One for AI, all for One: Learning and Transferring User Embeddings for Cross-Domain Recommendation. In _Proceedings of the Sixteenth ACM International Conference on Hypertext and Data Mining_. 360-374.
* Li et al. (2022) Chenghui Li, Mingqiu Zhao, Huanming Zhang, Chenyun Yu, Lei Cheng, Guoqiang Shu, Beichi Kong, and Di Ni. 2022. RecGURU: Adversarial learning of generalized user representations for cross-domain recommendation. In _Proceedings of the fifteenth ACM international conference on web search and data mining_. 571-581.
* Li et al. (2023) Jucheng Li, Ming Wang, Jin Li, Jinmiao Fu, Xin Shen, Jingbo Shang, and Julian McAuley. 2023. 1st AI with Local Net Learning Language Representations for Sequential Recommendation. _arXiv preprint arXiv:2305.1373_ (2023).
* Liu et al. (2020) Tian Li, Anit Kumar Sahu, Mantil Zaheer, Marian Sarnabi, Annex Talwalkar, and Virginia Smith. 2020. Federated optimization in heterogeneous networks. _Proceedings of Machine learning and systems_ 2 (2020), 429-450.
* Liu et al. (2021) Zhaohao Liu, Wenke Pan, and Zhong Ming. 2021. PR-PRSS: Federated recommendation via fake marks and secret sharing. In _Proceedings of the 15th ACM Conference on Recommender Systems_. 668-673.
* Liu et al. (2022) Zhiwei Liu, Liangwei Yang, Ziwei Fan, Hao Peng, and Philip S Yu. 2022. Federated social recommendation with graph neural network. _ACM Transactions on Intelligent Systems and Technology (TIST)_ 13, 4 (2022), 1-24.
* Mai and Pang (2023) Peihua Mai and Yan Pang. 2023. Vertical Federated Graph Neural Network for Recommender System. _arXiv preprint arXiv:2305.37586_ (2023).
* McAlshan et al. (2017) Brendan McAlshan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. 2017. Communication-efficient learning of deep networks from decentralized data. In _Artificial intelligence and statistics_. PMLR, 1273-1282.
* Meihan et al. (2022) Wu Meihan, Li Li Chang, Tao Eric Rigli, Wang Xiaodong, and Xu Cheng-Zhong. 2022. FedCLo: Federated Cross-Domain Recommendation for Privacy-Preserving Rating Prediction. In _Proceedings of the 13th ACM International Conference on Information & Knowledge Management_. 2179-2188.
* Mu et al. (2022) Shantle Mu, Yupeng Hou, Wayne Xin Zhao, Yailen Li, and Bohin Ding. 2022. ID-Aggnostic User Behavior Pre-training for Sequential Recommendation. In _China Conference on Information Retrieval_. Springer, 16-27.
* Qi et al. (2020) Tao Qi, Fangzhou Wu, Chuan Wu, Yongfeng Huang, and Xing Xie. 2020. Privacy-preserving news recommendation model learning. _arXiv preprint arXiv:2003.09598_ (2020).
* Rajput et al. (2018) Shashan Rajput, Naih Mehta, Anima Singh, Babbunnam H Keshavan, Trung Yu, Lukas Hecht, Lichan Hong, Yi Liu, Yingh Qin Tuan, Jonah Samnot, et al. 2023. Recommender Systems with Generative Retrieval. _arXiv preprint arXiv:2305.050605_ (2023).
* Reisatzadeh et al. (2001) Animishossein Reisatzadeh, Aryan Mokhtari, Hamed Hassani, Ali Jadbabaie, and Rantin Pedarsani. 2001. Fedapax: A communication-efficient federated learning method with periodic averaging and quantization. In _International Conference on Artificial Intelligence and Statistics_. PMLR, 2021-2031.
* Sammet et al. (2023) Scott Sammet, Kirstin Balag, Filip Radlinski, Eric Wojtan, and Lucas Dixon. 2023. Large Language Models are Competitive Near Cold-start Recommenders for Language-and Item-based Preferences. In _Proceedings of the 17th ACM Conference on Recommender Systems_. 908-998.
* Shin et al. (2007) Xuyoung Shin, Hanock Kwak, Kyung Min Kim, Mukhy Kim, Young-lin Park, Jisu Jeong, and Seungjie Jung. 2021. Oneful user representation for recommender systems in e-commerce. arXiv preprint arXiv:2106.00875_ (2021).
* Shlezinger et al. (2020) Nir Shlezinger, Minghee Chen, Yorina C Eldar, H Vincent Poor, and Shiguang Cui. 2020. Federated learning with quantization constraints. In _ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)_. IEEE, 851-855.
* Shlezinger et al. (2020) Nir Shlezinger, Minghee Chen, Yorina C Eldar, H Vincent Poor, and Shiguang Cui. 2020. UeProf: Universal vector quantization for federated learning. _IEEE Transactions on Signal Processing_ 69 (2020), 500-514.
* Sun et al. (2019) Fei Sun, Jun Liu, Jian Wu, Changhua Fu, Xiao Lin, Wen Xu, and Peng Jiang. 2019. BERT20e: Sequential recommendation with bidirectional encoder representations from transformer. In _Proceedings of the 28th ACM international conference on information and knowledge management_. 1441-1450.
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Farnur, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukas Kaiser, and Illia Polosukhin. 2017. Attention is all you need. _Advances in neural information processing systems_ 90 (2017).
* Voigt and van den Bossche (2017) Paul Voigt and Axel Von den Bossche. 2017. The eu general data protection regulation (qtrp). _A Practical Guide, Inc Ed., Cham: Springer International Publishing_ 10, 3159/36701), 10.15555.
* Wan et al. (2022) Sheng Wan, Dahan Qaz, Hanilin Gu, and Daning H. 2022. FedPD: A Privacy-preserving Double Distillation Framework for Cross-Ago Federated Recommendation. _arXiv preprint arXiv:2205.06272_ (2022).
* Wang and Basar (2022) Yongqiang Wang and Tanner Basar. 2022. Quantization enabled privacy protection in decentralized stochastic optimization. _IEEE Trans. Automat. Control_ (2022).
* Wang et al. (2020) Yansheng Wang, Yongxin Tong, and Dingyuan Sun. 2020. Federated latent dirichlet allocation: A local differential privacy based framework. In _Proceedings of the AAAI Conference on Artificial Intelligence_, Vol. 34, 6283-6290.
* Wang et al. (2023) Yuhao Wang, Xiangyu Zhao, Bo Chen, Qidong Liu, Huifeng Guo, Huanshao Liu, Fuchong Wang, Rui Zhang, and Jianming Tang. 2023. PLATE: A Prompt-Enhanced Paradigm for Multi-Scenario Recommendations. In _Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval_. 1098-1507.
* Warner (1965) Stanley L Warner. 1965. Randomized response: A survey technique for eliminating erasure aware bias. _J. Amer. Statist. Assoc._ 60, 399 (1965), 63-69.
* Wu et al. (2021) Chuban Wu, Fangzhou Wu, Yang, Yongjing Huang, and Xing Xie. 2021. Fright Federated graph neural network for privacy-preserving recommendation. _arXiv preprint arXiv:2102.04825_ (2021).
* Wu et al. (2023) Xuanzheng Wu, Huan Liu, Wenlin Yao, Xiao Huang, and Ninghao Liu. 2023. Towards Personalized Cold-start Recommendation with Prompts. _arXiv preprint arXiv:2306.1268_ (2023).
* Wu et al. (2022) Yiqing Wu, Fabbang Xie, Yongchan Zhu, Fuxhen Zhuang, Xu Zhang, Leyu Lin, and Qing. He. 2022. Personalized prompts for sequential recommendation. _arXiv preprint arXiv:2205.0068_ (2022).
* Xie et al. (2022) Ruobing Xie, Qi Liu, Liangdong Wang, Shukui Liu, Bo Zhang, and Leyu Lin. 2022. Contrastive cross-domain recommendation in matching. In _Proceedings of the 28th ACM SIGIR Conference on Knowledge Discovery and Data Mining_. 4226-4236.
* Yan et al. (2022) Dengheng Yan, Yuchuan Zhao, Zhongqin Yang, Ying Jin, and Yiwen Zhang. 2022. FedCB: Privacy-preserving federated cross-domain recommendation.

Digital Communications and Networks_ 8, 4 (2022), 552-560.
* Zhang and Jiang (2021) Junker Zhang and Yuchen Jiang. 2021. A vertical federation recommendation method based on clustering and latent factor model. In _2021 International Conference on Electronic Information Engineering and Computer Science (EECS)_. IEEE, 362-366.
* Zhang et al. (2019) Tingting Zhang, Pengpeng Zhao, Yanchi Liu, Victor S Sheng, Jiajie Xu, Deqing Wang, Guifeng Liu, Xiaodong Zhou, et al. 2019. Feature-level Deeper Self-Attention Network for Sequential Recommendation. In _IJCAI_. 4320-4326.
* Zhou et al. (2020) Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Siwei Wang, Puxheng Zhang, Zhongyuan Wang, and Ji-hong Wen. 2020. S5: XeI-supervised learning for sequential recommendation with mutual information maximization. In _Proceedings of the 2020 ACM international conference on information & knowledge management_. 1893-1902.
* Zhu et al. (2019) Ligeng Zhu, Zhijian Liu, and Song Han. 2019. Deep leakage from gradients. _Advances in neural information processing systems_ 32 (2019).

## Appendix A Datasets

We conduct experiments on two pairs of domains, i.e., "_Office-Arts_", and "_OnlineRetail-Pantry_", in Amazon and OnlineRetail to evaluate our PFCR method. Amazon is a product review dataset that records users' rating behaviors on products in different domains. OnlineRetail is a UK online retail platform that records users' purchase histories on products within it. In both datasets, each product has a descriptive text, which is used to introduce the product such as function, purpose, etc. To conduct CDR, we select the Office and Arts domains in Amazon as our learning objective (i.e., the "_Office-Arts_" dataset). To further evaluate PFCR on the cross-platform scenario, we select Pantry as one domain and the data from OnlineRetail as another (i.e., the "_OnlineRetail-Pantry_" dataset). To conduct sequential recommendations, all the users' interaction behaviors are organized in chronological order. To satisfy the non-overlapping characteristic, all the users and items are disjoint in different domains. The only connection between domains is that similar products may have similar descriptive texts. To alleviate the impact of sparse data, we filter out users and items with less than 5 interactions. The statistics of our resulting datasets are reported in Table 4. Note that, since we focus on disjoint domains, we do not need the same number of training samples for different domains as the overlapping CDR methods do.

## Appendix B Two-stage training process

The training process of PFCR is shown in Algorithm 1, which consists of the pre-training and prompt tuning stages. In the pre-training stage, each client first learns the _code embedding table_ and the sequence encoder locally based on users' local behavior data on this domain. At the end of each training round, the accumulated gradients of the code embedding, encrypted with LDP, are uploaded to the server for decoding and aggregation. Then, the server utilizes the aggregated gradients to update the _code embedding table_ and synchronizes it across all clients. In the second stage, each client conducts prompt tuning to adapt the pre-learned domain knowledge to the specific domain by fixing the parameters that are not in the _code embedding table_ and prompts.

```
Input: Interaction sequence from two clients, \(\mathcal{S}^{A}\) and \(\mathcal{S}^{B}\); descriptions of all items, \(\mathcal{T}\). Output: Next-item predictions for each user in the client.
1 Stage 1: Federated Pre-training:
2 Initialization: Obtain the representation vector \(v_{l}\) for each item using the method described in Section 3.3.;
3for each epoch \(i\) with \(i=1,2,\ldots\)do
4 ClientExecutes:
5for each client \(j\)do
6foreach batchdo
7 Calculate local loss \(L_{j}\) via Eq. (8) ;
8 Accumulate the gradients of code embedding:
9\(g_{j}=g_{j}+\nabla L_{j}(\theta)\) ;
10
11 end for
12 Apply the LDP encryption to \(g_{j}\) via Eq. (9) - (13) ;
13
14 Upload the encrypted gradients to the server ;
15 end for
16 ServerExecutes:
17 Decode and aggregate the received client gradients using Eq. (14) - (18) ;
18 Update global code embedding \(E\) via Eq. (19) ;
19 Synchronize \(E\) to all the clients ;
20
21 end for
22 Stage 2: Prompt Tuning in clients:
23whilenot convergedo
24 Freeze all parameters except for the code embedding ;
25 Obtain the domain prompt via Eq. (20) ;
26 Obtain the user prompt via Eq. (22) ;
27 Obtain the final output by combining prompts and sequence output using Eq. (23) or Eq. (24) ;
28 Train using the final output with Eq. (25) ;
29
30 end while
```

**Algorithm 1** The two-stage training process of PFCR.

\begin{table}
\begin{tabular}{l r r r r} \hline \hline
**Datasets** & **\#Users** & **\#Items** & **\#Inters.** & **Avg. \(n\)** \\ \hline
**Office** & 87,436 & 25,986 & 684,837 & 7.84 \\
**Arts** & 45,486 & 21,019 & 395,150 & 8.69 \\ \hline
**OnlineRetail** & 16,520 & 3,469 & 519,906 & 26.90 \\
**Pantry** & 13,101 & 4,898 & 126,962 & 9.69 \\ \hline \hline \end{tabular}
\end{table}
Table 4. Statistics of the preprocessed datasets. “Avg. \(n\)” denotes the average length of the user interaction sequence.