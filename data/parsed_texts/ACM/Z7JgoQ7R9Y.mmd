# Retrieval Augmented Zero-Shot Text Classification

Anonymous Author(s)

###### Abstract.

Zero-shot text learning enables text classifiers to handle unseen classes efficiently, alleviating the need for task-specific training data. A simple approach often relies on comparing embeddings of query text to those of potential classes. However, the embeddings of a simple query sometimes lack rich contextual information, which hinders the classification performance. Traditionally, this has been addressed by improving the embedding model with expensive training. We introduce QZero, a novel training-free augmentation approach that reformulates queries by retrieving supporting categories from Wikipedia to improve zero-shot text classification performance. Our experiments across six diverse datasets demonstrate that QZero enhances performance for state-of-the-art static and contextual embedding models without the need for retraining. Notably, in News and medical topic classification tasks, QZero improves the performance of even the largest OpenAI embedding model by at least 5% and 3%, respectively. Acting as a knowledge amplifier, QZero enables small word embedding models to achieve performance levels comparable to those of larger contextual models, offering the potential for significant computational savings. Additionally, QZero offers meaningful insights that illuminate query context and verify topic relevance, aiding in understanding model predictions. Overall, QZero improves embedding-based zero-shot classifiers while maintaining their simplicity. This makes it particularly valuable for resource-constrained environments and domains with constantly evolving information.1

Footnote 1: code available at [https://anonymous.4open.science/r/QZERO-566F](https://anonymous.4open.science/r/QZERO-566F)

Footnote 2: [https://github.com/s/QZero-shot](https://github.com/s/QZero-shot) Text Classification

Footnote 3: [https://doi.org/XXXXXXX.XXXX](https://doi.org/XXXXXXX.XXXX)

2
Footnote 4: [https://github.com/s/QZero-shot](https://github.com/s/QZero-shot) Text Classification

## 1. Introduction

Zero-shot learning enables classifiers to handle unseen classes efficiently, alleviating the need for task-specific training data. Unfortunately, supervised classification models encounter significant challenges in scenarios that are characterized by an unconstrained label space (Bishop, 2006; Krizhevsky et al., 2012). For example, consider the task of classifying recipes into categories based on their ingredients or regional cuisines. The diversity of culinary traditions and the continuous emergence of new dishes create an expansive and evolving label space. This complexity makes it difficult to define and collect labeled data for all possible label types, thus limiting the model's ability to accurately classify novel or unique recipes. The conventional practice of retraining a model for each new label set becomes impractical due to the substantial increase in expenses associated with annotation and computation. This has necessitated the widespread adoption of zero-shot text classification.

Generative Large Language Models (LLMs) have revolutionized the field of natural language processing (Bowling, 2016; Krizhevsky et al., 2012), especially for zero-shot learning. Their remarkable zero-shot abilities enable them to tackle diverse tasks with impressive efficiency and adaptability. However, applying generative LLMs directly for zero-shot text classification tasks presents challenges due to their massive size and computational demands. These models require significant computational resources for inference, which can render them impractical in resource-constrained settings. In addition, these models tend to make predictions that are independent of user-specified classes (Krizhevsky et al., 2012). For instance, when classifying cuisine as Chinese or Mexican, a generative LLM model might incorrectly predict Italian, even though it wasn't an option. This lack of user control over the classification process poses a significant limitation to achieving targeted results.

A straightforward and efficient alternative for zero-shot text classification involves assigning a class (or label) to a query (or text) by comparing the embeddings of the query and potential classes using a distance metric such as cosine similarity (Krizhevsky et al., 2012; Krizhevsky et al., 2012). This relatively cheap approach removes the need for retraining or additional data labeling and allows for control over the classification process. However, when applied to queries that do not explicitly reflect the context of the class or align well with the model's training data, the accuracy of this technique decreases. For example, consider Table 1, which illustrates the contrast between explicit and implicit queries for a text input whose ground truth class is Technology. In the explicit example, keywords like "Artificial Intelligence" directly suggest Technology as the correct class. In contrast, the implicit example does clearly indicate the ground truth class. Here, a model's ability to infer Technology heavily relies on prior knowledge stored within its representations. For example, if the model lacks prior knowledge that "Sam Altman" and "Anthropic" are related to technology, it will fail to generate appropriate embeddings. Consequently, queries lacking sufficient contextual cues will result in reduced accuracy and recall. (Krizhevsky et al., 2012).

We introduce QZero, a simple retrieval augmentation approach to enhance the quality of embeddings used in zero-shot classification. Retrieval systems have emerged as powerful, cost-effective tools for various knowledge augmentation tasks, offering access to relevant information to keep models updated (Krizhevsky et al., 2012; Simonyan and Zisserman, 2014; Krizhevsky et al., 2012; Arora et al., 2017). They also benefit tasks like evidence-based modeling and text generation by expanding vocabulary and facilitating domain adaptation. Despite their potential, retrieval models remain under-explored in improving the quality of embeddings in zero-shot text classification tasks. The QZero paradigm explores reformulating queries by retrieving supporting information from Wikipedia. This approach enhances the quality of input text and achieves better zero-shot classification performance without model retraining. QZero streamlines the classification process by ensuring the embedding is enriched with a diverse vocabulary while remaining current with minimal resources.

Specifically, QZero, adopts a two-step approach for zero-shot classification tasks, as illustrated in Figure 1. First, it utilizes a retrieval model to identify relevant categories within supporting documents for the input text. These retrieved categories are then reformulated as the new input, depending on the type of embedding model used. For contextual embedding models, the concatenation of the categories serves as the reformulated input. Static word embedding models, on the other hand, utilize keywords and frequencies extracted from the retrieved categories. Consequently, the model utilizes this reformulated input to generate embeddings for downstream text classification tasks.

We evaluate QZero on six diverse datasets, using embedding models of different sizes, ranging from a simple Word2Vec to Open AI's text embeddings. Our results demonstrate that QZero benefits models of all scales. Notably, the additional context provided by QZero acts as a knowledge boost for smaller models, allowing them to achieve performance levels comparable to larger models. This translates to significant computational cost savings, as smaller models require less processing power. In addition, QZero offers meaningful insights that illuminate query context and verify topic (class) relevance, aiding in understanding model predictions.

## 2. Related Work

### Zero-shot Text Classification with Generative LLMs

Zero-shot classification has seen significant advancements through various approaches. One prominent avenue utilizes large-scale generative models, like GPT (Generative Pre-trained Transformer) (Devlin et al., 2018), for inference tasks. Trained on massive text datasets, these models excel at understanding and generating natural language, making them attractive for zero-shot classification tasks. However, their immense size and computational demands limit their accessibility and efficiency for resource-constrained environments. In addition, these models tend to make predictions outside the scope of user-defined classes, hindering the model's applicability in scenarios where class-based predictions are required.

### Zero-shot Text Classification via Semantic Comparison

Beyond large generative models, an alternative approach leverages embedding techniques to compare semantic similarity between the text and potential classes during classification. This approach involves encoding textual information, like words or sentences, into vector spaces using innovative methods like (Krizhevsky et al., 2012; Arora et al., 2017; Krizhevsky et al., 2012; Arora et al., 2017). Pioneering work by (Chen et al., 2018) and (Chen et al., 2018) laid the groundwork for this strategy. However, early embedding techniques had limitations in accuracy because of their reliance on simple modeling approaches, hindering their ability to effectively capture the nuanced relationships and semantic context within textual data. Recent research by (Chen et al., 2018) and (Chen et al., 2018) addressed this by fine-tuning pre-trained models with external knowledge sources like Wikipedia to improve the embedding quality for zero-shot classification. Although cheaper, these methods don't entirely eliminate the need for retraining, which can be challenging for rapidly evolving domains.

### Retrieval Augmented Learning

To alleviate the need for retraining, recent studies like (Krizhevsky et al., 2012; Simonyan and Zisserman, 2014) have explored retrieval-augmented approaches. These methods leverage external information by retrieving documents relevant to the query at inference time. These retrieved documents are then prepended to the query itself, serving as a form of query expansion. This approach improves the performance of language models on various tasks without further training the entire model. In addition, retrieval systems excel at handling new information efficiently, allowing for quick updates when new information arrives. Inspired by these studies, we investigate whether a similar retrieval strategy can improve the performance of text embedding models for zero-shot text classification tasks. Notably, while retrieval has been explored for text embeddings before (e.g., ERATE (Krizhevsky et al., 2012)), our objectives differ. ERATE focuses on reducing the high computational cost incurred from generating embeddings using large models. In contrast, QZero prioritizes enriching query context for improved classification.

### Query Enrichment and Expansion

Query enrichment, the process of reformulating or augmenting a query with supplementary information, has been widely explored in Natural Language Processing (NLP) tasks as a means to enhance

\begin{table}
\begin{tabular}{p{56.9pt}|p{113.8pt}|p{113.8pt}} \hline
**Query Type** & **Definition** & **Example** \\ \hline Implicit & The intended class category is implied but not stated & Sam Altman to discuss possible collaboration with Anthropic \\ \hline Explicit & The intended class category is clearly stated & Sam Altman, the CEO of a popular Artificial Intelligence Tech company, is set to discuss possible collaboration with Anthropic, a significant competitor \\ \hline \end{tabular}
\end{table}
Table 1. Explicit and Implicit query formats example for a query with the ground truth Technology. In the Explicit query, the keyword Artificial Intelligence may directly suggest Technology as the potential label.

semantic understanding and overall performance. While earlier research predominantly concentrated on its utility in information retrieval tasks (Sutskever et al., 2015; Chen et al., 2016; Chen et al., 2017; Chen et al., 2018; Chen et al., 2019), recent investigations have extended its applicability to few-shot (Chen et al., 2019) and short-text classification tasks (Chen et al., 2019). Techniques leveraging linguistic resources like synonyms, knowledge graphs, and pseudo-relevance feedback have shown promising outcomes. Our work builds upon these prior studies by investigating the application of query enrichment specifically for zero-shot classification tasks.

## 3. Methodology

### QZero: The Retrieval Augmented Query Reformulation Pipeline

We present QZero, a simple retrieval augmented query reformulation pipeline designed to enhance the zero-shot classification performance of embedding-based models. This system enhances the representation of essential information by transforming input text into refined queries, resulting in richer embeddings. Figure 1 provides an illustration of the QZero pipeline. Consider the input example in the figure: "Sam Altman to discuss possible collaboration with Anthropic." The input text becomes a query to the retrieval system, which returns a list of relevant document categories. The first set of categories pertains to information about Sam Altman, while the second set describes Anthropic. Afterward, the original input is disregarded, and the reformulated query becomes the concatenation of the retrieved categories. Subsequently, the embedding of the reformulated query is compared to the embeddings of the potential classes using cosine similarity. The class with the highest cosine similarity becomes the model's prediction. In this section, we describe the components of the QZero pipeline in detail.

### Knowledge Corpus

We used Wikipedia2 as our knowledge corpus across all experiments. Wikipedia articles offer concise information on a wide range of subjects, organized into categories located at the bottom of the page. These categories serve as topics and keywords associated with an article. We indexed English Wikipedia articles, focusing on article content and categories. Articles with fewer than 20 words and those with no assigned categories were excluded, resulting in 5.85 million documents with at least one category.

Footnote 2: We downloaded wiki version: enwiki-20230820-pages articles-multistream.xml.bz2

### The Retrieval System

The QZero scheme can be used with arbitrary retrieval systems. To build our retrieval system, we constructed a one-time index of the selected Wikipedia articles, leveraging it for subsequent retrieval of relevant article categories. Each article, represented as a unique document, contains attributes such as content and

Figure 1. Overview of the Query Reformulation Pipeline. QZero. Before classification, given a raw input query, QZero generates a new refined query by retrieving categories of Wikipedia articles that best match the original input query. The parts of the input query highlighted in purple and red represent entities that may require context information. The refined query provides the context information that may be useful for the classification process.

categories. For classification tasks, the retrieval process focuses solely on returning the categories of the best-matched articles, ensuring a concise presentation of information.

### Keyword Extraction

Wikipedia category names are usually lengthy and more detailed than conventional label names, which poses a challenge for static word embedding models. For such models, we used different keyword extraction strategies to harness the extensive information encapsulated in the Wikipedia category as described below:

* **SpaCy's POS tagging3**: it identifies keywords within a sentence by breaking down the sentence into tokens. then it employs a trained statistical model to analyze each token and assign it a Part-of-Speech (POS) tag. We selected only Noun tokens as keywords for our experiments. Footnote 3: [https://spaCy.io/usage/linguistic-features](https://spaCy.io/usage/linguistic-features)
* **Capitalization**: SpaCy's POS tagging struggles with proper nouns representing nationalities (e.g., Indians, Filipinos). To improve accuracy with such datasets, we switched to a simpler method based on word capitalization using simple Regex functions.
* **MedCAT (Kang et al., 2017)**: When dealing with a specialized domain like medicine, we used the Medical Concept Annotation Tool. (MedCAT) for more precise extraction of medical terms and sentences. MedCAT is an open-source tool that was pre-trained to identify medical terms in sentences and link them to standardized medical vocabularies.

### Query Reformulation

QZero takes a query \(x\) as an input to the retrieval system (see Figure 1). The system ranks Wikipedia articles based on their relevance to the query and returns the categories associated with the top-ranked articles. We select the categories of the top 50 articles, ensuring comprehensive coverage of potentially relevant information. Each article has a varying number of categories, and similar categories might appear across multiple articles. Our approach retains all categories from each retrieved article and we also keep repeated categories to emphasize their potential importance.

We explore the use case of the reformulated queries for contextual models that are optimized for generating meaningful embeddings from sentences and static word embedding models optimized for word inputs (details in Section 3.6). The reformulated query for the contextual embedding models comprises a concatenation of all the retrieved categories. This concatenation process is executed following the order of their respective articles' ranks, as depicted in Figure 1 and outlined in the equation below:

\[x_{R}^{s}=(C_{1},C_{2},\ldots,C_{n})\]

In the above equation, \(C_{1}\) represents the retrieved categories for an article. Post-retrieval, the categories for each article are denoted as \(C_{i}\), where i signifies the rank of the article (1 for the top-ranked article). Therefore, the reformulated query \(x_{R}^{s}\) is the concatenation of the retrieved categories in the sequence of their corresponding article's rank.

While contextual embedding models offer advantages, they have limitations regarding the number of input tokens. Concatenating too many categories can introduce noise into the reformulated query, and processing longer queries increases computational cost. To address these limitations, we restrict all contextual embedding models to use only the first 512 tokens of the concatenated categories as their refined queries.

Static word embedding models are limited to single words as inputs; thus, we extracted keywords \(K\) from the reformulated query using the appropriate strategy from Section 3.4 based on the dataset. Upon obtaining our keywords, we assign weights \(w\) to them based on their frequency across the reformulated query, facilitating effective measurement of each keyword's importance (since keywords will be repeated across the reformulated query). This results in a refined query where each keyword is paired with its weight, forming a structured representation as a list of tuples expressed as:

\[x_{R}^{w}=((K_{1},w_{1}),(K_{2},w_{2}),\ldots,(K_{n},w_{n}))\]

In the equation, \(x_{R}^{w}\) represents the reformulated word representation that will be used as input into the static word embedding model, and (K, w) is the keyword and its corresponding weight.

### Zero-shot text classification

We explore zero-shot text classification using both contextual embedding and static word embedding models (Table 3) to leverage the distinct advantages of each embedding type. Contextual embeddings capture the overall meaning of the reformulated query, while static word embeddings allow for finer-grained analysis of the individual keywords in the reformulated query. Both approaches are explained as follows:

* **Contextual Embedding Models:** To perform zero-shot classification, we use the contextual embedding models to obtain embeddings for the reformulated query \(x_{R}^{s}\) and each potential class label. Next, we compute the cosine similarity (a measure of closeness between embeddings) between \(x_{R}^{s}\) and each class. The class with the highest cosine similarity is assigned to \(x_{R}^{s}\) (See Figure 1).
* **Static Word Embedding Models:** First, we compute the embedding of each class label. For class labels with single words, we obtain their representation directly from the embedding model, while we average the representations of the constituent words within the label if it is a phrase. We also obtain the representation for each keyword \(K\) in the \(x_{R}^{w}\). To determine the best matching topic, we compute the cosine similarity between each keyword \(K\) in \(x_{R}^{w}\) and each class label \(y\), subsequently multiplying this similarity by the corresponding weight \(w\) of \(K\). These weighted similarities across all keywords in \(x_{R}^{w}\) are then aggregated to yield cumulative scores for each class label. Finally, the class label with the highest accumulated score is assigned to \(x_{R}^{w}\). See Algorithm 1 for details.

```
1:\(x_{R}^{w}\) (List of tuples: keyword and weight)
2:word_embed (Model: returns an embedding vector)
3:\(Y\) (List of class labels)
4:Class label with the highest cosine similarity
5: Initialize \(\textit{class\_scores}\) as an empty dictionary
6:for each \(y\in Y\)do \(y\_score\gets 0\)
7:for each \((K,w)\in x_{R}^{w}\)do \(K\_embed\leftarrow\text{word\_embed}(K)\) \(y\_embed\leftarrow\text{word\_embed}(y)\) \(sim\leftarrow\text{cosine\_similarity}(K\_embed,y\_embed)\) \(W\_sim\gets sim\times w\) \(y\_score\gets y\_score+W\_sim\) endfor \(\textit{class\_scores}[y]\gets y\_score\) endfor \(best\_class\leftarrow\text{key with maximum value in }class\_scores\) return\(best\_class\)
```

**Algorithm 1** : Classification using Static Word Embedding Models

## 4. Experimental Setup

### Datasets

For a comprehensive understanding of our study and to ensure a thorough examination of textual nuances across diverse domains, we evaluated QZero on six distinct publicly available text classification datasets. These include the AG News articles (Zhu et al., 2017), and the DBPedia factual knowledge base (Kang et al., 2017), which contains a summary of Wikipedia extracts. We also utilize the Yahoo! Answers community-driven knowledge exchange (Zhu et al., 2017), and the Yummly dataset (Zhu et al., 2017) of recipes from various regional cuisines, obtained from Kaggle's What's cooking challenge 4. Additionally, we employed the TagMyNews dataset (Zhu et al., 2017)5, containing news from RSS feeds as adopted by (Kang et al., 2017), and the Channel corpus (Zhu et al., 2017)6, a collection of medical abstracts about various diseases. We used labels similar to (Kang et al., 2017; Kang et al., 2017; Zhu et al., 2017; Zhu et al., 2017), which we show in Table 2. We report the classification accuracy averaged over three runs on the test sets. We summarize all dataset statistics in Table 2.

Footnote 4: [https://www.kaggle.com/competitions/whats-cooking/data](https://www.kaggle.com/competitions/whats-cooking/data)

Footnote 5: [https://github.com/AllRobotZhang/STCKA](https://github.com/AllRobotZhang/STCKA)

Footnote 6: [https://disi.unitn.it/moschiti/corpora.htm](https://disi.unitn.it/moschiti/corpora.htm)

### Zero Shot Models and Baselines

We evaluated the impact of the QZero pipeline on embedding models by comparing their performance utilizing the original input versus reformulated queries. Six different static word and contextual embedding models (See Table 3) were tested.

* Zero-shot classification via Contextual Embeddings: The approach is similar to the contextual embedding classification

\begin{table}
\begin{tabular}{l l l l l} \hline \hline
**Dataset** & **Classification Type** & **\# Classes** & **\# Test** & **Class Labels** \\ \hline AG News & News Topic & 4 & 7.6K & politics \& government, sports, business \& finance, technology \\ DBPedia & Wikipedia Topic & 14 & 70K & companies, schools \& university, artists, athletes, politics, transportation buildings \& structures, mountains \& rivers \& lakes, villages, animals, plants \& trees, albums, films, books \& novels \& literature \\ Yahoo! Answers & Web QA Topic & 10 & 60K & society \& culture, science \& mathematics, health, education \& reference,internet \& computers, sports, business \& finance, entertainment, family \& relationships, politics \& government \\ \hline Yummly & Cuisine Type & 20 & 7.9K & Cajun creole, Jamaican, Chinese, French, Vietnamese, Filipino, Irish, Thai, Indian, Southern United States, Moroccan, Greek, Italian, Japanese, Mexican, Korean, Russian, Spanish, British, Brazilian \\ \hline TagMyNews & News Topic & 7 & 6.5K & sports, business, entertainment, United States, politics \& government, health, science \& technology \\ \hline Ohsumed & Disease Topic & 23 & 4K & bacterial infections, virus diseases, parasitic diseases, neoplasms, musculoskeletal diseases, digestive system diseases, stomatognathic diseases, respiratory tract diseases, otorhinolaryngology diseases, nervous system diseases, eye diseases, urologic male genital diseases, female genital diseases, pregnancy complications, nutritional \& metabolic diseases, cardiovascular diseases, hernia \& lymphatic diseases, neonatal diseases, skin \& connective tissue diseases, endocrine diseases, immunologic diseases, environmental disorders, animal diseases, pathological conditions \\ \hline \hline \end{tabular}
\end{table}
Table 2. Dataset Classification Summary

\begin{table}
\begin{tabular}{l|l} \hline \hline
**Model** & **Embedding Type** \\ \hline Word2Vec (Zhu et al., 2017) & static word \\ GloVe (Zhu et al., 2017) & static word \\ FastText (Chen et al., 2017) & static word \\ \hline All-mpnet-base-v2 (Zhu et al., 2017) & contextual \\ text-embedding-3-small (GPT-3-small) & contextual \\ text-embedding-3-large (GPT-3-large) & contextual \\ \hline \hline \end{tabular}
\end{table}
Table 3. Embedding Models Evaluatedmethod described in Section 3.6. The only difference is that instead of the reformulated query, we compare the cosine similarity between the embeddings of the original text input and class labels to achieve zero-shot classification.
* Zero-shot classification via Static Word Embeddings: The key difference between the baseline static word embedding approach in Section 3.6 and this baseline approach is the absence of weights in the input to be classified. This means we compared each class label to the average vector representation of words in the original input text.

We accessed the text-embedding-3 small and large models through the OpenAI API7 in March 2024, while the All-mpnet-base-v2 model was accessed via hugging face8. We describe the All-mpnet base-v2 model, text-embedding-3 small and large models as All-mpnet, GPT-3-small, and large, respectively, throughout the manuscript. Unlike the All-mpnet base-v2 model, with a 512-token limit, OpenAI models can handle longer inputs. To ensure consistency and address the 512-token limit across all models, we employed the GPT-2 tokenizer9 when reformulating queries.

Footnote 7: [https://platform.openai.com/docs/guides/embeddings](https://platform.openai.com/docs/guides/embeddings)

Footnote 8: [https://huggingface.co/sentence-transformers/all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)

### Retrieval Models

To retrieve supporting categories, we explored two methods: a dense and a sparse retriever. The sparse retriever utilized the BM25 (Zhao et al., 2017) algorithm implemented in Pytterier (Pytter, 2017) with default settings. For the dense retriever, we employed the Contriever (Quin et al., 2018) model, trained specifically on Wikipedia passages. The BEIR (Zhu et al., 2019) framework enabled indexing and retrieval for the Contriever model.

## 5. Results and Discussion

### Effect of Retrieval Augmentation on Zero-shot Performance

Table 4 showcases the benefits of the retrieval augmentation pipeline (QZero), demonstrating performance improvements across all model sizes, especially in News topic classification datasets. In the TagMyNews dataset, the smallest model (Word2vec) experienced a significant 13.00% boost in accuracy, while even the largest model (GPT-3-large) saw a 6.61% increase. Similarly, in the AG News dataset, all models achieved a minimum accuracy gain of 4.17%, except for GPT-3-large, which exhibited a slight 1.57% drop. This drop in GPT-3-large's performance suggests potential noise introduced by uninformative categories in the reformulated query.

QZero enhances smaller models to achieve performance comparable to larger models without QZero. In TagMyNews, a QZero-enhanced Word2vec outperformed GPT-3-large and GPT-3-small (with original input) by substantial margins of 3.56% and 9.27%, respectively. Similarly, in the AG News dataset, Word2vec outperformed GPT-3-small by 3.4% while achieving similar accuracy with GPT-3-large. This is particularly valuable for scenarios with limited computational resources or tight financial constraints, where utilizing OpenAI's expensive embeddings might not be feasible.

Furthermore, QZero enriches the original input with useful context information that may be outside of the model's training data. For example, in the Ohsumed disease topic dataset, GPT-3-large and Word2vec (a model with limited medical knowledge) achieved a minimum of 5.00% increase in accuracy. On the Yummyly recipe datasets, the static word embedding models achieved a boost as high as approximately 38.00%. In addition, even the All-mpnet-base-v2 model, also lacking training data in the culinary domain, improved by 17.54% on the Yummyly recipes. This is impressive considering the limited medical and culinary domain information present in the general Wikipedia corpus, which was the only Knowledge corpus for QZero. These results highlight the promising potential of QZero as a cost-effective solution for domain adaptation challenges.

Our results also demonstrate that retrieval augmented query reformulation is effective for classifying topics outside of the model's training data. By transforming the input query into a knowledge space the model is more familiar with, reformulation bridges the gap between the model's knowledge and unfamiliar topics. This enhances the model's adaptability, allowing it to handle a wider range of tasks and domains without extensive retraining. Additionally, this reformulation provides support for a more effective representation, leading to models that are better equipped to tackle unseen data or generalize across domains.

Interestingly, we observed that across datasets, QZero rarely hurts the performance of the static word embedding models, and in cases where it does, the performance decline is typically minimal, under 1.00%. However, the impact on larger models is more diverse, especially for common evaluation sets like DBpedia and Yahoo Answers. Here, QZero's effect can vary in magnitude, with the largest decrease observed in GPT-3-large. This disparity might be linked to the training data used for these larger models. The Yahoo Answers dataset, for instance, was part of the training data for All-mpnet-base-v210. Similarly, GPT-3 models are trained on massive-scale datasets that might already contain the information encoded in the reformulated queries. As a result, applying QZero for such datasets might introduce redundancy or contradict existing knowledge within these larger models, leading to performance drops.

Footnote 10: training datasets can be found: via.[https://huggingface.co/sentence-transformers/all-mpnet-base-v2](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)

### Effect of Dense vs. Sparse Retrieval Models

The findings presented in Table 4 highlight QZero's robustness, showcasing its compatibility with dense and sparse retrievers. The BM25 (sparse) retriever performs better on the Ohsumed dataset, which contains lengthy documents that most dense neural retrievers might struggle with. Interestingly, the BM25 retriever also outperforms the Contriever (dense) retriever on the Yummly dataset. This could be because Contriever's training data was not well-suited for the specific domain of recipes in Yummly. In contrast, the Contriever (dense) retriever excels in tasks related to News topics, general QA, and Wikipedia topics, domains likely aligning better with its training data. These findings demonstrate how QZero adapts to the complementary strengths of each retrieval method, ultimately broadening its applicability across various use cases.

### Analysis of QZero's Outputs

Table 5 shows that QZero's capabilities go beyond simply classifying queries. By leveraging Wikipedia's knowledge base, it generates insightful details for each query, including relevant categories and keywords. These insights illuminate the context of the query and verify its connection to specific topics, enriching our understanding of the model's predictions. For instance, for the query "Today's executives address Lauer and Vieiera's exit buzz," QZero identifies categories like "American television shows," confirming and validating the entertainment ground truth. Furthermore, QZero's outputs can help in understanding the query's focus. Even without prior sports knowledge, QZero's outputs reveal the sports-related nature of the query "Martinez leaves bitter like Roger Clemens did almost exactly eight years earlier, Pedro Martinez has left the Red Sox apparently bitter about the way he was treated by management," aiding in interpreting the model's predictions.

Our analysis of incorrect predictions reveals a few key sources of error. In some cases, the predicted topic and the annotated ground truth might differ, but both could be valid interpretations of the query's meaning. Alternatively, the ground truth itself could be inaccurate. For example, in Table 5, the query "Why do I not walk correctly when I have sinusitis? Your equilibrium could be 'off' due to your sinusitis, which could cause problems with your inner ear" clearly has no connection to the ground truth, "Business & Finance." Other errors stem from retrieving irrelevant categories or limitations of the embedding model itself. By understanding these nuances, we can leverage the model's capabilities to interpret its predictions, refine our evaluation methods, and ultimately enhance model accuracy.

### Effect of Number of Retrieved Documents

Figure 2 shows how QZero's performance changes with respect to the number of documents retrieved for reformulating queries. This applies to both GloVe and All-mpnet embedding models. We see a trend across all datasets: as more documents are retrieved initially, QZero's performance improves. However, once the number of retrieved documents exceeds 50, the accuracy plateaus in the case of the All-mpnet model due to the fixed number of maximum tokens

Figure 2. Performance of All-mpnet and GloVe embedding models on selected datasets as a function of the number of document categories retrieved. (Contriever) CTV represents the dense retriever, and BM25 represents the sparse retriever.

the model can take as input. On the other hand, the accuracy of the GloVe embedding model starts to decline. This suggests that there's a point of diminishing return where retrieving more documents starts to include irrelevant ones. Retrieving too many documents dilutes the pool of relevant categories that appropriately describe the input query, ultimately reducing QZero's effectiveness.

## 6. Conclusions and Future Work

Embedding models present an effective solution for zero-shot text classification. However, the conventional method of retraining or fine-tuning the entire model to improve embedding quality proves costly, especially in rapidly evolving domains. QZero addresses this challenge by offering a training-free solution that improves the embedding quality. It achieves this through the adoption of a retrieval augmented query reformulation, which is considerably more cost-effective to update. Our experiments and results demonstrate that QZero significantly boosts classification accuracy across a wide range of embedding models, ranging from smaller options like Word2Vec to larger models such as OpenAI's text embeddings.

\begin{table}
\begin{tabular}{p{142.3pt}|p{142.3pt}|p{142.3pt}|p{142.3pt}|p{142.3pt}} \hline \hline
**Input Query** & **Returned Categories** & **Top Keywords** & **Ground Truth** & **Predicted Topic** \\ \hline Martinez leaves bitter like Roger Clemens did almost exactly eight years earlier, Pedro Martinez has left the Red Sox apparently bitter about the way he was treated by management & 1999 Major League Baseball season, New York Yankees postseason, Boston Solar postseason, American League Championship Series, 1999 in sports in Massachusetts,... & (players, 414), (baseball, 120), (people, 76), (sports, 67), (birds, 39), (postseason, 38), (coaches, 35), (competitions, 33), (managers, 30), (season, 28) & Sports Sports \\ \hline Today’s executives address Lauer and Vieiera’s exit buzz American television series endings, English-language television shows, First-run syndicated television programs in the United States,... & (television, 122), (people, 64), (films, 62), (series, 53), (shows, 40), (episodes, 36), (language, 30), (news, 27), (books, 22) (births, 20) & Entertainment Entertainment & Entertainment \\ \hline soy sauce, salt, pork tenderloin, hoisin sauce, toasted sesame seeds, sugar, dry sherry & Beijing cuisine, Pork dishes, Chinese condients, Chinese sauces, Vietnamese cuisine, 13), (Pork, 12), (Oregon, 11), (Monesian, 10), (Korean, 9), (Thai, 7) & Chinese Cuisine & Chinese Cuisine & Chinese Cuisine \\ \hline Ky. Company Wins Grant to Study Peptides (AP) AP - A company founded by a chemistry researcher at the University of Louisville upon a grant to develop a method of producing better peptides, which are short chains of amino acids, the building blocks of proteins & Companies based in Suffolk County, Biotechnology companies of the United States, Biotechnology companies established in 2005, 2005 establishments in New York (state), Biotechnology companies of the United States, Research support companies... & (companies, 132), (biotechnology, 26), (century, 18), (establishments, 16), (people, 14), (pharmaceutical, 14), (alumni, 13), (care’, 10), (faculty, 9), (women, 8) & Science \& Technology & Business \\ \hline Why do I not walk correctly when I have sinusitis? Your equlibrium could be ’off’ due to your sinusitis, which could cause problems with your inner ear. & ‘Cancer, Head and neck cancer, Otorhinolaryngology, Infammations, Diseases of inner ear, Hygiene, Rhinology, Diseases of Lear and mastoid process... & (diseases, 17), (ear, 17), (disorders, 14), (medicine, 11), (head, 9), (mascid, 8), (syndromes, 4), (cancer, 3), (virus, 2) & Business \& Fichance \& Technology & Business \\ \hline Some People Not Eligible to Get in on Google IPO Google has billed its IPO as a way for everyday people to get in on the process, denying Wall Street the usual strangehold it’s had on IPOs. Public bidding, a minimum of just five shares, an open process with 28 underwriters - all this pointed to a new level of public participation. But this isn’t the case. & (companies, 27), (law, 20), (establishments, 9), (market, 7), (offering, 5), (scandals, 5), (finance, 4), (stock, 3), (storing, 2) & Science \& Technology & Business \\ \hline \hline \end{tabular}
\end{table}
Table 5. QZero gives useful insights about a query.

Furthermore, QZero enhances the performance of smaller models to match their larger counterparts.

Beyond improving classification accuracy, QZero incorporates relevant information from Wikipedia articles, providing valuable insights into the context of queries and validating their pertinence to specific topics. This dual functionality strengthens the rationale behind model predictions, leading to more reliable and trustworthy classification outcomes. In summary, QZero improves the performance of embedding-based zero-shot classifiers while maintaining their simplicity, paving a promising path for more efficient and interpretable zero-shot classification techniques.

While QZero shows promise, there are limitations that require future exploration. First, the current query reformulation process for contextual embedding models could benefit from further refinement. Secondly, there might be cases where the model's knowledge is simply insufficient, and query reformulation alone would not be enough to improve performance. Lastly, our experiments focused on embedding models. Investigating whether query reformulation or other forms of retrieval augmented learning can benefit other models, such as those for natural language inference or generation, would be an interesting avenue to explore.

## References

* T. Amina Abdullahi, L. Mercurio, R. Singh, and C. Eickhoff (2023)Retrieval-based diagnostic decision support. JMIR Preprints, pp. 10.2196/perspins.0209.
* Bojanowski et al. (2017) Piotr Bojanowski, E. Grave, A. Joulin, and T. Mikolov. 2017. Enriching word vectors with subword information. Transactions of the association for computational linguistics5, pp. 135-146. Cited by: SS1.
* T. Brown et al. (2020)Language models are few-shot learners. Advances in neural information processing systems33, pp. 1877-1901. Cited by: SS1.
* R. Chandrasekaran, E. Yang, M. Yarnohammadi, and E. Agichtein (2022)Learning to enrich query representation with pseudo-relevance feedback for cross-lingual retrieval. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 1790-1795. Cited by: SS1.
* M. Chang, L. Ratinov, D. Roth, and V. Srikumar (2008)Importance of semantic representation: datasets classification. In Aaai, Vol. 2, pp. 830-835. Cited by: SS1.
* V. Clawen (2020)Query expansion with artificially generated texts. arXiv preprint arXiv:2012.08787. Cited by: SS1.
* H. Ding, J. Yang, Y. Deng, H. Zhang, and D. Roth (2023)Towards open-domain topic classification. arXiv preprint arXiv:2306.17290. Cited by: SS1.
* E. Gabrilovich, S. Markovitch, et al. (2007)Computing semantic relatedness using wikipedia-based explicit semantic analysis. In IJeAI, Vol. 7, pp. 1606-1611. Cited by: SS1.
* J. Guan, R. Xu, J. Yao, Q. Tang, J. Xue, and N. Zhang (2021)Few-shot text classification with external knowledge expansion. In Proceedings of the 2021 5th International Conference on Innovation in Artificial Intelligence, pp. 184-189. Cited by: SS1.
* W. Hersh, C. Buckley, T. Leone, and D. Hickam (1994)Obsumed: an interactive retrieval evaluation and new large test collection for research. In SIGIR 94: Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, organised by Dublin City University, pp. 192-201. Cited by: SS1.
* G. Incasad, M. Caron, L. Hosseini, S. Riedel, P. B. Jawowski, A. Joulin, and E. Grave (2021)Unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118. Cited by: SS1.
* G. Incasad and E. Grave (2020)Leveraging passage retrieval with generative models for open domain question answering. arXiv preprint arXiv:2007.01282. Cited by: SS1.
* M. Karama, M. Sugimoto, C. Hookawa, K. Matsushima, L. R. Warshaw, and Y. Ishikawa (2018)A neural network system for transformation of regional cuisine style. Frontiers in ICT5 (14). Cited by: SS1.
* J. Kim, S. Hwang, S. Song, H. Ko, and Y. Song (2022)Pseudo-relevance for enhancing document representation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 11639-11652. Cited by: SS1.
* A. Kosar, G. De Pauw, and W. Daelemans (2023)Advancing topical text classification: a novel distance-based method with contextual embeddings. In Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing, pp. 586-597. Cited by: SS1.
* Z. Kraljevic, D. Bean, A. Macio, L. Roguski, A. Polarin, A. Roberts, R. Bendayan, and R. Dobson (2019)Medcat-medical concept annotation tool. arXiv preprint arXiv:1912.10166. Cited by: SS1.
* J. Lehmann and A. L. Ollgrove (2015)Dlp-alpha- a large-scale, multilingual knowledge base extracted from wikipedia. Semantic web6, p. 2, pp. 195-205. Cited by: SS1.
* H. Linnei, T. Yang, C. Shi, H. Ji, and X. Li (2019)Heterogeneous graph attention networks for semi-supervised short text classification. In Proceedings of the 2015 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP), pp. 4821-4830. Cited by: SS1.
* C. Macdonald and N. Tonellotto (2020)Declarative experimentation in information retrieval using pytter. In Proceedings of the 2020 ACM SIGIR on International Conference on Theory of Information Retrieval, pp. 161-168. Cited by: SS1.
* C. Malavlova, F. Shaw, M. Chang, K. Lee, and K. Toutanova (2023)Quest: a retrieval dataset of entity-seeking queries with implicit set operations. arXiv preprint arXiv:2303.11694. Cited by: SS1.
* T. Mikolov, K. Chen, G. Corda, and J. Dean (2013)Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. Cited by: SS1.
* J. Pennington, R. Socher, and C. D. Manning (2014)GloVe: global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp. 1523-1543. Cited by: SS1.
* C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu (2020)Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research21, pp. 140-167. Cited by: SS1.
* V. Raina, N. Kassner, K. Popat, P. Lewis, N. Cancedda, and T. Martin (2023)Erate: efficient retrieval augmented text embeddings. In The Fourth Workshop on Insights from Negative Results in NLP, pp. 11-18. Cited by: SS1.
* N. Reimers and I. Guverchy (2019)Sentence-bert: sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084. Cited by: SS1.
* S. E. Roberts, S. Walker, S. Jones, M. Hancock-Beaulieu, M. Gafford, et al. (1995)Okapi at trec-3. Nist Special Publication Sp, pp. 109. Cited by: SS1.
* T. Schopf, D. Braun, and P. Matthews (2022)Evaluating unsupervised text classification: zero-shot and similarity-based approaches. In Proceedings of the 2022 6th International Conference on Natural Language Processing and Information Retrieval, pp. 6-15. Cited by: SS1.
* W. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettlemoyer, and W. Yih (2023)Rephie: retrieval-augmented black box language models. arXiv preprint arXiv:2302.12652. Cited by: SS1.
* K. Song, X. Tan, T. Qin, J. Lu, and T. Liu (2020)Mpnet: masked and permuted pre-training for language understanding. Advances in neural information processing systems33, pp. 16857-16867. Cited by: SS1.
* N. Thakur, N. Reimers, A. Riedel, A. Srivastava, and I. Gurevych (2021)Beir: a heterogenous benchmark for zero-shot and validation of information retrieval models. arXiv preprint arXiv:2102.060845. Cited by: SS1.
* F. Xu, W. Shi, and F. Choi (2023)Recon: improving retrieval-augmented lms with context compression and selective augmentation. In The Twelfth International Conference on Learning Representations, Cited by: SS1.
* D. Yang, Y. Zhang, and H. Fang (2023)Zero-shot query reformulation for conversational search. In Proceedings of the 2023 ACM SIGIR International Conference on Theory of Information Retrieval, pp. 257-263. Cited by: SS1.
* Y. Yu, Y. Zhuang, R. Zhang, Y. Meng, J. Shen, and C. Zhang (2023)Regen: zero-shot text classification via training data generation with progressive dense retrieval. arXiv preprint arXiv:2305.10079. Cited by: SS1.
* Y. Yue, Y. Zhang, X. Hu, and P. Li (2020)Extremely short chinese text classification method based on bidirectional semantic extension. In Journal of Physics Conference Series number 1, Vol. 1437, pp. 012068. Cited by: SS1.
* R. Zhang, Y. Wang, and Y. Yang (2023)Generation-driven contrastive self-training for zero-shot text classification with instruction-tuned gpr. arXiv preprint arXiv:2304.11872. Cited by: SS1.
* X. Zhang, J. Zhao, and Y. LeCun (2015)Character-level convolutional networks for text classification. Advances in neural information processing systems28. Cited by: SS1.
* D. Zhu, S. Wu, B. Carreterte, and H. Liu (2014)Using large clinical corpora for query expansion in text-based cohort identification. Journal of biomedical informatics49, pp. 275-281. Cited by: SS1.