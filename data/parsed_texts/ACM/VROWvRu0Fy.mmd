# Towards Expansive and Adaptive Hard Negative Mining:

Graph Contrastive Learning via Subspace Preserving

Anonymous Author(s)

###### Abstract.

Graph Neural Networks (GNNs) have emerged as the predominant tool for analyzing graph data on the web and beyond. Contrastive learning (CL), a self-supervised paradigm, not only mitigates the reliance on annotations but also leads to breakthroughs in performance. The hard negative sampling strategy that benefits CL in other domains proves ineffective in the context of Graph Contrastive Learning (GCL) due to the message passing mechanism. Embracing the subspace hypothesis in clustering, we propose a method towards expansive and adaptive hard negative mining, referred to as Graph ContRastive LeArning via subsPace prEServing (GRAPE). Beyond homophily, we argue that false negatives are prevalent over an expansive range and exploring them confers benefits upon GCL. Diverging from existing neighbor-based methods, our method seeks to mine long-range hard negatives throughout subspace, where message passing is conceived as interactions between subspaces. Additionally, our method adaptively scales the hard negatives set through subspace preservation during training. In practice, we develop two schemes to enhance GCL that are pluggable into existing GCL frameworks. The underlying mechanisms are analyzed and the connections to related methods are investigated. Comprehensive experiments demonstrate that our method achieves state-of-the-art performance on multiple graph datasets and maintains competitiveness in various application settings. Our work contributes to the improvement of representation learning on web graphs, aligning with the scope of The Web Conference. Our code is available at [https://anonymous.4open.science/r/Grape-code](https://anonymous.4open.science/r/Grape-code).

Graph neural networks, Graph contrastive learning, Hard negative mining, Subspace preserving, Web data mining 2020 acmcopyright

## 1. Introduction

Graph data is ubiquitous in both real-world and virtual realms, encompassing a broad spectrum of areas such as social networks, molecular structures, trade circulation. Recently, GNNs have witnessed significant strides in the domain of analyzing graph data, exhibiting exceptional performance in tasks such as graph classification [90, 99], node clustering [16, 54], link prediction [98, 111] and graph generation [40, 93]. Following the pioneering contributions of GCN [32], GraphSAGE [20], GAT [71], etc., numerous GNN architectures have been developed and enhanced. Almost all GNNs are built upon the message passing mechanism between neighbors, where each node acquires feature information from its neighbors and contributes its own feature information. Analogous to most neural networks, GNNs are typically trained in a supervised manner and require an abundance of annotations.

Contrastive Learning (CL), as a category of self-supervised methods, has recently demonstrated a series of state-of-the-art performances in various domains [6, 7, 14, 101, 107]. These studies emphasize that the representations learned by CL perform comparably to supervised learning in downstream tasks. The essence of CL lies in learning representations that retain invariance under a variety of distortions, referred to as "data augmentations" [68, 69]. To achieve this, researchers develop InfoNCE objective [18, 53], which maximizes a lower bound of mutual information between augmented views [2, 25]. The core conception is to draw positive pairs closer while repelling negative pairs apart [19].

The breakthroughs of CL in computer vision have motivated studies to extend the analogous concepts from visual representation learning to graph data, referred to as Graph Contrastive Learning (GCL). These GCL methods achieve sofa in both graph-level and node-level tasks [21, 67, 83, 97, 108, 99]. GCL adheres to the typical CL paradigm, albeit with specific variations [63, 72]. In general, the application paradigms of CL in visual, textual, and graph data domains can be illustrated as Figure 1. As demonstrated, existing research in GCL can be summarized in the following two main threads: (1) augmentation for graph [28, 37, 59, 65, 63, 83, 94, 95, 102, 106, 110], which aims to adapt semantic-preserving augmentation techniques from visual data to irregular graph data. (2) contrastive loss for graph [21, 38, 64, 84, 97, 102, 109], which explores the loss functions suitable for GNN training within CL framework. Our work falls into the latter category. Unlike other mainstream instance-discriminating backbones [9, 10, 22, 23, 70, 76] where instances do not exhibit explicit interactions, GNNs rely on message passing among neighbors. A notable issue arises where hard negative sampling techniques, proven to contribute in CL [5, 29, 36, 60, 82], does not confer benefits in GCL and may even impair performance, which has been discussed in [46, 64, 84, 108]. The main concept behind is that hard negatives in GCL are prone to being false negatives, consequently, pushing away the semantically similar representations leads to a degradation in performance.

In this paper, we report that mining **expansive** and **adaptive** hard negatives enhances node-level tasks. To achieve both objectives, we introduce a negative hardness estimation scheme for GCL, aligning with the subspace preservation hypothesis in clustering. The core strength of our method lies in its ability to capture hard negatives beyond the scope of message passing and adjust the hard negatives set in a self-scaled manner. In node-level tasks, the concept of subspace preservation is intuitive. For instance, in a citation network, it can be elucidated as follows: from the semantic perspective, articles with the similar theme tends to share keywords (features); from the structural perspective, mutual citations within the same subfield are frequent whereas cross-domain article citations are limited. Prominent recommendation mechanisms within social or e-commerce networks, which curate personalized content for individual entities, have catalyzed the emergence of subspaces (Song et al., 2016; Wang et al., 2017; Wang et al., 2018). We provide theoretical and experimental analyses to illuminate why and how our method works. To the best of our knowledge, our work is the first to address the GCL through subspace techniques.

In summary, the main contributions of this paper can be encapsulate in threefold:

* We show that more expansive and adaptive hard negative mining is promising for enhancing node-level GCL. In line with this idea, we propose GRAPE, a novel negative hardness estimation method for GCL based on subspace theory.
* In GRAPE, the hard negatives beyond the scope of message passing can be captured and the hard negatives set can be adaptively scaled. Two schemes are devised to alleviate the influence of false negative samples on GCL. Besides, we provide a theoretical exposition of GRAPE's properties and its connection with related methods.
* In comparison to several advanced GCL methods, GRAPE exhibits superior performance on eight widely-used public graph datasets. We conduct comprehensive experiments under various settings to thoroughly analyze the results and behaviors of GRAPE.

The proofs of involved theorems, experimental settings and supplementary experiments are relegated to the appendix.

## 2. Related Work

In line with the focus of our work, we provide an overview of related works on graph contrastive learning and subspace preserving.

### Graph Contrastive Learning

Amidst the increasing recognition of contrastive learning's expressive capability, DGI (Zhu et al., 2017) and InfoGraph (Zhu et al., 2018) first leverage the maximization of mutual information (Zhu et al., 2019) at the node- and graph-level, respectively, to attain effective representations. In subsequent works, MVGRL (Zhu et al., 2019) utilizes graph diffusion (Zhu et al., 2019) to obtain augmented views and applies contrastive learning at both the node and graph levels. GMI (Zhu et al., 2019) extends mutual information computations from vector spaces to the graph domain and assesses the correlation between input graphs and high-level hidden representations. GRACE (Zhu et al., 2019), GCA (Zhu et al., 2019) employ the InfoNCF-style objective and obtain node representations by treating others as negative samples, which serves as a baseline in follow-up research. To mitigate the sampling bias issue, BGRL (Zhu et al., 2019) extends the BYOL (Zhu et al., 2019) framework to graph. In this strand, CCA-SSG (Zhu et al., 2019) optimizes a feature-level objective inspired by classical canonical correlation analysis. SpCo (Zhu et al., 2019) is introduced as a spectral GCL module based on the general graph augmentation rule to enhance existing GCL methods. In another thread, ProGL (Zhu et al., 2019) estimates the probability of a true negative using a two-component beta mixture model. Empirical studies (Zhu et al., 2019) verify that assigning higher weights to hard negatives or generating hard negatives fails to improve GCL. GDCL (Zhu et al., 2019) jointly performs GCL and DEC (Zhu et al., 2019); nevertheless, this unsupervised process may lead to training collapse. COSTA (Zhu et al., 2019) advocates generating covariance-preserving augmented features inspired by matrix sketching. HomoGCL (Zhu et al., 2019) proposes utilizing the homophily in graph to filter positive pairs. PHASES (Zhu et al., 2019) employs a progressive negative masking strategy to enhance tolerance between sample pairs. We recommend readers to refer to (Zhu et al., 2019; Wang et al., 2018; Wang et al., 2018) for a comprehensive overview.

### Subspace Preserving

One underlying tenet in machine learning is that the data contains certain type of structure for intelligent representation. From this, the subspace assumption, which runs through the research journey of machine learning, can be described as follows (Zhu et al., 2019): high-dimensional data is drawn from a union of multiple affine or linear subspaces. In a simplified perspective, affine subspace is more closely related to manifold learning (Bishop, 1996; Bishop, 1996; Bishop, 1996; Bishop, 1996; Bishop, 1997), whereas linear subspace aligns more closely with dictionary learning (Song et al., 2016; Wang et al., 2018; Wang et al., 2018; Wang et al., 2018). Over the past decade, subspace learning based on the self-expression model, which enjoys the benefits of the both, has made significant strides (Song et al., 2016; Wang et al., 2018; Wang et al., 2018). The main divergence among these methods is the constraints imposed on the self-expression coefficients, such as sparse constraint (Song et al., 2016; Wang et al., 2018; Wang et al., 2018), low-rank constraint (Wang et al., 2018; Wang et al., 2018; Wang et al., 2018), connectivity constraint (Wang et al., 2018; Wang et al., 2018).

Figure 1. A comparison of CL for visual, textual, and graph data. The irregularity of graph data and the message passing mechanism of GNNs distinguish GCL from CL in other domains. Graph convolutional operator introduces smoothing property among neighbors, while necessitating some technical changes to GCL.

and smooth constraint (Brockman, 2011; Chen et al., 2013; Chen et al., 2014). We adopt the fundamental principles of such methods to tackle hard negative mining in GCL. Both empirical investigations and theoretical analyses confirm the suitability in the context of GCL. Recent studies in graph dictionary learning (Brockman, 2011; Chen et al., 2014; Chen et al., 2014) focus on sparse encoding for molecules, which are not directly related to our work.

## 3. Methodology

### Notations and Preliminaries

Let \(G=(\mathbf{A},\mathbf{X})\) denotes a graph with \(n\) nodes, where \(\mathbf{A}\in\{0,1\}^{n\times n}\) denotes the adjacency matrix and \(\mathbf{X}\in\mathbb{R}^{n\times d}\) denotes the feature matrix. Let \(\mathbf{\hat{A}}=\mathbf{A}+\mathbf{I}_{n}\) be the adjacency matrix with self-loops. The normalized adjacency matrix is then given by \(\mathbf{\hat{A}}=D^{-\frac{1}{2}}\mathbf{\hat{A}}D^{-\frac{1}{2}}\), where \(\mathbf{D}\) is the degree matrix. Vectors and matrices in this paper are denoted by bold lowercase and bold uppercase letters, respectively. Set \(\{1,2,\cdots,n\}\) is abbreviated by \([n]\) for simplicity.

Our objective is to train a GNN encoder \(f_{\mathbf{0}}(\mathbf{A},\mathbf{X})\) in a label-scarce scenario, where \(\mathbf{\Theta}\) represents the network parameters. The output node embeddings are supposed to be directly applicable for downstream tasks, such as node classification and node clustering in this paper. Take GCN for example, the layer-wise forward-propagation operation at the \(l\)-th layer is formulated as:

\[\mathbf{Z}^{(l)}=\sigma\left(\mathbf{\hat{A}}\mathbf{Z}^{(l-1)}\mathbf{W}^{(l)}\right), \tag{1}\]

where \(\mathbf{W}^{(l)}\) is the trainable weights for feature transformation and \(\mathbf{Z}^{(l)}\) denotes the node embeddings at the \(l\)-th layer. Clearly, there is \(\mathbf{Z}^{(l)}=\mathbf{X}\) at the initial layer. \(\sigma(\cdot)\) denotes an activation function such as ReLU. In context of GCL, two views \(G_{1}=(\mathbf{A}_{1},\mathbf{X}_{1})\), \(G_{2}=(\mathbf{A}_{2},\mathbf{X}_{2})\) are generated by augmentation strategies (Glorot and Bengio, 2011) each epoch. \(G_{1}\) and \(G_{2}\) are fed into a Siamese GNN encoder (Chen et al., 2014) to produce node embeddings \(\{\mathbf{u}_{i}\}_{i=1}^{n}\) and \(\{\mathbf{v}_{i}\}_{i=1}^{n}\), respectively. The contrastive loss can then be computed, followed by backpropagation. The common baseline for graph contrastive loss is the InfoNCE-style loss in GRACE (Glorot and Bengio, 2011). Specifically, the contrastive loss for \(\mathbf{u}_{i}\) is defined as:

\[\ell\left(\mathbf{u}_{i}\right)=\] \[-\log\underbrace{\frac{e^{\theta\left(\mathbf{u}_{i},\mathbf{u}_{i}\right)/ \tau}}{\text{positive pair}}+\underbrace{\sum_{j\neq i}e^{\theta\left(\mathbf{u}_{i },\mathbf{v}_{j}\right)/\tau}}_{\text{inter-view negative pairs}}+\underbrace{\sum_{j \neq i}e^{\theta\left(\mathbf{u}_{i},\mathbf{u}_{j}\right)/\tau}}_{\text{intra-view negative pairs}}, \tag{2}\]

where \(\theta(\cdot,\cdot)\) denotes cosine similarity and \(\tau\) is the temperature parameter. The objective \(\ell\left(\mathbf{v}_{i}\right)\) is defined symmetrically. Then the overall loss is given as:

\[\mathcal{L}=\frac{1}{2n}\sum_{i=1}^{n}\left(\ell\left(\mathbf{u}_{i}\right)+\ell \left(\mathbf{v}_{i}\right)\right). \tag{3}\]

It can be observed in Eq. (2) that by minimizing loss \(\mathcal{L}\), embeddings of the same sample under two augmentations are pulled closer (**positives**), while embeddings of different samples are repelled away (**negatives**). For simplicity, in this paper, we represent \(\ell(\mathbf{u}_{i})\) in the following form

\[\ell(\mathbf{u}_{i})=-\log\frac{\text{pos}(\mathbf{u}_{i})}{\text{pos}(\mathbf{u}_{i})+ \text{neg}(\mathbf{u}_{i})}. \tag{4}\]

### Graph Contrastive Learning via Subspace Preserving

Recent studies (Wang et al., 2017; Wang et al., 2017) report that considering all samples other than the anchor itself as negatives (Eq. (2)) unduly distances **false negatives** (i.e., samples of the same class as the anchor). This so-called "class collisions" phenomenon makes the marriage of CL and GNNs seem subtle and, as a result, leads to a performance decline. Hard negative mining provides a remedy to rectify this deficiency, where **hard negatives** refer to samples exhibiting a high degree of similarity to the anchor or having a greater likelihood of being false negatives. Let \(\mathbf{\Phi}_{i}\) be the hard negatives set of the \(i\)-th sample. With \(\{\mathbf{\Phi}_{i}\}_{i=1}^{n}\) identified, hard negative mining mainly employs two forms of loss: one explicitly treats \(\mathbf{\Phi}_{i}\) as positives (Glorot and Bengio, 2011; Wang et al., 2017)(referred to as "Positive" strategy), i.e., modifying the "pos" term in Eq. (4) to:

\[\text{pos}(\mathbf{u}_{i})=e^{\theta\left(\mathbf{u}_{i},\mathbf{u}_{j}\right)/\tau}+\sum_{ j\in\mathbf{\Phi}_{i}}e^{\theta\left(\mathbf{u}_{i},\mathbf{u}_{j}\right)/\tau}+\sum_{j\in\mathbf{ \Phi}_{i}}e^{\theta\left(\mathbf{u}_{i},\mathbf{u}_{j}\right)/\tau}; \tag{5}\]

another strategy masks \(\mathbf{\Phi}_{i}\) within the negatives (Wang et al., 2017; Wang et al., 2017) (referred to as "Mask" strategy), i.e., modifying the "neg" term in Eq. (4) to:

\[\text{neg}(\mathbf{u}_{i})=e^{\theta\left(\mathbf{u}_{i},\mathbf{v}_{j}\right)/\tau}+\sum_{ j\neq\mathbf{\Phi}_{i}}e^{\theta\left(\mathbf{u}_{i},\mathbf{v}_{j}\right)/\tau}+\sum_{j\neq\mathbf{ \Phi}_{i}}e^{\theta\left(\mathbf{u}_{i},\mathbf{u}_{j}\right)/\tau}. \tag{6}\]

While both strategies are intuitive, their effectiveness on graphs remains to be thoroughly explored. We employ GRACE with two-layer GCN as a baseline and probe the quality of hard negative mining on three popular datasets. The results are shown in Table 1, with each value representing the average of 10 repeated runs. The different settings are explained as follows: "w/o MP" denotes GRACE without message passing, "x-hop" denotes selecting neighbors within \(x\)-hop as hard negatives, "x-hop" denotes selecting false negatives within \(x\)-hop as hard negatives, and "all labels" denotes selecting all false negatives as hard negatives. Here, 1-hop includes own neighbors for each node, while 2-hop encompasses the neighbors of neighbors and so forth. The "all labels" setting is an extreme scenario with all labels available in which \(f(\mathbf{u}_{i})\) is akin to the tuplet loss in metric learning (Wang et al., 2017; Wang et al., 2017).

Through empirical study, we make the following observations: (1) the performance of GCL significantly deteriorates in the absence of message passing; (2) "x-hop" settings provide limited benefits and may even be detrimental to GCL; (3) training under "x-hop" setting improves GCL performance; (4) "x-hop" setting with larger \(x\) leads to more noticeable performance improvements. A follow-up query

\begin{table}
\begin{tabular}{l|c|c|c|c} \hline \hline Datasets & Cora & CiteSeer & PubMed \\ \hline Settings & Positive & Mask & Positive & Mask \\ \hline GRACE & 81.05 & 71.27 & 79.57 & \\ w/o MP & 44.34 & 60.52 & 72.94 & \\
1-hop & 81.71 & 82.20 & 71.09 & 71.57 & 79.76 & 79.98 \\
2-hop & 80.68 & 81.95 & 69.43 & 71.52 & 77.41 & 78.80 \\
3-hop & 79.06 & 81.52 & 68.47 & 70.14 & 74.89 & 76.86 \\
1-hop & 82.49 & 81.83 & 71.42 & 71.06 & 80.54 & 80.31 \\
2-hop\({}^{*}\) & 84.12 & 83.49 & 72.11 & 72.19 & 81.42 & 80.96 \\
3-hop\({}^{*}\) & 86.61 & 86.15 & 74.86 & 73.52 & 83.26 & 83.20 \\ all labels & 97.57 & 94.63 & 95.93 & 93.44 & 97.01 & 95.52 \\ \hline \hline \end{tabular}
\end{table}
Table 1. Empirical study (node classification accuracy in percentage) on hard negative mining in GCL.

arises as to whether it is feasible to narrow the gap between "x-hop" and "x-hop"? Drawing insights from above observations: (2) and (3) inspire us to capture more _"precise"_ hard negatives, while (4) encourages capturing "_expansive_" hard negatives. In self-supervised scenarios, the notion of "_precise_" may appear impractical, and thus, we pivot towards the pursuit of an _"adaptive"_ solution.

We plot the average distributions of false negatives on eight most commonly used network datasets in Figure 2. Cora and Citeseer correspond to the left coordinate axis, while the rest correspond to the right coordinate axis. It can be observed that false negatives are prevalent over an expansive range. This gives rise to the following concern: on the one hand, capturing more expansive false negatives approximates the performance under "all labels" setting: on the other hand, it is essential to prevent the capture of true negatives and thus avert the occurrence of "x-hop" scenario. In other words, this is promising intuitively and entails practical risks.

For a specific anchor, its neighbors in close proximity frequently engage in message passing with it. Hence, its close neighbors are inherently hard to distance, whereas points with no message exchange with the anchor are susceptible to being pushed farther away, as shown in Figure 2(a). This also explains why the "1-hop" in Table 1 provides limited boost to the baseline. Beyond well-known graph homophily (Sundar, 2016), we employ subspace preserving techniques to address this issue. The essence behind is to mine hard negatives across the entire subspace, rather than limiting it to graph-structured neighbors. Next, we provide the brief definition of subspace preserving.

**Definition 1**.: _(Subspace Preserving) The given data \(\{x_{i}\}_{i=1}^{n}\) is drawn from a union of an unknown number \(k\) of subspaces \(\{S_{j}\}_{j=1}^{k}\) with unknown dimensions \(\{d_{i}\}_{i=1}^{k}\). \(\mathcal{S}_{j}\) is subspace preserving if \(\forall\mathbf{x}_{i}\in\mathcal{S}_{j}\) can be expressed as a linear combination of other points in \(\mathcal{S}_{j}\)._

Based on the so-called self-expressiveness property (Gardner, 2016), the coefficients representing the contribution to the anchor can be obtained by solving the optimization problem:

\[\min_{\mathbf{c}}\|z-\mathbf{H}\mathbf{c}\|_{2}^{2}+\lambda\Omega(\mathbf{c}), \tag{7}\]

where \(\mathbf{z}\in\mathbb{R}^{d}\) is the representation of the anchor, and matrix \(\mathbf{H}\in\mathbb{R}^{d\times m}\) is formed by concatenating the representations of \(m\) hard negatives of the anchor. \(\Omega\) corresponds to a specific constraint on \(\mathbf{c}\). Note that the anchor in problem (7) represents any sample and we omit subscript \(i\) for simplicity. Upon comparative analysis, we opt for elastic net (Gardner, 2016) as \(\Omega\) in this paper, which is a combination of the \(\ell_{1}\) and \(\ell_{2}\)-norms widely used in machine learning (Gardner, 2016; Gardner, 2016; Gardner, 2016). \(\ell_{1}\)-penalty encourages sparsity, while \(\ell_{2}\)-penalty promotes the connectivity. Furthermore, we expect to capture the consistent contribute from each hard negative throughout the entire process. The hard negatives selection for anchor \(z\) turns out to be:

\[\min_{\mathbf{c}}\sum_{l=1}^{L}\frac{1}{2d^{(l)}}\|\mathbf{z}^{(l)}-\mathbf{H} ^{(l)}\mathbf{c}\|_{2}^{2}+\lambda\left(\mu\|\mathbf{c}\|_{1}+\frac{1-\mu}{2} \|\mathbf{c}\|_{2}^{2}\right) \tag{8}\]

where \(L\) is the number of network layers and \(d^{(l)}\) is the dimension of the \(l\)-th layer. There is \(\mathbf{z}^{(l)}\in\mathbb{R}^{d^{(l)}}\) and \(\mathbf{H}^{(l)}\in\mathbb{R}^{d^{(l)}\times m}\). \(\lambda>0\) is the regularization parameter and \(\mu\in[0,1]\) controls the trade-off between two terms in the elastic net regularizer. As GNN performs message passing between neighbors at each layer, the subspaces at each layer may shift. Therefore, each forward propagation can be regarded as interactions between subspaces: some nodes are drawn into certain subspaces, while some are pushed out of their original subspaces. Scalar \(\frac{1}{2d^{(l)}}\) is for scale equilibrium. The interpretation of the first term in Eq. (8) is to seek consistent coefficients \(\mathbf{c}\) across training layers. In other words, if a node consistently resides in the same subspace as the anchor, it is highly likely to be a false negative of the anchor. The magnitude of this possibility depends on the magnitude of self-expression coefficients.

Due to the sparse constraints, problem (8) can not be computed in closed form by SVD. Multiple solutions are provided below.

**Full parameterization**: If each position of \(\mathbf{c}\) is considered as a parameter, then problem (8) can be solved in fully parametric way, such as Iterative Shrinkage Thresholding Algorithm (ISTA) (Bauer, 2016). Moreover, \(\mathbf{c}\) can also be solved by gradient-based training. Since each sample serves as an anchor, the number of parameters in this strategy is \(\sum_{i=1}^{n}m_{i}\). Updating these parameters during training may bring computational burdens on large-scale data.

**MLP parameterization**: In this scheme, self-expression coefficients can be computed on the lower-dimensional representations output from MLP. For example, SENet proposed in (Sundar, 2016) employs a lightweight query and key network to parameterize the self-expression coefficients. Since MLP parameters does not depend on \(n\), such methods alleviate computational overhead.

**Atentive parameterization**: Attentive models, such as GAT (Vaswani et al., 2017), presuppose varying contributions of distinct features. These models also utilize dimension-related memory to parameterize \(\mathbf{c}\).

The number of parameters in the above three ways decreases in order. Correspondingly, the expressive power decreases and the efficiency increases. Details are given in Appendix B. Since problem (8) is strongly convex, such accelerated proximal gradient method or linearized alternating direction method can be applied for seeking unique solution. Selecting non-zero indices in solution

Figure 3. Qualitative schematic of our method.

Figure 2. The number of average false negatives at each hops on eight graph datasets.

\(\mathbf{c}\) and applying strategies like Eq. (5) and (6) may help attract false negatives within the subspace. Ideally, as shown in Figure 3b, true negatives are pushed farther away while false negatives are masked or explicitly drawn closer. It encourages the emergence of clear class boundaries, i.e. the golden hand is stretched in Figure 3b. While the solution process is straightforward, problem (8) itself may not be static; in other words, the pre-selected matrix \(H\) may not be optimal. This naturally prompts the question: how is the hard negatives set \(\mathbf{H}\) selected? Moreover, when dealing with large-scale data, it is extravagant to employ the self-representation of all samples on one single sample, we use a subset instead. Instead, we necessitate the adaptive selection of a subset.

Therefore, we aim to seek an adaptive matrix \(\mathbf{H}\) which can be self-scaled during the training process. The selected indices are expected to effectively preserve hard negative samples without becoming excessively large and causing training difficulties. Remark that problem (8) is independent for each sample. Next we introduce the definition of Adaptive Hard Negative Set for individual anchor.

**Definition 2**: _(Adaptive Hard Negative Set) Assume \(\tilde{\mathbf{c}}(\mathbf{H})\) is the optimal solution of problem (8) with the \(i\)-th sample as the anchor. \(\mathbf{\Phi}\) is the adaptive hard negatives set of the \(i\)-th sample if the following conditions are satisfied:_

\[\begin{split}(a)&\forall j\in\Phi,\;\tilde{\mathbf{c}}^ {T}\left(\left[\mathbf{H},\mathbf{z}_{j}\right]\right)=\left[\tilde{\mathbf{c}}^{T}\left( \mathbf{H}\right),0\right],\\ (b)&\forall j\notin\mathbf{\Phi},\;\tilde{\mathbf{c}}^{T} \left(\left[\mathbf{H},\mathbf{z}_{j}\right]\right)=\left[\mathbf{q}^{T}\left(\mathbf{H}\right),\mathbf{\alpha}_{j}\right],\end{split} \tag{9}\]

_where \(\left[\mathbf{q}^{T}\left(\mathbf{H}\right),\mathbf{\alpha}_{j}\right]\) denotes the solution vector with scalar \(\mathbf{\alpha}_{j}\neq 0\)._

The interpretation of this definition is intuitive: \(j\) within \(\Phi\) make a contribution to the self-expression of the anchor (i.e., the optimal corresponding coefficient \(\mathbf{\alpha}_{j}\) are not zero), while \(j\) outside \(\Phi\) will not (i.e., the corresponding optimal coefficient equals to zero). In training, \(\mathbf{\Phi}\) can be ascertained via the following theorem.

**Theorem 1**: _Assume \(\tilde{\mathbf{c}}(\mathbf{H})\) is the optimal solution of problem (8) with the \(i\)-th sample as the anchor. The auxiliary function is defined as_

\[g(j)=\sum_{l=1}^{L}\frac{1}{d(l)}\mathbf{z}_{j}^{(l)T}\left(\mathbf{z}_{i}^{(l)}-\mathbf{H }^{(l)}\tilde{\mathbf{c}}(\mathbf{H})\right). \tag{10}\]

_Then hard negatives set can be computed by \(\mathbf{\Phi}=\left\{j\mid|g(j)|>\lambda\mu\right\}.\)_

We can now give an understanding of what kind of samples are "hard" for a given anchor in the subspace framework. Theorem 1 implies that a sample is indispensable for subspace preserving if its representation sufficiently resembles the residual of existing self-expression. This diverges from homophily and similarity-based methods. Hence, our method exhibits "adaptive" in two aspects: On the one hand, as evident from the proof, it is clear that \(\mathbf{\epsilon}_{j}=0\) is equivalent to \(j\notin\mathbf{\Phi}\). Therefore, it can be removed from the adaptive hard negative set by updating \(\mathbf{\Phi}\) once. On the other hand, throughout the training process, updating \(\mathbf{\Phi}\) continuously expands the hard negatives set for the \(i\)-th sample. The distributional shifts from contrastive loss make it possible for \(\mathbf{\Phi}\) to capture long-range hard negatives.

Inspired by the OMP in dictionary coding [56], we aim for the gradual expansion of \(\mathbf{\Phi}\) with the training process. Beginning with an initial set, \(\mathbf{\Phi}\) can be periodically updated every few epochs to reduce additional time overhead while capturing expansive hard negatives. In addition, to avoid large-scale computations, the size of \(\mathbf{\Phi}\) can be controlled by confining hard negatives within a specified \(K\)-hop radius. The hyperparameter \(K\) dictates the range of selectable hard negatives.

Combining the solutions of all subproblems, the self-expression matrix can be defined as \(\mathcal{C}=\left[\mathbf{c}_{1},\cdots,\mathbf{c}_{n}\right]\), where \(C_{ji}\) reflects the hardness of \(j\) with respect to \(i\). To incorporate the subspace information into the contrastive loss, the self-expression coefficients \(C_{ij}\) is supposed to be mapped to the probability that \(j\) serves as a false negative for \(i\). This can be done through either a softmax operation or a linear mapping as follows:

\[(a)\;S_{ij}=\frac{\exp\left(\left|C_{ij}\right|/\sigma\right)}{\sum_{k\in \mathbf{\Phi}_{j}}\exp\left(\left|C_{kj}\right|/\sigma\right)},\quad(b)\;S_{ij}= \min\left\{\frac{\left|\mathbf{\epsilon}_{ij}\right|}{\zeta},\rho\right\}. \tag{11}\]

\(S_{ij}\) in \((a)\) satisfies probabilistic properties and \(\sigma\) is tunable. \(S_{ij}\) in \((b)\) is proportionally scaled from \(C_{ij}\), where \(\zeta\) is the maximum value within a sampled subset \(\left\{C_{ij}\right\}_{(i,j)}\). The truncated parameter \(\rho\) controls the ceiling of \(S_{ij}\) and is set to \(1\) by default. Thus, numerous values of \(S_{ij}\) in equation \((b)\) can be equivalent to \(\rho\). In turn, \(S\) can be

Figure 4: The model architecture of GRAPE. The two views are generated through data augmentation of the initial graph. These three are fed into the parameter-sharing GNN encoder, where the projection header is alternative. The hard negatives set and the corresponding subspace coefficients \(\mathbf{C}\) are computed within the middle pathway. The green line in contrastive loss indicates hard negatives while the red line indicates true negatives, which are vary across epochs.

symmetrized by \((S^{T}+S)/2\). Upon obtaining \(S\), two schemes can be straightforwardly developed to enhance the performance of GCL. \(\mathbf{GRAPE}_{mask}\): GRACE in Eq. (2) treats all samples except itself as negatives, whose negatives set for anchor \(i\) can be denoted as \(\tilde{\mathcal{N}}_{i}=[n]\setminus\{i\}\). While GRAPE estimates negatives' hardness and obtains the probability \(S\) for false negatives in turn, it can subsequently excluded the highly probable false negatives from \(\tilde{\mathcal{N}}_{i}\). Specifically, in each epoch, \(j\) is included in the false negatives set \(\mathcal{T}_{i}\) for anchor \(i\) with a probability of \(S_{ji}\). The negatives set in this case turns out to be \(\mathcal{N}_{i}=\tilde{\mathcal{N}}_{i}\setminus\mathcal{T}_{i}\). Therefore, the objective for \(\mathbf{u}_{i}\) in GRAPE\({}_{mask}\) is defined as:

\[\ell_{mask}\left(\mathbf{u}_{i}\right)=\] \[-\log\frac{e^{\theta\left(\mathbf{u}_{i},\mathbf{u}_{i}\right)/\tau}}{e^ {\theta\left(\mathbf{u}_{i},\mathbf{u}_{i}\right)/\tau}+\sum_{j\in\mathcal{N}_{i}} \left(e^{\theta\left(\mathbf{u}_{i},\mathbf{u}_{j}\right)/\tau}+e^{\theta\left(\mathbf{u} _{i},\mathbf{u}_{j}\right)/\tau}\right)}, \tag{12}\]

\(\mathbf{GRAPE}_{pos}\): GRACE in Eq. (2) exclusively treats itself as positives, whose positives set for anchor \(i\) is \(\tilde{\mathcal{P}}_{i}=\{i\}\). For anchor \(i\), GRAPE\({}_{pos}\) incorporates \(j\) into the positives set with a probability of \(S_{ji}\) each epoch. The expanded positives set is denoted as \(\mathcal{P}_{i}\). Therefore, the objective for \(\mathbf{u}_{i}\) in GRAPE\({}_{pos}\) is defined as:

\[\ell_{pos}\left(\mathbf{u}_{i}\right)=\] \[-\log\frac{e^{\theta\left(\mathbf{u}_{i},\mathbf{u}_{i}\right)/\tau}+\sum _{k\in\mathcal{P}_{i}}\left(e^{\theta\left(\mathbf{u}_{i},\mathbf{u}_{k}\right)/\tau}+ e^{\theta\left(\mathbf{u}_{i},\mathbf{u}_{k}\right)/\tau}\right)}{e^{\theta\left(\mathbf{u}_{i}, \mathbf{u}_{k}\right)/\tau}+\sum_{j\neq i}\left(e^{\theta\left(\mathbf{u}_{i},\mathbf{u}_ {j}\right)/\tau}+e^{\theta\left(\mathbf{u}_{i},\mathbf{u}_{j}\right)/\tau}\right)}. \tag{13}\]

It is noteworthy that loss (13) is a variant of MIL-NCE (Miyato et al., 2018). Optimizing loss (13) enhances the overall similarity of positive pairs relative to negative pairs, rather than focusing on instance-specific distances.

Similar to GRACE, the overall contrastive loss is given as:

\[\mathcal{L}_{mask/pos}=\frac{1}{2n}\sum_{i=1}^{n}\left(\ell_{mask/pos}\left( \mathbf{u}_{i}\right)+\ell_{mask/pos}\left(\mathbf{o}_{i}\right)\right). \tag{14}\]

Reviewing the results in Table 1, we can empirically summarize that \(\text{Grape}_{pos}\) are suitable for high-confidence false negatives, while \(\text{Grape}_{mask}\) tolerates low-confidence false negatives. Therefore, \(\text{Grape}_{mask}\) is deemed as a more robust scheme. The model architecture is presented in Figure 4 and the procedure for GRAPE is detailed in Appendix A.

### Theoretical Analysis

**Why GRAPE works?** Reflecting on our motivation: we aim to identifies expansive and adaptive hard negatives as false negatives, which appears to be empirically derived. The essence behind this is the message-passing in GNNs: neighbors that encompass a substantial proportion of shared connections are not unduly distanced from each other. Therefore, local hard negative mining yield limited benefits. Recall the results in Table 1 that \(1\)-hop\({}^{*}\) (even \(2\)-hop\({}^{*}\)) does not significantly boost the baseline, while \(3\)-hop\({}^{*}\) shows a leap, which interprets the pursuit of expansive hard negatives. Besides, the self-expression loss can be expanded as follows

\[\min_{\mathbf{c}}\|\mathbf{z}-\mathbf{H}\mathbf{c}\|_{2}^{2}\Leftrightarrow\min_{\mathbf{c}}-2 \sum_{i}\mathbf{z}^{T}\mathbf{h}_{i}\mathbf{c}_{i}+\sum_{i,j}\mathbf{h}_{i}^{T}\mathbf{h}_{j}\mathbf{c }_{i}\mathbf{c}_{j}. \tag{15}\]

The first term endows larger self-expression coefficients for negatives similar to the anchor, while the second term endows smaller coefficients for those highly similar to the other negatives. In GCL, the second term implies that the contributions of those involved in message passing with other hard negatives are diminished in self-expression, which is consistent with the intent in Figure 3a. This is rooted in its capacity to capture global long-range interactions, as discussed in (Zhu et al., 2018). Moreover, the regularizer in Eq. (8) exhibits sparsity as \(\mu\) approaches \(1\) and group effect as \(\mu\) approaches \(0\). It is worth noting that \(\lambda\) and \(\mu\) directly impact the tightness of hard negative selection. GRAPE with large values of \(\lambda\) and \(\mu\) results in a small hard negatives set.

By iteratively updating self-expression coefficients during training, the efficacy of GRAPE loss is qualitatively described as follows:

**Proposition 1**.: _In cases where GRAPE captures hard negatives \(\{\Phi_{i}\}_{i=1}^{n}\) within each individual subspace, both \(\mathcal{L}_{mask}\) and \(\mathcal{L}_{pos}\) contribute to the inter-subspace separation and intra-subspace cohesion._

Furthermore, GRAPE is associated with various methods, such as graph attention (Zhu et al., 2018), nonlinear latent subspace clustering (Zhu et al., 2018), and uniformity-tolerance dilemma (Zhu et al., 2018), as revealed in Appendix D.

**Maximizing mutual information** The improvement of GRAPE over the baseline can also be elucidated from the perspective of maximizing Mutual Information (MI):

**Theorem 2**.: _The contrastive loss in Eq. (14) gives a stricter lower bound of MI between input features \(\mathbf{X}\) and embeddings in two views \(\mathbf{U}\) and \(\mathbf{V}\), compared with the contrastive loss \(\mathcal{L}\) in Eq. (3) proposed by GRACE. This can be written formally as_

\[-\mathcal{L}<-\mathcal{L}_{mask/pos}\leqslant\mathcal{I}\left(\mathbf{X};\mathbf{U}, \mathbf{V}\right) \tag{16}\]

Therefore, maximizing GRAPE loss corresponds to optimizing a more rigorous lower bound for the mutual information between node features and the acquired node representations, thereby furnishing a theoretical justification for the performance enhancement.

**Complexity Analysis** The procedure for GRAPE is detailed in Appendix A. Compared to our baseline, GRACE, extra complexity arises from the periodic updating of hard negatives set \(\{\mathbf{\psi}_{i}\}_{i=1}^{n}\) and the computation of self-expression coefficients \(\{\mathbf{c}_{i}\}_{i=1}^{n}\) every _intl_ epochs. Each of these \(n\) independent subproblems can be solved concurrently in parallel. The additional time overhead is \(\mathcal{O}(Md)\), where \(M\) represents the largest cardinality within \(\{\Phi_{i}\}_{i=1}^{n}\). Since the hard negatives sets are restricted within the \(K\)-hop, there is \(M\ll n\). Therefore, the additional time overhead is manageable.

## 4. Experiments

### Experimental Protocol

We conducted comparisons between GRAPE and ten advanced methods on eight node prediction datasets. The benchmark graph datasets include: **Cora**, **CiteSeer**, **PubMed**, **Wiki** CS, **Amazon Photo**, **Amazon Computers**, **Coauthor CS**, **Coauthor Physics**. They are all hosted by DGI. package 1. The dataset information is detailed in Appendix E.1. The comparative methods include: two supervised baselines (**GCN**(32), **GAT**(71)), two autoencoder-based baselines (**GAE**(33), **VGAE**(33)), eight state-of-the-art GCL methods (**DGI**(72), **GMI**(57), **MVGRL**(21), **GRACE**(109), **CCA-SSG**(97), **BGRL**(67), **ProGCLW**(84), **COSTA\({}_{MV}\)(102)).

For all augmentation-based methods, we adopt the most commonly used strategies for the graph augmentation: "edge removing" and "feature masking" (Kumar et al., 2018). At each epoch, "edge removal" randomly removes a certain proportion of edges from the original graph, while "feature masking" randomly masks a certain proportion of features. To be consistent with the comparison method, we configure the GNN encoder as a two-layer GCN. Self-supervised training is conducted on the entire graph and on the features of all samples. The embeddings obtained are fed into a semi-supervised linear classifier to get the final result. For Cora, CiteSeer, and PubMed datasets, we employ the standard split settings: 20 nodes per class are available for training, 500 nodes for validation and 1000 for testing. For the other datasets, we randomly assign 10% of the nodes for training, another 10% for validation, and allocate the remaining 80% for testing. The linear classifier is uniformly set to be a simple regularized logistic regression. The overall model is trained using the Adam optimizer.

We implement our GRAPE based on GRACE. The max training epoch is set to 100. The dimensions in the two-layer GNN encoder are set to 512 and 256, respectively. The learning rate for GRAPE is set to \(1\times 10^{-3}\), while that for linear classifiers is set to \(1\times 10^{-2}\). The interest for updating \(C\)_intol_ is fixed to 5 and the truncated parameter \(\rho\) is fixed to 1. Our graph augmentation is achieved through a combination of 40% edge removal and 10% feature masking. The trade-off parameter \(\lambda\) is selected within \(\{10^{-1},10^{0},10^{1},10^{2}\}\) and \(\mu\) is selected within \(\{0,0.1,\cdots,0.9,1.0\}\). The temperature parameter \(\tau\) is selected within \(\{0,1,0.2,\cdots,1.0\}\) and the range of hard negatives \(K\) is selected within \(\{1,\cdots,5\}\). For all comparative methods, we adhere to the authors' default parameter settings and, where necessary, conduct parameter grid searches to achieve fair comparisons. Their implementations are all open-sourced. All experiments are conducted on NVIDIA RTX A6000 GPU with 48GB memory.

### Main Results

The node classification results are presented in Table 2. The reported results are averaged over 10 runs with random seeds. The "Input" refers to data for training, where \(\mathbf{X}\), \(\mathbf{A}\) and \(\mathbf{Y}\) denotes feature matrix, adjacency matrix and label matrix respectively. OOM denotes out of memory. It can be observed that GRAPE achieves the state-of-the-art self-supervised performance on the first six datasets and surpasses the performance of supervised baselines (GCN, GAT) on the first seven datasets. Compared to its baseline GRACE, GRAPE achieves a comprehensive improvement. The hyperparameters involved in the experiment are listed in Appendix E.2. We perform node clustering performance evaluations in the completely unsupervised case in Appendix F.1. In addition, we execute comparative experiments on heterophily graphs (where connections primarily occur between dissimilar nodes) and the results are presented in Appendix F.2. These results corroborate GRAPE's capacity for precise identification of false negatives.

### How GRAPE Affects Training?

Subsequently, we conduct empirical investigations to explore what properties GRAPE learns and what hard negatives it captures. The experiments below are based on GRAPE\({}_{mask}\). Figure 4(a) illustrates the evolution of the percentage of false negatives within the hard negatives set \(\{\mathbf{\Phi}_{i}\}_{i=1}^{n}\) throughout the training process. Figure 4(b) depicts the distribution of hard negatives set over different hops after training. Despite the expansion of the hard negatives set, the proportion of false negatives within it scarcely declines. Besides, a substantial portion of the hard negatives set consists of large-hop neighbors. Both observations imply that we achieve expansive yet dependable sets of hard negatives through subspace preservation.

Wang et al. (2019) introduced the concept of uniformity-tolerance dilemma in contrastive representation. We employ the two metrics to showcase the difference between GRAPE and its baseline GRACE. Specifically, the cohesion (CO) and uniformity (UN) of the learned

\begin{table}
\begin{tabular}{l c c c c c c c c c} \hline \hline
**Methods** & **Input** & **Cora** & **CiteSeer** & **PubMed** & **Wiki CS** & **Am-Photo** & **Am-Computer** & **Co-CS** & **Co-Physics** \\ \hline GCN & \(\mathbf{X},\mathbf{A},\mathbf{Y}\) & \(81.32\pm 0.5\) & \(70.84\pm 0.7\) & \(77.69\pm 0.3\) & \(76.85\pm 0.1\) & \(92.16\pm 0.2\) & \(87.06\pm 0.5\) & \(92.54\pm 0.3\) & \(95.65\pm 0.2\) \\ GAT & \(\mathbf{X},\mathbf{A},\mathbf{Y}\) & \(82.57\pm 1.0\) & \(71.96\pm 1.0\) & \(77.51\pm 0.3\) & \(78.35\pm 0.1\) & \(91.45\pmembeddings can be defined as follows:

\[\text{CO}=\sum_{g_{i}=g_{j}}\left(f^{T}(\mathbf{x}_{i})f(\mathbf{x}_{j})\right),\text{ UN}=\sum_{i,j}\exp\left(-f^{T}(\mathbf{x}_{i})f(\mathbf{x}_{j})\right) \tag{17}\]

\(f\) denotes our GNN encoder \(f_{\mathbf{G}}(\mathbf{A},\mathbf{X})\). A higher CO implies higher intra-class cohesion, while a higher UN implies a more uniform embedding distribution. The comparison of the two metrics for GRAPE and GRACE during training is depicted in Figure 6.

At the beginning of training, CO and UN for both GRAPE and GRACE are nearly identical due to the similar initialization. As discussed ahead, GRAPE explicitly or implicitly brings the representations inside the same subspace closer, which strengthens the intra-class cohesion and reduces the global uniformity. With the expansion of the hard negatives set, the margin of cohesion between GRAPE and GRACE is enlarged, which is in line with our original intention. Since the mask mechanism of GRAPE\({}_{mask}\) is presented in a probabilistic form, uniformity doesn't exhibit significant decreases compared to GRACE. Additionally, Figure 7 shows how the test accuracy steadily improves as the GRAPE loss is optimized.

Furthermore, the efficiency of GRAPE can be enhanced through aforementioned the scalable parameterization or by integration into negatives-independent methods. The corresponding results are provided in Appendix F.3 and F.4.

### Visualization and Hyperparameter Study

In this subsection, we present intuitive results to illustrate the effectiveness of GRAPE. Figure 7 shows the distributions of the true/false negatives of the same anchor in different phases on Cora. The horizontal axis denotes the cosine similarity between negatives and anchor, which is non-negative due to the ReLU before output. The variation with training is discernible, especially from (b) to (c), validating the efficacy of adaptive hard negative selection.

We present t-SNE visualization of GRAPE's running results without labels (i.e., before classification). As depicted in Figure 9, nodes are partitioned into multiple distinct clusters.

The influence of the hyperparameters in GRAPE is examined to validate the feasibility. The sensitivity analysis of the two trade-off parameters in Eq. (8) is depicted in Figure 10. The test accuracy of GRAPE remains stable across a wide range of \(\mu\) and \(\lambda\), indicating its independence from meticulous parameter settings. Simultaneously, both parameters indeed exert an influence on the model. Besides, the parameter analyses of GRAPE under different interval \(intl\) and range \(K\) are shown in Appendix F.5.

## 5. Conclusion

In this paper we propose a novel method for estimating negatives' hardness in GCL. Our method emphasizes the potential in exploring expansive and adaptive negatives. These two goals are coupled in our subspace preserving scheme. We elucidate the motivation, provide empirical and theoretical underpinnings and conduct comprehensive experiments to dissect the effectiveness of GRAPE. Drawing from the contributions of this paper, we hopefully point out two interesting and promising avenues for further research. First, since subspace theory is not directly reliant on existing connections, it shows potential in addressing the impact of noisy, incomplete, or vulnerable graph structures on GNNs (a branch called graph structure learning). Second, self-expression contribute to preserving local structures and may serve as a form of constraint to slow down message passing for deeper GNNs.

Figure 8. Negatives distributions in different phases.

Figure 6. Variation of cohesion and uniformity.

Figure 7. Variation of loss and test accuracy with training.

Figure 9. Visualization of node embedding without labels.

Towards Espanuive and Adaptive Hard Negative Mining:

Graph Contrastive Learning via Subspace Preserving

## References

* A. Beck and M. Teboulle (2009)A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM J. Imaging Sci. 2 (1), pp. 183-202. Cited by: SS1.
* M. Ishmedel Belghair, A. Barani, S. Beijshaw, S. Ozair, Y. Bengio, A. Courville, and D. Hjila (2018)Mutual information neural estimation. In ICML, pp. 531-540. Cited by: SS1.
* F. B. Bellemaur, J. P. Hespanha, and D. J. Kriegman (1997)Eigenfaces vs. fisherfaces recognition using class specific linear projection. IEEE Trans. Pattern Anal. Intell.19 (9), pp. 7711-7720. Cited by: SS1.
* X. Cao, C. Zhang, H. Fu, S. Liu, and H. Zhang (2015)Diversity-induced multi-view subspace clustering. In CVPR, pp. 586-594. Cited by: SS1.
* S. Chen, G. Song, L. Li, J. Yang, and M. Sugiyama (2021)Large-margin contrastive learning with distance polarization regularizer. In ICML, pp. 1673-1683. Cited by: SS1.
* T. Chen, S. Kornblith, M. Norouzi, and G. Hinton (2020)A simple framework for contrastive learning of visual representations. In ICML, pp. 1557-1667. Cited by: SS1.
* X. Chen and K. He (2021)Exploring simple siamese representation learning. In CVPR, pp. 15750-15758. Cited by: SS1.
* C. Chuang, J. Robinson, Y. Lin, A. Torralba, and S. Jegelka (2020)Debiased contrastive learning. In NeurIPS, Vol. 33, pp. 8765-8775. Cited by: SS1.
* J. Devlin, M. Chang, K. Lee, and K. Toutanova (2018)Bert: pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04808. Cited by: SS1.
* A. Dosovitskiy, L. Beyer, A. Koloszhov, D. Weissenborn, S. Zhai, T. Uszkoreit, M. Dehghani, M. Minderer, G. Heigelwal, S. Gelly, et al. (2020)An image is worth 16x16 words: transformers for image recognition at scale. In ICLR, Cited by: SS1.
* D. Dwibedi, Y. Afyat, J. Tompson, P. Sermanet, and A. Zisserman (2020)With a simple help from my friend: nearest-neighbor contrastive learning of visual representations. In CVPR, pp. 5883-5997. Cited by: SS1.
* E. Elhamifar and R. Vidal (2013)Sparse subspace clustering: algorithms, theory, and applications. IEEE Trans. Pattern Anal. Mach. Intell.35 (11), pp. 2765-2788. Cited by: SS1.
* Y. Fang, R. Wang, B. Dai, and X. Wu (2014)Graph-based learning via auto-grouped sparse regularization and kernelized extension. IEEE Trans. Knowl. Data Eng.27 (1), pp. 1426-154. Cited by: SS1.
* T. Gao, X. Tang, and D. Chen (2021)SimCSE: simple contrastive learning of sentence embeddings. In EMNLP, pp. 6894-6910. Cited by: SS1.
* J. Gaettiger, S. Weinberger, and S. Gunnemann (2019)Diffusion improves graph learning. In NeurIPS, Vol. 32. Cited by: SS1.
* L. Gong, S. Zhou, W. Tu, and X. Lin (2022)Attributed graph clustering with dual redundancy reduction. In IJCAI, Cited by: SS1.
* J. Grill, F. Strath, P. Altich, C. Tallere, P. Hebeamsand, E. Buchatskaya, C. D. Derex, A. Paris, Z. Guo, M. Chellaghij, A. Azar, et al. (2020)Boosting your own latent-a new approach to self-supervised learning. In NeurIPS, Vol. 33, pp. 2121-21284. Cited by: SS1.
* M. U. Otum and A. Roy (1992)Noise-contrasting estimation of unnormalized statistical models. In Applications to Natural Image Statistics, Cited by: SS1.
* R. Hadsell, S. Chopra, and Y. LeCun (2006)Dimensionality reduction by learning an invariant mapping. In CVPR, Vol. 21, pp. 1735-1742. Cited by: SS1.
* W. Hamilton, J. Ying, and J. Leskovec (2017)Inductive representation learning on large graphs. In NeurIPS, Vol. 30. Cited by: SS1.
* K. Hassani and A. Hoshenshmadi (2020)Constrative multi-view representation learning on graphs. In ICML, pp. 4116-4126. Cited by: SS1.
* K. He, X. Zhang, S. Ren, and J. Sun (2016)Deep residual learning for image recognition. In CVPR, pp. 770-778. Cited by: SS1.
* X. He and P. Niyogi (2003)Locality preserving projections. In NeurIPS, Vol. 16. Cited by: SS1.
* R. Hejian, L. Fedorov, S. Lavoie-Marshol, K. Grewal, P. Bachman, A. Trischler, and Y. Bengio (2018)Learning deep representations by mutual information estimation and maintaining. In ICLR, Cited by: SS1.
* H. Hu, Z. Lin, J. Feng, and J. Zhou (2014)Smooth representation clustering. In CVPR, pp. 3834-3841. Cited by: SS1.
* A. Jin and J. Zongber (1997)Feature selection: evaluation, application, and small sample performance. IEEE Trans. Pattern Anal. Mach. Intell.19 (2), pp. 153-158. Cited by: SS1.
* Y. Jin, Y. Guo, J. Zhang, Y. Zhang, T. Zhang, and T. Zhu (2020)SAB depth contrast for scalable self-supervised graph representation learning. In ICML, pp. 222-231. Cited by: SS1.
* Y. Kalantidis, M. R. Sariyildiz, N. Pion, P. Weinzaepfel, and D. Larlus (2020)Hard negative mixing for contrastive learning. In NeurIPS, Vol. 33, pp. 21798-21809. Cited by: SS1.
* M. Kheirandishfard, F. Zohrizadeh, and F. Kannagur (2020)Deep low-rank subspace clustering. In CVPR, pp. 864-865. Cited by: SS1.
* P. Khosla, P. Teterwish, C. Wang, A. Sarna, Y. Tian, P. Isola, A. Machino, C. Li, and D. Krishnan (2020)Supervised contrastive learning. In NeurIPS, Vol. 3, pp. 18661-18673. Cited by: SS1.
* T. N. Kipf and M. Welling (2016)Semi-supervised classification with graph convolutional networks. In ICLR, Cited by: SS1.
* N. Kipf and M. Welling (2016)Semi-supervised classification with graph convolutional networks. In ICLR, Cited by: SS1.
* N. Kipf and M. Welling (2016)Semi-supervised classification with graph convolutional networks. In ICLR, Cited by: SS1.
* S. Kou, X. Yin, T. Wang, S. Chen, T. Chen, and Z. Wu (2023)Structure-aware subspace clustering. IEEE Trans. Knowl. Data Eng.20 (2), pp. 205-2088. Cited by: SS1.
* H. Lee, A. Battle, R. Rain, and A. Ng (2006)Efficient sparse coding algorithms. In NeurIPS, Vol. 19. Cited by: SS1.
* K. Lee, Y. Zhu, X. Schuh, C. Li, J. Shi, and H. Lee (2020)I-mix: a domain-agnostic strategy for contrastive representation learning. In ICLR, Cited by: SS1.
* N. Lee, L. Lee, and C. Park (2022)Augmentation-free self-supervised learning on graphs. In AAAI, Vol. 36, pp. 7727-780. Cited by: SS1.
* W. Li, C. Dong, W. Fu, and J. Lai (2023)Hon-moCEL: rethinking homophily in graph contrastive learning. arXiv preprint arXiv:2306.0614. Cited by: SS1.
* Z. Li, J. Yang, H. Yang, and H. Lu (2015)Robust structured subspace learning for data representation. IEEE Trans. Pattern Anal. Mach. Intell.37 (10), pp. 2055-2098. Cited by: SS1.
* R. Liao, Y. Li, Y. Song, S. Wang, W. Hamilton, D. K. Duvenaud, R. Urban, and E. Zemel (2019)Efficient graph generation with graph recurrent attention networks. In NeurIPS, Vol. 32. Cited by: SS1.
* G. Lin, Z. Lin, S. Zhuo, J. Sun, Y. Yu, and Y. Ma (2012)Robust recovery of subspace structures by low-rank representation. IEEE Trans. Pattern Anal. Mach. Intell.35 (1), pp. 171-184. Cited by: SS1.
* G. Lin and S. Tan (2011)Latent low-rank representation for subspace segmentation and feature extraction. In ICCV, pp. 1615-1622. Cited by: SS1.
* N. Liu, X. Wang, D. Chen, S. Shi, and J. Pei (2022)Howeing graph contrastive learning from the perspective of graph spectrum. In NeurIPS, Vol. 35, pp. 2972-2983. Cited by: SS1.
* W. Liu, J. Xie, C. Zhang, M. Yamada, N. Zheng, and H. Qian (2022)Robust graph dictionary learning. In ICLR, Cited by: SS1.
* Y. Liu, L. Jing, S. Yan, C. Zhou, Y. Zheng, F. Xia, and S. Yu (2022)Graph self-supervised learning: a survey. IEEE Trans. Knowl. Data Eng.35 (2), pp. 8879-900. Cited by: SS1.
* Y. Liu, L. Jing, S. Zhou, X. Liu, Z. Wang, K. Liang, W. Tu, L. Li, J. Duan, and C. Chen (2023)Hard sample aware network for contrastive deep graph clustering. In AAAI, Vol. 39, pp. 9914-9922. Cited by: SS1.
* C. Li, J. Feng, Z. Liu, T. Mei, and S. Tan (2018)Sub-space clustering by block diagonal representation. IEEE Trans. Pattern Anal. Mach. Intell.41 (2), pp. 487-501. Cited by: SS1.
* C. Lu, H. Min, Z. Qi, L. Zhu, D. Huang, and S. Yan (2012)Robust graph self-efficient subspace segmentation via least sparse regression. In ICCV, pp. 347-360. Cited by: SS1.
* J. Maat, J. Ponce, G. Sapiro, A. Nisserman, and F. J. Bach (2008)Supervised dictionary learning. In NeurIPS, Vol. 21. Cited by: SS1.
* M. McPherson, L. Smith-Lovin, and J. M. Cook (2001)Birds of a feather: homophily in social networks. Annual review of sociology27 (1), pp. 415-444. Cited by: SS1.
* A. Michel, J. Mayzoc, L. Smariva, I. Laptev, J. Sivic, and A. Zisserman (2020)End-to-end learning of visual representations from uncurated instructional videos. In CVPR, pp. 9879-9889. Cited by: SS1.
* A. Ng, M. Jordan, and Y. Weiss (2001)On spectral clustering: analysis and an algorithm. In NeurIPS, Vol. 14. Cited by: SS1.
* A. van den Oord, Y. Li, and O. Vinyals (2018)Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03708. Cited by: SS1.
* J. Park, M. Lee, H. Jin, G. Kiyewang Lee, and J. Yoo (2019)Symmetric graph convolutional autoencoder for unsupervised graph representation learning. In CVPR, pp. 6519-6525. Cited by: SS1.
* V. M. Patel, H. Nawe, and B. Vitali (2013)Latent space sparse subspace clustering. In ICCV, pp. 2235-2322. Cited by: SS1.
* Y. Chandra Pati, R. Rezaiifar, and P. Sambamurthy Krishnaprasad (1993)Orthogonal matching pursuit: generative function approximation with applications to wavelet decomposition. In Conf. Asilomar Conf. Systs Syst., pp. 40-44. Cited by: SS1.
* Z. Peng, W. Huang, M. Luo, Q. Zheng, Y. Rong, T. Xu, and J. Huang (2020)Graph representation learning via graphical mutual information maximization. In WWW, pp. 259-270. Cited by: SS1.
* X. Qian, H. Feng, G. Zhao, and T. Mei (2013)Personalized recommendation combining user interest and social circle. IEEE Trans. Knowl. Data Eng.26 (7), pp. 1763-1777. Cited by: SS1.

* Qiu et al. (2020) Jiezhong Qiu, Qihu Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding, Kunusan Wang, and Jie Tang. 2020. Gc: Graph contrastive coding for graph neural network pre-training. In _KDD_. 1150-1160.
* Robinson et al. (2020) Joshua David Robinson, Ching-Yao Chuang, Svarti Sra, and Stefanie Jegelka. 2020. Contrastive Learning with Hard Negative Samples. In _ICLR_.
* Roweis and Saul (2000) Sam T Roweis and Lawrence K Saul. 2000. Nonlinear dimensionality reduction by locally linear embedding. _Science_. 2001. 5500 (2000), 2323-2326.
* Sun et al. (2019) Kihylu Sun, Idenjoo L Improved deep metric learning with multi-class n-pair loss objective. In _NeurIPS_, Vol. 29.
* Sun et al. (2019) Tan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. 2019. Infograpk: Unsupervised and semi-supervised graph-level representation learning via mutual information maximization. _arXiv preprint arXiv:1908.00100_ (2019).
* Sun et al. (2023) Qingqing Sun, Wenjie Zhang, and X-emun Lin. 2023. Progressive Hard Negative Masking: From Global Uniformity to Local Tolerance. _IEEE Trans. Knowl. Data Eng._ (2023).
* Suresh et al. (2021) Suresh Suresh, Pan Li, Cong Hao, and Jennifer Neville. 2021. Adversarial graph augmentation to improve graph contrastive learning. In _NeurIPS_, Vol. 34. 1559-15933.
* Tenenbaum et al. (2000) Joshua Tenenbaum, Vin de Silva, and John C Langford. 2000. A global geo-metric framework for nonlinear dimensionality reduction. _Science_. 2000. 5500 (2000), 2319-2323.
* Thakoor et al. (2021) Shantam Thakoor, Corcorin Tatale, Mohammad Gheishdasar Auch, Mehdi Ashouk, Eva L Dyer, Remi Munos, Petar Velickovic, and Michal Valko. 2021. Large-Scale Representation Learning on Graphs via bootstrapping. In _CIK_.
* Tian et al. (2020) Yonglong Tian, Chen Sun, Ben Poole, Dizh Krishnan, Cordelia Schmid, and Phillip Isola. 2020. What makes for good views for contrastive learning?. In _NeurIPS_, Vol. 33. 2687-6839.
* Tschannen et al. (2019) Michael Tschannen, Joup Dolong, Paul K. Rubenstein, Sylvain Gelly, and Mario Lucic. 2019. On Mutual Information Maximization for Representation Learning. In _ICLR_.
* Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In _NeurIPS_, Vol. 30.
* Velickovic et al. (2018) Petar Velickovic, Gillen Cucurull, Arunta Cansova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2018. Graph Attention Networks. In _ICLR_.
* Velickovic et al. (2018) Petar Velickovic, William Fedus, William I. Hamilton, Pietro Lio, Yoshua Bengio, and R Devon Arjban. 2018. Deep Graph Informs. In _ICLR_.
* Vapal (2021) Petre Vapal. 2021. Attention: Self-Expressions in 31st QoN word. (2021).
* Vincent-Cinten et al. (2021) Cedric Vincent-Cinten, Tueu Vapet, Hamuyang Marc, Concad, and Nicolas Courty. 2021. Online graph dictionary learning. In _ICML_. 10564-10574.
* Wang and Liu (2021) Teng Wang and Hauging Liu. 2021. Understanding the behaviour of contrastive loss. In _CVPR_. 2895-2950.
* Wang et al. (2022) Huiqiang Wang, Junqi Peng, Peihu Huang, Jince Wang, Junhui Chen, and Yifei Xia. 2022. Micr: Multi-scale local and global context modeling for long-term series forecasting. In _ICLR_.
* Wang and Xu (2013) Yu-Xiang Wang and Huan Xu. 2013. Noisy sparse subspace clustering. In _ICML_. 98-97.
* Weston et al. (2003) Jason Weston, Andre Elisseeff, Bernhard Scholkopf, and Mike Tipping. 2003. Use of the zero norm with linear models and kernel methods. _J. Mach. Learn. Res._ 3 (2003), 1489-1484.
* Wright et al. (2008) John Wright, Allen Y Yang, Arvind Ganesh, S Shankar Sastry, and Yi Ma. 2008. Robust face recognition via sparse representation. _IEEE Trans. Pattern Anal. Mach. Intell._ 31, 2 (2008), 210-227.
* Yu et al. (1998) Chuanu Yu, Fangzhou Wu, Mingxin An, Jianqiang Huang, Yongfeng Huang, and Xing Xie. 1998. NPA: Neural news recommendation with personalized attention. In _KDD_. 2756-2884.
* Wu et al. (2019) Le Wu, Peiji Sun, Yanjie Fu, Richang Hong, Xiting Wang, and Meng Wang. 2019. A neural influence diffusion model for social recommendation. In _SIGIR_. 255-244.
* Wu et al. (2020) Mike Wu, Milan Mosse, Chengxu Zhuang, Daniel Yamins, and Noah Goodman. 2020. Conditional Negative Sampling for Contrastive Learning of Visual Representations. In _ICLR_.
* Xia et al. (2022) Jun Xia, Lingyu Wu, Junta Chen, Bouben Hu, and Stan Z Li. 2022. Smigrace: A simple framework for graph contrastive learning without data augmentation. In _WWW_. 1070-1079.
* Xia et al. (2022) Jun Xia, Lirenge Wu, Ge Wang, Jintao Chen, and Stan Z Li. 2022. ProGCL: Rethinking Hard Negative Mining in Graph Contrastive Learning. In _ICML_. 2323-24364.
* Xia et al. (2022) Jun Xia, Yanqiao Zhu, Yuanqi Du, and Stan Z Li. 2022. A survey of pretraining on graphs: Taxonomy, methods, and applications. _arXiv preprint arXiv:2202.07938_ (2022).
* Xie et al. (2016) Junyuan Xie, Ross Girshick, and Ali Farhadi. 2016. Unsupervised deep embedding for clustering analysis. In _ICML_. 478-487.
* Xie et al. (2022) Yaochen Xie, Zhao Xu, Jingtan Zhang, Zhengyang Wang, and Shuiwang Ji. 2022. Self-supervised learning of graph neural networks: A unified review. _IEEE Trans. Pattern Anal. Mach. Intell._ 52, 2 (2022), 2412-2429.
* Xu et al. (2019) Hongfeng Xu, Dixin Luo, Hongyuan Zha, and Lawrence Carin Duke. 2019. Gromov-wasserstein learning for graph matching and node embedding. In _ICML_. 6932-6941.
* Xu et al. (2019) Jun Xu, Mengyang Yu, Ling Shao, Wangmeng Zuo, Deyu Meng, Lei Zhang, and David Zhang. 2019. Scaled simplex representation for subspace clustering. _IEEE Trans. Cybern._ 51, 3 (2019), 1493-1505.
* Xu et al. (2018) Keyulu Xu, Wutian Liu, Huq, Leskovec, and Stefanie Jegelka. 2018. How Powerful are Graph Neural Networks?. In _ICLR_.
* Yan et al. (2018) Shuicheng Yan, Dong Xu, Benyang Zhang, Hong-Jiang Zhang, Qiang Yang, and Stephen Lin. 2018. Graph embedding and extensions: A general framework for dimensionality reduction. _IEEE Trans. Pattern Anal. Mach. Intell._ 29, 1 (2006), 40-51.
* You et al. (2016) Chong You, Chun-Guang Li, Daniel P Robinson, and Ren't Vlad. 2016. Oracle based active set algorithm for scalable elastic net subspace clustering. In _CVPR_. 3928-3937.
* You et al. (2018) Jiaruan You, Rex Ying, Xiang Ren, William Hamilton, and Jure Leskovec. 2018. GraphForm: Generating realistic graphs with deep auto-regressive models. In _ICML_. 5708-5717.
* Yan et al. (2021) Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. 2021. Graph contrastive learning automated. In _ICML_. 12121-12132.
* Yin et al. (2020) Yuning You, Tianlong Chen, Yonglong Sun, Ting Chen, Zhangyang Wang, and Yang Shen. 2020. Graph contrastive learning with augmentations. In _NeurIPS_, Vol. 33. 5812-5823.
* Yu et al. (2018) Junliang Yu, Hongjun Yin, Xin Xu, Tong Chen, Lihan Chen, and Qiao Yuei Huang. 2020. Zero graph augmentations necessary? _i_tapp graph contrastive learning for recommendation. In _SIGIR_. 1304-1303.
* Zhang et al. (2018) Tengyu Zhang, Qifan Wu, Junchi Yan, David Wipf, and Philip S Yu. 2018. From canonical correlation analysis to self-supervised graph neural networks. In _NeurIPS_, Vol. 34. 76-89.
* Zhang and Chen (2018) Muhan Zhang and Yixin Chen. 2018. Link prediction based on graph neural networks. In _NeurIPS_, Vol. 31.
* Zhang et al. (2018) Muhan Zhang, Zhicheng Cui, Matiou Neumann, and Yixin Chen. 2018. An end-to-end deep learning architecture for graph classification. In _AAAI_, Vol. 32.
* Zhang et al. (2012) Shanghui Zhang, Chong Tong, Yeben Wei, and Chun-Guang Li. 2012. Learning a self-expressive network for subspace clustering. In _CVPR_. 12393-12403.
* Zhang et al. (2022) Xiang Zhang, Ziyuan Zhao, Theodore Tsiigliatsi, and Maxima Zimaki. 2022. Self-supervised contrastive pre-training for time series in time-frequency consistency. In _NeurIPS_, Vol. 35. 3988-4003.
* Zhang et al. (2022) Yifan Zhang, Hao Zhu, Xiang Song, Piori Koniss, and Irun King. 2022. COSTA: covariance-preserving feature segmentation for graph contrastive learning. In _KDD_. 2524-2534.
* Zhang et al. (2017) Zheng Zhang, Zhifani Lai, Yong Xu, Ling Shao, Jian Wu, and Guo-Sen Xie. 2017. Discriminative delta-net regularized linear regression. _IEEE Trans Image Process._ 26, 3 (2017), 1466-1481.
* Zhao et al. (2021) Han Zhao, Xu Yang, Zhenxin Wang, Erkun Yang, and Cheng Deng. 2021. Graph Debiased Contrastive Learning with Joint Representation Clustering. In _IJCAI_. 3454-3440.
* Zhou et al. (2022) Tong Zhao, Wei Jin, Yoren Liu, Yingheng Wang, Gang Liu, Stephan Gunnemann, Neil Shaha, and Meng Jiang. 2022. Graph data augmentation for graph machine learning: A survey. _arXiv preprint arXiv:2202.08721_ (2022).
* Zhou et al. (2021) Tong Zhao, Yoren Liu, Leonardo Neves, Oliver Woodford, Meng Jiang, and Neil Shah. 2021. Data augmentation for graph neural networks. In _AAAI_, Vol. 35. 1105-11023.
* Zheng et al. (2022) Jiangbin Zheng, Yilei Wang, Ge Wang, Jun Xia, Yufei Huang, Guofiang Zhao, Yue Zhang, and Stan Li. 2022. Using Context-to-Vector with Graph Retrofitting in Improve Word Embeddings. In _ACL_. 8154-8163.
* Zhu et al. (2021) Yanqiao Zhu, Yixin Chen, Qiang Li, and Shu Wu. 2021. An empirical study of graph contrastive learning. _arXiv preprint arXiv:2109.01116_ (2021).
* Zhu et al. (2020) Yanqiao Zhu, Yichen Xu, Peng Yu, Qiang Liu, Shu Wu, and Liang Wang. 2020. Deep graph contrastive representation learning. _arXiv preprint arXiv:2206.04131_ (2020).
* Zhu et al. (2021) Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. 2021. Graph contrastive learning with adaptive augmentation. In _WWW_. 2069-2080.
* Zhu et al. (2021) Zhucheng Zhu, Zhoujang Zhang, Liong-Paecal Xhouimento, and Jian Tang. 2021. Neural bellman-rod networks: A general graph neural network framework for link prediction. In _NeurIPS_, Vol. 34. 2492-2490.
* Zou and Ha (2005) Hui Zou and Trevor Ha. 2005. Regularization and variable selection via the elastic net. _Journal of the Royal Statistical Society Series B: Statistical Methodology_ 67, 2 (2005), 301-320.

## Appendix A Procedure for Grape

```
1:Initial graph \(G=(\mathbf{X},\mathbf{A})\), temperature parameter: \(\tau\), trade-off parameters: \(\lambda\), \(\mu\), range of hard negatives: \(K\), interval for updating \(\mathbf{C}\): \(intol\), maximum epochs: \(Epo\).
2:Initialization: Randomly initialize the GNN parameters. Determine candidate set of hard negatives \(\{\Phi_{i}\}_{i=1}^{n}\) and set the self-expression coefficients \(\{\epsilon_{i}\}_{i=1}^{n}\) to zero.
3:forepoch \(=1\) to \(Epo\)do
4: Generate two augmented graphs \(G_{1}=(\mathbf{X}_{1},\mathbf{A}_{1})\) and \(G_{2}=(\mathbf{X}_{2},\mathbf{A}_{2})\). Feed \(G_{1},G_{2}\) to \(G_{2}\) into GNN encoder to obtain embedding \(\{z_{i}\}_{i=1}^{n}\), \(\{u_{i}\}_{i=1}^{n}\) and \(\{v_{i}\}_{i=1}^{n}\).
5:ifepoch(intol \(=0\)then
6: Compute the self-expression coefficients \(\{\epsilon_{i}\}_{i=1}^{n}\) on former hard negatives by solving Eq. (8).
7: Update \(\{\Phi_{i}\}_{i=1}^{n}\) within \(K\)-hop with solution \(\{\epsilon_{i}\}_{i=1}^{n}\).
8: Re-compute the self-expression coefficients \(\{\epsilon_{i}\}_{i=1}^{n}\) on \(\Phi\) and obtain \(S\) by Eq. (11)
9:endif
10: Compute contrastive loss \(\mathcal{L}_{mask/pos}\) in Eq. (12) or (13).
11: Update \(\hat{f}_{\Theta}(\mathbf{A},\mathbf{X})\) with Adam by minimizing the overall loss in Eq. (14);
12:endfor
13:The trained \(\hat{f}_{\Theta}(\mathbf{A},\mathbf{X})\) and the node embeddings \(\{z_{i}\}_{i=1}^{n}\).
14:
15:
16:
17:
18:
```

**Algorithm 1** Procedure for GRAPE.

## Appendix B Learning self-expression coefficients

The self-expression coefficients can be addressed through multiple efficient solutions. For instance, a common approach with iterative shrinkage thresholding algorithm (ISTA) [1] can be written as

\[\mathbf{c}^{(t+1)}= \tag{1}\] \[\Gamma_{\lambda\mu}\left(\mathbf{c}^{(t)}+\varepsilon\sum_{l=1}^{L} \frac{1}{d^{(l)}}\left(\mathbf{H}^{(l)T}\left(\mathbf{z}^{(l)}-\mathbf{H}^{(l)}\mathbf{ c}^{(t)}\right)+\lambda(\mu-1)\mathbf{c}^{(t)}\right)\right)\]

where \(\varepsilon>0\) is a step size and the soft-thresholding operator \(\Gamma_{\alpha}:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}\) is defined by \(\Gamma_{\alpha}(\mathbf{x})=(|\mathbf{x}|-\alpha\mathbf{1})_{\mathrm{t}}^{T}\operatorname{ sgn}(\mathbf{x})\). Since the problem is strongly convex, such accelerated proximal gradient mothed or linearized alternating direction method can be applied for seeking unique solution. The parameters to be solved are \(\sum_{i=1}^{n}m_{i}\), related to the total number of nodes \(n\).

In MLP-style parameterization, multi-layer mapping transforms the embeddings output by the GNN into another feature space. In the new feature space, \(\mathbf{c}_{ij}\) can be obtained through the dot product of the \(i\)-th and \(j\)-th samples, i.e., \(\mathbf{c}_{ij}=MLP_{k}(\mathbf{x}_{i})^{T}MLP_{q}(\mathbf{x}_{j})\) Readers may refer to [100] for implementation details.

Attentive parameterization can be implemented through various attention mechanisms, taking GAT as an example, where the self-expression coefficient can be parameterized as follows:

\[\mathbf{c}_{ij}=\frac{\exp\left(\operatorname{LeakyReLU}\left(\overline{ \mathbf{a}}^{T}\left[\mathbf{W}_{\mathrm{z}}||\mathbf{W}_{\mathrm{z}}\right]\right)\right)}{ \sum_{k\in h(i)}\exp\left(\operatorname{LeakyReLU}\left(\overline{\mathbf{a}}^{T} \left[\mathbf{W}_{\mathrm{z}}||\mathbf{W}_{\mathrm{z}}\right]\right)\right)} \tag{2}\]

where \(\mathbf{a}\in\mathbb{R}^{2d(x)}\) is the weight vector on feature and \(||\) is the concatenation operation. Both MLP parameterization and attentive parameterization have dimension-related parameters, making them suitable for large-scale graphs.

## Appendix C Detailed Proof

Theorem 1.: _Assume \(\tilde{\mathbf{c}}(\mathbf{H})\) is the optimal solution of problem (8). The auxiliary function is defined as_

\[g(\mathbf{h})=\sum_{l=1}^{L}\frac{1}{d^{(l)}}\mathbf{h}^{(l)T}\left(\mathbf{z}^{(l)}-\mathbf{H} ^{(l)}\tilde{\mathbf{c}}(\mathbf{H})\right). \tag{3}\]

_Then hard negatives set can be computed by \(\Phi=\{\mathbf{h}\mid|g(\mathbf{h})|>\lambda\mu\}\)._

Proof.: Problem (8) can be reformulated as

\[\min_{\mathbf{c}}\frac{1}{2}\|\mathbf{z}-\mathbf{H}\mathbf{c}\|_{2}^{2}+\lambda\left(\mu\| \mathbf{c}\|_{1}+\frac{1-\mu}{2}\|\mathbf{c}\|_{2}^{2}\right) \tag{4}\]

where

\[\mathbf{z}=[\frac{1}{d^{(1)}}{z}^{(1)T},\cdots,\frac{1}{d^{(L)}}{z}^{(L)T}]^{T} \tag{5}\]

\[\mathbf{H}=[\frac{1}{d^{(1)}}\mathbf{H}^{(1)T},\cdots,\frac{1}{d^{(L)}}H^{(L)T}]^{T} \tag{6}\]

By taking derivatives, the optimal solution \(\tilde{\mathbf{c}}(\mathbf{H})\) to problem (4) satisfies:

\[\lambda(1-\mu)\tilde{\mathbf{c}}(\mathbf{H})=\Gamma_{\lambda\mu}\left(\mathbf{H}^{T} \left(\mathbf{z}-\mathbf{H}\tilde{\mathbf{c}}(\mathbf{H})\right)\right). \tag{7}\]

Let \(\left[\mathbf{q}^{T}\left(\mathbf{H}\right),g(\mathbf{h})\right]^{T}\) be the optimal solution for problem

\[\min_{\mathbf{c}}\|\mathbf{z}-[\mathbf{H},\mathbf{h}]\operatorname{\mathbf{c}}\|_{2}^{2}+\lambda \left(\mu\|\mathbf{c}\|_{1}+\frac{1-\mu}{2}\|\mathbf{c}\|_{2}^{2}\right). \tag{8}\]

Then there exist

\[\lambda(1-\mu)\left[\mathbf{q}^{T}\left(\mathbf{H}\right),g(\mathbf{h})\right]^{T}=\Gamma_{ \lambda\mu}\left(\left[\mathbf{H},\mathbf{h}\right]^{T}\left(\mathbf{z}-[\mathbf{H},\mathbf{h}] \left[\mathbf{q}^{T}\left(\mathbf{H}\right),g(\mathbf{h})\right]^{T}\right)\right)\right) \tag{9}\]

By splitting the counterpart terms, the following two equations hold:

\[\lambda(1-\mu)\mathbf{q}\left(\mathbf{H}\right)=\Gamma_{\lambda\mu}\left(\mathbf{H}^{T} \left(\mathbf{z}-\mathbf{H}\mathbf{q}\left(\mathbf{H}\right)-\mathbf{h}g(\mathbf{h})\right)\right) \tag{10}\]

If \(\mathbf{h}\notin\Phi\), then \(\left[\mathbf{c}^{T}\left(\mathbf{H}\right),0\right]^{T}\) is an optimal solution because it meets Eq. (9) and (10). Since the optimal solution to problem (7) is unique, condition a is thus satisfied. Since the optimal solution to problem (7) is unique, term (a) stipulated in the definition of \(\Phi\) holds.

In the case where \(\mathbf{h}\in\Phi\), we show that \(g(\mathbf{h})\) is not equal to \(0\). If \(g(\mathbf{h})=0\), due to the uniqueness of the optimal solution in problem (4), Eq. (9) deduces \(\mathbf{q}\left(\mathbf{H}\right)=\tilde{\mathbf{c}}(\mathbf{H})\). However, the obtained solution \(\left[\mathbf{c}^{T}\left(\mathbf{H}\right),0\right]^{T}\) does not satisfy Eq. (10). Therefore \(g(\mathbf{h})\neq 0\) holds. Combining the above discussion, \(\Phi\) is the adaptive hard negatives set by Definition 2, which completes the proof. 

**Proposition 2**.: _If GRAPE captures hard negatives \(\{\Phi_{i}\}_{i=1}^{n}\) within each individual subspace, both \(\mathcal{L}_{mask}\) and \(\mathcal{L}_{pos}\) contribute to the inter-subspace separation and intra-subspace cohesion._Proof.: Compared to GRACE, GRAPE\({}_{pos}\) explicitly brings hard negative samples within the same subspace closer while repelling negatives outside the subspace. Our focus then turns to GRAPE\({}_{mask}\). From gradient analysis, the ratio of the gradients of negatives to that of positives can be defined following [75]:

\[r(\mathbf{u}_{i},\mathbf{v}_{j})=\left|\frac{\partial t(\mathbf{u}_{i})}{\partial\theta(\bm {u}_{i},\mathbf{v}_{j})}\right|/\left|\frac{\partial t(\mathbf{u}_{i})}{\partial\theta( \mathbf{u}_{i},\mathbf{v}_{i})}\right|, \tag{11}\]

representing the relative penalty on negatives. The ratio in GRACE and GRAPE can be derived as follows:

\[\text{GRACE:}\quad r_{1}(\mathbf{u}_{i},\mathbf{v}_{j})=\frac{e^{\theta}(\mathbf{u}_{i}, \mathbf{v}_{j})/\tau}{\sum_{k\neq i}\left(e^{\theta}(\mathbf{u}_{i},\mathbf{v}_{j})/\tau+ e^{\theta}(\mathbf{u}_{i},\mathbf{v}_{j})/\tau\right)}, \tag{12}\]

\[\text{GRAPE:}\quad r_{2}(\mathbf{u}_{i},\mathbf{v}_{j})=\frac{e^{\theta}(\mathbf{u}_{i}, \mathbf{v}_{j})/\tau}{\sum_{k\in N_{k}}\left(e^{\theta}(\mathbf{u}_{i},\mathbf{v}_{j})/ \tau+e^{\theta}(\mathbf{u}_{i},\mathbf{v}_{j})/\tau\right)}. \tag{13}\]

Clearly there is \(r_{2}(\mathbf{u}_{i},\mathbf{v}_{j})\geqslant r_{1}(\mathbf{u}_{i},\mathbf{v}_{j})\), which implies that GRAPE imposes a greater penalty on negative pairs that are not within the same subspace. Moreover, assume that \(\xi_{j}=S_{j\ell}=1\), i.e., \(\mathbf{v}_{j}\) is not in the denominator of \(\ell\)\((\mathbf{u}_{j})\) and vice versa. In this case, \(\theta\)\((\mathbf{u}_{i},\mathbf{v}_{j})\) is not penalized explicitly. Apart from self-alignment, the subproblem involving \(\mathbf{u}_{i}\) in the process of minimizing \(\mathcal{L}_{mask}\) is equivalent to:

\[\min_{\mathbf{u}_{i}}\sum_{i\notin\mathbf{N}_{k}\lor k\notin N_{i}}\left(e^{\theta(\bm {u}_{i},\mathbf{u}_{k})}+e^{\theta(\mathbf{u}_{i},\mathbf{v}_{k})}\right) \tag{14}\]

If we consider the first-order Taylor expansion of the problem and omit the second or higher-order infinitesimal terms, problem (14) simplifies to

\[\min_{\mathbf{u}_{i}}\sum_{i\notin\mathbf{N}_{k}\lor k\notin N_{i}}\left(\theta\left( \mathbf{u}_{i},\mathbf{u}_{k}\right)+\theta\left(\mathbf{u}_{i},\mathbf{v}_{k}\right)\right). \tag{15}\]

It is clear that there is a unique solution to the above problem. If \(i\) and \(j\) belong to the same subspace, the overlap between set \(\{k|\notin N_{k}\lor k\notin N_{i}\}\) and \(\{k|\neq N_{k}\lor k\notin N_{j}\}\) appears to be high. Hence, the optimal solutions of the subproblems for \(\mathbf{u}_{i}\) and \(\mathbf{v}_{j}\) tends to exhibit high similarity. As a result, \(\mathbf{u}_{i}\) and \(\mathbf{v}_{j}\) are implicitly drawn closer by updating the network parameters. Thereby we prove the Proposition 2 qualitatively. 

**Theorem 2**.: _The contrastive loss in Eq. (14) gives a stricter lower bound of MI between input features \(\mathbf{X}\) and embeddings in two views \(\mathbf{U}\) and \(\mathbf{V}\), compared with the contrastive loss \(\mathcal{L}\) in Eq. (3) proposed by GRACE. This can be written formally as_

\[-\mathcal{L}<-\mathcal{L}_{mask/pos}\leqslant I\left(\mathbf{X};\mathbf{U},\mathbf{V}\right) \tag{16}\]

Proof.: We only consider the GRAPE\({}_{mask}\) scheme, while GRAPE\({}_{pos}\) can be analogized, since the proof is trivial. The contrastive loss in GRAPE\({}_{mask}\) and GRACE can be reformulated as follows:

\[\ell_{mask}\left(\mathbf{u}_{i}\right)=-\log\frac{e^{\theta(\mathbf{u}_{i},\mathbf{v}_{i})/ \tau}}{e^{\theta(\mathbf{u}_{i},\mathbf{v}_{i})/\tau}+\sum_{j\in N_{i}}\left(e^{\theta (\mathbf{u}_{i},\mathbf{v}_{j})/\tau}+e^{\theta(\mathbf{u}_{i},\mathbf{u}_{j})/\tau}\right)}, \tag{17}\]

\[\ell\left(\mathbf{u}_{i}\right)=-\log\frac{e^{\theta(\mathbf{u}_{i},\mathbf{v}_{i})/\tau} }{e^{\theta(\mathbf{u}_{i},\mathbf{v}_{i})/\tau}+\sum_{j\neq i}\left(e^{\theta(\mathbf{u} _{i},\mathbf{v}_{j})/\tau}+e^{\theta(\mathbf{u}_{i},\mathbf{u}_{j})/\tau}\right)}, \tag{18}\]

It's clear that the overall loss satisfies \(\mathcal{L}>\mathcal{L}_{mask}\).

The InfoNCE loss [18, 53] can be formulated as

\[\mathcal{L}_{NCE}=-\mathbb{E}_{p(\mathbf{u},\mathbf{v})}\left(\frac{1}{n}\sum_{i=1}^{ n}\log\frac{e^{\theta(\mathbf{u}_{i},\mathbf{v}_{i})/\tau}}{\sum_{j=1}^{n}e^{\theta(\mathbf{u}_{i}, \mathbf{v}_{j})/\tau}}\right). \tag{19}\]

Analogously, replacing the empirical estimation by the expectation, the contrastive loss in GRAPE can be reformulated as

\[\mathcal{L}_{mask}=-\mathbb{E}_{p(\mathbf{u},\mathbf{v})}\left(\frac{1}{2}\left(\mathbf{t} _{mask}\left(\mathbf{u}\right)+\mathbf{t}_{mask}\left(\mathbf{v}\right)\right)\right), \tag{20}\]

where

\[\ell_{mask}\left(\mathbf{u}\right) \tag{21}\] \[=-\log\frac{e^{\theta(\mathbf{u},\mathbf{v})/\tau}}{e^{\theta(\mathbf{u},\mathbf{ v})/\tau}+\sum_{j\in N_{k}}\left(e^{\theta(\mathbf{u}_{i},\mathbf{v})/\tau}+e^{\theta(\mathbf{u},\mathbf{u})/\tau}\right)}\] (22) \[=-\frac{1}{n}\sum_{i=1}^{n}\log\frac{1}{\frac{e^{\theta(\mathbf{u}, \mathbf{v})/\tau}}{n}}\frac{e^{\theta(\mathbf{u},\mathbf{v})/\tau}}{\sum_{j=1}^{n}e^{ \theta(\mathbf{u},\mathbf{v})/\tau}+\sum_{j\in N_{k}}\left(e^{\theta(\mathbf{u},\mathbf{v})/ \tau}+e^{\theta(\mathbf{u},\mathbf{v})/\tau}\right)}.\]

This clearly gives \(\mathcal{L}_{NCE}\leqslant\mathcal{L}_{mask}\). The InfoNCE estimator is a lower bound of the true MI, i.e., \(-\mathcal{L}_{NCE}\leqslant\mathcal{I}\left(\mathbf{U};\mathbf{V}\right)\). From the data processing inequality in information theory, it can be deduced that \(\mathbf{I}\left(\mathbf{U};\mathbf{V}\right)\leqslant I\left(\mathbf{X};\mathbf{U},\mathbf{V}\right)\). Collecting the above discussion, there exists \(-\mathcal{L}_{mask}\leqslant I\left(\mathbf{X};\mathbf{U},\mathbf{V}\right)\), which completes the proof. 

## Appendix D Related Methods

**Graph attention** The connection between self-expression and attention mechanism has been deliberated in [73]. In this context, we underscore two pivotal distinctions between GRAPE and existing graph attention methods, such as GAT [71] and its variants: (1) Irrespective of the training paradigm, it's essential to note that in graph attention, the attention coefficients are learned with respect to the loss, whereas in GRAPE, the self-expression coefficients \(C_{ij}\) adapt during the training process and exert an influence on the loss function. (2) Diverging from the structure-dependent attention coefficients in graph attention methods, GRAPE's self-expression coefficients are not inherently contingent on the graph structure. This feature expands the application domain of GRAPE to noisy graphs, corrupted graphs, or even heterography graph structures.

**Nonlinear latent subspace clustering** The fundamental concept underlying latent subspace clustering [55] and its variants is that the subspaces delineating the representation of raw data may not be readily discernible, hence, they may be partitioned into subspaces via learnable transformations. The features of the samples may not lie within \(k\) specific subspaces, but their intrinsic semantics are encapsulated within \(k\) larger subspaces. GRAPE also enjoys this concept. Each layer of the GNN encoder serves as a nonlinear transformation of the previous layer, wherein point pairs that preserve the same subspace structure are included into the hard negatives set.

**Uniformity-tolerance dilemma** In [75], 'tolerance' pertains to the degree of similarity between false negatives, while 'uniformity' signifies the separability of all negatives. These two concepts are somewhat contradictory, a phenomenon commonly referred to as the uniformity-tolerance dilemma. Though temperature-based approaches [75] are straightforward, temperature \(\tau\) tend to be global and challenging to learn. An intuitive comparison, as depicted in Figure 11, showcases the average distribution of negatives on Cora.

[MISSING_PAGE_FAIL:13]

[MISSING_PAGE_EMPTY:14]