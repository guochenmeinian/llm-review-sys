# A Cooperative Multi-Agent Framework for

Zero-Shot Named Entity Recognition

Anonymous Author(s)

Submission Id: 812

###### Abstract.

Zero-shot named entity recognition (NER) aims to develop entity recognition systems from unannotated text corpora. This task presents substantial challenges due to minimal human intervention. Recent work has adapted large language models (LLMs) for zero-shot NER by crafting specialized prompt templates. And it advances the models' self-learning ability by incorporating self-annotated demonstrations. Two important challenges persist: (i) Correlations between contexts surrounding entities are overlooked, leading to wrong type predictions or entity omissions. (ii) The indiscriminate use of task demonstrations, retrieved through shallow similarity-based strategies, severely misleads the inferences made by LLMs.

In this paper, we introduce CMAS, viz., _cooperative multi-agent system_, a framework for zero-shot NER that uses the collective intelligence and tailored abilities of multiple agents to address the challenges outlined above. Cooperative multi-agent system (CMAS) has four main agents: (i) a self-annotator, (ii) a type-related feature (TRF) extractor, (iii) a demonstration discriminator, and (iv) an overall predictor. To explicitly capture correlations between contexts surrounding entities, CMAS reformulates NER into two subasks: recognizing named entities and identifying entity type-related features within the target sentence. Moreover, pseudo-labels for TRFs are generated using mutual-information criteria without requiring human effort, facilitating the prediction of the TRF extractor. To assess the quality of demonstrations, a demonstration discriminator is established to incorporate the self-reflection mechanism, automatically evaluating helpfulness scores for the target sentence and enabling controllable utilization of demonstrations.

Experimental results show that CMAS significantly improves zero-shot NER performance across six benchmarks, including both domain-specific and general-domain scenarios. Furthermore, CMAS demonstrates its effectiveness in few-shot settings and with various LLM backbones.1

Footnote 1: Our code is available at [https://anonymous.dopen.science/v/WWW25-CMAS-87UF](https://anonymous.dopen.science/v/WWW25-CMAS-87UF).

Information extraction, Named entity recognition, Zero-shot learning, Large language models, Multi-agent system +
Footnote †: copyright: © 2025: Copyright held by the owner/author(s).

+
Footnote †: copyright: © 2025: Copyright held by the owner/author(s).

+
Footnote †: copyright: © 2025: Copyright held by the owner/author(s).

+
Footnote †: copyright: © 2025: Copyright held by the owner/author(s).

+
Footnote †: copyright: © 2025: Copyright held by the owner/author(s).

+
Footnote †: copyright: © 2025: Copyright held by the owner/author(s).

+
Footnote †: copyright: © 2025: Copyright held by the owner/author(s).

## 1. Introduction

In named entity recognition (NER), the task is to identify predefined named entities, such as persons, locations, and organizations, based on their contextual semantics within input texts. NER serves as a fundamental task in information extraction and is integral to various downstream natural language processing (NLP) applications, including question answering [17, 37], document understanding [12], and information retrieval [5, 6]. Current NER methods primarily use fully supervised learning paradigms and show impressive performance across various benchmarks [30, 39, 51]. However, these fully-supervised paradigms rely heavily on large-scale, human-annotated data. In real-world scenarios, the availability of annotated data may be restricted to specific domains, severely hindering the generalizability and adaptability of NER models [47, 60].

Recently, large language models (LLMs) have transformed natural language processing with their zero-shot or few-shot generalization abilities [3, 24, 26, 27]. With their extensive search spaces and large-scale pre-training data, LLMs have the potential to overcome the challenges of data sparsity and generalizability faced by NER models. Motivated by these developments, one line of prior work explores prompting techniques to enhance few-shot in-context learning (ICL) for NER [45]. Other efforts use specialized knowledge to develop task-specific LLMs for NER [35, 60, 62] or employ LLMs as data annotators or generators to augment smaller language models [14, 22]. However, these approaches still require deliberately selected annotated task examples or external knowledge. The reasoning abilities of LLMs for zero-shot NER remain underexplored.

To address zero-shot NER, Wei et al. [49] transform this task, where no labeled data is available, into a two-stage question-answering process by chatting with LLMs. Xie et al. [54] conduct a systematic empirical study on zero-shot NER using LLMs and tailor prevalent reasoning methods, such as tool augmentation and majority voting, to adapt ChatGPT for zero-shot NER. Furthermore, to reduce reliance on external tools, Xie et al. [55] enhance the self-learning capabilities of LLMs through a self-improvement mechanism. Specifically, LLMs initially annotate unlabeled corpora with self-consistency scores. Subsequently, for each test input, inference is conducted using in-context learning (ICL) with demonstrations retrieved from the self-annotated dataset.

Despite these advances, current zero-shot NER methods still encounter two challenging problems:

**Challenge 1: Overlooking correlations between contexts surrounding entities.** Prior work [49, 54, 55] focuses exclusively onrecognizing entities within the target sentence. However, the contexts surrounding entities of the same type are correlated, and identifying contexts that are strongly associated with entity types plays a crucial role in facilitating the generalization of pretrained language models for the NER task (Wang et al., 2019). Neglecting these contextual correlations can lead to incorrect type predictions or entity omissions, severely impeding the adaptation of LLMs to zero-shot scenarios. For instance, as shown in Figure 1(a), the existing method (Wang et al., 2019) fails to recognize "member" and "teams," which are closely related to Person entities, in the target sentence, resulting in the omission of the entity "Atessis." To tackle this issue, we propose redefining the traditional NER task into two subtasks: recognizing named entities and identifying entity _type-related features_ (TRFs) (Wang et al., 2019), i.e., tokens strongly associated with entity types).

**Challenge 2: Indiscriminate use of task demonstrations.** To enhance task understanding and guide inference, recent zero-shot NER methods (Wang et al., 2019; Wang et al., 2020) use both task instructions and task-specific demonstrations as input prompts for LLMs. However, these methods employ shallow strategies for demonstration retrieval, such as random sampling and \(k\)-nearest neighbors, resulting in the frequent emergence of low-quality demonstrations. For instance, as illustrated in Figure 1(b), approximately 87.33% and 76.94% of selected demonstrations do not contain any target entity types in the test sentences from the WNUT-17 and OntoNotes datasets, respectively. The indiscriminate use of unhelpful or irrelevant demonstrations substantially misleads the inference process of LLMs and degrades models' prediction abilities. To address this problem, we incorporate a self-reflection mechanism (Brockman et al., 2019; Wang et al., 2020), enabling LLMs to reflect on the helpfulness of retrieved demonstrations and selectively learn from them.

**Contributions.** Note that existing LLMs suffer from severe performance degradation with long input contexts (Shi et al., 2019; Wang et al., 2020) and complex instruction following (Wang et al., 2019; Wang et al., 2020). Thus, it is challenging to effectively capture contextual correlations and discriminative use retrieved demonstrations through single-turn or multi-turn dialogues with LLMs. Inspired by the demonstrated complex problem-solving capabilities of multi-agent approaches (Wang et al., 2019; Wang et al., 2020), in this paper, we present a framework, named the _cooperative multi-agent system_ (CMAS) for zero-shot NER, harnessing the collective intelligence of LLM-based agents to address the challenges listed. As Figure 2 illustrates, CMAS consists of four main agents: (i) a self-annotator, (ii) a type-related feature extractor, (iii) a demonstration discriminator, and (iv) an overall predictor. First, adopting the self-improving strategy outlined in (Wang et al., 2020), CMAS employs an LLM as the self-annotator to create self-annotated data through predictions on unlabeled corpora. Then, to empower the simultaneous extraction of entities and contextual correlations, CMAS redefines the NER task into two subtasks: recognizing named entities and identifying entity type-related features within the target sentence. To achieve this, an LLM-based type-related feature extractor is developed to pinpoint words or phrases closely related to different entity types from the surrounding contexts using specialized in-context learning (ICL). Additionally, pseudo-labels for TRFs are generated using mutual information criteria, streamlining the inference process of the TFR extractor without requiring human interventions. Given the entity type features relevant to the target sentence, the demonstration discriminator integrates a self-reflection mechanism (Brockman et al., 2019) to automatically assess the helpfulness of each selected demonstration in making predictions on the target test sentences. Finally, with the extracted TRFs and predicted helpfulness scores of demonstrations, the overall predictor performs inference on each incoming test sentence with a two-stage self-consistency strategy (Wang et al., 2019; Wang et al., 2020), selectively learning from retrieved demonstrations while considering contextual correlations. Additionally, external tools, such as a syntactic structure analyzer (Brockman et al., 2019), can be used to further enhance CMAS (see Section 7.1).

Our contributions are summarized as follows: (i) To the best of our knowledge, ours is the first study to design a cooperative multi-agent system for zero-shot NER that harnesses the collaborations and unique roles of multiple agents to integrate contextual correlations and the self-reflection mechanism. (ii) We redefine NER into two subtasks: recognizing named entities and identifying entity type-related features. In this way, CMAS effectively captures contextual correlations between the contexts during entity recognition, thereby reducing incorrect type predictions and entity omissions. (iii) We incorporate a self-reflection mechanism into the demonstration discriminator. By evaluating the helpfulness scores for entity

Figure 1. (a) Examples of incorrect type prediction and entity omission from an existing method (Wang et al., 2019) due to overlooking correlated contexts surrounding entities. Red texts represent wrongly recognized entities; golden labels are included in the brackets. Blue texts highlight entity type-related features (TRFs), i.e., contexts strongly associated with the entity types. (b) Proportions of selected demonstrations lacking target entity types in the WikiGold (2019), WNUT-17 (Chen et al., 2019), OntoNotes,2 and BioNLP11 (Wang et al., 2020) datasets. More than 40% of demonstrations do not contain any entity types within the target sentence.

extractions in target sentences, CMAS is capable of discriminately using and learning from selected demonstrations. (iv) Experimental results across six benchmarks demonstrate that our proposed CMAS achieves state-of-the-art performance on zero-shot NER and exhibits strong robustness across varying numbers of task demonstrations.

## 2. Related Work

We investigate related work in three areas: (i) reasoning with LLMs, (ii) LLMs for IE, and (iii) LLM-based multi-agent systems.

### Reasoning with LLMs

LLMs demonstrate strong zero-shot and few-shot reasoning capabilities, particularly when prompted to provide intermediate rationales for solving problems. Recent studies in both few-shot and zero-shot frameworks explore eliciting a chain-of-thought (CoT) process from LLMs. These studies encourage LLMs to refine their responses incrementally, enhancing the reasoning process step-by-step (Golov et al., 2013; Chen et al., 2014; Chen et al., 2015; Chen et al., 2016). Additionally, strategies like problem decomposition such as least-to-most prompting (Zhu et al., 2016), break down complex problems into manageable sub-problems, addressing them sequentially. The self-consistency approach (Zhu et al., 2016) involves generating a diverse array of responses from an LLM, subsequently selecting the optimal answer by averaging over these possibilities. In this paper, we focus on investigating the zero-shot reasoning ability of LLM on the NER task.

### LLMs for IE

Evaluating the performance of LLMs on IE tasks is garnering significant attention (Golov et al., 2013; Chen et al., 2014; Chen et al., 2015; Chen et al., 2016; Chen et al., 2016). Wei et al. (Zhu et al., 2016) propose a two-stage chatting paradigm for IE. In the first stage, ChatGPT is tasked with recognizing types of elements. In the second stage, it extracts mentions corresponding to each identified type. Han et al. (Han et al., 2016) present a comprehensive analysis of LLMs' capabilities in IE tasks, examining aspects such as performance, evaluation criteria, robustness, and prevalent errors.

Xie et al. (Xie et al., 2016) conduct a systematic empirical study on the reasoning abilities of LLMs in IE, particularly examining performance in zero-shot NER tasks. Xie et al. (Xie et al., 2016) focus on enhancing the performance of zero-shot NER using LLMs by introducing a training-free self-improving framework that uses an unlabeled corpus to stimulate the self-learning capabilities of LLMs. Wan et al. (Wan et al., 2016) employ the chain-of-thought (CoT) approach for relation extraction (RE), using LLMs to generate intermediate rationales based on demonstrations from the training set.

### LLM-based multi-agent systems

LLMs exhibit useful capabilities in reasoning and planning, aligning with human expectations for autonomous agents capable of perceiving their environments, making decisions, and taking responsive actions (Xie et al., 2016; Chen et al., 2016; Chen et al., 2016; Chen et al., 2016). Consequently, LLM-based agents are increasingly designed to understand and generate human-like instructions, enhancing complex interactions and decision-making across various contexts (Golov et al., 2013; Chen et al., 2016; Chen et al., 2016). Building on the abilities of individual LLM-based agents, the concept of LLM-based multi-agent systems has been introduced. Such systems use the collective intelligence and specialized skills of multiple agents, enabling collaborative engagement in planning, decision-making, and discussions, mirroring the cooperative nature of human teamwork in problem-solving.

Figure 2. An overview of CMAS. The dotted red lines indicate the workflow of an existing method (Xie et al., 2016), which leads to incorrect predictions. In contrast, the solid black lines illustrate the workflow of the proposed CMAS, which consists of four key agents: (i) a self-annotator, (ii) a type-related feature extractor, (iii) a demonstration discriminator, and (iv) an overall predictor.

Recent research demonstrates the efficacy of LLM-based agents in diverse applications, including game simulation (Zhu et al., 2017; Zhang et al., 2018), software development (Zhu et al., 2017; Zhang et al., 2018), society simulation (Zhu et al., 2017; Zhang et al., 2018), multi-robot systems (Zhu et al., 2017; Zhang et al., 2018), and policy simulation (Zhu et al., 2017). Comprehensive updates on advances in LLM-based agents are detailed in recent surveys (Zhu et al., 2017; Zhang et al., 2018; Zhang et al., 2018). To the best of our knowledge, ours is the first study to develop an LLM-based cooperative multi-agent system tailored for zero-shot NER tasks.

In this paper, we focus on the zero-shot NER task. The work most closely related to ours is (Zhu et al., 2017; Zhang et al., 2018; Zhang et al., 2018). However, existing methods still face two challenging problems: (i) they overlook correlations between contexts surrounding entities, and (ii) they make indistinguishable use of task demonstrations. In our proposed CMAS, to explicitly model contextual correlations within target sentences, both named entities and TFKs are simultaneously extracted using specialized ICL. To enable controllable usage of demonstrations, a self-reflection mechanism is incorporated to automatically predict the helfulness score of each selected demonstration for inference on the target sentences.

## 3. Task Formulation

To explicitly capture contextual correlations during the entity extraction process, we reinterpret the original NER task as two subtasks: recognizing named entities and identifying entity type-related features within the target sentence.

**Zero-Shot NER.** In this paper, we focus on the NER task in the strict zero-shot setting (Zhang et al., 2018). In this setting, no annotated data is available; instead, we only have access to an unlabeled corpus \(\mathcal{D}_{u}\). Specifically, given an input sentence \(\mathbf{x}=x_{1},\ldots,x_{n}\) with \(n\) words from the test set \(\mathcal{D}_{t}\), our aim is to recognize structured outputs \(\mathbf{y}\) from \(\mathbf{x}\), consisting of a set of \((e,t)\) pairs. Here, \(e\) is an entity span, a sequence of tokens from \(\mathbf{x}\), and \(t\) is the corresponding entity type from a predefined set \(\mathcal{T}\), such as persons, location, or miscalaneous. **TRF extraction.** Type-related features (TRFs), which are tokens strongly associated with entity types, are critical for the generalization of NER models (Zhu et al., 2017). Given an input sentence \(\mathbf{x}\in\mathcal{D}_{t}\) including entity types \(\mathcal{T}_{\mathbf{x}}\), the goal of TRF extraction is to identify all TRFs \(\mathcal{R}\) that are related to the input sentence \(\mathbf{x}\) for all entity types in \(\mathcal{T}_{\mathbf{x}}\). Each TRF \(\mathbf{w}\in\mathcal{R}\) is an \(m\)-gram span. For instance, as illustrated in Figure 1(a), "member" and "teams" are TRFs associated with the Person entity type, while "music video games" is recognized as a TRF for the Miscalaneous type.

## 4. CMAS: A Cooperative Multi-Agent System

In this section, we present the four main agents of the proposed CMAS as described in Figure 2: (i) a self-annotator (see Section 4.1), (ii) a type-related feature extractor (see Section 4.2), (iii) a demonstration discriminator (see Section 4.3), and (iv) an overall predictor (see Section 4.4).

First, the self-annotator uses an LLM to produce self-annotated data by making predictions on the unlabeled corpus and preliminarily retrieves demonstrations using a similarity-based strategy. Next, the type-related feature extractor automatically acquires pseudolabels for TRFs using mutual information criteria and identifies words or phrases strongly associated with different entity types using specialized ICL. Subsequently, the demonstration discriminator incorporates a self-reflection mechanism to evaluate the helplfulness scores of each retrieved demonstration for predictions on the target input sentence. Finally, given the extracted TRFs and predicted helfulness scores, the overall predictor performs inference on the target sentences by employing question-answering prompts and a two stage self-consistency strategy.

### Self-annotator for unlabelled data

As mentioned in Section 1 and 3, we only have access to an unlabeled corpus \(\mathcal{D}_{u}\) in zero-shot NER. Inspired by the self-improvement strategy (Zhang et al., 2018), we specify an LLM-based self-annotator to guide the inference of LLMs, which first annotates the unlabeled corpus with zero-shot prompting and then preliminarily selects demonstrations from the self-annotated data for each target sentence.

**Self-annotation.** For each unlabeled sample \(x_{i}\in\mathcal{D}_{u}\), we generate predictions using LLMs with zero-shot prompting. This process is formulated as follows:

\[\mathbf{y}_{i}=\operatorname*{arg\,max}_{\mathbf{y}}P_{s}(\mathbf{y}|\mathbf{T }_{s},\mathbf{x}_{i}), \tag{1}\]

where \(\mathbf{T}_{s}\) is the prompt template used for self-annotation. Prompt 1 (in the Appendix) illustrates an instance of \(\mathbf{T}_{s}\). The predictions \(y_{i}=\{(e_{i}^{j},t_{i}^{j})\}_{j=1}^{j}\) consist of pairs of entity mentions and types, where \(l\) is the number of the predicted entity mentions. \(P_{s}\) is the output probability of the self-annotator. To enhance the reliability of the annotations, we use self-consistency (Zhu et al., 2017) and adopt a _two-stage majority voting_ strategy (Zhang et al., 2018). We sample multiple responses from the model. In the first stage, we consider a candidate mention as an entity if it is present in more than half of all responses; otherwise, we discard it. In the second stage, for each mention retained from the first stage, we determine the entity type label based on the majority agreement among the responses and assign this as the final predicted label.

**Preliminary demonstration selection.** When a target sentence \(\mathbf{x}^{q}\) arrives, our goal is to retrieve \(k\) relevant demonstrations \(\mathbf{S}=\{\mathbf{x}_{i},\mathbf{y}_{i}\}_{i=1}^{k}\) from the reliable self-annotated dataset. To achieve a better trade-off between similarity, diversity, and reliability, we employ a _diverse nearest neighbors with self-consistency ranking_ strategy (Zhang et al., 2018), which first retrieves \(K\) nearest neighbors based on cosine similarities between sentence representations and then selects samples with the top-\(k\) self-consistency scores.

### Type-related feature extractor

To capture correlations between contexts surrounding entities, we design an LLM-based type-related feature (TRF) extractor using specialized in-context learning (ICL). We use mutual information criteria (Zhu et al., 2017) to generate pseudo TRF labels \(\{\mathcal{R}_{i}\}_{i=1}^{k}\) for self-annotated demonstrations \(\mathcal{S}=\{\mathbf{x}_{i},\mathbf{y}_{i}\}_{i=1}^{k}\), which are selected for the test input \(\mathbf{x}^{q}\). Building on this, we apply the specialized ICL prompts to identify relevant TRFs \(\mathcal{R}^{q}\) for \(\mathbf{x}^{q}\).

**Pseudo-label generation.** To facilitate TRF extraction for the target sentence \(\mathbf{x}^{q}\), we generate pseudo TRF labels for its self-annotated demonstrations \(\mathcal{S}=\{\mathbf{x}_{i},\mathbf{y}_{i}\}_{i=1}^{k}\) using a mutual information-based method (Zhu et al., 2017). We define \(\mathcal{D}_{u}\) as the set containing all sentences from the unlabeled corpus \(\mathcal{D}_{u}\) where entities of the \(t\)-th type appear. The complementary set, \(\mathcal{D}_{u}\backslash\mathcal{D}_{t}\), includes sentencesthat do not contain entities of the \(t\)-th type. To identify TRFs \(\mathcal{R}^{t}\) associated with the \(t\)-th entity type, we apply the following filtering condition:

\[\frac{C_{\mathcal{D}_{u}\setminus\mathcal{D}_{t}}(\mathbf{w})}{C_{\mathcal{D}_{t }}(\mathbf{w})}\leq\rho,\quad C_{\mathcal{D}_{t}}(\mathbf{w})>0, \tag{2}\]

where \(C_{\mathcal{D}_{t}}(\mathbf{w})\) represents the count of the m-gram \(\mathbf{w}\) in \(\mathcal{D}_{t}\), and \(C_{\mathcal{D}_{u}\setminus\mathcal{D}_{t}}(\mathbf{w})\) represents its count in the rest of \(\mathcal{D}_{u}\) excluding \(\mathcal{D}_{t}\). The parameter \(\rho\) is an m-gram frequency ratio hyperparameter. By applying this criterion, we ensure that \(\mathbf{w}\) is considered a part of the TRF set of \(\mathcal{D}_{t}\) only if its frequency in \(\mathcal{D}_{t}\) is significantly higher than its frequency in other sets (\(\mathcal{D}_{u}\setminus\mathcal{D}_{t}\)). Given the smaller size of \(\mathcal{D}_{t}\) relative to \(\mathcal{D}_{u}\setminus\mathcal{D}_{t}\), we select \(\rho\geq 1\) but avoid excessively high values to include features associated with \(\mathcal{D}_{t}\) and potentially relevant to other entity types within the TRF set of \(\mathcal{D}_{t}\). Based on this, for every self-annotated demonstration \(\mathbf{x}_{i}\in\mathbf{S}\), we compute the Euclidean distance between BERT-based embeddings of each extracted TRF \(\mathbf{w}\) and each token in \(\mathbf{x}_{i}\), selecting the top-5 features as pseudo TRF labels \(\mathcal{R}_{i}\) of \(\mathbf{x}_{i}\).

**TRF Extraction.** Given the target sentence \(\mathbf{x}^{\emptyset}\) and its corresponding demonstrations \(\mathcal{S}_{d}=\{\mathbf{x}_{i},\mathbf{y}_{i},\mathcal{R}_{i}\}_{i=1}^{k}\) equipped with pseudo-labels, we construct specialized ICL prompts to facilitate the identification of relevant TRFs for \(\mathbf{x}^{\emptyset}\). The inference process is formulated as:

\[\mathcal{R}^{\emptyset}=\operatorname*{arg\,max}_{\mathcal{R}}P_{e}(\mathcal{ R}|\mathbf{T}_{e},\mathcal{S}_{d},\mathbf{x}^{\emptyset}), \tag{3}\]

where \(\mathbf{T}_{e}\) represents the specialized ICL prompt template. Prompt 2 (in the Appendix) provides a detailed instance of \(\mathbf{T}_{e}\). \(P_{e}(\cdot)\) represents the output probability of the TRF extractor.

### Demonstration discriminator

As mentioned in Section 1, demonstrations retrieved using shallow similarity-based strategies can be highly irrelevant to target sentences, severely misleading the predictions of LLMs. To address this issue, we employ an LLM-based demonstration discriminator with a self-reflection mechanism (Chen et al., 2017; Wang et al., 2018) to automatically evaluate the helpfulness of each initially selected demonstration for making predictions on the target test sentences. To achieve this, we consider the TRFs of both the demonstrations and the target sentence, extracted by the TRF extractor (see Section 4.2), as well as the self-labeled entity labels from the self-annotator (see Section 4.1). The prompts used for helpfulness score prediction are shown in detail in Prompt 3 (in the Appendix). Formally, given demonstrations \(\mathcal{S}_{d}=\{\mathbf{x}_{i},\mathbf{y}_{i},\mathcal{R}_{i}\}_{i=1}^{k}\) selected for the target sentence \(\mathbf{x}^{\emptyset}\) with extracted TRFs \(\mathcal{R}^{\emptyset}\), the corresponding helpfulness scores \(\{h_{i}\}_{i=1}^{k}\) are predicted by

\[h_{i}=\operatorname*{arg\,max}_{h}P_{d}(h|\mathbf{T}_{d},\mathcal{S}_{d}, \mathcal{R}^{\emptyset},\mathbf{x}_{i},\mathbf{x}^{\emptyset}), \tag{4}\]

where \(\mathbf{T}_{d}\) denotes the prompt template used in the demonstration discriminator. \(P_{d}(\cdot)\) denotes the output probability of the demonstration discriminator.

### Overall predictor

Finally, given the extracted TRFs and predicted helpfulness scores, we establish an LLM-based overall predictor to conduct inference on the target sentences. Let \(\mathcal{S}_{0}=\{\mathbf{x}_{i},\mathbf{y}_{i},h_{i},\mathcal{R}_{i}\}_{i=1} ^{k}\) represent the self-annotated demonstrations selected for the test input \(\mathbf{x}^{\emptyset}\), along with the corresponding helpfulness scores \(\{h_{i}\}_{i=1}^{k}\) and TRFs \(\{\mathcal{R}_{i}\}_{i=1}^{k}\). As Prompt 4 (in the Appendix) shows, CMAS conducts overall predictions on \(\mathbf{x}^{\emptyset}\) by integrating the dialogue in the demonstration discriminator and constructing a question-answering prompt template \(\mathbf{T}_{o}\). The overall prediction is obtained by

\[\mathbf{y}^{\emptyset}=\operatorname*{arg\,max}_{\mathbf{y}}P_{o}(\mathbf{y}| \mathbf{T}_{o},\mathcal{S}_{o},\mathbf{x}^{\emptyset}), \tag{5}\]

where \(P_{o}(\cdot)\) denotes the output probability of the overall predictor. Similar to the self-annotation process (see Section 4.1), to improve the reliability and consistency of the final results, we sample multiple responses from LLMs and adopt a two-stage self-consistency strategy.

Now that we have described the four specialized agents that make up the core of CMAS, we recall the coordinated workflow of CMAS, as illustrated in Figure 2. To start, the self-annotator incorporates a self-improvement strategy and employs an LLM to generate self-annotated data from an unlabeled corpus \(\mathcal{D}_{u}\), and preliminarily retrieves reliable demonstrations \(\mathbf{S}=\{\mathbf{x}_{i},\mathbf{y}_{i}\}_{i=1}^{k}\) for each test input \(\mathbf{x}^{\emptyset}\). Next, the type-related feature extractor uses mutual information criteria to derive pseudo TRF labels \(\{\mathcal{R}_{i}\}_{i=1}^{k}\) for the self-annotated demonstrations and facilitates identifying relevant contextual correlations \(\mathcal{R}^{\emptyset}\) for \(\mathbf{x}^{\emptyset}\) using specialized ICL. Furthermore, the demonstration discriminator, considering the TRFs of both the target sentence and its self-annotated demonstrations, applies a self-reflection mechanism to automatically assess the helpfulness scores \(\{h_{i}\}_{i=1}^{k}\) of selected demonstrations in making predictions for each target sentence \(\mathbf{x}^{\emptyset}\). Finally, the overall predictor constructs a question-answering prompt template \(\mathbf{T}_{o}\), synthesizing TRFs and helpfulness scores, to obtain the final prediction \(\mathbf{y}^{\emptyset}\) for each target sentence \(\mathbf{x}^{\emptyset}\). Through their specialized abilities and communications, the designed agents work collaboratively to enhance both the effectiveness and generalizability of our proposed CMAS for zero-shot and few-shot NER tasks (see Section 6 and 7).

## 5. Experiments

**Research questions.** We aim to answer the following research questions: (RQ1) Does CMAS outperform state-of-the-art methods on the zero-shot NER task? (See Section 6.1) (RQ2) Can CMAS be generalized to the few-shot setting? (See Section 6.2)

**Datasets.** In our experiments, we evaluate CMAS on both general-domain and domain-specific datasets. Detailed statistics of the datasets used are shown in Prompt 5 (in the Appendix). For our general-domain experiments, we consider four commonly used benchmarks, namely the CoNLL03 (Xu et al., 2018), WikiGold (Chen et al., 2017), WNUT-17 (Chen et al., 2017), and OntoNotes3 datasets. For our domain-specific experiments, we evaluate CMAS on the GENIA (Xu et al., 2018) and BioNLP11 (Xu et al., 2018) datasets in the biomedical domain.

Footnote 3: [https://catalog.ldc.upenn.edu/LDC2013T19](https://catalog.ldc.upenn.edu/LDC2013T19)

For zero-shot NER, to keep API usage costs under control, we randomly sample 300 examples from the test set three times and calculate the average performance. An exception is made for WikiGold, which has a test set of only 247 examples. Additionally, we randomly sample 500 examples from the training set as the unlabeled corpus. In the few-shot settings, we examine configurations including o-shot, 3-shot, 5-shot, and 10-shot, where "shot" refers to the number of demonstrations with gold labels provided to LLMs (see Section 6.2). For each setting, we randomly sample three sets of demonstrations and compute the average results.

**Baselines** For the zero-shot and few-shot NER tasks, we compare CMAS with the following baselines: (i) **Vanilla**(Vanilla, 2016; Zhang et al., 2017) employs a straightforward and common prompting strategy that directly asks LLMs to extract entity labels from input texts. (ii) **ChatIE**(Zhu et al., 2017) transforms the zero-shot NER task into a multi-turn question-answering problem using a two-stage framework. (iii) **Decomposed-QA**(Vanilla, 2016) breaks down the zero-shot NER task into a series of simpler sub-problems by labels and follows a decomposed-question-answering paradigm, where the model extracts entities of only one label at a time. (iv) Based on Decomposed-QA, **SALLM**(Vanilla, 2016) further adopts syntactic augmentation to stimulate the model's intermediate reasoning in two ways, including syntactic prompting and tool augmentation. (v) **SILLM**(Vanilla, 2016) applies a self-improving framework, which uses unlabeled corpora to stimulate the self-learning abilities of LLMs in zero-shot NER.

In our experiments, we evaluate all the aforementioned baselines for the zero-shot NER task. For few-shot NER, we assess SILLM (Vanilla, 2016; Zhang et al., 2017) and Vanilla (Vanilla, 2016; Zhang et al., 2017) since ChatIE(Zhu et al., 2017), Decomposed-QA (Vanilla, 2016), and SALLM (Vanilla, 2016) do not incorporate task demonstrations in their prompt templates. Additionally, we report the highest results obtained from the model variants of SALLM (Vanilla, 2016) and SILLM (Vanilla, 2016) in our experiments. For fair comparisons and cost savings, we employ GPT-3.5 (specifically, the gpt-3.5-turbo-0125 model1) as the LLM backbone for all baselines and agents in CMAS. We use the text-embedding-ada-002 model,2 a text-embedding model from OpenAI, to obtain sentence representations. We access OpenAI models using the official API.

Footnote 1: [https://platform.openai.com/docs/models/gpt-3.5-turbo](https://platform.openai.com/docs/models/gpt-3.5-turbo)

### Evaluation metrics

Following previous work (Vanilla, 2016; Zhang et al., 2017), we conduct our evaluation of zero-shot and few-shot NER tasks using only complete matching and employing the micro F1-score to assess the NER task. We consider a prediction correct only when both the boundary and the type of the predicted entity exactly match those of the true entity.

### Implementation details

Following Xie et al. (Xie et al., 2016) and Wang et al. (Wang et al., 2016), we set the number of nearest neighbors \(K=50\) during self-annotation and the number of task demonstrations \(k=16\). For self-consistency scores, we set the temperature to 0.7 and sample 5 answers. We set the frequency ratio hyperparameter \(\rho\) to 3 for all experiments and only consider 1-gram texts for simplicity.

## 6. Experimental Results

To answer RQ1 and RQ2, we assess the performance of CMAS on both zero-shot and few-shot NER tasks.

### Results on zero-shot NER

We turn to RQ1. Table 1 shows the experimental results on both general-domain and domain-specific datasets. To ensure fair comparisons, we reproduce all the baseline models using the gpt-3.5-turbo-0125 model, as they are implemented with different versions of GPT-3.5 in the original papers.

We have the following observations: (i) Zero-shot NER is challenging, and baseline models struggle to achieve an F1-score above 60% on most of the datasets. For instance, ChatIE only obtains F1-scores of 37.46% and 29.00% on the WNUT-17 and OntoNotes datasets, respectively. (ii) CMAS achieves the highest F1-scores across all datasets, indicating its superior performance. For instance, CMAS attains F1-scores of 76.23% and 60.51% on the WikiGold and BioNLP1 datasets, respectively. (iii) CMAS significantly outperforms the previous state-of-the-art baselines across all datasets. For example, CMAS achieves improvements of 13.21% and 4.49% over the best-performing baselines on the WNUT-17 and GENIA datasets, respectively.

In summary, CMAS demonstrates its effectiveness in recognizing named entities in a strict zero-shot setting. The identification of contextual correlations and the evaluation of helpfulness scores for task demonstrations are beneficial for zero-shot NER.

### Results on few-shot NER

To investigate the effectiveness of CMAS on the few-shot setting, we turn to RQ2. In our experiments, we evaluate CMAS and the baselines in 0-shot, 3-shot, 5-shot, and 10-shot settings, where micro F1-scores are reported. ChatIE, Decomposed-QA, and SALLM are excluded because incorporating gold demonstrations into their prompt templates is non-trivial and beyond the scope of this paper.

Based on Figure 3, we arrive at the following conclusions: (i) Increasing the number of demonstrations with gold labels does not necessarily enhance the prediction performance of LLMs. For example, as the number of demonstrations with gold labels increases from 0 to 10, the F1-score of the Vanilla model significantly drops from 40.10% to 27.10% on the WNUT-17 dataset. This decline may be due to the random selection of demonstrations, which can be highly irrelevant to the target sentence and severely misguide the inference process of LLMs. (ii) CMAS achieves the highest F1-scores and consistently outperforms the state-of-the-art baselines across all few-shot settings, demonstrating its effectiveness and robustness. For example, CMAS exhibits an average improvement of 19.51% and 13.10% over SILLM on the WNUT-17 and GENIA datasets, respectively.

In summary, our proposed CMAS not only effectively extracts entities in the strict zero-shot setting but also achieves the highest F1-scores across all few-shot settings while maintaining robustness to irrelevant demonstrations.

## 7. Analysis

Now that we have addressed our research questions, we take a closer look at CMAS to analyze its performance and generalizability. We examine the contributions of the type-related feature extractor and the demonstration discriminator to its effectiveness (see Section 7.1), investigate its generalizability to different LLM backbones (see Section 7.2) and varying numbers of task demonstrations (see Section A.3), and assess its capability in error correction (see Section 7.3).

### Ablation studies

To study the individual contributions of each component to CMAS's performance, we conduct ablation studies on the WikiGold, WNUT-17, and GENIA datasets. The results are presented in Table 2.

Given that the demonstration discriminator relies on entity type-related information from the TRF extractor, it is not feasible to independently remove the TRF extractor. When we ablate only the demonstration discriminator ( - Discriminator'), the overall predictor incorporates only TRF for retrieved demonstrations and target sentences. This exclusion results in a significant drop in CMAS's performance across all three datasets. For instance, CMAS achieves 3.34% and 5.59% higher F1-scores on the WikiGold and GENIA datasets, respectively, compared to its model variant without the demonstration discriminator. These findings highlight the crucial role of evaluating the usefulness of retrieved demonstrations in making predictions. In scenarios where both the demonstration discriminator and the TRF extractor are ablated ( - TRF Extractor'), CMAS reverts to the baseline model, SILLM. The results indicate that identifying contextual correlations surrounding entities considerably enhances SILLM's performance.

In summary, both the demonstration discriminator and the TRF extractor contribute markedly to CMAS's performance improvements over the baselines in the zero-shot NER task.

Furthermore, similar to SALLM, CMAS is readily adaptable for augmentation with external syntactic tools. Following Xie et al. (2020), we obtain four types of syntactic information (i.e., word segmentation, POS tags, constituency trees, and dependency trees) via a parsing tool (Chen et al., 2019) and integrate the syntactic information into the overall predictor of CMAS using a combination of tool augmentation and syntactic prompting strategies. As shown in Table 2, the inclusion of dependency tree information improves CMAS's performance by 2.52% and 2.94% on WNUT-17 and GENIA, respectively. These results demonstrate that the integration of appropriate external tools further enhances the performance of CMAS.

### Influence of different LLM backbones

To explore the impact of different LLM backbones, we evaluate CMAS and baseline models using the latest LLMs, including GPT (gpt-3.5-turbo-0125), Llama (Meta-Llama-3-88-Instruct6), and Qwen (Wen2.5-7B-Instruct7). Table 3 illustrates the zero-shot NER performance on the WNUT-17 and GENIA datasets. We exclude the performance of Charlie and Decomposed-QA, as their F1-scores with Qwen and Llama backbones are considerably lower than other baselines. As Table 3 shows, CMAS achieves the highest F1-scores when using GPT as the backbone model. Additionally, CMAS consistently outperforms the baselines across various LLM backbones, demonstrating its superiority and generalizability.

\begin{table}
\begin{tabular}{l c c c c c c} \hline \hline \multirow{2}{*}{**Model**} & \multicolumn{6}{c}{**Datasets**} \\ \cline{2-7}  & **CoNLL03** & **WikiGold** & **WNUT-17** & **OntoNotes** & **GENIA** & **BioNLP11** \\ \hline Vanilla (Xie et al., 2020) & 72.54 & 74.27 & 40.10 & 45.09 & 43.47 & 53.92 \\ ChatIE (Chen et al., 2020) & 50.13 & 56.78 & 37.46 & 29.00 & 47.85 & 45.56 \\ Decomposed-QA (Xie et al., 2020) & 52.61 & 64.05 & 42.38 & 35.96 & 34.03 & 57.26 \\ SALLM (Xie et al., 2020) & 68.97 & 72.14 & 38.66 & 44.53 & 42.33 & 55.06 \\ SILLM (Xie et al., 2020) & 72.96 & 72.72 & 41.65 & 45.34 & 45.66 & 44.99 \\ \hline CMAS (ours) & **76.43\({}^{\text{s}}\)** & **76.23\({}^{\text{s}}\)** & **47.98\({}^{\text{s}}\)** & **46.23\({}^{\text{s}}\)** & **50.00\({}^{\text{s}}\)** & **60.51\({}^{\text{s}}\)** \\ \hline \hline \end{tabular}
\end{table}
Table 1. Zero-shot NER results (F1) on both general-domain and domain-specific datasets. Numbers in bold are the highest results for the corresponding dataset, while numbers underlined represent the second-best results. Significant improvements against the best-performing baseline for each dataset are marked with \(*\) (t-test, \(p<0.05\)).

Figure 3. Few-shot NER results (F1) on WikiGold, WNUT-17, and GENIA.

### Error analysis

To investigate CMAS's error correction capabilities, we conduct an analysis of the following errors on the WNUT-17 dataset:

* **Type errors**: (i) **OOD types** are predicted entity types not in the predefined label set; (ii) **Wrong types** are predicted entity types incorrect but in the predefined label set.
* **Boundary errors**: (i) **Contain gold** are incorrectly predicted mentions that contain gold mentions; (ii) **Contained by gold** are incorrectly predicted mentions that are contained by gold mentions; (iii) **Overlap with gold** are incorrectly predicted mentions that do not fit the above situations but still overlap with gold mentions.
* **Completely-Os** are incorrectly predicted mentions that do not coincide with any of the three boundary situations associated with gold mentions.
* **OOD mentions** are predicted mentions that do not appear in the input text.
* **Omitted mentions** are entity mentions that models fail to identify.

Figure 5 (in the Appendix) visualizes the percentages of error types. The majority error types are _overlap with gold_ and _omnited mentions_, which account for 72.30% of all errors. These errors may result from incomplete annotations or predictions influenced by the prior knowledge of LLMs. Table 4 summarizes the statistics of error types. With the implementation of the proposed type-related feature extractor and demonstration discriminator, CMAS significantly reduces the total number of errors by 30.60% and 74.60% compared to state-of-the-art baselines SALLM and SILLM, respectively, demonstrating its remarkable effectiveness in error correction.

## 8. Conclusions

We have focused on named entity recognition in the strict zero-shot setting, where no annotated data is available. Previous approaches to zero-shot named entity recognition still encounter two significant challenges: they often overlook contextual correlations and use task demonstrations indiscriminately, both of which can significantly impede the inference process. To tackle these challenges, we have introduced a new framework, _cooperative multi-agent system_ (CMAS), which uses the collective intelligence and specialized capabilities of agents. CMAS explicitly captures correlations between contexts surrounding entities by reformulating named entity recognition into two subasks: recognizing named entities and identifying entity type-related features within the target sentence.

To evaluate the quality of demonstrations, a demonstration discriminator is established to incorporate a self-reflection mechanism, automatically evaluating helpfulness scores for the target sentence and enabling controlled use of demonstrations. Experimental results show that CMAS significantly enhances zero-shot NER performance across six benchmarks, spanning both domain-specific and general-domain datasets. and exhibits strong capabilities in correcting various types of errors.

A limitation of CMAS is that it primarily focuses on a limited set of predefined entity labels. Expanding it to support open NER tasks would be valuable. Future work also includes developing interactive prompt designs, such as multi-turn question answering, for LLM-based agents to iteratively refine or assess responses.

\begin{table}
\begin{tabular}{l c c c c c}
**Model** & \multicolumn{4}{c}{**WNUT-17**} & \multicolumn{2}{c}{**GENIA**} \\ \cline{2-7}  & **GPT** & **Llama** & **Qwen** & **GPT** & **Llama** & **Qwen** \\ \hline Vanilla [54, 55] & 40.10 & 34.88 & 34.93 & 43.47 & 15.36 & 9.97 \\ SALLM [54] & 38.66 & 40.95 & 41.50 & 42.33 & 36.23 & 19.13 \\ SILLM [54] & 41.65 & 22.43 & 36.23 & 45.66 & 28.13 & 33.80 \\ \hline CMAS (ours) & **47.98*** & **42.36*** & **44.62*** & **50.00*** & **45.68*** & **36.12*** \\ \hline \end{tabular}
\end{table}
Table 4. Numbers of different error types on GENIA. Numbers in bold denote the best results for the corresponding error type, i.e., the least errors, while numbers underlined represent the second-best results.

\begin{table}
\begin{tabular}{l c c c}
**Model** & \multicolumn{3}{c}{**Datasets**} \\ \cline{2-4}  & **WikiGold** & **WNUT-17** & **GENIA** \\ \hline Vanilla [54, 55] & 74.27 & 40.10 & 43.47 \\ ChatIE [49] & 56.78 & 37.46 & 47.85 \\ Decomposed-QA [54] & 64.05 & 42.38 & 34.03 \\ SALLM [54] & 72.14 & 38.66 & 42.33 \\ \hline CMAS (ours) & **76.23** & **47.98** & **50.00** \\ - Discriminator & 73.76 & 45.44 & 48.41 \\ - TRF extractor & 72.72 & 41.65 & 45.66 \\ \hline \multicolumn{4}{c}{**External tool augmentation**} \\ \hline Word segmentation & **76.92** & 47.63 & 49.22 \\ POS tag & 76.14 & 48.11 & 49.76 \\ Constituency tree & 75.71 & 47.44 & 49.64 \\ Dependency tree & 76.27 & **49.19** & **51.47** \\ \hline \end{tabular}
\end{table}
Table 2. Ablation studies (F1) on WikiGold, WNUT-17, and GENIA.

## References

* (1)
* Asai et al. (2024) Alari Asai, Zeqiu Wu, Yizhong Wang, Avirop Su, and Hannaneh Hajishirzi. 2024. Self-Ad: Learning to Retrieve, Generate, and Critique through Self-Reflection. In _ICLR_.
* Balasuriy et al. (2009) Dominic Balasuriy, Nicky Ringuid, Joel Nothman, Tara Murphy, and James R. G. Curran. 2009. Named Entity Recognition in Wikipedia. In _PWLWLPQWLSLP_.
* Chowdhury et al. (2020) Alexandra Chowdhury, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Setton, Sebastian Ghemawat, Patrick Schutzen, Sebastian Sidi, Shahar Tsyukcuhkos, Joshua Mayne, Alibaba Rao, Yazber Barnes, T. Tayu, Ngoar Shazeer, Vinodhumar Prabhakaran, Emily Rolf, Nan Das, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Land, Guy Gur-Auf, Pengyang Yu, Tsqi Zhao, Anelia Levaybas, Sangyin Ghisem, Swantjer, Deep Harsby, Michaela, Kayter Garcia, Vedant Misra, Kevin Robinson, Lian Zhu, Grednyaz Singh, Joseph P. Jacobsu, David Lian, Hyventuke Lim, Barner Zoph, Alexander S Spindl, Ryan Sypass, David Dolan, Shivam Agrawal, Mark Chernick, Andrew M. Dai, Thammaryan Sankaranarayana, Phillii Marie Pollat, Andrei Lewkowye, Erica Morreau, Bercha Oldelsandre, Sylvain Pedretti, Catherine Lee, Zongwei Zhou, Xinchi Wang, Brennan Seta, Mark Diaz, Orihan Patcher, Michelle Castan, Jason Wett, Kathy Metzler-Hellner, Douglas Eck, Jeff Dean, Steve Perkov, and Jason Feichel. 2023. Malt: Scaling Language Modeling with Pathways. _J. Mach. Learn. Res._ 24 (2020), 2404-2401.1138.
* Pleczynski et al. (2017) Leon Pleczynski, Eric Nichols, Mariske van Em, and Neil Limongstam. 2017. Results of the WNUT2017 Shared Task on Novel and Emerging Entity Recognition. In _INTER/IEEE International_. 140-147.
* Pan et al. (2021) Yixing Fan, Jiafeng Gao, Chiyua Ma, Ruging Zhang, Yanyan Lan, and Xueqi Cheng. 2021. A Linguistic Study on Relevance Modeling in Information Retrieval. In _WWW_. 1055-1064.
* Guo et al. (2007) Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li. 2007. Named Entity Recognition in Query. In _SIGIR_. 267-274.
* Gao et al. (2024) Yalong Gao, Guilin Xiong, Cheng Chen, Yaqi Wang, Ruidi Chang, Shichao Fei, Nietsh V. Chawla, Olaf Wolf, and Xiangliang Zhang. 2024. Large Language Model based Multi-Agents: A Survey of Embeddings and Categories. _CoRR_ abs/2402.046180 (2024).
* Guo et al. (2023) Taicheng Guo, Kehan Guo, Boohang Nan, Zhenguen Liang, Zhichen Guo, Nietsh V. Chawla, Olaf Wolf, and Xiangliang Zhang. 2023. What Indeed an CirqT Module is Do In Chemistry? A Comprehensive Benchmark on Eight Tasks. _CoRR_ abs/2305.138505 (2023).
* Han et al. (2023) Bidong Han, Dong Peng, Chaohao Yang, Renyou Wang, Lu Liu, and Xiang Wan. 2023. Is Information Extraction Solved by UniGIFT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors. _CoRR_ abs/2305.14450 (2023).
* Han and Choi (2021) Han and He and Jinho D. Choi. 2021. The Scent Cell Effectiveness: Dislemma behind Multi-Task Learning with Transformer Encoders. In _EMNLP_. 5553-5557.
* Hong et al. (2024) Swinqi Hong, Mingchen Zhang, Jonathan Chen, Xiaoxu Zheng, Yihong Cheng, Jinlin Wang, Cycwa Zhang, Zili Wang, Steven Ka Sising Yaz, Zijian Liu, Liyang Zhou, Chunyu Fan, Lingfeng Xiao, Chenghui Wu, and Jurgen Schmidhuber. 2024. MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework. In _ICLR_.
* Ito and Nakagawa (2024) Tomok Ito and Shu Nakagawa. 2024. Tender Document Analyzer with the Combination of Supervised Learning and LIM-based Improver. In _WWW_. 995-988.
* Jin et al. (2024) Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, and Xia Jin. 2024. LIM Maybe LongLM: Self-defined LIM Context Without Thinking. In _ICML_.
* Jonsfuski et al. (2023) Martin Jonsfuski, Marija Salocha, Maxime Peyoral, and Robert West. 2023. Exploiting Arguments for Synthetic Training Data Generation: Synthlife and the Case of Information Extraction. In _EMNLP_. 1555-1574.
* Kojima et al. (2022) Takehi Kojima, Shixing Shane Gao, Michel Reid, Yutaka Mattao, and Yunuke Iwasawa. 2022. Large Language Models Meore: Zero-Shot Reason. In _NeurIPS_.
* Laskar et al. (2023) Md. Tahmid Rahman Laskar, Satiful Bari, Mizunar Rahman, Md Amram Hossen Bhuyan, Shafiq Joty, and Jimmy Xiangli Huang. 2023. A Systematic Study and Comprehensive Evaluation of CirqT on benchmark Datasets. In _ACL_. 431-469.
* Lee et al. (2023) Seoungun Lee, Hyunjie Kim, and Jaewoo Kang. 2023. LIQUID: A Framework for List Question Answering Dataset Generation. In _AAAI_. 13014-13042.
* Li et al. (2023) Bo Li, Geiang Fang, Yang Tang Yuan, Wang Ji, Wen Zhao, and Shixun Zhang. 2023. Evaluating Character's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness. _CoRR_ abs/2304.11633 (2023).
* Zhao et al. (2023) Minghao Li, Yingxin Zhao, Bowen Yu, Feiin Song, Hangyu Li, Haiyang Yu, Zhequn Li, Fei Huang, and Yonghui Li. 2023. APH-Basic A Comprehensive Benchmark for Tool-Mengmented LIMs. In _EMNLP_. 3102-3116.
* Lung et al. (2023) Zhenwei Lung, Duo Yu, Tamayo Raynquith, Peter Clark, Xiangliang Zhang, and Ashwin Kalyan. 2023. LGT per a Multi-Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation. In _EMNLP_. 14384-14396.
* Liu et al. (2024) Nelson F. Liu, Kevin Lin, John Brouett, Ashvin Paranjape, Michele Bevinyua, Fabio Petroni, and Percy Liang. 2024. Lost in the Middle: How Language Models Use Long Contects. _Trans. Assoc. Comput. Linguistics_ 12 (2024), 157-173.
* Ma et al. (2024) Mingyu Derex Me, Xiaoxuan Wang, Po-Nien Kung, P. Jeffrey Brantingham, Nanyun Peng, and Wei Wang. 2024. TIR: Boosting Low-Resource Information Extraction by Structure-Text Data Generation with Large Language Models. In _AAAI_. 18751-18759.
* Mandl et al. (2023) Zhao Mandl, Sheeya Jain, and Shuran Song. 2023. RoCo: Dialectic Multi-Robot Collaboration with Large Language Models. _CoRR_ abs/2307.04738 (2023).
* Mei (2024) AI Mei. 2024. Introducing Meta Llamma 3: The Most Capable Openly Available ILM to Date. _Meta AI_ (2024).
* Ohta et al. (2022) Tomoko Ohta, Yuka Tateisi, Jin-Dong Kim, Hideki Minn, and Junichi Tsujii. 2022. The ERNAL Corpus: An Annotated Research Abstract Corpus in Molecular Biology Domain. In _ITL_. 73-77.
* Open2 (2022) Open22. Introducing ChatFORT. (2022).
* (2023) Open23. GP7-T-Technical Report. CoRR abs/2303.07874 (2023).
* Park et al. (2023) Joon Sung Park, Joseph C. O'Brien, Carrie Pan, Alex Merchik Ringel Morris, Perey Liang, and Mitchell S. Bernstein. 2023. Generative Agents: Interactive Sminkera of Human Behavior. In _IJST_. 221-222.
* Park et al. (2022) Joon Sung Park, Lindoyen Popovski, Carrie J. Cai, Merethith Ringel Morris, Perey Liang, and Michael S. Bernstein. 2022. Social Summar: Creating Postplated Prototypes for Social Computing Systems. In _IJST_. 741-7418.
* Peters et al. (2017) Matthew E. Peters, Waleed Ammar, Chandra Bhagovitath, and Russell Power. 2017. Semi-Supervised Sequence Tagging with Bidirectional Language Models. In _ACL_. 1765-1765.
* Pysala et al. (2012) Congyasala, Tomoko Ohta, Rafal Rak, Daniel E. Sullivan, Chunhong Mao, Chunxia Wang, Bruno W. S. Sobral, Jun'ichi Tsujii, and Sophia Ananiola. 2012. Overview of the ID, EP1 and REL tasks of BioNLP Shared Task 2011. _BMC Bioinform._ 13, 5-11 (2012), 52.
* Qian et al. (2023) Chen Qian, Yan Cong, Cheng Yang, Wie Lee, Yunheng Su, Jiyuan Xu, Zhiyuan Liu, and Maosong Sun. 2021. Communicate Agents for Software Development. _CoRR_ abs/2307.09742 (2023).
* Qi et al. (2024) Yiwei Qi, Kaiqing Su, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Kunming Wu, Fei Liu, Pengfei Liu, and Dong Yu. 2024. InfoBench: Evaluating Instruction Following Ability in Large Language Models. _CoRR_ abs/2401.06301 (2024).
* Ren et al. (2021) Mengfei Ren, Boi Cao, Hongyu Liu, Cao Liu, Ximeqi, Tan Ke, Zeng, Ganguli Wan, Xulmia, Cai, and L Sun. 2024. Learning or Self-aligning? Rethinking Instruction Fine-tuning. In _ACL_. 6009-6105.
* Sain et al. (2023) Oscar Sain, Berf Garcia-Ferrero, Rodrigo Ageri, Oeter Lopez de la Leale, German Rigau, and Enodo Agirre. 2023. CoILLE: Annotation Guidelines improve Zero-Shot Information: Extraction. _CoRR_ abs/2303.06823 (2023).
* Kim et al. (2023) Erk, Tjong Kim Sang and Feng De Meulder. 2023. Introduction to the CaNLL-2003 Shared Task: Language-independent Named Entity Recognition. In _CoNLL_. 142-147.
* Schmidt et al. (2024) Maximilian Schmidt, Andrea Barterzaghi, and Sogo-Thang Vu. 2024. Prompting-based Synthetic Data Generation for Few-Shot Question Answering. In _IEEE/COLING_. 13168-13178.
* Shim et al. (2023) Noah Shim, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunya Yao. 2023. Reflexion: Language Agents with Verbal Reinforcement Learning. In _NeurIPS_.
* Souma et al. (2019) Faiba Souma, Rodrigo Frascito Nogueira, and Roberto de Alencar Loutdo. 2019. Portuguese Named Entity Recognition using BERT-CRF. _CoRR_ abs/1909.10649 (2019).
* Wan et al. (2023) Zhen Wan, Fei Cheng, Zhuoyuan Mao, Qianying Liu, Haiyang Song, Jiwei Li, and Salao Kurohnishi. 2023. GPT-ER: In-context Learning for Relation Extraction using Large Language Models. _arXiv e2303.04235_ (2023).
* Wang et al. (2022) Boali Wang, Seem Nina, Xing, Deng, Jeming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. 2022. Towards Understanding Chain-of-Throught Prompting: An Empirical Study of What Matters. _arXiv e2212.10001_ (2022).
* Wang et al. (2021) Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, and Tao Qin. August 2021. Generalizing to Unseen Domains: A Survey on Domain Generalization. In _ICLR_. 4627-4635.
* Wang et al. (2024) Lei Wang, Chen, Xia, Yueyang Feng, Ziyang Zhang, Hao Zhang, Zhiyuan Chen, Jiashi Xue, Chen, Jiashi Li, Wayne Xin Zhao, Zhewei Wei, and Jiriong Wen. 2024. A Survey on Language Models. In _ICLR_.
* Wang et al. (2023) Shenish Wang, Chang Liu, Zidong Zheng, Suyan Qi, Shuo Chen, Qisen Yang, Andrey Zhao, Chaeifei Wang, Shiqi Song, and Gao Huang. 2023. Avalon's Game of Thought-Hellner Against Descript through Recursive Contemplation. _CoRR_ abs/2310.01320 (2023).
* Wang et al. (2023) Shuku Wang, Xueqiu Wen, Xiaoyang Li, Rongliu Ouyang, Fyi Wu, Tawan Zhang, and Liqi. 2023. GPT-NER: Named Entity Recognition via Large Language Models. _CoRR_ abs/2304.10428 (2023).
* Wang et al. (2023) Kwoih Wang, Jason Wei, Dale Schumans, Quoc V. Le, EdH. Chi, Suzhan Narang, Akakinsh Choudhary, and Denny Zhou. 2023. Self-Consistency Improves Chain of Thought Reasoning in Language Models. In _ICLR_.
* Zhu etLanguage Models. _CoRR_ abs/2201.11903 (2022).
* [49] Xiang Wei, Xingyu Cui, Ning Cheng, Xiaohui Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jianxai Xu, Yufeng Chen, Xiaohui Zhang, Yong Jiang, and Wenjian Han. 2023. Zero-shot Information Extraction via ChattGNT. _CoRR_ abs/2302.10205 (2023).
* [50] Michael J. Wooldridge and Nicholas R. Jennings. 1995. Intelligent Agents: Theory and Practice. _Knowl. Eng. Rev._ 159, 115-152.
* [51] Chennaiwa Wu, Wenjun Xie, Peng Wang, Zhihao Luo, Guangzheng Li, and Wanyi Chen. 2022. ConstantNER: Towards Instructing NER Demonstrations for ILMs with the Consistency of Ontology and Control. In _AAAI_. 19342-19342.
* [52] Zhiheng X. Wenniang Chen, Xiaohui Guo, Wei He, Yiwen Ding, Boywang Hong, Ming Zhang, Juntao Wang, Sengjie, Injie Ezw Zhou, Hui Zhuo, Zhiqiang Xu, Jianxiao Xiong, Juntao Zhou, Wenjun Xie, Qinghao Jiang, Yiheng Zou, Xiangyang Liu, Zizheng Yin, Shihun Dou, Rongzheng Wang, Wenxen Cheng, Qiang Zhu, Wenjun Qin, Yongjun Zhao, Kipeng Wang, and Tao Gui. 2023. The Rise and Potential of Large Language Model Based Agents: A Survey. _CoRR_ abs/2309.07864 (2023).
* [53] Bashi Xia, Ziyuan Yin, and Ziyuan Shan. 2023. Simulating Public Administration Crisis A Novel Generative Agent-Based Simulation System to Lower Technology Barriers in Social Science. _CoRR_ abs/2110.0057 (2023).
* [54] Tingyu Xie, Qi Li, Jian Zhang, Yan Zhang, Xiaohui Lu, and Hongwei Wang. 2023. Empirical Study of Zero-shot NER with ChattGNT. In _EMNLP_. 765-7956.
* [55] Tingyu Xie, Qi Li, Yan Zhang, Zoxolin Liu, and Hongwei Wang. 2024. Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models. In _NAACL_. 583-593.
* [56] Zeitat Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu. 2023. Language Agents with Reinforcement Learning for Strategic Play in the Werewolf Game. _CoRR_ abs/2310.18940 (2023).
* [57] Shinyu Yao, Dan Yu, Jeffrey Zhao, Inikah Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. In _NeurIPS_.
* [58] Hongxin Zhang, Weihua Du, Jianing Shan, Qinhong Zhou, Yiuan Du, Joshua B. Tenenbaum, Tianmin Shu, and Chuang Gan. 2023. Building Cooperative Embodied Agents Modularity with Large Language Models. _CoRR_ abs/2307.02485 (2023).
* [59] Ziusheng Zhang, Aston Zhang, Mo Li, and Alex Smola. 2023. Automatic Chain of Thought Propping in Large Language Models. In _ICLR_.
* [60] Zhen Zhang, Yutua Zhang, Hang Gao, and Mengting Hu. 2024. LinkNER: Linking Local Named Entity Recognition Models to Large Language Models using Uncertainty. In _WWW_. 0407-04038.
* [61] Denny Zhou, Nathansed Schatz, Le Hong, Noon Wei, Nathan Scales, Xuresh Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Lv, and Ed H. Chi. 2023. Least-to-Most Prompting Franbles Complex Reasoning in Large Language Models. In _ICLR_.
* [62] Wenxuan Zhou, Sheng Zhang, Yi Gu, Muhao Chen, and Hoifung Poon. 2024. UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition. In _ICLR_.

## Appendix A Appendix

### Prompts used in CMAS

Prompt 1, 2, 3, and 4 show prompts used in the self-annotator, TRF extractor, demonstration discriminator, and overall predictor, respectively.

### Statistics of the datasets used

Table 5 demonstrates the detailed statistics of the dataset used in our experiments.

### Influence of task demonstration amount

To assess the influence of the number of task demonstrations, we evaluate CMAS and SILLM with varying numbers of task demonstrations, ranging from 2 to 20. Figure 4 details the zero-shot NER performance on the WNUT-17 and GENIA datasets. It is important to note that other baselines are excluded from this analysis as they do not incorporate task demonstrations in their prompt templates. The results in Figure 4 indicate that CMAS consistently outperforms SILLM across various numbers of task demonstrations on both datasets. Specifically, CMAS achieves an average F1-score improvement of 18.70% and 10.72% over SILLM on the WNUT-17 and GENIA datasets, respectively. This is, CMAS is able to effectively discriminate against irrelevant task demonstrations, maintaining robustness across different numbers of task demonstrations.

### Percentages of different error types

Figure 5 presents the percentages of error types on GENIA using CMAS.

Figure 4. Influence of task demonstration amount (F1) on WNUT-17 and GENIA.

Figure 5. Percentages of different error types on GENIA using CMAS.

\begin{table}
\begin{tabular}{l l l l l l l} \hline \hline Dataset & CoNLL03 & WikiGold & WNUT-17 & OntoNotes & GENIA & BioNLP11 & 1380 \\ \hline \#Train & 14,382 & 1,422 & 4,403 & 68,452 & 16,692 & 3,217 & 1380 \\ \#Test & 3,453 & 274 & 1,287 & 8,262 & 1,854 & 1,961 & 1380 \\ \hline \hline \end{tabular}
\end{table}
Table 5. Statistics of the datasets used. The training set is formed by combining the original training and development sets.

[MISSING_PAGE_EMPTY:13]