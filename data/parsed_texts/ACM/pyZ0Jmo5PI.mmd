[MISSING_PAGE_FAIL:1]

distillation while introducing a knowledge contribution-aware aggregation rule via Shapley value to discern and aggregate high-quality model updates. To fulfill these objectives, FedMobile is guided by two primary goals: 1) Effectively reconstructing features of missing modalities without exacerbating modality heterogeneity or compromising main task performance. 2) Streamlining the process of identifying mobile nodes with substantial contributions while minimizing computational overhead. Next, we focus on responding to the following two challenges:

\(\bullet\)_(C1.) How to collaboratively interpolate missing sensing modal features for different nodes with cross-modal heterogeneity_

**(S1.)** - _The heterogeneous modality between different nodes has a common feature subspace._ In mobile sensing scenarios, malfunctioning sensor modalities at various nodes give rise to missing modalities, causing modality heterogeneity (Wang et al., 2017). In such circumstances, conventional solutions like zero-filling (Zhu et al., 2017) and parallel training of unimodal models (Zhu et al., 2017) often inadequately handle this inherent feature and modality diversity. Our goal, therefore, is to leverage knowledge distillation for constructing a shared feature subspace among node modalities to improve model performance. We implement a feature generator on both the server and node levels to tackle missing modality issues. This generator, trained in a coupled training manner, aims to align different modalities by capturing a common feature space.

\(\bullet\)_(C2.) How to find relevant metrics for measuring the contribution of a specific node in a computationally cost-friendly manner._

**(S2.)** - _Knowledge and model updates shared between nodes and server generalize the contributions of nodes._ In FedMobile, the contributor role of participating nodes is manifested across dual dimensions: knowledge shared by local generators and local model updates shared by local nodes. To incentivize local generators to yield high quality features for incomplete modalities, we devise a novel Clustered Shapley Value approach that quantifies the individual contributions of these generators. This subsequently allows for adaptive modulation of their respective weights, thus facilitating the aggregation of high-quality feature representations. Moreover, with the objective of discerning nodes that high-quality model updates, we introduce a contribution-aware aggregation mechanism designed to retain those elements that are conducive to the overall improvement of the global model. Conversely, it eliminates nodes that do not meet this criterion. By dynamically choosing nodes based on this principle, we effectively ensure the aggregation of high-quality model updates during the training.

Additionally, we evaluate FedMobile across five real-world multimodal sensing datasets: USC-HAD (Wang et al., 2017), MHAD (Zhu et al., 2017), Alzheimer's Disease Monitoring (ADM) (Zhu et al., 2017), C-MHAD (Wang et al., 2017), and FLASH (Zhu et al., 2017), which encompass tasks related to autonomous driving, mobile healthcare, and Alzheimer's disease detection. The results indicate that FedMobile effectively leverages various sensor types (such as GPS, gyroscopes, and radar) in scenarios with incomplete sensing modalities to accurately perform assigned tasks, even amid operational dynamics like sensor failures. Furthermore, we analyze the computational and communication overhead of FedMobile across different tasks. Extensive evaluations show that FedMobile outperforms existing multimodal FL systems (_e.g._, FedMM (Wang et al., 2017), AutoFed (Wang et al., 2017), and PmcmFL (Chen et al., 2018)), achieving higher model accuracy with reasonable additional computational and communication overheads, especially under dynamic modality missing conditions.

The contributions of our work can be summarized as follows:

_(1)_ We tailor FedMobile, a multimodal federated learning framework that is robust to incomplete modal data, for web-based mobile sensing systems in WoT.

_(2)_ We design a knowledge distillation-driven cross-node modality reconstruction network to efficiently reconstruct the missing modality data without introducing excessive overhead.

_(3)_ We design an efficient generator contribution evaluation module based on clustered Shapley value and contribution-aware aggregation mechanism to further improve system performance.

_(4)_ We implement our design and conduct extensive experiments on 5 datasets related to 3 mobile sensing downstream tasks to explore the performance, efficiency, generality, and parameter sensitivity of FedMobile. Compared to the baselines, our approach achieves state-of-the-art performance on all tasks while maintaining comparable computation and communication overhead.

## 2. Related Work

**Multimodal Learning for Mobile Sensing Systems.**Multimodal learning aims to extract complementary or independent knowledge from various modalities, enabling the representation of multimodal data (Zhu et al., 2017; Wang et al., 2017). This empowers machine learning models to comprehend and process diverse modal information (Zhu et al., 2017). As a result, multimodal learning techniques have become prevalent in mobile sensing, facilitating the development of systems that can understand and process diverse sensor data. For instance, multimodal learning can enhance model performance in areas such as traffic trajectory prediction (Zhu et al., 2017), disease diagnosis (Zhu et al., 2017), human activity recognition (Bengio et al., 2018), audio-visual speech recognition (Zhu et al., 2017), and visual question answering (Zhu et al., 2017). However, solving the problem of missing modalities in such systems remains an open challenge.

**Unimodal and Multimodal FL systems.** To address privacy concerns in mobile sensing systems, privacy-preserving distributed learning systems, notably FL (Zhu et al., 2017; Wang et al., 2017; Wang et al., 2017), are emerging as a solution. FL systems can be categorized into unimodal and multimodal FL based on the number of data modalities involved. Unimodal FL focuses on constructing a global model from unimodal data while preserving privacy (Zhu et al., 2017). Similarly, multimodal FL integrates data from multiple modalities to develop an effective global model (Wang et al., 2017). Multimodal FL systems are increasingly used in mobile sensing applications, particularly in tasks such as autonomous driving (Wang et al., 2017) and Alzheimer's disease detection (Zhu et al., 2017), due to their robust multimodal data processing capabilities.

**Multimodal FL Systems with Missing Modality.** Multimodal FL systems have emerged as a promising approach for training ML models across multiple modalities while preserving data privacy.

Figure 1. Unimodal FL vs multimodal FL.

However, in real-world scenarios, certain modalities may be missing from some nodes due to hardware limitations, data availability constraints, or privacy concerns (Han et al., 2017; Wang et al., 2018; Wang et al., 2019). To address this challenge, researchers have developed multimodal FL systems using various approaches, including modality filling (Wang et al., 2018), parallel training of unimodal models (Wang et al., 2018), and cross-model (Wang et al., 2019) techniques. For example, Xiong et al. (Xiong et al., 2019) introduced a modality-filling technique using reconstruction networks, while Ouyang et al. (Ouyang et al., 2019) proposed Harmony, a heterogeneous multimodal FL system based on disentangled model training. However, these methods often overlook the common feature space and the evaluation of node marginal contributions, leading to issues with model accuracy. This paper aims to address these challenges by developing a knowledge contribution-aware multimodal FL system for mobile sensing.

## 3. Preliminary

### Multimodal Federated Learning

Multimodal FL is a cutting-edge approach in machine learning (ML) that addresses the challenges of training models across multiple modalities while preserving data privacy. Formally, in mobile sensing scenarios, let us denote \(\mathcal{M}=\{m_{0},m_{1},\ldots,m_{M-1}\}\) as the set of modalities of the local multimodal dataset \(D_{k}\), \(K\) as the number of participating mobile nodes, \(n_{k}\) as the number of samples in the node \(k\), and \(d_{k}^{m}\) as the dimensionality of modality \(m\) in the node \(k\). The objective of Multimodal FL is to optimize a global model \(\mathcal{F}(\omega)\) parameterized by \(\omega\) across all modalities while minimizing the following federated loss function:

\[\min_{\omega}\sum_{k=1}^{K}\sum_{m\in\mathcal{M}}\frac{n_{k}}{n}\mathcal{L}( \omega;\mathbf{X}_{k}^{m},\mathbf{y}_{k}), \tag{1}\]

where \(\mathcal{L}(\omega;\mathbf{X}_{k}^{m},\mathbf{y}_{k})\) is the loss function for modality \(m\) at node \(k\), \(\mathbf{X}_{k}^{m}\) represents the data samples for modality \(m\) at node \(k\), \(\mathbf{y}_{k}\) is the target label associated with the samples at node \(k\), and \(n=\sum_{k=1}^{K}n_{k}\) represents the total number of samples across all nodes. In multimodal FL, the global model \(\mathcal{F}(\omega)\) is updated by aggregating local model updates from each node while respecting data privacy constraints. The update rule for the global model at iteration \(t\) can be formalized as:

\[\omega^{t+1}=\omega^{t}-\eta\sum_{k=1}^{K}\frac{n_{k}}{n}\nabla\mathcal{L}( \omega^{t};\mathbf{X}_{k},\mathbf{y}_{k}), \tag{2}\]

where \(\eta\) is the learning rate and \(\nabla\mathcal{L}(\omega^{t};\mathbf{X}_{k},\mathbf{y}_{k})\) is the gradient of the loss function with respect to the global model parameters \(\omega^{t}\) at node \(k\). Clearly, when a modal sensor on a mobile node fails or ceases to function, resulting in a missing modality, the optimization of Eqs. (1) and (2) becomes challenging. This impediment implies that multimodal FL may struggle to fulfill the designated task effectively under such circumstances.

### Shapley Value in ML

Shapley Value (Shapley, 1952) is a concept from cooperative game theory used to fairly distribute the value generated by a coalition of players. In the context of ML, it is often applied to understand the contribution of each feature to a model's prediction (Bahdanau et al., 2015). Let us denote a predictive model as \(f\), and \(\Phi_{i}(f)\) represents the Shapley value of feature \(i\) in the model \(f\). The Shapley value of feature \(i\) can be computed as:

\[\Phi_{i}(f)=\sum_{S\subseteq N\setminus\{i\}}\frac{|S|!(|N|-|S|-1)!}{|N|!}\left[ f(x_{S}\cup\{i\})-f(x_{S})\right], \tag{3}\]

where \(N\) is the set of all features, \(x_{S}\) represents the instance with only features in set \(S\), \(f(x_{S}\cup\{i\})\) is the prediction of the model when feature \(i\) is added to the set \(S\), \(f(x_{S})\) is the prediction of the model when only features in set \(S\) are considered, \(|S|\) denotes the cardinality of set \(S\), and \(|N|\) is the total number of features. The above formula computes the marginal contribution of feature \(i\) when added to different subsets \(S\) of features, weighted by the number of permutations of features in \(S\) to the total number of permutations of all features. In fact, calculating the Shapley value directly using the above formula might be computationally expensive (Bahdanau et al., 2015; Shapley, 1952), especially for models with a large number of features.

## 4. Our Approach

Overview.In this section, we present the proposed FedMobile framework, as illustrated in Fig. 2. First, to reconstruct missing sensing modalities, we design a knowledge distillation-driven cross-node modality reconstruction network. Secondly, to select generators with high-quality contributions, we design an efficient model contribution evaluation module based on clustered Shapley values. Finally, to further mitigate cross-node modal heterogeneity, we introduce a knowledge contribution-aware aggregation rule for robust aggregation. Next, we will introduce each functional component in detail.

### Knowledge Distillation-driven Cross-node Modalitiy Reconstruction Network

Impute Missing Modalities.Different from existing works such as AutoFed (Wang et al., 2019), which focus on reconstructing local missing modalities while ignoring cross-node feature information, FedMobile aims to collaboratively utilize the common feature subspace across nodes to iteratively reconstruct the feature information of missing modalities. Specifically, to gain insight into the data distribution of missing modalities across nodes and use this understanding to guide local model training with incomplete modalities, we employ conditional distributions to characterize the modal data distribution of each node. Let \(Q_{k}:\mathcal{Y}_{k}\rightarrow\mathcal{X}_{k}\) denote the above conditional distribution, which is tailored for each node and aligns with the ground truth data distribution. This distribution encapsulates the necessary knowledge to guide multimodal FL training with incomplete modalities:

\[Q_{k}=\operatorname*{arg\,max}_{Q_{k}:\mathcal{Y}_{k}\rightarrow\mathcal{X}_{k}} \mathbb{E}_{y\sim p(y_{k})}\mathbb{E}_{x\sim Q_{k}(\mathbf{x}_{k}|y_{k})}\left[ \log p(y|x;\mathbf{\omega}_{k})\right], \tag{4}\]

where \(p(y_{k})\) and \(p(y\mid x)\) denote the ground-truth prior and posterior distributions of the target labels, respectively. Given these conditions, we employ local models to infer \(p(y\mid x)\). Consequently, a straightforward approach involves the direct optimization of Eq. (4) in the input space \(\mathcal{X}_{k}\) to approximate features for missing modalities. However, when \(\mathcal{X}_{k}\) is of high dimensionality, this approach may lead to computational overload and could potentially disclose information about user data configuration files. Therefore,a more feasible alternative is to reconstruct an induced distribution \(\forall_{k}:\mathcal{M}_{k}\rightarrow\mathcal{Z}_{k}\) over a latent space. This latent space, being more compact than the raw data space, can help mitigate certain privacy-related concerns:

\[\forall_{k}=\underset{\forall_{k}:\mathcal{M}_{k}\rightarrow\mathcal{Z}_{k}}{ \arg\max}\ \mathbb{E}_{y\sim p(y_{k})}\mathbb{E}_{z\sim\mathcal{Y}_{k}(\mathbf{Z}_{k}| y_{k})}\left[\log p(y|z;\omega_{k})\right]. \tag{5}\]

Hence, nodes engage in knowledge extraction from missing modality data by acquiring insights from a parameterized condition generator \(\forall_{k}\) by \(\omega_{G}^{k}\). The optimization process is as follows:

\[\min_{\omega_{G}^{k}}J(\omega_{G}^{k})=\min\mathbb{E}_{y\sim p(y_{k})}\mathbb{ E}_{z\sim\mathcal{Y}_{k}(z|y_{s})}\left[\mathcal{L}\left(\sigma\left(g\left(z; \omega_{k}\right)\right),y\right)\right], \tag{6}\]

where \(y_{r}\) represents a set of random labels generated from the training dataset \(\mathcal{D}_{k}\), \(g(\cdot)\) denotes the logits output of a predictor, and \(\sigma(\cdot)\) signifies the non-linear activation applied to these logits.

Align Missing Modalities.On the other hand, it is necessary to refine features subspaces to more accurately encapsulate the local knowledge of nodes. For instance, considering a two-modality task, we can derive the generated latent space via the labels \(y_{k}\): \(\mathcal{Z}^{m_{1}},\mathcal{Z}^{m_{1}}=\forall_{k}\left(y_{k};\omega_{G}^{k}\right)\), where \(\mathcal{Z}^{m_{1}}\) and \(\mathcal{Z}^{m_{1}}\) represent the respective latent features of each modality. Assuming \(m_{1}\) denotes the missing modality, our objective is to further empower \(\forall_{k}\) to assimilate knowledge from various modalities, thereby enhancing the completeness and generalization of the feature space. For modality \(m_{0}\), the learning process can be expressed as follows:

\[\mathcal{L}_{KL}^{m_{1}}(\omega_{G}^{k};\omega_{k})=\min_{\omega_{G}^{k}}\sum _{l=1}^{B}\mathbb{E}_{x\sim\mathcal{D}_{k}}\left[D_{KL}\left[\left(\xi_{l} \left(x_{l}^{m_{1}};\omega_{k}^{m_{1}}\right)\left\|\mathcal{Z}_{i}^{m_{0}} \right)\right\|\right]\right], \tag{7}\]

where \(B\) represents the number of samples in the local training batch. For modality \(m_{1}\), we only learn from the missing data of this modality, which is formally expressed as follows:

\[\mathcal{L}_{KL}^{m_{1}}(\omega_{G}^{k};\omega_{k})=\min_{\omega_{G}^{k}}\sum _{l=1}^{I}\mathbb{E}_{x\sim\mathcal{D}_{k}}\left[D_{KL}\left[\left(f_{1}\left( x_{l}^{m_{1}};\omega_{k}^{m_{1}}\right)\left\|\mathcal{Z}_{i}^{m_{1}}\right) \right\|\right]\right], \tag{8}\]

where \(I\) represents the remaining number of samples. Finally, we use \(\mathcal{Z}^{m_{1}}\) instead of the feature \(f_{1}\left(x_{k}^{m_{1}};\omega_{k}^{m_{1}}\right)\) of modality \(m_{1}\) for multimodal feature fusion (_e.g._, concatenated fusion) to achieve feature alignment for missing modality. According to the above method, the overall optimization goal of every FL node is:

\[\min_{\omega_{G}^{k};\omega_{k}}\mathcal{L}_{Train}^{k}=J(\omega_{G}^{k})+ \mathcal{L}_{KL}^{m_{1}}(\omega_{G}^{k};\omega_{k})+\mathcal{L}_{KL}^{m_{1}}( \omega_{G}^{k};\omega_{k})+\mathcal{L}_{CE}(\omega_{k}), \tag{9}\]

where \(\mathcal{L}_{CE}(\omega_{k})\) represents the cross entropy loss of model training.

Transfer Feature Space.In this context, we consider the global distribution generator, denoted as \(\hat{\psi}_{k}\), and the set of local distribution generators, represented by \(\hat{\psi}_{k}\) for each node \(k\), as the source and target domains, respectively, in a framework of domain adaptation. This particular form of adaptation is referred to as global-to-local knowledge transfer. Conversely, the local-to-global knowledge transfer takes place at the server side. During the knowledge exchange process, the node \(k\) transmits its locally generated distribution model, \(\forall_{k}\), to the server. The server then orchestrates a guided adjustment of \(\forall_{k}\) with the aim of systematic reduction in the discrepancy between the local and global knowledge domains through the mechanism of FL aggregation. The above process can be formalized as follows:

\[\hat{\psi}=\frac{1}{K}\sum_{k=1}^{K}\hat{\psi}_{k}. \tag{10}\]

### Clustering Shape Value-driven Generator Contribution Evaluation Module

Evaluate Generator Contribution.Considering the inherent heterogeneity of data across nodes and the varied modality missing scenarios that often arise on individual nodes, a naive aggregation of the local distribution models \(\forall_{k}\) for knowledge transfer might inadvertently cause a general shift in the collective knowledge domain, leading to counterproductive outcomes. To mitigate this issue, we use the SV method to quantitatively evaluate the marginal contribution of each distinct \(\forall_{k}\) to the overarching learning task. However, directly applying the SV to compute the marginal contributions of individual nodes is computationally burdensome, especially in FL scenarios involving hundreds of mobile devices. To address this challenge, we incorporate the K-means clustering algorithm to reduce the computational complexity of the SV computation. Specifically, we employ the K-means clustering algorithm to cluster the \(\mathcal{Z}\) generated by \(\forall_{k}\), resulting in multiple clusters containing \(\forall_{k}\). We then perform average aggregation on the generator parameters in the cluster to obtain \(\hat{\psi}\) as the node representative

Figure 2. Workflow overview of FedMobile.

of the cluster. In this way, we can get \(\mathcal{K}\) node representatives and use these node representatives as a set \(N=\{\hat{y}_{1},\ldots,\hat{y}_{\mathcal{K}}\}\) in SV. Consequently, the final computation of SV can be expressed as:

\[\begin{split}\phi_{i}(\mathcal{P})-\sum\limits_{S\in\mathcal{K} \setminus\hat{y}_{i}\setminus\{\hat{y}_{i}\}}\frac{|\mathcal{N}(|N|-|S|-|1)}{| \mathcal{N}|}\left[p_{i}P_{i}(S\neq\mathcal{G}_{i}^{\prime})\cup\{(\hat{y}_{i} \setminus\{\hat{y}_{i}^{\prime}\})\}-p_{i}(\mathcal{P}_{i}(S\neq\mathcal{G}_{ i}^{\prime}))\right],\end{split} \tag{12}\]

where \(\mathcal{P}\) is a performance metric function, such as accuracy, F1-score, or loss, \(\mathcal{F}\) represents the global model, and \(y_{i}^{\prime}\) is a set of randomly generated labels from the proxy dataset \(\mathcal{D}_{proxy}\). Subsequently, the normalized \(\Phi_{1}(\mathcal{F}),\Phi_{2}(\mathcal{F}),\ldots,\Phi_{\mathcal{K}}(\mathcal{ F})\) is used as the aggregation weight, therefore Eq. (10) can be rewritten as:

\[\begin{split}\hat{y}=\frac{1}{\mathcal{K}}\sum\limits_{t=1}^{ \mathcal{K}}\Phi_{i}(\mathcal{F})\hat{y}_{i}.\end{split} \tag{13}\]

### Contribution-aware Aggregation Rule

**Node Contribution.**  To generalize node contribution in a fine-grained manner, we divide the contribution of each node in each round into local and global contributions. The local contribution represents the node's performance in that round of local training, while the global contribution represents the node's impact on model aggregation. We can calculate the local contributions as follows:

\[\begin{split} p_{\text{local},k}^{t}=\mathcal{P}(\omega_{k}^{t}; \mathcal{D}_{proxy}),\end{split} \tag{14}\]

where \(\mathcal{P}\) is a performance metric function, such as accuracy, F1-score, or loss, \(\mathcal{D}_{proxy}\) represents the proxy dataset, and \(\omega_{k}\) represents the local model parameters of node \(k\). It is important to note that the proxy dataset does not compromise the privacy of the training set and can be collected by the server, as is consistent with previous work (Zhu et al., 2017; Wang et al., 2018). Furthermore, we need to traverse all nodes to calculate the above contribution. To assess how much a node's update would improve the global model, we can perform a hypothetical update by applying only node \(k\)'s update to the global model and measuring the global contribution:

\[\begin{split}\mathbf{w}_{\text{temp},k}^{t+1}=\omega^{t}+\eta \Delta\omega_{k}^{t},\end{split} \tag{15}\]

\[\begin{split}\Delta p_{\text{global},k}^{t}=\mathcal{P}(\omega_{ \text{temp},k}^{t+1};\mathcal{D}_{proxy})-\mathcal{P}(\omega^{t};\mathcal{D}_{ proxy})\\ =p_{\text{global},k}^{t+1}-p_{\text{global}}^{t}\end{split} \tag{16}\]

where \(\eta\) is the learning rate. Due to computational constraints (since evaluating each node's update individually can be costly), we can approximate this by estimating the potential improvement based on surrogate metrics. Hence, we can approximate it using the node's local loss reduction metric as follows:

\[\begin{split}\Delta E_{k}^{t}=\mathcal{L}(\omega^{t};\mathcal{D} _{proxy})-\mathcal{L}(\omega_{k}^{t};\mathcal{D}_{proxy}).\end{split} \tag{17}\]

We use \(\Delta E_{k}^{t}\) as a proxy for \(\Delta p_{\text{global},k}^{t}\). The reason we do this is that larger differences have a larger impact on the global model.

**Node Contribution-aware Aggregation.**  Upon determining the global and local contributions, we strive to incorporate them adaptively into the quality assessment process of nodes participating in model aggregation, thereby mitigating the impact of updating nodes with lower quality. To achieve this goal, we can define the aggregation weight \(\alpha_{k}^{t}\) for a node \(k\) as a function of \(p_{\text{local},k}^{t}\) and \(\Delta p_{\text{global},k}^{t}\):

\[\begin{split} a_{k}^{t}=\frac{p(p_{\text{local},k}^{t};\Delta p _{\text{global},k}^{t})}{\sum_{j=1}^{K}p(p_{\text{local},j}^{t}\,\Delta p_{ \text{global},j}^{t})},\end{split} \tag{18}\]

where \(p\) is a function that combines the two performance metrics. Here, an intuitive choice for \(p\) is to multiply the normalized performance metrics. Thus, we normalize the Local Contribution Metrics:

\[\begin{split}\tilde{p}_{\text{local},k}^{t}=\frac{p_{\text{ local},k}^{t}}{\sum_{j=1}^{K}p_{\text{local},j}^{t}},\end{split}\]

Improvements: \(\tilde{\Delta}p_{\text{global},k}^{t}=\frac{\Delta p_{\text{global},k}^{t}}{ \sum_{j=1}^{K}\Delta p_{\text{global},j}^{t}}\). Combining Eqs. (16) and (17), if we use local loss reduction, we have:

\[\begin{split} a_{k}^{t}=\frac{n_{k}\times\tilde{p}_{\text{ local},k}^{t}\times\tilde{\Delta}\mathcal{L}_{k}^{t}}{\sum_{j=1}^{K}n_{j}\times\tilde{ \Delta}\mathcal{L}_{j}^{t}}\end{split} \tag{19}\]

where \(\sum_{k=1}^{K}\alpha_{k}^{t}=1\). Therefore, we can use this weight to update the global model:

\[\begin{split}\omega^{t+1}=\omega^{t}+\eta\sum_{k=1}^{K}a_{k}^{t} \Delta\omega_{k}^{t}.\end{split} \tag{20}\]

The above process is summarized in Algo. 1 in the Appendix B.

## 5. Experiment Setup

To evaluate the performance of our FedMobile system, we conduct extensive experiments on four benchmarking datasets. All experiments are developed using Python 3.9 and PyTorch 1.12 and evaluated on a server with an NVIDIA A100 GPU.

**Datasets.**  We adopt five multimodal datasets for evaluations, _i.e._, USC-HAD (Wang et al., 2018), MHAD (Wang et al., 2018), ADM (Wang et al., 2018), C-MHAD (Wang et al., 2018), and FLASH (Wang et al., 2018) datasets. The datasets cover different modalities, objects, domains, attributes, dimensions, and number of classes, as shown in Table 7, allowing us to explore the learning effectiveness of FedMobile. To simulate an environment characterized by incomplete sensing modalities, we adopt a random selection methodology to identify a target mode from the local dataset, which will represent the state of incompleteness. We then proceed to randomly eliminate a predetermined proportion of the modal data, thereby simulating the phenomenon of missing information. Note that we distinguish between the small-scale node and large-scale node scenarios according to the scale of users (_i.e._, nodes) involved in the dataset. In the dataset used, FLASH will be evaluated in the large-scale node scenario. More details can be found in Appendix C.1.

**Models.**  When processing ADM dataset, we harness the TDNN for audio feature extraction and combine it with CNN layers for radar and depth image feature extraction. For USC, MHAD, and FLASH datasets, a 2D-CNN model is utilized to process accelerometer data, whereas a 3D-CNN architecture is employed to analyze skeleton data. Finally, when working with the CMHAD dataset, we exploit a 2D-CNN architecture to derive video features, while 3D-CNN layers are used for extracting features from inertial sensors.

**Parameters.**  For the ADM dataset, we set the learning rate at 1e-3, with a batch size of 64. Regarding the USC dataset, the learning rate is 1e-6, and the batch size is 16. For the MHAD and FLASHdatasets, the learning rate is 1e-3, with a batch size of 16. When working with the CMHAD datasets, we maintain a learning rate of 1e-4, alongside a batch size of 16. Throughout this experiment, we utilize the SGD optimizer with a momentum of 0.9 and a weight decay of 1e-4. We set the total number of nodes \(K=10\), local epoch \(E=5\), global epoch \(T=100\), and node participation rate \(q=100\%\). We set \(\mathcal{K}=5\) in the K-means algorithm. We use a multilayer perceptron as our generator (see Appendix C.3).

**Baselines.** To make a fair comparison, we employ FedProx (Krizhevsky, 2009), FedBN (LeCun et al., 2011), FedMM (LeCun et al., 2011), PromFL (Chen et al., 2011), Harmony (Zhou et al., 2014), and AutoFed (Zhou et al., 2014) as baseline methods. Among these, the first three techniques require adaptation to cope with scenarios characterized by incomplete modalities. This adaptation is achieved through the integration of interpolation techniques, namely zero padding (ZP) and random padding (RP), which are incorporated into the FedProx, FedBN, and FedMM baselines. This augmentation enables us to gauge the effectiveness of FedMobile in dealing with heterogeneous modalities. On the other hand, PmcmFL, Harmony, and AutoFed are multimodal FL solutions that naturally cater to situations involving incomplete modalities without needing further modifications to their methodologies. Thus, we can directly assess FedMobile's learning performance in similar contexts using these baseline methods. Note that all comparison results are the average of five repeated experiments to eliminate the effect of randomness.

**Metrics.** To assess the performance of our proposed method and benchmark it against the baseline approaches, we employed accuracy as the evaluation metric, a convention that has been widely utilized in prior research (Brockman et al., 2016). To quantify the computational overhead, we tracked the aggregate time consumed in uploading and downloading models for all participating nodes throughout the FL training process, as was previously done in (Zhou et al., 2014). And we computed the cumulative GPU usage across all nodes engaged in the FL training phase. For communication overhead, we perform a fair comparison by calculating the model updates that need to be transmitted for 100 rounds of global training.

### Numerical Results

**Research Questions.** In this section, we aim to answer the following research questions:

\(\bullet\) (RQ1) How effectively does FedMobile, along with its respective baseline methods, fare in handling diverse and complex scenarios characterized by incomplete modal environments?

\(\bullet\) (RQ2) How does FedMobile demonstrate computational and communicational efficiency in its running processes?

\(\bullet\) (RQ3) How does FedMobile perform in heterogeneous data scenarios, especially in dynamic modality missing scenarios?

\(\bullet\) (RQ4) How does FedMobile perform in scenarios with large-scale nodes and missing modalities?

\(\bullet\) (RQ5) What are the capabilities of FedMobile in terms of multimodal feature extraction, and how proficiently can it harness and integrate features from multiple modalities?

\(\bullet\) (RQ6) How do the individual components of the FedMobile framework contribute to its overall performance, and what specific impact do they have on its effectiveness?

**System Performance (RQ1).** To address RQ1, we perform an extensive evaluation of FedMobile, along with its comparative baseline algorithms, using four benchmark multimodal datasets. To assess performance under diverse levels of modal data loss, we introduce a set of modality missing rates designated as \(\beta=\{20\%,40\%,60\%,70\%,80\%,90\%\}\). The experimental results demonstrate that FedMobile outperforms all other baseline algorithms consistently across all these varying degrees of missing modality data, as clearly depicted in Table 1. Notably, FedMobile showcases a 1.9% improvement relative to the current state-of-the-art baseline, AutoFed, specifically in the MiMAD dataset with \(\beta=80\%\). These enhanced results stem from FedMobile's innovative strategy, which entails reconstructing modal features across nodes and tactically selecting nodes with high-quality contributions. By discovering a shared feature subspace among distinct missing modalities, FedMobile efficiently reconstructs features and simultaneously excludes nodes with inferior-quality data, thereby boosting the performance.

To further evaluate the performance of FedMobile, we tested it under a more challenging scenario involving the absence of two modalities (_i.e._, two-modal data missing). Specifically, we randomly omitted two modalities in a fixed ratio within the ADM dataset, which consists of three modal data, and maintained this missing configuration throughout the training process. The numerical results, recorded in Table 2, demonstrate that FedMobile continues to deliver excellent performance, outperforming other state-of-the-art baselines, including an average 4.3% improvement over AutoFed on the ADM dataset. Additionally, we observed that existing methods struggle with missing data across multiple modalities, as they heavily depend on sufficient modal information to reconstruct the missing data. FedMobile, on the other hand, does not require this, making it more robust in handling such scenarios. Additionally, we provide a privacy analysis in Appendix A.

**Computational & Communication Overhead (RQ2).** To address RQ2, we systematically document and analyze the communication cost, local running time, and GPU usage of all examined methods on the USC dataset with \(\beta=60\%\). Note that since AutoFed and Harmony also include hardware equipment, they are not included in the comparison. For the convenience of comparison, we record the communication overhead of 100 global training rounds and ignore factors such as the network environment. First, while the introduction of the generator does cause additional communication overhead, this overhead is acceptable. Specifically, the additional overhead caused by the generator is 1.65 MB for each training round. Furthermore, compared to baselines such as FedProx, which do not introduce much additional overhead, our method only adds an additional 9.02% communication overhead, as shown in Fig. 3. In performance-critical multimodal services, a small amount of additional communication overhead is acceptable because it improves the quality of service (_i.e._, accuracy), which is a performance-overhead trade-off. The results depicted in Figs. 5-6 and Table 3 show that the GPU utilization and local running time of FedMobile consistently remains lower than or close to that of the comparative baseline methods. This indicates that our approach does not appreciably increase local computational overhead. Given that servers typically operate as resource-rich cloud infrastructure, computations related to the server-side SV calculation do not impose any significant extra computational load.

**Data Heterogeneity Scenarios (RQ3).** To address RQ3, we eval

[MISSING_PAGE_FAIL:7]

Specifically, we set modality missing rates at 40%, 60%, and 80%, and conduct five repeated experiments to record the average model accuracy. The results, presented in Table 5, show that FedMobile consistently outperforms the other advanced baselines even on large-scale nodes, demonstrating that its performance is not limited by the scale of the mobile node.

**Feature Visualization (RQ5).** In response to RQ5, we undertake a qualitative evaluation of the multimodal features generated by the competing methods by visualizing them. For this purpose, we employ the t-distributed Stochastic Neighbor Embedding (t-SNE) technique on dataset MHAD to project the high-dimensional multimodal features extracted by each method onto a lower-dimensional space. The resulting dimensionality reduction is presented in Fig. 4. Our visualization results indicate that FedMobile excels at extracting more precise and refined multimodal features, which in turn leads to enhanced classification accuracy. In comparison, alternative methods exhibit substantial deficiencies in feature extraction. This observation underscores the value of FedMobile's dual strategies of local-to-global knowledge transfer and modal feature reconstruction, which collectively facilitate the effective exploitation and extraction of information from incomplete modal sources.

**Ablation Studies (RQ6).** To investigate RQ6, we conduct a systematic dissection of FedMobile by analyzing the performance contributions of its constituent parts with \(\beta=40\%\). To this end, we experimentally validate the performance of three ablated versions of FedMobile on four benchmark multi-modal datasets. Specifically, we successively deactivate the modal reconstruction network, the contribution sensing module, and the dynamic parameter aggregation module, forming three distinct variations of FedMobile. The experimental outcomes are summarized in Table 6, demonstrating that the modal reconstruction network and the contribution sensing module play pivotal roles in determining FedMobile's performance. On the other hand, the impact of the dynamic parameter aggregation module on FedMobile's performance appears to be less pronounced. For illustration, when the modal reconstruction network is removed from FedMobile, the performance degradation on the MHAD dataset reaches 3.2%, relative to the complete version of FedMobile. These findings highlight the critical importance of the modal reconstruction and contribution sensing mechanisms within the FedMobile framework.

## 6. Conclusion

The paper addresses the challenge of incomplete modalities in multimodal Federated Learning systems by proposing a new framework called FedMobile. Unlike existing methods that rely on unimodal subsystems or interpolation, FedMobile leverages cross-node multimodal feature information for reconstructing missing data and employs a knowledge contribution-aware mechanism to evaluate and prioritize node inputs, improving resilience to modality heterogeneity. The framework demonstrates superior performance in maintaining robust learning under significant modality loss compared to current standards, all while not increasing computational or communication costs. Overall, FedMobile represents a significant step forward in developing more efficient and resilient multimodal FL systems.

\begin{table}
\begin{tabular}{c|c|c|c|c|c} \hline \hline
**Method** & **MHAD** & **USC** & **ADM** & **CMEAD** & **FLASH** \\ \hline Ours (w/o \(Z_{\text{T}}\)/tab) & 74.7 & 57.5 & 79.8 & 79.2 & 52.7 \\ Ours (w/o SV) & 77.3 & 61.6 & 81.8 & 82.7 & 56.9 \\ Ours (w/o ) & 74.5 & 58.6 & 84.1 & 77.4 & 53.8 \\ \hline
**Ours** & **77.9** & **62.8** & **84.3** & **75.8** & **57.6** \\ \hline \hline \end{tabular}
\end{table}
Table 6. Numerical results of ablation experiments.

\begin{table}
\begin{tabular}{c|c|c c} \hline \hline
**Dataset** & **Method** & **Scenario 1** & **Scenario 2** \\ \hline \multirow{8}{*}{**ADM**} & FedMM+ZP & 69.3 & 71.4 \\  & FedMM+RP & 69.5 & 72.1 \\  & FedProx+ZP & 70.2 & 73.7 \\  & FedProx+RP & 69.5 & 73.2 \\
**ADM** & FedBN+ZP & 69.8 & 71.3 \\  & FedBN+RP & 70.3 & 71.7 \\  & PmcmFL & 71.5 & 74.5 \\  & Harmony & 70.7 & 76.8 \\  & AutoFed & 70.1 & 77.9 \\  & **Ours** & **76.5** & **80.2** \\ \hline \hline \end{tabular}
\end{table}
Table 4. Performance results in the heterogeneity scenario.

Figure 4. Feature visualization results of different methods.

\begin{table}
\begin{tabular}{c|c|c|c|c} \hline \hline
**Dataset** & **Method** & **40\%** & **60\%** & **80\%** \\ \hline \multirow{8}{*}{**FLASH**} & FedMM+ZP & 52.7 & 51.1 & 50.4 \\  & FedMM+RP & 53.4 & 52.3 & 51.2 \\  & FedProx+ZP & 49.7 & 50.1 & 49.2 \\  & FedProx+RP & 49.4 & 48.7 & 49.4 \\
**FLASH** & FedBN+ZP & 49.9 & 49.1 & 47.8 \\  & FedBN+ZP & 50.4 & 49.7 & 48.2 \\  & PmcmFL & 52.4 & 51.6 & 50.4 \\  & Harmony & 55.8 & 54.7 & 54.1 \\  & AutoFed & 54.9 & 53.4 & 55.4 \\  & **Ours** & **57.6** & **57.1** & **56.8** \\ \hline \hline \end{tabular}
\end{table}
Table 5. Performance results on FLASH Dataset.

## References

* (1)
* Bao et al. (2023) Guangyin Bao, Qi Zhang, Duoqian Miao, Zixuan Gong, and Liang Hu. 2023. Multimodal Federated Learning with Mining Modality via Prototype Mask and Contract. _arXiv preprint arXiv:2312.13298_ (2023).
* Chen et al. (2023) Hugh Chen, Ian C. Covert, Scott M Lundberg, and Su-In Lee. 2023. Algorithms to estimate Shapley value feature attributions. _Nature Machine Intelligence_ 5, 6 (2023): 509-601.
* Chen and Ha (2022) Jiawei Chen and Chiu Man Ha. 2022. MM-VIT: Multi-modal video transformer for compressed video action recognition. In _Proc. of CVPR_.
* Cho et al. (2022) Hyunus Cho, Ali Makhtar, and Fahim Kawar. 2022. Flame: Federated learning across multi-device environments. In _Proc of VICPR_.
* Delgado-Santos et al. (2022) Paula Delgado-Santos, Guangyeeng Stragoel, Roblen Tolosanu, Richard Guest, Farin Dervat, and Raburu Verz-Rodriguez. 2022. A survey of privacy vulnerabilities of mobile device sensors. _ACM Computing Surveys (CSU)_ 54, 118 (2022), 1-30.
* Feng et al. (2020) Jie Feng, Can Rong, Funing Sun, Diamsheng Guo, and Yong Li. 2020. PMF: A privacy-preserving human mobility prediction framework via federated learning. In _Proc. of UICOm_.
* Feng et al. (2022) Tiantian Feng, Dleghjay Bose, Tuo Zhang, Rajat Hebbar, Anil Ramakrishna, Rahul Gupta, M Zhang, Salimon Avestimehr, and Shrikanth Narayanan. 2022. Featuredial and benchmark for multimodal federated learning. In _Proc. of KDD_.
* Gong et al. (2022) Chen Gong, Zhenzhe Zhang, Fan Wu, Yunfeng Shao, Bingshu Li, and Guish Chen. 2022. To store or not? online data selection for federated learning with limited storage. In _Proc. of WWW_.
* Guo et al. (2020) Yeung Guo, Fang Liu, Zhizhong Cai, Li Chen, and Nong Xiao. 2020. FEEL: A federated edge learning system for efficient and privacy-preserving mobile healthcare. In _Proc. of KDD_.
* Hard et al. (2018) Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Francoise Beaufays, Sean Augenstein, Heubert Eichner, Chike Kiddon, and Daniel Ramaswamy. 2018. Federated learning for mobile keyboard prediction. _arXiv preprint arXiv:1811.03604_ (2018).
* John and Kawanishi (2023) Vijay John and Yasutomo Kawanishi. 2023. Multimodal Cascaded Framework with Metric Learning Robust to Missing Modalities for Person Classification. In _Proc. of NIPS_.
* Kairova et al. (2021) Peter Kairova, H Brendan McMahan, Brendan Avent, Aurelien Bellet, Mehdi Bennis, Arjun Nain Bhagoly, Kailan Bonawitz, Zachary Charles, Graham Corne, Jacob Cummings, et al. 2021. Advances and open problems in federated learning. _Foundations and trends in machine learning_ 14, 1-2 (2021), 1-210.
* Kamumou et al. (2023) Nathan Kamumou, Lashif Megapabba, and Daniel Gulica-Pierre. 2023. Un-attending the Social Context of Rating with Madgraph: Syntactic Sensing: The Role of Country Diversity. In _Proc. of ICML_.
* Kubi et al. (2023) Yuki Kubi, Soto Sato, Anna Tomi Taniguchi, Koose Miyazaki, Akira Tsujimoto, Hiraki Yasuda, Takayuki Sakamoto, Takasaki Ishikawa, Kota Tsubcuichi, and Masanichi Shimouchi. 2023. CitySocoter: Exploiting the Atmosphere of Urban Landscapes and Vision Detands with Multimodal Data. In _Proc. of UICOm_.
* De et al. (2022) Hiroy de De, Cas Muyat Thrust, Yuqo de, Lin Yin Lu, Minh Ni Nguyen, and Chong Song Sun. 2022. Cross-Modal Prototype based Multimodal Federated Learning under Severely Missing Modality. _arXiv preprint arXiv:2001.13898_ (2022).
* Lin et al. (2020) Tim Lin, Anit Kumar Sahu, Manuf Zaheer, Matiar Sanjabi, Annet Talwalkar, and Virginia Smith. 2020. Federated optimization in heterogeneous networks. _Proc. of MLSys_ (2020).
* Li et al. (2020) Xiaoxiao Li, Metiri JIMNG, Xiaofei Zhang, Michael Kamp, and Qi Dou. 2020. FedShift: Federated Learning on Non-TD Features via Local Batch Normalization. In _Proc. of ICLR_.
* Yang et al. (2020) Wei Yang Hyan Lin, Nguyen Gao Liang, Dhun Ni, Thai Hong, Yutao Jiao, Ying-Hsiang Liang, Qiang Yang, Dushi Ngupta, and Chunyuan Miao. 2020. Federated Learning in mobile edge networks: A comprehensive survey. _IEEE Communications Surveys & Tutorials_ 22, 3 (2020), 2031-2063.
* Liu et al. (2020) Zihao Liu, Songfan Yang, Jiliang Tang, Neil Refferman, and Rose Lachin. 2020. Recent advances in multimodal educational data mining in k-12 education. In _Proc. of KDD_.
* Lan et al. (2018) Pan Lan, Lei Ji, Wei Zhang, Nuan Duan, Ming Zhou, and Jianyong Wang. 2018. R-VQA: Learning visual relation facts with semantic attention for visual question answering. In _Proc. of KDD_.
* McMahan et al. (2017) Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Shike Auger. 2017. Communication-efficient learning of deep networks from decentralized data. In _Proc. of AISATS_.
* Mouch et al. (2015) Yousef Mouch, Heime Marcher, and Vishnuza Goel. 2015. Deep multimodal learning for audio-visual speech recognition. In _Proc. of ICASSP_.
* Oki et al. (2018) Jerda Oki, Rixuan Chaudhry, Greggir Kaville, Rene Vidal, and Runzen Bajcsy. 2018. Berkeley mixed: A comprehensive multimodal human action database. In _Proc. of WACV_.
* Oh et al. (2022) Sentgen Oh, Jihong Park, Praneeth Vepakomma, Shinn Baek, Ramesh Raskar, Mehdi Bennis, and Seong-Iyun Kim. 2022. Lodefactmix-xl: Localize, federate, and mix for improved scalability, convergence, and latency in split learning. In _Proc. of WWW_.
* Oh et al. (2023) Se Won Oh, Hyunta Jeong, Seungu Chung, Jeong Mock Lin, and Kyoung Ju. 2023. Multimodal sensor data fusion and ensemble modeling for human locomotion activity recognition. In _Proc. of UIComp_.
* Owang et al. (2023) Xiaoming Owang, Ziyuan Xue, Fleming Fu, Yiu Siong Cheng, Li Pan, Neiwen Ling, Guohang Xing, Jiyu Zhou, and Jianwei Huang. 2023. Harmony: Heterogeneous multi-modal federated learning through disentangled model training. In _Proc. of MobiSys_.
* Park et al. (2023) JaeYouen Park, Kitchang Lee, Sungmin Lee, Mi Zhang, and JeongGil Ko. 2023. Artifi: A personalized federated learning framework for time-series mobile and embedded sensor data processing. In _Proc. of UICOm_.
* Putus et al. (2012) Antonio Putus, Davide Carboni, and Andrea Piras. 2012. Paraimpu: a platform for a social web of things. In _Proc. of WWW_.
* Radu et al. (2016) Valentin Radu, Nicholas D Lane, Sourey Bhattacharya, Cecilia Mascolo, Mahesh K Marina, and Fain Kamwa. 2016. Towards multimodal deep learning for activity recognition on mobile devices. In _Proc. of UHComp_.
* Salehi et al. (2022) Batol Salehi, Jerry Oke, Dekanti Roy, and Kanishk Chowdhury. 2022. Flash: Federated learning for automated detection of high-and mmwave accessors. In _Proc. of INFOCOM_.
* Shi et al. (2021) Weijie Shi, Jiajie Xu, Junhua Fang, Pingfu Chao, An Liu, and Xiaofeng Zhou. 2023. LHMt: A Learning Enhanced HMM Model for Cellular Trajectory Map Matching. In _Proc. of ICC_.
* Sung et al. (2023) Ding Sun, Xing Li, Jiaya Zhang, Li Xiong, Weiran Lin, Jingfei Li, Zhan Qin, and Kai Ren. 2023. Slapfeyl: Robust federated learning based on shapley value. In _Proc. of KDD_.
* Wang et al. (2023) Fei Wang, Ethan Hugh, and Baochun Li. 2023. More than Enough in Too Much: Adaptive Defenses against Gradient Leakage in Production Federated Learning. In _Proc. of INFOCOM_.
* Wang et al. (2008) Huoxhao Wang, Jiao Liu, Meng Zhang, Qinghao Hu, Hao Ren, Peng Sun, Yonggang Wen, and Tanwei Zhang. 2024. RoIHS: Distribution-aware Sub-model Extraction for Federated Learning over Resource-constrained Devices. In _Proc. of WWW_.
* Wang et al. (2022) Jiwong Wang, Handong Zhao, Yaqing Wang, Tong Yu, Jiuxiang Gu, and Jing Gao. 2022. FedEx: Federated knowledge composition for multilingual natural language understanding. In _Proc. of WWW_.
* Wang et al. (2023) Kaibun Wang, Qiang He, Feifei Chen, Chunyang Chen, Faliang Huang, Hai Jin, and Yun Tang. 2023. Federated Personalized federated learning for edge clients with heterogeneous model architectures. In _Proc. of WWW_.
* Wen et al. (2020) Haozin Wei, Pranavne Clopola, and Nasser Kekhtarawara. 2020. C-AHLAD: Continuous multimodal human action dataset of simultaneous video and inertial sensing. _Sensors_ 20, 10 (2020), 2905.
* Williams et al. (2007) John Williams, Rodrick Murray-Smith, and Stephen Hughes. 2007. Shoogle: excitatory multimodal interaction on mobile devices. In _Proc. of CHI_.
* Xu et al. (2021) Lianghu Xu, Chao Huang, Yuxu Peng, Yuqin Peng, Jianxin Lu, and Liefeng Bo. 2021. Multi-behavior enhanced recommendation with cross-interaction collaborative relation modeling. In _Proc. of ICDE_.
* Song et al. (2022) Raochen Song, Xiaohuan Tang, Jiegun Song, Yaowei Wang, and Changchang Xu. 2022. Client-Adaptive Cross-Model Reconstruction Network for Modality-Incomplete Multimedia Federated Learning. In _Proc. of ACM MM_.
* Xue et al. (2021) Yiuue Xue, Chao Wang, Jianheu Zheng, Shaojie Tang, Chengji Lyu, Fan Wu, and Guish Chen. 2021. Toward understanding the influence of individual clients in federated learning. In _Proc. of AAAI_.
* Yang et al. (2012) Chengqi Yang, Qiueng Wang, Mengwei Xu, Zhenpeng Chen, Kaiqi Han, Yunxin Liu, and Xunhao Liu. 2012. Characterizing impacts of heterogeneity in federated learning upon large-scale smartphone data. In _Proc. of WWW_.
* Yang et al. (2022) Xue Yang, Yan Feng, Weiqin Fang, Jun Shao, Xiaohu Tang, Shu-Tao Xia, and Rongpian. 2022. An accuracy-losses perturbation method for defending privacy attacks in federated learning. In _Proc. of WWW_.
* Yang et al. (2022) Xiaohuan Yang, Baochen Xiong, Yiuang Huang, and Changchang Xu. 2022. Cross-Modal Federated Human Activity Recognition. _IEEE Transactions on Pattern Analysis and Machine Intelligence_ (2022).
* Yuan et al. (2001) Jinliang Yuan, Shangzang Wang, Hongyu Li, Diliang Xu, Yanchun Liu, Mengwei Xu, and Xuanhe Liu. 2022. Towards Energy-efficient Federated Learning via INTS-based Training on Mobile DDs. In _Proc. of WWW_.
* Zhang et al. (2020) Chunxiang Zhang, Meng Jiang, Xing Zhang, Yang Yang, Ye, and Niresh V Chawla. 2020. Multi-modal network representation learning. In _Proc. of KDD_.
* Zhang and Savchuk (2015) Mi Zhang and Alexander A Savchuk. 2015. USC-HLP: A daily activity dataset for ubiquitous activity recognition using wearable sensors. In _Proc. of UbiComp_.
* Zheng et al. (2023) Tianyue Zheng, Ang Li, Zhe Chen, Hongbo Wang, and Jun Luo. 2023. Attodet: Heterogeneity-aware federated multimodal learning for robust autonomous driving. In _Proc. of MobiCom_.
* Zhu et al. (2019) Liegun Zhu, Zhiqian Liu, and Song Han. 2019. Deep leakage from gradients. In _Proc. of NeurIPS_.

## Appendix A Privacy Analysis

The proposed method for imputing missing modalities in FedMobile introduces privacy-preserving strategies by utilizing latent space reconstruction and conditional distributions across nodes. This privacy analysis explores how these strategies help mitigate privacy risks associated with multimodal FL under scenarios where data modalities are incomplete across nodes.

**Privacy Issues in Multimodal FL.** On the one hand, in traditional FL, model updates can inadvertently leak sensitive information about the local datasets, especially when gradients are shared directly (Krishna et al., 2017). Furthermore, when reconstructing missing modalities from available data, there is a risk that sensitive or private information about the original data can be exposed (Bahdanau et al., 2015). High-dimensional data spaces are particularly vulnerable to this risk.

**FedMobile's Privacy-Preserving Mechanisms.** FedMobile addresses these privacy risks through two key techniques:

\(\bullet\) Conditional Distribution in Latent Space. Instead of directly imputing missing modalities in the raw input space \(\mathcal{X}_{k}\), FedMobile reconstructs an induced distribution \(\psi_{k}\) over a latent space \(\mathcal{Z}_{k}\), as shown in Eq. (5). The latent space is more compact and lower-dimensional than the raw data space. This shift to a latent space significantly reduces the risk of privacy leakage because the latent representations contain abstracted information rather than raw, potentially sensitive features of the original data. Additionally, latent spaces typically obscure fine-grained details about individual data points, making it harder to reverse-engineer or infer private information from the shared model updates.

\(\bullet\) Conditional Distributions for Imputation. FedMobile uses conditional distributions \(Q_{k}\) (in Eq. (4)) and \(\psi_{k}\) (in Eq. (5)) to model the relationships between the missing and present modalities. This distribution-based approach means that only the learned relationships between modalities are shared, not the actual data or detailed feature information. By focusing on conditional probabilities \(P(y\mid x)\) or \(p(y\mid z)\), the model implicitly encodes privacy since no raw features or labels are directly shared between nodes or with the central server. Instead, only probabilistic inferences are utilized, reducing the risk of reconstructing sensitive raw data.

Furthermore, existing work (Shen et al., 2017) has shown that it is difficult to recover training data by only obtaining gradient or feature information, as gradient leakage attacks are less effective on large training batches (_e.g._, batch size = 32). Additionally, gradient leakage or feature reconstruction attacks are typically effective for image data (Shen et al., 2017; Kushner et al., 2017), but their effectiveness is limited for data types such as radar and gyroscope data.

## Appendix B Proposed Algorithm

An overview of the algorithm is as follows:

### Additional Information

#### Dataset Information

**MHAD Dataset.** The MHAD dataset is designed to support research in human action recognition using multiple modalities. It includes data from 12 subjects performing 11 actions such as jumping, walking, running, and more. The dataset captures data from multiple sensors including accelerometers, gyroscopes, and magnetometers, as well as from optical motion capture systems and video cameras. Link: [https://paperswithcode.com/dataset/berkeley-mhad](https://paperswithcode.com/dataset/berkeley-mhad)

**USC-HAD Dataset.** The USC-HAD dataset is a collection of data gathered for the purpose of recognizing human activities. The dataset includes data from 14 subjects performing 12 different activities such as walking, running, jumping, sitting, standing, and more. The data is captured using wearable sensors that record accelerometer and gyroscope readings. Link: [https://gipi.usc.edu/had/](https://gipi.usc.edu/had/)

**ADM Dataset.** The ADM dataset focuses on detecting Alzheimer's disease by analyzing 11 behavioral biomarkers in natural home environments. These biomarkers include activities such as cleaning living areas, taking medications, using mobile phones, writing, sitting, standing, getting in and out of chairs/beds, walking, sleeping, eating, and drinking. The three modal data of depth images, radar, and audio are obtained by sampling from the depth camera, mmWave radar, and microphone at sampling rates of 15 Hz, 20 Hz, 44 Hz, and 100 Hz, respectively. Link: [https://github.com/xmouyang/Harmony/blob/main/dataset.md](https://github.com/xmouyang/Harmony/blob/main/dataset.md)

**C-MHAD Dataset.** The C-MHAD dataset extends the concept of the MHAD dataset by providing continuous recordings of human activities. Unlike datasets that capture discrete instances of actions, C-MHAD includes long, continuous streams of activity data, simulating real-world scenarios where actions flow into one another without clear boundaries. This dataset is particularly useful for developing and testing algorithms that need to operate in real-time and handle continuous input, such as those used in surveillance, human-computer interaction, and assistive technologies. Link: [https://github.com/HaoranWeiUTD/C-MHAD-PytorchSolution](https://github.com/HaoranWeiUTD/C-MHAD-PytorchSolution)

**FLASH Dataset.** The FLASH dataset is a multimodal dataset designed specifically for multimodal FL in traffic scenarios. It includes 32,923 samples from three modalities, collected in real time from autonomous vehicles equipped with various sensors--GPS, LiDAR, cameras--and roof-mounted Talon AD7200 60GHz millimeter-wave radios. The dataset primarily supports research in autonomous driving and high-band millimeter-wave sector prediction, among other related fields. [https://repository.library.northeastern.edu/files/neu:k930bx06g](https://repository.library.northeastern.edu/files/neu:k930bx06g)

### Data Partitioning

**IID Setting.** For the IID data setting, we assign the multimodal dataset collected by each mobile sensor to each node. In addition,

\begin{table}
\begin{tabular}{c c c c c c c c} \hline
**Dataset** & **Modality Information** & **\# Classes** & **\# Users** & **Object** & **Domain** & **\# Samples** & **Size (MB)** \\ \hline USC-HAD & Acc, Gyro & 12 & 10 & People & Activity Detection & 38312 & 38.5 & 1228 \\ MHAD & Acc, Skeleton & 11 & 10 & People & Activity Detection & 3956 & 187 & 1226 \\ ADM & Audio, Radar, Depth Image & 11 & 10 & People & Medical & 22452 & 30208 & 1228 \\ C-MHAD & Video, Inertial Sensor & 7 & 10 & People & Activity Detection & 7802 & 24268.8 & 1228 \\ FLASH & GPS, LiDAR, Camera & 64 & 210 & Traffic Scenes & Autopilot & 32923 & 5232.64 & 1227 \\ \hline \end{tabular}
\end{table}
Table 7. Summary of the four multimodal datasets.

Figure 5. Numerical result of local GPU usage.

Figure 6. Numerical result of local running time.

during training, we keep the type and missing ratio of each node's missing modality consistent to construct an IID data scenario.

**Non-IID Setting.** We define two non-IID data scenarios: (1) scenarios with different distributions of the number of missing modality types and (2) scenarios with varying distributions of modality missing rates, as illustrated in Figs. 7-8. In Scenario 1, we set the modality missing rate at 40% and randomly omit different numbers of modality types. For Scenario 2, we control the number of missing modality types but dynamically adjust the modality missing rate, ranging from a maximum of 80% to a minimum of 20%.

### Generator Information

The generator used in this paper is a multi-layer perceptron (MLP), which performs updates and optimizations in conjunction with local training (see Eqs. (4)-(9)). Specifically, the generator architecture comprises two fully connected layers, a batch normalization (BN) layer, and an activation layer. Initially, the first fully connected layer maps the data labels into feature embeddings. This is followed by the BN layer and a non-linear activation layer. Finally, the second fully connected layer serves as the representation layer, converting the feature embeddings into a format suitable for model training. While more complex generators can be used, the MLP is a good choice to minimize overhead.

Figure 8. Distribution of missing rate at different nodes.

Figure 7. Distribution of missing modality at different nodes.