# User Distribution Mapping Modelling with Collaborative Filtering for Cross Domain Recommendation

Anonymous Author(s)

###### Abstract.

User cold-start recommendation aims to provide accurate items for the newly joint users and is a hot and challenging problem. Nowadays as people participant in different domains, how to recommend items in the new domain for users in an old domain has become more urgent. In this paper, we focus on the _Dual Cold-Start Cross Domain Recommendation_ (_Dual-CSCDR_) problem. That is, providing the most relevant items for new users on the source and target domains. The prime task in Dual-CSCDR is to properly model user-item rating interactions and map user expressive embeddings across domains. However, previous approaches cannot solve Dual-CSCDR well, since they separate the collaborative filtering and distribution mapping process, leading to the error superimposition issue. Moreover, most of these methods fail to fully exploit the cross-domain relationship among large number of non-overlapped users, which strongly limits their performance. To fill this gap, we propose User Distribution Mapping model with Collaborative Filtering (**UDMCF**), a novel end-to-end cold-start cross-domain recommendation framework for the Dual-CSCDR problem. **UDMCF** includes two main modules, i.e., _rating prediction module_ and _distribution alignment module_. The former module adopts one-hot ID vectors and multi-hot historical ratings for collaborative filtering via a contrastive loss. The latter module contains overlapped user embedding alignment and general user subgroup distribution alignment. Specifically, we innovatively propose unbalance distribution optimal transport with typical subgroup discovering algorithm to map the whole user distributions. Our empirical study on several datasets demonstrates that **UDMCF** significantly outperforms the state-of-the-art models under the Dual-CSCDR setting.

Recommendation, Cross Domain Recommendation, Domain Adaptation, Optimal Transport +
Footnote †: ccs: Information systems Recommender systems

+
Footnote †: ccs: Information systems Recommender systems

+
Footnote †: ccs: Information systems Recommender systems

+
Footnote †: ccs: Information systems Recommender systems

+
Footnote †: ccs: Information systems Recommender systems

+
Footnote †: ccs: Information systems Recommender systems

+
Footnote †: ccs: Information systems Recommender systems

+
Footnote †: ccs: Information systems Recommender systems

+
Footnote †: ccs: Information systems Recommender systems

+
Footnote †: ccs: Information systems Recommender systems

+
Footnote †: ccs: Information systems Recommender systems

## 1. Introduction

Recommender systems become more and more attractive with the big data explosion in recent years. With the advent of digital era, more and more users participant in multiple domains (platforms) for different purposes, e.g., watching movies on _Netflix_ and buying books on _Amazon_(Amazon, 2016; Krizhevsky et al., 2012). Meanwhile, how to provide the most relevant new items for users across domains has become a hot topic. Therefore, the Cross-Domain Recommendation (CDR) has emerged to utilise and exploit useful knowledge for achieving promising solution on the cold-start recommendation (Krizhevsky et al., 2012; Li et al., 2015; Li et al., 2016; Wang et al., 2017). CDR models can transfer sharing patterns inherited from multiple domains to enhance the model performance for better results. However, most of current CDR models always assume that all users or items are strictly overlapped which limits their potentials, especially for the cold-start CDR tasks with only few overlapped users/items.

In this paper, we concentrate on the _Dual Cold-Start Cross Domain Recommendation_ (Dual-CSCDR) problem, that is, providing the most relevant items in a domain for new users in the other domain in a bi-directional way (e.g., cold-start source users without historical interactions in the target domain) without other auxiliary representations. The Dual-CSCDR problem popularly exists in practice with two main tasks, i.e., (1) recommending source items to target users and (2) recommending target items to source users which have been shown in Fig.1. The prime task of Dual-CSCDR problem is bridging and mapping users' preferences across domains for reducing the domain bias and discrepancy.

Although there have been previous studies on the CSCDR problem, these models cannot solve Dual-CSCDR well. On one hand, current CSCDR models always separate the procedure of collaborative filtering and transferable bridge mapping. As a result, the knowledge across domains will not be fused together during the collaborative filtering step, which leads to the error superimposition problem and increase the final recommendation error (Li et al., 2016). On the other hand, most of the CSCDR models cannot fully explore

Figure 1. The problem of Dual Cold-Start Cross Domain Recommendation (Dual-CSCDR).

the useful representations behind the non-overlapped users. As a consequence, a large amount of information will be neglected, which leads to model degradation, especially when the number of non-overlapped users are much more than the overlapped users. Furthermore, they fail to fully exploit and align the embedding structure and probability distribution for the whole user feature space. Therefore, current CSCDR models cannot solve these challenges and lead to limited model performance.

To address the aforementioned issues, in this paper, we propose User Distribution Mapping with Collaborative Filtering (**UDMCF**) model, a recommendation framework for the Dual-CSCDR problem. We propose two modules in **UDMCF**, i.e., _rating prediction module_ and _distribution alignment module_ for better modelling user/item embeddings and transferring relevant information across domains. In the rating prediction module, we utilize the one-hot ID vector to generate user and item preference distribution via collaborative filtering on graph neural network. The distribution alignment module tends to reduce the domain discrepancy between the source and target domains. We first propose distribution alignment for both overlapped and non-overlapped users. Specifically, we innovatively propose latent subgroup distribution alignment with user subgroup distribution measurement and alignment. As a result, our proposed **UDMCF** can be trained end-to-end through modelling user-item preferences and mapping users with similar characteristics.

We summarize our main contributions as follows: (1) We propose a novel end-to-end training framework, i.e., **UDMCF**, for the Dual-CSCDR problem, which contains rating prediction module and distribution alignment module. (2) To our best knowledge, this is the first attempt in literature to align both the overlapped and non-overlapped users embedding distribution across domains based on proposed latent distribution alignment via the unbalanced distribution optimal transport. (3) Extensive empirical studies on four datasets demonstrate that **UDMCF** significantly improves the state-of-the-art models, especially under the Dual-CSCDR setting.

## 2. Related Work

**Traditional Cross Domain Recommendation.** Traditional Cross Domain Recommendation (CDR) models aim to provide a promising solution for tackling the data sparsity in the target domain. These methods leverage the relative rich auxiliary information to enhance the performance in the sparser domain. Existing CDR works on this are mainly of two types, i.e., _rating-based_ methods and _supplement-based_ methods (Golovin et al., 2013; He et al., 2016; He et al., 2017; He et al., 2018; Zhang et al., 2019). Rating-based methods only utilize user-item interactions for collaborative filtering. KerKT (Kirch et al., 2017) first introduced the kernel induction method with a shadow model for aligning the overlapped users and items. CoNet (Golovin et al., 2013) adopted the cross-connection unit to snitch useful knowledge across domains. DARec (Yang et al., 2017) applied adversarial training strategy for extracting preference patterns for overlapped users. Supplement-based methods further exploit the auxiliary user/item information, e.g., tags, categories, and reviews etc. TDAR (Yang et al., 2018) integrated the user-item textual features (e.g., reviews) to transfer useful knowledge. More recently, CFAA (Yang et al., 2018) further proposed vertical and horizontal attribution alignment between the latent user/item embeddings across domains. However, they cannot better recommendations for cold-start users who do not have rating interactions in the corresponding domain.

**Cold-Start Cross Domain Recommendation.** Cold-Start Cross Domain Recommendation (CSCDR) is set to alleviate the long-standing cold-start problem in recommendation (He et al., 2017; He et al., 2017). Existing CSCDR works on this are mainly of two types, i.e., _mapping-based_ methods and _meta-based_ methods (He et al., 2016; He et al., 2017). Mapping-based methods learn to align and transform between the source and target users/items. EMCDR (He et al., 2017) is the most popular method which first adopted the matrix factorization to learn embeddings then utilize a network to bridge users or items from one domain to the other. SSCDR (He et al., 2017) further extends the EMCDR by mapping both users and items with deep metric learning in the semi-supervised manner. Recently, DOML (He et al., 2018) implemented a novel latent orthogonal mapping to extract user preferences over different domains. Meta-based methods integrate the meta learning with the meta network by training on similar tasks. TMCDR (Zhu et al., 2018) took the advantage of the popular Model-Agnostic Meta-Learning (MAML) (He et al., 2016) to optimize a meta network on rating or ranking. PTUPCDR (Zhu et al., 2018) further improved TMCDR with users' characteristic and preference via personalized bridge for modelling. However, the all methods above separated the pre-training part and the mapping part and they cannot be trained end-to-end. Moreover, most of the current approaches cannot better exploit the information among the non-overlapped user-item interactions which also leads to the model degradation.

**Domain Adaptation.** Domain adaptation aims to transfer useful knowledge from the source samples with labels to target samples without labels for enhancing the target performance. Eric Tzeng et al. (Eric Tzeng et al., 2019) first implemented maximum mean discrepancy (Chen et al., 2019) to measure and reduce the domain bias. Baochen Sun et al. (Baochen Sun et al., 2019; Wang et al., 2020) further adopted correlation matching via covariance matrix. More recently,

Figure 2. The main framework for the proposed **UDMCF**.

ESAM (Chen et al., 2017) extended correlation matching with attribution correlation congruence for solving the long-tailed item recommendation problem. Meanwhile, Ganin et al. (2018) proposed Domain Adversarial Neural Network (DANN) which utilized a domain discriminator with adversarial training to align the embeddings across domains. Feng Yuan et al. (2019) and Wenhui Yu et al. (2018) adopted adversarial learning for the cross domain recommendation. Nowadays, more researchers have utilized optimal transport (Zhu et al., 2019; Wang et al., 2019), which have the ability of encoding class-structure in distributions for minimizing the global transportation cost. Yitong Meng et al. (2019) is the first attempt to apply Wasserstein distance optimal transport for item cold-start recommendation. In this paper, we propose latent distribution alignment for both overlapped and non-overlapped users via unbalance distribution optimal transport to reduce the domain discrepancy.

## 3. Modeling for UDMCF

First, we describe notations. We assume there are two domains, i.e., a source domain \(\mathcal{S}\) and a target domain \(\mathcal{T}\). There are \(N_{U_{S}}\) and \(N_{U_{T}}\) items in source and target domains respectively. There are \(R^{\mathcal{S}}\in\mathbb{R}^{N_{U_{S}}\times N_{V_{S}}}\) and \(R^{\mathcal{T}}\in\mathbb{R}^{N_{U_{T}}\times N_{U_{T}}}\) be the observed source and item rating matrices in \(\mathcal{S}\) and \(\mathcal{T}\) respectively. To simplify the problem, in this paper, we assume both domains have no other auxiliary information. Meanwhile there are some overlapped users across different domains under the CSCDR settings. We use the overlapped user ratio \(\mathcal{K}_{a}\) to measure how many users are concurrence according to previous researchers (Zhu et al., 2019; Wang et al., 2019). Our purpose for the Dual-CSCDR problem can be included into two main tasks, i.e., (1) **Task1:** recommending source items to target users. (2) **Task2:** recommending target items to source users.

We introduce the overview of our proposed **UDMCF** framework, as is illustrated in Fig. 2. **UDMCF** mainly has two modules, i.e., _rating prediction module_ and _distribution alignment module_. The rating prediction module aims to learn user/item distributions with observed user-item interactions via graph neural networks. The distribution alignment module is supposed to reduce the domain discrepancy between the source and target users on both overlapped and non-overlapped. We will introduce these two modules later.

### Rating Prediction Module

Firstly, we provide the details of the rating prediction module. For convenience, we use the notations and calculation process in the source domain as an example. For the \(i\)-th user and the \(j\)-th item, we define their corresponding one-hot ID vectors as \(X_{i}^{\mathcal{U}_{S}}\) and \(X_{j}^{\mathcal{V}_{S}}\), respectively. We adopt a trainable lookup table to exploit the user and item one-hot ID embedding as LookUp(\(X_{i}^{\mathcal{U}_{I}}\)) = \(E_{i}^{\mathcal{U}_{S}}\) and LookUp(\(X_{j}^{\mathcal{V}_{S}}\)) = \(E_{j}^{\mathcal{V}_{S}}\). Then we adopt the commonly-used graph neural network to aggregate useful information among the user-item interactions. We first regard the users and items as the nodes in each domain and construct the corresponding graph \(\mathbf{A}^{\mathcal{S}}\) and \(\mathbf{A}^{\mathcal{T}}\) based on the rating matrix \(\mathbf{R}^{\mathcal{S}}\) and \(\mathbf{R}^{\mathcal{T}}\) as \(\mathbf{A}^{\mathcal{S}}=\begin{bmatrix}\mathbf{0}&\mathbf{R}^{\mathcal{S}}\\ (\mathbf{R}^{\mathcal{S}})^{\top}&\mathbf{0}\end{bmatrix},\mathbf{A}^{\mathcal{T}}= \begin{bmatrix}\mathbf{0}&\mathbf{R}^{\mathcal{T}}\\ (\mathbf{R}^{\mathcal{T}})^{\top}&\mathbf{0}\end{bmatrix}\). After that we conduct the graph convolution network on both source and target domains. Graph convolution network can be computed as:

\[\text{GCN}(\mathbf{X},\mathbf{A}^{\mathcal{X}}\mid\mathbf{W}^{\mathcal{X}})=(\widetilde{ \mathbf{D}}^{\mathcal{X}})^{-\frac{1}{2}\widetilde{\mathbf{A}}^{\mathcal{X}}}( \widetilde{\mathbf{D}}^{\mathcal{X}})^{-\frac{1}{2}}\mathbf{X}\mathbf{W}^{\mathcal{X}} \tag{1}\]

where \(\mathcal{X}=\{\mathcal{S},\mathcal{T}\}\) denotes the source and target domains. \(\mathbf{X}\) denotes the input data and \(\mathbf{W}^{\mathcal{X}}\) denotes the trainable weights. \(\widetilde{\mathbf{D}}^{\mathcal{X}}=\text{diag}(\widetilde{\mathbf{A}}^{\mathcal{X} }\mathbf{1})\) denotes the degree matrix for the graph \(\widetilde{\mathbf{A}}^{\mathcal{X}}\) and \(\widetilde{\mathbf{A}}^{\mathcal{X}}=\mathbf{A}^{\mathcal{X}}+\mathbf{I}\). Specifically, we adopt \(\ell\)-th layers of graph convolution network layers to achieve the users/items' mean and covariance of their distribution:

\[[\mathbf{\mu}^{U_{X}},\mathbf{\mu}^{V_{X}}]=\text{GCN}(\cdots\text{GCN}([\mathbf{E}^{U_{X }},\mathbf{E}^{V_{X}}],\mathbf{A}^{\mathcal{X}}\mid\mathbf{W}^{\mathcal{X}}_{\mu})\cdots) \tag{2}\]

where \(\mathbf{W}^{\mathcal{X}}_{\mu}\) and \(\mathbf{W}^{\mathcal{X}}_{\sigma}\) denote two trainable weights for estimating the mean and covariance respectively. \(\mathbf{\mu}^{U_{X}}\) and \(\mathbf{\sigma}^{U_{X}}\) denote the mean and covariance in the domain \(\mathcal{X}\). Since using the single user or item embeddings cannot depict more complicated user-item relationship, we adopt the Gaussian distribution to parameterize user and item distribution. Specifically, the Gaussian distribution can capture the learning more accuracy relationship between the users and items (Zhu et al., 2019; Wang et al., 2019; Wang et al., 2019; Wang et al., 2019). Therefore, we can obtain the user/item latent distribution \(\mathbb{P}(\mathbf{U}^{\mathcal{X}})\) and \(\mathbb{P}(\mathbf{V}^{\mathcal{X}})\) as follows:

\[\mathbb{P}(\mathbf{U}^{\mathcal{X}})=\mathcal{N}(\mathbf{\mu}^{U_{X}},(\mathbf{\sigma}^{U_{ X}})^{2}),\ \mathbb{P}(\mathbf{V}^{\mathcal{X}})=\mathcal{N}(\mathbf{\mu}^{V_{X}},(\mathbf{\sigma}^{V_{X }})^{2}) \tag{3}\]

After we obtain the user/item latent distribution, we should train the model based on user-item ratings. To better achieve this goal, we propose the distribution-based metric learning loss with self-adaptive margin as given below:

\[\begin{split} L_{R}&=\sum_{(U^{\mathcal{X}}_{I},V^{ \mathcal{X}}_{I})\in\mathcal{O}^{\mathcal{X}}_{N}}\left[d_{W}(\mathbb{P}(U^{ \mathcal{X}}_{I}),\mathbb{P}(V^{\mathcal{X}}_{J}))-m^{\mathcal{X}}_{i}\right]_{ \star}\\ &+\sum_{(U^{\mathcal{X}}_{I},V^{\mathcal{X}}_{J})\in\mathcal{O}^{ \mathcal{X}}_{N}}\left[m^{\mathcal{X}}_{i}-d_{W}(\mathbb{P}(U^{\mathcal{X}}_{ I}),\mathbb{P}(V^{\mathcal{X}}_{K}))\right]_{\star},\end{split} \tag{4}\]

where \(\mathcal{O}^{\mathcal{X}}_{P}\) and \(\mathcal{O}^{\mathcal{X}}_{N}\) denote the positive and negative user-item pairs respectively. \([\cdot]_{+}\) denotes the operation as \([\mathbf{x}]_{+}=\max(0,x)\). \(m^{\mathcal{X}}_{i}\) denotes self-adaptive margin for the \(i\)-th user in domain \(\mathcal{X}\). We adopt a fully-connected network \(G^{\mathcal{S}}_{m}\) and \(G^{\mathcal{T}}_{m}\) to obtain the adaptive margin as \(m^{\mathcal{X}}_{i}=G^{\mathcal{X}}_{m}(E^{U_{X}}_{i})\). The \(d_{W}(\cdot)\) denotes Wasserstein distance among different Gaussian distributions which can be calculated as:

\[\begin{split} d_{W}(\mathbb{P}(U^{\mathcal{X}}_{i}),\mathbb{P}(V^{ \mathcal{X}}_{J}))&=d_{W}(\mathcal{N}(\mathbf{\mu}^{U_{X}}_{i},(\mathbf{ \sigma}^{U_{X}}_{i})^{2}),\mathcal{N}(\mathbf{\mu}^{V_{X}}_{j},(\mathbf{\sigma}^{V_{J }}_{j})^{2}))\\ &=||\mathbf{\mu}^{U_{X}}_{i}-\mathbf{\mu}^{V_{X}}_{j}||^{2}_{2}+||\mathbf{ \sigma}^{U_{X}}_{i}-\mathbf{\sigma}^{V_{X}}_{j}||^{2}_{2}\end{split}\]

After adopting the metric-based rating prediction loss, we can pull the positive user-item pairs while push away the negative user-item pairs. Meanwhile for the different users, we provide adaptive margins to better pursuit the user preferences.

### Distribution Alignment Module

In the common assumptions of CDR, two domains always share similar characteristics (Chen et al., 2017). Hence, the user distributions across domains are always similar and should be mapped for transferring useful knowledge. In this section of distribution alignment module, we will provide the details of how to reduce the discrepancy between the source and target domains. We denote \(\mathbb{P}_{S}\) and \(\mathbb{P}_{T}\) as the source and target user probability distributions, respectively. In Dual-CDCSR setting, \(\mathbb{P}_{S}\neq\mathbb{P}_{T}\) because the user distributionsgenerated from the source domain and the target domain are heterogeneous, which leads to the _domain discrepancy_ problem as shown in Fig. 3(a). Specifically, the orange-colored source user distribution and blue-colored target user distribution are separated with the existence of domain bias and discrepancy. Thus, how to reduce the domain discrepancy and transfer useful knowledge across domains has become the key to high-performance Dual-CDCSR. To fulfill this task, we propose a novel distribution alignment module. In the feature space, we consider both overlapped and non-overlapped users' distribution information. Distribution alignment module has two main components, i.e., _Overlapped User Distribution Alignment_, and _General User Subgroup Distribution Alignment_ as shown in Fig.2. Overlapped user distribution alignment tends to align the concurrent users across domains to obtain domain-invariant representations. General user distribution alignment tends to map the subgroups user distributions for whole users on source and target domains for knowledge sharing.

#### 3.2.1. Overlapped User Distribution Alignment

Intuitively, the overlapped users should share similar latent distributions since they have similar preferences and characteristics. Therefore, we propose the user distribution mapping method for aligning the overlapped users among the \(U^{\mathcal{S}}\) and \(U^{\mathcal{T}}\). We adopt the Wasserstein distance for the loss function of user embedding mapping as:

\[L_{OU}=\sum_{i=1}^{N}\sum_{k=1}^{N}\delta(\mathbf{U}_{i}^{\mathcal{S}},\mathbf{U}_{k}^{ \mathcal{T}})\cdot d_{W}(\mathbb{P}(\mathbf{U}_{i}^{\mathcal{S}}),\mathbb{P}(\mathbf{ U}_{k}^{\mathcal{T}})) \tag{5}\]

where \(\delta(\cdot)\) indicates whether a user occurs simultaneously in both domains under the Dual-CSCDR setting. \(\delta(\mathbf{U}_{i}^{\mathcal{S}},\mathbf{U}_{k}^{\mathcal{T}})=1\) means the \(i\)-th user in the source domain has the same identification with the \(k\)-th user in the target domain, otherwise \(\delta(\mathbf{U}_{i}^{\mathcal{S}},\mathbf{U}_{k}^{\mathcal{T}})=0\). The loss function can further transfer the useful knowledge among the overlapped users to make the representations more expressive as shown in Fig. 3(b). Specifically, the overlapped users in source and target domains can be aligned in the latent space.

#### 3.2.2. General User Subgroup Distribution Alignment

Although aligning the overlapped users can enhance the model performance on Dual-CSCDR, it still neglect the abundant knowledge hidden behind the non-overlapped users. The smaller the overlapped ratio \(\mathcal{K}_{\mathbf{u}}\) is, the less information could the overlapped users provided. Meanwhile, only aligning the user distributions among the overlapped users cannot directly reduce the domain discrepancy for the whole users as shown in Fig. 3(b). Therefore, it is crucial to figure out the high efficient method to exploit the knowledge for the large majority of non-overlapped users across domains. In order to better fulfill the task of distribution alignment for the whole users across domains, we innovatively propose latent subgroup distribution alignment method which includes two main steps, i.e., _general user subgroup distribution measurement_ for estimating the latent user distribution among the source and target domains, and _general user subgroup distribution adaptation_ for reducing the domain discrepancy via optimal transport.

**General User Subgroup Distribution Measurement.** We first assume that both source and target domains have \(M\) subgroups \(\mathbb{P}(\mathbf{Z}^{\mathcal{S}})\) and \(\mathbb{P}(\mathbf{Z}^{\mathcal{T}})\). Meanwhile, users in the corresponding subgroups may have similar tastes or characteristics. Then we should exploit the relationship between these users and their corresponding subgroups. Therefore, we set \(\xi^{\mathcal{X}}\in\mathbb{R}^{N\times M}\) be the similarity matrix between the users and different subgroups. To make things simple, we adopt \(\mathcal{X}=\{\mathcal{S},\mathcal{T}\}\) to represent the source or target domain. We also prefer the similar users should have the similarity value on \(\xi^{\mathcal{X}}\). Inspired by [28, 36, 2], we propose Typical Subgroup Discovering algorithm (TSD) for user distribution measurement with the proximal term as:

\[\min_{\mathbb{P}(\mathbf{Z}^{\mathcal{X}}),\xi^{\mathcal{X}}\in\Delta}\sum_{i=1}^{ N}\sum_{j=1}^{M}\left[\xi_{ij}^{\mathcal{X}}\mathrm{KL}(\mathbb{P}(\mathbf{U}_{i}^ {\mathcal{X}})||\mathbb{P}(\mathbf{Z}_{j}^{\mathcal{X}}))+\epsilon_{U}\cdot\xi_{ ij}^{\mathcal{X}}\log\xi_{ij}^{\mathcal{X}}\right] \tag{6}\]

where \(\Delta=\{\xi_{ij}^{\mathcal{X}}\geq 0,\sum_{j=1}^{M}\xi_{i}^{\mathcal{X}}=1\}\) denotes the subjective condition and \(\mathbb{P}(\mathbf{Z}_{j}^{\mathcal{X}})=\mathcal{N}(\mathbf{\mu}_{j}^{\mathcal{X}},( \mathbf{\sigma}_{j}^{\mathcal{X}})^{2})\) denotes the \(j\)-th subgroup normal distribution and \(\mathbf{\mu}_{j}^{\mathcal{X}}\), \(\mathbf{\sigma}_{j}^{\mathcal{X}}\) denote the corresponding mean and variance. The second term \(\sum_{i=1}^{N}\sum_{j=1}^{M}\zeta_{ij}^{\mathcal{X}}\log\zeta_{ij}^{\mathcal{X}}\) is the entropy regularization for achieving nonnegative and nonlinearly results on \(\xi^{\mathcal{X}}\)[2].

**Optimization.** We neglect the irrelevant constant terms in Eq.(6) and use Lagrangian multiplier to minimize the objective function:

\[\min_{\mathbb{P}(\mathbf{Z}^{\mathcal{X}}),\xi^{\mathcal{X}}}J =\sum_{i=1}^{N}\sum_{j=1}^{M}\left[\xi_{ij}^{\mathcal{X}}\sum_{d= 1}^{D}\left[\log\frac{\sigma_{i,d}^{\mathcal{X}}}{\sigma_{i,d}^{\mathcal{U}}} +\frac{(\sigma_{i,d}^{\mathcal{U}})^{2}+(\mu_{i,d}^{\mathcal{U}}-\mu_{j,d}^{ \mathcal{U}})^{2}}{2(\sigma_{j,d}^{\mathcal{U}})^{2}}\right]\right]\] \[+\sum_{i=1}^{N}\sum_{j=1}^{M}\epsilon_{U}\cdot\xi_{ij}^{\mathcal{ X}}\log\xi_{ij}^{\mathcal{X}}+\sum_{i=1}^{N}\delta_{i}\left(\sum_{j=1}^{M}\xi_{ ij}^{\mathcal{X}}-1\right) \tag{7}\]

where \(\mathbf{\vartheta}\) is the Lagrangian multiplier for the constraints. We first fix the variable \(\mathbf{\mu}_{j}^{\mathcal{Z}}\), \(\mathbf{\sigma}_{j}^{\mathcal{Z}}\) and update \(\xi_{ij}^{\mathcal{X}}\). Taking the differentiation of Eq.(7) w.r.t. \(\zeta_{ij}^{\mathcal{X}}\) and setting it to \(0\), we can obtain:

\[\frac{\partial f}{\partial\xi_{ij}^{\mathcal{X}}}=0\Rightarrow\xi_{ij}^{ \mathcal{X}}=\frac{\exp(-\mathrm{KL}(\mathbb{P}(\mathbf{U}_{i}^{\mathcal{X}})|| \mathbb{P}(\mathbf{Z}_{j}^{\mathcal{X}}))/\epsilon_{U})}{\sum_{k=1}^{M}\exp(-\mathrm{ KL}(\mathbb{P}(\mathbf{U}_{i}^{\mathcal{X}})||\mathbb{P}(\mathbf{Z}_{k}^{\mathcal{X}})/\epsilon_{U})}. \tag{8}\]

Figure 3. The basic procedure of distribution alignment module.

Then we fix the variable \(\sigma_{j}^{Z_{X}}\), \(\zeta_{ij}^{X}\) and update \(\mathbf{\mu}_{j}^{Z_{X}}\). Taking the differentiation of Eq. (7) w.r.t. \(\mathbf{\mu}_{j}^{Z_{X}}\) and setting it to 0, we can update \(\mathbf{\mu}_{j}^{Z_{X}}\) via \(\mu_{j,d}^{Z_{X}}=\sum_{i=1}^{N}\zeta_{ij}^{X}\zeta_{i,d}^{U_{X}}\zeta_{i=1}^{ N}\zeta_{ij}^{X}\). Finally we fix the variable \(\mathbf{\mu}_{j}^{Z_{X}}\), \(\zeta_{ij}^{X}\) and update \(\mathbf{\sigma}_{j}^{Z_{X}}\). Taking the differentiation of Eq. (7) w.r.t. \(\sigma_{j}^{Z_{X}}\) and setting it to 0, we can update \(\mathbf{\sigma}_{j}^{Z_{X}}\) as:

\[\frac{\partial\sigma_{X}}{\partial\sigma_{j,d}^{Z_{X}}}=0\Rightarrow(\sigma_{ j,d}^{Z_{X}})^{2}=\frac{\sum_{l=1}^{N}\zeta_{ij}^{X}\cdot\{\{\zeta_{i,d}^{U_{X}} \}^{2}+\{\mu_{i,d}^{U_{X}}-\mu_{j,d}^{Z_{X}}\}^{2}\}}{\sum_{l=1}^{N}\zeta_{ij} ^{X}}. \tag{9}\]

After several iterations, we can obtain the stable values of \(\zeta_{ij}^{X}\), \(\mathbf{\mu}_{j}^{Z_{X}}\) and \(\mathbf{\sigma}_{j}^{Z_{X}}\). Therefore, the whole user distribution \(\mathbb{P}^{X}(x)\) can be depicted as a combination of these subgroups:

\[\mathbb{P}^{X}(x)=\sum_{k=1}^{M}\pi_{k}^{X}\mathcal{N}(\mathbf{\mu}_{k}^{Z_{X}}, \mathbf{\Sigma}_{k}^{Z_{X}})=\sum_{k=1}^{M}\pi_{k}^{X}\mathbb{P}(\mathbf{\mu}_{k}^{X}),\pi_{k}^{X}=\frac{1}{N}\sum_{k=1}^{N}\zeta_{ik}^{X}. \tag{10}\]

Through the calculation above, we can extract the latent user subgroup distribution as shown in Fig. 3(c).

**General User Subgroup Distribution Adaptation.** After obtaining the latent user distribution on the feature space, we tend to reduce the domain discrepancy for transferring useful knowledge among the non-overlapped users. Inspired by the widely used optimal transport techniques based on Kantorovich problem (Kang et al., 2018), we first proposed unbalanced distribution optimal transport with coupling matrix \(\mathbf{y}\in\mathbb{R}^{M\times M}\) as:

\[\mathbf{y}^{*}\coloneqq\arg\min_{Y}\int_{\mathbb{P}^{S}\times\mathbb{P}^{T}} \mathcal{C}(\mathbb{P}^{S},\mathbb{P}^{T})\ d\gamma(\mathbb{P}^{S},\mathbb{P}^ {T}). \tag{11}\]

The matrix function \(\mathcal{C}(\mathbb{P}^{S},\mathbb{P}^{T})\) denotes the cost to move from the source to target user probability distribution. Traditional distribution optimal transport always assume that the components of each subgroup are balance (Zhu et al., 2019; Wang et al., 2020). However, it cannot handle the situation when the proportion of subgroup distributions are unbalance (Brockman et al., 2018). This situation will always occur, e.g., the majority of users in source domain prefer romantic items while the majority of users in target domain prefer realistic items. At that time, the traditional distribution optimal transport may result in the worse coupling matrix due to the strict constraints across domains (Zhu et al., 2019; Wang et al., 2020). In order to resolve this issue, we propose Unbalance Distribution Optimal Transport (UDOT) by relaxing the original hard constraint through Kulback-Leibler divergence as:

\[\begin{split}\min_{\mathbf{\gamma}}&\min_{\mathbf{\gamma}} \int_{\mathbb{P}^{S}\times\mathbb{P}^{T}}\mathcal{C}(\mathbb{P}^{S},\mathbb{P }(\mathbf{Z}_{j}^{T}))+\epsilon_{Y}\sum_{l=1}^{M}\sum_{j=1}^{M}\mathbf{y}_{ij}(\log (\mathbf{y}_{ij})-1)\\ &+\tau\text{KL}\left(\mathbf{y}^{T}\mathbf{1}_{M}||\mathbf{x}^{S}\right)+\tau \text{KL}\left(\mathbf{y}^{T}\mathbf{1}_{M}||\mathbf{x}^{T}\right),\end{split} \tag{12}\]

where \(d_{W}(\mathbb{P}(\mathbf{Z}_{i}^{S}),\mathbb{P}(\mathbf{Z}_{j}^{T}))\) denotes the Wasserstein distance between the \(i\)-th and the \(j\)-th subgroup in the source and target domains respectively. \(\epsilon_{Y}\) and \(\tau\) denote as the balanced hyper parameters. The \(\text{KL}(\mathbf{x}||\mathbf{y})\) denotes the KL Divergence between two \(d\)-dimensional data samples \(\mathbf{x}\in\mathbb{R}^{D}\) and \(\mathbf{y}\in\mathbb{R}^{D}\) as \(\text{KL}(\mathbf{x}||\mathbf{y})=\sum_{l=1}^{D}\left[x_{l}\log\frac{\mathbf{y}_{l}}{ \mathbf{y}_{l}}-x_{l}+y_{l}\right]\).

**Optimization.** The UDOT can be solved by taking the differentiation on the coupling matrix \(\mathbf{\gamma}\) in Eq.(12) to obtain:

\[\begin{split}\frac{\partial\epsilon_{Y}}{\partial\gamma_{ij}}=d _{W}(p_{i}^{S},p_{j}^{T})+\epsilon_{Y}\log\gamma_{ij}+r\log\frac{\mathbf{y}^{T} \mathbf{1}_{M}}{\pi_{i}^{S}}+r\log\frac{\mathbf{y}^{T}\mathbf{1}_{M}}{\pi_{j}^{T}}=0.\end{split} \tag{13}\]

We apply the new variables \(\mathbf{\kappa}\) and \(\mathbf{\omega}\) as:

\[\begin{split}\mathbf{y}\mathbf{1}_{M}=\pi_{1}^{S}\exp\left(-\frac{ \kappa_{i}}{\tau}\right),\quad\mathbf{y}^{T}\mathbf{1}_{M}=\pi_{j}^{T}\exp\left(- \frac{\omega_{j}}{\tau}\right).\end{split} \tag{14}\]

Meanwhile the coupling matrix can be depicted as \(\mathbf{y}_{ij}=\exp((\kappa_{i}+\omega_{j}-d_{W}(\mathbb{P}(\mathbf{Z}_{i}^{S}), \mathbb{P}(\mathbf{Z}_{j}^{T})))/\epsilon_{Y})\). Taking them back to the Eq.(12), we could obtain the Fenchel-Lagrange conjugate form of the UDOT as:

\[\begin{split}\min_{\mathbf{\kappa},\mathbf{\omega}}&=\tau \sum_{l=1}^{M}\left[\pi_{i}^{S}\exp\left(-\frac{\kappa_{i}}{\tau}\right) \right]+\tau\sum_{j=1}^{M}\left[\pi_{j}^{T}\exp\left(-\frac{\omega_{j}}{\tau} \right)\right]\\ &+\epsilon_{Y}\sum_{l=1}^{M}\sum_{j=1}^{M}\exp\left(\frac{\kappa_{ i}+\omega_{j}-d_{W}(\mathbb{P}(\mathbf{Z}_{i}^{S}),\mathbb{P}(\mathbf{Z}_{j}^{T}))}{ \epsilon_{Y}}\right).\end{split} \tag{15}\]

The UDOT problem can be effectively solved by alternatively updating \(\mathbf{\kappa}\) and \(\mathbf{\omega}\). Therefore, we take the differentiation w.r.t. on \(\mathbf{\kappa}\), \(\mathbf{\omega}\) and set it equals to 0 as follows:

\[\begin{split}\kappa_{i}&=\frac{\tau\cdot\epsilon_{Y}}{ \tau+\epsilon_{Y}}\left(\log(\pi_{i}^{S})-\log\left(\sum_{j=1}^{M}\exp\left( \frac{\omega_{j}-d_{W}(\mathbb{P}(\mathbf{Z}_{i}^{S}),\mathbb{P}(\mathbf{Z}_{j}^{T}))}{ \epsilon_{Y}}\right)\right)\right)\\ \omega_{j}&=\frac{\tau\cdot\epsilon_{Y}}{\tau+\epsilon_ {Y}}\left(\log(\pi_{j}^{T})-\log\left(\sum_{i=1}^{M}\exp\left(\frac{\kappa_{ i}-d_{W}(\mathbb{P}(\mathbf{Z}_{i}^{S}),\mathbb{P}(\mathbf{Z}_{j}^{T}))}{\epsilon_{Y}}\right)\right) \right)\end{split} \tag{16}\]

After several iterations, we will achieve the optimal solution of \(\kappa_{i}^{*}\) and \(\omega_{j}^{*}\). Then we can obtain accurate coupling matrix \(\mathbf{\gamma}^{*}\) shown in Fig. 3(d). Finally, the loss of user distribution alignment can be provided as follows:

\[\begin{split} L_{NU}&=\sum_{l=1}^{M}\sum_{j=1}^{M} \exp\left(\frac{\kappa_{i}^{*}+\omega_{j}^{*}-d_{W}(\mathbb{P}(\mathbf{Z}_{i}^{S}), \mathbb{P}(\mathbf{Z}_{j}^{T}))}{\epsilon_{Y}}\right)\cdot d_{W}(\mathbb{P}(\mathbf{Z}_{ i}^{S}),\mathbb{P}(\mathbf{Z}_{j}^{T}))\right)\end{split} \tag{17}\]

Utilizing the user distribution alignment loss, we can align the whole user distributions across domains in Fig. 3(e). Note that even if only few users are overlapped, we can reduce the domain bias and discrepancy for knowledge sharing.

### Overall Procedure

The total loss of **UDMCF** could be obtained by combining the losses of the rating prediction module and the distribution alignment module. That is, the loss of **UDMCF** is given as \(\min L_{\text{UDMCF}}=L_{R}+\lambda_{OU}L_{OU}+\lambda_{NU}L_{NU}\), where \(\lambda_{OU}\) and \(\lambda_{NU}\) are hyper-parameters to balance different types of losses. By doing this, users with similar preference will be gathered across domains as shown in Fig.3(e). In testing phase for solving the **Task1**, one can predict the ratings between the source items and target users by taking the inner product \(\langle\mathbf{U}^{T},\mathbf{V}^{S}\rangle\). Similarly for the **Task2**, one can predict the ratings between the source users and target items by taking the inner product \(\langle\mathbf{U}^{S},\mathbf{V}^{T}\rangle\). Furthermore, we provide the the time complexity of our proposed method. Specifically, the typical subgroup discovering algorithm in user subgroup distribution measurement has the time complexity of \(O(N\times M)\). Then the time complexity of unbalance distribution optimal transport in user subgroup distribution alignment is \(O(M\times M)\). Finally, the total time complexity of proposed **UDMCF** is \(O(N\times M+M\times M)\).

## 4. Empirical Study

In this section, we conduct experiments on several real-world datasets to answer the following questions: (1) **RQ1**: How does our approach perform compared with the state-of-the-art recommendation methods? (2) **RQ2**: How do the overlapped user distribution alignment and general user distribution alignment contribute to performance improvement on different value of \(\mathcal{K}_{u}\)? (3) **RQ3**: How does the performance of **UDMCF** vary with different values of the hyper-parameters?

### Datasets and Tasks

We conduct extensive experiments on two popularly used real-world datasets, i.e., _Douban_ and _Amazon_. The **Douban** dataset (Song et al., 2018; Zhang et al., 2019) has two domains, i.e., Movie as Book. The **Amazon** dataset (Song et al., 2018; Zhang et al., 2019) has three domains, i.e., Movies and TV (Movie), Books (Book), and CDs and Vinyl (Music). The detailed statistics of these datasets have be shown in Table 1. For both datasets, we binarize the ratings to 0 and 1. Specifically, we take the ratings higher or equal to 4 as positive and others as 0. We provide three main scenarios to evaluate our model, i.e., **Douban Movie** & **Douban Book**, **Amazon Movie & **Amazon Music**, and **Amazon Book & Amazon Music**. It is noticeable that each scenarios includes the two main tasks under Dual-CSCDR settings.

### Experiment Settings

We randomly divide the user-item rating data into training, validation, and test sets with a ratio of 8:1:1. Meanwhile, we vary the overlapped user ratio \(\mathcal{K}_{u}\) in \(\{5\%,50\%,90\%\}\). We adopt the same method to adjust the overlapped user ratios following previous works (Zhou et al., 2019). In practice, we first choose the overlapped user according to the overlapped user ratio which was given. Then we keep half of the non-overlapped users across domains. The rest of non-overlapped users-item interactions are removed in the training phase and they can be regarded as the cold-start users. These cold-start users-item interactions are evaluated in the testing phase. Different user overlapped ratio represents different situations, e.g., \(\mathcal{K}_{u}=5\%\) represents only few users are overlapped while \(\mathcal{K}_{u}=90\%\) means most of users are overlapped following previous researches (Zhou et al., 2019; Zhang et al., 2019). We set batch size \(N=256\) for training. The latent dimension of mean/covariance for users and items are set to \(D=128\). In the rating prediction module, we set the number of graph convolution layers as \(f=3\). We set the latent user subgroup clusters as \(M=20\) in user distribution measurement. The entropy regularization term is set as \(\epsilon_{U}=1\) for user distribution measurement. For the unbalanced distribution optimal transport, we set \(\epsilon_{Y}=1\) and \(\tau=3\). For **UDMCF** model, we set the balance hyper-parameters as \(\lambda_{OU}=0.5\) and \(\lambda_{NU}=0.5\) empirically. In practice, we first choose the overlapped user according to the overlapped user ratio which was given. Then we keep half of the non-overlapped users across domains. The rest of non-overlapped users-item interactions are removed in the training phase and they can be regarded as the cold-start users. For all the experiments, we perform five random experiments and report the average results. We choose Adam (Kingma and Ba, 2015) as optimizer, and adopt Hit Rate@\(k\) (HR@\(k\)) and NDCG@\(k\)(Zhu et al., 2019) as the ranking evaluation metrics with \(k=10\).

### Baseline

We compare our proposed **UDMCF** with the following state-of-the-art recommendation models. (1) **NeuMF**(Kang et al., 2018) is the most popular recommendation model which utilizes the neural network for collaborative filtering on the single domain. (2) **EMCDR**(Zhu et al., 2019) is the popular CSCDR model which utilizes neural network to bridge the user embeddings from the source to target domains based on the matrix factorization. (3) **DCDCSR**(Zhu et al., 2019) utilized the sparsity degrees of individual users and items to guide the collaborative filtering and the mapping process across domains. (4) **SSCDR**(Zhou et al., 2019) adopts a semi-supervised approach for metric space mapping and multi-hop neighborhood inference. (5) **LACDR**(Zhu et al., 2019) adopts the dual autoencoder framework to align the overlapped users in the latent embedding space for cold-start recommendation. (6) **TMCDR**(Zhu et al., 2019) proposes a transfer-meta framework with a transfer stage and a meta stage with matrix factorization. (7) **DOML**(Zhu et al., 2019) is the state-of-the-art cross-domain method which adopts dual metrics learning in cross-domain recommendation. (8) **BIFGCF**(Zhu et al., 2019) adopts the graph neural network with feature transfer layer to fuse users' representations for solving the CSCDR problem (9) **PTUPCDR**(Zhu et al., 2019) utilizes a meta network fed with users' characteristic embeddings for personalized preferences transfer. (10) **CDRIB**(Chen et al., 2019) is the state-of-the-art model for CSCDR which adopts the graph-based information bottleneck to derive user/item unbiased representations. Besides, for a fair comparison, all the models use the same types of data and pre-processing methods during experiments.

### Recommendation Performance (for RQ1)

**Results and discussion.** The comparison results on Douban and Amazon datasets are shown in Table 2. The superscript of (S) and (T) on the corresponding dataset represent the source and target domains respectively. Meanwhile the superscript of (T1) and (T2) on the corresponding evaluation metric (i.e., HR and NDCG) indicate the Task1 and Task2 respectively. From them, we can find that: (1) Single domain recommendation model (e.g., **NeuMF**) cannot provide satisfactory results on the Dual-CSCDR problem since it cannot reduce the discrepancy between the source and target domains. Therefore, it is essential to map and transfer useful information across domains. (2) Cold-start cross domain recommendation models (e.g., **EMCDR**) provides better results when the overlapped user ratio is relatively high (\(\mathcal{K}_{u}=90\%\)). However, the recommendation performance degraded when the overlapped user ratio is relatively small (\(\mathcal{K}_{u}=5\%\)). At that time, only very few knowledge can be transferred among these overlapped user. (3) Better modelling the latent user distributions and mapping function (e.g., utilizing the orthogonal weights on the transformation in **DOML**) can indeed improve the representation ability on the Dual-CSCDR task. Nevertheless, they cannot fully exploit the structure of the whole latent distributions on the feature space which limits their potentials. (4) Furthermore, all previous CSCDR methods separated the modelling and mapping process which finally leading to the insufficient performance caused by the error of superimposition. (5) **UDMCF** with distribution alignment module can further enhance the performance under the Dual-CSCDR settings. Moreover,

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multicolumn{2}{c}{**Datasets**} & **Users** & **Items** & **Ratings** & **Density** \\ \hline \multirow{2}{*}{**Douban**} & Movie & 29,476 & 24,091 & 591,258 & 0.08\% \\  & Book & 41,884 & 579,131 & 0.05\% \\ \hline \multirow{2}{*}{**Amazon**} & Movie & 15,914 & 17,794 & 416,228 & 0.14\% \\  & Music & 20,058 & 280,398 & 0.09\% \\ \hline \multirow{2}{*}{**Amazon**} & Book & 16,267 & 18,467 & 233,251 & 0.08\% \\  & Music & 16,267 & 21,054 & 195,550 & 0.07\% \\ \hline \multirow{2}{*}{**Amazon**} & **15** & **12** & **12** & **12** & **12** \\  & **15** & **12** & **12** & **12** \\  & **15** & **12** & **12** & **12** \\ \hline \multirow{2}{*}{**Amazon**} & Movie & 15,914 & 17,794 & 416,228 & 0.14\% \\  & Music & 20,058 & 280,398 & 0.09\% \\ \hline \multirow{2}{*}{**Amazon**} & Book & 18,467 & 233,251 & 0.08\% \\  & Music & 16,267 & 21,054 & 195,550 & 0.07\% \\ \hline \hline \end{tabular}
\end{table}
Table 1. Statistics on Douban and Amazon datasets.

[MISSING_PAGE_FAIL:7]

**Book\({}^{(\text{S})}\)** & **Amazon Music\({}^{(\text{T})}\)** (second column) with the user overlapped ratio is \(\mathcal{K}_{u}=50\%\) are shown in Fig. 4(a)-(f). From it, we can conclude that (1) The domain discrepancy between the source and target users is commonly exist as shown in Fig. 4(a)-(b). Therefore, it is difficult to share and transfer useful knowledge across domains and leading to the poor performance on the Dual-CSCDR problem. (2) Applying **UDMCF**-Overlapped can map and align the overlapped users across domains as shown in Fig. 4(c)-(d). However, the domain bias still exists among the rest non-overlapped users which hurdle the recommendation. (3) Utilizing both overlapped user distribution alignment and general user distribution alignment in **UDMCF** can better align both overlapped and non-overlapped users as shown in Fig. 4(e)-(f). Overall, the above ablation study demonstrates that our proposed distribution alignment module is effective in solving the Dual-CSCDR problem.

**Effect of hyper-parameters.** We finally study the effects of hyper-parameters on model performance with NDCG@10. We vary \(\lambda_{OU}\) and \(\lambda_{NU}\) in \(\{0.05,0.1,0.5,1,3,10\}\) on **Douban Movie & Douban Book** with user overlapped ratio \(\mathcal{K}_{u}=5\%\), \(\mathcal{K}_{u}=90\%\) and report the results in Fig.5(a)-(b). It is straightforward to choose the proper hyper-parameters \(\lambda_{OU}\) and \(\lambda_{NU}\) to balance the rating prediction and distribution alignment based on the bell-shaped curve in Fig.5. When the \(\lambda_{OU}\) and \(\lambda_{NU}\) are two small, the distribution alignment loss cannot be adequately trained. However, much larger \(\lambda_{OU}\) and \(\lambda_{NU}\) may also hinder the training of rating prediction. As a result, we set \(\lambda_{OU}\) and \(\lambda_{NU}\) equals to 0.5 empirically. Finally, we further tune the number of user subgroups \(M\). We vary the \(M=\{5,10,15,20,30\}\) in **Amazon Movie & Music** and **Amazon Book & Music** with \(\mathcal{K}_{u}=5\%\). Then we report the results of HR and NDCG in Fig.6. The results indicate that fewer subgroups (e.g., \(M=5\) or \(M=10\)) may cannot better depict the user general distributions in the latent space. While when number of subgroups are much larger (e.g., \(M=30\)), it will cost longer training time and brings about the overfitting in some cases. Therefore, we set \(M=20\) empirically.

## 5. Conclusion

In this paper, we propose User Distribution Mapping with Collaborative Filtering (**UDMCF**) for Dual Cold-Start Cross Domain Recommendation, which includes the _rating prediction module_ and the _distribution alignment module_. Rating prediction module integrates one-hot ID vector and multi-hot rating interactions for modeling user/item distributions. In distribution alignment module, we innovatively propose overlapped user embedding alignment and general user subgroup distribution alignment to map and transfer knowledge. Specifically, we first propose latent subgroup distribution alignment to measure and align global user distributions across domains. We first propose unbalance distribution optimal transport with typical subgroup discovering to align both overlapped and non-overlapped users. It is noticeable that our proposed **UDMCF** can be trained end-to-end to avoid the error of superimposition. We also conduct extensive experiments to demonstrate the superior performance of our proposed **UDMCF** on several datasets and tasks. In the future, we plan to extend **UDMCF** to more recommendation tasks (e.g., item cold-start cross-domain recommendation) and conduct more comprehensive experiments on new datasets.

Figure 4. The t-SNE visualization of user latent embeddings on Amazon Movie\({}^{(\text{S})}\)\(\&\) Amazon Music\({}^{(\text{T})}\) (first column) and Amazon Book\({}^{(\text{S})}\)\(\&\) Amazon Music\({}^{(\text{T})}\) (second column) when the user overlapped ratio is \(\mathcal{K}_{u}=50\%\). The user latent embeddings in the source domain are shown with red dots and that in the target domain are shown with blue dots.

Figure 5. The hyper-parameters of \(\lambda_{OU}\) and \(\lambda_{NU}\) on Douban Movie\({}^{(\text{S})}\)\(\&\) Douban Book\({}^{(\text{T})}\) with user overlapped ratio \(\mathcal{K}_{u}=5\%,90\%\) on Task1 and Task2.

Figure 6. The hyper-parameters of tuning \(M\).

[MISSING_PAGE_EMPTY:9]

* Zhang et al. (2018) Qian Zhang, Jie Lu, Dianshuang Wu, and Guangquan Zhang. 2018. A cross-domain recommender system with kernel-induced knowledge transfer for overlapping entities. _IEEE transactions on neural networks and learning systems_ 30, 7 (2018), 1998-2012.
* Zhao et al. (2020) Cheng Zhao, Chenliang Li, Rong Xiao, Hongbo Deng, and Aixin Sun. 2020. CATN: Cross-Domain Recommendation for Cold-Start Users via Aspect Transfer Network. 229-238. [https://doi.org/10.1145/3397271.3401169](https://doi.org/10.1145/3397271.3401169)
* Zheng et al. (2022) Xiaolin Zheng, Jiajie Su, Weiming Liu, and Chaochao Chen. 2022. DDGHM: Dual Dynamic Graph with Hybrid Metric Training for Cross-Domain Sequential Recommendation. _Proceedings of the 30th ACM International Conference on Multimedia_ (2022).
* Zhu et al. (2019) Feng Zhu, Chaochao Chen, Yan Wang, Guanfeng Liu, and Xiaolin Zheng. 2019. DTCDR: A framework for dual-target cross-domain recommendation. In _CIKM_. 1533-1542.
* Zhu et al. (2020) Feng Zhu, Yan Wang, Chaochao Chen, Guanfeng Liu, Mehmet Orgun, and Jia Wu. 2020. A deep framework for cross-domain and cross-system recommendations. _arXiv preprint arXiv:2006.06252_ (2020).
* Zhu et al. (2020) Feng Zhu, Yan Wang, Chaochao Chen, Guanfeng Liu, and Xiaolin Zheng. 2020. A Graphical and Attentional Framework for Dual-Target Cross-Domain Recommendation. In _ICAI_. 3001-3008.
* Zhu et al. (2021) Feng Zhu, Yan Wang, Chaochao Chen, Jun Zhou, Longfei Li, and Guanfeng Liu. 2021. Cross-Domain Recommendation: Challenges, Progress, and Prospects. _arXiv preprint arXiv:2103.01696_ (2021).
* Zhu et al. (2021) Feng Zhu, Yan Wang, Jun Zhou, Chaochao Chen, Longfei Li, and Guanfeng Liu. 2021. A Unified Framework for Cross-Domain and Cross-System Recommendations. _IEEE Transactions on Knowledge and Data Engineering_ (2021), 1-1. [https://doi.org/10.1109/TKDE.2021.3104873](https://doi.org/10.1109/TKDE.2021.3104873)
* Zhu et al. (2021) Yongchan Zhu, Kaikia Ge, Panhen Zhuang, Ruobing Xie, Dongbo Xi, Xu Zhang, Leyu Lin, and Qing He. 2021. Transfer-Meta Framework for Cross-domain Recommendation to Cold-Start Users. In _Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval_. 1813-1817.
* Zhu et al. (2022) Yongchun Zhu, Zhenwei Tang, Yudan Liu, Furzhen Zhuang, Ruobing Xie, Xu Zhang, Leyu Lin, and Qing He. 2022. Personalized Transfer of User Preferences for Cross-domain Recommendation. In _Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining_. 1507-1515.
* Zhu et al. (2020) Ziwei Zhu, Shahin Sefati, Parasa Saadaraman, and James Cavelee. 2020. Recommendation for New Users and New Items via Randomized Training and Mixture-of-Experts Transformation. In _SIGIR_. 1121-1130.
* Zhuang et al. (2021) Fuhane Zhuang, Zhiyuan Qi, Keyu Yuan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing He. 2021. A Comprehensive Survey on Transfer Learning. _Proc. IEEE_ 109, 1 (2021), 43-76. [https://doi.org/10.1109/JPROC.2020.3004555](https://doi.org/10.1109/JPROC.2020.3004555)