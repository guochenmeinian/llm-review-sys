# Aligning Out-of-Distribution Web Images and Caption Semantics via Evidential Learning

Anonymous Author(s)

###### Abstract.

Vision-language models, pre-trained on web-scale datasets, have the potential to greatly enhance the intelligence of web applications (e.g., search engines, chatbots, and art tools). Precisely, these models [15, 24] align disparate domains into a co-embedding space, achieving impressive _zero-shot_ performance on multi-modal tasks (e.g., image-text retrieval, VQA). However, existing methods often rely on well-prepared data that less frequently contain noise and variability encountered in real-world scenarios, leading to severe performance drops in handling out-of-distribution (OOD) samples. This work first comprehensively analyzes the performance drop between in-distribution (ID) and OOD retrieval in Fig. 1. Based on the observations, this paper introduces a novel approach, Evidential Language-Image Posterior (ELIP) to achieve robust alignment between web images and semantic knowledge across various OOD cases by leveraging evidential uncertainties. The proposed ELIP can be seamlessly integrated into general image-text contrastive learning frameworks, providing an efficient fine-tuning approach without exacerbating the need for additional data. To validate the effectiveness of ELIP, we systematically design a series of OOD cases (e.g., image distortion, spelling errors, and a combination of both) on two benchmark datasets to mimic noisy data in real-world web applications.

Our experimental results demonstrate that ELIP improves the performance and robustness of mainstream pre-trained vision-language models against OOD samples on image-text retrieval tasks.

Vision-language modeling, uncertainty estimation, evidential learning 1

Footnote 1: [https://doi.org/XXXXXXXX.XXXXXX](https://doi.org/XXXXXXXX.XXXXXX)

2
Footnote 2: [https://doi.org/XXXXXXXX.XXXXXX](https://doi.org/XXXXXXXX.XXXXXX)

3
Footnote 2: [https://doi.org/XXXXXXXX.XXXXXX](https://doi.org/XXXXXXXX.XXXXXX)

## 1. Introduction

Web applications such as search engines, recommendation systems, etc., greatly benefit human daily life [6, 8, 14, 28, 34], most dealing with complicated data formats from different domains (e.g., search engines require massive semantic knowledge, and recommendation systems rely on image and text data). Among these web applications, multi-modal data of vision and language (VL) usually play an indispensable role [25], and have attracted remarkable research efforts [30, 31] in recent years. Particularly, CLIP [24] aligns vision and language domains into a shared embedding space, showing a promising zero-shot learning capacity for broad applications. However, web data frequently contend with many practical challenges, such as low-resolution images due to unreliable internet connections and text marred by garbled characters, leading to many out-of-distribution (OOD) samples compared with the clean, well-prepared training data. This gap raises a question - _will the pre-trained VL models be vulnerable to OOD samples in web applications?_

To investigate the above question, Fig. 1 shows an empirical study of two pre-trained VL models (CLIP [24] and BLIP [15]) for image and text retrieval over in-distribution (ID) and OOD samples. The clear performance drop between ID and OOD retrieval of these two state-of-the-art models inevitably casts a shadow of directly applying VL models to handle the wild web data. While fine-tuning the VL model with OOD data (varying with different domains) could be a solution, it is highly costly and generally infeasible due to the unknown data on the fly. Thus, we will propose an efficient uncertainty-aware fine-tuning approach to mitigate the negative impact of OOD samples on the pre-trained VL models.

Typically, there are three categories to uncertainty modeling: 1) deep ensemble [13], 2) variational inference [3, 4], and 3) deep evidential learning [1, 2, 26, 33]. Accounting for the large size of recent VL models, the first two uncertainty estimation methods may be less applicable since they both require multiple inference steps,

Figure 1. Average performance in terms of Recall@K (R@K) among the image-text retrieval. To measure the vulnerability of large-scale pretraining (e.g., CLIP and BLIP) against OOD samples, we evaluate them under simple noisy cases (OOD-image: Gaussian noise, random rotate OOD-text: natural noise [21]). Also, we calculate the performance drop (MMI) [23] scores between the ID and OOD retrieval to test the robustness of each model.

which can be computationally expensive, especially for the image-text ranking problem (where the pairwise calculation occurs). By contrast, deep evidential learning (Chen et al., 2019) provides explicit uncertainty representations based on a single forward pass, enriching uncertainty knowledge without additional inference costs. However, it is still under-explored in large-scale VL models since fine-tuning such networks requires high memory and computation requirements.

In this study, we fill in the gap of reasoning uncertainty for VL models by marrying deep evidential uncertainty into a parameter-efficient tuning framework. Concretely, we propose a novel Evidential Language-Image Posterior (ELIP) method, which leverages evidential learning with VL alignment to improve the generalization and reliability of pre-trained VL models in both ID and OOD cases. The proposed ELIP develops adapter (Chen et al., 2019; Chen et al., 2019) layers to fine-tune the pre-trained VL models to acquire evidence knowledge by optimizing using evidential loss. Compared to traditional contrastive learning methods that primarily focus on point estimation for the class probability of a sample, the evidential loss framework considers the entire probability distribution over all samples (Selvin et al., 2019), enhancing robustness against OOD samples and disclosing less confident predictions. Based on the ID and OOD retrieval settings, we conduct extensive experiments to demonstrate the effectiveness of ELIP. Our method outperforms several state-of-the-art VL models on image-text retrieval in most OOD cases. Our work showcases the potential of evidential learning for VL models and its importance in improving model reliability in realistic web scenarios. We summarize the main contributions of this work as follows.

* We introduce and design multiple OOD cases to investigate large-scale VL models against various noise on web data. We provide analysis of the MultiModal Impact (MMI) (Zhou et al., 2019) score and uncertainty estimation based on ID and OOD samples, thoroughly discussing the robustness and reliability of VL models on image-text retrieval tasks.
* We propose a novel uncertainty-aware, parameter-efficient tuning method termed ELIP. The proposed ELIP adopts evidential learning to integrate image-text matching and uncertainty estimation in a single forward pass.
* Extensive experiments show that our method improves state-of-the-art VL models, CLIP (Zhou et al., 2019) and BLIP (Chen et al., 2019), on image-text retrieval tasks against diverse OOD samples.

## 2. Out-of-Distribution Scenarios

We introduce two OOD scenarios based on benchmark datasets (e.g., MS-COCO and FLickr30k), aiming to mimic diverse practical web noisy data to assess the effectiveness of our approach and mainstream VL models for image-text retrieval tasks. We first introduce _simple OOD_ cases by adding random Gaussian noise into each image with the normal distribution variance as 0.1 or subjecting each image to a random rotation within 0 to 180 degrees. We use the same random seed in all experiments to ensure consistent generation for the random rotation. For textual input, we adopt the implementation described in (Zhou et al., 2019), generating naturally noisy text encompassing various error aspects, including diacritics, casing, spelling, suffix/prefix alterations, punctuation variations, whitespace anomalies, word order shifts, insertions, and replacements. Notably, these noisy samples are generated without the reliance on manually designed rules, enhancing the realism of the perturbations.

Secondly, we introduce _web OOD_ cases (Fig. 2). In realistic web applications, massive amounts of low-quality images are uploaded to the web every day. Some common cases include non-focus images, overexposed images, and compressed images. To mimic such noises, we follow (Zhou et al., 2019) by utilizing blur (zoom), weather (snow), and compression (JPEG) as image-OOD perturbations. Also, the web contains a tremendous amount of noisy image description, which includes spelling and disordered issues. This paper uses word-level synonym replacement (sr) and sentence-level (formal) perturbation to generate noisy captions. We analyze encompass results aggregated across five perturbation levels for each type of web OOD case. This paper mainly focuses on testing the model's robustness against OOD cases. As shown in Fig. 3, we have 10% of simple OOD cases and 90% of web OOD cases over image and text domains.

## 3. Methodology

### Overall Architecture

#### Vision-language Contrastive Learning.

Recent vision-language (VL) models use vision transformer as the image encoder to encode an input image \(I\) into a sequence of embeddings as \(\{v_{\textit{cls}},v_{1},\cdots,v_{N}\}\). They also employ a transformer network as the text encoder to transform input text \(T\) into a sequence of embeddings \(\{v_{\textit{pos}},v_{1},\cdots,v_{\textit{pos}}\}\). Where \(v_{\textit{cls}}\) and the activation of the highest layer of the transformer of \(w_{\textit{pos}}\) are treated as extracted features are normalized and linearly projected into a multi-modal \(D\)-dimension embedding space. We use \(v\in\mathbb{R}^{D}\) and \(w\in\mathbb{R}^{D}\) to denote the image and text features.

To learn an unimodal representation, image-text contrastive learning is leveraged to learn a similarity function. Specifically, the image-to-text and text-to-image similarities between one query sample and

Figure 2. Generated OOD web images and text for OOD retrieval. We present web OOD images (e.g., zoom blur, snow, low resolution) paired with one ID and two web OOD texts (e.g., synonym replacement (sr) and formal (Zhou et al., 2019)).

all positive and negative samples in the target set were computed as:

\[\rho^{2\Pi t}=\left\{\begin{matrix}v^{\top}W_{0},\cdots,v^{\top}W_{M} \end{matrix}\right\},\] \[\rho^{2\Pi t}=\left\{\begin{matrix}w^{\top}V_{0},\cdots,w^{\top}V_ {M}\end{matrix}\right\}, \tag{1}\]

where \(\iota\) is a logit-scale. \(V\in\mathbb{R}^{M\times D}\) and \(W\in\mathbb{R}^{M\times D}\) are image-text pairs representations. \(\rho^{2\Pi t}\) and \(\rho^{2\Pi t}\) can be used to find the correct matching in a top-K list (retrieval), such that parallel image-text pairs should return higher similarity scores. Let \(y^{2\Pi t}\) and \(y^{2\Pi}\) be the one-hot label, representing positive sample as 1 and negative sample as 0. The image-text contrastive loss, consisting of image-to-text and text-to-image matching, is defined with cross-entropy (\(\ell\)) as

\[\mathcal{L}_{\ell t\epsilon}=\frac{1}{2}[\ell(y^{2\Pi},\sigma(\rho^{2\Pi}))+ \ell(y^{2\Pi},\sigma(\rho^{2\Pi}))], \tag{2}\]

where \(\sigma\) is a softmax function. However, Eq. (2) only considers the alignment between the correct pairs when getting the cross-embedding, without modeling the uncertainty between the query and all the other target samples. To estimate uncertainty in cross-alignment, this work introduces evidential knowledge in contrastive learning by learning a distribution over the similarities between all the cross-embedding.

**Bottleneck Adapter.** Adapter module (Bottleneck, 2018) can be easily plugin-and-play into existing network to enable parameter-efficient transfer learning. Specifically, the adapter is a bottleneck structure with linear layers governed by a residual connection between the block's input and output. This work used the pre-trained CLIP (Srivastava et al., 2014) and BLIP (Kumar et al., 2014) as the backbone models. Following the approach in (Bottleneck, 2018), we inserted one adapter after the self-attention and MLP layers, respectively, in each transformer layer of the vision and language encoders (see Fig. 4). Eventually, the CLIP model has 64M trainable extra parameters, accounting for 13% of the entire model, while the BLIP model has 141M trainable extra parameters, which is 38%. We obtained new image and text features after passing through the pre-trained normalization and linear projection layers. These features were then used to compute the similarities \(\rho^{2\Pi t}\) and \(\rho^{2\Pi}\) in Eq. (1).

### Uncertainty Estimation with Cross Embedding

Recent evidential deep learning (Bottleneck, 2018; Wang et al., 2019) methods aim to overcome the limitations of the standard Softmax-based model for uncertainty estimation. Specifically, the Softmax function provides a point estimation for the matching similarity between the query and targets, which keeps reporting low uncertainty in OOD cases. Differently, the evidential deep learning framework models the uncertainty by placing a Dirichlet distribution (Dir) over the probability distribution. Also, deep evidential allows quantifying the uncertainty under a well-defined theoretical framework by leveraging Subjective Logic (SL) (Brock, 2018). Typically, SL is beneficial when there are multiple sources of information with varying levels of trustworthiness or when dealing with subjective opinions and beliefs. In this paper, the proposed method targets an image-text retrieval task, which involves feature alignment and a ranking process that contains multiple sources of information and different levels of trustworthiness, respectively. Therefore, we consider using Subjective Logic to quantify cross-modal retrieval uncertainty.

Typically, SL considers a frame of K mutually exclusive singletons (e.g., class labels) by providing a belief mass

\[b_{k}=\frac{e_{k}}{\sum_{i=1}^{K}(e_{i}+1)}, \tag{3}\]

for each singleton \(k=1,\cdots,K\), where \(e_{k}>0\) is the evidence derived for the \(k^{th}\) singleton. Note that the overall uncertainty mass of \(u\) and all non-negative belief masses are sums up to one, i.e.,

\[u=1-\sum_{k=1}^{K}b_{k}=\frac{K}{\sum_{i=1}^{K}(e_{i}+1)}, \tag{4}\]

where the uncertainty is also inversely proportional to the total evidence. When the evidence for each singleton is zero, the total belief is zero, and the uncertainty is one. Current methods have different theories to define the Dir. Generally, the evidence assigned corresponds to a Dir with parameters \(a_{k}=e_{k}+1\). While in another work (Wang et al., 2019), given a sample \(x_{k}\) and a classical \(f(\theta)\) with parameters \(\theta\), the corresponding Dir has parameters \(a_{k}=f(x_{k}\mid\theta)+1\).

However, this work considers cross-domain information, which differs from the previous methods that use single-domain data. Specifically, we use multi-modal embedding and define \(a\) using cross similarities between \(M\) image-text pairs. Therefore, the subject opinion for the \(i^{th}\) query and the \(j^{th}\) target sample can be computed from the parameters of the corresponding Dir using

\[b_{j}^{(i)}=\frac{\alpha_{1}^{(i)}-1}{\sum_{k=1}^{M}(a_{l}^{(i)})}. \tag{5}\]

Let \(\alpha^{(i)}=<\alpha_{1}^{(i)},\cdots,\alpha_{M}^{(i)}>\) become the parameter of a Dir for the cross similarities, then \((\alpha_{2}^{(i)}-1)\) is the evidence estimated by the matching similarity between the \(i^{th}\) query and the \(j^{th}\) target sample, where \(i,j=1,\cdots,M\). Finally, given these parameters, the prediction uncertainty can be computed using Eq. (4) for each query samples.

Specifically, we define evidence as a measure of the amount of similarity between query and target samples in favor of aligning the positive sample and pushing away the negative samples. For convenience, we assign the similarity vector \(\rho\in\mathbb{R}^{M\times M}\) computed in Eq. (1) as the general representation for \(\rho^{2\Pi t}\) and \(\rho^{t2\Pi}\), since image-to-text and text-to-image similarities share the same computation process for evidence. Also, we assign \(a\) to represent \(\alpha^{2\Pi t}\) and \(\alpha^{t2\Pi}\). This work defines the Dir over cross-embedding between the query and the target samples. By taking the cross similarities \(\rho^{l}\in\mathbb{R}^{M}\)

Figure 3. Case study. Visualize the percentage of different OOD cases in the image and text domain. For simple OOD, we have rotation, Gaussian, and natural. For web OOD, we have snow, zoom, JPEG, formal, and synonym replacement (sr).

between the \(t^{th}\) query and all target samples, the \(j^{th}\) parameter of the Dir \(\alpha_{j}^{(i)}\) is computed as

\[\alpha_{j}^{(1)}=\exp{(\rho_{j}^{(1)})}+1. \tag{6}\]

We apply \(\exp(\cdot)\) as an activation function to ensure positive evidence for all cross-embedding. Because \(\rho^{(1)}\) is the cross similarity between image and texts, the value is greater than zero only for the parallel pair. Eventually, Eq. (6) takes input computed in Eq. (1), and the output \(\alpha\) can be used to calculate uncertainty in Eq. (4). Our proposed \(\alpha\) surprisingly connects cross-modal alignment and evidential learning in a single forward pass.

Eventually, our model learns and updates the Dir by using the image-text similarity as subjective opinions and collects evidence that leads to those opinions. During training, the expected matching similarity for the \(i^{th}\) query and the \(j^{th}\) target sample is computed as

\[\mathbb{E}[p_{j}^{(1)}]=\frac{\alpha_{j}^{(i)}}{\sum_{i^{th}_{1}}\alpha_{i}^{( i)}}, \tag{7}\]

since the distribution is a probability density for possible values of the probability mass \(p\), where \(p_{j}^{(1)}\in[0,1]\). For convenience, we assign \(p\) to represent \(p^{22t}\) and \(p^{22t}\). Throughout the training process, when an observation about a query sample relates it to one of the \(M\) target samples, the corresponding Dirichlet parameter is incremented to update the Dir with the new observation. For instance, the increment matching similarity between image and text may contribute to its feature alignment, which may benefit image/text encoder learning.

### Learning with Evidential Knowledge

Having formalized the use of a Dirichlet Distribution to capture evidence knowledge, we next describe our approach for optimizing the model to output the parameters of this distribution. For the VL model, the objective is to align two domains into the same space, and we follow this idea by first calculating the similarity between the image and text features. Secondly, instead of using the matching score for gradient computation, we structure the learning process with two distinct parts: (1) acquiring model evidence to support our observation and (2) minimizing evidence uncertainty when the feature alignment is low. Eventually, we can fit our data to the evidential model at a high level while enforcing a prior to remove false evidence and inflate uncertainty.

**Evidential Loss.** To better explain, we assign \(\alpha\) to \(\alpha^{(i)}\) as the cross similarities between the \(i^{th}\) query and all target samples in the following sections. We define a loss function and compute its Bayes risk for the Dirichlet parameters. For image-to-text and text-to-image matching, we denote the Bayes risk as

\[\mathcal{L}^{22t} =\sum_{j=1}^{M}y_{j}^{2t2}(\psi(S^{2t2})-\psi(\alpha_{j}^{(2t2)})), \tag{8}\] \[\mathcal{L}^{t2t} =\sum_{j=1}^{M}y_{j}^{2t2}(\psi(S^{2t2})-\psi(\alpha_{j}^{(2t2)})),\]

where \(\psi(\cdot)\) is the _digamma_ function and \(S=\sum_{j=1}^{M}\alpha_{j}\) is Dirichlet strength, we assign \(S\) to represent \(S^{2t2t}\) and \(S^{t2t}\).

**Minimizing Evidence on Errors.** The evidential loss provides an objective function for the training model to output image and text feature alignment distribution to fit the observations by maximizing the model evidence. However, due to the negative samples in the training batch, the model may be misdirected and put strong evidence for the wrong prediction. Therefore, we describe how to regularize training by applying an incorrect evidence penalty and aim to minimize the evidence of incorrect matching. We define \(\tilde{a}=y+(1-y)\odot a\), where \(\tilde{a}\) and \(y\) represent \(\tilde{a}^{2t2t}\), \(\tilde{a}^{2t2}\) and \(y^{2t2t}\), \(y^{t2t}\). Consequently, we incorporate a Kullback-Leibler (KL) divergence term into our loss function, where KL can be a regularization term by penalizing those divergences from negative samples that do not contribute to data fit. Overall, the evidential loss \(\mathcal{L}_{ev}(\theta)\) consists of

Figure 4. Illustration of our proposed ELIP model. Our method can perform image-text retrieval and uncertainty estimation in a single forward step. The image and text encoder were fine-tuned by different adapters with scalable parameters on clean data without augmentation. We develop a new evidential loss (\(\mathcal{L}_{ev}\)) to implement image-text matching tasks, and the learned Dirichlet distribution (Dir) posterior is used for uncertainty estimation and OOD detection.

two terms for maximizing and regularizing evidence, scaled by \(\lambda_{t}\)

\[\mathcal{L}_{eq}^{\ell zz}=\mathcal{L}^{\ell zz}+\lambda_{t}\text{KL}[D(p^{\ell zz }|\hat{\alpha}^{\ell zz})||D(p^{\ell zz}|\left\langle 1,\cdots,1\right\rangle)],\] \[\mathcal{L}_{eq}^{\ell zz}=\mathcal{L}^{\ell zz}+\lambda_{t}\text{KL }[D(p^{\ell zz}|\hat{\alpha}^{\ell zz})||D(p^{\ell zz}|\left\langle 1,\cdots,1\right\rangle)], \tag{9}\]

where \(\lambda_{t}=min(1.0,t/15)\) is the annealing coefficient, t is the index of the current training epoch, \(D(p|\left\langle 1,\cdots,1\right\rangle)\) is the uniform Dirichlet distribution, and \(\hat{\alpha}\) is the Dirichlet parameters of misleading evidence from \(\alpha\). The KL divergence term \(KL[D(p|\hat{\alpha})||D(p|\left\langle 1,\cdots,1\right\rangle)]\) can be compute as

\[\log(\frac{\Gamma(\hat{S})}{\Gamma(M)\prod_{i=1}^{M}\Gamma(\hat{\alpha}_{i})}) +\sum_{j=1}^{M}(\hat{\alpha}_{j}-1)[\psi(\hat{\alpha}_{j})-\psi(\hat{S})].\]

We use dynamic scaling \(\lambda_{t}\)to modify the weights of \(KL\) term, leading the model to focus on learning relationships between positive pairs at the beginning and gradually put more attention on negative pairs. Specifically, by gradually increasing the effect of the \(KL\) divergence, we allow the neural network to explore the parameter space and avoid premature convergence to the uniform distribution for the misaligned samples.

Empirically, the total loss \(\mathcal{L}_{EV}\) consists of two terms to update the image and text encoder evenly:

\[\mathcal{L}_{eq}=\frac{1}{2}(\mathcal{L}_{eq}^{\ell zz}+\mathcal{L}_{eq}^{\ell zz }). \tag{10}\]

Overall, we leverage evidential loss to fine-tune the pre-trained CLIP and BLIP models using ID data. By updating the inserted adapters, ELIP can preserve high performance on ID retrieval tasks [58] while achieving reliable performance on OOD retrieval tasks (refer to Table 1). During training within a high-level embedding dimension, the model captures deeper connections between images and text, which enables the generation of evidence for pairwise feature alignment based on these patterns, thereby minimizing the overall loss.

**Datasets and Evaluation Metrics.** We train and evaluate our model on MS-COCO [18] and Flickr30K dataset [32]. We follow the splits of COCO-Karpathy, which contains 118,287 image-text pairs for training and 5000 for testing. Flickr30K consists of 31783 image-text pairs, and we use the standard training and test split [11], which contains 28000 and 1000 samples. We evaluate the performance of our model using the common Recall@K (R@K) metric, which measures the proportion of correct matches among the top K retrieved results. Based on our OOD cases, Table 1 illustrates five evaluation metrics. The examples of image retrieval: R@K over ID retrieval (T \(\rightarrow\) 1), text-OOD retrieval (T* \(\rightarrow\) 1), image-OOD retrieval (T* \(\rightarrow\) 1), multi-OOD retrieval (T* \(\rightarrow\) 1), and Multi-Modal Impact score (MMI) [23] (% of performance drop between ID and OOD retrieval).

**Implementation Details.** We use pre-trained CLIP _zero-shot_ and BLIP _fine-tuning_ as our backbone models and initialize our implementation with their weights. To fine-tune the model efficiently, we independently modify the image and text encoder by inserting adapters. Expressly, we set the bottle-neck feature dimension to half of the feature dimension from the previous layer, and we use RELU as the activation function. In order to sustain the performance pre-trained on previous knowledge, we initialize all new parameters of adapters with values drawn from the normal distribution with \(\mu=0\), and \(\sigma=0.001\). We fine-tune all the models for 30 epochs with a batch size 280. We use the AdamW [19] optimizer with an initial

\begin{table}
\begin{tabular}{l c c c c c c c c c c c c c c c c} \hline \hline  & \multicolumn{3}{c}{T \(\rightarrow\)1} & \multicolumn{3}{c}{T \(\rightarrow\)1} & \multicolumn{3}{c}{T* \(\rightarrow\)1} & \multicolumn{3}{c}{T* \(\rightarrow\)1} & \multicolumn{3}{c}{MMI} & \multicolumn{3}{c}{528} \\  & \multicolumn{3}{c}{R@1} & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\ \hline CLIP [24] & 35.3 & 60.0 & 70.2 & 30.4 & 54.4 & 65.3 & 27.7 & 50.8 & 61.3 & 24.2 & 46.4 & 56.9 & 122.3\% & 15.8\% & 112.9\% \\ BLIP [15] & 56.9 & 80.8 & 87.9 & 43.1 & 67.8 & 76.5 & 50.0 & 74.7 & 82.8 & 36.9 & 60.6 & 70.1 & 123.8\% & 116.2\% & 113.0\% \\ \hline ALBEF [16] & 60.7 & 84.3 & 90.5 & 47.8 & 72.0 & 80.3 & 51.9 & 76.8 & 85.6 & 41.2 & 65.6 & 74.7 & 122.6\% & 115.2\% & 114.4\% \\ BLIP [15] & **64.3** & **85.7** & **91.5** & 51.4 & 74.5 & 82.1 & **57.2** & **80.3** & **87.4** & 45.2 & 68.8 & 77.2 & 120.3\% & 113.0\% & 110.1\% & 113.0\% \\ \hline ELIP (ours) & 60.4 & 83.9 & 90.5 & **51.5** & **76.9** & **85.0** & 52.3 & 76.9 & 851.5 & 43.7 & **69.4** & **78.8** & **116.6** & **111.3** & **18.3** & 534 \\ ELIP+(ours) & 63.7 & 85.4 & 91.3 & 51.0 & 74.5 & 82.3 & 57.0 & 80.0 & 87.2 & **45.6** & 69.3 & 77.8 & 119.6\% & 112.6\% & 19.7\% \\ \hline \hline \multicolumn{3}{c}{} & \multicolumn{3}{c}{I \(\rightarrow\) T} & \multicolumn{3}{c}{I* \(\rightarrow\) T} & \multicolumn{3}{c}{I* \(\rightarrow\) T*} & \multicolumn{3}{c}{MMI} & \multicolumn{3}{c}{538} \\  & \multicolumn{3}{c}{Text Retrieval} & \multicolumn{3}{c}{R@1} & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & R@1 & R@5 & R@10 & 538 \\ \hline CLIP [24] & 56.0 & 79.6 & 86.9 & 46.3 & 71.1 & 79.9 & 46.1 & 71.5 & 80.5 & 36.6 & 62.5 & 73.0 & 123.3\% & 126.7\% & 110.5\% \\ BLIP [15] & 72.5 & 90.0 & 94.7 & 52.1 & 73.4 & 81.0 & 67.6 & 87.9 & 93.3 & 48.2 & 71.1 & 78.9 & 122.8\% & 113.9\% & 110.9\% \\ \hline ALBEF [16] & 77.6 & 94.3 & 97.2 & 59.8 & 79.5 & 85.3 & 71.0 & 90.6 & 94.9 & 54.7 & 75.7 & 82.4 & 120.3\% & 113.1\% & 19.9\% & 541 \\ BLIP [15] & **81.9** & **95.4** & **97.8** & 64.8 & 82.6 & 87.6 & **76.4** & **93.3** & **96.5** & 59.8 & 79.5 & 85.5 & 118.2\% & 110.8\% & 18.1\% & 542 \\ \hline ELIP (ours) & 77.5 & 94.2 & 97.0 & **66.3** & **86.0** & **91.7** & 71.3 & 90.8 & 95.0 & **60.0** & **82.2** & **88.7** & **115.0\%** & **18.4\%** & **15.4\%** & 543 \\ ELIP+(ours) & 81.3 & 95.2 & 97.7 & 64.6 & 82.6 & 87.8 & 76.2 & 92.9 & 96.2 & 59.9 & 79.6 & 85.4 & 117.7\% & 110.7\% & 18.1\% & 544 \\ \hline \hline \end{tabular}
\end{table}
Table 1. Comparison of performance in terms of Recall@K (R@K) and MMI [23] score among ID and simple-OOD retrieval. CLIP and BLIP are pre-trained _zero-shot_; the others were fine-tuned on clean MS-COCO. ELIP and ELIP+ were transfer learned from pre-trained CLIP and BLIP. For ID image (I) and ID text (T) retrieval, BLIP reports the best performance, but ELIP surpasses other models in retrieval between OOD Image (I*) and OOD text (T*). From the MMI score, ELIP achieves the lowest performance drop.

Figure 5. OOD detection by uncertainty of ELIP on ID and OOD image-text retrieval on MS-COCO. The uncertainty values are in the range (0.75–1.00) within each distribution.

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

knowledge extraction with the assistance of extra adapters, leading to better cross-modal alignment. Furthermore, the model becomes more robust after optimizing using evidential learning since the MMI score of ELIP drops compared to ELIP w/o Ev. When all components were utilized, the effects of the adapters and evidential learning complemented each other, resulting in substantial improvements compared to regular image-text contrastive learning.

## 5. Related Work

**Vision-language Modeling and its Web Application.** The current research focuses on vision-language (VL) pre-training, whether using an encoder-based or complex encoder-decoder-based structure. Encoder-based methods are mainly single-stream and double-stream methods, where single-stream uses a single transformer encoder to concatenate image and text embedding, e.g., VL-BERT (Devlin et al., 2018), ImageBERT (Devlin et al., 2019), Unified VLP (Wang et al., 2020), ViLBERT (Wang et al., 2020), and VisualBERT (Devlin et al., 2021). In comparison, double-stream methods use image and text encoders to extract features separately, e.g., CLIP. Some encoder-decoder-based models leverage cross-modal attention and combine multi-tasks (e.g., image-text retrieval, image captioning) to achieve better performance and higher flexibility on many downstream tasks, e.g., BLIP. In the meantime, due to the demand for large-scale data and the limitation of human-annotated data, most methods use image-data pairs collected from the Web like LAION (Wang et al., 2020), VG (Chen et al., 2020). In our task, we exploit CLIP, a two-stream method with excellent image-text matching performance. As a significant step towards flexible and practical _zero-shot_ classifier, CLIP has a clean and relatively simple structure, with two transformer networks used to extract the image features and text features respectively and finally cross-connect during loss calculation. Also, CLIP was trained using 400M image-text pairs collected from the Web. According to the results reported in CLIP, its image-text retrieval _zero-shot_ performance surpasses some fine-tuned models. Due to this impressive performance, many soft works leverage the power of large-scale VL pre-training and benefit the development of web applications (Wang et al., 2020). Therefore, powerful vision-language pre-training plays a significant role in recent web application studies.

**Uncertainty Estimation.** Recent studies declared that uncertainty estimation in DNN contains four different steps (Chen et al., 2020), (1) Data acquisition; (2) DNN building; (3) applied inference model; and (4) Prediction's uncertainty model. Several factors may cause model and data uncertainty and affect the model prediction. There are many methods to achieve uncertainty estimation. Single deterministic methods predict uncertainty based on the forward pass; Bayesian (Chen et al., 2020) methods consider the class probabilities and distributions (Chen et al., 2020; Chen et al., 2020). Evidential Deep Learning (EDL) (Chen et al., 2020) starts to attract attention due to its convenience, allowing the uncertainty computation to be done in a single forward pass and set of weights. Existing works show the benefits from EDL when doing uncertainty estimation on their model, including regression task (Chen et al., 2020; Chen et al., 2020), classification task (Wang et al., 2020; Wang et al., 2020). We apply the EDL framework to the retrieval task, and the experimental results also show promising results for uncertainty estimation.

## 6. Conclusions and Social Impacts

This paper introduces an innovative methodology aimed at harnessing the power of evidential learning within the context of noisy web images and semantic knowledge alignment. By preliminary analysis of the performance drop between ID and OOD retrieval, we propose ELIP to effectively against noisy samples. Intuitively, our work can benefit cross-modal related web applications such as robust retrieval and recommendation. To accomplish this, we employ adapters to facilitate the efficient fine-tuning of CLIP and BLIP. We progressively transit the pre-trained model from a simplistic probability distribution (e.g., softmax) to the more robust Dirichlet Distribution (evidence) via deep evidential learning. We provide extensive studies encompassing multiple scenarios, catering to ID and OOD image-text retrieval tasks. Specifically, the OOD retrieval widely covers different noisy settings, including simple noisy and web-style noisy images and text. The proposed methodology is subjected to rigorous theoretical scrutiny and empirical validation, substantiating its efficacy in achieving dependable image-text retrieval and accurate uncertainty estimation. The efficiency and scalability inherent in our approach render it well-suited for precise and expeditious uncertainty estimation within cross-modal systems, especially within domains that demand safety-critical predictions in the context of image-text alignment.

While this work improves the robustness of multi-modal alignment against web OOD cases, it is crucial to keep exploring noisy samples to enhance our method for better web applications. Specifically, in real-world scenarios, web data may include unintended private information, unsuitable images, or biased texts, leading to more complex OOD samples in a multi-modal context. We hope the proposed method could inspire future work to focus more on improving the robustness of vision-language modeling.

\begin{table}
\begin{tabular}{l l l l l l l l l l l} \hline \hline Method & I\(\rightarrow\) T & T\(\rightarrow\) I & I\({}^{*}\)\(\rightarrow\) T & T \(\rightarrow\) I\({}^{*}\) & I\(\rightarrow\) T\({}^{*}\) & T\({}^{*}\)\(\rightarrow\) I & I\({}^{*}\)\(\rightarrow\) T\({}^{*}\) & I\({}^{*}\)\(\rightarrow\) I\({}^{*}\) & i2t MMI & i2i MMI & i2i MMI \\ \hline ELIP w/o A & 60.2 & 44.5 & 51.7 & 38.4 & 49.8 & 36.1 & 43.1 & 30.6 & \(\downarrow\)19.9\% & \(\downarrow\)21.3\% & 878 \\ ELIP w/o IA & 71.3 & 52.8 & 62.1 & 45.6 & 63.8 & 44.3 & 55.1 & 38.1 & \(\downarrow\)15.4\% & \(\downarrow\)19.2\% \\ ELIP w/o TA & 76.6 & 60.1 & 63.8 & 51.0 & 68.0 & 51.5 & 55.6 & 42.3 & \(\downarrow\)18.5\% & \(\downarrow\)19.7\% \\ ELIP w/o Ev & 76.7 & 60.3 & 64.3 & 51.4 & 70.5 & 51.9 & 58.2 & 43.3 & \(\downarrow\)16.1\% & \(\downarrow\)19.0\% \\ \hline ELIP & **77.5** & **60.4** & **66.3** & **51.5** & **71.3** & **52.3** & **60.0** & **43.7** & \(\downarrow\)**15.0**\% & \(\downarrow\)**18.6**\% \\ \hline \hline \end{tabular}
\end{table}
Table 4. Ablation study of the proposed ELIP by Recall @ 1. We report ID retrieval and average results on the simple-OOD retrieval. For fine-tuning the projection layers of CLIP using evidential loss (ELIP w/o \(\lambda\)), the performance of ID (I, T) retrieval and OOD (I\({}^{*}\), T\({}^{*}\)) retrieval is lower than fine-tuning with text adapter (ELIP w/o IA), fine-tuning with image adapter (ELIP w/o TA), ours (ELIP). Furthermore, the improvement of MMI (Chen et al., 2020) score between fine-tuning without evidence loss (ELIP w/o Ev) and ours (ELIP) proves the effectiveness of our method against the OOD issue.

* Amini et al. (2020) Alexander Amini, Wilko Schwarting, Ava Solicitamy, and Daniela Rus. 2020. Deep evidential regression. _Advances in Neural Information Processing Systems_ 33 (2020), 14927-14937.
* Bao et al. (2021) Wentao Bao, Qi Yi, and Xu Kong. 2021. Evidential Deep Learning for Open
* Set Action Recognition. _2021 IEEE/CVF International Conference on Computer Vision (ICCV)_ (2021), 13329-13338.
* Shindell et al. (2015) Charles Shindell, Julien Cornebise, Koray Kawukcuoglu, and Daan Wiestra. 2015. Weight Uncertainty in Neural Networks. _ArXiv_ abs/1505.05424 (2015).
* Gal and Ghahramani (2016) Yixin Gal and Zoubin Ghahramani. 2016. Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning. _arXiv_ abs/1506.02142 (2016).
* Goodfellow et al. (2014) Jakob Gozufikowski, Cadignie Rowe Ljjicuchu Tassi, Mohsin Ali, Jongsee Lee, Matthias Humt, Jianxiang Feng, Anna M. Kruspe, Rudolph Triebel, Peter Jung, Ribana Kocher, M. Shihadz, Wan Fang, Richard Bamler, and Xiaoxiang Zhu. 2021. A Survey of Uncertainty in Deep Neural Networks. _ArXiv_ abs/1207.03342 (2021).
* Grbovic and Cheng (2018) Mikhagi Grbovic and Habib Cheng. 2018. Real-Time Personalization Using Embeddings for Search Ranking at Airbnb. In _Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining_ (London, United Kingdom) _(KDD '18)_. Association for Computing Machinery, New York, NY, USA, 311-320. [https://doi.org/10.1145/125191839.25192858](https://doi.org/10.1145/125191839.25192858)
* Holubsky et al. (2019) Neil Holubsky, Andrei Girgio, Stanislav Jaremba, Bruna Morrone, Quentin de Larousville, Andrei Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-Efficient Transfer Learning for NLP. In _International Conference on Machine Learning_.
* Huang et al. (2013) Poe-Ben Huang, Xiaodong He, Fang Gao, Li Deng, Alex Aco, and Larry Heck. 2013. Learning Deep Structured Semantic Models for Web Search Using Clickthrough Data. In _Proceedings of the 22nd ACM International Conference on Information & Knowledge Management_ (San Francisco, California, USA), _(ICSM '13)_. Association for Computing Machinery, New York, NY, USA, 2333-2338. [https://doi.org/10.1145/2505515.2505665](https://doi.org/10.1145/2505515.2505665)
* Jiang et al. (2016) Audun Jiang, Qiqiu Xie, Qiweiqiqi, Weiqiqi, N. J. Springer.
* Maul et al. (2019) Rabesh Karimi Maul, Sebastian Ruder, Mostafa Dehghani, and James Henderson. 2019. Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks. In _Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)_. Association for Computational Linguistics, Online, 565-576. [https://doi.org/10.1086/35/12021.acl-long.47](https://doi.org/10.1086/35/12021.acl-long.47)
* Karpathy and Fei-Fei (2014) Andrei Karpathy and Li Fei-Fei. 2014. Deep visual-semantic alignments for generating image descriptions. _2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)_ (2014), 3128-3137. [https://tapj.semanticsholar.org/Coupts2015NT0670](https://tapj.semanticsholar.org/Coupts2015NT0670)
* Krishna et al. (2016) Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yamins Kalantidis, Li-13 Li, David An, A. Shamma, Michael S. Bernstein, and Li Fei-Fei. 2016. Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations. _International Journal of Computer Vision_ 123 (2016), 32-37.
* Lakshminarayanan et al. (2017) Babi Lakshminarayanan, Alexander Pritzel, and Charles Blundell. 2017. Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles. In _NIPS_.
* Li et al. (2019) Chenyi Li, Shudling Ji, and Zhao Li. 2019. TSSA: A Time Site Self-Attention Approach for Modeling Sequential Text Behaviors. In _The World Wide Web Conference_ (San Francisco, CA, USA) (WWW '19). Association for Computing Machinery, New York, NY, USA, 2964-2970. [https://doi.org/10.1145/3308558.3313495](https://doi.org/10.1145/3308558.3313495)
* Li et al. (2022) Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. 2022. BLIP: Bootstrapping Language-Image Pro-training for Unified Vision-Language Understanding and Generation. In _ICML_.
* Lampostieth et al. (2021) Junnan Li, Ramprasanh R Selvaraju, Athikesh Dehghani, Shafiq R., Jorsy Caiming Xiong, and Steven C. H. Hoi. 2021. Align before Fuse: Vision and Language Representation Learning with Momentum Distillation. In _Neural Information Processing Systems_.
* Lizunta Indi et al. (2020) Luizunta Indi Ltdi, Alar Kustkar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2020. VisuableRT: A Simple and Performant Baseline for Vision and Language. _ArXiv_ abs/1908.03557 (2019).
* Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Dave Ramanan, Piotr Dollar, and C. Lawrence Zitnick. 2014. Microsoft COCO: Common Objects in Context. In _ECCV_.
* Loshchilov and Hunter (2017) Ilya Loshchilov and Frank Hunter. 2017. Fixing Weight Decay Regularization in Adam. _ArXiv_ abs/1711.05101 (2017).
* Lu et al. (2019) Jiasu Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. VLBERT: Pretraining Style-Agnostic Visionistic Representations for Vision-and-Language Tasks. In _NeurIPS_.
* Naphava et al. (2021) Jakub Naphava, Martin Popel, Milan Straka, and Jana Strukov'a. 2021. Understanding Model Robustness to User-generated Noisy Texts. _ArXiv_ abs/2110.07428 (2021).
* Qi et al. (2020) Di Qi, Lin Su, Jianwei Song, Edward Cui, Taron Bharti, and Arun Sacheti. 2020. ImageNet: Cross-modal free-training with Large-scale Weak-supervised Zero-Text Data. _ArXiv_ abs/2001.07966 (2020).
* Qiu et al. (2020) Jiejin Qiu, Yi Zhu, Xingjian Shi, F Wenzel, Zhiqiang Tang, D. Zhao, Bo Li, and Ma Li. 2020. Are Multimodal Models Robust to Image and Text Perturbations? _ArXiv_ abs/2012.08044 (2020). [https://tapj.semanticsholar.org/Coupts2015-25468009](https://tapj.semanticsholar.org/Coupts2015-25468009)
* Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Gou, Sandhuil Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Chuck, Grotchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. In _ICML_.
* Schuhmann et al. (2018) Christoph Schuhmann, Richard Veena, Remain Beaumont, Robert Kacmarczyk, Pietro Voltika, Amshi Katra, Theo Comboas, Jeniuc, and Aaron Kucunawaki. 2018. LAON-4000-000. Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs: A mW-2111.01124 (2012).
* Sensoy et al. (2018) M Sensoy, Mehdi Kandemir, and Lance M. Kaplan. 2018. Evidential Deep Learning to Quantify Classification Uncertainty. _ArXiv_ abs/1806.01768 (2018).
* Su et al. (2020) Weihei Su, Xizhou Zhu, Yue Cao, Bin Li, Leval Lu, Furu Wei, and Jifeng Dai. 2020. VLBERT: Pre-training of Generic Visual-Linguistic Representations. _ArXiv_ abs/1908.08503 (2020).
* Tang et al. (2010) Jiaxi Tang, Francois Belletti, Sgar Jain, Minmin Chen, Alex Beutel, Can Xu, and Ed H. Chi. 2019. Towards Neural Mixture Recommender for Long Range Deep User Sequences. _The World Wide Web Conference_ (2019). [https://tapj.semanticsholar.org/Coupts2015/6785901](https://tapj.semanticsholar.org/Coupts2015/6785901)
* Uhzer (2021) Dennis Uhzer. 2021. A survey on evisonal deep learning for single-pass uncertainty estimation. _arXiv preprint arXiv:2110.03051_ (2021).
* Wang et al. (2018) Jing-Hong Yang, Carlos Lissauer, Samuel Sanguo de Rezende, Krishna Srinivasan, Miriam Redi, Stephane Chintam, and Jianxen. 2018. AToMi: An Image/Text Retrieval Test Collection to Support Multimedia Content Creation. _Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval_ (2018). [https://api.scannictecholar.org/Coupts2015/257921318](https://api.scannictecholar.org/Coupts2015/257921318)
* Yao et al. (2022) Lai Yao, Wei Chen, and Qin Jin. 2022. CapExitech: Enriching Caption Semantics for Web Images via Cross-modal Pre-trained Knowledge. _Proceedings of the 2011 ACM Web Conference 2022_ (2022). [https://api.scannictecholar.org/Coupts2015/253581370](https://api.scannictecholar.org/Coupts2015/253581370)
* Young et al. (2014) Peter Young, Alice Lai, Michal Hodsi, and Julia Hockenmaier. 2014. From image descriptions to visual conditions: New similarity metrics for semantic inference over event descriptions. _Transactions of the Association for Computational Linguistics_ 2 (2014), 67-78. [https://doi.org/10.162/act.0166](https://doi.org/10.162/act.0166)
* Zhao et al. (2020) Xujiang Zhao, Feng Chen, Shu Hu, and Jin-Hee Cho. 2020. Uncertainty aware semi-supervised learning on graph data. _Advances in Neural Information Processing Systems_ 35 (2020), 12827-1286.
* Zhang et al. (2022) Kafifu Zhang, Li Wang, Li, Xuxong Chen, Hu, Liq Jin, Ling, Xiuie Zhou, Chang-Ping Ren, Zhaqiang Liang, and Jingbing Shao. 2022. Implicit User Awareness Modeling via Candidate Items for CTR Prediction in Search Ads. In _Proceedings of the ACM Web Conference 2022_ (Virtual Event, Lyon, France) _(WWW '22)_. Association for Computing Machinery, New York, NY, USA, 246-255. [https://doi.org/10.1145/34484.3513595](https://doi.org/10.1145/34484.3513595)
* Zhou et al. (2020) Lzhou Zhou, Hamid Palanis, Lei Zhang, Houdong Hu, Jason J. Corso, and Jandeng Gao. 2020. Unified Vision-Language Pre-Training for Image Captioning and VQA. _ArXiv_ abs/1909.11059 (2020).
* Zhou et al. (2020)