# Robust Deep Signed Graph Clustering via Weak Balance Theory

Anonymous Author(s)

###### Abstract.

Signed graph clustering is a critical technique for discovering community structures in graphs that exhibit both positive and negative relationships. We have identified two significant challenges in this domain: i) existing signed spectral methods are highly vulnerable to noise, which is prevalent in real-world scenarios; ii) the guiding principle "an enemy of my enemy is my friend", rooted in _Social Balance Theory_, often narrows or disrupts cluster boundaries in mainstream signed graph neural networks. Addressing these challenges, we propose the Deep Signed Graph Clustering framework (DSGC), which leverages _Weak Balance Theory_ to enhance preprocessing and encoding for robust representation learning. First, DSGC introduces Violation Sign-Refine to denoise the signed network by correcting noisy edges with high-order neighbor information. Subsequently, Density-based Augmentation enhances semantic structures by adding positive edges within clusters and negative edges across clusters, following _Weak Balance_ principles. The framework then utilizes _Weak Balance_ principles to develop clustering-oriented signed neural networks to broaden cluster boundaries by emphasizing distinctions between negatively linked nodes. Finally, DSGC optimizes clustering assignments by minimizing a regularized clustering loss. Comprehensive experiments on synthetic and real-world datasets demonstrate DSGC consistently outperforms all baselines, establishing a new benchmark in signed graph clustering. The code is provided in [https://anonymous.4open.science/r/DSGC-C05C/](https://anonymous.4open.science/r/DSGC-C05C/).

Representation learning; Balance theory; Signed graph clustering +
Footnote †: journalyear: 2018

**ACM Reference Format:**

Anonymous Author(s). 2018. Robust Deep Signed Graph Clustering via Weak Balance Theory. In _Proceedings of conference title (Conference acronym XX)_. ACM, New York, NY, USA, 13 pages. [https://doi.org/XX](https://doi.org/XX)

## 1. Introduction

Deep graph clustering has emerged as a pivotal technique for uncovering underlying communities within complex networks. However, existing methods (Bishop, 2006; Kipf and Welling, 2016; Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017) predominantly target unsigned graphs, which represent relationships solely with "non-negative" edges and inherently fail to capture conflicting node interactions, such as friendship versus enmity, trust versus distrust, and approval versus denouncement. Such dynamics are commonplace in social networks and can be effectively modeled by signed graphs that incorporate both positive and negative edges. Although significant work has studied link prediction tasks in deep signed graphs (Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017), deep signed graph clustering remains substantially unexplored. In this paper, we aim to develop a deep signed graph clustering method that enhances the robustness of graph representations, facilitating more distinctive clusters and better reflecting the intricate relationships within signed graphs.

Signed graph clustering is broadly applied in the analysis of social psychology (Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017), biologic gene expressions (Song et al., 2018; Hamilton et al., 2017), etc. Recent studies have predominantly focused on spectral methods (Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017), which design various Laplacian matrices specific to a given network to derive node embeddings, aiming to find a partition of nodes that maximizes positive edges within clusters and negative edges between clusters. However, these methods are vulnerable to random noise, a common challenge in real-world scenarios. For instance, on shopping websites, the signed graph encoding user-product preferences often includes noisy edges, typically when customers unwillingly give positive ratings to items in exchange for meager rewards or coupons. Fig. 1 illustrates the significant impact of noise on signed spectral methods like BNC and SPONCE (Hamilton et al., 2017; Hamilton et al., 2017). As perturbation ratios increase, which indicates a higher percentage of randomly flipped edge signs or inserted negative edges in a synthetic signed graph with five clusters, these methods suffer a sharp decline in clustering accuracy. Therefore, denoising the graph structure is essential to enhance robust representation learning in deep signed clustering.

Furthermore, the investigation on deep signed graph neural networks (SGNNs) reveals that existing SGNNs -- which are mostly developed for link prediction (Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017; Hamilton et al., 2017) -- do not adapt well to signed clustering. Specifically, mainstream SGNNs models typically leverage principles from the well-established _Social Balance Theory_(Hamilton et al., 2017) (or Balance Theory) to design their messaging-passing aggregation mechanisms, including the classical principle "_an Enemy of my enemy is my Friend (EEF)_", "_a Friend of my Friend is my Friend (FFF)_", "_an Enemy of my Friend is my Enemy (EEF)_". However, "_EEF_" implies an assumption that a given signed network has only \(2\) clusters, which is not directly applied to signed graphs with \(K\) (\(K>2\)) clusters. Specifically, as illustrated in Fig. 2, "_EEF_"

Figure 1. Effects of different perturbations, including flipping signs and randomly adding negative edges, on the clustering performance of popular spectral methods in signed graphs.

can narrow cluster boundaries, leading to more nodes being located at the margins of clusters, which makes it difficult to assign them to the correct clusters and thus results in poor performance. For example, node \(v_{i}\) aggregates its positive neighbor \(v_{2}\) (recognized by "_EEF_" but inconsistent to the real semantic relationship in clusters), which causes its positive representation \(Z_{i}^{+}\) mapped closer to the cluster of node \(v_{2}\), thus leading to narrowed or even overlapped cluster boundaries. In contrast, _Weak Balance Theory_(Cheng et al., 2017) (or Weak Balance), introducing a new principle, "_an enemy of my enemy migh be my friend or enemy_", can generalize Balance Theory to \(K\)-way (\(K>2\)) clustering situation but remains underexplored.

To address these challenges, we propose eep Signed Graph Clustering (DSGC) for \(K\)-way clustering, designed to enhance representations' robustness against noisy edges and reduce the impact of the ill-suited principle on cluster boundaries. DSGC first introduces the Signed Graph Rewiring module (SGR) in the preprocessing stage for denoising and graph structure augmentation. SGR provides two rewiring strategies, including _Violation Sign-Refine_, which can identify and correct noisy edges with long-range neighbor relationships, and _Density-based Augmentation_, which follows Weak Balance principles to insert new positive edges to increase positive density within clusters and negative edges to increase negative density across clusters. Such refined graph topology can promote signed encoders to enhance the robustness of node representations. DSGC then constructs a clustering-oriented signed neural network that utilizes Weak Balance. This helps design clustering-specific neighbor aggregation mechanism for enhancing the discrimination among node representations, specifically for nodes with negative edges to widen cluster boundaries. Finally, DSGC designs a \(K\)-way clustering predictor that optimizes a non-linear transformation function to learn clustering assignments. This framework is designed to refine the clustering process by correcting noisy edges and enhancing the discriminative capability of node representations, ultimately leading to more accurate clustering outcomes.

Overall, our major contributions are as follows:

* We develop DSGC, the first Deep Signed Graph Clustering framework, by leveraging Weak Balance Theory.
* We design two graph rewiring strategies to denoise and augment the overall network topology.
* We propose a task-oriented signed graph encoder to learn more discriminative representations, particularly for nodes connected by negative edges.
* Extensive experiments on synthetic and real-world datasets demonstrate the superiority and robustness of DSGC.

## 2. Related Work

In this section, we succinctly review existing studies for signed graph neural networks and signed graph clustering.

**Signed Graph Neural Networks (SGNNs)**, which maps nodes within a signed graph to a low-dimensional latent space, has increasingly facilitated a variety of signed graph analytical tasks, including node classification (Shen et al., 2017), signed link prediction (Shen et al., 2017; Wang et al., 2018; Wang et al., 2019), node ranking (Shen et al., 2017; Wang et al., 2019), and signed clustering (Bahdanau et al., 2014; Chen et al., 2015; Wang et al., 2019; Wang et al., 2019). Most works of signed graph center around integrating _Social Balance Theory_ to signed convolutions into Graph Neural Networks (GNNs). As the pioneering work, SGCN (Gershtein et al., 2017) adapts unsigned GNNs for signed graphs by aggregating and propagating neighbor information with Balance Theory. Thereafter, other work has integrated additional social-psychological theories. (Bahdanau et al., 2014) appends the status theory, which is applicable to directed signed networks, interpreting positive or negative signs as indicators of relative status between nodes. SiGATs (Shen et al., 2017), which extends Graph Attention Networks (GATs) to signed networks, also utilizes these two signed graph theories to derive graph motifs for more effective message passing. SiNets (Shen et al., 2017) proposes a signed network embedding framework guided by the extended structural balance theory. SGDNET (Shen et al., 2017) leverages a random walk technique specifically tailored for signed graphs, effectively diffusing hidden node features in line with Social Balance Theory. GS-GNN (Shen et al., 2017) applies a dual GNN architecture that combines a prototype-based GNN to process positive and negative edges to learn node representations. SLGNN (Shen et al., 2017) especially design low-pass and high-pass graph convolution filters to capture both low-frequency and high-frequency information from positive and negative links.

**Signed Graph Clustering.** The study of signed graph clustering has its roots in Social Balance Theory (Bahdanau et al., 2014), which is equivalent to the 2-way partition problem in signed graphs (Shen et al., 2017). Building upon this foundational concept, (Shen et al., 2017) propose a signed spectral clustering method that utilizes the signed graph Laplacian and graph kernels to address the 2-way partition problem. However, (Wang et al., 2019) argues that community detection in signed graphs is equivalent to identifying \(K\)-way clusters using an agent-based heuristic. The _Weak Balance Theory_(Cheng et al., 2017) relaxes balance theory to enable \(K\)-way clustering. Following (Shen et al., 2017), (Bahdanau et al., 2014) proposed the "Balanced Normalized Cut (BNC)" for \(K\)-way clustering, aiming to find an optimal clustering assignment that minimizes positive edges between different clusters and negative edges within clusters with equal priority. SPONGE (Chen et al., 2015) transforms this discrete NP-hard problem into a continuous generalized eigenproblem and employs LOBPCG (Shen et al., 2017), a preconditioned eigensolver, to solve large positive definite generalized eigenproblems. In contrast to the above \(K\)-way complete partitioning, (Shen et al., 2017) targets detecting \(K\) conflicting groups in a signed network, allowing other nodes to be neutral regarding the conflict structure in search. This conflicting-group detection problem can be characterized as

Figure 2. Illustration of “_an Enemy of my Enemy is my Friend (EEF)_” narrowing cluster boundaries. Aggregating positive (/ negative) neighbor 2 (/ 3) causes \(Z_{i}^{+}\) (/ \(Z_{i}^{+}\)) mapped far from its clusters or even cross the boundary, where positive (/ negative) neighbors 2 (/ 3) are defined by “EEF”.

the maximum discrete Rayleigh's quotient problem and solved by two spectral methods.

While GNNs have been extensively applied to unsigned graph clustering (Gan et al., 2017; Li et al., 2018; Wang et al., 2019; Wang et al., 2019; Wang et al., 2019), their adoption in signed graph clustering remains overlooked. A notable exception is the Semi-Supervised Signed NETwork Clustering (SSSNET) (Gan et al., 2017), which simultaneously learns node embeddings and cluster assignments by minimizing the clustering loss and a Cross-Entropy classification loss. In contrast, our work develops an unsupervised method for signed graph clustering, eliminating the reliance on ground truth labels.

## 3. Preliminaries

### Notations

We denote an undirected signed graph as \(\mathcal{G}=\{\mathcal{V},\mathcal{E},\mathbf{X}\}\), where \(\mathcal{V}=\{v_{1},v_{2},\ldots,v_{n}\}\) is the set of nodes, \(\mathcal{E}\) is the set of edges, and \(\mathbf{X}\in\mathbb{R}^{|\mathcal{V}|\times\mathcal{A}_{0}}\) is the \(d_{0}\)-dimensional node attributes. Each edge \(e_{ij}\in\mathcal{E}\) between \(v_{i}\) and \(v_{j}\) can be either positive or negative, but not both. A is the adjacency matrix of \(\mathcal{G}\), where \(\mathbf{A}_{ij}=1\) if \(v_{i}\) has a positive link to \(v_{j}\); \(\mathbf{A}_{ij}=-1\) if \(v_{i}\) has a negative link to \(v_{j}\); \(\mathbf{A}_{ij}=0\) otherwise. The signed graph is conceptually divided into two subgraphs sharing the common vertex set \(\mathcal{V}\): \(\mathcal{G}=\{\mathcal{G}^{+},\mathcal{G}^{-}\}\), where \(\mathcal{G}^{+}=\{\mathcal{V},\mathcal{E}^{+}\}\) and \(\mathcal{G}^{-}=\{\mathcal{V},\mathcal{E}^{-}\}\) contain all positive and negative edges, respectively. Let \(\mathbf{A}^{+}\) and \(\mathbf{A}^{-}\) be the adjacency matrices of \(\mathcal{G}^{+}\) and \(\mathcal{G}^{-}\) with \(\mathbf{A}=\mathbf{A}^{+}-\mathbf{A}^{-}\), where \(\mathbf{A}^{+}_{ij}=max(\mathbf{A}_{ij},0)\) and \(\mathbf{A}^{-}_{ij}=min(\mathbf{A}_{ij},0)\).

### Relaxation of Social Balance

Balance and Weak Balance Theories, essential for signed graph clustering, are briefly explained here; more details are in Appx. A.

**Balance Theory**(Gan et al., 2017) consists of four fundamental principles: "_the friend of my friend is my friend_", "_the enemy of my friend is my enemy_", "_the friend of my enemy is my enemy_", and "_the enemy of my enemy is my friend (EEF)_". A signed network is balanced if it does not violate these principles. Theoretically, the Balance Theory is equivalent to 2-way clustering on graphs (Gan et al., 2017).

**Weak Balance Theory**(Gan et al., 2017) relaxes Balance Theory to accommodate \(K\)-way clustering, by replacing the "_EEF_" principle with "_the enemy of my enemy might be my enemy (EEE)_". This principle allows nodes in a triangle to belong to three different clusters, e.g., the blue triangle in Fig. 10 (b), thus relaxing Social Balance Theory. The partition \(\{\mathcal{C}_{1},\ldots,\mathcal{C}_{K}\}\) of a signed graph \(\mathcal{G}\) satisfying either theory can be uniformly defined as the following conditions:

\[\begin{cases}\mathbf{A}_{ij}>0&(e_{ij}\in\mathcal{E})\cap(v_{i}\in\mathcal{C} _{k})\cap(v_{j}\in\mathcal{C}_{k})\\ \mathbf{A}_{ij}<0&(e_{ij}\in\mathcal{E})\cap(v_{i}\in\mathcal{C}_{k})\cap(v_{j }\in\mathcal{C}_{l})(k\neq l)\end{cases}, \tag{1}\]

where \(\mathbf{A}_{ij}\) is the weight of edge \(e_{ij}\) and \(0<k,l<K\).

### Problem Definition

This paper aims to leverage the capabilities of deep representation learning to enhance robust graph signed clustering. Unsupervised **Deep Signed Graph Clustering** is formally defined below.

**Problem 1**.: _Given a signed graph \(\mathcal{G}=\{\mathcal{V},\mathcal{E},\mathbf{X}\}\), deep signed graph clustering is to train a function \(f(\mathbf{A},\mathbf{X})\longrightarrow\mathbf{Z}\) that transforms each node \(v\in\mathcal{V}\) into a low-dimensional vectors \(\mathbf{Z}_{v}\in\mathbb{R}^{d}\). It aims to optimize a partition to divide all nodes \(\{\mathcal{I}_{i}\}_{i=1}^{|\mathcal{V}|}\) into \(K\) disjoint clusters \(\mathcal{V}=\mathcal{C}_{1}\cup\cdots\cup\mathcal{C}_{K}\), by minimizing a signed clustering loss objection that makes as many as positive edges exist within clusters and as many as negative edges exist across clusters._

## 4. Methodology

As illustrated in Fig. 3, DSGC consists of 4 major components, including Violation Sign-Refine and Density-based Augmentation for graph rewiring, signed clustering encoder, and cluster assignment.

### Signed Graph Rewiring

In real-world signed graphs, noisy edges(violations)--negative edges within clusters and positive edges across clusters--can disrupt ideal clustering structures. To address this, we propose two graph rewiring methods to enhance clustering integrity: Violation Sign-Refine (VS-R), which corrects the signs of violated edges to align negative and positive edges with the expected inter-cluster and intra-cluster relationships; and Density-based Augmentation (DA), which adds new edges based on long-range interaction patterns to reinforce message passing. Both methods leverage Weak Balance and are used as preprocessing steps to denoise and augment the initial graph topology--specifically, the message-passing matrix.

#### 4.1.1. Violation Sign-Refine

To address noisy edges, we utilize high-order neighbor interactions to correct their signs. Based on Weak Balance, we first adapt the definitions of positive and negative walks for \(K\)-way clustering. Following Social Balance Theory, (Gan et al., 2017) defines a positive walk as one containing an even number of negative edges and a negative walk as one containing an odd number of negative edges. However, they are not suitable for \(K\)-way clustering due to the uncertainty brought by the "_the enemy of my enemy might be my enemy or my friend_" principle of Weak Balance. We formally redefine positive and negative walks as follows.

**Definition 1**.: _A walk of length \(l\in\mathbb{N}^{+}\) connecting nodes \(v_{l}\) and \(v_{j}\) is positive if all its edges are positive; it is negative when it contains exactly one negative edge and all other edges are positive._

Since violations are sparse in graphs, we assume that leveraging higher-order information from longer-range neighbors helps revise the signs of violated edges. Lemma 1 specifies the non-noise score between \(v_{i}\) and \(v_{j}\) w.r.t. the \(l\)-length positive and negative walks.

**Lemma 1**.: _For \(v_{i}\), \(v_{j}\in\mathcal{V}\) in a signed graph \(\mathcal{G}=(\mathcal{V},\mathcal{E},\mathbf{X})\), let \(\mu^{+}_{l}(i,j)\) and \(\mu^{-}_{l}(i,j)\) be the number of positive and negative walks with length \(l\) connecting \(v_{i}\) and \(v_{j}\), respectively. Then, \(\forall a\in\mathbb{N}\),_

\[\mu^{+}_{l}(i,j)-\mu^{-}_{l}(i,j)=(\mathbf{A}^{+})^{l}_{ij}-\sum_{a=0}^{l-1}(( \mathbf{A}^{+})^{a}\mathbf{A}^{-}(\mathbf{A}^{+})^{l-1-a})_{ij}. \tag{2}\]

If we consider all walks up to length \(L^{\prime}\), the non-noise score of the connection between \(v_{i}\) and \(v_{j}\) can be defined:

\[\Gamma_{ij}(L^{\prime})=\sum_{l=1}^{L^{\prime}}a_{l}(\mu^{+}_{l}(i,j)-\mu^{-}_ {l}(i,j)), \tag{3}\]

where \(\alpha_{l}=\begin{cases}1,&l=1\\ 1/l(l),&1<l<L^{\prime}\\ 1-\sum_{l^{\prime}=1}^{L^{\prime}-1}1/(l^{\prime}),&l=L^{\prime}\end{cases}.\)Here, \(a_{l}\) decreases with \(l\), indicating that shorter walks have more influence. \(\Gamma\) is utilized to correct violations as \(\Gamma_{lj}\) extracts high-order information from neighbors of \(v_{l}\) and \(v_{j}\) within \(L^{\prime}\)-hop. With \(\Gamma_{lj}\), we obtain a refined adjacency matrix \(\hat{A}\) via the following rules:

\[\hat{\mathbf{A}}_{lj}=\begin{cases}1,&\Gamma_{lj}>\delta^{+}\\ \mathbf{A}_{ij},&\delta^{-}\leq\Gamma_{lj}\leq\delta^{+}\\ -1,&\Gamma_{lj}<\delta^{-}\end{cases}, \tag{5}\]

where \(\delta^{+}>0\) and \(\delta^{-}<0\) are two thresholds. \(v_{l}\) and \(v_{j}\) are considered _effective friends_ when \(\Gamma_{lj}>\delta^{+}\), indicating a positive edge (+); \(v_{l}\) and \(v_{j}\) are considered _effective enemies_ when \(\Gamma_{lj}<\delta^{-}\), indicating a negative edge (-); otherwise, the original adjacency entries in \(\mathbf{A}\) retains. The magnitude \(\left|\Gamma_{lj}\right|\) represents the confidence level of two nodes being _effective friends_ or _enemies_. A larger (resp. smaller) \(\left|\Gamma_{lj}\right|\) represents a stronger (resp. weaker) positive or negative relationship between \(v_{l}\) and \(v_{j}\). This method refines the adjacency matrix by reinforcing accurate relational signals and reducing the impact of noisy edges, thereby facilitating more effective clustering.

#### 4.1.2. Density-based Augmentation

Following the noise corrections made by VS-R, the revised graph, denoted as \(\hat{\mathcal{G}}=\{\mathcal{V},\hat{\mathcal{E}},\mathbf{X}\}\), is processed through Density-based Augmentation to increase the density of positive edges within clusters and negative edges between clusters. The revised adjacency matrices for positive and negative edges, \(\hat{\mathbf{A}}^{+}\) and \(\hat{\mathbf{A}}^{-}\), are augmented as below:

\[\mathbf{A}^{\prime+}=(\hat{\mathbf{A}}^{+})^{m^{+}};\quad\mathbf{A}^{\prime-}= \sum_{a=0}^{m^{-}}(\hat{\mathbf{A}}^{+})^{a}\hat{\mathbf{A}}^{-}(\hat{\mathbf{ A}}^{+})^{m^{-}-a}, \tag{6}\]

where \(m^{+}\) and \(m^{-}\) are scalar hyper-parameters indicating the extent of augmentation. The augmented adjacency matrices are:

\[\mathbf{A}^{\prime\prime+}=\begin{cases}1,&\mathbf{A}^{\prime+}_{ij}>0,\;i\neq j \\ 0,&\mathbf{A}^{\prime+}_{ij}=0,\;i\neq j\end{cases};\quad\mathbf{A}^{\prime \prime-}=\begin{cases}1,&\mathbf{A}^{\prime-}_{ij}>0,\;i\neq j\\ 0,&\mathbf{A}^{\prime-}_{ij}=0,\;i\neq j\\ 0,&\mathbf{A}^{\prime+}_{ij},\;i=j\end{cases}. \tag{7}\]

If \(m^{+}=1\) (resp. \(m^{-}=0\)), no augmentation is performed on \(\hat{\mathbf{A}}^{+}\) (resp. \(\hat{\mathbf{A}}^{-}\)). For \(m^{+}>1\), it adds a positive edge between any two nodes connected by a \(m^{+}\)-length positive walk (Dfn. 1). For \(m^{-}>0\), it adds a negative edge between any two nodes connected by a \((m^{-}+1)\)-length negative walk. This strategy effectively enhances the clustering potential by reinforcing intra-cluster connectivity with positive edges and inter-cluster separations with negative edges. It is particularly effective for a signed graph with few violations.

### Signed Clustering Encoder

Signed Graph Convolution Network, our signed graph encoder in DSGC, is tailored for \(K\)-way clustering. It leverages Weak Balance Theory principles to learn discriminative node representations that signify greater separation between nodes connected by negative edges and closer proximity between those by positive edges 1.

Footnote 1: This goal is often reflected in the loss function designs in previous work [9, 16, 23, 24, 25] for link prediction. However, its importance has been overlooked in signed encoders.

Based on the rewired graph topology defined by \(\mathbf{A}^{\prime\prime+}\) and \(\mathbf{A}^{\prime\prime-}\), we first introduce self-loops to each node using \(\hat{\mathbf{A}}^{+}=\mathbf{A}^{\prime\prime+}+e^{+}\mathbf{1}\), \(\hat{\mathbf{A}}^{-}=\mathbf{A}^{\prime\prime-}+e^{-}\mathbf{1}\), where \(\mathbf{1}\) is the identity matrix and \(e^{+}\) and \(e^{-}\) are the balance hyperparameters. The adjacency matrices are then normalized as follow: \(\hat{\mathbf{A}}^{+}=(\hat{\mathbf{D}}^{+})^{-1}\hat{\mathbf{A}}^{+}\) and \(\hat{\mathbf{A}}^{-}=(\hat{\mathbf{D}}^{-})^{-1}\hat{\mathbf{A}}^{-}\), where \(\hat{\mathbf{D}}^{+}\) and \(\hat{\mathbf{D}}^{-}\) are diagonal degree matrices with \(\hat{\mathbf{D}}^{+}_{ii}=\sum_{j}\hat{\mathbf{A}}^{+}_{ij}\) and \(\hat{\mathbf{D}}^{-}_{ii}=\sum_{j}\hat{\mathbf{A}}^{-}_{ij}\). We learn \(d\)-dimensional positive and negative embeddings, \(\mathbf{Z}^{+}_{i}\) and \(\mathbf{Z}^{-}_{i}\), for each node \(v_{l}\in\mathcal{V}\), and concatenate them as the final node representation: \(\mathbf{Z}_{i}=\text{CONCAT}(\mathbf{Z}^{+}_{i},\mathbf{Z}^{-}_{i})\), where \(\mathbf{Z}^{+}_{i}\in\mathbb{R}^{1\times d}\) and \(\mathbf{Z}^{-}_{i}\in\mathbb{R}^{1\times d}\) are computed through layers of our signed graph convolution network:

\[\mathbf{Z}^{+}_{i}=\sum_{l=0}^{L}\omega^{+(I)}\mathbf{Z}^{+(I)}_{i},\;\mathbf{ Z}^{-}_{i}=\sum_{l=0}^{L}\omega^{-(I)}\mathbf{Z}^{-(I)}_{i}. \tag{8}\]

\(\omega^{+(I)}\) and \(\omega^{-(I)}\), shared by all nodes, are layer-specific trainable weights that modulate the contribution of different convolution layers to the final node representation. \(L\) is the number of layers

Figure 3. The overall framework of DSGC. The Violation Sign-Refine first computes non-noise scores to correct the signs of noisy edges. Then, the Density-based Augmentation adds positive edges within clusters and negative edges across clusters. These two rewiring methods generate a new adjacency matrix with reduced noise and enhanced semantic structures. Thereafter, clustering-specific signed convolutional networks can be trained by minimizing the differential clustering loss for learning and strengthening the discrimination among node representations linked negatively.

in the neural network. This design allows the encoder to leverage information from different neighborhood ranges. The intermediate representations of all nodes, \(\mathbf{Z}^{\ell(I)}\in\mathbb{R}^{|\mathcal{V}|\times d}\) and \(\mathbf{Z}^{-(I)}\in\mathbb{R}^{|\mathcal{V}|\times d}\), can be obtained as

(9) \[\mathbf{Z}^{\star(I)} =(\mathbf{A}^{\star})^{I}\mathbf{Z}^{\star(0)},\] \[\mathbf{Z}^{-(I)} =\sum_{b=0}^{I-1}(\bar{\mathbf{A}}^{\star})^{b}(-\bar{\mathbf{A}} ^{\star})^{I-1-b}\mathbf{Z}^{-(0)},\] (10)

where the superscript \((I)\) and \(I\) denote the layer index and power number, respectively. The initial node embeddings, \(\mathbf{Z}^{+(0)}\in\mathbb{R}^{|\mathcal{V}|\times d}\) and \(\mathbf{Z}^{-(0)}\in\mathbb{R}^{|\mathcal{V}|\times d}\), are derived from the input feature matrix \(\mathbf{X}\in\mathbb{R}^{|\mathcal{V}|\times d_{b}}\) by two graph-agnostic non-linear networks:

\[\mathbf{Z}^{+(0)} =\mathbf{W}_{1}^{\star}(\sigma(\mathbf{X}\mathbf{W}_{0}^{+})), \tag{12}\] \[\mathbf{Z}^{-(0)} =\mathbf{W}_{1}^{\star}(\sigma(\mathbf{X}\mathbf{W}_{0}^{-})), \tag{11}\]

where \(\sigma\) is the \(ReLU\) activation function. \(\mathbf{W}_{0}^{\star}\in\mathbb{R}^{d_{b}\times d}\) and \(\mathbf{W}_{1}^{\star}\in\mathbb{R}^{d\times d}\) are the trainable parameters of the positive network; \(\mathbf{W}_{0}^{\star}\in\mathbb{R}^{d_{b}\times d}\) and \(\mathbf{W}_{1}^{\star}\in\mathbb{R}^{d\times d}\) are that of the negative network. We claim that the positive aggregation function, Eq. (9), can pull the nodes linked by positive walks, thus reducing the intra-cluster variances. Meanwhile, the negative aggregation function, Eq. 10, can push nodes linked by negative walks, thus increasing the inter-cluster variances. We also investigate Weak Balance principles implied in Eq. (9) and Eq. (10), as well as the effect of the minus sign "-" in the term (\(-\bar{\mathbf{A}}^{\star}\)) to nodes representations linked negatively and the clustering boundary in App. B.

### \(K\)-way Signed Graph Clustering

With node representations \(\mathbf{Z}\in\mathbb{R}^{|\mathcal{V}|\times 2d}\) learned in our encoder, we propose a non-linear transformation to predict clusters.

**Clustering Assignment.** Considering a \(K\)-way clustering problem, where \(K\) is the number of clusters, node \(o_{i}\) is assigned a probabilities vector \(\Pi_{i}=[\pi_{i}(1),\ldots,\pi_{i}(K)]\), representing the likelihood of belonging to each cluster. \(k\in\{1,\ldots,K\}\) denotes the index of a cluster and \(\sum_{k=1}^{K}\pi_{i}(k)=1\). This probability is computed using a learnable transformation followed by a softmax operation:

\[\pi_{i}(k)=q_{\theta}(k|\mathbf{Z}_{i})=\frac{\exp(\mathbf{Z}_{i} \cdot\theta_{k})}{\sum_{k^{\prime}=1}^{K}\exp(\theta_{k^{\prime}}\cdot\mathbf{ Z}_{i^{\prime}})}, \tag{13}\]

where \(\theta_{k}\in\mathbb{R}^{d\times 1}\) is a parameter for cluster \(k\) to be trained by minimizing the signed clustering loss. The assignment vectors of all nodes \(\{\Pi_{i}\}_{i=1}^{|\mathcal{V}|}\) form an assignment matrix \(\Pi\in\mathbb{R}^{|\mathcal{V}|\times K}\).

**Differential Signed Clustering Loss.** Signed graph clustering aims to minimize violations, which was historically considered as an NP-Hard optimization problem (Kang and Yang, 2017) with designed discrete (non-differential) objectives in spectral methods. We transform it into a differentiable format by utilizing a soft assignment matrix \(\Pi\) in place of a hard assignment matrix \(\mathbf{C}\). Specifically, given that the cluster number \(K\) is known, let \(\mathbf{C}\in\{0,1\}^{|\mathcal{V}|\times K}\) be a hard cluster assignment matrix where \(\mathbf{C}_{(\cdot k)}(i)=1\) if node \(o_{i}\) belong to the cluster \(k\); otherwise \(\mathbf{C}_{(\cdot k)}(i)=0\). The number of positive edges between cluster \(k\) and other clusters can be captured by \(\mathbf{C}_{(\cdot k)}^{T}\mathbf{L}^{+}\mathbf{C}_{(\cdot k)}\) with the positive graph Laplacian \(\mathbf{L}^{+}=\mathbf{D}^{+}-\mathbf{A}^{+}\). The number of negative edges within cluster \(k\) can be measured by \(\mathbf{C}_{(\cdot k)}^{T}\mathbf{A}^{-}\mathbf{C}_{(\cdot,k)}\). So the violations w.r.t. cluster \(k\) can be measured by \(\mathbf{C}_{(\cdot k)}^{T}(\mathbf{L}^{+}+\mathbf{A}^{-})\mathbf{C}_{(\cdot,k)}\). By replacing the hard assignment \(\mathbf{C}_{(\cdot,k)}\) with the soft assignment probability \(\Pi_{(\cdot,k)}\), the differential clustering loss is constructed as:

\[\mathcal{L}=\frac{1}{|\mathcal{V}|}\sum_{k=1}^{K}\mathbf{\Pi}_{(\cdot,k)}^{T} (\mathbf{L}^{+}+\mathbf{A}^{-})\Pi_{(\cdot,k)}+\lambda\mathcal{L}_{r\text{gsu}}, \tag{14}\]

where \(\lambda\) is a hyperparameter, and \(\mathcal{L}_{r\text{gsu}}\) is a regularization term computing the degree volume in cluster to prevent model collapse:

\[\mathcal{L}_{r\text{gsu}}=-\frac{1}{|\mathcal{V}|}\sum_{k=1}^{K}\mathbf{\Pi}_{( \cdot,k)}^{T}\overline{\mathbf{\Theta}}\Pi_{(\cdot,k)}, \tag{15}\]

where \(\overline{\mathbf{\Theta}}\) is the degree matrix of \(\mathbf{A}\). Minimizing \(\mathcal{L}\) equals finding a partition with minimal violations. We iteratively optimize the signed encoder and non-linear transformation by minimizing \(\mathcal{L}\).

**Inference stage.** Each node \(v_{i}\in\mathcal{V}\) is assigned to the cluster with the highest probability in its vector \(\Pi_{i}\):

\[s_{i}=argmax_{\mathbf{x}_{i}}\Pi_{i}, \tag{16}\]

where \(s_{i}\in\{1,\ldots,K\}\) is the cluster index for \(v_{i}\). The set of all node cluster assignments, \(\{s_{i}\}_{i=1}^{|\mathcal{V}|}\), is used to evaluate the performance of the clustering approach.

## 5. Experiments

This section evaluates our DSGC model with both synthetic and real-world graphs to address the following research questions. **RQ1:** Can DSGC achieve state-of-the-art clustering performance on signed graphs without any labels? **RQ2:** How does each component contribute to the effectiveness of DSGC? **RQ3:** How does the Violation Sign-Refine (VS-R) impact signed topology structures by correcting noisy edges? **RQ4:** How do the strategies in our signed encoder, specifically abandoning the "\(EEF\)" principle and the minus sign in term (\(-\bar{\mathbf{A}}^{-}\)), contribute to forming wider clustering boundaries?

### Experimental Settings

#### 5.1.1. Datasets.

Follow SPONGE (Gueron et al., 2017), we evaluate DSGC with a variety of synthetic and real-world graphs: (i) **Synthetic SBM graphs.** The Signed Stochastic Block Model (SSBM) is commonly used to generate labeled signed graphs (Gueron et al., 2017; Wang et al., 2017), parameterized by \(N\) (number of nodes), \(K\) (number of clusters), \(p\) (edge probability or sparsity), and \(\eta\) (sign flip probability). This model first sets edges within the same cluster as positive, and edges between clusters as negative. It then models noises by randomly flipping the sign of each edge with probability \(\eta\in[0,1/2)\). Each generated graph can be represented as SSBM (\(N\),\(K\),\(p\),\(\eta\)). (ii) **Real-world graphs.** S&P is a stock correlation network from market excess returns during \(2003-2005\), consisting of \(1,193\) nodes, \(1,069,319\) positive edges, and \(353,930\) negative edges. Rainfall is a historical rainfall dataset from Australia, where edge weights are computed by the pairwise Pearson correlation. Rainfall is a complete signed graph with \(306\) nodes, \(64,408\) positive edges, and \(29,228\) negative edges.

#### 5.1.2. Baselines

DSGC is compared against 9 representative signed spectral clustering methods. Five are basic signed spectral methods utilizing various forms of median matrix: (1) symmetric adjacency matrix \(\mathbf{A}^{*}=\frac{1}{2}(\mathbf{A}+\mathbf{A}^{T})\); (2) simple normalized signed Laplacian \(\overline{\mathbf{L}}_{\mathit{sms}}=\overline{\mathbf{D}}^{-1}(\mathbf{D}^{*} -\mathbf{D}^{*})\); (3) balanced normalized signed Laplacian \(\overline{\mathbf{L}}_{\mathit{bms}}=\overline{\mathbf{D}}^{-1}(\mathbf{D}^{*} -\mathbf{A}^{*})\); (4) signed Laplacian graph \(\overline{\mathbf{L}}=\overline{\mathbf{D}}^{-1}\) - \(\mathbf{A}\) with a diagonal matrix \(\overline{\mathbf{D}}\); and (5) its symmetrically normalized version \(\mathbf{L}_{\mathit{svm}}\)(((21)\)). Two are \(K\)-way spectral clustering methods: (6) Balanced Normalized Cut (BNC) and (7) Balanced Ratio Cut (BRC) ((5)). The last two ((6)) are two generalized eigenproblem formulations: (8) SPONGE and (9) SPONGE\({}_{\mathit{svm}}\). Moreover, we compare with 6 state-of-the-art deep unsigned graph clustering methods: (10) DAEGC ((37)), (11) DPCN ((35)), (12) DCRN ((28)), (13) Dink-net ((27)), (14) DGCLUSTER ((1)) (15) MAGI ((26)). Please refer to App. E for hyperparameters settings and experiment details.

#### 5.1.3. Evaluation Metrics

For _Labeled graphs (SSBM)_, Accuracy (ACC), Adjusted Rand Index (ARI) ((12)), Normalized mutual information (NMI), and F1 score are used as the ground truths of nodes are available. For _Unlabeled graphs (S&P and Rainfall)_, due to the lack of ground truths, clustering quality is visualized by plotting network adjacency matrices sorted by cluster membership. See more detailed settings in App. E.

### Overall Performance

To address **RQ1**, we evaluated our DSGC and baselines on a variety of labeled signed graphs generated from four SSBM configurations, including SSBM (\(N=1000\), \(K=5\), \(p=0.01\), \(\eta\)) with \(\eta\in\{0,0.02,0.04,0.06,0.08\}\), SSBM (\(N=1000\), \(K=10\), \(p\), \(\eta=0.02\)) with \(p\in\{0.01,0.02,0.03,0.04,0.05\}\), SSBM (\(N\), \(K=5\), \(p=0.01\), \(\eta=0\)) with \(N\in\{300,500,800,100,1200\}\), and SSBM (\(N=1000\), \(K\), \(p=0.01\), \(\eta=0.02\)) with \(K\in\{4,5,6,7,8\}\). The performance of each experiment was measured by taking the average of 5 repeated executions. Table 1 reports the results in ACC and NMI. Appendix F provides the results in ARI and F1 score.

Table 1 shows: (i) _Superior performance:_ Our DSGC significantly outperforms all baseline models across all metrics, even though SPONGE and SPONGE\({}_{\mathit{svm}}\) are known for their effectiveness on such datasets. (ii) _Robustness_: Regardless of whether the graph is dense or sparse (\(p\)), large or small (\(N\)), noisy or clean (\(\eta\)), and the number of clusters is few or many (\(K\)), DSGC maintains notably superior performance on all 20 labeled signed graphs. (iii) _Comparative analysis:_ While deep unsigned clustering methods (DAEGC, DPCN, DCRN, Dink-net, DGCLUSTER MAGI) consistently underperform our DSGC due to the limitation of their capabilities to only handle non-negative edges. DSGC still has a clear advantage, highlighting the effectiveness of its design specifically tailored for signed graph clustering.

### Ablation Study

To address **RQ2** and evaluate the contributions of key components of DSGC, we performed an ablation study using labeled signed graphs, including SSBM (1000, \(b\), 0.01), SSBM (1000, 5, 0.01, \(\eta\)), SSBM (N, 5, 0.01, 0.01), and SSBM (1000, \(K\), 0.01, 0.01). The variants of DSGC tested are: **w/o VS-R** is DSGC without Violation Sign-Refine; **w/o DA** is DSGC without Density-based Augmentation; **w/o Regu** is DSGC without Regularization term; **w/o VS-R & DA** is DSGC without VS-R and DA; **w/o DA & Regu** is DSGC without VS-R and Regu; and **w/o All** is DSGC without VS-R, DA, and Regu.

From the results depicted in Fig. 4, it is evident that: (i) _Performance trends:_ As the edge probability (\(p\)) and the number of nodes (\(N\)) increase, the accuracy (ACC) of DSGC and its variants consistently improves. Conversely, increases in the sign flip probability (\(\eta\)) and the number of clusters (\(K\)) lead to a decline in ACC across all models. (ii) _Component impact:_ DSGC outperforms all variants on all labeled signed graphs, demonstrating the significant role each component plays in enhancing clustering performance. Specifically, DA emerges as the most influential component, affirming its effectiveness in reinforcing the graph structure and improving node representations by adding strategically placed new edges.

### Analysis of Violation Sign-Refine

To investigate **RQ3**, we analyzed the impact of applying Violation Sign-Refine (VS-R) on the performance of spectral clustering methods. VS-R was first used to pre-process and denoise signed graphs to generate new graphs. Then we compared the performance of all spectral methods before and after applying VS-R. Signed graphs were generated by fixing \(N=1000\) and varying \((K,\eta,p)\), including SSBM (1000, \(K,0.01,0.02\)) with \(K\in\{5,6\}\), SSBM (1000, 5, 0.01, 0.04), and SSBM (1000, \(10,p,0.02\)) with \(p\in\{0.01,0.02\}\).

Table 2 shows that VS-R significantly improves the clustering performance across all tested spectral methods w.r.t. ACC and NMI by generating cleaner graphs with better clustering structure. Specifically, the performance increments vary inversely with the strength of the baseline methods--stronger baselines show smaller gains, whereas weaker baselines benefit more substantially from the VS-R preprocessing. VS-R also consistently reduces the _violation ratio_, defined as the ratio of the number of violated edges to the number of non-violated edges, across various graph configurations.

In addition to numerical analysis, Fig. 14 in App. D provides visual evidence of the impact of VS-R. the embeddings of new

Figure 4. Ablation study. (a)–(d) The ACC(%) performance vs. edge probability (\(p\)), flip probability (\(\eta\)), node number \(N\) and cluster number \(K\).

graphs, displayed in the bottom row, exhibit clearer clustering boundaries than those of the original graphs in the top row.

### Impact of Signed Encoder to Clustering

To address **RQ4**, we developed two variants of DSGC encoder, including DSGC w/o (\(-\)A\({}^{-}\)) that replaces (\(-\)A\({}^{-}\)) with (A\({}^{-}\)), and DSGC w / _EEF_ that incorporates the '_the enemy of my enemy is my friend_ (_EEF_' principle from Balance Theory to DSGC. Both variants and DSGC used the layer number \(L=2\).

The positive and negative representations of DSGC w/ _EEF_ are \(\mathbf{Z}_{\mathit{eff}}^{+}=\mathbf{Z}^{+}+(\bar{\mathbf{A}}^{-})^{2}\mathbf{ Z}^{+(0)}\); \(\mathbf{Z}_{\mathit{eff}}^{-}=\mathbf{Z}^{-}\) where \(\mathbf{Z}^{+}\) and \(\mathbf{Z}^{-}\) are the positive and negative representations computed by Eq. 8 in DSGC. Similarly, the positive and negative embeddings of DSGC w/o (\(-\)A\({}^{-}\)) are \(\mathbf{Z}_{\mathit{-a}}^{+}=\mathbf{Z}^{+}\); \(\mathbf{Z}_{\mathit{-a}}=\mathbf{Z}^{-}\). We define a metric, _SoEN_, to measure the distance between nodes linked by negative edges:

\[\mathit{SoEN}=\frac{|\mathcal{E}^{+}|}{|\mathcal{E}^{-}|}\cdot\frac{\sum_{ \mathbf{z}_{\mathit{eff}}\in\mathcal{E}^{-}}s(\mathbf{z}_{\mathbf{i}},\mathbf{ z}_{\mathbf{j}})}{\sum_{\mathbf{z}_{\mathit{eff}}\in\mathcal{E}^{+}}s(\mathbf{z}_{ \mathbf{i}},\mathbf{z}_{\mathbf{j}})},\]

\begin{table}
\begin{tabular}{l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c} \hline \hline \multicolumn{1}{c|}{SSIM} & \multicolumn{12}{c}{(\(N=1000,K=5,p=031,\vartheta\))} & \multicolumn{12}{c}{(\(N=1000,K=10,p,q=0.02\))} \\ \hline
7881 & \multicolumn{2}{c|}{\(\overline{\eta}=0\)} & \multicolumn{2}{c|}{\(\overline{\eta}=0.62\)} & \multicolumn{2}{c|}{\(\overline{\eta}=0.64\)} & \multicolumn{2}{c|}{\(\overline{\eta}=0.66\)} & \multicolumn{2}{c|}{\(\overline{\eta}=0.68\)} & \multicolumn{2}{c|}{\(\overline{\eta}=0.61\)} & \multicolumn{2}{c}{\(\overline{\eta}=0.62\)} & \multicolumn{2}{c|}{\(\overline{\eta}=0.63\)} & \multicolumn{2}{c}{\(\overline{\eta}=0.64\)} & \multicolumn{2}{c}{\(\overline{\eta}=0.65\)} & \multicolumn{2}{c}{\(\overline{\eta}=0.67\)} & \multicolumn{2}{c}{\(\overline{\eta}=0.68\)} & \multicolumn{2}{c}{\(\overline{\eta}=0.69\)} & \multicolumn{2}{c}{\(\overline{\eta}=0.61\)} & \multicolumn{2}{c}{\(\overline{\eta}=0.61\)} & \multicolumn{2}{c}{\(\overline{\eta}=0.65\)} & \multicolumn{2}{c}{\(\overline{\eta}=0.69\)} & \multicolumn{2}{c}{\(\overline{\eta}=0.61\)} & \multicolumn{2}{c}{\(\overline{\eta}=0.65\)} & \multicolumn{2}{c}{\(\overline{\eta}=0.69\)} & \multicolumn{2}{c}{\(\overline{\eta}=0.61\)} & \multicolumn{2}{c}{\(\overline{\eta}=0.65\)} & \multicolumn{2}{c}{\(\overline{\eta}=0.68\)} & \multicolumn{2}{c}{\(\overline{\eta}=0.67\)} & \multicolumn{2}{c}{\(\overline{\eta}=0.69\)} & \multicolumn{2}{c}{\(\overline{\eta}=0.68\)} & \multicolumn{2}{c}{\(\overline{\eta}=0.67\)} & \multicolumn{2}{c}{\(\overline{\eta}=0.69\)} & \multicolumn{2}{c}{\(\overline{\eta}=0.68\)} & \multicolumn{2}{c}{\(\overline{\eta}=0.67\)} & \multicolumn{2}{c}{\(\overline{\eta}=0.69\)} & \multicolumn{2}{c}{\(\overline{\eta}=0.68\)} & \multicolumn{2}{c}{\(\overline{\eta}=0.

where \(s(\cdot,\cdot)\) is the inner product, indicating the similarity between two nodes. Ideally, _SoEN_ is a negative value and a lower _SoEN_ indicates a greater distance between nodes connected by negative edges and a clearer clustering boundary. Fig. 5 illustrates the ACC and _SoEN_ of DSGC and its variants. The results show that: (i) DSGC consistently outperforms its variants. Incorporating the "_EEF_" principle or altering the sign of \((-\hat{\mathbf{A}}^{-})\) significantly impacts clustering performance because DSGC achieves lower _SoEN_ along with epochs than its variants. This demonstrates its advantage in separating nodes linked by negative edges, leading to clearer clustering boundaries and larger inter-cluster variances. (ii) The term \((-\hat{\mathbf{A}}^{-})\) has higher impact than the inclusion of _EEF_, suggesting the original negative edge handling in DSGC is critical for maintaining clear cluster separations.

### Visualization

We utilized t-SNE to visualize the embeddings produced by DSGC and several strong baselines, including BNC (Wang et al., 2017), BRC (Wang et al., 2017), SPONGE (Wang et al., 2017), and SPONGE\({}_{sym}\)(Wang et al., 2017), on SSBM (\(N=1000\), \(K=5\), \(p=0.01\), \(\eta=0.02\)) in Fig. 6. Both BNC and BRC exhibit mode collapse, where most nodes are grouped into one or a few clusters. SPONGE and SPONGE\({}_{sym}\) show improved clustering structures. However, SPONGE lacks a clear boundary between clusters while SPONGE\({}_{sym}\) appears to form 6 clusters with a central cluster where nodes from different true clusters are mixed. This indicates its potential issue with handling nodes connected by negative edges, which are typically located at cluster boundaries. In contrast, DSGC successfully pushes nodes linked by negative edges apart, effectively eliminating the central cluster phenomenon in SPONGE\({}_{sym}\). This result is attributed to the exclusion of the "_EEF_" principle and the incorporation of the term \((-\mathbf{A}^{-})\) in the graph encoder.

### Unlabeled Graphs

We also evaluated DSGC on unlabeled real-world signed graphs, S&P1500 (Wang et al., 2017) and Rainfall (Wang et al., 2017), comparing it against three baselines, BRC (Wang et al., 2017), BNC (Wang et al., 2017), and SPONGE\({}_{\text{\_}}\)ym (Wang et al., 2017). The adjacency matrices of these graphs were sorted by predicted cluster membership to visually assess clustering outcomes.

**Rainfall.** Following (Wang et al., 2017), we analyzed the clustering structures for \(K=\{5,10\}\) in Fig. 7, where blue and red denote positive and negative edges, respectively2. Both BRC and BNC fail to identify the expected number of clusters, resulting in model collapse. In contrast, DSGC successfully identifies the specified clusters (5 or 10) and exhibits higher ratios of positive internal edges and stronger negative inter-cluster edges compared to SPONGE, indicating more cohesive and well-defined clusters.

Footnote 2: Darker blue diagonal blocks indicate more cohesive clusters, while darker pink non-diagonal blocks signify stronger negative relationships between clusters, enhancing the clarity of the clustering semantics.

**S&P1500.** Fig. 8 shows the clustering structures for \(K=\{5,10\}\). BRC and BNC suffer model collapse, placing most nodes into a single large, sparse cluster. In contrast, DSGC produces clear, compact clusters with significantly higher ratios of positive to negative internal edges than the entire graph, indicating more effective clustering that even surpasses SPONGE in identifying relevant groupings.

## 6. Conclusion

In this paper, we introduce DSGC, a novel deep signed graph clustering method, to enhance the clarity of cluster boundaries by effectively utilizing positive and negative edge connections for node partitioning. Existing approaches generally rely on the Social Balance Theory, which is primarily suitable for 2-way clustering. In contrast, DSGC leverages the Weak Balance Theory to address more general \(K\)-way clustering without the need for explicit labels. DSGC first introduces two pre-processing techniques, VS-R and DA, to denoise and structurally enhance signed graphs before clustering. Then, DSGC constructs a clustering-oriented signed neural network that produces more discriminative representations, specifically for nodes linked negatively. By optimizing a non-linear transformation for node clustering assignments, DSGC significantly outperforms existing methods, establishing clearer and more meaningful cluster distinctions in complex multi-cluster scenarios.

Figure 6. Visualization of clustering results from different algorithms. The ground truth class number is \(5\).

Figure 7. Sorted adjacency matrix for the Rainfall dataset with \(K=5\) (top row) and \(K=10\) (bottom row).

## References

* (1)
* Bhowmik et al. (2024) Arita Bhowmik, Mert Kasan, Zeei Huang, Anhui K. Singh, and Sosur Medya. 2024. DECLISTSER: A Neural Framework for Attributed Graph Clustering via Modularity Maximization. In _Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-First Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2024, February 20-27, 2024, Vancouver, Canada_, Michael J. Woodruff, Jesndre E. O. Dy, and Sriram Natarajan (Eds.). AAAI Press, 1069-11077. [https://doi.org/10.1009/AAAI.3182983](https://doi.org/10.1009/AAAI.3182983)
* Bao et al. (2020) Deyu Bao, Xiao Wang, Chuan Shi, Meiqi Zhu, Anna, and Peng Cui. 2020. Structural Deep Clustering Network In _WWW '20: The Web Conference_ 2020, Taipur, Taiwan, April 20-24, 2020, Yunnan Huang, Irwin King, Tue-Yan Liu, and Maarten van Noen (Eds.). ACM / IW3C2, 1400-1410. [https://doi.org/10.1145/3366423.3388214](https://doi.org/10.1145/3366423.3388214)
* Cartwright and Haney (1965) Dorwin Cartwright and Frank Haney. 1965. Structural balance: a generalization of Hele's theory. _Psychological review_ 53, 1056, 277.
* Chen et al. (2018) Yiqi Chen, Treyun Qian, Huan Liu, and Ke Sun. 2018. "Bridge?: Enhanced Signed Directed Network Embedding. In _Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM 2018, Torino, Italy_, October 22-20, 2018, Africa, Converse, James Allen, Nourman P. Polato, Drewis Sigurakos, Raissi Agrawal, Andrei E. Broder, Mohamed J. Zaki, K. Sekela, Candan, Alexandruks Labindria, Mark Sabatnet, and Hraisan Wang (Eds.). ACM, 275-782. [https://doi.org/10.1145/3520623.3527738](https://doi.org/10.1145/3520623.3527738)
* Chiang et al. (2012) Kai-Yang Chiang, Joyce Joyce Wyoung, and Inderjit S. Dhillon. 2012. Scalable clustering of signed networks using balance normalized cut. In _21st ACM International Conference on Information and Knowledge Management, CIKM 2012, Maui, Mumbai, 2012, Australia_, 2012, Xiu-Senior, Chen, Olejandro, Haxuan Wang, and Mohammed J. Zaki (Eds.). ACM, 615-624. [https://doi.org/10.1145/239661.2396841](https://doi.org/10.1145/239661.2396841)
* Cucuringu et al. (2019) Mihai Cucuringu, Peter Davies, Aldo Gilehmen, and Hemant Yegi. 2019. SPONGE: A generalized eigenproblem for clustering signed networks. In _The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naba, Okhawoon, Japan (Proceedings of Machine Learning Research Vol. 89)_, Kamalika Chaudhuri and Masashi Sugiyama (Eds.). PMLR, 1088-1098. [http://proceedings.mlr.press/v98/ccsurviving_19a.html](http://proceedings.mlr.press/v98/ccsurviving_19a.html)
* Davis (1967) James A Davis. 1967. Clustering and structural balance in graphs. _Human relations_ 20, 1967, 181-187.
* Derr et al. (2018) Tyler Derr, Chun C Aggarwal, and Illang Tang. 2018. Signed Network Modeling Based on Structural Balance Theory. In _Proceedings of the 27th ACM International Conference on Information and Knowledge Management, CIKM 2018, Torino, Italy_, October 22-26, 2018, Africa, Converse, James Allen, Nourman P. Polato, Drewis Sigurakos, Raissi Agrawal, Andrei E. Broder, Mohamed J. Zaki, K. Sekela, Candan, Alexandruks Labindria, Asadi Sosur, and Hraisan Wang (Eds.). ACM, 557-586. [https://doi.org/10.1145/35206296.3527146](https://doi.org/10.1145/35206296.3527146)
* Derr et al. (2018) Tyle Derr, Du Ma, and Jiliang Tang. 2018. Signed Graph Convolutional Networks. In _IEEE International Conference on Data Mining, ICDM 2018, Singapore_, November 17-20, 2018, IEEE Computer Society, 929-934. [https://doi.org/10.1109/ICDM.2018.00113](https://doi.org/10.1109/ICDM.2018.00113)
* Diaz-Diaz and Estrada (2020) Fernando Diaz-Diaz and Ernesto Estrada. 2020. Signed graphs in data sciences via communicationally geometry. _CoRR_ abs/2003.07493 (2020). [https://doi.org/10.4558/arxiv.2003.07493](https://doi.org/10.4558/arxiv.2003.07493)
* Diaz-Diaz and Estrada (2020) Andrei Fiuto, Patricia Severino, Kaname Kojima, Joao Ricardo Sato, Alexandru de Salvo, and Gaelvao Pantica. 2020. Financial clustering of time series gene expression data by Granger causality. _BMC Syst. Biol._ 6 (2), 137: [https://doi.org/10.1186/1735-2606-617](https://doi.org/10.1186/1735-2606-617)
* Latise and Allen (2017) Alexander J. Latise and Yong-fei Li. 2017. The Impact of Random Models on Clustering Similarity. _J. Mach. Learn. Res._ 18 (2017), 87:1-87:28. [http://jmlr.org/jmlr/18/97-039.html](http://jmlr.org/jmlr/18/97-039.html)
* Graham et al. (2013) Fan Chung Graham, Alexander Tsiats, and Wensong Xu. 2013. Dirichlet PageRank and Ranking Algorithms Based on Trust and Distrust. _J. Mach. Learn. Res._ 19, 113-134. [https://doi.org/10.1086/15427951.2617814](https://doi.org/10.1086/15427951.2617814)
* Harvey (1953) Frank Harvey. 1953. On the motion of a signed graph. _Michigan Mathematical Journal_ 2, 12 (1953), 143-146.
* He et al. (2022) Yixuan He, Geisine Rinert, Songzhou Wang, and Mikui C Cueumingu. 2022. SSSNET: Semi-Supervised Signed Network Clustering. In _Proceedings of the 2022 SIAM International Conference on Data Mining, SDM 2022, Alexandru, USA, April 28-30, 2022, Anizhan Banerjee, Zhi-Hua Zhou, Evangelista, and Matteo Rondaela (Eds.). SIAM, 244-255. [https://doi.org/10.1137/S181917772.28](https://doi.org/10.1137/S181917772.28)
* CLEAN 2019
- 28th International Conference on Artificial Neural Networks, Manish, Germany, September 17-19, 2019, Proceedings_. _Workshop and Special Networks (Lecture Notes in Computer Science, Vol. 117321)_, No. Y. Telcho, Veneku, Yaworsky, Pavel Karpov, and Fabian J. Theis (Eds.). Springer, 566-577. [https://doi.org/10.1007/978-3-0306-350495-5](https://doi.org/10.1007/978-3-0306-350495-5)
* Huang et al. (2021) Junjie Huang, Haawei Shen, Liang Hou, and Xueqi Cheng. 2021. SPON3: Learning Node Representation for Signed Directed Networks. In _Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The French Symposium on Educational Advances in Artificial Intelligence, IAAI 2021, Virtual Event, February 2-2, 2021_. AAAI Press, 196-203. [https://doi.org/10.3609/AAAI.V3511.16099](https://doi.org/10.3609/AAAI.V3511.16099)
* Islam et al. (2018) Mohammad Rahimat Islam, Balra Prakash, and Niran Ramakrishnan. 2018. SIGNet: Scalable Embeddings for Signed Networks. In _Advances in Knowledge Discovery and Data Mining, 22nd FloV-Fifth-Via Conference, PAKDD 2018, Melbourne, V.C., Australia, June 26-28, 2018, Proceedings_, Part II _. Lecture Notes in Computer Science, Vol. 10005, Dub. D. Phung, Vincent S. Teng, Geoffrey J. Webb, Rao Ho, Mahodesh Ganti, and Little Bashi (Eds.). Springer, 157-169. [https://doi.org/10.1007/978-3-319-9037-1_43](https://doi.org/10.1007/978-3-319-9037-1_43)
* Jiang et al. (2020) Jinhong Jiang, Jaeem You, and Ugang. 2020. Signed Graph Diffusion Network. _CoRR_ abs/2011.1491 (2020). arXiv:2012.14191. [https://doi.org/10.1145/350622006.350614](https://doi.org/10.1145/350622006.350614)
* Kwapw (2007) Andrew V. Kwapw. 2007. Toward the Optimal Preconditioned Eigenvalue: Locally Optimal Block Preconditioned Computing Gradient Method. _SIAM J. Sci. Comput._ 23, 20 (2000), 517-541. [https://doi.org/10.11375/s1050182750066124](https://doi.org/10.11375/s1050182750066124)
* Kueis et al. (2015) Jievene Kueis, Stephan Schmidt, Andreas Lommutsch, Jaysen Lerer, Ernesto Willman De Lao, and Sahin Albay. 2015. Spectral Analysis of Signed Graphs for Clustering, Prediction and Visualization. In _Proceedings of the SIAM International Conference on Data Mining, SDM 2014, April 29-24, 2015, Columbus, Ohio, USA_. SIAM, 539-570. [https://doi.org/10.1137/1.9781611972081.49](https://doi.org/10.1137/1.9781611972081.49)
* Lee et al. (2020) Youn-Chang Lee, Nayoung Son, and Sang-Yook Kim. 2020. Are Negative Links Reliefied in Network Embedding? In-Depth Analysis and Interesting Results, in _CIKM 20: The 29th ACM International Conference on Information and Knowledge Management, Virtual Event, London, October 29-23, 2020, Mathisetti &Aquin, Stefan Dietter, Claudia Hauff, Edward Curry, and Philippe Cudreau-Mauroux (Eds.). ACM, 2113-2116. [https://doi.org/10.1145/345353.3412407](https://doi.org/10.1145/345353.3412407)
* Ma et al. (2023) Yu Li, Meng Qi, Juan Tang, and Yi Cheng. 2023. Signed Laplacian Graph Neural Networks. In _Thirty-Seventh AAAI Conference on Artificial Intelligence, IAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, IAAI 2023, Washington, DC, USA, February 7-14, 2023, Brain Williams, Yiling and Chen, and Jennifer Nie (Eds.). AAAI Press, 4444-4452. [https://doi.org/10.1009/AAAI.3752555](https://doi.org/10.1009/AAAI.3752555)
* Yan et al. (2020) Yu Li, Yuan Tian, Jiawei Zhang, and Yi Chang. 2020. Learning Signed Network Embedding via Graph Attention. In _The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA_, February 7-12, 2020, AAAI Press, 4772-4779. [https://doi.org/10.1009/AAAI.374804.5911](https://doi.org/10.1009/AAAI.374804.5911)
* Lin et al. (2021) Hsin Lin, Ziwei Zhang, Peng Cui, Yafeng Zhang, Qiang Cui, Jiashua Liu, and Wenwu Zhu. 2021. Signed Graph Neural Network with Latent Groups. In KDD 2021, The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, Singapore, August 14-16, 2021, Eds., 2021, Eds., 2021, Eds., 2021, Eds., Deep Data Onia, and O. Meng (Eds.). ACM, 1066-1075. [https://doi.org/10.1145/345484.347355](https://doi.org/10.1145/345484.347355)
* Liu et al. (2024) Yunfe Liu, Jinting Li, Yue Chen, Ruan Wu, Erick Wang, Jing Zhou, Sheng Tian, Shung Shen, Xing Fu, Changhun Meng, Weiguang Wang, and Liang Chen. 2024. Revisiting Modularity Maximization for Graph Clustering: A Contrastive Learning Perspective. In _Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 2024, Barcelona, Spain_, August 29-29, 2024, Ricardo Baeza-Yates and Francesco Bonchi (Eds.). ACM, 1968-1979. [https://doi.org/10.1145/3637523.361767](https://doi.org/10.1145/3637523.361767)
* Liu et al. (2022) Yue Liu, Ke Jiang, Kun Jiang Zhou, Xiang Zhao, Xianyu Tang, and Jian Z. 2022. Distr-Neural Clustering on Large Graphs. In _International Conference on Machine Learning, ICML 2022, 23-29 July 2022, Honolulu, Hawaii, USA_, _Proceedings of Machine Learning Research, Vol. 2022, Andreas Krause, Emun Brunst-skill, Kyung Chu, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.). PMLR, 21794-2182. [https://proceedings.mlr.press/v.vn/202/la](https://proceedings.mlr.press/v.vn/202/la)

[MISSING_PAGE_FAIL:10]

**Comparison of Social Balance Theory and Weak Balance Theory**. The partition \(\{\mathcal{C}_{1},\ldots,\mathcal{C}_{K}\}\) of a signed graph \(\mathcal{G}\) satisfying both theories can be uniformly defined such that the following conditions hold:

\[\begin{cases}\mathbf{A}_{ij}>0&(e_{ij}\in\mathcal{E})\cap(v_{i}\in\mathcal{C}_{ k})\cap(v_{j}\in\mathcal{C}_{k})\\ \mathbf{A}_{ij}<0&(e_{ij}\in\mathcal{E})\cap(v_{i}\in\mathcal{C}_{k})\cap(v_{j }\in\mathcal{C}_{l})(k\neq l)\end{cases} \tag{17}\]

where \(\mathbf{A}_{ij}\) is the weight of edge \(e_{ij}\) and \(0<k,l<K\). However, the "_EEE_" principle is specific to \(K\)-clusterable (\(K>2\)) networks (e.g., Fig. 10(b)) and does not appear in \(2\)-clusterable systems (Fig. 10(a)).

Recent literature [8, 16, 17, 24, 23, 38, 43] has primarily leveraged Social Balance Theory principles to improve node representations for signed graphs, potentially overlooking the broader applicability of Weak Balance Theory in datasets with more than \(2\) antagonistic groups, especially when explicit labels are lacking. Our work aims to fully explore Weak Balance Theory and its principles in the design of a signed graph encoder for \(K\)-way clustering.

Appendix B Analyzing the impact of the signed encoder to node representations and clustering boundary

In this section, we specifically analyze the principles of Weak Balance Theory implied in positive and negative aggregation functions (Eq. (9) and Eq. (10)) and the term \((-\bar{\mathbf{A}}^{-})\) in Eq. (10) for the perspective of their impact to the node representations and the clustering boundaries.

**The term \((-\bar{\mathbf{A}}^{-})\).** The minus sign "\(-\)" helps push nodes linked by negative edges further apart in the latent space. For example, in Fig. 11 (a), the node \(u\) has three "_friend neighbors_", \(v_{1}\), \(v_{2}\), and \(v_{3}\). The positive embedding \(u^{*}\) of \(u\) is placed at the mean of these three "_friend neighbors_" according to Eq. (9), thus narrowing the distance between them and its central node \(u\). In Fig. 11 (b), the node \(u\) has three "_enemy neighbors_", \(v_{4}\), \(v_{5}\), and \(v_{6}\). "\(-\)" in the term \((-\bar{\mathbf{A}}^{-})\) indicates that two vertices with a negative edge should be placed on opposite sides. Then, the negative embedding \(u^{*}\) is placed at the mean of \(-v_{4}\), \(-v_{5}\), and \(-v_{6}\), which are the opposite coordinates of \(v_{4}\), \(v_{5}\), and \(v_{6}\), respectively. This leads to further distance between \(u\) and its "_enemy neighbors_". As nodes linked negatively are likely to be located at the cluster boundary, pushing them away from each other will create clearer cluster boundaries, thus effectively increasing inter-cluster variances. Section 5.5 quantitatively analyzes the effect of \((-\bar{\mathbf{A}}^{-})\) on node embeddings.

**Positive and Negative aggregation.** Eq. (9) aggregates the node embeddings of all \(l\)-_hop_ "_friend neighbors_" along the \(l\)-length positive walk (Dfn 1), implying the principle "_the friend of my friend is my friend (FFF_" and its transitivity. It pulls "_friend neighbors_" within \(L\)-hop toward the central node, thus reducing intra-cluster variances. Eq. (10) aggregates the node embeddings of all \(l\)-_hop_ "_enemy neighbors_" along the \(l\)-length negative walk (Dfn 1), implying the principles of "_the enemy of my friend is my enemy (EEE_")_, "_the friend of my enemy is my enemy (FEE_")_", and the transitivity of "_FFF_". Importantly, we no longer consider the specific principle "_the enemy of my enemy is my Friend (EEF_")_" of Social Balance so that the distance between nodes linked negatively can effectively increase, which can be verified by quantitatively comparing our DSGC and its variant that incorporates "_EEF_". Taking a signed graph with \(L=2\) as an example, Fig. 12 illustrates our positive and negative aggregation rules in Eq. (9) and (10).

## Appendix C Hyperparameter sensitivity analysis

This section explores the sensitivity of DSGC's performance to variations in its hyperparameters, specifically focusing on \(\delta^{+},\delta^{-}\), \(m^{+}\), and \(m^{-}\). \(\delta^{+},\delta^{-}\) are thresholds that determine the confidence levels for nodes being classified as _effective friends_ or _effective enemies_, respectively. \(m^{+}\) and \(m^{-}\) control the augmentation of positive and negative edge densities within and across clusters, respectively. We used synthetic signed graphs from SSBM (1000, 5, 0.01, 0.02) for this analysis. The results illustrated in Fig. 13 show that: (i) Optimal performance is achieved when both \(\delta^{+}\) and \(\delta^{-}\) are set to \(1\). Increasing \(\delta^{+}\) generally worsens accuracy (ACC) as less noisy edges are effectively refined. (ii) Both excessively high and low values of \(m^{+}\) degrade clustering performance due to imbalances in capturing local versus more extended neighborhood information. Setting \(m^{+}\) to \(3\) and \(m^{-}\) to \(2\) achieve optimal performance.

Figure 11. Impact of the term \((-\bar{\mathbf{A}}^{-})\) in Eq. (10) when both \(r^{+}\) and \(r^{-}\) are \(0\). (a) The positive embedding \(u^{+}\) is placed at the mean of its “_friend neighbors_”, including \(v_{1},v_{2}\), and \(v_{3}\). (b) Due to "\(-\)" in the term, the negative embedding \(u^{-}\) is placed at the mean of its “_enemy neighbors_” antipodal points, including \(-v_{4}\), \(-v_{5}\), and \(-v_{6}\), resulting in \(u^{-}\) further away from \(v_{4}\), \(v_{5}\), and \(v_{6}\).

Figure 12. The illustration of positive and negative aggregations in Eq. (9) and Eq. (10) on a signed graph with the central node \(v_{i}\) and its \(2\)-hop neighbors.

[MISSING_PAGE_FAIL:12]

E Implementation Setting

All experiments are implemented on PYtorch. The length of positive and negative walks \(L^{\prime}\) in VS-R is set to 3. The balance parameter \(\lambda\) in loss \(\mathcal{L}\) is set to 0.03. The layer number \(L\) in our graph encoder is set to 2. Node features X are derived from the \(K\)-dimensional embeddings corresponding to the largest \(K\) eigenvalues of the symmetrized adjacency matrix. The hidden dimension \(d\) is 32 in the our cluster-specific signed graph clustering. Besides, the hyperparameters sensitivity analysis of \(\delta^{+}\), \(\delta^{-}\), \(m^{+}\), and \(m^{-}\) in Eq. (5) and Eq. (6) can be found in App. C. Following (Zhu et al., 2017), we change a signed graph to an unsigned graph by revising all negative edges to positive edges, which are inputted to above unsigned graph clustering methods (DAEGC, DPCN, DCRN, Dink-net, DGCLUSTER MAGI).

## Appendix F ARI and F1 score of overall performance

This results in ARI and F1 score for DSGC and all baselines are reported in Table 3. They are generally consistent with the ACC and NMI results presented in the main text, reinforcing the conclusions drawn from those analyses. We can observe: (i) _Superior performance_: DSGC still significantly outperforms all baseline models in ARI and F1 score. (ii) _Robustness_: DSGC exhibits notably superior performance on all 20 labeled signed graphs in ARI and F1 score. This observation highlights the effectiveness and robustness of our approach regarding \(\eta\), \(p\), \(N\), and \(K\). (iii) _Comparative analysis_: In terms of ARI and F1 score, while unsigned clustering methods (DAEGC, DPCN, and DCRN) generally outperform non-deep spectral methods due to their advanced representation learning capabilities, DSGC still maintains a significant advantage, confirming that the specialized design of DSGC is effective for the unique challenges of signed graph clustering.