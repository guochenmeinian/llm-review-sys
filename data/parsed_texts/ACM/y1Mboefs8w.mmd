# EXGC: Bridging Efficiency and Explainability in Graph Condensation

Anonymity

###### Abstract.

Graph representation learning on vast datasets, like web data, has made significant strides. However, the associated computational and storage overheads raise concerns. In sight of this, Graph condensation (Gcond) has been introduced to distill these large real datasets into a more concise yet information-rich synthetic graph. Despite acceleration efforts, existing Gcond methods mainly grapple with efficiency, especially on expansive web data graphs. Hence, in this work, we pinpoint two major inefficiencies of current paradigms: (1) the concurrent updating of a vast parameter set, and (2) pronounced parameter redundancy. To counteract these two limitations correspondingly, we first (1) employ the Mean-Field variational approximation for convergence acceleration, and then (2) propose the objective of Gradient Information Bottleneck (GDIB) to prune redundancy. By incorporating the leading explanation techniques (_e.g._, GNNExplainer and GSAT) to instantiate the GDIB, our EXGC, the Efficient and **eX**plainable **G**raph **C**ondensation method is proposed, which can markedly boost efficiency and inject explainability. Our extensive evaluations across eight datasets underscore EXGC's superiority and relevance. Code is available at [https://anonymous.4open.science/r/EXGC](https://anonymous.4open.science/r/EXGC).

Graph Neural Networks, Graph Condensation, Model Explainability +
Footnote †: copyright: none

## 1. Introduction

Web data, such as social networks (Hamilton et al., 2017), transportation systems (Zhu et al., 2017; Li et al., 2017), and recommendation platforms (Xu et al., 2018; Li et al., 2018), are often represented as graphs. These graph structures are ubiquitous in everyday activities, including streaming on Netflix, interacting on Facebook, shopping on Amazon, or searching on Google (Li et al., 2018; Li et al., 2018). Given their tailor-made designs, Graph Neural Networks (GNNs) (Li et al., 2018; Li et al., 2018; Li et al., 2018) have emerged as a prevalent solution for various tasks on graph-structured data and showcased outstanding achievements across a broad spectrum of graph-related web applications (Gil et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018).

However, real-world scenarios often entail the handling of large-scale graphs encompassing millions of nodes and edges (Li et al., 2018; Li et al., 2018), posing substantial computational burdens during the training of GNN applications (Li et al., 2018; Li et al., 2018; Li et al., 2018). Worse still, the challenges are exacerbated when fine-tuning hyperparameters and discerning optimal training paradigms for over-parametrized GNN models. Against this backdrop, a crucial inquiry arises: _can we effectively simplify or reduce the graph size to accelerate graph algorithm operations, including GNNs, while also streamlining storage, visualization, and retrieval essential for graph data analysis (Li et al., 2018; Li et al., 2018; Li et al., 2018)_

As a primary solution, graph sampling emphasizes selecting pivotal edges/nodes and omitting the less relevant ones (Li et al., 2018; Li et al., 2018; Li et al., 2018; Li et al., 2018). However, this can lead to considerable information loss, potentially harming model performance (Zhu et al., 2017; Li et al., 2018). Conversely, graph distillation aims to compress the extensive real graph \(\mathcal{T}\) into a concise yet information-rich synthetic graph \(\mathcal{S}\), enhancing the efficiency of the graph learning training process. Within this domain, the **graph condensation** (Gcond) stands out due to its exceptional compression capabilities (Li et al., 2018; Li et al., 2018). For instance, as depicted in Figure 1 (a), the graph learning model trained on the synthetic graph \(\mathcal{S}\) (containing just 154 nodes generated by Gcond) yields a 91.2% test accuracy on Reddit, nearly matching the performance of the model trained on the original dataset with 153,932 nodes (_i.e._, 93.5% accuracy).

Despite their successes, we argue that even with various acceleration strategies, current Gcond methods remain facing efficiency challenges in the training process, particularly on large graph datasets such as web data. This inefficiency arises from two main factors:

* Firstly, as depicted in Figure 1 (b), the **prolonged convergence** stems from the concurrent updating of an overwhelming number of parameters (_i.e._, elements in node features of \(\mathcal{S}\)). Specifically, unlike conventional graph learning where parameter dimensionality is dataset-agnostic, in Gcond, the number of parameters grows with the nodes and node feature dimensions, imposing substantial computational and storage demands.
* Secondly, as illustrated in Figure 1 (c), the current Gcond approaches mainly exhibit **node redundancy**. Concretely, when compressing new datasets, to counteract the risk of insufficient information capacity from too few nodes, a higher node count

Figure 1. The compression capability and limitations of current Gcond. (a) Gcond adeptly compresses the dataset to just 0.1% of its initial size without compromising the accuracy benchmarks. (b) Contrary to traditional graph learning, Gcond’s parameters scale with node count. (c) To avoid insufficient information capacity, Gcond typically introduces node redundancy.

is typically employed by \(\mathcal{S}\), leading to parameter redundancy in the training process. Depending on the dataset attributes, this redundancy can vary, with some instances exhibiting as much as 92.7% redundancy. We put further discussion in Section 3.3.

In sight of this, in this work, we aim to refine the paradigm of GCond to mitigate the above limitations. Specifically, **for the first limitation**, we scrutinize and unify the paradigms of the current methods from the perspective of Expectation Maximization (EM) framework (Brockman, 1988; Goyal et al., 2017), and further formulate it as the theoretical basis for our forthcoming optimization schema. From this foundation, we pinpoint the efficiency bottleneck in the training process, _i.e._, the computation of intricate posterior probabilities during the Expectation step (E-step). This insight led us to employ the Mean-Field (MF) variational approximation (Brockman, 1988) - a renowned technique for improving the efficiency of E-step with intricate variables - to revise the paradigm of GCond. The streamlined method is termed Mean-Field Graph Condensation (MGCond).

Then, **for the second limitation**, our solution seeks to 'explain' the training process of the synthetic graph \(\mathcal{S}\): we prioritize the most informative nodes in \(\mathcal{S}\) (_i.e._, nodes encapsulating essential information for model training) and exclude the remaining redundant nodes from the training process. To formulate this objective, inspired by the principle of graph information bottleneck, we introduce the Gradient Information Bottleneck (GDIB). Building upon GDIB, our EXGC, the Efficient and eXplainable Graph Condensation method, is proposed by integrating the leading explanation strategies (e.g., GNNExplainer (Sandes et al., 2017) and GSAT (Sandes et al., 2017)) into the paradigm of MGCond.

Our contribution can be summarized as follow:

* For the limitation of inefficiency, we unify the paradigms of current approaches to pinpoint the cause and leverage Mean-Field variational approximation to propose the MGCond for boosting efficiency (Section 3.1 & 3.2).
* For the caveat posed by node redundancy, we introduce the objective of Gradient Information Bottleneck, and utilize the leading explanation methods to develop an explainable and efficient method, EXGC (Section 3.3 & 3.4).
* Extensive experiments demonstrate that our EXGC outperforms the baselines by a large margin. For instance, EXGC is 11.3 times faster than the baselines on Citeseer (Section 4).

Furthermore, it is worth mentioning that beyond the tasks of graph condensation, the superior performance of EXGC across various backbones (_i.e._, explainers) also verifies the effectiveness of the graph explanation methods in enhancing downstream graph tasks. To our knowledge, this stands as one of the vanguard efforts in the application of graph explainability, addressing a crucial yet rarely explored niche.

## 2. Problem Formulation

In this part, we retrospect the objective of graph condensation. Specifically, graph condensation endeavors to transmute a large, original graph into a compact, synthetic, and highly informative counterpart. The crux of this process is to ensure that the GNNs trained on the condensed graph manifest a performance comparable to those trained on the original graph.

**Notations.** Initially, we delineate the common variables utilized in this study. We start from the original graph \(\mathcal{T}=(\mathbf{A},\mathbf{X},\mathbf{Y})\), where \(\mathbf{A}\in\mathbb{R}^{N\times N}\) is the adjacency matrix, \(N\) is the number of nodes and \(\mathbf{X}\in\mathbb{R}^{N\times d}\) is the \(d\)-dimensional node feature attributes. Further, we note the label of nodes as \(\mathbf{Y}=\{0,1,\ldots,C-1\}^{N}\) denotes the node labels over \(C\) classes. Our target is to train a synthetic graph \(\mathcal{S}=(\mathbf{A}^{\prime},\mathbf{X}^{\prime},\mathbf{Y}^{\prime})\) with adjacency matrix \(\mathbf{A}^{\prime}\in\mathbb{R}^{N^{\prime}\times N^{\prime}}\) and feature attributes \(\mathbf{X}^{\prime}\in\mathbb{R}^{N^{\prime}\times D}\) (\(N^{\prime}\ll N\)), which can achieve comparable performance with \(\mathcal{T}\) under GNNs inference process.

**Graph condensation via gradient matching.** The above objective of graph condensation can be formulated as follows:

\[\begin{split}&\min_{\mathcal{S}}\mathcal{L}\left(f_{\theta_{ \mathcal{S}}}\left(\mathbf{A},\mathbf{X}\right),\mathbf{Y}\right),\\ &\text{s.t.}\ \ \theta_{\mathcal{S}}=\operatorname*{arg\,min}_{ \theta}\mathcal{L}\left(f_{\theta}(\mathbf{A}^{\prime},\mathbf{X}^{\prime}), \mathbf{Y}^{\prime}\right),\end{split} \tag{1}\]

where \(\mathcal{L}\) represents the loss function and \(f_{\theta}\) denotes the graph learning model \(f\) with parameters \(\theta\). In pursuit of this objective, the previous works typically employ the gradient matching scheme following (Sandes et al., 2017; Wang et al., 2018; Wang et al., 2018). Concretely, given a graph learning model \(f_{\theta}\), these methods endeavor to reduce the difference of model gradients _w.r.t._ real data \(\mathcal{T}\) and synthetic data \(\mathcal{S}\) for model parameters (Sandes et al., 2017). Hence, the graph learning models trained on synthetic data will converge to similar states and share similar test performance with those trained on real data.

## 3. Methodology

In this section, we first unify the paradigms of current GCond methods in Section 3.1. Building upon this, we propose the MGCond, which employs MF approximation to boost efficiency in Section 3.2. Furthermore, to eliminate the redundancy in the training process, we introduce the principle of GDIB in Section 3.3 and instantiate it to develop our EXGC in Section 3.4.

### The Unified Paradigm of GCond

As depicted in Section 2, graph condensation aims to match the model gradients _w.r.t_ large-real graph \(\mathcal{T}\) and small-synthetic graph \(\mathcal{S}\) for model parameters. This process enables GNNs trained on \(\mathcal{T}\) and \(\mathcal{S}\) to share a similar training trajectory and ultimately converge to similar states (parameters). We formulate this gradient matching process as follows:

\[\begin{split}&\max_{\mathcal{S}}\mathbb{E}_{\theta-\mathbb{P}_{ \theta}}P(\nabla^{\prime}_{\theta}=\nabla_{\theta}),\\ &\text{s.t.}\ \nabla^{\prime}_{\theta}=\frac{\partial\mathcal{L}(f_{ \theta}(\mathcal{S}),\mathbf{Y}^{\prime})}{\partial_{\theta}},\nabla_{\theta}= \frac{\partial\mathcal{L}(f_{\theta}(\mathcal{T}),\mathbf{Y})}{\partial_{ \theta}},\end{split} \tag{2}\]

where \(\mathbb{P}_{\theta}\) denotes the distribution of \(\theta\)'s potential states during the training process. For example, (Sandes et al., 2017) defines \(\mathbb{P}_{\theta}\) as the set of parameter states that can appear throughout a complete network training process, while (Sandes et al., 2017) simply defines it as the potential initial states of the parameters.

Considering the computational complexity of jointly optimizing \(\mathbf{X}^{\prime}\), \(\mathbf{A}^{\prime}\), and \(\mathbf{Y}^{\prime}\), and the interdependency between these three variables, current methods typically fix the labels \(\mathbf{Y}^{\prime}\) and design a MLP-based model \(g_{\Phi}\) with parameters \(\Phi\) to calculate \(\mathbf{A}^{\prime}\) following \(\mathbf{A}^{\prime}=g_{\Phi}\) (\(\mathbf{X}^{\prime}\)) (Zhou et al., 2017). In this case, Equation 2 can be rewrite as:

\[\max_{\mathbf{X}^{\prime},\Phi}E_{\theta\rightarrow\overline{p}_{ \theta}}P(\nabla^{\prime}_{\theta}=\nabla_{\theta}),\] \[\text{s.t.}\ \nabla^{\prime}_{\theta}= \frac{\partial\mathcal{L}(f_{\theta}(\mathbf{X}^{\prime},g_{\Phi }(\mathbf{X}^{\prime})),\mathbf{Y}^{\prime})}{\partial_{\theta}},\nabla_{ \theta}=\frac{\partial\mathcal{L}(f_{\theta}(\mathbf{X},\mathbf{A}),\mathbf{Y}) }{\partial_{\theta}}. \tag{3}\]

Without loss of generality, \(\nabla^{\prime}_{\theta}\) and \(\nabla_{\theta}\) are consistently defined as provided here in the following text, even though not all previous methods have employed the MLP-based simplification strategy 1.

Footnote 1: For methods that do not adopt the simplification strategy, by replacing \(g_{\Phi}\) with \(\mathbf{A}\), the subsequent theoretical sections still hold true.

After random initialization, Equation 3 can be achieved by alternately optimizing the variable \(\mathbf{X}^{\prime}\) and the model parameters \(\Phi\), which naturally adheres to the Expectation-Maximization schema, as shown in Figure 2 (a). Specifically, the EM algorithm alternates between the expectation step (E-step) and the maximization step (M-step):

* \(\mathbf{E}\)**-step:** Estimate the variable \(\mathbf{X}^{\prime}\) while freezing the model \(g_{\Phi}\), then utilize it to calculate the Evidence Lower Bound (ELBO) of the objective of the gradient matching in Equation 3.
* \(\mathbf{M}\)**-step:** Fine the parameters \(\Phi\) which maximizes the above ELBO.

After instantiating the above schema, graph condensation can be formulated as follows, where \(t\) represents the training epoch and \(\nabla_{\theta}\) is a simplified notation for \(\nabla^{\prime}_{\theta}=\nabla_{\theta}\):

* **Initialization:** Select the initial value of the parameter \(\Phi^{(0)}\) and the node feature \(\mathbf{X}^{\prime(0)}\), then start the iteration;
* \(\mathbf{E}\)**-step:** Use the model \(g(\Phi^{(t)})\) to estimate the node features \(\mathbf{X}^{\prime(t)}\) according to \(P(\mathbf{X}^{\prime(t)}|\nabla_{\theta},\Phi^{(t)})\) and calculate the ELBO:

\[\text{ELBO}\to E_{\mathbf{X}^{\prime(t)}|\nabla_{\theta},\Phi^{(t)}} \big{[}\log\frac{P(\mathbf{X}^{\prime(t)},\nabla_{\theta}\mid\Phi)}{P( \mathbf{X}^{\prime(t)}\mid\nabla_{\theta},\Phi^{(t)})}\big{]}; \tag{4}\]
* \(\mathbf{M}\)**-step:** Find the corresponding parameters \(\Phi^{(t+1)}\) when the above ELBO is maximized:

\[\Phi^{(t+1)}\coloneqq\arg\max_{\Phi}E_{\mathbf{X}^{\prime(t)}|\nabla_{\theta },\Phi^{(t)}}\big{[}\log\frac{P(\mathbf{X}^{\prime(t)},\nabla_{\theta}\mid \Phi)}{p(\mathbf{X}^{\prime(t)}\mid\nabla_{\theta},\Phi^{(t)})}\big{]}; \tag{5}\]
* **Output:** Repeat the E-step and M-step until convergence, then output the synthetic graph \(\mathcal{S}\) according to the final \(\mathbf{X}^{\prime}\) and \(\Phi\).

The detailed derivation of the above Equations is shown in Appendix B.

**Revealing the Limitation of Inefficiency.** However, we have noticed that even with various acceleration strategies (Zhou et al., 2017), the above paradigm remains facing efficiency challenges in the training process. We attribute this limitation to the estimation process of \(\mathbf{X}^{\prime}\) in E-step. Specifically, in contrast to traditional graph learning tasks where the number of network parameters is **dataset-agnostic**, for graph condensation task, the number of to-be-updated parameters in E-step (_i.e._, elements in \(\mathbf{X}^{\prime}\)) linearly **increases** with the number of nodes \(\mathbf{N}\) and feature dimensions \(d\), posing substantial burden of gradient computation and storage.

This flaw is particularly evident on large graph datasets such as web data with millions of nodes (\(N\)) and thousands of feature dimensions (\(d\)). Therefore, it is crucial to find a shortcut for expediting the current paradigm.

### Boost Efficiency: MGCond

To address the limitation of inefficiency, we aim to inject the Mean-Field variational approximation (Brockman and Komodakis, 2018) into the current GCond paradigm. In practice, MF approximation has been extensively verified to enhance the efficiency of the EM framework containing variables with complex distributions. Hence, it precisely matches the challenge encountered in our E-step, where the to-be-updated variable \(\mathbf{X}^{\prime}\) possesses large dimensions. Next, we elucidate the process of leveraging MF estimation to enhance the GCond paradigm.

Firstly, MF approximation assumes that the to-be-updated variable can be decomposed into multiple independent variables, aligning naturally with the property of node features \(\mathbf{X}^{\prime}=\{x^{\prime}_{1},x^{\prime}_{2},...,x^{\prime}_{N^{\prime}}\}\) of \(\mathcal{S}\) in our E-step (_i.e._, Equation 4):

\[P(\mathbf{X}^{\prime})=\prod_{i=1}^{N^{\prime}}P\left(x^{\prime}_{i}\right), \tag{6}\]

where \(x^{\prime}_{i}\) is the feature of the \(i\)-th node in graph \(\mathcal{S}\). By substituting Equation 6 into the ELBO in Equation 4 we obtain:

\[\begin{split}\text{ELBO}&=\int\prod_{i=1}^{N^{\prime }}P\left(x^{\prime}_{i}\right)\log P(\nabla_{\theta},\mathbf{X}^{\prime})d \mathbf{X}^{\prime}\\ &\quad-\int\prod_{i=1}^{N^{\prime}}P\left(x^{\prime}_{i}\right) \log\prod_{i=1}^{N^{\prime}}P\left(x^{\prime}_{i}\right)d\mathbf{X}^{\prime}. \end{split} \tag{7}\]

Figure 2. The paradigm of current GCond methods from the perspective of the EM schema, and the E-step of our proposed MGCond and EXGC.

In this case, while we focus on the node feature \(x^{\prime}_{j}\) and fix its complementary set \(\mathbf{X^{\prime}}_{\setminus j}=\{x^{\prime}_{1},...x^{\prime}_{j-1},x^{\prime} _{j+1},...,x^{\prime}_{N^{\prime}}\}\), the ELBO in Equation 7 can be rewritten as:

\[\begin{split}\text{ELBO}&=\int P\left(x^{\prime}_{j} \right)\int\prod_{i=1,i\neq j}^{N^{\prime}}P\left(x^{\prime}_{i}\right)\log P( \nabla_{\theta},\mathbf{X^{\prime}})d_{i\neq j}x^{\prime}_{i}dx^{\prime}_{j}\\ &-\int P\left(x^{\prime}_{j}\right)\log P\left(x^{\prime}_{j} \right)dx^{\prime}_{j}+\sum_{i=1,i\neq j}^{N^{\prime}}\int P\left(x^{\prime}_{i }\right)\log P\left(x^{\prime}_{i}\right)dx^{\prime}_{i},\end{split} \tag{8}\]

where the third term can be considered as the constant \(C\) because \(\mathbf{X^{\prime}}_{\setminus j}\) is fixed. Then, to simplify the description, we define:

\[\begin{split}\log\tilde{P}_{j}(\mathbf{X^{\prime}},\nabla_{ \theta})&=E_{\prod_{i=1,i\neq j}^{N^{\prime}}P\left(x^{\prime}_{ i}\right)}\left[\log P(\mathbf{X^{\prime}},\nabla_{\theta})\right]\\ &=\int\prod_{i=1,i\neq j}^{N^{\prime}}P\left(x^{\prime}_{i} \right)\log P(\mathbf{X^{\prime}},\nabla_{\theta})d_{i\neq j}x^{\prime}_{i}, \end{split} \tag{9}\]

and combine it with Equation 8 to obtain the final form of the ELBO which is streamlined by the MF variational approximation:

\[\begin{split}\text{ELBO}&=\int P(x^{\prime}_{j}) \frac{\log\tilde{P}_{j}(\mathbf{X^{\prime}},\nabla_{\theta})}{P(x^{\prime}_{j} )}dx^{\prime}_{j}+C\\ &=-KL\left(P(x^{\prime}_{j})\left[\log\tilde{P}(\mathbf{X^{\prime }},\nabla_{\theta})\right]+C,\end{split} \tag{10}\]

where \(KL\) denotes the Kullback-Leibler (KL) Divergence (Kirkpatrick et al., 2017). Due to the non-negativity of the KL divergence, maximizing this ELBO is equivalent to equating the two terms in the above KL divergence. Based on this, we have:

\[P(\mathbf{X^{\prime}})\propto\prod_{j=1}^{N^{\prime}}\log\tilde{P}_{j}(\mathbf{ X^{\prime}},\nabla_{\theta}), \tag{11}\]

which can be regarded as the theoretical guidance for the \(\mathbf{X^{\prime}}\) estimation process in the E-step. The detailed derivation is exhibited in Appendix C.

**The Paradigm of MGCond.** Equation 11 indicates that the estimation of node feature \(x^{\prime}_{j}\) in E-step can be performed while keeping its complementary features \(\mathbf{X^{\prime}}_{\setminus j}\) fixed. Without loss of generality, we generalize this conclusion from individual nodes to subsets of nodes, and distribute the optimization process of each set evenly over multiple iterations. This optimized E-step is the key distinction between our MGCond and the prevailing paradigm, as illustrated in Figure 2 (b). To be more specific, the paradigm of MGCond can be formulated as follows:

* **Initialization:** Select the initial value of the parameter \(\Phi^{(0)}\) and features \(\mathbf{X^{\prime}}^{(0)}\), divide the nodes in graph \(\mathcal{S}\) into \(K\) parts equally _i.e._, \(\mathbf{X^{\prime}}=\{\mathbf{X^{\prime}}_{1},\mathbf{X^{\prime}}_{2},..., \mathbf{X^{\prime}}_{K}\}\), and start the iteration;
* **E-step:** Use the model \(g(\Phi^{(t)})\) to estimate the features in subsets \(\mathbf{X^{\prime}}_{k}^{(t)}\) for \(k=\{1,2,...,K\}\) according to: (12) \[\mathbf{X^{\prime}}_{k}^{(t+1)}=\begin{cases}\max_{\mathbf{X}_{k}}P(\mathbf{X^ {\prime}}_{k}|\mathbf{X^{\prime}}_{k}^{(t)},\nabla_{\theta},\Phi^{(t)}),&\text{ if }k=r+1,\\ \mathbf{X^{\prime}}_{k}^{(t)},&\text{ otherwise,}\end{cases}\]

where \(r\) is the remainder when \(t\) is divided by \(K\).

* **M-step:** Find the corresponding parameters \(\Phi^{(t+1)}\) when the following ELBO is maximized: \[\Phi^{(t+1)}:=\arg\max_{\Phi}E_{\mathbf{X^{\prime}}^{(t+1)}|\nabla_{\theta}, \Phi^{(t)}}\left[\log\frac{P(\mathbf{X^{\prime}}^{(t+1)},\nabla_{\theta}\mid \Phi)}{p(\mathbf{X^{\prime}}^{(t+1)}\mid\nabla_{\theta},\Phi^{(t)})}\right];\]
* **Output:** Repeat the E-step and M-step until convergence, then output the synthetic graph \(\mathcal{S}\) according to the final \(\mathbf{X^{\prime}}\) and \(\Phi\).

### Node Redundancy and GDIB

After executing MGCond we summarize two empirical insights that primarily motivated the development of our XEGC as follows:

1. The training process of \(\mathbf{X^{\prime}}\) in E-step exhibits a _long-tail problem_. That is, when **20%** of the node features \(\mathbf{X^{\prime}}\) are covered in training (_i.e._, \(t\approx 0.2K\)), the improvement in test accuracy has already achieved **93.7%** of the total improvement on average. In other words, the remaining 80% of the node features only contribute to 6.3% of the accuracy improvement.
2. This long-tail problem has a larger variance. Specifically, even for the same task with the same setting and initialization, when 20% of the \(\mathbf{X^{\prime}}\) are covered in training, the maximum difference between test accuracy exceeds 25% (_i.e._, difference between 72.4% and 98.8%), since those 20% trained nodes are randomly selected from \(\mathcal{S}\).

These two observations indicate that there is a considerable **redundancy** in the number of to-be-trained nodes. That is, the synthetic graph \(\mathcal{S}\) comprises a subset of key nodes that possess most of the necessary information for gradient matching. If the initial random selections pinpoint these key nodes, the algorithm can yield remarkably high test accuracy in the early iterations. On the other side, entirely training all node features \(\mathbf{X^{\prime}}\) in \(\mathcal{S}\) would not only be computationally wasteful but also entail the potential risk of overfitting the given graph learning model.

Therefore, it naturally motivates us to identify and train these key nodes in E-step (instead of randomly selecting nodes to participate in training like MGCond). To guide this process, inspired by the Graph Information Bottleneck (GIB) for capturing key subgraphs (Stein et al., 2017; Zhang et al., 2017) and guiding GNNs explainability (Stein et al., 2018; Zhang et al., 2018), we propose the GraDient Information Bottleneck (GDIB) for the compact graph condensation with the capability of redundancy removal:

**Definition 1** (Gib).: _Given the the synthetic graph \(\mathcal{S}\) with label \(\mathbf{Y^{\prime}}\) and the GNN model \(f_{0}\), GDIB seeks for a maximally informative yet compact subgraph \(\mathcal{S}_{sub}\) by optimizing the following objective:_

\[\begin{split}\arg\max_{\mathcal{S}_{sub}}I\left(\mathcal{S}_{sub}; \nabla^{\prime}_{\theta}\right)-\beta I\left(\mathcal{S}_{sub};\mathcal{S} \right),\,\text{s.t.}\,\,\mathcal{S}_{sub}\in\mathbb{G}_{sub}(\mathcal{S}), \end{split} \tag{14}\]

_where \(\nabla^{\prime}_{\theta}\) denotes the gradients \(\partial\mathcal{L}(f_{\theta}(\mathcal{S}),\mathbf{Y^{\prime}})/\partial\theta\); \(\mathbb{G}_{sub}(\mathcal{S})\) indicates the set of all subgraphs of \(\mathcal{S}\); \(I\) represents the mutual information (MI) and \(\beta\) is the Lagrangian multiplier._

### Prune Redundancy: EXGC

**A Tractable Objective of GDIB.** To pinpoint the crucial node features to participate in the training process in E-step, we first derive a tractable variational lower bound of the GDIB. Detailed derivation can be found in Appendix D, which is partly adapted from (Zhang et al., 2017; Zhang et al., 2017).

Specifically, for the first term \(I(\mathcal{S}_{sub};\nabla^{\prime}_{\theta})\), a parameterized variational approximation \(Q(\nabla^{\prime}_{\theta}|\mathcal{S}_{sub})\) for \(P(\nabla^{\prime}_{\theta}|\mathcal{S}_{sub})\) is introduced to derive its lower bound:

\[I(\mathcal{S}_{sub};\nabla^{\prime}_{\theta})\geq\mathbb{E}_{\mathcal{S}_{sub} ;\nabla^{\prime}_{\theta}}\left[\log Q(\nabla^{\prime}_{\theta}|\mathcal{S}_{ sub})\right]. \tag{15}\]

For the second term \(I(\mathcal{S}_{sub};\mathcal{S})\), we introduce the variational approximation \(R(\mathcal{S}_{sub})\) for the marginal distribution \(P(\mathcal{S}_{sub})=\sum_{\mathcal{S}}R\left(\mathcal{S}_{sub}|\mathcal{S} \right)P(\mathcal{S})\) to obtain its upper bound:

\[I\left(\mathcal{S}_{sub};\mathcal{S}\right)\leq\mathbb{E}_{\mathcal{S}}\left[ \operatorname{KL}\left(P\left(\mathcal{S}_{sub}|\mathcal{S}\right)\|R( \mathcal{S}_{sub})\right)\right]. \tag{16}\]

By incorporating the above two inequalities, we derive a variational upper bound for Equation 14, serving as the objective for

\[\operatorname*{arg\,max}_{\mathcal{S}_{sub}}\mathbb{E}\left[\log Q(\nabla^{ \prime}_{\theta}|\mathcal{S}_{sub})\right]-\mathbb{E}\left[\operatorname{KL} \left(P\left(\mathcal{S}_{sub}|\mathcal{S}\right)\|R(\mathcal{S}_{sub})\right) \right]. \tag{17}\]

**Instantiation of the GDIB**. To achieve the above upper bound, we simply adopt \(\partial\mathcal{L}(f_{\theta}(\mathcal{S}_{sub}),\mathbf{Y}^{\prime})/\partial\theta\) to instantiate the distribution \(Q\). Then, we specify the distribution \(R\) in Equation 16 as a Bernoulli distribution with parameter \(r\) (_i.e._, each node is selected with probability \(r\)). As for \(P\left(\mathcal{S}_{sub}|\mathcal{S}\right)\), we suppose it assigns the importance score \(p_{i}\) (_i.e._, the probability of being selected into \(\mathcal{S}_{sub}\)) to the \(i\)-th node in \(\mathcal{S}\). After that, GDIB can be instantiated by the post-hoc explanation methods such as:

* **Gradient-based** methods like SA [1] and GradCAM [40]. For the \(i\)-th node, these methods first calculate the absolute values of the elements in the derivative of \(\mathcal{L}(f_{\theta}(\mathcal{S}),\mathbf{Y}^{\prime})\)_w.r.t_\(x_{i}\) (_i.e._, the features of the \(i\)-th node). After that, the importance score \(p_{i}\) is defined as the normalized sum of these values. More formally: \[p_{i}=\operatorname*{softmax}_{i\in[1,2,\ldots,N]}\left\{\left|\frac{\partial \mathcal{L}(f_{\theta}(\mathcal{S}),\mathbf{Y}^{\prime})}{\partial x_{i}} \right|\cdot\mathbf{1}^{T}\right\}.\] (18)
* **Local Mask-based** methods like GNNExplainer [57] and GraphMASK [39]. Concretely, for the first term of Equation 17, these methods firstly multiply the node's features \(x^{\prime}_{i}\) with the initialized node importance score \(p_{i}\) to get \(\mathbf{X}^{\prime\prime}=\{p_{1}x^{\prime}_{1},p_{2}x^{\prime\prime}_{2}, \ldots,p_{N}\cdot x^{\prime}_{N^{\prime}}\}\), and feed \(\mathbf{X}^{\prime\prime}\) into model \(f_{\theta}\) to obtain the output \(y_{p}\). Then they attempt to find the optimal score \(p_{i}\) by minimizing the difference between this processed output \(y\) and the original prediction. Concurrently, for the second term of Equation 17, these methods set \(r\) to approach \(0\), making the value of the KL divergence proportional to the score \(p_{i}\). As a result, they treat this KL divergence as the \(l_{1}\)-norm regularization term acting on \(p_{i}\) to optimize the training process of \(p_{i}\). After establishing these configurations, the optimal score can be approximated through several gradient descents following: \[\mathbf{p}=\min_{\mathbf{p}}D\left(y;y_{p}\right)+\lambda\mathbf{p}\cdot \mathbf{1}^{T},\] (19) where \(\mathbf{p}\) is defined as \(\{p_{1},p_{2},...,p_{N}\}\); \(D\) denotes the distance function; \(\lambda\) is the trade-off parameter; \(y\) and \(y_{p}\) represents: \[\left\{\begin{array}{l}y=f_{\theta}(\{\mathbf{X}^{\prime},g_{\Phi}(\mathbf{X }^{\prime})\}),\\ y_{p}=f_{\theta}(\{\mathbf{X}^{\prime\prime},g_{\Phi}(\mathbf{X}^{\prime\prime })\}),\end{array}\right.\] (20)
* **Global Mask-based** methods like GSAT2[32] and PGExplainer [31]. Here, during the instantiation process of the first term of Equation 17, the trainable \(p_{i}\) in Local Mask-based methods is replaced with a trainable \(\operatorname{MLP}_{\psi}\) (_i.e._, \(p_{i}=\operatorname{MLP}_{\psi}(x^{\prime}_{i})\)) and \(y_{p}\) is correspondingly replaced with \(y_{\text{MLP}}\). Meanwhile, for the second term in Equation 17, these methods set \(r\in(0,1)\) to instantiate the KL divergence as the _information constraint_ (\(\ell_{l}\)) proposed by [32], where \(\ell_{l}\) is defined as:

Footnote 2: The GSAT mentioned here refers to the GSAT in the post-explanation mode [32].

\[\ell_{l}=\sum_{i\in 1,2,\ldots,N}p_{i}\log\frac{p_{i}}{r}+(1-p_{i})\log\frac{1- p_{i}}{1-r}. \tag{21}\]

Treating \(\ell_{l}\) as a regularization term acting on \(\mathbf{p}\), the explainers can obtain the approximate optimal score \(\mathbf{p}\) through several gradient optimizations of \(\psi\) following:

\[\psi=\min_{\psi}D\left(y;y_{\text{MLP}}\right)+\lambda\ell_{l}. \tag{22}\]

After obtaining the importance score \(p_{i}\), the crucial subgraph \(\mathcal{S}_{sub}\) in GDIB can be composed of nodes with larger scores \(p_{i}\).

**The Paradigm of EXGC**. As illustrated in Figure 2 (c), after leveraging the above leading post-hoc graph explanation methods to achieve the objective of GDIB, we summarize the paradigm of our EXGC as follows:

* **Initialization:** Select the initial value of the parameter \(\Phi^{(0)}\), the node features \(\mathbf{X}^{\prime}\left({}^{0}\right)\), the set of the node index \(\mathbf{M}\) and the ratio of nodes optimized in each E-step as \(\kappa\), then start the iteration;
* **E-step:** Leverage the above explainers to assign an importance score \(p_{i}\) to the \(i\)-th node in \(\mathcal{S}\) for the index \(i\) in set \(\mathbf{M}\): \[\{p_{i}\}=\text{Explainer}\left(\{x_{i}\},f_{\theta}\right),\quad\text{for}\ i\in\mathbf{M}.\] (23)

Subsequently, remove the indices corresponding to the nodes with the top \(\lfloor\kappa N^{\prime}\rfloor\) scores from set \(\mathbf{M}\). Then use the model \(g(\Phi^{(t)})\) to estimate the features \(\mathbf{X}^{\prime}(t+1)\) according to:

\[\left\{\begin{array}{l}\mathbf{X}^{\prime}_{\mathbf{M}}(t+1)=\max_{\mathbf{X} _{\mathbf{M}}}P(\mathbf{X}^{\prime}_{\mathbf{M}}|\mathbf{X}^{\prime}_{ \mathbf{M}^{\prime}}\nabla_{\theta},\Phi^{(t)}),\\ \mathbf{X}^{\prime}_{\mathbf{M}}(t+1)=\mathbf{X}^{\prime}_{\mathbf{M}^{\prime}} \end{array}\right. \tag{24}\]

where \(\mathbf{X}^{\prime}_{\mathbf{M}}=\{x_{i}\}\) for \(i\in\mathbf{M}\), and \(\mathbf{X}^{\prime}_{\mathbf{M}}=\mathbf{X}^{\prime}\setminus\mathbf{X}^{\prime}_{\mathbf{M}}\).
* **M-step:** Find the corresponding parameters \(\Phi^{(t+1)}\) when the following ELBO is maximized: \[\Phi^{(t+1)}:=\operatorname*{arg\,max}_{\Phi}E_{\mathbf{X}^{\prime}(t+1)}|_{\nabla_{\theta},\Phi^{(t)}} \left[\log\frac{p(\mathbf{X}^{\prime}(t+1),\nabla_{\theta}\mid\Phi)}{p(\mathbf{X}^{\prime}(t+1) \mid\nabla_{\theta},\Phi^{(t)})}\right];\] (25)
* **Output:** Repeat the E-step and M-step until convergence, then output the synthetic graph \(\mathcal{S}\) according to the final \(\mathbf{X}^{\prime}\) and \(\Phi\).

The comparison between E-steps in the paradigms of GCM, MGCond and EXGC is exhibited in Figure 2 (d). By leveraging graph explanation methods to instantiate the objective of GDIB and seamlessly integrating it within the MGCond's training paradigm, our proposed EXGC adeptly identifies pivotal nodes in the synthetic graph \(\mathcal{S}\) during early training stages. Experimental results in the ensuing section underline that EXGC frequently converges early - specifically when a mere **20%** of the nodes in \(\mathcal{S}\) participate in training - attributed to the successful identification of these key nodes. EXGC's computational focus on these essential nodes ensures resource optimization, precluding superfluous expenditure on extraneous nodes. As a result, it can not only boost the efficiency but also enhance the test accuracy.

## 4. Experiments

In this section, we conduct experiments on six node classification graphs and three graph classification benchmarks to answer the following research questions:

* **RQ1.** How effective is our EXGC _w.r.t_ efficiency and accuracy?
* **RQ2.** Can the design of EXGC be transferred to the state-of-the-art graph condensation frameworks (_e.g._, DosGcond)?
* **RQ3.** What is the impact of the designs (_e.g._, the backbone explainers) on the results? Is there a guideline for node selection?
* **RQ4.** Does the condensed graph exhibit strong cross-architecture capabilities?

### Experimental Settings

**Datasets.** To evaluate the effectiveness of EXGC, we utilize six node classification benchmark graphs, including four transductive graphs, Cora (Zhou et al., 2017), Citeseer (Citeseer, 2018), Ogbn-Arxiv and Ogbn-Product (Zhou et al., 2017) and two inductive graphs, _i.e._, Flickr (Citeseer, 2018) and Reddit (Kawaguchi et al., 2018). For a fair comparison, we adopt the setup outlined in (Zhou et al., 2017) and document the performance of various frameworks on the aforementioned datasets. Without loss of generality, we also select three graph classification datasets for evaluation: the Ogbg-molhiv molecular dataset (Zhou et al., 2017), the TUDatasets (DD) (Zhu et al., 2017) and one superpixel dataset CIFAR10 (Krizhevsky et al., 2009).

**Backbones.** In this paper, we employ a wide range of backbones to systematically validate the capabilities of EXGC. We choose one representative model, GCN (Zhou et al., 2017), as our training model for the gradient matching process.

* To answer **RQ1**, we follow GCM to employ three coreset methods (_Random_, _Herding_(Zhu et al., 2017) and _K-Center_(Citeseer, 2018)) and two data condensation models (DC-Graph) and GCond provided in (Zhou et al., 2017). Here we showcase the detailed settings in Table 2.
* To answer **RQ2**, we choose the current SOTA graph condensation method, DosGcond as backbone (Citeseer, 2018). DosGcond eliminates the parameter optimization process within the inner loop of GCM, allowing for one-step optimization. This substantially reduces the time required for gradient matching. We employ DosGcond to further assess the generalizability of our algorithm.
* To answer **RQ3**, we select the explanation methods for node in \(\mathcal{S}\) based on gradient magnitude (SA) (Bog et al., 2018), global mask (GSAT) (Zhou et al., 2017), local mask (GNNExplainer) (Zhou et al., 2017) as well as random selection, to evaluate the extensibility of backbone explainers.
* To answer **RQ4**, we choose currently popular backbones, such as APPNP (Zhou et al., 2017), SGC (Kawaguchi et al., 2018) and GraphSAGE (Kawaguchi et al., 2018) to verify the transferability of our condensed graph (GCN as backbone). We also include MLP for validation.

**Measurement metric.** To ensure a fair comparison, we train our proposed method alongside the state-of-the-art approaches under identical settings, encompassing learning rate, optimizer, and so forth. Initially, we generate three condensed graphs, each developed using training methodologies with distinct random seeds. Subsequently, a GNN is trained on each of these graphs, with this training cycle repeated thrice (**Record the mean of the run time**). To gauge the information retention of the condensed graphs, we

\begin{table}
\begin{tabular}{c c c c c c c c c c c} \hline \hline \multirow{2}{*}{**Dataset**} & \multirow{2}{*}{**Ratio**} & \multicolumn{3}{c}{**Baselines**} & \multicolumn{2}{c}{**Ablation**} & \multicolumn{2}{c}{**Ours**} & \multicolumn{1}{c}{**Storage**} & \multicolumn{1}{c}{\(\rho\)} \\ \cline{3-11}  & & **Random** & **Hering** & **K-Center** & **Gcond-X** & **Gcond** & **EXGC-X** & **EXGC** & **Full graph** & \\ \hline Citeseer (47.1M) & 0.3\% & 33.87\({}_{40.82}\) & 31.31\({}_{1.20}\) & 34.03\({}_{42.52}\) & 64.13\({}_{4.18}\) & 63.98\({}_{44.31}\) & 67.82\({}_{41.31}\) & 69.16\({}_{26.00}\) & 71.12\({}_{0.06}\) & 0.142M & 333.3\(\times\) \\ Citeseer (47.1M) & 1.8\% & 42.66\({}_{41.30}\) & 40.61\({}_{2.13}\) & 51.79\({}_{3.20}\) & 67.24\({}_{1.85}\) & 66.82\({}_{22.70}\) & 69.60\({}_{41.88}\) & 70.09\({}_{40.72}\) & 71.12\({}_{0.06}\) & 0.848M & 55.6\(\times\) \\ Citeseer (47.1M) & 3.6\% & 59.74\({}_{2.88}\) & 63.85\({}_{1.77}\) & 67.25\({}_{1.80}\) & 69.86\({}_{0.97}\) & 69.74\({}_{2.36}\) & 70.18\({}_{1.17}\) & 70.55\({}_{0.98}\) & 71.12\({}_{0.06}\) & 1.696M & 27.8\(\times\) \\ Cora (14.9M) & 0.4\% & 37.04\({}_{7.41}\) & 43.47\({}_{0.55}\) & 46.33\({}_{3.32}\) & 69.10\({}_{4.01}\) & 72.76\({}_{0.45}\) & 80.91\({}_{3.09}\) & 82.02\({}_{0.42}\) & 80.91\({}_{0.10}\) & 0.060M & 250.0\(\times\) \\ Cora (14.9M) & 1.3\% & 59.62\({}_{2.48}\) & 62.18\({}_{1.91}\) & 69.12\({}_{2.25}\) & 75.38\({}_{1.89}\) & 79.29\({}_{2.06}\) & 80.74\({}_{0.41}\) & 81.94\({}_{0.81}\) & 80.91\({}_{0.10}\) & 0.194M & 76.9\(\times\) \\ Cora (14.9M) & 2.6\% & 73.29\({}_{1.00}\) & 70.91\({}_{2.12}\) & 73.66\({}_{1.86}\) & 75.93\({}_{0.80}\) & 80.02\({}_{0.61}\) & 65.07\({}_{2.82}\) & 82.26\({}_{0.80}\) & 80.91\({}_{0.10}\) & 0.388M & 38.5\(\times\) \\ Ogbn-arxiv (100.4M) & 0.5\% & 46.83\({}_{2.49}\) & 47.23\({}_{40.77}\) & 47.28\({}_{1.15}\) & 56.49\({}_{1.69}\) & 57.39\({}_{0.58}\) & 65.46\({}_{0.85}\) & 57.62\({}_{0.64}\) & 70.76\({}_{0.04}\) & 0.050M & 2000.0\(\times\) \\ Ogbn-arxiv (100.4M) & 0.25\% & 57.32\({}_{1.19}\) & 58.64\({}_{2.18}\) & 54.36\({}_{2.67}\) & 62.38\({}_{1.62}\) & 62.49\({}_{2.56}\) & 64.82\({}_{0.51}\) & 62.34\({}_{0.26}\) & 70.76\({}_{0.04}\) & 0.251M & 400.0\(\times\) \\ Ogbn-arxiv (100.4M) & 0.5\% & 60.09\({}_{0.49}\) & 61.25\({}_{0.88}\) & 60.84\({}_{0.59}\) & 63.77\({}_{0.95}\) & 64.85\({}_{0.74}\) & 65.79\({}_{0.32}\) & 64.99\({}_{0.70}\) & 70.76\({}_{0.04}\) & 0.502M & 200\(\times\) \\ Ogbn-Product (1412.5M) & 0.5\% & 57.49\({}_{2.53}\) & 60.10\({}_{0.36}\) & 59.46\({}_{1.22}\) & 61.59\({}_{0.61}\) & 62.15\({}_{0.36}\) & 62.71\({}_{0.91}\) & 62.09\({}_{0.74}\) & 70.76\({}_{0.04}\) & 70.63M & 200.0\(\times\) \\ Ogbn-Product (1412.5M) & 1.5\% & 58.84\({}_{1.87}\) & 63.17\({}_{0.93}\) & 60.71\({}_{0.85}\) & 62.98\({}_{1.30}\) & 63.89\({}_{0.51}\) & 65.85\({}_{0.95}\) & 64.69\({}_{1.43}\) & 70.76\({}_{0.04}\) & 21.189M & 66.7\(\times\) \\ Ogbn-Product (1412.5M) & 3\% & 60.10\({}_{0.94}\) & 63.87\({}_{0.94}\) & 62.60\({}_{1.08}\) & 65.82\({}_{0.95}\) & 65.30\({}_{0.92}\) & 67.50\({}_{1.05}\) & 63.76\({}_{0.72}\) & 70.76\({}_{0.04}\) & 42.378M & 33.3\(\times\) \\ Flickr (86.8M) & 0.1\% & 44.81\({}_{1.87}\) & 43.09\({}_{0.56}\) & 43.30\({}_{0.40}\) & 46.93\({}_{0.10}\) & 46.10\({}_{0.10}\) & 46.95\({}_{0.50}\) & 43.09\({}_{0.10}\) & 46.30\({}_{0.10}\) & 46.95\({}_{0.03}\) & 46.20\proceed to train GNN classifiers, which are then tested on the real graph's nodes or entire graphs. By juxtaposing the performance metrics of models on these real graphs, we discern the informativeness and efficacy of the condensed graphs. All experiments are conducted in three runs, and we report the mean performance along with its variance.

### Main Results (RQ1)

In this subsection, we evaluate the efficacy of a 2-layer GCN on the condensed graphs, juxtaposing the proposed methods, EXGC-X and EXGC, with established baselines. It's imperative to note that while most methods yield both structure and node features, note as \(\mathbf{A}^{\prime}\) and \(\mathbf{X}^{\prime}\), there are exceptions such as DC-Graph, GCond-X, and EXGC-X. Owing to the absence of structural output from DC-Graph, GCond-X, and EXGC-X, we employ an identity matrix as the adjacency matrix when training GNNs predicated solely on condensed features. Nevertheless, during the inference, we resort to the complete graph in a transductive setting or the test graph in an inductive setting to facilitate information propagation based on the pre-trained GNNs. Table 1 delineates performance across six benchmarks spanning various backbones, from which we can make the following observations:

**Obs 1. EXGC and EXGC-X consistently outperform other baselines** under extremely large condensation rates, thereby validating their exceptional performance. To illustrate, on smaller datasets such as Citeseer and Cora, our models achieve compression rates ranging from 0.3% \(\sim\)0.4%, representing an improvement of approximately 3.84% to 8.15% over the current state-of-the-art model, GCond and GCond-X. On larger graphs like Ogbn-product and Reddit, EXGC surpasses GCond by 0.56% to 0.78% when aiming for a 0.5% compression rate. These findings underscore the substantial contributions of iterative optimization strategy of the subset of the nodes in \(\mathcal{S}\) solely to the field of graph condensation (see Table 1). Additionally, our visualization results in Table 1 reveal that the graphs we condensed exhibit high density and compactness, with edges serving as efficient carriers of dense information, thereby facilitating effective information storage.

**Obs 2. Both EXGC and EXGC-X can achieve an extreme compression rate compared with the original graph** without significant performance degradation. On all six datasets, when compressing the original graph to a range of 0.05% to 5% of its original size, the compressed graph consistently maintains the performance of the original data while significantly accelerating the inference speed. This enhancement proves to be highly advantageous for information extraction and reasoning. Furthermore, as the storage requirement is reduced to a fraction of the original data's size (even better in some cases), this greatly facilitates the transmission of graph-type information in the web space, underscoring the exceptional contribution of our model to web systems.

### Generalizability on DosGCond (RQ2)

To answer RQ2, we choose a gradient-based explainer as the backbone explainer. We transfer the SA into the current SOTA graph condensation method, DosGCond, and named as EXDos. We record the **performance** and **training time** of each backbone. As shown in Table 3 and Figure 3, we can make the observations as following:

**Obs 3. Upon incorporating the backbone explainers, significant reductions in the training time of GCond are achieved.** Furthermore, when our approach is applied to DosGCond, EXDos still exhibits substantial efficiency gains. Specifically, we observe efficiency improvements ranging from \(1.78\sim 5.05\) times on five node classification datasets and speed enhancements of \(1.58\sim 2.51\) times on graph classification datasets. These findings validate the effectiveness of our algorithm, offering a viable solution for efficient data compression.

**Obs 4. When employing the backbone explainers, the algorithm accelerates without noticeable performance decline.** As shown in Table 3, we find that on the eight datasets, the model consistently achieves acceleration without evident performance deterioration. Particularly, it gains performance improvements ranging from \(2.08\%\sim 9.26\%\) on the Cora and Citeseer datasets. These

Figure 3. The training process of EXGC and GCond across Cora, Citeseer, Ogbn-Arxiv and Ogbn-Product four benchmarks. We can observe that EXGC achieves optimal performance ahead by 507, 1097, 832, and 366 epochs respectively, at which points training can be terminated.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline
**Dataset** & **Ratio** & **GCond** & **EXGC** & **DosGCond** & **EXDos** \\ \hline Cora & 0.4\% & 29.78\% (22.72\%) & 6.68\% (12.15\%) & 3.22\% (0.45\%) & 1.13\% (0.14\%) \\ Citeseer & 0.3\% & 30.12\% (38.98\%) & 2.67\% (14.04\%) & 2.83\% (27.70\%) & 0.56\% (0.04\%) \\ Ogbn-arxiv & 0.05\% & 184.90\% (29.37\%) & 96.31\% (37.22\%) & 20.49\% (0.28\%) & 5.60\% (0.03\%) \\ Flicker & 0.1\% & 8.77\% (48.81\%) & 4.54\% (42.27\%) & 1.16\% (0.48\%) & 0.56\% (0.03\%) \\ Reddit & 0.1\% & 53.04\% (98.56\%) & 19.83\% (98.04\%) & 5.75\% (0.47\%) & 1.78\% (0.11\%) \\ DD & 0.2\% & & & & 1.48\% (72.40\%) & 0.59\% (0.23\%) \\ CIFAR10 & 0.1\% & – & – & 3.57\% (0.41\%) & 1.85\% (0.38\%) \\ Ogbg\(\rightarrow\)moliv & 0.01\% & – & – & 0.49\% (72.2\%) & 0.31\% (73.44\%) \\ \hline \hline \end{tabular}
\end{table}
Table 3. Comparing the time consumption and performance across different backbones. All results in seconds should be multiplied by 100. We activate 5% of the nodes every 50 epochs and stop training if the loss does not decrease for 4 consecutive epochs, subsequently reporting the results should be multiplied by 100).

findings demonstrate that while reducing training and inference time, our approach does not lead to performance degradation and can even enhance the performance of the condensed graph.

### Selection guidelines of EXGC (RQ3)

In this section, we choose a backbone explainer for nodes in \(\mathcal{S}\) based on gradient magnitude (SA), random selection, and explainable algorithms (GNNExplainer and GSAT). Our target is to determine whether different backbone explainers influence the performance of the EXGC. We employ a 2-layer GCN and introduce an early stopping mechanism, halting the network training if the loss fails to decrease over 4 consecutive epochs. Subsequently, we monitor and record the model's performance. By leveraging various backbone explainers, every 50 epochs, we select an additional 5% of the elements in \(\mathcal{X}^{\prime}\). Here we can make the observations:

**Obs 5.** As shown in Table 4, EXDO denotes our backbone explainers transitioned into the overall framework of DosGcond. We discovered that the convergence time is similar for both random and gradient-based explainers. However, explainable algorithms necessitate considerable time due to the training required for the corresponding explainer. Intriguingly, despite the GNNExplainer-based selection method incurring the most substantial time cost, it still remains lower than the conventional DosGcond algorithm (see Table 3). Going beyond our explanation strategies, we need to further observe the performance of models under different algorithms to better assist users in making informed trade-offs.

**Obs 6.** After examining the efficiency, as illustrated in Figure 4, we observed that while balancing efficiency, GSAT can achieve the best results. In contrast, GNNExplainer has the lowest efficiency. Interestingly, while SA and random have similar efficiencies, SA manages to yield superior results in comparison.

### Transferability of EXGC (RQ4)

Finally, we illustrate the transferability of condensed graphs from the different architectures. Concretely, we show test performance across different GNN backbones using a 2-layer GCN as the training setting. We employ popular backbones, APPNP, SGC and GraphSAGE, as test architectures. Table 5 exhibits that:

**Obs 7.** Across five datasets, our algorithm consistently outperforms GCond and demonstrates relatively lower variance, validating the effectiveness of our approach. Notably, on the Cora dataset, our model achieves a performance boost of nearly 6.0%\(\sim\)7.0%. On the Citesser, we can observe that our framework achieves a performance improvement of approximately 5% to 6% over GCond. These results all underscore the transferability of our algorithm.

## 5. Limitation & Conclusion

**Limitation.** Our EXGC mitigates redundancy in the synthetic graph during training process without benefiting inference speed in downstream tasks. Moving forward, we aim to refine our algorithm to directly prune redundant nodes from the initialization of the synthetic graph, enabling simultaneous acceleration of both training and application phases. Additionally, we hope to adopt more advanced explainers in the future to better probe the performance boundaries.

**Conclusion.** In this work, we pinpoint two major reasons for the inefficiency of current graph condensation methods, _i.e._, the concurrent updating of a vast parameter set and the pronounced parameter redundancy. To address these limitations, we first employ the MeanField variational approximation for convergence acceleration and then incorporate the leading explanation techniques (_e.g._, GNNExplainer and GSAT) to select the important nodes in the training process. Based on these, we propose our EXGC, the efficient and explainable graph condensation method, which can markedly boost efficiency and inject explainability.

\begin{table}
\begin{tabular}{c c c c c c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{4}{c}{**EXGC**} & \multicolumn{2}{c}{**EXDO**} \\ \cline{2-7}  & **Cora** & **Citeseer** & **Ogbn-Araiv** & **DD** & **CIFAR10** & **Ogbg-molhiv** \\ \hline Random & 7.24s & 2.84s & 7.32s & 0.87s & 2.07s & 0.32s \\ SA & 6.89s & 2.67s & 6.31s & 0.59s & 1.85s & 0.31s \\ GSAT & 10.37s & 4.50s & 9.45s & 0.76s & 2.61s & 0.34s \\ GNNExplainer & 11.62s & 5.97s & 11.23s & 0.99s & 2.80s & 0.42s \\ \hline \hline \end{tabular}
\end{table}
Table 4. Time comusing of different backbone explainers. We set the compress ratio of Cora, Citesser and Ogbn-Araiv as 0.4%, 0.3%, 0.05%, respectively. As for graph classification, we set DD, CIFAR10 and Ogbg-molhiv as 0.2%, 0.1% and 0.01%. All displayed results should be multiplied by 100.

Figure 4. Performance comparison across six benchmarks under various explanation methods.

\begin{table}
\begin{tabular}{c c c c c c} \hline \hline \multirow{2}{*}{**Method**} & \multicolumn{2}{c}{**GCond (backbone=GCN)**} & \multicolumn{2}{c}{**EXGC**} \\ \cline{2-7}  & **APPNP** & **SGC** & **SAGE** & **APPNP** & **SGC** & **SAGE** \\ \hline Cora & 69.32s4.26 & 67.95s46.10 & 60.34s4.83 & 75.17s3.74 & 0.42s4.88 & 66.49s4.25 \\ Citeseer & 61.27s5.80 & 62.43s4.52 & 61.74s5.01 & 67.34s3.83 & 68.58s4.42 & 66.62s4.17 \\ Ogbn-Araiv & 58.50\({}_{1.66}\) & 59.11\({}_{1.13}\) & 59.04\({}_{1.13}\) & 59.37s.089 & 60.07s1.82 & 58.72s0.99 \\ Flicker & 45.94s2.37 & 45.82s3.73 & 43.62s4.65 & 44.06s1.72 & 46.51s2.18 & 45.10e.23 \\ Reddit & 85.42s1.76 & 87.33s2.97 & 84.80\({}_{1.34}\) & 87.46s2.73 & 36.10\({}_{1.15}\) & 87.59\({}_{2.92}\) \\ \hline \hline \end{tabular}
\end{table}
Table 5. Transferability of condensed graphs from the different architectures. Test performance across three popular GNN backbones, _i.e._, APPNP, SGC and GraphSAGE using 2-layer GCN as training setting is exhibited.

## References

* F. Baldassarre and H. Azizpour (2019)Explainability techniques for graph convolutional networks. CoRRabs/1905.13866. External Links: Link, 1905.13866 Cited by: SS1.
* M. Belkin, P. Niyogi, and V. Smidhwani (2006)Manifold regularization: a geometric framework for learning from labeled and unlabeled examples. Journal of machine learning research71 (1). Cited by: SS1.
* C. M. Bishop and N. M. Nasrabadi (2006)Pattern recognition and machine learning. Vol. 4, Springer. Cited by: SS1.
* G. Cazenavehte, T. Wang, A. Torralba, A. A. Efros, and J. Zim (2022)Dataset distillation by ducking training trajectories. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, pp. 10708-10717. Cited by: SS1.
* J. Chen, T. Ma, and C. Xiao (2018)Fastgcn: fast learning with graph convolutional networks via importance sampling. arXiv preprint arXiv:1801.10247. Cited by: SS1.
* T. Chen, Y. Sui, X. Chen, A. Zhang, and Z. Wang (2021)A unified lottery ticket hypothesis for graph neural networks. In International Conference on Machine Learning, pp. 1695-1706. Cited by: SS1.
* T. M. Cover and J. A. Thomas (2001)Elements of information theory. Cited by: SS1.
* A. P. Dempster (1977)Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society (397). Cited by: SS1.
* K. Duan, Z. Liu, F. Wang, W. Zheng, K. Zhou, T. Chen, X. Hu, and Z. Wang (2022)A comprehensive study on large-scale graph training: benchmarking and rethinking. arXiv preprint arXiv:2210.07494. Cited by: SS1.
* V. Prakash Dwivedi, C. J. Joshi, A. Tuan Luu, T. Laurent, T. Bengio, and X. Bresson (2020)Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982. Cited by: SS1.
* Y. Eden, S. Jain, A. Pinar, Z. Ron, and C. Gesakhut (2018)Provable and practical approximations for the degree distribution using sublinear graph samples. In Proceedings of the 2018 World Wide Web Conference, pp. 449-458. Cited by: SS1.
* W. Fan, Y. Ma, Q. Li, Y. T. He, E. Zhao, J. Tang, and D. Yin (2019)Graph neural networks for social recommendation. In The world wide web conference, pp. 417-426. Cited by: SS1.
* J. Fang, X. Wang, A. Zhang, Z. Liu, X. He, and T. Chua (2023)Coperative explanations of graph neural networks. In WSDM, Cited by: SS1.
* R. Farhali and M. Carbin (2018)The lottery ticket hypothesis: finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635. Cited by: SS1.
* J. Frankle, G. Karolina, D. Roy, and M. Carbin (2019)Stabilizing the lottery ticket hypothesis. arXiv preprint arXiv:1903.01611. Cited by: SS1.
* H. Gao and S. Ji (2019)Graph u-nets. In International conference on machine learning, pp. 2063-2092. Cited by: SS1.
* A. Gupta, P. Matus, and B. Pant (2021)Graph neural network: current state of art, challenges and applications. Materials Today: Proceedings46, pp. 10927-10932. Cited by: SS1.
* W. Hamilton, Z. Ying, and J. Leskovec (2017)Inductive representation learning on large graphs. In Advances in Neural Information Processing Systems, pp. 1024-1034. Cited by: SS1.
* P. He, Y. Li, and C. Xu (2022)Graph-based graph neural networks for graph neural networks. In Proceedings of the 2021 IEEE International Conference on Big Data, pp. 931-941. Cited by: SS1.
* W. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta, and J. Leskovec (2020)Open graph benchmark: datasets for machine learning on graphs. arXiv preprint arXiv:2005.00687. Cited by: SS1.
* S. Ji, S. Pan, F. Cambria, P. Martin, and S. Yu (2021)A survey on knowledge graphs: representation, acquisition, and applications. IEEE transactions on neural networks and learning systems33 (2), pp. 494-514. Cited by: SS1.
* W. Jin, X. Tang, H. Jiang, Z. Li, D. Zhang, J. Tang, and B. Yin (2022)Condensing graphs via one-step gradient matching. In KDD, pp. 720-730. Cited by: SS1.
* W. Jin, L. Zhao, S. Zhang, Y. Liu, J. Tang, and N. Shah (2022)Graph condensation for graph neural networks. In ICLR, Cited by: SS1.
* T. N. Kipf and M. Welling (2017)Semi-supervised classification with graph convolutional networks. In Proceedings of the 5th International Conference on Learning Representations, External Links: Link Cited by: SS1.
* J. Kipera, A. Bojchevski, and S. Gunnemann (2019)Predict the propagate: graph neural networks meet personalized PageRank. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019, Cited by: SS1.
* J. Lee, I. Lee, and J. Kang (2019)Self-attention graph pooling. In International conference on machine learning, pp. 3734-3743. Cited by: SS1.
* G. Li, C. Xiong, A. Thabet, and B. Ghanem (2020)DeepGcn: all you need to train deeper gcnrs. arXiv preprint arXiv:2006.07739. Cited by: SS1.
* C. Liu, X. Ma, Y. Zhang, L. Ding, D. Tao, B. Du, W. Hu, and D. P. Madz (2023)Compressing graph gradual pruning for sparse training in graph neural networks. IEEE Transactions on Neural Networks and Learning Systems. Cited by: SS1.
* G. Liu, T. Zhao, J. Xu, T. Luo, and M. Jiang (2022)Graph rationalization with environment-based augmentations. In KDD, pp. 1069-1078. Cited by: SS1.
* D. Luo, W. Cheng, D. Xu, W. Yu, B. Zong, H. Chen, and X. Zhang (2020)Parameterized explainer for graph neural network. In NeurIPS, Cited by: SS1.
* S. Miao, M. Liu, and P. Li (2022)Interpretable and generalizable graph learning via stochastic attention mechanism. In ICML, pp. 15524-15543. Cited by: SS1.
* T. K. Moon (1996)The expectation-maximization algorithm. IEEE Signal Process. Mag.13, pp. 47-60. Cited by: SS1.
* C. Morris, N. M. Kriege, F. Rause, K. Kersting, P. Mutset, and M. Neumann (2020)TUDataset: a collection of benchmark datasets for learning with graphs. CoRRabs/2007.08663. External Links: Link, 2007.08663 Cited by: SS1.
* T. Nguyen, R. Novak, L. Xiao, and J. Lee (2021)Dataset distillation with infinitely wide convolutional networks. Advances in Neural Information Processing Systems34, pp. 5186-5198. Cited by: SS1.
* M. Khandern and A. Azad (2022)Triple sparsification of graph convolutional networks without sacrificing the accuracy. arXiv preprint arXiv:2208.05359. Cited by: SS1.
* K. Rajan, S. Sanyal, and P. Talukdar (2020)Asp: adaptive structure aware pooling for learning hierarchical graph representations. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34, pp. 5470-5477. Cited by: SS1.
* K. R. Roy, A. Mabhopur Rahman, M. Ashrafani, A. Ahsan Ali, Z. Structure-Aware Hierarchical Graph Pooling using Information Bottleneck. In 2021 International Joint Conference on Neural Networks (IJCNN), pp. 1-8. Cited by: SS1.
* M. S. Schlichtkrull, N. De Cao, and I. Titov (2021)Interpreting graph neural networks for NLP with differentiable edge masking. In ICLR, Cited by: SS1.
* R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra (2017)Grad-cam: visual explanations from deep networks via gradient-based localization. In IEEE International Conference on Computer Vision, ICCV 2017, pp. 618-626. Cited by: SS1.
* Y. Sui, X. Wang, T. Chen, X. He, and T. Chua (2022)Inductive lottery ticket learning for graph neural networks. Cited by: SS1.
* L. Toader, A. Una, A. Mussari, and A. Isupu (2019)Graphless: toward severely graphic processing. In 2019 18th International Symposium on Parallel and Distributed Computing (SPDCG), pp. 6-73. Cited by: SS1.
* F. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio (2017)Graph attention networks. stat1050. Cited by: SS1.
* K. Wang, Y. Liang, P. Wang, W. Pengfei, G. Fang, and Y. Wang (2023)Searching lottery tickets in graph neural networks: a dual perspective. In The Eleventh International Conference on Learning Representations, Cited by: SS1.
* K. Wang, Z. Zhou, X. Wang, P. Wang, Q. Fang, and Y. Wang (2022)AZDJP: a two graph-based component fused learning framework for urban anomaly distribution and duration joint-prediction. IEEE Transactions on Knowledge and Data Engineering. Cited by: SS1.
* T. Wang, J. Zhu, A. Torralba, and A. A. Efros (2018)Dataset distillation. arXiv preprint arXiv:1811.109908. Cited by: SS1.
* Y. Wang, S. Liu, K. Chen, T. Zhu, J. Qiao, M. Shi, Y. Wan, and M. Song (2023)Adversarial erasing with pruned elements: towards better graph lottery ticket. arXiv preprint arXiv:2308.02619. Cited by: SS1.
* M. Welling (2009)Herding dynamical weights to learn. In Proceedings of the 26th Annual International Conference on Machine Learning, pp. 1121-1128. Cited by: SS1.
* F. Wu, A. Souza, T. Zhang, C. Fifty, L. Yu, and K. Weinberger (2019)Simplifying graph convolutional networks. In International conference on machine learning, pp. 6861-6871. Cited by: SS1.
* J. Wu, X. Chen, K. Xu, and S. Li (2022)Structural entropy guided graph hierarchical pooling. In International Conference on Machine Learning, pp. 24017-24030. Cited by: SS1.
* S. Wu, Y. Wu, F. Zhang, X. Xie, and B. Cui (2022)Graph neural networks in recommender systems: a survey. Comput. Surveys55 (5), pp. 1-37. Cited by: SS1.
* S. Wu, Y. Tang, Y. Zhu, L. Wang, X. Xie, and T. Tan (2019)Session-based recommendation with graph neural networks. In Proceedings of the AAAI conference on artificial intelligence, Vol. 33, pp. 346-353. Cited by: SS1.
* T. Wu, H. Ren, P. Li, and J. Leskovec (2020)Graph information bottleneck. In NeurIPS, Cited by: SS1.
* Y. Wu, X. Wang, A. Zhang, X. He, and T. Chua (2022)Discovering invariant rationalness for graph neural networks. CoRRabs/2201.12872. Cited by: SS1.

* Wu et al. (2020) Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. 2020. A comprehensive survey on graph neural networks. _IEEE transactions on neural networks and learning systems_ 23, 1 (2020), 4-24.
* Xu et al. (2019) Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How Powerful are Graph Neural Networks?. In _International Conference on Learning Representations_.
* Ying et al. (2019) Zhitao Ying, Dylan Bourgeois, Jiaxuan You, Mariuka Zitnik, and Jure Leskovec. 2019. GNNE2Diner: Generating Explanations for Graph Neural Networks. In _NeurIPS_, 9240-9251.
* Ying et al. (2018) Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. 2018. Hierarchical graph representation learning with differentiable pooling. _Advances in neural information processing systems_ 31 (2018).
* You et al. (2022) Haoran You, Zhixuan Lu, Zijian Zhou, Yonggan Fu, and Yingyan Lin. 2022. Early-bird gens: Graph-network co-optimization towards more efficient green training and inference via drawing only-bird lottery tickets. In _Proceedings of the AAAI Conference on Artificial Intelligence_, Vol. 36. 8910-8918.
* You et al. (2020) Yuming You, Tianheng Chen, Zhangyang Wang, and Yang Shen. 2020. L2-gcn: Layer-wise and learned efficient training of graph convolutional networks. In _Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition_. 2127-2135.
* Yu et al. (2021) Junchi Yu, Tingyang Xu, Yu Rong, Yatao Bian, Junzhou Huang, and Ran He. 2021. Graph Information Bottleneck for Subgraph Recognition. In _ICLR_. OpenReviewer.net.
* Yue et al. (2020) Xiang Yue, Zhen Wang, Jingong Huang, Srinivasan Parthasarathy, Sbeili Mooavinasnah, Yungui Huang, Simon M Lin, Wen Zhang, Ping Zhang, and Huan Sun. 2020. Graph embedding on biomedical networks: methods, applications and evaluations. _Bioinformatics_ 36, 4 (2020), 1241-1251.
* Zeng et al. (2019) Hanqing Zeng, Hongbun Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. 2019. GraphSINT: Graph Sampling Based Inductive Learning Method. _arXiv preprint arXiv:1907.04891_ (2019).
* Zhang et al. (2021) Shichang Zhang, Yonen Liu, Yizhou Sun, and Neil Shah. 2021. Graph-less neural networks: Teaching old mlps new tricks via distillation. _arXiv preprint arXiv:2110.08727_ (2021).
* Zhang et al. (2021) Zhenyao Zhang, Xinxi Chen, Tianlong Chen, and Zhangyang Wang. 2021. Efficient lottery ticket finding: Less data is more. In _International Conference on Machine Learning_. PMLR, 12380-12390.
* Zhao and Balen (2021) Bo Zhao and Hakan Balen. 2021. Dataset condensation with differentiable siamese augmentation. In _International Conference on Machine Learning_. PMLR, 12674-12685.
* Zhao and Balen (2023) Bo Zhao and Hakan Balen. 2023. Dataset condensation with distribution matching. In _Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision_. 6514-6523.
* Zhou et al. (2005) Dengyong Zhou, Jiaxuan Huang, and Bernhard Scholkopf. 2005. Learning from labeled and unlabeled data on a directed graph. In _Proceedings of the 22nd international conference on Machine learning_. 1036-1043.
* Zhou et al. (2020) Jie Zhou, Ganqu Cui, Shengjing Hu, Zhenyuan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. 2020. Graph neural networks: A review of methods and applications. _AI open_ 10 (2020), 57-81.
* Zhou et al. (2021) Zhengyang Zhou, Yang Wang, Xike Xie, Lei Qiao, and Yuantao Li. 2021. STU-aNet: Understanding uncertainty in spatiotemporal collective human mobility. In _Proceedings of the Web Conference 2021_. 1868-1879.

## Appendix A Related Work

**Graph neural networks (GNNs).** GNNs (Glorot and Bengio, 2010; Bengio et al., 2011; Goodfellow et al., 2014; Bengio et al., 2014) handle variable-sized, permutation-invariant graphs and learn low-dimensional representations through an iterative process that involves transferring, transforming, and aggregating representations from topological neighbors. Though promising, GNNs encounter significant inefficiencies when scaled up to large or dense graphs (Zhu et al., 2019). To address this challenge, existing research lines prominently focus on **graph sampling** and **graph distillation** as focal points for enhancing computational efficiency.

**Graph Sampling & Distillation.** Graph sampling alleviates the computational demands of GNNs by selectively sampling sub-graphs or employing pruning techniques (Bengio et al., 2011; Goodfellow et al., 2014; Bengio et al., 2014; Goodfellow et al., 2014; Goodfellow et al., 2014; Goodfellow et al., 2014; Goodfellow et al., 2014). Nevertheless, aggressive sampling strategies may precipitate significant information loss, potentially diminishing the representational efficacy of the sampled subset. In light of this, the research trajectory of graph distillation (Zhu et al., 2019; Goodfellow et al., 2014; Goodfellow et al., 2014) is influenced by **dataset distillation** (DD), which endeavors to distill (compress) the embedded knowledge within raw data into synthetic counterparts, ensuring that models trained on this synthetic data retain performance (Bengio et al., 2011; Goodfellow et al., 2014; Goodfellow et al., 2014; Goodfellow et al., 2014). Recently, within the domain of graph distillation, the notion of graph condensation (Zhu et al., 2019; Goodfellow et al., 2014) via training gradient matching serves to compress the original graph into an informative and synthesized set, which also resides within the scope of our endeavor.

**Graph Lottery Ticket (GLT) Hypothesis.** The Lottery Ticket Hypothesis (LTH) articulates that a compact, efficacious subnetwork can be discerned from a densely connected network via an iterative pruning methodology (Bengio et al., 2011; Goodfellow et al., 2014; Goodfellow et al., 2014). Drawing inspiration from the concepts of LTH, (Goodfellow et al., 2014) pioneered in amalgamating the concept of _graph sampling_ with _GNN pruning_, under the umbrella of Graph Lottery Ticket (GLT) research trajectory. Precisely, GLT is conceptualized as a coupling of pivotal core subgraphs and a sparse sub-network, which can be collaboratively extracted from the comprehensive graph and the primal GNN model. The ensuing amplification of GLT theory (Goodfellow et al., 2014), coupled with the advent of novel algorithms (Zhu et al., 2019; Goodfellow et al., 2014; Goodfellow et al., 2014; Goodfellow et al., 2014), has significantly enriched the graph pruning research narrative, delineating GLT as a prominent cornerstone in this field.

**Explainable Graph Learning.** Another line of research intimately related to our work delves into the explainability of graph learning. This line aims to reveal the black-box of the decision-making process by identifying salient subgraphs named _rationales_. Specifically, IB (Ioffe and Szegedy, 2015) and IB-subgraph (Goodfellow et al., 2014) transplant the concept of information bottlenecks into graph learning to pinpoint compact yet informative subgraphs; GSAT (Shi et al., 2016) utilizes a stochastic attention mechanism to assign a probability to each edge in the input graph, determining whether it should be chosen as part of the explanatory subgraphs; DIR (Shi et al., 2016), based on causal inference, removes the label-irrelevant features from the original graph and treating the remaining portion as the graph's rationale; GREA (Shi et al., 2016) employs data augmentation methods based on both removal and replacement to target the original graph, aiming to explore the salient nodes.

## Appendix B Derivation of the Equation 4 and 5

In this section we detail the derivation process of the Equation 4 and Equation 5 in Section 3.1. Specifically, to find a optimal model parameters \(\Phi\), the objective can be formulated as:

\[\Phi=\arg\max_{\Phi}\log P(\nabla_{\theta}\mid\Phi). \tag{26}\]

We define the value of this logarithm as \(F(\Phi)\) and rewrite it:

\[F(\Phi)=\log\sum_{\mathbf{X}^{\prime}}P\left(\mathbf{X}^{\prime}\right)\frac{ P\left(\mathbf{X}^{\prime},\nabla_{\theta}\mid\Phi\right)}{P\left(\mathbf{X}^{ \prime}\right)}, \tag{27}\]

then we can derive a low bound of \(F(\Phi)\) according to the Jensen Inequality:

\[F(\Phi)\geq L(\Phi)=\sum_{\mathbf{X}^{\prime}}P\left(\mathbf{X}^{\prime}\right) \log\frac{P\left(\mathbf{X}^{\prime},\nabla_{\theta}\mid\Phi\right)}{P\left( \mathbf{X}^{\prime}\right)}, \tag{28}\]

where \(L(\Phi)\) is the Variational Lower Bound of our objective.

To maximize the objective of \(L(\Phi)\), we endeavour to derive the gap between \(L(\Phi)\) and \(F(\Phi)\) following:

\[L(\Phi)=\sum_{\mathbf{X}^{\prime}}P\left(\mathbf{X}^{\prime}\right) \log\frac{P\left(\nabla_{\theta},\mathbf{X}^{\prime}|\Phi\right)}{P\left(\mathbf{ X}^{\prime}\right)} \tag{29}\] \[=\sum_{\mathbf{X}^{\prime}}P\left(\mathbf{X}^{\prime}\right)\log \frac{P\left(\mathbf{X}^{\prime}\mid\nabla_{\theta},\Phi\right)P\left(\nabla_{ \theta}|\Phi\right)}{P\left(\mathbf{X}^{\prime}\right)}\] \[=\log P\left(\nabla_{\theta}|\Phi\right)-\sum_{\mathbf{X}^{\prime }}P\left(\mathbf{X}^{\prime}\right)\ln\frac{P\left(\mathbf{X}^{\prime}\right)} {P\left(\mathbf{X}^{\prime}\mid\nabla_{\theta},\Phi\right)}\] \[=F(\Phi)-KL(P(\mathbf{X}^{\prime})\|p(\mathbf{X}^{\prime}\mid \nabla_{\theta};\Phi)).\]

It is well known that the KL divergence is non-negative. Therefore, with \(\Phi\) fixed and optimizing \(\mathbf{X}^{\prime}\), maximizing this lower bound is equivalent to:

\[P(\mathbf{X}^{\prime})\gets P(\mathbf{X}^{\prime}|\nabla_{\theta},\Phi). \tag{30}\]

Moreover, maximizing \(L(\Phi)\) is equivalent to maximizing the ELBO:

\[\text{ELBO}\to E_{\mathbf{X}^{\prime}|\nabla_{\theta},\Phi}[\log \frac{P(\mathbf{X}^{\prime},\nabla_{\theta}\mid\Phi)}{P(\mathbf{X}^{\prime} \mid\nabla_{\theta},\Phi)}] \tag{31}\]

by maximizing the conditional probability expectation following:

\[\Phi =\operatorname*{arg\,max}_{\Phi}L(\Phi) \tag{32}\] \[=\operatorname*{arg\,max}_{\Phi}\sum_{\mathbf{X}^{\prime}}P\left( \mathbf{X}^{\prime}\mid\nabla_{\theta},\Phi\right)\ln\left\{\frac{p(\nabla_{ \theta},\mathbf{X}^{\prime}\mid\Phi)}{p\left(\mathbf{X}^{\prime}\mid\nabla_{ \theta},\Phi\right)}\right\}\] \[=\operatorname*{arg\,max}_{\Phi}\left(\sum_{\mathbf{X}^{\prime}} p\left(\mathbf{X}^{\prime}\mid\nabla_{\theta},\Phi\right)\ln p(\nabla_{\theta}, \mathbf{X}^{\prime}\mid\Phi)-\right.\] \[\underbrace{\sum_{\mathbf{X}^{\prime}}p\left(\mathbf{X}^{\prime} \mid\nabla_{\theta},\Phi\right)\ln p\left(\mathbf{X}^{\prime}\mid\nabla_{ \theta},\Phi\right)}_{const}),\]

which is exactly what the current M-step does.

## Appendix C Derivation of the MF approximation

Let's start with Equation 7 to derive the optimized E-step based on mean-filed approximation. Specifically, by substituting Equation 6 into the ELBO in Equation 4 we obtain:

\[\text{ELBO}= \int\prod_{i=1}^{N}P\left(x_{i}^{\prime}\right)\log P(\nabla_{ \theta},\mathbf{X}^{\prime})d\mathbf{X}^{\prime} \tag{33}\] \[-\int\prod_{i=1}^{N}P\left(x_{i}^{\prime}\right)\log\prod_{i=1}^{ N}P\left(x_{i}^{\prime}\right)d\mathbf{X}^{\prime}.\]

For simplicity, let's make some variable assumptions below:

\[\mathcal{A}= \prod_{i=1}^{N^{\prime}}P\left(x_{i}^{\prime}\right)\log P( \mathbf{X}^{\prime},\nabla_{\theta})d\mathbf{X}^{\prime} \tag{34}\] \[\mathcal{B}= \int\prod_{i=1}^{N^{\prime}}P\left(x_{i}^{\prime}\right)\log \prod_{i=1}^{N^{\prime}}P\left(x_{i}^{\prime}\right)d\mathbf{X}^{\prime}.\]

In this case, the ELBO can be rewritten as:

\[\text{ELBO}=\mathcal{A}-\mathcal{B}. \tag{35}\]

Before deriving \(\mathcal{A}\) and \(\mathcal{B}\), we first fix the complementary set of \(x_{j}^{\prime}\), _i.e._, \(\mathbf{X}^{\prime}\backslash_{j}=\{x_{1}^{\prime},...x_{j-1}^{\prime},x_{j+1} ^{\prime},...,x_{N^{\prime}}^{\prime}\}\). Then \(\mathcal{A}\) is equal to:

\[\mathcal{A}=\int P\left(x_{j}^{\prime}\right)\int\prod_{i=1,i\neq j}^{N^{ \prime}}P\left(x_{i}^{\prime}\right)\log P(\nabla_{\theta},\mathbf{X}^{\prime })d_{i\neq j}x_{i}^{\prime}dx_{j}^{\prime}, \tag{36}\]

where:

\[\int\prod_{i=1,i\neq j}^{N^{\prime}}P\left(x_{i}^{\prime}\right)\log P( \mathbf{X}^{\prime},\nabla_{\theta})d_{i\neq j}x_{i}^{\prime}=E_{\prod_{i=1,i \neq j}^{N^{\prime}}P\left(x_{i}^{\prime}\right)}[\log P(\mathbf{X}^{\prime}, \nabla_{\theta})]. \tag{37}\]

By substituting Equation 37 into the Equation 36 we obtain:

\[\mathcal{A}=\int P\left(x_{j}^{\prime}\right)\cdot E_{\prod_{i=1,i\neq j}^{N^{ \prime}}P\left(x_{i}^{\prime}\right)}[\log P(\mathbf{X}^{\prime},\nabla_{ \theta})]d\mathbf{x}_{j}^{\prime}. \tag{38}\]

Next we focus on the term \(\mathcal{B}\). We first rewritten it following:

\[\mathcal{B}=\int\prod_{i=1}^{N^{\prime}}P\left(x_{i}^{\prime}\right)\cdot[\log P \left(x_{1}^{\prime}\right)+\log P\left(x_{2}^{\prime}\right)+\cdots+\log P \left(x_{N^{\prime}}^{\prime}\right)]\,d\mathbf{X}^{\prime}. \tag{39}\]

Note that for each terms in \(\mathcal{B}\) we have:

\[\int\prod_{i=1}^{N^{\prime}}P\left(x_{i}^{\prime}\right)\cdot\log P\left(x_{1} ^{\prime}\right)=\int P\left(x_{1}^{\prime}\right)\log P\left(x_{1}^{\prime} \right)d\mathbf{x}_{1}^{\prime}. \tag{40}\]

Hence, the value of \(\mathcal{B}\) can be simplified as:

\[\mathcal{B}=\sum_{i=1}^{N^{\prime}}\int P\left(x_{i}^{\prime}\right)\log P\left( x_{i}^{\prime}\right)dx_{i}^{\prime}. \tag{41}\]

Since \(\mathbf{X}_{\backslash j}\) is fixed, we can separate out the constants \(C\) from \(\mathcal{B}\):

\[\mathcal{B}=\int P\left(x_{j}^{\prime}\right)\log P\left(x_{j}^{\prime}\right) dx_{j}^{\prime}+C. \tag{42}\]

Combining the expression for \(\mathcal{A}\) mentioned above, we can obtain a new form of ELBO:

\[\text{ELBO}=\mathcal{A}-\mathcal{B}=\int P\left(x_{j}^{\prime} \right)\log\frac{E_{\prod_{i=1,i\neq j}^{N^{\prime}}P\left(x_{j}\right)}[ \log P(\mathbf{X}^{\prime},\nabla_{\theta})]}{P\left(x_{j}^{\prime}\right)}dx_{j}^{\prime} \tag{43}\] \[=-KL\left(P(x_{j}^{\prime})\|\log E_{\prod_{i=1,i\neq j}^{N^{ \prime}}P\left(x_{j}^{\prime}\right)}[\log P(\mathbf{X}^{\prime},\nabla_{ \theta})]\right)\leq 0.\]

Therefore, when KL is equal to 0, ELBO can reach its maximum value, so the value of \(P\left(x_{j}^{\prime}\right)\) derived here is:

\[P\left(x_{j}^{\prime}\right)=E_{\prod_{i=1,i\neq j}^{N^{\prime}}P\left(x_{j}^{ \prime}\right)}[\log P(\mathbf{X}^{\prime},\nabla_{\theta})]. \tag{44}\]

The methods for solving for distributions of \(P\left(x_{i}^{\prime}\right)\) for \(i\in\{1,...,j-1,j+1,...,N^{\prime}\}\) are the same.

## Appendix D Derivation of the Gdib

In this section we focus on the detailed derivation process in Section 3.4, which mainly contribute to the instantiation process of GDIB:

\[\operatorname*{arg\,max}_{Sub}I\left(\mathcal{S}_{sub};\nabla_{\theta}^{\prime} \right)-\beta I\left(\mathcal{S}_{sub};\mathcal{S}\right),\,\text{s.t.}\,\, \mathcal{S}_{sub}\in\mathbb{G}_{sub}(\mathcal{S}). \tag{45}\]At first, for the first term in GDIB, _i.e._, \(I\left(\mathcal{S}_{sub};\nabla^{\prime}_{\theta}\right)\), by definition:

\[\begin{split} I(\mathcal{S}_{sub};\nabla^{\prime}_{\theta})=H( \nabla^{\prime}_{\theta})-H(\nabla^{\prime}_{\theta}\mid\mathcal{S}_{sub})\\ =E\nabla_{\theta^{\prime}}S_{sub}\left[\log\frac{P\left(\mathcal{ S}_{sub}\mid\nabla^{\prime}_{\theta}\right)}{P\left(\nabla^{\prime}_{\theta} \right)}\right].\end{split} \tag{46}\]

Since \(P\left(\mathcal{S}_{sub}\mid\nabla^{\prime}_{\theta}\right)\) is intractable, an variational approximation \(Q\left(\mathcal{S}_{sub}\mid\nabla^{\prime}_{\theta}\right)\) is introduced for it. Then the LBO of the \(I(\nabla^{\prime}_{\theta^{\prime}},\mathcal{S}_{sub})\) can be obtained following:

\[\begin{split} I\left(\mathcal{S}_{sub};\nabla^{\prime}_{\theta} \right)=& E_{\mathcal{S}_{sub},\nabla^{\prime}_{\theta}}\left[ \log\frac{Q\left(\nabla^{\prime}_{\theta}\mid\mathcal{S}_{sub}\right)}{P( \nabla^{\prime}_{\theta})}\right]\\ &+E_{\mathcal{S}_{sub},\nabla^{\prime}_{\theta}}\left[\log\frac{ P\left(\nabla^{\prime}_{\theta}\mid\mathcal{S}_{sub}\right)}{Q\left(\nabla^{ \prime}_{\theta}\mid\mathcal{S}_{sub}\right)}\right]\\ =& E_{\mathcal{S}_{sub}}\left[\operatorname{KL} \left(Q\left(\nabla^{\prime}_{\theta}|\mathcal{S}_{sub}\right)\|\,P\left( \nabla^{\prime}_{\theta}\right)\right)\right]\\ &+E_{\mathcal{S}_{sub}}\left[\operatorname{KL}\left(P\left(\nabla ^{\prime}_{\theta}|\mathcal{S}_{sub}\right)\|\,Q\left(\nabla^{\prime}_{\theta }|\mathcal{S}_{sub}\right)\right)\right]\\ \geq&\underbrace{E_{\mathcal{S}_{sub}}\left[ \operatorname{KL}\left(Q\left(\nabla^{\prime}_{\theta}|\mathcal{S}_{sub} \right)\|\,P\left(\nabla^{\prime}_{\theta}\right)\right)\right]}_{LBO}.\end{split} \tag{47}\]

Then, for the second term in GDIB, _i.e._, \(I\left(\mathcal{S}_{sub};\nabla^{\prime}_{\theta}\right)\), by definition:

\[\begin{split} I(\mathcal{S},\mathcal{S}_{sub})=H(\mathcal{S})-H (\mathcal{S}\mid\mathcal{S}_{sub})\\ =E_{\mathcal{S},\mathcal{S}_{sub}}\left[\log\frac{P\left(\mathcal{ S}_{sub}\mid\mathcal{S}\right)}{P\left(\mathcal{S}_{sub}\right)}\right].\end{split} \tag{48}\]

Considering that \(P(\mathcal{S}_{sub})\) is intractable, an variational approximation \(R(\mathcal{S}_{sub})\) is introduced for the marginal distribution \(P(\mathcal{S}_{sub})=\sum_{\mathcal{S}}P\left(\mathcal{S}_{sub}\mid\mathcal{S }\right)P(\mathcal{S})\). Then the UBO of the \(I(\mathcal{S},\mathcal{S}_{sub})\) can be obtained following:

\[\begin{split} I(\mathcal{S},\mathcal{S}_{sub})=& E_{ \mathcal{S}_{sub},\mathcal{S}}\left[\log\frac{P\left(\mathcal{S}_{sub}\mid \mathcal{S}\right)}{R\left(\mathcal{S}_{sub}\right)}\right]-\operatorname{KL} \left(P\left(\mathcal{S}_{sub}\right)\|R\left(\mathcal{S}_{sub}\right)\right) \\ \leq&\underbrace{E_{\mathcal{S}}\left[\operatorname{KL }\left(P\left(\mathcal{S}_{sub}\mid\mathcal{S}\right)\|R\left(\mathcal{S}_{ sub}\right)\right)\right]}_{UBO}.\end{split} \tag{49}\]

The main paper presents an instantiation of \(P\left(\mathcal{S}_{sub}|\mathcal{S}\right)\) which assigns the importance score \(p_{i}\) (_i.e._, the probability of being selected into \(\mathcal{S}_{sub}\)) to the \(i\)-th node in \(\mathcal{S}\). Additionally, the distribution \(R\) is specified as a Bernoulli distribution with parameter \(r\) (_i.e._, each node is selected with probability \(r\)). This instantiation is consistent with the information constraint \(t_{I}\) proposed by GSAT [32], where \(r\) falls within the range of \((0,1)\), resulting in a collapse of the UBO to the \(t_{I}\):

\[t_{I}=\sum_{i\in 1,2,\ldots,N}p_{i}\log\frac{p_{i}}{r}+(1-p_{i})\log\frac{1-p_{i}} {1-r}. \tag{50}\]

When \(r\to 0\) we have:

\[p_{i}\log\frac{p_{i}}{r}>>(1-p_{i})\log\frac{1-p_{i}}{1-r}. \tag{51}\]

Then the Equation 50 collapses to:

\[t_{I}=\sum_{i\in 1}^{N^{*}}p_{i}\log\frac{p_{i}}{r}. \tag{52}\]

Since the value of Equation 52 is proportional to the value of \(p_{i}\), when \(r\to 0\), \(t_{I}\) can be instantiated as the \(l_{1}\)-norm of \(p_{i}\).