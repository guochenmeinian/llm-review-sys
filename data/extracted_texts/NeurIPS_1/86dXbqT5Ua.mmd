# Geometry-Informed Neural Operator

for Large-Scale 3D PDEs

Zongyi Li  Nikola Borislavov Kovachki  Chris Choy  Boyi Li  Jean Kossaifi

Shourya Prakash Otta  Mohammad Amin Nabian  Maximilian Stadler

Christian Hundt  Kamyar Azizzadenesheli  Anima Anandkumar

###### Abstract

We propose the geometry-informed neural operator (GINO), a highly efficient approach for learning the solution operator of large-scale partial differential equations with varying geometries. GINO uses a signed distance function (SDF) and point-cloud representations of the input shape and neural operators based on graph and Fourier architectures to learn the solution operator. The graph neural operator handles irregular grids and transforms them into and from regular latent grids on which Fourier neural operator can be efficiently applied. GINO is discretization-convergent, meaning the trained model can be applied to arbitrary discretizations of the continuous domain and it converges to the continuum operator as the discretization is refined. To empirically validate the performance of our method on large-scale simulation, we generate the industry-standard aerodynamics dataset of 3D vehicle geometries with Reynolds numbers as high as five million. For this large-scale 3D fluid simulation, numerical methods are expensive to compute surface pressure. We successfully trained GINO to predict the pressure on car surfaces using only five hundred data points. The cost-accuracy experiments show a \(26,000\) speed-up compared to optimized GPU-based computational fluid dynamics (CFD) simulators on computing the drag coefficient. When tested on new combinations of geometries and boundary conditions (inlet velocities), GINO obtains a one-fourth reduction in error rate compared to deep neural network approaches.

## 1 Introduction

Computational sciences aim to understand natural phenomena and develop computational models to study the physical world around us. Many natural phenomena follow the first principles of physics and are often described as evolution on function spaces, governed by partial differential equations (PDE). Various numerical methods, including finite difference and finite element methods, have been developed as computational approaches for solving PDEs. However, these methods need to be run at very high resolutions to capture detailed physics, which are time-consuming and expensive, and often beyond the available computation capacity. For instance, in computational fluid dynamics (CFD), given a shape design, the goal is to solve the Navier-Stokes equation and estimate physical properties such as pressure and velocity. Finding the optimal shape design often requires solving thousands of trial shapes, each of which can take more than ten hours even with GPUs .

To overcome these computational challenges, recent works propose deep learning-based methods, particularly neural operators , to speed up the simulation and inverse design. Neural operatorsgeneralize neural networks and learn operators, which are mappings between infinite-dimensional function spaces . Neural operators are discretization convergent and can approximate general operators . The input function to neural operators can be presented at any discretization, grid, resolution, or mesh, and the output function can be evaluated at any arbitrary point. Neural operators have shown promise in learning solution operators in partial differential equations (PDE)  with numerous applications in scientific computing, including weather forecasting , carbon dioxide storage and reservoir engineering , with a tremendous speedup over traditional methods. Prior works on neural operators developed a series of principled neural operator architectures to tackle a variety of scientific computing applications. Among the neural operators, graph neural operators (GNO) , and Fourier neural operators (FNO)  have been popular in various applications.

GNO implements kernel integration with graph structures and is applicable to complex geometries and irregular grids. The kernel integration in GNO shares similarities with the message-passing implementation of graph neural networks (GNN) , which is also used in scientific computing . However, the main difference is that GNO defines the graph connection in a ball defined on the physical space, while GNN typically assumes a fixed set of neighbors, e.g., k-nearest neighbors, see Figure 5. Such nearest-neighbor connectivity in GNN violates discretization convergence, and it degenerates into a pointwise operator at high resolutions, leading to a poor approximation of the ground-truth operator using GNN. In contrast, GNO adapts the graph based on points within a physical space, allowing for universal approximation of operators. However, one limitation of graph-based methods is the computational complexity when applied to problems with long-range global interactions. To overcome this, prior works propose using multi-pole methods or multi-level graphs  to help with global connectivity. However, they do not fully alleviate the problem since they require many such levels to capture global dependence, which still makes them expensive.

While GNO performs kernel integration in the physical space using graph operations, FNO leverages Fourier transform to represent the kernel integration in the spectral domain using Fourier modes. This architecture is applicable to general geometries and domains since the (continuous) Fourier transform can be defined on any domain. However, it becomes computationally efficient when applied to regular input grids since the continuous Fourier transform can then be efficiently approximated using discrete Fast Fourier transform (FFT) , giving FNO a significant quasi-linear computational complexity. However, FFT limits FNO to regular grids and cannot directly deal with complex geometries and irregular grids. A recent model, termed GeoFNO, learns a deformation from a given geometry to a latent regular grid  so that the FFT can be applied in the latent space. In order to transform the latent regular grid back to the irregular physical domain, discrete Fourier transform (DFT) on irregular grids is employed. However, DFT on irregular grids is more expensive than FFT, quadratic vs. quasi-linear, and does not approximate the Fourier transform in a discretization convergent manner. This is because, unlike in the regular setting, the points are not sampled at regular intervals, and therefore the integral does not take into account the underlying measure. Other attempts share a similar computational barrier as shown in Table 1, which we discussed in Section 5.

**In this paper**, we consider learning the solution operator for large-scale PDEs, in particular, 3D CFD simulations. We propose the geometry-informed neural operator (GINO), a neural operator

Figure 1: **The architecture of GINO. The input geometries are irregular and change for each sample. These are discretized into point clouds and passed on to a GNO layer, which maps from the given geometry to a latent regular grid. The output of this GNO layer is concatenated with the SDF features and passed into an FNO model. The output from the FNO model is projected back onto the domain of the input geometry for each query point using another GNO layer. This is used to predict the target function (e.g., pressure), which is used to compute the loss that is optimized end-to-end for training.**

architecture for arbitrary geometries and mesh discretizations. It uses a signed distance function (SDF) to represent the geometry and composes GNO and FNO architectures together in a principled manner to exploit the strengths of both frameworks.

The GNO by itself can handle irregular grids through graphs but is able to operate only locally under a limited computational budget, while the FNO can capture global interactions, but requires a regular grid. By using GNO to transform the irregular grid into a regular one for the FNO block, we can get the best of both worlds, i.e., computational efficiency and accuracy of the approximation. Thus, this architecture tackles the issue of expensive global integration operations that were unaddressed in prior works, while maintaining discretization convergence.

Specifically, GINO has three main components, \((i)\)**Geometry encoder**: multiple local kernel integration layers through GNO with graph operations, \((ii)\)**Global model**: a sequence of FNO layers for global kernel integration, and \((iii)\)**Geometry decoder**: the final kernel integral layers, as shown in Figure 1. The input to the GINO is the input surface (as a point cloud) along with the SDF, representing the distance of each 3D point to the surface. GINO is trained end-to-end to predict output (e.g., car surface pressure in our experiments), a function defined on the geometry surfaces.

**Geometry encoder:** the first component in the GINO architecture uses the surface (i.e., point cloud) and SDF features as inputs. The irregular grid representation of the surface is encoded through local kernel integration layers implemented with GNOs, consisting of local graphs that can handle different geometries and irregular grids. The encoded function is evaluated on a regular grid, which is concatenated with the SDF input evaluated on the same grid. **Global model:** the output of the first component is encoded on a regular grid, enabling efficient learning with an FNO using FFT. Our second component consists of multiple FNO layers for efficient global integration. In practice, we find that this step can be performed at a lower resolution without significantly impacting accuracy, giving a further computational advantage. **Geometry decoder:** the final component is composed of local GNO-based layers with graph operations, that decode the output of the FNO and project it back onto the desired geometry, making it possible to efficiently query the output function on irregular meshes. The GNO layers in our framework are accelerated using our GPU-based hash-table implementation of neighborhood search for graph connectivity of meshes.

We validate our findings on two large-scale 3D CFD datasets. We generate our own large-scale industry-standard Ahmed's body geometries using GPU-based OpenFOAM , composed of 500+ car geometries with \(O(10^{5})\) mesh points on the surface and \(O(10^{7})\) mesh points in space. Each simulation takes 7-19 hours on 16 CPU cores and 2 Nvidia V100 GPUs. Further, we also study a lower resolution dataset with more realistic car shapes, viz., Shape-Net car geometries generated by . GINO takes the point clouds and SDF features as the input and predicts the pressure fields on the surfaces of the vehicles. We perform a full cost-accuracy trade-off analysis. The result shows GINO is \(26,000\) faster at computing the drag coefficients over the GPU-based OpenFOAM solver, while achieving 8.31% (Ahmed-body) and 7.29% (Shape-Net car) error rates on the full pressure field. Further, GINO is capable of zero-shot super-resolution, training with only one-eighth of the mesh points, and having a good accuracy when evaluated on the full mesh that is not seen during training.

  
**Model** & **Range** & **Complexity** & **Irregular grid** & **Discretization convergent** \\  GNN & local & \(O(N)\) & ✔ & ✗ \\ CNN & local & \(O(N)\) & ✗ & ✗ \\ UNet & global & \(O(N)\) & ✗ & ✗ \\ Transformer & global & \(O(N^{2})\) & ✔ & ✔ \\ GNO (kernel) & radius \(r\) & \(O(N)\) & ✔ & ✔ \\ FNO (FFT) & global & \(O(N N)\) & ✗ & ✔ \\ GINO **[Ours]** & global & \(O(N N+N)\) & ✔ & ✔ \\   

Table 1: **Computational complexity of standard deep learning models. \(N\) is the number of mesh points; \(d\) is the dimension of the domain and degree is the maximum degree of the graph. Even though GNO and transformer both work on irregular grids and are discretization convergent, they become too expensive on large-scale problems.**Problem setting

We are interested in learning the map from the geometry of a PDE to its solution. We will first give a general framework and then discuss the Navier-Stokes equation in CFD as an example. Let \(D^{d}\) be a Lipschitz domain and \(\) a Banach space of real-valued functions on \(D\). We consider the set of distance functions \(\) so that, for each function \(T\), its zero set \(S_{T}=\{x D:T(x)=0\}\) defines a \((d-1)\)-dimensional sub-manifold. We assume \(S_{T}\) is simply connected, closed, smooth, and that there exists \(>0\) such that \(B_{}(x) D=\) for every \(x S_{T}\) and \(T\). We denote by \(Q_{T} D\), the open volume enclosed by the sub-manifold \(S_{T}\) and assume that \(Q_{T}\) is a Lipschitz domain with \( Q_{T}=S_{T}\). We define the Lipschitz domain \(_{T} D_{T}\) so that, \(_{T}= D S_{T}\). Let \(\) denote a partial differential operator and consider the problem

\[(u)&=f,_{T},\\ u&=g,_{T}, \]

for some \(f\), \(g\) where \(\), \(\) denote Banach spaces of functions on \(^{d}\) with the assumption that the evaluation functional is continuous in \(\). We assume that \(\) is such that, for any triplet \((T,f,g)\), the PDE (1) has a unique solution \(u_{T}\) where \(_{T}\) denotes a Banach space of functions on \(_{T}\). Let \(\) denote a Banach space of functions on \(D\) and let \(\{E_{T}:_{T}:T\}\) be a family of extension operators which are linear and bounded. We define the mapping from the distance function to the solution function

\[: \]

by \((T,f,g) E_{T}(u)\) which is our operator of interest.

Navier-Stokes Equation.We illustrate the above abstract formulation with the following example. Let \(D=(0,1)^{d}\) be the unit cube and let \(=C()\). We take \(\) to be some subset such that the zero level set of every element defines a \((d-1)\)-dimensional closed surface which can be realized as the graph of a Lipschitz function and that there exists \(>0\) such that each surface is at least distance \(\) away from the boundary of \(D\). We now consider the steady Naiver-Stokes equations,

\[- v+(v)v+ p&=f,_{T},\\  v&=0,_{T},\\ v&=q, D,\\ v&=0,S_{T}, \]

where \(v:_{T}^{d}\) is the velocity, \(p:_{T}\) is the pressure, \(\) is the viscosity, and \(f,q:^{d}^{d}\) are the forcing and boundary functions. The condition that \(v=0\) in \(S_{T}\) is commonly known as a "no slip" boundary and is prevalent in many engineering applications. The function \(q\), on the other hand, defines the inlet and outlet boundary conditions for the flow. We assume that \(f H^{-1}(^{d};^{d})\) and \(q C(^{d};^{d})\). We can then define our boundary function \(g C(^{d};^{d})\) such that \(g(x)=0\) for any \(x D\) with \((x, D)\) and \(g(x)=q(x)\) for any \(x D\) with \((x, D)>/2\) as well as any \(x D\). Continuity of \(g\) can be ensured by an appropriate extension for any \(x D\) such that \((x, D)<\) and \((x, D)/2\). We define \(u:_{T}^{d+1}\) by \(u=(v,p)\) as the unique weak solution of (3) with \(_{T}=H^{1}(_{T};^{d}) L^{2}(_{T})/ \). We define \(=H^{1}(D;^{d}) L^{2}(D)/\) and the family of extension operators \(\{E_{T}:_{T}\}\) by \(E_{T}(u)=E_{T}^{v}(v),E_{T}^{p}(p)\) where \(E_{T}^{v}:H^{1}(_{T};^{d}) H^{1}(D;^{d})\) and \(E_{T}^{p}:L^{2}(_{T})/ L^{2}(D)/\) are defined as the restriction onto \(D\) of the extension operators defined in [19, Chapter VI, Theorem 5]. This establishes the existence of the operator \(: H^{-1}(^{d};^{d}) C(^{d};^{d}) H^{1}(D;^{d}) L^{2}(D)/\) mapping the geometry, forcing, and boundary condition to the (extended) solution of the steady Navier-Stokes equation (3). Homomorphic extensions of deformation-based operators have been shown in . We leave for future work studying the regularity properties of the presently defined operator.

## 3 Geometric-Informed Neural Operator

We propose a geometry-informed neural operator (GINO), a neural operator architecture for varying geometries and mesh regularities. GINO is a deep neural operator model consisting of three main components, \((i)\) multiple local kernel integration layers, \((ii)\) a sequence of FNO layers for global kernel integration which precedes \((iii)\) the final kernel integral layers. Each layer of GINO follows the form of generic kernel integral of the form (5). Local integration is computed using graphs, while global integration is done in Fourier space.

### Neural operator

A neural operator \(\) maps the input functions \(a=(T,f,g)\) to the solution function \(u\). The neural operator \(\) is composed of multiple layers of point-wise and integral operators,

\[=_{L}_{1} . \]

The first layer \(\) is a pointwise operator parameterized by a neural network. It transforms the input function \(a\) into a higher-dimensional latent space \(:a v_{0}\). Similarly, the last layer acts as a projection layer, which is a pointwise operator \(:v_{l} u\), parameterized by a neural network \(Q\). The model consists of \(L\) layers of integral operators \(_{l}:v_{l-1} v_{l}\) in between.

\[v_{l}(x)=_{D}_{l}(x,y)v_{l-1}(y)y \]

where \(_{l}\) is a learnable kernel function. Non-linear activation functions are incorporated between each layer.

### Graph operator block

To efficiently compute the integral in equation (5), we truncate the integral to a local ball at \(x\) with radius \(r>0\), as done in ,

\[v_{l}(x)=_{B_{r}(x)}(x,y)v_{l-1}(y)\,y. \]

We discretize the space and use a Riemann sum to compute the integral. This process involves uniformly sampling the input mesh points and connecting them with a graph for efficient parallel computation. Specifically, for each point \(x D\), we randomly sample points \(\{y_{1},,y_{M}\} B_{r}(x)\) and approximate equation (6) as

\[v_{l}(x)_{i=1}^{M}(x,y_{i})v_{l-1}(y_{i})(y_{i}), \]

where \(\) denotes the Riemannian sum weights corresponding to the ambient space of \(B_{r}(x)\). For a fixed input mesh of \(N\) points, the computational cost of equation (7) scales with the number of edges, denoted as \(O(E)=O(MN)\). Here, the number of sampling points \(M\) is the degree of the graph. It can be either fixed to a constant sampling size, or scale with the area of the ball.

Encoder.Given an input point cloud \(\{x_{1}^{},,x_{N}^{}\} S_{T}\), we employ a GNO-encoder to transform it to a function on a uniform latent grid \(\{x_{1}^{},,x_{S}^{}\} D\). The encoder is computed as discretization of an integral operator \(v_{0}(x^{})_{i=1}^{M}(x^{},y_{i}^{ {in}})(y_{i}^{})\) over ball \(B_{r_{}}(x^{})\). To inform the grid density, GINO computes Riemannian sum weights \((y_{i}^{})\). Further, we use Fourier features in the kernel . For simple geometries, this encoder can be omitted, see Section 4.

Decoder.Similarly, given a function defined on the uniform latent grid \(\{x_{1}^{},,x_{S}^{}\} D\), we use a GNO-decoder to query arbitrary output points \(\{x_{1}^{},,x_{N}^{}\}_{T}\). The output is evaluated as \(u(x^{})_{i=1}^{M}(x^{},y_{i}^{})v_{l}(y_{i}^{})(y_{i}^{})\) over ball \(B_{r_{}}(x^{})\). Here, the Riemannian weight, \((y_{i}^{})=1/S\) since we choose the latent space to be regular grid. Since the queries are independent, we divide the output points into small batches and run them in parallel, which enables us to use much larger models by saving memory.

Efficient graph construction.The graph construction requires finding neighbors to each node that are within a certain radius. The simplest solution is to compute all possible distances between neighbors, which requires \(O(N^{2})\) computation and memory. However, as the \(N\) gets larger, e.g., 10 \(\) 100 million, computation and memory become prohibitive even on modern GPUs. Instead, we use a hash grid-based implementation to efficiently prune candidates that are outside of a \(^{}\)-ball first and then compute the \(^{2}\) distance between only the candidates that survive. This reduces the computational complexity to \(O(Ndr^{3})\) where \(d\) denotes unit density and \(r\) is the radius. This can be efficiently done using first creating a hash table of voxels with size \(r\). Then, for each node, we go over all immediate neighbors to the current voxel that the current node falls into and compute the distance between all points in these neighboring voxels. Specifically, we use the CUDA implementation from Open3D . Then, using the neighbors, we compute the kernel integration using gather-scatter operations from torch-scatter . Further, if the degree of the graph gets larger, we can add Nystrom approximation by sampling nodes .

### Fourier operator block

The geometry encoding \(v_{0}\) and the geometry specifying map \(T\), both evaluated on a regular grid discretizing \(D\) are passed to a FNO block. We describe the basic FNO block as first outlined in . We will first define global convolution in the Fourier space and use it to build the full FNO operator block. To that end, we will work on the \(d\)-dimensional unit torus \(^{d}\). We define an integral operator with kernel \( L^{2}(^{d};^{n m})\) as the mapping \(:L^{2}(^{d};^{m}) L^{2}(^{d}; ^{n})\) given by

\[(v)=^{-1}()(v) ,\;v L^{2}(^{d};^{m})\]

Here \(,^{-1}\) are the Fourier transform and its inverse respectively, defined for \(L^{2}\) by the appropriate limiting procedure. The Fourier transform of the function \(\) will be parameterized directly by some fixed number of Fourier modes, denoted \(\). In particular, we assume

\[(x)=_{ I}c_{}^{i(,x)},\; x^{d}\]

for some index set \(I^{d}\) with \(|I|=\) and coefficients \(c_{}^{n m}\). Then we may view \(:L^{2}(^{d};^{n m})^{2}( ^{d};^{n m})\) so that \(()()=c_{}\) if \( I\) and \(()()=0\) if \( I\). We directly learn the coefficients \(c_{}\) without ever having to evaluate \(\) in physical space. We then define the full operator block \(:L^{2}(^{d};^{m}) L^{2}(^{d}; ^{n})\) by

\[(v)(x)=Wv(x)+(v),\;x ^{d}\]

where \(\) is a pointwise non-linearity and \(W^{n m}\) is a learnable matrix. We further modify the layer by learning the kernel coefficients in tensorized form, adding skip connections, normalization layers, and learnable activations as outlined in . We refer the reader to this work for further details.

Adaptive instance normalization.For many engineering problems of interest, the boundary information is a fixed, scalar, inlet velocity specified on some portion of \( D\). In order to efficiently incorporate this scalar information into our architecture, we use a learnable adaptive instance normalization  combined with a Fourier feature embedding . In particular, the scalar velocity is embedded into a vector with Fourier features. This vector then goes through a learnable MLP, which outputs the scale and shift parameters of an instance normalization layer . In problems where the velocity information is not fixed, we replace the normalization layers of the FNO blocks with this adaptive normalization. We find this technique improves performance, since the magnitude of the output fields usually strongly depends on the magnitude of the inlet velocity.

Figure 2: **Visualization of a ground-truth pressure and corresponding prediction by GINO from the Shape-Net Car (top) and Ahmed-body (bottom) datasets, as well as the absolute error.**We do a benchmark study with several standard machine-learning methods on the Shape-Net and Ahmed body datasets. The training error is normalized L2 error; the test error is de-normalized L2.

## 4 Experiments

We explore a range of models on two CFD datasets. The large-scale Ahmed-Body dataset, which we generated, and also the Shape-Net Car dataset from . Both datasets contain simulations of the Reynold-Averaged Navier-Stokes (RANS) equations for a chosen turbulence model. The goal is to estimate the full pressure field given the shape of the vehicle as input. We consider GNO , MeshGraphNet , GeoFNO , 3D UNet  with linear interpolation, FNO , and GINO. We train each model for 100 epochs with Adam optimizer and step learning rate scheduler. The implementation details can be found in the Appendix. All models run on a single Nvidia V100 GPU.

### Ahmed-Body dataset

We generate the industry-level vehicle aerodynamics simulation based on the Ahmed-body shapes . The shapes are parameterized with six design parameters: length, width, height, ground clearance, slant angle, and fillet radius. We also vary the inlet velocity from 10m/s to 70m/s, leading to Reynolds numbers ranging from \(4.35 10^{5}\) to \(6.82 10^{6}\). We use the GPU-accelerated OpenFOAM solver for steady state simulation using the SST \(k-\) turbulence model  with 7.2 million mesh points in total with 100k mesh points on the surface. Each simulation takes 7-19 hours on 2 Nvidia v100 GPUs with 16 CPU cores. We generate 551 shapes in total and divide them into 500 for training and 51 for validation.

### Shape-Net Car dataset

We also consider the Car dataset generated by . The input shapes are from the ShapeNet Car category . In , the shapes are manually modified to remove the side mirrors, spoilers, and tires. The RANS equations with the \(k-\) turbulence model and SUPG stabilization are simulated to obtain the time-averaged velocity and pressure fields using a finite element solver . The inlet velocity is fixed at 20m/s (72km/h) and the estimated Reynolds number is \(5 10^{6}\). Each simulation takes approximately 50 minutes. The car surfaces are stored with 3.7k mesh points. We take the 611 water-tight shapes out of the 889 instances, and divide the 611 instances into 500 for training and 111 for validation.

As shown in Table 2 and Figure 2, GINO achieves the best error rate with a large margin compared with previous methods. On the Ahmed-body dataset, GINO achieves **8.31%** while the previous best method achieve 11.16%. On the Shape-Net Car, GINO achieves 7.12% error rate compared to 9.42% on FNO. It takes 0.1 seconds to evaluate, which is 100,000x faster than the GPU-parallel OpenFOAM solver that take 10 hours to generates the data. We further performance a full cost-accuracy analysis in the following section.

For ablations, we consider channel dimensions , latent space , and radius from 0.025 to 0.055 (with the domain size normalized to [-1, 1]). As depicted in Figure 4(a) and Table 4, larger latent spaces and radii yield superior performance.

time needed for a relative error in the drag coefficient for GINO and OpenFOAM. The detailed setup is discussed in the appendix.

### Discretization-convergence and ablation studies

We investigate discretization-convergence by varying different parts of GINO. Specifically, we vary the latent grid resolution and the sampling rates for input-output meshes. In these experiments, we fixed the training and test samples to be the same, i.e., same latent grid resolution or sampling rate, but varied the shape and input conditions.

Discretization-convergence wrt the latent grid.Here, each model is trained and tested on (the same) latent resolutions, specifically 32, 48, 64, 80, and 88, and the architecture is the same. As depicted in Figure 4(a), GINO demonstrates a comparable error rate across all resolutions. A minor improvement in errors is observed when employing a larger latent space. Conversely, the errors associated with the UNet model grow as the resolution is decreased due to the decreasing receptive field of its local convolution kernels.

Discretization-convergence in the input-output mesh.Here, GINO is trained and tested with sub-sampled input-output meshes at various sampling rates (2x, 4x, 6x, 8x). As illustrated in Figure 4(b), GINO exhibits a consistent error rate across all sampling rates. A slight increase in errors is observed on coarser meshes.

Zero-shot super-resolution.GINO possesses the ability to perform zero-shot super-resolution. The model is trained on a coarse dataset, sub-sampled by 2x, 4x, 6x, and 8x, and subsequently tested on the full mesh, that is not seen during training. The error remains consistent across all sampling rates 4(c). This characteristic enables the model to be trained at a coarse resolution when the mesh is dense, consequently reducing the computational requirements.

  
**Model** & **training error** & **test error** \\  MeshGraphNet & 9.08\% & 13.88\% \\ UNet (interp) & 9.93\% & 11.16\% \\ FNO (interp) & 12.97\% & 12.59\% \\ GINO (encoder-decoder) & 9.36\% & 9.01\%\% \\ GINO (decoder) & 9.34\% & **8.31\%** \\   

Table 3: **Ahmed-body dataset (100k mesh points).**

Figure 3: **Cost-accuracy trade-off analysis for the drag coefficient.**

## 5 Related Work

The study of neural operators and their extended applications in learning solution operators in PDE has been gaining momentum [3; 32; 33; 34]. A method that stands out is FNO, which uses Fourier transform . The FNO and its variations have proven to highly accelerate the simulations for large-scale flow problems, including weather forecasting , seismology [35; 36] and multi-phase flow . However, a challenge with the FNO is that its computation superiority is gained when applied on a regular grid, where the Fourier transform is approximated using FFT. Therefore, its reliance on FFT limits its use with irregular grids or complex geometries. There have been attempts to modify the FNO to work with these irregular structures, but scalability to large-scale 3D PDEs remains an issue. One such attempt is GeoFNO, which learns a coordinate transformation to map irregular inputs to a regular latent space . This method, while innovative, requires a geometric discrete Fourier transform, which is computationally demanding and lacks discretization insurance. To circumvent this, GINO limits the Fourier transform to a local GNO to improve efficiency. The locality is defined assuming the metrics of the physical space.

Additionally, the Non-Equispaced Fourier neural solvers (NFS) merge the FNO with non-equispaced interpolation layers, a method similar to global GNO . However, at the architecture level, their method replaces the integration of GNO with the summation of the nearest neighbor points on the graph. This step transitions this method to a neural network, failing to deliver a discretization convergent approach. The Domain-Agnostic Fourier Neural Operators (DAFNO) represents another attempt at improvement, applying an FNO to inputs where the geometry is represented as an indicator function . However, this method lacks a strategy for handling irregular point clouds. Simultaneously, researchers are exploring the combination of FNO with the attention mechanisms  for irregular meshes. This includes the Operator Transformer (OFormer) , Mesh-Independent Neural Operator (MINO) , and the General Neural Operator Transformer (GNOT) . Besides, the Clifford neural layers  use the Clifford algebra to compute multivectors, which provides Clifford-FNO implementations as an extension of FNO. The work  uses a physics-informed loss with U-Net for 3D channel flow simulation with several shapes. The work  innovatively proposes the use of multigrid training for neural networks that improves the convergence. Although these methods incorporate attention layers, which are special types of kernel integration  with quadratic complexity, they face challenges when scaling up for large-scale problems.

GNNs are incorporated in the prior attempts in physical simulations involving complex geometry, primarily due to the inherent flexibility of graph structures. Early research [7; 45; 46; 47] laid the foundation for GNNs, demonstrating that physical entities, when represented as graph nodes, and their interactions, as edges, could predict the dynamics of various systems. The introduction of graph element networks  marked a significant development, being the first to apply GNNs to PDEs by discretizing the domain into elements. The similar idea has been explored in term of continuous 3D point cloud convolution [49; 50; 51]. Another line of work, mesh graph networks [8; 9; 10], further explored PDEs in the context of fluid and solid mechanics. [52; 53] train a Graph convolutional neural works on the ShapeNet car dataset for inverse design. However, GNN architectures' limitations hinder their use in operator learning for PDEs. GNNs connect each node to its nearest neighbors according to the graph's metrics, not the metrics of the physical domain. As the input function's discretization becomes finer, each node's nearest neighbors eventually converge to the same node, contradicting the expectation of improved model performance with finer discretization. Furthermore, GNNs'

Figure 4: **Discretization-convergence studies and zero-shot super-resolution.**model behavior at the continuous function limit lacks a unique definition, failing the discretization convergence criterion. Consequently, as pointwise operators in function spaces at the continuous limit, GNNs struggle to approximate general operators between function spaces, Figure 5.

## 6 Conclusion

In this work, we propose the GINO model for 3D PDEs with complex geometries. The GINO model consists of the graph-kernel blocks for the encoder and decoder that go to a latent uniform space, where the Fourier blocks run on the latent space to capture the global interaction. We experiment on two CFD datasets: Shape-Net car geometries and large-scale Ahmed's body geometries, the latter encompassing over 600 car geometries featuring hundreds of thousands of mesh points. The evidence from these case studies illustrates that our method offers a substantial speed improvement, with a factor of 100,000 times acceleration in comparison to the GPU-based OpenFOAM solver. Concurrently, our approach has achieved one-fourth to one-half the error rates compared to prevailing neural networks such as 3D U-Net. This underscores the potential of our method to significantly enhance computational efficiency while maintaining a competitive level of accuracy within the realm of CFD applications. **Limitation:** The trained surrogate model is limited to a specific category of shapes. The quality of the model depends on the quality of the training dataset. For CFD with more complex shapes, it is not easy to obtain a large training dataset. We will explore physics-informed approaches  and generate time-dependent high-fidelity simulations in the future.

Figure 5: **Comparison of GNN and GNO as the discretization becomes finer. GNN is discretization dependent, while GNO is discretization convergent.**