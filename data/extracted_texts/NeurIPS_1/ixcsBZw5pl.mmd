# Non-adversarial training of Neural SDEs with signature kernel scores

Zacharia Issa

Corresponding author: zacharia.issa@kcl.ac.ukDepartment of Mathematics, King's College London, London, United Kingdom.Department of Mathematics, Oxford University, Oxford, United Kingdom.The Oxford Man Institute, Oxford, United Kingdom.The Alan Turing Institute, London, United Kingdom.Department of Mathematics, Imperial College London, London, United Kingdom.

Blanka Horvath

Department of Mathematics, Oxford University, Oxford, United Kingdom.The Oxford Man Institute, Oxford, United Kingdom.The Alan Turing Institute, London, United Kingdom.Department of Mathematics, Imperial College London, London, United Kingdom.

Maud Lemercier

Department of Mathematics, Oxford University, Oxford, United Kingdom.The Oxford Man Institute, Oxford, United Kingdom.The Alan Turing Institute, London, United Kingdom.Department of Mathematics, Imperial College London, London, United Kingdom.

Cristopher Salvi

The Alan Turing Institute, London, United Kingdom.The Alan Turing Institute, London, United Kingdom.

###### Abstract

Neural SDEs are continuous-time generative models for sequential data. State-of-the-art performance for irregular time series generation has been previously obtained by training these models adversarially as GANs. However, as typical for GAN architectures, training is notoriously unstable, often suffers from mode collapse, and requires specialised techniques such as weight clipping and gradient penalty to mitigate these issues. In this paper, we introduce a novel class of scoring rules on pathspace based on signature kernels and use them as objective for training Neural SDEs non-adversarially. By showing strict properness of such kernel scores and consistency of the corresponding estimators, we provide existence and uniqueness guarantees for the minimiser. With this formulation, evaluating the generator-discriminator pair amounts to solving a system of linear path-dependent PDEs which allows for memory-efficient adjoint-based backpropagation. Moreover, because the proposed kernel scores are well-defined for paths with values in infinite dimensional spaces of functions, our framework can be easily extended to generate spatiotemporal data. Our procedure permits conditioning on a rich variety of market conditions and significantly outperforms alternative ways of training Neural SDEs on a variety of tasks including the simulation of rough volatility models, the conditional probabilistic forecasts of real-world forex pairs where the conditioning variable is an observed past trajectory, and the mesh-free generation of limit order book dynamics.

## 1 Introduction

Stochastic differential equations (SDEs) are a dominant modelling framework in many areas of science and engineering. They naturally extend ordinary differential equations (ODEs) for modelling dynamical systems that evolve under the influence of randomness.

A _neural stochastic differential equation_ (Neural SDE) is a continuous-time generative model for irregular time series where the drift and diffusion functions of an SDE are parametrised by neural networks . These models have become increasingly popular among financial practitioners for pricing and hedging of derivatives and overall risk management . Training a Neural SDE amounts to minimising over model parameters an appropriate notion of distance between the law on pathspace generated by the SDE and the empirical law supported on observed data sample paths.

Various choices of training mechanisms have been proposed in the literature; state-of-the-art performance has been achieved by training Neural SDEs adversarially as Wasserstein-GANs . However, as typical for GAN architectures, training is notoriously unstable, often suffers from mode collapse, and requires specialised techniques such as weight clipping and gradient penalty.

In this paper we introduce a novel class of scoring rules based on _signature kernels_, a class of characteristic kernels on paths , SLL\({}^{+}\)21, LSC\({}^{+}\)21, CLS23, HLL\({}^{+}\)23], and use them as objective for training Neural SDEs non-adversarially. We provide existence and uniqueness guarantees for the minimiser by showing strict properness of the signature kernel scores and consistency of the corresponding estimators.

With this training formulation, the generator-discriminator pair becomes entirely mesh-free and can be evaluated by solving a system of linear path-dependent PDEs which allows for memory-efficient adjoint-based backpropagation. In addition, because the proposed kernel scores are well-defined for classes of paths with values in infinite dimensional spaces of functions, our framework can be easily extended to the generation of spatiotemporal signals.

We demonstrate how our procedure is more stable and outperforms alternative ways of training Neural SDEs on a variety of tasks from quantitative finance including the simulation of rough volatility models, the conditional probabilistic forecasts of real-world forex pairs where the conditioning variable is an observed past trajectory, and the mesh-free generation of limit order book dynamics.

## 2 Related work

Prior to our work, two main approaches have been proposed to fit a Neural SDE as a time series generative model, differing in their choice of divergence to compare laws on pathspace.

The SDE-GAN model introduced in  uses the \(1\)-Wasserstein distance to train a Neural SDE as a Wasserstein-GAN . Namely, the "witness functions" of the \(1\)-Wasserstein distance are parameterised by neural controlled differential equations  and the generator-discriminator pair is trained adversarially. SDE-GANs are relatively unstable to train mainly because they require a Lipschitz discriminator. Several techniques such as weight clipping and gradient penalty have been introduced to enforce the Lipschitz constraint and partially mitigate the instability issue . SDE-GANs are also sensitive to other hyperparameters, such as the choice of optimisers, their learning rate and momentum, where small changes can yield erratic behavior.

The latent SDE model  trains a Neural SDE with respect to the KL divergence using the principles of variational inference for SDEs . This approach consists in maximising an objective that includes the KL divergence between the laws produced by the original SDE (the prior) and an auxiliary SDE (the approximate posterior). The two SDEs have the same diffusion term but different initial conditions and drifts, and a standard formula for their KL divergence exists. After training, the learned prior can be used to generate new sample paths. Latent SDEs can be interpreted as variational autoencoders, and generally yield worse performance than SDE-GANs, which are more challenging to train, but offer greater model capacity.

Besides Neural SDEs, other time series generative models have been proposed, including discrete-time models such as  and 2 which are trained adversarially, continuous-time flow processes  and score-based diffusion models for audio generation .

The class of score-based generative models (SGMs) seeks to map a data distribution into a known prior distribution via an SDE . During training, the (Stein) score  of the SDE marginals is estimated and then used to construct a reverse-time SDE. By sampling data from the prior and solving the reverse-time SDE, one can generate samples that follow the original data distribution. We note that our techniques for generative modelling via scoring rules, although similar in terminology, are fundamentally different, as we train Neural SDEs with respect to a loss function that directly consumes the law on pathspace generated by the SDE.

Scoring rules  have been used to define training objectives for generative networks  which have been shown to be easier to optimize compared to GANs . Closer to our work is  which constructs statistical scores for discrete (spatio-)temporalsignals. However, their strict properness is ensured under Markov-type assumptions and their continuous-(space-)time limit has not been studied. A key aspect of our work is to develop consistent and effective scoring rules for generative modelling in the continuous-time setting. While  has also introduced scoring rules for continuous-time processes, our emphasis lies in constructing so-called kernel scores specifically for training Neural SDE and Neural SPDE generative models.

The Neural SPDE model introduced in  parametrises the solution operator of stochastic partial differential equations (SPDEs), which extend SDEs for modelling signals that vary both in space and in time. So far, Neural SPDEs have been trained in a supervised fashion by minimizing the pathwise \(L^{2}\) norm between pairs of spatiotemporal signals. While this approach has proven effective in learning fast surrogate SPDE solvers, it is not well-suited for generative modeling where the goal is to approximate probability measures supported on spatiotemporal functions. In this work, we propose a new training objective for Neural SPDEs to improve their generative modeling capabilities.

## 3 Training Neural SDEs with signature kernel scores

### Background

We take \((,,)\) as the underlying probability space. Let \(T>0\) and \(d_{x}\). Denote by \(\) be the space of continuous paths of bounded variation from \([0,T]\) to \(^{d_{x}}\) with one monotone coordinate3. For any random variable \(X\) with values on \(\), we denote by \(_{X}:= X^{-1}\) its law.

The _signature map_\(S:\) is defined for any path \(x\) as the infinite collection \(S(x)=1,S^{1}(x),S^{2}(x),...\) of iterated Riemann-Stieltjes integrals

\[S^{k}(x):=_{0<t_{1}<...<t_{k}<T}dx_{t_{1}} dx_{t_{2}}...  dx_{t_{k}}, k,\]

where \(\) is the standard tensor product of vector spaces and \(:=^{d_{x}}(^{d_{x}})^{  2}...\)

Any inner product \(,_{1}\) on \(^{d_{x}}\) yields a canonical Hilbert-Schmidt inner product \(,_{k}\) on \((^{d_{x}})^{ k}\) for any \(k\), which in turn yields, by linearity, a family of inner products \(,_{}\) on \(\). We refer the reader to  for an in-depth analysis of different choices. By a slight abuse of notation, we use the same symbol to denote the Hilbert space obtained by completing \(\) with respect to \(,_{}\).

### Neural SDEs

Let \(W:[0,T]^{d_{w}}\) be a \(d_{w}\)-dimensional Brownian motion and \(a(0,I_{d_{a}})\) be sampled from \(d_{a}\)-dimensional standard normal. The values \(d_{w},d_{a}\) are hyperparameters describing the size of the noise. A Neural SDE is a model of the form

\[Y_{0}=_{}(a), dY_{t}=_{}(t,Y_{t})dt+_{}(t,Y _{t}) dW_{t}, X_{t}^{}=A_{}Y_{t}+b_{} \]

for \(t[0,T]\), with \(Y:[0,T]^{d_{y}}\) the strong solution, if it exists, to the Stratonovich SDE, where

\[_{}:^{d_{a}}^{d_{y}},_{ }:[0,T]^{d_{y}}^{d_{y}},_{ }:[0,T]^{d_{y}}^{d_{y} d_{w}}\]

are suitably regular neural networks, and \(A_{}^{d_{x} d_{y}},b_{}^{d_{x}}\). The dimension \(d_{y}\) is a hyperparameter describing the size of the hidden state. If \(_{},_{}\) are Lipschitz and \(_{a}[_{}(a)^{2}]<\) then the solution \(Y\) exists and is unique.

Given a target \(\)-valued random variable \(X^{}\) with law \(_{X^{}}\), the goal is to train a Neural SDE so that the generated law \(_{X^{}}\) is as close as possible to \(_{X^{}}\), for some appropriate notion of closeness.

### Signature kernels scores

_Scoring rules_ are a well-established class of functionals to represent the penalty assigned to a distribution given an observed outcome, thereby providing a way to assess the quality of a probabilistic forecast. Scoring rules have been applied to a wide range of areas including econometrics ,weather forecasting , and generative modelling . How to effectively select a scoring rule is a challenging and somewhat task-dependent problem, particularly when the data is sequential. Scoring rules based on kernels offer the advantages of working on unstructured and infinite dimensional data without some of the concomitant drawbacks, such as the absence of densities. Next, we introduce a class of scoring rules on paths based on signature kernels to measure closeness between path-valued random variables. These will be used in the next section to train Neural SDEs.

The _signature kernel_\(k_{}:\) is a symmetric positive semidefinite function defined for any pair of paths \(x,y\) as \(k_{}(x,y):= S(x),S(y)_{}\). In  the authors provided a kernel trick proving that the signature kernel satisfies

\[k_{}(x,y)=f(T,T) f(s,t)=1+_{0}^{s}_{0} ^{t}f(u,v) dx_{u},dy_{v}_{1}, \]

which reduces to a linear hyperbolic PDE in the when the paths \(x,y\) are almost-everywhere differentiable. Several finite difference schemes are available for numerically evaluating solutions to Equation (2), see [1, Section 3.1] for details.

We denote by \(\) the unique reproducing kernel Hilbert space (RKHS) of \(k_{}\). From now on we endow \(\) with a topology with the respect to which the signature is continuous; see  for various choices of such topologies. Denote by \(()\) the set of Borel probability measures on \(\).

**Proposition 3.1**.: _The signature kernel is characteristic for every compact set \(\), i.e. the map \( k_{}(x,)(dx)\) from \(()\) to \(\) is injective._

**Remark 3.2**.: The proof of this statement is classical and is a simple consequence of the universal approximation property of the signature [1, Proposition A.6] and the equivalence between universality of the feature map and characteristincess of the corresponding kernel [1, Theorem 6]. In particular, Proposition (3.1) implies that the signature kernel is cc-universal, i.e. for every compact subset \(\), the linear span of the set of path functionals \(\{k_{}(x,):x\}\) is dense in \(C()\) in the the topology of uniform convergence.

We define the _signature kernel score_\(_{}:()\) for any \(()\) and \(y\) as

\[_{}(,y):=_{x,x^{}}[k_{ }(x,x^{})]-2_{x}[k_{}(x,y)].\]

A highly desirable property to require from a score is its _strict properness_, consisting in assigning the lowest expected score when the proposed prediction is realised by the true probability distribution.

**Proposition 3.3**.: _For any compact \(\), \(_{}\) is a strictly proper kernel score relative to \(()\), i.e. \(_{y}[_{}(,y)]_{ y}[_{}(,y)]\) for all \(,()\), with equality if and only if \(=\)._

The proof of this statement can be found in the appendix and follows from [1, Theorem 4] and Proposition 3.1. We note that the signature kernel score induces a divergence on \(()\) known as the signature kernel _maximum mean discrepancy_ (MMD), defined for any \(,()\) as

\[_{k_{}}(,)^{2}=_{y }[_{}(,y)]+_{y,y^{} }[k_{}(y,y^{})]. \]

The following result provides a consistent and unbiased estimator for evaluating the signature kernel score from observed sample paths. The proof can be found in the appendix and follows from standard results for the associated MMD [1, Lemma 6].

**Proposition 3.4**.: _Let \(()\) and \(y\). Given \(m\) sample paths \(\{x^{i}\}_{i=1}^{m}\), the following is a consistent and unbiased estimator of \(_{}\)_

\[_{}(,y)=_{j i}k_{ }(x^{i},x^{j})-_{i}k_{}(x^{i},y). \]

### Non-adversarial training of Neural SDEs via signature kernel scores

We now have all the elements to outline the procedure we propose to train the Neural SDE model (1) non-adversarially using signature kernel scores introduced in the previous section.

Unconditional settingWe are given a target \(\)-valued random variable \(X^{}\) with law \(_{X^{}}\). Recall the notation \(_{X^{}}\) for the law generated by the SDE (1). The training objective is given by

\[_{}()()= _{y_{X^{}}}[_{}(_{X^ {}},y)]. \]

Note that training with respect to \(_{k_{}}\) is an equivalent optimisation as the second expectation in equation (3) is constant with respect to \(\). This means that in the unconditional setting our model corresponds to a continuous time generative network of .

Combining equations (1), (2), (4) and (5) the generator-discriminator pair can be evaluated by solving a system of linear PDEs depending on sample paths from the Neural SDE; in summary:

\[\ X^{}() \ ()(X^{},X^{}). \]

**Remark 3.5**.: The generation of sample paths from \(X^{}\) from the SDE solver and the evaluation of the objective \(\) via the PDE solver can in principle be performed concurrently, although, in our implementation we evaluate the full model (6) in a sequential manner.

Conditional settingIt is straightforward to extend our framework to the conditional setting where \(\) is some distribution we wish to condition on, and \(_{X^{}}(|x)\) is a target conditional distribution with \(x\). By feeding the observed sample \(x\) as an additional variable to all neural networks of the Neural SDE (1), the generated strong solution provides a parametric conditional law \(_{X^{}}(|x)\), and the model can be trained according to the modified objective

\[_{}^{}()^{ }()=_{}_{x}_{y _{X^{}}(|x)}[_{}(_{X^{ }}(|x),y)]. \]

Because \(_{}\) is strictly proper, the solution to (7) is \(_{X^{}}(|x)=_{X^{}}(|x)\)\(\)-almost everywhere. With data sampled as \(\{(x^{i},y^{i})\}_{i=1}^{n}\) where \(x^{i}\) and \(y^{i}_{X^{}}(|x^{i})\) we can replace eq. (7) by

\[_{}_{i=1}^{n}_{}(_{X^{ }}(|x^{i}),y^{i}), \]

We note that in our experiments we focus on the specific case where the conditioning variable \(x\) is a path in \(\) corresponding to the observed past trajectory of some financial assets (see Figure 2).

### Additional details

InterpolationSamples from \(X^{}\) are observed on a discrete, possibly irregular, time grid while samples from \(X^{}\) are generated from (1) by means of an SDE solver of choice (see [13, Section 5.1] for details). Interpolating in time between observations produces a discrete measure on path space, the ones desired to be modelled. The interpolation choice is usually unimportant and simple linear interpolation is often sufficient. See  for other choices of interpolation.

BackpropagationTraining a Neural SDE usually means backpropagating through the SDE solver. Three main ways of differentiating through an SDE have been studied in the literature: 1) _Discretise-then-optimise_ backpropagates through the internal operations of the SDE solver. This option is memory inefficient, but will produce accurate and fast gradient estimates. 2) _Optimise-then-discretise_ derives a backwards-in-time SDE, which is then solved numerically. This option is memory efficient, but gradient estimates are prone to numerical errors and generally slow to compute. We note that unlike the case of Neural ODEs, giving a precise meaning to the backward SDE falls outside the usual framework of diffusions. However, _rough path theory_ provides an elegant remedy by allowing solutions to forward and backward SDEs to be defined pathwise, similarly to the case of ODEs; see [13, Appendix C.3.3] for a precise statement. 3) _Reversible solvers_ are memory efficient and accurate, but generally slow. Here we do not advocate for any particular choice as all of the above backpropagation options are compatible with our pipeline.

Similarly, because the signature kernel score can be evaluated by solving a system of PDEs, backpropagation can be carried out by differentiating through the PDE solver analogously to the discretise-then-optimise option for SDEs. We note that  showed that directional derivatives of signature kernels solve a system of adjoint-PDEs, which can be leveraged to backpropagate through the discriminator using an optimise-then-discretise approach. We used this approach in our experiments.

Ito vs StratonovichStratonovich SDEs are slightly more efficient to backpropagate through using an optimise-then-discretise approach. In the case of Ito SDEs, the backward equation is derived by applying the Ito-Stratonovich correction term to convert it into a Stratonovich SDE, deriving the corresponding backward equation through rough path theoretical arguments, and then converting it back to an Ito SDE by applying a second Stratonovich-Ito correction.

Paths with values in infinite dimensional spacesWhile we have defined the signature kernel for paths of bounded variation with values in \(^{d_{}}\), the kernel is still well-defined when \(^{d_{}}\) is replaced with a generic Hilbert space \(V\). Remarkably, even when \(V\) is infinite dimensional, the evaluation of the kernel can be carried out, as Equation (2) only depends on pairwise inner products between the values of the input paths. In particular, the kernel can be evaluated on paths taking their values in functional spaces, which has far-reaching consequences in practice. For example, this gives the flexibility to map the values of finite dimensional input paths into a possibly infinite dimensional feature space, such as the reproducing kernel Hilbert space of a kernel \(\) on \(^{d_{}}\), that is, \(V=_{}\). This also provides a natural kernel for spatiotemporal signals, such as paths taking their values in \(V=L^{2}(D)\), the space of square-integrable functions on a compact domain \(D^{d}\). For practical applications, the inner product in Equation (2) can be approximated using discrete observations of the input signals on a mesh of the spatial domain \(D\). The inner product in \(L^{2}(D)\) can be replaced with more general kernels as those introduced in . While it has become common practice to use signature kernels on the RKHS-lifts of Euclidean-valued paths, the ability to define and compute signature kernels on spatiotemporal signals has been, to our knowledge, overlooked in the literature.

## 4 Experiments

We perform experiments across five datasets. First is a univariate synthetic example, the benchmark Black-Scholes model, which permits to readily verify the quality of simulated outputs. The second synthetic example is a state of-the-art univariate stochastic volatility model, called rough Bergomi model. The rough Bergomi model realistically captures many relevant properties of options data, but due to its rough (and hence non-Markovian) nature it is well-known to be difficult to simulate. The third is a multidimensional example with foreign exchange (forex, or FX) currency pairs, which was chosen not only because of the relevance and capitalisation of FX markets but also due to its well-known intricate complexity. Fourth is a univariate example, where we demonstrate the method's ability to condition on relevant variables, given by paths. Finally we present a spatiotemporal generative example, where we seek to simulate the dynamics of the NASDAQ limit order book.

For the unconditional examples, we compare against the SDE-GAN from  and against the same pipeline as the one we proposed, but using an approximation \(_{}^{N}\) of the signature kernel score \(_{}\) obtained by truncating signatures at some level \(N\). We evaluate each training instance with a variety of metrics. The first is the Kolmogorov-Smirnov (KS) test, which is a nonparametric two-sample test between two empirical probability distributions on \(\), see  for more details. We apply the KS test on the marginals between a batch of generated paths against an unseen batch from the real data distribution. We repeated this test \(5000\) times at the 5% significance level and reported the average KS score along with the average Type I error. Each training instance was kept to a maximum of 2 hours for the synthetic examples, and 4 hours for the real data example. Finally, as mentioned at the end of Section 3.5, when training with respect to \(_{}\) we mapped path state values into \((,)\) where \(\) denotes the RBF kernel on \(^{d}\). Additional details on hyperparameter selection, learning rates, optimisers and further evaluation metrics can be found in the Appendix.

### Geometric Brownian motion

As a toy example, we seek to learn a _geometric Brownian motion_ (gBm) of the form

\[dy_{t}= y_{t}dt+ y_{t}dW_{t}, y_{0}=1, \]

We chose \(=0,=0.2\) and generated time-augmented paths of length 64 over the grid \(=\{0,1,,63\}\) with \(dt=0.01\). Thus our dataset is given by time-augmented paths \(y:^{2}\) embedded in path space via linear interpolation. For all three discriminators, the training and test set were both comprised of 32768 paths and the batch size was chosen to be \(N=128\). We trained the SDE-GAN for 5000 steps, \(_{}\) for 4000 and \(_{}^{N}\) for 10000 steps. Table 1 gives the KS scores along each of the specified marginals, along with the percentage Type I error. Here the generator trained with \(_{}\) performs the best, achieving a Type I error at the assumed confidence level.

### Rough Bergomi volatility model

It is well-known that the benchmark model (9) oversimplifies market reality. More complex models, _(rough) stochastic volatility_ (SV) were introduced in the past decades, that are able to capture relevant properties of market data are used by financial practitioners to price and hedge derivatives. Prominent examples of stochastic volatility models include the Heston and SABR models . State-of-the-art models in this context have been introduced in . They display a stochastic volatility with _rough_ sample paths. Most notable among these for pricing and hedging is the _rough Bergomi_ (rBergomi) model  which is of the form

\[dy_{t}=-V_{t}dt+}dW_{t} d_{t}^{ u}=_{t}^{u}(u-t)^{}dB_{t}, \]

and where \(_{t}^{u}\) is the instantaneous forward variance for time \(u\) at time \(t\), with \(_{t}^{t}=V_{t}\), and \(=H-1/2\) where \(H\) is the Hurst exponent. The parameter set is given by \((,,H)\) with initial conditions \(X_{0}=x\) and \(_{t}^{u}=_{0}\). It has been a well-known headache for modellers that--despite their many modelling advantages--rough volatility models (such as (10)) are slow to to simulate with traditional methods. We demonstrate how our method can be used to capture the dynamics of the rough Bergomi model (10), and in passing we also note that our method provides a significant simulation speedup for (10) compared to previously available simulation methods.

To do so, we simulate paths of length 64 over the time window to \(\), and specify \(dt=1/32\). Thus paths are of length \(64\). The parameters are \((_{0},,,H)=(0.04,1.5,-0.7,0.2)\) and set \(d=1\). Paths are again time-augmented. The hyperparameters for training are the same as in the previous section. The results on the marginal distributions are summarized in Table 2. We see that that training with respect to \(_{}\) vastly outperforms the other two discriminators.

### Foreign exchange currency pairs

We consider an example where samples from the data measure \(_{X}\) are time-augmented paths \(y:[0,T]^{3}\) corresponding to hourly market close prices of the currency pairs EUR/USD and USD/JPY4. To deal with irregular sampling, we linearly interpolate each sample \(y\) over a fixed grid \(=\{t_{0},t_{1},,t_{63}\}\). Training hyperparameters were kept the same as per the rBergomi example: paths are comprised of 64 observations, the batch size was taken to be \(N=128\), and the number of training epochs was taken to be 10000 for the SDE-GAN, 4000 for \(_{}\) and 15000 for \(_{}^{N}\). KS scores for each of the marginals are given in Table 3 and 4. We note that only the generator trained with \(_{}\) is able to achieve strong performance on nearly all marginals.

   Model & \(t=6\) & \(t=19\) & \(t=32\) & \(t=44\) & \(t=57\) \\  SDE-GAN & \(0.1641,41.1\%\) & **0.1094, 5.2\%** & \(0.1421,24.2\%\) & \(0.1104,5.9\%\) & \(0.1427,26.2\%\) \\ \(_{}^{N}\) (\(N=3\)) & \(0.1298,15.4\%\) & \(0.1277,16.1\%\) & \(0.1536,37.4\%\) & \(0.2101,78.8\%\) & \(0.2416,92.3\%\) \\ \(_{}\) (ours) & **0.1071, 5.0\%** & \(0.1084,6.0\%\) & **0.1086, 5.9\%** & **0.1089, 5.8\%** & **0.1075, 5.5\%** \\   

Table 1: KS test average scores and Type I errors on marginals on gBm.

   Model & \(t=6\) & \(t=19\) & \(t=32\) & \(t=44\) & \(t=57\) \\  SDE-GAN & \(0.1929,68.3\%\) & \(0.2244,86.2\%\) & \(0.2273,87.0\%\) & \(0.2205,83.4\%\) & \(0.1949,68.7\%\) \\ \(_{}^{N}\) (\(N=5\)) & \(0.1126,8.1\%\) & \(0.1172,10.1\%\) & \(0.1146,8.2\%\) & \(0.1153,8.5\%\) & \(0.1134,7.0\%\) \\ \(_{}\) (ours) & **0.1086, 5.4\%** & **0.1129, 5.9\%** & **0.1118, 5.2\%** & **0.1127, 6.2\%** & **0.1159, 6.9\%** \\   

Table 2: KS test average scores and Type I errors on marginals on rBergomi model We also present a histogram of sample correlations between generated EUR/USD and USD/JPY paths for each of the three discriminators alongside those from the data distribution. From Figure 1 it appears that only the Neural SDE trained with \(_{}\) correctly identifies the negative correlative structure between the two pairs. This is likely due to the fact that these dependencies are encoded in higher order terms of the signature that the truncated method does not capture.

We now consider a conditional generation problem, where the conditioning variables are time-augmented paths \( x:[t_{0}-dt,t_{0}]^{2}\) representing the trajectory of prior \(dt=32\) observations of EUR/USD 15-minute close prices, and the target distribution is \(X^{}:[t_{0},t_{0}+dt^{}]^{2}\) representing the following \(dt^{}=16\) observations. Given batched samples \(\{x^{i},y^{i}\}_{i=1}^{N}\), where \(x^{i}\) and \(y^{i}_{X^{}}(|x^{i})\), we train our generator according to equation (7). We encoded the conditioning paths via the truncated (log)signatures of order \(5\), and fed these values into each of the neural networks of the Neural SDE. In Figure 2, it is evident that the conditional generator exhibits the capability to produce conditional distributions that frequently encompass the observed path. Furthermore, it is noteworthy that these generated distributions capture certain distinctive characteristics of financial markets, such as martingality, mean reversion, or leverage effects when applicable.

### Simulation of limit order books

Here, we consider the task of simulating the dynamics of a limit order book (LOB), that is, an electronic record of all the outstanding orders for a financial asset, representing its supply and demand over time. Simulating LOB dynamics is an important challenge in quantitative finance and several synthetic market generators have been proposed [LWL\({}^{+}\)20],[VBP\({}^{+}\)20],[SCC21],[CPC\({}^{+}\)21],[CMVB22]. An order \(o=(t_{o},x_{o},v_{o})\) submitted at time \(t_{o}\) with price \(x_{o}\) and size \(v_{o}>0\) (resp., \(v_{o}<0\)) is a commitment to sell (resp., buy) up to \(|v_{o}|\) units of the traded asset at a price no less (resp., no greater)

  Model & \(t=6\) & \(t=19\) & \(t=32\) & \(t=44\) & \(t=57\) \\  SDE-GAN & \(0.1404,20.5\%\) & \(0.1665,44.2\%\) & \(0.1771,56.4\%\) & \(0.1855,63.8\%\) & \(0.1948,70.3\%\) \\ \(_{}^{N}\) (\(N=5\)) & \(0.1666,43.8\%\) & \(0.1877,72.4\%\) & \(0.2008,84.7\%\) & \(0.2154,93.2\%\) & \(0.2311,98.3\%\) \\ \(_{}\) (ours) & \(\) & \(\) & \(\) & \(\) & \(\) \\  

Table 4: KS test average scores on marginals (USD/JPY).

Figure 1: Histogram of correlation coefficients between EURUSD and USDJPY pairs, 1024 samples.

  Model & \(t=6\) & \(t=19\) & \(t=32\) & \(t=44\) & \(t=57\) \\  SDE-GAN & \(0.1889,62.9\%\) & \(0.2760,98.2\%\) & \(0.3324,99.9\%\) & \(0.3781,100.0\%\) & \(0.4209,100.0\%\) \\ \(_{}^{N}\) (\(N=5\)) & \(\) & \(0.1279,12.0\%\) & \(0.1399,18.7\%\) & \(0.1507,28.1\%\) & \(0.1608,37.5\%\) \\ \(_{}\) (ours) & \(0.1270,12.8\%\) & \(\) & \(\) & \(\) & \(\) \\  

Table 3: KS test average scores on marginals (EUR/USD)than \(x_{o}\). Various events are tracked (e.g. new orders, executions, and cancellations) and the LOB \((t)\) is the set of all active orders in a market at time \(t\). While prior work typically fit a generator that produces the next event, and run it iteratively to generate a sequence of events, we propose to model directly the spatiotemporal process \(Y_{t}(x)=_{o(t):x_{o}=x}v_{o}\). To generate LOB trajectories, we use the Neural SPDE model and train it by minimising expected spatiotemporal kernel scores constructed by composing the signature kernel \(k_{}\) with \(3\) different SE-T type kernels introduced in , namely the ID, SQR and CEXP kernels. We fit our model on real LOB data from the NASDAQ public exchange  which consists of about \(4M\) timestamped events with \(L=10\) price levels. We split this LOB trace into sub-traces of size \(T=30\) to construct our dataset. On Figure 3 report the average KS scores for each of the \(L T\) marginals, using the \(3\) different kernel scores.

## 5 Conclusion and future work

This work showcases the utilization of Neural SDEs as a generative model, highlighting their advantages over competitor models in terms of simplicity and stability, particularly via non-adversarial training. Additionally, we show how Neural SDEs exhibit the ability to be conditioned on diverse and intricate data structures, surpassing the capabilities of existing competitor works. We have achieved this by introducing the signature kernel score on paths and by showing their applicability to our setting (by proving strict properness). Performance of our methods are given computational time and memory is competitive with state-of-the-art methods. Moreover, we have shown that this approach extends to the generation of spatiotemporal signals, which has multiple applications in finance including limit order data generation. Further extensions of this work may include extending its generality to include jump processes in the driving noise of the approximator process (Neural SDEs) used. On the theoretical level extensions may include the validity of results to paths with lower regularity than currently considered. Although sample paths from a Stratonovich SDE are not of bounded variation almost surely, sample paths generated by an SDE solver, once interpolated, are piecewise linear, and hence of bounded variation. A similar point can be made about compactness of the support of the measures. It is possible to ensure characteristics of the signature kernel on non-compact sets of less regular paths using limiting arguments and changing the underlying

Figure 3: KS test average scores for each spatiotemporal marginal, \(100\) runs, NASDAQ data.

topology on pathspace. Further extensions for practical applications can (and should) include the inclusion of more varied evaluation metrics and processes. Notably, in a later step, the generated data should be tested by assessing whether existing risk management frameworks and investment engines can be improved when data used for backtesting is augmented with synthetic samples provided by our methods. Furthermore, the spatiotemporal results can be extended to more complex structures, including being used for the synthetic generation of implied volatility surface dynamics, which has been a notoriously difficult modelling problem in past decades.