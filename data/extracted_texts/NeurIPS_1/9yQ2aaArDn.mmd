# Probabilistic Inference in Reinforcement Learning

Done Right

 Jean Tarbouriech

Google DeepMind

jtarbouriech@google.com &Tor Lattimore

Google DeepMind

lattimore@google.com &Brendan O'Donoghue

Google DeepMind

bodonoghue@google.com

###### Abstract

A popular perspective in Reinforcement learning (RL) casts the problem as probabilistic inference on a graphical model of the Markov decision process (MDP). The core object of study is the probability of each state-action pair being visited under the optimal policy. Previous approaches to approximate this quantity can be arbitrarily poor, leading to algorithms that do not implement genuine statistical inference and consequently do not perform well in challenging problems. In this work, we undertake a rigorous Bayesian treatment of the posterior probability of state-action optimality and clarify how it flows through the MDP. We first reveal that this quantity can indeed be used to generate a policy that explores efficiently, as measured by regret. Unfortunately, computing it is intractable, so we derive a new variational Bayesian approximation yielding a tractable convex optimization problem and establish that the resulting policy also explores efficiently. We call our approach VAPOR and show that it has strong connections to Thompson sampling, K-learning, and maximum entropy exploration. We conclude with some experiments demonstrating the performance advantage of a deep RL version of VAPOR.

## 1 Introduction

Reinforcement learning (RL) is the problem of learning to control an unknown system by taking actions to maximize its cumulative reward through time . As the agent navigates the environment, it receives noisy observations which it can use to update its (posterior) beliefs about the environment . Unlike supervised learning, where the performance of an algorithm does not influence the data it will later observe, in RL the policy of the agent affects the data it will collect, which in turn affects the policy, and so on. As a result, an agent must sometimes take actions that lead to states where it has _epistemic uncertainty_ about the value of those states, and sometimes take actions that lead to more certain payoff. The tension between these two modes is the _exploration-exploitation_ trade-off . In this light, RL is a _statistical inference problem_ wrapped in a _control problem_, and the two problems must be tackled simultaneously for good data efficiency.

It is natural to apply Bayesian probabilistic inference to the uncertain parameters in RL  and, since the goal of the agent is to find the optimal policy, a relevant object of study is the posterior probability of _optimality_ for each state-action pair. In fact, the popular 'RL as inference' approach, most clearly summarized in the tutorial and review of , embeds the control problem into a graphical model by introducing optimality binary random variables that indicate whether a state-action is optimal. In the 'RL as inference' setup these optimality variables are assumed to be observed and take the value one with probability depending on the local reward. This surrogate potential seems like a peculiar and arbitrary choice, yet it has been widely accepted due to its mathematical convenience: for deterministic dynamics, the resulting probabilistic inference is equivalent to maximum entropy RL . However, this approximation leads to a critical shortcoming: The surrogate potential functions do not take into account the Bayesian (epistemic) uncertainty, and consequently do not perform genuine statistical inference on the unknowns in the MDP. This leads to 'posteriors' that arein no way related to the _true_ posteriors in the environment, and acting with those false posteriors leads to poor decision making and agents that require exponential time to solve even small problems .

Our contributions can be centered around **a new quantity that we uncover as key for inference and control**, denoted by \(_{^{*}}\), which represents the _posterior probability of each state-action pair being visited under the optimal policy_.

* We reveal that \(_{^{*}}\). formalizes from a Bayesian perspective what it means for a'state-action pair to be optimal', an event at the core of the 'RL as inference' framework which had never been properly analyzed.
* We establish that knowledge of \(_{^{*}}\) is sufficient to derive a policy that explores efficiently, as measured by regret (Section 3).
* Since computing \(_{^{*}}\) is intractable, we propose a variational optimization problem that tractably approximates it (Section 4).
* We first solve this optimization problem exactly, resulting in a new tabular model-based algorithm with a guaranteed regret bound (Section 5).
* We then solve a variant of this optimization problem using policy-gradient techniques, resulting in a new scalable model-free algorithm (Section 7).
* We show that both Thompson sampling [78; 75; 59; 71] and K-learning  can be directly linked to \(_{^{*}}\), thus shedding a new light on these algorithms and tightly connecting them to our variational approach (Section 6).
* Our approach has the unique algorithmic feature of adaptively tuning optimism and entropy regularization for _each_ state-action pair, which is empirically beneficial as our experiments on 'DeepSea' and Atari show (Section 8).

## 2 Preliminaries

We model the RL environment as a finite state-action, time-inhomogeneous MDP given by the tuple \(\{,,L,P,R,\}\), where \(L\) is the horizon length, \(=_{1}_{L}\) is the state space with cardinality \(S=_{l=1}^{L}S_{l}\) where \(S_{l}=|_{l}|\), \(\) is the action space with \(A\) possible actions, \(P_{l}:_{l}(_{l+1})\) denotes the transition dynamics at step \(l\), \(R_{l}:_{l}()\) is the reward function at step \(l\) and \((_{1})\) is the initial state distribution. Concretely, the initial state \(s_{1}_{1}\) is sampled from \(\), then for steps \(l=1,,L\) the agent is in state \(s_{l}_{l}\), selects action \(a_{l}\), receives a reward sampled from \(R_{l}(s_{l},a_{l})\) with mean \(r_{l}(s_{l},a_{l})\) and transitions to the next state \(s_{l+1}_{l+1}\) with probability \(P_{l}(s_{l+1} s_{l},a_{l})\). An agent following a policy \(_{l}()^{S_{l}}\) at state \(s\) at step \(l\) selects action \(a\) with probability \(_{l}(s,a)\). The value functions are defined as

\[Q_{l}^{}(s,a) =r_{l}(s,a)+\!\!_{s^{}_{l+1}}\!\!P_{l}(s^ {} s,a)V_{l+1}^{}(s^{}), V_{l}^{}(s)=_{a }_{l}(s,a)Q_{l}^{}(s,a), V_{L+1}^{}=0,\] \[Q_{l}^{}(s,a) =r_{l}(s,a)+\!\!_{s^{}_{l+1}}\!\!P_{l}(s^ {} s,a)V_{l+1}^{}(s^{}), V_{l}^{}(s)=_{a }Q_{l}^{}(s,a), V_{L+1}^{}=0.\]

An optimal policy \(^{}\) satisfies \({V_{l}}^{^{*}}(s)=V_{l}^{}(s)\) for each state \(s\) and step \(l\). Let \(\{_{l}^{}(s)=a\}\) be the event that action \(a\) is optimal for state \(s\) at step \(l\), with ties broken arbitrarily so that only one action is optimal at each state. Let \(_{+}^{L,S,A}\{_{+}^{S_{l} A}\}_{l=1}^{L}\) be the set of functions \(_{l}_{+}\) for \(l[L]\).

**Occupancy measures.** We denote the set of occupancy measures (a.k.a. stationary state-action distributions) with respect to the transition dynamics \(P\) as

\[(P)_{+}^{L,S,A}\;:\;\;_{a} _{1}(s,a)=(s),\;\;\;_{a^{}}_{l+1}(s^{},a^{ })=_{s,a}P_{l}(s^{} s,a)_{l}(s,a)}.\]

Note the correspondence between \((P)\) and the set of 

### Previous Approach to 'RL as Inference'

The popular line of research casting 'RL as inference' is most clearly summarized in the tutorial and review of Levine, 2018 Levine et al. (2018). It embeds the control problem into a graphical model, mirroring the dual relationship between optimal control and inference Levine et al. (2018); Wang et al. (2018); Wang et al. (2018). As shown in Figure 4(a), this approach defines an additional _optimality_ binary random variable denoted by \(_{l}^{}\), which indicates which state-action at timestep \(l\) is 'optimal', _i.e._, which state-action is visited under the (unknown) optimal policy. In these works the probability of \(_{l}^{}\) is then modeled as proportional to the exponentiated reward.

**Approximation 1** (Inference over exponentiated reward Levine et al. (2018)).: _Denoting by \(_{l}^{}(s,a)\) the event that state-action pair \((s,a)\) is 'optimal' at timestep \(l\), set \((_{l}^{}(s,a))(r_{l}(s,a))\)._

Under Approximation 1, the optimality variables become observed variables in the graphical model, over which we can apply standard tools for inference. This approach has the advantage of being mathematically and computationally convenient. In particular, under deterministic dynamics it directly leads to RL algorithms with'soft' Bellman updates and added entropy regularization, thus recovering the maximum-entropy RL framework Chen et al. (2018); Wang et al. (2018) and giving a natural, albeit heuristic exploration strategy. Many popular algorithms lie within this class (Zhu et al., 2019; Chen et al., 2020; Chen et al., 2021) and have demonstrated strong empirical performance in domains where efficient exploration is not a bottleneck.

Despite its popularity and convenience, the potential function of Approximation 1_ignores epistemic uncertainty_Bishop (2006). Due to this shortcoming, the inference procedure produces invalid and arbitrarily poor posteriors. It can be shown that the resulting algorithms fail even in basic bandit-like problems (_e.g._, (Bishop, 2006, Problem 1)), as they underestimate the probability of optimal actions and take actions which have no probability of optimality under the true posterior. This implies that the resulting agents (_e.g._, Soft Q-learning Chen et al. (2018)) can perform poorly in simple domains that require deep exploration Wang et al. (2018). The fact that the 'RL as inference' framework does not perform valid inference on the optimality variables leads us to consider how a proper Bayesian inference approach might proceed. This brings us to the first contribution of this manuscript.

## 3 A Principled Bayesian Inference Approach to RL

In this section, we provide a principled Bayesian treatment of statistical inference over 'optimality' variables in an MDP. Although it is the main quantity of interest of the popular 'RL as inference' framework, a rigorous definition of whether a state-action pair is optimal has remained elusive. We provide a recursive one below.

**Definition 1** (State-action optimality).: _For any step \(l[L]\), state-action \((s,a)_{l}\), we define_

\[_{1}^{}(s)\{s_{1}=s\}, _{l+1}^{}(s^{})_{s_{l},a}_{l}^{}(s,a)\{s_{l+1}=s^{}\},\] \[_{1}^{}(s,a)_{1}^{}(s)\{_{1 }^{}(s)=a\}, _{l+1}^{}(s^{},a^{})_{l+1} ^{}(s^{})\{_{l+1}^{}(s^{})=a^{}\}.\]

In words, \(_{l}^{}(s)\) is the _random_ event that state \(s\) is reached at step \(l\) after executing optimal actions from steps \(1\) to \(l-1\), and the event \(_{l}^{}(s,a)\) further requires that the action \(a\) is optimal at state \(s\) in step \(l\). Equipped with this definition, 'optimality' in an MDP flows both in a _backward_ way (via action optimality \(^{}\), which can be computed using dynamic programming) and in a _forward_ way (via state-action optimality \(^{}\)). We illustrate this bidirectional property in the simplified graphical model of Figure 4(b) (which isolates a single trajectory). In general, state-action optimality encapsulates all possible trajectories leading to a given state being optimal (and there may be exponentially many of them). Thanks to the MDP structure and Bayes' rule, we show in Lemma 1 that the _posterior probability_ of state-action optimality lies in the space of occupancy measures. To simplify exposition, we first assume that the transition dynamics \(P\) are known (only rewards are unknown), and we extend our analysis to unknown \(P\) in Section 5.

**Lemma 1**.: _For known \(P\), it holds that \(_{^{}}\{_{}(_{l}^{ })\}_{l=1}^{L}(P)\)._

The significance of Lemma 1 is that we can readily _sample_ from \(_{^{}}\) with the induced policy

\[_{l}(s,a)=_{}(_{l}^{}(s,a) )}{_{a^{}}_{}(_{l}^{ }(s,a^{}))}=_{}(_{l}^{}(s)=a _{l}^{}(s)), \]which is the probability under the posterior that action \(a\) is optimal at state \(s\) conditioned on all actions taken before timestep \(l\) being optimal. It turns out that this policy explores efficiently, as measured by regret (see Corollary 1). Concretely, we have shown that the two facets of RL (inference and control) can be distilled into a single quantity: \(_{^{*}}\). We use Bayesian inference to compute \(_{^{*}}\) and then use that to derive a good control policy. This is a principled and consistent Bayesian 'RL as inference' approach. The remaining challenge thus lies in computing \(_{^{*}}\). Unfortunately, doing so involves computing several complicated integrals with respect to the posterior and is intractable in most cases. In Section 4, we introduce the second contribution of this manuscript, which is a way to approximate \(_{^{*}}\) using a computationally tractable variational approach.

### Instructive Example

To gain intuition on the difference between _action optimality_ (_i.e._, \(_{}(^{*})\)) and _state-action optimality_ (_i.e._, \(_{^{*}}\)), we consider the simple decision problem of Figure 1. Each episode starts in the leftmost state and the agent can traverse a chain of states in order to reach the rightmost state, where the reward is _always_ either \(+1\) or \(-1\) with equal probability. Once the agent reaches the rightmost state it will resolve its uncertainty. At each state the agent has the option to move \(\), paying a cost of \(\) where \( L 1\), or move \(\) with no cost. We see that there are only \(2\) consistent trajectories: either take action \(\) at state \(s_{1}\) (optimal for \(^{-}\)), or repeat actions \(\) to reach \(s_{L}\) (optimal for \(^{+}\)). We detail how a selection of algorithms perform on this problem in Table 1.

It is worth examining in detail what has gone wrong for the policy that samples according to \(_{}(^{*})\). Even if the agent has traveled to the right a few times it will exit the chain with probability \(0.5\) at each new state, which leads to poor exploration. Taking action \(\) a few times and then exiting the chain has zero probability of being optimal under the posterior, so this sequence of actions is _inconsistent_ with the posterior. Conditioning on the event \(^{*}(s)\) when at state \(s\) forces the policy to be consistent with the set of the beliefs that would get the agent to state \(s\) in the first place, which, as we prove later, yields deep exploration. In our chain example, the only belief that takes action \(\) is the one in which the end state has reward \(+1\), so the only consistent policy conditioned on having taken \(\) once is to continue all the way to the end state. The same issue arises in Thompson sampling, where in order to achieve consistency the agent samples a policy from its posterior and keeps it fixed for the entire episode since resampling at every timestep leads to inefficient exploration . As we shall show in Section 6, Thompson sampling is exactly equivalent to sampling from \(_{^{*}}\). In conclusion:

 action optimality: & \(a_{l}_{}(^{*}_{l}(s_{l})=)\) & \(\) Poor exploration, \\ state-action optimality: & \(a_{l}_{}(^{*}_{l}(s_{l})=|\;^{*}_{l}(s_{l})) _{}(^{*}_{l}(s_{l},))\) & \(\) Efficient exploration. \\ 

 Policy & Agent starts at \(s_{1}\) & Agent reaches \(s_{l}\), \(l>1\) & Expected \# episodes \\  & \(_{1}(s_{1},a)\) for \(a\{,\}\) & \(_{l}(s_{l},a)\) for \(a\{,\}\) & to reach \(s_{L}\) \\  Bayes-optimal & \((0,1)\) & \((0,1)\) & \(1\) \\  Approximation 1 &  &  &  & )\)} \\ (e.g., Soft Q-Learning\()\) & & & & \\  Marginal \(_{}(^{*})\) & \((0.5,0.5)\) & \((0.5,0.5)\) & \((2^{L})\) \\  Conditional \(_{}(^{*}|\;^{*})\) & \((0.5,0.5)\) & \((0,1)\) & \(2\) \\  Thompson Sampling &  &  &  &  \\ (1,0) & & & & \\  VAPOR & \((0,1)\) & \((0,1)\) & \(1\) \\ 

Table 1: Comparison of algorithm performance on MDP in Figure 1. Unprincipled approaches like Soft Q-learning take time exponential in \(L\) to reach the uncertain reward state. Naively using the marginal posterior probability of _action optimality_ as the policy at each state also requires exponential time, however the policy derived from the posterior probability of _state-action optimality_\(_{^{*}}\) takes _consistent_ actions and is therefore efficient. Similar good performance is achieved by Thompson sampling (implicit approximation of \(_{^{*}}\), Section 6) and VAPOR (explicit approximation of \(_{^{*}}\), Section 4).

A Variational Bayesian Approach

Having access to \(_{^{*}}\) is sufficient to enable deep exploration, but computing this probability is intractable in general. In this section, we derive a _variational_, _i.e._, optimization-based, approach to approximate \(_{^{*}}\). In most variational Bayesian approaches , the optimization objective is to find a surrogate probability measure that minimizes some dissimilarity metric (_e.g._, a KL-divergence) from the intractable probability measure of interest. Yet unlike standard variational inference techniques, we cannot minimize the KL-divergence to this distribution as we do not have access to samples. However, we do have a downstream _control_ objective. That is to say, we are not simply interested in approximating the distribution, but also in our approximation doing well in the control problem. This insight provides an alternative objective that takes into account the rewards and uncertainties that the agent is likely to encounter. The optimal value function \(V^{*}\) is a random variable under the beliefs \(\). Our next lemma relates the expected value of \(V^{*}\) under \(\) to our state-action optimality event \(^{*}\).

**Lemma 2**.: _It holds that_

\[_{s}_{}V^{*}_{1}(s)=_{l,s,a} _{}(^{*}_{l}(s,a))_{}[r_{l}(s,a) ^{*}_{l}(s,a)].\]

### Information Theoretic Upper Bound

Lemma 2 depends on \(_{^{*}}\), but the conditional expectation is not easy to deal with. To handle this, we upper bound each \(_{}[r_{l}(s,a)^{*}_{l}(s,a)]\) in Lemma 2 using tools from information theory. To simplify the exposition, we consider the following standard sub-Gaussian assumption [70; 61; 47] and we defer to Appendix F.3 the treatment of the general case. We say that \(X:\) is \(\)-sub-Gaussian for \(>0\) if \((c(X-X))(c^{2}^{2}/2)\), for all \(c\).

**Lemma 3**.: _If \(r_{l}(s,a)\) is \(_{l}(s,a)\)-sub-Gaussian under \(\) for \(l[L]\) and \((s,a)_{l}\), then_

\[_{}[r_{l}(s,a)^{*}_{l}(s,a)] _{}r_{l}(s,a)+_{(s,a)>0}(^{2}(s,a)}{2(s,a)}-_{l}(s,a)_{}(^{*} _{l}(s,a)))\] \[=_{}r_{l}(s,a)+_{l}(s,a)_{}(^{*}_{l}(s,a))}.\]

### Optimization Problem

Finally, we combine Lemmas 2 and 3 to reveal a concave function in \(_{^{*}}\) that upper bounds the value function objective \(_{s}_{}V^{*}_{1}(s)\). For any \(^{s,}_{+}\) and occupancy measure \((P)\), we define the \(\)_-weighted entropy_ of \(\) (summed over steps \(l\)) as \(_{}()-_{l,s,a}_{l}(s,a)_{l}(s,a )_{l}(s,a)\). In the following definition we take division and the square-root to be applied elementwise, and \(\) denotes elementwise multiplication. With this we can define the following _optimistic_ value functions

\[_{}(,) ^{}(_{}r+ }{2})+_{}(), \] \[_{}() _{^{s,}_{+}}_ {}(,)=^{}(_{}r+). \]

**Lemma 4**.: _For known \(P\) and \(\)-sub-Gaussian \(r\), we have_

\[_{s}_{}V^{*}_{1}(s)_ {}(_{^{*}})_{(P)}_{ }(), \]

_where the VAPOR optimization problem is concave._

The above optimization problem is our _variational_ objective, the solution of which yields an occupancy measure that approximates \(_{^{*}}\). We call the approach "VAPOR", for variational approximation of the posterior probability of optimality in RL. We discuss various properties of the optimization problem in Appendix E (its unconstrained dual problem and message passing interpretation).

Since we are using variational inference a natural question to ask is how well our variational solution approximates \(_{^{*}}\). Here we provide a bound quantifying the dissimilarity between \(_{^{*}}\) and the solution of the VAPOR optimization problem, according to a weighted KL-divergence, where for any \(^{s,}_{+}\), \(,^{}(P)\) we define \(_{}(^{})_{l,s,a} _{l}(s,a)_{l}(s,a)(_{l}(s,a)/^{}_{l}(s,a))\).

**Lemma 5**.: _Let \(^{*}\) solve the VAPOR optimization problem (4) and \(^{*}*{argmin}_{}_{}(^{*},)\), then_

\[_{^{*}}(_{^{*}}^{*}) _{}(^{*})-_{s}_{}V_{1}^{ *}(s).\]

In other words, the (weighted) KL-divergence between \(_{^{*}}\) and \(^{*}\) is upper bounded by how loose our upper bound in (4) is. In practice however, we care less about a bound on the divergence metric than we do about performance in the control problem. Our next result shows that the variational policy also satisfies a strong sub-linear Bayesian regret bound, _i.e._, the resulting policy performs well.

### Bayesian Regret Analysis

Learning problem.We consider that the agent interacts with the MDP \(\) over a (possibly unknown) number of \(N\) episodes. We denote by \(_{t}\) the sigma-algebra generated by all the history (_i.e._, sequences of states, actions and rewards) before episode \(t\), with \(_{1}=\). We let \(^{t}[]=[_{t}]\), \(^{t}()=(_{t})\). We denote by \(n_{l}^{t}(s,a)\) the visitation count to \((s,a)\) at step \(l\) before episode \(t\), and \(( 1)(,1)\). In the Bayesian approach where \(\) is sampled from a known prior \(\), we want to minimize the _Bayes regret_ over \(T NL\) timesteps of an algorithm alg producing policy \(^{t}\) at each episode \(t\), which is defined as its expected regret under that prior distribution

\[_{}(,T)_{t=1}^{N}_{s }(V_{1}^{*}(s)-V_{1}^{^{t}}(s)),_{ }(,T)_{}_{ }(,T).\]

The VAPOR learning algorithm proceeds as follows: at the beginning of each episode, it solves the VAPOR optimization problem and executes the induced policy. We now show that it enjoys a sub-linear Bayesian regret bound under the following standard assumption .

**Assumption 1**.: _The mean rewards are bounded in \(\) almost surely with independent priors and the reward noise is additive \(\)-sub-Gaussian for a constant \(>0\)._

Assumption 1 implies that the mean rewards are sub-Gaussian under the posterior (see [70, App. D.2]), where we can upper bound the sub-Gaussian parameter \(_{l}^{t}(s,a)+1)/(n_{l}^{t}(s,a) 1)}\) at the beginning of each episode \(t\).

**Theorem 1**.: _For known \(P\) and under Assumption 1, it holds that_

\[_{}(,T)+1) TSA(SA)(1+ T/L)}=().\]

In the above \(\) suppresses log factors. The same regret bound holds for the (intractable) algorithm that uses (1) as the policy each episode.

**Corollary 1**.: _Denote by \(*{alg}_{^{*}}\) the algorithm that produces policies based on \(_{^{*}}^{t}\) for each episode \(t\) using (1). Then, under the same conditions as Theorem 1, we have \(_{}(*{alg}_{^{*}},T)( )\)._

### Interpretation of VAPOR

Inspecting \(_{}\) (3) reveals that the VAPOR optimization problem is equivalent to solving a two-player zero-sum game between a 'policy' player \((P)\) and a 'temperature' player \(_{+}^{,s,4}\). The latter finds the tightest \(\) that best balances two exploration mechanisms on a per state-action basis: an _optimism_ term \(^{2}/\) that augments the expected reward \(_{}\), and a \(\)-weighted _entropy regularization_ term. The policy player maximizes the entropy-regularized optimistic reward.

The fact that entropy regularization falls out naturally from VAPOR is interesting because the standard 'RL as inference' framework conveniently reveals policy entropy regularization (Section 2.1), which has been widely studied theoretically  and empirically for deep RL exploration . The specificity here is that it is with respect to the _occupancy measure_ instead of the policy, and it is _adaptively weighted for each state-action pair_. This enables us to obtain an 'RL as inference' framework with entropy regularization that explores provably efficiently. Regularizing with the (weighted) entropy of the occupancy measure is harder to implement in practice, therefore in Section 7 we propose a principled, albeit looser, upper bound of the VAPOR optimization problem that regularizes the optimistic reward with only the (weighted) entropy of the _policy_, making the resulting objective amenable to online optimization with policy gradients.

**For** episode \(t=1,2,\)**do**

**1.** Compute expected rewards \(^{t}r\), transitions \(^{t}P\), uncertainty measure \(^{t}\)

**2.** Solve VAPOR optimization problem \(^{t}*{argmax}_{(^{t}P)} _{^{t}}()\) from Equation (3)

**3.** Execute policy \(_{l}^{t}(s,a)_{l}^{t}(s,a)\), for \(l=1,,L\)

## 5 Extension to Unknown Transition Dynamics

So far we have focused on the special case of known transition dynamics \(P\). In this section, we derive a generic reduction from the case of unknown \(P\) to known \(P\), which may be of independent interest. We prove that _any_ algorithm that enjoys a Bayesian regret bound in the known-\(P\) special case can be easily converted into one that enjoys a regret bound for the more challenging unknown-\(P\) case. Our analysis relies on the mean rewards being sub-Gaussian under the posterior (which holds _e.g._, under Assumption 1) and on the following standard assumption .

**Assumption 2**.: _The transition functions are independent Dirichlet under \(\), with parameter \(_{l}(s,a)_{+}^{S_{l+1}}\) for each \((s,a)_{l}\) with \(_{s^{}_{l+1}}_{l}(s,a,s^{}) 1\)._

At a high level, our reduction transfers the uncertainty on the transitions to additional uncertainty on the rewards in the form of carefully defined zero-mean Gaussian noise. It extends the reward perturbation idea of the RLSVI algorithm , by leveraging a property of Gaussian-Dirichlet optimism  and deriving a new property of Gaussian sub-Gaussian optimism (Appendix G) which allows to bypass the assumption of binary rewards in \(\{0,1\}\) from [62, Asm. 3]. Our reduction implies the following 'dominance' property of the expected \(V^{}\) under the transformed and original beliefs.

**Lemma 6**.: _Define the mapping \(:\) that transforms the beliefs \(\) into a distribution \(\) on the same space, with transition dynamics equal to \(_{}P\) and rewards distributed as \((_{}r,^{2})\) with_

\[_{l}^{2}(s,a) 3.6^{2}_{l}^{2}(s,a)+}{_{s^{}}_{l}(s,a,s^{})}.\]

_Then it holds that_

\[_{s}_{}V_{1}^{}(s)_{s }_{}V_{1}^{}(s).\]

Now, let us return to the episodic interaction case and denote by \(^{t}(_{t})\) the posterior at the beginning of episode \(t\). Under Assumptions 1 and 2, we can upper bound the uncertainty \(^{t}\) of the transformed posteriors \(^{t}\) as \((_{l}^{t})^{2}(s,a)(3.6^{2}(^{2}+1)+(L-l)^{2})/(n_{l}^{ t}(s,a) 1)\). This brings us to a key result necessary for a general regret bound.

**Lemma 7**.: _Let alg be any procedure that maps posterior beliefs to policies, and denote by \(_{,}(,T)\) the Bayesian regret of alg where at each episode \(t=1,,T\), the policy and regret are computed by replacing \(^{t}\) with \((^{t})\) (Lemma 6), then under Assumptions 1 and 2,_

\[_{}(,T)_{,}(,T).\]

This tells us that if we have an algorithm with a Bayesian regret bound under known transitions \(P\), then we can convert it directly into an algorithm that achieves a regret bound for the case of unknown \(P\), simply by increasing the amount of uncertainty in the posterior of the unknown rewards and replacing the unknown \(P\) with its mean when executing the algorithm. We now instantiate this generic reduction to VAPOR in Algorithm 1. Combining Lemma 7 and Theorem 1 yields the following regret bound.

**Theorem 2**.: _Under Assumptions 1 and 2, it holds that \(_{}(,T)(L)\)._

The Bayes regret bound in Theorem 2 is within a factor of \(\) of the known information theoretic lower bound , up to constant and log terms. It matches the best known bound for K-learning  and Thompson sampling , under the same set of standard assumptions (see Appendix C). There exists a complementary line of work deriving minimax-optimal regret bounds in the _frequentist_ setting, both in the model-based  and model-free  cases. We refer to _e.g._,  for discussion on the advantages and disadvantages of the Bayesian and frequentist approaches. Withoutclaiming superiority of either, it is worth highlighting that compared to frequentist algorithms that tend to be _deterministic_ and _non-stationary_ (_i.e._, explicitly dependent on the episode number), VAPOR is naturally _stochastic_ and _stationary_ (_i.e._, independent of the episode number).

## 6 Connections

Thompson Sampling.Thompson sampling (TS) or posterior sampling for RL (PSRL) [78; 75; 59] first _samples_ an environment (_i.e._, rewards and transition dynamics) from the posterior and then computes the optimal policy for this sample to be executed during the episode. Our principled Bayesian inference approach to RL uncovers an alternative view of TS which deepens our understanding of this popular algorithm. The next lemma shows that TS _implicitly_ approximates \(_{^{*}}\) by sampling, thus tightly connecting it to VAPOR's _explicit_ approximation of \(_{^{*}}\).

**Lemma 8**.: _Let \(^{}\) be the occupancy measure of the TS policy, it holds that \([^{}]=_{^{*}}\)._

Plugging this observation into Lemma 4 and retracing the proof of Theorem 2 immediately yields regret bounds for TS matching that of VAPOR. The explicit variational approximation of \(_{^{*}}\) by VAPOR has several advantages over the implicit sampling-based approximation of \(_{^{*}}\) by TS:

* Having direct (approximate) access to this probability can be desirable in some practical applications, for example to ensure safety constraints or to allocate budgets.
* TS suffers linear regret in the multi-agent and constrained cases , while we expect suitable modifications of VAPOR to be able to handle these cases, as suggested in the multi-armed bandit setting .
* The VAPOR optimization problem can naturally extend to parameterizing the occupancy measure to be in a certain family (_e.g._, linear in some basis).
* The objective we use in VAPOR is _differentiable_, opening the door to differentiable computing architectures such as deep neural network. In contrast, performing TS requires to maintain an explicit model over MDP parameters , which becomes prohibitively expensive as the MDP becomes large. To alleviate this computational challenge of TS, some works have focused on cheaply generating approximate posterior samples [57; 55; 62; 54], but it is not yet clear if this approach is better than the variational approximation we propose.

K-learning.K-learning  endows the agent with a risk-seeking exponential utility function and enjoys the same regret bound as Theorem 2. Its update rule is similar to soft Q-learning [19; 25; 44; 22], which emerges from inference under Approximation 1 . Although soft Q-learning ignores epistemic uncertainty and thus suffers linear regret , K-learning _learns_ a _scalar_ temperature parameter which balances both an optimistic reward and a policy entropy regularization term. In contrast, VAPOR naturally produces a _separate_ risk-seeking parameter for _each state-action_. This enables a fine-grained control of the exploration-exploitation trade-off depending on the region of the state-action space, which can make a large difference in practice. In the multi-armed bandit case it was shown that K-learning and VAPOR coincide when VAPOR is constrained to have all temperature parameters (denoted \(\)) equal . It can be shown (see Appendix F.12) that the same observation holds in the more general MDP case, up to additional entropic terms added to the K-learning'soft' value functions, which only contribute a small constant to the regret analysis. This sheds a new light on K-learning as a variational approximation of our probabilistic inference framework with the additional constraint that all 'temperature' parameters are equal.

Maximum Entropy Exploration.A recent line of work called _maximum entropy exploration_ suggests that in the absence of reward the agent should cover the state-action space as uniformly as possible by solving a maximum-entropy problem over occupancy measures \(_{(P)}()\)[29; 9; 85; 79], where \(\) denotes the entropy summed over steps \(l\). VAPOR can thus be interpreted as regularizing a reward-driven objective with a maximum entropy exploration term, that is weighted with vanishing temperatures. In other words, the principle of (weighted) maximum-entropy exploration can be derived by considering a variational approximation to Bayesian state-action optimality.

A Policy-Gradient Approximation

Up to this point, we instantiated our new Bayesian 'RL as inference' approach with a tabular, model-based algorithm that exactly solves the variational optimization problem and has a guaranteed regret bound (Algorithm 1). In this section, we derive a principled, albeit looser, upper-bound approximation of the variational optimization problem, which can be solved using policy-gradient techniques by representing the policy using a deep neural network. This will yield a scalable, model-free algorithm that we call VAPOR-lite.

Denoting by \(^{}\) the stationary state distribution of a policy \(\), note the relation \(_{l}^{}(s,a)=_{l}(s,a)_{l}^{}(s)\). The regularization term of VAPOR can thus be decomposed in a (weighted) policy entropy regularization term and a (weighted) stationary state distribution entropy term, the latter of which is challenging to optimize in high-dimensional spaces [29; 36; 43]. In light of this, we introduce the following VAPOR-lite alternative optimization problem

\[_{} _{l,s}_{l}^{}(s)_{a}_{l}(s,a)(r_{l}( s,a)+_{l}(s,a)+_{_{l}(s,)}(_{l} (s,))), \]

where we define the weighted policy entropy \(_{_{l}(s,)}(_{l}(s,))-_{a }_{l}(s,a)_{l}(s,a)_{l}(s,a)\). Akin to policy-gradient methods, VAPOR-lite now optimizes over the policies \(\) which can be parametrized by a neural network. It depends on an uncertainty signal \(\) which can also be parametrized by a neural network (_e.g._, an ensemble of reward predictors, as we consider in our experiments in Section 8). Compared to VAPOR, VAPOR-lite accounts for a _weaker_ notion of weighted entropy regularization (_i.e._, in the policy space instead of the occupancy-measure space), which allows to solve the objective using policy-gradient techniques. For simplicity it also sets the temperatures \(\) to the uncertainties \(\) instead of minimizing for the optimal ones. Importantly, VAPOR-lite remains a principled approach. We indeed prove the following relevant properties of VAPOR-lite in Appendix H: **(i)** although the problem is no longer concave in \(\), solving it in the space of occupancy measures \(\) remains a computationally tractable, concave optimization problem, **(ii)**\(\)-lite upper bounds the VAPOR objective (up to a multiplicative factor in the uncertainty measure and a negligible additive bias), and **(iii)** it yields the same \((L)\) regret bound as VAPOR for a careful schedule of uncertainty measures \(\) over episodes.

Essentially, VAPOR-lite endows a policy-gradient agent with **(i)** an uncertainty reward bonus and **(ii)** an uncertainty-weighted policy entropy regularization. The latter exploration mechanism is novel and has an appealing interpretation: Unlike standard policy entropy regularization, it adaptively accounts for epistemic uncertainty, by eliminating actions from the entropy term where the agent has low uncertainty for each given state. In the limit of \( 0\) (_i.e._, no uncertainty), the regularization of VAPOR-lite vanishes and we recover the original policy-gradient objective.

## 8 Numerical Experiments

GridWorld.We first study empirically how well VAPOR approximates \(_{^{*}}\). Since \([^{}]=_{^{*}}\), we estimate the latter by averaging over \(1000\) samples of the (random) TS occupancy measure (we denote it by \(^{}_{}\)). We design simple \(10 10\) GridWorld MDPs with four cardinal actions, known dynamics and randomly generated reward. Figure 2 suggests that VAPOR and the TS average output similar approximations of \(_{^{*}}\), thus showing the accuracy of our variational approximation in this domain. Unlike TS which requires estimating it with many samples, VAPOR approximates \(_{^{*}}\) by only solving a single convex optimization problem.

DeepSea.We now study the learning performance of the VAPOR Algorithm 1 in the hard-exploration environment of _DeepSea_. In this \(L L\) grid MDP, the agent starts at the top-left cell and must reach the lower-right cell. It can move left or right, always descending to the row below. Going left yields no reward, while going right incurs a small cost of \(0.01/L\). The bottom-right cell yields a reward of \(1\), so that the optimal policy is to always go right. Local dithering (_e.g._, Soft Q-learning, \(\)-greedy) takes time exponential in \(L\), so the agent must perform deep exploration to reach the goal. Figure 3 shows the time required to'solve' the problem as a function of the depth \(L\), averaged over \(10\) seeds. The 'time to solve' is defined as the first episode where the rewarding state has been found at least in 10% of the episodes so far . We compare VAPOR to TS /PSRL , K-learning  and a variant of RLSVI  that runs PSRL under the transformed posteriors of Lemma 6. (Several

[MISSING_PAGE_EMPTY:10]