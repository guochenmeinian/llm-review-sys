# ProBio: A Protocol-guided Multimodal Dataset for Molecular Biology Lab

Jieming Cui\({}^{1,2,}\)

Cuijieming@stu.pku.edu.cn

&Ziren Gong\({}^{3,}\)

ziren.gong@outlook.com

Baoxiong Jia\({}^{2,}\)

jiabaoxiong@bigai.ai

&Siyuan Huang\({}^{2}\)

syhuang@bigai.ai

&Zilong Zheng\({}^{2,}\)

zlzheng@bigai.ai

&Jianzhu Ma\({}^{3,4,}\)

majianzhu@tsinghua.edu.cn

Yixin Zhu\({}^{1,2,5,}\)

yixin.zhu@pku.edu.cn

J. Cui, Z. Gong, and B. Jia contributed equally.\(\) corresponding authors

\({}^{1}\) Institute for Artificial Intelligence, Peking University

\({}^{2}\) National Key Laboratory of General Artificial Intelligence

\({}^{3}\) Institute for AI Industry Research, Tsinghua University

\({}^{4}\) Department of Electronic Engineering, Tsinghua University

\({}^{5}\) PKU-WUHAN Institute for Artificial Intelligence

###### Abstract

The challenge of replicating research results has posed a significant impediment to the field of molecular biology. The advent of modern intelligent systems has led to notable progress in various domains. Consequently, we embarked on an investigation of intelligent monitoring systems as a means of tackling the issue of the reproducibility crisis. Specifically, we first curate a comprehensive multimodal dataset, named **ProBio**, as an initial step towards this objective. This dataset comprises fine-grained hierarchical annotations intended for studying activity understanding in Molecular Biology Lab (BioLab). Next, we devise two challenging benchmarks, transparent solution tracking, and multimodal action recognition, to emphasize the unique characteristics and difficulties associated with activity understanding in BioLab settings. Finally, we provide a thorough experimental evaluation of contemporary video understanding models and highlight their limitations in this specialized domain to identify potential avenues for future research. We hope **ProBio** with associated benchmarks may garner increased focus on modern AI techniques in the realm of molecular biology.

[https://probio-dataset.github.io](https://probio-dataset.github.io)

## 1 Introduction

Despite notable progress in scientific research, the challenge of reproducing research findings has surfaced as a significant obstacle. Baker (2016) suggests that a significant proportion of researchers, exceeding 70%, have reported unsuccessful attempts to replicate experiments carried out by their colleagues, primarily attributed to inadequate clarity of protocols (Ioannidis, 2005; Begley and Ellis,2012). These protocols (_i.e._, procedural instructions) frequently exclude crucial factors, such as temperature or pH, that can substantially impact the results. As a result, researchers rely heavily on mentorship from seasoned experts when conducting experiments. The management of such experiments necessitates a significant investment of time and resources, often unattainable for many laboratories, particularly in Molecular Biology Labs (BioLabs), wherein replicating experiments is more time-consuming and costly (Calne, 2016). Since modern intelligent systems have brought significant advancements in various fields (Grosan and Abraham, 2011; Gretzel, 2011; Stephanopoulos and Han, 1996; Tavakoli et al., 2020), there is a growing need to develop an AI assistant to tackle this reproducibility crisis. Toward building such an AI assistant, we set off to curate a multimodal dataset recorded in BioLabs with benchmarks of modern AI methods.

Due to the inherent characteristics of BioLab, constructing this multimodal dataset faces **two grand challenges**. The **first** one is the lack of readily available protocols with _sufficient_ details; existing ones typically only provide high-level guidance (Latour, 1987; Cetina, 1999; Lopez-Rubio and Ratti, 2021; Peterson and Panofsky, 2021), lacking details necessary for reproducing results step by step. An example of process failure in cell culturing occurs when the culture medium is not inverted correctly after the addition of yeast. Nevertheless, such crucial information is frequently regarded as a standard practice in research and disregarded in written protocols. The heterogeneity of experimental instructions exacerbates the complexity of this scenario, as different labs document identical tasks in diverse manners contingent upon their respective resource availability (Braybrook, 2017). Therefore, curating standardized protocols with _sufficient_ details, coupled with videos of each instruction's execution, is essential for building an AI assistant for BioLab.

The **second** challenge pertains to comprehending domain-specific actions and objects at the intricate level of granularity. From the computer vision perspective, this poses a challenging task for achieving fine-grained understanding in contrast to typical scenarios like sports (Shao et al., 2020; Xu et al., 2022) or instructional videos (Zhang et al., 2023; Tang et al., 2019; Miech et al., 2019; Das et al., 2013; Zhou et al., 2018; Zhukov et al., 2019). The complexity of event understanding in BioLab is primarily attributed to the specialized instruments and the ambiguity of actions involved. For instance, experiments commonly involve liquid transfer between visually similar and transparent containers (Liang et al., 2016, 2018), posing additional challenges in object detection and event parsing (Jia et al., 2020; Huang et al., 2023). Moreover, actions that are perceptually similar may have divergent semantic meanings across various experiments due to the strong dependence between actions and experimental contexts (Stacy et al., 2022; Jiang et al., 2022, 2021; Chen et al., 2021). These visual ambiguities (Fan et al., 2022; Zhu et al., 2020; Zhu, 2018) have been mostly left untouched in prior arts (Murray et al., 2012; Shao et al., 2020; Goyal et al., 2017; Kay et al., 2017; Zhu et al., 2022; Panda et al., 2017; Kanehira et al., 2018) and present an ideal and unique testbed for _multimodal_ video understanding (Wang et al., 2022b; Huang et al., 2023).

We present **ProBio**, the first protocol-guided multimodal dataset in BioLab to tackle the above challenges. **ProBio** provides (i) a meticulously curated set of detailed and standardized protocols

Figure 1: **An overview of two challenging tasks identified and presented in **ProBio**. (**)denotes a set of cameras, and **)denotes intelligent monitoring models with access to BioLab protocols. Task (a): track transparent solutions. Task (b): action understanding guided by protocols. Object IDs share the same color across frames and in the texts.

with corresponding video recordings for each experiment and (ii) a natural and systematic evaluation framework for fine-grained multimodal activity understanding; see Fig. 1. We construct **ProBio** by selecting a set of 13 frequently conducted experiments and augmenting existing protocols by incorporating three-level hierarchical annotations. This configuration yields 3,724 practical-experiment instructions and 37,537 Human-Object Interaction (HOI) annotations with an overall length of 180.6 hours; see Sec. 3. We design two tasks in **ProBio**: transparent solution tracking and multimodal action recognition, assessing models' capability to leverage both visual observations and protocols to discern unique environmental states and actions. In light of the significant disparities observed between human and model performance in action recognition, we devise diagnostic splits that stratify experimental instruction into three categories based on difficulties (_i.e_., easy, medium, hard). We hope **ProBio** and associated benchmarks will foster new insights to mitigate the reproducibility crisis in BioLab and promote fine-grained multimodal video understanding in computer vision.

This paper makes three primary contributions:

* We introduce **ProBio**, the first protocol-guided dataset with dense hierarchical annotations in BioLab to facilitate the standardization of protocols and the development of intelligent monitoring systems for reducing the reproducibility crisis.
* We propose two challenging benchmarking tasks to measure models' capability in leveraging both visual observations and language protocols for fine-grained multimodal video understanding, especially for ambiguous actions and environment states.
* We provide an extensive experimental analysis of the proposed tasks to highlight the limitations of existing multimodal video understanding models and point out future research directions.

## 2 Related work

Fine-grained activity datasetsAction understanding has been a long-standing problem in computer vision with successful attempts in data curation (Murray et al., 2012; Kay et al., 2017; Soomro et al., 2012; Caba Heilbron et al., 2015; Monfort et al., 2019). To provide fine-grained activity annotations, datasets (Goyal et al., 2017; Stein and McKenna, 2013; Damen et al., 2020; Jia et al., 2020; Rai et al., 2021; Luo et al., 2022b; Grauman et al., 2022) come with HOI labels, object bounding boxes, hand masks, _etc_. Tab. 1 compares **ProBio** with existing datasets.

The task of delineating intricate action hierarchies for daily activities is challenging. One line of work justifies action hierarchy design by examining activities in sports (Shao et al., 2020; Xu et al., 2022) and kitchens (Kuehne et al., 2014; Li et al., 2018; Damen et al., 2020). The annotations provided, while detailed, may lack strong contextual information and may not be suitable for complex tasks that demand nuanced multimodal understanding. Another line of work leverages furniture assembly (Ben-Shabat et al., 2021; Sener et al., 2022; Zhang et al., 2023) as a means to highlight action dependencies. Nonetheless, the practical applications of these tasks are limited.

Multimodal video understandingComplex video understanding tasks require leveraging context in addition to direct visual inputs. Instructional videos (Zhou et al., 2018; Tang et al., 2019; Zhukov et al., 2019; Miech et al., 2019), as the most readily available multimodal video learning source, have been frequently utilized for various multimodal video understanding tasks, such as retrieval

  &  &  &  &  &  &  &  &  \\    & & & & & & & & & **cls** & **num** \\   & EPIC-Kitchen (2020) & 100h & 90,000 & ✓ & ✓ & ✗ & ✗ & ✗ & 20,000 & 39,996 \\  & YouCook2 (2018) & 175.6h & 13,829 & ✓ & ✓ & ✗ & ✗ & ✗ & 89 & 2,000 \\  & LEMMA (2020) & 10.1h & 11,781 & ✗ & ✓ & ✗ & ✓ & ✓ & 15 & 324 \\  & COIN (2019) & 476.63h & 46,354 & ✓ & ✓ & ✗ & ✗ & ✓ & 180 & 11,1287 \\  & HowTo(Anne Hendricks et al., 2017; Wang et al., 2019), captioning (Zhou et al., 2018; Xu et al., 2016; Yu et al., 2019), and question answering (Li et al., 2016; Lei et al., 2018; Grunde-McLaughlin et al., 2021; Xiao et al., 2021; Yang et al., 2021; Jia et al., 2022). These datasets are oftentimes large in scale and curated from internet videos with language primarily sourced from online encyclopedia platforms or transcribed from subtitles. The quality of the language modality is considerably impeded by the substantial human effort required to refine insufficient and inaccurate language descriptions (Miech et al., 2019). In addition, the extensive scale of data necessitates the frequent utilization of pre-trained models from other modalities, such as images, for the purpose of multimodal video understanding (Wang et al., 2021; Zellers et al., 2021; Xu et al., 2021; Luo et al., 2022a). Nevertheless, adapting such pre-trained models to specialized domains, such as BioLabs, presents a formidable challenge due to the distinctive nature of objects and actions involved. To tackle these issues, \(\)**ProBio** provides aligned video-protocol pairs, accompanied by detailed experimental instructions for every procedure. Benchmarks on \(\)**ProBio** offer a comprehensive examination of existing models for multimodal video comprehension in specific domains.

## 3 The \(\)**ProBio** Dataset

\(\)**ProBio** consists of 180.6 hours of multi-view recordings that encompass 13 common biology experiments conducted in BioLab. This section introduces the collection (Sec. 3.1) and annotation (Sec. 3.2) process of \(\)**ProBio** and describes the associated benchmarking tasks (Sec. 3.3).

### Data collection

Biology protocolWe have assembled a collection of standard protocols along with comprehensive instructions, and videos guided by these protocols, all included in \(\)**ProBio**. We collect our protocol database by first crawling publicly available protocols published in top-tier journals and conferences. As these protocols often contain only high-level instructions, commonly referred to as brief experiments (brf_exp), we construct an online annotation tool for seasoned researchers to augment them with additional experimental instructions, commonly referred to as practical experiments (prc_exp); of note, this augmentation could be different from experiments to experiments, resulting in a one-to-many mapping from brief to practical experiments. After this augmentation, we select 13 brief experiments with multiple practical experiments and instruct seasoned researchers in BioLab to perform. Please refer to Appx. A.2 for additional details.

Monitoring videoThe video data is recorded in a laboratory that adheres to the international standard for molecular biology (Nest.Bio Labs, 2023). A total of ten cameras are installed to oversee all experimental procedures. To ensure the quality and clarity of the collected videos, we consult with experienced biological researchers regarding the cameras' viewpoints and positions. A total of eight high-resolution RGB cameras are affixed above the operation tables and instruments. To capture intricate HOIs in detail, two supplementary RGB-D cameras have been positioned in close proximity to the primary operating table and sterility chamber. Over 700 hours of video are collected through a 24-hour monitoring process, which minimizes disruption to the researchers' regular activities. The raw videos undergo additional processing through two steps: (i) automatically filtering of no-action frames using OpenPose (Cao et al., 2017) and YOLOv5 (Ultralytics, 2022), and (ii) manual removal of frames depicting actions unrelated to the intended focus, such as conversing, note-taking, or texting. A total of 180.6 hours hours video pertaining to the 13 selected brief experiments has been obtained.

### Data annotation

In molecular biology experiments, it is common for routine operations to occur periodically. To facilitate the annotation process, a representative and distinct subset of video clips is selected. This subset consists of a total of 9.64 hours top-down view videos and 1.05 hours nearby-view videos, marked with detailed action labels to provide clear, fine-grained information. During the process of annotation, we consider (i) detailed HOIs in the form of HOI pairs for each frame and (ii) object segmentation masks for each interacted object. To establish a connection between the fine-grained annotations and the underlying biological experiment, supplementary annotations are furnished to denote the precise location of each action within the brief and practical experiments. This process establishes a hierarchical structure consisting of three levels, encompassing fine-grained action data for multimodal video understanding and categorical information for future research on intelligent 

[MISSING_PAGE_FAIL:5]

We evaluate the models' capability via TransST, leveraging all nearby-view videos with liquid solution labels. Each tracking problem includes the bounding box of the target object (_e.g_., a tube) and the category label of the liquid solution inside (_e.g_., double-distilled water). We further consider two diagnostic settings, pure visual and protocol-guided, to confirm the significance of protocols. Protocol-guided tracking leverages practical experiments as additional input to equip models with information w.r.t. invisible solution status changes. Please refer to Sec. 4.1 and Appx. B.1 for details.

Multimodal action recognition (MultiAR)An intelligent monitoring system in BioLabs must recognize actions and identify the corresponding protocol to track the experimental progress. However, establishing such a capability is challenging in BioLab: perceptually similar motions may have divergent semantic interpretations, and the same sub-experiment protocols across different experiments may refer to different meanings. However, current datasets have neglected the ambiguity present within fine-grained actions (Murray et al., 2012; Shao et al., 2020; Goyal et al., 2017; Kay et al., 2017; Zhu et al., 2022; Panda et al., 2017; Kanehira et al., 2018). Currently, there is no universally recognized standard for quantifying the ambiguity present in various actions. Our experiment indicates that the straightforward approach of using the similarity of human-object interactions hoi (_e.g_. Jaccard coefficient) is insufficient for adequately capturing both object ambiguity and procedural ambiguity. To address this, we propose a method for defining the ambiguity between two actions by employing the bidirectional Levenshtein distance ratio, as illustrated in Equation (1). In this equation, \(P(A)\) and \(P(B)\) signify the power sets of the given sets \(A\) or \(B\) of hoi. \(ratio\) here refers to the Levenshtein distance ratio. Notably, the ambiguity (labeled as \(amb\)) between two practical experiments can exceed a value of 1, indicating a significant similarity between the two procedures or experiments, referred to as prc_exp. To measure the average ambiguity of each action, we then introduce a method for

    & **ambiguity** & **hours** & **frame** & **segmap** & **brf\_exp.cls** & **prc\_exp.cls** & **hoi.cls** & **hoi.num** & **obj.cls** & **obj.num** & **action.cls** & **action.num** \\   & **easy** & 5.1 & 17485 & 75371 & 11 & 52 & 155 & 22965 & 36 & 22965 & 20 & 22965 \\  & **medium** & 2.81 & 7890 & 33205 & 13 & 19 & 63 & 10651 & 13 & 10651 & 16 & 10651 \\  & **hard** & 1.74 & 2492 & 8561 & 9 & 8 & 57 & 5866 & 11 & 5866 & 17 & 5866 \\  & **total** & 9.64 & 2629 & 112937 & 13 & 79 & 245 & 37537 & 48 & 75372 & 21 & 37537 \\   & **hours** & **frame** & **segmap** & **brf\_exp.cls** & **brf\_exp.num** & **prc\_exp.cls** & **prc\_exp.num** & **obj.cls** & **obj.num** & **solicls** & **sol. num** \\   & 1.05 & 41725 & 100424 & 6 & 31 & 17 & 34 & 14 & 90888 & 12 & 40443 \\   

Table 2: **Statistics of data and annotations used for TransST and MultiAR. segmap** denotes segmentation maps in each data split. **brf_exp** and **prc_exp** denote the brief and practical experiments. **sol** denotes solutions. We use the suffix **cls** to indicate the number of annotation categories for certain data categories and the suffix **num** to indicate the number of annotated instances in that data category.

Figure 3: (a) Despite the discrepancy between video lengths in TransST and MultiAR, we provide a comparable number of segmentation maps as ground truths in TransST for solution tracking. (b) We split the videos in MultiAR based on the ambiguity level of protocols, resulting in a 6:3:1 easy/medium/hard split. (c) One protocol is defined as _hard_ when its ambiguity score surpasses 0.7, _easy_ when below 0.45, and _medium_ otherwise.

calculating the average ambiguity for each action, expressed mathematically as \(_{amb N}amb_{i}\):

\[amb=*_{x P(A)}_{y P(B)}(ratio(x,y))+ *_{y P(B)}_{x P(A)}(ratio(y,x)). \]

As depicted in Fig. 3, there is considerable overlap among most practical experiments, leading to ambiguity when trying to distinguish them based solely on sequences of HOI. More comprehensive visual results of this phenomenon are presented in Appx. B.2. Considering the common occurrence of overlapping atomic actions, we focus on protocol-level ambiguity in MultiAR and leave perceptual-level action ambiguity as a natural intermediary challenge for models. The MultiAR benchmark is a protocol-level action recognition task with all annotated top-down view videos in \(}\). We split all videos into three folds (_i.e._, easy, medium, and hard) to evaluate protocol-level ambiguity. Over these splits, we devise four benchmarking settings: protocol-only, vision-only, vision with brief experiment guidance, and vision with detailed protocol guidance. In the protocol-only setting, we provide ground-truth HOI annotations (_i.e._, perfect perception) to models as a performance upper bound. We add protocols of varied granularity to multimodal learning training in protocol-guided scenarios. Vision-only models are tasked to recognize protocol-level activities during testing.

## 4 Experiments

In this section, we evaluate and analyze the performance of models on tasks associated with \(}\) Particularly, we provide details of the experimental setup, evaluation metrics, and result analysis for TransST and MultiAR. Fundamentally, we aim to address the following questions:

* How challenging is the fine-grained understanding of objects and actions in BioLab?
* How crucial are the protocols in tasks associated with \(}\)
* What is missing in existing models when adapted to the specialized BioLab environment?

### TransST

SetupAs mentioned in Sec. 3.3, we consider two settings in TransST: visual tracking and protocol-guided tracking. The training, validation, and testing sets are divided in a 6:3:1 ratio, respectively, across all videos captured from nearby perspectives. In **visual tracking**, we select a number of leading-edge models to serve as our baseline comparisons, including TransATOM (Xie et al., 2020), StrongSORT with different detection backbones (Brostrom, 2022; Wang et al., 2022a), and Segment-and-Track-Anything (Cheng et al., 2023) based on SAM (Kirillov et al., 2023). Since SAM (Sequential Attention Model) is initially trained on general images, we adopt the strategy suggested in Chen et al. (2023) and integrate a five-layer convolutional SAM adapter. This approach is intended to adapt the SAM weights for effective application within the BioLab setting. Regarding **protocol-guided tracking**, our preliminary experiments indicate that narrowing down the category of liquid solution types to only categories mentioned in the protocols is more effective than learning-based designs (_e.g._, fusing protocol features with tracking features). Please refer to Appx. B.1 for details.

Evaluation metricsFollowing Fan et al. (2019, 2021), we measure the tracking quality by the precision (PRE) and normalized precision (NPRE) with an intersection-over-union (IoU) over 0.45. In addition, we evaluate the prediction of the solution status within the tracked bounding box with classification accuracy (CLS). To provide a comprehensive analysis of models, we report the memory and time overhead of all methods in transparent solution tracking.

  
**Categories** & **Method** & **PRE \(\)** & **NPRE \(\)** & **CLS \(\)** & **FPS \(\)** & **Param \(\)** \\   & TransATOM (2021a) & 27.54 & 32.20 & 29.36 & 26.0 & 7.54M \\  & YOLOv5 (2022) + StrongSORT (2022) & 47.71 & 49.49 & 42.43 & 27.9 & 86.19M \\  & YOLOv7 (2022a) + StrongSORT (2022) & 59.27 & 66.41 & 57.22 & **35.8** & **6.22M** \\  & SAM (2023) + DeAOT (2022) & 91.07 & 96.94 & 45.83 & 2.3 & 641.27M \\   & YOLOv7 (2022a) + StrongSORT (2022) & 60.25 & 67.11 & 61.94 & 35.4 & **6.22M** \\  & SAM (2023) + DeAOT (2022) & **92.40** & **97.46** & **62.43** & 2.1 & 641.27M \\   

Table 3: **Tracking results of all models in TransST. We visualize the best results in bold.**Results and analysisWe report transparent solution tracking results in Tab. 3 and visualize qualitative results in Fig. 4. Specifically, we summarize our major findings as follows:

* **Visual tracking in TransST is challenging.** As shown in Tab. 3, the performance of traditional (_e.g._, StrongSORT) tracking models with only visual inputs is significantly lower than the near-perfect performance they present in common tracking scenarios (_e.g._, driving). We have achieved higher detection efficiency while maintaining computational speed, resulting in an optimal trade-off. In TransST, solutions and containers often have transparent appearances and similar shapes that are visually difficult to distinguish. The frequent occlusion further complicates this because of containment relationship changes in biology experiments. All these facts add difficulty to the visual tracking problem in BioLab environments.
* **More robust object detectors benefit tracking in TransST.** We observe a consistent performance improvement when adopting more robust object detectors (_e.g._, SAM). As these models are often pre-trained on large-scale object detection and segmentation datasets, we believe they are beneficial for mitigating the discrepancy between objects in daily life and specialized domains. However, limited by their memory and computation overhead, these models are still unsuitable for real-time monitoring. This urges the need for lightweight adaptations of existing pre-trained models for specialized downstream domains.
* **Understanding protocols is crucial in TransST.** As explained in Sec. 3.3, tracking the status of transparent liquid within containers is difficult as there are no direct visual features indicating the transition of liquid status. This makes label prediction for the tracked solution extremely challenging without protocol information. Our results on the classification accuracy reflect this fact, showing that event with the simplest heuristic of answer filtering, adding experimental protocols can significantly improve label prediction for all models. However, this improvement is still marginal. This implies that current models still fall short of reasoning about solution types. This promotes future research on multimodal methods for inferring visually unobservable object status changes.

### MultiAR

SetupAs discussed in Sec. 3.3, we evaluate performance under four distinct settings: protocol-only, vision-only, vision with brief experiment guidance, and vision with detailed protocol guidance. Similar to experiments in Sec. 4.1, we randomly split video data at each ambiguity level into train/validation/test with a 6:3:1 ratio. We evaluate the performance of BERT (Devlin et al., 2018) and SBERT (Reimers and Gurevych, 2019) in the protocol-only setting. For the vision-only scenario, we propose finetuning state-of-the-art (SOTA) video recognition models for the MultiAR task. This includes I3D (Carreira and Zisserman, 2017), SlowFast (Feichtenhofer et al., 2019), and Multiscale Vision Transformers (MViT) (Fan et al., 2021; Li et al., 2022). For settings with detailed protocol guidance, we select strong baselines, including ActionCLIP (Wang et al., 2021), EVL (Lin et al., 2022), and Vita-CLIP (Wasim et al., 2023). We feed additional protocol information into these models

Figure 4: **Examples of success and failure cases in two tasks.****Top:** Tracking results of _SAM-adapter+DeAoT_ with protocol-guidance in TransRT. We visualize correct tracking predictions in green boxes and failure cases in red boxes. We observe that most failure cases could be attributed to the perceptual difficulty of transparent objects or occlusion. **Bottom:** Protocol-level action recognition results of _ActionClip+SAM_ with protocol guidance in MultiAR. We visualize correct predictions in green and wrong ones in red. Of note, most incorrectly identified actions have similar HOIs.

during training. Considering that the text encoders utilized in these models are typically trained on text from general domains, we substitute them with SentenceBERT, which has been specifically fine-tuned in the protocol-only setting. In light of experimental findings in Sec. 4.1, we also explore the use of strong object segmentation models (_e.g._, SAM) for the multimodal understanding problem in MultiAR by adding an additional object branch into current models. We provide more model design and implementation details in Appx. B.2.

Evaluation metricsIn all experiments, we report the recognition performance with the top-1, top-5, and mean accuracy. Additionally, we conduct human evaluations and provide the average performance of 10 experienced biology researchers with and without protocols. We report the difference (\(\)) between the performance of each method and the human oracle to visualize the gap on all ambiguity levels in MultiAR.

Results and analysisWe present model performance results of the MultiAR task in Tab. 4 and provide qualitative results in Fig. 4. In summary, we identify the following major findings:

* **Actions are visually ambiguous in MultiAR.** As shown in Tab. 4, both human and vision-only models suffer from perceptual-level ambiguity in actions. Without protocol information, we observe a 40% human performance drop in recognizing the correct protocol being executed. This indicates that humans largely depend on protocols to distinguish visually similar actions. This is also reflected by the low performance of SOTA video recognition models on the hard split with pure vision input.
* **Recognition in MultiAR demands a detailed understanding of protocols.** In contrast to other multimodal video understanding benchmarks, leveraging pre-trained language models is insufficient for MultiAR due to the specialized domain. As shown in the protocol-only setting of Tab. 4, fine-tuning a pre-trained BERT model results in low overall performance. Meanwhile, we observe a significant improvement in SBERT with better modeling of protocol contexts. This suggests potential improvements from more powerful language models that can capture fine-grained experimental contexts illustrated within protocols.
* **Contextual information is crucial for multimodal understanding in MultiAR.** As shown in Tab. 4, the stark contrast between protocol-guided models and pure vision-based models verifies the importance of contextual information in video action recognition, regardless of the protocol granularity. Although we only provide protocols during training, this suggests that it is critical for models to align visual perceptions with fine-grained protocols. Meanwhile, improving the granularity of protocol guidance generally improves model performance, especially on the hard split of MultiAR. This reveals the potential of more fine-grained multimodal interaction designs in models for improving protocol-level action recognition in MultiAR.
* **Solving protocol-level ambiguities is a bottleneck for multimodal video understanding in MultiAR.** Across the three ambiguity levels, we observe a significantly lower performance of most models in the MultiAR hard split. Intuitively, with perceptual-level ambiguity in recognizing actions, identifying protocols depends on matching typical and recognizable actions between visual

    &  &  &  &  \\   & & **top1** & **top5** & **avg.** & \(\) & **top1** & **top5** & **avg.** & \(\) & **top1** & **top5** & **avg.** & \(\) \\   & w protocol & 98.99 & 98.99 & \(\) & 0 & 94.34 & \(-\) & 0 & 93.94 & \(-\) & 0 \\  & w/o protocol & 64.64 & \(-\) & \(-\) & \(-\) & 34.35 & 56.60 & \(-\) & 37.74 & 51.51 & \(-\) & \(-\) & 42.43 \\   & BERT 2018 & 56.19 & 67.00 & 59.14 & -42.8 & 46.11 & 71.33 & 54.72 & -48.23 & 33.72 & 47.55 & 37.23 & -60.22 \\  & SBERT 2019 & 82.39 & 98.14 & 89.97 & -16.6 & 69.28 & 98.07 & 86.28 & -25.06 & 65.03 & 87.7 & 77.21 & -28.91 \\   & I3D 2017 & 41.16 & 78.78 & 67.23 & -57.83 & 29.76 & 63.93 & 58.88 & -64.58 & 11.16 & 33.79 & 23.79 & -82.78 \\  & SlowFast 2019 & 50.03 & 89.79 & 70.40 & -48.96 & 21.6 & 67.64 & 59.72 & -52.18 & 15.00 & 42.22 & 33.08 & -78.94 \\  & MVIT 2021b & 47.72 & 89.02 & 69.92 & -51.27 & 39.92 & 64.84 & 54.29 & -54.42 & 13.34 & 38.01 & 28.14 & -80.66inputs and protocols. Protocol-level ambiguities aggravate this condition by adding variety in the protocols that could be matched. Nonetheless, as humans can accurately identify the protocol being executed in videos when provided with protocols, we argue that it is essential to mitigate this performance gap with better multimodal video understanding and reasoning designs.

## 5 Conclusion

What is \(\)ProBio?We curate \(\)ProBio, the first protocol-guided dataset, which includes comprehensive hierarchical annotations in BioLab. Our dataset aims to promote protocol standardization and the development of intelligent monitoring systems to address the reproducibility crisis in biology science. We've collected 180.6h of videos within an internationally recognized molecular biology laboratory and meticulously annotated all the instruments and solutions as 213,361 segmentation maps. Additionally, we provided annotations for the conditions and enclosures of 48 transparent objects and the state of 12 solutions in 1.05h nearby-view videos. We further restructure all 9.64h videos from a top-down view into three hierarchical levels: _i.e._, 13 brf_exp, 3,724 prc_exp, and 37,537 hoi. This arrangement allows for fine-grained multimodal data and categorical information insight for future research on intelligent monitoring systems.

What can we do with \(\)ProBio?Based on the fine-grained multimodal dataset, we devise two benchmarking tasks associated with \(\)ProBio, aimed at enhancing multimodal video understanding. These two tasks are referred to as transparent solution status tracking (TransST) and multimodal action recognition (MultiAR). In TransST, we provide the transparent object's bounding box and the _<liquid category, object id>_ paired labels. We further discuss the difference between pure-vision tracking and protocol-guided tracking to explore the contribution of procedure information in fundamental tasks. In MultiAR, perceptually similar motions may have divergent semantic interpretations, and the same sub-experiment protocols across different experiments may refer to different meanings. We quantify the definition of ambiguity and establish multimodal and protocol-guided action recognition to enhance the importance of fine-grained contextual information. Our experiments consistently highlight the value of such contextual information in both TransST and MultiAR tasks. Furthermore, this dataset paves the way for other tasks like video segmentation, task prediction, and procedural reasoning.

What will \(\)ProBio contribute?\(\)ProBio is the pioneering multimodal dataset captured within a molecular biology lab, aiming to enhance video comprehension by providing contextual information for visual data. The newly introduced protocol not only enhances the model's ability to process sequences over time but also incorporates the idea of "procedure." This helps our model to effectively handle action interpretations that may be ambiguous. For building a proficient monitoring system, a model skilled in understanding multimodal videos is crucial. Especially in a sterile biology lab, such a system becomes a superior tool to ensure that researchers follow the prescribed standards and procedures. The failure to reproduce experiments is frequently attributed to unintentional mistakes committed by experimenters. Within the 13 experimental categories in \(\)ProBio, a notable quantity of operational actions are not executed as intended, and errors have been identified. The implementation of a monitoring system that possesses advanced video comprehension skills holds the promise of rapidly notifying experimenters of their anomalies, thereby enhancing the overall reproducibility of experiments. Consequently, this serves as a deterrent against the squandering of several months' worth of work and significant financial resources amounting to tens of thousands of dollars.

Limitation and future workCurrently, we considered two distinct tasks associated with \(\)ProBio. However, these two tasks do not adequately reflect the myriad challenges present in today's biological laboratory environments, nor do they showcase the broad applicability and potential of our dataset. In an upcoming version of \(\)ProBio, we plan to add new tasks like video-text retrieval, video segmentation, task prediction, and procedural reasoning, along with more comprehensive annotations. Regarding the model structure, we have not closely aligned the procedure information with the video feature, indicating the potential for further improvement in action recognition accuracy. Our efforts will concentrate on models, aiming to extend their capabilities to recognize and interpret actions with higher levels of detail and complexity.

AcknowledgementThe authors would like to thank Xuan Zhang (Helixon Inc.) and Ningwan Sun (Helixon Inc.) for professional data annotation, Ms. Zhen Chen (BIGAI) for designing the figures, Tao Pu (SYSU) for assisting the experiments, and NVIDIA for their generous support of GPUs and hardware. This work is supported in part by the National Key R&D Program of China (2022ZD0114900), an NSFC fund (62376009), the Beijing Nova Program, and the National Comprehensive Experimental Base for Governance of Intelligent Society, Wuhan East Lake High-Tech Development Zone.