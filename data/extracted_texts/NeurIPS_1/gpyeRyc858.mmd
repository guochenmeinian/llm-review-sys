# Neural Relation Graph: A Unified Framework for Identifying Label Noise and Outlier Data

Jang-Hyun Kim

Seoul National University

janghyun@mllab.snu.ac.kr &Sangdoo Yun

NAVER AI Lab

sangdoo.yun@navercorp.com &Hyun Oh Song

Seoul National University

hyunoh@mllab.snu.ac.kr

Corresponding author

###### Abstract

Diagnosing and cleaning data is a crucial step for building robust machine learning systems. However, identifying problems within large-scale datasets with real-world distributions is challenging due to the presence of complex issues such as label errors, under-representation, and outliers. In this paper, we propose a unified approach for identifying the problematic data by utilizing a largely ignored source of information: a relational structure of data in the feature-embedded space. To this end, we present scalable and effective algorithms for detecting label errors and outlier data based on the relational graph structure of data. We further introduce a visualization tool that provides contextual information of a data point in the feature-embedded space, serving as an effective tool for interactively diagnosing data. We evaluate the label error and outlier/out-of-distribution (OOD) detection performances of our approach on the large-scale image, speech, and language domain tasks, including ImageNet, ESC-50, and SST2. Our approach achieves state-of-the-art detection performance on all tasks considered and demonstrates its effectiveness in debugging large-scale real-world datasets across various domains. We release codes at [https://github.com/snu-mllab/Neural-Relation-Graph](https://github.com/snu-mllab/Neural-Relation-Graph).

## 1 Introduction

Identifying problems within datasets is crucial for improving the robustness of machine learning systems and analyzing the model failures . For instance, identifying mislabeled or uninformative data helps construct concise and effective training datasets , while identifying whether test data is OOD or corrupted allows for more accurate model evaluation and analysis .

In recent years, efforts have been made to identify problematic data by utilizing unary scores on individual data from trained models, such as estimating data influence , monitoring prediction variability throughout training , and calculating prediction error margins . However, identifying such data can be challenging, particularly when dealing with large-scale datasets from real-world distributions. In real-world settings, datasets may have complex problems, including label errors, under-representation, and outliers, each of which can lead to the model error and prediction sensitivity . For example, Figure 1 shows that a neural network exhibits low negative prediction margins and high loss values for both a sample with label error and outlier data. This observation indicates that previous unary scoring methods may have limitations in discerning whether the problem lies with the label or the data itself.

In this work, we propose a unified framework for identifying label errors and outliers by leveraging the feature-embedded structure of a dataset that provides richer information than individual data alone . We measure the relationship among data in the feature embedding space while comparing the assigned labels independently. By comparing input data and labels separately, we are able toisolate the factors contributing to model errors, resulting in improved identification of label errors and outlier data, respectively. Based on the relational information, we construct a novel graph structure on the dataset and identify whether the data itself or the label is problematic (Figure 2). To this end, we develop scalable graph algorithms that accurately identify label errors and outlier data points.

In Section 3.5, we further introduce a visualization tool named data relation map that captures the relational structure of a data point. Through the map, we can understand the underlying relational structure and interactively diagnose data. In Figure 1, we observe different patterns in the relation maps of the second and third samples, despite their similar margin and loss scores. This highlights that the relational structure provides complementary information not captured by the unary scoring methods.

Our approach only requires the model's feature embedding and prediction score on data, making it more scalable compared to methods that require calculating the network gradient on each data point or retraining models multiple times to estimate data influence [39; 17]. Furthermore, our method is domain- and model-agnostic, and thus is applicable to various tasks. We evaluate our approach on label error and outlier/OOD detection tasks with large-scale image, speech, and language datasets: ImageNet , ESC-50 , and SST2 . Our experiments show state-of-the-art performance on all tasks, demonstrating its effectiveness for debugging and cleaning datasets over various domains.

## 2 Related works

Label error detectionLabel errors in datasets can negatively impact model generalization and destabilize evaluation systems [16; 34]. Prior works address this issue through label error detection using bagging and bootstrapping [46; 41], or employing neural networks [18; 27; 9; 19]. To mitigate overfitting on label errors, Pleiss et al.  propose tracking the training process to measure the area under the margin curve. Recent studies demonstrate that simple scoring methods with large pre-trained models, such as prediction margins or loss values, achieve comparable results to previous complex approaches [33; 5]. Meanwhile, Wu et al.  propose a unified approach for learning with open-world noisy data. However, the method involves a complicated optimization process during training, which is not suitable for large-scale settings. Another line of approach to identifying label errors involves measuring the influence of a training data point on its own loss [22; 39]. However, these approaches require calculating computationally expensive network gradients on each data point, and their performance is known to be sensitive to outliers and training schemes [2; 3]. In this work, we present a scalable approach that leverages the data relational structure of trained models without additional training procedures, facilitating practical analysis of label issues.

Figure 1: ImageNet samples with their labels and the corresponding relation maps by an MAE-Large model . We report the prediction margin score (\([-1,1]\)) and the loss value next to the label. The relation map draws a scatter plot of the mean and variance of relation values of a data pair throughout the training process. Here the color represents the relation value at the last converged checkpoint. We present the detailed procedure for generating the relation maps in Section 3.5.

Figure 2: The conceptual illustration of the conventional approaches (left) and our proposed approach (right). In the relation graph, positive edges signify complementary relations, negative edges denote conflicting relations, and dashed lines indicate negligible relations between data.

Outlier/OOD detectionDetecting outlier data is crucial for building robust machine learning systems in real-world environments . A recent survey paper defines the problem of finding outliers in training set as _outlier detection_ and finding outliers in the inference process as _OOD detection_. The conventional approach for detecting outliers involves measuring k-nearest distance using efficient sampling methods . More recently, attempts have been made to detect outlier data using scores obtained from trained neural networks, such as Maximum Softmax Probability , Energy score , and Max Logit score . Other approaches suggest adding perturbations on the inputs or rectifying the activation values to identify the outlier data [28; 49]. Lee et al.  propose fitting a Gaussian probabilistic model to estimate the data distribution. Recently, Sun et al.  propose a non-parametric approach measuring the \(k\)-nearest feature distance. In our work, we explore the use of the relational structure on the feature-embedded space for identifying outlier data. Our approach is applicable to a wide range of domains without requiring additional training while outperforming existing scoring methods on large-scale outlier/OOD detection benchmarks.

## 3 Methods

In this section, we describe our method for identifying label errors and outliers using a model trained on the noisy training dataset. We exploit the feature-embedded structure of the learned neural networks, which are known to effectively capture the underlying semantics of the data . We define data relation to construct a data relation graph on the feature space, and introduce our novel graph algorithms for identifying label errors and outlier data. In Section 3.5, we introduce the data relation map as an effective visualization tool for diagnosing and contextualizing data.

### Data relation

We describe our approach in the context of a classification task, while also noting that the ideas are generalizable to other types of tasks as well. We assume the presence of a trained neural network on a noisy training dataset with label errors and outliers, \(=\{(x_{i},y_{i}) i=1,,n\}\). By utilizing data features extracted from the network, we measure the semantic similarity between data points with a bounded kernel \(k:[0,M]\), where a higher kernel value indicates greater similarity between data points. Our framework can accommodate various bounded kernels such as RBF kernel or cosine similarity . We provide detailed information on the kernel function used in our main experiments in Section 3.4.

By incorporating the assigned label information with the similarity kernel \(k\), we define the relation function \(r:[-M,M]\):

\[r((x_{i},y_{i}),(x_{j},y_{j}))=1(y_{i}=y_{j}) k(x_{i},x_{j}), \]

where \(1(y_{i}=y_{j})\{-1,1\}\) is a signed indicator value. The relation function reflects the degree to which data samples are complementary or conflicting with each other. In Figure 3, the center image with a label error has negative relations to the left samples that belong to the same ground-truth class. In contrast, the two left samples with correct labels have a positive relation. We also note that samples with dissimilar semantics exhibit near-zero relations.

Our relation function \(r\) relies solely on the parallelizable forward computation of neural networks, ensuring scalability in large-scale settings.

### Label error detection

We consider a fully-connected undirected graph \(=(,,)\), where the set of nodes \(\) corresponds to \(\) and the weights \(\) on edges \(\) are the negative relation values defined in Equation (1). For notation clarity, we denote a data point by an index, _i.e._, \(=\{1,,n\}\). Then, for nodes \(i\) and \(j\), the edge weight is \(w(i,j)=-r(i,j)=-r((x_{i},y_{i}),(x_{j},y_{j}))\). We set \(w(i,i)\) to 0, which does not

Figure 3: Relation values of samples from ImageNet with MAE-Large . We denote the assigned label above each sample. Here, the center image has a label error.

correspond to any edges in the graph. Consistent with previous works , we aim to measure the _label noisiness score_ for each data, where a higher score indicates a higher likelihood of label error. We denote the label noisiness scores for \(\) as \(s^{n}\), where \(s[i]\) is the score for data \(i\).

As depicted in Figure 3, data with label errors exhibit negative relations with other samples, implying that the data have similar features in the embedding space yet have dissimilarly assigned labels. This suggests that the edge weights \(w(i,)\) quantify the extent to which the label assigned to node \(i\) conflicts with the labels of other nodes. However, simply aggregating all edge weights of a node can yield suboptimal results, as negative relations can also contribute to the score for clean data, as shown in Figure 3. In Appendix D.2, we provide a more detailed experimental analysis of this issue.

To rectify this issue, we develop an algorithm that considers the global structure of the graph instead of simply summing the edge weights of individual nodes. Specifically, we identify subsets of data likely to have correct/incorrect labels and calculate the label noisiness score based on the subsets. We partition the nodes in \(\) into two groups, where \(\) denotes the _estimated noisy subset_ and \(\) denotes the clean subset. To optimize \(\), we aim to maximize the sum of the edges between the two groups, indicating that the label information of the two groups is the most conflicting. To ensure that \(\) contains data with incorrect labels, which constitute a relatively small proportion of \(\), we impose regularization to the cardinality of \(\) with \( 0\) and formulate the following max-cut problem:

\[^{*}=*{argmax}_{}(,)(\,_{i }_{j}w(i,j))-| |. \]

The max-cut problem is NP-complete . To solve this problem, we adopt the Kerningham-Lin algorithm, which finds a local optimum by iteratively updating the solution . However, the original algorithm that swaps data one by one at each optimization iteration is not suitable for large-scale settings. To this end, we propose an efficient _set-level_ algorithm in Algorithm 1 that alternatively updates the noisy set \(\) and label noisiness score vector \(s\).

```
Input: Relation function \(r\) (\(=-w\)) Notation: The number of data \(n\) for\(i=1\)to\(n\)do \([i]=_{j=1}^{n}w(i,j)\)\(\#\) caching initial score endfor \(s=\) repeat \(=\{i s[i]>,i[1,,n]\}\) for\(i=1\)to\(n\)do \(s[i][i]-2_{j}w(i,j)\) endfor untilconvergence Output:\(s,\)
```

**Algorithm 1** Label noise identification

Specifically, given the current estimation of \(\), the cut value excluding edges of node \(i\) is \((\{i\}, \{i\})\). Algorithm 1 measures the label noisiness score of node \(i\) by comparing the objective cut values when including \(i\) in \(\) and when including \(i\) in \(\):

\[s[i]=(\{i\}, \{i\})-(\{i\}, \{i\})=_{j}w(i,j)-_{j }w(i,j).\]

Here we use the assumption \(w(i,i)=0\). In practice, the cardinality of \(\) is small, so we can efficiently update the score vector \(s\) by caching the initial score vector \(\) as in Algorithm 1. After calculating the score vector \(s\), we update the noisy set \(\) by selecting nodes with score values above the value \(\). Figure 4 illustrates the optimization process. Here larger values of \(\) result in smaller \(\) consisting of data samples that are more likely to have label noise. We provide the sensitivity analysis of \(\) in Appendix C.1, with Table 7.

Algorithm 1 satisfies the convergence property in Proposition 1. In Appendix A, we provide proof and present an empirical convergence analysis on large-scale datasets. We also conduct a runtime

Figure 4: Illustration of our scoring algorithms for identifying label noise (left) and outliers (right).

analysis of our algorithm in Appendix A.3, demonstrating that the computation overhead of Algorithm 1 is negligible in large-scale settings.

**Proposition 1**.: _Algorithm 1 with a single node update at each iteration converges to local optimum._

Complexity analysisThe time complexity of Algorithm 1 is \(O(n^{2})\), proportional to the number of edges in a graph. It is noteworthy that our method maintains the best performance when used with graphs consisting of a small number of nodes, as shown in Figure 5. This implies that we can partition large datasets and run the algorithm repeatedly for each partition to enhance efficiency while maintaining performance. In this case, the complexity becomes \(O(n/k k^{2})=O(nk)\), with \(k\) representing the size of each partition and \(n/k\) being the number of partitions. Also, computations on these partitions are embarrassingly parallelizable, meaning that the complexity becomes \(O(k^{2})\) for \(k n\) in distributed computing environments.

### Outlier/OOD detection

In the previous section, we presented a method for detecting label errors based on data relations with similar feature embeddings but different label information. By employing the identical feature embedding structure, we identify outlier data by measuring the extent to which similar data are absent in the feature embedding space. To quantify the extent of a data point being an outlier, we aggregate the similarity kernel values of a data point in Equation (3), thereby processing the entire relational information of the data point. Our approach leverages global information about the data distribution, resulting in a more robust performance across a range of experimental settings compared to existing methods that rely on local information such as \(k\)-nearest distance . Specifically, for a subset \(\) and data \(x\), we measure the _outlier score_ as

\[(x)=}k(x,x_{i})}.\]

Higher values in the outlier score indicate that the data are more distributionally outliers. We propose to use a _uniform random_ sampling for \(\), adjusting the computational cost and memory requirements for the outlier score calculation to suit the inference environment. In Section 4.2, we verify our method maintains the best OOD detection performance even when using only 0.4% of the data in ImageNet.

### Proposed similarity kernel

For \(x_{i}\), we extract the feature representation \(_{i}\) and the prediction probability vector \(_{i}\) from the trained model. We propose a class of bounded kernel \(k:[0,M]\) with the following form:

\[k(x_{i},x_{j})=|s(_{i},_{j}) c(_{i}, _{j})|^{t}. \]

A positive scalar value \(t\) controls the sharpness of the kernel value distribution. A larger value of \(t\) makes a small kernel value smaller, which is effective in handling small noisy kernel values. A scalar value \(s(_{i},_{j})^{+}\) denotes a similarity measurement between features. In our main experiments, we adopt the truncated cosine-similarity that has been widely used in representation learning [44; 42]. We use the hinge function at zero, resulting in the following positive feature-similarity function:

\[s(_{i},_{j})=(0,(_{i},_ {j})).\]

It is worth noting the utility of our framework is not limited to a specific kernel design. In Section 4.3, we verify our approach maintains the best performance with \(s(_{i},_{j})\) defined as the RBF kernel .

While the feature similarity captures the meaningful semantic relationship between data points, we observe that considering the prediction scores \(_{i}\) can further improve the identification of problematic data. To incorporate prediction scores into our approach, we introduce a scalar term \(c(_{i},_{j})\) that measures the compatibility between the predictions on data points. Any positive and bounded compatibility function is suitable for the kernel class defined in Equation (3). In our main experiments, we use the predicted probability of belonging to the same class as the compatibility term \(c(_{i},_{j})\). Specifically, given the predicted label random variables \(_{i}\) and \(_{j}\), the proposed compatibility term is

\[c(_{i},_{j})=P(_{i}=_{j})=_{i}^{ }_{j}. \]

From a different perspective, we interpret this term as a measure of confidence for feature similarity. In Section 4.3, we verify the effectiveness of the compatibility term through an ablation study.

InterpretationTo better understand our relation function with kernel defined in Equation (3), we draw a connection to the influence function , which estimates the influence between data points by computing the inner product of the network gradient on the loss function \(\) of each data point as \(_{w}(x_{i})^{}_{w}(x_{j})\), where \(w\) denotes the network weights. Following the convention, we consider an influence function on the feed-forward layer, where \((x_{i})=h(_{i}^{})=h(w^{}_{i})\). By the chain rule, we can decompose the network gradient as \(_{w}(x_{i})=_{^{}}h(_{i}^{} )_{i}^{}\), and represent the influence as \(_{^{}}h(_{i}^{})^{}_{ ^{}}h(_{i}^{})_{i}^{ }_{j}\). Our relation function differs from the influence function in that it does not rely on feature gradients \(_{^{}}h(_{i}^{})\) to evaluate the relationship between data points. Instead, our relation function compares model predictions and assigned labels independently using the terms \(c(_{i},_{j})\) and \(1(y_{i},y_{j})\). Our formulation does not require computationally expensive back-propagation and more robustly identifies conflicting data information than influence functions which are known to be sensitive to outliers . We provide a more detailed theoretical analysis in Appendix A.4.

### Data relation map

In this section, we present a visualization method based on our data relation function to contextualize data and comprehend its relational structure. One of the effective approaches for visualizing a dataset is dataset cartography , which projects the dataset onto a 2D plot. This approach draws a scatter plot of the mean and standard deviation of the model's prediction probabilities for each data sample during training. Inspired by the dataset cartography, we propose a _data relation map_, which visualizes the relationship between data along the training process. To this end, we uniformly store checkpoints during training. We denote a set of these checkpoints as \(\), where \(r_{k}\) refers to the relation function for checkpoint \(k\). For each data sample \(i\), we draw a scatter plot of the mean and standard deviation of relation values \(\{r_{k}(i,j) k\}\) for \(j\{i\}\).

In Figure 1, we provide relation maps of three samples from ImageNet, using 10 checkpoints of MAE-Large . The three samples each represent clean data, data with a label error, and outlier data. From the figure, samples show different relation map patterns. Specifically, the relation map of a clean data sample exhibits a majority of positive relations with relatively small variability. We note that there are gray-colored relations in high variability regions (0.2\(<\)std), indicating that the model resolves conflicting relations at convergence. On the other hand, the relation map of the sample with a label error demonstrates a majority of negative relations. Notably, high variance relations result in largely negative relations at convergence, suggesting that conflicts intensify. Lastly, the relation map of the outlier data sample reveals that relations are close to 0 during training. These relation maps can serve as a model-based fingerprint of the data, which our algorithm effectively exploits to identify problematic data. We provide additional data relational maps for various models in Appendix E.1.

## 4 Experimental results

In this section, we experimentally verify the effectiveness of our approach in detecting label errors and outliers. We provide implementation details, including hyperparameter settings in Appendix C.1. We provide qualitative results including detected label errors and outlier samples in Appendix E.2.

### Label error detection

#### 4.1.1 Setting

DatasetsWe conduct label error detection experiments on large-scale datasets: ImageNet , ESC-50 , and SST2 . ImageNet consists of about 1.2M image data from 1,000 classes. ESC-50 consists of 2,000 5-second environmental audio recordings organized into 50 classes. SST2 is a binary text sentiment classification dataset, consisting of 67k movie review sentences. We also conduct experiments on MNLI  and provide results in Appendix, Table 13.

Following Pruthi et al. , we construct a noisy training set by flipping labels of certain percentages of correctly classified training data with the top-2 prediction of the trained model. We use different neural network architectures for constructing a noisy training set and detecting label errors to avoid possible correlation. We leave a more detailed procedure for constructing the noisy training set in Appendix C.2. When training the MAE-Large model  on ImageNet with 8% label noise, the validation top1-accuracy decreases by 1.7% compared to the performance of the model trained on the original training set, as reported in Appendix Table 8. The decrease becomes more significant with 15% label noise, with a drop of 4.4%, highlighting the importance of label noise detection and cleaning.

BaselinesWe compare our method (_Relation_) to six baselines that are suitable for large-scale datasets. We consider fine-tuned loss from pre-trained models (_Loss_) , prediction probability margin score (_Margin_) , and the influence-based approach called _TracIn_. We also evaluate model-agnostic scoring methods: _Entropy_, _Least-confidence_, and Confidence-weighted Entropy (_CWE_) . For a fair comparison, we evaluate methods using a single converged neural network in our main experiments, while also providing results with a temporal ensemble suggested by  in Appendix D.1.

MetricWe evaluate the detection performance based on label noisiness scores by each method. We note that detecting label errors is an imbalanced detection problem, which makes the AUROC metric prone to being optimistic and misleading . In this respect, we mainly report the AP (average precision) and TNR95 (TNR at 0.95 TPR), and provide AUROC results in Appendix D.3.

#### 4.1.2 Results and analysis

ImageNetWe measure the label error detection performance on ImageNet with the synthetic label noise by training an MAE-Large model . Note that the model does not have access to information about the changed clean labels during the entire training process. Figure 5 (a) shows the detection performance over a wide range of label noise ratios from 4% to 15%. As shown in the figure, our approach achieves the best AP and TNR95 performance compared to the baselines. Especially, our method maintains a high TNR95 over a wide range of noise ratios, indicating that the number of data that need to be reviewed by human annotators is significantly smaller when cleaning the dataset. In Figure 9, we present detected label error samples by our algorithm.

It is worth noting that our method relies on the number of data for constructing a relation graph. To measure the sensitivity of our algorithm to the number of data, we evaluate the detection performance using a reduced number of data with uniform random sampling. Figure 5 (b) shows the detection performance on 8% label noise with MAE-Large. From the figure, we find that our algorithm maintains the best detection performance even with 1% of the data (12k). This demonstrates that our algorithm is effective even when only a small portion of the training data is available, such as continual learning or federated learning . In Table 1 (a), we provide detection performance for different scales of MAE models on 8% label noise. The table shows our approach achieves the best AP with MAE-Base, verifying the robustness of our approach to the network scales. From the table, we note that larger models are more robust to label noise and show better detection performance.

Speech and language domainsWe apply our method to speech and language domain datasets: ESC-50  and SST2 . We design the label noise detection settings identical to the previous ImageNet section. Specifically, we train the AST model  for ESC-50 and the RoBERTa-Base model  for SST2 under the 10% label noise setting. Table 1 (b) shows our approach achieves the best AP and TNR95 on the speech and language datasets, demonstrating the generality of our approach across various data types.

Figure 5: Label error detection performance on ImageNet with MAE-Large according to (a) label noise ratios and (b) the number of data. We obtain the results in (b) with 8% label noise. We report performance values of all methods in Appendix D.3, with Tables 10 and 11.

Realistic label noiseThe ImageNet validation set is known to contain numerous label errors . To tackle this issue, Beyer et al.  cleaned the labels with human experts and corrected around 29% of the labels via multi-labeling. With this re-labeled validation set, we conduct experiments under the realistic label noise, with the task of detecting the data samples with changed labels. We measure the detection performance with MAE-Large , BEIT-Large , and ConvNeXt-Large  models. To examine the impact of pre-training on external data, we also include ConvNeXt pre-trained on ImageNet-22k, denoted as ConvNeXt-22k. We construct the relation graph using only the validation set, considering scenarios where the training data are not available. Table 1 (c) verifies that our approach outperforms the best baseline across various models. The results on ConvNeXt-22k indicate that pre-training on external data improves the detection performance.

Memorization issueWe investigate the impact of large neural networks' ability to memorize label errors on detection performance . In the left figure of Figure 6, we find that the AP score decreases as the training progresses after 30 epochs with MAE-Large which converges at 50 epochs. The right figure of Figure 6 plots the precision-recall curves, where we observe that precision increases at low recall area but decreases at mid-level recall (\( 0.5\)) as the training progresses. This suggests that training has both positive and negative effects on detecting label noise, and we speculate that memorization is one cause. Leveraging these observations, we improve detection AP by 3.6%p by using a temporal ensemble of models . We provide a more detailed description and results in Appendix D.1.

### Outlier/OOD detection

BaselinesWe consider the following representative outlier scoring approaches: Maximum Softmax Probability (_MSP_) , _Max Logit_, _Mahalanobis_, _Energy_ score , _ReAct_, _KL-Matching_, and _KNN_. We tune the KNN method's hyperparameter \(k\) based on the paper's guidance as \(k=1000\), where \(\) represents the ratio of training data used for OOD detection. We also evaluate outlier detection approaches, _Iterative sampling_ and _Local outlier factor_.

OOD detectionFollowing Sun et al. , we evaluate OOD detection performance on the ImageNet validation set consisting of 50k in-distribution data samples, along with four distinct OOD datasets: _Places_, _SUN_, _iNaturalist_, and _Textures_. Each of these OOD datasets consists of 10k data samples except for Textures which has 5,640 data samples. We also combine these four datasets, denoted as _ALL_, and measure the overall OOD detection performance on this dataset.

Figure 7 shows OOD detection performance of MAE-Large on ALL outlier dataset. Note that our approach and KNN both rely on the number of training data samples (\(||\)) for outlier score calculation. We examine the effect of training set size by measuring the performance with a reduced number of data using uniform random sampling. Figure 7 verifies that our approach outperforms other baselines while maintaining performance even with 0.4% of the training dataset (5k). Note that KNN requires hyperparameter tuning according to the training set size, whereas our approach uses the identical

Table 1: Label error detection performance on ImageNet with 8% label noise. _Baseline_ refers to the _best_ performance among the six baselines considered in Figure 5. In Table (c), the evaluation metric is AP. We report the performance of all baselines in Appendix D.3, with Tables 12 to 14.

Figure 6: Label error detection performance of relation graph throughout the MAE-Large training process on ImageNet with 8% label noise.

hyperparameter (\(t=1\)) regardless of the size. In Appendix D.4, Tables 15 and 16, we provide OOD detection results on four individual OOD datasets as well as the performance with ResNet-50 , where our approach achieves the best OOD detection performance over the nine baselines considered.

Outlier detectionWe perform outlier detection experiments following the methodology by Wang et al. , where the training set contains outlier data with random labels. We construct the noisy ImageNet-100 training sets by using iNaturalist  and SUN  datasets. We train a ViT-Base model  from scratch on these noisy training datasets, and measure outlier detection performance using the trained model. For a more detailed description, please refer to Appendix C.3.

Table 2 shows the outlier detection results on two outlier datasets. As indicated, our method achieves the best performance in both outlier settings, demonstrating its effectiveness in outlier detection. It is worth noting that the considered OOD scoring methods (MSP, Max Logit, Energy) do not achieve good outlier detection performance. We speculate that this is due to the overfitting of the neural network's predictions on outliers.

Detecting outliers in validation setWe further utilize our method for identifying outliers in the validation set by retrieving data samples with the highest outlier score (Section 3.3). In Figure 10, we present samples detected by our algorithm from ImageNet and SST2. In the figure, we observe that these samples are not suitable for measuring the predictive performance on labels, which should be excluded from the evaluation dataset.

### Ablation study

Temperature \(t\)In Equation (3), we introduced a temperature \(t\), where a large value of \(t\) increases the influence of large relation values in our algorithm. We conduct sensitivity analysis on \(t\) with MAE-Large on ImageNet under 8% label noise. Figure 8 shows the effect of the temperature value

Table 2: Outlier detection performance with Vit-Base on noisy ImageNet-100. Some OOD scoring methods (Mahalanobis, ReAct, KL-Matching) are excluded from the comparison because they require a clean training dataset which is not available in the outlier detection setup.

Figure 7: OOD detection performance on ImageNet (ALL) with MAE-Large. _Unary-best_ refers to the best performance among the methods that do not rely on the training data for outlier score calculation. We provide performance values of all baselines in Appendix D.4, with Table 15.

Figure 8: The detection AP of the MAE-Large model across a range of kernel temperatures \(t\). The dashed blue line means the performance of the best baseline.

on our detection algorithm's performance. From the figure, we observe that the label error detection performance increases as the \(t\) value increases, saturating at \(t=6\). In the case of OOD detection, we achieve the best performance at around \(t=1\). Our algorithm outperforms the best baseline over a wide range of hyperparameters, demonstrating the robustness of our algorithm to the hyperparameter.

Similarity kernel designWe present an empirical analysis of the kernel design choices. Specifically, we replace the cosine similarity term in Equation (3) as the RBF kernel and evaluate the detection performance. We further conduct an ablation study on compatibility terms (Equation (4)). Table 3 summarizes the label error detection performance with different kernel functions on ImageNet with 8% noise ratio. The table shows that our approach largely outperforms the best baseline even with the RBF kernel. Also, we find that our approach without the compatibility term shows comparable AP performance while significantly outperforming baselines in TNR95. These results demonstrate the generality and utility of our relational structure-based framework, which is not limited to a specific kernel design.

## 5 Conclusion

In this paper, we propose a novel data relation function and graph algorithms for detecting label errors and outlier data using the relational structure of data in the feature embedding space. Our approach achieves state-of-the-art performance in both label error and outlier/OOD detection tasks, as demonstrated through extensive experiments on large-scale benchmarks. Furthermore, we introduce a data contextualization tool based on our data relation that can aid in data diagnosis. Our algorithms and tools can facilitate the analysis of large-scale datasets, which is crucial for the development of robust machine-learning systems.

Figure 10: Data samples with the highest outlier scores by our method on ImageNet (top) and SST2 (bottom) validation sets. We denote the assigned labels above each data sample. We present more outlier samples in Appendix E.2, Figure 15 and Table 18.

   Metric & Baseline & RBF & Cos & RBF \( c\) & Cos \( c\) \\  AP & 0.484 & 0.470 & 0.471 & 0.525 & 0.526 \\ TNR95 & 0.521 & 0.668 & 0.671 & 0.703 & 0.695 \\   

Table 3: Comparison of similarity kernel designs. _Baseline_ represents the best baseline performance. The term \(c\) denotes our compatibility term in Equation (4). Note, \( c\) is the kernel function considered in our main experiments, and \(\) / \(\) refers to our method without the compatibility term \(c\).

Figure 9: Detected data samples with label errors (marked in red) from ImageNet (top) and SST2 (bottom). We present samples with conflicting relations next to the detected samples and denote the corresponding relation value in parenthesis. We present more samples in Appendix E.2, Figure 14.