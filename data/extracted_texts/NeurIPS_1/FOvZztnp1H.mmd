# AutoTimes: Autoregressive Time Series Forecasters via Large Language Models

Yong Liu, Guo Qin, Xiangdong Huang, Jianmin Wang, Mingsheng Long

School of Software, BNRist, Tsinghua University, China

{liuyong21,qinguo24}@mails.tsinghua.edu.cn,

{huangxdong,jimwang,mingsheng}@tsinghua.edu.cn

Equal Contribution

###### Abstract

Foundation models of time series have not been fully developed due to the limited availability of time series corpora and the underexploration of scalable pre-training. Based on the similar sequential formulation of time series and natural language, increasing research demonstrates the feasibility of leveraging large language models (LLM) for time series. Nevertheless, the inherent autoregressive property and decoder-only architecture of LLMs have not been fully considered, resulting in insufficient utilization of LLM abilities. To fully revitalize the general-purpose token transition and multi-step generation capability of large language models, we propose **AutoTimes** to repurpose LLMs as **Autoregressive Time** series forecasters, which projects time series into the embedding space of language tokens and autoregressively generates future predictions with arbitrary lengths. Compatible with any decoder-only LLMs, the consequent forecaster exhibits the flexibility of the look-back length and scalability with larger LLMs. Further, we formulate time series as prompts, extending the context for prediction beyond the lookback window, termed **in-context forecasting**. By introducing LLM-embedded textual timestamps, AutoTimes can utilize chronological information to align multivariate time series. Empirically, AutoTimes achieves state-of-the-art with \(0.1\%\) trainable parameters and over \(5\) training/inference speedup compared to advanced LLM-based forecasters. Code is available at this repository: [https://github.com/thuml/AutoTimes](https://github.com/thuml/AutoTimes).

## 1 Introduction

Time series forecasting is of crucial demand in real-world applications, covering various domains including climate, economics, energy, operations, etc. . The growing challenges of general-purpose forecasting, where one model is versatile to handle variable-length scenarios  and the prediction is necessarily instructed by auxiliary information in other modalities , underscore the demand for foundation models  of time series, which are aimed to exhibit enhanced capabilities, including multi-step generation, zero-shot generalization , in-context learning and multimodal utilization , thereby expanding the scope of time series forecasting to a wider range of situations.

Nevertheless, the development of time series foundation models has been hampered by the limited availability of large-scale pre-training datasets and the technical uncertainty of scalable backbones. In contrast, rapid progress is witnessed in large language models (LLM), facilitated by extensive text corpora , available pre-trained models , and well-established adaptation techniques . Notably, language and time series share basic commonalities in sequence modeling and generation by learned token transitions, presenting opportunities to adopt off-the-shelf LLMs for time series.

Despite recent studies on large language models for time series (LLM4TS) achieving performance breakthroughs in current forecasting benchmarks , the mechanism by which LLMs are aligned to the time series modality still remains obscure. The pilot work, FPT  leverages LLMs as generic sequential representation extractors for time series, influencing subsequent LLM4TS methodologies. As depicted in Figure 1 (a), the non-autoregressive approach, where time series are segmented into tokens, flattens and projects all lookback tokens for the prediction in a single step. However, it causes inconsistencies in both model structure and generative approach of LLMs: _decoder-only models for autoregressive generation are converted to encoder-only and non-autoregressive forecasters_.

Given that prior studies [9; 38] reveal that generalization performance of LLMs is largely derived from the decoder-only structure trained autoregressively, talents of LLMs may not be fully exhibited. It is also supported by the recent rethinking of previous LLM4TS methods , which generally lack the maintenance of autoregression, the essential characteristic of both large language models and statistical forecasters [5; 39]. Therefore, autoregressive LLM4TS methods are underexplored, which can potentially unlock multi-step generation like LLMs, presenting one model for arbitrary lengths.

Motivated by the reflections, we propose **AutoTimes** to adapt LLMs as time series forecasters, which _retrieves the consistency of autoregression with revitalized LLM capabilities to produce foundation models for time series forecasting_. Technically, we independently embed time series segments into the latent space of language models by the consistent training objective: next token prediction . To fully leverage the inherent token transitions of LLMs and reduce the training cost, we freeze the LLM and establish token embedding and projection for time series, which only account for up to \(0.1\%\) total parameters. The consequent forecaster adopts autoregressive inference like LLMs, which is no longer constrained to specific lookback/forecast lengths. Going beyond conventional time series forecasting, we propose **in-context forecasting** as shown in Figure 1, where time series can be self-prompted by relevant contexts. We further adopt LLM-embedded timestamps as the position embedding to utilize chronological information and align multiple variates. Our contributions are summarized as follows:

* By refining the inconsistency of non-autoregressive LLM4TS methods, we propose to inherit the autoregressive property of LLMs, which frees our method from training respectively on the lookback length and allows arbitrary-length predictions with chronological awareness.
* We present AutoTimes, a simple but effective approach to acquire LLM-based forecasters by lightweight adaptation, which utilizes the inherent token transition as the future extrapolation of time series. Further, we propose in-context forecasting, which renovates the conventional paradigm by introducing relevant time series prompts to enhance forecasting.
* Compared with state-of-the-art methods, our repurposed forecaster achieves superior performance while saving over \(80\%\) training and inference time, and further exhibits zero-shot generalizability, in-context forecasting, and scaling behavior empowered by LLMs.

## 2 Related Work

### Autoregressive Models

Autoregression is an essential concept of both language modeling and time series forecasting. Despite prevalent deep forecasters [10; 26; 42; 48] adopt a non-autoregressive approach without the

Figure 1: (a) Prevalent LLM4TS methods non-autoregressively generate predictions with the globally flattened representation of lookback series, while large language models inherently predict the next tokens by autoregression . (b) Previous methods adopt language prompts that may lead to the modality disparity, while we find time series can be self-prompted, termed _in-context forecasting_.

requirement of iterative forecasting, autoregression, the absent exploration in deep forecasters, serves as the fundamental principle of statistical methods, which enables variable-length predictions. The most well-known model, ARIMA  is developed by incorporating differencing on AR and MA models, which are both autoregressive models with learned time-invariant transition from the past to the future. Incorporated with decomposition and pre-defined transitions, exponential smoothing  and state space models (SSM) [12; 23] also take the same autoregressive formulation.

Autoregressive language models [27; 31] are trained with fine-grained supervision, where the generated token of each position is independently supervised. Consequently, they are not constrained by specific input/output lengths and excel at multi-step generation. Furthermore, existing LLMs are inherently autoregressive models , which demonstrate advanced abilities that are not present in small models, such as the generalization , scalability , and task generality [31; 32]. Therefore, it is imperative to adapt off-the-shelf LLMs as autoregressive forecasters, which keeps the consistency to fully revitalize the model capacity and general-purpose token transitions.

### Large Language Models for Time Series

With the immense advancement of large language model infrastructure, LLM4TS methods have been experiencing significant development in recent years. PromptCast  reformulates time series as text pairs and accomplishes forecasting as a sentence-to-sentence task. LLMTime  regards time series as numerical tokens, demonstrating the zero-shot generalizability in time series forecasting. FPT  fine-tunes parameters of the LLM to adapt it as a general representation extractor serving for multiple time series analysis tasks. UniTime  adapts a language model across diverse time series for a unified forecaster of multiple domains. Based on thriving prompting techniques, deft language prompts [15; 21] and soft prompting  for time series are further investigated.

LLM4TS methods have achieved performance breakthroughs in time series forecasting, but the cost of training and inference can sometimes be resource-consuming due to the imminently of LLMs. Recent revisiting of LLM4TS methods has revealed the inefficacy of LLMs adapted in the non-autoregressive approach . By contrast, AutoTimes frozen LLMs, transfers the general-purpose token transition, and introduces minimal parameters to realize autoregressive next token prediction, thereby achieving better model efficiency and consistent utilization of large models. We further provide Table 1 that categorizes prevalent LLM4TS methods by several essential aspects.

### Multimodal Language Models

Multimodal models have been well-developed upon LLMs, among which vision language models (VLM) have experienced rapid growth [1; 27]. The booming pre-trained vision backbones [11; 30], together with the instruction tuning paradigm, has revealed the potential of LLMs for vision tasks, where visual tokens and language tokens are concatenated as the input of the LLM [19; 20]. Inspired by this, previous LLM4TS methods utilize instructive language tokens as prefix-prompts for time series analysis [15; 34; 44]. Unlike previous works, our proposed method regards time series itself as the instructive prompt. It avoids the modality gap caused by concatenating time series and language tokens directly. We incorporate chronological information, the textual timestamp of time series, such that the language model can effectively perceive date and periodicity as the position embedding, and align simultaneous events from different time series  for multivariate forecasting.

## 3 Method

The proposed AutoTimes adapts large language models for multivariate time series forecasting. Given lookback observations \(_{1:L}=\{_{1},,_{L}\}^{L C}\) with \(L\) time steps and \(C\) variates, the objective

   Method & **AutoTimes** & TimeLLM  & UniTime  & FPT  & LLMTime  & TEST  & TEMPO  & PromptCast  \\  Autoregressive & ✓ & ✗ & ✗ & ✗ & ✓ & ✗ & ✗ & ✗ \\ Freeze LLM & ✓ & ✓ & ✗ & ✓ & ✓ & ✗ & ✓ \\ Multimodal & ✓ & ✓ & ✓ & ✗ & ✓ & ✓ & ✓ \\   

Table 1: Comparison of LLM4TS methods: _Autoregressive_ categories LLM-based forecasters by whether to conduct autoregression. _Freeze LLM_ enables quick adaptation, which would otherwise require significant resources for fine-tuning. _Multimodal_ refers to the utilization of information from other modalities. Prior to AutoTimes, none of the LLM4TS methods achieved all three.

is to predict the future \(F\) time steps \(_{L+1:L+F}=\{_{L+1},,_{L+F}\}^{F  C}\). Besides, the textual timestamp \(_{t}\) (e.g. \(2016/07/05\) 00:00:00), as the most common covariate, is adopted for prediction, which is aligned with time points \(_{t}^{C}\) at time \(t\). The task is to train an LLM-based forecaster \(f\) that is able to predict with the (varying) lookback length \(L\) for the (arbitrary) forecast length \(F\) as:

\[f:(_{1:L},_{1:L+F})}_{L+1:L+F}. \]

### Modality Alignment

Time series tokenTo empower the forecaster with the capability to predict time series for arbitrary lengths, we repurpose autoregressive LLMs as time series forecasters as depicted in Figure 2. Prior to this, we define time series token as the consecutive and non-overlapping segment of a single variate. It is regarded as the common token of the LLM-based forecaster, which encompasses series variations and mitigates excessively long autoregression. To focus on modeling temporal variations, our forecaster predicts each variate independently. Beyond Channel Independence  that implicitly captures the multivariate correlation  by shared parameters, AutoTimes converts timestamps into position embeddings and explicitly aligns simultaneous segment tokens, which is detailed in the next paragraph. Therefore, we simplify \(_{t}\) as the time point of specific variate \(x_{t}\). Given a single-variate time series of context length \(NS\), the \(i\)-th segment of length \(S\) is denoted as:

\[_{i}=\{x_{(i-1)S+1},,x_{iS}\}^{S},\;i=1,,N. \]

Considering the general-purpose token transition, we freeze the parameters of large language models. To realize the token-wise alignment between time series tokens and language tokens, we establish \(():^{S}^{D}\) that independently embeds segments into the latent space:

\[_{i}=(_{i}),\;i=1,,N, \]

where \(D\) is consistent with the dimension of the LLM.

Position embeddingTimestamp, an essential covariate indicating the chronological information, is generally utilized as an extra embedding in previous deep forecasters . However, increasing models  have discarded the embedding and found the performance will not be greatly affected, implying the improper encoding of timestamps. In contrast, textual timestamps have been demonstrated as an enhancement in LLM4TS methods, which are always formulated into prefix-prompts . Nevertheless, it also leads to excessive context length, impeding LLMs from paying sufficient attention to time series tokens and inducing time-consuming feed-forwarding. Inspired by the functionality of position embedding, which incorporates information about the relative or absolute position of the tokens . We adopt LLM-embedded timestamps as position embeddings to utilize temporal information and align simultaneous events (segments) from different varieties.

Technically, we formulate the starting and end timestamps of corresponding segments by the template demonstrated in Figure 3. Experimentally, we observe that the simple template without deft design can consistently boost the forecasting performance in Appendix D.5, adding the LLM-based forecaster to comprehend the date and align different variates based on Channel Independence. Since all the previous language tokens are visible to the special ending token <EOS> of a sentence, we adopt the embedding of <EOS> as \(_{i}^{D}\) as the position embedding from textual timestamps:

\[_{i}=\,\,((_{i})). \]

Notably, \(_{i}\) is pre-computed by LLMs such that runtime forwarding for language tokens is not required during training. Given that the latent space of the LLM locates both time series tokens and

Figure 2: An example to illustrate how AutoTimes adapts language models for time series forecasting.

language tokens, the position embedding can be integrated with the corresponding time span without increasing the context length. Concretely, the token embedding \(_{i}^{D}\) is obtained by:

\[_{i}=_{i}+_{i}. \]

### Next Token Prediction

As shown in Figure 3, prevalent LLMs [6; 36] are endowed with the capability of predicting the next token \(_{i}\) based on the preceding tokens \(_{<i}\). We reutilize LLMs in a fully consistent approach and generate prediction of arbitrary lengths iteratively. Given a time series of context length \(NS\), the input series is segmented and embedded into \(N\) token embeddings \(\{_{1},,_{N}\}\). The training objective is to independently generate the next tokens \(\{_{2},,_{N+1}\}\). We feed the token embeddings \(_{i}\) into the intermediate layers of the LLM, which inherently parameterize token transitions:

\[\{}_{2},,}_{N+1}\}=(\{ _{1},,_{N}\}). \]

We adopt \(():^{D}^{S}\) to independently projects embeddings to segments:

\[}_{i}=(}_{i}),\;i=2, ,N+1. \]

Finally, each predicted segment is supervised by the token-wise ground truth to optimize the parameters of embedding and projection layers, which are simply implemented by multi-layer perceptrons:

\[_{}=||_{i}-}_{i} ||_{2}^{2},\;i=2,,N. \]

Notably, the context length \(NS\) is decided during training, representing the maximum input length during inference. Therefore, one consequent forecaster is suitable for different input lengths like the LLM, validated in Appendix D.4. Moreover, AutoTimes can generate predictions of arbitrary lengths by iterative multi-step forecasting, proven to overcome error accumulation better than state-of-the-art forecasters in Section 4.1, since autoregressive LLMs inherently excel at multi-step generation:

\[}_{i}=(_{<i}),\;i=1,, {F}{S}. \]

Instead of respectively training models on different lookback/forecast lengths, AutoTimes handles all the scenarios by one model. Surprisingly, with the consistency of autoregression, it also inherits notable generalizability and scaling behavior of LLMs, which is demonstrated in Sections 4.2 and 4.4.

Figure 3: Overview of AutoTimes: (1) time series and corresponding timestamps are segmented; (2) textual timestamps are converted into the position embeddings by the LLM; (3) time series segments are embedded and projected by next token prediction, where intermediate layers of LLM are frozen.

### In-Context Forecasting

Large language models are capable of generating expected outputs based on provided task demonstrations from downstream datasets without gradient updating, known as the in-context learning ability. The task demonstrations are generally constituted by paired questions and answers . Formally, the context \(=\{g(x^{(1)},y^{(1)}),,g(x^{(m)},y^{(m)})\}\) represents a set of demonstrations with \(m\) pairs, where \(g()\) is the template that transforms each question and answer into natural language.

In terms of time series forecasting, we propose to constitute the pair by lookback-forecast windows, which are exactly represented as successive time points from earlier historical observations. Hence, we use time series in target datasets as prompts, _extending the context for prediction beyond consecutive lookback series_. We denote the extended context as \(\), which contains \(m\) time series prompts \(^{(j)}\):

\[=\{^{(j)}=_{ t_{j}}|\},\;j=1,,m,\;t_{j} L. \]

During training, we first obtain an LLM-based forecaster on a source dataset and select time series prompts from the downstream target dataset based on a unified strategy. During inference, we ensure all the prompts appear before the window to be predicted, such that there is no data leakage from future information. As shown in Figure 4, we concatenate time series prompts with lookback series and feed them as the context of the forecaster, termed _in-context forecasting_:

\[f:(,_{1:L},_{1:L+F})}_{L+ 1:L+F}. \]

## 4 Experiments

We conduct thorough evaluations of the performance of AutoTimes, including time series forecasting, zero-shot forecasting, and the proposed in-context forecasting. Additional analyses are included to evaluate the generality, scaling behavior, and adaptation cost of large language models. Detailed code implementation for reproduction is provided in our public code repository.

### Time Series Forecasting

BenchmarksFor long-term time series forecasting, we extensively include real-world datasets, including ETM1, ECL, Traffic, Weather , and Solar-Energy . For short-term forecasting, we adopt the well-acknowledged M4 competition . Detailed descriptions are provided in Appendix A.

BaselinesWe compare AutoTimes with state-of-the-art models, including advanced LLM4TS methods: TimeLLM , UniTime , and FPT ; well-acknowledged deep forecasters: iTransformer , DLLinear , PatchTST , and TimesNet . For the challenging short-term forecasting, we further include competitive baselines: Koopa , N-HiTS  and N-BEATS . All baselines are officially implemented or reported. We adopt LLaMA-7B  as our base LLM. Detailed implementations, error bars, and hyperparameter analysis are provided in Appendix B and C.

SetupsFor short-term forecasting, we follow the well-acknowledged TimesNet , which assesses the fundamental ability of forecasters in modeling temporal variations. For long-term forecasting, we establish a novel _one-for-all_ benchmark: a single forecaster is trained on one dataset and subsequently utilized for all prediction lengths. We highlight that this approach evaluates the basic versatility as foundation models of time series, which aims to break the prevailing practice of extensive training across diverse real-world scenarios. To be specific, we evaluate all methods by rolling forecasting: a model is trained with predetermined input/output lengths, and the predicted values are integrated as part of the input in subsequent iterations until reaching the desired forecast length. Therefore, the key to success in this task lies in mitigating multi-step error accumulation. Still, the conventional _one-for-one_ approach that trains forecasters respectively on each length is also provided in Table 12.

ResultsThe average results are presented in Table 2- 3, with the best results in **bold** and the second best underlined. AutoTimes consistently outperforms all counterparts of short-term forecasting in Table 2, demonstrating the basic ability of LLM-based forecasters to capture diverse series variations. Further, in the one-for-all long-term scenarios, AutoTimes surpasses other LLM4TS methods and deep forecasters in \(80\%\) datasets in Table 3, outperforming previous state-of-the-art TimeLLM by\(9.12\%\) in average. Compared with other forecasters trained in the one-for-one scenario in Table 12, AutoTimes still achieved state-of-the-art performance in \(70\%\) of settings without respective training.

By diving into the proposed one-for-all and the traditional one-for-one benchmarks in Table 3 and 12, it is notable that prevalent deep forecasters, such as Transformer-based forecasters and DLinear, can achieve competitive and even better results under rolling forecasting. Nevertheless, the performance of non-autoregressive LLM4TS methods can degenerate a lot without respective training. Therefore, it highlights our persistent utilization of autoregression and thorough leveraging of inherent token transitions of LLMs, thereby mitigating error accumulation during multi-step rolling forecasting.

### Zero-Shot Forecasting

SetupsLarge language models have exhibited remarkable zero-shot generalization capability . To verify whether our LLM-based forecaster inherits this ability, where no training sample of the target domain is available, we assess the performance of zero-shot forecasting. Concretely, we adhere to the benchmark established by FPT , where the forecaster is initially trained on a source domain and subsequently evaluated on an unseen target domain. We conduct the transfer learning between the M3 and M4 competitions, both of which encompass abundant temporal variation patterns but follow different data distributions. We compare AutoTimes with deep forecasters and FPT as the only LLM4TS method, given that only FPT has exhibited zero-shot generalization in this benchmark.

ResultsThe comprehensive results of zero-shot forecasting are presented in Table 4. AutoTimes demonstrates superior performance compared to deep forecasters and FPT in both M4 \(\) M3 and M3 \(\) M4 scenarios. It is evident that LLM4TS methods generally achieve improved performance in this task due to the enhanced model capacity, leading to a 15% SMAPE reduction compared with the efficient forecaster DLinear. Despite sharing the same Transformer backbone, LLM4TS methods still outperform PatchTST due to the transferable knowledge pre-trained on large corpora of sequences. This underscores the advantage of leveraging LLMs for time series forecasting. Moreover, AutoTimes inherits general-purpose token transitions, surpassing FPT without tuning intermediate LLM layers.

   Models & **AutoTimes** & FPT & DLinear & PatchTST & TimesNet & NSFomer & FEDFormer & Informer & Reformer \\  M4 \(\) M3 & **12.75** & 13.06 & 14.03 & 13.06 & 14.17 & 15.29 & 13.53 & 15.82 & 13.37 \\  M3 \(\) M4 & **13.036** & 13.125 & 15.337 & 13.228 & 14.553 & 14.327 & 15.047 & 19.047 & 14.092 \\   

Table 4: Zero-shot forecasting results in averaged SMAPE. M4 \(\) M3 trains forecasters on the datasets of M4 and evaluates on M3, and vice versa. Detailed results are provided in Appendix D.2

    &  &  &  &  &  &  &  &  \\    & & & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\  ETH1 & **0.389** & **0.422** & 0.412 & 0.437 & 0.683 & 0.596 & 0.429 & 0.439 & 0.421 & 0.445 & 0.426 & 0.444 & 0.409 & 0.430 & 0.495 & 0.491 \\  ECL & **0.159** & **0.253** & 0.181 & 0.288 & 0.325 & 0.399 & 0.184 & 0.284 & 0.164 & 0.258 & 0.165 & 0.265 & 0.169 & 0.268 & 0.201 & 0.303 \\  Weather & 0.235 & 0.273 & **0.225** & **0.266** & 0.461 & 0.459 & 0.228 & **0.266** & 0.266 & 0.291 & 0.239 & 0.291 & 0.226 & 0.268 & 0.264 & 0.293 \\  Traffic & **0.374** & **0.264** & 0.410 & 0.303 & 0.584 & 0.367 & 0.461 & 0.326 & 0.384 & 0.274 & 0.423 & 0.298 & 0.391 & 0.275 & 0.602 & 0.322 \\  Solar. & **0.197** & **0.242** & 0.263 & 0.335 & 0.392 & 0.462 & 0.236 & 0.303 & 0.213 & 0.291 & 0.222 & 0.283 & 0.202 & 0.269 & 0.213 & 0.295 \\   

Table 3: Long-term forecasting results of one-for-all: we conduct rolling forecasting with a single model trained on each dataset and accomplish four desired forecast lengths in \(\{96,192,336,720\}\). AutoTimes adapt LLMs with the context length \(C=672\). We set the input length \(L=672\) and output length \(F=96\) in other methods. All results are averaged. Full results is provided in Table 10.

[MISSING_PAGE_EMPTY:8]

Scaling behaviorScalability is an essential characteristic that emerges from small models to large foundation models. By investigating the results presented in Table 5, we observe that the prediction accuracy of the forecaster generally improves with the increase in LLM parameters. This scaling behavior of LLM-based forecasters introduces a trade-off between performance and adaptation cost. To provide a comprehensive assessment, we evaluate each adapted forecaster from three perspectives: performance, training speed, and parameters, as presented in Figure 5. We observe that the largest LLaMA-7B consistently delivers optimal forecasting performance. As a relatively small language model, OPT-1.3B exhibits good parameter efficiency as an out-of-the-box forecaster.

Adaptation costTo mitigate the substantial cost of adapting large language models, AutoTimes introduces minimal parameters with all intermediate layers of LLM frozen. Additionally, we seamlessly integrate the language tokens (e.g. textual timestamps) without excessive context length and runtime overhead for training, thereby significantly reducing the adaptation cost. Figure 6 presents a comprehensive efficiency analysis with advanced LLM4TS methods: FPT is applicable on GPT-2 and TimeLLM is applicable on LLaMA-7B. Not only does AutoTime achieve better results in Table 3, but its training and reasoning time is also greatly reduced, bringing over 5\(\) speedup on average. In terms of parameter efficiency, AutoTimes focuses on establishing the embedding for time series segments, which is simply implemented by the MLP (\(0.79M\)) account for \(0.1\%\) parameters of the LLM (\(7B\)). Therefore, the results affirm the effectiveness of reutilizing the inherent token transition.

Figure 5: Efficiency comparison of alternative LLMs, evaluated by the same configuration of Table 5.

   LLM & GPT-2 (124M) &  &  &  &  &  \\  Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\  ECL & 0.173 & 0.266 & 0.168 & 0.263 & 0.164 & 0.258 & 0.164 & 0.258 & 0.162 & 0.256 & 0.159 & 0.253 \\  ETH1 & 0.397 & 0.425 & 0.401 & 0.429 & 0.396 & 0.424 & 0.394 & 0.424 & 0.394 & 0.424 & 0.389 & 0.423 \\  Traffic & 0.406 & 0.276 & 0.405 & 0.277 & 0.397 & 0.271 & 0.394 & 0.269 & 0.393 & 0.270 & 0.374 & 0.264 \\  Weather & 0.242 & 0.278 & 0.240 & 0.275 & 0.240 & 0.276 & 0.243 & 0.277 & 0.247 & 0.282 & 0.235 & 0.273 \\   

Table 5: Averaged results of alternative language models. Full results are provided in Table 18.

Figure 6: Comparison of AutoTimes and other LLM4TS methods in terms of training/inference time and tunable parameters with the same batch size (\(224\)) on the ETH1 dataset.

Ablation studyRecent research has raised doubts about the validity of previous LLM4TS methods , which predominantly adopt non-autoregression, that is, treating the LLM as a BERT-style pre-trained backbone and utilize a globally flattening projector on all lookback tokens. Here, we provide a thorough ablation study to examine our proposed AutoTimes in Table 6. The results underscore that our method maintains the consistency of the decoder-only architecture and autoregressive inference, effectively leveraging LLMs and addressing the concerns regarding performance improvement and adaptation cost. Further, we provide a comparison by substituting our token-wise segment projection (consistent with LLMs) with the flatten linear head  (common in non-autoregressive forecasters). Results of Table 21 in the Appendix reveal that the performance of non-autoregressive generation is consistently inferior to that of our autoregressive AutoTimes approach.

LoRA adaptationBy incorporating low-rank adaptation technique  on the intermediate LLM layers, the token transition of the large language model can be further fine-tuned to align the future extrapolation of time series. Table 7 provides the performance comparing the incorporation of LoRA, which consistently improves the performance of the LLM-based forecaster adapted by AutoTimes.

## 5 Conclusion

This paper aims to develop foundation models for time series forecasting. We utilize off-the-shelf LLMs as autoregressive forecasters by transferring the general-purpose and multi-step generation ability. Different from prior methods, we notice prevalent non-autoregressive LLM4TS methods may contradict the decoder-only structure and lead to insufficient utilization of LLMs. Experimentally, the proposed method achieves state-of-the-art performance with remarkable model efficiency. Further analysis reveals that our forecaster effectively inherits advanced capabilities such as zero-shot and in-context forecasting, and is able to utilize both instructive times series and timestamps. In the future, we will further incorporate advanced low-rank adaptation and utilize booming language backbones.

   Dataset &  &  \\  Type & **AutoTimes** & w/o LLM & LLM2Attn & LLM2Tsf &  &  & LLM2Attn & LLM2Tsf \\  Metric & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\  Pred-\(96\) & **0.360** & **0.400** & 0.365 & 0.399 & 0.383 & 0.404 & 0.377 & 0.401 & **0.129** & **0.225** & 0.171 & 0.263 & 0.156 & 0.255 & 0.162 & 0.263 \\  Pred-\(192\) & **0.388** & **0.419** & 0.405 & 0.425 & 0.414 & 0.422 & 0.406 & 0.420 & **0.147** & **0.241** & 0.192 & 0.282 & 0.178 & 0.276 & 0.189 & 0.287 \\  Pred-\(336\) & **0.401** & **0.429** & 0.429 & 0.441 & 0.431 & 0.432 & 0.421 & 0.431 & **0.162** & **0.258** & 0.216 & 0.304 & 0.198 & 0.295 & 0.216 & 0.309 \\  Pred-\(720\) & **0.406** & **0.440** & 0.450 & 0.468 & 0.456 & 0.454 & 0.449 & 0.452 & **0.199** & **0.288** & 0.264 & 0.342 & 0.230 & 0.320 & 0.258 & 0.340 \\   

Table 6: We follow the protocol of LLM4TS ablation studies  to verify whether the LLM is truly useful in our AutoTimes: (1) _w/o LLM_ replaces the language model entirely and passing input tokens directly to the last layer; (2) _LLM2Attn_ replaces the language model with a single multi-head attention layer; (3) _LLM2Tsf_ replaces the language model with a single transformer block.

    &  &  &  &  &  \\   & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\   & AutoTimes & 0.360 & **0.397** & 0.140 & 0.236 & 0.158 & 0.208 & 0.369 & 0.257 & 0.179 & 0.220 \\  & **+ LoRA** & **0.357** & **0.397** & **0.130** & **0.225** & **0.151** & **0.201** & **0.360** & **0.256** & **0.176** & **0.219** \\   & AutoTimes & **0.391** & **0.419** & 0.159 & 0.253 & 0.207 & 0.254 & 0.394 & 0.268 & 0.198 & 0.236 \\  & **+ LoRA** & **0.391** & 0.420 & **0.149** & **0.242** & **0.197** & **0.244** & **0.383** & **0.267** & **0.195** & **0.235** \\   & AutoTimes & **0.408** & **0.432** & 0.177 & 0.270 & 0.262 & 0.298 & 0.413 & 0.278 & 0.213 & 0.252 \\  & **+ LoRA** & 0.409 & 0.433 & **0.164** & **0.259** & **0.251** & **0.287** & **0.401** & **0.277** & **0.208** & **0.249** \\   & AutoTimes & 0.429 & 0.452 & 0.216 & 0.303 & 0.342 & 0.353 & 0.449 & **0.299** & 0.239 & 0.277 \\  & **+ LoRA** & **0.426** & **0.451** & **0.202** & **0.293** & **0.326** & **0.339** & **0.440** & 0.300 & **0.225** & **0.268** \\   

Table 7: Full long-term forecasting results of AutoTimes and AutoTimes equipped with LoRA .