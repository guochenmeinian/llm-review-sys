# Tight Risk Bounds for

Gradient Descent on Separable Data

Matan Schliserman\({}^{1}\)1   Tomer Koren\({}^{12}\)

\({}^{1}\)Blavatnik School of Computer Science, Tel Aviv University

\({}^{2}\)Google Research, Tel Aviv

###### Abstract

We study the generalization properties of unregularized gradient methods applied to separable linear classification--a setting that has received considerable attention since the pioneering work of Soudry et al. . We establish tight upper and lower (population) risk bounds for gradient descent in this setting, for any smooth loss function, expressed in terms of its tail decay rate. Our bounds take the form \((r_{,T}^{2}/^{2}T+r_{,T}^{2}/^{2}n)\), where \(T\) is the number of gradient steps, \(n\) is size of the training set, \(\) is the data margin, and \(r_{,T}\) is a complexity term that depends on the tail decay rate of the loss function (and on \(T\)). Our upper bound greatly improves the existing risk bounds due to Shamir  and Schliserman and Koren , that either applied to specific loss functions or imposed extraneous technical assumptions, and applies to virtually any convex and smooth loss function. Our risk lower bound is the first in this context and establish the tightness of our general upper bound for any given tail decay rate and in all parameter regimes. The proof technique used to show these results is also markedly simpler compared to previous work, and is straightforward to extend to other gradient methods; we illustrate this by providing analogous results for Stochastic Gradient Descent.

## 1 Introduction

Recently, there has been a marked increase in interest regarding the generalization capabilities of unregularized gradient-based learning methods. One specific area of attention in this context has been the setting of linear classification with separable data, where a pioneering work by Soudry et al.  showed that, when using plain gradient descent to minimize the empirical risk on a linearly separable training set with an exponentially-tailed classification loss (such as the logistic loss), the trained predictor will asymptotically converge in direction to the max-margin solution. As a result, standard margin-based generalization bounds for linear predictors suggest that, provided the number of gradient steps (\(T\)) is sufficiently large, the produced solution will not overfit, despite the lack of explicit regularization and the fact that its magnitude (i.e., Euclidean norm) increases indefinitely with \(T\). This result has since been extended to incorporate other optimization algorithms and loss functions .

Despite the high interest in this problem, the tight finite-time (population) risk performance of unregularized gradient methods, and even just of gradient descent, have not yet been fully understood. The convergence to a high margin solution exhibited by Soudry et al.  (for the logistic loss) occurs at a slow logarithmic rate, thus, the risk bounds for the trained predictors only become effective when \(T\) is at least exponentially large in comparison to the size of the training set \(n\) and the margin \(\). In a more recent work, Shamir  established risk bounds of the form \((1/^{2}T+1/^{2}n)\) for several gradient methods in this setting (still with the logistic loss), that apply for smaller and more realistic values of \(T\)Later Schliserman and Koren  gave a more general analysis that extends the bounds of Shamir  to a wide range of smooth loss functions satisfying a certain "self-boundedness" condition, and Telgarsky  provided a test-loss analysis of stochastic mirror descent on "quadratically-bounded" losses. However, all of these works either assume a specific loss (e.g., the logistic loss), impose various conditions on the loss function (beyond smoothness), or do not establish the tightness of their bounds in all regimes of parameters (\(T,n\) and \(\)), and especially in the regime where the number of gradient steps \(T\) is larger than the sample size \(n\) (see Table 1 for details).

### Our contributions

In this work, we close these gaps by showing nearly matching upper and lower risk bounds for gradient descent, which hold essentially for _any_ smooth loss function, and for any number of steps \(T\) and sample size \(n\). Compared to recent prior work in this context [13; 16; 12], our results do not require any additional assumptions on the loss function besides its smoothness, and they strictly improve upon the existing bounds by their dependence on \(T\). Further, to the best of our knowledge, risk lower bounds were not previously explored in this context, and the lower bounds we give establish, for the first time, the precise risk convergence rate of gradient descent for any smooth loss function and in all regimes of \(T\) and \(n\).

In some more detail, our results assume the following form. Let \(_{,}\) be the class of nonnegative, convex and \(\)-smooth loss functions \((u)\) that decay to zero (as \(u\)) faster than a reference "tail function" \(:[0,)\). The function \(\) is merely used to quantify how rapidly the tails of loss functions in \(_{,}\) decay to zero. Then, the risk upper and lower bounds we prove for gradient descent are of the following form:2

\[^{2}}{^{2}T}+^{2}}{^{2 }n}. \]

* The lower bound is shown for the empirical (logistic) risk.
* \(f\) is \((c,)\)-self-bounded if for every \(x\), it holds that \(\| f(x)\| cf(x)^{1-}\); for \(<1/2\), this property is stronger than smoothness of \(f\).
* \(\) \(f\) is \((c_{1},c_{2})\)-quadratically-bounded if, for every \(x\) and \(z\), \(|^{}(x,z)| c_{1}+c_{2}(\|x\|+\|z\|)\).
* The upper bound is valid when \(T n\).

Table 1: Comparison between the bounds established in this work and bounds established in previous work for gradient descent on \(\)-separable data; here, \(T\) is the number of gradient steps and \(n\) is the size of the training set. Logarithmic factors other than \( T\) factors are suppressed from the bounds. Examples of how our general bounds are instantiated for some concrete loss functions are presented in Table 2 below.

The bounds depend on the tail function \(\) through the term \(r_{,T}\) that, roughly, equals \(^{-1}()\) for \(\) chosen such that \((^{-1}())^{2}/^{2}T\) (for the precise statements refer to Theorems 1 and 2, and for concrete examples of implied bounds, see Table 2).

The form of Eq. (1) resembles the bounds given in recent work by Schliserman and Koren . However, the latter work imposed an extraneous "self-boundedness" assumption (stronger than smoothness, see Table 1) that we do not require, and they did not establish the tightness of their bounds, as we do in this paper by providing matching lower bounds. On the flip side, their upper bounds apply in a broader stochastic convex optimization setup, whereas our bounds are specialized to generalized linear models for classification.

In terms of techniques, our proof methodology is also distinctly (and perhaps somewhat surprisingly) simple compared to previous work. Our upper bounds rely on two elementary properties that gradient methods admit when applied to a smooth and realizable objective: low training loss and low norm of the produced solutions. Both properties are obtained from fairly standard arguments and convergence bounds for smooth gradient descent, when paired with conditions implied by the decay rate of the loss function. Finally, to bound the gap between the population risk and the empirical risk, we employ a classical result of Srebro et al.  that bounds the generalization gap of linear models in the so-called "low-noise" (i.e., nearly realizable) smooth regime using local Rademacher complexities. Somewhat surprisingly, this simple combination of tools already give sharp risk results that improve upon the state-of-the-art  both in terms of tightness of the bounds and the light set of assumptions they rely on.

We remark here that the proof scheme summarized above can be generalized to essentially any gradient method for which one can prove simultaneously bounds on the optimization error and the norm of possible solutions produced by the algorithm. In the sequel, we focus for concreteness on standard gradient descent, but in Appendix B we also give bounds for Stochastic Gradient Descent (SGD), which is shown to admit both of these properties with high probability over the input sample.

An interesting conclusion from our bounds pertains to the significance of early stopping. We see that gradient descent, even when applied on a smooth loss functions that decay rapidly to zero, might overfit with respect the the surrogate loss (i.e., reach a trivial \((1)\) risk) as the number of steps \(T\) grows. The time by which gradient descent starts overfitting depends on the tail decay rate of the loss function: the slower the decay rate, the shorter the time it takes to overfit. Interestingly, a similar phenomenon may not occur for the zero-one loss of the trained predictor. For example, Soudry et al.  show that with the logistic loss, gradient descent does not overfit in terms of the zero-one as \(T\) approached infinity--whereas, at the same time, it _does_ overfit as \(T\) in terms of the logistic loss itself, as seen from our lower bounds.

Summary of contributions.To summarize, the main contribution of this paper are as follows:

* Our first main result (in Section 3) is a high probability risk upper bound for gradient descent in the setting of separable classification with a convex and smooth loss function. Our bound matches the best known upper bounds  and greatly extend them to allow for virtually any convex and smooth loss function, considerably relaxing various assumptions imposed by prior work.

   Tail decay (\(\)) & \(r_{,T}\) & Risk bound \\  \((-x)\) & \((T)\) & \(((T)}{^{2}T}+(T)}{^{2}n})\) \\ \(x^{-}\) & \((^{2}T)^{}\) & \(()^{}(}}+}}{n})\) \\ \((-x^{})\) & \(^{}(T)\) & \((}(T)}{^{2}T}+}(T)}{^{2}n})\) \\   

Table 2: Examples of tight population risk bounds established in this paper for gradient descent on \(\)-separable data, instantiated for several different loss tail decay rates. Details on the derivation of the bounds from the general Theorems 1 and 2 can be found in Appendix A.

* Our second main result (in Section 4) is a nearly matching lower bound for the risk of gradient descent, establishing the tightness of our analysis given the smoothness and tail-decay conditions. The tightness of our bounds holds across tail decay rates and different regimes of the parameters \(T,n\) and \(\). To our knowledge, these constitute the first (let alone tight) risk lower bounds in this context, despite a long line of work on linear classification with separable data.
* We also provide analogous results for Stochastic Gradient Descent with replacement (in Appendix B), mainly to emphasize that that our analysis uses only two elementary properties of the optimization algorithm: low optimization error and low norm of the produced solution. The same analysis can be generalized to any gradient method that admits these two properties.

### Additional related work

Unregularized gradient methods on separable data.The most relevant work to ours is of Schliserman and Koren , who used algorithmic stability and two simple conditions of self-boundedness and realizability which the loss functions hold, to get generalization bounds for gradient methods with constant step size in the general setting of stochastic convex and smooth optimization. Then, they derived risk bounds which hold in expectation for the setting of linear classification with separable data for every loss function which decays to \(0\). The exact bound was depend in the rate of decaying to \(0\) of the function. For the Lipschitz case their risk bound with respect to the loss function \(\) is \(O(^{-1}()^{2}/^{2}T+^{-1}()^{2}/^{2}n)\) for any choice of \(\) such that \(/^{-1}()^{2} 1/^{2}T\). For example, for the logistic loss, the bound translates to \(O(^{2}(T)/^{2}T+^{2}(T)/^{2}n)\).

In another work, Telgarsky , showed a high probability risk bound for \(T n\) for Batch Mirror Descent with step size \( 1/\) in linear models, using a reference vector, which when selected properly, can be translated to a risk bound of \(O(^{-1}()/)\) for gradient descent applied on the loss function \(\), and to a \(O( T/)\) for the logistic loss.

Fast rates for smooth and realizable optimization.The problem of smooth and realizable optimization, also known as the "low-noise" regime of stochastic optimization, is a very well researched problem. Srebro et al.  showed that stochastic gradient descent achieved risk bound of \(O(1/n)\) in this setting. For linear models, they also showed that ERM achieve similar fast rates by using local Rademacher complexities. Later  showed that SGD converges linearly when the loss function is also strongly convex. In more recent works,  used stability arguments to show that SGD with replacement with \(T=n\) achieve risk of \(O(1/n)\).

Lower bounds.A lower bound related to ours appears in Ji and Telgarsky . In this work, the authors showed a lower bound of \(\|w_{t}^{}-w^{*}\|(n)/(T)\). In our work, however, we get lower bound directly for the loss itself and not for this objective. More recently, Shamir  showed a lower bound of \((1/^{2}T)\) for the empirical risk of GD when applied on the logistic loss which is tight up to log factors. When generalizing this technique for other objectives, e.g., functions that decay polynomially to zero, the log factors become polynomial factors and the bound becomes not tight, even in the regime \(T n\) where the \(1/T\) term in the bounds is dominant. In contrast, we establish nearly tight bounds for virtually any tail decay rate, and in all regimes of \(T\) and \(n\).

## 2 Problem Setup

We consider the following typical linear classification setting. Let \(\) be distribution over pairs \((x,y)\), where \(x^{d}\) is a \(d\)-dimensional feature vector and \(y\) is a real number that represents the corresponding label. We assume that data is scaled so that \(\|x\| 1\) and \(|y| 1\) (with probability one with respect to \(\)). We focus on the setting of _separable_, or _realizable_, linear classification with margin. Formally, we make the following assumption.

**Assumption 1** (realizability).: There exists a unit vector \(w^{*}^{d}\) and \(>0\) such that \(y(w^{*} x)\) almost surely with respect to the distribution \(\).

Equivalently, we will identify each pair \((x,y)\) with the vector \(z=yx\), and realizability implies that \(w^{*} z\) with probability \(1\). Our assumptions then imply that \(\|z\| 1\) with probability \(1\).

Given a nonnegative loss function \(:^{+}\), the objective is to determine a model \(w^{d}\) that minimizes the (population) risk, defined as the expected value of the loss function over the distribution 

[MISSING_PAGE_FAIL:5]

The expression \((^{-1}())^{2}/\) increases indefinitely as \(\) approaches \(0\); therefore, for any \(T\), there exists an \(>0\) that satisfies the theorem's condition. For several examples of how this bound is instantiated for different tail decay functions \(\), see Table 2 and Appendix A.

In the remainder of this section we prove Theorem 1. The structure of the the proof will be as follows: First, we bound the norm of the GD solution; by smoothness and realizability, we get that the norm will remain small compared to a reference point with small loss value. Second, we get a bound on the optimization error in this setting, the relies on the same reference point. Finally, we use a fundamental result due to Srebro et al.  (reviewed in the subsection below) together with both bounds to derive the risk guarantee. As discussed broadly in the introduction, this proof scheme can be generalized to other gradient methods which satisfy the properties of model with low norm and low optimization error.

### Preliminaries: Uniform Convergence Using Rademacher Complexity

One property of linear models is that in this class of problems is that we have dimension-independent and algorithm-independent uniform convergence bounds, that enables to bound the difference between the empirical risk and the population risk of a specific model. A main technical tool for bounding this difference is the Rademacher Complexity . The worst-case Rademacher complexity of an hypothesis class \(H\) for any sample size \(n\) is given by:

\[R_{n}(H)=_{z_{1},,z_{n}}_{(\{ 1\}^{n })}[\ _{h H}|_{i=1}^{n}h(z_{i})_{i} |\ ].\]

We are interested in models that achieve low empirical risk on smooth objectives. A fundamental result of Srebro et al.  bounds the generalization gap under such conditions:

**Proposition 1** (15, Theorem 1).: _Let \(H\) be a hypothesis class with respect to some non negative and \(\)-smooth function, \((t y)\), such that for every \(w H,x,y\), \(|(wx y)| b\). Then, for any \(>0\) we have, with probability at least \(1-\) over a sample of size \(n\), uniformly for all \(h H\),_

\[L(h)(h)+K((h)}(^{1.5} (n)R_{n}(H)+}{n}})+^{3}(n)R_{ n}^{2}(H)+}{n}).\]

_where \(K<10^{5}\) is a numeric constant._

### Properties of Gradient Descent on Smooth Objectives

In this section we prove that GD satisfies the two desired properties- low norm and low optimization error. We begin with showing that the norm of \(w_{T}\), the output of GD after \(T\) iterations, is low, as stated in the following lemma,

**Lemma 1**.: _Let \(\) be a tail function and let \(_{,}\). Fix any \(>0\) and a point \(w_{}^{*}^{d}\) such that \((w_{}^{*})\) (exists due to realizability). Then, the output of \(T\)-steps GD, applied on \(\) with stepsize \( 1/\) initialized at \(w_{1}=0\) has,_

\[\|w_{T}\| 2\|w_{}^{*}\|+2.\]

Now, we bound the optimization error of GD on every function \(_{,}\), by using a variant of Lemma 13 from . The proof is fairly standard and appears in Appendix C.

**Lemma 2**.: _Let \(\) be a tail function and let \(_{,}\). Fix any \(>0\) and a point \(w_{}^{*}^{d}\) such that \((w_{}^{*})\). Then, the output of \(T\)-steps GD, applied on \(\) with stepsize \( 1/\) initialized at \(w_{1}=0\) has,_

\[(w_{T})^{*}\|^{2}}{ T}+2.\]

### Proof of Theorem 1

We now turn to prove Theorem 1. The proof is a simple consequence of the properties proved above and Proposition 1. We first claim that there exists a model \(w_{}^{*}\) with low norm such that \((w_{}^{*})\)which implies, through Lemmas 1 and 2, that \(w_{T}\) of gradient descent has both low optimization error _and_ it remains bounded within a ball of small radius. Then, we use Proposition 1 to translate the low optimization error to low risk.

_Proof of Theorem 1._ First, we show that there exists a model \(w_{}^{*}\) with low norm such that \((w_{}^{*})\). Let \(\) and \( C_{,}\). By separability, there exists a unit vector \(w^{*}\) such that \(w^{*} z_{i}\) for every \(z_{i}\) in the training set \(S\). Moreover, \(\) is monotonic decreasing. Then, for \(w_{}^{*}=^{-1}()/w^{*}\) and every \(z_{i} S\),

\[(w_{}^{*} z_{i})=(()}{ }w^{*} z_{i})(()}{ })^{-1}().\]

Then, by the fact that \(^{-1}() 0\), we have \((w_{}^{*} z_{i})(^{-1}())( ^{-1}())=\) for all \(i\), hence

\[(w_{}^{*})=_{i=1}^{n}(w_{ }^{*} z_{i}).\]

Now, for \(\) such that \(^{2}T(^{-1}())^{2}/\), we get by Lemma 1,

\[\|w_{T}\| 2\|w_{}^{*}\|+2()}{}.\]

For the same \(\), by Lemma 2,

\[(w_{T})^{*}\|^{2}}{ T}+2  3()^{2}}{^{2} T}.\]

Denote \(B_{}=\{w:\|w\| r_{}\}\), where for brevity \(r_{}=4^{-1}()\). We have, by Lemma 10, \(f(x) 2f(y)+\|x-y\|^{2}\) for all \(x,y^{d}\) (see proof in Appendix C). Then, together with the fact that \(\|z\|\), \(\|z^{}\| 1\) and choosing \(\) such that \(^{-1}()^{2}^{2} T\), with probability 1,

\[b=_{w B_{}}|(w z)| 2(w_{}^{*}z)+4  r_{}^{2} 2+4 r_{}^{2}^{2}}{8 T}+4 r_{}^{2}.\]

Moreover, \(B_{}\) is hypothesis class of linear predictors with norm at most \(r_{}\). We know that the norm of the examples is at most 1, thus, it follows that the Rademacher complexity of \(B_{}\) is \(R_{n}(B_{})=r_{}/\) [e.g., 6, Theorem 3].

Now, by the choice of \(\), we have \((w_{T}) 3r_{}^{2}/16 T\). Thus, Proposition 1 implies that with probability at least \(1-\), for every \(w B_{}\) and any \(\) such that \(^{-1}()^{2}^{2} T\),

\[L(w_{T})^{2}}{16 T}+K(^{2}}{16 T}}(^{1.5}n}{}+}{n}})+^{3}n ^{2}}{n}+}{n}).\]

Plugging in the bound on \(b\), dividing by \(r_{}^{2}\) and using twice the fact that \(xyx^{2}+y^{2}\) for all \(x\), \(y\),

\[)}{r_{}^{2}}+K(}(^{1.5}n}{}++4)}{n}})+n}{n}+ +4)}{n})\]

\[+K(}(^{1.5}n}{}+}{8 Tn}+}{n}})+n}{n}+}{8 Tn}+}{n})\]

\[+K(+(^{1.5}n}{}+}{8 Tn}+}{n}})^{2}+n}{n}+}{8 Tn}+}{n})\]

\[+K(+n}{n}+ }{8 Tn}+}{n}+ n}{n}+}{8 Tn}+}{n})\]

\[+n+4 )}{n}+}{4 Tn}\]

The theorem follows by rearranging the inequality. \(\)Risk Lower Bounds

In this section we present our second main result: a lower bound showing that the bound we proved in Section 3 for gradient descent is essentially tight for loss functions in the class \(C_{,}\), for any given tail function \(\) and any \(>0\). Formally, we prove the following theorem.

**Theorem 2**.: _There exists a constant \(C\) such that the following holds. For any tail function \(\), sample size \(n 35\) and any \(T\), there exist a distribution \(\) and a loss function \( C_{,}\), such that for \(T\)-steps GD over a sample \(S=\{z_{i}\}_{i=1}^{n}\) sampled i.i.d. from \(\), initialized at \(w_{1}=0\) with stepsize \( 1/2\), it holds that_

\[[L(w_{T})] C(128 ))^{2}}{^{2}n}+C(8))^{2}}{^{ 2} T}.\]

_for any \(<\) such that \(^{2}T(^{-1}())^{2}/\)._

We remark that the right-hand side of the bound is well defined, as we restrict \(\) to be sufficiently small so as to ensure that all arguments to \(^{-1}\) are at most \(\) (recall that \(\) admits all values in \([0,]\) due to our assumptions that \((0)\)). Further, the lower bound above matches the upper bound given in Theorem 1 up to constants, unless the tail function \(\) decays extremely slowly, and slower than any polynomial (at this point, however, the entire bound becomes almost vacuous).

To prove Theorem 2, we consider two different regimes: the first is where \(T n\), when the first term in the right-hand side of the bound is dominant; and the \(T n\) regime where the second term is dominant. We begin by focusing on the first regime, and prove the following.

**Lemma 3**.: _There exists a constant \(C_{1}\) such that the following holds. For any tail function \(\), sample size \(n 35\) and any \(\) and \(T\), there exist a distribution \(\) with margin \(\), a loss function \( C_{,}\) such that for GD over a sample \(S=\{z_{i}\}_{i=1}^{n}\) sampled i.i.d. from \(\), initialized at \(w_{1}=0\) with stepsize \( 1/2\), it holds that_

\[[L(w_{T})] C_{1}(128)^{2}}{ ^{2}n},\]

_for any \(<\) such that \(^{2}T(^{-1}())^{2}/\)._

To prove the lemma, we construct a learning problem for which the risk of GD can be lower bounded. We design a loss function \(\) which exhibits a decay to zero at a rate similar to that of the function \(\) for \(x>0\), and behaves as a quadratic function for \(x 0\). The distribution \(\) is constructed such that there are two fixed examples in its support, denoted \(z_{1}\) and \(z_{2}\), that are both sampled with constant probability and are almost opposite in direction to each other: \(z_{2}\) is aligned with \(-z_{1}\) except for a small component, denoted \(v\), which is orthogonal to \(z_{1}\). Crucially, since \(z_{1}\) and \(z_{2}\) point in nearly opposite directions, minimizing optimization error requires the GD iterate to have a substantial component aligned with the vector \(v\) (otherwise, the loss for \(z_{2}\) will be large). Finally, we define another example, denoted as \(z_{3}\), that has a significant component in the opposite direction \(-v\). We arrange the distribution \(\) so that \(z_{3}\) is sampled with with probability roughly \(1/n\). The lower bound now follows by observing that, if \(z_{3}\) is sampled at test time, but did not appear in the training set (this happens with probability \( 1/n\)), the test error is quadratically large in the magnitude of the GD iterate along \(-v\).

We now turn to formally proving Lemma 3.

Proof.: Given \(\), let us define the following distribution \(\):

\[=z_{1}:=(1,0,0)&(1-);\\ z_{2}:=(-,3,0)&(1-);\\ z_{3}:=(0,-,4+)&,\]

and loss function:

\[(x)=(x)&x 0;\\ (0)+^{}(0)x+x^{2}&x<0.\]

First, note that the distribution is separable: for \(w^{*}=(,,)\) it holds that \(w^{*}z_{i}=\) for every \(i\{1,2,3\}\). Moreover, Lemma 11 in Appendix D ensures that indeed \( C_{,}\).

[MISSING_PAGE_EMPTY:9]

Second, we focus the second expression in the lower bound of Theorem 2, which is dominant in the early stages of optimization.

**Lemma 4**.: _There exists a constant \(C_{2}\) such that the following holds. For any tail function \(\), and for any \(n,T\) and \(\), there exist a distribution \(\) with margin \(\), a loss function \( C_{,}\) such that for GD initialized at \(w_{1}=0\) with stepsize \( 1/2\) over an i.i.d. sample \(S=\{z_{i}\}_{i=1}^{n}\) from \(\) and \(w_{1}=0\) holds_

\[[L(w_{T})] C_{2}(8))^{2}}{^{2}T },\]

_for any \(\) such that \(^{2}T^{-1}()^{2}\)._

The argument for proving Lemma 4 is similar to that of Lemma 3. Here we define a \(1\)-Lipschitz loss function \(\) that decays to zero at the same rate as \(\), and a distribution such that there is a possible example \(z_{1}\) which is sampled with high probability and an almost "opposite" example \(z_{2}\), that with constant probability, appears rarely in the training set \(S\). The lower bound follows from the fact that although \(z_{2}\) appears in the dataset, the gradients of the loss function are bounded and thus not large enough so as to make the trained predictor classify \(z_{2}\) correctly. The full proof can be found in Appendix D.2.

Finally, we derive Theorem 2 directly from Lemmas 3 and 4:

Proof of Theorem 2.: Let \(C=\{C_{1},C_{2}\}\), where \(C_{1}\) and \(C_{2}\) are the constants from Lemmas 3 and 4, respectively. If \((^{-1}(8))^{2}^{2}T(^{-1}(128 ))^{2}^{2}n\), the theorem follows from Lemma 4; otherwise, it follows from Lemma 3.