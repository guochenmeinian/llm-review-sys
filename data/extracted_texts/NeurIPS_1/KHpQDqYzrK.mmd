# Multi-Task Neural Network Mapping onto Analog-Digital Heterogeneous Accelerators

Hadjer Benmeziane\({}^{1}\), Corey Lammie\({}^{1}\), Athanasios Vasilopoulos\({}^{1}\), Irem Boybat\({}^{1}\),

Manuel Le Gallo\({}^{1}\), Sidney Tsai\({}^{2}\), Kaoutar El Maghraoui\({}^{3}\), Abu Sebastian\({}^{1}\)

\({}^{1}\)IBM Research Europe, 8803 Ruschlikon, Switzerland

\({}^{2}\)IBM Research Almaden, 650 Harry Road, San Jose, CA USA

\({}^{3}\)IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, USA

hadjer.benmeziane@ibm.com

###### Abstract

Multi-Task Learning (MTL) models are increasingly popular for their ability to perform multiple tasks using shared parameters, significantly reducing redundant computations and resource utilization. These models are particularly advantageous for analog-digital heterogeneous systems, where shared parameters can be mapped onto weight-stationary analog cores. This paper introduces a novel framework, entitled Multi-task Heterogeneous Layer Mapping, designed to strategically map MTL models onto an accelerator that integrates analog in-memory computing cores and digital processing units. Our framework incorporates a training process that increases task similarity and account for analog non-idealities using hardware-aware training. In the subsequent mapping phase, deployment on the accelerator is optimized for resource allocation and model performance, leveraging feature similarity and importance. Experiments on the COCO, UCI, and BelgiumTS datasets demonstrate that this approach reduces model parameters by up to 3x while maintaining performance within 0.03% of task-specific models.

## 1 Introduction

Recent advances in the emerging paradigm of In-Memory Computing (IMC) have propelled it as a candidate to overcome the limitations of traditional computing. Analog IMC (AIMC) is of particular interest as it has the potential to scale to higher computational density with improved energy efficiency , making it especially appealing for a wide range of applications . By performing computations directly within the memory, AIMC reduces data movement and accelerates computation. However, the inherent noise and variability in analog processing can pose challenges to achieving consistent accuracy . Limited IMC weight capacity and oversized models can prohibit model deployment in a full weight-stationary manner, which is the key to its advantages . As a result, heterogeneous accelerators, which integrate both digital and analog components, offer an effective solution , combining the precision and flexibility of digital computation with the energy-efficiency of AIMC. Combining the best of both worlds, heterogeneous accelerators are a strong candidate for future AI systems, both in the edge and in data centers.

With the increasing demand for AI on edge devices, developing efficient methods to optimize and reduce model sizes has become more critical than ever. To address some of the edge-related challenges, MTL  has emerged as a powerful approach, enabling a single model to perform multiple tasks simultaneously using the same input representation, thereby minimizing redundant computations and resource consumption. This is especially vital in use cases like autonomous driving, where models must handle tasks such as object detection, semantic segmentation, and decision-making in real time. Such scenarios highlight the growing importance of MTL for delivering high performance while maintaining the efficiency required for edge deployment.

In this paper, we introduce a novel framework, entitled _Multi-task Heterogeneous Layer Mapping (MHLM)_, for training and deployment of MTL models on heterogeneous accelerators with both Digital Processing Units (DPUs) and AIMC components. Our framework focuses on model mapping on heterogeneous analog-digital accelerators. To maximize energy-efficiency, a weight-stationary approach is employed, where all shared components are mapped to AIMC cores, and task-specific components are assigned to DPUs. Shared weights remain stationary on the AIMC cores throughout the computation, significantly reducing the costly data movement between memory and processing units which is required by DPUs. Meanwhile, task-specific components are handled by DPUs to maintain the high accuracy required for specialized tasks, ensuring an optimal balance between performance and energy-efficiency.

We simulate heterogeneous deployment using Phase Change Memory (PCM)-based AIMC cores, modeled with the AIHWKit  and DPUs. We demonstrate that MHLM can reduce the number of parameters by 3\(\) while maintaining performance within 0.03% of task-independent models on average, across three different tasks and multiple multi-task models.

## 2 Related Work

Multi-Task Learning (MTL) ModelsMTL [8; 10] is a sub-field of Machine Learning (ML) where multiple tasks are learned simultaneously using a shared model, leveraging task commonalities to improve learning efficiency, data utilization, and reduce overfitting . Current State-of-the-Art (SOTA) MTL models are often handcrafted, requiring extensive experimentation to determine which components should be shared across tasks, leading to sub-optimal performance. To address these challenges, automated approaches such as Neural Architecture Search (NAS) [12; 13; 14] and adaptive optimizations [14; 15] have been developed, aiming to dynamically discover optimal sharing strategies during training. While these methods can reduce the manual effort involved in designing MTL models and potentially improve scalability, they often add significant computational complexity, increased memory requirements, and poor noise-resiliency in heterogeneous analog-digital accelerators.

Mapping Strategies for Heterogeneous Analog-Digital AcceleratorsMapping ML models onto heterogeneous accelerators presents a unique set of challenges, which has spurred significant research efforts in recent years [16; 17]. Traditional approaches often involve splitting the model into layers or modules that can be either efficiently or accurately executed on either analog or digital components , optimizing the deployment for a given target in accuracy and efficiency on a workload. However, these strategies have primarily focused on single-task models, with no exploration of how MTL models can be mapped onto such accelerators.

## 3 Multi-task Heterogeneous Layer Mapping

We develop a framework that trains a given network on a set of tasks, optimizing for maximum weight reuse and deployment on analog hardware. It subsequently maps the network onto a heterogeneous accelerator, searching for the largest contiguous part of the network, starting from its first layer, that can be shared between the tasks with minimal loss in accuracy. In detail, our contributions are as follows:

Figure 1: Illustration of (a) a heterogeneous analog-digital accelerator, depicting (b) a traditional heterogeneous approach and (c) a MHLM mapping approach. In the traditional case, heterogeneous mapping is performed independently for each task, whereas for the MHLM case, heterogeneous mapping are performed for a single task-agnostic DNN.

1. A Hardware-aware (HWA) training algorithm that enhances the similarity of model weights across tasks, enabling more efficient resource sharing and reducing redundancy;
2. An adaptive post-training mapping algorithm that uses Jensen-Shannon Divergence (JSD) and Shapley values to dynamically search for the largest shared part of the network, while keeping model performance over a threshold, maximizing the shared parameters and thus the energy efficiency;
3. A comprehensive evaluation on benchmark datasets, including COCO, UCI, and BelgiumTS, demonstrating the effectiveness of our approach in improving resource utilization, reducing energy consumption, and enhancing overall performance in multi-task learning scenarios.

The two components of our framework, namely the training and mapping algorithms are presented in Fig. 2. We start with a 1 training phase, where Gaussian noise is injected into the model weights to simulate the variability found in analog computing environments. During this phase, a joint multi-task loss function encourages the similarity of features across tasks. The next 2 mapping phase uses JSD and Shapley values  to evaluate the similarity and importance of features, determining whether a feature should be shared across tasks, i.e., mapped on AIMC cores, or remain task-specific, i.e., mapped on DPUs.

### Similarity-Enhancing Training

1 involves a HWA training algorithm that injects Gaussian noise into the model's weights during training. This noise simulates the variability encountered in analog computing, encouraging the model to learn more robust and similar representations across tasks. However, the added noise increases feature dissimilarity, rendering standard multi-task learning methods ineffective in the context of analog deployment. To address this, we propose a novel training approach that includes a joint multi-task loss to explicitly enhance similarity among tasks by penalizing large differences between the feature distributions of different tasks. The training algorithm pseudo-code is provided in Alg. 1.

After injecting noise during the forward propagation passes and obtaining the outputs for all tasks, the algorithm calculates a floating-point joint multi-task loss \(_{}\) (L9). This loss comprises two components: the task-specific loss \(_{}\) for each task and a regularization term that penalizes large differences between the feature distributions \(_{i}\) and \(_{j}\) of different tasks using the KL divergence. The regularization term is weighted by a factor, \(\). Once the total loss is computed, gradients are accumulated (L10), and the model weights are updated (L11) to minimize the loss1.

The benefits of this training approach are illustrated in Fig. 3, where the performance different training strategies are compared using the COCO dataset. Since analog devices are prone to temporal variations, causing their performance to fluctuate over time , we report the 1-day performance after the devices are programmed for all experiments2. This is reported after each training epoch for

Figure 2: Overview of the MHLM model mapping approach, which includes a 1 training phase, where noise injection and joint multi-task loss are used to enhance similarity across tasks, followed by a 2 mapping phase, that decides which layer are shared or task-specific.

four different training strategies: with and without noise injection, and with and without the joint multi-task loss. When the joint multi-task loss is applied, the JSD decreases, reflecting improved similarity between the learned representations of the tasks. Without the joint loss, the JSD remains around \(0.6\), indicating that the tasks are less aligned in their feature distributions. The inclusion of Gaussian noise aids in reducing variability, contributing to more stable and similar representations across tasks.

### Multi-Objective Mapping Algorithm

For 2, the objective is twofold: (i) maximize the shared portion of the model, enabling its deployment in weight-stationary AIMC, and (ii) simultaneously maximize the average 1-day performance across all tasks. The mapping algorithm uses JSD to measure the similarity between the feature map distributions of different tasks and Shapley values to assess the importance of each feature map.

The adaptive mapping algorithm is described in Supplementary Alg.2. The core decision-making process balances these metrics, governed by a threshold \(\), which determines whether a feature map should be shared or remain task-specific. Additionally, a weighting factor \(\) is introduced to prioritize configurations that enhance the 1-day performance. These threshold are empirically set. Supplementary Fig.2 shows the impact of these threshold on the final average performance.

```
0: Number of tasks \(T\), Gaussian noise level \(\), joint multi-task loss weight \(\)
0: Number of epochs \(T_{}\)
1:for each epoch \(t=1,,T_{}\)do
2: Get input data \(\) and task labels \(_{t}\) for all tasks \(t\{1,,T\}\)
3: Clear gradients, optimizer.zero_grad()
4:for each task \(t\{1,,T\}\)do
5: Apply Gaussian noise to weights: \(_{t}=_{t}+(0,^{2})\)
6: Get task output and encoded features \(}_{t},_{t}=f_{t}(;_{t})\)
7:endfor
8: Compute joint multi-task loss: \[_{}=_{t=1}^{T}_{}(}_{t},_{t})+_{i<j}(_{i}|| _{j})\]
9: Accumulate gradients, \(_{}\).backward()
10: Update model weights, optimizer.step()
11:endfor
```

**Algorithm 1** Training for Multi-task Model Mapping

## 4 Experiments

### Experimental Setup

**Datasets:** We evaluate the performance of our method using three datasets - COCO , UCI  and an autonomous driving scenario with BelgiumTS .

Figure 3: Evaluation of Training Strategies on the COCO Dataset. The plot shows how the average loss decreases across epochs using multi-task training with a lambda value of 0.05.

**Comparison Methods** We compare our results to NAS methods including MTL-NAS  and EDNAS , and adaptive sharing methods such as AdaShare  and AdaMTL . For each of these methods, we apply a HWA on the final multi-task network. We use the same mapping, i.e., shared portion in analog. We also compare to the original full task-specific networks with and without HWA. Full digital baselines for the adaptive multi-task networks can be found in Supplementary Table 1. Supplementary Section F expands on the training hyperparameter for each model.

**Evaluation Metrics:** For a comprehensive assessment, we employ a range of evaluation metrics across the different tasks. For the object detection and segmentation tasks, we use mean Average Precision (mAP) and mean Intersection over Union (mIoU) as primary metrics. For classification tasks, accuracy is used to measure the effectiveness of our approach. Additionally, we report the shared portion of the model, which quantifies the proportion of the network (in the number of parameters) that is shared across tasks, providing insights into the trade-offs between resource efficiency and task-specific performance.

**Experiment Mapping Time:** The training process with hardware-aware training and joint multi-task loss takes about 1.4\(\) longer than conventional training, due to similarity enhancement, but remains manageable as it is a one-time process. The mapping process, which calculates JSD for shared portions, averages around 15 minutes for larger networks such as DETR and FocalNet.

### Results

**COCO:** The results highlight the substantial reduction in the number of parameters achieved by our MHLM framework compared to task-specific training. Across all models, MHLM uses up to 3x fewer parameters while maintaining performance within 1% of task-specific training. The increase in the shared portion in MHLM directly correlates to energy savings, as more of the model is deployed on analog components, which are more energy-efficient. This trade-off between shared portion and performance is critical for resource-constrained environments. Although AdaMTL offers a higher shared portion, it suffers from a drastic drop in performance due to its failure to account for analog noise, emphasizing the importance of our hardware-aware training. AdaShare performs slightly better but still under-performs compared to MHLM, demonstrating the effectiveness of our noise-aware approach for hybrid analog-digital platforms. The low standard deviations across the metrics indicate that the performance was consistently high across multiple runs.

**UCI:** The results presented in Fig. 4 provide a detailed comparison of accuracy and shared portion for different tasks for the UCI dataset under different conditions. Fig. 4(a) highlights the accuracy across tasks for the baseline (0% shared) trained with and without noise, as well as for the multi-task mapping method (MHLM) under similar noise conditions. The baseline without noise shows the highest accuracy, but it does not benefit from sharing, which limits resource efficiency. Introducing noise in the baseline configuration results in a noticeable drop in accuracy across most tasks, indicating the sensitivity of the models to analog noise.

    & **Training** & **1-day** & **1-day** & **1-day** & **Shared** & **Analog** & **Digital** & **Analog MAC** \\  & **Scenario** & **mIoU** (\%) & **mAP (OD)** & **Accuracy (IC)** & **Portion (\%)** & **Params (M)** & **Params (M)** & **Ops (\%)** \\ 
**MTL-NAS** & – & 6.092 \(\) 0.050 & 0.635 \(\) 0.040 & 0.822 \(\) 0.045 & 22 & 10.2 & 34.9 & 30\% \\
**EDNAS** & – & 6.088 \(\) 0.045 & 0.628 \(\) 0.045 & 0.820 \(\) 0.040 & 31 & 11.4 & 24.3 & 32\% \\ 
**ResNet50 ** & Task-specific w/o HWA\({}^{*}\) & 0.753 & 0.686 & 0.588 & 0 & 0 & 76.1 & 0\% \\ Task-specific w/ HWA & 0.743 \(\) 0.045 & 0.673 \(\) 0.040 & 0.852 \(\) 0.050 & 0 & 76.1 & 0 & 98\% \\ AdaShare & 0.712 \(\) 0.048 & 0.650 \(\) 0.042 & 0.810 \(\) 0.048 & 58 & 17.9 & 10.8 & 48\% \\ AdaMTL & 0.690 \(\) 0.045 & 0.610 \(\) 0.043 & 0.792 \(\) 0.045 & 75 & 18.3 & 11.0 & 55\% \\
**MHLM** & **0.739 \(\) 0.040** & **0.668 \(\)** 0.040 & **0.846 \(\)** 0.050 & **6** & **16.25** & **9.9** & **68\%** \\  
**DETR ** & Task-specific w/o HWA\({}^{*}\) & 0.762 & 0.702 & 0.878 & 0 & 0 & 122.6 & 0\% \\ Task-specific w/o HWA & 0.751 \(\) 0.050 & 0.691 \(\) 0.050 & 0.873 \(\) 0.040 & 0 & 122.6 & 0 & 86\% \\ AdaShare & 0.720 \(\) 0.041 & 0.652 \(\) 0.044 & 0.803 \(\) 0.041 & 66 & 28.7 & 12.2 & 5\% \\ AdaMTL & 0.695 \(\) 0.044 & 0.620 \(\) 0.044 & 0.803 \(\) 0.043 & 80 & 29.4 & 12.5 & 56\% \\
**MHLM** & **0.748 \(\) 0.040** & **0.688 \(\)** 0.040 & **0.867 \(\)** 0.040 & **72.5** & **28.8** & **10.6** & **68\%** \\  
**FocalNet ** & Task-specific w/o HWA\({}^{*}\) & 0.738 & 0.678 & 0.848 & 0 & 0 & 85.4 & 0\% \\ Task-specific w/o HWA & 0.732 \(\) 0.050 & 0.671 \(\) 0.060 & 0.841 \(\) 0.050 & 0 & 85.4 & 0 & 92\% \\ AdaShare & 0.710 \(\) 0.058 & 0.636 \(\) 0.057 & 0.805 \(\) 0.052 & 48 & 15.6 & 13.5 & 44\% \\ AdaMTL & 0.685 \(\) 0.059 & 0.610 \(\) 0.059 & 0.795 \(\) 0.058 & 70 & 15.8 & 13.8 & 58\% \\
**MHLM** & **0.725 \(\) 0.050** & **0.646 \(\)** 0.050 & **0.338 \(\)** 0.050 & **53** & **14.6** & **13.0** & **76\%** \\    \({}^{*}\) Full digital models are not susceptible to conductance drift or noise.

\({}^{}\) Batch Norm and non weight-stationary attention MACs are included in the computation of this percentage.

Table 1: 1-day Performance Results on COCO. SS: Semantic Segmentation, OD: Object Detection, IC: Image Classification.

Conversely, the MHL approach without noise demonstrates improved accuracy compared to the baseline with noise, showcasing the effectiveness of our method in sharing components while still delivering strong performance. When noise is introduced to MHLM, a slight decrease in accuracy is observed, but it remains competitive with the baseline without noise, underscoring the robustness of the method. Fig. 4(b) shows the shared portion across tasks, where the MHLM configurations achieve significant sharing without substantial drops in performance. The results emphasize the advantage of our approach in balancing resource efficiency with task performance, making it well-suited for deployment in noise-prone environments.

**BelgiumTS:** Maximizing shared portions resulted in better resource utilization, for real-time sign segmentation and detection, particularly in edge-like models. The performance drops were minimal, with low standard deviations. The 1-day performance metrics underscore the robustness of these models under time-constrained conditions, with only slight reductions in accuracy compared to full training (0.005).

## 5 Discussion & Conclusion

The overall results demonstrate the efficacy of our MHLM framework. Notably, models trained with MHLM consistently exhibited a high shared portion while maintaining robust performance metrics. The Pareto analysis, shown in Supplementary Fig.1, further emphasizes the strategic trade-offs between performance and resource sharing, revealing how our framework adeptly balances these objectives. The ablation study, Supplementary Table 3, highlights the critical role of each step in MHLM.

MHLM is particularly impactful in scenarios with stringent resource constraints, enabling significant shared component usage without substantial performance degradation. Note that our framework can be generalized for optimized deployment in mixed-precision solely digital accelerators, but it is out of the scope of this work. Future work will quantitatively determine the energy-efficiency using system-level simulations and explore the joint architecture search and mapping for heterogeneous analog-digital accelerators.

    & **Training** & **1-day** & **1-day** & **Shared** & **Analog** & **Digital** & **Analog MAC** \\  & **Scenario** & **mIoU (SS)** & **Accuracy (SC)** & **Portion (\%)** & **Params (M)** & **Params (M)** & **Ops (\%) \({}^{}\)** \\ 
**ViT-Adapte-S ** & Task-specific w/o HWA & 0.810 & 0.948 & 0 & 0 & 36.9 & 0\% \\  & Task-specific w/HWA* & 0.803 \(\) 0.007 & 0.940 \(\) 0.007 & 0 & 36.9 & 0 & 95\% \\  & AdaShare & 0.773 \(\) 0.007 & 0.910 \(\) 0.008 & 58 & 12.3 & 10.1 & 50\% \\  & AdaShare & 0.760 \(\) 0.006 & 0.900 \(\) 0.007 & 72 & 12.7 & 10.2 & 52\% \\  & **MHLM** & **0.800 \(\) 0.006 & **0.938 \(\)** 0.007 & **60** & **12.3** & **10.1** & **65\%** \\  
**MaskFormer ** & Task-specific w/o HWA & 0.820 & 0.955 & 0 & 0 & 43.5 & 0\% \\  & Task-specific w/ HWA* & 0.813 \(\) 0.005 & 0.948 \(\) 0.006 & 0 & 43.5 & 0 & 88\% \\  & AdaShare & 0.780 \(\) 0.006 & 0.910 \(\) 0.007 & 62 & 14.5 & 11.1 & 54\% \\  & AdaMTL & 0.768 \(\) 0.006 & 0.900 \(\) 0.007 & 70 & 15.0 & 11.5 & 57\% \\  & **MHLM** & **0.810 \(\)** 0.005 & **0.945 \(\)** 0.006 & **62** & **14.5** & **11.1** & **70\%** \\  
**MILA-JAM ** & Task-specific w/o HWA & 0.800 & 0.940 & 0 & 0 & 35.1 & 0\% \\  & Task-specific w/ HWA* & -0.793 \(\) 0.007 & 0.930 \(\) 0.008 & 0 & 35.1 & 0 & 90\% \\  & AdaShare & 0.760 \(\) 0.007 & 0.900 \(\) 0.008 & 61 & 11.7 & 8.5 & 48\% \\  & AdaMTL & 0.748 \(\) 0.007 & 0.800 \(\) 0.008 & 75 & 11.9 & 8.7 & 50\% \\  & **MHLM** & **0.790 \(\)** 0.007 & **0.928 \(\)** 0.007 & **61** & **11.7** & **8.5** & **68\%** \\   

* Full digital models are not susceptible to conductance drift or noise.
* Batch Norm and non weight-stationary attention MACs are included in the computation of this percentage.
* Batch Norm and non weight-stationary attention MACs are included in the computation of this percentage.

Table 2: 1-day Performance Results on BelgiumTS Dataset. SS: Sign Segmentation, SC: Sign Classification.

Figure 4: Task-wise comparison of (a) accuracy and (b) the shared portion for the UCI dataset.