# Appendices

Provable benefits of annealing for estimating normalizing constants: Importance Sampling, Noise-Contrastive Estimation, and beyond

Omar Chehab

Universite Paris-Saclay, Inria, CEA

Palaiseau, France

l-emir-omar.chehab@inria.fr

&Aapo Hyvarinen

Department of Computer Science

University of Helsinki

Helsinki, Finland

aapo.hyvarinen@helsinki.fi

&Andrej Risteski

Department of Machine Learning

Carnegie-Mellon University

Pittsburgh, USA

aristesk@andrew.cmu.edu

###### Abstract

Recent research has developed several Monte Carlo methods for estimating the normalization constant (partition function) based on the idea of annealing. This means sampling successively from a path of distributions that interpolate between a tractable "proposal" distribution and the unnormalized "target" distribution. Prominent estimators in this family include annealed importance sampling and annealed noise-contrastive estimation (NCE). Such methods hinge on a number of design choices: which estimator to use, which path of distributions to use and whether to use a path at all; so far, there is no definitive theory on which choices are efficient. Here, we evaluate each design choice by the asymptotic estimation error it produces. First, we show that using NCE is more efficient than the importance sampling estimator, but in the limit of infinitesimal path steps, the difference vanishes. Second, we find that using the geometric path brings down the estimation error from an exponential to a polynomial function of the parameter distance between the target and proposal distributions. Third, we find that the arithmetic path, while rarely used, can offer optimality properties over the universally-used geometric path. In fact, in a particular limit, the optimal path is arithmetic. Based on this theory, we finally propose a two-step estimator to approximate the optimal path in an efficient way.

## 1 Introduction

Recent progress in generative modeling has sparked renewed interest in models of data that are defined by an unnormalized distribution. A prominent example is energy-based models, which are increasingly used in deep learning , and for which there are a variety of parameter estimation procedures . Another example comes from Bayesian statistics, where the posterior model of parameters given data is frequently known only up to a proportionality constant. Such models can be evaluated and compared by the probability they assign to a dataset, yet this requires computing their normalization constants (partition functions) which are typically high-dimensional, intractable integrals.

Monte-Carlo techniques have been successful at computing these integrals using sampling methods . The most common is importance sampling  which draws a sample from a tractable, "proposal" distribution to integrate the unnormalized "target" density. Noise-contrastive estimation (NCE)  uses a sample from _both_ the proposal and the target, to compute the integral. Yet such methods suffer from high variance, especially when the "gap" between the proposal and target densities is large [7; 8; 9]. This has motivated various approaches to gradually bridge the gap with intermediate distributions, which is loosely referred to as "annealing". Among them, annealed importance sampling (AIS) [10; 11; 12] is widely adopted: it has been used to compute the normalization constants of deep stochastic models [13; 14] or to motivate a lower-bound for learning objectives [15; 16]. To integrate the unnormalized "target" density, it draws a sample from an entire path of distributions between the proposal and the target. While annealed importance sampling has been shown to be effective empirically, its theoretical understanding remains limited [17; 18]: it is yet unclear when annealing is effective, for which annealing paths, and whether AIS is a statistically efficient way to do it.

In this paper, we define a family of _annealed Bregman estimators (ABE)_ for the normalization constant. We show that it is general enough to recover many classical estimators as a special case, including importance sampling, noise-contrastive estimation, umbrella sampling , bridge sampling  and annealed importance sampling. We provide a statistical analysis of its hyperparameters such as the choice of paths, and show the following:

1. First, we establish that using NCE is more asymptotically statistically efficient -- in the sense of how many samples from the intermediate distribution need to be generated -- than the importance sampling estimator, but in the limit of infinitesimal path steps, the difference vanishes.
2. Second, we find that the near-universally used _geometric path_ brings down the estimation error from an exponential to a polynomial function of the parameter distance between the target and proposal distributions.
3. Third, we find that using the recently introduced _arithmetic path_ is exponentially inefficient in its basic form, yet it can be reparameterized to be in some sense optimal. Based on this optimality result, we finally propose a two-stage estimation procedure which first finds an approximation of the optimal (arithmetic) path, then uses it to estimate the normalization constant.

## 2 Background

Importance sampling and NCEThe problem considered here is computing the normalization constant 1, _i.e._ the integral of some unnormalized density \(f_{1}()\) called "target".

Importance sampling and noise-contrastive estimation are two common estimators for that integral which use a random sample drawn from a tractable density \(p_{0}()\) called "proposal"(Table 1, column 3). In fact, they are part of a larger family of Monte-Carlo estimators which can be interpreted as solving a binary classification task, aiming to distinguish between a sample drawn from the proposal and another from the target . These estimators are summarized in Table 1. Each estimator is obtained by minimizing a specific binary classification loss that is identified by a convex function \(()\). For example, minimizing the classification loss identified by \(_{}(x)=x x\) yields the importance sampling estimator. Similarly, \(_{}(x)=- x\) leads to the reverse importance sampling estimator , and \(_{}(x)=x x-(1+x)((1+x)/2)\) to the noise-contrastive estimator.

Annealed estimatorsAnnealing extends the above "binary" setup, by introducing a sequence of \(K+1\) distributions from the proposal to the target (included). The idea will be to draw a sample from _all_ these distributions to estimate the integral of the target \(f_{1}()\).

These intermediate distributions are obtained from a path \((f_{t})_{t}\), defined by interpolating between the proposal \(p_{0}\) and unnormalized target \(f_{1}\): this path is therefore unnormalized. Different interpolation schemes can be chosen. A general one, explained in Figure 1, is to take the \(q\)-mean of the proposal and target . Two values of \(q\) are of particular interest: \(q 0\) defines a a near-universal path , obtained by taking the geometric mean of the target and proposal, while \(q=1\) defines a path obtained by the arithmetic mean.

Once a path is chosen, it can be uniformly2 discretized into a sequence of \(K+1\) unnormalized densities, denoted by \((f_{k/K})_{k[0,K]}\) with corresponding normalizations \((Z_{k/K})_{k[0,K]}\). In practice, samples are drawn from the corresponding normalized densities \((p_{k/K})_{k[0,K]}\) using Markov Chain Monte Carlo (MCMC). This sampling step incurs a computational cost, which is paid in the hope of reducing the variance of the estimation. It is common in the literature [17; 18] to assume _perfect sampling_, meaning the MCMC has converged and produced exact and independent samples from the distributions along the path, which simplifies the analysis.

Estimation errorA measure of "quality" is required to compare different estimation choices, such as whether to anneal and which path to use. Such a measure is given by the Mean Squared Error (MSE), which is generally tractable when written at the first order in the asymptotic limit of a large sample size [24, Eq. 5.20]. These expressions have been derived for estimators obtained by minimizing a classification loss  and are included in Table 1. They measure the "gap" between the proposal and target distributions using statistical divergences. Note also that the estimation error depends on the _normalized_ target density (column 4), while the estimators are computed using the _unnormalized_ target density (column 3). Further details are available in Appendix A.

## 3 Annealed Bregman Estimators of the normalization constant

The question that we try to answer in this paper is: _How should we choose the \(K\) distributions in annealing (the target is fixed), and how are their samples best used?_ To answer this, we will study the error produced by different estimation choices. But first we define the set of estimators for which the analysis is done.

Definition of Annealed Bregman EstimatorsWe now define a new family of estimators, which we call _annealed Bregman estimators (ABE)_; the motivation for this terminology will become clear in the coming paragraphs. We will show that this is a general class of estimators for computing the normalization using a sample drawn from the sequence of \(K+1\) distributions. For ABE, the log

   Name & Loss identified by \(()\) & Estimator \(_{1}\) & MSE \\  IS & \(x x\) & \(_{p_{0}}}{p_{0}}\) & \(_{^{2}}(p_{1},p_{0})\) \\  RevIS & \(- x\) & \((_{p_{1}}}{f_{1}})^{-1}\) & \(_{^{2}}(p_{0},p_{1})\) \\  NCE & \(x x-(1+x)()\) & implicit & \(}{ N}_{}(p_{1},p_{0})}{1-_{}(p_{1},p_{0})}\) \\   

Table 1: Some estimators of the normalization obtained by minimizing a classification loss, and their estimation error in terms of well-known divergences . For details and definitions, see Appendix A.

Figure 1: \(q\)-mean paths between the proposal and target distributions. The geometric and arithmetic paths are obtained as limit cases. Here, the proposal (red) is a standard Gaussian. The target (blue) is a Gaussian mixture with two modes, and same first and second moments as the proposal. The path of distributions interpolates between blue and red.

normalization \( Z_{1}\) is estimated additively along the sequence of distributions

\[}=_{k=0}^{K-1}}{Z_{k/K}} )}+ Z_{0}. \]

Defining the estimation in log-space is analytically convenient, as it is easier to analyze a sum of estimators than a product. Exponentiating the result leads to an estimator of \(Z_{1}\). We naturally extend the binary setup (\(K=1\)) of Chehab et al.  and propose to compute each of the intermediate log-ratios, by solving a classification task between samples drawn from their corresponding densities \(p_{k/K}\) and \(p_{(k+1)/K}\). Each binary classification loss is now identified by a convex function \(_{k}()\) and defined as

\[_{k}(_{k}):=_{ p_{k/K}}[_{k}^{{}^{ }}(r_{k}(;_{k})) r_{k}(;_{k})-_{k}(r_{k} (;_{k}))]-_{ p_{(k+1)/K}}[_{k}^{{}^{} }(r_{k}(;_{k}))], \]

where the regression function \(r_{k}(;_{k})\) is parameterized by the unknown log-ratio \(_{k}\)

\[r_{k}(;_{k})=(-_{k}) f_{(k+1)/K}()/f_{k/K}( {x}). \]

For the true \(_{k}^{*}\), it holds \(_{k}^{*}=(Z_{(k+1)/K}/Z_{k/K})\) and \(r_{k}(;_{k}^{*})=p_{(k+1)/K}()/p_{k/K}()\). The convex functions \((_{k})_{k[0,K-1]}\) which identify the classification losses are called "Bregman" generators, hence ABE. As mentioned above, we assume perfect sampling and allocate the total sample size \(N\) equally among the \(K\) estimators in the sum.

HyperparametersThe annealed Bregman estimator depends on the following hyperparameters: (1) the choice of path \(q\); (2) the number of distributions along that path \(K+1\) (including the proposal and the target); (3) the classification losses identified by the convex functions \((_{k})_{k[0,K-1]}\).

Different combinations of these hyperparameters recover several common estimators of the log-partition function. In binary case of \(K=1\) this includes importance sampling, reverse importance sampling, and noise-contrastive estimation, each obtained for a different choice of the classification loss . To build intuition, consider \(K=2\) so that we add a single intermediate distribution \(p_{1/2}\) to the sequence. Using the importance sampling loss (\(_{0}=x x\)) for the first ratio, and reverse importance sampling (\(_{1}=- x\)) for the second ratio, recovers the _bridge sampling estimator_ as a special case 

\[}=-_{p_{1}}}{f_{1}}+_{p_{0}}}{f_{0}}+ Z_{0}. \]

Alternatively, we can use these classification losses in reverse order: reverse importance sampling (\(_{0}=- x\)) for the first ratio, and importance sampling (\(_{1}=x x\)) for the second ratio, and recover the _umbrella sampling estimator_ also known as the _ratio sampling estimator_

\[}=_{p_{1/2}}}{f_{1/2}}- _{p_{1/2}}}{f_{1/2}}+ Z_{0}. \]

Another option yet, is to use the same classification loss for all ratios. With importance sampling (\(_{k}=x x,\, k 0,K-1\)), we recover the _annealed importance sampling_ estimator 

\[}=_{k=1}^{K}_{ p_{k-1}} }{f_{k-1}}()+ Z_{0}. \]

The family of annealed Bregman estimators is visibly large enough to include many existing estimators, obtained for different hyperparameter choices. This raises the fundamental question of how these hyperparameters should be chosen, in particular in the challenging case where the _target and proposal have little overlap and the data is high dimensional_. To answer this question, we will study the estimation error produced by different hyperparameter choices.

## 4 Statistical analysis of the hyperparameters

We consider a fixed data budget \(N\) and investigate how the remaining hyperparameters are best chosen for statistical efficiency. The starting point for the analysis is that as ABE estimates the normalization in log-space, the estimator is obtained by a sum of independent and asymptotically unbiased estimators  given in Eq. 1 and thus the mean squared errors written in table 1 are additive. (Recall, the independence of these estimators is because new samples are drawn for each estimation task.) Each individual error actually measures an overlap between two consecutive distributions along the path, and annealing integrates these overlaps.

### Classification losses, \(_{k}\)

Given the popularity of annealed importance sampling, we should first ask if the importance sampling loss is really an acceptable default. We recall an important limitation of importance sampling: its estimation error is notoriously sensitive to distribution tails . Without annealing, it is infinite when the target \(p_{1}\) has a heavier tail than the proposal \(p_{0}\). When annealing with a geometric path, for example between two Gaussians with different covariances \(p_{0}=(,)\) and \(p_{1}=(,2\,)\), the geometric path produces Gaussians with increasing variances \(_{t}=(1-t/2)^{-1}\,\) and therefore increasing tails. Hence, the same tail mismatch holds along the path. Note that this concern is a realistic one for natural image data, as the target distribution over images is typically super-Gaussian  while the proposal is usually chosen as Gaussian.

This warrants a better choice for the loss: In the binary setup (\(K=1\)), the NCE loss is optimal [20; 22] and its error can be orders of magnitude less than importance sampling . This optimality result has been extended to a sequence of distributions \(K>1\)[29, eq. 16]. We further show that in the limit of a continuous path, the gap between annealed IS and annealed NCE is closed and we provide their estimation error:

**Theorem 1** (Estimation error and the Fisher-Rao path length) _For a finite value of \(K\), the optimal loss is NCE:_

\[(p_{0},p_{1};q,K,N,_{})(p_{0},p_{ 1};q,K,N,), q,K,N,. \]

_In the limit of \(K\) (such that \(K^{2}/N 0\)), NCE, IS, and revIS converge to the same estimation error, given by the Fisher-Rao path length from the proposal to the target:_

\[(p_{0},p_{1};q,K,N,)=_{0}^{1}I(t)dt+o +o}{N},\{_{ },_{},_{}\}, \]

_where the Fisher-Rao metric \(I(t):=_{ p(,t)}[( p_{t}())^{2}]\) is defined as the Fisher information over the path, using time \(t\) as the parameter._

This is proven in Appendix B. Note that in this theorem, to take limits successively in \(N\) then in \(K\), we assume that \(N\) grows at least as fast as \(K^{2}\). While the NCE estimator requires solving a (potentially non-convex) scalar optimization problem in Eq. 2 and IS does not, this is the price to pay for statistical optimality. In the following, we will keep the optimal NCE loss and will indicate the dependency of the estimation error on \(_{}\) with a subscript, instead. We highlight that our theorems in this paper apply to the MSE in the limit of \(K\): their results hold the same for the IS and RevIS losses by virtue of theorem 1. Just as in the binary case, while the estimator is computed with the _unnormalized_ path of densities (Eq. 2), the estimation error depends on the _normalized_ path of densities (Eq. 8).

### Number of distributions, \(K+1\)

It is known that estimating the normalization constant using plain importance sampling (\(K=1\)) can produce a statistical error that is exponential in the distance between the target and the proposal . We show that in the binary case, NCE also suffers from an estimation error that scales exponentially with the parameter-distance between the target and proposal dimension.

In the following, we consider a proposal \(p_{0}\) and target \(p_{1}\) that are in an exponential family with sufficient statistics \(()\). Note that exponential families have a rich history in statistical literature: they are classic parametric families of distributions  and certain of them have universal approximation capabilities [32; 33]. An exponential family is defined as

\[p(;):=(,()- Z( )) \]where \(Z()=((_{1},()))\) is the partition function. We will consider that the (unnormalized) target density \(f_{1}\) is what we call a _simply unnormalized model_ defined as

\[f_{1}()=((_{1},())) \]

Note that in general, a pdf can be unnormalized in many ways: one can multiply an unnormalized density by any positive function of \(\) and it will still be unnormalized. However, the simple and intuitive case defined above is what we base the analysis below on.

For exponential families, the log-normalization \( Z()\) is a convex function ("log-sum-exp") of the parameter \(\), which implies \(0^{2}_{} Z()\). In our theorems, we use the further assumptions of strong convexity with constant \(M\), and/or smoothness with constant \(L\) (gradient is \(L\)-Liptschitz):

\[^{2}_{} Z() M\, \]

\[^{2}_{} Z() L\, \]

For exponential families, the derivatives of the log partition function yield moments of the sufficient statistics, and the hessian \(^{2}_{} Z()=_{ p} [()]\) is in fact the Fisher matrix. We can interpret our two assumptions: Eq. 11 can be seen as a form of "strong identifiability". Namely, positive-definiteness is required of the Fisher matrix, for the Maximum-Likelihood loss to have a unique minimum: we further assume a lower-bound on the smallest eigenvalue, which can be viewed as a strong identifiability condition. Eq. 12 can be interpreted as a bound on the second-order moments of the distribution \(p(;)\), which is equivalent to the variance in every direction being bounded, which will be the case for parameters in a bounded domain \(\). An example along with the proofs of the following Theorems 2 and 3, are provided in Appendix B.

**Theorem 2** (Exponential error of binary NCE) _Assume the proposal \(p_{0}\) is from the normalized exponential family, while the (unnormalized) target \(f_{1}\) is from the simply unnormalized exponential family (Eq. 10). The log-partition function \( Z()\) is assumed to be strongly convex (Eq. 11). Then in the binary case \(K=1\), the estimation error of NCE is (at least) exponential in the parameter-distance between the proposal and the target:_

\[_{}(p_{0},p_{1};q,K,N) M\|_{1}-_{0}\|^{2}-1+o,K=1 \]

_where \(M\) is the strong convexity constant of \( Z()\)._

Annealing the importance sampling estimator (increasing \(K\)) was proposed in the hope that we can trade the statistical cost in the dimension for a computational cost (number of classification tasks) which could be considered more acceptable. Yet, there is no definitive theory on the ability of annealing to reduce the statistical cost in a general setup . For both importance sampling and noise-contrastive estimation, we next prove that annealing with the near-universally used geometric path brings down the estimation error, from exponential to polynomial in the parameter-distance between the proposal and target. Given that we expect \(\|_{1}-_{0}\|_{2}\) to scale as \(\) with the dimension, using these paths effectively makes annealed estimation amenable to high-dimensional problems. This corroborates empirical  and theoretical  results which suggested in simple cases that annealing with an appropriate path can reduce the estimator error up to several orders of magnitude.

**Theorem 3** (Polynomial error of annealed NCE with a geometric path) _Assume the proposal \(p_{0}\) is from the normalized exponential family, while the (unnormalized) target \(f_{1}\) is from the simply unnormalized exponential family (Eq. 10). The log-partition function \( Z()\) is assumed to be strongly convex and smooth (Eq. 11, Eq. 12). Then in the annealing limit of a continuous path \(K\), the estimation error of annealed NCE with the geometric path is (at most) polynomial in the parameter-distance between the proposal and the target:_

\[_{}(p_{0},p_{1};q,K,N)}{MN}\|_{1}-_{0}\|^{2}+o+o}{N},\ q=0 \]

_where \(M\) and \(L\) are respectively the strong convexity and smoothness constants of \( Z()\)._

To our knowledge, this is the first result building on Gelman and Meng [17, Table 1] and Grosse et al.  which showcases the benefits of annealed estimation for a general target distribution.

We conclude that annealing with the near-universally used geometric path provably benefits noise-contrastive estimation, as well as importance sampling and reverse importance sampling, when the proposal and target distributions have little overlap.

### Path parameter, \(q\) -- geometric vs. arithmetic

Despite the near-universal popularity of the geometric path (\(q 0\)), it is worth asking if there are other simple paths that are more optimal. Interpolating moments of exponential families was shown to outperform the geometric path by Grosse et al. , yet building such a path requires knowing the exponential family of the target. Other paths based on the arithmetic mean (and generalizations) of the target and proposal, were proposed in Masrani et al. , without a definitive theory of the estimation error.

Next, we analyze the error of the arithmetic path. We prove that the arithmetic path (\(q=1\)) does _not_ exhibit the same benefits as the geometric path: in general, its estimation error grows exponentially in the parameter-distance between the target and proposal distributions. However, in the case where an oracle gives us the normalization \(Z_{1}\) to be used only in the construction of the path (we will discuss what this means in practice below), the arithmetic path can be reparameterized so as to bring down the estimation error to polynomial, even constant, in the parameter-distance. We start by the negative result.

**Theorem 4** (Exponential error of annealed NCE with an arithmetic path): _Assume the proposal \(p_{0}\) is from the normalized exponential family, while the (unnormalized) target \(f_{1}\) is from the simply unnormalized exponential family (Eq. 10). The log-partition function \( Z()\) is assumed to be strongly convex (Eq. 11). Consider the annealing limit of a continuous path \(K\) path and a far-away target with large enough \(\|_{1}-_{0}\|>0\). For estimating the log normalization of the (unnormalized) target density \(f_{1}\), the estimation error of annealed NCE with the arithmetic path is (at least) exponential in the parameter-distance between the proposal and the target:_

\[_{}(p_{0},p_{1};q,K,N)>\|_{1}-_{0}\|^{2}+o +o}{N},\ \ \,q=1 \]

_where \(C\) is constant defined in Appendix B.3._

We suggest an intuitive explanation for this negative result. We begin with the observation that the estimation error (Eq. 8) depends on the _normalized_ path of densities. Suppose the target model is rescaled by a constant \(100\), so that the new unnormalized target density is \(f_{1}() 100\) and its new normalization is \(Z_{1} 100\). Looking at table 2, this rescaling does not modify the geometric path of normalized densities, while it does the arithmetic path of normalized densities. Because the estimation error depends on path of normalized densities, this makes the arithmetic choice sensitive to target normalization, even more so as the parameter distance grows and the log-normalization with it, as a strongly convex function of it (Appendix, Eq. 94). This suggests making the arithmetic path of normalized distributions "robust" to the choice of \(Z_{1}\). We will show this can be achieved by re-parameterizing the path in terms of \(Z_{1}\).

We next prove that certain reparameterizations can bring down the error to a polynomial and even constant function of the parameter-distance between the target and proposal. The following theorems may seem purely theoretical, as if necessitating an oracle for \(Z_{1}\), but they will actually lead to an efficient estimation algorithm later.

**Theorem 5** (Polynomial error of annealed NCE with an arithmetic path and oracle): _Assume the same as in Theorem 4, replacing the strong convexity of the log-partition by smoothness (Eq. 12). Additionally, suppose an oracle gives the normalization constant \(Z_{1}\) to be used only in the reparameterization of the arithmetic path with \(t(1-t)}\) (see Table 2). This brings down the estimation error of annealed NCE to (at most) polynomial in the parameter-distance:_

\[_{}(p_{0},p_{1};q,K,N)(2+L\| {}_{1}-_{0}\|^{2})+o+o }{N},\,q=1 \]

_where \(L\) is the smoothness constant of \( Z()\)._

In fact, supposing we have (oracle) access to the normalizing constant \(Z_{1}\), the arithmetic path can even be reparameterized such that it is the optimal path in a certain limit. We next prove such optimality in the limits of a continuous path \(K\) and "far-away" target and proposal :

**Theorem 6** (Constant error of annealed NCE with an arithmetic path and oracle): _Consider the limits of a continuous annealing path \(K\), and of a target distribution whose overlap with the proposalgoes to zero. Namely, consider the quantities:_

\[^{{}^{}}() :=()p_{1}()}  :=_{^{D}}^{{}^{}}()d \] \[^{{}^{}}() :=^{}}()-( t)p_{t}^{ }()}{p_{t}^{}()} ^{{}^{}} :=_{^{D}}_{0}^{1}^{{}^{}}( )1+_{t}p_{t}^{}() ^{2}}{p_{t}^{}()}dtd. \]

_Assume \(_{^{D}}^{}(x) 0,_{ ^{D}}^{}(x) 0, 0, ^{} 0\), and consider the distributions \(p_{t}^{}\) and \(p_{t}^{}\) as defined in Table 2._

_Then, the optimal annealing path convergences pointwise to an arithmetic path reparameterized trigonometrically with \(t()+Z_{1}(1-^{2}())}\) and the estimation error tends to the optimal estimation error (which is constant with respect to the parameter-distance):_

\[_{}(p_{0},p_{1};q,K,N)=^{2}+O ^{}}}{N}+o+o}{N}\,q=1. \]

By way of remarks, we note that the assumptions that the quantities \(^{}(x),^{}(x),,^{ }\) go to 0 are mutually incomparable (i.e. none of them implies the others). For example, \(_{^{D}}^{}()\) and \(\) going 0 require that the function \((x)p_{1}(x)}\) goes to 0 in the \(L_{}\) and \(L_{1}\) sense, respectively -- and these two norms are not equivalent in the Lebesgue measure.

Two-step estimationThus, we see that, perhaps unsurprisingly, the "optimal" mixture weights in the space of unnormalized densities depends on the true \(Z_{1}\): however, this dependency is simple. We propose a two-step estimation method: first, \(Z_{1}\) is pre-estimated, for example using the geometric path; second, the estimate of \(Z_{1}\) is plugged into the "oracle" or "oracle-trig" weight of the arithmetic path (table 2, column 2), and which is used to obtain a second estimation of \(Z_{1}\). Note that pre-estimating a problematic (hyper)parameter, here \(Z_{1}\), has proved beneficial to reduce the estimation error of NCE in a related context .

## 5 Numerical results

We now present numerical evidence for our theory and validate our two-step estimators. Importantly, we do _not_ claim to achieve state of the art in terms of practical evaluation of the normalization constants; our goal is to support our theoretical analysis. We follow the evaluation methods of importance sampling literature  and evaluate our methods on synthetic Gaussians. This setup is specially convenient for validating our theory: the optimal estimation error can conveniently be computed in closed-form, so too can the geometric and arithmetic paths which avoids a sampling error from MCMC algorithms. These derivations are included in the Appendix B. We specifically consider the high-dimensional setting, where the computation of the determinant of a high-dimensional (covariance) matrix which appears in the normalization of a Gaussian, can in fact be challenging .

   Path name & Unnormalized density & Normalized density & Error \\  Geometric & \(f_{t}()=p_{0}()^{1-t}f_{1}()^{t}\) & \(p_{t}() p_{0}()^{1-t}p_{1}()^{t}\) & poly \\  Arithmetic & \(f_{t}()=(1-w_{t})p_{0}()+w_{t}f_{1}()\) & \(p_{t}()=(1-_{t})p_{0}()+_{t}p_{1}()\) & \\ vanilla & \(w_{t}=t\) & \(_{t}=}{(1-t)+tZ_{1}}\) & exp \\ oracle & \(w_{t}=(1-t)}\) & \(_{t}=t\) & poly \\ oracle-trig & \(w_{t}=()}{^{2}( )+Z_{1}^{2}()}\) & \(_{t}=^{2}()\) & const \\   

Table 2: Geometric and arithmetic paths, defined in the space of unnormalized densities (second column); “oracle” and “oracle-trig” are reparameterizations of the arithmetic path which depend on the true normalization \(Z_{1}\). The corresponding normalized densities (third column) produce an estimation error (fourth column) which we quantify.

Numerical MethodsThe proposal distribution is always a standard Gaussian, while the target differs by the second moment: \(p_{1}=(,2\,)\) in Figure 2, \(p_{1}=(,0.25\,)\) in Figure 2(b) and \(p_{1}=(,^{2}\,)\) in Figure 2(a), where the target variance decreases as \((i)=i^{-1}\) so that the (natural) parameter distance grows linearly [34, Part II-4]. We use a sample size of \(N=50000\) points, and, unless otherwise mentioned, \(K+1=10\) distributions from the annealing paths and the dimensionality is \(50\). To compute an estimator of the normalization constant using the non-convex NCE loss, we used a non-linear conjugate gradient scheme implemented in Scipy . We chose the conjugate-gradient algorithm as it is deterministic (no residual variance like in SGD). The empirical Mean-Squared Error was computed over 100 random seeds, parallelized over 100 CPUs. The longest experiment was for Figure 2(b) and took 7 wall-clock hours to complete. For the two-step estimators ("two-step" and "two-step (trig)"), a pre-estimate of the normalization was first computed using the geometric path with \(10\) distributions. Then, this estimate was used to to re-parameterize an arithmetic path with \(10\) distributions which produced the second estimate.

ResultsFigure 2 numerically supports the optimality of the NCE loss for a finite \(K\) (here, \(K=2\) so three distributions are used) proven in Theorem 1; note that the proposal distribution was normalized for this experiment, so that the log normalizing constant is zero. Figure 3 validates our main results for annealing paths. It shows how the estimation error scales with the proposal and target distributions growing apart, either with the parameter-distance in Figure 2(a) or with the dimensionality in Figure 2(b). Using no annealing path (\(K=1\)) produces an estimation error which grows linearly in log space; this numerically supports the exponential growth predicted by Theorem 2. Meanwhile, annealing (\(K\)) sets the estimation error on different trends, depending on the choice of path. Choosing the geometric path brings the growth down to sub-exponential, as predicted by Theorem 3, while choosing the (basic) arithmetic path does not as in Theorem 4. To alleviate this, our two-step estimation methods consist in reparameterizing the arithmetic path so that it actually does bring down the estimation error. In fact, our two-step estimators in table 2 empirically approach the optimal estimation error in Figure 3. While this requires more computation, it has the appeal of making the estimation error _constant_ with respect to the parameter-distance between the target and proposal distributions. Practically, this means that in Figure 2(a), regular Noise-Contrastive Estimation (black, full line) fails when the parameter-distance between the target and proposal distributions is higher than \(20\), while our two-step estimators remain optimal.

We next explain interesting observations in Figure 3 which are actually coherent with our theory. First, in Figure 2(a), the "two-step (trig)" estimator is only optimal when the parameter-distance between the target and proposal distributions is larger than \(10\). This is because the optimality of this two-step estimator was derived in Theorem 6 conditionally on non-overlapping distributions, here achieved by a large parameter-distance. Second, in both Figures 2(a) and 2(b), the "two-step" estimator empirically achieves the optimal estimation error that was predicted for the "two-step (trig)" estimator. This suggests our polynomial upper bound from Theorem 5 may be loose in certain cases. This further explains why, in Figure 2(a), the arithmetic path is near-optimal for a single value of the parameter-distance. At this value of \(20\), the partition function happens to be equal to one \(Z(_{1})=1\), so that the arithmetic path is effectively the same as the "two-step" estimator.

## 6 Related work and Limitations

Previous work has mainly focused on annealed importance sampling [18; 39], which is a special case of our annealed Bregman estimator. They have evaluated the merits of different paths empirically, using an approximation of the estimation error called Effective Sample Size (ESS) and the consistency

Figure 2: Optimality of the NCE loss, using the geometric path with \(K=2\). NCE has the smallest deviation from zero, the true value of the log normalizing constant.

gap. In our analysis, we consider consistent estimators and derive and optimize the exact estimation error of the optimal Noise-Contrastive Estimation. Liu et al.  considered the NCE estimate for \(Z\) (not \( Z\)) with the name "discriminance sampling", and annealed the estimator using an extended state-space construction similar to Neal . Their analysis of the estimation error is relevant but does not deal with hyperparameters other than the classification loss. Other annealed estimators also include annealed NCE as a special case but with little theory on its estimation error [17; 40; 29].

We made the common assumption of perfect sampling [17; 18; 41; 42] in order to make the estimation error tractable and obtain practical guidelines to reduce it. We note however, that this leaves a gap to bridge with a practical setup where the sampling error cannot be ignored; in fact, annealed importance sampling  was originally proposed such that the samples can be obtained from a Markov Chain that has not converged. In this original formulation, AIS is a special case of a larger framework called Sequential Monte Carlo (SMC)  in which the path of distributions is implicitly defined (by Markov transitions), sometimes even "on the go" . Yet even within that theory, it seems that analyzing the estimation error for an inexplicit path of distributions is challenging [44, Eq. 38]. In particular, samples from MCMC will typically follow marginal distributions that are not analytically tractable, thus the stronger assumption of "perfect sampling" is often used to make estimation error depend explicitly on the path of distributions [45, Eq. 3.2]. Even then, results on how the estimation error scales with dimensionality are heuristic  or limited by assumptions such as an essentially log-concave path of distributions or a factorial target distribution .

It might also be argued that the limit of almost no overlap between proposal and target, which we use a lot, is unrealistic. To see why it can be realistic, consider the case of natural image data. A typical proposal is Gaussian, since nothing much more sophisticated is tractable in high dimensions. However, there is almost no overlap between Gaussian data and natural images, which is seen in the fact that a human observer can effortlessly discriminate between the two.

More generally, many methods based on "annealing" were developed to deal with sampling issues. In fact, the path costs for two such methods, parallel tempering [46, eq. 17] and tempered transitions [47; eq. 18], can be written with a sum of f-divergences which in the limit of a continuous path is the same cost function as in our Theorem 1. This suggests our results may apply to these methods as well.

## 7 Conclusion

We defined a class of estimators of the normalization constant, annealed Bregman estimation (ABE), which relies on a sampling phase from a path of distributions, and an estimation phase where these samples are used to estimate the log-normalization of the target distribution. Our results suggest a number of simple recommendations regarding hyperparameter choices in annealing. First, if the path has very few intermediate distributions, it is better to choose NCE due to its statistical optimality (Theorem 1). If however, the path has many intermediate distributions and approaches the annealing limit, then IS enjoys the same statistical optimality as NCE but has the advantage of its computational simplicity. Annealing can always provide substantial benefits (Theorem 2). Moreover, if we have a reasonable a priori estimate of \(Z_{1}\), the arithmetic path achieves very low error (Theorem 5) -- sometimes even approaching optimality (Theorem 6). On the other hand, even absent an initial estimate of \(Z_{1}\), the geometric path can exponentially reduce the estimation error compared with no annealing (Theorems 2 and 3).

Figure 3: Estimation error as the target and proposal distributions grow apart. Without annealing, the error is exponential in the parameter distance (diagonal in log-scale). Annealing with the geometric path and our two-step methods brings down the error to slower growth, as predicted by our theorems.

AcknowledgementsThis work was supported by the French ANR-20-CHIA-0016. Aapo Hyvarinen was supported by funding from the Academy of Finland and a Fellowship from CIFAR. Andrej Risteski was supported in part by NSF awards IIS-2211907, CCF-2238523, and an Amazon Research Award.