# Adversarial Model for Offline Reinforcement Learning

Mohak Bhardwaj

University of Washington

mohakb@cs.washington.edu

&Tengyang Xie

Microsoft Research & UW-Madison

tx@cs.wisc.edu

&Byron Boots

University of Washington

bboots@cs.washington.edu

&Nan Jiang

UIUC

nanjiang@illinois.edu

&Ching-An Cheng

Microsoft Research, Redmond

chinganc@microsoft.com

Equal contributionOpen source code is available at: [https://sites.google.com/view/armorofflinerl/](https://sites.google.com/view/armorofflinerl/).

###### Abstract

We propose a novel model-based offline Reinforcement Learning (RL) framework, called Adversarial Model for Offline Reinforcement Learning (ARMOR), which can robustly learn policies to improve upon an arbitrary reference policy regardless of data coverage. ARMOR is designed to optimize policies for the worst-case performance relative to the reference policy through adversarially training a Markov decision process model. In theory, we prove that ARMOR, with a well-tuned hyperparameter, can compete with the best policy within data coverage when the reference policy is supported by the data. At the same time, ARMOR is robust to hyperparameter choices: the policy learned by ARMOR, with _any_ admissible hyperparameter, would never degrade the performance of the reference policy, even when the reference policy is not covered by the dataset. To validate these properties in practice, we design a scalable implementation of ARMOR, which by adversarial training, can optimize policies without using model ensembles in contrast to typical model-based methods. We show that ARMOR achieves competent performance with both state-of-the-art offline model-free and model-based RL algorithms and can robustly improve the reference policy over various hyperparameter choices.2

## 1 Introduction

Offline reinforcement learning (RL) is a technique for learning decision-making policies from logged data (Lange et al., 2012; Levine et al., 2020; Jin et al., 2021; Xie et al., 2021). In comparison with alternate learning techniques, such as off-policy RL and imitation learning (IL), offline RL reduces the data assumption needed to learn good policies and does not require collecting new data. Theoretically, offline RL can learn the best policy that the given data can explain: as long as the offline data includes the scenarios encountered by a near-optimal policy, an offline RL algorithm can learn such a near-optimal policy, even when the data is collected by highly sub-optimal policies and/or is not diverse. Such robustness to data coverage makes offline RL a promising technique for solving real-world problems, as collecting diverse or expert-quality data in practice is often expensive or simply infeasible.

The fundamental principle behind offline RL is the concept of pessimism, which considers worst-case outcomes for scenarios without data. In algorithms, this is realized by (explicitly or implicitly) constructing performance lower bounds in policy learning which penalizes uncertain actions.

Various designs have been proposed to construct such lower bounds, including behavior regularization (Fujimoto et al., 2019; Kumar et al., 2019; Wu et al., 2019; Laroche et al., 2019; Fujimoto and Gu, 2021), point-wise pessimism based on negative bonuses or truncation (Kidambi et al., 2020; Jin et al., 2021), value penalty (Kumar et al., 2020; Yu et al., 2020), or two-player games (Xie et al., 2021; Uehara and Sun, 2021; Cheng et al., 2022). Conceptually, the tighter the lower bound is, the better the learned policy would perform; see a detailed discussion of related work in Appendix C.

Despite these advances, offline RL still has not been widely adopted to build learning-based decision systems beyond academic research. One important factor we posit is the issue of performance degradation: Usually, the systems we apply RL to have currently running policies, such as an engineered autonomous driving rule or a heuristic-based system for diagnosis, and the goal of applying a learning algorithm is often to further improve upon these baseline _reference policies_. As a result, it is imperative that the policy learned by the algorithm does not degrade the base performance. This criterion is especially critical for applications where poor decision outcomes cannot be tolerated.

However, running an offline RL algorithm based on pessimism, in general, is not free from performance degradation. While there have been algorithms with policy improvement guarantees (Laroche et al., 2019; Fujimoto et al., 2019; Kumar et al., 2020; Fujimoto and Gu, 2021; Cheng et al., 2022), such guarantees apply only to the behavior policy that collects the data, which might not necessarily be the reference policy. In fact, quite often these two policies are different. For example, in robotic manipulation, it is common to have a dataset of activities different from the target task. In such a scenario, comparing against the behavior policy is meaningless, as these policies do not have meaningful performance in the target task.

In this work, we propose a novel model-based offline RL framework, called \(}\)dvestigal \(}\) for \(}\)Rinforcement Learning (ARMOR), which can robustly learn policies that improve upon an arbitrary reference policy by adversarially training a Markov decision process (MDP) model, regardless of the data quality. ARMOR is designed based on the concept of relative pessimism (Cheng et al., 2022), which aims to optimize for the worst-case relative performance over uncertainty. In theory, we prove that, owing to relative pessimism, the ARMOR policy never degrades the performance of the reference policy for a range of hyperparameters which is given beforehand, a property known as Robust Policy Improvement (RPI) (Cheng et al., 2022). In addition, when the right hyperparameter is chosen, and the reference policy is covered by the data, we prove that the ARMOR policy can also compete with any policy covered by the data in an absolute sense. To our knowledge, RPI property of offline RL has so far been limited to comparing against the data collection policy (Fujimoto et al., 2019; Kumar et al., 2019; Wu et al., 2019; Laroche et al., 2019; Fujimoto and Gu, 2021; Cheng et al., 2022). In ARMOR, by adversarially training an MDP model, we extend the technique of relative pessimism to achieve RPI with _arbitrary_ reference policies, regardless of whether they collected the data or not (Fig. 1).

In addition to theory, we design a scalable deep-learning implementation of ARMOR to validate these claims that jointly trains an MDP model and the state-action value function to minimize the estimated performance difference between the policy and the reference using model-based rollouts. Our implementation achieves state-of-the-art (SoTA) performance on D4RL benchmarks (Fu et al., 2020), while using only a _single_ model (in contrast to ensembles used in existing model-based offline RL works). This makes ARMOR a better framework for using high-capacity world models (e.g.(Hafner et al., 2023)) for which building an ensemble is too expensive. We also empirically validate the RPI property of our implementation.

Figure 1: Robust Policy Improvement: ARMOR can improve performance over the reference policy (REF) over a broad range of pessimism hyperparameter (purple) regardless of data coverage. ORL denotes best offline RL policy without using the reference policy, and reference is obtained by behavior cloning on expert dataset.

## 2 Preliminaries

Markov Decision ProcessWe consider learning in the setup of an infinite-horizon discounted Markov Decision Process (MDP). An MDP \(M\) is defined by the tuple \(,,P_{M},R_{M},\), where \(\) is the state space, \(\) is the action space, \(P_{M}:()\) is the transition dynamics, \(R_{M}:\) is a scalar reward function and \(\) is the discount factor. A policy \(\) is a mapping from \(\) to a distribution on \(\). For \(\), we let \(d_{M}^{}(s,a)\) denote the discounted state-action distribution obtained by running \(\) on \(M\) from an initial state distribution \(d_{0}\), i.e \(d_{M}^{}(s,a)=(1-)}_{,M}[_{t=0}^{ }^{t}1(s_{t}=s,a_{t}=a)]\). Let \(J_{M}()=}_{,M}[_{t=m}^{}^{t }r_{t}]\) be the expected discounted return of policy \(\) on \(M\) starting from \(d_{0}\), where \(r_{t}=R_{M}(s_{t},a_{t})\). We define the value function as \(V_{M}^{}(s)=}_{,M}[_{t=0}^{} ^{t}r_{t}|s_{0}=s]\), and the state-action value function (i.e., Q-function) as \(Q_{M}^{}(s,a)=}_{,M}[_{t=0}^{} ^{t}r_{t}|s_{0}=s,s_{0}=a]\). By this definition, we note \(J_{M}()=}_{d_{0}}[V_{M}^{}(s)]=}_{d_{0},}[Q_{M}^{}(s,a)]\). We use \([0,V_{}]\) to denote the range of value functions, where \(V_{} 1\). We denote the ground truth MDP as \(M^{}\), and \(J=J_{M^{}}\)

Offline RLThe aim of offline RL is to find the policy that maximizes \(J()\), while using a fixed dataset \(\) collected by a behavior policy \(\). We assume the dataset \(\) consists of \(\{(s_{n},a_{n},r_{n},s_{n+1})\}_{n=1}^{N}\), where \((s_{n},a_{n})\) is sampled from \(d_{M^{}}^{}\) and \(r_{n},s_{n+1}\) follow \(M^{}\); for simplicity, we also write \((s,a)=d_{M^{}}^{}(s,a)\).

We assume that the learner has access to a Markovian policy class \(\) and an MDP model class \(\).

**Assumption 1** (Realizability).: _We assume the ground truth model \(M^{}\) is in the model class \(\)._

In addition, we assume that we are provided a reference policy \(_{}\). In practice, such a reference policy represents a baseline whose performance we want to improve with offline RL and data.

**Assumption 2** (Reference policy).: _We assume access to a reference policy \(_{}\), which can be queried at any state. We assume \(_{}\) is realizable, i.e., \(_{}\)._

If \(_{}\) is not provided, we can still run ARMOR as a typical offline RL algorithm, by first performing behavior cloning on the data and setting the cloned policy as \(_{}\). In this case, ARMOR has RPI with respect to the behavior policy.

Robust Policy ImprovementRPI is a notion introduced in Cheng et al. (2022), which means that the offline algorithm can learn to improve over the behavior policy, using hyperparameters within a known set. Algorithms with RPI are more robust to hyperparameter choices, and they are often derived from the principle of relative pessimism (Cheng et al., 2022). In this work, we extend the RPI concept to compare with an arbitrary reference (or baseline) policy, which can be different from the behavior policy and can take actions outside data support.

## 3 Adversarial Model for Offline Reinforcement Learning (ARMOR)

ARMOR is a model-based offline RL algorithm designed with relative pessimism. The goal of ARMOR is to find a policy \(\) that maximizes the performance difference \(J()-J(_{})\) to a given reference policy \(_{}\), while accounting for the uncertainty due to limited data coverage. ARMOR achieves this by solving a two-player game between a learner policy and an adversary MDP model:

\[=*{argmax}_{}_{M_{ }}J_{M}()-J_{M}(_{}) \]

based on a version space of MDP models

\[_{}=\{M:_{}(M)-_{M^{ }}_{}(M^{})\}, \]

where we define the model fitting loss as

\[_{}(M)-_{} P_{M}(s^{ } s,a)+(s,a)-r)^{2}}}{{V_{}^{2}}} \]

and \( 0\) is a bound on statistical errors such that \(M^{}_{}\). In this two-player game, ARMOR is optimizing a lower bound of the relative performance \(J()-J(_{})\). This is due to the construction that \(M^{}_{}\), which ensures \(_{M_{}}J_{M}()-J_{M}(_{}) J_{M^{ }}()-J_{M^{}}(_{})\).

One interesting property that follows from optimizing the relative performance lower bound is that \(\) is guaranteed to always be no worse than \(_{}\), for a wide range of \(\) and regardless of the relationship between \(_{}\) and the data \(\).

**Proposition 1**.: _For any \(\) large enough such that \(M^{}_{}\), it holds that \(J() J(_{})\)._

This fact can be easily reasoned: Since \(_{}\), we have \(_{}_{M_{}}J_{M}()-J_{M}(_{})_{M_{}}J_{M}(_{})-J_{M}(_{ })=0\). In other words, ARMOR achieves the RPI property with respect to any reference policy \(_{}\) and offline dataset \(\).

This RPI property of ARMOR is stronger than the RPI property in the literature. In comparison, previous algorithms with RPI (Fujimoto et al., 2019; Kumar et al., 2019; Wu et al., 2019; Laroche et al., 2019; Fujimoto and Gu, 2021; Cheng et al., 2022) are only guaranteed to be no worse than the behavior policy that collected the data. In Section 3.2, we will also show that when \(\) is set appropriately, ARMOR can provably compete with the best data covered policy as well, as prior offline RL works (e.g., Xie et al., 2021; Uehara and Sun, 2021; Cheng et al., 2022).

### An Illustrative Toy Example

Why does ARMOR have the RPI property, even when the reference policy \(_{}\) is not covered by the data \(\)? While we will give a formal analysis soon in Section 3.2, here we provide some intuitions as to why this is possible. First, notice that ARMOR has access to the reference policy \(_{}\). Therefore, a trivial way to achieve RPI with respect to \(_{}\) is to just output \(_{}\). However, this naive algorithm while never degrading \(_{}\) cannot learn to improve from \(_{}\). ARMOR achieves these two features simultaneously by _1)_ learning an MDP Model, and _2)_ adversarially training this MDP model to minimize the relative performance difference to \(_{}\) during policy optimization.

We illustrate this by a one-dimensional discrete MDP example with five possible states as shown in Figure 2. The dynamic is deterministic, and the agent always starts in the center cell. The agent receives a lower reward of 0.1 in the left-most state and a high reward of 1.0 upon visiting the rightmost state. Say, the agent only has access to a dataset from a sub-optimal policy that always takes the left action to receive the 0.1 reward. Further, let's say we have access to a reference policy that demonstrates optimal behavior on the true MDP by always visiting the right-most state. However, it is unknown a priori that the reference policy is optimal. In such a case, typical offline RL methods can only recover the sub-optimal policy from the dataset as it is the best-covered policy in the data.

ARMOR can learn to recover the expert reference policy in this example by performing rollouts with the adversarially trained MDP model. From the realizability assumption (Assumption 1), we know that the version space of models contains the true model (i.e., \(M^{}_{}\)). The adversary can then choose a model from this version space where the reference policy \(_{}\) maximally outperforms the learner. In this toy example, the model selected by the adversary would be the one allowing the expert policy to reach the right-most state. Now, optimizing relative performance difference with respect to this model will ensure that the learner can recover the expert behavior, since the only way for the learner to stay competitive with the reference policy is to mimic the reference policy in the region outside data support. In other words, the reason why ARMOR has RPI to \(_{}\) is that

Figure 2: A toy MDP illustrating the RPI property of ARMOR. (Top) The true MDP has deterministic dynamics where taking the left (\(a_{l}\)) or right (\(a_{r}\)) actions takes the agent to corresponding states; start state is in yellow. The suboptimal behavior policy visits only the left part of the state space, and the reference policy demonstrates optimal behavior by always choosing \(a_{r}\). (Bottom) A subset of possible data-consistent MDP models in the version space. The adversary always chooses the MDP that makes the reference maximally outperform the learner. In response, the learner will learn to mimic the reference outside data support to be competitive.

its adversarial model training procedure can augment the original offline data with new states and actions that would cover those generated by running the reference policy.3

### Theoretical Analysis

Now we make the above discussions formal and give theoretical guarantees on ARMOR's absolute performance and RPI property. To this end, we introduce a single-policy concentrability coefficient, which measures the distribution shift between a policy \(\) and the data distribution \(\).

**Definition 1** (Generalized Single-policy Concentrability).: _We define the generalized single-policy concentrability for policy \(\), model class \(\) and offline data distribution \(\) as \(_{}()_{M}_{}[^{}(M)]}{_{}[^{}(M)]},\) where \(^{}(M)=D_{}(P_{M}( s,a),P_{M^{} }( s,a))^{2}+{(R_{M}(s,a)-R^{}(s,a))^{2}(s,a)-R^{}(s,a)}}.-1.2pt}{V_{}^{2}}}\)._

Note that \(_{}()\) is always upper bounded by the standard single-policy concentrability coefficient \(\|d^{}/\|_{}\)(e.g., Jin et al., 2021; Rashidinejad et al., 2021; Xie et al., 2021b), but it can be smaller in general with model class \(\). It can also be viewed as a model-based analog of the one in Xie et al. (2021a). A detailed discussion around \(_{}()\) can be found in Uehara and Sun (2021).

First, we present the absolute performance guarantee of ARMOR, which holds for a well-tuned \(\).

**Theorem 2** (Absolute performance).: _Under Assumption 1, there is an absolute constant \(c\) such that for any \((0,1]\), if we set \(=c((/))\) in Eq. (2), then for any reference policy \(_{}\) and comparator policy \(^{}\), with probability \(1-\), the policy \(\) learned by ARMOR in Eq. (1) satisfies that \(J(^{})-J()\) is upper bounded by_

\[((_{}(^{})}+_{}(_{})})}} {1-}/)}{n}} ).\]

Roughly speaking, Theorem 2 shows that \(\) learned by ARMOR can compete with any policy \(^{}\) with a large enough dataset, as long as the offline data \(\) has good coverage on \(^{}\) (good coverage over \(_{}\) can be automatically satisfied if we simply choose \(_{}=\), which yields \(_{}(_{})=1\)). Compared to the closest model-based offline RL work (Uehara and Sun, 2021), if we set \(_{}=\) (data collection policy), Theorem 2 leads to almost the same guarantee as Uehara and Sun (2021, Theorem 1) up to constant factors.

In addition to absolute performance, below we show that, under Assumptions 1 and 2, ARMOR has the RPI property to \(_{}\): it always improves over \(J(_{})\) for _a wide range of parameter \(\)_. Compared with the model-free ATAC algorithm in Cheng et al. (2022, Proposition 6), the threshold for \(\) in Theorem 3 does not depend on sample size \(N\) due to the model-based nature of ARMOR.

**Theorem 3** (Robust strong policy improvement).: _Under Assumptions 1 and 2, there exists an absolute constant \(c\) such that for any \((0,1]\), if: i) \( c((/))\) in Eq. (2); ii) \(_{}\), then with probability \(1-\), the policy \(\) learned by ARMOR in Eq. (1) satisfies \(J() J(_{})\)._

The detailed proofs of Theorems 2 and 3, as well as the discussion on how to relax Assumptions 1 and 2 to the misspecified model and policy classes are deferred to Appendix A.

## 4 Practical Implementation

In this section, we present a scalable implementation of ARMOR (Algorithm 1) that approximately solves the two-player game in Eq. (1). We first describe the overall design principle and then the algorithmic details.

### A Model-based Actor Critic Approach

For computational efficiency, we take a model-based actor critic approach and solve a regularized version of Eq. (1). We construct this regularized version by relaxing the constraint in the inner minimization of Eq.1 to a regularization term and introducing an additional critic function. To clearly elaborate this, we first present the regularized objective in its complete form, and subsequently derive it from Eq.1.

Let \(:\{f:[0,V_{}]\}\) be a class of critic functions. The regularized objective is given as

\[*{argmax}_{}\ _{d^{^{}}_{M}}(,f) \]

where \(_{}(M)=_{}- P_{M}(s^{}\ \ s,a)+(R_{M}(s,a)-r^{2})/V_{}^{2}\) is the model-fitting error, \(_{d^{^{}}_{M}}(,f):=_{d^{^{}}_{M}}[f(s,)-f(s,_{})]\) is equal to the performance difference \((1-)(J_{M}()-J_{M}(_{}))\), \(_{_{_{}},}(,f,M)\) denotes the squared Bellman error on the distribution \(_{_{},}\) that denotes the distribution generated by first running \(_{}\) and then rolling out \(\) in \(M\) (with a switching time sampled from a geometric distribution of \(\)), and \(,\) act as the Lagrange multipliers.

This regularized formulation in Eq.6 can be derived as follows. Assuming \(Q_{M}^{}\), and using the facts that \(J_{M}()=_{d_{0}}[Q_{M}^{}(s,)]\) and the Bellman equation \(Q_{M}^{}(s,a)=r_{M}(s,a)+_{s^{} P_{M}(s,a)}[Q_{M} ^{}(s^{},)]\), we can rewrite Eq.1 as

\[_{}_{M,f}_{d^{ ^{}}_{M}}[f(s,)-f(s,_{})]. \]

\[_{}(M)+_{M^{} }_{}(M^{})\]

\[ s,a(_{_{},}), f(s,a)=r_{M}(s,a )+_{s^{} P_{M}(s,a)}[f(s^{},)]\]

We then convert the constraints in Eq.7 into regularization terms in the inner minimization by introducing Lagrange multipliers (\(\), \(\)), following (Xie et al., 2021a; Cheng et al., 2022), and drop the constants not affected by \(M,f,\), which results in Eq.6.

### Algorithm Details

Algorithm 1 is an iterative solver for approximating the solution to Eq. (6). Here we further approximate \(d_{M}^{_{}}\) and \(_{_{},}\) in Eq. (6) using samples from the state-action buffer \(_{}\). We want ensure that \(_{}\) has a larger coverage than both \(d_{M}^{_{}}\) and \(_{_{},}\). We do so heuristically, by constructing the model replay buffer \(_{}\) through repeatedly rolling out \(\) and \(_{}\) with the adversarially trained MDP model \(M\), such that \(_{}\) contains a diverse training set of state-action tuples.

Specifically, the algorithm takes as input an offline dataset \(_{}\), a policy \(\), an MDP model \(M\) and two critic networks \(f_{1},f_{2}\). At every iteration, the algorithm proceeds in two stages. First, the adversary is optimized to find a data-consistent model that minimizes the performance difference with the reference policy. We sample mini-batches of only states and actions \(_{}^{}\) and \(_{}^{}\) from the real and model-generated datasets respectively (creftype 4). The MDP model \(M\) is queried on these mini-batches to generate next-state and reward predictions. The adversary then updates the model and Q-functions (creftype 5) using the gradient of the loss described in Eq. (4), where

\[_{_{M}}(f,,_{}) _{_{M}}[f(s,(s))-f(s,_{}(s)]\] \[_{_{M}}^{w}(f,M,)(1-w)_{}^{td}(f,f,M,)+w_{}^{td}(f,,M,)\] \[_{_{}^{}}(M) _{_{}^{}}[- P_{M}(s^{}  s,a)+((s,a)-r)^{2}}}{{V_{}^{2}}}]\]

\(_{_{M}}\) is the pessimistic loss term that forces the \(f\) to predict a lower value for the learner than the reference on the sampled states. \(_{_{M}}^{w}\) is the Bellman surrogate to encourage the Q-functions to be consistent with the model-generated data \(_{M}\). We use the double Q residual algorithm loss similar to Cheng et al. (2022), which is defined as a convex combination of the temporal difference losses with respect to the critic and the delayed target networks, \(_{}^{td}(f,f^{},M,)_{ }[(f(s,a)-r- f^{}(s^{},))^{2} ]\). \(_{}(M)\) is the model-fitting loss that ensures the model is data-consistent. \(\) and \(\) control the effect of the pessimistic loss, by constraining Q-functions and models the adversary can choose. Once the adversary is updated, we update the policy (creftype 6) to maximize the pessimistic loss as defined in Eq. (5). Similar to Cheng et al. (2022), we choose one Q-function and a slower learning rate for the policy updates (\(_{}_{}\)).

We remark that \(_{_{M}}^{w}\) not only affects \(f_{1},f_{2}\), but also \(M\), i.e., it forces the model to generate transitions where the Q-function is Bellman consistent. This allows the pessimistic loss to indirectly affect the model learning, thus making the model adversarial. Consider the special case where \(=0\) in the loss of creftype 4. The model here is no longer forced to be data consistent, and the adversary can now freely update the model via \(_{_{M}}^{w}\) such that the Q-function is always Bellman consistent. As a consequence, the algorithm becomes equivalent to IL on the model-generated states. We empirically study this behavior in our experiments (creftype 5).

Lines 7 and 8 describe our model-based rollout procedure. We incrementally rollout both \(\) and \(_{}\) from states in \(_{}^{}\) for a horizon \(H\), and add the generated transitions to \(_{}\). The aim of this strategy is to generate a distribution with large coverage for training the adversary and policy, and we discuss this in detail in the next section.

Finally, it is important to note the fact that neither the pessimistic nor the Bellman surrogate losses uses the real transitions; hence our algorithm is completely model-based from a statistical point of view, that the value function \(f\) is solely an intermediate variable that helps in-model optimization and not directly fit from data.

## 5 Experiments

We test the efficacy of ARMOR on two major fronts: (1) performance comparison to existing offline RL algorithms, and (2) robust policy improvement over a reference policy that is not covered by the dataset, a novel setting that is not applicable to existing works4. We use the D4RL (Fu et al., 2020) continuous control benchmarks datasets for all our experiments and the code will be made public.

Experimental Setup:We parameterize \(,f_{1},f_{2}\) and \(M\) using feedforward neural networks, and set \(_{}=5e-4\), \(_{}=5e-7\), \(w=0.5\) similar to Cheng et al. (2022). In all our experiments, we vary only the \(\) and \(\) parameters which control the amount of pessimism; others are fixed. Importantly, we set the rollout horizon to be the max episode horizon defined in the environment.

The dynamics model is pre-trained for 100k steps using model-fitting loss on the offline dataset. ARMOR is then trained for 1M steps on each dataset. Refer to Appendix F for more details.

### Comparison with Offline RL Baselines

By setting the reference policy to the behavior-cloned policy on the offline dataset, we can use ARMOR as a standard offline RL algorithm. Table 1 shows a comparison of the performance of ARMOR against SoTA model-free and model-based offline RL baselines. In the former category, we consider ATAC (Cheng et al., 2022), CQL (Kumar et al., 2020) and IQL (Kostrikov et al., 2021), and for the latter we consider MoREL (Kidambi et al., 2020), MOPO (Yu et al., 2020), and RAMBO (Rigter et al., 2022). We also compare against COMBO (Yu et al., 2021) which is a hybrid model-free and model-based algorithm. In these experiments, we initially warm start the optimization for 100k steps, by training the policy and Q-function using behavior cloning and temporal difference learning respectively on the offline dataset to ensure the learner policy is initialized to be the same as the reference. Overall, we observe that ARMOR consistently outperforms or is competitive with the best baseline algorithm on most datasets. Specifically, compared to other purely model-based baselines (MoREL, MOPO and RAMBO), there is a marked increase in performance in the _walker2d-med, hopper-med-exp_ and _walker2d-med-exp_ datasets. We would like to highlight two crucial elements about ARMOR, in contrast to other model-based baselines - (1) ARMOR achieves SoTA performance using only a _single_ neural network to model the MDP, as opposed to complex network ensembles employed in previous model-based offline RL methods (Kidambi et al., 2020; Yu et al., 2021, 2020; Rigter et al., 2022), and (2) to the best of our knowledge, ARMOR is the only purely model-based offline RL algorithm that has shown performance comparable with model-free algorithms on the high-dimensional Adroit environments. The lower performance compared to RAMBO on _halfcheetah-med_ and _halfcheetah-med-replay_ may be attributed to that the much larger computational budget used by RAMBO is required for convergence on these datasets.

### Robust Policy Improvement

Next, we test whether the practical version of ARMOR demonstrates RPI of the theoretical version. We consider a set of 14 datasets comprised of the _medium_ and _medium-replay_ versions of D4RL locomotion tasks, as well as the _human_ and _cloned_ versions of the Adroit tasks, with the reference policy set to be the stochastic behavior cloned policy on the expert dataset. We chose these combi

  Dataset & ARMOR & MoREL & MOPO & RAMBO & COMBO & ATAC & CQL & IQL & BC \\  hopper-med & **101.4** & **95.4** & 28.0 & **92.8** & **97.2** & 85.6 & 86.6 & 66.3 & 29.0 \\ walker2d-med & **90.7** & 77.8 & 17.8 & **86.9** & **81.9** & **89.6** & 74.5 & 78.3 & 6.6 \\ halfcheetah-med & 54.2 & 42.1 & 42.3 & **77.6** & 54.2 & 53.3 & 44.4 & 47.4 & 36.1 \\ hopper-med-replay & **97.1** & **93.6** & 67.5 & **96.6** & 89.5 & **102.5** & 48.6 & **94.7** & 11.8 \\ walker2d-med-replay & **85.6** & 49.8 & 39.0 & **85.0** & 56.0 & **92.5** & 32.6 & 73.9 & 11.3 \\ halfcheetah-med-replay & 50.5 & 40.2 & 53.1 & **68.9** & 55.1 & 48.0 & 46.2 & 44.2 & 38.4 \\ hopper-med-exp & **103.4** & **108.7** & 23.7 & 83.3 & **111.1** & **111.9** & **111.0** & 91.5 & **111.9** \\ walker2d-med-exp & **112.2** & 95.6 & 44.6 & 68.3 & **103.3** & **114.2** & 98.7 & **109.6** & 6.4 \\ halfcheetah-med-exp & **93.5** & 53.3 & 63.3 & **93.7** & **90.0** & **94.8** & 62.4 & **86.7** & 35.8 \\  pen-human & **72.8** & - & - & - & - & 53.1 & 37.5 & **71.5** & 34.4 \\ hammer-human & 1.9 & - & - & - & - & 1.5 & **4.4** & 1.4 & 1.5 \\ door-human & 6.3 & - & - & - & - & 2.5 & **9.9** & 4.3 & 0.5 \\ relocate-human & **0.4** & - & - & - & - & 0.1 & 0.2 & 0.1 & 0.0 \\ pen-cloned & **51.4** & - & - & - & - & 43.7 & 39.2 & 37.3 & **56.9** \\ hammer-cloned & 0.7 & - & - & - & - & 1.1 & **2.1** & 0.8 \\ door-cloned & -0.1 & - & - & - & - & **3.7** & 0.4 & 1.6 & -0.1 \\ relocate-cloned & -0.0 & - & - & - & - & **0.2** & -0.1 & -0.2 & -0.1 \\ pen-exp & 112.2 & - & - & - & - & **136.2** & 107.0 & - & 85.1 \\ hammer-exp & **118.8** & - & - & - & - & **126.9** & 86.7 & - & **125.6** \\ door-exp & **98.7** & - & - & - & - & **99.3** & **101.5** & - & 34.9 \\ relocate-exp & **96.0** & - & - & - & - & **99.4** & **95.0** & - & **101.3** \\  

Table 1: Performance comparison of ARMOR against baselines on the D4RL datasets. The values for ARMOR denote last iteration performance averaged over 4 random seeds, and baseline values were taken from their respective papers. The values denote normalized returns based on random and expert policy returns similar to Fu et al. (2020). Boldface denotes performance within \(10\%\) of the best performing algorithm. We report results with standard deviations in Appendix F.

nations of dataset quality and reference, to ensure that the reference policy takes out-of-distribution actions with respect to the data. Unlike Sec. 5.1 here the reference policy is a black-box given as a part of the problem definition. This opens the question of how the learner should be initialized, since we can not trivially initialize the learner to be the reference as in the previous experiments.6 In a similar spirit to Sec. 5.1, one might consider initializing the learner close to the reference by behavior cloning the reference policy on the provided dataset during warmstart, i.e, by replacing the dataset actions with reference actions. However, when the reference chooses out of support actions, this procedure will not provide a good global approximation of the reference policy, which can make the optimization problem harder. Instead, we propose to learn a residual policy where the learned policy outputs an additive correction to the reference (Silver et al., 2018). This is an appropriate choice since ARMOR does not make any restrictive assumptions about the structure of the policy class. Figure 3 shows the normalized return achieved by ARMOR for different \(\), with fixed values for remaining hyperparameters. We observe that ARMOR is able to achieve performance comparable or better than the reference policy for a range of \(\) values uniformly across all datasets, thus verifying the RPI property in practice. Specifically, there is significant improvement via RPI in the _hammer_, _door_ and _relocate_ domains, where running ARMOR as a pure offline RL algorithm(Section 5.1) does not show any progress 7. Overall, we note the following metrics:

* In 14/14 datasets, ARMOR shows RPI (i.e., ARMOR policy is no worse than the reference when measured by overlap of confidence intervals). Further, considering the difference between ORL and REF as a rough indication of whether the reference is within data support, we note that in 12/14 cases REF is strictly better than ORL, and in all those cases ARMOR demonstrates RPI.
* In 5/14 datasets, the ARMOR policy is strictly better than the reference. (Criterion: the lower confidence of ARMOR performance is better than upper confidence of REF). It is important to note that this metric is highly dependent on the quality of the reference policy. Since the reference is near-expert, it can be hard for some environments to improve significantly over it.

## 6 Discussion

The RPI of ARMOR is highly valuable as it allows easy tuning of the pessimism hyperparameter without performance degradation. We believe that leveraging this property can pave the way for real-world deployment of offline RL. Thus, we next present a discussion of RPI.8

_When does RPI actually improve over the reference policy?_

Given ARMOR's ability to improve over an arbitrary policy, the following question naturally arises: Can ARMOR nontrivially improve the output policy of other offline algorithms, including itself? If this were true, can we repeatedly run ARMOR to improve over itself and obtain the _best_ policy any algorithm can learn offline? Unfortunately, the answer is negative. Not only can ARMOR not

Figure 3: Verification of RPI over the reference policy for different \(\) (purple). ORL denotes the performance of offline RL with ARMOR ( Table 1), and REF is the performance of reference policy.5

improve over itself, but it also cannot improve over a variety of algorithms (e.g., absolute pessimism or minimax regret). In fact, the optimal policy of an _arbitrary_ model in the version space \(_{}\) is provably unimprovable (Corollary10; AppendixD). With a deep dive into when RPI gives nontrivial improvement (AppendixD), we found some interesting observations, which we highlight here.

Return maximization and regret minimization are _different_ in offline RLThese objectives generally produce different policies, even though they are equivalent in online RL. Their equivalence in online RL relies on the fact that online exploration can eventually resolve any uncertainty. In offline RL with an arbitrary data distribution, there will generally be model uncertainty that cannot be resolved, and the worst-case reasoning over such model uncertainty (i.e., \(_{}\)) leads to definitions that are no longer equivalent. Moreover, it is impossible to compare return maximization and regret minimization and make a claim about which is better. _They are not simply an algorithm design choice, but are definitions of the learning goals and guarantees themselves_--and are thus incomparable: if we care about obtaining a guarantee for the worst-case return, the return maximization is optimal by definition; if we are more interested in a guarantee for the worst-case regret, then regret minimization is optimal. We also note that analyzing algorithms under a metric that is different from the one they are designed for can lead to unusual conclusions, e.g., Xiao et al. (2021) show that optimistic/neutral/pessimistic algorithms are equally minimax-optimal in terms of their regret guarantees in offline multi-armed bandits. However, the algorithms they consider are optimistic/pessimistic with respect to the _return_ (as commonly considered in the offline RL literature) not the _regret_ which is the performance metric they are interested in analyzing.

\(_{}\) **is more than a hyperparameter--it defines the performance metric and learning goal**Corollary10 in AppendixD shows that ARMOR has many different fixed points: when \(_{}\) is chosen from these fixed points, the solution to Eq.1 is also \(_{}\). Furthermore, some of them may seem quite unreasonable for offline learning (e.g., the greedy policy to an arbitrary model in \(_{}\) or even the optimistic policy). This is not a defect of the algorithm. Rather, because of the unresolvable uncertainty in the offline setting, there are many different performance metrics/learning goals that are generally incompatible/incomparable, and the agent designer must make a conscious choice among them and convey the intention to the algorithm. In ARMOR, such a choice is explicitly conveyed by \(_{}\), which makes ARMOR subsume return maximization and regret minimization as special cases.

## 7 Conclusion

We have presented a model-based offline RL framework, ARMOR, that can improve over arbitrary reference policies regardless of data coverage, by using the concept of relative pessimism. ARMOR provides strong theoretical guarantees with general function approximators, and exhibits robust policy improvement over the reference policy for a wide range of hyper-parameters. We have also presented a scalable deep learning instantiation of the theoretical algorithm. Empirically, we demonstrate that ARMOR indeed enjoys the RPI property, and has competitive performance with several SoTA model-free and model-based offline RL algorithms, while employing a simpler model architecture (a single MDP model) than other model-based baselines that rely on ensembles. This also opens the opportunity to leverage high-capacity world models (Hafner et al., 2023) with offline RL in the future. However, there are also some **limitations**. While RPI holds for the pessimism parameter, the others still need to be tuned. In practice, the non-convexity of the optimization can also make solving the two-player game challenging. For instance, if the adversary is not strong enough (i.e., far from solving the inner minimization), RPI would break. Further, runtime of ARMOR is slightly slower than model-free algorithms owing to extra computations for model rollouts.