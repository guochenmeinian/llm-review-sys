# Closed-Loop Visuomotor Control with Generative Expectation for Robotic Manipulation

Qingwen Bu\({}^{1,2,*}\)  Jia Zeng\({}^{1,*}\)  Li Chen\({}^{1,3,*}\)  Yanchao Yang\({}^{3,}\)  Guyue Zhou\({}^{4}\)

Junchi Yan\({}^{2}\)  Ping Luo\({}^{3}\)  Heming Cui\({}^{3}\)  Yi Ma\({}^{3}\)  Hongyang Li\({}^{1,}\)

\({}^{1}\) Shanghai AI Lab \({}^{2}\)  Shanghai Jiao Tong University \({}^{3}\) HKU \({}^{4}\)  Tsinghua University

\({}^{*}\) Equal contribution \({}^{}\) Corresponding authors

###### Abstract

Despite significant progress in robotics and embodied AI in recent years, deploying robots for long-horizon tasks remains a great challenge. Majority of prior arts adhere to an open-loop philosophy and lack real-time feedback, leading to error accumulation and undesirable robustness. A handful of approaches have endeavored to establish feedback mechanisms leveraging pixel-level differences or pre-trained visual representations, yet their efficacy and adaptability have been found to be constrained. Inspired by classic closed-loop control systems, we propose CLOVER, a closed-loop visuomotor control framework that incorporates feedback mechanisms to improve adaptive robotic control. CLOVER consists of a text-conditioned video diffusion model for generating visual plans as reference inputs, a measurable embedding space for accurate error quantification, and a feedback-driven controller that refines actions from feedback and initiates replans as needed. Our framework exhibits notable advancement in real-world robotic tasks and achieves state-of-the-art on CALVIN benchmark, improving by 8% over previous open-loop counterparts. Code and checkpoints are maintained at [https://github.com/OpenDriveLab/CLOVER](https://github.com/OpenDriveLab/CLOVER).

## 1 Introduction

Robotics and embodied generalists have gained enormous achievements in recent years, with the successful advancement of representation learning and visual generation in computer vision , large (vision-)language models , policy learning , _etc_. Remarkable behavior intelligence has been demonstrated in diverse and complex single-task settings from picking up a Lego block to solving a Rubik's cube . However, deploying the robots for long-horizon manipulation tasks remains a long-standing challenge .

Some literature have attempted to tackle the problem with large language models, splitting a prolonged job into detailed instructions for each minor movement , or sub-goals. Though effective in high-level descriptions, texts could still be inadequate for detailed portrayals of the environment and robot state, leading to considerable issues under cross-morphology or multi-environments . Therefore, recent efforts have started embracing vision as a universal medium to develop an embodied agent capable of planning diverse tasks through imagination and execution . These approaches involve a generative model for predicting future videos or goal images, followed by a goal-conditioned policy for translating the visual plan into actual actions. Despite success, they adhere to an _open-loop_ paradigm, _i.e._, proceeding with a fixed sequence of actions without verifying whether the actual trajectory aligns with the planned one. For instance, when a robot is tasked to "grab a Coke from the fridge", current works assume that the predicted sub-goal is the visual image of the door opening, and the robot should naturally achieve the state (sub-goal) prior to grabbing the bottle. However, the lack of error measurement and real-time feedback leads to accumulative deviation, undesirable robustness, and limited adaptability--particularly inadequate for long-horizon tasks and dynamic environments .

We are inspired by the conventional closed-loop control system as depicted in Figure 1(a). It aims to regulate physical quantities such as actuator velocity by enhancing control precision via a feedback mechanism. Three major components are worth mentioning, the reference input defines desired states which could comprise multiple stages for a prolonged task; the error measurement quantifies bias between the observed state and the planned sub-goal; and the controller adjusts output to reduce the deviation (error) . In fact, several works have introduced analogous feedback in robotics by measuring errors with pixel appearance [20; 21] or visual representations , _e.g._, CLIP features , and yet their performance and adaptability is limited, leaving accurate error quantification modeling unexplored.

To this end, we propose **CLOVER**, a **CLO**sed-loop **V**isuomotor control framework with generative **E**xpectation for **R**obotic manipulation. As shown in Figure 1(b), analogy to the classic closed-loop control system, the ingredients in our version are adapted accordingly. **1) Reference inputs.** With video frames as the interface describing desired states, a text-conditioned video diffusion model generates future frames as reference inputs. To further facilitate the subsequent planning accuracy, we endow the visual planner with the ability of depth map generation, and introduce optical flow regularization to prioritize motion consistency. **2) Error measurement.** Given the limitations of pixel-wise metrics and the inadequacy of pre-trained visual representations, we propose the establishment of a measurable embedding space to realize accurate and efficient error measurement between the observed and planned states. Our state embeddings are trained using an explicit error modeling approach, which yields a strong correlation with the process of converging towards or diverging from target states. **3) Feedback-driven controller.** We present a simple yet effective control framework, comprising a controller and an error-aware adaptive control strategy. The controller is optimized via an inverse dynamics objective  to achieve the predefined sub-goals. To address the issue of a goal-oriented policy failing to consistently achieve the desired state, our proposed framework, CLOVER, adopts an iterative refinement strategy. It continually adjusts its actions to minimize errors, re-evaluates and replans if sub-goals are unrealistic. To sum up, our contributions are three folds:

* We introduce CLOVER, a generalizable closed-loop visuomotor control framework that incorporates a feedback mechanism to improve adaptive robotic control.
* We investigate the state-measuring attribute for latent embeddings and propose a policy by quantifying feedback errors explicitly. The error quantification settles the construction of an execution pipeline to resolve the challenge of handling uncertainties in video generation and task horizons.
* Extensive experiments in simulation and real-world robots verify the effectiveness of CLOVER. It surpasses prior state-of-the-arts by a notable margin (+8%) on CALVIN. The average length of completed tasks on real-world long-horizon manipulation nearly doubles compared to RT-1.

Figure 1: **Motivation. The proposed CLOVER is inspired by the classic closed-loop control in automation systems (a). Our framework (b) employs a visual planner to predetermine a sequence of sub-goals (Section 3.1). Then these goals guide the policy to generate actions with an error measurement strategy (Section 3.2). Within the feedback loop, it automatically replans when the sub-goal is infeasible, and adapts to to the next one upon achievement (Section 3.3).**

Related Work

**Closed-loop mechanisms for robotics.** Model-predictive control (MPC) is a classic and popular approach for leveraging learned dynamics for robotic control and has gained great success in learning robust closed-loop policies [25; 26; 27]. Nonetheless, many of these prior works require knowledge about the system state in the planned future which is often infeasible. Visual foresight [20; 28] integrates an auxiliary register network to calculate pixel distances between the current and goal images, consequently providing feedback. And some methods [21; 29] utilize an additional detection model with preset rules to estimate the current state. However, the adaptability of these methods is constrained to single pick-and-pull tasks and does not cover long-horizon, multi-object manipulation tasks. HiP  investigates the feasibility of decomposed language sub-goals and the consistency of generated video plans through training with feedback. Like other visual planners [16; 15; 17], it lacks a quantitative assessment of trajectory achievement during test-time execution. Inner Monologue  leverages vision-language model to provide linguistic feedback for task success detection, but the substantial size of this model hinders the efficiency during test-test execution. In contrast, CLOVER constructs a measurable latent space from pixel observations and qualitatively measures deviations from planned goals for each action, thereby incorporating real-time feedback. Furthermore, the feedback mechanism is incorporated into long-horizon manipulation tasks.

**Diffusion model as a visual planner.** Recently, it is trending to utilize diffusion models as visual planners to generate goal states. UniPi  seminally leverages internet data to train a text-conditioned video generator and uses an inverse dynamics model to estimate ultimate actions. UniSim  creates a universal video diffusion model for simulating interactions and training policies through generative modeling. Ajay _et al._ propose compositional foundation models for hierarchical planning, including task decomposition, visual planning, and action inference. They also utilize an additional classifier to deal with the uncertainty of generation quality. SuSIE  utilizes an image-editing model as a high-level planner to set achievable sub-goals for a low-level controller, while ADVC  infers actions from predicted video content with dense correspondences. Though prior techniques can synthesize visually reasonable future sub-goals, one challenge is that the lack of consistency constraints related to geometry and motion potentially diminishes the fidelity of generated videos for policy prediction and increases generation instability. In our work, an RGB-D video prediction model is introduced and constrained by the optical flow to enhance sub-goals' reliability. Moreover, the instability of visual plans from diffusion models is rarely discussed in the aforementioned literature. Contrarily, at the core of CLOVER, we adopt an policy state estimation to detect unreachable plans by measuring the distance between consecutive frames.

## 3 Methodology

We aim at building a generalizable framework that integrates the closed-loop philosophy into robotic visuomotor control. The overall system is illustrated in Figure 1. Accordingly, in order to set the desired value before execution, we introduce a visual planner that generates consecutive sub-goals (Section 3.1). In Section 3.2, we detail the structure of our feedback-driven policy to decode actions, and demonstrate how to measure the deviation from the current to the goal states. Finally, the overall test-time execution pipeline of CLOVER is leveraged in Section 3.3.

### Visual Planner

The visual planner is to produce a reliable sequence of future plans based on the initial observation \(O_{0}\) and task descriptions \(c_{l}\). Inspired by previous successful attempts [15; 31; 17], we also employ the prevailing conditional diffusion model to realize text-conditioned video generation . Derived from the image diffusion model of Imagen [33; 31], our model is designed to generate future videos (_i.e._, predicted sub-goals) spanning predetermined time frames, denoted as \(\{_{1},_{2},...,_{K}\}\), with \(K=8\). However, different from targets of high-resolution and meticulous structures for general generative models, visual planners for robotic manipulations highlight the need to understand spatial environments and robot movements. Therefore, designs of effectively integrating depth information and leveraging optical flow's regularization are introduced in CLOVER to generate geometry-aware and temporally coherent futures, which we describe below.

**Text-conditioned RGB-D video generation.** In the framework of amalgamating video prediction and goal-conditioned policy modules, generating a visual plan that precisely corresponds to the task description is a prerequisite for accomplishing manipulation tasks. To encode language inputs, we employ the tokenizer and encoder from CLIP  as the basis, following . In addition to the condition injection techniques outlined in Imagen , which integrate language embeddings into the latent space of the diffusion model directly, our model further incorporates cross-attentional-based conditioning to enhance its language-following ability. Moreover, utilizing classifier-free guidance , the visual planner demonstrates encouraging controllability and generalization, being able to produce diverse and reasonable plans based on task descriptions (See analysis in Section 4.2).

For the vision inputs, robots operating in the 3D space face great challenges in learning from 2D observations directly . Therefore, considering the ease of acquisition of depth sensory nowadays in robotics and its accurate spatial depiction of the environment, we incorporate geometric information from depth maps to assist in manipulation. To predict RGB-D videos, we adopt a simple yet effective way. Specifically, the RGB image and depth map are concatenated on the channel dimension and embedded into a unified latent space throughout all layers of the model. Compared to devising distinct branches for each modality, this yields satisfactory generation results with high consistency between modalities in practice. Moreover, the straightforward approach opens the potential for pre-training the diffusion model on large-scale RGB-only datasets to further enhance its capabilities .

**Latent regularization with optical flow.** Besides the easy acquisition of the depth modality, the robotic manipulation tasks also feature in their interaction dynamics, _i.e._, the moving robot arm and interacted objects in the environment. Though existing works  have utilized video diffusion models for visual plan generation, they fall short in considering the essential gaps between robot manipulation and general video data adequately, particularly the static camera position and robot-initiated movements . Drawing inspiration from the importance of motion cues in robot manipulation, we propose to incorporate optical flow as an explicit regularization term to further foster the classic video diffusion models for manipulation tasks. Specifically, following the end-to-end optical flow estimation framework, RAFT , we first build the pixel-wise correspondence map between the diffusion latent of two consecutive frames. This map is then utilized by subsequent modules to iteratively refine the optical flow estimation through lookup and update operations. Given the final estimation, our flow-based regularization term is formulated as:

\[L_{}=_{k=1}^{K-1}\|O_{k+1}-(O_{k},_{k k+1})\|, \]

where \(_{k k+1}\) is the estimated optical flow, \(\) represents the wrapping function and \(\{O_{k},O_{k+1}\}\) are two consecutive frames in the ground-truth video. More details are provided in Appendix B.

### Feedback-Driven Policy

As depicted in Figure 2, from current and desired visual inputs to the ultimate action output, our policy can be divided into the following components: 1) State Encoding: Deriving informative features from visual inputs and producing compact state embeddings that encode current and desired sub-goal states; 2) Error Measurement: Formulating the deviation from current to goal state; 3) Action Decoding: Decoding the deviation signal into the action a robot can actuate.

Figure 2: **Architecture of our feedback-driven policy. 1) The state encoder takes in both current observation along with the synthesized sub-goal. A shared multimodal encoder generates fused RGB-D features, followed by two queries extracting informative features as the current and goal embeddings respectively. 2) The discrepancy of the two state embeddings is explicitly modeled as errors. 3) The resultant residual in error measurement is ultimately decoded to the final action.**

**State encoder.** To begin with, we employ a multimodal encoder to transform raw pixel inputs into enriched visual representations, which comprises two ViT-based  encoders for RGB and depth respectively along with a multimodal feature fusion module. The feature fusion process uses squeeze-and-excitation module  for channel-wise integration and selection. Subsequently, the token aggregator adaptively selects critical information pertaining to manipulation from the sequence of visual features, condensing them into a compact state embedding. Expressly, the token aggregator is built upon a multi-head attention pooling  with a two-layer multi-layer perceptron (MLP) performing nonlinear projection. Given fused RGB-D features \(\{E_{},\ E_{}\}^{l d}\) corresponding to current and goal inputs, respectively, this process can be specified as:

\[=(Q=q,K=V=E), \]

where \(\{_{},_{}\}\) denotes state embeddings and \(q\{q_{},q_{}\}\) are queries initialized to extract visual features \(E\{E_{},\ E_{}\}\). Here \(d\) is the hidden dimension size and \(l\) represents the visual token length. We employ shared weights for encoding both states in parallel, with the exception that two separately initialized queries are utilized to extract each state embedding. State encoder gives rise to an information bottleneck, prioritizing the encoding of manipulation-relevant features while filtering out irrelevant background details to ensure informativeness.

**Error measurement.** In conventional closed-loop control systems, the controller generates control signals based on the error between the desired value and feedback signals . Analogously, in our visuomotor control pipeline, we explicitly model the discrepancy between the current and goal states by performing element-wise subtraction of the two corresponding state embeddings. Despite its simplicity, this approach has been proven effective in practice. It induces a strong prior that latent actions formulate the transitions between latent states . More importantly, the transitions can be quantified and we note that not all embeddings hold the essential characteristics.

Notably, learning representations to distinguish among diverse instructions and visual states has been a long-standing topic , while there exist few works exploring their quantitative metric for action. In CLOVER, the measurement capability of state embeddings is observed by learning to act from deviation signals, which is absent in pre-trained visual encoders or policy models learned based on current observations solely, _i.e._, behavior cloning. As illustrated in Figure 3(c), the cosine distance between state embeddings decreases together with the robot approaching each predicted sub-goal. In the meantime, the distance to the previous sub-goal increases as proceeding to the next one. Besides, the numerical range of the distance spans approximately from 0 to 0.9, thereby providing a sufficient margin for distinguishing and identifying current states. On the contrary, concerning visual representations generated by pre-trained encoders (_i.e._, CLIP features in Figure 3(a)), there is a noticeable reduction in the range of value variations (within 6e-2) with pronounced fluctuations, although the curves show similar patterns in general. This result stands for all the pre-trained visual encoders we have studied, owing to the fact that manipulation-relevant features will be overwhelmed by immaterial background information. Furthermore, we also test employing state embeddings generated by our policy model but optimized without error measuring.

Figure 3: **Comparison on the measurement ability of different embeddings.** We visualize the cosine distance between embeddings of observations and generated sub-goals during a roll-out process. (a) CLIP feature  and (b) state embeddings trained without error measuring do not hold clear interrelations among frames. While (c) state embeddings obtained from our policy distribute reasonably in the latent space which benefits measuring the errors in feedback loops.

The resulting embeddings capture the state propagation as evidenced by the significant numerical variability (Figure 3(b)); however, they lack the capability to measure interrelations among sub-goals and exhibit poor monotonicity when reaching each sub-goal. Next, we introduce how to elevate the satisfactory error measuring feature for action decoding and adaptive feedback control autonomously.

**Action decoder.** To keep the framework concise and generalizable, we simply adopt an MLP to decode action outputs from error signals. We consider the action space of a 7-DoF robotic arm, encompassing the position of the end-effector \(a_{}^{6}\) and the gripper state \(a_{}^{1}\). Our policy is learned with an Inverse Dynamics objective \(_{}(a_{0}|O_{0},O_{k})\), where it infers action \(a_{0}\) based on current observation \(O_{0}\) and specified sub-goals \(O_{k}\). To promote the transferability of our framework, we exploit third-view RGB-D images as inputs only, with action labels as the training targets. State information such as proprioception signals or gripper-view images are not applied to facilitate manipulation. Please refer to Appendix B for further architectural and training details.

```
Input: Visual planner \(p_{}\); Policy \(_{}\); State encoding module \(g_{}()\); Cosine distance \(D_{C}(,)\). Hyper parameters:Time limit \(T\); Distance threshold for replan and sub-goal transition \(\{D_{R},D_{S}\}\).
1\(t 0,i_{} 0\)\(\) Initialize the sub-goal selection index while\(t T\)do
2iftReplanor\(t==0\)then
3\(_{1:K} p_{}(O_{1:K} O_{0},c)\)\(\) Generate language-conditioned sub-goals (Section 3.1) if\(_{k=1,K-1}\{D_{C}(g_{}(_{k}),\ g_{}(_{k+1}))\}>D _{R}\)then
4\(\)\(\)Replan if sub-goals are unreachable
5else
6\(\)
7 end if
8
9 end for
10if\(D_{C}(g_{}(_{0}),\ g_{}(_{i_{}}))<D_{S}\)then
11\(i_{} i_{}+1\)\(\) Transition if the current sub-goal has been reached
12 end if
13 Sample and Execute \(_{}(a_{0}|O_{0},_{i_{}})\)\(\) Predict and execute action (Section 3.2) \(O_{0}()\)\(\) Update current observation
14\(t t+1\)
15 end while
```

**Algorithm 1**CLOVER: Test-time Execution

### Clover

Equipped with the error quantification capability aforementioned, we have developed a closed-loop visuomotor control framework with feedback, illustrated in Algorithm 1. Notably, our framework distinguishes itself through two key aspects: 1) It can detect and address the instability of diffusion models by initiating replanning when sub-goals are unreachable; 2) It achieves adaptive transitioning between sub-goals based on the distance measurement. In contrast to previous literature such as SuSIE  that sets up dataset-dependent hyperparameters to manually regulate sub-goal refreshing (usually required to be consistent with the frame intervals during training) and thus potentially limit their performance and scalability, our proposed CLOVER is agnostic to training details and adaptable to visual planners with varying intra-frame intervals. We provide illustrative examples in Appendix A to demonstrate the functionality of replanning and adaptive sub-goal transitions.

## 4 Experiments

### Experimental Setup

**Simulation tasks.** We conduct the majority of our experiments using CALVIN , an evaluation benchmark designed for long-horizon, language-conditioned manipulation. CALVIN consists of four simulated environments (designated as A, B, C, and D), which differ in textures and object positions. Each environment comprises a Franka Emika Panda robot situated beside a desk equipped with various manipulable objects. We train policy models on demonstrations collected from environments A, B, and C, and conduct zero-shot evaluations in environment D. The evaluation protocol involves assessing model performance on a comprehensive set of 1,000 unique instruction chains, each comprising five distinct tasks. The CALVIN benchmark provides an extensive dataset paired with natural language annotations, thereby facilitating the training of a generalized and reliable visual planner. Detailed implementation and training protocols are provided in Appendix B.

**Real-world experiments.** The real-robot experiments are conducted on the AIRBOT Play robotic arm. We propose a long-horizon task comprising three consecutive sub-tasks and two additional single tasks ("Pour shrimp into plate" and "Stack two bowls", shown in Figure 4). The fish and pot lid in sub-task 2 and sub-task 3, as well as the plate and bowl in two individual tasks, are randomly placed to reflect position generalizability. All metrics are evaluated with 15 independent runs.

### Main Results

**Visuomotor control on CALVIN.** Table 1 indicates that CLOVER achieves state-of-the-art performance on CALVIN, significantly outperforming previous methods with similar "Planner + Executor" pipelines. Without using gripper view images and proprio signals, our approach exceeds methods employing GPT-style transformers with pretraining, such as RoboFlamingo  and GR-1 . Note that all previous methods follow the CALVIN standard evaluation protocol , where the simulator

    &  &  &  &  \\  & & episodes & 1 & 2 & 3 & 4 & 5 & Avg. Len. \(\) \\  MCL  & & & All & 30.4 & 1.3 & 0.2 & 0.0 & 0.0 & 0.31 \\ HULC  & & & All & 41.8 & 16.5 & 5.7 & 1.9 & 1.1 & 0.67 \\ RT-1  & & Language-conditioned & & Lang & 53.3 & 22.2 & 9.4 & 3.8 & 1.3 & 0.90 \\ RoboFlamingo  & & & Lang & 82.4 & 61.9 & 46.6 & 33.1 & 23.5 & 2.48 \\ GR-1  & & & Lang & 85.4 & 71.2 & 59.6 & 49.7 & 40.1 & 3.06 \\ 
3D Diffuser Actor  & & Diffusion Policy & Lang & 92.2 & 78.7 & 63.9 & 51.2 & 41.2 & 3.27 \\  UniPi\({}^{*}\) & & & All & 56.0 & 16.0 & 8.0 & 8.0 & 4.0 & 0.92 \\ SuSIE  & & & All & 87.0 & 69.0 & 49.0 & 38.0 & 26.0 & 2.69 \\ CLOVER(Ours) & & & Lang & **96.0** & **83.5** & **70.8** & **57.5** & **45.4** & **3.53** \\   

Table 1: **Long-horizon visuomotor control on CALVIN ABC\(\)D.** We report success rates along with the average length of completed tasks (out of the whole 5 tasks) per evaluation sequence. CLOVER outperforms all previous methods by a notable margin. _Lang_ and _All_ denote whether models are trained only with the subset vision-language data. \({}^{*}\)Results reported by .

    &  &  \\  & Sub-task 1 & Sub-task 2 & Sub-task 3 & Avg. Len. \(\) & Pour shrimp & Stack bowls \\  ACT  & 46.7 & 13.3 & 0.0 & 0.6 & 33.3 & 46.7 \\ R3M  & 53.3 & 20.0 & 0.0 & 0.7 & 46.7 & 53.3 \\ RT-1  & 66.7 & 40.0 & 0.0 & 1.1 & **80.0** & 66.7 \\  CLOVER (Ours) & **93.3** & **86.7** & **26.7** & **2.1** & **80.0** & **86.7** \\   

Table 2: **Performances with real-world robot tasks.** CLOVER achieves the best success rate and superior generalization capability across the board.

Figure 4: **Real-world robot setting.** We propose a long-horizon task encompassing three consecutive sub-tasks, where the failure of a prequel task will inevitably lead to failure of subsequent tasks. The additional single tasks are designed to validate the generalizability of CLOVER of all aspects.

returns the signal that marks the completion of the current task. However, such task completion signals are not accessible in real-world environments. In fact, by leveraging the advantageous properties of our state embedding, we can determine the completion of a task autonomously without the signals. We leave this exploration to Appendix A.

Figure 6 shows our diffusion model's proficiency in instruction-following, with "Slide down the switch" as a representative task from the validation set and three others randomly proposed from the CALVIN's task pool. Our planner exhibits robust generalizability, producing reliable action trajectories for the subsequent policy. More visualizations are given in Appendix D.

**Manipulation with real-world robots.** We present the evaluations of real-world robotic tasks in Table 2. CLOVER surpasses all baseline models by a considerable margin, especially on the long-horizon manipulation metric (+1.0 on Avg. Len.). Note that the lid knob is small and hard to grasp, which poses great challenges to policies' low-level precision. ACT  struggles to adjust the gripper to the right position before it should close in the first task, while CLOVER doubles the success rate. Moreover, all three baselines we test fail on the last task, which requires the robot to re-cover the pot lid that was previously placed down in Task 1. In this scenario with high uncertainty, CLOVER shows a success rate of 26.7%, indicating its stronger robustness and position generalization capability.

We further study the generalizability of CLOVER under visual distractions and dynamic environments, as illustrated in Figure 5. Table 3 lists the results of the experiment. CLOVER remains performant while manipulating under distractors, with the performance gap over baseline methods getting more pronounced. We provide qualitative analysis as shown in the Appendix Figure 17. The visual planner effectively disentangles background distractions with foreground movements and generates appropriate plans. Additionally, the feedback-driven policy proves robust to dynamic scene variations, yielding better generalizability over our leading baseline method, RT-1.

### Discussion on Closed-loop v.s. Open-loop

**Preliminaries.** In this section, we conduct the "open-loop" experiments with the same diffusion and policy model, but do not incorporate the feedback mechanism in Algorithm 1 to facilitate adaptive replan and sub-goal transitioning, which is a common practice in previous works [16; 30; 15].

**How does closed-loop work compared to open-loop?** Figure 7(a) compares the manipulation performance of open- and closed-loop execution. The proposed CLOVER, which adaptively selects sub-goals by assessing the distance between current observations and given sub-goals, demonstrates a significant performance improvement of +0.44 average completed task length on CALVIN. Incorporating adaptive replanning further boosts performance by detecting unreliable visual plans and preventing error propagation. Notably, open-loop roll-out performance hinges on understanding the training specifics of both the planner and executor. Synchronizing the time interval for sub-goal transition with the frame intervals used during diffusion model training (\( t=5\) in our experiments) is needed for optimal performance. Figure 7(a) illustrates the performance deterioration when these

Figure 5: **Experiment setting of the generalization evaluation. We place entirely new objects absent from training, alongside the interaction object to introduce visual distraction. We test policies under dynamic conditions by randomly placing and picking up a doll to create unpredictable visual changes.**

    &  &  \\  & & Sub-task 1 & Sub-task 2 & Sub-task 3 & Avg. Len. \(\) \\   & ACT  & 13.1 & 0 & 0 & 0.13 \\  & R3M  & 20.0 & 0 & 0 & 0.20 \\   & RT-1  & 40.0 & 6.7 & 0 & 0.47 \\   & CLOVER (**Ours**) & **73.3** & **66.7** & **6.7** & **1.47** \\   & RT-1  & 33.0 & 0 & 0 & 0.33 \\   & CLOVER (Ours) & **80.0** & **53.3** & **20.0** & **1.53** \\   

Table 3: **Generalization evaluation. CLOVER excels under visual distractions and dynamic scenes, while the success rate of baselines dramatically drops.**settings are not properly aligned. Its distribution pattern aligns with the closed-loop roll-out step distribution in Figure 7(b), supporting the necessity of adaptive steps for optimal performance.

**How generalizable is our feedback mechanism?** We investigate the effect of different visual encoders across varying distance thresholds (\(D_{S}\) in Algorithm 1) used in policy models, as depicted in Figure 7(c). A larger \(D_{S}\) indicates a higher error tolerance. It can be seen that there is a consistent pattern across different distance thresholds for all encoders, with peak performance observed at \(D_{S}=0.02\). Adopting VC-1  demonstrates the highest performance, while training a ViT-Base  encoder from scratch yields exceptional stability, with the lowest result being 3.19. The results manifest the robustness of our feedback mechanism which is independent of specific encoders and does not require customized hyperparameters for different policy models. We compare the robustness of our error measurement scheme against other representations, specifically the dense reward learning framework LIV  and CLIP features . We maintain the same model for both the visual planner and the low-level policy, with the exception of the measurements used to determine sub-goal transitions and replanning. Our findings (as in Figure 3) indicate that CLIP features and LIV exhibit a narrower range of values, prompting us to set \(D_{S}\) to \(2e-3\). The results presented in Table 4 show that unreliable measurements can lead to performance that is even worse than in the open-loop setting. Moreover, we do not incorporate additional contrastive objectives as in LIV, but investigate the inherent properties of the inverse dynamics-based policy.

### Ablation Studies

**Ablations on the diffusion model (visual planner).** We conduct a comparative analysis of the high-level plan (video generation) quality. Results presented in Table 5 reveal that our method outperforms AVDC  across all video generation metrics , both of which are built upon the

   &  &  \\  & 1 & 2 & 3 & 4 & 5 & Len. \(\) \\  CLIP  & 72.4 & 46.8 & 25.0 & 13.7 & 5.1 & 1.63 \\ LIV  & 70.8 & 48.2 & 29.2 & 18.2 & 10.2 & 1.77 \\  CLOVER & **96.0** & **83.5** & **70.8** & **57.5** & **45.4** & **3.53** \\  

Table 4: **Error measurments with different representations. Our method shows exceptional cross-tasks robustness on CALVIN benchmark.**

Figure 6: **Generated videos of diverse tasks conditioned on the same initial frame. CLOVER can generate precise visual plans corresponding to the tasks, facilitating low-level executor guidance. We downsample the video by 2 and exclude depth results in visualizations for simplicity.**

Figure 7: **Analysis and comparisons on closed-loop and open-loop roll-out on CALVIN. (a) Comparative analysis of performance (Avg. Len.) through varying step lengths in open-loop control. Evaluations are conducted using identical models but employing different roll-out strategies. (b) The distribution of action steps taken in closed-loop roll-out to achieve each sub-goal. (c) Examination of the robustness of closed-loop control employing various visual encoders and distance thresholds.**

Imagen framework , alleviating the learning and generalization burden on the policy model. Notably, AVDC struggles to generate visual plans consistent with the task description, resulting in significant performance degradation on the CALVIN benchmark. Additionally, the optical flow-based regularization not only brings a generation quality improvement across all aspects, but also significantly accelerates the training convergence. Please refer to Appendix C for further analysis.

**Ablations on the policy model.** Besides the ablation of feedback mechanisms in Section 4.3, we additionally evaluate the performance of CLOVER using various error measurement approaches, illustrated in Figure 8(a). Policies learned with behavior cloning (BC, Figure 3(b)) and LCBC  serve as baselines that do not employ error measurement, while CLOVER exceeds them by a notable margin. Figure 8(b) demonstrates that incorporating geometry information from depth data leads to a not trivial improvement on CALVIN, with convolution-based multimodal fusion modules achieving the best performance. We also examine the robustness of our policy to generation quality by varying the sampling steps of the diffusion model. As shown in Figure 8(c), increasing the sampling steps generally provides generated videos with more details but shows diminishing returns. We set the sampling step to 20 to strike a balance between performance and efficiency.

## 5 Conclusion

In this paper, we present a generalized closed-loop visuomotor control framework, termed CLOVER. It comprises a visual planner that specifies desired sub-goals, a policy that executes actions as planned, and a feedback-driven control strategy to realize long-horizon robotic tasks. CLOVER excels in both simulation and real-world applications, showcasing the virtue of our feedback mechanism.

**Limitation and future works.** We validate CLOVER for simulation and real-world experiments by training the models heavily on the corresponding data. However, emerging evidence suggests both the diffusion models and IDM-based policies exhibit out-of-distribution generalizability [59; 60; 61]. Visual planner can be trained with actionless videos, and IDM can be learned data-efficiently with random actions with corresponding observations. This points to the potential of our framework for performing few-shot and long-horizon manipulations by pre-training on web-scale datasets.