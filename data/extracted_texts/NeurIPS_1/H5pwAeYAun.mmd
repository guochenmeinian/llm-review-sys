# Failure-Aware Gaussian Process Optimization

with Regret Bounds

Shogo Iwazaki

MI-6 Ltd., Tokyo, Japan

iwazaki@mi-6.co.jp

&Shion Takeno

RIKEN AIP, Tokyo, Japan

shion.takeno@riken.jp

&Tomohiko Tanabe

MI-6 Ltd., Tokyo, Japan

tanabe@mi-6.co.jp

&Mitsuru Irie

MI-6 Ltd., Tokyo, Japan

irie@mi-6.co.jp

###### Abstract

Real-world optimization problems often require black-box optimization with observation failure, where we can obtain the objective function value if we succeed, otherwise, we can only obtain a fact of failure. Moreover, this failure region can be complex by several latent constraints, whose number is also unknown. For this problem, we propose a failure-aware Gaussian process upper confidence bound (F-GP-UCB), which only requires a mild assumption for the observation failure that an optimal solution lies on an interior of a feasible region. Furthermore, we show that the number of successful observations grows linearly, by which we provide the first regret upper bounds and the convergence of F-GP-UCB. We demonstrate the effectiveness of F-GP-UCB in several benchmark functions, including the simulation function motivated by material synthesis experiments.

## 1 Introduction

The effectiveness of optimization methods based on Gaussian process (GP) regression for expensive-to-evaluate black-box functions has been repeatedly shown in a wide range of real-world applications, including robotics , experimental design , and hyperparameter optimization . On the other hand, failure of the observation itself must often be considered. For example, in the optimization of hyperparameters for a complicated physical model, the evaluation may crash for some hyperparameters. Another example is materials development, in which experimental testing of new materials can reveal that the synthesis procedure fails.

Therefore, this study considers the optimization of a black-box function \(f\) with a black-box deterministic failure function \(c:\{0,1\}\), where \(\) is an input space. In this study, failure of observation at the input \(\) is represented as \(c()=1\), while the success of observation is represented as \(c()=0\). Then, our optimization problem can be formulated as follows:

\[^{*}=*{arg\,max}_{}f()\;\;s.t.\; \;c()=0, \]

where the observation of \(f\) can be obtained only if \(c()=0\), but the observation cost is consumed even if \(c()=1\). Hence, the goal is to efficiently identify the optimal point \(^{*}\) while considering the observation failure.

One of the notable technical difficulties of problem (1) is how to handle the failure function \(c\). To model \(c\), existing studies  use the GP classification (GPC) model  or its variants, which assume that \(c\) can be represented by a smooth latent function. However, the observation failure canbe caused by several latent constraints implicitly. In addition, since only the binary failure can be obtained, the number of latent constraints is also unknown. Therefore, modeling \(c\) with the usual GPC is hard and often unsuitable, which can degrade the optimization performance. Hence, an optimization method under a mild assumption on \(c\) is demanded. Furthermore, the existence of \(c\) makes theoretical analysis and even securing the number of successful observations difficult.

Our contribution.In this paper, we propose a novel GP-based optimization algorithm, failure-aware GP upper confidence bound (F-GP-UCB), which chooses the next evaluation in an input domain except for the adaptively adjusted neighborhood of the past observation failures. Furthermore, we show that the number of successful observations grows linearly under a very mild assumption of the failure function \(c\) that the optimal solution lies on an interior of a feasible region. Then, we provide the first regret upper bound of the GP optimization problem (1), which shows that F-GP-UCB converges to the optimal solution with high probability. We demonstrate the effectiveness of the F-GP-UCB algorithm with several benchmark functions, including a heuristic simulation function motivated by materials research of quasicrystals.

Related work.In the past few decades, many GP-based optimization algorithms have been developed [37; 42; 18; 8; 53]. In addition to the standard optimization settings, various extensions have been studied, such as parallel , high-dimensional , multi-fidelity , and robust optimization .

GP-based constrained optimization [10; 11; 19; 47] has a close relation to our study. It considers the black-box optimization under an inequality constraint \(g() 0\), where \(g\) is also a black-box function. If the failure function in this paper is recast as \(c()=\{g()>0\}\), the optimization problem matches that of problem (1). However, there are two crucial differences. First, in constrained optimization, observations at an input can be obtained even if the constraint is not satisfied, i.e., \(g()>0\). Under the setting of this paper, no information about the objective function can be obtained for input points that result in failure. Second, in constrained optimization problems, it is generally assumed that noisy observation of \(g()\) is possible. In the setting of our study, we cannot obtain a direct observation from the latent function \(g\).

There exist works on GP optimization that take into account failures. Lindberg and Lee  proposed an algorithm that combines the Expected Improvement (EI) criterion  with the posterior success probability of the classical GPC. Instead of GPC, Sacher et al.  used a support vector machine-based model. However, although \(c\) is deterministic, these classifiers assume that observation failure is essentially stochastic, i.e., the evaluations at the same input can fail and succeed. This model misspecification can degrade the optimization performance, as described in Bachoc et al. . Then, Bachoc et al.  proposed the deterministic variant of GPC which models \(c\) as \(c()=\{g() 0\}\), where \(g\) is a latent function modeled by a GP. They also provide an EI-based strategy and prove convergence to an optimal solution. From a practical point of view, surrogate models of \(c\) navigate the optimization process efficiently when \(c\) is well represented through a smooth latent function \(g\). However, practical applications often have too complex failure processes to model with one latent function (e.g., the failure function \(c\) depends on several latent functions.) Furthermore, it is difficult to know that such a complex failure structure exists beforehand. Finally, from a theoretical perspective, Bachoc et al.  give the convergence guarantee, but its rate and the regret-based analysis are not provided.

## 2 Preliminaries

Problem setup.Let \(f:\) and \(c:\{0,1\}\) be an unknown fixed objective function and an unknown failure function, respectively, where \(^{d}\) is the input space. At each time step \(t\), the user makes a selection \(_{t}\) and obtains the failure label \(c(_{t})\). If \(c(_{t})=0\), the user proceeds to make a noisy observation \(y_{t}=f(_{t})+_{t}\) where \(_{t}\) is a noise term which is conditionally \(\)-sub-Gaussian. Our noise model is a mild one; examples include an arbitrary distribution over \([-,]\) and a Gaussian noise with variance below \(^{2}\). Note that, in the case of \(c(_{t})=1\) (failure), the user obtains no further information.

The user's goal is to efficiently identify the optimal solution \(^{*}\) over the unknown feasible region \(S_{c}\{ c()=0\}\). For convenience, we rephrase the problem (1) using \(S_{c}\) as follows:

\[^{*}=*{arg\,max}_{ S_{c}}f(). \]We assume that \(S_{c}\) and that there exists a unique solution \(^{*}\). Furthermore, we define the failure region \(F_{c}\) as \(F_{c}= S_{c}\).

Regret.We employ the _regret_ to evaluate the algorithm's performance. The regret \(r_{t}\) at step \(t\) is defined as \(r_{t}=f(^{*})-f(}_{t})\) if \(}_{t} S_{c}\), otherwise \(r_{t}=f(^{*})-_{}f()\), where \(}_{t}\) is the algorithm's estimated solution1. In our definition of \(r_{t}\), the algorithm is supposed to incur the worst-case regret when \(}_{t}\) is not in \(S_{c}\). Similar definitions are also used in performance metrics of GP-based constrained optimization .

Regularity assumptions for the objective function.As a regularity assumption, we assume that \(f\) is an element of the reproducing kernel Hilbert space (RKHS) \(_{k}\), corresponding to a known positive-definite kernel \(k:\), and has a bounded Hilbert norm \(\|f\|_{_{k}} B\). Furthermore, the kernel \(k\) is assumed to be normalized, namely \(,k(,) 1\). These are common assumptions in existing GP optimization literature .

Gaussian process modeling.Our algorithm uses the modeling information of \(f\) from GP , using only the successful observations. First, we assume zero-mean GP with the covariance function \(k\) as the prior of \(f\). Next, we model the generating process of the value \(y_{t}\) queried at \(_{t}\) as \(y_{t}=f(_{t})+_{t}\), where \(_{t}(0,^{2})\). We note that the assumptions on the GP prior on \(f\) and Gaussian noise, as stated above, are assumptions only for constructing the GP model, which can differ from the underlying true function \(f\) and the noise. Let \(^{(S)}_{t}\{i\{1,,t\} c(_{i})=0\}\) be the index set of successful observations, \(^{(S)}_{t}(_{i})_{i^{(S)}_{t}}\) be the corresponding inputs, \(^{(S)}_{t}(y_{i})_{i^{(S)}_{t}}\), be the corresponding outputs, and \(n_{t}|^{(S)}_{t}|\) be the number of successful observations. In addition, we also define the failure index set \(^{(F)}_{t}\{i\{1,,t\} c(_{i})=1\}\). Given \(^{(S)}_{t},^{(S)}_{t}\), the posterior distribution of \(f()\) becomes a normal distribution, whose posterior mean \(_{t}()\) and posterior variance \(^{2}_{t}()\) are given as follows:

\[_{t}() =(,^{(S)}_{t})^{}((^{(S)}_ {t})+^{2}_{n_{t}})^{-1}^{(S)}_{t}, \] \[^{2}_{t}() =k(,)-(,^{(S)}_{t})^{}( {K}(^{(S)}_{t})+^{2}_{n_{t}})^{-1}(, ^{(S)}_{t}). \]

Here, \(_{n_{t}}\) is a \(n_{t} n_{t}\) identity matrix. Furthermore, denoting the index of the \(j\)-th least element of \(^{(S)}_{t}\) as \(i_{j}\), \((,^{(S)}_{t})^{n_{t}}\) represents a vector whose \(j\)-th element is \(k(,_{i_{j}})\). Similarly, \((^{(S)}_{t})^{n_{t} n_{t}}\) represents the kernel matrix whose \(jk\)-th element is \(k(_{i_{j}},_{i_{k}})\).

Lastly, we define the maximum information gain  as a quantity representing the GP complexity. The maximum information gain \(_{t;A}\) from observing \(t\) number of data points over the set \(A^{d}\) is defined as \(_{t;A}=_{(_{1},,_{t}) A^{t}}I( {f}_{},_{})\), where \(I(_{},_{}) 0.5(_{t}+^{-2}())\) is the mutual information between the latent function values \(_{}(,())\) and corresponding output values \(_{}=_{}+_{t},_{t} (,^{2}_{t})\) of GP. In GP-based optimization, \(_{t;A}\) is often used as a quantity that characterizes the confidence bound and regret bound of \(f\). Furthermore, for a compact convex set \(A\), the upper bound for \(_{t,A}\) has been derived for some commonly used kernels. For example, \(_{t;A}=(( t)^{d+1})\) for Gaussian kernel and \(_{t;A}=(t^{d/(+d)}( t)^{2/(+d)})\) for Matern kernel with the smoothness parameter \(>1/2\). The following Lemma 2.1 adapts the well-known result that gives the confidence bound of \(f\) for our problem setup, which is a direct consequence of Theorem 3.11 in .

**Lemma 2.1**.: _Fix \(f_{k}\) with \(\|f\|_{_{k}} B\). Suppose the observation \(y_{t}=f(_{t})+_{t}\) has a noise \(_{t}\) that is conditionally \(\)-sub-Gaussian. Define \(\{_{t}\}_{t}\) as \(_{t}^{1/2}=B+}+1+(1/))}\) for \((0,1)\). Then the following holds with probability at least \(1-\):_

\[ t 1,\;,\;_{t}() f( )_{t}(). \]

_Here, \(_{t}()\) and \(_{t}()\) are defined as \(_{t}()=_{t-1}()-_{t}^{1/2}_{t-1}()\) and \(_{t}()=()+_{t}^{1/2}_{t-1 }()}\), respectively._Regularity assumption for failure function.A regret upper bound cannot be obtained without any assumption on \(c\). As an extreme example, we consider the case where \(^{*}\) is an isolated point surrounded by \(F_{c}\). Under this scenario, the worst case exists in which an arbitrary algorithm can never observe \(^{*}\) in a finite number of trials. Therefore, in order to give a convergence guarantee, at least \(^{*}\) must be contained in a subset of the feasible region \(S_{c}\) having a non-zero volume. In this paper, we focus on the case that \(^{*}\) is the interior point of \(S_{c}\) as in the following assumption.

**Assumption 2.2**.: _There exists \(>0\) such that \(N_{^{*};} S_{c}\), where \(N_{^{*};}\{\|-^{*}\|_{ }<\}\) is an open infinity ball with a radius \(\) centered at \(^{*}\)2._

The parameter \(\) above depends on the size of the subset of the feasible region that \(^{*}\) belongs to and is an important quantity for theoretical analysis. Note that Assumption 2.2 is quite mild. For example, when \(c\) is defined as \(c()=\{g() 0\}\) with a continuous latent function \(g\), there exists \(>0\) such that \(N_{^{*};} S_{c}\) from the continuity of \(g\). Since the analysis of  assumes the existence of the latent function \(g\), which is a _continuous_ sample path generated from GP, our assumption also subsumes their assumption. Finally, note that \(\) is not needed for running the algorithm and is only used as a quantity that characterizes the regret bound.

## 3 Proposed algorithm

Our proposed algorithm F-GP-UCB is shown as pseudo-code in Algorithm 1. Roughly speaking, our F-GP-UCB searches the input domain excluding the adaptively adjusted neighborhood of past failure points based on the existing GP-UCB  strategy. Below, we start by describing the background of the algorithm construction.

Philosophy of algorithm construction.Since \(F_{c}\) is unknown, the algorithm needs to appropriately avoid the failure observation in \(F_{c}\) while balancing the trade-off between exploration and exploitation in a way that guarantees convergence. The difficulty of this problem lies in the fact that there exist cases where the small feasible region containing \(^{*}\) is surrounded by the past failure observations and is isolated as shown in the left plot of Fig. 1. Since the algorithm is unable to exclude the possibility that an arbitrary small feasible region exists between failure points only from Assumption 2.2, the algorithm should be constructed so that it simultaneously satisfies the following two points: the failure observation should be avoided; and the inputs evaluated by the algorithm should eventually cover the arbitrarily close area of the past failure point.

F-GP-UCB satisfies the above requirements by adaptively controlling the GP-UCB search region by eliminating the neighborhood of the past failure points, and shrinking the eliminated region as step increase. Next, we describe below the details of and ideas behind each step.

Selection of \(_{t}\).At each step \(t\), the F-GP-UCB algorithm firstly computes the search region \(_{t}\) as

\[_{t}=\{ i_{t-1}^{(F)},\|_{i}-\|_{}_{t}b(t)\}. \]

Here, \(_{t}\) and \(b:_{+}_{+}\) are the scale parameter and monotonically decreasing function, respectively. The parameter \(_{t}\) is controlled by the algorithm at every step so that it decreases monotonically with respect to \(t\) from its initial value \(_{0}\) given by the user. It plays the role of guaranteeing that the search space \(_{t}\) satisfies \(_{t}\). The function \(b\) is defined by the user before running the algorithm.

By using \(_{t}\), the F-GP-UCB algorithm chooses \(_{t}\) as

\[_{t}=*{arg\,max}_{_{t}}_{t}(). \]

The middle and right plots in Fig. 1 show an example behavior of the F-GP-UCB algorithm. By using the monotonically decreasing \(_{t}b(t)\) to control the search space, the algorithm's behavior can be qualitatively described as follows. First, during the early phase where \(t\) is small, \(_{t}b(t)\) is large, corresponding to choosing an unexplored point for observation while avoiding a large neighborhood of the past failure points where there is a high possibility of failure. Then as \(t\) increases and the remaining unexplored space shrinks, it is expected that the algorithm's behavior will become more aggressive in considering the possibility that the region near the observed failure points contains a feasible region that may be difficult to identify.

The choice of \(_{0}\) and \(b\) affects the performance of F-GP-UCB. In Sec. 4, we show the convergence of the regret under an appropriate choice of \(b\). In Sec. 5, the practical choices of \(_{0}\) and \(b\) are discussed.

Shrinking \(_{t}\).In the case where \(_{t}=\), \(_{t}\) is not defined. We ensure such cases do not occur by controlling the scale parameter \(_{t}\). In our algorithm, the main role of \(_{t}\) is to guarantee that \(_{t}\) is well-defined as follows. Specifically, before the step \(t\) starts, \(_{t}\) is computed by halving the previous scale parameter \(_{t-1}\) until the union of all the neighborhoods does not fully cover \(\). This procedure requires that we solve the set covering problem to find the cover of \(\) with the neighborhoods of the past failure points. Unfortunately, this problem is known to be NP complete . In Sec. 5, we provide a practical approach to this problem. On the other hand, note that our analysis in Sec. 4 assumes that this procedure can be computed exactly.

Algorithm estimated solution.The estimated solution of the F-GP-UCB algorithm \(}_{t}\) at step \(t\) is defined as follows:

\[}_{t}=_{}\;\;\;\;=*{ arg\,max}_{i^{(S)}_{t}}_{i}(_{i}). \]

The definition of the estimated solution based on the observed points and \(_{i}\) is often used in exisiting literature [6; 26; 21]. While the estimated solution of the proposed method resembles those in the

Figure 1: An example problem in one dimension. The left plot shows the situation where there exists an isolated feasible region, which includes the optimum. The green and grey shaded areas represent the feasible region \(S_{c}\) and the failure region \(F_{c}\), respectively. The green (black) points represent the observed successful (failure) points. In this situation, it is necessary to identify the feasible region which hides among the observed failure points. The middle and right plots are example behaviors of the F-GP-UCB algorithm in \(t=5\) and \(t=10\), respectively. The gray shaded regions represent the neighborhood of the observed failure points which is excluded from the search space at the given step. The F-GP-UCB algorithm performs searches in all the feasible regions while avoiding the observed failure points by iteratively narrowing down the excluded search space in the neighborhood of the failure points.

literature, it should be noted that the maximum of \(_{i}\) is computed over \(_{t}^{(S)}\). Note \(}_{t}\) is not defined if all the points up to \(t\) are failures (i.e., \(_{t}^{(S)}=\)). In Sec. 4, it is shown that there always exists \(}_{T}\) for a sufficiently large step size \(T\). Finally, although the estimated solution (8) is useful for discussing the theoretical performance, for practical purposes, \(}_{t}=_{t}\) where \(=*{argmax}_{i_{t}^{(S)}}_{i}(_{i})\) is often used instead [6; 21]. We use this modified definition of \(}_{t}\) in the benchmark experiments in Sec. 6.

## 4 Theoretical analysis

The results of the theoretical analysis are discussed in this section. The complete proofs of the theorems and lemmas are described in Appendix A.

Key of regret analysis.To analyze the regret, we start by evaluating the growth rate of the number of successful observations \(n_{t}\) in the algorithm. By noting that \(_{t}b(t)\) is monotonically decreasing, we can find that the failure points of F-GP-UCB are separated by at least a distance \(_{t}b(t)\) apart from each other. Therefore, \(n_{t}\) can be evaluated via \(_{t}b(t)\)_-packing number_ of \(F_{c}\). The following Lemma 4.1 is based on the standard argument for the packing number, which gives the lower bound for the number of successful observations \(n_{t}\).

**Lemma 4.1**.: _When running Algorithm 1, \(n_{t} t-(,\|\|_{},_{t}b(t)/4)\) holds for any \(t_{+}\), where \((,\|\|_{},)\) denotes the \(\)-packing number over the set \(\) with respect to the norm \(\|\|_{}\)._

Using the above Lemma 4.1, designing the choice of \(b\) so that \(t-(,\|\|_{},_{t}b(t)/4)=(t)\) holds leads to \(n_{t}=(t)\), which means that the number of successful observations can be secured at the same rate as in the standard GP optimization setting. By using the fact \((,\|\|_{},)= 1/^{d}\) (Lemma A.4), the choice of \(b(t)=o(t^{-1/d})\) is sufficient for \(n_{t}=(t)\)3. While our result holds for any \(b\) satisfying \(b(t)=o(t^{-1/d})\), we give results for \(b\) satisfying \(b(t)=t^{-}\) with \((0,1/d)\) in this section for simplicity. In Appendix A, the result for general \(\) and \(b\) is also provided.

Regret bound.The following Theorem 4.2 gives the regret bound of the F-GP-UCB algorithm.

**Theorem 4.2**.: _Fix \((0,1)\), \((0,d^{-1})\), \(=^{d}\), and \(f_{k}\) such that \(\|f\|_{_{k}} B\). Let \(_{t}^{1/2}=B+}+1+(1/))}\) and \(b(t)=t^{-}\). In addition, suppose that there exists \(>0\) such that \(N_{^{*};} S_{c}\) where \(N_{^{*};}=\{\ \ \|-^{*}\|_{ }<\}\). When applying Algorithm 1 under the above conditions, the estimated solution \(}_{T}\) defined in (8) exists for all \(T_{s}\{(10/^{d})^{1/(1-d)}, 2\}+1\), where \(=\{_{0},/2\}\). Moreover, the following holds with probability at least \(1-\):_

\[ T_{s},\;r_{T}[2B(_{*}-1)+ _{T}T_{T;S_{c}}}], \]

_where \(_{*}=(_{0}/)^{1/}+1\) and \(C_{1}=8/(1+^{-2})\)._

In the analysis above, \(\) is interpreted as the complexity of the failure function \(c\), and \(\) corresponds to the lower bound on \(_{t}\) while running the algorithm. The constants \(_{s}\) and \(_{*}\) are important quantities that characterize the regret and can be respectively interpreted as follows.

* The constant \(_{s}\) is the upper bound on the number of steps until the first successful observation.
* The constant \(_{*}\) is the upper bound on the number of steps needed until the search space \(_{t}\) of the algorithm always contains the optimal solution \(^{*}\).

The regret bound in Theorem 4.2 can be intuitively seen as evaluating the algorithm execution in terms of two stages along the time axis. The first term of (9) represents the regret generated when the algorithm's search space \(_{t}\) does not contain \(^{*}\). Specifically, the worst-case regret incurred during this phase is bounded by \(2B\) using the fact that \(_{}|f()| B\) from the kernel normalization condition. The second term is the regret incurred during the process of identifying \(^{*}\) after \(_{t}\) includes \(^{*}\) and is similar to the regret bound of standard GP-UCB. The only difference is that, in our setup, the observations of GP are only on \(S_{c}\), so the maximum information gain is defined over \(S_{c}\).

By noting that \(_{T;S_{c}}_{T;}\) in the second term and using the known results concerning the upper bound on \(_{T;}\), the more explicit form of regret upper bound can be obtained on a per-kernel basis. For example, in the case of the Gaussian kernel, we have that \(_{T;}=(( T)^{d+1})\) and \(r_{T}=(T^{-1}B}+T^{-1/2}( T)^{(d+1)/2}(B+ +(1/)}))\). Thus, indeed we obtain \(r_{T} 0\) (as \(T\)). The F-GP-UCB algorithm is guaranteed to converge when the cumulative regret for ordinary GP-UCB (corresponding to the second term in (9)) becomes sub-linear under the chosen setup and the kernel4.

The case where \(^{*}\) exists on the boundary of \(S_{c}\).The case where \(^{*}\) exists on the boundary \(S_{c}\) is not covered in Theorem 4.2, which is the case that often appears in a real-world application. If undesirable Lipschitz-style dependencies are allowed to emerge in the regret upper bound, our F-GP-UCB algorithm is also guaranteed to have convergence in such cases (Theorem B.2). The details of the result and discussion about its limitation are in Appendix B.

## 5 Practical considerations

In this section, we discuss several issues that arise in practical situations and provide their solutions. Due to the space limitation, we only give brief descriptions. The details are described in Appendix E.

Computation of \(_{t}\).The computation of \(_{t}\) requires us to know whether the union of all the neighborhoods of the failure points covers \(\). A simple solution might be to partition \(\) into a sufficiently fine grid by subdividing each axis and look for a feasible solution in each grid cell by brute force. However, as the step size \(t\) and dimension \(d\) become large, the number of grid cells grows rapidly. Furthermore, even a very fine grid will not be able to detect the case where a feasible solution exists on the cell boundary. Another heuristic approach is to use a constrained optimization solver which does not require a feasible solution as part of the initial points (e.g., penalty function method and augmented Lagrangian method ) and try to solve the problem (7) by using \(_{t-1}\). In case that the solver is unable to find a feasible solution, it is decided that \(}_{t}\), multiply \(_{t-1}\) by \(1/2\), then we try to solve problem (7) again. This is our chosen approach. A drawback of this approach is that, depending on the scale of \(_{t}\), the constrained optimization problem must be solved many times. We show that, by performing an appropriate preprocessing, a feasible solution can be obtained within two rounds of constrained optimization in the worst case. We give the details in Appendix E.1.

Selection of \(b\).As stated in Sec. 4, the F-GP-UCB algorithm is valid for an arbitrary monotonically decreasing function \(b\) such that \(t-(,\|\|_{},_{t}b(t)/4)=(t)\). However, if the rate is slower than \(1/t^{}\) (\((0,1/d)\)), for example, \(b(t)=1/(t+1)\), then the dominant term of our regret upper bound can become \((1/ T)\) (See Theorem B.3 in Appendix B and Corollary F.1 in Appendix F). This intuitively means that an exponentially large sample is needed to obtain an arbitrarily small regret and is not the desirable behavior. In this paper, we use \(b\) in the form of \(b(t)=1/t^{}\) with \(=1/(2d)\) as a practical choice. (Additional discussion about \(\) is given in Appendix E.2.)

Adaptive tuning of \(_{0}\).Although the choice of \(_{0}\) does not affect the no-regret guarantee, it has a large impact on the practical behavior when the number of steps is small. For example, if \(_{0}\) is set to a large value when the failure region \(F_{c}\) forms a relatively small region, then there is a risk of excessively avoiding the feasible region that should be searched. It is practically difficult to find a desirable choice of \(_{0}\) as it depends on some conditions that are unknown to the user, such as the failure region \(F_{c}\). In order to alleviate this issue while retaining the theoretical convergence, we use the strategy to adaptively choose \(_{t}\) based on the posterior standard deviation \(_{t-1}(_{t})\) of the observed points. We note that such a strategy using \(_{t-1}(_{t})\) to adaptively select an unknown parameter is also developed in  to set the unknown kernel hyperparameters for ordinary GP optimization.

Algorithm 4 in Appendix E.3 shows the pseudo-code of our modified strategy. In Algorithm 4, the user specifies the possible range for \(_{t}[_{},_{}]\), the threshold for the posterior standard deviation \(h_{}>0\), the threshold for the step number \(q_{+}\), and a scaling factor \(w(0,1)\). As the initial value, we set \(_{0}=_{}\). After that, if the observed point is lower than the posterior standard deviation for \(q\) consecutive number of times, \(_{t}\) is multiplied by \(w\) to lower its value, but no less than the minimum value \(_{}\). Intuitively, suppose the posterior standard deviation is repeatedly excessively small. In that case, there exists a possibility that \(_{0}\) was set to an excessively large value, which may lead to the unwanted exclusion of the feasible region from the search space. In this procedure, by setting \(_{0}\) to a large value of \(_{}\) at the start, we expect it to get adjusted towards an appropriate neighborhood scale. In our simulation experiments, we set \(w=0.75,h_{}=0.02\), \(q=3\), \(_{}=0.0001\), and \(_{}=0.5\). We emphasize that this procedure does not affect our convergence guarantee (Theorems F.2 in Appendix E).

Lastly, the pseudo-code, including all the considerations provided in this section, is described in Algorithm 5 of Appendix E.

## 6 Numerical experiments

In this section, we show the performance of F-GP-UCB through numerical experiments. The detailed settings of the experiments and additional results can be found in Appendices G and H, respectively. First, for comparison, we use EI  and GP-UCB  as the baseline algorithms which do not consider failures. In each of these algorithms, the GP model is constructed using only the successful observations; the failure observations are not used. We also compare F-GP-UCB with the EFI-GPC-EP and EFI-GPC-Sign algorithms that leverage GPC models . EFI-GPC-EP is the algorithm that uses the classic GPC model with probit likelihood, and posterior approximation is based on Expectation Propagation (EP). EFI-GPC-Sign uses a variant of GPC proposed in . Both EFI-GPC-EP and EFI-GPC-Sign choose the next input based on the posterior success probability of GPC and the EI value of \(f\) as in  (Details are in Appendix H).

In EFI-GPC-EP and EFI-GPC-Sign, as the prior for the latent function of GPC, we use zero-mean GP with Gaussian kernel \(k_{c}(,)_{c}^{2}(-\|-\|^{2}/(2l_{c }^{2}))\). Before the experiment, we fix the kernel parameters of GPC \(_{c}\) and \(l_{c}\) by marginal likelihood maximization using observations at a randomly generated Sobol sequence. All the kernel hyperparameters of the GPC fixed by this procedure are given in Appendix H. Furthermore, it is well-known that the theoretical choice of \(_{t}\) in GP-UCB is excessively conservative. We set \(_{t}=2(2t)\) as in  for GP-UCB and F-GP-UCB. The other parameters in F-GP-UCB are fixed as described in Sec. 5. For the evaluation of the algorithms, we confirm the behavior of the regret at each step \(r_{t}=f(^{*})-f(}_{t})\). We note that \(}_{t}\) is the estimated solution \(}_{t}=_{}\) where \(=*{argmax}_{i_{t}^{(S)}}_{t}(_{i})\), which is slightly modified from the theoretical value as discussed in Sec. 3 If we get that \(_{t}^{(S)}=\), then we set \(r_{t}=f(^{*})-_{}f()\). This definition is the same as the utility gap metric for constrained GP optimization . In each experiment, we report the average performance of \(20\) numbers of optimization trials with different seeds. In each trial, we generate one point uniformly at random over \(\) and use it as the initial point. Lastly, we add an artificial noise \(_{t}(0,^{2})\) with \(^{2}=0.0001\) to the observation of \(f\). We also fix the noise variance hyperparameter of the GP model for \(f\) to \(0.0001\).

Synthetic experiments with the Branin function.We perform synthetic benchmark experiments with the Branin function whose input space is scaled to \(^{2}\). The failure function used in these experiments is shown in Fig. 2. This failure function has a feasible region on the upper right side which is easily identifiable. In addition, the failure function also contains multiple small isolated feasible regions, one of which contains the optimal solution. We use the Gaussian kernel \(k(,)_{f}^{2}(-\|-\|_{2}^{2}/(2l_{f }^{2}))\) to construct a GP model of \(f\). The parameters \(_{f}\) and \(l_{f}\) are fixed beforehand by marginal likelihood maximization over a Sobol sequence of \(1024\) points. The subsequent experiments also use the Gaussian kernel; and the parameters are fixed as described above.

Figure 2 shows the result. Under this benchmark setting, F-GP-UCB has a better performance. We also give the detailed behavior of F-GP-UCB in Appendix G. We note that, in EI and GP-UCB, the regret stops decreasing in the early stage. This is because these algorithms get stuck on the same failure point.

Test functions for constrained optimization.We adopt the two benchmark settings which are used in existing GP-based constrained optimization literature. Gardner  is a 2D test problem whose objective and constraint functions are defined as a combination of sine and cosine functions. Hartmann  is a 3D test problem5 whose feasible region becomes a unit hypersphere. To adapt the constrained problem to our settings, the feasible region in each benchmark is defined as the success region, while the rest is defined as the failure region. Fig. 3 shows the result. Within the compared algorithms, F-GP-UCB has the best performance for the Gardner function, while EFI-GPC-Sign has the best performance for the Hartmann function. This can be understood from the fact that the feasible region of the Hartmann function, which is a unit hypersphere, and can be modeled easily by GPC. However, F-GP-UCB performs better compared to EFI-GPC-EP, which uses a classic GPC model. Moreover, the regret of F-GP-UCB continues to decrease as the number of iterations increases.

Numerical experiments with quasicrystals.We perform a numerical experiment involving quasicrystals in the Al-Cu-Mn system. We consider the optimization of the phonon thermal conductivity for quasicrystals. In this setting, the input space is the relative composition of the three elements, restricted to be around the feasible region of quasicrystal formation. The feasible region is the composition values that form quasicrystals based on data . Outside of this region, quasicrystals do not form and so we consider it as a failure region, as the material property could not be probed with methods designed for quasicrystals. For the objective function, we use an empirical formula for the phonon thermal conductivity which we extract from . The details of the setting are described in

Figure 3: Experiment results of benchmark function of the constrained optimization problems (left and middle) and quasicrystal simulation function (right). The left and middle plots show the result of Gardner and Hartmann test functions, respectively. The error bars correspond to two standard errors.

Figure 2: The left plot shows the Branin function whose input space is scaled to \(^{2}\). The shaded regions represent the failure regions. The right plot shows the regret in the synthetic problem using the Branin function, where the average of \(20\) experiments with different random seeds is shown. The error bars correspond to two standard errors.

Appendix H.2. In Fig. 3, the right plot shows the result of the numerical experiments for quasicrystals. It shows that F-GP-UCB performs the best among the compared algorithms.

## 7 Conclusions

In this paper, we propose a novel GP-based optimization algorithm in the presence of unknown failure regions in the search space. Our algorithm only requires a very mild assumption of the failure function that the optimal solution lies on an interior of a feasible region. We show that our algorithm achieves a convergence with high probability and provides the first regret upper bound by appropriately adjusting the search space. Its effectiveness is verified empirically through numerical experiments, including the heuristic simulation experiment motivated by the material research of quasicrystals.