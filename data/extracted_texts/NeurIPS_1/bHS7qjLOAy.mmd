# Riemannian Laplace approximations for Bayesian neural networks

Federico Bergamin, Pablo Moreno-Munoz, Soren Hauberg, Georgios Arvanitidis

Section for Cognitive Systems, DTU Compute, Technical University of Denmark

{fedbe, pabmo, sohau, gear}@dtu.dk

###### Abstract

Bayesian neural networks often approximate the weight-posterior with a Gaussian distribution. However, practical posteriors are often, even locally, highly non-Gaussian, and empirical performance deteriorates. We propose a simple parametric approximate posterior that adapts to the shape of the true posterior through a Riemannian metric that is determined by the log-posterior gradient. We develop a Riemannian Laplace approximation where samples naturally fall into weight-regions with low negative log-posterior. We show that these samples can be drawn by solving a system of ordinary differential equations, which can be done efficiently by leveraging the structure of the Riemannian metric and automatic differentiation. Empirically, we demonstrate that our approach consistently improves over the conventional Laplace approximation across tasks. We further show that, unlike the conventional Laplace approximation, our method is not overly sensitive to the choice of prior, which alleviates a practical pitfall of current approaches.

## 1 Introduction

_Bayesian deep learning_ estimates the weight-posterior of a neural network given data, i.e. \(p(|)\). Due to the generally high dimensions of the weight-space, the normalization of this posterior is intractable and approximate inference becomes a necessity. The most common parametric choice approximates the posterior with a Gaussian distribution, \(p(|) q(|)=(|,)\), which is estimated _variationally_(Blundell et al., 2015), using _Laplace approximations_(MacKay, 1992) or with other techniques (Madova et al., 2019). Empirical evidence, however, suggests that the log-posterior is not locally concave (Sagun et al., 2016), indicating that the Gaussian approximation is overly crude. Indeed, this approximation is known to be brittle as the associated covariance is typically ill-conditioned implying a suboptimal behavior (Daxberger et al., 2021; Farquhar et al., 2020), and for this reason, alternative approaches have been proposed to fix this issue (Mackay, 1992). Nonetheless, the Gaussian approximation is widely used due to the many benefits of parametric distributions, over e.g. _Monte Carlo sampling_(Neal, 1995) or _deep ensembles_(Lakshminarayanan et al., 2017).

**In this paper** we argue that the underlying issue is not with the Gaussian approximation, but rather with the weight-space over which the approximation is applied. We show that a Gaussian approximation can locally adapt to the loss by equipping the weight-space with a simple Riemannian metric and performing the approximation tangentially to the associated manifold. Practically, this ensures that samples from the Riemannian approximate posterior land in regions of weight-space yielding low training loss, which significantly improves over the usual Gaussian approximation. We obtain our

Figure 1: Our Riemannian Laplace approximation is a simple parametric distribution, which is shaped according to the local loss landscape through a Riemannian metric.

Riemannian approximate posterior using a generalization of the Laplace approximation (MacKay, 1992) to general Riemannian manifolds. Sampling from this distribution requires solving a system of ordinary differential equations, which we show can be performed efficiently by leveraging the structure of the used Riemannian metric and automatic differentiation. Empirically, we demonstrate that this significantly improves upon conventional Laplace approximations across tasks.

## 2 Background

Notation & assumptions.We consider independent and identically distributed (i.i.d.) data \(=\{_{n},_{n}\}_{n=1}^{N}\), consisting of inputs \(^{D}\) and outputs \(^{C}\). To enable _probabilistic modeling_, we use a likelihood \(p(|f_{}())\) which is either Gaussian (regression) or categorical (classification). This likelihood is parametrized by a deep neural network \(f_{}:^{D}^{C}\), where \(^{K}\) represent the weights for which we specify a Gaussian prior \(p()\). The predictive distribution of a new test point \(^{}\) equals \(p(|^{})= p(|^{}, )p(|)\) where \(p(|)\) is the true weight-posterior given the data \(\). To ensure tractability, this posterior is approximated. This paper focuses on the Laplace approximation, though the bulk of the methodology applies to other approximation techniques as well.

### The Laplace approximation

The Laplace approximation (LA) is widely considered in _probabilistic_ models for approximating intractable densities (Bishop, 2007). The idea is to perform a second-order Taylor expansion of an unnormalized log-probability density, thereby yielding a Gaussian approximation. When considering inference of the true posterior \(p(|)\), LA constructs an approximate posterior distribution \(q_{}(|)=(|_{*},)\) that is centered at the _maximum a-posteriori_ (MAP) estimate

\[_{*}=*{arg\,max}_{}\{ p(| )\}=*{arg\,min}_{}^{N} p(_{n}_{n},)- p() \}}_{()}. \]

A Taylor expansion around \(_{*}\) of the regularized loss \(()\) then yields

\[}()(_{*})+ _{}()_{=_{*}},(-_{ *})+(-_{*}),_{}[]()_{=_{*}}(-_{*}), \]

where we know that \(_{}()_{=_{*}} 0\), and \(_{}[]()^{K K}\) denotes the Hessian of the loss. This expansion suggests that the approximate posterior covariance should be the inverse Hessian \(=_{}[]()_{=_{*}}^{-1}\). The marginal likelihood of the data is then approximated as \(p()(-(_{*}))(2)^{D/2}()^{ 1/2}\). This is commonly used for training hyper-parameters of both the likelihood and the prior (Immer et al., 2021; Antoran et al., 2022). We refer to appendix A for further details.

Tricks of the trade.Despite the simplicity of the Laplace approximation, its application to modern neural networks is not trivial. The first issue is that the Hessian matrix is too large to be stored in memory, which is commonly handled by approximately reducing the Hessian to being diagonal, low-rank, Kronecker factored, or only considered for a subset of parameters (see Daxberger et al. (2021) for a review). Secondly, the Hessian is generally not positive definite (Sagun et al., 2016), which is commonly handled by approximating the Hessian with the generalized Gauss-Newton approximation (Foresee and Hagan, 1997; Schraudolph, 2002). Furthermore, estimating the predictive distribution using Monte Carlo samples from the Laplace approximated posterior usually performs poorly (Lawrence, 2001; Chapter 5)(Ritter et al., 2018) even for small models. Indeed, the Laplace approximation can place probability mass in low regions of the posterior. A solution, already proposed by (Mackay, 1992; Chapter 4), is to consider a first-order Taylor expansion around \(_{*}\), and use the sample to use the "linearized" function \(f_{}^{}()=f_{_{*}}()+_{ }f_{}()_{=_{*}},-_{*}\) as predictive, where \(_{}f_{}()_{=_{*}} ^{C K}\) is the Jacobian. Recently, this approach has been justified by Khan et al. (2019); Immer et al. (2021), who proved that the generalized Gauss-Newton approximation is the exact Hessian of this new linearized model. Even if this is a linear function with respect to the parameters \(\), empirically it achieves better performance than the classic Laplace approximation.

Although not theoretically justified, optimizing the prior precision post-hoc has been shown to play a crucial role in the Laplace approximation (Ritter et al., 2018; Kristiadi et al., 2020; Immer et al., 2021; Daxberger et al., 2021). This is usually done either using cross-validation or by maximizing the log-marginal likelihood. In principle, this regularizes the Hessian, and the associated approximate posterior concentrates around the map estimate.

Strengths & weaknesses.The main strength of the Laplace approximation is its simplicity in implementation due to the popularization of automatic differentiation. The Gaussian approximate posterior is, however, quite crude and often does not capture the shape locally of the true posterior (Sagun et al., 2016). Furthermore, the common reduction of the Hessian to not correlate all model parameters limit the expressive power of the approximate posterior.

## 3 Riemannian Laplace approximations

We aim to construct a parametric approximate posterior that better reflects the local shape of the true posterior and captures nonlinear correlations between parameters. The basic idea is to retain the Laplace approximation but change the parameter space \(\) to locally encode the _training loss_. To realize this idea, we will first endow the parameter space with a suitable Riemannian metric (Sec. 3.1) and then construct a Laplace approximation according to this metric (Sec. 3.2).

### A loss-aware Riemannian geometry

For a given parameter value \(\), we can measure the training loss \(()\) of the associated neural network. Assuming that the loss changes smoothly with \(\), we can interpret the loss surface \(=g()=[,()]^{K+1}\) as a \(K\)-dimensional manifold in \(^{K+1}\). The goal of Riemannian geometry (Lee, 2019; de Carmo, 1992) is to do calculations that are restricted to such manifolds.

The metric.We can think of the parameter space \(\) as being the _intrinsic coordinates_ of the manifold \(\), and it is beneficial to do all calculations directly in these coordinates. Note that a vector tangential to the manifold can be written as \(_{g}()^{K+1}\), where \(_{g}:^{K+1 K}\) is the Jacobian of \(g\) that spans the tangent space \(_{g()}\) at the point \(g()\) and \(^{K}\) is the vector of _tangential coordinates_ for this basis of the tangent space. We can take inner products between two tangent vectors in the same tangent space as \(_{g}()_{1},_{g}() _{2}=_{1}^{}_{g}()_{2}\), which, we note, is now expressed directly in the intrinsic coordinates. From this observation, we define the _Riemannian metric_\(()=_{g}()^{}_{g}()\), which gives us a notion of a local inner product in the intrinsic coordinates of the manifold (see ellipsoids in Fig. 2). The Jacobian of \(g\) is particularly simple \(_{g}()=[_{K},_{}]\), such that the metric takes the form

\[()=_{K}+_{}()_{ }()^{}. \]

The exponential map.A local inner product allows us to define the length of a curve \(c:\) as \([c]=_{0}^{1}(t),(c(t)) (t)}t\), where \((t)=_{t}c(t)\) is the velocity. From this, the _distance_ between two points can be defined as the length of the shortest connecting curve, where the latter is known as the _geodesic curve_. Such geodesics can be expressed as solutions to a system of second-order non-linear ordinary differential equations (odes), which is given in appendix B alongside further details on geometry. Of particular interest to us is the _exponential map_, which solves these odes subject to an initial position and velocity. This traces out a geodesic curve with a given starting point and direction (see Fig. 2). Geometrically, we can also think of this as mapping a tangent vector _back to the manifold_, and we write the map as \(:_{}\).

The tangential coordinates \(\) can be seen as a coordinate system for the neighborhood around \(\), and since the exponential map is locally a bijection we can represent any point locally with a unique tangent vector. However, these coordinates correspond to the tangent space that is spanned by \(_{g}()\), which implies that by changing this basis the associated coordinates change as well. By

Figure 2: The parameter space \(\) of the bnn together with examples of the Riemannian metric and the exponential map. Note that the Riemannian metric adapts to the shape of the loss which causes the geodesic to follow its shape.

orthonormalizing this basis we get the _normal coordinates_ where the metric vanishes. Let \(\) the tangential coordinates and \(}\) the corresponding normal coordinates, then it holds that

\[,()=}, }=()} ()=()^{-}{{2}}}. \]

We will use the normal coordinates when doing Taylor expansions of the log-posterior, akin to standard Laplace approximations.

### The proposed approximate posterior

In order to Taylor-expand the loss according to the metric, we first express the loss in normal coordinates of the tangent space at \(_{*}\), \(h(})=(_{_{*}}((_{*})^ {-}{{2}}}}))\). Following the standard Laplace approximation, we perform a second-order Taylor expansion of \(h\) as

\[(}) h(0)+_{}h(})_{}=0},}+ },_{}}[h](}) _{}=0}}, \]

where \(_{}h(})_{}=0}=(_{*})}_{}()_{= _{*}} 0\) as \(_{*}\) minimize the loss and \(_{}}[h](})_{}=0}= (_{*})}_{}[]() (_{*})_{=_{*}}\) with \(_{}[]()\) the standard Euclidean Hessian matrix of the loss. Further details about this step can be found in appendix B.

**Tangential Laplace.** Similar to the standard Laplace approximation, we get a Gaussian approximate posterior \((})=(} 0,\ )\) on the tangent space in the normal coordinates with covariance \(=_{}}[h]()_{}=0}^{-1}\). Note that changing the normal coordinates \(}\) to tangential coordinates \(\) is a linear transformation and hence \((0,(_{*})} (_{*})})\), which means that this covariance is equal to \(_{}[]()_{=_{*}}^{-1}\) since \((_{*})\) is a symmetric matrix, and hence, it cancels out. The approximate posterior \(q_{}()=( 0,)\) in tangential coordinates, thus, matches the covariance of the standard Laplace approximation.

**The predictive posterior.** We can approximate the predictive posterior distribution using Monte Carlo integration as \(p(y|^{},)= p(y|^{},,)q()= p(y|^{},, _{_{*}}())q_{}()_{s=1}^{S}p(y|^{},,_{ _{*}}(_{s})),\ _{s} q_{}()\). Intuitively, this generates tangent vectors according to the standard Laplace approximation and maps them back to the manifold by solving the geodesic ode. This lets the Riemannian approximate posterior take shape from the loss landscape, which is largely ignored by the standard Laplace approximation. We emphasize that this is a general construction that applies to the same Bayesian inference problems as the standard Laplace approximation and is not exclusive to Bayesian neural networks.

The above analysis also applies to the linearized Laplace approximation. In particular, when the \(f_{}^{}()\) is considered instead of the \(f_{}()\) the loss function in (1) changes to \(^{}()\). Consequently, our Riemannian metric is computed under this new loss, and \(_{}^{}()\) appears in the metric (3).

**Example.** To build intuition, we consider a logistic regressor on a linearly separable dataset (Fig. 3). The likelihood of a point \(^{2}\) to be in one class is \(p(C=1|)=(^{}+b)\), where \(()\) is the sigmoid function, \(^{2}\) and \(b\). After learning the parameters, we fix \(b_{*}\) and show the posterior with respect to \(\) together with the corresponding standard Laplace approximation (Fig. 3a).

Figure 3: The la assigns probability mass to regions where the true posterior is nearly zero, and a sample from this region corresponds to a poor classifier. Considering this sample as the initial velocity for the exponential map, the generated sample falls within the true posterior and the associated classifier performs well. As a result, our model quantifies better the uncertainty.

We see that the approximation assigns significant probability mass to regions where the true posterior is near-zero, and the result of a corresponding sample is a poor classifier (Fig. 2(b)). Instead, when we consider this sample as the initial velocity and compute the associated geodesic with the exponential map, we generate a sample at the tails of the true posterior which corresponds to a well-behaved model (Fig. 2(c)). We also show the predictive distribution for both approaches and even if both solve easily the classification problem, our model better quantifies uncertainty (Fig. 2(e)).

### Efficient implementation

Our approach is a natural extension of the standard Laplace approximation, which locally adapts the approximate posterior to the true posterior. The caveat is that computational cost increases since we need to integrate an ode for every sample. We now discuss partial alleviations.

Integrating the ode.In general, the system of second-order nonlinear odes (see appendix B for the general form) is non-trivial as it depends on the geometry of the loss surface, which is complicated in the over-parametrized regime (Li et al., 2018). In addition, the dimensionality of the parameter space is high, which makes the solution of the system even harder. Nevertheless, due to the structure of our Riemannian metric (3), the ode simplifies to

\[(t)=-_{}(c(t))(1+_{}(c(t))^{-1.5mu }_{}(c(t)))^{-1} (t),_{}[](c(t))(t), \]

which can be integrated reasonably efficiently with standard solvers. In certain cases, this ode can be further simplified, for example when we consider the linearized loss \(^{}()\) and Gaussian likelihood.

Automatic-differentiation.The ode (6) requires computing both gradient and Hessian, which are high-dimensional objects for modern neural networks. While we need to compute the gradient explicitly, we do not need to compute and store the Hessian matrix, which is infeasible for large networks. Instead, we rely on modern automatic-differentiation frameworks to compute the Hessian-vector product between \(_{}[](c(t))\) and \((t)\) directly. This both reduces memory use, increases speed, and simplifies the implementation.

Mini-batching.The cost of computing the metric, and hence the ode scales linearly with the number of training data, which can be expensive for large datasets. A reasonable approximation is to mini-batch the estimation of the metric when generating samples, i.e. construct a batch \(\) of \(B\) random data points and use the associated loss in the ode (6). As usual, we assume that \(()(N/B)_{}()\). Note that we only mini-batch the metric and not the covariance of our approximate posterior \(q_{T}()\).

We analyze the influence of mini-batching in our methods and provide empirical evidence in Fig. 4. In principle, the geometry of the loss surface \(()\) controls the geodesics via the associated Riemannian metric, so when we consider the full dataset we expect the samples to behave similarly to \(f_{_{*}}()\). In other words, our approximate posterior generates weights near \(_{*}\) resulting in models with similar or even better loss. When we consider a batch the geometry of the associated loss surface \(_{}()\) controls the generated geodesic. So if the batch represents well the structure of the full dataset, then the resulting model will be meaningful with respect to the original problem, and in addition, it may exhibit some variation that is beneficial from the Bayesian perspective for the quantification of the uncertainty. The same concept applies in the linearized version, with the difference that when the full dataset is considered the geometry of \(^{}()\) may over-regularize the geodesics. Due to the linear nature of \(f_{}^{}()\) the associated Riemannian metric is small only close to \(_{*}\) so the generated samples are similar to \(f_{_{*}}()\). We relax this behavior and potentially introduce variations in the resulting models when we consider a different batch whenever we generate a sample. Find more details in appendix D.

Figure 4: Analysis of mini-batching

Related work

**Bayesian neural networks.** Exact inference for bnn is generally infeasible when the number of parameters is large. Several methods rely on approximate inference, which differs in their trade-off between computational cost and the goodness of the approximation. These techniques are usually based on the Laplace approximation (MacKay, 1992), variational inference (Graves, 2011; Blundell et al., 2015; Khan et al., 2018), dropout (Gal and Ghahramani, 2016), stochastic weight averaging (Izmailov et al., 2018; Maddox et al., 2019) or Monte Carlo based methods (Neal, 1995), where the latter is often more expensive.

**Laplace approximations.** In this work, we are primarily focused on Laplace approximations, although the general geometric idea can be used in combination with any other inference approach listed above. Particularly, Laplace's method for bnn was first proposed by Mackay (1992) in his _evidence_ framework, where a closed-form approximation of predictive probabilities was also derived. This one uses a first-order Taylor expansion, also known as _linearization_ around the map estimate. For long, Laplace's method was infeasible for modern architectures with large networks due to the exact computation of the Hessian. The seminal works of Martens and Grosse (2015) and Botev et al. (2017) made it possible to approximate the Hessian of large networks, which made Laplace approximations feasible once more (Ritter et al., 2018). More recently, the Laplace approximation has become a go-to tool for turning trained neural networks into bnn in a _post-hoc_ manner, thanks to easy-to-use software (Daxberger et al., 2021) and new approaches to scale up computation (Antoran et al., 2022). In this direction, other works have only considered a subset of the network parameters (Daxberger et al., 2021, Sharma et al., 2023), especially the last-layer. This is _de facto_ the only current method competitive with _ensembles_(Lakshminarayanan et al., 2017).

**Posterior refinement.** Much work has gone into building more expressive approximate posteriors. Recently, Kristiadi et al. (2022) proposed to use normalizing flows to get a non-Gaussian approximate distribution using the Laplace approximation as a base distribution. Although this requires training an additional model, they showed that few bijective transformations are enough to improve the last-layer posterior approximation. Immer et al. (2021), instead, propose to refine the Laplace approximation by using Gaussian variational Bayes or a Gaussian process. This still results in a Gaussian distribution, but it has proven beneficial for linearized Laplace approximations. Other approaches rely on a mixture of distributions to improve the goodness of the approximation. Miller et al. (2017) expand a variational approximation iteratively adding components to a mixture, while Eschenhagen et al. (2021) use a weighted sum of posthoc Laplace approximations generated from different pre-trained networks. Havasi et al. (2021), instead, introduces auxiliary variables to make a local refinement of a mean-field variational approximation.

**Differential geometry.** Differential geometry is increasingly playing a role in inference. Arvanitidis et al. (2016) make a Riemannian normal distribution locally adapt to data by learning a suitable Riemannian metric from data. In contrast, our metric is derived from the model. This is similar in spirit to work that investigates pull-back metrics in latent variable models (Tosi et al., 2014; Arvanitidis et al., 2018; Hauberg, 2018). A similar Riemannian metric has been used as a surrogate for the Fisher metric for Riemannian Hamiltonian Monte-Carlo sampling (Hartmann et al., 2022). In addition to that, the geometry of the latent parameter space of neural networks was recently analyzed by Kristiadi et al. (2023) focusing on the invariance of flatness measures with respect to re-parametrizations. Finally, we note that Hauberg (2018) considers Laplace approximations on the sphere as part of constructing a recursive Kalman-like filter.

## 5 Experiments

We evaluate our Riemannian la (riem-la) using illustrative examples, image datasets where we use a convolutional architecture, and real-world classification problems. We compare our method and its linearized version to standard and linearized la. All predictive distributions are approximated using Monte Carlo (MC) samples. Although last-layer la is widely used lately, we focus on approximating the posterior of all the weights of the network. In all experiments, we maximize the marginal log-likelihood to tune the hyperparameters of the prior and the likelihood as proposed in (Daxberger et al., 2021). To evaluate the performance in terms of uncertainty estimation we considered the standard metrics in the literature: negative log-likelihood (NLL), the Brier score (BRIER), the expected calibration error (ECE), and the maximum calibration error (MCE). More experiments are available in appendix D together with the complete training and modeling details. In appendix C.2 we analyze the runtime to compute the exponential map over different dataset and model sizes. Code to reproduce the results is publicly available at [https://github.com/federicobergamin/riemannian-laplace-approximation](https://github.com/federicobergamin/riemannian-laplace-approximation)

### Regression problem

We consider the toy-regression problem proposed by Snelson and Ghahramani (2005). The dataset contains 200 data points, and we randomly pick 150 examples as our training set and the remaining 50 as a test set. As shown by Lawrence (2001), Ritter et al. (2018), using samples from the LA posterior performs poorly in regression even if the Hessian is not particularly ill-conditioned, i.e. when the prior precision is optimized. For this reason, the linearization approach is necessary for regression with standard LA. Instead, we show that even our basic approach fixes this problem when the prior is optimized. We tested our approach by considering two fully connected networks, one with one hidden layer with 15 units and one with two layers with 10 units each, both with tanh activations. Our approach approximates well the true posterior locally, so the resulting function samples follow the data. Of course, if the Hessian is extremely degenerate our approach also suffers, as the initial velocities are huge. When we consider the linearized version of our approach the result is the same as the standard LA-linearization, which we include in the appendix D, where we also report results for in-between uncertainty as proposed by Foong et al. (2019) and a comparison with Hamiltonian Monte Carlo.

### Classification problems

Illustrative example.We consider a 2-dimensional binary classification problem using the banana dataset which is shown in Fig. 6. We train a 2-layer fully connected neural net with 16 hidden units per layer and tanh activation. For all methods, we use 100 MC samples for the predictive distribution.

As in regression, direct samples from the vanilla la lead to a really poor model (Fig. 5(a)) with high uncertainty both within and away from the data support. Instead, the other three methods (Fig. 5(b)-5(d)) show a better-behaved confidence that decreases outside of the data support. This is also supported by the metrics in Table 1, where remarkably riem-la performs better in terms of NLL and Brier score on a separate test set.

Figure 5: Posterior samples under a simple _(top)_ and an overparametrized model _(bottom)_. Vanilla la is known to generate bad models, while our samples from riem-la quantify well the uncertainty.

Figure 6: Binary classification confidence estimate using \(100\) Monte-Carlo samples on the banana dataset. Vanilla la underfit, while the other three methods are able to be certain within the data and uncertain far away. Note, for linearized riem-la we solve the expmap using a different subset of the data. Confidence plots of all different methods can be found in the supplementary material. Black lines are the decision boundaries.

[MISSING_PAGE_FAIL:8]

approach is beneficial in producing better-calibrated models in both datasets. This holds both for our approach linearized riem-la and the standard l.a. Optimizing the prior precision post-hoc is crucial for the vanilla la and associated results can be seen in appendix D. Instead, both our methods appear to be robust and consistent, as they achieve similar performance no matter if the prior precision is optimized or not.

Note that for the mini-batches for our approaches, we consider 20% of the data by randomly selecting 1000 observations while we respect the label frequency based on the full dataset. Clearly, the batch-size is a hyperparameter for our methods and can be estimated systematically using cross-validation. Even if we do not optimize this hyperparameter, we see that our batched version of riem-la and lin-riem-la perform better than the standard la and on-par with our lin-riem-la without batches, implying that a well-tuned batch-size can potentially further improve the performance. Nevertheless, this also shows that our method is robust with respect to the batch-size.

## 6 Conclusion and future directions

We propose an extension to the standard Laplace approximation, which leverages the natural geometry of the parameter space. Our method is parametric in the sense that a Gaussian distribution is estimated using the standard Laplace approximation, but it adapts to the true posterior through a nonparametric Riemannian metric. This is a general mechanism that, in principle, can also apply to, e.g., variational approximations. In a similar vein, while the focus of our work is on Bayesian neural networks, nothing prevents us from applying our method to other model classes.

Empirically, we find that our Riemannian Laplace approximation is better or on par with alternative Laplace approximations. The standard Laplace approximation crucially relies on both linearization and on a fine-tuned prior to give useful posterior predictions. Interestingly, we find that the Riemannian Laplace approximation requires neither. This could suggest that the standard Laplace approximation has a rather poor posterior fit, which our adaptive approach alleviates.

Limitations.The main downside of our approach is the computational cost involved in integrating the ode, which is a common problem in computational geometry (Arvanitidis et al., 2019). The cost of evaluating the ode scales linearly with the number of observations, and in particular it is \(O(SWN)\), where \(S\) is the number of steps of the solver, \(W\) is the number of model parameters and \(N\) is the dataset size. Indeed, the solver needs all the training data points to compute both gradient and hvp at each step. We refer to appendix C.1 for a detailed explanation. For big datasets, we have considered

    &  \\  method & Accuracy \(\) & NLL\(\) & Brier\(\) & ECE\(\) & MCE\(\) \\  map & \(95.02 0.17\) & \(0.167 0.005\) & \(0.0075 0.0002\) & \(1.05 0.14\) & \(39.94 14.27\) \\ vanilla la & \(88.69 1.84\) & \(0.871 0.026\) & \(0.0393 0.0013\) & \(42.11 1.22\) & \(50.52 1.45\) \\ lin-la & \(94.91 0.26\) & \(0.204 0.006\) & \(0.0087 0.0003\) & \(6.30 0.8\) & \(39.30 16.77\) \\ riem-la & \(\) & \(\) & \(\) & \(2.48 0.06\) & \(38.03 15.02\) \\ riem-la (batches) & \(95.67 0.19\) & \(0.170 0.005\) & \(0.0072 0.0002\) & \(5.40 0.06\) & \(22.40 0.51\) \\ lin-riem-la & \(95.44 0.18\) & \(0.149 0.004\) & \(0.0068 0.0003\) & \(\) & \(39.40 14.75\) \\ lin-riem-la (batches) & \(95.14 0.20\) & \(0.167 0.004\) & \(0.0076 0.0002\) & \(3.23 0.04\) & \(\) \\    
    &  \\  Method & Accuracy \(\) & NLL\(\) & Brier\(\) & ECE\(\) & MCE\(\) \\  map & \(79.88 0.09\) & \(0.541 0.002\) & \(0.0276 0.0000\) & \(\) & \(24.07 1.50\) \\ vanilla la & \(74.88 0.83\) & \(1.026 0.046\) & \(0.0482 0.0019\) & \(31.63 1.28\) & \(43.61 2.95\) \\ lin-lathe "obvious" trick of using a random (small) batch of the data when solving the ODE to reduce the complexity. Empirically, we find that this introduces some stochasticity in the sampling, which sometimes is beneficial to explore the posterior distribution better and eventually boost performance, motivating further research. The computational cost also grows with the dimensionality of the parameter space, as the number of necessary solver steps increases, as well as the cost of the Hessian vector products. Our implementation relies on an off-the-shelf ODE solver which runs on the CPU while our automatic-differentiation based approach runs on the GPU, which is inefficient. We expect significant improvements by using an ODE solver that runs exclusively on GPU, while tailor-made numerical integration methods are also of particular interest.

Future directions.We showed that equipping the weight-space with a simple Riemannian metric is a promising approach that solves some of the classic LA issues. We believe that this opens up different interesting research directions that either aim to use different metrics instead of the one used in this work or to make this method scale to bigger dataset. Regarding the metric, a possible extension of this work would be to consider the Fisher information matrix in the parameter space. Potential ideas to improve efficiency is to consider approximations of the Riemannian metric leading to simpler ODE systems, for example by using the KFAC instead of the full-Hessian in (6). Another directions, instead, would be to focus on developing a better solver. Indeed, we know the structure and behavior of our ODE system, i.e. geodesics start from low loss which increases along the curve, therefore, a potential direction would be to develop solvers that exploit this information. Usually, general purpose solvers aim for accuracy, while in our case even inexact solutions could be potentially useful if computed fast.