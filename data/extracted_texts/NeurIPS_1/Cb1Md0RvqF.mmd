# UV-free Texture Generation with Denoising and Geodesic Heat Diffusions

**Simone Foti** **Stefanos Zafeiriou** **Tolga Birdal**

Department of Computing

Imperial College London

**Abstract**

Seams, distortions, wasted UVspace, vertex-duplication, and varying resolution over the surface are the most prominent issues of the standard UV-based texturing of meshes. These issues are particularly acute when automatic UV-unwrapping techniques are used. For this reason, instead of generating textures in automatically generated UV-planes like most state-of-the-art methods, we propose to represent textures as coloured point-clouds whose colours are generated by a denoising diffusion probabilistic model constrained to operate on the surface of 3D objects. Our sampling and resolution agnostic generative model heavily relies on heat diffusion over the surface of the meshes for spatial communication between points. To enable processing of arbitrarily sampled point-cloud textures and ensure long-distance texture consistency we introduce a fast re-sampling of the mesh spectral properties used during the heat diffusion and introduce a novel heat-diffusion-based self-attention mechanism. Our code and pre-trained models are available at github.com/simofoti/UV3-TeD.

Figure 1: Random textures generated by our method, _Uv_3-TeD, on the surface of general objects from the Amazon Berkeley Object dataset and of chairs from ShapeNet (miniatures on the shelves).

Introduction

Meshes are surface discretisations intentionally designed to represent the geometry of 3D objects. Since real objects are more than just their geometry, meshes are frequently augmented with textures, representing appearances. Currently, textures are mostly represented as images that can be wrapped on the mesh via a \(\)_-mapping_ procedure that maps every point on the surface of the mesh into a point on the \(\)-image-plane where texture information are stored. Textures are therefore mostly generated on images, but considering that \(\)-maps intrinsically suffer from distortions, seams, wasted \(\)-space, vertex-duplication, and varying resolution , is \(\)-mapping really the best approach? In this work, we wonder: what if instead of generating a texture on a plane and then wrapping it onto a shape we could directly generate a texture on the curved surface of the object?

The capability of generating textures by avoiding \(\)-unwrapping and mapping altogether alleviates the post-processing issues caused by the \(\)-mapping and can save time and resources for many 3D artists, while generally improving the realism of the textures by avoiding distortions, seams, or stretching artefacts. In addition, it could enable the creation of priors for a variety of computer vision tasks ranging from shape and appearance reconstruction to object detection, identification, and tracking. More generally, a texture representation adhering to the actual geometry of the surface could result in smaller memory footprints without compromising on the rendering quality.

While most state-of-the-art methods focus on generating textures in \(\)-space [23; 7; 62; 56; 13; 34; 12; 42; 41] and thus inherit all the drawbacks of \(\)-mapping (Fig. 2, _left_), we propose to represent textures with unstructured point-clouds sampled on the surface of an object and devise a technique to render them without going through \(\)-mapping (Fig. 2, _right_). We generate point-cloud textures with a denoising diffusion probabilistic model operating exclusively on the surface of the meshes. This is fundamentally different from generating coloured point-clouds  or triplane-based implicit textures : while our work respects the geodesic information provided by the meshes, they operate in the Euclidean space and ignore the topology of the objects they seek to texturise. When compared to methods like , which can generate textures directly on the surface, our method has the advantage of not requiring any remeshing operation and being adaptable to different sampling resolutions. In addition, unlike many other texture generation methods [7; 12; 42; 49], we generate albedo textures that by not factoring in the environment can be rendered with different lighting conditions to achieve more photorealistic results (Fig. 1). Finally, while most methods are class-specific, our method can be trained on datasets containing objects of different classes (Fig. 1). Our key contributions are:

1. We create a denoising diffusion probabilistic model generating point-cloud textures by operating only on the surface of the meshes,
2. We introduce a novel attention layer based on heat diffusion and farthest point sampling to improve the recently proposed DiffusionNet blocks  by facilitating global communication across the entire surface of the object.
3. We propose a mixed Laplacian operator to ensure that heat diffusion can spread even in the presence of topological errors and disconnected components while still mostly relying on the provided topological information,
4. We devise an online sampling strategy that allows us to sample point-clouds and their spectral properties during training without requiring to recompute them from scratch.

## 2 Related Work

**Texture Representations and Rendering**. Textures are traditionally represented by images which are mapped onto 3D shapes via a \(\)-mapping. Since manual \(\)-mapping is complex and labour-intensive, many methods tried to perform it automatically while attempting to address some of the most common artefacts: seams and distortion [44; 39]. Other texture representations such as [19; 50; 2; 61] have been proposed without finding a level of adoption comparable to the one of \(\)-textures, which are still the de facto standard for modelling the appearance of meshes. Although arguably the most convenient shape representation for computer graphics applications, meshes are not the only data structure used to represent 3D shapes. In fact, point-clouds are equally spread and they can also be associated with appearance information, stored as colour values associated to each point. Many techniques have tried to reduce their sparsity while rendering [46; 45; 10], but when photo-realism is required, textured meshes are still a superior representation which can better approximate continuous surfaces. Given the strengths and limitations of both representations, we propose to adopt a hybrid approach where the geometry is represented by a mesh and its appearance by an unstructured point-cloud texture. This can prevent seams, distortions, unused UV-space, and varying resolution while still enabling the rendering of continuous surfaces. Hybrid approaches mixing meshes and point-clouds have also been proposed by  and . However, in  the appearance was present in both representations, which were both rendered, and  used highly structured point-clouds with points regularly positioned on the mesh faces. The representation of  can be used only for high-resolution textures requiring significantly more points than the number of vertices, thus making it incompatible with current geometric deep learning models. For this reason, we use unstructured point-cloud textures arbitrarily sampled at any required resolution.

**Texture Generation**. Many texture generation techniques have been proposed, each relying on a different representation. Considering the wide adoption of UV-textures, and the maturity of deep-learning techniques operating on the image domain, it is not surprising that most methods still rely on UV-mapping and generate UV-images to wrap on meshes , or simultaneously optimise meshes and texture to achieve the desired result . Alternatively, another common approach consists in using a generative model to generate depth-conditioned images from multiple viewpoints. These images are then projected onto a mesh, refined, and stored as UV-textures . Other methods try to map textures on UV-spheres , tri-planes , NeRFs , or implicit functions , but they either fail to operate on the real geometry of the object or they end up-projecting the textures on a UV-plane. Even Point-UV Diffusion , whose coarse stage is conceptually similar to ours because it generates coloured point-clouds using point-cloud operators, effectively projects points in UV-textures that are refined with image diffusion models. Unfortunately, especially at the coarse stage, this often results in the visible artefacts that are typical of this parametrisation (Fig. 2). A method capable of operating directly on the surface of the objects is Texturify , which shows remarkable results adopting a shape-conditioned Style-GAN convolving coloured quad-faces. The main limitation of this method is its need to uniformly re-mesh the input shapes to a fixed resolution, potentially affecting the quality of the original mesh. Most of these methods are trained on class-specific shapes suggesting their difficulties in dealing with multi-class datasets. While our method can actually operate on datasets with shapes belonging to different classes, some methods exacerbate the single-class limitation, focusing exclusively on training their models on single shapes to then generate texture variations . Similarly, manifold diffusion fields  are capable of generating continuous functions -such as textures- over Riemannian manifolds. Unfortunately, they generate functions only on manifold meshes and they are always trained on single-shape datasets with multiple function variations. It is unclear whether their method would be capable of generalising to different geometries.

## 3 Notation and Background

We define a mesh as \(=\{,\}\), with \(^{V 3}\) representing the positions of the \(V\) vertices sampled on the surface (\(\)) of a shape, and \(^{F 3}\) the set of triangular faces describing the connectivity of the vertices. Throughout this work, we assume that the vertex positions and the mesh topology are given, but different for every shape we want to texturise. We think of textuers as continuous functions \(x:\) mapping points on \(\) to a signal space \(\) that, without any loss of generality, corresponds to the albedo colours, _i.e._, \(=^{3}\). In practice, we operate on textures defined as coloured point-clouds with colours \(=x()^{P 3}\), where \(^{P 3}\) represents the \(P\) 3D coordinates of the points sampled on \(\). Note that, in general, we assume \(\) (and \(P V\)) as the texture point-clouds do not necessarily need to follow the same vertex distribution.

Figure 2: Qualitative comparison between point-cloud-textures (_top-right_ halves) and automatically wrapped UV-textures (_bottom-left_ halves). All textures were generated by Point-UV Diffusion in order to showcase some of the most common issues of UV-mapping. Although the method generated good quality textures as point-clouds, projecting them in UV-space introduces significant artefacts.

Before introducing the main contributions of our work in Sec. 4, we formalise two pillars on which we base our method: the denoising diffusion models and the Laplace Beltrami operator.

**Denoising Diffusion Probabilistic Model**. Denoising Diffusion Probabilistic Models  (DDPMs) are now a well-established class of generative models that rapidly found adoption across different fields[11; 31]. They are parameterised by a Markov chain trained using variational inference and are essentially characterised by three steps: a forward noising procedure, a backward denoising, and a sampling procedure that is used during inference. During the forward process, a training sample \(_{0} p(_{0})\) corresponding to a point-cloud texture and coming from the original textures distribution at timestep \(0\) is iteratively perturbed to \(\{_{t}\}_{t=1}^{T}\) by progressively adding a small amount of isotropic Gaussian noise \(_{0}(,)\). Being \(p(_{t}|_{t-1})(_{t};}_{t},_{t})\) a single step in the discrete forward chain with noise schedule \(_{t}\), we represent the full chain as \(p(_{T}|_{0})=_{t=1}^{T}p(_{t}|_{t -1})\). Similarly, a generic step in the chain can be obtained as \(p(_{t}|_{0})(_{t};_{t}}_{t},(1-_{t}))\), where \(\) is the identity matrix, \(_{t}=_{s=1}^{t}_{s}\), and \(_{t}=1-_{t}\). This implies that \(_{t}=_{t}}_{0}+_{t}} _{0}\). In DDPM models like ours, the reverse process is parameterised by a neural network trained to predict the noise that needs to be progressively removed. This process is formulated as \(p_{}(_{t-1}|_{t})_{t-1};_{t}}}_{t}-}{_{t}}}_{}(_{t},t), _{t}\), where the variance is empirically fixed and the mean is leveraging the noise prediction network \(_{}(_{t},t)\). The variational inference objective can thus be simplified to \(=_{_{t},t}\|_{t}-_{ }(_{t},t)\|_{2}^{2}\). Finally, the sampling process follows the reverse process where the trained network transforms noise samples coming from the terminal distribution \(_{T} p(_{T})\) into the denoised \(}_{0} p_{}(_{0}) p(_{0})\).

**Eigenproperties of Laplace Beltrami Operator**. The Laplace Beltrami operator (LBO) plays an essential role in geometry processing. For triangle meshes, the LBO is usually based on a cotangent formulation [38; 63] derived from finite element analysis. The cotangent Laplacian \(^{V}\) is a sparse matrix with elements proportional to the cotangent of the angles subtended by the edges, and it is associated to a diagonal mass matrix \(^{V}\) whose diagonal elements are proportional to the total area of the faces surrounding each vertex . The eigendecomposition of the LBO, \(=\), determines a set of orthonormal eigenvectors \([_{k}]_{k=1}^{K}^{V  K}\) corresponding to the \(K\) smallest eigenvalues \([_{k}]_{k=1}^{K}^{K}\) of the weak Laplacian and its mass matrix. These eigenvalues and eigenvectors have been intensively studied in spectral geometry because not only can they be used as global and local shape descriptors, but they can also be used to formulate surface operations such as heat diffusion . As the name suggests, heat diffusion regulates the physical heat dispersion. This phenomenon can be modelled on any discrete surface representation with a Laplacian operator and it is resolution, sampling, and representation independent.

## 4 UV-free Texture Diffusion (UV3-TeD)

We now describe UV3-TeD, our generative model for learning point-cloud textures built upon heat-diffusion-based operators specifically designed to operate on the surface of the input shapes (detailed in Sec. 4.1 and depicted in Fig. 4). Our diffusion model UV3-TeD operates on a noised version of the colours, \(_{t}\), and predicts a denoised \(_{t-1}\) through a U-Net  shaped architecture (Fig. 9), backed by novel _attention-enhanced heat diffusion blocks_ (Sec. 4.1). We represent every mesh, \(\), by a novel mixed LBO, informed both by the geometry and the topology of \(\)(Sec. 4.2). We further introduce an _online sampling_, in order to obtain a point-cloud \(\) with corresponding albedo colours \(\) and tailored spectral operators (Sec. 4.3). UV3-TeD is trained with a denoising objective described in Sec. 3, using heterogeneous batching. Meshes with a point-cloud texture can finally be rendered by the nearest-neighbour interpolation we detail in Sec. 4.4.

### Attention-enhanced Heat Diffusion Blocks

**Diffusion Blocks (DB)**. Our blocks are inspired by DiffusionNet , consisting of three separate learnable parts: heat diffusion, spatial gradient features, and a vertex-wise multi-layer perceptron (MLP). The heat diffusion process is used to disperse and aggregate information on a surface and it has a closed-form solution leveraging the spectral properties of the LBO. Since we aim to operate on point-cloud textures while leveraging topological and geometric information provided by the mesh, we use our tailored versions of \(\) and \(\) later described in Sec. 4.3 and named \(_{p}\) and \(M_{p}\)respectively. Being \(_{p}^{P Y}\) a generic field defined on \(\), the heat diffusion layer is defined as:

\[(_{p},h)_{p}e^{- _{h}h}\\ \\ e^{-_{K}h}(_{p}^{}M_{p}), \]

where \(\) is the Hadamard product, \(\) is the transpose operator, and \(h\) is the channel-specific learnable parameter indicating the heat diffusion time and effectively regulating the spatial support of the operator, which can range from local (\(h 0\)) to global (\(h+\)). Since this block supports only radially symmetric filters about each point, it is combined with a spatial gradients features block that expands the space of possible filters by computing the inner product between pairs of feature gradients undergoing a learnable per-vertex rotation and scaling transformation in the tangent bundle (see  for more details). The spatial gradients are computed online on the sampled point-cloud with our faster implementation (see App. A.1). Then, the input is concatenated with the output of these two blocks and passed to a per-vertex MLP (see Fig. 4, _bottom_).

We further add a time embedding representing the denoising step of the DDPM and introduce a group normalisation  to stabilise training after the time injection. We also add a linear layer on the skip connection when input and output channels differ., _e.g._, when skip connections from the downstream branch of the U-Net are concatenated with the features of the upstream branch.

**Enhancing DB via Farthest-Sampled Attention**. As depicted in Fig. 4, each of our Attention-enhanced Heat Diffusion Blocks concatenate three Diffusion Blocks and combine them with our diffused farthest-sampled attention layer. Even though with the Diffusion Blocks alone it is theoretically possible to achieve global support when \(h+\), the longer the heat diffusion time, the closer the diffused features become to the average of the input over the domain. This can result in less meaningful features causing texture inconsistencies between distant regions. To improve long-range consistency we introduce attention layers in each network's block alongside the other operators. Since directly performing the scaled dot product operation characterising attention modules on full-resolution point-cloud textures would be prohibitive, we build upon the heat diffusion concept and define a more efficient attention operator (Fig. 4, _top_).

We start by heat-diffusing the \(_{p}^{(i-1)}\) features predicted by the previous layers over \(\) to spread information across the surface geodesically. Then, we collect the spread information (i.e., \((_{p}^{(i-1)},h)\)) on a subset of the diffused features, \(^{S C}\), which is obtained by selecting the diffused features with \(C\) channels corresponding to the \(S\) farthest samples  of \(\). \(\) is then fed to a multi-headed self-attention layer , where a set of linear layers first computes queries, keys, and values for each head, then computes a scaled dot-product attention, concatenates the results across the different heads and, after going through an additional linear layer, produces a new set of features over the farthest samples. These features still reside on the farthest samples. To spread them

Figure 3: Framework of UV3-TeD. Given a mesh \(=\{,\}\) we precompute the proposed mixed Laplacian (\(_{}^{R}\)) and its eigendecomposition (\(\) and \(\)). During the online sampling we compute a coloured point-cloud \(\{,\}\) alongside its spectral quantities and other information used by our network (Fig. 9). In particular, eigenvalues \(\), sampled eigenvectors \(_{p}\), and approximate mass \(M_{p}\) are used to compute the heat diffusion operations (Eq. (1)); the farthest point samples \(()\) are used in the proposed diffused farthest-sampled attention layers (Fig. 4), and the scale invariant heat kernel signatures \(sihks\) and slope-adjusted eigenvalues \(^{}\) are used as shape conditioning. UV3-TeD leverages these information to generate coloured point-clouds (\(_{0}\)) from noise (\(_{T}\)).

across the entire surface, we set the features of the other points to zero and perform another heat diffusion. The output of the diffused farthest-sampled attention is re-combined with the output of the other blocks learning a per-channel weighting constant.

**Conditioning**. While other point-cloud networks require additional inputs to represent the positions of the input points alongside their features, we just provide noised colours as inputs because the diffusion process intrinsically operates on the surface of the shapes we want to texturise. Nevertheless, we do provide geometric and positional conditioning to the diffused farthest-sampled attention layers, which are otherwise unaware of the relative position of their inputs. Instead of using \(\) to compute the positional conditioning directly, we rely on the scale-invariant heat kernel signatures (\(sihks\)) , intrinsic local shape descriptors that are not only sampling and format agnostic but also isometry and scale-invariant. The geometry conditioning is obtained from the eigenvalues \(\), which, like in , are normalised by \(()\) and deprived of their slope as:

\[^{}=_{k}^{}\;\;_{k}^ {}=}{()}-4*k,k=1,,K}. \]

Eigenvalues processed as in Eq. (2) can still be used as global shape descriptors that besides having the advantage of being scale invariant also fluctuate over a straight line, becoming easier to process for a neural network. Intuitively, \(sihks\) tell us the intrinsic coordinates of a point, while \(^{}\) whether we are supposed to generate a texture on a chair, a sofa, a vase, or something else. Both are embedded with a MLP and the resulting geometry embeddings are concatenated with the point features.

### Mixed Robust Laplacian

To operate on real-world datasets we propose a mixed Laplacian operator which is robust to any triangle mesh and can better diffuse heat in the presence of complex topological structures (see Fig. 5, _left_). Our mixed robust LBO (\(_{}^{R}\)) is defined as:

\[_{}^{R}=(1-)_{m}^{R}+_ {p}^{R},. \]

Instead of using the \(cotan\)-LBO directly, we use the robust mesh Laplacian \(_{m}^{R}\), computed on the vertices of the mesh, as it provides robustness to non-orientable and non-manifold meshes. \(_{m}^{R}\) ensures that heat is geodesically diffused, while \(_{p}^{R}\), its point-cloud counterpart, enables communication between distinct or disconnected components of a mesh. A small \(\) value leads to diffusing heat on the surface while allowing for some heat transmission to neighbouring regions (see Fig. 5, _right_).

### Online Sampling of Points, Colours, and Spectral Operators

Our sampling strategy is at the core of our method as it provides an efficient sampling strategy that can be used online without hindering training speeds. In particular, Poisson Disk Sampling produces a point-cloud with regularly-spaced points, enabling us to approximate the mass matrix quickly.

Figure 4: Attention-enhanced Heat Diffusion block. Three consecutive Diffusion blocks (_bottom_) inspired by  and conditioned with a denoising time embedding are combined with a diffused farthest-sampled attention layer (_top_). The proposed attention, conditioned with local and global shape embeddings (\(sihks_{e}\) and \(_{c}^{}\)), first spreads information to all the points on the surface, before computing a multi-headed self-attention on the features of the farthest samples (red points), and finally spreads them back to all the points with another heat diffusion.

To avoid recomputing the eigendecomposition of our Laplacian operator (\(_{}^{R}\)) on the sampled point-cloud, we recycle the spectral operators precomputed on the vertices of the meshes. Finally, we describe how colours are sampled during training.

**Poisson Disk Sampling** (PSD) . PDS produces a point-cloud \(^{P 3}\), by uniformly sampling points on the surface of a mesh. This is achieved with a parallel dart-throwing algorithm that uses a uniform radius \(r\) across the surface. The samples \(_{i}\) are randomly distributed on the surface but remain a minimum distance of \(r\) away from each other. Since PDS is designed to operate given a radius rather than a desired number of points \(P^{*}\), the radius can be estimated from the ideal quality measure expected from the radius statistics introduced in  (see also ) :

\[=P^{*}}{()} ^{} 0.7 \]

**Mass Matrix**. Since the point-cloud textures have been sampled using PDS, we hypothesise that the distance between neighbouring points will equal the radius \(r\) used by PDS. A triangulation of such points would result in equilateral faces with area \((_{ijk})=}{2}()= }{4}\). Therefore, said \(Q\) the number of faces incident to each vertex \(_{i}\) and computing the radius with Eq. (4) we can approximate the mass matrix as:

\[_{ii}=_{ijk}(_{ ijk})}{4}r^{2}Q}{6P^{*}} (),\  i. \]

Since we never explicitly compute a triangulation of the point-cloud texture, we estimate \(Q\) on \(\). Also, considering that the mass matrix derived in Eq. (5) has the same value on the diagonal elements, we represent it with a scalar, \(M_{p}\).

**Eigenvalues and Eigenvectors**. The eigenvalues of LBO are considered global shape descriptors, as such, they are sampling-independent. Eigenvectors are on the other hand defined on the vertices of the mesh on which LBO was computed. However, as mentioned in Sec. 3, a mesh \(\) is effectively discretising a continuous surface \(\). Similarly, a signal on the vertices of the mesh can be thought of as a discretisation of the continuous function defined on \(\). For this reason, eigenvectors can be resampled by interpolating the values of the eigenvectors defined at the vertices of the mesh. We indicate these with \(_{p}^{P K}\).

**Colour Sampling**. Although we advocate for a new texture representation based on point-clouds, the most widely adopted representation is still based on UV-mapping. Hence, for every point sampled with PDS, we also query the colour stored in the UVimage plane at its corresponding UVcoordinates. When UVtextures are not provided, we sample the base colour instead. Following this procedure, we obtain coloured point-clouds \(\{,\}\) that we use as point-cloud textures during training.

When images have a significantly higher resolution than the desired point-cloud texture resolution, we resize the image texture before sampling. We assume that properly textured meshes should intentionally have big UVtriangles where a high texture resolution is required. Being \(^{uv}\) the \(N\) biggest triangles in UVspace and \(^{3D}\) the corresponding triangles on the mesh, to estimate the scaling factor (\(s\)) needed to obtain the desired image texture size, we compute the square root of the

Figure 5: Heat diffusion on Ted sliced on the belly and on a topologically disconnected birdhouse. Using the mesh LBO prevents heat from spreading to disconnected regions, this is particularly visible on Ted as heat does not spread over the nose, mouth, and legs. Similarly, on the birdhouse heat spreads only on the right-hand side of the roof. Using our mixed LBO formulation heat can spread over the entire shape even in the presence of topological errors and disconnected components.

ratio between the number of samples on \(^{3D}\) and the number of pixels in \(^{uv}\):

\[s=_{n=1}^{N}(^{3D})(_{ijk})}{(W H)(_{n=1}^{N}( ^{uv}))/1^{2}}^{}. \]

The number of samples in \(^{3D}\) is estimated dividing their average area by the approximate area of the PD sampled point-cloud texture. The number of pixels in \(^{uv}\) by estimating the fraction of UVspace occupied by the biggest triangles and multiplying it by the number of pixels in the image plane, which is computed as the product between image width and height (\(W H\)). In practice, we set \(N=250\) to consider a significant number of triangles and use \(3s\) instead of \(s\) to account for non-perfectly textured meshes which retain useful high-resolution content in small UVtriangles.

### Rendering Point-Cloud Textures

We rely on Mitsuba3 , a physically-based differentiable renderer, and implement a new class of textures: the point-cloud textures that we previously characterised with the \(\{,\}\) pair. When a ray intersection occurs and the point-cloud texture is queried, we compute the three nearest point-cloud neighbours to the hit point and interpolate their colour values. This is analogous to the standard texture querying that would occur in UVspace. Note that the nearest neighbours are computed using Euclidean rather than geodesic distances. When enough points are sampled, this is a reasonable assumption that keeps rendering times low.

## 5 Experiments

**Datasets**. We conduct experiments on two datasets, the chairs of ShapeNet  and the Amazon Berkeley Objects (ABO) dataset . The chair category of ShapeNet has often been used for texture generation on 3D shapes because, compared to the other categories, it has a high number of samples with relatively high texture resolutions. This motivated us to train UV3-TeD on these data. However, driven by the objective of building a model capable of operating across multiple categories, we also decided to leverage the less widely used ABO. Despite its more limited adoption, this dataset contains multiple object categories with good-quality meshes and textures.

Since with UV3-TeD mesh and texture resolution are independent, we pre-filter data with more than \(60,000\) vertices. This choice doesn't hinder the quality of the generated point-cloud textures but reduces the GPU memory consumption during training. As we are interested in generating textures, we also discard meshes that have coloured parts, but no textures. In both cases, we operate a \(90:5:5\) split between train, test, and validation sets. The filtering and data split leave us with \(4,633\) chairs for training, \(266\) for validation, and \(317\) for testing. On ABO we have \(6,476\) shapes for training, \(364\) for validation, and \(443\) for testing.

**Implementation Details**. We implement our method using PyTorch , Pytorch Geometric  and Diffusers . We use \(32\)\(sihks\), \(K=128\) eigenvalues, and a mixed-LBO weighting of \(=0.05\). We train our models using the AdamW  optimiser for \(400\) epochs on chairs and \(250\) on ABO, with a learning rate of \(1e^{-4}\) and a cosine annealing with \(500\) warmup iteration steps. We use \(T=1,000\) DDPM timesteps, \(=250\) farthest point samples in the attention layers, and \(P^{*}=5,000\) target PDS samples. Since \(P^{*}\) is used to estimate the mesh-specific PDS radius \(r\), our point-cloud textures often have a slightly different amount of points \(P\). Thus, we made all our layers suitable for heterogeneous batching. Our batch size is set to \(8\) on ShapeNet chairs and to \(6\) on ABO.

**Unconditional Texture Generation**. In Fig. 7_top_ we showcase our texture generation results on chairs from ShapeNet, in Fig. 8 we depict results on objects from ABO, and in Fig. 1 we combine textured object from both datasets rendered with more advanced lighting (more textured samples are provided in App. A.2). Then, we proceed to compare our method against the state-of-the-art.

Although we acknowledge that many state-of-the-art methods operating in UV-space have generated impressive results, we want to highlight that they operate on images, a data type which has been vastly explored by the Deep Learning community and where models are well-engineered, mature in

Figure 6: Rendering a point-cloud textured cow . When a ray intersects the mesh, we interpolate the colours of the three nearest texture points.

terms of efficiency and quality, and trained on larger datasets. We find it important to note that we do not aim to compete against UV techniques, but rather attempt to prompt a paradigm shift towards a direction that will not require UV-mapping, with its many unsolved issues. As detailed in Sec. 2, we consider the coarse stage of Point-UV Diffusion  and Texturify  to be the best non-UV-based method currently available as well as the most relevant works to ours. The former already extensively proved its superiority over Texturify and  on the chairs of ShapeNet. On the same data, Texturify made additional comparisons against UV ( and a UV Baseline), Implicit , Tri-plane , and sparse grid  methods, outperforming all of them across all metrics. For this reason, we focus our comparison on Point-UV Diffusion, which does not have shadows baled in the texture and is therefore better suited to be rendered with our pipeline (Sec. 4.4). Because we adopt physically-based rendering techniques rather than rasterisation, we re-compute FID and KID scores  for Point-UV Diffusion. Each shape is rendered from \(5\) random viewpoints with the camera pointing at the object at an azimuth angle sampled from \([0,2]\) and an elevation sampled from \([0,}{{3}}]\). We also compute the LPIPS  metric, measuring the diversity of the generated textures. For this, we generate \(3\) texture variations for each shape, render them, and compute the LPIPS values for all the possible pairs of images with the same underlying shape. The results are then averaged for each method. We also evaluate two different scenarios: one in which we emulate the original formulation and render shapes whose point-cloud textures have been projected in \(\)-space, and one where we do not project their point-clouds in UV-space using our rendering technique instead. As we can observe from the quantitative results reported in Tab. 1, our method significantly outperforms Point-UV Diffusion across all metrics. In addition, as we can observe from the samples reported in Fig. 1 and Fig. 7, our method can generate more diverse samples, which are also more capable of capturing the semantics of the different object parts. Interestingly, this is achieved without providing any semantic segmentation. UV3-TeD also significantly outperforms a DiffusionNet DDPM in sample quality while maintaining comparable diversity (Tab. 1). This baseline mimics UV3-TeD by leveraging a DDPM model with a U-Net-like architecture having as many layers as ours, but without shape conditioning and using the point operators of . Therefore, not only there were no farthest-sampled attention layers, but no online sampling strategy was used and the point-cloud Laplacian and mass matrix were used instead. Colours were still sampled like for UV3-TeD. All the hyperparameters matched ours.

**Ablation Studies**. We here perform multiple ablations to examine how much each model component, conditioning, and choice contributes to the overall performance. The ablations are performed training the model for \(50\) epochs on the ABO (multi-class) dataset. Ablations are quantitatively evaluated on \(100\) test shapes using the three main metrics previously used during the comparisons: the FID, KID, and LPIPS scores. Results are reported in Tab. 2. Overall, our final model (UV3-TeD) reports the best quality scores while maintaining good diversity scores. A detailed discussion of the different ablations is provided in App. A.2.

   Method & FID (\(\)) & KID \( 10^{-4}\) (\(\)) & LPIPS (\(\)) \\  PointUVDiff  (uv) & 63.35 & 83.19 & 0.08 \\ PointUVDiff  (pcl-texture) & 65.09 & 126.18 & 0.09 \\ DiffusionNet  DDPM & 116.58 & 468.09 & **0.24** \\ UV3-TeD (ours) & **54.20** & **42.17** & 0.21 \\   

Table 1: Quantitative comparison on the chairs of ShapeNet.

Figure 7: Qualitative comparison between PointUVDiff (pcl-texture) and _UV_3-TeD (ours). Our textures are more diverse and more semantically meaningful. All shapes belonged to the test set.

## 6 Conclusion

We introduced UV3-TeD, a new method for representing and generating textures on sparse unstructured point-clouds constrained to lie on the surface of an input mesh. Our framework is based upon denoising diffusion over surfaces, in which we introduce a new _farthest-sampled multi-headed attention layer_ diffusing and capturing features over distant regions, required for coherent texture synthesis. To perform diffusion on meshes of arbitrary topologies, we proposed a mixed Laplacian, fusing both geometric and topological cues. In addition, we proposed online sampling strategies for efficiently working with different quantities related to the shape spectra. Acknowledging that rendering is as equally important as the texture representation, we proposed a path-tracing renderer tailored for our point-cloud textures living on shape surfaces.

**Limitations & Future Work**. Existing UV-based texturing pipelines are heavily engineered, leveraging the recent advances in image generation. We expect that our approach will similarly benefit from the advances in 3D foundation models. Learning high-frequency texture details requires significant training effort, usually exceeding thousands of epochs. More efficient architectures, utilising pooling are required to overcome the drawback and increase the resolution of the generated textures. To enhance quality even further, we recommend extending our method to support BRDFs generation and encourage additional research into sampling strategies capable of ensuring crisp texture borders between parts. With UV3-TeD and its promising results, we invite the community to re-think efficient texture representations, and pave the way to seam-free high-quality coding of appearances on surfaces. As such, we believe our work opens up ample room for future studies in texture generation and other applications requiring the generation of signals that reside on surfaces. For instance, our framework could be easily adapted to applications ranging from HDRI environment map generation, shape matching, and weather forecasting, to molecular shape analysis and generation.

**Broader Impact**. We believe our approach will have a predominantly positive impact, fostering research in generating UV-free textures and ultimately improving creative processes across various industries and empowering individuals with limited artistic skills to participate in content creation. Also, we do not expect nor wish to replace artists due to advancements in the field. Instead, we aim to make their work more efficient, allowing them to unlock their creativity faster.

    & FID (\(\)) & KID \( 10^{-4}\) (\(\)) & LPIPS (\(\)) \\  UV3-TeD (ours) & **77.14** & **58.59** & 0.14 \\ w/o farthest-sampled attention & 83.16 & 103.42 & 0.10 \\ w/o \(}\) & 79.96 & 66.29 & 0.15 \\ w/o \(sihks\) & 78.46 & 70.90 & 0.14 \\ \(_{p}\) instead of \(sihks\) & 79.62 & 65.17 & 0.14 \\ \(=0\) (\(_{}^{R}\) instead of \(_{}^{R}\)) & 78.07 & 62.64 & 0.15 \\ \(=1\) (\(_{}^{R}\) instead of \(_{}^{R}\)) & 78.47 & 64.53 & 0.14 \\ w/o GT-texture resizing & 83.72 & 97.00 & **0.16** \\   

Table 2: Ablation studies on our UV3-TeD on ABO. Models were trained for \(50\) epochs.

Figure 8: Random samples generated by UV3-TeD (our method) on ABO test set.

#### Acknowledgments and Disclosure of Funding

This work was supported by the UK Engineering and Physical Sciences Research Council (EPSRC) Project GNOMON (EP/X011364) for Imperial College London, Department of Computing.