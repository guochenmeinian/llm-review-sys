# Differentially Private Attention Computation

Yeqi Gao

The University of Washington

a916755226@gmail.com &Zhao Song

The Simons Institute for the Theory of Computing

at the University of California, Berkeley

magic.linuxkde@gmail.com &Xin Yang

The University of Washington

yangxin199207@gmail.com &Yufa Zhou

University of Pennsylvania

yufazhou@seas.upenn.edu

###### Abstract

Large language models (LLMs), especially those based on the Transformer architecture, have had a profound impact on various aspects of daily life, such as natural language processing, content generation, research methodologies, and more. Nevertheless, a crucial concern regarding the inference results of large language models is the issue of security and privacy. Given that large language models can generate results that may leak sensitive confidential or copyright information in many scenarios, it is crucial to compute the attention matrix with provable privacy guarantees, as attention is all you need.

In this work, we propose a novel and efficient algorithm for approximating the attention matrix while providing differential privacy (DP) guarantees. To achieve this, we build on recent advancements in fast attention computation and differentially private matrix publishing.

## 1 Introduction

The development of large language models (LLMs) has been rapid and significant in recent years, with numerous breakthroughs and advancements in the field. BERT  achieved state-of-the-art performance on a wide range of language tasks by training on a massive amount of text data in 2018. Since then, the GPT (Generative Pre-trained Transformer) family of models has further advanced the field. GPT-2  and GPT-3 , with billions of parameters, are able to generate highly coherent and human-like text. Other notable LLMs include XLNet , which addresses some of the limitations of BERT , and RoBERTa , which improves upon BERT 's training methods to achieve better performance. The rapid development of LLMs has been fueled by advancements in hardware, software, and data availability, allowing researchers and companies to train and deploy these models at an unprecedented scale.

As a result of their development, LLMs have found a wide range of applications in various fields. In the field of natural language processing (NLP) , LLMs are used for tasks such as language translation , sentiment analysis , and creative writing . In addition, LLMs are being used to develop chatbots and virtual assistants that can understand and respond to natural language queries . Outside of NLP, LLMs are being used in scientific research to generate new hypotheses and discover novel patterns in large datasets. The applications of LLMs are expanding rapidly, and it is likely that they will play an increasingly important role in many fields, such as computer vision , robotics , and autonomous vehicles .

[MISSING_PAGE_FAIL:2]

**Definition 1.2**.: _Given \(X^{n d}\), the goal is to find some \(Y^{n m}\) such that_

\[\|D(XX^{})^{-1}(XX^{})-D(YY^{})^{-1}(YY^{})\|\]

_where \(\|\|\) is some certain norm and \(D(XX^{})=((XX^{})_{n})\)._

One recent work  choose the angle of near access-freeness to study the privacy concerns in LLMs. However, in this work, we use the differential privacy concept , and the formal definition of differential privacy can be written as follows.

**Definition 1.3** (Differential Privacy ).: _A randomized mechanism \(\) is \((,)\)-differentially private if for any event \(()\) and for any pair of neighboring databases \(S,S^{}\) that differ in a single data element, one has_

\[[(S)]()[(S^{ })]+.\]

Finally, we're ready to define our differentially private attention computation problem.

**Definition 1.4** (General Differentially Private Attention).: _Let \(f:\) denote some fixed function. For a given matrix \(X^{n d}\) with \(d n\), let \(\) denote some mapping that maps \(^{n d}\) to \(^{n n}\), let \(A=(X)\), for parameter \(,(0,0.1)\), the goal is to design an \((,)\)-differentially private algorithm that takes \(X^{n d}\) as input and generates a PSD matrix \(B^{n n}\) such that_

\[\|}(A)^{-1}f(A)-}(B)^{-1}f(B )\| g(,)\]

_where \(f(A)_{i,j}=f(A_{i,j})\), \(}(A)=(f(A)_{n})\) and where \(g\) is some function._

Definition 1.4 is very general, and covers the standard self-attention computation. In particular, when \((X)=XX^{}\) and \(f(z)=(z)\), then above definition recovers the standard self-attention in LLMs.

### Our Result

Our results rely on good properties of the input data, which are defined as follows. They play a crucial role in the analysis of sensitivity with respect to \((X)=XX^{}\) (See Section 4.3).

**Definition 1.5** (Dataset).: _Fix \(>0,>0\). We say our dataset \(X^{n d}\) is \((,)\)-good if \(XX^{} I_{n}\) and for all \(i[d]\), \(\|X_{*,i}\|_{2}\)._

In addition, we will introduce our proposed definition of neighboring data as follows.

**Definition 1.6** (Neighboring data).: _Let \(X,^{n d}\) denote two datasets from distribution \(\), we say that \(X\) and \(\) are \(\)-close if there exists exact one \(i[d]\) so that \(\|X_{*,i}-_{*,i}\|_{2}\) and for all \(j[d]\{i\}\), \(X_{*,j}=_{*,j}\). In this work, we consider two datasets to be neighboring if they are \(\)-close._

The above definition facilitates a more straightforward analysis of the sensitivity of attention matrix computations. By regulating \(\)-closeness, we can establish bounds on how the attention matrix responds to minor variations in input data, which is essential for ensuring differential privacy guarantees. Furthermore, in practical scenarios, assessing dataset similarity based on feature-wise differences rather than individual data points can be more practical and aligns better with real-world considerations.

Based on the aforementioned definitions, our work demonstrates the sensitivity property of \((X)=XX^{}\) (attention matrix computation). Furthermore, we present a novel and efficient algorithm for approximating the attention matrix, which combines error analysis on matrix perturbation with provable privacy guarantees. We state our result as follows:

**Theorem 1.7** (Main result, informal of Theorem 3.1).: _Let \(d n\). Let \(X^{n d}\). Let \(f(z)\{(z),(z)\}\). Let \(r,,(0,0.1)\). Let \(=0.1\{},\}\). Let \(A=(X)=XX^{}\) and \(\|A\|_{} r\). Let \(f(A)\) and \(}(A)\) be defined as Definition 1.4. For all \(X\) sampled from \(\), \(X\) is \((,)\)-good (see Definition 1.5). Let \(<r\). Let \(\) be the parameter for the neighboring dataset. Let \(2/<\). Suppose \(\|(X)^{1/2}()^{-1}(X)^{1/2}-I\| _{F}\)_for all \(X^{n d},^{n d}\) (see Definition 1.6). Let \(=+(1/))/k}+(n^{2}+(1/))/k<0.1\). Then, there is an algorithm (Algorithm 1) that takes \(X\) as input and produces the matrix \(B^{n n}\) and also general attention \((B)^{-1}f(B)\) as output such that_

\[\|\,(A)^{-1}f(A)-(B)^{-1}f(B)\|_{} 4(1+ +2r) r\]

_which holds with probability \(1-\). With respect to \(X\), the algorithm is \((,)\)-differential private._

### Related Work

Differential Privacy and Deep Learning.Differential privacy (DP) is a rigorous and quantifiable notion of privacy that ensures individual data entries cannot be distinguished within a dataset. It has become the go-to standard for understanding information leakage . This widely recognized framework is increasingly being adopted in industry and has many real-world applications . There has been extensive research on applying differential privacy in deep learning . Recent works , LTLH21 have applied DP-SGD  to large language models (LLMs) for private fine-tuning. Our research, however, is orthogonal to these works as we focus on attention computation and consider general differential privacy mechanisms, not just DP-SGD.

Roadmap.Our paper is organized as follows. Section 2 presents the notations that are used throughout our paper. Our main result is presented in Section 3. We provide an overview of our techniques in Section 4. In Section 5, we give our conclusion of the paper.

## 2 Notations

\([X]\) represents the expected value (or mean) of a random variable \(X\). We use \(^{2}_{d}\) to denote a Chi-squared random variable with \(d\) degrees of freedom. If \(M\) and \(N\) are symmetric matrices, we define \(M N\) to mean that for all vectors \(x\), the inequality \(x^{}Mx x^{}Nx\) holds. If \(M\) is a symmetric matrix of dimension \(n n\), we define \(M\) to be positive semidefinite (\(M 0\)) if the inequality \(x^{}Mx 0\) holds for all vectors \(x^{n}\). We use the notation \(_{n}\) to denote an \(n\)-dimensional vector whose entries are all zero, and \(_{n}\) to denote an \(n\)-dimensional vector whose entries are all one. The symbol \(I_{n}\) represents the \(n n\) identity matrix, which is a square matrix with ones on the main diagonal and zeros elsewhere. Let \(x\) be an arbitrary vector in \(^{n}\). We define \((x)^{n}\) as a vector whose \(i\)-th entry \((x)_{i}\) is equal to \((x_{i})\), where \(()\) denotes the exponential function. We use \( x,y\) to denote \(_{i=1}^{n}x_{i}y_{i}\). For any matrix \(A\), we use \(\|A\|\) to denote the spectral norm of \(A\), i.e., \(\|A\|=_{\|x\|_{2}=1}\|Ax\|_{2}\), \(\|A\|_{F}\) to denote its Frobenius norm and \(\|A\|_{}\) to denote the infinity norm. \(A_{i,j}\) represents the element in the \(i\)-th row and \(j\)-th column of matrix \(A\)._

## 3 Main Result

```
1:procedureDPAttention(\(X\))
2:\(A XX^{}\)
3:\(B(A,k)\)\(\) See Algorithm 2.
4: Compute \(f(B)\)
5: Compute \((B)^{-1}f(B)\)
6:endprocedure
```

**Algorithm 1** Differential privacy algorithm

In this section, we provide a theoretical analysis of Algorithm 1, our primary algorithm for differentially private general attention computation. Our analysis leverages the tools established in Section 4.1, Section 4.2, and Section 4.3. From our previous proofs, it is evident that our algorithm possesses a rigorous differential privacy property, offering new insights into both differential privacy and attention mechanisms.

**Theorem 3.1** (Main result).: _If all of the following requirements are met Let \(d n\), \(X^{n d}\), and \(f(z)\{(z),(z)\}\). We define \(r(0,0.1)\) as bounded ratio and \(,(0,0.1)\) as the parameter of DP. Let \(=0.1\{},\}\). Let \(A=(X)=XX^{}\) and \(\|A\|_{} r\). For all \(X\) sampled from \(\), \(X\) is \((,)\)-good (see Definition 1.5). Let \(<r\). Let \(\) be the parameter for neighboring dataset. Let \(2/<\). Let \(\) denote the sensitivity parameter that \(\) satisfies a sensitivity bound that \(\|(X)^{1/2}()^{-1}(X)^{1/2}-I\| _{F}\) for any neighboring datasets \(X^{n d},^{n d}\) (see Definition 1.6). Let \(=+(1/))/k}+(n^{2}+(1/))/k\) and \(<0.1\). There is an algorithm (Algorithm 1) that takes \(X\) as input and produces the matrix \(B^{n n}\) and also attention \((B)^{-1}f(B)\) as output such that_

* **Part 1.**__\(\|\,(A)^{-1}f(A)-(B)^{-1}f(B)\|_{} 4(1+ +2r) r\)_._
* **Part 2.** _With respect to_ \(X\)_, the algorithm is_ \((,)\)_-differential private._
* **Part 3.** _It holds with probability_ \(1-\)_._

Proof of Theorem 3.1.: The proof can be divided into two parts as follows.

Proof of Part 1 and Part 3.Our proof focus on the function \((X):=XX^{}\) first. Let \(\) and \(\) be denoted in Definition 1.5 and \(\) be denoted as Definition 1.6. Based on the assumption on dataset above, we can obtain \(X\) is \((,)\)-good (See Definition 1.5) while \(X\) and \(\) are \(\)-close (See Definition 1.6). According to **Part 1** of Lemma 4.11, we can conclude the property on \((X)=XX^{}\) such that

\[\|(XX^{})^{-1/2}^{}(XX^{})^{-1/2}-I\|_{ F} 2/\]

Let \(\) be the function denoted in the theorem statement and let \(\) be denoted as follows:

\[:=O(+(1/))/k}+(n^{2}+(1/))/k)\]

Now, we will apply the conclusion drawn in Section 4.2. In order to satisfy the requirement specified in **Requirement 4** of Theorem 4.9, we need \((X)\) to meet the following assumption:

\[\|(X)^{1/2}()^{-1}(X)^{1/2}-I\| _{F}.\]

Now, if we choose \(2/<\). we will guarantee that our \((X)\) satisfies the assumption specified in **Requirement 4** of Theorem 4.3. According to **Part 3** of Theorem 4.3, there exists Algorithm 2 which can produce a matrix \(B^{n n}\) such that, with probability at least \(1-\)

\[(1-)A B(1+)A \]

By choosing \((0,0.1)\), we will have

\[(1-)B A(1+)B \]

Now according to Theorem 4.3 and Eq. (2), we have

\[\|\,(A)^{-1}f(A)-(B)^{-1}f(B)\|_{} 4(1+ +2r) r\]

Now, the proofs of **Part 1** and **Part 3** are completed.

Proof of Part 2.It simply follows from **Part 1** of Theorem 4.9. 

The main result implies that we can design an algorithm that computes a private approximation of the attention mechanism used in neural networks for functions like \(f(z)=(z)\) or \(f(z)=(z)\). Specifically, under certain conditions on the input matrix \(X\) and parameters \(,\), and with a small bounded ratio \(r\), the algorithm produces a matrix \(B\) such that the normalized attention matrices derived from \(A=XX^{}\) and \(B\) are close in the infinity norm. This closeness is quantified by a bound proportional to \(r\), ensuring that the utility of the attention mechanism is preserved. Additionally, the algorithm is \((,)\)-differentially private with respect to \(X\), meaning it protects individual data entries from being inferred. The privacy and utility guarantees hold with high probability \(1-\), demonstrating that it is possible to implement attention mechanisms in a way that maintains both model performance and data privacy.

Technique Overview

The objective of our research is to develop a differentially private algorithm that addresses the challenges of computing attention on large datasets. Specifically, we focus on scenarios where the size of the data matrix \(X\) is extremely large, with the number of features \(d\) significantly exceeding the number of samples \(n\) (i.e., \(d n\)). In these cases, the attention matrix \(A\) is obtained as the output of the function \((X)=XX^{}\), and our goal is to ensure that the computation of \(A\) is performed in a differentially private  manner.

Perturb PSD Matrix.We define the attention computation \((X)\) as Definition 4.2. By employing a more general version of Perturbation analysis presented in , we select \(f\) as specified in Definition 4.1. To complete the error analysis of attention computation, we will utilize the perturbation analysis of the diagonal normalization matrix and the PSD matrix presented in Appendix C. Under the assumption the relative error between input matrix \((X):=A\) and privacy required matrix output \(B\) is less than or equal to \((0,0.1)\) where \((1-)B A(1+)B\). To establish an upper bound for \(\|\,(A)^{-1}f(A)-(B)^{-1}f(B)\|_{}\), we first derive the following bound:

* **Part 1.**\(|\,(A)_{i,i}-(B)_{i,i}| c_{1} r\{(A)_{i,i},(B)_{i,i}\}\;\; i[n]\),
* **Part 2.**\(|f(A_{i,j})-f(B_{i,j})| c_{2} r\{f(A_{i,j}),f(B_{i,j})\}\; \; i,j[n][n]\)

And with the error of attention computation under control as mentioned above, we can obtain:

\[\|\,(A)^{-1}f(A)-(B)^{-1}f(B)\|_{} 4(1+ +2r) r\]

Sensitivity for PSD Matrix.Our work relies on the basic assumptions that \(X^{n d}\) is a \((,)\)-good dataset (See Definition 1.5) and that \(X\) and \(\) are \(\)-close to each other (See Definition 1.6). We choose \((X):=XX^{}\). Now we will demonstrate the property of our function \((X)=XX^{}\) based on the given assumptions. Since \(X\) and \(\) are neighbor datasets, we have the following:

\[\|(X)^{1/2}()^{-1}(X)^{1/2}-I\| _{F} 2\]

The proof details can be found in Section E. Let us denote \(\) as defined in Definition 4.5. By choosing \(2/<\), we will have

\[\|(}_{:=(X)})^{1/2}(^{}}_{:=()})^{-1}(}_{:=(X)})^{1/2}-I\|_{F} \]

The assumption specified in the **Requirement 5** of Theorem D.7 will be satisfied. Next, we will introduce our main algorithm using Eq. (3).

Differential Privacy Algorithm.Next we give the differential privacy algorithm described in Theorem 1.7. And we will demonstrate that our algorithm (Algorithm 1) is able to output a matrix that satisfies the **Part 1** of our formal main result (See Theorem 3.1).

To begin with, we demonstrate that there exists an algorithm capable of taking input \(A\) and producing a matrix \(B\) as output such that the difference between \(A\) and \(B\) is small enough, which can be seen as a small error resulting from the perturbation of \(A\) by \(:=O(+(1/))/k}+(n^{2}+(1/))/k)\). In other words, we have \((1-)A B(1+)A\). The above equation holds with probability \(1-\). Note that \(k\) and \(\) can be chosen according to our requirements. We can ensure that a satisfactory \(\) is obtained. By choosing a small enough \( 0.1\) and using the conclusions on perturbed PSD matrices, the algorithm can certainly output a satisfactory \(B\) which promises our attention computation is privacy .

### Error Control from Logit Matrix to Attention Matrix

In this section, we analyze the perturbations in the attention computation, which are used to control the error. First, we define the followings.

**Definition 4.1**.: _Let \(f(z)\) denote one of the following functions \((z)\) and \((z)\)._

[MISSING_PAGE_EMPTY:7]

**Definition 4.7**.: _Let \(\) be denoted in Definition 4.4 and \(():=()\). We define \(_{1}:=()\), \(_{2}:=(^{})\)._

**Definition 4.8**.: _Let \(g_{1},g_{2},,g_{k}\) be i.i.d samples from \((0,_{1})\) output by Algorithm 2. Then, we define \(h_{i,j}:=_{1}^{-1/2}g_{i},v_{j}\), \(Z:=_{i=1}^{k}(}(g_{i})}{f_{_{2}}(g_{i})})\) where \(_{1},_{2}\) are defined by Definition 4.7. Note that the random variables \(h_{i,j}\) are i.i.d copies of \((0,1)\)._

We will now present our theorem for Algorithm 2. The proof is delayed to Section D.6.

**Theorem 4.9** (Informal version of Theorem D.7).: _If all of the following requirements are met:_

**Requirement 1.** _Let \((0,1)\) and \((0,1)\)._

**Requirement 2.** _Let \(\) be denoted as Definition 4.5 and \(<1\)._

**Requirement 4.** _Let \(M,\) be denoted as Definition 4.4 and \(M\)._

**Requirement 5.** _An input \(=()\)._

**Requirement 6.** _\(=O(+(1/))/k}+(n^{2}+(1/))/k)\)._

_Then, there is an algorithm (Algorithm 2) such that_

* _Part 1. Algorithm 2 is_ \((,)\)_-DP (with respect to the original dataset_ \(\)_)._
* _Part 2. outputs_ \(\) _such that with probability at least_ \(1-\)_,_ \(\|^{-1/2}^{-1/2}-I_{n}\|_{F}\)_._
* _Part 3._ \((1-)(1+)\)_._

Using this theorem, we can see our Algorithm 2 is DP, which ensures the DP of Algorithm 1.

### Sensitivity for PSD Matrix

We have demonstrated the existence of a differential privacy algorithm under the assumption on \((X)=XX^{}\) introduced in Section 4.2. In this section, we show that \((X)=XX^{}\) satisfies the assumption specified in **Requirement 4** of Theorem 4.9 for \((X)\). The lemma following is based on the assumption on datasets \(X,\) (See Definition 1.5 and Definition 1.6).

**Lemma 4.10** (Informal version of Lemma E.1).: _If \(X^{n d}\) and \(^{n d}\) are neighboring dataset (see Definition 1.5 and Definition 1.6), then \((1-2/)XX^{}^{} (1+2/)XX^{}\)._

Now, we can have the following lemma. The subsequent lemma can be viewed as a variation of Lemma 4.10, yet it presents a more apparent result that can be directly employed in subsequent analyses.

**Lemma 4.11**.: _Let \(\) and \(\) be defined in Definition 1.5. Let \(\) be defined in Definition 1.6. Let \(X\) and \(\) be neighboring datasets such that \((1-2/)XX^{}^{} (1+2/)XX^{}\). Then, we have \(\|(XX^{})^{-1/2}^{}(XX^{})^{-1/2}-I\|  2/\) and \(\|(XX^{})^{-1/2}^{}(XX^{})^{-1/2}-I\|_{ F} 2/\)._

The proof of Lemma 4.11 follows directly from the Lemma 4.10.

Presently, we have delved into the sensitivity property of attention computation. We are able to illustrate that the computation of the attention matrix aligns with the assumptions introduced in Section 4.2. Building upon this foundation, we will subsequently address our primary result in Section 3.

## 5 Conclusion

In this work, we propose a differentially private algorithm for approximating the attention matrix. Our algorithm is built upon recent advances in fast attention computation and private matrix releasing. To the best of our knowledge, this is the first work of accelerating attention computation in the DP setting. Given the dominating presence of Transformer based language models, we hope our work can stand as a starting point for fully DP training and inferring algorithms on large language models. It is also an interesting open problem to approximate asymmetric attention computation with differential privacy.