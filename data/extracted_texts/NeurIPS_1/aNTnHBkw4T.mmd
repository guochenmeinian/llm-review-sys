# Understanding Hallucinations in Diffusion Models through Mode Interpolation

Sumukh K Aithal\({}^{1}\)  Pratyush Maini\({}^{1,2}\)  Zachary C. Lipton\({}^{1}\)  J. Zico Kolter\({}^{1}\)

Carnegie Mellon University\({}^{1}\)  DatologyAI\({}^{2}\)

{saithal, pratyus2, zlipton, zkolter}@cs.cmu.edu

###### Abstract

Colloquially speaking, image generation models based upon diffusion processes are frequently said to exhibit "hallucinations"--samples that could never occur in the training data. But where do such hallucinations come from? In this paper, we study a particular failure mode in diffusion models, which we term _mode interpolation_. Specifically, we find that diffusion models smoothly "interpolate" between nearby data modes in the training set to generate samples that are completely outside the support of the original training distribution; this phenomenon leads diffusion models to generate artifacts that never existed in real data (i.e., hallucinations). We systematically study the reasons for, and the manifestation of this phenomenon. Through experiments on 1D and 2D Gaussians, we show how a discontinuous loss landscape in the diffusion model's decoder leads to a region where any smooth approximation will cause such hallucinations. Through experiments on artificial datasets with various shapes, we show how hallucination leads to the generation of combinations of shapes that never existed. We extend the validity of mode interpolation in real-world datasets by explaining the unexpected generation of images with additional or missing fingers similar to those produced by popular text-to-image generative models. Finally, we show that diffusion models in fact _know_ when they go out of support and hallucinate. This is captured by the high variance in the trajectory of the generated sample towards the final few backward sampling steps. Using a simple metric to capture this variance, we can remove over 95% of hallucinations at generation time while retaining 96% of in-support samples in the synthetic datasets. We conclude our exploration by showing the implications of such hallucination (and its removal) on the collapse (and stabilization) of recursive training on synthetic data with experiments on MNIST and a 2D Gaussians dataset. We release our code at [https://github.com/locuslab/diffusion-model-hallucination](https://github.com/locuslab/diffusion-model-hallucination).

## 1 Introduction

The high quality and diversity of images generated by diffusion models  have made them the de facto standard generative models across various tasks including video generation , image inpainting , image super-resolution , data augmentation , and others. As a result of their uptake, large volumes of synthetic data are rapidly proliferating on the internet. The next generation of generative models will likely be exposed to many machine-generated instances during their training, making it crucial to understand ways in which diffusion models fail to model the true underlying data distribution. Like other generative model families, much research has been done to understand the failure modes of diffusion models as well. Past works have identified, and attempted to explain and remedy failures such as, training instabilities , memorization  and inaccurate modeling of objects such as hands and legs .

In this work, we formalize and study a particular failure mode of diffusion models that we call hallucination--a phenomenon where diffusion models generate samples that lie completely out of the support of the training distribution of the model. As a contemporary example, hallucinations manifest in large generative models like StableDiffusion  in the form of hands with extra (or missing) fingers or limbs. We begin our investigation with a surprising observation that an unconditional diffusion model trained on a distribution of simple shapes, generates images with combinations of shapes (or artifacts) that never existed in the original training distribution (Figure 1). While extensive research on generative models has focused on the phenomenon of'mode collapse' , which leads to a loss of diversity in the sampled distribution, such studies often overlook the complex nature of real data which typically comprise multiple distinct modes on a complex data manifold, and the effects of their mutual interactions are thus neglected. In our work, we explain hallucinations by introducing a novel phenomenon we term'mode interpolation' that considers this mutual interaction.

To understand the cause of these hallucinations and their relationship to mode interpolation, we construct simplified 1-d and 2-d mixture of Gaussian setups and train diffusion models on them (SS 4). We observe that when the true data distribution occurs in disjoint modes, diffusion models are unable to model a true approximation of the underlying distribution. This is because there exist'step functions' between different modes, but the score function learned by the DDPM is a smooth approximation of the same, leading to interpolation between the nearest modes, even when these interpolated values are entirely absent from the training data. Moreover, we observation that hallucinated samples usually have very high variance towards the end of their trajectory during the reverse diffusion process. Based on this observation, we use the trajectory variance during sampling as a metric to detect hallucinations (SS 5), and show that diffusion models usually 'know' when they hallucinate, allowing detection with sensitivity and specificity \(>0.92\) in our experiments.

We explore mode interpolation as a potential explanation for the common failure of large-scale generative models, to accurately generate human hands. To demonstrate this concretely, we trained a diffusion model on a dataset of high-quality hand images and observed that it generated hands with additional fingers. We then applied our proposed metric to effectively detect these hallucinated generations. Finally, we study the implications of this phenomenon in recursive generative model retraining where we train generative models on their own output (SS 6). Recently, recursive training and its downsides in model collapse have garnered a lot of attention in both language and diffusion modeling literature . We observe that the proposed detection mechanism is able to mitigate the model collapse during recursive training on 2D Grid of Gaussians, Shapes and MNIST dataset.

Figure 1: **Hallucinations in Diffusion Models: Original Dataset (Left) & Generated Dataset (Right). (Top) The original dataset consists of 64x64 images divided into three columns, each containing a triangle, square, or pentagon with a 0.5 probability of the shape being present. Each shape appears at most once per image. The generated dataset created using an unconditional DDPM includes some samples (_hallucinations_) with multiple occurrences of the same shape that is unseen in the original dataset. (Bottom) We also train a ADM  on a dataset of high-quality images of human hands and show that the diffusion model generates hallucinated images of hands with additional fingers.**

### Hallucination in Diffusion Models

Before formalizing our notions and definitions in SS 3, let us first consolidate the observation that has been loosely labeled as 'hallucination' until now. To illustrate this phenomenon, we design a synthetic dataset called Simple Shapes, and train a diffusion model to learn its distribution.

Simple Shapes Setup.Consider a dataset consisting of black and white images that contain three shapes: triangle, square, and pentagon. Each image in the dataset is 64x64 pixels in size and divided into three (implied) columns. The first, second, and third columns contain a triangle, square, and pentagon, respectively. Each column has a 0.5 probability of containing the corresponding shape. A representation of this setup is shown in Fig 1. It is important to note that in this data generation pipeline, each shape is present at most once in each image.

Observation.We train an unconditional Denoising Diffusion Probabilistic Model (DDPM)  on this toy dataset with \(T=1000\) timesteps. We observe that the DDPM generates a small fraction of images that are never observed in the training dataset, nor a part of the'support' of the data generation pipeline. Specifically, the model generates some images that contain two occurrences of the same shape, as shown in Fig 1. Furthermore, when the model is iteratively trained on its own sampled data, the fraction of these occurrences increases significantly as the generation process progresses.

Inspired by these observations and their implications, we will perform experiments through the rest of this work to formalize what we mean by hallucinations (SS 3), why do they occur (SS 4), how can we mitigate them (SS 5), and what are their implications for real-world datasets (SS 6).

## 2 Related Work

Diffusion Models.Diffusion models [15; 38; 43] are a class of generative models characterized by a forward process and a reverse process. In the forward process, noise is incrementally added to an image over time steps, ultimately converting the data into noise. The reverse process learns to denoise the image using a neural network essentially learning to convert noise to data. Diffusion models have various interpretations. Score-based generative modeling [41; 42] and DDPMs  are closely related, with  proposing a unified framework using stochastic differential equations (SDEs) that generalizes both Score Matching with Langevin Dynamics (SMLD)  and DDPM. In this framework, the forward process is a SDE with a continuous-time generalization instead of discrete timesteps and the reverse process is also an SDE that can be solved using a numerical solver. Another perspective is to view diffusion models as hierarchical Variational Autoencoders (VAEs) . Recent research  suggests that diffusion models learn the optimal transport map between Gaussian distribution and data distribution. In this paper, we discover a surprising phenomenon in diffusion which we coin mode interpolation.

Recursive Generative Model Training.Recent works [2; 3; 26; 27; 37] demonstrated that iteratively training the generative models on their own output (i.e recursive training) leads to model collapse. The model collapse can happen in two ways: either all samples collapse to a single mode (low diversity) or the model generates very low fidelity, unrealistic images (low sample quality). This has been shown in the visual domain with StyleGAN2 and diffusion models [2; 3], as well as in the text domain with Large Language Models (LLMs) [5; 9; 37]. The current solution to mitigate this collapse is to include a fraction of real data in the training loop at all the generations [2; 3]. Theoretical results have also proved that super-quadratic number of synthetic samples are necessary to prevent model collapse  in the absence of support from real data. A concurrent work  studied the setup of data accumulation in recursive training where data from previous iterations of generative models together with real data are accumulated over time. The authors conclude that data accumulation (including real data) can avoid model collapse in various settings including language modeling and image data.

Past works have only studied the collapse of the generative model to the mode of the existing distribution. Through some controlled experiments, we study the interaction between different modes (a mode can be a class) or novel modes being developed in the generative models. This provides novel insights into the reasons behind the collapse of generative models during recursive training.

Failure Modes of Diffusion Models.One of the common failure modes of diffusion models is the generation of images where the hands and legs appear distorted or deformed which is commonly observed in Stable Diffusion  and Sora . Diffusion models also fail to learn rare concepts which have less than 10k samples in the training set. Various other failure modes including ignoring spatial relationships or confusing attributes have been discussed in [4; 23].

Hallucination in Language Models.Hallucination in LLMs [46; 47] is a huge barrier to the deployment of LLMs in safety-critical systems. The LLMs may provide a factually incorrect output or incorrectly follow the instructions or be logically wrong. A simple example is that LLMs can generate new facts when asked to summarize a block of text (input-conflicting hallucination) . Current hallucination mitigation techniques in LLMs include factual data enhancement , retrieval augmentation  among other methods. Given the widespread adoption of image generation models, we argue that hallucination in diffusion models must also be studied carefully to identify its causes and mitigate it.

## 3 Definitions and Preliminaries

Let \(q(x)\) be the real data distribution. We define a forward process where Gaussian noise is iteratively added at each timestep for a total of \(T\) timesteps. Let \(x_{0} q(x)\), and \(x_{t}\) be the perturbed (noisy) sample after adding \(t\) timesteps of noise. The noise schedule is defined by \(_{t}(0,1)\), which represents the variance of Gaussian (added noise) at time \(t\). For large enough \(T\), \(x_{T}(0,)\)

\[q(_{t}|_{t-1})=(}_{ t-1},_{t}); q(_{1:T}|_{0})=_{t=1}^ {T}q(_{t}|_{t-1}) \]

In the forward diffusion process, we can directly sample \(x_{t}\) at any time step using the closed form \(q(_{t}|_{0})=(_{t};_{t}}_{0},(1-_{t}))\) where \(_{t}=1-_{t}\) and \(_{t}=_{j=1}^{t}_{j}\).

The reverse diffusion process aims to learn the process of denoising i.e, learning \(p_{}(x_{t-1}|x_{t})\) using a model (such as a neural network) with \(\) as the learnable parameters.

\[p_{}(_{0:T})=p(_{T})_{t=1}^{T}p_{}( _{t-1}|_{t}); p_{}(_{t-1}| _{t})=(_{t-1};_{}(_{t},t),_{ }(_{t},t)) \]

The mean can be derived as \(_{}(_{t},t)=}}(_{t} -}{_{t}}}_{}(_{t },t))\) where \(_{}(_{t},t)\) is the predict noise at timestep \(t\) using the neural network. The original DDPM is trained to predict the noise \(_{t}\) instead of \(x_{t}\) and the variance \(_{}(_{t},t)\) is fixed and time-dependent. Since then, improved methods have learned the variance . We define predicted \(x_{0}\) as \(}=}}(_{t}-_{t}}_{}(_{t},t))\)

Connections to Score Based Generative Models.The score function \(s(x)\) of a distribution \(p(x)\) is the gradient of the log probability density function i.e, \(_{x} p(x)\). The main premise of score-based generative modeling is to learn the score function of the data distribution given the samples from the same distribution. Once this score function is learned, annealed Langevin dynamics can be used to sample from the distribution using the formula \(_{t+1}_{t}+_{} p( )+_{t},\) where \(\) is the step size and \(z_{t}\) is sampled from standard normal. The score function can be obtained from the diffusion model using the equation \(s_{}(x_{t},t)=-(_{t},t)}{_{t}}}\).

## 4 Understanding Mode Interpolation and Hallucination

In this section, we provide initial investigations into the central phenomenon of hallucinations in diffusion models. Formally, we consider a hallucination to be a generation from the model that lies entirely outside the support of the real data distribution (or, for distributions that theoretically have full support, in a region with negligible probability). That is, the \(\)-Hallucination set \(H_{}(q)\)

\[H_{}(q)=\{x:q(x)\}, \]

where we typically take \(=0\) or take \(\) to be vanishingly small (well beyond numerical precision). We similarly define the \(\)-support set \(S_{}(q)\) to simply be the complement of the \(\)-Hallucination set.

Mode interpolation occurs when a model generates samples that directly _interpolate_ (in input space) between two samples in the \(\)-support set, such that the interpolation lies in the \(\)-Hallucination set. That, is for \(x,y S_{}(q)\) the model generates \( x+(1-)y H_{}(q)\). The main argument of this paper, shown through examples and numerical analysis of special cases, is that diffusion models frequently exhibit mode interpolation between "nearby" modes in the data distributions, and such interpolation leads to the generation of artifacts that did not exist in the original data (hallucinations).

### 1D Gaussian Setup

We have already seen how hallucinations manifest in the Simple Shapes set-up (SS 1.1). To investigate hallucinations via mode interpolation, we begin with a synthetic toy dataset characterized by a mixture of 1D Gaussians given by: \(p(x)=(_{1},^{2})+(_{2 },^{2})+(_{3},^{2})\). For our initial experiments, we set \(_{1}=1,_{2}=2,_{3}=3\) and \(=0.05\). We sample 50k training points from this true distribution and train an unconditional DDPM using these samples with \(T=1000\) timesteps for \(10,000\) epochs. Additional experimental details are present in the Appendix A.

We observe that diffusion models can generate samples that interpolate between the two nearest modes of the mixture of Gaussians (Figure 2). To clearly observe the distribution of these interpolated samples, we generated 100 million samples from the diffusion models. The probability of sampling from the interpolated regions (regions outside the support of the real data density, outlined in red) is non-zero, and decays with the distance from the modes. This region has nearly 0 probability mass of the true distribution, and no samples in this region occurred in the data used to train the DDPM.

The rate of mode interpolation depends primarily on three factors: **(i)** Number of training data points, **(ii)** variance of (and distance between) the distributions, and **(iii)** the number of sampling timesteps (\(T\)). As the number of training samples increases, we observe that the proportion of interpolated samples decreases. In this setup, the variance of \(p(x)\) not only depends on \(\) but also the distance between the modes i.e, \(|_{1}-_{2}|\) and \(|_{2}-_{3}|\). We run another experiment with \(_{1}=1,_{2}=2\) and \(_{3}=4\). In this case, we observe that the frequency of samples between \(_{2}\) and \(_{3}\) is much lower than \(_{1}\) and \(_{2}\). The number of interpolated samples also decreases as the distance from the modes increases. The frequency of interpolated samples is also inversely proportional to the number of timesteps \(T\). Additional experiments with varying Gaussian counts are in Appendix C.

### 2D Gaussian Grid

The reduction in density of mode interpolation as two modes with \(=\) are moved apart calls for closer inspection into when and how diffusion models choose to interpolate between nearby modes. To investigate this, we make a toy dataset with a mixture of 25 Gaussians arranged in a two-dimensional square grid. A total of 100,000 samples are present in the training set. Similar to the 1D case, we observe interpolated samples between the two nearest modes of the Gaussian. Again, these samples have close to zero probability if sampled from the original distribution (Figure 3).

Figure 2: **Mode Interpolation in 1D Gaussian**. The red curve indicates the PDF of the true data distribution \(q(x)\), which is a mixture of 3 Gaussians (notice that the y-axis is in log-scale). In blue, we show a density histogram of the samples generated by a DDPM trained on varying number of samples from the true data distribution. For each histogram, we sampled 100 million examples from the diffusion model to observe the interpolated distribution. **(a,b)** show how the density of samples generated in the interpolated region reduces with an increase in the number of samples from the real distribution (used for training the DDPM). **(c,d)** show the impact of moving one of the modes (originally at \(=3\)) to \(=4\). We see how the density of samples generated in the region between distant (but neighboring) modes is significantly lesser than that between nearby modes.

We note that mode interpolation only happens between the nearest neighbors. To demonstrate this occurrence, we also train a DDPM on the rotated version of the dataset where the modes are arranged in the shape of a diamond (Figure 3.c,d). The mode interpolation can be more clearly observed in this setting. Interestingly, there appears to be no interpolation between modes along the x-axis, indicating that only the nearest modes are being interpolated. We believe this empirical observation of mode interpolation being confined to nearby modes will spark further investigation in future research.

### What causes mode interpolation?

To understand the reason behind the observed mode interpolation, we analyze the score function learned by the model. The model learns to predict \(_{}\) which is related to the score function as \(s_{}(x_{t},t)=-(x_{t},t)}{}}\). We know the true score function for the given mixture of Gaussians, and we can estimate the learned score function using the model's output. In Figure 4, we plot the ground truth score (left) and the learned score (right) across various timesteps. We observe that the neural network learns a smooth approximation of the true score function, particularly around the regions between disjoint modes of the distribution from timesteps \(t=0\) to \(t=20\). Notice that the true score function has sharp jumps that separate two modes, however, the neural network can not learn such sharp functions and smoothly approximates a tempered version of the same. We also plot the estimated \(}\) and observe a smooth approximation of the step function instead of the exact step function. There is a region of uncertainty in the region between the two modes which leads to mode interpolation

Figure 4: **Explaining Mode Interpolation via Learned Score Function. The left panel shows the ground truth score function for a mixture of Gaussians across various timesteps, while the right panel illustrates the score function learned by the neural network. While the true score function exhibits sharp jumps that separate distinct modes (particularly in the initial time steps), the neural network approximates a smoother version.**

Figure 3: **Mode Interpolation in 2D Gaussian. The dataset consists of a mixture of 25 Gaussians arranged in a square grid, with a training set containing 100,000 samples. (a,b) The blue points represent samples generated by a DDPM, with visible density between the nearest modes of the original Gaussian mixture (in orange). These interpolated samples have near-zero probability in the original distribution. (c,d) We trained a DDPM on a rotated version of the dataset where the modes form a diamond shape. In this configuration, we see no interpolation along the x-axis, illustrating that diffusion models interpolate between nearest modes.**

i.e sampling in the regions between the two modes. As another sanity check, we used the true score function in the reverse diffusion process for sampling (instead of the learned network). In this case, we did not see any instance of mode interpolation. This explains why the diffusion model generates samples between two modes of a Gaussian when it was never in the training distribution.

### Simple Shapes

We now discuss the mode interpolation in the Simple Shapes dataset. In this context, the interpolation is not happening in the output space, but rather in the representation space. To investigate this, we performed a t-SNE visualization of the outputs from the bottleneck layer of the U-Net used in the Simple Shapes experiment, as shown in Figure 10. Regions 1 and 3 in the representation space semantically correspond to the images where squares are at the top and bottom of the image respectively. At inference time, we can see a clear emergence of region 2 which is between regions 1 and 3 (interpolated), and contains two squares (hallucinations) at the top and bottom of the image. This experiment concretely confirms that interpolation happens in representation space.

### Mode Interpolation in Real World datasets: Hands

We sought to demonstrate the occurrence of mode interpolation in a real-world setting. A well-documented challenge with popular text-to-image generative models is their difficulty in accurately generating human hands . Despite extensive research in modern diffusion models, there is no conclusive explanation for the missing/additional fingers generated by these models. One hypothesis attributes this difficulty to the anatomical complexity of human hands, which involve numerous joints, fingers, and diverse poses. Another hypothesis suggests that, although large datasets contain many images of hands, these hands are often partially obscured (e.g., when a person is holding a cup) and occupy only a small region of the overall image.

To investigate this further, we trained a diffusion model on a datasets with high-quality images of human hands. The Hands dataset  consists of high resolution images of hands from 190 subjects of various ages. Each subject's right and left hands were photographed while opening and closing fingers against a uniform white background. We sample 5000 images from the Hands dataset and train an ADM  model on this dataset. We resize the images to 128x128 and use the same hyperparameters as that of the FFHQ dataset . We mention all the hyperparameters in the Appendix A. We observe images with additional and missing fingers in the generated samples as seen in Figure 5. This is a pretty surprising result as it is non-trivial to assume that diffusion model generates images with additional fingers. Despite the potential for various failure modes, such as blurred hand images, these issues were not observed in our results. In some ways, the occurrence of 6-8 fingers is analogous to the occurrence of 2 squares in the Simple Shapes dataset. Thus, the presence of additional fingers in these images (i.e hallucinated images) generated by the diffusion model demonstrates the phenomenon of mode interpolation in real-world datasets. More example are shown in Fig. 20 & 21.

## 5 Diffusion Models know when they Hallucinate

Our previous sections established that hallucinations in diffusion models arise during sampling. More specifically, intermediate samples land in regions between different modes where the score

Figure 5: **Hands Dataset**. We train a ADM on the Hands dataset with 5000 images (first column) and show that the generated samples (second column) consists of hallucinated samples (additional/missing fingers). We then apply our proposed metric to detect these hallucinated samples (third column).

function has high uncertainty. Since neural networks find it hard to learn discrete 'jumps' between different modes (or a perfect step function), they end up interpolating between different modes of the distribution. This understanding suggests that the trajectory of the samples that generate hallucinations must have high variance due to the highly steep score function in the region of uncertainty. We will build upon this intuition to identify hallucinations in diffusion models.

### Variance in the trajectory of prediction

We revisit the hallucinated samples in the 1D Gaussian setup, and examine the trajectory of the predicted value of \(}\) during the reverse diffusion process. Figure 6 depicts the variance of trajectories leading to hallucinations (red shades) and those generating samples within the original data distribution (blue shades). For trajectories in shades of blue (non-hallucinations), the variance remains low beyond timestep \(t=20\). This indicates there is a minimal change in the predicted \(}\) during the final stages of reverse diffusion, signifying convergence. Conversely, the red trajectories (hallucinations) exhibit instability in the value of \(}\) in the same region. This suggests a high overall variance in these trajectories.

### Metric for detecting hallucination

Based on the above observation about high variance in predicted values of \(x_{0}\) in the reverse diffusion process, we use the same observation as a metric to distinguish hallucinated and non-hallucinated (in-support) samples. The intuition behind the metric is to capture the variance in the trajectory of \(}\). Let \(T_{1}\) be the starting timestep and \(T_{2}\) be the end timestep. Mathematically, the metric can be defined as follows:

\[(x)=-T_{1}|}_{i=T_{1}}^{T_{2}}( }^{(i)}-}^{(t)}})^{2} \]

Figure 6: **Variance of \(}\) Trajectories**. The trajectory of the predicted \(}\) for hallucinated (shades of red), and non-hallucinated samples (shades of blue). We see that non-hallucinated samples stabilize in their prediction in the last 20 time steps for both 1D Gaussian and 2D Gaussian setups, whereas the hallucinated samples have high variance in the predicted \(}\) across time steps.

Figure 7: **Histogram of Hallucination Metric**. We depict the hallucination metric values for (a) 1D Gaussian, (b) 2D Gaussian, and (c) Simple Shapes setups. The histograms show that trajectory variance can capture a separation between hallucinated (orange) and non-hallucinated (blue) samples.

where \(}^{(t)}\) represents the predicted values of the final image at different time steps \((t)\), and \(}^{(t)}\) is the mean of these predictions over the same time steps. We now utilize this metric to analyze the histogram values of each sample from the three experimental setups studied thus far. This metric can be implemented in two ways. One approach is to store \(}\) during the reverse diffusion process and then compute the variance. Alternatively, we explore a method where forward diffusion is performed for \(k\) steps between \(T_{1}\) and \(T_{2}\), predicting \(}\) at each step, and then computing the variance.

Simple Shapes.In the Simple Shapes setup, a sample is labeled as hallucinated if more than one shape of the same type occurs in the generated image. We generate 7500 images using a DDPM and study the separation between hallucinated and non-hallucinated images. We find that the reverse diffusion process of \(T=1000\) steps is rather long. Generally, the image stabilizes around \(T=700\) (as shown in Appendix 18). Therefore, we use the time range between \(T=850\) and \(T=700\) in the reverse diffusion process to compute the variance of the predicted sample value. Using this process, we can filter out 95% of the hallucinated samples while retaining 95% of the in-support samples. The histogram for the values is presented in Figure 7.

1d Gaussian.In the 1D-Gaussian setup, we label any examples as a hallucination if they have negligible probability (for instance values greater than \(6\) from the mean under normal) under the real data distribution (refer to Figure 2). We measure the variance of the last 15 steps of the \(}\) during the reverse diffusion process, and plot the histogram of values of the same in Figure 7. We can filter out 95 % of the hallucinated samples while retaining 98% of the in-support samples.

2d Gaussian.Next, we discuss our investigation on synthetic datasets with experiments on the 2D Gaussian dataset. Similar to the 1D Gaussian setup, we once again measure the prediction variance of the last 20 steps of the reverse diffusion process. We compute the variance per dimension and then take the mean across dimensions to. With this metric, we can filter out 96% of the hallucinated samples while retaining 95% of the in-support samples.

Hands.Finally, we conclude our investigation with experiments on the Hands dataset. To analyze the effectiveness of the proposed metric, we manually label 130 images from the generated samples as hallucinated vs. in-support. This includes 88 images with 5 fingers and 40 images with missing/additional fingers i.e. hallucinated samples. The histogram (in Figure 5) shows that the proposed metric can indeed detect these hallucinations to a reasonable degree. In our experiments, we observe that we can eliminate 80% of the hallucinated samples while retaining 81% of the in-support samples. The trajectories of the hallucinated and in-support samples are shown in Figures 22 and 23, respectively. A higher variance in the trajectory of \(}\) is clearly observed in the hallucinated samples compared to the in-support samples. We note that the detection is a hard problem and the fact that the method transfers to the real world is proof of the relationship between mode interpolation and hallucination in real-world data.

## 6 Implications on Recursive Model Training

The internet is increasingly populated by more and more synthetic data (data synthesized from generative models). It is likely that future generative models will be exposed to large volumes of machine-generated data during their training [26; 27]. Recursive training on synthetic data leads to mode collapse [2; 9] and exacerbates data biases. In this section, we study the impact of hallucinations within the context of recursive generative model training. We adopt the standard synthetic-only setup similar to  where we only use synthetic data from the current generative model in training the next generation of generative models. The first generation of generative model is trained on real data and samples from this generative model is used to train the second generation (and so on).

Most of the previous works  studied the model collapse to a single mode. In this work, we emphasize that the interaction between modes and mode interpolation plays a massive role when training generative models on their own output.

2d Gaussian.When we recursively train a DDPM on its own generated data using a square grid of 2D Gaussians (with \(T=500\)), the hallucinated samples significantly influence the learning of the next generation's distribution (see Figure 9). The frequency of the interpolated samples increases as we further train on the learned distribution that consists of interpolated samples. Figure 9 shows samples from Generation 20, where it is evident that the modes have almost collapsed into a single mode, differing greatly from the original data distribution.

Simple Shapes.We define a hallucinated sample as one that contains at least two shapes of the same type (which is never seen in the training distribution). We observe the presence of around 5% hallucinated samples when trained on the real data. We note that the ratio of hallucinated samples increases exponentially as the we iteratively train the diffusion model on its own data. This is expected as the diffusion model progressively learns from a distribution increasingly dominated by hallucinated images, compounding the effect in subsequent generations.

Mnist.We also run the recursive model training on the MNIST dataset . At every generation, we generate 65k images and sample 60k images using the filtering mechanism. For each generation, we train a class conditional DDPM with Classifier-Free Guidance  with \(T=500\) for 50 epochs. To evaluate the quality of the generated images, we compute the FID  using a LeNet  trained on MNIST instead of Inception backbone as MNIST is not a natural image dataset. In Figure 8, we clearly see that the proposed metric based on the variance of the trajectory outperforms the random filtering method across all generations (lower FID is better). We also plot the Precision and Recall  curves (in the Appendix Figure 18) where we observe that our filtering mechanism selects high quality samples without much loss in diversity.

Mitigating the curse of recursion with pre-emptive detection of hallucinations.Based on the metric developed in SS 5, we analyze the efficacy of the proposed metric in filtering out the hallucinated samples for the next generation of training. After training each generation of the generative model, we sample \(k\) images more than size of the training data and then filter out hallucinated samples based on the metric. Figure 8 shows the results on 2D Grid of Gaussians, Simple Shapes and MNIST dataset. We also compare with random filtering where we randomly sample points for the next generation. The variance-based filtering method easily outperforms the random sampling method in all the generations. We see the effectiveness of the proposed metric in minimizing the rate of hallucinations across generations and thus model collapse to a certain extent. This holds true for all the three datasets we have studied in this work.

## 7 Discussion

In this work, we performed an in-depth study to formulate and understand hallucination in diffusion models, focusing on the phenomenon of mode interpolation. We analyzed this phenomenon in four different settings: 1D Gaussian, 2D Grid of Gaussians, Shapes and Hands datasets, and saw how diffusion models learn smoothed approximations of disjoint score functions, leading to mode interpolation. Based on our analysis, we developed a metric to identify hallucinated samples effectively and explored the implications of hallucination in the context of recursive generative model training. This study is the first to propose mode interpolation as a potential hypothesis for explaining the generation of additional fingers in large-scale generative models. We hope that future research will build upon this hypothesis and develop methods to mitigate these issues in generative models. We hope our work inspires future research in understanding and mitigating hallucination in diffusion models.

Figure 8: **Mitigating Hallucinations with Pre-emptive Detection**. We filter out hallucinated samples using the metric from § 5 before training on samples from the previous generation of the diffusion model. In the case of (a) 2D Gaussian, (b) Simple Shapes, where we have clear definitions of hallucination (mode interpolation, and new shape combinations) we see the effectiveness of our variance-based filtering method in minimizing hallucinations across generations compared to random filtering. In the case of (c) MNIST dataset, we measure the FID of subsequent generations and notice that pre-emptive filtering of hallucinated samples makes the recursive model collapse slower.