# On Slicing Optimality for Mutual Information

Ammar Fayad

MIT

afayad@mit.edu

Majd Ibrahim

HIAST

###### Abstract

Measuring dependence between two random variables is of great importance in various domains but is difficult to compute in today's complex environments with high-dimensional data. Recently, slicing methods have shown to be a scalable approach to measuring mutual information (MI) between high-dimensional variables by projecting these variables into one-dimensional spaces. Unfortunately, these methods use uniform distributions of slicing directions, which generally discard informative features between variables and thus lead to inaccurate quantification of dependence. In this paper, we propose a principled framework that searches for an _optimal_ distribution of slices for MI. Importantly, we answer theoretical questions about finding the optimal slicing distribution in the context of MI and develop corresponding theoretical analyses. We also develop a practical algorithm, connecting our theoretical results with modern machine learning frameworks. Through comprehensive experiments in benchmark domains, we demonstrate significant gains in our information measure than state-of-the-art baselines.

## 1 Introduction

Mutual information (MI) measures statistical dependence between two random variables by quantifying the amount of information gained about one variable from an observation of the other variable (Shannon, 1948). Despite its popularity in various fields (Marinoni and Gamba, 2017; Hamma et al., 2016), MI suffers from the curse of dimensionality: the estimation of mutual dependence is often stymied by the large dimension of the variables (Kraskov et al., 2004). Recently, slicing methods (Bonneel et al., 2015; Nadjahi et al., 2020) have demonstrated a scalable approach for estimating probability divergences and mutual dependence. Specifically, sliced mutual information (\(\); Goldfeld and Greenewald (2021)) measures the average of MI terms between one-dimensional projections of the high-dimensional variables, where the projecting slicing directions are sampled uniformly from a unit sphere. Because slicing methods do not compute the dependence directly in the high-dimensional space but in the low-dimensional space, these methods significantly reduce computation costs and are scalable. However, a critical problem of these slicing frameworks is that they generally require many slices for accurate estimation. As we detail later in the paper, the root cause of this problem is the uniform sampling of the slicing directions: discarding informative features about the variables and generating noisy slices by equally favoring important and irrelevant regions of the variables' spaces. Hence, the uniform sampling fails to capture complex relationships of variables which renders current slicing methods inefficient and can even lead to inaccurate dependence quantification.

Our contribution.With this insight, this paper aims to address the uniform sampling issue with the slicing approaches for improving the dependence measurement. Specifically, motivated by the success of Nguyen et al. (2021) in the generative modeling domain, we seek to find an _optimal_ distribution of slicing directions in the context of MI that satisfies two criteria: 1) slices are distributed over maximally informative regions, and 2) slices are scattered over the unit sphere so that norelevant directions are discarded. We formalize these criteria in a regularized optimization problem and introduce a novel measure of dependence, the optimal sliced mutual information (denoted by \(^{*}\)). \(^{*}\) provides a much more efficient performance compared to previous slicing methods and dependence measures (e.g., improvements of up to \(30\%\) in dependence detection power). We show an important benefit of \(^{*}\), which is that it can transform the fundamental work of Goldfeld and Greenewald (2021) (i.e. \(\)) to larger and more complex machine learning problems.

Several unique challenges arise when developing our regularized optimization problem. First, MI requires at least two slicing variables and the optimization should be over the space of the joint distribution of the slices, but the constraint is enforced over the marginals. We find that this challenge essentially corresponds to solving optimal transportation (OT) (Villani, 2009) problems: optimization over a set of couplings (of the distributions of slices) with given constraints. To that end, we leverage OT properties that enable us to derive many interesting theorems about \(^{*}\) connecting it to MI, SI, and differential entropy. The second challenge is the continuity problem: the cost function in OT is assumed to be upper semicontinuous in the topology of weak convergence for the _maximizing_ solution to exist, but this continuity assumption no longer holds when dealing with information-theoretic quantities. We rigorously prove that, under very mild assumptions, a maximizing solution exists for any input variables regardless of their dimensions.

In summary, this paper bridges the gap between intractable information/dependence measures and modern computational frameworks by addressing the shortcomings of current slicing approaches. We investigate the optimality of slicing directions for MI with the following key contributions:

* **Formalization of a new dependence measure (Sections 3.1 and 3.2).** We introduce a novel and scalable dependence measure, \(^{*}\) with theoretical analyses of its properties and implications.
* **Scalable estimator (Sections 3.3 and 4).** We construct an optimal estimator with the tightest bounds and explicitly show that the effect of the variables' dimensions on the convergence rate is only up to a constant factor. We then employ deep neural networks to this optimal estimator to acquire an end-to-end \(^{*}\) neural estimator and show that it is computationally efficient.
* **Comprehensive evaluation of our approach (Section 6).** We demonstrate significant gains of \(^{*}\) in the accuracy of detecting complicated relations over state-of-the-art baselines on an extensive set of experiments. \(^{*}\) also works excellently across challenging domains (e.g., representation and reinforcement learning), which is an unprecedented advantage and a feature that is largely missing from current literature on statistical dependence.

## 2 Background

**Notation.** We follow standard notations from Villani (2009). Specifically, this paper considers Borel measures on Polish spaces; the latter are complete, separable metric spaces, equipped with their Borel \(\)-algebra. We denote the space of Borel probability measures on \(\) as \(()\). If \(\) is a Borel measure on \(\), and \(T\) is a Borel map \(\), then \(T_{\#}\) stands for the push-forward measure of \(\) by \(T\): it is a Borel measure on \(\), defined by \(T_{\#}[A]=[T^{-1}(A)]\) for any \(A\), where \(_{x}\) is the Dirac delta function at \(x\). If \((d d)\) is a probability measure in two variables \(\) and \(\), its marginal (or projection) on \(\) (resp. \(\)) is the measure \(p^{}_{\#}\) (resp. \(p^{}_{\#}\)), where \(p^{}(,)=\) and \(p^{}(,)=\). We focus on absolutely continuous random variables with bounded density functions. Let \(^{*}\): \(x^{}x\) and \(()\) is the uniform distribution on \(\). For any two probability measures \(\), \(\), \(d/d\) denotes the Radon-Nikodym derivative of \(\) with respect to \(\). Furthermore, let \(()(x,y)=((x),(y))\) be the Cartesian product of two functions \(,\). Denote by \((,)\) the set of continuous Borel maps from \(\) to \(\). \(\|.\|\) denotes the Euclidean norm. Finally, \(^{d-1}\) is the unit sphere in \(^{d}\), and \(_{d,k}}}{{=}}^{d-1} ^{k-1}\).

**Information theory.** The mutual information (Shannon, 1948) characterizes the distance between the joint distribution \(P_{X,Y}\) and the product of its marginals \(P_{X} P_{Y}\):

\[(X;Y)=(P_{X,Y}||P_{X} P_{Y}))=_{ }(}{dP_{X} P_{Y}})dP_{X, Y},\]

where \((||)\) is the Kullback-Leibler divergence (Kullback and Leibler, 1951) which is lower semicontinuous in the topology of weak convergence (Posner, 1975). The _Sliced_ mutual information (Goldfeld and Greenewald, 2021) is the mean of MI terms between one-dimensional projections of the variables:

\[(X;Y)=_{_{d_{x},d_{y}}}(^{}X;^{ }Y)d()(). \]

In Equation (1), \(X^{d_{x}},Y^{d_{y}}\); \(\) and \(\) are the slices corresponding to \(X\) and \(Y\), respectively. We note that slices are independently and uniformly sampled from \(^{d_{x}-1}\) and \(^{d_{y}-1}\). This uniformity causes redundancy in slices which deems optimality beneficial and necessary for the slicing process.

## 3 Definition and Theories of Slicing Optimality

In this section, we begin by motivating the definition of our novel measure by showing the advantage of our solution over uniform sampling. We then formalize our proposal as a regularized optimization problem. Lastly, we discuss our theoretical findings, such as the existence of an optimal slicing policy and the properties of \(^{*}\). These studies show that \(^{*}\) possesses desired characteristics of a dependence measure.

### Definition of \(^{*}\)

We first motivate our solution in an example, in which we incorporate the following two optimality criteria into \(\):

1. The projection directions are mainly concentrated into areas where the one-dimensional variables contain the maximum mutual information possible.
2. The slicing directions are also diversified over the whole sphere, ensuring that all regions with relevant information are visited.

**Motivating example.** Let \(Z(0,I_{3}),(0,0.1I_{2}),\) and let \(X\!=\![Z_{1},Z_{2}]^{},Y\!=\![Z_{1},Z_{3}]^{}\!+\!\). In this example, we aim to estimate the average MI between one-dimensional projections of the two variables \(X\) and \(Y\) and show that our optimal distribution of slices yields superior performance over standard \(\). Specifically, by sampling slicing directions, \(\) and \(\), uniformly and independently from the 2-sphere (i.e., unit circle), we obtain \(\). To construct the distribution of slices that meets the former criteria, we first note that the slices which project only the first entries of \(X,Y\) yield the maximum information because both variables share \(Z_{1}\) as their first entry (up to additive noise) while their second entries are independent. We leverage this observation and sample two-dimensional slices such that they are mainly concentrated around \(|_{0}|=1\), and \(|_{0}|=1\), respectively. We also enforce the second criterion of slices diversity by choosing a few slices away from the cluster (See Figure 1 middle and right). We refer to Appendix D.1 for details.

Figure 1 (left) shows the area under the curve (AUC) of the receiver operating characteristic (ROC) as a function of the number of slices along with a visualization of the customized distribution of \(,\). Note the improvement in the number of slices needed until reaching an ROC AUC = 1, as our customized measure requires \( 75\%\) fewer slices to reach perfect accuracy. This result shows that our proposed distribution can extract sufficient information more effectively.

Having established the importance of optimal slicing, we now formally define the optimal sliced mutual information (\(^{*}\)) as a weighted average of information stored in one-dimensional projections of two random variables \(X,Y\):

**Definition 1** (Optimal sliced mutual information).: _Given two random variables \(X^{d_{x}},Y^{d_{y}}\) and \(_{X},_{Y}[0,/2]\), define the following collection of probability measures \(_{d,}=\{:^{d_{x}}\}\):_

Figure 1: ROC AUC curve (left) and visualization of the custom distribution the slices (middle and right). Optimal distribution of slices can capture more information and yields a more accurate measure of dependence.

\((^{d-1})\), \(_{x,y}[|x^{}y|]\). The Optimal Sliced Mutual Information can be expressed as:

\[^{*}(X;Y)=_{_{d_{x},dy}}(^{}X;^{}Y)d(,):p_{\#}^{} _{d_{x},_{X}},p_{\#}^{}_{d_{y},_ {Y}}}. \]

We call the distribution \(\) a slicing policy. Note that \(p_{\#}^{}\) refers to the first marginal distribution of \(\) (in other words, the law of the random slicing vector of \(X\), \(\), i.e. \(()=p_{\#}^{}\)); \(()_{d_{x},_{X}}\). This observation also applies to \(()_{d_{y},_{Y}}\). The supremum is taken to ensure that the slices convey sufficient information to quantify the relationship between \(X\) and \(Y\), or equivalently to ensure that the slices are prevalent in maximally informative regions (i.e., criterion 1). The set over which the supremum is taken, thus, refers to the joint measures whose marginals produce slices that are scattered over the unit sphere of an appropriate dimension (i.e., criterion 2), where we use the \(|.|\) to measure slices diversity. Importantly, without the constraint, the measure might collapse to a Dirac delta probability measure at the _max_ slicing vector (i.e. collapse to \(_{,}\)). As such, the prominent feature of \(^{*}\) is that it reveals the maximum amount of information content stored in \(X,Y\) by searching for a distribution of slices that results in informative, noiseless, and diverse projections.

**Remark 1**.: \(^{*}\) _is not a mutual information estimator and should not be used as a proxy of MI but rather as a new dependency measure. In fact, although it shares many properties with MI, \(^{*}\) has multiple advantages such as scalability and efficiency, not to mention the cutting-edge performance on detecting complex relationships between high-dimensional random variables in nontrivial settings._

**Remark 2**.: _If \(d_{x}=d_{y}=1,^{*}(X;Y)=(X;Y)\). Essentially, for one-dimensional r.v.s, \(^{*}\) boils down to MI._

We also consider the optimality of slicing into \(k\)-dimensional subspaces by enforcing our same criteria on distributions over the Stiefel manifold (Chikuse, 2012). Appendix E details this definition.

### Theoretical Properties of \(^{*}\)

We propose optimizing the slicing distribution towards one-dimensional projections with higher MI values. However, due to the lower semicontinuity of MI in the maximization problem, it is difficult to establish whether an optimal slicing policy exists. In the following, we prove the existence of a solution that our problem can converge to by assuming that for a given random variable \(X\) with pdf \(p_{X}\), \(\|x\|^{}p_{X}(x)dx<\) for some \(>1\):

**Theorem 1** (Existence of an optimal slicing policy).: _For any random variables \(X\) and \(Y\) and any \(_{X},_{Y}[0,/2]\), there exists a slicing policy \(\) such that the information functional, \((^{}X;^{}Y)d(,)\), is maximized among all possible couplings of \(((),())_{d_{x},_{X}}_ {d_{y},_{Y}}\)._

Proof.: See Appendix A.1 for details. 

We refer to such \(\) as an optimal slicing policy. Theorem 1 enables us to treat the problem as a reward-maximizing OT. Roughly speaking, \(^{*}(X;Y)\) can be deemed as a reward-maximizing optimal transport problem, where the reward induced by slicing vectors \(\) and \(\) is the amount of information they reveal about \(X\) and \(Y\). As such, the formula in Equation (2) measures the maximum information gain possible among slicing policies.

Renyi (1959) and Bell (1962) postulate that a measure of dependence should satisfy properties found in Theorem 2:

**Theorem 2** (Properties of \(^{*}\)).: _For random variables \(X\) and \(Y\), we have:_

1. \(^{*}(X;Y)\) _is nonnegative and symmetric._
2. \(^{*}(X;Y)=0\) _if and only if_ \(X\) _and_ \(Y\) _are independent._
3. _If_ \(X_{n},Y_{n}\) _are sequences of random variables with joint distribution_ \(P_{X,Y}^{(n)}\) _that converges pointwise to the joint distribution_ \(P_{X,Y}\)_, then_ \(_{n}^{*}(X_{n};Y_{n})=^{*}(X;Y)\)_._
4. _Similar to MI,_ \(^{*}\) _has a relative entropy form:_ \(^{*}(X;Y)=_{\ (,)}_{ (^{*}^{*})_{\#}P_{X,Y}||^{*}_{\#}P_{X} ^{*}_{\#}P_{Y}}=_{} (^{*}^{*})_{\#}P_{X,Y}||^{*}_{\#}P_ {X}^{*}_{\#}P_{Y}.\)Proof.: See Appendix A.2.2 for details. 

Now that we have established that \(^{*}\) possesses the desirable properties of a dependence measure, we emphasize the importance of using two slices:

**Remark 3**.: _Unlike existing slicing methods (Nguyen et al., 2021), \(^{*}\) requires two slicing variables \(\) and \(\) since using only one slice violates the important property (2) in Theorem 2. To illustrate, Let \(X(0,I_{2})\) and \(Y=0&\\ -&0X, 0,\) clearly \(X\) and \(Y\) are dependent but \(^{}X\) and \(^{}Y\) are not for all \(^{1}\) (the latter follows from the independence of the entries of \(X\). See (Goldfeld and Greenewald, 2021)). The observation implies that for the such dependent \(X,Y,^{*}(X;Y)=0\). We thus resort to using two slicing directions._

Next, we use the property (4) in Theorem 2 to represent \(^{*}\) using a discriminator function.

**Corollary 1** (Discriminator-based form).: _Let \(=(^{*}^{*})_{\#}P_{X,Y},=_{\#}^{*}P_{X} _{\#}^{*}P_{Y}\). We write \(^{*}\) as:_

\[^{*}(X;Y)=_{}_{(,)}_{} *{arg\,max}_{h}_{}[(  h(.))]\ -_{}[1-( h(.))],\]

where \(\) is the Sigmoid function and the discriminator \(h\) is defined on \(\). This GAN-type form may be used to estimate \(^{*}\) in Reproducing Kernel Hilbert Space similarly to Ghimire et al. (2021)'s approach for estimating KL.

Proof.: See Appendix A.2.1 for details. 

Moreover, we present a variational representation of \(^{*}\) which will be later used to construct a neural estimator:

**Corollary 2** (Variational representation).: _Let \((X,Y) P_{X,Y}\) and \((,)(_{d_{x},d_{y}})\). We have:_

\[^{*}(X;Y)=_{T,f_{1},f_{2}}& [T(,,f_{1}(,)^{}X,f_{2}( ,)^{}Y)]-\\ &\ [(T(,,f_{1}(, )^{},f_{2}(,)^{}))] }, \]

_where \((,) P_{X} P_{Y}\). The supremum is taken over \(T(_{d_{x},d_{y}}^{2},),\)\(f_{1}_{_{X}}}}{{=}}\{f:f (_{d_{x},d_{y}},^{d_{x}-1}),\)\(f_{\#}(_{d_{x},d_{y}})_{d_{x},_{X}}\},f_{2} _{_{Y}}\), where \(_{_{Y}}\) is defined analogously to \(_{_{X}}\)._

Proof.: See Appendix A.2.2 for details. 

Corollaries 1 and 2 provide useful representations of \(^{*}\) that make it compatible with modern machine learning algorithms and optimization models. Next, we highlight the concept of data processing that connects \(^{*}\) to \(\) and \(\).

**Theorem 3** (Data processing).: _For \(_{X},_{Y}/4\),_

\[(X;Y)^{*}(X;Y)(X;Y). \]

_Notably, \((X;Y)_{,}(^{}X;^ {}Y)^{*}(X;Y)(X;Y)_{, }(^{}X;^{}Y).\)_

Proof.: See Appendix A.3 for details. 

Theorem 3 shows that \(^{*}\) is a generalization of \(\) because, for the given values of \(_{X}\) and \(_{Y}\), \(_{d_{x},_{X}}\) and \(_{d_{y},_{Y}}\) contain the uniform distributions over the respective sphere. On the other hand, \(^{*}\) does not exceed MI as a consequence of the Data Processing Inequality. However, due to the scalability and sample efficiency issues in MI, \(^{*}\) is a much better candidate for quantifying dependence than MI especially when the data have complex structures.

### Estimation

A key property of \(^{*}\) is its scalability which is intrinsic to slicing approaches, where the latter facilitate estimation from samples and do not suffer from the curse of dimensionality since computations are performed in low-dimensional subspaces. Here, we construct an optimal \(^{*}\) estimator based on one-dimensional information estimators. Let \(}_{n}\) be a one-dimensional MI estimator over \(n\) samples whose absolute error is uniformly bounded by \((n)\):

\[_{P_{S,Q}}[|(S;Q)-}_{n}(S;Q)|] (n).\]

Any optimal slicing policy \(^{*}\) that maximizes the information functional can be obtained by applying an appropriate transformation on the uniform measure on \(_{d_{x},d_{y}}\); this transformation can be learned using neural networks. By the definition of the push-forward measure, we express \(^{*}(X;Y)\) as2:

\[^{*}(X;Y)=_{f_{1}_{_{X}},f_{2} _{_{Y}}}_{(,)( _{d_{x},d_{y}})}(f_{1}(,)^{}X;f_{2}( ,)^{}Y),\]

where \(_{_{X}},_{_{Y}}\) are defined in Corollary 2. The approximation then becomes:

\[^{*}}_{n,m}(X;Y)}}{{=}}_{f_{1}_{_{X}},f_{2}_{_{Y}}} _{j=1}^{m}}_{n}(f_{1}(_{j}, _{j})^{}X;f_{2}(_{j},_{j})^{}Y). \]

**Theorem 4** (Convergence Rate).: _The uniform error bound of \(^{*}}_{n,m}(X;Y)\) is:_

\[_{P_{X,Y}}[|^{*}(X;Y)- ^{*}}_{n,m}(X;Y)|](n)+},\]

_where \(U(d_{x}^{-1}+d_{y}^{-1})^{1/2}\) is a constant factor._

Proof.: See Appendix A.4 for details. 

We note that the estimation rate depends on the dimensions of the problem \(d_{x},d_{y}\) only up to the constant factor \(U\). The explicit dependence of \(U\) on \(d_{x},d_{y}\) has been a recent challenge which is now solved in Theorem 4. By imposing additional regularity on \(P_{X,Y}\), we can obtain \((n) C(n n)^{-} n^{1-}+n^{- 1/2}\) where \(s,p:0<s 2 p,\) and \(C>0\)(see Han et al. (2020); Goldfeld and Greenewald (2021)), which is significantly faster than the MI estimation rate, i.e. \(n^{-1/(d_{x}+d_{y})}\).

## 4 Neural Estimator for \(^{*}\)

In this section, we introduce a practical implementation of \(^{*}\) (based on Corollary 2) by leveraging a neural network estimator and connect our theoretical results in Section 3 with modern machine learning frameworks. Let \(}}{{=}}\{(X_{k},Y_{k})\}_{k=1}^ {m}\) be a mini-batch with \(X^{d_{x}}\), \(Y^{d_{y}}\), and \(n=\). We take \((X,Y)\{(X_{k},Y_{k})\}_{k=1}^{n}\), \((,)\{(X_{k},Y_{k+n})\}_{k=1}^{n}\),3 and \((_{k},_{k})(_{d_{x},d_{y}})\). Then, the neural network estimator \(^{*}_{W}\) is computed as:

\[_{T,f_{1},f_{2}}_{k=1}^{n}T(_{k},_{k},{f_{1}^{(k)}}^{}X_{k},{f_{2}^{(k)}}^{}Y_{k})- {1}{n}_{k=1}^{n}T(_{k},_{k},{f_{1}^{(k)}}^{} _{k},{f_{2}^{(k)}}^{}_{k})\] \[+_{1}}_{k,j}| {f_{1}^{(k)}}^{}{f_{1}^{(j)}}|-_{X}+_{2} {1}{n^{2}}_{k,j}|{f_{2}^{(k)}}^{}{f_{2}^{(j)}}|-_{Y} }, \]

where \(f_{i}^{(k)}=f_{i}(_{k},_{k})\) for \(i=1,2;k[n]\). \(_{1},_{2}>0,\) and \(}}{{=}}[_{T}, _{1},_{2}]\) are the parameters of neural networks \(T,f_{1},f_{2}\), respectively. We also follow Song and Ermon (2020) and account for the possible high-variance issues by using a smoothed version of Equation (6), where we clip the \(()\) term in the second line between \((-)\) and \(()\) where \(>0\). We refer to Appendix C for the pseudocode of \(^{*}_{W}\). We later analyze the computational complexity of the estimator.

## 5 Related Work

The main idea in quantifying dependence between random variables \(X,Y\) is to characterize the distance between their joint distribution \(P_{X,Y}\) and the product of its marginals \(P_{X} P_{Y}\). Naturally, this problem has been studied in the area of optimal transportation theory (Mordant and Segers, 2022; Wiesel, 2021; Chuang et al., 2023). For example, Nies et al. (2021) defined the distance as the optimal transport cost on the spaces of the aforementioned distributions for a restricted set of cost functions. Other popular approaches study the distance covariance, which is the weighted Euclidean distance between the joint characteristic function and the product of the marginals' characteristic functions (Szekely et al., 2007). Also, Reshef et al. (2011) introduced MIC, a measure of dependence for the 2-variable relationships based on testing multiple grids with varying dimensions and searching for the grid with maximum information between the two variables. Notably, (Liu et al., 2021) benefit from the optimal transportation theory to calculate Squared-loss Mutual Information with a small number of paired samples (drawn from the joint distribution) and a large number of unpaired samples. Lastly, MID (Sugiyama and Borgwardt, 2013) uses the concept of fractal dimensions (Ott, 2002) to define the information dimension and shows benefits in various applications.

In the context of sliced statistical distances, Nguyen et al. (2021) proposed a similar solution to alleviate the problem of using a large number of slices and applied it to the Wasserstein (W-) distance by reformulating the distance as an optimization problem. While we share a similar motivation, the two slicing approaches have significant differences stemming from the fact that Sliced W-distances have only been studied in the context of generative modeling; they only require one slicing direction; and their cost function (1-Wasserstein distance) is continuous, which makes their problem a vanilla expectation maximization (EM) problem. In a different domain, Nguyen et al. (2020) proposed an improvement to relational regularized autoencoders by finding an important area of projections characterized by a von Mises-Fisher (vMF) distribution (Jupp and Mardia, 1979). Optimizing over the family of vMF distributions helps to identify important directions on the sphere (\(\) parameter) and how much weight to put there compared to other directions (\(\) parameter). We, however, do not restrict our optimization to a particular family of distributions since the optimal slicing policy might not take the form of vMF.

## 6 Experiments

We conduct extensive experiments to demonstrate \(^{*}\)'s efficacy. We compare \(^{*}\)'s dependence quantification quality to a diverse set of competitive baselines. Through careful analyses, we show \(^{*}\)'s scalability, computation speed, sample efficiency, and slicing efficiency. We further provide empirical evidence that \(^{*}\) works excellently across challenging representation learning and rein

Figure 2: Statistical efficiency of dependence measures with structures of varying complexity \(X^{3},Y^{3}\). The AUC ROC quantifies the measure’s discriminative ability, where a perfect classifier has AUC ROC of 1.0 while a random classifier has AUC ROC of 0.5. The figures are computed from 100 random runs.

forcement learning tasks. We refer to Appendix B for further analysis experiments and Appendix D for implementation details and hyperparameters provided at [https://bit.ly/3foLke2](https://bit.ly/3foLke2).

### Effectiveness of \(^{*}\) as a Dependence Measure

We first illustrate the superiority of \(^{*}\) as a dependence measure by comparing against the following state-of-the-art measures: Mutual Information (MI) (Shannon, 1948; Kraskov et al., 2004), Sliced Mutual Information (SI) (Goldfeld and Greenewald, 2021), distributional sliced Wasserstein Dependence (WD) (Nguyen et al., 2021), Transport Dependency Coefficient (TD) (a.k.a Transport Correlation) (Nies et al., 2021), Mutual Information Dimension (MID) (Sugiyama and Borgwardt, 2013), Maximal Information Coefficient (Reshef et al., 2011), and Distance Correlation (dCor) (Szekely et al., 2007). We perform ROC curve analysis to examine precision and recall in recognizing complex relationship types with the existence of uniformly distributed noise. We test on synthetic data that comprises positive samples that depict extremely complicated structures of \(P_{X,Y}(^{3}^{3})\). In this setting, we conduct \(10\) statistical tests and cover a wide range of structures, including sinusoidal features, elliptical surfaces, spherical harmonic surfaces, and hyper-hypocycloids. We provide the full details (e.g. the mappings formulae) in Appendix D.2.

Results are reported in Figure 2 as a function of the noise ratio. Notably, \(^{*}\) surpassed all other dependency measures in discerning structure from noise and sometimes by a significant margin (up to \(30\%\) improvement in accuracy). Although \(^{*}\) has shown similar performance to some of the baselines in the linear and parabolic settings, a clear superiority of \(^{*}\) is manifested in the eight complicated geometries, such as the spherical harmonics and sinusoidals. As a result, Figure 2 clearly shows the advantages of our slicing optimality in highly non-trivial settings.

### Sample and Slicing Efficiency of \(^{*}\)

Supporting our theory in Section 3.3, we show that \(^{*}\) scales efficiently to higher dimensions. We test against SI and MI estimators (Kraskov et al., 2004) on two structures of common signal and elliptical. We calculate the AUC of the ROC while varying the number of samples or projections. Figure 3 shows that \(^{*}\) attains the perfect accuracy with less number of slices compared to the uniform-sampling-based baselines. As such, this empirical result supports our claim that \(^{*}\) succeeds in detecting the dependence between the variables with an improved sample and projection efficiency. Appendix B.2 provides additional experiments in higher dimensions.

### Result on Representation Learning

Representation learning employs MI to discover useful representations by training a neural network encoder to maximize MI between its inputs and outputs. For example, Hjelm et al. (2019) introduces Deep InfoMax (DIM), a popular method for learning unsupervised representations, which improves the learning by including knowledge about the local structure of the input into the objective. We follow DIM settings and substitute MI with \(\) and \(^{*}\) as the information measure, respectively. We test these three methods along with BiGAN (Donahue et al., 2016) on the STL-10 (Coates et al., 2011) and CIFAR10 (Krizhevsky et al., 2009) datasets, which consist of high-dimensional images. Results in Tables 1 and 2 show that \(^{*}\) outperforms the baselines for all classifiers: high-level vector

Figure 3: Statistical efficiency of \(^{*}\), SI, and MI with dimension, number of samples (\(n\)), and number of slices (\(m\)). The figures with respect to \(m\) do not include MI because this method does not use slices.

representation (\(Y\)), the output of the previous fully-connected layer (fc) and the last convolutional layer (conv). As such, this experiment highlights the scalability aspect of \(^{*}\) with high-dimensional variables not to mention its excellent performance on a challenging ML task.

### Result on Reinforcement Learning

Previous frameworks have studied MI objectives as regularizers to the reinforcement learning (RL) objective that involves high-dimensional variables of states, actions, and rewards (Nachum et al., 2019; Schwarzer et al., 2021). Since it is practically difficult to estimate MI in high dimensions, most works resort to learning representations of the high-dimensional data by projecting them into low-dimensional embeddings. Thanks to the slicing technique, these representations are no longer needed when using \(^{*}\). We empirically evaluate the scalability of \(^{*}\) with an RL task, where we adapt the forward information objective in Rakelly et al. (2021) as:

\[^{*}_{^{*}}=_{}^{*}(S_{t+1};[A_{t},S_ {t}]), \]

where \(\) denotes the agent's policy, \(A_{t}(S_{t})\), \(S_{t+1}(S_{t},A_{t})\), and \(\) is the transition dynamics. For SI, we use the same objective (by replacing \(^{*}\) with the dependence measure \((.;.)\)). We run tests on three challenging Gym environments (Brockman et al., 2016), where the environment dimensions (observation dim, action dim) are as follows: Humanoid-v3: \((378,17)\); Ant-v3: \((113,8)\); Hopper-v3: \((12,3)\). Further details can be found in Appendix D.3. We report the results comparing \(^{*}\), SI, and the original paper objective (MI). Figure 4 shows clear gains (up to \(25\%\)) with a noticeable learning speed of \(^{*}\) compared to the baselines.

### Behavior Analysis

**Convergence of \(^{*}\).** We validate the convergence rate of \(^{*}\) (Theorem 4). In Figure 5, we show the RMSE between the ground truth \(^{*}\) and the estimated in the setting where \(X\) and \(Y\) are normal random variables with 5 overlapping entries: \(Z(0,I_{15}),(0,0.1I_{10}),X=Z_{},Y=Z _{}+\). The heat map in Figure 4(a) shows convergence when \(n,m\) (\(\#\) samples and \(\#\) slices, respectively) vary independently, while Figure 4(b) shows when one parameter is varying and the other is set to \(10^{3}\). The MI estimator used is the Kozachenko-Leonenko estimator (Kraskov et al., 2004). We also test with \(X,Y^{d}\) with linear dependence to study the effect of the dimension on the convergence. Results are reported in Figure 4(c) with \(n\) and \(m\) varying together.

**Computational complexity.** Results in Figure 6 (left) show that the computational complexity of \(^{*}\) is of an order similar to that of \(\). Note that, while \(^{*}\) is slower than \(\), the difference is

   & conv & fc & Y \\  BiGAN & 71.53 & 67.18 & 58.48 \\ DIM (MI) & 69.15 & 63.81 & 61.92 \\ DIM (SI) & 74.54 & 71.34 & 68.90 \\ DIM (SI\({}^{*}\)) & **76.89** & **71.67** & **70.04** \\   
   & conv & fc & Y \\  BiGAN & 62.57 & 62.74 & 52.54 \\ DIM (MI) & 72.66 & **70.66** & 64.71 \\ DIM (SI) & 74.37 & **70.23** & 65.99 \\ DIM (SI\({}^{*}\)) & **77.01** & **70.39** & **69.04** \\  

Table 1: Classification accuracy (\(\%\)) on STL-10

Figure 4: Results on high-dimensional Reinforcement Learning environments show a cutting-edge performance of \(^{*}\) on complex high-dimensional control problems. The mean and variance computed for 12 seeds are shown in the figures.

insignificant not to mention the clear advantage of \(^{*}\) over other baselines given its superiority in detecting complex dependencies. We also analyze the effect of number of gradient update steps of \(f_{1},f_{2}\) on the computation speed (Figure 6 (right)). Further results can be found in Appendix B.

## 7 Concluding Remarks

We presented a novel dependence measure, called \(^{*}\), that is scalable to high dimensions while being efficient regarding time, sample, and slicing complexity. We discussed its theoretical properties, proved that its estimation error depends on the problem dimensions only up to a constant factor, and empirically validated the competence of \(^{*}\) in detecting complicated dependencies against state-of-the-art dependence measures. We further placed \(^{*}\) into modern ML, where we proved the adequacy of our dependence measure on a more onerous set of tasks.

**Limitations.** Since the aim of this paper was to introduce optimality into slicing methods, \(^{*}\) is formalized to use two slicing variables. Although the current setting has proved effective and rather sufficient in complex machine learning scenarios, the extension into multivariate settings is encouraged for future works.

**Broader impact.** The findings and methodologies developed through this work can significantly improve the way we understand and analyze systems with complex relations. We believe this work will have noticeable impact on broader communities, since the study of quantifying information is active in research areas including physics, statistics, computational biology, economics, and neuroscience. At its current form, this work is mainly theoretical and does not impose any negative societal impact.

Figure 5: Figure shows the RMSE with varying \(n\), \(m\) and variables dimensions.

Figure 6: (Left) Computation speed of measures based on the number of samples and slices (dependency is linear and noise ratio \(=0.6\)). (Right) Measures with respect to the number of update steps (\(m=700\)).