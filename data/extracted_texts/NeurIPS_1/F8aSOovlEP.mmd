# Mecd: Unlocking Multi-Event Causal Discovery in Video Reasoning

Tieyuan Chen\({}^{1}\), Huabin Liu\({}^{1}\), Tianyao He\({}^{1}\), Yihang Chen\({}^{1}\), Chaofan Gan\({}^{1}\),

**Xiao Ma\({}^{2}\), Cheng Zhong\({}^{2}\), Yang Zhang\({}^{2}\), Yingxue Wang\({}^{3}\), Hui Lin\({}^{3}\), Weiyao Lin\({}^{1}\)\({}^{1}\) Shanghai Jiao Tong University, \({}^{2}\) Lenovo Research, AI Lab,

\({}^{3}\) China Academic of Electronics and Information Technology

{tieyuanchen, huabinliu, wylin}@sjtu.edu.cn

[https://github.com/tychen-SJTU/MECD-Benchmark](https://github.com/tychen-SJTU/MECD-Benchmark)

Equal Contribution.Corresponding Author.

###### Abstract

Video causal reasoning aims to achieve a high-level understanding of video content from a causal perspective. However, current video reasoning tasks are limited in scope, primarily executed in a question-answering paradigm and focusing on short videos containing only a single event and simple causal relationships, lacking comprehensive and structured causality analysis for videos with multiple events. To fill this gap, we introduce a new task and dataset, **Multi-Event**Causal **D**iscovery (MECD). It aims to uncover the causal relationships between events distributed chronologically across long videos. Given visual segments and textual descriptions of events, MECD requires identifying the causal associations between these events to derive a comprehensive, structured event-level video causal diagram explaining why and how the final result event occurred. To address MECD, we devise a novel framework inspired by the Granger Causality method, using an efficient mask-based event prediction model to perform an _Event Granger Test_, which estimates causality by comparing the predicted result event when premise events are masked versus unmasked. Furthermore, we integrate causal inference techniques such as front-door adjustment and counterfactual inference to address challenges in MECD like causality confounding and illusory causality. Experiments validate the effectiveness of our framework in providing causal relationships in multi-event videos, outperforming GPT-4o and VideoLLaVA by 5.7% and 4.1%, respectively.

## 1 Introduction

Video causal reasoning aims to achieve a high-level understanding and analysis of video content from a causal perspective. Video Question Answering (VQA)  represents one of the most prominent tasks in causal reasoning, where models are tested on their causal ability to understand video content through causal questions such as explanations, predictions, and counterfactual assumptions. Recently, some studies have sought to move beyond the single QA task, attempting to construct more complex and challenging video reasoning tasks and methodologies. For example, CLEVRER , V-CDN  and CATER  explored more difficult causal reasoning tasks in virtual scenes by constructing object-aware features or using graph neural networks. Neural-symbolic paradigm AAR  and LMLN  extended to derive inference rules by symbolizing data. VAR  and BiGED  extended to daily video causal reasoning by introducing causality during prediction.

However, current video causal reasoning tasks are still limited in scope (primarily QA-based) and mainly focus on short videos containing only a single event or a few events. Most importantly, theycannot provide a comprehensive and structured causal representation for multi-event video reasoning, which is typically required in real-world scenarios. For instance, in traffic surveillance videos, it is necessary to cross-analyze events happening at different times to determine which events, or combinations of events, led to the final traffic accident event.

To address this gap, we set up a new task: **M**ulti-**E**vent **C**ausal **D**iscovery (MECD), which aims to uncover causal relationships among events that distribute chronologically in long videos. As illustrated in Fig. 1, given multiple chronologically arranged event segments in a video along with their corresponding textual descriptions (Fig. 1(a)), MECD requires identifying causal associations between these events to derive a comprehensive and structured event-level causal diagram (Fig. 1(b)), indicating why and how the final result event happens. Meanwhile, we contribute a new dataset for the training and evaluation of MECD by collecting long-form videos involving multiple events and manually annotating real causal relations between events for them. However, to our knowledge, no available solutions can directly comprehend causal relationships at the event level, necessitating the development of a new framework to tackle this complex task.

To this end, we draw inspiration from the _Granger Causality Method_ for solution, which is widely used in traditional causal discovery for low-dimensional time-series data (e.g., stock prices, weather patterns). The main idea is that temporal causality can be effectively estimated by predictive ability. Specifically, applied to videos, if Event \(A\) occurs prior to Event \(B\), we consider \(A\) to be a cause of \(B\) only if \(A\) could facilitate the prediction of \(B\). We term this criterion the _Event Causality Test_. However, compared to simple low-dimensional data, the inputs of MECD involve much more complex modalities, including both visual and textual content, which may introduce bias in the estimation of causality using such a predictive paradigm. Specifically, we observe that directly applying _Event Causality Test_ to video causal discovery presents two main problems:

(1) **Causality confounding** indicates that the original causal relationships between events are disrupted or interfered with by other relay or adjacent events. Such confounding stems from the fact that many causal relationships flow through an intermediary event that acts as a bridge. As shown in Fig. 1(c), event "submitting the paper" serves as a necessary bridge between "taking the test" and "obtaining a grade." In this case, this bridge event might be mistakenly regarded as the only cause of the result event, while another cause, "taking the test," is overlooked. However, the bridge event can only occur if "taking the test" happens first. Therefore, we cannot identify the real causality between events that linked by such bridges following a simple predictive criterion, and eliminating such confounding is thus crucial for an accurate causal discovery.

(2) **Illusory Causality**, which includes illusory temporal and existence causality. Illusory temporal causality exists when events exhibit a close correlation in temporal distribution. Such correlation may mislead the test of real causality. As shown in Fig. 1(d), the event "adding oil

Figure 1: (a): Illustration of Multi-Event Causal Discovery Task, where the 3rd and 5th premise events account for the occurrence of the final event. The objective of our task is to determine whether a causal relation exists between events and outputs a structured causal diagram. (c): Example of causality confounding. (d)&(e): Illustration of illusory causality.

when cooking" often occurs before "adding vegetables to stir-fry," but there is no real causality between them. As for _illusory existence causality_, it occurs when some objects in early events may serve as necessary existence conditions of a later event. For instance (Fig. 1(e)), consider determining the causal relation between "a large brown dog enters the room" (at the start of the video) and "the dog runs towards the camera." (at the end of the video). Although the presence of the dog in the former event is a prerequisite for the subsequent event, it does not directly cause the dog to rush towards the camera.

Building upon the preceding discussion, we introduce a novel framework to tackle MECD. This framework executes the _Event Granger Test_ via an efficient mask-based event prediction model. It deduces the causality of a premise event by comparing the predicted features of the result event when the premise is either masked or unmasked. Furthermore, to mitigate the challenges of causality confounding and illusory causality discussed earlier, we integrate two additional causal inference techniques--front-door adjustment [15; 16; 17] and counterfactual inference [15; 18; 19]--into our framework. Specifically, these techniques compensate for or remove the causal effects of previous or subsequent adjacent bridge events to eliminate confounding. Simultaneously, they address the issue of illusory causality through the incorporation of an extra chain of thought [20; 21; 22] and existence-only descriptions during inference. Extensive experiments validate the effectiveness of our proposed framework in predicting structured causal relationships for given long-form videos.

## 2 Related Work

**Video causal reasoning** Many tasks in the past have tried to carry out causal reasoning in videos. Among these, the most common task is Video Question Answering (VQA) [1; 2; 3; 4], aiming to give a reasonable answer to the question, methods such as SeViLA and LocAns [3; 4] made abductions based on the result, they grounded a single reason in previous time. However, VQA does not extend to abduct multiple reasons, merely creating a single causal link from reason to result.

Many tasks were based on VQA task for further causal reasoning attempts. CLEVRER , CATER  and V-CDN  explored causal reasoning based on physics and other basic laws in virtual scenes. However, these tasks haven't been committed to extending to the general video causal reasoning. AAR  and LMLN  symbolized data and derived inference rules using external knowledge. However, they can only reason within a defined symbol domain. The most similar VAR  predicted explanation events with premise events, and the causality was introduced during its prediction process. However, firstly it hasn't been committed to discovering the complete causal diagram. Besides, there is no explicit utilization of causal methods which constrains its ability.

All tasks above are for causal reasoning in short videos, while ours aims to handle long-duration videos. Besides, most of these are coarse video-level tasks, ours is more fine-grained event-level reasoning. Additionally, we want to establish a whole causal diagram rather than a single causal link. In conclusion, all these tasks haven't been committed to discovering causality among complex multi-event videos. Consequently, there exists a necessity need for a more comprehensive task.

**Causal discovery in low-dimensional temporal data** Traditional causal discovery methods of simple temporal data are mainly divided into three categories. Constraint-based methods use conditional independence tests to identify causal relations [23; 24; 25]. Score-based methods search through the space of all possible causal structures to optimize a specified metric [26; 27; 28]. The Granger Causality method discovers causal relations by calculating the degree to which the earlier occurred event contributes to the prediction of the latter occurred event [29; 30; 31]. The constraint-based and score-based methods require stringent assumptions about data distribution, making them less suitable for video data. The Granger Causality methods are more suitable yet face challenges when applied directly to video data, our method reaches better performance by utilizing causal inference methods.

## 3 Benchmark

### MECD task settings

Our Multi-Event Causal Discovery (MECD) task is designed to test the ability of causal discovery in multi-event videos. Given a video \(\) that contains chronologically organized \(N\) events, \(:=\{e_{1},,e_{N}\}\), the task aims at determining whether any previous event \(e_{n}\) (\(n<N\)) has a causal relation with the last one (_i.e._, \(e_{N}\)). Specifically, an event \(e_{n}=\{v_{n},c_{n}\}\) consists of a video clip \(v_{n}\) and the corresponding caption \(c_{n}\). Without loss of generality, relations of previous events to the last one can be expressed as \(=[r_{1},,r_{N-1}]\), where \(r_{k}\) (\(k<N\)) is set to "1" to indicate the existence of \(e_{k}\)'s causal relation with \(e_{N}\), and "0" otherwise. Notably, this setting is generalizable to causal relations of any of two events as long as we cut off the video and treat the latter one as the last event.

### MECD task dataset

**Data Source** The Multi Events Causal Discovery (MECD) task contains videos with multiple events and intricate causal relationships. The ActivityNet Captions dataset  is built on ActivityNet v1.3 which includes 20k 120-second YouTube untrimmed videos. We carefully reorganize the ActivityNet Captions dataset and select 1,105 lifestyle videos that span diverse scenarios. We call this new dataset as MECD dataset, where 806 and 299 videos are randomly split for training and testing, respectively. Specifically, each video in the MECD dataset contains 4 to 11 events, with a minimum of 2 premise events exhibiting causal relations with the last one. Fig. 2 (a1) presents the main categories and word clouds of video types. Please refer to Appendix Sec. B.4 for more dataset examples.

**Data Cleaning** We further clean our dataset by excluding non-causal videos. For example, videos that describe multiple non-causal action steps such as washing hands and shaving were excluded.

**Dataset Annotation** The annotations of MECD dataset include 4 attributes. The "duration", "sentence", and "timestamps" attributes in annotations remain the same as the ActivityNet Captions dataset. Specifically, in the context of our task, a new attribute "relation" is introduced. To obtain this attribute, relations among events are firstly annotated by GPT-4 API , and subsequently refined by five human annotators. Through a cross-annotation process, gt labels are determined by the majority of the annotators' causal relation choices, thus mitigating potential inaccuracies and subjective biases to a certain extent. We also present the impact of positions of events on their causality and number of events in videos in Fig. 2 (a2), annotation pipeline is in Appendix Sec. B.3.

## 4 Methodology

In this section, we present our Video Granger Causality Model (VGCM), as shown in Fig. 3. This model establishes the global connections across all events, and deduces the causality of a premise event by comparing the output features when it is masked or not, under the concept of the _Event Causality Test_. However, masking out an event may lead to the problem of confounding and illusion. In this context, we further utilize causal inference methods to address these by compensating or removing the effect of previous or subsequent causal events to mitigate the confounding meanwhile during inference the extra chain of thoughts and existence-only descriptions relieve the illusion.

### VGCM: Video Granger Causality Model

Building upon the Granger Causality method introduced in [34; 35; 36], our core motivation for constructing VGCM is _Event Causality Test_: To compare the prediction result of the last event using all the premise events with or without a certain event in it. If the results exhibit obvious divergence, it indicates that the current premise event is causally related to the result event.

Figure 2: **Constitute of MECD dataset. In (a1), we present 5 main video categories of the dataset. The word cloud is also summarized for video types. In (a2), the left chart indicates the impact of positions of events on their causality where we find the second last event tends to be more significant; while the right chart plots the number of events in videos.**

We design VGCM to take in both the video clips and the captions to maximize information utilization. As illustrated in Fig. 3, our proposed VGCM is a multi-modal transformer-based structure with a video encoder and caption encoder, and a multi-modal decoder with causal relation head to discover causal relations through the predicting process and the comparison of predicting results.

Based on this, we denote \(^{p}\) as the set of all the _premise_ events \(^{p}:= e_{N}\), and \(^{m}_{k}:=^{p} e_{k}\) as the event set where the premise event \(e_{k}\) (\(k<N\)) is masked. Notably, we mask the event \(e_{k}\) by setting all zeros to its video clip \(v_{k}\) and assign constant characters to the caption \(c_{k}\).

Following , we firstly pretrain a video encoder \(_{pre}\) under an action recognition task to extract the features of the video clips. We essentially create two paths, one for the unmasked event set \(^{p}\) (orange path in Fig. 3) while the other for the set with one event (_i.e._, \(e_{k}\)) masked \(^{m}_{k}\) (green path in Fig. 3). The video clips and captions are first separately encoded using Enc\({}_{V}\) and Enc\({}_{C}\) to obtain compact features, then their features are sent to a multi-modal decoder Dec that shares weights for both paths to fuse the information. Afterward, several model heads are employed for feature comparison and loss measurement. \(^{p}\) and \(^{p}\) are the video clip and caption matrix concatenated from all premise events set \(^{p}\), similarly, \(^{m}_{k}\) and \(^{m}_{k}\) are from \(^{m}_{k}\).

\[^{p}&=_{V}(_{pre}( ^{p})),\;^{p}=[((^{p},_{C}( ^{p}))]_{N-1}\\ ^{m}_{k}&=_{V}(_{pre}(^{m}_{ k})),\;^{m}_{k}=[((^{m}_{k},_{C}( ^{m}_{k}))]_{N-1} \]

where Enc\({}_{V}\) and Enc\({}_{T}\) represent the encoder module of video clips and captions, respectively. Dec is a multi-modal decoder that shares weights for both paths. Cat indicates the concatenate operation, and \([-]_{N-1}\) indicates the \((N-1)\)-th slice at dimension 0. \(^{p}\) and \(^{m}_{k}\) are features after encoding, and \(^{p}\) and \(^{m}_{k}\) are the output features, which are then used for comparison of difference. Incorporating both visual and linguistic representations, the decoder conducts cross-modal reasoning and leverages the context from the unmasked premise events to posit a meaningful representation of the most likely explanatory result event.

Subsequently, the feature \(^{p}\) deduced from the unmasked events is sent to the caption head for prediction. Additionally, in order to compare the difference of the prediction result, \(^{p},^{m}_{k}\) are directed to the relation head for causal relation discovery. The result event \(e_{N}\) is encoded the same way as \(e_{k}\) to get feature \(_{N}=_{V}(_{pre}(v_{N}))\) and the output \(_{N}=((_{N},_{C}(_{N}))\), \(_{N}\) is aggregated for reasoning (red path in Fig. 3). The relation head consists of a semantic query module and a self-enhancement module, where outputs are concatenated and then passed through the cross-reasoning layer \(g_{r}\) for further interaction. Last but not least, the auxiliary similarity is measured between \(^{p}\) and \(^{m}_{k}\) as a supplement to the output information of the relation head. After

Figure 3: **Video Granger Causality Model.** Two data streams \(^{p}\) and \(^{m}_{k}\) serve as input, video and text embeddings are concatenated after being separately embedded. The VGCM incorporates two classifiers, the caption head takes the unmasked stream to accomplish the event-predicting task, while the relation head discovers the causal relations with two embedding streams.

the reasoning process, the prediction output of the causal relation \(_{k}\) can be represented by:

\[_{k}=g_{r}((_{att}^{C}((_{wt}^{m },_{N})),(^{p},_{N})), _{att}^{I}((_{k}^{m},_{N}))))) \]

where \(_{att}^{C}\) represents cross-attention, \(_{att}^{I}\) represents self-attention, \(g_{r}\) represents linear layer. The training objective consists of two main directions as previously discussed:

To reconstruct the textual and visual representation of the result event \(e_{N}\), we introduce caption loss and reconstruction loss, respectively. Caption loss \(_{C}\) ensures an accurate prediction of the result caption \(_{N}\) given all the premise events \(^{p}\). Simultaneously, visual reconstruction loss \(_{V}\) forces the encoder to "imagine" a representation of the result video clip \(_{N}\) that better aligns with the original representation \(v_{N}\). These losses allow the model to predict visual and textual representations that are close to the original representations, which better supports our method of inferring causal relations by comparing the results of the two-stream predictions.

For the objective of causal discovery, we introduce causal relation loss and an auxiliary semantics similarity loss. Causal relation loss \(_{R}\) supervised the output relations \(_{k}\). Meanwhile, the semantics similarity loss \(_{S}\) is introduced to guarantee the semantics similarity of result event prediction under the presence or absence of a causal-relation-free premise event. The complete loss function is:

\[=_{C}(c_{N},_{N})+_{R}_{R}(r_{k },_{k})+_{V}_{V}(_{N}^{p},_{N})+_{S}(r_{k})_{S}(_{k}^{m}, _{p}) \]

where \(_{R}\), \(_{V}\), and \(_{S}\) are weights for trade off. \(_{C}\) and \(_{R}\) are the cross-entropy losses, \(_{V}\) and \(_{S}\) are the mse losses, \(_{N}^{p}\) is the N-th slice of \(^{p}\), which represents the encoded feature of \(e_{N}\).

### Causal Inference in Vgcm

In Sec. 4.1, we employ the concept of Granger Causality to design our VGCM model under the principle of _Event Causality Test_ which may, however, introduce causality confounding and illusory. Below we introduce these issues in detail, as well as how we manage to solve the problems.

**Causality confounding** is a phenomenon where the original causal relations across events are impacted due to modification (_i.e._, masking) of some intermediate events (_i.e._, \(e_{k}\)). Existing disentangled representation learning works [40; 41] disentangled different attributes of a variable by supervising high-order distribution under strict assumptions but failed in disentangling different variables.

Specifically, when \(e_{k}\) is masked for the comparison in VGCM, the causal relations between \(e_{k}\)'s adjacent events and the last event are impacted, leading to a confounding of causal effects. Notably, for brevity, we only employ \(e_{k}\)'s previous one event \(e_{k-1}\) and its subsequent one event \(e_{k+1}\) for analysis, but the same analysis also applies to all the previous or subsequent events. To be specific, there exist two distinct kinds of confounding when \(e_{k}\) is absent: **1)** Causal effects of \(e_{k-1}\) to \(e_{N}\) may be lost, as its connection to \(e_{N}\) is built upon \(e_{k}\), (green path in Fig. 4 (a1)). **2)** Causal effects of \(e_{k+1}\) to \(e_{N}\) may be redundant, as \(e_{k}\) must be a necessary prior of its causality, (red path in Fig. 4 (a1)).

**Illusory causality** is another issue that may lead to some spatial or temporal misunderstandings, including illusory temporal and existence causality. **1)** Illusory temporal causality is the situation that events could have tight temporal ordering, but they in fact have no causal relations. **2)** Additionally, illusory existence causality occurs when an object introduced in the premise event is a necessary condition for the result, but the premise event does not semantically serve as a reason. Notably, we find that illusory in multi-event videos is much more significant than two independent events, which also tends to be exacerbated by causality confounding.

Overall, **causality confounding** and **illusory causality** both bring difficulties for relation modeling of events in videos. Notably, these two issues are coupled in that **causality confounding** tends to exacerbate **illusory causality** by misallocating attention to temporal ordering and causal effect. Therefore, illusory causality can be partially relieved by solving the problem of causality confounding.

When considering taking the illusory causality, the chain of thoughts [20; 21; 22] has been shown in LLMs to lead the model to logical thinking which is similar to human thought process, the chain of thoughts \(T_{cot[e_{k-1}:e_{N}]}\) provides a step-by-step process of reasoning the \(e_{N}\) from \(e_{k-1}\). Specifically, \(T_{cot[e_{k-1}:e_{N}]}\) is obtained using GPT-4 API  by feeding it with \(e_{k-1}\), \(e_{N}\) along with a prompt asking it to provide the probable reasoning chain. We consider utilizing it in causal inference to eliminate the attention bias on temporal correlations introduced by non-causal temporal knowledge.

Besides, as the illusory existence causality is caused by the objects' correlation between the events, we address this influence by keeping objects in the green path in Fig. 3 the same as those in the orange path. We introduce an alternative event \(e_{k}^{0}=\{v_{k}^{0},c_{k}^{0}\}\) of \(e_{k}\) to briefly recaps the objects in \(e_{k}\). Specifically, \(c_{k}^{0}\) is obtained using GPT-4 API  by feeding it with \(c_{k}\) along with a prompt asking it to extract the objects from \(c_{k}\) and organize them as the sentence such as "There are objects A, B and C.". Consequently, we opt to employ \(c_{k}^{0}\) to approximate \(e_{k}^{0}\) in our VGCM model while omitting \(v_{k}^{0}\), as \(c_{k}^{0}\) is sufficient already to convey the information of objects. By providing \(e_{k}^{0}\), all the necessary objects are still available in this path, thus effectively mitigating the illusory existence causality, facilitating the model to focus more on essential and causality-related semantic information.

To tackle the issues above, we introduce two causal inference methods: the front-door adjustment  for the missing causal effect of \(e_{k-1}\) and counterfactual inference  for the redundant causal effect of \(e_{k+1}\). Meanwhile, the chain of thoughts \(T_{cot[e_{k-1}:e_{N}]}\) and the descriptions of existence \(c_{k}^{0}\) are also provided to carefully address illusory causality, which in turn mitigates confounding.

We establish a causality diagram in Fig. 4 (a2) for an improved elaboration. On masking \(e_{k}\), the causality confounding that requires compensation \(^{C}\) or removement \(^{R}\) can be expressed as:

\[^{C}=P(e_{N}|e_{k})-P(e_{N}|do(e_{k})),^{R}=P(e_{N}|e_{k+1})-P(e_{N }|do(e_{k+1})) \]

where \(P(e_{N}|e_{k})\) and \(P(e_{N}|e_{k+1})\)represents the process by which we predict \(e_{N}\) from \(e_{k}\) and \(e_{k+1}\) in the orange path in Fig. 3, and \(do()\) represents do-operation in causal inference  that cuts off the causal relation between the event and its causes.

We aggregate the subsequent events \(e_{k+1}\), the current event \(e_{k}\) and the chain of thoughts \(T_{cot[e_{k-1}:e_{N}]}\) using a linear layer \(g_{do}\) for aggregation and the cross-attention and self-attention, according to the study in , \(P(e_{N}|do(e_{k}))\) can be implemented as:

\[P(e_{N}|do(e_{k}))=g_{do}(((_{att}^{C}(e_{k},e_{k+1},e_{k+1}), _{att}^{I}(e_{k},e_{k},e_{k}),_{c}(T_{cot[e_{k-1}:e_{N}]}))), \]

Here, we re-use the cross-attention \(_{att}^{C}\) and the self-attention \(_{att}^{I}\) modules as in (2) to cut off the causal effect from \(e_{k-1}\) to \(e_{k}\) through do-operation, \(e_{k}\) only interacts with subsequent events in predicting \(e_{N}\). Then the missing causal effect \(^{C}\) can be compensated since the causal-view operation and illusory temporal causality can be suppressed at the same time with the introduction of the chain of thoughts. Similarly, the redundant causal effect \(^{R}\) can be removed by applying counterfactual intervention, then \(P(e_{N}|do(e_{k+1}))\) can be represented by:

\[P(e_{N}|do(e_{k+1}))=P(e_{N}|e_{k+1})[P(e_{k+1}|e_{k})-P(e_{k+1}|e_{k}^{0})], \]

\(P(e_{N}|do(e_{k+1}))\) effectively cuts off the redundant causal effect between \(e_{k+1}\) and \(e_{N}\) for the reason that the causes of \(e_{k+1}\) are replaced with counterfactual description \(e_{k}^{0}\), then the illusory existence causality can be suppressed at the same time.

To refine the originally decoded feature \(_{k}^{m}\) from the path with premise events masking:

\[_{k}^{ m}=_{k}^{m}-(^{C})+( ^{R}) \]

where \(_{k}^{ m}\) is the refined feature that replaces \(_{k}^{m}\) for further deduction of the model. With the refinement feature \(_{k}^{ m}\), our VGCM model effectively compensates the connections between \(e_{k-1}\) and \(e_{N}\) that were originally lost due to the removal of \(e_{k}\), and effectively removes the redundant causal effect between \(e_{k+1}\) and \(e_{N}\) as well.

Figure 4: **Causal Effect of the Adjacent Events and Causality Diagram.** (a1) shows the causality of the third event analyzed, the red causal effect needs to be compensated while the green needs to be mitigated. (a2) shows the causal inference methods corresponding to the two causal effects.

## 5 Experiments

### Main results

**Implementation details.** including the pretraining process, detailed architecture of VGCM, and hyper-parameters settings can be found in Appendix Sec. A due to space constraints.

**Baselines.** We mainly compared our model with basic multi-modal models such as baseline model Videobert  and widely used CLIP-L  and the most similar reasoning model VAR . Besides, we also conduct experiments on powerful LLM, including GPT-4  and Gemini-Pro . VLLM utilized for comparison includes widely accepted GPT4-o , VideoLLaVA , MiniGPT-4 , Video-Ilama , VideoChat2  and MiniGPT4-video . Specifically, LLMs and VLLMs are conducted under the few shot setting (In-Context Learning) following the causal discovery tasks in NLP , additionally, we reported the performance of fine-tuned VideoLLaVA and VideoChat2.

**Metrics.** We utilize the top-1 accuracy of the output causal relation chains with respect to the final event to evaluate the model's capability in causal discovery. Although our VGCM is designed to discover the causal relations leading to the final event, when truncating the video during inference and redefining the final event as the new result, VGCM can generate a comprehensive causal diagram for the entire video without introducing additional training. Consequently, in addition to the primary metric accuracy, we introduce Structural Hamming Distance (SHD)  as a supplementary metric. SHD measures the degree of matching between comprehensive causal graphs by summing the number of incorrect causal relations. In the MECD test set, the average number of causal relations in video causal graphs is 12.31, and a lower Ave SHD value of the test set indicates better performance.

**Results.** We report the quantitative results in Tab. 1. Our VGCM without causal inference reaches an accuracy of 66.9%, demonstrating basic reasoning capabilities. Furtherly, the complete VGCM reaches a better performance with an accuracy of 71.2%, outperforming the GPT-4, GPT4-o, fine-tuned VideoLLaVA  by 11.6%, 5.7%, and 4.1%. Additionally, we explored the effect of altering the input format of the two modalities in Appendix Sec. C.1, indicating that VGCM is not dependent on the input format. The results compared with the two metrics indicate that for most models, accuracy is already adequate to represent their causal discovery capabilities. However, the additional metric Ave SHD indicates that Gemini and GPT-4 exhibit a superior overall capacity for discovering complete relations. An example of the output complete causal diagram is visualized in Figure 7.

GPT-4  stands out as one of the most advanced LLM models, however, we found that even being provided with sufficient few-shot examples (detailed in Appendix Sec. C.2), its accuracy remains at

    & Paradigm & Method & Ave SHD \(\) & Accuracy \(\) \\   &  & Guess all causal. & 6.95 & 42.4 \\  & & Guess all non-causal. & 5.36 & 57.6 \\   &  & Gemini-1.5-Pro  & 4.91 & 59.3 \\  & & GPT-4.0613  & 4.92 & 59.6 \\   & & MiniGPT4-video  & 5.16 & 56.8 \\  & & MiniGPT4-4  & 5.14 & 57.5 \\  & & Video-Ilama  & 5.10 & 60.6 \\  & & VideoChat2  & 4.89 & 60.7 \\  & & VideoLLaVA  & 4.85 & 62.5 \\  & & GPT-4o  & **4.69** & **65.5** \\   &  & VAR  & 4.96 & 57.3 \\  & & Videobert  & 4.95 & 60.9 \\  & & CLIP (ViT-L/14)  & 4.77 & 62.9 \\    &  & VideoChat2  & 4.77 & 66.9 \\  & & VideoLLaVA  & **4.73** & **67.1** \\    & &  & VGCM\({}^{}\) & 4.51 & 67.0 \\  & & **VGCM** & **4.19** & **71.2** \\  - & Humans & Deductive Reasoning & 2.05 & 87.2 \\   

Table 1: **Main results.** Experiments validate the effectiveness of our VGCM framework in reasoning causal relations towards multi-event videos, outperforming GPT-4o and VideoLoLAVA by 5.7% and 4.1%, respectively. \({}^{}\) indicates without causal inference. Random results and human performances are also provided.

    &  &  & Acc \\ \(_{}\) & \(_{}\) & \(_{}\) & Adj & Inter & Acc \\   &  & ✓ & ✓ & & & 64.8 \\  & & ✓ & & & 65.1 \\  & & ✓ & & & 65.3 \\  & ✓ & ✓ & & & 67.0 \\  & ✓ & ✓ & ✓ & & 68.7 \\  & ✓ & ✓ & ✓ & ✓ & 69.3 \\  & ✓ & ✓ & ✓ & ✓ & 71.2 \\   &  & VideoChat2  & 4.77 & 66.9 \\  & & VideoLLaVA  & **4.73** & **67.1** \\   & &  & VGCM\({}^{}\) & 4.51 & 67.0 \\  & & **VGCM** & **4.19** & **71.2** \\  - & Humans & Deductive Reasoning & 2.05 & 87.2 \\   

Table 2: **Ablation Study.** Adj indicates the front-door adjustment, and inter indicates the counterfactual intervention.

    &  &  & Acc \\ \(_{}\) & \(_{}\) & \(_{}\) & Adj & Inter & Acc \\   &  & Guess all causal. & 6.95 & 42.4 \\  & & Guess all non-causal. & 5.36 & 57.6 \\   &  & Gemini-1.5-Pro  & 4.91 & 59.3 \\  & & GPT-4.0613  & 4.92 & 59.6 \\   & & MiniGPT4-video  & 5.16 & 56.8 \\   & & MiniGPT4-4  & 5.14 & 57.5 \\  & & Video-Ilama  & 5.10 & 60.6 \\  & & VideoChat2  & 4.89 & 60.7 \\  & & VideoLLaVA  & 4.85 & 62.5 \\  & & GPT-4o  & **4.69** & **65.5** \\   &  & VAR  & 4.96 & 57.3 \\  & & Videobert  & 4.95 & 60.9 \\  & & CLIP (ViT-L/14)  & 4.77 & 62.9 \\    & & VideoChat2  & 4.77 & 66.9 \\    & & VideoLLaVA  & **4.73** & **67.1** \\    & &  & VGCM\({}^{}\) & 4.51 & 67.0 \\  & & **VGCM** & **4.19** & **71.2** \\  - & Humans & Deductive Reasoning & 2.05 & 87.2 \\   

Table 3: Illusory existence causality experiment. w/o C indicates without counterfactual intervention.

only 59.6%. Possible explanations may be due to task contamination , GPT-4 mainly performs well on datasets released before the training date, while our task is novel. Moreover, other reasons may include the causal hallucination problem of establishing a threshold for differentiating between scenarios with and without causality . For further insights into GPT-4's failure cases, refer to Appendix Sec. B.2.

As illustrated in Tab. 6, we have assessed the inference speed of various models, with our VGCM achieving a swift 0.76 seconds per sample. The proposed method incurs an overhead of only 8.57% over the Videobert baseline. It is noteworthy that our inference speed is 3 to 6 times faster than that of all Video LLMs. The inference speed experiments were conducted on 1 NVIDIA A6000 GPU.

### Ablation Study

**Video Granger Causality Model design** We designed our causal discovery model based on the Granger Causality, three auxiliary losses are applied. The performances in Tab. 2 indicate that our VGCM benefits from the design of \(_{V}\) and \(_{C}\), for they support our method of inferring causal relations by facilitating the model with event prediction ability. \(_{S}\) also benefits our model by supervising the causal feature similarity of \(e_{N}\) with and without non-causal event \(e_{k}\) masked.

**Front-door adjustment with chain of thoughts candidate** The method does improve reasoning ability in Tab. 2. We conduct an experiment in Tab. 4 for further proof. Since events closer to the result event are higher as the cause, the model likely learns these biased time-domain tendencies. So we compare the accuracy of VGCM without front-door adjustment with chain of thought candidate and VGCM in determining the first relation \(r_{1}\) and the last relation \(r_{N-1}\). The results demonstrate that temporal illusory causality is greatly mitigated, visualization can be found in Fig. 8 Example 1.

**Counterfactual intervention with existence-only descriptions** The performance in Tab. 2 shows that counterfactual intervention with existence-only descriptions does facilitate the model with powerful reasoning ability. We dive into further analysis on the basis that when a non-causal event is masked, the causal feature \(_{k}^{m}\) fed into the causal relation head should be similar to the unmasked feature \(^{p}\), instead, a bigger gap appears when masking a causal event. For stronger proof, we measure the difference in feature similarity in Tab. 3 and Fig. 6. We define the similarities division as the quotient of the similarity(\(_{k}^{m}\), \(^{p}\)) with a non-causal \(e_{k}\) masked over with a causal \(e_{k}\) masked. In the experiment, we find that the similarity division is always above 1 without the counterfactual intervention, however, the existence illusory is solved with counterfactual intervention for the reason that the division is below 1 of VGCM, example visualization can be found in Fig. 8 Example 2.

### Robustness Analysis

**Model Robustness** To prove our model's robust reasoning ability, we split the MECD dataset into five categories, and conduct an experiment similar to the open-set setting with cross-validation. VGCM

Figure 5: **Dataset robustness. Accuracy decreases slightly when increasing noise, and increases slowly when increasing the training data.**

Figure 6: **Causality discovered analysis. The similarity of masking causal premise events is obviously lower through counterfactual intervention.**

   Method & TOP-1 Accuracy \\  VAR  & 54.8 \\ VGCM\({}^{}\) & 59.2 \\
**VGCM** & **64.4** \\   

Table 5: Open-set ability of VGCM.

Figure 7: Complete causal diagram.

reaches an average accuracy of 64.4%, outperforms VGCM without causal inference and VAR by 5.2% and 9.6%, details can be found in Appendix Sec. D.1.

Moreover, to further validate the generalization capabilities of our model, we evaluate the quality of output causal relations on a related and representative video reasoning task: Video Question Answering (VQA) as shown in Tab. 7. Specifically, during inference on the multi-event subset of ActivityNet-QA  (The part that overlaps with the MECD test set), we prompted MiniGPT4-video  with additional causal relations outputs alongside the standard question inputs. This paradigm facilitates the VLLMs in considering the task from a causal perspective. As shown in the table below, when prompted with these additional causal relations, the answering accuracy of MiniGPT4-video  improved by our VGCM surpasses other strong VLLMs like VideoChat2 . These findings confirm that our model can provide accurate causal perception for videos, significantly improving performance on related video reasoning tasks.

**Dataset Robustness** We study the subjectivity and data volume of our proposed MECD dataset, which is shown in Tab. 5. In the experiments of increasing the ratio of randomly flipped annotated causal relations (flipping only one relation of the whole causal relations of video), the accuracy decreases slightly, demonstrating the small amount of subjectivity in labeling does not have a serious impact. Besides, we analyze the scale of data, the increment from 600 examples to 806 examples yields a very modest improvement, indicating the adequacy of our dataset.

## 6 Conclusion

We proposed a novel task, multi-event video causal discovery (MECD), which focuses on event-level causal discovery in long-term videos. Besides, we built the MECD dataset with long-term daily life video datasets with causal relations to support this task and proposed the first video events causal discovery framework VGCM in the principles of Granger Causality. Additionally, our proposed VGCM was facilitated with deeper reasoning ability through causal inference with the chain of thoughts and existence-only descriptions. Our VGCM significantly outperforms GPT-4o and VideoLLaVA by 5.7% and 4.1%, respectively, demonstrating its robust reasoning ability.

   Model & Inference Speed & Output Causal Relations & VQA Acc & VQA Score \\  Videobert  & 0.70 & w/o (Standard QA setting for VLLMs) & 43.17 & 2.82 \\
**Our VGCM** & **0.76** & w Gemini-Pro  & 49.10 & 2.90 \\ VideoLLaVA  & 2.12 & w GPT-4  & 49.36 & 2.89 \\ VideoCha2  & 2.96 & w VideoDat2  & 51.01 & **2.95** \\ MiniGPT4-video  & 3.98 & w VideoLLaVA  & **51.88** & 2.93 \\ MiniGPT4- & 4.72 & **62.21** & **3.12** \\   

Table 6: **Infernce speed. Our VGCM is 3-6 times faster than all Video LLMs while slightly slower than the baseline.**

Figure 8: **Successful abduction examples of our VGCM. Results indicate that after utilizing causal inference methods, illusory causality is suppressed and robust abduction ability is facilitated.**

Acknowledgement

The paper is supported in part by the National Natural Science Foundation of China (No. 62325109, U21B2013) and the Lenovo Academic Collaboration Project.