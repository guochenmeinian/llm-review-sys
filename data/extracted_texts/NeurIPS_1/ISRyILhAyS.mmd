# Coordinating Distributed Example

Orders for Provably Accelerated Training

A. Feder Cooper

Wentao Gu

Khiem Pham

Tiancheng Yuan

Charlie F. Ruan

Yucheng Lu

Christopher De Sa

Cornell University

{afc78, wg247, dkp45, ty373, cfr54, yl2967, cmd353}@cornell.edu

Equal contribution

###### Abstract

Recent research on online Gradient Balancing (GraB) has revealed that there exist permutation-based example orderings for SGD that are guaranteed to outperform random reshuffling (RR). Whereas RR arbitrarily permutes training examples, GraB leverages state gradients from prior epochs to order examples -- achieving a provably faster convergence rate than RR. However, GraB is limited by design: while it demonstrates an impressive ability to scale-up training on _centralized_ data, it does not naturally extend to modern _distributed_ ML workloads. We therefore propose _Coordinated Distributed GraB_ (CD-GraB), which uses insights from prior work on kernel thinning to translate the benefits of provably faster permutation-based example ordering to distributed settings. With negligible overhead, CD-GraB exhibits a linear speedup in convergence rate over centralized GraB and outperforms distributed RR on a variety of benchmark tasks.

## 1 Introduction

Random reshuffling, which samples training-data examples without replacement, has become the _de facto_ example-ordering method in modern deep-learning libraries , given that it tends to accelerate optimizer convergence in practice. However, some recent theoretical work has identified cases in which random reshuffling can lead to data orderings that have a poor effect on convergence . This has encouraged a line of research to investigate if there exist provably better permutation-based orderings that afford greater scalability in training . Notably, Lu et al.  connects permuted-order SGD to the _herding problem_, and proposes the herding-based online Gradient Balancing algorithm (GraB), which converges provably faster than random reshuffling, and does so with little memory or computational overhead. In fact, in follow-on work, Cha et al.  proves that GraB is optimal: in theory, GraB is the fastest possible permutation-based example ordering algorithm.

These results are very exciting, suggesting that GraB should unseat random reshuffling as the example ordering method-of-choice for SGD; however, they only hold with respect to a _single_ machine. GraB is optimal in settings with _centralized_ data, but does not naturally translate to problems of modern-ML scale, which demand that training workloads be distributed across _multiple parallel_ workers that each only have access to a subset of the training data. This drawback raises an important question:

_Can we simultaneously achieve the scalability benefits of distributed training and provably faster permutation-based example ordering for SGD -- both in theory and in practice?_

In this work, we show that it is indeed possible to attain these twin objectives. To do so, we suggest the online **C**oordinated **D**istributed **G**radiant **B**alance algorithm (CD-GraB), which leverages insightsfrom kernel thinning to elevate the herding framework of centralized GraB (GraB) to the parallel setting. Felicitously, as a side effect, this choice of formulation brings about positive practical performance benefits (that can also improve the empirical behavior of centralized GraB). Using the exact same assumptions as the original GraB paper, **we show analytically that coordinating example orders across parallel workers leads a linear speedup in convergence rate**. For \(T\) epochs and \(m\) parallel workers, each with access to \(n\) examples, CD-GraB's convergence rate is \(((mnT)^{-2/3})\) on smooth, non-convex objectives and \(((mnT)^{-2})\) under the Polyak-Lojasiewicz (P.L.) condition2

We run a series of experiments to verify these improvements in practice, implementing CD-GraB on a single node that distributes computation across multiple GPUs. We also run an ablation study in order to disentangle the benefits of parallelism from the positive side effects of using kernel thinning to formulate the CD-GraB algorithm. Similar to how centralized GraB demonstrates improved generalization over centralized random reshuffling (RR), we observe that CD-GraB exhibits improved generalization over distributed random reshuffling (D-RR). Altogether, the success of our work suggests a new distributed training paradigm to explore in future work, which we call the _Order Server_ (Section6). In summary, we:

* Propose the online **C**oordinated **D**istributed **G**radient **B**alancing (CD-GraB) algorithm, which enables provably accelerated training using SGD in the parallel setting (Section3);
* Prove that the convergence rate for CD-GraB exhibits a linear speedup over GraB, using the exact same assumptions as the original GraB paper (Section4);
* Produce extensive empirical validation of CD-GraB's improved scalability on a variety of tasks in deep learning and on large-scale logistic regression (Section5).

## 2 Preliminaries and Related Work

In this section, we discuss the preliminaries and prior scholarship on permutation-based example ordering, with particular attention paid to the centralized online Gradient Balancing Algorithm (GraB) . This lays the groundwork for how our coordinated, distributed GraB algorithm (Section3) imparts the efficiency guarantees of GraB to the parallelized regime (Section4).

Ordering data examples during training.Training a model can be formulated as minimizing a differentiable loss function \(f\!:\!^{d}\!\!\) over \(N\) data examples. The goal of this minimization is to obtain the target model weights \(^{*}\!=\!*{arg\,min}_{}f()\), where \(f()\!=\!_{j=1}^{N}f(;j)\), for which \(f(;j)\) denotes the loss incurred on the \(j\)-th example. A typical training process iteratively updates the model parameters \(\) by scanning over the \(N\) data examples repeatedly, with \(t\)-th scan (or epoch) following

\[_{t}^{j+1}\!=\!_{t}^{j}\!-\! f(_{t}^{j};\!_{ t}(j)),\  j\!\![N], \]

where \(\) denotes the learning rate, and \(_{t}\!:\![N]\!\![N]\) denotes a permutation ordering \(\!\! proves that faster convergence is possible for SGD when the averages of consecutive stochastic gradients converge faster to the full gradient. Based on this result, in follow-on work Lu et al.  proposes the centralized online Gradient Balancing algorithm (GraB), which outperforms RR, and upon which we base this work.

### GraB: Optimal, online, permutation-based example ordering for centralized ML

GraB is a permutation-based example-ordering algorithm that identifies provably better-than-random orderings _in centralized, single-node settings_ for SGD. GraB finds such orderings by leveraging information in stale stochastic gradients from previous epochs to guide ordering in the next epoch. More formally, for smooth, non-convex objectives, Lu et al.  proves that any permutation \(^{*}\) that guarantees

\[_{k[N]}_{j=1}^{k} f( ;^{*}(j))- f()_{}= (1)\ \ \ ( f()), \]

will yield a convergence rate of \(((NT)^{-2/3})\) (for epochs \(T\)) for SGD, which is superior to the \(O(N^{-1/3}T^{-2/3})\) convergence rate of random reshuffling .

GraB's connection to herding and balancing.To find such a permutation \(^{*}\), Lu et al.  connect  to the _herding problem_ and vector _balancing_. Understanding why GraB does not naturally extend to the distributed setting -- and our main contributions (Sections  and  -- requires some additional details on the fundamentals of herding:

Given \(N\) vector \(}\{_{j}\}_{j=1}^{N}\) (\(_{j}^{d}\)), \(_{j}_{2} 1\) (\( j\)), herding identifies a permutation \(^{*}\) such that

\[_{k[N]}_{j=1}^{k} _{^{*}(j)}-}_{}= (1),\ \ \ \ \ \ \ }=_{j=1}^{N} _{j}. \]

It is clear that  generalizes , which is a specific case of herding in an optimization setting.

Harvey and Samadi solve  with a method called _balancing_. Balancing uses a _signed_ version of the herding problem to optimize any given permutation \(\) to reduce the bound in . That is, balancing formulates the signed herding problem

\[_{k[N]}_{j=1}^{k}s_{(j)} _{(j)}-}_{ },\ \ \ \ \ \ \{s_{j}\}_{j=1}^{N}\{+1,-1\}. \]

Given a group of such signs \(\{s_{j}\}_{j=1}^{N}\) and an arbitrary permutation \(\), Harvey and Samadi prove that Algorithm  produces a new permutation \(^{}\) such that

\[_{k[N]}_{j=1}^{k}_{ ^{}(j)}-}_{} {2}_{k[N]}_{j=1}^{k}s_{(j)} _{(j)}-}_{}+ _{k[N]}_{j=1}^{k} _{(j)}-}_{}.\]

This says that, with new permutation \(^{}\), the objective of  now approaches the bound of . Importantly, recent advances show that it is quite cheap to find a group of signs, such that  is on the order of \((1)\) (e.g., Alweiss et al. , in Algorithm . We are therefore able to call Algorithm  repeatedly, which will eventually obtain the \(^{*}\) that solves the \((1)\) herding objective in .

GraB's application of herding to gradient balancing.Lu et al.  applies this framework of herding and balancing to develop GraB, i.e., to minimize . The main challenge for the success of this approach is to find the right gradients \(_{j}\) in the optimization context of . Notably, the herding and balancing framework requires the vector mean \(}\) in advance. To satisfy this requirement, GraB "centers" the gradient vectors using a _stale mean_. That is, GraB runs the herding algorithm on vectors that are defined as

\[_{j}= f(_{t}^{j};_{t}(j))- _{p=1}^{N} f(_{t-1}^{p};_{t-1}(p)), \]

where \(_{t}^{p}\) denotes the model weights after \(p-1\) updates in the \(t\)-th epoch, and \(_{t}\) denotes the permutation adopted in the \(t\)-th epoch. Lu et al.  proves that this definition of \(_{j}\) preserves the benefits of balancing with negligible noise or overhead. The only overhead comes from storing the running average of the gradients in epoch \(t-1\) to "center" the gradients in the subsequent epoch \(t\).

With this approach, Lu et al.  proves that GraB demonstrates more efficient convergence than RR for SGD. Better still, Cha et al.  demonstrates that GraB is in fact the _optimal_ permutation-based ordering method for SGD: it is not possible to produce a permutation-based ordering in the centralized setting that achieves a faster convergence rate for SGD.

Despite GraB's clear benefits over RR, it assumes local access to all examples. This assumption does not hold for popular, modern, parallel settings (e.g., parameter server ), in which workers only have access to subsets of examples. No present work has attempted to investigate GraB's applicability to this setting. While some work has studied distributed RR (D-RR) , it remains an open question if GraB's efficiency benefits for SGD can be conferred to the modern-scale, distributed-ML setup.

## 3 CD-GraB: A Provably Efficient Ordering Algorithm for Distributed Training

Our main contribution is to elevate GraB to the parallel regime, so that distributed training can enjoy the efficiency benefits of provably better example ordering. Based on the preliminaries, we can now explain why this is not a straightforward task: **While GraB achieves the optimal convergence rate for SGD on centralized data, it does not naturally translate to a distributed setting** (Section3.1). Our key insights for resolving these problems are to reformulate the herding framework in Lu et al.  to work in parallel, and to leverage insights from kernel thinning  to derive the _online_ PairBalance algorithm, which solves this parallelized herding objective (Section3.2). Lastly, we present the full-stack CD-GraB algorithm that makes our solution work in practice (Section3.3). The server implements online PairBalance, which coordinates gradient information from the distributed workers in training epoch \(t\) in order to determine a provably efficient example order for the next epoch \(t+1\) (Section4).

### Issues with GraB in the distributed setting

To clarify the issues with distributing GraB, we first need to define the distributed training setup more precisely. We consider the standard data-parallel regime with \(m\) parallel workers, where each worker keeps a copy of the model weights \(^{d}\) and maintains \(n=N/m\) local examples  As in many data-parallel training applications  such as geo-distributed model training , we assume _the data examples cannot be shared or moved across workers_. More formally, this setup can be expressed as

\[_{^{d}}[f()=_{i=1}^{m}f^{i}( )] f^{i}()=_{j=1}^{n}f^ {i}(;j), \]

where \(f^{i}(;j):^{d}\), \(j[n]\), denotes the loss incurred on the \(j\)-th example on the \(i\)-th worker for model weights \(\). We can now consider running  using this setup, for which each worker scans over their \(n\) local-data examples using (potentially) different permutations. We denote \(_{t,i}:[n][n]\) as the permutation-based ordering adopted on the \(i\)-th worker in the \(t\)-th training epoch. Adjusting  to accommodate the setup in , the update to the model can be summarized as

\[_{t}^{j+1}=_{t}^{j}-_{i=1}^{m} f^{i}( {w}_{t}^{j};_{t,i}(j)),\ \  j[n]. \]

That is, in epoch \(t\), each worker \(i\) selects their respective, local \(j\)-th example according to \(\{_{t,i}\}_{i=1}^{n}\) in order to compute stochastic gradients (Appendix).

**Following this setup, Algorithmho longer guarantees the \((1)\) bound to the herding problem , a bound that is valid only when _all_ data examples can be permuted _freely_. This constraint is fine for centralized GraB, but, in distributed training, workers only have access to a _subset_ of examples. Distributed training requires that _worker-specific permutations only involve the examples in their respective local subsets_. Further, recall that GraB uses stale means to center gradients  in order to solve the herding objective. This, too, causes problems in distributed training. In practice, it is typical to employ larger learning rates \(\) for greater scalability ; larger \(\) increases the discrepancy between averaged gradients in adjacent epochs, which, in turn, would make GraB's use of stale means unreliable.

### Our efficient solution: parallel herding and pair balancing

To address the limitations presented in the prior section, which preclude the direct application of GraB to distributed training, we will need to **1) reformulate the herding problem to fit the parallel setting, and 2) redesign how to do gradient balancing**, such that it both solves our new herding formulation and allows for reliability with higher learning rates. We now present our solution to both these problems; we introduce the _parallel herding_ problem and the online PairBalance subroutine that solves it.

Parallel Herding.To extend herding to the parallel setting, consider the following setup: There are \(m\) workers, which each have local access to \(n\) vectors. Let \(_{i,j}^{d}\) denote the vector indexed by \(j\) on the \(i\)-th worker. Assuming \(\|_{i,j}\|_{2} 1\) (\( i\!\![m], j\!\![n]\)), the goal of parallel herding is to find \(m\) permutations, \(_{1}\),\(_{2}\),...,\(_{m}\) where \(_{i}\!:\![n]\!\![n]\) (\( i\!\![m]\)), so as to minimize:

\[_{k[n]}\|_{j=1}^{k}_{i=1}^{m}_{i, _{i}(j)}\!-\!}\|_{}, }\!=\!\!_{i=1}^{m}\!_{j=1}^{n}_{i,j}. \]

When directly comparing 3 with 3, it is clear that parallel herding differs in two notable ways from the original herding problem. First, each permutation \(_{i}\!:\![n]\!\![n]\) (\( i\!\![m]\)) only decides the ordering of the \(n\) vectors that are associated with worker \(i\). Second, the prefix sum taken in the objective norm is accumulated over all the workers (the inner sum from \(i\!=\!1...m\)). This formulation naturally captures the setting in a distributed environment: **workers need to decide permutations collaboratively, and the worker-specific vectors are processed simultaneously rather than sequentially**.

Given that this formulation fits the distributed setting, we next need to show that parallel herding does in fact address the limitations posed by centralized GraB: that it is possible recover the original \((1)\) herding bound, and that we can solve the issue of unreliable stale gradients (Section3.1). The solution that we present in the remainder of this section is a new vector balancing subroutine: online PairBalance. To give an intuition, as its name suggests, online PairBalance leverages insights from kernel thinning to _balance_ vector differences over vector _pairs_. This also eliminates the need to perform vector centering, and thus solves the stale mean problem.

Using kernel thinning to solve parallel herding.We call our solution to the parallel herding objective 3_pair balancing_, which we derive from key insights in _kernel thinning_. In particular, Dwivedi and Mackey show that it is possible to solve the herding objective in \((1)\)**by only examining differences on _pairs of examples_. They derive an algorithm that generalizes Alweiss et al.  subroutine in Algorithm2, which solves herding in \((1)\) (Section2), and does so by operating only on vector-pair differences 3. This comes with a very useful property: eliminating the requirement of knowing the maximum vector norm ahead of time and centering the vectors (i.e., making all the vectors sum to zero) in order to solve the herding problem. This is the key to solving the parallel herding objective 3 in \((1)\), and elevating the benefits of GraB to a distributed setting.

Following Dwivedi and Mackey , we will balance over paired vectors, and will do so in an _online_ fashion (Section3.3). This eliminates GraB's requirement of using a stale mean to center gradient vectors (Section2.1), but still minimizes the parallel herding objective to \((1)\). We defer proving this result to Section4 and first describe our concrete algorithm. Online PairBalance applies Algorithm1 on the"flattened" and "paired" sequence of all of the workers' paired-difference gradients, i.e.,

\[_{n(k-1)+i}\!=\!_{i,2k-1}\!-\!_{i,2k}, k\!\! [], i\!=\!1...m.\]

That is, we fit these ordered-paired differences \(\{_{i}\}_{i=1}^{mn/2}\) into the herding and balancing framework (Algorithm1): if sign \(s\) is associated with \(_{n(k-1)+i}\), then \(_{i,2k-1}\) and \(_{i,2k}\) receive \(s\) and \(-s\), respectively.

### The full-stack CD-GraB algorithm

Having solved the parallel herding problem with pair balancing, we now demonstrate how to bring everything together in an optimization context to _coordinate distributed gradient balancing_ for distributed training. That is, we can now introduce our full-stack CD-GraB algorithm, which trains models in a distributed setting (Section3.1) while efficiently ordering the examples by using PairBalance (Section3.2) Algorithm2 in an online manner.

We describe CD-GraB at two levels of abstraction: a high-level illustration (Figure steps 1-7) and a detailed pair of worker-server algorithm statements (Figure. Since the workers only have access to a subset of the training data, in parallel they compute local, per-example stochastic gradients and send them to the server. The server simultaneously calls PairBalance online, which coordinates information from all the workers' gradients (i.e., using adjacent example-specific gradients) to determine the next epoch's worker-specific permutations. In more detail:

In epoch \(t\), (Figure  step 1) the two workers have permutations \(_{t,1}\) and \(_{t,2}\), respectively. Each worker computes per-example gradients \(^{i}_{j}\) (2; Algorithm ), and sends them to the server (3; Algorithm ). The server we implement functions as a parameter server : It computes the average of the workers' per-example gradients (Algorithm ), and sends it back to all workers (Algorithm ) so that they can update their local models (Algorithm -6-7). Simultaneously, as the server receives gradients (Algorithm ), it calls PairBalance (Algorithm  on adjacent vectors (4; Algorithm -13). PairBalance produces signs to supply to the reordering algorithm (Algorithm , which, using the current worker permutations \(_{t,i}\), produces the new per-worker permutations for the next epoch (5; Algorithm 14). In Figure  these correspond to \(_{t+1,1}\) and \(_{t+1,2}\), which the server then sends back to the respective workers (6; Algorithm 15). Lastly, before the start of the next epoch, the workers reorder their examples according to the new permutations (7; Algorithm ).

## 4 Convergence Analysis

We next demonstrate formally that our CD-GraB algorithm (Section 3.3) confers the efficiency benefits of centralized GraB (Section 2.1) to the distributed setting. In brief, our main theoretical results show that **CD-GraB enjoys a linear speedup in convergence rate** under two sets of conditions: smoothness (Theorem  and the Polyak-Lojasiewicz (P.L.) condition (Theorem . Both results guarantee that CD-GraB is faster than distributed random reshuffling (D-RR). Our proofs rely on Corollary 7 from Dwivedi and Mackey , which shows that, with high probability, RandomizedBalance (subroutine in Algorithm  from Alweiss et al. ) guarantees a \((1)\) bound to the signed herding objective 10

To begin, we restate this result to cohere with our framework, for which the vectors \(_{j}\) are gradients in an optimization context:

**Theorem 1** (Corollary 7, Dwivedi and Mackey ).: _Consider any vectors \(\{_{j}\}_{j=1}^{N}\) (\(_{j}^{d}\)) with \(\|_{j}\|_{2} 1\) supplied as input to the RandomizedBalance subroutine in Algorithm . Then for any \(>0\), with probability at least \(1-\), RandomizedBalance outputs a sequence of signs \(\{s_{j}\}_{j=1}^{N}\{-1,1\}\) that satisfy \(_{k[N]}\|_{j=1}^{k}s_{j}_{j}\|_{} {A}\), where \(=)()}=(1)\)._

Figure 1: **Left:** The PairBalance algorithm, which the server runs online. **Right:** CD-GraB running on one server (top) and two workers (bottom). The workers do not share data examples.

```
1:\(m\) workers, \(n\!:=\!\) ex. per worker input: initial\(_{1}^{1}\), epochs \(T\), learning rate \(\)
2:receive: initial permutations \(\)\(\{_{1,i}\}_{i=1}^{m}\)
3:for epoch \(t\!:=\!1...T\)do\(\) Run in parallel for workers \(i\!=\!1...m\)
4:for example \(j\!:=\!1...n\)do
5:compute:\(_{j}^{i}\!:\!\! f^{i}(_{t}^{j},_{t,i}(j))\)
6:send:\(_{j}^{i}\)\([j]{\ _{j}^{i}}\)
7:receive:\(}_{j}\)\(\) avg. \(j\)-th stochastic grad. \(}_{j}\)
8:update:\(_{t}^{j+1}\!:=\!_{t}^{j}\!-\!}_{j}\)
9:endfor
```

**Algorithm 3** CD-GraB Workers

To integrate this result with our parallel setting, we need some additional assumptions that are standard in the literature on distributed optimization -- that the variance of the per-example gradients on each worker is uniformly bounded (Assumption1), and that the variance between worker-specific gradients is similarly bounded (Assumption2). More precisely, following the distributed setup in 1, we denote the global loss gradient to be \( f()\), each \(i\)-th worker's local loss gradient to be \( f^{i}()\) (\( i\!\![m]\)), and each \(i\)-th worker's per-example loss gradients to be \( f^{i}(;j)\) (\( j\!\![n]\)). We assume:

**Assumption 1** (**Bounded Gradient Variance)**.: _For all \(i\!\![m]\) there exists a constant \(\!>\!0\) such that for all \(j\!\![n]\) and for all \(\!\!^{d}\), it holds that \( f^{i}(;j)\!-\! f^{i}()_{2}^{2}\! \!^{2}\)._

**Assumption 2** (**Bounded Data Heterogeneity)**.: _There exists a constant \(\!>\!0\) such that \( i\!\![m]\), \( f^{i}()\!-\! f()_{2}^{2}\!\! ^{2}\)._

Lastly, we include one additional assumption from the original GraB paper : we assume a cross norm \(L_{2,}\) (which can be easily adapted to \(L_{2}\)-smoothness by setting \(L_{2,}\) to be \(L_{2}\)).

**Assumption 3** (**Smoothness)**.: _There exists constant \(L_{2,}\!>\!0\) such that for any \(\!,\!\!\!^{d}\), any \(i\!\![m]\), and any \(j\!\![n]\), it holds that \( f^{i}(;j)\!-\! f^{i}(;j)_{2}\!\!L _{2,}\|\!-\!\|_{}\)._

Given these assumptions, we can prove a convergence guarantee for CD-GraB:

**Theorem 2**.: _Suppose that Assumptions1,2 and 3 hold. For any \(\!>\!0\), if we set learning rate \(\) to be_

\[\!=\!\!\{(2n\!+\!/m)},\!( m^{2}}{42L_{2,}^{2}(\!+\!)^{2}^{2} nT\!+\!18L_{2,}^{2}m^{2}n^{3}^{2}})^{1/3}\}\!,\]

_where \(F_{1}=f(_{1})-_{^{d}}f()\) and \(\) comes from Theorem1. Then, with probability at least \(1\!-\!T\),_

\[\!_{t=1}^{T}\!\| f(_{t})\|_{2}^{2} \!\!L_{2,}(\!+\!))^ {2/3}}{(mnT)^{2/3}}\!+\!L_{2,})^{2/3}\!+\!64F_{1}L_ {2,}(2\!+\!/(mn))}{T}\] \[=\!\!(}\!+\!)\!.\]

We can also prove an accelerated rate for CD-GraB if we additionally assume the P.L. condition:

Figure 2: CD-GraB worker and server (here, a parameter server ) algorithms.

**Assumption 4** (P.L. Condition).: _We say the loss function \(f\) fulfills the P.L. condition if there exists \(\!>\!0\) such that for any \(\!\!^{d}\), \(\| f()\|_{2}^{2}\!\!(f()-_{ ^{d}}\!f())\)._

**Theorem 3**.: _Suppose that Assumptions_  _ _ _and_  _hold. For any \(\!>\!0\), we set constants \(\) and \(C_{3}\) to be_

\[C_{3}\!=\!\!+\!^{2}/L_{2,})^{2}}{224L_{2,}^{2 }(\!<\!\!)^{2}^{2}}\!=\!W_{0}(T^{2}m^{2 }n^{2}C_{3}),\]

_where \(\) comes from Theorem_  _ _F\({}_{1}\) is from Theorem_  _and \(W_{0}\) is the Lambert-W function. If we set learning rate \(\!=\!}{Tn}\) and if the number of epochs \(T\) satisfies_

\[T\!\!10\!+\!32L_{2,}(2\!+\!/(mn))W_{0}((mnT)^ {2}C_{3})\!=\!(1),\]

_then, with probability at least \(1\!-\!T\), it holds that_

\[F_{T+1}\!\!}\!+\!L_{2,}^{2} ^{2})}{C_{3}}\!+\!^{2}(\!<\!\!)^{2} ^{2}^{2}}{^{3}}\!=\!\!(})\!,\]

_where \(F_{T+1}\!=\!f(_{T+1})-_{^{d}}\!f()\)._

We prove Theorems2 and 3 in the Appendix. Together, they show that CD-GraB exhibits a linear speedup in the number of workers \(m\) over GraB 's convergence rates (\(((nT)^{-2/3})\) and \(((nT)^{-2})\), respectively)10 under both smoothness and the P.L. condition. Further, CD-GraB's convergence rate of \(((mnT)^{-2})\) is faster than many previous rates11 such as the high probability bound of \(((mn)^{-1}T^{-2})\) for D-RR in Yun et al. .

## 5 CD-GraB in Practice: Distributed and Simulation Experiments

We next verify CD-GraB's accelerated convergence on a variety of empirical tasks12 For ease of comparison, we follow the experimental plan from the original GraB paper13 and add some additional large-scale logistic regression experiments. We also run an ablation study to isolate the effects of different improvements in CD-GraB. We do this because online PairBalance exhibits performance benefits that are separate from parallelism -- namely, removing the need for gradient centering with a stale mean and allowing for higher learning rates (Section3.2)14

**Evaluating CD-GraB's convergence speedup.** We use the following three tasks for evaluating distributed training efficiency: logistic regression on a large-scale mortgage application (New York 2017 subset, 244,107 examples with 18 features)11 (Figure3(a), Long Short-Term Memory (LSTM)17 on the WikiText-2 dataset  (Figure3(b), and autoregressive Multi-Layer Perceptron (MLP) on the M4 Weekly dataset  (Figure3(c). We measure the loss incurred on the entire training set (Full Train Loss) and task-appropriate test metrics during evaluation, with respect to both the number of epochs and wall-clock time. Regarding test metrics, we measure test accuracy for the mortgage application, perplexity for WikiText-2, and SMAPE for M4. Additional details regarding the datasets, models, and test metrics can be found in the Appendix.

For all three tasks, we use a single 128 GiB memory machine with 4 NVIDIA GeForce RTX 2080 Ti GPUs. For the mortgage application and WikiText-2 (Figures3(a) and 3(b), we launch \(m\!=\!4\) workers (processes), where each worker runs on one GPU. For the M4 task, we launch \(m=32\) workers, where each of the 4 GPUs hosts 8 process workers. We use NCCL as the distributed communication backend  for the mortgage application and WikiText-2 tasks, and GLOO  as the distributed communication backend for the M4 task.

As shown in Figure3 we compare CD-GraB's convergence to the standard distributed-training example-ordering method: random reshuffling (D-RR). From all subfigures in Figure3 we observe that CD-GraB outperforms the D-RR baseline significantly and consistently: CD-GraB exhibits better training loss and test metrics, measured against both the number of epochs and wall-clock time. We also note that theresults for CD-GraB are much smoother than for D-RR. This is likely due to the variance of stochastic gradients during training, which CD-GraB reduces as a side-effect (so, too, does GraB, in comparison to RR). For smoother D-RR results, we can reduce the learning rate (Appendix). CD-GraB allows for the use of a larger learning rate, which accelerates training while preserving the final model's performance.

**Ablation simulation study: the importance of coordination at large scale.** CD-GraB has several design benefits over the original centralized GraB algorithm : coordinating parallel workers' specific permutations using PairBalance on the server (Algorithm2 and removing the dependency on a stale mean (Section2.1), which enables the ability to using larger learning rates reliably (Section3.2). Clearly, not all of these benefits come directly from distributing training. For example, being able to use larger learning rates, is a side effect of our solution to develop CD-GraB, not our main contribution. Therefore, we run a simulation ablation study to disentangle the relative importance of each of CD-GraB's efficiency benefits over GraB. To do so, we compare the convergence of CD-GraB to two additional baselines in the distributed setting, beyond D-RR: (1) **ID-GraB (Bal)**, where each independent worker runs GraB locally using RandomizedBalance (subroutine in Algorithm2 to perform gradient vector balancing; (2) **ID-GraB (PairBal)**, where each independent worker runs GraB locally using PairBalance.

Figure4 summarizes the results, with convergence curves for \(m\{4,8,16,32,64\}\) workers training LeNet on CIFAR-10. We choose this task and architecture to cohere with the experiments done in the original GraB paper. For these experiments, we denote \(B\) to be the _aggregated_ minibatch across all the workers, which refers to the number of stochastic examples used for an overall optimization step; each worker thus has a subset of this minibatch -- an equivalently-sized subset of \(B\) examples. We make two main observations. First, when scaling up training with more workers, CD-GraB converges increasingly faster than the no-coordination-ordering methods **ID-GraB (Bal)** and **ID-GraB (PairBal)**. This result aligns with our theory and intuition that, when the number of workers \(m\) increases, the parallel herding bound  will increase linearly if there is no coordination. Second, as we scale up to larger \(m\), the convergence curves of **ID-GraB (Bal)** and **ID-GraB (PairBal)** gradually approach

Figure 3: Convergence of CD-GraB in comparison to D-RR. For each experiment, we show train loss over epochs and time (**left** of each subfigure) and test performance over epochs and time (**right** of each subfigure). We run at least 3 random seeds, and plot the mean \(\) STD.

the curve for D-RR: at larger scales, herding-based example ordering will be no better than randomly permuting the dataset. Both observations give strong evidence that coordination (i.e., running online PairBalance on the server to coordinate per-worker permutations) is critical for accelerating training.

We note that all of these experiments use SGD, since both the theoretical results of the original GraB paper and our results for CD-GraB here are for SGD. In the Appendix, we additionally include results for training GPT-2 on WikiText-103, for which we use AdamW as the optimizer. We find that GraB with AdamW works in practice; however, our theory results do not directly apply to these experiments. We additionally include results on memory usage in the Appendix, which show that CD-GraB results in negligible overhead in practice.

## 6 Conclusion and Future Work: Toward an Order Server Architecture

We elevate the benefits of provably faster, permutation-based example ordering to the contemporary ML distributed-training setting. We focus on reformulating the online **G**radient **B**alancing algorithm (GraB)  because, even though it is the provably optimal permutation-based example-ordering method , it is limited by design to _centralized_ settings (Section[3.1]. To overcome these limitations, we redesign GraB's herding and balancing framework to account for parallel workers: A _parallel herding_ objective, which we solve with an online PairBalance subroutine, based on key insights from kernel thinning . PairBalance operates on ordered _pairs_ of vectors to do _balancing_, which enables our full-stack, low-overhead, _Coordinated_ and _Distributed_ online CD-GraB algorithm. We give a full specification of our online CD-GraB algorithm (Section[3.3], provide convergence rate guarantees regarding its speedups on both 1) smooth non-convex and 2) P.L. objectives (Section, and verify these speedups in practice on single-node distributed tasks and a simulated ablation study (Section).

Both our theory and experiments demonstrate that CD-GraB really shines when there are multiple training epochs (Appendix). This is another reason that we do not emphasize experiments involving fine-tuning pre-trained models like GPT-2, as fine-tuning can be achieved in just a couple of epochs. As noted above, it is also more common to train such models using optimizers from the Adam family. In future work, we intend to extend the theory on GraB and CD-GraB to such optimizers, which would make the results on optimal, permutation-based example ordering more useful for base-model pre-training.

Pre-training from scratch would demonstrate the tremendous power of CD-GraB to scale to very large models; however, we did not have the training budget to perform such experiments for the present work. Further, to truly exercise the benefits of CD-GraB in such large-scale settings, future work should investigate moving beyond the single-node setup that we present. Notably, to train larger models, our results suggest a novel distributed training architecture. The ordering operation performed by the server (Algorithm) is _not_ very latency sensitive; the server has the duration of the entire epoch \(t\) to compute the new permutations for the next, \(t+1\) epoch. Given this relaxed latency requirement, and the success of our algorithmic results, it would be an exciting direction for future ML-systems research to invest in building an _Order Server_ architecture. Such an architecture, which could be composed with traditional parameter servers, would afford the scalability benefits of CD-GraB to a host of massive-scale ML applications.

Figure 4: Convergence for CD-GraB, D-RR, ID-GraB (Bal), and ID-GraB (PairBal) training LeNet on CIFAR-10, with \(m\{4,8,16,32,64\}\) workers. For each, the aggregated minibatch size per update is 64.