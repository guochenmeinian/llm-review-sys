# Adversarial Prompt Evaluation: Systematic Benchmarking of Guardrails Against Prompt Input Attacks on LLMs

Adversarial Prompt Evaluation: Systematic Benchmarking of Guardrails Against Prompt Input Attacks on LLMs

Giulio Zizzo  Giandomenico Cornacchia  Kieran Fraser  Muhammad Zaid Hameed

Ambrish Rawat  Beat Buesser  Mark Purcell  Pin-Yu Chen

Prasanna Sattigeri  Kush Varshney

IBM Research

{giulio.zizzo2,giandomenico.cornacchia1,kieran.fraser

zaid.hameed,beat.buesser,pin-yu.chen}@ibm.com

{ambrish.rawat,markpurcell}@ie.ibm.com

{psattig,krvarshn}@us.ibm.com

###### Abstract

As large language models (LLMs) become more integrated into everyday applications, ensuring their robustness and security is increasingly critical. In particular, LLMs can be manipulated into unsafe behaviour by prompts known as jailbreaks. The variety of jailbreak styles is growing, necessitating the use of external defenses known as guardrails. While many jailbreak defences have been proposed, not all defences are able to handle new out-of-distribution attacks due to the narrow segment of jailbreaks used to align them. Moreover, the lack of systematisation around defences has created significant gaps in their practical application. In this work, we perform systematic benchmarking across 18 different defences considering a broad swathe of malicious and benign datasets. We find that there is significant performance variation depending on the style of jailbreak a defence is subject to. Additionally, we show that based on current datasets available for evaluation, simple baselines can display competitive out-of-distribution performance compared to many state-of-the-art defences. Code is available at [https://github.com/IBM/Adversarial-Prompt-Evaluation](https://github.com/IBM/Adversarial-Prompt-Evaluation).

## 1 Introduction

Large language models (LLMs) have gained attention due to their advanced capabilities, and are increasingly becoming part of more complex systems , which necessitates the requirement that these models be robust against adversarial manipulations. LLMs not only inherit traditional security pitfalls like evasion and poisoning attacks , but are also prone to safety vulnerabilities like jailbreaks and prompt injection attacks. To make LLMs robust, they are usually trained/fine-tuned to produce safe output in a process called'safety training' or 'alignment' .

To evaluate the safety aspects of aligned LLMs, prompt injection and jailbreak attacks are of particular importance, as they are employed to target aligned LLM models to produce adversarially-controlled outputs . As jailbreaks have been shown to break alignment of safety-trained models, additional layers of protection called guardrails have been proposed. These guardrails can be used in addition to the alignment process, and make the overall LLM-based system more secure. Some of these guardrails can be composed of perplexity filters, tools for input prompt paraphrasing , keyword-based detectors, semantic similarity based detectors , or output filters that monitor the response generated by LLMs to detect any harmful information . Despite showing improvement in defending against jailbreak attacks, these approaches have limitations and their applicability against more sophisticated attackers remains an open research problem [7; 14].

Currently, there is no standard benchmark framework for evaluating different guardrails as the approaches proposed in the literature vary widely in terms of evaluation approaches, representative datasets used for comparison, and metrics, e.g. string matching evaluation or BERT-based models for classifying the prompt as jailbreak or benign . In this context, our work addresses the following research questions (RQs):

* Are the currently available benchmarking datasets sufficient to adequately assess the quality of proposed guardrails, and how well do existing guardrails and defences perform on a wide cross-section of different attacks and datasets?
* How do guardrails compare when considering additional constraints such as memory size, inference time, and extensibility?
* How to approach and recommend guardrails to practitioners for deployment and use?

Guided by the above RQs and existing limitations in jailbreak evaluation benchmarks, we present an extensive benchmark evaluation with the following contributions:

* We highlight the limitations of previous benchmark evaluations, and how they might result in inaccurate attack and defence evaluation.
* We evaluate attack success rates on known adversarial datasets in a systematic manner, using an evaluation framework combining different evaluation metrics.
* We evaluate different defences proposed in the literature including different guardrails using the evaluation benchmark presented in this paper.
* We provide insights on whether model complexity in the defence provides better out-of-distribution (OOD) generalization.

## 2 Related Works

**Attacks:** Prompt injection describe attacks where crafted inputs aim to generate an inappropriate response. This can be achieved by circumventing existing safeguards via jailbreaks [8; 9; 15; 10] or via indirect injection attacks [16; 17]. Here, adversarial prompts are crafted to pursue different goals like mislead models into producing unwanted output, leak confidential information, or even perform malicious actions [18; 19; 20]. Furthermore, attacks can be categorised based on their methods of generation, e.g optimization-based attacks, manually crafted attacks, and parameter-based attacks that exploit the model's sampling and decoding strategies for output generation [8; 21].

**Defences:** Strategies to defend against prompt injection attacks include safety training [22; 23], guardrails [24; 25], or prompt engineering and instruction management [26; 27; 28]. These techniques have different resource requirements, and currently, there is neither a silver bullet to defend against prompt injection attacks, nor a way to prescribe a specific defense. Our work on benchmarking guardrails creates a system of recommendations for defences against prompt injection attacks.

**Benchmarks:** Our first line of benchmarking work includes representative datasets of inputs generating unwanted outputs. One such repository of sources is available at www.safetyprompts.com which contains multiple databases characterised along dimensions of safe vs. unsafe. Our second line of work attempts to consolidate prompt injection attacks for comparison, which includes works like HarmBench , Jailbreakbench , and EasyJailbreak . However, defences have not received the same attention and there is currently no benchmarking suite specifically for guardrails.

## 3 Datasets

Our benchmarking is founded on a compilation of diverse datasets containing both benign and malicious prompts. These datasets are categorized based on their target type, either "_jailbreak_" or "_benign_", and their details in terms of their splits, the number of samples, and the types of prompts they include is described in Table 1. The prompt types span several categories, including instruction-based, question-based, artificial attacks (e.g., those generated iteratively with the use of language models), role-playing, harmful behavior, toxic content, and chat-based interactions. A detailed dataset description is found in the Appendix.

This characterisation of jailbreak datasets is useful for contextualising guardrails. Comparing guardrails across these datasets highlights their strengths and shortcomings in terms of handling different jailbreak styles. Additionally, we include several benign datasets to assess the false positive rate, and thus the feasibility of deploying guardrail defenses in production. Generalisation capability of guardrail beyond the anecdotally observed jailbreaks is critical to their deployment. Our analysis of out-of-distribution evaluation set is specifically tailored for this analysis.

## 4 Model Defences

Broadly, defences can be categorised into two groups. First, are detection-based approaches that construct guardrails externally to the LLM to detect attacks. Second, are methods that use LLMs to judge and filter out malicious prompts based on their alignment coupled with a defence algorithm.

### Detection-Based Approaches

**Perplexity Threshold:** This detector uses perplexity as a mechanism for detecting perturbations within prompts. We implement the perplexity filter from Jain et al. , which was proposed for identifying sequences of text that contain adversarial perturbation, like those added by GCG . We use GPT-2 for computing perplexity, and fix a threshold at the maximum perplexity calculated over all prompts in the _AdvBench_ dataset, as per the author implementation.

**Random Forest:** The classifier consists of a simple _random forest_ trained on unigram features extracted from the training dataset (see Section 5). The text corpus is initially transformed to lower-case and then tokenized, using each _word_ and _punctuation_ as single token (i.e., feature).

**Transformer Based Classifiers:** We implement a series of simple baseline classifiers consisting of the BERT , DeBERTa , and GPT2  architectures. The classifiers are fine-tuned to detect malicious vs non-malicious prompts over the training datasets described in Section 5.

    & & & & & & & & &  \\  Target & Dataset & Split & Samples & Instruction & Question & Artificial & Role & Harmful & Toxic & \\  & & & & & & & Attack & Playing & Behavior & Behavior & Chat \\   & aart  & Train/Test & 3224 & ✓ & & ✓ & & & & & \\  & atmq  & Train/Test & 455 & ✓ & & & & & & \\  & do\_not & & & & & & & & & \\  & \_answer  & & Train/Test & 938 & ✓ & ✓ & ✓ & & & & \\  & gandal\_ignore & & Train/Test & 1000 & ✓ & & & ✓ & & & \\  & \_instructions  & & Train/Test & 520 & & ✓ & ✓ & & ✓ & & \\  & gcg\_vicun  & Train/Test & 512 & & & & & ✓ & & ✓ & \\  & harmful & & Train/Test & 512 & & & & & ✓ & & \\  & behavior  & & Jiailebreak & & & & & & & & \\  & jailbreak & & & & & & & & & ✓ & ✓ \\  & prompts  & Train/Test & 652 & & & & & & & ✓ & ✓ \\  & sp  & Train/Test & 1600 & & & & ✓ & & & & \\  & tap  & Train/Test & 2134 & & & ✓ & & ✓ & & \\  & touchetat  & OOD Test & 204 & & & & & ✓ & ✓ & \\  & malicious & & & & & & & ✓ & ✓ & \\  & instruct  & OOD Test & 100 & ✓ & & & ✓ & & & \\   & aylpack & & & & & & & & & ✓ & ✓ \\  & alpaca  & Train/Test & 52002 & ✓ & & & & & & \\  & awesome\_chatgrt & & Train/Test & 152 & ✓ & & & & & \\  & \_prompts  & Train/Test & 12697 & & ✓ & & & & & \\  & bools  & Train/Test & 12697 & & & ✓ & & & & \\  & no\_robots  & Train/Test & 9996 & ✓ & & & & & & \\  & puffin  & Train/Test & 5833 & & & & & & & ✓ \\  & super\_natural & & Train/Test & 1545 & ✓ & & & & & \\  & \_instructions  & Train/Test & 256026 & & & & & & & ✓ \\   

Table 1: A overview of the characteristics of the datasets used. The prompt types are specified across several categories: instruction-based, question-based, artificial attack (e.g., if generated through other models), role playing, harmful- and toxic-behavior, and chat-based.

**LangKit Injection Detection:** In this approach1, a prompt is transformed to its embedding and compared to embeddings of known jailbreaks. Cosine similarity is used as the closeness metric. The exact prompts used for constructing the malicious embedding are not specified by WhyLab's LangKit.

**ProtectAI:** ProtectAI Guard is a security tool designed to detect prompt injection attacks. The model is a fine-tuned version of the microsoft/deberta-v3-base model, which is based on Microsoft's BERT Language Model and features 86 million backbone parameters . ProtectAI Guard is trained on a diverse dataset comprising prompt injections, jailbreaks, and benign prompts. In this work, we utilize both versions available on Hugging Face (i.e., v1\({}^{2}\) and v2\({}^{3}\)).

**Azure AI Content Safety:** The Azure AI Content Safety API is a service provided by Microsoft Azure for moderating content safety . It utilizes a combination of classification models designed to prevent the generation of harmful content. For our experiment, we use the jailbreak endpoint API4.

**OpenAI Moderation:** OpenAI Moderator  is an AI-powered content moderation API designed to monitor and filter potentially harmful user-generated content . In our experiments, we use the text-moderation-007 model, which classifies content into 11 categories, each associated with a probability score. We treat content moderation as a binary classification task, where the highest probability among the harmful categories indicates the likelihood of a jailbreak.

### LLM as a Judge

**Vicuna:** As a baseline we use the Vicuna LLM model and check if it refused to answer a particular prompt. We follow a similar strategy to [8; 58] and check for the presence of refusal keywords to automate the output analysis.

**SmoothLLM:** SmoothLLM  aims to tackle GCG-style attacks. The core of the defence is to perturb the prompt such that the functionality of the adversarial payload breaks, and the LLM then refuses to answer the question. The principal drawback is the high computational cost: each prompt needs to be perturbed multiple times which can incur an order of magnitude higher compute costs, and the defence is relatively specialised tackling only a particular style of jailbreak.

**LangKit Proactive Defence:** This defence [12; 17] relies on the idea of supplying a specific secret string for the LLM to repeat when concatenated with user prompts. As many attacks will contain elaborate instructions to override system prompt directives, then, when under attack, the model will not repeat the secret string but rather respond to the adversarial prompt.

**NeMo Guardrails:** NeMo guardrails  provides a toolkit for programmable guardrails that can be categorized into topical guardrails and execution guardrails. The input moderation guardrail is part of the execution guardrails where input is vetted by a well-aligned LLM, and then passed to the main system after vetting it. The input moderation guardrail implementation in this work is inspired by the NeMo input moderation guardrail5, and is modified by including additional instructions and splitting the template between system prompt and post-user prompt, which guides the initial response of the LLM. Changes are specified in the Appendix.

**Llama-Guard:** Llama-Guard is an LLM-based safeguard model specifically designed for Human-AI conversation scenarios . Two versions of the Llama-Guard model are considered: Llama-Guard  which belongs to the Llama2 family of models and Llama-Guard-2  which belongs to the Llama3 family of models. Llama-Guard models function as binary classifiers, categorizing prompts as either "_safe_" or "_unsafe_" with its first generated token.

[MISSING_PAGE_FAIL:5]

their minimal computational cost and training time indicates that they can act as a viable defence that can be continuously updated with new datasets.
* Guardrails based on LLMs generally boost the detection rate at the cost of an increase in FP rate and this FP rate increase is not necessarily uniform among datasets, e.g., SmoothLLM incurred significant penalties on BoolQ and XSTest datasets. This highlights that defences must be evaluated using a broad a set of datasets, as SmoothLLM defence's evaluation in the original paper did not show low performance on benign datasets.

Overall, based on current benchmark results, we can remark that: (i) _either_ the breadth and range of openly available data is sufficient to adequately represent jailbreak attack diversity, in which case simpler classification-based defences can provide competitive performance at a fraction of the compute cost. (ii) _Or_, if we are to hypothesise that LLM-based defences _can_ generalise better than their classifier-based counterparts, then we do not currently have a rich-enough source of data to demonstrate this in the academic literature, particularly when some papers evaluate only on a small quantity of data . This highlights both the limitations of available datasets in covering the attack space and, consequently, the rapid growth of new unexplored attacks, which makes it challenging to evaluate a defence's generalisation capability.

**How do the Guardrails compare beyond performance metrics? (RQ2)**: We record model size and inference conditions for comparing guardrails in practical use. The latter determines how input prompts of different lengths are handled, the inference time for each request, and the throughput of the guardra

Figure 1: Heatmap illustration of the true positive rates of different guardrails defenses on each jailbreak dataset. NB: the GCG attack was computed against Vicuna 7b.

    &  & Malicious \\  & & Instruct \\  Random Forest & 0.1228 & 0.2400 \\ BERT & 0.7105 & 0.9400 \\ DeBERTa & 0.7281 & 0.9000 \\ GPT2 & 0.6930 & 0.8000 \\ Protect AI (v1) & 0.4386 & 0.0000 \\ Protect AI (v2) & 0.5702 & 0.0000 \\ Llama-Guard & 0.2636 & 0.8200 \\ Llama-Guard & 0.1491 & 0.8900 \\ Langlit Injection Detection & 0.4386 & 0.0000 \\ SmoothLLM & 0.4649 & 0.4800 \\ Perplexity & 0.0088 & 0.0000 \\ OpenAI\_moderation & 0.0702 & 0.0200 \\ Azure AI Content Safety & 0.5614 & 0.0000 \\ NcMo Inspired Input rail & 0.9210 & 1.0000 \\ (Vicuna-7b-v1.5) & 0.4912 & 1.0000 \\ (Vicuna-23b-v1.5) & 0.4912 & 1.0000 \\ (Langlit Proactive Defence & 0.4474 & 0.4200 \\ (Vicuna-7b-v1.5 Refusal Rate & 0.3421 & 0.4200 \\ (Vicuna-13b-v1.5 Refusal Rate & 0.3596 & 0.6900 \\   

Table 3: TP rate performance of each defence on _out-of-distribution_ (OOD) datasets. We want point out to the reader that what can be considered OOD samples for Random Forest, BERT, DeBERTa, and GPT2 could not be guaranteed for others defences.

guardrails this includes the number of inferences required by the defence. Exact time and throughput results can be seen in the Appendix.

Firstly, memory footprint of guardrails varies from as little as 91 MB from LangKit Injection Detection scheme to host the embedding model, to as high as 26.03 GB to handle the memory footprint of Vicuna 13b. Detection-based approach rely on an underlying classification pipeline and generally are amongst those with the highest memory vs performance ratios: with the transformers of BERT, DeBERTa, and GPT2 varying in memory footprint between 371MB - 548MB.

Secondly, inference scheme for the different guardrails is tightly coupled with their latency and throughput. For any guardrail that implicitly relies on a transformer, the maximal token length of the model determines the length of input prompts that the system can handle. Chunking and windowing can be used to extend this to strings of arbitrary length, but this will increase the inference time and reduce the throughput.

Lastly, LLM-based guardrails including NeMo and LangKit's Proactive defence can be used as standalone guardrails, or as modules that protect larger/unaligned LLMs. Comparing NeMo with the baseline provides an insight into the added benefits of using the NeMo pre-filtering step. However, in this modality using LLM based schemes incur additional non-negligible inference calls. SmoothLLM can add up to 10 extra inferences, while NeMo adds 1 extra inference per prompt.

In conclusion, when comparing guardrails beyond performance for deployment scenarios, model size, inference performance, and response metrics are crucial. LLM-based approaches can require additional inference calls which may impact memory footprint, latency and throughput.

**How to recommend guardrails for practical use? (RQ3)**:

Recommending a guardrail for practical use requires knowledge of the defender's capabilities. With access to compute resources, guardrails can be deployed as a standalone service which filters every inference request before feeding it to an LLM. Most of the guardrails we have discussed within the context of this work do not use additional information about the LLM they are seeking to protect. However, one can envision scenarios where white-box access to the underlying LLM is used to determine and filter a prompt attack vector. We can draw the following suggestions:

* As discussed in Section 6 guardrails have significantly different resource requirements, and currently, there is no one-size-fits-all solution. LLMs receive safety training and often continue to receive updates for patching observed security vulnerabilities like jailbreaks and prompt injections. Therefore, the choice of guardrail for an LLM depends on a model's inherent defense mechanism against these threats as there will be an overlap between their defense capabilities. Moreover,

Figure 2: FP rate heatmap results of different guardrails defence on each benign dataset.

the spectrum of threat vectors can vary from direct instructions to adversarial manipulation via persuasive language , or even algorithmically computed strings . Off-loading the detection of all such vectors to one guardrail is a significant challenge requiring a large range of representative datasets to have an effective true positive rate vs. false positive rate trade-off.
* Another dimension to consider is the extensibility of these models to new attack vectors. Score-based guardrails like the perplexity threshold filter is only parameterised by a threshold. Therefore, it does not need model training, and can be easily adapted to new scenarios. Similarly, the LangKit detector may be extended to new scenarios by adopting the database of vectors used for similarity comparisons. Classifier-based approaches require model re-training to extend to new attack vectors. NeMo guardrail is only parameterised by its prompt, and so is highly extensible and can be used in combination with an aligned LLM. Llama-Guard on the other hand is parameterised by a system prompt but in addition also relies on the underlying model that has been tuned for the task of safe vs unsafe classification. Adopting Llama-Guard to new scenarios will therefore require both training and potentially changes to the prompt template. Finally, while the guardrails seek a binary decision of safe vs unsafe, it is useful to assess their performance by considering a third dimension of unsure. LLM as a judge based defences may be more easily extended to include this third option via prompting. However, adopting the binary classifiers to such scenarios may require retraining or techniques like conformal calibration on output probabilities .

In conclusion, recommending a guardrail for practical use requires understanding the defender's capabilities, as guardrails vary significantly in resource requirements and extensibility to new attacks. The choice of a guardrail depends on the model's inherent defenses and the spectrum of threat vectors it faces, highlighting the need for a tailored approach rather than a one-size-fits-all solution.

## 7 Conclusion and Limitations

In this work, we performed a wide benchmarking of guardrails over a large number of datasets. We show that many defences can have large performance differences depending on the attack style considered, highlighting that evaluating over many different categories of attacks is essential for accurately determining guardrail performance. Furthermore, guardrails can vary significantly in both memory footprint, computational cost, and extensibility to new attack vectors. Increasing the computation by an order of magnitude to defend against jailbreaks may thus not be a feasible solution in relation to far more lightweight approaches that have been relatively under-explored, and able to satisfy practical deployment constraints. A principal limitation of this work is that we were required to subsample our final evaluation data due to the high computation cost of several defences, and the range of datasets being evaluated.