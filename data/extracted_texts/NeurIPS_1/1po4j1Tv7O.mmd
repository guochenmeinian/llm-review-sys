# Sample-Efficient Constrained Reinforcement Learning with General Parameterization

Washim Uddin Mondal

Department of Electrical Engineering

Indian Institute of Technology Kanpur

Kanpur, UP, India 208016

wmondal@iitk.ac.in

&Vaneet Aggarwal

School of IE and ECE

Purdue University

West Lafayette, IN, USA 47906

vaneet@purdue.edu

###### Abstract

We consider a constrained Markov Decision Problem (CMDP) where the goal of an agent is to maximize the expected discounted sum of rewards over an infinite horizon while ensuring that the expected discounted sum of costs exceeds a certain threshold. Building on the idea of momentum-based acceleration, we develop the Primal-Dual Accelerated Natural Policy Gradient (PD-ANPG) algorithm that ensures an \(\) global optimality gap and \(\) constraint violation with \(}((1-)^{-7}^{-2})\) sample complexity for general parameterized policies where \(\) denotes the discount factor. This improves the state-of-the-art sample complexity in general parameterized CMDPs by a factor of \(((1-)^{-1}^{-2})\) and achieves the theoretical lower bound in \(^{-1}\).

## 1 Introduction

Reinforcement learning (RL) is a framework where an agent repeatedly interacts with an unknown Markovian environment to find a policy that maximizes the expected discounted sum of its observed rewards. Such problems, often modeled via Markov Decision Processes (MDPs), find applications in many areas, including transportation , communication networks , robotics , etc. In many applications, however, the agents must also obey certain constraints. For example, in a food delivery network, the orders must be delivered within a stipulated time window; the marketing decisions of a firm must satisfy its budget constraints, etc. Such constraints are incorporated into RL by introducing a cost function. In constrained MDPs (CMDPs), the agents not only maximize the expected sum of discounted rewards but also ensure that the expected sum of discounted costs does not cross a predefined boundary.

Finding an optimal policy for a CMDP is a challenging problem, especially when the environment, i.e., the state transition function, is unknown. The efficiency of a solution to a CMDP is measured by its sample complexity, which essentially states how many state-transition samples it takes to yield a policy that is \(\) close to the optimal one while also ensuring that the expected sum of discounted costs does not violate the imposed boundary by more than \(\) amount. Many articles in the literature solve the CMDP with an unknown environment. Most of these works, however, focus on the tabular case where the number of states is finite. These solutions cannot be applied to many real-life scenarios where the state space is either large or infinite. To tackle this issue, the concept of policy parameterization must be invoked. Unfortunately, as exhibited in Table 1, only a few works are available on CMDPs with parameterized policies. While, for softmax parameterization, the state-of-the-art (SOTA) sample complexity is \((^{-2})\), the same for the general parameterization is \(}(^{-4})\) which is far from the lower bound \((^{-2})\). It should be noted that the number of parameters needed in softmax parameterization is \((SA)\) where \(S,A\) are the sizes of the state and action spaces of the underlying CMDP. On the other hand, general parameterization uses \( SA\) number of parameters, which makes it appropriatefor dealing with large or infinite states. Given the importance of general parameterization for large state space CMDPs, the following question naturally arises: _"Is it possible to solve CMDPs with general parameterization and achieve a sample complexity better than the SOTA \(}(^{-4})\) bound?"_

In this article, we provide an affirmative answer to the above question. We propose a Primal-Dual-based Accelerated Natural Policy Gradient (PD-ANPG) algorithm to solve \(\)-discounted CMDPs with general parameterization. We theoretically prove that PD-ANPG achieves \(\) optimality gap and \(\) constraint violation with \(}((1-)^{-7}^{-2})\) sample complexity (Theorem 1) that improves the SOTA \(}((1-)^{-8}^{-4})\) sample complexity result of . It closes the gap between the theoretical upper and lower bounds of sample complexity in general parameterized CMDPs (in terms of \(^{-1}\)), which was an open problem for quite some time (see the results of , ,  in Table 1).

### Challenges and Key Insights

Our algorithm builds upon the idea of primal dual-based NPG [7; 4]. However, unlike the previous works, we use accelerated stochastic gradient descent (ASGD) in the inner loop to compute the estimate of the NPG. The improvement in the sample complexity results from two key observations. Firstly, we establish a global-to-local convergence lemma (Lemma 3), which dictates how the global convergence of the Lagrange function is related to the first and second-order estimation error of the NPG. Here, via careful analysis, we show that the first-order term can be written as the expected bias of the NPG estimator (i.e., the difference between the true NPG and the expectation of its estimate). Secondly, we show (Lemma 5 and its subsequent discussion) that the bias of the NPG estimate can be interpreted as the convergence error of an ASGD program with non-stochastic (i.e., deterministic) gradients. These, combined with the ASGD convergence result provided by , lead to a convergence result of the Lagrange function (Corollary 1).

Finally, Theorem 1 segregates the objective and constraint violation rates from Lagrange convergence. Corollary 1 shows that Lagrange convergence error is bounded by an independent function of \(\) (the dual learning rate). One might, hence, be tempted to make \(\) arbitrarily small. However, our analysis shows that although small \(\) leads to better objective convergence, it worsens the constraint violation rate. We demonstrate how to optimally choose \(\) to reach the middle ground which eventually leads us to \(}(^{-2})\) sample complexity.

### Related Works

**Unconstrained RL:** Many algorithms solve MDPs with exact gradients [12; 13; 14; 15; 16]. Moreover, many works use generative models to show either first-order [17; 18; 19; 20; 21] or global convergence [22; 23; 24; 25; 7; 26; 27].

  Algorithm & Sample Complexity & Parameterization \\  PMD-PD  & \((^{-3})\) & Softmax \\  PD-NAC  & \((^{-6})\) & Softmax \\  NPG-PD  & \(((1-)^{-5}^{-2})\) & Softmax \\  CRPO  & \(((1-)^{-7}^{-4})\) & Softmax \\  NPG-PD  & \(((1-)^{-8}^{-6})\) & General \\  CRPO  & \(((1-)^{-13}^{-6})\) & General \\  C-NPG-PDA  & \(}((1-)^{-8}^{-4})\)1 & General \\ 
**PD-ANPG (This Work)** & \(}((1-)^{-7}^{-2})\) & General \\  Lower Bound  & \(((1-)^{-5}^{-2})\) & \(-\) \\  

Table 1: Summary of sample complexity results on CMDP with parameterized policies. The parameter \(\) indicates the discount factor. The dependence of the sample complexities of PMD-PD and PD-NAC on \(\) is not depicted in [9; 10].

**Constrained RL:** The tabular setting is well investigated, and many model-based [28; 29; 30; 31] and model-free [6; 30; 32; 33] algorithms are available in the literature. In comparison, there are relatively fewer works on parameterized policies. Policy mirror descent-primal dual (PMD-PD) algorithm was proposed by  that achieves \((^{-3})\) sample complexity for softmax policies. For the same parameterization,  achieved \((^{-6})\) sample complexity via their proposed Online Primal-Dual Natural Actor-Critic Algorithm. The primal-dual Natural Policy Gradient algorithm suggested by  yields \((^{-2})\) and \((^{-6})\) sample complexities for softmax and general parametrization respectively.  also proposed a primal policy-based algorithm that works for both the softmax and general function approximation cases. The state of the art for the general parameterization is given by , where the \(}(^{-4})\) sample complexity is obtained. This work improves upon this direction to obtain \(}(^{-2})\) sample complexity. The comparisons are summarized in Table 1.

## 2 Formulation

Let us consider a constrained Markov Decision Process (CMDP) characterized by the tuple \(=(,,r,c,P,,)\) where \(\), \(\) denote the state space and the action space respectively, \(r:\) defines the reward function, \(c:[-1,1]\) is the cost function, \(P:()\) indicates the state transition kernel (where \(()\) is the collection of all probability distributions over \(\)), \(\) is the discount factor, and \(()\) is the initial state distribution. Note that the state space \(\), in our setting, can potentially be a compact set of infinite size. However, for simplicity, we assume it to be countable. The action space, \(\), is assumed to be of finite size. The range of the cost function is chosen to be \([-1,1]\), rather than \(\), to ensure that the constraint in our central optimization problem (defined later in (2)) is non-trivial. A policy, \(:()\) is defined as a distribution over the action space for a given state of the environment. For a given policy, \(\), and a state-action pair \((s,a)\), we define the \(Q\)-value associated with \(g\{r,c\}\) as follows.

\[Q_{g}^{}(s,a)_{}[_{t=0}^{}^{t}g (s_{t},a_{t})s_{0}=s,a_{0}=a]\]

where \(_{}\) is the expectation computed over all \(\)-induced trajectories \(\{(s_{t},a_{t})\}_{t=0}^{}\) where \(s_{t+1} P(s_{t},a_{t})\) and \(a_{t}(s_{t})\), \( t\{0,1,\}\). Similarly, the \(V\)-value associated with policy \(\), state \(s\), and \(g\{r,c\}\) is defined below.

\[V_{g}^{}(s)_{}[_{t=0}^{}^{t}g(s _{t},a_{t})s_{0}=s]=_{a}(a|s)Q_{g}^{}(s,a)\]

Below we define the advantage value for a policy \(\), a state-action pair \((s,a)\), and \(g\{r,c\}\).

\[A_{g}^{}(s,a) Q_{g}^{}(s,a)-V_{g}^{}(s)\]

Define a function \(J_{g,}^{}\), \( g\{r,c\}\) as follows.

\[J_{g,}^{}_{s}[V_{g}^{}(s)]=_{s,a}d_{}^{}(s)(a|s)g(s,a)\]

where \(d_{}^{}()\) is the state occupancy measure given by,

\[d_{}^{}(s)=(1-)_{t=0}^{}^{t}(s_{t}=s| s_{0},),\; s\]

Similarly, the state-action occupancy measure is defined as,

\[_{}^{}(s,a)=d_{}^{}(s)(a|s),\;(s,a)  \]

Our goal is to maximize the function \(J_{r,}^{}\) over all policies \(\) while ensuring that \(J_{c,}^{}\) does not lie below a predefined threshold. Without loss of generality, we can formally express this problem as,

\[_{}J_{r,}^{}\;\;\;\;J_{c,}^{} 0 \]

If the state space, \(\), is large or infinite (which is the case in many application scenarios), the policies can no longer be represented in the tabular format; rather, they are indexed by a parameter, \(\). Inthis paper, we assume \(=^{ d}\). Such indexing can be done via, for example, neural networks (NNs). Let \(J_{g,}() J_{g,}^{_{}}\). This allows us to redefine the constrained optimization problem as follows.

\[_{}J_{r,}()\ \ \ \ J_{c,}() 0 \]

We assume the existence of at least one interior point solution of the above optimization. This is also known as Slater condition which can be formally expressed as follows.

**Assumption 1**.: _There exists \(\) such that \(J_{c,}() c_{ slater}\) for some \(c_{ slater}(0,]\)._

## 3 Algorithm

The dual problem associated with the constraint optimization (3) can be written as follows.

\[_{ 0}_{}J_{ L,}(,)\ \ \ J_{ L,}(,) J_{r,}()+ J_{c,}() \]

The function, \(J_{ L,}(,)\) is called the Lagrange function while \(\) is said to be the Lagrange multiplier. The above problem can be solved by iteratively applying the following update rule \( k\{0,,K-1\}\), starting with \((_{0},_{0})\) where \(_{0}\) is arbitrary and \(_{0}=0\).

\[_{k+1} =_{k}+ F_{}(_{k})^{}_{}J_ { L,}(_{k},_{k}) \] \[_{k+1} =_{}[_{k}- J_{c,}(_{ k})] \]

where \(,\) are learning rates, \(_{}\) denotes the projection function onto the set, \([0,_{}]\), and \(\) is the Moore-Penrose pseudoinverse operator. The choice of \(_{}\) will be specified later. Note that the update rule of \(\) is similar to that of the standard policy gradient method except here the learning rate, \(\) is modulated by the inverse of the Fisher matrix, \(F_{}()\) which is defined below.

\[F_{}()_{(s,a)_{}^{_{}}} [_{}_{}(a|s)_{}_{ }(a|s)] \]

where \(\) indicates the outer product. Using a variation of the classical policy gradient theorem , one can obtain the gradient of the Lagrange function as follows.

\[_{}J_{ L,}(,)=H _{}(,), \ \ H_{}(,)_{(s,a)_{}^{_{}}} [A_{ L,}^{_{}}(s,a)_{}_{}(a|s)] \] \[\ A_{ L,}^{_{}}(s,a) A_{r}^{ _{}}(s,a)+ A_{c}^{_{}}(s,a)\]

In most application scenarios, the learner is unaware of the state transition function, \(P\), and thereby, of the advantage function, \(A_{ L,}^{_{}}\) and the occupancy measure, \(_{}^{_{}}\). This makes the exact computation of \(F_{}()\) and \(H_{}(,)\) an impossible task. Fortunately, there is a way to obtain an approximate value of the _natural policy gradient_\(_{,}^{*} F_{}()^{}_{ }J_{ L,}(,)\) that does not require the knowledge of \(P\). Invoking (8), one can prove that \(_{,}^{*}\) is a solution of a quadratic optimization. Formally, we have,

\[_{,}^{*} _{^{ d}}L_{_{}^{_{ }}}(,,), \] \[\ L_{_{}^{_{}}}(,,) _{(s,a)_{}^{_{}}}[ (A_{ L,}^{_{}}(s,a)-^{ T} _{}_{}(a|s))^{2}]\]

The above reformulation opens up the possibility to compute \(_{,}^{*}\) via a gradient descent-type iterative procedure. Observe that the gradient of \(L_{_{}^{_{}}}(,,)\) can be calculated as follows.

\[_{}L_{_{}^{_{}}}(,,)=F_{}( )-H_{}(,) \]

Algorithm 1 describes a procedure to obtain unbiased estimates of this gradient. This is inspired by Algorithm 3 of . Additionally, observe from (6) that the update of the Lagrange variable, \(\) requires the computation of \(J_{c,}()\) which is also difficult to accomplish without having an explicit knowledge about \(P\). Algorithm 1 also provides an unbiased estimation of the above quantity.

Algorithm 1 first samples a horizon length, \(T\), from the geometric distribution with success probability \((1-)\) and executes the CMDP for \(T\) instances following the policy, \(_{}\), starting from a state \(s_{0}\).

```
1:Input: Parameters \((,,,)\), Initial Distribution \(\)
2:\(T(1-)\), \(s_{0}\), \(a_{0}_{}(s_{0})\)
3:for\(j\{0,,T-1\}\)do
4: Execute \(a_{j}\), observe \(s_{j+1} P(s_{j},a_{j})\) and sample \(a_{j+1}_{}(s_{j+1})\)
5:\(_{c,}()_{j=0}^{T}c(s_{j},a_{j}),(,)(s_{T},a_{T})\)
6:\(T(1-)\), \((s_{0},a_{0})(,)\)\(\) Q-function Estimation
7:for\(j\{0,,T-1\}\)do
8: Execute \(a_{j}\), observe \(s_{j+1} P(s_{j},a_{j})\), and sample \(a_{j+1}_{}(s_{j+1})\)
9:for\(g\{r,c\}\)do
10:\(_{g}^{_{}}(,)_{j=0}^{T}g(s_{j},a _{j})\)
11:\(T(1-)\), \(s_{0}\), \(a_{0}_{}(s_{0})\)\(\) V-function Estimation
12:for\(j\{0,,T-1\}\)do
13: Execute \(a_{j}\), observe \(s_{j+1} P(s_{j},a_{j})\), and sample \(a_{j+1}_{}(s_{j+1})\)
14:for\(g\{r,c\}\)do
15:\(_{g}^{_{}}()_{j=0}^{T}g(s_{j},a_{j})\)
16:\(_{g}^{_{}}(,)_{g}^{_{}}( ,)-_{g}^{_{}}()\)
17:\(\) Estimation of Relevant functions
18:\(_{,}^{_{}}(,)_{r}^{_{}}(,)+_{c}^{_{}}(,)\)
19:\(_{}()_{}_{}(|)_{}_{}(|)\)
20:\(_{}(,)_{,}^{_{ }}(,)_{}_{}(|)\)
21:\(\) Gradient Estimate
22:\[_{}L_{_{}^{_{}}}(,,) _{}()-_{}( ,)\]
23:Output:\(_{c,}(),_{}L_{_{}^{_{}}}( ,,)\)
```

**Algorithm 1** Unbiased Sampling

The total cost observed in the resulting trajectory is assigned as the estimate \(_{c,}()\). The state-action pair \((s_{T},a_{T})\) can be assumed to be an arbitrary sample \((,)\) chosen from the occupancy measure \(_{}^{_{}}\). The algorithm then generates a \(_{}\)-induced trajectory of length \(T(1-)\), taking \((,)\) as the starting point. The total reward and cost observed in this trajectory are assigned as \(_{r}^{_{}}(,)\) and \(_{c}^{_{}}(,)\) respectively. Next, another \(_{}\)-induced trajectory of length \(T(1-)\) is generated assuming the state, \(\) as the initiation point. The total reward and cost of this trajectory are assigned as \(_{r}^{_{}}()\) and \(_{r}^{_{}}()\) respectively. For \(g\{r,c\}\), an estimate of the advantage value is computed as \(_{g}^{_{}}(,)=_{g}^{_{}}(,)-_{g}^{_{}}()\). Finally, the estimates of \(F_{}()\) and \(H_{}(,)\) are obtained via (12) and (13) respectively which produces an estimation of the desired gradient in (14). The following Lemma demonstrates that the estimates produced by Algorithm 1 are unbiased.

**Lemma 1**.: _Let \(_{c,}()\), \(_{}L_{_{}^{_{}}}(,,)\) be the estimates produced by Algorithm 1 for a predefined set of parameters \((,,)\). The following equations hold._

\[[_{c,}()]=J_{c,}( )\;\;\;\;[_{}L_{_{}^ {_{}}}(,,),,]= _{}L_{_{}^{_{}}}(,,)\]

In the absence of knowledge about the transition model, \(P\), one can utilize the estimates generated by Algorithm 1 as good proxies for their true values. In particular, one can obtain an approximate value of the natural policy gradient \(_{,}^{*}\) by iteratively minimizing the function \(_{_{}^{_{}}}(,,)\) using the gradient estimate \(_{}L_{_{}^{_{}}}(,,)\). On the other hand, using the estimate \(_{c,}()\), an approximate update equation of the Lagrange parameter can be formed. Algorithm 2 uses these two ideas to obtain a policy that is close to the optimal one.

Algorithm 2 has a nested loop structure. The _outer loop_ runs \(K\) number of times. At a given instance, \(k\), of the outer loop, the policy parameter \(_{k}\) and the Lagrange parameter, \(_{k}\) are updated via (20) and (21). The estimate, \(_{c,}(_{k})\) is computed via Algorithm 1. On the other hand, \(_{k}\), the approximate value of the natural policy gradient \(_{_{k},_{k}}^{*}\) is obtained by iteratively minimizing \(L_{_{}^{*}_{k}}^{*}(,_{k},_{k})\) in \(H\) number of _inner loop_ steps via the Accelerated Stochastic Gradient Descent (ASGD) procedure as stated in . ASGD comprises the iterative updates (15)\(-\)(18) with tunable learning parameters \((,,,)\) followed by a tail-averaging step (19). The gradient estimate utilized in (16) and (18) is obtained via Algorithm 1. It is worth mentioning that existing NPG algorithms such as that given in  typically apply the SGD, rather than the ASGD procedure, to obtain \(_{k}\). The difference between these subroutines is that while SGD uses only the current gradient estimate to update \(_{k}\), ASGD considers the contribution of all previous gradient estimates (momentum) using its convoluted iteration and tail-averaging steps.

```
1:Input: Parameters \((_{0},_{0})\), Distribution \(\), Run-time Parameters \(K,H\), Learning Parameters \(,,,,,\)
2:for\(k\{0,,K-1\}\)do\(\) Outer Loop
3:\(_{0},_{0}\)
4:for\(h\{0,,H-1\}\)do\(\) Inner Loop
5:\(\) Accelerated Stochastic Gradient Descent
6:\(_{h}_{h}+(1-)_{h}\)
7:\(_{}L_{_{}^{*}_{k}}^{*_{k}}( ,_{k},_{k})_{=_{h}}\) (Algorithm 1) \[_{h+1}_{h}-\] (16) \[_{h}_{h}+(1-)_{h}\] (17) \[_{h+1}_{h}-\] (18)
8: Tail Averaging: \[_{k}_{<h H}_{h}\] (19)
9: Obtain \(_{c,}(_{k})\) via Algorithm 1.
10: Parameter Updates: \[_{k+1}_{k}+_{k}\] (20) \[_{k+1}_{}[_{k}- _{c,}(_{k})]\] (21)
11:Output:\(\{_{k}\}_{k=0}^{K-1}\)
```

**Algorithm 2** Primal-Dual Accelerated Natural Policy Gradient (PD-ANPG)

## 4 Analysis

Our goal in this section is to characterize the rate of convergence of the objective function and the constraint violation if policy parameters are generated via Algorithm 2. We start by stating a few assumptions needed for the analysis.

**Assumption 2**.: _The log-likelihood function is \(G\)-Lipschitz and \(B\)-smooth where \(B,G>0\). Formally, the following relations hold \(,_{1},_{2}\), and \((s,a)\)._

\[\|_{}_{}(a|s)\| G,\|_{ }_{_{1}}(a|s)-_{}_{_{2}}(a|s)\|  B\|_{1}-_{2}\|\]

**Remark 1**.: _Assumption 2 is commonly applied in proving convergence guarantees of policy gradient-type algorithms [35; 12; 7]. This assumption is obeyed by many widely used policy classes such as the class of neural networks with bounded weights._Assumption 2 implies the boundedness of the gradient of the Lagrange function. This can be formally expressed as follows.

**Lemma 2**.: _If Assumption 2 holds, then the following inequality is true \(\) and \(\)._

\[\|_{}J_{,}(,)\|})}{(1-)^{2}}\]

Proof.: Statement (a) can be proven using (8) along with Assumption 2 and the facts that \(|A^{_{}}_{,}(s,a)|\) is bounded by \((1+)/(1-)\) and \(_{}\), \(\). 

The result established by Lemma 2 will be pivotal in our further analysis.

**Assumption 3**.: _The compatible function approximation error defined in (9) satisfies the inequality \(L_{^{*}_{}}(^{*}_{,},,) _{}/2\), \(\) and \(\) where \(^{*}\) is a solution to the original constrained optimization problem (2) and \(^{*}_{,}\) is defined in (9). The term \(_{}\) is a non-negative constant. The factor \(2\) is used for notational convenience._

**Remark 2**.: _The term \(_{}\) quantifies the expressivity of the parameterized policy class. For example, if the parameterization is complete i.e., includes all possible policies (such as in direct or softmax parameterization), then \(_{}=0\). A similar result can be proven for linear MDPs . For incomplete policy classes, we have \(_{}>0\). However, if the class is sufficiently rich (such as neural networks with a large number of parameters), \(_{}\) can be assumed to be negligibly small ._

**Assumption 4**.: _There exists a positive constant \(_{F}\) such that \(F_{}()-_{F}I_{}\) is positive semidefinite i.e., \(F_{}()_{F}I_{}\), \(\) where \(I_{d}\) is a \(\) identity matrix and \(F_{}()\) is defined in (7)._

The property of the policy classes laid out in Assumption 4 is called Fisher Non-Degeneracy (FND) which essentially ensures that \(\), the Fisher matrix \(F_{}()\) is away from the zero matrix by a certain amount. Observe that the Hessian of the function, \(l_{,}() L_{^{*}_{}}(,,)\) is \(F_{}()\). Therefore, Assumption 4 also indicates that \(l_{,}\) is \(_{F}\)-strongly convex. This assumption is commonly applied in analyzing policy-gradient algorithms . Assumption 4 also ensures that the matrix \(F_{}()\) is invertible, which, in turn, implies the uniqueness of the maximizer \(^{*}_{,}=_{^{d}}l_{, }()\).  describes a concrete set of policies that obeys Assumption \(2-4\).

### Local-to-Global Convergence Lemma

Recall that our goal is to establish the global convergence rates. Lemma 3 (stated below) is the first step in that direction. Specifically, it demonstrates how the average optimality gap of the Lagrange function can be bounded by the first and second-order error of the gradient estimates.

**Lemma 3**.: _If the parameters \(\{_{k},_{k}\}_{k=0}^{K-1}\) are updated via (20) and (21) and assumptions \(2-4\) hold, then the following inequality holds for any \(K\)._

\[_{k=0}^{K-1}J_{,} (^{*},_{k}) -J_{,}(_{k},_{k})}}+_{k=0}^{K-1}\|( [_{k}|_{k},_{k}]-^{*}_{k})\| \] \[+_{k=0}^{K-1}\|_{k}\|^{2}+ _{s d^{*}_{}}[KL(^{*}(|s)\|_{ _{0}}(|s))]\]

_where \(^{*}_{k}^{*}_{_{k},_{k}}\), \(^{*}_{_{k},_{k}}\) is the natural policy gradient defined in (9), and \(^{*}\) is the solution to the constrained optimization (2). Finally, \(_{k}\) is the approximation of \(^{*}_{k}\) given by (19) and \(KL(\|)\) is the KL-divergence._

Note the presence of the term, \(_{}\) in (22). It shows that due to the incompleteness of the parameterized policy class, the average optimality error cannot be made arbitrarily small. It is worth mentioning that many existing CMDP analyses (such as ) follow a path similar to that of Lemma 3. However, while the first order term in those works turns out to be \(\|_{k}-^{*}_{k}\|\), we improved it to \(\|[_{k}|_{k},_{k}]-^{*}_{k}\|\). Such seemingly insignificant improvement has important ramifications for our analysis as explained later in the paper. The second order term in (22) can be expanded as,

\[_{k=0}^{K-1}\|_{k}\|^{2}& _{k=0}^{K-1}\|_{k}-_{k}^{*}\|^{2}+ _{k=0}^{K-1}\|_{k}^{*}\|^{2}\\ &}{{}}_{k=0}^{K-1} \|_{k}-_{k}^{*}\|^{2}+^{2}K}_{k=0}^{ K-1}\|_{}J_{,}(_{k},_{k})\|^{2}\\ &}{{}}_{k=0}^{K-1} \|_{k}-_{k}^{*}\|^{2}+(1+_{})^{2} }{_{F}^{2}(1-)^{4}} \]

where (a) utilises \(_{k}^{*}=F_{}(_{k})^{}_{}J_{, }(_{k},_{k})\) and Assumption 4. The second inequality applies Lemma 2 together with the fact that \(_{k}\). Our next subsection provides a bound on \(\|_{k}-_{k}^{*}\|^{2}\) and the first order term \(\|[_{k}|_{k},_{k}]-_{k}^{*}\|\).

### Local Convergence of the Natural Policy Gradient

To deliver the promised bounds, we first provide some characterization of the gradient estimate.

**Lemma 4**.: _Let \(_{}L_{_{}^{*}}(,,)\) be the estimate produced by Algorithm 1. Under assumptions 2 and 4, the following semidefinite inequality holds for any \(\) and \(\)._

\[[_{}L_{_{}^{*}}(_{, }^{*},,)_{}L_{_{}^{* }}(_{,}^{*},,)]^{2}F_{}()\]

_where \(_{,}^{*}\), \(F_{}()\) are given by (9) and (7) respectively and \(^{2}\) is defined below._

\[^{2}}[}{_{F}^{2}}+ 32](1+_{})^{2} \]

The term \(^{2}\) defined in Lemma 4 can be described as the scaled variance of the gradient estimate, \(_{}L_{_{}^{*}}(,,)\). Note that if the estimates were non-stochastic (i.e., deterministic), we would have \(^{2}=0\) since \(_{}L_{_{}^{*}}(_{,}^{*},, )=0\). The last equation can be proved using the definition of \(_{,}^{*}\) given in (9), the gradient expression provided in (10), and observing that the Fisher matrix, \(F_{}()\) is invertible due to Assumption 4. The above information is crucial in bounding the first-order error, as stated in the following lemma.

**Lemma 5**.: _If assumptions 2 and 4 hold, then the following relations are satisfied \( k\{0,,K-1\}\) with learning rates \(=G^{2}}{_{F}+3G^{2}}\), \(=}{9G^{2}}\), \(=G^{2}}\), and \(=}\) provided that the inner loop length of Algorithm 2 obeys \(H>}{_{F}}(}}{_{F}})\) for some universal constant, \(\)._

\[\|_{k}-_{k}^{*}\|^{2} 22}{_{F}H}+C(-}{20G^{2}}H) [)^{2}}{_{F}(1-)^{4}}], \] \[\|[_{k}|_{k},_{k} ]-_{k}^{*}\|(-}{40G^{2}}H) [}{}(1-)^{2}}] \]

_where \(C\) denotes a universal constant, \(_{k}\) is given by (19), and \(^{2}\) is defined in (24)._

The first bound (25) is a consequence of Lemma 4 and the ASGD convergence result provided in  (Corollary 2). To gain intuition about the second result (26), note that by taking the conditional expectation \([|_{k},_{k}]\) on both sides of the ASGD iterations (15)\(-\)(18), and applying the unbiasedness of the gradient estimate (Lemma 1) we obtain the following \( h\{0,,H-1\}\).

\[&}_{h}=}_{h}+(1- )}_{h},\\ &}_{h+1}=}_{h}-_{ }L_{_{}^{*_{k}}}(,_{k},_{k})_{ =}_{h}}\\ &}_{h}=}_{h}+(1-)}_{h},\\ &}_{h+1}=}_{h}-_{}L _{_{}^{*_{k}}}(,_{k},_{k})_{=}_{h}} \]where \(_{h}=[l_{h}|_{k},_{k}]\), \(l\{,,,\}\). Moreover, taking conditional expectation on both sides of the tail averaging process (19), we arrive at the following.

\[_{k}[_{k}_{k},_{ k}]=_{<h H}}_{h} \]

Note that the steps (27)\(-\)(28) resemble the iterative updates of a deterministic ASGD. This allows us to obtain \(\|_{k}-_{k}^{*}\|\) by substituting \(^{2}=0\) in (25) and applying the Cauchy-Schwarz inequality.

### Global Convergence of the Lagrange

Combining Lemma 3, (23) and using the expected gradient errors provided by Lemma 5, we bound the average Lagrange optimality gap as a function of tunable parameters \(H\) and \(K\) as stated in the following corollary.

**Corollary 1**.: _Consider the same setup and the choice of parameters described in Lemma 3\(-\)5. The following inequality holds if assumptions 2\(-\)4 are met._

\[_{k=0}^{K-1}J_{,}( ^{*},_{k})-J_{,}(_{k},_{k}) }}+[(1+_{}) }{}(1-)^{2}}](-}{40G^{2}}H)\] \[+B[}{_{F}H}+(- }{20G^{2}}H)[})^{2}}{ _{F}(1-)^{4}}]+(1+_{})^{2}}{_{ F}^{2}(1-)^{4}}]\] \[+_{s d_{}^{*}}[KL(^{*}( |s)\|_{_{0}}(|s))] \]

Corollary 1 bounds the optimality error of the Lagrange function as \((}}+(-C_{0}H)++ )\) where \(C_{0}\) is some problem specific constant. Interestingly, the dual learning parameter, \(\) does not appear in this bound. However, the next section shows that \(\) plays a pivotal role in deciding the objective and constraint violation rates.

### Decoupling the Objective and the Constraint Violation Rates

The goal of the following theorem is to choose optimal values of the tunable parameters and decouple the objective and constraint violation rates from the Lagrange convergence result given in (29).

**Theorem 1**.: _Consider the same setup and the choice of parameters given in Lemma 3\(-\)5. Assume \(=(1-)^{2}(1+_{})^{-1}/\), \(=_{}(1-)/\), and \(_{}=2/[(1-)c_{}]\). For sufficiently small \(>0\), the following inequalities hold_

\[_{k=0}^{K-1}J_{r,}^{*^{*}}-J _{r,}(_{k})}}+, \] \[[_{k=0}^{K-1}-J_{c,}(_{k })](1-)c_{}}}+\]

_whenever assumptions 1\(-\)4 are met, \(H=((^{-1}))\) and \(K=((1-)^{-6}^{-2})\). Therefore, the sample complexity to ensure (30) is \(((1-)^{-1}HK)=}((1-)^{-7}^{-2})\). It is to be clarified that the \((1-)^{-1}\) factor in the sample complexity calculation appears due to the fact that it requires \(((1-)^{-1})\) samples on an average to obtain a gradient estimate via Algorithm 1._

Theorem 1 dictates that, with appropriate choice of the parameters, the rate of convergence of the objective and that of constraint violation can be bounded as \((}}+)\) with \(}((1-)^{-7}^{-2})\) samples. This beats the SOTA \(}(^{-4})\) sample complexity of  and achieves the theoretical lower bound. Our derived sample complexity is also dependent on \(c_{}\). However, this is not explicitly mentioned in Theorem 1. Interested readers can find such details in the appendix.

**Remark 3**.: _Note the importance of the nested expectation in the first-order term \(\|[_{k}|_{k},_{k}]-_{k}^{*}\|\) in Lemma 3. Lemma 5 bounds this term as \(((-C_{0}H))\) where \(C_{0}\) is a problem dependent constant. Other terms in the Lagrange optimality bound (Lemma 3) can be expressed as \((+)\). Moreover, following the proof of Theorem 1, one sees that decoupling the objective optimality error and constraint violation bounds incurs additional \((+)\) terms. Choosing \(,\) as prescribed in Theorem 1, makes both the objective and constraint violation errors as \((}}+(-C_{0}H)+K^{-0.5})\). This allows us to take \(H=}(1)\) and \(K=(^{-2})\), leading to \(}(^{-2})\) sample complexity. Had the first-order term been \(\|_{k}-_{k}^{*}\|\), it would have resulted in \((}}+H^{-0.5}+K^{-0.5})\) objective and constraint violation errors which would have lead to \((^{-4})\) sample complexity._

## 5 Conclusions and Limitations

This paper considers the problem of learning a CMDP where the goal is to maximize the objective value function while guaranteeing that the cost value exceeds a predefined threshold. We propose an acceleration-based primal-dual natural policy gradient algorithm that ensures \(\) optimality gap and \(\) constraint violation with \(}(^{-2})\) sample complexity. This improves upon the previous state-of-the-art sample complexity of \((^{-4})\) and achieves the theoretical lower bound. Future works include applying the idea of acceleration-based NPG to improve sample complexities in other related domains of constrained reinforcement learning, e.g., non-linear CMDP, average reward CMDP, etc.