# Bayesian Extensive-Rank Matrix Factorization with Rotational Invariant Priors

Farzad Pourkamali & Nicolas Macris

School of Computer and Communication Sciences,

Ecole Polytechnique Federale de Lausanne

{farzad.pourkamali,nicolas.macris}@epfl.ch

###### Abstract

We consider a statistical model for matrix factorization in a regime where the rank of the two hidden matrix factors grows linearly with their dimension and their product is corrupted by additive noise. Despite various approaches, statistical and algorithmic limits of such problems have remained elusive. We study a Bayesian setting with the assumptions that (a) one of the matrix factors is symmetric, (b) both factors as well as the additive noise have rotational invariant priors, (c) the priors are known to the statistician. We derive analytical formulas for _Rotation Invariant Estimators_ to reconstruct the two matrix factors, and conjecture that these are optimal in the large-dimension limit, in the sense that they minimize the average mean-square-error. We provide numerical checks which confirm the optimality conjecture when confronted to _Oracle Estimators_ which are optimal by definition, but involve the ground-truth. Our derivation relies on a combination of tools, namely random matrix theory transforms, spherical integral formulas, and the replica method from statistical mechanics.

## 1 Introduction

Matrix factorization (MF) is the problem of reconstructing two matrices \(\) and \(\) from the noisy observations of their product. Applications in signal processing and machine learning abound, such as for example dimensionality reduction , sparse coding , representation learning , robust principal components analysis , blind source separation , or matrix completion .

In this work we approach the problem from a Bayesian perspective and assume that an observation or data matrix \(=+\) is given to a statistician who knows the prior distributions of \(\) and \(\) as well as the prior of the additive noise matrix \(\) and the signal-to-noise ratio \(>0\). The task of the statistician is to construct estimators \(_{}()\), \(_{Y}()\) for the matrix factors \(\), \(\), that ideally, minimize the average mean-square-error (MSE) \(\|-_{}()\|_{}^{2}\) and \(\|-_{}()\|_{}^{2}\) (\(\|.\|_{}\) the Frobenius norm and \(\) the expectation w.r.t \(,,\)). We consider priors which are rotation invariant for all three matrices \(\), \(\), \(\) and for \(\) we furthermore impose that it is square and symmetric. These matrix ensembles are defined precisely in section 2.1, but the reader can keep in mind the examples of Wigner or Wishart matrices for \(\), and general Gaussian \(\) and \(\) with i.i.d elements. We look at the asymptotic regime where all matrix dimensions and ranks tend to infinity at the same speed. We remark that the usual "rotation ambiguity" occuring in MF is not present because we impose that at least one of the two matrix factors is symmetric. We also remark that MF is different (and more difficult) than matrix denoising which would consist in constructing an estimator \(_{}()\) for the signal as a whole by minimizing \(\|-_{}()\|_{}^{2}\).

The rotation invariance of the model implies that the estimators minimizing the MSE belong to the class of rotation invariant estimators (RIE). RIEs are matrix estimators which have the same singular vectors (or eigenvectors) as the observation or data matrix. These estimators have beenproposed for matrix _denoising_ problems (see references  for covariance estimation,  for cross-covariance estimation, and  for extensions to rectangular matrices). For the present MF model, we derive optimal estimators (minimizing the MSE) that belong to the RIE class and can be computed explicitly in the large dimensional limit from the observation matrix and the knowledge of the priors. We propose:

1. an explicit RIE to estimate \(\), which requires the knowledge of the priors of _both_\(,\) and of the noise \(\). Moreover, under the assumption that \(\) is positive-semi-definite, a _sub-optimal_ RIE can be derived which _does not_ require any prior on \(\).
2. an explicit RIE to estimate \(\), which requires the knowledge of the priors of the noise \(\) and \(\)_only_ (the prior of \(\) is not required).
3. combined with the singular value decomposition (SVD) of the observation matrix, our explicit RIEs provide a spectral algorithm to reconstruct both factors \(\) and \(\).

The derivation of the proposed estimators relies on the replica method from statistical mechanics combined with techniques from random matrix theory and finite-rank spherical integrals . Although the replica method is not rigorous and involves concentration assumptions, the derivation is entirely analytical and suggests that the estimators are optimal in the limit of large dimensions. This is corroborated by numerical calculations comparing our explicit RIEs with Oracle Estimators which are optimal by definition and involve the ground-truth matrices.

### Related literature and discussion

When the matrices \(\) and \(\) are assumed to have _low-rank_ compared to their dimension, the mathematical theory of MF has enjoyed much progress under various settings (Bayesian, spectral, algorithmic) and fundamental information theoretical and algorithmic limits have been rigorously derived. The behavior of eigenvalues/singular values and eigenvector/singular vectors of finite-rank perturbations of a Gaussian matrix is studied in  which leads to spectral estimators when the noise matrix is Gaussian distributed. For the case of factorized prior, and Gaussian noise, closed form expressions have been established for the asymptotic Bayes-optimal estimation error , and iterative algorithms based on approximate message passing has been proposed . The low-rank matrix denoising problem has been addressed in various other settings, such as structured noise matrix , mismatched estimation problem , and estimation in the regime with diverging aspect-ratio of matrices .

In extensive-rank regimes, when the rank grows like the matrix dimensions, despite various attempts there is no solid theory of MF. One approach is based on Approximate Message Passing (AMP) methods developed in . Despite acceptable performance in practical settings , as pointed out in  the AMP algorithms developed in these works are (theoretically) sub-optimal. Other approaches rooted in statistical physics have been considered in  but have not led to explicit reconstructions of matrix factors or algorithms. A practical probabilistic approach to MF problem is based on variational Bayesian approximations , in which one tries to approximate the posterior distribution with proper distribution. In  it is shown that under Gaussian priors, the solution to the MF problem is a reweighted SVD of the observation matrix. We point out here that these estimators can be seen as a RIE and therefore there seems to be a rather close relation between the RIE studied here and the variational Bayesian approach. This also suggests that adapting RIEs to real data is an interesting direction for future research. Finally, let us also mention optimization approaches where one constructs estimators by following a gradient flow (or gradient descent) trajectory of a training loss of the type \(\|-\|_{ F}^{2}+\) (see  for analysis in rotation invariant models). Benchmarking these various other algorithmic approaches against our explicit RIEs (conjectured to be optimal) is outside the scope of this work and is left for future work.

Constraints such as sparsity or non-negativity of the matrix entries which have important applications  are not covered by our theory. Despite this drawback, we believe that the proposed estimators are important both for theoretical and practical purposes. Even in non-rotation invariant problems our explicit RIEs may serve as sub-optimal estimators, and as we show in an example they can be used as a "warmed-up" spectral initialization for more efficient algorithms (see for example  for related ideas in other contexts). The methodology developed here may open up the way to further analysis in inference and learning problems perhaps also in the context of neural networks where extensive rank weight matrices must be estimated.

### Organization and notations

In section 2, we introduce the precise MF model, general class of RIEs, and the Oracle estimators. In section 3, we present the explicit RIEs (and algorithm) to estimate \(\) and \(\). We provide the numerical examples and calculations in section 4. In section 5, we sketch the derivation of RIE for \(\), while the one for \(\) is similar and deferred to the appendices.

The following notations are used throughout. For a vector \(^{N}\) we denote by \(^{N M}\) a matrix constructed as \(=[_{N}&_{N(M-N)} ]\) with \(_{N}^{N N}\) a diagonal matrix with diagonal \(\). The same notations will also be used for the vector \(\) and the corresponding matrix \(\) and. For a sequence of non-symmetric matrices \(\) of growing size, we denote the limiting empirical singular value distribution (ESD) by \(_{A}\), and the limiting empirical eigenvalue distribution of \(^{}\) by \(_{A}\). For a sequence of symmetric matrices \(\) of growing size, we denote the limiting empirical eigenvalue distribution by \(_{B}\), and the limiting eigenvalue distribution of \(^{2}\) by \(_{B^{2}}\).

## 2 Matrix factorization model and rotation invariant estimators

### Matrix factorization model

Let \(=^{}^{N N}\) a symmetric matrix distributed according to a rotationally invariant prior \(P_{X}()\), i.e., for any orthogonal matrix \(^{N N}\) we have \(P_{X}(^{})=P_{X}()\). Let also \(^{N M}\) be distributed according to a bi-rotationally invariant prior \(P_{Y}()\), i.e. for any orthogonal matrices \(^{N N},^{M M}\) we have \(P_{Y}(^{})=P_{Y}()\). We observe the data matrix \(^{N M}\),

\[=+ \]

where \(^{N M}\) is also bi-rotationally invariant distributed, and \(_{+}\) is proportional to the signal-to-noise-ratio (SNR). The goal is to recover _both factors_\(\) and \(\) from the data matrix \(\). For definiteness, we consider the regime \(M N\) with aspect ratio \(N/M(0,1]\) as \(N\). The case of \(>1\) can be analyzed in the same manner and is presented in appendix F. Furthermore, we assume that the entries of \(,\) and \(\) are of the order \(O(}{{}})\). This scaling is such that the eigenvalues of \(\) and singular values of \(\), \(\) and \(\) are of the order \(O(1)\) as \(N\).

**Assumption 1**.: _The empirical eigenvalue distribution of \(\) converge weakly to measure \(_{X}\), and the ESD of \(,\) converge weakly to measures \(_{Y},_{W}\) with bounded support on the real line. Moreover, these measures are known to the statistician. He can deduce (in principle) these measures from the priors on \(,,\)._

**Remark 1**.: _In a general formulation of matrix factorization the hidden matrices have dimensions \(^{N H},^{H M}\), and in the Bayesian framework with bi-rotational invariant priors for both factors, the optimal estimators are trivially the zero matrix. Indeed, from bi-rotational invariance we have \(P_{X}(-)=P_{X}()\), \(P_{Y}(-)=P_{Y}()\), which implies that the Bayesian estimate is zero. Here, by imposing that \(^{N N}\) is symmetric and \(P_{X}(^{})=P_{X}()\), we can break this symmetry and find non-trivial estimators. This is due to the fact that the map \(-\) cannot be realized as a (real) orthogonal transformation, so \(P_{X}(-)=P_{X}()\) does not hold in general (various examples are given in section 4 and appendices). Of course, if the prior is even, e.g. Wigner ensemble, again the Bayesian posterior estimate is trivially zero for both factors. As we will see our RIEs are consistent with these observations._

### Rotation invariant estimators

To recover matrices \(,\) from \(\), we consider two denoising problems. One is recovering \(\) by treating both \(,\) as "noise" matrices, and the other is estimating \(\) by treating \(,\) as "noise". As will become clear the procedure is not iterative, and the two denoising problems are solved independently and simultaneously. In the following, for each of these two problems, we introduce two rotation invariant classes of estimators and discuss their optimum _Oracle_ estimators. We then provide an explicit construction and algorithm for RIEs which we conjecture have the optimum performance of Oracle estimators in the large \(N\) limit.

#### 2.2.1 RIE class for \(\)

Consider the SVD of \(=_{S}_{S}^{}\), where \(_{S}^{N N}\), \(_{S}^{M M}\) are orthogonal, and \(^{N M}\) is a diagonal matrix with singular values of \(\) on its diagonal, \((_{i})_{1 i N}\). A rotational invaraint estimator for \(\) is denoted \(_{X}()\), and is constructed as:

\[_{X}()=_{S}(_{x1},,_{xN})\, _{S}^{} \]

where \(_{x1},,_{xN}\) are the eigenvalues of the estimator.

First, we derive an _Oracle estimator_ by minimizing the squared error \(-_{X}()_{}^{2}\) for a given instance, over the RIE class or equivalently over the choice of the eigenvalues \((_{xi})_{1 i N}\). Let the eigen-decomposition of \(\) be \(=_{i=1}^{N}_{i}\,_{i}_{i}^{}\) with \(_{i}^{N}\) eigenvectors of \(\). The error can be expanded as:

\[-_{X}()_{}^{2}=_{i=1}^{N}_{i}^{2}+_{i=1}^{N}_{xi}^{2}-_{i=1}^{N}_{xi}_{j=1}^{N}_{j}(_{i}^{ }_{j})^{2}\]

where \(_{i}\)'s are columns of \(_{S}\). Minimizing over \(_{xi}\)'s, we find the optimum among the RIE class:

\[_{X}^{*}()=_{i=1}^{N}_{xi}^{*}\,_{i}_{i}^{ },_{xi}^{*}=_{j=1}^{N}_{j}(_{i}^{ }_{j})^{2}=_{i}^{}_{i} \]

Expression (3) defines the Oracle estimator which requires the knowledge of signal matrix \(\). Surprisingly, in the large \(N\) limit, the optimal eigenvalues \((_{xi}^{*})_{1 i N}\) can be computed from the observation matrix and knowledge of the measures \(_{X},_{Y},_{W}\). In the next section, we show that this leads to an _explicitly computable_ (or algorithmic) RIE, which we conjecture to be optimal as \(N\), in the sense that its performance matches the one of the Oracle estimator.

Now we remark that the Oracle estimator is not only optimal within the rotation invariant class but is also Bayesian optimal. From the Bayesian estimation point of view, one wishes to minimize the average mean squared error (MSE) \(_{}}- }()_{}^{2}\), where the expectation is over \(,,\), and \(}()\) is an estimator of \(\). The MSE is minimized for \(}^{*}()=[|]\) which is the posterior mean. Therefore, the posterior mean estimator has the minimum MSE (MMSE) among all possible estimators, in particular \(_{}^{*}}_{_{X}^{*}}\) for any \(N\). In appendix A.1, we show that, for rotational invariant priors, the posterior mean estimator is inside the RIE class. Thus, since \(_{X}^{*}()\) is optimum among the RIE class \(_{_{X}^{*}}_{}^{*}}\). Therefore, we conclude that the Oracle estimator (3) is Bayesian optimal in the sense that \(_{_{X}^{*}}=_{}^{*}}= \).

#### 2.2.2 RIE class for \(\)

Estimators for \(\) from the rotation invariant class are denoted \(_{Y}()\), and are constructed as:

\[_{Y}()=_{S}[( _{y1},,_{yN})\ \ _{N(M-N)}]_{S}^{} \]

where \(_{y1},,_{yN}\) are the singular values of the estimator.

Let the SVD of \(\) be \(=_{i=1}^{N}_{i}\,_{i}^{(l)}\,_{i}^{(r)}\) with \(_{i}^{(l)}^{N}\), \(_{i}^{(r)}^{M}\) the left and right singular vectors of \(\). To derive an _Oracle estimator_, we proceed as above. Expanding the error, we have:

\[-_{Y}()_{}^{2}=_{i=1}^{N}_{i}^{2}+_{i=1}^{N}_{yi}^{2}-_{i=1}^{N}_{yi}_{j=1}^{N}_{j}(_{i}^{ }_{j}^{(l)})_{i}^{}_{j}^{(r)}\]

where \(_{i}\)'s are columns of \(_{S}\). Minimizing over \(_{y_{i}}\)'s, we find the optimum among the RIE class:

\[_{Y}^{*}()=_{i=1}^{N}_{y_{i}}^{*}\,_{i}_{i}^{ },_{y_{i}}^{*}=_{j=1}^{N}_{j}(_{i}^{ }_{j}^{(l)})_{i}^{}_{j}^{(r) }=_{i}^{}_{i} \]

Expression (5) defines the Oracle estimator which requires the knowledge of signal matrix \(\). Like for the case of \(\), in the large \(N\) limit we can derive the optimal singular values \((_{y_{i}}^{*})_{1 i N}\) in termsof the singular values of observation matrix and knowledge of the measures \(_{X},_{W}\). This leads to an _explicitly computable_ (or algorithmic) RIE, which is conjectured to be optimal as \(N\), in the sense that it has the same performance as the Oracle estimator. Note that unlike the estimator for \(\), we do not need the knowledge of \(_{Y}\).

In appendix A.2, we show that for bi-rotationally invariant priors the posterior mean estimator \(}^{*}()=[|]\) belongs to the RIE class, which (by similar arguments to the case of \(\)) implies that the Oracle estimator (5) is Bayesian optimal.

## 3 Algorithmic RIEs for the matrix factors

In this section, we present our explicit RIEs for \(,\) and the corresponding algorithm. We conjecture that their performance matches the one of Oracles estimators in the large \(N\) limit and they are therefore Bayesian optimal in this limit. Let us first give a brief reminder on useful transforms in random matrix theory.

### Preliminaries on transforms in random matrix theory

For a probability density function \((x)\) on \(\), the _Stieltjes_ (or _Cauchy_) transform is defined as

\[_{}(z)=_{}(x)\,dx()$}\]

By Plemelj formulae we have for \(x\),

\[_{ 0^{+}}_{}(x-)=[](x)+(x) \]

with \([](x)=_{}dt\) the _Hilbert_ transform of \(\) (here \(\) stands for "principal value"). Denoting the inverse of \(_{}(z)\) by \(_{}^{-1}(z)\), the _R-transform_ of \(\) is defined as :

\[_{}(z)=_{}^{-1}(z)-\]

For a probability density function \(\) with support contained in\([-K,K]\) with \(K>0\), we define a generating function of (even) moments \(_{}:[0,K^{-2}]_{+}\) as \(_{}(z)=z}(t)\,dt-1\). For \((0,1]\), define \(T^{()}(z)=( z+1)(z+1)\), and \(_{}^{()}(z)=zT^{()}_{}(z)\). The _rectangular R-transform_ with aspect ratio \(\) is defined as :

\[_{}^{()}(z)=T^{()}{}^{-1}_{}^{()}}^{-1}(z)}\]

### Explicit RIE for \(\)

The RIE for \(\) is constructed as \(_{X}^{*}}()=_{i=1}^{N}_{x_{i}}^{*}}}^{}\) with eigenvalues \(_{x_{i}}^{*}_{1 i N}\) :

\[_{x_{i}}^{*}=_{S}(_{i})}\, \,_{z_{i}-i0^{+}}\,} _{_{X}}}{_{3}}} +_{_{X}}-}{_{3}}} } \]

where \(_{i}\) is the \(i\)-th singular value of \(\), \(_{S}\) is the symmetrized limiting ESD of \(\), and

\[_{1}=_{_{S}}(z)}_{}^{( )}_{_{S}}(z)_{_{S}}(z)+ \]

and \(_{3}\) satisfies 1:

\[(z-_{1})_{_{S}}(z)-1=_{}^{()} }_{_{S}}(z)+(z-_{1})_{_{S}}(z)-1  \]

**Remark 2**.: _If \(_{X}\) is a symmetric measure, \(_{X}(x)=_{X}(-x)\), then \(_{_{X}}(-z)=-_{_{X}}(z)\). This implies that the optimal eigenvalues \(_{x_{i}}^{*}_{1 i N}\) in (7) are all zero, and \(_{X}^{*}}()=\), see figure 4._

#### 3.2.1 An estimator for \(^{2}\)

It is interesting to note that we can construct a RIE for \(^{2}\) as \(_{X^{2}}}()=_{i=1}^{N}i}}_{i }_{i}^{}\) with eigenvalues \(i}}_{1 i N}\):

\[i}}=_{S}(_{i})} _{z_{i}-i0^{+}}}{ _{3}}_{_{S}}(z)-} \]

with \(_{1},_{3}\) as in (8), (9). Note that, \(_{1},_{3}\) can be evaluated using the observation matrix and the knowledge of \(_{Y},_{W}\), and therefore this time the statistician _does not need to know the prior of \(\)_. Furthermore, assuming that \(\) is positive semi-definite (PSD), we can construct a sub-optimal RIE for \(\) by using \(i}}}\) for the eigenvalues of the estimator.

#### 3.2.2 Case of Gaussian \(,\)

If \(\), \(\) have i.i.d. Gaussian entries with variance \(}{{N}}\), then \(_{_{Y}}^{()}(z)=_{_{W}}^{()}(z)= {{z}}{{}}\). Consequently, \(_{1},_{3}\) can easily be computed to be \(_{1}=_{3}=_{_{S}}(z)+}{{(  z)}}\), thus the estimator (7) can be evaluated from the observation matrix. In particular, the estimator (10) simplifies to:

\[i}}=[\ -1+^{2} _{S}(_{i})^{2}+[_{S}](_{i})+ }^{2}}] \]

### Explicit RIE for \(\)

Our explicit RIE for \(\) is constructed as \(_{Y}^{*}}()=_{i=1}^{N}}}_ {i}_{i}^{}\) with singular values \(}}_{1 i N}\):

\[}^{*}}=}_{S}( _{i})}_{z_{i}-i0^{+}}q_{4} \]

where \(_{i}\) is the \(i\)-th singular value of \(\), and \(q_{4}\) is the solution to the following system of equations 2:

\[_{1}=_{_{W}}^{()}(q_{1}q_{2})}{q_ {1}}+}{q_{1}}}_{_{X}} q_{4}+q_{3}}-_{_{X}}q_{4}-q_{3}} \\ _{4}=_{_{X}}q_{4}+q_{3 }}+_{_{X}}q_{4}-q_{3}}\\ q_{1}=_{_{S}}(z), q_{2}=_{_{S }}(z)+(1-)\\ q_{3}=)^{2}}{_{4}^{2}}_{_{S}}(z)- }{_{4}^{2}}, q_{4}=}{_{4}} _{_{S}}(z)-} \]

Similarly to the estimator derived for \(\), if \(_{X}\) is a symmetric measure then the optimal singular values for the estimator of \(\) are all zero, see remark 5.

If \(\) is a shifted Wigner matrix, i.e. \(=+c\) with \(=^{}^{N N}\) having i.i.d. Gaussian entries with variance \(}{{N}}\) and \(c 0\) a real number, then \(_{_{X}}(z)=z+c\). Moreover, if \(\) is Gaussian matrix with variance \(}{{N}}\), then the set of equations (13) simplifies to a great extent, and we can compute \(q_{4}\) analytically in terms of \(_{_{S}}(z)\), see appendix D.4.

### Algorithmic nature of the RIEs

The explicit RIEs (7) and (12) proposed in this section, provide spectral algorithms to estimate the matrix factors from the data matrix (and the priors). An essential ingredient that must be extracted from the data matrix is \(_{_{S}}(z)\). This quantity can be approximated from the observation matrix using Cauchy kernel method introduced in (see section 19.5.2), from which \(_{S}(.)\) can be approximated using (6). Therefore, given an observation matrix \(\), the spectral algorithm proceeds as follows:

1. Compute the SVD of \(\).
2. Approximate \(_{_{S}}(z)\) from the singular values of \(\).
3. Construct the RIEs for \(,\) as proposed in paragraphs 3.2, 3.3.

## 4 Numerical results

### Performance of RIE for \(\)

We consider the case where \(,\) both have i.i.d. Gaussian entries of variance \(}{{N}}\), and \(\) is a Wishart matrix, \(=}\) with \(^{N 4N}\) having i.i.d. Gaussian entries of variance \(}{{N}}\). For various SNRs, we examine the performance of two proposed estimators, the RIE (7), and the square-root of the estimator (10) (since \(\) is PSD), which is sub-optimal. In figure 1, the MSEs of these algorithmic estimators are compared with the one of Oracle estimator (3). We see that the average performance of the algorithmic RIE \(_{X}^{*}}()\) is very close to the (optimal) Oracle estimator \(_{X}^{*}()\) (relative errors are small and provided in the appendices) and we believe that the slight mismatch is due to the numerical approximations and finite-size effects. Note that, although the estimator \(_{X^{2}}}()}\) is sub-optimal, it does not use any prior knowledge of \(\). For more examples, details of the numerical experiments and the relative error of the estimators, we refer to appendix C.3.

### Performance of RIE for \(\)

We consider the case where \(\) has i.i.d. Gaussian entries of variance \(}{{N}}\), and \(\) is a shifted Wigner matrix with \(c=3\). Matrix \(\) is constructed as \(=_{Y}_{Y}^{}\) with \(_{Y}^{N N},_{Y}^{M M}\) are Haar distributed, and the singular values are generated independently from the uniform distribution on \(\). MSEs of the RIE (12) and the Oracle estimator (5) are illustrated in figure 2. We see that the performance of the algorithmic RIE \(_{Y}^{*}}()\) is very close to the optimal estimator \(_{Y}^{*}()\).

Non-rotational invariant priorIn another example, which we omit here, with the same settings for \(,\), we consider the case where \(\) is a sparse matrix with entries distributed according to Bernoulli-Rademacher prior. The RIE is not optimal in this setting (since the prior is not bi-rotational invariant), however applying a simple thresholding function on the matrix constructed by RIE yields an estimate with lower MSE. This observation suggests that for the case of general priors, the RIEs can provide a spectral initialization for more efficient estimators. For more details and examples, see appendix D.4.

### Comparing RIEs of matrix factorization and denoising

The proposed RIEs, namely (7) and (12), simplify greatly when the matrices \(,\) are Gaussian, and \(\) is a shifted Wigner matrix. We perform experiments with these priors, where for a given observation matrix \(\), we look at the RIEs of \(\), \(\) for the _MF problem_, and simultaneously at the RIE of the product \(\) as a whole for the _denoising problem_ with formulas introduced in  (which can also be obtained by taking \(\) to be the identity matrix, see appendix D.3.1). Figure 3

Figure 2: MSE of estimating \(\). MSE is normalized by the norm of the signal, \(\|\|_{p}^{2}\). \(\) has uniform spectral density, \(()\). \(\) is a shifted Wigner matrix with \(c=3\), and \(\) is a \(N M\) matrix with i.i.d. Gaussian entries of variance \(}{{N}}\). RIE is applied to \(N=2000,M=4000\), and the results are averaged over 10 runs (error bars are invisible).

illustrates these experiments. In particular the MSE of the denoising-RIE matches well the one of the associated Oracle estimator, and as expected is lower than the MSE of the product of MF-RIEs.

## 5 Derivation of the explicit RIEs

In this section, we sketch the derivation of our explicit RIE for \(\). The RIE for \(\) is derived similarly, but requires more involved analysis and is presented in appendix D. For simplicity, we take the SNR parameter in (1) to be 1, so the model is \(=+\). The optimal eigenvalues are constructed as \(_{xi}^{*}=_{j=1}^{N}_{j}_{i}^{}_{j} ^{2}\). We assume that in the large \(N\) limit, \(_{xi}^{*}\) can be approximated by its expectation and we introduce

\[_{xi}^{*}=_{j=1}^{N}_{j}\,_{i}^{}_{j}^{2} \]

where the expectation is over the (left) singular vectors of the observation matrix \(\). Therefore, to compute these eigenvalues, we need to find the mean squared overlap \(_{i}^{}_{j}^{2}\) between eigenvectors of \(\) and singular vectors of \(\). In what follows, we will see that (a rescaling of) this quantity can be expressed in terms of \(i\)-th singular value of \(\) and \(j\)-th eigenvector of \(\) (and the limiting measures, indeed). Thus, we will use the notation \(O_{X}(_{i},_{j}):=N_{i}^{ }_{j}^{2}\) in the following. In the next section, we discuss how the overlap can be computed from the resolvent of the "Hermitized" version of \(\).

### Relation between overlap and resolvent

Construct the matrix \(}^{(N+M)(N+M)}\) from the observation matrix:

\[}=[_{N N}&\\ ^{}&_{M M}]\]

By Theorem 7.3.3 in , \(}\) has the following eigen-decomposition:

\[}=[}_{S}&}_{S}& \\ }_{S}^{(1)}&-}_{S}^{(1)}&_{S}^{(2)} ][_{N}&&\\ &-_{N}&\\ &&][}_{S }&}_{S}&\\ }_{S}^{(1)}&-}_{S}^{(1)}&_{S}^{(2)} ]^{} \]

with \(_{S}=[_{S}^{(1)}&_{S}^{(2)}]\) in which \(_{S}^{(1)}^{M N}\). And, \(}_{S}^{(1)}=}_{S}^{(1)}\), \(}_{S}=}_{S}\). Eigenvalues of \(}\) are signed singular values of \(\), therefore the limiting eigenvalue distribution of \(}\) (ignoring zero eigenvalues) is the same as the limiting symmetrized singular value distribution of \(\). Define the resolvent of \(}\),

\[_{}(z)=(z-})^{-1}\]

We assume that as \(N\) and \(z\) is not too close to the real axis, the matrix \(_{}(z)\) concentrates around its mean. Consequently, the value of \(_{}(z)\) becomes uncorrelated with the particular

Figure 3: MSE of factorization problem. MSE is normalized by the norm of the signal. \(\) is a shifted Wigner matrix with \(c=1\), and both \(\) and \(\) are \(N M\) matrices with i.i.d. Gaussian entries of variance \({}^{1}\!/\!\!N\). RIE is applied to \(N=2000,M=4000\). In each run, the observation matrix \(\) is generated according to (1), and the factors \(\), \(\) are estimated simultaneously from \(\). Results are averaged over 10 runs (error bars are invisible).

realization of \(\). Specifically, as \(N\), \(_{}(z)\) converges to a deterministic matrix for any fixed value of \(z\) (independent of N). Denote the eigenvectors of \(\) by \(_{i}^{M+N}\), \(i=1,,M+N\). For \(z=x-\) with \(x\) and small \(\), we have:

\[_{}(x-)=_{k=1}^{2N} }{(x-_{k})^{2}+^{2}}_{k}_{k}^ {}+}{x^{2}+^{2}}_{k=2N+1}^{N+ M}_{k}_{k}^{}\]

where \(_{k}\) are the eigenvalues of \(\), which are in fact the (signed) singular values of \(\), \(_{1}=_{1},,_{N}=_{N},_{N+1}=-_{1},,_{2N}=-_{N}\).

Define the vectors \(}_{i}=[_{i}^{},_{M}]^{}\) for \(_{i}\) eigenvectors of \(\). We have

\[}_{i}^{}\,_{}(x- )}_{i}=_{k=1}^{2N}_{k})^{2}+^{2}}}_{i}^{} _{k}^{2}++^{2}}_{k=2N+1}^{N +M}}_{i}^{}_{k}^{2} \]

Given the structure of \(_{k}\)'s in (15), \(}_{i}^{}_{j}^{2}= _{i}^{}_{j}^{2}=}_ {i}^{}_{j+N}^{2}\) for \(1 j N\), and the second sum in (16) is zero. We assume that in the limit of large N this quantity concentrates on \(O_{X}(_{j},_{i})\) and depends only on the singular values and eigenvalue pairs \((_{j},_{i})\). We thus have:

\[}_{i}^{}\,_{}(x- )}_{i}_{ }+^{2}}O_{X}(t,_{i})_{S}(t)\,dt \]

where the overlap function \(O_{X}(t,_{i})\) is extended (continuously) to arbitrary values within the support of \(_{S}\) (the symmetrized limiting singular value distribution of \(\)) with the property that \(O_{X}(t,_{i})=O_{X}(-t,_{i})\) for \(t(_{S})\). Sending \( 0\), we find

\[}_{i}^{}\,_{}(x- )}_{i}_{S}(x)O_{X}(x, _{i}) \]

This is a crucial relation as it allows us to study the overlap by means of the resolvent of \(\). In the next section, we establish a connection between this resolvent and the signal \(\), which enables us to determine the optimal eigenvalues values \(_{xi}^{*}\) in terms of the singular values of \(\).

### Resolvent relation

To derive the resolvent relation between \(\) and \(\), we fix the matrix \(\) and consider the model

\[=_{1}_{1}^{}+_{2}_{2} ^{}\]

with \(,^{N M}\) fixed matrices with limiting singular value distribution \(_{Y},_{W}\), and \(_{1},_{2}^{N N},_{1},_{2} ^{M M}\) independent random Haar matrices. Indeed, if we substitute the SVD of the matrices \(,\) in model (1) we find the latter model. Now, the average over the singular vectors of \(\) (with fixed \(\)) is equivalent to the average over the matrices \(_{1},_{2},_{1},_{2}\). In appendix C.1, using the Replica trick, we derive the following relation in the limit \(N\):

\[_{}(z)=[ _{3}^{-1}_{X^{2}}}{_{3}}&\\ &(z-_{2})^{-1}_{M}] \]

with \(_{1},_{2},_{3}\) satisfying set of equations (41). \(.\) is the expectation w.r.t. the singular vectors of \(\) (or equivalently over \(_{1},_{2},_{1},_{2}\)), and \(_{X^{2}}\) is the resolvent of \(^{2}\). As stated earlier, we assume that the resolvent \(_{}(z)\) concentrates in the limit \(N\), therefore we drop the brackets in the following computation.

### Overlaps and optimal eigenvalues

From (18), (19), we find:

\[ O_{X}(,_{i})&_{S}()}\,_{z-0^{+}}\, {x}_{i}^{}\,_{3}^{-1}_{X^{2}}}{ _{3}}\,_{i}\\ &=_{S}()}\,_{z- 0^{+}}\,-_{3}_{i}^{2}} \]

In Fig. 4 we illustrate that the theoretical predictions (20) are in good agreement with numerical simulations for a particular case of \(\) a Wigner matrix, and \(,\) with i.i.d. Gaussian entries.

Once we have the overlap, we can compute the optimal eigenvalues to be

\[_{x_{i}}^{*}_{j=1}^{N}_{j}O_{X}(_{ i},_{j})_{S}(_{i})}\ _{z_{i}-i0+}_{j=1}^{N}}{z- _{1}-_{3}_{j}^{2}} \]

With a bit of algebra, we find the estimator in (7) in the limit \(N\), see appendix C.2.

## 6 Conclusion

We studied the MF problem with extensive-rank hidden matrices, and proposed explicit (optimal) RIEs to recover the distinct factors. The model we considered, although limited, is the first analytically solvable model in this challenging regime, and we believe it paves the way for future investigations. Extending the methodology developed here to the general case where both matrices can be non-symmetric is an interesting research direction (note that in this case factors can be recovered up to rotations). Moreover, adapting RIEs to incorporate additional structures/ constraints on the signals is a problem with practical importance, that we leave for future investigations.

In general, the MF problem in the extensive-rank regime has not received an in-depth exploration compared to its counterpart in the low-rank regime. While substantial progress has been made in understanding and devising algorithms for low-rank matrices, the challenges posed by extensive-rank matrices remain relatively uncharted. Specifically, the study of general non-rotation invariant priors with i.i.d entries (e.g. a prior supported on the positive real line as in non-negative MF) stands out as an underdeveloped area from the theory side.