# SugarCrepe++ Dataset: Vision-Language Model Sensitivity to Semantic and Lexical Alterations

Sri Harsha Dumpala\({}^{*}\)   Aman Jaiswal\({}^{*}\)   Chandramouli Sastry

Evangelos Milios   Sageev Oore   Hassan Sajjad

Dalhousie University, Canada.

The authors contribute equally to this work.

###### Abstract

Despite their remarkable successes, state-of-the-art large language models (LLMs), including vision-and-language models (VLMs) and unimodal language models (ULMs), fail to understand precise semantics. For example, semantically equivalent sentences expressed using different lexical compositions elicit diverging representations. The degree of this divergence and its impact on encoded semantics is not very well understood. In this paper, we introduce the SugarCrepe++ dataset to analyze the sensitivity of VLMs and ULMs to lexical and semantic alterations. Each sample in SugarCrepe++ dataset consists of an image and a corresponding triplet of captions: a pair of semantically equivalent but lexically different positive captions and one hard negative caption. This poses a 3-way semantic (in)equivalence problem to the language models. We comprehensively evaluate VLMs and ULMs that differ in architecture, pre-training objectives and datasets to benchmark the performance of SugarCrepe++ dataset. Experimental results highlight the difficulties of VLMs in distinguishing between lexical and semantic variations, particularly to object attributes and spatial relations. Although VLMs with larger pre-training datasets, model sizes, and multiple pre-training objectives achieve better performance on SugarCrepe++, there is a significant opportunity for improvement. We demonstrate that models excelling on compositionality datasets may not perform equally well on SugarCrepe++. This indicates that compositionality alone might not be sufficient to fully understand semantic and lexical alterations. Given the importance of the property that the SugarCrepe++ dataset targets, it serves as a new challenge to the vision-and-language community. Data and code is available at [https://github.com/Sri-Harsha/scpp](https://github.com/Sri-Harsha/scpp).

## 1 Introduction

Large language models (LLMs), including vision-and-language models and unimodal language models, have shown tremendous results in solving a majority of vision and natural language processing (NLP) tasks. Surprisingly, despite such success, LLMs can exhibit different behaviors for semantically equivalent sentences composed with different syntactic or lexical structures. Previous works have reported such lack of compositional reasoning in both vision-and-language models (e.g., ) and unimodal language models (e.g., ). For instance, the performance of the state-of-the-art (SOTA) LLMs including GPT-4, Gemini and Llama are sensitive to the prompt formatting . The model editing techniques  suffer from misfired edits due to the dominance of lexical overlap . Similarly, Zou et al.  demonstrated that safety aligned models can be "jailbroken" by simply appending an adversarial suffix causing them to generate objectionable content bypassing all safeguards.

These observations suggest that language models' perception of semantic similarity crucially depends on the lexical representation of the sentence and calls for a stricter evaluation of semantic text similarity that factors in lexical and syntactic structures. Semantic text similarity is one of the oldest metrics to evaluate language understanding  and despite recent evidence of lexical sensitivity, large benchmarks (e.g., ) evaluate semantic similarity without explicitly considering the lexical influence. In this work, we aim to address this gap by proposing a dataset to perform joint evaluation of semantic understanding -- through the semantic equivalence detection task (elaborated below) -- and lexical sensitivity in language models.

Recognizing semantic similarity is often viewed as being fundamental to language understanding. In fact, strong performance on semantic text similarity is often predictive of a language model's performance in various downstream applications  including question-answering, retrieval and summarization. Based on overlap in their meaning, a pair of sentences can be roughly labeled as semantically equivalent, semantically similar or semantically dissimilar. More specifically, semantically equivalent sentences convey the _same meaning_2, perhaps differing in terms of syntactic 3 and lexical 4 structures. On the other hand, sentences that are not semantically equivalent but describe the _same topic_ are said to be semantically similar . Important examples include MRPC , QQP  and STS : while MRPC and QQP contain binary labels indicating semantic equivalence, STS uses a score between 0 to 5 to indicate the degree of semantic equivalence.

The timely release of these datasets have fueled the research and development of improved language models. While these datasets remain relevant even today and are included as part of the challenging GLUE benchmark , we aim to improve upon the following aspects to evaluate language understanding through _semantic equivalence task under controlled lexical constraints_:

1. Varying Definitions of Semantic Equivalence: While MRPC  aims to evaluate if language models can detect semantic equivalence, it ultimately uses a loose definition of equivalence in that the sentence pairs that convey different information about the same topic are also considered to be semantically equivalent. Different from semantically equivalent sentences, a pair of questions are defined to be semantically equivalent if they have the same answer  and hence, datasets on semantically equivalent question pairs (e.g., QQP) require additional knowledge beyond language understanding. In contrast, we focus our evaluation on fundamental language understanding ability and evaluate a language model in terms of its ability to recognize semantic equivalence between a pair of sentences. In this work, two sentences are said to be semantically equivalent if the sentences convey the same meaning and can be inferred from each other (i.e., bidirectional entailment).
2. Lack of Lexical Constraints: While achieving perfect scores on the existing semantic similarity datasets is indeed challenging, trivial baselines using lexical overlap also provide reasonable estimates of semantic similarity (for example, see Figure 2 and Table 6 in Abdalla et al. ). Therefore, the extent to which language models rely upon lexical structure when identifying semantic equivalence and semantic similarity is not clearly known. We are thus motivated to explore a more challenging setting that requires a language model to encode semantics beyond superficial lexical structure.

Closer to our goal, Hsieh et al.  introduced the challenging SugarCrepe dataset to evaluate the ability of vision-language models (VLMs) to identify _one_ correct caption in a pair of lexically similar sentences. As an example, given an image, the model may be asked to select the correct caption between "_A tractor and two boats far from the water_" (incorrect) and "_A tractor and two boats beside the water_"(correct). Hsieh et al.  reported that several VLMs face challenges in selecting the correct caption and attributed the low performance to the text encoder's inability to identify semantic differences in a pair of lexically similar sentences. While this dataset provides a good starting point, it is insufficient for comprehensively evaluating the lexical sensitivity and semantic understanding of a model; the model's understanding of semantic equivalence in the presence of lexical differences remains unclear from such an evaluation. For instance, "_Couple of boats and a tractor located next to the water_" is semantically identical to the correct caption despite lexical dissimilarities. A precise evaluation of lexical influence upon semantic understanding _should_ include pairs of semantically-equivalent, semantically-opposite, lexically-similar, and lexically-dissimilar sentences. In this work, we target these four cases and define the following research questions:

1. How well do LLMs understand the _semantic equivalence_ between a pair of sentences given their _syntactic and lexical differences_?
2. How well do LLMs understand the _semantic differences_ between a pair of sentences given their _syntactic and lexical similarities_?

To that end, we extend SugarCrepe to introduce SugarCrepe++ dataset which additionally contains semantically equivalent sentences that are lexically dissimilar. The answer to these two questions enable us to evaluate the semantic understanding while disentangling the effect of lexical matches between sentences.

Our contributions to this work are as follows:

* **SugarCrepe++ dataset.** We introduce SugarCrepe++, a diverse, multi-modal and human-validated dataset, especially designed to evaluate the sensitivity of encoded semantics in language models to lexical composition. The introduction of a three-way semantic (in)equivalence task enables evaluation with increased resolution previously not possible with two captions. Figure 1, illustrates instances from five categories incorporated in SugarCrepe++ and highlights the apparent gaps in performance of VLMs when re-evaluated using SugarCrepe++.
* **Unified evaluation.** We designed SugarCrepe++ dataset such that the overlap of semantic information between the two positive captions is always higher than between the positive and negative captions, even without considering the image. This allows us to evaluate on the Text-to-Text task twice for the same triplet, using each positive caption as the reference once.

Figure 1: Examples from SugarCrepe++ (SC++) dataset. \(P_{1}\) and \(P_{2}\) are semantically equivalent but lexically different while \(N\) is semantically different than both \(P_{1}\) and \(P_{2}\) despite its lexical similarity with \(P_{1}\). The adjacent line charts highlight the performance gaps in VLMs discovered upon re-evaluation using SC++, and shows that strong lexical and semantic understanding may not be required to achieve better performance on SugarCrepe (SC). Refer to Appendix C.4 for details on negative captions (e.g., swap-object, replace-relation).

These caption triplets combined with an image enables a previously unexplored dual-mode evaluation of VLMs in both Image-to-Text and Text-to-Text settings.

In this work, we evaluate a comprehensive list of VLMs and standalone/unimodal language models (ULMs) using SugarCrepe++. A few of the notable findings are summarized below:

* VLMs struggle to identify the differences between semantic and lexical alterations, particularly if the lexical alterations are based on swapping attributes or objects, or replacing relations.
* There exists a large gap between the VLMs and human-level performance signifying huge scope for improvement in VLMs.
* Text encoders of VLMs form the major bottleneck in achieving better performance on SugarCrepe++.
* Even state-of-the-art ULMs fail to consistently dissociate semantics from lexical forms i.e., they tend to select captions with higher lexical overlap than the ones with higher semantic overlap.

## 2 SugarCrepe++

In this section, we describe the data generation and validation pipeline used to create SugarCrepe++ dataset. The SugarCrepe dataset  -- derived from MS-COCO , a dataset of image-caption pairs -- consists of a correct caption and an incorrect caption for each image while ensuring that the two captions are lexically similar. To create SugarCrepe++ dataset, we generate another _correct_ caption for each image such that it uses alternative lexical representation while being semantically identical to the original correct caption in SugarCrepe. If we refer to the correct captions as positives, the incorrect caption can be termed as a _hard negative_ due to its high lexical overlap with one of the positive captions. In contrast to SugarCrepe, SugarCrepe++ enables evaluation across both multimodal and unimodal settings while also providing a comprehensive evaluation of semantic understanding with lexical constraints. Additional related work is discussed in Appendix B.

### Dataset Generation and Validation

Prior works [8; 24; 64; 41] have extensively used image-caption pairs from MS-COCO; specifically Crepe  and its improved derivative SugarCrepe  that leverages the recent advancements in conditional text generation using large language models (LLMs) to generate hard negative captions, thereby overcoming the issues with procedurally generated captions. The SugarCrepe dataset consists of (only) one positive and one hard negative caption for each image. Relative to the negative caption, a single positive caption can either have low or high lexical overlap. The original SugarCrepe only captures the high overlap case. To evaluate the sensitivity of encoded semantics to lexical alteration, we require an additional positive caption with a different lexical composition. We build SugarCrepe++ using instruction fine-tuned Mistral 7B  model to further introduce an additional positive caption as illustrated in Algorithms 1 and 2. The generation process of the additional caption can be divided into two stages: (1) generation using meta-prompts and (2) automated and human validation of generated captions.

```
0: Original caption \(P_{1}\), Generated caption \(P_{2}\)
1:\(Valid LLM(V\{P_{2},P_{1}\})\)
2:while not Valid do
3:\(P_{2} Generation Pipeline(M\{P_{1}\})\)
4:\(valid LLM(V\{P_{2},P_{1}\})\)
5:Return\(P_{2}\)
```

**Algorithm 1** Generation Pipeline

```
0: Original caption \(P_{1}\), Generated caption \(P_{2}\)
1:\(Valid LLM(V\{P_{2},P_{1}\})\)
2:while not Valid do
3:\(P_{2} Generation Pipeline(M\{P_{1}\})\)
4:\(valid LLM(V\{P_{2},P_{1}\})\)
5:Return\(P_{2}\)
```

**Algorithm 2** Automatic Validation Pipeline

```
0: Original caption \(P_{1}\), Generated caption \(P_{2}\)
1:\(Valid LLM(V\{P_{2},P_{1}\})\)
2:while not Valid do
4:\(Valid LLM(V\{P_{2},P_{1}\})\)
5:Return\(P_{2}\)
```

**Algorithm 3** Automatic Validation Pipeline

```
0: Original caption \(P_{1}\), Generated caption \(P_{2}\)
1:\(Valid LLM(V\{P_{2},P_{1}\})\)
2:while not Valid do
3:\(P_{2} Generation Pipeline(M\{P_{1}\})\)
4:\(valid LLM(V\{P_{2},P_{1}\})\)
5:Return\(P_{2}\)
```

**Algorithm 4** Automatic Validation Pipeline

**Generation using meta-prompts:** Algorithm 1 presents our generation pipeline. We followed an iterative prompting methodology to refine a meta-prompt (M) that generates optimal second positive captions (P\({}_{2}\)) given the original positive caption (P\({}_{1}\)). The meta-prompt (M) is a composition of constituent prompts (M\({}_{i}\)) formed by concatenating (;) different sub-prompts.

\[M=M_{1};M_{2};M_{3};;M_{i}\]

The meta-prompt is applied to an input (\(x\)) to obtain the final prompt M{x} that conditions the LLM to generate the second positive caption (P\({}_{2}\)) as \(LLM(M\{P_{1}\}) P_{2}\). We find integrating different techniques into a larger meta-prompt (\(M\)) improves the generation quality, exploiting the benefits from each of them. Following , we prefix \(M\) with a 'role-play' prompt (M\({}_{1}\)) that conditions the LLM to the high-level task of data generation and simulates the role of a 'Data Generating AI.' We utilize 'Rules Prompting'  to enhance LLMs' faithfulness in instruction following using explicit and itemized rules: in particular, we further condition M using a 'rules' prompt (M\({}_{2}\)) that describes three rules to ensure the consistency of the generated caption (P\({}_{2}\)). Expanding on the prompting methodology from , we also append per-shot 'demonstrations' (M\({}_{3}\)) that include additional reasoning and the generated caption. The 'role-play' (M\({}_{1}\)) prompt is described in Figure 2. The 'rules' (M\({}_{2}\)) and 'demonstrations' (M\({}_{3}\)) sub-prompts are elaborated in Figure 3 of Appendix C. During our initial testing, we noticed the following systematically recurring errors: (i) LLM generated caption is _identical_ to the input caption; (ii) parsing failure due to superfluous outputs. Consequently, we incorporate the following safeguards in the generation pipeline as shown in Algorithm 1: 1) If the generated caption is found to be identical to the input caption (using automatic tools), we use a meta-prompt that includes the complete context with basic instructions to regenerate P\({}_{2}\); 2) We detect superfluous outputs based on word overlap and then discard any generation that does not meet the minimum threshold.

**Automated and human validation of generated captions:** Since the above safeguards do not ensure that the generated caption is _always_ semantically equivalent to the input caption, we require additional steps to ensure semantic equivalence. We notice subtle differences in generated caption that break the semantic equivalence and this also highlights the limitations of semantic understanding in LLMs. Despite such limitations, prior works  have demonstrated that LLM agents -- e.g., LLMs instantiated with different prompts -- can interact with each other to solve complex tasks. We build on this to reduce human effort and optimize labelling costs using automatic validation of generated captions similar to . We employ a validator LLM agent, that is responsible for validating the semantic consistency of the generated caption with the original caption and signalling the Generator agent to retry as needed. Here, we refer to the LLM agents as two instances of the same LLM conditioned on different meta-prompts.

We define a validation meta-prompt (V) that uses P\({}_{1}\) and P\({}_{2}\) from the previous step to form a validation input to the validator LLM. Similar to the meta-prompt (M), the validation prompt (V) consists of a sequence of sub-prompts, including the 'Validation instruction' (V\({}_{1}\)) prompt and the 'Validation demonstration' (V\({}_{2}\)). The validator LLM is conditioned using V to generate a boolean value indicating the caption's validity,

\(LLM(V\{P_{1},P_{2}\})\{True,False\}\), where \(V=V_{1};V_{2}\). We use the boolean output value to trigger a regeneration step that starts the generation pipeline again as described in Algorithm 2. The prompt for employing validator LLM is detailed in Figure 4 in Appendix C.3. In our automatic validation, we assume that the image is captioned correctly and do not consider the image when ensuring semantic correctness.

To ensure the quality of the SugarCrepe++ dataset, we conducted human validation with two experts to correct the errors in the positive sentences (P\({}_{2}\)) generated by LLMs (Appendix C.5 provides a list of common errors generated by LLMs) and any disagreements between the expert annotators were mutually resolved through inter-annotator discussion. These human annotators also assessed the validity of caption triplets (P\({}_{1}\), P\({}_{2}\), \(N\)) and paired images (\(I\)) as a data point. The final statistics of the

Figure 2: Role playing prompt for “Data Generator AI\({}^{*}\).

SugarCrepe++ dataset after the human validation are described in Table 1. Additional statistics, including the categories from MS-COCO and the VERA or grammar scores of SugarCrepe++ are detailed in Appendix I. We formulated a measure of syntactic and lexical similarity to analyze pairs of sentences in our dataset. See Appendix D for more details.

## 3 Benchmark on SugarCrepe++ Dataset

Experimental setupWe benchmark VLM performance on SugarCrepe++ dataset under two different evaluation settings. (1) Multi-modal image-to-text (ITT) evaluation: both image and text are provided as inputs to evaluate VLMs in a multi-modal setting. (2) Uni-modal text-only (TOT) evaluation: only text is provided as input to evaluate the text encoders of VLMs in a unimodal setting.

Each sample in the SugarCrepe++ dataset consists of an image \(I\) and corresponding two positive captions (P\({}_{1}\) and P\({}_{2}\)) and a negative caption (N). If \(p(X|I)\) denotes the likelihood of caption \(X\) for image \(I\), we compute the ITT evaluation metric given P\({}_{1}\), P\({}_{2}\) and N as:

\[_{hit}=1&(p(P_{1}|I)>p(N|I))(p(P_{2}|I)>p(N|I))\\ 0&\]

For a VLM model such as CLIP which relies upon embeddings, the log-likelihood \( p\) is defined to be proportional to the cosine similarity between the respective embeddings. Similarly, the TOT evaluation is defined as:

\[_{hit}=1&(p(P_{1}|P_{2})>p(N|P_{2}))(p(P_{2}|P_{ 1})>p(N|P_{1})).\\ 0&\]

As above, we use cosine-similarity for embedding-based models. We report the performance in terms of Accuracy (%), computed as the ratio of the number of hits to the total number of samples. we also perform a human evaluation to calculate the human performance on the benchmark. Human evaluation is performed by \(4\) graduate-level students where each person was provided with a randomly selected \(150\) samples (\(30\) from each subset) and was asked to select the negative caption for each sample. In TOT setting, only the three captions were provided. In the ITT setting, image along with the captions were provided to the human evaluators. The average human performance is reported in terms of accuracy (%).

### Evaluation of VLMs on SugarCrepe++

We consider a variety of VLMs for evaluation using SugarCrepe++: 1) Models trained with a contrastive learning objective such as CLIP , RoBERTa-ViT-B/32 , ALIGN  and ALIP . 2) Models trained by combining multiple objective functions, such as FLAVA , ALBEF , BLIP  and BLIP-2 . 3) Models with a unified encoder for text and images, such as ViLT , and multi-lingual distilled models like AltCLIP ; 4) Models that align text with corresponding visual concepts in the image, such as SegCLIP , and XVLM  - with two variants, XVLM-4M and XVLM-16M. We consider a wide array of VLMs that differ in terms of model architecture, total number of parameters, embedding dimension and pre-training objectives to measure the effect of various training choices on the model's semantic understanding capabilities. For further model details, refer to Appendix E.1.

Performance of VLMs on SugarCrepe++ is strongly influenced by the type of hard negative.The performance of VLMs on different subsets of SugarCrepe++ are provided in Table 2. Swap type hard negatives, which are generated by swapping either Objects or attributes, pose a significant challenge to VLMs, as most achieve very low performance. Failing in examples with simple reordering of words, as in swap subset, highlights a key limitation of VLMs in understanding the structure of the input text. For replace-type hard negatives (generated by replacing objects, attributes,

   Swap Object & Swap Attribute & Replace Object & Replace Attribute & Replace relation & **Total** \\ 
245 & 666 & 1652 & 788 & 1406 & 4757 \\   

Table 1: SugarCrepe++ consists of 4757 examples with the following distribution of sample sizes or relations), VLMs are comparatively better at discerning the negative from the positive caption when the object is replaced. VLMs can also somewhat discern hard negatives from positives when attributes are replaced. However, they struggle when the relation between objects is replaced. Additionally, large performance gaps (ranging from 10% to 50%) between the best models and human performance across most subsets signifies opportunity for improvement in VLMs' semantic understanding abilities.

**Pre-training data size and objective functions affect VLM performance.** Table 2 shows that the models trained with multiple objective functions, particularly FLAVA and BLIP, perform better on SugarCrepe++ compared to models trained using contrastive loss function alone. This indicates that the contrastive learning objective alone may not be sufficient for VLMs to effectively learn the semantic relations between text and images. Furthermore, models pre-trained with smaller datasets, such as ALIP, ALBEF and XVLM-4M, have lower performance compared to other models. Interestingly, these observations are consistent across all subsets of SugarCrepe++.

**Text encoders bottleneck VLM performance on SugarCrepe++.** All VLMs perform significantly better on the ITT task compared to the TOT task on SugarCrepe++ (see Table 2). This shows that there is a higher ambiguity in identifying the semantic and lexical alterations using only the text embeddings (TOT) compared to the case of comparing the text embeddings with the image embeddings (ITT). Moreover, the text encoders of VLMs perform inferior to the text encoders of ULMs (see Table 7). This is in agreement with the findings in . Additionally, FLAVA, the best performing model on most of the subsets also achieves a good performance in TOT setting, further signifying the importance of a strong text encoder in achieving better performance.

**Fine-tuning VLMs for image-text retrieval improves performance with opportunity for further improvements.** Table 3 provides the performance of VLMs (ViLT and XVLM-16M) fine-tuned for the task of image-to-text retrieval (ITR). While we observe performance improvements on SugarCrepe++, VLMs still face significant challenges in discerning negative captions from positive ones, particularly for the Swap object and Replace relation subsets. Moreover, there remains a substantial gap between VLM performance and human-level performance. This indicates that VLMs capable of matching images to corresponding captions do not necessarily learn the intricate details regarding semantic information and lexical variations in the text.

    &  &  &  &  &  \\   & ITT & TOT & ITT & TOT & ITT & TOT & ITT & TOT & ITT & TOT \\  Human & 100.00 & 96.67 & 96.67 & 93.3 & 100.00 & 95.00 & 100.00 & 98.33 & 100.00 & 96.67 \\  CLIP  & 45.18 & 19.74 & 45.21 & 33.03 & 86.80 & 83.72 & 65.61 & 59.14 & 56.26 & 38.62 \\ RoBERTa-ViT-B/32  & 44.30 & 29.39 & 56.32 & 52.66 & 89.04 & 94.55 & **74.49** & **80.46** & 59.39 & 57.75 \\ ALIGN  & 41.23 & 25.43 & 51.90 & 44.00 & 90.19 & 84.62 & 69.92 & 69.04 & 51.71 & 45.23 \\ ALIP  & 36.84 & 20.18 & 46.12 & 28.77 & 71.49 & 50.06 & 54.95 & 34.52 & 47.80 & 23.47 \\  FLAVA  & **54.39** & **45.18** & 59.21 & **57.84** & 89.59 & 94.43 & 73.35 & 72.46 & **60.10** & **57.97** \\ ALBEF  & 28.94 & 10.09 & 36.83 & 18.87 & 76.27 & 55.57 & 56.35 & 30.33 & 47.80 & 30.65 \\ BLIP  & 47.37 & 31.14 & **60.58** & 52.97 & **92.62** & 89.04 & 72.08 & 75.13 & 56.76 & 57.47 \\ BLIP  & 35.09 & 21.49 & 37.60 & 29.98 & 89.41 & 72.58 & 62.82 & 64.47 & 53.27 & 43.47 \\  ViLT  & 35.23 & – & 52.20 & – & 91.10 & – & 55.33 & – & 37.48 & – \\ AltCLIP  & 42.54 & 25.43 & 45.81 & 35.77 & 92.61 & 93.46 & 71.06 & 74.62 & 57.25 & 56.69 \\ SegCLIP  & 45.61 & 25.44 & 46.12 & 40.64 & 85.90 & **95.16** & 62.69 & 67.89 & 54.84 & 41.96 \\ XVLM-4M  & 31.14 & 10.96 & 36.52 & 19.48 & 79.42 & 67.07 & 59.39 & 40.74 & 46.23 & 29.23 \\ XVLM-16M  & 34.21 & 18.86 & 40.33 & 31.05 & 90.81 & 92.07 & 68.02 & 70.43 & 52.47 & 47.87 \\   

Table 2: Comparison of VLMs performance on SugarCrepe++. Performance reported in terms of Accuracy (%). Overall best values are in bold, and group-level best values are underlined.

    &  &  &  &  &  &  \\   & & ITT & TOT & ITT & TOT & ITT & TOT & ITT & TOT & ITT & TOT \\  ViLT  & – & 35.23 & – & 52.20 & – & 91.10 & – & 55.33 & – & 37.48 & – \\ XVLM-16M  & – & 34.21 & 18.86 & 40.33 & 31.05 & 90.81 & 92.07 & 68.02 & 70.43 & 57.47 & 47.87 \\  ViLT-ITR-COCO  & MS-COCO & **50.88** & – & **73.36** & – & 89.89 & – & 71.95 & – & 61.24 & – \\ XVLM-16M-CO  & MS-COCO & 39.91 & 21.06 & 49.93 & **51.60** & 91.65 & **96.79** & **74.24** & **83.63** & 63.09 & **62.87** \\ XVLM-16M-Flickr  & Flickr & 45.61 & **21.49** & 50.53 & 44.44 & **91.71** & 96.01 & **74.24** & 81.59 & **64.01** & 59.89 \\   

Table 3: Evaluation of VLMs fine-tuned for image-text retrieval task. Performance reported in terms of Accuracy (%). ITR dataset is the dataset used to fine-tune the model for image-to-text retrieval.

**Compositionality enhancing methods improve performance on SugarCrepe++ by strengthening the VLM text encoder.** We evaluate recent methods proposed to improve compositionality of VLMs, including NegCLIP , SVLC , CyCLIP , and BLIP-SGVL . As shown in Table 4, methods that improve compositionality of CLIP such as NegCLIP and CLIP-SVLC also achieve better performance on SugarCrepe++ compared to CLIP, undersciving the importance of compositionality as a critical component for understanding semantic and lexical differences. Interestingly, the text encoders of models with improved compositionality (NegCLIP and CLIP-SVLC) perform significantly better than the text encoder of CLIP in the TOT setting. Improved text encoders, in turn, lead to improvements in ITT. On the other hand, methods such as BLIP-SGVL and CyCLIP, which do not use explicit techniques to strengthen the text encoders, show degradation in performance on SugarCrepe++. This further highlights the importance of the text encoder in achieving better performance.

**Larger pre-training data and model size improve CLIP's performance.** We evaluated variants of CLIP that differ in model architecture and size, as well as pre-training data size, on SugarCrepe++ (see Table 5). For CLIP variants pre-trained on a dataset of 400 million image-text pairs, smaller models (CLIP and RN50\(\)4) performed better than larger models (RN50\(\)64). Interestingly, larger models (ViT-bigG/14) performed better than smaller models when the pre-training data was increased to 2 billion samples. Moreover, the text encoders also performed better when the pre-training data was increased to 2 billion image-text pairs.

**Fine-tuning on SugarCrepe++ does not necessarily improve semantic understanding of VLMs.** Table 13 (see Appendix E.3) presents the fine-tuning results.hl Experimental results show that while there are performance improvements on some subsets, fine-tuning can lead to performance degradation on others, which may be attributed to catastrophic forgetting . To address this issue, we used LoRA . Results demonstrate that LoRA improved performance across all subsets.

**Comparison of performance between SugarCrepe and SugarCrepe++ reveals significant differences.** Table 6 compares the performance of VLMs between SugarCrepe and SugarCrepe++. Since a direct comparison of the models' absolute performance on SugarCrepe and SugarCrepe++ is not possible, we assess their relative rankings using CLIP as a baseline. Notably, we observe significant differences in performance trends across the models. For instance, ALBEF and XVLM which achieve better performance on SugarCrepe show substantial degradation in performance on SugarCrepe++. On the other side, there are models such as BLIP and NegCLIP that show improvements on both SugarCrepe and SugarCrepe++. Interestingly for the replace relation subset, NegCLIP achieves better performance on SugarCrepe but shows degradation in performance on SugarCrepe++. These results emphasize the importance of our dataset in

    &  &  &  &  &  \\   & ITT & TOT & ITT & TOT & ITT & TOT & ITT & TOT & ITT & TOT \\  Human & 100.00 & 96.67 & 96.67 & 93.3 & 100.00 & 97.00 & 100.00 & 98.33 & 100.00 & 96.67 \\  CLIP  & 45.18 & 19.74 & 45.21 & 33.03 & 86.80 & 83.72 & 65.61 & 59.14 & **56.26** & 38.62 \\  NegCLIP  & **55.25** & **34.65** & **57.99** & **56.47** & **89.53** & **94.55** & **69.41** & **76.27** & 52.27 & **51.57** \\ CLIP-SVLC  & 42.98 & 18.86 & 48.40 & 34.56 & 80.93 & 91.56 & 56.98 & 66.88 & 47.30 & 51.28 \\ BLIP-SGVL  & 13.16 & – & 38.81 & – & 53.75 & – & 34.39 & – & 30.65 & – \\ CyCLIP  & 37.72 & 13.60 & 34.40 & 18.72 & 70.28 & 78.29 & 49.87 & 49.12 & 40.41 & 29.87 \\   

Table 4: Performance of the methods for improving compositionality of VLMs on SugarCrepe++. Performance reported in terms of Accuracy (%). Here, performance of CLIP is the baseline.

    & \#Model & Pre-train & Swap & Object & Swap Attribute & Replace Object & Replace Attribute & Replace Relation \\ Model & Params & Data Size & ITT & TOT & ITT & TOT & ITT & TOT & ITT & TOT \\  CLIP  & 151M & 400M & 45.18 & 19.74 & 45.21 & 33.03 & 86.80 & 83.72 & 65.61 & 59.14 & 56.26 & 38.62 \\ RN50\(\)4  & 178M & 400M & **46.93** & 21.49 & 46.42 & 30.59 & 87.77 & 80.87 & 67.51 & 53.93 & 53.91 & 38.55 \\ RN50\(\)64  & 623M & 400M & 44.74 & 16.67 & 45.36 & 31.51 & 90.79 & 73.31 & 64.47 & 48.61 & 54.27 & 38.12 \\ RoI-ViT-bigG  & 212M & 2000M & 44.30 & 23.95 & 56.32 & 52.66 & 89.04 & 94.55 & 74.49 & 80.46 & 59.39 & 57.75 \\ VLT-HJ4  & 986M & 2000M & 43.42 & 27.63 & **54.19** & 50.69 & 93.71 & 90.43 & 71.06 & 73.98 & 56.62 & 51.92 \\ VLT-bigG/14  & 2540M & 2000M & 45.61 & **29.22** & **57.38** & 52.05 & **94.13** & 90.44 & **76.41** & 72.84 & **59.45** & 53.49 \\ XLM-Rob-ViT-B/32  & 366M & 5000M & 42.55 & **30.26** & 55.25 & **55.56** & 89.41 & **95.34** & 72.97 & **80.96** & 55.48 & **57.82** \\   

Table 5: Evaluation of several variants of CLIP on SugarCrepe++. Performance reported in terms of Accuracy (%). Best performances in bold. RoB refers to RoBERTa.

evaluating the models in terms of their semantic and lexical understanding, which is not evident by evaluating on compositionality datasets such as SugarCrepe.

### Evaluation of ULMs on SugarCrepe++

**ULMs show promise over VLMs in text-only task of SugarCrepe++.** We evaluate a comprehensive list of unimodal language models (ULMs) to determine the semantic and lexical sensitivity of text-only models. We sample ULMs covering various model sizes, architectures, and training objectives. Recent state-of-the-art small-sized models include MiniLM , GTE  and BGE . From the results presented in Table 7, we observe trends similar to the VLM's performance in TOT task but more importantly, we observe ULMs achieve significant improvements on average as compared to VLMs. Nevertheless, we notice that some of these performances are far below human performance -- for instance, we observe a performance gap of \(\)40% in swap object and swap attribute subsets. Comprehensive results are available in Appendix F.

### Evaluation on GPT-4o and Other Generative VLMs on SugarCrepe++

We evaluated generative VLMs such as BLIP, BakLLaVA  and GPT-4o using SugarCrepe++ dataset in a prompt-based format. We prompted the generative VLMs with the following semantically identical prompts, where N, P\({}_{1}\), P\({}_{2}\) refer to the negative (N) and the two positive captions (P\({}_{1}\) and P\({}_{2}\)) corresponding to the image, respectively.

**Prompt - 1**: Do any of the following captions not match the image? (1) <N>; (2)<P\({}_{1}\)>"; (3) <P\({}_{2}\)>; provide output as (1), (2), (3) or none
**Prompt - 2**: Do any of these captions fail to correspond with the image? (1) <N>; (2) <P\({}_{1}\)>; (3) <P\({}_{2}\)>; provide output as (1), (2), (3) or none
**Prompt - 3**: Do any of these captions fail to correspond with the image? (1) <N>; (2) <P\({}_{1}\)>; (3) <P\({}_{2}\)>; provide output as (1), (2) or (3)

   Model &  &  &  &  &  \\   & SC & SC++ & SC & SC++ & SC & SC++ & SC & SC++ & SC & SC++ \\  CLIP  & 59.21 & 45.18 & 64.99 & 45.21 & 90.86 & 86.8 & 80.33 & 65.61 & 70.48 & 56.26 \\  ALBEF  & 63.16(\(\)) & 28.94(\(\)) & 69.25(\(\)) & 36.83(\(\)) & 93.04(\(\)) & 76.27(\(\)) & 84.65(\(\)) & 56.35(\(\)) & **77.60**(\(\)) & 47.80(\(\)) \\ XVLM  & 64.91(\(\)) & 31.14(\(\)) & 73.79(\(\)) & 36.52(\(\)) & 95.22(\(\)) & 79.42(\(\)) & **87.69**(\(\)) & 59.39(\(\)) & 77.45(\(\)) & 46.23(\(\)) \\ BLIP  & 66.22(\(\)) & 47.37(\(\)) & 76.25(\(\)) & **60.58**(\(\)) & **96.55**(\(\)) & **92.62**(\(\)) & 81.98(\(\)) & **72.08**(\(\)) & 68.35(\(\)) & **56.76**(\(\)) \\ NegCLIP  & **75.44**(\(\)) & **55.25**(\(\)) & **76.87**(\(\)) & 57.99(\(\)) & 93.88(\(\)) & 89.53(\(\)) & 87.18(\(\)) & 69.41(\(\)) & 74.47(\(\)) & 52.27(\(\)) \\   

Table 6: Comparing the performance of VLMs on SugarCrepe (SC) and SugarCrepe++ (SC++) for the ITT task. \(\) and \(\) show increases and decreases in performance with the corresponding CLIP performance as the baseline. Expanded version of the Table is provided in Appendix E.5.

   Model &  \#Params \\ (BERT Scale) \\  &  Swap \\ Object \\  &  Swap \\ Attribute \\  &  Replace \\ Object \\  &  Replace \\ Attribute \\  & 
 Replace \\ Relation \\  &  \\   Human & & 96.67 & 93.3 & 97.00 & 98.33 & 96.67 & 96.40 \\  BGE-small-en-v1.5  & 0.3 & 15.51 & 24.02 & 94.19 & 75.00 & 75.53 & 56.85 \\ All-MiniLM-L12-v2  & 0.3 & 18.78 & 25.38 & 95.22 & 73.86 & 70.41 & 56.73 \\  Angle-BERT-base  & 1 & 25.71 & 33.63 & 92.07 & 78.43 & 75.32 & 61.03 \\ BGE-base-en-v1.5  & 1 & 17.14 & 25.23 & 93.83 & 78.55 & 76.10 & 58.17 \\  UAE-Large-v1  & 3 & 40.41 & 41.44 & **96.85** & 76.14 & 75.82 & 66.13 \\ All-RoBERTa-large-v1  & 3.1 & 42.04 & 45.20 & 94.61 & 74.75 & 74.96 & 66.31 \\ Sentence-T5-xl  & 11.3 & **47.35** & **49.25** & 90.98 & 75.38 & 75.32 & 67.66 \\  Angle-Llama-7b-nil-v2  & 62.3 & 37.96 & 45.80 & 95.22 & 84.39 & **81.44** & **68.96** \\ E5-Mistral-7b-instruct  & 64.9 & 33.47 & 37.84 & 96.67 & **87.06** & 80.51 & 67.11 \\   

Table 7: Comparison of SOTA unimodal language models (ULMs) on SugarCrepe++. We report the TOT accuracy (%), and group the results row-wise based on the model size as reflected by the parameter count. We include the number of parameters in text encoders relative to BERT-base, i.e., 109.5 Million parameters. Overall, best values are in bold, and group-level best values are underlined. We report the average across different subsets as an additional column. Refer to Table 16 for additional results.

First two prompts (Prompt-1 and Prompt-2) are paraphrases of each other and are 4-class problems (Output to be (1), (2), (3) or none). Whereas Prompt-3 is a 3-class problem i.e., outputs to be (1), (2) or (3).

Table 8 provides GPT-4o's performance on SugarCrep++ across the three different prompts. Significant differences between Prompt-1 and Prompt-2, which are paraphrases, highlight the model's sensitivity to prompt structure. GPT-4o struggled to identify negative captions when given four options ((1), (2), (3), or none) but improved when limited to three ((1), (2), or (3)). The model performed best when negative captions were created by replacing an object or attribute in the positive caption but struggled when objects or relations were swapped. While GPT-4o's performance on "replace object" and "replace attribute" tasks neared human levels, it fell short significantly in "swap object," "swap attribute," and "replace relation" cases. Prompt-based evaluation of BLIP and BakLLaVA are provided in Appendix G.1.

**Inference techniques can influence the performance of generative VLMs on SugarCrepe++**. We evaluated generative VLMs on SugarCrepe++ (see Appendix G.2) using recent approaches such as VGPTScore  and VQAScore . We find that despite using significantly larger models (3B-11B parameters), VGPTScore performs comparably with several discriminative VLMs previously considered in our paper (e.g., Table 2). Interestingly, using the same generative VLMs, VQAScore achieves significant performance improvements on SugarCrepe++ as compared to VGPTScore. However, this is still below human performance signifying opportunity for further improvement. Based on several experiments with generative VLMs, we can conclude that the performance of generative VLMs ultimately depends on the inference-technique (e.g., VGPTScore/VQAScore/Prompting-styles) and with the right inference-technique, generative-VLMs may outperform discriminative models.

## 4 Conclusion

We introduce SugarCrepe++, a dataset for evaluating the ability of language models, including both vision-language models (VLMs) and unimodal language models (ULMs), to understand their sensitivity to semantic and lexical alterations in text. Our dataset supports evaluations in both image-to-text (ITT) and text-to-text (TOT) settings. We evaluated a comprehensive list of VLMs and ULMs to highlight a fundamental limitation with these language models in understanding semantic and lexical alterations. The key findings from our evaluation are: (1) There is a significant performance gap between VLMs and human-level performance signifying huge scope for improvement in VLMs. (2) All VLMs exhibit difficulty in comprehending semantic and lexical alterations, especially when these alterations involve swapping attributes or objects, or replacing relations. (3) Similarly, state-of-the-art (SOTA) ULMs lack a robust understanding of lexical composition and consistently fail to separate semantics from lexical forms. (4) While increasing pre-training data, model size, and improving compositionality enhance performance on SugarCrepe++ dataset, these models still fall considerably short of human performance. These insights underscore the critical need for advancements to close the performance gap between models and human understanding. Our SugarCrepe++ dataset serves as a valuable tool for driving future research in this area.

    & Swap Object & Swap Attribute & Replace Object & Replace Attribute & Replace Relation \\  Human & 100.00 & 96.67 & 100.00 & 100.00 & 100.00 \\  Prompt-1 & 46.93 & 73.36 & 91.64 & 87.94 & 69.06 \\ Prompt-2 & 48.25 & 75.04 & 90.82 & 84.90 & 71.19 \\ Prompt-3 & **67.61** & **85.82** & **96.25** & **93.27** & **84.13** \\   

Table 8: Prompt-based evaluation of GPT-4o on SugarCrepe++. We provide both image and a prompt to the GPT-4o and receive the output from GPT-4o. Based on the response, we compute the performance i.e., it is a hit if the model outputs (1) else a miss. Performance is reported in terms of Accuracy (%), where accuracy is computed as the ratio of the #hits/(#hits + #misses).