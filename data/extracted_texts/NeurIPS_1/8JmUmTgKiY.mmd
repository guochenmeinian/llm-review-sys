# Kolmogorov-Smirnov GAN

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We propose a novel deep generative model, the Kolmogorov-Smirnov Generative Adversarial Network (KSGAN). Unlike existing approaches, KSGAN formulates the learning process as a minimization of the Kolmogorov-Smirnov (KS) distance, generalized to handle multivariate distributions. This distance is calculated using the quantile function, which acts as the critic in the adversarial training process. We formally demonstrate that minimizing the KS distance leads to the trained approximate distribution aligning with the target distribution. We propose an efficient implementation and evaluate its effectiveness through experiments. The results show that KSGAN performs on par with existing adversarial methods, exhibiting stability during training, resistance to mode dropping and collapse, and tolerance to variations in hyperparameter settings. Additionally, we review the literature on the Generalized KS test and discuss the connections between KSGAN and existing adversarial generative models.

## 1 Introduction

Generative modeling is about fitting a model to a target distribution, usually the data. A fundamental taxonomy of models assigns them into _prescribed_ and _implicit_ statistical models , with partial overlap between the two classes. Prescribed models directly parameterize the distribution's probability

Figure 1: A schematic depiction of how the Generalized Kolmogorov-Smirnov (KS) distance between target \(_{F}\) and approximate \(_{G}\) distributions with respect to critic \(c_{}\) is computed. The critic is evaluated on samples \(x_{F}\) (\(\)) and \(x_{G}\) (\(\)) from the target and approximate distributions respectively. The \(\) threshold moves from \(-\) to \(+\) establishing a stack of level sets. At each level, the fraction of datapoints (\(\) and \(\)) below the threshold is calculated for each distribution independently. This produces the \(_{F}(_{c_{}}())\) and \(_{G}(_{c_{}}())\) curves. The Generalized KS distance is the largest absolute difference between the curves shown as \(\) in the right figure. Best viewed in color.

density function, while implicit models parameterize the generator that allows samples to be drawn from the distribution. The ultimate application of the model primarily dictates the choice between the two approaches. It does, however, have consequences regarding the available types of divergences that we can minimize when fitting the model. The divergences differ in the stability of optimization and computational efficiency, as well as statistical efficiency, which all affect the final performance of the model.

The natural approach for fitting a prescribed model is maximum likelihood estimation (MLE), equivalently formulated as minimization of Kullback-Leibler divergence. Likelihood evaluation for normalized models is straightforward. In non-normalized models, density evaluation is expensive; in this context, Hyvarinen  proposed the score matching objective, which can be interpreted as the Fisher divergence . This approach is very effective for simulation-free training of ODE/SDE-based models which are state-of-the-art in multiple domains today.

The principle driving the fitting of implicit statistical models is to push the model to generate samples that are indistinguishable from the target. An inflection point for this family of models came with the Generative Adversarial Network (GAN) , which took the principle literally and introduced an auxiliary classifier trained in an adversarial process to discriminate between the two distributions. The classification error given an optimal classifier relates to the Jensen-Shannon divergence between generator and the target. Initial work in this area involved applying heuristic tricks to deal with learning problems, namely vanishing gradients, unstable training, and mode dropping or collapse. Further advancements focused on using other distances based on the principle of adversarial learning of auxiliary models, which were supposed to have certain favorable properties with respect to the original GAN.

The Bayesian inference community has been reluctant to adopt adversarial methods , and the attempts to apply them in this context  indicate a credibility problem. A significant drawback of approximate methods is the excessive reduction of diversity in the distribution , the extremes of which lead to mode dropping . In this work, we consider another distance for training implicit statistical models, i.e., the Kolmogorov-Smirnov (KS) distance, which, to the best of our knowledge, has not been used in this context before. The distinctive feature of the KS distance is that it directly measures the coverage discrepancy of each other's credibility regions by the distributions under analysis at all confidence levels. Thus, its minimization straightforwardly leads to the correct spread of the probability mass, avoiding mode dropping, overconfidence, and mode collapse when applied with a sufficient sampling budget.

We term the proposed model as _Kolmogorov-Smirnov Generative Adversarial Network_ (KSGAN). We show how to generalize the standard KS distance to higher dimensions based on Polonik  in section 2, allowing our method to be used for multidimensional distributions. Next, in section 3, we show how to efficiently leverage the distance in an adversarial training process and show formally that the proposed algorithm leads to an alignment of the approximate and target distributions. We support the theoretical findings with empirical results presented in section 6.

## 2 Generalized Kolmogorov-Smirnov distance

We generalize the Kolmogorov-Smirnov (KS) distance (sometimes called simply Kolmogorov distance) between continuous probability distributions on one-dimensional spaces to multidimensional spaces and show that it is a metric. The test statistic of the KS test is a KS distance between empirical and target distributions (or two empirical in the case of the two-sample case). For this reason, our proposal is directly inspired by the generalization of the test introduced in Polonik .

Let us consider two probability measures \(_{F}\) and \(_{G}\) on a measurable space \((,)\), where the sample space \(\) is a vector space such as \(I\!\!R^{d}\) and \(\) is the corresponding event space; \(F:\) and \(G:\) are the cumulative distribution functions (CDFs) of \(_{F}\) and \(_{G}\) respectively.1 We say that \(_{F}=_{G}\) iff \(\;A,\;_{F}(A)=_{G}(A)\). When \(()=1\) then the KS distance is

\[D_{}(_{F},_{G}):=_{x }|F(x)-G(x)|. \]

In the multivariate case, the problem with using the KS distance as is is that on a \(d\)-dimensional space, there are \(2^{d}-1\) ways of defining a CDF. The distance has to be independent of the particular definition and thus should be the largest across all the possibilities . This, however, becomes prohibitive for any \(d>2\). In other words, the challenge comes from a multidimensional vector space not being a partially ordered set. Everything that follows in this section consists of proposing a partial order, showing that, under certain conditions, a probability distribution can be uniquely determined on its basis and operationalizing it in an optimization problem.

We begin by bringing the classical result that

\[D_{}(_{F},_{G})=_{ }|F(G^{-1}())-|, \]

where \(G^{-1}:\) is the inverse CDF also called the quantile function. Einmahl and Mason  show that there exists a natural generalization of the quantile function to multivariate distribution, which we restate below.

**Definition 1** (Generalized Quantile Function).: _Let \(:_{+}\) be a measure, and \(\) an arbitrary subset of the event space, then a function \(C_{,}():\) such that_

\[C_{,}()*{arg\,min}_{C }\{(C):(C)\} \]

_is called the generalized quantile function in \(\) for \(\) with respect to \(^{2}\)._

The generalized quantile function evaluated at level \(\) yields a _minimum-volume set_ whose probability is at least \(\), and it is the smallest with respect to \(\) such set in \(\), thus the name. For the remainder of this paper, we assume that \(\) is the Lebesgue measure.

It may seem that it is enough to plug \(C_{_{G},}()\) in place of \(G^{-1}()\) and \(_{F}\) in place of \(F\) in eq. (2) to establish the Generalized KS distance but it turns out that such a distance does not satisfy the positivity condition \(D_{}(_{F},_{G})>0\) if \(_{F}_{G}\) as the example below shows.

**Example 1** (Polonik ).: _Let \(_{F}\) be the probability measure of a chi distribution with one degree of freedom \(^{2}}\) which has support on \(_{+}\) and \(_{G}\) the probability measure of a standard Gaussian distribution \((0,1)\) which has support on the whole I-2.1ptR\). Given \(=\) we have_

\[_{F}(C_{_{G},}())=\; , \]

_while clearly \(_{F}_{G}\). The statement in eq. (4) is easy to show by observing that \( x[0,)\) the density of \(_{F}\) is twice the density of \(_{G}\) and \(C_{_{G},}()\) are intervals centered at 0._

Instead, a solution based on the quantile functions of both distributions is needed, which we present in definition 2.

**Definition 2** (Generalized Kolmogorov-Smirnov distance).: _Let the Generalized Kolmogorov-Smirnov distance be formulated as follows:_

\[D_{}(_{F},_{G}):=_{ {c}\\ C\{C_{_{G},},C_{_{F},}\}} [|_{F}(C())-_{G}(C())|]. \]

Such distance is symmetric, satisfying the triangle inequality as shown in appendix A.1. For the remainder of this section, we will show that the Generalized KS distance in eq. (5) meets the necessary \(D_{}(,)=0\) and sufficient \(D_{}(_{F},_{G})>0\) if \(_{F}_{G}\) conditions to consider it a metric. In the proof, we will rely on the probability density function of \(\) with respect to a reference measure \(\), which we denote with \(p:[0,)\). Let

\[_{p}():=\{x:p(x)\} \]

denote the _density level set of \(p\) at level \( 0\)_ (also called the highest density region ), and let \(_{p}:=\{_{p}(): 0\}\). The following observations about level sets will introduce the fundamental tools to prove the necessary and sufficient conditions for the generalized KS distance.

**Remark 1** (The silhouette ).: _For any density \(p\), the following holds_

\[p(x)=_{0}^{}_{_{p}()}(x), \]

_where \(_{C}\) denotes the indicator function of a set \(C\). The RHS of eq. (7) is called the silhouette._An immediate consequence of remark 1 is that \(_{p}\) ordered with respect to \( 0\) fully characterizes \(\), because \(p\) does. Graphically, the silhouette is a multidimensional stack of level sets.

**Remark 2**.: _Density level sets are minimum-volume sets  The quantity \((C)-(C)\) is maximized over \(\) by \(_{p}()\), and thus if \(_{p}()\), then \(_{p}()=C_{,}()\)3 at level \(=(_{p}())= p(x)_{[,)}(p (x))x\)._

Below, we present the fundamental theoretical result behind the proposed method, which restates Lemma 1.2. of Polonik .

**Theorem 1** (Necessary and sufficient conditions).: _Let \(\) be a measure on \((,)\). Suppose that \(_{F}\) and \(_{G}\) are probability measures on \((,)\) with densities (with reference measure \(\)) \(f\) and \(g\) respectively. Assuming that_

1. \(_{f}_{g}\)_;_
2. \(C_{_{F},}()\) _and_ \(C_{_{G},}()\) _are uniquely determined_4 _in_ \(\) _with respect to_ \(\)__

_the following two statements are equivalent:_

1. \(_{F}=_{G}\)_;_
2. \(D_{}(_{F},_{G})=0\)_._

See proof in Appendix A.

Meeting assumption **A.1** is a demanding challenge, almost equivalent to learning the target distribution. Below, we propose a relaxation of it, which we will use to show the validity of our method.

**Theorem 2** (Relaxation of assumption **A.1**).: _Theorem 1 holds if assumption **A.1** is relaxed to the case that \(\) contains sets that are uniquely determined with density level sets of \(_{F}\) and \(_{G}\) up to a set \(C\) such that_

\[_{C^{}^{C}}\;_{F}(C^{})= _{G}(C^{}), \]

_and let \(r:=_{F}(C)=_{G}(C)\), then the supremum in statement **S.2** is restricted to \([0,1-r]\)._

See proof in Appendix A.

## 3 Kolmogorov-Smirnov GAN

For the remainder of the paper, we will consider \(_{F}\) as the target distribution represented by a dataset \(\{x_{F}\}\), and \(_{G}\) as the approximate distribution that we want to train by minimizing the Generalized KS distance in eq. (5) with Stochastic Gradient Descent. We model \(_{G}\) as a pushforward \(g_{\,\#}_{Z}\) of a simple (e.g., Gaussian, or Uniform) latent distribution \(_{Z}\) supported on \(\), with a neural network \(g_{}:\), parameterized with \(\), which we call the _generator_.

The major challenge in utilizing eq. (5) is the necessity of finding the \(C_{,}()\) terms which is an optimization problem on its own. The idea that we propose in this work is to amortize the procedure by modeling the generalized quantile functions \(C_{_{F},}()\) and \(C_{_{G},}()\) with additional neural networks which have to be trained in parallel to the generator \(g_{}\). Therefore, our method is based on adversarial training , where optimization proceeds in alternating phases of minimization and maximization for different sets of parameters. Hence the name of the proposed method, the _Kolmogorov-Smirnov Generative Adversarial Network_.

### Neural Quantile Function

The generalized quantile function defined in definition 1 is an infinite-dimensional vector function \(C_{,}: C\). Such objects do not have an expressive, explicit representation that allows for gradient-based optimization. Therefore, we use an implicit representation inspired by density level sets in eq. (6). We propose to use _neural level sets_ defined in definition 3 that are modeled by a neural network \(c:\), which we will refer to as the _critic_.

**Definition 3** (Neural level set).: _Given a neural network \(c:\), the neural level set at level \(\) is defined as5_

\[_{c}():=\{x:c(x)\},_{c}:=\{ _{c}():\}. \]

Neural level sets are used, for example, in image segmentation  and surface reconstruction from point clouds . They fit our application because for computing the Generalized KS distance in eq. (5), the explicit materialization of generalized quantiles is not required as long as the probability measure can be efficiently evaluated on the implicitly specified sets. We set \(=_{c}\), and thus \(C_{,_{c}}()=_{c}(_{})\), with \(_{}=*{arg\,min}_{}\{: (_{c}())\}\). For a probability measure \(^{}\) the following holds:

\[^{}(C_{,_{c}}())=_{x ^{}}[_{(-,_{}]}(c(x)) ], \]

which shows that the terms in eq. (5) under neural level sets can be Monte-Carlo estimated given samples from the respective distributions. Assumption **A.2** is satisfied by neural level sets by construction.

The formulation of the Generalized KS distance in eq. (5) includes two generalized quantile functions \(C_{_{F},}()\) corresponding to target distribution \(_{F}\) and \(C_{_{G},}()\) corresponding to the approximate distribution \(_{G}\). Both have to be modeled with the respective neural networks \(c_{_{F}}\) and \(c_{_{G}}\), where we use \(=\{_{F},_{G}\}\) to denote the joint set of their parameters. In section 3.3, we show how to parameterize both critics with a single neural network. We set \(=_{c_{_{F}}}_{c_{_{G}}}\).

### Optimizing generator's parameters \(\)

The Generalized KS distance in eq. (5) is a supremum over a unit interval and two functions; thus, it can be upper-bounded as

\[D_{}(_{F},_{G})_{C \{C_{_{G},},C_{_{F},}\}}_{ }[|_{F}(C())-_{G}(C() )|]. \]

Next, we plug in \(=_{c_{_{F}}}_{c_{_{G}}}\) to eq. (11) and use eq. (10) to get generator's objective:

\[_{g}=_{c_{}\{c_{_{G}},c_{_{F}}\}}_{ }[[_{x_{F}}[_{(-,]}(c_{}(x))]-_{x_{G}} [_{(-,]}(c_{}(x))]]. \]

In practice, the expectations in eq. (12) are estimated on finite samples from the two distributions, i.e. \(\{x_{F}\}\) mentioned before, and \(\{x_{G}\}\) sampled from the approximate distribution \(_{G}\) using the reparametrization trick to facilitate backpropagation of gradients. Therefore, the two terms become step functions in \(\), and the supremum is located on one of the steps. That way, a line search on \(\) reduces to a maximum over a finite set. To preserve the differentiability of the cost function calculated in this way, we apply Straight-through Estimator  in place of indication function \(\). A schematic depiction of the process for a single critic is shown in fig. 1.

### Optimizing critics' parameters \(\)

By optimizing critics' parameters \(\), we want to satisfy assumption **A.1** so that Generalized KS distance becomes a metric. For the problem posed in such a way, we lack supervision, i.e., we do not know the target sets' shapes. However, we can reformulate the problem as an estimation of the density functions of the two considered measures \(_{F}\) and \(_{G}\) and use the obtained approximate density models to build level sets. We can constitute an optimization problem for such a task based solely on finite sets of samples, which we have for \(_{F}\) and can arbitrarily generate from \(_{G}\). As the estimator, we propose to use the Energy-based model (EBM) , which, thanks to the lack of constraints in the choice of architecture, can be very expressive while having favorable computational complexity at inference. To carry out EMB training effectively, we will introduce a new min-max game, the "min phase" of which will turn out to be the initial objective in eq. (5), and in this way, we will close the adversarial cycle.

Let the critic \(c_{_{F}}(x)\) serve as the energy function. The density given by the EBM is then \(p_{c_{_{F}}}(x)=(-c_{_{F}}(x))/Z_{c_{_{F}}}\), where \(Z_{c_{_{F}}}=(-c_{_{F}}(x))x\) is the normalizing constant called partitionfunction. The standard technique for learning the model given target data distribution \(_{F}\) is MLE, where the likelihood

\[_{x_{F}}[ p_{c_{_{F}}}(x)]=_{x _{F}}[-c_{_{F}}(x)]- Z_{c_{_{F}}} \]

is maximized wrt \(_{F}\). An unbiased estimate of the gradient of the second term can be obtained with samples from the EBM itself, typically achieved with MCMC sampling. Many approaches to avoid this expensive procedure have been described in the literature , and among them, the one based on adversarial training  is the most appealing to us. It introduces an auxiliary distribution \(_{aux(F)}\), such that the gradient of eq.13 wrt \(_{F}\) is approximated with the gradient of

\[_{x_{F}}[-c_{_{F}}(x)]-_{x _{aux(F)}}[-c_{_{F}}(x)]. \]

Consequently, an additional objective \(_{aux(F)}\) must be introduced, the optimization of which will lead to the alignment of \(_{aux(F)}\) and \(_{c_{_{F}}}\), where \(_{c_{_{F}}}\) denotes the probability distribution with density \(p_{c_{_{F}}}(x)\). We take an analogous approach to estimate \(c_{_{G}}(x)\).

When we (i) set \(c_{_{G}}(x):=-c_{_{F}}(x)\), and (ii) repurpose \(_{G}\) as \(_{aux(F)}\) and \(_{F}\) as \(_{aux(G)}\), we show in appendixA.2 that the MLE objectives for the critics - now, denoted as \(c_{}\) - simplify as \(_{c}=_{x_{G}}[c_{}(x)]-_{x _{F}}[c_{}(x)]\), which is then maximized in an adversarial game against the Generalized KS distance in eq.5.

```
Input : Target distribution \(_{F}\); latent distribution \(_{Z}\); generator network \(g_{}\); critic network \(c_{}\); number of critic updates \(k_{}\); number of generator updates \(k_{}\); score penalty weight \(\); Output : Trained model \(_{G}\) approximating \(_{F}\); repeat
1fore\(i=1\) to \(k_{}\)do
2 Draw batch \(\{x\}_{F}\) and \(\{z\}_{Z}\) ; // generator's inner loop \(\{c_{F}\}\{c_{}(x):\{x\}\}\) and \(\{c_{G}\}\{c_{}(g_{}(z)):\{z\}\}\);
3\(\{\}\{c_{F}\}\{c_{G}\}\);
4\(_{g,F}_{\{\}}|}_{\{c_ {G}\}}_{(-,]}(c_{G})-|}_{\{c_{F}\}} _{(-,]}(c_{F})|\);
5\(_{g,G}_{\{\}}||}_{\{c_ {F}\}}_{(-,-]}(-c_{F})-|}_{\{c_{G}\}} _{(-,-]}(-c_{G})|\);
6\(_{g}_{g,F}+_{g,G}\);
7 Update \(\) by using \(_{g}}{}\) to minimize \(_{g}\);
8untilnot converged; return\(g_{\#}_{Z}\)
```

**Algorithm 1**Learning a generative model by minimizing Generalized KS distance.

The standard approach for aligning the auxiliary distributions with their targets is to use the Kullback-Leibler divergence. We propose using the Generalized KS distance instead. We set \(_{aux(F)}=D_{}(_{G},_{c_{ }})\) and \(_{aux(_{G})}=D_{}(_{F}, _{-c_{}})\). By analyzing these objectives in the fashion of section3.2, we note that \(_{aux(_{G})}\) is the same as our original objective \(D_{}(_{F},_{G})\) - which is symmetric - when we approximate sampling from \(_{c_{}}\) with the target distribution \(_{F}\). Analogously for \(_{aux(_{G})}\) where sampling from \(_{-c_{}}\) is approximated with \(_{G}\). Therefore, we have shown that the auxiliary objectives are already integrated into the adversarial game.

In practice, we find the _score penalty_ regularizer of Kumar et al. , derived from the score matching objective, helpful to stabilize training. Therefore, we subtract it from \(_{c}\) weighted by a hyperparameter \(\). In this way, we get a critic that is smoother and, therefore, generates regular level sets that facilitate optimization. We summarize the proposed training procedure in algorithm1.

## 4 Discussion

In section 3.3, where we justify the choice of the critic's objective function, we refer to methods for training EBMs, which are approximate density distribution models. Thus, the reader can expect that our proposed critic \(_{}\) in the limit of convergence of the algorithm will become a source of information about the density distribution of the target distribution \(_{F}\) accompanying the model that generates samples \(_{G}\). However, this does not happen as a consequence of the design choice (i), that is, the setup of \(c_{_{F}}=-c_{_{G}}=c_{}\). An EBM can only be equivalent to its inverse in the case of a uniform distribution. In addition, because of design choice (ii), during training, the critic is not evaluated outside of the support of \(_{F}\) and \(_{G}\) and, therefore, can reach arbitrary values there. Despite these observations, the Generalized KS distance present in our algorithm exposes sufficient conditions because of theorem 2.

The feature distinguishing KSGAN from other adversarial generative modeling approaches is that regardless of the outcome of the critic's inner problem, minimizing eq. (5) is justified because Generalized KS distance, despite not meeting assumption **A.1**, is a pseudo-metric . For comparison, the dual representation of Wasserstein distance, used in WGAN  requires attaining the supremum in the inner problem.

The distances used for training generative models all fall into either the category of \(f\)-divergences \(D_{f}(_{F},_{G})=_{}f( _{F}/_{G})_{G}\) or integral probability metrics (IPMs) \(D_{}(_{F},_{G})=_{f}| _{_{F}}f(x)-_{x_{G}}f(x)|\). The classical one-dimensional KS distance is an instance of IPM with \(=\{_{(-,t]}|t I\!\!R\}\) or \(=\{_{G^{-1}()}|\}\) when having access to the inverse CDF of one of the distributions based on eq. (2). One can see the Generalized KS distance from the perspective of IPM with \(=\{_{C()}|\,\&\,C\{C_{_{ F,}},C_{_{G},}\}\}\). Assuming direct access to \(C_{_{F},}\) and \(C_{_{G},}\), for example when both \(_{F}\) and \(_{G}\) are Normalizing Flows [24; 34], measuring the distance comes down to a line search.

## 5 Related work

The need to generalize the KS test, and therefore distance, to multiple dimensions arose naturally from the side of practitioners who collected such data and wished to test related hypotheses. It was first addressed by Peacock , where a two-dimensional test for applications in astronomy was proposed. It involves considering all possible orders in this space and using the one that maximizes the distance between the distributions. A modification of this procedure has been proposed by Fasano and Franceschini  where only four candidate CDFs have to be considered, causing the test to be applicable in three dimensions, with eight candidates, under similar computational constraints. Chronologically, the following approach was the one on which we base our work, proposed in Polonik  but made possible by the author's earlier work [36; 37]. To the best of our knowledge, the first work that practically uses the theory developed by Polonik is Glazer et al. , which we recommend as an introduction to our work. It proposes applying the Generalized KS test based on the support vector machines for detecting distribution shifts in data streams.

As an instance of the adversarial generative modeling family, our work is related to all the countless GAN  follow-ups. We highlight those that study the learning process from the perspective of the distance being minimized. The work of Arjovsky and Bottou  provides a formal analysis of the heuristic tricks used for stabilizing the training of GANs. The \(f\)-GAN  proposes a unified training framework targeting \(f\)-divergences, which relies on a variational lower bound of the objective that results in the adversarial process. Approaches relying on the integral probability metric include FisherGAN , the Generative Moment Matching Networks  based on MMD, just like the later, more sophisticated MMD GAN , and finally the Wasserstein GAN (WGAN)  with the WGAN-GP follow-up  which shares common features with our work. Our maximum likelihood approach to fitting the critic results in the same functional form of the loss as WGAN(-GP) uses. In addition, the score penalty we use is similar to the gradient penalty of WGAN-GP.

## 6 Experiments

We evaluate the proposed method on eight synthetic 2D distributions (see appendix B.1 for details) and two image datasets, i.e. MNIST  and CIFAR-10 . We compare against other adversarialmethods, GAN and WGAN-GP, using the same neural network architectures and training hyperparameters unless specified otherwise (see appendix C for details). All the quantitative results are presented based on five random initializations of the models. The source code for all the experiments is provided in anonymous code repository.

In all KSGAN experiments, we relax the maximum in line 11 and line 12 of algorithm 1 with sample average. In all experiments, we re-use the last batch of samples from the latent distribution (and target distribution in the case of KSGAN) from the critic's optimization inner loop as the first batch for the generator's optimization inner loop.

### Synthetic distributions

Analyzing adversarial methods on synthetic, low-dimensional distributions is not popular. However, we conduct such an experiment because we are interested in whether the model generates samples from the support of the target distribution and how accurately it approximates the distribution. Working with small-dimensional distributions, we do not have to be as concerned about the curse of dimensionality when calculating sample-based distances, and we can visually compare the resulting histograms.

In table 1, we report the squared population MMD  between target and approximate distributions, computed with Gaussian kernel on 65536 samples from each distribution. Details about how we chose the kernel's bandwidth can be found in appendix B.1. GAN and WGAN-GP fail to converge with \(k_{}=k_{}=1\) (we do not report the results to economize on space); thus, we set \(k_{}=5\) for them. The proposed KSGAN with \(k_{}=1\) performs at a similar level to WGAN-GP, the better of the two former, despite using five times less training budget. We present additional results on the synthetic datasets in appendix D.1, which include performance with different training dataset sizes, non-default hyper-parameter setups for KSGAN, and histograms of the samples for qualitative comparison.

### Mnist

We use the \(50000\) training instances to train the models, and based on visual inspection of the generated samples (reported in appendix D.2), we conclude that all the methods achieve comparable, high samples quality. To assess the quality of the distribution approximation, we use a pre-trained classifier on the same data as the generative models (details in appendix B.2). We run the same experiment on 3StackedMNIST , which has 1000 modes. We report the results in table 2.

In this experiment, we set the training budget for all methods to \(k_{}=1\), \(k_{}=1\) for a fair comparison. We find that all methods always recover all the modes with the standard MNIST target. However, GAN fails to distribute the probability mass uniformly between the digits. As the number of modes increases with the 3StackedMNIST target, GAN demonstrates its inferiority to other methods by losing 198 modes on average (four initialization cover approx. 985 modes, and one fails to converge, achieving only 98 modes). WGAN-GP and KSGAN consistently recover all the modes while being on par regarding KL divergence, which differs little between networks' initialization.

    & \), \(k_{}\))} \\  Distribution & GAN (5, 1) & WGAN-GP (5, 1) & KSGAN (1, 1) \\  swissroll & 0.00337 (0.001023) & 0.00029 (0.000119) & 0.00039 (0.000100) \\ circles & 0.00298 (0.001501) & 0.00027 (0.000215) & 0.00049 (0.000240) \\ rings & 0.00200 (0.001264) & 0.00013 (0.000082) & 0.00043 (0.000162) \\ moons & 0.00141 (0.000757) & 0.00035 (0.000136) & 0.00053 (0.000189) \\
8gaussians & 0.00357 (0.002719) & 0.00035 (0.000248) & 0.00032 (0.000277) \\ pinwheel & 0.00166 (0.001451) & 0.00027 (0.000184) & 0.00040 (0.000086) \\
2spirals & 0.00093 (0.000822) & 0.00027 (0.000191) & 0.00044 (0.000232) \\ checkerboard & 0.00143 (0.000899) & 0.00038 (0.000296) & 0.00086 (0.000468) \\   

Table 1: Squared population MMD (\(\)) between test data and samples from the methods trained on 65536 samples, averaged over five random initializations with the standard deviation calculated with Bessel’s correction in the parentheses. The proposed KSGAN with \(k_{}=1\) performs on par with the WGAN-GP trained with five times the budget \(k_{}=5\). See appendix D.1 for qualitative comparison.

### Cifar-10

We use the \(50000\) training instances to train the models and report the generated samples in appendix D.3. We train the models in a fully unconditional manner, i.e., not using the class information at all - contrary to many unconditional models that use class information in normalization layers. We quantify the quality of fitted models by computing the Inception Score (IS)  and Frechet inception distance (FID)  from the test set and report the results in table 3 based on five random initializations. For reference, in the table, we include the IS of the training dataset and the FID between the training and test sets.

In this experiment, we set the training budget for all methods to \(k_{}=1\), \(k_{}=1\) for a fair comparison. All models fail to accurately approximate the target distribution, which is evident from a quantitative comparison in table 3 and a qualitative one in appendix D.3. KSGAN is characterized by the lowest variance between initializations among the methods considered.

## 7 Conclusions and future work

In this work, we investigated the use of Generalized Kolmogorov-Smirnov distance for training deep implicit statistical models, i.e., generative networks. We proposed an efficient way to compute the distance and termed the resulting model Kolmogorov-Smirnov Generative Adversarial Network because it uses adversarial learning. Based on the empirical evaluation of the proposed model, the results of which we report, we conclude that it can be considered as an alternative to existing models in its class. At the same time, we point out that many properties of KSGAN have not been studied, and we leave this as a future work direction.

Interesting aspects to explore are the characteristics of learning dynamics with the number of generator updates exceeding the number of critic updates, alternative ways to train the critic, and alternative representations of generalized quantile sets. The natural scaling of the Generalized KS distance may also prove beneficial regarding the interpretability of learning curves, learning rate scheduling, or early stopping. In addition, we hope that our work will draw the attention of the machine learning community to the Generalized KS distance, applications of which remain to be explored.

    &  &  \\  Method (\(k_{}\), \(k_{}\)) & \# modes \(\) & KL \(\) & \# modes \(\) & KL \(\) \\  GAN (1,1) & 10 (0.00) & 0.6007 (0.27550) & 808 (396.91) & 1.4160 (1.36819) \\ WGAN-GP (1,1) & 10 (0.00) & 0.0087 (0.00499) & 1000 (0.00) & 0.0336 (0.00461) \\ KSGAN (1,1) & 10 (0.00) & 0.0056 (0.00045) & 1000 (0.00) & 0.0362 (0.00534) \\   

Table 2: The number of captured modes and Kullback-Leibler divergence between the distribution of sampled digits and target uniform distribution averaged over five random initializations with the standard deviation calculated with Bessel’s correction in the parentheses. All the methods were trained with the same budget \(k_{}=1\), \(k_{}=1\). WGAN-GP and KSGAN cover all the modes in all experiments while demonstrating low KL divergence.

   Method (\(k_{}\), \(k_{}\)) & IS \(\) & FID \(\) \\  Real data & 9.7256 & 5.8600 \\ GAN (1,1) & 2.1900 (0.08303) & 47.6419 (10.6864) \\ WGAN-GP (1,1) & 2.3464 (0.08397) & 43.0660 (6.73299) \\ KSGAN (1,1) & 2.3832 (0.04066) & 39.8881 (2.42623) \\   

Table 3: Inception Score (IS) and Fréchet inception distance (FID) metrics averaged over five random initializations with the standard deviation calculated with Bessel’s correction in the parentheses. All the methods were trained with the same budget \(k_{}=1\), \(k_{}=1\). The scores for the training dataset are included in the top row, as “Real data” for reference. WGAN-GP and KSGAN perform similarly on average, while KSGAN exhibits lower variance between networks’ initialization.