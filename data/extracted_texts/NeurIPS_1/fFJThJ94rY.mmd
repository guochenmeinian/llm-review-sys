# Switching Autoregressive Low-rank Tensor Models

Hyun Dong Lee

Computer Science Department

Stanford University

hdlee@stanford.edu

&Andrew Warrington

Department of Statistics

Stanford University

awarring@stanford.edu

&Joshua I. Glaser

Department of Neurology

Northwestern University

j-glaser@northwestern.edu

&Scott W. Linderman

Department of Statistics

Stanford University

scott.linderman@stanford.edu

###### Abstract

An important problem in time-series analysis is modeling systems with time-varying dynamics. Probabilistic models with joint continuous and discrete latent states offer interpretable, efficient, and experimentally useful descriptions of such data. Commonly used models include autoregressive hidden Markov models (ARHMMs) and switching linear dynamical systems (SLDSs), each with its own advantages and disadvantages. ARHMMs permit exact inference and easy parameter estimation, but are parameter intensive when modeling long dependencies, and hence are prone to overfitting. In contrast, SLDSs can capture long-range dependencies in a parameter efficient way through Markovian latent dynamics, but present an intractable likelihood and a challenging parameter estimation task. In this paper, we propose _switching autoregressive low-rank tensor_ (SALT) models, which retain the advantages of both approaches while ameliorating the weaknesses. SALT parameterizes the tensor of an ARHMM with a low-rank factorization to control the number of parameters and allow longer range dependencies without overfitting. We prove theoretical and discuss practical connections between SALT, linear dynamical systems, and SLDSs. We empirically demonstrate quantitative advantages of SALT models on a range of simulated and real prediction tasks, including behavioral and neural datasets. Furthermore, the learned low-rank tensor provides novel insights into temporal dependencies within each discrete state.

## 1 Introduction

Many time series analysis problems involve jointly segmenting data and modeling the time-evolution of the system within each segment. For example, a common task in computational ethology (Datta et al., 2019) -- the study of natural behavior -- is segmenting videos of freely moving animals into states that represent distinct behaviors, while also quantifying the differences in dynamics between states (Wiltschko et al., 2015; Costacurta et al., 2022). Similarly, discrete shifts in the dynamics of neural activity may reflect changes in underlying brain state (Saravani et al., 2019; Recanatesi et al., 2022). Model-based segmentations are experimentally valuable, providing an unsupervised grouping of neural or behavioral states together with a model of the dynamics within each state.

One common probabilistic state space model for such analyses is the _autoregressive hidden Markov model_ (ARHMM) (Ephraim et al., 1989). For example, MoSeq (Wiltschko et al., 2015) uses ARHMMs for unsupervised behavioral analysis of freely moving animals. ARHMMs learn a set of linear autoregressive models, indexed by a discrete state, to predict the next observation as a function of previous observations. Inference in ARHMMs then reduces to inferring which ARprocess best explains the observed data at each timestep (in turn also providing the segmentation). The simplicity of ARHMMs allows for exact state inference via message passing, and closed-form updates for parameter estimation using expectation-maximization (EM). However, the ARHMM requires high order autoregressive dependencies to model long timescale dependencies, and its parameter complexity is quadratic in the data dimension, making it prone to overfitting.

_Switching linear dynamical systems_ (SLDS) (Ghahramani and Hinton, 2000) ameliorate some of the drawbacks of the ARHMM by introducing a low-dimensional, continuous latent state. These models have been used widely throughout neuroscience (Saravani et al., 2019; Petreska et al., 2011; Linderman et al., 2019; Glaser et al., 2020; Nair et al., 2023). Unlike the ARHMM, the SLDS can capture long timescale dependencies through the dynamics of the continuous latent state, while also being much more parameter efficient than ARHMMs. However, exact inference in SLDSs is intractable due to the exponential number of potential discrete state paths governing the time-evolution of the continuous latent variable. This intractability has led to many elaborate and specialized approximate inference techniques (Ghahramani and Hinton, 2000; Barber, 2006; Fox, 2009; Murphy and Russell, 2001; Linderman et al., 2017; Zoltowski et al., 2020). Thus, the SLDS gains parameter efficiency at the expense of the computational tractability and statistical simplicity of the ARHMM.

We propose a new class of unsupervised probabilistic models that we call _switching autoregressive low-rank tensor_ (SALT) models. Our novel insight is that when you marginalize over the latent states of a linear dynamical system, you obtain an autoregressive model with full history dependence. However, these autoregressive dependencies are not arbitrarily complex -- they factor into a low-rank tensor that can be well-approximated with a finite-history model. We formalize this connection in Proposition 1. SALT models are constrained ARHMMs that leverage this insight. Rather than allowing for arbitrary autoregressive dependencies, SALT models are constrained to be low-rank. The low-rank property allows us to construct a low-dimensional continuous description of the data, jointly with the discrete segmentation provided by the switching states. Thus, SALT models inherit the experimentally useful representations and parsimonious parameter complexity of an SLDS, as well as the ease of inference and estimation of ARHMMs. We demonstrate the advantages of SALT models empirically using synthetic data as well as real neural and behavioral time series. Finally, in addition to improving predictive performance, we show how the low-rank nature of SALT models can offer new insights into complex systems, like biological neural networks.

## 2 Background

This section introduces the notation used throughout the paper and describes preliminaries on low-rank tensor decomposition, vector autoregressive models, switching autoregressive models, linear dynamical systems, and switching linear dynamical systems.

NotationWe follow the notation of Kolda and Bader (2009). We use lowercase letters for scalar variables (e.g. \(a\)), uppercase letters for scalar constants (e.g. \(A\)), boldface lowercase letters for vectors (e.g. \(\)), boldface uppercase letters for matrices (e.g. \(\)), and boldface Euler script for tensors of

Figure 1: **SALT imposes a low-rank constraint on the autoregressive tensor: (A) The probabilistic graphical model of an ARHMM. (B) An example multi-dimensional time series generated from an ARHMM. Background color indicates which discrete state (and hence autoregressive tensor) was selected at each time. (C) In SALT, each autoregressive dynamics tensor of an ARHMM is parameterized as a low-rank tensor.**

order three or higher (e.g. \(\)). We use \(_{i:i}\), \(_{:j:}\), and \(_{::k}\) to denote the horizontal, lateral, and frontal slices respectively of a three-way tensor \(\). Similarly, we use \(_{i:}\) and \(_{:j}\) to denote the \(i^{th}\) row and \(j^{th}\) column of a matrix \(\). \(\) represents the vector outer product between vectors \(\) and \(\). The \(n\)-mode tensor-matrix (tensor-vector) product is represented as \(_{n}\) (\(_{n}\)). We denote the vectorization of an \(n\)-way tensor \(\), with dimensions \(D_{1:n}\), as \(()\). This is performed by successively flattening the last dimensions of the tensor, and results in a vector of size equal to the product of the dimensions of the tensor. We denote the mode-\(n\) matcritization of a tensor \(\) as \(_{(n)}\). This is defined as the stack of vectors resulting from vectorizing the matrix (or tensor) defined by each slice through the \(n^{}\) dimension. This results in a matrix with leading dimension \(D_{n}\), and second dimension equal to the product of the sizes of the other dimensions. We will denote a \(T\)-length time series of \(N\)-dimensional observed data as \(^{N T}\). Note that we will use the shorthand \(_{t}^{N}\) to denote the observation at time \(t\), and \(y_{j,t}\) to denote the \(j^{}\) element in the \(t^{}\) observation. It will be clear from context which dimension is being indexed.

Tensor DecompositionFor \(^{N_{1} N_{2} N_{3}}\), the Tucker decomposition is defined as,

\[=_{i=1}^{D_{1}}_{j=1}^{D_{2}}_{k=1}^{D_{3}}g_{ijk}\, _{:i}_{:j}_{:k}, \]

where \(_{:i}\), \(_{:j}\), and \(_{:k}\) are the columns of the factor matrices \(^{N_{1} D_{1}}\), \(^{N_{2} D_{2}}\), and \(^{N_{3} D_{3}}\), respectively, and \(g_{ijk}\) are the entries in the core tensor \(^{D_{1} D_{2} D_{3}}\).

The CANDECOMP/PARAFAC (CP) decomposition is a special case of the Tucker decomposition, with \(D_{1}=D_{2}=D_{3}\) and a diagonal core tensor \(\).

Vector autoregressive modelsLet \(^{N T}\) denote a multivariate time series with \(_{t}^{N}\) for all \(t\). An order-\(L\) vector autoregressive (VAR) model with Gaussian innovations is defined by,

\[_{t}(_{j=1}^{N}_{k=1}^{L}_{:jk }y_{j,t-k}+,\,), \]

where \(^{N N L}\) is the autoregressive tensor, whose frontal slice \(_{:::l}\) is the dynamics matrix for lag \(l\), \(^{N}\) is the bias, and \(^{N N}_{ 0}\) is a positive semi-definite covariance matrix. The parameters \(=(,,)\) can be estimated via ordinary least squares (Hamilton, 2020).

We note that, to our knowledge, there is no clear consensus on the best way to regularize the potentially large parameter space of vector autoregressive (hidden Markov) models; several possibilities exist, see, e.g., Melnyk and Banerjee (2016) or Ni and Sun (2005). Many regularizers and priors are difficult to work with, and so are not widely used in practice. Beyond this, even well-regularized ARHMMs do not natively capture interpretable low-dimensional dynamics, as both SALT and SLDS models do (see Figure 3). These low-dimensional continuous representations are as experimentally useful as the discrete segmentation, and hence are a key desiderata for any method we consider.

Switching autoregressive modelsOne limitation of VAR models is that they assume the time series is stationary; i.e. that one set of parameters holds for all time steps. Time-varying autoregressive models allow the autoregressive process to change at various time points. One such VAR model, referred to as a switching autoregressive model or autoregressive hidden Markov model (ARHMM), switches the parameters over time according to a discrete latent state (Ephraim et al., 1989). Let \(z_{t}\{1,,H\}\) denote the discrete state at time \(t\), an ARHMM defines the following generative model,

\[z_{t}(^{(z_{t-1})}),_{t}(_{j=1}^{N}_{k=1}^{L}_{:jk}^{(z_{t})}y_{j,t-k}+^{(z_{t})},\,^{(z_{t})}), \]

where \(^{(h)}\{^{(h)}\}_{h=1}^{H}\) is the the \(h\)-th row of the discrete state transition matrix.

A switching VAR model is simply a type of hidden Markov model, and as such it is easily fit via the expectation-maximization (EM) algorithm within the Baum-Welch algorithm. The M-step amounts to solving a weighted least squares problem.

Linear dynamical systemsThe number of parameters in a VAR model grows as \((N^{2}L)\). For high-dimensional time series, this can quickly become intractable. Linear dynamical systems (LDS) (Murphy, 2012) offer an alternative means of modeling time series via a continuous latent state \(_{t}^{S}\),

\[_{t}(_{t-1}+,\,),_{t}(_{t}+,\, ), \]

where \(_{ 0}^{S S}\) and \(_{ 0}^{N N}\). Here, the latent states follow a first-order VAR model, and the observations are conditionally independent given the latent states. As we discuss in Section 3.3, marginalizing over the continuous latent states renders \(_{t}\) dependent on the preceding observations, just like in a high order VAR model.

Compared to the VAR model, however, the LDS has only \((S^{2}+NS+N^{2})\) parameters if \(\) is a full covariance matrix. This further reduces to \((S^{2}+NS)\) if \(\) is diagonal. As a result, when \(S N\), the LDS has many fewer parameters than a VAR model. Thanks to the linear and Gaussian assumptions of the model, the parameters can be easily estimated via EM, using the Kalman smoother to compute the expected values of the latent states.

Switching linear dynamical systemsA switching LDS combines the advantages of the low-dimensional continuous latent states of an LDS, with the advantages of discrete switching from an ARHMM. Let \(z_{t}\{1,,H\}\) be a discrete latent state with Markovian dynamics (3), and let it determine some or all of the parameters of the LDS (e.g. \(\) would become \(^{(z_{t})}\) in (4)). We note that SLDSs often use a _single-subspace_, where \(\), \(\) and \(\) are shared across states, reducing parameter complexity and simplifying the optimization.

Unfortunately, parameter estimation is considerably harder in SLDS models. The posterior distribution over all latent states, \(p(_{1:T},_{1:T}_{1:T},)\), where \(\) denotes the parameters, is intractable (Lerner, 2003). Instead, these models are fit via approximate inference methods like MCMC (Fox, 2009; Linderman et al., 2017), variational EM (Ghahramani and Hinton, 2000; Zoltowski et al., 2020), particle EM (Murphy and Russell, 2001; Doucet et al., 2001), or other approximations (Barber, 2006). Selecting the appropriate fitting and inference methodologies is itself non-trivial hyperparameter. Furthermore, each method also brings additional estimation hyperparameters that need to be tuned prior to even fitting the generative model. We look to define a model that enjoys the benefits of SLDSs, but avoids the inference and estimation difficulties.

## 3 SALT: Switching Autoregressive Low-rank Tensor Models

Here we formally introduce SALT models. We begin by defining the generative model (also illustrated in Figure 1), and describing how inference and model fitting are performed. We conclude by drawing connections between SALT and SLDS models.

### Generative Model

SALT factorizes each autoregressive tensor \(}^{(h)}\) for \(h\{1,,H\}\) of an ARHMM as a product of low-rank factors. Given the current discrete state \(z_{t}\), each observation \(_{t}^{N}\) is modeled as being normally distributed conditioned on \(L\) previous observations \(_{t-1:t-L}\),

\[z_{t} (^{(z_{t-1})}), \] \[_{t} }}{{}}( _{j=1}^{N}_{k=1}^{L}_{,:jk}^{(z_{t})}y_{j,t-k}+ ^{(z_{t})},^{(z_{t})}),\] (6) \[}_{}^{(z_{t})} =_{i=1}^{D_{1}}_{j=1}^{D_{2}}_{k=1}^{D_{3}}g_{ijk}^{ (z_{t})}\,_{:i}^{(z_{t})}_{:j}^{(z_{t})}_{:k}^{(z_{t})}, \]

where \(_{:i}^{(z_{t})}\), \(_{:j}^{(z_{t})}\), and \(_{:k}^{(z_{t})}\) are the columns of the factor matrices \(^{(z_{t})}\!\!^{N D_{1}}\), \(^{(z_{t})}\!\!^{N D_{2}}\), and \(^{(z_{t})}\!\!^{L D_{3}}\), respectively, and \(g_{ijk}^{(z_{t})}\) are the entries in the core tensor \(^{(z_{t})}\!\!^{D_{1} D_{2} D_{3}}\). The vector \(^{(z_{t})}\!\!^{N}\) and positive definite matrix \(^{(z_{t})}\!\!_{ 0}^{N N}\) are the bias and covariance for state \(z_{t}\). Without further restriction this decomposition is a Tucker decomposition (Kolda and Bader, 2009). If \(D_{1}=D_{2}=D_{3}\) and \(_{z_{t}}\) is diagonal, it corresponds to a CP decomposition (Kolda and Bader, 2009). We refer to ARHMM models with these factorizations as Tucker-SALT and CP-SALT respectively. Note that herein we will only consider models where \(D_{1}=D_{2}=D_{3}=D\), where we refer to \(D\) as the "rank" of the SALT model (for both Tucker-SALT and CP-SALT). In practice, we find that models constrained in this way perform well, and so this constraint is imposed simply to reduce the search space of models and could easily be relaxed.

Table 1 shows the number of parameters for order-\(L\) ARHMMs, SLDSs, and SALT. Focusing on the lag dependence, the number of ARHMM parameters grows as \((HN^{2}L)\), whereas SALT grows as only \((HDL)\) with \(D N\). SALT can also make a simplifying single-subspace constraint, where certain emission parameters are shared across discrete states.

Low-dimensional RepresentationNote that SALT implicitly defines a low-dimensional continuous representation, analogous to the continuous latent variable in SLDS,

\[ =_{j=1}^{D_{2}}_{k=1}^{D_{3}}_{:jk}^{(z_{t})} _{:j}^{(z_{t})}_{:k}^{(z_{t})}, \] \[_{t} =_{j=1}^{N}_{k=1}^{L}_{:jk}y_{j,t-k}. \]

The low-dimensional \(_{t}^{D_{1}}\) vectors can be visualized, similar to the latent states in SLDS models, to further interrogate the learned dynamics, as we show in Figure 3. Note the vector \(_{t}^{D_{1}}\), when multiplied by the output factors \(^{(z_{t})}\), is the mean of the next observation.

### Model Fitting and Inference

Since SALT models are ARHMMs, we can apply the expectation-maximization (EM) algorithm to fit model parameters and perform state space inference. We direct the reader to Murphy (2012) for a detailed exposition of EM and include only the key points here.

The E-step solves for the distribution over latent variables given observed data and model parameters. For SALT, this is the distribution over \(z_{t}\), denoted \(_{t}^{(h)}=[z_{t}=h_{1:T},]\). This can be computed exactly with the forward-backward algorithm, which is fast and stable. The marginal likelihood can be evaluated exactly by taking the product across \(t\) of expectations of (6) under \(_{t}^{(h)}\).

The M-step then updates the parameters of the model given the distribution over latent states. For SALT, the emission parameters are \(=\{^{(h)},^{(h)},^{(h)},^{( h)},^{(h)},^{(h)},^{(h)}\}_{h=1}^{H}\). We use closed-form coordinate-wise updates to maximize the expected log likelihood evaluated in the E-step. Each factor update amounts to solving a weighted least squares problem. We include just one update step here for brevity, and provide all updates in full in Appendix A. Assuming here that \(^{(h)}=\) for simplicity, the update rule for the lag factors is as follows:

\[^{(h)}=(_{t}_{t}^{(h)}}_{t} ^{(h)}(^{(h)})^{-1}}_{t}^{(h)})^{-1 }(_{t}_{t}^{(h)}}_{t}^{(h)}(^{(h)})^{-1}_{t}) \]

where \(}_{t}^{(h)}=^{(h)}^{(h)}_{(1)}( ^{(h)}_{t-1:t-L}_{D_{3}})\) and \(^{(h)}=(^{(h)})\). Crucially, these coordinate wise updates are exact, and so we recover the fast and monotonic convergence of EM.

  
**Model** & **Parameter Complexity** & **(Example from Section 5.4)** \\  SLDS & \((NS+HS^{2})\) & 2.8K \\ CP-SALT & \((H(ND+LD))\) & 8.1K \\ Tucker-SALT & \((H(ND+LD+D^{3}))\) & 17.4K \\ Order-\(L\) ARHMM & \((HN^{2}L)\) & 145.2K \\   

Table 1: Comparison of number of parameters for the methods we consider. We exclude covariance matrix parameters, as the parameterization of the covariance matrix is independent of method. Throughout our experiments, we find \(S D\).

### Connections Between SALT and Linear Dynamical Systems

SALT is not only an intuitive regularization for ARHMMs, it is grounded in a mathematical correspondence between autoregressive models and linear dynamical systems.

**Proposition 1** (Low-Rank Tensor Autoregressions Approximate Stable Linear Dynamical Systems).: _Consider a stable linear time-invariant Gaussian dynamical system. We define the steady-state Kalman gain matrix as \(=_{t}_{t}\), and \(=(-)\). The matrix \(^{S S}\) has eigenvalues \(_{1},,_{S}\). Let \(_{}=_{s}|_{s}|\); for a stable LDS, \(_{}<1\)(Davis and Vinter, 1985). Let \(n\) denote the number of real eigenvalues and \(m\) the number of complex conjugate pairs. Let \(}_{t}^{()}=[_{t}_{ 1:t-1}]\) denote the predictive mean under a steady-state LDS, and \(}_{t}^{()}\) the predictive mean under a SALT model. An order-\(L\) Tucker-SALT model with rank \(n+2m=S\), or a CP-SALT model with rank \(n+3m\), can approximate the predictive mean of the steady-state LDS with error \(\|}_{t}^{()}-}_{t}^{()}\|_ {}=(_{}^{L})\)._

Proof.: We give a sketch of the proof here and a full proof in Appendix B. The analytic form of \([_{t}_{1:t-1}]\) is a linear function of \(_{t-l}\) for \(l=1,,\). For this sketch, consider the special case where \(==\). Then the coefficients of the linear function are \(^{L}\). As all eigenvalues of \(\) have magnitude less than one, the coefficients decay exponentially in \(l\). We can therefore upper bound the approximation error introduced by truncating the linear function to \(L\) terms to \((_{}^{L})\). To complete the proof, we show that the truncated linear function can be represented exactly by a tensor regression with at most a specific rank. Thus, only truncated terms contribute to the error. 

This proposition shows that the steady-state predictive distribution of a stable LDS can be approximated by a low-rank tensor autoregression, with a rank determined by the eigenspectrum of the LDS. We validate this proposition experimentally in Section 5.1. Note as well that the predictive distribution will converge to a fixed covariance, and hence can also be exactly represented by the covariance matrices \(^{(h)}\) estimated in SALT models.

Connections with Switching Linear Dynamical SystemsWith this foundation, it is natural to hypothesize that a _switching_ low-rank tensor autoregression like SALT could approximate a _switching_ LDS. There are two ways this intuition could fail: first, if the dynamics in a discrete state of an SLDS are unstable, then Proposition 1 would not hold; second, after a discrete state transition in an SLDS, it may take some time before the dynamics reach stationarity. We empirically test how well SALT approximates an SLDS in Section 5 and find that, across a variety of datasets, SALT obtains commensurate performance with considerably simpler inference and estimation algorithms.

## 4 Related Work

Low-rank tensor decompositions of time-invariant autoregressive modelsSimilar to this work, Wang et al. (2021) also modeled the transition matrices as a third-order tensor \(}^{N N L}\) where the \(_{::l}\) is the \(l\)-th dynamics matrix. They then constrained the tensor to be low-rank via a Tucker decomposition, as defined in (1). However, unlike SALT, their model was time-invariant,did not have an ARHMM structure, or, make connections to the LDS and SLDS, as in Proposition 1.

Low-rank tensor decompositions of time-varying autoregressive modelsLow-rank tensor-based approaches have also been used to model time-varying AR processes (Harris et al., 2021; Zhang et al., 2021). Harris et al. (2021) introduced TVART, which first splits the data into \(T\) contiguous fixed-length segments, each with its own AR-1 process. TVART can be thought of as defining a \(T N N\) ARHMM dynamics tensor and progressing through discrete states at fixed time points. This tensor is parameterized using the CP decomposition and optimized using an alternating least squares algorithm, with additional penalties such that the dynamics of adjacent windows are similar. By contrast, SALT automatically segments, rather than windows, the time-series into learned and re-usable discrete states.

Zhang et al. (2021) constructed a Bayesian model of higher-order AR matrices that can vary over time. First, \(H\) VAR dynamics tensors are specified, parameterized as third-order tensors with a rank-1 CP decomposition. The dynamics at a given time are then defined as a weighted sum of the tensors, where the weights have a prior density specified by an Ising model. Finally, inference over the weights is performed using MCMC. This method can be interpreted as a factorial ARHMM, hence offering substantial modeling flexibility, but sacrificing computational tractability when \(H\) is large.

Low-rank tensor decompositions of neural networksLow-rank tensor decomposition methods have also been used to make neural networks more parameter efficient. Novikov et al. (2015) used the tensor-train decomposition (Oseledets, 2011) on the dense weight matrices of the fully-connected layers to reduce the number of parameters. Yu et al. (2017) and Qiu et al. (2021) applied the tensor-train decomposition to the weight tensors for polynomial interactions between the hidden states of recurrent neural networks (RNNs) to efficiently capture high-order temporal dependencies. Unlike switching models with linear dynamics, recurrent neural networks have dynamics that are hard to interpret, their state estimates are not probabilistic, and they do not provide experimentally useful data segmentations.

Linear dynamical systems and low-rank linear recurrent neural networksValente et al. (2022) recently examined the relationship between LDSs and low-rank linear RNNs. They provide the conditions under which low-rank linear RNNs can exactly model the first-order autoregressive distributions of LDSs, and derive the transformation to convert between model classes under those conditions. This result has close parallels to Proposition 1. Under the conditions identified by Valente et al. (2022), the approximation in Proposition 1 becomes exact with just one lag term. However, when those conditions are not satisfied, we show that one still recovers an LDS approximation with a bounded error that decays exponentially in the number of lag terms.

## 5 Results

We now empirically validate SALT by first validating the theoretical claims made in Section 3, and then apply SALT to two synthetic examples to compare SALT to existing methods. We conclude by using SALT to analyze real mouse behavioral recordings and _C. elegans_ neural recordings.

### SALT Faithfully Approximates LDS

To test the theoretical result that SALT can closely approximate a linear dynamical system, we fit SALT models to data sampled from an LDS. The LDS has \(S=7\) dimensional latent states with random rotational dynamics, where \(\) has \(n=1\) real eigenvalue and \(m=3\) pairs of complex eigenvalues, and \(N=20\) observations with a random emission matrix.

For Figure 2, we trained CP-SALT and Tucker-SALT with \(L=50\) lags and varying ranks. We first analyzed how well SALT reconstructed the parameters of the autoregressive dynamics tensor. As predicted by Proposition 1, Figure 2A shows that the mean squared errors between the SALT tensor and the autoregressive tensor corresponding to the simulated LDS are the lowest when the ranks of CP-SALT and Tucker-SALT are \(n+3m=10\) and \(n+2m=7\) respectively. We then computed log-likelihoods on 5,000 timesteps of held-out test data (Figure 2B). Interestingly, the predictive performance of both CP-SALT and Tucker-SALT reach the likelihood of the ground truth LDS model with rank \(n+2m=7\), suggesting that sometimes smaller tensors than suggested by Proposition 1 may still be able to provide good approximations to the data. We also show in Figures 2C and 2D that, as predicted, SALT models require much less data to fit than ARHMMs. We show extended empirical results and discussion on Proposition 1 in Appendix D.1.

Figure 2: **SALT approximates LDS**: Data simulated from an LDS for which \(n=1\) and \(m=3\) (see Proposition 1). **(A-B)**: Average mean squared error of the autoregressive tensor corresponding to the LDS simulation and the log-likelihood of test data, as a function of SALT rank. According to Proposition 1, to model the LDS Tucker-SALT and CP-SALT require 7 and 10 ranks respectively (indicated by vertical dashed lines). Note the _parameter_ error increases above the predicted threshold as a result of overfitting. **(C-D)**: Mean squared error of the learned autoregressive tensor and log-likelihood of test data as a function of training data.

### Synthetic Switching LDS Examples

Proposition 1 quantifies the convergence properties of low-rank tensor regressions when approximating stable LDSs. Next we tested how well SALT can approximate the more expressive _switching_ LDSs. We first applied SALT to data generated from a recurrent SLDS (Linderman et al., 2017), where the two-dimensional ground truth latent trajectory resembles a NASCAR(r) track (Figure 3A). SALT accurately reconstructed the ground truth filtered trajectories and discrete state segmentation, and yielded very similar results to an SLDS model. We also tested the ability of SALT to model nonlinear dynamics - specifically, a Lorenz attractor - which SLDSs are capable of modeling. Again, SALT accurately reconstructed ground truth latents and observations, and closely matched SLDS segmentations. These results suggest that SALT models provide a good alternative to SLDS models. Finally, in Appendix D.3, we used SLDS-generated data to compare SALT and TVART (Harris et al., 2021), another tensor-based method for modeling autoregressive processes, and find that SALT more accurately reconstructed autoregressive dynamics tensors than TVART.

### Modeling Mouse Behavior

Next we considered a video segmentation problem commonly faced in the field of computational neuroethology (Datta et al., 2019). Wiltschko et al. (2015) collected videos of mice freely behaving in a circular open field. They projected the video data onto the top 10 principal components (Figure 4A) and used an ARHMM to segment the PCA time series into distinct behavioral states. Here, we compared ARHMMs and CP-SALT with data from three mice. We used the first 35,949 timesteps of each recording, which were collected at 30Hz resolution. We used \(H=50\) discrete states and fitted ARHMMs and CP-SALT models with varying lags and ranks.

The likelihood on a held-out validation set shows that the ARHMM overfitted quickly as the number of lags increased, while CP-SALT was more robust to overfitting (Figure 4B). We compared log-likelihoods of the best model (evaluated on the validation set) on a separate held-out test set and found that CP-SALT consistently outperformed ARHMM across mice (Figure 4C).

We also investigated the quality of SALT segmentations of the behavioral data (Appendix E.3). We found that the PCA trajectories upon transition into a discrete SALT state were highly stereotyped, suggesting that SALT segments the data into consistent behavioral states. Furthermore, CP-SALT used fewer discrete states than the ARHMM, suggesting that the ARHMM may have oversegmented and that CP-SALT offers a more parsimonious description of the data.

Figure 3: **SALT reconstructs simulated SLDS data and Lorenz attractor**: (**Top row**) Observation generated from a low-dimensional trajectory. (A) shows ten observations generated from a recurrent “NASCAR” SLDS trajectory Linderman et al. (2017). (B) 20-dimensional observations generated from a Lorenz attractor (5 observed dimensions are shown). **(Middle and bottom rows)**: filtered observations and inferred low-dimensional trajectories from SLDS and SALT models. Colors indicate discrete state for ground truth (if available) and fitted models. SLDS and SALT find comparable filtered trajectories and observations. It is important to note that the latent spaces in both SLDS and SALT are only identifiable up to a linear transformation. We therefore align the latent trajectories for ease of comparison. This latent structure is reliably found by both SALT and SLDS.

### Modeling _C. elegans_ Neural Data

Finally, we analyzed neural recordings of an immobilized _C. elegans_ worm from Kato et al. (2015). SLDS have previously been used to capture the time-varying low-dimensional dynamics of the neural activity (Linderman et al., 2019; Glaser et al., 2020). We compared SLDS, ARHMM, and CP-SALT with 18 minutes of neural traces (recorded at 3Hz; \(\)3200 timesteps) from one worm, in which 48 neurons were confidently identified. The dataset also contains 7 manually identified state labels based on the neural activity.

We used \(H=7\) discrete states and fitted SLDSs, ARHMMs, and CP-SALT with varying lags and ranks (or continuous latent dimensions for SLDSs). Following Linderman et al. (2019), we searched for sets of hyperparameters that achieve \(\)90% explained variance on a held-out test dataset (see Appendix F for more details). For ARHMMs and CP-SALT, we chose a larger lag (\(L=9\), equivalent to 3 seconds) to examine the long-timescale correlations among the neurons.

We find that SALT can perform as well as SLDSs and ARHMMs in terms of held-out explained variance ratio (a metric used by previous work (Linderman et al., 2019)). As expected, we find that CP-SALT can achieve these results with far fewer parameters than ARHMMs, and with a parameter count closer to SLDS than ARHMM (as more continuous latent states were required in an SLDS to achieve \(\)90% explained variance; see Appendix F). Figure 5A shows that SALT, SLDS and ARHMM produce similar segmentations to the given labels, as evidenced by the confusion matrix having high entries on the leading diagonal (Figure 5B and Appendix F).

Figure 5C shows the one-dimensional autoregressive filters learned by CP-SALT, defined as \(_{i=1}^{D_{1}}_{j=1}^{D_{2}}_{k=1}^{D_{3}}g_{ijk}^{(h)}u_{pi}^{(h )}v_{qj}^{(h)}_{:k}^{(h)}\) for neurons \(p\) and \(q\). We see that neurons believed to be involved in particular behavioral states have high weights in the filter (e.g., SDV during the "Ventral Turn" state and SDMD during the "Dorsal Turn" state (Linderman et al., 2019; Kato et al., 2015; Gray et al., 2005; Kaplan et al., 2020; Yeon et al., 2018)). This highlights how switching autoregressive models can reveal state-dependent functional interactions between neurons (or observed states more generally). In Appendix F, we show the autoregressive filters learned by an ARHMM, an SLDS, and a generalized linear model (GLM), a method commonly used to model inter-neuronal interactions (Pillow et al., 2008). Interestingly, the GLM does not find many strong functional interactions between neurons, likely because it is averaging over many unique discrete states. In addition to its advantages in parameter efficiency and estimation, SALT thus provides a novel method for finding changing functional interactions across neurons at multiple timescales.

## 6 Discussion

We introduce switching autoregressive low-rank tensor (SALT) models: a novel model class that parameterizes the autoregressive tensors of an ARHMM with a low-rank factorization. This constraint

Figure 4: **CP-SALT consistently outperforms ARHMM on mouse behavior videos and segments data into distinct behavioral syllables: (A) An example frame from the MoSeq dataset. The models were trained on the top 10 principal components of the video frames from three mice. (B) CP-SALT and ARHMM trained with different ranks and lags. Mean and standard deviation across five seeds evaluated on a validation set are shown. CP-SALT parameterization prevents overfitting for larger lags. (C) Test log-likelihood, averaged across 5 model fits, computed from the best ARHMM and CP-SALT hyperparameters in (B). CP-SALT outperforms ARHMM across all three mice.**allows SALT to model time-series data with fewer parameters than ARHMMs and with simpler estimation procedures than SLDSs. We also make theoretical connections between low-rank tensor regressions and LDSs. We then demonstrate, with both synthetic and real datasets, that SALT offers both efficiency and interpretability, striking an advantageous balance between the ARHMM and SLDS. Moreover, SALT offers an enhanced ability to investigate the interactions across observations, such as neurons, across different timescales in a data-efficient manner.

However, SALT is not without limitations. Foremost, SALT cannot readily handle missing observations, or share information between multiple time series with variable observation dimensions. "Hierarchical SALT" is an interesting extension, where information is shared across time series, but the factors of individual time series are allowed to vary. Furthermore, SALT could be extended to handle non-Gaussian data. For example, neural spike trains are often modeled with Poisson likelihoods instead of SALT's Gaussian noise model. In this case, the E-step would still be exact, but the M-step would no longer have closed-form coordinate updates. Despite these limitations, SALT offers simple, effective and complementary means of modeling and inference methodology for complex, time-varying dynamical systems.

Ethical ConcernsWe note that there are no new ethical concerns as a result of SALT.

Figure 5: **CP-SALT provides good segmentations of _C. elegans_ neural data, and inferred low-rank tensors give insights into temporal dependencies among neurons in each discrete state: (A) Example data with manually generated labels (Given), as well as segmentations generated by SALT, SLDS and ARHMM models. Learned states are colored based on the permutation of states that best matches given labels. All methods produce comparable segmentations, with high agreement with the given labels. (B) Confusion matrix of SALT-generated labels. (C) One-dimensional autoregressive filters learned in two states by SALT (identified as ventral and dorsal turns). Colors indicate the area under curve (red is positive; blue is negative). The first four rows are neurons known to mediate ventral turns, while the last two rows mediate dorsal turns (Kato et al., 2015; Gray et al., 2005; Yeon et al., 2018). These known behavior-tuned neurons generally have larger magnitude autoregressive filters. Interestingly, AVFL and AVFR also have large filters for dorsal turns. These neurons do not have a well-known function. However, they are associated with motor neurons, and so may simultaneously activate due to factors that co-occur with turning. This highlights how SALT may be used for proposing novel relationships in systems.**