# Benchmarking Robustness to Adversarial Image Obfuscations

Florian Stimberg

Google DeepMind &Ayan Chakrabarti

Google Research &Chun-Ta Lu

Google Research &Hussein Hazimeh

Google Research &Otilia Stretcu

Google Research &Wei Qiao

Google Ads Safety &Yintao Liu

Google Ads Safety &Merve Kaya

Google Research &Cyrus Rashtchian

Google Research &Ariel Fuxman

Google Research &Mehmet Tek

Google Ads Safety &Sven Gowal

Google DeepMind

###### Abstract

Automated content filtering and moderation is an important tool that allows online platforms to build striving user communities that facilitate cooperation and prevent abuse. Unfortunately, resourceful actors try to bypass automated filters in a bid to post content that violate platform policies and codes of conduct. To reach this goal, these malicious actors may obfuscate policy violating images (e.g. overlay harmful images by carefully selected benign images or visual patterns) to prevent machine learning models from reaching the correct decision. In this paper, we invite researchers to tackle this specific issue and present a new image benchmark. This benchmark, based on ImagenNet, simulates the type of obfuscations created by malicious actors. It goes beyond ImagenNet-C and ImagenNet-C by proposing general, drastic, adversarial modifications that preserve the original content intent. It aims to tackle a more common adversarial threat than the one considered by \(_{p}\)-norm bounded adversaries. We evaluate 33 pretrained models on the benchmark and train models with different augmentations, architectures and training methods on subsets of the obfuscations to measure generalization. Our hope is that this benchmark will encourage researchers to test their models and methods and try to find new approaches that are more robust to these obfuscations.

## 1 Introduction

Advances in in computer vision have lead to classifiers that nearly match human performance in many applications. However, while the human visual system is remarkably versatile in extracting semantic meaning out of even degraded and heavily obfuscated images, today's visual classifiers significantly lag behind in emulating the same robustness, and often yield incorrect outputs in the presence of natural and adversarial degradations. This is why evaluating robustness  and improving the robustness of visual classifiers has been the subject of considerable research , with multiple benchmarks looking at out of distribution examples , distribution shift  or focusing on robustness to "natural degradations", like blur and noise, which are inherent in the imaging process . However, with more and more visual classifiers being deployed in real systems, robustness to adversarial perturbations--deliberate changes to an image introduced by an adversary to fool classifiers--has emerged as an important research direction .

Such work on adversarial robustness has largely focused on perturbations that are imperceptible to human observers--often cast as an explicit \(_{p}\)-norm constraint between the original and perturbedimage [12; 15]. However, for many visual classifiers that are focused on keeping offensive, dangerous, pirated, or other policy-violating content off of online platforms, this limited definition does not suffice. In the real world, attackers who want to hide malicious content in images are not prevented by the fact that an image appears obviously perturbed to human observers, as long as the observer is also able to glean the underlying violating content . This scenario is not theoretical--such malicious image manipulations are being used today by bad actors daily and at scale on modern online platforms .

Our work focuses on enabling research into making visual classifiers robust to such adversarial obfuscations--image transformations that can fool classifiers while leaving the underlying semantic content intelligible to human observers, but differing from prior adversarial attacks [12; 15] in allowing it to be obvious that the image is transformed. To this end, we introduce a benchmark to characterize the performance of classifiers on obfuscated images.

Naturally, we do not claim this benchmark to be exhaustive, since the space of adversarial obfuscations is limited only by the creativity of attackers and the resilience of the human visual system in inferring the underlying content despite significant manipulation. However, to provide a concrete starting point to make and measure progress in obfuscation robustness, we introduce a set of 22 transforms, illustrated in fig. 1, that (a) are compositions of various transformations available in image editing software--geometric and color transformations, image splicing and blending, style transfer, etc.; (b) are strong enough to fool most current classifiers while still leaving the underlying semantic content identifiable; and (c) are diverse enough to allow us to measure generalization of current and future robust training techniques, by measuring performance on obfuscations that are held out during training. Moreover, these transforms are similar to actual obfuscations that we have observed being used in the wild by attackers attempting to bypass Google's image policy classifiers.

Overall, our main contributions are as follows:

* We create, curate and tune a set of 22 strong, diverse, adversarial obfuscations. Compared to other benchmarks our obfuscations imitate methods by bad actors trying to fool content filler models and are allowed to drastically change the images. Our benchmark is set up to have training and hold-out obfuscations which allows to measure generalization to unknown obfuscations and monitor progress of leveraging known attacks.
* We evaluate 38 different pretrained models on our benchmark and train additional models to compare the effects of 7 different augmentation and 4 distribution shift algorithms. Our experiments show that scaling architectures, pretraining on larger datasets and choosing the right augmentations can make models more robust to unseen obfuscations.
* We train models on over 60 subsets of our training obfuscations to show relationships between them, which of them have the biggest effect on generalization and that we get diminishing returns when adding obfuscations to the training set.
* Finally, we show that training on our training obfuscations increases performance on all of 8 ImageNet variants and that \(_{p}\)-norm based adversarial training does not help robustness to our obfuscations.

We expect that our analysis, and particularly, this benchmark will stimulate research in the development of training techniques that make classifiers more robust to adversarial obfuscations. Such techniques promise to be of immediate practical value since they can be used by online platforms to strengthen their automated classification and detection models, thereby helping improve their users' online experience by keeping out unsafe and illegal content.

## 2 Related Work

Datasets of natural distribution shifts.Characterizing model failures and empirically estimating their consequences often requires collecting and annotating new datasets. Hendrycks et al.  collected datasets of natural adversarial examples (ImageNet-A and ImageNet-O) to evaluate how model performance degrades when inputs have limited spurious cues. Hendrycks et al.  collected real-world datasets (including ImageNet-R and DeepFashion Remixed) to understand how models behave under large distribution shifts such as artistic renditions of various objects. Particular shortcomings can only be explored using synthetic datasets . Hendrycks and Dietterich  introduced ImageNet-C, a synthetic set of common corruptions.

Recently,  proposed ImageNet-\(}\) in a bid to understand whether progress on ImageNet-C is truthful. They sample corruptions that are perceptually dissimilar from ImageNet-C in feature space and observe that data augmentations may not generalize well.

Extending upon , Kar et al.  introduce corruptions that capture 3D information. They aim to guard against natural corruptions, such as camera rotation, camera focus change, motion blur.

All of the previously mentioned datasets either collect natural out-of-distribution examples or create variations that mimic natural corruptions that occur when capturing images. Our benchmark dataset goes beyond the realistic corruptions considered by above work and includes artificial corruptions that adversaries could produce using common image editing software.

Some examples of datasets that feature less natural transformations are Geirhos et al. , who study the propensity of Convolutional Neural Networks (CNNs) to over-emphasize texture cues, by evaluating such models on 4 types of obfuscations focused on texture and shape, as well as a dataset with texture-shape cue conflict. Xiao et al. , Sagawa et al.  investigate whether models are biased towards background cues by compositing foreground objects with various background images (ImageNet-9, Waterbirds). While these not necessarily try to simulate natural processes, they are focused mostly on understanding how models deal with changes in specific aspects of images.

\(_{p}\)-norm adversarial robustness.In some cases, it is possible to discover failures via optimization or brute-force search. \(_{p}\)-norm adversarial attacks introduce small, imperceptible perturbations to input examples, with the aim of causing misclassifications . While the majority of the attacks in the literature assume white-box access (i.e., all model weights are available to the attacker) , another line of work considers the more practical black-box setting where the attacker has no or limited knowledge about the model . While there is a lot of merit in investigating white- or black-box adversarial robustness, most real world attacks on machine learning models are not of this form  which is why our benchmark tries to include obfuscations that are or realistically could be used by attackers.

Beyond \(_{p}\) robustness.Given the practical implications of \(_{p}\) robustness, there has been growing interest in studying broader threat models that allow for large, perceptible perturbations to images. Examples include robustness to spatial transformations  and adversarial patches . The majority of the work in this category considers local or simple transformations that retain most of original pixel content. Our benchmark considers a wider range of transformations, including ones that cause significant visual changes but do not change the image label.

## 3 Benchmark

### Dataset

We choose to base our benchmark on the ImageNet Large Scale Visual Recognition Challenge 2012 (ILSVRC2012)1 dataset . It has been established in the community as the main benchmark for image classification and there already exist several variations of it that aim to measure different aspects of robustness. This allows us to easily evaluate models trained on ImageNet and its variants

Figure 1: Example images for obfuscations. From top left to bottom right: _Clean_, _IconOverlay_, _Texture_, _ColorPatternOverlay_, _LowContrastTriangles_, _PerspectiveComposition_. See section appendix A.1 for examples of all obfuscations.

on our benchmark and vice versa2. We apply each obfuscation to each image from the ImageNet train and validation split. As the test split includes no labels, we use the validation split as our test set, similar to ImageNet-C , and use 10k images of the original training set as a validation set.

The fine grained 1000-class task of the original ImageNet set makes it hard to apply strong obfuscations as even for humans it is hard to, e.g., distinguish over 100 different breeds of dogs. To make the classification task easier, we use the 16 super-classes introduced by . They encompass 207 of the 1000 ImageNet classes (see appendix A.7 for a detailed listing). We only do this grouping at evaluation time, which allows us to compare models trained on the standard 1000-class ImageNet scenario. As derived in the appendix of  we calculate the probability of an image to be part of a super-class by using the average probability of all member classes for each super class.

### Obfuscations

Our benchmark includes 22 obfuscations in total: 19 training obfuscations and 3 hold-out obfuscations. These represent a wide range of strong and varied manipulations covering, color changes, transformations, compositions, overlays, machine-learning based obfuscations and combinations of them. To create a static benchmark, we do not include manipulations that require access to the model prediction.

The obfuscations have a number of hyperparameters, which each has an allowed range that we randomly draw from for each image. This makes the obfuscations more diverse and avoids overfitting. We tune these hyperparameter ranges manually to get strong obfuscations, that still keep the label intact. For each obfuscation we did a grid search to find the parameters that get the worst accuracy on a Big Transfer model  with an underlying ResNet 152x4 pretrained on ImageNet-21k and fine-tuned on ImageNet-1k. We then checked visually that out of 50 example images, a maximum of 2 are not classifiable as the right super-class. If more of the labels were not recognisable we checked parameters which resulted in higher accuracy until we found a fitting set. Next, we created the range of the possible parameters around this set to get variation in the applied obfuscation but also consistent performance.

Some of the obfuscations are very scale dependent. As the original ImageNet images are of varying sizes, we do a central crop and resizing to 224 \(\) 224 before applying the obfuscations. None of

Figure 2: Accuracy on the hold-out obfuscations for multiple TensorFlow-Hub models trained on clean ImageNet.

the obfuscations change the input size, therefore all our training and evaluation data is 224 \(\) 224. To stay consistent, we also do this to the clean training data, which will reduce the accuracy of the models trained for this paper as random cropping now operates on a smaller pixel space and needs to upsample.

We group the obfuscations into 5 categories: _Color Changes_, _Transformations_, _Compositions_, _Overlays_, _Machine-Learning based Obfuscations_ and fig. 1 shows the hold-out obfuscations and a few examples of training obfuscations. For additional examples and a detailed description of each obfuscation, see appendix A.1 in the supplementary material.

When an obfuscation uses other images to overlay, merge or stylize the original images, we make sure that the chosen images do not introduce any objects which could be categorized into one of the 16 super-classes we evaluate on.

We chose _ColorPatternOverlay_, _LowContrastTriangles_ and _PerspectiveComposition_ as hold-out obfuscations because they cover multiple obfuscation categories (e.g. _ColorPatternOverlay_ being both and overlay and a color change), they combine concepts from training obfuscations (_PerspectiveComposition_ being a combination of _PerspectiveTransform_ and _PhotoComposition_) and are among the hardest obfuscations for multiple model families (_LowContrastTriangles_ is the strongest for ResNet models, while _PerspectiveComposition_ is the strongest for Bit, ConvNext and ViT models).

### Metric

Merging the classes into the 16 super-classes introduces a class imbalance in the evaluation data. We counteract this by weighting the accuracy by the inverse of the number of images of that super-class, thereby giving each super-class equal contribution.

As described earlier, we have 3 hold-out obfuscations that cannot be used during training. The accuracy on these is our main metric. To simulate adversaries trying out multiple obfuscations, we combine the obfuscations on a worst-case basis, i.e., to correctly classify an image, a model has to correctly classify it under all 3 hold-out obfuscations.

Since our super-classes are very unbalanced (see appendix A.7) calculating the average accuracy across all images would be mostly determined by a model's performance on a few super-classes. As an example: Just one super-class (Dog) would account for more than half of the overall accuracy, i.e. a classifier who perfectly classifies all dogs, but no other images, would get almost 53% accuracy, a classifier that perfectly classifies dogs and birds even over 76%. Therefore when we calculate the overall accuracy, we weight each image by the inverse of the super-class occurance, i.e. we average the per-class accuracy.

Figure 3: Worst case accuracy on hold-out obfuscations over GFLOPs (_left_, log scale) and number of parameters (_right_, log scale) for the models taken from TensorFlow-Hub. We did not include GFLOP numbers for the CLIP models as they are more than 1000x than the largest other models when doing 80 prompts for each of the 1000 ImageNet classes.

In summary we calculate our final metric by these steps:

1. Load the ImageNet 2012 validation dataset
2. Filter out all images that do not share a class label with our 16 super classes
3. Obfuscate each image with the 3 hold-out obfuscations
4. For each obfuscated image evaluate the class probabilities for all 1000 ImageNet classes
5. Calculate probabilities for each super class by averaging probabilities of all member classes
6. For each image check if the highest probability is for the correct super-class for **all** obfuscated version of that image
7. Calculate the final accuracy by averaging over all images weighted by the inverse of the super-class occurrence in the dataset

## 4 Experimental Results

We start our experiments by evaluating a range of pretrained ImageNet classification models to analyse the robustness of different architectures to the obfuscations, and the effect of scaling models and pretraining datasets. We then compare 7 data augmentation schemes, and in section 4.3, train models on different subsets of training obfuscations to see their contribution to generalizing to the hold-out obfuscations. We then look at models trained on all training obfuscations, evaluate the effect of distribution shift algorithms, and at the end compare with results on other robustness benchmarks.

### Evaluating Pretrained ImageNet Models

All of the following models are evaluated on training and hold-out obfuscations as is, without any fine-tuning. We evaluate models from four different collections on TensorFlow-Hub3: Inception

Figure 4: Top 1 accuracy for the best models from the 5 pretrained model collections.

Figure 5: Accuracy on hold-out obfuscations and worst case accuracy for models trained with different augmentation schemes. All models are ResNet50 trained only on clean images. The augmentations used are color , AugMix , AutoAugment , RandAugment , random erasing , CutMix  and MixUp .

and ResNet4, Big Transfer5, Vision Transformer6 and ConvNext7. We also include several zero-shot models based on CLIP , with different image encoder architectures8. To obtain the best performance possible for CLIP, we used the best text-prompt configuration reported by  for ImageNet, consisting of a combination of 80 curated prompts. The accuracy of all models on the hold-out obfuscations and the worst case accuracy for all the models is plotted in fig. 2. We see that the ViT-B8 performs the best across all models, as it does on the standard ImageNet dataset. The results for the Big Transfer models show the gain in accuracy by pretraining on the ImageNet-21k dataset (BiT-S models are trained on standard ImageNet, i.e. ILSVRC2012, while BiT-M models, are pretrained on ImageNet-21k). This can be seen in more detail in fig. C.2 in the appendix, where we see accuracy for each individual obfuscation for all BiT models.

The performance of zero-shot models exhibits an interesting behavior. On some obfuscations such as _ColorPatternOverlay_ and _LowContrastTriangles_ the performance of the zero-shot CLIP model is slightly worse than that of the equivalent trained model with the same vision tower. However, on _PerspectiveComposition_ it actually performs better than the trained model. This behavior is probably due to the distribution of the pretraining data distribution--_PerspectiveComposition_ is a more "natural" looking type of obfuscation, which might have been present in CLIP's pretraining dataset.

Figure 3 shows that when comparing models of the same type, scaling them up in terms of parameters or computation leads to increased robustness to the obfuscations. It also shows that ConvNext and ViT models outperform BiT and ResNet models. This might be due to their patchified stem that has been shown to be more robust to \(_{p}\)-norm adversarial attacks . Additionally, both ViT and ConvNext models are all pretrained on ImageNet-21k. In fig. 4, we look at the results for the best model from each category over all obfuscations. While the largest vision transformer model outperforms all other models on the worst-case hold-out accuracy, it does not get the best performance across all obfuscations, e.g. the largest ConvNext model performs better on 8 of the 22 obfuscations, in some cases by a large margin (e.g. _HighContrastBorder_ by 8% and _PhotoComposition_ by 8.7%), indicating that different architectures are more robust to different obfuscations.

### Comparing Augmentation Methods

One way to make models robust to out of distribution data is to use general augmentation schemes. In fig. 5, we see that all augmentations help the worst case accuracy, but only CutMix  and MixUp  improve accuracy across all 3 hold-out obfuscations, with MixUp giving by far the strongest boost overall. AugMix , which is one of the strongest methods on ImageNet-C, gives a big boost on _LowContrastTriangles_ but does not improve accuracy on _PerspectiveComposition_, which is very different from the natural corruptions in ImageNet-C. This highlights that while many augmentations help with obfuscation robustness most are not universally helpful across all of them.

Figure 6: Accuracy on hold-out obfuscations and worst case for models trained on clean images plus a single obfuscations.

### Training on Obfuscations

In this section we train models on all or subsets of the 19 training obfuscations to analyse the effects and interactions. Unless specified otherwise, the models use a ResNet50 architecture and if error bars are plotted they represent the standard deviation from training 5 identical models with different random seeds. When we train on obfuscated images we always sample clean images with a weight of 0.5 and each obfuscation with equal weights summing up to 0.5.

#### 4.3.1 Training on Subsets of Training Obfuscations

To see if there are interactions between different training obfuscations and how each of them helps with generalizing to the hold-out obfuscations, we train models only on one obfuscation (and clean data). The results can be seen in fig. 6. There are some clear connections between training and hold out obfuscations, e.g. _Halfoning_, _LineShift_, _StyleTransfer_, _TextOverlay_ and _Texturize_ all increasing the accuracy on _ColorPatternOverlay_ significantly, while _PerspectiveTransform_ and _PhotoComposition_ lead to small improvements of _PerspectiveComposition_.

In appendix C.4 we also investigate the opposite, where we train models on all but one of the training obfuscations and observe similar behaviour for some obfuscations, e.g. that omitting _LineShift_ significantly reduces the performance on _ColorPatternOverlay_ but in other cases, like excluding _StyleTransfer_, this is compensated by the other training obfuscations.

To evaluate the effect of compounding training obfuscations, we proceed by sorting the training obfuscations by the mean accuracy on the training obfuscations when training only on images from that obfuscation. We then train models by an increasing number of obfuscations starting with the obfuscation that increases the mean accuracy the most. From the results in fig. 7 we can observe that the accuracy makes jumps when adding specific obfuscations that help with one of the hold-out obfuscations but there does not seem to be constant improvement from adding more and more obfuscations to the training data. We also observe that adding the obfuscated images to the training data does not reduce the clean accuracy of the models. We rather see a mild increase in clean accuracy when looking at fig. C.14.

#### 4.3.2 Using All Training Obfuscations

In fig. 8 we see all models, even small ones, can achieve high accuracy on the training obfuscations when they see them during training. However, there is only limited generalization to the hold-out obfuscations even for the bigger models. When comparing results from fig. 2, we see that training a ResNet200 on all 19 training obfuscations is clearly outperformed by the larger BiT, ConvNext and

Figure 8: Top 1 accuracy for 3 different ResNet sizes when training on all training obfuscations.

Figure 7: Accuracy on hold-out obfuscations (_top_) and worst case accuracy (_bottom_) when training on an increasing number of obfuscations.

ViT models despite them never encountering any of the obfuscations during training. This shows that there is still a lot of potential to better leverage the training obfuscations for generalization.

We also train a vision transformer model both only on clean, and obfuscated images. Similar to results in section 4.1, fig. 9 shows that this achieves high accuracy on the hold-out obfuscations even without seeing obfuscations during training. Interestingly, only _ColorPatternOverlay_ and _LowContrastTriangles_ receive a significant boost from the addition of obfuscations to training. Pretraining on ImageNet-21k seems to not provide significant benefits in contrast to the effect we see for the BiT models in fig. 2.

Evaluating Algorithms Specialized for Distribution ShiftTo see if we can improve generalization, we employ several approaches that were proposed to help with distribution shifts. The algorithms we evaluated are standard training using cross-entropy loss, Invariant Risk Minimization (IRM) , DeepCORAL , Domain-Adversarial Neural Network (DANN) , and Just Train Twice (JTT) . Additional to the image and the label, these algorithms (besides JTT) are given side information about the obfuscation that has been applied to each training image. Similar to the observations in , fig. 10 indicates that none of the algorithms give significant improvements over the baseline.

### Comparing to other benchmarks

In section 2, we gave an overview over existing image robustness benchmarks, many also based on ImageNet. In this section we investigate how performance on these relates to our image obfuscation benchmark. Not surprisingly, training on our obfuscations does not give any robustness to \(L_{p}\)-norm attacks but figure fig. 11 shows that adversarial training reduces the obfuscation robustness. This is in contrast to prior observations on the effect of adversarial training for robustness to common corruptions  but understandable because our obfuscations often change the images drastically.

Figure 11: Hold-out accuracy of models with and without adversarial training both for training only on clean data and training on the training obfuscations. For adversarial training we used an \(L_{}\) attack with \(=4/255\). Figure 12: Comparison of a ResNet50 trained with and without the training obfuscations on multiple ImageNet variants.

Figure 10: Comparing performance of different domain shift algorithms on the hold-out obfuscations, when training on all training obfuscations.

Figure 9: Comparison of a vision transformer model trained only on clean or clean and obfuscated images both with and without pretraining on the ImageNet-21k dataset.

We further investigate how models trained on our training observations do on other ImageNet variants. fig.12 shows results for ImageNet-Real , ImageNet-V2 , ImageNet-A , ImageNet-R , ImageNet-Sketch , Conflict Stimuli , ImageNet-C  and ImageNet-C . Training on the obfuscations improves accuracy across all of these variants, however, the improvements are relatively small, indicating that our dataset represents a significantly different distribution shift than existing variants. This can also be seen in the fact that the largest improvement is seen on ImageNet-C, as its corruptions are more similar to our obfuscations compared to the other variants.

## 5 Conclusion

In this paper, we presented a new benchmark that evaluates the robustness of image classifiers to obfuscations. To our knowledge, this is the first benchmark that curates obfuscations similar to what bad actors use to circumvent content filter models. We show that when training on obfuscations, even smaller models can achieve high robustness to them, but this does not necessarily lead to strong generalization on similar but unseen obfuscations. In our experiments, we see that newer architectures, larger models, augmentation schemes and pretraining on bigger datasets all can make models more robust to obfuscations, even if they did not have access to any during training. But there is still a gap to fill to make models robust to unseen attacks and approach human perception. We have shown that models trained on our training obfuscations also achieve better performance across multiple other robustness benchmarks. On the other hand, adversarial training does not improve obfuscation robustness. We hope this benchmark gives practitioners guidance on robustifying their models, and can drive research towards finding ways to leverage known attacks into better generalization.

### Limitations

Investigating adversarial obfuscations leads to a large scope of design decisions. In terms of transformations, we focus on obfuscations that are independent of the model and semantic image category. This allows precomputation of obfuscated images, but also limits the space of attacks. We could further develop obfuscation methods that adapt to the image content (e.g., human-centric images may be treated differently than object-centric). In terms of data, ImageNet is far from perfect (see e.g.  for an evaluation of errors made by state-of-the-art models) and has the limitation of having a single label for each image. One could generate obfuscated versions of other datasets to check the robustness of models trained in a multi-class setting or on other vision tasks like segmentation or image retrieval. The main consideration for other datasets should be to ensure that the main target label (or other output) should be very unlikely to be altered by any of the introduced obfuscations.

### Ethical Considerations

The specific obfuscations (as in fig.1) that we use in our benchmark may have the potential to fool automatic filters and therefore increase the amount of harmful content on digital platforms. To reduce this risk, we decided against releasing the code to create the obfuscations systematically and instead only releasing the precomputed dataset. Furthermore, our obfuscations have been tuned specifically to ImageNet images of a fixed size. Using the same obfuscations on other images would require a reimplementation and additional tuning. It is already known that cloud based image classifiers can be bypassed with widely available transformations [66; 67]. Additionally, the type of obfuscations that we cover in our benchmark are already available in standard image editing software and it is not hard to imagine that adversaries can already think of much more elaborate approaches than the ones we have presented here. Therefore, the benefits of creating a publicly available benchmark that can help discover new methods for training robust models and set an objective baseline for the evaluation of safety far outweigh the risks of presenting examples of obfuscations on ImageNet.