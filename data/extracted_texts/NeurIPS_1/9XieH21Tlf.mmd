# Hierarchical Decomposition of Prompt-Based

Continual Learning:

Rethinking Obscured Sub-optimality

 Liyuan Wang\({}^{1}\), Jingyi Xie\({}^{1}\), Xingxing Zhang\({}^{1}\)1, Mingyi Huang\({}^{1}\), Hang Su\({}^{1}\), Jun Zhu\({}^{1}\)

Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, THBI Lab,

Tsinghua-Bosch Joint Center for ML, Tsinghua University, Beijing, China.

wly19@tsinghua.org.cn, jingyi_xie96@163.com, xxzhang1993@gmail.com

huangmingyi2002@126.com, {suhangss, dcszj}@tsinghua.edu.cn

Corresponding authors.

###### Abstract

Prompt-based continual learning leverages pre-trained knowledge for downstream continual learning, and has almost reached the performance pinnacle under supervised pre-training. However, our empirical study reveals that the current strategies fall short of their full potential under the more realistic self-supervised pre-training, which is essential for handling vast quantities of unlabeled data in practice. This is largely due to the difficulty of task-specific knowledge being incorporated into instructed representations via prompt parameters and predicted by uninstructed representations at test time. To overcome such sub-optimality, we conduct a theoretical analysis of the continual learning objective in the context of pre-training, and decompose it into hierarchical components: within-task prediction, task-identity inference, and task-adaptive prediction. Based on these empirical and theoretical insights, we propose Hierarchical Decomposition (HiDe-)Prompt, an innovative approach that explicitly optimizes the hierarchical components with an ensemble of task-specific prompts and statistics of both uninstructed and instructed representations, further with the coordination of a contrastive regularization strategy. Our extensive experiments demonstrate the superior performance of HiDe-Prompt and its robustness to pre-training paradigms in continual learning (e.g., up to 15.01% and 9.61% lead on Split CIFAR-100 and Split ImageNet-R, respectively). Our code is available at [https://github.com/thu-ml/HiDe-Prompt](https://github.com/thu-ml/HiDe-Prompt).

## 1 Introduction

In the realm of artificial intelligence, continual learning has become an area of significant interest. One of the pivotal techniques that greatly facilitate this domain is pre-training, which not only delivers positive knowledge transfer but also enhances resilience to catastrophic forgetting . A recent innovation is the implementation of prompt-based methodologies, which freeze a pre-trained transformer backbone and employ a few prompt parameters to steer representation learning. Such approaches typically involve _construction_ of adaptive prompts for each task and _inference_ of appropriate prompts during the test phase. By exploring prompt architectures to accommodate task-sharing and task-specific knowledge, this emerging direction demonstrates distinct superiority, almost reaching the upper bound of continual learning performance under supervised pre-training.

Nonetheless, given that robust pre-trained models typically necessitate the learning of substantial amounts of unlabeled data in a self-supervised manner, the influence of pre-training paradigmson the effectiveness of prompt-based continual learning represents a significant and unresolved query. To answer this question, we first perform an extensive empirical investigation, and it clearly demonstrates the sub-optimality of recent prompt-based approaches under the more realistic self-supervised pre-training. Since self-supervised representations tend to be more general, task-specific knowledge is difficult to incorporate into instructed representations via prompt parameters, as well as predicted by uninstructed representations at test time. Consequently, the performance of many recent approaches, such as L2P , DualPrompt , S-Prompt  and CODA-Prompt , is seriously compromised. We further disclose the importance of adaptive prediction for all tasks together, which can potentially mitigate the aforementioned shortcomings to some extent.

Motivated by these observations, we provide an in-depth theoretical analysis of the continual learning objective in the context of pre-training, which can be decomposed into hierarchical components such as _within-task prediction_, _task-identity inference_ and _task-adaptive prediction_. Thanks to the well-distributed representations resulting from adequate pre-training, the hierarchical components can be optimized explicitly by constructing an ensemble of task-specific prompts and exploiting the preserved statistics of uninstructed and instructed representations. A novel contrastive regularization is further devised to coordinate these hierarchical components. We refer to this approach as Hierarchical Decomposition (HiDe-)Prompt and demonstrate its superiority through extensive continual learning experiments, especially under the more realistic self-supervised pre-training.

Our contributions include: (1) We provide an extensive empirical study under self-supervised pre-training to demonstrate the sub-optimality of current progress in prompt-based continual learning; (2) To overcome such sub-optimality, we theoretically analyze the objective of continual learning with pre-training, and decompose it into hierarchical components for model design; (3) With task-specific prompts and representation statistics, we propose an innovative approach to optimize the hierarchical components explicitly; (4) Across various continual learning benchmarks and pre-training paradigms, our approach achieves clearly state-of-the-art performance in a rehearsal-free manner.

## 2 Related Work

**Continual Learning:** The ability of continual learning is critical for artificial neural networks to accommodate real-world changes [37; 35]. Numerous efforts in this direction have been devoted to overcoming catastrophic forgetting [22; 34; 33]. According to a recent survey , representative strategies include selective stabilization of network parameters, replay of a few old training samples, explicit manipulation of optimization programs, exploitation of well-distributed representations, construction of task-specific parameters, etc. The performance of such strategies varies with particular settings of continual learning. As one of the most challenging and representative settings, class-incremental learning (CIL) [31; 37] requires a continual learning model to perform all old tasks (or classes) without the oracle of task identity. Strong CIL methods generally depend on storage and rehearsal of old training samples [28; 8; 38], which result in efficiency and privacy issues.

**Self-Supervised Learning and Pre-Training:** The exploitation of well-distributed representations, especially from the success of large-scale pre-training, brings significant benefits for downstream continual learning [37; 27; 23]. Due to the scarcity and expense of explicit labeling in many real-world applications, self-supervised learning is typically involved in the pre-training stage to cope with huge amounts of unlabeled data. In particular, instance discrimination [4; 7] with contrastive learning  has become the dominant strategy, which aims to maximize representation similarity of the same instance and minimize representation similarity of different instances. Besides, self-supervised paradigms have been shown less sensitive to catastrophic forgetting in upstream continual learning , providing a practical way to enrich pre-trained knowledge from in-the-wild data.

**Prompt-Based Approach:** Inspired by parameter-efficient fine-tuning techniques in NLP [11; 10], recent prompt-based approaches [3; 41; 40; 39; 30] are developed to leverage pre-trained knowledge adaptively for downstream continual learning. The basic idea includes _construction_ and _inference_ of adaptive prompts for each task, so as to instruct a frozen transformer backbone. The former mainly focuses on exploring prompt architectures to instruct representations with task-sharing and task-specific knowledge, closely related to the discussion of model architectures in continual learning [37; 36], while the latter attempts to predict appropriate (combinations of) prompts with uninstructed representations. Although such methods have achieved remarkably strong performance under supervised pre-training, whether these advantages are consistent under the more realistic self supervised pre-training remains to be explored. A concurrent study  observed that self-supervised pre-training is more challenging for continual learning approaches that require fine-tuning of the backbone, implying a non-trivial impact of pre-training paradigms on downstream continual learning.

## 3 Preliminary Analysis

In this section, we first introduce the problem formulation of prompt-based continual learning, and then evaluate the impact of pre-training paradigms with an extensive empirical study.

### Formulation of Prompt-Based Continual Learning

**Continual learning** aims to learn a sequence of tasks on their respective training sets \(_{1},...,_{T}\) and excel on their corresponding test sets. The training set for task \(t\) typically consists of various data-label pairs \(_{t}=\{(_{t,n},y_{t,n})\}_{n=1}^{N_{t}}\), where \(_{t,n}_{t}\) and \(y_{t,n}_{t}\) represent the sample and label elements, respectively. We will use \(||\) to denote the cardinality of a set and \([N]=\{1,2,,N\}\) as the set of intergers from \(1\) to \(N\). Consider a neural network model with a backbone \(f_{}\) parameterized by \(\), and an output layer \(h_{}\) parameterized by \(\). This model seeks to learn the projection from \(=_{t=1}^{T}_{t}\) to \(=_{t=1}^{T}_{t}\), aiming to predict the label \(y=h_{}(f_{}())\) of an unseen test sample \(\) drawn from previous tasks. The backbone function \(f_{}\) is assumed to be pre-trained with a substantial quantity of additional training samples external to each \(_{t}\). There are commonly three distinct settings for continual learning : task-, domain-, and class-incremental learning (TIL, DIL, and CIL). Specifically, \(_{1},...,_{T}\) are identical for DIL while disjoint for TIL and CIL. The task identity is provided for TIL at test time but is not available for DIL and CIL. Therefore, CIL is considered to be more representative and challenging in general. Of note, the continual learning process is _rehearsal-free_--all elements of \(_{t}\) are available only when learning task \(t\).

**Prompt-based approaches** for vision tasks further specify the backbone \(f_{}\) as a pre-trained vision transformer (ViT), where multiple consecutive multi-head self-attention (MSA) layers can transform an input sample into a sequence-like output representation \(^{L_{h} D}\) of sequence length \(L_{h}\) and embedding dimension \(D\). The backbone parameters \(\) are typically frozen to obtain generalizable representations. A few prompt parameters \(^{L_{p} D}\) of sequence length \(L_{p}\) and embedding dimension \(D\) are prepended to \(\) to exploit the pre-trained knowledge adaptively. Here we denote the input of the \(l\)-th MSA layer as \(^{l}^{L_{h}t D}\), which consists of query \(^{l}_{Q}\), key \(^{l}_{K}\) and value \(^{l}_{V}\), and denote the prompt as \(^{l}^{L_{p}t D}\). For notation clarity, we take one MSA layer as an example and omit the layer label \(l\) if not necessary. Then, the output of this MSA layer is given as

\[(_{Q},_{K},_{V})=(h_{1},...,h_{m} )W_{O}, \]

\[h_{i}=(_{Q}W_{Q,i},_{K}W_{K,i},_{V}W_{V,i} ),i[m], \]

where \(W_{O}\), \(W_{Q,i}\), \(W_{K,i}\) and \(W_{V,i}\) are projection matrices, \(m\) is the number of heads, and \(_{Q}=_{K}=_{V}\) in ViT. There are two major implementations of prompt-based methodologies , i.e., Prompt Tuning (ProT)  and Prefix Tuning (PreT) . Specifically, ProT prepends an identical \(\) to \(_{Q}\), \(_{K}\) and \(_{V}\):

\[f_{}(,)=([;_{Q}],[;_{K}],[;_{V}]), \]

where \([\,]\) denotes the concatenation operation along the dimension of sequence length, and the output in \(^{(L_{h}+L_{p}) D}\) has increased dimensions. In contrast, PreT splits \(\) into \(_{K}^{L_{}/2 D}\) and \(_{V}^{L_{}/2 D}\) only for \(_{K}\) and \(_{V}\), respectively:

\[f_{}(,)=(_{Q},[_{K};_{K }],[_{V};_{V}]), \]

where the output dimension remains the same as the input \(^{L_{h} D}\). As the training samples for each task are introduced sequentially, prompt-based continual learning needs to incorporate task-specific knowledge into prompt parameters while overcoming their catastrophic forgetting. The mainstream idea is to construct adaptive prompts for each task and then infer appropriate (combinations of) prompts at test time. Here we compare state-of-the-art approaches from these two aspects, demonstrated conceptually in Fig. 1:

**L2P** constructs a prompt pool \(=\{_{1},...,_{M}\}\) potentially shared by all tasks where \(M\) is the total number of prompts, and then instructs the last MSA layer in a ProT fashion. Each prompt \(_{i}\) is associated to a learnable key \(_{i}^{D}\), optimized by the cosine distance \((q(),_{i})\) of the top-\(N\) keys to a query function \(q()=f_{}()\) to incorporate knowledge. Therefore, the most relevant keys and the corresponding prompts can be selected with uninstructed representations for inference.

**DualPrompt** constructs task-sharing prompts \(^{l}\) and task-specific prompts \(^{l}_{t}\) to instruct different MSA layers in a PreT fashion. All \(^{l}_{t}\) belonging to the same task is associated to a task-specific key \(_{t}^{D}\), optimized by \((q(),_{t})\), and the index of the best-matched key is selected for inference.

**S-Prompt** constructs only task-specific prompts \(_{t}\) for each task, and adopts a similar ProT strategy as L2P to instruct the last MSA layer. The inference of task identity is achieved by a simple KNN strategy for the nearest task centroid. Unlike other methods, S-Prompt associates an exclusive output head \(_{t}\) to each task \(t=1,...,T\).

**CODA-Prompt** exploits the prompt pool \(\) by its weighted summation, i.e., \(=_{i=1}^{M}_{i}_{i}\), where \(_{i}=(q(),_{i})\) is the weighting factor, and adopts a similar PreT strategy as DualPrompt to instruct multiple MSA layers. The inference of \(_{i}\) enables construction of adaptive prompts.

### Empirical Study of Pre-Training Paradigms

Either explicitly or implicitly, the above prompt-based approaches all incorporate the knowledge of each task into prompt parameters and predict their identities from uninstructed representations. To evaluate the impact of pre-training paradigms, we perform an empirical study with widely-used CIL benchmarks such as Split CIFAR-100 and Split ImageNet-R . In addition to supervised pre-training of ImageNet-21K  (denoted as Sup-21K), we consider several powerful self-supervised models that release ViT checkpoints2, such as iBOT , DINO  and MoCo v3 .

We carefully evaluate the official implementations of all baselines for fair comparison. We follow largely the training regimes of L2P  and DualPrompt , which are basically consistent. As S-Prompt  is initially designed for DIL, we slightly modify its implementation by inserting task-specific prompts into the same layers as DualPrompt (i.e., layers 1-5) in a PreT manner, so as to evaluate the impact of prompt architectures. The output layer retains multiple heads associated

Figure 1: Illustration of prompt-based continual learning.

Figure 2: Empirical study of prompt-based continual learning under different pre-training paradigms.

with the task identity (still denoted as S-Prompt), or a single head as with other baselines (denoted as S-Prompt++). CODA-Prompt  is officially implemented in a DualPrompt-like architecture but depends heavily on the use of a smaller learning rate with cosine decay. Here we present its performance with both default and reduced learning rates. Using the same learning rate as , we grid search for an appropriate number of epochs (detailed in Appendix D) and report the best performance for all baselines.

As shown in Fig. 2, a, b, the above prompt-based approaches achieve outstanding performance under Sup-21K, where the use of task-specific prompts clearly outperforms task-sharing prompts (i.e., S-Prompt++ \(\) CODA-Prompt \(>\) DualPrompt \(>\) L2P) due to explicit avoidance of catastrophic forgetting. However, the four baselines suffer **significant performance degradation** under the more realistic self-supervised pre-training. In particular, the performance differences between prompt architectures have become much smaller, suggesting that the task-specific and task-sharing knowledge are not well differentiated. Besides, CODA-Prompt can generally achieve leading performance as a direct result of the learning rate (LR) reduction rather than the prompt architecture.

We perform two additional experiments to demonstrate the **obscured sub-optimality3**. First, we evaluate the CKA similarity of uninstructed and instructed representations by learning task-specific prompts (Fig. 2, c). The CKA similarity of self-supervised pre-training is significantly higher, suggesting a greater difficulty for prompt parameters to incorporate task-specific knowledge. Second, we evaluate the ability to predict task identity from uninstructed representations and task-specific keys, where self-supervised pre-training exhibits much lower accuracy (Fig. 2, d). Interestingly, although only less than 40% task identities are correctly predicted, S-Prompt++ can still achieve considerable (albeit sub-optimal) performance, owning to the compensation effects of using a single-head output layer (Fig. 2, e). Together with the results in Fig. 2, c, d, it is conceivable that using an "incorrect" prompt would not severely affect the instructed representations, which can still be correctly predicted in a well-balanced single-head output layer. In contrast, S-Prompt performs much worse than S-Prompt++, as its multi-head output layer undertakes all errors of task-identity inference.

## 4 Theoretical Foundation and Our Approach

In this section, we first present a theoretical analysis of the sufficient and necessary conditions for improving continual learning in the context of pre-training, and then present an innovative approach for prompt-based continual learning to achieve this objective.

### Hierarchical Decomposition of Continual Learning Objective

For continual learning of sequentially arrived \(_{t}\), \(_{t}\) and \(_{t}\) are the domain and label of task \(t\). Here we take CIL as a typical scenario for theoretical analysis where \(_{t}_{t^{}}=\), \( t t^{}\) (see Appendix A for DIL and TIL). Let \(_{t}=_{j}_{t,j}\) and \(_{t}=\{_{t,j}\}\), where \(j[|_{t}|]\) indicates the \(j\)-th class in task \(t\). Now assume we have a ground event denoted as \(=\{_{1},...,_{t}\}\) and a pre-trained model \(f_{}\). For any sample \(_{k=1}^{t}_{k}\), a general goal of the CIL problem is to learn \(P(_{t,j}|,)\), where \(i[t]\) and \(j[|_{i}|]\). This can be decomposed into two probabilities, including task-identity inference (TII) and within-task prediction (WTP), denoted as \(P(_{i}|,)\) and \(P(_{i,j}|_{i},,)\), respectively. Based on Bayes' theorem, we have

\[P(_{i,j}|,)=P(_{i,j}| _{i},,)P(_{i}| ,). \]

Let \([t]\) and \([|_{i}|]\) be the ground truth of an \(\) w.r.t. the task identity and within-task index. Eq. (5) shows that if we can improve either the WTP performance \(P(_{,j}|_{},,)\), the TII performance \(P(_{}|,)\), or both, then the CIL performance \(P(_{,j}|,)\) would be improved. However, such an improvement is limited since it is upper-bounded by WTP or TII. To further improve the CIL performance, we propose a hierarchical decomposition of its objective. That is, besides the improvement of \(P(_{,}|,)\), we also need to improve the performance of task-adaptive prediction (TAP), denoted as \(P(^{y}|,)\), where \(^{y}\) represents the domain of class \(y\) in all previous tasks, and \(y=_{i,j}\) is the ground truth label of \(\). Then the final goal of CIL is formulated as a multi-objective optimization problem, i.e., \([P(_{i,j}[,),P(^{y} [,)]\). Notice that the TII probability is a categorical distribution over all observed tasks upto \(t\), while the TAP probability is over all observed classes \(_{k=1}^{t}_{k}\).

To resolve the problems above, we derive the sufficient and necessary conditions in the context of the widely-used cross-entropy loss. Specifically, we define

\[H_{}() =(_{},\{P(_{i, j}|_{i},,)\}_{j}), \] \[H_{}() =(_{},\{P(_{i} |,)\}_{i}),\] (7) \[H_{}() =(_{},\{P(^{c} |,)\}_{c}), \]

where \(H_{}\), \(H_{}\), and \(H_{}\) are the cross-entropy values of WTP, TII, and TAP, respectively. The operation \((p,q)-_{p}[ q]=-_{i}p_{i} q_{i}\). \(_{}\) is a one-hot encoding function.

We now present the first theorem under the CIL scenario (see Appendix A for a detailed proof):

**Theorem 1**: _For continual learning with pre-training, if \(_{}[H_{}()]\), \(_{}[H_{}()]\), and \(_{}[H_{}()]\), we have the loss error \([0,\{+,\}]\), regardless whether WTP, TII and TAP are trained together or separately._

With the use of cross-entropy, the continual learning performance tends to be better as the bounds are tightened. In Theorem 1 we have shown that good performances of WTP, TII and TAP are sufficient to guarantee a good performance of CIL. For completeness, we now study the necessary conditions of a well-performed CIL model in Theorem 2.

**Theorem 2**: _For continual learning with pre-training, if the loss error \(\), then there always exist (1) a WTP, s.t. \(H_{}\); (2) a TII, s.t. \(H_{}\); and (3) a TAP, s.t. \(H_{}\)._

Theorem 2 suggests that if a continual learning model is well trained (i.e., with low loss), then the WTP, TII and TAP for sequential tasks are always implied to be small. It is worth noting that without the pre-trained knowledge carried by \(\), Theorem 1 and Theorem 2 would degrade to the main conclusion of a previous theoretical study , suggesting that the presented theorems are particularly directed to the impact of _pre-training_ for continual learning (detailed in Appendix B). Besides, the paradigm of pre-training is indeed related to the performance of continual learning, because it can affect the distribution of representations from \(f_{}\), and further WTP, TII and TAP. Previous work has demonstrated that self-supervised representations tend to be more robust to parameter changes than supervised ones [9; 21; 37], which is beneficial for accumulating pre-trained knowledge (if applicable) but challenging for adapting to downstream tasks on a continual basis (see Fig. 2, c, d).

### HiDe-Prompt for Prompt-Based Continual Learning

Motivated by the above empirical and theoretical insights, we propose to optimize explicitly the hierarchical components (i.e., WTP, TII and TAP) for prompt-based continual learning, as shown in Fig. 3. Our proposal stems from a particular advantage of pre-training, where the distributions of uninstructed and instructed representations can be effectively preserved through their statistical information. In the case of classification, for example, since each class tends to have single-peaked

Figure 3: Illustration of Hierarchical Decomposition (HiDe-)Prompt.

representations (see Appendix D, Fig. 5 and Fig. 6), we can naturally approximate them with Gaussian distributions. For generality, here we denote the approximated distributions of uninstructed and instructed representations as \(}_{c}\) and \(_{c}\) for each class \(c_{i},i[t-1]\), respectively, and discuss their specific forms latter.

First, we improve **WTP** through effective incorporation of task-specific knowledge. We construct an expandable prompt pool with only task-specific prompts \(_{t}\) to incorporate the knowledge of \(_{t}\), optimized by a cross-entropy (CE) loss for \(H_{}\). The previous prompts \(_{1},...,_{t-1}\) are frozen to avoid catastrophic forgetting. In order to transfer knowledge for learning each task effectively, we employ a _prompt ensemble_ (PE) strategy, where the current prompt is initialized by the last prompt \(_{t}_{t-1}\) and then optimized with a weighted combination of all previous prompts \(_{t}=_{i=1}^{t-1}_{i}+(1-)_{t}\). \(\) is a hyperparameter that controls the strength of inherited old knowledge to facilitate \(_{t}\) in learning the current task. Meanwhile, the instructed representations of \(_{t}\), although allowing the new task to be performed well, may overlap with that of the old tasks and thus affect TAP. To overcome this issue, we exploit the old-task statistics of instructed representations (collected by \(f_{}\) and \(_{i}\) for \(i=1,...,t-1\)), where for classification we calculate the mean \(_{c}\) of \(_{c}\) for each class \(c_{i}\), and design a _contrastive regularization_ (CR):

\[_{}(_{t})=_{_{c}}_{c}/)}{_{i=1}^{t-1}|_{i}|}_{i= 1}^{t-1}_{c_{i}}^{}/ )}{_{^{}_{c}}(^{} /)+_{i=1}^{t-1}_{c_{i}}(_{c}/ )}, \]

where \(_{t}\) is the embedding transformation of \(_{t}\) with \(f_{}\) and \(_{t}\). \(\) is the temperature coefficient, which is insensitive and set to 0.8 in practice. Notably, here we use only \(_{c}\) to represent each class for efficiency, which can be optionally replaced by sampling from \(_{c}\) for better performance.

Then, the loss function of WTP can be defined as

\[_{}(,_{t})=_{}(, _{t})+_{}(_{t}). \]

Therefore, the instructed representations of new classes can be well distinguished for WTP while avoiding overlap with the previous ones. \(\) is a hyperparamter to balance the impact of old classes.

Second, we improve **TII** and **TAP** through exploiting the approximated distributions of uninstructed and instructed representations, respectively. For TII, we construct an auxiliary output layer \(_{}:^{D}^{T}\) parameterized by \(\), learning explicitly the projection from uninstructed representations to task identity via cross-entropy (i.e., \(H_{}\)):

\[_{}()=^{t}|_{i}|} _{i=1}^{t}_{c_{i}}_{}_{i,c} }-_{}(})[i])}{_{j=1}^{t}(_{}(})[j])}, \]

where \(}_{i,c}\) is constructed by sampling an equal number of pseudo representations from \(}_{c}\) for \(c_{i}\) and \(i[t]\). Unlike other baselines that freeze the projection of old tasks (i.e., the previous keys), our \(_{}\) is continually adapted for all tasks and thus greatly facilitates TII.

Similarly, the final output layer \(h_{}:^{D}^{||}\) can be further optimized for TAP (i.e., \(H_{}\)):

\[_{}()=^{t}|_{i}|} _{i=1}^{t}_{c_{i}}_{_{i,c}}- ()[c])}{_{j=1}^{t}_{c^{}_ {j}}(h_{}()[c^{}])}, \]

where \(_{i,c}\) is constructed by sampling an equal number of pseudo representations from \(_{c}\) for \(c_{i}\) and \(i=1,...,t\). As \(\) and \(\) are usually _light-weight_, the optimization of TII and TAP is computationally efficient. At test time, HiDe-Prompt predicts the task identity \(i=_{}(f_{}())\) and then the label \(y=h_{}(f_{}(;_{i}))\). Please refer to Appendix Algorithm 1 for more details.

Since the pre-trained representations are usually well-distributed, there are many reasonable strategies to model \(}_{c}\) and \(_{c}\). On default, the distributions of uninstructed and instructed representations can be faithfully recovered by modeling each class as a Gaussian with a dedicated mean and covariance. With adequate pre-training, the covariance can be further reduced to variance for efficiency. Alternatively, such statistical modeling can employ multiple centroids obtained from KNN and add Gaussian noise, which is also an efficient choice and is applicable to other task types.

## 5 Experiment

In this section, we first describe the experimental setups, and then present the experimental results.

**Benchmark:** We consider multiple CIL benchmarks that are widely used for prompt-based continual learning . Specifically, Split CIFAR-100  includes 100-class small-scale images, randomly split into 10 incremental tasks of disjoint classes. Split ImageNet-R  includes 200-class large-scale images that are hard examples of ImageNet  or newly collected examples of different styles, randomly split into 10 incremental tasks of disjoint classes. 5-Datasets  includes CIFAR-10 , MNIST , Fashion-MNIST , SVHN  and notMNIST  datasets, each treated as an incremental task to evaluate the impact of large inter-task differences. Split CUB-200  includes 200-class fine-grained images of birds, randomly split into 10 incremental tasks of disjoint classes.

**Baseline:** We compare four representative prompt-based approaches as discussed in Sec. 3.1, such as L2P , DualPrompt , S-Prompt++  and CODA-Prompt . To evaluate the performance of continual learning, we record the average accuracy of all seen classes after learning each task, presenting the last one as the final average accuracy (FAA) and their historical average as the cumulative average accuracy (CAA) . We also present the final forgetting measure (FFM) of all tasks . We consider a variety of pre-training paradigms for ImageNet-21K and ImageNet-1K, including Sup-21K, iBOT-21K, iBOT-1K, DINO-1K and MoCo-1K, as described in Sec. 3.2.

**Implementation:** We follow similar implementations as previous work . Specifically, we adopt a pre-trained ViT-B/16 backbone and train with an Adam optimizer (\(_{1}=0.9\), \(_{2}=0.999\)), a batch size of 128, and a constant learning rate of 0.005 (except for CODA-Prompt with a cosine-decaying learning rate of 0.001), and grid search for a proper epoch number. The image inputs are resized to \(224 224\) and normalized to \(\). Please refer to Appendix C for more details.

**Overall Performance:** Table 1 presents the main results of all approaches. Consistent with the observations in Sec. 3.2, representative prompt-based approaches achieve outstanding performance under supervised pre-training (i.e., Sup-21K), while perform significantly worse under the more realistic self-supervised pre-training. In particular, the most recent CODA-Prompt  outperforms other baselines in general (here we take FAA as the primary metric), but is sensitive to learning rate (see Fig. 2, a, b and Appendix Table 6). In contrast, our HiDe-Prompt achieves generally the

    &  &  &  \\  & & **FAA** (\(\)) & CAA (\(\)) & FFM (\(\)) & **FAA** (\(\)) & CAA (\(\)) & FFM (\(\)) \\   & L2P  & 83.06 \(\)0.17 & 88.25 \(\)0.01 & 6.58 \(\)0.40 & 63.65 \(\)0.12 & 67.25 \(\)0.02 & 7.51 \(\)0.17 \\  & DualPrompt  & 86.60 \(\)0.19 & 90.64 \(\)0.01 & 4.45 \(\)0.16 & 68.79 \(\)0.31 & 71.96 \(\)0.04 & 4.49 \(\)0.14 \\  & S-Prompt++  & 88.81 \(\)0.18 & 92.25 \(\)0.03 & 3.87 \(\)0.05 & 69.68 \(\)0.12 & 72.50 \(\)0.04 & 3.29 \(\)0.05 \\  & CODA-Prompt \({}^{*}\) & 86.94 \(\)0.63 & 91.57 \(\)0.75 & 4.04 \(\)0.18 & 70.03 \(\)0.47 & 74.26 \(\)0.24 & 5.17 \(\)0.22 \\  & HiDe-Prompt (Ours) & **92.61**\(\)0.28 & **94.03**\(\)0.01 & **3.16**\(\)0.10 & **75.06**\(\)0.12 & **76.60**\(\)0.01 & **2.17**\(\)0.19 \\   & L2P  & 79.00 \(\)0.28 & 85.13 \(\)0.05 & 5.55 \(\)0.36 & 55.35 \(\)0.28 & 58.62 \(\)0.05 & 3.73 \(\)0.53 \\  & DualPrompt  & 78.76 \(\)0.23 & 86.16 \(\)0.02 & 9.84 \(\)0.24 & 54.55 \(\)0.53 & 58.69 \(\)0.01 & 5.38 \(\)0.70 \\  & S-Prompt++  & 79.14 \(\)0.65 & 85.85 \(\)0.17 & 9.17 \(\)1.33 & 53.16 \(\)0.83 & 58.48 \(\)0.18 & 4.07 \(\)0.16 \\  & CODA-Prompt  & 80.83 \(\)0.27 & 87.02 \(\)0.20 & 7.50 \(\)0.25 & 61.22 \(\)0.35 & 66.76 \(\)0.37 & 9.66 \(\)0.20 \\  & HiDe-Prompt (Ours) & **93.02**\(\)0.15 & **94.56**\(\)0.05 & **1.33**\(\)0.24 & **70.83**\(\)0.17 & **73.23**\(\)0.08 & **2.46**\(\)0.21 \\   & L2P  & 75.57 \(\)0.41 & 82.69 \(\)0.06 & 7.23 \(\)0.93 & 60.97 \(\)0.26 & 65.95 \(\)0.02 & 4.07 \(\)0.66 \\  & DualPrompt  & 76.63 \(\)0.05 & 85.08 \(\)0.12 & 8.41 \(\)0.40 & 61.51 \(\)1.05 & 67.11 \(\)0.08 & 5.02 \(\)0.52 \\  & S-Prompt++  & 77.53 \(\)0.56 & 85.66 \(\)0.16 & 8.07 \(\)0.97 & 60.82 \(\)0.68 & 66.03 \(\)0.91 & 4.16 \(\)0.14 \\  & CODA-Prompt  & 79.11 \(\)1.02 & 86.21 \(\)0.49 & 7.69 \(\)1.57 & 66.56 \(\)0.68 & 73.14 \(\)0.57 & 7.22 \(\)0.38 \\  & HiDe-Prompt (Ours) & **93.48**\(\)0.11 & **95.02**\(\)0.01 & **1.00**\(\)0.24 & **71.33**\(\)0.21 & **73.62**\(\)0.13 & **2.79**\(\)0.26 \\   & L2P  & 70.65 \(\)0.57 & 79.02 \(\)0.01 & 9.46 \(\)1.68 & 57.40 \(\)0.23 & 62.56 \(\)0.20 & 3.58 \(\)0.28 \\  & DualPrompt  & 74.90 \(\)0.21 & 83.98 \(\)0.16 & 10.26 \(\)0.62 & 58.57 \(\)0.45 & 64.89 \(\)0.15 & 5.80 \(\)0.21 \\  & S-Prompt++  & 74.97 \(\)0.46 & 83.82 \(\)0.39 & 7.78 \(\)0.66 & 57.64 \(\)0.16 & 63.79 \(\)0.05 & 5.08 \(\)0.31 \\  & CODA-Prompt  & 77.50 \(\)0.64 & 84.81 \(\)0.30 & 8.10 \(\)0.01 & 63.15 \(\)0.39 & 69.73 \(\)0.25 & 6.86 \(\)0.11 \\  & HiDe-Prompt (Ours) & **92.51**\(\)0.11 & **94.25**\(\)0.01 & **0.99**\(\)0.21 & **68.11**\(\)0.18 & **71.70**\(\)0.01 & **3.11**\(\)0.17 \\   & L2P  & 74.85 \(\)0.28 & 83.14 \(\)0.03 & 6.51 \(\)0.95 & 51.64 \(\)0.19 & 58.87 \(\)0.24 & **2.37**\(\)0.59 \\  & DualPrompt  & 77.77 \(\)0.68 & 85.31 \(\)0.07 & 6.61 \(\)0.18 & 52.57 \(\)0.82 & 60.65 \(\)0.16 & 2.73 \(\)0.49 \\   & S-Prompt++  & 76.30 \(\)0.54 & 83.88 \(\)0.12 & 14.67 \(\)0.64 & 53.15 \(\)1.10 & 60.03 \(\)0.95 & 4.11 \(\)1.84 \\   & CODA-Prompt  & 76.83 \(\)0.34 & 84.97 \(\)0.23 & 12.60 \(\)0.02 & 55.75 \(\)0.26 & 65.49 \

[MISSING_PAGE_FAIL:9]

with WTP+TAP rather than with WTP only, suggesting that these hierarchical components are highly _synergistic_ instead of operating in isolation. The proposed contrastive regularization (CR) helps the instructed representations of individual tasks to be compatible with each other and avoid inter-task overlap, thus further facilitating the performance of WTP+TII+TAP. Moreover, we observe that the improvements of WTP, TII, TAP and CR are generally more significant under _self-supervised pre-training_, due to effectively addressing the obscured sub-optimality.

**Detailed Analysis:** Now we further analyze the contributions of the three components for continual learning. First, we evaluate the average performance of learning each new task in Fig. 4, a, where WTP clearly outperforms the naive architecture, indicating that the knowledge of resolving each task is better incorporated into the task-specific prompts. Second, we present the accuracy of predicting task identity from uninstructed representations in Fig. 4, b, which has been substantially improved (up to 67.74%) through optimizing TII explicitly. Third, we evaluate the effects of CR on only WTP and the full model of HiDe-Prompt (i.e., WTP+TII+TAP) in Table 4. Increasing the strength of CR decreases the performance of WTP since the task-specific prompt includes more knowledge about other tasks, but improves the performance of HiDe-Prompt since the inter-task representations become more compatible. These results suggest a potential _trade-off_ between the knowledge for WTP and TAP, which can be modulated explicitly by CR.

## 6 Discussion and Conclusion

In this work, we analyze extensively the advanced prompt-based continual learning from both empirical and theoretical perspectives. An important finding is that, the continual learning objective (which can be decomposed into WTP, TII and TAP in the context of pre-training) is not adequately achieved, and this sub-optimality is clearly exposed under the more realistic self-supervised pre-training. By leveraging statistics of uninstructed and instructed representations, we present a strong approach to optimize explicitly the hierarchical components, which achieves superior performance across various pre-training paradigms. In particular, our theoretical analysis and the proposed approach can serve as a general framework for implementing parameter-efficient fine-tuning techniques (e.g., prompt, adapter, LoRA, FiLM, etc.) in continual learning5, which differ only in the form of task-specific parameters. Interestingly, our proposal is consistent with recent advances in neuroscience [17; 16], where the activation of non-memory cells and memory cells (as well as their specific populations) is internally switched. Based on these results, we expect subsequent work to further explore the architecture and optimization of continual learning with an effective use of pre-trained knowledge.

This work remains some potential _limitations_. First, we assume adequate pre-training to provide meaningful representations, which may not be available in some applications. Second, the prompt-based strategy is mainly applicable to the transformer backbone rather than other backbone architectures. Third, the transformer backbone is frozen and adapted to downstream tasks via prompt parameters, which prevents the pre-trained knowledge from being enriched and updated. As a fundamental research in machine learning, the potential _negative societal impact_ is not obvious at this stage.