# Generative Neural Fields

by Mixtures of Neural Implicit Functions

 Tackgeun You\({}^{3}\)

tackgeun.you@postech.ac.kr

Mijeong Kim\({}^{1}\)

mijeong.kim@snu.ac.kr

Jungtaek Kim\({}^{4}\)

jungtaek.kim@pitt.edu

Bohyung Han\({}^{1,2}\)

bhhan@snu.ac.kr

###### Abstract

We propose a novel approach to learning the generative neural fields represented by linear combinations of implicit basis networks. Our algorithm learns basis networks in the form of implicit neural representations and their coefficients in a latent space by either conducting meta-learning or adopting auto-decoding paradigms. The proposed method easily enlarges the capacity of generative neural fields by increasing the number of basis networks while maintaining the size of a network for inference to be small through their weighted model averaging. Consequently, sampling instances using the model is efficient in terms of latency and memory footprint. Moreover, we customize denoising diffusion probabilistic model for a target task to sample latent mixture coefficients, which allows our final model to generate unseen data effectively. Experiments show that our approach achieves competitive generation performance on diverse benchmarks for images, voxel data, and NeRF scenes without sophisticated designs for specific modalities and domains.

## 1 Introduction

Implicit neural representation (INR) is a powerful and versatile tool for modeling complex and diverse data signals in various modalities and domains, including audio , images , videos , 3D objects [28; 6], and natural scenes. INR expresses a data instance as a function mapping from a continuous coordinate space to a signal magnitude space rather than using a conventional representation on a discrete structured space. In particular, a representation with a continuous coordinate allows us to query arbitrary points and retrieve their values, which is desirable for many applications that only have accessibility to a subset of the target data in a limited resolution. INRs replace the representations based on high-dimensional regular grids, such as videos  and 3D scenes , with multi-layer perceptrons (MLPs) with a relatively small number of parameters, which effectively memorize scenes.

Generative neural fields aim to learn distributions of functions that represent data instances as a form of neural field. They typically rely on INRs to sample their instances in various data modalities and domains. The crux of generative neural fields lies in how to effectively identify and model shared and instance-specific information. To this end, feature-wise linear modulation (FiLM)  and hyper-network (HyperNet)  are common methods for modulating representations. FiLMadjusts hidden features using an affine transform, which consists of element-wise multiplication and bias addition. Due to the ease of optimization, the effectiveness of FiLM has already been validated in several tasks including 3D-aware generative modeling . However, the operation of FiLM is too restrictive while introducing additional parameters for modulation. On the other hand, HyperNet is more flexible because it directly predicts network parameters. Unfortunately, the direct prediction of model parameters is prone to unstable training and limited coverage of the data distribution generated by the predicted network. Therefore, to reduce solution spaces, HyperNet often employs dimensionality reduction techniques such as low-rank decompositions of weight matrices  and combinations of multiple network blocks . However, all these works give limited consideration to the inference efficiency.

Inspired by the flexibility of HyperNet and the stability of FiLM while considering efficiency for inference, we propose a _mixture of neural implicit functions_ (mNIF) to represent generative neural fields. Our approach employs mNIFs to construct generative networks via model averaging of INRs. The mixture components in an mNIF serve as shared implicit basis networks while their mixands define relative weights of the bases to construct a single instance. Our modulation step corresponds to computing a weighted average of the neural implicit functions. The mixture coefficients are optimized by either meta-learning or auto-decoding procedures with signal reconstruction loss. Such a design is effective for maximizing the expressibility of INRs while maintaining the compactness of inference networks. The proposed approach is versatile for diverse modalities and domains and is easy to implement.

Figure 1 demonstrates the overview of the proposed approach, and we summarize the contributions of our work as follows:

* We introduce a generative neural field based on mNIF, a modulation technique by a linear combination of NIFs. The proposed approach effectively extracts instance-specific information through conducting meta-learning or applying auto-decoding procedures.
* Our modulation allows us to easily extend the model capacity by increasing the number of basis networks without affecting the structure and size of the final model for inference. Such a property greatly improves the efficiency of the sampling network in terms of inference time and memory requirements.
* Our model achieves competitive performance on several data generation benchmarks without sophisticated domain-specific designs of algorithm and architecture configuration.

The rest of this paper is organized as follows. We first discuss related works about generative neural fields in Section 2. The main idea of the proposed approach and the training and inference procedures are discussed in Sections 3 and 4, respectively. Section 5 presents empirical quantitative and qualitative results with discussions, and Section 6 concludes this paper.

Figure 1: Overview of the generation procedure using the proposed generative neural field based on mixtures of neural implicit function (mNIF). Our model is applicable to various types of data such as images, voxels, and radiance fields. To generate an instance, we first sample a context vector \(_{j}\) from a prior distribution (\(P_{}\)) estimated by a denoising diffusion probabilistic model. We then perform a weighted model averaging on implicit bases \(\{_{m}^{(i)}_{m}^{(i)}\}\) using mixture coefficients (\(_{m}^{(i)}\)) derived from the context vector.

Related Work

Implicit neural representations (INR) are often optimized for representing a single instance in a dataset, and have been extended in several ways to changing activation functions [33; 30], utilizing input coordinate embeddings [36; 26] or taking advantage of a mixture of experts [37; 23; 24]. Since the INR framework is difficult to generalize a whole dataset, generative neural fields have been proposed to learn a distribution of functions based on INRs, each of which corresponds to an example in the dataset.

Generative manifold learning (GEM)  and generative adversarial stochastic process (GASP)  are early approaches to realizing generative neural fields. GEM implements the generative neural field using the concept of hyper-network (HyperNet) , which predicts a neural network representing an instance. Training GEM is given by the auto-decoding paradigm with manifold learning. However, GEM involves extra computational cost because the manifold constraint requires accessing training examples. GASP also employs a HyperNet for sampling instances, and its training utilizes a discriminator taking an input as a set of coordinate and feature pairs. Since the output of the discriminator should be permutation invariant, it follows the design of the network for the classification of point cloud data. However, training GASP is unstable and the discriminator design is sometimes tricky, especially in NeRF. Functa  proposes the generative neural fields with SIREN  modulated by FiLM  and introduces a training strategy based on meta-learning. However, meta-learning is computationally expensive due to the Hessian computation. On the other hand, diffusion probabilistic fields (DPF)  proposes a single-stage diffusion process with the explicit field parametrization . However, its high computational cost hampers the applicability to complex neural fields such as NeRF. HyperDiffusion , which is concurrent to our work, presents a framework directly sampling the entire INR weight from the learned diffusion process instead of exploiting latent vectors for modulating INR weights. However, this work demonstrates instance generation capability only on voxel domains, not on images.

Model averaging  is often used for enhancing performance in discriminative tasks without increasing an inference cost. Our approach applies model averaging to generative models and verifies its effectiveness in generative neural fields based on INRs. Note that we are interested in efficient prediction by learning a compact representation through a weighted mixture; our method jointly trains basis models and their mixture coefficients.

## 3 Generative Neural Fields with a Mixture of Neural Implicit Functions

This section describes how to define generative neural fields. The formulation with a mixture of neural implicit functions (mNIF) enforces implicit basis networks to represent shared information across examples and allows latent mixture coefficients to encode instance-specific information through a linear combination of basis networks. We show that generative neural field via mNIF is effectively optimized to predict the latent vector for the construction of models generating high-quality samples.

### Implicit Neural Representations

Implicit neural representation (INR) expresses a data instance using a function from an input query, \(^{d}\), to its corresponding target value, \(^{k}\). Since each input coordinate is independent of the others, a mapping function parametrized by \(\), \(_{}()\), is typically represented by a multi-layer perceptron (MLP) with \(L\) fully connected (FC) layers, which is given by

\[=_{}()=^{(L+1)} ^{(1)}^{(0)}(), \]

where \(^{(i)}\) for \( i\{1,2,,L\}\) denote FC layers while \(^{(0)}\) and \(^{(L+1)}\) indicate the input and output layers, respectively. Each layer \(^{(i)}\) performs a linear transform on its input hidden state \(^{(i)}\) and then applies a non-linear activation function to yield the output state \(^{(i+1)}\), expressed as

\[^{(i+1)}=^{(i)}(^{(i)})=(^ {(i)}^{(i)}+^{(i)}),\;i\{1,2,,L\}, \]

where \(()\) is an activation function and \(^{(i)}^{W W}\) and \(^{(i)}^{W}\) are learnable parameters. The operations of the input and output layers are defined as \(_{1}=^{(0)}()\) and \(}=^{(L+1)}(^{(L)})\)respectively. Consequently, a collection of learnable parameters \(\) in all layers is given by

\[\{^{(0)},^{(0)},^{(1)}, ^{(1)},,^{(L+1)},^{(L+1)}\}, \]

where \(^{(0)}^{W d}\), \(^{(0)}^{W}\), \(^{(L+1)}^{k W}\), and \(^{(L+1)}^{k}\). For INR, the mean squared error between a prediction \(}\) and a target \(\) is typically adopted as a loss function, \((,)\), which is given by

\[(},)=||}-||^{2}. \]

Among the variations of INRs, we employ SIREN , which adopts the sinusoidal function as an activation function and introduces a sophisticated weight initialization scheme for MLPs. Supposing that SIREN is defined with an MLP with \(L\) hidden layers, the \(i^{}\) layer of SIREN is given by

\[^{(i+1)}=(w_{0}(^{(i)}^{(i)}+ ^{(i)})),\;i\{1,,L\}, \]

where \(w_{0}\) is a scale hyperparameter to control the frequency characteristics of the network. The initialization of SIREN encourages the distribution of hidden sine activations to follow a normal distribution with a standard deviation of 1.

### Mixtures of Neural Implicit Functions

We propose generative neural fields based on mNIF, which is an extension of the standard INR for modulating its model weight. We define a set of NIFs, which is used as basis networks, and construct a generative neural field using a mixture of the NIFs. This is motivated by our observation that a generative neural field is successfully interpolated using multiple basis networks and model averaging works well for model generalization .

The operation in each layer of our mixture model is given by

\[^{(i+1)}=(w_{0}(_{m=1}^{M}_{m}^{(i)}_{m}(^{(i)}))), \]

where \(_{m}^{(i)}(^{(i)})=_{m}^{(i)}^{(i)}+ _{m}^{(i)}\) is the \(i^{}\)-layer operation of the \(m^{}\) neural implicit function, and \(_{m}^{(i)}\) is a mixture coefficient of the same layer of the same neural implicit function. Note that modulating the network is achieved by setting the mixand values, \(\{_{m}^{(i)}\}\). Similar to the definition of \(\) described in Eq. (3), the parameter of each mixture is given by

\[_{m}\{_{m}^{(0)},_{m}^{(0)}, ,_{m}^{(L+1)},_{m}^{(L+1)}\}. \]

The operation in the resulting INR obtained from model averaging is given by

\[_{m=1}^{M}_{m}^{(i)}_{m}(^{(i)})=(_{m= 1}^{M}_{m}^{(i)}_{m}^{(i)})^{(i)}+_{m=1}^ {M}_{m}^{(i)}_{m}^{(i)}=}^{(i)} ^{(i)}+}^{(i)}, \]

where

\[}^{(i)}_{m=1}^{M}_{m}^{(i)}_{m}^{(i)}}^{(i)}_{m=1}^ {M}_{m}^{(i)}_{m}^{(i)}. \]

All the learnable parameters in the mixture of NIFs is defined by \(\{_{1},,_{M}\}\). The remaining parameters are mixture coefficients, and we have the following two options to define them: (i) sharing mixands for all layers \(_{m}^{(i)}=_{m}\) and (ii) setting mixands to different values across layers. The first option is too restrictive for the construction of INRs because all layers share mixture coefficients while the second is more flexible but less stable because it involves more parameters and fails to consider the dependency between the coefficients in different layers. Hence, we choose the second option but estimate the mixture coefficients in a latent space, where our method sets the dimensionality of the latent space to a constant and enjoys the high degree-of-freedom of layerwise coefficient setting. To this end, we introduce a projection matrix \(\) to determine the mixture coefficients efficiently and effectively, which is given by

\[[_{1}^{(0)},,_{M}^{(0)},_{1}^{(1)},,_ {M}^{(1)},,_{1}^{(L+1)},,_{M}^{(L+1)}]=, \]

[MISSING_PAGE_FAIL:5]

we introduce the sampling strategy for the context vectors \(\) and customize it to a specific target task. To this end, we adopt the denoising diffusion probabilistic model (DDPM) , which employs the residual MLP architecture introduced in Functa .

```
1:Randomly initialize the shared parameter \(\) of mNIF.
2:Initialize a latent vector \(_{j}(,^{2})^{H}\) for all samples.
3:while training do
4: Sample a mini-batch \(=\{(_{j},_{j})\}_{j=1:||}\).
5: Define a joint parameter: \(_{}=\{_{j}\}_{j=1:||}\)
6: Update the parameters: \(\{,_{}\}\{,_{ }\}-_{,_{}}_{j} (f_{,}(_{j}),_{j})|_{=_{j}}\)
7:endwhile
```

**Algorithm 2** Auto-decoding with mNIF

## 5 Experiments

This section demonstrates the effectiveness of the proposed approach, referred to as mNIF, and discusses the characteristics of our algorithm based on the results. We run all experiments on the Vessl environment , and describe the detailed experiment setup for each benchmark in the appendix.

### Datasets and Evaluation Protocols

We adopt CelebA-HQ \(64^{2}\), ShapeNet \(64^{3}\) and SRN Cars  dataset for image, voxel and neural radiance field (NeRF) generation, respectively, where \(64^{2}\) and \(64^{3}\) denotes the resolution of samples in the dataset. We follow the protocol from Functa  for image and NeRF scene and generative manifold learning (GEM)  for voxel.

We adopt the following metrics for performance evaluation. To measure the reconstruction quality, we use mean-squared error (MSE), peak signal-to-noise ratio (PSNR), reconstruction Frechet inception distance (rFID), reconstruction precision (rPrecision) and reconstruction recall (rRecall). In image generation, we use Frechet inception distance (FID) score , precision, recall [32; 27] and F1 score between sampled images and images in a train split. Voxel generation performance is evaluated by coverage and maximum mean discrepancy (MMD) metrics  on a test split. In NeRF scene generation, we use FID score between rendered images and images in a test split for all predefined views for evaluation. To compare model size of algorithms, we count the number of parameters for training and inference separately; the parameters for training contain all the weights required in the training procedure, _e.g._, the parameters in the mixture components and project matrix for our algorithm, while the model size for inference is determined by the parameters used for sampling instances. For the evaluation of efficiency, we measure the number of floating point operations per second (FLOPS), latency in terms of frames per second (fps), and the amount of memory consumption for a single sample.

### Main Results

We present quantitative and qualitative results, and also analyze the effectiveness of the proposed approach in comparison to the previous works including Functa , GEM , GASP , and DPF . Note that we adopt the model configurations with the best generation performance for the other baselines.

#### 5.2.1 Quantitative Performance

We compare results from our approach, mNIF, with existing methods in terms of four aspects: reconstruction accuracy, generation quality, model size, and inference efficiency. We present quantitative results from two configurations of our model, mNIF (S) and mNIF (L), which correspond to small and large networks, respectively.

As shown in Tables 1, 2, and 3, our approach consistently achieves state-of-the-art reconstruction and generation quality on the image, voxel, and NeRF scene datasets except the reconstruction on the voxel dataset, in which our model is ranked second. Moreover, mNIF is significantly more efficient than other methods in terms of model size and inference speed in all cases. Considering the model size and inference speed, the overall performance of mNIF is outstanding.

Note that the light configuration of our model denoted as mNIF (S) is most efficient in all benchmarks and also outperforms the baselines in voxel and NeRF. In SRN Cars, mNIF (S) demonstrates huge benefit in terms of efficiency compared to Functa; 199 times less FLOPS, 49 times more fps, and 22 times less memory in single view inference. In the image domain, the performance of mNIF (S) is comparable to GASP in terms of F1 but worse in FID. We conjecture that FID metric overly penalizes blurry images, which is discussed in several works [12; 13; 31; 20].

#### 5.2.2 Qualitative Generation Performance

Figure 2 illustrates generated samples for qualitative evaluation. We visualize the results from our models on each benchmark together with GASP on CelebA-HQ \(64^{2}\) and Functa on SRN Cars to compare results in the image and NeRF scene tasks, respectively. In the image domain, our model, mNIF (L), generates perceptually consistent yet slightly blurry samples compared to ground-truths, while the examples from GASP with a similar FID to ours have more artifacts. In the NeRF scene, rendered views from our model, mNIF (S), and Functa exhibit a similar level of image blur compared to ground-truths. Since both methods use vanilla volumetric rendering, adopting advanced rendering techniques, such as hierarchical sampling [25; 6] and anti-aliasing methods [4; 3] for NeRF scenes, would improve rendering quality.

    &  &  &  &  \\   & Learnable & Inference & PSNR \(\) & rFID \(\) & FID \(\) & Precision \(\) & Recall \(\) & F1 \(\) & GFLOPS \(\) & fps \(\) & Memory (MB) \(\) \\  Functa  & 3.3 M & 2,629.6 K & 26.6 & 28.4 & 40.4 & 0.577 & 0.397 & 0.470 & 8.602 & 332.9 & 144.1 \\ GEM  & 99.0 M & 921.3 K & – & – & 30.4 & 0.642 & 0.502 & 0.563 & 3.299 & 559.6 & 70.3 \\ GASP  & 34.2 M & 83.1 K & – & – & 13.5 & 0.836 & 0.312 & 0.454 & 0.305 & 1949.3 & 16.4 \\ DPF  & 62.4 M & – & – & – & 13.2 & 0.866 & 0.347 & 0.495 & & & \\ mNIF (S) & 4.6 M & 17.2 K & – & 51.3 & 0.79 & 21.6 & 0.787 & 0.324 & 0.459 & & & \\ mNIF (L) & 33.4 M & 83.3 K & 34.5 & 5.8 & 13.2 & 0.902 & 0.544 & 0.679 & 0.340 & & 891.3 & 24.4 \\   

Table 1: Image reconstruction and generation performance on CelebA-HQ \(64^{2}\). The results of DPF, GEM, and Functa are brought from the corresponding papers.

    &  &  &  &  \\   & Learnable & Inference & MSE \(\) & PSNR \(\) & Coverage \(\) & MMD \(\) & GFLOPS \(\) & fps \(\) & Memory (MB) \(\) \\  GASP  & 34.2 M & 83.1 K & 0.026 & 16.5 & 0.341 & 0.0021 & 8.7 & 180.9 & 763.1 \\ GEM  & 99.0 M & 921.3 K & 0.0153 & 21.3 & 0.409 & 0.0014 & 207.0 & 16.7 & 4010.0 \\ DPF  & 62.4 M & – & – & – & 0.419 & 0.0016 & & & & \\ mNIF (S) & 4.6 M & 17.2 K & 0.161 & 20.5 & 0.430 & 0.6014 & & & & \\ mNIF (L) & 46.3 M & 83

### Analysis

We analyze the characteristics of our trained model for better understanding via latent space exploration and various ablation studies.

#### 5.3.1 Latent Space Interpolation

Figure 3 illustrates the results of interpolation in the latent space; the corner images are samples in the training dataset and the rest of the images are constructed by bilinear interpolations in the latent space. These images demonstrate the smooth transition of sampled data, confirming that the latent vectors learned at the first stage manage to capture the realistic and smooth manifold of each dataset.

#### 5.3.2 Configurations of Mixture Coefficients in mNIF

Table 4(a) examines the performance by varying the types of mixture coefficients: i) shared mixture coefficients across all layers, _i.e._, \(_{m}^{(i)}=_{m}\), ii) layer-specific mixture coefficients, and iii) layer-specific mixture coefficients projected from a latent vector as shown in Eq. (10). Among the three mixture settings, the last option yields the best performance and is used for our default setting.

Table 4(b) presents the effect of the number of mixtures, denoted by \(M\). Reconstruction performance generally improves as the number of mixtures increases, but such a tendency saturates at around \(M=256\). We also measure the precision and recall between the ground-truth and reconstructed samples denoted by rPrecision and rRecall, respectively. According to our observation, increasing the size of the mixture improves the recall more than it does the precision.

Table 4(c) shows the effect of the latent dimension on the reconstruction performance of mNIF. Increasing the latent dimension \(H\) leads to improving the reconstruction performance in both the train and test splits due to the degree-of-freedom issue.

    &  &  &  &  \\   & Learnable & Inference & PSNR \(\) & FID \(\) & TFLOPS \(\) & fps \(\) & Memory (GB) \(\) \\  Functa  & 3.9 M & 3.418.6 K & 24.2 & 80.3 & 1.789 & 2.0 & 28.0 \\ mNIF (S) & 4.6 M & 17.2 K & 25.9 & 79.5 & 0.005 & 97.7 & 1.5 \\   

Table 3: NeRF scene reconstruction and generation performance on SRN Cars. The results of Functa are brought from the corresponding papers.

Figure 2: Comparison of generated samples from our models, mNIF (S) and mNIF (L), with the ground-truth and the baseline methods such as GASP  and Functa , on the CelebA-HQ \(64^{2}\) (2a), ShapeNet \(64^{3}\) (2b) and SRN Cars (2c) datasets.

#### 5.3.3 Diversity in Learned Neural Bases

To investigate the diversity of the basis functions per layer, Figure 4 visualizes the absolute value of the pairwise cosine similarity between the weight matrices of the basis models. In the cosine similarity matrices, an element with a small value indicates that the corresponding two neural basis functions are nearly orthogonal to each other, which serves as evidence of diversity in the learned basis functions. The visualization also reveals the following two characteristics. First, the lower layers are more correlated than the upper ones partly because low-level features inherently have less diversity. Second, a small subset of neural bases have high correlations with others. We hypothesize that the concentration of high correlation among the small subset of neural bases could be mitigated by introducing a loss function enforcing orthogonality.

#### 5.3.4 Context Adaptation Strategies

We perform the ablation study with respect to diverse optimization strategies and demonstrate the resulting reconstruction performance in Table 5. We train small mNIFs with \((L,W,M,H)=(2,64,256,256)\) on the CelebA-HQ \(64^{2}\) dataset. It is clear that the meta-learning approach using second-order gradient computation yields favorable results compared to other options. Running more inner-loop iterations marginally improves performance, but at the cost of additional computational complexity in time and space. Therefore, we set the number of inner-loop iterations to 3 for all meta-learning experiments.

Interestingly, the auto-decoding strategy surpasses meta-learning with first-order gradient computation. Note that auto-decoding is more efficient than other methods in terms of both speed and memory usage when evaluated under the same optimization environment. This efficiency of auto-decoding is

    &  &  &  &  &  \\   & & & Learnable & Inference & PSNR \(\) & rFID \(\) & rPrecision \(\) & rRecall \(\) & PSNR \(\) & rFID \(\) \\   & Shared & \((2,64,64,64)\) & 557.2 K & 8.7 K & 22.20 & 50.70 & 0.497 & 0.003 & 22.11 & 55.87 \\  & Layer-specific & \((2,64,64,256)\) & 357.2 K & 8.7 K & 24.35 & 35.23 & 0.461 & 0.013 & 24.35 & 42.65 \\  & Latent & \((2,64,64,256)\) & 623.0 K & 8.7 K & 25.27 & 31.67 & 0.354 & 0.040 & 25.09 & 56.85 \\   &  & \((2,64,16,256)\) & 155.8 K & 22.02 & 53.01 & 0.433 & 0.001 & 21.84 & 57.23 \\  & & \((2,64,64,256)\) & 623.0 K & 8.7 K & 25.27 & 31.67 & 0.534 & 0.040 & 25.09 & 36.85 \\  & & \((2,64,256,256)\) & 2.5 M & 26.64 & 24.62 & 0.640 & 0.134 & 25.74 & 32

[MISSING_PAGE_FAIL:10]