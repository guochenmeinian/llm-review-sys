# Toward Semantic Gaze Target Detection

Samy Tafasca

Idiap Research Institute

Ecole Polytechnique Federale de Lausanne

stafasca@idiap.ch &Anshul Gupta

Idiap Research Institute

Ecole Polytechnique Federale de Lausanne

agupta@idiap.ch &Victor Bros

Idiap Research Institute

Ecole Polytechnique Federale de Lausanne

vbroso@idiap.ch &Jean-Marc Odobez

Idiap Research Institute

Ecole Polytechnique Federale de Lausanne

odobez@idiap.ch

###### Abstract

From the onset of infanthood, humans naturally develop the ability to closely observe and interpret the visual gaze of others. This skill, known as gaze following, holds significance in developmental theory as it enables us to grasp another person's mental state, emotions, intentions, and more . In computer vision, gaze following is defined as the prediction of the pixel coordinates where a person in the image is focusing their attention. Existing methods in this research area have predominantly centered on pinpointing the gaze target by predicting a gaze heatmap or gaze point. However, a notable drawback of this approach is its limited practical value in gaze applications, as mere localization may not fully capture our primary interest -- understanding the underlying semantics, such as the nature of the gaze target, rather than just its 2D pixel location. To address this gap, we extend the gaze following task, and introduce a novel architecture that simultaneously predicts the localization and semantic label of the gaze target. We devise a pseudo-annotation pipeline for the GazeFollow dataset, propose a new benchmark, develop an experimental protocol and design a suitable baseline for comparison. Our method sets a new state-of-the-art on the main GazeFollow benchmark for localization and achieves competitive results in the recognition task on both datasets compared to the baseline, with \(40\%\) fewer parameters.

## 1 Introduction

Gaze is an important marker in non-verbal communication that is indicative of a person's visual attention. It is also a proxy measure of cognition and can be used to evaluate a subject's intentions, preferences, emotions, among others. Consequently, it received a lot of attention over the years from different research communities such as neuroscience , psychology , cognitive science , robotics , and education .

In computer vision, the analysis and understanding of attention was formulated through different tasks. One research direction focuses on predicting a gaze direction representing the 3D line of sight from a frontal image of a face . Another one tries to estimate the visual focus of attention (VFOA), _i.e._ the gaze target of a person, given 3D information about the subject (_e.g._ body, head, eyes) and the scene (_e.g._ layout, object positions) . Beyond predicting gaze as a standalone signal, several research efforts focused on understanding gaze dynamics in the context of social communication such as inferring mutual gaze , joint attention , or gaze aversion .

This paper focuses on the task of gaze following , which extends the idea of Visual Focus of Attention (VFOA). Gaze following aims to predict the 2D pixel coordinates where a person in animage is looking. The major benefit of this formulation is that it makes no assumptions about the scene and doesn't require additional equipment, such as wearable devices. However, a notable drawback is that solely predicting the pixel location of the gaze target often falls short for real-world applications that demand additional information, such as object class or social gaze class.

One possibility to address this limitation is to post-process the output of a gaze following method by verifying if a gaze point falls within the bounding box of a detected object . However, this multi-stage process entails additional computation, often requiring the use of additional pre-trained models, leading to inefficiency and less than optimal results. Furthermore, pre-trained detectors typically ignore uncountable objects (_e.g_. wall, sea) which are often possible gaze targets. Finally, unlike object detection, in gaze following we predict heatmaps and not boxes, which makes applying a separate object detector afterwards challenging. In such case, how do we match a gaze heatmap to the right object box? We could consider the gaze point (_i.e_. \(*{arg\,max}\)) as mentioned before, but what if the point falls within multiple boxes? And what if the heatmap is multimodal, and the \(*{arg\,max}\) happens to land on the wrong target? The joint training of the gaze heatmap and the gaze target class, aside from being the more natural formulation, allows the model to learn the best way to dynamically make sense of the heatmap in order to infer the right class.

An alternative approach is to frame the problem as a Human-Object-Interaction (HOI) recognition task, where _looking_ serves as an interactive action. However, many existing HOI datasets lack consistent and systematic labeling of the looking behavior. To the best of our knowledge, V-COCO  is the only dataset doing so, but it is limited to 80 object classes, which may not encompass the full spectrum of potential gaze targets found in images, including cases where individuals look at locations categorized as _stuff_ semantic classes. Moreover, other datasets focus on a limited number of classes (_e.g_., watching TV or a cell phone), thereby biasing the looking task towards specific objects. Consequently, utilizing an HOI verb-object task formulation with existing HOI datasets might result in learning spurious correlations, wherein the detection of a particular object (_e.g_. _TV_) strongly suggests a specific verb (_e.g_. _watching_). This is a well known issue in HOI and has fostered the development of benchmark datasets specifically designed to evaluate HOI methods based on evidence rather than relying on dataset-specific correlation priors . Given these limitations, there is a clear need to explore the task in a novel manner and develop new datasets and protocols to address this research topic effectively.

In this paper, we propose an end-to-end architecture that predicts both the localization and the class label of the gaze target, addressing the limitations above. Furthermore, we frame the recognition part as a visual-text alignment task, which offers the benefit of generalizing to other classes beyond the training vocabulary. To this end, we make the following contributions

* We address, for the first time, the semantic gaze following problem by devising a visual-language architecture that efficiently tackles both localization and target class categorization tasks simultaneously.
* We introduce novel benchmarks, a new baseline for comparison, and experimental protocols for investigating the extended task, drawing from datasets within the gaze following and HOI communities.
* Our architecture sets a new state-of-the-art in gaze target localization on the main GazeFollow benchmark dataset, and demonstrates strong categorization performance compared to more complex and computationally intensive baselines.

## 2 Related Work

**Gaze Target Detection.** Traditional methods for estimating the Visual Focus of Attention (VFOA) [47; 1; 39; 18; 43; 2; 36] were limited by their reliance on specific scene or activity priors, which hindered their generalization to more arbitrary settings where such priors could not be provided. To overcome these limitations, Recasens et al.  proposed a novel formulation of the problem, aiming to infer the 2D image coordinates that correspond to the scene target being observed by a person in the image. Standard methods for gaze following adopt a two-branch architecture, comprising a scene branch for saliency detection and a head analysis branch for the person of interest to infer a gaze direction. Information from the two branches is then fused to predict the final gaze heatmap. This type of architecture has demonstrated robust performance in gaze following, with the ability to also predict if the person is looking within the image or outside [9; 32; 48; 49; 26; 16; 5; 27].

**Semantic Gaze Following.** Building on this foundation, we introduce an architecture that not only estimates gaze target localization but also identifies the class label of the gaze target. To the best of our knowledge, we are the first to address this task. The closest work in this direction is from Tonini et al.  which proposed a model that simultaneously predicts the gaze heatmap and detects objects in the scene. Their approach involves a primary branch that identifies both the head and object bounding boxes within a scene, followed by feature extraction and gaze cone estimation to predict the best candidate object of focus in the cone and its corresponding heatmap. It is important to note that object detection here only serves as an auxiliary task to improve gaze localization performance and is not meant to produce a class label. In fact, the target object is often not detected, and only a bounded vocabulary set is supported. Our work seeks to address this limitation by adopting a weakly supervised contrastive objective aiming to align vision and text.

## 3 Architecture

Inspired by the idea of promptable segmentation , we design an architecture (See Figure 1) for promptable gaze following where the scene image is processed separately from people. We decode gaze outputs by _prompting_ the encoded image representation using person-specific information (_i.e._ head crop and box coordinates). This is achieved by a lightweight transformer decoder that can process multiple people at the same time or separately in a batch. The rationale behind this design is for the expensive scene encoding operation to be performed only once in a person-agnostic manner. Then, after we obtain the gaze tokens of people, their gaze targets (_i.e._ location and label) can be decoded efficiently. Intuitively, we expect the image representation to identify gaze target candidates (_i.e._ salient objects), and for the decoding step to act as a filtering mechanism that selects the right gaze target based on the query person. We empirically verify this hypothesis in the qualitative analysis section. It is important to note that this design is in stark contrast to most previous two-stream gaze following approaches [10; 16; 27; 21] that fuse the person and image representations early on. This makes them inefficient for multi-person inference because the expensive encoding is repeated for each individual. We provide more details about each component below.

### Image Encoder

Given an input image \(^{H W C}\), we employ a transformer encoder to produce image tokens \(^{}^{N D}\), where \(N=w h\) is the number of patches, \(w=\), \(h=\), \(P\) is the patch size, and \(D\) is the dimension of the transformer. We can also use a convolutional backbone as the image encoder, assuming we equip the output feature map with positional information.

Figure 1: Overview of our architecture. [A] The scene image is passed through an Image Encoder to produce image tokens (blue squares). [B] The head crops and head box coordinates are processed by a Gaze Encoder to generate gaze tokens (orange squares). [C] The image and gaze tokens are fed to a Gaze Decoder to predict both the gaze heatmaps and (normalized) gaze label embeddings (yellow squares) through a series of cross-attention operations. [D] The text encoder computes (normalized) class embeddings (green squares) based on a predefined vocabulary of concept classes. [E] Finally, we compute similarity scores between the predicted gaze label embeddings and vocabulary embeddings.

### Gaze Encoder

The goal of the gaze encoder is to process the head crop of the target person at a higher resolution (\(z24 224\)), as well as its location in the image in order to produce a position-sensitive embedding capturing directional gaze information. Given an input head crop \(_{}^{H W C}\), we pass it through a backbone followed by a 2-layer MLP to output a gaze token \(^{}^{D}\). Additionally, we project the corresponding head bounding box \(_{}^{4}\) to \(D\) using a linear layer and add it to the gaze token to make it aware of the person's position in the image. If we process \(N_{p}\) persons, we get the final gaze tokens \(^{}^{N_{p} D}\). Please note that we can batch process multiple people in different images by merging the image and person dimensions as the batch dimension. However, this requires the use of padding or truncation of people to ensure \(N_{p}\) is the same across all images.

### Gaze Decoder

The goal of the gaze decoder is to combine information from the scene and the people in order to predict the gaze heatmaps and label embeddings. It is composed of a transformer with \(N\) blocks, an upscaler module and two MLPs for the final predictions.

Once the image is encoded, the image tokens \(^{}\) and gaze tokens \(^{}\) go through the transformer decoder blocks, which are composed of two cross-attention operations and a feed-forward module, with residual connections and normalization layers in-between. The cross-attention operations go in two ways: one where the gaze tokens \(^{}\) generate the queries, while the image tokens \(^{}\) generate the keys and values (we call this \(^{}^{}\) or gaze-to-image cross-attention), and one where image tokens \(^{}\) generate the queries while the gaze tokens \(^{}\) generate the keys and values (we call this \(^{}^{}\) or image-to-gaze cross-attention). This two-way design allows image tokens to incorporate information from gaze tokens, and vice-versa. This helps them better align for the final dot-product operation that will produce the predicted gaze heatmap.

After the \(N\) blocks, we transform the gaze tokens one last time using a final \(^{}^{}\) cross-attention layer. The output is an updated version of the gaze and image tokens, which we also call \(^{}^{N_{p} D}\) and \(^{}^{N D}\) to ease notation. In Figure 1, \(^{}\) flows through the decoder following the blue arrows, while \(^{}\) flows through the orange arrows.

To obtain the final predictions, we use the task-specific MLPs in the following manner: (i) the gaze tokens \(^{}^{N_{p} D}\) are passed through a Gaze Label MLP composed of \(6\) layers to predict the gaze label embeddings \(_{}^{N_{p} D_{i}}\), where \(D_{i}\) is the dimension of the output gaze label embedding, (ii) the gaze tokens \(^{}\) are fed to a Gaze Heatmap MLP composed of \(3\) layers to project them to a lower dimensional embedding \(}^{}^{N_{p} d}\), where \(d<D\). At the same time, the image tokens are rearranged into a spatial feature map \(^{}^{h w D}\) and upscaled to the resolution of the output heatmap while reducing the number of channels \(}^{}^{H_{} W_{ } d}\). The Upscaler module is composed of two blocks of one interpolation followed by one convolutional layer each. Finally, for each projected gaze token, we apply a dot-product with every spatial position of the upscaled image tokens to obtain the final heatmaps \(^{N_{p} H_{} W_{}}\). Essentially, this dot-product acts as a filtering mechanism that localizes the gaze target of the subject thanks to the alignment done by the decoder between the gaze token and the image tokens at the right spatial positions.

Additionally, the gaze decoder uses a learnable token called the _not gaze token_ whose role is to expand the range of the \(^{}^{}\) cross-attention. Recall that this operation updates the image representation with information from the gaze tokens \(z\). each image token gets updated with a weighted sum of the (values of the) gaze tokens. Conceptually, the _not gaze token_ allows tokens of image regions where nobody focuses their attention to be updated by a different "null" token instead of forcing a weighted sum on the existing gaze tokens. We verify this hypothesis in a later section.

### Text Encoder

Since we frame the prediction of the gaze label as a text-vision alignment task, our architecture also requires a text encoder to convert the vocabulary of class labels into text embeddings. We chose CLIP  as our text encoder, and we kept it frozen.

During training, the predicted gaze label embedding is aligned with the available class label embeddings. During inference, we first build a vocabulary of class labels, pass them through the text encoder to get their embeddings, then compare the predicted gaze label embedding with each one of them and select the class label with the highest similarity score.

### Losses

**Heatmap Loss (\(_{reg}\)).** It is the standard pixel-wise MSE between the GT and the predicted heatmaps:

\[_{hm}=_{x,y}^{W_{hm},H_{hm}}||_{x,y}^{}- _{x,y}^{}||_{2}^{2}\]

**Label Loss (\(_{lab}\)).** The label loss follows a similar formulation to the multimodal contrastive InfoNCE used in  with two main differences: 1) our loss is not symmetric (_i.e_. we only consider the image to text loss component), and 2) since our language labels are (pseudo-) classes and not full captions, there will inevitably be class redundancies within each batch (_i.e_. two or more samples having the same gaze label), so we consider the unique ground-truth gaze labels in each batch. Formally, given a batch of \(N\) predicted visual gaze label embeddings \(I_{i}=_{,i}^{pred}\) and their associated ground-truth class embeddings \(T_{i}=_{,i}^{gt}\) and ground-truth classes \(y_{i}\), the loss can be expressed as:

\[_{lab}=-_{i=1}^{N}(,T_{i})/)}{_{j}(s(I_{i},T_{j})/)})\]

where \(\) is a learnable temperature parameter, \(s(a,b)\) is the similarity between \(a\) and \(b\), and \(\) is a minimum subset of indices in \([1,N]\) which identifies all classes in the batch, _i.e_. \(\{y_{i},i\}=\{y_{i},i=1 N\}\) and \((j,j^{})^{2},y_{j} y_{j^{}}\). This loss aims to maximize the cosine similarity between each predicted visual gaze embedding and the corresponding class embedding, while minimizing the similarity between the negative pairs. This formulation bears a resemblance to the idea of supervised contrastive learning presented in .

**Angular Loss (\(_{ang}\)).** Optionally, we can append a second MLP head to the backbone in the gaze encoder in order to predict a normalized gaze direction vector from the input head crop. This vector can be supervised using the angular loss, which is defined based on the cosine of the angle between the predicted and ground truth gaze vectors according to:

\[_{ang}=1-<_{}^{gt},_{}^{pred }>\]

where \(<a,b>\) denotes the inner product between \(a\) and \(b\). While the angular loss doesn't influence the final performance, we found it to be very useful in interpreting model predictions. For example, it helps to understand whether a failure mode is due to the head processing part (_e.g_. when the face is not visible), or the target selection part. Furthermore, it can be informative in real-world applications when the gaze heatmap is not reliable (_e.g_. when the person is looking outside the frame).

**Global loss.** The global loss is a given by: \(=_{hm}_{hm}+_{lab}_{lab}+ _{ang}_{ang}\)

## 4 Datasets

### GazeFollow

In order to train our end-to-end architecture, we need annotations for the semantic label of the gaze target. Unfortunately, there is no gaze following benchmark that provides annotations for both the gaze target position and class label. In order to solve this problem, we design a pseudo-annotation pipeline to automatically infer the semantic label of the ground-truth gaze target. Considering that gaze following datasets come with point annotations for gaze targets, our approach is to first segment the image, then match the gaze point with the predicted semantic class of the underlying pixel. Since we need to segment the images entirely, it is important to ensure that the segmentation we perform incorporates an open vocabulary that is able to describe any object encountered in the dataset.

To this end, we use the GazeFollow dataset , and leverage two open-source projects that implement open-world segmentation using various foundational vision and language models. The first method, known as RAM-Grounded-SAM1, generates precise masks and accurate semantic labels. However, it has a tendency to overlook many regions in the image.To fill this gap and pseudo-annotate the missed gaze instances, we utilize a second method called Semantic-Segment-Anything2. This tool provides more comprehensive coverage of the image, but it comes with the trade-off of introducing noisy labels and oversegmentation -- where many small segments are constituents of a larger object. We provide a comparison of the two segmentation methods in the supplementary.

Finally, in order to reliably test our models, we manually annotate the test set of GazeFollow with target classes. This annotation process is not restricted to a predefined set of objects, but we ensure that the labels are consistent (_e.g._ by avoiding synonyms when possible). Also, since the area where a person is looking, represented as a heatmap, often includes multiple objects, we also annotated other possible gaze targets whenever possible. For example, a person cutting a cake is probably looking at both the cake and the knife. This multi-label annotation approach also helps to deal with the ambiguity related to object hierarchy (_e.g._ a person looking at the _wheel_ of a car is also looking at the _car_, so it makes sense to assign both to the gaze instance). At the end, we obtained a test set ground-truth vocabulary of \(346\) classes (_cf._ the supplementary for a word cloud).

### GazeHOI

Aside from GazeFollow, we also introduce a new benchmark for the simultaneous localization and recognition of the gaze target. Since the assignment of a class label is predicated on determining the location of the gaze target first, it is very difficult to manually annotate a new dataset from scratch. This is because we can not define a vocabulary of object classes that we want to annotate in advance, and ensure people are only looking at those objects.

To circumvent this problem, we repurpose existing human-object interaction datasets (_i.e._ annotations for person box, object box, object class and interaction verb) to create GazeHOI. The process is described as follows: (i) we combine \(5\) HOI datasets (V-COCO , HICO-DET , HCVRD , SWiG-HOI , HOI-A ), (ii) for each dataset, we manually select a subset of verbs from the top \(200\) frequent ones where the person is also likely looking at the object (_e.g._\(\) or _repair_, but not _carry_), (iii) we use an off-the-shelf head detector to detect head bounding boxes and match them to the annotated person's bounding box, then we filter out instances without detected heads, (iv) for each head-object HOI instance, we draw their bounding boxes on the corresponding image, (v) a team of annotators looks at these images, and answers _yes_ or _no_ based on whether or not the bounding boxes are correct, and the person is looking at the object they are interacting with, and (vi) discard _no_ instances. This process solves our problem, and has the benefit of reducing the usually expensive manual annotation to a simpler manual verification.

At the end, we obtain a total of \(43808\) images and \(58146\) instances after discarding about \(50\%\) of images in the verification step. Each instance is annotated with the person's body and head boxes, the object box, the object class, and the interaction verb. The vocabulary has \(985\) object classes, from which we isolate \(522\) rare classes (_i.e._ less than \(10\) instances each) into a separate split. This can be used for future research on open-vocabulary semantic gaze following, similar to the practice adopted in open-vocabulary object detection . The final dataset used in our experiments features a vocabulary of \(463\) object labels and \(55995\) gaze instances which we split into \(47214/3781/5000\) for the train, val and test sets. Last but not least, we also run a deduplication pipeline to ensure that the validation and test sets contain no images from the training sets of GazeHOI or GazeFollow since both of them are based on popular overlapping vision benchmarks. In Figure 2, we show a few samples from GazeHOI (_cf._ the supplementary for a word cloud of the vocabulary).

## 5 Experiments

### Baseline

Aside from comparing with previous gaze target localization methods from the literature, we also propose a strong baseline to assess our gaze target recognition performance given that no such work was attempted before. To this end, we design a 2-stage method as follows: first, we freeze our proposed gaze model and use it to predict a gaze heatmap. Next, we use this heatmap to condition the original image by emphasizing the focused area. Then, we apply the CLIP's  pretrained vision model on the resulting image to produce the gaze label embedding. The idea is to leverage the alignment between CLIP's vision and text encoders. Finally, in terms of conditioning the input image based on the predicted heatmap, we consider three variants: masking, blurring and cropping. Figure 3 shows an overview of the baseline as well as the different conditioning variants.

As an additional benefit, since our model's predicted label is dependent on the predicted gaze location, using our own gaze model to generate the heatmap for the baseline allows us to control for the localization factor, enabling an unbiased comparison of recognition performance.

### Comparison with the State-of-the-art

We summarize our quantitative results on the GazeFollow benchmark in Table 1 (the experimental protocol is provided in the supplementary). In terms of localization performance, our architecture sets a new state-of-the-art across both metrics, surpassing the second best method of Tafasca _et al_. by \(4.4\%\) and third best of Jin _et al_. by \(8.5\%\) on the Avg. Dist metric. In terms of recognition, our method outperforms all \(3\) variants of the frozen baseline by a significant margin, _i.e_. more than \(20\%\) flat accuracy points compared to the second best (_i.e_. crop variant).

Since CLIP has probably not been trained on many images that are heavily masked, blurred or cropped, we decided to add two more variants by fine-tuning CLIP's vision encoder: (1) we directly fine-tune the baseline (crop) on GazeFollow. During training, the cropping is based on the ground-truth heatmap instead of a predicted one. (2) instead of feeding CLIP modified images, we use the original ones and apply the conditioning at the feature level. Specifically, we perform a weighted average of the output image tokens (_i.e_. from the CLIP vision encoder applied to the input image) based on the (downscaled) heatmap (_i.e_. predicted by the gaze model), followed by a projection. Since the heatmap might not cover the entire object, this last variant allows the model to have access to the surrounding context. The performance of these two variants is much closer to our method, where the cropping one is slightly worse, and the heatmap weighting is slightly better.

It is important to remember two key facts about the design of our baselines: (i) They build upon our model's localization ability (_i.e_. the most accurate to date) which contributes to recognition performance. To verify this, we replaced the gaze heatmap prediction part in the best version of the baseline by the model from , and found that performance drops and becomes worse than our proposed model (refer to Table 1, Baseline\({}^{}\)). (ii) They add an entire vision transformer that

Figure 3: [Left] Overview of the proposed baseline architecture using masked conditioning. The similarity scores \(S_{i}\) are computed from the gaze label embedding \(I_{g}\) and the class embeddings \(T_{i}\). [Right] Comparison of the conditioning variants based on the predicted gaze heatmap: original image (top left), masking (top right), blurring (bottom left), and cropping (bottom right).

Figure 2: Samples from GazeHOI. We show the head box (white) and the object’s box (red).

is pretrained on a large-scale dataset specifically designed for such semantic recognition tasks. In terms of parameter count, our model features 116M while the baseline has 200M. This is because our model only uses an MLP head (3.3M parameters) for label prediction while the baseline uses a ViT (86.6M parameters) to encode the semantics of the image separately. That is a decrease of 42% in the total number of parameters (_cf_. the supplementary material for a comparison of FLOPS). We later show in our ablations that performance drops significantly if we try to use the same CLIP vision encoder to do both localization and recognition, which emphasizes the need for the baseline to have two specialized ViT encoders. Finally, upon manual inspection of test set samples where our model and the best performing heatmap-weighted baseline don't agree, we noticed that most of them are failures due to hierarchy or semantic similarity (_e.g_. prediction of _ball_ then _racket_ when the ground-truth is _racket_). Please refer to the supplementary material for some qualitative examples to illustrate this comparison.

On GazeHOI (_cf_. Table 2), we observe a similar trend with our method outperforming the zero-shot baselines by a large margin, while both fine-tuned variants outperform our model on recognition scores. We note that the pre-trained GazeFollow model is already able to perform the localization task (_i.e_. \(65\%\)_vs_. \(72\%\) for Gaze Accuracy). However, the zero-shot performance is much lower than the fine-tuned counterpart (_i.e_. \(30\%\)_vs_. \(58\%\)). This is probably due to the mismatch between the pseudo-labels seen in GazeFollow and the vocabulary of GazeHOI (_i.e_. about \(150\) classes are new).

Finally, we need to emphasize that unlike the baselines, our gaze label prediction head is trained from scratch, so we believe that the small scale of the datasets also plays an important role in limiting recognition performance (_i.e_. \(100K\)_vs_. \(400M\) instances).

    &  &  \\   &  & Acc@1 \(\) & Acc@3 \(\) \\  Random & 0.471 & 0.391 & 0.002 & 0.010 & 0.003 \\ Bias / Majority & 0.295 & 0.229 & 0.010 & 0.015 & 0.011 \\  Chong _et al_.  & 0.137 & 0.077 & — & — & — \\ Fang _et al_.  & 0.124 & 0.067 & — & — & — \\ Jin _et al_.  & 0.118 & 0.063 & — & — & — \\ Bao _et al_.  & 0.122 & — & — & — & — \\ Tafasca _et al_.  & 0.125 & 0.064 & — & — & — \\ Jin _et al_.  & 0.126 & 0.076 & — & — & — \\ Tafasca _et al_.  & 0.113 & 0.057 & — & — & — \\  Baseline (Mask) & 0.108 & 0.051 & 0.124 & 0.253 & 0.147 \\ Baseline (Blur) & 0.108 & 0.051 & 0.190 & 0.362 & 0.222 \\ Baseline (Crop) & 0.108 & 0.051 & 0.239 & 0.428 & 0.278 \\ Baseline (Crop) fine-tuned & 0.108 & 0.051 & 0.437 & 0.588 & 0.504 \\ Baseline (heatmap weight) & 0.108 & 0.051 & **0.466** & **0.653** & **0.542** \\ Baseline† (heatmap weight) & 0.137 & 0.077 & 0.442 & 0.620 & 0.514 \\  Ours† & **0.108** & **0.051** & 0.447 & 0.642 & 0.516 \\   

Table 1: GazeFollow dataset. The best scores are given in bold, while the second best are underlined. All baselines use our own model for gaze heatmap prediction, except for Baseline† which uses .

    &  &  \\   & GazeAcc \(\) & Acc@1 \(\) & Acc@3 \(\) \\  Random & 0.166 & 0.002 & 0.006 \\ Bias / Majority & 0.352 & 0.082 & 0.083 \\  Baseline (Mask) & 0.723 & 0.197 & 0.298 \\ Baseline (Blur) & 0.723 & 0.306 & 0.458 \\ Baseline (Crop) & 0.723 & 0.388 & 0.546 \\ Baseline (Crop FT) & 0.723 & 0.617 & 0.707 \\ Baseline (Hm weight) & 0.723 & **0.646** & **0.748** \\  Ours† & 0.652 & 0.306 & 0.463 \\ Ours & **0.723** & 0.583 & 0.706 \\   

Table 2: Results of our model and baselines on the GazeHOI dataset. The best scores are given in bold, while the second best are underlined. The † sign means the model was trained on GazeFollow and evaluated on GazeHOI without fine-tuning.

[MISSING_PAGE_FAIL:9]

predicting one gaze label embedding, we think it will likely produce some weighted average of those objects' embeddings. Furthermore, while the GazeFollow dataset is relatively small, there are significantly more objects in the images than annotated gaze instances. These additional semantic pseudo-labels (from the segmentation) can also be leveraged through auxiliary losses to align spatially localized image content with their textual counterpart (a similar approach was used in ) in order to enhance recognition performance. We leave the investigation of these ideas to future work.

**Societal Impact.** Gaze following methods can bring tremendous value in many real-world applications that foster positive change in society (_e.g._ screening neurodevelopmental disorders). However, care must be taken when deploying this technology in order to avoid privacy violations, and mitigate risks of malfunction in sensitive applications (_e.g._ surveillance systems). We encourage the community to use these models responsibly.

## 7 Conclusion

In conclusion, our study represents a step forward in gaze target detection by extending the traditional gaze following formulation to incorporate the class label of the target. Our proposed architecture has successfully integrated semantic understanding into the task while maintaining state-of-the-art performance in terms of localization. To this end, we leveraged a weakly supervised training regime based on pertinent pseudo-labels derived from open-world segmentation pipelines. Naturally, we employed a contrastive learning objective to align the visual embedding representing the gaze target with its textual counterpart, thereby allowing for some flexibility to build specialized vocabularies during inference based on the application. We hope that our code, datasets, model checkpoints and research insights will pave the way for future research on semantic gaze following.

**Acknowledgement.** This research has been supported by the AI4Autism project (Digital Phenotyping of Autism Spectrum Disorders in Children, grant agreement number CRSII5 202235/1) of the the Sinergia interdisciplinary program of the SNSF.

Figure 4: Qualitative samples from our model on images from the internet. The top row shows gaze point predictions for all people. The second row show the last attention map from the image encoder. The third row shows a predicted heatmap of a single person. The last row shows the weight of the gaze tokens in the last image to gaze cross-attention of the decoder for the same person.