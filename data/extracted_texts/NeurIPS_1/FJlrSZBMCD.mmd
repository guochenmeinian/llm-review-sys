# Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models

Aviv Bick\({}^{1*}\), Kevin Y. Li\({}^{1*}\), Eric P. Xing\({}^{12}\), J. Zico Kolter\({}^{1}\), Albert Gu\({}^{13}\)

\({}^{1}\)Carnegie Mellon University, \({}^{2}\)MBZUAI, \({}^{3}\)Cartesia.ai

{abick, kyl2}@cs.cmu.edu

Authors contributed equally to this work.

###### Abstract

Transformer architectures have become a dominant paradigm for domains like language modeling but suffer in many inference settings due to their quadratic-time self-attention. Recently proposed subquadratic architectures, such as Mamba, have shown promise, but have been pretrained with substantially less computational resources than the strongest Transformer models. In this work, we present a method that is able to distill a pretrained Transformer architecture into alternative architectures such as state space models (SSMs). The key idea to our approach is that we can view both Transformers and SSMs as applying different forms of mixing matrices over the token sequences. We can thus progressively distill the Transformer architecture by matching different degrees of granularity in the SSM: first matching the mixing matrices themselves, then the hidden units at each block, and finally the end-to-end predictions. Our method, called MOHAWK, is able to distill a Mamba-2 variant based on the Phi-1.5 architecture (Phi-Mamba) using only 3B tokens. Despite using less than 1% of the training data typically used to train models from scratch, Phi-Mamba boasts substantially stronger performance compared to all past open-source non-Transformer models. MOHAWK allows models like SSMs to leverage computational resources invested in training Transformer-based architectures, highlighting a new avenue for building such models.

## 1 Introduction

Large language models based upon Transformer architectures have become a staple of natural language processing but suffer from their reliance on quadratic self-attention -- the need to compute inner products between tokens at all positions up to the context length. This has motivated the development of several alternative subquadratic models, either approximations of self-attention  or entirely different architectures, such as state space models (SSMs) . Training strong subquadratic models such as SSMs can benefit the community through their cheaper finetuning and inference costs; however, they have not benefitted from the same amount of community effort in the form of training and compute as for Transformers. This raises a natural question: is it possible to leverage the vast amounts of resources that have been invested in training quadratic-time Transformers and use these models to produce stronger alternative models, such as state-space models?

In this paper, we present an approach for training subquadratic state-space models (specifically from the class of Mamba SSMs ) through the distillation of different elements of a pretrained Transformer model. The key intuition is viewing both Attention and SSMs as sequence transformations that mix different token embeddings by applying different classes of matrices across them. Sequence model _architectures_ can then be factored into separate (i) sequence mixing and (ii) channel mixing blocks, e.g., a Transformer is composed of Attention (sequence mixer) and MLP (channel mixer)blocks. Using this breakdown, we can separately distill the _mixing_ elements of each model explicitly at different levels of granularity. Specifically, we propose a three-phase distillation process that progressively targets higher levels of supervision from the teacher model: (1) a matrix orientation phase that aligns the sequence transformation matrices themselves; (2) a hidden-state distillation that aligns the hidden-state representations of each individual layer of the network without sacrificing pre-existing learned representations; and (3) an end-to-end training phase with weight transfer that finally distills the final output of the network using only a fraction of training data. We term our approach **MOHAWK** after these three stages (**M**atrix **O**rientation, **H**idden-**S**ate **A**lignment, **W**eight-Transfer and **K**nowledge Distillation).

We apply our approach to a modified instantiation of the Mamba-2 architecture , termed Phi-Mamba, which is aimed at more directly corresponding to the different architectural blocks of the Phi-1.5 language model  -- a very strong Transformer model at the 1.3B parameter scale. Using our approach, the Phi-Mamba model achieves performance on benchmarks _stronger than any previous Mamba models of similar size_. Although performance still lags behind that of the base Phi-1.5 model on these benchmarks, the model is distilled with only 3.0B tokens, less than 1% of the data used to train either the previously best-performing Mamba models and 2% for the Phi-1.5 model itself. For instance, our Phi-Mamba achieves a 71.7% accuracy on the Winogrande dataset, compared to the pretrained Mamba-2 model's 60.9% accuracy, and 44.1% accuracy on the ARC-C dataset, compared to Mamba-2's 33.3% accuracy. Our results highlight the benefit of our three-phase distillation approach: we show in ablation experiments that each phase is highly beneficial for the eventual performance of the model, and that, e.g., _only_ attempting to directly distill the Phi-1.5 model (i.e., Phase 3 alone) substantially underperforms the full MOHAWK method. Moreover, our findings emphasize the benefits of state-space models while training on fewer than \(100\) tokens than the original pretrained Mamba model.

## 2 Related Work

Sequence Models.State-of-the-art autoregressive language models have been pretrained on massive amounts of data, resulting in models that exhibit extensive downstream capabilities, such as zero-shot translation and long-range reasoning . Recent work has focused on addressing the quadratic complexity of Transformers by developing subquadratic alternatives based on RNN , SSM , and linear attention mechanisms , highlighting the importance of efficient sequence models in the era of large-scale autoregressive language models.

Figure 1: Plot of trained token budget to averaged accuracy on Winogrande, Arc-E, Arc-C, PIQA, and Hellaswag on various open-source models (mainly non-Transformer-based models). Our model (Phi-Mamba) uses more than \(33\) less token budget to achieve 5% higher average accuracy than the next best model.

Distillation.Knowledge distillation can be used to transfer knowledge from a large teacher model to a smaller student model, resulting in a more efficient model that retains the performance of the teacher model . Distillation has been applied to various language modeling tasks, such as text generation [5; 17], machine translation [16; 48; 36], and question-answering system [19; 44].

Distillation in language models has been largely focused on _compression_: turning a larger pretrained Transformer into a smaller one by utilizing the weights of the teacher model [41; 21; 42]. Some of the techniques proposed look similar to ours; for example,  match attention matrices in a step similar to our matrix orientation, and  align outputs of each block (i.e., the hidden states). However, these differ in subtle and important ways because of our setting; for example, the former uses a different loss function than us that relies on softmax attention, and the latter is an end-to-end objective while our hidden state alignment occurs completely independently block-per-block. Consequently, prior work has observed that combining these objectives does not actually help or even hurts distillation , whereas we show that our techniques all significantly help improve the student model.

A smaller body of work has focused on our objective of distilling _across architectures_, in particular, turning a pretrained Transformer into a different architecture (usually some recurrent model) of the same size.  converted a pretrained softmax attention into linear attention by directly transferring weights and continuing fine-tuning. A similar approach was taken by concurrent works for converting Attention into linear RNNs [27; 40]. Recently, [47; 46] also proposed distilling into linear attention by first matching attention matrices. Our approach differs by using a different loss function that works beyond linear attention; incorporating more stages (e.g., the hidden state alignment step); and using recent, more expressive classes of efficient student models (Mamba-2), which we show are significantly easier to distill into (Table 4).

## 3 Preliminaries

To facilitate a clear understanding of our distillation approach, we start with the necessary background and definitions. An overview of the Mamba-2 architecture, which forms the foundation of our Phi-Mamba model, is also provided.

### Matrix Mixers

Following , we refer to an equivalent function that represents the input and output of a sequence model as a _sequence transformation_ or a _sequence mixer_. Formally,

**Definition 1** (Sequence Transformation).: _We use the term to refer to a parameterized map on sequences \(Y=f_{}(X)\) where \(,^{(T,P)}\) and \(\) is an arbitrary collection of parameters. \(T\) represents the sequence or time axis; subscripts index into the first dimension, e.g. \(X_{t},Y_{t}^{P}\)._

To put it differently, sequence mixers combine tokens at various time steps, facilitating the model's comprehension of temporal information and interactions. Sequence transformations form the foundation of deep sequence models, being integral components of neural network frameworks such as Transformers. A particular family of sequence transformations can be represented by \(=\) for a matrix \(^{(T,T)}\), which we refer to as a _sequence transformation matrix_ or _matrix mixer_. An example of such a matrix mixer is the vanilla self-attention, \((^{})\), which is applied to the input-dependent \(\) resulting in the familiar \((^{})\). Similarly, Linear Attention  has a sequence transformation matrix of the form \(^{}\). In addition, we can easily obtain their causal variants by multiplying by \(\), a lower triangular matrix filled with 1s, to obtain \((^{})\) and \(^{}\), respectively. Another example is a Toeplitz matrix \(\) used to perform discrete convolution on input \(\), resulting in \(\).

A naive approach to computing the output of a sequence transformation is to multiply the input sequence \(\) by the matrix \(\). However, this approach has a time complexity of \(O(T^{2})\), which is prohibitive for long sequences. Subquadratic sequence transformations, such as Mamba-2, have been developed to address such inefficiencies through structured matrix multiplication.

### Mamba-2

Mamba-2 , a type of structured state space models (SSMs) [13; 11], was recently introduced. Similarly to the original Mamba model , Mamba-2 uses a time-varying state-space model which can selectively focus on or ignore inputs due to its input-dependent parameterization of the system components. The time-varying SSM is defined as follows:

\[h_{t+1} =_{t}h_{t}+_{t}x_{t} \] \[y_{t} =_{t}h_{t}\]

Here, \(_{t}\) and \(_{t}\) are input-dependent projections of the system, as in Mamba-1; however, \(_{t}\) is the identity matrix \(\) multiplied by a scalar \(_{t}\). The above formulation also differs from the previous one by treating the underlying sequence as originating from a discrete signal instead of a continuous one and therefore omits the sampling component \( t\) from the original Mamba model.

Importantly, Mamba-2 draws a new connection between SSMs and Transformers, termed _Structured State Space Duality (SSD)_, which shows that a special case of SSMs can be viewed as a form of causal linear attention. In particular, fixing \(_{t}=I\) (a further restriction of Mamba-2 to \(_{t}=1\)) results in the formulation of causal linear attention  with the matrices \(\) and \(\) representing the projections of the key and the query, respectively, while the input projection \(\) corresponds to the projection of the value.

Mamba-2 as a matrix sequence transformation.Inspired by the aforementioned connection between SSMs and Transformers,  shows that Mamba-2's _SSD_ mixer family is equivalent to sequentially-semi-separable matrices . Formally, the SSD mixer family can be represented as:

\[ h_{t+1}&=_{t} Ih_{t}+ x_{t}\\ y_{t}&= h_{t} _{1}&0&0&&0\\ _{2:1}&_{2}&0&&0\\ _{3:1}&_{3:2}&_{3}&&0\\ &&&&\\ _{n:1}&_{n:2}&_{n:3}&&_{n}(C  B^{}) X \]

where \(_{t:i}=_{t-1}_{t-2}_{i}\). An interesting observation is that the Mamba-2 architecture can be viewed as a causal linear attention with a _learnable causal mask_.

## 4 Methods

Throughout this section, we will describe each phase of MOHAWK. Specifically, we will cover the stages of matrix orientation, hidden-state alignment, and knowledge distillation, all three of which are crucial for developing an effective student model from the pretrained Transformer model. Unlike traditional distillation techniques, the student model retains the overall architecture of the teacher model, differing only in the replacement of the attention matrix mixer with a subquadratic alternative. We will progressively unveil our architecture, Phi-Mamba, along with the specifics of its distillation process. This section concludes with an in-depth description of the Phi-Mamba architecture and its hybrid version, which surpasses the performance of other subquadratic matrix mixers. Further examinations of the effectiveness of the method and ablation studies are discussed in Section 5.

For clarity, the term _block_ refers to the repeating components that form the end-to-end model. The blocks are composed of _layers_, such as the self-attention layer (including projections), the SSM layer (including the mixer and convolution), and the convolutional layer. In this manner, many Transformer models, like Llama , are viewed as a stack of alternating self-attention and MLP blocks, whereas the Phi and Phi-Mamba models are comprised of Phi blocks that have parallel Attention/SSM and MLP blocks.

### Stage 1: Matrix Orientation

The first stage of MOHAWK aims to align the student matrix mixer with the teacher's self-attention matrix. Achieving this alignment is a two-step process: first, at every mixing layer, the student components preceding the matrix mixer are set to match the teacher's components. This ensures that each layer's input undergoes the same transformation up to the matrix mixer section. Consequently, the only variation from the input to the mixing process is the matrix calculation. We then minimize the distance between the matrix mixer, e.g., the self-attention matrix and the materialized SSM matrix (2), of each layer within the student and teacher models:

\[_{}\|()-_{}()\|_{F} \]where \(\) denotes the parameters within the student's sequence mixing layer, and \(\) indicates any arbitrary input. In our experimental setup, \(\) was chosen as the output from the teacher model's preceding layer to better mimic the input distribution to the layer. This stage ensures that the student and teacher models have roughly similar mixing layers and sets the foundation for the subsequent stages of the distillation process. In particular, this stage can be done in parallel across all the student layers, as the inputs to the student and teacher blocks are identical.

For Mamba-2, we begin by setting the convolution to an identity function, effectively nullifying its initial impact. This results in the computation of the semi-separable matrix being the sole distinction between the layers. We then proceed to minimize the distance between the two matrix mixers: the semiseparable scalar identity and the attention matrix (see Figure 2). Figure 3 demonstrates the importance of this stage in the distillation process. Furthermore, Table 3 shows that the Mamba-2 matrix mixer is more expressive than popular alternatives and can closely approximate the self-attention matrix of various data samples across all layers of a Transformer model through gradient descent, solidifying it as a strong sequence mixer.

### Stage 2: Hidden-State Alignment

Following the optimization of Equation (3), we must still address the differences between the outputs of the student and teacher blocks. To achieve this, we further align the components of the two blocks using initialization and distillation. Specifically, our goal is to match each student and teacher mixing blocks by minimizing the L2 norm of their output (e.g., the entire Mamba block with the self-attention block):

\[_{}\|()-_{}()\|_{2} \]

where similar to Section 4.1, \(\) represents student's block parameters, and \(\) is an input. Once again, this stage can be done in parallel across all the student layers.

In the case of Mamba-2, we modify the remaining components to be identical to the Phi-1.5's Attention block, so that the overall functionality is preserved from Stage 1. Concretely, we initialize the gate (see Figure 2) to a constant value of 1 to "open" the gate, canceling its initial effect. In addition, we remove the normalization prior to the output projection, as it cannot be set to align with the Attention block. We then minimize the distance between the output of the Mamba-2 block and the output of the teacher's self-attention block. Our analysis indicates that the distance between the Mamba-2 block and the self-attention block is strongly correlated with the model's ability to learn the teacher's distribution, as shown in Table 3. Furthermore, Figure 3 shows that a better independent alignment of the student and teacher blocks results in performance improvements, highlighting the importance of this stage in the distillation process.

### Stage 3: Weight-Transfer and Knowledge Distillation

The final stage of the distillation process aims to fine-tune the student model to match the performance of the teacher model. Although each student mixing block is aligned with its corresponding teacher mixing block, discrepancies are still present between consecutive blocks throughout the network To bridge these gaps and address the remaining components of the language model, we transfer the remaining weights of the teacher model to the student's respective components. For Phi-Mamba, this involves the token embedding, the final layer normalization, the Language Model head, and the MLP and input norm at each block (see Figure 2). We then fine-tune the complete end-to-end student model under teacher supervision. Concretely, we use a distillation loss to encourage the student model to mimic the distribution of the teacher model's logits, also known as knowledge distillation :

\[_{}_{}(), _{}() \]

where \(\) is the input tokens to the models.

It has been hypothesized that much of the information stored in language models resides in MLP blocks . To utilize the work already done pretraining the teacher, MOHAWK adjusts the structure of the student blocks to utilize the MLP in the same way as the teacher model, effectively swapping the teacher's matrix mixer with that of the student.

Interestingly, during this step, the MLP weights _can be kept frozen_ while keeping the model performant. This showcases Mamba-2's powerful expressiveness crucial for replacing Attention, cuts the number of trained parameters by more than half, and, in larger models, helps prevent the student model from experiencing catastrophic forgetting of the teacher model's information. We validate Mamba-2's ability to do so in Table 5.

### Phi-Mamba architecture

Combining the three stages of MOHAWK, we introduce the _Phi-Mamba_ architecture, which merges the Mamba-2 model of  with the Phi-1.5 Transformer model of . It consists of a stack of Phi-Mamba blocks (Figure 2), initialized and distilled as described in previous sections.

Overall, the Phi-Mamba architecture, as depicted in Figure 2, differs from the vanilla Mamba-2 architecture by modifying the structure of the SSM matrix mixer, removing components from the SSM block and incorporating dense layers from the teacher model. In particular, each Mamba-2 block was modified by removing post-convolution activation and pre-output projection normalization, while setting the gate and convolution to be identity functions. Interestingly, although these components were found to be beneficial for performance when Mamba-2 was trained from scratch , we find that they are unnecessary for our distillation process.

Two key changes were made to the Mamba-2 matrix mixer. The first was converting the SSM head structure from multi-value to multi-head, much like the multi-head attention mechanism found in Transformers , enabling the independent distillation of each Transformer head into a Mamba head. Moreover, we handle the sequence mixer as entirely discrete-time by making the \(\) matrix a projection of the input and eliminating the \(\) discretization parameter. Although this formulation slightly differs from Mamba-2, the original algorithm can still be applied as a black-box method.

## 5 Empirical Validation

We empirically validate the MOHAWK framework is able to achieve better performance on various downstream benchmarks compared to previous subquadratic models of similar size. Our final Phi-Mamba-1.5B model is distilled on 3 billion tokens (distributed as 80M in Stage 1, 160M in Stage 2, and 2.76B tokens in Stage 3 as described in Appendix A) from the C4 dataset, with a sequence length of 2048. This constitutes less than 1% of the resources used by many top-performing subquadratic open-source models (e.g., the original Mamba-1/2 models pretrain on 315B tokens).

Table 1 presents a comprehensive breakdown of downstream evaluation results for our models and multiple baselines on a standard set of commonsense reasoning and language understanding tasks: WinoGrande , HellaSwag , PIQA , ARC-Challenge and ARC-Easy , and

Figure 2: The Phi-Mamba architecture consists of a stack of blocks, each of which contains a Mamba block and an MLP block. The Mamba block is a simplified version of the Mamba-2 block  that omits the non-linear activation function after the convolutional operation and the layer normalization present before the output projection, so that the parts of the model outside the matrix mixer can be transferred from the teacher model. The MOHAWK distillation process involves progressively matching fine-to-coarse parts of the model to the corresponding part of the teacher model: (1) the mixer mixer itself (2) the full Mamba vs. Attention blocks, and (3) the end-to-end model.

LAMBADA . Figure 1 shows the performance versus the training cost of Phi-Mamba compared to many open-source baselines from the literature at similar model sizes.

For the remainder of this section, we will analyze the impact of the 3 stages of MOHAWK one by one. Throughout the experiments detailed in this section, we use the AdamW optimizer with \(=(0.9,0.95)\), a weight decay of 0.1, and a learning rate of \(1 10^{-4}\), combined with a Warmup-Stable-Decay (WSD) scheduler featuring 10% warmup and 10% decay. The training law figures and the final Phi-Mamba model use the regime detailed in Appendix A.

### Stage 3 (Weight-Transfer and Knowledge Distillation)

As described in Section 4.3, this phase employs a simple end-to-end distillation of teacher-model logits. It leverages the alignment among all sequence mixers and successive blocks to jointly fine-tune all components of the network. Experiments shown in Table 2 highlight the relevance of implementing this end-to-end alignment, with all three architectures achieving their highest scores only after this phase. Predictably, the impact of end-to-end alignment varies by architecture: models with more mixing layers similar to the teacher model see a reduced importance of this phase.

Stage 3 is the only stage in MOHAWK that trains the student model end-to-end and can be seen as the "main" stage. Many distillation methods employ only this stage; however, Table 2 shows that using only end-to-end knowledge distillation is less than ideal. Although it is slightly advantageous to use only Stage 3 compared to only Stage 2, there is a significant gap between using only Stage 2 versus using Stage 2 + 3. As elaborated in Section 6, this phase can freeze all network components except the Mamba-2 sequence mixer without a significant performance drop. This in particular indicates that the third stage (like the other stages of MOHAWK) can operate in computationally limited settings.

### Stage 2 (Hidden-State Alignment)

Following the analysis of the model's end-to-end distillation in Stage 3, we evaluate the impact of aligning the hidden-state outputs of mixer blocks (Stage 2) on both the subsequent Stage 3 process

   Model & Tokens & WinoG. & Arc-E & Arc-C & PIQA & Hellas. & Lamb. & Avg. \(\) \\  Phi-1.5-1.3B & 150B & 73.4 & 75.6 & 48.0 & 76.6 & 62.6 & 53.4 & 64.9 \\ 
**Phi-Mamba-1.5B** & 3.0B & **71.7** & **74.0** & **44.1** & **75.5** & 60.2 & 50.1 & **62.6** \\ Mamba-1-1.4B & 315B & 61.5 & 65.5 & 32.8 & 74.2 & 59.1 & 64.9 & 59.7 \\ Mamba-2-1.3B & 315B & 60.9 & 64.3 & 33.3 & 73.2 & 59.9 & 65.7 & 59.6 \\ Finch-1.6B & 1.1T & 59.4 & 64.2 & 34.1 & 72.6 & 57.3 & **66.8** & 59.1 \\ \(\)LSTM-1.4B & 300B & 60.6 & 64.3 & 32.6 & 74.6 & **60.9** & 57.8 & 88.5 \\ Eagle-1.5B & 1.1T & 59.1 & 64.3 & 33.5 & 71.1 & 55.0 & 65.7 & 58.1 \\ Pythia-1.4B & 300B & 57.3 & 60.6 & 26.0 & 71.1 & 52.1 & 61.6 & 54.8 \\ RWKV4-1.5B & 330B & 54.6 & 60.5 & 29.4 & 72.4 & 52.5 & 56.4 & 54.3 \\ DeltaNet-1.3B & 100B & 53.6 & 57.2 & 28.3 & 71.2 & 50.2 & 48.9 & 51.6 \\ GLA-1.3B & 100B & 53.9 & 57.2 & 26.6 & 71.8 & 49.8 & 46.9 & 51.0 \\   

Table 1: Downstream evaluation results for full methods, comparing Phi-Mamba against open-source models of similar sizes pretrained on standard language modeling corpuses. Phi-Mamba attains performance close to the teacher model and better than all pretrained models, while using less than \(1\%\) of the training data.

   Stages & WinoG. & ARC-E & ARC-C & PIQA & Hellas. & Lamb. & Avg. \\ Type & Applied & Acc \(\) & Acc \(\) & Acc \(\) & Acc \(\) & Acc \(\) & Acc \(\) \\ 
2 & 55.9 & 75.4 & 38.0 & 75.2 & 56.6 & 18.9 & 53.3 \\
3 & 62.8 & 64.3 & 27.8 & 75.6 & 52.6 & 43.8 & 54.5 \\
2-3 & 72.3 & 75.0 & 40.8 & 75.2 & 59.7 & 50.6 & 62.3 \\
1-3 & 74.8 & 72.7 &and overall downstream model performance. We accomplish this by training Phi-Mamba instances from scratch using Stage 2 to various token counts. From these checkpoints, we proceed to Stage 3 training, ending with different total budgets to allow us to analyze how the degree of Stage 2 "pretraining" impacts Stage 3 performance at various token budgets.

Figure 3 demonstrates that given an adequate training budget, models beginning with weights with lower hidden state distances (after Stage 2) outperform those that depend exclusively on knowledge distillation (Stage 3). These lower hidden states are also correlated with lower starting perplexities, which in turn are correlated with downstream performance, as shown in Figure 5. Furthermore, Table 2 shows the synergy between Stage 2 and Stage 3, as applying Stage 3 on top of Stage 2 outperforms vanilla knowledge distillation, highlighting the importance of incorporating both hidden-state alignment and knowledge distillation methods for the tested architectures.

### Stage 1 (Matrix Mixer Orientation)

Motivated by our previous finding, we then analyze how matching the matrix mixers can decrease the overall mixer block's hidden-state distance with the teacher model even further. Similarly to our previous protocol, we assess the positive impact of the current stage on the following phase's metrics and final model's performance by comparing models with varying amount of Stage 1 and Stage 2 training on both stage metrics.

Figure 4 shows that even with constrained budgets, performing Stage 1 for a small period can help with subsequent stages and their performances. Thus, even a small amount of Stage 1 training can help their respective Stage 2 models reach better hidden-state distances compared to the from-scratch counterpart. This is despite the phenomenon that the teacher and student mixers diverge and then re-converge in Stage 2 after mixer similarity is no longer directly optimized. Coupled with Section 5.2, which discovers that lower hidden state initializations lead to better perplexity and downstream performance, it can be inferred that Stage 1 aids the overall distillation process. We further empirically validate this intuition in Table 2, which indicates that this stage aligns the matrix mixers to a stronger degree than only the hidden state alignment.

## 6 Self-Attention Approximation with Structured Matrix Mixers

We start by testing the ability of various matrix mixer families to match the empirical self-attention matrices of a pretrained Transformer. We take 1000 samples from each layer of a Llama2-7b-Chat model , materialize the attention matrices, and project them onto given classes of structured matrices. The results in Table 3 are averaged across all layers.

Figure 3: Training laws comparing the token budget between Stages 2 and 3, as measured by the Stage 2 metric (hidden state distance) and Stage 3 metric (perplexity). Stage 2 initializations are used as the starting checkpoint for their respective Stage 3 finetuning models. Stage 3 pretrained is trained from scratch only with weight transfer and knowledge distillation. Despite training for less tokens on Stage 3 than the Stage 3 from scratch, almost all Stage 2 initialized models eventually outperform the baseline in perplexity on a fixed budget. In general, better aligned Stage 2 initializations improve post-Stage 3 performance.

In particular, to describe the class of linear attention matrices (3.1), we use the fact that \(\) and \(\) are projections of the input \(x^{d_{in}}\) onto \(^{d_{out}}\), and therefore their rank is bounded by \(\{d_{in},d_{out}\}\). For multihead linear attention, \(d_{out}\) (also known as head dimension) is typically a small value (e.g., Phi-1.5 and Llama2-7b-Chat have head dimensions of \(64\) and \(128\), respectively). Thus, we approximate this family of sequence mixers using causal low-rank matrices \(^{}\), where \(\) is a lower-triangular causal mask of 1s, and \(\), \(\) are in \(^{n d}\) with \(d n\) (indicating that the head dimension is substantially smaller than the sequence length).

To describe the multi-head Mamba-2 matrix family, we utilize the state space dual (SSD) layer (3.2) in a manner similar to the previous linear attention, but now the causal matrix \(\) possesses an \(n\)-degree rolling multiplicative structure for \(\) which can be seen as a more expressive mask that generalizes the causal mask (Section 3.2).

Both causal low-rank and SSD matrix families were approximated with 10,000 steps of gradient descent per sample. To approximate the general class of SSM matrix mixers, we utilize _balanced truncation_, a gradient-free projection algorithm. This method is mainly known in the field of time-invariant Dynamical System model reduction  and has been modified for use in time-varying systems . Similarly, for the family of causal Toeplitz matrices, representing convolution operations, we employ a simple heuristic that minimizes the error for each attention matrix.

Table 3 shows that while the SSM matrix family provides the closest approximation to the self-attention matrix mixer, the Mamba-2 mixer family (SSD) has just twice the distance from the SSM matrices. This is in contrast to Linear Attention, which has three times the distance, all while keeping a computational cost on par with Linear Attention. More details can be found in Appendix C.

We further validate the ability of a Mamba-2 block to replace an Attention layer within a language model. Firstly, we create two variants of our architecture, Phi-Toeplitz and Phi-LR, and run the MOHAWK process for 1B tokens at each stage (see Table 4) to verify that the previous finding

Figure 4: Training laws comparing the token budget between Stages 1 and 2, as measured by the Stage 1 metric (matrix mixer distance) and Stage 2 metric (hidden state distance). Even a small amount of Stage 1 training can improve the model’s hidden-state distances in subsequent stages. Notably, this improvement occurs despite an increase in matrix mixer distance during Stage 2. This suggests that early Stage 1 training provides a foundational benefit that enhances the model’s performance in later stages, demonstrating the importance of initial training phases in model optimization.

  Structure & Toep. & LR & SSD & SSM & LR & SSD & SSM & LR & SSD & SSM \\ (_State size N_) & - & (16) & (16) & (16) & (32) & (32) & (32) & (64) & (64) & (64) \\  WT-103 & 12.0 & 0.619 & 0.477 & 0.266 & 0.322 & 0.237 & 0.127 & 0.132 & 0.097 & 0.046 \\ OWT & 12.2 & 0.606 & 0.466 & 0.259 & 0.314 & 0.231 & 0.123 & 0.129 & 0.095 & 0.045 \\ C4 & 12.3 & 0.595 & 0.453 & 0.236 & 0.310 & 0.226 & 0.112 & 0.128 & 0.093 & 0.041 \\ IMdB & 12.3 & 0.598 & 0.455 & 0.238 & 0.312 & 0.226 & 0.113 & 0.129 & 0.094 & 0.043 \\  

Table 3: Attention matrix approximation by structured matrix mixers (Frobenius distance; lower is better). Structures are Toeplitz, low-rank (LR), state space dual (SSD) model (3.2) and general semi-separable matrices (SSM), all causal. We used 1,000 samples, each 512 tokens. Samples were passed through Llama2-7B-Chat, and one attention head from each layer was randomly chosen for approximation. We evaluated (LR) and \(\) families with 10,000 gradient descent steps per sample.

hold in a multilayer, end-to-end model case. Secondly, we run MOHAWK while freezing various parts of the Phi-Mamba modules (refer to Table 5), revealing that limiting the trainable elements to the Mamba-2 blocks (excluding the embedding, head and all MLP layers) results in only a minor performance decrease during MOHAWK distillation.

Interestingly, in all of the aforementioned experiments, we have found a consistent correlation between the projection distances of the matrix (Frobenius distance) in Table 3 and the downstream performance metrics (accuracy) in Table 4. Essentially, a better matrix approximation (lower Frobenius distance) is correlated with better model performance (higher accuracy) on various tasks. This connection highlights the relationship between the quality of the matrix approximation and the performance of the model. Such findings are echoed in , which find that more expressive matrix mixers lead to more performant models, e.g., Low-rank-based BERT models outperform Toeplitz-based ones.

## 7 Discussion and Conclusion

Our experiments shows that the Mamba-2 model can be successfully distilled from a pretrained Transformer teacher model, utilizing its extensive knowledge learned from custom datasets and higher computational resources. Despite using less than \(100\) data compared to many open-source models, including Mamba, our subquadratic model outperforms other subquadratic models in various benchmark tests by a wide margin.

The MOHAWK framework's multi-stage process which gradually increased the scope of distillation is essential extracting the teacher model's knowledge to the fullest extent as shown in our ablations and training laws. We continue to find the effectiveness of MOHAWK when distilling hybrid Attention-SSM models and provide ablations on the number and position of Attention layers.

Additionally, we demonstrate that Mamba-2's relationship to Transformers is evident not only in theory, but also in practice, as it captures interactions similar to those of Transformers, and is able to replace Attention with little drop in performance. Coupled with past research which has posited that much of a language model's knowledge is embedded in the MLP blocks, we believe that any subquadratic model with a sufficiently expressive matrix mixer can replicate the behavior of pretrained Transformers, bringing quadratic knowledge to subquadratic models. We recommend further research to explore the role of sequence mixing layers in subquadratic models and their impact on performance. Advancements in both the distillation process and the sequence mixer architecture could lead to further improved performance in a range of tasks. We propose that "trainability" and "distillability" are distinct properties of the models, and therefore, distillation techniques should be more appropriately tailored to the model.

   Matrix & Block Output & WinoG. & ARC-E & ARC-C & PIQA & Hellas. & Avg. \\ Structure & L2 Dist. \(\) & Acc \(\) & Acc \(\) & Acc \(\) & Acc \(\) & Acc \(\) & Acc \(\) \\  Causal Toeplitz & 9.6 & 49.3 & 21.2 & 26.2 & 52.3 & 25.9 & 35.0 \\ Causal low-rank & 7.6 & 50.2 & 27.9 & 25.0 & 53.3 & 25.6 & 36.4 \\ SSD & **5.5** & **67.2** & **71.0** & **38.6** & **74.2** & **45.0** & **59.2** \\   

Table 4: Ablations of matrix structure using the same training recipe (Stages 2 and 3). While many efficient sequence models (e.g. global convolutions, linear attention, and state space models) can be represented as structured matrix mixers (e.g. Toeplitz, low-rank, and semi-separable matrices respectively), more expressive structured matrix families can match the attention matrix more closely.

   Trainable & WinoG. & ARC-E & ARC-C & PIQA & Hellas. & Lamb. & Avg. \\ Components & Acc \(\) & Acc \(\) & Acc \(\) & Acc \(\) & Acc \(\) & Acc \(\) & Acc \(\) & Acc \(\) \\  All & 74.8 & 72.7 & 43.5 & 75.6 & 59.6 & 49.2 & **62.7** \\ Mamba-2 & 69.1 & 73.5 & 43.8 & 74.7 & 59.3 & 48.2 & **61.4** \\   

Table 5: MOHAWK distillation for Phi-Mamba-1.5B on the entire model vs just the Mamba-2 component, i.e., frozen MLP, embedding, etc. MOHAWK can be used on just the sequence mixer blocks while keeping all other components frozen without compromising performance (Section 5.1).