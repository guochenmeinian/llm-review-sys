# Adaptive Visual Scene Understanding:

Incremental Scene Graph Generation

Naitik Khandelwal\({}^{1,\,2}\), Xiao Liu\({}^{1,\,2}\) Mengmi Zhang\({}^{1,\,2}\)

\({}^{1}\) College of Computing and Data Science, Nanyang Technological University (NTU), Singapore

\({}^{2}\) Deep NeuroCognition Lab, Agency for Science, Technology and Research (A*STAR), Singapore

Address correspondence to mengmi.zhang@ntu.edu.sg

###### Abstract

Scene graph generation (SGG) analyzes images to extract meaningful information about objects and their relationships. In the dynamic visual world, it is crucial for AI systems to continuously detect new objects and establish their relationships with existing ones. Recently, numerous studies have focused on continual learning within the domains of object detection and image recognition. However, a limited amount of research focuses on a more challenging continual learning problem in SGG. This increased difficulty arises from the intricate interactions and dynamic relationships among objects, and their associated contexts. Thus, in continual learning, SGG models are often required to expand, modify, retain, and reason scene graphs within the process of adaptive visual scene understanding. To systematically explore Continual Scene Graph Generation (CSEGG), we present a comprehensive benchmark comprising three learning regimes: relationship incremental, scene incremental, and relationship generalization. Moreover, we introduce a "Replays via Analysis by Synthesis" method named RAS. This approach leverages the scene graphs, decomposes and re-composes them to represent different scenes, and replays the synthesized scenes based on these compositional scene graphs. The replayed synthesized scenes act as a means to practice and refine proficiency in SGG in known and unknown environments. Our experimental results not only highlight the challenges of directly combining existing continual learning methods with SGG backbones but also demonstrate the effectiveness of our proposed approach, enhancing CSEGG efficiency while simultaneously preserving privacy and memory usage. All data and source code are publicly available here.

## 1 Introduction

Scene graph generation (SGG) aims to extract object entities and their relationships in a scene. The resulting scene graph, carrying semantic scene structures, can be used for a variety of downstream tasks such as object detection, image captioning , and visual question answering . Despite the notable advancements in SGG, current works have largely overlooked the critical aspect of continual learning. In the dynamic visual world, new objects and relationships are introduced incrementally, posing challenges for SGG models to account for new changes without forgetting previously acquired knowledge. This problem of Continual ScenE Graph Generation (CSEGG) holds great potential for various applications, such as real-time robotic navigation in dynamic environments and adaptive augmented reality experiences.

The field of continual learning has witnessed significant growth in recent years, with a major focus on tasks such as image classification , object detection , and visual question answering . However, these endeavors have largely neglected the distinctive complexities associated with CSEGG. Here, we highlight several unique challenges of CSEGG: (1) In contrast to object detection,SGG involves understanding and capturing the relationships between objects, which can be intricate and diverse. Consequently, in CSEGG, conveying the spatial and semantic relationships between objects demands adaptive reasoning from the dynamic scene. (2) SGG introduces a higher level of combinatorial complexity than object detection and image classification because each detected object pair may have multiple potential spatial and functional relationships. Thus, as new objects are introduced to the scenes, the complexity of relationships among all the objects increases significantly in a non-linear fashion. (3) The long-tailed distribution in both objects and relationships in SGG can be attributed to the inherent characteristics of real-world scenes, where certain objects are more prevalent than others. Consequently, CSEGG requires the computational models to adapt continually to the evolving long-tailed distributions over different scenes. Due to a scarcity of research specifically addressing these challenges of CSEGG, there is a pressing need for specialized investigations and methodologies to enable computational models with the ability of CSEGG.

In this study, we re-organize existing SGG datasets  to establish a novel and comprehensive CSEGG benchmark with 3 learning protocols as shown in **Fig. 2. (S1).** Relationship-incremental setting: an SGG agent learns to recognize new relationships among familiar objects within the same scene. **(S2).** Scene-incremental setting: an SGG agent is deployed in new scenes where it has to jointly learn to detect new objects and classify new relationships. **(S3).** Relationship generalization setting: an SGG agent generalizes to recognize known relationships among unknown objects, as the agent learns to recognize new objects.

We curate a set of competitive CSEGG baselines by directly combining three major categories of continual learning methods with two SGG backbones and benchmark them in our CSEGG dataset. Their inferior performances show the difficulties of our benchmark tasks, which require the ability to expand, modify, retain, and reason scene graphs within the process of adaptive visual scene understanding. Specifically, the weight-regularization methods fail to estimate the importance of learnable parameters given the complicated model design in SGG backbones. Although image-replay methods retain knowledge from prior tasks through replays, the extensive combinatorial complexity of relationships among objects surpasses the complexity accommodated by a restricted set of replay images with efficient storage. Additionally, none of these baseline methods consider the shifts inherent in long-tailed distributions in dynamic scenes.

To address the CSEGG challenges, we present a method called "Replays via Analysis by Synthesis", abbreviated as RAS. RAS employs scene graphs from previous tasks, breaks them down and re-composes them to generate diverse scene structures. These compositional scene graphs are then used for synthesizing scene images for replays. Due to its nature of symbolic replays, RAS does not require the storage of original images, which often carry excessive and redundant details. This also ensures data privacy preservation and data efficiency. Furthermore, by synthesizing scenes using composable scene graphs, RAS maintains the semantic context and structure of previous scenes and also enhances the diversity of scene generation. To prevent biased predictions stemming from long-tailed distributions, we moderate the distribution of replayed scene graphs by balancing tail and head classes. This ensures a uniform sampling of relationships and objects during replays. Extensive

Figure 1: (a) **A scene graph is a graph structure,** where objects are represented as nodes (red boxes), and the relationships between objects are represented as edges connecting the corresponding nodes (green boxes). Each node in the graph contains information such as the object’s class label, and spatial location. The edges in the graph indicate the relationships between objects, often described by predicates. A scene graph can be parsed into a set of triplets, consisting of three components: a subject, a relationship predicate, and an object that serves as the target or object of the relationship. The graph allows for a compact and structured representation of the objects and their relationships within a visual scene. (b) **An example CSEGG application** is presented, where a robot continuously encounters new objects (blue) and new relationships (yellow) over time across new scenes.

experiments underscore the effectiveness of our approach. Network analysis reveals our crucial design choices that can be beneficial for the future development of CSEGG models.

## 2 Related Works

**Scene Graph Generation Datasets.** Visual Phrase  stands as one of the earliest datasets in the field of visual phrase recognition and detection. Over time, various large-scale datasets have emerged to tackle the challenges of Scene Graph Generation (SGG) on static images [23; 42; 25; 27; 37; 78; 74; 72; 80; 12; 35; 81]. Subsequent works further extend the SGG to dynamic videos [22; 50; 56]. Despite the significant contributions of these datasets to SGG, none focuses on continual learning in SGG. As the preliminary efforts towards CSEGG, we start with fundamental and straightforward settings of SGG on static images. Among all the SGG datasets on static images, the Visual Genome dataset  has played a pioneering role by providing rich annotations of objects, attributes, and relationships in images. Thus, we re-structure the Visual Genome dataset  and establish a novel and comprehensive CSEGG benchmark, where AI models are deployed to dynamic scenes where new objects and new relationships are introduced.

**Scene Graph Generation (SGG) Models.** SGG models are categorized into two main approaches: top-down and bottom-up. Top-down approaches[38; 77] typically rely on object detection as a precursor to relationship prediction. They involve detecting objects and then explicitly modeling their relationships using techniques such as rule-based reasoning or graph convolutional networks . On the other hand, bottom-up approaches focus on jointly predicting objects and their relationships in an end-to-end manner [34; 35; 72]. These methods often employ graph neural networks [33; 82] or message-passing algorithms  to capture the contextual information and dependencies between objects. Furthermore, recent works have explored the integration of language priors [48; 42; 69] and attention mechanisms in transformers  to enhance the accuracy and interpretability of scene graph generation. However, none of these works evaluate SGG models in the context of continual learning. In our work, we directly combine continual learning methods with SGG backbones and benchmark these competitive baselines in CSEGG. Our results reveal the limitations of these methods and highlight the challenges of our CSEGG learning protocols.

**Continual Learning Methods.** Existing continual learning works can be categorized into several approaches. (1) Regularization-based methods [24; 9; 79; 2; 4] aim to mitigate catastrophic forgetting by employing regularization techniques in the parameter space. (2) Dynamic architecture-based approaches[66; 76; 21; 47] adapt the model's architecture dynamically to accommodate new tasks without interfering with the existing ones. (3) Replay-based methods [57; 10; 55; 65; 52; 7] utilize a memory buffer to store and replay past data during training, enabling the model to revisit and learn from previously seen examples, thereby reducing forgetting. The special variants of these methods include generative replay methods, such as [61; 71; 75; 51], where synthetic data is generated and replayed. Although these generative replay methods, as well as other continual learning methods, have been extensively studied in image classification [8; 70; 43] and object detection[68; 60; 45], few works focus on the challenges in CSEGG, such as adaptive reasoning from the dynamic scenes, the evolving long-tailed distribution across scenes, and the combinatorial complexity involving objects and their multiple relationships. In this work, we introduce a continual learning method, abbreviated as RAS (Replays via Analysis by Synthesis). To address the distinct challenges of CSEGG, RAS

 
**S.** & **\#Tasks** & **\#Objs** & **\#Rels** & **Eval. metrics** & **SGG Backbone** & **CL base.** & **Kn.** & **Unk.** \\ 
**S1** & 5 & 150 (_All_) & 10 _per task_ & F, R, & Transformer based & Joint, Naive, &  Objs \\ (bbox, labels) \\  & 
 Rels \\  \\ 
**S2** & 2 & _Task 1_: 100 & _Task 1_: 40 &  Transfer \\ mf, mR, \\ FWT, BWT, \\ Gen \\  &  Transformer based \\ (SGTR) \\  &  Joint, Naive, \\ Replay M\%, \\ EWC, \\ PackNet, \\ RAS\_GT \\  & None & 
 Rels and Obs \\ (bbox, labels) \\  \\ 
**S3** & 4 & 30 _per task_ & 35 _per task_ &  **objs** \\ (IMP) \\  &  **objs** \\ **RAS\_GT** \\  &  Rels \\  & 
 Obs \\ (bbox, labels) \\  \\  

Table 1: **Overview of three CSEGG learning scenarios.** This table summarizes the three learning scenarios (Column 1) in CSEGG, including the number of tasks, the number of object (#Objs) and relationship (#Rels) classes, the evaluation metrics, the SGG-Backbones used, and the continual learning (CL) baselines. The Kn. and Unk. columns provide information regarding what is known to the CSEGG models during training in that scenario and what is being incrementally learned by the models. Unknown information is being incrementally learned by the models. See **Sec. 3** for details.

involves creating in-context synthetic scene images based on re-composable scene graphs from previous tasks to reinforce continual learning. The components in RAS facilitate memory-efficient training and preserve privacy while maintaining the scene diversity and scene context for SGG in dynamic environments. With the rise of pretrained vision-language models (VLMs) [49; 31; 83; 40], various SGG methods [11; 30] have been proposed to tackle open-vocabulary and zero-shot SGG challenges. However, these settings differ fundamentally from CSEGG, where we aim to simulate scenarios where the model encounters novel predicates or objects, unseen by any models, including LLMs or multi-modal models. Using pre-learned information from LLMs or frozen encoders conflicts with the continual learning setting we address in CSEGG.

## 3 Continual ScenE Graph Generation Benchmark

In CSEGG, to cater to the three continual learning scenarios below, we re-organize the Visual Genome  dataset and follow its standard image splits for training, validation, and test sets specified in . In each learning scenario, we consider a sequence of \(T\) tasks consisting of images and corresponding scene graphs with new objects, or new relationships, or both. Let \(D_{t}=\{(I_{i},G_{i})\}_{i=1}^{N_{t}}\) represent the dataset at task \(t\), where \(I_{i}\) denotes the \(i\)-th image and \(G_{i}\) represents the associated scene graph. The scene graph \(G_{i}\) comprises a set of object nodes \(O_{i}\) and their corresponding relationships \(R_{i}\). Each object node \(o_{j}\) is defined by its class label \(c_{j}\) and its bounding box locations and sizes \(b_{j}\). Each relationship \(r_{k}\) is represented by a triplet \((o_{s},p_{k},o_{o})\), where \(o_{s}\) and \(o_{o}\) denote the subject and object nodes, and \(p_{k}\) represents the relationship predicate.

### Learning Scenarios

**Scenario 1 (S1): Relationship Incremental Learning.** To uncover contextual information and go beyond studies of object detection and recognition, we introduce this scenario consisting of 5 tasks where 10 new relationship classes are incrementally added in every task (**Fig. 2, left**; **Fig. S1**; **Tab. 1**). All object classes and their locations are made known to all CSEGG models over all the tasks. This scenario resembles a human learning scenario where a parent gradually teaches a baby to recognize new relationships among all objects in the same room, focusing on one new relationship at a time during continual learning. This scenario also has implications in medical imaging where identical cell types may form new relationships with nearby cells depending on the context (**Sec. A.1.1**).

**Scenario 2 (S2): Scene Incremental Learning.** To simulate the real-world cases when there are demands for detecting new objects and new relationships from old to new scenes, we introduce this scenario where new objects and new relationships are incrementally introduced over tasks (**Fig. 2, middle**; **Fig. S1**; **Tab. 1**). There are 2 tasks in total with the first task containing 100 object classes and 40 relationship classes with 25 more object classes and 5 more relationship classes in the second task. This aligns with the real-world use cases where common objects and relationships are learned in the first scene, and incremental learning in the second scene only happens on less frequent relationships and objects. See **Sec. A.1.2** for details.

Figure 2: **Three learning scenarios are introduced. From left to right, they are S1. relationship (Rel.) incremental learning (Incre.); S2. scene incremental learning; and S3. relationship generalization (Rel. Gen.) in Object Incre.. In S1 and S2, example triplet labels in the training (solid line) and test sets (dotted line) from each task are presented. The training and test sets from the same task are color-coded. Blue color indicates task 1 and orange color indicates task 2. The new objects or relationships in each task are bold and underlined. In S3, one single test set (dotted gray box) is used for benchmarking the relationship generalization of object incre. learning models across all the tasks.**

**Scenario 3 (S3): Relationship Generalization.** Humans have no problem at all recognizing the relationships of unknown objects with other nearby objects. This scenario is designed to investigate the relationship generalization ability of CSEGG models. This capability is essential for real-world implications, such as in robotic navigation where it often encounters unknown objects and requires classifying their relationships. In total, there are four tasks, each introducing an incremental addition of 30 new object classes. All relationship classes are made known to all CSEGG models over all the tasks (**Fig. 2, right**; **Fig. S1**; **Tab. 1**). Different from scenarios **S1** and **S2**, a standalone generalization test set is curated, where the objects are unknown but the relationship classes among these unknown objects are common to the training set of every task. The CSEGG models trained after every task are tested on this standalone generalization test set to predict relationships among the unknown objects. See **Sec. A.1.3** for details.

**Data sampling and distributions.** To allocate data for every task of each scenario, we perform the following sampling strategies. In **S1** and **S3** above, either object or relationship classes are randomly sampled from the Visual Genome dataset and incrementally added to every task. Due to the inherent characteristics of real-world scenes, the long-tailed class distribution is present in **S1** and **S3**. However, in **S2**, only tail classes are sampled and added in subsequent tasks. The number of tasks in each scenario is experimentally determined to optimize the training data configuration, ensuring sufficient training samples in each task while maximizing the number of tasks. For detailed statistics, see **Fig. S2** and **Sec. A.2**.

### Competitive CSEGG Baselines

Due to the scarcity of CSEGG works, we contribute a diverse set of competitive CSEGG baselines and implement them on our own. Each CSEGG baseline requires three components: a backbone model for scene graph generation (SGG), a continual learning (CL) method to prevent the SGG model from forgetting, and an optional data sampling technique to deal with imbalanced data at every task for training SGG models. Next, we introduce the 2 SGG backbones, the 5 continual learning methods, and the 5 optional data sampling techniques. See **Sec. A.3** for implementation and training details of CSEGG baselines.

**SGG Backbones.** We use the two state-of-the-art backbones: (1) one-stage Scene graph Generation TRansformer (SGTR)  and (2) the traditional Two-stage SGG model (TCNN) . Briefly, SGTR (**Fig. S3 left**) uses a transformer-based architecture for image feature extraction and fusion. During training,  formulates SGG as a bipartite graph construction and matching problem. In contrast, TCNN detects objects with Faster-RCNN backbone and infers their relationships separately via Iterative message passing . We use implementations from  and  with default hyperparameters.

**Baselines.** We include the following continual learning methods (**Fig. S3 right**): (1) Naive (lower bound) is trained on each task in sequence without any measures to prevent catastrophic forgetting. (2) EWCB is a weight-regularization method, where the weights of the network are regularized in the parameter space, based on their "importance" to the previous tasks. (3) PackNet is a parameter-isolation method, iteratively pruning the network parameters after every task, so that it can sequentially pack multiple tasks within one network. (4) Replay@M includes a memory buffer with the capacity of storing \(M\) percentages of images in the entire dataset as well as their corresponding ground truth object and predicate notations depending on the task at each learning scenario. We vary \(M=\) 10%, 20%, and 100%. (5) Joint Training is an upper bound where the SGG model is trained on the entire CSEGG dataset. (6) RAS_GT is a baseline in which we use the ground truth scene graph labels from each task to create replay buffers using an image generation model explained in detail in **Sec. 4**. See **Fig. S4** for schematics of CSEGG baselines. We provide mathematical formulations of these baselines in **Sec. A.4**.

**Sampling Methods to Handle Long-Tailed Distribution.** We adopt the five data sampling techniques to alleviate the problem of imbalanced data distribution during training. (1) LVIS is an image-level over-sampling strategy for the tailed classes. (2) Bi-level sampling (BLS)  balances the trade-off between image-level oversampling for the tailed classes and instance-level under-sampling for the head classes. (3) Equalized Focal Loss (EFL)  is an effective loss function, re-balancing the loss contribution of head and tail classes according to their imbalanced distribution. EFL is enabled all the time for all the CSEGG baselines. In addition to applying data sampling techniques to the training sets, we can also apply LVIS and BLS techniques to the data stored in thereplay buffer. We name these data sampling techniques applied during both training and replays as (4) LVIS@Replay and (5) BLS@Replay.

### Evaluation Metrics

Same as existing SGG works [72; 32], we adopt the evaluation metric recall@**K** (**R@K**) on the top \(K\) predicted triplets in the scene graphs \(G\). As CSEGG is long-tailed, we further report the results in mean recall (**mR@K**) over the head, body, and tail classes. Forgetfullness (F), Average (Avg.) performance, Forward Transfer (FWT)  and Backward Transfer (BWT)  are standard evaluation metrics used for continual learning in image recognition and object detection tasks. In Scenario 1 and 2, we adapt these metrics to recalls R@K and introduce **F@K**, **Avg.R@K**, **FWT@K**, and **BWT@K** respectively for CSEGG settings. Similarly, we also adapt these metrics to mR@K. We explored CSEGG with K=20, 50, and 100. Since our results are consistent among Ks, we omit "@K" and analyze all the results based on K=20 in the entire text.

In scenario S3, we evaluate all CSEGG methods in the standalone generalization test set, shared over all the tasks. To benchmark generalization abilities in unknown object localization and relationship classification among these unknown objects, we introduce two evaluation metrics: **Gen R\({}_{bbox}\)@K** and **Gen R@K**. As the CSEGG models have never been taught to classify unknown objects, we discard the class labels of the bounding boxes and only evaluate the predicted bounding box locations with **Gen R\({}_{bbox}\)@K**. To evaluate whether the predicted bounding box location is correct, we apply a hard threshold of Intersection over Union (**IoU**) between the predicted bounding box locations and the ground truth. Any predicted bounding boxes with their IoU values above the hard threshold are deemed to be correct. We vary IoU thresholds from 0.3, 0.5, to 0.7.

To assess whether the CSEGG model generalizes to detect known relationships over unknown objects, we evaluate the recall **Gen R@K** of the predicted relationships \(r_{k}\) only on _correctly predicted_ bounding boxes. See **Sec. A.5** for details. All results are averaged over 3 runs.

## 4 Replays via Analysis by Synthesis (RAS)

To address the complexities in CSEGG, we introduce our "Replays via Analysis by Synthesis" method, dubbed RAS. Our RAS belongs to the group of generative replay methods for continual learning. We create an exemplar set \(E_{t}\) for replays. At task \(t\), we jointly train the scene graph generation model \(M_{t}\) on \(E_{t}\) and the current training dataset \(D_{t}\). However, different from the existing generative replay methods [61; 15; 16], our RAS leverages symbolic replays with state-of-the-art diffusion models. Moreover, rather than generating any random images for replays, our RAS is capable of generating in-context images following semantic rules, such as object co-occurrences. The schematic of our RAS is presented in **Fig. 3**. Next, we focus on how RAS creates \(E_{t}\) containing the generated images and their SGG annotations on these images for replays.

Figure 3: **Schematic of our proposed Replays via Analysis by Synthesis (RAS) method.** At task \(t+1\), our RAS stores all the triplet labels \(U_{t}\), such as <man, on, horse>, from the previous tasks. It then re-composes these triplet labels to create in-context prompts, utilizing them as inputs to generative image models to synthesize images for replays. For predicting scene graphs on these synthesized images, we employ the frozen model \(M_{t}\) from the preceding task \(t\), marked with “snowflakes”. Subsequently, these predicted scene graph notations, along with their corresponding synthesized images, contribute to “pseudo” replays, preventing the current model \(M_{t+1}\) from experiencing forgetting. See **Sec. 4** for more details.

**Image Generation.** At the current task \(t+1\), our RAS requires storing the frozen old model snapshot \(M_{t}\) at the end of the previous task \(t\) and all the triplet labels \(U_{t}\), which are parsed from all the scene graphs aggregated from all the previous tasks. These triplets contain object labels, subject labels, and relationships among them. For example, <man, on, horse> and <man, in front of, horse> are two unique triplet labels. Unlike the traditional replay methods in continual learning literature, our method refrains from storing original images \(I_{i}\) or scene graphs \(G_{i}\) in the training sets, thereby eliminating storage issues and privacy concerns.

To generate images \(I^{}_{j}\) for replays, our RAS feeds text prompts, which are formed by a set of chosen triplet labels and describe the diverse in-context scenes, into the state-of-the-art Stable Diffusion model . As previous works suggests context plays important roles in visual perceptions . To generate text prompts describing context-congruent scenes, we employ a context checker. First, the context checker uses the pre-trained large language model BERT  to extract embeddings for each triplet label in \(U_{t}\). As BERT has been pre-trained on a large corpus of text data, it learns to capture context-relevant representations of words. Next, hierarchical clustering is performed on these embeddings using the agglomerative clustering algorithm . This ensures that each cluster contains only embeddings that are semantically close. The threshold for the agglomerative clustering algorithm is set to 0.6. As real-world images often contain complex scenes involving multiple triplets, we select any cluster with more than 3 triplet labels to create a text prompt for image generation.

In practical applications, conducting agglomerative clustering on all triplet labels in \(U_{t}\) is computationally demanding, as it requires computing pairwise embedding similarities among all the triplet labels. To address this, RAS opts for a more efficient approach during replays by selecting a subset of triplet labels and clustering their embeddings. Recognizing that real-world scenes often exhibit a long-tailed distribution with certain objects or relationships being more prevalent, we introduce the Long-Tailed Distribution (LTD) module for balancing this distribution in \(U_{t}\). Unlike image-level sampling methods like BLS and LVIS [33; 19] discussed in **Sec. 3.2**, our LTD module in RAS operates at the triplet level. For each triplet label, its dropout rate is determined proportionally to its frequency in \(U_{t}\). Specifically, we define the drop-out rate \(d_{k}\) for the \(k\)-th triplet as: \(d_{k}=f_{k}/(_{i=1}^{i=N}f_{i})*\), where \(N\) is the total number of triplets in \(U_{t}\), \(=0.7\) is a scaling factor, and \(f_{i}\) is the frequency of the \(i\)-th triplet in \(U_{t}\). This sampling formula enables RAS to select triplets from tail classes more frequently compared to those from head classes.

To generate a text prompt from the chosen triplet labels, we employ a straightforward English language construct using the conjunction "and". This involves combining all the selected triplet labels into a sentence by starting with "Realistic Image of". For instance, if the triplet labels are <man, on, horse>, <house, behind, horse>, and <man, in front, house>, the generated prompt becomes "Realistic Image of man on horse and house behind horse and man in front of house." To increase exemplar diversity for replays, we use the Stable Diffusion model  to generate \(\) number of images for the same text prompt. In practice, we set \(=10\) over all the learning scenarios.

We provide the visualization examples of some synthesized images along with the corresponding text prompts in **Fig. S5**. From these examples, we found that the composed text prompts and the synthesized images are often of high quality and contextual coherence.

**Scene Graph Prediction on Synthesized Images.** During replays, to train the model \(M_{t+1}\) on \(I^{}_{j}\), we also need to predict their corresponding scene graph notations \(G^{}_{j}\) on \(I^{}_{j}\). As the frozen model snapshot \(M_{t}\) at the end of task \(t\) carries prior knowledge for SGG from the previous tasks, we use it to predict notations \(G^{}_{j}\) on \(I^{}_{j}\). These \(G^{}_{j}\) comprises object nodes \(O^{}_{j}\) with their respective classes \(c^{}_{j}\), along with object locations \(b^{}_{j}\). Additionally, it includes corresponding relationship nodes \(R^{}_{j}\) formed by triplets <\(o^{}_{s}\), \(p^{}_{k}\), \(o^{}_{j}\)> representing subject, predicate, and object nodes, respectively. These generated notations \(G^{}_{j}\), along with \(I^{}_{j}\), serve to construct the exemplars \(E_{t}\), used for replays.

## 5 Results

### RAS outperforms all the CSEGG baselines in Scenarios 1 and 2

The results for Avg. R, F, mR, mF, FWT, and BWT in learning scenarios 1 (S1) and 2 (S2) are presented in **Tab. 2**. Our observations align with established research in continual learning, especially in image classification and object detection: regardless of the SGG architectures, over both learning scenarios, Naive consistently performs the worst, showcasing significant catastrophic forgetting.

Replay-based methods, such as Replay@10% and Replay@20%, outperform techniques like EWC and PackNet. However, none of them surpass our RAS.

RAS achieves superior performance compared to Replay@20% (\(\)2 Gb) while requiring less storage (\(\)1.2 Gb), equivalent to storing 15% exemplary images. This storage efficiency is due to only needing to store the old model snapshot, triplet labels in \(U_{t}\), and the image generation model, thus avoiding privacy concerns. Also, RAS outperforms RAS_GT (**Tab. 2**), indicating that decomposing scene graphs into smaller, more diverse ones, along with more comprehensible prompts, is more effective than storing the ground truth scene graphs and directly using them for image generation.

In Learning Scenario 2 (S2), the task involves classifying both new objects and new relationships, significantly escalating the level of difficulty compared to S1. As evident from **Tab. 2**, all CSEGG models, including Replay@100%, exhibit a decline in overall performance when compared to the upper bound Joint. Although our RAS achieves the leading performance among all the baselines, its performance is still far from Joint. This underscores the persistent challenge posed by S2 in the context of CSEGG. Future work should explore new approaches to address this gap.

To tackle the issue of imbalanced data distribution in real-world scenarios, we incorporate two established data sampling techniques (LVIS@Replay@10% and BLS@Replay@10%) into our experiment (**Sec. 3.2**). The outcomes in learning scenario S1 are presented in **Tab. 3**. We observed that their performance falls short of our RAS, underscoring the efficacy of RAS in addressing long-tailed distribution during generative replays. We also noticed that BLS@Replay@10% significantly

    &  \\   &  &  \\   & Avg.R  & F† & mR  & mF† & FWT† & BWT† & Avg.R  & F† & mR  & mF† & FWT† & BWT† \\  Joint & 20.15 & 0 & 4.6 & 0 & - & 12.64 & 0 & 9.84 & 0 & - & - \\ Replay@100\% & 16.17 & -12.24 & 3.32 & -13.4 & -1.77 & -11.72 & 4.56 & -4.13 & 4.56 & -5.61 & -1.045 & -30.25 \\  Naive & 1.33 & -28.7 & 0.86 & -1.74 & -2.03 & -60.67 & 0.51 & -23.22 & 0.05 & -11.31 & -3.77 & -62.34 \\ EWC & 1.89 & -28.4 & 0.96 & -1.72 & -1.17 & -52.45 & 0 & -23.22 & 0 & -11.31 & -2.65 & -50.12 \\ RAS\_GT & 5.78 & -26.51 & 1.43 & -1.54 & -1.2 & -44.27 & 0.98 & -23.11 & 0.76 & -10.86 & -1.6 & -43.25 \\ PackNet & 7.19 & -25.67 & 1.35 & -1.64 & -1.03 & -42.35 & 1.67 & -22.77 & 0.9 & -10.33 & -1.4 & -42.45 \\ Replay@10\% & 8.55 & -22.21 & 4.33 & -1.44 & **4.29** & -38.35 & 1.81 & -20.72 & 1.15 & -9.64 & -0.9 & -40.67 \\ Replay@20\% & 9.25 & -20.35 & 4.78 & -1.42 & 3.21 & -31.98 & 2.57 & -17.17 & 1.56 & -8.07 & -0.67 & -38.27 \\
**Ours*** & **10.78** & **-18.92** & **5.6** & **-1.39** & 2.3 & **-25.56** & **3.45** & **-10.23** & **2.75** & **-6.57** & **-0.54** & **-35.67** \\   &  \\   &  &  \\   & Avg.R  & F† & mR  & mF† & FWT† & BWT† & Avg.R  & F† & mR  & mF† & FWT† & BWT† \\  Joint & 19.53 & 0 & 3.9 & 0 & - & - & 4.3 & 0 & 3.7 & 0 & - & - \\ Replay@100\% & 13.45 & -8.83 & 3.6 & -0.35 & -1.5 & -10.45 & 12.45 & -4.13 & 3.2 & -0.56 & -2.1 & -20.34 \\  Naive & 0.98 & -21.2 & 0.74 & -1.35 & -3.45 & -43.87 & 0 & -18.22 & 0.45 & -2.67 & -4.12 & -53.12 \\ EWC & 2.36 & -21.05 & 0.67 & -1.34 & -2.34 & -39.89 & 0 & -18.22 & 0.03 & 0 & -3.77 & -51.67 \\ PackNet & 3.2 & -19.7 & 1.1 & -1.13 & -1.3 & -32.45 & 1.1 & -17.82 & 0.84 & -1.97 & -2.84 & -40.34 \\ Replay@10\% & 5.67 & -18.9 & 3.21 & -1.05 & **1.45** & -28.34 & 1.81 & -16.72 & 1.03 & -1.74 & -1.4 &outperforms LVIS@Replay@10%, contrary to findings in the classical SGG problem where BLS is considered more effective than LVIS . The performance difference may stem from variations in the number of replay instances between the two approaches after applying these data re-sampling methods to exemplar images in the memory buffer (**Sec. 3.2**). This observation suggests that the original sampling methods designed for addressing long-tailed distributions in the classical SGG problem may not be as effective when applied to CSEGG.

We explored the impact of task sequence permutations on CSEGG performance, finding an effect consistent with existing literature  (**Fig. S6; Sec. A.6.1**). We also observed that fine-tuning DETR in S1 has minimal impact on forgetting, indicating that any forgetfulness in S1 is solely due to relationship incremental learning (**Fig. S7; Sec. A.6.2**). Moreover, to gain qualitative insights, we provide visualizations of predicted scene graphs for all CSEGG baselines in Scenario 1 (**Fig. S8** and **Sec. A.7.1**) and Scenario 2 (**Fig. S9** and **Sec. A.7.2**).

### CSEGG Models Can Generalize in Unknown Scenes

**Fig. 4** illustrates the generalization results for detecting unknown objects and classifying known relationships among these objects in Learning Scenario 3 (S3). In **Fig. 4 (a)**, an increasing trend in Gen R\({}_{bbox}\) is observed with the increasing task number for all CSEGG methods, indicating improved generalization in detecting unknown objects. Notably, even with minimal training in Task 1, all CSEGG methods propose 23% reasonable object regions with threshold IoU = 0.7, showcasing the SGTR model's ability to generalize to locate "objectness". As expected, with an increase in IoU threshold from 0.3 to 0.7, we found that Gen R\({}_{bbox}\) decreases due to fewer bounding boxes being considered correct. Moreover, we also compared the generalization performance in object detection between Replay@10% and Naive. Contrary to previous observations in S1 and S2 (**Tab. 2**), we found that Replay@10% show a decline in Gen R\({}_{bbox}\), possibly due to a fixed number of detected object bounding boxes output by CSEGG methods. Similarly, our RAS exhibits a reduced G R\({}_{bbox}\) compared to Replay@10% and Naive, likely for the same underlying reason.

**In Fig. 4 (b)**, Replay@10% outperforms Naive in Gen R when considering correctly detected unknown object locations, emphasizing that minimizing forgetting in continual learning enhances the SGTR model's overall relationship generalization in unknown scene understanding. However, the performance of Replay@10% is still inferior to our RAS method; implying that our RAS is more proficient in generalizing to classify relationships among unknown objects. Interestingly, we also noted that even with minimal training in Task 1, all the CSEGG methods achieve 45% recall of known relationships among unknown objects, demonstrating the SGTR model's ability to generalize to classify "relationship". Visualization examples, when CSEGG models can generalize to recognize relationships, are presented in **Fig. S10** and **Sec. A.7.3**.

### Ablation Studies on Our RAS Reveal Key Design Insights

We introduce our default method designs in **Sec. 4**. Here, we vary the components in our RAS to reveal key design insights. We propose a context checker in RAS. Here, we conduct an ablation by removing this module. Triplet labels are randomly selected and combined for text prompts. In A1 of **Tab. 4**, we observe a performance decrease of approximately 2% across all evaluation metrics, compared with our RAS. This suggests that generating images adhering to real-world context rules is crucial for replays. The lower performance may be attributed to the challenge of generating

    & \(\) & Context & LTD & Triplet & Avg.R \(\) & F \(\) & mR \(\) & mF \(\) \\  A1 & 10 & ✗ & ✓ & Multiple & 8.23 & -21.35 & 3.98 & -1.98 \\ A2 & 10 & ✓ & ✗ & Multiple & 9.75 & -20.65 & 4.12 & -1.65 \\ A3 & 10 & ✓ & ✓ & Single & 7.75 & -22.45 & 3.12 & -2.48 \\  A4 & 2 & ✓ & ✓ & Multiple & 2.45 & -27.43 & 0.45 & -9.84 \\ A5 & 4 & ✓ & ✓ & Multiple & 5.67 & -26.42 & 2.41 & -3.26 \\ A6 & 8 & ✓ & ✓ & Multiple & 7.89 & -22.42 & 3.89 & -2.17 \\ 
**Ours** & 10 & ✓ & ✓ & Multiple & **10.78** & **-18.92** & **5.6** & **-1.39** \\   

Table 4: **Ablation results of our RAS on learning scenario S1 reveals key design insights.** This table presents the results of ablation studies conducted to identify key components of our method, as discussed in **Sec. 5.3**. Results in Avg.R, F, mR, mF are reported after the last task in Scenario S1.

good-quality out-of-context images for Stable Diffusion Models and the potential domain differences affecting the SGG model \(M_{t}\) in predicting out-of-context SGG notations.

The LTD sampling module in our RAS is designed to balance the distribution of head and tail triplet labels from \(U_{t}\). Here, we remove the LTD sampling module and report the performance of the ablated method in A2 of **Tab. 4**. Compared to our RAS, we observe an absolute decrease of 1-2% across all metrics. Notably, the relative decrease is more pronounced in mR and mF than Avg.R and F. As mR and mF indicate mean Recall and mean Forgetfulness over both tail and head classes, the larger drops in these metrics suggest that the absence of LTD sampling significantly hinders the SGG model's ability to predict tail classes from previous tasks.

In our RAS, we employ multiple triplet labels to construct text prompts for image generation. In contrast to single triplet labels, our approach yields rich text descriptions of complex scenes, allowing the SGG model to capture intricate relationships among multiple objects in the same scene. Additionally, using multiple triplets is more efficient in rehearsing, as it enables the model to practice predicting multiple triplets simultaneously within the same number of synthesized images. Indeed, when we replace multiple triplet labels with single triplet labels for text prompts, we note a decrease of approximately 3% across all metrics (compare A3 with ours in **Tab. 4**).

Lastly, we investigate the impact of generating \(\) images using the same text prompt in RAS, varying \(\) from 2 to 8 (**Tab. 4**, A4-6). As expected, performance improves with higher \(\), showing that increased sample diversity enhances CSEGG performance. With ample computing resources, dynamically synthesizing more images could further improve performance. This highlights RAS's advantage in generating numerous images for replays without expanding storage usage.

## 6 Discussion

In the dynamic world, adapting scene graph generation (SGG) models to new objects and relationships poses challenges. Despite progress in SGG and continual learning, there is still a gap in understanding Continual Scene Graph Generation (CSEGG). We address this by operationalizing CSEGG, and introducing benchmarks, datasets, and evaluation protocols. Our study explores three learning scenarios, analyzing continual object detection and relationship classification in long-tailed class-incremental settings for CSEGG baselines. Our findings show that integrating sampling methods with CSEGG baselines to address long-tailed distributions moderately eliminates forgetting; however, a large performance gap between current CSEGG baselines and the joint training upper bound persists. To address CSEGG challenges, we propose RAS, a Replays via Analysis by Synthesis method. RAS parses previous task scene graphs into triplet labels for diverse in-context scene graph reconstruction. Based on these re-compositional context-congruent scene graphs, RAS synthesizes images with Stable Diffusion models for replays. Unlike other image replay methods, RAS stores only triplet labels and the model snapshot, maintaining constant memory usage and preserving privacy. Extensive experiments demonstrate our RAS's superior performance over current CSEGG baselines in knowledge transfers and reducing forgetting. Interestingly, our RAS model is also capable of generalizing to classify known relationships among unseen objects.

Moving forward, there are several key avenues for future research. First, our current endeavors focus on tackling CSEGG problems from static images in an Independent and Identically Distributed (i.d.d) manner, diverging from how humans learn from video streams. Future research can look into CSEGG problems on video SGG datasets. Second, our plans also involve expanding the set of continual learning baselines and integrating more long-tailed distribution sampling techniques. Third, we aim to construct a synthetic SGG dataset to systematically quantify the aspects of SGG that influence continual learning performance under controlled conditions. In RAS, SGG annotations for synthesized images in the replay buffer are predicted by the preceding SGG model, which can lead to error propagation across training iterations. In the future work, integrating a generative model with fine-grained control signals (such as bounding boxes and captions)  could provide more precise supervision, potentially mitigating these accumulated errors and further enhancing the performance of our approach. Although the CSEGG method holds promise for many downstream applications like monitoring systems, medical imaging, and autonomous navigation, we should also be aware of its misuse in privacy, data biases, fairness, security concerns, and misinterpretation (see **Sec. A.8** for an expanded discussion). We invite the research community to join us in expanding and updating the safe use of CSEGG benchmarks, thereby fostering its advancements in research and technology.