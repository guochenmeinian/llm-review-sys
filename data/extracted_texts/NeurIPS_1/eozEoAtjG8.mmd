# Understanding and Improving Feature Learning for Out-of-Distribution Generalization

Yongqiang Chen\({}^{1}\), Wei Huang\({}^{2}\), Kaiwen Zhou\({}^{1}\)

\({}^{1}\)The Chinese University of Hong Kong \({}^{2}\)RIKEN AIP

{yqchen,kwzhou,jcheng}@cse.cuhk.edu.hk   wei.huang.vr@riken.jp

\({}^{1}\)The Chinese University of Hong Kong \({}^{2}\)RIKEN AIP

{yqchen,kwzhou,jcheng}@cse.cuhk.edu.hk   wei.huang.vr@riken.jp

\({}^{2}\)Yatao Bian\({}^{3}\), Bo Han\({}^{4}\), James Cheng\({}^{1}\)

\({}^{3}\)Tencent AI Lab \({}^{4}\)Hong Kong Baptist University

yatao.bian@gmail.com   bhanml@comp.hkbu.edu.hk

Equal Contribution. Work done during Yongqiang's internship at Tencent AI Lab.Code is available at [https://github.com/LFhase/FeAT](https://github.com/LFhase/FeAT).

###### Abstract

A common explanation for the failure of out-of-distribution (OOD) generalization is that the model trained with empirical risk minimization (ERM) learns spurious features instead of invariant features. However, several recent studies challenged this explanation and found that deep networks may have already learned sufficiently good features for OOD generalization. Despite the contradictions at first glance, we theoretically show that ERM essentially learns _both_ spurious and invariant features, while ERM tends to learn spurious features faster if the spurious correlation is stronger. Moreover, when fed the ERM learned features to the OOD objectives, the invariant feature learning quality significantly affects the final OOD performance, as OOD objectives rarely learn new features. Therefore, ERM feature learning can be a _bottleneck_ to OOD generalization. To alleviate the reliance, we propose **F**eature Augmented Training (FeAT), to enforce the model to learn richer features ready for OOD generalization. FeAT iteratively augments the model to learn new features while retaining the already learned features. In each round, the retention and augmentation operations are performed on different subsets of the training data that capture distinct features. Extensive experiments show that FeAT effectively learns richer features thus boosting the performance of various OOD objectives1.

## 1 Introduction

Understanding feature learning in neural networks is crucial to understanding how they generalize to different data distributions . Deep networks trained with empirical risk minimization (ERM) learn highly predictive features that generalize surprisingly well to in-distribution (ID) data . However, ERM also tends to learn _spurious_ features or shortcuts such as image backgrounds  whose correlations with labels do not hold in the out-of-distribution (OOD) data, and suffers from serious performance degeneration . Therefore, it is widely believed that the reason for the OOD failures of deep networks is that ERM fails to learn the desired features that have _invariant_ correlations with labels across different distributions .

However, several recent works find that ERM-trained models have _already learned sufficiently good features_ that are able to generalize to OOD data . In addition, when optimizing various penalty terms  that aim to regularize ERM to capture the invariant features (termed as OOD objectives), there also exists a curious phenomenon that theperformance of OOD objectives largely relies on the pre-training with ERM before applying the OOD objectives [16; 83]. As shown in Fig. 1(b), the number of ERM pre-training epochs _has a large influence_ on the final OOD performance. These seemingly contradicting phenomena raise a challenging research question:

_What features are learned by ERM and OOD objectives, respectively, and how do the learned features generalize to in-distribution and out-of-distribution data?_

To answer the question, we conduct a theoretical investigation of feature learning in a two-layer CNN network, when trained with ERM and a widely used OOD objective, IRMv1 , respectively. We use a variation of the data models proposed in [2; 12], and include features with different correlation degrees to the labels to simulate invariant and spurious features .

First, we find that ERM essentially learns _both_ spurious features and invariant features (Theorem 4.1). The degrees of spurious and invariant feature learning are mostly controlled by their correlation strengths with labels. Moreover, merely training with IRMv1 _cannot learn new_ features (Theorem 4.2). Therefore, the _quality_ of ERM feature learning affects the final OOD performance significantly. Hence, as the number of ERM pre-training epochs increases, the model learns invariant features better and thus the final OOD performance will increase (Fig. 1). However, when ERM does not capture _all_ useful features for OOD generalization, i.e., there exist some useful features that are poorly learned by ERM, the model can hardly learn these features during OOD training and the OOD performance will be limited. Given a limited number of pre-training steps, it could often happen due to low invariant correlation strength, the feature learning biases of ERM , or the model architectures . Consequently, ERM feature learning can be a _bottleneck_ to OOD generalization .

To remedy the issue, we propose **F**eature **A**ugmented **T**raining (FeAT), an iterative strategy to enforce the model to learn richer features. As shown in Fig. 1(a), in each round, FeAT separates the train set into two subsets according to whether the underlying features in each set are already learned (Retention set \(^{r}\)) or not (Augmentation set \(^{a}\)), by examining whether the model yields correct (\(^{r}\)) or incorrect (\(^{a}\)) predictions for samples from the subsets, respectively. Intuitively, \(^{a}\) and \(^{r}\) will contain distinct features that are separated in different rounds. Then, FeAT performs distributionally

Figure 1: _(a) An illustration of FeAT (top row) compared to ERM (bottom row)._ Different colors in samples denote the respective dominant features. As the original data is dominated by spurious features (blue), ERM tends to learn more spurious features but limited invariant features (orange). Thus the OOD training with IRMv1 can only leverage limited invariant features and achieve limited performance. In contrast, iteratively, FeAT divides \(_{}\) into augmentation \(D^{a}\) and retention sets \(D^{r}\) that contain features not learned and already learned by the current model at the round, respectively. In each round, FeAT augments the model with new features contained in the growing augmentation sets while retaining the already learned features contained in the retention sets, which will lead the model to learn richer features for OOD training and obtain a better OOD performance. Then FeAT augments the model with new features while retaining the already learned features, which leads to richer features for OOD training and better OOD performance. _(b) OOD Performance vs. the number of ERM pre-training epochs in ColoredMNIST-025._ The performance of various OOD objectives largely relies on the quality of ERM-learned features. When there exist underlying useful features poorly learned by ERM, the OOD performance will be limited. In contrast, FeAT learns richer features with \(2\) rounds (or \(300\) epochs) and improves the OOD performance.

robust optimization (DRO) [50; 83] on all subsets, which _augments_ the model to learn new features by minimizing the maximal ERM losses on all \(^{a}\) and _retains_ the already learned features by minimizing ERM losses on all \(^{r}\). Along with the growth of the augmentation and retention sets, FeAT is able to learn richer features for OOD training and obtain a better OOD performance. FeAT terminates when the model cannot learn any new predictive features (Algorithm 1).

We conduct extensive experiments on both ColoredMNIST [4; 16] and \(6\) datasets from the challenging benchmark, Wilds, and show that FeAT effectively learns richer features and thus consistently improves the OOD performance when applied to various OOD objectives (Sec. 6).

## 2 Related Work

We discuss the most related work to ours and leave more details in Appendix C.

**On Feature Learning and Generalization.** Understanding feature learning in deep networks is crucial to understanding their generalization [2; 11; 12; 22; 32; 62; 70]. Beyond the empirical probing [21; 26; 28; 65], Allen-Zhu and Li  proposed a new theoretical framework for analyzing the feature learning process of deep networks, which has been widely adopted to study various deep learning phenomena [12; 32; 78; 86]. However, how the learned features from ID data can generalize to OOD data remains elusive. The only exceptions are  and . Kumar et al.  find fine-tuning can distort the pre-trained features while fine-tuning can be considered as a special case in our framework. Shen et al.  focus on how data augmentation helps promote good but hard-to-learn features and improve OOD generalization. Deng et al.  finds neural networks tend to learn spurious features under imbalanced groups. In contrast, we study the direct effects of ERM and OOD objectives to feature learning and provide a theoretical explanation for the curious phenomenon [33; 63]. To the best of our knowledge, we are the _first_ to analyze the feature learning of ERM and OOD objectives and their interactions in the general OOD generalization setting.

**Rich Feature Learning.** Recently many OOD objectives have been proposed to regularize ERM such that the model can focus on learning invariant features [4; 41; 55; 57; 76]. However, the final OOD performance has a large dependence on the number of ERM pre-training epochs [16; 83]. To remedy the issue, Zhang et al.  proposed Bonsai to construct rich feature representations as network initialization for OOD training. Although both Bonsai and FeAT perform DRO on grouped subsets, Bonsai rely on multiple initializations of the whole network to capture diverse features from the subsets, and complicated ensembling of the features, which requires more training epochs for convergence. In contrast, FeAT relieves the requirements via direct augmentation-retention on the grouped subsets, and thus obtains better performance. More crucially, although rich feature learning algorithms such as Bonsai and weight averaging [5; 59] have gained some successes, explanations about the reliance of OOD performance on ERM pre-training and why rich feature learning mitigates the issue remain elusive. In addition to a new rich feature learning algorithm, our work provides theoretical explanations for the success of rich feature learning in OOD generalization.

## 3 Preliminaries and Problem Definition

**Notations.** We use old-faced letters for vectors and matrices otherwise for scalar; \(\|\|_{2}\) to denote the Euclidean norm of a vector or the spectral norm of a matrix, while \(\|\|_{F}\) for the Frobenius norm of a matrix. \(_{d}\) refers to the identity matrix in \(^{d d}\). Full details are deferred to Appendix A.

Our data model \(=\{_{i},y_{i}\}_{i=1}^{n}\) is adapted from [2; 12] and further characterizes each data point \(_{i}\) as invariant and spurious feature patches from the two-bit model [16; 36].

**Definition 3.1**.: \(=\{_{e}\}_{e_{a}}\) is composed of multiple subsets \(_{e}\) from different environments \(e_{}\), where each \(_{e}=\{(_{i}^{e},y_{i}^{e})\}_{i=1}^{n_{e}}\) is composed of i.i.d. samples \((_{i}^{e},y_{i}^{e})^{e}\). Each data \((^{e},y^{e})_{e}\) with \(^{e}^{2d}\) and \(y^{e}\{-1,1\}\) is generated as follows:

1. Sample \(y^{e}\{-1,1\}\) uniformly;
2. Given \(y^{e}\), each input \(^{e}=[_{1}^{e},_{2}^{e}]\) contains a feature patch \(_{1}\) and a noise patch \(_{2}\), that are sampled as: \[_{1}=y()_{1}+y( )_{2}_{2}=\] where \(()\) is a random variable taking value \(-1\) with probability \(\) and \(+1\) with probability \(1-\), \(_{1}=[1,0, 0]^{}\) and \(_{2}=[0,1,0, 0]^{}\).

3. A noise vector \(\) is generated from the Gaussian distribution \((,_{p}^{2}(_{d}-_{1}_{1}^{}-_{2}_{2}^{}))\)

Definition 3.1 is inspired by the structure of image data in image classification with CNN , where the inputs consist of different patches, some of the patches consist of features that are related to the class label of the image, and the others are noises that are irrelevant to the label. In particular, \(_{1}\) and \(_{2}\) are feature vectors that simulate the invariant and spurious features, respectively. Although our data model focuses on two feature vectors, the discussion and results can be further generalized to multiple invariant and spurious features with fine-grained characteristics . Following previous works , we assume that the noise patch is generated from the Gaussian distribution such that the noise vector is orthogonal to the signal vector \(\). Each environment is denoted as \(_{}\!=\!\{(,_{e}):0<_{e}<1\}\), where \(_{1}\) is the invariant feature as \(\) is fixed while \(_{2}\) is the spurious feature as \(_{e}\) varies across \(e\).

**CNN model.** We consider training a two-layer convolutional neural network with a hidden layer width of \(m\). The filters are applied to \(_{1}\), \(_{2}\), respectively,2 and the second layer parameters of the network are fixed as \(\) and \(-\), respectively. Then the network can be written as \(f(,)\!=\!F_{+1}(_{+1},)-F_{-1}( _{-1},)\), where \(F_{+1}(_{+1},)\) and \(F_{-1}(_{-1},)\) are defined as follows:

\[F_{j}(_{j},)=_{r=1}^{m}[(_{j,r}^{}_{1})+(_{j,r}^{}_{2}) ], \]

where \((x)\) is the activation function. We assume that all network weights are initialized as \((0,_{0}^{2})\).

**ERM objective.** We train the CNN model by minimizing the empirical cross-entropy loss function:

\[L()=_{e_{}}}_{i=1}^{n_ {e}}(y_{i}^{e} f(,_{i}^{e})), \]

where \((z)\!=\!(1\!+\!(-z))\) and \(\{_{e}\}_{e_{}}\!=\!\{\{_{i}^{e}, y_{i}^{e}\}_{i=1}^{n_{e}}\}_{e_{}}\) is the trainset with \(_{e_{}}n_{e}\!=\!n\).

**OOD objective.** The goal of OOD generalization is, given the data from training environments \(\{_{e}\}_{e_{}}\), to find a predictor \(f:\) that generalizes well to all (unseen) environments, or minimizes \(_{e_{}}L_{e}(f)\), where \(L_{e}\) is the empirical risk under environment \(e\). The predictor \(f=w\) is usually composed of a featurizer \(:\) that learns to extract useful features, and a classifier \(w:\) that makes predictions from the extracted features.

Since we are interested in cases where the OOD objective succeeds in learning the invariant features. In the discussion below, without loss of generality, we study one of the most widely discussed OOD objective, IRMv1 objective, from IRM framework , and the data model where IRMv1 succeeds. Specifically, the IRM framework approaches OOD generalization by finding an invariant representation \(\), such that there exists a classifier acting on \(\) that is simultaneously optimal in \(_{}\). Hence, IRM leads to a challenging bi-level optimization problem as

\[_{w,}_{e_{}}L_{e}(w), \ w*{arg\,min}_{: }L_{e}(),\  e_{}. \]

Due to the optimization difficulty of Eq. (3), Arjovsky et al.  relax Eq. (3) into IRMv1 as follows:

\[_{}_{e_{}}L_{e}()+|_ {w|w=1}L_{e}(w)|^{2}. \]

Given the convolutional neural network (Eq. 1) and logistic loss (Eq. 2), IRMv1 can be written as

\[L_{}()=_{e_{}}}_{i=1}^{n_{e}}(y_{i}^{e} f(,_{i}^{e}) )+_{e_{}}^{2}}( _{i=1}^{n_{e}}_{i}^{ e} y_{i}^{e} f(, _{i}^{e}))^{2}, \]

where \(_{i}^{ e}=^{}(y_{i}^{e} f(,_{i}^{ e}))=-^{e} f(,_{i}^{e}))}{1+(-y_{i}^{e}  f(,_{i}^{e}))}\). Due to the complexity of IRMv1, in the analysis below, we introduce \(C_{}^{e}\) for the ease of expressions. Specifically, we define \(C_{}^{e}\) as

\[C_{}^{e}}_{i=1}^{n_{e}}^{} y_{i}^{e}_{i}^{e} y_{i}^{e}_{i}^{e},\]

where \(_{i}^{e} f(,_{i}^{e})\) is the logit of sample \(_{i}\) from environment \(e\). The convergence of \(C_{}\) indicates the convergence of IRMv1 penalty. The following lemma will be useful in our analysis.

**Lemma 3.2**.: _(Cao et al. ) Let \(_{j,r}(t)\)3 for \(j\{+1,-1\}\) and \(r\{1,2,,m\}\) be the convolution filters of the CNN at \(t\)-th iteration of gradient descent. Then there exists unique coefficients \(_{j,r}^{inv}(t),_{j,r}^{syn}(t) 0\) and \(_{j,r,i}(t)\) such that,_

\[_{j,r}(t)=_{j,r}(0)+j_{j,r}^{inv}(t) _{1}+j_{j,r}^{spu}(t)_{2}+_{i=1}^{n} _{j,r,i}(t)\|_{i}\|_{2}^{-2}_{i}. \]

We refer Eq. (6) as the _signal-noise decomposition_ of \(_{j,r}(t)\). We add normalization factor \(\|_{i}\|_{2}^{-2}\) in the definition so that \(_{j,r}^{(t)}_{j,r}^{(t)},_{i}\). Note that \(\|_{1}\|_{2}=\|_{2}\|_{2}=1\), the corresponding normalization factors are thus neglected. Furthermore,\(_{j,r}^{inv}_{j,r},_{1}\) and \(_{j,r}^{spu}_{j,r},_{2}\) respectively denote the degrees of invariant and spurious feature learning.

## 4 Theoretical Understanding of Feature Learning in OOD Generalization

### ERM Feature Learning

With the setup in Sec. 3, we first study the feature learning of the ERM objective. We consider a two training environments setup \(_{tr}=\{(,_{1}),(,_{2})\}\) where the signal of invariant feature is weaker than the average of spurious signals (i.e., \(>+_{2}}{2}\)), which corresponds to Figure 2. For a precise characterization of the training dynamic, we adopted a minimal setup where \((x)=x\) in Figure 2(a) and the following theorem, which already captures the key phenomenon in ERM feature learning. We study ERM feature learning with _non-linear_ activations in Appendix D.2.3.

**Theorem 4.1**.: _(Informal) For \(>0\), let \(_{e_{tr}}n_{e}\). Suppose that we run \(T\) iterations of GD for the ERM objective. With sufficiently large \(\) and \((x)=x\), assuming that (i) \(,_{1},_{2}<\), and (ii) \(>+_{2}}{2}\), with properly chosen \(_{0}^{2}\) and \(_{p}^{2}\), there exists a constant \(\), such that with probability at least \(1-2\), both invariant and spurious features are converging and the increment of the spurious feature is larger than that of the invariant feature at any iteration \(t\{0,,T-1\}\) (the detailed quantitative result of this gap can be found at (15) in Appendix D.2)._

As the formal statement of Theorem 4.1 is too complicated and lengthy, we leave it and its proof in Appendix D.2, while giving an informal but more intuitive version here. Theorem 4.1 states that ERM training learns both invariant feature and spurious feature at the same time, and if the average of spurious signals is stronger, the coefficient of spurious feature learning will dominate that of invariant feature learning in the whole training process, corresponding to Figure 2(b). We establish the proof based on inspecting a novel recursive equation, which might be of independent interest. Note that Theorem 4.1 can be directly generalized to handle any number of environments.

Speaking of implications, Theorem 4.1 provides answers to the seemingly contradicting phenomena that ERM fails in OOD generalization [7; 19] but still learns the invariant features [33; 63; 38]. In fact, ERM fails since it learns the spurious features more quickly, when spurious correlations are stronger than invariant correlations. Nevertheless, invariant feature learning also happens, even when the spurious correlations are strong, so long as the invariant feature has a non-trivial correlation strength with the labels. Therefore, simply re-training a classifier based on a subset of unbiased data on top of the ERM-trained featurizer achieves impressive OOD generalization performance [33; 38; 63].

Figure 2: The convergences of \(C_{}\) and feature learning coefficients (FL) with or without ERM pre-training (PT). The invariant and spurious feature learning terms are the mean of \(_{j,r},j_{1}\) and \(_{j,r},j_{2}\) for \(j\{ 1\},r[m]\), respectively. The training environments are \(_{tr}=\{(0.25,0.1),(0.25,0.2)\}\). The black dashed line indicates the end of pre-training. More details are given in Appendix D.1.

Theorem 4.1 also provides an explanation for the ID-OOD performance correlations when fine-tuning or training neural networks (especially large pre-trained models like CLIP , GPT ) [46; 71; 79; 80]. We provide a detailed discussion in Appendix C.

### IRM Feature Learning

Although Theorem 4.1 states that ERM learns both invariant and spurious features, the following questions remain unanswered: (1) whether IRMv1 learns new features or simply amplifies the already learned ERM features, and (2) how the quality of the ERM-learned features affects the feature learning when IRMv1 is incorporated. We first study IRMv1 training from scratch (w/o pre-training).

**Theorem 4.2**.: _Consider training a CNN model (1) with data model (3.1), define \((t)[C^{1}_{}(,t),C^{2}_{}(,t),,C^{|_{}|}_{}( ,t)],\) and \(_{0}=_{}(^{})\), where \(^{}_{,e^{}}n_{ }}_{i=1}^{n_{e}}^{}((_{j,r}(0),^{e}_{i,i})) ^{e}_{1,i}_{i^{}=1}^{n_{}}^{}(( _{j,r}(0),^{e}_{i,i^{}}))^{e^{}}_{i,i^{}}\). Suppose that dimension \(d=((m/))\), network width \(m=(1/)\), regularization factor \( 1/(_{0}|_{tr}|^{3/2})\), noise variance \(_{p}=O(d^{-2})\), weight initial scale \(_{0}=O(_{tr}|^{/2}^{3}L}{d^{1/2}m^{2} _{0}^{2}(1/)})\), then with probability at least \(1-\), after training time \(T=(})\), we have \(\|(T)\|_{2},\ ^{inv}_{j,r}(T)\!=\!o(1),\ ^{spu}_{j,r}(T)\!=\!o(1)\)._

The proof is given in Appendix D.3. We highlight that Theorem 4.2 allows any number of training environments, which indicates a fundamental limitation of pure IRMv1 training. Intuitively, Theorem 4.2 implies that, when a heavy regularization of IRMv1 is applied, the model will not learn any features, corresponding to Figure 2(d). Instead, IRMv1 suppresses any feature learning, even at the beginning of the training. Then, what would happen when given a properly pre-trained network?

After ERM pre-training, according to Theorem 4.1, we have \(|_{j,r},_{1}|=(1)\), \(|_{j,r},_{2}|=(1)\), and \(|_{j,r},|=O(_{0}_{p})\). Then, we have the following hold.

**Proposition 4.3**.: _Given the same setting as Theorem 4.2, suppose that \((x)=x\), \(^{inv}_{j,r}(t_{1})=^{inv}_{j,r}(t_{1}-1)\), and \(^{spu}_{j,r}(t_{1})=^{spu}_{j,r}(t_{1}-1)\) at the end of ERM pre-train \(t_{1}\), \(>0\), and \(n>C(1/)\), with \(C\) being a positive constant, then with a high probability at least \(1-\), we have \(_{e}C^{e}_{}(t_{1})=0\), \(^{inv}_{j,r}(t_{1}+1)>^{inv}_{j,r}(t_{1})\), and \(^{spu}_{j,r}(t_{1}+1)<^{spu}_{j,r}(t_{1})\)._

The proof is given in Appendix D.4, which takes converged feature learning terms from Theorem 4.1 as the inputs. Proposition 4.3 demonstrates that with sufficient ERM pre-training, IRMv1 can enhance the learning of invariant features while suppressing the learning of spurious features, which is verified in Figure 2(b) and 2(a). Thus, when given the initialization with better learned invariant features, i.e., longer ERM pre-training epochs, IRMv1 improves the invariant feature better. Proposition 4.3 explains why the final OOD performance highly depends on the ERM pre-training [16; 83].

### Limitations of ERM Feature Learning

Combining results from both Sec. 4.1 and Sec. 4.2, we know that the invariant features will be learned during ERM pre-training and discovered during OOD training. However, given poorly learned invariant features, can IRMv1 still improve it? In practice, there often exist some invariant features that are not properly learned by ERM. For example, in our data model Def. 3.1 when the invariant correlation is much weaker than the spurious correlation, given a limited number of training steps, the spurious feature learning can dominate the invariant feature learning. Besides, when considering other factors such as the simplicity bias of ERM  or the inductive biases of the network architecture , it is more likely that there exist invariant features that are not properly learned . Then we have:

**Corollary 4.4**.: _Consider training the CNN with the data generated from Def. 3.1, suppose that \((x)=x\), \(^{inv}_{j,r}(t_{1})=o(1)\), and \(^{spu}_{j,r}(t_{1})=(1)\) at the end of ERM pre-training \(t_{1}\). Suppose that \(>0\), and \(n>C(1/)\), with \(C\) being a positive constant, then with a high probability at least \(1-\), we have \(^{inv}_{j,r}(t_{1}+1)<^{inv}_{j,r}(t_{1})\)._

Corollary 4.4 shows that IRMv1 requires sufficiently well-learned features for OOD generalization. It is also consistent with the experimental results in Fig. 2(b), 2(c), and Fig. 1, where all the OOD objectives only achieve a performance comparable to random guesses.

Feature Augmented Training

### Rich Features for OOD Generalization

The results in Sec. 4 imply the necessity of learning all potentially useful features during the pre-training stage for OOD generalization. Otherwise, the OOD training is less likely to enhance the poorly learned features. It also explains the success of learning diverse and rich features by weight averaging [5; 59] and rich feature construction (or Bonsai) , and other approaches [58; 81].

Despite the empirical success, however, the learning of rich features in both Bonsai and weight averaging is unstable and expensive. On the one hand, they may discard previously learned useful features or fail to explore all the desired features as it is hard to evaluate the quality of the intermediate learned features. On the other hand, they also need multiple initializations and training of the whole networks with different random seeds to encourage the diversity of feature learning, which brings more instability and computational overhead, especially when applied to large and deep networks.

### The FeAT Algorithm

To overcome the limitations of previous rich feature learning algorithms, we propose **F**eature **A**ugmented **T**raining (FeAT), that directly augment the feature learning in an iterative manner.

```
1:Input: Training data \(_{}\); the maximum augmentation rounds \(K\); predictor \(f:=w\); length of inner training epochs \(t\); termination threshold \(p\);
2:Initialize groups \(G^{a}_{},G^{r}\{\}\);
3:for\(k[1,,K]\)do
4: Randomly initialize \(w_{k}\);
5:for\(j[1,,t]\)do
6: Obtain \(_{}\) with \(G\) via Eq. 7;
7: Update \(w_{k},\) with \(_{}\);
8:endfor
9:// Early Stop if\(f_{k}=w_{k}\) fails to find new features.
10:if Training accuracy of \(f_{k}\) is smaller than \(p\)then
11: Set \(K=k-1\) and terminate the loop;
12:endif
13:Split \(_{}\) into groups \(^{a}_{k},^{r}_{k}\) according to whether \(f_{k}\) classifies the examples in \(_{}\) correctly or not;
14: Update groups \(G^{a} G^{a}\{^{a}_{k}\},G^{r} G^{r}\{ ^{r}_{k}\}\);
15:endfor
16: Synthesize the final classifier \(w_{i=1}^{K}w_{i}\);
17:return\(f=w\);
```

**Algorithm 1** FeAT: **F**eature **A**ugmented **T**raining

Intuitively, the potentially useful features presented in the training data are features that have non-trivial correlations with labels, or using the respective feature to predict the labels is able to achieve a _non-trivial training performance_. Moreover, the invariance principle assumes that the training data comes from different environments , which implies that each set of features can only dominate the correlations with labels in a _subset_ of data. Therefore, it is possible to differentiate the distinct sets of useful features entangled in the training data into different subsets, where ERM can effectively learn the dominant features presented in the corresponding subset as shown in Theorem 4.1.

The intuition naturally motivates an iterative rich feature learning algorithm, i.e., FeAT, that identifies the subsets containing distinct features and explores to learn new features in multiple rounds. The details of FeAT are given in Algorithm 1, where we are given a randomly initialized or pre-trained model \(f=w\) that consists of a featurizer \(\) and a classifier \(w\). In round \(k\), FeAT first identifies the subset that contains the already learned features by collecting the samples where \(f\) yields the correct prediction, denoted as \(G^{r}_{k}\), and the subset of samples that contains the features that have not been learned, denoted as \(G^{a}_{k}\).

At the \(k\)-th round, given the grouped subsets \(G=\{G^{r},G^{a}\}\) with \(2k-1\) groups, where \(G^{a}=\{^{a}_{i}\}_{i=0}^{k-1}\) is the grouped sets for new feature augmentation, and \(G^{r}=\{^{r}_{i}\}_{i=1}^{k-1}\) is the grouped sets for already learned feature retention (notice that \(^{r}_{0}\) is the empty set), where \(^{a}_{i}\) and \(^{r}_{i}\) are the corresponding augmentation and retention set elicited at \(i\)-th round. FeAT performs distributionally robust optimization (DRO) [50; 83] on \(G^{a}\) to explore new features that have not been learned in previous rounds. Meanwhile, FeAT also needs to _retain_ the already learned features by minimizing the empirical risk at \(G^{r}\), for which we store and use the historical classifiers \(w_{i}\) with the current featurizer to evaluate the feature retention degree. Then, the FeAT objective at round \(k\) is

\[_{}=_{^{r}_{i} G^{a}}_{^{r}_{i }}(w_{k})+_{^{r}_{i} G^{r}}_{ ^{r}_{i}}(w_{i}), \]

[MISSING_PAGE_FAIL:8]

rounds following Zhang et al. , while for FeAT the automatic termination stopped at round \(2\) in ColoredMNIST-025 and round \(3\) in ColoredMNIST-01. For ERM, we pre-trained the model with the same number of overall epochs as FeAT in ColoredMNIST-01, while early stopping at the number of epochs of \(1\) round in ColoredMNIST-025 to prevent over-fitting. All methods adopted the same backbone and the same training protocol following previous works . More details are given in Appendix F.1.

The results are reported in Table 1. It can be found that ERM will learn insufficiently good features under both stronger spurious correlations and invariant correlations, confirming our discussion in Sec. 4.3. Besides, Bonsai learns richer features in ColoredMNIST-025 and boosts OOD performance, but Bonsai sometimes leads to suboptimal performances in ColoredMNIST-01, which could be caused by the unstable feature learning in Bonsai. In contrast, FeAT consistently improves the OOD performance of all OOD objectives for all the ColoredMNIST datasets, demonstrating the advances of direct feature learning control in FeAT than Bonsai and ERM.

**Experiments on real-world benchmarks.** We also compare FeAT with ERM and Bonsai in \(6\) real-world OOD generalization datasets curated by Koh et al.  that contain complicated features and distribution shifts. The learned features are evaluated with several representative state-of-the-art OOD objectives in Wilds, including GroupDro , IRMv1 , VREx  as well as IRMX . By default, we train ERM, Bonsai and FeAT the same number of steps, and kept the rounds of Bonsai and FeAT the same (though Bonsai still requires one more round for feature synthesis). The only exception is in RxRx1 where both Bonsai and FeAT required more steps than ERM to converge. We use the same evaluation protocol following the practice in the literature  to ensure a fair comparison. More details are given in Appendix F.2.

In addition to OOD objectives, we evaluate the learned features with Deep Feature Reweighting (DFR) . DFR uses an additional OOD validation set where the _spurious correlation does not hold_, to perform logistic regression based on the learned features. Intuitively, DFR can serve as a proper measure for the quality of learned invariant features . When the original dataset does not provide a proper OOD validation set, e.g., Camelyon17, we use an alternative implementation based on a random split of the training and test data to perform the invariant feature quality measure . Similarly, we also report DFR-s by regression with the environment labels (when available) to evaluate the spurious feature learning of different methods. More details are given in Appendix F.2.

The results are presented in Table 2. Similarly, when the tasks grow more challenging and neural architectures become more complicated, the ERM learned features can have a lower quality as discussed Sec. 4.3. For example, ERM can not sufficiently learn all useful features in FMoW, while ERM can learn more spurious correlations in CivilComments. Moreover, it can also be observed the instability of Bonsai in learning richer features that Bonsai even under-performs ERM in rich feature learning and OOD generalization in multiple datasets. In con

    &  &  &  &  &  &  &  \\   & & Avg. acc. (\%) & Worst acc. (\%) & Worst acc. (\%) & Macro F1 & 10-th per. acc. (\%) & Avg. acc. (\%) \\  ERM & DFR\({}^{}\) & 95.14 (\(\)1.96) & **77.34** (\(\)0.50) & 41.96 (\(\)1.90) & 23.15 (\(\)0.24) & 48.00 (\(\)0.00) & - \\ ERM & DFR\({}^{}\) & \(\) & \(\) & 82.24 (\(\)0.1) & 56.17 (\(\)0.02) & 52.44 (\(\)1.93) & - & - \\  Bonsai & DFR\({}^{}\) & 95.17 (\(\)0.18) & 71.07 (\(\)0.68) & 43.26 (\(\)0.25) & 21.36 (\(\)0.47) & 46.67 (\(\)0.00) & - & - \\ Bonsai & DFR\({}^{}\) & \(\) & 81.26 (\(\)0.86) & 58.58 (\(\)1.17) & 50.85 (\(\)0.18) & - & - & - \\ FeAT & DFR\({}^{}\) & \(\) (\(\)0.19) & **77.34** (\(\)0.39) & **43.54** (\(\)1.20) & **23.54** (\(\)0.32) & \(\) (\(\)0.00) & - \\ FeAT & DFR\({}^{}\) & \(\) (\(\)0.80) & 57.69 (\(\)0.78) & 52.31 (\(\)0.88) & - & - & - \\   ERM & ERM & 74.30 (\(\)3.50) & 55.53 (\(\)1.78) & 33.85 (\(\)1.42) & 28.22 (\(\)0.79) & 51.11 (\(\)0.60) & 30.21 (\(\)0.00) \\ ERM & GroupDRO & 76.09 (\(\)4.64) & 69.50 (\(\)4.93) & 33.03 (\(\)0.82) & 28.51 (\(\)0.58) & 52.00 (\(\)0.00) & 29.99 (\(\)0.11) \\ ERM & IRMv1 & 75.68 (\(\)1.74) & 68.44 (\(\)0.95) & 33.45 (\(\)1.07) & 28.76 (\(\)0.45) & 52.00 (\(\)0.00) & 30.10 (\(\)0.00) \\ ERM & V-REx & 71.60 (\(\)7.89) & 69.03 (\(\)1.40) & 33.06 (\(\)0.46) & 28.82 (\(\)0.47) & 52.44 (\(\)0.69) & 29.88 (\(\)0.45) \\ ERM & IRMX & 73.49 (\(\)4.93) & 69.81 (\(\)1.79) & 33.13 (\(\)0.86) & 28.82 (\(\)0.47) & 52.00 (\(\)0.00) & 30.10 (\(\)0.00) \\ Bonsai & ERM & 73.98 (\(\)5.30) & 63.34 (\(\)4.49) & 31.91 (\(\)0.31) & 28.27 (\(\)1.09) & 48.58 (\(\)0.36) & 24.22 (\(\)0.44) \\ Bonsai & GroupDRO & 72.82 (\(\)3.57) & 70.23 (\(\)1.33) & 33.12 (\(\)1.20) & 27.16 (\(\)1.18) & 42.67 (\(\)1.09) & 22.95 (\(\)0.48) \\ Bonsai & IRMv1 & 73.59 (\(\)4.68) & 68.39 (\(\)2.02) & 33.51 (\(\)1.22) & 27.60 (\(\)1.57) & 47.11 (\(\)0.66) & 23.15 (\(\)0.44) \\ Bonsai & v-REx & 76.39 (\(\)5.12) & 68.67 (\(\)1.29) & 33.17 (\(\)1.26) & 25.81 (\(\)0.42) & 48.00 (\(\)0.00) & 23.34 (\(\)0.42) \\ Bonsai & IRMX & 67.47 (\(\)0.11) & 69.56 (\(\)9.36) & 32.63 (\(\)0.75) & 27.22 (\(\)0.00) & 46.67 (\(\)0.00) & 23.34 (\(\)0.40) \\ FeAT & ERM & 77.80 (\(\)2.42) & 68.11 (\(\)2.27) & 33.13 (\(\)0.78) & 28.47 (\(\)0.67) & **52.89** (\(\)0.63) & **30.66** (\(\)0.22) \\ FeAT & GroupDRO & **80.41** (\(\)3.30) & **71.29** (\(\)0.46) & 35.55 (\(\)1.67) & 28.38 (\(\)1.32) & 52.58 (\(\)0.56) & 29.99 (\(\)0.11) \\ FeAT & IRMv1 & 77.97 (\(\)0.09) & 70.33 (\(\)1.41) & **34.04** (\(\)0.47) & **29.66trast, FeAT consistently achieves the best invariant feature learning performance across various challenging realistic datasets. Meanwhile, compared to ERM and Bonsai, FeAT also reduces over-fitting to the spurious feature learning led by spurious correlations. As a result, FeAT achieves consistent improvements when the learned features are applied to various OOD objectives.

**The termination check in FeAT.** As elaborated in Sec. 5.2, a key difference between FeAT and previous rich feature learning algorithms such as Bonsai is that FeAT is able to access the intermediate feature representations and thus can perform the automatic termination check and learn the desired features stably. To verify, we list the FeAT performances in various subsets of ColoredMNIST-025 at different rounds in Table 3. By inspecting the retention accuracy, after FeAT learns sufficiently good features at Round \(2\), it is not necessary to proceed with Round \(3\) as it will destroy the already learned features and lead to degenerated retention and OOD performance. More details and results are given in Appendix F.1.

**Computational analysis.** We also analyze the computational and memory overhead of different methods, for which the details are given in Appendix F.4. Compared to ERM and Bonsai, iFeAT achieves the best performance without introducing too much additional overhead.

**Feature learning analysis.** We visualize the feature learning of ERM and FeAT on ColoredMNIST-025. As shown in Fig. 3, ERM can learn both invariant and spurious features to predict the label, aligned with our theory. However, ERM focuses more on spurious features and even forgets certain features with longer training epochs, which could be due to multiple reasons such as the simplicity biases of ERM. Hence predictions based on ERM learned features fail to generalize to OOD examples. In contrast, FeAT effectively captures the meaningful features for all samples and generalizes to OOD examples well. More analysis including results on Wilds benchmark can be found in Appendix F.5.

## 7 Conclusions

In this paper, we conducted a theoretical investigation of the invariant and spurious feature learning of ERM and OOD objectives. We found that ERM learns both invariant and spurious features when OOD objectives rarely learn new features. Thus, the features learned in the ERM pre-training can greatly influence the final OOD performance. Having learned the limitations of ERM pre-training, we proposed FeAT to learn all potentially useful features. Our extensive experimental results verify that FeAT significantly boosts the OOD performance when used for OOD training.

Figure 3: GradCAM visualization on ColoredMNIST-025, where the shortcuts are now concentrated to a colored path at the up left. Three visualizations are drawn for each sample: the original figure, the gray-colored gradcam, and the gradcam. It can be found that ERM can not properly capture the desired features while FeAT can stably capture the desired features.

   ColoredMNIST-025 & Round-1 & Round-2 & Round-3 \\  Training Acc. & 85.08\(\) 0.14 & 71.87\(\) 0.96 & 84.93\(\) 1.26 \\ Retention Acc. & - & 88.11\(\) 4.28 & 43.82\(\) 0.59 \\ OOD Acc. & 11.08\(\) 0.30 & 70.64\(\) 0.62 & 10.07\(\) 0.26 \\   

Table 3: Performances at different FeAT rounds.