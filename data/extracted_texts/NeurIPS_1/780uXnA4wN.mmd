# An Efficient High-Dimensional Gradient Estimator

for Stochastic Differential Equations

 Shengbo Wang

MS&E

Stanford University

Stanford, CA 94305

shengbo.wang@stanford.edu

&Jose Blanchet

MS&E

Stanford University

Stanford, CA 94305

jose.blanchet@stanford.edu

&Peter Glynn

MS&E

Stanford University

Stanford, CA 94305

glynn@stanford.edu

###### Abstract

Overparameterized stochastic differential equation (SDE) models have achieved remarkable success in various complex environments, such as PDE-constrained optimization, stochastic control and reinforcement learning, financial engineering, and neural SDEs. These models often feature system evolution coefficients that are parameterized by a high-dimensional vector \(^{n}\), aiming to optimize expectations of the SDE, such as a value function, through stochastic gradient ascent. Consequently, designing efficient gradient estimators for which the computational complexity scales well with \(n\) is of significant interest. This paper introduces a novel unbiased stochastic gradient estimator-the generator gradient estimator-for which the computation time remains stable in \(n\). In addition to establishing the validity of our methodology for general SDEs with jumps, we also perform numerical experiments that test our estimator in linear-quadratic control problems parameterized by high-dimensional neural networks. The results show a significant improvement in efficiency compared to the widely used pathwise differentiation method: Our estimator achieves near-constant computation times, increasingly outperforms its counterpart as \(n\) increases, and does so without compromising estimation variance. These empirical findings highlight the potential of our proposed methodology for optimizing SDEs in contemporary applications.

## 1 Introduction

We consider a family of jump diffusions \(\{X_{}^{x}(t,s)^{d}:s[t,T]\}\) that are generated by stochastic differential equations (SDEs) and indexed by the initial condition \(x^{d}\) at time \(s\) and a parameter \(^{n}\). In modern applications, the parameter \(\), encoding characteristics of an engineering model, often represents the weights of a deep neural network. This paper focuses particularly on scenarios where the dimension \(n\) of \(\) is significantly greater than the dimension \(d\) of the space. This setting naturally arises in the implementation of large AI architectures in modern applications.

Concretely, for each \(1 i d\), the \(i\)'th entry of \(X^{x}_{}(t,)\), denoted by \(X^{x}_{,i}(t,)\), satisfies the Ito SDE:

\[ X^{x}_{,i}(t,s)&=x_{i}+_{t} ^{s}_{,i}(r,X^{x}_{}(t,r))d\\ &+_{t}^{s}_{k=1}^{d^{}}_{,i,k}(r,X^ {x}_{}(t,r-))dB_{k}(r)+_{t}^{s}dJ_{,i}(r) \]

Here, \(\{_{,i}:1 i d\}\) and \(\{_{,i,k}:1 i,k d\}\) are the drift and volatility, respectively, satisfying suitable regularity conditions (to be discussed). For simplicity in our introductory explanations, we will assume that the jump term \(J_{}\) is zero. However, incorporating this jump feature is valuable in many applied settings, and arises in various fields such as financial engineering , stochastic control , and neural SDE models . Accordingly, we will fully integrate and discuss the jump components in our main results in Section 3.

The primary objective of this paper is to develop an efficient gradient estimator, with respect to \(\), for a large class of path-dependent expectations derived from an SDE. Concretely, we consider

\[v_{}(t,x)=E[_{t}^{T}_{}(s,X^{x}_{}(t,s))ds+g_{ }(X^{x}_{}(t,T))]. \]

The value \(v_{}(t,x)\) represents the expected cumulative reward running \(X^{x}_{}\) from time \(t\) to \(T\). Here, \(_{}\) and \(g_{}\) represents the reward rate and the terminal reward, respectively. This formulation encompasses a wide range of science and engineering problems including PDE-constrained optimization , stochastic control and reinforcement learning , and neural SDE models .

The gradient \(_{}v_{}(t,x)=(_{_{t}}v_{}(t,x),, _{_{t}}v_{}(t,x))^{n}\) is of significant interest in the sensitivity analysis, learning, and optimization of these models. In particular, finding an efficient unbiased estimator for \(_{}v_{}(t,x)\) with low variance is essential if one is to apply stochastic gradient descent to find near optimal policies or model parameters within the parametric class \(\).

Under reasonable smoothness and integrability conditions, it is natural to consider the pathwise differentiation estimator obtained by applying infinitesimal perturbation analysis (IPA) to the sample path of \(X^{x}_{}\) w.r.t. the \(i\)th coordinate of \(\). For instance, if \(_{}()=()\) independent of \(\) and \(g=0\), then we have a representation

\[_{_{i}}v_{}(0,x)=E[_{0}^{T}_{j=1}^{d} _{x_{j}}_{}(t,X^{x}_{}(t))_{_{i}}X^{x}_{,j }(t)dt]. \]

where \(_{_{i}}X^{x}_{}(t)\) is the pathwise derivative of the process \(X^{x}_{}\) w.r.t. \(_{i}\). The processes \(\{X^{x}_{,j},_{_{i}}X^{x}_{,j}:i=1,,n;j=1, ,d\}\) satisfy a system of \(d+d n\) SDEs [11, Equation (3.31)], which must be jointly simulated. Therefore, to estimate the gradient, the pathwise differentiation method requires simulating this \(d+d n\) dimensional SDE. Note that the dimension is linear in \(n\), the dimension of the parameter space. Contemporary applications of SDEs in physics-informed and data-driven environments such as deep neural SDEs and deep RL where overparameterization excel, necessitate a model with exceptionally large \(n\) that is often many orders of magnitude larger than \(d\). Hence, simulating the SDE of dimension \(d+d n\) becomes extremely resource-intensive. Motivated by these applications, we ask the following question:

_Can we device an efficient, unbiased, and finite variance estimator for \(_{}v_{}(t,x)\) with a computation time insensitive to \(n\)?_

The answer is affirmative. Precisely, our main contribution is designing the unbiased _generator gradient_ estimator of \(_{}v_{}(t,x)\) that requires only simulating \(O(d^{2})\) SDEs when the volatility parameters \(_{}\) do not depend on \(\) and \(O(d^{3})\) SDEs in the general setting, as summarized in Table 1.

We remark that in addition to pathwise differentiation, likelihood ratio-based estimators are also popular for sensitivity analysis in SDEs; see e.g. Yang and Kushner . However, typically they are only applicable if \(_{}\) is independent of \(\) and under more restrictive jump structures. When applicable, likelihood ratio-based estimators could be appealing alternatives as they introduce a change of measure that represents the derivatives as a functional of the \(d\)-dimensional processes \(X^{x}_{}\). Nevertheless, these estimators typically have significantly higher variance.

Finally, we apply our estimator to linear-quadratic control problems and test its performance in optimizing neural-network-parameterized controls. As we increase the number of network parameters \(n\), the results in Figure 0(a) and Table 2 highlight a substantial improvement in computational efficiency, as compared to the pathwise differentiation method, while still maintaining competitive variance levels. Furthermore, Figure 0(a) confirms that the computation time of our estimator is robust to increases in \(n\), even in extremely high-dimensional scenarios with \(n\) approaching \(10^{8}\).

### Literature Review

**Gradient Estimation:** Gradient estimation, particularly likelihood ratios and IPA methods, is crucial in sensitivity analysis. Foundational works in the late 20th century by Glynn [5; 4] and further adaptations to the SDE setting [26; 3] highlight these developments. IPA has evolved to apply stochastic flow techniques to SDEs, both with and without reflecting boundaries [23; 12; 16; 24; 14].

**Applications of Gradient Estimators:** Gradient estimators are widely used in stochastic control and reinforcement learning (RL) models. Policy gradient methods in discrete-time RL, including REINFORCE and deep policy gradient approaches, are notable applications [25; 13; 21]. Continuous-time RL have been explored using policy gradients in settings with continuous diffusion dynamics . Jump diffusions are important models in financial engineering and stochastic control [17; 15; 9; 6]. Gradient estimators can also be used for optimizing these models. Neural SDE models are modern computational frameworks that model the dynamics of stochastic systems using a neural-network-parameterized SDE. Chen et al. , Tzen and Raginsky , Kidger  focus on the continuous case, while Jia and Benson  consider ODEs modulated by compound Poisson jumps. Efficient gradient estimators in high-dimensional settings are crucial for fitting these SDE models.

**Diffusion with Jumps and Stochastic Flow:** The main technical tools for this paper are SDEs with jumps and stochastic flows. Our references are Protter , Kunita , Oksendal and Sulem .

### Remarks on Paper Organizations

The paper is structured as follows: Section 2 outlines the core concepts of our estimator in a zero-jump setting, focusing on intuitive understanding over technical detail. In Section 3, we introduce the SDE model with jumps and provide a set of sufficient conditions that rigorously support the earlier insights. While more general and complex assumptions exist that lead to similar conclusions, these are presented in Appendix A to align with the concise format of the conference proceedings. The paper concludes with Section 4, where we conduct numerical experiments on neural-network-parameterized linear-quadratic control problems, demonstrating the effectiveness of our methodology.

## 2 Key Methodological Insights

In this section, we motivate our proposed generator gradient estimator by first providing a non-rigorous derivation. We assume the SDE model (1.1) where the jumps \(J_{} 0\) and \(^{n}\) is a bounded open neighbourhood of the origin. W.l.o.g, we are interested in estimating the gradient at \(=0\) and \(t=0\); i.e. \(_{}v_{0}(0,x)=_{}v_{}(0,x)|_{=0}\).

To simplify notation, we denote \(X_{}^{x}(t):=X_{}^{x}(0,t)\) and \(X_{}^{x}(t-):=X_{}^{x}(0,t-)\), and the function

\[a_{,i,j}(t,x):=_{k=1}^{d^{}}_{ ,i,k}(t,x)_{,j,k}(t,x). \]

    &  \\  & Yes & No \\  Pathwise Differentiation & \(d+d n\) & \(d+d n\) \\ Generator Gradient & \(d+d^{2}+d^{3}\) & \(d+d^{2}\) \\   

Table 1: Comparison of the dimensions of SDEs needed to be simulated.

Also, for function \(v_{}(t,x)\), we use \(_{i}v_{}(t,x)\) to denote the space derivative \(v}{ x}_{,t,x}\) and \(\) the space gradient. Similarly, \(_{_{i}}\) and \(_{}\) denotes the \(\) partials.

Under sufficient regularity conditions, by the Feynman-Kac formula, \(v_{}\) in (1.2) is the solution to the partial differential equation (PDE)

\[_{t}v_{}+_{}v_{}+_{}=0, v_ {}(T,)=g_{} \]

for all \(\), where \(_{}\) is the _generator_ of \(X^{x}_{}\) given by

\[_{}f(t,x):=_{i=1}^{d}_{,i}(t,x)_{i}f(t, x)+_{i,j=1}^{d}a_{,i,j}(t,x)_{i}_{j}f(t,x)\]

for \(f\) that is twice differentiable in \(x\). Assuming enough smoothness, we formally differentiate the PDE (2.2) w.r.t. \(_{i}\) and then set \(=0\) to obtain

\[_{t}_{_{i}}v_{0}+_{0}_{_{i}}v_ {0}+(_{_{i}}_{0}v_{0}+_{_{i}}_{0}) =0,_{_{i}}v_{0}(T,)=_{_{i}}g_{0}. \]

Here, the operator \(_{_{i}}_{0}\) is defined as

\[_{_{i}}_{0}f(t,x):=_{j=1}^{d}_{_{i }}_{0,j}(t,x)_{j}f(t,x)+_{j,l=1}^{d}_{_{i}}a_{0,j,l}(t,x)_{j}_{l}f(t,x). \]

Interpreted as the derivative of \(_{}\) w.r.t. \(\) at \(0\), this inspires the name "generator gradient" method.

Next, define \(u_{0}=_{_{i}}v_{0}\). Treating \(_{_{i}}_{0}v_{0}\) as fixed, we observe that \(u_{0}\) solves the PDE (2.3) which is of the form (2.2). Hence, applying the Feynman-Kac formula again to \(_{_{i}}v_{0}(0,x)=u_{0}(0,x)\) yields the following expectation representation

\[_{_{i}}v_{0}(0,x)=E[_{0}^{T}_{_{i}} _{0}v_{0}(t,X^{x}_{0}(t))+_{_{i}}_{0}(t,X^{x}_{0} (t))dt+_{_{i}}g_{0}(X^{x}_{0}(T))]. \]

Note that the expression inside the expectation contains only space derivatives (due to \(_{_{i}}_{0}\)) of the value function \(v_{0}\) but not the \(\) derivatives. In particular, if we can estimate the gradient \( v_{0}(t,x)\) and the Hessian matrix \(H[v_{0}](t,x):=\{_{i}_{j}v_{0}(t,x):1 i,j d\}\) efficiently, then the representation in (2.5) will lead to a natural estimator of \(_{_{i}}v_{0}(0,x)\).

To estimate \( v_{0}(t,x)\) and \(H[v_{0}](t,x)\), we employ the pathwise differentiation estimator from (1.3). Specifically, under enough regularity conditions, we can interchange the derivatives and integration

\[ v_{0}(t,x)^{} =EZ(t,x)^{}:=E[_{t}^{T}_{0}^{} X ^{x}_{0}(t,r)dr+ g_{0}^{} X^{x}_{0}(t,T)], \] \[H[v_{0}](t,x) =EH(t,x):=E[ X^{x}_{0}(t,T)^{}H[g_{0}] X^{x }_{0}(t,T)+ g_{0},H[X^{x}_{0,}](t,T)]\] \[+E[_{t}^{T} X^{x}_{0}(t,r)^{}H[_{0}]  X^{x}_{0}(t,r)+_{0},H[X^{x}_{0,}](t,r)  dr].\]

Here, we write \( X^{x}_{0}:=_{a}X^{x}_{0,}:i,a=1, d}\) and \(H[X^{x}_{0}]:=_{b}_{a}X^{x}_{0,}:i,a,b=1, d }\). The notation \( h,H[X^{x}_{0,}]:=_{a=1}^{d}_ {a}hH[X^{x}_{0,a}]^{d d}\) for \(h=_{0},g_{0}\). The dependence of \(_{0},g_{0}\) on time and the state process is hidden.

We estimate these expectations by simulating the SDEs for \(\{X^{x}_{0}, X^{x}_{0},H[X^{x}_{0}]\}\) given by (1.1) and

\[_{a}X^{x}_{0,i} =_{i,a}+_{t}^{s}_{l=1}^{d}_{l}_{0,i} _{a}X^{x}_{0,l}dr+_{t}^{s}_{l=1}^{d}_{k=1}^{d^{}} _{l}_{0,i,k}_{a}X^{x}_{0,l}dB_{k}(r)\] \[_{b}_{a}X^{x}_{0,i} =_{t}^{s}_{l=1}^{d}[_{l}_{0,i}_ {b}_{a}X^{x}_{0,l}+_{m=1}^{d}_{m}_{l}_{0,i} _{a}X^{x}_{0,l}_{b}X^{x}_{0,m}]dr \] \[+_{t}^{s}_{k=1}^{d^{}}_{l=1}^{d}[ _{l}_{0,i,k}_{b}_{a}X^{x}_{0,l}+_{m=1}^{d} _{m}_{l}_{0,i,k}_{a}X^{x}_{0,l}_{b}X^{x}_{0,m}]dB_{k}(r)\]where the dependence of the coefficients on \(r\), \(X^{x}_{0}(t,r-)\), and \(z\), as well as the dependence of \(X^{x}_{0},_{a}X^{x}_{0},_{a}_{b}X^{x}_{0}\) on \((t,s),(t,r-)\) are suppressed.

The dimension of these SDEs is \(d+d^{2}+d^{3}\), where the \(\) comes from the Hessian being symmetric. Moreover, when the volatility \(\) is independent of \(\), our method only necessitates estimating \( v_{0}\). This reduction leads to simulating the SDEs for \(\{X^{x}_{0}, X^{x}_{0}\}\) of dimension only \(d+d^{2}\).

Assuming sufficient integrability, the unbiasedness of \(Z\) implies

\[E[_{0}^{T}_{_{k}}_{0}^{}Z(t,X^{x}_{0}(0,t))dt ]=E[_{0}^{T}_{_{k}}_{0}^{} v_{0}(t, X^{x}_{0}(0,t))dt] \]

which we will elaborate upon in (A.1). The same holds for the \(H(t,x)\) process as well. Therefore, we can replace the derivatives \( v_{0}\) with \(Z\) and \(H[v_{0}]\) with \(H\) in (2.5) without changing the expectation.

Also note that producing a sample of \(Z(t,x)\) requires simulating the solution to SDEs (1.1) and (2.7) within time \([t,T]\) starting from \(x,I,0\). So, it is not very efficient to compute \(Z(t,X^{x}_{0}(0,t))\) for every \(t\); a similar issue exists for \(H\) as well. This can be addressed by randomizing the integral.

With these considerations, we proceed to define the generator gradient estimator. First, let \(_{}L_{0}V_{0}(t,x)\) be defined by replacing \(_{i}v(t,x)\) with \(Z_{i}(t,x)\) and \(_{j}_{i}v\) with \(H_{i,j}(t,x)\) in the definition (2.4) of \(_{_{i}}_{0}v_{0}(t,x)\). Then, define the generator gradient estimator as

\[D(x):=T_{}L_{0}V_{0}(,X^{x}_{0}(0,))+_{0}^{T}_{ }_{0}(t,X^{x}_{0}(t))dt+_{}g_{0}(X^{x}_{0}(T)). \]

where \([0,T]\) is sampled independently. We can also randomize the integral of \(_{}_{0}(t,X^{x}_{}(t))\) if the gradient is hard to compute. With the derivation in (2.8), it is easy to see that \(ED(x)=_{}v_{0}(0,x)\) is unbiased.

In summary, due to the observation in (2.5), we are able to "move" the estimation of \(_{}v_{0}\) onto that of \( v_{0}\) and \(H[v_{0}]\). This results in a significant reduction in the dimension of the SDEs we need to simulate, underlying the remarkable efficiency of our methodology, especially when the dimension \(n\) of \(\) significantly exceeds \(d\).

## 3 Jump Diffusions and the Generator Gradient Estimator

In this section, we rigorously formulate a jump diffusion process driven by an SDE. We extend the generator gradient estimator to this context by first rigorously establishing an expectation representation of the derivative as in (2.5). Then, we also validate the representation (2.6) using the jump version of (2.7). These lead to our generator gradient estimator in the jump diffusion context. To improve the clarity of the paper (at a cost of generalizability), we will state a set of sufficient assumptions that are easy to verify. However, we will state and prove our theorems using a set of more general assumptions in the Appendix A.

We consider jump diffusions on the canonical probability space of cadlag functions \([0,T]^{d}\) generated by SDEs of the form (1.1) where the jump term is given by

\[ X^{x}_{,i}(t,s)&=x_{i}+_{t }^{s}_{,i}(r,X^{x}_{}(t,r))dr+_{t}^{s}_{k=1}^{d^{ }}_{,i,k}(r,X^{x}_{}(t,r-))dB_{k}(r)\\ &+_{t}^{s}_{^{d^{}}_{0}}_{, i}(t,X^{x}_{}(s,r-),z)d(dr,dz). \]

In this expression, \(B\) is a standard Brownian motion in \(^{d^{}}\); \(\) is a compensated Poisson random measure with intensity measure \(dt(dz)\) with \(\) a Levy measure on \((^{d^{}}_{0}:=^{d^{}}\{0\}\,,(^{d^{}}_{0}))\), i.e. \(_{^{d^{}}_{0}}1|z|^{2}(dz)<\); the \(-r\) notation in \(X^{x}_{}(t,r-)\) denotes the left limit; and the stochastic integrations are Ito integrals. Here, for a vector/matrix/tensor \(v^{d_{1} d_{2} d_{3}}\), we denote \(|v|^{2}:=_{i,j,k}|v_{i,j,k}|^{2}\). We further define \((z)=|z| 1\) and \((dz)=(z)^{2}(dz)\). Then \(\) is a finite measure on \((^{d^{}}_{0}\,,(^{d^{}}_{0}))\). Also, since we are interested in the gradient at \(=0\), we can assume w.l.o.g. that \(\) is a bounded open neighbourhood of \(0\).

The generator of this system of SDEs is \(_{}:=_{}^{C}+_{}^{J}\), where

\[_{}^{C}f(t,x) =_{i=1}^{d}_{,i}(t,x)_{i}f(t,x)+_{i,j=1}^{ d}a_{,i,j}(t,x)_{i}_{j}f(t,x) \] \[_{}^{J}f(t,x) =_{_{0}^{d^{}}}[f(t,x+_{}(t,x, z))-f(t,x)-_{i=1}^{d}_{,i}(t,x,z)_{i}f(t,x)](dz).\]

for \(f C^{1,2}([0,T],^{d})\). We remark that for open subsets \(,\), the space \(C^{i,j,k}([0,T],,)\) represents the set of functions \(f\) on \([0,T]\) that has continuous mixed partial derivatives \(_{t}^{a}_{w}^{b}_{c}^{c}f\) on \((0,T)\) for every \(a i,b j,c k\). Moreover, these mixed partial derivatives have continuous extensions on \([0,T]\).

### Probabilistic Representation of the Gradient

In this section, we rigorously establish the probabilistic representation of the gradient \(_{}v_{0}(0,x)\) as outlined in equation (2.5). Our approach leverages the continuous dependence of \( X_{}^{x}\) of the solutions to (3.1) in a neighbourhood of \(0\), given sufficient regularity conditions. This behavior extends the properties associated with stochastic flows, as explained in the work by Kunita .

Recall that \(\) is a bounded neighbourhood of \(0^{n}\). To clarify the assumptions, we enlarge \(\) and consider \(_{}=\{+v:,v B^{n}(0,)\}\) where \(B^{n}(0,)\) is the open ball in \(^{n}\) at \(0\) of radius \(\).

**Assumption 1**.: _For some \(>0\), the following regularity conditions hold_

1. _The mappings_ \((s,,x)_{}(s,x),_{}(s,x),_{}(s,x),g_{ }(s,x)\) _are_ \(C^{0,1,1}([0,T],_{},^{d})\)_. For each_ \(z_{0}^{d^{}}\)_,_ \((s,,x)_{}(s,x,z)/(z)\) _is_ \(C^{0,1,1}([0,T],_{},^{d})\)_. Moreover,_ \(|_{}(s,0,z)/(z)|\) _is uniformly bounded in_ \(s[0,T]\) _and_ \(z_{0}^{d^{}}\)_._
2. _The spacial derivatives_ \(|_{}|\)_,_ \(|_{}|\)_, and_ \(|_{}|\) _are uniformly bounded. The_ \(\) _derivatives satisfy linear growth_ \[|_{}_{}(s,x)|+|_{}_{}(s,x)|+| _{}(s,x,z)}{(z)}|(|x|+1)\] _for all_ \(s[0,T]\)_,_ \(x^{d}\)_,_ \(z^{d^{}}\)_, and_ \(\)_._
3. _The_ \(\) _derivatives of the rewards satisfy polynomial growth: for some_ \(m 1\)_,_ \[|_{}_{}(s,x)|+|_{}g_{}(x)|(|x| +1)^{m}\] _for all_ \(s[0,T]\)_,_ \(x^{d}\)_, and_ \(\)_._

_Remark_.: Requirement 1 implies that for each fixed \(x\), the \(\) derivatives of the coefficients are uniformly bounded in \([0,T]\), as \(\) is assumed to be bounded. So, the seemingly strong requirements of the \(\) derivative satisfying the growth condition in items 2 and 3 are not very restrictive. The boundedness of \(_{}(s,x,z)/(z)\) in \(z\) is relaxed in Assumption 5 in the appendix, allowing unbounded jumps. The strong condition is the uniform boundedness of \(|_{}|\), \(|_{}|\), and \(|_{}|\). However, this is typically necessary for the existence and uniqueness of strong solutions to the SDE (3.1).

**Assumption 2**.: _Assume that \(v_{} C^{1,2}([0,T],^{d}):}\) are classical solutions to the partial-integro-differential equations (PIDE)_

\[_{t}v_{}+_{}v_{}+_{}=0, v _{}(T,)=g_{}\]

_where \(_{}=_{}^{C}+_{}^{J}\) are defined in (3.2). Moreover, \(v_{}\) and its space derivatives satisfy polynomial growth: for each \(\), there exists \(0<c_{}<\) and \(m 1\) s.t._

\[_{x^{d},t[0,T]}(t,x)|}{(|x|+1)^{m}} c _{},_{x^{d},t[0,T]}(t, x)|}{(|x|+1)^{m}} c_{},_{x^{d},t[0,T]} ](t,x)|}{(|x|+1)^{m}} c_{}.\]

_Remark_.: By classical solution, we mean that \(v_{}\) satisfies \(_{t}v_{}+_{}v_{}+_{}=0\) on \((0,T)^{d}\) with its continuous extensions of satisfying \(v_{}(T,)=g_{}\). This is possible, for example, in settings with \(C^{2}\) terminal rewards. Note that is a stronger requirement compared to the definition in Evans .

As we have motivated in Section 2, Assumption 2 follows from a generalized version of the Feynman-Kac formula, under additional technical assumptions. Moreover, the growth of \(v_{}\) and its space derivatives can be derived from assumptions on the growth of the rewards. However, in order to not obscure the main message of the paper and to streamline the proof, we directly assume these properties. We refer interested readers to Kunita (Kunita, Chapter 4) where stochastic flow techniques similar to the proofs in the paper are employed to establish the PIDE and validate the growth rates.

**Theorem 1** (Probabilistic Representation of the Gradient).: _If Assumptions 1 and 2 are in force, then \( v_{}(0,x)\) is differentiable at \(0\). Moreover, the gradient_

\[_{}v_{0}(0,x)=E[_{0}^{T}_{}_{0}v_{ 0}(s,X_{0}^{x}(s))+_{}_{}(X_{0}^{x}(s))ds+_{ }g_{}(X_{0}^{x}(T))],\]

_where \(_{}_{0}:=_{}_{0}^{C}+_{ }_{0}^{J}\) s.t. for \(f(t,x) C^{1,2}\),_

\[_{}_{}^{C}f(t,x) =_{i=1}^{d}_{}_{,i}(t,x)_{i}f(t, x)+_{i,j=1}^{d}_{}a_{,i,j}(t,x)_{i}_{j }f(t,x), \] \[_{}_{}^{J}f(t,x) =_{_{0}^{d^{}}}[_{i=1}^{d}_{ }_{,i}(t,x,z)(_{i}f(t,x+_{}(t,x,z))- _{i}f(t,x))](dz). \]

In Theorem 1, we have successfully established an expectation representation of the gradient \(_{}v_{0}(0,x)\) of the form (2.5). This naturally leads to the consideration of using Monte Carlo to estimate \(_{}v_{0}(0,x)\). However, one observes that the representation in Theorem 1 involves the space derivatives \(_{i}v_{0}(t,x)\) and \(_{i}_{j}v_{0}(t,x)\), which are usually hard to compute exactly.

In the next section, following the heuristics in (2.6) we establish conditions on the model primitives so that the space derivatives \(_{i}v_{0}(t,x)\) and \(_{i}_{j}v_{0}(t,x)\) admit probabilistic representations as expectations of random processes \(\{X_{0}^{}, X_{0}^{},H[X_{0}^{x}]\}\) that can be easily simulated.

### Probabilistic Representation of the Space Derivatives

We proceed with introducing assumptions that guarantee Theorem 2, providing representations of \(_{i}v_{0}(t,x)\) and \(_{i}_{j}v_{0}(t,x)\) as illustrated in (2.6). To achieve this, we first need to ensure that the derivative of the mapping \(x X_{0}^{x}\) is well defined. This is formally established in Proposition A.1.

**Assumption 3**.: _For each \(z_{0}^{d^{}}\), the SDE coefficients \((s,x)(_{0}(s,x),_{0}(s,x),_{0}(s,x,z))\) are \(C^{0,2}([0,T],^{d})\). For each \(i,j=1,,d\), the coefficients and derivatives, seen as functions \((s,x)((s,x),(s,x),(s,x,))\) where \((,,)=(_{0},_{0},_{0}/)\), \((_{i}_{0},_{i}_{0},_{i}_{0}/)\), and \((_{j}_{i}_{0},_{j}_{i}_{0},_{j }_{i}_{0}/)\) are uniformly Lipschitz; i.e. there exists \(0<\) s.t. for all \(s[0,T],z^{d^{}}\)_

\[|(s,x)-(s,x^{})|+|(s,x)-(s,x^{})|+|(s,x,z)-(s,x^{},z)||x-x^{}|.\]

_Moreover, \(|(s,0,z)|\) is uniformly bounded for \(s[0,T]\) and \(z_{0}^{d^{}}\)._

In view of this assumption, we consider the following SDEs, as jump versions of (2.7), for which the strong solutions should be the space derivatives of \(X_{0}^{x}\). Again, the dependence of the coefficients on \(r\), \(X_{0}^{x}(t,r-)\), and \(z\), as well as the dependence of \(X_{0}^{x},_{a}X_{0}^{x},_{a}_{b}X_{0}^{x}\) on \((t,s),(t,r-)\) has been suppressed.

\[_{a}X^{x}_{0,i} =_{i,a}+_{t}^{s}_{l=1}^{d}_{l}_{0,i} _{a}X^{x}_{0,l}dr+_{t}^{s}_{l=1}^{d}_{k=1}^{d^{}} _{l}_{0,i,k}_{a}X^{x}_{0,l}dB_{k}(r)\] \[+_{t}^{s}_{l=1}^{d}_{l}_{0,i}_{a} X^{x}_{0,l}d(dr,dz)\] \[_{b}_{a}X^{x}_{0,i} =_{t}^{s}_{l=1}^{d}[_{l}_{0,i}_{ b}_{a}X^{x}_{0,l}+_{m=1}^{d}_{m}_{l}_{0,i} _{a}X^{x}_{0,l}_{b}X^{x}_{0,m}] \] \[+_{t}^{s}_{k=1}^{d^{}}_{l=1}^{d}[ _{l}_{0,i,k}_{b}_{a}X^{x}_{0,l}+_{m=1}^{d} _{m}_{l}_{0,i,k}_{a}X^{x}_{0,l}_{b}X^{x} _{0,m}]dB_{k}(r)\] \[+_{t}^{s}_{l=1}^{d}[_{l}_{0,i} _{b}_{a}X^{x}_{0,l}+_{m=1}^{d}_{m}_{l} _{0,i}_{a}X^{x}_{0,l}_{b}X^{x}_{0,m}]d(dr,dz).\]

As we will show in Proposition A.1, under Assumption 3 the process \(X^{x}_{0}(t,s)\) has a version that is twice continuously differentiable in \(x\) for every \(0 t<s T\). The processes \(\{ X^{x}_{0},H[X^{x}_{0}]\}\), as defined in (3.5), will then correspond to the derivatives. Moreover, these processes, as well as \(X^{x}_{0}\), will possess desirable integrability properties.

To guarantee sufficient integrability and to provide a variance bound for our estimator, we also need to assume growth conditions on the rewards.

**Assumption 4**.: _Assume that the mapping \(x_{0}(t,x),g_{0}(x)\) is \(C^{2}\) for all \(t[0,T]\). Moreover, for \(h(t,x)=_{0}(t,x)\) and \(g_{0}(x)\) there exists \(c_{h}\) s.t._

\[_{x^{d},t[0,T]}} c_{h}, _{x^{d},t[0,T]}} c_{ h},_{x^{d},t[0,T]}} c_{ h}.\]

With these assumptions, we validate the representations in (2.6) using the following theorem.

**Theorem 2** (Probabilistic Representation of the Space Derivatives).: _Under Assumptions 3 and 4, the representations in (2.6) hold with the jump version of \(\{X^{x}_{0}, X^{x}_{0},H[X^{x}_{0}]\}\) in (3.1) and (3.5)._

### The Generator Gradient Estimator

With Theorems 1 and 2, we construct our generator gradient estimator and show that it is unbiased with a variance that grows polynomially in \(x\). Recall the estimators \(Z(t,x)\) and \(H(t,x)\) in (2.6).

By Theorem 2 and the integrability in Proposition A.1 under Assumption 3, the equality (2.8) holds. Then, following the notation in (2.9), we define

\[_{}L_{0}V_{0}(t,x):=_{}L_{0}^{C}V_{0}(t,x)+_{ }L_{0}^{J}V_{0}(t,x)\]

where \(_{}L_{0}^{C}V_{0}(t,x)\) and \(_{}L_{0}^{J}V_{0}(t,x)\) are defined by replacing \(_{i}v(t,x)\) with \(Z_{i}(t,x)\) and \(_{j}_{i}v\) with \(H_{i,j}(t,x)\) in (3.3) and (3.4), respectively. Then, our estimator \(D(x)\) is given by (2.9).

**Theorem 3**.: _Suppose Assumptions 1-4 are in force. Then, the generator gradient estimator \(D(x)\) is unbiased; i.e. \(ED(x)=_{}v_{0}(0,x)\). Moreover, the variance \((D(x)) C(|x|+1)^{2m+4}\) has at most polynomial growth in \(x\), where the constant \(C\) can be dependent on other parameters of the problem but not \(x\)._

_Remark_.: The \(m\) signifies the growth rate of the rewards and their derivatives. The extra additive factor \(2\) in the variance is from the growth of the \(\) derivative of \(a_{0}\), the volatility squared.

## 4 Example: Linear System with Quadratic Loss

In this section, we illustrate some analytical properties and the effectiveness of our estimator by considering a linear quadratic control problem.

Let \(X^{d}\) be the controlled process, given by the solution to the SDE

\[X^{x}(t)=x+_{0}^{t}AX^{x}(s)+BU(t)ds+_{0}^{t}CdB(s),\]

where \(B(t)^{d^{}}\) is a standard Brownian motion, \(U(t)^{m}\) is the control process that is adapted to the filtration generated by \(X\), \(A^{d d},B^{d m},C^{d d ^{}}\) are non-random matrices. The objective is to choose an admissible control \(U(t)\) that minimizes the quadratic loss

\[E[_{0}^{T}X^{x}(t)^{}QX^{x}(t)+U(t)^{}RU(t)dt+X^{x}(T)^{} Q_{T}X^{x}(T)]\]

where \(Q,Q_{T}^{d d}\) and \(R^{m m}\) are non-random matrices.

In various applications of interests, the admissible control \(U(t)\) is a parameterized function of time and state \(U(t)=u_{}(t,X^{x}_{}(t))\) where the state process under control \(u_{}\) is denoted by \(X^{x}_{}\). The dimension of \(\) could potentially be very high--e.g. when \(u_{}\) is a neural network. To achieve an optimized loss in this over-parameterized setting, one common approach is to run gradient descent. Hence, an efficient gradient estimator that scales well with the dimension \(n\) of \(\) is highly desirable.

We compare the performance of the proposed generator gradient estimator and the pathwise differentiation estimator. In this context, these estimators take the following form. The detailed derivations are presented in Appendix F.1.

**The Generator Gradient Estimator:** In this setting, our generator gradient estimator in (2.9) is

\[D_{i}(x)=T_{_{i}}u_{}(,X^{x}_{}())^{}B^{ }Z(,X^{x}_{}())+Tu_{}(,X^{x}_{}())^{} (R+R^{})_{_{i}}u_{}(,X^{x}_{}())\]

where the definition of \(Z\) follows from (2.6), and is given by (F.1) in Appendix F.1. As explained in (2.9), we also randomize the integral corresponding to the gradient of the reward rate \(_{}_{0}\).

**The Pathwise Differentiation Estimator:** From (1.3), we find the following IPA estimator that randomizes the time integral

\[_{i}(x) =Tu_{}(,X^{x}_{}())(R+R^{}) u_{ }(,X^{x}_{}())_{_{i}}X^{x}_{}()+ TX^{x}_{}()^{}(Q+Q^{})_{_{i}}X^{x}_{}()\] \[+Tu_{}(,X^{x}_{}())^{}(R+R^{}) _{_{i}}u_{}(,X^{x}_{}())+X^{x}_{}(T) ^{}(Q_{T}+Q^{}_{T})_{_{i}}X^{x}_{}(T).\]

Here, the pathwise derivatives \(_{_{i}}X^{x}_{}(t)\) is the solution to (F.3).

We deploy these estimators in an environment where the state variable \(x^{4}\) represents the x-y positions and velocities of a point mass on a 2D plane. The controller applies a force to this mass. The cost function is designed to encourage the controller to swiftly move the point mass to the origin with minimal force. The force is state-time-dependent and parameterized through a 4-layer fully connected neural network with variable width. All computation times are recorded from a Tesla V100 GPU. Further details about the setup of our numerical experiments can be found in Appendix F.2.

Figure 1: Comparisons of 100-sample estimation statistics and averaged runtime.

In Figure 0(a), we present a comparison of the average runtime for computing a single sample of the generator gradient and the pathwise differentiation estimators \(D(x),(x)^{n}\), across increasing values of \(n\) the dimension of \(\). Our findings indicate that the generator gradient estimator not only outperforms the widely used pathwise differentiation method across all tested values of \(n\) but also surpasses it by more than an order of magnitude for larger values of \(n\). Additionally, the computation time for our estimator shows remarkable stability with respect to increases in \(n\), displaying only a slight uptrend when \(n 10^{7}\).

Figure 0(b) confirms that, at \(n=102\), the estimated values by the two estimators are very similar with high confidence. This confirms that our estimator is consistently estimating the gradient \(_{}v_{}(0,x)\).

Finally, Table 2 presents the standard errors (SE) (F.4) from 400 replications of both estimators, averaged over the gradient coordinates. It also displayed the averaged ratios of the standard errors (F.5). We observe averaged SE ratios that are consistently less than 1 for all \(n\), suggesting that our generator gradient estimator not only provides significantly faster computations as shown in Figure 0(a) but also achieves lower estimation variances. Further analysis of the SEs for each gradient coordinate is conducted and displayed in Figure 2 in Appendix F.2, highlighting similar histogram shapes and observable reduction in large values of SEs of our estimator.

## 5 Concluding Remarks

The theoretical results in this paper have the limitation of requiring second-order continuous differentiability and uniform boundedness of the space derivatives of the parameters of the underlying jump diffusion. These strong conditions, which are standard in the literature of stochastic flows (cf. ) to guarantee global existence and uniqueness of the derivative processes in (3.5), are necessary to achieve the generality of the results presented in this paper.

However, our generator gradient estimator often works even when coefficients are not continuously differentiable. This is true if the generator and rewards gradients are defined almost everywhere, and the derivative processes in (3.5), with almost everywhere derivatives of the SDE parameters, exist for every \(t[0,T]\) and satisfy some integrability conditions. Examples include neural networks parameterized stochastic control with ReLU activation functions, heavy-traffic limits of controlled multi-server queues, and the Cox-Ingersoll-Ross (CIR) model. For these models, the existence and integrability of the derivative processes can be checked on a case-by-case basis, allowing the consistency and unbiasedness of the generator gradient estimator to be established. We confirm this by numerically investigating the CIR process and an SDE with ReLU drift in Appendix G.