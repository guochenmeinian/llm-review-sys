# Greedy Poisson Rejection Sampling

Gergely Flamich

Department of Engineering

University of Cambridge

gf332@cam.ac.uk

###### Abstract

One-shot channel simulation is a fundamental data compression problem concerned with encoding a single sample from a target distribution \(Q\) using a coding distribution \(P\) using as few bits as possible on average. Algorithms that solve this problem find applications in neural data compression and differential privacy and can serve as a more efficient alternative to quantization-based methods. Sadly, existing solutions are too slow or have limited applicability, preventing widespread adoption. In this paper, we conclusively solve one-shot channel simulation for one-dimensional problems where the target-proposal density ratio is unimodal by describing an algorithm with optimal runtime. We achieve this by constructing a rejection sampling procedure equivalent to greedily searching over the points of a Poisson process. Hence, we call our algorithm greedy Poisson rejection sampling (GPRS) and analyze the correctness and time complexity of several of its variants. Finally, we empirically verify our theorems, demonstrating that GPRS significantly outperforms the current state-of-the-art method, A* coding. Our code is available at [https://github.com/gergely-flamich/greedy-poisson-rejection-sampling](https://github.com/gergely-flamich/greedy-poisson-rejection-sampling).

## 1 Introduction

It is a common misconception that quantization is essential to lossy data compression; in fact, it is merely a way to discard information deterministically. In this paper, we consider the alternative, that is, to discard information stochastically using _one-shot channel simulation_. To illustrate the main idea, take lossy image compression as an example. Assume we have a generative model given by a joint distribution \(P_{,}\) over images \(\) and latent variables \(\), e.g. we might have trained a variational autoencoder (VAE; Kingma and Welling, 2014) on a dataset of images. To compress a new image \(\), we encode a single sample from its posterior \( P_{|}\) as its stochastic lossy representation. The decoder can obtain a lossy reconstruction of \(\) by decoding \(\) and drawing a sample \(} P_{|}\) (though in practice, for a VAE we normally just take the mean predicted by the generative network).

Abstracting away from our example, in this paper we will be entirely focused on _channel simulation_ for a pair of correlated random variables \(, P_{,}\): given a source symbol \( P_{}\) we wish to encode \(\)**single sample**\( P_{|}\). A simple way to achieve this is to encode \(\) with entropy coding using the marginal \(P_{}\), whose average coding cost is approximately the entropy \([]\). Surprisingly, however, we can do much better by using a _channel simulation protocol_, whose average coding cost is approximately the mutual information \(I[;]\) (Li and El Gamal, 2018). This is remarkable, since not only \(I[;][]\), but in many cases \(I[;]\) might be finite even though \([]\) is infinite, such as when \(\) is continuous. Sadly, most existing protocols place heavy restrictions on \(P_{,}\) or their runtime scales much worse than \((I[;])\), limiting their practical applicability (Agustsson and Theis, 2020).

In this paper, we propose a family of channel simulation protocols based on a new rejection sampling algorithm, which we can apply to simulate samples from a target distribution \(Q\) using a proposal distribution \(P\) over an arbitrary probability space. The inspiration for our construction comes from an exciting recent line of work which recasts random variate simulation as a search problem over a setof randomly placed points, specifically a Poisson process (Maddison, 2016). The most well-known examples of sampling-as-search are the Gumbel-max trick and A* sampling (Maddison et al., 2014). Our algorithm, which we call _greedy Poisson rejection sampling_ (GPRS), differs significantly from all previous approaches in terms of what it is searching for, which we can succinctly summarise as: "GPRS searches for the first arrival of a Poisson process \(\) under the graph of an appropriately defined function \(\)". The first and simplest variant of GPRS is equivalent to an exhaustive search over all points of \(\) in time order. Next, we show that the exhaustive search is embarrassingly parallelizable, leading to a parallelized variant of GPRS. Finally, when the underlying probability space has more structure, we develop branch-and-bound variants of GPRS that perform a binary search over the points of \(\). See Figure 1 for an illustration of these three variants.

While GPRS is an interesting sampling algorithm on its own, we also show that each of its variants induces a new one-shot channel simulation protocol. That is, after we receive \( P_{}\), we can set \(Q P_{|}\) and \(P P_{}\) and use GPRS to encode a sample \( P_{|}\) at an average bitrate of a little more than the mutual information \(I[;]\). In particular, on one-dimensional problems where the density ratio \(dP_{|}/dP_{}\) is unimodal for all \(\) (which is often the case in practice), branch-and-bound GPRS leads to a protocol with an average runtime of \((I[;])\), which is optimal. This is a considerable improvement over A* coding (Flamich et al., 2022), the current state-of-the-art method.

In summary, our contributions are as follows:

* We construct a new rejection sampling algorithm called _greedy Poisson rejection sampling_, which we can construe as a greedy search over the points of a Poisson process (Algorithm 3). We propose a parallelized (Algorithm 4) and a branch-and-bound variant (Algorithms 5 and 6) of GPRS. We analyze the correctness and runtime of these algorithms.
* We show that each variant of GPRS induces a one-shot channel simulation protocol for correlated random variables \(, P_{,}\), achieving the optimal average codelength of \(I[;]+_{2}(I[;]+1)+(1)\) bits.
* We prove that when \(\) is a \(\)-valued random variable and the density ratio \(dP_{|}/dP_{}\) is always unimodal, the channel simulation protocol based on the binary search variant of GPRS achieves \((I[;])\) runtime, which is optimal.
* We conduct toy experiments on one-dimensional problems and show that GPRS compares favourably against \(\)* coding, the current state-of-the-art channel simulation protocol.

Figure 1: Illustration of three GPRS procedures for a Gaussian target \(Q=(1,0.25^{2})\) and Gaussian proposal distribution \(P=(0,1)\), with the time axis truncated to the first \(17\) units. All three variants find the first arrival of the same \((1,P)\)-Poisson process \(\) under the graph of \(= r\) indicated by the **thick dashed black line** in each plot. Here, \(r=dQ/dP\) is the target-proposal density ratio, and \(\) is given by Equation (5). **Left:** Algorithm 3 sequentially searching through the points of \(\). The green circle () shows the first point of \(\) that falls under \(\), and is accepted. All other points are rejected, as indicated by red crosses (). In practice, Algorithm 3 does not simulate points of \(\) that arrive after the accepted arrival. **Middle:** Parallelized GPRS (Algorithm 4) searching through two independent \((1/2,P)\)-Poisson processes \(_{1}\) and \(_{2}\) in parallel. Blue points are arrivals in \(_{1}\) and orange points are arrivals in \(_{2}\). Crosses () indicate rejected, and circles () indicate accepted points by each thread. In the end, the algorithm accepts the earliest arrival across all processes, which in this case is marked by the blue circle (). **Right:** Branch-and-bound GPRS (Algorithm 5), when \(\) is unimodal. The shaded red areas are never searched or simulated by the algorithm since, given the first two rejections, we know points in those regions cannot fall under \(\).

Background

The sampling algorithms we construct in this paper are search procedures on randomly placed points in space whose distribution is given by a Poisson process \(\). Thus, in this section, we first review the necessary theoretical background for Poisson processes and how we can simulate them on a computer. Then, we formulate standard rejection sampling as a search procedure over the points of a Poisson process to serve as a prototype for our algorithm in the next section. Up to this point, our exposition loosely follows Sections 2, 3 and 5.1 of the excellent work of Maddison (2016), peppered with a few additional results that will be useful for analyzing our algorithm later. Finally, we describe the channel simulation problem, using rejection sampling as a rudimentary solution and describe its shortcomings. This motivates the development of greedy Poisson rejection sampling in Section 3.

### Poisson Processes

A Poisson process \(\) is a countable collection of random points in some mathematical space \(\). In the main text, we will always assume that \(\) is defined over \(=^{+}^{d}\) and that all objects involved are measure-theoretically well-behaved for simplicity. For this choice of \(\), the positive reals represent _time_, and \(^{d}\) represents _space_. However, most results generalize to settings when the spatial domain \(^{d}\) is replaced with some more general space, and we give a general measure-theoretic construction in Appendix A, where the spatial domain is an arbitrary Polish space.

**Basic properties of \(\):** For a set \(A\), let \((A)}{{=}}| A|\) denote the number of points of \(\) falling in the set \(A\), where \(||\) denotes the cardinality of a set. Then, \(\) is characterized by the following two fundamental properties (Kingman, 1992). First, for two disjoint sets \(A,B,A B=\), the number of points of \(\) that fall in either set are independent random variables: \((A)(B)\).

Second, \((A)\) is Poisson distributed with _mean measure_\((A)}{{=}}[(A)]\).

**Time-ordering the points of \(\):** Since we assume that \(=^{+}^{d}\) has a product space structure, we may write the points of \(\) as a pair of time-space coordinates: \(=\{(T_{n},X_{n})\}_{n=1}^{}\). Furthermore, we can order the points in \(\) with respect to their time coordinates and _index_ them accordingly, i.e. for \(i<j\) we have \(T_{i}<T_{j}\). Hence, we refer to \((T_{n},X_{n})\) as the _\(n\)th arrival_ of the process.

As a slight abuse of notation, we define \((t)}{{=}}([0,t)^{d})\) and \((t)}{{=}}[(t)]\), i.e. these quantities measure the number and average number of points of \(\) that arrive before time \(t\), respectively. In this paper, we assume \((t)\) has derivative \(^{}(t)\), and assume for each \(t 0\) there is a conditional probability distribution \(P_{X|T=t}\) with density \(p(x t)\), such that we can write the mean measure as \((A)=_{A}p(x t)^{}(t)\,dx\,dt\).

**Simulating \(\):** A simple method to simulate \(\) on a computer is to realize it in time order, i.e. at step \(n\), simulate \(T_{n}\) and then use it to simulate \(X_{n} P_{X|T=T_{n}}\). We can find the distribution of \(\)'s first arrival by noting that no point of \(\) can come before it and hence \([T_{1} t]=[(t)=0]=(-(t))\), where the second equality follows since \((t)\) is Poisson distributed with mean \((t)\). A particularly important case is when \(\) is _time-homogeneous_, i.e. \((t)= t\) for some \(>0\), in which case \(T_{1}()\) is an exponential random variable with _rate_\(\). In fact, all of \(\)'s inter-arrival times \(_{n}=T_{n}-T_{n-1}\) for \(n 1\) share this simple distribution, where we set \(T_{0}=0\). To see this, note that

\[[_{n} t T_{n-1}]=[([T_{n -1},T_{n-1}+t)^{d})=0 T_{n-1}]=(- t), \]

i.e. all \(_{n} T_{n-1}()\). Therefore, we can use the above procedure to simulate time-homogeneous Poisson processes, described in Algorithm 1. We will refer to a time-homogeneous Poisson process with time rate \(\) and spatial distribution \(P_{X|T}\) as a \((,P_{X|T})\)-Poisson process.

**Rejection sampling using \(\):** Rejection sampling is a technique to simulate samples from a _target distribution_\(Q\) using a _proposal distribution_\(P\), assuming we can find an upper bound \(M>0\) for their density ratio \(r=dQ/dP\) (technically, the Radon-Nikodym derivative). We can formulate this procedure using a Poisson process: we simulate the arrivals \((T_{n},X_{n})\) of a \((1,P)\)-Poisson process \(\), but we only keep them with probability \(r(X_{n})/M\), otherwise, we delete them. This algorithm is described in Algorithm 2; its correctness is guaranteed by the _thinning theorem_(Maddison, 2016).

```
Input :Proposal distribution \(P\),  Density ratio \(r=dQ/dP\),  Upper bound \(M\) for \(r\). Output :Sample \(X Q\) and its index \(N\). // Generator for a \((1,P)\)-Poisson process using Algorithm 1. \((1,P)\) for\(n=1,2,\)do \((T_{n},X_{n})()\) \(U_{n}(0,1)\) if\(U_{n}<r(X_{n})/M\)then return\(X_{n},n\) end if  end for
```

**Algorithm 2**Standard rejection sampler.

**Rejection sampling is suboptimal:** Using Poisson processes to formulate rejection sampling highlights a subtle but crucial inefficiency: it does not make use of \(\)'s temporal structure and only uses the spatial coordinates. GPRS fixes this by using a rejection criterion that does depend on the time variable. As we show, this significantly speeds up sampling for certain classes of distributions.

### Channel Simulation

The main motivation for our work is to develop a _one-shot channel simulation protocol_ using the sampling algorithm we derive in Section 3. Channel simulation is of significant theoretical and practical interest. Recent works used it to compress neural network weights, achieving state-of-the-art performance (Havasi et al., 2018); to perform image compression using variational autoencoders (Flamich et al., 2020) and diffusion models with perfect realism (Theis et al., 2022); and to perform differentially private federated learning by compressing noisy gradients (Shah et al., 2022).

One-shot channel simulation is a communication problem between two parties, Alice and Bob, sharing a joint distribution \(P_{,}\) over two correlated random variables \(\) and \(\), where we assume that Alice and Bob can simulate samples from the marginal \(P_{}\). In a single round of communication, Alice receives a sample \( P_{}\) from the marginal distribution over \(\). Then, she needs to send the minimum number of bits to Bob such that he can **simulate a single sample** from the conditional distribution \( P_{|}\). Note that Bob **does not want to learn \(P_{|}\)**; he just wants to simulate a single sample from it. Surprisingly, when Alice and Bob have access to _shared randomness_, e.g. by sharing the seed of their random number generator before communication, they can solve channel simulation very efficiently. Mathematically, in this paper we will always model this shared randomness by some time-homogeneous Poisson process (or processes) \(\), since given a shared random seed, Alice and Bob can always simulate the same process using Algorithm 1. Then, the average coding cost of \(\) given \(\) is its conditional entropy \([]\) and, surprisingly, it is always upper bounded by \([] I[;]+_{2}(I[;]+1)+(1)\), where \(I[;]\) is the mutual information between \(\) and \(\)(Li and El Gamal, 2018). This is an especially curious result, given that in many cases \([]\) is infinite while \(I[;]\) is finite, e.g. when \(\) is a continuous variable. In essence, this result means that given the additional structure \(P_{,}\), channel simulation protocols can "offload" an infinite amount of information into the shared randomness, and only communicate the finitely many "necessary" bits.

**An example channel simulation protocol with rejection sampling:** Given \(\) and \( P_{}\), Alice sets \(Q P_{|}\) as the target and \(P P_{}\) as the proposal distribution with density ratio \(r=dQ/dP\), and run the rejection sampler in Algorithm 2 to find the first point of \(\) that was not deleted. She counts the number of samples \(N\) she had to simulate before acceptance and sends this number to Bob. He can then decode a sample \( P_{|}\) by selecting the spatial coordinate of the \(N\)th arrival of \(\).

Unfortunately, this simple protocol is suboptimal. To see this, let \(D_{}[Q\|P]}}}{{=}}_{ }\{_{2}r()\}\) denote Renyi \(\)-divergence of \(Q\) from \(P\), and recall two standard facts: (1) the best possible upper bound Alice can use for rejection sampling is \(M_{opt}=_{2}(D_{}[Q\|P])\), where \(_{2}(x)=2^{x}\), and (2), the number of samples \(N\) drawn until acceptance is a geometric random variable with mean \(M_{opt}\)(Maddison, 2016). We now state the two issues with rejection sampling that GPRS solves.

**Problem 1:****Codelength.** By using the formula for the entropy of a geometric random variable and assuming Alice uses the best possible bound \(M_{opt}\) in the protocol, we find that

\[[]=_{ P_{}}[ [N]]_{ P_{}}[D_ {}[P_{|}\|P_{}]]}}}{{}}I[;], \]

see Appendix I for the derivation. Unfortunately, inequality (a) can be _arbitrarily loose_, hence the average codelength scales with the expected \(\)-divergence instead of \(I[;]\), as would be optimal.

**Problem 2:****Slow runtime.** We are interested in classifying the time complexity of our protocol. As we saw, for a target \(Q\) and proposal \(P\), Algorithm 2 draws \(M_{opt}=_{2}(D_{}[Q\|P])\) samples on average. Unfortunately, under the computational hardness assumption \(\), Agustsson & Theis (2020) showed that without any further assumptions, there is no sampler that scales polynomially in \(D_{}[Q\|P]\). However, with further assumptions, we can do much better, as we show in Section 3.1.

## 3 Greedy Poisson Rejection Sampling

We now describe GPRS; its pseudo-code is shown in Algorithm 3. This section assumes that \(Q\) and \(P\) are the target and proposal distributions, respectively, and \(r=dQ/dP\) is their density ratio. Let \(\) be a \((1,P)\)-Poisson process. Our proposed rejection criterion is now embarrassingly simple: for an appropriate invertible function \(:^{+}^{+}\), accept the first arrival of \(\) that falls under the graph of the composite function \(= r\), as illustrated in the left plot in Figure 1. We refer to \(\) as the _stretch function_ for \(r\), as its purpose is to stretch the density ratio along the time-axis towards \(\).

**Deriving the stretch function (sketch, see Appendix A for details):** Let \(= r\), where for now \(\) is an arbitrary invertible function on \(^{+}\), let \(U=\{(t,x) t(x)\}\) be the set of points under the graph of \(\) and let \(= U\). By the restriction theorem (Kingman, 1992), \(\) is also a Poisson process with mean measure \((A)=(A U)\). Let \((,)\) be the first arrival of \(\), i.e. the first arrival of \(\) under \(\) and let \(Q_{}\) be the marginal distribution of \(\). Then, as we show in Appendix A, the density ratio \(dQ_{}/dP\) is given by

\[}{dP}(x)=_{0}^{(x)}[ t]\,dt. \]

Now we pick \(\) such that \(Q_{}=Q\), for which we need to ensure that \(dQ_{}/dP=r\). Substituting \(t=(x)\) into Equation (3), and differentiating, we get \((^{-1})^{}(t)=[ t]\). Since \((,)\) falls under the graph of \(\) by definition, we have \([ t]=[ t,r() ^{-1}(t)]\). By expanding the definition of the right-hand side, we obtain a time-invariant ODE for \(^{-1}\):

\[(^{-1})^{}=w_{Q}(^{-1})-^{-1}  w_{P}(^{-1}),^{-1}(0)=0, \]

where \(w_{P}(h)}}}{{=}}_{Z P}[ r(Z) h]\) and \(w_{Q}\) defined analogously. Finally, solving the ODE, we get

\[(h)=_{0}^{h}()- w_{P}()}\,d. \]

Remember that picking \(\) according to Equation (5) ensures that GPRS is **correct by construction**. To complete the picture, in Appendix A we prove that \((,)\) always exists and Algorithm 3 terminates with probability \(1\). We now turn our attention to analyzing the runtime of Algorithm 3 and surprisingly, we find that the expected runtime of GPRS matches that of standard rejection sampling; see Appendix B.2 for the proof. Note that in our analyses, we identify GPRS's time complexity with the number of arrivals of \(\) it simulates. Moreover, we assume that we can evaluate \(\) in \((1)\) time.

**Theorem 3.1** (Expected Runtime).: _Let \(Q\) and \(P\) be the target and proposal distributions for Algorithm 3, respectively, and \(r=dQ/dP\) their density ratio. Let \(N\) denote the number of samples simulated by the algorithm before it terminates. Then,_

\[[N]=_{2}(D_{}[Q\|P]) [N]_{2}(D_{}[Q\|P]), \]

_where \([]\) denotes the variance of a random variable._Furthermore, using ideas from the work of Liu and Verdu (2018), we can show the following upper bound on the fractional moments of the index; see Appendix B.3 for the proof.

**Theorem 3.2** (Fractional Moments of the Index).: _Let \(Q\) and \(P\) be the target and proposal distributions for Algorithm 3, respectively, and \(r=dQ/dP\) their density ratio. Let \(N\) denote the number of samples simulated by the algorithm before it terminates. Then, for \((0,1)\) we have_

\[[N^{}](1+[Q\|P]}{ _{2}e})_{2}( D_{}[Q\|P]), \]

_where \(D_{}[Q\|P]=_{2}_{Z Q}[r(Z)^{ -1}]\) is the Renyi \(\)-divergence of \(Q\) from \(P\)._

**GPRS induces a channel simulation protocol:** We use an analogous solution to the standard rejection sampling-based scheme in Section 2.2, to obtain a channel simulation protocol for a pair of correlated random variables \(, P_{,}\). First, we assume that both the encoder and the decoder have access to the same realization of a \((1,P_{})\)-Poisson process \(\), which they will use as their shared randomness. In a single round of communication, the encoder receives \( P_{}\), sets \(Q P_{|}\) as the target and \(P P_{}\) as the proposal distribution, and runs Algorithm 3 to find the index \(N\) of the accepted arrival in \(\). Finally, following Li and El Gamal (2018), they use a zeta distribution \((n s) n^{-s}\) with \(s^{-1}=I[;]+2_{2}e\) to encode \(N\) using entropy coding. Then, the decoder simulates a \(P_{|}\)-distributed sample by looking up the spatial coordinate \(X_{N}\) of \(\)'s \(N\)th arrival. The following theorem shows that this protocol is optimally efficient; see Appendix B.4 for the proof.

**Theorem 3.3** (Expected Codelength).: _Let \(P_{,}\) be a joint distribution over correlated random variables \(\) and \(\), and let \(\) be the \((1,P_{})\)-Poisson process used by Algorithm 3. Then, the above protocol induced by the the algorithm is optimally efficient up to a constant, in the sense that_

\[[] I[;]+_{2}(I[ ;]+1)+6. \]

Note, that this result shows we can use GPRS as an alternative to the _Poisson functional representation_ to prove of the _strong functional representation lemma_ (Theorem 1; Li and El Gamal, 2018).

**GPRS in practice:** The real-world applicability of Algorithm 3 is limited by the following three computational obstacles: (1) GPRS requires exact knowledge of the density ratio \(dQ/dP\) to evaluate \(\), which immediately excludes a many practical problems where \(dQ/dP\) is known only up to a normalizing constant. (2) To compute \(\), we need to evaluate Equation (4) or (5), which requires us to compute \(w_{P}\) and \(w_{Q}\), which is usually intractable for general \(dQ/dP\). Fortunately, in some special cases they can be computed analytically: in Appendix G, we provide the expressions for discrete, uniform, triangular, Gaussian, and Laplace distributions. (3) However, even when we can compute \(w_{P}\) and \(w_{Q}\), analytically evaluating the integral in Equation (5) is usually not possible. Moreover, solving it numerically is unstable as \(\) is unbounded. Instead, we numerically solve for \(^{-1}\) using Equation (4) in practice, which fortunately turns out to be stable.

However, we emphasize that GPRS's _raison d'etre_ is to perform channel simulation using the protocol we outline above or the ones we present in Section 3.1, not general-purpose random variate simulation. Practically relevant channel simulation problems, such as encoding a sample from the latent posterior of a VAE (Flamich et al., 2020) or encoding variational implicit neural representations (Guo et al., 2023; He et al., 2023) usually involve simple distributions such as Gaussians. In these cases, we have readily available formulae for \(w_{P}\) and \(w_{Q}\) and can cheaply evaluate \(^{-1}\) via numerical integration.

**GPRS as greedy search:** The defining feature of GPRS is that its acceptance criterion at each step is _local_, since if at step \(n\) the arrival \((T_{n},X_{n})\) in \(\) falls under \(\), the algorithm accepts it. Thus it is _greedy_ search procedure. This is in contrast with A* sampling (Maddison et al., 2014), whose acceptance criterion is _global_, as the acceptance of the \(n\)th arrival \((T_{n},X_{n})\) depends on all other points of \(\) in the general case. In other words, using the language of Liu and Verdu (2018), GPRS is a _causal_ sampler, as its acceptance criterion at each step only depends on the samples the algorithm has examined in previous steps. On the other hand, A* sampling is _non-causal_ as its acceptance criterion in each step depends on future samples as well. Surprisingly, this difference between the search criteria does not make a difference in the average runtimes and codelengths in the general case. However, as we show in the next section, GPRS can be much faster in special cases.

### Speeding up the greedy search

This section discusses two ways to improve the runtime of GPRS. First, we show how we can utilize available parallel computing power to speed up Algorithm 3. Second, we propose an advanced search strategy when the spatial domain \(\) has more structure and obtain a **super-exponential improvement** in the runtime from \(_{2}(D_{}[Q\|P])\) to \((D_{}[Q\|P])\) in certain cases.

**Parallel GPRS:** The basis for parallelizing GPRS is the _superposition theorem_(Kingman, 1992): For a positive integer \(J\), let \(_{1},_{J}\) all be \((1/J,P)\)-Poisson processes; then, \(=_{j=1}^{J}_{j}\) is a \((1,P)\)-Poisson process. As shown in Algorithm 4, this result makes parallelizing GPRS very simple given \(J\) parallel threads: First, we independently look for the first arrivals of \(_{1},,_{J}\) under the graph of \(\), yielding \((T_{N_{1}}^{(1)},X_{N_{1}}^{(1)}),,(T_{N_{J}}^{(J)},X_{N _{J}}^{(J)})\), respectively, where the \(N_{j}\) corresponds to the index of \(_{j}\)'s first arrival under \(\). Then, we select the candidate with the earliest arrival time, i.e. \(j^{*}=_{j\{1,,J\}}T_{N_{j}}^{(j)}\). Now, by the superposition theorem, \((T_{N_{j^{*}}}^{(j^{*})},X_{N_{j^{*}}}^{(j^{*})})\) is the first arrival of \(\) under \(\), and hence \(X_{N_{j^{*}}}^{(j^{*})} Q\). Finally, Alice encodes the sample via the tuple \((j^{*},N_{j^{*}})\), i.e. which of the \(J\) processes the first arrival occurred in, and the index of the arrival in \(_{j^{*}}\). See the middle graphic in Figure 1 for an example with \(J=2\) parallel threads.

```
Input :Proposal distribution \(P\),  Density ratio \(r=dQ/dP\),  Stretch function \(\),  Number of parallel threads \(J\). Output :Sample \(X Q\) and its code \((j^{*},N_{j^{*}})\). \(T^{*},X^{*},j^{*},N_{j^{*}},,,\)in parallel for\(j=1,,J\)do \(_{j}(1/J,P)\)for\(n_{j}=1,2,\)do \((T_{n_{j}}^{(j)},X_{n_{j}}^{(j)})(_{j})\)if\(T^{*}<T_{n_{j}}^{(j)}\)then terminate thread \(j\).  end if if\(T_{n_{j}}^{(j)}<(r(X_{n_{j}}^{(j)}))\)then terminate thread \(j\).  end if  end for return\(X^{*},(j^{*},N_{j^{*}})\)
```

**Algorithm 4**Parallel GPRS with \(J\) available threads.

Our next results show that parallelizing GPRS results in a linear reduction in both the expectation and variance of its runtime and a more favourable codelength guarantee for channel simulation.

**Theorem 3.4** (Expected runtime of parallelized GPRS).: _Let \(Q,P\) and \(r\) be defined as above, and let \(_{j}\) denote the random variable corresponding to the number of samples simulated by thread \(j\) in Algorithm 4 using \(J\) threads. Then, for all \(j\),_

\[[_{j}]=(_{2}(D_{}[Q\|P])-1)/J+1 [_{j}](_{2}(D_{}[Q\|P ])-1)J+1. \]

**Theorem 3.5** (Expected codelength of parallelized GPRS).: _Let \(P_{,}\) be a joint distribution over correlated random variables \(\) and \(\), and let \(_{1},,_{J}\) be \((1/J,P_{})\)-Poisson processes. Then, assuming \(_{2}J I[;]\), parallelized GPRS induces a channel simulation protocol such that_

\[[_{1},,_{J}] I[;]+ _{2}(I[;]-_{2}J+1)+8. \]

See Appendix C for the proofs. Note that we can use the same parallelisation argument with the appropriate modifications to speed up \(^{*}\) sampling / coding too.

**Branch-and-bound GPRS on \(\):** We briefly restrict our attention to problems on \(\) when the density ratio \(r\) is unimodal, as we can exploit this additional structure to more efficiently search for \(\)'s first arrival under \(\). Consider the example in the right graphic in Figure 1: we simulate the first arrival \((T_{1},X_{1})\), and reject it, since \(T_{1}>(X_{1})\). Since \(r\) is unimodal by assumption and \(\) is increasing, \(\) is also unimodal. Hence, for \(x<X_{1}\) we must have \((x)<(X_{1})\), while the arrival time of any of the later arrivals will be larger than \(T_{1}\). Therefore, none of the arrivals to the left of \(X_{1}\) will fall under \(\) either! Hence, it is sufficient to simulate \(\) to the right of \(X_{1}\), which we can easily do using generalized inverse transform sampling.1 We repeat this argument for all further arrivals to obtain an efficient search procedure for finding \(\)'s first arrival under \(\), described in Algorithm 5: We simulate the next arrival \((T_{B},X_{B})\) of \(\) within some bounds \(B\), starting with \(B=\). If \(T_{B}(X_{B})\) we accept; otherwise, we truncate the bound to \(B B(-,X_{B})\), or \(B B(X_{B},)\) based on where \(X_{B}\) falls relative to \(\)'s mode. We repeat these two steps until we find the first arrival.

Since Algorithm 5 does not simulate every point of \(\), we cannot use the index \(N\) of the accepted arrival to obtain a channel simulation protocol as before. Instead, we encode the _search path_, i.e. whether we chose the left or the right side of our current sample at each step. Similarly to A* coding, we encode the path using its _heap index_(Flamich et al., 2022): the root has index \(H_{root}=1\), and for a node with index \(H\), its left child is assigned index \(2H\) and its right child \(2H+1\). As the following theorems show, this version of GPRS is, in fact, optimally efficient; see Appendix E for proofs.

**Theorem 3.6** (Expected Runtime of GPRS with binary search).: _Let \(Q,P\) and \(r\) be defined as above, and let \(D\) denote the number of samples simulated by Algorithm 5 and \(=1/_{2}(4/3)\) Then,_

\[[D] D_{}[Q\|P]+(1). \]

**Theorem 3.7** (Expected Codelength of GPRS with binary search).: _Let \(P_{,}\) be a joint distribution over correlated random variables \(\) and \(\), and let \(\) be a \((1,P_{})\)-Poisson process. Then, GPRS with binary search induces a channel simulation protocol such that_

\[[] I[;]+2_{2}(I[ ;]+1)+11. \]

**Branch-and-bound GPRS with splitting functions:** With some additional machinery, Algorithm 5 can be extended to more general settings, such as \(^{D}\) and cases where \(r\) is not unimodal, by introducing the notion of a _splitting function_. For a region of space, \(B\), a splitting function split simply returns a binary partition of the set, i.e. \(\{L,R\}=(B)\), such that \(L R=\) and \(L R=B\). In this case, we can perform a similar tree search to Algorithm 5, captured in Algorithm 6. Recall that \((,)\) is the first arrival of \(\) under \(\). Starting with the whole space \(B=\), we simulate the next arrival \((T,X)\) of \(\) in \(B\) at each step. If we reject it, we partition \(B\) into two parts, \(L\) and \(R\), using split. Then, with probability \([ R B, T]\), we continue searching through the arrivals of \(\) only in \(R\), and only in \(L\) otherwise. We show the correctness of this procedure in Appendix F and describe how to compute \([ R B, T]\) and \(\) in practice. This splitting procedure is analogous to the general version of A* sampling/coding (Maddison et al., 2014; Flamich et al., 2022), which is parameterized by the same splitting functions. Note that Algorithm 5 is a special case of Algorithm 6, where \(=\), \(r\) is unimodal, and at each step for an interval bound \(B=(a,b)\) and sample \(X(a,b)\) we split \(B\) into \(\{(a,X),(X,b)\}\).

## 4 Experiments

We compare the average and one-shot case efficiency of our proposed variants of GPRS and a couple of other channel simulation protocols in Figure 2. See the figure's caption and Appendix H for details of our experimental setup. The top two plots in Figure 2 demonstrate that our methods' expected runtimes and codelengths align well with our theorems' predictions and compare favourably to other methods. Furthermore, we find that the mean performance is a _robust statistic_ for the binary search-based variants of GPRS in that it lines up closely with the median, and the interquartile range is quite narrow. However, we also see that Theorem 3.7 is slightly loose empirically. We conjecture that the coefficient of the \(_{2}(I[;]+1)\) term in Equation (12) could be reduced to \(1\), and that the constant term could be improved as well.

On the other hand, the bottom plot in Figure 2 demonstrates the most salient property of the binary search variant of GPRS: unlike all previous methods, its runtime scales with \(D_{}[Q\|P]\) and not \(D_{}[Q\|P]\). Thus, we can apply it to a larger family of channel simulation problems, where \(D_{}[Q\|P]\) is finite but \(D_{}[Q\|P]\) is large or even infinite, and other methods would not terminate.

```
Input : Proposal distribution \(P\),  Density ratio \(r=dQ/dP\),  Stretch function \(\),  Splitting function split. Output : Sample \(X Q\) and its  heap index \(H\) \(T_{0},H,B(0,1,)\) for\(d=1,2,\)do \(X_{d} P|_{B}\) \(_{d}(P(B))\) \(T_{d} T_{d-1}+_{d}\) if\(T_{d}<(r(X_{d}))\)then return\(X_{d},H\) else \(B_{0},B_{1}(B)\) \(\) \([ B_{1} B, T_{d}]\) \(()\) \(H 2H+\) \(B B_{}\)  end if  end for
```

**Algorithm 6**Branch-and-bound GPRS with splitting function.

## 5 Related Work

**Poisson processes for channel simulation:** Poisson processes were introduced to the channel simulation literature by Li and El Gamal (2018) via their construction of the _Poisson functional representation_ (PFR) in their proof of the strong functional representation lemma. Flamich et al. (2022) observed that the PFR construction is equivalent to a certain variant of A* sampling (Maddison et al., 2014; Maddison, 2016). Thus, they proposed an optimized version of the PFR called A* coding, which achieves \((D_{}[Q\|P])\) runtime for one-dimensional unimodal distributions. GPRS was mainly inspired by A* coding, and they are _dual_ constructions to each other in the following sense: _Depth-limited A* coding_ can be thought of as an _importance sampler_, i.e. a Monte Carlo algorithm that returns an approximate sample in fixed time. On the other hand, GPRS is a _rejection sampler_, i.e. a Las Vegas algorithm that returns an exact sample in random time.

Figure 2: **Left: Binary search GPRS with arbitrary splitting function. Right: Performance comparison of different channel simulation protocols. In each plot, _dashed lines_ indicate the mean, _solid lines_ the median and the _shaded areas_ the 25 - 75 percentile region of the relevant performance metric. We computed the statistics over 1000 runs for each setting. Top right: Runtime comparison on a 1D Gaussian channel simulation problem \(P_{,}\), plotted against increasing mutual information \(I[;]\). Alice receives \((0,^{2})\) and encodes a sample \((,1)\) to Bob. The abbreviations in the legend are: _GPRS_ – Algorithm 3; _PFR_ – Poisson functional representation / Global-bound A* coding (Li and El Gamal, 2018; Flamich et al., 2022), _AS*_ – Split-on-sample A* coding (Flamich et al., 2022); _sGPRS_ – split-on-sample GPRS (Algorithm 5); _dGPRS_ – GPRS with dyadic split (Algorithm 6). Middle right: Average codelength comparison of our proposed algorithms on the same channel simulation problem as above. _UB_ in the legend corresponds to an _upper bound_ of \(I[;]+_{2}(I[;]+1)+2\) bits. We estimate the algorithms’ expected codelengths by encoding the indices returned by GPRS using a Zeta distribution \((n) n^{-}\) with \(=1+1/I[;]\) in each case, which is the optimal maximum entropy distribution for this problem setting (Li and El Gamal, 2018). Bottom right: One-shot runtime comparison of sGPRS with AS* coding. Alice encodes samples from a target \(Q=(m,s^{2})\) using \(P=(0,1)\) as the proposal. We computed \(m\) and \(s^{2}\) such that \(D_{}[Q\|P]=2\) bits for each problem, but \(D_{}[Q\|P]\) increases. GPRS’ runtime stays fixed as it scales with \(D_{}[Q\|P]\), while the runtime of A* keeps increasing.**

**Channel simulation with dithered quantization:**_Dithered quantization_ (DQ; Ziv, 1985) is an alternative to rejection and importance sampling-based approaches to channel simulation. DQ exploits that for any \(c\) and \(U,U^{}(-1/2,1/2)\), the quantities \( c-U+U\) and \(c+U^{}\) are equal in distribution. While DQ has been around for decades as a tool to model and analyze quantization error, Agustsson & Theis (2020) reinterpreted it as a channel simulation protocol and used it to develop a VAE-based neural image compression algorithm. Unfortunately, basic DQ only allows uniform target distributions, limiting its applicability. As a partial remedy, Theis & Yosri (2022) showed DQ could be combined with other channel simulation protocols to speed them up and thus called their approach hybrid coding (HQ). Originally, HQ required that the target distribution be compactly supported, which was lifted by Flamich & Theis (2023), who developed an adaptive rejection sampler using HQ. In a different vein, Hegazy & Li (2022) generalize and analyze a method proposed in the appendix of Agustsson & Theis (2020) and show that DQ can be used to realize channel simulation protocols for one-dimensional symmetric, unimodal distributions.

**Greedy Rejection Coding:** Concurrently to our work, Flamich et al. (2023) introduce greedy rejection coding (GRC) which generalizes Harsha et al.'s rejection sampling algorithm to arbitrary probability spaces and arbitrary splitting functions, extending the work of Flamich & Theis (2023). Furthermore, they also utilize a space-partitioning procedure to speed up the convergence of their sampler and prove that a variant of their sampler also achieves optimal runtime for one-dimensional problems where \(dQ/dP\) is unimodal. However, the construction of their method differs significantly from ours. GRC is a direct generalization of Harsha et al. (2007)'s algorithm and, thus, a more "conventional" rejection sampler, while we base our construction on Poisson processes. Thus, our proof techniques are also significantly different. It is an interesting research question whether GRC could be formulated using Poisson processes, akin to standard rejection sampling in Algorithm 2, as this could be used to connect the two algorithms and improve both.

## 6 Discussion and Future Work

Using the theory of Poisson processes, we constructed greedy Poisson rejection sampling. We proved the correctness of the algorithm and analyzed its runtime, and showed that it could be used to obtain a channel simulation protocol. We then developed several variations on it, analyzed their runtimes, and showed that they could all be used to obtain channel simulation protocols. As the most significant result of the paper, we showed that using the binary search variant of GPRS we can achieve \((D_{}[Q\|P])\) runtime for arbitrary one-dimensional, unimodal density ratios, significantly improving upon the previous best \((D_{}[Q\|P])\) bound by A* coding.

There are several interesting directions for future work. From a practical perspective, the most pressing question is whether efficient channel simulation algorithms exist for multivariate problems; finding an efficient channel simulation protocol for multivariate Gaussians would already have far-reaching practical consequences. We also highlight an interesting theoretical question. We found that the most general version of GPRS needs to simulate exactly \(_{2}(D_{}[Q\|P])\) samples in expectation, which matches A* sampling. From Agustsson & Theis (2020), we know that for general problems, the average number of simulated samples has to be at least on the order of \(_{2}(D_{}[Q\|P])\). Hence we ask whether this lower bound can be tightened to match the expected runtime of GPRS and A* sampling or whether there exists an algorithm that achieves a lower runtime.

## 7 Acknowledgements

We are grateful to Lennie Wells for the many helpful discussions throughout the project, particularly for suggesting the nice argument for Lemma D.2; Peter Wildemann for his advice on solving some of the differential equations involved in the derivation of GPRS and Stratis Markou for his help that aided our understanding of some high-level concepts so that we could derive Algorithm 6. We would also like to thank Lucas Theis for the many enjoyable early discussions about the project and for providing helpful feedback on the manuscript; and Cheuk Ting Li for bringing Liu & Verdu's work to our attention. Finally, we would like to thank Daniel Goc for proofreading the manuscript and Lorenzo Bonito for helping to design Figures 1 and 2 and providing feedback on the early version of the manuscript. GF acknowledges funding from DeepMind.