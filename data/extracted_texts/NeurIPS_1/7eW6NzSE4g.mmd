# Information Maximizing Curriculum:

A Curriculum-Based Approach for Imitating Diverse Skills

Denis Blessing \({}^{}\) Onur Celik\({}^{@sectionsign}\) Xiaogang Jia\({}^{}\) Moritz Reuss\({}^{}\)

**Maximilian Xiling Li\({}^{}\) Rudolf Lioutikov\({}^{}\) Gerhard Neumann\({}^{@sectionsign}\) \({}^{}\) Autonomous Learning Robots, Karlsruhe Institute of Technology \({}^{}\) Intuitive Robots Lab, Karlsruhe Institute of Technology \({}^{@sectionsign}\) FZI Research Center for Information Technology**

Correspondence to denis.blessing@kit.edu

###### Abstract

Imitation learning uses data for training policies to solve complex tasks. However, when the training data is collected from human demonstrators, it often leads to multimodal distributions because of the variability in human actions. Most imitation learning methods rely on a maximum likelihood (ML) objective to learn a parameterized policy, but this can result in suboptimal or unsafe behavior due to the mode-averaging property of the ML objective. In this work, we propose _Information Maximizing Curriculum_, a curriculum-based approach that assigns a weight to each data point and encourages the model to specialize in the data it can represent, effectively mitigating the mode-averaging problem by allowing the model to ignore data from modes it cannot represent. To cover all modes and thus, enable diverse behavior, we extend our approach to a mixture of experts (MoE) policy, where each mixture component selects its own subset of the training data for learning. A novel, maximum entropy-based objective is proposed to achieve full coverage of the dataset, thereby enabling the policy to encompass all modes within the data distribution. We demonstrate the effectiveness of our approach on complex simulated control tasks using diverse human demonstrations, achieving superior performance compared to state-of-the-art methods.

## 1 Introduction

Equipping agents with well-performing policies has long been a prominent focus in machine learning research. Imitation learning (IL)  offers a promising technique to mimic human behavior by leveraging expert data, without the need for intricate controller design, additional environment interactions, or complex reward shaping to encode the target behavior. The latter are substantial advantages over reinforcement learning techniques  that rely on reward feedback. However, a significant challenge in IL lies in handling the multimodal nature of data obtained from human demonstrators, which can stem from differences in preferences, expertise, or problem-solving strategies. Conventionally, maximum likelihood estimation (MLE) is employed to train a policy on expert data. It is well-known that MLE corresponds to the moment projection , causing the policy to average over modes in the data distribution that it cannot represent. Such mode averaging can lead to unexpected and potentially dangerous behavior. We address this critical issue by introducing _Information Maximizing Curriculum_ (IMC), a novel curriculum-based approach.

In IMC, we view imitation learning as a conditional density estimation problem and present a mathematically sound weighted optimization scheme. Data samples are assigned curriculum weights, which are updated using an information projection. The information projection minimizes the reverse

[MISSING_PAGE_FAIL:2]

Information Maximizing Curriculum

In this section, we propose Information Maximizing Curriculum (IMC), a novel algorithm for training mixtures of expert polices. We motivate our optimization objective using a single policy. Next, we generalize the objective to support learning a mixture of experts policy. Thereafter, we discuss the optimization scheme and provide algorithmic details.

### Objective for a Single Expert Policy

We propose an objective that jointly learns a curriculum \(p(,)\) and a parameterized policy \(p_{}(|)\) with parameters \(\). The curriculum is a categorical distribution over samples of a dataset \(\{(_{n},_{n})\}_{n=1}^{N}\), assigning probability mass to samples according to the performance of the policy. To allow the curriculum to ignore samples that the policy cannot represent, we build on the I-projection (see Equation 1). We, therefore, formulate the objective function as

\[(p(,),)=_{p(, )}[ p_{}(|)]+( ,), \]

which is optimized for \(p(,)\) and \(\) in an alternating fashion using coordinate ascent . We additionally introduce a trade-off factor \(\) that determines the pacing of the curriculum. For \(\) the curriculum becomes uniform, exposing all samples to the policy and hence reducing to maximum likelihood estimation for \(\). In contrast, if \( 0\) the curriculum concentrates on samples where the policy log-likelihood \( p_{}(|)\) is highest. The objective can be solved in closed form for \(p(,)\) (see Appendix B), resulting in

\[p^{*}(_{n},_{n}) p_{}(_{n}| _{n})^{1/}.\]

Maximizing the objective w.r.t \(\) reduces to a weighted maximum-likelihood estimation, that is,

\[^{*}=*{arg\,max}_{}_{n}p(_{n},_{n}) p_{}(_{n}|_{n}).\]

The optimization is repeated until reaching a local maximum, indicating that the curriculum has selected a fixed set of samples where the policy attains high log-likelihoods \( p_{}(|)\). Proposition 3.1 establishes convergence guarantees, the details of which are elaborated upon in Appendix A.1.

**Proposition 3.1**.: _Let \(\) be defined as in Equation 2 and \(0<<\). Under mild assumptions on \(p_{}\) and the optimization scheme for \(\), it holds for all \(i\), denoting the iteration index of the optimization process, that_

\[(p(,)^{(i+1)},^{(i+1)})(p (,)^{(i)},^{(i)}),\]

_where equality indicates that the algorithm has converged to a local optimum._

The capacity to disregard samples where the policy cannot achieve satisfactory performance mitigates the mode-averaging problem. Nevertheless, a drawback of employing a single expert policy is the potential for significantly suboptimal performance on the ignored samples. This limitation is overcome by introducing multiple experts, each specializing in different subsets of the data.

### Objective for a Mixture of Experts Policy

Assuming limited complexity, a single expert policy is likely to ignore a large amount of samples due to the zero-forcing property of the I-projection. Using multiple curricula and policies that specialize to different subsets of the data is hence a natural extension to the single policy model. To that end, we make two major modifications to Equation 2: Firstly, we use a mixture model with multiple components \(z\) where each component has its own curriculum, i.e., \(p(,)=_{z}p(z)p(,|z)\). Secondly, we employ an expert policy per component \(p_{_{z}}(|,z)\), that is paced by the corresponding curriculum \(p(,|z)\). The resulting objective function is given by

\[J()=_{p(z)}_{p(,|z)}[ p_{ _{z}}(|,z)]+(,), \]

where \(\) summarizes the dependence on \(p(z)\), \(\{p(,|z)\}_{z}\) and \(\{_{z}\}_{z}\). However, Equation 3 is difficult to optimize as the entropy of the mixture model prevents us from updating the curriculum of each component independently. Similar to , we introduce an auxiliary distribution \(q(z|,)\) to decompose the objective function into a lower bound \(L(,q)\) and an expected \(D_{}\) term, that is,

\[J()=L(,q)+_{p(,)}D_{ }(p(z|,)\|q(z|,)), \]with \(p(z|,)=p(,|z)p(z)/p(,)\) and

\[L(,q)=_{p(z)}_{p(, |z)}[R_{z}(,)]+(, |z)]}_{J_{z}(p(,|z),_{z})}+ (z),\]

with \(R_{z}(_{n},_{n})= p_{_{z}}(_{n}| _{n},z)+ q(z|_{n},_{n})\), allowing for independent updates for \(p(,|z)\) and \(_{z}\) by maximizing the per-component objective function \(J_{z}(p(,|z),_{z})\). A derivation can be found in Appendix B.1. Since \(_{p(,)}D_{}(p(z|,)\|q(z|)) 0\), \(L\) is a lower bound on \(J\) for \( 0\). Please note that the per-component objective function \(J_{z}\) is very similar to Equation 2, with \(J_{z}\) including an additional term, \( q(z|,)\), which serves the purpose of preventing different curricula from assigning probability mass to the same set of samples: Specifically, a component \(z\) is considered to 'cover' a datapoint \((_{n},_{n})\) when \(q(z|_{n},_{n}) 1\). Since \(_{z}q(z|_{n},_{n})=1\), it follows that for other components \(z^{} z\) it holds that \(q(z^{}|_{n},_{n}) 0\). Consequently, \( q(z^{}|_{n},_{n})-\), implying that \(R_{z^{}}(_{n},_{n})-\). As a result, the other curricula effectively disregard the datapoint, as \(p(_{n},_{n}|z^{}) R_{z^{}}(_{n},_{n}) 0\).

We follow the optimization scheme of the expectation-maximization algorithm , that is, we iteratively maximize (M-step) and tighten the lower bound (E-step) \(L(,q)\).

### Maximizing the Lower Bound (M-Step)

We maximize the lower bound \(L(,q)\) with respect to the mixture weights \(p(z)\), curricula \(p(,|z)\) and expert policy parameters \(_{z}\). We find closed form solutions for both, \(p(z)\) and \(p(,|z)\) given by

\[p^{*}(z)_{p(,|z)}[R_{z}( ,)/]+(,|z),\; \;(_{n},_{n}|z)=R_{z}( _{n},_{n})/, \]

where \((_{n},_{n}|z)\) are the optimal unnormalized curricula, such that holds \(p^{*}(_{n},_{n}|z)=(_{n},_{n }|z)/_{n}(_{n},_{n}|z)\). However, due to the hierarchical structure of \(L(,q)\) we implicitly optimize for \(p(z)\) when updating the curricula. This result is frequently used throughout this work and formalized in Proposition 3.2. A proof can be found in Appendix A.2.

**Proposition 3.2**.: _Let \(p^{*}(z)\) and \((,|z)\) be the optimal mixture weights and unnormalized curricula for maximizing \(L(,q)\). It holds that_

\[p^{*}(z)=_{n}(_{n},_{n}|z)/_{z}_{n} (_{n},_{n}|z).\]

The implicit updates of the mixture weights render the computation of \(p^{*}(z)\) obsolete, reducing the optimization to computing the optimal (unnormalized) curricula \((,|z)\) and expert policy parameters \(_{z}^{*}\). In particular, this result allows for training the policy using mini-batches and thus greatly improves the scalability to large datasets as explained in Section 3.5. Maximizing the lower bound with respect to \(_{z}^{*}\) results in a weighted maximum likelihood estimation, i.e.,

\[_{z}^{*}=*{arg\,max}_{_{z}}\;_{n} {p}(_{n},_{n}|z) p_{_{z}}(_{n}| _{n},z), \]

where the curricula \((_{n},_{n}|z)\) assign sample weights. For further details on the M-step, including derivations of the closed-form solutions and the expert parameter objective see Appendix B.2.

### Tightening the Lower Bound (E-Step)

Tightening of the lower bound (also referred to as E-step) is done by minimizing the expected Kullback-Leibler divergence in Equation 4. Using the properties of the KL divergence, it can easily be seen that the lower bound is tight if for all \(n\{1,...,N\}\)\(q(z|_{n})=p(z|_{n},_{n})\) holds. To obtain \(p(z|_{n},_{n})\) we leverage Bayes' rule, that is, \(p(z|_{n},_{n})=p^{*}(z)p^{*}(_{n},_{n} |z)/_{z}p^{*}(z)p^{*}(_{n},_{n}|z)\). Using Proposition 3.2 we find that

\[p(z|_{n},_{n})=(_{n},_{n}|z)/ _{z}(_{n},_{n}|z).\]

Please note that the lower bound is tight after every E-step as the KL divergence is set to zero. Thus, increasing the lower bound \(L\) maximizes the original objective \(J\) assuming that updates of \(_{z}\) are not decreasing the expert policy log-likelihood \( p_{_{z}}(|,z)\).

### Algorithmic Details

**Convergence Guarantees.** Proposition 3.3 establishes convergence guarantees for the mixture of experts policy objective \(J\). The proof mainly relies on the facts that IMC has the same convergence guarantees as the EM algorithm and that Proposition 3.1 can be transferred to the per-component objective \(J_{z}\). The full proof is given in Appendix A.1.

**Proposition 3.3**.: _Let \(J\) be defined as in Equation 4 and \(0<<\). Under mild assumptions on \(p_{_{z}}\) and the optimization scheme for \(_{z}\), it holds for all \(i\), denoting the iteration index of the optimization process, that_

\[J(^{(i+1)}) J(^{(i)}),\]

_where equality indicates that the IMC algorithm has converged to a local optimum._

**Stopping Criterion.** We terminate the algorithm if either the maximum number of training iterations is reached or if the lower bound \(L(,q)\) converges, i.e.,

\[| L|=|L^{(i)}(,q)-L^{(i-1)}(,q)|,\]

with threshold \(\) and two subsequent iterations \((i)\) and \((i-1)\). The lower bound can be evaluated efficiently using Corollary 3.2.1.

**Corollary 3.2.1**.: _Consider the setup used in Proposition 3.2. For \(p^{*}(z)\) and \(\{p^{*}(,|z)\}_{z}\) it holds that_

\[L,q=_{z}_{n}(_{n}, _{n}|z).\]

See Appendix A.3 for a proof.

**Inference.** In order to perform inference, i.e., sample actions from the policy, we need to access the gating distribution for arbitrary observations \(\) which is not possible as \(p(z|,)\) is only defined for observations contained in the dataset \(,\). We therefore leverage Corollary 3.2.2 to learn an inference network \(g_{}(z|)\) with parameters \(\) by minimizing the KL divergence between \(p(z|)\) and \(g_{}(z|)\) under \(p()\) (see Appendix A.4 for a proof).

**Corollary 3.2.2**.: _Consider the setup used in Proposition 3.2. It holds that_

\[_{}_{p()}D_{}p(z| )\|g_{}(z|)=\ _{}_{n}_{z}( _{n},_{n}|z) g_{}(z|_{n}).\]

Once trained, the inference network can be used for computing the exact log-likelihood as \(p(|)=_{z}g_{}(z|)p_{_{z }}(|,z)\) or sampling a component index, i.e., \(z^{} g_{}(z|)\).

**Mini-Batch Updates.** Due to Proposition 3.2, the M- and E-step in the training procedure only rely on the unnormalized curricula \((,|z)\). Consequently, there is no need to compute the normalization constant \(_{n}(_{n},_{n}|z)\). This allows us to utilize mini-batches instead of processing the entire dataset, resulting in an efficient scaling capability for large datasets. Please refer to Algorithm 1 for a detailed description of the complete training procedure.

## 4 Related Work

**Imitation Learning.** A variety of algorithms in imitation learning [1; 11] can be grouped into two categories: Inverse reinforcement learning [12; 13], which extracts a reward function from demonstrations and optimizes a policy subsequently, and behavioral cloning, which directly extracts a policy from demonstrations. Many works approach the problem of imitation learning by considering behavior cloning as a distribution-matching problem, in which the state distribution induced by the policy is required to align with the state distribution of the expert data. Some methods [14; 15] are based on adversarial methods inspired by Generative Adversarial Networks (GANs) . A policy is trained to imitate the expert while a discriminator learns to distinguish between fake and expert data. However, these methods are not suitable for our case as they involve interacting with the environment during training. Other approaches focus on purely offline training and use various policy representations such as energy-based models , normalizing flows [18; 19], conditional variational autoencoders (CVAEs) [20; 21], transformers , or diffusion models [23; 24; 25; 26; 27; 28]. Thesemodels can represent multi-modal expert distributions but are optimized based on the M-Projection, which leads to a performance decrease. Recent works [29; 30] have proposed training Mixture of Experts models with an objective similar to ours. However, the work by  requires environment-specific geometric features, which is not applicable in our setting, whereas the work by  considers linear experts and the learning of skills parameterized by motion primitives . For a detailed differentiation between our work and the research conducted by , please refer to Appendix D.1.

**Curriculum Learning.** The authors of  introduced curriculum learning (CL) as a new paradigm for training machine learning models by gradually increasing the difficulty of samples that are exposed to the model. Several studies followed this definition [33; 34; 35; 36; 37; 38; 39]. Other studies used the term curriculum learning for gradually increasing the model complexity [40; 41; 42] or task complexity [43; 44; 45; 46]. All of these approaches assume that the difficulty-ranking of the samples is known a-priori. In contrast, we consider dynamically adapting the curriculum according to the learning progress of the model which is known as self-paced learning (SPL). Pioneering work in SPL was done in  which is related to our work in that the authors propose to update the curriculum as well as model parameters iteratively. However, their method is based on maximum likelihood which is different from our approach. Moreover, their algorithm is restricted to latent structural support vector machines. For a comprehensive survey on curriculum learning, the reader is referred to .

## 5 Experiments

We briefly outline the key aspects of our experimental setup.

**Experimental Setup.** For all experiments, we employ conditional Gaussian expert policies, i.e., \(p_{_{s}}(|,z)=(|_ {_{s}}(),^{2})\). Please note that we parameterize the expert means \(_{_{s}}\) using neural networks. For complex high-dimensional tasks, we share features between different expert policies by introducing a deep neural network backbone. Moreover, we use a fixed variance of \(^{2}=1\) and \(N_{z}=50\) components for all experiments and tune the curriculum pacing \(\). For more details see Appendix C.2.

**Baselines.** We compare our method to state-of-the-art generative models including denoising diffusion probabilistic models (DDPM) , normalizing flows (NF)  and conditional variational autoencoders (CVAE) . Moreover, we consider energy-based models for behavior learning (IBC)  and the recently proposed behavior transformer (BeT) . Lastly, we compare against mixture of experts trained using expectation maximization (EM)  and backpropagation (MDN)  and the ML-Cur algorithm . We extensively tune the hyperparameters of the baselines using Bayesian optimization  on all experiments.

Figure 1: **Behavior learning environments: Visualization of the obstacle avoidance task (left), the block pushing task (middle), and the table tennis task (right).**

[MISSING_PAGE_FAIL:7]

enhancing interpretability, as it ensures \(()\). An entropy value of 0 signifies a policy that consistently executes the same behavior, while an entropy value of 1 represents a diverse policy that executes all behaviors with equal probability and hence matches the true behavior distribution by design of the data collection process. The results are shown in Table 1. Additionally, we provide a visualization of the learned curriculum in Figure 3. Further details are provided in Appendix C.1.1.

### Block Pushing

The block pushing environment is visualized in Figure 1 (middle) and uses the setup explained in Section 5.1. The robot manipulator is tasked to push blocks into target zones. Having two blocks and target zones amounts to four different push sequences (see e.g. Figure 8), each of which we define as a different behavior \(\). Using a gamepad, we recorded \(500\) demonstrations for each of the four push sequences with randomly sampled initial block configurations \(_{0}\) (i.e., initial positions and orientations), amounting to a total of \(463\)k \((,)\) pairs. The observations \(^{16}\) contain information about the robot's state and the block configurations. The actions \(^{2}\) represent the desired position of the robot. We evaluate the models using three different metrics: Firstly, the _success rate_ which is the proportion of trajectories that manage to push both boxes to the target zones. Secondly, the _entropy_, which is computed over different push sequences \(\). Since high entropy values can be achieved by following different behaviors for different initial block configurations \(_{0}\) in a deterministic fashion, the entropy of \(p()\) can be a poor metric for quantifying diversity. Hence, we evaluate the expected entropy conditioned on the initial state \(_{0}\), i.e., \(_{p(_{0})}[(|_{0})] -}_{_{0} p(_{0})}_{ }p(|_{0})_{4}p(|_{0})\). If, for the same \(_{0}\), all behaviors can be achieved, the expected entropy is high. In contrast, the entropy is 0 if the same behavior is executed for the same \(_{0}\). Here, \(p(_{0})\) and \(N_{0}\) denote the distribution over initial block configurations and the number of samples respectively. See Appendix C.1.2 for more details. Lastly, we quantify the performance on non-successful trajectories, via _distance error_, i.e., the distance from the blocks to the target zones at the end of a trajectory. The success rate and distance error indicate whether a model is able to avoid averaging over different behaviors. The entropy assesses the ability to represent multimodal data distributions by completing different push sequences. The results are reported in Table 1 and generated by simulating \(16\) evaluation trajectories for \(30\) different initial block configurations \(_{0}\) per seed. The difficulty of the task is reflected by the low success rates of most models. Besides being a challenging manipulation task, the high task complexity is caused by having various sources of multimodality in the data distribution: First, the inherent versatility in human behavior. Second, multiple human demonstrators, and lastly different push sequences for the same block configuration.

### Franka Kitchen

The Franka kitchen environment was introduced in  and uses a seven DoF Franka Emika Panda robot with a two DoF gripper to interact with a simulated kitchen environment. The corresponding dataset contains \(566\) human-collected trajectories recorded using a virtual reality setup amounting to \(128\)k \((,)\) pairs. Each trajectory executes a sequence completing four out of seven different tasks. The observations \(^{30}\) contain information about the position and orientation of the task-relevant objects in the environment. The actions \(^{9}\) represent the control signals for

Figure 3: **Curriculum Visualization: Visualization of the curricula \(p(,|z)\) for a different number of components \(N_{z}\) on the obstacle avoidance task. The color indicates different components \(z\) and the size of the dots is proportional to \(p(_{n},_{n}|z)\). For \(N_{z}=1\) we observe that the model ignores most samples in the dataset as a single expert is not able to achieve high log-likelihood values \(p_{_{z}}\) on all samples (Figure 3a). Adding more components to the model results in higher coverage of samples as shown in Figure 3b-3d. Figure 3e visualizes all samples contained in the dataset.**

the robot and gripper. To assess a model's ability to avoid mode averaging we again use the _success rate_ over the number of tasks solved within one trajectory. For each number of solved tasks \(\{1,2,3,4\}\), we define a behavior \(\) as the order in which the task is completed and use the entropy \(()=-_{}p() p()\) to quantify diversity. The results are shown in Figure 5 and are generated using \(100\) evaluation trajectories for each seed. There are no results reported for IBC, MDN and ML-Cur as we did not manage to obtain reasonable results.

### Table Tennis

The table tennis environment is visualized in Figure 1 (right) and consists of a seven DOF robot arm equipped with a table tennis racket and is simulated using the MuJoCo physics engine. The goal is to return the ball to varying target positions after it is launched from a randomized initial position. Although not collected by human experts, the \(5000\) demonstrations are generated using a reinforcement learning (RL) agent that is optimized for highly multimodal behavior such as backhand and forehand strokes . Each demonstration consists of an observation \(^{4}\) defining the initial and target ball position. Movement primitives (MPs)  are used to describe the joint space trajectories of the robot manipulator using two basis functions per joint and thus \(^{14}\). We evaluate the model performance using the _success rate_, that is, how frequently the ball is returned to the other side. Moreover, we employ the _distance error_, i.e., the Euclidean distance from the landing position of the ball to the target position. Both metrics reflect if a model is able to avoid averaging over different movements. For this experiment, we do not report the entropy as we do not know the various behaviors executed by the RL agent. The results are shown in Table 1 and are generated using \(500\) different initial and target positions. Note that the reinforcement learning agent used to generate the data achieves an average success rate of \(0.91\) and a distance error of \(0.14\) which is closely followed by IMC.

### Ablation Studies

Additionally, we compare the performance of IMC with EM for a varying number of components on the obstacle avoidance and table tennis task. The results are shown in Figure 4 and highlight the properties of the moment and information projection: Using limited model complexity, e.g. \(1\) or \(5\) components, EM suffers from mode averaging, resulting in poor performances (Figure 4(a) and Figure 4(b)). This is further illustrated in Figure 4(c). In contrast, the zero forcing property of the information projection allows IMC to avoid mode averaging (see Figure 4(d)) which is reflected in the success rates and distance error for a small number of components. The performance gap between EM and IMC for high model complexities suggests that EM still suffers from averaging problems. Moreover, the results show that IMC needs fewer components to achieve the same performance as EM.

Figure 4: **Franka Kitchen:** Performance comparison between various policy learning algorithms, evaluating the success rate and entropy for a varying number of task completions. IMC is able to outperform the baselines in both metrics, achieving a high success rate while showing a higher diversity in the task sequences. Performance comparison between various policy learning algorithms. IMC is more successful in completing tasks (success rate) while at the same time having the highest diversity in the sequence of task completions (entropy).

## 6 Conclusion

We presented _Information Maximizing Curriculum_ (IMC), a novel approach for conditional density estimation, specifically designed to address mode-averaging issues commonly encountered when using maximum likelihood-based optimization in the context of multimodal density estimation. IMC's focus on mitigating mode-averaging is particularly relevant in imitation learning from human demonstrations, where the data distribution is often highly multimodal due to the diverse and versatile nature of human behavior.

IMC uses a curriculum to assign weights to the training data allowing the policy to focus on samples it can represent, effectively mitigating the mode-averaging problem. We extended our approach to a mixture of experts (MoE) policy, where each mixture component selects its own subset of the training data for learning, allowing the model to imitate the rich and versatile behavior present in the demonstration data.

Our experimental results demonstrate the superior performance of our method compared to state-of-the-art policy learning algorithms and mixture of experts (MoE) policies trained using competing optimization algorithms. Specifically, on complex multimodal simulated control tasks with data collected from human demonstrators, our method exhibits the ability to effectively address two key challenges: _i)_ avoiding mode averaging and _ii)_ covering all modes present in the data distribution.

**Limitations.** While our current approach achieves state-of-the-art performance, there are still areas for improvement in parameterizing our model. Presently, we employ simple multilayer perceptrons to parameterize the expert policies. However, incorporating image observations would require a convolutional neural network (CNN)  backbone. Additionally, our current model relies on the Markov assumption, but relaxing this assumption and adopting history-based models like transformers  could potentially yield significant performance improvements. Lastly, although this work primarily concentrates on continuous domains, an intriguing prospect for future research would be to explore the application of IMC in discrete domains.

**Broader Impact.** Improving imitation learning algorithms holds the potential to enhance the accessibility of robotic systems in real-world applications, with both positive and negative implications. We acknowledge that identifying and addressing any potential adverse effects resulting from the deployment of these robotic systems is a crucial responsibility that falls on sovereign governments.

Figure 5: **Obstacle Avoidance:** IMC and EM improve the success rate and entropy with an increasing number of components (a). For a small number of components, IMC archives a high success rate, as it allows the policy to focus on data that it can represent. In contrast, the policy trained with EM fails as it is forced to cover the whole data. This is visualized in the end-effector trajectories (c + d). Similar observations can be made for **Table Tennis:** Both performance metrics increase with a higher number of components. IMC manages to achieve good performance with a small number of components.