# Geometry-aware training of factorized layers in tensor Tucker format

Emanuele Zangrando

School of Mathematics,

Gran Sasso Science Institute,

L'Aquila, Italy

emanuele.zangrando@gssi.it

&Steffen Schotthofer

Computer Science and Mathematics Division,

Oak Ridge National Laboratory,

Oak Ridge, TN, USA

schotthoefers@ornl.gov

Equal Contribution

Gianluca Ceruti

Department of Mathematics,

University of Innsbruck,

Innsbruck, Austria

gianluca.ceruti@uibk.ac.at

&Jonas Kusch

Department of Data Science,

Norwegian University of Life Sciences,

As, Norway

jonas.kusch@nmbu.no

Francesco Tudisco

School of Mathematics and Maxwell Institute,

University of Edinburgh, Edinburgh, UK;

School of Mathematics,

Gran Sasso Science Institute, L'Aquila, Italy

f.tudisco@ed.ac.uk

###### Abstract

Reducing parameter redundancies in neural network architectures is crucial for achieving feasible computational and memory requirements during training and inference phases. Given its easy implementation and flexibility, one promising approach is layer factorization, which reshapes weight tensors into a matrix format and parameterizes them as the product of two small rank matrices. However, this approach typically requires an initial full-model warm-up phase, prior knowledge of a feasible rank, and it is sensitive to parameter initialization. In this work, we introduce a novel approach to train the factors of a Tucker decomposition of the weight tensors. Our training proposal proves to be optimal in locally approximating the original unfactorized dynamics independently of the initialization. Furthermore, the rank of each mode is dynamically updated during training. We provide a theoretical analysis of the algorithm, showing convergence, approximation and local descent guarantees. The method's performance is further illustrated through a variety of experiments, showing remarkable training compression rates and comparable or even better performance than the full baseline and alternative layer factorization strategies.

## 1 Introduction

The memory footprint and computational cost for inference and training are major limitations of modern deep learning architectures. A variety of techniques have been developed to address this issue, aiming to reduce model size and computational complexity. Popular approaches includeweight sparsification  and quantization . However, pruning via sparsification struggles to take advantage of GPU hardware designed for dense matrices, and it is difficult to provide error estimates on model performance when using quantization . Moreover, while able to reduce resource requirements for inference, these methods struggle to achieve memory reduction during training without affecting performance. As pointed out in , training accurate sparse or quantized neural networks from the start is particularly challenging. At the same time, the training phase of modern architectures can require several days on several hundreds of GPUs , requiring huge memory storage and energy consumption overheads. Thus, being able to reduce the resource demand of both inference and training while maintaining model performance is of critical importance.

Along with sparsification and quantization, layer factorization is another popular and successful model compression approach. Representing layer weights using different types of matrix and tensor factorizations can yield huge memory reduction while retaining model performance and robustness . A wealth of recent work has shown theoretical and experimental evidence suggesting that layer weights of over-parametrized networks tend to be low-rank  and that removing small singular values may even lead to increased model performance while dramatically reducing model size . One significant advantage of low-rank factorizations is that a low-parametric factorized model can be used throughout the entire training  and fine-tuning phase . The most direct way of doing this is by representing each layer's weight tensor \(W\) as the product of small factors and then directly training each factor independently in a block-coordinate descent. In the matrix case, this boils down to imposing the rank-\(r\) parametrization \(W=UV^{}\) for each layer \(W\), with \(U,V\) rectangular matrices with only \(r\) columns. The network is then trained on the set of rank-\(r\) matrices \(_{r}=\{UV^{}:U,V\) have \(r\) columns\(\}\), interpreting the loss as a function of \(U,V\) alone. This is the approach taken also by popular fine-tuning strategies such as LoRA . When \(W\) is a higher-dimensional tensor, as in the case of convolutional kernels for example, the same approach can be implemented using different types of tensor low-rank factorizations, including canonical polyadic (CP) and Tucker formats . While direct training of a layer's factors is widely used in deep learning, this approach has two major disadvantages:

1. The rank \(r\) of the factorization needs to be chosen a-priori, and the performance of the compressed model can highly depend on it ;
2. The training flow is highly sensitive with respect to the choice of the initialization, which may result in a high-oscillatory slow-converging loss and sub-optimal performance and may require a warm-up phase during which the full model is trained prior to the rank reduction .

Point 2, in particular, is directly related to the geometry of the constraints set. In the matrix case, it is well-known that \(_{r}\) is a Riemannian manifold with points of very high curvature near small singular values . These points give rise to stiffness and result in ill-conditioning. This intrinsic poor conditioning can be overcome by projecting the gradient flow on the tangent bundle of \(_{r}\) as presented in .

In the higher-dimensional tensor case, we face the same problems. However, trying to adapt the approach for matrices to other tensor factorizations is not trivial, as it may lead to prohibitive computational costs and memory requirements scaling with the order of the tensor. Moreover, not all the tensor factorizations have the required Riemannian structure to design projections and tangent planes. This paper introduces a rank-adaptive geometry-aware training algorithm that trains factorized tensor layers in Tucker format taking advantage of the underlying Riemannian structure, yielding strictly better performance than direct factorizations and overcoming both Points 1 and 2 above. Our main contributions are:

* We design an algorithm for training tensor layers in Tucker format that is **rank-adaptive**, as the ranks of the layers are dynamically updated during training to match a desired compression rate;
* We provide theoretical guarantees of loss **descent**, **convergence** to stationary points in expectation, and **approximation** to the ideal full model;
* We provide extensive **experimental evaluation** showing that the proposed method yields remarkable training compression rates (e.g., more than \(95\%\) for VGG16 on CIFAR10), while achieving comparable or even better performance than the full baseline and alternative baselines.

### Related work

Related work on network compression methods differs structurally by the mathematical object of consideration, i.e., matrix- or tensor-valued parameter structures, as well as the type of parameter reduction. Weight pruning [28; 61; 73; 59; 81] enables parameter reduction by enforcing sparsity, i.e., zero-valued weights, whereas low-rank compression imposes parameter reduction by factorization of weight matrices [34; 49; 79; 66; 84] and tensors [44; 71; 4; 63; 42; 38; 42; 35; 42]. On top of approaches that transform tensor layers into compressed matrices [68; 34; 49; 79], different tensor decompositions have been used to compress convolutional layers. Such approaches include CP decomposition [44; 71; 4; 63], Tucker [42; 38], tensor trains [42; 72] or a combination of these . Other works focus on performing efficient optimization on Stiefel manifolds to preserve orthonormality, with methods based on regularization or landing [57; 8; 13; 1], cheap parametrizations of orthogonal groups [46; 48] and Riemannian schemes [58; 67; 2]. Other methods consider only the floating point representation of the weights, e.g. [75; 25; 27; 14; 76], or a combination of the above . From the algorithmic point of view, related work can be categorized into methods that compress networks entirely in a postprocessing step after full-scale training [60; 56; 44; 38; 21; 4], iterative methods where networks are pre-trained and subsequently compressed and fine-tuned [28; 34; 79], and methods that directly compress networks during training [68; 61]. As no full-scale training is needed, the latter approach offers a better potential reduction of the overall computational footprint. Only a few of these methods propose strategies for dynamically choosing the compression format during training or fine-tuning, e.g., by finding the ranks via alternating, constraint optimization in discrete  and discrete-continuous fashions . However, both these approaches require knowledge of the full weights during training and overall are more computationally demanding than standard training. In , a rank-adaptive evolution of the gradient flow on a low-rank manifold was proposed to train and compress networks without using the full-weight representation; however, only for matrix-valued layers. While a direct extension of this method to Tucker tensors is possible, the resulting algorithm exhibits a prohibitive memory footprint and computational complexity. The development of rank-adaptive training methods for tensor-valued layers poses non-trivial challenges that may prevent loss descent and performance of the compressed net. For example, numerical instabilities arising from the CP decomposition during training have been observed in  and .

## 2 Geometry-aware training in Tucker format

For a tensor \(W\), we write \(_{i}(W)\) to denote the matrix obtained by unfolding \(W\) along its \(i\)-th mode. The tuple \(=(r_{1},r_{2},,r_{d})\) is called Tucker rank of \(W\) if \(r_{i}=(_{i}(W))\). Every \(d\)-order tensor \(W\) with Tucker rank \(=(r_{1},,r_{d})\) can be written in Tucker form (or Tucker decomposition) \(W=C_{i=1}^{d}U_{i}\), entry-wise defined as

\[W(i_{1},,i_{d})=_{_{1},,_{d}=1}^{r_{1},,r_{ d}}C(_{1},,_{d})U_{1}(i_{1},_{1}) U_{d}(i_{d}, _{d})\]

where \(C^{r_{1} r_{d}}\) is a _core tensor_ of full Tucker rank \(=(r_{1},,r_{d})\) and the \(U_{i}^{n_{i} r_{i}}\) are matrices with orthonormal columns. From this representation, we immediately see that if \(W\) is represented in Tucker format, then the cost of storing \(W\) and of performing linear operations with \(W\) (e.g. matvecs or convolutions) is \(O(r_{1} r_{d}+n_{1}r_{1}++n_{d}r_{d})\), as opposed to the \(O(n_{1} n_{d})\) cost required by the standard full representation. When \(n_{i} r_{i}\), e.g., \(n_{i}>1.5r_{i}\), the latter is much larger than the former.

In the following, we develop a rank-adaptive algorithm that trains layers in Tucker form in a robust and efficient manner. Our derivation follows five points:

1. We introduce the dynamical low-rank approximation framework in Section 2.1, which provides gradient flows for layers in Tucker format. However, the direct use of these evolution equations to train the network will require prohibitively small learning rates due to the high curvature of the manifold of Tucker tensors.
2. We introduce a reparameterization in Theorem 2.1 that allows us to formulate robust dynamics for the reparametrized factors.
3. By integrating numerically the resulting gradient system (with e.g. SGD as explicit Euler) along with a basis augmentation step, we propose a geometry-aware rank-adaptive training strategy forthe network in Tucker format. This approach, however, requires \(d+1\) forward and backward evaluations of the network, resulting in significantly increased computational costs.
4. We show in Corollary 2.2 that computational costs can be substantially reduced by noting that the computation of the augmented basis can largely be simplified. This leads to our proposed training scheme in Algorithm 1. The algorithm is equivalent to the integration of the gradient system but requires only two instead of \(d+1\) gradient evaluations.
5. Due to its equivalence to the approach constructed in point 3, we can show that Algorithm 1 indirectly updates weights along straight lines on the manifold, thus leading to three main theoretical properties: loss descent (Theorem 3.1), convergence in expectation (Theorem 3.2), and a robust bound showing approximation of the full model (Theorem 3.3).

### Dynamical low-rank approximation

For \(=(r_{1},,r_{d})\), the set

\[_{}=\{W:(_{i}(W))=r_{i},\, i=1,,d\}\]

is a manifold with the following tangent space at any point \(W=C_{i=1}^{d}U_{i}_{}\)

\[T_{W}_{}= C}U_{ i}+}C_{j} U_{j}_{k j}U_{k}:  C^{r_{1} r_{d}},\, U_{j} T_{U_{ j}}_{j}} \]

where \(_{j}\) is the Stiefel manifold of real \(n_{i} r_{i}\) matrices with orthonormal columns. To design a strategy that computes layer weights within \(_{}\) using only the low-rank Tucker factors \(C\) and \(\{_{i}\}_{i}\), we formulate the training problem as a continuous-time gradient flow projected onto the tangent space (1). As shown in Section 3, the continuous formulation will allow us to derive a modified backpropagation pass which uses only the individual small factors \(C,\{U_{i}\}_{i}\) and that does not suffer from a slow convergence rate due to potential ill-conditioned tensor modes (see also Section 4.2).

Let \(f\) be a neural network and let \(W\) be a weight tensor within \(f\). Consider the problem of minimizing the loss function \(\) with respect to just \(W\) while keeping the other parameters fixed. This problem can be equivalently formulated as the gradient flow

\[(t)=-_{W}(W(t)) \]

where, for simplicity, we write the loss as a function of only \(W\) and where "dot" denotes the time derivative. When \(t\), the solution of (2) approaches the desired minimizer. Now, suppose we parametrize each tensor layer in a time-dependent Tucker form \(W(t)=C(t)_{i=1}^{d}U_{i}(t)_{}\). Then \((t) T_{W(t)}_{}\), the tangent space of \(_{}\) at \(W(t)\). Thus, (2) boils down to

\[(t)=-P(W(t))_{W}(W(t)) \]

where \(P(W)\) denotes the orthogonal projection onto \(T_{W}_{}\). Using standard derivations from dynamical model order reduction literature , the projected gradient flow in (3) leads to the following evolution equations for the individual factors \(C(t)\) and \(U_{i}(t)\)

\[&_{i}=-(I-U_{i}U_{i}^{})_{i}_{W}(W)_{j i}U_{j}^{} _{i}(C)^{}\\ &=-_{W}(W)_{j=1}^{d}U_{j}^{} \,, \]

where \(\) denotes the pseudoinverse and where we omitted the dependence on \(t\) for brevity. Even though (4) describes the dynamics of the individual factors, the equations for each factor are not fully decoupled. A direct integration of (4) would still require taping the gradients \(_{W}\) with respect to the full convolutional kernel \(W\). Moreover, the pseudoinverse of the matrices \(_{i}(C)^{}\) adds a stiffness term to the differential equation, making its numerical integration unstable. The presence of this stiff term is actually due to the intrinsic high-curvature of the manifold \(_{}\) and is well understood in the dynamic model order reduction community . As observed in , an analogous term arises when looking at low-rank matrix parameterizations, and it is responsible for the issue of slow convergence of low-rank matrix training methods, which is observed in .

To overcome these issues, we use the following change of variables. Let \(_{i}(C)^{}=Q_{i}S_{i}^{}\) be the QR decomposition of \(_{i}(C)^{}\). Note that \(S_{i}\) is a small square invertible matrix of size \(r_{i} r_{i}\). Then, the matrix \(K_{i}=U_{i}S_{i}\) has the same size as \(U_{i}\) and spans the same vector space. However, the following key result holds for \(K_{i}\).

**Theorem 2.1**.: _Let \(W=C_{i=1}^{d}U_{i}_{}\) be such that (3) holds. Let \(_{i}(C)^{}=Q_{i}S_{i}^{}\) be the QR decomposition of \(_{i}(C)^{}\) and let \(K_{i}=U_{i}S_{i}\). Then,_

\[_{i} =-_{K_{i}}_{i}(Q_{i}^{ })_{j i}U_{j}_{i}K_{i}\,, \] \[ =-_{C}(C_{j=1}^{d}U_{j})\]

_where \(_{i}\) denotes "tensorization along mode \(i\)", i.e. the inverse reshaping operation of \(_{i}\)._

The supplementary material provides the proof in Appendix D. The theorem above allows us to simplify (4), obtaining a gradient flow that only depends on the small matrices \(K_{i}\) and the small core tensor \(C\). Moreover, it eliminates a stiffness term; this added regularity appears reasonable as no inversion is now involved in the differential equations. A rigorous regularity statement can be found in Theorem 3.3. We would like to underline the importance of the careful construction of \(K_{i}\) to arrive at this theorem. A naive extension of  to Tucker tensors can be constructed by a reshaping of \(W\) into matrices \(_{i}(W)=U_{i}S_{i}V_{i}^{}\) with \(S_{i}=_{i}(C)\) and \(V_{i}=_{j i}U_{j}\). Then, \(K_{i}=U_{i}S_{i}\) can be used to update \(U_{i}\) into all directions \(i d\) which directly inherits the robustness properties presented in . However, this construction of \(K\) yields a prohibitive memory footprint of \((n_{i}_{j i}r_{j})\) and computational costs of \(O(n_{i}_{j i}r_{j}^{2})\) rendering the resulting method impractical.

Based on the numerical integration of (5), we propose a robust, memory-efficient, and rank-adaptive method to update the network parameters by using only the core tensor \(C\) and the basis matrices \(K_{i}\). The proposed approach is as follows: Fix an approximation tolerance \(>0\), then, first update the basis matrices performing for all \(i=1,,d\):

1. Form \(K_{i}=U_{i}S_{i}\), where \(S_{i}\) is the square \(r_{i} r_{i}\) matrix from QR decomposition of \(_{i}(C)^{}\)
2. Compute \(K_{i}^{}\) with one step integration of (5) starting from \(K_{i}\)
3. Form the new augmented matrix \(U_{i}^{}\) by orthonormalizing the columns of \([U_{i},K_{i}^{}]\)

Second, update the core tensor and truncate:

1. Lift the core tensor \(=C_{i=1}^{d}(U_{i}^{})^{}U_{i}\) using the new augmented basis matrices
2. Compute \(C^{}\) with one step integration of (5) starting from \(\) using fixed basis matrices \(U_{i}^{}\)
3. Perform a rank adjustment step to the prescribed tolerance by computing the best Tucker approximation of \(C^{}\), i.e. solving the following optimization (rounding) task: Find \(C_{ 2}\) of smallest rank \(^{}=(r_{1}^{},,r_{d}^{})\) such that \(\|C^{}-C\|\|C^{}\|\), (6) where \(=(r_{1},,r_{d})\) and \(_{ 2}\) denotes the set of tensors with component-wise Tucker rank lower than \(2\).

In practice, the final rank adaptive step is done by performing a high-order SVD (HOSVD)  on \(C^{}\). The parameter \(\) is responsible for the compression rate of the method, as larger values of \(\) yield smaller Tucker ranks and thus higher parameter reduction. The computed tensor \(C_{^{}}\) has the form \(C=C^{}_{i=1}^{d}U_{i}^{}_{^{}}\) and the computed \(U_{i}^{}^{2r_{i} r_{i}^{}}\) with \(r_{i}^{} 2r_{i}\) are then pulled back to the initial dimension by the change of basis \(U_{i}=U_{i}^{}U_{i}^{}^{n_{i} r_{i}^{}}\), and the new core tensor \(C\) is then assigned \(C^{}\). This implementation, however, comes at the expense of evaluating the network and gradient tapes \(d+1\) times for an order \(d\) tensor.

The next key result will overcome this issue and will allow us to reduce the necessary network and gradient tape evaluations to two.

**Corollary 2.2**.: _Let \(W=C_{i=1}^{d}U_{i}_{}\) be such that (6) holds. Let \(_{i}(C)^{}=Q_{i}S_{i}^{}\) be the QR decomposition of \(_{i}(C)^{}\) and let \(K_{i}=U_{i}S_{i}\). Then,_

\[([U_{i},_{i}])=([U_{i},_{U_{i}}W]).\]

The supplementary material provides the proof in Appendix E. Using Corollary 2.2, one can replace the individual forward evaluation and descend steps for \(K_{i}\) by a single backpropagation. All available new information is given by the gradients \(_{U_{i}}\), which can be evaluated from the same tape.

Combining the above strategy with Corollary 2.2 we obtain Algorithm 1: an efficient rank-adaptive geometry-aware training method for tensor in Tucker format. Note that without the explicit computation of \(K_{i}=U_{i}S_{i}\) we compute all basis gradients \(_{U_{i}}\) in a single network evaluation and we use \(_{U_{i}}\) to augment the basis. Note that stochastic gradient evaluations can be done in practice and thatmomentum methods are applicable for the descent step on line 5 of Algorithm 1. In case the rank decreases after the retraction step, we only use the corresponding subset of the old basis functions to form the momentum term. In case of rank increase, the momentum term of the new basis vectors is initialized as zero. As a side note, the rank truncation proposed in Algorithm 1 allows to maintain the nice theoretical guarantees of the algorithm, but in practice any tensor-completion/factorization method such as  could be used for this step.

```
Input : Initial low-rank factors \(C r_{1} r_{d}\); \(U_{i} n_{i} r_{i}\); adaptive: Boolean flag that decides whether or not to dynamically update the ranks; \(\): singular value threshold for the adaptive procedure.
1\(G_{i}_{U_{i}}(C_{i=1}^{d}U_{i})\) /* Single-sweep grad evaluation */
2foreach mode \(i\)do
3\(U_{i}^{}\), \(\) QR\(([U_{i},G_{i}])\) /* Augmentation and orthonormalization */
4\( C_{i=1}^{d}(U_{i}^{})^{}U_{i}\) /* Projection of Tucker core onto new basis */
5\(C\) descent step with direction \(_{C}_{i=1}^{d}U_{i}^{} \)
6\((C,U_{1},,U_{d})\) Tucker decomposition of \(C\) up to relative error \(\) as in (6)
7\(U_{i} U_{i}^{}U_{i}\), for \(i=1,,d\) /* Rank adjustment */
```

**Algorithm 1**TDLRT: Efficient Tensor Dynamical Low-Rank Training in Tucker format.

### Computational Complexity

The computational costs for the full network training come from back and forward passes through each layer. For a layer with weight tensor \(W^{n_{1} n_{d}}\), they require \((b_{i}n_{i})\) operations, where \(b\) is the batch size. When using TDLRT, these computational costs reduce to \((b_{i}r_{i}+b_{i}n_{i}r_{i})\) operations, yielding a significant reduction in computational costs to determine the gradient. However, performing low-rank updates also adds computational costs due to several factorizations. Here, the QR and SVD on \(_{i}(C)\), which are needed in the updates of \(U_{i}\) and the truncation step, require \((_{i}r_{i}_{j}r_{j})\), and the QR on \(K_{i}\) requires \((_{i}n_{i}r_{i}^{2})\) operations. Hence, in total, we have for every layer a cost of \((b_{i}r_{i}+b_{i}n_{i}r_{i}+_{i=1}^{d}(n_{i}r_{i}^{2}+r _{i}_{j=1}^{d}r_{j}))\) operations for TDLRT, vs. the \((b_{i}n_{i})\) required by the full baseline. Thus, TDLRT scales linearly with the dimensions \(n_{i}\), and for \(r_{i} n_{i}\), which is typically the case, see Appendix C.2, it has advantageous computational cost.

## 3 Convergence and approximation analysis

In this section, we present our main theoretical results. First, we show Algorithm 1 guarantees descent of the training loss, provided the compression tolerance is not too large. Second, we show that when Algorithm 1 is implemented with SGD with a decaying learning rate, the method converges to a stationary point in expectation. Third, we prove that the compressed network computed via the rank-adaptive TDLRT scheme in Algorithm 1 well-approximates the full model that one would obtain by standard training, provided the gradient flow of the loss is, at each step, approximately low-rank. The latter result shows that if a high-performing subnetwork of low Tucker rank exists, then the proposed TLDRT will probably approximate it. For brevity, some statements here are formulated informally, and all proofs and details are deferred to Appendix F in the SM.

Suppose that for each convolution \(W\), the gradient \(_{W}\), as a function of \(W\), is locally bounded and Lipschitz, i.e.,\(\|_{W}(Y)\| L_{1}\) and \(\|_{W}(Y_{1})-_{W}(Y_{2})\| L_{2}\|Y_{1 }-Y_{2}\|\) around \(W\). Then,

**Theorem 3.1** (Descent).: _Let \(W()=C_{j=1}^{d}U_{j}\) be the Tucker low-rank tensor obtained after one training iteration using Algorithm 1 and let \(W(0)\) be the previous point. Assuming the one-step integration from \(0\) to \(\) is done exactly, it holds \(_{W}(W())_{W}(W(0))-+\), where \(,>0\) are constants independent of \(\) and \(\), and where \(_{W}\) denotes the loss as a function of only \(W\)._

We now prove that the rank-adaptive training method in Algorithm 1 converges to a stationary point in expectation if implemented with SGD and decaying learning rate.

**Theorem 3.2** (Convergence).: _Denote with \((t)\) is the weight tensor after \(t\) passes of Algorithm 1 before the rank truncation step, and \(W(t)\) the one obtained after the rank truncation. Assume Algorithm 1 is implemented using SGD as a descent method with learning rate sequence \(\{_{t}\}\) satisfying the Robbins-Monro conditions:_

\[_{t}_{t}=+_{t}_{t}^{2}<+\,.\]

_Suppose also that the spectral distribution stabilizes fast enough over time, i.e.,_

\[_{t 0}[\|(t)-W(t)\|]<+\]

_and that the projected stochastic gradient has a controlled drift, namely_

\[\|(t-1)_{j}P_{_{j}(t)}\| ^{2}\,|\,t-1+\|(t-1)_{j}P_{U_{j}(t-1 )}\|^{2}\]

_for some \(,>0\), where \(P_{U}\) is the orthogonal projection onto the range of \(U\). Then, the following convergence condition holds_

\[_{t}\|(t-1)_{j}P_{U_{j}(t-1 )}\|^{2}=0\]

Details of the proof are contained in the appendix.

**Theorem 3.3**.: _For an integer \(k\), let \(t=k\), and let \(W(t)\) be the full convolutional kernel, solution of (2) at time \(t\). Let \(C(t),\{U_{i}(t)\}_{i}\) be the Tucker core and factors computed after \(k\) training steps with Algorithm 2, where the one-step integration from \(0\) to \(\) is done exactly. Finally, assume that for any \(Y\) in a neighborhood of \(W(t)\), the gradient flow \(-_{W}(Y)\) is "\(\)-close" to \(T_{Y}_{}\). Then,_

\[\|W(t)-C(t)_{j=1}^{d}U_{j}(t)\| c_{1}+c_{2}+c_{3} / \]

_where the constants \(c_{1}\), \(c_{2}\), \(c_{3}\) depend only on \(L_{1}\) and \(L_{2}\)._

In particular, both bounds in the above theorems do not depend on the higher-order singular values of the exact nor the approximate solution, which shows that the method does not suffer instability and slow convergence rate due to potential ill-conditioning (small higher-order singular values). Note that this result is crucial for efficient training on the low-rank manifold and is not shared by direct gradient descent training approaches, as we will numerically demonstrate in the following section. Moreover, we emphasize that (7) provides a sufficient condition for the computation of a high-performing subnetwork. In fact, for smooth enough network models \(f\), condition (7) implies that \(f(W(t)) f(C(t)_{j=1}^{d}U_{j}(t))\), i.e. the computed subnetwork approximates the full model.

## 4 Experiments

In the following, we conduct a series of experiments to evaluate the performance of the proposed method as compared to the full model and to standard layer factorization and model pruning baselines. The full baseline is the network trained via standard implementation. In order to test the method on tensor layers, we consider here convolutional networks and apply the decomposition to the

Figure 1: Comparison of compression performance for different models against the full baseline for the Cifar10 (a-c) and Tiny-Imagenet (d) benchmark. The mean accuracy of \(20\) weight initializations is displayed. TDLRT achieves higher compression rates at higher accuracy with lower variance between initializations.

convolutional kernel \(W\). In terms of layer factorization, we compare against different baseline approaches: direct training of the factors in the low-rank matrix factorization format [34; 49; 79; 36], the low-rank tensor Canonic-Polyadic format [44; 71; 4; 63], the low-rank tensor Tucker format [42; 38], and the low-rank Tensor-Train format . The convolution operation can then be written completely in terms of the small factors using the factorization ansatz. While the above papers propose different initialization and rank selection strategies, all the referenced literature trains the factors in the chosen layer factorization format by implementing forward and backward propagations simultaneously and independently on each factor in a block-coordinate fashion. This way of training on the low-rank manifold ignores the geometry of the manifold, whereas TDLRT directly exploits the underlying geometry to avoid points of high curvature. We compare the training strategy alone. Thus, we ignore any model-specific initialization and regularization addition. We also compare with Riemannian gradient descent (RGD) for tensors in Tucker format implemented using the HOSVD retraction [74; 17] and with the matrix dynamical training algorithm , where the standard forward and backward passes are endowed with a rank-adaptive QR projection step, similar to the proposed Algorithm 1. In terms of pruning techniques based on sparsification, we compare with methods from two of the most popular strategies: iterative magnitude pruning (IMP) , and single-shot pruning at initialization, single-shot network pruning (SNIP)  and Gradient Signal Preservation (GraSP) . The experiments are performed on an Nvidia RTX3090, Nvidia RTX3070 and one Nvidia A100 80GB. The code is available in the supplementary material.

### Compression Performance

The compression performance of TDLRT is evaluated on CIFAR10 and tiny-imagenet. The typical data augmentation procedure is employed for this dataset: a composition of standardization, random cropping, and a random horizontal flip. All methods are trained using a batch size of \(128\) for \(70\) epochs each, as done in [79; 36]. All the baseline methods are trained with the SGD optimizer; the starting learning rate of \(0.05\) is reduced by a factor of \(10\) on plateaus, and momentum is chosen as \(0.1\) for all layers. The rank \(\) of each tensor mode for the fixed-rank baseline methods is determined by a parameter \(\), i.e., we set \(= r_{}\). The proposed TDLRT method employs Algorithm 2, where SGD is used for the descent steps at lines 4 and 9, with momentum and learning rate as above. Dynamic compression during training is governed by the singular value threshold \(\), see Equation (6).

Figure 1 (a-c) shows the mean accuracy of TDLRT as compared to competing factorization baselines. TDLRT achieves higher compression rates at higher accuracy with lower variance between weight initializations than the competing methods. In the case of the VGG16 benchmark, TDLRT is able to maintain baseline accuracy for compression rates over \(90\%\) and exceeds the baseline on average for \(=0.03\), i.e., \(95.3\%\) compression. Alexnet has \(16.8\%\) of the parameters of VGG16. Thus, compression is naturally more challenging to achieve. Nevertheless, TDLRT outperforms the baselines and remains close to the full network performance. Similar behavior is observed on ResNet18.

Table 1 shows a comparison of the best-performing compression between all the factorization-based and pruning-based baseline methods as well as TDLRT in the CIFAR10 benchmark for Alexnet, ResNet18, and VGG16. In the proposed evaluation, TDLRT is on par or outperforms all the alternatives, including pruning based on sparsity (implemented without warmup for the sake of a fair

Figure 2: Left panel: Computational footprint of low-rank convolutions. TDLRT surpasses the baseline performance for meaningful compression rates. Middle panel: Convergence behavior of Lenet5 on MNIST dataset in the case of an initial overestimation of the rank, with exponentially decaying singular values. Mean and standard deviation (shaded area) over \(10\) random initializations. Right panel: Compared to standard Tucker decomposition, with and without rank adaption, wall training time to reach \(60\%\) accuracy for TDLRT.

comparison), Tensor-Train (TT) and Tucker factorization, Riemmanian gradient descend (RGD) for Tucker decompositions, as well as the matrix-valued DLRT, due to the higher flexibility of the Tucker format where compression along each tensor mode individually is possible. The compression rate (c.r.) is computed as \(1-c/f\), where \(c\) is the number of convolutional parameters in the compressed model after training and \(f\) is the number of convolutional parameters of the full model. While this is the compression rate after training, we emphasize that methods based on factorizations yield an analogous compression rate during the entire training process. We also remark that no DLRT version of CP decomposition is shown as CP is not suited for dynamical low-rank training due to its lack of a manifold structure. Similar results are obtained for the tiny-imagenet benchmark, see Fig. 1 (d) and Table 3. The rank evolution of the networks during training is discussed in Appendix C.2.

### Robustness of the Optimization

To further highlight the advantages of Algorithm 2 as compared to standard simultaneous gradient descent on the factors of the decomposition, we show in Figure 2 the accuracy history of LeNet5 on MNIST using TDLRT as compared to standard training on Tucker and CP decompositions. In the case of TDLRT, an optimization step denotes the evaluation of Algorithm 2 for all convolutional layers for one batch of training data, while for the other methods, we refer to a standard SGD batch update for all factors of the tensor decompositions of all layers. All linear layers of the network are trained with a traditional gradient descent update and are not compressed. In this experiment, we initialize the network weights to simulate a scenario where the rank is overestimated. To this end, we employ spectral initialization with singular values decaying exponentially with powers of ten. Integrating the low-rank gradient flow with the TDLRT Algorithm 2 leads to faster and more robust convergence rates of the network training process.

### Computational Performance

The computational performance in inference and training of convolutional layers in Tucker decomposition depends on their current tensor ranks, see Section 2. We evaluate the inference time of \(120K\) RGB images and memory footprint of VGG and AlexNet in Tucker factorization as used in Algorithm 2 and compare them to the non-factorized baseline models in Figure 2. As a result, for realistic compression rates, see also Figure 1, the computational footprint of TDLRT is significantly lower than the corresponding baseline model.

Rank adaptive TDLRT training comes at the additional expense of QR and SVD operations per optimization step compared to standard fixed rank training without orthonormalization. However, the increased robustness of the optimization and faster convergence reduces the computational overhead of TDLRT as demonstrated in Figure 2. In order to provide a fair comparison between the methods, we report the time to achieve \(60\%\) accuracy target at a compression rate of \(90\%\) for all methods, on

    & & &  &  &  \\   & & test acc. [\%] & c.r. [\%] & test acc. [\%] & c.r. [\%] & test acc. [\%] & c.r. [\%] \\   & Baseline & \(92.01\) & \(0.0\) & \(85.46\) & \(0.0\) & \(94.33\) & \(0.0\) \\   & **TDLRT** (ours) & **90.23** & **94.40** & **82.39** & 83.12 & **92.72** & \(78.73\) \\  & Matrix DLRT  & \(89.13\) & \(83.22\) & \(73.57\) & \(71.57\) & \(80.98\) & \(56.85\) \\  & Tucker-factorized  & \(86.71\) & \(91.4\) & \(70.30\) & \(69.74\) & \(91.11\) & \(74.19\) \\  & Matrix-factorized  & \(84.54\) & \(94.34\) & \(77.07\) & \(68.20\) & \(92.07\) & \(77.49\) \\  & CP-factorized  & \(82.53\) & \(89.98\) & \(76.14\) & \(71.46\) & \(91.87\) & \(69.95\) \\  & Tucker RGD  & \(81.48\) & \(84.26\) & \(73.88\) & \(74.01\) & **92.76** & \(74.18\) \\  & TT-factorized  & \(87.27\) & \(90.30\) & \(78.13\) & **88.14** & \(87.13\) & \(81.24\) \\   & SNIP  & \(89.58\) & \(56.23\) & – & – & \(89.50\) & \(78.50\) \\  & IMP  & \(87.21\) & \(58.54\) & – & – & \(90.50\) & **82.50** \\  & GraSP  & \(88.50\) & \(77.30\) & – & – & \(89.40\) & \(77.90\) \\   

Table 1: Comparison of the best-performing compression rates for different methods on the CIFAR10 benchmark with Alexnet, VGG16, and ResNet18. For each column, we report first and second best results.

LeNet5 MNIST with the setting as in Section 4.2. We refrain from measuring time to convergence since the standard Tucker decomposition is not able to reach similar accuracy levels at the same compression ratio as TDLRT.

### Fine-tuning with LoRA-like low-rank adapters

In this section, we are presenting another application of our method, namely fine-tuning pre-trained models through the use of low-rank adapters. In particular, our approach Algorithm 1 is completely agnostic between model compression and adaptation, the approach is the same. More precisely, given a pre-trained model \(f_{W^{*}}\) with tensor pre-trained weights \(W^{*}\), it is possible to add a low-rank Tucker correction \(W\). Then, given a task loss \(L(f)\), by defining \((W)=L(f_{W^{*}+W})\) we report ourselves to the original formulation of the problem. Moreover, we would like to stress that this approach can be applied also to matrices, which are a particular case of tensors with \(d=2\) modes. To showcase these two settings, we present two different settings in which we test our method. In Table 2 (left) we show the fine-tuning of Deberta V3  on the GLUE benchmark . In this test case here, the low-rank adapters have been applied to all matrices in attention layers, and the final performance against LoRA  fine-tuning is reported. Apart from our additional hyperparameter \(\), all the other hyperparameters had been kept as reported in .

In Table 2 (right), we report the results for fine-tuning stable diffusion  with Dreambooth . To adapt the model, we applied Tucker tensor corrections to each convolution of the Unet, and a matrix adapter to each attention layer of the text encoder network. We applied our method on all these correctors, while keeping the same hyperparameters setting as in . We would like to observe that the kind of adapters they propose for convolutions consist in a matrix factorization of a reshaping of the convolutional tensor. This results in a potentially bigger number of parameters needed, as a matrix factorization would be \(O((Cd_{1}d_{2}+F)r)\) against a \(O(Cr_{1}+Fr_{2}+d_{1}r_{3}+d_{2}r_{4}+r_{1}r_{2}r_{3}r_{4})\) for a plain Tucker factorization, where \(F\) is the number of output features, \(C\) is the number of input channels, \(d_{1}\) and \(d_{2}\) are the spatial dimensions of the convolutional kernel. This observation is in fact reflected in the numbers in Table 2, in which we can observe a higher compression potential for tensor decompositions compared to matrix ones.

## 5 Discussion and limitations

This work leverages the geometry of the Tucker tensor factorization manifolds to construct a robust and efficient training algorithm for neural networks in compressed Tucker format. The proposed method has a theoretical backbone of approximation error bounds to the full model and guarantees of loss descent and convergence to stationary points in expectation. The method is superior to standard factorization approaches with alternating or simultaneous gradient descent, as demonstrated in the compression-to-accuracy ratio for various benchmarks and models. The method provides a significant reduction of hyperparameters to a single parameter \(\). This parameter has a clear interpretation as compression rate per layer depending on the layer's importance on the overall network training dynamics. We, however, note that further strategies to pick an adequate \(\) are possible and remain to be investigated. Further, we note that an efficient implementation on GPUs requires an efficient tensor rounding algorithm in Algorithm 1. Finally, the proposed method assumes well-performing low-rank Tucker sub-nets exist in the reference network. While we observe this empirically, further investigations are required to provide theoretical evidence supporting this assumption, similar to the case of fully-connected linear layers, see .

  
**GLUE** & **LoRA** & **TDRLT(Ours)** \\ \# params & 1.33M (rank 8) & 0.9M (\(=0.15\)) \\  CoLa (Corr.) & \(0.6759\) & \(0.7065\) \\ MRPC (Acc.) & \(0.8971\) & \(0.9052\) \\ QQP (Acc.) & \(0.9131\) & \(0.9215\) \\ RTE (Acc.) & \(0.8535\) & \(0.8713\) \\ SST2 (Acc.) & \(0.9484\) & \(0.9594\) \\   

Table 2: Fine-tuning performance metrics on Deberta V3 Glue benchmark (left) and on Stable diffusion Dreambooth (right).