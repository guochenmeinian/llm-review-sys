# STONE: A Submodular Optimization Framework for Active 3D Object Detection

Ruiyu Mao  Sarthak Kumar Maharana  Rishabh K Iyer  Yunhui Guo

Department of Computer Science

The University of Texas at Dallas, Richardson, TX, USA

{rxm210041, skm200005, rishabh.iyer, yunhui.guo}@utdallas.edu

###### Abstract

3D object detection is fundamentally important for various emerging applications, including autonomous driving and robotics. A key requirement for training an accurate 3D object detector is the availability of a large amount of LiDAR-based point cloud data. Unfortunately, labeling point cloud data is extremely challenging, as accurate 3D bounding boxes and semantic labels are required for each potential object. This paper proposes a unified active 3D object detection framework, for greatly reducing the labeling cost of training 3D object detectors. Our framework is based on a novel formulation of submodular optimization, specifically tailored to the problem of active 3D object detection. In particular, we address two fundamental challenges associated with active 3D object detection: data imbalance and the need to cover the distribution of the data, including LiDAR-based point cloud data of varying difficulty levels. Extensive experiments demonstrate that our method achieves state-of-the-art performance with high computational efficiency compared to existing active learning methods. The code is available at [https://github.com/RuiyuM/STONE](https://github.com/RuiyuM/STONE)

## 1 Introduction

In many emerging applications such as autonomous driving, it is critical to localize objects in a 3D scene for accurate scene understanding . This is usually achieved by using a 3D detection model based on LiDAR point cloud data with oriented bounding boxes and semantic labels. While highly accurate recognition and localization of objects can be achieved due to recent advancements in deep learning, this performance often comes at the expense of requiring a large volume of labeled point cloud data, which is much more costly to collect compared to typical RGB images .

Active learning (AL) is the standard method for reducing labeling costs in machine learning , AL often starts with a small labeled set and iteratively selects the most informative samples for label acquisition from a large pool of unlabeled data, given a labeling budget. The informativeness, of a selected sample, can be measured in various ways. For example, by computing the sampling uncertainty as measured by maximum Shannon entropy  or estimated model changes or finding the most _representative_ samples to avoid sample redundancy  by using greedy coreset algorithms  or clustering-based approaches .

Active learning has proven highly effective in reducing labeling costs for recognition tasks, but its application in LiDAR-based object detection remains limited and under-explored . Compared to standard recognition tasks, there are two fundamental challenges: **1)** Depending on the specific scene, each category involves different difficulty levels (EASY, MODERATE, or HARD), which are determined by the size, occlusion level, and truncation of 3D objects. _Ideally, the selected labeled point cloud data should include varying difficulty levels._ **2)** Each 3D scene can contain multiple objects, leading to highly imbalanced label distributions in the point cloud data. For example, mostpoint clouds include cars, but not cyclists. Addressing data imbalance is thus crucial for selecting informative point clouds to train the 3D object detector.

In a recent study, CRB  proposed three stages to label unlabeled point clouds hierarchically, ensuring they are concise, representative, and geometrically balanced. One of the critical components of CRB is to achieve label balance in individual point clouds. However, it cannot guarantee the label distribution is balanced across all the labeled point clouds. More recently, KECOR  was proposed for characterizing sample informativeness in both classification and regression tasks with a unified measurement. The main idea is to select a subset of point clouds that maximizes the kernel coding rate. This approach enables the selection of representative samples from the unlabeled point cloud. However, it does not address the issue of data imbalance.

In this paper, we propose a **unified** submodular optimization framework, called **STONE**, for active 3D object detection, to address limitations in existing methods. Our framework leverages submodular functions , a classical tool for measuring set quality, due to its well-known property of diminishing returns. Building on these fundamental results, we introduce a novel formulation of submodular optimization for active 3D object detection. This formulation not only ensures that the selected point cloud achieves maximal coverage of the unlabeled point cloud but also addresses the issue of data imbalance. Based on the formulation, we propose a two-stage algorithm. Firstly, we select representative point clouds using a submodular function based on gradients computed from hypothetical labels using Monte Carlo dropout . Secondly, we employ a greedy search algorithm to select unlabeled point clouds, aiming to balance the data distribution as measured by another submodular function. Our work makes the following core **contributions**. **1)** We introduce the first submodular optimization framework for active 3D object detection, addressing two fundamental challenges in this domain. **2)** We develop a simple and efficient two-stage algorithm within the framework for selecting representative point clouds and addressing the issue of data imbalance. **3)** We extensively validate the proposed framework on real-world autonomous driving datasets, including KITTI  and Waymo Open dataset , achieving state-of-the-art performance in active 3D object detection. Furthermore, additional results in active 2D object detection demonstrate the high generalizability of our proposed method.

## 2 Related Works

**Active Learning** (AL) has been a deeply studied topic in machine learning [7; 45], which involves alleviating labeling annotation costs by selecting the most _representative_ samples from a pool of unlabeled data, all the while not comprising model performance. Broadly speaking, AL can be categorized into two strategies - _uncertainty_ sampling and _representative/diversity_ sampling. Algorithms under _representative/diversity_ sampling select subsets of data that act as stand-ins or surrogates for the entire dataset . Prior works have explored coreset-based subset selection [44; 18], clustering algorithms [38; 57; 3], and generative adversarial learning . In uncertainty sampling , samples are selected by maximizing the Shannon entropy  of the posterior probability, minimizing the model's subsequent training error through variance reduction , based on the maximum gradient magnitude , and considering the disagreement among a committee of model hypotheses [47; 54]. Hybrid sampling strategies are proposed recently [25; 1; 26; 20], combining _representative/diversity_ and _uncertainty_ sampling, like BADGE , where the authors propose to use the gradient magnitude with respect to the classifier's parameters as a measure of uncertainty. This approach selects samples whose gradients cover a wide range of directions.

**Active Learning for 3D Object Detection.** While AL has been actively studied and applied to image classification and regression tasks, it has recently been garnering interest in the 3D object detection community. Initial works like MC-MI  make use of Monte-Carlo (MC)-dropout  and Deep Ensembles  to compute the Shannon entropy in the predicted labels and mutual information between class predictions and model parameters. CONSENSUS  estimates uncertainty using ensembles for 2D/3D object detection. While these works relied on generic metrics to measure prediction uncertainty, two other works on 3D object detection, CRB  and KECOR , were proposed recently. In CRB, the method greedily searches and samples unlabeled point clouds that exhibit concise labels, representative features, and geometric balance. KECOR shows that, for sample selection, maximizing the kernel coding rate is beneficial and improves over task-specific AL methods for 3D detection, at fast-running times.

**Submodular Optimization.** Submodular functions (see Section 3), and thereby optimization, have found wide applications in the selection of data subsets [58; 29], active learning [27; 28], speech recognition [59; 60], continual learning , and hyper-parameter tuning . The effectiveness of submodular functions primarily stems from their ability to model diversity via clustering in representation learning. This enhances their capacity to discriminate between different data subsets or classes while ensuring the preservation of unique and relevant features.

## 3 Background

**3D Object Detection.** LiDAR-based 3D object detection aims to localize and recognize objects in point cloud data using oriented bounding boxes and semantic labels. Point clouds are typically generated by LiDAR sensors by emitting pulsed light waves into the surrounding environment and then analyzing the time difference of receiving the bounced-back pulses. The newly generated orderless point \(P_{i}=\{(x,y,z,r)\}\) is represented by \(xyz\) spatial coordinates and reflectance \(r\). Based on the point cloud data, ground-truth bounding boxes can be labeled as \(B_{i}=\{b_{i}\}_{i=1}^{N_{i}}\) where \(b_{i}^{7}\) which include the relative center \(xyz\) spatial coordinates to the object ground planes, the box size, the heading angle, and the box label, with their associated bounding box semantic labels \(C_{i}=\{c_{i}\}_{i=1}^{N_{i}}\). \(N_{i}\) represents the number of bounding boxes in the \(i\)-th point cloud. In 3D object detection, a 3D object \(M_{}\) with parameters \(\) first extracts logits \(f_{i}\) from the raw points. These logits are then processed, resulting in modified logits \(f^{}_{i}\), which are subsequently used by a classification head for predicting the semantic label. Additionally, a regression head will be used to predict the bounding boxes. Finally, the output of the 3D detector is the predicted bounding box \(\{b^{}_{i}\}_{i=1}^{N_{i}}\), which includes the semantic labels \(\{c^{}_{i}\}_{i=1}^{N_{i}}\) for each bounding box, where \(c^{}_{i}\{1,2,,C\}\) with \(C\) being the total number of semantic classes, e.g., cars, cyclists and pedestrians.

**Active Learning for 3D Object Detection.** Active 3D object detection aims to reduce the labeling cost for training the 3D detector. In the beginning stage, a small number of labeled point clouds \(D_{L}\) are randomly selected from the unlabeled data pool \(D_{U}\) to train the backbone 3D object detection model. During active learning, for each query iteration \(q\{1,2,,Q\}\), a given active learning method will select \(\) number of unlabeled point clouds from \(D_{U}\). These selected point clouds are then given to a human annotator for labeling the bounded boxes and semantic labels, and the labeled point clouds \(D_{S}=\{P_{j},B_{j}\}_{j[N_{q}]}\) along with the previously selected point clouds are combined to create selected point clouds \(D_{L}=D_{L} D_{S}\) for the backbone model to re-train. When the query round \(Q\) is reached or the total queried bounding box number, or budget, \(N_{Q}\) is reached, it will stop repeating the above process, where \(N_{Q}=_{q=1}^{Q}N_{q}\).

**Submodular Functions and Optimization.** A set function \(f\), in the discrete space, defined as follows: \(f\):\(2^{D}\)\(\)\(\), where \(2^{D}\) is a power set of \(D\) with \(f\) coming from the discrete space in \(^{2^{n}}\), is considered submodular if it satisfies the following property of diminishing returns,

\[f(A)+f(B) f(A B)+f(A B) \]

\(\)\(A\),\(B D\). A similar alternative property is,

\[f(A\{x\})-f(A) f(B\{x\})-f(B) \]

\(\)\(A\),\(B D\), \(A B\) and \(x B\). Additionally, \(f\) is strictly monotone if _f(A) \(<\) f(B)_ for \(A\)\(\)\(B\). An important example of a submodular function, in the context of active learning, is the Shannon entropy . We refer the reader to  for proof of the submodularity of the Shannon entropy .

In order to maximize the representativeness of samples or subsets, a natural solution is to maximize \(f\) in the form of \(_{A}f(A)\) with \(\) being a constrained set and \(\)\(\)\(2^{D}\). The idea is to select a subset set that would accept feasible solutions. Since \(f\) is monotone submodular, such discrete maximization problems, being NP-complete , can be guaranteed to be approximated to a factor of 1 - \(\) 0.63  if maximized under \(\) using a greedy approximation algorithm .

## 4 Algorithms

### A Submodular Optimization Approach to Active 3D Object Detection

To tackle the core challenges of active 3D object detection, we propose a **unified** framework grounded in submodular optimization. Our objective is to select a set of unlabeled point clouds that are (1)representative, including various levels of difficulty, and, (2) preservative of the label distribution of the selected samples.

To achieve the first goal, we leverage a submodular function \(f_{1}\) to measure the representativeness of the selected unlabeled point clouds \(D_{S}\) with respect to the whole unlabeled set \(D_{U}\). This can be achieved by minimizing the absolute difference between \(f_{1}(D_{U})\) and \(f_{1}(D_{S})\) since \(D_{S}{}D_{U}\). This translates to maximizing the absolute difference \(f_{1}(D_{S})-f_{1}(D_{U})\). To achieve the second goal, we leverage a different submodular function \(f_{2}\) to ensure that once the selected unlabeled point clouds \(D_{S}\) are added to the labeled set \(D_{L}\), the overall quality, as measured by label distribution, will not decrease. In sum, we aim to select unlabeled point clouds \(D_{S}\) from the unlabeled pool \(D_{U}\) that optimize the following two objectives,

\[_{D_{S} D_{U}}[f_{1}(D_{S})-f_{1}(D_{U})]+[f_{2}(D_{L})-f_{2}(D_{L}  D_{S})] \]

Unlike existing active learning methods that rely on sample uncertainty, such as Shannon entropy , which tends to select difficult point clouds, our proposed formulation selects samples of varying difficulty levels. Additionally, unlike existing algorithms  or clustering-based approaches , our formulation also considers the selected labeled point clouds to prevent data imbalance when training a 3D detector. The formulation is also general, allowing for different choices of submodular functions. In practice, we use a feature-based submodular function  as \(f_{1}\) and the entropy of the label distribution as \(f_{2}\). Although the proposed formulation is motivated by the problem of active 3D object detection, it can also be useful for active learning problems in similar domains, such as 2D object detection, as we will demonstrate in the experiments. Finding the set \(D_{S}\) that solves Equation 3 is NP-complete . Therefore, we propose a simple algorithm called **STONE** to efficiently solve the problem as will be detailed below.

### Stone

In order to optimize the submodular optimization framework in Equation 3, we have designed an active learning pipeline with two stages, aligning with the dual objectives of the submodular criteria. To enhance computational efficiency and simplify optimization, we have implemented a hierarchical structure to eliminate unselected samples at each stage, with the remaining samples comprising our final selection. Initially, we select \(_{1}\) samples using the proposed Gradient-Based Submodular Subset Selection (**GBSSS**), which maximizes diversity and coverage while minimizing redundancy from \(D_{U}\). Subsequently, we select \(_{2}\) samples from \(_{1}\) through Submodular Optimization for Class Balancing (**SDMCB**). A detailed explanation of these stages is provided in the following paragraphs. We illustrate our proposed pipeline in Figure 1.

Figure 1: **STONE**: An illustrative pipeline of our proposed active learning method for 3D object detection leveraging submodular functions.

#### 4.2.1 Gradient-Based Submodular Subset Selection (GBSSS)

To address the challenge outlined in the introduction--arising from variations in object size, occlusion, and truncation, which introduce different levels of difficulty across categories--we need to identify representative and diverse samples not only across categories but also within each category with varying difficulty levels. This challenge can be addressed by using a submodular function \(f_{1}\) for maximizing \(f_{1}(D_{S})-f_{1}(D_{U})\). In the proposed gradient-based submodular subset selection, we select a subset of unlabeled point clouds from \(D_{U}\) that maximizes gradient feature coverage while ensuring diversity.

Due to the absence of ground-truth semantic labels in the pool of unlabeled point clouds, we employ Monte Carlo dropout (MC-dropout)  at the detector head for each point cloud \(P_{i}\) to generate multiple regression predictions, as in . By averaging these predictions, we obtain regression hypothetical labels as well as the classification hypothetical labels. We then calculate the loss function \(_{i}\) of the 3D detector \(M_{}\), with parameters \(\), using the classification and regression hypothetical labels. Following backpropagation, we extract gradients \(_{}_{i}\) from the fully connected (FC) layer of the detector head.

However, when the dataset is highly imbalanced, as is usually the case in 3D object detection, the gradients generated by the above approach become inaccurate for classes with fewer samples . To address this issue, we introduce two novel reweighing approaches for handling regression loss \(L_{reg}\) and classification loss \(L_{cls}\) in 3D object detection for a better computation of the gradients for rare classes. In particular, after calculating the regression loss for each bounding box, we calculate the average loss \(L_{reg}^{c}\) for class \(c\) based on the regression hypothetical labels. For a given label \(c\), the regression loss reweighing factors can be expressed as \(w_{c}=}\), where \(n_{c}\) is the number of the bounding boxes that belong to class \(c\). We then normalize \(w_{c}\) as \(}=}{(w_{c})}\). The purpose of doing so is two-fold - 1) The class-specific weights in \(L_{reg}^{c}\) are uniformly scaled to prevent bias toward frequently occurring classes. 2) This approach helps reduce overfitting to bounding boxes associated with those classes. Then, we perform an element-wise multiplication between \(}\) and \(L_{reg}^{c}\) to re-scale the loss, for each class \(c\), and compute the mean, across all the semantic classes \(C\), to get the final reweighed regression loss \(_{reg}\), as \(_{reg}=_{c=1}^{C}} L_{reg}^{c}\). The reweighed regression loss \(_{reg}\) places more emphasis on classes with fewer samples, which is critical for computing the gradient.

Inspired by the theoretical formulation in , we additionally introduce a new classification reweighing loss function designed to handle class imbalances by adapting the classification margins according to the label distribution during the active learning stage. In essence, for rare classes, the distance of the samples from the decision boundary i.e., the margin, would have to be penalized more. This way the generalization error of minor classes can be improved without worsening the performance on frequent classes. However, penalizing the rare classes more might affect the margins of frequent classes, leading to a complex trade-off . In order to balance this, for the \(i\)-th point cloud, we leverage a margin vector \(m_{i}\), where the margin for class \(c\) is defined as \(m_{i,c}=}}\) (\(n_{c}\) is the class frequency of class \(c\)). We then subtract the predicted logits \(f_{i}\) by the margin vector \(m_{i}\) to reweigh the predicted logits. Finally, the reweighed logits and hypothetical classification labels \(_{i}\) are used in the classification loss function \(L_{cls}\) to compute the class-balanced classification loss \(_{cls}\) as,

\[_{cls}=L_{cls}(_{i},f_{i}-m_{i}) \]

We then use the class-balanced detection loss \(=_{reg}+_{cls}\) to compute the gradient for each point cloud. After obtaining the gradients, we use a feature-based submodular function \(f_{1}\) to select the top \(_{1}\) diverse samples from \(D_{U}\),

\[_{D_{S} D_{U},|D_{S}|=_{1}}_{P_{i} D_{S}}g(( _{}_{i})) \]

where \(g(x)=(1+x)\). The concavity of \(g(x)\) ensures that the marginal gain of adding more instances of similar features decreases as more such instances are added. This means that the function will favor adding elements that introduce new features rather than redundant ones. For the score function , \(()\), we use gradients distribution entropy \(H(_{})\) as the measure of informativeness. More details are given in the Appendix. A greedy algorithm is then applied to iteratively add samples to a subset, providing the most significant increase in marginal gain until the target size, \(_{1}\), is reached.

#### 4.2.2 Submodular Diversity Maximization for Class Balancing (SDMCB)

One of the main challenges for active 3D object detection is that each 3D scene can contain multiple semantic classes, such as cars and cyclists. As the queried point clouds increase, it will inevitably introduce an imbalanced label distribution in \(D_{L}\), eventually resulting in performance drops. This situation is more prominent in autonomous driving datasets. A recent study, CRB, attempts to address this imbalance issue by introducing individual point cloud balancing. However, it cannot guarantee a balanced label distribution in \(D_{U}\) across multiple active learning queries. In Figure 2, we illustrate the imbalance in label distribution entropy caused by existing methods , particularly KECOR, where label imbalance begins to increase as the number of query rounds rises. Aiming to alleviate this label imbalance, we utilize entropy calculated from the predicted label distribution as the submodular function \(f_{2}\). This approach supports the principle of diminishing returns as \(D_{L}\) becomes more balanced. Our method is divided into two steps. In Step 1, we select point clouds which have balanced label distribution based on the hypothetical labels. In Step 2, from the selected ones, we further choose a subset that can be labeled to ensure that the label distribution is balanced in the labeled set \(D_{L}\).

**Step 1: Balance the label distribution of individual point cloud.** To balance each individual point cloud, we select the top \(\) most balanced samples according to their individual point cloud label distribution entropy. This selection process applies a heuristic that maximizes the immediate gain in entropy. To calculate the probability \(_{i,c}\) of a specific label \(c\) within point cloud \(P_{i}\), we first normalize the count of predicted label \(c\), denoted as \(n_{c}\), by the total number of predicted bounding boxes \(N_{i}\) in \(P_{i}\). This normalization is done using a softmax function and adjusts the raw count of label \(c\) to reflect its proportion relative to the total number of labels. Next, the entropy of the individual point cloud, \(H(P_{i})\), is calculated as follows,

\[H(P_{i})=-_{c=1}^{C}_{i,c}_{i,c},_{i,c}=/N_{i}}}{_{c=1}^{C}e^{n_{c}/N_{i}}} \]

We calculate \(H(P_{i})\) for each unlabeled point cloud \(P_{i} D_{U}\) and identify the top \(_{1}\) point cloud samples with the highest entropy values.

Step 2: Balance the label distribution of labeled point clouds.We then aim to choose a subset of point clouds from the selected ones in Step 1 for balancing the label distribution of the labeled point clouds. To achieve this, we follow a similar approach from the previous step to calculate label probability. Instead of normalizing \(n_{c}\) by the number of bounding boxes \(N_{i}\) in the point cloud \(P_{i}\), we first compute the sum of the number of labeled bounding boxes \(N_{L,c}\) of a certain class \(c\) in the labeled set and \(n_{c}\), which represents the total number of objects belonging to class \(c\) once the point cloud \(P_{i}\) is labeled. Then we compute the sum of total labeled bounding boxes \(N_{L}\) and \(N_{i}\) to find \(_{i,c}\) which represents the normalized number of predicted label \(c\). Finally, we calculate \(}_{i,c}\) indicating the cumulative label probability of certain class \(c\) once the point cloud \(P_{i}\) is labeled. The cumulative entropy \(H(_{i})\) of the cumulative label probability then can be computed as,

\[H(_{i})=-_{c=1}^{C}}_{i,c}}_{i,c}, }_{i,c}=}}{_{c=1}^{C}e^{_{i,c }}},_{i,c}=+n_{c}}{N_{L}+N_{i}} \]

\(H(_{i})\) can be used to measure label balancing after the point cloud \(P_{i}\) is added into the labeled set. To construct the final selected point clouds in the current query round \(q\), we use a greedy search algorithm to iteratively select samples from \(D_{U}\) that maximizes the cumulative entropy, to finally select the top \(_{2}\) samples. Thus, using this two-stage progressive approach, we achieve both individual point cloud label balance and label balance across all the labeled point clouds which is crucial for the real-world application of active 3D object detection.

Figure 2: The proposed **STONE** method more effectively maintains the balance of label distribution in active 3D object detection. The plots show the cumulative label distribution entropy values on KITTI  validation set split with PV-RCNN .

Experiments and Results

### Experimental Setup

**Datasets.** For our experiments, we use the KITTI dataset , one of the commonly used datasets in autonomous driving tasks. The dataset consists of 3,712 training samples and 3,769 validation samples, which include a total of 80,256 labeled objects. These objects include cars, pedestrians, and cyclists, each annotated with class categories and bounding boxes. We also use the more challenging dataset for 3D object detection in autonomous driving - the Waymo Open dataset . It includes 158,361 training samples and 40,077 testing samples. In the KITTI dataset, task difficulty levels are defined as EASY for fully visible objects, MODERATE for partially occluded objects, and HARD for significantly occluded objects. For the Waymo test set, the framework categorizes difficulty into two levels: LEVEL 1 with more than five LiDAR points inside the ground-truth bounding box and LEVEL 2 with less or equal to five points. The sampling intervals are set to 1 and 10 for KITTI and Waymo Open, respectively.

**Baselines.** We compare our work against several generic active learning baselines, 1) **RANDOM:** Naive sampling strategy that randomly selects fixed samples at every round; 2) **ENTROPY**: Selects samples with the highest degree of uncertainty as measured by entropy of the sample's posterior probability; 3) **LLAL**: Task-agnostic method with a parametric module that chooses samples where the model is likely to make wrong predictions, based on an indicative loss; 4) **CORESET**: Greedy furthest-first method using the core-set selection on both labeled and unlabeled embeddings. 5) **BADGE**: Batch-mode AL method to select diverse samples, with high gradient magnitude, that span a wide range of directions in the gradient space.

We also draw contrasts between our method and AL methods (and variants) for 2D/3D detection, 6) **MC-MI**: Uses MC-dropout  to estimate model uncertainty and mutual information to select the most uncertain point cloud samples; 7) **MC-REG**: Uses several rounds of MC-dropout  to approximate the regression uncertainty and picks samples with the greatest variance for labeling; 8) **LT/C**: Adapted from 2D detection, it selects samples based on both localization uncertainty and classification confidence to improve model performance; 9) **CONSENSUS**: Employs ensemble-based uncertainty estimation and continuous training to reduce labeling efforts. 10) **CRB** and 11) **KECOR**. To compare the performance of our method on the 2D object detection task, we compare with 12) **AL-MDN**: Constructs mixture density networks to estimate probability distributions for the outputs of localization and classification heads.

**Evaluation Metrics.** To maintain fairness with the baselines on KITTI, we measure the performance using Average Precision (AP) for 3D and Bird Eye View (BEV) detection, with rotated Intersection over Union (IoU) thresholds of 0.7 for cars and 0.5 for pedestrians and cyclists, following . For the Waymo Open, performance is measured using Average Precision (AP) and Average Precision Weighted by Heading (APH), with IoU thresholds of 0.7 for vehicles and 0.5 for pedestrians and cyclists.

**Implementation Details.** We train our proposed method and all baselines on a GPU cluster with 4 NVIDIA RTX A5000 GPUs. To make fair comparisons, we adopt the implementation settings as outlined in CRB. All the baselines use PV-RCNN  as the backbone detection model.

* Training settings. For KITTI and Waymo Open, the training batch sizes are set to 6 and 4 respectively. However, the evaluation batch sizes are set to 16 for both datasets. We optimize the network parameters using Adam with a fixed learning rate of 0.01. For all the methods, we perform 5 stochastic forward passes of the MC-Dropout .
* Active learning parameters. For KITTI, we set \(_{1}\) and \(_{2}\) to 400 and 300 respectively. In the case of Waymo Open, \(_{1}\) and \(_{2}\) are set to 2,000 and 1,200 respectively. To ensure fairness, \(N_{q}\) is set to 100 in all the methods.

### Results

**KITTI Dataset Results.** We evaluate the performance of STONE against other baseline methods in Table 1. We clearly notice that STONE outperforms all the prior active learning methods, irrespective of the detection difficulty level and backbone model. In particular with PV-RCNN  as backbone, on average, we observe 3D AP improvements of 1.47%, 0.84%, and 1.24%, over CRB for the EASY,

[MISSING_PAGE_FAIL:8]

classes. For the experiments, we use VOC07 _trainval_ and VOC07+12 _trainval_ to train a Single Shot MultiBox Detector (SSD)  with a VGG-16 backbone , and test on VOC07 _test_. We follow the training guidelines and setup as outlined in AL-MDN  to reproduce the results. For all methods, \(N_{q}\) is set to be 1000 for a total of three queries. In Table 2, we summarize the results in terms of mean Average Precision (mAP), noting that our method either outperforms or is comparable to all baseline methods. In the first query, we observe improvements of 2.93% and 2.43% over AL-MDN\({}_{gmm}\) and AL-MDN\({}_{eff}\) respectively. Our method, while achieving a balance in label distribution, also scales well to large datasets with more semantic labels.

**Computational Complexity.** STONE achieves significant GPU memory savings compared to KECOR, which computes gradients of the output of the ROI head's fully connected shared layer, resulting in a gradient matrix of high dimensions and memory. In contrast, STONE focuses gradient computation on the outputs of the classification and regression loss layers within the ROI head, which are much lower in dimensionality. On the KITTI dataset, with a batch size of 6 (for a fair comparison), our method consumes 10 GB of GPU memory, whereas KECOR consumes 24 GB, which is 140% more GPU memory. Additionally, STONE maintains a similar running time to KECOR.

## 6 Ablation Study

We perform ablation studies, on KITTI, to assess the efficacy of our approach and to better understand the key mechanisms and components involved. In the ablation study, all the experiments use 5 rounds of active learning selection, acquiring a total of 500 point clouds for annotation. We present and discuss additional ablation experiments in the Appendix (8).

**Contributions of \(_{reg}\) and \(_{cls}\).** Given that our method incorporates both regression loss \(_{reg}\) and classification loss \(_{cls}\) to handle potential class imbalance, we have carried out extensive experiments to analyze their impact on model performance. We first investigated whether regression or classification loss has a greater impact by using gradients generated from each loss separately during the active learning stage. The results showed that using only \(_{reg}\) resulted in an average 3D AP drop of 0.77% for HARD while using only \(_{cls}\) resulted in an average 3D AP drop of 3.12% for HARD on the KITTI validation dataset. This indicates that \(_{cls}\) has a larger impact on our model's performance, and both losses are essential for optimal results.

**Effect of the reweighing factor.** To further evaluate the reweighing factor's importance on our method's performance, we perform three crucial experiments. In Table 3 (row 1), we study the effect of not having the reweighing factor on both the regression and classification losses. We observe performance drops of 3.78%, 1.55%, and 2.45% for the EASY, MODERATE, and HARD levels of difficulty, respectively. We also observe similar performance drops in Table 3 (row 2) and Table 3 (row 3) with no reweighing factor for the regression or classification loss, respectively. This fundamental issue arises because, in contrast to classification tasks where a single loss function is typically considered, detection tasks require the simultaneous optimization of multiple loss components. Specifically, reweighing only the classification or regression loss in detection tasks can disrupt the

    &  & Bounding Boxes \\  Stages & EASY & MOD. & HARD & \\  GMSSS & 79.60 & 66.50 & 62.78 & 2623 \\ GMSSS + STANCE (Sept 1) & 80.37 & 67.80 & 63.17 & 1571 \\ GMSSS + STANCE (Sept 2) & 79.30 & 68.86 & 64.63 & 2473 \\ SMDN & 80.13 & 67.83 & 62.69 & **1484** \\ SMDN (Sept 1) & 80.98 & 60.44 & 64.51 & 1567 \\ SMDNCS (Sept 2) & 80.66 & 63.56 & 64.5 & 2514 \\ All & **85.03** & **70.33** & **65.33** & 1530 \\   

Table 4: Stage-wise performance comparisons.

    &  \\  } &  & 24 (\(\)k) & 34.04 (\(\)k) \\  RANDOM  & 62.43\(\)0.10 & 66.36\(\)0.13 & 68.47\(\)0.09 \\ ENTROPY  & 62.43\(\)0.10 & 66.85\(\)0.12 & 68.70\(\)0.18 \\ COREST  & 62.43\(\)0.10 & 66.57\(\)0.20 & 68.57\(\)0.26 \\ LL4 & 62.47\(\)0.16 & 67.02\(\)0.11 & 68.90\(\)0.15 \\ MC-DROPOUT  & 62.43\(\)0.19 & 67.10\(\)0.07 & 69.39\(\)0.09 \\ ENSEDAE  & 62.43\(\)0.10 & 67.11\(\)0.26 & 69.26\(\)0.14 \\ AL-MDN\({}_{gmm}\) & 62.43\(\)0.10 & 67.33\(\)0.12 & 69.43\(\)0.11 \\ AL-MDN\({}_{eff}\) & 62.91\(\)0.16 & **67.61\(\)0.17** & **69.66\(\)0.17** \\  
**STONE** & **65.34\(\)0.34** & **67.01\(\)0.47** & **69.03\(\)0.55** \\   

Table 2: VOC07 : mAP(%) of STONE against AL baselines.

    &  \\  Components & EASY & MOD. & HARD \\  w/o reweighing factor & 80.81 & 68.78 & 62.88 \\ w/o reweighing factor on \(L_{reg}\) & 80.32 & 69.64 & 63.59 \\ w/o reweighing factor on \(L_{cls}\) & 80.22 & 68.08 & 62.93 \\ none & **83.03** & **70.33** & **65.33** \\   

Table 3: Ablation on the reweighing factor.

balance within the model, as it does not adequately model the interdependencies between these loss functions, leading to the suboptimal performance of the detector.

**Relevance of each stage.** In Table 4, we examine the importance and the relevance of each stage of our method. We assess the performance using 3D AP (%) and the number of bounding boxes annotated with 500 point cloud selections. From Table 4 we observe that the removal of any single component leads to a drop in 3D AP performance. In particular, using the GBMSS stage only leads to a drop of 3D AP by 3.43%, 3.83%, and 2.55% for the EASY, MODERATE and HARD difficulty levels respectively. In such a scenario, the bounding boxes annotated increase by 71.43%, which shoots up the labeling cost. It is interesting to note that while using only the SDMCB stage results in annotating the lowest bounding boxes, it also leads to a drop in performance. In conclusion, both stages of our method are necessary for the optimal selection of samples, with a lower labeling cost, to ensure the efficiency and accuracy of the proposed active learning method.

**Backbone-agnostic performance.** In all of the experiments in this paper, we use PV-RCNN as the backbone detection model. However, to show the invariance of our method towards a change in the backbone, we perform experiments with SECOND , a widely used 3D object detector. The results, as shown in Table 5, indicate that STONE achieves a 3.4% higher 3D mAP score at the HARD level and 2.43% higher mAP score at the HARD level in BEV detection compared to the state-of-the-art method, KECOR. This demonstrates the performance and generality of the proposed approach.

## 7 Conclusion

In this paper, we propose a novel approach called **STONE,** a **unified** active 3D object detection methodology, based on submodular optimization. We provide a robust and compute-efficient solution by proposing a two-stage algorithm that first utilizes a submodular function based on the gradients using the Monte Carlo dropout  to select representative point clouds. We then apply a greedy search algorithm to balance the data distribution due to the possibility of having an imbalance across all the labeled point clouds. Extensive experiments on benchmark autonomous driving datasets, including KITTI  and Waymo Open  datasets, demonstrate the effectiveness and generalization of our method.

**Limitations.** One limitation of the proposed method is that it does not reduce the running time of existing active 3D detection methods, which is primarily due to the large number of unlabeled point clouds. In future work, we will investigate more efficient methods for active 3D object detection.

**Societal Impact.** The proposed method will be important for several real-world applications such as autonomous driving and robotics, by reducing the cost of labeling.

**Acknowledgements.** We would like to thank the anonymous reviewers for their helpful comments. This project was supported by a grant from the University of Texas at Dallas. This work is also supported by the National Science Foundation under Grant Numbers IIS-2106937, a gift from Google Research, an Amazon Research Award, and the Adobe Data Science Research award.

    &  &  \\  Method & EASY & MOD. & HARD & EASY & MOD. & HARD \\  Random & 66.33 & 55.48 & 51.53 & 75.66 & 63.77 & 59.71 \\  COREST  & 66.86 & 53.22 & 48.97 & 73.08 & 61.03 & 56.95 \\  LLAL  & 69.19 & 55.38 & 50.85 & 76.52 & 63.25 & 59.07 \\  BADGE  & 69.92 & 55.60 & 51.23 & 76.07 & 63.39 & 59.47 \\  BATT  & 69.45 & 55.61 & 51.25 & 76.04 & 63.49 & 53.40 \\  CRB  & 72.33 & 58.06 & 53.09 & 78.84 & 65.82 & 61.25 \\  KECOR  & 74.05 & 60.38 & 55.34 & 80.00 & 68.20 & 63.20 \\  
**STONE** & **76.86** & **64.04** & **58.75** & **82.14** & **70.82** & **65.68** \\   

Table 5: 3D mAP(%) scores on KITTI validation set with 1% queried bounding boxes, using SECOND  as the backbone detection model.