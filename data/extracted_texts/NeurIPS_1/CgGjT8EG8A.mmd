# Universal Exact Compression of Differentially Private Mechanisms

Yanxiao Liu

The Chinese University of Hong Kong

yanxiaoliu@link.cuhk.edu.hk

&Wei-Ning Chen

Stanford University

wnchen@stanford.edu

&Ayfer Ozgur

Stanford University

aozgur@stanford.edu

&Cheuk Ting Li

The Chinese University of Hong Kong

ctli@ie.cuhk.edu.hk

###### Abstract

To reduce the communication cost of differential privacy mechanisms, we introduce a novel construction, called Poisson private representation (PPR), designed to compress and simulate any local randomizer while ensuring local differential privacy. Unlike previous simulation-based local differential privacy mechanisms, PPR exactly preserves the joint distribution of the data and the output of the original local randomizer. Hence, the PPR-compressed privacy mechanism retains all desirable statistical properties of the original privacy mechanism such as unbiasedness and Gaussianity. Moreover, PPR achieves a compression size within a logarithmic gap from the theoretical lower bound. Using the PPR, we give a new order-wise trade-off between communication, accuracy, central and local differential privacy for distributed mean estimation. Experiment results on distributed mean estimation show that PPR consistently gives a better trade-off between communication, accuracy and central differential privacy compared to the coordinate subsampled Gaussian mechanism, while also providing local differential privacy.

## 1 Introduction

In modern data science, there is a growing dependence on large amounts of high-quality data, often generated by edge devices (e.g., photos and videos captured by smartphones, or messages hosted by social networks). However, this data inherently contains personal information, making it susceptible to privacy breaches during acquisition, collection, or utilization. For instance, despite the significant recent advancement in foundational models , studies have shown that these models can accidentally memorize their training data. This poses a risk where malicious users, even with just API access, can extract substantial portions of sensitive information . In recent years, differential privacy (DP)  has emerged as a powerful framework for safeguarding users' privacy by ensuring that local data is properly randomized before leaving users' devices. Apart from privacy concerns, communicating local data from edge devices to the central server often becomes a bottleneck in the system pipeline, especially with high-dimensional data common in many machine learning scenarios. This leads to the following fundamental question: how can we efficiently communicate privatized data?

Recent works have shown that a wide range of differential privacy mechanisms can be "simulated" and "compressed" using shared randomness, resulting in a "compressed mechanism" which has a smaller communication cost compared to the original mechanism, while retaining the (perhaps slightly weakened) privacy guarantee. This can be done via rejection sampling , importance sampling , or dithered quantization  with each approach having its own advantages and disadvantages. For example, importance-sampling-based methods  and therejection-sampling-based method  can simulate a wide range of privacy mechanisms; however, the output distribution of the induced mechanism does not perfectly match the original mechanism. This is limiting in scenarios where the original mechanism is designed to satisfy some desired statistical properties, e.g. it is often desirable for the local randomizer to be unbiased or to be "summable" noise such as Gaussian or other infinitely divisible distributions. Since the induced mechanism is different from the original one, these statistical properties are not preserved. On the other hand, dithered-quantization-based approaches  can ensure a correct simulated distribution, but they can only simulate additive noise mechanisms. More importantly, dithered quantization relies on shared randomness between the user and the server, and the server needs to know the dither for decoding. This annuls the local privacy guarantee on the user data, unless we are willing to assume a trusted aggregator , use an additional secure aggregation step , or restrict attention to specific privacy mechanisms (e.g., one-dimensional Laplace ).

#### Our contribution

In this paper, we introduce a novel "DP mechanism compressor" called _Poisson private representation (PPR)_, designed to compress and _exactly_ simulate _any_ local randomizer while ensuring local DP, through the use of shared randomness.1 We elaborate on three main advantages of PPR, namely universality, exactness and communication efficiency.

**Universality.** Unlike dithered-quantization-based approaches which can only simulate additive noise mechanisms, PPR can simulate any local or central DP mechanism with discrete or continuous input and output. Moreover, PPR is _universal_ in the sense that the user and the server only need to agree on the output space and a proposal distribution, and the user can simulate any DP mechanism with the same output space. The user can choose a suitable DP mechanism and privacy budget according to their communication bandwidth and privacy requirement, without divulging their choice to the server.

**Exactness.** Unlike previous DP mechanism compressors such as Feldman and Talwar , Shah et al. , Triastcyn et al. , PPR enables _exact_ simulation, ensuring that the reproduced distribution perfectly matches the original one. Exact distribution recovery offers several advantages. Firstly, the compressed sample maintains the same statistical properties as the uncompressed one. If the local randomizer is unbiased (a crucial requirement for many machine learning tasks like DP-SGD), the outcome of PPR remains unbiased. In contrast, reconstruction distributions in prior simulation-based compression methods  are often biased unless specific debiasing steps are performed (only possible for certain DP mechanisms ). Secondly, when the goal is to compute the mean (e.g., for private mean or frequency estimation problems) and the local noise is "summable" (e.g., Gaussian noise or other infinitely divisible distributions ), exact distribution recovery of the local noise enables precise privacy accounting for the final _central_ DP guarantee, without relying on generic privacy amplification techniques like shuffling . PPR can compress a central DP mechanism (e.g., the Gaussian mechanism ) and simultaneously achieve weaker local DP (i.e., with a larger \(_{}\)) and stronger central DP (i.e., with a smaller \(_{}\)), while maintaining exactly the same privacy-utility trade-offs as the uncompressed Gaussian mechanism.

**Communication efficiency.** PPR compresses the output of any DP mechanism to a size close to the theoretical lower bound. For a mechanism on the data \(X\) with output \(Z\), the compression size of PPR is \(I(X;Z)+(I(X;Z)+1)+O(1)\), with only a logarithmic gap from the mutual information lower bound \(I(X;Z)\).2 The "\(O(1)\)" constant can be given explicitly in terms of a tunable parameter \(>1\) which controls the trade-off between compression size, computational time and privacy.

The main technical tool we utilize for PPR is the Poisson functional representation , which provides precise control over the reconstructed joint distribution in channel simulation problems . Channel simulation aims to achieve the minimum communication for simulating a channel (i.e., a specific conditional distribution). Typically, these methods rely on shared randomness between the user and server, and privacy is only preserved _when the shared randomness is hidden from the adversary_. This setup conflicts with local DP, where the server (which requires access to shared randomness for decoding) is considered adversarial. To ensure local DP, we introduce a randomized encoder based on the Poisson functional representation, which stochastically mapsa private local message to its representation. Hence, PPR achieves order-wise trade-offs between privacy, communication, and accuracy, while preserving the original distribution of local randomizers.

**Notations.** Entropy \(H(X)\), mutual information \(I(X;Y)\), KL divergence \(D(P\|Q)\) and logarithm are to the same base, e.g., they can be all in bits (base \(2\)), or all in nats (base \(e\)). For \(P,Q\), \(P()/Q\) denotes the Radon-Nikodym derivative.

## 2 Related Work

Generic compression of local DP mechanisms.In this work, we consider both central DP  and local DP . Recent research has explored methods for compressing local DP randomizers when shared randomness is involved. For instance, when \( 1\), Bassily and Smith  demonstrated that a single bit can simulate any local DP randomizer with a small degradation of utility, as long as the output can be computed using only a subset of the users' data. Bun et al.  proposed another generic compression technique based on rejection sampling, which compresses a \(\)-DP mechanism into a \(10\)-DP mechanism. Feldman and Talwar  proposed a distributed simulation approach using rejection sampling with shared randomness, while Shah et al. , Triastcyn et al.  utilized importance sampling (or more specifically, minimum random coding ). However, all these methods only _approximate_ the original local DP mechanism, unlike our scheme, which achieves an _exact_ distribution recovery.

Distributed mean estimation under DP.Mean estimation is the canonical problems in distributed learning and analytics. They have been widely studied under privacy , communication , or both constraints . Among them, Asi et al.  has demonstrated that the optimal unbiased mean estimation scheme under local differential privacy is privUnit. Subsequently, communication-efficient mechanisms introduced by Feldman and Talwar , Shah et al. , Isik et al.  aimed to construct communication-efficient versions of privUnit, either through distributed simulation or discretization. However, these approaches only approximate the privUnit distribution, while our proposed method ensures exact distribution recovery.

Distributed channel simulation.Our approach relies on the notion of channel simulation . One-shot channel simulation is a lossy compression task, which aims to find the minimum amount of communications over a noiseless channel that is in need to "simulate" some channel \(P_{Z|X}\) (a specific conditional distribution). By Harsha et al. , Li and El Gamal , the average communication cost is \(I(X;Z)+O((I(X;Z)))\). In , algorithms based on rejection sampling are proposed, and it is further generalized in  by introducing the greedy rejection coding. Dithered quantization  has also been used to simulate an additive noise channel in  for neural compression. As also shown in , the time complexity of channel simulation protocols (e.g., in ) is usually high, and  try to improve the runtime under certain assumptions. Moreover, channel simulation tools have also been used in neural network compression , image compression via variational autoencoders , diffusion models with perfect realism  and differentially private federated learning .

Poisson functional representation.The Poisson functional representation is a channel simulation scheme studied in . Also refer to  for related constructions for Monte Carlo simulations. Based on the Poisson functional representation, the Poisson matching lemma has been used in proving one-shot achievability results for various network information theory problems . Also see applications on unequal message protection , hypothesis testing , information hiding , minimax learning  and secret key generation . A variation called the importance matching lemma  has also used in distributed lossy compression. By , the Poisson functional representation can be viewed as a certain variant of the A\({}^{*}\) sampling , and hence an optimized version with better runtime for one-dimensional unimodal distribution has been proposed in .

## 3 Preliminaries

We begin by reviewing the formal definitions of differential privacy (DP). We consider two models of DP data analysis. In the central model, introduced in Dwork et al. , the data of the individuals is stored in a database \(X\) by the server. The server is then trusted to perform data analysis whose output \(Z=(X)\) (where \(\) is a randomized algorithm), which is sent to an untrusted data analyst, does not reveal too much information about any particular individual's data. While this model requires a higher level of trust than the local model, it is possible to design significantly more accurate algorithms. We say that two databases \(X,X^{}\) are neighboring if they differ in a single data point. More generally, we can consider a symmetric neighbor relation \(^{2}\), and regard \(X,X^{}\) as neighbors if \((X,X^{})\).

On the other hand, in the local model, each individual (or client) randomizes their data before sending it to the server, meaning that individuals are not required to trust the server. A local DP mechanism  is a local randomizer \(\) that maps the local data \(X\) to the output \(Z=(X)\). Note that here \(X\) is the data at one user, unlike central-DP where \(X\) is the database with the data of all users. We now review the notion of \((,)\)-central and local DP.

**Definition 3.1** (Differential privacy ).: Given a mechanism \(\) which induces the conditional distribution \(P_{Z|X}\) of \(Z=(X)\), we say that it satisfies \((,)\)-DP if for any neighboring \((x,x^{})\) and \(\), it holds that

\[(Z\,|\,X=x) e^{}(Z\,|\,X=x^{ })+.\]

In particular, if \(=^{2}\), we say that the mechanism satisfies \((,)\)-local DP .3

When a mechanism satisfies \((,0)\)-central/local DP, we will refer to it simply as \(\)-central/local DP. \(\)-DP can be generalized to _metric privacy_ by considering a metric \(d_{}(x,x^{})\) over \(\).

**Definition 3.2** (\( d_{}\)-privacy ).: Given a mechanism \(\) with conditional distribution \(P_{Z|X}\), and a metric \(d_{}\) over \(\), we say that \(\) satisfies \( d_{}\)-privacy if for any \(x,x^{}\), \(\), we have

\[(Z\,|\,X=x) e^{ d_{}(x,x^{ })}(Z\,|\,X=x^{}).\]

This recovers the original \(\)-central DP by considering \(d_{}\) to be the Hamming distance among databases, and recovers the original \(\)-local DP by considering \(d_{}\) to be the discrete metric .

The reason we use \(X\) to refer to both the database in central DP and the user's data in local DP is that our proposed method can compress both central and local DP mechanisms in exactly the same manner. In the following sections, the mechanism \(\) to be compressed (often written as a conditional distribution \(P_{Z|X}\)) can be either a central or local DP mechanism, and the neighbor relation \(\) can be any symmetric relation. The "encoder" refers to the server in central DP, or the user in local DP. The "decoder" refers to the data analyst in central DP, or the server in local DP.

## 4 Poisson Private Representation

**Definition 4.1** (Poisson functional representation ).: Let \((T_{i})_{i}\) be a Poisson process with rate \(1\) (i.e., \(T_{1},T_{2}-T_{1},T_{3}-T_{2},}{{}} (1)\)), independent of \(Z_{i}}{{}}Q\) for \(i=1,2,\). Then \((Z_{i},T_{i})_{i}\) is a Poisson process with intensity measure \(Q_{[0,)}\), where \(_{[0,)}\) is the Lebesgue measure over \([0,)\). Fix any distribution \(P\) over \(\) that is absolutely continuous with respect to \(Q\). Let

\[_{i}:=T_{i}P}{Q}(Z_{i}) ^{-1}. \]

Then \((Z_{i},_{i})\) is a Poisson process with intensity measure \(P_{[0,)}\), which is from the mapping theorem . The _Poisson functional representation_ (PFR)  selects the point \(Z=Z_{K}\) with the smallest associated \(_{K}\), i.e., let \(K:=_{i}_{i}\) and \(Z:=Z_{K}\).4

The PFR selects a sample following the target distribution \(P\) using another distribution \(Q\). It draws a random sequence \((Z_{i})_{i}\) from \(Q\) and a sequence of times \((T_{i})_{i}\) according to a Poisson process. If we select the sample \(Z_{i}\) with the smallest \(T_{i}\), then the selected sample follows \(Q\). To obtain a sample from \(P\) instead, we multiply the time by the factor \((P}{Q}(Z_{i}))^{-1}\) in (1) to give \(_{i}\), so the \(Z_{i}\) with the smallest \(_{i}\) will follow \(P\).

The Poisson functional representation guarantees that \(Z P\). To simulate a DP mechanism with a conditional distribution \(P_{Z|X}\) using the Poisson functional representation, we can use \((Z_{i})_{i}\) as the shared randomness between the encoder and the decoder. 5 Upon observing \(X\), the encoder generates the Poisson process \((T_{i})_{i}\), computes \(_{i}\) and \(K\) using \(P=P_{Z|X}\), and transmits \(K\) to the decoder. The decoder simply outputs \(Z_{K}\), which follows the conditional distribution \(P_{Z|X}\). The issue is that \(K\) is a function of \(X\) and the shared randomness \((Z_{i},T_{i})_{i}\), and a change of \(X\) may affect \(K\) in a deterministic manner, and hence this method cannot be directly used to protect the privacy of \(X\).

**Poisson private representation.** To ensure privacy, we introduce randomness in the encoder by a generalization of the Poisson functional representation, which we call _Poisson private representation (PPR)_ with parameter \((1,]\), proposal distribution \(Q\) and the simulated mechanism \(P_{Z|X}\). Both \(X\) and \(Z\) can be discrete or continuous, though as a regularity condition, we require \(P_{Z|X}(|X)\) to be absolutely continuous with respect to \(Q\) almost surely. The PPR-compressed mechanism is given as:

1. We use \((Z_{i})_{i=1,2,}\), \(Z_{i}}{{}}Q\) as the shared randomness between the encoder and the decoder. Practically, the encoder and the decoder can share a random seed and generate \(Z_{i}}{{}}Q\) from it using a pseudorandom number generator.6 2. The encoder knows \((Z_{i})_{i},X,P_{Z|X}\) and performs the following steps: 1. Generates the Poisson process \((T_{i})_{i}\) with rate \(1\). 2. Computes \(_{i}:=T_{i}(P}{Q}(Z_{i}))^{-1}\), where \(P:=P_{Z|X}(|X)\). Take \(_{i}=\) if \(P}{Q}(Z_{i})=0\). 3. Generates \(K_{+}\) using local randomness with \[(K=k)=_{k}^{-}}{_{i=1}^{}_{i}^{- }}.\] 4. Compress \(K\) (e.g., using Elias delta coding ) and sends \(K\). 3. The decoder, which knows \((Z_{i})_{i},K\), outputs \(Z=Z_{K}\).

Note that when \(=\), we have \(K=_{i}_{i}\), and PPR reduces to the original Poisson functional representation . PPR can simulate the privacy mechanism \(P_{Z|X}\) precisely, as shown in the following proposition. The proof is in Appendix A.

**Proposition 4.2**.: _The output \(Z\) of PPR follows the conditional distribution \(P_{Z|X}\) exactly._

Due to the _exactness_ of PPR, it guarantees unbiasedness for tasks such as DME. If the goal is only to design a stand-alone privacy mechanism, we can focus on the privacy and utility of the mechanism without studying the output distribution. However, if the output of the mechanism is used for downstream tasks (e.g., for DME, after receiving information from clients, the server sends information about the aggregated mean to data analysts, where central DP is crucial), having an exact characterization of the conditional distribution of the output given the input allows us to obtain precise (central) privacy and utility guarantees.

Notably, PPR is _universal_ in the sense that only the encoder needs to know the simulated mechanism \(P_{Z|X}\). The decoder can decode the index \(K\) as long as it has access to the shared randomness \((Z_{i})_{i}\). This allows the encoder to choose an arbitrary mechanism \(P_{Z|X}\) with the same \(\), and adapt the choice of \(P_{Z|X}\) to the communication and privacy constraints without explicitly informing the decoder which mechanism is chosen.

Practically, the algorithm cannot compute the whole infinite sequence \((_{i})_{i}\). We can truncate the method and only compute \(_{i},,_{N}\) for a large \(N\) and select \(K\{1,,N\}\), which incurs a small distortion in the distribution of \(Z\).7 While this method is practically acceptable, it might defeat the purpose of having an exact algorithm that ensures the correct conditional distribution \(P_{Z|X}\). In Appendix B, we will present an exact algorithm for PPR that terminates in a finite amount of time, using a reparametrization that allows the encoder to know when the optimal point \(Z_{i}\) has already been encountered (see Algorithm 1 in Appendix B).

By the lower bound for channel simulation , we must have \(H(K) I(X;Z)\), i.e., the compression size is at least the mutual information between the data \(X\) and the output \(Z\). The following result shows that the compression provided by PPR is "almost optimal", i.e., close to the theoretical lower bound \(I(X;Z)\). The proof is given in Appendix F.

**Theorem 4.3** (Compression size of PPR).: _For PPR with parameter \(>1\), when the encoder is given the input \(x\), the message \(K\) given by PPR satisfies_

\[[ K] D(P\|Q)+((3.56))/\{(-1)/2,1\},\]

_where \(P:=P_{Z|X}(|x)\). As a result, when the input \(X P_{X}\) is random, taking \(Q=P_{Z}\), we have_

\[[ K] I(X;Z)+((3.56))/\{(-1)/2,1\}.\]

Note the running time complexity (which depends on the number of samples \(Z_{i}\) the algorithm must examine before outputting the index \(K\)) can be quite high. Since \([ K] I(X;Z)\), \(K\) (and hence the running time) is at least exponential in \(I(X;Z)\). See more discussions in Section 8.

If a prefix-free encoding of \(K\) is required, then the number of bits needed is slightly larger than \(_{2}K\). For example, if Elias delta code  is used, the expected compression size is \([_{2}K]+2_{2}([_{2}K]+1)+1\) bits. If the Shannon code  (an almost-optimal prefix-free code) for the Zipf distribution \(p(k) k^{-}\) with \(=1+1/[_{2}K]\) is used, the expected compression size is \([_{2}K]+_{2}([_{2}K]+1)+2\) bits (see ). Both codes yield an \(I(X;Z)+O( I(X;Z))\) size, within a logarithmic gap from the lower bound \(I(X;Z)\). This is similar to some other channel simulation schemes such as , though these schemes do not provide privacy guarantees.

Note that if \(P_{Z|X}\) is \(\)-DP, then by definition, for any \(z\) and \(x,x_{0}\), it holds that

\[D(P_{Z|X=x}P_{Z|X=x_{0}})=_{Z P_{Z|X=x}}[ (P_{Z|X=x}}{P_{Z|X=x_{0}}}(Z))]  e.\]

Setting the proposal distribution \(Q=P_{Z|X=x_{0}}\) for an arbitrary \(x_{0}\) gives the following bound.

**Corollary 4.4** (Compression size under \(\)-LDP).: _Let \(P_{Z|X}\) satisfy \(\)-differential privacy. Let \(x_{0}\) and \(Q=P_{Z|X=x_{0}}\). Then for PPR with parameter \(>1\), the expected compression size is at most \(+_{2}(+1)+2\) bits, where \(:=_{2}e+(_{2}(3.56))/\{(-1)/2,1\}\)._

Next, we analyze the privacy guarantee of PPR. The PPR method induces a conditional distribution \(P_{(Z_{i})_{i},K|X}\) of the knowledge of the decoder \(((Z_{i})_{i},K)\), given the data \(X\). To analyze the privacy guarantee, we study whether the randomized mapping \(P_{(Z_{i})_{i},K|X}\) from \(X\) to \(((Z_{i})_{i},K)\) satisfies \(\)-DP or \((,)\)-DP. 8 This is similar to the privacy condition in , and is referred as _decoder privacy_ in , which is stronger than _database privacy_ which concerns the privacy of the randomized mapping from \(X\) to the final output \(Z\) (which is simply the privacy of the original mechanism \(P_{Z|X}\) to be compressed since PPR simulates \(P_{Z|X}\) precisely). Since the decoder knows \(((Z_{i})_{i},K)\), more than just the final output \(Z\), we expect that the PPR-compressed mechanism \(P_{(Z_{i})_{i},K|X}\) to have a worse privacy guarantee than the original mechanism \(P_{Z|X}\), which is the price of having a smaller communication cost. The following result shows that, if the original mechanism \(P_{Z|X}\) is \(\)-DP, then the PPR-compressed mechanism is guaranteed to be \(2\)-DP.

**Theorem 4.5** (\(\)-DP of PPR).: _If the mechanism \(P_{Z|X}\) is \(\)-differentially private, then PPR \(P_{(Z_{i})_{i},K|X}\) with parameter \(>1\) is \(2\)-differentially private._

Similar results also apply to \((,)\)-DP and metric DP.

**Theorem 4.6** (\((,)\)-DP of PPR).: _If the mechanism \(P_{Z|X}\) is \((,)\)-differentially private, then PPR \(P_{(Z_{i})_{i},K|X}\) with parameter \(>1\) is \((2,2)\)-differentially private._

**Theorem 4.7** (Metric privacy of PPR).: _If the mechanism \(P_{Z|X}\) satisfies \( d_{}\)-privacy, then PPR \(P_{(Z_{i})_{i},K|X}\) with parameter \(>1\) satisfies \(2 d_{}\)-privacy._

Refer to Appendices C and D for the proofs. In Theorem 4.5 and Theorem 4.6, PPR imposes a multiplicative penalty \(2\) on the privacy parameter \(\). This penalty can be made arbitrarily close to \(2\) by taking \(\) close to \(1\), which increases the communication cost (see Theorem 4.3). Compared to minimal random coding which has a factor \(2\) penalty in the DP guarantee [47; 71], the \(2\) factor in PPR is slightly larger, though PPR ensures exact simulation (unlike [47; 71] which are approximate). The method in  does not have a penalty on \(\), but the utility and compression size depends on computational hardness assumptions on the pseudorandom number generator, and there is no guarantee that the compression size is close to the optimum. In comparison, the compression and privacy guarantees of PPR are _unconditional_ and does not rely on computational assumptions.

In order to make the penalty of PPR close to \(1\), we have to consider \((,)\)-differential privacy, and allow a small failure probability, i.e., a small increase in \(\). The following result shows that PPR can compress any \(\)-DP mechanism into a \((, 0)\)-DP mechanism as long as \(\) is close enough to \(1\) (i.e., almost no inflation). More generally, PPR can compress an \((,)\)-DP mechanism into an \((, 2)\)-DP mechanism for \(\) close to \(1\). The proof is in Appendix E.

**Theorem 4.8** (Tighter \((,)\)-DP of PPR).: _If the mechanism \(P_{Z|X}\) is \((,)\)-differentially private, then PPR \(P_{(Z_{i})_{i},K|X}\) with parameter \(>1\) is \((+,\,2(+))\)-differentially private, for every \((0,1]\) and \((0,1/3]\) that satisfy \( e^{-4.2}^{2}/(-)+1\)._

## 5 Applications to Distributed Mean Estimation

We demonstrate the efficacy of PPR by applying it to distributed mean estimation (DME) . Note that private DME is the core sub-routine in various private and federated optimization algorithms, such as DP-SGD  or DP-FedAvg .

Consider the following general distributed setting: each of \(n\) clients holds a local data point \(X_{i}\), and a central server aims to estimate a function of all local data \((X^{n})\), subject to privacy and local communication constraints. To this end, each client \(i\) compresses \(X_{i}\) into a message \(Z_{i}_{n}\) via a local encoder, and we require that each \(Z_{i}\) can be encoded into a bit string with an expected length of at most \(b\) bits. Upon receiving \(Z^{n}:=(Z_{1},,Z_{n})\), the central server decodes it and outputs a DP estimate \(\). Two DP criteria can be considered: the \((,)\)-central DP of the randomized mapping from \(X^{n}\) to \(\), and the \((,)\)-local DP of the randomized mapping from \(X_{i}\) to \(Z_{i}\) for each client \(i\).

In the distributed \(L_{2}\) mean estimation problem, \(=_{d}(C):=\{v^{d}\,\,v _{2} C\}\), and the central server aims to estimate the sample mean \((X^{n}):=_{i=1}^{n}X_{i}\) by minimizing the mean squared error (MSE) \([\|-\|_{2}^{2}]\). It is recently proved that under \(\)-local DP, privUnit [8; 4] is the optimal mechanism. By simulating privUnit with PPR and applying Corollary 4.4 and Theorem 4.6, we immediately obtain the following corollary:

**Corollary 5.1** (PPR simulating privUnit).: _Let \(P\) be the density defined by \(\)-privUnit\({}_{2}\) Bihownick et al. [8; Algorithm 1]. Let \(Q\) be the uniform density over the sphere \(^{d-1}(1/m)\) where the radius \(1/m\) is defined in Bhowmick et al. [8; (15)]. Let \(r^{*}:=e^{}\). Then the outcome of PPR (see Algorithm 1) satisfies (1) \(2\)-local DP; and (2) \((+,2)\)-DP for any \( e^{-4.2}^{2}/(1/)+1\). In addition, the average compression size is at most \(+_{2}(+1)+2\) bits where \(:=+(_{2}(3.56))/\{(-1)/2,1\}\). Moreover, PPR achieves the same MSE as \(\)-privUnit\({}_{2}\), which is \(O(d/(,^{2}))\)._

Note that PPR can simulate arbitrary local DP mechanisms. However, we present only the result of privUnit\({}_{2}\) because it achieves the optimal privacy-accuracy trade-off. Besides simulating local DP mechanisms, PPR can also compress central DP mechanisms while still preserving some (albeit weaker) local guarantees. We give a corollary of Theorems 4.3 and 4.6. The proof is in Appendix H.

**Corollary 5.2** (PPR-compressed Gaussian mechanism).: _Let \(,(0,1)\). Consider the Gaussian mechanism \(P_{Z|X}(|x)=(x,}{n}_{d})\), and the proposal distribution \(Q=(0,(}{d}+}{n})_{d})\), where \(}{}\). For each client \(i\), let \(Z_{i}\) be the output of PPR applied on \(P_{Z|X}(|X_{i})\). We have:_

* \((Z^{n}):=_{i}Z_{i}\) _yields an unbiased estimator of_ \((X^{n})=_{i=1}^{n}X_{i}\) _satisfying_ \((,)\)_-central DP and has MSE_ \([\|-\|_{2}^{2}]=^{2}d/n^{2}\)_._
* _As long as_ \(<1/\)_, PPR satisfies_ \((2,2)\)_-local DP__._9__ * _The average per-client communication cost is at most_ \(+_{2}(+1)+2\) _bits where_ \[:=_{2}n}{d^{2}}+1+_{ }\ \ _{2}}{2d(1.25/ )}+1+_{},\] _where_ \(_{}:=(_{2}(3.56))/\{(-1)/2,\,1\}\)_._

A few remarks are in order. First, notice that when \(\) is fixed, for an \(O(d}{n^{2}^{2}}(1/))\) MSE, the per-client communication cost is

\[Od}{d(1/)}+1+1,\]

which is at least as good as the \(O(n^{2}/(1/)+1)\) bound in [75; 19], and can be better than \(O(n^{2}/(1/)+1)\) when \(n d\). Hence, the PPR-compressed Gaussian mechanism is order-wise optimal. Second, compared to other works that also compress the Gaussian mechanism, PPR is the only lossless compressor; schemes based on random sparsification, projection, or minimum random coding (e.g., Triastcyn et al. , Chen et al. ) are _lossy_, i.e., they introduce additional distortion on top of the DP noise. Finally, other DP mechanism compressors tailored to local randomizers [31; 71] do not provide the same level of central DP guarantees when applied to local Gaussian noise since the reconstructed noise is no longer Gaussian. Refer to Section 7 for experiments.

## 6 Applications to Metric Privacy

Metric privacy [16; 3] (see Definition 3.2) allows users to send privatized version \(Z^{d}\) of their data vectors \(X^{d}\) to an untrusted server, so that the server can know \(X\) approximately but not exactly. A popular mechanism is the _Laplace mechanism_[16; 3; 33; 34], where a \(d\)-dimensional Laplace noise is added to \(X\). The conditional density function of \(Z\) given \(X\) is \(f_{Z|X}(z|x) e^{- d_{X}(x,z)}\), where \(\) is the privacy parameter, and the metric \(d_{X}(x,z)=\|x-z\|_{2}\) is the Euclidean distance. The Laplace mechanism achieves \( d_{}\)-privacy, and has been used, for example, in geo-indistinguishability to privatize the users' locations , and to privatize high-dimensional word embedding vectors [33; 34].

A problem is that the real vector \(Z\) cannot be encoded into finitely many bits. To this end,  studies a _discrete Laplace mechanism_ where each coordinate of \(Z\) is quantized to a finite number of levels, introducing additional distortion to \(Z\). PPR provides an alternative compression method that preserves the statistical behavior of \(Z\) (e.g., unbiasedness) exactly. We give a corollary of Theorems 4.3 and 4.7. The proof is in Appendix I. Refer to Appendix J for an experiment on metric privacy.

**Corollary 6.1** (PPR-compressed Laplace mechanism).: _Consider PPR applied to the Laplace mechanism \(P_{Z|X}\) where \(X_{d}(C)=\{x^{d}\,|\,\|x\|_{2} C\}\), with a proposal distribution \(Q=(0,(}{d}+})_{d})\). It achieves an MSE \(}\), a \(2 d_{}\)-privacy, and a compression size at most \(+_{2}(+1)+2\) bits, where_

\[:=_{2}(^{2}}{d }+d+1)-_{2}+1)}+_{ },\]

_where \(_{}:=(_{2}(3.56))/\{(-1)/2,\,1\}\)._Empirical Results

We empirically evaluate our scheme on the DME problem (which is formally introduced in Section 5), examine the privacy-accuracy-communication trade-off, and compare it with the Coordinate Subsampled Gaussian Mechanism (CSGM) [19, Algorithm 1], an order-optimal scheme for DME under central DP. In Chen et al. , each client only communicates partial information (via sampling a subset of the coordinates of the data vector) about its samples to amplify the privacy, and the compression is mainly from subsampling. Moreover, CSGM only guarantees central DP.

We use the same setup that has been used in : consider \(n=500\) clients, and the dimension of local vectors is \(d=1000\), each of which is generated according to \(X_{i}(j)}}{{}}(2(0.8)-1)\), where \((0.8)\) is a Bernoulli random variable with parameter \(p=0.8\). We require \((,)\)-central DP with \(=10^{-6}\) and \([0.05,6]\) and apply the PPR with \(=2\) to simulate the Gaussian mechanism, where the privacy budgets are accounted via Renyi DP.

We compare the MSE of PPR (\(=2\), using Theorem 4.3) and CSGM under various compression sizes in Figure 1 (the \(y\)-axis is in logarithmic scale).10 Note that the MSE of the (uncompressed) Gaussian mechanism coincides with the CSGM with \(1000\) bits, and the PPR with only \(400\) bits. We see that PPR consistently achieves a smaller MSE compared to CSGM for all \(\)'s and compression sizes considered. For \(=1\) and we compress \(d=1000\) to \(50\) bits, CSGM has an MSE \(0.1231\), while PPR has an MSE \(0.08173\), giving a \(33.61\%\) reduction. For \(=0.5\) and we compress \(d=1000\) to \(25\) bits (the case of high compression and conservative privacy), CSGM has an MSE \(0.3877\), while PPR has an MSE \(0.3011\), giving a \(22.33\%\) reduction. These reductions are significant, since all considered mechanisms are asymptotically close to optimal and a large improvement compared to an (almost optimal) mechanism is unexpected. See Section L for more about MSE against the compression sizes.

We also emphasize that PPR provides _both_ central and local DP guarantees according to Theorem 4.5, 4.6 and 4.8. In contrast, CSGM only provides central DP guarantees. Another advantage of PPR under conservative privacy (small \(\)) is that the trade-off between \(\) and MSE of PPR exactly coincides with the trade-off of the Gaussian mechanism for small \(\) (see Figure 1), and CSGM is only close to (but strictly worse than) the Gaussian mechanism. This means that for small \(\), PPR provides compression without any drawback in terms of \(\)-MSE trade-off compared to the Gaussian mechanism (which requires an infinite size communication to exactly realize).

Moreover, although directly applying PPR on the \(d\)-dimensional vectors is impractical for a large \(d\), one can ensure an efficient \(O(d)\) running time (see Section 8 for details) by breaking the vector with \(d=1000\) dimensions into small chunks of fixed lengths (we use \(d_{}=50\) dimensions for each chunk), and apply the PPR to each chunk. We call it the _sliced PPR_ in Figure 1. Though the sliced PPR has a small penalty on the MSE (as shown in Figure 1), it still outperforms the CSGM (\(400\) bits) for the range of \(\) in the plot. For the sliced PPR for one \(d=1000\) vector, when \(=0.05\), the running time is \(1.3348\) seconds on average.11 For larger \(\)'s, we can choose smaller \(d_{}\)'s to have reasonable running time: For \(=6\) and \(d_{}=2\) we have an average running time \(0.0127\) seconds and with \(d_{}=4\) we have an average running time \(0.6343\) seconds; for \(=10\) and \(d_{}=2\) we have an average running time \(0.0128\) seconds and with \(d_{}=4\) we have an average running time \(0.7301\) seconds. See Appendix K for more experiments on the running time of the sliced PPR.

## 8 Limitations

While PPR is communication-efficient, having only a logarithmic gap from the theoretical lower bound on the compression size as shown in Theorem 4.3, the running time complexity can be high. However, we note that an exponential complexity is also needed in sampling methods that do not ensure privacy, such as . It has been proved in  that no polynomial time general sampling based method exists (even without privacy constraint), if \(RP NP\). All existing polynomial time exact channel simulation methods can only simulate specific noisy channels.12 Hence, a polynomial time algorithm for exactly compressing a general DP mechanism is likely nonexistent.

Nevertheless, this is not an obstacle for simulating local DP mechanisms, since the mutual information \(I(X;Z)\) for a reasonable local DP mechanism must be small, or else the leakage of the data \(X\) in \(Z\) would be too large. For an \(\)-local DP mechanism, we have \(I(X;Z)\{,^{2}\}\) (in nats) . Hence, the PPR algorithm can terminate quickly even if has a running time exponential in \(I(X;Z)\).

Another way to ensure a polynomial running time is to divide the data into small chunks and apply the mechanism to each chunk separately. For example, to apply the Gaussian mechanism to a high-dimensional vector, we break it into several shorter vectors and apply the mechanism to each vector. Experiments in Section 7 show that this greatly reduces the running time while having only a small penalty on the compression size. See Appendix K for experiments on the running time of PPR.

## 9 Conclusion

We proposed a novel scheme for compressing DP mechanisms, called Poisson private representation (PPR). Unlike previous schemes which are either constrained on special classes of DP mechanisms or introducing additional distortions on the output, our scheme can compress and exactly simulate arbitrary mechanisms while protecting differential privacy, with a compression size that is close to the theoretic lower bound. A future direction is to reduce the running time of PPR under certain restrictions on \(P_{Z|X}\). For example, the techniques in [38; 35] may be useful when \(P_{Z|X}\) is unimodal.