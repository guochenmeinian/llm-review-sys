# Scaling Laws for Hyperparameter Optimization

Arlind Kadra

Representation Learning Lab

University of Freiburg

kadraa@cs.uni-freiburg.de &Maciej Janowski

Representation Learning Lab

University of Freiburg

janowski@cs.uni-freiburg.de &Martin Wistuba

Amazon Web Services

Amazon Berlin

marwistu@amazon.com &Josif Grabocka

Representation Learning Lab

University of Freiburg

grabocka@cs.uni-freiburg.de

###### Abstract

Hyperparameter optimization is an important subfield of machine learning that focuses on tuning the hyperparameters of a chosen algorithm to achieve peak performance. Recently, there has been a stream of methods that tackle the issue of hyperparameter optimization, however, most of the methods do not exploit the dominant power law nature of learning curves for Bayesian optimization. In this work, we propose Deep Power Laws (DPL), an ensemble of neural network models conditioned to yield predictions that follow a power-law scaling pattern. Our method dynamically decides which configurations to pause and train incrementally by making use of gray-box evaluations. We compare our method against 7 state-of-the-art competitors on 3 benchmarks related to tabular, image, and NLP datasets covering 59 diverse tasks. Our method achieves the best results across all benchmarks by obtaining the best any-time results compared to all competitors.

## 1 Introduction

Hyperparameter Optimization (HPO) is a major challenge for the Machine Learning community. Unfortunately, HPO is not yet feasible for Deep Learning (DL) methods due to the high cost of evaluating multiple configurations. Recently multi-fidelity HPO (a sub-problem of gray-box HPO) has emerged as a promising paradigm for HPO in DL, by discarding poorly-performing hyperparameter configurations after observing the validation error on the low-level fidelities of the optimization procedure . The advantage of multi-fidelity HPO compared to online HPO , or meta-gradient HPO  is the ability to tune all types of hyperparameters.

In recent years, a stream of papers highlights the fact that the performance of DL methods is predictable , concretely, that the validation error rate is a power law function of the model size, or dataset size . Such a power law relationship has been subsequently validated in the domain of NLP, too . In this paper, we demonstrate that the power-law principle has the potential to be a game-changer in HPO, because we can evaluate hyperparameter configurations in low-budget regimes (e.g. after a few epochs), then estimate the performance on the full budget using dataset-specific power law models. Concretely, we hypothesize and empirically demonstrate that optimization curves (training epochs versus accuracy, or loss) can be efficiently modeled as simple power law functions.

As a result, we introduce Deep Power Law (DPL) ensembles, a probabilistic surrogate for Bayesian optimization (BO) that estimates the performance of a hyperparameter configuration at future budgets using ensembles of deep power law functions. Subsequently, our novel formulation of BO dynamically decides which configurations to pause and train incrementally by relying on the performanceestimations of the surrogate. We demonstrate that our method achieves the new state-of-the-art in HPO for DL by comparing against 7 strong HPO baselines, and 59 datasets of three diverse modalities (tabular, image, and natural language processing). Our experimental protocol contains state-of-the-art deep learning architectures such as Transformers , XFormer , ResNeXt  and large language models like GPT-2 . As a result, we believe the proposed method has the potential to finally make HPO for DL a feasible reality.

Overall, our contributions can be summarized as follows:

* We introduce a novel probabilistic surrogate for multi-fidelity HPO based on ensembles of deep power law functions.
* We derive a simple mechanism to combine our surrogate with Bayesian optimization.
* Finally, we demonstrate the empirical superiority of our method against the current state-of-the-art in HPO for Deep Learning, with a large-scale HPO experimental protocol.

## 2 Related Work

**Multi-fidelity HPO** assumes a method has access to the learning curve of a hyperparameter configuration. Such a learning curve is the function that maps either training time or dataset size, to the validation performance. The early performance of configurations (i.e. first segment of the learning curve) is used to discard unpromising configurations, before waiting for full convergence. Successive halving  is a widely used multi-fidelity method that randomly samples hyperparameter configurations, starts evaluating them, and ends a fraction of them upon reaching a predefined budget. Afterwards, the budget is divided by the fraction of discarded hyperparameter configurations and the process continues until the maximum budget is reached. Although the method relies only on the last observed value of the learning curve, it is very efficient. In recent years, various flavors of successive halving have been suggested, including Hyperband , which effectively runs successive halving in parallel with different settings. A major improvement to Hyperband is replacing random search with a more efficient sampling strategy . A more efficient approach is to allocate the budget dynamically to the most promising configurations . However, the only assumption these methods make about the learning curve is that it will improve over time, while recent work  exploits a power law assumption on the curves. Similarly, we fit surrogates that exploit a power law assumption, however, our method is able to estimate the performance of unobserved configurations through probabilistic surrogates learned jointly for all hyperparameter configurations.

**Learning curve prediction** is a related topic, where the performance of a configuration is predicted based on a partially observed learning curve . Typically, the assumptions about the learning curve are much stronger than those described above. The prediction is often based on the assumption that the performance increases at the beginning and then flattened towards the end. One way to model this behavior is to define a weighted set of parametric functions . Then, the parameters of all functions are determined so that the resulting prediction best matches the observed learning curve. Another approach is to use learning curves from already evaluated configurations and to find an affine transformation that leads to a well-matched learning curve . A more data-driven approach is to learn the typical learning curve behaviour directly from learning curves across different datasets . Learning curve prediction algorithms can be combined with successive halving . In contrast to this research line, we fit ensembles of power law surrogates for conducting multi-fidelity HPO with Bayesian optimization.

**Scaling laws** describe the relationship between the performance of deep learning models as a function of dataset size or model size as a power law . Another work tunes hyperparameters on a small-scale model and then transfers them to a large-scale version . In contrast to these papers, we directly use the power law assumption for training surrogates in Bayesian optimization for HPO.

## 3 Preliminaries

**Hyperparameter Optimization (HPO)** demands finding the configurations \(\) of a Machine Learning method that achieve the lowest validation loss \(^{()}\) of a model (e.g. a neural network), which is parameterized with \(\) and learned to minimize the training loss \(^{()}\) as:\[^{*}:=*{arg\,min}_{}\ \ ^{(Val)} (,^{*}()),\ \ \ \ \ \ ^{*}():=*{arg\,min}_{}\ ^{(Train)}(,) \]

For simplicity, we denote the validation loss as our function of interest \(f():=^{(Val)}(,^{*}())\). The optimal hyperparameter configurations \(^{*}\) of Equation 1 are found via **an HPO policy**\(\) (also called an HPO method) that given a history of \(N\) evaluated configurations \(H^{(N)}:=\{_{i},f(_{i})\}_{i=1}^{N}\) suggests the \((N+1)\)-th configuration to evaluate as \(_{N+1}:=(H^{(N)})\) where \(:[_{+}]^{N}\). The search for an optimal HPO policy is a bi-objective problem in itself, aiming at (i) finding a configuration out of \(N\) evaluations that achieves the smallest validation loss \(f()\), and (ii) ensuring that the costs of evaluating the \(N\) configurations do not exceed a total budget \(\), as shown in Equation 2.

\[*{arg\,min}_{} \ _{i\{1,,N\}}f(_{i}:= (H^{(i-1)})), \] \[\ \ \ \ \ H^{(i)}:=\{(_{j},f(_{j}))\}_{j=1}^{i}&i>0\\ &i=0,\] \[\ \ \ _{i=1}^{N}(f(_{i}))\]

**Bayesian optimization (BO)** is the most popular type of policy for HPO, due to its ability to balance the exploration and exploitation aspects of minimizing the validation loss \(f\) in terms of hyperparameters \(\). Technically speaking, BO fits a surrogate \((;)\) parametrized with \(\) to approximate the observed loss \(f()\) using the history \(H^{(N)}\), as \(^{*}:=*{arg\,max}_{}_{(,f( )) p_{H^{(N)}}}\ p(f()|,)\), where, \(p\) represents the probability. Afterwards, BO uses an acquisition/utility function \(a:_{+}\) to recommend the next configuration as \(_{N+1}:=(H^{(N)})=*{arg\,max}_{ }a((;^{*}))\).

**Multi-fidelity HPO** refers to the case where an approximation of the validation loss can be measured at a lower budget \(b B\), where \(B:=(0,b_{}]\). For instance, in Deep Learning we can measure the validation loss after a few epochs (\(0<b<\)), rather than wait for a full convergence (\(b=b_{}\)). Throughout this paper, the term budget refers to a learning curve step. The evaluation of a configuration \(\) for a budget \(b\) is defined as \(f(,b): B_{+}\).

## 4 Power Law Surrogates for Bayesian Optimization

Prior work has demonstrated that the performance of Machine Learning methods as a function of budgets (i.e. dataset size, number of optimization epochs, model size, image resolution) follows a power law relationship [41; 40; 11; 20; 15]. In this work, we employ this power law dependence between the validation loss and the number of optimization epochs in Deep Learning. We propose a novel multi-fidelity Hyperparameter Optimization method which is based on power law surrogates. We assume that every learning curve \(f(,)\) can be described by a power law function defined by \((,,)\). Concretely, we define a power law function for the validation loss of a configuration \(\) at a budget \(b\) (a.k.a. the number of epochs) as shown in Equation 3.

\[(,b):=_{}+_{}\ b^{-_{ }},\ \ _{},_{},_{} \]

Instead of fitting one separate power law function to each learning curve, we fit a single **shared power law function** across all configurations by conditioning the power law coefficients \(,,\) on \(\) using a parametric neural network \(g\) that maps a configuration to the power law coefficients of its learning curve as \(g:^{3}\). The network \(g\) has three output nodes, corresponding to the power law coefficients, denoted as \(g()_{},g()_{},g()_{}\), as defined in Equation 4.

\[(,b):=g()_{}+g()_{}\ b^{-g( )_{}},\ \ g:^{3} \]Using a history of learning curve evaluations \(H^{(N)}:=\{(_{i},b_{i},f(_{i},b_{i}))\}_{i=1}^{N}\) we can train the power law surrogate to minimize the following loss function using stochastic gradient descent:

\[*{arg\,min}_{f}\ _{(,b,f(,b)) p_{H^{(N)}}} \ f(_{i},b_{i})-(_{i},b_{i})  \]

BO surrogates need to be probabilistic regression models because the acquisition functions require the posterior variance of the predictions. As a result, we train an ensemble of \(K\) diverse surrogates \(^{(1)}(,b),,^{(K)}(,b)\) by initializing each surrogate with different weights and by training with a different sequence of mini-batches as in prior work . The posterior mean \(\) and the posterior variance \(^{2}\) of the power law ensemble are trivially computed as:

\[_{}(,b):=_{k=1}^{K}^{(k)}(,b), ^{2}_{}(,b):=_{k=1}^{K} ^{(k)}(,b)-_{}(,b)^{2} \]

A commonly used acquisition function in the domain is Expected Improvement (EI)  which incorporates both the mean and uncertainty of predictions, applying a trade-off between exploration and exploitation. Consequently, in our work, we use the EI acquisition with the estimated full budget's (\(b_{}\)) posterior mean and variance. We briefly define the acquisition function in Equation 7:

\[_{N+1} :=*{arg\,max}_{}\ \ (,b_{}|H^{(N)}), \] \[(,b_{}|H) :=_{f(,b_{}) (_{}(,b_{}),^{2}_{}(,b_{ }))}[\{(,b_{})-f( _{},b_{}),0\}]\]

where, \(f(_{},b_{})\) corresponds to the best observed loss for any budget \(b^{} b_{}\) from the history \(H^{(N)}\). After selecting a configuration with our variant of the EI acquisition, we do not naively run it until convergence. Instead, we propose a novel multi-fidelity strategy that advances the selected \(_{N+1}\) of Equation 7 by a small budget of \(b_{}\), e.g. 1 epoch of training. Therefore, the selected \(_{N+1}\) will be evaluated at \(b_{N+1}\) as defined in Equation 8. Notice, our proposed strategy also covers new configurations with no learning curve evaluations in \(H^{(N)}\).

\[b_{N+1}:=b_{},&_{N+1}:(_{N+1}, ,) H^{(N)}\\ b_{}+_{(_{N+1},b_{}) H^{(N)}}b,& \]

We provide the detailed pseudocode of our method at Algorithm 1.

## 5 Experimental Protocol

### Benchmarks

**LCBench:** A benchmark that features 2,000 hyperparameter configurations that parametrize the architecture of simple feedforward neural networks, as well as, the training pipeline . The benchmark features 7 numerical hyperparameters and 35 different datasets from the AutoML benchmark .

**PD1:** A deep learning benchmark  that consists of recent DL (including Transformers) architectures run on large vision datasets such as CIFAR-10, CIFAR-100, ImageNet, as well as statistical modeling corpora and protein sequence datasets from bioinformatics. Every search space includes varying learning curve lengths, ranging from 5 to 1414, and a different number of evaluated hyperparameter configurations ranging from 807 to 2807. The search space includes hyperparameter configurations that parametrize the learning rate, the learning rate scheduler, and the momentum.

**TaskSet:** A benchmark that features different optimization tasks evaluated in 5 different search spaces . For our work, we focus on the Adam8p search space, which is among the largest search spacesin the benchmark with 1000 hyperparameter configurations. Every hyperparameter configuration features 8 continuous hyperparameters. The hyperparameters control the learning rate, the learning rate schedulers, and the optimizer. For variety among our benchmarks, we focus on 12 RNN text classification tasks that feature different RNN cells, embedding sizes, and batch sizes.

For a more detailed explanation of the benchmarks, we refer the reader to Appendix F.

### Baselines

**Random Search** stochastically samples hyperparameter configurations for the largest possible budget. **Hyperband** uses multiple brackets with different trade-offs of the initial budget and number of epochs to initially train . It then applies Successive Halving (SH)  on every bracket. **ASHA** is an asynchronous version of SH  that does not wait for all configurations to finish in an SH bracket before starting the next one. Furthermore, **BOHB** is a follow-up of Hyperband that uses TPE  to sample the initial hyperparameter configurations of a bracket . **DEHB**, on the other hand, modifies Hyperband by using evolutionary strategies to sample the initial hyperparameter configurations . Similarly, multi-fidelity **SMAC** extends Hyperband but uses random forests to sample the initial hyperparameter configurations for a bracket . We also use the **Dragonfly** Library  to compare against BOCA , a multi-fidelity method that uses Gaussian Processes to predict the next hyperparameter to evaluate, as well as the fidelity for which it should be evaluated. For all the baselines, we use their official public implementations. We provide additional details in Appendix G.

```
Input : Search space \(\), initial design \(H^{()}\), budget increment \(b_{}\) Output : Best hyperparameter configuration \(^{*}\)
1 Iteration \(i 0\); Evaluate initial configurations and budgets \(H^{(i)}:=H^{()}\);
2whilestill budgetdo
3 Fit a DPL ensemble \(^{(1)}(,b),,^{(K)}(,b)\) from history \(H^{(i)}\) using Equation 5;
4 Recommend the next configuration \(_{i+1}\) and its budget \(b_{i+1}\) using Equation 6, 7 and 8;
5 Train \(_{i+1}\) until \(b_{i+1}\) and measure the validation loss \(f(_{i+1},b_{i+1})\);
6 Append to history \(H^{i+1} H^{i}\{(_{i+1},b_{i+1},f(_{i+1},b_{i+1 }))\}\);
7\(i i+1\) ;
8 end while return Best configuration \(^{*}\) with the smallest validation loss \(_{(^{*},b,f(^{*},b)) H^{i}}\ f(^{*},b)\) ;
```

**Algorithm 1**Multi-Fidelity HPO with Deep Power Laws

### Architecture & Training

For our method, we use an ensemble of 5 models, where every model consists of a 2-layer feedforward neural network with 128 units per layer and Leaky ReLU for the non-linearity. The architecture of our method is motivated by prior work . Our network has 3 output units, that are then combined with the budget \(b\) to yield the power law output. We apply the GLU non-linearity activation only on the \(\) and \(\) output units, allowing \(\) to take any value.

We use the L1 loss to train our network, coupled with Adam featuring an initial learning rate of \(10^{-3}\). For the first 10 iterations of our multi-fidelity HPO method in Algorithm 1 we train every network of our ensemble for 250 epochs with randomly sampled initial weights. This choice helps the convergence of the weights in the early stage of HPO. Next, we continuously refine the model for 20 epochs every HPO iteration. However, if the optimization stagnates (surrogate fitting loss does not improve) for more than the LC Length + a buffer of 0.2 \(\) LC Length, the training procedure is restarted with random weights, where every model is trained again for 250 epochs and then only refined. During the refining phase, the new data point at an HPO iteration (Line 9 at Algorithm 1) is sampled with repeat on every batch, to learn new data points equally compared to older data points. Since we are working with discrete search spaces, we evaluate the acquisition function exhaustively on every hyperparameter configuration. When dealing with continuous search spaces, the acquisition function can either be evaluated exhaustively on a discretized version of the search space, or in a gradient-based way. Our implementation of DPL is publicly available.1

### Protocol

In our experiments, we standardize the hyperparameter values by performing min-max scaling for our method and every baseline. If a baseline has a specific preprocessing protocol, we do not apply min-max scaling but we apply the protocol as suggested by the authors.

The benchmarks do not support a common evaluation metric for configurations (i.e. the function \(f\)). As a consequence, the evaluation metric for LCBench is the balanced accuracy, for TaskSet the log-likelihood loss, while for PD1 the accuracy. Moreover, the benchmarks do not offer learning curves with a common step size. For LCBench and PD1, one step size is equivalent to one epoch, while for TaskSet one step size is 200 batches. The HPO budget is defined as the maximum number of steps needed to fully evaluate 20 hyperparameter configurations. In that context, one unit step of the HPO budget signifies training a particular configuration for one more optimization step (e.g. 200 batches in TaskSet or 1 epoch in LCBench).

In the following experiments, for all methods, we report the regret of the best-found configuration as shown in Equation 9:

\[R=f(_{},b_{})-f(_{},b_{}) \]

where the oracle is given as \(f(_{},b_{}):=\{f( ,b)\;|\;(,b,f(,b)) H^{(D) } b b^{}\}\), and \(H^{(D)}\) corresponds to the set of all the exhaustively-evaluated hyperparameter configurations' performances on a dataset \(D\). If the oracle configuration is not known in advance for the search space, \(H^{(D)}\) can be replaced with the history \(H^{(N)}\) at the end of the HPO procedure. The only difference between \(f(_{},b_{})\) and \(f(_{},b_{})\) is that the former only considers the history at the HPO step for which we are reporting the results.

In short, the regret is the difference in the evaluation metric performance from the best-found hyperparameter configuration during optimization to the best possible hyperparameter configuration (oracle) on the dataset (in a minimization setting). On a dataset level, we report the average regret across 10 repetitions with different seeds. To be able to aggregate results over datasets, we report the averaged normalized regret. As normalization, we divide the regret by the difference between the performances of the best and the worst hyperparameter configuration on a dataset. Then we compute the mean of the normalized regrets across all the datasets of a benchmark.

Moreover, in the experiments that report the average normalized regret over time, we provide results over normalized wall clock time. The wall clock time includes both the method's overhead (i.e. training the surrogate \(\) and selecting the next hyperparameter configuration to evaluate) and the time taken to evaluate the selected hyperparameter configuration (i.e. evaluating \(f\)). Since the methods have different run times, we normalize the individual times by the time it took Random Search (the fastest non-model-based method) to complete the HPO optimization process. To provide a fair any-time comparison, we report results until the time it took Random Search to evaluate 20 hyperparameter configurations.

Furthermore, when reporting the learning curve (LC) length fraction, we imply the fraction of the total learning curve length. LCBench and TaskSet have LCs of a fixed length for all datasets, corresponding to 51 epochs for LCBench and 50 epochs for TaskSet. In contrast, PD1 has varying LC lengths for different datasets.

In our experiments, all methods start with a history \(H^{(init)}\) of 1 randomly sampled hyperparameter configuration evaluated for 1 step/epoch in the case of multi-fidelity techniques (Hyperband, BOHB, DEHB, SMAC, ASHA, Dragonfly; descriptions in Section 5.2), or for the full budget for the black-box technique (Random Search). We ran experiments on a CPU cluster, where every node contains two Intel Xeon E5-2630v4 CPUs with 20 CPU cores running at 2.2 GHz. The total memory of every node is 120GB, and every experiment is limited to 2 cores which offer 12GB.

## 6 Research Hypotheses and Experiments

**Hypothesis 1:** _The power law assumption improves the quality of learning curve forecasting._

In this experiment, we evaluate the predictive performance of forecasting models that given a fraction of the observed learning curve, estimate the remaining unobserved segment of the curve, on the LCBench benchmark. The results of Figure 1 compare three different forecasting models, concretely, neural networks (NN), Gaussian Processes (GP), and Power Law functions (PL). For the three variants (PL, NN, GP) we fitted one model on every learning curve of each hyperparameter configuration (i.e. given \(b\) in the x-axis estimate one \((b)\) separately for every \(\)). For the other two variants (DPL and Cond NN) we fit a single forecasting model (not an ensemble) for all configurations, by conditioning the surrogate on the configuration (i.e. given b and \(\), estimate \((,b)\)). The purpose of the experiment is to assess whether a power law function regressor leads to superior predictive accuracy, compared to generic forecasting models, such as neural networks, or Gaussian processes. The evaluation metric of the experiment highlighted in Figure 1 is the rank correlation between the estimated performances at the end of the learning curve and the true performances.

We notice that although a Power Law regressor has significantly fewer parameters than a neural network (3 to 288 parameters), PL still achieves higher predictive performance than NN. Furthermore, our conditioning of the power law function to the hyperparameter configuration further improves the predictive quality, as the difference between DLP and PL demonstrates.

Lastly, we refer the reader to Appendix H, where we provide an analysis of the distributions for the absolute error rate between the DPL predictions and the ground truth values over the different LC length fractions, showing that DPL does not only retain the ranks but, it also accurately predicts the final performance. Based on the results, we consider Hypothesis 1 to be validated and that **DPL is accurate in terms of learning curve forecasting**.

**What about learning curves that do not follow a power law pattern?** Although the provided empirical evidence in this section strongly suggests that the presented power law model can accurately forecast learning curves, it is also true that some learning curves have divergent behaviour that does not follow the power law assumption. As a consequence, we investigate two different ways to handle curves that do not follow the power law assumption: _i)_ recently-proposed power law functions that include breaking points , or shifts in the curve , and _ii)_ min-smoothing the learning curves to transform them into monotonically decreasing time series. In Appendix A we provide ample empirical evidence showing that although the alternative formulations achieve comparable performances, still they do not outperform our simpler power law formulation. The findings indicate that even though not all learning curves are power laws, most of them are, therefore a power law surrogate is efficient in forecasting the final performance of a partially-observed learning curve. As a result, the forthcoming experiments will provide further empirical evidence that our power law surrogates lead to state-of-the-art HPO results when deployed in the proposed multi-fidelity Bayesian optimization setup.

**Hypothesis 2:** _Our method DPL achieves state-of-the-art results in HPO._

In Figure 2, we show the performance of the considered methods over the HPO budget, where DPL manages to outperform all the rival baselines. In the case of LCBench, DPL quickly finds well-performing hyperparameter configurations compared to the competitor methods and continues to discover even better configurations until the HPO process ends. Furthermore, we observe the same trend with TaskSet and PD1, where after ca. 25% of the HPO budget, our method DPL converges to a better regret compared to the baselines and increases the lead until HPO ends. For a detailed overview of the performances of all methods on all individual datasets, we point the reader to Appendix H.

Figure 1: Rank correlations of learning curve forecasting models, which are given a fraction of the learning curve and estimate the remaining curve segment. **DPL**: Deep Power Law, **Cond NN**: Conditioned neural network, **PL**: Power Law, **NN**: Neural Network, **GP**: Gaussian processes.

In addition, Figure 3 provides the critical difference diagrams of the per-dataset regret ranks at 50% and 100% of the HPO budget. Our method DPL outperforms all baselines in 5 out of 6 cases (in 4 of which with a statistically significant margin), while being second best only at the 50% of the HPO budget on the PD1 benchmark. We investigate the lack of statistical significance in PD1, by analyzing the individual dataset performances where DPL performs worse compared to other baselines. We notice that the datasets have a skewed distribution of hyperparameter configuration performances, where, a majority of the configurations achieve top performance. Based on the results, we conclude that a lack of statistical significance is the outcome of a search space that includes relatively simple optimization tasks and not a specific failure state of our method. We provide the detailed results of our analysis in Appendix D.

Lastly, we analyse the performance of DPL over time in Figure 4. As it can be observed, DPL manages to outperform the competitors even when the method's overhead time is included, showing that the overhead of DPL (i.e. fitting surrogate and running the acquisition) is negligible in terms of the quality of the HPO results. For a more detailed information, regarding the DPL overhead time, we point to Appendix E. TaskSet is not included in Figure 4 since the benchmark does not offer runtimes. Given the results, we conclude that Hypothesis 2 is validated and that **DPL achieves state-of-the-art performance in HPO**.

Figure 4: The average normalized regret of DPL and rival methods over the normalized time for all the considered benchmarks. Solid curves and shaded regions represent the mean and standard average normalized regret.

Figure 3: The critical difference diagrams at 50% and 100% of the HPO budget. The ranks indicate the sorted position in terms of regret, aggregated across datasets (the lower the better). Thick horizontal lines highlight differences that are not statistically significant.

Figure 2: DPL discovers better hyperparameter configurations than all rival baselines in terms of regret (distance to oracle). Solid curves and shaded regions represent the mean and standard error of the averaged normalized regret.

**Hypothesis 3:**_DPL explores the search space more efficiently compared to the baselines._

We conduct further analyses to understand the source of the efficiency of DPL versus the baselines. As a result, we analyze two important aspects, the quality of the evaluated configurations, as well as the exploration capability of our multi-fidelity HPO method. Initially, we measure what fraction of the top 1% configurations (ranked by accuracy) can our method discover. Figure 5 (left) shows that until convergence our method can discover significantly more top configurations compared to the baselines. The middle plots of Figure 5, show the average regret for each configuration promoted to the respective budget. According to the plot, DPL is more efficient and assigns the budget only to configurations with lower regret compared to the other methods. The precision and regret plots demonstrate that the quality of the evaluated configurations is largely better than all baselines, therefore, giving our method a significant lift in the performance rank. Last but not least, the right plot shows the percentage of configurations that were performing poorly in an earlier epoch (i.e. accuracy-wise in the bottom \(2/3\) of configurations up to the epoch indicated at the x-axis) but performed better at later epochs (i.e. at the top \(1/3\) of configurations). Furthermore, we added a line labeled with "Baseline", which represents the fraction of previously poor-performing configurations of all configurations. This behavior is observed often with learning curves, where for instance, strongly regularized networks converge slowly. For the same analysis regarding the PD1 benchmark, we point the reader to Appendix H.

The results indicate that our method explores well the unpromising early configurations, by considering them through the uncertainty estimation of our ensemble and the respective Bayesian optimization mechanism. The results validate Hypothesis 3 and confirm that **DPL explores the search space more efficiently.**

**Hypothesis 4:**_Our method DPL offers an effective tool for HPO in Large Language Models._

In this experiment, we consider the case of tuning the hyperparameters of transformers in Large Language Models. To this end, we computed a tabular benchmark by training a smaller GPT-2  model on the OpenWebText dataset  for a series of different hyperparameter configurations. We tune three learning rate hyperparameters: the fraction of warmup steps, the maximum learning rate at the end of warmup, and the minimum learning rate at the end of the decay. We repeat the experiments for seven model sizes ranging from 0.3M to 30M total parameters, ablating the embedding size of the multi-head attention layers (details in Appendix B).

We follow the common practice of conducting HPO with small transformers and then deploying the discovered optimal configuration on the full-scale transformers . As a result, we search for the optimal hyperparameters of small transformers (embedding size of \(\{6,12,,96,192\}\)) and then evaluate the discovered configurations at a full-scale transformer with an embedding size of \(384\).

Figure 5: Post-hoc analysis to study DPL’s efficiency. **Left:** Share of the best candidates selected during training. **Middle:** Average regret of configurations chosen to be trained at each budget. **Right:** Share of top third configurations at a given budget which were bottom two third configurations at a previous budget.

Figure 6 shows the HPO results of DPL against Random Search and BOHB (a rival multi-fidelity HPO baseline). In the top row of plots, we observe the performance of the discovered configurations at the small transformers for the indicated embedding size. We observe that our method finds better configurations than the baselines at any proxy space with small embedding sizes.

On the other hand, the bottom row of plots presents the performance of the discovered configurations in the small embedding space, by applying such hyperparameter configurations to the full-scale transformers. We observe that the configurations discovered by DPL on the small search space achieve very competitive results on the full-scale transformers, finding the oracle configuration of the full-scale transformers in the majority of cases. It takes DPL 3.6 hours to find the oracle configuration for the largest model via HPO for the smallest model. In turn, it takes 21.52 hours to train the largest model only once. For more details, we refer the reader to Appendix B. The results validate Hypothesis 4 and confirm that **DPL is an efficient HPO technique for tuning the hyperparameters of large language models when the HPO is conducted using smaller transformer model sizes.**

## 7 Conclusions

**Summary.** In this work, we introduce Deep Power Law (DPL), a probabilistic surrogate based on an ensemble of power law functions. The proposed surrogate is used within a novel multi-fidelity Hyperparameter Optimization (HPO) method based on Bayesian optimization. In contrast to the prior work, we exploit scaling laws for estimating the performance of Deep Learning (DL) models. Through extensive experiments comprising \(7\) baselines, \(59\) datasets, and search spaces of diverse deep learning architectures, we show that DPL outperforms strong HPO baselines for DL by a large margin. As an overarching contribution, we advance the state-of-the-art in the important field of HPO for DL.

**Limitations and future work.** Contrary to the common perception, we experienced that the uncertainty estimation arising from the Deep Ensemble approach  is suboptimal compared to standard BO surrogates such as Gaussian Processes. In addition, having to train an ensemble has additional computational costs, due to the necessity of training multiple power law models. In the future, we plan to investigate the combination of power laws with Gaussian Processes, as well as investigate additional fidelity types.

Figure 6: HPO for Transformer architectures. **Top:** HPO on small-scale transformers in terms of the embedding size. **Bottom:** Error on the full-scale transformer, using the hyperparameter configuration discovered by conducting HPO using the small transformers. We present three analyses, ablating the HPO time on the small-scale transformer up to the HPO budget of 2 full function evaluations.