# Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance

Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance

Jonathan Crabbe

DAMTP

University of Cambridge

jc2133@cam.ac.uk Mihaela van der Schaar

DAMTP

University of Cambridge

mv472@cam.ac.uk

###### Abstract

Interpretability methods are valuable only if their explanations faithfully describe the explained model. In this work, we consider neural networks whose predictions are invariant under a specific symmetry group. This includes popular architectures, ranging from convolutional to graph neural networks. Any explanation that faithfully explains this type of model needs to be in agreement with this invariance property. We formalize this intuition through the notion of explanation invariance and equivariance by leveraging the formalism from geometric deep learning. Through this rigorous formalism, we derive (1) two metrics to measure the robustness of any interpretability method with respect to the model symmetry group; (2) theoretical robustness guarantees for some popular interpretability methods and (3) a systematic approach to increase the invariance of any interpretability method with respect to a symmetry group. By empirically measuring our metrics for explanations of models associated with various modalities and symmetry groups, we derive a set of 5 guidelines to allow users and developers of interpretability methods to produce robust explanations.

## 1 Introduction

With their increasing success in various tasks, such as computer vision , natural language processing  and scientific discovery , deep neural networks (DNNs) have become widespread. State of the art DNNs typically contain millions to billions parameters and, hence, it is unrealistic for human users to precisely understand how these models issue predictions. This opacity increases the difficulty to anticipate how models will perform when deployed ; distil knowledge from the model  and gain the trust of stakeholders in high-stakes domains . To address these shortcomings, the field of _interpretable machine learning_ has received increasing interest . There exists mainly 2 approaches to increase model interpretability . (1) Restrict the model's architecture to _intrinsically interpretable architectures_. A notorious example is given by _self-explaining models_, such as attention models explaining their predictions by highlighting features they pay attention to  and prototype-based models motivating their predictions by highlighting related examples from their training set . (2) Use _post-hoc_ interpretability methods in a plug-in fashion after training the model. The advantage of this approach is that it requires no assumption on the model that we need to explain. In this work, we focus on several post-hoc methods: _feature importance_ methods (also known as feature attribution or saliency) that highlight features the model is sensitive to ; _example importance_ methods that identify influential training examples  and _concept-based explanations_ that exhibit how classifiers relate classes to human friendly concepts .

With the multiplication of interpretability methods, it has become necessary to evaluate the quality of their explanations . This stems from the fact that interpretability methods need to faithfully describe the model in order to provide actionable insights. Existing approaches to evaluate the quality of interpretability methods fall in 2 categories . (1) Human-centred evaluation investigate how the explanations help humans (experts or not) to anticipate the model's predictions  and whether the model's explanations are in agreement with some notion of ground-truth . (2) Functionality-grounded evaluation measure the explanation quality based on some desirable properties and do not require humans to be involved. Most of the existing work in this category measure the _robustness_ of interpretability methods with respect to transformations of the model input that should not impact the explanation . Since our work falls in this category, let us now summarize the relevant literature.

**Related Works.** showed that feature importance methods are sensitive to constant shifts in the model's input. This is unexpected because these constant shifts do not contribute to the model's prediction. Building on this idea of invariance of the explanations with respect to input shifts,  propose a _sensitivity_ metric to measure the robustness of feature importance methods based on their stability with respect to small perturbations of the model input. By optimizing small adversarial perturbations,  show that imperceptible changes in the input can modify the feature importance arbitrarily by approximatively keeping the model prediction constant. This shows that many interpretability methods, as neural networks, are sensitive to adversarial perturbations. Subsequent works have addressed this pathologic behaviour by fixing the model training dynamic. In particular, they showed that penalizing large eigenvalues of the training loss Hessian with respect to the inputs make the interpretations of this model more robust with respect to adversarial attacks . To the best of our knowledge, the only work that discusses the behaviour of explanations under more general transformations of the input data is . However, the work's focus is more on model regularization rather than on the evaluation of post-hoc interpretability robustness.

**Motivations.** In reviewing the above literature, we notice 3 gaps. (1) The existing studies mostly focus on evaluating feature importance methods. In spite of the predominance of feature importance in the literature , we note that other types of interpretability methods exist and deserve to be analyzed. (2) The existing studies mostly focus on images. While computer vision is undoubtedly an interesting application of DNNs, it would be interesting to extend the analysis to other modalities, such as times series and graph data . (3) The existing studies mostly focus on simple transformation of the model input, such as small shifts. This is motivated by the fact that the predictions of DNNs are mostly invariant under these transformations. Again, this is another direction that could be explored more thoroughly as numerous DNNs are also invariant to more complex transformation of their input data. For instance, graph neural networks are invariant to permutations of the node ordering in their input graph . Our work bridges these gaps in the interpretability robustness literature.

Figure 1: Illustration of model invariance and explanation invariance/equivariance with the simple case of an electrocardiogram (ECG) signal. In this case, the heartbeat described by the ECG remains the same if we apply any translation symmetry with periodic boundary conditions. (1) A model is invariant under the symmetry if the modelâ€™s prediction are not affected by the symmetry we apply to the signal. In this case, the model identifies an abnormal heartbeat before and after applying a translation. Any explanation that faithfully describes the model should reflect this symmetry. (2) For some explanations, the right behaviour is invariance as well. For instance, the most influential examples for the prediction should be the same for the original and the transformed signal, since the model makes no difference between the two signals. (3) For other type of explanations, the right behaviour is equivariance. For instance, the most important part of the signal for the prediction should be the same for the original and the transformed signal, since the model makes no difference between the two signals. Hence, the saliency map undergoes the same translation as the signal.

**Contributions.** We propose a new framework to evaluate the robustness of interpretability methods. We consider a setting where the model we wish to interpret is invariant with respect to a group \(\) of symmetry acting on the model input. Any interpretability method that faithfully describes this model should have explanations that are conserved by this group of symmetry \(\). We illustrate this reasoning in Figure 1 with the simple group \(\) of time translations acting on the input signal. We show examples of interpretability methods failing to conserve the model's symmetries, hence leading to inconsistent explanations, in Figure 2 and Appendix I. With this new framework, we bring several contributions. (**1**) Rigorous Interpretability Robustness.** We define interpretability robustness with respect to a group \(\) of symmetry through explanation invariance and equivariance. In agreement with our motivations, we demonstrate in Section 2.2 that our general definitions cover different type of interpretability methods, modalities and transformations of the input data. (**2**) Evaluation of Interpretability Methods. Not all interpretability methods are equal with respect to our notion of robustness. In Section 2.3, we show that some popular interpretability methods are naturally endowed with theoretical robustness guarantees. Further, we introduce 2 metrics, the invariance and equivariance scores, to empirically evaluate this robustness. In Section 3.1, we use these metrics to evaluate the robustness of 3 types of interpretability methods with 5 different model types corresponding to 4 different modalities and symmetry groups. Our empirical results support our theoretical analysis. (**3**) Insights to Improve Robustness.** By combining our theoretical and empirical analysis, we derive a set of 5 actionable guidelines to ensure that interpretability methods are used in a way that guarantees robustness with respect to the symmetry group \(\). In particular, we show in Sections 2.3 and 3.2 that we can improve the invariance score of any interpretability method by aggregating explanations over various symmetries. We summarize the guidelines with a flowchart in Figure 6 from Appendix A, that helps users to obtain robust model interpretations.

## 2 Interpretability Robustness

In this section, we formalize the notion of interpretability robustness through explanation invariance and equivariance. We start with a reminder of some useful definitions from geometric deep learning. We then define two metrics to measure the invariance and equivariance of interpretability methods. We leverage this formalism to derive some theoretical robustness guarantees for popular interpretability methods. Finally, we describe a rigorous approach to improve the invariance of any interpretability method.

### Useful Notions of Geometric Deep Learning

Some basic concepts of group theory are required for our definition of interpretability robustness. To that aim, we leverage the formalism of _Geometric Deep Learning_. Please refer to  for more details. To rigorously define explanation equivariance and invariance, we need some form of structure in the data we are manipulating. This precludes tabular data but includes graph, time series and image data. In this setting, the data is defined on a finite domain set \(\) (e.g. a grid \(=_{n}_{n}\) for \(n n\) images). On this domain, the data is represented by signals \(x(,)\), mapping each point \(u\) of the domain to a channel vector \(x(u)=^{d_{C}}\). We note that \(d_{C}^{+}\), corresponds to the

Figure 2: Examples of non-robust explanations obtained with Gradient Shap on the FashionMNIST dataset. From left to right: the original image for which the invariant model predicts t-shirt with a given probability, the Gradient Shap saliency map to explain the modelâ€™s prediction for this image, the transformed image for which the model predicts t-shirt with the exact same probability and the Gradient Shap saliency map for this transformed image. Clearly, the image transformation changes the explanation when it should not.

number of channels of the signal (e.g. \(d_{C}=3\) for RGB images). The set of signals has the structure of a vector space since \(x_{1},x_{2}(,)\;\;_{1} x_{1 }+_{2} x_{2}(,)\) for all \(_{1},_{2}\).

**Symmetries.** Informally, symmetries are transformations of the data that leave the information content unchanged (e.g. moving an image one pixel to the right). More formally, symmetries correspond to a set \(\) endowed with a composition operation \(:^{2}\). Clearly, this set \(\) includes an identity transformation \(id\) that leaves the data untouched. Similarly, if a transformation \(g\) preserves the information, then it could be undone by an inverse transformation \(g^{-1}\) such that \(g^{-1} g=id\). Those properties 1 give \(,\) the structure of a _group_. In this paper, we assume that the symmetry group has a _finite_ number of elements.

**Group Representation.** We have yet to formalize how the above symmetries transform the data. To that aim, we need to link the symmetry group \(\) with the signal vector space \((,)\). This connection is achieved by choosing a _group representation_\(:[(,)]\) that maps each symmetry \(g\) to an _automorphism_\([g][(,)]\). Formally, the automorphisms \([(,)]\) are defined as bijective linear transformations mapping \((,)\) onto itself. In practice, each automorphism \([g]\) is represented by an invertible matrix acting on the vector space \((,)\). For instance, an image translation \(g\) can be represented by a permutation matrix \([g]\). To qualify as a group representation, the map \(\) needs to be compatible with the group composition: \([g_{2} g_{1}]=[g_{2}][g_{1}]\). This property guarantees that the composition of two symmetries can be implemented as the multiplication between two matrices.

**Invariance.** We first consider the case of a deep neural network \(f:(,)\), where the output \(f(x)\) is a vector with no underlying structure (e.g. class probabilities for a classifier). In this case, we expect the model's prediction to be unchanged when applying a symmetry \(g\) to the input signal \(x(,)\). For instance, the probability of observing a cat on an image should not change if we move the cat by one pixel to the right. This intuition is formalized by defining the \(\)_-invariance_ property for the model \(f\): \(f([g]x)=f(x)\) for all \(g,x(,)\).

**Equivariance.** We now turn to the case of deep neural networks \(f:(,)(^{},^{ })\), where the output \(f(x)(^{},^{})\) is also a signal (e.g. segmentation masks for an object detector). We note that the domain \(^{}\) and the channel space \(^{}\) are not necessarily the same as \(\) and \(\). When applying a transformation \(g\) to the input signal \(x(,)\), it is legitimate to expect the output signal \(f(x)\) to follow a similar transformation. For instance, the segmentation of a cat on an image should move by one pixel to the right if we move the cat by one pixel to the right. This intuition is formalized by defining the \(\)_-equivariance_ property for the model \(f\): \(f([g]x)=^{}[g]f(x)\). Again, the representation \(^{}:[(^{}, ^{})]\) is not necessarily the same as the representation \(\) since the signal spaces \((,)\) and \((^{},^{})\) might have different dimensions.

### Explanation Invariance and Equivariance

We will now restrict to models that are \(\)-invariant2. It is legitimate to expect similar invariance properties for the explanations associated to this model. We shall now formalize this idea for generic explanations. We assume that explanations are functions of the form \(e:(,)\), where \(^{d_{E}}\) is an explanation space with \(d_{E}^{+}\) dimensions3.

**Invariance and Equivariance.** The invariance and equivariance of the explanation \(e\) with respect to symmetries \(\) are defined as in the previous section. In this way, we say that the explanation \(e\) is \(\)-invariant if \(e([g]x)=e(x)\) and \(\)-equivariant if \(e([g]x)=^{}[g]e(x)\) for all \(g,x(,)\). There is no reason to expect these equalities to hold exactly a priori. This motivates the introduction of two metrics that measure the violation of explanation invariance and equivariance by an interpretability method.

**Definition 2.1** (Robustness Metrics).: Let \(f:(,)\) be a neural network that is invariant with respect to the symmetry group \(\) and \(e:(,)\) be an explanation for \(f\). We assume that \(\) acts on \((,)\) via the representation \(:[(,)]\). We measure the _invariance_ of \(e\) with respect to \(\) for some \(x(,)\) with the metric

\[_{}(e,x)|}_{g}s_{}[e([g]x),e(x)], \]

where \(s_{}:^{2}\) is a similarity score on the explanation space \(\). We use the cos-similarity \(s_{}(a,b)=b}}{{\|a\|_{2}}} \|b\|_{2}\) for real-valued explanations \(a,b^{d_{E}}\) and the accuracy score \(s_{}(a,b)=d_{E}^{-1}_{i=1}^{d_{E}}1(a_{i}=b_{i})\) for categorical explanations \(a,b_{K}^{d_{E}}\), where \(\) is the indicator function and \(K^{+}\) is the number of categories. If we assume that \(\) acts on \(\) via the representation \(^{}:[]\), we measure the _equivariance_ of \(e\) with respect to \(\) for some \(x(,)\) with the metric

\[_{}(e,x)|}_{g }s_{}[e([g]x),^{}[g]e(x) ]. \]

A score \(_{}(e,x)=1\) or \(_{}(e,x)=1\) indicates that the explanation method \(e\) is \(\)-invariant or equivariant for the example \(x(,)\).

_Remark 2.2_.: The metrics \(_{}\) and \(_{}\) might be prohibitively expensive to evaluate whenever the size \(||\) of the symmetry group \(\) is too big. Note that this is typically the case in our experiments as we consider large permutation groups of order \(|| 10^{32}\). In this case, we use Monte Carlo estimators for both metrics by uniformly sampling \(G U()\) and averaging over a number of sample \(N_{}||\). We study the convergence of those Monte Carlo estimators in Appendix E.

The above approach to measure the robustness of interpretability method applies to a wide variety of settings. To clarify this, we explain how to adapt the above formalism to 3 popular types of interpretability methods: _feature importance_, _example importance_ and _concept-based explanations_.

**Feature Importance.** Feature importance explanations associate a saliency map \(e(x)(,)\) to each example \(x(,)\) for the model's prediction \(f(x)\). In this case, we note that the explanation space corresponds to the model's input space \(=(,)\), since the method assigns an importance score to each individual feature. If we apply a symmetry to the input, we expect the same symmetry to be applied to the saliency map, as illustrated by the example from Figure 1. Hence, the most relevant metric to record here is the explanation equivariance \(_{}\). Since the input space and the explanation space are identical \(=(,)\), we work with identical representations \(^{}=\). We note that this metric generalizes the self-consistency score introduced by  beyond affine transformations.

**Example Importance.** Example importance explanations associate an importance vector \(e(x)^{N_{}}\) to each example \(x(,)\) for the model's prediction \(f(x)\). Note that \(N_{}^{+}\) is typically the model's training set size, so that each component of \(e(x)\) corresponds to the importance of a training example. If we apply a symmetry to the input, we expect the relevance of training examples to be conserved, as illustrated by the example from Figure 1. Hence, the most relevant metric to record here is the invariance \(_{}\).

**Concept-Based Explanations.** Concept-based explanations associate a binary concept presence vector \(e(x)\{0,1\}^{C}\) to each example \(x(,)\) for the model's prediction \(f(x)\). Note that \(C^{+}\) is the number of concepts one considers, so that each component of \(e(x)\) corresponds to the presence/absence of a concept. If we apply a symmetry to the input, there is no reason for a concept to appear/vanish, since the information content of the input is untouched by the symmetry. Hence, the most relevant metric to record here is again the invariance \(_{}\).

### Theoretical Analysis

Let us now provide a theoretical analysis of robustness in a setting where the model \(f\) is \(\)-invariant. We first show that many popular interpretability methods naturally offer some robustness guarantee if we make some assumptions. For methods that are not invariant when they should, we propose an approach to enforce \(\)-invariance.

**Robustness Guarantees.** In Table 1, we summarize the theoretical robustness guarantees that we derive for popular interpretability methods. All of these guarantees are formally stated and proven in Appendix D. When it comes to feature importance methods, there are mainly two assumptions that are necessary to guarantee equivariance. (1) The first assumption restricts the type of baseline input \((,)\) on which the feature importance methods rely. Typically, these baselines signals are used to replace ablated features from the original signal \(x(,)\) (i.e. remove a feature \(x_{i}\) by replacing it by \(_{i}\) ). In order to guarantee equivariance, we require this baseline signal to be invariant to the action of each symmetry \(g:[g]=\). (2) The second assumption restricts the type of representation \(\) that can be used to describe the action of the symmetry group on the signals. In order to guarantee equivariance, we require this representation to be a _permutation representation_, which means that the action of each symmetry \(g\) is represented by a permutation matrix \([g]\) acting on the signal space \((,)\).

When it comes to example importance methods, the assumptions depend on how the importance scores are obtained. If the importance scores are computed from the model's loss, then the invariance of the explanation immediatly follows from the model's invariance. If the importance scores are computed from the model's internal representations \(h:(,)^{d_{}}\), then the invariance of the explanation can only be guaranteed if the representation map \(h\) is itself invariant to action of each symmetry: \(h([g]x)=h(x)\). Similarly, concept-based explanations are also computed from the model's representations \(h\). Again, the invariance of these explanations can only be guaranteed if the representation map \(h\) is itself invariant.

**Enforcing Invariance.** If the explanation \(e\) is not \(\)-invariant when it should, we can construct an auxiliary explanation \(e_{}\) built upon \(e\) that is \(\)-invariant. This permits to improve the robustness of any interpretability method that has no invariance guarantee. The idea is simply to aggregate the explanation over several symmetries.

**Proposition 2.3**.: _[Enforce Invariance] Consider a neural network \(f:(,)\) that is invariant with respect to the symmetry group \(\) and \(e:(,)\) be an explanation for \(f\). We assume that \(\) acts on \((,)\) via the representation \(:[(,)]\). We define the auxiliary explanation \(e_{}:(,)\) as_

\[e_{}(x)|}_{g}e([ g]x)\]

_for all \(x(,)\). The auxiliary explanation \(e_{}\) is invariant under the symmetry group \(\)._

Proof.: Please refer to Appendix D. 

_Remark 2.4_.: Once again, a Monte Carlo estimation for \(e_{}\) might be required for groups \(\) with many elements. This produces explanations that are approximatively invariant.

## 3 Experiments

In this section, we use our interpretability robustness metrics to draw some insights with real-world models and datasets. We first evaluate the \(\)-invariance and equivariance of popular interpretability methods used on top of \(\)-invariant models. With this analysis, we identify interpretability methods that are not robust. We then show that the robustness of these interpretability methods can largely be improved by using their auxiliary version defined in Proposition 2.3. Finally, we study how the \(\)-invariance and equivariance of interpretability methods varies when we decrease the invariance of the underlying model. From these experiments, we derive 5 guidelines to ensure that interpretability

  
**Type** & **Computation** & **Example** & **Invariant** & **Equivariant** & **Details** \\  Feature & Grad. \(_{x}f(x)\) &  & âœ— & \(\) & Prop. D.6 \\ Importance & Pert. \(f(x+ x)\) &  & âœ— & \(\) & Prop. D.8 \\  Example & Loss \((x),y\) &  & âœ“ & âœ— & Prop. D.9 \\ Importance & Rep. \(h(x)\) &  & \(\) & âœ— & Prop. D.12 \\  Concept-Based & Rep. \(h(x)\) &  & \(\) & âœ— & Prop. D.14 \\   

Table 1: Theoretical robustness guarantees that we derive for explanations of invariant models. We split the interpretability methods according to their type and according to model information they rely on (model gradients, perturbations, loss or representations). We consider 3 levels of guarantees: âœ“ indicates unconditional guarantee, \(\) conditional guarantee and âœ— no guarantee.

methods are robust with respect to symmetries from \(\). We summarize these guidelines with a flowchart in Figure 6 from Appendix A.

The datasets used in our experiment are presented in Table 2. We explore various modalities and symmetry groups throughout the section, as described in Table 3. For each dataset, we fit and study a classifier from the literature designed to be _invariant_ with respect to the underlying symmetry group. For each model, we evaluate the robustness of various feature importance, example importance and concept-based explanations. More details on the experiments are available in Appendix F. We also include a comparison between our robustness metrics and the sensitivity metric in Appendix G. The code and instructions to replicate all the results reported below are available in the public repositories [https://github.com/JonathanCrabbe/RobustXAI](https://github.com/JonathanCrabbe/RobustXAI) and [https://github.com/vanderschaarlab/RobustXAI](https://github.com/vanderschaarlab/RobustXAI).

### Evaluating Interpretability Methods

**Motivation.** The purpose of this experiment is to measure the robustness of various interpretability methods. Since we manipulate models that are invariant with respect to a group \(\) of symmetry, we expect feature importance methods to be \(\)-equivariant (\(_{}[e,x]=1\) for all \(x(,)\)). Similarly, we expect example and concept-based methods to be \(\)-invariant (\(_{}[e,x]=1\) for all \(x(,)\)). We shall now verify this empirically.

**Methodology.** To measure the robustness of interpretability methods empirically, we use a set \(_{}\) of \(N_{}\) examples (\(N_{}=433\) for Mutagenicity, \(N_{}=1,000\) for ModelNet40 and Electrocardiograms (ECG) and \(N_{}=500\) in the other cases). For each interpretability method \(e\), we evaluate the appropriate robustness metric for each test example \(x_{}\). For Mutagenicity and ModelNet40, the large order \(||\) makes the exact evaluation of the metric unrealistic. We therefore use a Monte Carlo approximation with \(N_{}=50\). As demonstrated in Appendix E, the Monte Carlo estimators have already converged with this sample size. In all the other cases, these metrics are evaluated exactly since \(\) has a tractable order \(||\). Since the E(2)-WideResNets for CIFAR100 and STL10 are only approximatively invariant with respect to \(_{8}\), we defer their discussion to Section 3.3. We note that some interpretability methods cannot be used in some settings. Whenever this is the case, we simply omit the interpretability method. Please refer to Appendix F for more details.

**Analysis.** We report the robustness score for each metric and each dataset on the test set \(_{}\) in Figures 3(a) to 3(c). We immediately notice that not all the interpretability methods are robust. We provide some real examples of non-robust explanations in Appendix I in order to visualize the failure modes. When looking at feature importance, we observe that equivariance is not guaranteed by methods that rely on baseline that are not invariant. For instance, Gradient Shap and Feature Permutation rely on a random baseline, which has no reason to be \(\)-invariant. We conclude that the

  
**Dataset** & **\# Classes** & **Modality** & **Symmetry Group** & **Model** \\  Electrocardiograms  & 2 & Time Series & Cyclic Translations \(/T\) & All-CNN  \\ Mutagenicity  & 2 & Graphs & Node Permutations \(S_{v}\). & GraphConv GNN  \\ ModelNet40  & 40 & 3D Point Clouds & Point Permutations \(S_{v}\). & Deep Set  \\ IMDb  & 2 & Text & Token Permutation \(S_{T}\) & Bag-of-words MLP \\ FashionMNIST  & 10 & Images & Cyclic Translations \((/10)^{2}\) & All-CNN  \\ CIFAR100  & 100 & Images & Dheldral Group \(_{8}\) & E(2)-WideResNet  \\ STL10  & 10 & Images & Dheldral Group \(_{8}\) & E(2)-WideResNet  \\   

Table 2: Different datasets used in the experiments.

  
**Symmetry Group** & **Acting on** & **Description** \\  Translation \(/N\) & Time series, Images & Shifts signals in time and images horizontally \& vertically. \\ Permutation \(S_{N}\) & Graph nodes, Points in clouds, Tokens & Changes the ordering of nodes / points / tokens in feature matrices. \\ Dihedral \(_{8}\) & Images & Rotate / reflects the images though angles \(45^{},90^{},135^{},180^{},225^{},315^{}\) \\   

Table 3: Various symmetry groups used in the experiments.

invariance of the baseline \(\) is crucial to guarantee the robustness of feature importance methods. When it comes to example importance, we note that loss-based methods are consistently invariant, which is in agreement with Proposition D.9. Representation-based and concept-based methods, on the other hand, are invariant only if used with invariant layers of the model. This shows that the choice of what we call the _representation space_ matters for these methods. We derive a set of guidelines from these observations.

**Guideline 1**.: Feature importance methods should be used with group invariant baseline signal (\([g]=\) for all \(g\)) to guarantee explanation equivariance. Only methods that conserve the invariance of the baseline can guarantee equivariance.

**Guideline 2**.: Loss-based example importance methods guarantee explanation invariance, unlike representation-based methods. When using the latter, only invariant layers guarantee explanation invariance.

**Guideline 3**.: To guarantee invariance of concept-based explanations, concept classifiers should be used on invariant layers of the model.

### Improving Robustness

**Motivation.** In the previous experiment, we noticed that not all the interpretability methods are \(\)-invariant when they should. Consider, for instance, concept-based methods used on equivariant layers. The lack of invariance for these methods implies that they rely on concept classifiers that

Figure 3: Explanation robustness of interpretability methods for invariant models. The interpretability methods are grouped by type. Each box-plot is produced by evaluating the robustness metrics \(_{}\) or \(_{}\) across several test samples \(x_{}\). The asterisk (*) indicates a dataset where the model is only approximatively invariant. Those models are discussed in Section 3.3. For all other models, any value below 1 for the metrics is unexpected, as the model is \(\)-invariant.

are not \(\)-invariant. This behaviour is undesirable for two reasons: (1) since any symmetry \(g\) preserve the information of a signal \(x(,)\), the signal \([g]x\) should contain the same concepts as \(x\) and (2) the layer that we use implicitly encodes these symmetries through equivariance of the output representations. Hence, concept classifiers that are not \(\)-invariant fail to generalize by ignoring the symmetries encoded in the structure of the model's representation space. Fortunately, Proposition 2.3 gives us a prescription to obtain explanations (here concept classifiers) that are more robust with respect to the model's symmetries. We shall now illustrate how this prescription improves the robustness of concept-based methods.

Methodology.In this experiment, we restrict our analysis to the ECG and FashionMNIST datasets. For each test signal, we sample \(N_{}\) symmetries \(G_{i},i_{N_{}}\) without replacement. As prescribed by Proposition 2.3, we then compute the auxiliary explanation \(e_{}(x)=N_{}^{-1}_{i=1}^{N_{}}e([G _{i}]x)\) for each concept importance method.

Analysis.We report the average invariance score \(_{X U(_{})}_{}(e _{},X)\) for several values of \(N_{}\) in Figure 4. As we can see, the invariance of the explanation grows monotonically with the number of samples \(N_{}\) to achieve a perfect invariance for \(N_{}=||\). Interestingly, the explanation invariance increases more quickly for CAR. This suggests that enforcing explanation invariance is less expensive for certain interpretability methods and motivates the below guideline.

Guideline 4.Any interpretability method can be made invariant through Proposition 2.3. In doing so, one should increase the number of samples \(N_{}\) until the desired invariance is achieved. In this way, the method is made robust without increasing the number of calls more than necessary. Note that it only makes sense to enforce invariance of the interpretability method if the explained model is itself invariant.

### Relaxing Invariance

Motivation.In practice, models are not always perfectly invariant. A first example is given by the CIFAR100 and STL10 WideResNet that has a strong bias towards being \(_{}\)-invariant, although it can break this invariance at training time (see Appendix H.3 of ). Another popular example is a CNN that flattens the output of convolutional layers, which violates translation invariance . This motivates the study of interpretability methods robustness when models are not perfectly invariant.

Methodology.This experiment studies the two aforementioned settings. First, we replicate the experiment from Section 3.1 with the CIFAR100 and STL10 WideResNet. Second, we consider CNNs that flatten their last convolutional layer with the ECG and FashionMNIST datasets. In this case, we introduce 2 variants of the All-CNN where the global pooling is replaced by a flatten operation: an _Augmented-CNN_ trained by augmenting the training set \(_{}\) with random translations and a _Standard-CNN_ trained without augmentation. We measure the invariance/equivariance of the interpretability methods for each model.

Analysis.The results for the WideResNets are reported in Figures 3(a) to 3(c). We see that the robustness of various interpretability methods substantially drops with the model invariance. This is particularly noticeable for feature importance methods. To illustrate this phenomenon, we plot in Figure 3(d) the evolution during training of the model's prediction \(f(x)\)\(\)-invariance and the

Figure 4: Explanation invariance can be increased according to Proposition 2.3. This plot shows the score averaged on a test set \(_{}\) together with a \(95\%\) confidence interval.

\(\)-equivariance of its gradient \(_{x}f(x)\), on which the attribution methods rely. As we can see, the model remains almost invariant during training, while the gradients equivariance is destroyed. Similar observations can be made with the CNNs from Figure 5. In spite of the Augmented-CNN being almost invariant, we notice that the symmetry breaks significantly for feature importance methods. These results suggest that the robustness of interpretability methods can be (but is not necessarily) fragile if model invariance is relaxed, even slightly. This motivates our last guideline, which safeguards against erroneous interpretations of our robustness metrics.

**Guideline 5.** One should _not_ assume a linear relationship between model invariance and explanation invariance/equivariance. In particular, the robustness of an interpretability method for an invariant model _does not_ imply that this method is robust for an approximatively invariant model.

## 4 Discussion

Building on recent developments in geometric deep learning, we introduced two metrics (explanation invariance and equivariance) to assess the faithfulness of model explanations with respect to model symmetries. In our experiments, we considered a wide range of models whose predictions are invariant with respect to transformations of their input data. By analyzing feature importance, example importance and concept-based explanations of these models, we observed that many of these explanations are not invariant/equivariant to these transformations when they should. This led us to establish a set of guidelines in Appendix A to help practitioners choose interpretability methods that are consistent with their model symmetries.

Beyond actionable insights, we believe that our work opens up interesting avenues for future research. An important one emerged by studying the equivariance of saliency maps with respect to models that are approximately invariant. This analysis showed that state-of-the-art saliency methods fail to keep a high equivariance score when the model's invariance is slightly relaxed. This important observation could be the seed of future developments of robust feature importance methods.

Figure 5: Effect of relaxing the model invariance on interpretability methods invariance/equivariance. The interpretability methods are grouped by type in each column. The error bars represent a \(95\%\) confidence interval around the mean for \(\) and \(\). Lin1 is to the output of the first dense layer of the CNN, which corresponds to the invariant layer used in Section 3.1.