# Globally Convergent Variational Inference

Declan McNamara Jackson Loper Jeffrey Regier

Department of Statistics

University of Michigan

{declan, jaloper, regier}@umich.edu

###### Abstract

In variational inference (VI), an approximation of the posterior distribution is selected from a family of distributions through numerical optimization. With the most common variational objective function, known as the evidence lower bound (ELBO), only convergence to a _local_ optimum can be guaranteed. In this work, we instead establish the _global_ convergence of a particular VI method. This VI method, which may be considered an instance of neural posterior estimation (NPE), minimizes an expectation of the inclusive (forward) KL divergence to fit a variational distribution that is parameterized by a neural network. Our convergence result relies on the neural tangent kernel (NTK) to characterize the gradient dynamics that arise from considering the variational objective in function space. In the asymptotic regime of a fixed, positive-definite neural tangent kernel, we establish conditions under which the variational objective admits a unique solution in a reproducing kernel Hilbert space (RKHS). Then, we show that the gradient descent dynamics in function space converge to this unique function. In ablation studies and practical problems, we demonstrate that our results explain the behavior of NPE in non-asymptotic finite-neuron settings, and show that NPE outperforms ELBO-based optimization, which often converges to shallow local optima.

## 1 Introduction

In variational inference (VI), the parameters \(\) of an approximation to the posterior \(Q(;)\) are selected to optimize an objective function, typically the evidence lower bound (ELBO) (Blei et al., 2017). However, the ELBO is generally nonconvex in \(\), even for simple variational families such as the family of Gaussian distributions, and so only convergence to a local optimum of the ELBO can be guaranteed (Ghadimi and Lan, 2015; Ranganath et al., 2014; Hoffman et al., 2013). As the number of such optima and the degree of suboptimality of each are generally unknown, the lack of global convergence guarantees constitutes a significant complication for practitioners and a longstanding barrier to the broader adoption of VI.

In this work, we present the first global convergence result for variational inference. We accomplish this in the context of an increasingly popular alternative objective for variational inference, the expected forward KL divergence:

\[L_{P}():=_{P(X)}[P( X) Q(; f(X;))]. \]

Here, \(P(X)\) denotes a marginal of the model and \(P( X)\) denotes the posterior. For each \(x\), the approximation \(Q(;)\) to \(P( X=x)\) is indexed by the distributional parameters \(^{q}\), which are themselves the output of a neural network \(f(x;)\) with weights \(\). Approximating a posterior distribution by minimizing \(L_{P}\) has a long history (Section 2.1), and is sometimes known as neural posterior estimation (NPE) (Papamakarios and Murray, 2016) and the "sleep" objective of reweighted wake-sleep (Bornschein and Bengio, 2015; Le et al., 2019). Minimization of this objective is straightforward: computing unbiased gradients requires only sampling \(,x P(,X)\) from thejoint model (Section 2.1), which is readily accomplished by ancestral sampling. This approach is "likelihood-free" in that the density of \(P(X)\) need not be evaluated, and therefore expected forward KL minimization is more widely applicable than ELBO-based optimization, which requires a tractable likelihood function. Analysis of the amortized problem (i.e., optimizing an objective that averages over \(P(X)\)) is beneficial when considering the forward KL; for the non-amortized problem in which a single observation \(x\) is considered, only biased estimates of the gradient of the forward KL can be obtained using self-normalized importance sampling, making convergence difficult to establish (Bornschein and Bengio, 2015; Le et al., 2019; Owen, 2013). Our analysis considers a functional form of variational objective \(L_{P}\), given by

\[L_{F}(f):=_{P(X)}[P( X) Q(;f(X) )], \]

where \(L_{F}:\) is defined over a general reproducing kernel Hilbert space of functions \(\). We refer to (1) as the "parametric objective", as its argument is the parameters \(\), and we refer to (2) as the "functional objective" as its argument is a function \(f\). These objectives are closely related: under a given network parameterization, provided \(f(;)\), we have \(L_{P}()=L_{F}(f(;))\). The objective \(L_{P}\) has been considered in several related works (see Section 2.1). The formulation of \(L_{F}\) and the analysis of its minimizer relative to those of \(L_{P}\) is our main contribution.

We first demonstrate strict convexity of the functional objective \(L_{F}\) when \(Q\) is parameterized as an exponential family distribution (Section 3). This implies the existence of a unique global optimizer \(f^{*}\) of \(L_{F}\) for a large class of variational families. Afterward, we analyze kernel gradient flow dynamics using the neural tangent kernel to show that minimization of \(L_{P}\) results (asymptotically) in an empirical mapping \(f\) that is at most \(\)-suboptimal relative to \(f^{*}\), provided a sufficiently flexible neural network is used to parameterize \(f\) (Section 4). Together, these results imply that in the infinite-width limit, optimization of \(L_{P}\) by gradient descent recovers a unique global solution.

Our analysis relies on fairly mild conditions, the most important of which are the positive definiteness of the neural tangent kernel and the structure of the variational family (e.g., an exponential family) (Section 6). Our proofs further assume a two-layer ReLU network architecture, but we conjecture that this assumption can be relaxed, and our experiments (Section 5) demonstrate global convergence for a wide variety of architectures. We illustrate that the minimization of \(L_{P}\) converges to a global solution for problems with both synthetic and semi-synthetic data, and that finite network widths exhibit the behavior of the asymptotic regime (i.e., that of an infinitely wide network).

We further show that optimizing \(L_{P}\) can produce better posterior approximations than likelihood-based ELBO methods, which suffer from convergence to shallow local optima. These results suggest, surprisingly, that a likelihood-free approach to inference can outperform likelihood-based approaches. Further, even for practitioners interested in inference for a single observation \(x_{0}\), for whom amortization is not needed for computational efficiency, our approach may still be preferable to traditional ELBO-based inference due to the convergence guarantees of the former.

**Related work.** There is a large body of literature that analyzes the convergence of variational inference methods that target the ELBO. These works typically prove rates of convergence to the posterior (Zhang and Gao, 2020) or to a local optimum of the objective, as the ELBO is not amenable to global minimization because it is nonconvex (Domke, 2020; Domke et al., 2023; Kim et al., 2023). Liu et al. (2023) demonstrated nonconvexity of the ELBO in the context of small-object detection, and showed empirically that the expected forward KL was more robust to the pitfalls of numerical optimization. The nonconvexity of the variational objective has previously been addressed through workarounds such as convex relaxations (Fazelnia and Paisley, 2018) or iterated runs of the optimization routine to improve the quality of the local optimum (Altosaar et al., 2018). Our work differs from previous work in that our convergence result is global. Additionally, our approach is novel compared to related analyses because we consider the (arguably) more complicated problem of amortized inference, where the variational parameters are the weights of a neural network and are shared among observations.

Background

### The Expected Forward KL Divergence

The expected forward KL objective is equivalent to the sleep-phase objective of Reweighted Wake-Sleep (RWS) (Bornschein and Bengio, 2015), and to the objective optimized by forward amortized variational inference (FAVI) (Ambrogioni et al., 2019). It is also a special case of the thermodynamic variational objective (TVO) (Masrani et al., 2019). Similar objectives have been referred to as neural posterior estimation (NPE) (Papamakarios and Murray, 2016; Papamakarios et al., 2019), though in these works the prior distribution, and thus the marginal \(P(X)\), mutates during training.

Objectives based on the forward KL divergence generally result in variational posteriors that are overdispersed, a desirable property compared to reverse KL-based optimization (Le et al., 2019; Domke and Sheldon, 2018).

Unbiased gradient estimation for the parametric objective \(L_{P}\) is straightforward. The outer expectation over \(P(X)\) allows for gradients to be computed as

\[_{}_{P(X)}[P( X) Q(; f(X;))]=-_{P()}_{P(X|)}_{}  q(;f(X;)),\]

where \(q\) is the density of \(Q\) with respect to Lebesgue measure (see Appendix B for details). Rewriting the left-hand side as an expectation over \(P()\) and \(P(X)\) by Bayes' rule illustrates that samples can be drawn from this model by ancestral sampling of \(\) followed by \(X\).

Other methods targeting the forward KL, such as the wake-phase of RWS, often optimize over a different expectation, typically \(_{X}[P( X) Q(; f(X;))]\). Here, the outer expectation is over an empirical dataset \(\) rather than \(P(X)\). In this case, approximation techniques such as importance sampling are required to estimate the gradient, as sampling from \(P( X=x)\) is intractable for any \(x\). Relying on importance sampling results in _biased_ gradient estimates, with which stochastic gradient descent (SGD) may not converge (Bornschein and Bengio, 2015; Le et al., 2019).

### The Neural Tangent Kernel

A neural network architecture and the parameter space \(\) of its weights together define a family of functions \(\{f(;):\}\). Let \((x,f(x))\) denote a general real-valued loss function and consider selecting the parameters \(\) to minimize \(_{P(X)}(X,f(X;))\), where \(P(X)\) is a distribution on the data space \(\). The neural tangent kernel (NTK) (Jacot et al., 2018) analyzes the evolution of the function \(f(;)\) while \(\) is fitted by gradient descent to minimize the above objective. Continuous-time dynamics are used in the formulation; \((t)\) and \(f(;(t))\) are defined for continuous time \(t\). The parameters \(\) thus follow the ODE

\[(t)=-_{}_{P(X)}(X,f(X;(t))). \]

Here, \(\) denotes the derivative with respect to \(t\), and by the chain rule, the function values \(f(x;(t))\) evolve via

\[(x;(t))=-_{P(X)}f(x;(t))J_{}f (X;(t))^{}}_{}^{}(X,f(X;(t))).\]

We define \(^{}(X,f(X)):=_{f}(X,f(X))\) to simplify the notation. The product of Jacobians above is known as the _neural tangent kernel_ (NTK):

\[K_{}(x,x^{})=J_{}f(x;)J_{}f(x^{};)^{}. \]

The seminal work of Jacot et al. (2018) defined and studied this kernel and established the convergence of \(K_{}\) to a limiting kernel for certain neural network architectures as the layer width grows large.

### Vector-Valued Reproducing Kernel Hilbert Spaces

Most existing NTK-based analyses consider neural networks with scalar outputs and squared error loss. Instead, we consider neural networks with multivariate outputs to accommodate the multidimensional distributional parameter \(\), which parameterizes our variational distribution \(Q(;)\). Furthermore, we consider the objective functions \(L_{P}\) and \(L_{F}\). Consequently, we rely on results from the vector-valued reproducing kernel Hilbert space (RKHS) literature, as these spaces contain vector-valued functions such as the network function \(f(;)\). Carmeli et al. (2008, 2006) provide a detailed review, and in the following, we summarize the key properties.

Recall that \(=f(;)\) and \(^{q}\). An \(^{q}\)-valued kernel on \(\) is a map \(K:^{q q}\). The neural tangent kernel (4) is precisely such a kernel. If the \(^{q}\)-valued kernel \(K\) is positive definite, then \(K\) defines a unique Hilbert space of functions \(\) whose elements are maps from \(\) to \(^{q}\)(Carmeli et al., 2006). This kernel \(K\) is called the reproducing kernel, and the corresponding space of functions is the RKHS associated with the kernel \(K\).

In an \(^{q}\)-valued RKHS, the reproducing property takes on a more general form. For any \(x\), \(f\), and \(v^{q}\),

\[f(x)^{}v= f(x),v_{^{q}}= f(),K(, x)v_{},\]

where \(,_{}\) is the inner product of the Hilbert space \(\). For any fixed \(x\) and \(v^{q}\), \(K(,x)v\) is a function mapping from \(^{q}\), as is required to be an element of \(\).

## 3 Convexity of the Functional Objective

We now turn to the analysis of the functional objective \(L_{F}\) given in Equation (2). We fix an RKHS \(\) over which to minimize \(L_{F}\) for now, specializing to the particular choice of \(\) based on the neural tangent kernel subsequently. Let \((x,f(x))=[P( X=x) Q(;f(x))]\). The functional \(L_{F}\) then has the form \(L_{F}=_{P(X)}(X,f(X))\); we will use this form subsequently for our neural tangent kernel analysis. Our first result shows that targeting \(L_{F}\) is highly desirable theoretically: \(L_{F}\) admits a unique global minimizer if the variational family \(Q\) is an exponential family, as is common practice in VI.

**Lemma 1**.: _Suppose that \(Q(;)\) is an exponential family distribution in minimal representation with natural parameters \(\), sufficient statistics \(T()\), and density \(q(;)\) with respect to Lebesgue measure \(()\). Then, for any observation \(x^{d}\), the loss function_

\[(x,)=[P( X=x) Q(;)]\]

_is strictly convex in \(\), provided that \(P( X=x) Q(;)()\) for all \(^{q}\)._

A proof of Lemma 1, which follows quickly from the convexity of the log partition function in the natural parameter (Wainwright and Jordan, 2008, Proposition 3.1), is provided in Appendix A. Lemma 1 shows the strict convexity of the function \(\) in \(\). This implies the strict convexity of the functional \(L_{F}(f)=_{P(X)}(X,f(X))\) in \(f\) by the linearity of expectation, which in turn implies the existence of at most one global minimizer.

**Corollary 1**.: _Suppose that \(Q(;)\) is an exponential family distribution. Then, under the conditions of Lemma 1, the functional objective_

\[L_{F}(f):=_{P(X)}[P( X) Q(; f(X))]\]

_is strictly convex in \(f\). Consequently, the set of global minimizers of \(L_{F}\) is either a singleton set or empty._

We will assume that the set of global minimizers is nonempty (so that the minimization of \(L_{F}\) is well-posed) and let \(f^{*}\) denote the global minimizer. We also assume that \(||f^{*}||_{}<\) so that \(f^{*}\). Hereafter, we use the term "unique" to mean unique almost everywhere with respect to \(P(X)\). Furthermore, in a slight abuse of notation, \(f^{*}\) will denote the unique equivalence class of functions that minimize \(L_{F}(f)\).

Whereas Lemma 1 establishes the convexity of the (non-amortized) forward KL divergence, Corollary 1 establishes the convexity of \(L_{F}\), an amortized objective, in function space. The convexity of the functional objective \(L_{F}\) holds regardless of the distribution chosen for the outer expectation by the same linearity argument. Choices other than \(P(X)\), however, may not permit unbiased gradient estimation, as is the case for the wake-phase updates of RWS (Section 2.1).

Global Optima of the Parametric Objective

In practice, we must directly minimize \(L_{P}\) rather than \(L_{F}\), as optimizing the latter over the infinite-dimensional space \(\) directly is not tractable. Thus, in the second phase of our analysis, we consider convergence to \(f^{*}\) by minimizing the parametric objective \(L_{P}\) with gradient descent. We define \(\) across continuous time as in Equation (3). Continuous-time dynamics simplify theoretical analysis; SGD with unbiased gradients follows a (noisy) Euler discretization of the continuous ODE (Santambrogio, 2017; Yang et al., 2021). Considering \(X P(X)\) for the outer expectation in both \(L_{P}\) and \(L_{F}\) is key in this context: this choice enables unbiased stochastic gradient estimation for \(L_{P}\) (see Appendix B), whereas other choices require approximations that result in biased gradient estimates (see Section 2.1) and thus follow different gradient dynamics.

Analysis of the trajectories of the parametric objective \(L_{P}\) throughout its minimization initially seems infeasible: the argument of this objective is the neural network parameters \(\), and even well-behaved loss functions such as the mean squared error (MSE) are nonconvex in these parameters. Nevertheless, neural tangent kernel (NTK)-based results enable analysis of \(L_{P}\). We bridge the divide between the minimizers of the convex functional \(L_{F}\) and the nonconvex objective \(L_{P}\) using the limiting kernel, and show that in the large-width limit, the optimization path of \(L_{P}\) converges arbitrarily close to \(f^{*}\), the unique minimizer of the functional objective \(L_{F}\).

**Theorem 1**.: _Consider the width-\(p\) scaled 2-layer ReLU network, evolving via the flow_

\[_{t}(x)=-_{P(X)}K_{(t)}^{p}(x,X)^{}(X,f_{t}(X)), \]

_where \(f_{t}\) denotes \(f(,(t))\). Let \(f^{*}\) denote the unique minimizer of \(L=L_{F}\) from Lemma 1, and fix \(>0\). Then, under conditions (C1)-(C4), (D1)-(D4), and (E1)-(E5), there exists \(T>0\) such that almost surely_

\[[_{p}L(f_{T})] L(f^{*})+. \]

Regularity conditions (C1)-(C4), (D1)-(D4), and (E1)-(E5) are provided in Appendices C, D, and E, respectively. We consider a scaled two-layer ReLU network architecture (further detailed in Appendix C) and use this simple architecture to prove the results as the network width \(p\) tends to infinity. Our results may also be extended to multilayer perceptrons with other activation functions. Below, we briefly sketch the key ingredients needed to prove Theorem 1.

Recall the NTK \(K_{}^{p}\) from Equation (4), where we now let \(p\) denote the network width. For certain neural network architectures, Jacot et al. (2018) show that as the network width \(p\) tends to infinity, the neural tangent kernel becomes stable and tends (pointwise) towards a fixed, positive-definite limiting neural tangent kernel \(K_{}\).

Under suitable positivity conditions on the limiting kernel, we take the domain \(\) of \(L_{F}\) to be the RKHS with kernel \(K_{}\) (Section 2.3). Because \(L_{F}\) has a unique minimizer \(f^{*}\), under mild conditions on \(K_{}\), \(f^{*}\) may be characterized as the solution obtained by following kernel gradient flow dynamics in \(\), that is, the ODE given by

\[_{t}(x)=-_{P(X)}K_{}(x,X)^{}(X,f_{t}(X)).\]

In other words, starting from some function \(f_{0}\), following the _limiting_ NTK gradient flow dynamics above minimizes the functional objective \(L_{F}\) for sufficiently large \(T\).

**Lemma 2**.: _Let \(f^{*}\) denote the minimizer of \(L_{F}\) from Lemma 1, and \(>0\). Fix \(f_{0}\), and let \(K_{}\) denote the limiting neural tangent kernel. Let \(f_{0}\) evolve according to the dynamics_

\[_{t}(x)=-_{P(X)}K_{}(x,X)^{}(X,f_{t}(X)).\]

_Suppose that the conditions of Lemma 1 and (E1)-(E3) hold. Then, there exists \(T>0\) such that \(L(f_{T}) L(f^{*})+\), where \(L\) is the loss functional of \(L_{F}\)._

Appendix E enumerates regularity conditions (E1)-(E3) and provides a proof of Lemma 2. The characterization of \(f^{*}\) in Lemma 2 clarifies how the analysis of the parametric objective \(L_{P}\) will proceed. The gradient flow \(L_{P}\) causes the network function to similarly evolve according to a kernel gradient flow via the empirical neural tangent kernel, that is,

\[(x;(t))=-_{P(X)}K_{(t)}^{p}(x,X)^{}(X,f(X;( t))),\]

as derived in Section 2.2. Comparison of the minimizers of \(L_{P}\) and \(L_{F}\) can be accomplished by comparing the two gradient flows above, i.e. kernel gradient flow dynamics that follow \(K_{(t)}^{p}\) and \(K_{}\), respectively. As \(K_{(t)}^{p} K_{}\) (Appendix D), these trajectories should not differ greatly: for any fixed \(T\), the functions obtained by following the kernel gradient dynamics with \(K_{(t)}^{p}\) and \(K_{}\) can be made arbitrarily close to one another, provided \(p\) is sufficiently large. The proof of Theorem 1 first selects a \(T\) using Lemma 2, and then bounds the difference in the trajectories on \([0,T]\) for sufficiently large width \(p\) by convergence of the kernels \(K_{(t)}^{p} K_{}\). Our proof differs from previous results in that it relies on _uniform_ convergence of kernels (cf. Appendices C and D), enabling the analysis of population quantities such as \(_{P(X)}(X,f(X))\).

Theorem 1 proves convergence to an \(\)-neighborhood of the global solution when optimizing \(L_{P}\) despite the highly nonconvex nature of this optimization problem in the network parameters \(\). For sufficiently flexible network architectures, optimization of \(L_{P}\) thus behaves similarly to that of \(L_{F}\), which we have shown is a convex problem in the function space \(\) in Section 3.

## 5 Experiments

Having established conditions under which global convergence is guaranteed, the main aim of our experiments is to demonstrate approximate global convergence in practice, even for scenarios where the conditions assumed in our proofs are not satisfied exactly. Section 5.1 demonstrates that finite-neuron layer widths used in practice approximate the limiting behavior well, while Section 5.2 and Section 5.3 utilize problem-specific network architectures for amortized inference. Our results suggest that there may exist weaker assumptions under which global convergence is still guaranteed.

### Toy Example

We first assess whether the asymptotic regime of Theorem 1 is relevant to practice with layers of finite width. We use a diagnostic motivated by the lazy training framework of Chizat et al. (2019), which provides the intuition that in the limiting NTK regime, the function \(f\) behaves much like its linearization around the initial weights \(_{0}\):

\[f(x;) f(x;_{0})+J_{}f(x;_{0})(-_{0}). \]

Liu et al. (2020) prove that equality holds exactly in the equation above if and only if \(f(x;)\) has a constant tangent kernel (i.e., \(K_{}\)). Therefore, similarity between \(f\) and its linearization indicates that the asymptotic regime of the limiting NTK is approximately achieved. Note that even if \(f\) is linear in \(\), as in the above expression, it may still be highly nonlinear in \(x\).

We consider a toy example for which \(||x||_{2}=1\). The generative model first draws a rotation angle \(\) uniformly between \(0\) and \(2\), and then a rotation perturbation \(Z(0,^{2})\), where we take \(=0.5\). Conditional on \(\) and \(Z\), the data \(x\) is deterministic: \(x=[(+z),(+z)]^{}\). This construction ensures that the data lie on the sphere \(^{1}^{2}\), which guarantees the positivity of the limiting NTK for certain architectures (Jacot et al., 2018). We aim to infer \(\) given a realization \(x\), marginalizing over the nuisance latent variable \(Z\). Our variational family \(Q(;f(x))\) is a von Mises distribution, whose support is the interval \([0,2]\). This family is an exponential family distribution, allowing for the application of Lemma 1. The encoder network \(f(;)\) is given by a dense two-layer network (that is, one hidden layer) with rectified linear unit (ReLU) activation, which we study as the network width \(p\) grows. The network outputs \(f(x;)\) parameterize the natural parameter \(\).

We fit the neural network \(f(x;)\) in two ways. First, we use SGD to fit the network parameters \(\) to minimize \(L_{P}\). Second, we fit the linearization \(f_{}(x;)=f(x;_{0})+J_{}f(x;_{0})(-_{0})\) in \(\). We perform both of these fitting procedures for various widths \(p\). For both settings, stochastic gradient estimation was performed by following the procedure in Appendix B. For evaluation, we fix \(N=1000\) independent realizations \(x_{1}^{*},,x_{N}^{*}\) from the generative model with underlying ground-truth latent parameter values \(_{1}^{*},,_{N}^{*}\), and evaluate the held-out negative log-likelihood (NLL), \(-_{i=1}^{N} q(_{i}^{*} f(x_{i}^{*};))\), for both functions: \(f(x;)\) and \(f_{}(x;)\). Figure 1 showsthe evolution of the held-out NLL across the fitting procedure for three different network widths \(p\): \(64,256\) and \(1024\). The difference in quality between the linearizations and the true functions at convergence diminishes as the width \(p\) grows; for \(p=1024\), the two are nearly identical, providing evidence that the asymptotic regime is achieved.

### Label Switching in Amortized Clustering

In this experiment and the subsequent one, we consider a more diverse set of network architectures. Although Theorem 1 assumes a shallow, dense network architecture, these experimental results show that empirically, global convergence can be obtained for many other architectures as well. We consider the difficult problem of amortizing clustering. In this problem, we are given an unordered collection (set) of data \(x=\{x_{1},,x_{n}\}\), \(x_{i}\) and our objective is to infer the locations and labels of the cluster centers from which the data were generated. The generative model first draws a shift parameter \(S(0,100^{2})\) followed by

\[Z(S=s) ([_{1}+s,,_{d}+s]^{},^ {2}I_{d})\] \[X_{i}(Z=z) }{{}}_{j=1}^{d}p_{j}(z_{j},^{2}).\]

In the above, \(^{2},^{2},^{d}\) and \(p^{d}\) are known variances, locations, and proportions, respectively. The global parameter \(S=s\) shifts the \(d\) locations \(_{1},,_{d}\) to \(_{1}+s,,_{d}+s\). The cluster centers \(Z\) are then obtained by adding noise to these locations. An implicit labeling is imposed on the centers by assigning each a dimension in \(^{d}\) via the prior. Finally, the data are drawn independently from a mixture of \(d\) univariate Gaussians with centers \(Z_{i}\), \(i=1,,d\). We consider the tasks of inferring the scalar shift \(S\) and the vector of cluster centers \(Z\). We fix \(=[-20,-10,0,10,20]^{}\) with \(d=5\). We fix the hyperparameters \(=0.5\) and \(=0.1\), and artificially fix the shift as \(S=100\) to generate \(n=1000\) independent realizations \(X=x\) from the generative model.

Inferring \(S\) should be straightforward because the vector \(\) is known and fixed, ensuring that the joint likelihood \(p(x,s)\) (marginalizing over \(z\)) is unimodal in \(s\). However, inference on \(Z\) may be difficult for likelihood-based methods because the order of the entries of \(Z\) matters: the joint density \(p(x,z,s) p(x,(z),s)\) for a permutation \(\), even though \(p(x z)=p(x(z))\). "Label switching" can thus pose a significant obstacle for likelihood-based methods, as any permutation of the cluster centers \(Z\) will still explain the data well. This problem formulation thus results in a likelihood, and hence a posterior density, with many local optima but a single global optimum (i.e., where the cluster centers have the correct labeling).

Now we show that likelihood-based approaches to variational inference, such as maximizing the evidence lower-bound (ELBO), result in suboptimal solutions compared to the minimization of \(L_{P}\). We take the variational distributions \(q(S;f_{1}(x;_{1}))\) and \(q(Z;f_{2}(x;_{2}))\) to be Gaussian. We fit the networks \(f_{1}\) and \(f_{2}\). Due to the exchangeability of the observations \(x=\{x_{1},,x_{n}\}\), we parameterize each as permutation-invariant neural networks, fitting both \(_{1}\) and \(_{2}\) to either minimize \(L_{P}\) or to maximize the ELBO (see Appendix F). We perform 100 replications of this experiment across different random seeds, and consider two different parameterizations of the Gaussian variatio

Figure 1: Negative log-likelihood across gradient steps, for network widths \(64,256\), and \(1024\) neurons. NLL for the exact posterior is denoted by the red line.

distribution: a mean-only parameterization with fixed unit variance and a natural parameterization with an unknown mean and unknown variance. Both of these variational families are exponential families, and so convexity (in the sense of Corollary 1) holds.

Figure 2 plots kernel-smoothed frequencies of point estimates of \(S\), where each point estimate is the mode of the variational posterior for an experimental replicate. Both ELBO- and \(L_{P}\)-based training estimate \(S=100\) well. However, the limitations of ELBO-based training are evident in the fidelity of the posterior approximation of \(Z\). Table 2 plots the average \(_{1}\) distance \(||-Z||_{1}\) between the variational mode \(\) and the true latent draw \(Z\). Optimization of the ELBO converges to a local optimum that is a permutation of the entries of \(Z\), resulting in a large \(_{1}\) distance on average. Minimization of \(L_{P}\), on the other hand, converges to the global optimum without any label switching. Table 1 indicates the degree of label switching, showing the proportion of trials in which the entries \(\) were correctly ordered.

### Rotated MNIST Digits

We consider the task of inferring a shared rotation angle \(\) for a set of \(N\) MNIST digits. The generative model is as follows. The rotation angle is drawn as \((0,2)\), and for all \(i[N]\) a noise term is drawn as \(Z_{i}(0_{p},^{2}I_{p})\). Finally, each image is drawn as

\[X_{i}(Z_{i}=z_{i},=)(( z_{i},),^{2}),\;\;i[N]. \]

Here, RotateMNIST is fitted ahead of time and fixed throughout this experiment. Given a latent representation \(z\) and angle \(\), RotateMNIST returns a \(28 28\) MNIST digit image rotated counterclockwise by \(\) degrees. (See Appendix F for additional experimental details.) We aim to fit the variational posterior \(q(;f(x_{1},,x_{N};))\), implicitly marginalizing over the nuisance latent variables \(Z_{1},,Z_{N}\). The true data \(\{x_{i}\}_{i=1}^{N}\) are generated from the above model, with \(N=1000\) digits and an underlying counterclockwise rotation angle of \(=260\) degrees. We provide visualizations of some of these digits in Figure 3. The variational distribution \(q(;f(x_{1},,x_{N};))\) is taken to be a von Mises distribution with natural parameterization, as in Section 5.1, although for this example the architecture of the encoder network makes \(f\) invariant to permutations of the inputs \(\{x_{i}\}_{i=1}^{N}\), to reflect the exchangeability of the data. We fit \(f\) to minimize \(L_{P}\) using 100,000 iterations of SGD.

We compare to fitting \(\) to directly maximize the likelihood of the data \(p(,\{x_{i}\}_{i=1}^{N})\). As this quantity is intractable, we maximize the importance-weighted bound (IWBO), which is a generalization of the ELBO (Burda et al., 2016), by maximizing the joint likelihood \(p(,\{z_{i}\}_{i=1}^{N},\{x_{i}\}_{i=1}^{N})\) in \(\) while fitting a Gaussian variational distribution on the variables \(Z\).

The likelihood function is multimodal in \(\) due to the approximate rotational symmetry of several handwritten digits: zero, one, and eight are approximately invariant under rotations of 180 degrees. Additionally, the digits six and nine are similar following 180-degree rotations. These symmetries yield a multimodal posterior distribution on \(\). Likelihood-based fitting procedures, such as maximizing the IWBO, often get stuck in these shallow local optima, while fitting \(f\) to minimize the parametric objective \(L_{P}\) finds a unique global solution. Figure 4 shows estimates of the angle \(\) conditional on the data \(\{x_{i}\}_{i=1}^{N}\) during training with the IWBO objective, with \(\) initialized to a variety of values. For some initializations, the IWBO optimization converges quickly to near the correct value of \(260\) degrees, but in many others, it converges to a shallow local optimum.

We perform the same routine, fitting \(q(;f(x_{1},,x_{N};))\) to minimize the expected forward KL divergence \(L_{P}\). Figure 5 shows that across a variety of initializations of the angles, this approach always converges to a unique solution and fits the posterior mode to the correct value of 260 degrees. \(L_{P}\) minimization converges rapidly, and so Figure 6 zooms in on the initial few thousand gradient steps to show the various trajectories among initializations.

### Local Optima vs. Global Optima

In this experiment, we directly compare the quality of the variational approximation minimizing the expected forward KL objective to that of the local optima found by optimizing the ELBO. We consider an adapted version of the rotated MNIST digit problem outlined above. Each digit \(x_{i}\) is drawn from the model \(x_{i}((x_{i}^{0},),^ {2})\) for \(i=1,,50\) with angle set to \(_{}=260\) degrees. The unrotated digits \(x_{i}^{0}\) are fixed a priori, eliminating the nuisance latent variables \(Z\) in this setting. This allows us to directly perform ELBO-based variational inference on \(\), instead of merely maximizing the likelihood (or a bound thereof) as in Section 5.3. We fit an amortized Gaussian variational distribution with fixed variance \(^{2}=0.5^{2}\) for inference on \(\), and use the same prior as above. This is an exponential family with only one location parameter to be learned.

Fitting \(q(;f(x_{1},,x_{50};))\) is performed by minimizing either the negative ELBO or the expected forward KL divergence. The only differences between the two approaches are their objective functions: the data, network architecture, learning rates, and all other hyperparameters are the same. We optimize each objective function over 10,000 gradient steps, and measure the quality of the obtained variational approximations by a variety of metrics, including: the (non-expected) forward KL divergence; the reverse KL divergence; negative log-likelihood; and the angle point estimate. Intuition suggests that a global optimum should outperform local ones, and we find this to be the case. By any of

Figure 4: Estimate of angle \(\) across gradient steps, with fitting performed to maximize the IWBO.

Figure 5: Mode of \(q(;f(x_{1},,x_{N};))\) across training (starting at different initializations) when minimizing objective \(L_{P}\).

Figure 3: 100 of the \(N=1000\) data observations with counterclockwise rotation of 260 degrees.

Figure 6: Zoomed-in trajectories across the first 2000 gradient steps, showing similar estimates regardless of initialization.

the performance metrics we consider, the global solution of the expected forward KL minimization outperforms the variational approximations found by optimizing the ELBO.

## 6 Discussion

In this work, we showed that in the asymptotic limit of an infinitely wide neural network, gradient descent dynamics on the expected forward KL objective \(L_{P}\) converge to an \(\)-neighborhood of a unique function \(f^{*}\), a global minimizer. Our results depend on several regularity conditions, the most important of which is the positive definiteness of the limiting neural tangent kernel, the compactness of the data space \(\), and the specific architecture considered, a single hidden-layer ReLU network. We conjecture and show experimentally that global convergence holds more generally. First, we illustrate that the asymptotic regime describes practice with a finite number of neurons (see Section 5.1). Second, our conditions allow for a wide variety of activation functions beyond ReLU (see Appendix D). Third, empirically, global convergence can be achieved with many network architectures. Beyond multilayer perceptrons, we illustrate global convergence for convolutional neural networks (CNNs) and permutation-invariant architectures in Section 5.2 and Section 5.3.

Expected forward KL minimization is a likelihood-free inference (LFI) method. For Bayesian inference, likelihood-based and likelihood-free methods are not typically viewed as competitors, but as different tools for different settings. In a setting where the likelihood is available, the prevailing wisdom suggests utilizing it. However, our results suggest that likelihood-free approaches to inference may be preferable even when the likelihood function is readily available. We find likelihood-based methods are prone to suffer the shortcomings of numerical optimization, often converging to shallow local optima of ELBO-based variational objectives. Expected forward KL minimization instead converges to a unique global optimum of its objective function.

There are situations in which ELBO optimization may nevertheless be preferable. First, if the likelihood function of the model is approximately convex and well-conditioned in the region of interest, ELBO optimization should recover a nearly global optimizer. Second, in certain situations, the ELBO can be optimized using deterministic optimization methods, which can be much faster than SGD. Third, if the generative model has free model parameters, with the ELBO they can be fitted while simultaneously fitting the variational approximation, with a single objective function for both tasks. Fourth, the ELBO can be applied with non-amortized variational distributions, which can have computational benefits in settings with few observations. Many important inference problems do not fall into any of these four categories. Even for those that do, the benefits of expected forward KL minimization, including global convergence, may outweigh the benefits of ELBO optimization.

Figure 7: Forward and reverse KL divergences to the true posterior across fitting for minimization of the expected forward KL (blue) or the negative ELBO (green). We also plot the negative log likelihood of the true angle, as well as the variational mode (true angle \(_{}\) is plotted in red.)