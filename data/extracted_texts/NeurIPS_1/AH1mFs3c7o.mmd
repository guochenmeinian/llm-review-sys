# InterControl: Zero-shot Human Interaction Generation by Controlling Every Joint

Zhenzhi Wang\({}^{1}\), Jingbo Wang\({}^{2}\), Yixuan Li\({}^{1}\), Dahua Lin\({}^{1,2}\), Bo Dai\({}^{3,2}\)

\({}^{1}\)The Chinese University of Hong Kong, \({}^{2}\)Shanghai Artificial Intelligence Laboratory,

\({}^{3}\)The University of Hong Kong

{wz122,ly122,dhlin}@ie.cuhk.edu.hk, wangjingbo@pjlab.org.cn

bdai@hku.hk

###### Abstract

Text-conditioned motion synthesis has made remarkable progress with the emergence of diffusion models. However, the majority of these motion diffusion models are primarily designed for a single character and overlook multi-human interactions. In our approach, we strive to explore this problem by synthesizing human motion with interactions for a group of characters of any size in a zero-shot manner. The key aspect of our approach is the adaptation of human-wise interactions as pairs of human joints that can be either in contact or separated by a desired distance. In contrast to existing methods that necessitate training motion generation models on multi-human motion datasets with a fixed number of characters, our approach inherently possesses the flexibility to model human interactions involving an arbitrary number of individuals, thereby transcending the limitations imposed by the training data. We introduce a novel controllable motion generation method, InterControl, to encourage the synthesized motions maintaining the desired distance between joint pairs. It consists of a motion controller and an inverse kinematics guidance module that realistically and accurately aligns the joints of synthesized characters to the desired location. Furthermore, we demonstrate that the distance between joint pairs for human-wise interactions can be generated using an off-the-shelf Large Language Model (LLM). Experimental results highlight the capability of our framework to generate interactions with multiple human characters and its potential to work with off-the-shelf physics-based character simulators. Code is available at [https://github.com/zhenzhiwang/intercontrol](https://github.com/zhenzhiwang/intercontrol).

## 1 Introduction

Generating realistic and diverse human motions is a vital task in computer vision, as it has diverse applications in VR/AR, games, and films. In recent years, great progress has been achieved in human motion generation by introducing VAE , Diffusion Model  and large language models . These methods commonly investigated single-person motion generation given texts or action classes , part of motion , or other related modalities , yet overlooked multi-person interactions. By naively putting their generated single-person motions in a shared global space, such motions could easily penetrate each other. They cannot even perform simple interactions like handshaking due to lack of the ability to control two people's hands to reach the same location at the same time. Many multi-person datasets  lacks text annotations and focus on motion completion given prefix motions. Recently, InterGen  collected a two-person interaction generation dataset, and let model to learn two-person motions from data. It is limited by the fixed number of characters and cannot generalize to arbitrary numbers. Previous methods commonly ignore a good design for general interaction modeling.

This paper investigates a special yet widely used form of human interactions: interactions that could be quantitatively described by spatial relations of human joints, such as distances or orientations, as shown in Fig. 1 (a) and (b). Such interactions are conceptually simple, as their semantics are almost from spatial relations. Thus, they do not require additional interaction data. It only needs pretrained models from single-person data and could be generalized to an arbitrary number of humans. We define _human interactions_ as steps of _joint-joint contact pairs_ and devise a single-person motion generation model to take such contact pairs as control signals. Besides, orientations could also be used in control, such as making two people face each other. In this way, interaction generation is transformed to controllable motion generation. Inspired by , we adapt descriptions of interactions as joint contact pairs by leveraging Large Language Models (LLMs). Thus, human interactions are annotation-free, and interactions could also involve multiple human joints.

As interactions are adapted to our defined joint contact pairs, the key challenge to generate interactions is the _precise spatial control_ to satisfy the constraint of spatial controls. This difficulty lies in two parts: (1) the discrepancy between _control signals in global space_ and _relative motion representation_ in mainstream pretrained models : As semantics of motions are independent to global locations, previous works  commonly utilize the relative motions, where global locations could only be inferred by aggregating velocities. It poses challenges to control local human poses with global conditions. Previous attempts  exploit the inpainting ability of a pretrained model, yet they are unable to control global joints. GMD  proposes a two-stage model of separated root trajectory generation and local pose generation. Although it manages to control root positions, controlling _every joint at any time_ is still infeasible. (2) the _sparse_ control signals in the motion sequence: Control signals could be sparse in both temporal and joint dimension, model needs to adaptively adjust trajectories in uncontrolled frames to satisfy the intermittent constraints.

In this paper, we propose InterControl, a novel human interaction generation method that is able to precisely control the position of any joint at any time for any person, and it is only trained on single-person motion data. By adding spatial controls to MDM , InterControl is a unified framework of two types of spatial control modules: (1) _Motion ControlNet_ inspired by ControlNet : It is initialized from a pretrained MDM  and takes global spatial locations as input for joint control in the global space. It is able to generate coherent and high-fidelity motions yet joint positions in global space are not perfect. (2) _Inverse Kinematics (IK) Guidance_ for joint locations: To further align generated motions and spatial conditions precisely, we use inverse kinematics (IK)  to guide the denoising steps towards desired positions. It could be regarded as a classifier guidance ,

Figure 1: InterControl is able to generate **interactions of a group of people** given joint-joint **contact** or **separation** pairs as spatial condition, and it is only trained on **single-person data**. Our generated interactions are realistic and similar to real interactions in internet images in (a) daily life and (b) fighting. (c) shows our generated group motions (red dots) could serve as reference motions for physics animation.

yet it has no extra classifiers. We utilize L-BFGS  as the optimizer to directly align the global conditions in the local space. With two proposed modules, InterControl is able to control multiple joints of any person at any time. Furthermore, InterControl is able to jointly optimize multiple types of spatial controls, such as orientation alignment, collision avoidance, and joint contacts, as long as the distance measures in IK guidance are differentiable. By exploiting its joint control ability, our model is able to generate multi-person interactions with rich contacts, where no multi-person interaction datasets are needed. Our generated interactions could further serve as the reference motion to generate physical animation with meaningful human-wise reactions in simulators. As shown in Fig. 1 (c), one character could actually hit down the other with his fits by taking our generated fighting motions as input. Extensive experiments in HumanML3D  and KIT-ML  datasets quantitatively validates our joint control ability, and the user study on generated interactions shows a clear preference over previous methods.

To summarize, our contributions are twofold: (1) We are the first to generate multi-person interactions with a single-person motion generation model in a zero-shot manner. (2) We are the first to perform precise spatial control of every joint in every person at any time for interaction generation.

## 2 Related Work

### Human Motion Generation

Synthesizing human motions is a long-standing topic. Previous efforts integrate extensive multimodal data as condition to facilitate conditional human motion generation, including text [15; 14; 46; 71; 55; 6; 30], action label [13; 45], part of motion [10; 19; 55], music [35; 34; 56], speech [3; 18] and trajectory [49; 27; 28]. As texts are free-form information that convey rich semantics, recent progress in motion generation are mainly based on text conditions. For example, FLAME  introduces transformer  to process variable-length motion data and language description. MDM  introduces the diffusion model and uses classifier-free guidance for text-conditioned motion generation. MLD  further incorporates a VAE  to encode motions into vectors and makes the diffusion process in the latent space. Physdiff  integrates physical simulators as constraints in the diffusion process to make the generated motion physically plausible and reduce artifacts. PriorMDM  treats pretrained MDM  as a generative prior and controls MDM by motion inpainting. Our InterControl also use a pretrained MDM, yet we further train a Motion ControlNet instead of using inpainting. A concurrent work OmniControl  also incorporate classifier guidance  and controlnet  modules to control all joints in MDM, yet it focuses on single-person motion generation and does not investigate human interaction generation.

### Human-related Interaction Generation.

As human motions could be affected or interacted by surrounding humans [72; 29; 57], objects [66; 54; 12; 33; 26] and scenes [62; 63; 64; 73; 20; 61], generating interactions is also an important topic. Previous methods are mainly about human-scene/object interaction. For example, Interdiff  uses the contact point of human joints and objects as the root to generate object motions. UniHSI  exploits LLM to generate contact steps between human joints and scene parts as an action plan and control the agent perform the plan via reinforcement learning. As previous human-human interactions datasets [42; 59] only contains very few multi-person sequences, previous human-human interaction methods [60; 67] are mainly limited to unsupervised motion completion without texts. Recently, InterHuman dataset  is proposed for text-conditioned multi-person interaction generation, yet it only consider the two-person situation and is not able to model more people's interaction. To the best of our knowledge, we are the first to enable a single-person text-conditioned motion generation model to perform interactions between a group of people by controlling diverse joints of each person.

### Controllable Diffusion Models

Diffusion-based generative models have achieved great progress in generating various modalities, such as image [50; 22; 9; 53], video [11; 17; 24] and audio . Conditions and controlling ability in diffusion models are also well studied: (1) Inpainting-based methods [8; 7] predict part of the data with the observed parts as condition and rely on diffusion model to generate consistent output, which is used in PriorMDM . (2) Classifier-guidance  trains a separate classifier and exploits the gradient of classifier to guide the diffusion process. Our InterControl inherits the spirit of classifier-guidance, yet our guidance is provided by Inverse Kinematics (IK) and no classifier is needed. (3) Classifier-free guidance  trains a conditional and an unconditional diffusion model simultaneously and trade-off its quality and diversity by setting weights. (4) ControlNet  introduces a trainable copy of pretrained diffusion model to process the condition and freezes the original model to avoid degeneration of generation ability. It enables diverse types of dense control signals for various purpose with minimal finetuning effort. Our InterControl also incorporate the idea of ControlNet  to finetune the pretrained MDM  to process spatial control signals and improve the quality of generated motions after joint control.

## 3 InterControl

InterControl aims to generate interactions with only single-person motion data by precisely controlling every joint of every person at any time, conditioned on text prompts and joint relations. We first formulate interaction generation in Sec. 3.1, and then introduce control modules for a single-person motion diffusion model in Sec. 3.3 and Sec. 3.4. Finally we show details to generate interactions from our model in Sec. 3.5.

### Formulation of Interaction Generation

Inspired by human-scene interaction , we define human interactions as joint contact pairs \(=\{_{1},_{2},\}\), where \(_{i}\) is the \(i^{th}\) contact step. Taking two-person interaction as an example, each step \(\) has several contact pairs \(=\{\{j_{1}^{1},j_{1}^{2},t_{1}^{s},t_{1}^{e},c_{1},d_{1} \},\{j_{2}^{1},j_{2}^{2},t_{2}^{s},t_{2}^{e},c_{2},d_{2}\},\}\), where \(j_{k}^{1}\) is the joint of person 1, \(j_{k}^{2}\) is the joint of person 2, \(t_{k}^{s}\) and \(t_{k}^{e}\) means the start and end frame of the interaction, \(c_{k}\) means contact type from {contact, avoid} to pull or push the joint pairs, \(d_{k}\) is the desired distance in the interaction. By converting the contact pairs \(\) to the mask \(\) and distance \(d\), and taking others' joint positions as condition, we could guide the multi-person motion generation process to interact between joints in the form of spatial distance. In this way, interaction generation is transformed to be controllable single-person motion generation taking a text prompt \(\) and a spatial control signal \(^{N J 3}\) as input. Its goal is to predict motion sequence \(^{N D}\) whose joints in the global space is aligned with spatial control \(\), where \(N\) is number of frames, \(J\) is number of joints (e.g., 24 in SMPL ), and \(D\) is the dimension of relative joint representations (e.g., 263 in HumanML3D ). Incorporating spatial control in motion generation presents challenges due to the discrepancy between relative motion representation \(\) and global \(\).

### Human Motion Diffusion Model (MDM)

**Relative Motion Representation.** HumanML3D  dataset proposes a widely-used [55; 68; 51; 6] relative motion representation, and is proved to be easier to learn realistic motions, as the semantics of human motion is independent of global positions. It consists of root joint velocity, other joints' positions, velocities and rotations in the root space, and foot contact labels. To convert it to the global space, root velocities are aggregated, then other joints will be computed based on root. Please refer to Sec. 5 of HumanML3D  for details. Due to such discrepancy, previous inpainting-based methods [55; 51] is not able to control MDM in global space. GMD  decouples motion generation to two separated generation process of root trajectory and pose relative to root, yet it can only control root joint. Directly adopting global joint positions to generate motions yields unnatural human poses, such as unrealistic limb lengths.

**Diffusion Process in MDM.** Motivated by the success of image diffusion models [22; 50; 70; 9; 53], Motion Diffusion Model (MDM)  is proposed to synthesize sequence-level human motions conditioned on texts \(\) via classifier-free guidance . The diffusion process is modeled as a noising Markov process \(q(_{t}_{t-1})=(}_{t-1},(1-_{t}))\), where \(_{t}(0,1)\) are small constant hyper-parameters, thus \(_{T}(0,)\) if \(_{t}\) is small enough. Here \(_{t}^{N D}\) is the entire motion sequence at denoising time-step \(t\), and there are \(T\) time-steps in total. Thus, \(_{0}\) is the clean motion sequence, and \(_{T}\) is a random noise to be sampled. The denoising Markov process is defined as \(p_{}(_{t-1}_{t},)=(_{}(_{t},t,),(1-_{t})),\) where \(_{}(_{t},t,)\) is the estimated posterior mean for the \(t-1\) step from a neural network based on the input \(_{t}\) and \(\) is its parameters. Following MDM, we predict the clean motion \(_{0}(_{t},t,;)\) instead of the noise \(\) via a transformer , and the posterior mean \(_{}(_{t},t,)\) is 

### Motion ControlNet for MDM

As MDM is initially conditioned on texts \(\), it requires fine-tuning to accommodate spatial conditions \(\). This is challenging due to the potential sparsity of \(\) across temporal and joint dimensions: (1) Control may be required for only a few joints, necessitating adaptive adjustment of the remaining joints to preserve realistic motion. (2) Control may be desired for only a select few frames, thus the model must interpolate natural human motions for the rest of the sequence.

Inspired by ControlNet , we introduce Motion ControlNet to generate realistic and high-fidelity motions guided by condition \(\). It is a trainable copy of MDM, while MDM is frozen in our training process. Each transformer encoder layer in ControlNet is connected to its MDM counterpart via a zero-initialized linear layer. This allows InterControl to commence training from a state equivalent to a pretrained MDM, acquiring a residual feature for \(\) in each layer through back-propagation. To process \(\), the uncontrolled joints, frames, and XYZ-dim are masked as \(0\). We find that the vanilla \(^{N 3,J}\) is effective enough to control the pelvis (root) joint, yet it is still sub-optimal for other joints. Thus, we design a relative condition indicating the distance from the current positions of each joint to \(\). Suppose \(R()\) is a forward kinematics (FK) to convert relative motion \(^{N D}\) to global space \(R()^{N J 3}\), the relative condition is \(^{}=-R()\). To provide additional clues, we also use \(^{}=-R()^{root}\) to represent the distance from the current root to the desired position. We also use the normal of triangles (pelvis, left/right shoulder) \(^{s}\) and (pelvis, left/right hip) \(^{h}\) to represent the current orientation of human. The final condition passed to ControlNet is \(^{final}=(^{}||^{}||^{s}||^{h})\), where \(||\) is concatenation. Please refer to Appendix A.2 for more details.

**Network Training.** Motion ControlNet is the only part that needs finetuning in our framework, while IK guidance is an optimization method in the test time and the LLM in our framework is an off-the-shelf GPT-4 . We adopt the standard ControlNet  training strategy, and the only difference is the data format: we first convert the relative motion to be global locations by FK, and then use random masks that keeps part of global joints to be non-zero as spatial control signals. The training objective is identical to MDM. The spatial conditions are randomly sampled in the temporal or joint dimension. The training data is single-person data only, e.g., HumanML3D .

Figure 2: **Overview. Our model could precisely control human joints in the global space via the Motion ControlNet and IK guidance module. By leveraging LLM to adapt interaction descriptions to joint contact pairs, it could generate multi-person interactions via a single-person motion generation model in a zero-shot manner.**

### Inverse Kinematics (IK) Guidance

While Motion ControlNet can adapt joint positions according to sparse conditions, the alignment between predicted poses and global spatial conditions often lacks precision. As Inverse Kinematics (IK) is a classic method for optimizing joint rotations to achieve specific global positions, we employ it to guide the diffusion process towards spatial conditions at test time in a classifier guidance  manner, named IK guidance.

**IK Guidance on general form of losses.** Inspired by classifier guidance  and loss-guided diffusion , we employ losses in the global space to steer the denoising process. IK guidance accommodates various forms of distance measurements, enabling both minimization and maximization for flexible control over joint interactions, such as attraction or repulsion. Given the global position \(^{N J 3}\), the distance between a joint and condition is \(_{nj}=\|_{nj}-R(_{t})_{nj}\|_{2}\), where \(_{t}\) is short for \(_{}(_{t},t,)\) mentioned in Sec. 3.2, and \(R()\) is forward kinematics (FK). To allow the interaction of joints with some given distances \(d^{}^{N J 3}\), loss of one joint is \(_{nj}=(_{nj}-d^{}_{nj})\) to make the joint and condition be **contacted** within distance \(d^{}_{nj}\); and it is \(_{nj}=(d^{}_{nj}-_{nj})\) to make the joint and condition be **far away**, where ReLU is a function to keep values \( 0\) and set values \( 0\) to \(0\). Finally, with a binary mask \(\{0,1\}^{N J 3}\), the total loss for all joints and frames is

\[L(_{t},)=_{j}_{nj}_{nj}}{_ {n}_{j}_{nj}}, \]

As \(_{2}\)-loss and FK are highly differentiable, we optimize \(L(_{t},)\) in Equ. 2 w.r.t \(_{t}\) using the second-order optimizer L-BFGS , which is commonly used in Inverse Kinematics, rather than first-order gradient methods. Classifier guidance  utilizes a pre-trained image classifier to direct the diffusion towards a target image class by the gradient \(_{_{t}} f_{}(y_{t})\), where \(f_{}\) is the classifier, \(y\) is image class. Unlike this method, we do not rely on a large neural network classifier. L-BFGS has been demonstrated to better align global positions and offer quicker convergence than first-order methods. We update the posterior mean \(_{t}\) using L-BFGS for \(k\) iterations at each denoising step, where \(k\) is a hyper-parameter. This optimization facilitates both pull and push types of IK guidance, corresponding to two contact types in our interaction model. To maintain consistency in data distribution between training and inference, we also apply IK guidance when training ControlNet. Additionally, employing IK guidance on \(_{0}\) eliminates the need for training Motion ControlNet, thus enhancing training efficiency. In practice, using L-BFGS on both \(_{0}\) and \(_{t}\) can yield satisfactory joint and spatial condition alignment. Detailed algorithm for interaction generation is presented in Appendix A.1.

As the root position at frame \(n\) is derived from cumulative root velocities up to frame \(n\) in FK, a single condition at frame \(n\) can influence all preceding root positions. This effect also extends to non-root joints, as their global positions are calculated from the root. Consequently, IK guidance can adaptively modify velocities from the start to frame \(n\) to meet the condition at frame \(n\). Moreover, IK guidance can control any combination of human joints, frames or XYZ-dims, such as controlling the left hand and right foot at a specific frame \(n\).

### Interaction Generation

Inverse Kinematics (IK) guidance can optimize various distance measures to facilitate interactions such as avoiding obstacles, preventing collisions, facilitating face-to-face engagements, or enabling joint contacts between individuals. This method allows for intricate interactions among any human joints for an indefinite number of people, despite being trained exclusively on single-person data. As delineated in Section 3.1, we characterize interactions as pairs of contacting joints. A notable feature of our IK guidance in generating interactions is that both terms of the IK guidance loss function are predicted, allowing for simultaneous optimization within a single process. Specifically, the single-person loss \(L_{single}(_{t},)\) transforms into \(L_{multi}(_{t}^{a},_{t}^{b})\) for interactions, where \(a\) and \(b\) represent two individuals. The L-BFGS optimizer concurrently optimizes both participants by minimizing \(L_{multi}(_{t}^{a},_{t}^{b})\), with \(_{t}^{a}\) and \(_{t}^{b}\) being the respective joints engaged in interaction. Beyond distance measures, our IK guidance can optimize orientation measures as well. For example, one can calculate a person's orientation through the spatial relationship of their joints, like the cross-product of vectors from the left shoulder to the right and from the pelvis to the head. By setting two individuals' unit orientation vectors to \(0\), they can face each other or turn away. To ensure they face each other, we can further adjust the relation between one person's orientation vector and the vector from their head tothe other's. Such orientation relationships are vital for producing realistic interactions when we only exploit single-person motion generation ability and can be easily expanded to include larger groups. Another useful strategy in IK guidance is to prevent collision through joint separation pairs, ensuring that the torso joints of two people (such as pelvis, hips, and spines) maintain a certain distance, thereby reducing the likelihood of collisions when other joints are in contact. Besides, we can also regulate the motion region by confining the root joints within the XZ-plane using IK guidance. For the PyTorch-like code illustrating loss functions that enforce joint contacts, separations, or orientation alignment, please refer to Appendix A.1 for details.

In our framework, interaction generation is realized by using joint-joint contact pairs as control signals. These pairs can be manually crafted by users to create desired interactions, akin to utilizing ControlNet  in image generation. However, manually constructing joint contact pairs can be tedious, so we employ an automatic off-the-shelf GPT-4  as a planner. GPT-4 infers text prompts that describe the actions of multiple people, \(^{multi}\), and converts them into single-person prompts, \(\), and contact plans, \(\), through prompt engineering. The inputs for the LLM Planner include the multi-person sentences \(^{multi}\), background scenario details \(\), human joint data \(\), and predefined instructions, rules, and examples. Specifically, \(\) encompasses the number of individuals, total motion sequence frames, and video playback speed; \(\) contains names of all joints (for example, the 22 joint names in HumanML3D ); and the rules outline the joint contact pair format and guide the LLM to generate feasible contacts and timesteps. Our method leverages the pre-trained capabilities of GPT-4 to comprehend human joint relationships from interaction descriptions via prompt engineering without any fine-tuning. Thus, the inference process of our model is not related to LLMs, making our comparison with other methods be fair. Please refer to Appendix A.3 for details of prompts and contact plans.

## 4 Experiments

**Datasets.** We conduct experiments on HumanML3D  and KIT-ML  following MDM . HumanML3D contains 14,646 high-quality human motion sequences from AMASS  and HumanAct12 , while KIT-ML contains 3,911 motion sequences with more noises.

**Evaluation Protocol.** We adopt metrics suggested by _Guo et. al._ to evaluate the quality of alignment between text and motion, which are Frechet Inception Distance (**FID**), **R-Precision**, and **Diversity**. We also report metrics related to spatial controls following GMD  on HumanML3D dataset, which are **Foot skating ratio**, **Trajectory error**, **Location error** and **Average error**. Please refer to Appendix B.5 or papers  for more details.

Due to the page limit, we put the implementation details and text-to-motion generation in the Appendix B.1 and B.2.

### Single-Person Controllable Motion Generation

In Tab. 1, we compare InterControl with other spatially controllable methods . We also include results of MDM  to show the controlling metrics  without spatial control.MDM's trajectory can significantly deviate from the intended path in the absence of control signals, with an average error often exceeding 1m. In contrast, inpainting-based control, unaware of global spatial information, results in considerable divergence, as seen with PriorMDM . GMD  decouples this problem and generates root trajectories in the global space, so it achieves better performance in spatial control metrics. However, its limitation to only the root joint constrains its spatial control and interaction capabilities. Our InterControl could achieve very small errors in spatial control metrics for all-joint control thanks to the power of Inverse Kinematics and L-BFGS optimizer. Meanwhile, Motion ControlNet could ensure the motion data is still in the same distribution with the training set by adapting to the posterior mean updated by IK guidance in its training stage, leading to even better FID than previous methods. It is worth noting that we only use a single model to learn the control strategy for all joints, while previous method  needs to train separate models and blend them for multiple joints. Our method achieves similar performance with controlling one joint when extending it to control multiple joints (last two rows in Tab. 1). Compared to the recent concurrent work , we achieve significantly better FID and Traj./Loc. errors than it in both root joint control or random joint control. It  also shows a notable gap between two form of joint controls (0.310 vs. 0.218), while our method is more robust to joint variants (0.178 vs. 0.159) thanks to our special designsof more inputs in Motion ControlNet. Its R-precision and foot-skating ratio are slightly better than ours, we believe the reason is that their 1-st order optimization tolerates more errors when the joint alignment is hard. It is also supported by their worse Traj./Loc. yet better Avg. err., i.e., their method shows more outliers with large errors. However, their design need much more times of optimization compared to ours (e.g., 100 vs. 5) and leads to longer inference time than ours (120s vs. 80s).

### Zero-Shot Multi-Person Interaction Generation

To validate our model's interaction generation ability, we analyze the spatial control results in interaction scenarios and perform an user study to qualitatively compare our model with PriorMDM . We also introduce an potential application of our interaction generation method for physics animation.

**Spatial Control.** In Tab. 2 (left), we compare spatial-related metrics with PriorMDM in zero-shot human interaction generation. Specifically, we collect \(100\) descriptions of two-person actions from InterHuman Dataset  and let an off-the-shelf GPT-4  to adapt them to single-person motion descriptions and joint-joint contact pairs via prompt engineering (see Tab. 7 in Appendix). Then, we utilize an InterControl model pretrained on the HumanML3D dataset to generate human interactions conditioned on text prompts and joint contact pairs. The spatial-related metrics are reported over controlled joints and frames. InterControl achieves good performance of spatial errors in interaction scenarios, indicating its robustness in precise spatial control for multiple humans. In contrast, PriorMDM  could only take interaction descriptions as input and unable to perform spatial control, leading to much larger spatial errors.

**User Study.** We conduct a user study to qualitatively compare our method with PriorMDM on the text-conditioned two-person interaction generation. 134 unique users were participating in the user study, where each user will answer 19 single choice questions to compare our results with PriorMDM . Results in Tab. 2 (right) shows that our generated interactions are clearly preferred over PriorMDM by a percent of 81.2%. We also shows an example sequence of qualitative comparison with PriorMDM  in the user study in Fig. 3. PriorMDM  shows severe torso collision between two human skeletons and the generated two-people motion is not aligned with the interaction description, while our model has no torso collision thanks to the collision avoidance loss in our IK guidance. Besides, our method also produces reasonable kicking actions between two people according to the semantics of interaction description. Please refer to Appendix B.4 for details.

**Qualitative results:** Although our model is only trained on single-person data, it is still possible to generate interactions between an arbitrary number of people via our designed format of interaction. In Fig. 4, we show two representative results of zero-shot interaction generation. (1) Two-person dancing: In addition to the single person dancing from the pretrained ability of single-person model, we further let them hold hands from time to time and prevent them from collision between their torsos. To further make their dance natural, we also employ a loss to promote their orientations to

    &  &  &  &  & Foot skating & Traj. err. \(\) & Loc. err. \(\) & Avg. err. \(\) \\  & & & (Top-3) & & ratio \(\) & (50 cm) & (50 cm) & (m) \\  Real data & - & 0.002 & 0.797 & 9.503 & 0.0000 & 0.0000 & 0.0000 & 0.0000 \\ MDM  & No Control & 0.544 & 0.611 & 9.446 & 0.0943 & 0.8909 & 0.6015 & 1.1843 \\  PriorMDM \({}^{}\) & & 0.498 & 0.586 & 9.167 & 0.0924 & 0.3726 & 0.2210 & 0.4552 \\ GMD \({}^{}\) & & 0.276 & 0.655 & 9.245 & 0.1108 & 0.0997 & 0.0356 & 0.1457 \\ OmitControl  & & 0.218 & 0.687 & 9.422 & **0.0547** & 0.0387 & 0.0096 & **0.0338** \\ Ours & & **0.159** & 0.671 & 9.482 & 0.0729 & **0.0132** & **0.0004** & 0.0496 \\  OmitControl  & & 0.310 & **0.693** & **0.502** & 0.0608 & 0.0617 & 0.0107 & 0.0404 \\ Ours & & 0.178 & 0.669 & 9.498 & 0.0968 & 0.0403 & 0.031 & 0.0741 \\  Ours & Random two & 0.184 & 0.670 & 9.410 & 0.0948 & 0.0475 & 0.0030 & 0.0911 \\ Ours & Random three & 0.199 & 0.673 & 9.352 & 0.0930 & 0.0487 & 0.0026 & 0.0969 \\   

Table 1: **Spatial control results on HumanML3D . \(\) means closer to real data is better. _Random One/Two/Three_ reports the average performance over 1/2/3 randomly selected joints in evaluation. \({}^{}\) means our evaluation on their model.**

   Spatial Errors & Traj. err. (20 cm) \(\) & Loc. err. (20 cm) \(\) & Avg. err. (m) \(\) & User-study & Preference \\  PriorMDM  & 0.6931 & 0.3487 & 0.6723 & PriorMDM  & 18.8\% \\ Ours & **0.0082** & **0.0005** & **0.0084** & Ours & **81.2\%** \\   

Table 2: Evaluation on (left) spatial errors and (right) user preference in interactions.

be face-to-face. (2) Three-person fighting: In addition to a single person performing punching and kicking, we further let them punch or kick others' head and torso, and also prevent their torsos from collision. Compared to existing interaction generation method , our method is able to generate interaction between any number of people, while InterGen  is only able to generate two-person interaction. Besides, our method is the first method to leverage single-person motion generation model to generate human interactions in a zero-shot manner.

**Application:** Our method is able to seamlessly integrate with off-the-shelf character simulation approaches, allowing us to synthesize physically plausible human reactions. As shown in Fig. 1 (c), our method synthesizes the motions, where the orange character is fighting with other two characters, as the reference of the SoTA physics-aware motion imitator . The interactions of our motions are designed to hit heads of other characters with fists. Leveraging the precise spatial control provided by our approach, the animated characters in the simulator can accurately respond to these impacts, resulting in realistic reactions such as being knocked down. This capability to generate spatially coherent multi-human interactions enables our method to improve the plausibility and responsiveness of synthesized reactions within physics-based character animations.

### Ablation Studies

To further investigate the effectiveness of InterControl, we ablate our method in Tab. 3 and reveal some key information in controlling the motion generation model in the global space. Then we also analyze the computational costs of our method to ensure our control is efficient. We will refer to the variants of InterControl by row numbers in Tab. 3. All experiments are trained on all joints and evaluated with randomly selected joints to report average performance.

**Motion ControlNet.** By dropping ControlNet, we find that IK guidance could still follow spatial controls with very low errors, yet the motion quality (e.g., FID) is significantly damaged (row 1 vs. row 2). Our ControlNet could adapt to the posterior distribution updated by IK guidance, and produce high-quality motion data. We also find that our \(^{final}\) provides key information in controlling all joints: For root control only, the FID of \(^{final}\) and \(\) shows small difference. However, the FID of root control is always slightly better than all-joint control (\( 0.07\)) when we use \(\), indicating insufficient information in all-joint control. We alleviate this by introducing extra information in \(^{final}\) for Motion ControlNet and improve the FID of all-joint control from 0.227 (row 3) to 0.178 (row 1).

**IK guidance.** By dropping IK guidance, Motion ControlNet can produce good semantic-level metrics (e.g., FID) compared with MDM by using extra spatial cues (row 4). However, this variant will lead to more spatial errors and cannot strictly follow spatial controls in global space. As precise joint alignment is vital for interactions, IK guidance is important for our InterControl. Another variant is

Figure 4: **Qualitative results of zero-shot human interaction generation.**

Figure 3: Comparison with PriorMDM  in **user-study** of zero-shot human interaction generation.

updating IK guidance on ControlNet's prediction \(_{0}\) (row 5), instead of the posterior mean \(_{t}\). Its advantage is faster training speed because IK guidance is no longer needed in training ControlNet (similar to classifier guidance ) yet it leads to slightly worse FID than using \(_{t}\). We believe the reason is that IK guidance still changes the data distribution in denoising steps even if it is updated on \(_{0}\). Finally, we also report the result of 1-st order gradient in classifier guidance  (row 6) instead of L-BFGS. We find it takes more computations to achieve similar performance with L-BFGS, which is analyzed below.

**Inference time analysis.** In practice, we find that IK guidance in last few denoising steps (e.g., \(t\)) is vital for precise joint control, while most denoising steps \(t\) are less important yet take most of computations. IK guidance on \(_{0}\) with only once L-BFGS in \(t\) and \(10\) times in \(t\) could leads FID \(0.234\) in controlling all joints, yet leads to minimal extra computations. We report its total inference time of \(1000\) denoising steps by adding sub-modules step-by-step in Tab. 4. GMD  needs \(110\)s to run two-stage diffusion models, while we only needs 80s. Gradient-based optimization in the recent work  needs \(120\)s to achieve similar control quality. Leveraging GPU parallel computing capabilities, InterControl can efficiently generate motions for a batch of \(32\) people in \(91\) seconds, enabling efficient group motion generation.

**Sparse control signals in temporal.** As a key challenge of spatial control is the sparsity, we also report results with sparsely selected frames as control (sparsity = \(0.25\) and \(0.025\)) in Tab. 3 (row 7 and 8). Our model demonstrates consistent performance in both spatial error and semantic-level metrics when using sparse signals, e.g., FID \(0.255\) and avg. err. \(0.0467\) with sparsity \(0.025\), while GMD  achieves FID \(0.523\) and avg. err. \(0.139\) with the same sparsity.

## 5 Conclusion and Limitations

We presented InterControl, a multi-person interaction generation method that is only trained on single-person motion data. It could generate interactive human motions of an arbitrary number of people. We achieve this by enabling a text-conditioned motion generation model with the ability to control every joint of every person at any time. We propose two complementary modules, named Motion ControlNet and IK guidance, to improve both the spatial alignment between joints and desired positions, and the overall quality of whole motions. Extensive experiments are conducted on HumanML3D and KIT-ML benchmarks to validate the effectiveness and efficiency of our proposed modules. We enable InterControl the ability of text-conditioned interaction generation by leveraging the knowledge of LLMs. Qualitative results and user study validate that InterControl could generate high-quality interactions by precise spatial joint control.

**Limitations.** As InterControl is not trained on multi-person data, its definition of interaction is based on distances (being _contacted_ or _separated_) or orientations. Its motion quality is from motion generation model trained on single-person motion data, and the plausibility of interactions is from the knowledge of LLMs, i.e., to what extent the joint contact pairs are consistent to the semantics of interaction descriptions. Yet, InterControl could generate interactions of an arbitrary number of people, while all existing interaction generation methods cannot.

   Sub-Modules & MDM & + Control Module & + Guidance \(t\) & + Guidance \(t\) \\  Time (s) & 39.1 & 57.3 & 76.5 & 80.1 \\   

Table 4: **Inference time analysis** on a NVIDIA A100 GPU.

   Item & Method & FID \(\) &  R-precision \(\) \\ (Top-3) \\  &  Diversity \(\) \\  &  Foot skating \\ ratio \(\) \\  &  Traj. er. \(\) \\ (50 cm) \\  &  Loc. err. \(\) \\ (50 cm) \\  & 
 Avg. err.\(\) \\ (m) \\  \\  (1) & Ours (random joint) & **0.178** & 0.669 & **9.498** & 0.0968 & 0.0403 & 0.0031 & 0.0741 \\  (2) & w/o ControlNet & 0.965 & 0.621 & 9.216 & 0.1624 & 0.0879 & 0.0059 & 0.1013 \\ (3) & w/ original c & 0.227 & 0.656 & 9.544 & 0.1004 & 0.0697 & 0.0042 & 0.0785 \\  (4) & w/o IK guidance & 0.187 & 0.664 & 9.598 & **0.0704** & 0.8569 & 0.4553 & 0.6557 \\ (5) & IK guidance on \(_{0}\) & 0.211 & 0.668 & 9.394 & 0.1164 & 0.0907 & 0.0088 & 0.0981 \\ (6) & w/ 1-st order grad & 0.198 & 0.668 & 9.472 & 0.0987 & 0.0879 & 0.0096 & 0.0877 \\  (7) & sparsity = 0.25 & 0.248 & **0.671** & 9.442 & 0.0801 & 0.0106 & 0.0007 & 0.0546 \\ (8) & sparsity = 0.025 & 0.255 & 0.663 & 9.520 & 0.0705 & **0.0015** & **0.0001** & **0.0067** \\   

Table 3: **Ablation studies** on the HumanML3D  dataset.

**Acknowledgment.** This project is funded in part by Shanghai Artificial Intelligence Laboratory, CUHK Interdisciplinary AI Research Institute, and the Centre for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technology Commission (ITC)'s InnoHK. We would like to thank Tianfan Xue for his insightful discussion.