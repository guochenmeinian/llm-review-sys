# Breaking Determinism: Fuzzy Modeling of Sequential Recommendation Using Discrete State Space Diffusion Model

Breaking Determinism: Fuzzy Modeling of Sequential Recommendation Using Discrete State Space Diffusion Model

Wenjia Xie Hao Wang1 Luankang Zhang Rui Zhou Defu Lian Enhong Chen

University of Science and Technology of China & State Key Laboratory of Cognitive Intelligence

xiaohulu@mail.ustc.edu.cn,wanghao3@ustc.edu.cn,zhanglk5@mail.ustc.edu.cn, zhou_rui@mail.ustc.edu.cn,liandefu@ustc.edu.cn,cheneh@ustc.edu.cn

###### Abstract

Sequential recommendation (SR) aims to predict items that users may be interested in based on their historical behavior sequences. We revisit SR from a novel information-theoretic perspective and find that conventional sequential modeling methods fail to adequately capture the randomness and unpredictability of user behavior. Inspired by fuzzy information processing theory, this paper introduces the DDSR model, which uses fuzzy sets of interaction sequences to overcome the limitations and better capture the evolution of users' real interests. Formally based on diffusion transition processes in discrete state spaces, which is unlike common diffusion models such as DDPM that operate in continuous domains. It is better suited for discrete data, using structured transitions instead of arbitrary noise introduction to avoid information loss. Additionally, to address the inefficiency of matrix transformations due to the vast discrete space, we use semantic labels derived from quantization or RQ-VAE to replace item IDs, enhancing efficiency and improving cold start issues. Testing on three public benchmark datasets shows that DDSR outperforms existing state-of-the-art methods in various settings, demonstrating its potential and effectiveness in handling SR tasks.

## 1 Introduction

For a long time, sequential recommendation (SR) has been attracting increasing attention due to its excellent performance and significant commercial value (Chen et al. (2020), Qiu et al. (2021), Yin et al. (2024), Han et al. (2024)). Unlike traditional collaborative filtering or certain graph-based methods (Wang et al. (2019), Zhang et al. (2024), Wang et al., Tong et al. (2024)), SR systems emphasize the inherent dynamic behaviors of users rather than relying solely on structured data (Chen et al. (2022), Ma et al. (2020), Cen et al. (2020)). This approach enhances the accuracy of personalized recommendations, allowing for more precise tracking of changes in user interests and needs. Typical deep learning-based SR models, such as those utilizing CNN, RNN, and Transformer architectures (Tang and Wang (2018), Hidasi et al. (2015), Kang and McAuley (2018)), have achieved remarkable success in modeling user historical interaction data.

However, these methods are formalized models based on a narrow information theory assumption (Shannon (1948)), which only acknowledges determinism (Rosas et al. (2020)). They assume that all phenomena strictly adhere to mechanical laws and that the states of motion of objects at different times can be uniquely determined. In reality, however, user behavior is characterized by randomness and unpredictability. They might change their mind about buying a down jacket due to a sudden warm-up, or they might impulsively buy desserts due to a breakup. As illustrated on the left in Figure 1, a user's interest at any given moment might be focused on'some items' with blurred boundaries, only converging finally when the user makes a selection.

Although increasing the sample size is an effective strategy to address the above issue, in reality, the data in recommendation systems is usually quite sparse (He and McAuley (2016)), limiting the practicality of this strategy. Inspired by the theory of fuzzy information processing (Tanaka et al. (1976); Tanaka and Sommer (1977)), we believe that making the absolute membership relations in traditional sets more flexible is another effective way to solve the problem. In other words, it is not necessary to strictly limit the modeling of user interests to the items they have interacted with. Therefore, we propose using a diffusion model (Ho et al. (2020)) for fuzzy modeling of user interests, which enhances the model's performance by introducing perturbations during the training process.

We have noticed existing work that introduces diffusion models into SR (Xie et al. (2024)), such as DiffuRec (Li et al. (2023)) and DreamRec (Yang et al. (2023)), which focus on Gaussian diffusion processes operating within a continuous state space. They add Gaussian noise to the embedded representations of candidate items for recommendation through a forward diffusion process until the noise reaches a pure state (standard normal distribution). Subsequently, they iteratively sample from this noise using a reverse denoising process guided by historical interaction information to recover meaningful representations and recommend items most similar to these representations.

However, unlike our desire to fuzzily model interaction sequences, the aforementioned methods follow the form of diffusion models in the image domain and operate on candidate items. They introduce the crucial sequence information merely as conditional information, without leveraging the diffusion model's performance on it. On the other hand, these methods relax discrete interaction data into a continuous space and introduce noise, which may lead to distortion or loss of meaning in the original discrete space, as the addition of noise could push data points away from any meaningful discrete state. Therefore, we hope that state transitions occur under discrete conditions for the entire interaction sequence, which is discrete diffusion. Based on this, we have proposed our DCSR (Discrect Diffusion Sequential Recommendation model), which uses a directed graph to model sequential recommendation. In this model, all interaction items are viewed as nodes, and transitions between items are treated as directed edges. Discrete diffusion is used to enable structured transitions of nodes, with the resulting new sets treated as fuzzy sets, as shown in the middle of Figure 1. By designing the transition matrix, we can achieve uniform transitions or importance-based transitions for the nodes, ensuring controllability. In Section 3.3, we theoretically demonstrate the reliability and effectiveness of modeling on these generated fuzzy sets, based on the principles of information diffusion. During the inference stage, we refer to the sampling formula for discrete diffusion but start from the historical interaction sequence rather than from noise, iteratively generating refined results.

Furthermore, we have found that the excessive number of items involved in the recommendation problem leads to a high-dimensional transition matrix, resulting in inefficient diffusion transitions. Additionally, item IDs themselves do not contain any prior information, which poses a challenge in determining beneficial transition directions. To address this issue, we have further introduced semantic tags to replace meaningless item IDs, using quantization techniques and VQ-VAE to derive

Figure 1: Illustration of DDSR constructing fuzzy sets and incorporating semantic IDs to enhance sequential recommendations. In real-world scenarios, a userâ€™s final choice often reflects their immediate interests (left subfigure). We reconstruct the true evolution of interests by constructing fuzzy sets for each item in the interaction sequence (middle subfigure). The right subfigure provides an overview of the process of generating semantic IDs for recommendations based on item-related descriptions.

these tags from semantic information, thus reducing the size of the discrete space. We will provide specific details on how this can be achieved in 4.1, and a vivid illustration of this is given on the right side of Figure 1. Simultaneously, the introduction of semantic information has enhanced the model's generalization capability and effectively solved the cold start problem. We conducted extensive experiments on three public benchmark datasets, comparing DDSR with several state-of-the-art methods. The results demonstrate that DDSR significantly outperforms baseline methods in various settings and effectively handles cold-start recommendations.

## 2 Related Work

### Sequence Recommendation

SR suggests potential subsequent items based on users' historical interaction records (Yin et al. (2023); Wang et al. (2024)). Early research primarily relied on Markov chains and matrix factorization techniques for recommendation (He et al. (2016)). However, with the development of deep learning, efforts such as GRU4Rec (Hidasi et al. (2015)), Caser (Tang and Wang (2018)), and others have focused on designing neural network models to capture sequential dependencies in user behavior sequences. The introduction of the Transformer architecture (Vaswani et al. (2017)) in SASRec (Kang and McAuley (2018)) pioneered SR and quickly became the mainstream method in the field. Additionally, BERT4Rec (Sun et al. (2019)) utilizes bidirectional encoders to capture bidirectional dependencies in sequences, using a masked language model to predict the user's next action.

Recent studies have shown that high-quality high-dimensional embeddings are crucial for obtaining accurate recommendation results (Hou et al. (2022); Wang et al. (2021)). To this end, researchers are striving to leverage the rich attribute information of items to improve data representation. For example, TransFM (Pasricha and McAuley (2018)) introduces arbitrary real-valued features through factorization machines, while S3-Rec (Zhou et al. (2020)) designs four self-supervised learning tasks as pre-training objectives to learn context-aware data representations with attribute awareness. Furthermore, researchers like Hou et al. (2022); Zhao (2022); Harte et al. (2023) further utilize pre-trained language models (Yin et al. (2024)) to process item description texts, obtaining universal item representations with rich semantic information to enhance the performance (Wu et al. (2024)). VQ-Rec (Hou et al. (2023)) and TIGER (Rajput et al. (2024)) further employ quantization techniques (Jacob et al. (2018)) and RQ-VAE (Lee et al. (2022)) to obtain tokenized semantic IDs for recommendations, replacing semantic embeddings.

### Discrete Diffusion Models

Diffusion models, inspired by non-equilibrium thermodynamics, have been introduced and demonstrated significant results in fields such as computer vision, sequence modeling, and audio processing (Dhariwal and Nichol (2021); Rasul et al. (2021); Ho et al. (2022)). Most diffusion models are based on the Denoising Diffusion Probabilistic Model (DDPM) proposed by Ho et al. (2020), as well as the score-based generative models (SGMs) proposed by Song et al. (2020), targeting continuous data domains. We provide detailed descriptions of DDPM and SGMa in Appendix D to facilitate comparisons with the discrete diffusion approach we employ. Diffusion models in discrete state space are first described in Sohl-Dickstein et al. (2015) and later applied to text and image domains in D3PMs (Austin et al. (2021)). VQ-Diffusion (Gu et al. (2022)) utilizes them to eliminate unidirectional bias in text-to-image generation.

## 3 Discrete Diffusion Process of DDSR

In this section, we present the problem definition (Section 3.1) and illustrate how item sequences undergo discrete diffusion to obtain the corresponding fuzzy sets (Section 3.2). Finally, the effectiveness of this fuzzy modeling is theoretically demonstrated (Section 3.3). Please note that the actual diffusion and inference in DDSR occur at the semantic ID level, but this chapter discusses items.

### Problem Statement

Let \(\) be the set of users and \(\) be the set of discrete items in the dataset, \(||\) and \(||\) represent the number of elements in their respective sets. For each user \(\), \(v_{1:n-1}=[v_{1},v_{2},,v_{n-1}]\)represents his historical interaction sequence sorted by timestamp. The goal of the model is to predict the next item \(v_{n}\) that the user is most likely to interact with. To facilitate better discrete diffusion and for the convenience of subsequent theoretical derivations, we model each user's interaction sequence as a directed graph \(^{u}\). In this graph, each item represented by a semantic ID is regarded as a node, while transitions between items are viewed as directed edges. Specifically, an edge exists from \(v_{i}\) to \(v_{j}\) if and only if \(v_{j}\) is the next item interacted with by the user after \(v_{i}\).

### Node Diffusion Transition

A typical diffusion model transforms data \(x_{0} q(x_{0})\) into a sequence of gradually noisier latent variables \(x_{1:T}=x_{1},x_{2},...,x_{T}\) via forward process \(q(_{1:T}|_{0})=_{t=1}^{T}q(_{t}|_{t-1})\). In diffusion models within continuous state spaces, the forward distribution is typically set with \(q(_{t}|_{t-1})=(_{t}|}_{t-1},_{t})\) as a hyperparameter controlling the level of noise added at each step. As the number of time steps \(T\) approaches infinity, \(x_{T}\) converges to a standard Gaussian distribution. Beyond the limitations mentioned above, information loss due to diffusion into pure noise is another reason for unstable training and inadequate alignment of continuous diffusion with SR.

Continuous diffusion always operates on embeddings, while in diffusion models within discrete state spaces, categories are directly transformed. Transition matrices \([_{t}]_{ij}=q(x_{t}=j|x_{t-1}=i)\) are used to describe the probability of single-step diffusion transitions, where \(i\) and \(j\) represent categories within the domain. Denoting the one-hot version of \(x\) with the row vector \(\) (bold), then the one-step transition probabilities can be expressed as:

\[q(_{t}|_{t-1})=(_{t};=_{t-1}_{t }), \]

where \(Cat(;p)\) is the categorical distribution corresponding to the one-hot row vector \(\) with probabilities given by the row vector \(p\), and \(_{t-1}_{t}\) is understood as a row vector-matrix product. Starting from \(_{0}\), we obtain the following \(t\)-step marginal and posterior at time \(t-1\):

\[q(_{t}|_{0})=(_{t};=_{0}}_{t}),}_{t}=_{1} _{2}_{t}, \]

We take the set \(\) as the domain in SR. Each \(v_{i}\) in the interaction sequence is represented as a one-hot encoding \(_{i}^{0}\). Using the transition form defined by 2 enables transitions to other nodes, denoted as \(_{i}^{t}\) in the domain with a certain probability at any time step \(t\). Unlike continuous diffusion, which only allows noise addition, discrete diffusion models offer the advantage of controlling the data blurring process by selecting the transition matrix. Here, we present two strategies to select transition matrices, i.e. Uniform transition and Importance transition.

**Uniform transition.** Similar to the study by Hoogeboom et al. (2021), the natural idea is to maintain nodes with a certain probability \(_{t}(0,1)\) unchanged. In contrast, in other cases, nodes are randomly transformed into any other node in the domain with equal probability \((1-_{t})/(||-1)\). That is

\[[_{t}]_{ij}=(1-_{t})/(||-1)&i  j\\ _{t}&i=j. \]

Uniform transfer can be regarded as a special case of linear information allocation, thus theoretically affected by the size of the discrete space. It can compute the cumulative product \(}_{t}\) in closed form.

**Importance transition.** For data with certain prior knowledge, we propose transitioning between more similar nodes rather than uniformly transitioning to any other state, thus defining the matrix:

\[[_{t}]_{ij}=^{2}/2^{2}) }{_{v_{k}}(-d_{ik}^{2}/2^{2})}& i j\\ 1-_{k=0,k i}^{||-1}[_{t}]_{ik}& i=j. \]

Here, \(d_{ij}\) represents the distance between item \(v_{i}\) and \(v_{j}\), calculated using the square of the Euclidean distance. The parameter \(^{2}\) denotes the variance of the diffusion process. Consequently, the transition probabilities cannot be solved in closed form; instead, they can only be updated alongside the embeddings in the model. The importance transfer matrix adheres to the Gaussian information diffusion function \(f(x)=}e^{-}{2^{2}}}\). Therefore, it remains unaffected by the number of discrete points but necessitates the sample point distribution to closely resemble a Gaussian distribution.

This modeling approach appears to align more closely with our intuition, as a user's interests at a given moment often form a cluster of similar nodes (items). Only upon the user's final selection of an item does the 'neighborhood' converge to a single data point. This point represents the representative of the interest cluster and the ambiguity in information naturally dissipates. In recommendation tasks, we can only access the user's final choice at each moment, without knowledge of the interest cluster, reflecting incomplete knowledge. Regardless of the transition method employed, the key lies in transitioning the sample space from incomplete to complete, as detailed in the subsequent section.

### Completeness and Reliability

Here, we aim to demonstrate that the discrete diffusion spaces generated by the two methods in Section 3.2 are completions of the original space, and the models constructed on these fuzzy sets are solvable and effective. We first provide the formal definition of a complete sample space.

**Definition 1**.: _Let \(\) denote the sample space. For any sample \(W\), if \(W\) is complete, i.e., unbiased estimates can be obtained through certain mathematical processing, then \(\) is called a complete sample space; otherwise, it is called an incomplete sample space._

In SR, \(W\) is a user's behavior sequence; \(\) is all possible combinations of these behavior sequences; the domain \(V\) is all items in the dataset. Datasets in SR systems do not form a complete sample space, as they often consist of incomplete interaction data and potential selection biases. The principle of information diffusion ensures that when the given sample is incomplete, there exist reasonable diffusion functions that can improve non-diffusion estimates. Below we define information diffusion.

**Definition 2**.: _An information diffusion about a set \(W\) is defined by a mapping \(:W V\), satisfying the following conditions:_

1. \( w_{j} W\)_, if_ \(v_{j}\) _is the observed value of_ \(w_{j}\)_, then_ \((w_{j},v_{j})=_{v V}(w_{j},v)\)_._
2. \( w_{j} W\)_,_ \((w_{j},v)\) _decreases as_ \(\|v_{j}-v\|\) _increases._
3. \( w W\)_,_ \(_{v}(w,v)v=1\)_._

The diffusion estimates obtained using uniform transition and importance transition, as defined in Section 3.2, clearly adhere to Definition 2. To illustrate that the space resulting from discrete state transitions provides more information than the original state space, it is necessary to further demonstrate that this space serves as a completion of the original space. In other words, the new metric space is complete, with the original metric space serving as its dense subspace. This will be more precisely discussed in the following theorem.

**Theorem 3.1**.: _After information diffusion, the subsequent space must be an entirely separable metric space. Any model constructed in this space will assuredly possess a solution._

Proof in Appendix A. According to Theorem 3.1, since the space after information diffusion is equidistant isomorphism with the original space, it can be used to replace the sample space with insufficient information in SR to establish a model. On this complete space, predictive models are solvable, demonstrating that modeling on fuzzy sets is a reasonable and effective approach.

## 4 Learning and Inference of DDSR

### Obtaining Semantic IDs

As mentioned in Section 3, the indices \(i\) and \(j\) in the transition matrix \([_{t}]_{ij}\) represent categories in the discrete space, making \(_{t}\) a two-dimensional matrix with dimensions equal to the size of the discrete space. However, in recommendation tasks, the number of items involved can reach tens of thousands, posing a significant challenge in terms of computational resources if we were to use all item IDs as the discrete state space. Inspired by VQ-Rec and the recently proposed Tiger model by Google, we attempt to train recommendation models using semantic IDs instead of item IDs. A semantic ID is a codebook of length \(m\). Assuming we set the size of the codebook to \(K\), the entire codebook can represent \(K^{m}\) categories. Though we set each code from a different codebook, the state space only needs \(m*K\) nodes to store them. Additionally, the use of semantic IDs further introduces semantic information, addressing the scarcity of information inherent in recommendations, while also allowing the model to extend to unseen items, thus enabling cold-start recommendations. We provide the specific method for obtaining semantic IDs in the Appendix B.

```
0: historical interaction sequence \(v_{1:n-1}=c_{1:n-1;1:m}\); target item \(v_{n}=c_{n;1:m}\); transition matrix \(_{t}\); Approximator \(f_{}()\).
0: well-trained Approximator \(f_{}()\).
0: While not converged do:
1: Sample Diffusion Time: \(t[0,1,,T]\);
2: Calculate \(t\)-step transition probability: \(}_{t}=_{1}_{2}_{t}\);
3: Convert \(c_{n;1:m}\) to one-hot encoding \(^{0}_{n;1:m}\);
4: Obtain the discrete state \(x^{t}_{n;1:m}\) after \(t\) steps by Equation 2, thereby obtaining the 'fuzzy set' \(c^{t}_{1:n-1;1:m}\);
5: Modeling \(c_{2;n;1:m}\) based on 'fuzzy sets' through Equation 5;
6: Take gradient descent step on \( L_{CE}(_{2:n;1:m},c_{2:n;1:m})\).
```

**Algorithm 1** Training of DDSR.

```
0: historical sequence \(c_{1:n-1;1:m}\); well-trained Approximator \(f_{}()\); sampling step \(T\).
0: predicted target item \(v_{n}\).
1: Let \(_{T}=c_{1:n-1;1:m}\);
2: Let \(t=T\);
3:while\(t>0\)do
4: Use the trained \(f_{}()\) to obtain predictions \(}_{0}\) with \(_{t}\) and \(t\) as inputs;
5: Substitute \(_{0}\) into equation 7 to obtain the distribution of \(t-1\) step;
6:endwhile
7:\(_{n}=_{0}[-1;1:m]\);
8:if the same code project exists: \(v_{n}=_{n}\);
9:else: \(v_{n}\) is the project in the space closest to \(_{n}\).
```

**Algorithm 2** Inference of DDSR.

### Model Training

After introducing the Semantic ID, we convert the historical interaction sequence \(v_{1:n-1}\) into sequence \((c_{1,1},,c_{1,m};c_{2,1},,c_{2,m};;c_{n-1,1},,c_{n-1,m})\), abbreviated as \(c_{1:n-1;1:m}\). We convert them into one-hot encodings \((^{0}_{1,1},,^{0}_{1,m};;^{0}_{n-1,1},, {x}^{0}_{n-1,m})\), which is considered as the initial state for discrete diffusion. Then we perform discrete diffusion through the state transition formula defined in 2 (for more details, see Section 3.2) to obtain the discrete state after \(t\) steps \(^{t}_{i,j}\), for any \(i\{1,,n-1\}\) and \(j\{1,,m\}\). Accordingly, the labels changes from \(c_{i,j}\) to \(c^{t}_{i,j}\). Then \((c^{t}_{1,1},,c^{t}_{1,m};;c^{t}_{n-1,1},,c^{t}_{n-1,m})\) forms a "fuzzy set" of \(c_{1:n-1;1:m}\), denoted as \(c^{t}_{1:n-1;1:m}\), which can also be viewed as the state \(x_{t}\) of the diffusion transition at step \(t\).

Considering the suitability of the Transformer for sequence-to-sequence tasks, along with its well-demonstrated effectiveness in modeling sequential dependencies, we use it with an embedding layer as Approximator \(f_{}()\) to predict \(c_{2:n;1:m}\) with \(c^{t}_{1:n-1;1:m}\) as input. This approach differs from the common practice in diffusion models, which often focus on modeling noise. It aligns more closely with typical SR tasks that use \(v_{1:n-1}\) to predict \(v_{2:n}\), that is, the distribution \(_{}(}_{0}|_{t})\). We have adopted sinusoidal time step embeddings, which are added after the embedding layer, allowing the model to capture information about the time steps. This process can be represented by:

\[_{2:n;1:m}=f_{}(c^{t}_{1:n-1;1:m},t). \]

Generally, the loss function of diffusion models is designed based on KL divergence, or it can be simplified to mean-squared error. However, guided by the theory of information diffusion, we choose to use a cross-entropy loss function, which is more suitable for recommendation tasks, to optimize our model without being constrained by the aforementioned methods.

### Model Inference

In the inference phase, we aim to emulate the reverse process of the diffusion model, iteratively producing refined recommendation results. According to Bayes' theorem, we have

\[q(_{t-1}|_{t},_{0})=_{t}|_{t-1},_{0}) q(_{t-1}|_{0})}{q(_{t}|_{0})}=(_{t- 1};=_{t}_{t}^{}_{0}}_{t -1}}{_{0}}_{t}_{t}^{}}), \]

where \(\) represents the Hadamard product. Following the approach of Ho et al. (2020) and Hoogeboom et al. (2021), we employ the trained model \(f_{}()\) as described in Section 4.2 to derive the distribution \(_{}(}_{0}|_{t})\). Combining it with \(q(_{t-1}|_{t},_{0})\), we obtain the following parameterized expression:

\[p_{}(_{t-1}|_{t})=_{}_{0}}q(_{t-1}, _{t}|}_{0})_{}(}_{0 }|_{t}). \]

For the historical interactions \(v_{1:n-1}\), we use \(c_{1:n-1;1:m}\) as \(x_{T}\), and starting from \(t=T\), we iteratively execute Equation 7 until \(t=0\). This parameterization also allows us to perform inference for \(k\) steps at a time by predicting \(p_{}(_{t-k}|_{t})= q(_{t-k},_{t}|}_{0})_{}(}_{0}|_{t})\), leading to efficiency improvements. After obtaining \(_{0}\), we take its last component, which is a semantic ID of length m. If the corresponding item exists, we directly select that item; otherwise, we search for the item closest to it in the embedding space as the final recommendation result. The training and inference phase of DDSR are demonstrated in Algorithm 1 and Algorithm 2.

## 5 Experiment

### Experiment Settings

**Datasets**. We employ three real-world datasets to evaluate the performance of our DDSR model. Following some works on text-based recommendation (Li et al. (2023); Hou et al. (2022)), these datasets include two specific subcategories from the Amazon Reviews dataset (**Scientific** and **Office**), and a cross-platform dataset known as **Online Retail**, which operates from the UK. Following the method of Hou et al. (2022), we filter out users and items with fewer than five interactions. Subsequently, interaction behaviors within each sub-dataset are grouped by user and sorted chronologically. For the Amazon sub-datasets, product descriptions are formed by concatenating fields such as title, category, and brand, while for the Online Retail dataset, the description field is used. The product texts are truncated to 512 characters. Please refer to Table 1 for detailed descriptions of these datasets.

**Baselines**. We compare DDSR with eight state-of-the-art SR methods, including two conventional SR methods, three methods based on semantic information, and three generative SR methods:

**1). Conventional Baselines**: **SASRec**(Kang and McAuley (2018)) utilizes a causal Transformer architecture with a self-attention mechanism to model user behavior. **BERT4REC**(Sun et al. (2019)) proposes a bidirectional Transformer with a cloze task predicting the masked target items for SR.

**2). Semantic-based Baselines: UniSRec**(Hou et al. (2022)) utilizes the associated description text of items to learn transferable representations across different recommendation scenarios, using an enhanced mixture-of-experts adaptor to enhance domain fusion and adaptation. **VQ-Rec**(Hou et al. (2023)) maps item text to a vector of discrete indices for learning transferable sequential recommenders. **TIGER**(Rajput et al. (2024)) trains a Transformer-based sequence-to-sequence model with semantic IDs obtained from RQ-VAE to enhance its generalization ability.

**3). Generative Baselines: ACVAE**(Xie et al. (2021)) proposes an adversarial and contrastive variational autoencoder for SR combining the ideas of CVAE and GAN. **DiffuRec**(Li et al. (2023))

   Datasets & Users & Items & Interactions & Avg.length & Avg.num \\  Scientific & \(8442\) & \(4385\) & \(59\,427\) & \(7.04\) & \(182.87\) \\ Office & \(87\,436\) & \(25\,986\) & \(684\,837\) & \(7.84\) & \(193.22\) \\ Online Retail & \(16\,520\) & \(3469\) & \(519\,906\) & \(26.90\) & \(27.80\) \\   

Table 1: Detailed descriptions and statistics of datasets. â€™Avg. lengthâ€™ represents the average length of item sequences, while â€™Avg. numâ€™ indicates the average number of words in item text.

introduces the diffusion model into the field of SR reconstructing target item representation from a Transformer backbone with the user's historical interaction behaviors. **DreamRec**(Yang et al. (2024)) uses the historical interaction sequence as conditional guiding information for the diffusion model to enable personalized recommendations.

**Evaluation Settings**. Following previous works Hou et al. (2022); Zhao et al. (2022); Zhou et al. (2020), we evaluate all models using metrics Recall@K and NDCG@K, and report experimental results for \(K=10,50\). We employ the leave-one-out strategy for performance evaluation across all datasets. Concretely, we consider the last interaction as the test set, the second-to-last interaction as the validation set, and all preceding interactions as the training set. The ground-truth item of each sequence is ranked among all the other items while evaluating (Krichene and Rendle (2022)). The implementation details of DDSR are illustrated in Appendix C.1.

### Overall Performance

In this section, we compare the performance of DDSR with baseline models in terms of Top-\(K\) recommendation accuracy under consistent experimental conditions (same data preprocessing), as summarized in Table 2. For models that recommend based on item IDs, we provide semantic information to them by jointly utilizing fixed text embeddings obtained from pre-trained BERT and the embeddings corresponding to item IDs in the model's embedding layer, to ensure fairness in the experimental setup. For all models, the final table records the better of the three methods, using only ID, only text, or both text and ID.

We observe that text-enhanced SR methods (UniSRec, VQ-Rec, TIGER) tend to benefit from textual information, leading to improved performance compared to conventional methods in most cases. Notably, VQ-Rec, employing discrete semantic encoding, generally outperforms UniSRec, which relies on continuous text embeddings, across various settings. This is despite UniSRec already using techniques like parameter whitening and MoE-enhanced Adaptor to enhance textual information. We posit that an excessive emphasis on text similarity can yield suboptimal outcomes, while the conversion to codes mitigates the coupling between items and semantic information. The corresponding representations of the codes are renamed in the sequence-to-sequence model, allowing them to include more sequential structural information. While similarly based on discrete semantic encoding, the performance of TIGER is not stable. We do not rule out the possibility that there may be discrepancies between our implementation and the actual model, as it has not been open-sourced. Furthermore, we attribute the instability to TIGER's semantic ID length, which is limited to only 4 characters, potentially insufficient for expressing complex information.

In methods grounded in generative models, the performance of DiffuRec and DreamRec, based on diffusion models, surpasses that of ACVAE, relying on GAN and VAE. This disparity arises from the inherent advantages of diffusion models over VAE and GAN, as they circumvent the issue of posterior collapse, wherein the generated hidden representations lack crucial information about user preferences. Notably, DiffuRec achieves superior performance despite its limited capacity to handle semantic information, yet it still exhibits recommendation performance comparable to VQ-Rec. This suggests that diffusion models can yield effective hidden representations of items and users.

    &  &  &  \\   & R@10 & N@10 & R@50 & N@50 & R@10 & N@10 & R@50 & N@50 & R@10 & N@10 & R@50 & N@50 \\  SASRec\({}_{T}\) & 0.1049 & 0.0527 & 0.1754 & 0.0683 & 0.1047 & 0.0714 & 0.1638 & 0.0857 & 0.1461 & 0.0674 & 0.3781 & 0.1186 \\ BERT4RecID & 0.0473 & 0.0258 & 0.1092 & 0.0394 & 0.0798 & 0.0605 & 0.1207 & 0.0717 & 0.1354 & 0.0661 & 0.3517 & 0.1159 \\ UniSRec & 0.1104 & 0.0537 & 0.1890 & 0.0787 & 0.1024 & 0.0621 & 0.1668 & 0.0798 & 0.1274 & 0.0598 & 0.3294 & 0.1054 \\ VQ-Rec\({}_{T}\) & 0.1129 & 0.0577 & 0.2046 & 0.0749 & 0.1090 & 0.0676 & 0.1714 & 0.0845 & 0.1532 & 0.0713 & 0.3975 & 0.1254 \\ TIGER\({}_{T}\) & 0.1057 & 0.0597 & 0.1803 & 0.0682 & 0.1056 & 0.0712 & 0.1597 & 0.0868 & 0.0745 & 0.0390 & 0.2216 & 0.0701 \\ ACVAE\({}_{}\) & 0.0463 & 0.0315 & 0.0906 & 0.0457 & 0.0549 & 0.0397 & 0.1003 & 0.0519 & 0.0884 & 0.0410 & 0.1897 & 0.0648 \\ DiffuRecID & 0.1145 & 0.0594 & 0.1079 & 0.0752 & 0.1056 & 0.0689 & 0.1718 & 0.0832 & 0.0402 & 0.0189 & 0.0849 & 0.0321 \\ DreamRecID\({}_{T}\) & 0.0845 & 0.0421 & 0.1645 & 0.0688 & 0.0954 & 0.0557 & 0.1662 & 0.0694 & 0.0577 & 0.0261 & 0.0997 & 0.0544 \\
**DDSR\({}_{T}\)** & **0.1207*** & **0.0663*** & **0.2153*** & **0.0842*** & **0.1138*** & **0.0768*** & **0.1926*** & **0.0925*** & **0.1687*** & **0.0876*** & **0.4021** & **0.1322*** \\  Improv. & +5.41\% & +11.61\% & +5.23\% & +6.99\% & +4.40\% & +8.14\% & +6.46\% & +7.93\% & +10.12\% & +22.86\% & +1.16\% & +5.42\% \\   

Table 2: Performance of different models. Bold (underline) is used to denote the best (second-best) metric, and '*â€™ indicates significant improvements relative to the best baseline (t-test P<.05). â€™R@Kâ€™ (â€˜N@Kâ€™) is short for â€˜Recall@Kâ€™ (â€˜NDCG@Kâ€™). The features of items have been listed, whether ID, text (T), or both (ID+T).

[MISSING_PAGE_FAIL:9]

**Impact of Sampling Step \(S\).** The sampling step \(S\) represents the diffusion step number divided by the number of inference steps executed simultaneously, rounded down to the nearest integer. We illustrate in Figure 1(b) the influence of various sampling step settings. The model demonstrates optimal performance with approximately 50 sampling steps, with a slight increase for more complex datasets, albeit without significant disparities. Excessive sampling steps prolong the inference time without commensurate performance improvements, while inadequate steps lead to decreased performance.

**Efficiency Analysis.** We compared the time complexity and specific running overhead of DDSR with several other baseline algorithms, and the detailed results can be found in Appendix C.2.

## 6 Discussion

We proposed the DDSR model for the sequential recommendation, which employed discrete diffusion to construct fuzzy sets of user interaction sequences. This process was iteratively refined during inference, utilizing the sampling formula for discrete diffusion to derive the ultimate recommendation outcomes. Notably, although DDSR had borrowed the form of diffusion and sampling over time steps from diffusion models, it fundamentally differed from directly using diffusion models. If we viewed sequential recommendation through the lens of causality, the interaction sequence was the 'cause' and the recommended item was the 'effect'. Diffusion models typically address the target, blurring the 'effect', whereas DDSR has blurred the 'cause', inspired by the theory of fuzzy information processing. Although dual assurances, both theoretical and experimental results, have been provided to substantiate the superior performance of DDSR, it is imperative to recognize its inherent limitations. Despite our efforts to implement efficient computational methods, the nature of diffusion and sampling processes inevitably results in reduced efficiency and increased time complexity. Potential refinements, such as approximating the diffusion process and accelerating the sampling algorithm, could offer effective strategies, which we will explore in future work.

## 7 Acknowledgments

This work is supported by the National Natural Science Foundation of China (No. U23A20319, 62202443). Hao Wang also thanks the CCF-Tencent Rhino-Bird Open Research Fund (RAGR20230124).