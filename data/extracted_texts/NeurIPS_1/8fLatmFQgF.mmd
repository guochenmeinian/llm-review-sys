# Uniform-in-Time Wasserstein Stability Bounds

for (Noisy) Stochastic Gradient Descent

 Lingjiong Zhu1, Mert Gurbuzbalaban2,3, Anant Raj4,5, Umut Simsekli5

1: Dept. of Mathematics, Florida State University

2: Dept. of Management Science and Information Systems, Rutgers Business School

3: Center for Statistics and Machine Learning, Princeton University

4: Coordinated Science Laboratory, University of Illinois Urbana-Champaign

5: Inria Paris, CNRS, Ecole Normale Superieure, PSL Research University

###### Abstract

Algorithmic stability is an important notion that has proven powerful for deriving generalization bounds for practical algorithms. The last decade has witnessed an increasing number of stability bounds for different algorithms applied on different classes of loss functions. While these bounds have illuminated various properties of optimization algorithms, the analysis of each case typically required a different proof technique with significantly different mathematical tools. In this study, we make a novel connection between learning theory and applied probability and introduce a unified guideline for proving Wasserstein stability bounds for stochastic optimization algorithms. We illustrate our approach on stochastic gradient descent (SGD) and we obtain time-uniform stability bounds (i.e., the bound does not increase with the number of iterations) for strongly convex losses and non-convex losses with additive noise, where we recover similar results to the prior art or extend them to more general cases by using a single proof technique. Our approach is flexible and can be generalizable to other popular optimizers, as it mainly requires developing Lyapunov functions, which are often readily available in the literature. It also illustrates that ergodicity is an important component for obtaining time-uniform bounds - which might not be achieved for convex or non-convex losses unless additional noise is injected to the iterates. Finally, we slightly stretch our analysis technique and prove time-uniform bounds for SGD under convex and non-convex losses (without additional additive noise), which, to our knowledge, is novel.

## 1 Introduction

With the development of modern machine learning applications, understanding the generalization properties of stochastic gradient descent (SGD) has become a major challenge in statistical learning theory. In this context, the main goal is to obtain computable upper-bounds on the _population risk_ associated with the output of the SGD algorithm that is given as follows: \(F():=_{x}[f(,x)]\), where \(x\) denotes a random data point, \(\) is the (unknown) data distribution defined on the data space \(\), \(\) denotes the parameter vector, and \(f:^{d}\) is an instantaneous loss function.

In a practical setting, directly minimizing \(F()\) is not typically possible as \(\) is unknown; yet one typically has access to a finite data set \(X_{n}=\{x_{1},,x_{n}\}^{n}\), where we assume each \(x_{i}\) is independent and identically distributed (i.i.d.) with the common distribution \(\). Hence, given \(X_{n}\), one can then attempt to minimize the _empirical risk_\((,X_{n}):=_{i=1}^{n}f(,x_{i})\) as a proxy for \(F()\). In this setting, SGD has been one of the most popular optimization algorithms for minimizing\(()\) and is based on the following recursion:

\[_{k}=_{k-1}-_{k}(_{k-1},X_{n}), _{k}(_{k-1},X_{n}):=_{i_{k}}  f(_{k-1},x_{i}), \]

where \(\) is the step-size, \(b\) is the batch-size, \(_{k}\) is the minibatch that is chosen randomly from the set \(\{1,2,,n\}\), and its cardinality satisfies \(|_{k}|=b\).

One fruitful approach for estimating the population risk attained by SGD, i.e., \(F(_{k})\), is based on the following simple decomposition:

\[F(_{k})(_{k})+|(_{k})-F(_{k})|, \]

where the last term is called the _generalization error_. Once a computable upper-bound for the generalization error can be obtained, this decomposition directly leads to a computable upper bound for the population risk \(F(_{k})\), since \((_{k})\) can be computed thanks to the availability of \(X_{n}\). Hence, the challenge here is reduced to derive upper-bounds on \(|(_{k})-F(_{k})|\), typically referred to as generalization bounds.

Among many approaches for deriving generalization bounds, _algorithmic stability_ has been one of the most fruitful notions that have paved the way to numerous generalization bounds for stochastic optimization algorithms . In a nutshell, algorithmic stability measures how much the algorithm output differs if we replace one data point in \(X_{n}\) with a new sample. More precisely, in the context of SGD, given another data set \(_{n}=\{_{1},,_{n}\}=\{x_{1},,x_{i-1},_{ i},x_{i+1}, x_{n}\}^{n}\) that differs from \(X_{n}\) by at most one element, we (theoretically) consider running SGD on \(_{n}\), i.e.,

\[_{k}=_{k-1}-_{k}( _{k-1},_{n}),_{k}(_{k-1},_{n}):=_{i_{k}} f(_{k-1},_{ i}), \]

and we are interested in the discrepancy between \(_{k}\) and \(_{k}\) in some precise sense (to be formally defined in the next section). The wisdom of algorithmic stability indicates that a smaller discrepancy between \(_{k}\) and \(_{k}\) implies a smaller generalization error.

The last decade has witnessed an increasing number of stability bounds for different algorithms applied on different classes of loss functions. In a pioneering study,  proved a variety of stability bounds for SGD, for strongly convex, convex, and non-convex problems. Their analysis showed that, under strong convexity and bounded gradient assumptions, the generalization error of SGD with constant step-size is of order \(n^{-1}\); whereas for general convex and non-convex problems, their bounds diverged with the number of iterations (even with a projection step), unless a decreasing step-size is used. In subsequent studies  extended the results of , by either relaxing the assumptions or generalizing the setting to more general algorithms. However, their bounds still diverged for constant step-sizes, unless strong convexity is assumed. In a recent study,  proved stability lower-bounds for projected SGD when the loss is convex and non-smooth. Their results showed for general non-smooth loss functions we cannot expect to prove time-uniform (i.e., non-divergent with the number of iterations) stability bounds for SGD, even when a projection step is appended.

In a related line of research, several studies investigated the algorithmic stability of the stochastic gradient Langevin dynamics (SGLD) algorithm , which is essentially a 'noisy' version of SGD that uses the following recursion: \(_{k}=_{k-1}-_{k}(_{k-1},X_{n})+_ {k}\), where \((_{k})_{k 0}\) is a sequence of i.i.d. Gaussian vectors, independent of \(_{k-1}\) and \(_{k}\). The authors of  proved stability bounds for SGLD for non-convex losses, which were then extended to more general (non-Gaussian) noise settings in . While these bounds hinted at the benefits of additional noise in terms of stability, they still increased with the number of iterations, which limited the impact of their results. More recently,  proved the first time-uniform stability bounds for SGLD under non-convexity, indicating that, with the presence of additive Gaussian noise, better stability bounds can be achieved. Their time-uniform results were then extended to non-Gaussian, heavy-tailed perturbations in  for quadratic and a class of non-convex problems.

While these bounds have illuminated various properties of optimization algorithms, the analysis of each case typically required a different proof technique with significantly different mathematical tools. Hence, it is not straightforward to extend the existing techniques to different algorithms with different classes of loss functions. Moreover, currently, it is not clear how the noisy perturbations affect algorithmic stability so that time-uniform bounds can be achieved, and more generally, it is not clear in which circumstances one might hope for time-uniform stability bounds.

In this study, we contribute to this line of research and prove novel time-uniform algorithmic stability bounds for SGD and its noisy versions. Our main contributions are as follows:

* We make a novel connection between learning theory and applied probability, and introduce a unified guideline for proving Wasserstein stability bounds for stochastic optimization algorithms with a constant step-size. Our approach is based on Markov chain perturbation theory , which offers a three-step proof technique for deriving stability bounds: (i) showing the optimizer is geometrically ergodic, (ii) obtaining a Lyapunov function for the optimizer and the loss, and (iii) bounding the discrepancy between the Markov transition kernels associated with the chains \((_{k})_{k 0}\) and \((_{k})_{k 0}\). We illustrate this approach on SGD and show that time-uniform stability bounds can be obtained under a pseudo-Lipschitz-like condition for smooth strongly-convex losses (we recover similar results to the ones of ) and a class of non-convex losses (that satisfy a dissipativity condition) when a noisy perturbation with finite variance (not necessarily Gaussian, hence more general than ) is introduced. Our results shed more light on the role of the additional noise in terms of obtaining time-uniform bounds: in the non-convex case the optimizer might not be geometrically ergodic unless additional noise is introduced, hence the bound cannot be obtained. Moreover, our approach is flexible and can be generalizable to other popular optimizers, as it mainly requires developing Lyapunov functions, which are often readily available in the literature .
* We then investigate the case where no additional noise is introduced to the SGD recursion and the geometric ergodicity condition does not hold. First, for non-convex losses, we prove a time-uniform stability bound, where the bound converges to a positive number (instead of zero) as \(n\), and this limit depends on the 'level of non-convexity'. Then, we consider a class of (non-strongly) convex functions and prove stability bounds for the stationary distribution of \((_{k})_{k 0}\), which vanish as \(n\) increases. To the best of our knowledge, these results are novel, and indicate that the stability bounds do not need to increase with time even under non-convexity and without additional perturbations; yet, they might have a different nature depending on the problem class.

One limitation of our analysis is that it requires Lipschitz surrogate loss functions and does not directly handle the original loss function, due to the use of the Wasserstein distance . Yet, surrogate losses have been readily utilized in the recent stability literature (e.g., ) and we believe that our analysis might illuminate uncovered aspects of SGD even with this requirement. All the proofs are provided in the Appendix.

## 2 Technical Background

### The Wasserstein distance and Wasserstein algorithmic stability

**Wasserstein distance.** For \(p 1\), the \(p\)-Wasserstein distance between two probability measures \(\) and \(\) on \(^{d}\) is defined as :

\[_{p}(,)=\{\|X-Y\|^{p}\}^{1/p}, \]

where the infimum is taken over all couplings of \(X\) and \(Y\). In particular, the dual representation for the \(1\)-Wasserstein distance is given as :

\[_{1}(,)=_{h(1)}|_{^{d}}h (x)(dx)-_{^{d}}h(x)(dx)|, \]

where \((1)\) consists of the functions \(h:^{d}\) that are \(1\)-Lipschitz.

**Wasserstein algorithmic stability.** Algorithmic stability is a crucial concept in learning theory that has led to numerous significant theoretical breakthroughs . To begin, we will present the definition of algorithmic stability as stated in :

**Definition 2.1** (, Definition 2.1).: _Let \((^{d})\) denote the set of \(^{d}\)-valued random vectors. For a (surrogate) loss function \(:^{d}\), an algorithm \(:_{n=1}^{}^{n}( ^{d})\) is \(\)-uniformly stable if_

\[_{X}_{z}[((X), z)-((),z)], \]

_where the first supremum is taken over data \(X,^{n}\) that differ by one element, denoted by \(X\)._

In this context, we purposefully employ a distinct notation for the loss function \(\) (in contrast to \(f\)) since our theoretical framework necessitates measuring algorithmic stability through a surrogate loss function, which may differ from the original loss \(f\). More precisely, our bounds will be based on the 1-Wasserstein distance, hence, we will need the surrogate loss \(\) to be a Lipschitz continuous function, as we will detail in (2.2). On the other hand, for the original loss \(f\) we will need some form of convexity (e.g., strongly convex, convex, or dissipative) and we will need the gradient of \(f\) to be Lipschitz continuous, in order to derive Wasserstein bounds. Unfortunately, under these assumptions, we cannot further impose \(f\) itself to be Lipschitz, hence the need for surrogate losses. Nevertheless, the usage of surrogate losses is common in learning theory, see e.g, , and we present concrete practical examples in the Appendix.

Now, we present a result from  that establishes a connection between algorithmic stability and the generalization performance of a randomized algorithm. Prior to presenting the result, we define the empirical and population risks with respect to the loss function \(\) as follows:

\[(,X_{n}):= _{i=1}^{n}(,x_{i}), R():= _{x}[(,x)].\]

**Theorem 2.1** (, Theorem 2.2).: _Suppose that \(\) is an \(\)-uniformly stable algorithm, then the expected generalization error is bounded by_

\[|_{,X_{n}}\ [((X_{n}),X_{n})-R ((X_{n}))]|. \]

For a randomized algorithm, if \(\) and \(\) denotes the law of \((X)\) and \(()\) then for a \(\)-Lipschitz surrogate loss function \(\), we have the following generalization error guarantee,

\[|_{,X_{n}}\ [((X_{n}),X_{n})-R ((X_{n}))]|_{X}_{1}(,). \]

The above result can be directly obtained from the combination of the results given in (2.2), Definition 2.1, and Theorem 2.1 (see also ).

### Perturbation theory for Markov chains

Next, we recall the Wasserstein perturbation bound for Markov chains from . Let \((_{n})_{n=0}^{}\) be a Markov chain with transition kernel \(P\) and initial distribution \(p_{0}\), i.e., we have almost surely

\[(_{n} A|_{0},,_{n-1})=(_ {n} A|_{n-1})=P(_{n-1},A), \]

and \(p_{0}(A)=(_{0} A)\) for any measurable set \(A^{d}\) and \(n\). We assume that \((_{n})_{n=0}^{}\) is another Markov chain with transition kernel \(\) and initial distribution \(_{0}\). We denote by \(p_{n}\) the distribution of \(_{n}\) and by \(_{n}\) the distribution of \(_{n}\). By \(_{}\), we denote the Dirac delta distribution at \(\), i.e. the probability measure concentrated at \(\). For a measurable set \(A^{d}\), we also use the notation \(_{}P(A):=P(,A)\).

**Lemma 2.1** (, Theorem 3.1).: _Assume that there exist some \([0,1)\) and \(C(0,)\) such that_

\[_{,^{d}:}_{1}(P^{n}(,),P^{n}(,))}{\|- \|} C^{n}, \]

_for any \(n\). Further assume that there exist some \((0,1)\) and \(L(0,)\) and a measurable Lyapunov function \(:^{d}[1,)\) of \(\) such that for any \(^{d}\):_

\[()()()+L, \]_where \(()():=_{^{d}}()(,d )\). Then, we have_

\[_{1}(p_{n},_{n}) C(^{n}_{1}(p_{0}, {p}_{0})+(1-^{n})), \]

_where \(:=_{^{d}}_{1}(_{}P, _{})}{()},:=\{_{ ^{d}}()d_{0}(),\}\)._

Lemma 2.1 provides a sufficient condition for the distributions \(p_{n}\) and \(_{n}\) after \(n\) iterations to stay close to each other given the initial distributions \(p_{0}\). Lemma 2.1 will provide a key role in helping us derive the main results in Section 3.1 and Section 3.2. Later, in the Appendix, we will state and prove a modification of Lemma 2.1 (see Lemma E.5 in the Appendix) that will be crucial to obtaining the main result in Section 3.3.

## 3 Wasserstein Stability of SGD via Markov Chain Perturbations

In this section, we will derive _time-uniform_ Wasserstein stability bounds for SGD by using the perturbation theory presented in . Before considering general losses that can be non-convex, we first consider the simpler case of quadratic losses to illustrate our key ideas.

### Warm up: quadratic case

To illustrate the proof technique, we start by considering a quadratic loss of the form: \(f(,x_{i}):=(a_{i}^{}-y_{i})^{2}/2\) where, \(x_{i}:=(a_{i},y_{i})\) and \( f(,x_{i})=a_{i}(a_{i}^{}-y_{i})\). In this setting, the SGD recursion takes the following form:

\[_{k}=(I-H_{k})_{k-1}+q_{k},  H_{k}:=_{i_{k}}a_{i}a_{i}^{}, q _{k}:=_{i_{k}}a_{i}y_{i}\,. \]

The sequence \((H_{k},q_{k})\) are i.i.d. and for every \(k\), \((H_{k},q_{k})\) is independent of \(_{k-1}\).

Similarly, we can write down the iterates of SGD with a different data set \(_{n}:=\{_{1},,_{n}\}\) with \(_{i}=(_{i},_{i})\), where \(_{n}\) differs from \(X_{n}\) with at most one element:

\[_{k}=(I-_{k})_{k-1}+ _{k},_{k}:=_{i _{k}}_{i}_{i}^{},_{k}:=_{i_{k}} {a}_{i}_{i}\,. \]

Our goal is to obtain an algorithmic stability bound, through estimating the \(1\)-Wasserstein distance between the distribution of \(_{k}\) and \(_{k}\) and we will now illustrate the three-step proof technique that we described in Section 1. To be able to apply the perturbation theory , we start by establishing the geometric ergodicity of the Markov process \((_{k})_{k 0}\) with transition kernel \(P(,)\), given in the following lemma.

**Lemma 3.1**.: _Assume that \(:=\|I-H_{1}\|<1\). Then, for any \(k\), we have the following inequality: \(_{1}(P^{k}(,),P^{k}(, ))^{k}\|-\|\)._

We note that since \(H_{1} 0\), the assumption in Lemma 3.1 can be satisfied under mild assumptions, for example when \(H_{1} 0\) with a positive probability, which is satisfied for \(\) small enough.

In the second step, we construct a Lyapunov function \(\) that satisfies the conditions of Lemma 2.1.

**Lemma 3.2**.: _Let \(():=1+\|\|\). Assume that \(:=\|I-_{1}\|<1\). Then, we have_

\[()()()+1-+\|_{1}\|. \]

In our third and last step, we estimate the perturbation gap based on the Lyapunov function \(\) in the form of (2.7), assuming that the data is bounded. Such bounded data assumptions have been commonly made in the literature .

**Lemma 3.3**.: _If \(_{x}\|x\| D\) for some \(D<\), then, we have \(_{^{d}}_{1}(_{}P,_{ })}{()}}{n}\)._Note that Lemma 2.1 relies on three conditions: the Wasserstein contraction in (2.7), which is obtained through Lemma 3.1, the drift condition for the Lyapunov function in (2.8), which is obtained in Lemma 3.2 and finally the estimate on \(\) in (2.9) which is about the one-step \(1\)-Wasserstein distance between two semi-groups that in our context are associated with two datasets that differ by at most one element, which is obtained in Lemma 3.3. The only place the neighborhood assumption (\(_{x}\|x\| D\)) is used is in the expression of \(\) in equation (2.9). Now, having all the ingredients, we can invoke Lemma 2.1 and we obtain the following result which provides a \(1\)-Wasserstein bound between the distribution of iterates when applied to datasets that differ by one point.

For \(Y_{n=1}^{}^{n}\) and \(k 0\), let \((Y,k)\) denote the law of the \(k\)-th the SGD iterate when \(Y\) is used as the dataset, i.e., \((X,k)\) and \((,k)\) denote the distributions of \(_{k}\) and \(_{k}\) obtained by the recursions (3.1) and (3.2) respectively. As shorthand notation, set \(_{k}:=(X,k)\) and \(_{k}:=(,k)\).

**Theorem 3.1**.: _Assume \(_{0}=_{0}=\). We also assume that \(:=\|I-H_{1}\|<1\) and \(:=\|I-_{1}\|<1\) and \(_{x}\|x\| D\) for some \(D<\). Then, we have_

\[_{1}(_{k},_{k})}{1-}}{n}\{1+\|\|,+\| _{1}\|}{1-}\}. \]

Proof.: The result directly follows from Lemma 3.1, Lemma 3.2, Lemma 3.3 and Lemma 2.1. 

By a direct application of (2.5), we can obtain a generalization bound for an \(\)-Lipschitz surrogate loss function, as follows:

\[|_{,X_{n}}\ [((X_{n}),X_{n})- R((X_{n}))]|}{1-_{0}}}{n}\{1+\|\|,+ \|_{1}\|}{1-_{0}}\},\]

where \(_{0}=_{X}\|1-H_{X}\|\), \(H_{X}=_{i_{k},a_{j} X}a_{j}a_{j}^{}\) and \(X\) is a random set of \(n\)-data points from the data generating distribution. The generalization bound obtained above does not include the mean square error in the unbounded case but covers a larger class of surrogate loss functions. Because of this incompatibility, a direct comparison is not possible; however, the rate obtained in the equation above has the same dependence on the number of samples that were obtained in the previous works . For least squares, there are other works using integral operators that develop generalization bounds for SGD under a capacity condition . However, these bounds only hold for the least square loss.

### Strongly convex case

Next, we consider strongly convex losses. In the remainder of the paper, we will always assume that for every \(x\), \(f(,x)\) is differentiable.

Before proceeding to the stability bound, we first introduce the following assumptions.

**Assumption 3.1**.: _There exist constants \(K_{1},K_{2}>0\) such that for any \(,^{d}\) and every \(x\),_

\[\| f(,x)- f(,)\| K_{1}\|-\|+K_{2}\|x-\|(\|\|+\|\|+1). \]

This assumption is a pseudo-Lipschitz-like condition on \( f\) and is satisfied for various problems such as GLMs . Next, we assume that the loss function \(f\) is strongly convex.

**Assumption 3.2**.: _There exists a universal constant \(>0\) such that for any \(_{1},_{2}^{d}\) and \(x\),_

\[ f(_{1},x)- f(_{2},x),_{1}-_{2} \|_{1}-_{2}\|^{2}.\]

By using the same recipe as we used for quadratic losses, we obtain the following stability result.

**Theorem 3.2**.: _Let \(_{0}=_{0}=\). Assume that Assumption 3.1 and Assumption 3.2 hold. We also assume that \(<\{,^{2}+64D^{2}K_{2}^{2}}\}\), \(_{x}\|x\| D\) for some \(D<\), and \(_{x}\| f(0,x)\| E\) for some \(E<\). Let \(_{k}\) and \(_{k}\) denote the distributions of \(_{k}\) and \(_{k}\) respectively. Then, we have_

\[_{1}(_{k},_{k})\] \[\{1+2\|\|^{2}+}{^{2}},2-K_{1}^{2}-D^{2}K_{2}^{2}+}D^{ 2}K_{2}^{2}E^{2}\}. \]

Similarly to the quadratic case, we can now directly obtain a bound on expected generalization error using (2.5). More precisely, for an \(\)-Lipschitz surrogate loss function \(\), we have

\[|_{,X_{n}}[((X_{n }),X_{n})- R((X_{n}))]|( 1-(1-)^{k})}{n}(+1)\] \[\{1+2\|\|^{2}+}{^{2}},2-K_{1}^{2}-D^{2}K_{2}^{2}+}D^{ 2}K_{2}^{2}E^{2}\}.\]

The bound above has the same dependence on the number of samples as the ones of the previous stability analysis of (projected) SGD for strongly convex functions . However, we have a worse dependence on the strong convexity parameter \(\).

### Non-convex case with additive noise

Finally, we consider a class of non-convex loss functions. We assume that the loss function satisfies the following dissipativity condition.

**Assumption 3.3**.: _There exist constants \(m>0\) and \(K>0\) such that for any \(_{1},_{2}^{d}\) and \(x\),_

\[ f(_{1},x)- f(_{2},x),_{1}-_{2}  m\|_{1}-_{2}\|^{2}-K.\]

The class of dissipative functions satisfying this assumption are the ones that admit some gradient growth in radial directions outside a compact set. Inside the compact set though, they can have quite general non-convexity patterns. As concrete examples, they include certain one-hidden-layer neural networks ; they arise in non-convex formulations of classification problems (e.g. in logistic regression with a sigmoid/non-convex link function); they can also arise in robust regression problems, see e.g. . Also, any function that is strongly convex outside of a ball of radius for some will satisfy this assumption. Consequently, regularized regression problems where the loss is a strongly convex quadratic plus a smooth penalty that grows slower than a quadratic will belong to this class; a concrete example would be smoothed Lasso regression; many other examples are also given in . Dissipative functions also arise frequently in the sampling and Bayesian learning and global convergence in non-convex optimization literature .

Unlike the strongly-convex case, we can no longer obtain a Wasserstein contraction bound using the synchronous coupling technique as we did in the proof of Theorem 3.2. To circumvent this problem, in this setting, we consider a noisy version of SGD, with the following recursion:

\[_{k}=_{k-1}-_{k}(_{k-1},X_{n})+ _{k},_{k}(_{k-1},X_{n}):=_ {i_{k}} f(_{k-1},x_{i}), \]

where \(_{k}\) are additional i.i.d. random vectors in \(^{d}\), independent of \(_{k-1}\) and \(_{k}\), satisfying the following assumption.

**Assumption 3.4**.: \(_{1}\) _is random vector on \(^{d}\) with a continuous density \(p(x)\) that is positive everywhere, i.e. \(p(x)>0\) for any \(x^{d}\) and \([_{1}]=0,\ \ ^{2}:=[\|_{1}\|^{2}]<\)._

Note that the SGLD algorithm  is a special case of this recursion, whilst our noise model can accommodate non-Gaussian distributions with finite second-order moment.

Analogously, let us define the (noisy) SGD recursion with the data set \(_{n}\) as

\[_{k}=_{k-1}-_{k}( _{k-1},_{n})+_{k},\]

and let \(p(,_{1})\) denote the probability density function of \(_{1}=-_{i_{1}} f(,x_{i})+ _{1}\). Further let \(_{*}\) be a minimizer of \((,X_{n})\). Then, by following the same three-step recipe, we obtain the following stability bound. Here, we do not provide all the constants explicitly for the sake of clarity; the complete theorem statement is given in Theorem E.1 (Appendix E.1).

**Theorem 3.3**.: _Let \(_{0}=_{0}=\). Assume that Assumption 3.1, Assumption 3.3 and Assumption 3.4 hold. We also assume that \(<\{,^{2}+64D^{2}K_{2}^{2}}\}\) and \(_{x}\|x\| D\) for some \(D<\) and \(_{x}\| f(0,x)\| E\) for some \(E<\). For any \((0,1)\), define \(M>0\) so that \(_{\|_{1}-_{*}\| M}p(_{*},_{1})d_{1} }\) and for any \(R>}{m}\) where \(K_{0}\) is defined in (E.2) so that_

\[_{,_{1}^{d}:V() R,\|_{1}-_{ *}\| M})}{p(_{*},_{1})}}. \]

_Let \(_{k}\) and \(_{k}\) denote the distributions of \(_{k}\) and \(_{k}\) respectively. Then, we have_

\[_{1}(_{k},_{k})(1-^{k})}{2 )}(1-)}, \]

_where for any \(_{0}(0,)\) and \(_{0}(1-m+}{R},1)\), \(=}{ K_{0}}\) and \(=(1-(-_{0}))}{2+R}\). The constant \(C_{1} C_{1}(,,,R,_{0},K_{1},K_{2},^{2},D,E)\) is explicitly stated in the proof._

Contrary to our previous results, the proof technique for showing Wasserstein contraction (as in Lemma 3.1) for this theorem relies on verifying the drift condition (Assumption B.1) and the minorization condition (Assumption B.2) as given in . Once these conditions are satisfied, we invoke the explicitly computable bounds on the convergence of Markov chains developed in .

From equation (2.5), we directly obtain the following generalization error bound for \(\)-Lipschitz surrogate loss function,

\[|_{,X_{n}}\ [((X_{n}),X_{n})- R((X_{n}))]|(1-^{k})}{2)}(1-)},\]

where the constants are defined in Theorem 3.31. The above result can be directly compared with the result in [13, Theorem 4.1] that has the same dependence on \(n\) and \(b\). However, our result is more general in the sense that we do not assume our noise to be Gaussian noise. Note that  can also accommodate non-Gaussian noise; however, their bounds increase with the number of iterations.

**Remark 3.4**.: _In Theorem 3.3, we can take \(R=}{m}(1+)\) for some fixed \((0,1)\) so we can take_

\[=(_{M>0}\{\{_{\|_{1}-_{*}\|  M}p(_{*},_{1})d_{1},_{, _{1}^{d}:\|_{1}-_{*}\| M\\ \|-_{*}\|^{2}}{m}(1+)-1} )}{p(_{*},_{1})}\}\})^{ 2}. \]

_Moreover, we can take \(_{0}=}{2}\), \(_{0}=1-\), and \(=}{2 K_{0}}\), so that_

\[=\{1-}{2},}{m}(1-)}{2+}{m}} \}=1-}{4m+2(1+)}, \]

_provided that \( 1\). Note that the parameter \(\) in (3.10) appears in the upper bound in equation (3.9) that controls the \(1\)-Wasserstein algorithmic stability of the SGD. It is easy to see from equation (3.9) that the smaller \(\), the smaller the \(1\)-Wasserstein bound. By the defintion of \(\), the larger \(\), the smaller the \(1\)-Wasserstein bound. As a result, we would like to choose \(\) to be as large as possible, and the equation (3.10) provides an explicit value that \(\) can take, which is already the largest as possible._

Next, let us provide some explicitly computable lower bounds for \(\) in (3.10). This is achievable if we specify further the noise assumption. Under the assumption that \(_{k}\) are i.i.d. Gaussian distributed, we have the following corollary.

**Corollary 3.5**.: _Under the assumptions in Theorem 3.3, we further assume the noise \(_{k}\) are i.i.d. Gaussian \((0,)\) so that \([\|_{1}\|^{2}]=()=^{2}\). We also assume that \( I_{d}\). Then, we have_

\[_{M_{x}\| f( _{*},x)\|}1-(-_{x}\| f(_{*},x)\|)^{2})}{-)}}^{2},\] \[(1+K_{1})(}{m}(1+ )-1)^{1/2}+2(M+_{x}\| f( _{*},x)\|)}}}. \]

The above corollary provides an explicit lower bound for \(\) (instead of the less transparent inequality constraints in Theorem 3.3), and by combining with Remark 3.4 (see equation (3.11)) leads to an explicit formula for \(\) which is essential to characterize the Wasserstein upper bound in (3.9) in Theorem 3.3.

## 4 Wasserstein Stability of SGD without Geometric Ergodicity

While the Markov chain perturbation theory enabled us to develop stability bounds for the case where we can ensure geometric ergodicity in the Wasserstein sense (i.e., proving contraction bounds), we have observed that such a strong ergodicity notion might not hold for non-strongly convex losses. In this section, we will prove two more stability bounds for SGD, without relying on , hence without requiring geometric ergodicity. To the best of our knowledge, these are the first uniform-time stability bounds for the considered classes of convex and non-convex problems.

### Non-convex case without additive noise

The stability result we obtained in Theorem 3.3 required us to introduce an additional noise (Assumption 3.4) to be able to invoke Lemma 2.1. We will now show that it is possible to use a more direct approach to obtain 2-Wasserstein algorithmic stability in the non-convex case under Assumption 3.3 without relying on . However, we will observe that without geometric ergodicity will have a non-vanishing bias term in the bound. Note that, since \(_{1}(_{k},_{k})_{p}(_{k},_{k})\) for all \(p 1\), the following bound still yields a generalization bound by (2.5).

**Theorem 4.1**.: _Assume \(_{0}=_{0}=\). We also assume that Assumption 3.1 and Assumption 3.3 hold and \(<\{,^{2}+64D^{2}K_{2}^{2}}\}\) and \(_{x}\|x\| D\) for some \(D<\) and \(_{x}\| f(0,x)\| E\) for some \(E<\). Let \(_{k}\) and \(_{k}\) denote the distributions of \(_{k}\) and \(_{k}\) respectively. Then, we have_

\[_{2}^{2}(_{k},_{k})(1-(1- m)^{k}) (K_{2}^{2}(8B+2)}{bnm}+D(1+K_{1})}{ nm}(1+5B)+), \]

_where the constant \(B\) is explicitly defined in the proof._

While the bound (4.1) does not increase with the number of iterations, it is easy to see that it does not vanish as \(n\), and it is small only when \(K\) from the dissipativity condition (Assumption 3.3) is small. In other words, if we consider \(K\) to be the level of non-convexity (e.g., \(K=0\) corresponds to strong convexity), as the function becomes'more non-convex' the persistent term in the bound will get larger. While this persistent term might make the bound vacuous when \(n\), for moderate \(n\) the bound can be still informative as the persistent term might be dominated by the first two terms.

Moreover, discarding the persistent bias term, this bound leads to a generalization bound with rate \(n^{-1/2}\), rather than \(n^{-1}\) as before. This indicates that it is beneficial to add additional noise \(_{k}\) in SGD as in Theorem 3.3 in order for the dynamics to be geometrically ergodic that can lead to a sharp bound as \(n\). Finally, we note that as Theorem 4.1 involves 2-Wasserstein distance, it can pave the way for generalization bounds without requiring a surrogate loss. Yet, this is not immediate and would require deriving uniform \(L^{2}\) bounds for the iterates, e.g., .

### Convex case with additional geometric structure

We now present our final stability bound, where we consider relaxing the strong convexity assumption (Assumption 3.2) to the following milder assumption.

**Assumption 4.1**.: _There exists universal constants \(>0\) and \(p(1,2)\) such that for any \(_{1},_{2}^{d}\) and \(x\), \( f(_{1},x)- f(_{2},x),_{1}-_{2} \|_{1}-_{2}\|^{p}\)._

Note that as \(p<2\), the function class can be seen as an intermediate class between convex and strongly convex functions, and such a class of functions has been studied in the optimization literature .

We analogously modify Assumption 3.1 and consider the following assumption.

**Assumption 4.2**.: _There exist constants \(K_{1},K_{2}>0\) and \(p(1,2)\) such that for any \(,^{d}\) and every \(x\), \(\| f(,x)- f(,)\| K_{1}\|-\|^{}+K_{2}\|x-\|(\|\|^{p-1}+\|\| ^{p-1}+1)\)._

The next theorem establishes a stability bound for the considered class of convex losses in the stationary regime of SGD.

**Theorem 4.2**.: _Let \(_{0}=_{0}=\). Suppose Assumption 4.1 and Assumption 4.2 hold (with \(p(1,2)\)) and \(^{2}+2^{p+4}D^{2}K_{2}^{2}}\) and \(_{x}\|x\| D\) for some \(D<\) and \(_{x}\| f(0,x)\| E\) for some \(E<\). Then \(_{k}\) and \(_{k}\) converge to the unique stationary distributions \(_{}\) and \(_{}\) respectively and moreover, we have_

\[_{p}^{p}(_{},_{})}{bn}+ {C_{3}}{n}, \]

_where the constants \(C_{2} C_{2}(,,K_{2},D,E)\) and \(C_{3} C_{3}(,,K_{1},K_{2},D,E)\) are explicitly stated in the proof._

While we have relaxed the geometric ergodicity condition for this case, in the proof of Theorem 4.2, we show that the Markov chain \((_{k})_{k 0}\) is simply ergodic, i.e., \(_{k}_{p}(_{k},_{})=0\). Hence, even though we still obtain a time-uniform bound, our bound holds asymptotically in \(k\), due to the lack of an explicit convergence rate for \(_{p}(_{k},_{})\). On the other hand, the lack of strong convexity here results in a generalization bound with rate \(n^{-1/p}\), whereas for the strongly convex case, i.e., \(p=2\), we previously obtained a rate of \(n^{-1}\). This might be an indicator that there might be still room for improvement in terms of the rate, at least for this class of loss functions.

## 5 Conclusion

We proved time-uniform Wasserstein-stability bounds for SGD and its noisy versions under different strongly convex, convex, and non-convex classes of functions. By making a connection to Markov chain perturbation results , we introduced a three-step guideline for proving stability bounds for stochastic optimizers. As this approach required geometric ergodicity, we finally relaxed this condition and proved two other stability bounds for a large class of loss functions.

The main limitation of our approach is that it requires Lipschitz surrogate loss functions, as it is based on the Wasserstein distance. Hence, our natural next step will be to extend our analysis without such a requirement. Finally, due to the theoretical nature of this study, it does not contain any direct potential societal impacts.