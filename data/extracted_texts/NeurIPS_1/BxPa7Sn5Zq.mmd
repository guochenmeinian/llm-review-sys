# Learning Interaction-aware 3D Gaussian Splatting for One-shot Hand Avatars

Xuan Huang\({}^{1}\), Hanhui Li\({}^{1}\), Wanquan Liu\({}^{1}\), Xiaodan Liang\({}^{1}\),

Yiqiang Yan\({}^{2}\), Yuhao Cheng\({}^{2}\), Chengqiang Gao\({}^{1}\)

\({}^{1}\)Shenzhen Campus of Sun Yat-Sen University

\({}^{2}\)Lenovo Research

huangx355@mail2.sysu.edu.cn lihh77@mail.sysu.edu.cn liuwq63@mail.sysu.edu.cn xdliang328@gmail.com yanyq@lenovo.com chengyh5@lenovo.com gaochq6@mail.sysu.edu.cn

Both authors contributed equally.Corresponding author.

###### Abstract

In this paper, we propose to create animatable avatars for interacting hands with 3D Gaussian Splatting (GS) and single-image inputs. Existing GS-based methods designed for single subjects often yield unsatisfactory results due to limited input views, various hand poses, and occlusions. To address these challenges, we introduce a novel two-stage interaction-aware GS framework that exploits cross-subject hand priors and refines 3D Gaussians in interacting areas. Particularly, to handle hand variations, we disentangle the 3D presentation of hands into optimization-based identity maps and learning-based latent geometric features and neural texture maps. Learning-based features are captured by trained networks to provide reliable priors for poses, shapes, and textures, while optimization-based identity maps enable efficient one-shot fitting of out-of-distribution hands. Furthermore, we devise an interaction-aware attention module and a self-adaptive Gaussian refinement module. These modules enhance image rendering quality in areas with intra- and inter-hand interactions, overcoming the limitations of existing GS-based methods. Our proposed method is validated via extensive experiments on the large-scale InterHand2.6M dataset, and it significantly improves the state-of-the-art performance in image quality. Project Page: [https://github.com/XuanHuang0/GuassianHand](https://github.com/XuanHuang0/GuassianHand).

## 1 Introduction

Recent advancements in 3D reconstruction and differential rendering techniques have significantly improved hand avatar creation and related applications. However, creating avatars for interacting hands from a single image remains challenging. The limited input view does not provide sufficient geometry and texture information for accurate reconstruction. Moreover, intra- and inter-hand interactions exacerbate information loss and introduce complex geometric deformations.

Extensive efforts have been made to tackle these issues, as shown in Figure 2: (a) Early approaches depend on explicit parametric meshes (e.g. MANO ) for geometry modeling, and utilize UV map , vertex color , or image space rendering  for appearance. Despite the efficiency in rendering, these methods fail to achieve realistic rendering results with the coarse mesh resolution and the simple combination of hand appearance and geometry. (b) More recently,with the significant success of neural radiance fields (NeRF), extensive studies [9; 10; 11; 12; 13] have employed NeRF-based models for implicit modeling. These methods [10; 11; 12] usually require per-scene optimization for each new identity using densely calibrated images, which results in expensive training costs. Generalizable NeRFs [14; 15; 13; 16] get rid of per-scene training by leveraging image-aligned features to enable reconstruction from a few or even a single view. Yet their dependence on image-aligned features also limits their performance under large view or pose variations. Besides, (c) One-shot NeRF-based methods [17; 18] propose to exploit data-driven priors with condition optimizations  and inversions . Nevertheless, these methods are not suitable for our task, as they do not include any module to detect and handle interactions. Moreover, the inversion of identity vectors used in  omit the spatial image structure, which not only hinders its performance but also introduces extra time consumption for fine-tuning networks.

To tackle the above issues in existing methods, we aim to create animatable avatars for interacting hands with 3D Gaussian Splatting (GS) and single-image inputs. To this end, we introduce a novel two-stage interaction-aware GS framework as shown in Figure 2 (d). We disentangle the 3D presentation of hands into (i) features that can be effectively captured by training networks in the first stage of our framework (e.g., geometric features and latent neural texture maps), and (ii) identity maps that can be optimized efficiently in the per-subject one-shot fitting stage. In this way, our method not only enables leveraging cross-subject priors with learning-based features, but also well preserves per-subject characteristics via optimizing identity maps.

Additionally, to achieve robust reconstruction and enhance rendering quality, we devise an interaction-aware attention module and a self-adaptive Gaussian refinement module. The former module identifies Gaussian points with potential intra- and inter-hand interactions to enhance their features

Figure 1: We present a novel interaction-aware Gaussian splatting framework that creates animatable interacting hand avatars from a single image. These high-fidelity avatars support various applications, such as editing, animation, combination, duplication, re-scaling, and text-to-avatar conversion.

Figure 2: Paradigm comparison between existing one-shot hand avatar methods (a-c) and the proposed method (d). By decoupling the learning and fitting stages, our method leverages the advantages of learning-based methods (a, b) in modeling cross-subject hand priors, and the advantages of inversion-based methods (c) in one-shot fitting without the extra cost of network fine-tuning.

with attention mechanisms. This enhancement allows our GS network to better model geometric deformations and fine-grained textures caused by interactions (e.g., wrinkles). The latter module is introduced to address the limitations of the coarse geometry of parametric hand meshes, which is achieved by learning to eliminate redundant Gaussians and assigning extra Gaussians in regions with complex textures and deformations. Consequently, our method can reconstruct realistic inter-hand avatars with great flexibility for animation and editing, as shown in Figure 1.

Overall, our contributions can be summarized as follows:

\(\) We propose a novel two-stage interaction-aware GS framework to create animatable avatars for interacting hands from single-image inputs. Our method generates high-fidelity rendering results and supports various applications. Experimental results on the large-scale Interhand2.6M dataset  validate the superior performance of our method compared to previous methods.

\(\) We disentangle the 3D presentation of hands into learning-based features that can be generalized well to different subjects and identity maps that are individually optimized for each subject. This disentanglement provides us with flexible and reliable priors for poses, shapes, and textures.

\(\) We introduce an interaction-aware attention module, which identifies intra- and inter-hand interactions and further exploits interaction context to improve rendering quality.

\(\) We devise a Gaussian refinement module that adaptively adjusts the number and positions of 3D Gaussian, which results in rendered images of higher quality under various hand poses and shapes.

## 2 Related Work

**One-shot Human Reconstruction**. One-shot reconstruction of 3D humans is a challenging and long-standing problem due to the limited information from a single input image. To alleviate this limitation, previous works leveraged parametric models [20; 1] as coarse geometry prior. Traditional methods [21; 22; 23; 24; 25] utilized UV maps for human appearance representation. To complete the unseen texture from the single image, some approaches [25; 24] inpainted missing textures via pre-trained diffusion models. Neural Radiance Fields (NeRF)  have also been explored for reconstruction from sparse views [27; 15; 14; 28] or a single view [29; 16; 13; 18]. KeypointNeRF  encoded spatial information using 3D skeleton keypoints. SHERF  created 3D human avatars from a single image with hierarchical features for informative encoding. VANeRF  leveraged visibility in both feature fusion and adversarial learning for single-view interacting-hand image synthesis. These methods greatly enhance the NeRF one-shot reconstruction performance. Nonetheless, generalizable NeRF [27; 30; 31; 14; 15] fails to achieve satisfactory results when the input image information is not sufficient for novel view prediction due to the under-utilization of hand priors. To overcome this issue, the pioneering research in  enabled one-shot single-hand avatar creation by learning data-driven hand priors which are further utilized with inversion and fitting. Compared with previous methods, our approach further disentangles the presentation of hand priors into latent geometric features, neural texture maps, and optimizable identity maps to enhance rendering quality and reduce the cost of one-shot fitting. Moreover, we introduce the interaction-aware module and self-adaptive refinement module, which helps to significantly improve the visual quality of synthesized images.

**Animatable Hand Avatar**. Conventional methods created hand avatars by incorporating UV textures with explicit parametric hand models. HTML  is the first parametric texture model of human hands which models hand appearance with several dimensions of variability. HARP  further introduced albedo and normal information into UV maps to represent hand appearance without any neural components. Handy  realistically captured high-frequency detailed texture using a GAN-based texture model. However, the rendering quality of these methods is constrained by the coarse geometry and sparsity of the MANO mesh. The recent advancements in the neural radiance field have resulted in the development of approaches that utilize implicit representation for hand reconstruction. LISA  is the first method that employs NeRF to learn the implicit shape and appearance of hands. HandAvatar  developed a high-resolution variant of MANO to fit personalized hand shapes and further disentangled the implicit representations for hands into geometry, albedo, and illumination. HandNeRF  designed a pose-driven deformation field with pose-disentangled NeRF to reconstruct single or interacting hands from multi-view images. LiveHand  proposed a low-resolution rendering of NeRF together with a super-resolution module to achieve real-time performance.

**Point-based 3D Representation and Rendering.** There has been a growing interest in point-based neural rendering due to recent advancements in 3D Gaussian splatting (3DGS) , which has led to high-quality and real-time rendering speed. Since 3DGS is primarily designed for static scenes, many efforts [33; 34; 35; 36] are dedicated to expanding its applicability to free pose animation and rendering. 3D-PSHR  achieved real-time and photo-realistic hand reconstruction from large-scale multi-view videos based on 3D points splatting. Our method differs from 3D-PSHR as it allows instant single-view one-shot hand avatar reconstruction and saves the expensive computational cost of per-scene optimization.

## 3 Methodology

In this section, we introduce the details of the proposed two-stage framework for creating interacting hand avatars from a single image. The key ideas of our framework contain three aspects: (i) We address the lack of information caused by limited inputs by learning disentangled priors for hand poses, shapes, and textures (Sec. 3.1). (ii) We construct an interaction-aware Gaussian splatting network to handle both intra- and inter-hand interactions (Sec. 3.2). (iii) Leveraging invertible identity and neural texture maps, we reduce the time consumption of one-shot avatar reconstruction while simultaneously improving the quality of synthesized images (Sec. 3.3).

### Disentangled 3D Hand Representation

To demonstrate the motivations of the proposed disentangled 3D hand representation, we first provide the formulation of our task.

**Task formulation**. Given a reference image of interacting hands \(I_{r}\), our task is to reconstruct an animatable two-hand avatar that can generate images of the hands with novel poses and from novel views. To achieve this, we propose to construct a differentiable renderer \(:^{|||c|}^{H W  3}\), where \(\) and \(c\) denote the hand pose parameters and camera parameters, respectively. \(H\) and \(W\) denote the height and width of rendered three-channel RGB images.

More specifically, we propose to implement \(\) via Gaussian splatting (GS)  for its advantages in explicit modeling and computational efficiency. Essentially, GS is a point-based rasterization technique that leverages a set of 3D points (Gaussians) with attributes like colors, opacity, and spherical harmonic coefficients to represent reference images. Let \(^{N 3}\) denote the set of \(N\) Gaussians and \(^{N D}\) denote their D-dimensional attributes. To preserve the characteristic of the hands in \(I_{r}\), we need to optimize \(\) via minimizing the following \(l_{1}\) loss,

\[*{arg\,min}_{,,,c}((,c,-I_{r}_{1}). \]

Due to the high-dimensional attribute space and inter-attribute interference, direct optimization for Eq. (1) is intractable. Although learning-based methods can alleviate this issue to a certain extent, their

Figure 3: The architecture of the proposed interaction-aware Gaussian splatting network, of which the core components are the disentangled hand representation, the interaction detection module, the interaction-aware attention module, and the Gaussian refinement module labeled in green.

results may fall short if \(I_{r}\) lies outside of the distribution (OOD) of their training data. Therefore, a disentangled representation that integrates optimization-based and learning-based Gaussian attributes is essential for addressing our task effectively. Furthermore, a reliable representation should capture both the geometric and textural properties of hands. Given the availability of extensive hand mesh reconstruction methods for geometric information, we propose leveraging learning-based geometric properties alongside optimizable textures in our disentangled representation. This approach allows us to handle the diversity and potential OOD issues of hand textures.

To this end, we devise the disentangled representation shown in Figure 3 to combine explicit geometric embeddings from hand meshes with neural texture maps encoding implicit latent fields. The encoders for this representation are learned on a training dataset consisting of images of \(S\) subjects. Specifically, we first construct the optimizable cross-subject identity maps \(^{S 2C H W}\), where \(C\) denotes the number of feature channels of one hand. For each training image \(I_{t}\), we reconstruct a parameterized hand mesh \(\) from it and use the vertices of \(\) to initialize \(\). Let \(s[1,..,S]\) denote the subject ID of \(I_{t}\), we retrieve its corresponding identity map \(_{s}\) and combine it with the pose embedding from \(\) to infer its neural texture map \(_{s}^{2C H W}\). Consequently, \( p\), we can query their feature vectors from \(_{s}\) based on their texture coordinates and predict their Gaussian attributes. In the one-shot fitting stage, to obtain the neural texture map of \(I_{r}\), we only need to optimize a new identity map initialized with zeros. This is applicable, as our identity maps and natural texture maps share an important advantage: they both preserve the spatial structure of textures, which overcomes the burdens of previous vector-based inversion methods .

Below we introduce the core components for implementing our disentangled representation. These components will be integrated into the GS network in the next section for end-to-end learning.

**Parameterized Hand Mesh**. We employ MANO  to reconstruct hand meshes from images for its convenience in animation. MANO is a parametric model that represents a hand mesh by pose parameters \(^{48}\) and shape parameters \(^{10}\). To further improve the mesh quality, we use a high-resolution version of MANO . We follow  to obtain normalized UV coordinates \((u,v)\) of mesh vertices and project them onto the neural texture plane.

**Geometric Encoding**. To exploit explicit geometric features from hand meshes, we utilize a pose encoder and a positional encoder. The pose encoder is an MLP taking the pose parameters \(\) and the camera parameters \(c^{25}\) as inputs (which is the flattened concatenation of an extrinsic matrix \(^{4 4}\) and an intrinsic matrix \(^{3 3}\)). Our pose embeddings do not involve \(\) and hence they are independent of identities. The positional encoder is a shallow PointNet  with local pooling . We further employ a transformer-based decoder to merge the outputs of the pose encoder and the positional encoder, similar to .

**Texture Encoding**. Given the UV coordinates of a vertex, we retrieve its feature vector on the optimizable identity map, along with the \(\) positional encoding  of its coordinates to generate its identity embedding. The identity embeddings of all vertices are concatenated with the pose embedding and projected (scattered) back to the UV plane to form a texture condition map. We again adopt a transformer-based decoder to process the condition map and yield the neural texture map.

Finally, we combine the geometric feature vectors of all vertices with their texture feature vectors using element-wise addition. This results in a unified latent representation \(^{|| C}\), which we use for predicting Gaussian attributes.

### Interaction-Aware Gaussian Splatting Network

To better reconstruct interacting hand avatars with various poses, we propose to enhance the Gaussian features \(\) via an interaction-aware attention (IAttn) module and a Gaussian point refinement module (GRM). IAttn identifies potential points with intra- or inter-hand interaction. By exploring the context around interaction points, IAttn improves the reconstruction quality of geometric deformations and texture details resulting from interactions (such as shading, wrinkles, and veins). Furthermore, GRM not only eliminates redundant Gaussians but also generates additional Gaussians near regions with complex textures. With these two modules, our network can render high-quality hand images with rich details.

#### 3.2.1 Interaction-aware Attention

To detect interacting points in \(\), we propose a straightforward yet effective strategy that calculates the difference between the neighboring point sets of posed hand meshes and a canonical mesh. This strategy is practical as we can define an interaction-free mesh as the canonical one. For an arbitrary query point \(q\), if its top-\(N_{c}\) nearest neighboring points on the canonical mesh \(_{c}(q)\) are significantly overlapped with its top-\(N_{p}\) nearest neighbors on the posed mesh (denoted as \(_{p}\)), the chance that \(q\) is an interacting point is low. This strategy can be formulated as follows,

\[d(q)=1,&|_{c}(q)_{p}(q)-_{c}(q) _{p}(q)|>T,\\ 0,&, \]

where \(T\) is a user-defined threshold. Additionally, we append the interacting label \(d(q)\) to the pose embedding introduced in the last section. Note that our proposed strategy can detect both self-interacting and cross-interacting points. To maintain the efficiency of our method, we only conduct self-attention on detected interacting points.

#### 3.2.2 Self-adaptive Refinement for 3D Gaussians

Gaussians initialized from MANO can only provide coarse hand geometry with restricted deformations. To better model the geometry of hands with various poses and shapes, we devise the self-adaptive GRM to control the density of Gaussians and refine their locations. Given a Gaussian point \(p\) and its corresponding feature vector \(_{p}\), we utilize an MLP \((_{p})\) with the sigmoid activation to predict the validity of \(p\), i.e., \(:^{C}\). We remove \(p\) from \(\) if \((_{p})\) is below a pre-defined threshold \(T_{d}\) while splitting \(p\) if \((_{p})\) is larger than another threshold \(T_{s}\). We also exploit \(\) to predict the offsets of Gaussians to adjust their positions.

#### 3.2.3 Network Optimization

With the search space of Gaussian attributes reduced significantly by the proposed disentangled representation, we are now ready to optimize our GS network and learn cross-subject hand priors. We train the GS network along with the optimizable cross-subject identity maps \(\) by minimizing the following loss:

\[I=(,c,,|),_{rec}= _{rgb}\|I-I_{t}\|_{1}+_{VGG}_{VGG}(I, I_{t}), \]

where \(\) are the learnable parameters in our GS network. \(_{VGG}\) denote the perceptual loss . \(_{rgb}\) and \(_{VGG}\) are user-defined weights.

### One-shot Hand Avatar Reconstruction

In the stage of one-shot hand avatar reconstruction, the parameters of IGSN are fixed and we fine-tune the identity map of the new subject \(^{*}^{2C H W}\). This can be formulated as,

\[^{*}}{}_{inv.}=_{rec} ((_{r},c_{r},^{*},_{r}|),I_{r })+_{mask}\|M-M_{r}\|_{2}^{2}, \]

where the geometric parameters \(_{r}\), \(c_{r}\), and \(_{r}\) can be obtained via off-the-shelf MANO regressors . \(M\) and \(M_{r}\) denote the hand mask of \(I\) and \(I_{r}\), respectively. \(_{mask}\) is the empirical weight of the loss term on masks.

 suggests that optimization tricks like color calibration and view regularization can further improve the synthesized images. Inspired by this, we also introduce a texture map bias \(^{C H W}\) to modulate the latent neural feature maps \(\) of two hands (\(=\{_{l},_{r}\}\)). That is, \(_{l}:=_{l}+\) and \(_{r}:=_{r}+\). We assume that \(\) can be shared by \(_{l}\) and \(_{r}\) due to the symmetry of the left and right hands. We include a regularization term on \(\) into Eq. (4) to prevent drastic shifts of \(\) as follows:

\[^{*},}{}(_{inv.}+ _{reg}\|\|_{2}^{2}), \]

where the regularization weight \(_{reg}\) is user-defined. In our experiments, we find that adding \(\) accelerates the fitting process and helps to prevent undue changes.

## 4 Experiments

### Setup

**Learning IGSN.** Our experiments are conducted on the publicly available Interhand2.6M dataset  (CC-BY-NC 4.0 licensed) that consists of large-scale multi-view sequences of different subjects performing various hand poses. Following , we adopt interacting-hands pose sequences of 21 subjects from InterHand2.6M training set for pre-training. For each subject, an unseen pose sequence is used for evaluation. Our network is trained on three A6000 GPUs using the Adam optimizer  with the learning rates of \(1 10^{-4}\) for eight epochs. Loss weights in Eq. (3) are set as \(_{rgb}=10.0,_{VGG}=0.1\). For interaction detection, we set \(N_{c}=100\) and \(T=90\). For self-adaptive GRM, we set \(T_{d}=0.1\) and \(T_{s}=0.9\). We adopt a coarse-to-fine mesh refinement strategy during training: For the first 5 epochs, we upsample hand meshes to 12,337 points per hand while for the last three epochs, we further upsample hand meshes to 49,281 points per hand.

**One-shot Reconstruction.** We conduct one-shot reconstruction evaluations on the testing set of InterHand2.6M as in [18; 11]. To evaluate the novel pose rendering quality, We evenly sample 349 frames from four pose sequences including four common views in the "test/capture0" subset as the test set. To assess the quality of novel view synthesis, we have selected the initial 50 views from the "test/capture0" subset. The one-shot fitting takes 50 optimization steps with the learning rate of \(1 10^{-2}\). The whole process takes 2.5 minutes with an A6000 GPU. Loss weights in Eq. (4,5) are set as \(_{mask}=1.0,_{reg}=0.01\).

**Baselines and Metrics.** We select four state-of-the-art methods for comparison. We adopt two generalizable NeRFs including KeypointNeRF  designed for human novel view synthesis and VANeRF  designed for single-view interacting-hand image novel view synthesis. We further adapt VANeRF to the one-shot animatable interacting hands reconstruction. Moreover, We include SMPLpix  as an image-space baseline. Besides, although OHTA  is not designed for interacting hands reconstruction, we still implement its one-shot strategy with our pre-trained model (denoted as OHTA*) for one-shot performance comparison. Following previous works [13; 8; 15; 18], we report LPIPS, PSNR, and SSIM as the metrics of rendering quality.

### Comparison with State-of-the-art Methods

**Quantitative Comparison.** Table 1 reports the quantitative results of our method against the baselines in the one-shot reconstruction scenario, including novel view synthesis and novel pose synthesis. We can see that our method significantly outperforms all methods on all metrics in both tasks. NeRF-based methods, KeypointNeRF and VANeRF fail to have good performance facing large view or pose variations due to the under-utilization of the hand priors. SMPLpix as an image-space method lacks generalization and 3D understanding when coping with single-view reconstruction. Compared with OHTA, our method captures more accurate characteristics of the target identity using the identity map and neural map bias.

**Qualitative Comparison.** Figure 4 demonstrates the visual comparison between our approach and the baselines. SMPLpix fails to produce a reasonable hand appearance with the limited information from a single image. VANeRF predicts the basic hand geometry while leaving high-frequency details like wrinkles and veins. OHTA recovers a reasonable hand geometry with most of the appearance close to the target identity while failing to capture fine-grained identity features. Compared with baselines, our method successfully recovers hand details (e.g. nails, wrinkles, and veins) of the

    &  &  \\   & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  KeypointNeRF & 23.55 & 0.804 & 0.326 & - & - & - \\ SMPLpix & 24.50 & 0.868 & 0.170 & 24.26 & 0.854 & 0.173 \\ VANeRF & 25.38 & 0.848 & 0.226 & 24.42 & 0.822 & 0.250 \\ OHTA* & 25.31 & 0.851 & 0.184 & 25.93 & 0.880 & 0.156 \\ Ours & **26.14** & **0.869** & **0.161** & **26.56** & **0.890** & **0.133** \\   

Table 1: One shot synthesis comparison with state-of-the-art methods on Interhand2.6M.

target identity for various views and poses. The qualitative comparisons demonstrate our robust performance for one-shot animatable hand avatars creation.

### Ablation Study

We conduct ablations studies in each of the two stages of our framework. Particularly, we focus on the components of IGSN in the first stage while the loss terms for one-shot fitting in the second stage.

#### 4.3.1 Interaction-aware Gaussian Splatting

The evaluation is conducted in the "train/capture0" subset with 23 pose sequences for training and 1 for testing. The quantitative results are reported in Table 2 Stage-One. The results reflect that each design does bring performance gains and the best performance is obtained by the full model.

**Effectiveness of GRM.** To validate the effectiveness of our Gaussian points refinement module, we implement a variant by substituting refined Gaussian points with upsampled hand mesh points (denoted as w/o GRM. in Table 2). To further verify the effectiveness of our points modification based on the points validation prediction, we implement a variant by replacing the Gaussian points

Figure 4: Qualitative comparisons with state-of-the-art methods. The input image is shown in the top-left grid labeled in red. The first row presents results without changing the pose from the input view (left) and an alternative view (right), while results in the remaining rows are with novel poses.

Figure 5: Visual examples of the ablation study on the proposed components in the hand-prior learning stage (top) and the one-shot fitting stage (bottom).

[MISSING_PAGE_FAIL:9]

that are one-shot optimized on the hands of new subjects. Additionally, our framework employs an interaction-aware attention module and a self-adaptive refinement module to detect and handle regions with intra- and inter-hand interactions. The proposed method outperforms cutting-edge methods on the Interhand2.6M dataset and creates high-quality avatars for various tasks successfully.

## 6 Acknowledgments

This work was supported in part by National Key R&D Program of China (2022YFA1004100), National Science Foundation of China Grant No. 62176035, No. 62372482, No. 62476293, and No. 61936002, National Science and Technology Major Project (2020AAA0109704), Guangdong Outstanding Youth Fund (Grant No. 2021B1515020061), Shenzhen Science and Technology Program (Grant No. GJHZ20220913142600001), Nansha Key RD Program under Grant No.2022ZD014.