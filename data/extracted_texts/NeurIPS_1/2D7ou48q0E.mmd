# Navigating Data Heterogeneity in Federated Learning:

A Semi-Supervised Federated Object Detection

 Taehyeon Kim\({}^{1}\)  Eric Lin\({}^{2}\)  Junu Lee\({}^{3}\)  Christian Lau\({}^{2}\)  Vaikkunth Mugunthan\({}^{2}\)

\({}^{1}\)KaiST \({}^{2}\)DynamoFL \({}^{3}\)The Wharton School

potter32@kaist.ac.kr

###### Abstract

Federated Learning (FL) has emerged as a potent framework for training models across distributed data sources while maintaining data privacy. Nevertheless, it faces challenges with limited high-quality labels and non-IID client data, particularly in applications like autonomous driving. To address these hurdles, we navigate the uncharted waters of Semi-Supervised Federated Object Detection (SSFOD). We present a pioneering SSFOD framework, designed for scenarios where labeled data reside only at the server while clients possess unlabeled data. Notably, our method represents the inaugural implementation of SSFOD for clients with 0% labeled non-IID data, a stark contrast to previous studies that maintain some subset of labels at each client. We propose **FedSTO**, a two-stage strategy encompassing **S**elective **T**raining followed by **O**rthogonally enhanced full-parameter training, to effectively address data shift (e.g. weather conditions) between server and clients. Our contributions include selectively refining the backbone of the detector to avert overfitting, orthogonality regularization to boost representation divergence, and local EMA-driven pseudo label assignment to yield high-quality pseudo labels. Extensive validation on prominent autonomous driving datasets (BDD100K, Cityscapes, and SODA10M) attests to the efficacy of our approach, demonstrating state-of-the-art results. Remarkably, FedSTO, using just 20-30% of labels, performs nearly as well as fully-supervised centralized training methods.

## 1 Introduction

Federated Learning (FL) enables decentralized training across distributed data sources, preserving data privacy . It has emerged in response to the need for privacy, security, and regulatory compliance such as GDPR  and CCPA . FL trains models on local devices and shares only model updates, thereby improving privacy and efficiency. In a typical FL cycle, each client updates a shared model with local data, sends the updates to a server for parameter aggregation, and then updates its local model with the newly aggregated global model sent back by the server.

Despite the potential of FL, the assumption of fully labeled data restricts its practicality . In order to acquire high-quality labels, data is often transmitted from edge clients to a central server, thereby compromising the privacy assurances provided by FL. Limited labels at the edge necessitate the adoption of transfer learning, self-supervised learning, and semi-supervised learning (SSL) techniques. However, the separation of labeled and unlabeled data complicates the application of these techniques to FL, which can undermine the system's effectiveness. This issue is amplified in labels-at-server scenarios where only the server possesses labeled data, and clients hold only unlabeled data . In autonomous driving, a novel approach is required to bridge the knowledge gap between labeled and unlabeled data without the need for direct data exchange.

While Semi-Supervised Federated Learning (SSFL) has been explored for image classification tasks,, these studies have faced the following challenges:1. Limited scale and complexity of tasks with datasets such as CIFAR and ImageNet, while semi-supervised federated object detection (SSFOD) presents sizably greater difficulties.
2. Non-IID data shift from labeled to unlabeled data. Our investigation stands apart in tackling the most challenging FL situations where clients hold exclusively unlabeled data from a different distribution from labeled server data. This acknowledges inherent heterogeneity of real-world FL settings, such as diverse weather conditions across clients. For instance, one client's dataset may predominantly consist of images captured under cloudy conditions, while others may include images from overcast, rainy, snowy, etc. conditions.

To surmount these inadequately addressed challenges of SSFOD, we introduce FedSTO (Federated Selective Training followed by Orthogonally enhanced training), a two-stage training strategy tailored specifically for our SSFOD framework (Figure 1). Our key contributions include:

* **Selective Training and Orthogonal Enhancement:** FedSTO begins with selective training of the model's backbone while other components remain frozen, fostering more consistent representations and establishing a robust backbone. This promotes generalization across non-IID clients, even in the absence of local labels. The subsequent stage involves fine-tuning all parameters with orthogonal regularizations applied to the non-backbone part of the model. This enhancement step is designed to imbue the predictors with resilience against skewed representations induced by local data heterogeneity, thereby promoting representation divergence and robustness.
* **SSFL with a Personalized EMA-Driven Semi-Efficient Teacher:** To prevent deterioration of teacher pseudo labeling models for non-IID unlabeled clients, we showcase for the first time an SSFOD framework that applies an alternate training methodology , integrated with a Semi-Efficient Teacher , driven by a local Exponential Moving Average (EMA). Our empirical observations suggest that this personalized EMA-driven model provides superior quality pseudo labels for detection, contrary to the commonly used global model

Figure 1: An overview of our FedSTO method within the SSFOD framework with key components: selective training, orthogonal enhancement, and local Exponential Moving Average (EMA)-driven pseudo label assignment, organized into two stages. Algorithm steps are numbered accordingly.

for pseudo labeling in related studies . This approach further enhances the quality of the learning process, mitigating potential pitfalls of noisy pseudo labeling.
* **Performance Improvements:** FedSTO achieves 0.082 and 0.035 higher mAP@0.5 when compared to partially supervised and SSFL baselines respectively, nearly matching the fully supervised model's performance (0.012 gap) on BDD100K  with a mere 25% of labeled data. We demonstrate similar considerable improvements in model generalization (Figure 2) on rigorous benchmark and ablation experiments with 20k-120k datapoints from Cityscapes  and SODA10M , utilizing the YOLOv5 object detector .

Our above contributions present a pioneering approach for utilizing unlabeled data in FL to enhance non-IID detection performance, especially for dynamic objects--an aspect not yet considered in previous research. Despite the paucity of research on SSFOD, we contend that our methods and experiments offer a valuable benchmark for future investigations across diverse domains.

## 2 Related Works

### Federated Learning (FL): Challenges and Advances

FL has gained significant attention in recent years as a privacy-preserving approach to harness the potential of distributed data [27; 20; 21; 15; 28; 33]. Despite the progress made in FL, most research has focused primarily on classification tasks, which may limit its applicability and generalizability to a broader range of real-world problems. Advanced FL techniques are essential to revolutionize various fields, including autonomous driving, healthcare, and finance, by enabling collaborative learning from distributed data sources [6; 30].

Addressing data heterogeneity is of paramount importance in FL, as clients frequently hold data with diverse distributions, which may impact the performance of the global model. To tackle this challenge, researchers have proposed various techniques to handle non-IID data, including adaptive aggregation algorithms and local fine-tuning of models [20; 33]. Personalization constitutes another vital aspect of FL, since clients may exhibit unique requirements or preferences not entirely captured by the global model [3; 19; 39]. Methods such as model distillation  and meta-learning  have been investigated to facilitate client-specific model adaptation and personalization. Finally, communication efficiency is crucial in FL, as exchanging model updates can be resource-intensive. To alleviate this issue, researchers have introduced strategies like quantization , sparsification , and the utilization of a supernet containing multiple subnetworks, with only the selected subnetwork transmitted to the server to reduce communication overhead while preserving model performance .

Figure 2: Performance comparison on BDD100K dataset . “Partially Supervised Training” shows lower-bound performance using partial labels in a centralized setting. “Vanilla Semi-Supervised Federated Learning” and “Our FedSTO” demonstrate improved performance with non-IID federated data. FedSTO approaches the “Fully Supervised Training” upper-bound performance under full label use in a centralized setting. The x-axis shows the number of labeled examples, and the y-axis displays the mean average precision (mAP@0.5) on the test set.

### Semi-Supervised Object Detection (SSOD) with YOLO Object Detector

SSOD has been an active research area, focusing on improving the quality of pseudo labels to enhance overall detection performance [35; 40]. The evolution of SSL techniques in object detection primarily revolves around using pretrained architectures and applying strong data augmentation strategies to generate consistent and reliable pseudo labels.

Traditional single-stage detectors, such as the family of YOLO detectors, have faced notable challenges in leveraging SSL techniques, often underperforming compared to their two-stage counterparts (e.g., Faster RCNN). The limited efficacy of existing SSL methods for single-stage detectors has inspired researchers to develop innovative solutions to overcome these limitations . Recently, a novel pipeline incorporating EMA of model weights has exhibited remarkable enhancements in the performance of single-stage detectors like YOLO detectors . By utilizing the EMA model for pseudo labeling, researchers have adeptly addressed the inherent weaknesses in single-stage detectors, substantially elevating their performance in SSL contexts for object detection tasks.

### Semi-Supervised Federated Learning (SSFL)

SSFL has emerged as a promising approach to address the challenge of limited labeled data in FL scenarios [5; 10; 43; 2; 18; 42]. SSFL aims to jointly use both labeled and unlabeled data owned by participants to improve FL. Two primary settings have been explored: Labels-at-Client and Labels-at-Server [10; 11]. In the Labels-at-Client scenario, clients possess labeled data, while the server only has access to unlabeled data. Conversely, in the Labels-at-Server scenario, the server holds labeled data, and clients have only unlabeled data. Despite the progress in SSFL, there remain limitations in the current research landscape. The majority of existing SSFL research predominantly focuses on image classification tasks, leaving other applications relatively unaddressed. In this study, we address these limitations by tackling the more realistic and challenging scenarios with edge clients having (1) no labels and (2) non-IID data (domain shift from the server labeled data), specifically in the context of object detection tasks.

## 3 Problem Statement

SSFODWe tackle a semi-supervised object detection task involving a labeled dataset \(=\{_{i}^{s},_{i}^{s}\}_{i=1}^{N_{S}}\) and an unlabeled dataset \(=\{x_{i}^{u}\}_{i=1}^{N_{U}}\), focusing on scenarios where \(N_{S} N_{U}\). In our SSFOD setup, as illustrated in Figure 1, we assume \(M\) clients each possessing an unsupervised dataset \(x^{u,m}\). The server retains the labeled dataset \(\{x^{s},^{s}\}\) and a model parameterized by \(W^{s}\). Each client model, parameterized by \(W^{u,m}\), shares the object detection architecture denoted by \(f:(,W) f(,W)\). We assume that all models share the same object detection architecture, denoted by \(f:(,W) f(,W)\), which maps an input \(\) and parameters \(W\) to a set of bounding boxes and their corresponding class probabilities on the \(K\)-dimensional simplex (e.g., using the sigmoid function applied to model outputs unit-wise).

Data HeterogeneityOur study addresses non-IID data resulting from varying weather conditions such as cloudy, overcast, rainy, and snow, inspired by feature distribution skew or covariate shift . We utilize three datasets, BDD100K , CityScapes , and SODA10M , each displaying class distribution heterogeneity and label density heterogeneity. Our aim is an SSFOD framework that can manage this heterogeneity, maintaining performance across diverse conditions and distributions. Data is considered IID when each client exhibits a balanced weather condition distribution.

EvaluationIn our framework, we assess the performance of all detection tasks based on mean average precision (mAP@0.5), a standard metric in object detection literature that provides a comprehensive view of model performance across various object classes and sizes. Importantly, we evaluate the post-training performance of our method by assessing the personalized models of the server and client on their respective datasets. This approach ensures a fair and context-specific evaluation, reflecting the true performance of the personalized models in their intended environments.

Baseline TrainingOur work explores two principal baselines: "Centralized Training" and "Federated Learning". Depending on the degree of labeled data utilization, we categorize the training into "Partially Supervised" and "Fully Supervised". An ideal situation is one where a fully supervised model is trained in a centralized fashion, utilizing all labeled data. In contrast, a more challenging scenario involves a partially supervised model trained solely on the server's limited labeled data. Under our problem setup, we initially establish a baseline by performing partial supervision on theserver's limited labeled data, which serves as a pretraining step. Following this, each client conducts "Unsupervised learning with unlabeled data". Upon completion, clients transmit their model weights to the server. The server then aggregates these weights and fine-tunes the amalgamated model using its labeled data. The updated model is subsequently disseminated back to the clients, culminating one learning round. This cyclical process, known as alternate training in Diao et al. , continues effectively. It merges the strength of supervised and unsupervised learning to capitalize on unlabeled data while preventing model deterioration, thereby optimizing model performance.

Personalized Pseudo Labeling for Unlabeled ClientsA crucial obstacle in SSFOD lies in precise pseudo label assignment, as improper allotments can result in label inconsistencies, thus negatively impacting mutual learning performance. Building upon the foundation by Xu et al.  in centralized settings, we present the first extension of this approach to federated settings, leveraging a personalized Pseudo Label Assigner (PLA) equipped with local EMA models. This technique bifurcates pseudo labels into reliable and unreliable categories using high and low thresholds, thus ensuring a robust and precise learning mechanism in federated environments. In FL, the PLA can be applied to both global and local models. However, the global model may fall short in capturing unique features of local data, compromising pseudo label quality. As demonstrated in our evaluation (Table 1), locally updated EMA models outperform global models. While it is feasible to federate the local EMA model, it introduces certain trade-offs, such as increased communication costs and minor performance degradation compared to the local EMA model. Our SSFOD framework, therefore, incorporates a local PLA with a local EMA model, optimally balancing communication efficiency and model stability, ensuring an effective learning process for SSOD tasks in distributed environments.

SSFOD with YOLOWe utilize the YOLOv5 model, a single-stage object detector, in our evaluation. Existing literature shows a scarcity of research on SSFOD within FL like FedAvg , particularly for single-stage detectors like YOLO. Figure 3 compares various learning approaches in centralized and federated settings, denoted by green dotted and blue hatched boxes, respectively. We highlight non-IID scenarios with labeled (cloudy) and unlabeled data (overcast, rainy, snowy). In the CL scenario, fully supervised methods noticeably surpass partially supervised ones, and SSL approaches almost match the performance of fully supervised methods. However, baseline training for FL falls substantially short of these high standards, particularly with unlabeled data.

    &  &  &  \\   & & Cloudy & Overcast & Rainy & Snowy & Cloudy & Overcast & Rainy & Snowy \\   & Fully Supervised & 0.600 & 0.604 & 0.617 & 0.597 & 0.600 & 0.604 & 0.617 & 0.597 \\  & Partially Supervised & 0.540 & 0.545 & 0.484 & 0.474 & 0.528 & 0.545 & 0.533 & 0.510 \\   & Global Model  & 0.555 & 0.560 & 0.497 & 0.488 & 0.540 & 0.551 & 0.576 & 0.542 \\  & Local EMA Model  & 0.560 & 0.566 & 0.553 & 0.553 & 0.572 & 0.588 & 0.593 & 0.610 \\   

Table 1: Performance under different weather conditions for non-IID and IID data splits with 1 server and 3 clients. The results are presented for centralized (Fully Supervised and Partially Supervised) and federated approaches with a pseudo label assigner (Global Model and Local EMA Model).

Figure 3: Performance of various methods on the BDD100K dataset , with the server containing labeled data for the “Cloudy” category and 3 clients having unlabeled data for “Rainy”, “Snowy”, and ”Overcast” categories. Baseline SSFL (red hatched boxes) struggles in comparison to centralized learning (bars in green dotted boxes). “Fully Supervised” and “Partially Supervised” refer to training a centralized model with the complete labeled dataset and only the “Cloudy” labeled data, respectively.

## 4 Main Method: FedSTO

To mitigate these inherent hurdles presented by FL, we introduce FedSTO, a method that unfolds in two stages, preceded by a warmup stage. The process commences with an emphasis on robust representation learning for pretraining (Subsection 4.1), followed by full parameter training (Subsection 4.2). The initial stage of pretraining integrates a warm-up period utilizing labeled data at the server, transitioning into selective training. This groundwork is fortified by the orthogonal enhancement implemented in the subsequent full parameter training phase.

### Selective Training (ST)

Selective Training (ST) is designed to address the primary challenge of establishing a robust backbone for object detector in FL. The approach unfolds as follows:

1. **Labeled dataset training**: All model parameters are updated using a labeled dataset. This step ensures training commences on high quality labeled data, mitigating the potential destabilizing effect of noisy, unlabeled data, and heterogeneity from diverse weather conditions.
2. **Client-side training with unlabeled dataset**: The model, updated in the previous step, is dispatched to the clients. Each client trains the model on their local unlabeled dataset. However, during this phase, only the backbone part of the model is updated, leaving other components frozen. This selective updating procedure fosters more consistent representations by sharing the same non-backbone part (e.g., neck and head), and thus enhances its potential generalization capabilities by concentrating on feature extraction.
3. **Server-side aggregation**: The server aggregates the updated backbone parameters from clients, effectively synthesizing the learned information from diverse unlabeled datasets. The aggregated backbone is then utilized in the first step for further training, repeating the process until performance convergence.

By adhering to this procedure, ST effectively navigates the challenges inherent in the progression of FL while simultaneously accruing substantial benefits. Ensuring stability in semi-supervised object detection tasks is paramount. The exposure to heterogeneous unlabeled data, potentially characterized by noise or variable quality, can induce biases into the neck and head components of the model, thereby risking performance degradation by inadvertently generating low-quality or imprecise pseudo annotations. To mitigate this, ST employs a selective update strategy specifically targeting the backbone of the model, which is predominantly entrusted with the task of extracting salient features from the input data. By concentrating on the backbone during training, ST aids in the preservation of model stability and the enhancement of its generalization capabilities. Furthermore, in this stage, the communication cost between the server and clients is significantly reduced by uploading only the backbone part from clients to a server. Consequently, it significantly minimizes the deleterious impacts of heterogeneous unlabeled data on overall model performance (Table 2). While ST brings marginal improvements in IID conditions, it presents potent effects under Non-IID circumstances, emphasizing its efficacy in handling heterogeneous data distributions.

### Full Parameter Training (FPT) with Orthogonal Enhancement

Inspired by the critical need for personalized models to exhibit robustness against feature distribution skewness--predominantly due to diverse weather conditions--we integrate orthogonality regularization presented by Kim et al.  which penalizes the symmetric version of the spectral restricted isometry property regularization \(_{}(^{T}-I)+(^{T}-I)\) within the SSFOD framework where \(()\) calculates the spectral norm of the input matrix and \(\) is a weight matrix from non-backbone parts. This regularization is applied during both server and client training stages and targets non-backbone

    &  &  \\   & Cloudy & Overcast & Rainy & Snowy & Total & Cloudy & Overcast & Rainy & Snowy & Total \\  Partially Supervised & 0.540 & 0.545 & 0.484 & 0.474 & 0.511 & 0.528 & 0.545 & 0.533 & 0.510 & 0.529 \\  + SSFL  with Local EMA Model & 0.560 & 0.566 & 0.553 & 0.553 & 0.558 & 0.572 & 0.588 & 0.593 & **0.610** & 0.591 \\ + Selective Training & 0.571 & 0.583 & 0.557 & 0.556 & 0.567 & 0.576 & 0.578 & 0.594 & 0.599 & 0.587 \\ \#FPT with Orthogonal Enhancement  & **0.596** & **0.607** & **0.590** & **0.580** & **0.593** & **0.591** & **0.634** & **0.614** & 0.595 & **0.609** \\   

Table 2: Performance on the BDD dataset with 1 labeled server and 3 unlabeled clients as each element of our FedSTO approach within the SSFOD framework is added. It highlights how each added method contributes to the overall performance under both Non-IID and IID conditions.

components of the architecture. Our approach promotes generation of diverse, non-redundant, and domain-invariant feature representations, thereby enhancing the model's robustness, reducing noise influence, and significantly augmenting its ability to handle unlabeled data across varied domains.

Incorporating orthogonality regularization into our framework substantially amplifies the divergence in the embedding space, enhancing the model's overall detection quality and the reliability of pseudo labels. Importantly, our strategy of embedding orthogonality into the non-backbone parts of the model, such as the neck and head, fosters a more balanced and comprehensive training process. This reduces the bias towards specific weather conditions and the heterogeneity of object categories, leading to improved performance as demonstrated in Table 2. Our approach draws upon successful techniques from fine-tuning , and transfer learning, and is particularly inspired by meta-learning concepts,. In particular, the tendency of the non-backbone components of the model to develop biases prompts us to introduce an orthogonal property to this section. This measure helps counteract these biases, thereby further enhancing the model's robustness and adaptability when confronted with diverse, unlabeled data across multiple domains.

### Main Algorithm: FedSTO

```
input : server model parameterized by \(W_{s}\), the number of rounds for each phase \(T_{1},T_{2}\), client models parameterized by \(\{W_{u,1},...,W_{u,M}\}\), client backbone part parameterized by \(\{B_{u,1},...,B_{u,M}\}\)
1:\(W_{s}(x_{s},y_{s},W_{s})\)// Supervised training at server /* Phase 1: Selective Training for Pretraining */
2:for\(t 0,,T_{1}-1\)do
3:\(S^{t}\)
4:for each client \(k S^{t}\) in parallel do
5:\(W_{u,k}(x_{u,k},B_{u,k})\)// Client-Update
6:endfor
7:\(W_{s}_{k S^{t}}p_{k}W_{u,k}\)// Aggregation
8:\(W_{s}(x_{s},y_{s},W_{s})\)// Server-Update
9:endfor /* Phase 2: Full Parameter Training with Orthogonal Enhancement */
10:for\(t 0,,T_{2}-1\)do
11:\(S^{t}\)
12:for each client \(k S^{t}\) in parallel do
13:\(W_{u,k}(x_{u,k},W_{u,k})\)// Client-OrthogonalUpdate
14:endfor
15:\(W_{s}_{k S^{t}}p_{k}W_{u,k}\)// Aggregation
16:\(W_{s}(x_{s},y_{s},W_{s})\)// Server-OrthogonalUpdate
17:endfor
```

**Algorithm 1**FedSTO Algorithm within the SSFOD Framework

Algorithm 1 illustrates the overall procedure of FedSTO within the SSFOD framework. The server model, parameterized by \(W_{s}\), is first trained in a supervised fashion during the warm-up phase (Line 1). The algorithm then transitions to Phase 1: Selective Training for Pretraining. This phase involves multiple training iterations (Line 3), where in each iteration, a subset of clients is sampled (Line 4). The backbone part of each client's model, \(W_{u,k}\), is updated using their local unlabeled datasets (Line 6). The updated parameters are then aggregated at the server (Line 8), and the server model is updated using its labeled dataset (Line 9). In Phase 2: Full Parameter Training with Orthogonal Enhancement, the Client-OrthogonalUpdate and Server-OrthogonalUpdate methods are employed (Lines 14 and 18), introducing orthogonality regularization to the training process. This second phase debiases the non-backbone parts of the model, ensuring they have a robust predictor across various weather conditions that effectively counterbalance the inherent data heterogeneity.

## 5 Experiment

### Experimental Setup

#### 5.1.1 Datasets

Bdd100K We utilize the BDD100K dataset, which consists of 100,000 driving videos recorded across diverse U.S. locations and under various weather conditions, to evaluate our method. Eachvideo, approximately 40 seconds in duration, is recorded at 720p and 30 fps, with GPS/IMU data available for driving trajectories. For our experiments, we specifically select 20,000 data points, distributed across four distinct weather conditions--cloudy, rainy, overcast, and snowy. In this study, we primarily focus on five object categories: person, car, bus, truck, and traffic sign. The dataset is partitioned into clients based on these weather conditions, simulating data-heterogeneous clients. This experimental setup enables us to investigate the influence of data heterogeneity on our framework and to evaluate its robustness under realistic conditions.

Cityscape We conduct additional experiments using the Cityscapes dataset, which consists of urban street scenes from 50 different cities. Given that this dataset does not provide precise weather information for each annotation, we distribute the data to clients in a uniformly random manner. For our studies, we employ the package, encompassing fine annotations for 3,475 images in the training and validation sets, and dummy annotations for the test set with 1,525 images. We also include the other package, providing an additional 19,998 8-bit images for training.

SODA10M To evaluate our approach under diverse conditions, we employ the SODA10M dataset, which features varied geographies, weather conditions, and object categories. In an IID setup, 20,000 labeled data points are uniformly distributed among one server and three clients. For a more realistic setup, the 20,000 labeled data points are kept on the server while 100,000 unlabeled data points are distributed across the clients. This arrangement enables performance evaluation under distinct weather conditions--clear, overcast, and rainy--showcasing resilience and robustness.

#### 5.1.2 Training Details

We conduct our experiments in an environment with one server and multiple clients, depending on the experiment. Both the server and the clients operate on a single local epoch per round. Our training regimen spans 300 rounds: 50 rounds of warm-up, 100 rounds of pretraining (\(T_{1}\)), and 150 rounds of orthogonal enhancement (\(T_{2}\)). We use the YOLOv5 Large model architecture with Mosaic, left-right flip, large scale jittering, graying, Gaussian blur, cutout, and color space conversion augmentations. A constant learning rate of 0.01 was maintained. Binary sigmoid functions determined objectiveness and class probability with a balance ratio of 0.3 for class, 0.7 for object, and an anchor threshold of 4.0. The ignore threshold ranged from 0.1 to 0.6, with an Non-Maximum Suppression (NMS) confidence threshold of 0.1 and an IoU threshold of 0.65. We incorporate an exponential moving average (EMA) rate of 0.999 for stable model parameter representation.

### Results

Table 3 illustrates the efficacy of our proposed SSFOD method against various baselines and state-of-the-art approaches on the BDD100K dataset. FedSTO significantly outperforms other techniques under different weather conditions and data distribution scenarios, i.e., IID and Non-IID. In the CL scenarios, the fully supervised approach yields the highest performance, with SSL methods, such as EMA Teacher , demonstrating competitive results. However, the real challenge lies in federated settings, where data privacy and distribution shift become critical considerations. In the SSFOD framework, our FedSTO method consistently surpasses other SSFL techniques. Notably, it achieves superior results even in challenging Non-IID settings, demonstrating its robustness to data distribution shifts. Similar trends hold when increasing the number of clients as shown in the appendix. In IID

    &  &  &  &  \\   & & & Cloudy & Overcast & Rainy & Snowy & Total & Cloudy & Overcast & Rainy & Snowy & Total \\   &  & Fully Supervised & 0.600 & 0.604 & 0.617 & 0.597 & 0.605 & 0.600 & 0.604 & 0.617 & 0.597 & 0.605 \\  & & Partially Supervised & 0.540 & 0.545 & 0.484 & 0.474 & 0.511 & 0.528 & 0.545 & 0.533 & 0.510 & 0.529 \\   & & Unbiased Teacher  & 0.551 & 0.550 & 0.502 & 0.503 & 0.527 & 0.546 & 0.557 & 0.541 & 0.533 & 0.544 \\   & & EMA Teacher  & 0.598 & 0.539 & 0.568 & 0.588 & 0.581 & 0.586 & 0.570 & 0.571 & 0.573 & 0.575 \\   &  & Fully Supervised & 0.627 & 0.614 & 0.607 & 0.585 & 0.608 & 0.635 & 0.612 & 0.608 & 0.595 & 0.613 \\   & & FedAvg  & 0.560 & 0.566 & 0.553 & 0.553 & 0.558 & 0.572 & 0.588 & 0.593 & **0.610** & 0.591 \\   & & FedDrn & 0.508 & 0.569 & 0.541 & 0.522 & 0.535 & 0.355 & 0.414 & 0.420 & 0.397 & 0.400 \\   & & FedOpr & 0.561 & 0.572 & 0.565 & 0.566 & 0.566 & 0.591 & 0.587 & 0.588 & 0.577 & 0.586 \\   & & FedPAC & 0.514 & 0.532 & 0.496 & 0.489 & 0.508 & 0.510 & 0.549 & 0.547 & 0.554 & 0.540 \\   & & **FedSTO** & **0.596** & **0.607** & **0.590** & **0.580** & **0.593** & **0.591** & **0.634** & **0.614** & 0.595 & **0.609** \\   

Table 3: Comparison of FedSTO within the SSFOD framework against the Baselines, SSL, SSFL methods with 1 server and 3 clients on BDD100K dataset . FedSTO exhibits improvements under various weather conditions on both IID and Non IID cases, and performs close to the centralized fully supervised case. \(\) denotes the SSFL with the local EMA model as a pseudo label generator.

conditions, our method continues to excel, achieving results close to fully supervised centralized approach. These findings highlight the strength of our FedSTO method in leveraging the benefits of FL while mitigating its challenges. The robust performance of our approach across various weather conditions and data distributions underscores its potential for real-world deployment.

When examining the performance on the Cityscapes dataset under uniformly random distributed conditions, the superiority of FedSTO within the SSFOD framework also remains apparent, as shown in Table 4. Compared to other methods, FedSTO consistently demonstrates improved generalization across most object categories, both for labeled and unlabeled data. Intriguingly, the performance of FedSTO surpasses even that of SSL in CL environments.

Evaluation with mAP@0.75The mAP@0.75 results on the BDD dataset highlight the efficacy of the FedSTO approach (Table 5). In Non-IID settings, while the Fully Supervised centralized method achieve an average mAP of 0.357, FedSTO recorded 0.338, exhibiting comparable performance. However, under IID conditions, FedSTO registers an mAP@0.75 of 0.357, closely matching the SFL result of 0.359. These results indicate that FedSTO offers competitive object detection capabilities, even with stricter IoU thresholds.

Results on Real World Dataset, SODA10m Figure 3(a) illustrates the performance of our method and other baselines on the SODA10m dataset, where labeled data is synthetically divided in an IID manner across one server and three clients. Our method demonstrates near-parity with the fully supervised approach, evidencing its efficacy. Figure 3(b) represents the averaged performance across varying weather conditions on the SODA10m dataset. Here, all 20k labeled data resides on the server, and 100k unlabeled data points from SODA10m are distributed across three clients. Despite these variations in conditions, our method consistently outperforms other baselines, confirming its robustness and applicability in diverse environments.

    &  &  &  &  \\   & & & &  & & & \\   & & Person & Car & Bus & Truck & Traffic Sign & Person & Car & Bus & Truck & Traffic Sign \\   & SL & Fully Supervised & 0.569 & 0.778 & 0.530 & 0.307 & 0.500 & 0.560 & 0.788 & 0.571 & 0.283 & 0.510 \\  & & Partially Supervised & 0.380 & 0.683 & 0.193 & 0.302 & 0.246 & 0.358 & 0.648 & 0.343 & 0.138 & 0.255 \\   & SSL & Unbiased Teacher  & 0.391 & 0.695 & 0.225 & 0.320 & 0.297 & 0.410 & 0.689 & 0.373 & 0.129 & 0.354 \\  & & EAM Teacher  & 0.475 & 0.711 & 0.354 & 0.347 & 0.379 & 0.460 & 0.727 & 0.436 & 0.144 & 0.378 \\   & SPL & Fully Supervised & 0.498 & 0.715 & 0.357 & 0.289 & 0.410 & 0.492 & 0.714 & 0.451 & 0.251 & 0.425 \\   & & FedAvg  & 0.450 & 0.697 & 0.310 & **0.304** & 0.356 & 0.482 & 0.725 & 0.425 & **0.247** & 0.397 \\   & SSFL\({}^{}\) & FedBN  & 0.488 & 0.709 & 0.325 & 0.285 & 0.411 & 0.375 & 0.618 & 0.046 & 0.031 & 0.286 \\   & & **FedSTO** & **0.504** & **0.720** & **0.342** & 0.261 & **0.415** & **0.487** & **0.740** & **0.460** & 0.181 & **0.437** \\   

Table 4: Performance under random distributed cases of Cityscapes . FedSTO exhibits improvements under various object categories, and significantly outperforms the performance for unlabeled clients. \(\) denotes the SSFL with the local EMA model as a local pseudo label generator.

Figure 4: (a) Performance of various methods on the SODA10m dataset in an IID setting, (b) Average performance across different weather conditions using unlabeled data from the SODA10m dataset.

    &  &  &  &  \\   & & & Cloudy & Overcast & Rainy & Snowy & Total & Cloudy & Overcast & Rainy & Snowy & Total \\   &  & Fully Supervised & 0.351 & 0.352 & 0.368 & 0.356 & 0.357 & 0.351 & 0.352 & 0.368 & 0.356 & 0.357 \\  & & Partially Supervised & 0.281 & 0.295 & 0.261 & 0.303 & 0.285 & 0.281 & 0.295 & 0.261 & 0.303 & 0.285 \\   & SFL & Fully Supervised & 0.384 & 0.366 & 0.357 & 0.317 & 0.356 & 0.377 & 0.356 & 0.352 & 0.349 & 0.359 \\   & & FedSTO & WO ST & 0.321 & 0.303 & 0.265 & 0.267 & 0.289 & 0.298 & 0.321 & 0.324 & 0.302 & 0.311 \\  & & FedSTO & 0.347 & 0.351 & 0.341 & 0.312 & 0.338 & 0.343 & 0.375 & 0.361 & 0.350 & 0.357 \\   

Table 5: mAP@0.75 on the BDD dataset with 1 labeled server and 3 unlabeled clients.

Varying Number of ClientsIn a non-IID BDD dataset configuration with 1 server and 20 clients, our proposal advances beyond competing methods, scoring 0.455 and 0.458 on labeled and unlabeled data, respectively. This outcome showcases our method's aptitude for tackling intricate real-world circumstances.

Varying Sampling RatioTable 7 demonstrates the impact of different client sampling ratios on the FedSTO performance using the BDD100k dataset. Notably, even at a lower sampling ratio of 0.1, FedSTO yields commendable results, especially in the unlabeled set for categories like 'Car' (0.738) and 'Bus' (0.573). This underscores that a reduced client sampling can still lead to significant performance improvements, emphasizing the efficiency and adaptability of the FL approach.

Efficiency on Network BandwidthTable 8 highlights the communication costs over 350 rounds of training involving 100 clients with a 0.5 client sampling ratio per round. By removing the neck component of the Yolov5L model, its size is reduced from 181.7MB to 107.13MB. This reduction significantly benefits FedSTO in Phase 1, leading to overall bandwidth savings. When comparing with traditional SSFL methods such as FedAvg and FedProx,, FedSTO utilizes only **2,166.23 GB** - a substantial **20.52%** reduction in network bandwidth.

## 6 Conclusion

This paper introduces a novel Semi-Supervised Federated Object Detection (SSFOD) framework, featuring a distinctive two-stage training strategy known as FedSTO. Designed to address the challenges of heterogeneous unlabeled data in federated learning, FedSTO employs selective training and orthogonality regularization with personalized pseudo labeling. These mechanisms facilitate robust and diverse feature learning, thereby enhancing object detection performance across multiple weather conditions and data distributions. Empirical results provide compelling evidence of the superiority of FedSTO over established federated and semi-supervised learning methodologies. Notably, despite operating with the challenging constraint where non-IID clients have no labels, FedSTO successfully counteracts domain shift and achieves performance that is comparable to fully supervised centralized models. This accomplishment constitutes significant strides toward realizing more efficient and privacy-preserving learning in realistic FL settings. As we venture ahead, we aim to concentrate our research efforts on refining FedSTO and exploring additional strategies for leveraging unlabeled data with various domains and model architectures. We anticipate the work presented in this paper will stimulate continued progress in this rapidly evolving field.

    &  &  \\   &  &  \\   & Person & Car & Bus & Truck & Traffic Sign & Person & Car & Bus & Truck & Traffic Sign \\  Server Only (i.e., client sampling ratio 0.0) & 0.378 & 0.710 & 0.141 & 0.425 & 0.490 & 0.337 & 0.707 & 0.160 & 0.338 & 0.491 \\ FedSTO with client sampling ratio 0.1 & 0.393 & 0.714 & 0.442 & 0.510 & 0.540 & 0.487 & **0.738** & **0.573** & **0.589** & **0.617** \\ FedSTO with client sampling ratio 0.2 & **0.458** & **0.747** & **0.476** & **0.521** & **0.571** & 0.440 & 0.731 & 0.378 & 0.525 & 0.573 \\ FedSTO with client sampling ratio 0.5 & 0.444 & 0.745 & 0.437 & 0.502 & 0.550 & **0.489** & 0.730 & 0.438 & 0.512 & 0.538 \\   

Table 7: Performance (mAP@0.5) under Non-IID scenarios of BDD100k dataset with 1 server and 100 clients according to the changes of client sampling ratio for implementing FedSTO. The term ‘Server Only’ aligns with the notion of ‘partially supervised’ in CL settings.

   Method & 
 Warm-up \\ (50 rounds) \\  & Phase 1 (150 rounds) & Phase 2 (150 rounds) & Total & Reduction \\  FedAvg & 0 & 100 * 0.50 * 150 * 181.7 & 1,362.75 GB & 100 * 0.50 * 150 * 181.7 & 1,362.75 GB & 2,725.50 GB & - \\  FedBN & 0 & 100 * 0.50 * 150 * 181.24 & 1359.30 GB & 100 * 0.50 * 150 * 181.24 & 1359.30 GB & 2,718.60 GB & 0.25 \% \\ FedSTO & 0 & 100 * 0.50 * 150 * 107.13 & 803.48 GB & 100 * 0.50 * 150 * 181.7 & 1,362.75 GB & **2,166.23 GB** & **20.52 \%** \\   

Table 8: Communication costs over 350 rounds of training with 100 clients when the client sampling ratio is 0.5 per each round. The total Yolov5L size is 181.7MB while the model without the neck part is 107.13MB. Additionally, the model size without BN layers (FedBN ) is 181.24 MB. Here, ‘Reduction’ expresses how much communication cost is reduced compared to using vanilla SSFL (FedAvg and FedProx ).