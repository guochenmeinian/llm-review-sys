# Data-driven Optimal Filtering for

Linear Systems with Unknown Noise Covariances

 Shahriar Talebi\({}^{1,2}\)   Amirhossein Taghvaei\({}^{1}\)   Mehran Mesbahi\({}^{1}\)

\({}^{1}\)University of Washington, Seattle, WA, 98105

\({}^{2}\)Harvard University, Cambridge, MA, 02138

talebi@seas.harvard.edu   amirtag@uw.edu   mesbahi@uw.edu

###### Abstract

This paper examines learning the optimal filtering policy, known as the Kalman gain, for a linear system with unknown noise covariance matrices using noisy output data. The learning problem is formulated as a stochastic policy optimization problem, aiming to minimize the output prediction error. This formulation provides a direct bridge between data-driven optimal control and, its dual, optimal filtering. Our contributions are twofold. Firstly, we conduct a thorough convergence analysis of the stochastic gradient descent algorithm, adopted for the filtering problem, accounting for biased gradients and stability constraints. Secondly, we carefully leverage a combination of tools from linear system theory and high-dimensional statistics to derive bias-variance error bounds that scale logarithmically with problem dimension, and, in contrast to subspace methods, the length of output trajectories only affects the bias term.

## 1 Introduction

The duality of control and estimation plays a crucial role in system theory, linking two distinct synthesis problems . This duality is an effective bridge between two distinct disciplines, facilitating development of theoretical and computational techniques in one domain and then adopting them for use in the other. For example, the stability proof of the Kalman filter relies on the stabilizing characteristic of the optimal feedback gain in the dual Linear Quadratic Regulator (LQR) optimal control problem [7, Ch. 9]. In this paper, we leverage this duality to learn optimal filtering policies using recent advances in data-driven algorithms for optimal control.

We consider the estimation problem for a system with a known linear dynamic and observation model, but unknown process and measurement noise covariances. Our objective is to learn the optimal steady-state Kalman gain using a training dataset comprising independent realizations of the observation signal. This problem has a rich history in system theory, often explored within the context of adaptive Kalman filtering . A comprehensive summary of four solution approaches to this problem can be found in the classical reference . These approaches include Bayesian inference , Maximum likelihood , covariance matching , and innovation correlation methods . While Bayesian and maximum likelihood approaches are computationally intensive, and covariance matching introduces biases in practice, the innovation correlation-based approaches have gained popularity and have been the subject of recent research . For an excellent survey on this topic, refer to the article . However, it is important to note that these approaches often lack non-asymptotic guarantees and heavily depend on statistical assumptions about the underlying model.

In the realm of optimal control, significant progress has been made in the development of data-driven synthesis methods. Notably, recent advances have focused on the adoption of first-order methods for state-feedback LQR problems . The direct optimization of policies from a gradient-dominantperspective has first proven in  to be remarkably effective with global convergence despite non-convex optimization landscape. It has been demonstrated that despite the non-convex nature of the cost function, when expressed directly in terms of the policy, first-order methods exhibit global convergence to the optimal policy. Building upon this line of work, the use of first-order methods for policy optimization has been explored in variants of the LQR problem. These include Output-feedback Linear Quadratic Regulators (OLQR) , model-free setup , risk-constrained setup , Linear Quadratic Gaussian (LQG) , and most recently, Riemannian constrained LQR . These investigations have expanded the scope of data-driven optimal control, demonstrating the versatility and applicability of first-order methods for a wide range of synthesis problems.

The objective of this paper is to provide fresh insights into the classical estimation problem by leveraging the duality between control and estimation and incorporating recent advances in data-driven optimal control. Specifically, building on the fundamental connection between the optimal mean-squared error estimation problem and the LQR problem (Prop. 1), we reformulate determining the optimal Kalman gain as a problem of synthesizing an optimal policy for the adjoint system, under conditions that differ from those explored in the existing literature (see (10) and Remark 3). Upon utilizing this relationship, we propose a Stochastic Gradient Descent (SGD) algorithm for learning the optimal Kalman gain, accompanied by novel non-asymptotic error guarantees in presence of biased gradient and stability constraint. Our approach opens up promising avenues for addressing the estimation problem with robust and efficient data-driven techniques. The following is an informal statement of our main results (combination of Thm. 1 and Thm. 2), and missing proofs appear in the supplementary materials.

**Theorem 3** (Informal).: _Suppose the system is observable and both dynamic and measurement noise are bounded. Then, with high probability, direct policy updates using stochastic gradient descent with small stepsize converges linearly and globally (from any initial stabilizing policy) to the optimal steady-state Kalman gain._

More recently, the problem of learning the Kalman gain has been considered from a system identification perspective, for completely _unknown_ linear systems [31; 32; 33; 34]. In  and , subspace system identification methods are used to obtain error bounds for learning the Markov parameters of the model over a time horizon and establish logarithmic regret guarantee for output prediction error. Due to the inherent difficulty of learning a completely unknown stochastic system from partial observations, subspace methods assume marginal _stability of the unknown system_, and lead to sub-optimal sample complexity bounds that grow with the number of Markov parameters, instead of the number of unknowns [35; 14]. Alternatively,  considers minimizing the output prediction error and introduces a model-free policy gradient approach, under the same stability assumptions, that achieves _sublinear convergence rate_. This paper provides a middle ground between completely known and completely unknown systems, for a learning scenario that not only has relevant practical implications, but also utilizes the duality relationship to LQR to establish _linear convergence rates_ even for _unstable systems_ as long as they are _observable_. See the Appendix A for the discussion of additional related works [36; 37; 38].

## 2 Background and Problem Formulation

Herein, first we propose the model setup in detail and discuss the Kalman filter as the estimation strategy. Consider the discrete-time filtering problem given by the stochastic difference equations,

\[x(t+1)=Ax(t)+(t), y(t)=Hx(t)+(t), \]

where \(x(t)^{n}\) is the state of the system, \(y(t)^{m}\) is the observation signal, and \(\{(t)\}_{t}\) and \(\{(t)\}_{t}\) are the uncorrelated zero-mean random vectors, that represent the process and measurement noise respectively, with the following covariances,

\[[(t)(t)^{}]=Q^{n n}, [(t)(t)^{}]=R^{m  m},\]

for some positive (semi-)definite matrices \(Q,R 0\). Let \(m_{0}\) and \(P_{0} 0\) denote the mean and covariance of the initial condition \(x_{0}\).

In the filtering setup, the state \(x(t)\) is hidden, and the objective is to estimate it given the history of the observation signal \((t)=\{y(0),y(1),,y(t-1)\}\). The best linear mean-squared error (MSE)estimate of \(x(t)\) is defined according to

\[(t)=*{argmin}_{((t))}[\|x(t)-\|^{2}] \]

where \(((t))\) denotes the space of all linear functions of the history of the observation signal \((t)\). If the model parameters \((A,H,Q,R)\) are known, the optimal MSE estimate \((t)\) can be recursively computed by the Kalman filter algorithm :

\[(t+1) =A(t)+L(t)(y(t)-H(t)),(0)=m_{0}, \] \[P(t+1) =AP(t)A^{}+Q-AP(t)H^{}S(t)^{-1}HP(T)A^{ }, P(0)=P_{0}, \]

where \(S(t)=HP(t)H^{}+R\), \(L(t):=AP(t)H^{}S(t)^{-1}\) is the Kalman gain, and \(P(t):=[(x(t)-(t))(x(t)-(t))^{}]\) is the error covariance matrix.

**Assumption 1**.: _The pair \((A,H)\) is detectable, and the pair \((A,Q^{})\) is stabilizable, where \(Q^{}\) is the unique positive semidefinite square root of \(Q\)._

Under this assumption, the error covariance \(P(t)\) converges to a steady-state value \(P_{}\), resulting in a unique steady-state Kalman gain \(L_{}=AP_{}H^{}(HP_{}H^{}+R)^{-1}\)[39; 40]. It is common to evaluate the steady-state Kalman gain \(L_{}\) offline and use it, instead of \(L(t)\), to update the estimate in real-time. Furthermore, we note that the assumption of uncorrelated random vectors is sufficient to establish that Kalman filter provides the best _linear_ estimate of the states given the observations for minimizing the MSE criterion [2, Theorem 2].

### Learning problem

Now, we describe our learning setup: 1) The system matrices \(A\) and \(H\) are known, but the process and the measurement noise covariances, \(Q\) and \(R\), are _not_ available. 2) We have access to an oracle that generates independent realizations of the observation signal for given length \(T\): \(\{y(t)\}_{t=0}^{T}\). However, ground-truth measurements of the state \(x(t)\) is _not_ available.

_Remark 1_.: Our proposed learning setup arises in various important engineering applications where merely approximate or reduced-order linear models are available due to difficulty in analytically capturing the effect of complex dynamics or disturbances, hence represented by noise with unknown covariance matrices. Additionally, the system identification procedure often occurs through the application of physical principles and collection of data from experiments in a controlled environment (e.g., in a wind tunnel). However, identifying the noise covariance matrices strongly depends on the operating environment which might be significantly different than the experimental setup. Therefore, it is common engineering practice to use the learned system matrices and tune the Kalman gain to improve the estimation error. We refer to  for the application of this procedure for gust load alleviation in wings and  for estimation in chemical reactor models. We also emphasize that this learning setup has a rich history in adaptive filtering with numerous references with a recent survey on this topic . As part of our future research, we will carry-out a robustness analysis, similar to its LQR dual counterpart [43; 44], to study the effect of the error in system matrices on the learning performance.

Inspired by the structure of the Kalman filter, our goal is to learn the steady-state Kalman gain \(L_{}\) from the data described in the learning setup:

Given: independent random realizations of \(\{y(0),,y(T)\}\) with the parameters \(A,H\) Learn: steady-state Kalman gain \(L_{}\)

For that, we formulate the learning problem as a stochastic optimization described next.

### Stochastic optimization formulation

Define \(_{L}(T)\) to be the estimate given by the Kalman filter at time \(T\) realized by the constant gain \(L\). Rolling out the update law (3a) for \(t=0\) to \(t=T-1\), and replacing \(L(t)\) with \(L\), leads to the following expression for the estimate \(_{L}(T)\) as a function of \(L\),

\[_{L}(T)=A_{L}^{T}m_{0}+_{t=0}^{T-1}A_{L}^{T-t-1}Ly(t), \]where \(A_{L} A-LH\). Note that evaluating this estimate does not require knowledge of \(Q\) or \(R\). However, it is not possible to directly aim to learn the gain \(L\) by minimizing the MSE (2) because the ground-truth measurement of the state \(x(T)\) is not available. Instead, we propose to minimize the MSE in predicting the observation \(y(T)\) as a surrogate objective function:1

\[_{L}\,J_{T}^{}(L):=[\|y(T)-_{L}(T)\|^{2}] \]

where \(_{L}(T):=H_{L}(T)\). Note that while the objective function involves finite time horizon \(T\), our goal is to learn the steady-state Kalman gain \(L_{}\).

The justification for using the surrogate objective function in (5) instead of MSE error (2) lies in the detectability assumption 1. Detectability implies that all unobservable states are stable; in other words, their impact on the output signal vanishes quickly--depending on their stability time constant.

Numerically, the optimization problem (5) falls into the category of stochastic optimization and can be solved by algorithms such as Stochastic Gradient Descent (SGD). Such an algorithm would need access to independent realizations of the observation signal which are available. Theoretically however, it is not yet clear if this optimization problem is well-posed and admits a unique minimizer. This is the subject of following section where certain properties of the objective function, such as its gradient dominance and smoothness, are established. These theoretical results are then used to analyze first-order optimization algorithms and provide stability guarantees of the estimation policy iterates. The results are based on the duality relationship between estimation and control that is presented next.

## 3 Estimation-Control Duality Relationship

The stochastic optimization problem (5) is related to an LQR problem through the application of the classical duality relationship between estimation and control [45, Ch.7.5]. In order to do so, we introduce the adjoint system (dual to (1)) according to:

\[z(t)=A^{}z(t+1)-H^{}u(t+1), z(T)=a \]

where \(z(t)^{n}\) is the adjoint state and \((T):=\{u(1),,u(T)\}^{mT}\) are the control variables (dual to the observation signal \((T)\)). The adjoint state is initialized at \(z(T)=a^{n}\) and simulated _backward in time_ starting with \(t=T-1\). We introduce an LQR cost for the adjoint system:

\[J_{T}^{}(a,_{T}):=z^{}(0)P_{0}z(0)+_{t=1}^{ T}[z^{}(t)Q_{Z}(t)+u^{}(t)Ru(t)], \]

and formalize a relationship between linear estimation policies for the system (1) and linear control policies for the adjoint system (6). A linear estimation policy takes the observation history \(_{T}^{mT}\) and outputs an estimate \(_{}(T):=(_{T})\) where \(:^{mT}^{n}\) is a linear map. The adjoint of this linear map, denoted by \(^{}:^{n}^{mT}\), is used to define a control policy for the adjoint system (6) which takes the initial condition \(a^{n}\) and outputs the control signal \(_{^{}}=^{}(a)\); i.e.,

\[\{y(0),,y(T-1)\}}_{}(T) \{u(1),,u(T)\}^{}}a.\]

The duality relationship between optimal MSE estimation and LQR control is summarized in the following proposition. The proof is presented in the supplementary material.

**Proposition 1**.: _Consider the estimation problem for the system (1) and the LQR problem (7) subject to the adjoint dynamics (6). For any linear estimation policy \(_{}(T)=(_{T})\), and for any \(a^{n}\), we have the identity_

\[[|a^{}x(T)-a^{}_{}(T)|^{2} ]=J_{T}^{}(a,_{^{}}(T)), \]

_where \(_{^{}}(T)=^{}(a)\). In particular, for a Kalman filter with constant gain \(L\), the output prediction error (5)_

\[[\|y(T)-_{L}(T)\|^{2}]=_{i=1}^{m}J_{T}^{}(H_{i},_{L^{}}(T))+[R], \]

_where \(_{L^{}}(T)=\{L^{}z(1),L^{}z(2),,L^ {}z(T)\}\), i.e., the feedback control policy with constant gain \(L^{}\), and \(H_{i}^{}^{n}\) is the \(i\)-th row of the \(m n\) matrix \(H\) for \(i=1,,m\).__Remark 2_.: The duality is also true in the continuous-time setting where the estimation problem is related to a continuous-time LQR. Recent extensions to the nonlinear setting appears in  with a comprehensive study in . This duality is distinct from the maximum likelihood approach which involves an optimal control problem over the original dynamics instead of the adjoint system .

### Duality in steady-state regime

Using the duality relationship (9), the MSE in prediction (5) is expressed as:

\[J_{T}^{}(L)=[X_{T}(L)H^{}H]+ [R],\]

where \(X_{T}(L) A_{L}^{T}P_{0}(A_{L}^{})^{T}+_{t=0}^{T-1}A_{L} ^{t}(Q+LRL^{})(A_{L}^{})^{t}\). Define the set of Schur stabilizing gains

\[\{L^{n m}:(A_{L})<1\}.\]

For any \(L\), in the steady-state, the mean-squared prediction error assumes the form,

\[_{T}J_{T}^{}(L)=[X_{}(L)H^ {}H]+[R],\]

where \(X_{}(L):=_{T}X_{T}(L)\) and coincides with the unique solution \(X\) of the discrete Lyapunov equation \(X=A_{L}XA_{L}^{}+Q+LRL^{}\) (existence of unique solution follows from \((A_{L})<1\)). Given the steady-state limit, we formally analyze the following constrained optimization problem:

\[_{L}\; J(L)[X_{(L) }H^{}H], X_{(L)}=A_{L}X_{(L)}A_{L}^{ }+Q+LRL^{}. \]

_Remark 3_.: Note that the latter problem is technically the dual of the optimal LQR problem as formulated in  by relating \(A A^{}\), \(-H B^{}\), \(L K^{}\), and \(H^{}H\). However, the main difference here is that the product \(H^{}H\) may _not_ be positive definite, for example, due to rank deficiency in \(H\) specially when \(m<n\) (whereas \(()>0\) appears in all of the bounds in [25; 23]). Thus, in general, the cost function \(J(L)\) is not necessarily coercive in \(L\), which can drastically effect the optimization landscape. For the same reason, in contrast to the LQR case [25; 23], the gradient dominant property of \(J(L)\) is not clear in the filtering setup. In the next section, we show that such issues can be avoided as long as the pair \((A,H)\) is observable. Also, the learning problem posed here is distinct from its LQR counterpart (see Table 1).

### Optimization landscape

The first result is concerned with the behaviour of the objective function at the boundary of the optimization domain. It is known  that the set of Schur stabilizing gains \(\) is regular open, contractible, and unbounded when \(m 2\) and the boundary \(\) coincides with the set \(\{L^{n m}:(A-LH)=1\}\). For simplicity of presentation, we consider a slightly stronger assumption:

**Assumption 2**.: _The pair \((A,H)\) is observable, and the noise covariances \(Q 0\) and \(R 0\)._

**Lemma 1**.: _The function \(J(.):\) is real-analytic and coercive with compact sublevel sets; i.e.,_

\[L\|L\|\;\;J(L),\]

_and \(_{}\{L^{n m}:J(L)\}\) is compact and contained in \(\) for any finite \(>0\)._

The next result establishes the gradient dominance property of the objective function. While this result is known in the LQR setting ([25; 23]), the extension to the estimation setup is not trivial as \(H^{}H\), which takes the role of the covariance matrix of the initial state in LQR, may not be positive definite (instead, we only assume \((A,H)\) is observable). This, apparently minor issue, hinders establishing the gradient dominated property globally. However, we recover this property on every sublevel sets of \(J\) which is sufficient for the subsequent convergence analysis.

**Lemma 2**.: _Consider the constrained optimization problem (10).Then,_

* _The explicit formula for the gradient of_ \(J\) _is:_ \( J(L)=2Y_{(L)}(-LR+A_{L}X_{(L)}H^{}),\) _where_ \(Y_{(L)}=Y\) _is the unique solution of_ \(Y=A_{L}^{}YA_{L}+H^{}H\)_._
* _The global minimizer_ \(L^{*}=*{arg\,min}_{L}J(L)\) _satisfies_ \(L^{*}=AX^{*}H^{}(R+HX^{*}H^{})^{-1},\) _with_ \(X^{*}\) _being the unique solution of_ \(X^{*}=A_{L^{*}}X^{*}A_{L^{*}}^{}+Q+L^{*}R(L^{*})^{}\)* _The function_ \(J(.):_{}\)_, for any non-empty sublevel set_ \(_{}\) _for some_ \(>0\)_, satisfies the following inequalities; for all_ \(L,L^{}_{}\)_:_ \[c_{1}[J(L)-J(L^{*})]+c_{2}\|L-L^{*}\|_{F}^{2}  J(L), J(L),\] (11a) \[c_{3}\|L-L^{*}\|_{F}^{2}  J(L)-J(L^{*}),\] (11b) \[\| J(L)- J(L^{})\|_{F} \,\|L-L^{}\|_{F},\] (11c) _for positive constants_ \(c_{1},c_{2},c_{3}\) _and_ \(\) _that are only a function of_ \(\) _and independent of_ \(L\)_._

Note that the expression for the gradient is consistent with Proposition 3.8 in  after applying the duality relationship explained in Remark 3.

_Remark 4_.: The proposition above implies that \(J(.)\) has the Polyak-Lojasiewicz (PL) property (aka gradient dominance) on every \(_{}\); i.e., for any \(L_{}\) we have \(J(L)-J(L^{*})()} J(L), J(L)\). The inequality (11a) is more general as it characterizes the dominance gap in terms of the iterate error from the optimality. This is useful in obtaining the iterate convergence results in the next section. Also, the Lipschitz bound resembles its "dual" counterpart in [23, Lemma 7.9], however, it is _not_ implied as a simple consequence of duality because \(H^{}H\) may not be positive definite.

## 4 SGD for Learning the Kalman Gain

In order to emphasize on the estimation time horizon \(T\) for various measurement sequences, we use \(_{T}:=\{y(t)\}_{t=0}^{T}\) to denote the measurement time-span. Note that, any choice of \(L\) corresponds to a filtering strategy that outputs the following prediction,

\[_{L}(T)=HA_{L}^{T}m_{0}+_{t=0}^{T-1}HA_{L}^{T-t-1}Ly(t).\]

We denote the squared-norm of the estimation error for this filtering strategy as,

\[(L,_{T})\|e_{T}(L)\|^{2},\]

where \(e_{T}(L) y(T)-_{L}(T)\). We also define the _truncated_ objective function as

\[J_{T}(L):=[(L,_{T})],\]

where the expectation is taken over all possible random measurement sequences, and note that, at the steady-state limit, we obtain \(_{T}J_{T}(L)=J(L)\).

The SGD algorithm aims to solve this optimization problem by replacing the gradient, in the Gradient Descent (GD) update, with an unbiased estimate of the gradient in terms of samples from the measurement sequence. In particular, with access to an oracle that produces independent realization of the measurement sequence, say \(M\) random independent measurements sequences \(\{_{T}^{}\}_{i=1}^{M}\), the gradient can be approximated as follows: denote the approximated cost value

\[_{T}(L)_{i=1}^{M}(L,_{T}^{}),\]

then the approximate gradient with batch-size \(M\) is \(_{T}(L)=_{i=1}^{M}_{L}(L, _{T}^{})\). This forms an unbiased estimate of the gradient of the "truncated objective", i.e., \([_{T}(L)]= J_{T}(L)\). Next, for implementation purposes, we compute the gradient estimate explicitly in terms of the measurement sequence and the filtering policy \(L\).

**Lemma 3**.: _Given \(L\) and a sequence of measurements \(=\{y(t)\}_{t=0}^{T}\), we have,_

\[_{L}(L,)=-2H^{}e_{T}(L)y(T- 1)^{}\] \[+2_{t=1}^{T-1}-(A_{L}^{})^{t}H^{}e_{T}(L)y (T-t-1)^{}+_{k=1}^{t}(A_{L}^{})^{t-k}H^{}e_{T} (L)y(T-t-1)^{}L^{}(A_{L}^{})^{k-1}H^{}.\]

Finally, using this approximate gradient, the so-called SGD update proceeds as,

\[L_{k+1}=L_{k}-_{k}_{L}_{T}(L),\]

for \(k\), where \(_{k}>0\) is the step-size. Numerical results of the application of this algorithm appears in Appendix G.

_Remark 5_.: Computing this approximate gradient only requires the knowledge of the system matrices \(A\) and \(H\), and does _not_ require the noise covariance information \(Q\) and \(R\). Simulation results for the SGD algorithm are provided in the supplementary material.

Although the convergence of the SGD algorithm is expected to follow similar to the GD algorithm under the gradient dominance condition and Lipschitz property, the analysis becomes complicated due to the possibility of the iterated gain \(L_{k}\) leaving the sub-level sets. It is expected that a convergence guarantee would hold under high-probability due to concentration of the gradient estimate around the true gradient. The complete analysis in this direction is provided in the subsequent sections.

We first provide sample complexity and convergence guarantees for SGD with a biased estimation of gradient for locally Lipschitz objective functions and in presence of stability constraint \(\). Subsequently, we study the stochastic problem of estimating the gradient for the estimation problem. Distinct features of our approach as compared with similar formulations in the literature are highlighted in Table 1.

We now provide a road map to navigate the technical results that concludes with Theorem 3: Section 4.1 is concerned with the convergence analysis of the SGD algorithm under an assumption for the biased gradient oracle, summarized in Theorem 1 which concludes the linear convergence of the iterates for sufficiently small stepsize. Section 4.2 is concerned with the bias-variance error analysis of the gradient estimate, summarized in Theorem 2 providing the sufficient values for the batch-size and trajectory length that guarantees the desired bound on the gradient oracle required in Theorem 1. Finally, combining the results from Theorem 1 and Theorem 2 concludes our main result in Theorem 3.

### SGD with biased gradient and stability constraint

First, we characterize the "robustness" of a policy at which we aim to estimate the gradient. This is formalized in the following lemma which is a consequence of [30, Lemma IV.1].

**Lemma 4**.: _Consider any \(L\) and let \(Z\) be the unique solution of \(Z=A_{L}ZA_{L}^{}+\) for any \( 0\). Then, \(L+\) for any \(^{n m}\) satisfying \(0\|\|_{F}()2\, {}(Z)\|H\|\)._

Second, we provide a uniform lowerbound for the stepsize of gradient descent for an approximated direction "close" to the true gradient direction.

**Lemma 5** (Uniform Lower Bound on Stepsize).: _Let \(L_{0}_{}\) for some \(^{*} J(L^{*})\), and choose any finite \(\). Consider any direction \(E\) such that \(\|E- J(L_{0})\|_{F}\| J(L_{0})\|_{F}\) for some \(\), then we have \(J(L_{0}- E)\) for any \(\) satisfying:_

\[0}+()}{()[-^{*}]}}.\]

_Remark 6_.: Note that for the case of exact gradient direction, i.e. when \(E= J(L_{0})\), we have \(=0\) and choosing \(=\) implies the known uniform bound of \(\) for feasible stepsizes as expected. Also, by this choice of \(\), this guarantees that the next iterate remains in sublevel set \(_{}\). This lemma generalizes this uniform bound for general directions and (potentially) larger sublevel set.

   Problem &  &  &  \\   & cost value & \(Q\) and \(R\) & \(A\) and \(H\) & stability \(\) & model & biased \\  LQR  & known & known & unknown & yes & \([J(L+r)]\) & yes \\  & & & & & & \( U(^{m})\) & \\ Estimation (this work) & unknown & unknown & known & yes & \([(L,)]\) & yes \\  & & & & & & \(\)output data & \\ Vanila SGD & * & * & * & no & \([(L,)]\) & no \\  & & & & & & \(\)data dist. & \\   

Table 1: Differences between SGD algorithms for optimal LQR and optimal estimation problemsThe next proposition provides a decay guarantee for one iteration of gradient descent with an approximate direction which will be used later for convergence of SGD with a biased gradient estimate.

**Proposition 2** (Linear Decay in Cost Value).: _Suppose \(L_{0}_{}\) for some \(>0\) and a direction \(E 0\) is given such that \(\|E- J(L)\|_{F}\| J(L)\|_{F}\) for some \(<1\). Let \(}{{(+1)^{2}()}}\). Then, \(L_{1} L_{0}-E\) remains in the same sublevel set, i.e., \(L_{1}_{}\). Furthermore, we obtain the following linear decay of the cost value:_

\[J(L_{1})-J(L^{*})[1-c_{1}()(1-)/2][J(L_{0 })-J(L^{*})].\]

Next, we guarantee that SGD algorithm with this biased estimation of gradient obtains a linear convergence rate outside a small set \(_{}\) around optimality defined as

\[_{}\{L\;\;\| J(L)\|_{F} s _{0}/\},\]

for some \((0,1)\) and arbitrarily small \(s_{0}>0\). First, we assume access to the following oracle that provides a biased estimation of the true gradient.

**Assumption 3**.: _Suppose, for some \(>0\), we have access to a biased estimate of the gradient \((L)\) such that, there exists constants \(s,s_{0}>0\) implying \(\|(L)- J(L)\|_{F} s\| J(L)\|_{F}+s_{0}\) for all \(L_{}_{}\)._

**Theorem 1** (Convergence).: _Suppose Assumption 3 holds with small enough \(s\) and \(s_{0}\) such that \(s/2\) and \(_{}_{/2}\) is non-empty for some \((0,1)\). Then, SGD algorithm starting from any \(L_{0}_{}_{/2}\) with fixed stepsize \(()}\) generates a sequence of policies \(\{L_{k}\}\) that are stable (i.e. each \(L_{k}_{}\)) and cost values decay linearly before entering \(_{/2}\); i.e.,_

\[J(L_{k})-J(L^{*})[1-c_{1}()(1-)/2]^{k} [J(L_{0})-J(L^{*})],\]

_for each \(k 0\) unless \(L_{j}_{/2}\) for some \(j k\)._

_Remark 7_.: By combining Theorem 1 and the PL property of the cost in (11a), we immediately obtain a sample complexity for our algorithm; e.g., choose \(=1/2\), then in order to guarantee an \(\) error on the cost \(J(L_{k})-J(L^{*})\), it suffices to have a small enough bias term \(s_{0}()}}{4}\) and variance coefficient \(s\) for the oracle, and run the SGD algorithm for \(k>()/(1-()}{18()}))\) steps.

### Observation model for the estimation problem

Herein, first we show that the estimation error and its differential can be characterized as a "simple norm" of the concatenated noise (Proposition 3). This norm is induced by a metric that encapsulates the system dynamics which is explained below. Before proceeding to the results of this section, we assume that both the process and measurement noise are bounded:

**Assumption 4**.: _Assume that (almost surely) \(\|x_{0}\|\), \(\|(t)\|_{}\) and \(\|(t)\|_{}\) for all \(t\). Also, for simplicity, suppose the initial state has zero mean, i.e., \(m_{0}=_{n}\)._

For two vectors \(,^{(T+1)n}\), we define

\[,_{_{L}} {tr}[^{}_{L}^{}H^{}H _{L}],\]

where \(_{L}A_{L}^{0}&A_{L}^{1}&&A_{L}^{T} .\) Also, define \(_{L}[E](M_{0}[E]&M_{1}[E]&&M_{T}[E])\) with \(M_{0}[E]=0\), \(M_{1}[E]=EH\) and \(M_{i+1}[E]=_{k=0}^{i}A_{L}^{i-k}EHA_{L}^{k}\) for \(i=1,,T-1\).

**Proposition 3**.: _The estimation error \((L,_{T})\) takes the following form_

\[(L,_{T})=\|_{}\|_{_{L}}^ {2},\]

_where \(_{L}-(I L)\) with \(^{}=((T-1)^{}\;\;\;(0)^{}\;x(0) ^{})\) and \(^{}=((T-1)^{}\;\;\;(0)^{ }\;\;_{m}^{})\,.\) Furthermore, its differential acts on small enough \(E^{n m}\) as,_

\[(,_{T})_{L}(E)=-2 _{L},(I E)_{_{L}}+ [_{L}_{L}[E]],\]

_where \(_{L}_{L}_{L}^{}\) and \(_{L}[E]_{L}[E]^{}H^{}H _{L}+_{L}^{}H^{}H_{L}[E]\)._Now, we want to bound the error in the estimated gradient \(_{T}(L)\) by considering the concentration error (on length \(T\) trajectories) and truncation error separately as follows:

\[\|_{T}(L)- J(L)\|\|_{T}(L)- J _{T}(L)\|+\| J_{T}(L)- J(L)\|,\]

recalling that \(J_{T}(L)=[(L,_{T})]\) by definition.

Next, we aim to provide the analysis of concentration error on trajectories of length \(T\) with probability bounds. However, for any pair of real \((T T)\)-matrices \(M\) and \(N\), by Cauchy Schwartz inequality we obtain that \(|[MN]|\|M\|_{F}\|N\|_{F}\|M\|\|N\|_{F}\). This bound becomes loose (in terms of dimension \(T\)) as the condition number of \(N\) increases2.

Nonetheless, we are able to provide concentration error bounds that "scale well with respect to the length \(T\)" that hinges upon the following idea: from von Neumann Trace Inequality (49, Theorem 8.7.6) one obtains,

\[|[MN]|_{i=1}^{T}_{i}(M)_{i}(N) \|M\|\|N\|_{*}, \]

where \(\|N\|_{*}[}N}]=_{i} _{i}(N)\) is the _nuclear norm_ with \(_{i}(N)\) denoting the \(i\)-th largest singular value of \(N\). Additionally, the same inequality holds for non-square matrices of appropriate dimension which is tight in terms of dimension.

**Proposition 4** (Concentration Error Bound).: _Consider \(M\) independent length \(T\) trajectories \(\{_{T}^{i}\}_{i=1}^{M}\) and suppose Assumption 4 holds. Let \(_{T}(L)_{i=1}^{M}(L,_{T}^{i})\), then for any \(s>0\),_

\[[\|_{T}(L)- J_{T}(L)\| s] 2 n[/2}{_{L}^{2}+2_{L}s/3}],\]

_where \(_{L} 4_{L}^{2}C_{L}^{3}\|H\|^{2}\|H\|_{*}/[1- )}]\)3 with \(_{L}=_{}+\|L\|_{}\)._

We can also show how the truncation error decays linearly as \(T\) grows, with constants that are independent of the system dimension \(n\):

**Proposition 5** (Truncation Error Bound).: _Under Assumption 4, the truncation error is bounded:_

\[\| J(L)- J_{T}(L)\|_{L}\ )}^{T+1},\]

_where \(_{L} 10_{L}^{4}C_{L}^{6}\|H\|^{2}\|H\|_{*}/ [1-(A_{L})]^{2}\)._

_Remark 8_.: Notice how the trajectory length \(T\) determines the bias in the estimated gradient. However, the concentration error bound is independent of \(T\) and only depends on the noise bounds proportionate to \(_{}^{4}\), \(_{}^{4}\) and the stability margin of \(A_{L}\) proportionate to \(C_{L}^{6}/(1-)})^{6}\).

Finally, by combining the truncation bound in Proposition 5 with concentration bounds in Proposition 4 we can provide probabilistic bounds on the "estimated cost" \(_{T}(L)\) and the "estimated gradient" \(_{T}(L)\). Its precise statement is deferred to the supplementary materials (Theorem 5).

### Sample complexity of SGD for Kalman Gain

Note that the open-loop system may be unstable. Often in learning literature, it is assumed that the closed-loop system can be contractible ( i.e., the spectral norm \(\|A_{L}\|<1\)) which is quite convenient for analysis, however, it is not a reasonable system theoretic assumption. Herein, we emphasize that we only require the close-loop system to be Schur stable, meaning that \((A_{L})<1\); yet, it is very well possible that the system is not contractible. Handling systems that are merely stable requires more involved system theoretic tools that are established in the following lemma.

**Lemma 6** (Uniform Bounds for Stable Systems).: _Suppose \(L\), then there exit a constants \(C_{L}>0\) such that_

\[\|A_{L}^{k}\| C_{L}\ )}^{k+1}, k 0.\]

_Furthermore, consider \(_{}\) for some \(>0\), then there exist constants \(D_{}>0\), \(C_{}>0\) and \(_{}(0,1)\) such that \(\|L\| D_{},\ C_{L} C_{},\) and \((A_{L})_{}\) for all \(L_{}\)._The following result provides sample complexity bounds for this stochastic oracle to provide a biased estimation of the gradient that satisfies our oracle model of SGD analysis in Assumption 3.

**Theorem 2**.: _Under the premise of Proposition 4, consider \(_{}\) for some \(>0\) and choose any \(s,s_{0}>0\) and \((0,1)\). Suppose the trajectory length \(T(_{}/s_{0})/(1/ })\) and the batch size \(M 4_{}^{2}(n,m)(2n/)/(s\,s_{0})^{2},\) where \(_{} 10(_{}+D_{}_{})^{4}C_{ }^{6}\,\|H\|^{2}\|H\|_{*}\) and \(_{} 5C_{}^{3}\|H\|^{2}\|H\|_{*}(_{}+D_{} _{})^{2}/[1-}]^{3}\). Then, with probability no less than \(1-\), Assumption 3 holds._

Proof.: First, note that for any \(L_{}\) by Lemma 6 we have that \(_{L}_{}\) and \(_{L}_{}\). Then, note that the lower bound on \(T\) implies that \(_{}}^{T+1} s_{0}\). The claim then follows by applying Theorem 5 and noting that for any \(L_{}\) the gradient is lowerbound as \(\| J(L)\|>s_{0}/\). 

**Theorem 3**.: _Consider the linear system (1) under Assumptions 2 and 4. Suppose the SGD algorithm is implemented with initial stabilizing gain \(L_{0}\), the step-size \())},\) for \(k\) iterations, a batch-size of \(M\), and data-length \(T\). Then, \(>0\) and with probability larger than \(1-\), \(J(L_{k})-J(L^{*})\) if_

\[T O(()), M O(()(())), k O(()). \]

Proof of Theorem 3.: Recall that according to Remark 7, in order to obtain \(\) error on the cost value we require \(k O((1/))\) of SGD algorithm using gradient estimates with small enough bias term \(s_{0}()}}{4}\) and variance coefficient \(s<\). Now, we can guarantee Assumption 3 holds for such \(s_{0}\) and \(s\) (with probability at least \(1-\)) by invoking Theorem 2. In fact, it suffices to ensure that the length of data trajectories \(T\) and the batch size \(M\) are large enough; specifically, \(T O((1/s_{0}))=O((1/))\) and by using union bound for bounding the failure probability \(M O((1/)(k)/s_{0}^{2})=O((1/)((1/)) /)\). 

## 5 Conclusions, Broader Impact, and Limitations

In this work, we considered the problem of learning the optimal (steady-state) Kalman gain for linear systems with unknown process and measurement noise covariances. Our approach builds on the duality between optimal control and estimation, resulting in a direct stochastic Policy Optimization (PO) algorithm for learning the optimal filter gain. We also provided convergence guarantees and finite sample complexity with bias and variance error bounds that scale well with problem parameters. In particular, the variance is independent of the length of data trajectories and scales logarithmically with problem dimension, and bias term decreases exponentially with the length.

This work contributes a generic optimization algorithm and introduces a filtering strategy for estimating dynamical system states. While theoretical, it raises privacy concerns similar to the model-based Kalman filter. Limitations include the need for prior knowledge of system parameters; nonetheless, parameter uncertainties can be treated practically as process and measurement noise. Finally, sample complexities depend on the stability margin \(1-)}\), inherent to the system generating the data.

Finally, a direction for future research is to study how to adapt the proposed algorithm and error analysis for the setting when a single long trajectory is available as opposed to several independent trajectories of finite length. Another interesting direction is to carry out a robustness analysis, similar to its LQR dual counterpart, to study the effect of the error in system parameters on the policy learning accuracy. Of course, the ultimate research goal is to use the recently introduced duality in nonlinear filtering  as a bridge to bring tools from learning to nonlinear filtering.