# Towards a Scalable Reference-Free Evaluation of Generative Models

Azim Ospanov

aospanov9@cse.cuhk.edu.hk

Department of Computer Science & Engineering, The Chinese University of Hong Kong, Hong Kong

Jingwei Zhang

jwzhang22@cse.cuhk.edu.hk

Mohammad Jalali\({}^{*}\)

mjalali24@cse.cuhk.edu.hk

&Xuenan Cao

xuenancao@cuhk.edu.hk

&Andrej Bogdanov

abogdano@uottawa.ca

&Farzan Farnia

farnia@cse.cuhk.edu.hk

Department of Computer Science & Engineering, The Chinese University of Hong Kong, Hong Kong

###### Abstract

While standard evaluation scores for generative models are mostly reference-based, a reference-dependent assessment of generative models could be generally difficult due to the unavailability of applicable reference datasets. Recently, the reference-free entropy scores, VENDI  and RKE , have been proposed to evaluate the diversity of generated data. However, estimating these scores from data leads to significant computational costs for large-scale generative models. In this work, we leverage the random Fourier features framework to reduce the computational price and propose the _Fourier-based Kernel Entropy Approximation (FKEA)_ method. We utilize FKEA's approximated eigenspectrum of the kernel matrix to efficiently estimate the mentioned entropy scores. Furthermore, we show the application of FKEA's proxy eigenvectors to reveal the method's identified modes in evaluating the diversity of produced samples. We provide a stochastic implementation of the FKEA assessment algorithm with a complexity \(O(n)\) linearly growing with sample size \(n\). We extensively evaluate FKEA's numerical performance in application to standard image, text, and video datasets. Our empirical results indicate the method's scalability and interpretability applied to large-scale generative models. The codebase is available at [https://github.com/aziksh-ospanov/FKEA](https://github.com/aziksh-ospanov/FKEA).

## 1 Introduction

A quantitative comparison of generative models requires evaluation metrics to measure the quality and diversity of the models' produced data. Since the introduction of variational autoencoders (VAEs) , generative adversarial networks (GANs) , and diffusion models  that led to impressive empirical results in the last decade, several evaluation scores have been proposed to assess generative models learned by different training methods and architectures. Due to the key role of evaluation criteria in comparing generative models, they have been extensively studied in the literature.

While various statistical methods have been applied to measure the fidelity and variety of a generative model's produced data, the standard scores commonly perform a reference-based evaluation of generative models, i.e., they quantify the characteristics of generated samples in comparison to a reference distribution. The reference distribution is usually chosen to be either the distribution ofsamples in the test data partition or a comprehensive dataset containing a significant fraction of real-world sample types, e.g. ImageNet  for evaluating image-based generative models.

To provide well-known examples of reference-dependent metrics, note that the distance scores, Frechet Inception Distance (FID)  and Kernel Inception Distance (KID) , are explicitly reference-based, measuring the distance between the generative and reference distributions. Similarly, the standard quality/diversity score pairs, Precision/Recall [9; 10] and Density/Coverage , perform the evaluation in comparison to a reference dataset. Even the seemingly reference-free Inception Score (IS)  can be viewed as implicitly reference-based, since it quantifies the variety and fidelity of data based on the labels and confidence scores assigned by an ImageNet pre-trained neural net, where ImageNet implicitly plays the role of the reference dataset. The reference-based nature of these evaluation scores is desired in many instances including standard image-based generative models, where either a sufficiently large test set or a comprehensive reference dataset such as ImageNet is available for the reference-based evaluation.

On the other hand, a reference-based assessment of generative models may not always be feasible, because the selection of a reference distribution may be challenging in a general learning scenario. For example, in prompt-based generative models where the data are created in response to a user's input text prompts, the generated data could follow an a priori unknown distribution depending on the specific distribution of the user's input prompts. Figure 1 shows one such example where we compare reference-based diversity scores of regular and colored elephant image samples generated by Stable Diffusion XL . While the diversity of the colored images looks significantly higher to the human eye, the evaluated reference-based FID, Recall, and Coverage metrics do not suggest a higher diversity. As this example suggests, a proper reference-based evaluation of every user's generated data would require a distinct reference dataset, which may not be available to the user during the assessment time. Moreover, finding a comprehensive text or video dataset to choose as the reference set would be more difficult compared to image data, because the higher length of text and video samples could significantly contribute to their variety, requiring an inefficiently large reference set to cover all text or video sample types.

The discussed challenging scenarios of conducting a reference-based evaluation highlight the need for reference-free assessment methods that remain functional in the absence of a reference dataset. Recently, entropy-based diversity evaluation scores, the VENDI metric family [1; 14] and \(\) score , have been proposed to address the need for reference-free assessment metrics. These scores calculate the entropy of the eigenvalues of a kernel similarity matrix for the generated data. Based on the theoretical results in , the evaluation process of these scores can be interpreted as an unsupervised identification of the generative model's produced sample clusters, followed by the entropy calculation for the frequencies of the detected clusters. In Figure 1, we observe that the

Figure 1: Reference-based vs. reference-free scores on two datasets of Stable Diffusion XL generated elephant images. FID, Recall, and Coverage scores (colored orange) are reference-based, whereas VENDI and RKE scores (colored blue) are reference-free. Inception.V3 is used as the backbone embedding. Reference-based metrics use ‘Indian elephant’ samples in ImageNet as reference data.

reference-free \(\) and \(\) scores grow when the generated samples are colored, which is due to the increase in the quantity of identified clusters in the colored case.

While the \(\) and \(\) entropy scores provide reference-free assessments of generative models, estimating these scores from generated data could incur significant computational costs. In this work, we show that computing the precise \(\) and \(\) scores would require at least \((n^{2})\) and \((n^{2.373})\)4 computations for a sample size \(n\), respectively. While the randomized projection methods in  can reduce the computational costs to \(O(n^{2})\) for a general \(_{}\) score, the quadratic growth would be a barrier to the method's application to large \(n\) values. Although the computational expenses could be reduced by limiting the sample size \(n\), an insufficient sample size would lead to significant error in estimating the entropy scores. As an example on the ImageNet dataset, Figure 7 in the Appendix shows the adverse effects of limiting the sample size on the quality of clusters used in the calculation of the \(\) scores.

To overcome the challenges of computing the scores, we leverage the random Fourier features (RFFs) framework  and develop a scalable entropy-based evaluation method that can be efficiently applied to large sample sizes. Our proposed method, _Fourier-based Kernel Entropy Approximation (FKEA)_, is designed to approximate the kernel covariance matrix using the RFFs drawn from the Fourier transform-inverse of a target shift-invariant kernel. We prove that using a Fourier feature size \(r=}\), FKEA computes the eigenspace of the kernel matrix within an \(\)-bounded error. Furthermore, we demonstrate the application of the eigenvectors of the FKEA's proxy kernel matrix for identifying the sample clusters used in the reference-free evaluation of entropic diversity.

Finally, we present numerical results of the entropy-based evaluation of standard generative models using the baseline eigendecomposition and our proposed FKEA methods. In our experiments, the baseline spectral decomposition algorithm could not efficiently scale to sample sizes above a few ten thousand. On the other hand, our stochastic implementation of the FKEA method could scalably apply to large sample sizes. Utilizing the standard embeddings of image, text, and video data, we tested the FKEA assessment while computing the sample clusters and their frequencies in application to large-scale datasets and generative models. Here is a summary of our work's main contributions:

* Characterizing the computational complexity of the kernel entropy scores of generative models,
* Developing the Fourier-based FKEA method to approximate the kernel covariance eigenspace and entropy of generated data,
* Proving guarantees on FKEA's required size of random Fourier features indicating a complexity logarithmically growing with the dataset size,
* Providing numerical results on FKEA's reference-free assessment of large-scale image,text, video-based datasets and generative models.

## 2 Related Work

**Evaluation of deep generative models.** The assessment of generative models has been widely studied in the literature. The existing scores either quantify a distance between the distributions of real and generated data, as in FID  and KID  scores, or attempt to measure the quality and diversity of the trained generative models, including the Inception Score , quality/diversity metric pairs Precision/Recall  and Density/Coverage . The mentioned scores are reference-based, while in this work we focus on reference-free metrics. Also, we note that the evaluation of memorization and novelty has received great attention, and several scores including the authenticity score , the feature likelihood divergence , and the rarity score  have been proposed to quantify the generalizability and novelty of generated samples. Note that the evaluation of novelty and generalization is, by nature, reference-based. On the other hand, our study focuses on the diversity of data which can be evaluated in a reference-free way as discussed in .

**Role of embedding in quantitative evaluation.** Following the discussion in , we utilize DinoV2  image embeddings in most of our image experiments, as 's results indicate DinoV2 can yield scores more aligned with the human notion of diversity. As noted in , it is possible to utilize other non-ImageNet feature spaces such as CLIP  and SwAV  as opposed to InceptionV3  to further improve metrics such as FID. In this work, we mainly focus on DinoV2 feature space, while we note that other feature spaces are also compatible with entropy-based diversity evaluation.

**Diversity assessment for text-based models.** To quantify the diversity of text data, the n-gram-based methods are commonly used in the literature. A well-known metric is the BLEU score , which is based on the geometric average of n-gram precision scores times the Brevity Penalty. To adapt BLEU score to measure text diversity,  proposes the Self-BLEU score, calculating the average BLEU score of various generated samples. To further isolate and measure diversity, N-Gram Diversity scores [28; 29; 30] were proposed and defined by a ratio between the number of unique n-grams and overall number of n-grams in the text. Other prominent metrics include Homogenization (ROUGE-L) , FBD  and Compression Ratios .

**Kernel PCA, Spectral Cluttering, and Random Fourier Features**. Kernel PCA  is a well-studied method of dimensionality reduction that utilizes the eigendecomposition of the kernel matrix, similar to the kernel-based diversity evaluation methods in [1; 2]. The related papers [35; 36] study the connections between kernel PCA and spectral clustering. Also, the analysis of random Fourier features  for performing scalable kernel PCA has been studied in [37; 38; 39; 40; 41]. We note that while the mentioned works characterize the complexity of estimating the eigenvectors, our analysis focuses on the complexity of computing the kernel matrix's eigenvalues via Fourier features, as we primarily seek to quantify the diversity of generated data using the kernel matrix's eigenvalues.

## 3 Preliminaries

Consider a generative model \(\) generating random samples \(_{1},,_{n}^{d}\) following the model's probability distribution \(P_{}\). In our analysis, we assume the \(n\) generated samples are independently drawn from \(P_{}\). Note that in VAEs  and GANs , the generative model \(\) is a deterministic function \(G:^{r}^{d}\) mapping an \(r\)-dimensional latent random vector \( P_{Z}\) from a known distribution \(P_{Z}\) to \(G()\) distributed according to \(P_{}\). On the other hand, in diffusion models, \(\) represents an iterative random process that generates a sample from \(P_{}\). The goal of a sample-based diversity evaluation of generative model \(\) is to quantify the variety of its generated data \(_{1},,_{n}\).

### Kernel Function, Kernel Covariance Matrix, and Matrix-based Renyi Entropy

Following standard definitions, \(k:^{d}^{d}\) is called a kernel function if for every integer \(n\) and set of inputs \(_{1},,_{n}^{d}\), the kernel similarity matrix \(K=k(_{i},_{j})_{n n}\) is positive semi-definite. We call a kernel function \(k\) normalized if for every input \(\) we have \(k(,)=1\). A well-known example of a normalized kernel function is the Gaussian kernel \(k_{(^{2})}\) with bandwidth parameter \(^{2}\) defined as

\[k_{(^{2})},^{}\,: \,=\,--^{}\|_{2}^{2} }{2^{2}}\]

For every kernel function \(k\), there exists a feature map \(:^{d}^{m}\) such that \(k(,^{})=(),(^{ })\) is the inner product of the \(m\)-dimensional feature maps \(()\) and \((^{})\). Given a kernel \(k\) with feature map \(\), we define the kernel covariance matrix \(C_{X}^{m m}\) of a distribution \(P_{X}\) as

\[C_{X}\,:\,=\,_{ P_{X}}()( )^{}\,=\, p_{X}()()( )^{}\]

The above matrix \(C_{X}\) is positive semi-definite with non-negative values. Furthermore, assuming a normalized kernel \(k\), it can be seen that the eigenvalues of \(C_{X}\) will add up to \(1\) (i.e., it has unit trace \((C_{X})=1\)), providing a probability model. Therefore, one can consider the entropy of \(C_{X}\)'s eigenvalues as a quantification of the diversity of distribution \(P_{X}\) based on the kernel similarity score \(k\). Here, we review the general family of Renyi entropy used to define \(\) and \(\) scores.

**Definition 1**.: _For a positive semi-definite matrix \(C_{X}^{m m}\) with eigenvalues \(_{1},,_{m}\), the order-\(\) Renyi entropy \(H_{}(C_{X})\) for \(>0\) is defined as_

\[H_{}(C_{X})\,:\,=\,_{i=1}^{m}_{ i}^{}\,\]To estimate the entropy scores from finite empirical samples \(_{1},,_{n}\), we consider the empirical kernel covariance matrix \(_{X}\) defined as \(_{X}\,:=\,_{i=1}^{n}_{i} _{i}^{}\). This matrix provides an empirical estimation of the population kernel covariance matrix \(C_{X}\).

It can be seen that the \(m m\) empirical matrix \(_{X}\) and normalized kernel matrix \(K=k(_{i},_{j})_{n n}\) share the same non-zero eigenvalues. Therefore, to compute the matrix-based entropy of the empirical covariance matrix \(_{X}\), one can equivalently compute the entropy of the eigenvalues of the kernel similarity matrix \(K\). This approach results in the definition of the \(\) and \(\) diversity scores:  defines the family of \(\) scores as

\[_{}(_{1},,_{n})\,:=\,\! (H_{}K)\,=\,(_{i=1}^{n} _{i}^{})^{},\]

where \(_{1},,_{n}\) denote the eigenvalues of the kernel matrix \(K\). Also,  proposes the \(\) score, which is the special order-2 Renyi entropy, \((_{1},,_{n})=(H_{2}(K))\). To compute \(\) without computing the eigenvalues,  points out the RKE score reduces to the Frobenius norm \(\|\|_{F}\) of the kernel matrix as follows:

\[(_{1},,_{n})\,=\,K _{F}^{-2}\,=\,\,}_{i=1}^{n}_{j=1}^{n}k _{i},_{j}^{2}\,^{-1}\]

### Shift-Invariant Kernels and Random Fourier Features

A kernel function \(k\) is called shift-invariant, if there exists a function \(:^{d}\) such that \(k(,^{})=(-^{})\) for every \(,^{}^{d}\). Bochner's theorem proves that a function \(:^{d}\) will lead to a shift-invariant kernel similarity score \((-^{})\) between \(,^{}\)_if and only if_ its Fourier transform \(:^{d}\) is non-negative everywhere (i.e, \(() 0\) for every \(\)). Note that the Fourier transform \(\) is defined as

\[()\,:=\,}( )\!(-i^{})\!\]

Specifically, Bochner's theorem shows the Fourier transform \(\) of a normalized shift-invariant kernel \(k(,^{})=(-^{})\), where \((0)=1\), will be a probability density function (PDF). The framework of random Fourier features (RFFs)  utilizes independent samples drawn from PDF \(\) to approximate the kernel function. Here, given independent samples \(_{1},,_{r}\), we form the following proxy feature map \(_{r}:^{d}^{2r}\)

\[_{r}()\,=\,}( {}_{1}^{}),(_{1}^{}),,(_{r}^{}),(_{r}^{}). \]

As demonstrated in [16; 42], the \(2r\)-dimensional proxy map \(_{r}\) can approximate the kernel function as \(k(,^{})=_{}(^{})(^{}^{})+(^{}) (^{}^{})_{r}()^{}_{r}(^{})\).

## 4 Computational Complexity of VENDI & RKE Scores

As discussed, computing \(\) and general \(_{}\) scores requires computing the order-\(\) entropy of kernel matrix \(K\). Using the standard definition of \(\)-norm \(\|\|_{}=_{i=1}^{n}|v_{i}|^{}^{1/}\), we observe that the computation of \(_{}\) score is equivalent to computing the \(\)-norm \(\|\|_{}\) of the \(n\)-dimensional eigenvalue vector \(=[_{1},,_{n}]\) where \(_{1}_{n}\) are the sorted eigenvalues of the normalized kernel matrix \(K\).

In the following theorem, we prove that except order \(=2\), which is the \(\) score, computing any other \(_{}\) score is at least as expensive as computing the product of two \(n n\) matrices. Therefore, the theorem suggests that the computational complexity of every member of the VENDI family is lower-bounded by \((n^{2.372})\) which is the least known cost of multiplying \(n n\) matrices.

In the theorem, we suppose \(\) is any fixed set of "basis" functions. A circuit \(\) is a directed acyclic graph each of whose internal nodes is labeled by a _gate_ coming from a set \(\). A subset of gates are designated as outputs of \(\). A circuit with \(n\) source nodes and \(m\) outputs computes a function from \(^{n}\) to \(^{m}\) by evaluating the gate at each internal gate in topological order. The size of a circuit is the number of gates. Also, \(\) is the basis consisting of the gradients of all functions in \(\). We will provide the proof of the theorems in the Appendix.

**Theorem 1**.: _If \(_{}(K)\) for \( 2\) is computable by a circuit \(\) of size \(s(n)\) over basis \(\), then \(n n\) matrices can be multiplied by a circuit \(\) of size \(O(s(n))\) over basis \(\{+,\}\)._

**Remark 1**.: _The smallest known circuits for multiplying \(n n\) matrices have size \((n^{})\), where \( 2.372\). Despite tremendous research efforts only minor improvements have been obtained in recent years. There is evidence that \(\) is bounded away from 2 for certain classes of circuits . In contrast, \(_{2}\) is computable in quadratic time \((n^{2})\) in the basis \(B=\{,+,\}\)._

The above discussion indicates that except the \((_{1},,_{n})\), i.e. order-2 Renyi entropy, whose computational complexity is quadratically growing with sample size \((n^{2})\), the other members of the VENDI family \(_{}\) would have a super-quadratic complexity on the order of \((n^{2.372})\). In practice, the computation of \(_{}\) scores is performed by the eigendecomposition of the \(n n\) kernel matrix that requires \(O(n^{3})\) computations for precise computation and \(O(n^{2}M)\) computations using a randomized projection onto an \(M\)-dimensional space .

## 5 A Scalable Fourier-based Method for Computing Kernel Entropy Scores

As we showed earlier, the complexity of computing \(\) and \(\) scores are at least quadratically growing with the sample size \(n\). The super-linear growth of the scores' complexity with sample size \(n\) can hinder their application to large-scale datasets and generative models with potentially hundreds of sample types. In such cases, a proper entropy estimation should be performed over potentially hundreds of thousands of data, where the quadratic complexity of the scores would be a significant barrier toward their accurate estimation.

Here, we consider a shift-invariant kernel matrix \(k(,^{})=(-^{})\) where \(()=1\) and propose applying the random Fourier features (RFF) framework  to perform an efficient approximation of the RKE and VENDI scores. To do this, we utilize the Fourier transform \(\) that, according to Bochner's theorem, is a valid PDF, and we independently generate \(_{1},,_{r}}}{{ }}\). Note that in the case of the Gaussian kernel \(k_{(^{2})}\), the corresponding PDF will be an isotropic Gaussian \((,}I_{d})\) with zero mean and covariance matrix \(}I_{d}\). Then, we consider the RFF proxy feature map \(_{r}:^{d}^{2r}\) as defined in (1) and define the proxy kernel covariance matrix \(_{X,r}^{2r 2r}\).

\[_{X,r}\,=\,_{i=1}^{n}\,_{r} _{i}\,_{r}_{i}^{} \]

Note that the \(2r 2r\) matrix \(_{X,r}\) has the same non-zero eigenvalues as the \(n n\) RFF proxy kernel matrix \(_{r}\), and therefore can be utilized to approximate the eigenvalues of the original \(n n\) kernel matrix \(K\). Therefore, we propose the _Fourier-based Kernel Entropy Approximation (FKEA)_ method to approximate the \(\) and \(_{}\) scores as follows:

\[_{1},,_{n}\,=\, \!H_{2}(_{X,r})\,=\,_{X,r }_{F}^{-2}, \]

\[_{}_{1},,_{n} \,=\,\!H_{}(_{X,r})\,=\, _{i=1}^{2r}\,_{r,i}^{}^{} \]

Note that in the above, \(_{r,i}^{}\) denotes the \(i\)th eigenvalue of the \(2r 2r\) matrix \(_{X,r}\). We remark that the computation of both \(\) and \(_{}\) can be done by a stochastic algorithm which computes the proxy covariance matrix (2) by summing the sample-based \(2r 2r\) matrix terms, and then computing the resulting matrix's Frobenius norm for \(\) score or all the \(2r\) matrix's eigenvalues for a general \(_{}\) with \( 2\). Algorithm 1 presents the steps of the FKEA method where the computation needed for the proxy kernel covariance matrix is \(O(n)\) and grows only linearly with sample size \(n\).

Therefore, to show the FKEA method's scalability, we need to bound the required RFF size \(2r\) for an accurate approximation of the original \(n n\) kernel matrix. The following theorem proves that the needed feature size will be \(}\) for an \(\)-accurate approximations of the matrix's eigenspace.

**Theorem 2**.: _Consider a shift-invariant kernel \(k(,^{})=(-^{})\) where \(()=1\). Suppose \(_{1},,_{r}\) are independently drawn from PDF \(\). Let \(_{1}_{n}\) be the sorted eigenvalues of the normalized kernel matrix \(K=k(_{i},_{j})_{n n}\). Also, consider the eigenvalues of \(_{1}_{2r}\) of random matrix \(_{X,r}\) with their corresponding eigenvectors \(}_{1},,}_{2r}\). Let \(_{j}=0\) for every \(j>2r\). Then, for every \(>0\), the following holds with probability at least \(1-\):_

\[^{n}_{i}-_{i}^{2}} \,\,}^{n}K}_{i}-_{i}}_{i}_{2}^{2}}\,\,},\]

_where \(}_{i}:=_{j=1}^{r}}_{2 j}^{}_{i}}_{2j}+ }_{2j-1}^{}_{i}}_{2j-1}\) is the ith proxy eigenvector for \(K\)._

**Corollary 1**.: _In the setting of Theorem 2, the following approximation guarantees hold for \(\) and \(_{}\) scores_

* _For every_ \(_{}\) _with_ \( 2\)_, including_ \(\) _for_ \(=2\)_, the following dimension-independent bound holds with probability at least_ \(1-\)_:_ \[_{}_{1}, ,_{n}^{}}\,-\,_{ }_{1},,_{n}^{}\,\,}\]
* _For every_ \(_{}\) _with_ \(1<2\)_, assuming a finite dimension for the kernel feature map_ \(()=m\)_, the following bound holds with probability at least_ \(1-\)_:_ \[_{}_{1}, ,_{n}^{}}\,-\,_{ }_{1},,_{n}^{}\,\,m^{-}}\]

**Remark 2**.: _According to the theoretical results in , the top-\(t\) eigenvectors of kernel covariance matrix \(C_{X}\) will correspond to the mean of the modes of a mixture distribution with \(t\) well-separable modes. Theorem 2 shows for every \(1 i 2r\), FKEA provides the proxy score function \(_{i}:^{d}\) that assigns a likelihood score for an input \(\) to belong to the \(i\)th identified mode:_

\[_{i}()\,=\,_{j=1}^{r}}_{2j}^{}}_{2j,i}+ }_{2j-1}^{}}_{2j-1,i} \]

_Therefore, one can compute the above FKEA-based score for each of the \(2r\) eigenvectors over a sample set, and use the samples with the highest scores according to every \(_{i}\) to characterize the \(i\) sample cluster captured by the FKEA method._

## 6 Numerical Results

We evaluated the FKEA method on several image, text, and video datasets to assess its performance in quantifying diversity in different domains. In the experiments, we computed the empirical covariance matrix of \(2r\)-dimensional Fourier features with a Gaussian kernel with bandwidth parameter \(\) tuned for each dataset, and then applied FKEA approximation for the \(_{1}\), \(_{1.5}\), and the \(\) (same as \(_{2}\)) scores. An algorithm to compute these scores is presented in Algorithm 1. Experiments were conducted on RTX3090 GPUs. We interpreted the modes identified by FKEA entropy-based diversity evaluation using the eigenvectors of the proxy covariance matrix as discussed in Remark 2. For each eigenvector, we presented the training data with maximum eigenfunction values corresponding to the eigenvector.

**Time Complexity of FKEA metrics**. To highlight the computational advantages of transitioning to FKEA, Table 1 presents a comparison of the metric computations for VENDI and RKE on the ImageNet dataset, with sample sizes ranging from 10k to 250k. Our results show that VENDI and RKE become computationally intractable due to memory overflow. In contrast, the FKEA method efficiently scales up to \(n=250k\) samples, maintaining optimal computational time.

**Experimental Results on Image Data**. To investigate the FKEA method's diversity evaluation in settings where we know the ground-truth clusters and their quantity, we simulated an experiment on the colored MNIST  data with the images of 10 colored digits as shown in Figure 2. We evaluated the FKEA approximations of the diversity scores while including samples from \(t\) digits for \(t\{1,,10\}\). The plots in Figure 2 indicate the increasing trend of the scores and FKEA's tight

    &  &  \\   Metric & n=10k & n=20k & n=30k & n=40k & n=100k & n=250k & n=10k & n=20k & n=30k & n=40k & n=50k & n=100k & n=250k \\  FKEA-RKE & 7 & 16 & 25 & 34 & 43 & 87 & 238 & 37 & 78 & 120 & 162 & 203 & 433 & 1138 \\ FEA-VENDI & 11 & 19 & 27 & 37 & 45 & 104 & 267 & 48 & 89 & 130 & 173 & 213 & 459 & 1236 \\ RKE & 217 & 1324 & 4007 & - & - & - & - & 218 & 1330 & 4021 & - & - & - & - & - \\ VENDI & 286 & 1774 & 5488 & - & - & - & - & - & 287 & 1780 & 5502 & - & - & - & - \\   

Table 1: Time complexity for FKEA and non-FKEA based metrics (RKE and VENDI) on ImageNet dataset with _DinoV2_ embedding. Computation of VENDI and RKE on 40k+ samples are omitted due to memory overflow during metric computation.

Figure 2: RFF-based identified clusters used in FKEA Evaluation in single-colored MNIST  dataset with _pixel_ embedding, Fourier feature dimension \(2r=4000\) and bandwidth \(=7\). The graphs indicate increase in FKEA RKE/VENDI diversity metrics with increasing number of labels.

approximations of the scores. Also, we show the top-20 training samples with the highest scores according to the top-10 FKEA eigenvectors, showing the method captured the ground-truth clusters.

We conducted an experiment on the ImageNet data to monitor the scores' behavior evaluated for 50k samples from an increasing number of ImageNet labels. Figure 3 shows the increasing trend of the scores as well as the top-9 samples representing the top-4 identified clusters used for the entropy calculation. Also, Figure 4 presents the FKEA approximated entropy scores with different truncation factors in StyleGAN3  on 30k generated data for each truncation factor, showing the increasing diversity scores with the truncation factor. We defer discussing the results on AFHQ , MS-COCO , F-MNIST  datasets to the Appendix.

    &  &  &  &  &  \\  Burkina Faso & 34\% & Argentina & 77\% & Azerbaijan & 100\% & Cambodia & 94\% & Belarus & 100\% & Bolivia & 97\% \\ Benin & 23\% & Chile & 23\% & Afghanistan & 6\% & & Ecuador & 3\% \\ Chad & 22\% & & & & & & & & \\ Burundi & 13\% & & & & & & & & \\ Cameroon & 8\% & & & & & & & & \\   

Table 2: Top 5 synthetic countries dataset modes with _text-embedding-3-large_ embedding, Fourier features dim \(2r=8000\) and Gaussian Kernel bandwidth \(=0.9\). The table summarises the mentions of each country in the top 100 paragraphs identified for the eigenvectors corresponding to each mode.

Figure 4: FKEA metrics behavior under different truncation factor \(\) of StyleGAN3  generated FFHQ samples.

Figure 3: RFF-based identified clusters used in FKEA Evaluation in ImageNet dataset with _DinoV2_ embedding, Fourier feature dimension \(2r=16k\) and Gaussian Kernel bandwidth \(=25\). The graphs indicate increase in FKEA diversity metrics with increasing number of labels per 50k samples.

**Experimental Results on Text and Video Data**. To perform experiments on the text data with known clustering ground-truth, we generated 500,000 paragraphs using GPT-3.5  about 100 randomly selected countries (5k samples per country). In the experiments, the text embedding used was _text-embedding-3-large_[52; 53; 51]. We evaluated the diversity scores over data subsets of size 50k with different numbers of mentioned countries. Figure 5 shows the growing trend of the diversity scores when including more countries. The figure also shows the countries mentioned in the top-6 modes provided by FKEA-based principal eigenvectors, which shows the RFF-based clustering of countries correlates with their continent and geographical location. We discuss the numerical results on non-synthetic text datasets, Wikipedia, CNN/Dailymail , CMU Movie Corpus , in the Appendix.

For video data experiments, we considered two standard video datasets, UCF101  and Kinetics-400 . Following the video evaluation literature [59; 60], we used the I3D pre-trained model  as embedding, which maps a video sample to a 1024-dimensional vector. As shown in Figure 6, increasing the number of video classes of test samples led to an increase in the FKEA approximated diversity metrics. Also, while the samples identified for the first identified cluster look broad, the next modes seemed more specific. We discuss the results of the Kinetics dataset in the Appendix.

## 7 Conclusion

In this work, we proposed the Fourier-based FKEA method to efficiently approximate the kernel-based entropy scores \(_{}\) and \(\) scores. The application of FKEA results in a scalable reference-free evaluation of generative models, which could be utilized in applications where no reference data is available for evaluation. A future direction to our work is to study the sample complexity of the matrix-based entropy scores and the FKEA's approximation under high-dimensional kernel feature maps, e.g. the Gaussian kernel. Also, analyzing the role of feature embedding in the method's application to text and video data would be interesting for future exploration.

Figure 5: FKEA diversity metrics with the increasing number of countries in the synthetic dataset.

Figure 6: RFF-based identified clusters used in FKEA evaluation in UCF101 dataset with _I3D_ embedding. The graphs indicate an increase in FKEA diversity metrics with more classes.