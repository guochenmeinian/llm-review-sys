# TAPTRv2: Attention-based Position Update Improves Tracking Any Point

Hongyang Li\({}^{1,2}\)   Hao Zhang\({}^{2,3}\)   Shilong Liu\({}^{2,4}\)   Zhaoyang Zeng\({}^{2}\)

**Feng Li\({}^{2,3}\)   Tianhe Ren\({}^{2}\)   Bohan Li\({}^{5}\)   Lei Zhang\({}^{1,2}\)**

\({}^{1}\)South China University of Technology.

\({}^{2}\)International Digital Economy Academy (IDEA).

\({}^{3}\)The Hong Kong University of Science and Technology.

\({}^{4}\)Dept. of CST., BNRist Center, Institute for AI, Tsinghua University.

\({}^{5}\)Shanghai Jiao Tong University.

Corresponding author.

###### Abstract

In this paper, we present TAPTRv2, a Transformer-based approach built upon TAPTR for solving the Tracking Any Point (TAP) task. TAPTR borrows designs from DEtection TRansformer (DETR) and formulates each tracking point as a point query, making it possible to leverage well-studied operations in DETR-like algorithms. TAPTRv2 improves TAPTR by addressing a critical issue regarding its reliance on cost-volume, which contaminates the point query's content feature and negatively impacts both visibility prediction and cost-volume computation. In TAPTRv2, we propose a novel attention-based position update (APU) operation and use key-aware deformable attention to realize. For each query, this operation uses key-aware attention weights to combine their corresponding deformable sampling positions to predict a new query position. This design is based on the observation that local attention is essentially the same as cost-volume, both of which are computed by dot-production between a query and its surrounding features. By introducing this new operation, TAPTRv2 not only removes the extra burden of cost-volume computation, but also leads to a substantial performance improvement. TAPTRv2 surpasses TAPTR and achieves state-of-the-art performance on many challenging datasets, demonstrating the superiority.

## 1 Introduction

Tracking any point (TAP) in videos is a more fine-grained task compared to tracking objects using bounding boxes  or their instance masks . As point correspondence and its visibility prediction in long video sequence is fundamental to many downstream applications, such as augmented reality, 3D reconstruction, and visual imitation , TAP has received increasing attention in the past few years .

Some works solve TAP from the 3D perspective , where they learn an underlying 3D representation of the scene and enable it to transform over time. Although such an approach has obtained impressive results, the learning of the 3D representation is nontrivial and challenging. Thus most methods are not general and have to be fine-tuned for each video.

To develop a more general solution while keeping a good performance, some methods  solve the TAP task in 2D space directly. Building upon existing optical flow methods , especially RAFT , such methods jointly estimate optical flow and point visibility across multiple frames. Supplemented with temporal processing methods such as sliding windows, they achieve remarkable results. However, these methods are largely affected by previous optical flow estimation methods and model each tracking point as a concatenation of multiple features, including point flow vector, point flow embedding, point visibility, point content feature, and local correlation as cost volume . These features normally have clear physical meanings in optical flow, but are simply concatenated and sent as a blackbox vector to MLPs or Transformers and expect MLPs or Transformers to decipher and utilize the features . Such a black box modeling not only makes the model cluttered, but also hinders its optimization and learning efficiency.

To more effectively utilize the features, TAPTR takes inspiration from DEtection TRansformer (DETR)  and models each tracking point as a point query as in DETR with a content part and a positional part (point coordinates). Each query is refined layer by layer, with its visibility predicted by its updated content feature. Point queries exchange information through spatial or temporal attention in the same frame or along the temporal dimension. Such a point query formulation not only makes the TAP pipeline conceptually simple, but also lead to a remarkable performance.

However, despite its demonstrated performance improvement, TAPTR still relies on the cost-volume feature and has a questionable design, which concatenates the cost-volume feature of a point query and its content part, followed with an MLP transformation (See Eq. 4 in ). As after each Transformer decoder layer, the updated point query needs to predict a relative position to update the query's coordinates, aggregating cost-volume, which is a local correlation information, to the query's content part helps the point query predict a more accurate position. However, aggregating cost-volume also contaminates the query's content part, which has two negative impacts. First, the cross-attention operation in each Transformer decoder layer needs to compute attention maps, which are the similarities between point queries and image feature keys2. Yet queries and keys have different formulations. While both queries and keys have their content part and positional part, queries are contaminated by cost-volume whereas keys are not. Such a difference makes the attention computation implausible. Second, a contaminated point query also yields to inaccurate cost-volume as the computation of cost-volume also needs to compare the point query with its local image features. The experiments in TAPTR show that, with such contaminated cost-volumes, the performance will suffer a big drop. Moreover, the incorporation of cost-volume in TAPTR not only results in redundant computations, but also leaves the simplicity one step behind query-based object detection methods . This raises several intriguing questions: Why is cost-volume necessary? Is there any alternative that can be developed without redundant effort? How can the cost-volume or its alternative be better utilized without contaminating a point query?

With this motivation, we propose TAPTRv2. Compared to TAPTR, TAPTRv2 does not aggregate cost-volume to queries to avoid contaminating their content features. Meanwhile, with a deeper

Figure 1: Comparison of the frameworks among previous works, TAPTR, and TAPTRv2. Inspired by DETR-based detection algorithms, TAPTR formulates the point tracking problem as a detection problem and simplifies the overall pipeline to a well-studied DETR-like framework. After introducing the attention-based position update operation into Transformer decoder layers, the overall pipeline is further simplified to be as straightforward as detection methods. The operations within dashed boxes are executed only once.

analysis recognizing the importance of the information captured by cost-volume, we propose a novel Attention-based Position Update (APU) operation, which, for each query, uses its local attention weights to combine its local relative positions to predict a new query position. Such an operation is equivalent to a cross-attention operation from a point query (Q) to image features (K) using local attention, but the values are local relative positions (V) instead of image features. This design is based on the observation that local attention is essentially the same as cost-volume, both of which are computed by dot-production between a query and its surrounding features. By introducing this new operation, the TAP framework is further simplified in TAPTRv2, which not only removes the extra burden of cost-volume computation, but also yields a substantial performance improvement.

In our implementation, we follow TAPTR and adopt deformable attention for its proven efficiency and effectiveness in DETR-based detection algorithms. However, as deformable attention directly predicts attention weights for a query without comparing the query with image features, we use its variant, key-aware deformable attention  which computes attention weights by explicitly comparing a query with image features. Our ablation studies show that key-aware deformable attention is indeed more effective as it precisely matches the design of attention-based position update.

As shown in Fig. 1, with the help of our analysis and our simple yet effective designs, TAPTRv2 is much simpler and clearer than previous methods. To further verify the superiority of TAPTRv2 brought by our clear point query design, we conduct experiments on several TAP datasets, TAPTRv2 achieves the best performance on all of the datasets.

## 2 Related Work

**Optical Flow Estimation.** Optical flow is a long-standing problem in computer vision, which has attracted a great amount of research [15; 1; 2] over the past few decades. Particularly, in the last decade, deep learning-based methods [10; 17; 47; 37; 42; 19; 46; 53; 16; 35; 55] have demonstrated a strong advantage in this field. DCFlow  was the first to verify the feasibility of using cost-volume to address the optical flow problem. The robustness of cost-volume has enabled many subsequent works [37; 42; 39] and dominated this field. However, optical flow estimation methods can only handle flow estimation between two frames, which prevents them from utilizing long-term temporal information to improve accuracy. More importantly, in the presence of occlusions, optical flow methods often suffer from the problem of tracking target change. These issues make it challenging for optical flow estimation methods to process videos directly.

**Tracking Any Point.** The TAP task is defined to estimate the flow of any point between any two consecutive frames and predict the visibility of the tracked point in every frame in the entire video. Some works [44; 45; 36] aim to address the TAP task by constructing a time-varying 3D field. Due to the difficulty of learning a 4D field, such methods have to retrain their network to fit each video, which is normally too slow and impractical for many applications. Given the similarities between TAP and optical flow, most current methods [14; 56; 7; 9; 20] follow the optical flow methods, especially RAFT , but extend to multi-frame scenarios. By contrast, TAPTR  takes inspiration from Transformer-based object detection algorithms and models point tracking as a point detection problem, which makes TAP conceptually simple and leads to a remarkable performance improvement.

## 3 TAPTRv2

### Overview

As shown in Fig. 2, TAPTRv2 shares a similar architecture to DETR-based object detection. More specifically, its point query bears a strong resemblance to queries designed for visual prompt-based object detection [23; 18]. Thus TAPTRv2 mainly consists of three parts, image feature preparation, point query preparation, and target point detection. To process videos of dynamic lengths, we follow previous works [14; 20; 9; 7; 26] and utilize the sliding window strategy, which divides a video into windows of lengths \(W\) and processes \(W\) frames in parallel once at a time. Since TAPTRv2 is built upon TAPTR, to make this section self-contained, we will first provide a brief overview of the TAPTR framework and then describe how TAPTRv2 improves TAPTR.

**Image Feature Preparation.** Our method is orthogonal to any vision backbones. In this work, we use ResNet-50 as our backbone as it is the most widely used backbone for fair comparison in DETR-related research works . After obtaining multi-scale image feature maps from the image backbone, we send them into a Transformer-encoder to further enhance the features as well as the receptive fields of image features. After that, each frame \(X_{t}\) is ended up with a set of high-quality multi-scale image feature maps \(F_{t*}\).

**Point Query Preparation.** Considering general TAP application scenarios, each tracking point has its unique start frame and initial position. We define their initial locations as \(l_{e}=\{l_{e^{i}}^{i}\}_{i=1}^{N}\), where \(N\) is the number of points to be tracked, \(e^{i}\) indicates the start frame ID when the \(i\)-th tracking point first emerges or starts to be tracked. Similar to the visual prompt-based detection methods , TAPTRv2 needs to prepare a visual feature to describe each target tracking point. Following previous methods , without loss of generality, for the \(i\)-th target tracking point, its initial feature \(f_{e}^{i}\) can be obtained by conducting bilinear interpolation on the multi-scale feature maps of its start frame \(F_{e^{i}}\) at its initial position \(l_{e^{i}}^{i}\). Then the sampled results are transformed using an MLP to fuse multi-scale information. Since the tracking of a target point across a video can be treated as detecting the target point in every frame of the video. Following the formulation of object queries in DETR-based object detection methods, for every video frame, each point query consists of a content part and a positional part, i.e. \(Q_{t}^{i}=(f_{t}^{i},l_{t}^{i})\), which are initialized with the prepared initial feature and location of its corresponding target tracking point

\[ 1 i N, 1 t T,Q_{t}^{i}=(f_{t}^{i},l_{t}^{i}) (f_{e}^{i},l_{e}^{i}). \]

**Target Point Detection in Every Frame.** After preparing the image features of every frame and every point query in each frame, the TAP task can be clearly formulated as point detection. Taking the \(t\)-th frame for example, we treat its image features \(F_{t}\) as keys and values, the point queries \((f_{t},l_{t})\) as queries, and send them to a sequence of Transformer decoder layers. In every Transformer decoder layer, both the content part and positional part of the point queries will be refined. After the multi-layer refinement, the final positional part \(l_{t}^{{}^{}}\) of each point query is treated as the predicted position of its corresponding target tracking point in the \(t\)-th frame. Meanwhile, the content part is used to predict the visibility of the tracking point using an MLP-based visibility classifier

\[v_{t}=(f_{t}^{{}^{}}). \]

**Window Post-Processing.** After obtaining the detection result of all point queries in a window, each tracking point's trajectory and visibility states in this window can be updated. To proceed with the next window, we use the predicted tracking point positions and their corresponding content features in the last frame of the current window to initialize point queries in the next window. This simple strategy effectively propagates the latest prediction result to the next window.

Figure 2: The overview of TAPTRv2. The image feature preparation part and the point query preparation part prepare the image features of each frame of an input video and the point queries for each tracking point in every frame. The target point detection part takes the prepared image features and point queries as input. For every frame, each point query aims to predict the position and visibility of its target point.

### Analysis of Cost Volume Aggregation in TAPTR Decoder

TAPTR regards cost-volume as indispensable and adds extra cost-volume aggregation blocks before sending point queries to Transformer decoder layers. The extra block for cost-volume not only contaminates the point queries' content feature but also makes the pipeline complex as in Fig 3 (a).

**Cost Volume Aggregation.** Taking the \(i\)-th point query \(Q^{i}_{t}\) in the \(t\)-th frame as an example, TAPTR conducts dot-production between \(Q^{i}_{t}\) and the image feature maps \(F_{t}\) of the \(t\)-th frame to obtain the point query's cost-volume \(C^{i}_{t}\). With the help of grid sampling, TAPTR obtains the sampled cost vector \(c^{i}_{t}\) from \(C^{i}_{t}\) around the location of the point query \(l^{i}_{t}\).

**Contaminating Content Feature.** After obtaining \(c^{i}_{t}\), it is fused into the point query's content feature \(f^{i}_{t}\) through an MLP

\[^{i}_{t}((f^{i}_{t},c^{i} _{t})), \]

where Cat denotes concatenation along the channel dimension, \(^{i}_{t}\) indicates the contaminated content feature. Although such a fusion makes use of the cost volume, the point query's content feature, which is expected to describe its target tracking point's visual feature, is contaminated. The contamination will further affect the calculation of cost volume in the next layer, preventing TAPTR from using more accurate cost-volume. The ablation study in TAPTR verifies that, if TAPTR updates the cost volume in every decoder layer, the performance will drop significantly.

**Cost-volume Necessity Analysis.** Although the use of cost-volume leads to a questionable feature contamination problem, cost-volume still contributes to the performance greatly in TAPTR. To understand the reason why cost-volume is necessary, we conduct an ablation study on TAPTR. As shown in Table 1, we remove the self-attention, temporal-attention, and cost-volume components from TAPTR's decoder, and add them one by one and observe their impact on the performance of in-domain and out-of-domain datasets. The results show that both self-attention and temporal-attention bring significant improvement on both in-domain and out-of-domain datasets. However, while cost-volume also brings a significant improvement on the out-of-domain dataset, it leads to a slightly negative effect (0.2 AJ drop) on the in-domain dataset. This contradictory result indicates that cost-volume is only essential for mitigating the domain gap and enhancing the generalization capability of the model. This is quite reasonable because cost-volume is essentially the information of similarities between features, which is why it is called correlation map in some works . Due to the domain gap, the features learned by a TAP model can hardly be generalized to out-of-domain datasets. In comparison, the correlation information is more robust to domain changes as it captures the similarity information between local features. This motivates us to design a more effective approach to utilizing cost-volume, which we find is equivalent to attention weight in essence.

### Cross Attention with Attention-based Position Update

According to our analysis in Sec. 3.2, the effectiveness of cost-volume comes from its robust deep feature similarity, which is also in essence equivalent to how attention weights are computed. To leverage this insight, we still choose the deformable operation for its computational efficiency in using

    & Self & Temporal & Cost &  &  \\ Row & Attention & Attention & Volume & AJ & \(<^{x}_{avg}\) & OA & AJ & \(<^{x}_{avg}\) & OA \\ 
1 & ✗ & ✗ & ✗ & 47.4 & 62.2 & 82.5 & 79.7 & 87.8 & 94.3 \\ 
2 & ✓ & ✗ & ✗ & 50.6 (\(\)3.2) & 64.5 & 85.7 & 83.7 (\(\)4.0) & 90.8 & 95.7 \\
3 & ✗ & ✓ & ✗ & 54.3 (\(\)6.9) & 68.3 & 87.0 & 83.4 (\(\)2.7) & 90.6 & 96.5 \\
4 & ✗ & ✗ & ✓ & 52.0 (\(\)4.6) & 66.3 & 84.7 & 79.5 (\(\)0.2) & 87.9 & 94.6 \\   

Table 1: We start with a baseline (Row 1) without using self-attention, temporal-attention, and cost-volume, and add each component from TAPTR in turn to show their impact on in-domain and out-of-domain datasets. The addition of self-attention and temporal attention leads to a significant improvement on both the in-domain and out-of-domain datasets. However, the addition of cost-volume only leads to a significant improvement on the out-of-domain dataset but a negative impact on the in-domain dataset, showing that the importance of cost-volume mainly comes from its ability to mitigate the domain gap. Note that the in-domain evaluation set is created by rendering additional 150 videos using the same setting as the training set.

multi-scale image features, but replace its attention prediction with key-aware attention prediction, which is called key-aware deformable attention .

**Key-Aware Deformable Attention Revisiting.** Deformable attention directly predicts the attention weights for a query without comparing the query with image features. While this design is proven effective in object detection, it is inappropriate for TAP, as we want to leverage the attention weights as a replacement of cost-volume. Using key-aware deformable attention meets this need. Taking \(Q_{t}^{i}\) as an example, key-aware deformable attention can be formulated as

\[ S_{t}^{i}=W^{S} f_{t}^{i},K_{t}^{i}=V_{t}^{i}= (F_{t},l_{t}^{i}+S_{t}^{i}),\\ Q_{t}^{i}=f_{t}^{i},A_{t}^{i}=f_{t}^{i} K_{t}^{i}, f _{t}^{i}=(A_{t}^{i}/) V_{t}^{i}\\ f_{t}^{i} f_{t}^{i}+ f_{t}^{i}, \]

where \(S_{t}^{i}\) denotes the sampling offsets, \(Q_{t}^{i}\), \(K_{t}^{i}\), \(V_{t}^{i}\) and \(A_{t}^{i}\) indicate the query, key, value, and attention weights inside the attention mechanism, respectively, \(W^{S}\) is a learnable parameter, \(d\) is the number of key channels, \(\) indicates the bilinear interpolation, \( f_{t}^{i}\) is the update of content feature. Note that, for notation simplicity, we assume there is only one attention head and \(F_{t}\) has only one scale.

**Attention-based Position Update.** Since the attention weights \(A_{t}^{i}\) in Eq. 4 reflect the similarity between the point query \(Q_{t}^{i}\) and the sampled image features (K), the attention weights and their corresponding sampling offsets imply where the target tracking point is in the current frame. Thus we combine the sampling offsets using the computed attention weights to obtain a position update, and the update will be used to update the location of the point query. This is exactly a (sparse) cross-attention operation, in which the sampling offsets are values (V). Note that to update the content part of the point query, there is another cross-attention operation, in which the sampled image features are values (V). These two cross-attention operations can use the same attention weights. However, we empirically find that the sharing of attention weights for content and position update is detrimental to model optimization. We guess the update of content and position may need different distribution of the attention weights (e.g. more spiked or more smooth). Thus, we introduce an MLP to work as a Disentangler to disentangle the weights required for content and position update. The process can

Figure 3: Comparison of the decoder layer in TAPTR (a) and TAPTRv2 (b). In TAPTR (a), cost-volume aggregation will contaminate the content feature, affecting cross-attention and leading to the contaminated cost-volume in the next layer. In TAPTRv2 (b), with the introduction of Attention-based Position Update (APU) in cross attention, not only the attention weights are properly used to update the position of each point query and mitigate the domain gap, but also the content feature of each point query is kept uncontaminated, which is crucial for visibility prediction. We use an RGB image to represent the multi-scale feature maps for better visualization.

be formulated as

\[ l_{t}^{i}=((A_{t}^{i}/ )) S_{t}^{i}, \]

\[l_{t}^{i} l_{t}^{i}+ l_{t}^{i},\]

where \( l_{t}^{i}\) indicates the position update. Thanks to the separation of cost-volume from the content feature, the content feature can be kept clean, which leads to more accurate point visibility prediction as evidenced in Table 2. Meanwhile, our proposed attention-based position update operation deliberately utilizes attention weights as an equivalent form of cost-volume to perform position update, which effectively helps mitigate the domain gap problem.

## 4 Experiments

We conduct extensive experiments on multiple challenging evaluation datasets collected from real world to verify the superiority of TAPTRv2. Detailed ablation studies for our main contribution are also provided to show the effectiveness of each design in modeling.

### Datasets and Evaluation Settings

**Datasets.** Following previous works [26; 20; 14; 9] we train TAPTRv2 on the Kubric dataset, which consists of 11,000 synthetic videos generated by Kubric Engine . In each video of Kubric, Kubric Engine simulates a set of rigid objects falling down the floor from the air and bouncing. In each video, 2,048 points on the surface of background and moving objects are randomly sampled to generate point trajectories for training. During training, for training efficiency, the resolution of the videos is resized to 512\(\)512, and we randomly select 700-800 trajectories for training from each video. We evaluate our method on the challenging TAP-Vid-DAVIS  and TAP-Vid-Kinetics  datasets. Both datasets are from TAP-Vid  and are collected from real world and annotated by well-trained annotators. TAP-Vid-DAVIS has 30 challenging videos with complex motions and large-scale changes of the objections. TAP-Vid-Kinetics has over 1,000 YouTube videos, and the camera shaking and complex environment make it also a challenging dataset.

**Evaluation Metrics and Settings.** For evaluation, we follow the metrics proposed in TAP-Vid , including Occlusion Accuracy (OA) which describes the accuracy of classifying whether the target tracking points are visible or occluded, \(<_{avg}^{x}\) which reflects the average precision of the predicted tracking points' location at thresholds of 1,2,4,8,16 pixels, and Average Jaccard (AJ) which is a comprehensive metric to measure the overall performance of a point tracker from the perspective of both location and visibility classification. Meanwhile, there are two evaluation modes to accommodate online and offline trackers. The "Strided" mode is for offline trackers. The "First" mode is for online trackers and is much harder. In this paper, without specification, we evaluate our method on the "First" mode, and to facilitate comparisons with offline methods, we follow previous methods [20; 26] to further report our performance on TAP-Vid-DAVIS dataset in the "Stride" mode. Note that, since

    &  &  &  \\ Method & AJ & \(<_{avg}^{x}\) & OA & AJ & \(<_{avg}^{x}\) & OA & AJ & \(<_{avg}^{x}\) & OA \\  PIPs  & – & – & – & 42.0 & 59.4 & 82.1 & 31.7 & 53.7 & 72.9 \\ TAP-Net  & 36.0 & 52.9 & 80.1 & 38.4 & 53.1 & 82.3 & 38.5 & 54.4 & 80.6 \\ MFT  & 47.3 & 66.8 & 77.8 & 56.1 & 70.8 & 86.9 & 39.6 & 60.4 & 72.7 \\ TAPIR  & 56.2 & 70.0 & 86.5 & 61.3 & 73.6 & 88.8 & 49.6 & 64.2 & 85.0 \\ OmniMotion & 52.7 & 67.5 & 85.3 & – & – & – & – & – & – \\ CoTracker-Single & 60.6 & 75.4 & 89.3 & 64.8 & 79.1 & 88.7 & 48.7 & 64.3 & **86.5** \\ CoTracker2-All & 60.7 & 75.7 & 88.1 & – & – & – & – & – & – \\ CoTracker2-Single & 62.2 & 75.7 & 89.3 & 65.9 & 79.4 & 89.9 & – & – & – \\ TAPTR  & 63.0 & **76.1** & 91.1 & 66.3 & 79.2 & 91.0 & 49.0 & **64.4** & 85.2 \\ LocoTrack  & 63.0 & 75.3 & 87.2 & **67.8** & **79.6** & 89.9 & **52.9** & **66.8** & 85.3 \\  BoostTAP\({}^{}\) & 61.4 & 74.0 & 88.4 & 66.4 & 78.5 & 90.7 & 54.7 & 68.5 & 86.3 \\  Ours (TAPTRv2) & **63.5** & 75.9 & **91.4** & 66.4 & 78.8 & **91.3** & 49.7 & 64.2 & 85.7 \\   

Table 2: Comparison of TAPTRv2 with prior methods. Note that, LocoTrack and BootSTAP\({}^{}\) are concurrent works, and BootSTAP introduces extra 15M video clips for training.

the resolution of the input image has a great influence on the performance, for fair comparison, we follow previous works to limit the resolution of our input image to 256\(\)256.

### Implementation Detail

We follow the previous work  and use ResNet-50 as the image backbone for both experimental efficiency and fair comparison. We employ two Transformer encoder layers with deformable attention  to enhance feature quality, and five Transformer decoder layers by default to achieve the results that are fully optimized. We use AdamW  and EMA  for training. We use 8 NVIDIA A100 GPUs, accumulating gradients 4 times to approximate a total batch size of 32, and train TAPTRv2 for approximately 44,000 iterations.

### Comparison with the State of The Arts

We compare TAPTRv2 with previous methods on TAP-Vid-DAVIS and TAP-Vid-Kinetics to show its superiority in online tracking. To broaden our comparison, we also present the performance of TAPTRv2 in the "Strided" mode on DAVIS dataset (DAVIS-S). The results in Table 3 show that TAPTRv2 obtains the best performance in all of the datasets' comprehensive metric AJ. Meanwhile, the consistent improvement of OA on all datasets further verifies the importance of our designs in keeping content feature uncontaminated for more accurate visibility classification. Note that, although the concurrent BootsTAP  obtains remarkable performance on Kinetics, it introduces extra 15M real world video clips for training. Moreover, we still outperform BootsTAP by about 2.1 AJ on the DAVIS dataset.

### Ablation Studies and Analysis

We conduct ablation studies for each key design in our main contribution to gain a deeper understanding of what specifically contributes to performance improvement. We also perform ablation on the number of decoder layers.

**Ablation On The Introduction of Key-Aware Attention.** We take the type of attention mechanism in cross-attention as the only variable. The results in Table 3 show that (Row 2 vs. Row 1), the introduction of the key-aware deformable attention brings 0.7 AJ improvement, which is significant. The improvement indicates that the robust attention weights obtained through dot-production helps cross-attention obtain better query results from image feature maps, thereby improving the quality of point queries' content features.

**Ablation On The Position Update.** To verify the effectiveness of enabling the key-aware attention weights to function in the positional part of point queries, we conduct ablation studies as shown in Table 3. The results (Row 3 vs. Row 2) show that using the attention weights for updating both the content and positional parts leads to a significant improvement (1.0 AJ). This improvement verifies that the local correlation information helps position estimation greatly, and our proposed attention-based position update is an effective operation to utilize correlation information.

**Ablation On The Weight Disentangling.** As shown in Table 3, decoupling the attention weights used for updating the content feature and position of a point query through an MLP enhances performance (0.9 AJ) (Row 4 vs. Row 3). This results verify that the attention weights required for the content and position parts may have different distributions, and simply mixing them confuses the network and may lead to sub-optimal results.

   Row & Key-Aware & Pos. Update. & Disentangle A. W. & Supervision & AJ & \(<^{x}_{avg}\) & OA \\ 
1 & ✗ & ✗ & ✗ & ✗ & 60.0 & 73.1 & 88.6 \\
2 & ✓ & ✗ & ✗ & ✗ & 60.7 & 73.9 & 89.9 \\
3 & ✓ & ✓ & ✗ & ✗ & 61.7 & 74.8 & 90.4 \\
4 & ✓ & ✓ & ✓ & ✗ & 62.6 & 75.5 & 91.0 \\
5 & ✓ & ✓ & ✓ & ✓ & **63.5** & **75.9** & **91.4** \\   

Table 3: Ablation on each key design of the attention-based position updating. “Pos.” is short for “Position”, and “A. W.” for “Attention Weights.

**Ablation On The Additional Supervision.** To guarantee that the attention-based position update in cross attention is always beneficial, it is important to supervise the updated positions in each decoder layer additionally. The results in Table 3 show that this extra supervision leads to a significant improvement (0.9 AJ) (Row 5 vs. Row 4), verifying its importance.

**Ablation On The Number of Decoder Layers.**

Since our improvements over TAPTR mainly focus on the decoder, we conduct ablation studies on the number of decoder layers to verify whether TAPTRv2 still satisfies the conclusion drawn from TAPTR. The results shown in Table 4 indicate that, the performance of TAPTRv2 also improves with increased number of decoder layers, but reaches optimal performance with five decoder layers. This may be because that, with the help of the additional position update, fewer decoder layers are needed for an optimal position update result.

## 5 Visualization

**Stable Tracking Results In The Wild.** As shown in Fig. 4, TAPTRv2 shows its stability in point tracking and potential application in 3D reconstruction as well as video editing. More visualizations and corresponding videos please refer to our supplementary materials.

## 6 Conclusion and Limitation

In this paper, we have presented TAPTRv2, a new approach for solving the TAP task. TAPTRv2 improves TAPTR by developing a novel attention-based position update operation to address the query content feature contamination problem caused by the inappropriate integration of cost-volume in TAPTR. This operation is based on the observation that local attention is essentially the same as cost-volume, both of which are computed by dot-production between a query and its surrounding features. With this new operation, TAPTRv2 not only removes extra burden of cost-volume computation, but also leads to a substantial performance improvement. Compared with TAPTR, TAPTRv2 further simplifies the Transformer-based TAP framework, which we hope will help the TAP community scale up the training process and accelerate the development of more practical TAP algorithms.

**Limitation and Future work.** For self-attention in our decoder, we currently use vanilla attention, which suffers from a computational cost quadratic to the number of queries. However, there have been many studies to reduce this cost to near linear. We will devote future research to solving it for a larger impact on dense point tracking. Additionally, TAPTRv2 aligns the frameworks of point tracking and object detection, which will facilitate the integration of multiple tasks. This will also be a topic we aim to address in the future.

   \#Decoder Layers & AJ & \(<_{avg}^{x}\) & OA \\ 
2 & 56.9 & 70.7 & 88.2 \\
3 & 60.3 & 74.0 & 89.8 \\
4 & 62.3 & 75.2 & 90.3 \\
5 & **63.5** & **75.9** & **91.4** \\
6 & 62.7 & 75.7 & 90.7 \\   

Table 4: Ablation on the number of decoder layers.

Figure 4: Visualization of the tracking results of TAPTRv2 in the wild. A user writes “house” on one frame and requires TAPTRv2 to track the points in the writing area. Best view in electronic version.