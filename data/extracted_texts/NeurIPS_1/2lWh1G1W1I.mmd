# Density of States Prediction of Crystalline Materials

via Prompt-guided Multi-Modal Transformer

 Namkeyeong Lee\({}^{1}\), Heewoong Noh\({}^{1}\), Sungwon Kim\({}^{1}\),

**Dongmin Hyun\({}^{2}\)**, **Gyoung S. Na\({}^{3}\)**, **Chanyoung Park\({}^{1}\)**

\({}^{1}\) KAIST \({}^{2}\) Yahoo Research \({}^{3}\) KRICT

{namkyeong96,heewoongnoh,swkim,cy.park}@kaist.ac.kr

dhyun@yahooinc.com, ngs0@krict.re.kr

Corresponding author. \({}^{}\) These authors contributed equally.

###### Abstract

The density of states (DOS) is a spectral property of crystalline materials, which provides fundamental insights into various characteristics of the materials. While previous works mainly focus on obtaining high-quality representations of crystalline materials for DOS prediction, we focus on predicting the DOS from the obtained representations by reflecting the nature of DOS: _DOS determines the general distribution of states as a function of energy_. That is, DOS is not solely determined by the crystalline material but also by the energy levels, which has been neglected in previous works. In this paper, we propose to integrate heterogeneous information obtained from the crystalline materials and the energies via a multi-modal transformer, thereby modeling the complex relationships between the atoms in the crystalline materials and various energy levels for DOS prediction. Moreover, we propose to utilize prompts to guide the model to learn the crystal structural system-specific interactions between crystalline materials and energies. Extensive experiments on two types of DOS, i.e., Phonon DOS and Electron DOS, with various real-world scenarios demonstrate the superiority of DOSTransformer. The source code for DOSTransformer is available at [https://github.com/HeewoongNoh/DOSTransformer](https://github.com/HeewoongNoh/DOSTransformer).

## 1 Introduction

Along with recent advances in machine learning (ML) for scientific discovery, ML models have rapidly been adopted in computational materials science thanks to the abundant experimental and computational data in the field. Most ML models developed in computational materials science to date have been focused on crystalline materials property consisting of single-valued quantities , e.g., band gap energy [31; 69], formation energy [3; 56], and Fermi energy . On the other hand, spectral properties are ubiquitous in materials science, characterizing various properties of crystalline materials, e.g., X-ray absorption, dielectric function, and electronic density of states  (Figure 1).

The density of states (DOS), which is the main focus of this paper, is a spectral property that provides fundamental insights on various characteristics of crystalline materials, even enabling direct computation of single-valued properties . For example, DOS is utilized as a feature of materials for analyzing the underlying reasons for changes

Figure 1: A crystal structure and various types of its properties.

in electrical conductivity . Moreover, band gaps and edge positions, which can be directly derived from DOS, are utilized to discover new photoanodes for solar fuel generation . As a result, the exploration of ML capabilities for predicting DOS takes us one step further toward the core principles of materials science, thereby expediting the process of materials discovery. However, credible computation of DOS with traditional density functional theory (DFT) requires expensive time/financial costs of exhaustively conducting experiments with expertise knowledge . Therefore, alternative approaches for DOS calculation are necessary, whereas demonstrating ML capabilities for learning such spectral properties of the crystalline materials is relatively under-explored.

In this paper, we focus on predicting DOS by following the nature of DOS calculation: _DOS determines the general distribution of states as a function of energy_. That is, while single-valued properties can be described solely by the crystalline material, DOS is determined by not only the material itself but also the _energy levels_ at which DOS is calculated. Therefore, integrating the heterogeneous signals from both the crystalline material and the various energy levels is crucial for DOS prediction. However, previous works for DOS prediction focus on obtaining a qualified representation of material , thereby overlooking the unique characteristics of DOS.

On the other hand, learning from multiple input types, i.e., crystalline material and various energy levels, is not trivial due to their heterogeneity. To this end, we formulate the DOS prediction problem into a multi-modal learning problem, which has recently gained significant attention from ML researchers in various domains thanks to its capability of extracting and relating information from heterogeneous data types . For example, VisualBERT proposes to align elements of input modalities, i.e., images and text, thereby achieving SOTA performance in various vision-and-language tasks . Moreover, Dall-E shows its power in generating images based on text data by encoding images and texts together . Among various multi-modal learners, the multi-modal transformer demonstrates its intrinsic advantages and scalability in modeling multiple modalities , by associating heterogeneous modalities with the cross-attention mechanism .

Inspired by the recent success of the multi-modal transformer, we propose a multi-modal transformer model for DOS prediction, named DOSTransformer, which incorporates crystalline material and energies as heterogeneous input modalities. Unlike previous works that overlook the energy levels for predicting DOS , DOSTransformer learns embeddings of energy levels which are used for modeling complex relationships between the energy levels and crystalline material through a cross-attention mechanism. By doing so, DOSTransformer obtains multiple representations for a single crystalline material according to various energy levels, enabling the prediction of a single DOS value on each energy level. However, simply combining a crystalline material with energy levels may fail to consider the significant impact of the crystal's structure, which sometimes results in distinct material properties even when it is composed of identical elements (e.g., carbon). For example, although graphite and diamond are made entirely out of carbon, their arrangement (or structure) of carbon atoms differs significantly, resulting in distinct material properties. Therefore, we utilize learnable prompts to inform DOSTransformer of the input crystal system (among the seven crystal systems2), which navigates the model to capture structure-specific interactions between a crystalline material and energy levels. By doing so, DOSTransformer learns to extract not only the relational information that is shared with a crystal system, but also across the crystal systems. In this work, we make the following contributions:

* Inspired by the fact that DOS is determined by not only the materials themselves but also the energy levels at which DOS is calculated, we propose a multi-modal transformer model for DOS prediction, called DOSTransformer, which incorporates crystalline materials and energy as heterogeneous input modalities.
* To capture structure-specific interactions between a crystalline material and energy levels, thereby being able to predict DOS that significantly differs according to the crystal's structure, we utilize learnable prompts to inform DOSTransformer of the input crystal system.
* Our extensive experiments on two types of DOS, i.e., Phonon DOS and Electron DOS, demonstrate that DOSTransformer outperforms a wide range of state-of-the-art methods in various real-world scenarios, i.e., in-distribution scenarios and out-of-distribution scenarios.

To the best of our knowledge, this is the first work that considers various energy levels during DOS prediction and introduces prompts for crystal structural systems.

Related Works

### Machine Learning for Materials and Density of States

Traditional materials science research heavily relies on theory, experimentation, and computer simulations, but these approaches are time-consuming, costly, and inefficient, hindering their ability to keep up with the field's development . As an alternative research tool, ML, which requires neither expensive trial and error experiments nor domain expert knowledge, has been attracting a surge of interest from materials scientists . Consequently, various ML methods have been established to learn the relationship between materials and their properties which are usually obtained from ab-initio calculations . Inspired by the recent success of GNNs in biochemistry [19; 48; 25; 33; 34; 32], CGCNN proposes a message-passing framework for crystal structure providing high-accuracy prediction for 8 different materials properties . However, most studies have focused on single-valued properties , whereas various spectral properties, such as DOS, are also crucial.

The DOS describes the number of different states at a particular energy level that particles are allowed to occupy, determining various properties of crystalline materials. For example, phonon DOS has a crucial influence in determining the specific heat and vibrational entropy of crystalline materials as well as the interfacial thermal resistance , while electron DOS is used to derive the electronic contribution to heat capacity in metals  and the effective mass of electrons in charge carriers .

Despite the importance of DOS, how to incorporate ML capabilities for _decoding_ such spectral properties is under-explored since the vast majority of methods concentrate only on _encoding_ crystalline materials. Specifically, Chandrasekaran et al.  proposes to predict electron DOS based on the hand-crafted fingerprint of each grid point in the crystalline materials, while Del Rio et al.  leverages that of each atom in the crystalline materials. Chen et al.  predicts phonon DOS by learning the representations of the crystalline materials that are equivariant to 3D rotations, translations, and inversion with Euclidean neural networks [49; 28; 58], achieving high-quality prediction with a small number of training materials with over 64 atom types. In this work, we concentrate on decoding spectral properties regarding the complex relationship between the various energy levels and crystalline materials.

### Multi-modal Transformer, Prompt Tuning, and Positional Encoding

**Multi-modal Transformer.** In recent years, much progress in multi-modal learning has grown rapidly by extracting and aligning the rich information from heterogeneous modalities [2; 6]. Among the various multi-modal learning methods, the multi-modal transformer demonstrates its superior capability in learning multiple modalities , by associating the heterogeneous modalities with a cross-attention mechanism. Specifically, MuIT  learns the representations directly from unaligned multi-modal streams with multi-modal transformer, while Yao & Wan  improves machine translation quality by learning the image-aware text representations. In this paper, we investigate how to decode DOS of a crystalline material by learning the energy-aware crystal representations via a multi-modal transformer.

**Prompt Tuning.** On the other hand, prompt designing has gained significant attention from NLP researchers as an efficient method for fine-tuning large language models (LLMs) for various tasks . There are two main approaches to prompt designing: discrete prompt designing  and continuous prompt tuning . While discrete prompt designing involves manually creating task description tokens for LLMs through trial and error, continuous prompt tuning trains learnable prompts in a continuous latent space without requiring expert knowledge. Inspired by the success of continuous prompt tuning in NLP domain, it has been widely adopted in various domains such as computer vision  and vision-language . While previous works utilize prompts for efficient fine-tuning of the models, we employ prompts to capture the heterogeneity of various crystal structures.

**Positional Encoding.** In the original proposal of the Transformer architecture, a sinusoidal function was suggested as a method for positional encoding. However, the sinusoidal function may have limitations in terms of learnability and flexibility, which can affect its effectiveness . To address this issue, most pre-trained language models [16; 40] employ learnable vector embeddings as positional representations. By conceptualizing each energy value as the position of a word in a sentence, various energy embeddings in Section 4.2 can be analogous to the positional encoding used in traditional transformers.

## 3 Preliminaries

**Notations.** Let \(=(,)\) denote a crystalline material, where \(=\{v_{1},,v_{n}\}\) represents the set of atoms, and \(\) represents the set of edges connecting the atoms in the crystalline material \(\). Moreover, \(\) is associated with a feature matrix \(^{n F}\) and an adjacency matrix \(^{n n}\) where \(_{ij}=1\) if and only if \((v_{i},v_{j})\) and \(_{ij}=0\) otherwise.

**Task: Density of States Prediction.** Given a set of crystalline materials \(_{}=\{_{1},_{2},, _{N}\}\) and a set of energies \(_{}=\{_{1},_{2},,_{M}\}\), our goal is to train a model \(\) that predicts the DOS of a crystalline material given a set of energies, i.e., \(^{i}=(_{i},_{})\), where \(^{i}^{M}\) is an \(M\) dimensional vector containing the DOS values of a crystalline material \(_{i}\) at each energy \(_{1},,_{M}\), and \(^{i}_{j}\) is the DOS value of \(_{i}\) at energy level \(_{j}\).

## 4 Methodology: DOSTransformer

In this section, we introduce our proposed method, named DOSTransformer, a novel DOS prediction framework that learns the complex relationship between the atoms in the crystalline material and various energy levels by utilizing a cross-attention mechanism of the multi-modal transformer.

### Crystalline Material Encoder

Before modeling the pairwise interaction between the crystalline material and the energies, we first encode the crystalline material with GNNs to learn the representation of each atom, which contains not only the feature information but also the structural information. Formally, given a crystalline material \(=(,)\), we generate an atom embedding matrix for the crystalline material as follows:

\[=(,), \]

where \(^{n d}\) is an atom embedding matrix for \(\), whose \(i\)-th row indicates the representation of atom \(v_{i}\), and we stack \(L^{}\) layers of GNNs. Among various GNNs, we adopt graph networks  as our crystalline material encoder, which is a generalized and extended version of various GNNs. We provide further details on the GNNs in Appendix C.

### Prompt-based Multi-modal Transformer

After obtaining the atom embedding matrix \(\), we aim to capture the relationship between a crystalline material and various energy levels by utilizing the cross-attention layers of a multi-modal transformer  with self-attention layers . In a nutshell, we expect the cross-attention layers to integrate the heterogeneous signals from a crystalline material and various energy levels, while self-attention layers aim to integrate the information of material-specific energy representations.

**Cross-Attention Layers.** Specifically, we expect the cross-attention layers to generate the material-specific representation of the energies by repeatedly reinforcing the energy representation with the crystalline materials. To do so, we first introduce a learnable energy embedding matrix \(^{0}^{M d}\), whose \(j\)-th row, i.e., \(^{0}_{j}\), indicates the embedding of energy \(_{j}_{}\). Then, we present cross-modal attention for fusing the information from the crystal structure into various energy levels as follows:

\[^{l} =(_{^{l-1}}, _{},_{})^{M d} \] \[=(^{l-1}^{}}{ }),\]

where \(l=1,,L_{1}\) indicates the index of the cross-attention layers. In contrast to the conventional Transformer, which introduces learnable weight matrices for query \(\), key \(\), and value \(\), we directly employ the previously obtained energy embedding matrix \(^{l-1}\) as the query matrix, and the atom embedding matrix \(\) as the key and value matrices. Based on the above cross-attention mechanism, we obtain the material-specific energy embedding \(^{l}^{M d}\) by aggregating the information regarding the atoms in the crystalline material that were important at each energy level. The final material-specific energy embedding matrix \(^{L_{1}}^{M d}\) generated by the cross-attention layer reflects the relationship between the atoms in the crystalline material and various energy levels.

**Global Self-Attention.** In addition to cross-attention layers, we propose to enhance the material-specific energy embedding matrix \(^{L_{1}}\) by aggregating the information from other energy levels. To do so, we employ variants of self-attention layers in conventional Transformer  as follows:

\[}^{p}&=(_{}^{p-1}},_{}^{p-1}}, _{}^{p-1}})^{M d}\\ &=(}^{p-1}}^{ p-1}}{})}^{p-1}, \]

where \(p=1,,L_{2}\) indicates the index of the self-attention layer. We use an enhanced material-specific energy embedding \(}^{0}\) as an input of the self-attention, which is obtained by concatenating the material-specific energy embedding \(^{L_{1}}\) with the material representation \(_{i}^{d}\), i.e., \(}^{0}_{j}=_{1}(^{glob}_{j})\), where \(^{glob}_{j}=(^{L_{1}}_{j}||_{i})\), \(_{1}:^{2d}^{d}\), and \(||\) indicates the concatenation operation. Note that \(_{i}\) is a sum pooled representation of the material \(_{i}\), and \(^{L_{1}}_{j}\) indicates the \(j\)-th row of the energy embedding matrix \(^{L_{1}}\) computed by Equation 2 where \(j=0,,M\). Although the output of cross-attention layers, i.e., \(^{L_{1}}^{M d}\), captures the local atom-level information regarding the material, we also recognize the importance of incorporating the global material-level information \(_{i}\) that may not have been fully encoded during the cross-attention phase. Note that the importance of considering the global contextual information for enhancing the learning of dependencies among neural representations has been widely studied in various domains such as computer vision and natural language processing . By additionally considering the global material-level information, we aim to enhance the material-specific representation of the energies more comprehensively.

**System Self-Attention with Crystal System Prompts.** However, solely aggregating the information of material-specific energy embeddings may neglect the crucial influence of the structural properties in crystalline materials, whereas the structural properties play a significant role in determining the unique characteristics and properties of these materials . To illustrate this point, let's examine the case of graphite and diamond, both composed entirely of carbon atoms. In graphite, carbon atoms are arranged in stacked layers, where each carbon atom is bonded to three neighboring carbon atoms in a hexagonal lattice. In contrast, the carbon atom in diamond forms strong covalent bonds with four neighboring carbon atoms, resulting in a tetrahedral arrangement. Even though graphite and diamond are made entirely out of carbon, due to the structural difference, graphite exhibits properties such as electrical conductivity and lubricity due to its layered structure , while diamond is renowned for its hardness and thermal conductivity owing to its tightly bonded tetrahedral network .

Despite the importance of structural information, effectively incorporating it into the model is not trivial. Naively concatenating the structural information as an input feature of materials is straightforward but may lead to interference, hindering the model's ability to learn and generalize knowledge across different crystal structures. To this end, we propose to provide additional information to the self-attention layers with learnable prompts, which indicate the structural information of the crystalline materials. Specifically, we adopt learnable prompts \(^{7 d_{p}}\), whose \(k\)-th row \(_{k}\) represents one of the seven widely known crystal systems: Cubic, Hexagonal, Tetragonal, Trigonal, Orthorhombic, Monoclinic, and Triclinic. Then, given a crystalline material \(_{i}\), whose crystal system is given as \(k\), we incorporate the learnable prompts into the material-specific energy embedding \(}^{0}\) as an input

Figure 2: Overall model architecture and attention layers in prompt-based multi-modal Transformer.

of the self-attention, which is obtained as follows: \(}_{j}^{0}=_{2}(_{j}^{sys})\), where \(_{j}^{sys}=(_{j}^{L_{1}}||_{i}||_{k})\) and \(_{2}:^{2d+d_{p}}^{d}\). By doing so, we obtain high-level energy features that contain the crystal structural system information (i.e., \(}^{L_{2}}\)), while keeping the low-level features to share the knowledge across all the materials (i.e., \(^{L_{1}}\)). In Section 5.4, we demonstrate the effectiveness of our prompt-based approach compared to other variants.

Besides the self-attention layers, we use additional cross-attention layers on top of the self-attention layers, whose input energy embedding \(^{0}\) in Equation 2 is given as \(}^{L_{2}}\), and output the final material-specific energy embeddings \(^{L_{3}}\), where \(L_{3}\) is the number of additional cross-attention layers. By doing so, the model extracts the enhanced relationships between the crystalline materials and energy levels regarding the crystal structural system information.

### Energy Decoder

After obtaining the final material-specific energy embedding matrix \(^{L_{3},i}\) of a crystalline material \(_{i}\), the DOS value at each energy level \(_{j}\), i.e., \(}_{j}^{i}\), is given as follows: \(}_{j}^{i}=_{pred}(_{j}^{L_{3},i})\), where \(_{pred}:^{d}^{1}\) is a parameterized MLP for predicting DOS. While previous approaches directly predict DOS from the material representation \(_{i}\) using a function \(_{pred}:^{d}^{M}\), \(\) takes a different approach by making pointwise predictions that align with the nature of DOS calculation, i.e., DOS determines the general distribution of states as a function of energy To the best of our knowledge, \(\) is the first work that predicts DOS values in a pointwise manner at each energy level, and we later show in Section 5 that such an approach further enhances the performance of not only \(\) but also existing models.

### Model Training and Inference

\(\) is trained to minimize the root mean squared error (RMSE) loss \(\) between the predicted DOS value \(}_{j}^{i}\) and the ground truth DOS value \(_{j}^{i}\), i.e., \(=_{i=1}^{N}_{j=1}^{M}}_{j}^{i}-_{j}^{i})^{2}}\). More specifically, \(\) is trained by combining two distinct RMSE losses with balancing term \(\), i.e., \(^{total}=^{glob}+^{sys}\), where \(^{glob}\) and \(^{sys}\) are obtained by utilizing \(^{L_{3},glob}\) and \(^{L_{3},sys}\), respectively. By doing so, \(\) extracts the relationship information that is shared among the distinct crystal systems and within a crystal system, respectively. For inference, we utilize the model prediction obtained based on \(^{L_{3},sys}\). The overall model architecture is depicted in Figure 2.

## 5 Experiments

### Experimental Setup

**Datasets.** We use two datasets to comprehensively evaluate the performance of \(\), i.e., Phonon DOS and Electron DOS. We use the **Phonon DOS** dataset following the instructions of the official Github repository 3 of a previous work . For **Electron DOS** dataset, we collect crystalline materials and their electron DOS data from Materials Project (MP) website 4. We provide further detailed preprocessing procedures and statistics on each dataset in Appendix A.

**Methods Compared.** We mainly compare \(\) to recently proposed state-of-the-art method, i.e., E3NN , which utilizes the Euclidean network for encoding material representation. We also compare \(\) to simple baseline methods, i.e., MLP and Graph Network , which predicts the entire DOS sequence directly from the learned representation of the materials without regarding the energies during training. Moreover, to evaluate the effectiveness of the transformer layer that considers the relationship between the atoms and various energy levels, we integrate energy embeddings into baseline methods for DOS prediction by concatenating the energy embeddings to the material representation. We provide more details on the implementation of \(\) and compared methods in Appendix C and D, respectively.

### In-Distribution Evaluation

**Evaluation Protocol.** In this section, we evaluate the model performances under in-distribution scenarios. While we evaluate DOSTransformer with given data splits in a previous work  for **Phonon DOS** dataset, we randomly split the **Electron DOS** dataset into train/valid/test of 80/10/10%. Note that while all measures are reported in the original scale, we report MSE values for DOS prediction multiplied by a factor of 10 for clear interpretation during all experiments.

**Experimental Results.** In Table 1, we have the following observations: **1)** Comparing the baseline methods that overlook the energy levels for DOS prediction (i.e., Energy \(}\)) with their counterparts that incorporate both the energy levels and the crystal structure as heterogeneous input modalities through the energy embeddings (i.e., Energy \(}\)), we find out that using the energy embeddings consistently enhances the model performance. This indicates that making pointwise predictions on each energy level is crucial for DOS prediction as discussed in Section 4.3, which also aligns with the domain knowledge of materials science, i.e., DOS determines the general distribution of states as a function of energy. **2)** However, we observe that E3NN shows a relatively small performance gain compared with other methods after incorporating the energy information. This is because the integration of the energy embedding may confound the model to learn the proper equivariance of the materials. **3)** On the other hand, DOSTransformer outperforms previous methods that do not consider the complex relationships between the atoms in materials and various energy levels via the various attention mechanisms. This again implies that a naive integration of the energy information into previous models cannot fully benefit from the energy information. We also provide qualitative analysis on the obtained DOS in Section 5.5.

**Physical Validity of DOS.** In addition to the accuracy of DOS prediction, we assess the physical validity of the model-predicted DOS by deriving various physical properties, i.e., bulk modulus, band gap, and Fermi energy, of materials, based on the model-predicted DOS. To do so, given the DFT-calculated DOS, which is considered the ground-truth DOS based on which accurate derivation of various physical properties is possible, we train an MLP with a non-linearity in each layer to predict the properties of a crystal structure. Then, based on the obtained MLP weights, we predict the material properties given the model-predicted DOS as the input and calculate the MSE for each material property. We provide further details on the experimental setting in Appendix B. In Table 1, we observe that DOS predicted by DOSTransformer shows the superiority of predicting physical properties of materials, indicating that DOSTransformer not only achieves high accuracy but also produces physically valid predictions.

### Out-of-Distribution Evaluation

In this section, we evaluate the model performances under out-of-distribution (OOD) scenarios. It is well recognized that existing DFT calculation-based databases have limitations in terms of

    &  &  &  \\   & MSE & MAE & \(R^{2}\) & MSE & MAE & \(R^{2}\) & Bulk M. & Band G. & Ferm. E. \\  }\)**} \\  MLP & 0.309 & 0.106 & 0.576 & 0.347 & 0.130 & 0.487 & 0.695 & 1.597 & 4.650 \\  & (0.016) & (0.003) & (0.016) & (0.015) & (0.003) & (0.029) & (0.031) & (0.171) & (0.202) \\ Graph Network & 0.259 & 0.099 & 0.638 & 0.264 & 0.103 & 0.613 & 0.688 & 1.457 & 3.362 \\  & (0.009) & (0.001) & (0.013) & (0.005) & (0.000) & (0.008) & (0.060) & (0.051) & (0.189) \\ E3NN & 0.210 & 0.077 & 0.705 & 0.296 & 0.109 & 0.552 & 0.504 & 0.896 & 2.925 \\  & (0.004) & (0.001) & (0.007) & (0.005) & (0.001) & (0.013) & (0.033) & (0.093) & (0.111) \\  }\)**} \\  MLP & 0.251 & 0.099 & 0.652 & 0.341 & 0.128 & 0.499 & 0.521 & 1.409 & 4.372 \\  & (0.004) & (0.001) & (0.006) & (0.011) & (0.002) & (0.012) & (0.011) & (0.182) & (0.067) \\ Graph Network & 0.226 & 0.092 & 0.685 & 0.240 & 0.099 & 0.650 & 0.543 & 1.263 & 3.080 \\  & (0.007) & (0.002) & (0.011) & (0.005) & (0.001) & (0.005) & (0.085) & (0.107) & (0.100) \\ E3NN & 0.200 & 0.074 & 0.724 & 0.291 & 0.114 & 0.564 & 0.451 & 1.605 & 3.777 \\  & (0.001) & (0.001) & (0.002) & (0.000) & (0.000) & (0.008) & (0.023) & (0.231) & (0.175) \\   & **0.191** & **0.071** & **0.733** & **0.221** & **0.089** & **0.679** & **0.427** & **0.461** & **2.337** \\  & (0.003) & (0.002) & (0.004) & (0.006) & (0.001) & (0.006) & (0.024) & (0.019) & (0.094) \\   

Table 1: Overall model performance under in-distribution scenarios (Bulk M. : Bulk Modulus / Band G. : Band Gap / Ferm. E. : Fermi Energy).

coverage, as they often focus on specific types of materials or structural archetypes, resulting in biased distributions . Therefore, it is essential to evaluate the model's performance in OOD scenarios to assess its real-world applicability and generalizability beyond the limitations of the available databases .

**Evaluation Protocol.** To do so, we evaluate the model performance on the crystal structures that 1) contain a different number of atom species with the training set (i.e., # Atom species), and 2) belong to different crystal systems that were not included in the training set (i.e., Crystal System). In both scenarios, training data primarily consist of relatively simple structures compared to those in the test data. We provide further details on data split and evaluation in Appendix B.

**Experimental Results.** In Table 2, we again observe that utilizing energy embeddings consistently brings performance gain to all baseline models. Especially, Graph Network significantly benefits from energy embeddings compared to in-distribution scenarios (See Table 1). We attribute this to the inherent restrictive inductive biases in Graph Network , i.e., Graph Network heavily relies on the structural information of materials. Specifically, as the structure of the materials totally varies in the OOD scenarios, which makes the prediction of the whole DOS sequence challenging, incorporating the energy embeddings becomes especially helpful in the OOD scenarios for Graph Network. Moreover, DOSTransformer also alleviates the restrictive inductive bias of Graph Networks by elaborately modeling the complex relationships between energies and materials. It is worth noting that similar observations have been made by comparing Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) in the computer vision domain . In conclusion, we argue that incorporating energy information during model training is crucial not only for in-distribution scenarios but also for OOD scenarios, demonstrating the applicability of DOSTransformer in real-world applications. For a more detailed analysis, please refer to Appendix E.1.

**Fine-tuning on Few Materials from Complex Crystal Systems.** In addition to evaluating the performance under the OOD scenarios, we verify how well the models can make predictions for materials from complex crystal systems with only few training samples, which is a practical situation in reality. For this, we sampled a small subset of of materials (i.e., 10%) from the test set used in OOD scenarios of crystal systems, and fine-tune the models that are already trained on the training set. As for DOSTransformer, we fine-tune the model using two different approaches: i) fine-tuning all model parameters (referred to as "All"), and ii) tuning only the parameters of the prompts and the energy decoder while keeping all other model parameters frozen (referred to as "Only Prompt"). In Table 3, we have the following observations: **1)** As we expected, the additional fine-tuning step achieves performance gain for all models, while it was marginal due to a limited number of materials used for fine-tuning. **2)** Only fine-tuning the prompts of DOSTransformer achieved more performance gain compared to fine-tuning the whole model. This is because while fine-tuning the whole model on a small subset of materials may easily incur overfitting, fine-tuning only prompts enables the model to additionally learn from few new samples while maintaining the knowledge obtained previously. In conclusion, we believe that the proposed prompt tuning approach can have a significant impact in the

    &  \\   &  &  \\   & MSE & MAE & \(R^{2}\) & MSE & MAE & \(R^{2}\) \\  
**Energy ✗** & & & & & \\  MLP & 0.545 & 0.161 & 0.250 & 0.455 & 0.149 & 0.437 \\  & (0.005) & (0.001) & (0.003) & (0.006) & (0.002) & (0.007) \\ Graph Network & 0.460 & 0.144 & 0.381 & 0.407 & 0.132 & 0.505 \\  & (0.015) & (0.003) & (0.002) & (0.004) & (0.001) & (0.006) \\ E3NN & 0.541 & 0.154 & 0.231 & 0.421 & 0.134 & 0.483 \\  & (0.007) & (0.001) & (0.005) & (0.004) & (0.001) & (0.007) \\ 
**Energy ✓** & & & & & \\  MLP & 0.545 & 0.159 & 0.260 & 0.455 & 0.149 & 0.445 \\  & (0.005) & (0.001) & (0.016) & (0.004) & (0.001) & (0.007) \\ Graph Network & 0.455 & 0.144 & 0.388 & 0.384 & 0.129 & 0.534 \\  & (0.002) & (0.002) & (0.004) & (0.012) & (0.004) & (0.014) \\  & 0.534 & 0.152 & 0.237 & 0.420 & 0.133 & 0.483 \\  & (0.013) & (0.001) & (0.017) & (0.007) & (0.001) & (0.008) \\   & **0.454** & **0.136** & **0.399** & **0.373** & **0.122** & **0.552** \\  & (0.008) & (0.001) & (0.017) & (0.006) & (0.001) & (0.006) \\   

Table 2: Overall model performance in OOD scenarios.

  
**Model** & MSE & MAE & \(R^{2}\) \\  
**Energy ✗** & & & \\  MLP & 0.401 & 0.137 & 0.510 \\  & (0.017) & (0.003) & (0.021) \\ Graph Network & 0.394 & 0.131 & 0.519 \\  & (0.007) & (0.002) & (0.010) \\ E3NN & 0.414 & 0.133 & 0.490 \\  & (0.006) & (0.001) & (0.007) \\ 
**Energy ✓** & & & \\  MLP & 0.394 & 0.136 & 0.519 \\  & (0.008) & (0.002) & (0.009) \\ Graph Network & 0.382 & 0.130 & 0.533 \\  & (0.003) & (0.000) & (0.002) \\ E3NN & 0.417 & 0.133 & 0.487 \\  & (0.004) & (0.001) & (0.007) \\    \\  All & 0.365 & **0.122** & 0.558 \\  & (0.005) & (0.001) & (0.008) \\ Only Prompt & **0.355** & **0.122** & **0.570** \\  & (0.008) & (0.001) & (0.010) \\   

Table 3: Fine-tuning on OOD systems.

field of materials science beyond simple DOS prediction, where the existing databases exhibit a bias towards dominant types of materials [13; 30; 20]. We further explore different proportions beyond the 10% of the materials used for fine-tuning, as detailed in the Appendix E.3.

### Model Analysis

**Ablation Studies.** To verify the benefit of each component of DOS-Transformer, we conduct ablation studies under in-distribution scenarios in Figure 3. We observe that utilizing only either one of the global and system losses deteriorates the model performance. This is because jointly optimizing the losses incentivizes the model to extract the relational information that is shared across the crystal systems and within a crystal system. On the other hand, it is worth noting that DOSTransformer with only one of the losses still outperforms baseline models shown in Table 1, indicating that modeling complex relationships between materials and various energies is crucial in DOS prediction.

**Sensitivity Analysis.** To verify the robustness of DOSTransformer, we measure the model performance by varying \(\), which is introduced to control the effect of the crystal system loss, i.e. \(^{sys}\), in Section 4.4. In Figure 4, DOSTransformer shows robustness over various levels of \(\), consistently outperforming baseline models in Table 1. This verifies that DOSTransformer can be easily trained without an expensive hyperparameter tuning, further demonstrating the practicality of DOSTransformer.

**Crystal System Prompts.** As discussed in Section 4.2, it is not trivial to effectively incorporate the crystal system information into the model. To evaluate the effectiveness of our proposed prompt-based approach, we explore different approaches for incorporating the crystal system information into the model. We make modifications to determine where and how to inject this information. In terms of the location, we consider two options: injecting the information into the input atoms (i.e., Atom Feat.) and injecting it before the self-attention layers (i.e., Before SA). Regarding the method of injection, we explore two strategies: one-hot encoding of crystal systems and the use of learnable crystal system prompts. In Table 4, we have following observations: **1)** Naively incorporating prompts into atom feature even perform worse than the model without using crystal system (see _"w/o \(^{sys}\)"_ in Figure 3), demonstrating the importance of an elaborate design choice for injecting crystal system information. **2)** We observe that our approach, i.e. Before SA and prompt, outperforms all other possible choices, indicating that DOSTransformer successfully incorporates crystal system information during training. We also examine the benefits of injecting crystal system information into baseline methods in Appendix E.2.

### Qualitative Analysis

In this section, we provide a qualitative analysis of the predicted DOS by mainly comparing it to the DFT-calculated (i.e., Ground Truth) DOS and our main baseline (i.e., E3NN). In Figure 5 (a), which represents the predicted DOS of materials not containing transitional metals, both E3NN and DOSTransformer successfully capture the overall trend of the DOS for several materials (e.g., mp-982366, mp-1009129, and mp-16378). However, DOSTransformer shows a much more precise prediction that closely aligns with the ground truth DOS, providing even more useful information beyond the shape of DOS. For example, peak points represent regions of high density and are likely to be strongly influenced when materials undergo changes in property, and thus represents the probabilistically important energy regions of the materials in the process of material discovery. Notably, our model better captures the peak points in the ground truth DOS compared to E3NN, demonstrating the applicability of DOSTransformer-predicted DOS for material discovery process.

    &  &  \\   &  &  &  &  \\   &  & &  & 0.222 & 0.089 \\  & & & & (0.004) & 0.001 \\  & & & & 0.227 & 0.090 \\  & & & & (0.005) & 0.001 \\   &  & & & 0.226 & 0.090 \\  & & & & (0.005) & 0.001 \\  & & & & **0.221** & **0.089** \\  & & & & (0.006) & 0.001 \\   

Table 4: Comparing various crystal system information injection approaches.

Figure 4: Sensitivity analysis.

Figure 3: Ablation studies.

On the other hand, Figure 5 (b) shows the DOS prediction for materials containing transition materials. Although \(\) provides more reliable prediction, we observe that the prediction errors of both models get larger compared to the materials that do not contain transition metals shown in Figure 5 (a). This can be attributed to the inherent complexity of physical properties in materials containing transition metals, as discussed in Section 6. Therefore, for our future work, we plan to design expert models in which each expert is responsible for materials with and without transition metals, to achieve more refined and accurate predictions of the DOS. This approach would enable a more comprehensive and elaborate analysis of the DOS in different material compositions.

## 6 Limitations & Future Work

It is widely known that materials containing transitional metals inherently have complex physical properties compared to other materials. As a result, the density functionals that yield optimal performance in DFT calculations differ between materials with and without transitional metals . However, despite the distinction, most ML-based DOS prediction research, including \(\), have attempted to encode the properties of both material types into a single model , which may provide conflicting signals to the model. Therefore, as a future work, we plan to address this issue by developing specialized approaches that work well on both material types, e.g., utilizing expert models  in which an expert is assigned to each material type and later combined to perform well on both material types. We expect this approach to result in the robust and reliable prediction of DOS for both types of materials, enhancing the potential impact in the materials science field and broadening our understanding of material properties.

## 7 Conclusion

In this paper, we propose \(\), which predicts DOS of crystalline materials with various energy levels by following the nature of DOS calculation. Specifically, \(\) associates the heterogeneous information from materials and energy levels with cross-attention and self-attention layers. Moreover, the crystal system prompts are introduced to effectively train the model to learn the relational information that is shared across all the crystal systems and within a crystal system. By doing so, \(\) outperforms previous works in predicting two types of DOS, i.e., phonon DOS and electron DOS, in various scenarios, i.e., in-distribution scenarios and out-of-distribution scenarios. Extensive experiments verify that incorporating energy information is crucial in predicting the DOS of a crystal structure for real-world application and \(\) effectively utilizes the crystal structural information with crystal system prompts.

Figure 5: Qualitative Analysis.