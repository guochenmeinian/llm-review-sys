# BitsFusion: 1.99 bits Weight Quantization of Diffusion Model

Yang Sui\({}^{1,2,}\)  Yanyu Li\({}^{1}\)  Anil Kag\({}^{1}\)  Yerlan Idelbayev\({}^{1}\)  Junli Cao\({}^{1}\)  Ju Hu\({}^{1}\)

**Dhritiman Sagar\({}^{1}\)  Bo Yuan\({}^{2}\)  Sergey Tulyakov\({}^{1}\)  Jian Ren\({}^{1,*}\) \({}^{1}\)**Snap Inc. \({}^{2}\)Rutgers University

Project Page: [https://snap-research.github.io/BitsFusion](https://snap-research.github.io/BitsFusion)

###### Abstract

Diffusion-based image generation models have achieved great success in recent years by showing the capability of synthesizing high-quality content. However, these models contain a huge number of parameters, resulting in a significantly large model size. Saving and transferring them is a major bottleneck for various applications, especially those running on resource-constrained devices. In this work, we develop a novel weight quantization method that quantizes the UNet from Stable Diffusion v1.5 to \(1.99\) bits, achieving a model with \(7.9\) smaller size while exhibiting even _better_ generation quality than the original one. Our approach includes several novel techniques, such as assigning optimal bits to each layer, initializing the quantized model for better performance, and improving the training strategy to dramatically reduce quantization error. Furthermore, we extensively evaluate our quantized model across various benchmark datasets and through human evaluation to demonstrate its superior generation quality.

## 1 Introduction

Recent efforts in developing diffusion-based image generation models  have demonstrated remarkable results in synthesizing high-fidelity and photo-realistic images, leading to various applications such as content creation and editing , video generation , and 3D asset synthesis , among others.

Figure 1: _Top_: Images generated from full-precision Stable Diffusion v1.5. _Bottom_: Images generated from BitsFusion, where the weights of UNet are quantized into _1.99_ bits, achieving \(7.9\)_smaller_ storage than the one from Stable Diffusion v1.5. All the images are synthesized under the setting of using PNDM sampler  with \(50\) sampling steps and random seed as \(1024\). Prompts and more generations are provided in App. M.

However, Diffusion Models (DMs) come with the drawback of a large number of parameters, _e.g._, millions or even billions, causing significant burdens for transferring and storing due to the bulky model size, especially on resource-constrained hardware such as mobile and wearable devices.

Existing studies have explored reducing the model size of large-scale text-to-image diffusion models by designing efficient architectures and network pruning [41; 92; 32]. These approaches usually require significant amounts of training due to the changes made to the pre-trained networks. Another promising direction for model storage reduction is quantization [12; 30], where floating-point weights are converted to low-bit fixed-point representations, thereby saving computation memory and storage.

There have been emerging efforts on compressing the DMs through quantization [73; 38; 17; 39]. However, these approaches still face several major challenges, especially when quantizing large-scale text-to-image diffusion models like Stable Diffusion v1.5 (SD-v1.5) . _First_, many of these methods are developed on relatively small-scale DMs trained on constrained datasets. For example, models trained on CIFAR-10 require modest storage of around \(100\) MB [21; 39]. In contrast, SD-v1.5 necessitates \(3.44\) GB of storage in a full-precision format. Adapting these methods to SD-v1.5 remains to be a challenging problem. _Second_, current arts mainly focus on quantizing weights to \(4\) bits. How to quantize the model to extremely low bit is not well studied. _Third_, there is a lack of fair and extensive evaluation of how quantization methods perform on large-scale DMs, _i.e._, SD-v1.5.

To tackle the above challenges, this work proposes BitsFusion, a quantization-aware training framework that employs a series of novel techniques to compress the weights of pre-trained large-scale DMs into _extremely low_ bits (_i.e._, 1.99 bits), achieving even _better_ performance (_i.e._, higher image quality and better text-image alignment). Consequently, we compress the \(1.72\) GB UNet (FP16)1 of SD-v1.5 into a \(219\) MB model, achieving a \(7.9\) compression ratio. Specifically, our contributions can be summarized into the following four dimensions:

* **Mixed-Precision Quantization for DMs.** We propose an effective approach for quantizing DMs in a mixed-precision manner. _First_, we thoroughly analyze the appropriate metrics to understand the quantization error in the quantized DMs (Sec. 3.2). _Second_, based on the analysis, we quantize different layers into different bits according to their quantization error (Sec. 3.3).
* **Initialization for Quantized DMs.** We introduce several techniques to initialize the quantized model to improve performance, including time embedding pre-computing and caching, adding balance integer, and alternating optimization for scaling factor initialization (Sec. 4.1).
* **Improved Training Pipeline for Quantized DMs.** We improve the training pipeline for the quantized model with the proposed two-stage training approach (Sec. 4.2). In the first stage, we use the full-precision model as a teacher to train the quantized model through distillation. Our distillation loss forces the quantized model to learn both the predicted noise and the intermediate features from the teacher network. Furthermore, we adjust the distribution of time step sampling during training, such that the time steps causing larger quantization errors are sampled more frequently. In the second stage, we fine-tune the model using vanilla noise prediction .
* **Extensive Quantitative Evaluation.** For the first time in the literature, we conduct extensive quantitative analysis to compare the performance of the quantized model against the original SD-v1.5. We include results on various benchmark datasets, _i.e._, TIFA , GenEval , CLIP score  and FID  on MS-COCO 2014 validation set . Additionally, we perform human evaluation on PartiPrompts . Our \(1.99\)-bit weights quantized model _consistently outperforms_ the full-precision model across various evaluations, demonstrating the effectiveness of our approach.

## 2 Related Works

To enhance model efficiency in terms of storage and computational costs, quantization [11; 59; 58; 43; 36; 60; 48; 84; 45; 53] is adopted for diffusion models [73; 38; 18; 76; 81; 83; 51; 85; 4; 82; 7; 93; 27; 17; 39; 91] with primarily two types: post-training quantization (PTQ) and quantization-aware training (QAT). PTQ does not require a full training loop; instead, it utilizes a limited calibration dataset to adjust the quantization parameters. For example, PTQ4DM  calibrates the quantization parameters to minimize the quantization error of DMs. Q-Diffusion  minimizes the quantization error via the block-wise reconstruction . PTQD  integrates quantization noise into the stochastic noise inherent in the sampling steps of DMs. TDQ  optimizes scaling factors for activations across different time steps, applicable to both PTQ and QAT strategies. TFMQ  focuses on reconstructing time embedding and projection layers to prevent over-fitting. However, PTQ often results in performance degradation compared to QAT, particularly when aiming for extremely low-bit DMs. In contrast, QAT involves training the full weights to minimize the quantization error, thereby achieving higher performance compared to PTQ. For instance, EfficientDM , inspired by LoRA , introduces a quantization-aware low-rank adapter to update the LoRA weights, avoiding training entire weights. Q-DM  employs normalization and smoothing operation on attention features through proposed Q-attention blocks, enhancing quantization performance. Nevertheless, existing works primarily study \(4\) bits and above quantization on small-scale DMs trained on constrained datasets. In this paper, we focus on quantizing large-scale Stable Diffusion to extremely low bits and extensively evaluating the performance across different benchmark datasets.

## 3 Mixed Precision Quantization for Diffusion Models

In this section, we first go through the formulations of weight quantization and generative diffusion models. We then determine the mixed-precision strategy, assigning optimized bit widths to different layers to reduce the overall quantization error. Specifically, we first analyze the quantization error of each layer in the diffusion model and conclude sensitivity properties. Then, based on the analysis, we assign appropriate bits to each layer by jointly considering parameter efficiency (_i.e._, size savings).

### Preliminaries

**Quantization** is a popular and commonly used technique to reduce model size. While many quantization forms exist, we focus on uniform quantization, where full-precision values are mapped into discrete integer values as follows:

\[_{}=(_{}}{8 }+I_{z},0,2^{b}-1), \]

where \(_{}\) denotes the floating-point weights, \(_{}\) is the quantized integer weights, \(\) is the scaling factor, \(I_{z}\) is the zero point, and \(b\) is the quantization bit-width. \(\) denotes the nearest rounding operation and \(()\) denotes the clipping operation that constrains \(_{}\) within the target range. Following the common settings [38; 17], we apply the channel-wise quantization and set \(8\) bits for the first and last convolutional layer of the UNet.

**Stable Diffusion.** Denoising diffusion probabilistic models [77; 21] learn to predict real data distribution \( p_{}\) by reversing the ODE flow. Specifically, given a noisy data sample \(_{t}=_{t}+_{t}\) (\(_{t}\) and \(_{t}\) are SNR schedules and \(\) is the added ground-truth noise), and a _quantized_ denoising model \(}_{_{},}\) parameterized by \(_{}\) and \(\), the learning objective can be formulated as follows,

\[_{_{},}=_{t,} [\|-}_{_{}, }(t,_{t},)\|], \]

where \(t\) is the sampled time step and \(\) is the input condition (_e.g._, text embedding). Note that during the training of quantized model, we optimize \(_{}\) and \(\) by backpropagating \(_{_{},}\) via Straight-Through Estimator (STE)  and quantize the weights to the integers for deployment. Here, for the notation simplicity, we directly use \(_{}\) to represent the optimized weights in the quantized models.

The latent diffusion model  such as Stable Diffusion conducts the denoising process in the latent space encoded by variational autoencoder (VAE) [34; 69], where the diffusion model is the UNet . This work mainly studies the quantization for the UNet model, given it is the major bottleneck for the storage and runtime of the Stable Diffusion . During the inference time, _classifier-free guidance_ (CFG)  is usually applied to improve the generation,

\[}_{_{},}(t,_{t}, )=w}_{_{},}(t, _{t},)-(w-1)}_{_{}, }(t,_{t},), \]

where \(w 1\) and \(}_{_{},}(t,_{t},)\) denotes the generation conditioned on the null text prompt \(\).

### Per-Layer Quantization Error Analysis

**Obtaining Quantized Models.** We first perform a per-layer sensitivity analysis for the diffusion model. Specifically, given a pre-trained full-precision diffusion model, we quantize _each_ layer to \(1\), \(2\), and \(3\) bits while freezing others at full-precision, and performing quantization-aware training (QAT)respectively. For instance, for the SD-v1.5 UNet with \(256\) layers (excluding time embedding, the first and last layers), we get a total of \(768\) quantized candidates. We perform QAT over each candidate on a pre-defined training sub dataset, and validate the incurred quantization error of each candidate by comparing it against the full-precision model (more details in App. B).

**Measuring Quantization Errors.** To find the appropriate way to interpret the quantization error, we analyze four metrics: _Mean-Squared-Error (MSE)_ that quantifies the pixel-level discrepancies between images (generations from floating and the quantized model in our case), _LPIPS_ that assesses human-like perceptual similarity judgments, _PSNR_ that measures image quality by comparing the maximum possible power of a signal with the power of a corrupted noise, and _CLIP score_ that evaluates the correlation between an image and its language description. After collecting the scores (examples in Fig. 1(b) and Fig. 1(c), full metrics are listed in App. F), we further measure the consistency of them by calculating the Pearson correlation  for different metrics under the same bit widths (in Tab. 1), and different bit widths under the same metric (in Tab. 2). With these empirical results, we draw the following two main observations.

**Observation 1**: _MSE, PSNR, and LPIPS show strong correlation and they correlate well with the visual perception of image quality._

Tab. 1 shows that MSE is highly correlated with PSNR and LPIPS under the same bit width. Additionally, we observe a similar trend of per-layer quantization error under different bit widths, as in Tab. 2. As for visual qualities in Fig. 1(a) and 1(b), we can see that higher MSE errors lead to severe image quality degradation, _e.g._, the highlighted RB _conv shortcut_. Therefore, the MSE metric effectively reflects quality degradations incurred by quantization, and it is unnecessary to incorporate PSNR and LPIPS further.

**Observation 2**: _After low-bit quantization, changes in CLIP score are not consistently correlated with MSE across different layers. Although some layers show smaller MSE, they may experience larger semantic degradation, reflected in larger CLIP score changes._

We notice that, after quantization, the CLIP score changes for all layers only have a weak correlation with MSE, illustrated in Tab. 1. Some layers display smaller MSE but larger changes in CLIP score.

Figure 2: \(1\)-bit quantization error analysis for all the layers from the UNet of SD-v1.5.

For example, in Fig. 1(b), the MSE of CA _tok_ layer (\(5_{th}\) highlighted layer (green) from left to right) is less than that of RB _conv_ layer (\(6_{th}\) highlighted layer (orange) from left to right), yet the changes in CLIP score are the opposite. As observed in the first row of Fig. 1(a), compared to RB _conv_ layer, quantizing this CA _tok_ layer changes the image content from "a teddy bear" to "a person", which diverges from the text prompt _A teddy bear on a skateboard in Times Square, doing tricks on a cardboard box ramp._ This occurs because MSE measures only the difference between two images, which does not capture the semantic degradation. In contrast, the CLIP score reflects the quantization error in terms of semantic information between the text and image. Thus, we employ the CLIP score as a complementary metric to represent the quantization error.

### Deciding the Optimal Precision

With the above observations, we then develop the strategy for bit-width assignments. We select MSE and CLIP as our quantitative metrics, along with the number of parameters of each layer as the indicator of size savings.

**Assigning bits based on MSE.** Intuitively, layers with more parameters and lower quantization error are better candidates for extremely low-bit quantization, as the overall bit widths of the model can be significantly reduced. According to this, we propose a layer size-aware sensitivity score \(\). For the \(i_{th}\) layer, its sensitivity score for the \(b\)-bits (\(b\{1,2,3\}\)) is defined as \(_{i,b}=M_{i,b}N_{i}^{-}\), where \(M\) denotes the MSE error, \(N\) is the total number of parameters of the layer, and \(\) denotes the parameter size factor. To determine the bit width (_i.e._, \(b^{*}\)) for each layer, we define a sensitivity threshold as \(_{o}\), and the \(i_{th}\) layer is assigned to \(b_{i}^{*}\)-bits, where \(b_{i}^{*}=\{b|_{i,b}<_{o}\}\). The remaining layers are \(4\) bits.

**Assigning bits based on CLIP score.** For the layers with a high CLIP score dropping after quantization, instead of assigning bits based on sensitivity score as discussed above, we directly assign higher bits to those layers. Therefore, the quantized model can produce content that aligns with the semantic information of the prompt. We provide the detailed mixed-precision algorithm in Alg. 1 of App. B.

## 4 Training Extreme Low-bit Diffusion Model

With the bits of each layer decided, we then train the quantized model with a series of techniques to improve performance. The overview of our approach is illustrated in Fig. 3.

### Initializing the Low-bit Diffusion Model

**Time Embedding Pre-computing and Caching.** During the inference time of a diffusion model, a time step \(t\) is transformed into an embedding through projection layers to be incorporated into the diffusion model. As mentioned by existing works , the quantization of the projection layers can lead to large quantization errors. However, the embedding from each time step \(t\) is always the same, suggesting that we can actually pre-compute the embedding _offline_ and load cached values during inference, instead of computing the embedding every time. Furthermore, the storage size of the time embedding is \(25.6\) smaller than the projection layers. Therefore, we pre-compute the time embedding and save the model without the project layers. More details are provided in App. C.

**Adding Balance Integer.** In general, weight distributions in deep neural networks are observed as symmetric around zero . To validate the assumption on SD-v1.5, we analyze its weight

    & MSE _vs._ PSNR & MSE _vs._ LPIPS & MSE _vs._ CS \\ 
1 bit & 0.870 & 0.984 & 0.733 \\
2 bit & 0.882 & 0.989 & 0.473 \\
3 bit & 0.869 & 0.991 & 0.535 \\   

Table 1: Pearson correlation (absolute value) of quantization error between different metrics (_e.g._, MSE _vs._ PSNR denotes the correlation between two metrics) when quantizing individual layers to 1, 2, and 3 bits. CS denotes CLIP Score.

    & MSE & PSNR & LPIPS & CLIP Score \\ 
1 _vs._ 2 bit & 0.929 & 0.954 & 0.943 & 0.504 \\
1 _vs._ 3 bit & 0.766 & 0.843 & 0.802 & 0.344 \\
2 _vs._ 3 bit & 0.887 & 0.923 & 0.895 & 0.428 \\   

Table 2: Pearson correlation (absolute value) of quantization error between different bit pairs (_e.g._, 1 _vs._ 2 denotes the correlation between the two bit widths) for a single metric when quantizing individual layers to 1, 2, and 3 bits.

distribution for the layers under full precision by calculating the skewness of weights. Notably, the skewness of more than \(97\%\) of the layers ranges between \([-0.5,0.5]\), indicating that the weight distributions are symmetric in almost all layers. Further details are provided in App. D.

However, existing works on diffusion model quantization overlook the symmetric property , as they perform relatively higher bits quantization, _e.g._, \(4\) or \(8\) bits. This will hurt the model performance at extremely low bit levels. For example, in \(1\)-bit quantization, the possible most symmetric integer outcomes can only be \(\{0,1\}\) or \(\{-1,0\}\). Similarly, for \(2\)-bit quantization, the most balanced mapping integers can be either \(\{-2,-1,0,1\}\) or \(\{-1,0,1,2\}\), significantly disrupting the symmetric property. The absence of a single value among \(2\) or \(4\) numbers under low-bit quantization can have a significant impact. To tackle this, we leverage the bit balance strategy  to initialize the model. Specifically, we introduce an additional value to balance the original quantization values. Namely, in a 1-bit model, we adjust the candidate integer set from \(\{0,1\}\) to \(\{-1,0,1\}\), achieving a more balanced distribution. By doing so, we treat the balanced \(n\)-bits weights as \((2^{n}+1)\)-bits.

**Scaling Factor Initialization via Alternating Optimization.** Initializing scaling factors is an important step in quantization. Existing QAT works typically employ the Min-Max initialization strategy  to ensure the outliers are adequately represented and preserved. However, such a method faces challenges in extremely low-bit quantization settings like \(1\)-bit, since the distribution of the full-precision weights is overlooked, leading to a large quantization error and the increased difficulty to converge. Therefore, we aim to minimize the \(_{2}\) error between the quantized weights and full-precision weights with the optimization objective as:

\[_{}\|(_{}-I_{z})-_{}\|^{2}. \]

Nevertheless, considering the rounding operation, calculating an exact closed-form solution is not straightforward . Inspired by the Lloyd-Max algorithm , we use an optimization method on scaling factor \(\) to minimize the initialization error of our quantized diffusion model as follows:

\[_{}^{j}=Q_{}(_{}, ^{j-1});\ \ ^{j}=_{}^{j}(_{}^{j}-I_{z})^{}}{(_{}^{j}- I_{z})(_{}^{j}-I_{z})^{}}, \]

where \(Q_{}()\) denotes the integer mapping quantization operation that converts the full-precision weights to integer as Eq. (1), and \(j\) represents the iterative step. The optimization is done for \(10\) steps.

Figure 3: **Overview of the training and inference pipeline for the proposed BitsFusion.**_Left:_ We analyze the quantization error for each layer in SD-v1.5 (Sec. 3.2) and derive the mixed-precision recipe (Sec. 3.3) to assign different bit widths to different layers. We then initialize the quantized UNet by adding a balance integer, pre-computing and caching the time embedding, and alternately optimizing the scaling factor (Sec. 4.1). _Middle:_ During the Stage-I training, we freeze the teacher model (_i.e._, SD-v1.5) and optimize the quantized UNet through CFG-aware quantization distillation and feature distillation losses, along with sampling time steps by considering quantization errors (Sec. 4.2). During the Stage-II training, we fine-tune the previous model with the noise prediction. _Right:_ For the inference stage, using the pre-cached time features, our model processes text prompts and generates high-quality images.

### Two-Stage Training Pipeline

With the mixed-precision model initialized, we introduce the _two-stage_ training pipeline. In Stage-I, we train the quantized model using the full-precision model as the teacher through distillation loss. In Stage-II, we fine-tune the model from the previous stage using noise prediction [21; 80].

**CFG-aware Quantization Distillation.** Similar to existing works , we fine-tune the quantized diffusion model to improve the performance. Here both the weights and scaling factors are optimized. Additionally, we notice that training the quantized model in a distillation fashion using the full-precision model yields better performance than training directly with vanilla noise prediction. Furthermore, during distillation, it is crucial for the quantized model to be aware of CFG, _i.e._, text dropping is applied during distillation. Specifically, our training objective is as follows:

\[_{_{,}}^{}= _{t,}[\|}_{_{}}(t, _{t},)-}_{_{,}}(t, _{t},)\|],=P  U<p, \]

where \(P\) controls the text dropping probability during training and \(p\) is set as \(0.1\).

**Feature Distillation.** To further improve the generation quality of the quantized model, we distill the full-precision model at a more fine-grained level through feature distillation  as follows:

\[_{_{,}}^{}= _{t,}[\|_{_{}}(t, _{t},)-_{_{,}}(t, _{t},)\|], \]

where \(_{}()\) denotes the operation for getting features from the Down and Up blocks in UNet. We then have the overall distillation loss \(^{}\) in Stage-I as follows:

\[^{}=_{_{,}}^{ }+_{_{,}}^{}, \]

where \(\) is empirically set as \(0.01\) to balance the magnitude of the two loss functions.

**Quantization Error-aware Time Step Sampling.** The training of diffusion models requires sampling different time steps in each optimization iteration. We explore how to adjust the strategy for time step sampling such that the quantization error in each time step can be effectively reduced during training. We first train a \(1.99\)-bit quantized model with Eq. (8). Then, we calculate the difference of the predicted latent features between the quantized model and the full-precision model as \(_{t,}[}{_{t}}\|}_ {_{}}(t,_{t},)-}_{_{,}}(t,_{t},)\|^{2}]\), where \(t[0,1,,999]\) and \(_{t}\) is the noise scheduler (detailed derivation in App. E). The evaluation is conducted on a dataset with \(128\) image-text pairs. Fig. 4 shows the quantization error does not distribute equally across all time steps. Notably, _the quantization error keeps increasing as the time steps approach \(t=999\)_.

To mitigate the quantization error prevalent near the time steps \(t=999\), we propose a sampling strategy by utilizing a distribution specifically tailored to sample more time steps exhibiting the largest quantization errors, thereby enhancing performance. To achieve this goal, we leverage the Beta distribution. Specifically, time steps are sampled according to \(t(,)\), as shown in Fig. 4. We empirically set \(=3.0\) and \(=1.0\) for the best performance. Combining the strategy of time steps sampling with Eq. (8), we conduct the Stage-I training.

**Fine-tuning with Noise Prediction.** After getting the model trained with the distillation loss in Stage-I, we then fine-tune it with noise prediction, as in Eq. (2), in Stage-II. We apply a text dropping with probability as \(10\%\) and modify the distribution of time step sampling based on the quantization error, as introduced above. The reason we leverage two-stage fine-tuning, instead of combining Stage-I and Stage-II, is that we observe more stabilized training results.

## 5 Experiments

**Implementation Details.** We develop our code using _diffusers_ library2 and train the models with AdamW optimizer  and a constant learning rate as \(105\) on an internal dataset. For Stage-I,

Figure 4: More time steps are sampled towards where larger quantization error occurs.

we use \(8\) NVIDIA A100 GPUs with a total batch size of \(256\) to train the quantized model for \(20\)K iterations. For Stage-II, we use \(32\) NVIDIA A100 GPUs with a total batch size of \(1024\) to train the quantized model for \(50\)K iterations. During inference, we adopt the PNDM scheduler  with \(50\) sampling steps to generate images for comparison. Other sampling approaches (_e.g._, DDIM  and DPMSolver ) lead to the same conclusion (App. K).

**Evaluation Metrics.** We conduct evaluation on CLIP Score and FID on MS-COCO , TIFA , GenEval , and human evaluation on PartiPrompts . We adopt ViT-B/32 model  in CLIP score and the Mask2Former(Swin-S-8\(\)2)  in GenEval. App. I provides details for the metrics.

### Main Results

**Comparison with SD-v1.5.** Our quantized \(1.99\)-bits UNet consistently _outperforms_ the full-precision model across all metrics.

* \(30\)**K MS-COCO 2014 Validation Set.** For the CLIP score, as demonstrated in Fig. 4(a), attributed to the proposed mixed-precision recipe with the introduced initialization techniques and advanced training schemes in Stage-I, our \(1.99\)-bits UNet, with a storage size of \(219\)MB, achieves performance comparable to the original SD-v1.5. Following Stage-II training, our model surpasses the performance of the original SD-v1.5. With CFG scales ranging from \(2.5\) to \(9.5\), our model yields \(0.002 0.003\) higher CLIP scores.
* **TIFA.** As shown in Fig. 4(b), our \(1.99\)-bits model with Stage-I training performs comparably to the SD-v1.5. With the Stage-II training, our model achieves better metrics over the SD-v1.5.
* **GenEval.** We show the comparison results for GenEval in Fig. 4(c) (detailed comparisons of GenEval score are presented in Appn. L). Our model outperforms SD-v1.5 for all CFG scales.
* **Human Evaluation.** With the question: _Given a prompt, which image has better aesthetics and image-text alignment?_ More users prefer the images generated by our quantized model over SD-v1.5, with the ratio as \(54.4\%\). The results are shown in Fig. 6. We provide a detailed comparison in App. J.

**Comparison with Other Quantization Approaches.** Additionally, we conduct the experiments by comparing our approach with other works including LSQ , Q-Diffusion , EfficientDM ,

   Method &  & 3.5 & 5.5 & 7.5 & 9.5 & Average & \(\) \\  SD-v1.5 & 32 & 0.310 & 0.3159 & 0.3175 & 0.3180 & 0.3156 & - \\  QAT-Base & 2 & 0.2679 & 0.2793 & 0.2849 & 0.2869 & 0.2797 & - \\ \(+\)Balance & 2.32 & 0.2990 & 0.3059 & 0.3080 & 0.3086 & 0.3054 & +0.0257 \\ \(+\)Alternating Opt. & 2.32 & 0.3061 & 0.3108 & 0.3117 & 0.3115 & 0.3100 & +0.0046 \\ \(+\)Mixed/Caching & 1.99 & 0.3055 & 0.3129 & 0.3142 & 0.3145 & 0.3118 & +0.0018 \\ \(+\)Feat Dist. & 1.99 & 0.3086 & 0.3147 & 0.3167 & 0.3169 & 0.3142 & +0.0024 \\ \(+\)Time Sampling & 1.99 & 0.3098 & 0.3159 & 0.3181 & 0.3184 & 0.3156 & +0.0014 \\ \(+\)Fine-tuning & 1.99 & 0.3163 & 0.3192 & 0.3212 & 0.3205 & 0.3183 & +0.0027 \\   

Table 4: Analysis of our proposed methods measured using CFG scales, _i.e._, \(3.5\), \(5.5\), \(7.5\), and \(9.5\). We use LSQ  as the basic QAT method, which involves the training of weights and scaling factors of a uniformly \(2\)-bit quantized UNet. Then, we gradually introduce each proposed technique to evaluate their effectiveness. CLIP scores are measured on 1K PartiPrompts.

Figure 5: Comparison between our \(1.99\)-bits model _vs._ SD-v1.5 on various evaluation metrics with CFG scales ranging from \(2.5\) to \(9.5\). Ours-I denotes the model with Stage-I training and Ours-II denotes the model with Stage-II training.

   Method &  & CLIP score \\  SD-v1.5 & 32 & 0.3175 \\  LSQ & 2 & 0.2849 \\ Q-Diffusion & 4 & 0.3137 \\ EfficientDM & 2 & 0.2918 \\ Apple-MBP & 2 & 0.3023 \\  Ours & 1.99 & 0.3212 \\   

Table 3: Comparison with existing quantization methods, including LSQ , Q-Diffusion , EfficientDM , and Apple-MBP . The CLIP score is measured on 1K PartiPrompts.

and Apple-MBP , as shown in Tab. 3. Our model achieves a higher CLIP score compared with all other works and better performance than SD-v1.5.

### Ablation Analysis

Here we perform extensive analysis for our proposed method. We mainly evaluate different experimental settings using the CLIP score measured on 1K PartiPrompts .

**Analysis of the Proposed Techniques.** We adopt the LSQ  as the basic QAT method to update the weights and scaling factors of a uniform \(2\)-bit UNet with Min-Max initialization. Results are presented in Tab. 4 with the following details:

* **+Balance.** By adding a balance integer, a \(2\)-bit model that typically represents \(4\) integer values can now represent \(5\) integers, becoming a \(2.32\)-bit model by \((4+1)\) bits. The average CLIP score has significantly increased from \(0.2797\) to \(0.3054\).
* **+Alternating Opt.** By further utilizing the scaling factor initialization via alternating optimization, the average CLIP score of the \(2.32\)-bit model increases to \(0.3100\).
* **+Mixed/Caching.** By leveraging time embedding pre-computing and caching, we minimize the storage requirements for time embedding and projection layers by only retaining the calculated features. This significantly reduces the averaged bits. Combined with our mixed-precision strategy, this approach reduces the average bits from \(2.32\) to \(1.99\) bits and can even improve the performance, _i.e._, CLIP score improved from \(0.3100\) to \(0.3118\).
* **+Feat Dist.** By incorporating the feature distillation loss, _i.e._, Eq. (7), the model can learn more fine-grained information from the teacher model, improving CLIP score from \(0.3118\) to \(0.3142\).
* **+Time Sampling.** By employing a quantization error-aware sampling strategy at various time steps, the model focuses more on the time step near \(t=999\). With this sampling strategy, our \(1.99\)-bits model performs very closely to, or even outperforms, the original SD-v1.5.
* **+Fine-tuning.** By continuing with Stage-II training that incorporates noise prediction, our \(1.99\)-bits model consistently outperforms the SD-v1.5 across various guidance scales, improving the CLIP score to \(0.3183\).

**Effect of \(\) in Mixed-Precision Strategy.** Tab. 5 illustrates the impact of the parameter size factor \(\) (as discussed in Sec. 3.3) in determining the optimal mixed precision strategy. We generate six different mixed precision recipes with different \(\) with 20K training iterations for comparisons. Initially, we explore the mixed precision strategy determined with and without the parameter size factor. Setting \(=0\) results in \(N^{-}=1\), indicating that the mixed precision is determined without considering the impact of parameter size. The results show that neglecting the parameter size significantly degrades performance. Further, we empirically choose \(=0.3\) in our experiments after comparing different values of \(\).

**Effect of \(\) of Distillation Loss.** Tab. 6 illustrates the impact of the balance factor \(\) for loss functions in Eq. (8). We empirically choose \(=0.01\) in our experiments after comparing the performance.

**Effect of \(\) in Time Step-aware Sampling Strategy.** Tab. 7 illustrates the impact of the \(\) for different Beta sampling distribution. As analyzed in Sec. 4.2, the quantization error increases near \(t=999\). To increase sampling probability near this time step, Beta distribution requires \(>1\) with \(=1\). A larger \(\) enhances the sampling probability near \(t=999\). Compared to \(=1.5\) and \(=2.0\), \(=3.0\) concentrates more on later time steps and achieves the best performance. We choose \(=3.0\) in our experiments.

**Analysis for Different Schedulers.** One advantage of our training-based quantization approach is that our quantized model consistently outperforms SD-v1.5 for various sampling approaches. We conduct extensive evaluations on TIFA to show we achieve better performance than SD-v1.5 for using both DDIM  and DPMSolver  to perform the sampling. More details are shown in App. K.

**FID Results.** As stated in SDXL  and PickScore , FID may not honestly reflect the actual performance of the model in practice. FID measures the average distance between generated imagesand reference real images, which is largely influenced by the training datasets. Also, FID does not capture the human preference which is the crucial metric for evaluating text-to-image synthesis. We present FID results evaluated on the 30K MS-COCO 2014 validation set in Fig. 7. Our Stage-I model has a similar FID as SD-v1.5. However, as training progresses, although our Stage-II model is preferred by users, its FID score is higher than both Stage-I and SD-v1.5.

## 6 Conclusion

To enhance the storage efficiency of the large-scale diffusion models, we introduce an advanced weight quantization framework, BitsFusion, which effectively compresses the weights of UNet from SD-v1.5 to \(1.99\) bits, achieving a \(7.9\) smaller model size. BitsFusion even outperforms SD-v1.5 in terms of generation quality. Specifically, we first conduct a comprehensive analysis to understand the impact of each layer during quantization and establish a mixed-precision strategy. Second, we propose a series of effective techniques to initialize the quantized model. Third, during the training stage, we enforce the quantized model to learn the full-precision SD-v1.5 by using distillation losses with the adjusted distribution of time step sampling. Finally, we fine-tune the previous quantized model through vanilla noise prediction. Our extensive evaluations on TIFA, GenEval, CLIP score, and human evaluation consistently demonstrate the advantage of BitsFusion over full-precision SD-v1.5.