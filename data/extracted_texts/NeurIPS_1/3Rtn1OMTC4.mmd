# Spatiotemporal Predictive Pre-training

for Robotic Motor Control

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Robotic motor control necessitates the ability to predict the dynamics of environments and interaction objects. However, advanced self-supervised pre-trained visual representations (PVRs) in robotic motor control, leveraging large-scale egocentric videos, often focus solely on learning the static content features of sampled image frames. This neglects the crucial temporal motion clues in human video data, which implicitly contain key knowledge about sequential interacting and manipulating with the environments and objects. In this paper, we present a simple yet effective robotic motor control visual pre-training framework that jointly performs spatiotemporal prediction with dual decoders, utilizing large-scale video data, termed as **STP**. STP adheres to two key designs in a multi-task learning manner. First, we perform spatial prediction on the masked current frame for learning content features. Second, we utilize the future frame with an extremely high masking ratio as a condition, based on the masked current frame, to conduct temporal prediction of future frame for capturing motion features. This asymmetric masking and decoder architecture design is very efficient, ensuring that our representation focusing on motion information while capturing spatial details. We carry out the largest-scale BC evaluation of PVRs for robotic motor control to date, which encompasses 21 tasks within a real-world Franka robot arm and 5 simulated environments. Extensive experiments demonstrate the effectiveness of STP as well as unleash its generality and data efficiency by further post-pre-training and hybrid pre-training. Our code and weights will be released for further applications.

## 1 Introduction

In NLP and CV, adapting pre-trained foundation models from large-scale data to various downstream tasks has seen great success. For example, pre-trained visual representations using self-supervised  or weakly-supervised  methods exhibit strong generalization ability for visual understanding. However, in robot learning, due to data scarcity and homogeneity, some groundbreaking methods  resort to training from scratch only using domain-specific data. Recently, inspired by the success of transfer learning in CV, many works  have explored developing a pre-trained visual representation (PVR) using large-scale out-of-domain data for various robotic motor control tasks. Currently, one successful paradigm  is to use large-scale egocentric video datasets  and train vanilla vision transformers (ViT)  based on MAE , which exhibits excellent learning efficiency and generalization ability for learning policy from raw pixel. Among them, the Ego4D  dataset offers numerous first-person human-object interaction scenes and good motion clues. We argue that although learning static spatial structure priors from task-relevant pre-training data sources is crucial, designing a more relevant self-supervised proxy task for motor control should not be overlooked. Therefore, in this paper, we aim to develop a more relevant self-supervised proxy task for robotic motor control representation learning.

Robotic motor control typically requires fine-grained spatial localization and relatively dense semantics. With its ability to effectively capture low-level geometry and space structure, MAE  pre-training excels at this task. However, is dense spatial content sufficient for robotic motor control? Some neuroscientific studies [50; 21; 88] suggest the brain's different areas or cells show specialization. Some are dedicated to processing the information of temporal object motion, while others focus on static spatial details. Their combination results in subjective pattern perception. Inspired by this finding, we hypothesize that an effective robotic motor control pre-training proxy task should require joint learning of spatial content features and temporal motion features. However, current methods [73; 59; 19] use MAE pre-training with image frames from human videos, capturing only static content features. They overlook the temporal motion clues in human videos, which implicitly contain key knowledge about sequential interaction with environment and manipulation of objects. Therefore, we aim to bridge this gap by incorporating these motion clues into our proxy task.

Based on the analysis above, the most critical challenge is the absence of action annotations in human video data for modeling object motion. To model interaction and manipulation actions from actionless video data, we propose to implicitly capture them by predicting future frame pixels based on current frame. However, predicting the future frame without any conditions could contain high uncertainty and be extremely difficult. Therefore, we propose to use the future frame with an extremely high masking ratio as a prompt condition, specifically 95%, which serves to reveal some behavior and dynamic priors, i.e. what to do and how to do it. In the experiments section, we will further explore different condition alternatives, including language narration and their combination. Additionally, directly and simply executing temporal prediction could lead the model to overlook static spatial details, and it is also not efficient enough. Therefore, another technical contribution of STP is to jointly perform spatial prediction by masking the current frame with 75% masking ratio. In summary, we present **STP**, a multi-task self-supervised pre-training framework through spatiotemporal predictive learning. Our STP asymmetrically mask the current frame and future frame from a video clip, using a spatial decoder to conduct spatial prediction for content learning and a temporal decoder to conduct temporal prediction for motion learning. This asymmetric masking and decoder architecture design ensures that our pre-trained encoder focusing on motion information while capturing spatial details.

Subsequently, we establish our evaluation scheme. Currently, how to adapt pre-trained visual representations for robotic motor control still remain an open question. Considering the expensive cost of robot data collection or exploration, we employ a data-efficient paradigm of few-shot behavior cloning by learning from demonstrations (Lfd). To demonstrate the generalization ability of visual representation, our primary evaluation scheme involves freezing the visual encoder during policy training. Additionally, considering that fine-tuning ViT with few demonstrations might lead to overfitting and masked modeling exhibits excellent data efficiency [86; 102; 52] in domain-in data, we further follow the post-pre-training [7; 93; 59] paradigm to perform STP pre-training with task-specific data to achieve better results. It is noteworthy that different tasks do not share representation in this setting. Finally, we conduct the largest-scale BC evaluation of PVRs for robotic motor control to date to demonstrate the effectiveness of STP, which encompasses 21 tasks ( 2 real-world tasks and 19 simulation tasks across 5 environments). These simulation tasks are derived from the union of manipulation and locomotion tasks from prior works [65; 59].

We make the following **four contributions**: **(1)** We present STP, a _self-supervised_ visual pre-training framework for robotic motor control, which jointly conducts spatiotemporal prediction with _asymmetric masking and decoder architecture design_ for content and motion features learning. **(2)** We further expand STP by performing hybrid pre-training with ImageNet-MAE and post-pre-training with task-specific data, unleashing its _generality_ and _data efficiency_. **(3)** To our best knowledge, we conduct the _largest-scale BC evaluation_ of PVRs for robotic motor control to date to demonstrate the effectiveness of STP. **(4)** Our experiments yield some insightful observations. In temporal prediction, language does not significantly enhance performance. Instead, _single-modality self-supervised paradigm_ achieves the best results. This finding is highly encouraging for self-supervised robotic motor control representation learning. Moreover, in the few-shot BC setting, naively scaling up model size does not necessarily lead to improved outcomes. Finally, incorporating _more diverse_ data and _domain-in_ data into the pre-training can further enhance performance.

## 2 Related Work

**Pre-trained Visual Representation Learning.** Large-scale visual representation pre-training are continually empowering computer vision. The primary supervised learning methods include learning image recognition [40; 87] from ImageNet  and learning multi-modal alignment  from image-text pairs. Currently, self-supervised learning methods are enjoying significant popularity, primarily falling into two main categories. The first category utilizes contrastive learning [39; 15; 14] technique or joint-embedding architecture  to learn view-invariance. The second category performs masked modeling [7; 38; 100; 95; 4; 2] and predict the pixel or representation of invisible parts in space. In addition, some methods [106; 67; 8] have also proposed to combine different optimization objectives in a multi-task learning manner. Recently pre-trained visual representation learning for robotic motor control have bee rapidly developing [69; 65; 73; 99; 58; 57; 46; 59; 19]. These methods cover different backbones (ResNet , ViT ), different policy learning methods (reinforcement learning , behavior cloning [69; 65; 59], reward function  and task specification ), different adaptation schemes (linear probing [69; 65; 46; 59], fine-tuning  and designing adapters [78; 56]), and different evaluation environments (diverse simulation benchmarks). At present, it is still unclear how these factors collectively influence the performance. In this paper, we choose scalable vanilla vision transformer  as our backbone and data-efficient few-shot behavior cloning paradigm to conduct policy learning, while ensuring the backbone is frozen during policy training.

**Temporal Predictive Learning.** Early works once explored representation learning through future prediction, encompassing image , video [35; 80] and audio . VideoMAE [86; 93] extend MAE  to 3D video architecture. Recently TrackMAE  and SiamMAE  predict the masked future frame based on unmasked current frame, leading to a better capture of temporal correspondence and achieving outstanding performance in object tracking and segmentation tasks. In robot learning, predicting future visual states primarily serves as a transition dynamic model such as World Models [62; 77] and Dreamer . [85; 9] predict the future visual states using goal image in robot data. GR-1  conducts language-conditioned video prediction for policy model pre-training in a frozen visual representation space.  proposed dynamics-aware representation learning, and [82; 72] employed forward dynamics for self-supervised pre-training. Some works explored to train video prediction models and utilize visual foresight , inverse dynamics models , goal-conditioned policy learning , and geometry estimation  methods for motor control, respectively.  fine-tuned pre-trained representations into dynamic and functional distance modules for manipulation tasks. Unlike these works, we utilize the public large-scale egocentric video data and employ masked spatiotemporal predictive learning as a _self-supervised proxy task (without any language or action annotations) for robotic motor control representation learning_, instead of designing elaborate architectures or methods for specific predictive tasks [28; 37].

**Vision-based Robot Learning.** Vision-based robot learning plays a crucial role in robotics community. Recently some related works focus on studying model architectures [44; 12; 47], observation spaces , downstream policy learning methods , sim-to-real transfer , designing adapters [78; 56], learning-from-scratch baseline , and affordance model [6; 105; 45; 60], in visuo-motor representation learning. Other related works [70; 5; 91; 101; 48] attempt to learn manipulation skills from small-scale and in-domain human videos. In addition, language-conditioned vision robot learning has received significant attention. Some works scale multimodal robotic data [42; 11; 34; 90; 24; 68; 84] or introduce Internet data and knowledge [81; 103; 10; 54; 43; 94; 64] for end-to-end robot learning. In our study, we pre-train a off-the-shelf visual representation from large-scale egocentric video datasets for robotic motor control tasks. Our method is more simple and general for different downstream tasks of motor control.

## 3 Method

In this section, we describe our method in details. First, we give an overview of our spatiotemporal predictive pretraining (STP) framework. Then, we give a technical description on our core components during pre-training: the masked image encoder and dual decoders scheme. Finally, we describe how to adapt our pre-trained encoder to downstream robotic motor control tasks.

### Overview of STP

As illustrated in Figure 1, our STP aims to pre-train an image encoder for robotic motor control from video datasets. This pre-trained image encoder is subsequently frozen and directly transferred to solve motor control tasks. Specifically, given a video dataset \(\), our goal is to learn an image encoder \(_{enc}\), that maps images to the visual representations. During pre-training and post-pre-training, \(\) represents large-scale out-of-domain videos and task-specific demonstration videos, respectively. After pre-training, we reuse \(_{enc}\) for downstream motor control policy learning. Specifically, the downstream task will require an agent to make sequential action decisions based on visual observations \(\). Instead of using the raw observation images as direct input like end-to-end policy learning from pixel, the agent will employ the pre-trained \(_{enc}\) to extract its visual state representation \(_{enc}()\) for the subsequent policy learning module.

### Masked Image Encoder

We first introduce the pipeline of our image encoder. Our image encoder processes image frames using a vanilla vision transformer . Given a image \(^{C H W}\), we initially process it by the patch embedding layer to obtain its token sequences \(\), where \(=\{P_{i}\}_{i=1}^{N}\) and \(N\) is the the total token number, (e.g., N = 196 for a 224 x 224 image with a patch size of 16 x 16). Then we add the fixed 2D sine-cosine positional embeddings for all tokens. Following this, we mask and remove a part of tokens, according to a randomly generated masking map \(()\), where \(\) is the masking ratio. The encoder applies several transformer blocks (consisting of a global self-attention layer and a FFN layer) on all unmasked tokens: \(=_{enc}(^{u})\), where \(^{u}=\{T_{i}\}_{i(1-())}\). During this process, a [CLS] token is added at the beginning.

Then we describe our encoding process during pre-training. We randomly sample two frames from a video clip based on an interval: the current frame \(}\) and the future frame \(}\). Following the above pipeline, we randomly generate two asymmetric masking maps for the current frame and the future frame, denoted as \(_{c}=_{c}(^{c})\) and \(_{f}=_{f}(^{f})\), respectively. Each of these maps has a different masking ratio. We then use these maps to separately process the two frames and obtain their features, \(_{c}\) and \(_{f}\). As analyzed above, our STP aims to jointly learn content and motion features by spatiotemporal predictive learning. For content feature learning, we follow MAE , masking a portion of the current frame based on \(_{c}\), with \(^{c}=75\%\), and predict the masked parts during the decoding process. This encourages the model to learn spatial and geometric structure priors from the current frame data through spatial reasoning. For motion feature learning, we establish an objective to predict the future frame based on the masked current frame. However, predicting the future frame without any conditions could be meaningless and extremely challenging. Therefore, we use the future frame with an extremely high masking ratio as a condition, specifically \(^{f}=95\%\), which reveals some behavior and dynamic priors. In the experiments section, we will further discuss different condition schemes, including language narration and the combination between them. In summary, our encoding process during pre-training can be formally described as follows:

\[_{c}=_{enc}(_{c},_{c}),\\ _{f}=_{enc}(_{f},_{f}). \]

Figure 1: **STP framework. Left: During pre-training, we sample the current frame and the future frame from the video clip, and carry out spatiotemporal predictive pre-training. Right: During motor control tasks evaluation, we freeze the pre-trained encoder to extract visual state representations and discard the decoders.**

### Dual Decoders

To jointly capture static content and object motion features for better spatiotemporal understanding, our STP present a dual decoders scheme to predict both the pixel of current and future frame simultaneously in a multi-task learning manner. As shown in Figure 1, our dual decoder scheme includes a spatial decoder \(_{dec\_s}\) for spatial prediction and a temporal decoder \(_{dec\_t}\) for temporal prediction. We firstly give a technical description on them, respectively. Then we describe how we combine them into our final method.

**Spatial Decoder.** To capture static content features, our spatial decoder is solely utilized for processing the current frame visual feature. Specifically, after obtaining the masked current frame visual feature \(_{c}\), we concatenate it with some learnable masking tokens, leading to the formation of \(_{c}^{d}=_{c}\{_{i}\}_{i_{c}}\), where \(_{c}\) is the current frame masking map. Then, each of these tokens further adds a corresponding positional embedding. Subsequently, \(_{c}^{d}\) undergoes decoding in the decoder and is continuously updated. The architecture of the spatial decoder block aligns with the standard transformer encoder block, comprised of a global self-attention layer and a FFN layer. Finally, with the decoded token sequence \(_{c}^{d}\), our spatial decoder predicts the invisible tokens of the current frame \(}_{c}^{d}\), operating under the current frame masking map \(_{c}\).

**Temporal Decoder.** To capture motion features, our temporal decoder jointly processes the current frame and the future frame which serves as the temporal prediction condition. To elaborate, we firstly obtain the masked current frame feature \(_{c}\) and the masked future frame feature \(_{f}\). We then concatenate \(_{f}\) with the masking tokens that have the positional embedding added, resulting in \(_{f}^{d}\). Following this, \(_{f}^{d}\) and \(_{c}\) interact within the temporal decoder for decoding. The architecture of our temporal decoder block is in alignment with the standard transformer decoder block , consisting of a self-attention layer, a cross-attention layer, and a FFN layer, as shown in Figure 2 (b). During decoding, the self-attention layer and FFN are solely used to process \(_{f}^{d}\). For the cross-attention layer, \(_{f}^{d}\) is continuously updated as the query, while \(_{c}\), acting as the key and value, is kept constant. Compared to standard architecture, it ensures that the past frame representation space will not be updated in the temporal decoder and are specifically used for temporal correlation and prediction. This asymmetric interact architecture not only achieves more efficient training but also produces better results. Finally, with the decoded token sequence \(_{f}^{d}\), our temporal decoder predicts the invisible tokens of the future frame \(}_{f}^{d}\), operating under the future frame masking map \(_{f}\).

**Multi-task Predictive Learning.** As mentioned above, our STP jointly conducts spatiotemporal prediction by asymmetric masking ratio and dual decoders scheme, the whole decoding pipeline can be formally described as follows:

\[}_{c}^{d}=_{dec\_s}(_{c}^{d}),\\ }_{f}^{d}=_{dec\_t}(_{c},_{f}^{d}).  \]

Our loss function is the mean squared error (MSE) loss between the normalized masked pixels and the predicted pixels. So our loss function \(\) is as follows:

\[=(}_{c},_{c})+(}_{f},_{f}). \]

### Downstream Policy Learning

To enable data and computation efficiency during the policy learning process, we adopt the paradigm of few-shot behavior cloning by learning from demonstrations (Lfd), and we keep the image encoder

Figure 2: Temporal decoder design. **(a)** Standard joint-self architecture. **(b)** Our self-cross architecture.

frozen. Concretely, for each task, we are given offline expert demonstrations \(=\{_{1},...,_{n}\}\), where each \(_{i}\) is a trajectory of robot observations and actions, denoted as \(_{i}=[(o_{0},a_{0}),,(o_{T},a_{T})]\). Based on the \(\), we train a policy model, \(_{}(a|(_{enc}(o)))\), parameterized by \(\), which maps from robot's state representations to actions. Here, \(\) represents an optional concatenation operation that effectively fuses multi-view and multi-frame visual features, along with the robot's proprioceptive state in the channel dimension. We optimize the \(_{}\) through a standard behavior cloning MSE loss:

\[_{}_{(,)}}(a,_{ }((_{enc}(o)))). \]

## 4 Experiments

### Implementation on Pre-training

We execute pre-training with data from EgoVLP  for comprehensive ablation and fair comparison. It processes untrimmed videos of Ego4D and filters out that miss language narrations and belong to validation or test sets, resulting in a total of 3.8 million clips, called as Egoclip. In pre-training, we sample a frame pair from each clip for training. As for all experiments, we employ ViT  as backbone. Additionally, we maintain consistency with prior works [73; 59], directly using the [CLS] token as the global representation. The pre-training hyperparameters can be found in section A.3.

### Implementation on Downstream Policy

**Evaluation Scheme.** Following popular settings on PVRs for robotic motor control [65; 46; 59], for each task, we learn a single policy \(\) which is structured as a MLPs network. The policy models utilize both the history of visual observation embeddings and optional robot proprioceptive as inputs, subsequently generating executable actions as outputs.

**Simulation Tasks.** We select the union of manipulation and locomotion tasks from prior works [65; 59] for evaluation, encompassing 19 tasks across 5 simulated environments. These include Meta-World  (Assembly, Bin-Picking, Button-Press, Drawer-Open, and Hammer), Franka-Kitchen  (Sliding Door, Turning Light On, Opening Door, Turning Knob, and Opening Microwave), Adroit  (Relocate and Reorient-Pen), DMControl  (Finger-Spin, Reacher-Hard, Cheetah-Run, Walker-Stand, and Walker-Walk), and Trifinger  (Reach-Cube and Push-Cube). More detailed simulation evaluation details can be found in section A.4.

**Real-World Tasks.** In our real-world experiments, we evaluate contact-rich picking and pouring tasks using a Franka Emika Research 3 robot arm in a tabletop environment, ensuring no duplication with simulation Franka-Kitchen . For each task, we collect 100 noise demonstrations for training, and we conduct 20 trials per task during evaluation phase. The robotic arm and objects have different initial pose between training and testing. The evaluation demonstrations of our real-world tasks is shown in Figure 3. Please see section A.5 for more real-world setup details.

Figure 3: **The evaluation demonstrations of our real-world tasks.** For picking, the robot arm needs to pick up the bowl on the desktop. For pouring, the robot arm needs to pour the ingredients from the bowl into the pot.

### Performance on Downstream Simulation Tasks

In this section, we mainly analyze the performance of some pre-trained image representations on reproducible simulation tasks. Specifically, we first evaluate the following models: (1) public DINOv2  that combines masked image modeling with self-distillation on large-scale image datasets; (2) public CLIP  that conducts contrastive learning on large-scale image-text pairs; (3) R3M trained based on Egoclip ; (4) public VC-1 ; (5) MAE trained based on Egoclip; (6) STP trained based on Egoclip. (7) STP that conducts hybrid pre-training with initialization using ImageNet-MAE . Among them, (1) and (2) achieve excellent performance on core visual understanding tasks using zero-shot or linear probing evaluation settings. (3) and (4) utilize egocentric videos for robotic motor control. (5), (6) and (7) are used for fair comparison and exploring the potential benefits of STP from more diverse image data, respectively. The experimental results are presented in Table 1. Consistent with prior findings , there is not a universal foundation model that performs optimally across all benchmarks. However, on the whole, the MAE method is superior due to its effective modeling of low-level geometry and spatial structure, especially for the MetaWorld tasks that demand fine-grained control. Another intriguing observation is that MAE outperforms in the Franka-Kitchen and Adroit tasks. We believe that this could be due to its relatively weaker semantic representation. Under a fair comparison, our STP outperforms MAE by 4.1 (59.6 \(\) 63.7), and additionally benefits from a more diverse image data, improving by 0.5 (63.7 \(\) 64.2). This is attributed to that our STP not only captures static content features but also effectively models motion information by extracting temporal clues from videos of interactions and manipulations with the environment and objects. Additionally, we provide the visualization of the attention maps (model (5) and (6)) of several specific tasks in Figure 4. The results indicate that, on top of effectively capturing spatial information, our method further encourages the model to focus on motion areas or objects, thereby providing a more _sparse and compact_ representation for downstream low-data BC paradigm.

Next, we also evaluate and compare the adaptation results of our representations to downstream motor control tasks. Specifically, we evaluate following settings: (a) The MAE pre-trained representation undergoes further MAE post-pre-training with task-specific data, and is frozen during policy training; (b) The STP pre-trained representation undergoes further STP post-pre-training with task-specific data, and is frozen during policy training; (c) The STP pre-trained representation undergoes end-to-end fine-tuning with task-specific data; (d) STP pre-training is performed directly using task-specific data and the resulting representation is frozen during policy training. The results show that end-to-end fine-tuning fails to yield the best results, suggesting that naively fine-tuning VIT-base could still lead to overfitting under few-shot behavior cloning scheme. Conversely, (a) and (b) achieve competitive results, with our STP achieving a 3.9 (72.5 \(\) 76.4) improvement on the weight average success rate than MAE, further demonstrating the effectiveness and data efficiency of our STP for in-domain data. In addition, the comparison between (a) and (d) also proves the effectiveness of pre-training with out-of-domain data. Finally, we also scale up both MAE and our STP to ViT-L/16, and the results still demonstrate the superiority of STP. Among them, compared to ViT-B/16, ViT-L/16 brings a smaller performance improvement, which may be due to the task's performance saturation. However, the ViT-L/16 of STP does not show improvement in Meta-World and Trifinger, indicating that simply

Figure 4: **Attention Visualization.** We use the [CLS] token as query, average the attention of all heads at the last layer of the frozen ViT encoder, and perform min-max normalization. We then upsample the attention map and overlay it on the original image, where the size of the attention value is directly proportional to the intensity of the yellow light. **Top:** MAE pre-training. **Bottom:** STP pre-training.

scaling up model capacity does not necessarily lead to performance gains. In the few-shot BC setting, there is a risk of overfitting in both policy and backbone training.

### Ablation on Downstream Simulation Tasks

In this section, we perform extensive ablation studies to further demonstrate the effectiveness of our joint spatial and temporal prediction, as well as temporal prediction condition design. In addition, we also study the influence of temporal decoder architecture design and future frame sampling strategy.

**Current frame masking.** The design of the current frame masking is crucial. On one hand, similar to MAE , masking some patches and predicting the missing parts can effectively promote the learning of image content features. On the other hand, the visible patches of the current frame need to interact with the condition to predict the future frame. Specifically, we mask the current frame at masking rates of 75%, 50%, and 0%, respectively, and optionally predict the missing parts through the spatial decoder. The results are shown in Table 2 (a). From results, we see that the masking ratio of 75% and performing spatial prediction still lead to the best performance. This demonstrates the importance of retaining MAE  for content features learning, especially for low-level manipulation in Meta-World, while a current frame with a high masking ratio (75%) is sufficient to interact with other conditions to predict the future frame.

**Temporal prediction condition design.** Subsequently, we discuss the influence of temporal prediction condition design. We implicitly model motion in actionless video data by predicting the pixels of the future frame. A direct and simple idea is to use language narration as a condition. The text tokens can be flexibly utilized as inputs to ViT , forming a multimodal encoder. Language narration

Table 1: Performance comparations of visual representations on simulation benchmarks. We report the average score across all tasks for each simulation environment. DINOv2 uses **ViT-B/14**, CLIP uses **ViT-B/32**, and unless otherwise specified, others use **ViT-B/16**. Mt-Wd, Fri-Ki, DMC, Adro, Tr-fi, and WA respectively represent MetaWorld, Franka-Kitchen, DMControl, Adroit, Trifinger, and weight average. * denotes that public VC-1 samples image frmas form full Ego4D dataset.

Table 2: The ablation experiment results. Me, Fra, DMC, Adr, Tri, and WA respectively represent MetaWorld, Franka-Kitchen, DMControl, Adroit, Trifinger, and weight average. All models use **ViT-B/16**.

provides a high-level behavior description, but lacks low-level visual dynamic priors for pixel-level prediction. However, leaking part of the future frame can effectively provide these priors. In order to explore how to construct a more meaningful temporal prediction proxy task, we compare the following schemes: (1) only language narration, (2) masking 95% of the future frame, (3) masking 90% of the future frame, (4) masking 95% of the future frame and language narration, and (5) masking 95% of the future frame and language narration, but the language is added in the temporal decoder, instead of being fused with the visible image patches in the multimodal encoder. We tokenize all language narration by pre-trained DistilBERT . The results are shown in Table 2 (b). From results, we see that using only language as a prediction condition leads to a significant decline in performance, while leaking a small amount of future frame (masking 95%) in the temporal decoder can achieve competitive results. As for joint conditions of language and future frame with 95% masking ratio, adding language in the encoder is better than in the decoder. Additionally, adding language performs better on DMControl (64.1 vs. 62.1) and Trifinger (70.8 vs. 69.3), while not adding language performs better on Meta-World (92.0 vs. 91.0), Franka-Kitchen (40.9 vs. 37.7) and Adroit (48.0 vs. 46.7). We speculate the reasons for language hurts performance are as follows: (i) The input gap (multi-modal and single-modal) between upstream and downstream; (ii) Extra language in ViT may result in the loss of some fine-grained information capture. Furthermore, the latter does not require language supervision, and can provide a more scalable self-supervised solution.

**Temporal decoder design.** We also investigate the impact of the temporal decoder design. Specifically, we consider two types of decoder blocks. One is the joint-self architecture, as shown in Figure 2 (a), and similar joint architecture are adopted in [26; 102]. The other is the self-cross architecture, as shown in Figure 2 (b), and similar cross architecture are adopted in [3; 33]. We consider the following settings: (1) 8 joint-self decoder blocks, (2) 12 joint-self decoder blocks, (3) 8 self-cross decoder blocks. Among them, setting (2) and (3) have similar amounts of parameters for a fairer comparison. The results are shown in Table 2 (c). The results demonstrate the importance of maintaining a fixed representation space of the past frame during temporal prediction.

**Frame sampling strategy.** Finally, we investigate the impact of the sampling strategy between the current frame and future frame. The difficulty of temporal prediction is directly proportional to the frame interval values. We establish four settings where we fix the sampling intervals at 8, 16, and 24 respectively, and for the fourth setting, we randomly select an interval within the range of [8; 24]. The results are shown in Table 2 (d). The results show that an interval of 16 achieves the best balance for building temporal prediction proxy task.

### Performance on Downstream Real-world Tasks

In this section, we report our experiment results on real-world picking and pouring tasks. We report the average success rate for each task. Specifically, we compare STP with the baseline MAE, both of which are trained on out-of-domain videos and kept frozen during policy training. The results are shown in Table 3. From the results, it can be seen that STP has achieved significant advantages in the pouring task. It can more accurately align with the moving bowl and the pot. In addition, although MAE and STP have a same success rate in picking tasks, STP tends to execute grasping in a better position. This indicates that the trend and conclusion of our STP are consistent in both simulation and the real-world, which also aligns with the findings of .

## 5 Conclusion

In this work, we have proposed the STP, a simple, efficient and effective self-supervised visual representation pre-training framework for robotic motor control. Our STP jointly performs spatiotemporal predictive learning on large-scale videos within a multi-task learning manner. Our STP captures content features by predicting the invisible areas within the masked current frame, and simultaneously captures motion features by using a future frame with an extremely high masking ratio as a condition to predict the invisible areas within that future frame. We carry out the largest-scale BC evaluation of PVRs for robotic motor control to date to demonstrate the effectiveness of STP. Furthermore, as for pre-training data, we also prove that extending STP to hybrid pre-training and post-pre-training could further unleash its generality and data efficiency.

   Method & Picking & Pouring & Average \\  MAE & 65.0 & 45.0 & 55.0 \\  STP & 65.0 & 65.0 & **65.0** \\   

Table 3: Performance comparations on real-world tasks.