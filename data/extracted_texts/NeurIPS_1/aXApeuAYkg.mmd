# CA-SSLR: Condition-Aware Self-Supervised Learning Representation for Generalized Speech Processing

Yen-Ju Lu\({}^{}\), Jing Liu, Thomas Thebaud\({}^{}\), Laureano Moro-Velazquez\({}^{}\), Ariya Rastrow, Najim Dehak\({}^{}\), Jesus Villalba\({}^{}\)

\({}^{}\)Center for Language and Speech Processing, Johns Hopkins University

{ylu125, tthebau1, laureano, ndehak3, jvillal7}@jhu.edu

###### Abstract

We introduce Condition-Aware Self-Supervised Learning Representation (CA-SSLR), a generalist conditioning model broadly applicable to various speech-processing tasks. Compared to standard fine-tuning methods that optimize for downstream models, CA-SSLR integrates language and speaker embeddings from earlier layers, making the SSL model aware of the current language and speaker context. This approach reduces the reliance on the input audio features while preserving the integrity of the base SSLR. CA-SSLR improves the model's capabilities and demonstrates its generality on unseen tasks with minimal task-specific tuning. Our method employs linear modulation to dynamically adjust internal representations, enabling fine-grained adaptability without significantly altering the original model behavior. Experiments show that CA-SSLR reduces the number of trainable parameters, mitigates overfitting, and excels in under-resourced and unseen tasks. Specifically, CA-SSLR achieves a 10% relative reduction in LID errors, a 37% improvement in ASR CER on the ML-SUPERB benchmark, and a 27% decrease in SV EER on VoxCeleb-1, demonstrating its effectiveness.

## 1 Introduction

The emergence of Self-Supervised Learning Representations (SSLRs) models has revolutionized speech processing, setting new standards in the field. Pioneering models like Wav2vec 2.0 Baevski et al. (2020), HuBERT (Hsu et al., 2021), and WavLM (Chen et al., 2022) leverage unlabeled audio data to learn rich representations of spoken language. These models are pivotal in a wide range of applications, including Speech Recognition (ASR) (Chang et al., 2021), Speaker Verification (SV) (Chen et al., 2022, Tak et al., 2022), Language Identification (LID) (Bartley et al., 2023), and Speech Translation (ST) (Tang et al., 2022). Benchmarks such as SUPERB (Yang et al., 2021) and ML-SUPERB (Shi et al., 2023) have been crucial in evaluating SSL model performance, providing standardized tasks.

Although SSLR training approaches combine speech from various sources, these models learn representations solely from unpaired audio-only data. When extending SSLR features to multilingual scenarios and low-resource languages, unsupervised training limits the model's ability to distinguish between different languages, resulting in unified features for all languages. Additionally, labeling all SSL training data with language and speaker information requires significant human effort and is impractical. Thus, a post-training conditioning approach is more favorable. In other fields, methods like (Zhang et al., 2023) and IP-Adaptor (Ye et al., 2023) in image processing, and CTRL (Keskar et al., 2019) in NLP, have successfully integrated conditioning into pretrained models, demonstrating potential applications for speech processing.

In response to these challenges, we propose Condition-Aware Self-Supervised Learning Representation (CA-SSLR), a generalist conditioning model applicable to various speech-processing tasks suchas language identification, multilingual speech recognition, and speaker verification. Unlike standard adaptation methods that heavily revise the SSLR for downstream tasks, CA-SSLR minimally adjusts the pretrained model by integrating language and speaker embeddings from earlier layers, making the SSLR aware of the current language and speaker context. This technique enables the creation of models that perform multiple tasks with a single adapted SSL encoder by strategically injecting conditional adapters into each encoder block while keeping the pretrained encoder weights frozen. CA-SSLR employs a hierarchical self-adaptation structure in which adapters at each layer leverage intermediate task-specific embeddings derived from lower layers. Through attention mechanisms and linear modulation, CA-SSLR dynamically adjusts scaling and biasing, effectively tailoring the model's response to language and speaker contexts. Our initialization strategy enables the conditioning module to perform identity transformations, preserving the original model behavior when introducing new conditions. By applying channel-wise linear modulation and time-wise scaling without mixing channel dimensions, CA-SSLR avoids introducing new cross-channel couplings, thereby mitigating overfitting and catastrophic forgetting. We demonstrate its versatility and efficiency on three widely studied multilingual speech processing tasks--ASR, LID, and SV.

The key contribution of this work is a novel approach to condition SSLRs using limited supervised labels, resulting in generalized speech representations that maintain the underlying model's behavior while improving performance with minimal additional parameters. Specifically, CA-SSLR offers:

* **Hierarchical Dynamic Conditioning**: We develop attention-based conditional adapters that are integrated throughout the SSL model. By periodically leveraging language- and speaker-specific information extracted from previous layers, CA-SSLR dynamically tailors the model's behavior at each time step, offering robust adaptation to multilingual tasks.
* **Preservation of Pre-trained Weights with Efficient Parameter Utilization**: CA-SSLR leverages the pre-trained encoder by introducing lightweight, channel-wise scala \(\) and bias \(\) adjustments. Since these adjustments do not mix information across different channels, the original encoder's behavior is minimally altered. This strategy ensures stable, parameter-efficient training while preserving the benefits of the pretrained model.
* **Harmonized Task Compatibility with Notable Performance Gains**: Our experiments show that CA-SSLR effectively reduces trainable parameters, mitigates overfitting, and excels in under-resourced and unseen tasks. Notably, CA-SSLR achieves a 27% relative reduction in LID error rates, a 37% improvement in ASR CER on the ML-SUPERB benchmark, and a 27% decrease in SV EER on VoxCeleb-1. These results highlight CA-SSLR's effectiveness in enhancing multilingual SSLRs while also minimizing computational overhead for multitask fine-tuning.

## 2 Related Work

Self-supervised learning representation.Self-Supervised Learning (SSL) models, such as Wav2Vec 2.0 , HuBERT , and WavLM , leverage large-scale of unlabeled audio to learn expressive speech representations. These representations capture acoustic, phonetic, and semantic features, which can then be subsequently fine-tuned on smaller labeled datasets for tasks such as speech recognition , emotion recognition , vocal intensity classification , and speaker change detection . In multilingual settings, Wav2Vec 2.0-XLSR  extends pre-training to diverse languages for cross-lingual transfer, while mHuBERT  expands HuBERT to effectively handle multiple languages. These multilingual extensions facilitate a broad range of tasks, including speech recognition , spoken language understanding , and speech generation tasks , across various languages. Meanwhile, the scope of evaluation has broadened from monolingual benchmarks like SUPERB  to multilingual ones such as ML-SUPERB , reflecting growing demand for robust and scalable SSL representations.

Adaptation Methods.In many studies , the SSL representations (SSLR) remains frozen while a decoder is trained for a specific task. Because the same encoder is shared across tasks, this approach allows multiple tasks to be evaluated on a single speech signal with only one encoder run. However, such systems often underperform compared to approaches that adapt the SSLR to the target task--by fine-tuning the entire SSL encoder Chen et al. (2022), fine-tuning only selected layers, or adding lightweight adapters (Chen et al., 2023). Unfortunately, these methods require a separate encoder for each task, leading to a significant increase in computational load that scales linearly with the number of tasks.

Conditioning Pre-trained Models.Image processing has successfully integrated conditioning into pretrained models using methods such as ControlNet (Zhang et al., 2023) and IP-Adaptor (Ye et al., 2023). ControlNet enables precise control over generated images by incorporating additional inputs (e.g. edge maps or sketches), while IP-Adaptor utilizes lightweight adapter modules to modulate the model's behavior based on specific conditions--without modifying the pre-trained model's parameters. These techniques have achieved significant success and offer insights for potential applications in speech processing. Similarly, in Natural Language Processing (NLP), the Conditional Transformer Language Model (CTRL) (Keskar et al., 2019) introduces conditioning via control codes, guiding text generation based on attributes like style or domain and enabling efficient adaptation without extensive retraining. These successes in image processing and NLP demonstrate the potential for conditioning pre-trained SSLRs in speech applications.

Hierarchical Conditioning.Hierarchical modeling has also been explored in previous research. For instance, (Sanabria and Metze, 2018) propose a multi-task ASR model that applies intermediate representations by performing connectionist temporal classification (CTC) at multiple layers, each targeting different granularities. Lower layers predict character tokens, whereas higher layers predict subword units with increasingly larger vocabularies--scaling from 300 to 10k subword units in the final layer. (Chen et al., 2023) extend this idea by incorporating hierarchical conditional layers into the ASR decoder, using tokens predicted by earlier layers to guide the predictions of subsequent layers.

## 3 Methodology

We introduce Condition-Aware SSLR (CA-SSLR), a universal encoder designed for multiple downstream speech tasks. CA-SSLR enriches a pre-trained SSL model by integrating intermediate LID and SV predictions to condition and adapt subsequent layers dynamically. This strategy allows the model to capture key language and speaker attributes, progressively refining outputs and excelling in multilingual, multispeaker scenarios. Figure 0(a) illustrates the CA-SSLR architecture. It comprises a frozen SSL encoder, which is augmented with trainable conditioners and lightweight task-specific decoders. The conditioner modulates the encoder's hidden representations based on conditioning features drawn from intermediate LID and SV embeddings. This hierarchical conditioning mechanism enables dynamic adaptation to a variety of input conditions while keeping the pre-trained

Figure 1: CA-SSLR scheme and its time-channel attention conditioner. Only the conditioner and linear projections for the decoders are trainable, and all other parameters are frozen during adaptation.

parameters fixed. The subsections below describe (1) the conditioner module, (2) how it integrates into the overall architecture, and (3) the incremental training strategy for incorporating conditioning information without catastrophic forgetting.

### Channel-wise and Time-wise Attention Conditioner

A key component of CA-SSLR is the channel-wise conditioner (CC) or the time-channel attention conditioner (TCAC), which modulates the SSL encoder's hidden representations based on the conditioning features. As shown in Fig. 0(b), the TCAC receives latent representations \(^{(l)}^{C T}\) from layer \(l\) of the SSL encoder and a conditioning feature vector \(^{R}\), derived from intermediate LID or SV embeddings. The TCAC then produce modulated representations \(}^{(l)}\) by applying time- and channel-dependent scaling and bias:

\[}^{(l)}_{t,c}=(^{(l)}_{t,c},)= ^{(l)}_{t,c}(,^{(l)})^{(l)}_{t,c} +^{(l)}_{t,c}(,^{(l)}) \]

where \(t\) and \(c\) index time and channel dimensions, respectively. The scales \(^{(l)}_{t,c}\) and biases \(^{(l)}_{t,c}\) are products of time- and channel-dependent components:

\[^{(l)}_{t,c}(,^{(l)})=^{(l)}_{t}( ,^{(l)})^{(l)}_{c}()^{(l)}_{t,c}(,^{(l)})=^{(l)}_{t}(, ^{(l)})^{(l)}_{c}() \]

The channel-dependent scales \(^{(l)}^{C}\) and biases \(^{(l)}^{C}\) are computed by linear transformations of the conditioning feature, akin to feature-wise linear modulation (Perez et al., 2018):

\[^{(l)}()=^{(l)}_{}+^{(l)}_ {}^{(l)}()=^{(l)}_{}+ ^{(l)}_{}\, \]

while the time-dependent scales \(^{(l)}^{T}\) are obtained via an additive attention mechanism:

\[^{(l)}_{t}(,^{(l)})=^{}_{ }f(^{(l)}_{}^{(l)}_{t}\\ +^{(l)}_{}) \]

where \(f()\) denotes a ReLU nonlinearity, \(^{(l)}_{}^{C^{}(C+R)}\), \(^{(l)}_{}^{C^{}}\), and \(_{}^{C^{}}\). The conditioning feature \(\) is produced by processing intermediate embeddings \(^{E}\) from the LID or SV decoders, as \(=(+)\), where \(^{R E}\) and \(^{R}\) are shared linear parameters. If time-based modulation is not required, the simpler channel-wise conditioner (CC) can be used by retaining only the channel-dependent components \(\) and \(\). This flexibility allows the model to be adapted for tasks with varying complexity requirements. By integrating these conditioning methods, CA-SSLR dynamically adapts its internal representations according to language and speaker features, without altering the frozen pre-trained encoder's parameters.

Figure 2: Architecture of the CA-SSLR model employing hierarchical self-conditioning with Time-Channel Attention Conditioners (TCACs).

### Hierarchical Self-Conditioning in CA-SSLR

CA-SSLR employs a hierarchical self-conditioning mechanism within the SSL encoder layers, building on the TCAC module. In Figure 2, the SSL encoder is segmented into layer groups, with TCACs inserted after each attention module to modulate hidden representations using updated conditioning features. The model aggregates SSL features through a weighted sum of outputs from all preceding layer groups, and this aggregate is passed to the LID and SV decoders. The decoders extract and transform LID and SV embeddings via a linear layer followed by layer normalization, forming the conditioning feature \(\) used by the TCACs.

The conditioning feature \(\) is re-estimated at intervals--every three layers for LID and every six layers for SV--using aggregated SSL features from earlier groups. This hierarchical design progressively refines the model's representations, adapting to the input's language and speaker characteristics at different depths of the network. For instance, the initial SSL layer group captures fundamental linguistic and speaker cues, producing embeddings that subsequently condition the next layer group via the TCAC modules. This ongoing refinement allows the model to adapt dynamically based on intermediate predictions, leading to context-aware, flexible representations.

Each layer group has distinct TCAC parameters, enabling fine-grained scaling and bias adjustments at different stages of the model. Notably, only the TCAC modules and the linear projections for the decoders are trainable, while all other SSL encoder parameters remain frozen throughout the conditioning process. This design mitigates overfitting risks and accelerates training by confining the number of learnable parameters. As a result, the hierarchical self-conditioning mechanism equips the model to capture diverse aspects of input audio, making CA-SSLR a robust framework for comprehensive speech analysis.

### Incremental Training Strategy

Introducing new components into a pre-trained SSL encoder carries the risk of catastrophic forgetting. To address this, we use an incremental training strategy that gradually incorporates conditioning information. We initialize the TCAC parameters so that the initially modulated features match the original SSL outputs exactly. Specifically, we set \(_{t}=1\) for all \(t\), \(_{c}=1\), and \(_{c}=0\) for all \(c\). According to Eq. (1), this ensures that \(}_{t,c}^{(1)}=_{t,c}^{(1)}\) at the outset, allowing a seamless transition from the pre-trained model to the conditioned model. For multiple conditioning features (e.g. LID and SV), each task's parameters \((_{},_{},_{})\) and \((_{},_{},_{})\) are initialized in the same manner (i.e., \(=1\), \(=1\), and \(=0\)). We then combine them as follows:

\[_{}=_{}_{},_ {}=_{}_{},_{}=_{}+_{}\;. \]

By initializing each set of parameters to identity and then combining them, newly added conditioning tasks do not alter the features contributed by already integrated tasks. This allows the model to steadily integrate extra conditions without disrupting the knowledge acquired from previous tasks.

## 4 Experimental Setup

### Datasets

We use the ML-SUPERB benchmark  for both LID and ASR experiments. ML-SUPERB provides two data configurations: (i) 10-minute/language and (ii) 1-hour/language, covering 123 well-represented languages (_Normal_). Both configurations also include five training utterances for each of 20 low-resource languages (_Few-shot_)1. For the _Few-shot_ languages, we introduce an _Extended Few-shot_ condition, which increases their data to match the _Normal_ languages but provides only language labels without ASR transcripts. Our main goals are to address the severe LID constraints imposed by just five training utterances and to leverage the comparative ease of gathering LID-only data over fully transcribed data. For simplicity, we continue to refer to these extended data in subsequent LID experiments as _Few-shot_. As ML-SUPERB lacks speaker labels, we incorporate VoxCeleb2  to train the models on the SV task. VoxCeleb2 contains 1,092 hoursof speech from 5,994 speakers but lacks LID labels and ASR transcripts. SV performance is tested on the VoxCeleb1 original set. We augment the speech with Musan noise Snyder et al. (2015) and reverberation Ko et al. (2017) during SV training.

### Model Architecture

SSLR Models.Our system leverages top multilingual SSL backbones from the ML-SUPERB benchmark: (i) Wav2Vec2-XLSR (300M parameters) (Babu et al., 2021), trained on 128 languages, and (ii) mHuBERT (100M parameter) (Lee et al., 2021), trained on English, Spanish, and French VoxPopuli (Wang et al., 2021) data. Both have demonstrated their efficacy in processing a wide range of linguistic inputs and form the backbone of our system. We conduct experiments using S3PRL (Yang et al., 2021) and ESPnet (Watanabe et al., 2018). Our training set comprises data with (1) ASR+LID labels, (2) LID only labels, or (3) SV only labels; we computed the task-specific loss only when corresponding labels are available. Detailed information on the remaining hyperparameters is provided in the appendix2. Training a single model requires about one day on 2 A100 GPUs.

Speaker and Language Decoders.The speaker and language decoders are based on the ECAPAT-TDNN architecture (Desplanques et al., 2020). First, a convolutional layer projects the SSL representation to the decoder dimension (512 for LID, 1024 for SV). This is followed by one (for LID) or three (for SV) 1-dimensional SE-Res2Net (Gao et al., 2021) layers. Next, channel-wise attentive statistic pooling aggregates the frame-level features into an utterance-level vector, which is projected into lower-dimensional speaker embedding. We employ additive angular margin-softmax (Deng et al., 2019) with margin=0.3 for SV and margin=0.0 for LID. Large margin helps to create highly compact speaker representations (Villa1ba et al., 2022), while being detrimental in LID (Villa1ba et al., 2023). Both the SV and LID decoders use a weighted average of all SSL layers for final predictions. In CA-SSLR, the intermediate embeddings that generate conditioning embeddings are similarly drawn from a weighted average of the SSL layers up to that point. Because all SV and LID decoders share parameters, the total number of trainable parameters does not increase with more frequent recomputation of conditioning embeddings.

ASR decoder.The ASR decoder conforms to the framework set by the ML-SUPERB benchmark (Shi et al., 2023), facilitating comparable evaluations. A convolutional downsampling layer halves the SSL feature sequence duration. The resulting features are feed into a two-layer Transformer with 256-dims self-attention, eight heads, and a 1024-dims feed-forward layer. A linear output layer applies connectionist temporal classification (CTC) to predict multilingual character-level tokens.

## 5 Experiments and Results

### Generalization Ability on Unseen Tasks

Experiment Setting.We conducted experiments to evaluate the generalization capabilities of the adapted SSLR models on LID, ASR, and SV tasks. The SSLR models are adapted for one task (either LID or ASR) and then evaluated on both the adapted task and an unseen task. For LID adaptation, the SSLR is trained exclusively with LID labels. We compared three setups: full fine-tuning (LID-FT), Houlsby adaptors Houlsby et al. (2019) (LID-Houlsby), and our proposed condition-aware approach (LID-CA-XLSR\({}^{L}_{}\)). In this setup, we employed an additional LID decoder using the pre-trained SSLR to pre-generate language embeddings, which are then used to condition the SSLR model for a second inference pass. For ASR adaptation, the models are trained with ASR loss using three setups: full fine-tuning (ASR-FT), Houlsby adaptors (ASR-Houlsby), and our proposed hierarchical conditioning method with TCAC layers integrated into the SSLR model with single inference (ASR-CA-XLSR\({}^{L}\)). During ASR adaptation, the LID decoder is integrated into the SSLR model to provide conditioning features, but SV information is not included during training.

Results.In LID adaptation (Table 1a), both LID-FT and LID-Houlsby improved LID performance compared to the pre-trained SSL baseline. However, on the unseen ASR task, the fully fine-tuned SSLR encoder improved ASR CER by only 2%, while LID-Houlsby showed limited generalization, with CER improvements of 5.4% and 3.9% for normal and few-shot languages, respectively. OurLID-CA-XLSR\({}^{L}_{}\) method achieved substantially better generalization, improving ASR CER by 7.3% and 6.6% for normal and few-shot languages. In ASR adaptation (Table 0(b)), all models enhanced ASR performance, but ASR-Houlsby and full fine-tuning degraded SV performance relative to the baseline, highlighting their limited generalization. Our ASR-CA-XLSR\({}^{L}\) approach not only preserved but improved SV performance, reducing EER by relative 10.9% and DCF by 5.4%, showcasing strong generalization to the unseen SV task. These results demonstrate that CA-SSLR notably outperforms full fine-tuning and standard adaptation methods in terms of generalization. By effectively leveraging conditioning information, CA-SSLR adapts across tasks while maintaining performance on unseen ones. Our proposed conditioner offers both robust adaptations on training tasks and superior generalization, making CA-SSLR a versatile and effective solution for multilingual and multispeaker speech processing.

### Condition-Aware SSLR Model

Experiment Setting.Table 2 investigates the CA-SSLR approach with hierarchical language conditioning. The first block of the table refers to the baseline where the foundational models are frozen, while the second block (CA-XLSR\({}^{L}_{}\)) utilizes a separate task-specific LID model to pre-generate the language embedding. The third block presents our proposed approach, where we re-estimate the language embedding every fourth or third layer (CA-XLSR\({}^{L}\) (4L, 3L)) within the XLSR model, not required a separate LID system. The experiments utilized two types of conditioners: TCAC, which incorporates attention, and a variant without attention--referred to as channel-wise conditioners (CC)--where the same scale and bias are applied uniformly across all time frames. The real-time factors (RTF) as proc-time/signal-length are provided for assessing efficiency3.

Results.First, we observed that both CA-XLSR\({}^{L}_{}\) and CA-XLSR\({}^{L}\) systems with TCAC (with attention) generally performed better than the CC (w/o attention) counterparts, reaffirming the benefits of the time-wise attention design. In the second block, CA-XLSR\({}^{L}_{}\) slightly outperformed CA-XLSR\({}^{L}\) in terms of CER for both the 10-minute and 1-hour datasets. However, its real-time factor (RTF) is akin to the combined RTFs of separate LID and ASR models since it runs Wav2Vec2 twice--once for language embedding extraction and again for ASR conditioning--posing challenges for streaming applications. On the other hand, CA-XLSR\({}^{L}\)(CC, 3L) excelled among the three approaches, achieving a 35.9% and 19.0% relative improvement in Normal and few-shot languages,

Table 1: Evaluation of adapted XLSR models on the 10-min ML-SUPERB and VoxCeleb dataset for LID, ASR, and SV tasks. These evaluations test the encoderâ€™s generalizability across different tasks, demonstrating effectiveness without further task-specific tuning.

respectively, compared to the baseline in the 10-minute setup, and 33.5% and 19.8% in the 1-hour setup. LID accuracy remained comparable among the various CA-XLSR models, with a notable performance improvement from 90.9% to 93.4% in 1-hour setup.

### Generalist Condition-Aware SSLR Model

Experiment Setting.Table 3 presents results for general CA-SSLR models that combine multi-lingual ASR, LID, and SV tasks. The table compares the baselines, with frozen and fine-tuned SSL models, to two different CA-SSLR Hierarchical models (CA-SSLR\({}^{L}\) and CA-SSLR\({}^{L,S}\)). We further include another well-known multilingual SSLR model, mHuBERT, for a comprehensive comparison. The LID conditioning systems (CA-SSLR\({}^{L}\)) are the same as from the previous section, conditioning the SSL model only on LID embeddings, with the SV decoder added on top of SSL features without further adaptation. The LID + SV conditioning system (CA-SSLR\({}^{L,S}\)) combines both LID and SV embeddings and is jointly trained on ASR, SV, and LID losses. The intermediate LID embeddings are recomputed every three layers as the best configuration in Table 2, and SV embeddings are recomputed every six SSL layers. Apart from ASR CER and LID Acc on ML-SUPERB, we present SV equal error rates (EER) and detection cost function (DCF), measured at target prior probability \(p=0.05\), on VoxCeleb1. SV performance varied depending on whether we trained the model combining 10min ML-SUPERB + VoxCeleb2 or 1h ML-SUPERB + VoxCeleb2.

Fine-tuning Baseline.In the fully fine-tuning experiment, we initialized the model with pretrained ASR, LID, and SV decoders and fine-tuned for a few epochs. However, this approach resulted in suboptimal performance compared to the frozen SSLR baseline. The "FT" experiments showed degraded performance, with LID accuracy decreasing by 5.7%, ASR CER increasing by 3.1%, and SV EER worsening by 4.2 in absolute values on average across the four settings. This decline is unexpected, as fine-tuning typically improves performance. This suggests that simultaneous

    &  &  &  &  \\   & & **RTF\(\)** & LID (ACC \(\)) & ASR (CER \(\)) & LID (ACC \(\)) & ASR (CER \(\)) \\   & &  &  &  &  &  &  \\  XLSR  & 0.021 & 1.00 & 66.9 & 29.2 & 40.9 & 87.9 & 22.0 & 39.3 \\ MMS-1b  & - & - & 84.8 & 21.3 & 30.2 & 86.1 & 18.1 & 30.8 \\ XLSR (Ours) & 0.021 & 1.00 & 89.0 & 29.0 & 39.0 & 90.9 & 22.7 & 36.9 \\  CA-XLSR\({}^{L}_{}\)(CC) & 0.037 & 1.75 & 89.0 & 18.6 & 32.2 & 90.9 & 14.1 & 31.5 \\ CA-XLSR\({}^{L}_{}\)(TCAC) & 0.037 & 1.75 & 89.0 & **17.8** & 31.8 & 90.9 & **13.5** & 31.4 \\  CA-XLSR\({}^{L}\)(CC, 4L) & **0.024** & **1.17** & **89.1** & 19.7 & 31.7 & 89.6 & 16.5 & 32.2 \\ CA-XLSR\({}^{L}\)(CC, 3L) & **0.027** & **1.27** & 88.6 & 19.4 & **31.5** & 90.0 & 16.0 & 32.4 \\ CA-XLSR\({}^{L}\)(TCAC, 3L) & **0.027** & **1.27** & 88.6 & 18.6 & 31.6 & **93.4** & 15.1 & **29.6** \\   

Table 2: ASR CER(%) and LID Acc (%) in ML-SUPERB 10min. and 1h. sets, comparing different layers to generate the language embedding to condition the following layers. We adapt the XLSR model for LID and ASR tasks.

    &  &  &  &  \\   & & **RTF\(\)** & LID & ASR & SV & LID & ASR & SV \\   & ACC\(\) & CER\(\) & EER\(\) & DCF\(\) & ACC\(\) & CER\(\) & EER\(\) & DCF\(\) \\  mHuBERT & 0.015 & 1.00 & 81.9 & 38.2 & 2.19 & 0.145 & 86.2 & 30.9 & 2.19 & 0.145 \\ + FT & 0.015 & 1.00 & 73.0 & 36.5 & 5.85 & 0.350 & **87.7** & 32.3 & 4.01 & 0.251 \\  CA-mHuBERT\({}^{L}\)(CC) & 0.017 & 1.13 & 82.0 & 31.9 & **1.77** & 0.120 & 86.1 & 25.1 & **1.77** & **0.118** \\ CA-mHuBERT\({}^{L,S}\)(CC) & 0.018 & 1.16 & **82.2** & **31.7** & 1.79 & **0.117** & 87.3 & **24.8** & 1.78 & 0.121 \\   XLSR & 0.024 & 1.00 & 89.0 & 29.0 & 1.29 & 0.093 & 90.9 & 22.7 & 1.29 & 0.093 \\ + FT & 0.024 & 1.00 & 81.5 & 35.6 & 7.23 & 0.353 & 83.2 & 28.7 & 6.72 & 0.330 \\  CA-XLSR\({}^{L}\)(CC) & 0.029 & 1.23 & 88.6 & 19.4 & 1.11 & 0.076 & 90.0 & 16.0 & 1.02 & 0.078 \\ CA-XLSR\({}^{L}\)(TCAC) & 0.029 & 1.23 & 88.6 & 18.6 & 1.15 & 0.088 & 93.4 & 15.1 & 1.06 & 0.077 \\ CA-XLSR\({}^{L,S}\)(CC) & 0.032 & 1.34 & **89.1** & 18.8 & **1.04** & **0.075** & 88.1 & 15.0 & **0.94** & **0.073** \\ CA-XLSR\({}^{L,S}\)(TCAC) & 0.032 & 1.34 & 89.0 & **18.3** & 1.11 & 0.086 & **93.5** & **14.4** & 1.01 & 0.077 \\   

Table 3: Experiments on LID and LID + SV Hierarchical Conditioning. We adapt the XLSR and mHuBERT models for LID and ASR tasks using CA-SSLR\({}^{L}\), and for SV tasks using CA-SSLR\({}^{L,S}\). Results for Normal languages with 10-min and 1-hour datasets alongside VoxCeleb SV results.

adaptation of the SSL layers to multiple tasks causes conflicting adjustments, reducing the model's robustness. Consequently, catastrophic forgetting led to worse performance compared to the baseline. Conversely, the condition-aware SSLR models exhibited superior performance comparing with the frozen baseline, indicating that training the inserted condition layers does not alter the model's behavior for downstream tasks but improves its ability to represent the input speech data.

Language Conditioned SSLR.CA-SSLR\({}^{L}\)(CC) notably enhanced SV performance w.r.t. the baseline, despite its encoder being solely tuned for ASR and LID tasks. For XLSR, the EER improved by 14% and 20% relative for the 10-min. and 1-h. sets, respectively, while DCF improved by 16-18%. Similarly, for mHuBERT, we observed comparable enhancements, with the EER improving by 17% in both sets and the DCF improving by 17-18%. This demonstrates that the CA-SSLR approach offers superior generalization capabilities compared to the original pre-trained SSL encoder, delivering improved performance. CA-SSLR\({}^{L}\)(TCAC) performance in SV is comparable to its non-attention counterpart with better performance in LID and ASR as discussed in Sec.5.2.

Language and Speaker Conditioned SSLR.Adding a speaker conditioner to CA-SSLR\({}^{L,S}\) further improved its performance. In ASR tasks, incorporating the speaker conditioner to CA-XLSR\({}^{L,S}\) reduced CER by 3.1% for the 10-min. set and 6.2% for the 1-hr set, relative to CA-XLSR\({}^{L}\). For LID task, CA-SSLR\({}^{L,S}\) shows similar performance to other models with relative differences below 3%. For SV, CA-XLSR\({}^{L,S}\) using channel-wise conditioner (CC) reduced EER by 19.4-27.1%, outperforming CA-XLSR\({}^{L}\). Switching from CC to TCAC yielded additional gains in ASR, adding a relative improvement of 2.7-4.0%. In contrast, its impact on SV is more modest, with improvements in EER by 14.0-21.7%. Overall, TCAC demonstrated better adaptation ability, while CC excelled in generalization.

ASR and RTF Discussion.Generally, we observed the largest improvement for ASR when including the language conditioner, as it enables the system to adapt to produce output tokens in the correct language. Conversely, adapting the model to the input speaker provided fewer ASR gains. The XLSR model benefitted from our approach better than mHuBERT, possibly because mHuBERT is \(3\) smaller than XLSR, but more importantly, because mHuBERT is trained on just four languages compared to 128 in XLSR. Therefore, the pre-trained mHuBERT has not encountered enough diversity in terms of languages and speakers, thereby limiting its performance in multilingual ASR and SV. In terms of RTF, while the conditioned models are 13-34% slower compared to sharing the pre-trained SSL encoder for the three tasks, both CA-SSLR\({}^{L}\) and CA-SSLR\({}^{L,S}\) offer superior performance while being much faster than running task-specific models separately, indicating a more efficient use of computational resources while running the generalist model.

### Analysis of the TCA Conditioner

Ablation study of Conditioning Approach.Table 4 conducts an ablation study for different conditioning methods with CA-XLSR\({}^{L}_{}\) settings within the ML-SUPERB 10min dataset regarding ASR CER. First, we used conditioners without attention (CC) on the ground truth LID predictions

  
**ASR Adapted.** & **Normal** & **Few-shots** \\
**CER \(\)** & **CER \(\)** & **CER \(\)** \\  XLSR & 29.0 & 39.0 \\ + G.T. CC & 17.2 & 27.9 \\  + Hard CC & 19.8 & **28.6** \\ + Soft CC & 19.3 & 32.5 \\ + Embed CC & 18.6 & 32.2 \\ + Embed TCAC & **17.8** & 31.8 \\   

Table 4: Ablation study of condition-aware settings for ASR-adapted XLSR models on 10-min ML-SUPERB dataset, using CC or TCAC. Conditioning is based on predicted language labels or LID embeddings, except in the ground truth (G.T.) experiment.

(G.T.), serving as the upper bound for the performance of our proposed approach. This improved the _Normal_ languages from 29.0% to 17.2%, and _Few-shot_ languages from 39.0% to 27.9%, w.r.t. the pre-trained XLSR model. This showcases the potential of the condition-aware SSLR. Following, we compared conditioning on hard-predicted language labels (_Hard_), soft-predicted language labels (_Soft_), and language embeddings from the LID decoder bottleneck layer (_Embed_) for comparison. Conditioning on hard LID labels improved the most in _Few-shot_ languages, improving by 26% relative to the baseline. On the other hand, the _Embed_ case outperformed the _Soft_ case and provided balanced performance for both _Normals_ and _Few-shots_ languages. Additionally, we compared CC to TCAC. The TCAC provided the best overall results, improving _Normals_ and _Few-shots_ by 38.6% and 18.5%, respectively, w.r.t. baseline.

Parameter Efficiency in CER Reduction.Figure 3 compares CER versus the number of trainable parameters for different adaptation methods, including our proposed Channel-wise Conditioner and Time-Channel Attention Conditioner (CC-TCAC), the Houlsby adapter, LoRA (Hu et al., 2021), full fine-tuning (FT), and the baseline XLSR model. The Houlsby adapters, with hidden dimensions of 256 and 512, have 18.4M and 30.9M trainable parameters. In comparison, the CC-TCAC approach, conditioned on precomputed LID embeddings with 256 dimensions (18.7M for CC and 22.6M for TCAC), achieves lower CERs with similar parameter counts. LoRA provided only marginal gains over the baseline, aligning with findings from Chen et al. (2023b). In contrast, FT required fine-tuning 16-24 layers (200-300M parameters) to achieve comparable CER reductions, making CC-TCAC about ten times more efficient. As discussed in Sec 5.1, CC-TCAC's key contribution is its superior generalization ability. While the Houlsby adapter enhances task-specific adaptation, it falls short in generalizing to unseen tasks. In contrast, CC-TCAC achieves both effective adaptation and robust generalization, making it a versatile solution for diverse applications.

## 6 Conclusion

This paper introduces the CA-SSLR framework, an innovative approach that integrates conditioning into pre-trained Self-Supervised Learning (SSL) models by adapting only the trainable conditioner. Through a hierarchical self-conditioning mechanism, where intermediate language and speaker features condition the upper layers of the SSL model, CA-SSLR achieves a 33% reduction in ASR CER compared to the pre-trained baseline, matching the performance of single-task fully fine-tuned models. Additionally, it improves Speaker Verification EER by 27% and reduces Language Identification error rates by relative 10% in average. The results indicate that condition-aware SSLR models enhance the model's interpretation of input speech data, providing superior performance compared to traditional fine-tuning methods. This improvement is achieved by dynamically tailoring the model's response to the input language and speaker characteristics, ensuring robust generalization across various tasks. In summary, CA-SSLR offers a versatile and efficient approach to integrating conditioning information into pre-trained models. This method not only enhances performance across multiple tasks but also ensures efficient parameter utilization, supported by an improved RTF that facilitates its application in real-world scenarios.

Broader Impact and LimitationsThe CA-SSLR methodology improves the conditioning of pre-trained SSL models for speech processing, improving performance with minimal fine-tuning and reducing computational resource requirements. This advancement facilitates the deployment of robust models in resource-constrained environments, promoting broader access to advanced speech technology. Nevertheless, the methodology carries potential risks, as the conditioning mechanisms might amplify biases in the training data, leading to unfair outcomes, particularly for underrepresented languages and speaker groups. Ensuring diverse and balanced datasets, along with continuous monitoring, is crucial to mitigate biases and avoid reinforcing existing inequities.