# Abductive Reasoning in Logical Credal Networks

Radu Marinescu

IBM Research, Ireland

radu.marinescu@ie.ibm.com

&Junkyu Lee

IBM Research, USA

junkyu.lee@ibm.com

&Debarun Bhattacharjya

IBM Research, USA

debarunb@us.ibm.com

&Fabio Cozman

Universidade de Sao Paulo, Brazil

fgcozman@usp.br

&Alexander Gray

Centaur AI Institute, USA

alexander.gray@centaurinstitute.org

###### Abstract

Logical Credal Networks or LCNs were recently introduced as a powerful probabilistic logic framework for representing and reasoning with imprecise knowledge. Unlike many existing formalisms, LCNs have the ability to represent cycles and allow specifying marginal and conditional probability bounds on logic formulae which may be important in many realistic scenarios. Previous work on LCNs has focused exclusively on marginal inference, namely computing posterior lower and upper probability bounds on a query formula. In this paper, we explore abductive reasoning tasks such as solving MAP and Marginal MAP queries in LCNs given some evidence. We first formally define the MAP and Marginal MAP tasks for LCNs and subsequently show how to solve these tasks exactly using search-based approaches. We then propose several approximate schemes that allow us to scale MAP and Marginal MAP inference to larger problem instances. An extensive empirical evaluation demonstrates the effectiveness of our algorithms on both random LCN instances as well as LCNs derived from more realistic use-cases.

## 1 Introduction

Probabilistic logic which combines probability and logic in a principled manner has emerged over the past decades as a unified representational and reasoning framework capable of dealing effectively with complex real-world applications that require efficient handling of uncertainty and compact representations of domain expert knowledge . Logical Credal Networks or LCNs  were introduced recently as a probabilistic logic designed for representing and reasoning with imprecise knowledge. Unlike many existing probabilistic logics, LCNs have the ability to represent cycles (e.g., feedback loops) as well as allow specifying marginal and conditional probability bounds on logic formulae which may be important in many realistic usecases.

Up until now, the work on LCNs has focused exclusively on marginal inference, i.e. efficiently computing posterior lower and upper probability bounds on a query formula. However, _abductive reasoning_ tasks such as explaining the evidence observed in an LCN are equally important in many real-world applications. In probabilistic graphical models, these tasks are commonly known as MAP and Marginal MAP (MMAP) inference and have received extensive attention over the past decades . They are typically tackled efficiently with dynamic programming (e.g., variable elimination) or heuristic search (e.g., depth-first branch and bound) based algorithms .

Contribution.In this paper, we consider solving MAP and Marginal MAP inference queries in LCNs. Unlike in graphical models, an LCN encodes a set of probability distributions over its interpretations. Therefore, a complete or a partial explanation of the evidence which represents a complete or a partial truth assignment to the LCN's propositions may correspond to more than one distribution. Our work builds on very recent work on Marginal MAP inference for credal networks, a class of probabilistic graphical models that allow reasoning with imprecise probabilities . We formally introduce the MAP and Marginal MAP tasks for LCNs as finding a complete or a partial truth assignment to the LCN's propositions with maximum _lower_ (respectively, _upper_) probability, given some evidence in the LCN. We show how to evaluate such MAP assignments using exact marginal inference for LCNs and, subsequently, propose several search schemes based on depth-first search, limited discrepancy search and simulated annealing to solve these tasks in practice. We then extend a recent message-passing scheme for approximate marginal inference in LCNs  to handle effectively the MAP and MMAP inference tasks in LCNs as well as adapt the limited discrepancy search and simulated annealing methods to use an approximate evaluation of the MAP assignments during search. We experiment and evaluate our proposed exact and approximate algorithms on several classes of LCNs including random as well as more realistic LCN instances. Our results show that the search methods based on exact evaluation of the MAP assignments are limited to solving small size problems in practice, while the approximate message-passing scheme and, to some extent, the approximate search-based methods can scale to much larger problem instances. This is important because it allows us to tackle practical problems involving hundreds and possibly many thousands of propositions. The supplementary material includes additional details and experiments.

## 2 Background

We provide next a brief overview of basic concepts about LCNs and marginal inference in these models. Throughout the paper we will use the following notations. Logical propositions are denoted by uppercase letters (e.g., \(A,B,C,...\)) while for sets of propositions we use boldfaced uppercase letters (e.g., \(,,,...\)). Truth assignments to propositions (i.e., _literals_) are denoted by either lowercase or uppercase letters, namely we use \(a\) or \(A\) to indicate that proposition \(A\) holds true, and \( a\) or \( A\) if \(A\) is false. Sets of literals are denoted by boldfaced lowercase letters (e.g., \(,,,...\)).

### Logical Credal Networks

A Logical Credal Network (LCN)  is defined by a tuple \(=,\), where \(=\{A_{1},,A_{n}\}\) is a set of propositions (or atoms), and \(\) is a set of probability labeled sentences (or constraints) having the following two forms:

\[  P() \] \[  P(|) \]

Here, \(\) and \(\) are arbitrary propositional logic formulae1 involving propositions in \(\) and logical connectives such as negation, disjunction and conjunction, and \(0 1\) are lower and upper probability bounds, respectively.

An LCN is associated with _primal graph_ which is a directed graph \(G\) containing _formula nodes_ and _proposition nodes_, as well as directed edges from each proposition node in a formula \(\) to the formula node representing \(\) (for type 1 sentences), and directed edges from each of the proposition nodes in \(\) to \(\), a directed edge from \(\) to \(\), and bi-directed edges from \(\) to the proposition nodes in \(\), respectively (for type 2 sentences) . A _parent_ of a proposition \(A\) in \(G\) is a proposition \(B\) such that there is a directed path in \(G\) from \(B\) to \(A\) in which all intermediate nodes are formulae. A _descendant_ of a proposition \(A\) in \(G\) is a proposition \(B\) such that there is a directed path in \(G\) from \(A\) to \(B\) in which no intermediate node is a parent of \(A\).

An LCN is endowed with a _Local Markov Condition_ (LMC) where a proposition node \(A\) is independent, given its parents, of all proposition nodes that are not \(A\) itself nor descendants of \(A\) nor parents of \(A\). Therefore, an LCN represents a set of probability distributions over all interpretations of its formulae that satisfy the constraints represented by the type (1) and (2) sentences as well as the constraints induced by the independence relations given by the local Markov condition .

**Example 1**.: _Figure 1 describes a simple LCN whose sentences shown in Figure 0(a) state that: Bronchitis (\(B\)) is more likely than Smoking (\(S\)); Smoking may cause Cancer (\(C\)) or Bronchitis; Dyspnea (\(D\)) or shortness of breadth is a common symptom for Cancer and Bronchitis; in case of Cancer we have either a positive X-Ray result (\(X\)) and Dyspnea, or a negative X-Ray and no Dyspnea. Figure 0(b) shows the primal graph where the formula and proposition nodes are displayed as rectangles and shaded circles, respectively._

### Marginal Inference in Logical Credal Networks

\[_{i=1}^{m}p_{i}=1 \] \[p_{i} 0, i=1,,m\] (4) \[_{}\] (5) \[_{}_{ }_{}\] (6) \[(_{a})(_{b})-(_ {c})(_{d})=0\] (7) \[_{} \]

assumptions given by the local Markov condition, and a linear objective function encoding the query \(P()\) which is minimized and maximized to yield the desired bounds. More specifically, let \(=(p_{1},,p_{m})\) be the vector of real-valued variables representing the probabilities of \(\)'s interpretations, where \(m=2^{n}\), and let \(_{}=(a_{1}^{},,a_{m}^{})\) be a binary vector, called an _indicator vector_, such that \(a_{i}^{}\) is 1 if formula \(\) is true in the \(i\)-th interpretation and \(0\) otherwise. Since the probability of a formula \(\) is the sum of the probabilities of the interpretations in which \(\) is true, we can write \(P()\) as \(_{}\) where \(\) is the dot-product of two vectors. Therefore, Equations (3) and (4) ensure that \(\) is a valid probability distribution, Equations (5) and (6) encode the type (1) and (2) sentences in \(\) while Equation 7 encodes the conditional independencies of the form \(P(X_{j}|_{j},_{j})=P(X_{j}|_{j})\), where \(X_{j}\) is a proposition, \(_{j}=\{S_{j1},,S_{jk}\}\) and \(_{j}=\{T_{j1},,T_{jl}\}\) are \(X_{j}\)'s parents and non-descendants in the primal graph of \(\), \(_{}\) and \(_{}\) are the indicator vectors for formulae \(\) and \(\) involved in \(\)'s sentences, and \(_{a}\), \(_{b}\), \(_{c}\) and \(_{d}\) are the indicator vectors corresponding to the formulae \(a=(x_{j} s_{j1} s_{jk} t_{j1} t _{jl})\), \(b=(s_{j1} s_{jk})\), \(c=(x_{j} s_{j1} s_{jk})\), and \(d=(s_{j1} s_{jk} t_{j1} t_{jl})\), respectively (see also  for more details).

## 3 MAP and Marginal MAP Inference in LCNs

Maximum A Posteriori (MAP) and Marginal MAP (MMAP) inference are well known abductive reasoning tasks in probabilistic graphical models such as Bayesian networks and Markov networks [12; 13; 14; 15; 16]. Specifically, the MAP task calls for finding a complete assignment to all variables having maximum probability, given the evidence. Marginal MAP generalizes MAP and looks for a partial variable assignment that has maximum marginal probability, given the evidence. MAP

Figure 1: A simple LCN and its primal graph.

and MMAP inference tasks appear in many real-world applications such as diagnosis, abduction and explanation and are typically tackled with dynamic programming (e.g., variable elimination) or heuristic search (e.g., depth-first branch and bound) based algorithms [13; 14; 15; 16].

In this section, we present our novel approach for solving the MAP and Marginal MAP inference tasks in Logical Credal Networks. Unlike in graphical models, a (partial) variable assignment (or interpretation) in an LCN may correspond to more than one distribution. Therefore, we begin by formally defining two MAP and MMAP inference tasks for LCNs, called _maximin MAP_ (resp. _maximin MMAP_) and _maximax MAP_ (resp. _maximax MMAP_). Subsequently, we develop several exact and approximation schemes for solving these tasks efficiently in practice.

### The MAP and Marginal MAP Tasks in LCNs

Let \(=,\) be an LCN with \(n\) propositions and let \(=\{E_{1},,E_{k}\}\) be a subset of \(k\) propositions, called _evidence_, for which the truth values \(=\{e_{1},,e_{k}\}\) are known. Let \(=\{Y_{1},,Y_{m}\}\) be a subset of \(m\) propositions called _MAP propositions_. A truth assignment to \(\) is is called a _MAP assignment_ and is denoted by \(=\{y_{1},,y_{m}\}\), respectively. Clearly, if \(=\) (i.e., \(m=n-k\)) then we have a MAP task, otherwise we have a MMAP task (i.e., \(m<n-k\)). The _maximin_ and _maximax_ MAP/MMAP tasks are defined as follows:

**Definition 1** (maximin).: _Given an LCN \(\) with \(n\) propositions, evidence \(\), and MAP propositions \(\), the maximin MAP (or maximin MMAP if \(m<n-k\)) task is finding a truth assignment \(^{*}\) to \(\) having maximum lower probability, given evidence \(\), namely:_

\[^{*}=*{argmax}_{()} {P}(_{}) \]

_where \(()\) is the set of all truth assignments to the MAP propositions, and \(_{}=y_{1} y_{m} e_{1}  e_{k}\) is the conjunction of the literals in \(\) and \(\), respectively._

**Definition 2** (maximax).: _Given an LCN \(\) with \(n\) propositions, evidence \(\), and MAP propositions \(\), the maximax MAP (or maximax MMAP if \(m<n-k\)) task is finding a truth assignment \(^{*}\) to \(\) having maximum upper probability, given evidence \(\), namely:_

\[^{*}=*{argmax}_{()} (_{}) \]

_where \(()\) is the set of all truth assignments to the MAP propositions, and \(_{}=y_{1} y_{m} e_{1}  e_{k}\) is the conjunction of the literals in \(\) and \(\), respectively._

### Search Algorithms Using Exact MAP Assignment Evaluations

We present next three search-based schemes for solving the MAP and MMAP tasks in LCNs. These methods employ different search strategies for exploring the search space defined by the MAP propositions while evaluating exactly each complete or partial MAP assignment.

Exact Evaluation of a MAP Assignment.Clearly, computing the lower and upper probabilities \((_{})\) and \((_{})\) of a MAP assignment \(\) given evidence \(\) can be done easily by minimizing and, respectively maximizing the non-linear program defined by Equations (3)-(8), where the query formula is the conjunction of positive or negative literals in \(\) and \(\), namely \(_{}=y_{1} y_{m} e_{1}  e_{k}\). Therefore, evaluating a MAP assignment in case of both MAP and Marginal MAP inference in LCNs is quite difficult as it involves solving a maginal inference problem for LCNs which is know to be NP-hard . This is in contrast with graphical models where, at least for MAP inference, the evaluation of a MAP assignment is linear in the number of variables .

**Example 2**.: _For illustration, consider the LCN example from Figure 1 and assume that we have evidence \(=\{x, s\}\), namely a patient has a positive X-Ray result (\(X=x\)) and is not smoking (\(S= s\)). The MAP propositions in this case are \(=\{B,C,D\}\) and the MAP assignment \(=(b, c, d)\) corresponds to the query formula \(_{}=b c d x s\). The lower and upper probabilities \((_{})\) and \((_{})\) of the MAP assignment are 9.9e-09 and 0.1, respectively._Depth-First Search.Our first approach for solving the MAP and MMAP tasks, called DFS, is described by Algorithm 1. It takes as input an LCN \(=,\), evidence \(=\) and a set of MAP propositions \(\) and outputs the optimal MAP assignment \(^{*}\). The method conducts a _depth-first search_ over the space of partial assignments to the MAP propositions, and, for each complete MAP assignment \(\) computes its score as the exact lower probability \((_{})\) for maximin tasks, and respectively, the upper probability \((_{})\) for maximin tasks, given the evidence \(\). This way, the optimal solution \(^{*}\) corresponds to the MAP assignment with the highest score.

**Theorem 1** (complexity).: _Given an LCN \(=,\) with \(n\) propositions, evidence \(=\) and MAP propositions \(\), algorithm DFS is sound and complete. The time and space complexity of the algorithm is \(O(2^{m+2^{n}})\) and \(O(2^{n})\), respectively, where \(m\) is the number of MAP propositions._

**Example 3**.: _Consider again the LCN from Figure 1 with evidence \(=\{x, s\}\). In this case, the exact maximin MAP assignment found by algorithm DFS is \(^{*}=\{ b,c,d\}\) with value 9.99e-09, while the exact maximax MAP assignment is \(^{*}=\{ b, c,d\}\) with value 0.7, respectively._

Limited Discrepancy Search.Our second approach for MAP and MMAP inference in LCNs uses Limited Discrepancy Search (LDS)  to explore the search space and is described by Algorithm 2. Specifically, LDS is a depth-first search strategy that searches for new solutions by iteratively increasing the number of _discrepancy_ values, where a discrepancy value indicates the maximum number of allowed variable-value assignment changes to an initial solution . Function Search (lines 7-22) performs the actual exploration of the search space limited by discrepancy \(\). If the selected truth value \(y\{y_{i}, y_{i}\}\) is different from the one corresponding to proposition \(Y_{i}\) at position \(i\) in the assignment \(\), \(\) is decremented to reduce the number of changes allowed to the remaining MAP propositions. Otherwise, the truth value for proposition \(Y_{i}\) remains unchanged and the \(\) value is preserved. As before, complete MAP assignments are evaluated exactly (lines 9-12) and the best solution found so far is maintained (lines 13-14).

```
1:procedureSA(\(=,\), \(=\), \(\))
2: initialize \(_{0}\) randomly and let \(^{*}_{0}\)
3:\(best score(^{*})\)
4:for all iterations \(i=1 N\)do
5: set \(^{*}\), \(T T_{init}\)
6:for all flips \(j=1 M\)do
7: let \(\) be \(\)'s neighbors
8: select random neighbor \(^{}\)
9:\( score(^{})- score()\)
```

**Algorithm 3** Simulated Annealing for MAP and Marginal MAP Inference in LCNs

**Theorem 2** (complexity).: _Given an LCN \(=,\) with \(n\) propositions, evidence \(=\) and MAP propositions \(\), algorithm LDS is sound and complete. The time and space complexity of the algorithm is \(O(2^{m+2^{n}})\) and \(O(2^{n})\), respectively, where \(m\) is the number of MAP propositions._

Simulated Annealing.The third approach for solving MAP and MMAP tasks in LCNs is described by Algorithm 3 and employs a form of stochastic local search known as Simulated Annealing (SA)  to explore the search space defined by the MAP propositions. The algorithm starts from an initial guess \(\) as a truth assignment to the MAP propositions \(\), and iteratively tries to improve it by moving to a better neighbor \(^{}\) that has a higher score. A _neighbor_\(^{}\) of \(\) is defined as a new assignment \(^{}\) which results from changing the truth value of a single proposition \(Y\) in \(\). At each step, the transition from the current state \(\) to a neighboring state \(^{}\) is decided probabilistically using an acceptance probability function \(P(^{},,T)\) that depends on the scores of the two states as well as a global time-varying parameter \(T\) called _temperature_ which is decreased using a cooling schedule \(<1\). We chose \(P(^{},,T)=e^{}\), where \(= score(^{})- score()\).

**Theorem 3** (complexity).: _Given an LCN \(=,\) with \(n\) propositions, evidence \(=\) and MAP propositions \(\), the time and space complexity of algorithm SA is \(O(N M 2^{2^{n}})\) and \(O(2^{n})\), respectively, where \(N\) is the number of iterations and \(M\) is the number of flips per iterations._

```
1:procedureAMAP(\(=,\), \(=\), \(\))
2: Create factor graph \(\) of \(\)
3: Apply the ARIEL scheme from  on \(\)
4:for all MAP propositions \(Y\)do
5:ifmaximin then
6:\(P(y)=_{f N(Y)}l_{f Y}\)
7:\(( y)=1-(y)\)
8:if\((y)>( y)\)then\(^{*}^{*}\{y\}\)
9:else\(^{*}^{*}\{ y\}\)
10:else
11:\((y)=_{f N(Y)}u_{f Y}\)
12:\(( y)=1-(y)\)
13:if\((y)>( y)\)then\(^{*}^{*}\{y\}\)
14:else\(^{*}^{*}\{ y\}\)
15:return\(^{*}\)
```

**Algorithm 4** Approximate MAP and Marginal MAP Inference in LCNs

### Approximate MAP and Marginal MAP Inference

The main bottleneck in the proposed search algorithms is the exact evaluation of the MAP assignments which is computationally very expensive . This limits the applicability of these methods to relatively small LCNs. Therefore, in order to be able to tackle larger LCNs, we extend a recent message-passing approximation scheme for marginal inference in LCNs  to solve the MAP and MMAP tasks in LCNs. Subsequently, we also adapt the limited discrepancy search and simulated annealing methods to use an approximate evaluation of the MAP assignments during search.

Algorithm 4 describes our message-passing based approximation scheme for MAP and MMAP inference in LCNs which we denote hereafter by AMAP. We build upon a recent scheme for approximate marginal inference in LCNs, called ARIEL , which propagates messages along the edges of a _factor graph_ associated with the input LCN until convergence. The factor graph \(\) of an LCN \(\) is a bi-partite graph the connects _proposition nodes_ labeled by the propositions in \(\) with _factor nodes_ associated with sentences that involve the same set of propositions . The messages propagated between the nodes of \(\) are intervals representing lower and upper bounds on the marginal probabilities of \(\)'s propositions and are computed as follows: the message sent from a proposition to a factor node tightens these bounds based on the incoming messages from the factor nodes connected to it; the message sent from a factor to a proposition node computes new bounds by solving a local non-linear program defined by the factor's sentences and the constraints encoding the assumption that the factor's propositions are independent of each other and the marginal probabilities of the factor's propositions are within the bounds given by the incoming proposition-to-factor messages (see also  for more details). Upon convergence, the maximin MAP assignment \(^{*}\) can be obtained as follows: for each MAP proposition \(Y\) we compute the tightest lower probability bound \((y)\) by maximizing the lower bound of all incoming factor-to-proposition messages to \(Y\), and, subsequently, select \(y\) as the most likely value assignment to \(Y\) if \((y)>( y)\) and \( y\) otherwise (for the maximax tasks we use the upper probability bounds \((y)\) and \(( y)\), respectively).

**Theorem 4** (complexity).: _Given an LCN \(=,\) with \(n\) propositions, evidence \(=\) and MAP propositions \(\), the time and space complexity of algorithm AMAP is \(O(N M 2^{2^{r}})\) and \(O(2^{r})\), where \(N\) is the number of iterations, \(M\) bounds the number of factor-to-node messages per iteration and \(r\) bounds the number of propositions in the factor nodes, respectively._

### Search Algorithms Based on Approximate MAP Evaluations

The main assumption behind algorithm AMAP is that all MAP propositions are independent of each other and therefore the solution \(^{*}\) returned by AMAP is likely to correspond to a local maxima. One way to escape such a local optima and obtain a potentially better solution is to employ a search scheme based on either limited discrepancy search or simulated annealing that continues the exploration of the search space starting from \(^{*}\). However, in order to scale to larger LCNs, we would like the search schemes to rely on an approximate rather than an exact evaluation of the MAP assignments.

Approximate Evaluation of a MAP Assignment.Estimating the lower and upper probabilities of a MAP assignment \(\) can be done by approximate marginal inference on an _augmented_ LCN as follows. Let \(=,\) be the input LCN and let \(=(y_{1},,y_{m})\) be a MAP assignment to propositions \(=\{Y_{1},,Y_{m}\}\) (for simplicity, we include the evidence \(\) in \(\)). The _augmented_ LCN \(^{}=^{},^{}\) is constructed by adding a set of auxiliary propositions \(=\{W_{1},,W_{m}\}\), one for each MAP proposition, and additional constraints of the following two forms: \(P(W_{1}|Y_{1})\) and \(P(W_{j}|W_{j-1} Y_{j})\), for all \(2 j m\), such that \(P(w_{1}|y_{1})=1\), \(P(w_{1}| y_{1})=0\), \(P(w_{j}|w_{j-1} y_{j})=1\), \(P(w_{j}|w_{j-1} y_{j})=0\), \(P(w_{j}| w_{j-1} y_{j})=0\) and \(P(w_{j}| w_{j-1} y_{j})=0\), respectively. Then, we can estimate \((_{})\) and \((_{})\), where \(_{}=y_{1} y_{m}\), by computing the posterior marginals \((w_{m})\) and \((w_{m})\) in the augmented LCN \(^{}\) using the method from .

Limited Discrepancy Search and Simulated Annealing.Our approximate LDS and SA based algorithms denoted by ALDS and ASA can be obtained from Algorithms 2 and 3 by replacing the \(score()\) function with the approximate MAP evaluation scheme described above. These algorithms can start the search either from a random MAP assignment or from the solution found by algorithm AMAP. Finally, the time complexity of algorithms ALDS and ASA can be bounded by \(O(2^{m+2^{r}})\) and \(O(N M 2^{2^{r}})\), respectively, where \(m\) is the number of MAP propositions, \(N\) is the number of iterations used by ASA, \(M\) is the maximum number of flips per iteration, and \(r\) bounds the number of propositions in the factor nodes of the factor graph associated with the input LCN .

## 4 Experiments

In this section, we empirically evaluate the proposed exact and approximate schemes for MAP and MMAP inference in LCNs. All competing algorithms were implemented2 in Python 3.10 and used the ipopt 3.14 solver  with default settings to handle the non-linear constraint programs. We ran all experiments on a 3.0GHz Intel Core processor with 128GB of RAM.

Table 1 summarizes the results obtained for maximax MAP queries on the random LCNs.

For each problem class we consider both smaller (\(5 n 10\)) and larger (\(30 n 70\)) scale instances, respectively. We report the average CPU time in seconds and number of problem instance solved (out of 10) for each problem size. A '-' indicates that the respective algorithm exceeded the 2 hour time limit. The maximum discrepancy value use by algorithms LDS and ALDS was set to \(=3\), while algorithms SA and ASA used up to 30 flips over a single iteration. We can see that the algorithms using exact MAP assignment evaluations (i.e., DFS, LDS and SA) are limited to small scale problem instances with up to 8 propositions and they caused by the prohibitively large computational overhead associated with the exact evaluation of the MAP assignments during search. In contrast, the approximate search algorithms ALDS and specially ASA can scale to much larger problem instances due to the less expensive approximate MAP assignment evaluations. AMAP is the best performing algorithm in terms of running time and number of problems solved for all reported problem sizes. However, since the solution found by AMAP is only a local maxima, in Figure 2 we report on the solution quality found by algorithms AMAP, ALDS and ASA on LCN instances of size 10. Specifically, we show the number of wins as the number of times (out of 10) each algorithm found the best solution. In this case, algorithms ALDS and ASA were initialized with the MAP assignment

  size &  &  \\ \(n\) & DFS & LDS(3) & SA & AMAP & ALDS(3) & ASA \\    \\ 
5 & 15.30 (10) & 26.07 (10) & 20.18 (10) & 2.87 (10) & 174.17 (10) & 188.27 (10) \\
8 & 3246.28 (4) & 3072.18 (4) & 1199.51 (10) & 8.05 (10) & 1054.53 (10) & 518.18 (10) \\
10 & - & - & - & 11.81 (10) & 2273.16 (10) & 813.30 (10) \\ 
30 & - & - & - & 31.55 (10) & - & 3091.74 (10) \\
50 & - & - & - & 52.30 (10) & - & 5324.71 (10) \\
70 & - & - & - & 79.28 (10) & - & 7279.56 (10) \\   \\ 
5 & 21.09 (10) & 15.66 (10) & 24.04 (10) & 5.54 (10) & 163.02 (10) & 156.34 (10) \\
8 & 1633.38 (8) & 1958.16 (9) & 633.77 (10) & 13.05 (10) & 1339.71 (10) & 571.55 (10) \\
10 & - & - & - & 15.55 (10) & 2903.05 (10) & 944.17 (10) \\ 
30 & - & - & - & 49.94 (10) & - & 3593.71 (10) \\
50 & - & - & - & 89.13 (10) & - & 5639.90 (10) \\
70 & - & - & - & 132.34 (10) & - & 6093.28 (10) \\   \\ 
5 & 19.51 (10) & 17.56 (10) & 20.37 (10) & 5.26 (10) & 152.99 (10) & 143.60 (10) \\
8 & 3152.57 (1) & 3209.54 (5) & 1226.88 (10) & 10.29 (10) & 954.46 (10) & 444.17 (10) \\
10 & - & - & - & 12.21 (10) & 2150.27 (10) & 717.75 (10) \\ 
30 & - & - & - & 40.54 (10) & - & 3335.14 (10) \\
50 & - & - & - & 76.83 (10found by AMAP. We can see that almost always the search-based approaches ALDS and ASA are able to find better solutions than AMAP. This is important in practice, particularly on larger scale problems where we can use AMAP to find a MAP solution quickly, and subsequently refine that solution using a search-based algorithm like ALDS or ASA if the time budget allows it. Finally, in Figure 3 we show the impact of the maximum discrepancy value \(\) on the running time of algorithm ALDS(\(\)). It is easy to see that as the discrepancy value \(\) increases, the search space explored by ALDS(\(\)) becomes larger, and therefore its corresponding running time increases as well.

Realistic LCNs.We experimented with a set of more realistic LCNs which were first introduced in . These LCNs were derived from real-world Bayesian networks  and contain up to 10 propositions as well as up to 24 sentences of the form \(l P(x_{i}) u\) and \(l P(x_{i}|_{i}) u\), respectively, where \(x_{i}\) is the positive literal of proposition \(X_{i}\) and \(_{i}=y_{i1} y_{ik}\) is the conjunction of the positive or negative literals corresponding to a particular configuration of the parents \(\{Y_{i1}, Y_{ik}\}\) of \(X_{i}\) in the Bayesian network. The specification of these LCNs is included in the supplementary material. Table 2 reports the results obtained on 10 LCN instances for the maximax MMAP task with 4 MAP propositions selected randomly. As before, algorithms DFS, LDS(3) and SA which rely on exact evaluations of the MAP assignments during search can only solve the smallest problem instances within the 2 hour time limit. In contrast, algorithms ALDS(3) and ASA solve all problem instances due to a much reduced overhead associated with the approximate MAP assignment evaluations. In this case, the search spaces explored by ALDS(3) and ASA are approximately the same in size and therefore the corresponding running times are comparable. AMAP is the fastest algorithm in this case as well.

Application to Factuality in Large Language Models.We consider an application of MMAP inference in LCNs to assess the factuality of the output \(A\) generated by a large language model (LLM) in response to a user query \(Q\) with respect to an external source of knowledge \(C\) that may contain contradicting factual information (e.g., Wikipedia) . The goal is to compute a _factuality score_ for response \(A\), denoted by \(f_{C}(A)\), in the context of the information from \(C\). In the following, we assume that \(A\) can be decomposed into a set of \(n\)_atomic facts_ (or just _atoms_) \(A=\{A_{1},,A_{n}\}\) (e.g., one way to do that is to split \(A\) into sentences) and, for each atom \(A_{i}\), up to \(k\) relevant passages \(\{C_{i1},,C_{ik}\}\) called _contexts_ can be retrieved from \(C\). A natural language inference (NLI) classifier such as SBERT  can be used to infer the _entailment_, _contradiction_ and _neutrality_ relationships between the texts corresponding to the atoms and contexts together

   LCN &  &  \\  & DFS & LDS(3) & SA & AMAP & ALDS(3) & ASA \\   Toy & 2.20 & 3.18 & 1.85 & 0.85 & 134.83 & 141.17 \\ Earth & 9.19 & 7.67 & 2.75 & 1.28 & 150.99 & 162.35 \\ Cancer & 16.34 & 14.09 & 8.52 & 2.64 & 157.92 & 159.66 \\ Asia & 811.82 & 800.18 & 312.10 & 4.07 & 187.44 & 201.76 \\ Credit & - & 6719.30 & 2976.55 & 5.09 & 204.77 & 222.52 \\ Engine & 4786.12 & 4502.34 & 2033.77 & 6.57 & 212.61 & 235.70 \\ Suicide & - & - & - & 5.99 & 220.31 & 203.68 \\ Tank & - & - & - & 8.04 & 263.65 & 281.73 \\ Alarm & - & - & - & 4.28 & 216.19 & 186.67 \\ Hepatitis & - & - & - & 8.22 & 260.38 & 250.45 \\   

Table 2: Results for MMAP tasks on realistic LCNs. CPU time in seconds. Time limit is 2 hours.

Figure 3: Average CPU time in seconds and standard deviation vs discrepancy \(\) for ALDS(\(\)).

with their corresponding probabilities (or scores). Specifically, we consider relationships between an atom and a context \(r(A_{i},C_{ij})\), and between two contexts \(r(C_{ij},C_{pq})\), respectively, where \(r\{,\}\). We define an LCN \(\) containing \(n+n k\) propositions for each of the atoms and contexts, and two types of sentences corresponding to the entailment and contradiction relationships as follows: \(l P(Y|X) u\) if \(X\) entails \(Y\), and \(l P( Y|X) u\) if \(X\) contradicts \(Y\), where \(X\) and \(Y\) are the propositions corresponding to a context and an atom, or to two different contexts, respectively. The lower and upper bounds \(l\) and \(u\) can be calculated easily from the probabilities obtained by running multiple NLI classifiers. Finally, the factuality score \(f_{C}(A)\) is the proportion of true atoms in the MAP assignment obtained by solving a maximax MMAP task over \(\) where the MAP propositions are those corresponding to \(A\)'s atoms.

Table 3 displays the results obtained on randomly generated factuality LCNs. More specifically, for each reported problem size \(n\{2,4,6,10,20,50,100\}\), we generated 10 random instances with \(n\) atoms and \(k=2\) contexts per atom such that \(10\%\) of all possible pairwise relationships between atoms and contexts were selected to be either _entailment_ or _contradiction_ with probability \(0.5\) while the remaining relationships were labeled as _neutral_ and thus ignored. The lower and upper probability bounds \(l\) and \(u\) in the corresponding LCN sentences were also generated randomly between 0 and 1 such that \(u-l 0.6\). In this case, the maximum discrepancy value was set to 2 and simulated annealing was allowed a single iteration and 30 flips. We observe again that algorithms DFS, LDS(2) and SA can only solve the smallest instances due to large computational overhead associated with exact evaluation of the MAP assignments. In contrast, algorithms ALDS(2) and ASA which rely on less expensive approximate evaluations of the MAP assignments can scale to larger problems with up to 20 atoms. Algorithm AMAP outperforms its competitors and solves all problem instances.

In summary, our empirical evaluation showed that the exact search-based MAP/MMAP algorithms are limited to solving relatively small problem instances. In contrast, the approximate MAP/MMAP schemes based on either message-passing or search can scale to much larger LCN instances.

## 5 Conclusions

In this paper, we address abductive reasoning tasks such as generating MAP and Marginal MAP (MMAP) explanations in Logical Credal Networks (LCNs), a recently introduced probabilistic logic framework for reasoning with imprecise knowledge. Since an LCN encodes a set of distributions over its interpretations, a complete or partial explanation of the evidence (i.e., a MAP assignment) may correspond to more than one distribution. Therefore, we define the maximin/maximax MAP and MMAP tasks for LCNs as finding complete or partial MAP assignments that have maximum lower/upper probability given the evidence. We propose several search algorithms that combine depth-first search, limited-discrepancy search or simulated annealing with exact evaluations of the MAP assignments using marginal inference for LCNs. We also develop an approximate message-passing scheme as well as extend limited discrepancy search and simulated annealing to use an approximate evaluation of the MAP assignments during search. Our experiments with random LCNs and LCNs derived from realistic use-cases demonstrate conclusively that the search methods based on exact evaluations of the MAP assignments are limited to small size problems, while the approximation schemes can scale to much larger problems. For future work we plan to investigate more advanced depth-first branch-and-bound and best-first search techniques. However, these kinds of methods require developing novel heuristic bounding schemes to guide the search more effectively .

    &  &  \\ \(n\), \(k=2\) & DFS & LDS(2) & SA & AMAP & ALDS(2) & ASA \\  
2 & 56.95 (10) & 57.37 (10) & 60.09 (10) & 0.31 (10) & 5.25 (10) & 4.13 (10) \\
4 & - & - & - & 0.98 (10) & 80.07 (10) & 54.15(10) \\
6 & - & - & - & 1.97 (10) & 453.88 (10) & 219.57 (10) \\
10 & - & - & - & 7.33 (10) & 2713.90 (10) & 928.28 (10) \\
20 & - & - & - & 28.42 (10) & - & 3809.23 (10) \\
50 & - & - & - & 379.18 (10) & - & - \\
100 & - & - & - & 1807.10 (10) & - & - \\   

Table 3: Results for factuality LCNs. Average CPU time in seconds and number of problem instances solved. Time limit is 2 hours.