# Cost-effective Reduced-Order Modeling via Bayesian Active Learning

Amir Hossein Rahmati\({}^{1}\) Nathan M. Urban\({}^{2}\) Byung-Jun Yoon\({}^{1,2}\) Xiaoning Qian\({}^{1,2}\)

\({}^{1}\) Texas A&M University, College Station, TX \({}^{2}\) Brookheaven National Laboratory, Upton, NY

{amir_hossein_rahmati, bjyoon, xqian}@tamu.edu

{nurban, byoon, xqian1}@bnl.gov

###### Abstract

Machine Learning surrogates have been developed to accelerate solving systems dynamics of complex processes in different science and engineering applications. To faithfully capture governing systems dynamics, these methods rely on large training datasets, hence restricting their applicability in real-world problems. In this work, we propose BayPOD-AL, an active learning framework based on an uncertainty-aware Bayesian proper orthogonal decomposition (POD) approach, which aims to effectively learn reduced-order models from high-fidelity full-order models representing complex systems. Experimental results on predicting the temperature evolution over a rod demonstrate BayPOD-AL's effectiveness in suggesting the informative data and reducing computational cost related to constructing a training dataset compared to other uncertainty-guided active learning strategies. Furthermore, we demonstrate BayPOD-AL's generalizability and efficiency by evaluating its performance on a dataset of higher temporal resolution than the training dataset.

## 1 Introduction

Many real-world decision-making problems benefit from proper modeling of high-dimensional complex systems dynamics. While traditional physics-principled computational models based on differential equations provide high-fidelity solutions, their prohibitive computational cost hinders their applications . With the abundance of data, from both high-fidelity simulations and real-world measurements, machine learning (ML) surrogate models have become one of the exciting emerging solutions to learn the underlying governing dynamics of many real-world problems . These surrogates have enabled efficient modeling of complex processes, instead of solely depending on solving their corresponding time-consuming, computationally expensive Ordinary or Partial Differential equation systems (ODEs/PDEs). More specifically, recent studies have been investigating the development of ML methods to accelerate these computations, in a spectrum from purely data-driven ML surrogates, physics-informed neural networks (PINNs), to more recent hybrid models such as ML-augmented reduced-order models . For instance, Rudy et al.  utilize sparse regression to learn a system's governing PDEs while Raissi et al. , Raissi , Zhu and Zabaras  consider black-box neural network models trained by "physics-informed" loss functions. Although these methods center their attention on providing accurate solutions, they often need retraining with a change of system settings, including parameters as well as initial and boundary conditions. In DeGennaro et al. , the authors proposed a two-step method to infer the model parameter posterior enabling uncertaintyquantification after differential equation system identification. A Bayesian framework for deriving reduced-order models (ROMs), BayPOD (Boluki et al., 2024), has been recently developed in an end-to-end manner based on proper orthogonal decomposition (POD), motivated by the idea of developing ML-augmented ROMs with embedded physics constraints (Swischuk et al., 2019). ROM methods aim to derive physics-principled surrogates of high-fidelity complex models in significantly reduced lower-dimensional space to reduce the computational load while maintaining the desired accuracy (Hesthaven and Ubbiali, 2018). By formulating ROM learning in a Bayesian framework, BayPOD is capable of quantifying the uncertainty in addition to providing approximate high-fidelity differential equation solutions. This is a pivotal feature in scenarios where there is little observed data available, not atypical in science and engineering applications.

Although these endeavors have facilitated new ML surrogates for more efficient modeling and forecasting of complex processes, they often require considerably large training datasets to achieve satisfactory performances. Acquiring such training data from high-fidelity full-order models (FOMs) has heavy computational demands. To develop more data-efficient ML surrogates for ROM learning of complex systems, inspired by recent advancements of Active Learning (AL) in the ML community (Wang and Shang, 2014; Ash et al., 2020; Houlsby et al., 2011; Wu et al., 2022; Settles, 2009; Ren et al., 2021), we here aim to bridge this gap by developing active learning for ROMs instead of constructing ROMs based on large batches of randomly generated FOM data. More specifically, via iteratively suggesting the most informative data we intend to improve sample efficiency while preserving the underlying surrogate model's performance, reliability, and interoperability.

To develop and evaluate active learning for ROMs, we focus on BayPOD as the learned surrogate models by BayPOD come with their inherent uncertainty quantification capabilities in the adopted Bayesian learning framework. Here we promote a robust AL framework, BayPOD-AL, designed to showcase the feasibility and effectiveness of active learning for ROMs. With quantified uncertainty in BayPOD, we explore different uncertainty-based active learning strategies to utilize the estimated uncertainty for efficient guidance of the AL procedure. Recent studies in ML have suggested that uncertainty-based AL (UAL) methods can be unreliable in improving sample efficiency while optimizing the model's performance under specific scenarios (Munjal et al., 2022; Saifullah et al., 2022; Hacohen et al., 2022; Rahmati et al., 2024). Exploring and evaluating different UAL strategies for ROM learning is critical to help understand and prevent potential performance degradation under the new ROM learning settings. In particular, as we focus on modeling complex systems behavior with ROMs, there is an inherent mismatch between the ROM surrogates and the systems to approximate. In Rahmati et al. (2024), the authors suggested error-based acquisition functions to alleviate the issues caused by the model mismatch. When developing ROMs for differential equation systems, the Mean Squared Error (MSE) is often considered as the target criterion. In this study, we propose and evaluate _BayPOD-UAL_ that is guided by an acquisition function depending solely on the estimated uncertainty and _BayPOD-EAL_ that relies on the estimated error. BayPOD-EAL, by taking advantage of the results in Savvides et al. (2024), is expected to achieve higher sample efficiency due to its objective-driven formulation directly targeting reducing MSE for ROM learning (Yoon et al., 2013; Boluki et al., 2019; Yoon et al., 2021). Throughout our experiments, we demonstrate the effectiveness of BayPOD-AL in improving sample efficiency when learning a BayPOD surrogate predicting the temperature evolution over a rod. Finally, by investigating its performance on a temporally high-resolution dataset, we further show its efficiency and robustness.

## 2 Active Learning for Reduced-Order Models

Before delving into active learning for ROMs, we first briefly review ROMs, especially the family of POD-based ROMs, including BayPOD.

### Reduced-Order Models

As mentioned in Section 1, solving the full-order models (FOMs) to provide the high-fidelity solutions of the governing ODEs/PDEs is prohibitively expensive. Reduced-order models (ROMs) are designed with the objective of reducing the computational cost by estimating FOM solutions in a lower-dimensional space, subject to keeping the information loss to a minimum (Benner et al., 2015; Besselink et al., 2013; Penzl, 2006; Swischuk et al., 2019; Pant et al., 2021). Compared to pure data-driven "black-box" ML surrogates, learning ROMs of differential equation systems, naturally allowsintegrating the underlying scientific principles. We focus on proper orthogonal decomposition (POD), one of the most widely used model reduction methods to derive low-dimensional representations of the high-dimensional system states (Swischuk et al., 2019). Following the notations from Boluki et al. (2024), consider a function \(f:\), where \(\), the spatial domain, \(\), the time domain, and \(\), the input domain are mapped to a physical field. Denoting a snapshot \((t;)^{n_{x}}\) as the discretized spatial domain at time \(t\) and input model parameters \(\), with \(n_{t}\) different time points and \(n_{}\) different inputs, the POD bases can be obtained via singular value decomposition (SVD). Specifically, defining \(F=[(t_{i};_{j})]^{n_{x} n_{x}}\) with total \(n_{s}=n_{t}n_{p}\) snapshots, the SVD is \(F=V W\), which enables approximating field \(f\) by the \(K\)-dimension POD basis for any input model parameters.

### Bayesian POD (BayPOD)

BayPOD focuses on simultaneously deriving low-dimensional projection and mapping from input parameters of the full-order model to latent projection coefficients. To learn the map \(:\), we consider neural network mappings. To account for physics constraints, BayPOD reformulates the linear representation of each snapshot's approximation, \(}\), constituting of the POD approximation and a _particular solution_ given the corresponding initial and boundary conditions \(}(t;)=}+_{k=1}^{K}}_{k} _{k}(t;)\), where \(_{k}(t;)\) is the corresponding _learnable_ POD expansion coefficient, and \(}_{k}^{n_{x}}\) is the \(k\)-th left singular vector of \(F\) as the POD bases that satisfy homogeneous boundary conditions. Here \(}\) is the _particular solution_ given the set of potentially inhomogeneous initial and boundary conditions for better embedding physics constraints (Swischuk et al., 2019; Boluki et al., 2024). The corresponding field value of snapshot \(s\) at the spatial point \(x\), \(_{sx}\), is modeled as a normal random variable:

\[_{sx}(_{x}^{}_{s},_{x}^{-1}) \]

with \(_{x}^{K}\) the \(K\)-dimensional POD basis at position \(x\), and \(_{x}^{-1}\) the variance at \(x\). Using mean-field variational inference, BayPOD finds the variational posterior of model parameters. Due to its generative nature, it provides uncertainty estimates of predicted system dynamics in different setups, which is the enabler of optimal and adaptive decision making, active learning for sample efficiency in this work.

### BayPOD-AL

We now present our active learning framework, BayPOD-AL, by exploring different uncertainty-based active learning (UAL) strategies, leveraging the inherent UQ capabilities of BayPOD. Consider \(}\) and \(}\) the iteratively updated 'labeled' and 'unlabeled' datasets corresponding to collected snapshots from high-fidelity FOMs and the FOM settings without actual simulated snapshots, respectively. BayPOD-AL focuses on reducing the cost of running FOM simulations to train BayPOD by choosing the most informative FOM settings in \(}\) to collect new BayPOD training data from FOM based on an acquisition function \(\). Specifically, BayPOD-AL iteratively queries for snapshots, solutions to corresponding FOM differential equations, with the considered most informative settings, the corresponding input FOM parameters in this work \(^{*}=*{arg\,max}_{}}(q (|}),_{})\). Here the acquisition function \((q(|}),_{})\) guides the AL procedure considering the potential uncertainty of iteratively updated BayPOD models. At each AL iteration, with the currently learned BayPOD model, BayPOD-AL determines the most informative input FOM parameter in \(}\), i.e. \(^{*}\), as the output to query additional training snapshots from the FOM. Note that BayPOD directly provides the model posterior given the labeled data or simulated snapshots, \(q(|})\). Until the trained BayPOD model reaches a satisfactory performance, BayPOD-AL continues to add new simulated FOM snapshots to \(}\).

In our BayPOD-AL framework, due to solving a homogeneous problem that frees us from worrying about boundary/initial constraints, we only search for the most 'informative' input FOM parameters \(^{*}}\). Assuming that \(}\) contains \(n_{}\) inputs, and for each input \(_{}}\), we want to query FOM solutions for \(n_{}^{}\)**fixed** time points. At each AL iteration, we compute the acquisition function for a batch of \(n_{}^{}\) snapshots for **each** input. Finally, the most informative snapshots from FOMs will be appended to \(}\) for the next AL and model update iteration. This process continues

Figure 1: A schematic illustration of the proposed BayPOD-AL framework.

until the model reaches the desired performance. Having access to the BayPOD's variational posterior \(q(|})\), we define a measure function \(M^{(t)}(q(|},},x)): \) estimating the 'informativeness' of snapshots at input \(}\), time \(t\), and position \(x\). By taking the average over all snapshots given \(}\), we define the following acquisition function evaluating such 'informativeness' based on the adopted measure function:

\[^{(i)}_{}=^{}}_{t}^{n_{t} ^{}}}_{x}^{n_{x}}M^{(i,t)}(q(|}, },x)), i\{0,,n_{}\} \]

BayPOD-AL then queries the corresponding FOM for snapshots by the most 'informative' input:

\[}=^{(i)}=_{i}^{(i)}_{}. \]

Figure 1 provides a schematic illustration of BayPOD-AL. As discussed in Hino (2020); Zhao et al. (2021); Rahmati et al. (2024), the acquisition function plays a critical role in achieving desired AL efficiency. To derive efficient active learning for BayPOD, we evaluate two different choices by considering: 1) _BayPOD-UAL_: the predictive model uncertainty based on the estimated posterior predictive variance from BayPOD; and 2) _BayPOD-EAL_: the estimated approximation error, directly targeting the ROM learning objective, for which we estimate the upper bound of the approximation error and utilize that as the measure function in (2). Details of each strategy are provided in Appendix B and C.

## 3 Experiments

Using the same example as in Boluki et al. (2024), we implement our BayPOD-AL on predicting the evolution of temperature fields, \(f\), with heat diffusivity parameter, \(\), over a rod of length \(L\) with time-dependent boundary conditions. In the following, we report the performance statistics of five runs of experiments for each of AL algorithms. Each run is different only in the initial \(}\). We compare the performance of BayPOD-UAL and BayPOD-EAL with the random sampling strategy which selects the next \(\) for FOM simulations randomly at each iteration.

We train BayPOD on the temporally low-resolution training snapshots and report the AL performances on both temporally low-\((n_{t}^{}=50)\) and high-resolution (\(n_{t}^{}=200\)) test datasets. Figure 2 compares the performance of BayPOD-EAL, BayPOD-UAL, and random sampling strategies. Table 1 summarizes the performance statistics of our results after 5 AL iterations. It is clear after 5 AL iterations (250 new snapshots), BayPOD-EAL leads to a model with the best empirical performance. Over the first 10 AL iterations, on average it has \(4.7\) and \(9\) lower MSE than BayPOD-UAL, and \(6\) and \(6.5\) lower MSE than random sampling when evaluated on low- and high-resolution datasets respectively. This further demonstrates the robustness of BayPOD-EAL, in leading the model to optimal performance even when the training data resolution differs from the test data, owing to it being objective-driven. As mentioned in Section 1, due to the inherent model mismatch, uncertainty alone cannot efficiently reduce the labeling cost for learning ROMs. On both datasets, BayPOD-UAL grants an initial performance lead compared to random sampling, but it soon slows down with comparable performance for the low-resolution dataset and worse performance for the high-resolution dataset. After 15 to 20 AL iterations, both BayPOD-EAL and random sampling provide comparable performance, meaning that BayPOD-AL reduces the computational cost related to training data by a factor of \(3\) to \(4\), demonstrating its cost-effectiveness in learning ROMs.

## 4 Conclusion

To model the complex systems dynamics, ML surrogate models have shown promising performance. However, the prohibitive cost of acquiring the solutions of the high-fidelity FOMs representing them

Figure 2: Performance comparison of BayPOD-EAL, BayPOD-UAL, and random sampling strategies on ‘low’ (left) and ‘high’ (right) temporal resolution test datasets.

hinders the applicability of these models in real-world scenarios. Inspired by recent advances in AL, we present the BayPOD-AL framework, which iteratively suggests the most informative data that can boost the surrogate model's performance. The Bayesian framework explicitly updates the model posterior, with which different objective-driven acquisition functions targeting ROM learning can be adopted to achieve efficient AL. Our experiments demonstrate the robustness of the proposed framework as well as its efficacy in reducing the cost of learning ROMs, thereby improving their applicability in real-world problems.