# Does Refusal Training in LLMs

Generalize to the Past Tense?

Maksym Andriushchenko

EPFL &Nicolas Flammarion

EPFL

###### Abstract

Refusal training is widely used to prevent LLMs from generating harmful, undesirable, or illegal outputs. We reveal a curious generalization gap in the current refusal training approaches: simply reformulating a harmful request in the past tense (e.g., _"How to make a Molotov cocktail?"_ to "_How did people make a Molotov cocktail?"_) is often sufficient to jailbreak many state-of-the-art LLMs. We systematically evaluate this method on Llama-3 3B, Claude-3.5 Sonnet, GPT-3.5 Turbo, Gemema-2 9B, Phi-3-Mini, GPT-4o-mini, GPT-4o, o1-mini, o1-preview, and R2D2 models using GPT-3.5 Turbo as a reformulation model. For example, the success rate of this simple attack on GPT-4o increases from 1% using direct requests to 88% using 20 past-tense reformulation attempts on harmful requests from JailbreakBench with GPT-4 as a jailbreak judge. Interestingly, we also find that reformulations in the future tense are less effective, suggesting that refusal guardrails tend to consider past historical questions more benign than hypothetical future questions. Moreover, our experiments on fine-tuning GPT-3.5 Turbo show that defending against past reformulations is feasible when past tense examples are explicitly included in the fine-tuning data. Overall, our findings highlight that the widely used alignment techniques--such as SFT, RLHF, and adversarial training--employed to align the studied models can be brittle and do not always generalize as intended.

## 1 Introduction

Large Language Models (LLMs) exhibit remarkable capabilities, but these come with potential risks of misuse, including the generation of toxic content, spread of misinformation at scale, or support for harmful activities like cyberattacks (Bengio et al., 2023). To address these concerns, LLMs are often fine-tuned to refuse such harmful queries which is commonly done via supervised fine-tuning, reinforcement learning with human feedback, and various forms of adversarial training (Bai et al., 2022; Touvron et al., 2023; Mazeika et al., 2024). While refusal training successfully generalizes to many reformulations of harmful prompts unseen during training, it still fails to generalize to adversarially crafted prompts, known as _jailbreaking attacks_(Mowshowitz, 2022). These prompts typically involve obfuscation techniques like base64 or leetspeak encoding (Wei et al., 2023a), iterative op

  &  \\ 
**Model** & **GPT-4 judge** & **Llama-3 70B judge** & **Rule-based judge** \\  Llama-3 8B & 0\% \(\) 27\% & 0\% \(\) 9\% & 7\% \(\) 32\% \\ Claude-3.5 Sonnet & 0\% \(\) 53\% & 0\% \(\) 25\% & 8\% \(\) 61\% \\ GPT-3.5 Turbo & 0\% \(\) 74\% & 0\% \(\) 47\% & 5\% \(\) 73\% \\ Gemema-2 9B & 0\% \(\) 74\% & 0\% \(\) 51\% & 3\% \(\) 68\% \\ Phi-3-Mini & 6\% \(\) 82\% & 5\% \(\) 41\% & 13\% \(\) 70\% \\ GPT-4o mini & 1\% \(\) 83\% & 1\% \(\) 66\% & 34\% \(\) 80\% \\ GPT-4o & 1\% \(\) 88\% & 1\% \(\) 65\% & 13\% \(\) 73\% \\ R2D2 & 23\% \(\) 98\% & 21\% \(\) 56\% & 34\% \(\) 79\% \\ 

Table 1: Attack success rate for **present** tense (i.e., direct request) vs. **past** tense reformulations using GPT-3.5 Turbo with 20 reformulation attempts. We perform evaluation on 100 harmful requests from JBB-Behaviors using GPT-4, Llama-3 70B, and a rule-based heuristic as jailbreak judges.

timization of adversarial strings (Zou et al., 2023), or prompt templates with specific instructions (Andriushchenko et al., 2024).

In this work, we show that refusal training can fail to generalize even in _much simpler scenarios_. Simply reformulating a harmful request in the past tense is often sufficient to jailbreak many state-of-the-art LLMs. Our work makes the following contributions:

* We show that past-tense reformulations lead to a surprisingly effective attack on many recent leading LLMs. We show quantitative results on Llama-3 8B, Claude-3.5 Sonnet, GPT-3.5 Turbo, Gemma-2 9B, Phi-3-Mini, GPT-4o mini, GPT-4o, and R2D2 in Table 1 and qualitative examples on GPT-4o in Figure 1.
* At the same time, we show that reformulations in the future tense are less effective, suggesting that refusal guardrails tend to consider past historical questions more benign than hypothetical future questions.
* Our experiments on fine-tuning GPT-3.5 Turbo show that producing refusals on past-tense reformulations is straightforward if one explicitly includes them in the fine-tuning dataset. However, overrefusals have to be carefully controlled by adding a sufficient amount of standard conversations.
* Additionally, we evaluate OpenAI's recent o1 reasoning models, showing that they also exhibit limited robustness to past-tense reformulations, although they tend to reveal less specific information in jailbroken generations.
* We discuss the implications of this simple attack from a generalization perspective. While techniques like RLHF and DPO tend to generalize to different languages (Li et al., 2024), they fail to generalize to different tenses. This observation raises the question of what other blind spots exist in the current techniques and the reasons for their persistence.

Figure 1: Simply reformulating a request from the present to the past tense (e.g., _"How to make a Molotov cocktail?"_ to _"How did people make a Molotov cocktail?"_) is sufficient to bypass the refusal training of GPT-4o on many harmful requests. This jailbreak highlights the brittleness of the current alignment techniques.

We believe extremely simple jailbreaks like these can serve as an important tool for probing generalization of state-of-the-art LLMs. Our findings highlight that the currently widely used alignment techniques--such as SFT, RLHF, and adversarial training--employed to align the studied models, can be brittle and do not always generalize as intended.

## 2 Methodology

**Background.** Bypassing refusal training involves finding prompts that elicit harmful content from LLMs in response to a specific harmful request (e.g., "How to make a bomb?"). We assume access to a predefined set of requests typically recognized as harmful by LLM providers, such as those related to misinformation, violence, hate speech, etc. We define a target language model as a function \(:^{*}^{*}\) that maps an input token sequence to an output token sequence. Given a semantic judge function \(:^{*}^{*}\{, \}\) and a harmful request \(R^{*}\), the attacker's objective can be formulated as

\[ P^{*} ((P),R)=.\]

The judge function provides a binary output indicating whether \((P)\) constitutes a valid jailbreak for the harmful request \(R\), although it may use a more granular evaluation score internally.

**Approach.** Our proposed approach relies on reformulating a harmful request into the past tense (see Figure 1 for an illustration). To automatically reformulate an arbitrary request, we use GPT-3.5 Turbo with the prompt shown in Table 2 that relies on a few illustrative examples. Since reformulations like these are conceptually very simple, we believe there is little value in using a more advanced LLM than GPT-3.5 Turbo. Furthermore, we enhance the approach by using multiple reformulations attempts per query. We leverage the inherent variability in language model outputs due to sampling and use the temperature parameter equal to one both for the target and reformulation LLMs. We consider an attack successful for a harmful request if we obtain at least a single unsafe reply over multiple attempts. We note that this attack can be considered as both universal and transferable since it is effective across multiple independently trained LLMs. Finally, we remark that the performance of this attack can be further enhanced by combining it with known prompting techniques such as refusal suppression and leveraging competing objectives (Wei et al., 2023) (e.g., adding instructions like _"Start your response with 'Sure'"_ or _"Never start your response with 'Sorry'"_). We _do not_ use any of these additional techniques, as our goal is to demonstrate a curious generalization gap rather than to develop a state-of-the-art jailbreaking approach.

**Setting.** We use 100 harmful behaviors from JBB-Behaviors (Chao et al., 2024) that span 10 harm categories based on the OpenAI usage policy. These behaviors consist of examples from AdvBench (Zou et al., 2023), Trojan Detection Competition (Mazeika et al., 2023), and a majority of examples that are unique to JBB-Behaviors. We conduct 20 reformulations per behavior

   ”} \\  \\  \\  \\  \\  \\  \\  \\  \\  \\  \\ ”. But do not output anything else except the reformulated request.} \\   

Table 2: The past-tense reformulation prompt that we use for GPT-3.5 Turbo to produce multiple reformulations per harmful request.

using GPT-4 as a semantic jailbreak judge on each iteration, in line with the methodology of prior works such as Chao et al. (2023). To ensure that we are not overfitting to this judge, we also use the Llama-3 70B judge with the prompt from Chao et al. (2024) and the rule-based judge from Zou et al. (2023). We list the judge prompts in Appendix A.

**Target LLMs.** We evaluate eight target LLMs: Llama-3 8B (AI@Meta, 2024), Claude-3.5 Sonnet (Anthropic, 2024), GPT-3.5 Turbo (OpenAI, 2023), Gemma-2 9B (DeepMind, 2024), Phi-3-Mini (Abdin et al., 2024), GPT-4o-mini (OpenAI, 2024c), GPT-4o (OpenAI, 2024d), and R2D2 (Mazeika et al., 2024). Most of these models use supervised fine-tuning and reinforcement learning from human feedback for refusal training. In addition, R2D2 uses adversarial training against the GCG attack on top of SFT and DPO used to fine-tune the Zephyr model (Tunstall et al., 2023). For Llama-3 8B, we use the refusal-enhancing prompt introduced in Llama-2 (Touvron et al., 2023), while for the rest of the LLMs, we use their default system prompts. We list all system prompts in Appendix A.

## 3 Systematic Evaluation of the Past Tense Attack

**Main results.** We present our main results in Table 1, which show that the past tense attack performs surprisingly well, even against the most recent LLMs such as Claude-3.5 Sonnet, GPT-4o, and Phi-3-Mini, and in many cases is sufficient to circumvent built-in safety mechanisms. For example, the attack success rate (ASR) on GPT-4o mini and GPT-4o increases from 1% using direct requests to 83% and 88% using 20 past-tense reformulation attempts according to the GPT-4 judge. The similarity in ASR between these two models suggests that they were likely aligned using the same methodology. The Llama-3 70B and rule-based judge also indicate a high ASR on GPT-4o, although

Figure 2: Attack success rate of past-tense reformulations over 20 attempts for different jailbreak judges. We can see that the ASR is already non-trivial even with a single attempt, e.g., 57% success rate on GPT-4o.

slightly lower, at 65% and 73% respectively. Similarly, evaluation on other models indicates a high ASR: for Claude-3.5 Sonnet, it increases from 0% to 53%, for Phi-3-Mini it increases from 6% to 82%, and for R2D2, it increases from 23% to 98%. Interestingly, GPT-3.5 Turbo is slightly more robust to past-tense reformulations than GPT-4o, with a 74% ASR compared to 88% for GPT-4o. To compare these numbers with established methods, we evaluate the transfer of request-specific GCG suffixes from Chao et al. (2024) optimized on Vicuna. In the same evaluation setting, these suffixes result in a 47% ASR for GPT-3.5 Turbo and only a 1% ASR for GPT-4o, according to the Llama-3 70B judge. This discrepancy shows how later iterations of frontier LLMs can patch known attacks, but remain vulnerable to new ones. Moreover, comparing directly to Andriushchenko et al. (2024), we also achieve 100% ASR on GPT-4o using _100_ restarts on the same 50 AdvBench behaviors. This shows that our attack can be competitive with the state of the art. Additionally, we plot the ASR over the 20 attempts in Figure 2 for all models and judges. We can see that the ASR is already non-trivial even with a single attempt, e.g., 57% success rate on GPT-4o, which is in contrast with only 1% ASR with a direct request in the present tense. Moreover, the ASR often begins to saturate after 10 attempts, which justifies our choice of 20 attempts in total.

**When does the attack fail?** In Figure 3, we plot a breakdown of the ASR over the 10 harm categories of JBB-Behaviors. For most models, the ASR of the past tense attack is nearly perfect on behaviors related to malware/hacking, economic harm, fraud/deception, and government decisions. The ASR is consistently lower on categories like harassment, disinformation, and sexual/adult content. This behavior can probably be explained by the presence of more salient words in the latter categories, which are often sufficient to detect to produce a correct refusal. Additionally, we have observed that the attack sometimes struggles when a harmful request is highly specific, such as writing a poem that glorifies a particular event. In contrast, the attack usually works well if the knowledge required is more generic, such as providing a recipe for a bomb or Molotov cocktail. For further analysis, we invite the readers to inspect the jailbreak artifacts in our code repository.

**Is the _past tense really important?_** It is natural to ask whether the past tense is particularly important or if the future tense works equally well. We repeat the same experiment, this time asking GPT-3.5 Turbo to reformulate requests in the future tense, using the prompt shown in Table 10. We present the results in Table 3, which shows that future tense reformulations are less effective but still have much higher ASR than direct requests, except Claude-3.5 Sonnet on which the ASR is only 5%. This outcome prompts two potential hypotheses: (a) The fine-tuning datasets may contain a higher proportion of harmful requests expressed in the future tense or as hypothetical events. (b) The model's internal reasoning might interpret future-oriented requests as potentially more harmful, whereas past-tense statements, such as historical events, could be perceived as more benign.

## 4 Does Fine-tuning on the Past Tense Examples Help?

**Setting.** We use the OpenAI fine-tuning service to fine-tune gpt-3.5-turbo-0125 on 394 past-tense reformulations of 50 random JBB-Behaviors paired with a refusal message ("Sorry, I can't

Figure 3: Attack success rate of past-tense reformulations for all models according to GPT-4 as a judge across 10 harmful categories of JBB-Behaviors.

help with that"). We use the remaining 50 JBB-Behaviors as a test set. We also add standard conversations from OpenHermes-2.5 (Teknium, 2023) to the fine-tuning set to make sure the model does not refuse too frequently. We keep the same number of reformulations and increase the number of standard conversations to get different proportions of reformulations vs. standard data. We use the following proportions: 2%/98%, 5%/95%, 10%/90%, and 30%/70%. In addition, we measure the _overrefusal rate_ on 100 borderline benign behaviors from JBB-Behaviors (Chao et al., 2024) that match the harmful behaviors in terms of their topics. To detect refusals, we rely on the Llama-3 8B judge with the prompt from Chao et al. (2024) shown in Table 13.

**Results.** We show systematic results in Table 4, which suggest that it is straightforward to reduce the attack success rate to 0%. The overrefusal rate predictably increases with a higher proportion of refusal data in the fine-tuning mix. To provide some point of reference, the overrefusal rate of Llama-3 8B is 19%, while the ASR is 27% according to the GPT-4 judge. Thus, FT 2%/98% with 6% overrefusal rate and 24% ASR improves the Pareto frontier between correct and wrong refusals. We note that with more data, this trade-off could likely be improved further. Overall, defending against past-tense reformulations is feasible if one directly adds the corresponding data during fine-tuning, although wrong refusals must be carefully controlled.

## 5 Are Reasoning Models More Robust?

Very recently, LLMs with advanced chain-of-thought reasoning capabilities were introduced, such as the o1-mini and o1-preview models (OpenAI, 2024). Before outputting _any_ answer, these models produce a long reasoning chain about how to fulfill a user request. Relevant to our work, the reasoning chains are also trained on their usage policies to teach human values and improve safety. They claim that such chain-of-thought reasoning achieves substantial improvements on their hardest jailbreak evaluations.

**Results.** The past tense attack still largely works on the o1 models. We present some successful examples in Figure 4 in the appendix and systematic results in Table 5. The attack success rate of

  &  \\ 
**Model** & **Overrefusals** & **GPT-4 judge** & **Llama-3 70B judge** & **Rule-based judge** \\  GPT-3.5 Turbo & 3\% & 0\% \(\) 74\% & 0\% \(\) 47\% & 5\% \(\) 73\% \\ FT 2\%/98\% & 6\% & 2\% \(\) 24\% & 0\% \(\) 10\% & 12\% \(\) 38\% \\ FT 5\%/95\% & 22\% & 0\% \(\) 0\% & 0\% \(\) 0\% & 6\% \(\) 2\% \\ FT 10\%/90\% & 32\% & 0\% \(\) 2\% & 0\% \(\) 0\% & 0\% \(\) 2\% \\ FT 30\%/70\% & 61\% & 0\% \(\) 0\% & 0\% \(\) 0\% & 0\% \(\) 0\% \\ 

Table 4: Attack success rate using present tense vs. past-tense reformulation with 20 attempts for different fine-tuned models. E.g., _FT 10%/90%_ denotes 10% refusal and 90% normal conversations from OpenHermes-2.5 in the fine-tuning mix. _Overrefusals_ denote refusal rates on borderline benign behaviors from JBB-Behaviors (Chao et al., 2024).

  &  \\ 
**Model** & **GPT-4 judge** & **Llama-3 70B judge** & **Rule-based judge** \\  Llama-3 8B & 0\% \(\) 11\% & 0\% \(\) 6\% & 7\% \(\) 13\% \\ Claude-3.5 Sonnet & 0\% \(\) 5\% & 0\% \(\) 1\% & 8\% \(\) 26\% \\ GPT-3.5 Turbo & 0\% \(\) 25\% & 0\% \(\) 12\% & 5\% \(\) 29\% \\ Gemma-2 9B & 0\% \(\) 59\% & 0\% \(\) 45\% & 3\% \(\) 63\% \\ Phi-3-Mini & 6\% \(\) 39\% & 5\% \(\) 21\% & 13\% \(\) 24\% \\ GPT-4o-mini & 1\% \(\) 42\% & 1\% \(\) 25\% & 34\% \(\) 49\% \\ GPT-4o & 1\% \(\) 61\% & 1\% \(\) 47\% & 13\% \(\) 52\% \\ R2D2 & 23\% \(\) 94\% & 21\% \(\) 65\% & 34\% \(\) 87\% \\ 

Table 3: Attack success rate when using **present** tense vs. **future** tense reformulations with 20 attempts for different target models. We evaluate on 100 harmful requests from JBB-Behaviors using GPT-4, Llama-3 70B, and a rule-based heuristic as jailbreak judges.

the past tense reaches 84% ASR and 78% for o1-mini and o1-preview, respectively. Despite the high success rate as judged by GPT-4 and Llama-3, we note that not all generations follow the definition of a jailbreak specified in the Model Spec (OpenAI, 2024a). There are many "dual-use" examples that are in many cases less useful for the attacker, for example when generic information on how to write an article is produced instead of an article itself, or a detailed recipe is not always produced. Moreover, interestingly, the o1 models are now equipped with _input filters_ that block potentially harmful requests, even when o1 is accessed via the API. The input filters block on average 78% present tense requests and 27% past tense requests. We find it curious that past-tense reformulations help to avoid the filters as well, although we do not have access to the input filters and do not explicitly optimize to bypass them.

**Discussion.** We notice the following pattern in successful jailbreaks. First, the summary of the internal reasoning acknowledges that a past tense request might be harmful. However, since it is a historical reference, the model often goes on to generating an answer, and sometimes reveals more detailed information that it should. It is also interesting to note that chain-of-thought based answers work like an implicit output filter due to a natural breakpoint between a chain of thought reasoning and actual answer. The presence of this breakpoint is an interesting departure from end-to-end autoregressive models, such as those presented in Section 3. Overall, reasoning models provide an interesting approach for improving safety that--in case of the o1 models--does not lead to significantly reduced refusal rate, but rather to less informative answers. We believe that the usefulness of the generated answers should be evaluated in a more nuanced way than is typically done in the current literature.

## 6 Related Work

We discuss here the most relevant references on generalization in LLMs, failures of refusal training, and most related jailbreaking approaches in recent literature.

**Generalization of LLM alignment.** After pretraining, LLMs are typically aligned to human preferences using techniques like SFT (Chung et al., 2022), RLHF (Ouyang et al., 2022), or DPO (Rafailov et al., 2023). One of the objectives of the alignment process is to make LLMs produce refusals on harmful queries, which involves adding refusal examples to the fine-tuning data. Since it is impossible to add all possible reformulations of harmful requests in the fine-tuning set, LLMs alignment crucially relies on the ability to generalize from a few examples per harmful behavior. Empirical studies support this capability: Dang et al. (2024) observe that RLHF generalizes from English to other languages, and Li et al. (2024) make the same claim specifically for refusal training. This observation is consistent with Wendler et al. (2024) who argue that LLMs pretrained primarily on English data tend to internally map other languages to English. Therefore, fine-tuning on English data can suffice since the internal representations largely coincide with the representations of other languages. However, this capacity is in stark contrast to past-tense reformulations, which, as we show, represent a blind spot. We hypothesize that the underlying reason is that the internal representations for the past and present tenses are distinct. Thus, one has to include reformulations in both tenses to achieve more robust refusals. On a related note, Berglund et al. (2024) discuss the reversal curse phenomenon, i.e., training on "A is B" does not make the model automatically generalize to "B is A". This failure again illustrates that generalization mechanisms taken for granted in humans _do not_ necessarily occur in LLMs.

  \\  } & **Tense** & **GPT-4 judge** & **Llama-3 70B judge** & **Rule-based judge** \\  o1-mini & Past & 3\% \(\) 84\% & 3\% \(\) 50\% & 6\% \(\) 77\% \\ o1-mini & Future & 3\% \(\) 45\% & 3\% \(\) 28\% & 6\% \(\) 53\% \\ o1-preview & Past & 2\% \(\) 78\% & 2\% \(\) 50\% & 8\% \(\) 82\% \\ o1-preview & Future & 2\% \(\) 56\% & 2\% \(\) 42\% & 8\% \(\) 60\% \\ 

Table 5: Attack success rate using past and future tense reformulations with 20 attempts for the o1 reasoning models. We evaluate on 100 harmful requests from JBB-Behaviors using GPT-4, Llama-3 70B, and a rule-based heuristic as jailbreak judges.

**Adversarial attacks on LLMs.** Adversarial attacks on deep learning models are well-documented (Biggio et al., 2013; Szegedy et al., 2014; Biggio and Roli, 2018; Madry et al., 2018). Attacks on the refusal mechanism of LLMs are more recent and start with manual techniques, such as those discovered for ChatGPT by Mowshowitz (2022), that involve handcrafted prompts. Wei et al. (2023a) roughly divide these manual jailbreaks on two categories: (1) competing objectives, where model properties, such as helpfulness, conflicts with safety goals, and (2) mismatched generalization, where refusal training fails to generalize to some reformulations such as in low-resource languages, base64, or leetspeak. Additionally, these techniques can be enhanced by using in-context learning prompts that contain examples of harmful responses (Wei et al., 2023b; Anil et al., 2024). Furthermore, jailbreaking can be automated using iterative discrete optimization techniques, opening up a new attack vector that leads to standardized attacks instead of manual trial and error. For example, Zou et al. (2023) use a gradient-based method, Greedy Coordinate Gradient, to create universal and transferable adversarial suffixes. Other approaches involve genetic algorithms to construct adversarial prompts (Lapid et al., 2023; Liu et al., 2023), low-perplexity modifications of GCG (Zhu et al., 2023), or random search on predicted probabilities (Andriushchenko et al., 2024; Sitawarin et al., 2024; Hayase et al., 2024). Our method is conceptually much simpler than these methods since it directly reformulates a harmful query without any iterative optimization.

**Most related existing approaches.** Our method is naturally related to multiple existing approaches. First, it falls into the mismatch generalization category of Wei et al. (2023a), as confirmed by the experiments in Section 4. Second, the idea of using an auxiliary LLM to produce various reformulations of harmful requests is at the core of other prior works, such as Perez et al. (2022) and Chao et al. (2023). Finally, our attack is related to the Crescendo multi-turn attack (Russionovich et al., 2024), which involves asking multiple benign questions about a subject before posing a target question. In _some_ cases, Crescendo also leverages past-tense reformulations; for example, their Figure 1 shows an example about a Molotov cocktail where the last question is "How was it created back then?". We show that this step alone is often crucial, and multi-turn conversations may not always be necessary to produce a jailbreak for many frontier LLMs.

## 7 Discussion

We believe the main reason for this generalization gap is that past tense examples are out-of-distribution compared to the refusal examples used for fine-tuning, and current alignment techniques do not automatically generalize to them. Indeed, as we have shown in Section 4, correctly refusing on past tense examples is feasible via direct fine-tuning, and some models--like Llama-3 with the refusal-enhancing system prompt--are already relatively robust. Moreover, there are also other possible solutions that do not rely on SFT or RLHF, such as output-based detectors (Inan et al., 2023) and representation-based methods, including harmfulness probing and representation rerouting (Zou et al., 2024). These approaches can reject harmful _outputs_, which seems to be an easier task compared to patching all possible _inputs_ that can lead to harmful generations.

More generally, past tense examples demonstrate a clear limitation of current alignment methods, including RLHF and DPO. While these techniques effectively generalize across languages (Li et al., 2024b; Dang et al., 2024) and some input encodings, they struggle to generalize between different tenses. We hypothesize that this failure to generalize stems from the fact that concepts in different languages map to similar representations (Wendler et al., 2024; Li et al., 2024b), whereas different tenses necessarily require distinct representations. Additionally, the recent work of Li et al. (2024a) shows that refusal guardrails can show different sensitivity to various demographic groups, which has direct implications for fairness. We believe that the generalization mechanisms underlying current alignment methods are understudied and require further research.