# Conformal Classification with Equalized Coverage for Adaptively Selected Groups

Yanfei Zhou

Department of Data Sciences and Operations

University of Southern California

Los Angeles, California, USA

yanfei.zhou@marshall.usc.edu

&Matteo Sesia

Department of Data Sciences and Operations

University of Southern California

Los Angeles, California, USA

sesia@marshall.usc.edu

###### Abstract

This paper introduces a conformal inference method to evaluate uncertainty in classification by generating prediction sets with valid coverage conditional on adaptively chosen features. These features are carefully selected to reflect potential model limitations or biases. This can be useful to find a practical compromise between efficiency--by providing informative predictions--and algorithmic fairness--by ensuring equalized coverage for the most sensitive groups. We demonstrate the validity and effectiveness of this method on simulated and real data sets.

## 1 Introduction

### Uncertainty, Fairness, and Efficiency in Machine Learning

Increasingly sophisticated machine learning (ML) models, like deep neural networks, are revolutionizing decision-making in many high-stakes domains, including medical diagnostics , job screening , and recidivism prediction . However, serious concerns related to _uncertainty quantification_ and _algorithmic fairness_ underscore the need for novel methods that can provide reliable and unbiased measures of confidence, applicable to any model.

Uncertainty quantification is crucial because ML models, although effective on average, can make errors while displaying overconfidence . Consequently, in some situations users may lack sufficient warning about the potential unreliability of a prediction, raising trust and safety concerns. A promising solution is _conformal inference_, which enables converting the output of any model into prediction sets with precise coverage guarantees. These sets reflect the model's confidence on a case-by-case basis, with smaller sets indicating higher confidence in a specific prediction.

Algorithmic fairness focuses on the challenges of prediction inaccuracies that disproportionately impact specific groups, often identified by sensitive attributes like race, sex, and age. Among the many sources of algorithmic bias are training data that do not adequately represent the population's heterogeneity and a focus on maximizing average performance. However, fairness is partly subjective and lacks a universally accepted definition , leading to sometimes conflicting interpretations .

This complexity makes conformal inference with _equalized coverage_ an appealing approach. Equalized coverage aims to ensure that the prediction sets attain their coverage not only on average for the whole population (e.g., above 90%) but also at the same level within each group of interest. While this does not necessarily imply that the prediction sets will have equal size on average across different groups--since it is possible the predictive model may be more or less accurate for different groups--it objectively communicates the possible limitations of a model. This transparency helps decision-makers recognize when predictions may be less reliable for specific subgroups, allowing them to either avoid unnecessary actions or adopt more cautious strategies in cases of higher uncertainty, thereby minimizing the potential harm from inaccurate predictions.

A limitation of the method developed in  for conformal inference with equalized coverage is that it does not scale well to situations involving diverse populations with multiple sensitive attributes. In such cases, it necessitates splitting the data into exponentially many subsets, significantly reducing the effective sample size and leading to less informative predictions. Balancing this trade-off  between _efficiency_--aiming for highly informative predictions with small set sizes--and _fairness_-ensuring unbiased treatment--is challenging and requires novel approaches. This paper introduces a method to address this by providing equalized coverage conditional on carefully chosen features, informed by the model and data. While it cannot guarantee equalized coverage for _all_ sensitive groups, it seeks a _reasonable compromise_ with finite data sets, mitigating significant biases while retaining predictive power.

### Background on Conformal Inference for Classification

Consider a data set comprising \(n\) exchangeable (e.g., i.i.d.) observations \(Z_{i}\) for \(i:=[n]:=\{1,,n\}\), sampled from an arbitrary and unknown distribution \(P_{Z}\). In classification, one can write \(Z_{i}=(X_{i},Y_{i})\), where \(Y_{i}[L]:=\{1,,L\}\) is a categorical label and \(X_{i}\) represents the individual's features, taking values in some space \(\). As explained below, we will assume these features include some sensitive attributes. Further, we consider a test point \(Z_{n+1}=(X_{n+1},Y_{n+1})\), also sampled exchangeably from \(P_{Z}\), and whose label \(Y_{n+1}[L]\) has not yet been observed.

A standard goal for split conformal prediction methods is to quantify the predictive uncertainty of a given "black-box" ML model (e.g., pre-trained on an independent data set) by constructing a prediction set \((X_{n+1})\) for \(Y_{n+1}\), guaranteeing _marginal coverage_ at some desired level \((0,1)\):

\[[Y_{n+1}(X_{n+1})] 1-. \]

This probability is taken over the randomness in \(Y_{n+1}\) and \(X_{n+1}\), as well as in the data indexed by \(\). Intuitively, marginal coverage means the prediction sets are expected to cover the correct outcomes for a fraction \(1-\) of the population. However, this is not always satisfactory, especially if the miscoverage errors may disproportionately affect individuals characterized by well-defined features.

To address these concerns, one might consider _feature-conditional coverage_, \([Y_{n+1}(X_{n+1}) X_{n+1}=x] 1-\) for all \(x\). This would ensure consistent coverage for all possible test features \(X_{n+1}\). However, it is impossible to achieve without additional assumptions, such as modeling the distribution \(P_{Z}\) or significantly restricting the feature space \(\). Given that such assumptions may be unrealistic in real-world settings, exact feature-conditional coverage is typically unachievable.

Equalized coverage  seeks a practical middle ground between the two extremes of marginal and feature-conditional coverage, focusing on accounting for specific _discrete_ attributes encapsulated by \(X_{n+1}\). To facilitate the subsequent exposition of our method, it is useful to recall the definition of equalized coverage with the following notation.

Let \(K\) denote the number of sensitive attributes, and for each \(k[K]\) let \(M_{k}\) count the possible values of the \(k\)-th attribute. Consider a function \(:\{0,1\}^{K}^{d}\) for any subset \(A[K]\) with \(|A|=d\) elements, so that \((x,A)\) is a vector of length \(|A|\) representing the values of all attributes indexed by \(A\) for an individual with features \(x\). In the special case where \(A\) is an empty set, \(\) returns a constant. If \(A\) is a singleton, e.g., \(A=\{k\}\) for some \(k[K]\), then \((x,\{k\})[M_{k}]\) denotes the value of the \(k\)-th attribute; e.g., someone's academic degree. More generally, \((x,\{k,l\})[M_{k}][M_{l}]\), for any distinct \(k,l[K]\), denotes the joint values of two attributes, characterizing a smaller group, such as "males with a bachelor's degree.".

When multiple sensitive attributes are involved, i.e., \(K>1\), the concept of equalized coverage introduced by  can be naturally extended to _exhaustive equalized coverage_, defined as:

\[[Y_{n+1}(X_{n+1})(X_{n+1},[K])] 1-. \]

In words, this says \((X_{n+1})\) has valid coverage conditional on all \(K\) sensitive attributes. Prediction sets satisfying (2) can be obtained by applying the standard conformal calibration method separately within each of the \(M=_{k=1}^{K}M_{k}\) groups characterized by a specific combination of the protected attributes represented by \((X_{i},[K])\); see Appendix A1 for details. However, a downside of this approach is that the calibration subsets may be too small if \(M\) is large, leading to uninformative predictions for even moderate values of \(K\). This limitation forms the starting point of our work.

### Preview of Our Contributions: Adaptive Equalized Coverage

In practice, for a given model and data set, different groups may not exhibit the same need for rigorous equalized coverage guarantees (2), as conformal predictions may be able to approximately achieve the desired coverage even without explicit constraints. Algorithmic bias typically affects only a minority of the population, so standard prediction sets with marginal coverage (1) may approximately satisfy (2) for most groups. Therefore, we propose Adaptively Fair Conformal Prediction (AFCP), a new method that efficiently identifies and addresses groups suffering from algorithmic bias in a data-driven way, adjusting their prediction sets to equalize coverage without sacrificing informativeness.

AFCP involves two main steps. First, as sketched in Figure 1, it carefully selects a sensitive attribute \((X_{n+1})\{,\{1\},,\{K\}\}\), based on \(X_{n+1}\) and the data in \(\). Although AFCP can be extended to select multiple attributes, we begin by focusing on this simpler version for clarity. Intuitively, AFCP searches for the attribute corresponding to the group most negatively affected by algorithmic bias. It may also opt to select no attribute (\((X_{n+1})=\)) in the absence of significant biases.

Next, AFCP constructs a prediction set \((X_{n+1})\) for \(Y_{n+1}\) that guarantees the following notion of _adaptive equalized coverage_ at the desired level \((0,1)\):

\[[Y_{n+1}(X_{n+1})\ |\ (X_{n+1},(X_{n+1}))] 1 -. \]

In words, this tells us \((X_{n+1})\) is well-calibrated for the groups defined by the selected attribute \((X_{n+1})\). It is worth highlighting the key distinctions between (3) and the existing notions of coverage reviewed above. On the one hand, if AFCP identifies no significant bias, selecting \((X_{n+1})=\), then (3) reduces to marginal coverage (1), following the convention that \((X_{n+1},)\) is a constant. On the other hand, exhaustive equalized coverage (2) would correspond to simultaneously selecting all possible sensitive attributes instead of only that identified by \((X_{n+1})\). To clarify the terminology, in this paper we will say that an attribute is _sensitive_ if it may identify a group affected by algorithmic bias. By contrast, a _protected_ attribute is one for which equalized coverage is explicitly sought.

Figure 2 illustrates this intuition through a simulated example. In this scenario, we generate synthetic medical diagnosis data, considering six possible diagnosis labels, and designate race, sex, and age group as potentially sensitive attributes alongside other demographic factors. Notably, the female group, identified by sex, is characterized by fewer samples and higher algorithmic bias, resulting in marginal prediction sets with low group-conditional coverage. By contrast, the model leads to no significant disparities across races and age groups in this dataset.

For two example patients from the critical group, the standard marginal prediction sets fail to cover the true label. Conversely, sets calibrated for exhaustive equalized coverage are too conservative to be informative. By contrast, AFCP generates prediction sets that are both efficient and fair.

Without additional sample splitting, which would be inefficient, constructing informative prediction sets that satisfy (3) is challenging due to potential selection bias from using the same data for attribute selection and conformal calibration. This paper presents a novel solution to address this challenge.

Figure 1: Schematic visualization of the automatic sensitive attribute selection carried out by our Adaptively Fair Conformal Prediction (AFCP) method. This method is designed to find the attribute corresponding to the group most negatively affected by algorithmic bias, on a case-by-case basis.

### Table of Contents

Section 2 presents our AFCP method, focusing on the special case in which at most one sensitive attribute may be selected. Section 3 demonstrates the empirical performance of AFCP on synthetic and real data. Section 4 discusses some limitations and suggests ideas for future work.

Additional content is presented in the Appendices. Appendix A1 reviews relevant details of existing approaches. Appendices A2 and A3 present two extensions of our method, respectively enabling the selection of more than one sensitive attribute and providing valid coverage also conditional on the true test label; both extensions involve distinct technical challenges. Additionally, a variation of AFCP designed for outlier detection tasks is detailed in Appendix A4. Appendix A5 contains all mathematical proofs. Appendix A6 explains how to implement our method efficiently and studies its computational cost. Appendix A7 describes the results of numerous additional experiments.

### Related Works

Conformal inference is a very active research area, with numerous methods addressing diverse tasks, including outlier detection , classification , and regression . Overcoming the limitations of the standard marginal coverage guarantees (1) is a main interest in this field.

Some works have proposed _conformity scores_ designed to seek high feature-conditional coverage while calibrating prediction sets for marginal coverage . Others attempt to mitigate overconfidence while training the ML model , and several have developed calibration methods for non-exchangeable data, accounting for possible distribution shifts . These works are complementary to ours, as we focus on guaranteeing a new adaptive notion of equalized coverage.

In addition to , several other works have considered constructing prediction sets adhering to various notions of equalized coverage and have empirically investigated the performance of conformal predictors in this regard . In the context of regression,  and  proposed strategies to enhance conditional coverage given several protected attributes, but they targeted a different notion of equalized coverage designed for continuous outcomes. In classification, a classical approach to move beyond marginal coverage is label-conditional coverage, where the "protected" groups are defined not based on the features \(X_{n+1}\) but by the label itself, \(Y_{n+1}\). As explained in Appendix A3, the method proposed in this paper can also be extended to provide label-conditional coverage.

More closely related to the notion of equalized coverage  are the works of , which differ from ours as they do not consider the automatic selection of the sensitive groups. To tackle a related challenge due to unknown biased attributes,  studied how to identify unfairly treated groups by establishing a simultaneously valid confidence bound on group-wise disparities. In principle, their approach can be integrated within the selection component of our method. Very recently,  proposed an elegant method to obtain valid conformal prediction sets for adaptively selected subsets of test cases. While their perspective aligns more closely with ours, their approach and focus differ as they study different selection rules not specifically aimed at mitigating algorithmic bias.

Figure 2: Prediction sets constructed with different methods for patients in groups negatively affected by algorithm bias. Our method (AFCP) is designed to provide informative prediction sets that are well-calibrated conditional on the automatically identified critical sensitive attribute.

## 2 Method

### Automatic Attribute Selection

Given a pre-trained classifier, an independent calibration data set \(\), and a test point \(Z_{n+1}=(X_{n+1},Y_{n+1})\) with an unknown label \(Y_{n+1}\), we will select (at most) one sensitive attribute, \((X_{n+1})\{,\{1\},,\{K\}\}\), according to the following _leave-one-out_ procedure.

For each \(y[L]\), imagine \(Y_{n+1}\) is equal to \(y\), and define an _augmented_ calibration set \(^{}_{y}:=\{(X_{n+1},y)\}\). For each \(i[n+1]\), define also the leave-one-out set \(^{}_{y,i}:=^{}_{y}\{(X_{i},Y_{i})\}\), with \(y\) acting as a placeholder for \(Y_{n+1}\). Then, for each \(i[n+1]\), we construct a conformal prediction set \(^{}_{y}(X_{i})\) for \(Y_{i}\) given \(X_{i}\) by calibrating the classifier using the data in \(^{}_{y,i}\). Any method can be applied for this purpose, although it may be helpful for concreteness to focus on employing the standard approach seeking marginal coverage (1) using the adaptive conformity scores proposed by . Let \(E_{y,i}\) denote the binary indicator of whether \(^{}_{y}(X_{i})\) fails to cover \(Y_{i}\):

\[E_{y,i}:=\{Y_{i}^{}_{y}(X_{i})\}. \]

After evaluating \(E_{y,i}\) for all \(i[n+1]\), we will assess the leave-one-out miscoverage rate for the worst-off group identified by each sensitive attribute \(k[K]\). That is, we evaluate

\[_{y,k}:=_{m[M_{k}]}^{n+1}E_{y,i}\{ (X_{i},\{k\})=m\}}{_{i=1}^{n+1}\{(X_{i},\{k\})=m\}}.\]

Intuitively, \(_{y,k}\) denotes the maximum miscoverage rate across all groups identified by the \(k\)-th attribute. Large values of \(_{y,k}\) suggest that the \(k\)-th attribute may be a sensitive attribute corresponding to at least one group suffering from algorithmic bias.

To assess whether there is evidence of significant algorithmic bias, we can perform a statistical test for the null hypothesis that no algorithmic bias exists. Note that this test can be heuristic since it does not need to be exact for our method to rigorously guarantee (3). Therefore, we do not need to carefully consider the assumptions underlying this test. As a useful heuristic, we define:

\[_{y}:=_{k[K]}_{y,k}, \]

and carry out a one-sided t-test for the null hypothesis \(H_{0}:_{y}\) against \(H_{1}:_{y}>\).

If \(H_{0}\) is rejected (at any desired level, like 5%), we conclude there exists a group suffering from significant algorithmic bias, and we identify the corresponding attribute through

\[(X_{n+1},y)=\{*{arg\,max}_{k[K]}_{y,k}\}. \]

Otherwise, we set \((X_{n+1},y)=\), which corresponds to selecting no attribute. See Algorithm 1 for an outline of this procedure, as a function of the placeholder label \(y\).

After repeating this procedure for each \(y[L]\), the final selected attribute \((X_{n+1})\) is:

\[(X_{n+1})=_{y[L]}(X_{n+1},y). \]

Therefore, an attribute is selected if and only if it is consistently flagged by our leave-one-out procedure for all values of the placeholder label \(y[L]\). This approach minimizes the potential arbitrariness due to the use of a placeholder label and is necessary to guarantee that our method constructs prediction sets achieving (3), as discussed in the next section.

Before explaining how our method utilizes the selected sensitive attribute obtained in (7) to construct prediction sets satisfying (3), we pause to make two remarks. First, as long as \(n\) is large enough, \((X_{n+1},y)\) is quite stable with respect to both \(X_{n+1}\) and \(y\), as each of these variables plays a relatively small role in determining the leave-one-out miscoverage rates. Therefore, the selected attribute \((X_{n+1})\) given by (7) is also quite stable for different values of \(X_{n+1}\). This stability will be demonstrated empirically in Section 3. Second, despite its iterative nature, our method can be implemented efficiently; see Appendix A6. Further, if \(n\) is very large, our method could be streamlined using cross-validation instead of a leave-one-out approach.

```
1:Input: calibration data \(\); test point with features \(X_{n+1}\); list of \(K\) sensitive attributes;
2: pre-trained classifier \(\); fixed rule for computing nonconformity scores; level \((0,1)\);
3: placeholder label \(y[L]\).
4: Assume \(Y_{n+1}=y\) and define the augmented data set \(^{}_{y}:=\{(X_{n+1},y)\}\).
5:for\(i[n+1]\)do
6: Pretend that \((X_{i},Y_{i})\) is the test point and \(^{}_{y}\{(X_{i},Y_{i})\}\) is the calibration set.
7: Construct a conformal prediction set \(^{}_{y}(X_{i})\) for \(Y_{i}\).
8: Evaluate the miscoverage indicator \(E_{y,i}\) using (4).
9:endfor
10: Perform a one-sided test for \(H_{0}:_{y}\) vs. \(H_{1}:_{y}>\), with \(_{y}\) defined as in (5).
11: Select the attribute \((X_{n+1},y)\) using (6) if \(H_{0}\) is rejected, else set \((X_{n+1},y)=\).
12:Output: \((X_{n+1},y)\), either a selected sensitive attribute or an empty set.
```

**Algorithm 1** Automatic attribute selection using a placeholder test label.

### Constructing the Adaptive Prediction Sets

After evaluating \((X_{n+1},y)\) by applying Algorithm 1 with placeholder label \(y\) for \(Y_{n+1}\) for all \(y[L]\), and selecting either an empty set or a single attribute \((X_{n+1})\) using (7), AFCP constructs an adaptive prediction set for \(Y_{n+1}\) that satisfies (3) as follows.

First, it constructs a _marginal_ conformal prediction set \(^{}(X_{n+1})\) targeting (1), by applying the standard approach reviewed in Appendix A1. Then, for each \(y[L]\), it constructs a conformal prediction set \((X_{n+1},(X_{n+1},y))\) with equalized coverage for the group identified by attribute \((X_{n+1},y)\), as if it had been fixed. This is achieved by applying the standard marginal method based on a restricted calibration sample indexed by \(\{i[n]:(X_{i},(X_{n+1},y))=(X_{n+1},(X_{n+1},y))\}\); see Algorithm A1 in Appendix A1 for further details. Therefore, note that \((X_{i},(X_{n+1},y))\) becomes equivalent to \(^{}(X_{n+1})\) if \((X_{n+1},y)=\). Finally, the AFCP prediction set for \(Y_{n+1}\) is given by:

\[(X_{n+1})=^{}(X_{n+1})\{_{y=1}^{L}(X _{n+1},(X_{n+1},y))\}. \]

See Algorithm 2 for an outline of this procedure.

Note that the AFCP set \((X_{n+1})\) given by (8) always contains the marginal set \(^{}(X_{n+1})\); this is essential to prove the validity of our approach. Second, in practice the selection \((X_{n+1},y)\) tends to be very consistent for different values of the placeholder label \(y\), as long as the sample size \(n\) is large enough; therefore, the union in (8) will typically not lead to a very large prediction set.

```
1:Input: calibration data \(\); test point with features \(X_{n+1}\); list of \(K\) sensitive attributes;
2: pre-trained classifier \(\); fixed rule for computing nonconformity scores; level \((0,1)\).
3:for\(y[L]\)do
4: Select an attribute \((X_{n+1},y)\) by applying Algorithm 1 with placeholder label \(y\).
5: Construct \((X_{n+1},A)\) by applying Algorithm A1 with the attribute \(A=(X_{n+1},y)\).
6:endfor
7: Construct \(^{}(X_{n+1})\) by applying Algorithm A1 without protected attributes.
8:Output: selected attribute \((X_{n+1})\) given by (7) and prediction set \((X_{n+1})\) given by (8).
```

**Algorithm 2** Adaptively Fair Conformal Prediction (AFCP).

The following result, proved in Appendix A5, establishes that the prediction sets \((X_{n+1})\) output by AFCP guarantee adaptive equalized coverage (3) with respect to the adaptively selected attribute \((X_{n+1})\). It is worth emphasizing this result is not straightforward and involves an innovative proof technique to address the lack of exchangeability introduced by the adaptive selection step.

**Theorem 1**.: _If \(\{(X_{i},Y_{i})\}_{i=1}^{n+1}\) are exchangeable, the prediction set \((X_{n+1})\) and the selected attribute \((X_{n+1})\) output by Algorithm 2 satisfy the adaptive equalized coverage defined in (3)._Numerical Experiments

### Setup and Benchmarks

This section demonstrates the empirical performance of AFCP, focusing on the implementation described in Section 2, which selects at most one sensitive attribute. Our method is compared with three existing approaches, which utilize the same data, ML model, and conformity scores but produce prediction sets with different guarantees. The first is the _marginal_ benchmark, which constructs prediction sets guaranteeing (1) by applying Algorithm A1 without protected attributes. The second is the _exhaustive_ equalized benchmark, which constructs prediction sets guaranteeing (2) by applying Algorithm A1 with all \(K\) sensitive attributes simultaneously protected. The third is a _partial_ equalized benchmark that separately applies Algorithm A1 with each possible protected attribute \(k[K]\), and then takes the union of all such prediction sets. This is an intuitive approach that can be easily verified to provide a coverage guarantee intermediate between (2) and (3), namely:

\[[Y_{n+1}(X_{n+1})(X_{n+1},\{k\})] 1-,  k[K]. \]

However, we will see that these prediction sets are often still too conservative in practice.

In addition, we apply a variation of AFCP that always selects one sensitive attribute, regardless of the outcome of the significance test. This method is denoted as AFCP1 in our experiments.

For all methods considered, the classifier is based on a five-layer neural network with linear layers interconnected via a ReLU activation function. The output layer uses a softmax function to estimate the conditional label probabilities. The Adam optimizer and cross-entropy loss function are used in the training process, with a learning rate set at 0.0001. The loss values demonstrate convergence after 100 epochs of training. For all methods, the miscoverage target level is set at \(=0.1\).

### Synthetic Data

We generate synthetic classification data to mimic a medical diagnosis task with six possible labels: Skin cancer, Diabetes, Asthma, Stroke, Flu, and Epilepsy. The available features include three sensitive attributes--Age Group, Region, and Color--and six additional non-sensitive covariates. Color is categorized as Blue or Grey, with 10% and 90% marginal frequencies, respectively. The Age Group is cyclically repeated as \(<18,18-24,25-40,41-65,>65\), and Region is sampled from an i.i.d. multinomial distribution across {West, East, North, South} with equal probabilities. The six non-sensitive features are i.i.d. random samples from a uniform distribution on \(\). For simplicity, Color is denoted as \(X_{0}\) and the first non-sensitive feature as \(X_{1}\). Conditional on \(X\), the label \(Y\) is generated based on a decision tree model that depends only on \(X_{0}\) and \(X_{1}\), as detailed in Appendix A7. This model is designed so that the diagnosis label for individuals with Color equal to Blue is intrinsically harder to predict, mimicking the presence of algorithmic bias.

Figure 3 shows the performance of all methods as a function of the total sample size, ranging from 200 to 2000. In each case, \(50\%\) of the samples are used for training and the remaining \(50\%\) for calibration. Results are averaged over 500 test points and 100 independent experiments.

While the marginal benchmark produces the smallest prediction sets on average, it leads to significant empirical undercoverage within the Blue group. In contrast, the exhaustive benchmark, which achieves the highest coverage overall, tends to lead to overly conservative and thus uninformative prediction sets, especially for the Blue group. The partial benchmark, though less conservative than the exhaustive method, still generates prediction sets that are too large when the sample size is small.

Our AFCP method and its simpler variation, AFCP1, not only achieve valid coverage for the Blue group but do so with prediction sets that, on average, are not much larger than the marginal ones. AFCP1 is slightly more robust than AFCP when the sample size is very small, as it never fails to select a sensitive attribute. This is advantageous in scenarios where we know there is a sensitive attribute worth equalizing coverage for, though this may not always be the case in practice. See Figure A1 and Table A1 for detailed results with standard errors.

Figure 4 provides additional insight into our method's performance by plotting the selection frequencies of each sensitive attribute as a function of sample size, within the same experiments described in Figure 3. These results show that our method behaves as anticipated. When the sample size and algorithmic bias are both small, AFCP shows more variability in selecting the sensitive attribute,

[MISSING_PAGE_EMPTY:8]

[MISSING_PAGE_FAIL:9]

### Comparison between AFCP and AFCP1

Both AFCP and AFCP1 outperform the benchmark approaches when applied to datasets with small sample sizes, each excelling in different scenarios. AFCP is better suited to situations where there is uncertainty regarding the presence of significant algorithmic bias, while AFCP1 is more effective when prior knowledge suggests that at least one attribute may be biased. For example, in Figure 3, which illustrates a case where one group (Color-Blue) is consistently biased, AFCP1 achieves slightly higher conditional coverage than AFCP. While AFCP exhibits slight undercoverage for the blue group with small sample sizes, it still outperforms the Marginal approach. The occasional inability of AFCP to select a sensitive attribute in small samples reflects the inherent challenges posed by limited datasets. When the method does not select an attribute, it often signifies a lack of sufficient evidence of algorithmic bias, making it reasonable to calibrate the prediction sets solely for marginal coverage.

## 4 Discussion

This paper presents a practical and statistically principled method to construct informative conformal prediction sets with valid coverage conditional on adaptively selected features. This approach balances efficiency and equalized coverage, which may be particularly useful in applications involving multiple sensitive attributes. While we believe it offers substantial benefits, a potential limitation of this method is that it does not always identify the most relevant sensitive attribute, particularly when working with limited sample sizes. Nevertheless, our empirical results are quite encouraging, demonstrating that AFCP effectively mitigates significant instances of algorithmic bias when the sample size is adequate. Moreover, our method is flexible, allowing for the integration of prior knowledge about which sensitive attributes might require protection against algorithmic bias.

This paper creates several opportunities for further work. Future research could focus on theoretically studying the conditions under which our method can be guaranteed to select the correct sensitive attribute with high probability. Additionally, future extensions might explore implementing different attribute selection procedures, such as those inspired by  - within our flexible AFCP framework to delve into the subtle trade-offs associated with different selection algorithms. Moreover, adapting our approach to accommodate different fairness criteria by adaptively adjusting the coverage rate target for each subgroup is another promising area of study. Extending our method to more efficiently handle scenarios with an extremely high number of possible classes is also worthwhile, potentially drawing inspiration from . Furthermore, investigating extensions for classification tasks where the target variable is ordered could be both intriguing and practically useful. In such cases, a naive modification of our method would involve utilizing the discrete convex hull of all components instead of unions of subintervals on the right-hand side of Equation (8). However, developing a more refined approach would be a valuable contribution for future work. Future extensions of our work could focus on adapting to distributional shifts or enhancing the robustness and efficiency of our method under adversarial attacks or contaminated data, potentially drawing connections with . Finally, extending our method to accommodate regression tasks with continuous outcomes presents additional computational challenges, but potential solutions could be inspired by .

The numerical experiments described in this paper were carried out on a computing cluster. Individual experiments, involving 1000 calibration samples and 500 test samples, required less than 25 minutes and 5GB of memory on a single CPU. The entire project took approximately 100 hours of computing time, and did not involve preliminary or failed experiments.

Software implementing the algorithms and data experiments are available online at [https://github.com/FionaZ3696/Adaptively-Fair-Conformal-Prediction](https://github.com/FionaZ3696/Adaptively-Fair-Conformal-Prediction).