# (Implicit) Ensembles of Ensembles: Epistemic Uncertainty Collapse in Large Models

Andreas Kirsch blackhc@gmail.com

###### Abstract

We uncover a paradoxical phenomenon in deep learning models: as model complexity increases, epistemic uncertainty often collapses, challenging the assumption that larger models invariably offer better uncertainty quantification. We propose that this collapse stems from _implicit ensembling_ within large models. To support this hypothesis, we offer two lines of evidence: first, we demonstrate the epistemic uncertainty collapse empirically across various architectures, from explicit _ensembles of ensembles_ and simple MLPs to state-of-the-art vision models; second, we introduce _implicit ensemble extraction_, a technique that decomposes larger models into diverse sub-models, recovering hidden ensemble structure and epistemic uncertainty. We provide theoretical justification for these phenomena and explore their implications for uncertainty estimation.

## 1 Introduction

Bayesian deep learning provides us with a principled framework for quantifying uncertainty in complex machine learning models (MacKay, 1992; Neal, 1994). A key concept in this framework is _epistemic uncertainty_, which represents a model's uncertainty about its predictions due to limited knowledge or data (Smith & Gal, 2018; Der Kiureghian & Ditlevsen, 2009). This form of uncertainty is distinct from _aleatoric uncertainty_, which captures inherent noise or randomness in the data (Kendall & Gal, 2017). A wide range of applications relies on accurate epistemic uncertainty estimation. These include active learning, where uncertainty guides data acquisition; anomaly detection, where uncertainty can signal out-of-distribution inputs; and safety-critical systems, where understanding model confidence is crucial for responsible deployment.

Intuitively, one might expect that as deep learning models grow in size and complexity, their capacity for epistemic uncertainty would increase. As Fellaji & Pennerath (2024) argue, "the more parameters a model has, the more likely it is to fit the data in multiple ways. Put another way, the posterior and thus the posterior predictive will tend to be flatter, making the epistemic uncertainty grow," which is aligned with the conventional understanding of model complexity and uncertainty.

However, our work provides evidence for a simple yet paradoxical phenomenon: when constructing higher-order ensembles, _ensembles of ensembles_, we observe an _epistemic uncertainty collapse_. We initially observed and documented this behavior in Spring 20211 and have since confirmed it in additional independent experiments that we share here. This collapse occurs because individual ensembles, given sufficient size and training, converge to similar predictive distributions, causing inter-ensemble disagreement to vanish as the ensemble size grows.

We hypothesize that, similar to ensembles of ensembles, _implicit ensembling_ might occur within the layers of large over-parameterized neural networks, potentially leading to significant underestimation of epistemic uncertainty for traditional uncertainty estimators that rely on final logits. Hence, similarto deep ensembles that have been found to offer better calibration (Ovadia et al., 2019), implicit ensembling may explain why larger models also appear more calibrated (Tran et al., 2022). Recent work by Fellaji and Pennerath (2024) provides additional evidence of this phenomenon occurring even in simple over-parameterized MLPs trained on standard benchmark datasets but fails to provide an explanation. Our theoretical contributions together with experiments that show both implicit ensembling as well as initial results on how to recover epistemic uncertainty from a single large model by extracting implicit ensembles from it provide a possible explanation for this phenomenon.

## 2 Background

**Bayesian Model Average.** The _Bayesian Model Average (BMA)_ provides a principled framework for combining predictions from multiple models. Let \(()\) be the posterior distribution over model parameters \(\), given observed data \(\). The BMA computes the predictive distribution for a new input \(^{*}\) by integrating over all possible parameter values:

\[(y^{*}^{*},)=(y^ {*}^{*},)()d. \]

This averaging naturally accounts for model uncertainty by weighting predictions according to their posterior probabilities.

**Information-Theoretic Quantities.** The quantification of uncertainty is crucial for robust and reliable predictions. To formally quantify and differentiate between aleatoric and epistemic uncertainty, we can use an information-theoretic decomposition (Houlsby et al., 2011; Gal et al., 2017; Smith and Gal, 2018). Let \(Y\) be the predicted output, and \(\) be the model parameters. We define:

1. **Total Uncertainty** as the _entropy_ of the predictive distribution of the BMA: \[[Y,]=-(Y ,)(Y,)dY.\] (2)
2. **Epistemic Uncertainty** (\([Y;,]\)) as the _mutual information_ which estimates the expected reduction in uncertainty about the prediction \(Y\) that would be obtained if we knew the model parameters \(\): \[[Y;,]=[Y ,]-}_{( )}[[Y,]].\] (3)

## 3 Epistemic Uncertainty Collapse for Ensembles of Ensembles

Consider a scenario where we construct a higher-order ensemble by creating multiple deep ensembles, each comprised of \(M\) models. Let \(_{1:}\{_{1},_{2},..., _{}\}\) be a set of \(\) deep ensembles, each containing \(\) models: \(_{k}=\{_{1}^{k},_{2}^{k},...,_{M}^{k}\}\), where \(_{m}^{k}\) represents the parameters of the \(m\)-th model in the \(k\)-th ensemble. For a given input \(\), the predictive distribution of the \(k\)-th ensemble is:

\[(y,_{k})=_{m=1}^{M} (y,_{m}^{k}), \]

Figure 1: **Epistemic Uncertainty Collapse in a Toy Regression Problem. As the sub-ensemble size increases, epistemic uncertainty vanishes. Ensembles of 10 sub-ensembles with different sub-ensemble sizes. _Left:_ True function, data, and ensemble predictions. _Middle:_ Epistemic uncertainty across input space. _Right:_ Mean epistemic uncertainty vs. sub-ensemble size.**

where we have dropped conditioning on the data \(\) for brevity. Consequently, the predictive distribution of the ensemble of ensembles is the BMA over all individual members:

\[(y,_{1:})=} _{k=1}^{}(y,_{k})=}}_{k=1}^{}_{m=1}^{}(y,_{m}^{k}). \]

By defining \(\) to depend on \(K\) and \(M\) as categorical random variables with uniform distribution, we can rephrase the epistemic uncertainty as: \([Y;(K,M)]=[Y;_{M}^{K}]\).

**Infinite Sub-Ensemble Size.** How does the epistemic uncertainty change with increasing size of the sub-ensembles? For this, we note that if we let \(\),

\[}_{m=1}^{}(y, _{m}^{k})_{()}[(y,)]=(y) \]

independent of \(k\). Hence, thanks to the central limit theorem, we have:

\[[Y;_{K},]=[ (Y)]-_{(k)}[[(Y ,k)]][(Y)]-[ (Y)]=0. \]

**Infinite Uncertainty Collapse**

As the size of the sub-ensemble in an ensemble of ensembles increases, the epistemic uncertainty of the overall ensemble approaches zero, and we observe an _epistemic uncertainty collapse_. This collapse occurs because the individual ensembles converge to similar predictive distributions.

## 4 Empirical Results

In the following section, we present a series of experiments that not only demonstrate the epistemic uncertainty collapse in explicit ensembles of ensembles but also find parallels in the behavior of wide

Figure 2: **Epistemic Uncertainty Collapse on MNIST via Implicit Ensembling.**_(a) Mutual Information Empirical Cumulative Distribution Function (ECDF) for Different MLP Widths._ As MLP size increases, mutual information decreases while accuracy remains stable. This trend persists across training and other distributions. _(b) MNIST vs. Fashion-MNIST OoD Detection AUROC Curves._ The mean difference in uncertainty scores between in-distribution and out-of-distribution samples (in parentheses) also decreases with width, further evidencing epistemic uncertainty collapse, while the AUROC for OoD detection slightly improves across both uncertainty metrics.

neural networks, providing evidence for the hypothesized effects of implicit ensembling. Details on the models, training setup, datasets, and evaluation are provided in SSD and in E in the appendix.

**Toy Example.** To illustrate the epistemic uncertainty collapse in ensembles of ensembles, we present a one-dimensional regression task with the ground-truth function \(f(x)=(x)+\), where \((0,0.1)\). Figure 1 presents the results across three panels, which show narrowing uncertainty bands, decreasing epistemic uncertainty across the input space for larger sub-ensembles between training points and the inverse relationship between sub-ensemble size and mean epistemic uncertainty.

**Explicit Ensemble of Ensemble.** In Figure 4, we construct a deep ensemble comprising of 24 Wide-ResNet-28-1 models (Zagoruyko & Komodakis, 2016; He et al., 2015) trained on CIFAR-10 (Krizhevsky et al., 2009), which we then partition into ensembles of ensembles with varying sub-ensemble sizes (\(24 1,12 2,8 3,6 4,4 6,3 8,2 12\)). In Figure 4, we observe a clear epistemic uncertainty collapse, manifested by the mutual information concentrating on smaller values, and the AUROC shows a deterioration as the sub-ensemble size increases, directly resulting from the epistemic uncertainty collapse. While the decrease in AUROC may appear modest, it is large enough

Figure 3: **Classification with Rejection for Implicit Ensemble Extractions from Pre-Trained Models. Each subfigure shows accuracy, negative log-likelihood, and calibration error as a function of epistemic uncertainty quantiles for different ensemble sizes. Solid lines represent extracted ensembles of increasing size (from 2 to 7/16), while the dashed black line represents the original single model. _(a)_ The mutual information between predictions is used as the epistemic uncertainty measure for ensembles, while entropy is used for the single model. As the ensemble size increases, we observe improved performance for the area under curve (AUC), which indicates better epistemic uncertainty calibration (with the notable exception of the calibration error). _(b)_ Temperature scaling improves epistemic uncertainty calibration in general but benefits the original model most. Accuracy and NLL for extracted epistemic uncertainty only benefit in the low-uncertainty regime. _(c)_ For VIT models, we find that a mutual information weighted by the logit sum of each ensemble performs better than the mutual information (_(c)_ vs _(d)_ with mutual information).**

to make the difference between state-of-the-art performance and baseline methods, e.g., compare to the results in Mukhoti et al. (2023).

**Implicit Ensembling on MNIST.** Surprisingly, the effect of the epistemic uncertainty collapse is even visible when training relatively small MLP models of varying width on MNIST in a controlled setting. We reproduce the results from Fellaji and Pennerath (2024) in Figure 2. As the width of the MLP increases, we observe a clear trend of decreasing mutual information across all datasets: MNIST (LeCun and Cortes, 1998), Dirty-MNIST (Mukhoti et al., 2023), and Fashion-MNIST (Xiao et al., 2017). The decrease in mutual information indicates a reduction in the model's epistemic uncertainty as it grows larger, despite maintaining similar accuracy. The mean difference in uncertainty scores between in-distribution (MNIST) and out-of-distribution (Fashion-MNIST) samples decreases with increasing model width. However, the AUROC for OoD detection using different uncertainty metrics slightly improves as the model width increases, in line with the results in Fellaji and Pennerath (2024), who report a deterioration of OoD performance on CIFAR-10 but not on MNIST.

**Implicit Ensemble Extraction.** To substantiate that implicit ensembling might be driving epistemic uncertainty collapse, we mitigate this collapse by decomposing larger models into constituent sub-models.

To extract implicit ensembles, we train boolean masks on the weights to recover individual models with maximally different masks while maintaining low individual loss. For vision models, we leverage the common use of average pooling to obtain per-tile class logits, which we average with different target sizes to create differently-sized ensembles. Full details are provided in SSD.

First, we extract implicit ensembles from the MLPs trained on MNIST above. In Figure 5, we see the effectiveness of this implicit ensemble extraction technique. The results demonstrate that this decomposition can recover much of the epistemic uncertainty of an ensemble from a single model, providing support for our hypothesis about the mechanism underlying epistemic uncertainty collapse.

Second, we explore implicit ensemble extraction from pre-trained vision models based on ResNet (He et al., 2015) and Vision Transformer (Dosovitskiy et al., 2021) model architectures on ImageNet-v2 (Recht et al., 2019). Leveraging the common use of average pooling in these models to aggregate spatial information, we extract implicit ensembles without optimizing masks. Concretely, we remove the global average pooling layer. This allows us to obtain per-tile class logits, which we average with different target sizes to create differently-sized ensembles. Figure 3 shows three key performance metrics--accuracy, negative log-likelihood (NLL), and calibration error--plotted against epistemic uncertainty quantiles for different ensemble sizes for the original models and temperature-scaled versions (Guo et al., 2017). Overall, the results are mixed but promising.

**Limitations.** These _exploratory_ results provide initial evidence of epistemic uncertainty collapse and implicit ensembling. We can even sometimes enhance a model's uncertainty quantification, without the need for retraining or additional data. At the same time, the mutual information is not always the best uncertainty metric, the comparative performance changes depending on the model temperature, and the pool size of the best implicit ensemble is not always the same for different metrics and model architectures.

## 5 Conclusion

This study has uncovered and analyzed the collapse of epistemic uncertainty in large neural networks and hierarchical ensembles. Our findings challenge the assumption that more complex models invariably offer better uncertainty quantification out of the box. Our theoretical framework and empirical results demonstrate this phenomenon across various architectures and datasets, from explicitly constructed ensemble of ensembles to implicit ensembling in simple MLPs and state-of-the-art vision models, and we have explored implicit ensemble extraction to recover hidden ensemble structures and improve epistemic uncertainty estimates.

#### Acknowledgments

Thanks to Jannik Kossen for helpful and inspiring discussions and feedback in 2021 and throughout, and to Freddie Bickford Smith for helpful feedback on the manuscript.