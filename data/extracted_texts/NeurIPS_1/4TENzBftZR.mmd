# iVideoGPT: Interactive VideoGPTs are Scalable World Models

Jialong Wu\({}^{1}\), Shaofeng Yin\({}^{1,2}\), Ningya Feng\({}^{1}\), Xu He\({}^{3}\), Dong Li\({}^{3}\), Jianye Hao\({}^{3,4}\),

**Mingsheng Long\({}^{1}\)\({}^{}\) \({}^{1}\)School of Software, BNRist, Tsinghua University, \({}^{2}\)Zhili College, Tsinghua University \({}^{3}\)Huawei Noah's Ark Lab, \({}^{4}\)College of Intelligence and Computing, Tianjin University wujialong0229@gmail.com, ysf22@mails.tsinghua.edu.cn, mingsheng@tsinghua.edu.cn**

Equal Contribution

###### Abstract

World models empower model-based agents to interactively explore, reason, and plan within imagined environments for real-world decision-making. However, the high demand for interactivity poses challenges in harnessing recent advancements in video generative models for developing world models at scale. This work introduces Interactive VideoGPT (iVideoGPT), a scalable autoregressive transformer framework that integrates multimodal signals--visual observations, actions, and rewards--into a sequence of tokens, facilitating an interactive experience of agents via next-token prediction. iVideoGPT features a novel compressive tokenization technique that efficiently discretizes high-dimensional visual observations. Leveraging its scalable architecture, we are able to pre-train iVideoGPT on millions of human and robotic manipulation trajectories, establishing a versatile foundation that is adaptable to serve as interactive world models for a wide range of downstream tasks. These include action-conditioned video prediction, visual planning, and model-based reinforcement learning, where iVideoGPT achieves competitive performance compared with state-of-the-art methods. Our work advances the development of interactive general world models, bridging the gap between generative video models and practical model-based reinforcement learning applications. Code and pre-trained models are available at [https://thuml.github.io/iVideoGPT](https://thuml.github.io/iVideoGPT).

## 1 Introduction

Recent years have witnessed remarkable advancements in generative models of multimodal contents, including text , audio , and images , with video generation now emerging as a new frontier . A particularly significant application of these generative video models, learned in an unsupervised way on diverse Internet-scale data, is to construct predictive world models  at scale. These world models are expected to accumulate commonsense knowledge about how the world works, enabling the prediction of potential future outcomes (e.g., visual observations and reward signals) based on the actions of agents. By leveraging these world models, agents employing model-based reinforcement learning (RL) can imagine, reason, and plan inside world models , thus acquiring new skills more safely and efficiently with a handful of trials in the real world.

Despite the fundamental connection, significant gaps remain between generative models for video generation and visual world models for agent learning. One primary challenge is achieving the best of both interactivity and scalability. In model-based RL, world models predominantly utilize recurrent network architecture. This design naturally allows the transition of observations or latent states conditioned on actions in each step, facilitating interactive behavior learning . However, these recurrent models mostly focus on games or simulated environments with simplevisuals and have limited capability to model complex, in-the-wild data at scale [48; 81]. On the other hand, Internet-scale video generative models [37; 7; 11] can synthesize realistic long videos that are controllable via text descriptions  or future action sequences  at the beginning of generation. Although suitable for high-level planning , their trajectory-level interactivity does not provide sufficient granularity needed by agents to intervene step-by-step during the simulation to learn precise basic skills efficiently. This dilemma naturally raises the question:

_How can we leverage the advancements in scalable video generative models for developing interactive visual world models?_

In this work, we explore world models that are both interactive and scalable within a GPT-like autoregressive transformer framework [90; 75]. Pioneering efforts have been made recently through diffusion models  and masked generative models . Nevertheless, utilizing autoregressive transformers offers distinct advantages such as seamless integration with the established Large Language Model (LLM) ecosystem  and greater flexibility in handling diverse conditions without the need for specific architectural modifications like adapter modules [77; 107]. We present _Interactive VideoGPT (iVideoGPT)_, a scalable world model architecture that incorporates multimodal signals, including visual observations, actions, and rewards, in an interactively autoregressive manner. Unlike multimodal LLMs that discretize visual observations into tokens frame-by-frame using image tokenizers , a key innovation of iVideoGPT for enhancing scalability is to learn compressive tokenization for each observation conditioned on rich contextual observations, achieving an asymptotic 16\(\) reduction in token sequence length. We highlight that more compact video tokenization could not only facilitate more efficient training and generation but also enhance video quality. This is achieved by decoupling context from dynamics, allowing the model to focus on predicting the motion of objects while maintaining temporal consistency within the scene .

We demonstrate a series of practical applications of iVideoGPT for visual robotic manipulation, as illustrated in Figure 1. Mirroring the two-phase approach popularized by LLMs, our method involves pre-training followed by domain-specific adaptation. During pre-training, iVideoGPT is scalable for action-free video prediction across a mixture of over one million robotic and human manipulation trajectories [70; 25]. The pre-trained iVideoGPT serves as a single, adaptable foundation of interactive world models for various downstream tasks, such as action-conditioned video prediction [21; 16], visual planning , and visual model-based RL . Additionally, we showcase the pre-trained transformer's preliminary zero-shot video generation capability without fine-tuning, requiring only tokenizer adaptation for unseen domains. We further explore a variant of iVideoGPT for goal-conditioned video prediction, underscoring the flexibility of sequence modeling.

The main contributions of this work can be summarized as follows:

* We introduce Interactive VideoGPT (iVideoGPT), an autoregressive transformer architecture for scalable world models, which features compressive tokenization for visual observations.
* We pre-train iVideoGPT on a large-scale dataset comprising millions of robotic and human manipulation trajectories and adapt it to domain-specific tasks. The pre-trained models have been publicly available to encourage further research.
* Extensive experiments covering video prediction, visual planning, and visual model-based RL demonstrate that iVideoGPT can simulate accurate and realistic experiences and provide competitive performance compared with state-of-the-art methods.

Figure 1: Practical applications of iVideoGPT, which is designed to scale, allowing pre-training on millions of human and robotic manipulation trajectories. This results in a single, versatile foundation of interactive world models, adaptable to a wide range of downstream tasks.

## 2 Problem Formulation

A world model is an internal model learned by the agent to simulate the environment. This environment is typically modeled as a _partially observable Markov decision process (POMDP)_\((,,,,p,r,)\). At each step, \(s_{t}\) represents the underlying state of the environment, and \(o_{t}=(s_{t})\) is the observation received by the agent, only providing incomplete information of \(s_{t}\). After taking an action \(a_{t}\), \(p(s_{t+1}|s_{t},a_{t})\) defines the transition probability from state \(s_{t}\) to \(s_{t+1}\). The agent also receives immediate rewards \(r_{t+1}=r(s_{t},a_{t})\), and aim to learn a policy \(\) such that \(a_{t}(o_{1:t})\) maximizing the \(\)-discounted accumulated rewards \(_{p,}[_{t}^{t-1}r_{t}]\).

While world models can be learned from many types of data, video is one modality that is task-agnostic, widely available, and embeds broad knowledge that can be learned in a self-supervised way. Thus, we formulate learning world models for visual control as an _interactive video prediction_ problem [102; 12] where \(=^{H W 3}\) is the space of video frames2. Concretely, given a short history visual observations of \(T_{0}\) frames \(o_{1:T_{0}}\), at each step \(t=T_{0},,T-1\), the agent takes an action \(a_{t}\) based on its policy and previous imagined observations, and then the world model need to approximate and sample the transition \(p(o_{t+1},r_{t+1} o_{1:t},a_{T_{0}:t})\) to feedback the agent.

As depicted in Figure 2, a majority of advanced video generation models [101; 8; 104], including VideoGPT, can not deal with the interactive video prediction problem because they design non-causal modules fusing information along the temporal dimension, lacking the ability for causal, intermediate action control during generation (see extended discussion in Appendix C.2). Existing world models in the literature of MBRL [29; 80], such as Dreamer, utilize recurrent architecture but lack scalability.

## 3 Interactive VideoGPT

In this section, we introduce Interactive VideoGPT, a scalable world model architecture with great flexibility to integrate multimodal signals, including visual observations, actions, rewards, and other potential sensory inputs. At its core, iVideoGPT consists of a compressive tokenizer to discretize video frames and an autoregressive transformer predicting subsequent tokens (Section 3.1). This model can acquire common knowledge of motions and interactions in various scenes through pre-training on diverse human and robotic manipulation videos (Section 3.2) and then effectively transfer to downstream tasks incorporating additional modalities (Section 3.3).

### Architecture

Compressive tokenization.Transformers particularly excel in operating over sequences of discrete tokens. VQGAN  is a commonly used visual tokenizer that converts from raw pixels to discrete tokens. Instead of using an image tokenizer to discretize each frame independently [55; 63; 27], leading to rapidly increasing sequence lengths, or using a 3D tokenizer that compresses videos

Figure 2: Conceptual comparison among architectures, illustrated using a single context frame (\(T_{0}=1\)) for simplicity. (a) Recurrent architectures for world models like Dreamer  and MuZero  provide step-level interactivity but limited scalability. (b) Recent video generation advancements like VideoGPT  and Stable Video Diffusion [8; 7] use non-causal temporal modules that can only offer trajectory-level interactivity. (c) Our model utilizes an autoregressive transformer that separately maps each step into a sequence of tokens, achieving both scalability and interactivity.

spatiotemporally at the expense of interactivity , we propose to tokenize videos with a novel conditional VQGAN consisting of dual encoders and decoders \(\{(E_{c},D_{c}),(E_{p},D_{p})\}\). As illustrated in Figure 2(a), initial context frames \(o_{1:T_{0}}\), rich in contextual information, are independently tokenized and reconstructed through \(N\) tokens: \(z_{t}^{(1:N)}=E_{c}(o_{t}),_{t}=D_{c}(z_{t})\) for \(t=1,,T_{0}\). In contrast, due to the temporal redundancy between context and future frames, only essential dynamics information, such as the position and pose of moving objects, needs to be encoded. This is achieved using a conditional encoder and decoder, which require a far smaller number of \(n\) tokens (\(n N\)):

\[z_{t}^{(1:n)}=E_{p}(o_{t}|o_{1:T_{0}}),_{t}=D_{p}(z_{t}|o_{1:T_{0}}) \;\;t=T_{0}+1,,T. \]

We implement this conditioning mechanism using cross-attention between multi-scale feature maps (see details in Appendix A.1). Overall, the proposed tokenizer is trained with the following objective:

\[_{}=_{t=1}^{T_{0}}_{}(o _{t};E_{c}(),D_{c}())+_{t=T_{0}+1}^{T}_{ }(o_{t};E_{p}(|o_{1:T_{0}}),D_{p}(|o_{1:T_{0}})), \]

where \(_{}(o;E,D)\) is a combination of a \(L_{1}\) reconstruction loss, a commitment loss , a perceptual loss , and optionally an adversarial loss .

There are primarily two benefits of the proposed tokenization. First, it significantly reduces the sequence length of tokenized videos, which grows linearly with the number of frames but at a much smaller rate \(n\). In this work, we set \(N=16 16\) and \(n=4 4\), resulting in an asymptotic reduction of \(16\), facilitating faster rollouts for model-based planning and reinforcement learning. Second, by conditional encoding, transformers predicting subsequent tokens can maintain temporal consistency of the context much easier and focus on modeling essential dynamics information . We discuss the assumptions and limitations of our tokenization in Section 6.

Interactive prediction with Transformers.After tokenization, the video is flattened into a sequence of tokens: \(x=(z_{1}^{(1)},,z_{1}^{(N)},\)\([\$],z_{2}^{(1)},,z_{2}^{(N)},,\)\([\$],z_{T_{0}+1}^{(1)},,z_{T_{0}+1}^{(n)},)\) with a length of \(L=(N+1)T_{0}+(n+1)(T-T_{0})-1\). Special slot tokens [\(\$]\) are inserted to delineate frame boundaries and facilitate the integration of extra low-dimensional modalities such as actions (see Section 3.3 for details). As Figure 2(b), a GPT-like autoregressive transformer is utilized for interactive video prediction through next-token generation frame-by-frame. In this work, we take the model size of GPT-2  but adopt the LLaMA architecture  in order to embrace the latest innovations for LLM architecture, applying pre-normalization using RMSNorm , SwiGLU activation function , and rotary positional embeddings .

Figure 3: Architecture of iVideoGPT, simplified to show only a single context frame (\(T_{0}=1\)). (a) Compressive tokenization utilizes a conditional VQGAN that discretizes future frames conditioned on context frames to handle temporal redundancy, significantly reducing the number of video tokens. (b) An autoregressive transformer integrates multimodal signals—visual observations, actions, and rewards—into a sequence of tokens, enabling interactive agent experiences through next-token prediction. Actions and rewards are optional and not included in action-free video pre-training.

### Pre-Training

Large language models can gain extensive knowledge from Internet text in a self-supervised way via next-word prediction. Similarly, the _action-free video pre-training_ paradigm for world models  involves video prediction as a pre-training objective, providing Internet-scale supervision with physical world knowledge absent in LLMs. We pre-train iVideoGPT on this generic objective, applying a cross-entropy loss to predict subsequent video tokens:

\[_{}=-_{i=(N+1)T_{0}+1}^{L} p(x_{i} |x_{<i}), \]

where \(L\) is the total sequence length and \((N+1)T_{0}+1\) marks the first token index of the frames to be predicted. Notably, we do not train iVideoGPT to generate context frames, making its capacity focus on dynamics information, as previously discussed.

Pre-training data.While there are numerous videos available on the Internet, due to computational limitations, we specifically pre-train iVideoGPT for the robotic manipulation domain. We leverage a mixture of 35 datasets from the Open X-Emphotiment (OXE) dataset  and the Something-Something v2 (SSv2) dataset , totaling 1.4 million trajectories (see Appendix A.2 for details). OXE is a diverse collection of robot learning datasets from a variety of robot embodiments, scenes, and tasks. These datasets are highly heterogeneous but can be easily unified in the action-free video prediction task. To further enhance the diversity, we also include SSv2, a dataset of human-object interaction videos, as previous work has demonstrated knowledge transfer from these human manipulation videos for learning a world model for robotic manipulation tasks .

Flexibility of sequence modeling.A sequence of tokens provides a flexible way to specify tasks, inputs, and outputs . To preliminarily showcase this flexibility, we introduce a variant of iVideoGPT for goal-conditioned video prediction: \(p(o_{T_{0}+1:T}|o_{1:T_{0}},o_{T})\), where the model predicts a video sequence reaching a specified goal observation \(o_{T}\). This is simply achieved by rearranging the frame sequence as \(_{1:T}=(o_{T},o_{1},o_{2},,o_{T-1})\) while keeping the architecture and training procedure consistent as above (see details in Appendix A.2). Qualitative results of goal-conditioned prediction are shown in Figure 4, with further exploration left for future work3.

### Fine-Tuning

Action conditioning & reward prediction.Our architecture is also designed to flexibly incorporate additional modalities for learning interactive world models, as illustrated in Figure 2(b). Actions are integrated by linear projection and adding to the slot token embeddings. For reward prediction, instead of learning independent reward predictors, we add a linear head to the last token's hidden state of each observation. This multi-task learning approach can enhance the model's focus on task-relevant information, thereby improving prediction accuracy for control tasks . We use a mean-squared error loss for reward prediction in addition to the cross-entropy loss in Eq. (3).

Tokenizer adaptation.We choose to update the full model, including the tokenizer, for downstream tasks, finding this strategy more effective than parameter-efficient fine-tuning methods . This is likely due to the limited diversity of our pre-trained data compared to Internet-scale images, which, while extensive, may also not adequately cover specific real-world applications like robotics. Minimal literature explores adapting a VQGAN tokenizer to domain-specific data. As our tokenization is designed for decoupling dynamics information from context conditions, we hypothesize that while our model may encounter unseen objects like different robot types in downstream tasks, the fundamental knowledge of physics--such as motions and interactions--learned by the transformer from diverse scenes is commonly shared. This hypothesis is supported by our experiments transferring iVideoGPT from mixed pre-training data to the unseen BAIR dataset , where the pre-trained transformer can zero-shot generalize to predict natural motions, requiring only the tokenizer to be fine-tuned for unseen robot grippers (see Figure 8). This property is particularly important for scaling GPT-like transformers to large sizes, enabling lightweight alignment across domains while keeping the transformer intact. We leave an in-depth analysis of tokenizer adaptation for future work.

## 4 Experiments

In this section, we evaluate iVideoGPT in three different control-relevant settings and compare its performance with prior state-of-the-art methods. We demonstrate that iVideoGPT is versatile to provide competitive performance across a range of tasks (Section 4.1, 4.2, and 4.3) and conduct in-depth analysis to understand the tokenization and prediction ability, data efficiency, model scaling, and computational efficiency (Section 4.4). Experimental details can be found in Appendix B.1.

### Video Prediction

Setup.The BAIR robot pushing dataset  consists of 43k training and 256 test videos, where we predict 15 frames from a single initial frame, a standard protocol of prior works. The RoboNet dataset  contains 162k videos across 7 robotic arms. Following prior works, we use 256 videos for testing, predicting 10 frames from two frames. Notably, RoboNet overlaps with our pre-training data OXE, from which we have carefully filtered test videos. We compare against a variety of video prediction models, including variational , diffusion , masked , and autoregressive models , across four metrics: FVD , PSNR , SSIM , and LPIPS .

Results.As shown in Table 1, iVideoGPT provides competitive performance compared to state-of-the-art methods, MAGVIT  for BAIR and FitVid  for RoboNet, while achieving both interactivity and scalability in its architecture. Initially pre-trained action-free, our model flexibly allows for action-conditioning, which notably improves FVD for BAIR by almost 20%. Although primary experiments are at a low resolution of \(64 64\), iVideoGPT can be easily extended to \(256 256\) for RoboNet. We highlight that MaskViT, a prior method leveraging per-frame tokenization, suffers from temporal inconsistency and flicker artifacts in VQGAN reconstructions. Our model, which employs compressive tokenization conditioned on consistent contextual information, improves this and significantly outperforms MaskViT. For qualitative results, refer to Figure 4.

### Visual Planning

Setup.VP\({}^{2}\) is a control-centric benchmark  that evaluates video prediction models for visual model-predictive control (MPC)  across four Robosuite  and seven RoboDesk tasks .

Figure 4: Qualitative evaluation: video prediction results of iVideoGPT on Open X-Embodiment, RoboNet, and VP\({}^{2}\). Zoom in for details. Extended examples can be found in Appendix B.1.

Each environment's training dataset includes noisy scripted interaction trajectories. Following the protocol from the original benchmark paper, we trained iVideoGPT on 5k trajectories for Robosuite and 35k for RoboSek, comparing our models with established baselines.

Results.Figure 5 presents the success rates of iVideoGPT compared to baseline models. While Tian et al.  observed that excellent perceptual metrics do not always correlate with effective control performance, iVideoGPT outperforms all baselines in two RoboDesk tasks with a large margin and achieves comparable average performance to the strongest model, SVG\({}^{}\). In Appendix C.3, we analyze iVideoGPT's suboptimal performance on the open slide task, which is attributed to both limitations of discretization in our model and imperfect built-in reward design of the benchmark.

### Visual Model-based Reinforcement Learning

Setup.We conduct experiments on six robotic manipulation tasks of varying difficulty from MetaWorld . Leveraging iVideoGPT as interactive world models, we have developed a model-based RL method adapted from MBPO , which augments the replay buffer with synthetic rollouts to train a standard actor-critic RL algorithm (see Appendix A.5 for the pseudo-code). Our implementation builds upon DrQ-v2 , a state-of-the-art visual model-free RL method. We also compare against a state-of-the-art model-based RL algorithm, DreamerV3 , with and without world model pre-training .

  
**BABIR** & FVD\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\   \\  VideoGPT  & 103.3 & - & - & - \\ MaskViT  & 93.7 & - & - & - \\ FitVid  & 93.6 & - & - & - \\ MCVD  & 89.5 & 16.9 & 78.0 & - \\ MAGVIT  & **62.0** & 19.3 & 78.7 & 12.3 \\ iVideoGPT (ours) & 75.0\(\)0.20 & **20.4**:0.01 & **82.3\(\)0.05** & **95.5\(\)0.01** \\   \\  MaskViT  & 70.5 & - & - & - \\ iVideoGPT (ours) & **60.8\(\)0.08** & **24.5\(\)0.01** & **90.2\(\)0.03** & **5.0\(\)0.01** \\   

Table 1: Video prediction results on the BAIR robot pushing and RoboNet datasets. We report the mean and standard deviation for each metric calculated over three runs. ”-” marks that the value is not reported in the original papers. LPIPS and SSIM scores are scaled by 100 for convenient display.

Figure 5: Visual MPC results on the VP\({}^{2}\) benchmark. We report the mean and min/max performance of iVideoGPT over 3 control runs. On the right, we show the mean scores averaged across all tasks except flat block due to low simulator performance, normalized by the performance of the simulator.

Figure 6: Powerful iVideoGPTs enable a simple yet performant MBRL algorithm, decoupling model rollouts and policy learning.

Results.Figure 7 shows that our model-based algorithm not only remarkably improves the sample efficiency over its model-free counterpart but also matches or exceeds the performance of DreamerV3. To our knowledge, this reports the _first successful application of MBPO to visual continuous control tasks_. These results highlight the opportunity, with powerful world models, to eliminate the need for latent imagination--a common strategy used in advanced MBRL systems to train policies on rollouts of latent states within world models  (see comparison in Figure 6). Our development of performant MBRL algorithms decouples model and policy learning, where iVideoGPT simply serves as a drop-in replacement of the environment. This can substantially simplify the design space, thereby greatly enhancing the practicality and effectiveness of MBRL algorithms in real-world applications.

Comparison to recurrent world models.We argue that recurrent world models lack the capacity for large-scale pre-training on real-world data--a crucial capability for modern foundation models. To validate this, we pre-train DreamerV3 XL (200M parameters, comparable to iVideoGPT) on the same dataset. As shown in Figure 11 in the Appendix, DreamerV3 fails to capture natural robot dynamics, yielding low-quality, blurred predictions. Further evaluation on the Meta-World benchmark in Figure 7 reveals that DreamerV3 cannot benefit from such ineffective pre-training.

### Model Analysis

Zero-shot prediction.We first analyze the zero-shot video prediction ability of large-scale pre-trained iVideoGPT on the unseen BAIR dataset. Interestingly, we observe in the second row of Figure 8 that iVideoGPT, without fine-tuning, predicts a natural movement of a robot gripper--albeit a different one from our pre-training dataset. This indicates that while, due to insufficient diversity of pre-training data, our model has a limited ability of zero-shot generalization to completely unseen robots, it effectively separates scene context from motion dynamics. In contrast, with an adapted tokenizer, the transformer that is not fine-tuned itself successfully transfers the pre-trained knowledge and predicts movements for the new robot type in the third row, providing a similar perceptual quality as the fully fine-tuned transformer in the fourth row. Quantitative results can be found in Figure 8(a).

Few-shot adaptation.Large-scale pre-trained models have proven effective, especially in data-scarce scenarios. Figure 8(a) shows iVideoGPT's performance when fine-tuned with various sizes of action-free BAIR trajectories. We observe that pre-training offers minimal benefits when full downstream data is available, yet the advantages become significant under data scarcity (with 100 or 1,000 trajectories). We also adapt iVideoGPT using 1,000 action-conditioned BAIR trajectories, achieving an FVD of 82.3. The fast adaptation ability with a handful of data is particularly crucial in model-based RL. As shown in Figure 7, world models trained from scratch may generate inaccurate predictions, thereby degenerating the sample efficiency that is vital for model-based agents.

Model scaling.All previous experiments are conducted using an iVideoGPT with 12 transformer layers and 768-dimensional hidden states (138M parameters). To initially investigate the scaling

Figure 7: Visual model-based RL on Meta-world. _(Left)_ Aggregated results report interquartile mean and 95% confidence interval (CI)  across a total of 30 runs over six tasks. _(Right)_ Individual results for each task, report mean and 95% CI across five runs, measuring success rates over 20 evaluation episodes. _PT_ denotes pre-training.

behavior of our model, we trained a larger iVideoGPT with 24 layers and 1024-dimensional hidden states (436M parameters). Figure 8(b) illustrates the validation loss curves on the pre-trained dataset. It shows that (1) the validation loss (perplexity) continues to decrease regardless of model size, and (2) increasing the model size accelerates the loss decrease. These results align with our expectation that larger model sizes and increased computation  can build more powerful iVideoGPTs.

Tokenization efficiency.We evaluate the effectiveness of our compressive tokenization by comparing it against standard VQGAN tokenizers that independently convert each frame into \(16 16\) and \(4 4\) tokens. We train three tokenizers from scratch on RoboNet for the same number of steps. As Figure 8(c), the tokenizer with \(4 4\) tokens suffers from low reconstruction quality due to its insufficient capacity. Our proposed tokenization method slightly compromises reconstruction quality compared to the standard \(16 16\) tokenizer but can provide more consistent contextual information, which is beneficial for video prediction tasks. More importantly, it significantly enhances computational efficiency with a significantly fewer amount of tokens, which greatly saves time and memory, allowing us to scale the model size with fewer costs (see quantitative results in Appendix B.5).

Context-dynamics decoupling.Our tokenizer is designed with a bottleneck of much fewer tokens, focusing only on capturing necessary dynamics information for future frames while sharing contextual information with initial frames to reconstruct raw pixels. To explicitly visualize this decoupling of context and dynamics information, we drop cross-attention blocks to context frames in the decoder when reconstructing future frames. The results in Figure 10 show that the decoder can still reproduce the movement trajectories accurately but with minimal contextual information. This visualization supports the explanation of our model's generalization capability shown in Figure 8.

Goal-conditioned prediction.In Figure 4, we also showcase video prediction generated by goal-conditioned iVideoGPT, pre-trained on massive human and robotic videos (Section 3.2). Unlike action-free prediction, which often results in trajectories diverging from the ground truth, the goal-conditioned model produces more accurate paths to reach specified goals. We believe this highlights the potential of leveraging the flexibility of a unified sequence modeling paradigm.

Figure 8: Zero-shot prediction by pre-trained transformer in iVideoGPT. Without fine-tuning, the transformer predicts natural movements of a different robot gripper using the pre-trained tokenizer _(second row)_ but accurately predicts for the correct gripper type with an adapted tokenizer _(third row)_.

Figure 9: Model analysis. (a) Video prediction results with various fine-tuning strategies and data sizes on BAIR. (b) Validation losses for the 138M and 436M transformer models on the pre-training dataset. (c) Computational efficiency and reconstruction quality of different tokenizers.

## 5 Related Work

World models for visual control.Learning general world models in visual domains remains a significant challenge in model-based reinforcement learning. A straightforward method involves learning action-conditioned video prediction models [69; 45]. Advanced model-based RL algorithms [29; 31; 32; 80; 34; 33] utilize latent imagination for more efficient and accurate rollouts but complicate themselves to tightly coupling model and policy learning. We show that this complexity can be reduced with powerful world models that have accumulated generalizable knowledge beyond specific tasks. Recent efforts to facilitate this include leveraging scalable architectures like transformers  and pre-training from large-scale data [99; 62]. Of particular relevance to our work are UniSim  and Genie , which have developed extensively trained world models with diffusion and masked models, respectively, though neither is publicly available. Our work distinguishes itself by utilizing a generic autoregressive transformer framework, advancing the flexibility of scalable world models.

Video generation and prediction.Recent developments in Internet-scale video generation models now enable the synthesis of realistic videos conditioned on class labels, text descriptions, and initial frames--the last one also known as the video prediction problem. Various models have been developed, including deterministic RNNs [84; 96], variational autoencoders [18; 3; 30; 4], diffusion [38; 11], masked [104; 27], and autoregressive models [101; 50; 55]. However, most recent works do not treat video prediction as a dynamics modeling problem and perform spatiotemporal compression [101; 8], thus providing limited interactivity to serve as world models. We achieve both compressive tokenization and interactivity by context-aware representation, employing cross-attention mechanisms with minimal inductive bias. This method diverges from previous techniques that rely on motion vectors  or optical flows  and offers a more generic form of video tokenization.

## 6 Discussion

We introduced Interactive VideoGPT (iVideoGPT), a generic and efficient world model architecture that leverages a scalable autoregressive transformer to integrate multimodal signals into a sequence of tokens, providing an interactive agent experience via next-token prediction. iVideoGPT has been pre-trained on millions of human and robotic manipulation trajectories and adapted to a wide range of downstream tasks. As a powerful foundation of world models, it enables accurate and generalizable video prediction as well as simplified yet performant model-based planning or reinforcement learning.

Limitations and future work.While iVideoGPT marks significant progress, there is substantial room for improvement. We found limited diversity in publicly available robotic data, including the large-scale Open X-Embodiment dataset, and initiated efforts to transfer knowledge from human videos . We believe iVideoGPT should be pre-trained on more extensive data  to bridge knowledge between humans and robots. This also requires iVideoGPT to incorporate more modalities, such as multi-view observations, proprioceptive robot states, and actions, within the unified formulation beyond action-free video prediction. Specifically, to process high-dimensional visual observations, our compressive tokenization assumes that initial frames provide sufficient contexts for future frames, which works for low-level control tasks as model-based agents often foresee tens of steps, but may falter in scenarios with long videos and significant camera motion. This issue can be mitigated by keyframe extraction  but leaves an important future avenue of exploration. Finally, extending to more complex real-robot tasks is essential, as the benefits of model scaling to even larger sizes remain unobserved in this work within visually simple simulation for downstream control tasks.

Figure 10: Context-dynamics decoupling in our compressive tokenization. By removing cross-attention from future frames to context frames, the decoder can still reconstruct a trajectory that moves in the same way as the original, but the visual context is almost entirely missing.