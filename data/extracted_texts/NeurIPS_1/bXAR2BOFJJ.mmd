# Bayesian Nonparametric Learning using the

Maximum Mean Discrepancy Measure for Synthetic Data Generation

Forough Fazeli Asl

Department of Mathematical and Statistical Sciences

University of Alberta

Edmonton, Canada

fazelias@ualberta.ca

&Michael Minyi Zhang

Department of Statistics and Actuarial Science

University of Hong Kong

Hong kong, China

mzhang18@hku.hk

&Lizhen Lin

Department of Mathematics

The University of Maryland

College Park, MD, USA

lizhen01@umd.edu

Corresponding author

###### Abstract

We introduce a Bayesian estimator for maximum mean discrepancy (MMD), enabling a novel approach to measure-based data generation. To demonstrate the adaptability of our method, we embed this estimator within a generative adversarial network (GAN) framework. This integration offers a powerful avenue for Bayesian nonparametric (BNP) learning, showcasing the estimator's broad applicability. Our BNP-driven GAN not only enhances sample diversity but also improves inferential accuracy, surpassing the performance of traditional methods. Further theoretical properties, proofs, and experiments are given by the Appendix.

## 1 Introduction

Data augmentation is the technique of generating synthetic data, often to train machine learning models when the data are scarce or the model is non-robust to perturbations in the data. When the likelihood is intractable to compute, evaluating the model's fit can be difficult. Maximum mean discrepancy (MMD) addresses this problem by enabling comparisons between distributions without explicit likelihoods through its feature matching properties. This property ensures that generated data matches the features of real data, making MMD an effective tool for evaluating and developing deep generative models. Bayesian nonparametric methods are a powerful tool with strong theoretical justifications but they have seen limited applications in MMD estimation. A key advantage of the Bayesian approach is that it incorporates expert knowledge through a prior distribution, offering regularization by introducing uncertainty in the sampling distribution via the Dirichlet process (DP). The absence of such methods in MMD estimation restricts statisticians who prefer Bayesian frameworks without making strong assumptions. This paper addresses this gap.

We propose a BNP estimator to accurately estimate the MMD between a parametric model and an unknown distribution by placing a DP prior on the unknown distribution. We extend the bootstrap method from  beyond posterior parameter inference, applying our estimator to training a generative adversarial network (GAN). Our approach uses the MMD estimator as a robust discriminator,combining MMD measurement with BNP inference to enhance GAN training, reduce mode collapse, and improve generator performance compared to frequentist (FNP) methods.

## 2 Previous work

Previous work on simulation-based inference has mainly used discrepancy measures from a frequentist nonparametric (FNP) perspective. Notably, GANs have been extensively explored in data augmentation and medical data synthesis where the fake images created by the GAN are used to supplement the training data [2; 3; 4; 5; 6]. A standard GAN features two neural networks: the generator \(\{G_{}\}_{}\) and the discriminator \(\{D_{}\}_{}\). The generator tries to fool the discriminator into misidentifying generated samples as real. However, these models are expanded beyond the classic loss function, which could potentially introduce challenges such as mode collapse-memorizing certain modes of data distribution while overlooking other diversities-and training instability.

A Bayesian approach, known as approximate Bayesian computation (ABC), estimates model parameters through simulation by comparing summary statistics of simulated and observed data . ABC faces challenges in selecting informative summary statistics, which affects the accuracy of posterior inference [9; 10]. One particularly attractive choice of statistic is to use the MMD . As the threshold decreases, ABC converges to the standard Bayesian posterior, which can be sensitive to model misspecification and lacks robustness .

To improve robustness, generalized Bayesian inference (GBI) replaces the likelihood with a robust loss function . Example applications of GBI include using MMD in pseudo-likelihood approaches  or stochastic gradient MCMC for posterior inference . Despite these advancements, GBI's sensitivity to hyperparameters and the computational demands of MCMC remain challenges . To address these issues, an MMD posterior bootstrap method has been developed, offering a more efficient alternative [1; 15; 16; 17].

## 3 Dirichlet process

To perform Bayesian nonparametric learning (BNPL), we first must take samples from the Dirichlet process. The DP is an infinite generalization of the Dirichlet distribution that is considered on the sample space denoted as \(\), which possesses a \(\)-algebra \(\) comprising subsets of \(\). \(F\) follows a DP with parameters \((a,H)\) with the notation \(F^{}:=(F DP(a,H))\), if for any measurable partition \(A_{1},,A_{k}\) of \(\) with \(k 2\), the joint distribution of the vector \((F(A_{1}),,F(A_{k}))\) follows a Dirichlet distribution characterized by parameters \((aH(A_{1}),,aH(A_{k}))\). Moreover, it is assumed that \(H(A_{j})=0\) implies \(F(A_{j})=0\) with probability one. The base measure \(H\) captures the prior knowledge regarding the data distribution, while \(a\) signifies the strength or intensity of this knowledge.

As a conjugate prior, the posterior distribution of \(F\) also follows a DP, denoted by \(F^{}:=(F|_{1:n}(a+n,H^{*}))\), for \(n\) independent and identically distributed (IID) draws, \(_{1:n}^{d}\), from the random probability measure \(F\) where \(H^{*}=a(a+n)^{-1}H+n(a+n)^{-1}F_{_{1:n}}\), and \(F_{_{1:n}}\) represents the empirical cumulative distribution function of the sample \(_{1:n}\).

To sample from the DP posterior, we use a finite approximation devised by Ishwaran and Zarepour , which allows for convenient simulation. In the context of posterior inference, this approximation is given by

\[F_{N}^{}:=_{i=1}^{N}J_{i,N}^{}_{_{i}^ {}}, \]

where \((J_{1:N,N}^{})((a+n)/N,,(a+n)/N)\), \(_{1:N}^{}}{}H^{*}\), and \(_{^{}}\) is the Dirac delta measure. In this study, the variables \(J_{i,N}^{}\) and \(_{i}^{}\) represent the DP's weight and location, respectively. The sequence \((F_{N}^{})_{N 1}\) converges in distribution to \(F^{}\), where \(F_{N}^{}\) and \(F^{}\) are random values in \(M_{1}(^{d})\), the space of probability measures on \(^{d}\) endowed with the topology of weak convergence . Although the stick-breaking representation is a commonly employed series representation for DP inference , it lacks the necessary normalization terms to convert it into a probability measure . Additionally, simulating from an infinite series is only feasible through using a random truncation approach to handle the terms within the series. In the subsequent sections, we investigate the efficacy of this approximation within a regularization method in a BNP generative model.

## 4 DPMD-GAN: A Bayesian Nonparametric Learning in Data Generation

In our BNPL method, we define a DP prior on \(F\), leading to a DP posterior on \(F\) given the data. The key idea is that any posterior on the generator's parameter space \(\) can be derived by mapping \(F^{pos}\) approximately through the push-forward measure

\[^{*}(F^{pos}):=_{} _{}(F_{N}^{pos},F_{G_{}}), \]

where for a given sample \(_{1:m} F_{G_{}}\) the posterior-based MMD estimator is defined by

\[_{}^{2}(F_{1,N}^{pos},F_{2,m})=_{,t=1}^{N}J_{ ,N}^{*}J_{t,N}^{*}k(_{}^{*},_{t}^{*})- _{=1}^{N}_{t=1}^{m}J_{,N}^{*}k(_{}^{*},_{t})+}_{,t=1}^{m}k(_{},_{t}). \]

In this context, the discriminator \(D\) can be viewed as a black box that uses the MMD estimator to differentiate between the real and fake data, reducing the computational cost compared to a neural network-based discriminator. We discuss some of the properties of estimator (3) in the Appendix. Let \(^{*}\) be the optimal parameter of \(G_{}\) that minimizes \(_{}(F_{N}^{pos},F_{G_{},m})\). Since \(_{}(F_{N}^{pos},F_{G_{},m})\) serves as a BNP estimation of (5), it is crucial to evaluate this estimation's accuracy, focusing on the GAN's ability to generate realistic samples (generalization error) and handle outliers (robustness). Lemma 2 in the appendix addresses these aspects. While the previous statements provide upper bounds for the MMD estimator's expectation, the next lemma offers stochastic bounds on the estimation error to assess posterior consistency.

**Lemma 1**: _Building upon the general assumptions stated in Lemma 2, for a given sample \(_{1},,_{n}\) from distribution \(F\) in the probability space \((,,)\) and any \(>0\), \(i.(|(F_{N}^{pos},F_{G_{^{*}},m})- (F,F_{G_{^{}}})| h(n,m,K,)+| _{1}|+|_{2}|) 2nm}{2K(n+m)},\)_

\[ii.((F,F_{G_{^{*}}})>) ((F,F_{G_{^{ }}})+}++2}.),\]

_where, \(h(n,m,K,)=2(+)/+\), \(_{1}=(F_{N}^{pos},F_{G_{^{*}}})-(F_{n},F_{G_{^{}},m})\), and \(_{2}=(F,F_{G_{^{*}}})-(F,F_{G_ {^{}}})\)._

A direct consequence of Lemma 1(ii) is that for a fixed value of \(a\), \(((F,F_{G_{^{*}}})) 0\), as \(n\) and \(N\), for any \(>0\), when \((F,F_{G_{^{}}})=0\) (well-specified case). This implies \(F_{G_{^{*}}}\) converges in probability to the data distribution \(F\) as the sample size increases in well-specified cases. A detailed guide on choosing DP hyperparameters and kernel settings is provided in the Appendix.

## 5 Experimental results

We consider the MNIST dataset including handwritten digits with 10 modes, bone marrow biopsy (BMB) histopathology, Labeled Faces in the Wild (LFW) Dataset, and brain MRI images to analyze the model performance. All data description are given by the Appendix. Following the design choices of , we use the Gaussian neural network for the generator with four hidden layers each having rectified linear units activation function and a sigmoid function for the output layer. We also set mini-batch sizes to be \(n_{mb}=1,000\) and use a mixture of six Gaussian kernels corresponding to the bandwidth parameters \(2,5,10,20,40,\) and \(80\) to train networks in \(40,000\) iterations. We generate samples from the trained BNP GAN using Algorithm 2 from the Appendix, as depicted in Figure 1-Row 2. The results of  are also presented by Figure 1-Row 3, as the frequentist counterpart of our BNP procedure. Based on these preliminary results, we can see that our generated images can, at least, replicate the results of  and in some cases produce sharper images. This result can also be deduced from the presented scores of MMD, Kernel Inception Distance (KID)  and FrechetInception Distance (FID)  in Table 1. The low scores suggest better performance, as smaller values indicate closer similarity to real images.

Conversely, our results show that the BNP GAN with a mixture of Gaussian kernels outperforms the single Gaussian kernel approach. To explore this further, Figure 2 in the Appendix presents samples from the trained generator using various \(\) values and the median heuristic \(_{MH}\). Note that \(_{MH}\) is updated in each iteration, so no specific value is reported. Although higher \(\) increases image diversity, the resolution remains below that achieved with the mixture kernel.

To assess whether the proposed discriminator used in the BNP GAN leads to faster or better convergence of the generated samples compared to the baseline proposed by , we consider the synthetic distribution \(N(-_{d},I_{d})+N(_{d},I_{d})\) as the true distribution and provide the corresponding MMD values for both models over 20,000 iterations in the data generation process, as shown in Figure 3 of Appendix. Our proposed GAN clearly displays a higher speed of convergence for the corresponding cost function to zero, and thus better performance compared to the baseline.

   Scores &  \\   &  &  &  &  \\   & BNP & FNP & BNP & FNP & BNP & FNP & BNP & FNP \\  MMD & \(0.0384\) & \(0.0404\) & \(0.0285\) & \(0.0315\) & \(0.0281\) & \(0.0302\) & \(0.2059\) & \(0.2231\) \\ KID & \(0.0034\) & \(0.0046\) & \(0.0030\) & \(0.0036\) & \(0.0019\) & \(0.0026\) & \(0.0260\) & \(0.0264\) \\ FID & \(35.560\) & \(37.934\) & \(17.006\) & \(17.264\) & \(14.010\) & \(14.473\) & \(87.975\) & \(87.831\) \\   

Table 1: The values of MMD, KID, and FID scores for four groups of datasets.

Figure 1: Generated samples from training dataset (Row 1), BNP model (Row 2), and FNP model (Row 3).

Concluding remarks

Our BNP approach effectively estimates the MMD between an unknown and an intractable parametric distribution, showing promise in training GANs by using the estimator as a discriminator to induce a posterior on the generator's parameters. The stick-breaking representation, however, lacks normalization and shows stochastic decrease, making it inefficient for simulating from a DP . Exploring alternative DP approximations for MMD estimation is a promising direction for future research. Future work will also focus on generating 3D medical images to further improve results.