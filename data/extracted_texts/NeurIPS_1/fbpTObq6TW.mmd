# A fast heuristic to optimize time-space tradeoff for large models

Akifumi Imanishi

Preferred Networks

imanishi@preferred.jp

&Zijian Xu

Preferred Networks

joe@preferred.jp

&Masayuki Takagi

Preferred Networks

mtakagi@preferred.jp

&Sixue Wang

Preferred Networks

cecilwang@preferred.jp

Equal contribution

Emilio Castillo

Preferred Networks

ecastill@preferred.jp

###### Abstract

Training large-scale neural networks is heavily constrained by GPU memory. In order to circumvent this limitation, gradient checkpointing, or recomputation is a powerful technique. There is active research in this area with methods such as Checkmake  or Moccasin . However, both Checkmate and Moccasin rely on mixed integer linear programming or constraint programming, resulting in limited scalability due to their exponentially large search space.

This paper proposes a novel algorithm for recomputation (FastSA) based on a simulated annealing heuristic that achieves comparable or even better solutions than state-of-the-art alternatives. FastSA can optimize computational graphs with thousands of nodes within 3 to 30 seconds, several orders of magnitude faster than current solutions.

We applied FastSA to PyTorch models and verified its effectiveness through popular large vision and text models, including recent language models with the transformer architecture. The results demonstrate significant memory reductions by 73% with extra 18% computational overheads on average. Our experiments demonstrate the practicality and effectiveness of our recomputation algorithm, further highlighting its potential for wide application in various deep learning domains.

## 1 Introduction

The memory requirements for deep neural networks continue to grow together with model complexity. For example, training a state-of-the-art Large Language Model (LLM) such as LLaMA  with 65 billion parameters needs 1024 NVIDIA A100 GPU devices for 21 days and different techniques to overcome the immense memory consumption of the process. While the model parameters may fit in the device memory, the results of several operations of the forward pass are saved and remain in the device memory to compute the gradients during the backpropagation step. This limits the size of trainable models. One of the most widely used solutions is to split the model parameters across multiple devices into vertical or horizontal ways  in what is called Pipeline or Model parallelism. However, these approaches may have the following problems. (1) It may be difficult to split the model equally across the workers when the model architecture is complex. (2) We need to modify the model for the parallelism. (3) Hiding communication behind computation needs further tuning. Heavy inter-device communication may result in a GPU utilization of only 5% of the hardware peak.

Numerous techniques exist to lessen memory requirements while training deep neural networks. Gradient checkpointing, or recomputation, is a widely-known approach that recomputes some of the activations during the backward pass, rather than saving them in memory, consequently reducing memory requirements. Recomputation plans can be identified by the user or done heuristically. Originally, division-based methods were used for automatic recomputation, where the forward computation graph was split into stages, and only tensors active across stages were stored . However, these methods are less effective for complex large networks that are far from sequential networks. Checkmate is one of the state-of-the-art methods in automatic recomputation. It can be applied to any computational graph, considering the costs of operators and the sizes of values. However, Checkmate requires to solve a large mixed integer linear programming problem, where the search space is exponential to the square of the size of the computational graph, thus requires immense computational time and a large RAM resource. Moreover, Checkmate cannot efficiently reduce the memory usage if the initial computation order is not memory efficient enough (discussed in Appendix A.1). A recent work, Moccasin , uses constraint programming to build a set of restrictions and introduces a hyperparameter that limits the amount of times a variable can be recomputed. Although the search space remains exponential like Checkmate, this hyperparameter lessens the number of integer variables from quadratic to linear with respect to the graph size, resulting in a faster execution.

This paper proposes a novel technique to recompute the activations that need to be saved for the backpropagation pass by using a fast heuristic algorithm that operates on the joint computational graph of the forward and backward passes and determines which operations to be recomputed to reduce the amount of used temporal memory. We formalize the problem of recomputation as finding a candidate sequence of operators that minimizes an objective function. We apply a simulated annealing algorithm where each step can be an operator addition or deletion from the computational sequence, or altering the order in which operators are sequentially applied, while maintaining the dependencies between them. The performance of the simulated annealing is significantly improved by using a segment tree data structure  which allows to lazily evaluate the memory usage of each candidate sequence, enabling nearly 1 million mutations on the candidate sequence per second.

The main contributions of our paper are:

* We present a novel approach to recompute intermediate results in computational graphs using a heuristic algorithm based on simulated annealing that works for any computational graph.
* We further improve the algorithm's performance using a data structure that allows efficient computation of peak memory usage when the sequence of operators is changed. This can optimize the recomputation within 3 to 30 seconds even for large computational graphs.
* We evaluate our proposal using a representative set of models obtained from Hugging Face, including vision models, text models, and recent language models such as LLaMa . We show reductions in memory consumption by 73% with an overhead of extra 18% computational cost on average.

Figure 1: Optimization time for all 23 models used in our experiments with different budgets. Checkmate LP could not find a feasible solution within 6 hours for models with more than 2300 nodes. For most instances, FastSA could found recomputation plan around 3 seconds.

## 2 Fast Simulated Annealing for Recomputation

### Problem Description

To apply the Simulated Annealing optimization strategy to the recomputation problem, we present a series of formal concepts that will be used throughout the algorithm description.

Computational Graph and SequenceWe consider computational graphs in which each node represents an operator. A computational graph consists of a finite set of nodes \(\) and a finite set of values \(\). Each node takes some values as inputs and produces some values as outputs; therefore, we introduce notations \((n)\) and \((n)\) for the values associated to a node \(n\). In most cases, the numbers of \((n)\) and \((n)\) are small. The computational graph is often visualized as a directed acyclic graph (DAG) with directed edges from \(n_{i}\) to \(n_{j}\) if an output of \(n_{i}\) is used as an input of \(n_{j}\), i.e., \((n_{i})(n_{j})\). A value that is not the output of any node is a model input, which often represents the arguments passed to the model and the model parameters. We also have model outputs, the target of the computation. Both nodes and values are weighted: the computational cost of node \(n\) is represented by \(c(n) 0\), and the size of value \(v\) is represented by \(s(v) 0\).

Rather than performing the optimization over the DAG, the proposed method is easier to understand if the computation is treated as a sequence of nodes \((n_{1},,n_{T})\), which means that \(n_{t}\) is operated at integer time \(t\). We say that a sequence of nodes has a valid dependency if we can execute the operators in the order of the sequence so that all model outputs are correctly computed. Formally, \( i. v(n_{i}). j<i\) such that \(v(n_{j})\) or \(v\) is a model input. It is worth noting that node duplication is allowed in the above node sequence \((n_{1},...,n_{T})\), and the recomputation of a value can be represented by multiple appearances of the same node.

Memory usageIn order to optimize memory usage, we must keep track of when values are computed and used as inputs. Specifically, we must keep value \(v\) in memory from the time it is computed as an output until the last time it is used as an input. However, when \(v\) is recomputed at time \(t\), we can temporarily free \(v\) from memory if it is not used until \(t\). When recomputation is considered, the lifetime of \(v\) can be represented as a set of independent time intervals, as shown in Figure 2. To determine whether \(v\) is kept in memory at a given time \(t\), we use a function \(L_{S}(v,t)\{0,1\}\). This function outputs 1 if \(v\) is kept in memory at time \(t\), and 0 otherwise.

\[L_{S}(v,t):=\{1&v\\ 1&v t^{}>t,v(n_{t^{}})\\ 1& t^{} t,v(n_{t^{} }) t^{}[t,t^{}),v(n_{t^{}})\\ 0&.\]

Let \(M(S)\) be the minimum memory size required to compute a valid sequence \(S\). Then, we have \(M(S)=_{1 t T}_{v V}L_{S}(v,t) s(v)\). Also, we define the cost of the sequence, \(C(S)\), by \(C(S)=_{1 t T}c(n_{t})\).

Figure 2: Lifetime of recomputed values. The left side shows the full lifetime of a value without recomputation, while the right side demonstrates that recomputation results in the value having two distinct lifetimes.

RecomputationLet \(f\) be an objective function which maps sequences to a real number. The re-computation problem is defined as the construction of a valid sequence \((n_{1},,n_{T})\) that minimizes \(f(S)\).

When minimizing memory usage, we set \(f(S):=M(S)\). The objective function can depend not only on \(M(S)\) but also on computational costs. In our experiments, we utilized Equation 1:

\[f(S):=(,M(S)) C(S) \]

This function minimizes the total computational cost when the memory budget is met. Otherwise, it tries to minimize both the peak memory and the cost.

#### 2.1.1 Add-max segment tree

The range-add/range-max segment tree  is a data structure that can evaluate add and sum queries for intervals lazily. The add-max segment tree, utilized in this paper, holds an array \(A[T]\) of length \(T\), and can carry out the following operations in \(O( T)\) time:

* Add \(x\) to \(A[i]\) for \(i[l,r)\)
* Obtain the maximum value of \(A[i]\) for \(i[l,r)\)

### Fast Simulated Annealing Algorithm

The proposed algorithm is based on simulated annealing (SA) with a straightforward outline. We make small modifications to the sequence and accept them if they decrease the objective function \(f(S)\). Please see Figure 3 for an overview of the algorithm. Two critical mutations must be performed in sublinear time for the algorithm to be efficient: (1) Slightly modifying \(S\) to create \(S^{}\). (2) Computing \(f(S^{})\).

To generate \(S^{}\) from \(S\), we either add or remove a node. We represent the sequence as a fixed-sized vector of nodes, where each position in the sequence corresponds to a specific time step. This representation makes it easy to manage the lifetime of each value. The vector contains a special node, \(\), which has zero computational cost and no inputs or outputs associated with it. When adding or removing a node, its value is exchanged with that of \(\).

We start with a valid sequence \(S=(n_{1},...,n_{T})\) and for all \(1 i T\), \(n_{i}\{\}\). To enable the insertion of a long computation between nodes, most elements of \(S\) should be initialized with \(\). In practice, the initial \(S\) is constructed from the sequence representing the initial execution order by inserting \(\) nodes between each element.

In each iteration, we perform one of the following three mutations:

1. (Add computation) Select a random node \(n\) and a random time \(t\) where \(n_{t}=\). We attempt to update \(n\), i.e., \(n_{t} n\).
2. (Remove computation) Select a random time \(t\) such that \(n_{t}\) and attempt to update \(n_{t}\).
3. (Rotating sequence) Select two random times \(t_{1}\) and \(t_{2}\) such that \(n_{t_{1}}\) and \(n_{t_{2}}=\). Try swapping \(n_{t_{1}}\) and \(n_{t_{2}}\). This modification can be implemented with the combination of the first two modifications.

Figure 3: Overview of the simulated annealing.

To ensure that the mutated sequence \(S^{}\) has a valid dependency, we validate the following conditions and update the sequence only if they are satisfied. Note that we can efficiently check these conditions by maintaining a set of produced and used times for each value \(v\).

1. (Add computation) For each input \(v\) of \(n_{t}\), \(v\) must be produced before time \(t\).
2. (Remove computation) For each output \(v\) of \(n_{t}\), either \(v\) must already be produced before time \(t\), or there must not be any user of \(v\) before the next production of \(n_{t}\).

An important point to note is that the mutations 1 and 2 are inverses of each other, meaning that to undo a change, one simply needs to reapply the inverse mutation.

#### 2.2.1 Updating peak memory and objective

In the following discussion, we consider the update of \(f(S)\) by mutation 1 and 2. Since the objective function \(f(S)\) often depends on both memory usage \(M(S)\) and the overall cost \(C(S)=_{t}c(n_{t})\), we assume this is still the case here. We can easily update \(C(S)\) by adding or subtracting the corresponding cost of a newly added or removed node from \(S\). Updating \(M(S)\) is more challenging; to calculate the maximum peak memory consumption when a node is added or removed from \(S\), we have to calculate the aggregated memory consumption by the values with overlapping lifetimes.

The lifetime of inputs and outputs change when adding or removing node \(n\) (see Figure 5 in Appendix for illustration). However, the insertion and deletion of a node only slightly modify the life intervals for each node's input and output values. Consider a value \(v\) with life intervals \(L(v):=\{(t_{i},t_{j})\}\) determined by \(L_{S}(v,t)\). Suppose node \(n\) is inserted at time \(t\), and \(v\) is an input of \(n\). We update the life intervals of \(v\) as follows: If there is \((t_{i},t_{j}) L(v)\) such that \(t_{i} t t_{j}\), do nothing. Otherwise, take a \((t_{i},t_{j}) L(v)\) such that \(t_{j} t\) is maximum (such \(t_{j}\) exists if the sequence after insertion is valid). Update \(L(v) L(v)(t_{i},t_{j})(t_{i},t)\). Similar rules apply for updating the life intervals of the outputs of the inserted node and the inputs and outputs of the node to remove. Specifically, on inserting or deleting a node, the update of life intervals for an input or output value applies to one of the following four cases:

1. No update.
2. Extend or shrink a range \((t_{L},t_{R})\) to \((t_{L},t_{R^{}})\) or \((t_{L^{}},t_{R})\).
3. Split a range \((t_{L},t_{R})\) into \((t_{L},t_{R^{}})\) and \((t_{L^{}},t_{R})\).
4. Merge two ranges \((t_{L},t_{R^{}})\) and \((t_{L^{}},t_{R})\) into \((t_{L},t_{R})\).

Because the update of live intervals involves adding or subtracting the value size \(s(v)\) from a certain range for each input or output value \(v\) (as depicted in Figure 5 in Appendix), we can efficiently maintain memory usage and calculate the peak memory using a segment tree. This data structure allows for range-max and range-sum queries to be performed in \(O( T)\) time, as introduced in Section 2.1.1. The segment tree maintains memory usage for time \(t\) in an internal array \(A[t]\), and peak memory can be determined efficiently by taking the maximum value in the range \([0,T)\) in \(O( T)\) time. To be specific, we add \(s(v)\) for the extended range and subtract \(s(v)\) for the removed range when updating the live intervals. The segment tree significantly contributes to the simulated annealing as it can reduce the time for the differential update of a lifetime interval from the naive \(O(T)\) time to \(O( T)\) time.

#### 2.2.2 Improving memory reduction by grouping

To determine an optimal recomputation sequence, it may be necessary to replicate specific node patterns to rematerialize a larger value. However, achieving this using random insertion and deletion of nodes is challenging. To address this issue, we introduce the concept of _grouped nodes_, which concatenates multiple nodes in a series to enable the recomputation of a series of nodes. A grouping node, represented by \(g\), is formed by concatenating two nodes, \(n_{1}\) and \(n_{2}\), and has the following properties:

* \(c(g)=c(n_{1})+c(n_{2})\)
* \((g)=(n_{1})((n_{2}) (n_{1}))\)
* \((g)=((n_{1})(n_{2})) (n_{2})\)Grouped nodes can be especially useful for sequenced patterns where \(n_{1}\) must be computed immediately before \(n_{2}\), as grouping these nodes together can make the sequence more likely to converge during simulated annealing. Additionally, we observed that after conducting annealing with grouped nodes, further improvement can be achieved by decomposing the grouped node and performing another round of annealing with lower temperatures. In Appendix C.1, we provide further discussion on the benefits of using node grouping for optimization.

### Other Considerations

Conceptually, recomputation enables to optimize the time-space tradeoff without changing the graph semantics, i.e., the output values of the neural network. However, in real models, certain operators with internal states or side effects may produce different output values if recomputed or if their computation order is changed. Our algorithm can handle these cases by simply prohibiting the node addition or removal for these nodes.

An additional way to reduce memory usage that can be easily incorporated into our recomputation algorithm is offloading, which involves moving tensors from GPU memory to host memory when they are not immediately needed, and then moving them back before performing the dependent operation. Our algorithm can be extended to support offloading, as outlined in Appendix B.3. Supporting offloading together with recomputation offers the potential for even greater reductions in memory usage, particularly for larger computational graphs.

## 3 Experiments

In this section, we present the results of our experiments with the recomputation algorithm. Our algorithm was integrated into the PyTorch framework, and we used it to optimize the internal computational graph of various popular models, including vision and text architectures.

### Configuration

Model and input dataOur experiments involved the latest vision models and vision transformers obtained from timm (PyTorch Image Models), as well as text models (including language models) from Hugging Face transformers. To obtain the full computation graph with backpropagation, the vision models were set up for image classification, and model variants with sequence classification heads for the text models. The computational graphs were obtained by PyTorch's symbolic tracing. The value sizes are estimated from shapes and data types. Since node cost estimations were not available in symbolic tracing, all nodes were estimated to have unit costs. For memory budgets, we used the 50% and 25% values of the simulated initial peak memory. For additional details on the environment, PyTorch integration, hyperparameters of the models (e.g., batch sizes, sequence length in transformers), and the effect of using simulated costs instead of actual ones, please refer to Appendix D.

Objective function and hyperparamters of SAAs outlined in Section 2, we utilized an objective function of the form \(f(S)=(,M(S)) C(S)\) for our SA algorithm unless mentioned otherwise2. This objective function seeks to minimize \(C(S)\) once the memory budget is met, and until that point, it minimizes both memory and cost. We included cost in the memory minimization objectives to address the risk of large numbers of nodes, which can result in slower convergence. This objective function is continuous and independent of the cost or size units, and it works effectively for a broad range of practical models.

To ensure efficient convergence, we utilized the first SA on grouped nodes for at most 20 million iterations, or until the memory budget was met, whichever occurred first. The second SA ran for a fixed 2 million iterations for the purpose of cost reduction as detailed in section 2.2.2.

CheckmateTo the best of our knowledge, the most powerful recomputation planner currently available is Checkmate . We re-implemented Checkmate for PyTorch. However, Checkmatecould not find any feasible solution within 6 hours for all models with more than 500 nodes due to the size of the integer linear programming (MILP) to be solved. Therefore, for the comparison with large models, we resort to Checkmate LP, where the MILP was relaxed to linear programming (LP). For clarification, the original version is referred as Checkmate MILP. Gurobi  is used as the internal solver for Checkmate MILP together with PDLP solver, provided by OR-Tools  for Checkmate LP3.

MococassinAnother alternative to perform recomputation is Moccasin . Similarly to Checkmate, Moccasin uses constant programming (CP) to solve the recomputation problem. It introduces a new hyperparameter that acts as an upper-bound on how many times a value can be recomputed. Thanks to this limitation, the number of integer variables involved in the CP set is linear to the number of nodes in the graph, allowing a faster execution time than Checkmate. However, the possible solutions that Moccasin can converge to are heavily limited by this hyperparameter value and the achievable memory reductions can be sub-optimal.

### Memory reduction on large models

Figure 4 provides a comparison between FastSA and Checkmate LP in terms of simulated memory reduction and cost increase. We selected the most representative models for this figure, in addition to the geometric mean of all 23 models tested in Appendix D.7. Checkmate LP failed to find solutions for 11 models due to time limits or out-of-memory errors of 100 GiB, so the geometric mean is not reported. Moreover, memory usage was not fully reduced to the budget due to LP relaxation and randomized rounding, which resulted in larger overhead for some models, such as ViT (small); we discuss this behavior in Appendix D.6.

For the 50% memory budgets, FastSA algorithm was successful in meeting the target budget for all models, with an average increase in model execution time of 7%. However, for the 25% budges, FastSA could not reduce memory to the budget for some models due to either limitations on the model itself or convergence to a suboptimal solution due to SA heuristic limitations. Despite this, the algorithm reduced memory usage by an average of 73% with an average increase in overhead of 18%. Overall, these results demonstrate the superior performance of FastSA compared to Checkmate LP in reducing memory usage, particularly for larger neural networks.

A comparison with Moccasin was also conducted by applying FastSA to the publicly available data in . Table1 presents the results from the original Moccasin study, extended to include outcomes for FastSA. The experiments pertain to two graph variants, namely random-layered (RL) and graphs mentioned in  (CM). For the RL graphs, FastSA achieved the lowest recomputation overhead

Figure 4: Comparison of simulated memory decrease and cost increase by recomputation. For each model, the memory budget is set to 50% and 25% of the simulated initial peak memory (darker bars represent the 25% budget and light bars represent 50% budget). FastSA AVE represents the geometric mean for all 23 models by FastSA. Checkmate LP’s average bar is not shown since failed to solve 11 out of 23 model instances.

in all instances. Remarkably, for RL1 and RL2 cases, FastSA managed to decrease memory usage without adding new recomputation nodes, through the optimization of the execution's topological ordering.

### Solution optimality

We conducted a study on how closely FastSA solutions match with the optimal solution by comparing its results with those of Checkmate MILP on small models. The results, compiled in Table 2, show that for the model VGG11 under the strictest budget constraint (80%), FastSA located the same plan as Checkmate MILP for recomputation. For ResNet18, except for a 50% budget, FastSA managed to cut down memory use under the budget in all cases, but with up to 2.4% more overhead from recomputation compared to Checkmate MILP. Even though FastSA fell short of finding the optimal solutions for these cases, it was up to 1000x faster than Checkmate MILP, particularly

  &  &  &  \\ Graph & \((n,m)\) & B & CI & Mem & Time & CI & Mem & Time & CI & Mem & Time \\   RL 1 & 100 & 90 & 0.8 & 89.2 & 18.5 & 0.8 & 88.1 & 9.3 & **0.0** & 79.4 & 11.7 \\  & 236 & 80 & 2.3 & 79.5 & 22.7 & 2.3 & 79.5 & 9.5 & **0.3** & 79.4 & 11.5 \\  & 70 &  &  &  & 77.3 & 12.0 \\ RL 2 & 250 & 90 & 0.9 & 90.0 & 685.1 & 0.9 & 89.8 & 55.0 & **0.0** & 77.3 & 15.1 \\  & 944 & 80 & time limit exceeded & 4.9 & 80.0 & 639.5 & **0.0** & 72.2 & 15.0 \\  & 70 &  &  &  & 68.9 & 14.5 \\ RL 3 & 500 & 90 & time limit exceeded & 0.7 & 90.0 & 1803.3 & **0.03** & 87.4 & 21.0 \\  & 2461 & 80 & time limit exceeded & 3.4 & 80.0 & 1804.8 & **2.3** & 78.6 & 20.9 \\  & 70 &  &  &  & (73.5) & 21.2 \\ RL 4 & 1000 & 90 & time limit exceeded & 0.7 & 90.0 & 3612.9 & **0.4** & 87.5 & 36.0 \\  & 5857 & 80 & time limit exceeded & 3.4 & 80.0 & 3611.8 & **2.5** & 78.4 & 36.3 \\  & 70 &  &  &  & 70.0 & 36.5 \\  CM 1 & 73 & 90 & 0.0 & 88.4 & 6.3 & 0.0 & 88.4 & 3.1 & **0.1** & 75.3 & 9.4 \\  & 149 & 80 & 0.1 & 76.9 & 5.6 & 0.1 & 78.9 & 3.1 & **0.1** & 75.1 & 9.7 \\  & 70 &  &  &  & 62.3 & 9.7 \\ CM 2 & 353 & 90 & 0.1 & 89.0 & 434.1 & 0.2 & 89.9 & 65.2 & **0.2** & 86.2 & 10.9 \\  & 751 & 80 & 0.3 & 79.7 & 485.3 & 0.3 & 80.0 & 69.3 & **0.4** & 76.6 & 10.9 \\  & 70 &  &  &  & 66.8 & 11.0 \\ 

Table 1: **Comparison with Moccasin. The table includes results of Checkmate MILP and Moccasin from , extended with FastSA data. The columns B and CI represent memory budget and cost increase percentage, respectively. Alongside original results for 90% and 80% budgets, a 70% budget row only demonstrating FastSA results is also inserted. For all random-layered (RL) cases, FastSA exhibited the smallest CI, as it optimizes topological ordering, reducing memory without materialization.**

  &  &  \\ Model & \((n,m)\) & B & CI & Mem & Time & CI & Mem & Time \\  VGG11 & (69, 119) & 90 & 1.4 & 87.6 & 5.1 & 2.9 & 87.5 & 2.4 \\  & 80 & 2.9 & 79.8 & 1.5 & 2.9 & 79.8 & 2.4 \\  & 70 &  &  & (79.8) & 2.4 \\  ResNet18 & (171, 437) & 90 & 0.6 & 85.7 & 47.6 & 0.6 & 85.7 & 2.1 \\  & 80 & 1.8 & 78.6 & 45.3 & 2.9 & 75.0 & 2.1 \\  & 70 & 2.3 & 67.9 & 367.3 & 4.7 & 66.1 & 2.1 \\  & 60 & 4.1 & 57.1 & 18under strict memory budget scenarios. In certain situations where the computational graph has high topological freedom, Checkmate might present suboptimal recomputation plans (discussed in Appendix A.1). This can be found in the results for RL graphs in Table 1, where FastSA could find better solutions than Checkmate MILP or Moccasin.

## 4 Related Work

### Model Parallelism

One common approach to scaling a single neural network that is limited by memory is to partition it into multiple devices using either Model Parallelism or Pipeline Parallelism. Model Parallelism  involves horizontally splitting the neural network by distributing the parameters of each layer across multiple GPU devices, while Pipeline Parallelism  proposes to vertically split the network by assigning each device several contiguous layers of the model. While these approaches enable deep neural networks to be trained at scale, their performance can be limited by the communication overhead required for each iteration of the model. Rajbhandari et al.  found that these approaches can achieve only 5% of the performance peak of a V100 device, highlighting the limitations of these techniques. These limitations have spurred further research into improving neural network scaling, which is discussed in more detail below.

### Recomputation

Recomputation is a technique that was first introduced in classical compiler research to minimize the number of required registers, and later on was adapted for use in deep neural networks (DNNs) by Chen et al.  as a means of reducing memory consumption during training of sequential models. However, this method is limited to sequential graphs and disregards node costs. Kusumoto et al.  proposed dynamic programming algorithms for more general computational graphs. Also, Kumar et al.  leverages tree decomposition to handle more general graph structures. However, these methods still requires large computational overhead, making them impractical for larger networks.

Jain et al.  formulated recomputation as a mixed integer linear programming (MILP) problem and proposed Checkmate, a solver to find an optimal recomputation plan. Checkmate tries to minimize the total computational cost under memory budget and dependency constraints. Although Checkmate has shown to significantly outperform existing methods in terms of solution quality, it requires substantial computational resources to solve the MILP. The number of decision variables in the MILP scales quadratically to the graph size and time scales exponentially to it. To address the limitation of Checkmate, Bartan et al.  proposed Moccasin, which formulates recomputation using constraint programming (CP). The number of integer variables is reduced from quadratic to linear in Moccasin, by setting a hyperparamter for the maximum times a value can be recomputed, thus expediting execution. However, this formulation also narrows the search space and may impact overall quality of the recomputation plans when compared to Checkmate or FastSA. Also, in a parallel development, Rockmate  was proposed for models with repeated layers. It decomposes the problem of recomputation into intra-layer and inter-layer recomputation. It applies Checkmate to a single layer, which forms a smaller graph than the entire model, and then finds the recomputation plan across the layers by Rotor , a dynamic programming based recomputation algorithm that works for sequential models.

In contrast, our proposed method converges to solutions that are comparable or even better than Checkmate and Moccasin with a single CPU core, taking less than four seconds on average. This significant reduction in computational time is achieved by leveraging efficient heuristics and optimization techniques. Our results demonstrate that our approach has great potential in the context of real-world applications, especially for cases where large computational resources are not available.

### Other techniques for memory reduction

There are several other techniques that have been proposed to reduce device memory consumption in addition to recomputation, including offloading, which involves transferring some of the model parameters to a system's CPU or an external memory device when they are not immediately required. Beaumont et al.  discuss the combination of offloading and recomputation as a means of further reducing memory utilization. Our proposed method can easily be extended to support offloading, as detailed in appendix B.3. Another approach that leverages offloading is the ZeRO technique proposed by Rajbhandari et al. , which partitions the model parameters, optimizer states, and activations among several devices to increase parallelism and reduce the memory used by each device. This approach enables the training of exceptionally large models with hundreds of billions of parameters, making it a powerful tool for advanced natural language processing and computer vision applications. Other techniques focus on reducing the size of the parameters and intermediate results, such as quantization , which reduces the floating point precision of computations and weight storage up to 2-bits in extreme cases. Sparsification  exploits the sparsity patterns that arise during computation to reduce the total needed memory while keeping the same precision. There are also more exotic approaches, such as the reversible residual network , which is a memory-efficient architecture that can perform backward computation without saving the activations. However, its applicability is limited to residual networks only. It is worth noting that these techniques are largely orthogonal to our proposed method and can be combined to further improve memory savings in neural network optimization.

## 5 Conclusion

In this paper, we present a novel method for recomputation that offers several key advantages over existing approaches. Our method is applicable to general graphs and can support any objective function that depends on peak memory usage and the total cost of computation. Moreover, it can find near-optimal recomputation plans within a remarkably short computational time of 3 to 30 seconds, even for large computational graphs.

Another major advantage of our method is its efficiency, as it is single-threaded and uses memory resources in a highly efficient manner. This makes it ideal for integration into neural network compilers, where it can further streamline the optimization process. Additionally, our algorithm is highly flexible and can handle a wide range of problem settings, including recomputation with offloading and consideration of node-intermediate memory usage.

We explored the tradeoff between computation time and memory usage and evaluated the effectiveness of our algorithm in terms of reducing peak memory usage. Our experiments demonstrate that our approach can achieve significant reductions in peak memory usage, with reduced impact on computation time, for a wide range of neural network architectures. Overall, our experiments validate the effectiveness and practicality of our recomputation algorithm, highlighting its potential for widespread application in many areas of deep learning research and development.

LimitationsIn conclusion, while our algorithm offers a powerful solution for complex neural network optimization problems, it has certain limitations that must be considered. As demonstrated in Figure 4, our approach was able to reduce memory usage more significantly than Checkmate LP in most cases, but the result was not ideal for some models due to suboptimal node grouping.

When planning recomputation in distributed training, the computational graph may need to have extra requirements such as (1) the optimized graph must be able to split equally for each worker in Pipeline Parallel or (2) the communication operators must be overlapped with arithmetic computations. Currently, it is difficult to handle these constraints with the standard FastSA.

Additionally, the peak memory estimation may not be always precise due to memory aliasing, caused by operators that change the memory views or do inplace updates. This is one of the reason why there is a gap between the simulated and actual GPU memory usage as shown in Appendix D.8.

Overall, our proposed approach offers a fast and efficient method for neural network optimization, providing significant improvements over existing techniques. However, future research may be required to support a wider variety of computation.