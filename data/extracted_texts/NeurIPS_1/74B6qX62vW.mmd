# Sample-Efficient Private Learning of Mixtures of Gaussians

Hassan Ashtiani

McMaster University

zokaeiam@mcmaster.ca

&Mahbod Majid

MIT

mahbod@mit.edu

&Shyam Narayanan

Citadel Securities

shyam.s.narayanan@gmail.com

Work done as a student at MIT

###### Abstract

We study the problem of learning mixtures of Gaussians with approximate differential privacy. We prove that roughly \(kd^{2}+k^{1.5}d^{1.75}+k^{2}d\) samples suffice to learn a mixture of \(k\)_arbitrary_\(d\)-dimensional Gaussians up to low total variation distance, with differential privacy. Our work improves over the previous best result  (which required roughly \(k^{2}d^{4}\) samples) and is provably optimal when \(d\) is much larger than \(k^{2}\). Moreover, we give the first optimal bound for privately learning mixtures of \(k\)_univariate_ (i.e., \(1\)-dimensional) Gaussians. Importantly, we show that the sample complexity for learning mixtures of univariate Gaussians is linear in the number of components \(k\), whereas the previous best sample complexity  was quadratic in \(k\). Our algorithms utilize various techniques, including the _inverse sensitivity mechanism_, _sample compression for distributions_, and methods for bounding volumes of sumsets.

## 1 Introduction

Learning **Gaussian Mixture Models** (GMMs) is one of the most fundamental problems in algorithmic statistics. Gaussianity is a common data assumption, and the setting of Gaussian mixture models is motivated by heterogeneous data that can be split into numerous clusters, where each cluster follows a Gaussian distribution. Learning mixture models is among the most important problems in machine learning , and is at the heart of several unsupervised and semi-supervised machine learning models. The study of Gaussian mixture models has had numerous scientific applications dating back to the 1890s , and is a crucial tool in modern data analysis techniques in a variety of fields, including bioinformatics , anomaly detection , and handwriting analysis .

In this work, we study the problem of learning a GMM from samples. We focus on the _density estimation_ setting, where the goal is to learn the overall mixture distribution up to low total variation distance. Unlike the _parameter estimation_ setting for GMMs, density estimation can be done even without any boundedness or separation assumptions on the parameters of the components. In fact, it is known that mixtures of \(k\) Gaussians in \(d\)-dimensions can be learned up to total variation distance \(\) using \((kd^{2}/^{2})\) samples .

Ensuring data privacy has emerged as an increasingly important challenge in modern data analysis and statistics. _Differential privacy_ (DP)  is a rigorous way of defining privacy, and is considered to be the gold standard both in theory and practice, with deployments by Apple , Google , Microsoft , and the US Census Bureau . As is the case for many data analysis tasks, standard algorithms for learning GMMs leak potentially sensitive information about the individuals who contributed data. This raises the question of whether we can do density estimation for GMMs under the constraint of differential privacy.

Private density estimation for GMMs with unrestricted Gaussian components is a challenging task. In fact, privately learning a single unrestricted Gaussian has been the subject of multiple recent studies , AL22, KMV22, AKT\({}^{+}\)23, HKMN23]. Private learning of GMMs is significantly more challenging, because even without privacy constraints, parameter estimation for GMMs require exponentially many samples in terms of the number of components . Therefore, it is not clear how to use the typical recipe of "adding noise" to the estimated parameters or "privately choosing" from the finite-dimensional space of parameters. Consequently, the only known sample complexity bounds for privately learning unrestricted GMMs are loose .

Let us first formally define the problem of learning GMMs. We represent a GMM \(=_{i=1}^{k}w_{i}(_{i},_{i})\) by its parameters, namely \(\{(w_{i},_{i},_{i})\}_{i=1}^{k}\), where \(w_{i} 0\), \(_{i}w_{i}=1\), \(_{i}^{d}\), and \(_{i}\) is a positive definite matrix. In the following, a GMM learning algorithm \(\) receives a set of data points in \(^{d}\) and outputs a (representation of) a GMM. The total variation distance between two distributions is \(_{}(},)=_{ ^{d}}|(x)-}(x)|dx^{2}\).

**Definition 1.1** (Learning GMMs).: For \(,(0,1)\), we say \(\) learns GMMs with \(n\) samples up to accuracy \(\) and failure probability \(\) if for every GMM \(\), given samples \(X_{1},,X_{n}}{{}}\), it outputs (a representation of) a GMM \(}\) such that \(_{}(},)\) with probability at least \(1-\).

\(\) and \(\) are called the accuracy and failure probability, respectively. For clarity of presentation, we will typically fix the value of \(\) (e.g., \(=1/3\)). The above definition does not enforce the constraint of differential privacy. The following definitions formalizes (approximate) differential privacy.

**Definition 1.2** (Differential Privacy (DP) ).: Let \(, 0\). A randomized algorithm \(:^{n}\) is said to be \((,)\)-differentially private (\((,)\)-DP) if for any two neighboring datasets \(,^{}^{n}\) and any measurable subset \(O\),

\[[(^{}) O] e^{} [() O]+.\]

If the GMM learner \(\) of Definition 1.1 is \((,)\)-DP, we say that \(\) privately learns GMMs. Formally, we have the following definition.

**Definition 1.3** (Privately learning GMMs).: Fix the number of samples \(n\), dimension \(d\), and number of mixture components \(k\). For \(,(0,1)\) and \(, 0\), a randomized algorithm \(\), that takes as input \(X_{1},,X_{n}^{d}\), \((,)\)-privately learns GMMs up to accuracy \(\) and failure probability \(\), if:

1. For any GMM \(\) that is a mixture of up to \(k\) Gaussians in \(d\) dimensions, if \(=\{X_{1},,X_{n}\}}{{}} \), \((X_{1},,X_{n})\) outputs a GMM \(}\) such that \(_{}(},)\) with probability at least \(1-\) (over the randomness of the data \(\) and the algorithm \(\)).
2. For _any_ neighboring datasets \(,^{}(^{d})^{n}\) (not necessarily drawn from any GMM) and any measurable subset \(O\), \([(^{}) O] e^{} [() O]+\).

Finally, we assume a default value for \(\) of \(1/3\), meaning that if not stated, the failure probability \(\) is assumed to equal \(1/3\).

Our main goal in this paper is to understand the number of samples (as a function of the dimension \(d\), the number of mixture components \(k\), the accuracy \(\), and the privacy parameters \(,\)) that are needed to privately and accurately learn the GMM up to low total variation distance.

### Results

In this work, we provide improved sample complexity bounds for privately learning mixtures of arbitrary Gaussians, improving over previous work of . Moreover, our sample complexity bounds are optimal in certain regimes, when the dimension is either \(1\) or a sufficiently large polynomial in \(k\) and \(\). For general dimension \(d\), we prove the following theorem.

**Theorem 1.4**.: _For any \(,,(0,1),k,d\), there exists an inefficient \((,)\)-DP algorithm that can learn a mixture of \(k\) arbitrary full-dimensional Gaussians in \(d\) dimensions up to accuracy \(\), using the following number of samples:_

\[n=(}{^{2}}++d^{1.75}k^{1.5}^{0. 5}(1/)+k^{1.5}^{1.5}(1/)}{}+d}{ }).\]Notably, the mixing weights and the means can be arbitrary and the covariances of the Gaussians can be arbitrarily poorly conditioned, as long as the covariances are non-singular3.

We remark that we omit the dependence on \(\) (and assume by default a failure probability of \(1/3\)). However, it is well-known that one can obtain failure probability \(\) with only a multiplicative \(O( 1/)\) blowup in sample complexity, in a black-box fashion4. In fact, our analysis can yield even better dependencies on \(\) in some regimes, though to avoid too much complication, we do not analyze this.

For reasonably large dimension, i.e., \(d k^{2}^{2}(1/)\), this can be simplified to \((}{^{2}}+}{})\), which is in fact optimal (see Theorem 1.6). Hence, we obtain the _optimal_ sample complexity for sufficiently large dimension. Theorem 1.4 also improves over the previous best sample complexity upper bound of , which uses

\[(d^{4}+kd^{2}(1/)}{^{2} }+}+d^{2}} {^{4}})\]

samples. Our results provide a polynomial improvement in all parameters, but to simplify the comparison, if we ignore dependencies in the error parameter \(\) and privacy parameters \(,\), we improve the sample complexity from \(k^{2}d^{4}\) to \(kd^{2}+k^{2}d+k^{1.5}d^{1.75}\): note that our result is quadratic in the dimension whereas  is quartic.

When the dimension is \(d=1\), we can provide an improved result, which is _optimal_ for learning mixtures of univariate Gaussians (see Theorem 1.6 for a matching lower bound).

**Theorem 1.5**.: _For any \(,,(0,1),k\), there exists an inefficient \((,)\)-DP algorithm that can learn a mixture of \(k\) arbitrary univariate Gaussians (of nonzero variance) up to accuracy \(\), using the following number of samples:_

\[n=(}+).\]

For privately learning mixtures of univariate Gaussians, the previous best-known result for arbitrary Gaussians required \((^{3/2}(1/)}{^{2}})\) samples . Importantly, we are the first paper to show that the sample complexity can be _linear_ in the number of components.

Our work purely focuses on sample complexity, and as noted in Theorems 1.4 and 1.5, they do not have polynomial time algorithms. We note that the previous works of  also do not run in polynomial time. Indeed, there is reason to believe that even non-privately, it is impossible to learn GMMs in polynomial time (in terms of the optimal sample complexity) .

Finally, we prove the following lower bound for learning GMMs in any fixed dimension \(d\).

**Theorem 1.6**.: _Fix any dimension \(d 1\) number of components \(k 2\), any \(,\) at most a sufficiently small constant \(c^{*}\), and \((/d)^{O(1)}\). Then, any \((,)\)-DP algorithm that can learn a mixture of \(k\) arbitrary full-dimensional Gaussians in \(d\) dimensions up to total variation distance \(\), with probability at least \(2/3\), requires at least the following number of samples:_

\[(}{^{2}}+}{ }+).\]

Note that for \(d=1\), this matches the upper bound of Theorem 1.5, thus showing that our univariate result is near-optimal in all parameters \(,,\). Moreover, our lower bound refutes the conjecture of , which conjectures that only \((}++)\) samples are needed in the univariate case and \((}{^{2}}+}{}+)\) samples are needed in the \(d\)-dimensional case. However, we note that our lower bound asymptotically differs from the conjectured bound in  only when \(\) is extremely small.

### Related work

In the non-private setting, the sample complexity of learning unrestricted GMMs with respect to total variation distance (a.k.a. density estimation) is known to be \((kd^{2}/^{2})\), where the upper bound is obtained by the so-called distributional compression schemes.

In the private setting, the only known sample complexity upper bound for unrestricted GMMs  is roughly \(k^{2}d^{4}(1/)/(^{4})\), which exhibits sub-optimal dependence on various parameters5. This bound is achieved by running multiple non-private list-decoders and then privately aggregating the results. For the special case of axis-aligned GMMs, an upper bound of \(k^{2}d(1/)^{3/2}/(^{2})\) is known . These are the only known results even for privately learning (unbounded) univariate GMMs. In other words, the best known upper bound for sample complexity of privately learning univariate GMMs has quadratic dependence on \(k\).

In the related public-private setting , it is assumed that the learner has access to some public data. In this setting,  show that unrestricted GMMs can be learned with a moderate amount of public and private data.

Assuming the parameters of the Gaussian components (and the condition numbers of the covariance matrices) are bounded, one can create a cover for GMMs and use private hypothesis selection  or the private minimum distance estimator  to learn the GMM. On the flip side,  prove a lower bound on the sample complexity of learning GMMs, though their lower bound is weaker than ours and is only against pure-DP algorithms.

The focus of our work is on density estimation. A related problem is learning the parameters a GMM, which has received extensive attention in the (non-private) literature (e.g.,  among many other papers). To avoid identifiability issues, one has to assume that the Gaussian components are sufficiently separated and have large-enough weights. In the private setting, the early work of  demonstrated a privatized version of  for learning GMMs with fixed (known) covariance matrices. The strong separation assumption (of \((k^{1/4})\)) between the Gaussian components in  was later relaxed to a weaker separation assumption . A substantially more general result for privately learning GMMs with unknown covariance matrices was established in , based on a privatized version of . Yet, this approach also requires a polynomial separation (in terms of \(k\)) between the components, as well as a bound on the spectrum of the covariance matrices.  weakened the separation assumption of  and improved over their sample complexity. This result is based on a generic method that learns a GMM using a private learner for Gaussians and a non-private clustering method for GMMs. Finally,  designed an efficient reduction from private learning of GMMs to its non-private counterpart, removing the boundedness assumptions on the parameters and achieving minimal separation (e.g., by reducing to ). Nevertheless, unlike density estimation, parameter estimation for unrestricted GMMs requires exponentially many samples in terms of \(k\).

A final important question is that of _efficient_ algorithms for learning GMMs. Much of the work on learning GMM parameters focuses on computational efficiency (e.g., ), as does some work on density estimation (e.g., ). However, under some standard hardness assumptions, it is known that even non-privately learning mixtures of \(k\)\(d\)-dimensional Gaussians with respect to total variation distance cannot be done in polynomial time as a function of \(k\) and \(d\).

Addendum.In a concurrent submission,  extended the result of  for learning unrestricted GMMs to the agnostic (i.e., robust) setting. In contrast, our algorithm works only in the realizable (non-robust) setting. Moreover,  slightly improved the sample complexity result of  from \(((1/)k^{2}d^{4}/(^{4}))\) to \(((1/)k^{2}d^{4}/(^{2}))\). The sample complexity of our approach is still significantly better than  in terms of all parameters--similar to the way it improved over .

Technical overview and roadmap

We highlight some of our conceptual and technical contributions. We mainly focus on the high-dimensional upper bound, and discuss the univariate upper bound at the end.

### Reducing to crude approximation

Suppose we are promised a bound on the means and covariances of the Gaussian components, i.e., \( I_{i} R I\) and \(\|_{i}\|_{2} R\) for all \(i[k]\). In this case, there is in fact a known algorithm, using private hypothesis selection , that can privately learn the distribution using only \(O( R}{^{2}}+ R}{})\) samples. Moreover, with a more careful application of the hypothesis selection results (see Appendix D), we can prove that result holds even if \((_{i},_{i})\) are possibly unbounded, but we have some very crude approximation. By this, we mean that if for each \(i[k]\) we know some \(_{i}\) such that \(_{i}_{i} R_{i}\), then it suffices to have \(n=O( R}{^{2}}+ R}{})\) samples to learn the full GMM in total variation distance.

Our main goal will be to learn every covariance \(_{i}\) with such an approximation, for \(R=(k,d,,)\), so that \( R\) can be hidden in the \(\) factor. To explain why this goal is sufficient, suppose we can crudely learn every covariance \(_{i}\) with approximation ratio \(R\), using \(n^{}\) samples. We then need \(O( R}{^{2}}+ R}{} )=(}{^{2}}+}{})\) additional samples to learn the full distribution using hypothesis selection, so the total sample complexity is \((n^{}+ R}{^{2}}+ R}{ })\). Hence, we will aim for this easier goal of crudely learning each covariance, for both Theorem 1.4 and Theorem 1.5, using as few samples \(n^{}\) as possible. We will also need to approximate each mean \(_{i}\), though for simplicity we will just focus on covariances in this section.

### Overview of Theorem 1.5 for univariate GMMs

The main goal will be to simply provide a rough approximation to the set of standard deviations \(_{i}=}\), as we can finish the procedure with hypothesis selection, as discussed above.

Bird's eye view:Say we are given a dataset \(=\{X_{1},,X_{n}\}\): note that every \(X_{j}\) since we are dealing with univariate Gaussians. The main insight is to sort the data in increasing order (i.e., reorder so that \(X_{1} X_{2} X_{n}\)) and consider the unordered multiset of successive differences \(\{X_{2}-X_{1},X_{3}-X_{2},,X_{n}-X_{n-1}\}\). One can show that if a single datapoint \(X_{j}\) is changed, then the set of consecutive differences (up to permutation) does not change in more than \(3\) locations (see Lemma F.5 for a formal proof).

We then apply a standard private histogram approach. Namely, for each integer \(a\), we create a corresponding bucket \(B_{a}\), and map each \(X_{j+1}-X_{j}\) into \(B_{a}\) if \(2^{a} X_{j+1}-X_{j}<2^{a+1}\). If some mixture component \(i\) had variance \(_{i}=_{i}^{2}\), we should expect a significant number of \(X_{j+1}-X_{j}\) to at least be crudely close to \(_{i}\), such as for the \(X_{j}\) drawn from the \(i^{}\) mixture component. So, some corresponding bucket should be reasonably large. Finally, by adding noise to the count of each bucket and taking the largest noisy counts, we will successfully find an approximation to all variances.

In more detail:For each (possibly negative) integer \(a\) let \(c_{a}\) be the number of indices \(i\) such that \(2^{a} X_{j+1}-X_{j}<2^{a+1}\). We will prove that, if the weight of the \(i^{}\) component in the mixture is \(w_{i}\) and there are \(n\) points, then we should expect at least \((w_{i} n)\) indices \(j\) to be in a bucket \(a\) with \(2^{a}\) within a \((n)\) multiplicative factor of the standard deviation \(_{i}\) (see Lemma F.3). The point of this observation is that there are at most \(O( n)\) buckets \(B_{a}\) with \(2^{a}\) between \(}{(n)}\) and \(O(_{i})\), but there are at least \((w_{i} n)\) indices mapping to one of these buckets. So by the Pigeonhole principle, one of these buckets has at least \(( n}{ n})\) indices, i.e., \(c_{a}( n}{ n})\) for some \(a\) with \(}{(n)} 2^{a} O(_{i})\).

Moreover, we know that if we change a single data point \(X_{j}\), the set of consecutive differences \(\{X_{j+1}-X_{j}\}\) after sorting changes by at most \(3\). So, if we change a single \(X_{j}\), at most \(3\) of the counts \(c_{a}\) can change, each by at most \(3\).

Now, for every integer \(a\), draw a noise value from the _Truncated Laplace_ distribution (see Definition A.2 and Lemma A.3), and add it to \(c_{a}\) to get a noisy count \(_{a}\). The details of the noise distribution are not important, but the idea is that this distribution is _always_ bounded by \(O()\). Moreover, the Truncated Laplace distribution preserves \((,)\)-DP. This means that the counts \(\{_{a}\}_{a}\) will have \((O(),O())\)-DP, because the true counts \(c_{a}\) only change minimally across adjacent datasets.

Our crude approximation to the set of standard deviations will be the set of \(2^{a}\) such that \(_{a}\) exceeds some threshold which is a large multiple of \(\). If \(n()\) and the weight \(w_{i}/k\), it is not hard to verify that \( n}{ n}\) exceeds a large multiple of \(\). So, for each \(i k\) with weight at least \(/k\), some corresponding \(a\) with \(}{(n)} 2^{a} O(_{i})\) will have count \(c_{a}\) significantly exceeding the threshold, and thus noisy count \(_{a}\) exceeding the threshold. This will be enough to crudely approximate the values \(_{i}\) coming from large enough weight. We can ignore any component with weight less than \(/k\), as even if all but one of components have such small weight, together they only contribute \(\) weight. So, we can ignore them and it will only cost us \(\) in total variation distance, which we can afford.

Putting everything together:In summary, we needed \(()\) samples to approximate each covariance (of sufficient weight) up to a \((n)\) multiplicative factor. By setting \(R=(n)\) and using the reduction described in Section 2.1, we then need an additional \(O(}+)\) samples. If we set \(n=(}+)\), we will obtain that \(n()+O( }+),\) which is sufficient to solve our desired problem in the univariate setting.

Note that this proof relies heavily on the use of private histograms and the order of the data points in the real line. Therefore, it cannot be extended to the high-dimensional setting. We will use a completely different approach to prove Theorem 1.4.

### Overview of Theorem 1.4 for general GMMs

As in the univariate case, we only need rough approximations of the covariances. We will learn the covariances one at a time: in each iteration, we privately identify a single covariance \(_{i}\) which crudely approximates some covariance \(_{i}\) in the mixture, with \((/,/k)\)-DP. Using the well-known _advanced composition_ theorem (see Theorem A.1), we will get an overall privacy guarantee of \((,)\)-DP. However, to keep things simple, we will usually aim for \((,)\)-DP when learning each covariance, and we can replace \(\) with \(/\) and \(\) with \(/k\) at the end.

A natural approach for parameter estimation, rather than learn \(_{i}\) one at a time, is to learn all of the covariances together. However, we believe that this approach would cause the sample complexity to multiply by a factor of \(k\), compared to learning a single covariance. The advantage of learning the covariances one at a time is that we can apply advanced composition: this will cause the sample complexity to multiply by roughly \(\) instead.

In the rest of this subsection, our main focus is to identify a single crude approximation \(_{i}\).

Applying robustness-to-privacy:The first main insight is to use the robustness-to-privacy conversion of Hopkins et al.  (see also ). Hopkins et al. prove a black-box (but not computationally efficient) approach that can convert robust algorithms into differentially private algorithms, using the exponential mechanism and a well-designed score function. This reduction only works for finite dimensional parameter estimation and therefore cannot be applied directly to density estimation. On the other hand, parameter estimation for arbitrary GMMs requires exponentially many samples in the number of components . However, we will demonstrate that this lower bound does not hold when we only need a very crude estimation of the parameters.

The idea is the following. For a dataset \(=\{X_{1},,X_{n}\}\), let \(=(,)\) be a _score_ function, which takes in a dataset \(\) of size \(n\) and some candidate covariance \(\), and outputs some non-negative integer. At a high level, the score function \((,)\) will be the smallest number of data points \(t\) that we should change in \(\) to get to some new data set \(^{}\) with a specific desired property: \(^{}\) should "look like" a sample generated from a mixture distribution with \(\) being the covariance of one of its components - namely, a component with a significant mixing weight. One can define "looks like" in different ways, and we will adjust the precise definition later. We remark that this notion of score roughly characterizes robustness, because if the samples in \(\) were truly drawn from a Gaussian mixture model with covariances \(\{_{i}\}_{i=1}^{k}\), we should expect \((_{i},)\) to be 0 (since \(\) should already satisfy the desired property), but if we altered \(k\) data points, the score should be at most \(k\). The high-level choice of score function is somewhat inspired by a version of the exponential mechanism called the _inverse sensitivity mechanism_, though the precise way of defining the score function requires significant care and is an important contribution of this paper.

The robustness-to-privacy framework of , tailored to learning a single covariance, implies the following general result, which holds for any score function \(\) following the blueprint above. For now, we state an informal (and slightly incorrect) version.

**Theorem 2.1** (informal - see Theorem C.3 for the formal statement).: _For any \([0,1)\), and any dataset \(\) of size \(n\), define the value \(V_{}()\) to be the volume (i.e., Lebesgue measure) of the set of covariance matrices \(\) (where the covariance can be viewed as a vector by flattening), such that \((,) n\)._

_Fix a parameter \(<0.1\), and suppose that for any dataset \(\) of size \(n\) such that \(V_{/2}()\) is strictly positive, the ratio of volumes \(V_{}()/V_{/2}()\) is at most some \(K\) (which doesn't depend on \(\)). Then, if \(n\), there is a differentially private algorithm that can find a covariance \(\) of low score (i.e., where \((,) n\)) using \(n\) samples._

Note that Theorem 2.1 does not seem to say anything about whether \(\) comes from a mixture of Gaussians. However, we aim to instantiate this theorem with a score function that is carefully designed for GMMs. Recall that we want \((,)\) to capture the number of points in \(\) that need to be altered to make it look like a data set that was generated from a mixture, with \(\) being the covariance of one of the Gaussian components. In other words, \((,)\) should be small if (and hopefully only if) a "mildly corrupted" version of \(\) includes a subset of points that are generated from a Gaussian with covariance \(\). At the heart of designing such a score function, one needs to come up with a form of "robust Gaussianity tester" that tells whether a given set of data points are generated from a Gaussian distribution. Aside from this challenge, the volume ratio associated with the chosen score function needs to be small for every dataset \(\) otherwise the above theorem would require a large \(n\) (i.e., number of samples). These two challenges are, however, related. If the robust Gaussianity tester has high specificity--i.e., rejects most of the sets that are not generated from a (corrupted) Gaussian--then the volume ratio is likely to be small (i.e., a smaller number of candidates \(\) would receive a good/low score).

First Attempt:We first try an approach which resembles that of  for privately learning a single Gaussian. We "define" \((,)\) as the smallest integer \(t\) satisfying the following property: there exists a subset \(\) of size \(n/k\), such that we can change \(t\) data points from \(\) to get to \(^{}\), where \(^{}\) "looks like" i.i.d. samples from a Gaussian with some covariance \(\) that is "similar to" \(\). The choice of \(\) having size \(n/k\) is motivated by the fact that each mixture component, on average, has \(n/k\) data points in \(\).

The notions of "looks like" and "similar to" of course need to be formally defined. We say \(\) is "similar to" \(\) (or \(\)) if they are spectrally close, i.e., \(0.5 2\). We say that \(^{}\) "looks like" samples from a Gaussian with covariance \(\) if some covariance estimation algorithm predicts that \(^{}\) came from a Gaussian with covariance \(\). The choice of covariance estimation algorithm will be quite nontrivial and ends up being a key ingredient in proving Theorem 1.4.

To apply Theorem 2.1, we first set \(=c/k\) for some small constant \(c\). We cannot set a larger value \(\), because if we change \(t n/k\) data points, we could in fact create a totally arbitrary new Gaussian component with large weight. Since there is no bound on the eigenvalues of the covariance matrix, this could cause the volume \(V_{}()\) to be infinite. The main question we must answer is how to bound the volume ratio \(V_{}()/V_{/2}()\). To answer this question, we first need to understand what it means for \((,) n\). If \((,) n\), then there exists a corresponding set \(\) of size \(n/k\), and we can change \( n=c||\) points from \(\) to get to some \(^{}\) which looks like samples from a Gaussian with covariance \(\). Thus, \(\) looks like \(c\)-corrupted samples from such a Gaussian (i.e.,a \(c\) fraction of the data is corrupted). This motivates using a _robust_ covariance estimation algorithm: indeed, robust algorithms can still approximately learn \(\) even if a small constant fraction of data is corrupted, so for any \(\), we expect that no matter how we change a \(c\) fraction of \(\) to obtain \(^{}\), the robust algorithm's covariance estimate should not change much. So, for any fixed \(\), the set of possible \(\), and thus the set of possible \(\), should not be that large.

In summary, to bound \(V_{}()\) versus \(V_{/2}()\), there are at most \(\) choices for \(\) in the former case, and at least \(1\) choice in the latter case (since we assume \(V_{/2}()>0\) in Theorem 2.1). Moreover, for any such \(\), the volume of corresponding \(\) should be exponential in \(d^{2}\) (either for \(V_{}()\) or \(V_{/2}()\)), since the dimensionality of the covariance is roughly \(d^{2}\). So, this suggests that the overall volume ratio is at most \( e^{O(d^{2})}\). Since \((n/k) k\), if we plug into Theorem 2.1 it suffices to have \(n}{e^{(c/k)}}\). Unfortunately this is impossible unless \( k\).

These ideas will serve as a good starting point, though we need to improve the volume ratio analysis. To do so, we also modify the robust algorithm, by strengthening the assumptions on what it means for samples to "look like" they came from a Gaussian.

Improvement via Sample Compression:To improve the volume ratio, we draw inspiration from a technique called _sample compression_, which has been used in previous work on non-private density estimation for mixtures of Gaussians [ABH\({}^{+}\)18, ABDH\({}^{+}\)20]. The idea behind sample compression is that one does not need the full set \(\) to do robust covariance estimation; instead, we look at a smaller set of samples. For instance, if \(\) looks like \(c\)-corrupted samples from a Gaussian of covariance \(\), we expect that a random subset \(\) of \(\) also looks like \(c\)-corrupted samples from such a Gaussian. Moreover, as long as one uses \(m O(d)\) corrupted samples from a Gaussian, we can still (inefficiently) approximate the covariance. This motivates us to modify the robust algorithm as follows: rather than just checking whether \(\) looks like \(c\)-corrupted samples from a Gaussian of covariance roughly \(\), we also test whether an average subset \(\) of size \(m\) does as well. Therefore, if \(\) has low score, there exists a corresponding set \(\) of size \(m\), and there are only \( e^{m n}\) choices for \(\). So, now it suffices to have \(n}{(c/k)}\), which will give us a bound of \((d^{2}k/)\), as long as \(m O(d^{2})\). Importantly, we still check the robust algorithm on \(\) of size roughly \(n/k\), which allows us to keep the robustness threshold \(\) at roughly \(c/k\).

There is one important caveat that for each subset \(\), there is a distinct corresponding covariance \(\), and the volume of \(\) can change drastically as \(\) changes. (For instance, the volume of \( T\) is \(T^{(d^{2})}\) times as large as the volume of \(\). Since we have no bounds on the possible covariances, \(T\) could be unbounded.) For our volume ratio to actually be bounded by about \( e^{O(d^{2})}\), we want the volume of \(\) to stay invariant with respect to \(\). This can be done by defining a "normalized volume" where the normalization is inversely proportional to the determinant (see Appendix C.1 for more details). The robustness-to-privacy conversion (Theorem 2.1) will still hold.

While the bound of \((d^{2}k/)\) seems good, we recall that this bound is merely the sample complexity for \((,)\)-DP crude approximation of a _single_ Gaussian component. As discussed at the beginning of this subsection, to learn all \(k\) Gaussians, we actually need \((/,/k)\)-DP, rather than \((,)\)-DP, when crudely approximating a single component. This will still end up leading to a significant improvement over previous work , but we can improve the volume ratio even further and thus obtain even better sample complexity.

Improving Dimension Dependence:Previously, we used the fact that the volume of candidate \(\) (corresponding to a fixed \(\)) was roughly exponential in \(d^{2}\) for either \(V_{/2}()\) or \(V_{}()\), so the ratio should also be roughly exponential in \(d^{2}\). Here, we improve this ratio, which will improve the overall volume ratio.

First, we return to understanding the guarantees of the robust algorithm. It is known that, given \(m O(d)\) samples from a Gaussian of covariance \(\), we can provide an estimate \(\) such that \((1-O())(1-O())\). As above, we need to solve this even if a \(c\) fraction of samples are corrupted. While this can cause the relative spectral error to increase from \(1 O()\) to \(1 O(c+)\), for now let us ignore the additional \(c\) factor.

If \(V_{/2}()>0\), then there is some covariance \(\) and some set \(\) of size \(n/k\), where the robust algorithm thinks \(\) looks like (possibly corrupted) Gaussian samples of covariance \(\). So, every \(\) such that \(0.5 2\) has score of at most \(/2 n\). This gives us a lower bound on \(V_{/2}()\). We now want to upper bound \(V_{}()\). If \((,)< n\), we still have that the robust algorithm thinks some \(\) looks like Gaussian samples of covariance \(\), where \(0.5 2\). But now, we use the additional fact that for some \(\) of size \(m\), the robust algorithm on \(\) finds a covariance \(\). By the accuracy of the robust algorithm, \(\) and \(\) should be similar, i.e., \((1-O())(1-O())\) (where we ignored the \(c\) factor). Thus, there exists some \(\) of size \(m\) and a \(\) corresponding to \(\), such that \(0.5(1-O()) 2(1+O( ))\).

Therefore, from \(/2\) to \(\), we have dilated the candidate set of \(\) by a factor of \(1+O()\) in the worst case, and we have at least \(1\) choice in the \(/2\) case but at most \(\) choices in the \(\) case. Thus, the overall volume ratio is at most \((1+O())^{d^{2}}=e^{O(m n+d^{2})}\), since the dimension of \(\) is roughly \(d^{2}\). Consequently, it now suffices to have \(n}{(c/k)}:\) setting \(m=d^{5/3}\) gives us an improved bound of \((d^{5/3}k/)\) for learning a single \(_{i}\).

There are some issues with this approach: most notably, we ignored the fact that the spectral error is really \(c+\) rather than \(\). However, the robust algorithm can do better than just estimating up to spectral error \(c+\): it can also get an improved _Frobenius_ error. While we will not formally state the guarantees on the robust algorithm here (see Theorem B.3 for the formal statement), the main high-level observation is that if the robust estimator \(\) can be \(1 c\) times as large as the true covariance \(\) in only \(O(1)\) directions then for an average direction the ratio of \(\) to \(\) will be \(1 O()\). We can utilize this observation to bound the volume ratio, using some careful \(\)-net arguments (this is executed in Appendix C.3). Our dimension dependence of \(d^{5/3}\) will increase to \(d^{7/4}\), though this still improves over the previous \(d^{2}\) bound.

We will formally define the score function \((,)\) in Appendix E.1 and fully analyze the application of the robustness-to-privacy conversion, as outlined here, in Appendix E.2.

Accuracy:One important final step that we have so far neglected is ensuring that any \(\) of low score must be crudely close to some \(_{i}\), if \(\) is actually drawn from a GMM. We will just focus on the case where \((,)=0\), so some \(\) of size \(n/k\) looks like a set of samples from a Gaussian with covariance \(\). If the samples \(\) all came from the \(i\)-th component of the GMM, then it will not be difficult to show \(\) is similar to \(_{i}\). The more difficult case is when data point in \(\) are generated from several components.

However, if \(n 20k^{2}d\), then \(|| 20kd\), which means that by the Pigeonhole Principle, at least \(20d\) points in \(\) come from the same mixture component \((_{i},_{i})\). We are able to prove that, with high probability over samples drawn from a single Gaussian component \((_{i},_{i})\), that the empirical covariance of any subset of size at least \(20d\) is crudely close to \(_{i}\) (see Corollary B.5). As a result, when verifying that a subset \(\) "looks like" i.i.d. Gaussian samples with covariance \(\), we can ensure that the empirical covariance of every subset \(\) of size \(20d\) is crudely close to \(\). Thus, if the score \((,)=0\), \(\) is close to \(\), which is crudely close to some \(_{i}\).

We also formally analyze the accuracy in Appendix E.2.

Putting everything together:To crudely learn a single Gaussian component with \((,)\)-DP, we will need \(n(d^{7/4}k/)\) samples to find some covariance \(\) with low score, and we also need \(n O(k^{2}d)\) so that a covariance \(\) of low score is actually a crude approximation to one of the real mixture components. To crudely approximate all components, we learn each Gaussian component with \((/,/k)\)-DP. The advanced composition theorem will imply that repeating this procedure \(k\) times on the same data (to learn all \(k\) components) will be \((,)\)-DP. Hence, by replacing\(\) with \(/\), we get that it suffices for \(n(k^{3/2}}{}+k^{2 }d)\), if we need to crudely learn all of the covariances. Finally, we can apply the private hypothesis selection technique (recall Section 2.1), which requires an additional \((k}{^{2}}+k}{})\). Combining these terms will give the final sample complexity.

We remark that the sample complexity obtained above is actually better than the complexity in Theorem 1.4. There are two reasons for this. The first is that we have been assuming each component has weight \(1/k\), meaning it contributes about \(n/k\) data points. In reality, the weights may be arbitrary and thus some components may have much fewer data points. However, it turns out that one can actually ignore any component with less than \(/k\) weight, if we want to solve density estimation up to total variation distance \(\). This will multiply the sample complexity terms \((k^{3/2}}{}+k^{2}d)\), needed for crude approximation, by a factor of \(1/\). Finally, the informal Theorem 2.1 is slightly inaccurate, and the accurate version of the theorem will end up adding the additional term of \(^{3/2}(1/)}{}\). Along with the final term \((k}{^{2}}+k}{})\) from the private hypothesis selection, these terms will exactly match Theorem 1.4.

### Roadmap

In Appendix A, we note some general preliminary results. In Appendix B, we note some additional preliminaries on robust learning of a single Gaussian. In Appendix C, we discuss the robustness-to-privacy conversion and prove some volume arguments needed for Theorem 1.4. In Appendix D, we explain how to reduce to the crude approximation setting, using private hypothesis selection. In Appendix E, we design and analyze the algorithm for multivariate Gaussians, and prove Theorem 1.4. In Appendix F, we design and analyze the algorithm for univariate Gaussians, and prove Theorem 1.5. In Appendix G, we prove Theorem 1.6. Finally, Appendix H proves some auxiliary results that we state in Appendices B and C.

## Limitations

Our results are on theoretical guarantees on the sample complexity of privately learning Mixtures of Gaussians. We do not provide any efficient or practical algorithms, and focus on statistical guarantees. We also do not discuss how to set the parameters and accuracy guarantees for practical applications, this is a question best left to practitioners. Finally, we assume each sample is i.i.d. drawn from a Gaussian Mixture Model distribution, though we remark that we use a "robustness-to-privacy" framework that will automatically make our algorithm robust to a roughly \(/k\) fraction of corruptions.

Acknowledgements.Hassan Ashtiani was supported by an NSERC Discovery grant. Shyam Narayanan was supported by an NSF Graduate Fellowship and a Google Fellowship.