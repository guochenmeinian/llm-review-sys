# No-Regret Learning in Harmonic Games:

Extrapolation in the Face of Conflicting Interests

Davide Legacci

University. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG 38000 Grenoble, France

{davide.legacci,panayotis.mertikopoulos}@univ-grenoble-alpes.fr

Panayotis Mertikopoulos

University. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG 38000 Grenoble, France

{davide.legacci,panayotis.mertikopoulos}@univ-grenoble-alpes.fr

Christos Papadimitriou

Columbia University, NYC, &

Archimedes/Athena RC, Greece

christos@columbia.edu

Georgios Piliouras

Google DeepMind

London, UK

gpil@google.com

Bary Pradelski

CNRS, Maison Francaise d'Oxford

2-10 Norham Road, Oxford, OX2 6SE, United Kingdom

bary.pradelski@cnrs.fr

Corresponding author.

###### Abstract

The long-run behavior of multi-agent learning - and, in particular, _no-regret learning_ - is relatively well-understood in potential games, where players have aligned interests. By contrast, in harmonic games - the strategic counterpart of potential games, where players have _conflicting_ interests - very little is known outside the narrow subclass of 2-player zero-sum games with a fully-mixed equilibrium. Our paper seeks to partially fill this gap by focusing on the full class of (generalized) harmonic games and examining the convergence properties of follow-the-regularized-leader (FTRL), the most widely studied class of no-regret learning schemes. As a first result, we show that the continuous-time dynamics of FTRL are _Poincare recurrent_, that is, they return arbitrarily close to their starting point infinitely often, and hence fail to converge. In discrete time, the standard, "vanilla" implementation of FTRL may lead to even worse outcomes, eventually trapping the players in a perpetual cycle of best-responses. However, if FTRL is augmented with a suitable extrapolation step - which includes as special cases the optimistic and mirror-prox variants of FTRL - we show that learning converges to a Nash equilibrium from any initial condition, and all players are guaranteed at most \((1)\) regret. These results provide an in-depth understanding of no-regret learning in harmonic games, nesting prior work on 2-player zero-sum games, and showing at a high level that harmonic games are the canonical complement of potential games, not only from a strategic, but also from a dynamic viewpoint.

## 1 Introduction

The question of "as if" rationality - that is, whether selfishly-minded, myopic agents may learn to behave "_as if_" they were fully rational - has been one of the cornerstones of non-cooperative game theory, and for good reason. Especially in modern applications of game theory to machine learning and data science - from online ad auctions to recommender systems and multi-agent reinforcementlearning - the standard postulates of rationality (knowledge of the game, capacity to compute an equilibrium, flawless execution of equilibrium strategies, common knowledge of rationality, etc.) are almost never met in practice; as a result, game-theoretic predictions that rely on these assumptions are likewise put into question. By contrast, given the ease of implementing and deploying cheap, computationally efficient learning algorithms and policies at a large scale, it is often more logical to turn to the policy being deployed as the object of interest. The aim is then to understand its long-run behavior - and, in particular, whether it ultimately leads to equilibrium.

A major obstacle in this approach is the complexity of computing a Nash equilibrium, a problem which is known to be complete for PPAD - and hence intractable - by the seminal work of Daskalakis et al. . This result implies that it is not plausible to expect any algorithm to converge to Nash equilibrium in _all_ games (at least, not in a reasonable amount of time), so it dovetails naturally with the impossibility results of Hart & Mas-Colell  who showed that there are no uncoupled learning dynamics that converge to Nash equilibrium in all games. On that account, it is natural to ask in which classes of games we can expect a learning algorithm to converge, in which classes we cannot, and under what conditions.

Perhaps the most well-behaved class of games in terms of learning is the class of _potential games_, where players have _common_ interests - not necessarily driving them to play the same strategy, but in the sense that externalities are symmetric and aligned along a common objective (the potential of the game). In this class of games, the behavior of learning dynamics - and, in particular, no-regret learning  - are relatively well understood, and there is a wide range of equilibrium convergence results, from continuous to discrete time, and even with bandit, payoff-based feedback .

By contrast, in the presence of _conflicting_ interests, the situation can be quite different. In two-player zero-sum games with a fully-mixed equilibrium - such as Matching Pennies - the continuous-time dynamics of no-regret, regularized learning are recurrent in the sense of Poincare - that is, the induced trajectory of play returns arbitrarily close to where it started infinitely many times . In discrete time, the situation becomes more complicated: the vanilla version of follow-the-regularized-leader (FTRL) - the most widely studied family of no-regret algorithms - is no longer recurrent, but it diverges away from equilibrium in the same class of games . On the other hand, if players employ an optimistic / extra-gradient variant of FTRL, the induced trajectory of play converges to equilibrium  and, under certain conditions, it is even possible to show that it converges at a geometric rate .

At the same time, zero-sum games may also admit a potential function, so it is not possible to predict the outcome of a learning process based on where it stands along the potential / zero-sum axis. The non-trivial intersection of these classes means that potential and zero-sum games are _not_ complementary, and this, not only from a strategic, but also from a dynamic viewpoint. Instead, the true strategic complement of potential games is the class of _harmonic games_. This class was first considered by Candogan et al. , who established a remarkable decomposition result: Every game in normal form can be decomposed as the sum of a potential game and a harmonic game, and this decomposition is unique up to affine transformations that do not alter the equilibrium outcomes of the game. In particular, the class of potential and harmonic games intersect trivially (up to strategic equivalence), and all two-player zero-sum games with an interior equilibrium are harmonic, thus lending credence to the fact that it is harmonic games, not zero-sum games, that correctly capture the notion of conflicting interests in this context.

This raises the question:

_What is the behavior of no-regret algorithms and dynamics in harmonic games?_

Except for a very recent paper by Legacci et al.  (which we discuss below), almost nothing is known on this question. With this backdrop, our contributions can be summarized as follows:

1. Starting with a continuous-time model of no-regret learning, we show that all FTRL dynamics are Poincare recurrent in all harmonic games. This generalizes and extends the recent result of Legacci et al.  for the replicator dynamics in uniform harmonic games (a subclass of harmonic games in which the uniform distribution is always a Nash equilibrium).22. In discrete-time models of learning, the standard implementation of FTRL cannot be expected to converge (since it fails to do so in Matching Pennies). To correct this behavior, we consider a flexible algorithmic template, inspired by Azizian et al.  and dubbed _extrapolated FTRL_ (FTRL+), which augments FTRL with a forward-looking, extrapolation step (including as special cases the optimistic and extra-step variants of FTRL, cf. Section4). We then establish the following results: 1. Under extrapolated FTRL, players are guaranteed constant individual regret (so, as a consequence, the players' empirical frequency of play converges to coarse correlated equilibrium at a rate of \((1/T)\)).3 This should be contrasted with the results of  who showed that players can achieve _polylogarithmic_ regret in any game (finite or convex). 2. The induced trajectory of play converges to Nash equilibrium from any initial condition. 
Our results aim to provide an in-depth understanding of no-regret learning in harmonic games, nesting prior work on 2-player zero-sum games - from Poincare recurrence  to constant regret  and convergence under optimistic / extra-gradient schemes . In partiucular, at a high level, our results show that harmonic games are the canonical complement of potential games, not only from a strategic, but also from a dynamic, learning viewpoint.

## 2 Preliminaries

**2.1**.: **Preliminaries on finite games.** Throughout the sequel, we will work with _finite games in normal form_. Formally, such games consist of (\(i\)) a finite set of _players_\(i\{1,,N\}\); (\(ii\)) a finite set of _actions_\(_{i}\) per player \(i\); and (\(iii\)) an ensemble of _payoff functions_\(u_{i}_{j}_{j}\), each determining the reward \(u_{i}()\) of player \(i\) in a given action profile \(=(_{1},,_{N})\). Putting everything together, we will write \(_{i}_{i}\) for the game's _action space_ and \((,,u)\) for the game with primitives as above.

During play, each player selects an action according to some _mixed strategy_, that is, a probability distribution \(x_{i}\) over \(_{i}\) which assigns probability \(x_{i_{i}}\) to \(_{i}_{i}\). In a slight abuse of notation, if \(x_{i}\) assigns all probability mass to some action \(_{i}_{i}\) (that is, \(x_{i_{i}}=1\)), we will identify \(x_{i}\) with \(_{i}\) and we will call it _pure_. We will also write \(_{i}(_{i})^{_{i}}\) for the mixed strategy space of player \(i\), \(x=(x_{1},,x_{N})\) for the _strategy profile_ collecting the strategies of all players, and \(_{i}_{i}\) for the game's _strategy space_.

The _mixed payoff_ of player \(i\) under a mixed strategy profile \(x\) may then be written as

\[u_{i}(x)=_{ x}[u_{i}()]=_{}u _{i}()\,x_{}=_{_{i}_{i}}u_{i}(_{i}; x_{-i})\,x_{i_{i}} \]

where \(x_{}_{i}x_{i_{i}}\) denotes the joint probability of \(=(_{1},,_{N})\) under \(x\), and, in standard game-theoretic notation, we write \((x_{i};x_{-i})=(x_{1},,x_{i},,x_{N})\) for the profile where player \(i\) plays \(x_{i}_{i}\) against the strategy \(x_{-i}_{-i}_{j i}_{j}\) of all other players. We also respectively define the _individual payoff field_ of player \(i\) and the _game's payoff field_ as

\[v_{i}(x)=(u_{i}(_{i};x_{-i}))_{_{i}_{i}} v(x)=(v_{1}(x),,v_{N}(x)) \]

so \(u_{i}(x)=_{_{i}_{i}}v_{i_{i}}(x)x_{i_{i}}  v_{i}(x),x_{i}\), where \(,\) is the standard duality pairing on \(^{_{i}}\). By multilinearity, each player's individual payoff field is Lipschitz continuous on \(\), and we will write \(G_{i}\) for its Lipschitz modulus, that is

\[\|v_{i}(x^{})-v_{i}(x)\|_{*} G_{i}\|x^{}-x\|x,x^{}. \]

_Remark_.: In the above and throughout, \(\|\|\) denotes an ambient norm on \(^{_{i}}\) (usually the \(L^{1}\) norm), and \(\|\|_{*}\) is the corresponding dual norm (usually the \(L^{}\) norm). To simplify notation, we will not carry the player index \(i\) in \(\|\|\), and we will instead rely on the context to resolve any ambiguities.

In terms of solution concepts, we will focus almost exclusively on the notion of a _Nash equilibrium_ (NE), i.e., a strategy profile \(x^{*}\) that is unilaterally stable in the sense that

\[u_{i}(x^{*}) u_{i}(x_{i};x^{*}_{-i})x_{i}_{i},\,i\,.\] (NE)

Equivalently, (NE) can be expressed in terms of the game's payoff field as a variational inequality of the form

\[ v(x^{*}),x-x^{*} 0x\,.\] (VI)

Thus, writing \((x^{*}_{i})=\{_{i}_{i}:x_{i_{i}}>0\}\) for the _support_ of \(x^{*}_{i}\), it follows that \(x^{*}\) is a Nash equilibrium if and only if \(u_{i}(_{i};x^{*}_{-i}) u_{i}(_{i};x^{*}_{-i})\) for all \(_{i}(x^{*}_{i})\) and all \(_{i}_{i}\), \(i\). We will use all this freely in the rest of our paper.

### Harmonic games

Our main focus in what follows will be the class of _harmonic games_, first introduced by Candogan et al.  as a game-theoretic model for strategic situations with conflicting, anti-aligned interests. Specifically, as was shown by Candogan et al.  - and, in a more general setting, by Abdou et al.  - every game in normal form can be decomposed as the sum of a potential game and a harmonic game, and this decomposition is unique up to affine transformations that do not alter the equilibrium outcomes of the game.4 In this decomposition, the potential component of a game captures multi-agent strategic interactions with _common_ interests, whereas the harmonic component covers interactions with _conflicting_ interests.5

Formally, adapting the more general setup by Abdou et al. , we have the following definition:

**Definition 1**.: A finite game \((,,u)\) is said to be _harmonic_ when it admits a _harmonic measure_, i.e., a collection of weights \(_{i,_{i}}(0,)\), \(_{i}_{i},i\), such that

\[_{i}_{_{i}_{i}}_{_{i}}[ u_{i}(_{i};_{-i})-u_{i}(_{i};_{-i})]=0\,.\] (HG)

In particular, if \(\) is harmonic relative to the uniform measure \(_{i_{i}}=1\), \(_{i}_{i},i\), we will say that \(\) is a _uniform harmonic game_ (UHG). \(\)

_Remark_.: With regard to terminology, Candogan et al.  call "harmonic games" what we call "uniform harmonic games", and Abdou et al.  call "\(\)-harmonic games" what we call "harmonic games".6 We use this convention because it simultaneously simplifies notation and terminology while capturing all relevant strategic features of the game; for a detailed discussion, see Appendix A. To avoid needless repetition, and unless there is a danger of confusion, when we say that \(\) is harmonic, we will write \(_{i}\) for the corresponding measure, and we will write \(m_{i}=|_{i}|=_{_{i}_{i}}_{i_{i}}\) for the total mass of \(_{i}\). \(\)

Broadly speaking, in harmonic games, for any player considering a deviation toward a specific pure strategy profile, there exist other players with an incentive to deviate _away_ from said profile. In this regard, harmonic games can be seen as the strategic complement of potential games, where player interests are aligned and sequences of unilateral best responses generate a finite improvement path that terminates at a pure Nash equilibrium . By contrast, except for trivial cases (like the zero game) harmonic games _do not_ admit pure Nash equilibria, and they possess non-terminating best-response paths. For all these reasons, harmonic games can be considered as "orthogonal" to potential games, in a sense made precise by the decomposition results of Candogan et al.  and Abdou et al. .

It is of course natural to ask what is the relation between harmonic games and zero-sum games. Games belonging to the latter class - such as Matching Pennies and Rock-Paper-Scissors - have long been used as prototypical examples of strategic conflict; at the same time, there are zero-sum games that are also potential (and even possess strict equilibria), so the potential / zero-sum distinction does not capture the whole picture. As a matter of fact, it is not a coincidence that the textbook examples of zero-sum games admit fully-mixed Nash equilibria: as we discuss in Appendix A, two-player zero-sum games with a fully mixed Nash equilibrium are harmonic, so the existing results for such games are, in a sense, more closely attuned to their harmonic character.

Continuous-time analysis: Poincare recurrence

The most basic rationality postulate in the context of online learning is the minimization of a player's (external) regret, i.e., the difference between a player's cumulative payoff and that of the player's best possible strategy in hindsight. In more detail, assuming for the moment that play evolves in continuous time, the _regret_ of player \(i\) relative to a sequence of play \(x(t)\) is defined as

\[_{i}(T)=_{p_{i}_{i}}_{0}^{T}[u_{i}(p_{i};x _{-i}(t))-u_{i}(x(t))]\,dt \]

and we say that the player has _no regret_ under \(x(t)\) if \(_{i}(T)=o(T)\) as \(T\).

The most widely used scheme for attaining no regret is the family of policies known as _follow-the-regularized-leader_ (FTRL) . At a high level, the idea behind FTRL is that, at all times \(t 0\), each player \(i\) plays a mixed strategy \(x_{i}(t)_{i}\) that maximizes the player's cumulative payoff up to time \(t\) minus a certain regularization penalty. In our continuous-time setting, this gives rise to the FTRL dynamics

\[x_{i}(t)=*{arg\,max}_{p_{i}_{i}}\{_{0}^{t }u_{i}(p_{i};x_{-i}())\;d-h_{i}(p_{i})\}=*{arg\, max}_{p_{i}_{i}}\{_{0}^{t} v_{i}(x()),p_{i} \;d-h_{i}(p_{i})\} \]

or, more compactly,

\[_{i}(t)=v_{i}(x(t)) x_{i}(t)=Q_{i}(y_{i}(t))\] (FTRL-D)

where \(h_{i}_{i}\) is a convex penalty function known as the _regularizer_ of the method, \(Q_{i}\) denotes the _regularized choice map_ of player \(i\), and \(Q=(Q_{1},,Q_{N})\) denotes the profile thereof. Formally, writing \(_{i}^{A_{i}}\) for the _payoff space_ of player \(i\) - that is, the space of all possible payoff vectors \(v_{i}\) of player \(i\) - the regularized choice map \(Q_{i}_{i}_{i}\) is defined as

\[Q_{i}(y_{i})=*{arg\,max}_{x_{i}_{i}}\{(y_{i}, x_{i})-h_{i}(x_{i})\}y_{i}_{i}\;. \]

In essence, \(Q_{i}\) is a "soft" version of the \(*{arg\,max}\) correspondence \(y_{i}*{arg\,max}_{x_{i}_{i}} y_{i},x _{i}\), suitably regularized by a penalty term intended to incentivize exploration. For technical reasons, we will also assume that each \(h_{i}\) is _strongly convex_, i.e.,

\[h_{i}(tx_{i}+(1-t)x_{i}^{}) th_{i}(x_{i})+(1-t)h_{i}(x_{i}^{}) -K_{i}t(1-t)\|x_{i}-x_{i}^{}\|^{2} \]

for some \(K_{i}>0\) (commonly referred to as the _strong convexity modulus_ of \(h_{i}\)), and for all \(x_{i},x_{i}^{}\), \(t\). In plain words, this simply means that \(h_{i}\) has "enough curvature" in the sense that it can be bounded from below by a (positive) quadratic function which agrees with \(h_{i}\) to first order.

The go-to example of this setup is the entropic regularizer

\[h_{i}(x_{i})=_{_{i}_{i}}x_{i}_{i} x_{i} _{i} \]

which yields the so-called _logit choice map_

\[Q_{i}(y_{i})_{i}(y_{i})_{i}))_{ _{i}_{i}}}{_{_{i}_{i}}(y_{i} _{i})}y_{i}_{i}. \]

By Pinsker's inequality, the entropic regularizer is \(1\)-strongly convex relative to the \(L^{1}\)-norm on \(_{i}\), and by a standard calculation , the induced sytem (FTRL-D) boils down to the replicator dynamics of Taylor & Jonker . Some other standard examples of (FTRL-D) include the Euclidean projection dynamics of Friedman  when \(h_{i}(x_{i})=(1/2)\|x_{i}\|_{2}^{2}\), the \(q\)-replicator dynamics , etc. To streamline our presentation, we defer a detailed discussion of these examples to Appendix C, and we proceed below to state the main regret guarantee of (FTRL-D), originally due to :

**Theorem 1**.: _Under (FTRL-D), each player's regret is bounded as \(_{i}(T) H_{i} h_{i}- h_{i}\)._

Theorem 1 showcases the strong no-regret properties of (FTRL-D): it is not possible to guarantee less than constant, \((1)\) regret, so (FTRL-D) is optimal in this regard. In turn, by standard results , Theorem 1 implies further that the players' (correlated) empirical frequencies \(z_{_{1},,_{N}}(t)(1/t)_{0}^{t}_{i}x_{i} _{i}()\;d\) converge to the game's set of coarse correlated equilibria (CCE) at a rate of \((1/t)\).

Importantly, this result makes no assumptions about the underlying game, but it does not carry the same predictive power in all games: for one thing, a game's set of CCE may include highly non-rationalizable outcomes (such as dominated strategies and the like) ; for another, the time-averaging that is inherent in the definition of empirical distributions may conceal a wide range of non-convergence phenomena, from cycles to chaos [48; 56]. On that account, the day-to-day behavior of (FTRL-D) in harmonic games cannot be understood from Theorem 1 alone, and requires a closer, more in-depth look.

Our first result below provides such a lense and shows that (FTRL-D) is almost-periodic in harmonic games, a property known as _Poincare recurrence._

**Theorem 2**.: _Suppose \(\) is harmonic. Then almost every orbit \(x(t)\) of (FTRL-D) returns arbitrarily close to its starting point infinitely often: specifically, for (Lebesgue) almost every initial condition \(x(0)=Q(y(0))\), there exists an increasing sequence of times \(t_{n}\) such that \(x(t_{n}) x(0)\)._

An immediate consequence of Theorem 2 is that no-regret learning under (FTRL-D) fails to converge in _any_ harmonic game; in particular, since the orbits of (FTRL-D) eventually return to (almost) where they started, it is debatable if the players have learned anything at all, despite the fact that they incur at most constant regret. This cyclic, non-convergent landscape is the polar opposite of the long-run behavior of (FTRL-D) in _potential_ games, where the dynamics are known to converge globally . Thus, in addition to the strategic viewpoint of the previous section, Theorem 2 shows that harmonic games are orthogonal to potential games also from a _dynamic_ viewpoint.

Theorem 2 also provides a far-reaching generalization of existing results on Poincare recurrence in (possibly networked) two-player zero-sum games with an interior equilibrium  to general-sum, \(N\)-player games. Combined with our previous remark, and given that the zero-sum property is not as meaningful for \(N\) players as it is for two,7 the class of harmonic games can be seen as the more natural \(N\)-player generalization of two-player zero-sum games from a learning viewpoint.

To the best of our knowledge, the only comparable result to Theorem 2 in the literature is the very recent paper of Legacci et al.  who showed that the replicator dynamics - a special case of (FTRL-D) - are Poincare recurrent in _uniform_ harmonic games, that is, in harmonic games where the uniform distribution is a Nash equilibrium, cf. (A.1) and the discussion surrounding Definition 1. In this regard, Theorem 2 extends the recent results of Legacci et al.  along two axes: (_i_) it applies to the entire class of FTRL dynamics (not only the replicator dynamics); and (_ii_) it applies to the entire class of harmonic games (and not only _uniformly_ harmonic games).

In terms of techniques, Legacci et al.  obtained their result through a surprising connection between a certain Riemannian metric underlying the replicator dynamics and the defining relation of uniformly harmonic games. This relation no longer holds for different instances of (FTRL-D) or for non-uniform harmonic games, so the techniques of  cannot be extended - and, in fact, Legacci et al.  stated this generalization as an open problem. Our techniques instead rely on the fact that the orbits \(y(t)\) of (FTRL-D) comprise a volume-preserving flow in the game's payoff space \(_{i}_{i}\) (though not necessarily on \(\)), and then deriving a suitable constant of motion. In the case of the logit map (9), this constant of motion can be written as

\[G(x)=_{i}_{_{i}_{i}}x^{_{ _{i}}}_{_{i}}x, \]

where \(=(_{i_{i}})_{_{i}_{i},i}\) is the harmonic measure on \(\) defining \(\). In the more general case, the construction of a constant of motion for (FTRL-D) involves a characterization of harmonic games in terms of a "strategic center", which we carry out in detail in Appendix C.

## 4 Discrete-time analysis: Convergence and constant regret via extrapolation

We now proceed to examine the regret and convergence properties of regularized learning algorithms in harmonic games. Starting with the standard, vanilla implementation of FTRL, we reproduce a well-known observation that FTRL spirals out to a non-terminating cycle of best-responses in Matching Pennies (which is a harmonic game). Subsequently, to correct this non-convergent behavior, we examine a flexible algorithmic template, which we call _extrapolated FTRL_ (FTRL+), and which includes as special cases the optimistic and extra-gradient versions of FTRL.

### Vanilla implementation of FTRL

Building on the discussion of the previous section, the standard implementation of FTRL in discrete time for \(n=1,2,\) is

\[x_{i,n+1}=*{arg\,max}_{p_{i}_{i}}_{k=1}^ {n}u_{i}(p_{i};x_{-i,n})-_{i}h_{i}(p_{i})}=*{arg\, max}_{p_{i}_{i}}_{k=1}^{n} v_{i}(x_{k}),p_{i} -_{i}h_{i}(p_{i})} \]

or, in more compact, iterative notation

\[y_{i,n+1}=y_{i,n}+_{i}v_{i}(x_{n}) x_{i,n}=Q_{i}(y_{i,n})\] (FTRL)

where, as per (6), the map \(Q_{i}_{i}_{i}\) denotes the _regularized choice map_ of player \(i\), \(_{i}\) is a player-specific regularization weight parameter, and \(_{i}=1/_{i}\) represents the _learning rate_ of player \(i\). Apart from their obvious differences - discrete vs. continuous time - a salient point that sets (FTRL) apart from (FTRL-D) is the inclusion of the parameter \(_{i}\); this parameter is necessary to control the algorithm's behavior, and we will discuss it in detail in the sequel.

As mentioned in the introduction, a major shortfall of (FTRL) - and one of the main reasons for the increased popularity of optimistic / extra-gradient methods - is that it may spiral away from Nash equilibrium, even in simple \(2 2\) games with a unique equilibrium. The standard example of this behavior is Matching Pennies, a two-player zero-sum game with a fully-mixed equilibrium which is also uniformly harmonic, so the trajectories of (FTRL-D) are Poincare recurrent (and, in fact, periodic). In more detail, this game can be compactly represented by the payoff field \(v(x_{1},x_{2})=(4x_{2}-2,2-4x_{1})\) for \(x_{1},x_{2}\), and its unique Nash equilibrium is \(x^{*}=(1/2,\,1/2)\). Thus, if we run (FTRL) with a Euclidean regularizer - that is, \(h_{i}(x_{i})=x_{i}^{2}/2\) for \(i=1,2\) - and the same learning rate \(\) for both players, a straightforward calculation shows that the distance \(D_{n}=(x_{1,n}-x_{1}^{*})^{2}/2+(x_{2,n}-x_{2}^{*})^{2}/2\) between \(x_{n}\) and \(x^{*}\) evolves as

\[D_{n+1}=(x_{1,n}+ v_{1}(x_{n})-x_{1}^{*})^{2}+(x_{2,n}+ v_{2}(x_{n})-x_{2}^{*})^{2}=(1+16^{2})D_{n} \]

as long as \(x_{n}+ v(x_{n})\). In other words, the distance of the iterates of (FTRL) from the game's equilibrium grows at a geometric rate until \(x_{n}\) reaches the boundary of \(\) and is ultimately trapped in a non-terminating cycle of best responses, cf. Fig. 1. In this regard, the rationality properties of (FTRL) are even worse than those of (FTRL-D) because the game's equilibrium is now _repelling_.

### Extrapolated FTRL

To mitigate this undesirable, divergent behavior of (FTRL), a standard approach in the literature is the inclusion of a forward-looking, "_extrapolation step_". Instead of updating the algorithm's "base state" \(x_{n}\) directly, players first move to an interim "leading state" \(x_{n+1/2}\) using payoff information from \(x_{n}\) (this is the extrapolation step); subsequently, players update \(x_{n}\) using payoff information from the leading state \(x_{n+1/2}\), and the process repeats. In this way, players attempt to anticipate their payoff landscape and, in so doing, to take a more informed update step at each iteration.

The seed of this idea goes back to Korpelevich  and Popov  in the context of solving monotone variational inequality problems, and it has since percolated to a wide array of "_extra-gradient_" or "_optimistic_" methods, such as the mirror-prox algorithm of Nemirovski , the dual extrapolation variant of Nesterov , the optimistic mirror descent algorithm of Chiang et al.  and Rakhlin & Sridharan , and many others. Given the different operational envelope of each of these methods, we consider below an integrated algorithmic template, which we call _extrapolated FTRL_ (FTRL+), and which is sufficiently flexible to account for a broad range of these schemes.

Formally, the proposed algorithmic blueprint unfolds in two phases as follows:

* _Extrapolation phase:_ \[y_{i,n+1/2}=y_{i,n}+_{i}_{i,n} x_{i,n+1/2}=Q_{i}(y_{i,n+1/2})\]
* _Update phase:_ \[y_{i,n+1}=y_{i,n}+_{i}_{i,n+1/2} x_{i,n+1}=Q_{i}(y_{i,n+1})\] (FTRL+)

In the above, \(_{i}>0\) is the learning rate of player \(i\), \(x_{n}\) and \(x_{n+1/2}\) denote respectively the method's _base_ and _leading_ states at stage \(n=1,2,\), and \(_{i,n}\) and \(_{i,n+1/2}\) are sequences of "black-box" payoff models at \(x_{n}\) and \(x_{n+1/2}\) respectively.

Specifically, following Azizian et al. , we will assume throughout that

\[_{i,n+1/2}=v_{i}(x_{n+1/2})$ and all $n=1,2,$}\] (13a) i.e., players always update the base state \[x_{n}\] using payoff information from the leading state \[x_{n+1/2}\]. By contrast, the leading state \[x_{n+1/2}\] can be generated in many different ways, depending on the targeted update structure. In this regard, we will consider the linear model \[_{i,n}=a_{i}\,v_{i}(x_{n})+b_{i}\,v_{i}(x_{n-1/2})$ and all $n=1,2,$} \]where the player-specific coefficients \(a_{i},b_{i} 0\) satisfy \(a_{i}+b_{i} 1\) and represent a mix of past and present payoff information. In this way, depending on the values of \(a_{i}\) and \(b_{i}\), we obtain the following prototypical regularized learning methods as special cases of (FTRL+):

* _FTRL:_ if \(a_{i}=b_{i}=0\) for all \(i\), players essentially forego any look-ahead efforts, so we get \[_{n}=0n=1,2,\] (14a) In turn, this gives \(x_{n+1/2}=x_{n}\), i.e., (FTRL+) regresses to (FTRL).
* _Extra-Step FTRL:_ if \(a_{i}=1\) and \(b_{i}=0\) for all \(i\), we have \[_{n}=v(x_{n})n=1,2,\] (14b) i.e., players use payoff information from their current state to generate the leading state \(x_{n+1/2}\). This update structure requires two payoff queries per iteration and its origins can be traced back to the work of Korpelevich . Specifically, depending on the choice of \(h_{i}\), it is essentially equivalent to the mirror-prox  and dual extrapolation  algorithms, it contains as a special case the forward-looking algorithm of , etc.
* _Optimistic FTRL:_ if \(a_{i}=0\) and \(b_{i}=1\) for all \(i\), we have \[_{n}=v(x_{n-1/2})n=1,2,\] (14c) i.e., players reuse the latest available payoff information instead of making a fresh query at \(x_{n}\) (so the algorithm only requires one payoff query per iteration). In this way, (FTRL+) recovers the optimistic algorithms of , the OMW update scheme of  when \(Q=\), etc.

Clearly, the list above is not exhaustive: many other configurations are possible, e.g., with different players using different parameter settings for \(a_{i}\) and \(b_{i}\), depending on the information they have at hand and any other individual considerations. To avoid needlessly complicating the analysis, our only standing assumption will be that \(a_{i}+b_{i}>0\) for all \(i\) (since, otherwise, the benefits of the extrapolation step would vanish). In particular, by rescaling the players' learning rates if needed, we will normalize \(a_{i}\) and \(b_{i}\) to \(a_{i}+b_{i}=1\), leading to the convex model

\[_{i,n}=_{i}\,v_{i}(x_{n})+(1-_{i})\,v_{i}(x_{n-1/2}) \]

for some arbitrarily chosen ensemble of player-specific _extrapolation coefficients \(_{i}\)_, \(i\)_.

_Remark_.: To simplify the presentation of our results, we will assume throughout the rest of our paper that (FTRL+) is initialized with \(y_{1}=y_{1/2}=0\).

### Analysis & results

With all this in hand, we are finally in a position to state our main results for (FTRL+) in harmonic games. We begin by showing that (FTRL+) achieves order-optimal regret:

**Theorem 3**.: _Suppose that each player in a harmonic game \(\) is following (FTRL+) with learning rate \(_{i} m_{i}K_{i}\,[2(N+2)_{j}m_{j}G_{j}]^{-1}\) and payoff models as per (13a) and (15). Then the individual regret of each player \(i\) is bounded as_

\[_{i}(T)_{p_{i}_{i}}_{n=1}^{T }[u_{i}(p_{i};x_{-i,n})-u_{i}(x_{n})]}{_{i}}+}{ N+2}_{j}}{_{j}G_{j}} \]

_where \(H_{i}= h_{i}- h_{i}\), and \(G_{i}\) is the Lipschitz modulus of \(v_{i}\)._

Even though Theorem 3 invites a natural comparison with the constant regret bound of Theorem 1, the continuous- and discrete-time settings are fundamentally different, so any conclusions drawn from such a comparison would be specious. Indeed, constant regret guarantees in the spirit of (16) are particularly rare in the context of discrete-time algorithms, and as far as we are aware, similar bounds have only been established for optimistic methods in variationally stable and two-player zero-sum games ; other than that - and always to the best of our knowledge - the tightest regret bounds available for general games (finite or convex) seem to be (poly)logarithmic . In this regard, just like the recurrence result of Theorem 2, the \((1)\) regret bound of Theorem 3 represents a significant extension of existing results on zero-sum games (and polylogarithmic regret in general games), and suggests that, from a learning viewpoint, harmonic games are the most natural generalization of two-player zero-sum games to a general \(N\)-player context. We defer the proof of Theorem 3 to Appendix D.

As an immediate corollary of the above, we conclude that, under (FTRL+), the empirical frequencies of play \(z_{,n}(1/n)_{k=1}^{n}x_{,k}\), \(\), converge to the game's set of CCE at a rate of \((1/n)\). This rate is, again, optimal, but as we discussed in Section 3, it offers little information in games where the marginalization of CCE does not lead to Nash equilibrium - and, in general \(N\)-player harmonic games, there is little hope that it would. In addition, even when the marginalization of CCE is Nash, the actual trajectory of play may - and, in fact, often _does_ - behave very differently from the time-averaged frequency of play.

Despite these hurdles, we show below that (FTRL+) _does_ converge to Nash equilibrium. To state this result formally, we will focus on the case where each player's regularizer is _smooth_ in the sense that

\[h_{i}(x_{i}+t(x^{}_{i}-x_{i}))t=0 \]

for all \(x_{i}Q_{i}\) and all \(x^{}_{i}_{i}\).8 Our prototypical examples - the entropic and Euclidean regularizers - both satisfy this mild requirement, as do all regularizers of the form \(h_{i}(x_{i})=_{_{i}}_{i}(x_{i_{i}})\) for some smooth convex function \(_{i}\). We then have the following convergence result:

**Theorem 4**.: _Suppose that each player in a harmonic game \(\) follows (FTRL+) with learning rate \(_{i} m_{i}K_{i}[2(N+2)_{j}m_{j}G_{j}]^{-1}\) and payoff models as per (13a) and (15). Then \(x_{n}\) converges to the set of Nash equilibria of \(\)._

To the best of our knowledge, Theorem 4 is the first result of its kind for harmonic games - and, in that regard, it is somewhat unexpected. To be sure, two-player zero-sum games with a fully-mixed equilibrium exhibit a comparable pattern: FTRL is Poincare recurrent in continuous time, its vanilla discretization is unstable, and its optimistic / forward-looking implementation is convergent. However, the convex-concave structure of min-max games which enables this analysis is completely absent in harmonic games, so it is less clear what to expect in this case (where even the set of Nash equilibria is non-convex, cf. Fig. 1). By this token, the convergence of (FTRL+) in harmonic games is a property that one could optimistically hope for, but not one that can be taken for granted.

From a technical standpoint, the proof of Theorems 3 and 4 involves two concurrent challenges:

1. Deriving a Lyapunov function with a "sufficient descent" property for all harmonic games.
2. Providing an integrated analysis for all possible update structures in (FTRL+).

With regard to the first point, our analysis hinges on the "energy function"

\[E(p,y)=_{i}}{_{i}}F_{i}(p_{i},y_{i}) p ,y, \]

In the above, \(p\) is a benchmark strategy profile acting as a "reference point" for the analysis while

\[F_{i}(p_{i},y_{i})=_{x_{i}_{i}}\{ y_{i},x_{i}- h_{i}(x_{i})\}-[ y_{i},p_{i}-h_{i}(p_{i})] \]

Figure 1: The evolution of vanilla vs. extrapolated FTRL schemes in harmonic games. In the left figure, we consider the game of Matching Pennies (blue: FTRL+; green: FTRL; red: continuous time FTRL); in the center and to the right, two different orbits in a \(2 2 2\) harmonic game from two different viewpoints (blue: FTRL+; green/orange:FTRL; payoff profiles on vertices). In all cases, we ran the optimistic variant of FTRL+ (\(_{i}=0\) for all players), and we see that the trajectories of (FTRL) diverge away from equilibrium and the trajectories of (FTRL-D) are recurrent (actually, periodic), whereas (FTRL+) converges. We also see the highly non-convex structure of harmonic games as evidence by their equilibrium set (thick red line in center and right subfigures).

denotes the _Fenchel coupling_ associated to the regularizer \(h_{i}\) of player \(i\), and represents a "primal-dual" measure of divergence between \(p_{i}_{i}\) and \(y_{i}_{i}\) (for an in-depth discussion, see Appendices B and D). Then, letting \(E_{n}=E(p,y_{n})\), the heavy lifting for our analysis is provided by the "template inequality"

\[E_{n+1} E_{n} +_{i}m_{i} v_{i}(x_{n+1/2}),x_{i,n+1/2}-p _{i}\] \[+_{i}m_{i} v_{i}(x_{n+1/2})-v_{i}(x_{n}),x_{i,n+1}-x_{i,n+1/2}\] \[+_{i}m_{i}(1-_{i}) v_{i}(x_{n})-v _{i}(x_{n-1/2}),x_{i,n+1}-x_{i,n+1/2}\] \[-_{i}K_{i}}{_{i}}\|x_{i,n +1}-x_{i,n+1/2}\|^{2}+\|x_{i,n+1/2}-x_{i,n}\|^{2}\;. \]

A first important consequence of (20) is that the sequences \(A_{n}=\|x_{n+1}-x_{n+1/2}\|^{2}\) and \(B_{n}=\|x_{n+1/2}-x_{n}\|^{2}\) are both summable: this requires a repeated use of the Fenchel-Young inequality, and an instantiation of \(p\) to the strategic center \(q\) of \(\); we detail the relevant arguments in Appendices A and D. Then, by establishing a similar template inequality for _each_ player \(i\), we are able to bound the players' individual regret by the same upper bound that we derived for \(_{n}A_{n}\) and \(_{n}B_{n}\), and which is (up to certain secondary factors) the bound (16).

For the convergence to Nash equilibrium, the summability argument above also plays a crucial role. First, by a standard result on numerical sequences, the summability of \(A_{n}\) and \(B_{n}\) coupled with the template inequality (20) implies that the energy \(E_{n}\) of the algorithm relative to the game's strategic center converges to some limit value \(E_{}\). In turn, this implies that the score sequence \(y_{n}\) is bounded up to a multiple of the vector \((1,,1)\), which corresponds to a constant payoff shift in the underlying game. Then, by focusing on convergent subsequences of \(y_{n}\) and the optimality condition resulting from the definition of \(Q\), we are able to show that any limit point of \(v(x_{n})\) satisfies the variational characterization (VI) of Nash equilibria, from which our claim follows.

## 5 Concluding remarks

Our results suggest that the long-run behavior of no-regret algorithms and dynamics in harmonic games is a very rich topic, and one which opens the door to an entirely new class of games where positive convergence results can be obtained. We find this particularly appealing, not only because harmonic games comprise the strategic complement of potential games, but also because they go beyond standard problems with a convex structure - for instance, even their equilibrium set is non-convex. As such, the fact that it is possible to obtain optimal regret guarantees and positive equilibrium convergence results in this setting is very promising for future work on the topic.

In terms of open questions, it would be important to examine the rate of convergence of (FTRL+) to equilibrium. Even though (FTRL+) has order-optimal regret bounds, this only helps in establishing a convergence rate to the game's set of coarse correlated equilibria; for Nash equilibria, earlier work by Golowich et al.  and some more recent results by Cai et al.  and Gorbunov et al.  have shed some light on the convergence of constrained Euclidean optimistic methods, but the technology therein does not extend to non-monotone, non-Euclidean problems. Inspired by Wei et al. , we conjecture that the convergence rate of (FTRL+) in harmonic games is linear: this is based on the observation that any harmonic game admits a fully-mixed Nash equilibrium, and the weighted sum in the definition of a harmonic game looks formally similar to the condition needed to establish metric subregularity in ; however, a proof would likely require different techniques.

Another important research direction has to do with the information available to the players. A first open question here concerns the case where players do not have access to full information on their mixed payoff vectors, but can only observe their pure payoffs - either in a "what if", counterfactual manner, or in the form of bandit, payoff-based feedback. In a similar manner, the algorithms presented here are not adaptive, in the sense that the players' step-size policy has to satisfy a certain bound that depends on correctly estimating some of the game's parameters. Obtaining an adaptive version of (FTRL+) which, in the spirit of Rakhlin & Sridharan  and Hsieh et al. [27; 28; 29], remains convergent and attains order-optimal regret in both adversarial and game-theoretic settings without any pre-play tuning is also an ambitious question for future research.