# Subsurface Scattering for 3D Gaussian Splatting

Jan-Niklas Dihlmann Arjun Majumdar Andreas Engelhardt Raphael Braun Hendrik P.A. Lensch

University of Tubingen

###### Abstract

3D reconstruction and relighting of objects made from scattering materials present a significant challenge due to the complex light transport beneath the surface. 3D Gaussian Splatting introduced high-quality novel view synthesis at real-time speeds. While 3D Gaussians efficiently approximate an object's surface, they fail to capture the volumetric properties of subsurface scattering. We propose a framework for optimizing an object's shape together with the radiance transfer field given multi-view OLAT (one light at a time) data. Our method decomposes the scene into an explicit surface represented as 3D Gaussians, with a spatially varying BRDF, and an implicit volumetric representation of the scattering component. A learned incident light field accounts for shadowing. We optimize all parameters jointly via ray-traced differentiable rendering. Our approach enables material editing, relighting and novel view synthesis at interactive rates. We show successful application on synthetic data and introduce a newly acquired multi-view multi-light dataset of objects in a light-stage setup. Compared to previous work we achieve comparable or better results at a fraction of optimization and rendering time while enabling detailed control over material attributes. Project page: [https://sss.jdihlmann.com/](https://sss.jdihlmann.com/)

## 1 Introduction

Subsurface scattering (SSS) is an important aspect of our visual reality and therefore an indispensable part of realistic rendering. It is the process by which light penetrates a surface and is scattered

Figure 1: **SSS GS â€“ We propose photorealistic real-time relighting and novel view synthesis of subsurface scattering objects. We learn to reconstruct the shape and translucent appearance of an object within the Gaussian Splatting framework. To do so we leverage our newly created multi-view multi-light dataset of synthetic and real-world objects acquired in a light-stage setup. The object is decomposed in a PBR fashion allowing for easy material editing and relighting. For a trailer visit our project page at [https://sss.jdihlmann.com/](https://sss.jdihlmann.com/).**beneath it before being reflected back out. This phenomenon is responsible for the soft and diffuse appearance of materials such as wax, marble, skin and many other organic substances. Modeling SSS is challenging because it requires capturing the complex interactions between light and matter between different points on the surface. Approximating SSS by a simple surface reflection model where the light is reflected directly at the point of incidence typically leads to unnatural appearance of those objects. As a result, efforts in computer graphics to explicitly model SSS are either computationally expensive or approximate for interactive rendering. Additionally, capturing the complexity of spatially varying real-world scattering properties of an object using conventional computer vision techniques is difficult as they are often focused on reconstructing only visible surfaces and their BRDF.

In recent years, modeling SSS using neural networks has been a topic of interest, e.g. [36; 29] use Neural SSS materials as part of a Monte Carlo global illumination rendering pipeline. These methods are trained in a supervised fashion using synthetic datasets. In contrast, Neural Radiance Fields (NeRFs) [27; 49; 25] can learn the volumetric properties of SSS under varying lighting conditions implicitly and achieve photorealistic novel view synthesis and relighting. However, NeRFs are slow in training and inference due to the volumetric rendering requiring large or multiple MLPs to be evaluated for many point samples along each ray. Recently, 3D Gaussian Splatting (3D GS)  has been introduced as a method for 3D reconstruction with high-quality novel view synthesis. It achieves real-time speeds by avoiding costly volume rendering, which, however, also limits the representation of volumetric effects. In this work, we introduce subsurface scattering to the 3D GS framework.

Specifically, we propose the first method based on 3D GS for capturing detailed SSS effects of single objects, allowing for rendering and relighting in real-time.

At the core, we propose a hybrid representation that extends 3D Gaussian Splatting with PBR material parameters  and deferred shading  with a light-weight residual prediction network to learn a subsurface scattering (SSS) shading component not modeled by the surface shader. We constrain the network predicting the outgoing SSS radiance by jointly predicting the incident radiance used for the PBR rendering step enforcing a neural representation of the local and global light transport in the scene. To overcome the inherent resolution limit of 3D Gaussians we perform the shading in image space. This improves the representation of specularity in particular.

We further introduce a newly acquired OLAT (one-light-at-a-time) dataset of SSS objects. It comprises object-centric 360\({}^{}\) multi-view image collections of synthetic objects rendered in Blender using the Cycles PBR renderer  as well as real-world examples we acquired using a light stage and motorized camera.

Our method provides object based decomposition for PBR material components and SSS effects, which allows for editing and novel material synthesis. We show that our method has improved training time and faster rendering speed compared to previous SSS approaches based on NeRFs while achieving comparable or better results.

Possible applications include \(\) Medical Imaging and Visualization for Tissue Rendering and Surgical Simulation [35; 40]\(\) Entertainment for visual effects and animation \(\) VR and AR by providing a more realistic and immersive experience.

## 2 Related Work

Scene RepresentationNeural Radiance Fields (NeRF) introduced by Mildenhall et al. , have started a trend in synthesizing novel views of complex 3D scenes with high fidelity, by representing the scene as a continuous 5D function using neural networks. While NeRFs have been widely adopted, they are computationally expensive due to the volumetric evaluation. There have been several works to accelerate NeRFs [28; 38; 33] one of which is KiloNeRF  that uses multiple small MLPs to represent different parts of the scene. Kerbl et al.  introduced representing the scene as a set of explicit learnable 3D Gaussians. The proposed method 3D Gaussian Splatting (3D GS) is more efficient than NeRFs due to splatting i.e. rasterization of the Gaussians. However, both these representations are limited to static scene representations and do not support religting or material decomposition.

Material Decomposition and RelightingThere have been prior works accomplishing 3D shape and material reconstruction for relighting: NeRD  was one of the first works to extend NeRF to decompose the input images into shape, BRDF and illumination for relighting. NeRFactor  and NeRV  aim for similar goals with slightly different network architectures. Neural-PIL  added a split-sum pre-integrated illumination network for more accurate and faster optimization. Likewise, NVDiffrec  use a pre-integrated illumination representation to optimize materials together with the object's shape.

In the realm of Gaussian Splatting, there have been several works focusing on relighting and BRDF decomposition of static scenes [10; 17; 23]. With Relightable 3D Gaussians (R3DGS) , the authors decompose the scene into explicit metallic, roughness, color, and normal components, which can be composed and relit in real-time. The method models light with a neural incident light field  and a local learnable representation as Spherical Harmonics. The illumination and shading is optimized per scene with differentiable rendering. Gaussian Shader  uses a similar decomposition but predicts the environmental illumination to not only model diffuse but also reflective surfaces. Further, works have utilized defered shading  with Gaussian Splatting [39; 23; 19] to improve specular reflections, by focusing on the blending and propagation of normal directions between overlapping 3D Gaussians. Additionaly DeferredGS  train a SDF in parallel to improve the surface geometry. We identify that the areas of the 3D Gaussians are a key factor for the representation of high frequency details in the scene and propose deferred shading to improve the specular reflections without the need of normal optimization or additional SDF training. We base our work of R3DGS  and augment the reflectance calculation to accommodate for subsurface scattering.

Subsurface ScatteringThere have been a multitude of prior works in SSS. Jensen et al.  presents a practical model for subsurface light transport in translucent materials that captures effects beyond BRDF models. Donner et al.  further explore light diffusion in multi-layered translucent materials using a variant of the Kubelka-Munk theory. Lensch et al.  introduce a rendering method for translucent diffuse objects, in which viewpoint and illumination can be modified at interactive rates. and filtered using the precomputed kernels. Vicini et al.  introduce a new shape-adaptive BSSRDF model that is based on a conditional variational autoencoder which learns to sample from a reference distribution produced by a brute-force volumetric path tracer. The distribution is conditional on both material properties and a set of features characterizing geometric variations in the neighborhood of the incident location. Zheng et al. learn neural representations for participating media with a complete simulation of global illumination. They estimate direct illumination via ray tracing and compute indirect illumination with Spherical Harmonics. Zhu et al.  propose a novel framework for learning the radiance transfer field via volume rendering and utilizing various appearance cues to refine the geometry end-to-end. They extend relighting and reconstruction prospects to tackle a wider range of materials in a data-driven manner. They use a NeRF-like architecture to represent subsurface scattering based on known incident light directions from controlled acquisition. Similarly, Neural Radiance Transfer Fields  use one light at a time (OLAT) data as supervision to learn global light transport. This data representation is very similar to ours, however, their dataset of translucent objects has not been released so far. Due to using a NeRF-only approach they lack editing capabilities and have longer run times. Object-Centric Neural Scattering Functions (OSF)  introduce a framework for representing and rendering objects under varying lighting conditions and from arbitrary viewpoints using object-centric neural scattering functions. OSFs allow for flexible composition of scenes with multiple objects, each maintaining realistic interactions with light and shadows but they also suffer from long training time, even their variant based on KiloNeRF . Further, in the realm of Gaussian Splatting, there have been works on human avatars incorporating some modeling of subsurface scattering with Spherical Harmonics .

Our method merges the implicit MLP-based shader representation for SSS with the efficiency of 3D GS. Although this is not the first time that 3D GS has been augmented with an implicit component, e.g. previously used to simulate view-dependent effects [41; 26], ours is the first to adapt it for modeling SSS.

## 3 Method

Our goal is to reconstruct photorealistic 3D objects with strong subsurface scattering (SSS) effects from multi-view, multi-light image sets and to render them in real-time. We propose a novel method that extends 3D Gaussian Splatting (3D GS) with an explicit surface appearance model and combines it with an implicit SSS model (Fig. 2).

### Background

3D Gaussian Splattingrepresents scene geometry and appearance using a set of 3D Gaussians . Rasterizing these Gaussians with an efficiently designed splatting technique allows for fast 3D reconstruction and novel view synthesis. A single 3D Gaussian is defined by its mean \(\), i.e. the center position in 3D space, and the covariance matrix \(\) expressing its rotation and scale following the Gaussian function

\[G(x)=(-(x-)^{}^{-1}(x-)), \]

Further, each Gaussian is associated with a color, modeled as Spherical Harmonics (SH) coefficients \(c_{i}\), to represent view-dependent effects and an opacity \(o\) for transparency. Consequentially, a set of 3D Gaussians is defined as \(=\{(_{i},_{i},c_{i},o_{i})\}\). Rendering a scene represented by 3D Gaussians is done in the first step by projecting the Gaussians onto the image plane , where \(J\) is the Jacobian of the affine approximation of the projective transformation and \(W\) is the viewing transformation matrix. The covariance matrix is transformed as follows:

\[^{}=JW W^{T}J^{T} \]

The second step is to accumulate and rasterize the Gaussians onto the image plane, which is done by alpha blending the splatted colors and opacities of the Gaussians. The final color \(C\) for pixel \((u,v)\) is given by the summation of the set of layered sequenced Gaussians \(_{i}\) contributing to the pixel:

\[C_{u,v}=_{i}(_{j=1}^{i-1}(1-_{j}))_{i}c_{i} \]

The \(\) term is constructed by multiplying the opacity \(o\) with the 2D covariance contribution \(^{}\) at a given pixel position. To facilitate optimization the covariance \(\) is parameterized as a 3D vector for scale \(s\) and a unit quaternion \(q\) for rotation.

Reliphtable 3D GaussiansIn Relightable 3D Gaussians (R3D GS)  the authors decompose the appearance of 3D Gaussians into explicit material properties, which can be relit in real-time. The method models light with a global neural incident light field  and a local learnable incident light representation based on SHs. The illumination and shading are optimized for a static scene with differentiable rendering.

Each Gaussian receives additional physically based rendering (PBR) parameters, such as a basecolor \(b\), a roughness \(r\), a metalness \(m\), and a normal \(n\). The approach adopts the Disney BRDF model  with the diffuse and specular terms as follows:

\[f_{}=, f_{}(_{o},_{i})=,h,b,m)G(_{o}, _{i},h,r)}{(_{o} n)(_{i} n)}. \]

Figure 2: **Subsurface Scattering Pipeline** - Our method implicitly models the subsurface scattering appearance of an object and combines it with an explicit surface appearance model. The object is represented as a set of 3D Gaussians, consisting of geometry and appearence properties. We utilize a small MLP to evaluate the subsurface scattering residual given the view and light direction and a subset of properties for each Gaussian. Further, we evaluate the incident light for each Gaussian as a joint task within the same MLP given the visibility supervised by ray-tracing. Based on the computed properties we accumulate and rasterize each property on the image plane in a deferred shading pipeline. We evaluate the diffuse and specular color with a BRDF model for every pixel in image space and combine it with the SSS residual to get the final color of the object.

Here \(D\) is the microfacet distribution function, \(F\) is the Fresnel term, \(G\) is the geometry term and \(h\) is the half vector between the view and light direction. The method first optimizes the geometry of the scene and then the shading parameters such as the incident light field and the PBR parameters. Further, the normal is derived from the geometry based on the depth in the scene. The authors additionally build efficient ray-tracing for 3D GS to supervise a learnable visibility SH term \(v\) per Gaussian, which guides the incident light field optimization.

While showing impressive relighting results on a variety of objects, it fails for translucent objects featuring SSS. The method is limited to learning from scenes with a single static illumination setting. Consequently, as SSS is not explicitly modeled, the SSS effect is baked-in into the basecolor parameter, preventing SSS effects from being correctly rendered during relighting (Fig. 5).

### Subsurface Scattering for 3D Gaussian Splatting

By building upon the 3D GS framework including R3D GS, our approach extends it to capture the SSS effects of objects. An implicit neural SSS model is jointly trained with the explicit surface BRDF model. We utilize a global neural network to estimate the SSS effects for each Gaussian in the scene and therefore approximate the internal light transport. At the same time, the neural network also takes care of the incident illumination on the object including local visibility to allow for fast evaluation.

SSS ModelingOur core contribution is the modeling of SSS effects in the 3D Gaussian representation. For photorealistic rendering, the internal scattering of light can be modeled with a BSSRDF  as simulated in the rendering equation as follows:

\[L_{}(_{},_{})=_{A}_{ }f_{}(_{},_{},_{ },_{})L_{}(_{},_{ })(_{} n)\,_{}\,x_{}, \]

Here \(f_{}\) is the BSSRDF, \(L\) is the radiance, \(\) is the position, \(\) are the directions, \(A\) is the surface area of the object, and \(\) is the hemisphere at \(_{}\). The light transport through the object is sketched on the left side of Figure 2. To exactly evaluate the scattered radiance due to the BSSRDF one would need to integrate all incoming light directions over the entire surface, which is expensive.

Rather than modeling the BSSRDF directly or explicitly evaluating the light transport by integrating over the surface we propose an implicit SSS shader. For a small surface area, in our case a 3D Gaussian, a neural network learns to estimate the outgoing radiance due to SSS when the entire scene is illuminated from a single direction. This global implicit network (Fig. 2) is formulated as follows:

\[(L_{\,()},_{})=f_{}(,,n, _{},_{},v), \]

where \(f_{}\) is the neural network, \(\) and \(\) are the mean and covariance of a 3D Gaussian, \(n\) is the normal, \(_{}\) and \(_{}\) are the incident and outgoing light directions, and \(v\) is the shadowing term (see below). We apply a Fourier encoding to \(\) as in  which is omitted in Equation 6. The network is not only estimating the outgoing SSS radiance \(L_{\,()}\) but at the same time the incident light \(_{}\) is predicted which will later be used to evaluate the direct reflection at \(\). We want to note that \(_{}\) is a prediction of our model that is close to the true physical quantity \(L_{}\) but might be slightly offset to compensate for limitations in the model. By forcing the network to predict the incident as well as the outgoing SSS radiance we let the network implicitly learn about the local and global light transport in the scene. \(L_{\,()}\) and \(_{}\) predictions share the same MLP but apply separate output heads (Fig. 2).

Besides this general modeling of the outgoing radiance, we introduce a local parameter \(sss\) per 3D Gaussian to control the ratio of SSS vs. direct reflection.

In order to constrain the network to predict physically plausible results, we combine it with the previously described BRDF model and optimize it jointly with the incident light field prediction. We formulate the combination of the BRDF and the SSS network as follows:

\[L_{}=sss L_{\,()}+(1-sss)(f_{}+f_{})_{}(n_{}), \]

where \(sss\) is subsurfaceness, which is optimized during training to balance the direct reflection and the SSS effects as formulated in OpenPBR .

Incident LightWe assume a single light source at a time (OLAT) setup, where the position of the light source is known. Similar to  we trace rays using a BVH to quickly estimate the visibility for each Gaussian and learn this as a per Gaussian SH visibility term \(v\). As explained above we model the incident light field with a neural network that takes the scene geometry, the light positions and the visibility into account. This network is jointly evaluated with our SSS radiance. We optimize the incident light field as one additional component of our differentiable rendering pipeline.

Per-Pixel Deferred ShadingInstead of the direct illumination model of the R3DGS  we introduce a deferred shading  to capture sharper highlights. We noticed that computing the BRDF model in Gaussian space is insufficient to capture high-frequency details such as specular highlights. This is because some Gaussians represent a large surface area as seen in Figure 5. A single point-wise evaluation of the BRDF model at the Gaussian's center as done by  is too sparse. Therefore, we propose a deferred shading approach for Gaussians, where we evaluate the BRDF model in image space after the rasterization of the Gaussians. The image space surface position is projected back to the 3D space to evaluate the BRDF model at this surface point. This way, high-frequency details such as specular highlights are properly reproduced. In addition, a large range of editing capabilities is enabled as pixel operations can be applied to the shading buffers.

With this formulation, we achieve joint learning of geometry and the direct and global appearance of SSS objects. See Appendix F for more details on the architecture and the training. Further, the choice of the 3D GS representation and our lightweight MLP combined with explicit PBR shading allow for real-time rendering speeds. Due to the deferred shading approach, we can capture high-frequency details such as specular highlights. Furthermore, the decomposition into explicit distinct appearance components allows for a high degree of editability, even controlling the degree of SSS effects.

## 4 Experiments

Our proposed method facilitates real-time rendering of SSS objects and enables relighting and material editing. In the following, we present results of our method which is evaluated on synthetic and real-world datasets.

### Experimental setup

DatasetWhile other NeRF-based SSS reconstruction and novel view relighting methods exist [46; 49] none of them provide a public dataset. We created a new OLAT dataset from synthetically rendered objects and real-world captured objects that capture various effects of SSS materials. In total, our datasets consist of \(20\) distinct objects from translucent material categories such as plastic, wax, marble, jade, and liquids (Sec. B).

For the **synthetic dataset** we created a synthetic light stage setup in Blender  that follows the formulation of . It models \(112\) fixed light positions on the upper hemisphere divided into \(7\) rings of \(16\) lights each. For training, we render \(100\) random camera views of each object with a fixed distance to the object. For testing, we use the NeRF synthetic camera path proposed by Mildenhall et al. , which consists of 200 camera views positioned outside the light stage hemisphere. In total,

Figure 3: **Results of Decomposition â€“ showing two different views with different light directions. Further, the decomposition of PBR parameters is shown. The first two objects shown are synthetic while the lower two are scanned real-world world objects.**

we have \(11.200\) train and \(22.400\) test images of \(800 800\) resolution for each object. We rendered datasets for \(5\) distinct objects with Blender's  Cycles renderer and the Principled BSDF shader. The 3D models are sourced from BlenderKit library .

The **real-world dataset** was captured in a light stage with a turntable supporting the object and a camera mounted on a motorized sled which can move on the vertical main arc of a sphere. Currently, the dataset includes \(15\) objects selected for a diverse representation of geometric detail and materials exhibiting SSS based on local availability and suitability for the acquisition setup in terms of rigidness and size. We captured \(158\) positions per object with \(167\) light positions each. Additionally, one image with uniform illumination was captured for camera pose optimization and object masking. The relative camera positions were optimized with COLMAP  resulting in an inward-facing \(360^{}\) multi-view dataset. The cameras were aligned to the known light source positions in a joined reference frame. Similar to the camera reconstruction, object masks are generated from the uniformly lit images using automatic image matting based on . Depending on the object we use a text prompt or points from the SfM stage as query to first generate pseudo trimaps leveraging SAM  and an open-vocabulary model . The matting is then performed by ViTMatte  yielding masks with transparency. See appendix section A for more information on the image processing pipeline. We will release more details when publishing the dataset.

A total of approximately 25,000 images are split evenly into a train and test set by uniform sampling from the camera and light positions. We exclude frames with strong light flares or incomplete or wrong masks based on heuristics and some manual annotation. In summary, we discard roughly 10 % of the dataset. We don't always use the full size of the datasets for training, as shown in Table 1. Our method can also be trained sparsely using only \(500\) images.

Implementation DetailsSS GS builds upon the 3D GS framework  and its extension Relightable 3D Gaussians (R3DGS)  that we in turn extend to capture the subsurface scattering effects of an object. For our implicit representation of the scattering component and the incident light prediction, we use a shallow MLP with \(3\) layers with Leaky-ReLU activations . The whole pipeline is implemented with Pytorch using the provided custom CUDA kernels from [19; 10] for rendering. For more details on the training setup see Appendix F.

### Qualitative Results

Our qualitative results are shown in Figure 3 and Figure 6. However, relighting and novel view synthesis are best experienced in the supplementary video. In 3 we show results rendered from our test set of objects from the synthetic (first two rows) and real-world (bottom two rows) part of our newly created dataset. In addition to novel view synthesis (view 0, view 1) we can freely change the light direction which we show for both views. We present the decomposition of the object into the surface appearance parameters, SSS effects and incident light map as they are used to render the final views on the left. Our method captures the SSS effect well using the SSS residual and adds the object's specularity from the PBR shading. Together with the predicted incident light the model achieves a plausible decomposition into basecolor, roughness and metalness material parameters. Note how the volumetric component can represent the complex light transport inside the entire volume. This is particularly observable in the case of the Tupperware object while the normals show a detailed representation of the surface. Find further quantitative analysis regarding the decomposition in section E.

ComparisonIn Figure 5 we show that Relightable 3D Gaussian (R3DGS)  fail to capture the subsurface scattering effects of the object. As they only allow training on a static scene without any dynamic lighting the scattering component is baked into the basecolor which will fail to represent the SSS for a new light position (see also Sec. C) Our method can capture the subsurface scattering effects and the specular highlights of the object. Achieving clearer results than R3DGS  in that regard due to the formulation of shading in image space. Figure 5 also visualizes the difference between shading in world space vs. our deferred shading approach in image space. As visualized on the left the Gaussians occupy different, sometimes very large areas along the surface limiting the rendering of high-frequency effects like specular highlights. NeRF-based methods have a lot more training capacity due to their large MLPs that can represent the complex light transport connected to SSS. For a fair comparison we select KiloOSF a variant of OSF  that is optimized for real-time rendering. Compared to KiloOSF  in Figure 6 it becomes apparent that also a small MLP paired with our explicit shading approach can achieve higher quality results after a shorter training time. Even after 20 hours of training, there are still some artifacts visible in the geometry that stem from the voxelization performed by the underlying KiloNeRF . While the Stanford Bunny object overall works well, the soap bar and car toy object are lacking in the representation of the surface reflectivity and show frequent errors in the geometry.

### Quantitative Results

We evaluate our method on the test split of the synthetic SSS dataset and the real-world SSS dataset on the task of novel view synthesis using the following metrics: PSNR, SSIM, and LPIPS. The results are shown in Table 1. Our method can successfully render novel view synthesis and relighting of subsurface scattering objects at real-time speeds, with high-quality results. Each component of our method is crucial to achieving such results, we show the ablation of the components in the appendix (Sec. C).

ComparisonWe compare our method against the state-of-the-art NeRF method KiloOSF  which claims to achieve rendering at real-time speeds (\(<14\) FPS). We use a single NVIDIA RTX 4090 GPU per run on a compute server with a total of 512 GB of RAM. We use the provided code and default configuration for the experiments, converting our datasets to their dataset specification. To be comparable and to fit within the framework of  we downscale our images to a resolution of \(256 256\) and only use \(500\) images of our synthetic dataset. We want to point out, that our method achieves similar results at real-time speeds on images of \(800 800\) resolution. In comparison, our method is faster to train since we can carry over the efficiency of the 3D GS framework by carefully designing our pipeline around a small MLP and an explicit BRDF decomposition. Similarly, we can save on the number of parameters per Gaussian since we directly store color instead of the SH coefficients, for example.

Up until now we only compare within the domain of real-time rendering and novel view synthesis, for SSS reconstruction and relighting, as our method is optimized for speed and editability through decomposition. Comparing the numbers of Tabel 1 with the results reported in subsurface NeRF-based methods that aim for quality  we still achieve similar results within the domain of synthetic scenes, and are en-par on real-world scenes. However, the datasets on which they reported the measurements are not publicly available. Therefore, we cannot directly compare.

### Applications

In Figure 4, we provide a detailed demonstration of the editing applications enabled by our approach. Our method facilitates adjustments to the base color and SSS residual, allowing for seamless color changes. Additionally, it supports the editing of material parameters, enabling modifications that make the appearance more metallic, shinier, or rougher. We also demonstrate the ability to alter the opacity and intensity of the SSS residual, and show single light illumination beyond the training domain. Moreover, our approach introduces editing capabilities, powered by the SSS residual, that surpass those available in previous methods .

 
**Method** & **Category** & **PSNR\(\)** & **SSIM\(\)** & **LPIPS\(\)** & **FPS** & **Train T.** & **Res.** & **Data** \\  
**Ours** & Synthetic & \(37.35 2.13\) & \(0.986 0.006\) & \(0.03 0.01\) & \(161.2 11.95\) & \(\) 2h & \(800^{*}\) & \(11.200\) \\
**Ours** & Real World & \(31.12 2.11\) & \(0.96 0.02\) & \(0.042 0.03\) & \(155.25 22.38\) & \(\) 2h & \(800^{*}\) & \( 12.000\) \\ 
**Ours** & Synthetic & \(\) & \(\) & \(\) & \(\) & \(<\) 1h & \(256^{2}\) & \(500\) \\ KiloOSF & Synthetic & \(25.91 1.88\) & \(0.93 0.02\) & \(0.097 0.03\) & \(14.4\) & \(>\) 20h & \(256^{2}\) & \(500\) \\ 
**Ours** & Real World & \(\) & \(\) & \(\) & \(\) & \(<\) 1h & \(256^{2}\) & \(500\) \\ KiloOSF & Real World & \(23.24 1.58\) & \(0.83 0.09\) & \(0.21 0.07\) & \(14.4\) & \(>\) 20h & \(256^{2}\) & \(500\) \\  

Table 1: Quantitative results on test views of our mehoad on large images and a bigger dataset (top) and comparison against KiloOSF  within their setting (bottom). We excluded runs of KiloOSF that couldnâ€™t reconstruct anything for more comparable results. The best results are highlighted in bold. All experiments where timed and run on a single NVIDIA RTX 4090. \({}^{*}\): cropped images with one side length of \(800\).

Find details on further qualitative and quantitative analysis of the application possibilities of our method such as image based relighting (Sec. D) in the appendix.

## 5 Limitations

Our approach reproduces SSS by modeling the apparent effect at the Gaussian where the light leaves the object. To avoid the costly integration over the whole object surface, this information is predicted by an MLP. Intricate variation due to strongly heterogeneous materials or angularly dependent SSS effects might only be roughly approximated. Furthermore, altering the geometry or the BSSRDF of the object would require retraining the SSS representation. While the screen space shading improves specular rendering significantly the shadowing is still limited by the low resolution

Figure 4: **Editing results, showcasing PBR based edits such as (roughness / metalness / base color) as well as method specific properties (subsurfaceness / residual color). The latter highlights editing only possible with this method. The rightmost column shows light positions not sampled from the light stage.**

Figure 5: **Limits of Reightable 3D Gaussians (left) â€“ While Relightable 3D Gaussians can reproduce view and illumination-dependent reflections they fail to properly relight subsurface scattering objects.**

**Deferred Shading (right) â€“ allows us to evaluate the surface reflectance for each rendered pixel instead of per Gaussian. This way, specular highlights are rendered with crisper detail.**

Figure 6: **Comparison against the KiloOSF  method. The top two objects are real-world objects and the bottom two synthetic objects. Note the qualitative improvement in shape and appearance compared to KiloOSF.**

of the 3D Gaussians in the currently used rendering scheme. Additionally, our screen space shading complicates the modeling of transparent refracting surfaces as it would require multiple passes to correctly warp the buffers. We do not explicitly account for this in our method. The white light assumption helps the severely under-constrained decomposition task. Still, while the decomposition is constrained to be physically plausible, it might not always match the ground truth as there are a multitude of possible explanations given only the appearance and light position. Moving forward we would like to also enable reconstruction under a less constrained illumination setting. Although not the intended use case and scope of this work our method could potentially be used to improve the scanning of humans as SSS is an integral property of the appearance of human skin. This has implications on personal rights and privacy and potential misuse that need to be addressed in these cases.

## 6 Conclusion

We present Subsurface Scattering (SSS) for 3D Gaussian Splatting (3D GS), a method to reconstruct translucent objects from OLAT multi-view image sets. By decomposing light transport into explicit PBR materials and an implicitly represented scattering component we enable novel view synthesis and relighting, as well as light and material editing in real-time. A per 3D Gaussian SSS parameter is learned to merge the two components. Our formulation enables a small MLP to reason about local and global light transport in the scene and predict incident light in addition to the SSS radiance. By evaluating the BRDF in image space we can achieve high-quality specular shading independent of the resolution of the 3D Gaussians. Compared to 3D GS we enable high-quality reconstructions for a new class of objects and achieve faster optimization and rendering speed than previous NeRF-based methods with similar or improved quality. Some limitations connected to the SSS representation and the rendering scheme remain which open up an interesting trajectory for future work. We also plan to release our dataset as the first OLAT SSS dataset including real-world translucent objects to the community.