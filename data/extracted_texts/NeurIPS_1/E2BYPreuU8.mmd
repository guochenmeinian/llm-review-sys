# On Mesa-Optimization in Autoregressively Trained Transformers: Emergence and Capability

Chenyu Zheng\({}^{1,2}\), Wei Huang\({}^{3}\), Rongzhen Wang\({}^{1,2}\), Guoqiang Wu\({}^{4}\),

**Jun Zhu\({}^{5}\), Chongxuan Li\({}^{1,2}\)**

\({}^{1}\) Gaoling School of Artificial Intelligence, Renmin University of China

\({}^{2}\) Beijing Key Laboratory of Big Data Management and Analysis Methods

\({}^{3}\) RIKEN AIP \({}^{4}\) School of Software, Shandong University

\({}^{5}\) Dept. of Comp. Sci. & Tech., BNRist Center, THU-Bosch ML Center, Tsinghua University

{cyzheng,wangrz,chongxuanli}@ruc.edu.cn; wei.huang.vr@riken.jp; guoqiangwu@sdu.edu.cn; dcszj@mail.tsinghua.edu.cn

Correspondence to Chongxuan Li.

###### Abstract

Autoregressively trained transformers have brought a profound revolution to the world, especially with their in-context learning (ICL) ability to address downstream tasks. Recently, several studies suggest that transformers learn a mesa-optimizer during autoregressive (AR) pretraining to implement ICL. Namely, the forward pass of the trained transformer is equivalent to optimizing an inner objective function in-context. However, whether the practical non-convex training dynamics will converge to the ideal mesa-optimizer is still unclear. Towards filling this gap, we investigate the non-convex dynamics of a one-layer linear causal self-attention model autoregressively trained by gradient flow, where the sequences are generated by an AR process \(_{t+1}=_{t}\). First, under a certain condition of data distribution, we prove that _an autoregressively trained transformer learns \(\) by implementing one step of gradient descent to minimize an ordinary least squares (OLS) problem in-context. It then applies the learned \(}\) for next-token prediction_, thereby verifying the mesa-optimization hypothesis. Next, under the same data conditions, we explore the capability limitations of the obtained mesa-optimizer. We show that a stronger assumption related to the moments of data is the sufficient and necessary condition that the learned mesa-optimizer recovers the distribution. Besides, we conduct exploratory analyses beyond the first data condition and prove that generally, the trained transformer will not perform vanilla gradient descent for the OLS problem. Finally, our simulation results verify the theoretical results, and the code is available at _[https://github.com/ML-GSAI/MesaOpt-AR-Transformer_](https://github.com/ML-GSAI/MesaOpt-AR-Transformer_).

## 1 Introduction

Foundation models based on transformers  have revolutionized the AI community in lots of fields, such as language modeling [2; 3; 4; 5; 6], computer vision [7; 8; 9; 10] and multi-modal learning [11; 12; 13; 14; 15]. The crux behind these large models is a very simple yet profound strategy named _autoregressive (AR) pretraining_, which encourages transformers to predict the next token when a context is given. In terms of the trained transformers, one of their most intriguing properties is the _in-context learning (ICL)_ ability , which allows them to adapt their computation and perform downstream tasks based on the information (e.g. examples) provided in the context without any updates to their parameters. However, the reason underlying the emergence of ICL ability is still poorly understood.

Recently, we are aware that some preliminary studies [16; 17] have attempted to understand the ICL ability from the AR training and connected its mechanisms to a popular hypothesis named _mesa-optimization_, which suggests that transformers learn some algorithms during the AR pretraining. In other words, the inference process of the trained transformers is equivalent to optimizing some inner objective functions on the in-context data.

Concretely, the seminal work  constructs a theoretical example where a single linear causally-masked self-attention layer with manually set parameters can predict the next token using one-step gradient-descent learning for an ordinary least squares (OLS) problem over the historical context. Moreover, they conduct numerous empirical studies to establish a close connection between autoregressively trained transformers and gradient-based mesa-optimization algorithms. Built upon the setting of , recent work  precisely characterizes the pretraining loss landscape of the one-layer linear transformer trained on a simple first-order AR process with a fixed full-one initial token. As a result, they find that the optimally-trained transformer recovers the theoretical construction in . However, their results rely on imposing the diagonal structure on the parameter matrices of the transformer and do not discuss whether the practical non-convex dynamics can converge to the ideal global minima. Besides, it is still unclear about the impact of data distribution on the trained transformer, which has been proven to be important in practice [19; 20; 21; 22] and theory .

In this paper, we take a further step toward understanding the mesa-optimization in autoregressively trained transformers. Specially, without an explicit diagonal structure assumption, we analyze the non-convex dynamics of a one-layer linear transformer trained by gradient flow on a controllable first-order AR process, and try to answer the following questions rigorously:

1. _When do mesa-optimization algorithms emerge in autoregressively trained transformers?_
2. _What is the capability limitation of the mesa-optimizer if it does emerge?_

Our first main contribution is to characterize a sufficient condition (Assumption 4.1) on the data distribution for a mesa-optimizer to emerge in the autoregressively trained transformer, in Section 4.1. We note that any initial token \(_{1}\) whose coordinates \(x_{1i}\) are i.i.d. random variables with zero mean and finite moments satisfy this condition, including normal distribution \((_{d},^{2}_{d})\). Under this assumption, the non-convex dynamics will exactly converge to the theoretical construction in  without any explicit structural assumption in , resulting in the trained transformer implementing one step of gradient descent for the minimization of an OLS problem in-context.

Our second main contribution is to characterize the capability limitation of the obtained mesa-optimizer under Assumption 4.1 in Section 4.2. We characterize a stronger assumption (Assumption 4.2) related to the moment of data distribution as the necessary and sufficient condition that the mesa-optimizer recovers the true distribution (a.k.a. predict the next token correctly). Unfortunately, we find that the mesa-optimizer can not recover the data distribution when the initial token is sampled from the standard normal distribution, which suggests that ICL by AR pretraining [16; 17] is different from ICL by few-shot learning pretraining [24; 25; 26; 27; 28; 29; 30; 23] (see details in Section 2), a setup attracting attention from many theorists recently. We think ICL in the setting of AR pretraining needs more attention from the theoretical community.

In Section 4.3, we further study the convergence of the training dynamics when Assumption 4.1 does not hold anymore by adopting the setting in . In this case, as a complement of , we prove that under a similar but weaker structural assumption, training dynamics will converge to the theoretical construction in  and the trained transformer implements exact gradient-based mesa-optimization. However, we prove that without any structural assumption, the trained transformer will not perform vanilla gradient descent for the OLS problem in general. Finally, we conduct simulations to validate our theoretical findings in Section 6.

## 2 Additional related work

### Mesa-optimization in ICL for few-shot linear regression

In addition to AR pretraining, much more empirical [31; 25; 32; 33] and theoretical studies [24; 26; 27; 28; 29; 34; 30] have given evidence to the mesa-optimization hypothesis when transformers are trained to solve few-shot linear regression problem with the labeled training instances in the context. On the experimental side, for example, the seminal empirical works by [31; 35] considersICL for linear regression, where they find the ICL performance of trained transformers is close to the OLS. On the theoretical side, considering a one-layer linear transformer, [26; 24] prove that the global minima of the population training loss is equivalent to one step of preconditioned gradient descent. Notably,  further proves that the training dynamics do converge to the global minima, and the obtained mesa-optimizer solves the linear problem. For multi-layer attention models, recent works suggest that they can perform efficient high-order optimization algorithms such as Newton's method [36; 37; 34]. Unfortunately, the pretraining goal of these studies is different from the AR training. Therefore, it is still unclear whether these findings can be transferred to transformers autoregressively trained on sequential data.

### Other explanations for ICL

In addition to the mesa-optimization hypothesis, there are other explanations for the emergence of ICL. [38; 39; 40; 41] explain ICL as inference over an implicitly learned topic model.  connects ICL to multi-task learning and establishes generalization bounds using the algorithmic stability technique.  and  study the implicit bias of the next(last)-token classification loss when each token is sampled from a finite vocabulary. Specially,  proves that self-attention with gradient descent learns an automaton that generates the next token by hard retrieval and soft composition.  explains ICL as kernel regression. These results are not directly comparable to ours because we study the ICL of a one-layer linear transformer with AR pretraining on the first-order AR process.

## 3 Problem setup

Elementary notations.We define \([n]=\{1,2,,n\}\). We use lowercase, lowercase boldface, and uppercase boldface letters to denote scalars, vectors, and matrices, respectively. For a vector \(\), we denote its \(i\)-th element as \(a_{i}\). For a matrix \(\), we use \(_{k:}\), \(_{:k}\) and \(A_{ij}\) to denote its \(k\)-th row, \(k\)-th column and \((i,j)\)-th element, respectively. For a vector \(\) (matrix \(\)), we use \(^{*}\) (\(^{*}\)) to denote its conjugate transpose. Similarly, we use \(}\) and \(}\) to denote their element-wise conjugate. We denote the \(n\)-dimensional identity matrix by \(I_{n}\). We denote the one vector of size \(n\) by \(_{n}\). In addition, we denote the zero vector of size \(n\) and the zero matrix of size \(m n\) by \(_{n}\) and \(_{m n}\), respectively. We use \(\) and \(\) to denote the Kronecker product and the Hadamard product, respectively. Besides, we denote \(()\) the vectorization operator in column-wise order.

### Data distribution

We consider a first-order AR process as the underlying data distribution, similar to recent works on AR pretraining [16; 17]. Concretely, to generate a sequence \((_{1},,_{T})^{d T}\), we first randomly sample a unitary matrix \(^{d d}\) uniformly from a candidate set \(_{}=\{(_{1},,_{d})| _{i}|=1, i[d]\}\) and the initial data point \(_{1}\) from a controllable distribution \(_{_{t}}\) to be specified later, then the subsequent elements are generated according to the rule \(_{t+1}=_{t}\) for \(t[T-1]\). For convenience, we denote the vector \((_{1},,_{d})^{}\) by \(\). We note that the structural assumption on \(\) is standard in the literature on learning problems involving matrices [46; 17]. In addition, it is a natural extension of the recent studies on ICL for linear problems [24; 26; 28; 29; 30], where they focus on \(y=^{}=_{d}^{}()\). Furthermore, adopting this new data model is a necessary approach to investigate AR pretraining because the regression dataset in the previous ICL theory is not suitable since each token \(_{i}\) does not have a relation with the context. In this paper, we mainly investigate the impact of the initial distribution \(_{_{1}}\) on the convergence result of the AR pretraining.

### Model details

Linear causal self-attention layer.Before introducing the specified transformer module we will analyze in this paper, we first recall the definition of the standard causally-masked self-attention layer , whose parameters \(\) includes a query matrix \(^{Q}^{d_{k} d_{a}}\), a key matrix \(^{K}^{d_{k} d_{a}}\), a value matrix \(^{V}^{d_{a} d_{a}}\), a projection matrix \(^{P}^{d_{e} d_{a}}\) and a normalization factor \(_{t}>0\). At time step \(t\), let \(_{t}=(_{1},,_{t})^{d_{e} t}\) be the tokens embedded from the prompt sequence \(_{t}=(_{1},,_{t})^{d t}\), the causal self-attention layer will output

\[_{t}(_{t};)=_{t}+^{P}^{V}_{t} (^{K}_{t})^{*}^{Q}_{t }}{_{t}})^{d_{e}},\]

where the \(\) operator is applied to each column of the input matrix. Similar to recent theoretical works on ICL with few-shot pretraining [24; 25; 16; 17; 26; 27; 28; 29], we consider the linear attention module in this work, which modifies the standard causal self-attention by dropping the \(\) operator. Reparameterizing \(^{KQ}=^{K*}^{Q}\) and \(^{PV}=^{P}^{V}\), we have \(=(^{KQ},^{PV})\), and the output can be rewritten as:

\[_{t}(_{t};)=_{t}+^{PV}_{t}_{t}^{*}^{KQ}_{t}}{_{t}}.\]

Though it is called linear attention, we note that the output \(_{t}(_{t};)\) is non-linear w.r.t. the input tokens \(_{t}\) due to the existence of the \(_{t}_{t}^{*}\). In terms of the normalization factor \(_{t}\), like existing works [24; 26], we take it to be \(t-1\) because each element in \(_{t}_{t}^{*}\) is a Hermitian inner product of two vectors of size \(t\).

Embeddings.We adopt the natural embedding strategy used in recent studies on AR learning [16; 17]. Given a sequence \(_{t}=(_{1},,_{t})\), the \(i\)-th token is defined as \(_{i}=(_{d}^{},_{i}^{},_{i-1}^{})^{} ^{3d}\), thus the corresponding embedding matrix \(_{t}\) can be formally written as:

\[_{t}=(_{1},,_{t})=_{d}&_{d} &&_{d}\\ _{1}&_{2}&&_{t}\\ _{0}&_{1}&&_{t-1}^{3d t},\]

where \(_{0}=_{d}\) as a complement. This embedding strategy is a natural extension of the existing theoretical works about ICL for linear regression [30; 24; 25; 26; 28; 29]. The main difference is that the latter only focus on predicting the last query token while we need to predict each historical token. We note that practical transformers do learn similar token construction in the first softmax attention layer (e.g., see Fig. 4B in ).

Next-token prediction.Receiving the prompt \(_{t}=(_{1},,_{t})\), the network's prediction for the next element \(_{t+1}\) will be the first \(d\) coordinates of the output \(_{t}(_{t};)\), aligning with the setup adopted in [16; 17]. Namely, we have

\[}_{t}(_{t};)=[_{t}(_{t};)]_{1:d}^{d}.\]

Henceforth, we will omit the dependence on \(_{t}\) and \(\), and use \(}_{t}\) if it is not ambiguous. Since only the first \(d\) rows are extracted from the output by the attention layer, the prediction \(}_{t}\) just depends on some parts of \(^{PV}\) and \(^{KQ}\). Concretely, we denote that

\[^{PV}=_{11}^{PV}&_{12}^{PV}&_{13}^{PV}\\ _{21}^{PV}&_{22}^{PV}&_{23}^{PV}\\ _{31}^{PV}&_{32}^{PV}&_{33}^{PV},^{KQ}= _{11}^{KQ}&_{12}^{KQ}&_{13}^{KQ}\\ _{23}^{KQ}&_{22}^{KQ}&_{23}^{KQ}\\ _{31}^{KQ}&_{32}^{KQ}&_{33}^{KQ},\]

where \(_{ij}^{PV},_{ij}^{KQ}^{d d}\) for all \(i,j\). Then the \(}_{t}\) can be written as

\[}_{t}=_{12}^{PV}&_{13}^{PV} _{t}^{}_{t}^{*}}{_{t}}_ {22}^{KQ}&_{23}^{KQ}\\ _{32}^{Q}&_{33}^{Q}_{t}^{ }. \]

Here \(_{t}^{}=(_{1}^{},,_{t}^{}) ^{2d t}\) denotes the last \(2d\) rows of the \(_{t}\), where \(_{i}^{}=(_{i}^{},_{i-1}^{})^{}\). Therefore, we only need to analyze the selected parameters in Eq. 1 during the training dynamics. The derivation of Eq. 1 can be found in Appendix A.1.

### Training procedure

Loss function.To train the transformer model over the next-token prediction task, we focus on minimizing the following population loss:

\[L()=_{t=2}^{T-1}L_{t}()=_{t=2}^{T-1}_{ _{1},}\|}_{t}-_{t+1}\|_{2} ^{2}, \]where the expectation is taken with respect to the start point \(_{1}\) and the transition matrix \(\). Henceforth, we will suppress the subscripts of the expectation for simplicity. The population loss is a standard objective in the optimization studies [24; 47], and this objective has been used in recent works on AR modeling [16; 17]. The summation starts from \(t=2\) because we do not have any information to predict \(_{2}\) given only \(_{1}\).

Initialization strategy.We adopt the following diagonal initialization strategy, and similar settings have been used in recent works on ICL for linear problem [24; 30; 17].

**Assumption 3.1** (Diagonal initialization).: _At the initial time \(=0\), we assume that_

\[^{KQ}(0)=_{d d}&_{d d}& _{d d}\\ _{d d}&_{d d}&_{d d}\\ _{d d}&a_{0}_{d}&_{d d} \,,^{PV}(0)=_{d d}&b_{0}_{d}& _{d d}\\ _{d d}&_{d d}&_{d d}\\ _{d d}&_{d d}&_{d d}\,,\]

_where the red submatrices are related to the \(_{t}\) and changed during the training process._

The most related paper  considers a stronger diagonal structure than ours, and it only investigates the loss landscape. Our results deepened the understanding of AR transformers by considering practical training dynamics. We think this assumption might be inevitable for a tractable analysis and leave theory for standard (Gaussian) initialization to future work. In Section 6, we also conduct experiments under standard initialization, which further supports the rationality of Assumption 3.1.

Optimization algorithm.We utilize the gradient flow to minimize the learning objective in Eq. 2, which is equivalent to the gradient descent with infinitesimal step size and governed by the ordinary differential equation (ODE) \(}{}=- L()\).

## 4 Main results

In this section, we present the main theoretical results of this paper. First, in Section 4.1, we prove that when \(_{_{1}}\) satisfies some certain condition (Assumption 4.1), the trained transformer implements one step of gradient descent for the minimization of an OLS problem, which validates the rationality of the mesa-optimization hypothesis . Next, in Section 4.2, we further explore the capability limitation of the obtained mesa-optimizer under Assumption 4.1, where we characterize a stronger assumption (Assumption 4.2) as the necessary and sufficient condition that the mesa-optimizer recovers the true distribution. Finally, we go beyond Assumption 4.1, where the exploratory analysis proves that the trained transformer will generally not perform vanilla gradient descent for the OLS problem.

### Trained transformer is a mesa-optimizer

In this subsection, we show that under a certain assumption of \(_{_{1}}\), the trained one-layer linear transformer will converge to the mesa-optimizer [16; 17]. Namely, it will perform one step of gradient descent for the minimization of an OLS problem about the received prompt. The sufficient condition of the distribution \(_{_{1}}\) can be summarized as follows.

**Assumption 4.1** (Sufficient condition for the emergence of mesa-optimizer).: _We assume that the distribution \(_{_{1}}\) of the initial token \(_{1}^{d}\) satisfies \(_{_{1}_{_{1}}}[x_{1i}x_{12}^{r} x_{1 i_{n}}^{r}]=0\) for any subset \(\{i_{1},,i_{n} n 4\}\) of \([d]\), and \(r_{2}, r_{n}\). In addition, we assume that \(_{1}=[x_{1j}^{4}]\), \(_{2}=[x_{1j}^{6}]\) and \(_{3}=_{r j}[x_{1j}^{2}x_{1r}^{4}]\) are finite constant for any \(j[d]\)._

Finding Assumption 4.1 is non-trivial since we need to derive the training dynamics first. The key intuition of this assumption is to keep the gradient of the non-diagonal elements of \(_{32}^{KQ}\) and \(_{12}^{PV}\) as zero, thus they can keep diagonal structure during the training. We note that any random vectors \(_{1}\) whose coordinates \(x_{1i}\) are i.i.d. random variables with zero mean and finite moments of order 2, 4, and 6 satisfy this assumption. For example, it includes the normal distribution \((_{d},^{2}_{d})\), which is a common setting in the learning theory field [47; 48; 49; 50; 51]. Under this assumption, the final fixed point found by the gradient flow can be characterized as the following theorem.

**Theorem 4.1** (Convergence of the gradient flow, proof in Section 5).: _Consider the gradient flow of the one-layer linear transformer (see Eq. 1) over the population AR pretraining loss (see Eq. 2).

_Suppose the initialization satisfies Assumption 3.1, and the initial token's distribution \(_{_{1}}\) satisfies Assumption 4.1, then the gradient flow converges to_

\[_{22}^{KQ}}&_{23}^{KQ}}\\ _{32}^{KQ}}&_{33}^{KQ}}= _{d d}&_{d d}\\ _{d}&_{d d}, _{12}^{PV}}&_{13}^{PV}}= _{d}&_{d d}.\]

_Though different initialization \((a_{0},b_{0})\) lead to different \((,)\), the solutions' product \(\) satisfies_

\[=}{_{2}+}{T-2} _{t=2}^{T-1}}.\]

As far as we know, Theorem 4.1 is the first theoretical result for the training dynamics and the mesa-optimization hypothesis of autoregressive transformers. The technical challenge compared to existing ICL theory for regression  mainly has two parts. First, our data model breaks the independence between data at different times, which causes difficulty in decomposing and estimating the gradient terms. Second, we modify the embedding strategy (more dimensions), scale the attention model (much more parameters), and change the loss function (more terms) to perform the full AR pertaining. All these parts are not well studied in the literature and make the gradients more complicated.

Theorem 4.1 is also a non-trivial extension of recent work , which characterizes the global minima of the AR modeling loss when imposing the diagonal structure on all parameter matrices during the training and fixing \(_{1}\) as \(_{d}\). In comparison, Theorem 4.1 does not depend on the special structure, and further investigates when the mesa-optimizer emerges in practical non-convex optimization.

We highlight that the limiting solution found by the gradient flow shares the same structure with the careful construction in , though the pretraining loss is non-convex. Therefore, our result theoretically validates the rationality of the mesa-optimization hypothesis  in the AR pretraining setting, which can be formally presented as the following corollary.

**Corollary 4.1** (Trained transformer as a mesa-optimizer, proof in Appendix A.3).: _We suppose that the same precondition of Theorem 4.1 holds. When predicting the \((t+1)\)-th token, the trained transformer obtains \(}\) by implementing one step of gradient descent for the OLS problem \(L_{,t}()=_{i=1}^{t-1}\|_{i+1}- {x}_{i}\|^{2}\), starting from the initialization \(=_{d d}\) with a step size \(}{t-1}\)._

### Capability limitation of the mesa-optimizer

Built upon the findings in Theorem 4.1, a simple calculation (details in Appendix A.3) shows that the prediction of the obtained mesa-optimizer given a new test prompt of length \(T_{te}\) is

\[}_{T_{te}}=^{T_{te}-1}_{i}_{i}^{*}}{T_{te}-1}_{T_{te}}. \]

It is natural to ask the question: where is the capability limitation of the obtained mesa-optimizer, and what data distribution can the trained transformer learn? Therefore, in this subsection, we study under what assumption of the initial token's distribution \(_{_{1}}\), the one step of gradient descent performed by the trained transformer can exactly recover the underlying data distribution. First, leveraging the result from Eq. 3, we present a negative result, which proves that not all \(_{_{1}}\) satisfies Assumption 4.1 can be recovered by the trained linear transformer.

**Proposition 4.1** (AR process with normal distributed initial token can not be learned, proof in Appendix A.4).: _Let \(_{_{1}}\) be the multivariate normal distribution \((_{d},^{2}_{d})\) with any \(^{2}>0\), then the "simple" AR process can not be recovered by the trained transformer even in the ideal case with long training context. Formally, when the training sequence length \(T_{tr}\) is large enough, for any test context length \(T_{te}\) and dimension \(j[d]\), the prediction from the trained transformer satisfies_

\[E_{x_{1},W}[_{T_{te}})_{j}}{(Wx_{T_{te}})_{j}}].\]

_Therefore, the prediction \(}_{T_{te}}\) will not converges to the true next token \(_{T_{te}}\)._Proposition 4.1 suggests that ICL by AR pretraining [16; 17] is different from ICL by few-shot pretraining [24; 25; 26; 27; 28; 29; 30; 23], which attracts much more attention from the theoretical community. In the latter setting, recent works [24; 26] proves that one step of gradient descent implemented by the trained transformer can in-context learn the linear regression problem with input sampled from \((_{d},^{2}_{d})\). However, in the AR learning setting, the trained linear transformer fails.

This negative result shows that one-step GD learned by the AR transformer can not recover the distribution, but this can be solved by more complex models. Even for more complex data (\(\) is not diagonal),  has empirically verified that multi-layer linear attention can perform multi-step gradient descent to learn the data distribution. Therefore, to address the issue, future works are suggested to study more complex architecture such as softmax attention , multi-head attention [52; 53; 54], deeper attention layers [55; 16], transformer block [56; 57; 58], and so on. Future theory considering more complex AR transformers can adopt the same data model and token embeddings in this paper, and try to use a similar proof technique to derive the training dynamics.

Proposition 4.1 implies that if we want the trained transformer to recover the data distribution by performing one step of gradient descent, a stronger condition of \(_{_{1}}\) is needed. Under Assumption 4.1, the following sufficient and necessary condition related to the moment of \(_{_{1}}\) is derived from Eq. 3 by letting \(}_{T_{te}}\) converges to \(_{T_{te}}\) when context length \(T_{tr}\) and \(T_{te}\) are large enough.

**Assumption 4.2** (Condition for success of mesa-optimizer).: _Based on Assumption 4.1, we further suppose that \(}{_{2}}^{T_{te}-1}_{t}_{t}^{ }}{T_{te}-1}_{T_{te}}_{T_{te}}\) for any \(_{1}\) and \(\), when \(T_{te}\) is large enough._

Assumption 4.2 is strong and shows the poor capability of the trained one-layer linear transformer because common distribution (e.g. Gaussian distribution, Gamma distribution, Poisson distribution, etc) always fails to satisfy this condition. Besides, it is a sufficient and necessary condition for the mesa-optimizer to succeed when the distribution \(_{_{1}}\) has satisfied Assumption 4.1, thus can not be improved in this case. We construct the following example that satisfies Assumption 4.2.

_Example 4.1_ (sparse vector).: If the random vector \(_{1} R^{d}\) is uniformly sampled from the candidate set of size \(2d\)\(\{(c,0,,0)^{},(0,c,,0)^{},(0,,0,c)^{}\}\) for any fixed \(c\), then the distribution \(_{_{1}}\) satisfies Assumption 4.2. The derivation can be found in Appendix A.5.

For completeness, we formally summarize the following distribution learning guarantee for the trained transformer under Assumption 3.1 and 4.1.

**Theorem 4.2** (Trained transformer succeed to learn the distribution satisfies Assumption 4.2, proof in Appendix A.6).: _Suppose that Assumption 3.1 and 4.1 holds, then Assumption 4.2 is the sufficient and necessary condition for the trained transformer to learn the AR process. Formally, when the training sequence length \(T_{tr}\) and test context length \(T_{te}\) are large enough, the prediction from the trained transformer satisfies_

\[}_{T_{te}}_{T_{te}}, T_{tr},T_{te} +.\]

### Go beyond the Assumption 4.1

The behavior of the gradient flow under Assumption 4.1 has been clearly understood in Theorem 4.1. The follow-up natural question is what solution will be found by the gradient flow when Assumption 4.1 does not hold. In this subsection, we conduct exploratory analyses by adopting the setting in , where the initial token \(_{1}\) is fixed as \(_{d}\).

First, sharing the similar but weaker assumption of , we impose \(_{32}^{KQ}\) and \(_{12}^{PV}\) to stay diagonal during training by masking the non-diagonal gradients, then the trained transformer will perform one step of gradient descent, as suggested by . Formally, it can be written as follows.

**Theorem 4.3** (Trained transformer as mesa-optimizer with non-diagonal gradient masking, proof in Appendix A.7).: _Suppose the initialization satisfies Assumption 3.1, the initial token is fixed as \(_{d}\), and we clip non-diagonal gradients of \(_{32}^{KQ}\) and \(_{12}^{PV}\) during the training, then the gradient flow of the one-layer linear transformer over the population AR loss converges to the same structure as the result in Theorem 4.1, with_

\[=_{t=2}^{T-1}}.\]

_Therefore, the obtained transformer performs one step of gradient descent in this case._Theorem 4.3 can be seen as a complement and an extension of Proposition 2 in  from the perspective of optimization. We note that  assumes all the parameter matrices to be diagonal and only analyzes the global minima without considering the practical non-convex optimization process.

Next, we adopt some exploratory analyses for the gradient flow without additional non-diagonal gradient masking. The convergence result of the gradient flow can be asserted as the following proposition. The key intuition of its proof is that when the parameters matrices share the same structure as the result in Theorem 4.1, the non-zero gradients of the non-diagonal elements of \(_{32}^{KQ}\) and \(_{12}^{PV}\) will occur. In addition, we note the result does not depend on Assumption 3.1.

**Proposition 4.2** (Trained transformer does not perform on step of gradient descent, proof in Appendix A.8).: _The limiting point found by the gradient does not share the same structure as that in Theorem 4.1, thus the trained transformer will not implement one step of vanilla gradient descent for minimizing the OLS problem \(_{i=1}^{t-1}\|_{i+1}-W_{i}\|^{2}\)._

To fully solve the problem and find the limiting point of the gradient flow in this case (or more generally, any case beyond Assumption 3.1 and 4.1), one can not enjoy the diagonal structure of \(_{32}^{KQ}\) and \(_{12}^{PV}\) anymore. When \(_{32}^{KQ}\) and \(_{12}^{PV}\) are general dense matrices, computation of the gradient will be much more difficult than that in Proposition 4.2. Therefore, we leave the general rigorous result of convergence without Assumption 3.1 and 4.1 for future work.

We are aware that recent theoretical studies on ICL for linear regression have faced a similar problem. [24; 26; 23] find that when the input's distribution does not satisfy Assumption 4.1 (e.g., \((_{d},)\)), the trained transformer will implement one step of preconditioned gradient descent on for some inner objective function. We conjecture similar results will hold in the case of in-context AR learning. We will empirically verify this conjecture when \(_{1}\) is a full one vector, in Section 6.

## 5 Proof skeleton

In this section, we outline the proof ideas of Theorem 4.1, which is one of the core findings of this paper, and also a theoretical base of the more complex proofs of Theorem 4.3 and Proposition 4.2. The full proof of this Theorem is placed in Appendix A.2.

The first key step is to observe that each coordinate of prediction \(}_{t}\) (Eq. 1) can be written as the output of a quadratic function, which will greatly simplify the follow-up gradient operation.

**Lemma 5.1** (Simplification of \(_{t,j}\), proof in Appendix A.2.1).: _Each element of the network's prediction \(_{t,j}\) (\(j[d]\)) can be expressed as the following._

\[_{t,j}=_{j}^{}_{t}^{} _{t}^{}_{t}^{}}{_{t}}()=^{}()_{t}^{} _{t}^{}_{t}^{}}}{_{t}} {B}_{j},\]

_where the \(\) and \(_{j}\) are defined as_

\[=_{1}&&_{2d}= _{22}^{KQ}&_{23}^{KQ}\\ _{32}^{KQ}&_{33}^{KQ},_{j}= _{j1}\\ _{j2}=_{12}^{PV}\\ _{13,j}^{PV},\]

_with \(_{i}^{2d}\) and \(_{j1},_{j2}^{d}\)._

Next, We calculate the gradient for the parameter matrices of the linear transformer and present the dynamical system result, which is the most complex part in the proof of Theorem 4.1.

**Lemma 5.2** (dynamical system of gradient flow under Assumption 4.1, proof in Appendix A.2.2).: _Suppose that Assumption 4.1 holds, then the dynamical process of the parameters in the diagonal of \(_{32}^{KQ}\) and \(_{12}^{PV}\) satisfies_

\[}{}a =-ab^{2}(T-2)_{2}+_{t=2}^{T-1} _{3}+b(T-2)_{1},\] \[}{}b =-a^{2}b(T-2)_{2}+_{t=2}^{T-1} _{3}+a(T-2)_{1},\]

_while the gradients for all other parameters were kept at zero during the training process._Similar ODEs have occurred in existing studies, such as the deep linear networks  and recent ICL for linear regression . Notably, these dynamics are the same as those of gradient flow on a non-convex objective function with clear global minima, which is summarized as the following.

**Lemma 5.3** (Surrogate objective function, proof in Appendix A.2.3).: _Suppose that Assumption 4.1 holds and denote \((T-2)_{2}+_{t=2}^{T-1}_{3}\) and \((T-2)_{1}\) by \(c_{1}\) and \(c_{2}\), respectively. Then, the dynamics in Lemma 5.2 are the same as those of gradient flow on the following objective function:_

\[(a,b)=}(c_{2}-c_{1}ab)^{2},\]

_whose global minimums satisfy \(ab=c_{2}/c_{1}\)._

Furthermore, We show that although the objective \((a,b)\) is non-convex, the Polyak-Lojasiewicz (PL) inequality [60; 61] holds, which implies that gradient flow converges to the global minimum.

**Lemma 5.4** (Global convergence of gradient flow, proof in Appendix A.2.4).: _Suppose that Assumption 4.1 holds, then \((a,b)\) is a non-convex function and satisfies the PL inequality as follows._

\[|(a,b)|^{2}+| {}{ b}(a,b)|^{2} 2c_{1}(a^{2}+b^{2}) (a,b)-_{a,b}(a,b).\]

_Therefore, the gradient flow in Lemma 5.2 converges to the global minimum of \((a,b)\)._

Finally, Theorem 4.1 can be proved by directly applying the above lemmas.

## 6 Simulation results

In this section, we conduct simulations to verify and generalize our theoretical results. In terms of the train set, we generate 10\(k\) sequences with \(T_{tr}=100\) and \(d=5\). In addition, we generate another test set with 10\(k\) sequences of the same shape. We train for 200 epochs with vanilla gradient descent, with different diagonal initialization of \((a_{0},b_{0})\) by \((0.1,0.1)\), \((0.5,1.5)\), \((2,2)\). The detailed configurations (e.g., step size) and results of different experiments can be found in Appendix B.

**Initial token sampled from \((_{d},^{2}_{d})\).** We conduct simulations with \(=0.5,1,2\) respectively. With any initialization of \((a_{0},b_{0})\), simulations show that \(ab\) converges to \(_{1}/_{2}=1/5^{2}\), and \(}_{T_{te}-1}\) converges to \(_{T_{te}}/5\) in expectation, which verifies Theorem 4.1 and Proposition 4.1, respectively. In the main paper, we present the convergence results with \(=0.5\) in Fig. 0(a) and 0(b). We also verify our theory in the small-context scenarios (\(T_{tr}=5\)), which is placed in Fig. 4 in Appendix B.3.

**Initial token sampled from Example 4.1.** We conduct simulations with scale \(c=0.5,1,2\) respectively. With any initialization of \((a_{0},b_{0})\), simulations show that \(ab\) converges to \(_{1}/_{2}=1/c^{2}\) (see details in Appendix A.5), and \(}_{T_{te}-1}\) converges to the truth \(_{T_{te}}\), which verifies Theorem 4.1 and Theorem 4.2, respectively. In the main paper, we present the results with \(c=0.5\) in Fig. 0(c) and 0(d).

**Initial token fixed as \(_{d}\).** We conduct experiment with \(_{1}=_{d}\).The results Fig. 7 in Appendix B.5 show that \(_{32}^{KQ}\) and \(_{12}^{PV}\) converge to dense matrices with strong diagonals, and other matrices converge to \(_{d d}\), which means that the trained transformer performs somewhat preconditioned gradient descent. The detailed derivation is placed in Appendix B.5.

**Go beyond the diagonal initialization.** Finally, in order to extend our theory, we repeat experiments under Gaussian initialization with different variance (\(_{w}=0.001,0.01,0.1\)). The results of Gaussian start points and sparse start points (Example 4.1) can be found in Fig. 5 of Appendix B.3 and Fig. 6 of Appendix B.4, respectively. As a result, though the convergence results of parameters are not the same as those under diagonal initialization, they keep the same diagonal structure, which can be understood as GD with adaptive learning rate in different dimensions. In addition, the test results (ratio or MSE loss) under the standard Gaussian initialization are the same as those under diagonal initialization, which further verifies the capability limitation of the trained transformers. To sum up, these experimental results demonstrate that our theoretical results have a certain representativeness, which further supports the rationality of the diagonal initialization.

## 7 Conclusion and Discussion

In this paper, we towards understanding the the mechanisms underlying the ICL by analyzing the mesa-optimization hypothesis. To achieve this goal, we investigate the non-convex dynamics of a one-layer linear transformer autoregressively trained by gradient flow on a controllable AR process. First, we find a sufficient condition (Assumption 4.1) for the emergence of mesa-optimizer. Second, we explore the capability of the mesa-optimizer, where we find a sufficient and necessary condition (Assumption 4.2) that the trained transformer recovers the true distribution. Third, we analyze the case where Assumption 4.1 does not hold, and find that the trained transformer will not perform vanilla gradient descent in general. Finally, our simulation results verify the theoretical results.

**Limitations and social impact.** First, our theory only focuses on the one-layer linear transformer, thus whether the results hold when more complex models are adopted is still unclear. We believe that our analysis can give insight to those cases. Second, the general case where Assumption 3.1 and 4.1 does not hold is not fully addressed in this paper due to technical difficulties. Future work can consider that setting based on our theoretical and empirical findings. Finally, this is mainly theoretical work and we do not see a direct social impact of our theory.

## 8 Acknowledgement

This work was supported by Beijing Natural Science Foundation (L247030); NSF of China (Nos. 62076145, 62206159); Beijing Nova Program (No. 20230484416); Major Innovation & Planning Interdisciplinary Platform for the "Double-First Class" Initiative, Renmin University of China; the Fundamental Research Funds for the Central Universities, and the Research Funds of Renmin University of China (22XNKJ13); the Natural Science Foundation of Shandong Province (Nos. ZR2022QF117), the Fundamental Research Funds of Shandong University. The work was partially done at the Engineering Research Center of Next-Generation Intelligent Search and Recommendation, Ministry of Education. G. Wu was also sponsored by the TaiShan Scholars Program.

Figure 1: Simulations results on Gaussian and Example 4.1 show that the convergence of \(ab\) satisfies Theorem 4.1. In addition, the trained transformer can recover the sequence with the initial token from Example 4.1, but fails to recover the Gaussian initial token, which verifies Theorem 4.2 and Proposition 4.1, respectively.