# Near-Optimal Streaming Heavy-Tailed Statistical Estimation with Clipped SGD

Aniket Das

Stanford University

aniketd@cs.stanford.edu

Work done while at Google

Dheeraj Nagaraj

Google DeepMind

dheerajnagaraj@google.com

Soumyabrata Pal

Adobe Research

soumyabratap@adobe.com

Work done while at Google

Arun Sai Suggala

Google DeepMind

arunss@google.com

Prateek Varshney

Stanford University

vprateek@stanford.edu

###### Abstract

We consider the problem of high-dimensional heavy-tailed statistical estimation in the streaming setting, which is much harder than the traditional batch setting due to memory constraints. We cast this problem as stochastic convex optimization with heavy tailed stochastic gradients, and prove that the widely used Clipped-SGD algorithm attains near-optimal sub-Gaussian statistical rates whenever the second moment of the stochastic gradient noise is finite. More precisely, with \(T\) samples, we show that Clipped-SGD, for smooth and strongly convex objectives, achieves an error of \(()+()\|\|_{2}( (T)/)}}{T}}\) with probability \(1-\), where \(\) is the covariance of the clipped gradient. Note that the fluctuations (depending on \(}{{}}\)) are of lower order than the term \(()\). This improves upon the current best rate of \(()(}{{}})}{T}}\) for Clipped-SGD, known _only_ for smooth and strongly convex objectives. Our results also extend to smooth convex and lipschitz convex objectives. Key to our result is a novel iterative refinement strategy for martingale concentration, improving upon the PAC-Bayes approach of Catoni and Giulini .

## 1 Introduction

A fundamental problem in machine learning and statistics is the estimation of an unknown parameter of a probability distribution, given samples from that distribution. This can be expressed as the minimization of the expected loss: \(_{}F()_{ P}[f(; )],\) where \(\) represents the parameter to be estimated, \(P\) is the underlying probability distribution which can only be accessed through samples, and \(f(;)\) is a function which quantifies the loss incurred at a point \(\) by parameter \(\). In this paper, we focus on the setting where \(P\) is a heavy-tailed distribution for which the extreme values are more likely than in distributions like the Gaussian, \(f(;)\) is convex and the learner only has access to \(O(d)\) memory.

The heavy-tailed statistical estimation problem has received increased attention of late because of the prevalence of heavy-tailed distributions in many statistical applications dealing with real world data . The presence of such heavy-tailed distributions can significantly degrade the performance of statistical estimation and testing procedures designed under Gaussian (or sub-Gaussian) tail assumptions . This has spurred recent research efforts towards developing estimators specifically tailored for heavy-tailed settings (e.g., ; see Section 1.2 for a more detailed literature review). Despite substantial progress on this problem in recent years, much of theexisting work has concentrated on batch learning, where the entire dataset is available upfront, and the learner can revisit data points multiple times, without memory constraints. However, the streaming setting, where data arrives sequentially and must be processed with limited memory, is increasingly pertinent in the era of large-scale models. Consequently, in this work, we focus on understanding estimators for statistical estimation under heavy-tailed distributions, in the streaming setting.

A popular approach to study heavy-tailed streaming statistical estimation casts it as a stochastic convex optimization (SCO) problem with heavy-tailed gradients [17; 44; 52; 48] - with Clipped-SGD as the favored solution due to its simplicity . Indeed, clipping has become a standard component in the training of modern deep neural networks and thus, the properties of Clipped-SGD have been studied widely in the literature [1; 56; 38; 48; 52] in various contexts. Specifically, several works have shown that Clipped-SGD has sub-Exponential or sub-Gaussian tails despite the presence of heavy tailed noise in the gradient [45; 21; 52; 49]. Despite this progress, the best known rates for Clipped-SGD with smooth and strongly convex losses, under a bounded \(2^{}\) moment assumption on gradient distribution, are of the order \(()(}{{}})}{T}}\), where \(\) is the failure probability . Note that this is still far from the optimal sub-Gaussian rates of \(()+\|\|(}{{}})}{T}}\). In this work, we bridge this gap with a sharper analysis of Clipped-SGD for SCO problems, achieving nearly sub-Gaussian rates (see Section 1.1). Our approach leverages a novel technique obtained by bootstrapping the Donsker-Varadhan Variational Principle to Freedman's inequality, yielding tighter concentration inequalities for vector martingales compared to those in . This enables us to derive more refined rates for a variety of settings than a direct application of Freedman's inequality as in .

### Sub-Gaussian Error Guarantees for Statistical Estimation

**Mean Estimation** We motivate our style of results with the case of mean estimation. The Central Limit Theorem (CLT) posits that that the empirical mean of \(T\) independent and identically distributed (i.i.d) random variables with a finite covariance, behaves roughly like the empirical mean of Gaussian random variables with the same covariance, as \(T\). That is, the empirical mean \(\), the true mean \(\) and the covariance \(\) are such that \(_{T}(\|-\|>( )+\|\|_{2}()})\). However, these asymptotic rates need not hold with a practical number of samples. Therefore, recent works on heavy-tailed high dimensional mean estimation consider algorithms and non-asymptotic guarantees which move beyond the empirical mean (see [36; 10; 9; 27; 28; 15]). Estimators such as the clipped mean estimator [8; 55], trimmed mean estimator , and the geometric median-of-means estimator [39; 29] achieve an error of at-most \(()()}{T}}\) with probability \(1-\) with a finite covariance assumption. Recent ground breaking works [37; 28; 8; 36] further improve upon these results to construct estimators which can achieve the CLT convergence rates of \(C()+\|\|_{2}()}{T}}\) for every \(T\) and \(\). Some of these estimators work under just the assumption that the second moment is bounded [37; 28; 9] and some even provide a nearly linear time algorithm .

**General Statistical Estimation** In this work, we are interested in the general statistical estimation problem. Among the various approaches, framing this problem as SCO with heavy-tailed gradients has gained traction recently (see  and references there in). While one obvious candidate is to use SGD with state-of-the-art _optimal_ mean estimators for robust gradient estimation, such methods can face significant challenges. First, most optimal mean estimators aren't designed for the streaming data setting with batch-size being \(1\). Second, these estimators can be complex, frequently relying on semi-definite programming or other demanding techniques. Third, and perhaps most importantly, they don't typically provide guarantees on the bias of their estimates. This lack of bias control is problematic because SGD-style algorithms, even when equipped with accurate gradient estimates, can perform poorly if those estimates are systematically biased (See [3, Theorem 4], where bias does not cancel across iterations). Given these challenges, the clipped mean estimator of  has emerged as a popular choice for gradient estimation in SCO, due mainly to is simplicity. Several recent works analyze the performance of SGD with clipped mean estimator for the gradients (i.e, Clipped SGD). However, as previously mentioned, the best known analysis for clipped SGD achieves a sub-optimal rate of \(()(}{{}})/T}\), under bounded \(2^{}\) moment assumption. In this work, we improve upon these rates and show that with \(T\) samples, clipped-SGD obtains a sharper rate of \(()+()\|\|}()}{T}\) with probability \(1-\), which is closer to the truly sub-Gaussian rates.

### Related Work

**Clipped SGD** Clipped SGD and it's variants have been studied under a variety of settings including convex, strongly-convex, non-convex losses, with various assumptions on the moments of stochastic gradients. The estimators of [21; 45; 14; 40] work under the assumption of bounded \(2^{nd}\) moments, but require \(O(1/)\) batch size, to converge to an \(\)-approximate solution. Consequently, they are not suitable for streaming setting. The recent work of , which is closest to our work, addresses this issue by analysing Clipped-SGD for batch size \(1\) for smooth, strongly convex losses. But they achieve a sub-optimal rate of \(()(1/)/T}\). These rates are improved in our work (see Table 1 for a detailed comparison). Additionally, our work provides convergence rates for convex objectives that are not strongly convex. Recent works [48; 46; 41; 34; 13] have studied Clipped-SGD with the assumption that the stochastic gradient has a finite \(p\)-th moment for some \(p(1,2]\). They derive fine-grained near optimal results in terms of dependence of \(T\) and \(p\) (but their dependence on \(^{-1}\) is sub-optimal). In contrast, our work specifically the case considers \(p=2\) with a focus on improving the sub-Gaussian dependence in the high probability bounds in these works from \(()(1/)\) and approaching the truly sub-Gaussian rates for estimation 1.1.

**Heavy-tailed Estimation** Heavy-tailed estimation has a rich history in statistics and we only review some of the recent advances. Several recent works have studied the problem of heavy-tailed mean estimation, and have derived estimators that achieve sub-Gaussian rates under the bounded \(2^{}\) moment assumption [36; 10; 9; 27; 28; 15; 45]. Among these, the works of [15; 32] are particularly relevant to our work. The algorithm of  runs in linear time while requiring \(O(d^{-1})\) memory. But it is not immediately clear how to use their estimator in the framework of SGD.  study the trimmed mean estimator (an estimator that is closely related to clipped mean estimator, where outliers are removed instead of being clipped) and show that when \(T=(^{3}^{-1}),d=(^{2}(^{-1}))\), the estimator achieves the optimal rates. We not that our analysis of clipped SGD, when instantiated for mean estimation, leads to similar rates. But unlike  which is primarily focused on mean estimation, we focus on the more general SCO problem.

Heavy-tailed linear regression has been widely studied, with classical estimators based on Huber regression [30; 50; 33] known to provide optimal rates when the response variables are heavy-tailed, but the covariates are light-tailed. Recently, there has been a surge of interest in developing estimators when both covariates and response variables are heavy tailed [5; 44; 17; 43]. However, most of these works are in the batch setting. Another line of work has considered streaming algorithms in the Huber-contamination model, which is a much harder contamination model than heavy-tails . However, these algorithms when adapted to heavy-tailed setting, do not provide optimal rates.

   Method & Sample Complexity & Batchsize & Domain \\   Clipped SGD  & \((^{2}}{}(^{-1}+ ^{2}}{}))\) & \(O(^{2}}{})( ^{2}}{})\) & Unbounded \\  R-Clipped SGD  & \(+^{2}}{}( ^{-1}+^{2}}{})\) & \(O(^{2}}{ })\) & Unbounded \\  R-Clipped SSTM  & \(+^{2}}{}( ^{-1}+^{2}}{})\) & \(O(^{2}}{ })\) & Unbounded \\  RobustGD  & \(O\) with \(=^{2}}{}\) & \(O\) & Unbounded \\  proxBoost  & \(+^{2}}{}^{-1}\) & \(O\) & Unbounded \\  restarted-RSMD  & \(+^{2}}{}( ^{-1}+^{2}}{})\) & \(O^{-1}+^{2}}{ }\) & Bounded \\  Clipped SGD  & \(+}{}^{-1}\) & 1 & Unbounded \\  Clipped SGD (**Ours**) & \(}}{}+(^{-1}  T)}{}\) & **1** & **Unbounded** \\   

Table 1: Sample complexity bounds (for converging to an \(\) approximate solution) of various algorithms for SCO under heavy tailed stochastic gradients. Results are instantiated for smooth and strongly convex losses, and for the case where the gradient noise has bounded covariance equal to the Identity matrix. \(D_{1}\) is the distance of the initial iterate from the optimal solution. For readability, we ignore the dependence of rates on the condition number. Observe all prior works have \(d^{-1}\) dependence in the sample complexity.

### Contributions

**Iteratively Refined Martingale Concentration via PAC Bayes** Our key technical result obtains fine-grained concentration guarantees for vector-valued martingales by using the Donsker-Varadhan Variational Principle to iteratively refine baseline concentration inequalities. This allows us to sharpen the PAC Bayes bounds of Catoni and Giulini  (and its martingale based extensions like ), which were used to analyze the clipped mean estimator. We believe these iterative refinement arguments could be of independent interest for developing fine-grained concentration bounds.

**Sharp Analysis of Clipped SGD** Leveraging these fine-grained concentration results, We perform a fine-grained analysis of clipped SGD for heavy-tailed SCO problem obtain _nearly_ subgaussian performance guarantees in the streaming setting with a batchsize of \(1\) and \(O(d)\) space complexity. In particular, we demonstrate that the sub-optimality gap after \(T\) steps scales as \(()+()}(}{{}})\), improving upon the best known scaling of \(()(}{{}})\) obtained by prior works  only for smooth strongly convex problems. To the best of our knowledge, we derive the first such guarantees for smooth convex and lipschitz convex problems in the streaming setting.

**Streaming Heavy Tailed Statistical Estimation** We use the above results to develop streaming estimators for various heavy-tailed statistical estimation problems including heavy-tailed mean estimation as well as linear, logistic and Least Absolute Deviation (LAD) regression with heavy tailed covariates, all of which exhibit nearly subgaussian performance. Our mean estimation results improve upon the previous best known guarantees for trimmed mean based estimators [8; 52; 32] (either in performance or in generality) For heavy-tailed linear regression under the assumption of bounded \(4^{}\) moments for the covariates and bounded \(2^{}\) moments for the response, our rates significantly improve upon that of the previous best known streaming estimator . To the best of our knowledge, we develop the first known streaming estimators for heavy-tailed logistic regression and LAD regression which attain nearly subgaussian rates

## 2 Notation and Organization

We work with Euclidean spaces \(^{d}\) equipped with the standard inner product \(,\) and the induced \(_{2}\) norm \(\|\|\). For any matrix \(A^{m n}\), we use \(\|\|_{2}\) to denote its Euclidean operator norm \(\|\|=_{ 0}\|\|/\|_{}\|\). For \(A^{d d}\), we denote its trace as \((A)\). For any random vector \(\), we denote its covariance matrix as \([]\). We use \(,\) and \(\) to denote \(,\) and \(=\) respectively, upto universal multiplicative constants. We use \( f()\) to denote the gradient of a differentiable function For any convex function \(f\), we use \( f()\) to denote an arbitrary subgradient of \(f\) at \(\).

## 3 Background and Problem Formulation

Our work studies the Stochastic Convex Optimization (SCO) problem, described as follows: Let \(\) denote a closed convex subset of \(^{d}\) and let \(F:\) be a convex function. We aim to solve:

\[_{}F(),\] (SCO)

assuming access to a convex projection oracle \(_{}\) and a _stochastic gradient oracle_, which we define as follows: Let \(P\) denote a probability measure supported on an arbitrary domain \(\) from which we can draw samples. A stochastic gradient oracle for \(F\) is a function \(g:\), which, given a point \(\) and a sample \( P\) returns an unbiased estimate \(g(;)\) of \( F()\) i.e., \(_{ P}[g(;)]= F()\). If \(F\) is nondifferentiable, \(_{ P}[g(;)]= F()\). Note that we do not assume direct access to \( F()\), which may be expensive or intractable to compute. Our objective is to (approximately) solve SCO subject to a constraint on the number of samples we can draw from \(P\).

This is an alternative formulation of the statistical estimation problem by recognizing \(P\) as the data distribution, \(\) as the parameter space and defining the population risk \(F():=_{ P}[f(;)]\), where \(f\) denotes the sample-level loss function. The associated stochastic gradient oracle is \(g(;):= f(;),\  P\), which is usually easy to compute. As we shall discuss in Section 5, several statistical estimation problems such as mean estimation, linear regression, logistic regression and least absolute deviation regression naturally fit into the SCO framework.

[MISSING_PAGE_FAIL:5]

```
0: Initialization \(_{1}\), Horizon \(T\), Step Sizes \((_{t})_{t[T]}\), Clipping Level \(\)
1:for\(t[T]\)do
2:\(_{t} g(_{t};_{t}),_{t} P\)
3:\(_{t+1}_{}(_{t}-_{t} _{}(_{t}))\)
4:endfor
5:Last Iterate : Output \(_{T+1}\)
6:Average Iterate : Output \(}_{T}=_{t=1}^{T}_{t}\)
```

**Algorithm 1** Clipped Stochastic Gradient Descent

### Smooth Strongly Convex Objectives

Theorems 1 and 2, proved in, Appendix B and C respectively, derive high probability convergence bounds for smooth and strongly convex objectives with second moment assumption.

**Theorem 1** (Smooth Strongly Convex Objectives).: _Let the \(L\)-smoothness, \(\)-Strong Convexity and Bdd. \(2^{}\) Moment assumptions be satisfied. Then, for any \((0,}{{2}})\), the last iterate of Algorithm 1 run for \(T((d))\) iterations with stepsize \(_{t}=\) and clipping level \(=}{{2}})/)}D_{ 1}^{2}+}(()+( )\|\|_{2}}(}{{2}}(T/))}\)satisfies the following with probability at least \(1-\)_

\[\|_{T+1}-^{}\|}{T+}+ ()+()\| \|_{2}}(}{{2}})}{T+}} \]

_where \(\{^{2}(}{{2}}(T/ ))}{()},^{}{{2}}}(} {{2}}(T/),(}{{2}}(T)/)^{2}\}\)_

We use Theorem 1 to derive sharp rates for streaming heavy tailed mean estimation in Section 5.1 and the following result to derive sharp rates for streaming heavy tailed linear regression in section 5.2

**Theorem 2** (Smooth Strongly Convex Objectives with Quadratic Growth Noise Model).: _Let Assumptions \(\)-Strong Convexity, \(L\)-smoothness and \(QG\)\(2^{}\) Moment be satisfied and let \(=}{{}}\). For any \((0,}{{2}})\), the last iterate of Algorithm 1 run for \(T((d))\) iterations with step-size \(_{t}=\) and clipping level \(=}{{2}}(}{{2}})/)}D_{1}^{2}+}(T+)(d_{}+ }}(}{{2}}(T)/))}\)satisfies the following with probability at least \(1-\)_

\[\|_{T+1}-^{}\|}{T+}+ }+}}( }{{2}}(T)/))}{T+}} \]

\[\{}}{^{2}}, }}}{^{2}}(}{{2}}(}{{}}),}{}(}{{2}}( }{{}}),}}}{}( }{{2}}(T)/),\] \[}{{3}}}^{}{{3}}}d_ {}^{}{{3}}}}{^{}{{3}}}}(}{{2}}(}{{2}}),^{}{{2}}}(}{ {2}}(}{{2}})/),(}{{2}}(}{{2}})/^{2}),}{d_{}}(}{{2} }(}{{2}})/)\}\]

**Comparison to Prior Works** To the best of our knowledge, the result closest to Theorem 2 is [52, Theorem 1] which analyzes streaming strongly convex SCO and obtains a \(}{T+}+}( }{{2}})}{T+}}\) rate for \(}}{{2}}}{^{2}}\). We note that Theorem 2 obtains a significantly better confidence bound which is closer to the optimal subgaussian rate compared [52, Theorem 1].

**Extra \( T\) term:** Our bounds for the statistical error is of the form \(}+}}( }{{2}})/)}{T+}}\) which has an extra \( T\) factor in the lower order term. This is still sharper than prior works with bounds of the form \(}(}{{2}})}{T+ }}\) as long as \( T}}()\).

### Beyond Strongly Convex Objectives

Moving beyond strong convexity, we present Theorems 3 for smooth convex functions and 4 for Lipschitz convex function, proved in Appendix D and E respectively. To the best of our knowledge,these are the first results for streaming heavy-tailed convex SCO that exhibits near-subgaussian concentration without strong convexity.

**Theorem 3** (Smooth Convex Objectives).: _Let Convexity, \(L\)-smoothness and Bdd. \(2^{}\) Moment be satisfied. Then, for any \((0,}{{2}})\) and \(T((d))\), there exists an \((0,}{{2L}}]\) such that the average iterate of Algorithm 1 run for \(T\) iterations with step-size \(_{t}=\) and clipping level \(=}(()}+LD_{1})}{ (()/)}}\) satisfies the following with probability at least \(1-\):_

\[F(}_{T})-F(^{*})^{2}}{T}+D_{1} ()+}(( )}+LD_{1})((T)/)}{T}}+o_{T}(L,D_{1},)\]

_where \(o_{T}(L,D_{1},)\) represents terms that are of lower order in \(T\) (explicated in Appendix D)_

**Theorem 4** (Lipschitz Convex Objectives).: _Let Assumptions Convexity, \(G\)-Lipschitzness and Bdd. \(2^{}\) Moment be satisfied. Then, for any \((0,}{{2}})\) and \(T((d))\), there exists an \((0,}{{}}]\) such that the average iterate of Algorithm 1 run for \(T\) iterations with step-size \(_{t}=\) and clipping level \(=}(()}+G)}{( ()/)}}\) satisfies the following with probability at least \(1-\)_

\[F(}_{T})-F(^{*})G}{}+D_{1} ()+}(( )}+G)(()/)}{T}}+o_{T}(G,D_{1},)\]

_where \(o_{T}(G,D_{1},)\) represents terms that are lower order in \(T\) (explicated in Appendix E)_

**Remark:** We use Theorem 3 to design the first known streaming estimator for logistic regression with heavy-tailed covariates in Section 5.3 and Theorem 4 to design the first known streaming estimator for LAD regression with heavy-tailed covariates in Section 5.4.

**Remark:** In Theorems 3 and 4, the leading order term in the error is of the form: \(D_{1}()+}(()}+)(()/)}{T}}\), where \(\{G,LD_{1}\}\). Assuming \(G,D_{1},()}\), we note that the term dependent on the confidence level \((1/)\) is lower order compared to \(()\). To the best of our knowledge, this is the first work which establishes strong confidence bounds in the setting of SCO without strong convexity. Interestingly, our results also improve the best known rates for sub-Gaussian gradient noise. To be precise, [35, Theorem 3.1] shows a _weaker_ bound of \(^{2}(G^{2}+()())/T}\) in the setting of Theorem 4, but when the noise is sub-Gaussian.

## 5 Applications to Streaming Heavy Tailed Statistical Estimation

### Streaming Heavy-Tailed Mean Estimation

Consider streaming heavy tailed mean estimation with clipped SGD with access to \(N\) i.i.d samples from the distribution \(P\). Let \(=\), \(_{ P}[]=\). We further assume \([]\) and allow the higher moments to be infinite. As described in Appendix G.1, this is an SCO problem with the sample loss \(f(;)=\|-\|^{2}\). The population loss and the stochastic gradient are given by:

\[F()=\|-\|^{2}+(_{ P}[]); g(;)=-\]

The following result, proved in Appendix G.1 via an application of Theorem 1, shows that the last iterate of clipped SGD attains near-subgaussian rates for the heavy tailed mean estimation problem

**Corollary 1** (Heavy Tailed Mean Estimation).: _Under the stochastic gradient oracle described above, implemented using \(N((d))\) i.i.d samples \(_{1},,_{N} P\), the last iterate of Algorithm 1 when run under the parameter settings of Theorem 1 satisfies the following with probability at least \(1-\)_

\[\|_{N+1}-\|_{1}-\|}{N+}+()+()}((N)/)}{N+}}\]

_where \((}{{}})^{2}\)_

**Comparison to Prior Works** The clipped mean estimator of  and the clipped-SGD based estimator in  come with a guarantee of the form \(\|}-\|()( )}}{{N}}/{N}}\) with probability \(1-\). Our result in Corollary 1 obtains a sharper rate of convergence. In a recent work, Lee and Valiant  showed that the trimmed mean estimator achieves the optimal rate of \(()/N}\) when \(N=(^{3}^{-1}),d=(^{2}(^{-1}))\). Our result matches this optimal rate in those settings, but is considerably more general, as it holds for any \(N,d\).

### Streaming Heavy Tailed Linear Regression

In the current and subsequent sections, we use \(\) to denote the parameter of \(F\). Let \(=^{d}\). Given a target parameter \(^{*}\), \(P\) defines the following linear model:

\[ Q,\;[]=0,\;[^ {T}]= 0; y=,^{*}+,\; [|]=0,\;[^{2}|] ^{2}\]

In addition, we make the following bounded \(4^{}\) moment asumption on the covariates \(\)

\[[,^{4}] C_{4}([ ,^{2}])^{2}\; ^{d}\]

for some numerical constant \(C_{4} 1\). Note that we allow both the covariate \(\) and the target \(\) to be heavy tailed, assuming only finite moments of upto order \(4\) for \(\) and order \(2\) for \(\). The assumption \([]=0\) is only made for ease of presentation and our arguments easily adapt to \([] 0\). Our task is to estimate \(^{*}\) in a streaming fashion with access to \(N\) i.i.d samples from \(P\). As described in Appendix G.2, we reframe this problem as SCO under the sample loss \(f(;,y)=(,-y)^{2}\). The associated population loss \(F()\) and the stochastic gradient oracle \(g(;,y)\) are given by:

\[F()=(-^{*})^{T}(-^{*}); g (;,)=(,-y)\]

**Corollary 2** (Heavy Tailed Linear Regression).: _Under the stochastic gradient oracle described above, implemented using \(N(())\) i.i.d samples from \(P\), the last iterate of Algorithm 1 when run under the parameter settings of Theorem 2 satisfies the following with probability at least \(1-\):_

\[\|_{N+1}-^{*}\|-^{*}\|}{N+ }+()}()+()(}{{ }})}}{N+}}\]

_where \(\{^{2}()}{\|\|_ {2}},C_{4}^{2}()}{\|\|_{2}}}(( N)/),(}{{}})^{2}\}\) and \(=}{_{}()}\)_

To the best of our knowledge, [52, Corollary 4] is the only other streaming estimator for this problem with subgaussian-style concentration. Our result above significantly improves upon their rates of \(-^{*}\|}{N+}+( )}d(}{{}})}{N+}}\) with \(=C_{4}d^{2}(}{{}})\). Furthermore, our result is much closer to the optimal subgaussian rate and gracefully adapts to the _stable rank_ or effective dimension , i.e., \(d_{}=()}}{{\|\|_{2}}}\), therefore implying significant speedups over  in settings where \(d_{} d\).

### Streaming Heavy Tailed Logistic Regression

Let \(=^{d}\{0,1\}\) and given a target parameter \(^{*}\), \(P\) denote the following linear-logistic model:

\[ Q,\;[]=0,\;[^ {T}]; y((^{*}, ))\]

where \((t)=(1+e^{-t})^{-1}\). The covariates \(\) are heavy tailed, with only bounded second moments. The negative log likelihood of \(y|\) is given by \(f(;,y)=(1+(,))-y\, ,\). The objective of the logistic regression problem is to estimate \(^{*}\) by minimizing the population-level negative log likelihood:

\[F()=_{,y P}[(1+(, ))-y\,,]\]

which is minimized at \(^{*}\). Here, the stochastic gradient oracle is \(g(;,)=(,) -y\). The following result applies Theorem 3 to show that the output of clipped SGD attains near-subgaussian rates for heavy tailed logistic regression. We refer to Appendix G.3 for the proof.

**Corollary 3** (Heavy Tailed Logistic Regression).: _Under the stochastic subgradient oracle described above, realized using \(N((d))\) i.i.d samples from \(P\), the average iterate of Algorithm 1, when run under the parameter settings of Theorem 4 satisfies the following with probability at least \(1-\):_

\[F(_{N})-F(^{*}) D_{1}()+ }(()}+\|\|_{2}D_{1} )((N)/)}{N}}+o_{N}(,D_{1})\]

_where \(o_{N}(,D_{1})\) represents terms that are lower order in \(N\) (explicated in Appendix G.3_

Note that the standard analysis of SGD, with the assumption that \(\|\| R\) almost surely leads to a bound of the form [4, Proposition 5]: \(F(_{N})-F(^{*}))}}{}\)

### Streaming Heavy Tailed LAD Regression

Let \(=^{d}\). Given a target parameter \(^{*}\), \(P\) defines the following linear model:

\[ Q,\;[]=0,\;[^ {T}]; y=,^{*}+,\; (|)=0\]

We allow both the covariate \(\) and target \(y\) to be heavy tailed, assuming only bounded second moments for \(\). We do not assume any moment bounds on \(|\). The assumption \([]=0\) is made for the sake of clarity and can be straightforwardly relaxed. The Least Absolute Deviation (LAD) Regression problem involves estimating \(\) by solving SCO with the sample loss \(f(;,y)=|\;,-y|\). The stochastic subgradient oracle and population risk is given by:

\[g(;,)=(,- ), F()=[|\;- ^{*},-|]\]

where \((t)=\) for \(t 0\) and \((0)=0\). The following result, whose full statement and proof is presented in Appendix G.4, applies Theorem 4 to show that the average iterate of clipped SGD attains near-subgaussian rates for heavy tailed LAD regression. To the best of our knowledge, this is the first known streaming estimator for this problem.

**Corollary 4** (Heavy Tailed LAD Regression).: _Under the stochastic subgradient oracle described above, realized using \(N((d))\) i.i.d samples from \(P\), the average iterate of Algorithm 1, when run under the parameter settings of Theorem 4 satisfies the following with probability at least \(1-\):_

\[F(_{N})-F(^{*}) D_{1}() +()((N)/)}}{N}}+o_{N}(,D _{1})\]

_where \(o_{N}\) denotes terms that are lower order in \(N\) (explicated in Appendix G.4)_

## 6 Improved Martingale Concentration via Iterative Refinement

Our results are based on the following concentration result for \(^{d}\) valued martingales. The proof appears in Appendix F. Suppose \(M_{t}\) for \(t=0,,T\) is an \(^{d}\) valued martingale such that \(M_{0}=0\) almost surely, the difference sequence \(_{t}:=M_{t}-M_{t-1}\) is such that \(\|_{t}\|\) and \([_{t}_{t}^{*}|_{t-1}]=_{t}\) almost surely for every \(t=1,,T\) for some \(>0\). Assume that there exist deterministic sequences \(p_{1},,p_{T}\) and \(q_{1},,q_{T}\) such that \((_{t}) q_{t}\) and \(\|_{t}\| p_{t}\) almost surely.

**Theorem 5**.: _Let \(:=_{t=1}^{T}q_{t}\) and \(:=_{t=1}^{T}p_{t}\). Then, for any \((0,)\):_

\[(_{t T}\|M_{t}\| g(T,))\]

_Where \(g(T,)=C_{M}[}+}{}+}()]\) and \(K=((}{}+1)(d+1))+C_{M}\) for some universal constant \(C_{M}\)_

To prove this result, we first use Freedman's inequality  to obtain a coarse-grained \(g_{0}\) such that \((_{t}\|M_{t}\|>g_{0})\). We then iteratively refine this inequality via a PAC Bayesian  argument to show that \((_{t}\|M_{t}\|>g_{k+1}_{k})\), where \(_{k}=\{_{t}\|M_{t}\| g_{k}\}\) and \(g_{k+1}^{2}()+g_{k}\|(}{{}})}\). This iterative refinement strategy, proved in Theorem 14 is one of the main technical contributions of our work, which could be of independent interest. We arrive at Theorem 5 after \(K(T d)\) refinement steps.

**Remark** Theorem 5 is used to control the influence of the fluctuations introduced by clipped SGD. To this end, let \(_{t}\) be the centered version of \(_{}(_{t})\), ensuring \(\|_{t}\| 2\) almost surely. Suppose \(_{t}=\) for some fixed \(\) and let \(=)}\). Then, with probability \(1-\): \(_{t T}\|M_{t}\|()+T\|\|( {K}{})}\). This is sharper than the \(_{t T}\|M_{t}\|()()}\) guarantee implied by the Matrix Freedman inequality [51, Corollary 1.6].

## 7 Proof Sketch

We sketch our proof technique for the case of smooth convex functions considered in 3. We consider the SGD iterations \(_{1},,_{T}\) with clipped stochastic gradient at time \(t\) denoted by \(_{}(_{t})= F(_{t})+_{t} +_{t}\). Here, \(_{t}\) is the zero mean 'variance' such that \([_{t}|_{t}]=0\) and \(\|_{t}\| 2\) almost surely. \(_{t}\) is the non-zero mean 'bias' which arises due to clipping. Using the usual analysis of SGD for convex functions (see for instance ), we consider:

\[\|_{t+1}-^{*}\|^{2}\|_{t}-^{*}\|^ {2}-2_{t}[F(_{t})-F(^{*})]-2_{t}_ {t}+_{t},_{t}-^{*}+_{t}^{2}\| F( _{t})+_{t}+_{t}\|^{2}\]

Considering constant step-sizes, we sum the inequalities for each \(t\) to conclude:

\[_{t=1}^{T}F(_{t})-F(^{*}) \|_{1}-^{*}\|^{2}+ {1}{T}_{t=1}^{T}_{t}+_{t},_{t}- ^{*}\] \[+_{t}[\| F(_{t})\|^{2}+ \|_{t}\|^{2}+\|_{t}\|^{2}] \]

The 'random' terms to bound compared to gradient descent here are \(_{t}_{t}+_{t},_{t}-^{*}\) and \(_{t}\|_{t}\|^{2}+\|_{t}\|^{2}\) Lemma 13 shows that \(\|_{t}-^{*}\| 2\|_{1}-^{*}\|\) with high probability. Under this event, we bound \(_{t}_{t},_{t}-^{*}\) using the standard Freeman's inequality and \(\| F(_{t})\|^{2}\) by using smoothness and the fact that \( F(^{*})=0\). The bias of the estimator \(\|_{t}\|\) is bound using arguments similar to  (see Lemma 4). The main improvement of our method is given by our method of bounding \(_{t}\|_{t}\|^{2}\). We show by an application of Theorem 5 that \(_{t}\|_{t}\|^{2}()+()\|\|_{2}}()\) with probability at-least \(1-\) whenever the clipping factor \(\) is appropriately chosen. Choosing the step size \(\) appropriately gives us the result in Theorem 3.

## 8 Conclusion and Limitations

Our work obtained nearly subgaussian rates for heavy-tailed SCO using clipped SGD by developing a fine-grained iterative refinement strategy for martingale concentration. As corollaries, we obtained state-of-the-art streaming estimators for various heavy tailed statistical problems. We note Clipped-SGD is widely used to optimize neural networks with highly nonconvex landscapes, which is currently outside the scope of our work. Nevertheless, we believe our techniques could be useful for providing sharp high-probability guarantees for non-convex losses. Our bounds are currently of the form \(((T)/)}{T}}\), which is suboptimal compared to the tight subgaussian rate of \(}{{}})}{T}}\). Further research is required to understand if it is possible to obtain truly subgaussian rates with clipped mean type estimators. Another notable suboptimality of our result is the \((}{{}})\) dependence on the confidence level (as opposed to the typical \((}{{}})\) scaling). However, this is not a major drawback as our results continue to significantly outperform prior works unless \(T e^{(-1)(}{{}})}\) (which is an impractical regime). This drawback arises due to the \(((T))\) iterations of our iterative refinement technique and we believe it can be removed via more sophisticated martingale concentration arguments. Our work lays the foundation for several interesting avenues for future work including the analysis of heavy tailed statistical estimation under bounded \(p^{}\) moment assumptions (for \(p<2\)) and the development of parameter free statistical estimators that do not require knowledge of problem-dependent parameter such as \(\|\|,\) etc. (or their respective upper bounds). Deriving anytime valid guarantees for clipped SGD using our techniques is also an interesting future direction.