# Beyond Accuracy: Ensuring Correct Predictions With Correct Rationales

Tang Li   Mengmeng Ma   Xi Peng

DeepREAL Lab: [https://deep-real.github.io](https://deep-real.github.io)

Department of Computer & Information Science, University of Delaware

{tangli, mengma, xipeng}@udel.edu

###### Abstract

Large pretrained foundation models demonstrate exceptional performance and, in some high-stakes applications, even surpass human experts. However, most of these models are currently evaluated primarily on prediction accuracy, overlooking the validity of the rationales behind their accurate predictions. For the safe deployment of foundation models, there is a pressing need to ensure _double-correct predictions_, _i.e._, correct prediction backed by correct rationales. To achieve this, we propose a two-phase scheme: First, we curate a new dataset that offers structured rationales for visual recognition tasks. Second, we propose a rationale-informed optimization method to guide the model in disentangling and localizing visual evidence for each rationale, without requiring manual annotations. Extensive experiments and ablation studies demonstrate that our model outperforms state-of-the-art models by up to 10.1% in prediction accuracy across a wide range of tasks. Furthermore, our method significantly improves the model's rationale correctness, improving localization by 7.5% and disentanglement by 36.5%. Our dataset, source code, and pretrained weights: [https://github.com/deep-real/DCP](https://github.com/deep-real/DCP)

## 1 Introduction

Large foundation models, such as CLIP  and GPT-4V , exhibit exceptional performance or even surpass human experts in some high-stakes applications, such as medical diagnosis  and autonomous driving . However, most of these models are currently evaluated primarily on prediction accuracy, overlooking a critical aspect for ensuring safety, _i.e._, the validity of the reasons behind their accurate predictions. Understanding the _rationales_ - the "how" and "why" behind model predictions - is crucial for developing safe predictions. Fig. 1 shows typical examples of unsafe predictions: CLIP might predict accurately yet based on wrong rationales, whereas GPT-4V might make wrong predictions based on rationales that are plausible to humans. To build trust in real-world deployment, a natural question arises: _Can models make double-correct predictions, i.e., correct predictions backed by correct rationales?_

Correct rationales generally align with how humans would reason about the same decision and are based

Figure 1: Unsafe prediction examples. **Correct prediction, incorrect rationale**: CLIP identifies a red light, but wrongly based on red balloons. **Incorrect prediction, correct rationale**: GPT-4V incorrectly predicts a closed door, yet based on plausible visual evidence.

on valid _visual evidence_[6; 7; 8]. There are existing attempts to provide rationales for machine learning models' predictions. They either explicitly force the models to make decisions based on human-understandable concepts by introducing bottleneck layers [9; 10], or implicitly inject commonsense knowledge into models by contrastive learning between similar yet distinct textual concepts [11; 12]. However, none of them ensures _double-correct predictions._ Observations from our previous research  and recent studies in the field [14; 15] reveal that these models might provide _incorrect rationales_, as they fail to base the rationales on valid _visual evidence_.

To this end, we develop _double-correct predictions_ by focusing on two foundational aspects:

**i) "What" are the correct rationales? Structured rationale acquisition.** Existing vision datasets typically provide ground truth labels of predictions, whereas missing the rationales behind these decisions [16; 17]. To fill this gap, we curate a new dataset that offers over 4,000 unique _textual rationales_ designed for predicting the 1,000 categories in _ImageNet_, structured in a tree format. This design differs from existing knowledge graphs [19; 20; 21], which either provide irrelevant knowledge for the vision task or are too coarse-grained, providing insufficient information. Our rationale dataset is tailored to capture the detailed reasoning processes for visual recognition.

**ii) "Where" are the correct rationales? Rationale-informed optimization.** The other challenge in developing double-correct predictions is the absence of pixel-wise annotations for rationales' _visual evidence_. Although some datasets provide segmentation masks of object parts [17; 22], they lack sufficient rationale coverage and are limited to small-scale use cases . To address this issue, we propose a rationale-informed optimization method to guide the model in disentangling and localizing the visual evidence of rationales, without requiring manual annotations. Our method can be integrated into the existing model training process without architectural changes and extra parameters.

We evaluate the proposed method on a wide range of benchmark datasets and tasks. For prediction correctness, our model outperforms state-of-the-art models in zero-shot, linear probe, and fine-tuning settings by 2.6%, 2.0%, and 10.1%. For rationale correctness, the empirical results exhibit that our model significantly improves ground truth rationale localization and rationale disentanglability by 7.5% and 36.5%. Furthermore, the extensive qualitative results and ablation studies demonstrate the effectiveness of the proposed method.

Our contribution includes: **1)** We curate a new structured rationale dataset. **2)** A faithful explanation method tailored for explaining CLIP-ViT predictions. **3)** A principled optimization method that seamlessly integrates structured rationale information to develop _double-correct predictions_. **4)** Empirical results in a wide range of benchmark datasets and tasks including image classification and retrieval demonstrate the superior prediction and rationale correctness of our model.

## 2 Problem Formulation

In this section, we first formally define _rationales_, then provide the mathematical formulation of the _double-correct prediction_ problem.

**Definition 1** (_Rationales_) _Given a category \(y\), rationales are a set of \(K\) underlying abstract notions \(\{r_{k}^{y}\}_{k=1}^{K}\) and relations that capture the reasoning process leading to the recognition of \(y\)._

In the real world, rationales can be represented through textual descriptions [24; 25]. For example, when recognizing a specific breed of dog in an image, the rationales could be a set of concepts such as the shape of the ears, the color of the fur, and the size of the dog. Mathematically, given a textual rationale \(r\), we assume the existence of a ground truth labeling function \(V(x,r)\) that can provide the pixel-wise annotations of _visual evidence_ corresponding to \(r\) on an input \(x\).

**Definition 2** (_Double-Correct Predictions_) _A correct prediction is double-correct when it is backed by correct rationales that are based on valid visual evidence._

Denote \((x,y) P(X,Y)\) as a data point sampled from the training distribution \(P(X,Y)\), \(g()\) as an explanation method that attributes the prediction of text \(r\) to a group of pixels in input \(x\) depending on model \(f\), \(()\) as the task-specific loss function, and \(\) as a function class that is model-agnostic for the prediction task. To ensure the model \(f\) makes _double-correct prediction_, we propose to solve the following constrained optimization problem:

\[_{f}(f):=_{(x,y) P(X,Y)}[(f(x), y)]\ \ g(x,r;f)=V(x,r),\ \  r\{r_{k}^{y}\}_{k=1}^{K}. \]The problem in Eq. 1 is challenging to solve, since we neither have access to the rationales \(\{r_{k}^{y}\}_{k=1}\), nor to the ground truth labeling functions \(V()\). There are existing attempts that employ domain experts to manually collect textual descriptions of rationales [22; 26], or pixel-wise annotations of object parts on the image . However, these approaches are often limited to small-scale datasets, and impractical in large-scale settings due to the high cost of fine-grained annotations [27; 23].

## 3 Double-Correct Predictions

To bridge the gaps, in Sec. 3.1 we present how to acquire rationales \(\{r_{k}^{y}\}_{k=1}^{K}\), in Sec. 3.2 we propose a new explanation method \(g()\), and in Sec. 3.3 we develop _double-correct predictions_ without \(V()\).

### Structured Rationale Dataset

In this section, we curate a new rationale dataset to offer \(\{r_{k}^{y}\}_{k=1}^{K}\) in Eq. 1. According to Def. 1, rationales are structured human knowledge. Therefore, _ontologies_ that encapsulate complex, interconnected information while maintaining semantic relationships between entities [28; 29], present a proper tool to represent rationales. The benefits are bi-directional: **i)** in the human-to-machine direction, it offers a standardized, machine-readable format; **ii)** in the machine-to-human direction, ontology structure mirroring how humans organize and retrieve information to explain the model's decision-making process.

**Acquire structured rationales:** Different from existing works that are limited to small-scale manual annotation, we generate our rationale dataset in a scalable manner. Specifically, we utilize Large Language Models (LLMs) like GPT-4  to extract the structured rationales. Existing studies prove that GPT-4 has expert-level expertise in commonsense  and domain knowledge . However, we find that directly querying LLMs would yield inconsistent tree structures that can hardly be used by machine learning models. To address this issue, we provide a series of exemplary structured rationales before the query, employing in-context learning  to extract _standardized_ rationales in a.JSON format. See Appendix A for our full prompt and rationale examples.

**Rationale dataset statistics:** Our dataset covers all 1,000 categories in the _ImageNet_. For each category, we generate an ontology tree with a maximum height of two. As illustrated in Fig. 2, the root node is the category, the children of the root are the attributes, and the leaves are the sub-attributes. The edges represent the relationships between nodes. Combining attributes and sub-attributes, our dataset contains over 4,000 unique rationales. Our rationale ontology trees capture the reasoning processes leading to the recognition of the corresponding root categories.

**Can we trust the rationales extracted from GPT-4?** Although there are plenty of works showing GPT-4's remarkable capabilities [30; 31], it still could suffer from _hallucinations_[33; 34]. However, evaluations on the generation quality are largely missing from existing works that generate data from LLMs [9; 35; 36]. To fill this gap and ensure the quality of our rationale data, we conduct comprehensive human and machine evaluations. As detailed in Sec. 4.1, on a 5-point Likert scale across three metrics, 964 out of 1,000 categories are scored as having high-quality rationales (\(\)4.0).

In contrast to existing Knowledge Graphs [19; 20; 21] that either offer knowledge _unrelated_ to the visual prediction task, or are too coarse-grained that provide _insufficient_ information, our structured rationales are tailored for visual recognition tasks in a fine-grained attribute level. Furthermore, our dataset can expand to accommodate new rationales, providing flexibility to dealing with evolving datasets where more data becomes available. For example, our rationale ontologies can be seamlessly integrated following the _ImageNet_ category ontology derived from WordNet .

### Faithful Explanation Method

In this section, we develop a new explanation method to implement \(g()\) in Eq. 1. To incorporate both image and text inputs, we instantiate the model \(f\) using the CLIP-ViT architectures  because of

Figure 2: Our structured rationales capture the major attributes and their sub-attributes that lead to the recognition of objects. Our dataset offers over 4,000 unique rationales covering all 1,000 categories from _ImageNet_.

their proven capability . Existing methods for explaining the ViT model either directly use the attention maps as explanations , or weigh them using gradients . However, these methods might be _unfaithful_ to the ViT predictions. This is because the computation of each ViT prediction involves queries, keys, and values, whereas the attention maps only capture the inner products of queries and keys, ignoring information in values that also affect predictions . Therefore, explanations based on attention maps might not fully reflect the reasons behind ViT predictions.

**Decompose ViT outputs:** Recent works  prove that, for ViT models, the image embeddings can be decomposed into the contributions of each token within each attention head. Let \(\) and \(\) parameterize the image- and text-encoder of the CLIP-ViT model, \(P\) is the projection matrix, \(L\), \(M\), \(N\) are the numbers of layers, heads, and image tokens, \(a_{i}^{l,m}\) is the output of the \(m\)-th attention head in layer \(l\) for the \(i\)-th image token, then the embedding of image \(I\) can be decomposed as:

\[e_{I}=f_{}(I)=P(I)=_{l=1}^{L}_{m=1}^{M}_{i=0}^{N}Pa_ {i}^{l,m}. \]

By contracting along layers and heads,  calculates the contribution of the \(i\)-th image token to the final image embedding using \(_{l=1}^{L}_{m=1}^{M}Pa_{i}^{l,m}\).

**Faithful explanations weighted by mean-ablation results:** As indicated by our mean-ablation results in Fig. 3, the final layers contribute the most to the predictions, whereas the earlier layers have minimal impact. Thus, noise from early layers could obscure key information by a naive summation across all layers as in . To address this issue, we weigh each layer's contribution based on its importance, measured by the corresponding performance drop in the mean-ablation study. Denote the performance drop of layer \(l\) as \(_{l}\), we calculate the contribution of the \(i\)-th image token by:

\[e_{i}=_{l=1}^{L}w^{l}_{m=1}^{M}Pa_{i}^{l,m},\;\;w^{ l}=}{_{j=1}^{L}_{j}}. \]

Note that \(e_{i}\) is projected onto the image-text embedding space by \(P\). Thus, we can use \(g(I,r)=\{ e_{i},f_{}(r)\}_{i I}\) to calculate the explanations of rationale \(r\) on an image \(I\), _i.e._, _visual evidence_.

Our method significantly improves the explanation accuracy, as shown in Tab. 1. In contrast to attention-based explanations , our method fully utilizes the information from queries, keys, and values that are used for ViT predictions. Compared to gradient-weighted attention maps , our method cuts down the computational complexity from \(O(n^{2})\) to \(O(n)\) over \(n\) image tokens.

### Rationale-informed Optimization

In this section, we develop _double-correct predictions_ by disentangling and localizing rationales without pixel-wise human annotations \(V()\) in Eq. 1.

**Disentanglement via reconstruction:** Drawing insights from our previous research , we propose to contrast between explanation heatmaps of rationales to guide the model training in a _self-supervised_ manner. Specifically, we enforce the following two constraints: **i)** the image embeddings for different rationales within the same category are disentangled, and **ii)** the aggregated image embedding of all rationales within the same category aligns with the text embedding of the category.

   Exp. Methods & Pixel Acc. \(\) & mIoU \(\) & mAP \(\) \\  LRP  & 52.81 & 33.57 & 54.37 \\ rollout  & 60.63 & 40.64 & 74.47 \\ row attention & 65.67 & 43.83 & 76.05 \\ GradCAM  & 70.27 & 44.50 & 70.30 \\ Chefer et al.  & 69.21 & 47.47 & 78.29 \\ TextSpan  & 75.21 & 54.50 & 81.61 \\ Ours & **76.27** & **58.04** & **82.17** \\   

Table 1: Weakly-supervised segmentation accuracy on _ImageNet-Seg_. We threshold explanation heatmaps from CLIP-ViT-L-14 as segmentation masks. Our method outperforms existing explanation methods in segmentation accuracy, demonstrating the high faithfulness of our explanations.

Figure 3: Multi-head Self Attention (MSA) accumulated mean-ablation study. Based on Eq. 2, we replace the direct effects of MSAs up to a specific layer with their mean values calculated across the _ImageNet_ validation set. Most of the performance gains can be attributed to the final layers of the ViT.

Mathematically, the backbone objective is to learn a mapping function \(f\) such that for each image-text pair \((I,T) P(,)\), the embeddings \(f_{}(I)\) and \(f_{}(T)\) are aligned in a shared space if they are a correct match, where \(T\) is a text description of category \(y\). Let \(()\) be the InfoNCE loss . \(h(g(I,r))=_{i}e_{i}(g(I,r)_{i}>)\) extracts the image embedding of a given rationale. \((,)\) is a distance metric such as L2 distance. \(\), \(\), and \(\) are thresholding hyperparameters. For all \(r,r^{}\{r^{b}_{k}\}_{k=1}^{K}\), we propose to develop _double-correct predictions_ by optimizing:

\[_{f} (f):=_{(I,T) P(,)}[ (f_{}(I),f_{}(T))] 14.226378pt \] \[ (h(g(I,r)),h(g(I,r^{}))) }_{}, 5.690551pt( _{r}h(g(I,r)),f_{}(y))}_{}. 14.226378pt \]

Intuitively, the reconstruction term prevents the disentanglement from collapsing into trivial solutions, thereby ensuring localization. Solving Eq. 4 often leads to a non-convex problem, wherein methods such as stochastic gradient descent (SGD) cannot guarantee constraint satisfaction [49; 50]. To address this issue, we leverage Karush-Kuhn-Tucker (KKT) conditions [51; 52] and introduce Lagrange multipliers \(\) and \(\) to convert the constrained problem into its unconstrained counterpart:

\[_{f} \{(f):=_{(I,T) P(,)}[ (f(I,T))] \] \[ 14.226378pt+(h(g(I,r)),h(g(I,r^{}) ))\ +\ (_{r}h(g(I,r)),f_{}(y))\}.\]

Our method has the following merits: **i)** In contrast to existing works that rely on expensive pixel-wise annotations to localize objects [53; 54], the proposed rationale-informed optimization achieves a more fine-grained, attribute-level localization without manual annotations. **ii)** Our method can be integrated into vision-language model training without architectural changes and extra parameters.

## 4 Experiments

In this section, we first evaluate the quality of our curated rationale dataset in Sec. 4.1. To best validate _double-correct predictions_, we then conduct a series of experiments to compare the proposed method with existing methods in Secs 4.2 - 4.7. The experimental results prove that our model achieves superior prediction and rationale correctness on a wide range of benchmark datasets and tasks.

### Evaluation of Rationale Quality

**Metrics:** We focus on three essential aspects of the rationale quality. (1) _Factual Consistency_: whether the rationales are consistent with facts. (2) _Comprehensiveness_: whether the rationales provide sufficient information necessary to predict the category. (3) _Visual Disentanglement_: whether the rationales are visually disentanglable or non-overlap. We rate them on a 5-point Likert scale scoring system, where higher scores indicate better performance. For example, in Factual Consistency, score 5 means 100% of the generated rationales are consistent with facts, score 4 means 75%, score 3 means 50%, score 2 means 25%, and score 1 means completely wrong.

**Evaluators:** (1) _Human Evaluators_: We recruited four human evaluators, who are mostly graduate students. They are asked to conduct assessments based on commonsense knowledge and perform Internet searches for validation. On average, it takes them around one minute per sample. (2) _Machine Evaluators_: The latest GPT-4o and GPT-4v models (date accessed: Aug. 6th, 2024). For each evaluation, we perform three independent runs and calculate the average scores. Note that expanding human evaluations to the entire dataset is not scalable. To this end, we first prove the reliability of machine evaluations, then use it to automatically evaluation the entire dataset.

**Human evaluations:** We sample three independent groups of data from our rationale dataset, each consisting of 50 categories and their corresponding rationales. Specifically, categories were randomly selected from their superclasses: Animals (20), Objects & Artifacts (15), Natural Scenes (5), Plants (5), and Human Activities (5). This ensures that not only each superclass is represented but also that our results are robust . As shown in Tab. 2, The dataset consistently achieves scores of 4.61 or higher on the average of evaluators for each metric, indicating that over 90.3% of the rationales for each category are highly factual, comprehensive, and visually disentanglable.

**Machine evaluations:** Note that the scores of all three metrics are almost identical between machines and humans. The Pearson Correlation coefficient of 0.82 reveals the strong positive correlation between machine and human evaluators. Based on this observation, we further conduct machine evaluations on the entire dataset efficiently. Our results indicate that 964 out of 1,000 categories have high-quality rationales (\(\)4.0). See detailed results for the entire dataset in Appendix. B.

### Benchmark Datasets and Implementation Details

**Backbone model:** Due to the computational cost of training large vision-language models (VLMs) from scratch, we focus on fine-tuning experiments. Specifically, we fine-tune the ViT-B/32 variant of CLIP on the _ImageNet_ dataset combined with our curated rationale dataset. To maintain simple and interpretable rationales, the ontology graph for each category is limited to a maximum depth of two, allowing for the extraction of five to six independent concepts on average.

**Baseline models:** We compare our model with state-of-the-art VLMs that use ViT-B/32 as their vision encoders, including large-scale pretrained models (CLIP , DeCLIP ), knowledge-augmented model (NegCLIP ), and fine-grained alignment models (FILIP , PyramidCLIP ). For fair comparisons, we also compare our model with _ImageNet_ fine-tuned models using the same CLIP initialization and augmented text descriptions as our model, including full model fine-tuning (-ft) and vision-encoder-only fine-tuning (-ft-vision).

**Evaluation datasets:** We validate the prediction correctness of the models on image classification and image-text retrieval tasks. For image classification (zero-shot, linear probe), experiments are carried out on nine benchmark datasets, including _CUB_, _Caltech101_, _OxfordPets_, _Food101_, _SUN397_, _StanfordCars_, _DTD_, _CIFAR-10_, and _CIFAR-100_. For retrieval, we conduct experiments on _Flickr30K_ and _MSCOCO_. To evaluate the correctness of rationales, we evaluate the models' rationale localizability on _CUB-Part_ and _PartImageNet_ that provide ground truth segmentation masks of object parts, _e.g._, "head" and "body". Furthermore, we evaluate the rationale disentanglability on the aforementioned nine benchmark datasets. More details can be found in Appendix D.

**Implementation details:** We follow the same architecture design as CLIP  for ViT-B/32. The input resolution of image encoder is 224\(\)224 and the maximum context length of text encoder is 77. We train our model using an AdamW  optimizer and the cosine learning rate scheduler with a linear warmup. Specifically, the learning rate linearly increases from 0 to the peak value within 10% of the total steps, and then decreases with a cosine anneal strategy. Our learning rate is set to 5e-7 and train the model for eight epochs. More details can be found in Appendix D.

### Evaluation Metrics

**Prediction correctness:** We use standard category _prediction accuracy_ to evaluate the prediction correctness for zero-shot, linear probe, and fine-tuned settings.

**Rationale correctness:** We define two new metrics to measure rationale correctness.

i) _Rationale localizability._ We evaluate the correctness of rationales using ground truth segmentation masks of object parts . Following the standard evaluation protocol , we threshold the rationale explanation heatmaps to segmentation masks and calculate a mean Intersection over Union

   Evaluators & Factual Consistency & Comprehensiveness & Visual Disentanglement \\  GPT-4o & 4.89\(\)0.05 & 4.55\(\)0.06 & 4.66\(\)0.06 \\ GPT-4v & 4.92\(\)0.03 & 4.67\(\)0.05 & 4.70\(\)0.02 \\ Machine Avg. & 4.91 & 4.61 & 4.68 \\  Human-A & 4.85\(\)0.11 & 4.64\(\)0.19 & 4.42\(\)0.15 \\ Human-B & 4.97\(\)0.02 & 4.77\(\)0.02 & 4.20\(\)0.11 \\ Human-C & 4.78\(\)0.04 & 4.60\(\)0.11 & 4.78\(\)0.10 \\ Human-D & 4.81\(\)0.08 & 4.65\(\)0.05 & 4.77\(\)0.07 \\ Human Avg. & 4.85 & 4.66 & 4.54 \\   

Table 2: Evaluation results of rationale quality. Both machine and human evaluators receive the same instructions about the metrics. The scores for all three metrics are nearly identical between machine and human evaluators, indicating that over 90.3% of our rationales are of high quality.

(mIoU \(\)) score with the ground truth masks across different object parts. Specifically, the dynamic threshold \(=+\), where \(\) and \(\) are the mean and standard deviation of importance values of all pixels in a heatmap. The pixel with an importance value larger than \(\) is set to 1, otherwise 0.

ii) _Rationale disentanglability_. As shown in Fig. 4, for the CLIP model , the visual evidence of different rationales is entangled. Specifically, we treat the disentanglement between the visual evidence of different rationales as an important metric to evaluate whether the model can distinguish rationales. Specifically, we treat rationale explanation heatmaps \(\) and \(^{}\) as vectors and calculate \(1-|,^{}|\) as an intuitive measure of disentanglability, the higher metric value the better.

### Evaluation on Prediction Correctness

**Zero-shot image classification:** We compare our model against other state-of-the-art and fine-tuned VLMs on zero-shot image classification tasks. The results are shown in Tab. 3. On the average of nine datasets, our model outperforms the second-best result by 2.6%. The results indicate the strong transferability of our model to other vision datasets.

**Linear probe:** Following the common practice , we conduct linear probe experiments on the nine image classification datasets. As shown in Tab. 3, our model outperforms the second-best result by 2.0%. These results demonstrate the superior vision representations learned by our model.

**Fair comparison with fine-tuned models:** As shown in Tab. 3, our model outperforms the best fine-tuned model by 10.1% and 5.3% on zero-shot and linear probe results. This suggests that the proposed Rationale-informed Optimization is essential in improving the model's performance.

### Evaluation on Rationale Correctness

**Rationale localizability:** We compare our model with state-of-the-art and fine-tuned VLMs. As shown in Tab. 4, our model significantly improves the localization accuracy of rationales by 7.5% and 6.0% on _CUB-Part_ and _PartImageNet_. This suggests that even without using explicit region annotations, our method significantly enhances the model's localizability of rationales.

**Rationale disentanglability:** We compare the rationale disentanglement performance of our model with state-of-the-art and fine-tuned models. As shown in Tab. 5, on the average of nine image classification datasets, our model outperforms the second-best result by 36.5%. This significant improvement reveals that our model can distinguish between different rationales.

**Fair comparison with fine-tuned models:** To evaluate whether our model's performance gain can be obtained by solely introducing information from our rationale dataset, we conduct fair comparison

   Metrics & Models & C10 & C100 & CUB & CAL & PETS & F101 & SUN & CARS & DTD & AVG \\   & CLIP & **91.3** & 65.1 & 51.5 & 87.9 & 87.0 & **84.4** & 63.2 & 59.4 & 44.5 & 70.5 \\  & DecLIP & 91.2 & 66.4 & 51.2 & **89.5** & 79.5 & 74.6 & 63.4 & 50.6 & 42.7 & 67.7 \\  & NegCLIP & 85.7 & 60.9 & 37.4 & 81.0 & 79.7 & 71.1 & 57.0 & 45.4 & 37.5 & 61.7 \\  & FILIP & 86.9 & 65.5 & 37.5 & 91.9 & 88.1 & 82.8 & 69.1 & 55.4 & 49.3 & 69.6 \\  & PyramidCLIP & 81.5 & 53.7 & 52.7 & 81.7 & 83.7 & 67.8 & 65.8 & **65.0** & 47.2 & 66.6 \\   & CLIP-ft & 83.6 & 59.5 & 46.3 & 83.6 & 81.6 & 78.7 & 54.2 & 45.3 & 33.9 & 63.0 \\  & CLIP-ft-vision & 86.1 & 56.0 & 42.2 & 81.0 & 79.8 & 65.1 & 56.7 & 42.2 & 38.7 & 60.9 \\  & Ours & 90.8 & **68.1** & **56.0** & 89.3 & **88.5** & 84.3 & **70.6** & 62.3 & **47.7** & **73.1** \\   & CLIP & 95.1 & 80.5 & 71.4 & 93.0 & 90.0 & **88.8** & 76.6 & 81.1 & 76.5 & 83.7 \\  & DeCLIP & **96.5** & **84.7** & 65.0 & 94.8 & 89.2 & 85.0 & 75.0 & 81.6 & 78.5 & 83.4 \\   & NegCLIP & 94.3 & 79.3 & 71.8 & 98.7 & 89.5 & 85.6 & 78.6 & 75.0 & 81.3 & 83.8 \\   & FILIP & 95.1 & 82.4 & 77.0 & 99.1 & 88.3 & 83.4 & 78.7 & 76.8 & 88.3 & 85.5 \\   & PyramidCLIP & 96.0 & 82.5 & 72.3 & 96.4 & 87.8 & 83.3 & 77.5 & 82.6 & 77.3 & 84.0 \\    & CLIP-ft & 93.1 & 76.5 & 70.7 & 98.1 & 88.1 & 81.7 & 75.8 & 58.6 & 76.3 & 79.9 \\   & CLIP-ft-vision & 93.7 & 77.9 & 71.7 & 98.3 & 88.6 & 84.3 & 76.4 & 73.9 & 75.4 & 82.2 \\   & Ours & 95.6 & 82.7 & **77.2** & **99.3** & **92.9** & 88.1 & **79.8** & **83.0** & **88.9** & **87.5** \\   

Table 3: Comparison of prediction accuracy (%) on nine benchmark datasets. Our results are on the average of three trials of experiments using different random seeds. We highlight the **best results** and the _second best_ results. Surprisingly, different from most interpretability methods that compromise benchmark performance, our method also enhances prediction accuracy.

experiments with fine-tuned CLIP models. We compare our model with baseline (CLIP-zs), full model fine-tuning (CLIP-ft), and vision-only fine-tuning (CLIP-ft-vision). All fine-tuned models use the same CLIP initialization and receive the same language supervision as our model. As shown in Tabs. 4& 3, our model outperforms the best fine-tuned model by 9.8% and 41.1% on rationale localizability and disentanglability. This indicates that naive fine-tuning using augmented information without constraints would deteriorate the rationale correctness of the model.

**Qualitative results:** In Fig. 4, we show the visualizations of our visual evidence of different rationales. As shown, the rationales' visual evidence of the CLIP model  are entangled and mislocalized. In contrast, the rationales' visual evidence of our model are visually distinct and correctly localized.

### Ablation Study

**Ablation on rationale disentanglement:** The "w/o disen." refers to a variant of our method without rationale disentanglement constraint. As shown in Tab. 7, the rationale localizability decreased by 10.4%, indicating the model might not learn to distinguish between rationales without constraints.

**Ablation on reconstruction:** The "w/o recon." refers to a variant of our method without reconstruction constraint. As shown in Tab. 7, the rationale localizability and prediction accuracy drastically decreased by 13.3% and 30.2%. This reveals that recklessly optimizing the disentanglement between rationale can easily fall into trivial solutions.

**Generalize to different rationale sets:** According to DCLIP , using the text embeddings of concepts as a bottleneck layer to force the CLIP model  to predict based on them can improve prediction accuracy and interpretability. Specifically, the final prediction will be made by the average embedding similarity between the image and all concepts, namely \(=*{argmax}_{y}_{k=1}^{K} f_{}( I),f_{}(c^{y}_{k})\). We

    &  &  &  \\   &  &  &  &  &  &  &  &  \\  CLIP & 400M & 16.6 & 3.1 & 9.9 & 25.5 & 3.3 & 28.0 & 14.4 & 5.2 \\ DeCLIP & 88M & 6.9 & 2.0 & 5.1 & 16.2 & 1.5 & 18.4 & 8.3 & 3.7 \\ NegCLIP & 400M+COCO & 15.1 & 3.0 & 7.5 & 26.1 & 2.5 & 29.4 & 13.9 & 5.2 \\ FILIP & 340M & 10.3 & 2.4 & 7.3 & 20.5 & 3.4 & 23.7 & 11.2 & 4.0 \\ PyramidCLIP & 143M & 10.7 & 2.9 & 6.0 & 17.0 & 1.8 & 20.5 & 9.8 & 3.9 \\  CLIP-ft & 400M+IN & 13.5 & 3.3 & 5.8 & 22.9 & 2.1 & 25.5 & 12.1 & 4.5 \\ CLIP-ft-vision & 400M+IN & 7.4 & 2.5 & 7.9 & 26.4 & 1.6 & 22.0 & 11.3 & 4.4 \\ Ours & 400M+IN & **25.3** & **10.1** & **12.7** & **32.6** & **15.7** & **35.2** & **21.9** & **11.2** \\   

Table 4: Comparison of rationale localizability on _CUB-Part_ and _PartImageNet_. As detailed in Sec. 4.3, we threshold rationales’ explanation heatmaps as segmentation masks and calculate their mIoU (\(\)) with ground truth masks of corresponding object parts. Our model significantly improves the localization accuracy of fine-grained object parts. Full table in Appendix C.

Figure 4: Qualitative results of rationale disentanglement and localization. The rationales’ visual evidence of the CLIP model  typically highlights the entire object, lacking precise localization. In contrast, our model can correctly localize rationales, thereby enhancing trust in its predictions.

use the concept set provided in  rather than our training rationale dataset. As shown in Tab. 8, our model can generalize to an unseen concept set with improved prediction accuracy.

**Ablation using random string:** WaffleCLIP  shows that random concept strings as bottlenecks can achieve similar performance gains in DCLIP . We conduct an ablation study using the random strings provided by . As shown in Tab. 8, since our model can distinguish between different rationales, the random strings deteriorate the prediction accuracy of our model.

### Evaluation on Retrieval Tasks

**Zero-shot image-text retrieval:** We evaluate our model on zero-shot image-text retrieval tasks. As shown in Tab. 6, the improved rationale correctness also benefits retrieval tasks.

**Rationale-based text-to-image retrieval:** To better evaluate the rationale correctness of our model, we conduct a novel retrieval task: rationale-based text-to-image retrieval. The model should retrieve the image with a specified rationale presented. As shown in Fig. 5, in contrast to the CLIP model  that entangles rationales with specific categories, our model precisely understands the semantic meaning of rationales independent to categories.

## 5 Related Works

**Vision model explainability.** A widely adopted branch of explainability methods _post hoc_ generates heatmaps to identify the image regions most crucial to the model's predictions, _e.g._, GradCAM , LIME , and SHAP . Although useful for revealing the correlations between inputs and outputs, such explanations might be ambiguous, and fail to correspond to high-level concepts that humans easily understand . Methods like TCAV  curate attribute datasets to explain vision models using concepts familiar to humans. However, such methods can fail when the models do not learn these concepts . Another branch of methods attempts to design specific architecture to _intrinsically interpret_ model predictions, _e.g._, CBM  and ProtoPNet . However, they cannot guarantee the model learns the semantic meanings of the concepts correctly  and yield compromised prediction

    &  &  \\   & I2T & T2I & I2T & T2I \\  CLIP & 32.5 & 28.6 & 64.0 & 60.9 \\ DeCLIP & 32.6 & 22.1 & 59.8 & 46.2 \\ NegCLIP & - & - & 69.3 & 68.1 \\ FILIP & 33.6 & 36.4 & 52.9 & 53.3 \\ PyramidCLIP & 37.1 & **37.6** & 69.0 & **69.6** \\  CLIP-ft & 24.3 & 25.1 & 42.5 & 41.6 \\ CLIP-ft-vision & 25.9 & 27.2 & 49.1 & 55.5 \\ Ours & **38.4** & 37.1 & **69.5** & 68.9 \\   

Table 6: Comparison of zero-shot image-text retrieval accuracy (%). Double-correct prediction enhances the model’s visual understanding. (Note that NegCLIP is trained on _MSCOCO_)

   Models & C10 & C100 & CUB & CAL & PETS & F101 & SUN & CARS & DTD & AVG \\  CLIP & 0.249 & 0.445 & 0.475 & 0.442 & 0.540 & 0.481 & 0.519 & 0.353 & 0.287 & 0.420 \\ DeCLIP & 0.303 & 0.297 & 0.359 & 0.395 & 0.388 & 0.392 & 0.360 & 0.332 & 0.258 & 0.343 \\ NegCLIP & 0.386 & 0.319 & 0.456 & 0.401 & 0.440 & 0.491 & 0.495 & 0.389 & 0.261 & 0.404 \\ FILIP & 0.367 & 0.359 & 0.267 & 0.260 & 0.305 & 0.384 & 0.427 & 0.371 & 0.378 & 0.346 \\ PyramidCLIP & 0.299 & 0.300 & 0.428 & 0.418 & 0.391 & 0.318 & 0.397 & 0.359 & 0.283 & 0.355 \\  CLIP-ft & 0.378 & 0.346 & 0.469 & 0.389 & 0.434 & 0.374 & 0.383 & 0.339 & 0.251 & 0.374 \\ CLIP-ft-vision & 0.327 & 0.393 & 0.433 & 0.431 & 0.491 & 0.475 & 0.423 & 0.357 & 0.274 & 0.400 \\ Ours & **0.697** & **0.714** & **0.831** & **0.821** & **0.920** & **0.823** & **0.749** & **0.776** & **0.734** & **0.785** \\   

Table 5: Comparison of rationale disentanglement. We conduct experiments on nine image classification datasets. Our results are on the average of three trials using different random seeds.

   Models & mIoU (\(\)) & Acc. (\%) \\  Ours w/o disen. & 11.5 \(\) 1.3 & 43.3 \(\) 0.8 \\ Ours w/o recon. & 8.6 \(\) 0.8 & 25.8 \(\) 0.7 \\ Ours (full) & \(\) 1.6 & \(\) 0.5 \\   

Table 7: Ablation study on proposed constraints using _CUB-Part_ and _CUB_.

accuracy . Different from existing works, our method incorporates explanations to guide the model training, achieving accurate predictions backed by correct rationales.

**Knowledge augmentation for vision-language models.** Visual models often learn spurious correlations that stem from data biases unrelated to the causal explanation of interest [78; 79], whereas external knowledge allows models to learn the right features [77; 80]. Existing attempts for injecting knowledge into the models are often from the language modality. K-LITE  enrich the image caption using knowledge from WordNet  and Wiktionary . NegCLIP  and DANCE  improve the commonsense understanding of CLIP by generating hard negative captions, the latter uses knowledge from ConceptNet . StructureCLIP  leverages scene-graphs  to incorporate knowledge into text embeddings. However, our results (Tabs. 4& 7) reveal that solely augmenting information in the language cannot guarantee the model learning correct features. In contrast to these works, our method offers supervision signals from both modalities to ensure double-correctness.

**Contrastive vision-language alignment.** Different from conventional multimodal learning that fuses different modalities [84; 85], large-scale vison-language pretrained models, such as CLIP  and ALIGN , exhibit promising zero-shot transferability to downstream tasks. However, their global alignment objective is coarse, which only learns the existence of objects like bag-of-word while ignoring their localizations . Recent attempts like PyramidCLIP  and X-VLM  leverage object region annotations to align word phrases with image regions. DeCLIP  and FILIP  align text with image regions through self-supervised learning. However, their supervision is limited to a coarse, object-level granularity. Different from these works, our method offers fine-grained, concept-level supervisory signals of rationales without expensive manual annotations.

## 6 Limitation

While our study advances the double-correctness of predictions, it is not without limitations. First, the absence of explicit ground truth for rationale localization in large-scale datasets remains a significant challenge. We mitigated this by leveraging a self-supervised rationale disentanglement and localization method, but this approach depends heavily on the quality of the structured rationale ontologies. Second, our methods, though effective, are computationally intensive, which may limit their applicability in resource-constrained scenarios.

## 7 Conclusion

We introduce a new concept of _double-correct predictions_ aimed at training vision-language foundation models to make accurate predictions backed by correct rationales, thereby enhancing their safety for real-world deployment. To support this, we establish a solid foundation for the development of double-correct predictions. Specifically, we develop a unique dataset with structured rationales that clearly outline the reasoning processes necessary for visual recognition tasks. Furthermore, we propose a principled rationale-informed optimization method tailored for double-correct prediction. Our comprehensive empirical evaluations demonstrate that our method significantly enhances the double correctness of vision-language model predictions.

Figure 5: Qualitative results of zero-shot text-to-image retrieval on _MSCOCO_. The task is to retrieve the top-5 images with a given rationale presented. The CLIP results reveal a significant entangle of rationales with a specific category, such as “long neck” with giraffes and “wings” with airliners. In contrast, our model treats rationales independently from categories, thus offering diverse retrieval results. For example, the “long neck” found in birds, giraffes, dears, and bottles.