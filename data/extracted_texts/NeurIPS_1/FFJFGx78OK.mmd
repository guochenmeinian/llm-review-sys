# Consistency Diffusion Bridge Models

Guande He\({}^{ 1}\), Kaiwen Zheng\({}^{ 1}\), Jianfei Chen\({}^{1}\), Fan Bao\({}^{12}\), Jun Zhu\({}^{ 123}\)

\({}^{1}\)Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, THBI Lab

\({}^{1}\)Tsinghua-Bosch Joint ML Center, Tsinghua University, Beijing, China

\({}^{2}\)Shengshu Technology, Beijing \({}^{3}\)Pazhou Lab (Huangpu), Guangzhou, China

guande.he17@outlook.com; zkwthu@gmail.com;

fan.bao@shengshu.ai; {jianfeic, dcszj}@tsinghua.edu.cn

Work done during an internship at Shengshu; \({}^{}\)Equal contribution; \({}^{}\)The corresponding author.

###### Abstract

Diffusion models (DMs) have become the dominant paradigm of generative modeling in a variety of domains by learning stochastic processes from noise to data. Recently, diffusion denoising bridge models (DDBMs), a new formulation of generative modeling that builds stochastic processes between fixed data endpoints based on a reference diffusion process, have achieved empirical success across tasks with coupled data distribution, such as image-to-image translation. However, DDBM's sampling process typically requires hundreds of network evaluations to achieve decent performance, which may impede their practical deployment due to high computational demands. In this work, inspired by the recent advance of consistency models in DMs, we tackle this problem by learning the consistency function of the probability-flow ordinary differential equation (PF-ODE) of DDBMs, which directly predicts the solution at a starting step given any point on the ODE trajectory. Based on a dedicated general-form ODE solver, we propose two paradigms: consistency bridge distillation and consistency bridge training, which is flexible to apply on DDBMs with broad design choices. Experimental results show that our proposed method could sample \(4\) to \(50\) faster than the base DDBM and produce better visual quality given the same step in various tasks with pixel resolution ranging from \(64 64\) to \(256 256\), as well as supporting downstream tasks such as semantic interpolation in the data space.

## 1 Introduction

Diffusion models (DMs)  have reached unprecedented levels as a family of generative models in various areas, including image generation , audio synthesis , video generation , as well as image editing , solving inverse problems , and density estimation . In the era of AI-generated content, the stable training, scalability & state-of-the-art generation performance of DMs successfully make them serve as the fundamental component of large-scale, high-performance text-to-image  and text-to-video  models.

A critical characteristic of diffusion models is their iterative sampling procedure, which progressively drives random noise into the data space. Although this paradigm yields a sample quality that stands out from other generation models, such as VAEs , GANs , and Normalizing Flows , it also results in a notoriously lower sampling efficiency compared to other arts. In response to this, consistency models  have emerged as an attractive family of generative models by learning a consistency function that directly predicts the solution of a probability-flow ordinary differential equation (PF-ODE) at a certain starting timestep given any points in the ODE trajectory, designed to be a one-step generator that directly maps noise to data. Consistency models can benaturally integrated with diffusion models by adapting the score estimator of DMs to a consistency function of their PF-ODE via distillation  or fine-tuning , showing promising performance for few-step generation in various applications like latent space  and video .

Despite the remarkable achievements in generation quality and better sampling efficiency, a fundamental limitation of diffusion models is that their prior distribution is usually restricted to a non-informative Gaussian noise, due to the nature of their underlying data to noise stochastic process. This characteristic may not always be desirable when adopting diffusion models in some scenarios with an informative non-Gaussian prior, such as image-to-image translation. Alternatively, an emergent family of generative models focuses on leveraging diffusion bridges, a series of altered diffusion processes conditioned on given endpoints, to model transport between two arbitrary distributions . Among them, denoising diffusion bridge models (DDBMs)  study the reverse-time diffusion bridge conditioned on the terminal endpoint, and employ simulation-free, non-iterative training techniques for it, showing superior performance in application with coupled data pairs such as distribution translation compared to diffusion models. However, DDBMs generally require hundreds of network evaluations to produce samples with decent quality, even using an advanced high-order hybrid sampler, potentially hindering their deployments in real-world applications.

In this work, inspired by recent advances in consistency models with diffusion ODEs , we introduce consistency diffusion bridge models (CDBMs) and develop systematical techniques to learn the consistency function of the PF-ODEs in DDBMs for improved sampling efficiency. Firstly, to facilitate flexible integration of consistency models in DDBMs, we present a unified perspective on their design spaces, including noise schedule, prediction target, and network parameterizations, termed the same as in diffusion models . Additionally, we derive a first-order ODE solver based on the general-form noise schedule. This universal framework largely decouples the formulation of DDBMs and the corresponding consistency models from highly practical design spaces, allowing us to reuse the successful empirical choices of various diffusion bridges for CDBMs regardless of their different theoretical premises. On top of this, we then propose two paradigms for training CDBMs: consistency bridge distillation and consistency bridge training. This approach is free of dependence on a restricted form of noise schedule and the corresponding Euler ODE solver as in previous work , thus enhancing the practical versatility and extensibility of the CDBM framework.

We verify the effectiveness of CDBMs in two applications: image translation and image inpainting by distilling or fine-tuning DDBMs with various design spaces. Experimental results demonstrate that our approach can improve the sampling speed of DDBMs from \(4\) to \(50\), in terms of the Frechet inception distance  (FID) evaluated with two-step generation. Meanwhile, given the same computational budget, CDBMs have better performance trade-offs compared to DDBMs, both quantitatively and qualitatively. CDBMs also retain the desirable properties of generative modeling, such as sample diversity and the ability to perform semantic interpolation in the data space.

Figure 1: Illustration of consistency models (CMs) on PF-ODEs of diffusion models and our proposed consistency diffusion bridge models (CDBMs) building on PF-ODEs of diffusion bridges. Different from diffusion models, the PF-ODE of diffusion bridge is only well defined in \(t<T\) due to the singularity induced by the fixed terminal endpoint. To this end, a valid input for CDBMs is some \(_{t}\) for \(t<T\), which is typically obtained by one-step posterior sampling with a coarse estimation of \(_{0}\) with an initial network evaluation.

## 2 Preliminaries

### Diffusion Models

Given the data distribution \(p_{}(),^{m}\), diffusion models [53; 21; 60] specify a forward-time diffusion process from an initial data distribution \(p_{0}=p_{}\) to a terminal distribution \(p_{T}\) within a finite time horizon \(t[0,T]\), defined by a stochastic differential equation (SDE):

\[_{t}=(_{t},t)t+g(t)_{t}, _{0} p_{0}, \]

where \(_{t}\) is a standard Wiener process, \(:^{m}[0,T]^{m}\) and \(g:[0,T]^{d}\) are drift and diffusion coefficients, respectively. The terminal distribution \(p_{T}\) is usually designed to approximate a tractable prior \(p_{}\) (e.g., standard Gaussian) with the appropriate choice of \(\) and \(g\). The corresponding reverse SDE and the probability flow ordinary differential equation (PF-ODE) of the forward SDE in Eqn. (1) is given by [1; 60]:

\[_{t}=[(_{t},t)-g^{2}(t) p_{t}(_{t} )]t+g(t)}_{t},_{T} p_{T} p _{}, \]

\[_{t}=[(_{t},t)-g^{2}(t) p _{t}(_{t})]t,_{T} p_{T} p_{}, \]

where \(}_{t}\) is a reverse-time standard Wiener process and \(p_{t}(_{t})\) is the marginal distribution of \(_{t}\). Both the reverse SDE and PF-ODE can act as a generative model by sampling \(_{T} p_{}\) and simulating the trajectory from \(_{T}\) to \(_{0}\). The major difficulty here is that the score function \( p_{t}(_{t})\) remains unknown, which can be approximated by a neural network \(s_{}(_{t},t)\) with _denoising score matching_:

\[_{t(0,T)}_{p_{0}(_{0})p_{t|0}(_{ t}|_{0})}[(t)\|}(_{t},t)}- p _{t|0}(_{t}|_{0})\|_{2}^{2}], \]

where \((0,T)\) is uniform distribution, \((t)>0\) is a weighting function, and \(p_{t|0}(_{t}|_{0})\) is the transition kernel from \(_{0}\) to \(_{t}\). A common practice is to use a linear drift \(f(t)_{t}\) such that \(p_{t|0}(_{t}|_{0})\) is an analytic Gaussian distribution \((_{t}_{0},_{t}^{2})\), where \(_{t}=e^{_{0}^{t}t()},_{t}^{2}=_{t }^{2}_{0}^{t}()}{_{t}^{2}}\) is defined as the _noise schedule_. The resulting score predictor \(}}(_{t},t)\) can replace the true score function in Eqn. (2) and (3) to obtain the empirical diffusion SDE and ODE, which can be simulated by various SDE or ODE solvers [55; 38; 39; 16; 70].

### Consistency Models

Given a trajectory \(\{_{t}\}_{t=}^{T}\) with a fixed starting timestep \(\) of a PF-ODE, consistency models  aim to learn the solution of the PF-ODE at \(t=\), also known as the _consistency function_, defined as \(:(_{t},t)_{}\). The optimization process for consistency models contains the online network \(}}\) and a reference target network \(^{-}}}\), where \(^{-}\) refers to \(\) with operation \(\), i.e., \(^{-}=()\). The networks are hand-designed to satisfy the boundary condition \(}}(_{},)=_{}\), which can be typically achieved with proper parameterization on the neural network. For PF-ODE taking the form in Eqn. (3) with a linear drift \(f(t)_{t}\), the overall learning objective of consistency models can be described as:

\[_{t(,T),r=r(t)}_{p_{0}(_{0})p_ {t|0}(_{t}|_{0})}[(t)d(}}( {x}_{t},t),^{-}}}(}_{r},r))], \]

where \(r(t)\) is a function that specifies another timestep \(r\) (usually with \(t>r\)), \(d\) denotes some metric function with \(,:d(,) 0\) and \(d(,)=0\) iff. \(=\). Here \(}_{r}\) is a function that estimates \(_{r}=_{t}+_{t}^{}_{r}}{ }\), which can be done by simulating the empirical diffusion ODE with a pre-trained score predictor \(}}(_{t},t)\) or empirical score estimator \(-_{t}-_{t}_{0}}{_{t}^{2}}\). The corresponding learning paradigms are named _consistency distillation_ and _consistency training_, respectively.

### Denoising Diffusion Bridge Models

Given a data pair sampled from an arbitrary unknown joint distribution \((,) q_{}(,),,^{m}\) and let \(_{0}=\), _denoising diffusion bridge models_ (DDBMs)  specify a stochastic processthat ensures \(_{T}=\) almost surly via applying _Doob's h-transform_[13; 47] on a reference diffusion process in Eqn. (1):

\[_{t}=[(_{t},t)+g^{2}(t)_{_{t}} p _{T|t}(_{T}=|_{t})]t+g(t)_{t},\;\;(_{0},_{T})=(,) q_{}, \]

where \(p_{T|t}(_{T}=|_{t})\) is the transition kernel of the reference diffusion process from \(t\) to \(T\), evaluated at \(_{T}=\). Denoting the marginal distribution of Eqn. (6) as \(\{q_{t}\}_{t=0}^{T}\), it can be shown that the forward bridge SDE in Eqn. (6) is characterized by the diffusion distribution conditioned on both endpoints, that is, \(q_{t|0T}(_{t}|_{0},_{T})=p_{t|0T}(_{t}|_{0},_{T})\), which is an analytic Gaussian distribution. A generative model can be obtained by modeling \(q_{t|T}(_{t}|_{T}=)\), whose reverse SDE and PF-ODE are given by:

\[_{t}=[(_{t},t)-g^{2}(t)(_{_ {t}} q_{t|T}(_{t}|_{T}=)-_{_{t}} p_{T|t} (_{T}=|_{t}))]t+g(t)}_{t}, \]

\[_{t}=[(_{t},t)-g^{2}(t)[ _{_{t}} q_{t|T}(_{t}|_{T}=)-_{_{t }} p_{T|t}(_{T}=|_{t})]]t. \]

The only unknown term remains is the score function \(_{_{t}} q_{t|T}(_{t}|_{T}=)\), which can be estimated with a neural network \(_{}(_{t},t,)\) via _denoising bridge score matching_ (DBSM):

\[_{t(0,T)}_{q_{}(,)q _{t|0T}(_{t}|_{0}=,_{T}=)}[(t)\|_{}(_{t},t,)- q_{t|0T}(_{t}|_{0}= ,_{T}=)\|_{2}^{2}]. \]

Replacing \(_{_{t}} q_{t|T}(_{t}|_{T}=)\) in Eqn. (7) and (8) with the learned score predictor \(_{}(_{t},t,)\) would yield the empirical bridge SDE and ODE that could be solved for generation purposes.

## 3 Consistency Diffusion Bridge Models

In this section, we introduce consistency diffusion bridge models, extending the techniques of consistency models to DDBMs to further boost their performance and sample efficiency. Define the consistency function of the bridge ODE in Eqn. (8) as \(:(_{t},t,)_{e}\) with a given starting timestep \(\), our goal is to learn the consistency function using a neural network \(_{}(,,)\) with the following high-level objective similar to Eqn. (5):

\[_{t(,T),r=r(t)}_{q_{}( ,)q_{t|0T}(_{t}|_{0}=,_{T}=)}[ (t)d(_{}(_{t},t,),_{ -}(}_{r},r,))]. \]

To begin with, we first present a unified view of the design spaces such as noise schedule, network parameterization & precondition, as well as a general ODE solver for DDBMs. This allows us to: (1) decouple the successful practical designs of previous diffusion bridges from their different theoretical premises; (2) decouple the framework of consistency models from certain design choices of the corresponding PF-ODE, such as the reliance on VE schedule with Euler ODE solver of the original derivation of consistency models . This would largely facilitate the development of consistency models that utilize the rich design spaces of existing diffusion bridges on DDBMs in a universal way. Then, we elaborate on two ways to train \(_{}\) based on different choices of \(}_{r}\), consistency bridge distillation, and consistency bridge training, with the proposed unified design spaces.

### A Unified View on Design Spaces of DDBMs

Noise ScheduleWe consider the linear drift \(f(t)_{t}\) and define:

\[_{t}=e^{_{0}^{t}f()},_{t}=e^{- _{t}^{T}f()},_{t}^{2}=_{0}^{t}( )}{_{}^{2}},_{t}^{2}=_{t}^{T} ()}{_{}^{2}}, \]

which aligns with the common notation of noise schedules used in diffusion models by denoting \(_{t}=_{t}_{t}\). Then we could express the analytic conditional distributions of DDBMs as follows:

\[q_{t|0T}(_{t}|_{0},_{T})=p_{t|0T}(_{t}|_{0}, {x}_{T})=(a_{t}_{T}+b_{t}_{0},c_{t}^{2} ), \]

\[ a_{t}=_{t}_{t}^{2}}{_{T}^{2}}, b _{t}=_{t}^{2}}{_{T}^{2}}, c_{t}^{2}=^{2}_{t}^{2}_{t}^{2}}{_{T}^{2}}.\]

The form of \(q_{t|0T}\) is consistent with the original formulation of DDBM in . Here, inspired by , we opt to adopt a more neat set of notations for enhanced compatibility. As shown in Table 1, with such notations, we could easily unify the design choices for diffusion bridges [33; 72; 6] that have shown effectiveness in various tasks and expeditiously employ consistency models on top of them.

Network Parameterization & PreconditionIn practice, the neural network \(}\) in DBMs does not always directly regress to the target score function; instead, it can predict other equivalent quantities, such as the _data predictor_\(}=_{t}-a_{t}_{T}+c_{t}^{2}}}{b_{ t}}\) for a Gaussian \((a_{t}_{T}+b_{t}_{0},c_{t}^{2})\) like \(q_{t}|_{0T}\). Meanwhile, the inputs and outputs of the network \(}\) could be rescaled for a better-behaved optimization process, known as the network precondition. As shown in Table 1, we could consistently use \(_{0}\) as the prediction target with different choices of network precondition to unify the previous practical designs for DBMs.

PF-ODE and ODE SolverThe validity of a consistency model relies on an underlying PF-ODE that shares the same marginal distribution with the forward process. In the original DDBM paper , the marginal preserving property of the proposed ODE is justified following an analogous logic from the derivation of the PF-ODE of diffusion models  with Kolmogorov forward equation. However, its validity suffers from doubts as there is a singularity at the deterministic starting point \(_{T}\). Here, we provide a simple example to show that the ODE can indeed maintain the marginal distribution as long as we use a valid stochastic step to skip the singular point and start from \(T-\) for any \(>0\).

**Example 3.1**.: _Assume \(T=1\) and consider a simple Brownian Bridge between two fixed points \((x_{0},x_{1})\):_

\[x_{t}=-x_{t}}{1-t}t+w_{t}, \]

_with marginal distribution \(q_{t|01}(x_{t}|x_{0},x_{1})=((1-t)x_{0}+tx_{1},t(1-t))\). The ground-truth reverse SDE and PF-ODE are given by:_

\[x_{t}=-x_{0}}{t}t+_{t}, \] \[x_{t}=(x_{t}+x_ {1}-x_{0})t. \]

_Then first simulating the reverse SDE in Eqn. (14) from \(t=1\) to \(t=1-\) for some \((0,1)\) and then starting to simulate the PF-ODE in Eqn. (15) will preserve the marginal distribution._

The detailed derivation can be found in Appendix. B.2. Therefore, the time horizon of the consistency model based on the bridge ODE needs to be set as \(t[,T-]\) for some pre-specified \(,>0\). Additionally, the marginal preservation of the bridge ODE for more general diffusion bridges can be strictly justified by considering non-Markovian variants, as done in DBIM .

Another crucial element for developing consistency models is the ODE solver, as a solver with a lower local error would yield lower error for consistency distillation, as well as the corresponding

    & **Brownian Bridge** & **I2SB** & **DDBM** &  \\  & _default_ & _default_\({}^{}\) & _VP_\({}^{}\) & _VE_ & _gmax_ & _VP_ \\ 
**Schedule** & & & & & & \\ \(T\) & 1 & 1 & 1 & T & 1 & 1 \\ \(J(t)\) & 0 & 0 & \(-_{0}\) & 0 & 0 & \(-_{0}-_{d}\) \\ \(g^{2}(t)\) & \(^{2}\) & \((n_{1}-_{0}|2t-1|)^{2}\) & \(_{0}\) & \(2t\) & \(_{0}+_{d}t\) & \(_{0}+_{d}t\) \\ \(_{t}\) & 1 & 1 & \(e^{-_{0}t}\) & 1 & 1 & \(e^{-_{0}t-_{d}t^{2}}\) \\ \(_{t}^{2}\) & \(^{2}t\) & \(_{0}^{t}g^{2}()\) & \(1-e^{-_{0}t}\) & \(t^{2}\) & \(_{0}t+_{d}t^{2}\) & \(1-e^{-_{d}t-_{d}t^{2}}\) \\ \(_{t}\) & 1 & \(e^{_{0}t}\) & \(1\) & 1 & 1 & \(_{t}/_{t}\) \\ \(_{t}^{2}\) & \(^{2}t\) & \(_{0}^{t}g^{2}()\) & \(e^{_{0}t}-1\) & \(t^{2}\) & \(_{0}t+_{d}t^{2}\) & \(e^{_{0}t+_{d}t^{2}}-1\) \\ \(_{t}^{2}\) & \(^{2}(1-t)\) & \(_{t}^{2}-_{t}^{2}\) & \(e^{_{0}}-e^{_{0}t}\) & \(T^{2}-t^{2}\) & \(_{t}^{2}-_{t}^{2}\) & \(_{t}^{2}-_{t}^{2}\) \\ 
**Parameters** & \(\) & \(_{0}=}-}}{_{1}}\) & \(_{0}\) & \(T=80\) & \(_{0}=0.01\) & \(_{0}=0.01\) \\  & & \(_{1}=}+}}{_{1}}\) & \(_{0}=0.1\) & & \(_{d}=49.99\) & \(_{d}=19.99\) \\  & & \(_{1}=0.3/1.0\) & & & & \\ 
**Parameterization by Network \(}\)** & & & & & \\ Data Predictor \(}\) & Dependent on Training & \(_{t}-_{t}}\) & \(c_{}(t)_{t}+c_{}(t)}\) & \(}\) \\    \({}^{}\) Though I2SB is built on a discrete-time schedule for \(T=1000\) timesteps, it can be converted to a continuous-time schedule on \(t\) approximately by mapping \(t\) to \(t/(T-1)\).

\({}^{}\) The authors change to the same VP schedule as Bridge-TTS with parameters \(_{0}=0.1,_{d}=2\) in a revised version of their paper.

Table 1: Specifications of design spaces in different diffusion bridges. The details of network parameterization are in Appendix B.4 due to space limit.

consistency training objectives [58; 57]. Inspired by the successful practice of advanced ODE solvers based on the Exponential Integrator (EI) [4; 22] in diffusion models, we present a first-order bridge ODE solver in a similar fashion:

**Proposition 3.1**.: _Given an initial value \(_{t}\) at time \(t>0\), the first-order solver of the bridge ODE in Eqn. (8) from \(t\) to \(r[0,t]\) with the noise schedule defined in Eqn. (11) is:_

\[_{r}=_{r}_{r}}{_{t}_{t} _{t}}_{t}+}{_{T}^{2}}[(_{r}^{2}- _{t}_{r}_{r}}{_{t}})_{ }(_{t},t,)+(_{r}^{2}-_{r}_{r}} {_{t}})}{_{T}}]. \]

We provide detailed derivation in the Appendix B.1. Typically, an EI-based solver enjoys a lower discretization error and therefore has better empirical performance [16; 38; 39; 67; 70]. Another notable advantage of this general form solver, as we will show in Section 3.3, is that it could naturally establish the connection between consistency training and consistency distillation for any noise schedules that take the form in Eqn. (11), eliminating the dependence of the VE schedule and the corresponding Euler ODE solver in the common derivation .

### Consistency Bridge Distillation

Analogous to consistency distillation with the empirical diffusion ODE, we could leverage a pre-trained score predictor \(_{}(_{t},t,)_{_{t}} q_{t|T}( _{t}|_{T}=)\) to solve the empirical bridge ODE to obtain \(}_{r}\), i.e., \(}_{r}=}_{}(_{t},t,r,)\), where \(}_{}\) is the update function of a one-step ODE solver with fixed \(_{}\). We define the _consistency bridge distillation_ (CBD) loss as:

\[_{}^{ t_{}}:= \] \[_{t(,T-),r=r(t)}_{ q_{}(,)q_{t|0T}(_{t}|_{0}=,_{T}= )}[(t)d(_{}(_{t},t,),_{^{-}}(}_{}(_{t},t,r,),r,) )],\]

where \(t\) is sampled from the uniform distribution over \([,T-]\), \(r(t)\) is a function specifies another timestep \(r\) such that \( r<t\) with \( t_{}:=_{t}\{t-r(t)\}\) and \( t_{}:=_{t}\{t-r(t)\}\), \((t)\) is a positive weighting function, \(d\) is some distance metric function with \(,:d(,) 0\) and \(d(,)=0\) iff. \(=\), and \(^{-}=()\). Similarly to the case of consistency distillation in empirical diffusion ODEs, we have the following asymptotic analysis of the CBD objective:

**Proposition 3.2**.: _Given \( t_{}=_{t}\{t-r(t)\}\) and let \(_{}(,,)\) be the consistency function of the empirical bridge ODE taking the form in Eqn. (8). Assume \(_{}\) is a Lipschitz function, i.e., there exists \(L>0\), such that for all \(t[,T-],_{1},_{2},\), we have \(\|_{}(_{1},t,)-_{}(_{2},t, )\|_{2} L\|_{1}-_{2}\|_{2}\). Meanwhile, assume that for all \(t,r[,T-], q_{}():=_{ }[q_{}(,)]\), the ODE solver \(}_{}(,t,,)\) has local error uniformly bounded by \(O((t-r)^{p+1})\) with \(p 1\). Then, if \(_{}^{ t_{}}=0\), we have: \(_{t,,}\|_{}(,t,)-_{}(,t,)\|_{2}=O(( t_{})^{p})\)._

The vast majority of the analysis can be done by directly following the proof in  with minor differences between the overlapped timestep intervals \(\{t,r(t)\}\) for \(t[,T-]\) used in Eqn. (17) and the fixed timestep intervals \(\{t_{n}\}_{n=1}^{N}\) used in . We include it in Appendix B.5 for completeness. In this work, unless otherwise stated, we use the first-order ODE solver in Eqn. (16) as \(}_{}\).

### Consistency Bridge Training

In addition to distilling from pre-trained score predictor \(_{}\), consistency models can be trained [58; 57] or fine-tuned  by maintaining only one set of parameters \(\). To accomplish this, we could leverage the unbiased score estimator:

\[_{_{t}} q_{t|T}(_{t}|_{T}=)= _{_{0}}[_{_{t}} q_{t|0T}(_{t}|_{0},_{T})|_{t},_{T}=], \]

that is, with a single sample \((,) q_{}\) and \(_{t} q_{t|0T}(_{t}|_{0}=,_{T}=)\), the score \(_{_{t}} q_{t|T}(_{t}|_{T}=)\) can be estimated with \(_{_{t}} q_{t|0T}(_{t}|_{0},_{T})\). Substituting such an estimation of \(_{}\) into the one-step ODE solver \(}_{}\) in Eqn. (17) with the transformation between data and score predictor \(_{}=_{t}-a_{t}_{T}+c_{t}^{2}_{}}{ b_{t}}\), we can obtain an alternative \(}_{r}\) that does not rely on the pre-trained \(_{}\) for any noise schedule taking the form in Eqn. (11) as follows (detail in Appendix B.3):

\[}_{r}=}(_{t},t,r,,)=a_{r} +b_{r}+c_{r}, \]where \(a_{r},b_{r},c_{r}\) are defined as in Eqn. (11), and \(=_{}-a_{}-b_{}}{c_{}} (,)\). Based on this instantiation of \(}_{r}\), we define the _consistency bridge training_ (CBT) loss as:

\[_{}^{ t_{}}:= \] \[_{t(,T-),=r(t)} _{q_{}(,)}[(t)d(_{ }(a_{t}+b_{t}+c_{t},t,),_{ -}(a_{r}+b_{r}+c_{r},r,))],\]

where \(t,r(),(),^{-1}\) are defined the same as in Eqn. (17), and \((,)\) is a shared Gaussian noise used in both \(_{}\) and \(_{^{-1}}\). We have the following proposition demonstrating the connection between \(_{}^{ t_{}}\) and \(_{}^{ t_{}}\) with the first-order one-step ODE solver:

**Proposition 3.3**.: _Given \( t_{}=_{t}\{t-r(t)\}\) and assume \(d,_{},f,g\) are twice continuously differentiable with bounded second derivatives, the weighting function \(()\) is bounded, and \([\|_{_{}} q_{t|T}(_{t}|_{T})\|_ {2}^{2}]<\). Meanwhile, assume that \(_{}^{ t_{}}\) employs the one-step ODE solver in Eqn. (16) with ground truth pre-trained score model, i.e., \( t[,T-], q_{}():_{ }(_{t},t,)_{_{}} q_{t|T}( _{t}|_{T}=)\). Then, we have: \(_{}^{ t_{}}=_{}^{ t_{}}+o( t_{})\)._

The core part of our analysis also follows , except the connection between the CBD & CBT objective relies on the proposed first-order ODE solver and the estimated \(}_{r}\) in Eqn. (19) with the general noise schedule for DDBM. We include the details in Appendix B.6.

### Network Precondition and Sampling

Network PreconditionFirst, we focus on enforcing the boundary condition \(_{}(_{},,)=_{}\) of our consistency bridge model, which can be done by designing a proper network precondition. Usually, a variable substitution \(=t-\) could work in most cases. For example, for the precondition for I\({}^{2}\)SB in Table 1, we have \(_{}+_{}_{}=_{ }+^{-}g^{2}()}= _{}\). Also, the common "EDM"  style precondition used in DDBM also satisfies \(c_{}()=1\) and \(c_{}()=0\). We also give a universal precondition to satisfy the boundary conditions based on the form of the ODE solver in Eqn. (16) in Appendix B.4 to cope with the case where the variable substitution is not applicable.

SamplingAs explained in Section 3.1, the PF-ODE is only well-defined within the time horizon \(0 t T-\) for some \((0,T)\). Hence, the sampling of CDBMs should start with \(_{T-} q_{T-|T}(_{T-}|_{T}=)\), which can be obtained by simulating the reverse SDE in Eqn. (7) from \(T\) to \(T-\). Here we opt to use one first-order stochastic step, which is equivalent to performing posterior sampling, i.e., \(_{T-} q_{T-|0T}(_{T-}|_{0}=_{ {}}(_{T},T,),_{T}=)\). This sampling approach defaults to two NFEs (Number of Function Evaluations), which is aligned with the practical guideline that employing two-step sampling in CM allows for a better trade-off between quality and computation compared to other treatments such as scaling up models . We could also alternate a forward noising step and a backward consistency step multiple times to further improve sample quality as consistency models do.

## 4 Experiments

### Experimental Setup

Task, Datasets, and MetricsIn this work, we conduct experiments for CDBM on image-to-image translation and image inpainting tasks with various image resolutions and scales of the data set. For image-to-image translation, we use the Edges\(\)Handbags  with \(64 64\) pixel resolution and DIODE-Outdoor  with \(256 256\) pixel resolution. For image inpainting, we choose ImageNet \(256 256\) with a center mask of size \(128 128\). Regarding the evaluation metrics, we report the Frechet inception distance (FID)  for all datasets. Furthermore, following previous works [33; 72], we measure Inception Scores (IS) , LPIPS  and Mean Square Error (MSE) for image-to-image translation and Classifier Accuracy (CA) of a pre-trained ResNet50 for image-inpainting. The metrics are computed using the complete training set for Edges\(\)Handbags and DIODE-Outdoor, and a validation subset of 10,000 images for ImageNet.

Training ConfigurationsWe train CDBM in two ways: distill pre-trained DDBM with CBD or _fine-tuning_ DDBM with CBT. We keep the noise schedule and prediction target of the pre-trained DDBMunchanged and modify the network precondition to satisfy the boundary condition. Specifically, we adopt the design space of DDBM-VP and I\({}^{2}\)SB in Table 1 on image-to-image translation and image inpainting, respectively. We specify complete training details in Appendix C.

**Specification of Design Choices** We illustrate the specific design choices for CDBM. In this work, we use \(t[,1-]\) and set \(=0.0001,=0.001\) and sample \(t\) uniformly during training. We employ two different sets of the timestep function \(r(t)\) and the loss weighting \((t)\), also named the _training schedule_ for CDBM. The first, following , specifies a constant quantity for \( t=t-r(t)\) with a simple loss weighting of \((t)=1\). The constant gap \( t\) is treated as a hyperparameter and we search it among \(\{1/9,1/18,1/36,1/60,1/80,1/120\}\). The other employs \(r(t)\) that gradually shrinks \(t-r(t)\) during the training process and a loss weighting of \((t)=\), which enjoys a better trade-off between faster convergence and performance [58; 57; 15]. Following , we use a sigmoid-style function \(r(t)=t(1-/}})(1+}})\), where \(\) is the number of training iterations, \(q,s,k,b\) are hyperparameters. We use \(q=2,k=8\), and tune \(b\{1,2,5,10,20,50\}\) and \(s\{5000,10000\}\).

reference for the sampling acceleration ratio (Reduction factor of NFE to achieve the same FID) of CDBM. Following , we report the result of other baselines with \( 40\), which consists of diffusion-based methods, diffusion bridges with different formulations, or samplers. We mainly focus on the two-step generation scenario for CDBM, which is the minimal NFEs required for CDBM using the sampling procedure described in Section 3.4.

For image-to-image translation, as shown in Table. 2, we first observed that our proposed first-order ODE solver has superior performance compared to the hybrid high-order sampler used in DDBM . On top of that, CDBM's FID at \(=2\) is close to or even better than DDBM's at NFE around \(100\) with the advanced ODE solver, achieving a sampling speed-up around \(50\). This can be corroborated by the qualitative demonstration in Fig. 4, where CDBMs drastically reduce the blurring effect on DDBMs under few-step generation settings while enjoying realistic and faithful translation performance.

For image inpainting, as shown in Table. 3, the baseline ODE solver for DDBM achieves decent sample quality at \(=10\). For CDBM, as shown in Fig. 2, the acceleration ratio is relatively modest in such a large-scale and challenging dataset, achieving close to a \(4\) increase in sampling speed. Notably, CBT's FID at \(=4\) matches DDBM at \(=10\). Moreover, we find that CDBMs have better visual quality than DDBM given the same computation budget, as shown in Fig. 4 and Appendix D, which illustrates that CDBM yields a better quality-efficiency trade-off.

Meanwhile, we observe that fine-tuning DDBMs with CBT generally produces better results than CBD in all three data sets, demonstrating fine-tuning a pre-trained score model to a consistency function is a more promising solution with less computational and memory cost compared to distillation, which is consistent with recent findings . We also conducted an ablation study for CBD and CBT under different training schedules (i.e., the combination of the timestep function \(r(t)\) and the loss weighting \((t)\)) on ImageNet \(256 256\). As shown in Fig. 3, for a small timestep interval \(t-r(t)\), e.g., a small \( t\) in Fig. 3a or a large \(b\) in Fig. 3b (detail in Appendix C.2), the performance is generally better but also suffers from training instability, indicated by the sharp increase in FID during training when \( t=1/120\) and \(b=50\). While for a large timestep interval, the performance at convergence is usually worse. In practice, we found that adopting the training schedule that gradually shrinks \(r(t)-t\) with \(b=20\) or \(50\) with CBT could work across all tasks, whereas CBD generally needs a meticulous design for \( t\) or \(b\) to ensure stable training and satisfactory performance.

### Semantic Interpolation

We show that CDBMs support performing downstream tasks, such as semantic interpolation, similar to diffusion models . Recall that the sampling process for CDBM alternates between consistency

Figure 4: Qualitative demonstration between DDBM and CDBM.

Figure 5: Example semantic interpolation result with CDBMs

function evaluation and forward sampling, we could track all noises and the corresponding timesteps to re-generate the same sample. By interpolating the noises of two sampling trajectories, we can obtain a series of samples lying between the semantics of two source samples, as shown in Fig. 5, which demonstrates that CDBMs have a wide range of generative modeling capabilities, such as sample diversity and semantic interpolation.

## 5 Conclusion

In this work, we introduce consistency diffusion bridge models (CDBMs) to address the sampling inefficiency of DDBMs and present two frameworks, consistency bridge distillation and consistency bridge training, to learn the consistency function of the DDBM's PF-ODE. Building on a unified view of design spaces and the corresponding general-form ODE solver, CDBM exhibits significant flexibility and adaptability, allowing for straightforward integration with previously established successful designs for diffusion bridges. Experimental evaluations across three datasets show that CDBM can effectively boost the sampling speed of DDBM by \(4\) to \(50\). Furthermore, it achieves the saturated performance of DDBMs with less than five NFEs and possesses the broad capacity of generative models, such as sample diversity and semantic interpolation.

Limitations and Broader ImpactWhile significantly improving the sampling efficiency in the datasets we used, it remains to be explored how the proposed CDBM, along with the DDBM formulation, performs in datasets with larger-scale or more complex characteristics. Furthermore, the consistency model paradigm typically suffers from numerical instability and it would be a promising research direction to keep improving CDBM's performance from an optimization perspective. With enhanced sampling efficiency, CDBMs could contribute to more energy-efficient deployment of generative models, aligning with broader goals of sustainable AI development. However, it could also lower the cost associated with the potential misuse for creating deceptive content. We hope that our work will be enforced with certain ethical guidelines to prevent any form of harm.