# What Do You See in Common?

Learning Hierarchical Prototypes over Tree-of-Life to Discover Evolutionary Traits

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

A grand challenge in biology is to discover evolutionary traits--features of organisms common to a group of species with a shared ancestor in the tree of life (also referred to as phylogenetic tree). With the growing availability of image repositories in biology, there is a tremendous opportunity to discover evolutionary traits directly from images in the form of a hierarchy of prototypes. However, current prototype-based methods are mostly designed to operate over a flat structure of classes and face several challenges in discovering hierarchical prototypes, including the issue of learning over-specific features at internal nodes. To overcome these challenges, we introduce the framework of **H**ierarchy aligned **C**ommonality through **P**rototypical **N**etworks (**HComP-Net**). We empirically show that HComP-Net learns prototypes that are accurate, semantically consistent, and generalizable to unseen species in comparison to baselines on birds, butterflies, and fishes datasets.

## 1 Introduction

A central goal in biology is to discover the observable characteristics of organisms, or _traits_ (e.g., beak color, stripe pattern, and fin curvature), that help in discriminating between species and understanding

Figure 1: Sample images of bird species with zoomed-in views of learned prototypes along with their associated score maps. We consider the problem of finding evolutionary traits common to a group of species derived from the same ancestor (blue) that are absent in other species from a different ancestor (red). We can infer that descendants of the blue node share a common trait: _long tail_, absent from descendants of the red node.

how organisms evolve and adapt to their environment . For example, discovering traits inherited by a group of species that share a common ancestor on the tree of life (also referred to as the _phylogenetic tree_, see Figure 1) is of great interest to biologists to understand how organisms diversify and evolve . The measurement of such traits with evolutionary signals, termed _evolutionary traits_, is not straightforward and often relies on subjective and labor-intensive human expertise and definitions , hindering rapid scientific advancement .

With the growing availability of large-scale image repositories in biology containing millions of images of organisms , there is an opportunity for machine learning (ML) methods to discover evolutionary traits automatically from images . This is especially true in light of recent advances in the field of explainable ML, such as the seminal work of ProtoPNet  and its variants  which find representative patches in training images (termed _prototypes_) capturing discriminatory features for every class. We can thus cast the problem of discovering evolutionary traits into asking the following question: _what image features or prototypes are common across a group of species with a shared ancestor in the tree of life that are absent in species with a different shared ancestor?_

For example, in Figure 1, we can see that the four species of birds on the left descending from the blue node show the common feature of having "long tails," unlike any of the descendant species of the red node. Learning such common features at every internal node as a hierarchy of prototypes can help biologists generate novel hypotheses of species diversification (e.g., the splitting of blue and red nodes) and accumulation of evolutionary trait changes.

Despite the success of ProtoPNet  and its variants in learning prototypes over a flat structure of classes, applying them to discover a hierarchy of prototypes is challenging for three main reasons. _First_, existing methods that learn multiple prototypes for every class are prone to learning "over-specific" prototypes at internal nodes of a tree, which cover only one (or a few) of its descendant species. Figure 2 shows a few examples to illustrate the concept of over-specific prototypes. Consider the problem of learning prototypes common to descendant species of the Fellade family: Lion and Bobcat. If we learn one prototype focusing on the feature of the name (specific only to Lion) and another prototype focusing on the feature of spotted back (specific only to Bobcat), then these two prototypes taken together can classify all images from the Fellade family. However, they do not represent _common_ features shared between Lion and Bobcat and hence are not useful for discovering evolutionary traits. Such over-specific prototypes should be instead pushed down to be learned at lower levels of the tree (e.g., the species leaf nodes of Lion and Bobcat).

_Second_, while existing methods such as ProtoPShare , ProtoPool , and ProtoTree  allow prototypes to be shared across classes for re-usability and sparsity, in the problem of discovering evolutionary traits, we want to learn prototypes at an internal node \(n\) that are not just shared across all it descendant species but are also absent in the _contrasting set_ of species (i.e., species descending from sibling nodes of \(n\) representing alternate paths of diversification). _Third_, at higher levels of the tree, finding features that are common across a large number of diverse species is challenging . In such cases, we should be able to abstain from finding common prototypes without hampering accuracy at the leaf nodes--a feature missing in existing methods.

To address these challenges, we present **H**ierarchy aligned **Comm**onality through **P**rototypical **N**etworks **(HComP-Net)**, a framework to learn hierarchical prototypes over the tree of life for discovering evolutionary traits. Here are the main contributions of our work:

Figure 2: Examples to illustrate the problem of learning “over-specific” prototypes at internal nodes, which only cover one descendant species of the node instead of learning prototypes _common_ to all descendants.

1. HComP-Net learns common traits shared by all descendant species of an internal node and avoids the learning of over-specific prototypes in contrast to baseline methods using a novel _overspecificity loss_.
2. HComP-Net uses a novel _discriminative loss_ to ensure that the prototypes learned at an internal node are absent in the contrasting set of species with different ancestry.
3. HComP-Net includes a novel _masking module_ to allow for the exclusion of over-specific prototypes at higher levels of the tree without hampering classification performance.
4. We empirically show that HComP-Net learns prototypes that are accurate, semantically consistent, and generalizable to unseen species compared to baselines on data from 190 species of birds (CUB-200-2011 dataset) , 38 species of fishes , and 30 species of butterflies . We show the ability of HComP-Net to generate novel hypotheses about evolutionary traits at different levels of the phylogenetic tree of organisms.

## 2 Related Works

One of the seminal lines of work in the field of prototype-based interpretability methods is the framework of ProtoPNet  that learns a set of "prototypical patches" from training images of every class to enable case-based reasoning. Following this work, several variants have been developed such as ProtoPShare , ProtoPool , ProtoTree , and HPnet  suiting to different interpretability requirements. Among all these approaches, our work is closely related to HPnet , the hierarchical extension of ProtoPNet that learns a prototype layer for every parent node in the tree. Despite sharing a similar motivation as our work, HPnet is not designed to avoid the learning of over-specific prototypes or to abstain from learning common prototypes at higher levels of the tree.

Another related line of work is the framework of PIPNet , which uses self-supervised learning methods to reduce the "semantic gap" [19; 20] between the latent space of prototypes and the space of images, such that the prototypes in latent space correspond to the same visual concept in the image space. In HComP-Net, we build upon the idea of self-supervised learning introduced in PIPNet to learn semantically consistent hiearchy of prototypes. Our work is also related to ProtoTree , which structures the prototypes as nodes in a decision tree to offer more granular interpretability. However, ProtoTree differs from our work in that it learns the tree-based structure of prototypes automatically from data and cannot handle a known hierarchy. Moreover, the prototypes learned in ProtoTree are purely discriminative and allow for negative reasoning, which is not aligned with our objective of finding common traits of descendant species.

Other related works that focus on finding shared features are ProtoPShare  and ProtoPool . Both approaches aim to find common features among classes, but their primary goal is to reduce the prototype count by exploiting similarities among classes, leading to a sparser network. This is different from our goal of finding a hiearchy of prototypes to find evolutionary traits common to a group of species (that are absent from other species).

Outside the realm of prototype-based methods, the framework of Phylogeny-guided Neural Networks (PhyloNN)  shares a similar motivation as our work to discover evolutionary traits by representing biological images in feature spaces structured by tree-based knowledge (i.e., phylogeny). However, PhyloNN primarily focuses on the tasks of image generation and translation rather than interpretability. Additionally, PhyloNN can only work with discretized trees with fixed number of ancestor levels per leaf node, unlike our work that does not require any discretization of the tree.

## 3 Proposed Methodology

### HComP-Net Model Architecture

Given a phylogenetic tree with \(N\) internal nodes, the goal of HComP-Net is to jointly learn a set of prototype vectors \(}\) for every internal node \(n\{1,,N\}\). Our architecture as shown in Figure 3 begins with a CNN that acts as a common feature extractor \(f(x;)\) for all nodes, where \(\) represents the learnable parameters of \(f\). \(f\) converts an image \(x\) into a latent representation \(Z^{H W C}\), where each "patch" at location \((h,w)\) is, \(}^{C}\). Following the feature extractor, for every node \(n\), we initialize a set of \(K_{n}\) prototype vectors \(}=\{}\}_{i=1}^{K_{n}}\), where \(}^{C}\). Here, the number of prototypes \(K_{n}\) learned at node \(n\) varies in proportion to the number of children of node \(n\), with \(\) as the proportionality constant, i.e., at each node \(n\) we assign \(\) prototypes for every child node. To simplify notations, we drop the subscript \(n\) in \(}\) and \(K_{n}\) while discussing the operations occurring in node \(n\).

We consider the following sequence of operations at every node \(n\). We first compute the similarity score between every prototype in \(\) and every patch in \(Z\). This results in a matrix \(^{H W K}\), where every element represents a similarity score between image patches and prototype vectors. We apply a softmax operation across the \(K\) channels of \(\) such that the vector \(}_{}^{K}\) at spatial location \((h,w)\) in \(\) represents the probability that the corresponding patch \(_{}\) is similar to the \(K\) prototypes. Furthermore, the \(i^{th}\) channel of \(\) serves as a prototype score map for the prototype vector \(}\), indicating the presence of \(}\) in the image. We perform global max-pooling across the spatial dimensions \(H W\) of \(\) to obtain a vector \(^{K}\), where the \(i^{th}\) element represents the highest similarity score of the prototype vector \(}\) across the entire image. \(\) is then fed to a linear classification layer with weights \(\) to produce the final classification scores for every child node of node \(n\). We restrict the connections in the classification layer so that every child node \(n_{c}\) is connected to a distinct set of \(\) prototypes, to ensure that every prototype uniquely maps to a child node. \(\) is restricted to be non-negative to ensure that the classification is done solely through positive reasoning, similar to the approach used in PIP-Net . We borrow the regularization scheme of PIP-Net to induce sparsity in \(\) by computing the logit of child node \(n_{c}\) as \((()^{2}+1)\). \(\) and \(\) here are again unique to each node.

### Loss Functions Used to Train HComP-Net

**Contrastive Losses for Learning Hierarchical Prototypes:** PIP-Net  introduced the idea of using self-supervised contrastive learning to learn semantically meaningful prototypes. We build upon this idea in our work to learn semantically meaningful hierarchical prototypes at every node in the tree as follows. For every input image \(\), we pass in two augmentations of the image, \(^{}\) and \(^{}\) to our framework. The prototype score maps for the two augmentations, \(^{{}^{}}\) and \(^{{}^{}}\), are then considered as positive pairs. Since \(}_{}^{K}\) represents the probabilities of patch \(_{}\) being similar to the prototypes from \(\), we align the probabilities from the two augmentations \(}^{{}^{}}_{}\) and \(}^{{}^{}}_{}\) to be similar using the following alignment loss:

\[_{A}=-_{(h,w) H W}(}^{ {}^{}}_{}}^{{}^{}}_{}) \]

Since \(_{i=1}^{K}}_{}=1\) due to softmax operation, \(_{A}\) is minimum (i.e., \(_{A}=0\)) when both \(}^{{}^{}}_{}\) and \(}^{{}^{}}_{}\) are identical one-hot encoded vectors. A trivial solution that minimizes \(_{A}\) is when all

Figure 3: Schematic illustration of HComP-Net model architecture.

patches across all images are similar to the same prototype. To avoid such representation collapse, we use the following tanh-loss \(_{T}\) of PIP-Net , which serves the same purpose as uniformity losses in  and :

\[_{T}=-_{i=1}^{K}((_{b=1}^{B}})), \]

where \(}\) is the prototype score for prototype \(i\) with respect to image \(b\) of mini-batch. \(_{T}\) encourages each prototype \(}\) to be activated at least once in a given mini-batch of \(B\) images, thereby helping to avoid the possibility of representation collapse. The use of \(\) ensures that only the presence of a prototype is taken into account and not its frequency.

**Over-specificity Loss:** To achieve the goal of learning prototypes common to all descendant species of an internal node, we introduce a novel loss, termed _over-specificity loss_\(_{ovsp}\) that avoids learning over-specific prototypes at any node \(n\). \(_{ovsp}\) is formulated as a modification of the tanh-loss such that prototype \(}\) is encouraged to be activated at least once in every one of the descendant species \(d\{1,,D_{i}\}\) of its corresponding child node in the mini-batch of images fed to the model, as follows:

\[_{ovsp}=-_{i=1}^{K}_{d=1}^{D_{i}}(( _{b B_{d}}})), \]

where \(B_{d}\) is the subset of images in the mini-batch that belong to species \(d\).

**Discriminative loss:** In order to ensure that a learned prototype for a child node \(n_{c}\) is not activated by any of its _contrasting set_ of species (i.e., species that are descendants of child nodes of \(n\) other than \(n_{c}\)), we introduce another novel loss function, \(_{disc}\), defined as follows:

\[_{disc}=_{i=1}^{K}_{d}}_ {b B_{d}}(}), \]

where \(}\) is the contrasting set of all descendant species of child nodes of \(n\) other than \(n_{c}\). This is similar to the seperation loss used in other prototype-based methods such as , , and .

**Orthogonality loss:** We also apply kernel orthogonality as introduced in  to the prototype vectors at every node \(n\), so that the learned prototypes are orthogonal and capture diverse features:

\[_{orth}=\|}}^{}-I\|_{F}^{2} \]

where \(}\) is the matrix of normalized prototype vectors of size \(C K\), \(I\) is an identity matrix, and \(\|.\|_{F}^{2}\) is the Frobenius norm. Each prototype \(_{i}}\) in \(}\) is normalized as, \(_{i}}=}}{\|}\|}\).

**Classification loss:** Finally, we apply cross entropy loss for classification at each internal node as follows:

\[_{CE}=-_{b}^{B}y_{b}(_{b}) \]

where \(y\) is ground truth label and \(\) is the prediction at every node of the tree.

### Masking Module to Identify Over-specific Prototypes

We employ an additional masking module at every node \(n\) to identify over-specific prototypes without hampering their training. The learned mask for prototype \(}\) simply serves as an indicator of whether \(}\) is over-specific or not, enabling our approach to abstain from finding common prototypes if there are none, especially at higher levels of the tree. To obtain the mask values, we first calculate the over-specificity score for prototype \(}\) as the product of the maximum prototype scores obtained across all images in the mini-batch belonging to every descendant species \(d\) as:

\[_{i}=-_{d=1}^{D_{i}}_{b B_{d}}(}) \]

where \(}\) is the prototype score for prototype \(}\) with respect to image \(b\) of mini-batch and \(B_{d}\) is the subset of images in the mini-batch that belong to descendant species \(d\). Since \(}\) takes a value between 0 to 1 due to the softmax operation, \(_{i}\) ranges from -1 to 0, where -1 denotes least over-specificity and 0 denotes the most over-specificity. The multiplication of the prototype scores ensures that even when the score is less with respect to only one descendant species, the prototype will be assigned a high over-specificity score (close to 0).

As shown in Figure 3, \(_{i}\) is then fed into the masking module, which includes a learned mask value \(M_{i}\) for every prototype \(}\). We generate \(M_{i}\) from a Gumbel-softmax distribution  so that the values are skewed to be very close to either 0 or 1, i.e., \(M_{i}=(_{i},)\), where \(_{i}\) are the learnable parameters of the distribution and \(\) is temperature. We then compute the masking loss, \(_{mask}\), as:

\[_{mask}=_{i=1}^{K}(_{mask}M_{i}( _{i})+_{L_{1}}}_{1}) \]

where \(_{mask}\) and \(_{L_{1}}\) are trade-off coefficients, \(_{1}\) is the \(L_{1}\) norm added to induce sparsity in the masks, and stopgrad represents the stop gradient operation applied over \(_{i}\) to ensure that the gradient of \(_{mask}\) does not flow back to the learning of prototype vectors and impact their training. Note that _the learned masks are not used for pruning the prototypes during training_, they are only used during inference to determine which of the learned prototypes are over-specific and likely to not represent evolutionary traits. Therefore, even if all the prototypes are identified as over-specific by the masking module at an internal node, it will not affect the classification performance at that node.

### Training HComP-Net

We first pre-train the prototypes at every internal node in a self-supervised learning manner using alignment and tanh-losses as \(_{SS}=_{A}_{A}+_{T}_{T}\). We then fine-tune the model using the following combined loss: \((_{CE}_{CE}+_{SS}+_{cwg}_{cwg}+ _{disc}_{disc}+_{orth}_{orth}+_ {mask})\), where \(\)'s are trade-off parameters. Note that the loss is applied over every node in the tree. We show an ablation of key loss terms in our framework in Table 6 in the Supplementary Section.

## 4 Experimental Setup

**Dataset:** In our experiments, we primarily focus on the 190 species of birds (**Bird**) from the CUB-200-2011  dataset for which the phylogenetic relationship  is known. The tree is quite large with a total of 184 internal nodes. We removed the background from the images to avoid the possibility of learning prototypes corresponding to background information such as the bird's habitat as we are only interested in the traits corresponding to the body of the organism. We also apply our method on a fish dataset with 38 species (**Fish**)  along with its associated phylogeny  and 30 subspecies of Heliconius butterflies (**Butterfly**) from the Jiggins Helicionius Collection dataset  collected from various sources 1 along with its phylogeny . The qualitative results of Butterfly and Fish datasets are provided in the supplementary materials. The complete details of hyper-parameter settings and training strategy are also provided in the Supplementary Section E.

**Baselines:** We compare HComP-Net to ResNet-50 , INTR (Interpretable Transformer)  and HPnet . For HPnet, we used the same hyperparameter settings and training strategy as used by ProtoPNet for CUB-200-2011 dataset. For a fair comparison, we also set the number of prototypes for each child in HPnet to be equal to 10 similar to our implementation. We follow the same training strategy as provided by ProtoPNet for CUB-200-2011 dataset.

## 5 Results

### Fine-grained Accuracy

Similar to HPnet , we calculate the fine-grained accuracy for each leaf node by calculating the path probability over every image. During inference, the final probability for leaf class \(Y\) given an image \(X\) is calculated as, \(P(Y|X)=P(Y^{(1)},Y^{(2)},...,Y^{(L)}|X)=_{l=1}^{L}P(Y^{(l)}|X)\), where \(P(Y^{(l)}|X)\) is the probability of assigning image \(X\) to a node at level \(l\), and \(L\) is the depth of the leaf node. Every image is assigned to the leaf class with maximum path probability, which is used to compute the fine-grained accuracy. The comparison of the fine-grained accuracy calculated for HComP-Net and the baselines are given in Table 1. We can see that HComP-Net performs better than the other interpretable methods, such as INTR and HPNet, and is also able to nearly match the performance of non-interpretable models, such as ResNet-50, even outperforming it for the Fish and Butterfly dataset. This shows the ability of our proposed framework to achieve competitive classification accuracy along with serving the goal of discovering evolutionary traits.

### Generalizing to Unseen Species in the Phylogeny

We analyze the performance of HComP-Net in generalizing to unseen species that the model hasn't seen during training. The biological motivation for this experiment is to evaluate if HComP-Net can situate newly discovered species at its appropriate position in the phylogeny by identifying its common ancestors shared with the known species. An added advantage of our work is that along with identifying the ancestor of an unseen species, we can also identify the common traits shared by the novel species with known species in the phylogeny. Since unseen species cannot be classified to the finest levels (i.e., up to the leaf node corresponding to the unseen species), we analyze the ability of HComP-Net to classify unseen species accurately up to one level above the leaf level in the hierarchy. With this consideration, the final probability of an unseen species for a given image is calculated as, \(P(Y|X_{unseen})=P(Y^{(1)},Y^{(2)},...,Y^{(L-1)}|X)=_{l=1}^{L-1}P(Y^{(l)}|X)\). Note that we leave out the class probability at the \(L^{th}\) level, since we do not take into account the class probability of the leaf level. We leave four species from the Bird training set and calculate their accuracy during inference in Table 2. We can see that HComP-Net is able to generalize better than HPnet for all four species.

### Analyzing the Semantic Quality of Prototypes

Following the method introduced in PIPNet , we assess the semantic quality of our learned prototypes by evaluating their part purity. A prototype with high part purity (close to 1) is one that consistently highlights the same image region in the score maps (corresponding to consistent local features such as the eye or wing of a bird) across images belonging to the same class. The part

   Model & Hierarchy & Bird & Butterfly & Fish \\  ResNet-50 & No & **74.18** & 95.76 & 86.63 \\ INTR & & 69.22 & 95.53 & 86.73 \\  HPnet & Yes & 36.18 & 94.69 & 77.51 \\ HComP-Net & & 70.01 & **97.35** & **90.80** \\   

Table 1: % Accuracy

   Species Name & HComP-Net & HPnet \\  Fish Crow & 53.33 & 10.55 \\ Rock Wren & 53.33 & 10.22 \\ Indigo Bunting & 96.67 & 49.2 \\ Bohemian Waxwing & 70.00 & 44.9 \\   

Table 2: % Accuracy (on unseen species)

Figure 4: Comparing the part consistency of HPnet and HComP-Net for their prototype learned at an internal node in the bird dataset that corresponds to 3 descendant species (names shown on the rows). For every species, we are visualizing the top-3 images with highest prototype score for both HPnet and HComP-Net, shown as the four columns with zoomed in views of their discovered prototypes. We can see that _HPnet highlights varying parts of the bird_ across the 3 species and across multiple images of the same species, making it difficult to associate a consistent semantic meaning to its learned prototype. In contrast, _HComP-Net consistently highlights the head region_ of the bird across all four species and their images.

purity is calculated using the part locations of 15 parts that are provided in the CUB dataset. For each prototype, we take the top-10 images from each leaf descendant. We consider the \(32 32\) image patch that is centered around the max activation location of the prototype from the top-10 images. With these top-10 image patches, we calculate for each part how frequently the part is present inside the image patch. For example, a part that is found inside the image patch 8 out of 10 times is given a score of 0.8. In PIP-Net, the highest value among the values calculated for each part is given as the part purity of the prototype. In our approach, since we are dealing with a hierarchy and taking the top-10 from each leaf descendant, a particular part, let's say the eye, might have a score of 0.5 for one leaf descendant and 0.7 for a different leaf descendant. Since we want the prototype to represent the same part for all the leaf descendants, we take the lowest score (the weakest link) among all the leaf descendants as the score of the part. By following this method, for a given prototype we can arrive at a value for each part and finally take the maximum among the values as the purity of the prototype. We take the mean of the part purity across all the prototypes and report the results in Table 3 for different ablations of HComP-Net and also HPnet, which is the only baseline method that can learn hierarchical prototypes.

We can see that HComP-Net, even without the use of over-specificity loss performs much better than HPnet due to the contrastive learning approach we have adopted from PIPNet . The addition of over-specificity loss improves the part purity because over-specific prototypes tend to have poor part purity for some of the leaf descendants which will affect their overall part purity score. Further, for both ablations with and without over-specificity loss, we apply the masking module and remove masked (over-specific) prototypes during the calculation of part purity. We see that the part purity goes higher by applying the masking module, demonstrating its effectiveness in identifying over-specific prototypes. We further compute the purity of masked-out prototypes and notice that the masked-out prototypes have drastically lower part purity (\(0.29 0.17\)) compared to non-masked prototypes (\(0.77 0.16\)). An alternative approach to learning the masking module is to identify over-specific prototypes using a fixed global threshold over \(_{i}\). We show in Table 9 of Supplementary Section F, that given the right choice of such a threshold, we can identify over-specific prototypes. However, selecting the ideal threshold can be non-trivial. On the other hand, our masking module learns the appropriate threshold dynamically as part of the training process.

Figure 4 visualizes the part consistency of prototypes discovered by HComP-Net in comparison to HPnet for the bird dataset. We can see that HComP-Net is finding a consistent region in the image (corresponding to the head region) across all three descendant species and all images of a species, in contrast to HPnet. Futhermore, thanks to the alignment loss, every patch \(_{h,w}}\) is encoded as nearly a one-hot encoding with respect to the \(K\) prototypes which causes the prototype score maps to be highly localized. The concise and focused nature of the prototype score maps makes the interpretation much more effective compared to baselines.

### Analyzing Evolutionary Traits Discovered by HComP-Net

We now qualitatively analyze some of the hypothesized evolutionary traits discovered in the hierarchy of prototypes learned by HComP-Net. Figure 5 shows the hierarchy of prototypes discovered over a small subtree of the phylogeny from Bird (four species) and Fish (three species) dataset. In the visualization of bird prototypes, we can see that the two Pelican species share a consistent region in the learned Prototype labeled 2, which corresponds to the head region of the birds. We can hypothesize this prototype to be capturing the white colored crown common to the two species. On the other hand, Prototype 1 finds the shared trait of similar peak morphology (e.g., sharpness of beaks) across the two Cormorant species. We can see that HComP-Net avoids the learning of over-specific prototypes at internal nodes, which are pushed down to individual leaf nodes, as shown in visualizations of Prototype 3, 4, 5, and 6. Similarly, in the visualization of the fish prototypes, we can see that Prototype 1 is highlighting a specific fin (dorsal fin) of the _Carassius auratus_ and _Notropis hudsonius_ species, possibly representing their pigmentation and structure, which is noticeably different compared to the contrasting species of _Alosa chrysochloris_. Note that while HComP-Net identifies the common

   Model & \(_{ovsp}\) & Masking & Part purity & \% masked \\  HPnet & - & - & 0.14 \(\) 0.09 & - \\ HComP-Net & - & - & 0.68 \(\) 0.22 & - \\ HComP-Net & - & ✓ & 0.75 \(\) 0.17 & 21.42\% \\ HComP-Net & ✓ & - & 0.72 \(\) 0.19 & - \\ HComP-Net & ✓ & ✓ & **0.77 \(\) 0.16** & 16.53\% \\   

Table 3: Part purity of prototypes on **Bird** dataset.

regions corresponding to each prototype (shown as heatmaps), the textual descriptions of the traits provided in Figure 5 are based on human interpretation.

Figure 6 shows another visualization of the sequence of prototypes learned by HComP-Net for the Western Grebe species at different levels of the phylogeny. We can see that at level 0, we are capturing features closer to the neck region, indicating the likely difference between the length of necks between Grebe species and other species (Cuckoo, Albatross, and Fulmar) that diversify at an earlier time in the process of evolution. At level 1, the prototype is focusing on the eye region, potentially indicating to difference in the color of red and black patterns around the eyes. At level 2, we are differentiating Western Grebe from Horned Grebe based on the feature of bills. We also validate our prototypes by comparing them with the multi-head cross-attention maps learned by INTR . We can see that some of the prototypes discovered by HComP-Net can be mapped to equivalent attention heads of INTR. However, while INTR is designed to produce a flat structure of attention maps, we are able to place these maps on the tree of life. This shows the power of HComP-Net in generating novel hypotheses about how trait changes may have evolved and accumulated across different branches of the phylogeny. Additional visualizations of discovered evolutionary traits for butterfly species and fish species are provided in the supplementary section in Figures 7 to 16.

## 6 Conclusion

We introduce a novel approach for learning hierarchy-aligned prototypes while avoiding the learning of over-specific features at internal nodes of the phylogenetic tree, enabling the discovery of novel evolutionary traits. Our empirical analysis on birds, fishes, and butterflies, demonstrates the efficacy of HComP-Net over baseline methods. Furthermore, HComP-Net demonstrates a unique ability to generate novel hypotheses about evolutionary traits, showcasing its potential in advancing our understanding of evolution. We discuss the limitations of our work in Supplementary Section I. While we focus on the biological problem of discovering evolutionary traits, our work can be applied in general to domains involving a hierarchy of classes, which can be explored in future research.

Figure 5: Visualizing the hierarchy of prototypes discovered by HComP-Net for birds and fishes. *Note that the textual descriptions of the hypothesized traits shown for every prototype are based on human interpretation.

Figure 6: We trace the prototypes learned for Western Grebe at three different levels in the phylogenetic tree (corresponding to different periods of time in evolution). Text in blue is the interpretation of common traits of descendants found by HComP-Net at every ancestor node of Western Grebe.