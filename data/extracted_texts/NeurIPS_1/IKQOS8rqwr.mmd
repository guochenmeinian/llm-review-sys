# QuACK: Accelerating Gradient-Based Quantum Optimization with Koopman Operator Learning

Di Luo

Center for Theoretical Physics,

Massachusetts Institute of Technology,

Cambridge, MA 02139, USA

Department of Physics, Harvard University,

Cambridge, MA 02138, USA

The NSF AI Institute for Artificial

Intelligence and Fundamental Interactions

diluo@mit.edu

&Jiayu Shen

Department of Physics,

University of Illinois, Urbana-Champaign

Urbana, IL 61801, USA

Illinois Quantum Information Science

and Technology Center

Illinois Center for Advanced Studies

of the Universe

jiayus@illinois.edu

Co-first authors

&Rumen Dangovski

Department of Electrical Engineering

and Computer Science,

Massachusetts Institute of Technology

Cambridge, MA 02139, USA

rumenrd@mit.edu

&Marin Soljacic

Department of Physics,

Massachusetts Institute of Technology

Cambridge, MA 02139, USA

soljacic@mit.edu

###### Abstract

Quantum optimization, a key application of quantum computing, has traditionally been stymied by the linearly increasing complexity of gradient calculations with an increasing number of parameters. This work bridges the gap between Koopman operator theory, which has found utility in applications because it allows for a linear representation of nonlinear dynamical systems, and natural gradient methods in quantum optimization, leading to a significant acceleration of gradient-based quantum optimization. We present Quantum-circuit Alternating Controlled Koopman learning (QuACK), a novel framework that leverages an alternating algorithm for efficient prediction of gradient dynamics on quantum computers. We demonstrate QuACK's remarkable ability to accelerate gradient-based optimization across a range of applications in quantum optimization and machine learning. In fact, our empirical studies, spanning quantum chemistry, quantum condensed matter, quantum machine learning, and noisy environments, have shown accelerations of more than 200x speedup in the overparameterized regime, 10x speedup in the smooth regime, and 3x speedup in the non-smooth regime. With QuACK, we offer a robust advancement that harnesses the advantage of gradient-based quantum optimization for practical benefits.

## 1 Introduction

The dawn of quantum computing has ushered in a new era of technological advancement, presenting a paradigm shift in how we approach complex computational problems. Central to these endeavors are Variational Quantum Algorithms (VQAs) , which are indispensable for configuring and optimizing quantum computers. These algorithms serve as the backbone of significant domains,including quantum optimization  and quantum machine learning (QML) . VQAs have also influenced advances in various other quantum learning applications [17; 24; 32; 45; 28; 43; 27; 67]

At the core of VQAs lies the challenge of effectively traversing and optimizing the high-dimensional parameter landscapes that define quantum systems. To solve this, there are two primary strategies: gradient-free  and _gradient-based_ methods . Gradient-free methods, while straightforward and widely used in practice, do not provide any guarantees of convergence, which often results in suboptimal solutions. On the other hand, gradient-based methods, such as those employed by the Variational Quantum Eigensolver (VQE) [59; 78], _offer guarantees_ for convergence. This characteristic has facilitated their application across a multitude of fields, such as high-energy physics [34; 65], condensed matter physics , quantum chemistry , and important optimization problems such as max-cut problem [19; 23].

More specifically, recent developments in overparameterization theory show that gradient-based methods like gradient descent provide convergence guarantees in a variety of quantum optimization tasks [37; 44; 87]. New research indicates these methods surpass gradient-free techniques such as SPSA  in the context of differentiable analog quantum computers . Notably, the quantum natural gradient, linked theoretically with imaginary time evolution, captures crucial geometric information, thereby enhancing quantum optimization .

Despite these advantages, the adoption of gradient-based methods in quantum systems is not without its obstacles. The computation of gradients in these hybrid quantum-classical systems is notoriously _resource-intensive_ and scales linearly with the number of parameters. This computational burden presents a significant hurdle, limiting the practical application of these methods on quantum computers, and thus, the full potential of quantum optimization and machine learning. Hence, we pose the question, "_Can we accelerate gradient-based quantum optimization?_" This is crucial to harness the theoretical advantages of convergence for practical quantum optimization tasks. In this work, we answer our question affirmatively by providing order-of-magnitude speedups to VQAs. Namely, our contributions are as follows:

* By perceiving the optimization trajectory in quantum optimization as a dynamical system, similarly to [15; 61] we bridge quantum natural gradient theory, overparameterization theory, and Koopman operator learning theory , which allows for a linear represenation of nonlinear dynamical systems and thus is useful in applications.
* We propose Quantum-circuit Alternating Controlled Koopman Operator Learning (QuACK), a new algorithm grounded on this theory. We scrutinize its spectral stability, convex problem convergence, complexity of speedup, and non-linear extensions using sliding window and neural network methodologies.
* Through extensive experimentation in fields like quantum many-body physics, quantum chemistry, and quantum machine learning, we underscore QuACK's superior performance, achieving speedups over 200x, 10x, 3x, and 2x-5.5x in overparameterized, smooth, non-smooth and noisy regimes respectively.

## 2 Related Work

**Quantum Optimization Methods.** Owing to prevailing experimental constraints, quantum optimization has frequently employed gradient-free methods such as SPSA, COBYLA, and Bayesian optimization [72; 70; 76], among others that alleviate the challenges inherent in quantum optimization [89; 25; 86]. Regarding gradient-based methods, the quantum natural gradient  exhibits compelling geometric properties, and conventional gradient methods such as SGD and Adam are applicable as well. Recent developments in overparameterization theory [37; 44; 87] have provided assurances for SGD's convergence in quantum optimization. We demonstrate an example where the gradient-based methods find the minimum while the gradient-free method gets trapped in Appendix C. While meta-learning techniques [81; 33; 85] have been explored to accelerate optimization across various tasks, our focus in this work is on the acceleration of gradient-based quantum optimization, given the appealing theoretical properties of such methods. We emphasize, however, that our approach is complementary to meta-learning and could be integrated as a subroutine for potential enhancements.

Some additional lines of work are related to our study. You et al.  analyze the convergence of quantum neural networks through the lens of the neural tangent kernel. Similarly, working through an effective quantum neural tangent kernel theory, Wang et al.  propose symmetric pruning to improve the loss landscape and the convergence of quantum neural networks. Finally, Garcia-Martin et al.  study the effect of noise on overparameterization in quantum neural networks.

**Koopman Theory.** The Koopman operator theory, originating from the 1930s, furnishes a framework to understand dynamical systems [36; 80]. The theory has evolved, incorporating tools such as Dynamic Mode Decomposition (DMD)  (connected to Koopman mode decomposition [50; 66]) extended-DMD [84; 15; 61; 10; 3; 30; 79; 9], and machine learning approaches [48; 41; 4; 64]. While there have been successful applications in neural network optimization [16; 77; 60] and quantum mechanics [22; 35], the linking of this theory with quantum natural gradient and overparameterization theory for optimization acceleration, as we have done, is novel to our knowledge. Ours and the above contributions are built upon works on machine learning using Koopman operator theory [52; 53; 55; 71; 42].

## 3 Background

Variational Quantum Algorithm (VQA).For a quantum mechanical system with \(N\) qubits, the key object that contains all the information of the system is called a wave function \(\). It is an \(l_{2}\)-normalized complex-valued vector in the \(2^{N}\)-dimensional Hilbert space. A parameterized quantum circuit encodes a wave function as \(_{}\) using a set of parameters \(^{n_{}}\) via an ansatz layer on a quantum circuit in a quantum computer, as shown in the top-left part of Figure 1. The number of parameters, \(n_{}\), is typically chosen to be polynomial in the number of qubits \(N\), which scales much slower than the \(2^{N}\)-scaling of the dimension of \(\) itself in the original Hilbert space.

Variational quantum eigensolver (VQE) aims to solve minimal energy wave function of a Hamiltonian with parameterized quantum circuit. A Hamiltonian \(\) describes interactions in a physical system, which mathematically is a Hermitian operator acting on the wave functions. The energy of a wave function is given by \(()=|\). VQE utilizes this as a loss function to find the optimal \(^{*}=_{}()=_{}_{}|_{}\). Quantum machine learning follows a similar setup, aiming to minimize \(()\) involved data with parameterized quantum circuits \(_{}\), which in QML are usually referred to as quantum neural networks.

To minimize the loss function, one can employ a gradient-based classical optimizer such as Adam, which requires calculating the gradient \(/_{i}\). In the quantum case, one usually has to explicitly evaluate the loss with a perturbation in each direction \(i\), for example, using the parameter-shift rule [51; 69]: \(((_{i}+/2)-(_{i}-/2))/2\). This leads to a linear scaling of \(n_{}\) computational cost, making quantum optimization significantly more expensive than classical backpropagation

Figure 1: QuACK: Quantum-circuit Alternating Controlled Koopman Operator Learning. (a) Parameterized quantum circuits process information; loss function is evaluated via quantum measurements. Parameter updates for the quantum circuit are computed by a classical optimizer. (b) Optimization history forms a time series, the computational cost of which is proportional to the number of parameters. (c) Koopman operator learning finds an embedding of data with approximately linear dynamics from time series in (b). (d) Koopman operator predicts parameter updates with computational cost independent of the number of parameters. Loss from predicted parameters is evaluated, and optimal parameters are used as starting point for the next iteration.

which computational complexity is independent of \(n_{}\), while the required memory is still proportional to \(n_{}\). It is worth noting that the classical computational components involved in VQE, even including training neural-network-based algorithms in the following sections, typically are much cheaper than the quantum gradient cost given the scarcity of quantum resources in practice.

Quantum Natural Gradient.The quantum natural gradient method  generalizes the classical natural gradient method in classical machine learning  by extending the concept of probability to complex-valued wave functions. It is also theoretically connected to imaginary time evolution . In the context of parameter optimization, the natural gradient for updating the parameter \(\) is governed by a nonlinear differential equation \((t)=- F^{-1}_{} ((t))\), where \(\) denotes the scalar learning rate, and \(F\) represents the quantum Fisher Information matrix defined as \(F_{ij}=_{}/_{i}| _{}/_{j}-_{ }/_{i}|_{} _{}|_{}/_{ j}\).

Koopman Operator Learning.Consider a dynamical system characterized by a set of state variables \(x(t)^{n}\), governed by a transition function \(T\) such that \(x(t+1)=T(x(t))\). According to the Koopman operator theory articulated by Koopman, a linear operator \(\) and a function \(g\) exist, satisfying \(g(x(t))=g(T(x(t)))=g(x(t+1))\), where \(\) represents the Koopman operator. Generally, this operator can function in an infinite-dimensional space. However, when \(\) is restricted to a finite dimensional invariance subspace with \(g:^{n}^{m}\), the Koopman operator can be depicted as a Koopman matrix \(K^{m m}\). Data acquired from the dynamics are needed to compute the Koopman operator . The standard Dynamic Mode Decomposition (DMD) approach assumes \(g\) to be the identity function, predicated on the notion that the underlying dynamics of \(x\) are approximately linear, _i.e._, \(T\) operates as a linear function. The extended-DMD method broadens this scope, utilizing additional feature functions such as polynomial and trigonometric functions as the basis functions for \(g\). Further enhancing this approach, machine learning methods for the Koopman operator leverage neural networks as universal approximators for learning \(g\).

## 4 QuACK - Quantum-circuit Alternating Controlled Koopman Operator Learning

QuACK.Our QuACK algorithm, illustrated in Algorithm 1 and Figure 1, employs a quantum circuit \(_{}\) with parameters \(\) for quantum optimization or QML tasks. In Panel (a) of Figure 1 the loss function \(()\) is stochastically evaluated through quantum measurements, and a classical optimizer updates the parameters. In Panel (b) following \(m\) gradient optimization steps, we obtain a time series of parameter updates \((t_{0}),(t_{1}),,(t_{m})\). 2 Then in Panel (c) this series is utilized by the Koopman operator learning algorithm to find an embedding for approximately linear dynamics. In Panel (d) this approach predicts the parameter updates for \(n_{}\) future gradient dynamics steps and calculates \((t_{m+1}),(t_{m+2}),,(t_{m+n_{}})\).

In each time step, the parameters are set directly in the quantum circuit for loss function evaluation via quantum measurements. This procedure has constant cost in terms of the number of parameters, same as the forward evaluation cost of the loss function. From the \(n_{}\) loss function values, we identify the lowest loss and the corresponding optimal time \(t_{}=_{t_{m} t t_{m+n}}}( (t))\). This step includes the last VQE iteration \(t_{m}\) to prevent degradation in case of inaccurate DMD predictions. The algorithm iteratively alternates the simulation steps with the Koopman steps for \(n_{}\) steps, similarly to . To facillate the Koopman operator learning algorithm, we introduce DMD, Sliding Window DMD and neural DMD into QuACK as follows.

DMD and Sliding Window DMD.Dynamic Mode Decomposition (DMD) employs a linear fit for vector dynamics \(^{n}\), where \((t_{k+1})=K(t_{k})\). By concatenating \(\) at \(m+1\) consecutive times, we define \((t_{k}):=[(t_{k})\;(t _{k+1})(t_{k+m})]\). We create data matrices \((t_{0})\) and \((t_{1})\), the latter being the one-step evolution of the former. In approximately linear dynamics, \(K\) is constant for all \(t_{k}\), and hence \((t_{1}) K(t_{0})\). The best fit occurs at the Frobenius loss minimum, given by \(K=(t_{1})(t_{0})^{+}\), where \(+\) is the Moore-Penrose inverse.

When the dynamics of \(\) is not linear, we can instead consider a time-delay embedding with a sliding window and concatenate the steps to form an extended data matrix 

\[((t_{0}))=[(t_{0}) (t_{1})(t_{m})]= (t_{0})&(t_{1})&&(t_{m})\\ (t_{1})&(t_{2})&&(t_{m+1})\\ &&&\\ (t_{d})&(t_{d+1})&&(t_{m+d}). \]

\(\) is generated by a sliding window of size \(d+1\) at \(m+1\) consecutive time steps. Each column of \(\) is a time-delay embedding for \(\), and the different columns \(\) in \(\) are embeddings at different starting times. The time-delay embedding captures some nonlinearity in the dynamics of \(\), with \((t_{d+1}) K((t _{0}))\), where \(K^{n n(d+1)}\). The best fit is given by

\[K=*{arg\,min}_{K}\|(t_{d+1})-K((t_{0}))\|_{F}=(t_{d+1}) ((t_{0}))^{+}. \]

During prediction, we start with \((t_{m+d+2})=K(t_{m+1})\) and update from \((t_{m+1})\) to \((t_{m+2})\) by removing the oldest data and adding new predicted data. This iterative prediction is performed via \((t_{k+d+1})=K(t_{k})\). Unlike the approach of Dylewsky et al. , we do not use an additional SVD before DMD, and our matrix \(K\) is non-square. We term this method Sliding Window DMD (SW-DMD), with standard DMD being a specific case when the sliding window size is 1 (\(d=0\)). The time-delay embedding is related Takens' theorem  with similar implementations in Hankel DMD  and streaming DMD .

Neural DMD.To provide a better approximation to the nonlinear dynamics, we ask whether the hard-coded sliding window transformation \(\) can be a neural network. Thus, by simply reformulating \(\) in Eq. 2 as a neural network, we formulate a natural neural objective for Koopman operator learning as \(_{K,}\|(t_{d+1})-K_{ }((t_{0}))\|_{F}\), where \(K^{N_{in} N_{out}}\) is a linear Koopman operator and \(_{}((t_{0}))\) is a nonlinear neural embedding by a neural network \(_{}\) with parameters \(\). \(_{}:=_{}\) is a composition of the neural network architecture \(_{}\) and the sliding window embedding \(\) from the previous section.

Drawing from the advancements in machine learning for DMD, we introduce three methods: MLP-DMD, CNN-DMD, and MLP-SW-DMD. MLP-DMD uses a straightforward MLP architecture for \(\), comprising two linear layers with an ELU activation and a residual connection, as shown in the Appendix. Unlike MLP-DMD, CNN-DMD incorporates information between simulation steps, treating these steps as the temporal dimension and parameters as the channel dimension of a 1D CNN encoder shown in the Appendix. To avoid look-ahead bias, we use causal masking of the CNN kernels, and the architecture includes two 1D-CNN layers with a bottleneck middle channel number of 1 and ELU activation to prevent overfitting. MLP-SW-DMD is similar to MLP-DMD but includes the time-delay embedding in the input, a feature we also add to CNN-DMD. As a result, MLP-DMD and CNN-DMD extend the principles of DMD, whereas MLP-SW-DMD and CNN-DMD with time-delay embedding generalize from SW-DMD. More details are available in the Appendix.

Limitations.QuACK is currently only equipped with relatively simple neural network architecture while more advanced architectures like Transformer can be explored for future work. Even though QuACK reduces the number of necessary gradient steps largely and thus achieves speedup, training of VQA with a few gradient steps is still required to obtain the data for Koopman operator learning. \(n_{}\) for the training gradient steps in our design is a hyperparameter (see ablation study in Appendix), and we do not have a theoretical formula for it yet.

## 5 Theoretical Results

Connection to Quantum Nature Gradient.While there exists a Koopman embedding that can linearize a given nonlinear dynamics, we provide more insights between Koopman theory and gradient-based dynamics under quantum optimization. The dynamical equation from quantum natural gradients \(d(t)/dt=- F^{-1}_{}((t))\) is equivalent to \(d_{}(t)/dt=-_{_{}}_{ }(t)\), where \(F\) is the quantum Fisher information matrix, and \(_{_{}}\) represents a projector onto the manifold of the parameterized quantum circuit. When the parameterized quantum circuit possesses full expressivity, it covers the entire Hilbert space, resulting in \(d_{}(t)/dt=-_{}(t)\). This is a linear differential equation in the vector space of unnormalized wave functions. The normalized \(_{}\) from the parameterized quantum circuit together with additional normalization factor function \(N()\) can serve as an embedding to linearize the quantum natural gradient dynamics. For a relative short time scale, the normalized \(_{}\) already provides a good linearization which only differs from the exact dynamics by the normalization change in the short time scale. The special underlying structure of quantum natural gradient may make it easier to learn the approximate linear dynamics for Koopman operator learning.

Connection to Overparameterization Theory.Recent advancement on overparameterization theory [37; 44; 87] finds that when the number of parameters in the quantum circuit exceed a certain limit, linear convergence of the variational quantum optimization under gradient descent is guaranteed. You et al.  shows that the normalized \(_{}\) from the parameterized quantum circuit in the overparatermization regime follows a dynamical equation which has a dominant part linear in \(_{}\) with additional perturbation terms. Similar to the quantum natural gradient, the overparameterization regime could provide an effective structure to simplify the Koopman operator learning with approximate linear dynamics.

### Stability Analysis

We first note that the direct application of DMD without alternating the controlled scheme will lead to unstable or trivial performance in the asymptotic limit.

**Theorem 5.1**.: _Asymptotic DMD prediction for quantum optimization is trivial or unstable._

Proof.: The asymptotic dynamics from DMD prediction is given by \((T)=K^{T}(t_{0})\) for \(T\). It follows that \((T) w_{m}^{T}v_{m}\), where \(w_{m}\) and \(v_{m}\) are the largest magnitude eigenvalue and the corresponding eigenvector of the Koopman operator \(K\). If \(|w_{m}|<1\), then \((T)\) will converge to zero, and the quantum circuit will reach a trivial fixed state. If \(|w_{m}| 1\), \((T)\) will keep oscillating or growing unbounded. For a unitary gate \(U()=e^{-i P}\) where \(P\) is a Pauli string, \(U()\) is periodic with respect to \(\). The oscillation or unbounded growing behavior of \((T)\) will lead to oscillation of the unitary gate in the asymptotic limit resulting in unstable performance. 

The above issue is also found to exist in numerical results plotted in Appendix, indicating that the eigenvalues of the Koopman operators from quantum optimization dynamics can lead to trivial or unstable DMD prediction, which motivates us to develop QuACK. Indeed, our QuACK has controllable performances given by the following

**Theorem 5.2**.: _In each iteration of QuACK, the optimal parameters \((t_{})\) yield an equivalent or lower loss than the \(m\)-step gradient-based optimizer._

Proof.: From \(t_{}=_{t_{m} t t_{m+n_{}}}( (t))\), we have \(((t_{}))((t_{m}))\) where \(((t_{m}))\) is the final loss from the \(m\)-step gradient-based optimizer. 

As long as QuACK is able to capture certain dynamical modes that decrease the loss, then it will produce a lower loss even when the predicted dynamics does not exactly follow the same trend of the \(m\)-step gradient descent updates. We also note that it is possible for QuACK to converge to a different local minimum than the baseline gradient-based optimizer, but our experiments generally demonstrate that our QuACK achieves accuracy the same as or even better than the baseline with much faster convergence. Our procedure is robust against long-time prediction error and noise with much fewer gradient calculations, which can efficiently accelerate the gradient-based methods. Furthermore, our scheme has the following important _implication_, the proof of which we provide in Appendix.

**Corollary 5.3**.: _QuACK achieves superior performance over the asymptotic DMD prediction._

### Complexity and Speedup Analysis

To achieve a certain target loss \(_{}\) near convergence, the traditional gradient-based optimizer as a baseline takes \(T_{b}\) gradient steps. To achieve the same \(_{}\), QuACK takes \(T_{Q,t}=T_{Q,1}+T_{Q,2}\) steps where \(T_{Q,1}\) (\(T_{Q,2}\)) are the total numbers of \(\) training (prediction) steps. We denote the ratio between the computational costs of the baseline and QuACK as \(s\), which serves as the speedup ratio from QuACK. Our definition of speedup has some difference with the speedup defined in Ref.  although shares a similar spirit. In gradient-based optimization, the computational costs of each step of QuACK training and prediction are different, with their ratio defined as \(f(p)\) where \(p:=n_{}\). In general, \(f(p)\) is \((p)\) since only QuACK training, not prediction, involves gradient steps.

**Theorem 5.4**.: _With a baseline gradient-method, the speedup ratio \(s\) is in the following range_

\[a s a}+n_{})}{f(p)n_{}+n_{}}. \]

_where \(a:=T_{b}/T_{Q,t}\). In the limit of \(n_{}\) the upper bound can be achieved._

We present the proof in the Appendix. \(a\) is a metric of the accuracy of Koopman prediction. Higher \(a\) means better prediction, and a perfect prediction has \(a=1\). The exact form of \(f(p)\) depends on the details of the gradient method. For example, we have \(f(p)=2p+1\) for parameter-shift rules and \(f(p) p^{2}+p\) for quantum natural gradients . If \(a\) is fixed, then the upper bound of \(s\) in Eq. 13 increases when \(p\) increases with an asymptote \(a(n_{}+n_{})/n_{}\) at \(p\). The upper bound in Eq. 13 implies \(s af(p)\) where the equal sign is achieved in the limit \(n_{}/n_{}\). If \(a=1\) one could in theory achieve \(f(p)\)-speedup, at least linear in \(n_{}\).

In practice, the variable \(a\) is influenced by \(n_{}\) and \(n_{}\). A decrease in \(a\) can occur when \(n_{}\) diminishes or \(n_{}\) enlarges, as prediction accuracy may drop if the optimization dynamics length for QuACK training is insufficient or the prediction length is too long. Moreover, estimating the gradient for each parameter requires a specific number of shots \(n_{}\) on a fixed quantum circuit. Quantum measurement precision usually follows the standard quantum limit \( 1/}}\), hence a finite \(n_{}\) may result in noisy gradients for Koopman operator learning, influencing \(a\). Intriguingly, when the prediction aligns with the pure VQA, QuACK's loss might decrease faster than pure baseline VQA, leading to \(a>1\) and a higher speedup. This could be due to dominant DMD spectrum modes with large eigenvalues aligning with the direction of fast convergence in the \(\)-space.

## 6 Experiments

### Experimental Setup

We adopt the _Relative loss_ metric for benchmark, which is \((-_{})/(_{}-_{})\), where \(\) is the current loss, and \(_{}\) and \(_{}\) are the initial and minimum loss of full VQA. We use pure VQE  and pure QML  as the baseline. We use \(_{}\) at 1% relative loss for VQE and the target test accuracy (0.5% below maximum) for QML to compute the computational costs and the speedup \(s\) as a metric of the performance of QuACK. Our experiments are run with Qiskit , Pytorch , Yao  (in Julia ), and Pennylane . Details of the architectures and hyperparameters of our experiments are in Appendix. More ablation studies for hyperparameters are also in Appendix.

**Quantum Ising Model.** Quantum Ising model with a transverse field \(h\) has the Hamiltonian \(=-_{i=1}^{N}Z_{i} Z_{i+1}-h_{i=1}^{N}X_{i}\) where \(\{I_{i},X_{i},Y_{i},Z_{i}\}\) are Pauli matrices for the \(i\)-th qubit. On the quantum circuit, we then use the RealAmplitudes ansatz and hardware-efficient ansatz which then define \(_{}\), and then \(()=_{}|_{}\). We implement the VQE algorithm and QuACK for \(h=0.5\) using gradient descent, quantum natural gradient, and Adam optimizers.

**Quantum Chemistry.** Quantum chemistry is of great interest as an application of quantum algorithms, and has a Hamiltonian \(=_{j=0}^{n_{P}}h_{j}P_{j}\) with \(n_{P}\) polynomial in \(N\), where \(P_{j}\) are tensor products of Pauli matrices, and \(h_{j}\) are the associated real coefficients. The loss function of quantum chemistry is typically more complicated than the quantum Ising model. We explore the performance of QuACK by applying it to VQE for the 10-qubit Hamiltonian of a LiH molecule with a interatomic distance 2.0 A provided in Pennylane, with Adam and the RealAmplitudes ansatz.

**Quantum Machine Learning.** In addition to VQE, we consider the task of binary classification on a filtered MNIST dataset with samples labeled by digits "1" and "9". We use an interleaved block-encoding scheme for QML, which is shown to have generalization advantage [29; 13; 40; 63] and recently realized in experiment . We use a 10-qubit quantum circuit with stochastic gradient descent for QML and QuACK, and a layerwise partitioning  in \(\) for neural DMD. Similar to the target loss, we set the target test accuracy as 0.5% below the maximum test accuracy to measure the closeness to the best test accuracy during the QML training, and use the ratios of cost between pure QML and QuACK achieving the target test accuracy as the speedup ratio.

### Accurate Prediction for Quantum Natural Gradients by QuACK

In Figure 1(a), we present the 5-qubit quantum Ising model results (learning rate \(0.001\)) with the standard DMD method in the QuACK framework. We observe that the DMD prediction is almost perfect, i.e. \(a 1\). The speedup in Figure 1(a) is 20.18x close to 21.19x, the theoretical speedup from the upper bound in Eq. 13 under \(a=1\). This experiment shows (1) the success of DMD in predicting the dynamics (2) the power of QuACK of accelerating VQA (3) the precision of our complexity and speedup analysis in Sec. 5.2. We show 10-qubit results for all DMD methods in the Appendix.

Figure 3: Experimental results for (a) LiH molecule with 10 qubits using Adam (b) Quantum Ising model with 12 qubits using Adam (c) test accuracy of binary classification in QML. The solid piecewise curves are true gradient steps, and the dashed lines connecting them indicate when the DMD prediction is applied to find \((t_{})\) in our controlled scheme. Our QuACK with all the DMD methods bring acceleration, with maximum speedups (a) 4.63x (b) 3.24x (c) 4.38x.

Figure 2: Performance of our QuACK with the standard DMD in the following cases. (a) For quantum natural gradient, with short training (4 steps per piece) and long prediction (40 steps per piece), DMD accurately predicts the intrinsic dynamics of quantum optimization, and QuACK has 20.18x speedup. (b) In the overparameterization regime, QuACK has >200x speedup with 2-5 qubits. (c) In smooth optimization regimes, QuACK has >10x speedup with 2-12 qubits.

### More than 200x Speedup near the Overparametrization Regime by QuACK

As it is used by the overparameterization theory in You et al. , we use the 250-layer hardware-efficient ansatz  on the quantum Ising model with gradient descent, with numbers of qubits \(N=2,3,4,5\) so \(n_{ params}=1000,1500,2000,2500\), all in the regime of \(n_{ params}(2^{N})^{2}\), near the overparameterization regime . In addition, we use a small learning rate 5e-6 so that the dynamics in the \(\)-space is approximately linear dynamics. We expect QuACK with the standard DMD to have good performance in this regime. In Figure 1(b), for each \(n_{ params}\), we randomly sample 10 initializations of \(\) to calculate mean values and errorbars and obtain >200x speedup in all these cases. The great acceleration from our QuACK is an empirical validation and an application of the overparameterization theory for quantum optimization. On the other hand, the overparameterization regime is difficult for near-term quantum computers to realize due to the large number of parameters and optimization steps, and our QuACK makes its realization more feasible.

### More than 10x Speedup in Smooth Optimization Regimes by QuACK

For the quantum Ising model with gradient descent and 2-layer RealAmplitudes, rather than the overparameterization regime, we consider a different regime with lower \(n_{ params}\) with learning rate 2e-3 so that the optimization trajectory is smooth and the standard DMD is still expected to predict the dynamics accurately for a relatively long length of time. With the number of qubits \(N\) (\(n_{ params}\)) and 100 random samples for initialization of \(\) for each \(n_{ params}\), in Figure 1(c), our QuACK achieves >10x speedup in all these cases. This shows the potential of our methods for this regime with fewer parameters than overparameterization, which is more realistic on near-term quantum hardware.

### More than 3x Speedup in Non-smooth Optimization by QuACK

We further demonstrate speedup from QuACK in the non-smooth optimization regimes with learning rate 0.01 for examples of quantum Ising model, quantum chemistry, and quantum machine learning with performance shown in Figure 3 with numerical speedups in Appendix. Only the gradient steps are plotted, but the computational cost of QuACK prediction steps are also counted when computing speedup. All the 5 DMD methods are applied to all the examples. Our QuACK with all the DMD methods accelerates the VQA, with maximum speedups (a) LiH molecule: 4.63x (b) quantum Ising model: 3.24x (c) QML: 4.38x. These applications are of broad interest across different fields and communities and show that our QuACK works for a range of loss functions and tasks.

    & & &  \\  Noise & \)} &  & SW- & MLP- & MLP- & CNN- \\  & & & DMD & DMD & SW-DMD & DMD \\  & & & (ours) & (ours) & (ours) & (ours) \\ 
10-qubit & 100 & 5.51x & **5.54x** & 3.25x & 3.23x & 1.99x \\ shot noise & 1,000 & 3.20x & **4.36x** & 2.38x & **4.36x** & 2.54x \\  & 10,000 & 1.59x & **3.49x** & 1.59x & 2.41x & 1.96x \\ 
5-qubit & 100 & 1.50x & 2.64x & 1.83x & **4.57x** & 1.47x \\ shot noise & 1,000 & 1.39x & **3.84x** & **1.45x** & 2.25x & 1.95x \\  & 10,000 & 1.49x & **3.43x** & 1.80x & 2.00x & 1.78x \\ 
5-qubit & 100 & **2.44x** & 2.24x & 2.43x & 2.15x & 2.34x \\ FakeLima & 1,000 & 1.50x & **2.61x** & 2.07x & 2.41x & 1.84x \\  & 10,000 & 1.48x & 2.32x & 1.93x & **2.56x** & 1.92x \\ 
5-qubit & 100 & 2.14x & 2.51x & 2.54x & **2.91x** & 1.25x \\ FakeManila & 1,000 & 1.49x & 2.02x & 1.90x & **2.09x** & 1.89x \\  & 10,000 & 1.95x & 2.13x & 2.13x & **2.27x** & 1.82x \\   

Table 1: Speedup ratios of our QuACK with all DMD methods for various noise systems.

Figure 4: Noisy quantum optimization with \(n_{ shots}=100\). (a) 10-qubit shot noise system (b) 5-qubit FakeManila

### 2x to 5.5x Speedup from Ablation the Robustness of QuACK to Noise

Near-term quantum computers are imperfect and have two types of noise: (1) shot noise from the probabilistic nature of quantum mechanics, which decreases as \(1/}\), (2) quantum noise due to the qubit and gate errors which cannot be removed by increasing \(n_{ shots}\). Therefore, we consider two categories of ablation studies (1) with shot noise only (2) with both shot noise and quantum noise. We apply our QuACK with all 5 DMD methods to 4 types of noise systems: 10-qubit and 5-qubit systems with only shot noise, FakeLima, and FakeManila. The latter two are noise models provided by Qiskit to mimic 5-qubit IBM real quantum computers, Lima and Manila, which contain not only shots noise but also the machine-specific quantum noise. We use the quantum Ising model with Adam and show generic dynamics in Figure 4 with statistical error from shots as error bands. The fluctuation and error from baseline VQE in Figure 4(a) 100-shot 10-qubit system are less than Figure 4(b) 100-shot FakeManila. Our QuACK works well in (a) up to 5.54x speedup and in (b) up to 2.44x speedup. In all examples, we obtain speedup and show them in Table 1, which demonstrate the robustness of our QuACK in realistic setups with generic noisy conditions. We have also implemented an experiment to accelerate VQE on the real IBM quantum computer Lima with results in Appendix.

## 7 Conclusion

We developed QuACK, a novel algorithm that accelerates quantum optimization. We derived QuACK from connecting quantum natural gradient theory, overparameterization theory, and Koopman operator learning theory. We rigorously tested QuACK's performance, robustness, spectral stability, and complexity on both simulated and real quantum computers across a variety of scenarios. Notably, we observed orders of magnitude speedups with QuACK, achieving over 200 times faster in overparameterized regimes, 10 times in smooth regimes, and 3 times in non-smooth regimes. This research highlights the significant potential of Koopman operator theory for accelerating quantum optimization and lays the foundation for stronger connections between machine learning and quantum optimization. We elaborate more on the broader impact of our work in the Appendix.