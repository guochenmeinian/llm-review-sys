# On the Complexity of Teaching a Family of Linear Behavior Cloning Learners

Shubham Bharti

UW-Madison

skbharti@cs.wisc.edu

&Stephen Wright

UW-Madison

swright@cs.wisc.edu &Adish Singla

MPI-SWS

adishs@mpi-sws.org &Xiaojin Zhu

UW-Madison

jerryzhu@cs.wisc.edu

###### Abstract

We study optimal teaching for a family of Behavior Cloning learners that learn using a linear hypothesis class. In this setup, a knowledgeable teacher can demonstrate a dataset of state and action tuples and is required to teach an optimal policy to an entire family of BC learners using the smallest possible dataset. We analyze the linear family and design a novel teaching algorithm called 'TIE' that achieves the instance optimal Teaching Dimension for the entire family. However, we show that this problem is NP-hard for action spaces with \(||>2\) and provide an efficient approximation algorithm with a \((||-1)\) guarantee on the optimal teaching size. We present empirical results to demonstrate the effectiveness of our algorithm in different teaching environments. The code is available at [https://github.com/skbharti/Optimal-Teaching-Linear-BC-Family](https://github.com/skbharti/Optimal-Teaching-Linear-BC-Family)

## 1 Motivation

Behavior Cloning (BC)  is an important paradigm of learning in Reinforcement Learning (RL), that has been applied extensively to solve real-world problems like teaching machines to drive autonomous vehicles , fly planes , perform robotic manipulations  etc. These real-world environments have large state space where the ability to generalize using linear or neural hypothesis class becomes essential for effective learning.

However, naively teaching an optimal policy to a BC learner using i.i.d. sample often demands a dataset that scales with the horizon length, the complexity of learner's hypothesis class and desired error . In many scenarios, like teaching to drive cars, an expert teacher may know a (near)-optimal policy and can leverage this knowledge to construct a small, non-i.i.d. dataset to teach the target policy to the BC learner far more efficiently. This problem is known as Machine Teaching and the size of smallest teaching set so produced is called Teaching Dimension (TD) .

Several existing works  have studied optimal teaching in linear settings, primarily targeting individual surrogate learners, such as linear support vector machines (SVM). These surrogate learners often exhibit optimization biases, arguably making it easier to teach them individually. Consequently, the teaching set for a specific learner is often highly tailored to their biases, limiting its effectiveness for others. In contrast, in many real-world scenarios, such as teaching a classroom of students , the teacher must teach the entire class of students with a single lesson, even though each student may have unique biases. In this work, we focus on the task of optimally teaching a family of linear BC learners that satisfy the consistency property, meaning each learner in this family produce a (subset of) hypotheses consistent with a demonstration dataset. We seek to answer the following question:_What is the smallest dataset required to teach a policy to a family of consistent linear BC learners?_

To demonstrate the effectiveness of optimally teaching the linear BC family with a single dataset, we consider the following example.

**Example 1** (Pick the Right Diamond).: _The game is shown in Figure 0(a). There is a board with \(n=6\) slots where each slot can have one of 4 different types of diamonds or can be empty. The game rule says that one must pick the most expensive diamond i.e. one with the highest number of edges, first; and if there are ties one must pick the rightmost one. The game continues until the board is empty. The teacher wants to find a minimal demonstration set to convey this rule to the agent._

_There are \(5^{n}-1\) number of states with \(=[n]\). Consider the family of consistent linear BC learners with a two-dimensional feature space denoting slot index and the number of edges in the slot. A naive teacher would demonstrate target action in all \(5^{n}-1\) states which grows exponentially with \(n\). However, a clever teacher succeeds by just demonstrating two states(refer to Section 4.1 for complete results), thereby significantly saving the teaching cost from \(O(5^{n})\) to 2._

Towards our goal of optimal teaching, we make the following contributions:

1. We formulate the problem of optimally teaching a family of linear BC learners and show that this problem is equivalent to teaching the hardest member in the family, i.e., a linear version space learner (Lemma 1).
2. We characterize optimal teaching in terms of covering extreme rays of primal cone and design a novel algorithm called 'TIE' 1 to optimally teach the family (Theorem 4).
3. However, as shown in Theorem 5, solving this problem is NP-hard and we propose an efficient algorithm with an approximation ratio of \((||-1)\) on TD (Theorem 6).
4. Through a set of experiments on real-world environments, we demonstrate the effectiveness of our TIE algorithm compared to other baselines (Section 4).

## 2 Problem Formulation

Consider a Markov Decision Process (MDP) \(=(,,R,P,,)\) where \(\) is a state space, \(\) a finite action space, \(R:\) is reward function, \(P:()\) is transition function, \(\) is the discount factor and \(\) is the initial state distribution. For simplicity, we assume \(\) is finite, however, our analysis also extends to infinite case under reasonable assumptions. Let \(:^{d}\) be a feature function that defines a structured linear policy class. Given a fixed \(w^{d}\), it induces a set of policies \(_{w}\) defined as follows:

\[ s,\,_{w}(s)=(*{arg\,max}_{a }w^{}(s,a)).\]

Consider a linear hypothesis class \(=^{d}\) and let \(=_{w}_{w}\), \(_{}=\{_{w}:_{w}^{}\}\) be the set of all stochastic and deterministic policies induced by \(\) respectively. The value of a policy \(\) in MDP \(\) is given by \(V^{}_{}=_{,P}[_{t=0}^{}^{t}r(s_{a},a_{ t})]\). Furthermore, a class optimal policy \(^{*}\) is the one that maximizes value among all policies in the linear class, i.e., \(^{*}=*{arg\,max}_{}V^{}_{}\).

Figure 1: a) A board in a “Pick the Right Diamond” game. In this example 1, the target policy says to pick the diamond with the highest edge breaking the tie in favor of the rightmost slot if any. There are a total of \(5^{n}-1\) candidate teaching state and action pairs. We ask what is the minimum set of demonstrations of such boards would allow the teacher to teach the target policy to consistent linear learners. b) Only two carefully chosen demonstrations are sufficient to teach.

### The Learner Family

We consider a Behavior Cloning (BC) learner \(:D 2^{}\) that learns using a linear policy class \(=^{d}\). On receiving a dataset \(D=\{(s_{i},a_{i}):i[n]\}\), it aims to learn a 'good' policy by imitating the dataset using a supervised learning algorithm [1; 27].

Given a dataset \(D\), the learner maintains a set \(_{l}(D)\) of empirical risk minimizing (ERM) hypotheses defined by a loss function \(:^{+}\), i.e.,

\[_{}(D)_{_{w},w}_{( s,a) D}_{a^{}(s)}[(a^{},a)].\]

During deployment, the learner first arbitrarily selects a \(w_{}(D)\) and a \(_{w}\) and then uses \(\) to execute all its actions. Correspondingly, it suffers a worst-case value risk of \(R(D;)=V_{}^{^{*}}-_{_{w},w_{}( D)}V_{}^{}\) in the MDP environment. We remark that a BC learner is nothing but a supervised learner applied to RL setting.

Consistent Linear BC Learners:We consider teaching a family of linear BC learners that have the consistency property and denote the family by \(\). The consistency property is as follows: given any realizable dataset \(D\), i.e., a dataset generated by any policy \(_{}\), \(\) always maintains a non-empty subset of hypotheses consistent with \(D\), i.e.,

\(_{},D(_{s}\{(s,(s))\}),\) we have that, \( w_{}(D),_{w}(s)=(s),\,(s,(s)) D.\)

In linear settings, many well-known learners, such as the linear support vector machine(SVM), linear perceptron are consistent learners. We remark that each consistent learner may have their own bias to prefer certain consistent hypotheses over others which is directly influenced by their surrogate loss function or update methods [14; 29]. For example, an SVM learner always prefers a max-margin hypothesis over other hypotheses.

However, this family also contains arguably the most simplest linear BC learner, one that maintains the entire version space of consistent hypotheses and does not have any bias to prefer one consistent hypothesis over the other. We call it a linear version space(LVS) learner.

Linear Version Space (LVS) Learner:An LVS learner maintains the entire version space of hypothesis \((D)\) consistent with input dataset \(D\), i.e.,

\[(D)=\{w^{d}:w^{}((s,a)-(s,b))>0,\,( s,a) D,a b\}. \]

Equivalently, it does empirical risk minimization with respect to zero-one loss, i.e., \((D)=_{}(D)\). Note that for a realizable dataset \(D,(D)\{\}\) is an open polyhedral cone in \(^{d}\).

**Remark 1**.: _We introduce the following notation: let \(_{sab}:=(s,a)-(s,b)\) be the feature difference vector for preferring action a over \(b\) in state \(s\), \((D)\) be the set of all feature difference vectors induced by dataset \(D\), i.e., \((D)=\{_{sab}:(s,a) D,b,b a\}\). We define the primal cone of \((D)\) as \(((D)):=\{_{(D)}_{}: _{} 0, 0\}\), and its dual as \(^{*}((D)):=\{w^{d}: w,>0, (D)\}\). Note that the version space is the dual cone of \((D)\), i.e., \((D)=^{*}((D))\). We refer to Example 2 for an illustration._

### The Teacher

In our setup, there is a helpful teacher who controls the dataset \(D\) provided to the learner. The teacher knows an optimal deterministic policy \(^{*}:\) induced by a \(w^{*},i.e.,^{*}=_{w^{*}}\) and has the following teaching objective:

_It wants to unambiguously teach the target policy \(^{*}\) to the entire family of consistent linear BC learners \(\) using as few demonstrations as possible._

We remark that our framework can handle teaching any deterministic policy in \(_{}\) to the learner. But for simplicity, we will consider teaching an optimal deterministic policy \(^{*}\). Formally, given a teaching instance \((,,^{*})\), the optimal teaching problem of the teacher is defined by the following optimization problem:

\[() D^{*}_{D} |D|\] s.t. \[, ^{*}. \]This formulation models a classroom teaching setting, where the teacher is required to teach \(^{*}\) to all learners in \(\) using a single dataset which is more challenging than teaching individual biased learners studied in prior works . The size of the optimal teaching set \(TD(^{*};)=|D^{*}|\) is called the teaching dimension(TD) of the family \(\).

**Remark 2**.: _Teaching the entire family \(\) has its drawback; if the teacher knows the learning bias of a specific learner, it may be able to possibly teach them with a smaller dataset. For example, to optimally teach a linear SVM in \(^{d}\) just requires two examples . However, such individual learner-specific teaching sets may not even be a valid teaching set for other learners in the \(\) like version space learners, hence useless for teaching the entire family. See Figure 2(a) for an example._

In a finite state setting, a naive teacher could succeed in teaching by demonstrating a full dataset \(D_{}=\{(s,^{*}(s)):s\}\) to the learner. However, teaching on entire state space can be suboptimal and prohibitively expensive for large state space environments. A clever teacher who knows \(^{*}\) can utilize the linear feature function of the learner family to teach \(^{*}\) to them using a much smaller dataset. As shown in Example 2, demonstrating \(^{*}\) on only one state is sufficient for teaching on the entire state space.

We recall that our problem 2 requires the teacher to teach \(^{*}\) to all learners in \(\), which includes a large and diverse set of learners. In fact, enumerating all consistent learners may not even be practical. To address this issue, our next lemma shows that it is sufficient to focus on teaching the most challenging member of the family, i.e., the linear version space learner. The proof can be found in the Appendix.

**Lemma 1**.: _Optimally teaching the family of consistent linear BC learners is equivalent to optimally teaching the linear version space BC learner._

Hence, the teacher can achieve its objective by just focusing on optimally teaching the LVS learner. From now on, we will focus on optimally teaching \(^{*}\) to an LVS learner given by the following optimization problem:

\[LVS)} D^{*}_{D} |D|\] s.t. \[ w(D),_{w}, (s)=^{*}(s), s. \]

This requires finding a minimal data \(D^{*}\) that induces \(^{*}\) uniquely as the version space under \(D^{*}\).

Previous works have studied the problem of optimal teaching of version space learners, but have mostly been limited to either a finite hypothesis setting  or highly structured hypothesis classes like axis-aligned rectangles  which is very different from our structured linear setting. Before delving into the algorithm, we present an illustrative example in \(^{2}\).

**Example 2** (An instance of teaching linear version space BC learner in \(^{2}\)).: _Let \(=\{s,t,u\}\), \(=\{a,b,c\}\), and \(^{*}(s)=a,\, s\). Consider the full demonstration set \(D=\{(s,a),(t,a),(u,a)\}\) that induce \((D)=\{_{sab},_{sac},_{tab},_{tac},_{uab},_{uac}\}\) as indicated by dots in Figure 1(a). The primal cone \(((D))\) is shown in blue, and the version space \((D)\) is in green. We note that the primal cone is supported by two extreme rays._

_The subset \(D^{}\) is not valid/feasible teaching set as its version space \((D^{})\) (shown in green in Figure 1(b)) is wider than \((D)\) and contains some \(w\)'s that do not induce \(^{*}\) in all states, thus violating the feasibility condition in equation 3. On the other hand, both \(D^{}\) and \(D^{}\) induce the

Figure 2: A simple illustration on importance of extreme rays. \(D,D^{},D^{}\) succeed in teaching but \(D^{}\) fails depending on if they cover the extreme rays of \(((D))\).

correct version space \((D_{})\) (as shown in green in Figures 1(c) and 1(d)) on the learner and succeeds in teaching \(^{*}\) to it. Furthermore, \(D^{}\) which consists of teaching on only one state is the optimal set. The problem becomes challenging as we move to higher dimensions where we can have a large number of extreme rays as shown in Figure 2(b)._

## 3 Teaching Algorithm and Analysis

We first describe a naive teaching algorithm that frames optimal teaching as an infinite set covering problem in the hypothesis space. This approach underscores the challenge of addressing our problem using the greedy inconsistent hypothesis elimination algorithm proposed in prior works .

### Optimal Teaching as an Infinite Set Cover Problem in \(w\) Space

We observe that demonstrating \(^{*}(s)\) on a state \(s\) induces \(||-1\) feature difference vectors \(_{s^{*}(s)}=\{_{s^{*}(s)b}:\,b,b^{*}(s)\}\) in the primal (feature) space and correspondingly a version space \(^{*}(_{s^{*}(s)})=\{w^{d}:\,w^{}>0, _{s^{*}(s)}\}\) in the dual (weight) space of the LVS learner. Each such inequality, \(w^{}_{s^{*}(s)b}>0\), eliminates a halfspace \(W_{sb}:=\{w:w^{}_{s^{*}(s)b} 0\}^{d}\). Therefore, the effect of demonstrating \((s,^{*}(s))\) is to eliminate the set of weights \(W_{s}:=_{b^{*}(s)}W_{sb}=(^{*}(_{s^{*}(s)}))^{C}\). The full demonstration set \(D_{}=_{s}\{(s,^{*}(s))\}\) over all states eliminates the union \(_{s}W_{s}\), such that only the consistent version space \((D_{})=\{w^{d}:w^{}_{s^{*}(s)b}>0, s,b,b a\}\) survives.

The optimal teaching problem requires finding the smallest demonstration set that produces \((D_{})\) in the dual space which is equivalent to covering/eliminating the infinite set of inconsistent weights \((D_{})^{C}\) by a smallest finite collection of infinite subsets \(\{W_{s}\}_{s}\). This is an infinite set cover problem in the weight space given as follows:

\[_{T}|T|(D_{})^{C}=_{t T}W_{t}.\]

At first glance, solving this problem may seem daunting. Certainly, since the inconsistent hypotheses set is uncountably infinite, we cannot keep track of inconsistent weights that have been eliminated so far and perform a greedy hypotheses elimination by greedily selecting the state that eliminates the maximal number of inconsistent hypotheses, as proposed by prior works [16; 17].

However, we note that \((D_{})=^{*}((D_{}))\) has a nice polyhedral cone structure that can be utilized further to simplify our problem as we show in the next section.

Teaching as a Finite Set Cover Problem on Extreme Rays of Primal \(((D_{}))\)

To overcome the challenge mentioned above, we characterize the target version space cone \((D_{})\) in terms of extreme rays of primal \(((D_{}))\) and devise an optimal teaching algorithm based on this insight. Before doing that, we introduce some definitions below.

Figure 3: a.) Optimal teaching set \(D\) of a (biased) consistent learner like SVM induce a larger space of weights \(w\) some of which (shown in yellow region) are inconsistent wrt \(^{*}\) and so they cannot succeed in teaching LVS learner and the entire family of consistent learners. b.) Optimal teaching example in higher dimension \(d 3\) can have a large number of extreme rays to be covered using a subset of states making it an NP-hard problem 5.

**Definition 1** (Extreme Ray and its Cover).: _A ray \(\) induced by a vector \(v^{d}\{0\}\) is the set \(=\{cv:c>0\}\). Any vector in \(\) serves as a representative of \(\). A ray \(\) is called an extreme ray of a cone \(K^{d}\) if for any \(x,y K\), \(x+y x,y\). We say that a state \(s\) covers a ray \(\) if \( b^{*}(s):_{^{*}(s)b}\). Similarly, \(T\) is said to cover \(\) if \( s T\) that covers \(\)._

Recall that demonstrating \(^{*}\) on a state \(s\) induces the feature difference set \(_{s^{*}(s)}\) in the primal space. Collectively teaching \(^{*}\) on entire \(\) induces feature difference set \((D_{})\) in primal and correspondingly version space \(^{*}((D_{}))\) in the dual space. By definition, a \(w^{d}\) induces \(^{*}\) if and only if \(w^{*}((D_{}))\). Thus, for successful teaching 2, the teacher needs to exactly induce the version space \(^{*}((D_{}))\) in the dual space of the learner. This is equivalent to covering all the extreme rays of the primal \(((D_{}))\) as shown by the next lemma. We defer the proof to the appendix.

**Lemma 2** (Necessary and Sufficient Condition for Teaching).: _A subset \(T\) is a valid teaching set if and only if it induces a representative vector on each extreme ray of the primal \(((D_{}))\)._

We denote the extreme ray set of primal \(((D_{}))\) by \(^{*}\). Note that demonstrating \(D_{}\) trivially induces \(^{*}\), however, doing so may not be optimal. Instead, as suggested by the above lemma, it is sufficient to find a minimal subset of states that covers all rays in \(^{*}\).

At a high level, our algorithm TIE 1 utilizes this insight to solve the optimal teaching problem in two stages. It first finds the extreme ray set \(^{*}\) of primal \(((D_{}))\). Next, it solves a set cover problem to find a minimal set of states that covers \(^{*}\).

Stage 1: Finding extreme rays of primal \(((D_{}))\):Given a set of vectors \(\), we propose an iterative algorithm to find all the extreme rays, i.e., a representative for each extreme rays of the primal \(()\). The algorithm solves a sequence of linear program \(LP(x,)\), where at each step it tests whether a candidate \(x\) is a unique representative of an extreme ray of \(()\). If not, it removes \(x\) from \(\) and moves to the next candidate as shown in **MinimalExtreme** procedure 1. Otherwise, it has to keep \(x\) to cover all extreme rays. The proof can be found in the appendix.

**Lemma 3** (Extreme Ray Test).: _Given a set of vectors \(^{d}\), a candidate \(x\) is a unique representative of an extreme ray of primal \(()\), i.e., \(x(\{x\})\) if and only if \(LP(x,)=-\), where,_

\[(x,):_{w} w,x  w,x^{} 1 x^{} \{x\}.\]

We remark that \(x\) is not a unique representative if and only if \(LP(x,)>0\) and in that case we can safely remove \(x\). Employing this test iteratively on each element of \(\) produces a unique representative for each extreme ray of primal \(()\). We apply this process to \(=(D_{})\) to obtain an extreme ray set \(^{*}(D_{})\) that contains exactly one representative for each extreme ray of \(((D_{}))\).

Stage 2: Finding minimal subset of states that cover the extreme rays \(^{*}\):Once we have the extreme ray set \(^{*}\), Lemma 2 requires a valid teaching set to cover all the rays in \(^{*}\). To do that optimally using the smallest dataset, the teacher has to solve the following set covering problem on extreme rays space:

\[_{T}|T|_{s T}V_{s}=U.\]

where universe \(U=^{*}\) and each state \(s\) covers a subset of extreme rays \(V_{s}^{*}\). Note that, unlike the infinite set cover problem over hypotheses space (3.1), this is a finite set cover problem over an extreme ray set. An optimal solution to this subproblem produces the optimal teaching set for teaching LVS learners which, by Lemma 1, is also an optimal teaching set for teaching the entire family of consistent linear BC learners.

### Theoretical Results

We provide a complete pseudocode of our teaching algorithm'TIE' in Algorithm 1. 'TIE' achieves the following guarantee on the Teaching Dimension.

**Theorem 4** (Optimal Teaching in Finite State Setting).: _Given an optimal teaching problem instance \((,,^{*})\) 2, our teaching algorithm TIE 1 correctly finds the optimal teaching set \(D^{*}\) and achieves the Teaching Dimension \(TD(^{*};)\)._Computational Complexity:We note that stage 1 of 'TIE' is efficiently solvable. However, stage 2 involves solving a finite set cover problem where each subset \(V_{s}\) can cover as many as \(||-1\) elements. Note that for \(||=2\), each subset is singular, and the set cover problem can be efficiently computed. Hence, 'TIE' efficiently computes the optimal teaching set for instances with \(|| 2\).

```
def MinimalExtreme(\(\)):
1:for each \(x_{j}\)do
2: Solve LP(\(x_{j},/\{x\}\)) defined by 3
3:if\(v_{j}>0\)then
4:\( x_{j}\)\(\) eliminate \(x_{j}\) if not necessary
5:return \(\)\(\) extreme vectors def OptimalTeach(\(,,^{*},\)):
6: let \((D_{})=\{_{s^{*}(s)b}^{d}:s,b ,b^{*}(s)\}\)\(\) compute feature differences
7:\(^{*}\)MinimalExtreme(\((D_{})\))
8:for\(s\)do
9:\(V_{s}\{^{*}:_{s^{*}(s)b}(D_{ }),_{s^{*}(s)b}=\}\)\(\) extreme rays covered by \(s\)
10:\(\{V_{s}:s T^{*}\}\) SetCover(\(^{*},\{V_{s}\}|_{s}\))\(\)\(T^{*}\) is smallest cover of all extreme rays
11: teach \(D^{*}=\{(t,^{*}(t)):t T^{*}\}\) to the agent\(\)\(D^{*}\) is the minimum demonstration set
```

**Algorithm 1** Teach using Iterative Elimination (TIE)

However, for \(||>2\), stage 2 requires solving a general set cover problem which is NP-hard to solve. We show that no teacher can avoid this hardness by giving a poly-time reduction from a finite set cover problem to our optimal teaching problem. We defer the proof to the appendix.

**Theorem 5** (Hardness of Optimal Teaching).: _Finding an optimal teaching set for teaching a linear version space BC learner is NP-hard in general for instances with action space size \(||>2\)._

Although the set cover subproblem is NP-hard to solve, we can obtain an approximate solution efficiently using a greedy covering strategy. Applying this approach to solve our set cover problem in line 5 of Algorithm 1 yields an efficient, approximately optimal algorithm, called '_Greedy-TIE_' with the following guarantee:

**Corollary 6** (Approximately Optimal Teaching).: _Our algorithm Greedy-TIE 1 efficiently teaches a family of consistent linear BC learners, \(\), and finds an approximately optimal teaching set \(\) such that \(||(||-1)|D^{*}|\)._

The \((||-1)\) approximation ratio of Greedy-TIE comes from the approximating set cover problem . Furthermore, '_Greedy-TIE_' runs in poly-time \(O((||||)^{3})\).

So far, we have assumed that the state space is finite. This assumption can be relaxed to infinite state setting under mild assumption as stated next. The proof can be found in the appendix.

**Corollary 7** (Optimal Teaching for Infinite State Setting).: _Consider our optimal teaching problem with infinite state space \(\). Under the assumption that \(((D_{}))\) is a closed and convex with finite extreme rays and the teacher knows the extreme rays to state mapping, our algorithm Greedy-TIE 1 correctly finds an approximately optimal teaching set._

In the general case of an infinite state space, the induced version space may contain an (uncountable) infinite number of extreme rays. Consequently, covering this infinite set of extreme rays with a finite teaching set becomes impossible, which renders optimal teaching impractical.

Now, we turn to the issue of distribution shift which has been a pertinent issue in behavior cloning in RL . For a BC learner imitating teacher's policy \(^{*}\) using a learnt policy \(\), the error amplification in value is given by \(|V_{}^{}-V_{}^{^{*}}|_{s  d^{^{*}}}[\|(s)-^{*}(s)\|_{1}].\)

However, in our teaching setting, this issue is resolved as the optimal teacher ensures the learner precisely learns \(^{*}\), leading to the following corollary.

**Corollary 8** (Optimal Value Guarantee).: _Under teaching by our algorithm TIE, the entire family of linear learners \(\) achieve a zero approximate value risk, i.e., \(,\;R(D^{*};)=0\)._Furthermore, it can be argued that BC learners are the most natural choice for a learner when a supportive teacher is available to demonstrate the target behavior. Unlike other learners like inverse RL [3; 8], BC operates directly in policy space, eliminating the need for planning. On the downside, since they maintain consistent hypotheses, they are limited to teaching only deterministic policies.

## 4 Experiments

We evaluate our teaching algorithm _Greedy-TIE_ on three environments: 1) _Pick the Right Diamond_, 2) _Visual Programming in Maze with Repeat Loops_ and 3) _Polygon Tower environment_ (provided in the appendix). Through these experiments, we aim to demonstrate the following: a) Our algorithm _Greedy-TIE_ finds an optimal or near-optimal teaching set in all these environments. b) The optimal teaching dataset so produced is competitive with a learner-specific optimal teaching set and can teach any consistent linear BC learners, and c) _Greedy-TIE_ performs significantly better than competitive baselines like _Teach-Random_ and _Teach-All_ that we define below.

Baselines:We consider two baselines. 1) _Teach-All_: This teacher simply teacher the target action in all states to the learner, 2) _Teach-Random_: This teacher draws states uniformly at random \(s U()\) and adds it to a collection until the collection becomes a valid teaching set, i.e., it induces the target cone \((D_{})\). We note that the teaching set produced by prior works [20; 23] are specialized to individual learners and do not yield a feasible set for teaching the entire family of consistent linear learners. Furthermore, their teacher directly constructs covariate vectors (features) in \(^{d}\) and is not able to choose individual states, thus, not directly applicable to our setting.

### Pick the Right Diamond

Recall the game from Example 1. A state in \(=\{,,,,o\}^{n}/\{o\}^{n}\) consists of a \(n\) dimensional board with one of four types of diamond or be empty(\(o\)). Each action in action space \(=[n]\) represents picking an object in one of the cells. The complete description of the MDP environment can be found in the appendix.

Feature representation & optimal policy:The learner uses a natural feature function in \(^{2}\) given as follows, \((s,a)=[a,\#a]\), where \([\#a]\) is 0 if the slot is empty. The optimal policy is to collect the diamonds in order of decreasing value i.e. from a large to a small number of edges. In the case of ties, the learner should choose the rightmost diamond. This policy is feasible under the above featurization, for example, \(w^{*}=\) uniquely induces \(^{*}\). For a board of size \(n=6\), there are a total of \(5^{6}-1\) states, and their feature difference vectors \((D_{})\) are shown as blue dots in Figure 3(a). The primal cone \(((D_{}))\) is the blue-shaded area. It contains two extreme rays, both need to be covered for successful teaching. The version space is denoted in green.

Optimal teaching set:We note that any set that covers the two extreme rays is a valid teaching set. On a board instance of size \(n=6\), our algorithm _Greedy-TIE_ produces a teaching set of size two as

Figure 4: Optimal teaching in “Pick the right diamond” with \(n=6\) slots. a) Feature difference vectors \((D_{})\) induced by target policy is shown as blue dots, primal cone \(((D_{}))\) as blue area, and dual version space \((D_{})\) as green area. b) A teaching set produced by _Greedy-TIE_ on board of size 6. c) Comparison of our _Greedy-TIE_ algorithm with other baselines.

illustrated in Figure 3(b). This is an instance optimal teaching set and shows a dramatic improvement over teaching all \(5^{6}-1\) states. We performed experiments on boards of different sizes and found that _Greedy-TIE_ significantly outperforms the other two baselines as shown in Figure 3(c).

### Visual Block Programming in Maze with Repeat Loop

We consider a real-world visual programming platform used for teaching kids/learners to write code to complete visual tasks in a maze environment [5; 9; 11; 15]. Further, we choose a domain that aims to teach learners to use repeat code blocks to write succinct code to complete a navigation-based task in maze environments of different sizes. The environment state consists of a \(n n\) maze with a turtle (shown in green in Figure 4(a)) facing one of four directions, a goal cell (shown by a red star), and a (partial) piece of code that can be executed to move the turtle in the maze. The learners' objective is to assemble code blocks in sequence to write a piece of code that can solve the given maze task.

The action space \(\) consists of \(n\) actions (each representing a basic code block) available to the learner to write code and is given as follows: _Turn-Left (TL)_: turns turtle to its left, _Turn-Right (TR)_: turns turtle to its right, _Move-Forward (MV)_: moves turtle forward by one cell, _Repeat-k-Times-Move (R\({}_{k}\)-MV)_ is a complex block with repeat loop that moves the turtle forward by \(k\) cells in a single command where \(k\{3,,n-1\}\) The task is to teach the agent to write most succinct piece of code that can be executed to make the turtle reach the goal cell. This is captured by a reward function that gives a reward of \(-1\) to the first three code blocks (_TL/TR/MV_) and a reward of \(-2\) to repeat blocks \(R_{k}\)-MV_. 1 The complete description of the MDP defining this problem can be found in the appendix.

Feature representation & optimal policy:We consider an execution-guided feature representation that takes an initial board with a partial piece of code and constructs a feature vector by first executing the partial code to get an intermediate state and extracting features from that state. We use a natural feature representation \(:^{d}\) that encodes the relative orientation and distance of the goal cell from the turtle cell; refer to the appendix for more details. The optimal policy is realizable by a linear policy under this representation. The teacher knows \(\) and can construct a dataset \(D\) of (state and optimal action) tuples and provide it to the learners. Its goal is to teach the target optimal policy of writing a succinct code to the entire family of learners \(\).

Optimal teaching set:We run our algorithm _Greedy-TIE_ on environments with different sizes of maze and observe that it is able to find an optimal teaching set for each of the environments; refer to Figure 6 for an example on \(5 5\) maze. This optimal teaching set demonstrates each action exactly once on a suitable maze state where that action is an optimal one. Our algorithm performs significantly better than the other two baselines: _Teach-Random_ and _Teach-All_ when run on a maze of

Figure 5: a) An example of a programming task in \(5 5\) with solution code. The maze contains a turtle facing one of four directions (shown by a green arrow) and a goal cell (shown by a red star). The optimal (smallest) solution code to lead the turtle to the goal is shown on the side. The action space consisting of 5 basic code blocks is shown on the right. b) Performance of _Greedy-TIE_ compared to baselines on this domain with different maze sizes.

different sizes as shown by Figure 4(b). We also trained other candidate consistent learners like linear SVM, linear perception, and linear logistic regression on teaching set obtained by _Greedy-TIE_ and verified that all of them achieve a risk of zero as claimed by our Theorem 4.

## 5 Related Work

Several prior works have studied optimal teaching of version space learners but mostly in finite or countable infinite version space settings[16; 17]. Some works like  have studied teaching multiple learners simultaneously but in an unsupervised learning setting of mean teaching. Instead, we study teaching a family of consistent behavior-cloning learners in a linear hypothesis space setting.

Comparatively, studies on optimal teaching of different linear learners are highly relevant to our work. For example, [20; 23] examined teaching linear learners like SVM, perceptron and logistic regression which can be seen as individual instances of consistent linear BC learners. These works focus on teaching individual learners, where teachers could exploit the strong biases of these learners to teach them relatively easily. On the other hand, we aim to teach the entire family of consistent linear BC learners where the teacher cannot base their teaching on the bias of individual learners. Additionally,  delved into the optimal teaching of iterative learners like gradient descent which is also biased . Further, [18; 26] have explored the teaching dimension of kernel learners for teaching a linear/non-linear boundary in \(^{d}\) space. Furthermore, these studies typically assume a more powerful teacher capable of constructing arbitrary covariate and label pairs, whereas our teacher is restricted to selecting states from a fixed state space and aims to teach the learner to generalize to other states using feature covariates induced by the feature function.

Another significant line of research involves teaching-by-demonstration in an RL setting. Relevant studies by [8; 10] have focused on teaching linear IRL learners[2; 22] which reward based imitation learners that learn primarily in reward space and require planning access to the environment to eventually learn an optimal policy. Unlike them, our linear BC learners learn directly in the policy space by only using teaching demonstrations and do not require access to the MDP environment.

## 6 Limitations & Future Work

We studied the optimal teaching of a family of linear BC learners and provided an efficient algorithm that achieves a \((||-1)\)-approximation guarantee on the teaching dimension. Our work focused mainly on teaching a deterministic policy to consistent linear family and we hope future works would extend this to more complex non-linear learners and stochastic policies.

We assumed the existence of a powerful teacher who could construct a dataset using any state in the state space. This may not always be possible in real-world when the states are complex and the teacher may actually have to efficiently navigate the MDP environment to lead the students to different states first and then teach them there. Another interesting future direction would be to study optimal teaching the family of linear learners under budget constraints on the teacher.