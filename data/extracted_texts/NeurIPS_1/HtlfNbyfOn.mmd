# _bit2bit_: 1-bit quanta video reconstruction by

self-supervised photon location prediction

Yehe Liu\({}^{1,2}\)   Alexander Krull\({}^{3,*}\)   Hector Basevi\({}^{3}\)   Ales Leonardis\({}^{3}\)   Michael Jenkins\({}^{1,2,*}\)

\({}^{1}\)Case Western Reserve University  \({}^{2}\)OpsiClear LLC  \({}^{3}\)University of Birmingham

{yehe, mwj5}@case.edu  {a.f.fr.krull, h.r.a.basevi, a.leonardis}@bham.ac.uk   Joint supervision\({}^{*}\)

###### Abstract

Quanta image sensors, such as single-photon avalanche diode (SPAD) arrays, are an emerging sensor technology, producing 1-bit arrays representing photon detection events over exposures as short as a few nanoseconds. In practice, raw data are post-processed using heavy spatiotemporal binning to create more useful and interpretable images at the cost of degrading spatiotemporal resolution. In this work, we propose _bit2bit_, a new method for reconstructing high-quality image stacks at the original spatiotemporal resolution from sparse binary quanta image data. Inspired by recent work on Poisson denoising, we developed an algorithm that creates a dense image sequence from sparse binary photon data by predicting the photon arrival location probability distribution. However, due to the binary nature of the data, we show that the assumption of a Poisson distribution is inadequate. Instead, we model the process with a Bernoulli lattice process from the truncated Poisson. This leads to the proposal of a novel self-supervised solution based on a masked loss function. We evaluate our method using both simulated and real data. On simulated data from a conventional video, we achieve 34.35 mean PSNR with extremely photon-sparse binary input (<0.06 photons per pixel per frame). We also present a novel dataset containing a wide range of real SPAD high-speed videos under various challenging imaging conditions. The scenes cover strong/weak ambient light, strong motion, ultra-fast events, etc., which will be made available to the community, on which we demonstrate the promise of our approach. Both reconstruction quality and throughput substantially surpass the state-of-the-art methods (e.g., Quanta Burst Photography (QBP)). Our approach significantly enhances the visualization and usability of the data, enabling the application of existing analysis techniques.

## 1 Introduction

Quanta image sensor (QIS) (e.g., SPAD arrays ) offer unique advantages over other image sensors for capturing fast temporal dynamics in low-light settings, as they can detect individual photons within nanoseconds exposures . This allows for the measurement of ultra-fast phenomena, such as fluorescence lifetime , time-of-flight , and other time-resolved processes . Quanta image data is different from traditional digital images. The QIS raw data often consists of sparse detection events described in 1-bit arrays (Fig. 1). In each frame, pixels only indicate the presence or absence of a photon without intensity information . This makes the data non-interpretable or usable as traditional images. A common approach to make effective use of the binary data is to bin the image in the time domain. While this can provide clear images (Fig. 1c), it comes at the cost of losing the valuable high-temporal-resolution information offered by QIS. In this paper, we present a novel self-supervised approach that directly denoises binary photon detection events from a QIS, reconstructing a clean video at the original temporal resolution.

Generative Accumulation of Photons (GAP) recently introduced an effective self-supervised method that denoises shot noise corrupted photon counting images by splitting images into paired data training pairs based on Poisson statistics . The self-supervised method is valuable to scientific imaging applications, where very often ground truth data is unavailable and cannot be simulated. However, this method is ineffective for binary images, where it creates significant artifacts due to the non-Poissonian nature of photon statistics in 1-bit data [10; 11]. This inspired us to develop _bit2bit_, a new method that specialized for binary data. Our key new insight is a masking strategy that hides the complementary dependency within the training pairs, which effectively addresses the problem. We further extend our method to 3D to leverage the information in both space and time, substantially improving the reconstruction quality. In addition to these measures, we also explored different architectures and sampling parameter spaces to reduce the significant issue of overfitting.

Problem statement and claims.The goal of this work is to predict the underlying spatiotemporal signal from a stack of quanta raw data. Each frame in the stack is a 1-bit 2D array representing locations where at least one photon is detected during the exposure period (Fig. 1a). Our method is based on the following 2 assumptions: 1) Each detection event is independent and follows truncated Poisson statistics. 2) The underlying signal exhibits some local and global spatiotemporal structures that can be described by a model.

Given this challenging reconstruction problem, we claim the following main contributions:

1. Developing a self-supervised method that denoises photon-sparse binary quanta images, which effectively handles the binary nature of the data through a novel masking strategy.
2. Enhancing the performance of reconstruction by leveraging the temporal information that is readily available in most quanta image data.
3. Providing insights on stochastic sampling strategies, network designs, and regularization techniques to manage overfitting and improve the reconstruction quality.
4. Presenting a novel dataset with real and simulated 1-bit SPAD data to support further research and quantitative evaluation of quanta image processing.

Figure 1: **Visualization of the reconstruction task.** a. A signal in spacetime generates discrete photons through a Poisson process. Real detectors can only count one photon at a time. The discrete nature of photons and the discrete counting process introduce shot noise, resulting in a sparse binary map. Our goal is to predict the underlying signal from this information-sparse data. b. Real SPAD raw data captured by a detector. The highlighted box indicates a zoomed-in region, revealing sparse photon detection events. To the right is a cross-section of the time-height dimensions, showing a similar binary noisy pattern. c. Our method produces the video from the data in b at the original spatiotemporal resolution (Video S1). d. Left: effect of accumulating raw data frames directly, showing shot noise and motion artifacts. Right: additional keyframe pairs are provided for reference.

## 2 Related Work

**Quanta image reconstruction.** A common quanta image reconstruction task is finding the optimal transformations of individual 1-bit frames to minimize motion blur prior to applying binning [8; 12; 13; 14]. Noise is another problem being addressed, especially in SPAD-based 3D reconstruction [15; 16; 17]. Some higher-level tasks address the motion deblur of the binned data using specially engineered deep learning methods because the mixing of shot noise renders traditional debluring methods ineffective . Besides direct image reconstruction, there are also efforts to perform traditional computer vision (CV) tasks directly to quanta image data (e.g., image classification) [19; 20; 21]. The task described in this work - reconstructing single binary frames at original temporal resolution and addressing both shot noise and motion blur - is challenging and has not been well explored. Only one previous work explored a similar task using the supervised method with ground truth data , in contrast to our self-supervised method.

Self-supervised denoising.Convolution neural networks (CNN) can be trained to denoise images by minimizing the difference (e.g., mean squared error (MSE)) between the prediction and a reference target [23; 24]. This is usually referred to as the minimal mean squared error (MMSE) denoising. Noise2Noise (N2N) demonstrated that it is possible to minimize the MSE loss between a pair of noisy images and still get quality denoising results without knowing the noise-free prior . However, N2N still requires real data pairs with the same underlying signal, which are often not obtainable. More recently, self-supervised denoising methods have been introduced to perform denoising from a single noisy image. The methods create input and target training pairs from the original noisy image by assuming the augmentations minimally affect the underlying signal. Noise2Void (N2V)(2) predicts the value of random pixels in a noisy image by hiding these pixels during training and only using the surrounding context to minimize loss only from the hidden pixels [26; 27]. Noise2Self (N2S) presented a similar concept that uses a moving grid mask . Noisier2Noise adds more noise to the input to alter the original noise . Neighbour2Neighbour, Noise2Fast, etc., assume pixel-wise independence of noise and create image pairs from neighboring pixels [30; 31].

However, the binary, sparse nature of quanta image creates a special, challenging case. A recent self-supervised denoising method, GAP , effectively addressed shot noise corruption in photon counting images by Poisson-statistic-based resampling. GAP introduced the idea of splitting the data by pixel-wise binomial sampling to create nearly unlimited training pairs, which inspired our work. However, implementing GAP directly on binary quanta images resulted in extremely poor

Figure 2: **Example Results from Our Method Using Real SPAD Data** The top row displays raw SPAD data. The middle row shows the corresponding reconstructions using our method. **CPU Fan + motion:** Imaged under camera motion. Additional paired raw data and reconstruction keyframes are shown below. **H&E slide:** Moving under a microscope. **Sonicating bubbles:** Humidifier generates bubbles, water droplets, and mist. **USAF 1951 + drill:** Resolution target spinning on a drill. **Plasma ball:** Firing plasma. A color-coded accumulation of 50 frames is shown on the right. [More in supp]reconstructions. This motivated us to identify the root cause of the problem and develop a specialized self-supervised method for sparse binary image reconstruction.

## 3 Theories and methods

Brief review of GAP.Here, we summarize the self-supervised denoising approach described in GAP . GAP is based on the assumption that observed image \(\) is a shot noise corrupted version of the clean image \(\), with each pixel \(x_{i}\) drawn from a Poisson distribution parameterized by the clean signal at the pixel \(s_{i}\), where \(i\) indicates the pixel index. GAP proposes to split the noisy image \(\) by sampling from a binomial distribution randomly assigning photons to an input image \(_{}^{k}\) and a target image \(_{}^{k}\). The images are conditionally independent given the underlying signal \(\), so can be treated as two independent observations, which can then be utilized in a N2N-style training procedure  to train a neural network \(f(;)\) with parameters \(\). By attempting to predict \(_{}\) from \(_{}\), the procedure trains a network to find the MMSE estimate for the clean signal. It uses a cross-entropy loss over pixels

\[L(f(_{}^{t};),_{}^{t})=_{i} ^{n}_{}^{t}_{}^{t}; )_{i})}{_{j}^{n}(f(_{}^{t};)_{j})}, \]

viewing the problem as a classification task, trying to predict the photon locations in the target image. Here, \(f(_{}^{t};)_{i}\) corresponds to the logit output for pixel \(i\). The authors show that in the limit, their loss is equivalent to the MSE loss, which is more frequently used in N2N training.

Photon counting.While the assumption of the Poisson shot noise model in GAP seems reasonable for photon counting, it often does not accurately hold for truncated QIS data such as SPAD. We can understand this by modeling photon counting as a Poisson point process and considering each counted photon as an element of a set defining this process. Subsequently, we can model the compound processes and the sub-processes within the framework of point process statistics and differentiate between photon counting and sector activation events (e.g., binary activations in each frame). This broader insight is discussed in the Appendix. This section focuses on the practical implementation of this insight in self-supervised denoising.

Quanta image generation.Instead of truly counting photons, SPAD (and other quanta imaging methods) acquire a series of binary images \(^{t}\), with each pixel \(x_{i}^{t}\{0,1\}\) indicating whether photons have been detected or not. Unfortunately, this means that the number of photons hitting the pixel is not recorded in a unit detection window, and we cannot know if an active pixel \(x_{i}^{t}=1\) receives single or multiple photons during one exposure. Formally, pixel values \(x_{i}^{t}\) are drawn from a Bernoulli distribution with parameter \(=1-e^{-s_{i}}\), with \(e^{-s_{i}}\) corresponding to the probability of zero photons hitting a pixel according to the underlying Poisson distribution (Appendix A.1). For small \(s_{i}\), individual pixels rarely receive multiple photons during an exposure. Thus, it is convenient to consider \(_{i}=1-e^{-s_{i}} s_{i}\).

In order to generate images that are visually pleasing and easier to analyze, the sparse binary data is usually processed by summing multiple frames \(^{t}\), effectively counting the number of detection

Figure 3: **Overview of the sampling/masking strategy.** The raw data is processed in 3D to use space and time similarly. Data pairs are created by random 3D crop from the raw data, then randomly split the positive values into an input or a target matrix. The split ratio is controlled by a parameter p. A mask is created by flipping the bits in the input image, which prevents gradient back-propagation from locations of 1s in the input. This process is repeated indefinitely, each time creating a new pair of data equivalent to independent observations from the underlying signal.

events for each pixel . Unfortunately, this aggregation over time leads to an inevitable loss of temporal resolution, as information about the time in which a photon hit is lost. It is important to note that the noise in such data is not Poisson distributed but instead follows a binomial distribution. Unfortunately, this means GAP, which relies on this assumption, cannot be directly applied to SPAD data. In the Appendix (Photon detection), we give an alternative theoretical perspective on this phenomenon by modeling photon counting as a Poisson point process with sub-processes to understand the difference between photon counting and the counting of photon detection events. While it is true that we can produce approximate Poisson statistics by summing a large number of frames, this comes at the cost of significantly reduced time resolution.

Splitting of 1-bit quanta data.This work explores an alternative approach, attempting to directly split the recorded binary images without temporal aggregation. We split the recorded binary image according to GAP , randomly assigning each photon detection event in \(^{t}\) to \(^{t}_{}\) or \(^{t}_{}\) with a probability \(p\) (Fig. 3). Assuming low light intensity \(s_{i}\) this means that the probability for finding a photon detection event at a pixel is \(_{i,} ps_{i}\) and \(_{i,}(1-p)s_{i}\). In order to enable the use of temporal information, we apply the same not to 2dimensional images but to 3D space-time-volumes \(X=(^{1}_{1},,^{T})\), splitting them into input \(X_{}=(^{1}_{},,^{T}_{})\) and target volumes \(X_{}=(^{1}_{},,^{T}_{})\).

However, unlike for true Poisson distributed photon counts, \(^{t}_{}\) and \(^{t}_{}\) cannot be considered conditionally independent. Since a photon detection event in \(x^{t}_{i}=1\) can only be assigned to either \(^{t}\) to \(^{t}_{}\) or \(^{t}_{}\), an event \(^{t}_{i,}=1\) in the input image must necessarily mean that corresponding pixel in the target image does not contain an event \(^{t}_{i,}=0\). As a consequence, naively using \(^{t}_{}\) and \(^{t}_{}\) as training pairs for a denoiser, will lead to the artifacts, with the network predicting dark values for any pixel which has a photon detection event in its input.

Masked loss function photon prediction for quanta images.To remedy this effect of correlated input and target images, we propose an adapted loss function. Considering that our data is sparse, that is, only very few pixels contain photon detection events, we avoid the problem by excluding

Figure 4: **Real data examples of photon splitting and the effect of the masked loss a. Example of splitting a randomly selected quanta image raw data frame. The Raw data consists of only binary pixels indicating the location of the photon counting event. The Split indicates the Input (black) and Target (white) of the split. The Mask is calculated by inverting the Input and is applied to the loss. b. Comparison of the training results with unmasked and masked loss. Without the masked loss, the network learns that whenever a pixel location has a photon in the input, it never has a photon in the target. The deterministic relationship leads to the artifacts. The pixel locations where the input is 1 appear dark in the network output. The masked loss effectively addresses the problem.**

pixels that contain a photon detection event in the input image using the loss

\[Lf(X_{};),X_{}=_{i,t}^{n.T}(1-x_{i, }^{t})x_{}^{t}};)_{i}^{t })}{_{j,}^{n.T}(f(X_{};)_{j}^{T})}, \]

with \(f(X_{};)\) referring to the logit output of the denoiser network and \(f(X_{};)_{i}^{t}\) referring to the value at the pixel/time index \(i,t\) of the output. Note that, while the masking is achieved by multiplying \((1-x_{i,}^{t})\) the remainder of the function corresponds exactly to the cross entropy loss over pixels from Eq. 1.

Implementation details.We view each binary quanta image dataset as a 3D array with two spatial dimensions and a temporal dimension. We randomly crop each sample \(\) from the volume (Fig. 2). \(_{}\) is sampled from a volume \(\) by Bernoulli sampling with dropout probability \(p\) defined based on desired strategy. We then calculate the target \(_{}\) by subtracting the input \(_{}\) from \(\). The mask \(_{}\) is computed from the input by \(_{}=1-\{_{},1\}\) at runtime. In the binary case, the mask is equivalent to the bitwise complement of the input -\(_{}\) (Fig. 3). We multiply the mask by the output of the network and the target before the cross entropy loss is calculated, which is equivalent to the loss function discussed in the previous section.

Selecting photon splitting variable p.The Bernoulli sampling probability \(p\) is an important new hyperparameter introduced in the sampling process. Its value and selection strategy can drastically impact the performance of production. We will demonstrate this in our experimental section. If we only work at a fixed input signal level (e.g., mean photon count), we can use a fixed \(p\) to generate training pairs. However, it is necessary to reduce the photon count to the same signal level for inference. A fixed large or small \(p\) can lead to over-fitting and slow training. In extreme cases, the model will recreate the target if \(p=1\), and the target is always empty if \(p=0\). A single input at the reduced signal level does not contain all the available information. We could resample multiple copies of the input and combine the inference results, but this strategy requires a longer inference time. Alternatively, we can randomly pick \(p\) from a desired range, which allows the model to adapt to different signal levels and produce acceptable results from all the inputs within the range. If we use a large \(p\) close to 1 as the upper limit (e.g., \(1-10^{-6}\)), we should be able to use raw data directly as the input, which is more efficient in practice.

## 4 Experiments

Network architecture.We used a 3D ResUNet  with an option to switch the network to 2D at a specific depth. The primary reason for this design choice is to handle the rapid increase of the receptive field as the depth increases, which eventually becomes bigger than the sampled area. The network will learn a lot from the padded areas during training. In the photon sparse binary case, this problem is more prominent, as the 0 padding is not different from the real data. To avoid significant weight from the padded area, a large crop window size is necessary to achieve better results. However, this can easily lead to memory issues. We can prevent the receptive field from growing in the temporal dimension by switching from 3D to 2D at the desired depth. This allows us to use a smaller window size in the temporal dimension and a larger window size in the spatial dimension. A detailed diagram of the network architecture is shown in Fig. S1. For most experiments, we set the network depth at 5, with only the first 2 levels being 3D to control the size of the receptive field. We used 32 initial input features, which provide a balance between performance and memory efficiency. The number of features doubles at each depth. Group normalization of 8 is applied at each convolution . We use GeLU for the activation function and pixel shuffling for up-sampling [34; 35].

Training.Models were trained using the ADAMW optimizer for 150 epochs, with 250 steps per epoch and 4 batches of random crops of 32x256x256 (TXY) per step . The large patch size is desired to prevent performance degradation due to substantial padding weights. However, 3D UNet with a large patch size is not memory-efficient, and data transfer is a bottleneck. Sometimes, only 2 batches of the crop size can be trained at a time while leaving a substantial memory unused with 24GB VRAM. Therefore, we implemented gradient check-pointing , which allowed us to double the batch size and utilize the majority of the VRAM. We primarily used Nvidia 3090/4090 for training, and each training takes about 6-10 hours until the loss curve stabilizes.

Inference.We run patch-based inference on a GPU using the original image width and height, maximizing the frame count for each inference. With 24GB VRAM, we can use a 48x512x512 volume for each inference with the typical 5-depth ResUNet mentioned above. The inference speed is above 3 volumes per second (150 fps) on a NVIDIA RTX 4090 GPU. If the training uses \(p_{} 1\), the raw data is used directly as input. Otherwise, we randomly reduce the photon count in the data by a factor of \(p_{}\) through photon splitting. For a 10-shot inference, we Bernoulli sample 10 times from the raw input at a fixed \(p\) value, run inference independently using the model trained at the \(p\) value, and calculate the mean of all the outputs.

Data.To evaluate our method quantitatively and qualitatively, we use a synthetic video with simulated noise consisting of 3990 frames and a total of 7 real SPAD videos, each containing 100k-130k frames. Additionally, we use a real video with 100k frames published in . Simulated data is essential for quantitative assessment as QIS ground truth is impractical to get. Assuming reference images/videos are ground truth with no shot noise and the pixel values \(n_{i}\) are proportional to the Poisson rate \(_{i}\), we calculate a variable \(q\) based on the selected mean counting rate \(=q(_{i=1}^{M}n_{i})\), where \(_{i=1}^{M}n_{i}\) is the mean of the reference. We then run pixel-wise Poisson sampling with \(_{i}=n_{i}p\) and clip between 0 and 1 to simulate the 1-bit quanta data. A ground truth reference video of a person shaking a ThorLab's box was acquired from iPhone 15 slow motion mode at 240 fps (Fig. S2). The simulation was using \(=0.0625\). After the clipping, the resulting simulation has 0.0590 photons per pixel. Real QIS data were acquired using a SPAD12S camera (Pi Imaging Technology, Switzerland). Each dataset has 100k-130k binary frames. We created scenes to represent a combination of different imaging conditions, including low-signal, high-signal, high-contrast, high-ambient-light, moving camera, moving object, linear movement, random movement, combined movement, ultra-fast events, and stochastic events (Fig.S6, S7, S8, S9, S10, S11, S12). We made the simulated data, real SPAD data, and reference results available to the community*.

Footnote *: [https://drive.google.com/drive/folders/1M5bsmsaLBkYmO7nMUjK5_m71RonOp-P9](https://drive.google.com/drive/folders/1M5bsmsaLBkYmO7nMUjK5_m71RonOp-P9)

Comparisons of different methods.We tested supervised training, N2N, N2V, and the original GAP with simulated data. To ensure fair comparisons, we incorporated the same network architectures, hyperparameters, and training steps into individual baselines. All implementations are in 3D except for GAP. Inference was conducted on the first 512 frames, and the data was normalized by the mean. PSNR and SSIM were calculated frame-wise, and the statistics were derived from the entire output stack. We tested N2N in three different ways: Using two independent simulated samples for training. Sampling two independent image stacks using the proposed splitting method (with \(p=0.5\), just once before training). Using the same samples but applying the masking strategy during training. We tested N2V using the original masking strategy. The N2V masks were created from 1000 random points in space. A median filter was applied to these points in the input. The minimum distance between the points was smaller than the median filter size, and loss was only computed from these points. We tested the original GAP in 2D with and without masking. Finally, we evaluated our method with and without masking.

## 5 Results and discussions

Findings from the comparisons.Tab. 1 summarizes key numerical and visual results. The supervised methods perform the best, which reflects the approximate upper limit of the network's performance. In reality, the ground truth is often not available, and high SNR data is only available from binning, which degrades time resolution and blends dynamics. N2N with 2 independent samples created a granular pattern on the images, as well as increasing validation loss since the beginning of the training (Fig. S16). Besides, two independent quanta data with the same underlying signal do not exist. One way the address this with N2N is to use adjacent frames, but this reduces the temporal resolution and introduces motion artifacts (e.g., Fig. S3: Interframe N2N). Without the mask, the image shows a strong pepper noise. After implementing the mask, it generates a smooth image, but the granular pattern persists. We also tested our method, which is similar to GAP in 3D. Masking addresses the image artifact problem. The 3D implementation led to substantially improved PSNR and image quality. N2V also produced poor reconstruction results. The result from the original 2D GAP produced also has the presence of pepper noise, similar to our implementation with large fixed p values. Implementing the mask in the GAP resolved the problem. The images are smooth but lack fine details. Our method also shows the pepper noise and the fine details are missing when the mask is not applied. Implementing the mask substantially improved the reconstruction quality.

Inference strategies.We achieved a PSNR of 33.93 using a network with \(p[0,1-10^{-6}]\) that takes the original raw data as input and performs inference in one shot. Further, training a network in a less extreme p range \([0.3,0.7]\) to avoid residual overfitting, then running inferences using 10 different samples sampled from the original data with \(p=7\), and finally combining the data by simple averaging, improved the PSNR to 34.35 (Table. S8). It is worth noting that it should be assumed that the result is specific to this simulated dataset and model configurations.

1-bit self-resampling.We noticed that different self-supervised resampling strategies become very similar across different methods in 1-bit. When we create a training pair and largely preserve the original information, we could either keep a positive in one image or move it to another image. Then, the sampling method essentially describes the spatial rule of splitting. For example, GAP uses binomial sampling to randomly drop out some photons. Self2Self  uses random binary masks to drop out some pixels, which is equivalent to GAP in 1-bit. N2S and N2V both select some pixels

  Method & Mask & 2D3D & PSNR & SSIM & Denoised patch \\  Raw & n/a & n/a & 4.81/0.55 & 0.017/0.006 & & \\ Supervised & N & 3D & 36.51/1.56 & 0.976/0.006 & & \\  N2N (2-sample) & N & 3D & 32.15/1.16 & 0.931/0.009 & & \\ N2N & N & 3D & 23.14/0.73 & 0.651/0.027 & & \\ N2N & Y & 3D & 28.83/0.58 & 0.843/0.010 & & \\ N2V & N & 3D & 26.16/0.97 & 0.804/0.028 & & \\ GAP & N & 2D & 20.54/0.64 & 0.588/0.059 & & \\ GAP & Y & 2D & 29.09/0.57 & 0.911/0.015 & & \\ bit2bit & N & 3D & 20.89/0.84 & 0.599/0.062 & & \\ bit2bit & Y & 3D & **33.93**/**0.99** & **0.959**/**0.007** & & \\  

Table 1: Results from comparison experiments.

in the input and use them as a target . The pixels are replaced by a new value using various strategies, such as median, random, and random dropout, which usually remove the selected positive pixel and very occasionally create new positive pixels. If a fixed grid is used, the operation becomes spatial-dependent. In the cases of neighboring pixel-based methods , image pairs are created by sub-sampling from a 2x2 grid, equivalent to a balanced spatial-dependent selection followed by downsampling.

Group normalization.We noticed that group normalization is _critical_ to for this task. Fig. 5a and Table. S2 shows that PSNR increased from 30.9 to 33.5 by using group normalization of 2. In creating the group number to 8 further increased the PSNR to 33.9. We could not implement batch normalization , as 24GB VRAM is only sufficient for 1 batch in some use cases (e.g., 512x512x32 window size).

Over-fitting, stochastic splitting and choices of p.Overfitting is often unavoidable in self-supervised denoising because the model can learn to fit the noise patterns specific to the finite training data. We noticed granular image artifacts in N2N, N2V, and GAP with large, fixed p-values, which resulted in substantially reduced reconstruction quality (fig. 3). Further analysis indicates that these three examples share a common issue: the input of the model is approximately a fixed binary pattern. In N2N, there is no variation between the input and target data. In N2V, we used a random grid to select the training target, but the grid size was relatively small, and most selected pixels were 0. The input data was very similar to the raw data, with a few photons removed, making the process similar to GAP with a large, fixed p-value.

To analyze the problem, we conducted an experiment using randomly Poisson noise with a small \(\) (Fig. S4). Ideally, the method should produce a uniform plane image with small pixel value variations. Our results show that if a new random input is used for every training step, the output of the network converges to a uniform image as expected, regardless of whether a fixed output is used. If we fix the input and randomize the output, the result is also uniform. However, if we fix both the input and output or if the output contains few photons (e.g., GAP with fixed large p), granular patterns similar to the N2N results start to appear, also reflected by substantially higher standard deviation. Increasing the photon counts in the input reduces shot noise and mitigates the problem. This suggests that diversity in the training pairs is crucial to mitigate overfitting in photon-sparse quanta image data.

Model size selection.Increasing model size can negatively impact the performance, potentially due to overfitting. We found a smaller 5-depth network with 24 startup filters outperformed a larger network with 48 filters (Fig. 5e), along with non-significant decaying from 24 to 40 filters. In another comparison, a network with depth 4 and 40 startup filters can achieve comparable performance (PSNR=33.83) as our best 5-depth network with 32 startup filters (33.93) (Table. S7). A 6-depth network (32.55) has a similar performance as a 3-depth network (32.42). The model size parameter space should be explored to balance overfitting and under-representation for different tasks.

Comparison to QBP.Quanta burst photography (QBP) is a recognized state-of-the-art quanta image reconstruction method based on spatial-temporal hierarchical alignment and frequency domain combination . We selected a challenging non-static scene from the original work for comparison, where a person is playing the guitar. The results are shown in Fig. 6 and Video S2. It is important to

Figure 5: **Results of ablation studies.** a. Group normalization substantially improved the PSNR. b. The choice of lower and c. upper bound of the thinning probability p affects the reconstruction quality. (.9x6:\(1-10^{6}\), etc.) d. Fixed large p led to performance degradation despite the proposed single photon prediction suggested in GAP. e. Large model size could negatively impact PSNR. Rome numbers indicate the corresponding images in Fig. S3. Numerical values in Table S2-6.

note that QBP reconstructs 1 image from 100-10,000 frames in relatively static scenes, each taking 30 minutes as reported (not optimized) . In contrast, our method can start to produce quality results in less than 1 hour on a 100k-frame dataset. Longer training further improves the results. Our method achieves over 150 fps inference on this dataset, equivalent to processing the entire 100k frames in less than 30 minutes with 50% patch overlap.

## 6 Conclusions

We demonstrated a versatile method for predicting the underlying signal of binary quanta image data, offering a practical solution for quanta image reconstruction where high SNR data and ground truth are unavailable. However, it is necessary to consider the limitations of the method when applying it to real data. Our method assumes a simple Poisson point process model, where the observation is sampled from a noise-free signal. Thus, it is not intended to restore the signal from other types of noise. From a different perspective, the method removes the Poisson noise component from the data, making it easier to identify and correct other noise components. For example, hot pixels on SPAD can be revealed and corrected with median filtering.

A fundamental limitation of self-supervised denoising is the lack of ground truth data, making it difficult to evaluate performance and accuracy in specific cases. In our case, performance could be evaluated by simulating the Poisson process, though these simulations should be based on reasonable assumptions and approximations. Additionally, the method requires offline training for each dataset. While there is potential for real-time reconstruction, the model should be trained on similar scenes.

Our approach can potentially be effective on any binary higher-dimensional spatial data created from Poisson point processes. For example, any data represented by an n-dimensional scatter plot, where points are independent measurements, can be analyzed with this method. Fig. S14 and S15 show the application and benchmark of the method to real and simulated 3D stacks of confocal microscope data with extremely high shot noise, which also demonstrates the performance advantage in 3D.

We also see the task could be generalized for all spatial event point processes as follows . For an n-dimensional signal in spacetime, a finite amount of measurements can be made from disjoint, uniform, and bounded sub-regions. Their collection can be viewed as a Poisson point process. The measurements are clipped between 0 and 1, and the process becomes a Bernoulli process. Given a finite number of such measurements, the task is to predict the distribution of the underlying signal measured by the Poisson rate. We envision that the core concept of creating data pairs by randomly splitting a point process and then hiding their complementary dependencies can potentially be used in other relevant regression models.

Finally, it is worth noting that the method can be misused in research if inappropriate data or statistical assumptions about the noise are made (e.g., the data points are not independent). In such cases, the prediction results can be misinterpreted, leading to incorrect scientific conclusions and potential negative societal impact.

Figure 6: **Comparison of our method to QBP with real SPAD data presented in the paper.** a) Different rendering of the data. The data is from the original QBP, indicating a dynamic scene with a person playing guitar. Our result is shown on the right. b) Height-time slicing of the raw data and our reconstruction. Top: raw data. Middle: our reconstruction, showing the top 3 strings. Bottom: the difference between adjacent frames, indicating sub-pixel movements of the string.