# Bridging Multicalibration and Out-of-distribution Generalization Beyond Covariate Shift

Jiayun Wu

Depart. of Computer Science & Tech.

Tsinghua University

Beijing, China 100084

wujy22@mails.tsinghua.edu.cn

This research was conducted as part of a visit to Carnegie Mellon University.

Jiashuo Liu

Depart. of Computer Science & Tech.

Tsinghua University

Beijing, China 100084

liujiashuo77@gmail.com

Peng Cui

Key Laboratory of Pervasive Computing, Ministry of Education

Depart. of Computer Science & Tech., Tsinghua University

Beijing, China 100084

cuip@tsinghua.edu.cn

Zhiwei Steven Wu

School of Computer Science

Carnegie Mellon University

Pittsburgh, PA 15213

zhiwei@cs.cmu.edu

###### Abstract

We establish a new model-agnostic optimization framework for out-of-distribution generalization via multicalibration, a criterion that ensures a predictor is calibrated across a family of overlapping groups. Multicalibration is shown to be associated with robustness of statistical inference under covariate shift. We further establish a link between multicalibration and robustness for prediction tasks both under and beyond covariate shift. We accomplish this by extending multicalibration to incorporate grouping functions that consider covariates and labels jointly. This leads to an equivalence of the extended multicalibration and invariance, an objective for robust learning in existence of concept shift. We show a linear structure of the grouping function class spanned by density ratios, resulting in a unifying framework for robust learning by designing specific grouping functions. We propose MC-Pseudolabel2, a post-processing algorithm to achieve both extended multicalibration and out-of-distribution generalization. The algorithm, with lightweight hyperparameters and optimization through a series of supervised regression steps, achieves superior performance on real-world datasets with distribution shift.

## 1 Introduction

We revisit the problem of out-of-distribution generalization and establish new connections with multicalibration , a criterion originating from algorithmic fairness. Multicalibration is a strengthening of calibration, which only requires a predictor \(f\) to be correct _on average_ within each level set:

\[[Y-f(X) f(X)]=0\]Calibration is a relatively weak property, as it can be satisfied even by the uninformative constant predictor \(f(X)=[Y]\) that predicts the average outcome. More broadly, calibration provides only a marginal guarantee that does not extend to sub-populations. Multicalibration  mitigates this issue by requiring the calibration to hold over a family of (overlapping) subgroups \(\): for all \(h\),

\[[(Y-f(X))\,h(X) f(X)]=0\]

Multicalibration is initially studied as measure of subgroup fairness for boolean grouping functions \(h\), with \(h(X)=1\) indicating \(X\) is a member of group \(h\). Subsequently, Gopalan et al.  and Kim et al.  adopt a broader class of real-valued grouping functions that can identify sub-populations through reweighting. The formulation of real-valued grouping function has enabled surprising connections between multicalibration and distribution shifts. Prior work  studied how distribution shift _affects_ the measure of multicalibration, with a focus on covariate shift where the relationship between \(X\) and \(Y\) remains fixed. Kim et al.  show that whenever the set of real-valued grouping functions \(\) includes the density ratio between the source and target distributions, a multicalibrated predictor with respect to the source remains calibrated in the shifted target distribution.

Our work substantially expands the connections between multicalibration and distribution shifts. At a high level, our results show that robust prediction under distribution shift can actually be _facilitated_ by multicalibration. We extend the notion of multicalibration by incorporating grouping functions that simultaneously consider both covariates \(X\) and outcomes \(Y\). This extension enables us to go beyond covariate shift and account for concept shift, which is prevalent in practice due to spurious correlation, missing variables, or confounding .

**Our contributions.** Based on the introduction of joint grouping functions, we establish new connections between our extended multicalibration notion and algorithmic robustness in the general setting of out-of-distribution generalization, where the target distribution to assess the model is different from the source distribution to learn the model.

1. We first revisit the setting of covariate shift and show multicalibration implies Bayes optimality under covariate shift, provided a sufficiently rich class of grouping functions. Then, in the setting of concept shifts, we show the equivalence of multicalibration and invariance , a learning objective to search for a Bayes optimal predictor \([Y|(X)]\) under a representation over features \((X)\), even though \([Y|X]\) is different across target distributions. We show correspondence between an invariant representation \((X)\) and a multicalibrated predictor \([Y|(X)]\), with a grouping function class containing all density ratios of target distributions and the source distribution.

2. As part of our structural analysis of the new multicalibration concept, we investigate the maximal grouping function class that allows for a nontrivial multicalibrated predictor. For traditional covariate-based grouping functions, the Bayes optimal predictor \(f(X)=[Y|X]\) is always multicalibrated, which is no longer the case for joint grouping functions. We show the maximal grouping function class is a linear space spanned by the density ratio of the target distributions where the predictor is invariant. As a structural characterization of distribution shift, this leads to an efficient parameterization of the grouping functions by linear combination of a spanning set of density ratios. The spanning set can be flexibly designed to incorporates implicit assumptions of various methodologies for robust learning, including multi-environment learning  and hard sample learning .

3. We devise a post-processing algorithm to multicalibrate predictors and simultaneously producing invariant predictors. As a multicalibration algorithm, we prove its convergence under Gaussian distributions of data and certify multicalibration upon convergence. As a robust learning algorithm, the procedure is plainly supervised regression with respect to models' hypothesis class and grouping function class, introducing an overhead of linear regression. This stands out from heavy optimization techniques for out-of-distribution generalization, such as bi-level optimization  and multi-objective learning , which typically involves high-order gradients . The algorithm introduces no extra hyperparameters. This simplifies model selection, which is a significant challenge for out-of-distribution generalization since validation is unavailable where the model is deployed . Under the standard model selection protocol of DomainBed , the algorithm achieves superior performance to existing methods in real-world datasets with concept shift, including porverty estimation , personal income prediction  and power consumption  prediction.

Multicalibration and Bayes Optimality under Covariate Shift

### Multicalibration with Joint Grouping Functions

We consider prediction tasks where covariates are denoted by a random vector \(X\) and the target by \(Y\). Lowercase \(x,y\) denote the specific values of these random variables. Predictors are defined as real-valued functions \(f:\). Our theoretical analysis focuses on the setting where \(=\). In this context, we propose a new definition of \(_{2}\) approximate multicalibration with joint grouping functions.

**Definition 2.1** (Multicalibration with Joint Grouping Functions).: _For a probability measure \(P(X,Y)\) and a predictor \(f\), let \(^{}\) be a real-valued grouping function class. We say that \(f\) is \(\)-approximately \(_{2}\) multicalibrated w.r.t. \(\) and \(P\) if for all \(h\):_

\[K_{2}(f,h,P)=_{P}[h(X,Y)(Y-v)f(X)=v] ^{2}dP_{f(X)}(v). \]

\(P_{f(X)}(v)=P(f^{-1}(v))\) _is the pushforward measure. We say \(f\) is \(\)-approximately calibrated if \(\) includes the constant function \(h 1\). We say \(f\) is multicalibrated (calibrated) for \(=0\). If the grouping function is defined on \(X\), which implies \(h(x,y_{1})=h(x,y_{2})\) for any \(x\) and \(y_{1},y_{2}\), we abbreviate \(h(x,)\) by \(h(x)\)._

Our definition generalizes several notions of (multi)calibration in the literature by specific choices of grouping functions. For example, \(K_{2}(f,1,P)\) recovers the overall calibration error in the case of a constant grouping function \(h 1\). For boolean grouping functions defined on \(X\), \(K_{2}(f,h,P)\) computes the calibration error of the subgroup with \(h(x)=1\). For real-valued grouping functions defined on \(X\)[14; 20], \(K_{2}(f,h,P)\) evaluates a reweighted calibration error, whose weights \(h(x)\) are proportional to the likelihood of a sample belonging to the subgroup. Furthermore, we propose an extended domain of grouping functions defined on covariates and outcomes jointly, for which the Bayes optimal predictor \([Y|x]\) may not be multicalibrated, in contrast to all existing multicalibration frameworks with \(X\)-based grouping functions. Multicalibration with joint groupings thus implies a distinct learning objective from accuracy, which we will characterize as _invariance_ in section 3.

**Example 2.2** (Multicalibration Does Not Imply Bayes Optimality).: _Consider covariates \(X=(X_{1},X_{2})^{T}\) and an outcome \(Y\) generated by the following structural equations:_

\[Y =X_{1}+_{1}.\] \[X_{2} =Y+_{2}.\]

\(X_{1}\)_, \(_{1}\), \(_{2}\) are independent gaussian variables with zero mean, and variances \([_{1}^{2}]=_{1}^{2}\) and \([_{2}^{2}]=_{2}^{2}\). For a singleton grouping function class containing \(h(x,y)=y-x_{2}\), the Bayes optimal predictor \(f(x)=[Y|x]=^{2}}{_{1}^{2}+_{2}^{2}}x_{1} +^{2}}{_{1}^{2}+_{2}^{2}}x_{2}\) is not multicalibrated because \([h(X,Y)(Y\ -\ f(X))]=^{2}_{2}^{2}}{_{1}^{2}+ _{2}^{2}} 0\). However, \(g(x)=x_{1}\) is multicalibrated since \([h(X,Y)(Y-g(X))|g(X)]=-[_{2}_{1}[X_{1}]=0\)._

While the Bayes optimal predictor is always multicalibrated for covariate-based grouping functions, it may not be multicalibrated for grouping functions that depend on the outcome. In fact, there may be no predictor that achieves multicalibration in such cases. For example, when we consider a grouping function class that includes both \(h 1\) and \(h(x,y)=y\), a multicalibrated predictor \(f\) satisfies \(f(X)=[Y|f(X)]\) for \(h 1\), and \([Y^{2}|f(X)]=[Yf(X)|f(X)]=([Y|f(X)])^{2}\) for \(h(x,y)=y\). This implies \([Y|f(X)]=0\), which is impossible for regression with label noise. Therefore, we study the structure of the _maximal grouping function class_ that allows for a multicalibrated predictor in section 4.

Most importantly, multicalibration with joint grouping functions is useful for capturing more general distribution shifts. By interpreting multicalibration error as calibration error reweighted by grouping functions, it quantifies the maximal calibration error for all subgroups associated with grouping functions in \(\). If grouping functions are defined on \(X\), only the covariate distribution \(P(X)\) distinguishes between subgroups. In contrast, the joint distribution \(P(X,Y)\) differentiates subgroups for joint grouping functions. We will discuss multicalibration with covariate-based grouping functions in the next sub-section and joint grouping functions in section 3.

### Multicalibration Implies Bayes Optimality under Covariate Shift

**Settings of Out-of-distribution Generalization.** We characterize distribution shift by an _uncertainty set_ of absolutely continuous probability measures, denoted by \((X,Y)\), where there is an accessible source measure \(P_{S}\) and unknown target measure \(P_{T}\). We use _capital_ letters such as \(P\) to denote a single probability measure and _lowercase_ letters such as \(p\) to denote its probability density function. A predictor \(f\) is learned in the source distribution \(P_{S}\) and assessed in the target distribution \(P_{T}\). Given a loss function \(:\), we evaluate the average risk of a predictor \(f\) w.r.t. a probability measure \(P\), defined by \(R_{P}(f):=_{P}[(f(X),Y)]\). We focus on \((,y)=(-y)^{2}\) in our theoretical analyses.

In this subsection we focus on grouping functions \(h(x)\) defined on covariates. We will prove approximately multicalibrated predictors simultaneous approaches Bayes optimality in each target distribution with covariate shift, bridging the results of Kim et al.  and Globus-Harris et al. . To recap, Kim et al.  studies multicalibration under covariate shift and shows that a multicalibrated predictor remains calibrated in target distribution for a sufficiently large grouping function class. Further, it is shown that multicalibration predictors remain multicalibrated under covariate shift , assuming the grouping function class \(\) is closed under some transformation by density ratios (Assumption 2.3.1). Second, Globus-Harris et al.  shows multicalibration implies Bayes optimal accuracy , assuming \(\) satisfies a weak learning condition (Assumption 2.3.2). Detailed discussion on other related works is deferred to section A in the appendix.

**Assumption 2.3** (Sufficiency of Grouping Function Class (informal, see Assumption F.1)).:

_1. (Closure under Covariate Shift) For a set of probability measures \((X)\) containing the source measure \(P_{S}(X)\), \(h\) implies \(p/p_{S} h\) for any density function \(p\) of distributions in \(\)._

_2. (\((,)\)-Weak Learning Condition) For any \(P(X)P_{S}(Y|X)\{P^{}(X)P_{S}(Y X):P^{} \}\) with the source measure \(P_{S}(Y|X)\), and every subset \(G\) with \(P(X G)>\), if the Bayes optimal predictor \(_{P}[Y|X]\) has lower risk than the constant predictor \(_{P}[Y|X G]\) by a margin \(\), there exists a predictor \(h\) that is also better than the constant predictor with the margin \(\)._

**Theorem 2.4** (Risk Bound under Covariate Shift).: _For a source measure \(P_{S}(X,Y)\) and a set of probability measures \((X)\) containing \(P_{S}(X)\), given a predictor \(f:\) with finite range \(m:=|(f)|\), consider a grouping function class \(\) closed under affine transformation and satisfying Assumption 2.3 with \(=/m\). If \(f\) is \(}{256m^{2}}\)-approximately \(_{2}\) multicalibrated w.r.t \(P_{S}\) and \(_{1}:=\{h:_{x}h(x)^{2} 1\}\), then for any target measure \(P_{T}(X)P_{S}(Y|X)\),_

\[R_{P_{T}}(f)_{f^{*}:}R_{P_{T}}(f^{*})+3. \]

**Remark 2.5**.: _Following prior work in multicalibration , we study functions \(f\) with finite cardinality, which can be obtained by discretization._

## 3 Multicalibration and Invariance under Concept Shift

Theorem 2.4 shows multicalibration implies Bayes optimal accuracy for target distributions under covariate shift. However, in practical scenarios, there are both marginal distribution shifts of covariates (\(X\)) and _concept shift_ of the conditional distributions (\(Y|X\)). Concept shift is especially prevalent in tabular data due to missing variables and confounding . In order to go beyond covariate shift, we will focus on grouping functions defined on covariates and outcomes jointly. We show that multicalibration notion w.r.t. joint grouping functions is equivalent to invariance, a criterion for robust prediction under concept shift. Extending the robustness of multicalibration to general shift is non-trivial. The fundamental challenge is that there is no shared predictor that is generally optimal in each target distribution because the Bayes optimal predictor varies for different \(Y|X\) distributions. As a first step, we show multicalibrated predictors w.r.t. joint grouping functions are robust as they are optimal over any post-processing functions in each target distribution.

**Theorem 3.1** (Risk Bound under Concept Shift).: _For a set of absolutely continuous probability measures \((X,Y)\) containing the source measure \(P_{S}(X,Y)\), consider a predictor \(f:\). Assume the grouping function class \(\) satisfies the following condition:_

\[\{h(x,y)=(x,y)}P(X,Y)\}. \]_If \(f\) is \(\)-approximately \(_{2}\) multiclibrated w.r.t. \(\) and \(P_{S}\), then for any measure \(P(X,Y)\),_

\[R_{P}(f)_{g:}R_{P}(g f)+2. \]

The theorem shows an _approximately multiclibrated_ predictor on the source _almost cannot be improved by post-processing_ for each target distribution. To ensure such robustness, the grouping function class must include all density ratios between target and source measures, which are functions over \(\). This characterization of robustness in terms of post-processing echoes with Invariant Risk Minimization (IRM) , a paradigm for out-of-distribution generalization with \(Y|X\) shift. However, their analysis focuses on representation learning.

**Definition 3.2** (Invariant Predictor).: _Consider data selected from multiple environments in the set \(\) where the probability measure in an environment \(e\) is denoted by \(P_{e}(X,Y)\). Denote the representation over covariates by a measurable function \((x)\). We say that \(\) elicits an \(\)-approximately invariant predictor \(g^{*}\) across \(\) if there exists a function \(g^{*}:=\{g:supp()\}\) such that for all \(e\):_

\[R_{P_{e}}(g^{*})_{g}R_{P_{e}}(g)+. \]

**Remark 3.3**.: _(1) Predictors in \(\) take a representation \(\) extracted from the covariates as input. For a general predictor \(f(x)\), if we take \((x)=f(x)\) and \(g^{*}\) as an identity function, Equation 5 reduces to the form of Equation 4. Therefore, \(f\) in Equation 4 is a \(2\)-approximately invariant predictor across environments collected from the uncertainty set \(\). (2) We give an approximate definition of invariant predictors, which recovers the original definition  when \(=0\). In this case, there exists a shared Bayes optimal predictor \(g^{*}\) across environments, taking \(\) as input. This implies \(_{e_{1}}[Y|]=_{e_{2}}[Y|]\) almost surely for any \(e_{1},e_{2}\)._

IRM searches for a representation such that the optimal predictors upon the representation are _invariant_ across environments. Motivated from causality, the interaction between outcomes and their causes are also assumed invariant, so IRM learns a representation of causal variables for stable prediction. We extend Theorem 3.1 to representation learning and prove equivalence between multiclibrated and invariant predictors.

**Theorem 3.4** (Equivalence of Multiclibration and Invariance).: _Assume samples are drawn from an environment \(e\) with a prior \(P_{S}(e)\) such that \(_{e}P_{S}(e)=1\) and \(P_{S}(e)>0\). The overall population satisfies \(P_{S}(X,Y)=_{e}P_{e}(X,Y)P_{S}(e)\) where \(P_{e}(X,Y)\) is the environment-specific absolutely continuous measure. With a measurable function \((x)\), define a function class \(\) as:_

\[:=\{h(x,y)=(x,y)}{p_{S}(x,y)}e\}. \]

_1. If there is a bijection \(g^{*}:supp()\) such that \(g^{*}\) is \(\)-approximately \(_{2}\) multiclibrated w.r.t. \(\) and \(P_{S}\), then \(\) elicits an \(2\)-approximately invariant predictor \(g^{*}\) across \(\)._

_2. If there is \(g^{*}:supp()\) such that \(\) elicits an \(\)-approximately invariant predictor \(g^{*}\) across \(\), then \(g^{*}\) is \(\)-approximately \(_{2}\) multiclibrated w.r.t. \(\) and \(P_{S}\), where \(D=_{e}P_{S}(e)\)._

**Remark 3.5**.: _(1) In the first statement, assuming \(g^{*}\) is a bijection avoids degenerate cases where \(\) contains redundant information. For example, every predictor \(g^{*}(X)\) upon representation \(\) equals \(g^{*}()(X)\) upon representation \(X\). Confining \(g^{*}\) to bijections ensures some unique decomposition into predictors and representations. (2) Wald et al.  proves equivalence between exact invariance and simultaneous calibration in each environment. We strengthen their result to show multiclibration on a single source distribution suffices for invariance. Moreover, our results can be directly extended beyond their multi-environment setting to a general uncertainty set of target distributions, by the mapping between grouping functions and density ratios. Further, our theorem is established for both exact and approximate invariance._

The theorem bridges _approximate multiclibration_ with _approximate invariance_ for out-of-distribution generalization beyond covariate shift. The equivalence property indicates that the density ratios of target and source distributions constitute the _minimal grouping function class_ required for robust prediction in terms of invariance.

Structure of Grouping Function Classes

Section 3 inspires one to construct richer grouping function classes for stronger generalizability. However, fewer predictors are multicalibrated to a rich function class, and a multicalibrated predictor may not exist at all, as illustrated by the example in section 2.1. In this section, we first study the _maximal grouping function class_ that is feasible for a multicalibrated predictor. Then, we will leverage our structural results to inform the design of grouping functions.

### Maximal Grouping Function Space

We focus on continuous grouping functions defined on a compact set \(^{d+1}\), i.e., \(h C()\), and consider absolutely continuous probability measures supported on \(\) with continuous density functions. Our first proposition shows that the maximal grouping function class for any predictor is a linear space.

**Proposition 4.1** (Maximal Grouping Function Class).: _Given an absolutely continuous probability measure \(P_{S}(X,Y)\) and a predictor \(f:\), define the maximal grouping function class that \(f\) is multicalibrated with respect to:_

\[_{f}:=\{h C():K_{2}(f,h,P_{S})=0\}. \]

_Then \(_{f}\) is a linear space._

In the following, we further analyze the spanning set of maximal grouping function classes for nontrivial predictors which are at least calibrated.

**Theorem 4.2** (Spanning Set).: _Consider an absolutely continuous probability measure \(P_{S}(X,Y)\) and a calibrated predictor \(f:\). Then its maximal grouping function class \(_{f}\) is given by:_

\[_{f}=\{(x,y)}:pR_{P}(f)=_{g:}R_{P}(g f)\}. \]

A predictor's maximal grouping function class is spanned by density ratios of target distributions where the predictor is invariant. Correspondingly, Theorem 3.1 gives the minimal grouping function class, comprised of density ratios between target and source distributions, in order to ensure \(f(x)\) is an invariant predictor. In contrast, Theorem 4.2 states the maximal grouping function class for \(f(x)\) is exactly the linear space spanned by those density ratios. Next, we further investigate sub-structures of the maximal grouping function class. We focus on the representation learning setting of IRM.

**Theorem 4.3** (Decomposition of Grouping Function Space).: _Consider an absolutely continuous probability measure \(P_{S}(X,Y)\) and a measurable function \(:^{d}^{d_{}}\) with \(d_{} Z^{+}\). We define the Bayes optimal predictor over \(\) as \(f_{}(x)=_{P_{S}}[Y|(x)]\). We abbreviate \(_{f_{}}\) with \(_{}\). Then \(_{}\) can be decomposed as a Minkowski sum of \(_{1,}+_{2,}\)._

\[_{1,}=\{(,y)}:p R_{P}(f_{})=_{g:}R_{P}(g f_{})\}. \]

\[_{2,}=\{(x|,y)}: \}. \]

_1. If a predictor \(f\) is multicalibrated with \(_{1,}\), then \(R_{P_{S}}(f) R_{P_{S}}(f_{})\)._

_2. \(f_{}\) is an invariant predictor elicited by \(\) across a set of environments \(\) where \(P_{e}(,Y)=P_{S}(,Y)\) for any \(e\). If a predictor \(f\) is multicalibrated with \(_{2,}\), then \(f\) is also an invariant predictor across \(\) elicited by some representation._

**Remark 4.4**.: \(_{1,}\) _and \(_{2,}\) contain functions defined on \(x,(x),y\) which can both be rewritten as functions on \(x,y\) by variable substitution. Thus, \(_{1,}\), \(_{2,}\) are still subspaces of grouping functions. \(_{1,}\) is spanned by the density ratio of \(P(,Y)\) where the Bayes optimal predictor over \(\) must be invariant on the distribution of \(P\). \(_{2,}\) is spanned by general density ratio of \(P(X|,Y)\)._

Multicalibration w.r.t. \(_{1,}\) ensures at least the _accuracy_ of the Bayes optimal predictor on \(\), and multicalibration w.r.t. \(_{2,}\) ensures at least the _invariance_ of this predictor. However, we show in the following proposition that sizes of two subspaces are negatively correlated. When \(\) is a variable selector, \(_{1,}\) expands with more selected covariates while \(_{2,}\) shrinks. By choosing a combination of \(_{1,}\) and \(_{2,}\), we strike a balance between accuracy and invariance of the multicalibrated predictor.

**Proposition 4.5** (Monotonicity).: _Consider \(X^{d}\) which could be sliced as \(X=(,)^{T}\) and \(=(,)^{T}\). Define \(^{}_{1,}:=\{h((x)) C()\}\), with \(^{}_{1,}_{1,}\). \(^{}_{1,X}\) and \(^{}_{1,}\) are similarly defined. We have:_

1. \(^{}_{1,X}^{}_{1,}^{}_{1,}^{}_{1,}=\{C\}\)_._
2. \(\{C\}=_{2,X} H_{2,}_{2,} _{2,}\)_._

\(C\) _is a constant value function._

### Design of Grouping Function Classes

The objective of a robust learning method can be represented by a tuple consisting of an assumption about the boundary of distribution shift and a metric of robustness. Multicalibration is equivalent to invariance as a metric of robustness, while the grouping function class provides a unifying view for assumptions over potential distribution shift. Given any uncertainty set of target distributions \(\), Theorem 4.2 implies an efficient and reasonable construction of grouping functions as linear combinations of density ratios from \(\). We implement two designs of grouping functions for the learning setting with and without environment annotations respectively.

From EnvironmentsIf samples are drawn from multiple environments and the environment annotations are available, we assume the uncertainty set as the union of each environment's distribution \(P_{e}\). This completely recovers IRM's objective, but we approach it with a different optimization technique in the next section. Taking pooled data as the source \(S\), density ratios spanning the grouping function class are \(h_{e}(x,y)=p_{e}(x,y)/p_{S}(x,y)=p_{S}(e|x,y)/p_{S}(e)\), where \(p_{S}(e|x,y)\) is estimated by an environment classifier. Then a grouping function can be represented as a linear combination of \(h_{e}\):

\[h(x,y)=_{e}_{e}p_{S}(e|x,y),\ \ _{e} . \]

From Hard SamplesWhen data contains latent sub-populations without annotations, the uncertainty set can be constructed by identifying sub-populations. Hard sample learning [27; 28; 29] suggests the risk is an indicator for sub-population structures. Samples from the minority sub-population \(M\) are more likely to have high risks. For example, JTT  identified the minority subgroup using a risk threshold of a trained predictor \(f_{id}\). We adopt a continuous grouping by assuming \(P_{S}(X,Y M)(f_{id}(X)-Y)^{2}\). We construct the uncertainty set as the union of the source \(S\) and minority sub-population \(M\), resulting in a grouping function represented as:

\[h(x,y)=_{M}(f_{id}(x)-y)^{2}+_{S},\ \ _{M},_{S} . \]

Another design utilizing Distributionally Robust Optimization's assumption  is in section B.

## 5 MC-PseudoLabel: An Algorithm for Extended Multicalibration

In this section, we introduce an algorithm for multicalibration with respect to joint grouping functions. Simultaneously, the algorithm also provides a new optimization paradigm for invariant prediction under distribution shift. The algorithm, called MC-PseudoLabel, post-processes a trained model by supervised learning with pseudolabels generated by grouping functions. As shown in Algorithm 1, given a predictor function class \(\) and a dataset \(D\) with an empirical distribution \(_{D}(X,Y)\), a regression oracle \(A\) solves the optimization: \(A_{}(D)=_{f}R_{_{D}}(f)\). We take as input a model \(f_{0}\), possibly trained by Empirical Risk Minimization. \(f_{0}\) has a finite range following conventions of prior work in multicalibration . For continuous predictors, we discretize the model output and introduce a small rounding error (see section C). For each iteration, the algorithm performs regression with grouping functions on each level set of the model. The prediction of grouping functions rectify the uncalibrated model and serves as pseudolabels for model updates.

Since we regress \(Y\) with grouping functions defined on \(Y\), a poor design of groupings violating Theorem 4.2 can produce trivial outputs. For example, if grouping functions contain \(h(x,y)=y\), then \(err_{t-1}-er_{t}\) never decreases and the algorithm outputs \(f_{0}\), because there does not exist a multicalibrated predictor. However, the algorithm certifies a multicalibrated output if it converges.

**Theorem 5.1** (Certified Multicalibration).: _In Algorithm 1, for \(,B>0\), if \(err_{t-1}-er_{t}\), the output \(f^{}_{t-1}(x)\) is \(\)-approximately \(_{2}\) multicalibrated w.r.t. \(_{B}=\{h: h(x,y)^{2} B\}\)._MC-PseudoLabel reduces to LSBoost Globus-Harris et al. , a boosting algorithm for multicalibration if \(\) only contains covariate-based grouping functions. In this case, Line 14 of Algorithm 1 reduces to \(f_{t+1}(x)=_{t+1}(x,)\) where \(_{t+1}\) does not depend on \(y\). For joint grouping functions, since \(_{t+1}^{}\), we project it to models' space of \(^{}\) by learning the model with \(_{t+1}\) as pseudolabels. The projection substantially changes the optimization dynamics. LSBoost constantly decreases risks of models, due to \(R_{_{D}}(f_{t+1})=R_{_{D}}(_{t+1})<R_{_{D} }(f_{t})\). The projection step disrupts the monotonicity of risks, implying that MC-Pseudolabel can output a predictor with a higher risk than input. This is because multicalibration with joint grouping functions implies balance between accuracy and invariance, as is discussed in Theorem 4.3. The convergence of LSBoost relies on the monotonicity of risks, which is not applicable to MC-Pseudolabel. We study the algorithm's convergence in the context of representation learning. Assume we are given a grouping function class \(_{}\) with a latent representation \(\). If a predictor is multicalibrated w.r.t \(_{1,},_{2,}\) respectively, then it is also multicalibrated w.r.t. \(_{}\). Therefore, we separately study the convergence with two grouping function classes. In Proposition F.26, we show the convergence for a subset of \(_{1,}\) consisting of covariate-based grouping functions, which is a corollary of Globus-Harris et al.'s result. As a greater challenge, we derive convergence for \(_{2,}\) when data follows multivariate normal distributions.

**Theorem 5.2** (Covergence for \(_{2,}\) (informal, see Theorem F.27)).: _Consider \(X^{d}\) with \(X=(,)^{T}\). Assume that \((,,Y)\) follows a multivariate normal distribution \(_{d+1}(,)\) where the random variables are in general position such that \(\) is positive definite. For any distribution \(D\) supported on \(\), take the predictor class \(=^{}\) and the grouping function class \(\) as a subset of \(_{2,}\) which is defined in Equation 10:_

\[=\{h:h_{2,}h(x,y)=c_{x}^{T}x+c_{y}y+ c_{b},c_{x}^{d},c_{y},c_{b}\}. \]

_For an initial predictor \(f_{0}(x)=[Y|x]\), run MC-Pseudolabel\((D,,)\) without rounding, then \(f_{t}(x)\) converges pointwise to \([Y|(x)]\) as \(t\), with a convergence rate of \((M()^{t})\) where \(0 M()<1\)._

MC-Pseudolabel is also an optimization paradigm for invariance. Certified multicalibration in Theorem 5.1 also implies certified invariance. Furthermore, MC-Pseudolabel introduces no extra hyperparameters to tradeoff between risks and robustness. Both certified invariance and light-weighted hyperparameters simplify model selection, which is challenging for out-distribution generalization because of unavailable validation data from target distributions . MC-Pseudolabel has light-weighted optimization consisting of a series of supervised regression. It introduces an overhead to Empirical Risk Minimization by performing regression on level sets. However, the extra burden is linear regression by designing the grouping function class as linear space. Furthermore, regression on different level sets can be parallelized. Computational complexity is further analyzed in section D.

Experiments

### Settings

We benchmark MC-Pseudolabel on real-world _regression_ datasets with distributional shift. We adopt two experimental settings. For the _multi-environment_ setting, algorithms are provided with training data collected from multiple annotated environments. Thereafter, the trained model is assessed on new environments. For the _single-environment_ setting, algorithms are trained on a single source distribution. There could be latent sub-populations in training data, but environment annotations are unavailable. The trained model is assessed on a target dataset with distribution shift from the source. The grouping function class is implemented according to Equation 11 and Equation 12 for the multi-environment and single-environment setting respectively.

DatasetsWe experiment on PovertyMap  and ACSIncome  for the multi-environment setting, and VesselPower  for the single-environment setting. As the only regression task in WILDS , a popular benchmark for in-the-wild distribution shift, PovertyMap performs poverty index estimation for different spatial regions by satellite images. Data are collected from both urban and rural regions, by which the environment is annotated. The test dataset also covers both environments, but is collected from different countries. The primary metric is _Worst-U/R Pearson_, the worst Pearson correlation of prediction between rural and urban regions. The other two datasets are tabular, where natural concept shift (\(Y|X\) shift) is more common due to existence of missing variables and hidden confounders . ACSIncome  performs personal income prediction with data collected from US Census sources across different US states. The task is converted to binary classification by an income threshold, but we take raw data for regression. Environments are partitioned by different occupations with similar average income. VesselPower comes from Shifts , a benchmark focusing on regression tasks with real-world distributional shift. The objective is to predict power consumption of a merchant vessel given navigation and weather data. Data are sampled under different time and wind speeds, causing distribution shift between training and test data.

BaselinesFor the multi-environment setting, baselines include ERM (Empirical Risk Minimization); methods for invariance learning which mostly adopts multi-objective optimization: IRM , MIP , IB-IRM , CLOvE , MRI , REX , Fishr ; an alignment-based method from domain generalization: IDGM ; and Group DRO . Notably, CLOvE learns a calibrated predictor simultaneously on all environments, but it is optimized by multi-objective learning with a differentiable regularizer for calibration. For the singe-environment setting, baselines include reweighting based techniques: CVaR , JTT , Tilted-ERM ; a Distributionally Robust Optimization method \(^{2}\)-DRO ; and a data augmentation method C-Mixup . Other methods are not included because of specification in classification  or exposure to target distribution data during training . For all experiments, we train an Oracle ERM with data sampled from target distribution.

ImplementationWe implement the predictor with MLP for ACSIncome and VesselPower, and Resnet18-MS  for PovertyMap, following WILDS' default architecture. We follow DomainBed's protocol  for _model selection_. Specifically, we randomly sample 20 sets of hyperparameters for each method, containing both the training hyperparameters and extra hyperparameters from the robust learning algorithm. We select the best model across hyperparameters based on three model selection criteria, including in-distribution validation on the average of training data, worst-environment validation with the worst performance across training environments, and oracle validation on target data. Oracle validation is not recommended by DomainBed, which suggests limited numbers of access to target data. The entire run is repeated with different seeds for three times to measure standard errors of performances. Specifically for PovertyMap, we perform 5-fold cross validation instead of three repeated experiments, following WILDS' setup.

### Results

Results are shown in Table 1 for multi-environment settings and Table 2 for single-environment settings. MC-Pseudolabel achieves superior performance in all datasets with in-distribution and worst-environment validation which does not violate test data. For oracle validation, MC-Pseudolabel achieves comparable performances to the best method. For example, CLOvE, which also learns invariance by calibration, achieves best performance under oracle validation in PovertyMap, but it sharply degrades when target validation data is unavailable. It'sbecause CLOvE tunes its regularizer's coefficient to tradeoff with ERM risk, whose optimal value depends on the target distribution shift. In contrast, MC-Pseudolabel exhibits an advantage with in-distribution model selection. This is further supported by Figure 1, which shows that MC-Pseudolabel's out-of-distribution errors strongly correlates with in-distribution errors. The experiment spans across different hyperparameters and seeds with the same model architecture on VesselPower. The phenomenon, known as accuracy-on-the-line , is well known for a general class of models under covariate shift. However, Liu et al.  shows accuracy-on-the-line does not exist under concept shift, which is the case for ERM and C-Mixup. This introduces significant challenge for model selection. However, MC-Pseudolabel recovers the accuracy to the line.

## 7 Conclusion

To conclude, we establish a new optimization framework for out-of-distribution generalization through extended multicalibration with joint grouping functions. While the current algorithm focuses on regression, there is potential for future work to extend our approach to general forms of tasks, particularly in terms of classification.

    &  &  \\  Method & ID & Worst & Oracle & ID & Worst & Oracle \\  ERM & \(0.487 0.009\) & \(0.487 0.009\) & \(0.452 0.012\) & \(0.48 0.06\) & \(0.48 0.06\) & \(0.49 0.07\) \\ IRM & \(0.466 0.002\) & \(0.466 0.002\) & \(0.465 0.002\) & \(0.38 0.07\) & \(0.39 0.06\) & \(0.45 0.08\) \\ MIP & \(0.457 0.008\) & \(0.454 0.012\) & \(0.454 0.012\) & \(0.40 0.09\) & \(0.39 0.10\) & \(0.43 0.08\) \\ IB-IRM & \(0.463 0.003\) & \(0.463 0.003\) & \(0.438 0.009\) & \(0.39 0.07\) & \(0.37 0.05\) & \(0.43 0.06\) \\ CLOvE & \(0.455 0.005\) & \(0.454 0.002\) & \(0.450 0.005\) & \(0.46 0.09\) & \(0.42 0.13\) & \( 0.06\) \\ MRI & \(0.458 0.011\) & \(0.458 0.011\) & \(0.455 0.013\) & \(0.47 0.10\) & \(0.46 0.08\) & \(0.49 0.07\) \\ REX & \(0.466 0.009\) & \(0.464 0.009\) & \(0.458 0.003\) & \(0.43 0.00\) & \(0.42 0.09\) & \(0.45 0.09\) \\ Fishr & \(0.458 0.006\) & \(0.455 0.012\) & \(0.450 0.008\) & \(0.42 0.09\) & \(0.41 0.09\) & \(0.43 0.08\) \\ IDGM & \(1.843 0.018\) & \(1.843 0.018\) & \(1.843 0.018\) & \(0.02 0.07\) & \(0.01 0.15\) & \(0.13 0.14\) \\ GroupDRO & \(0.481 0.035\) & \(0.449 0.017\) & \(0.433 0.013\) & \(0.38 0.15\) & \(0.37 0.16\) & \(0.42 0.12\) \\ MC-Pseudolabel & \(_{ 0.009}\) & \(_{ 0.012}\) & \(_{ 0.011}\) & \(_{ 0.06}\) & \(_{ 0.06}\) & \(0.50 0.06\) \\  Oracle ERM &  &  \\   

Table 1: Results on multi-environment datasets, evaluated on test data using three model selection criteria. ID: validation with averaged performance on training data. Worst: validation with the worst performance across training environments. Oracle: validation with performance on sampled test set.

Figure 1: Accuracy-on-the-line beyond covariate shift: correlation between models’ in-distribution and out-of-distribution risks on VesselPower.