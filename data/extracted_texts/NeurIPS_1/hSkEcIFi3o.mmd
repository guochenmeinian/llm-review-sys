# Adversarial Examples Are Not Real Features

Ang Li\({}^{1}\) Yifei Wang\({}^{2}\) Yiwen Guo\({}^{3}\) Yisen Wang\({}^{4,5}\)

\({}^{1}\) School of Electronics Engineering and Computer Science, Peking University

\({}^{2}\) School of Mathematical Sciences, Peking University

\({}^{3}\) Independent Researcher

\({}^{4}\) National Key Lab of General Artificial Intelligence,

School of Intelligence Science and Technology, Peking University

\({}^{5}\) Institute for Artificial Intelligence, Peking University

Equal Contribution.Corresponding Author: Yisen Wang (yisen.wang@pku.edu.cn).

###### Abstract

The existence of adversarial examples has been a mystery for years and attracted much interest. A well-known theory by Ilyas et al.  explains adversarial vulnerability from a data perspective by showing that one can extract non-robust features from adversarial examples and these features alone are useful for classification. However, the explanation remains quite counter-intuitive since non-robust features are mostly noise features to humans. In this paper, we re-examine the theory from a larger context by incorporating multiple learning paradigms. Notably, we find that contrary to their good usefulness under supervised learning, non-robust features attain poor usefulness when transferred to other self-supervised learning paradigms, such as contrastive learning, masked image modeling, and diffusion models. It reveals that non-robust features are not really as useful as robust or natural features that enjoy good transferability between these paradigms. Meanwhile, for robustness, we also show that naturally trained encoders from robust features are largely non-robust under AutoAttack. Our cross-paradigm examination suggests that the non-robust features are not really useful but more like paradigm-wise shortcuts, and robust features alone might be insufficient to attain reliable model robustness. Code is available at [https://github.com/PKU-ML/AdvNotRealFeatures](https://github.com/PKU-ML/AdvNotRealFeatures).

## 1 Introduction

Alongside the human-level or even superior performance of Deep Neural Networks (DNNs) in various tasks , concerns on the existence of adversarial examples constantly rise, regarding them as main threats that fool DNN classifiers with invisible perturbations . Among many explanations of adversarial examples , the robust and non-robust feature perspective developed by Ilyas et al.  has received wide attention. Compared to previous explanations that regard adversarial examples as "bugs" of neural networks (_i.e.,_ the model), they claim that adversarial examples stem from non-robust "features" in the inputs (_i.e.,_ the data). Specifically, they argue that each example is composed of human-perceptible _robust features_ (invariant to attack) and human-imperceptible _non-robust features_ (sensitive to attack) that are both _useful_ for classification. Under adversarial attacks, non-robust features from other classes are injected into the adversarial examples and lead to misclassification. To verify this point, they show that datasets containing only non-robust features can attain good classification performance; and similarly, datasets containing robust features can attain good robustness even with natural training. Supported by these observations, the perspective has been widely accepted ever since and many subsequential works are built upon their framework .

Although both robust and non-robust features are shown to be useful for classification, the two still have large discrepancies, particularly in perceptibility. As shown by Ilyas et al. , robust features usually have rich semantic information such as distinguishable edges and meaningful combinations of colors; on the contrary, non-robust features are always noise-like and meaningless to humans, as shown in Figure 1. Therefore, it is natural for robust features to be useful, while the usefulness of noise-like non-robust features still seems suspicious and counter-intuitive. One may raise a natural question: _are non-robust features real (useful) features?_

In this paper, we aim to re-examine robust and non-robust features in a wider context in order to understand the true distinction between them. Particularly, we notice that a major limitation of Ilyas et al.  is the scoop of learning paradigms considered, as the usefulness of non-robust features is only evaluated on the supervised image classification. However, from the backdoor literature, we know that even a meaningless backdoor pattern can lead to the desired classification. A feature being useful for classification does not necessarily imply that it is truly useful. Therefore, it is reasonable to assume that truly useful features, _e.g.,_ those utilized by humans, should work well for a wide range of learning paradigms instead of a single paradigm like the backdoor pattern.

Driven by the analysis, we take the first step by defining the _cross-paradigm_ usefulness of robust and non-robust features, as a viable measure of true usefulness that generalizes previous definitions of Ilyas et al. . A feature is defined to be truly useful only if it yields good representations across different learning paradigms. Aside from supervised learning, we further consider three modern self-supervised learning paradigms as the representatives: 1) contrastive learning that aligns augmented samples in the latent space, _e.g.,_ SimCLR ; 2) masked image modeling that predicts the masked patches from the unmasked context, _e.g.,_ MAE ; and 3) diffusion models that learn to restore images corrupted by Gaussian noise, _e.g.,_ DDPM . Among the four paradigms, supervised learning and contrastive learning are discriminative tasks, while masked image modeling and diffusion models are generative tasks. Previous studies show that all these different paradigms yield representations with good downstream performance when pretraining on _natural images_. Therefore, we would intuitively expect that if non-robust features are as useful as natural image features, we can also learn good representations from non-robust features alone. Similarly, we can define _cross-paradigm_ robustness that evaluates whether a feature can yield robust representations (with robust prediction on downstream tasks) across different learning paradigms.

To validate their true usefulness, we follow the same procedure of Ilyas et al.  and construct robust (non-robust) datasets that only have robust (non-robust) features extracted from supervised models. To evaluate their cross-paradigm usefulness and robustness, we learn features with the four different learning paradigms on the constructed datasets and evaluate the learned features with a linear probing head for classification (a common evaluation protocol in self-supervised learning). Surprisingly, we find that there exist notable discrepancies in the cross-paradigm usefulness: on the three (transferred) self-supervised paradigms, robust features yield excellent performance that nearly matches natural features, while non-robust features yield much worse prediction that is hardly usable, in sharp contrast to its own good performance on supervised learning. It clearly conveys that **non-robust features**

Figure 1: Tiny-ImageNet instances containing natural, robust, and non-robust features, respectively. The robust and non-robust instances are generated following the iterative optimization procedure in Ilyas et al.  from random noise. The robust features are semantically aligned to natural images, while the non-robust features are always noise-like.

are _not_ as (cross-paradigm) useful as robust/natural features**. Therefore, unlike robust or natural images, non-robust features do not contain much meaningful information that is truly useful across different paradigms. Intuitively speaking, non-robust features are more like a certain backdoor pattern of natural images subject to the chosen learning paradigm (_e.g.,_ supervised classification). As paradigm-wise shortcuts, these features are essentially uninformative when examined with other learning paradigms. We further verify this point by showing that adversarial examples crafted under different paradigms can hardly transfer among each other. To this end, we conclude that robust features are truly useful features while non-robust features are not (in the cross-paradigm sense). Unlike the counter-intuitive explanation of Ilyas et al. , our result justifies the human intuition that the noise-like non-robust features do not really capture the essence of the images.

Furthermore, we also re-evaluate the robustness of robust and non-robust features in this way. Interestingly, contrary to the findings in Ilyas et al.  that natural training on the robust dataset produces a robust classifier, we find these so-called robust features hardly show robustness when learned with other learning paradigms. We also observe that the supervised learned classifier is non-robust under more reliable attacks like AutoAttack . Thus, we arrive at the conclusion that **on real-world datasets, both robust and non-robust features extracted by Ilyas et al.  are actually non-robust, in both in-paradigm and cross-paradigm senses**. Although robust features are shown to exist in toy models when explicitly designed , there is by far no evidence that robust features exist in real-world datasets, at least not extractable.

To summarize, the main contributions of this work are three folds:

* In order to evaluate robust and non-robust features in a broader context, we generalize the notions on the usefulness and robustness of robust and non-robust features to a cross-paradigm sense. This perspective enables us to get rid of potential paradigm-wise shortcut effects and evaluate true feature informativeness.
* For usefulness, we find that robust features are both in-paradigm and cross-paradigm useful like natural features. Instead, non-robust features are only useful in-paradigm, and their usefulness dramatically degrades when transferred to other paradigms, suggesting that they are more like paradigm-wise shortcuts instead of real features. We further verify this point by showing that adversarial examples also have poor transferability across different paradigms.
* For robustness, we find that robustness obtained from the constructed robust dataset is actually a false sense of robustness when evaluated cross-paradigmly on more reliable attacks. The loss of this key evidence would raise the question of whether robust features really exist in real-world datasets.

Last but not least, although the main messages of this paper seem rather negative, this view also points out potential avenues to better adversarial robustness. In particular, the paradigm-wise behaviors of non-robust features suggest that it may be inadequate to perform adversarial training on a single learning paradigm, and a mixture of adversarial training on multiple paradigms may come to the aid. In the meantime, the non-robustness of robust dataset indicates a more comprehensive view of adversarial vulnerability: input data (or features) are _not_ the only source of adversarial vulnerability, and only combating the vulnerabilities in both data and models can lead to true robustness.

## 2 A Cross-Paradigm View of Robust and Non-robust Features

### Background

**Robust and Non-Robust Features.** As in Ilyas et al. , features are defined as a function mapping from input space to real numbers, namely \(f:\). The robust and non-robust features are then described with the following definitions for a binary classification task:

* \(\)**-useful features:** For a dataset \(\), a feature \(f\) is \(\)-useful if the feature is correlated with the label: \[_{(x,y)}[y f(x)].\] (1)
* \(\)**-robustly useful features:** For a \(\)-useful feature, it is regarded as \(\)-robustly useful if it remains useful under certain range of perturbation: \[_{(x,y)}_{}y f(x+ ).\] (2)* **Useful, non-robust features:** These feature are the ones that are \(\)-useful features, but are not \(\)-robust features for any \( 0\).

**Construction of Robust and Non-Robust Datasets.** Given a classifier \(C\) and a dataset \(\), Ilyas et al.  propose to construct a dataset \(}\) which satisfies:

\[_{(x,y)}[f(x) y]=\{ _{(x,y)}[f(x) y]&f F_{C}\\ 0&,. \]

where \(F_{C}\) represents the set of features utilized by \(C\). Denoting \(g\) as the mapping from input \(x\) to the representation layer in \(C\), the new instance \(x_{r}\) is obtained from \(x\) through following optimization:

\[_{x_{r}}||g(x)-g(x_{r})||_{2}. \]

If the classifier \(C\) (_e.g.,_ ResNet-50) is adversarially trained, the constructed dataset \(}\) is regarded as a _robust dataset_. As for a standardly trained classifier, the dataset \(}\) is regarded as a _non-robust datatset_.

**Basic Conclusions of Ilyas et al. .** Utilizing the constructed datasets, Ilyas et al.  empirically show that models trained on the non-robust dataset are _useful_, _i.e.,_ they can attain good classification performance on natural test data, comparable to models trained on the natural dataset. This supports their claim that non-robust features are sufficiently useful to obtain good generalization. This perspective well explains the _existence_ of adversarial examples as well as their _transferability_, since adversarial examples contain non-robust features from the misclassified classes that are useful for different models. Therefore, as they put it, adversarial examples are features, not bugs. Also, they also show that natural training on the robust dataset (containing robust features alone) can yield robust models. In this way, they view adversarial vulnerability purely from a data (or feature) perspective, regarding them as a result of different priors on extracted features.

**Self-Supervised Learning.** Aside from the supervised learning paradigm studied in Ilyas et al. , self-supervised learning (SSL) also received wide attention in recent years. Without manual labels, SSL methods utilize self-supervision to learn meaningful features from unlabeled data and have achieved impressive progress in recent years [6; 16; 17]. Generally speaking, an SSL algorithm pre-trains a feature encoder \(f:\), mapping from the input space to the latent space \(\). Afterward, the learned features are typically evaluated on the so-called linear probing task. Specifically, given a labeled dataset \((x,y)\) (usually a subset of the pretraining data), we train a linear classifier \(p:\) on top of learned features and use its classification accuracy (with the composed classifier \(p f\)) on the test data as a measure of the usefulness of learned features (representations).

### Cross-Paradigm Notions of Robust and Non-Robust Features

In this part, we generalize the definitions of robust and non-robust features to a wider context beyond the supervised classification. For a rigorous discussion, we introduce some general definitions of the paradigm-wise feature usefulness and robustness and then define true usefulness and robustness in the cross-paradigm sense.

**Paradigm-Wise Definitions.** To facilitate features to generalize across different paradigms, we differ from Ilyas et al.  and adopt a more classic definition of features, filters in the input space, _i.e.,_\(g:\). Note that common image features like textures, edges, and colors naturally fall into this category. We define a learning paradigm \(T\) as a specific learning algorithm that learns a feature encoder \(f\) with a loss function \(L_{T}\) over a given dataset \(\):

\[f_{T}=*{arg\,min}_{f}_{x,y}L_{T}(f (x),y). \]

Incorporating a feature \(g\), the minimization problem can be further specified as

\[f_{T}^{(g)}=*{arg\,min}_{f}_{x,y} L_{T}(f(g(x)),y). \]

With each learning paradigm \(T\), we can train an encoder \(f_{T}^{(g)}\) for feature \(g\). Then we evaluate the learned representations with a linear probing head \(p\) on top of \(f_{T}^{(g)}\) on \(\). The usefulness of a feature \(g\) relevant to the paradigm \(T\) is defined as follows:

\[U(g,,T)=_{p}_{x,y} [p(f_{T}^{(g)}(x))=y]. \]That is, a feature \(g\) is useful on paradigm \(T\) if the linear probing head shows good classification performance with representations learned from the feature \(g\) under the learning objective defined by the paradigm \(T\).

Accordingly, the paradigm-wise feature robustness is defined as the remaining feature usefulness under local adversarial perturbations,

\[R(g,,T)=_{p}_{x,y}_{ \|\|}[p(f_{T}^{(g)}(x+_{x}))=y]. \]

In other words, a feature is robust on paradigm \(T\) if a standardly trained linear probing head shows good robustness with representations learned from the feature \(g\) under \(T\).

From this perspective, the conventional definitions of feature usefulness and robustness of Ilyas et al.  correspond to a special case of our definitions - when choosing the supervised learning task as the learning paradigm. In this way, the two processes, feature learning and feature evaluation, actually share the same learning objective, which may introduce spurious paradigm-wise shortcut effects. To get rid of this potential risk, we propose to re-define these concepts in a cross-paradigm sense.

**Cross-Paradigm Definitions.** Given a diverse set of learning paradigms \(=\{T_{1},T_{2},,T_{3}\}\), we define a feature to be truly useful if it can generalize across multiple learning paradigms. Specifically, we define cross-paradigm usefulness as the worst paradigm-wise usefulness among these paradigms,

\[CU_{}(g,)=_{T }U(g,,T). \]

Similarly, we can define cross-paradigm robustness as the worst robustness

\[CR_{}(g,)=_{T }R(g,,T). \]

Nevertheless, since different paradigms usually attain performance on different levels, a direct comparison of their absolute performance may be unfair. In the worst case, if a certain paradigm has poor performance even when pretrained on the raw images, it would dominate other paradigms when computing cross-paradigm metrics, which, however, does not reflect the true feature usefulness. To mitigate this potential issue, we propose the paradigm-wise relative metrics as the ratio between the performance of the chosen feature and the performance of using all features (raw inputs),

\[RU(g,,T)=,T)}{U(,T)}, \] \[RR(g,,T)=,T)}{R(,T)}, \]

where given an encoder \(f_{T}\) learned from the _raw images_ under the paradigm \(T\), we define

\[U(,T):=_{p}_{x,y}[p(f_{T}(x))=y],R(,T)=_{p}_{x,y }_{\|\|}[p(f_{T}(x+_{x}))=y].\]

Accordingly, we can define the cross-paradigm relative usefulness and robustness as follows:

\[CRU_{}(g, )=_{T}RU(g,,T), \] \[CRR_{}(g, )=_{T}RR(g,,T). \]

## 3 Cross-Paradigm Usefulness of Robust and Non-robust Features

Built upon the evaluation framework established in Section 2.2, we first investigate the cross-paradigm usefulness of robust and non-robust features on real-world datasets in this section. With this generalized notion of feature usefulness, we are trying to answer the key question: _are robust and non-robust features truly useful?_

### Setup

**Data Construction.** We include two commonly adopted datasets in our study, CIFAR10  and Tiny-ImageNet-200 . Aside from the raw images, following the same construction process in Ilyas et al.  (details in Appendix A), we further construct a robust version and a non-robust version for each dataset, which only contain robust and non-robust features, respectively.

**Learning Paradigms.** Besides supervised learning with the cross-entropy loss, we also include three self-supervised learning paradigms for a cross-paradigm evaluation: SimCLR  for Contrastive Learning (CL), MAE  for Masked Image Modeling (MIM), and DDPM  for Diffusion Models (DM). We then train linear probing heads for the encoders on the same dataset that it was trained on. The probing heads are directly applied to the output of the encoder for SimCLR and MAE models, while for DDPM model, since the U-Net encoder has complex high-dimensional hidden features, we use the global average pooled feature in the fourth upsampling layer in U-Net following Xiang et al. , which is shown to deliver excellent linear classification performance on par with other self-supervised learning paradigms. Also, we train a ResNet-50 on the datasets with Supervised Learning (SL) as our baseline. To summarize, the four learning paradigms considered in this work are \(=\{,,,\}\). After pretraining, we evaluate the classifiers' performance on the test sets of the two datasets. More details on training and evaluation are in Appendix A.

### Non-robust Features are Not Cross-paradigmly Useful

We first examine the paradigm-wise usefulness of robust and non-robust features extracted from the supervised models. As shown in Table 1, models pretrained from robust datasets (containing only robust features) show comparable performance to those pretrained from the natural datasets (containing all features). In comparison, models pretrained from the non-robust datasets (containing only non-robust features) differ a lot across different paradigms: they work well for learning from the supervised task (ResNet-50), but much worse on all the other SSL paradigms (SimCLR, MAE, DDPM). It shows that in contrary to the perceptually aligned robust features that are useful for different paradigms, the noise-like non-robust features do not really contain meaningful information that is also useful for other SSL paradigms. Instead, these non-robust features extracted from supervised models only show usefulness on the supervised task. This sharp distinction suggests that non-robust features are more like paradigm-wise shortcut features rather than being truly useful.

For a comprehensive evaluation, we further compute the cross-paradigm _relative_ usefulness following the definitions in Section 2.2. As shown in Table 1, the relative usefulness of non-robust features is significantly lower than that of robust features under various self-supervised learning paradigms. The overall cross-paradigm ends up being 0.307, which is much smaller than robust features (0.791). In comparison, these two kinds of features show similar performance (0.854 and 0.823) in Ilyas et al.  when only considering the supervised task. It shows that from a paradigm-wise view, the usefulness of features dramatically differs from the case where only supervised learning is considered. Again, it verifies that non-robust features are not as useful as natural and robust features when transferred to many other learning paradigms. For completeness, in Appendix C.1, we further examine the cross-paradigm transferability of the non-robust features extracted from the three SSL paradigms. Similarly, we find that these SSL non-robust features are non-transferable as well.

## 4 Cross-Paradigm Robustness of Robust Features

The results above challenge the claim from Ilyas et al.  that non-robust features are useful features by showing that non-robust features are not really useful in the cross-paradigm sense. Aside from the usefulness of non-robust features, another notable finding in Ilyas et al.  is that robust features

   Data & Feature & MIM & CL & DM & SL & Cross. Rel. Useful. & Ilyas et al.  \\   & Robust & **0.896** & **0.791** & **0.831** & 0.880 & 0.791 & 0.854 \\  & Non-Robust & 0.307 & 0.433 & 0.512 & **0.914** & **0.307** & _0.823_ \\   & Robust & 0.691 & 0.775 & 0.861 & 0.880 & 0.672 & 0.407 \\  & Non-Robust & 0.134 & 0.074 & 0.141 & 0.645 & **0.134** & _0.396_ \\   

Table 1: Evaluation of relative usefulness of robust features and non-robust features on four learning paradigms: MIM, CL, DM, and SL. The cross-paradigm relative usefulness is computed as the worst relative usefulness over the four paradigms. We also include the usefulness scores (evaluated only on the supervised task) reported in Ilyas et al.  for a comparison.

alone are enough for attaining model robustness. Specifically, they extract a robust version of a given dataset (_e.g.,_ CIFAR-10) by distilling from a robust classifier such that the transformed samples only contain robust samples, and they show that a naturally trained classifier on the constructed "robust dataset" has good robustness. In this section, we further examine the robustness of the extracted robust features on different learning paradigms.

### Setup

We construct the robust versions of CIFAR10 with a supervised \(_{}\)-robust ResNet-18 encoder  and a self-supervised \(_{}\)-robust ResNet-18 encoder , largely following the convention of Ilyas et al. . We then train encoders on the robust datasets with the learning paradigms set \(=\{,,,\}\) and use the learned representations to naturally train a linear head. Finally, we evaluate the robustness of the composed classifiers (encoders + linear heads) with a modern reliable attack method, AutoAttack , under the perturbation budget of \(4/255\), instead of the PGD attack  that may lead to over-estimated robustness . More experimental details can be found in Appendix A.

### Robust Features are Not Robust both In-Paradigm and Cross-Paradigm

We summarize the results in Table 2. Viewing from the in-paradigm results in Table 2(a), we can observe that in supervised learning, PGD attacks can over-estimate the robustness of models trained from robust datasets, _e.g.,_\(15.71\) (PGD) _v.s._\(3.65\) (AutoAttack). When evaluated under AutoAttack, robust datasets generated from either \(_{2}\)-robust or \(_{}\)-robust models show poor robustness, being hard to match the robustness of adversarially trained source models. In other words, in practice, we believe it difficult to attain good robustness with only robust features (extracted following Ilyas et al. ). Their original conclusion on the good robustness of robust datasets could be due to the unreliability of PGD-based robust evaluation (also shown in the AutoAttack paper ). The severe over-estimation on robust datasets is potentially due to gradient obfuscation or masking  since these robust datasets are essentially generated with the PGD attack.

Extending the finding, we also evaluate the robustness of robust features on other self-supervised learning paradigms in Table 2(b). Likewise, robust features also perform poorly and do not attain non-trivial robustness. Interestingly, when training on generative models like MIM and DM, robust features obtain slightly better robustness compared to the discriminative ones (CL and SL), even on natural data. Overall, the cross-paradigm robustness (defined in Section 2.2) is still bottlenecked by the SL itself. Based on these results, we conclude that robust features still cannot achieve model robustness alone (in either in-paradigm or cross-paradigm sense), without model-level interventions like adversarial training. It suggests that adversarial vulnerability may not come from the data alone, and a joint training strategy against both data-level and model-level vulnerabilities may be needed to attain real robustness. It worths noting that Tsilivis et al.  also found the non-robustness of the robust dataset of Ilyas et al.  in the supervised domain, while our results here give a full examination of robust features from both in-paradigm and cross-paradigm aspects.

## 5 Cross-Paradigm Transferability of Adversarial Attacks

The transferability across different models, even with different architectures [47; 40], is a fascinating fact of adversarial examples. The perspective of Ilyas et al.  provides a natural explanation for the phenomenon since non-robust features in adversarial examples are inherently useful such that they can affect the prediction of different models. Nevertheless, our studies in Section 3 reveal that non-robust features are only paradigm-wise but not cross-paradigm useful. It implies that adversarial transferability is also paradigm-wise. In other words, adversarial examples can hardly transfer between different paradigms. To verify the hypothesis above, we further investigate the cross-paradigm transferability of adversarial attacks in this section.

### Cross-Paradigm Transferability of Attack Objectives

Since different paradigms mainly differ by their learning objectives, adversarial attacks _w.r.t._ different learning objectives (_e.g.,_ SL, CL, MIM, DM) can generate adversarial examples of different paradigms. To study the transferability between different attack objectives, we consider two ResNet-18 backbone networks (learned with SL and CL, respectively), attack them with an objective, and observe the change of both learning objectives.

**Setup.** We consider two objectives: InfoNCE loss  in CL and Cross Entropy (CE) loss in SL. Adversarial examples are generated with \(l_{}\) AutoAttack bounded by \(=4/255\), using the default attack settings on CIFAR-10 (details in Appendix A). To be consistent, we compute the InfoNCE loss for a supervised encoder by appending a re-trained projection head on top of the fixed encoder. When computing the gradient during attacks, we also retain the standard data augmentations in SimCLR  and differentiate through these operators with the Kornia package . For further ablation study on the influence of these components on transferability, see Appendix B.

**Results.** We plot the change of loss values during the attacking process in Figure 2, from which we can identify a general trend: there is a significant difference between the changes of different objectives, and maximizing one attack objective has a limited effect on the other objective. This distinction clearly implies that adversarial examples generated with attack objectives from different paradigms have poor transferability among each other.

### Cross-Paradigm Transferability of Backbone Encoders

Aside from the attack objective discussed in Section 5.1, different paradigms also learn different paradigm-wise feature encoders that also have a large influence on the generation of adversarial examples. To further analyze the paradigm-wise influence of feature encoder on adversarial transferability, we generate adversarial examples with backbone encoders obtained by the four different paradigms, while keeping the attack objective to be the same CE loss (using the linear head learned for each paradigm). Besides the default ResNet-50(SL-RN) model, we further include three different

    & \), \(=0.5\)} & \), \(=4/255\)} \\  & PGD-1000 & AutoAttack & PGD-1000 & AutoAttack \\  \(_{2}\)-robust classifier & 15.71 \(\) 0.21 & 3.65 \(\) 0.58 & 6.36 \(\) 1.66 & 0.54 \(\) 0.16 \\ \(_{}\)-robust classifier & 13.96 \(\) 0.12 & 5.03 \(\) 0.16 & 17.63 \(\) 0.16 & 3.52 \(\) 0.31 \\   

Table 2: Absolute robustness of robust features on four paradigms: MIM, CL, DM, and SL, on CIFAR-10. The robust features are extracted by pretrained models from two different source paradigms: supervised learning and contrastive learning. Different from Ilyas et al.  that use 1000-step PGD  for robust evaluation, we adopt a more reliable attack algorithm, AutoAttack .

Figure 2: The change of loss value _v.s._ the attack iteration steps when using different attack objectives, CE loss (blue lines) or InfoNCE loss (orange lines), and backbones, trained by SL (Figure 1(a) & Figure 1(b)) or CL (Figure 1(c) & Figure 1(d)) from different paradigms.

supervised models, DenseNet-121 (SL-DN) , and Inception-V3 (SL-InC)  as baselines. We plot the transferred adversarial robustness in the confusion matrix in Figure 3.

The results in Figure 3 demonstrate two important messages: 1) adversarial examples transfer well across different architectures (SL-RN, SL-DN, SL-InC) under the same paradigm (SL), and 2) adversarial examples transfer poorly between different paradigms, even under the same architecture (for example, both CL and SL adopt ResNet-50 as backbones). It suggests that for adversarial transferability, paradigms have much more influence than model architectures. This phenomenon can be well explained by the paradigm-wise shortcut-like behaviors of non-robust features (Section 3), and it indicates that adversarial transferability is also largely a paradigm-wise phenomenon.

### Relationship to Natural Transferability between Paradigms

In the discussions above, we demonstrate that adversarial examples do not have (good) transferability between different learning paradigms. We note that this discovery in _adversarial transferability_ is not in contradiction to the good _natural transferability_ between different paradigms. Indeed, many existing works [6; 15; 41; 16; 50; 52] show that representations learned from SSL have good downstream performance, particularly when evaluated with linear probing. Moreover, theoretical guarantees on the downstream classification performance have also been established for contrastive learning [13; 42; 43; 8], non-contrastive learning , and masked image modeling . Since adversarial examples are essentially out-of-distribution examples (not drawn from the natural data distribution), the generalization guarantees on natural data do not apply. The fact that these paradigms have good natural transferability and poor adversarial transferability serves as another piece of evidence for our understanding that the misclassification of adversarial examples is caused by paradigm-wise shortcuts instead of real useful features.

## 6 Conclusion

In this paper, we have investigated the real usefulness and robustness of robust and non-robust from a wider context. By studying their behaviors across four different learning paradigms, we have found that robust features are as useful as natural features, while non-robust features generated by attacking supervised models become largely useless when transferred to other self-supervised learning paradigms, indicating that non-robust features are not real features but more like paradigm-wise shortcuts. Meanwhile, we have also shown that robust datasets containing only robust features are insufficient to attain model robustness under AutoAttack, indicating that feature non-robustness is not the only source of adversarial vulnerability. Posed as a challenge to common beliefs of robust and non-robust features, this work advocates the idea that their real usefulness and robustness should be examined under a cross-paradigm perspective as well as more reliable attacks.

Figure 3: The cross-paradigm robustness of adversarial examples generated with encoders from different learning paradigms. The \((A,B)\)-th cell represents the accuracy of adversarial examples generated with an \(A\)-paradigm model (encoder with linear head) when evaluated on a \(B\)-paradigm model (encoder with linear head). Darker colors (_i.e.,_ higher accuracy) indicate worse transferability of adversarial examples.