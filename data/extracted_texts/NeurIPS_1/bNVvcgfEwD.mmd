# Convergence of Adafactor under Non-Convex Smooth Stochastic Optimization

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

As model sizes in deep learning continue to expand, memory-efficient optimizers are increasingly critical to manage the substantial memory demands of popular algorithms like Adam and AdamW. Among these, Adafactor has emerged as one of the widely adopted choices for training deep learning tasks, particularly large language models. However, despite its practical success, there is limited theoretical analysis on Adafactor's convergence. This paper presents a comprehensive analysis on Adafactor in a non-convex smooth setting, demonstrating its convergence to find a stationary point at a rate of \(}(1/)\). We find that the default hyper-parameter setting results in a sub-optimal rate in our framework, and propose an alternative setting that could theoretically achieve optimal convergence rate. This finding is further supported by some experimental results. We also prove that Adafactor with a suitable time-varying clipping threshold could also converge, achieving performance in experiments comparable to that of the standard constant setting.

## 1 Introduction

The adaptive gradient-based methods, such as the well-known AdaGrad [9; 29], RMSProp , Adadelta , Adam  and AdamW , have become the preferred approaches in solving the following unconstrained stochastic optimization problem in deep learning fields:

\[_{^{n m}}f()=_{}[l(;)], \]

where the object function \(f\) is non-convex and \(\) denotes a probability distribution. During the training process, these adaptive methods require to store the historical gradients' information so as to adaptively tune their step-sizes. For example, both Adam and AdamW maintain the exponential average of gradients and squared gradients, and AdaGrad stores the cumulative of squared gradients. Despite their effectiveness, these algorithms pose substantial memory challenges for GPUs to save these additional gradients' information, especially when training large language models (LLMs), such as GPT-3 , which contains over 175 billion parameters.

To address memory constraints, several memory-efficient optimization algorithms have been developed, e.g., [26; 1; 23; 17]. One of the most popular optimizers is Adafactor  which employs a rank-1 matrix factorization to approximate the second moment matrix in Adam. For an \(n m\) weight matrices, this technique reduces memory usage from \((mn)\) to \((m+n)\) by only tracking the moving averages of the row and column sums of the squared gradients matrix. Additionally, Adafactor eliminates the first-order momentum used in Adam and incorporates update clipping to enhance training stability.

The empirical results reveal that Adafactor achieves comparable performance to Adam in training Transformer models . In real applications, several LLMs including PaLM  and T5  haveapplied Adafactor as their main optimizers . In spite of Adafactor's widely usage, there is still limited understanding on its convergence in theory, especially the effect of the matrix approximation and update clipping, and the explanation for its hyper-parameter setting in experiments.

In this paper, we take a closer look on Adafactor's convergence under non-convex smooth optimization problems, considering the typical bounded gradient setting as those for AdaGrad [19; 32] and Adam . We aim to provide a convergence rate for Adafactor and explain the influence of the hyper-parameters for the convergence speed. We also prove in theory why the default parameter setting is effective in practical scenarios. The analysis to Adafactor is non-trivial compared to other adaptive methods such as AdaGrad and Adam due to the unique matrix factorization and update clipping mechanisms. Based on a new proxy step-size construction and some new compositions as well as estimations, we analyze the additional error terms in the Descent Lemma introduced by the matrix approximation and update clipping. Our main contributions are summarized as follows.

#### Contributions

* We provide a convergence analysis for the full-batch Adafactor considering bounded gradients and a broader range of parameter setting which covers the default one in . The result shows that Adafactor could converge to find a stationary point with a rate of \(}(1/)\) where \(T\) denotes the total iteration number.
* We further investigate the more realistic stochastic Adafactor. It's found that a simple variant of Adafactor, which drops the update clipping, could attain the best convergence rate of \(}(1/)\) when the second moment decay rate is \(1-1/k\). We also verify that the default decay rate \(1-1/k^{0.8}\) could lead to a sub-optimal convergence rate in our framework. To illustrate this finding, we provide some empirical results, showing that the potential best hyper-parameter setting in theory could perform better than the default one used in experiments.
* We extend our study to include a time-varying clipping threshold. Our analysis implies that with proper selections of clipping threshold and hyper-parameters, Adafactor could also achieve the best convergence rate of \(}(1/)\). We also do some experiments to show that the new clipping threshold scheme achieves comparable performance and training stability to the original constant threshold setting.

The rest of the paper is organized as follows. The next section provides some most relevant works. Section 3 presents some necessary notations definitions and problem setup. Section 4 reviews Adafactor and introduces its essential mechanism. In Section 5 and Section 6, we separately provide convergence bounds for full-batch Adafactor and stochastic Adafactor without update clipping. We further discuss the hyper-parameters' dependency. In Section 7, we investigate Adafactor using a time-increasing update clipping threshold. Section 8 provides experimental results to support our theory. All the detailed proof could be found in the appendix.

## 2 Related work

In this paper, we mainly investigate the theoretical convergence of Adafactor. Although there is limited works on Adafactor in theory, it's necessary to briefly discuss related works on the convergence of other adaptive methods, particularly on non-convex smooth optimization. Here, we briefly list some of the most related works.

Convergence of adaptive methodsSeveral studies address the convergence of AdaGrad in non-convex settings. For example,  considered a simple variant with delayed step-size, while  and  assumed bounded stochastic gradients. Other works [14; 10; 21; 3; 31; 27; 33] derived convergence bounds under more relaxed assumptions. Another line of research has investigated the convergence of Adam. For instance, [34; 7; 39; 11; 8] assumed bounded gradients. [28; 36; 31] considered more relaxed noise assumptions without relying on bounded gradients. Additionally,  derived convergence bounds for Adam under generalized smooth conditions.

Overall, the convergence analysis of optimizers typically starts with standard assumptions, such as bounded gradients and smooth objective functions. In subsequent studies, these assumptions are gradually relaxed to investigate the convergence properties of the optimizers under less stringent conditions.

Memory efficient algorithmsAs large models are increasingly used in deep learning, memory constraints have become a central issue during training. Consequently, several memory-efficient optimizers have been developed to address this challenge.

One approach to save memory involves applying matrix factorization to oeptimization algorithms. For instance,  used matrix factorization in the second moment estimator of gradients in Adam, similar to the concept behind Adafactor.  introduced CAME, a variant of Adafactor, which incorporates a confidence-guided strategy to mitigate instability caused by erroneous updates.  proposed Adapprox, leveraging randomized low-rank matrix approximation for Adam's second moment estimator, demonstrating superior performance and reduced memory usage compared to AdamW.

There are some other techniques to save the memory. For example,  relied on a "Shampoo" technique to reduce the storage requirement of full-matrix preconditioning methods. Notably, their method could be further extended to the more realistic tensor case.  presented a memory-saved version of AdaGrad, called SM3, by maintaining \(k\) sets gradient accumulator. They proved the convergence guarantee of SM3 on online convex optimization and the effectiveness in experiments. Recently,  built a 4-bit Adam using quantization techniques to compress the first and second moment estimators in Adam, also reducing memory usage.

In summary, many existing optimizers, particularly adaptive methods like AdaGrad and Adam, face memory overhead. In response, the discussed works have designed memory-efficient optimizers that aim to achieve comparable performance to these existing methods while achieving memory benefits.

## 3 Problem setup

To start with, we introduce some necessary notations.

NotationsFor any two matrices \(=(x_{ij})_{ij},=(y_{ij})_{ij}^{n m}\), we define \(,=_{i=1}^{n}_{j=1}^{m}x_{ij}y_{ij}\). \(\), \(/\) and \(}\) denote the coordinate-wise product, quotient and squared root respectively. \(_{n}\) and \(_{n}\) denote the zero and one \(n\)-dimensional vector respectively, and \(_{n m}\) denotes the one \(n m\)-dimensional matrix. The index set \([n]\) denotes \(\{1,2,,n\}\). \(\|\|_{F}\) denotes the Frobenius norm. For a positive sequence \(\{_{i}\}_{i 1}\), we define \(_{i=a}^{b}_{i}=0\) and \(_{i=a}^{b}_{i}=1\) if \(a>b\). The operator \(()\) denotes

\[()=_{i=1}^{n}_{j=1}^{m}x_{ij}^{2}}.\]

We consider unconstrained stochastic optimization (1) over \(^{n m}\) with the Frobenius norm. The objective function \(f:^{n m}\) is differentiable. Given an \(n m\) matrix \(\), we assume a gradient oracle that returns a random matrix \(g(,)^{n m}\) dependent by the random sample \(\). The deterministic gradient of \(f\) at \(\) is denoted by \( f()^{n m}\).

AssumptionsWe make the following standard assumptions throughout the paper.

* (**A1**) \(L\)-smoothness: For any \(,^{n m}\), \(\| f()- f()\|_{F} L\|-\|_{F}\);
* (**A2**) Bounded below: There exists \(f^{*}>-\) such that \(f() f^{*},^{n m}\);
* (**A3**) Unbiased estimator: The gradient oracle provides an unbiased estimator of \( f()\), i.e., \(_{}[g(,)]= f(),^{n m}\);
* (**A4**) Almost surely bounded stochastic gradient: for any \(^{n m}\), \(\|g(,)\|_{F} G\), a.s..

Combining (A3) and (A4), it's easy to verify that \(\| f()\| G,^{n m}\). Assumptions (A1)-(A3) are standard in the non-convex smooth convergence analysis. Although Assumption (A4) is a bit strong since it requires an almost surely bounded stochastic gradients instead of an expected one, it's still commonly used to derive the high probability convergence bound, see e.g., [32; 14], which is a stronger result than an expected convergence. In coordinate-wise algorithm, another standard assumption is \(l_{}\)-bounded gradient where \(\|g(,)\|_{} G_{}\), see e.g., . These two types of assumption are equivalent up to dimension factors.

Review of Adafactor

In this section, we briefly discuss Adafactor based on the reference . The pseudocode for Adafactor is presented in Algorithm 1.

```
Input: Initialization point \(_{1}^{n m}\), \(_{0}=_{m}\), \(_{0}=_{n}^{}\), relative step-sizes \(\{_{k}\}_{k 1}\), decay rate \(\{_{2,k}\}_{k 1}[0,1)\), regularization constants \(_{1},_{2}>0\), clipping threshold \(d\). for\(k=1,,T\)do \(_{k}=g(_{k},_{k})\); \(_{k}=_{2,k}_{k-1}+(1-_{2,k})(_{k}_{k}+ _{1}_{n}_{m}^{})_{m}\); \(_{k}=_{2,k}_{k-1}+(1-_{2,k})_{n}^{}(_{k} _{k}+_{1}_{n}_{m}^{})\); \(_{k}=(_{k}_{k})/_{n}^{}_{k}\); \(_{k}=_{k}/_{k}}\); \(_{k}=\{_{2},(_{k})\}_{k}/\{1, (_{k})/d\}\); \(_{k+1}=_{k}-_{k}_{k}/_{k}}\); endfor
```

**Algorithm 1** Adafactor

Matrix factorizationAdafactor could be severed as a saved-memory version of Adam. Throughout the training process, Adam maintain two \(n m\) matrices \(_{k}\) and \(_{k}\) using exponential moving average update,

\[_{k}=_{1,k}_{k-1}+(1-_{1,k})_{k},_{k}= _{2,k}_{k-1}+(1-_{2,k})_{k}_{k}, \]

where \(_{1,k},_{2,k}(0,1)\), thereby tripling the memory usage. The innovation in Adafactor lies in its method of approximating \(_{k}\) by factoring it into two rank-1 matrices, specifically the row sums and column sums of \(_{k}\). This approximation is guided by maintaining a minimal general Kullback-Leibler (KL) divergence as follows,

\[_{^{n},^{1 m}}_{i=1}^{n} _{j=1}^{m}d((_{k})_{ij},()_{ij}), ()_{i} 0,()_{j} 0, i[n],j[m],\]

where \(d(p,q)=p(p/q)-p+q\). The choice of KL-divergence over the more typical Frobenius norm allows for an analytical solution to be derived, specifically given by

\[=_{k}_{m},=_{n}^{}_{k}/( {1}_{n}^{}_{k}_{m}).\]

Therefore, Adafactor only requires to maintain two vectors \(_{k}=_{k}_{m}\), \(_{k}=_{n}^{}_{k}\), sufficiently reducing the memory from \(2mn\) to \(m+n\). Although this factorization sacrifices some information of the squared gradients, Adafactor still delivers performance comparable to Adam in many real application tasks, making it a practical choice where memory is a constraint.

Increasing decay rateIn Adam, corrective terms are introduced into \(_{k}\) and \(_{k}\), resulting in two increasing-to-one decay rates. Theoretically, it has been demonstrated that a value closed to one for \(_{2,k}\) would ensure the convergence, e.g., . Inspired by this observation, Adafactor used an increasing second moment decay rate \(_{2,k}=1-1/k^{c},c 0\), and the empirical default setting is \(c=0.8\). As pointed out by , this setting allows for enjoying the stability of a low \(_{2,k}\) at the early stage of training and the insurance of convergence from a high \(_{2,k}\) as the run progresses. Moreover, it also leverages the bias correction.

Update clippingAdafactor modifies the update process by discarding the first-order moment \(_{k}\) and instead applies an update clipping technique inside the step-size \(_{k}\). This involves dividing the root-mean-square of the update \(}_{k}\), denoted as \((_{k})\), when it exceeds a threshold \(d\). This mechanism helps to calibrate the second moment estimator \(_{k}\) when it's larger-than-desired \(_{k}_{k}\). Empirical findings in  indicated that implementing update clipping leads to significant performance improvements when the warm-up technique is not used.

Relative step-sizesAdafactor incorporates a step-size proportional to scale of \(_{k}\), denoted by \((_{k})\), which is shown in experiments more resilient to the more naive parameter initialization and scaling schemes .

Convergence result for full-batch Adafactor

We first provide the convergence bound for full-batch Adafactor. At each iteration, full-batch Adafactor obtains the deterministic gradient \( f(_{k})\) and then updates \(_{k},_{k}\) using \( f(_{k})\) instead of \(_{k}\) in Algorithm 1.

**Theorem 5.1**.: _Let \(\{_{k}\}_{k 1}\) be generated by Algorithm 1 with \(g(_{k},_{k})= f(_{k}), k 1\). If Assumptions (A1) and (A2) hold, \(\| f(_{k})\|_{F} G, k 1\), \(_{2,1}=\) and_

\[_{k}=_{0}/, 0<_{2,k}<1, k 1, \]

_for some positive constant \(_{0}\), then for any \(T 1\),_

\[_{k[T]}\| f(_{k})\|_{F}^{2}(}). \]

The result indicates that full-batch Adafactor could find a stationary point at a rate of \(( T/)\) under the non-convex smooth case, which is similar to gradient descent but with a sub-optimal rate compared to \((1/T)\). The hyper-parameter setting in (3) only requires \(_{2,k}(0,1)\), denoting a much wider range including the default one which requires \(_{2,k}\) to increase to one. The detailed version for the above result can be found in Theorem A.1 from the appendix.

## 6 Stochastic Adafactor without update clipping

In the stochastic case, we start from the simple scenario of

\[_{k}=\{_{2},(_{k})\}_{k} \]

without considering the update clipping \(1/\{1,(_{k})/d)\}\) in Algorithm 1, where the main reasons are as follows.

* As pointed out in the experiments from , Adafactor's performance shows little difference with and without update clipping when implementing learning rate warm-up. Since the warm-up technique is a popular method in deep learning , it's reasonable to drop the update clipping.
* In stochastic Adafactor, the correlation between \(_{k}\) and \(_{k}\) would be more complex if the update clipping is involved. The proof would be simpler when dropping the update clipping, which could help to better understand the analysis for Adafactor.

We now present the probabilistic convergence bound for Adafactor without update clipping as follows, where we summarize different convergence rate with respect to the factor \(c\) from \(_{2,k}=1-1/k^{c},c[1/2,1]\).

**Theorem 6.1**.: _Let \(\{_{k}\}_{k 1}\) be generated by Algorithm 1 without update clipping where \(_{k}\) is given by (5) for each \(k 1\). If Assumptions (A1)-(A4) hold, and_

\[_{2,1}=1/2,_{1}=_{0}, \] \[_{2,k}=1-1/k^{c},_{k}=_{0}/, k  2,\]

_for some constants \(1/2 c 1,_{0}>0\), then for any \(T 1,(0,1)\), with probability at least \(1-\),_

\[_{k[T]}\| f(_{k})\|_{F}^{2}(}()).\]

The above result indicates that with appropriate hyper-parameters, Adafactor without update clipping could approximately find a stationary point. When the decay rate \(_{2,k}\) is \(1-1/k\), the convergence rate could attain to \(( T/)\), matching the rate of stochastic gradient descent  and the lower rate in  up to only a logarithm factor. The hyper-parameter setting in (6) covers the experimental default setting where \(c=0.8\). The result shows a sub-optimal rate of \(( T/T^{0.3})\) under the default setting. This finding is further complemented by the coming numerical experiments in Section 8. The detailed version of the above results can be found in Theorem B.1 from the appendix.

### Discussion of the hyper-parameter dependency

In this section, we discuss the dependency of several important hyper-parameters in Theorem 6.1 and the detailed version in Theorem B.1 in the appendix. It's worthy to mention that the dominated order in our convergence bound is determined by the total iteration number \(T\), whereas other hyper-parameters could be regarded as constants. However, we hope to improve the dependency of these hyper-parameters as much as possible to make the convergence bound tight.

Discussion of \(c\) and the optimal rateThe convergence bound in Theorem 6.1 reveals that when \(c=1,_{2,k}=1-1/k\) and \(_{k}=_{0}/\), the convergence rate attains the optimal rate matching the lower bound. In addition, when \(c\) is closed to \(1/2\), the convergence rate deteriorates. This phenomenon somehow explains that a small decay rate \(_{2,k}\) (\(c\) is low) may harm the convergence speed, as \(_{2,k}\) should be closed enough to \(1\) to ensure the convergence, which has been pointed out similarly for Adam in [8; 39; 36].

The theoretical best parameter setting remains a small gap to the default one of \(c=0.8\). To verify our theoretical finding, we provide some empirical evidence in Section 8, showing that \(_{2,k}=1-1/k\) performs even better than the default one and the performance would be better when \(c\) increases from \(1/2\) to \(1\).

Dependency to \(mn\)It's clear to see that the convergence bounds in Theorem A.1 and Theorem B.1 are free of the curse of the dimension factor \(mn\) as \(mn\) only appears on the denominator in each coefficient. We think that solving the curse of dimension is vital since the applied range for Adafactor includes many deep learning tasks where \(mn\) are comparable large to \(T\).

Dependency to \(_{1},_{2}\)The convergence bounds in (37) and (39) from Theorem B.1 has a dependency of \((_{1}^{-1}(1/_{1}))\) on \(_{1}\).1 Although the polynomial dependency to \(_{1}\) is a bit worse since \(_{1}\) ususally takes a small value in experiments, e.g., the default setting is \(10^{-30}\), it's still common in some theoretical convergence results, e.g., [34; 18]. We also perform some experiments to show that a relatively large \(_{1}\), roughly \(10^{-3}\), makes no observable effect on the performance. Thereby, \(_{1}\) could be regarded as a constant in comparison to \(T\) and the influence brought by \(1/_{1}\) could be somehow acceptable.

Since the default value of \(_{2}\) is \(10^{-3}\) in experiments, it could also be regarded as a constant compared to \(T\). Therefore, the dependency \((1/_{2})\) on \(_{2}\) shows little effect on convergence bounds given the sufficiently large \(T\).

Dependency to the scale of parameters.The convergence bounds in Theorem B.1 contain a \((_{})\) factor where \(_{}\) denotes the maximum values of \(\|_{k}\|_{}\) along the training process. It's reasonable to assume that \(_{} G_{0}\) for a comparable large constant \(G_{0}\) in practice.

## 7 Convergence of Adafactor with update clipping

In this section, we take a closer look on the comprehensive Adafactor with both matrix factorization and update clipping. We slightly change the update clipping threshold \(d\) in Algorithm 1 to a time-varying threshold \(d_{k}\). The step-size \(_{k}\) then becomes

\[_{k}=,(_{k})\}_{k}}{\{1, (_{k})/d_{k}\}}. \]

Then, we present the following convergence bound.

**Theorem 7.1**.: _Let \(\{_{k}\}_{k 1}\) be generated by Algorithm 1 with \(_{k}\) given by (7) for each \(k 1\). If Assumptions (A1)-(A4) hold, and_

\[ d_{1}&=1,_{2,1}=1/2, _{1}=_{0},\\ d_{k}&=k^{},_{2,k}=1 -1/k^{c},_{k}=_{0}/, k 2, \]_for some constants \(>1,1/2 c 1,_{0}>0\), then for any \(T 1,(0,1)\), with probability at least \(1-\),_

\[_{k[T]}\| f(_{k})\|_{F}^{2}( }()).\]

Discussion of Theorem 7.1The convergence result indicates that with a proper selection of the clipping threshold, along with the commonly used step-size \(_{k}\) and decay rate \(_{2,k}\), Adafactor can find a stationary point when \(T\) is sufficiently large. The dependency of convergence bound on \(c\) remains consistent with Theorem 6.1, achieving the optimal order when \(c=1\). In addition, the convergence bound can still avoid the curse of dimension, which is shown in the detailed version Theorem C.1 from the appendix.

The additional hyper-parameter \(\) primarily influences the dependency on \(_{1}\), specifically as \((_{1}^{-}(1/_{1}))\). Thus, our convergence bound may deteriorate as \(\) increases, possibly due to the limitation of our proof framework. This dependency could be potentially improved to \((_{1}^{-1}(1/_{1}))\) when \(mn\) is comparable to \(1/_{1}\), which is practical when implementing a large-size model.1 In our experiments, we tested different values of \(\) and found that suitably small values, such as \(=4,6,7,8\) can lead to performance and training stability comparable to the default setting, even without implementing the warm-up technique. This finding suggests that our new threshold setting plays a similar role in enhancing training stability as the default one, which is also the main motivation of update clipping. Since \(_{1}\) can be set to a relatively large value, e.g., \(10^{-3}\), a dependency like \((_{1}^{-4}(1/_{1}))\) is somewhat acceptable for sufficiently large \(T\).

## 8 Experiments

In this section, we will report our experimental results based on the insights obtained in our theory. We will mainly provide the following three experiments:

* We test Adafactor without update clipping under different decay rate parameters \(c\), aiming to demonstrate performance improvement as \(c\) increases from \(0.5\) to \(1\) with optimal performance at \(c=1\), as indicated in Theorem 6.1 and Theorem 7.1.
* We evaluate the sensitivity of Adafactor to different values of \(_{1}\), particularly showing that a relatively large \(_{1}\) does not significantly impact performance.
* We assess the performance of Adafactor with a time-increasing \(d_{k}\) setting, as described in Theorem 7.1, and compare it to the default constant setting.

### Experiment setup

In all experiments, the initialization is \(_{0}=_{m}\) and \(_{0}=_{n}^{}\). We use a learning rate with the warm-up technique as described in , specifically \(_{k}=\{10^{-6} k,1/\}\) for all experiments unless otherwise specified. The batch size is set to 256, and the total number of epochs is 400 by default. Our models are ResNet-20 and ResNet-110 , and we use the CIFAR-10 and CIFAR-100 datasets  without any data augmentation. The experiments are conducted using the PyTorch implementation of Adafactor on a single NVIDIA GeForce RTX 4090 GPU.

### Report on Experiment 1

We test Adafactor without update clipping using decay rate parameter \(c\) ranging from \(0.5\) to \(1.0\) in increments of \(0.05\), while keeping other hyper-parameters at their default values. Each experiment is run 10 times with 100 epochs, and we plot the average test accuracy and standard deviation (shallow blue region) in Figure 1. The results indicate that \(c=1.0\) yields better performance and stability compared to \(c<1.0\) on different models and datasets, corresponding to the highest test accuracy and thinner shallow blue band. These performances show a noticeable improving trend as \(c\) increases from 0.5 to 1.0, aligning roughly with the results in Theorem 6.1.

### Report on Experiment 2

In the second experiment, we test Adafactor without update clipping under different \(_{1}\) values. We plot the training loss against the step \(t\) on different models and datasets in Figure 2. The performance for \(_{1}=10^{-8}\) and \(_{1}=10^{-5}\) is nearly identical to that for \(_{1}=10^{-30}\). Moreover, even a larger value of \(10^{-3}\) achieves comparable training performance, though with a slower decrease in loss. Notably, \(_{1}=10^{-3}\) requires approximately the same number of steps (\(t 20000\)) as \(_{1}=10^{-30}\) to achieve near-zero training loss. We conclude that Adafactor is not sensitive to the choice of \(_{1}\), and a relatively large \(_{1}\) can still lead to convergence, making the polynomial dependency \((1/_{1})\) in our convergence bounds acceptable.

### Report on Experiment 3

In this experiment, we explore the appropriate values of \(\) in Theorem 7.1 to achieve performance comparable to the default setting of \(d=1\). As indicated by Theorem 7.1, a relatively small \(\) is desirable for better dependency on \(_{1}\). We train models with \(\) set to 4, 6, 7, 8, and 9, keeping other hyper-parameters at their default values. We also train models with the default \(d=1\) setting as the baseline. We plot the training loss against the steps in Figures 3 without step-size warm-up and 4 with step-size warm-up.

Figure 1: Average test accuracy and standard deviation (shallow blue region) under different decay rate parameters \(c\).

Figure 2: Training loss vs. steps using Adafactor without update clipping under different \(_{1}\). The step-size \(_{t}\), decay rate \(_{2,k}\), and learning rate warm-up are set by default.

The results indicate that, for these values of \(\), Adafactor achieves comparable or even better convergence speed compared to the default threshold (represented by "Baseline"). The comparable results to the "Baseline" in Figure 3 further suggest that the time-increasing \(d_{k}\) in Theorem 7.1 plays a role similar to that of the default setting, enhancing training stability even when the step-size warm-up is turned off.

## 9 Conclusions

In this paper, we investigate the convergence behavior of Adafactor on non-convex smooth landscapes, considering bounded stochastic gradients. We introduce a new proxy step-size to decouple the stochastic gradients from the unique adaptive step-size. Additionally, we use new estimations to control the errors introduced by matrix factorization and update clipping in Adafactor.

Our findings reveal that full-batch Adafactor is capable of finding a stationary point, requiring only a step-size \(_{k}(1/)\) and a second moment decay rate \(_{2,k}(0,1)\), denoting a wide range including the default setup. In the case of stochastic Adafactor without update clipping, the convergence rate can achieve the optimal order \(}(1/)\) when \(_{2,k}=1-1/k^{c},c=1\). However, performance deteriorates as \(c\) decreases. This finding is supported by experimental results. We also explore Adafactor with a time-increasing clipping threshold and derive similar convergence results. The empirical results demonstrate that the new clipping threshold provides performance comparable to the default constant setting.

LimitationsThere are several limitations in our work that warrant further investigation. First, the polynomial dependency on \(_{1}\) in our convergence bounds may be further improved to a better dependency, such as \((1/_{1})\). Second, although we provide convergence results for several variants of Adafactor and demonstrate comparable performance to the original one in experiments, the convergence bound for stochastic vanilla Adafactor remains unknown. Finally, our experimental results primarily focus on traditional deep learning tasks due to our GPU limitations. It would be beneficial to test the scalability of our theoretical results, e.g., on large language models.

Figure 4: Training loss vs. steps on different models and datasets. We use step-size with warm-up technique by default and test under different \(\).

Figure 3: Training loss vs. steps on different models and datasets. We use step-size without warm-up technique and test under different \(\).