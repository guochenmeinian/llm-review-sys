# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

different variants of Data Shapley have been proposed , reflecting its effectiveness in quantifying data point contributions to ML model training.

**KNN-Shapley.** While being a principled approach for data valuation, the exact calculation of the Shapley value is computationally prohibitive . Various approximation algorithms for Data Shapley have been proposed , but these approaches still require substantial computational resources due to model retraining. Fortunately, a breakthrough by  showed that computing the _exact_ Data Shapley for K-Nearest Neighbors (KNN), one of the oldest yet still popular ML algorithms, is surprisingly easy and efficient. KNN-Shapley quantifies data value based on KNN's Data Shapley score; it can be applied to large, high-dimensional CV/NLP datasets by calculating the value scores on the last-layer neural network embeddings. Owing to its superior efficiency and effectiveness in discerning data quality, KNN-Shapley is recognized as one of the most practical data valuation techniques nowadays . It has been applied to various ML domains including active learning , continual learning , NLP , and semi-supervised learning .

**Motivation: privacy risks in data valuation.** In this work, we study a critical, yet often overlooked concern in the deployment of data valuation: privacy leakage associated with data value scores released to data holders. The value of a single data point is always relative to other data points in the training set. This, however, can potentially reveal sensitive information about the rest of data holders in the dataset. This problem becomes even more complex when considering a strong threat model where multiple data holders collude, sharing their received data values to determine the membership of a particular individual. As data valuation techniques such as KNN-Shapley become increasingly popular and relevant in various applications, understanding and addressing the privacy challenges of data valuation methods is of utmost importance. In this work, we study this critical issue through the lens of differential privacy (DP) , a de-facto standard for privacy-preserving applications.

Our technical contributions are listed as follows.

**Privacy Risks & Challenges of Privatization for KNN-Shapley** (Section 3). We demonstrate that data value scores (specifically KNN-Shapley) indeed serve as a new channel for private information leakage, potentially exposing sensitive information about individuals in the dataset. In particular, we explicitly design a privacy attack (in Appendix B.2.2) where an adversary could infer the presence/absence of certain data points based on the variations in the KNN-Shapley scores, analogous to the classic membership inference attack on ML model . Additionally, we highlight the technical challenges in incorporating the current KNN-Shapley technique with differential privacy, such as its large global sensitivity. All these results emphasize the need for a new, privacy-friendly approach to data valuation.

**TKNN-Shapley: an efficient, privacy-friendly data valuation technique** (Section 4.2). To address the privacy concerns, we derive a novel variant of KNN-Shapley. This new method considers the Data Shapley of an alternative form of KNN classifier called _Threshold-KNN_ (TKNN) , which takes into account all neighbors within a pre-specified threshold of a test example, rather than the exact \(K\) nearest neighbors. We derive the closed-form formula of Data Shapley for TKNN (i.e., TKNN-Shapley). We show that it can be computed _exactly_ with exact linear-time computational efficiency over the original KNN-Shapley. **DP-TKNN-Shapley** (Section 5). Importantly, we recognize that TKNN-Shapley only depends on three simple counting queries. Hence, TKNN-Shapley can be conveniently transformed into a differentially private version by DP's post-processing property. Moreover, we prove that such a DP variant satisfies several favorable properties, including (1) the high computational efficiency, (2) the capability to withstand collusion among data holders without compromising the privacy guarantee, and (3) the ease of integrating subsampling for privacy amplification.

**Numerical experiments** (Section 6). We evaluate the performance of TKNN-Shapley across 11 commonly used benchmark datasets and 2 NLP datasets. Key observations include: (1) TKNN-Shapley surpasses KNN-Shapley in terms of computational efficiency; (2) DP-TKNN-Shapley significantly outperforms the naively privatized KNN-Shapley in terms of privacy-utility tradeoff in discerning data quality; (3) even non-private TKNN-Shapley achieves comparable performance as KNN-Shapley.

Overall, our work suggests that TKNN-Shapley, being a privacy-friendly, yet more efficient and effective alternative to the original KNN-Shapley, signifies a milestone toward practical data valuation.

Background of Data Valuation

In this section, we formalize the data valuation problem for ML, and review the method of Data Shapley and KNN-Shapley.

**Setup & Goal.** Consider a dataset \(D:=\{z_{i}\}_{i=1}^{N}\) consisting of \(N\) data points where each data point \(z_{i}:=(x_{i},y_{i})\) is collected from a data owner \(i\). The objective of data valuation is to attribute a score to each training data point \(z_{i}\), reflecting its importance or quality in ML model training. Formally, we aim to determine a score vector \((_{z_{i}})_{i=1}^{N}\), wherein \(_{z_{i}}\) represents the value of data point \(z_{i}\). For any reasonable data valuation method, the value of a data point is always relative to other data points in the dataset. For instance, if a data point has many duplicates in the dataset, its value will likely be lower. Hence, \(_{z_{i}}\) is a function of the leave-one-out dataset \(D_{-z_{i}}:=D\{z_{i}\}\). We write \(_{z_{i}}(D_{-z_{i}})\) when we want to stress the dependency of a data value score with the rest of the data points.

**Utility Function.** Most of the existing data valuation techniques are centered on the concept of _utility function_, which maps an input dataset to a score indicating the usefulness of the training set. A common choice for utility function is the _validation accuracy_ of a model trained on the input training set. Formally, for a training set \(S\), a utility function \(v(S):=((S))\), where \(\) is a learning algorithm that takes a dataset \(S\) as input and returns a model; \(()\) is a metric function that evaluates the performance of a given model, e.g., the classification accuracy on a hold-out validation set.

### The Shapley Value

The Shapley value (SV)  is a classic concept from game theory to attribute the total gains generated by the coalition of all players. At a high level, it appraises each point based on the (weighted) average utility change caused by adding the point into different subsets of the training set. Formally, given a utility function \(v()\) and a training set \(D\), the Shapley value of a data point \(z D\) is defined as

\[_{z}(D_{-z};v):=_{k=1}^{N}^{-1} _{S D_{-z},|S|=k-1}[v(S\{z\})-v(S)] \]

For notation simplicity, when the context is clear, we omit the utility function and/or leave-one-out dataset, and write \(_{z}(D_{-z})\), \(_{z}(v)\) or \(_{z}\) depending on the specific dependency we want to stress.

The popularity of the Shapley value is attributable to the fact that it is the _unique_ data value notion satisfying four axioms: Dummy player, Symmetry, Linearity, and Efficiency. We refer the readers to ,  and the references therein for a detailed discussion about the interpretation and necessity of the four axioms in the ML context. Here, we introduce the _linearity_ axiom which will be used later.

**Theorem 1** (Linearity of the Shapley value ).: _For any of two utility functions \(v_{1},v_{2}\) and any \(_{1},_{2}\), we have \(_{z}(_{1}v_{1}+_{2}v_{2})=_{1}_{z}( v_{1})+_{2}_{z}(v_{2})\)._

### KNN-Shapley

Formula (1) suggests that the exact Shapley value can be computationally prohibitive in general, as it requires evaluating \(v(S)\) for all possible subsets \(S D\). Surprisingly, [29; 61] showed that for \(K\)-Nearest Neighbor (KNN), the computation of the exact Data Shapley score is highly efficient. Following its introduction, KNN-Shapley has rapidly gained attention and follow-up works across diverse areas of machine learning [24; 43; 42; 9]. In particular, it has been recognized by recent studies as "the most practical data valuation technique capable of handling large-scale data effectively" [52; 33].

Specifically, the performance of an unweighted KNN classifier is typically evaluated by its validation accuracy. For a given validation set \(D^{()}=\{z_{i}^{()}\}_{i=1}^{N_{}}\), we can define KNN's utility function \(v_{D^{()}}^{}\) on a non-empty training set \(S\) as \(v_{D^{()}}^{}(S):=_{z^{()} D^{( )}}v_{z^{()}}^{}(S)\), where

\[v_{z^{()}}^{}(S):=_{j=1}^{ (K,|S|)}[y_{^{(S)}(j;x^{()})}=y^{()}] \]

is the probability of a (soft-label) KNN classifier in predicting the correct label for a validation point \(z^{()}=(x^{()},y^{()}) D^{()}\). In (2), \(^{(S)}(i;x^{()})\) is the index of the \(i\)th closest data point in \(S\) to \(x^{()}\). When \(|S|=0\), \(v^{}_{z_{z()}}(S)\) is set to the accuracy by random guessing. The distance is measured through suitable metrics such as \(_{2}\) distance. Here is the main result in [29; 61]:

**Theorem 2** (KNN-Shapley [30; 61] (simplified version)).: _Consider the utility function in (2). Given a validation data point \(z^{()}=(x^{()},y^{()})\) and a distance metric \(d(,)\), if we sort the training set \(D=\{z_{i}=(x_{i},y_{i})\}_{i=1}^{N}\) according to \(d(x_{i},x^{()})\) in ascending order, then the Shapley value of each data point \(^{}_{z_{i}}\) corresponding to utility function \(v^{}_{z^{()}}\) can be computed recursively as follows:_

\[^{}_{z_{N}}:=f_{N}(D),\ \ \ \ ^{}_{z_{i}}:= ^{}_{z_{i+1}}+f_{i}(D)\ \ i=1,,N-1\]

_where the exact form of functions \(f_{i}(D)\) can be found in Appendix B.1._

**Runtime:** _the computation of all Shapley values \((^{}_{z_{1}},,^{}_{z_{N}})\) can be achieved in \(O(N N)\) runtime in total (dominated by the sorting data points in \(D\))._

**The Shapley value corresponding to full validation set:** _for a validation set \(D^{()}\), recall that \(v^{}_{D^{()}}(S):=_{z^{()} D^{( )}}v^{}_{z^{()}}(S)\). One can compute the Shapley value corresponding to \(v^{}_{D^{()}}\) by summing each \(^{}_{z_{i}}(v^{}_{z^{()}})\), i.e., \(^{}_{z_{i}}(v^{}_{D^{()}})= _{z^{()} D^{()}}^{}_{z_{i}}( v^{}_{z^{()}})\)._

Theorem 2 tells that for any validation data point \(z^{()}\), we can compute the _exact_ Shapley value \(^{}_{z_{i}}(D_{-z_{i}};v^{}_{z^{() }})\) for _all_\(z_{i} D\) by using a recursive formula within a total runtime of \(O(N N)\). After computing the Shapley value \(^{}_{z_{i}}(v^{}_{z^{()}})\) for each \(z^{()} D^{()}\), one can compute the Shapley value corresponding to the full validation set by simply taking the sum of each \(^{}_{z_{i}}(v^{}_{z^{()}})\), due to the linearity property (Theorem 1) of the Shapley value.

**Remark 1** (Criteria of Computational Efficiency).: _As prior data valuation literature , we focus on the total runtime required to release all data value scores \((^{}_{z_{1}},,^{}_{z_{N}})\). This is because, in a practical data valuation scenario (e.g., profit allocation), we rarely want to compute the data value score for only a single data point; instead, a typical objective is to compute the data value scores for all data points within the training set._

## 3 Privacy Risks & Challenges of Privatization for KNN-Shapley

**Scenario.** Figure 1 illustrates the data valuation scenario and potential privacy leakages considered in our paper. Specifically, a centralized, trusted server collects data point \(z_{i}\) from data owner \(i\) for each \(i[N]\). The central server's role is to provide each data owner \(i\) with an assessment of the value of their data \(z_{i}\), e.g., the KNN-Shapley value \(^{}_{z_{i}}\). **A real life example:** Mayo Clinic has created a massive digital health patient data marketplace platform , where the patients submit part of their medical records onto the platform, and life science companies/labs pay a certain amount of money to purchase patients' data. The platform's responsibility is to gauge the worth of the data of each patient (i.e., the data owner) to facilitate fair compensation.

Figure 1: The potential privacy risks in data valuation arise from the dependency of the data value score \(_{z_{i}}\) on the rest of the dataset \(D_{-z_{i}}\). Our goal is to privatize \(_{z_{i}}(D_{-z_{i}})\) such that it provides strong differential privacy guarantee for the rest of the dataset \(D_{-z_{i}}\).

The privacy risks associated with KNN-Shapley (as well as other data valuation techniques) arise from the fact that \(_{z_{i}}^{}}(D_{-z_{i}})\) depends on other data owners' data \(D_{-z_{i}}\). Consequently, the data value score \(_{z_{i}}^{}}\) may inadvertently reveal private information (e.g., membership) about the rest of the dataset. The dependency of a data value score on the rest of the dataset is an unavoidable aspect of data valuation, as the value of a data point is inherently a relative quantity determined by its role within the complete dataset.

**Remark 2** (Other privacy risks in data valuation).: _It is important to note that in this work, we do not consider the privacy risks of revealing individuals' data to the central server. This is a different type of privacy risk that needs to be addressed using secure multi-party computation (MPC) technique , and it should be used together with differential privacy in practice. In addition, to use KNN-Shapley or many other data valuation techniques, the central server needs to maintain a clean, representative validation set, the privacy of which is not considered by this paper._

**A Simple Membership Inference (MI) Attack on KNN-Shapley (detailed in Appendix B.2.2).** We further illustrate the privacy risks of revealing data value scores with a concrete example. Analogous to the classic membership inference attack on ML model , in Appendix B.2.2 we show an example of privacy attack where an adversary could infer the presence/absence of certain data points in the dataset based on the variations in the KNN-Shapley scores. The design is analogous to the membership inference attack against ML models via the _likelihood ratio test_. The AUROC score of the attack results is shown in Table 1. As we can see, our MIA attack can achieve a detection performance that is better than the random guess (0.5) for most of the settings. On some datasets, the attack performance can achieve \(>0.7\) AUROC. This demonstrates that privacy leakage in data value scores can indeed lead to non-trivial privacy attacks, and underscores the need for privacy safeguards in data valuation.1

### Privatizing KNN-Shapley is Difficult

The growing popularity of data valuation techniques, particularly KNN-Shapley, underscores the critical need to mitigate the inherent privacy risks. In the quest for privacy protection, differential privacy (DP)  has emerged as the leading framework. DP has gained considerable recognition for providing robust, quantifiable privacy guarantees, thereby becoming the de-facto choice in privacy protection. In this section, we introduce the background of DP and highlight the technical difficulties in constructing a differentially private variant for the current version of KNN-Shapley.

**Background of Differential Privacy.** We use \(D,D^{}^{}\) to denote two datasets with an unspecified size over space \(\). We call two datasets \(D\) and \(D^{}\)_adjacent_ (denoted as \(D D^{}\)) if we can construct one by adding/removing one data point from the other, e.g., \(D=D^{}\{z\}\) for some \(z\).

**Definition 3** (Differential Privacy ).: _For \(, 0\), a randomized algorithm \(:^{}\) is \((,)\)-differentially private if for every pair of adjacent datasets \(D D^{}\) and for every subset of possible outputs \(E\), we have \(_{}[(D) E] e^{}_{}[ (D^{}) E]+\)._

That is, \((,)\)-DP requires that for all adjacent datasets \(D,D^{}\), the output distribution \((D)\) and \((D^{})\) are close, where the closeness is measured by the parameters \(\) and \(\). In our scenario, we would like to modify data valuation function \(_{z_{i}}(D_{-z_{i}})\) to satisfy \((,)\)-DP in order to protect the privacy of the rest of the dataset \(D_{-z_{i}}\). Gaussian mechanism  is a common way for privatizing a function; it introduces Gaussian noise aligned with the function's _global sensitivity_, which is the maximum output change when a single data point is added/removed from any given dataset.

**Definition 4** (Gaussian Mechanism ).: _Define the global sensitivity of a function \(f:^{}^{d}\) as \(_{2}(f):=_{D D^{}}\|f(D)-f(D^{})\|_{2}\). The Gaussian mechanism \(\) with noise level \(\) is then given by \((D):=f(D)+(0,^{2}_{d})\) where \(\) is \((,)\)-DP with \(=_{2}(f)/\)._

    & \(K=1\) & \(K=3\) & \(K=5\) & \(K=7\) & \(K=9\) & \(K=11\) & \(K=13\) & \(K=15\) \\ 
**Diffances** & 0.56 & 0.955 & 0.518 & 0.52 & 0.57 & 0.55 & 0.515 & 0.6 \\
**Parameters** & 0.674 & 0.513 & 0.565 & 0.505 & 0.588 & 0.512 & 0.502 \\
**CPU** & 0.765 & 0.548 & 0.520 & 0.572 & 0.588 & 0.512 & 0.612 & 0.615 \\
**Front** & 0.625 & 0.654 & 0.6 & 0.590 & 0.622 & 0.532 & 0.538 & 0.558 \\
**Celemental** & 0.542 & 0.683 & 0.626 & 0.665 & 0.603 & 0.607 & 0.602 & 0.60 \\
**Avginal** & 0.522 & 0.595 & 0.625 & 0.65 & 0.53 & 0.645 & 0.532 & 0.57 \\
**Cele** & 0.61 & 0.552 & 0.505 & 0.538 & 0.558 & 0.588 & 0.562 & 0.618 \\
**Wish** & 0.576 & 0.518 & 0.538 & 0.538 & 0.558 & 0.562 & 0.505 & 0.577 \\ 
**Wish** & 0.575 & 0.505 & 0.561 & 0.564 & 0.577 & 0.522 & 0.505 & 0.572 \\   

Table 1: Results of the AUROC of the MI attack proposed in Appendix B.2.2 on KNN-Shapley. The higher the AUROC score is, the larger the privacy leakage is. The detailed algorithm description and experiment settings can be found in Appendix B.2.2.

**Challenges in making KNN-Shapley being differentially private (overview).** Here, we give an overview of the inherent difficulties in making the KNN-Shapley \(_{z_{i}}^{}}}(D_{-z_{i}})\) (Theorem 2) to be differentially private, and we provide a more detailed discussion in Appendix B.3. **(1) Large global sensitivity:** In Appendix B.3, we show that the global sensitivity of \(_{z_{i}}^{}}}(D_{-z_{i}})\) can significantly exceed the magnitude of \(_{z_{i}}^{}}}\). Moreover, we prove that the global sensitivity bound cannot be further improved by constructing a specific pair of neighboring datasets that matches the bound. Hence, if we introduce random noise proportional to the global sensitivity bound, the resulting privatized data value score could substantially deviate from its non-private counterpart, thereby compromising the utility of the privatized data value scores. **(2) Computational challenges in incorporating privacy amplification by subsampling:** "Privacy amplification by subsampling"  is a technique where the subsampling of a dataset amplifies the privacy guarantees due to the reduced probability of an individual's data being included. Being able to incorporate such a technique is often important for achieving a decent privacy-utility tradeoff. However, in Appendix B.3 we show that the recursive nature of KNN-Shapley computation causes a significant increase in computational demand compared to non-private KNN-Shapley.

These challenges underscore the pressing need for new data valuation techniques that retain the efficacy and computational efficiency of KNN-Shapley, while being amenable to privatization.

## 4 The Shapley Value for Threshold-based Nearest Neighbor

Considering the privacy concerns and privatization challenges associated with the original KNN-Shapley method, we introduce _TKNN-Shapley_, a privacy-friendly alternative of KNN-Shapley which also achieves improved computational efficiency. At the core of this novel method is Threshold-KNN (TKNN) classifier, a simple variant of the KNN classifier. We will discuss how to incorporate DP for TKNN-Shapley in Section 5.

### Threshold-based Nearest Neighbor Classifier (TKNN)

Threshold-KNN (TKNN) [5; 72] is a variant of KNN classifier that considers neighbors within a pre-specified threshold of the query example, rather than exclusively focusing on the exact \(K\) nearest neighbors. Formally, for a training set \(S\) and a validation data point \(z^{()}=(x^{()},y^{()})\), we denote \(_{x^{()},}(S):=\{(x,y)|(x,y) S,d(x,x^{() })\}\) the set of neighbors of \(x^{()}\) in \(S\) within a pre-specified threshold \(\). Similar to the utility function for KNN, we define the utility function for TKNN classifier when using a validation set \(D^{()}\) as the aggregated prediction accuracy \(v_{D^{()}}^{}(S):=_{z^{()} D^{()}}v_{z^{()}}^{}}}(S)\) where

\[v_{z^{()}}^{}}}(S):=& |_{x^{()},}(S)|=0\\ _{x^{()},}(S)|}_{(x,y)_{x ^{()},}(S)}[y=y^{()}]&|_{x^{( )},}(S)|>0 \]

where \(\) can be the trivial accuracy of random guess.

**Comparison with standard KNN. (1) Robustness to outliers.** Compared with KNN, TKNN is better equipped to deal with prediction phase outliers . When predicting an outlier that is far from the entire training set, TKNN prevents the influence of distant, potentially irrelevant neighbors, leading to a more reliable prediction score for outliers. **(2) Inference Efficiency.** TKNN has slightly better computational efficiency compared to KNN, as it has \(O(N)\) instead of \(O(N N)\) inference time. This improvement is achieved because TKNN only requires the computation of neighbors within the threshold \(\), rather than searching for the exact \(K\) nearest neighbors. **(3) TKNN is also a consistent estimator.** The consistency of standard KNN is a well-known theoretical result . That is, for any target function that satisfies certain regularity conditions, KNN binary classifier/regressor is guaranteed to converge to the target function as the size of the training set grows to infinite. In Appendix C.1, we derived a similar consistency result for TKNN binary classifier/regressor.

**Remark 3** (Intuition: Why we consider TKNN?).: _The'recursive form' of KNN-Shapley is due to the sorting operation in the prediction of the standard KNN. The recursive form of the formula causes difficulties in incorporating KNN-Shapley with differential privacy. In contrast, TKNN avoids the recursive formula for its Data Shapley value; the intuition is that for TKNN, the selection of neighbors for prediction solely depends on the queried example and the validation data, and is independent of the other training data points._

### Data Shapley for TKNN (TKNN-Shapley)

With the introduction of TKNN classifier and its utility function, we now present our main result, the closed-form, efficiently computable Data Shapley formula for the TKNN classifier.

**Theorem 5** (TKNN-Shapley (simplified version)).: _Consider the utility function \(v_{z^{}}^{}\) in (3). Given a validation data point \(z^{}=(x^{},y^{})\) and a distance metric \(d(,)\), the Shapley value \(_{z_{i}}^{}\) of each training point \(z_{i}=(x_{i},y_{i}) D\) corresponding to utility function \(v_{z^{}}^{}\) can be calculated as_

\[_{z_{i}}^{}=f(_{z^{}}(D_{-z_{i }})) \]

_where \(_{z^{}}:=(,_{x^{}, },_{z^{},}^{(+)})\) is a 3-dimensional function/vector s.t._

\[=(D_{-z_{i}}):=|D_{-z_{i}}| (D_{-z_{i}})\] \[_{x^{},}=_{x^{},}(D_{-z_{i}}):=1+|_{^{},}(D_ {-z_{i}})| (1+x^{}D_{-z_{i}})\] \[_{z^{},}^{(+)}=_{z^{},}^{(+)}(D_{-z_{i}}):=_{(x,y)_{^{ { val}},}(D_{-z_{i}})}[y=y^{}] (D_{-z_{i}})\]

_and \(f()\) is a function whose exact form can be found in Appendix C.2._

**Runtime:**_the computation of all \((_{z_{1}}^{},,_{z_{N}}^{})\) can be achieved in \(O(N)\) runtime (see Appendix C.2.1)._

**The Shapley value when using full validation set:**_The Shapley value corresponding to the utility function \(v_{D^{}}^{}\) can be calculated as \(_{z_{i}}^{}(v_{D^{}}^{ })=_{z^{} D()}_{z_{i}}^{}(v_{z^{}}^{})\)._

The main technical challenges of proving Theorem 5 lies in showing \(_{z^{}}=(,_{x^{}, },_{z^{},}^{(+)})\), three simple counting queries on \(D_{-z_{i}}\), are the key quantities for computing \(_{z_{i}}^{}\). In later part of the paper, we will often view \(_{z_{i}}^{}\) as a function of \(_{z^{}}\) and denote \(_{z_{i}}^{}[_{z^{}}]:= f(_{z^{}})\) when we want to stress this dependency. The full version and the proof for TKNN-Shapley can be found in Appendix C.2.1. Here, we briefly discuss why TKNN-Shapley can achieve \(O(N)\) runtime _in total_.

**Efficient Computation of TKNN-Shapley.** As we can see, all of the quantities in \(_{z^{}}\) are simply counting queries on \(D_{-z_{i}}\), and hence \(_{z^{}}(D_{-z_{i}})\) can be computed in \(O(N)\) runtime for any \(z_{i} D\). A more efficient way to compute \(_{z^{}}(D_{-z_{i}})\) for _all_\(z_{i} D\), however, is to first compute \(_{z^{}}(D)\) on the full dataset \(D\), and we can easily show that each of \(_{z^{}}(D_{-z_{i}})\) can be computed from that in \(O(1)\) runtime (see Appendix C.2.2 for details). Hence, we can compute TKNN-Shapley \(_{z_{i}}^{}(v_{z^{}}^{})\) for _all_\(z_{i} D\) within an overall computational cost of \(O(N)\).

**Comparison with KNN-Shapley.** TKNN-Shapley offers several advantages over the original KNN-Shapley. **(1) Non-recursive:** In contrast to the KNN-Shapley formula (Theorem 2), which is recursive, TKNN-Shapley has an explicit formula for computing the Shapley value of every point \(z_{i}\). This non-recursive nature not only simplifies the implementation, but also makes it straightforward to incorporate techniques like subsampling. **(2) Computational efficiency:** TKNN-Shapley has \(O(N)\) runtime in total, which is better than the \(O(N N)\) runtime for KNN-Shapley.

## 5 Differentially Private TKNN-Shapley

Having established TKNN-Shapley as an alternative to KNN-Shapley which shares many advantages, our next step is to develop a new version of TKNN-Shapley that incorporates differential privacy. In this section, we introduce our proposed Differentially Private TKNN-Shapley (DP-TKNN-Shapley).

Differentially private release of \(_{z_{i}}^{}(v_{z^{}}^{})\)

As \(_{z^{}}=(,_{x^{}, },_{z^{},}^{(+)})\) are the only quantities that depend on the dataset \(D_{-z_{i}}\) in (4), privatizing these quantities is sufficient for overall privacy preservation, as DP guarantee is preserved under any post-processing . Since all of \(,_{x^{},}\) and \(_{z^{},}^{(+)}\) are just counting queries, the function of \(_{z^{}}()\) has global sensitivity \(\) in \(_{2}\)-norm. This allows us to apply the commonly used Gaussian mechanism (Theorem 4) to release \(_{z^{}}\) in a differentially private way, i.e., \(}_{z^{}}(D_{-z_{i}}):=( _{z^{}}(D_{-z_{i}})+(0,^{2} _{3}))\) where \(}_{z^{}}\) is the privatized version of \(_{z^{}}\), and the function round rounds each entry of noisy value to the nearest integer. The differentially private version of TKNN-Shapley value \(_{z_{i}}^{}(D_{-z_{i}})\) is simply the value score computed by(4) but use the privatized quantities \(}_{z^{}}\). The privacy guarantee of releasing DP-TKNN-Shapley \(_{z_{i}}^{}(D_{-z_{i}})\) follows from the guarantee of Gaussian mechanism (see Appendix D.1 for proof).

**Theorem 6**.: _For any \(z^{} D^{}\), releasing \(_{z_{i}}^{}(v_{z^{}}^{}):= _{z_{i}}^{}[}_{z^{}}(D_{- z_{i}})]\) to data owner \(i\) is \((,)\)-DP with \(=/\)._

While our approach may seem simple, we stress that simplicity is appreciated in DP as complex mechanisms pose challenges for correct implementation and auditing [47; 21].

### Advantages of DP-TKNN-Shapley (Overview)

We give an overview (detailed in Appendix D.2) of several advantages of DP-KNN-Shapley here, including the efficiency, collusion resistance, and simplicity in incorporating subsampling.

**By reusing privatized statistics, \(_{z_{i}}^{}\) can be efficiently computed for all \(z_{i} D\).** Recall that in practical data valuation scenarios, it is often desirable to compute the data value scores for _all of \(z_{i} D\)_. As we detailed in Section 4.2, for TKNN-Shapley, such a computation only requires \(O(N)\) runtime in total if we first compute \(_{z^{}}(D)\) and subsequently \(_{z^{}}(D_{-z_{i}})\) for each \(z_{i} D\). In a similar vein, we can efficiently compute the DP variant \(_{z_{i}}^{}\) for _all_\(z_{i} D\). To do so, we first calculate \(}_{z^{}}(D)\) and then, for each \(z_{i} D\), we compute \(}_{z^{}}(D_{-z_{i}})\). It is important to note that when releasing \(_{z_{i}}^{}\) to individual \(i\), the data point they hold, \(z_{i}\), is not private to the individual themselves. Therefore, as long as \(}_{z^{}}(D)\) is privatized, \(}_{z^{}}(D_{-z_{i}})\) and hence \(_{z_{i}}^{}\) also satisfy the same privacy guarantee due to DP's post-processing property.

**By reusing privatized statistics, DP-TKNN-Shapley is collusion resistance.** In Section 5.1, we consider the single Shapley value \(_{z_{i}}^{}(D_{-z_{i}})\) as the function to privatize. However, when we consider the release of all Shapley values \((_{z_{i}}^{},,_{z_{N}}^{ {TKNN}})\) as a unified mechanism, one can show that such a mechanism satisfies _joint differential privacy_ (JDP)  if we reuse the privatized statistic \(}_{z^{}}(D)\) for the release of all \(_{z_{i}}^{}\). The consequence of satisfying JDP is that our mechanism is resilient against collusion among groups of individuals without any privacy degradation. That is, even if an arbitrary group of individuals in \([N] i\) colludes (i.e., shares their respective \(_{z_{i}}^{}\) values within the group), the privacy of individual \(i\) remains uncompromised. Our method also stands resilient in scenarios where a powerful adversary sends multiple data points and receives multiple value scores.

**Incorporating Privacy Amplification by Subsampling.** In contrast to KNN-Shapley, the non-recursive nature of DP-TKNN-Shapley allows for the straightforward incorporation of subsampling. Besides the privacy guarantee, subsampling also significantly boosts the computational efficiency of DP-TKNN-Shapley.

Differentially private release of \(_{z_{i}}^{}(v_{D^{}}^{})\)

Recall that the TKNN-Shapley corresponding to the full validation set \(D^{}\) is \(_{z_{i}}^{}(v_{D^{}}^{})= _{z^{} D^{}_{z_{i}}^{} }(D_{-z_{i}};z^{})\). We can compute privatized \(_{z_{i}}^{}(v_{D^{}}^{})\) by simply releasing \(_{z_{i}}^{}(D_{-z_{i}};z^{})\) for all \(z^{} D^{}\). To better keep track of the privacy cost, we use the current state-of-the-art privacy accounting technique based on the notion of the Privacy Loss Random Variable (PRV) . We provide more details about the background of privacy accounting in Appendix D.3. We note that PRV-based privacy accountant computes the privacy loss numerically, and hence the final privacy loss has no closed-form expressions.

## 6 Numerical Experiments

In this section, we systematically evaluate the practical effectiveness of our proposed TKNN-Shapley method. Our evaluation aims to demonstrate the following points: **(1)** TKNN-Shapley offers _improved runtime efficiency_ compared with KNN-Shapley. **(2)** The differentially private version of TKNN-Shapley (DP-TKNN-Shapley) achieves _significantly better privacy-utility tradeoff compared to naively privatized KNN-Shapley in discerning data quality. **(3)** Non-private TKNN-Shapley maintains a _comparable performance_ to the original KNN-Shapley. These observations highlight TKNN-Shapley's potential for data valuation in real-life applications. Detailed settings for our experiments are provided in Appendix E.

**Remark 4**.: _Given that differential privacy offers a provable guarantee against any potential adversaries, our experiments prioritize evaluating the utility of DP-TKNN-Shapley rather than its privacy properties. However, for readers interested in understanding the efficacy of DP in safeguarding training data, we have included an evaluation of the proposed MI attack on DP-TKNN-Shapley in Appendix B.2.2, where it shows a significant drop in attack performance compared to non-DP version._

### Computational Efficiency

We evaluate the runtime efficiency of TKNN-Shapley in comparison to KNN-Shapley (see Appendix E.2 for details as well as more experiments). We choose a range of training data sizes \(N\), and compare the runtime of both methods at each \(N\). As demonstrated in Figure 2, TKNN-Shapley achieves better computational efficiency than KNN-Shapley across all training data sizes. In particular, TKNN-Shapley is around 30% faster than KNN-Shapley for large \(N\). This shows the computational advantage of TKNN-Shapley mentioned in Section 4.2.

### Discerning Data Quality

In this section, we evaluate the performance of both DP and non-DP version of TKNN-Shapley in discerning data quality. **Tasks:** We evaluate the performance of TKNN-Shapley on two standard tasks: mislabeled data detection and noisy data detection, which are tasks that are often used for evaluating the performance of data valuation techniques in prior works . Since mislabeled/noisy data often negatively affect the model performance, it is desirable to assign low values to these data points. In the experiment of mislabeled (or noisy) data detection, we randomly choose 10% of the data points and flip their labels (or add strong noise to their features). **Datasets:** We conduct our experiments on a diverse set of 13 datasets, where 11 of them have been used in previous data valuation studies . Additionally, we experiment on 2 NLP datasets (AG News  and DBPedia ) that have been rarely used in the past due to their high-dimensional nature and the significant computational resources required. **Settings & Hyperparameters of TKNN-/KNN-Shapley:** for both TKNN/KNN-Shapley, we use the popular cosine distance as the distance measure , which is always bounded in \([-1,+1]\). Throughout all experiments, we use \(=-0.5\) and \(K=5\) for TKNN-/KNN-Shapley, respectively, as we found the two choices consistently work well across all datasets. We conduct ablation studies on the choice of hyperparameters in Appendix E.

#### 6.2.1 Experiment for Private Setting

**Baselines: (1) DP-KNN-Shapley without subsampling.** Recall from Section 3.1, the original KNN-Shapley has a large global sensitivity. Nevertheless, we can still use Gaussian mechanism to privatize it based on its global sensitivity bound. We call this approach as DP-KNN-Shapley. **(2) DP-KNN-Shapley with subsampling.** Recall from Section 3.1, it is computationally expensive to incorporate subsampling techniques for DP-KNN-Shapley (detailed in Appendix B.3.2). For instance, subsampled DP-KNN-Shapley with subsampling rate \(q=0.01\) generally takes \(\)**time** compared with non-subsampled counterpart. Nevertheless, we still compare with subsampled DP-KNN-Shapley for completeness. These two baselines are detailed in Appendix B.4. We also note that an unpublished manuscript  proposed a DP version of Data Shapley. However, their DP guarantee requires a hard-to-verify assumption of uniform stability (see Appendix A). Hence, we do not compare with .

**Results:** We evaluate the privacy-utility tradeoff of DP-TKNN-Shapley. Specifically, for a fixed \(\), we examine the AUROC of mislabeled/noisy data detection tasks at different values of \(\), where \(\) is adjusted by changing the magnitude of Gaussian noise. In the experiment, we set the subsampling rate \(q=0.01\) for TKNN-Shapley and subsampled KNN-Shapley. Table 2 shows the results on 3 datasets, and we defer the results for the rest of 10 datasets to Appendix E.3.2. As we can see, **DP-TKNN-Shapley shows a substantially better privacy-utility tradeoff compared to both DP-KNN-Shapley with/without subsampling.** In particular, DP-TKNN-Shapley maintains a high

Figure 2: Runtime comparison of TKNN-Shapley and KNN-Shapley across various training data sizes \(N\). The plot shows the average runtime based on 5 independent runs. Experiments were conducted with an AMD 64-Core CPU Processor.

[MISSING_PAGE_FAIL:10]