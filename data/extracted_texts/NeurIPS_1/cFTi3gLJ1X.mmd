# Depth Anything V2

Lihe Yang\({}^{1}\)  Bingyi Kang\({}^{2}\)  Zilong Huang\({}^{2}\)

Zhen Zhao  Xiaogang Xu  Jiashi Feng\({}^{2}\)  Hengshuang Zhao\({}^{1}\)

\({}^{1}\)HKU \({}^{2}\)TikTok

\({}^{}\)project lead \({}^{}\)corresponding author

[https://depth-anything-v2.github.io](https://depth-anything-v2.github.io)

###### Abstract

This work presents _Depth Anything V2_. Without pursuing fancy techniques, we aim to reveal crucial findings to pave the way towards building a powerful monocular depth estimation model. Notably, compared with V1 , this version produces much finer and more robust depth predictions through three key practices: 1) replacing all labeled real images with synthetic images, 2) scaling up the capacity of our teacher model, and 3) teaching student models via the bridge of large-scale pseudo-labeled real images. Compared with the latest models  built on Stable Diffusion, our models are significantly more efficient (more than \(10\) faster) and more accurate. We offer models of different scales (ranging from 25M to 1.3B params) to support extensive scenarios. Benefiting from their strong generalization capability, we fine-tune them with metric depth labels to obtain our metric depth models. In addition to our models, considering the limited diversity and frequent noise in current test sets, we construct a versatile evaluation benchmark with precise annotations and diverse scenes to facilitate future research.

Work done during an internship at TikTok.

Figure 1: Depth Anything V2 significantly outperforms V1  in robustness and fine-grained details. Compared with SD-based models , it enjoys faster inference speed, fewer parameters, and higher depth accuracy.

## 1 Introduction

Monocular depth estimation (MDE) is gaining increasing attention, due to its fundamental role in widespread downstream tasks. Precise depth information is not only favorable in classical applications, such as 3D reconstruction [47; 32; 93], navigation , and autonomous driving , but is also preferable in modern scenarios, _e.g._, AI-generated content, including images , videos , and 3D scenes [87; 64; 68]. Therefore, there have been numerous MDE models [56; 7; 6; 95; 26; 38; 31; 89; 88; 25; 20; 52; 28] emerging recently, which are all capable of addressing open-world images.

From the aspect of model architecture, these works can be divided into two groups. One group [7; 6; 89; 28] is based on discriminative models, _e.g._, BEiT  and DINOv2 , while the other [31; 20; 25] is based on generative models, _e.g._, Stable Diffusion (SD) . In Figure 2, we compare two representative works from the two categories respectively: Depth Anything  as a discriminative model and Marigold  as a generative model. It can be easily observed that Marigold is superior in modeling the details, while Depth Anything produces more robust predictions for complex scenes. Moreover, as summarized in Table 1, Depth Anything is more efficient and lightweight than Marigold, with different scales to choose from. Meantime, however, Depth Anything is vulnerable to transparent objects and reflections, which are the strengths of Marigold.

In this work, taking all these factors into account, we aim to build a more capable foundation model for monocular depth estimation that can achieve all the strengths listed in Table 1:

* produce robust predictions for complex scenes, including but not limited to complex layouts, transparent objects (_e.g._, glass), reflective surfaces (_e.g._, mirrors, screens) , _etc._
* contain fine details (comparable to the details of Marigold) in the predicted depth maps, including but not limited to thin objects (_e.g._, chair legs) , small holes, _etc._
* provide varied model scales and inference efficiency to support extensive applications .
* be generalizable enough to be transferred (_i.e._, fine-tuned) to downstream tasks, _e.g._, Depth Anything V1 serves as the pre-trained model for all the leading teams in the 3rd MDEC1.

Since the nature of MDE is a discriminative task, we start from Depth Anything V1 , aiming to maintain its strengths and rectify its weaknesses. Intriguingly, we will demonstrate that, to achieve

    & Fine & Transparent &  & Complex &  &  \\  & Detail & Objects & & & Scenes & \\  Marigold  & ✓ & ✓ & ✓ & ✗ & ✗ & ✗ \\ Depth Anything V1  & ✗ & ✗ & ✗ & ✓ & ✓ & ✓ \\  Depth Anything V2 (Ours) & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Preferable properties of a powerful monocular depth estimation model.

Figure 2: _Robustness_ (1st row, the misleading room layout) of Depth Anything V1 and _Fine-grained detail_ (2nd row, the thin basketball net) of Marigold.

such a challenging goal, no fancy or sophisticated techniques need to be developed. The most critical part is still _data_. It is indeed the same as the data-driven motivation of V1, which harnesses large-scale unlabeled data to speed up data scaling-up and increase the data coverage. In this work, we instead will first revisit its _labeled data_ design, and then highlight the key role of unlabeled data.

We first present three key findings below. We will clarify them in detail in the following three sections.

**Q1 [Section 2]:**_Whether the coarse depth of MiDaS or Depth Anything come from the discriminative modeling itself? Is it a must to adopt the heavy diffusion-based modeling manner for fine details?_

**A1:** No, efficient discriminative models can also produce extremely fine details. The most critical modification is replacing all labeled real images with precise synthetic images.

**Q2 [Section 3]:**_Why do most prior works still stick to real images, if as A1 mentioned, synthetic images are already clearly superior to real images?_

**A2:** Synthetic images have their drawbacks, which are not trivial to address in previous paradigms.

**Q3 [Section 4]:**_How to avoid the drawbacks of synthetic images and also amplify its advantages?_

**A3:** Scale up the teacher model that is solely trained on synthetic images, and then teach (smaller) student models via the bridge of large-scale pseudo-labeled real images.

After the explorations, we successfully build a more capable MDE foundation model. However, we find current test sets  are too noisy to reflect the true strengths of MDE models. Thus, we further construct a versatile evaluation benchmark with precise annotations and diverse scenes (Section 6).

## 2 Revisiting the Labeled Data Design of Depth Anything V1

Building on the pioneering work of MiDaS [56; 7] in zero-shot MDE, recent studies tend to construct larger-scale training datasets in an effort to enhance estimation performance. Notably, Depth Anything V1 , Metric3D V1  and V2 , as well as ZeroDepth , have amassed 1.5M, 8M, 16M, and 15M labeled images from various sources for training, respectively. However, few studies have critically examined this trend: _is such a huge amount of labeled images truly advantageous?_

Before answering it, let us first dig into the potentially overlooked drawbacks of _real_ labeled images.

**Two disadvantages of real labeled data.** 1) _Label noise_, _i.e._, _inaccurate labels_ in depth maps. Stemming from the limitations inherent in various collection procedures, real labeled data inevitably contain inaccurate estimations. Such inaccuracies can arise from various factors, such as the inability of depth sensors to accurately capture the depth of transparent objects (Figure 2(a)), the vulnerability of stereo matching algorithms to textureless or repetitive patterns (Figure 2(b)), and the susceptible nature of SfM methods in handling dynamic objects or outliers (Figure 2(c)). _2) Ignored details_. These real datasets often overlook certain details in their depth maps. As depicted in Figure 3(a), the depth

Figure 3: Various noise in “GT” depth labels (a: NYU-D , b: HRWSI , c: MegaDepth ) and prediction errors in correspondingly trained models (d). Black regions are ignored during training.

representation of the tree and chair is notably coarse. These datasets struggle to provide detailed supervision at object boundaries or within thin holes, resulting in over-smoothed depth predictions, as seen in the middle of Figure 3(c). Hence, these noisy labels are so unreliable that the learned models make similar mistakes (Figure 2(d)). For example, MiDaS and Depth Anything V1 obtain poor scores of 25.9% and 53.5% respectively in the Transparent Surface Challenge  (more details in Table 12: our V2 achieves a competitive score of 83.6% in a zero-shot manner).

To overcome the above problems, we decide to change our training data and seek images with substantially better annotation. Inspired by several recent SD-based studies [31; 20; 25], that exclusively utilize synthetic images with complete depth information for training, we extensively check the label quality of synthetic images and note their potential to mitigate the drawbacks discussed above.

**Advantages of synthetic images.** Their depth labels are highly precise in two folds. 1) All fine details (_e.g_., boundaries, thin holes, small objects, _etc_.) are correctly labeled. As demonstrated in Figure 3(b), even all thin mesh structures and leaves are annotated with true depth. 2) We can obtain the actual depth of challenging transparent objects and reflective surfaces, _e.g_., the vase on the table in Figure 3(b). In a word, the depth of synthetic images is truly "GT". In the right side of Figure 3(c), we show the fine-grained prediction of a MDE model trained on synthetic images. Moreover, we can quickly enlarge synthetic training images by collecting from graphics engines [58; 63; 53], which would not cause any privacy or ethical concerns, as compared to real images.

## 3 Challenges in Using Synthetic Data

If synthetic data are so advantageous, why are real data still dominating MDE? In this section, we identify **two limitations of synthetic images** that hinder them from being easily used in reality.

**Limitation 1.** There exists _distribution shift_ between synthetic and real images. Although current graphics engines strive for photorealistic effects, their style and color distributions still evidently differ from real images. Synthetic images are too "clean" in color and "ordered" in layout, while real images contain more randomness. For instance, when comparing the images in Figure 3(a) and 3(b), we can immediately distinguish the synthetic ones. Such distribution shift makes models struggle to transfer from synthetic to real images, even if the two data sources share similar layouts [57; 9].

**Limitation 2.** Synthetic images have _restricted scene coverage_. They are iteratively sampled from graphics engines with pre-defined fixed scene types, _e.g_., "living room" and "street scene". Consequently, despite the astonishing precision of Hypersim  or Virtual KITTI  (Figure 3(b)), we cannot expect models trained on them to generalize well in real-world scenes like "crowded people". In contrast, some real datasets constructed from web stereo images (_e.g_., HRWSI ) or monocular videos (_e.g_., MegaDepth ), can cover extensive real-world scenes.

Figure 4: Depth labels of real images (a) and synthetic images (b), and the corresponding model predictions (c). The labels of synthetic images are highly precise, and so are their trained models.

**Therefore, synthetic-to-real transfer is non-trivial in MDE.** To validate this claim, we conduct a pilot study to learning MDE models solely on synthetic images with four popular pre-trained encoders, including BEiT , SAM , SynCLR , and DINOv2 . As illustrated in Figure 5, only DINOv2-G achieves satisfying results. All other model serials, as well as smaller DINOv2 models, suffer from severe generalization issues. This pilot study seems to give a straightforward solution to employing synthetic data in MDE, _i.e._, building on the largest DINOv2 encoder, and relying on its inherent generalization ability. However, this naive solution faces two problems. First, DINOv2-G frequently encounters failure cases when the patterns of real test images are rarely presented in synthetic training images. In Figure 6, we can clearly observe incorrect depth predictions for the sky (cloud) and the human head. Such failures can be expected as our synthetic training sets do not include diverse sky patterns or humans. Moreover, most applications cannot accommodate the resource-intensive DINOv2-G model (1.3B) in terms of storage and inference efficiency. Actually, the smallest model in Depth Anything V1 is used most widely due to its real-time speed.

To alleviate the generalization issue, some works [7; 89; 28] use a combined training set of real and synthetic images. Unfortunately, as shown in Section B.9, the coarse depth map of real images is destructive to fine-grained prediction. Another potential solution is to collect more synthetic images, which is unsustainable as creating graphic engines mimicking every real-world scenario is intractable. Therefore, a reliable solution is demanding in building MDE models with synthetic data. In this paper, we will close this gap and present a roadmap that solves the preciseness and robustness dilemma _without any trade-offs_, and applicable to _any model scale_.

## 4 Key Role of Large-Scale Unlabeled Real Images

Our solution is straightforward: incorporating _unlabeled real_ images. Our most capable MDE model, based on DINOv2-G, is initially trained purely on high-quality synthetic images. Then it assigns pseudo depth labels on unlabeled real images. Lastly, our new models are solely trained with large-scale and precisely pseudo-labeled images. Depth Anything V1  has highlighted the importance of large-scale unlabeled real data. Here, in our special context of synthetic labeled images, we will demonstrate its _indispensable_ role in more details from three perspectives.

**Bridge the domain gap.** As aforementioned, due to the distribution shift, directly transferring from synthetic training images to real test images is challenging. But if we can leverage extra real images as an intermediate learning target, the process will be more reliable. Intuitively, after explicitly training on pseudo-labeled real images, models can be more familiar with real-world data distribution. Compared with manually annotated images, our auto-generated pseudo labels are much more fine-grained and complete, as visualized in Figure 17.

Figure 5: Qualitative comparison of different vision encoders on synthetic-to-real transfer. Only DINOv2-G produces a satisfying prediction. For quantitative comparisons, please refer to Section B.6.

Figure 6: Failure cases of the most capable DINOv2-G model when purely trained on synthetic images. Left: the sky should be ultra far. Right: the depth of the head is not consistent with the body.

**Enhance the scene coverage.** The diversity of synthetic images is limited, without including enough real-world scenes. Nevertheless, we can easily cover numerous distinct scenes by incorporating large-scale unlabeled images from public datasets. Moreover, synthetic images are indeed very redundant due to being repetitively sampled from pre-defined videos. In comparison, unlabeled real images are clearly distinguished and very informative. By training on sufficient images and scenes, models not only demonstrate stronger zero-shot MDE capability (as shown in Figure 6 "+ _unlabeled real images_"), but they can also serve as better pre-trained sources for downstream related tasks .

**Transfer knowledge from the most capable model to smaller ones.** We have shown in Figure 5, that smaller models cannot directly benefit from synthetic-to-real transfer by themselves. However, armed with large-scale unlabeled real images, they can learn to mimic the high-quality predictions of the most capable model, similar to knowledge distillation . But differently, our distillation is enforced at the label level via extra unlabeled real data, instead of at the feature or logit level with original labeled data. This practice is safer because there is evidence showing feature-level distillation is not always beneficial, especially when the teacher-student scale gap is huge . Finally, as supported in Figure 16, unlabeled images boost the robustness of our smaller models tremendously.

## 5 Depth Anything V2

### Overall Framework

According to all the above analysis, our final pipeline to train Depth Anything V2 is clear (Figure 7). It consists of three steps:

* train a reliable teacher model based on DINOv2-G _purely_ on high-quality _synthetic_ images.
* produce precise pseudo depth on large-scale unlabeled _real_ images.
* train final student models on _pseudo-labeled real_ images for robust generalization (we will show the synthetic images are not necessary in this step).

We will release four student models, based on DINOv2 small, base, large, and giant, respectively.

### Details

As shown in Table 7, we use five precise synthetic datasets (595K images) and eight large-scale pseudo-labeled real datasets (62M images) for training. Same as V1 , for each pseudo-labeled sample, we ignore its top-\(n\)-largest-loss regions during training, where \(n\) is set as 10%. We consider them as potentially noisy pseudo labels. Similarly, our models produce affine-invariant inverse depth2. We use two loss terms for optimization on labeled images: a scale- and shift-invariant loss \(_{ssi}\) and a gradient matching loss \(_{gm}\). These two objective functions are not new, as they are proposed by MiDaS . But differently, we find \(_{gm}\) is super beneficial to the depth sharpness when using synthetic images (Section B.7). On pseudo-labeled images, we follow V1 to add an additional feature alignment loss to preserve informative semantics from pre-trained DINOv2 encoders.

Figure 7: Depth Anything V2. We first train the most capable teacher on precise synthetic images. Then, to mitigate the distribution shift and limited diversity of synthetic data, we annotate unlabeled real images with the teacher. Finally, we train student models on high-quality pseudo-labeled images.

## 6 A New Evaluation Benchmark: DA-2K

### Limitations in Existing Benchmarks

In Section 2, we demonstrated that commonly used real training sets have noisy depth labels. Here, we further argue that widely adopted _test benchmarks_ are also noisy. Figure 8 illustrates incorrect annotations for mirrors and thin structures on NYU-D  despite using specialized depth sensors. Such frequent label noise makes the reported metrics of powerful MDE models not reliable anymore.

Apart from label noise, another drawback of these benchmarks is _limited diversity_. Most of them were originally proposed for a single scene. For example, NYU-D  focuses on a few indoor rooms, while KITTI  only contains several street scenes. Performance on these benchmarks may not reflect real-world reliability. Ideally, we expect MDE models can handle any unseen scenes robustly.

The last problem in these existing benchmarks is _low resolution_. They mostly provide images with a resolution of around 500\(\)500. But with modern cameras, we usually require precise depth estimation for higher-resolution images, _e.g_., 1000\(\)2000. It remains unclear whether the conclusions drawn from these low-resolution benchmarks can be safely transferred to high-resolution benchmarks.

### Da-2k

Considering the above three limitations, we aim to construct a versatile evaluation benchmark for relative monocular depth estimation, that can 1) provide _precise_ depth relationship, 2) cover _extensive_ scenes, and 3) contain mostly _high-resolution_ images for modern usage. Indeed, it is impractical for humans to annotate the depth of each pixel, especially for in-the-wild images. Thus, following DIW , we annotate _sparse_ depth pairs for each image. Generally, given an image, we can select two pixels on it, and decide their relative depth between them (_i.e_., which pixel is closer).

Concretely, we employ two distinct pipelines to select pixel pairs. In the first pipeline, as shown in Figure 8(a), we use SAM  to automatically predict object masks. Instead of using the masks, we leverage key points (pixels) that prompt out them. We randomly sample two key pixels and query four expert models ([89; 31; 20] and ours) to vote on their relative depth. If there is disagreement, the pair will be sent to human annotators to decide the true relative depth. Due to potential ambiguity, annotators can skip any pair. However, there may be cases where all models incorrectly predict challenging pairs, and they are not flagged. To address this, we introduce a second pipeline, where we carefully analyze images and manually identify challenging pairs.

Figure 8: Visualization of widely adopted but indeed noisy test benchmark . As highlighted, the depth of the mirror and thin structures are incorrect (black pixels are ignored). In comparison, our model predictions are accurate. The noise will cause better models instead achieve lower scores.

Figure 9: Our proposed evaluation benchmark DA-2K. (a) The annotation pipeline for relative depth between two points. Points are sampled based on SAM  mask predictions. Disagreed pairs among four depth models will be popped out for annotators to label. (b) Detail of our scenario coverage.

To ensure preciseness, all annotations are triple-checked by the other two annotators. To ensure diversity, we first summarize eight important application scenarios of MDE (Figure 8(b)), and ask GPT-4 to produce diverse keywords related to each scenario. We then use these keywords to download corresponding images from Flickr. Finally, we annotate 1K images with 2K pixel pairs in total. Limited by space, please refer to Section C for details and comparisons with DIW .

**Position of DA-2K.** Despite the advantages, we _do not_ expect DA-2K to _replace_ current benchmarks. Accurate sparse depth is still far from the precise dense depth required for scene reconstruction. However, DA-2K can be considered a prerequisite for accurate dense depth. As such, we believe DA-2K can serve as _a valuable supplement_ to existing benchmarks due to its extensive scene coverage and precision. It can also serve as a quick prior validation for users selecting community models for specific scenarios covered in DA-2K. Lastly, we believe it is also a potential testbed for the 3D awareness of future multimodal LLMs [41; 21; 3].

## 7 Experiment

### Implementation details

Follow Depth Anything V1 , we use DPT  as our depth decoder, built on DINOv2 encoders. All images are trained at the resolution of 518\(\)518 by resizing the shorter size to 518 followed by a random crop. When training the teacher model on synthetic images, we use a batch size of 64 for 160K iterations. In the third stage of training on pseudo-labeled real images, the model is trained with a batch size of 192 for 480K iterations. We use the Adam optimizer and set the learning rate of the encoder and the decoder as 5e-6 and 5e-5, respectively. In both training stages, we do not balance the training datasets, but simply concatenate them. The weight ratio of \(_{ssi}\) and \(_{gm}\) is set as 1:2.

### Zero-Shot Relative Depth Estimation

**Performance on conventional benchmarks.** Since our model predicts affine-invariant _inverse_ depth, for fairness, we compare with Depth Anything V1  and MiDaS V3.1  on five unseen test datasets. As shown in Table 2, our results are superior to MiDaS and comparable to V1 . We are slightly inferior to V1 in _metrics_ on two of the datasets. However, the plain metrics on these datasets are not the focus of this paper. This version aims to produce fine-grained predictions for thin structures and robust predictions for complex scenes, transparent objects, _etc._. Improvement in these dimensions cannot be correctly reflected in current benchmarks.

**Performance on our proposed benchmark DA-2K.** As shown in Table 3, on our proposed benchmark with diverse scenes, even our smallest model is significantly better than other heavy SD-based

    &  &  &  &  &  &  \\   & & AbsRel & \(_{1}\) & AbsRel & \(_{1}\) & AbsRel & \(_{1}\) & AbsRel & \(_{1}\) & AbsRel & \(_{1}\) \\  MiDaS V3.1  & ViT-L & 0.127 & 0.850 & 0.048 & 0.980 & 0.587 & 0.699 & 0.139 & 0.867 & 0.075 & 0.942 \\   & ViT-S & 0.080 & 0.936 & 0.053 & 0.972 & 0.464 & 0.739 & 0.127 & **0.885** & 0.076 & 0.939 \\  & ViT-B & 0.080 & 0.939 & 0.046 & 0.979 & **0.432** & 0.756 & **0.126** & 0.884 & 0.069 & 0.946 \\  & ViT-L & 0.076 & 0.947 & **0.043** & **0.981** & 0.458 & 0.760 & 0.127 & 0.882 & 0.066 & 0.952 \\   & ViT-S & 0.078 & 0.936 & 0.053 & 0.973 & 0.500 & 0.718 & 0.142 & 0.851 & 0.073 & 0.942 \\  & ViT-B & 0.078 & 0.939 & 0.049 & 0.976 & 0.495 & 0.734 & 0.137 & 0.858 & 0.068 & 0.950 \\   & ViT-L & **0.074** & 0.946 & 0.045 & 0.979 & 0.487 & 0.752 & 0.131 & 0.865 & 0.066 & 0.952 \\   & ViT-G & 0.075 & **0.948** & 0.044 & 0.979 & 0.506 & **0.772** & 0.132 & 0.862 & **0.065** & **0.954** \\   

Table 2: Zero-shot _relative_ depth estimation. Better: AbsRel \(\), \(_{1}\). Solely from the metrics, Depth Anything V2 is better than MiDaS, but merely comparable with V1. But indeed, the focus and strengths of our V2 (_e.g._, fine-grained details, robust to complex layouts, transparent objects, _etc._) cannot be correctly reflected on these benchmarks. Similar results (_i.e._, better model but worse score) are also observed in [7; 28].

    &  &  \\   & Marigold  & Gewizard  & DepthFM  & Depth Anything V1  & ViT-S & ViT-B & ViT-L & ViT-G \\  Accuracy (\%) & 86.8 & 88.1 & 85.8 & 88.5 & 95.3 & 97.0 & 97.1 & **97.4** \\   

Table 3: Performance on our proposed DA-2K evaluation benchmark, which encompasses eight representative scenarios. Even our most lightweight model is superior to all other community models.

models, _e.g._, Marigold  and Geowizard . Our most capable model achieves 10.6% higher accuracy than Margold in terms of relative depth discrimination. Please refer to Table 14 for the comprehensive per-scenario performance of our models.

### Fine-tuned to Metric Depth Estimation

To validate the generalization ability of our model, we transfer its encoder to the downstream metric depth estimation task. First, same as V1 , we follow the ZoeDepth  pipeline, but replace its MiDaS  encoder with our pre-trained encoder. As shown in Table 4, we achieve significant improvements over previous methods on both NYU-D and KITTI datasets. Notably, even our most lightweight model which is based on ViT-S, is superior to other models built on ViT-L .

Although the reported metrics look impressive, models trained on NYUv2 or KITTI fail to produce fine-grained depth prediction and are not robust to transparent objects, due to the inherent noise in training sets. Therefore, to satisfy real-world applications such as multi-view synthesis, we fine-tune our powerful encoder on Hypersim  and Virtual KITTI  synthetic datasets, for indoor and outdoor metric depth estimation, respectively. We will release these two metric depth models. Please refer to Figure 15 for qualitative comparisons with the previous ZoeDepth method.

### Ablation Study

Limited by space, we defer most of our ablations to the appendix except for two on pseudo labels.

**Importance of large-scale pseudo-labeled real images.** As shown in Table 5, compared with solely trained on synthetic images, our models are greatly enhanced by incorporating pseudo-labeled real images. Different from Depth Anything V1 , we further attempt to remove the synthetic images during training student models. We find this can even lead to slightly better results for smaller models (_e.g._, ViT-S and ViT-B). So we finally choose to train student models purely on pseudo-labeled images. This observation is indeed similar to SAM  that only releases its pseudo-labeled masks.

Table 4: Fine-tuning our Depth Anything V2 pre-trained encoder to in-domain metric depth estimation, _i.e._, training and test images share the same domain. All compared methods use the encoder size close to ViT-L.

Table 5: Importance of pseudo-labeled (unlabeled) real images (\(^{n}\)). \(^{l}\): precisely labeled synthetic images.

**Pseudo label _vs._ manual label on real labeled images.** We have demonstrated before in Figure 3(a) that existing labeled real datasets are very noisy. Here we conduct a quantitative comparison. We use real images from the DIML  dataset, and compare the transferring performance under its original manual label and our produced pseudo label respectively. We can observe in Table 6 that the model trained with pseudo labels is significantly better than the manual-label counterpart. The huge gap indicates the high quality of our pseudo labels and the rich noise in current labeled real datasets.

## 8 Related Work

**Monocular depth estimation.** Early works [18; 19; 5] focus on the in-domain metric depth estimation, where training and test images must share the same domain [70; 24]. Due to their restricted application scenarios, recently there has been increasing attention on zero-shot relative monocular depth estimation. Among them, some works address this task through better modeling manners, _e.g._, using Stable Diffusion  as a depth denoiser [31; 25; 20]. Other works [94; 96; 89] focus on the data-driven perspective. For example, MiDaS [56; 55; 7] and Metric3D  collect 2M and 8M labeled images respectively. Aware of the difficulty of scaling up labeled images, Depth Anything V1  leverages 62M unlabeled images to enhance the model's robustness. In this work, differently, we point out multiple limitations in widely used labeled real images. We thus especially highlight the necessity of resorting to synthetic images to ensure depth preciseness. Meantime, to tackle the generalization issue caused by synthetic images, we adopt both data-driven (large-scale pseudo-labeled real images) and model-driven (scaling up the teacher model) strategies.

**Learning from unlabeled real images.** How to learn informative representations from unlabeled images is widely studied in the field of semi-supervised learning [36; 86; 71; 90]. However, they focus on academic benchmarks  which only allow usage of small-scale labeled and unlabeled images. In comparison, we study a real-world application scenario, _i.e._, how to further boost the baseline of 0.6M labeled images with 62M unlabeled images. Moreover, distinguished from Depth Anything V1 , we exhibit the indispensable role of unlabeled real images especially when we replace all labeled real images with synthetic images [22; 23; 61]. We demonstrate "precise synthetic data + pseudo-labeled real data" is a more promising roadmap than labeled real data.

**Knowledge distillation.** We distill transferable knowledge from our most capable teacher model to smaller models. This is similar to the core spirit of knowledge distillation (KD) . But we are also fundamentally different in that we perform distillation at the _prediction level_ through extra _unlabeled_ real images, while KD [2; 73; 100] typically studies better distillation strategies at the _feature or logit_ level through _labeled_ images. We aim to reveal the importance of large-scale unlabeled data and larger teacher model, rather than delicate loss designs [43; 69] or distillation pipelines . Moreover, it is indeed non-trivial and risky to directly distill feature representations between two models with a tremendous scale gap . In comparison, our pseudo-label distillation is easier and safer.

## 9 Conclusion

In this work, we present _Depth Anything V2_, a more powerful foundation model for monocular depth estimation. It is capable of 1) providing robust and fine-grained depth prediction, 2) supporting extensive applications with varied model sizes (from 25M to 1.3B parameters), and 3) being easily fine-tuned to downstream tasks as a promising model initialization. We reveal crucial findings to pave the way towards building a strong MDE model. Furthermore, realizing the poor diversity and rich noise in existing test sets, we construct a versatile evaluation benchmark DA-2K, covering diverse high-resolution images with precise and challenging sparse depth labels.

**Acknowledgment.** This work is supported by the National Natural Science Foundation of China (No.62201484), HKU Startup Fund, and HKU Seed Fund for Basic Research.

    & KITTI  &  &  &  &  &  \\   & AbsRel & \(_{1}\) & AbsRel & \(_{1}\) & AbsRel & \(_{1}\) & AbsRel & \(_{1}\) & AbsRel & \(_{1}\) & Acc (\%) \\  Manual Label & 0.122 & 0.882 & 0.074 & 0.952 & 0.581 & 0.693 & 0.159 & 0.832 & 0.126 & 0.890 & 80.2 \\ Pseudo Label & **0.099** & **0.901** & **0.062** & **0.963** & **0.514** & **0.701** & **0.147** & **0.843** & **0.084** & **0.929** & **89.7** \\   

Table 6: Comparison between originally manual label and our produced pseudo label on the DIML dataset . Our produced pseudo labels are of much higher quality than the manual labels provided by DIML.