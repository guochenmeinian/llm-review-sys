# UDPM: Upsampling Diffusion Probabilistic Models

Shady Abu-Hussein

Department of Electrical Engineering

Tel Aviv University

shady.abh@gmail.com

&Raja Giryes

Department of Electrical Engineering

Tel Aviv University

raja@tauex.tau.ac.il

###### Abstract

Denoising Diffusion Probabilistic Models (DDPM) have recently gained significant attention. DDPMs compose a Markovian process that begins in the data domain and gradually adds noise until reaching pure white noise. DDPMs generate high-quality samples from complex data distributions by defining an inverse process and training a deep neural network to learn this mapping. However, these models are inefficient because they require many diffusion steps to produce aesthetically pleasing samples. Additionally, unlike generative adversarial networks (GANs), the latent space of diffusion models is less interpretable. In this work, we propose to generalize the denoising diffusion process into an Upsampling Diffusion Probabilistic Model (UDPM). In the forward process, we reduce the latent variable dimension through downsampling, followed by the traditional noise perturbation. As a result, the reverse process gradually denoises and upsamples the latent variable to produce a sample from the data distribution. We formalize the Markovian diffusion processes of UDPM and demonstrate its generation capabilities on the popular FFHQ, AFHQv2, and CIFAR10 datasets. UDPM generates images with as few as three network evaluations, whose overall computational cost is less than a single DDPM or EDM step while achieving an FID score of 6.86. This surpasses current state-of-the-art efficient diffusion models that use a single denoising step for sampling. Additionally, UDPM offers an interpretable and interpolable latent space, which gives it an advantage over traditional DDPMs. Our code is available online: [https://github.com/shadyabh/UDPM/](https://github.com/shadyabh/UDPM/)

Figure 1: The Upsampling Diffusion Probabilistic Model (UDPM) scheme for 3 diffusion steps (\(L=3\)). In addition to the gradual noise perturbation in traditional DDPMs, UDPM also downsamples the latent variables. Accordingly, in the reverse process, UDPM denoises and upsamples the latent variables to generate images from the data distribution.

Introduction

In recent years, Denoising Diffusion Probabilistic Models (DDPMs) have become popular for image generation due to their ability to learn complex data distributions and generate high-fidelity images. These models work by starting with data samples and gradually adding noise through a Markovian process until pure white noise is reached. This process is known as the forward diffusion process, defined by the joint distribution \(q(_{0:L})\). The reverse diffusion process, used for generating new samples, is defined by the learned reverse process \(p_{}(_{0:L})\) using a deep neural network. This methodology allows them to achieve impressive performance in learning data distributions and sampling from them.

Although DDPMs have shown impressive results in image generation, they possess some limitations. One major limitation is that they require a large number of denoising diffusion steps to produce aesthetically pleasing samples. This can be quite intensive computationally, which makes the sampling process slow and resource-intensive.

Additionally, the latent space of these models is not interpretable, which limits their utility for certain types of image generation tasks, such as video generation or animation, especially when used in an unconditional setting. The vast majority of works using diffusion models for editing rely on manipulating the CLIP  embeddings used with these models and not the latent space itself.

In this work, we propose a generalized scheme of DDPMs called the Upsampling Diffusion Probabilistic Model (UDPM). In addition to the gradual noise addition in the forward diffusion process, we downsample the latent diffusion variables to "dissolve" the data information spatially, as demonstrated in Figure 1. We thoroughly formulate the generalized model and derive the assumptions required for obtaining a viable scheme.

Using our approach, we can sample images from CIFAR10 , AFHQv2 , and FFHQ  datasets, with as few as 3 UDPM steps, where the cost of all three steps together is \(\)\(30\%\) of a single regular diffusion step. This is less than two orders of magnitude compared to standard DDPMs: guided diffusion  typically requires 1000 iterations, denoising diffusion implicit models  require \(250\) iterations, stable diffusion  requires at least 50 iterations with an additional decoder, EDM  can reduce the number of steps to 39, and other recent works [24; 25; 10] can sample with 10-20 network evaluations, which is still considerably more than UDPM. Indeed, in , an integration of discriminative loss with diffusion models has shown great generation performance while requiring only 2 diffusion steps. Recent works [43; 26; 31] have demonstrated that sampling can be performed with as few as a single denoising diffusion step while maintaining competitive generation performance. Yet, UDPM achieves better generation quality than these works on the CIFAR10 dataset  while requiring a smaller computational cost (\(1/3\) of a typical diffusion step).

Because UDPM gradually reduces the dimensions of the latent variables in each step, the size of the random noise added is considerably smaller. Specifically, all the dimensions of the latent variables together are smaller than the dimensions of the original images. As a result, UDPM is much more interpretable compared to conventional DDPMs. In our experiments, we show how one may manipulate the generated images by changing the latent variables, which is similar to what has been done in Generate Adversarial Networks (GANs) .

Our contributions may be summarized as follows: (i) A novel efficient diffusion model for image generation that achieves a significant improvement over current state-of-the-art methods by reducing the number and cost of diffusion steps required to generate high-quality images; (ii) achieving good interpretability and interpolability of the latent space.

## 2 Related Work

**Diffusion models** are latent variable models defined through a diffusion probabilistic model. On one side we have the data \(_{0} q(_{0})\) and on the other side, we have pure noise \(_{L} q(_{L})\). Both are related to each other using a Markovian diffusion process, where the forward process is defined by the joint distribution \(q(_{1:L}|_{0})\) and the reverse process by \(p(_{0:L-1}|_{L})\). Using the Markov chain property, \(q(_{1:L}|_{0})\) and \(p(_{0:L-1}|_{L})\) can be expressed by

\[p(_{0:L-1}|_{L})=_{l=1}^{L}p(_{l-1}|_{l}), \]

and

\[q(_{1:L}|_{0})=_{l=1}^{L}q(_{l}|_{ l-1}). \]

As stated in previous literature [14; 9; 34], the common approach is to assume that the Markov chain is constructed using normal distributions defined by

\[q(_{l}|_{l-1}):=(}_{ l-1},_{l}), \]

and

\[p(_{l-1}|_{l}):=(_{l}(_{l}), _{l}(_{l})), \]

where \(_{1},_{2},_{L}\) are hyperparameters that control the noise levels of the diffusion process, while \(_{l},_{l}\) denote the mean and variance of the reverse process, respectively.

By learning the reverse process \(p(_{l-1}|_{l})\) using a deep neural network, one can generate samples from the data distribution by running the reverse process: starting from pure noise \(_{L}(0,)\), then progressively predicting the next step of the reverse process using a network trained to predict \(_{l-1}\) from \(_{l}\), until reaching \(_{0}\).

Over the past couple of years, this principle has shown marvelous performance in generating realistic-looking images [9; 29; 14; 34; 27; 20; 11; 32; 1]. However, as noted by , the number of diffusion steps \(L\) is required to be large for the model to produce pleasing-looking images.

Recently, many studies have utilized diffusion models for image manipulation and reconstruction tasks [40; 30], where a denoising network is trained to learn the prior distribution of the data. At test time, some conditioning mechanism is combined with the learned prior for solving highly challenging imaging tasks [3; 2; 7]. Note that our novel adaptive diffusion ingredient can be incorporated into any conditional sampling scheme that is based on diffusion models.

The works in [40; 30] addressed the problems of deblurring and super-resolution using diffusion models. These works try to deblur  or increase the resolution  of a blurry or low-res input image. Unlike our work, their goal is not image generation, but rather image reconstruction from a given degraded image. Therefore, their trained model is significantly different from ours.

Cold diffusion  performs diffusion steps by replacing the steps of noise addition with steps of blending with another image or other general steps. This approach differs significantly from ours, as their goal is to demonstrate that denoising can be replaced with other operations without developing the corresponding diffusion equations. In our case, we formally show what operations can be used without losing the Markovian property of the forward and reverse diffusion processes, and without omitting the noise component.

Figure 2: Generated \(64 64\) images of AFHQv2  with **FID=7.10142**, produced using unconditional UDPM with only 3 steps, which are equivalent to 0.3 of a single typical \(64 64\) diffusion step.

Soft diffusion  proposes to blur the signal before adding noise to it in the forward process. Then, for solving the reverse process they train a network to deblur and denoise the signal. However, the addition of the blur operation to the forward process prohibits explicit access to the reverse process and therefore relies solely on the network to predict the clean sharp sample. In contrast, in our method, the reverse process is explicitly accessible and can be sampled easily.

Another effort [12; 17] suggests replacing the denoising steps by learning the wavelet coefficients of the high frequencies. They show that this can reduce the number of diffusion steps. Our work differs from theirs in the fact that we rely on upsampling with an additive noise step. We also employ advanced loss functions and present state-of-the-art generation results. This is in addition to our interpretable latent space, a component missing from many of the recent diffusion models.

Many works tried to accelerate the sampling procedure of the denoising diffusion model [35; 18; 24; 25; 10]. However, they only focus on reducing the number of sampling steps, while ignoring the diffusion structure itself. By contrast, in this work we propose to degrade the signal not only over the noise domain but also in the spatial domain, thereby "dissolving" the signal much faster.

## 3 Method

Traditional denoising diffusion models assume that the probabilistic Markov process is defined by (3) and (4). These equations construct forward and backward processes that progress by adding and removing noise, respectively. In this work, we generalize this scheme by adding a degradation element to the forward process. Specifically, we downsample the spatial dimension of the latent variable and upsample it when reversing the process.

### Upsampling Diffusion Probabilistic Model (UDPM)

We begin by redefining the marginal distributions \(q(_{l}|_{l-1})\) and \(p(_{l-1}|_{l})\) of the forward and reverse processes:

\[q(_{l}|_{l-1}):=(_{l}_{l-1},_{l}^{2}), \]

and

\[p(_{l-1}|_{l}):=((_{l};l),_{ l}), \]

where in contrast to previous diffusion models that used \(=I\), in this work we define the operator \(\) as a downsampling operator, defined by applying a blur filter \(\) followed by subsampling with stride \(\). As a result, the forward diffusion process decreases the variables' dimensions in addition to the increased noise levels.

In diffusion models, the goal is to match the joint distributions \(p_{}(_{1:L}|_{0})\) (learned) and \(q(_{1:L}|_{0})\) under some statistical distance. One particular choice is the Kullback-Leibler (KL) divergence, which we adopt here. Formally,

\[D_{}(q(_{1:L}|_{0})||p_{}(_{1:L}| _{0})):=_{q}[_{1:L}|_ {0})}{p_{}(_{1:L}|_{0})}]= p_{}( _{0})_{q}[(_{0 :L})}{q(_{1:L}|_{0})}]}_{}. \]

Thus, one can minimize the KL-divergence between \(p_{}(_{1:L}|_{0})\) and \(q(_{1:L}|_{0})\) by minimizing the Evidence Lower Bound (ELBO). As we show in Appendix B.1, this is equivalent to

\[_{q}[-(_{0:L})}{q( _{1:L}|_{0})}] =_{q}[D_{}(p(_{L})||q(_{L} |_{0}))\] \[+_{l=2}^{L}D_{}(p_{}(_{l-1}|_{l})||q(_{l-1}|_{l},_{0})- p_{}( _{1}|_{0})]. \]

The right-hand side of (8) can be then minimized stochastically w.r.t. \(\) using gradient descent, where at each step a random \(l\) is chosen and a single term of (8) is optimized.

In order to be able to use (8) for training \(p_{}()\), one needs explicit access to \(q(_{l-1}|_{l},_{0})\), for which we need to obtain \(q(_{l}|_{0})\) first. Then, using Bayes' theorem, we can derive \(q(_{l-1}|_{l},_{0})\). To do so, we first present Lemma 1 (the proof is in Appendix B.2)

**Lemma 1**.: _Let \(}{{}}(0,) ^{N}\) and \(=_{}\), where \(_{}\) is a subsampling operator with stride \(\) and \(\) is a blur operator with blur kernel \(\). Then, if the support of \(\) is at most \(\), we have \(}{{}}(0,\| \|_{2}^{2})\)._

If Lemma 1 holds, then by assuming that \(\|\|_{2}^{2}=1\), we get the following result (see Appendix B.3)

\[q(_{l}|_{0})=(_{l} ^{l}_{0},_{l}^{2})_{l}=_{k=0}^{l}_{k}_{l}=_{l}^{2}_{k=1}^{l}^{2}}{ _{l}^{2}}. \]

Using (9), we can obtain \(_{t}\) from \(_{0}\) simply by applying \(\)\(l\)-times on \(_{0}\), followed by the addition of white Gaussian noise with standard deviation \(_{l}\).

Given \(q(_{l}|_{0})\), we can use Bayes' theorem and utilize the Markov chain property to get

\[q(_{l-1}|_{l},_{0})=_{l}|_{l-1})q(_{l-1}|_{0})}{q(_{l}|_{0})},\]

which, as shown in Appendix B.4, is of the following form

\[q(_{l-1}|_{l},_{0})=( (_{l},_{0},l),_{l}), \]

where

\[_{l}=(^{2}}{_{l}^{2}} ^{T}+_{l-1}})^{-1}, \]

and

\[(_{l},_{0},l)=_{l}(}{_{l}^{2}}^{T}_{l}+_{l-1}}{ _{l-1}^{2}}^{l-1}_{0}). \]

Although expression (11) seems to be implacable, in practice it can be implemented efficiently using the Discrete Fourier Transform and the poly-phase filtering identity used in , where \(^{T}\) is equivalent to a convolution between \(\) and its flipped version, followed by subsampling with stride \(\). More details are provided in Appendix B.5.

Following (10), the true posterior \(q(_{l-1}|_{l},_{0})\) in this new setup is a Gaussian distribution with parameters \(((_{l},_{0},l),_{l})\). Therefore, one may assume that \(p_{}()\) is also Gaussian with parameters \((_{},_{l})\), where \(_{}\) is parameterized by a deep neural network with learned parameters \(\).

For normal distributions, a single term of (8) is equivalent to

\[^{(l)} =D_{}(p_{}(_{l-1}|_{l})||q( _{l-1}|_{l},_{0})\] \[=C_{l}+(_{}-_{l})^{T}_{l}^{-1}(_{ }-_{l}), \]

where \(C_{l}\) is a constant value independent of \(\).

Figure 3: Generated \(64 64\) images of FFHQ with **FID=7.41065**, produced using unconditional UDPM with only 3 steps, which are equivalent to 0.3 of a single typical \(64 64\) diffusion step.

```
0:\(f_{}(),L,q(),D_{}()\)
1:while Not converged do
2:\(_{0} q()\)
3:\(l\{1,2,,L\}\)
4:\((0,I)\)
5:\(_{l}=_{l}^{T}_{0}+_{l} \)
6:\(=_{}^{(l)}_{}+_{}^{(l)} _{}+_{}^{(l)}_{}\)
7: ADAM step on \(\)
8: Adversarial ADAM step on \(\)
9:endwhile
10:return\(f_{}()\)
```

**Algorithm 1** UDPM training algorithm

From our experiments and following [9; 14], training \(p_{}\) to predict \(\) directly leads to worse results. Therefore, we train the network to predict the second term in (12), i.e. to estimate \(^{l-1}_{0}\) from \(_{l}\). As a result, minimizing (13) can be simplified to minimizing the following term

\[_{}^{(l)}=(f_{}(_{l})-^{l -1}_{0})^{T}_{l}^{-1}(f_{}(_{l})-^{ l-1}_{0}), \]

where \(f_{}()\) is a deep neural network that upsamples its input by a scale factor of \(\). By the definition of \(_{l}\), it is easy to show that it is a diagonal positive matrix, and therefore it can be dropped. As a result, one may simplify the objective to the following term

\[_{}^{(l)}=\|f_{}(_{l})-^{l-1} _{0}\|_{2}^{2}. \]

Unlike denoising diffusion models, UDPM tackles a super-resolution task at each reverse step. Consequently, relying solely on \(_{}\) for training \(f_{}()\) produces softer images, as shown in Figure 7 and discussed in the ablation studies section 5. To address this, we propose to incorporate two additional regularization terms, following . The first term is a perceptual loss , denoted by \(_{}\), which aligns the VGG features of \(^{l-1}_{0}\) and \(f_{}(_{l})\). The second term is an adversarial loss denoted as \(_{}\), which is similar to the one used in [38; 23]. Both are used to ensure sharp detailed results. Overall, \(f_{}()\) is trained using the following objective function

\[=_{}^{(l)}_{}+_{}^{(l) }_{}+_{}^{(l)}_{}. \]

Algorithm 1 and Figure 4 provide an overview of the UDPM training scheme.

### Image Generation using UDPM

The UDPM scheme presented in the previous section introduces a new approach for capturing the true implicit data distribution \(q(_{0})\) of a given dataset. Given that we trained UDPM, the remaining question is how it can be utilized for sampling from \(q(_{0})\).

Given a deep neural network \(f_{}\) trained to predict \(^{l-1}_{0}\) from \(_{l}\), we start with a pure Gaussian noise sample \(_{L}(0,I)\). Subsequently, by substituting \(f_{}(_{l})\) into \(^{l-1}_{0}\) in (12), we get an estimate of \(_{L}\). Then, the next reverse diffusion step \(_{L-1}\) can be obtained by sampling from \((_{L},_{L})\). By iteratively repeating these steps \(L\) times, we can acquire a sample \(_{0} p_{}(_{0})\), as outlined in Algorithm 2.

Note that sampling the posterior requires parameterizing \((_{l},_{l})\) in the form of

\[_{l-1}=_{l}+_{l}^{},\]

where \((0,I)\) has the same dimensions as \(_{l-1}\). This requires having access to \(_{l}^{}\). Due to the structure of \(_{l}\), it is possible to apply it on \(\) efficiently, as shown in Appendix B.6.

```
0:\(f_{}(),L\)
1:\(_{L}(0,I)\)
2:for all\(l=L,,1\)do
3:\(=(^{2}}{_{l}^{2}}^{T}+ ^{2}})^{-1}\)
4:\(_{}=[}{_{l}^{2}}^{T} _{l}+}{_{l-1}^{2}}f_{}^{(l)}(_{l})]\)
5:\(_{l-1}(_{},)\)
6:endfor
7:return\(_{0}\)
```

**Algorithm 2** UDPM sampling algorithm

## 4 Experiments

In this section, we present the evaluation of UDPM under multiple scenarios. We tested our method on CIFAR10 , FFHQ , and AFHQv2  datasets. Here we focus on the qualitative performance of UDPM and demonstrate its interpolatable latent space. In the appendix, we provide additional quantitative and qualitative results.

We set \(L=3\) and fix \(=2\) for all datasets. We also use a uniform box filter of size \(2 2\) as the downsampling kernel \(\) as it satisfies the condition in Lemma 1. We then normalize it w.r.t. its norm \(\|\|\) and use it to construct the downsampling operator \(\). Unlike previous diffusion approaches that are limited to analytically defined noise schedulers due to the large diffusion steps number, UDPM allows the noise scheduler to be fine-tuned manually by setting only 6 values (\(\{_{l}\}_{l=1}^{L}\) and \(\{_{l}\}_{l=1}^{L}\)). In our tests we set \(\{_{l}\}_{l=1}^{3}=\{0.5,0.2,10^{-3}\}\) and \(\{_{l}\}_{l=0}^{3}=\{0.1,0.2,0.3\}\) for all datasets.

Figure 4: The training and sampling procedures of UDPM. During the _training_ phase, an image \(_{0}\) is randomly selected from the dataset. It is then degraded using (9) to obtain a downsampled noisy version \(_{l}\), which is then plugged into \(f_{}^{(l)}()\), that is trained to predict \(^{l-1}_{0}\). In the _sampling_ phase, we start from pure noise \(_{L}(0,)\). This noise is passed through the network \(f_{}^{(L)}()\) to estimate \(^{L-1}_{0}\), used to compute \(_{L}\) through (12), with \(_{L}\) obtained from (11). Afterwards, \(_{L-1}\) is drawn from \((_{L},_{L})\) using the technique described in Appendix B.6. By repeating this procedure for \(L\) iterations, the final sample \(_{0}\) is obtained.

Figure 5: Latent space interpolation for \(64 64\) generated images. The four corner images are interpolated by a weighted mixture of their latent noises, such that the other images are “in-between” images from the latent perspective, similar to what has been done in GANs .

We use the same UNet architecture proposed by  and utilized in  for all diffusion steps (the specific implementation details are presented in Table 3). We increase the number of output channels of the network by a factor of \(^{2}\) and use the depth-to-space layer  to rearrange the output pixels to the desired dimension, which is equivalent to a scale-up by a factor of \(\). For all datasets, we train the network for 600K training steps using ADAM  with learning rate and batch size set to \(10^{-4}\) and 64, respectively. We save the model weights every 10K training steps and pick the model with the best Frechet inception distance (FID) . For stabilizing the training we use exponential moving averaging (EMA) with the dampening parameter set to 0.9999. We also set \(_{}=(1,1,0)\), \(_{}=(4,4,0)\), and \(_{}=(0.2,0.5,1)\). Particularly, we found that setting \(_{}=_{}=0\) for \(l=3\) achieves the best results with minimal mode-collapse. For the discriminator network, we use the discriminator architecture used in , which is a variant of the original discriminator network proposed by . We train all models using a single NVIDIA RTX A6000 GPU.

### Unconditional Generation

In the unconditional scheme, we evaluate UDPM on FFHQ and AFHQv2 datasets, where each dataset contains \(50K\) and \(14K\) images, respectively. We resize the images to 64\(\)64 and train the UNet using Algorithm 1.

The number of sampling steps required by UDPM is two orders of magnitude smaller than the original denoising diffusion models [14; 34], one order of magnitude less than [35; 18], and up-to 4 times smaller than many recent sophisticated samplers [24; 25; 10] designed for accelerating diffusion generation. Particularly, since the size of latent variables increases progressively when sampling, UDPM requires less total computations than a single denoising step used in traditional denoising diffusion, as detailed in 4.3. While reducing the computational costs, UDPM retains great image generation quality, as can be seen in Figures 3 and 2.

In addition to the decreased number of sampling steps, UDPM has much smaller latent dimensions. This property allows us to smoothly interpolate the latent space, similar to what has been done in GANs . Figures 5, 10, and 11 show the results, where the corner images are randomly generated and the in-between images are generated by averaging the noises used to generate the corner images:

\[_{l}(i,j)=_{i}(_{j}_{1}^{l}+^{ 2}}_{2}^{l})+^{2}}(_{j}_{3}^{l}+ {1-_{j}^{2}}_{4}^{l}),\]

where \(_{1}^{l},_{2}^{l},_{3}^{l},_{4}^{l}\) are the noises at diffusion step \(l\) used to generate the 4 corner images, \((i,j)\) are the indices of the interpolated image in the figure, and \((_{i},_{j})\) are the interpolation coefficients that lie in the range \(\). In the supplementary material, we investigate other interpolation regimes, such as latent variable swapping between two generated images (see Figure 13).

Another benefit of the smaller latent dimensions is that it allows better interpretability of the generative model. While in previous diffusion approaches one needed to add a conditioning mechanism to the network to be able to control the generation, in UDPM, one may control the generation by modifying only the noise maps of the diffusion, as can be seen in Figures 6 and 12.

Figure 6: Latent space perturbation for \(64 64\) generated images. The original image is on the left. To its right we present images that were generated by adding a small noise to the latent noise from diffusion step \(l\). As can be seen, the initial diffusion step (\(l=1\)) controls the fine details of the image, while the final diffusion step (\(l=3\)) changes the semantics of the image.

### Class Conditional Generation

Similarly to previous works, we also propose a conditional generation scheme for UDPM, where we use the label encoding block used in  to control the generation class. We evaluate UDPM on the popular CIFAR10  dataset, which has 50K \(32 32\) images of 10 different classes.

Figure 14 shows the generation results of UDPM on CIFAR10. We also compare the performance of UDPM empirically to many other state-of-the-art non-distillation diffusion methods in Table 1, where we examine the FID results on 50K images generated from the 10 classes equally. For the baseline methods, we show the original results presented in their papers and the results when the number of steps is reduced to 5 diffusion steps. As can be seen, when the number of steps is limited to 5, UDPM outperforms all the methods, while also requiring considerably less computation; specifically, \(1/3\) of a single typical diffusion step.

### Runtime

Unlike typical denoising diffusion models, UDPM decreases the dimensions of the latent variables when proceeding with the diffusion process. Thus, the computations required to run the reverse diffusion process for sampling are significantly less extensive. Specifically, sampling a \(64 64\) image using a single denoising diffusion step requires 40.62 GFLOPS, while UDPM samples images with the same network structure using 13.35 GFLOPs, which is equivalent to \(30\%\) of the computations. The reason is that in its 3 steps, the input dimensions of the UDPM network are \(8 8\), \(16 16\), and \(32 32\)

When comparing the runtimes, UDPM can sample \(64 64\) images from FFHQ and AFHQv2 datasets at the rate of 765.21 FPS (frames per second) when benchmarked on NVIDIA RTX A6000 GPU, while a single denoising diffusion step can be run at the rate of 255 FPS using a very similar network.

## 5 Ablation studies

To further understand the diffusion process captured by UDPM, we present two ablation studies in which we examine the effect of each diffusion step on the generation results and demonstrate the effectiveness of the perceptual loss terms used to train the network.

**PCA of the generations.** Given the noise maps of a generated image, we marginally change the single diffusion step and then perform Principal Component Analysis (PCA) on the generated images, we then examine the top principal components to understand what features are captured in each diffusion step. In Figure 8 we present the top 8 principal components when the \(l\)-th noise map is using small noise, as can be seen, the initial diffusion step controls the fine details while the final step controls the semantics and the overall structure of the image. Similarly, in Figure 9 we show the top 8 principal components of the generated images when a single noise map is replaced entirely.

**Perceptual loss.** From our experiments we saw that using \(\|\|_{p}\) for training \(f_{}()\) leads to very soft results. Hence we use the more sophisticated loss term in (16), which promotes perception over distortion. For evaluating the contribution of each loss term, we examine three different networks:

   & steps & FID \\  DDIM  & 10/5 & 13.36/93.51 \\  DPM-Solver  & 10/5 & 6.96/288.99 \\  EDM  & 35/5 & 1.79/35.54 \\  GENIE  & 5 & 11.20 \\  DEIS  & 5 & 15.37 \\  GGDM  & 5 & 13.77 \\  DDGAN  & 2 & 4.08 \\   TDPM  & 1 & 8.91 \\  CT  & 1 & 8.70 \\ 
**UDPM (ours)** & **<1** & **6.86** \\  

Table 1: FID scores on the CIFAR10 dataset . UDPM uses 3 steps, which are equivalent in terms of complexity to 0.3 of a single denoising step used in typical diffusion models like DDPM or EDM.

The first one is trained using \(_{}\), while the second has \(_{}\) in addition to \(_{}\) in its objective, and finally the network trained using (16). We show qualitative and quantitative comparisons in Figure 7; the advantage of the perceptual terms can be seen clearly, where the FID score and the visual comparison show that the results of using the full loss are significantly better.

## 6 Conclusion

We have proposed a new diffusion-based generative model called the Upsampling Diffusion Probabilistic Model (UPDM). Our approach reduces the number of diffusion steps required to produce high-quality images, which makes it significantly more efficient than previous solutions.

We have demonstrated the effectiveness of our approach on three different datasets: CIFAR10, FFHQ, and AFHQv2. It is capable of producing high-quality images with only 3 diffusion steps, whose computational cost is less than for one step of the original diffusion models, while significantly outperforming current state-of-the-art methods dedicated to diffusion sampling acceleration.

Furthermore, we have shown that our interpolatable latent space has potential for further exploration, particularly in the realm of image editing. Based on the initial results shown in the paper, it contains semantic directions, such as making people smile or changing their age as shown in Figure 6. Future research may explore using UDPM to perform editing operations similar to those performed in styleGAN while maintaining better generation capabilities and favorable diffusion properties.

## 7 Limitation

While our approach produces impressive generative performance, it has some limitations. One limitation of our work is the evaluation of relatively small datasets, such as CIFAR10, FFHQ, and AFHQv2, which is a consequence of our limited computational resources. This restricted us from evaluating our approach on larger and more diverse datasets. Future work could overcome this limitation by leveraging a more powerful computational infrastructure to conduct experiments on larger datasets, providing a more comprehensive validation of the model's capabilities. Additionally, while UDPM offers improved interpretability over "standard" denoising diffusion models, it still falls short compared to the interpretability shown for GANs. To address this, further research could explore enhancing the latent space structure of UDPMs to make it more interpretable, perhaps by incorporating techniques from GANs or further analyzing the currently generated latent space.

## 8 Acknowledgement

This research was partially supported by the Israeli Innovation Authority through the meta consortium. The authors are grateful to the Neubauer family for their kind support.

Figure 7: Visual comparison of the loss terms effect on the FFHQ64 dataset generation results.