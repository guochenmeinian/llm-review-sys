# Flow Priors for Linear Inverse Problems via

Iterative Corrupted Trajectory Matching

 Yasi Zhang

UCLA

yasminzhang@ucla.edu

&Peiyu Yu

UCLA

yupeiyu98@g.ucla.edu

&Yaxuan Zhu

UCLA

yaxuanzhu@g.ucla.edu

&Yingshan Chang

CMU

yingshac@andrew.cmu.edu

&Feng Gao

Corresponding author: yu@stat.ucla.edu

&Ying Nian Wu

UCLA

ywu@stat.ucla.edu

&Oscar Leong

UCLA

oleong@stat.ucla.edu

This work is not related to the author's position at Amazon.

###### Abstract

Generative models based on flow matching have attracted significant attention for their simplicity and superior performance in high-resolution image synthesis. By leveraging the instantaneous change-of-variables formula, one can directly compute image likelihoods from a learned flow, making them enticing candidates as priors for downstream tasks such as inverse problems. In particular, a natural approach would be to incorporate such image probabilities in a maximum-a-posteriori (MAP) estimation problem. A major obstacle, however, lies in the slow computation of the log-likelihood, as it requires backpropagating through an ODE solver, which can be prohibitively slow for high-dimensional problems. In this work, we propose an iterative algorithm to approximate the MAP estimator efficiently to solve a variety of linear inverse problems. Our algorithm is mathematically justified by the observation that the MAP objective can be approximated by a sum of \(N\) "local MAP" objectives, where \(N\) is the number of function evaluations. By leveraging Tweedie's formula, we show that we can perform gradient steps to sequentially optimize these objectives. We validate our approach for various linear inverse problems, such as super-resolution, deblurring, inpainting, and compressed sensing, and demonstrate that we can outperform other methods based on flow matching. Code is available at [https://github.com/YasminZhang/ICTM](https://github.com/YasminZhang/ICTM).

## 1 Introduction

Linear inverse problems are ubiquitous across many imaging domains, pervading areas such as astronomy , medical imaging , and seismology . In these problems the goal is to reconstruct an unknown image \(x_{*}^{n}\) from observed measurements \(y^{m}\) of the form:

\[y=(x_{*})+, \]

where \(:^{n}^{m}\) with \(m n\) is a linear operator that degrades the clean image \(x_{*}\), and the additive noise is drawn from a known distribution. In this work, we assume the noise follows \((0,_{y}^{2}I)\). Due to the under-constrained nature of such problems, they are typically ill-posed, i.e.,there are an infinite number of undesirable images that fit to the observed measurements. Hence, one requires further structural information about the underlying images, which constitutes our prior.

With the advent of large generative models , there has been a surge of interest in exploiting generative models as priors to solve inverse problems. Given a pretrained generator to sample from a distribution or grant access to image probabilities, one can solve a variety of inverse problems in a task- or forward model-agnostic fashion, without the need for large-scale supervision . This has been successfully done for a variety of models, including implicit generators such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) , invertible generators such as Normalizing Flows , and more recently Diffusion models .

A recent paradigm in generative modeling , based on the concept of flow matching , has made significant strides in scaling ODE-based generators to high-resolution images. Flow matching models map a simple base distribution, such as a Gaussian, to a complex, high-dimensional data distribution by defining a flow field that represents the transformation between these distributions. These generative models have demonstrated scalability to high dimensions, forming the backbone of several state-of-the-art generative models . Moreover, flow matching models follow straighter and more direct probability paths compared to diffusion models, allowing for more efficient and faster sampling . Additionally, due to their invertibility, flow matching models provide direct access to image likelihoods through the instantaneous change-of-variables formula . Given these advantages and the relatively recent application of these models to inverse problems , we investigate their use as image priors in this work.

Leveraging knowledge about the corruption process \(p(y|x)\) and a natural image prior \(p(x)\), the Bayesian approach suggests analyzing the image reconstruction posterior \(p(x|y) p(y|x)p(x)\) to solve the inverse problem. A proven and effective method based on this approach is maximum-a-posteriori (MAP) estimation , which maximizes the posterior to identify the image most likely to match the observed measurements:

\[*{argmin}_{x^{n}}- p(x|y)=*{argmin} _{x^{n}}- p(y|x)- p(x). \]

MAP estimation provides a single, most probable point estimate of the posterior distribution, making it simple and interpretable. This deterministic approach ensures consistency and reproducibility, which are essential in applications requiring reliable outcomes, particularly in compressed sensing tasks such as Computed Tomography (CT)  and Magnetic Resonance Imaging (MRI) . While posterior sampling methods can offer diverse reconstructions to quantify uncertainty, they can be prohibitively slow in high-dimensions . Hence, in this work, we propose to integrate flow priors to solve linear inverse problems by MAP estimation.

A significant challenge in employing flow priors for MAP estimation lies in the slow computation of the image probabilities, as it requires backpropagating through an ODE solver . In this work, we show how one can address this challenge via Iterative Corrupted Trajectory Matching (ICTM), a novel algorithm to approximate the MAP solution in a computaionally efficient manner. In particular, we show how one can approximately find an MAP solution by sequentially optimizing a novel simpler, auxillary objective that approximates the true MAP objective in the limit of infinite function evaluations. For finite evaluations, we demonstrate that this approximation is sufficient to optimize by showcasing strong empirical performance for flow priors across a variety of linear inverse problems. We summarize our **contributions** as follows:

1. We propose ICTM, an algorithm to approximate the MAP solution to a variety of linear inverse problems using a flow prior. This algorithm optimizes an auxillary objective that partitions the flow model's trajectory into \(N\) "local MAP" objectives, where \(N\) is the number of function evaluations (NFEs). By leveraging Tweedie's formula, we show that we can perform gradient steps to sequentially optimize these objectives.
2. Theoretically, we demonstrate that the auxillary objective converges to the true MAP objective as the NFEs goes to infinity. We validate the correctness of our algorithm in finding the MAP solution on a denoising problem.
3. We demonstrate the utility of ICTM on a wide variety of linear inverse problems on both natural and scientific image datasets, with problems including denoising, inpainting, super-resolution, deblurring, and compressed sensing. Extensive results show that ICTM is both computationaly efficient and obtains high-quality reconstructions, outperforming other reconstruction algorithms based on flow priors.

Background

NotationWe follow the convention for flow-based models, where Gaussian noise is sampled at timestep 0, and the clean image corresponds to timestep 1. Note that this is the opposite of diffusion models. For \(t\), we denote \(x_{t}(x_{0})\) as the point at time \(t\) whose initial condition is \(x_{0}\). In this work, we use \(x\) and \(x_{1}\) interchangeably, i.e., \(x_{1}(x_{0})=x(x_{0})\).

### Flow-Based Models

We consider generative models that map samples \(x_{0}\) from a noise distribution \(p(x_{0})\), e.g., Gaussian, to samples \(x_{1}\) of a data distribution \(p(x_{1})\) using an ordinary differential equation (ODE):

\[dx_{t}=v_{}(x_{t},t)\,dt, \]

where the velocity field \(v\) is a \(\)-parameterized neural network, e.g., using a UNet [28; 29; 42] or Transformer [13; 51] architecture. Generative models based on flow matching [28; 29] can be seen as a simulation-free approach to learning the velocity field. This approach involves pre-determining paths that the ODE should follow by specifying the interpolation curve \(x_{t}\), rather than relying on the MLE algorithm to implicitly discover them . To construct such a path, which is not necessarily Markovian, one can define a **differentiable** nonlinear interpolation between \(x_{0}\) and \(x_{1}\):

\[x_{t}=_{t}x_{1}+_{t}x_{0}, x_{0}(0,I), \]

where both \(_{t}\) and \(_{t}\) are differentiable functions with respect to \(t\) satisfying \(_{0}=0\), \(_{0}=1\), and \(_{1}=1\), \(_{1}=0\). This ensures that \(x_{t}\) is transported from a standard Gaussian distribution to the natural image manifold from time 0 to time 1. In contrast, the diffusion process [48; 45; 20] induces a non-differentiable trajectory due to the diffusion term in the SDE formulation.

The idea behind flow matching is to utilize the power of deep neural networks to efficiently predict the velocity field at each timestep. To achieve this, we can train the neural network by minimizing an \(L_{2}\) loss between the sampled velocity and the one predicted by the neural network:

\[()=_{t,p(x_{1}),p(x_{0})}\|v_{}(x_{t},t)-( _{t}x_{1}+_{t}x_{0})\|^{2}. \]

We denote the optimal (not necessarily unique) solution to \(_{}()\) as \(\). The optimal velocity field \(v_{}\) can be derived in closed form and is the expected velocity at state \(x_{t}\):

\[v_{}(x_{t},t)=_{p(x_{1}),p(x_{0})}[_{t}x_{1} +_{t}x_{0} x_{t}]. \]

For convenience, in the following text, we use \(v_{}\) to refer to the optimal \(v_{}\). In the rest of the paper, we assume that the flow \(v_{}\) and its parameters are pretrained on a dataset of interest and fixed. We are then interested in leveraging its utility as a prior to solve inverse problems.

### Probability Computation for Flow Priors

Denote the probability of \(x_{t}\) in Eq. (3) as \(p(x_{t})\) dependent on time. Assuming that \(v_{}\) is uniformly Lipschitz continuous in \(x_{t}\) and continuous in \(t\), the change in log probability also follows a differential equation [9; 18]:

\[)}{ t}=-(v_{}(x_{t},t)). \]

One can additionally obtain the likelihood of the trajectory via integrating Eq. (7) across time

\[ p(x_{t})= p(x_{})-_{}^{t}(v_{}(x_{s},s))ds,\;0<t 1. \]

## 3 Method

In this work, we aim to solve the MAP estimation problem in Eq. (2) where \(p(x)\) is given by a pretrained flow prior. We first discuss in Section 3.1 how the MAP problem could, in principle, be solved via a latent-space optimization problem. As we will see, this problem is challenging to solve computationally due to the need to backpropagate through an ODE solver. To overcome this, we show in Section 3.2 that the ideal MAP problem can be approximated by a weighted sum of "local MAP" optimization problems, which operates by partitioning the flow's trajectory to a reconstructed solution. We then introduce our ICTM algorithm to sequentially optimize this auxiliary objective. Finally, in Section 3.3, we experimentally validate that our algorithm finds a solution that is faithful to the MAP estimate in a simplified setting where the globally optimal MAP solution is known.

### Flow-Based MAP

Given a pretrained flow prior, one can compute the log-likelihood of \(x\) generated from an initial noise sample \(x_{0}\) via Eq. (8). Hence, to find the MAP estimate, one could equivalently optimize the initial point of the trajectory \(x_{0}\) and return \(x_{1}(x_{0})\) where \(x_{0}\) is found by solving

\[_{x_{0}^{n}}\ _{^{2}}\|y- (x_{1}(x_{0}))\|^{2}}_{}+ \|x_{0}\|^{2}+_{0}^{1}(v_ {}(x_{t},t))dt}_{}, \]

where \(x_{t}:=x_{t}(x_{0})\) denotes the intermediate state \(x_{t}\) generated from \(x_{0}\). Intuitively, this loss encourages finding an initial point \(x_{0}\) such that the reconstruction \(x_{1}:=x_{1}(x_{0})\) fits the observed measurements, but is also likely to be generated by the flow.

In practice, \(x_{1}\) and the prior term can be approximated by an ODE solver. The trajectory of \(x_{t}=x_{0}+_{0}^{t}v_{}(x_{t},t)dt\) can be approximated by an ODE sampler, i.e. ODESolve\((x_{0},0,t,v_{})\), where \(x_{0}\) is the initial point, and the second and third arguments represent the starting time and the ending time, respectively. For example, with an Euler sampler, we iterate over \(x_{t+ t}=x_{t}+v_{}(x_{t},t) t\) where \( t=1/N\) and \(N\) is the predetermined NFEs. After acquiring the optimal \(_{0}\) by optimizing the Eq. (9), we obtain the MAP solution \(x_{1}\) by using ODESolve\((_{0},0,1,v_{})\) again.

### Flow-Based MAP Approximation

The global flow-based MAP objective Eq. (9) is tractable for low-dimensional problems. The challenge for high-dimensional problems, however, is that optimizing Eq. (9) is simulation-based, and thus each update iteration requires full forward and backward propagation through an ODE solver, resulting in issues regarding memory inefficiency and time, making it hard to optimize [9; 15; 16; 47].

As a way to address this, we prove a result in Theorem 1 that shows that the MAP objective can be approximated by a weighted sum of \(N\) local posterior objectives. These objectives are "local" in the sense that they mainly depend on likelihoods and probabilities of intermediate trajectories \(x_{t}\) and \(x_{t}+v_{}(x_{t},t) t\) for \(t=0, t,,N t\) where \( t:=1/N\). Given an initial noise input \(x_{0}\), each local posterior objective depends on a non-Markovian **auxiliary path**\(y_{t}=_{t}y+_{t}(x_{0})\) by connecting the points between \(y\) and \(x_{0}\). We prove this result for straight paths \(_{t}=t\) and \(_{t}=1-t\) for simplicity, but other interpolation paths can be used. The proof is in Section A.2.

**Theorem 1**.: _For \(N 1\), set \(_{i}:=()^{N-i+1}\) and \( t=1/N\). Suppose \(y=(x_{*})+\) where \(x_{*}=x_{1}(x_{0})\) with \(x_{0}\) being the solution to Eq. (9), \((0,_{y}^{2}I)\), and \(x_{t}\) exactly follows the straight path \(x_{t}=tx+(1-t)x_{0}\) for any timestep \(t\). Suppose the velocity field \(v_{}:^{n}^{n}\) satisfies \(_{z^{n},s}|v_{}(z,s)| C_{1}\) for some universal constant \(C_{1}\). Then, there exists a constant \(c(N)\)2 that does not depend on \(x_{0}\) such that_

\[_{N}| p(x(x_{0})|y)-_{i=1}^{N}_{i} }_{i}-c(N)|=0,\]

_where \(}_{i}= p(x_{(i-1) t})-((x_{(i-1) t},(i-1) t)}{ x})  t+ p(y_{i t}|x_{i t})\)._

This result shows that the true MAP objective evaluated at the optimal solution can be approximated by a weighted sum of objectives that depend locally at a time \(t\) for the trajectory \(\{x_{t}:t\}\). The intuition regarding \(}_{i}\) arises from the fact that \(}_{i}_{i}\), where \(_{i}\) is the local posterior distribution

\[_{i}= p(y_{i t}|x_{i t}(x_{(i-1) t}))+ p(x _{i t}).\]Optimizing each of these local posterior distributions in a sequential fashion captures the fact that we would like each intermediate point in our trajectory \(x_{i t}\) to be likely and fit to our measurements, ideally resulting in a final reconstruction \(x_{1}\) that satisfies this as well. The benefit of \(_{i}}\), as we will show in the sequel, is that it is efficient to optimize.

Discussion of assumptions:We assume that the trajectory \(\{x_{t}\}_{t}\) exactly follows the predefined interpolation path \(\{_{t}x+_{t}x_{0}\}_{t}\). In Section B of the appendix, we analyze this assumption and show that we can bound the deviation from the predefined interpolation path to the learned path via a path compliance measure. Moreover, we impose a regularity assumption on the velocity field \(v_{}\), effectively requiring a uniform bound on the spectrum of the Jacobian of \(v_{}\). This can be easily satisfied with neural networks using Lipschitz continuous and differentiable activation functions.

As we see in Theorem 1, one can approximate the true MAP objective via a sum of local objectives of the form

\[_{i}}:=|x_{i t})}_{ {local data likelihood}}+)- ((x_{(i-1) t},(i-1) t)}{ x} ) t}_{}. \]

At first glance, \(_{i}}\) still appears challenging to optimize, but there are additional insights we can exploit for computation. We discuss each term in \(_{i}}\) below.

Local data likelihood:The intuition behind ICTM is that we aim to match a **corrupted** trajectory \(\{u_{t}\}_{t}\) with an auxiliary path \(\{y_{t}\}_{t}\) specified by an interpolation between our measurements \(y\) and \((x_{0})\) for each timestep \(t\), defined by \(y_{t}:=_{t}y+_{t}(x_{0})\). The corrupted trajectory \(u_{t}:=(x_{t})\) follows the **corrupted flow ODE**\(du_{t}=(v_{}(x_{t},t))dt\). To optimize the above "local MAP" objectives, we must understand the distribution of \(p(y_{t}|x_{t})\). Generally speaking, this distribution is intractable. However, by assuming exact **compliance** of the trajectory generated by flow to the predefined interpolation path (as done in Theorem 1), we can show that \(y_{t}|x_{t}(u_{t},_{t}^{2}_{y}^{2})\). This is proven in Lemma 3 in the appendix. While exact compliance of the trajectory may not hold for learned flow matching models, we show empirically that making this assumption leads to strong performance in practice. We further analyze this notion of compliance in Section B of the appendix.

Local prior:The approximation in Eq. (10) addresses one of the main concerns of MAP in that the intensive integral computation is circumvented with a simpler Riemannian sum. This approximation holds for small time increments \( t\): \(_{t}^{t+ t}(v_{ }(x_{s},s))ds(v_{}(x_{t},t)) t\). Note that one can additionally improve the efficiency of this term by employing a Hutchinson-Skilling estimate  for the trace of the Jacobian matrix. However, at first glance, it appears we have simply shifted the problem to the computation of the prior at timestep \((i-1) t\). Fortunately, it is possible to derive a formula for the gradient of \( p(x_{t})\) for all timesteps \(t\) using Tweedie's formula . This allows us to optimize each objective \(_{i}}\) using gradient-based optimizers. The following result gives a precise characterization of \(_{x_{t}} p(x_{t})\), proven in Section A.1.

**Proposition 1**.: _Let \(_{t}=_{t}/_{t}\) denote the signal-to-noise ratio. The relationship between the score function \(_{x_{t}} p(x_{t})\) and the velocity field \(v_{}(x_{t},t)\) is given by:_

\[_{x_{t}} p(x_{t})=^{2}}[(}{dt})^{-1}(v_{}(x_{t},t)-}{ dt}x_{t})-x_{t}]. \]

In summary, we have derived an efficient approximation to the MAP objective. For our algorithm, we iteratively optimize each term \(_{t}}\) sequentially for each \(t=0, t,,N t\), fitting our current iterate \(x_{t}\) to induce an increment \(x_{t+ t}\) such that \((x_{t+ t})\) fits to our auxiliary corrupted path \(y_{t+ t}\) while

Figure 1: **Illustration of the idea of ICTM.** The corrupted trajectory \(u_{t}:=(x_{t})\) follows the corrupted flow ODE \(du_{t}=(v_{}(x_{t},t))dt\).

being likely under our local prior. We call this approach Iterative Corrupted Trajectory Matching (ICTM). Our algorithm is summarized in Algo. 1. In lines 7 and 12, instead of directly optimizing the local data likelihood, we choose \(\) as a new hyper-parameter to tune. We find a constant \(\) works well in practice.

```
0: measurement \(y\), matrix \(\), pretrained flow-based model \(\), NFEs \(N\), interpolation coefficients \(\{_{t}\}_{t}\) and \(\{_{t}\}_{t}\), step size \(\), guidance weight \(\), and iteration number \(K\)
0: recovered clean image \(x_{1}\)
1:Initialize\((0,I)\), \(x_{0}\), \(t 0\), \( t 1/N\)
2:Generate an auxiliary path \(y_{s}=_{s}y+_{s}(x_{0})\) for \(s(0,1)\)
3:while\(t<1\)do
4:\(x_{t+ t} x_{t}+v_{}(x_{t},t) t\)
5:if\(t=0\)then
6:for\(k=1, K\)do
7:\(x_{t} x_{t}-_{x_{t}}[\|(x_{t+ t }(x_{t}))-y_{t+ t}\|^{2}+\|x_{t}\|^{2}+ ((x_{t},t)}{ x}) t]\)
8:endfor
9:else
10:for\(k=1, K\)do
11:# use Eq. (11) to obtain the gradient of \( p(x_{t})\)
12:\(x_{t} x_{t}-_{x_{t}}[\|(x_{t+ t }(x_{t}))-y_{t+ t}\|^{2}- p(x_{t})+((x_{t},t)}{ x}) t]\)
13:endfor
14:endif
15:\(x_{t+ t} x_{t}+v_{}(x_{t},t) t\)
16:\(t t+ t\)
17:endwhile
18:return\(x_{1}\)
```

**Algorithm 1** Iterative Corrupted Trajectory Matching (ICTM) with Euler Sampler

### Toy Example Validation

We experimentally validate that the reconstruction found via ICTM is close to the optimal MAP solution in a simplified denoising problem where the MAP solution can be obtained in closed-form. Specifically, we fit a Gaussian distribution \((,)\) using 1,000 samples from the FFHQ dataset. Consider a denoising problem \(y=x+\) where \(x(,)\) and \((0,_{y}^{2}I)\). In this case, the analytical solution to the MAP estimation problem (Eq. (2)) is \(x_{*}=(^{-1}+_{y}^{-2}I)^{-1}(^{-1}+_{y}^{-2}y)\). We set \(_{y}=0.1\). Then, we train a flow-based model on 10,000 samples from the true Gaussian distribution and showcase the deviation of our reconstruction found via ICTM to the closed-form MAP solution \(x_{*}\) in Fig. 2. We see that ICTM can obtain a faithful estimate of the MAP solution across many samples.

Figure 2: Results of a toy example modeling 1,000 FFHQ faces as a Gaussian distribution. Subfigure (a) shows the qualitative results of our method; Subfigure (b) presents the histogram of the differences between ours and the true MAP; Subfigure (c) displays the MSE values as the NFEs varies.

## 4 Experiments

In our experimental setting, we use optimal transport interpolation coefficients, i.e. \(_{t}=t\) and \(_{t}=1-t\). We test our algorithm on both natural and medical imaging datasets. For natural images, we utilize the pretrained checkpoint from the official Rectified Flow repository3 and evaluate our approach on the CelebA-HQ dataset [31; 24]. We address four common linear inverse problems: super-resolution, inpainting with a random mask, Gaussian deblurring, and inpainting with a box mask. For the medical application, we train a flow-based model from scratch on the Human Connectome Project (HCP) dataset  and test our algorithm specifically for compressed sensing at different compression rates. Our algorithm focuses on the reconstruction faithfulness of generated images, therefore employing PSNR and SSIM  as evaluation metrics.

BaselinesWe compare our method with five baselines. 1) OT-ODE . To our knowledge, this is the only baseline that applies flow-based models to inverse problems. They incorporate a prior gradient correction at each sampling step based on conditional Optimal Transport (OT) paths. For a fair comparison, we follow their implementation of Algorithm 1, providing detailed ablations on initialization time \(t^{}\) in Appendix E.3. 2) DPS-ODE. Inspired by DPS , we replace the velocity field with a conditional one, i.e., \(v(x_{t}|y)=v(x_{t})+_{t}_{x_{t}} p(y|_{1}(x_{t}))\), where \(_{t}\) is a hyperparameter to tune. Following the hyperparameter instruction in DPS, we provide detailed ablations on \(_{t}\) in Appendix E.3. 3) Ours without local prior. To examine the local prior term's effectiveness in our optimization algorithm, we drop the local prior term as defined in Eq. (10) in our algorithm. In the experiments with natural images, in addition to the flow-based baselines, we have included two representative diffusion-based baselines: 4) RED-Diff , a variational Bayes-based method; and 5) IIGDM , an advanced MCMC-based method. We also note one concurrent work, D-Flow , which formulates the MAP as a constrained optimization problem in their Eq. 9. As documented in their Sec. 3.4, it takes 5-10 minutes to recover each image. This is because each of its optimization step requires backpropagation through an ODE solver to compute the full log-likelihood.

    &  &  &  &  \\  Method & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM \\  OT-ODE & 27.46 & 0.775 & 28.57 & 0.838 & 26.28 & 0.727 & 19.80 & 0.795 \\ DPS-ODE & 27.85 & 0.791 & 29.57 & 0.872 & 25.97 & 0.704 & 23.59 & 0.758 \\ RED-Diff & 27.20 & 0.760 & 25.13 & 0.711 & 27.23 & 0.765 & 17.50 & 0.651 \\ IIGDM & 28.33 & 0.803 & 29.98 & 0.858 & 24.30 & 0.583 & 24.10 & 0.853 \\ Ours (w/o prior) & 26.06 & 0.724 & 29.01 & 0.835 & 25.13 & 0.676 & 22.42 & 0.803 \\ Ours & 27.91 & 0.805 & 30.65 & 0.894 & 26.54 & 0.760 & 24.34 & 0.866 \\   

Table 1: Quantitative comparison results in terms of PSNR and SSIM on the CelebA-HQ dataset. Our algorithm surpasses all other baselines across all tasks. The best values are highlighted in blue and the second-best are underlined.

Figure 3: Qualitative comparison results on the CelebA-HQ dataset. The reconstructions generated by our method align more faithfully with the ground truth and exhibit a higher degree of refinement.

In contrast, our method is significantly faster (approximately 1.6 minutes per image) due to our principled local MAP approximation, as demonstrated in Appendix D.

### Natural Images

Experimental setupWe evaluate our algorithm using 100 images from the CelebA-HQ validation set with a resolution of 256\(\)256, normalizing all images to the \(\) range for quantitative analysis. All experiments incorporate Gaussian measurement noise with \(_{y}=0.01\). We address the following linear inverse problems: (1) 4\(\) super-resolution using bicubic downsampling, (2) inpainting with a random mask covering 70% of missing values, (3) Gaussian deblurring with a 61\(\)61 kernel and a standard deviation of 3.0, and (4) box inpainting with a centered 128\(\)128 mask.

We present the quantitative and qualitative results of all the methods in Tab. 1 and Fig. 3, respectively. In Tab. 1, our method surpasses all other baselines across all tasks. For more challenging tasks such as Gaussian deblurring and box inpainting, our method significantly outperforms others in terms of SSIM. Based on the MAP framework, as shown in Fig. 3, our method prefers more faithful and artifact-free reconstructions, whereas others trade off for perceptual quality. We note that there is an unavoidable tradeoff between perceptual quality and restoration faithfulness . Overall, our method presents a higher degree of refinement. The comparison between ours and ours (w/o prior) indicates the effectiveness of the local prior term in enhancing the accuracy of the reconstructions, as evidenced by the increases in both PSNR and SSIM.

### Medical application

HCP T2w datasetWe utilize images from the publicly available Human Connectome Project (HCP)  T2-weighted (T2w) images dataset for the task of compressed sensing, which contains brain images from 47 patients. The HCP dataset includes cross-sectional images of the brain taken at different levels and angles.

    &  &  &  \\  Method & PSNR & SSIM & PSNR & SSIM & PSNR & SSIM \\  Wavelet Prior & 18.02 \(\) 1.38 & 0.495 \(\) 0.02 & 11.99 \(\) 1.34 & 0.230 \(\) 0.02 & 7.37 \(\) 1.85 & 0.090 \(\) 0.02 \\ TV Prior & 25.36 \(\) 2.79 & 0.657 \(\) 0.04 & 18.70 \(\) 2.36 & 0.496 \(\) 0.03 & 14.38 \(\) 3.04 & 0.309 \(\) 0.04 \\  OT-ODE & 18.71 \(\) 1.02 & 0.422 \(\) 0.17 & 18.16 \(\) 1.06 & 0.271 \(\) 0.07 & 12.21 \(\) 1.43 & 0.096 \(\) 0.04 \\ DPS-ODE & 31.06 \(\) 3.91 & 0.765 \(\) 0.08 & 25.01 \(\) 1.87 & 0.608 \(\) 0.08 & 22.06 \(\) 1.66 & 0.479 \(\) 0.09 \\ Ours & 32.72 \(\) 1.53 & 0.878 \(\) 0.05 & 27.03 \(\) 1.77 & 0.733 \(\) 0.04 & 24.03 \(\) 1.23 & 0.503 \(\) 0.04 \\   

Table 2: Results of compressed sensing with varying compression rate \(\) on the HCP T2w dataset. Note that compressed sensing is more challenging due to the complexity of the forward operator, as evidenced by the poor performance of OT-ODE, which assumes a Gaussian distribution of measurement \(y\) given \(x_{t}\). The best values are highlighted in blue.

Figure 4: Qualitative comparison results on compressed sensing. Our method produces more faithful reconstructions with fewer artifacts, ensuring higher accuracy and clarity in the details.

Compressed sensingWe train a flow-based model from scratch on 10,000 randomly sampled images, utilizing the _ncsmpp_ architecture  with minor adaptations for grayscale images. We employ compression rates \(\{1/2,1/4,1/10\}\), meaning \(m= n\). The measurement operator is given by a subsampled Fourier matrix, whose sign patterns are randomly selected. We evaluate our reconstruction algorithm's performance on 200 randomly sampled test images.

We present the quantitative and qualitative results of compressed sensing in Tab. 2 and Fig. 4, respectively. In addition to flow-based methods, we include results for two classical recovery algorithms, Wavelet  and TV  priors. As shown in Tab. 2, our method outperforms the classical recovery algorithms and other flow-based baselines across varying compression rates \(\), demonstrating our method's capability to handle challenging scenarios and the advantages of utilizing modern generative models as priors. In Fig. 4, our method produces reconstructions that are more faithful to the original images, with fewer artifacts, leading to higher accuracy and clearer details.

### Ablation studies

We use the Adam optimizer  for our optimization steps due to its effectiveness in neural network computations. For all tasks, we utilize \(N=100\) steps.

Step size \(\) and Guidance weight \(\)The use of the Adam optimizer ensures that the choice of hyperparameters, particularly the step size \(\) and the guidance weight \(\), remains consistent across various tasks, as illustrated in Fig. 5. Specifically, a step size of \(=10^{-2}\) is optimal for Inpainting (random), Inpainting (box), and Super-resolution in terms of SSIM. For PSNR, Gaussian deblurring also achieves optimal performance at \(=10^{-2}\). Consequently, we employ \(=10^{-2}\) for all tasks. Based on the results shown in the right two subfigures of Fig. 5, we select \(=10^{3}\) for Gaussian

Figure 5: Ablation results of step size \(\) and guidance weight \(\). The choice of hyperparameters for our algorithm is fairly consistent across all tasks. We choose \(=10^{-2}\) for all experiments on CelebA-HQ. For \(\), we choose \(=10^{3}\) for Gaussian deblurring and \(=10^{4}\) for the other tasks.

Figure 6: Ablation results of iteration number \(K\) on different tasks. For super-resolution and the other three tasks, \(K=1\) is sufficient to achieve the best performance with the optimal step size \(\) and guidance weight \(\). However, for compressed sensing, it is necessary to increase \(K\) to obtain the best performance. We hypothesize that this is due to the increased complexity of the compressed sensing operator, which requires more iteration steps to ensure the correct optimization direction.

deblurring and \(=10^{4}\) for the other tasks. This consistency extends to the compressed sensing experiments, where we set \(=10^{3}\) and \(=10^{-2}\) for all experiments involving medical images.

Iteration number \(K\)We present ablation results of the iteration number \(K\) on different tasks in Fig. 6. We focus on the behavior of \(K\) in super-resolution and compressed sensing, as it performs similarly to super-resolution in the other three tasks. With the optimal choice of \(\) and \(\) in super-resolution, i.e., \(=10^{-2}\) and \(=10^{3}\), \(K=1\) provides superior performance on CelebA-HQ. A decreased step size, e.g., \(=10^{-3}\), can help performance as \(K\) increases, but it fails to exceed the performance achieved with the optimal parameters at \(K=1\). However, for compressed sensing, it is necessary to increase \(K\) to achieve the best performance. Consequently, we set \(K=10\) for all compressed sensing experiments. We hypothesize that the complexity of the compressed sensing operator directly determines the number of iterations required for optimal performance.

## 5 Conclusion

In this work, we have introduced a novel iterative algorithm to incorporate flow priors to solve linear inverse problems. By addressing the computational challenges associated with the slow log-likelihood calculations inherent in flow matching models, our approach leverages the decomposition of the MAP objective into multiple "local MAP" objectives. This decomposition, combined with the application of Tweedie's formula, enables effective sequential optimization through gradient steps. Our method has been rigorously validated on both natural and scientific images across various linear inverse problems, including super-resolution, deblurring, inpainting, and compressed sensing. The empirical results indicate that our algorithm consistently outperforms existing techniques based on flow matching, highlighting its potential as a powerful tool for high-resolution image synthesis and related downstream tasks.

## 6 Limitations and Future Work

While our algorithm has demonstrated promising results, there are certain limitations that suggest avenues for future research. First, our theoretical framework, built on optimal transport interpolation paths, is currently limited and cannot be applied to solve the general interpolation between Gaussian and data distributions. Additionally, in order to broaden the applicability of flow priors for inverse problems, it is important to generalize our approach to handle nonlinear forward models. Moreover, the algorithm currently lacks the capability to quantify the uncertainty of the generated images, an aspect crucial for many scientific applications. It would be interesting to consider approaches to post-process our solutions to understand the uncertainty inherent in our reconstruction. These limitations highlight important directions for future work to enhance the robustness and applicability of our method.