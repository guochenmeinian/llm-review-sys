# Approximately Equivariant Neural Processes

Matthew Ashman

University of Cambridge

mca39@cam.ac.uk

&Cristiana Diaconu

University of Cambridge

cdd43@cam.ac.uk

&Adrian Weller

University of Cambridge

The Alan Turing Institute

aw665@cam.ac.uk

&Wessel Bruinsma

Microsoft Research AI for Science

wessel.p.bruinsma@gmail.com

&Richard E. Turner

University of Cambridge

Microsoft Research AI for Science

The Alan Turing Institute

ret23@cam.ac.uk

Equal contribution.

###### Abstract

Equivariant deep learning architectures exploit symmetries in learning problems to improve the sample efficiency of neural-network-based models and their ability to generalise. However, when modelling real-world data, learning problems are often not _exactly_ equivariant, but only approximately. For example, when estimating the global temperature field from weather station observations, local topographical features like mountains break translation equivariance. In these scenarios, it is desirable to construct architectures that can flexibly depart from exact equivariance in a data-driven way. Current approaches to achieving this cannot usually be applied out-of-the-box to any architecture and symmetry group. In this paper, we develop a general approach to achieving this using existing equivariant architectures. Our approach is agnostic to both the choice of symmetry group and model architecture, making it widely applicable. We consider the use of approximately equivariant architectures in neural processes (NPs), a popular family of meta-learning models. We demonstrate the effectiveness of our approach on a number of synthetic and real-world regression experiments, showing that approximately equivariant NP models can outperform both their non-equivariant and strictly equivariant counterparts.

## 1 Introduction

The development of equivariant deep learning architectures has spearheaded many advancements in machine learning, including CNNs (LeCun et al., 1989), group equivariant CNNs (Cohen and Welling, 2016; Finzi et al., 2020), transformers (Vaswani et al., 2017), DeepSets (Zaheer et al., 2017), and GNNs (Scarselli et al., 2008). When appropriate, equivariances provide useful inductive biases that can drastically improve sample complexity (Mei et al., 2021) and generalisation capabilities (Elesedy and Zaidi, 2021; Bulusu et al., 2021; Petrache and Trivedi, 2023) by exploiting symmetries present in the data. Yet, real-world data are seldom strictly equivariant. As an example, consider modelling daily precipitation across space and time. Such data may be close to translation equivariant--and therefore translation equivariance serves as a useful inductive bias--however, it is clear that it is not strictly translation equivariant due to geographical and seasonal variations. More generally, whilst there are aspects of modelling problems which are universal and exhibit symmetries such as equivariance (e.g. atmospheric physics) there are often unknown local factors (e.g. precise local topography) which break these symmetries and which the models do not have access to. It is thus desirable for themodel to be able to depart from strict equivariance when necessary; that is, to develop _approximately equivariant_ models.

A family of models that have benefited greatly from the use of equivariant deep learning architectures are neural processes [NPs; Garnelo et al., 2018a,b]. However, many of the problem domains that NPs are applied to exhibit only approximate equivariance. It is therefore desirable to build approximately equivariant NPs through the use of approximately equivariant deep learning architectures. Whilst there exist several approaches to constructing approximately equivariant architectures, they are limited to CNN- and MLP-based models [Wang et al., 2022a, van der Oudera et al., 2022, Finzi et al., 2021, Romero and Lohit, 2022]--which restricts their applicability to certain symmetry groups--and often require modifications to the loss function used to train the models [Finzi et al., 2021, Kim et al., 2023a]. As there exist NPs that utilise a variety of architectures, such as transformers and GNNs [Carr and Wingate, 2019, Nguyen and Grover, 2022, Kim et al., 2019, Feng et al., 2022], these approaches are not sufficient. We address this shortcoming through the development of an approach to constructing approximately equivariant models that is both architecture and symmetry-group agnostic. Importantly, our approach can be realised without modifying the core architecture of existing equivariant models, enabling use in a variety of equivariant NPs such as the convolutional conditional NP [ConvCNP; Gordon et al., 2019], the equivariant CNP [EquivCNP; Kawano et al., 2021], the steerable CNP [SteerCNP; Holderrieth et al., 2021], the translation equivariant transformer NP [TE-TNP; Ashman et al., 2024], and the relational CNP [RCNP; Huang et al., 2023].

We outline our core contributions as follows:

1. We demonstrate that under certain technical conditions, such as Holder regularity and compactness, any non-equivariant mapping between function spaces can be approximated by an equivariant mapping with fixed additional inputs (Theorems 2 and 3). This provides insight into how to construct approximately equivariant models, and generalises several existing approaches to relaxing equivariant constraints.
2. We apply this result and the insights it provides to construct approximately equivariant versions of several popular equivariant NPs. The modifications required are _very simple, yet effective_: we demonstrate improved performance relative to both strictly equivariant and non-equivariant models on a number of spatio-temporal regression tasks.

## 2 Background

We consider the supervised learning setting, where \(\), \(\) denote the input and output spaces, and \((,)\) denotes an input-output pair. Let \(=_{N=0}^{}()^{N}\) be a collection of all finite data sets, which includes the empty set \(\). Let \(_{M}=()^{M}\) be the collection of \(M\) input-output pairs and \(_{ M}=_{m=1}^{M}_{m}\) be the collection of at most \(M\) pairs. We denote a context and target set with \(_{c},\;_{t}\), where \(|_{c}|=N_{c},\,|_{t}|=N_{t}\). Let \(_{c}()^{N_{c}},\,_{c}()^{N_{c}}\) be the inputs and corresponding outputs of \(_{c}\), and let \(_{t}()^{N_{t}}\), \(_{t}()^{N_{t}}\) be defined analogously. We denote a single task as \(=(_{c},_{t})=((_{c},_{c}),( _{t},_{t}))\). Let \(()\) denote the collection of \(\)-valued stochastic processes on \(\). Let \(\) denote the parameter space for some family of probability densities, e.g. means and variances for the space of all Gaussian densities.

### Neural Processes

NPs [Garnelo et al., 2018a,b] can be viewed as neural-network-based parametrisations of _prediction maps_\(()\) from data sets \(\) to predictions \(()\), where predictions are represented by \(\)-valued stochastic processes on \(\)[Foong et al., 2020, Bruinsma, 2022]. Throughout, we denote the density of the finite-dimensional distribution of \(()\) at inputs \(\) by \(p(,)\). In this work, we restrict our attention to conditional NPs [CNPs; Garnelo et al., 2018a], which only target marginal predictive distributions by assuming that the predictive densities factorise: \(p(|,)=_{n}p(_{n}|_{n },)\). We denote all parameters of a CNP by \(\). CNPs are trained in a meta-learning fashion, in which the expected predictive log-probability is maximised:

\[_{}=_{}_{}() _{}()=_{p()} _{n=1}^{N_{t}} p_{}(_{t,n}|_{t,n}, _{c}). \]

Here, the expectation is taken with respect to the distribution over tasks \(p()\). In practice, we only have access to a finite number of tasks for training, so the expectation is approximated with an average over tasks. The global maximum is achieved if and only if the model recovers the ground-truth marginals (Proposition 3.26 by Bruinsma, 2022).

### Group Equivariance

We consider equivariance with respect to transformations in some group \(G\). For example, \(G\) can be the group of translations or the group of rotations. Mathematically, a group \(G\) is a set endowed with a binary operation \(G G G\) (denoted as multiplication) such that (i) \((fg)h=f(gh)\) for all \(f,g,h G\); (ii) there exists an identity element \(e G\) such that \(eg=ge=g\) for all \(g G\); and (iii) every element \(g G\) has an inverse \(g^{-1} G\). A \(G\)-space is a space \(X\) for which there exists a function \(G X X\) called a group action (again denoted by multiplication) such that (i) \(ex=x\) for all \(x X\); and (ii) \(f(gx)=(fg)x\) for all \(f,g G\) and \(x X\). The notion of group equivariance is used to describe mappings for which, when the input to the mapping is transformed by some \(g G\), the output of the mapping is transformed equivalently. This is formalised in the following definition.

**Definition 1** (\(G\)-equivariance).: _Let \(X\) and \(Y\) be \(G\)-spaces. Call a mapping \( X Y\)\(G\)-equivariant if \((gx)=g(x)\) for all \(g G\) and \(x X\)._

### Group-Equivariant Conditional Neural Processes

The property of \(G\)-equivariance is particularly useful in NPs when modelling ground-truth stochastic processes that exhibit \(G\)-stationarity:

**Definition 2** (\(G\)-stationary stochastic process).: _Let \(\) be a \(G\)-space. We say that a stochastic process \(P()\) is \(G\)-stationary if, for \(f P\) and all \(g G\), \(f(\,\,)\) is equal in distribution to \(f(g\,\,)\)._

Let \(P()\) be a ground-truth stochastic process. For a dataset \(\), define \(_{P}()\) by integrating \(P\) against some density \(^{}_{P}()\), such that \(_{P}()=^{}_{P}()\,P\). \(^{}_{P}()\) is the Radon-Nikodym derivative of the posterior stochastic process with respect to the prior; intuitively, \(^{}_{P}()(f)\) is proportional to the likelihood, \(^{}_{P}()(f)=p( f)/p()\), so it determines how the data is observed (e.g. under which noise?). Assume that \(^{}_{P}() 1\) so that \(_{P}()=P\). We say that \(^{}_{P}\) is \(G\)-invariant if \(^{}_{P}(g) g=^{}_{P}()\).

**Proposition 1** (\(G\)-stationarity and \(G\)-equivariance).: _The ground-truth stochastic process \(P\) is \(G\)-stationary and \(^{}_{P}\) is \(G\)-invariant if, and only if, \(_{P}\) is \(G\)-equivariant._

Proof.: This is Thm 2.1 by Ashman et al. (2024) with translations replaced by applications of \(g G\). 

Thus, for data generated from a stochastic process that is approximately \(G\)-stationary, incorporating \(G\)-equivariance into NP approximations of the corresponding prediction map can serve as a useful inductive bias that can help generalisation and improve parameter efficiency. Intuitively, requiring a NP to be equivariant effectively ties parameters together, which significantly reduces the search space during optimisation, enabling better solutions to be found with fewer data. The improved generalisation capabilities of \(G\)-equivariant models is formalised by Elesedy and Zaidi (2021), Bulusu et al. (2021), Petrache and Trivedi (2023).

For a CNP, assume that every marginal \(p(\,\,|,)\) is in some fixed parametric family with parameters \((;)\). For example, \(\) could consist of the mean and variance for a Gaussian distribution. Let \(C(,)\) denote the set of continuous functions \(\). For a CNP, define the associated _parameter map_\( C(,)\) by \(()()=(;)\). Intuitively, the parameter map \(\) maps a dataset \(\) to a function \(()\) giving the parameters \(()()\) for every input \(\). Assume that \(\) is a \(G\)-space. We turn \(\) and \(C(,)\) into \(G\)-spaces by applying \(g\) to the inputs: \(g\) consists of the input-output pairs \((g_{n},_{n})\), and \(g(;) C(,)\) is defined by \(g(;)=(g^{-1};)\).

**Definition 3** (\(G\)-equivariant CNP).: _A CNP is \(G\)-equivariant if the associated parameter map \(\) is \(G\)-equivariant: \((g)=g()\) for all datasets \(\)._

To parametrise a \(G\)-equivariant CNP, we must parametrise the associated parameter map \(\). Here we present a general construction by Kawano et al. (2021).

**Theorem 1** (Representation of \(G\)-equivariant CNPs, Theorem 2 by Kawano et al. (2021)).: _Let \(^{D}\) be compact. Consider an appropriate collection \(^{}_{ M}_{ M}\). Then a function \(^{}_{ M}^{}_{ M}\)\(C(,)\) is continuous, permutation-invariant, and \(G\)-equivariant if and only if it is of the form_

\[()=(f_{}()) f_{ }((_{1},_{1}),,(_{m},_{ m}))=_{i=1}^{m}(_{i})(\,\,,_{i}) \]

_for some continuous \(^{2D}\), an appropriate \(G\)-invariant positive-definite kernel \(^{2}\) (i.e. \((_{n},_{m})=(g_{n},g_{m})\) for all \(g G\)), and some continuous and \(G\)-equivariant \( C(,)\) with \(\) an appropriate \(G\)-invariant space of functions (i.e. closed under \(g G\))._

Theorem 1 naturally gives rise to architectures which can be deconstructed into two components: an _encoder_ and a _decoder_. The encoder, \(f_{}\), maps datasets to some embedding space \(\). The decoder, \( C(,)\), takes this representation and maps to a function that gives the parameters of the CNP's predictive distributions: \(p(|,)=p(|(f_{}( ))())\) where \((f_{}())()\). For simplicity, we often view the decoder as a function \(\) and more simply write \((f_{}(),)\). In Theorem 1, both the encoder \(f_{}\) and decoder \(\) are \(G\)-equivariant. Many neural process architectures are of this form, including the ConvCNP (Gordon et al., 2019), the EquivCNP (Kawano et al., 2021), the RCNP (Huang et al., 2023), and the TE-TNP (Ashman et al., 2024).2 We discuss the specifics of each of these architectures in Appendix B.

## 3 Equivariant Decomposition of Non-Equivariant Functions

In this section, we demonstrate that, subject to regularity conditions, any _non-equivariant_ operator between function spaces can be constructed as, or approximated by, an _equivariant_ mapping with additional, fixed functions as input. These results motivate a simple construction for approximately equivariant models, which we use to construct approximately equivariant NPs in Section 3.1. We first illustrate the proof technique by proving the result for linear operators (Theorem 2) and then extend the result to nonlinear operators (Theorem 3).

**Setup.** Let \((,\,\,,\,\,)\) be a Hilbert space of functions \(\). Let \(G\) be a group acting linearly on \(\) from the left. For every \(g G\) and \(f\), applying \(g\) to \(f\) gives another function: \(gf\). By linearity, for \(f_{1},f_{2}\), \(g(f_{1}+f_{2})=gf_{1}+gf_{2}\). Assume that \(\) is separable and that \(\) is \(G\)-invariant, meaning that, for all \(f_{1},f_{2}\) and \(g G\), \( gf_{1},gf_{2}= f_{1},f_{2}\). If \(=L^{2}()\) and \(\) is a separable metric space, then \(\) is also separable (Brezis, 2011, Theorem 4.13). Let \(B\) be the collection of bounded linear operators on \(\). We say that an operator \(T B\) is _of finite rank_ if the dimensionality of the range of \(T\) is finite. Moreover, we say that an operator \(T B\) is _compact_ if the image of every bounded set is relatively compact. Let \((e_{i})_{i 1}\) be an orthonormal basis for \(\). Define \(P_{n}=_{i=1}^{n}e_{i} e_{i},\,\,\). The following proposition is well known:

**Proposition 2** (Finite-rank approximation of compact operators; e.g., Corollary 6.2 by Brezis (2011).): _Let \(T B\). Then \(T\) is compact if and only if there exists a sequence of operators of finite rank \((T_{n})_{n 1} B\) such that \(\|T-T_{n}\| 0\). In particular, one may take \(T_{n}=P_{n}T\), so \(T\) is a compact operator if and only if \(\|T-P_{n}T\| 0\)._

Intuitively, compactness of an operator implies that its inputs and outputs can be well approximated with a finite-dimensional basis. The following new result shows that every compact \(T B\) can be approximated with a \(G\)-equivariant function with additional fixed inputs: \(T E_{n}(\,\,,\,t_{1},,t_{2n})\) where \(E_{n}\) is \(G\)-equivariant and \(t_{1},,t_{2n}\) are the additional fixed inputs.

**Theorem 2** (Approximation of non-equivariant linear operators.).: _Let \(T B\). Assume that \(T\) is compact. Then there exists a sequence of continuous nonlinear operators \(E_{n}^{1+2n}\), \(n 1\), and a sequence of functions \((t_{n})_{n 1}\) such that every \(E_{n}\) is \(G\)-equivariant,_

\[E_{n}(gf_{1},gf_{2},,gf_{2n+1})=gE_{n}(f_{1},f_{2},,f_{2n+1}) ,,f_{2n+1}$}, \]

_and \(\|T-E_{n}(\,\,,t_{1},,t_{2n})\| 0\)._

Proof.: Observe that \(P_{n}Tf=_{i=1}^{n}e_{i} T^{*}e_{i},f=_{i=1}^{n}e_{i} _{i},f\), with \(_{i}=T^{*}e_{i}\). Define the continuous nonlinear operators \(E_{n}^{1+2n}\) as \(E_{n}(f,_{1},e_{1},,_{n},e_{n})=_{i=1}^{n}e_{i}_{i},f\). By \(G\)-invariance of \(\), these operators are \(G\)-equivariant:

\[E_{n}(gf,g_{1},ge_{1},)=_{i=1}^{n}ge_{i} g_{i},gf =g_{i=1}^{n}e_{i}_{i},f=gE_{n}(f,_{1},e_{1}, ). \]Since \(T\) is compact, \(\|T-P_{n}T\| 0\) (Proposition 2), so \(\|T-E_{n}(\,\,,_{1},,_{n},e_{1},,e_{n})\| 0\), which proves the result. 

As an example, consider \(G\) to be the group of translations such that \(E_{n}\) is translation equivariant. Translation-equivariant mappings between function spaces can be approximated with a CNN [Kumagai and Sannai, 2020]. Therefore, Theorem 2 gives that \(T(\,\,,t_{1},,t_{k})\) where \(t_{1},,t_{k}\) are additional fixed inputs that are given as additional channels to the CNN and can be treated as model parameters to be optimised. These additional inputs in the CNN break translation equivariance, because they are not translated whenever the original input is translated. The number \(k\) of such inputs roughly determines to what extent equivariance is broken: the larger \(k\), the more non-equivariant the approximation becomes. This example holds true for any CNN architecture provided it can approximate any translation-equivariant operator.

Theorem 2 is not exactly what we set out to achieve: we wish to construct approximately equivariant mappings, not use equivariant mappings to approximate non-equivariant models. However, instead of applying Theorem 2 directly to \(T\), consider the decomposition \(T=T_{}+T_{}\) where \(T_{}\) is in some sense the best "equivariant approximation" of \(T\) and \(T_{}\) is the residual. Then, if we approximate \(T_{}\) with \(E_{}\) using Theorem 2, and approximate \(T_{}\) with some \(G\)-equivariant mapping \(E_{}\) directly, we have \(T E_{}+E_{}(\,\,,t_{1},,t_{k})\). If \(k=0\), this approximation roughly recovers \(E_{}\), the best equivariant approximation of \(T\); and, if \(k\) is increased, the equivariance starts to break and the approximation starts to wholly approximate \(T\). Specifically, in Theorem 2, \(k=2n\) determines the dimensionality of the range of the approximation \(E_{n}\). Therefore, a small \(k\) means that one would deviate from \(T_{}\) in only a few degrees of freedom. The idea that \(k\) can be used to control the degree to which our approximation is \(G\)-equivariant inspires our training procedure in Section 3.1, where, with some probability, we set the additional inputs to zero. This forces the model to learn the 'best equivariant approximation', so that the number of additional fixed inputs has the desired influence of controlling only the flexibility of the non-equivariant component.

We now extend the result to any continuous, possibly nonlinear operator \(T\). For \(T B\), it is true that \(T\) is compact if and only if \(\|T-P_{n}TP_{n}\| 0\) (Proposition 3 in Appendix A). In generalising Theorem 2 to nonlinear operators, we shall use this equivalent condition. Roughly speaking, \(\|T-P_{n}TP_{n}\| 0\) says that both the domain and range of \(T\) admit a finite-dimensional approximation, and the proof then proceeds by discretising these finite-dimensional approximations.

**Theorem 3** (Approximation of non-equivariant operators.).: _Let \(T\) be a continuous, possibly nonlinear operator. Assume that \(\|T-P_{n}TP_{n}\| 0\), and that \(T\) is \((c,)\)-Holder for \(c,>0\), in the following sense:_

\[\|T(u)-T(v)\| c\|u-v\|^{}u,v. \]

_Moreover, assume that the orthonormal basis \((e_{i})_{i 1}\) is chosen such that, for every \(n\) and \(g G\), \(\,\{e_{1},,e_{n}\}=\,\{ge_{1},,ge_{n}\}\), meaning that subspaces spanned by finitely many basis elements are invariant under the group action \((*)\). Let \(M>0\). Then there exists a sequence \((k_{n})_{n 1}\), a sequence of continuous nonlinear operators \(E_{n}^{1+k_{n}}\), \(n 1\), and a sequence of functions \((t_{n})_{n 1}\) such that every \(E_{n}\) is \(G\)-equivariant,_

\[E_{n}(gf_{1},,gf_{1+k_{n}})=gE_{n}(f_{1},,f_{1+k_{n}})g Gf_{1},,f_{1+k_{n}}, \]

_and_

\[_{u:\|u\| M}\|T(u)-E_{n}(u,t_{1},,t_{k_{n}})\| 0. \]

_If assumption (\(*\)) does not hold, then the conclusion holds with \(E_{n}(\,\,,t_{1},,t_{k_{n}})\) replaced by \(E_{n}(P_{n}\,\,,t_{1},,t_{k_{n}})\). We provide a proof in Appendix A._

Condition (\(*\)) says that the orthonormal basis \((e_{i})_{i 1}\) must be chosen in a way such that applying the group action does not "introduce higher basis elements". Note that only one such basis needs to exist for the result to hold. An important example where this condition holds is \(=L^{2}(^{1})\) where \(^{1}=/\) is the one-dimensional torus (\(\) with endpoints identified); \(G\) the one-dimensional translation group; and \((e_{i})_{i 1}\) the Fourier basis: \(e_{k}(x)=e^{i2 kx}\). Then \(e_{k}(x-)=e^{i2 k}e_{k}(x) e_{k}(x)\), so \(\,\{e_{k}(\,\,-)\}=\,\{e_{k}\}\). This example shows that the result holds for translation equivariance, which is the symmetry that we will primarily consider in the experiments. Importantly, Theorem 3 requires \(\|T-P_{n}TP_{n}\| 0\) to hold. For this example, this condition roughly means that the dependence of \(T\) on high-frequency basis elements goes to zero as the frequency increases.

In Theorem 3, the sequence \((k_{n})_{n 1}\) determines how many additional fixed inputs are required to obtain a good approximation. For linear \(T\) (Theorem 2), \(k_{n}\) grows linearly in \(n\). In the nonlinear case, \(k_{n}\) may grow faster than linear in \(n\), so more basis functions may be required. We leave it to future work to more accurately identify how the growth of \(k_{n}\) in \(n\) depends on \(T\). A promising idea is to consider \(\|Tgf-gTf\|\) as a measure of how equivariant a mapping is (Wang et al., 2022).

### Approximately Equivariant Neural Processes

Theorem 3 provides a general construction of non-equivariant functions from equivariant functions, lending insight into how approximately equivariant neural processes can be constructed. Consider a \(G\)-equivariant CNP with \(G\)-equivariant encoder \(f_{}\) and decoder \(\). The construction that we consider in this paper is to insert additional fixed inputs into the decoder \(\):

\[p(|,)=_{n}p(_{n}|_{n}, )=_{n}p(_{n}|_{n},(f_{}( ),t_{1},,t_{B})) \]

where the decoder \(^{1+B} C(,)\) now takes in the dataset embedding \(f_{}()\) as well as \(B\) additional fixed inputs \((t_{b})_{b=1}^{B}\). These become additional model parameters and break \(G\)-equivariance of the decoder. The more are included, the more non-equivariant the decoder becomes. Crucially, Theorem 3 shows that including sufficiently many additional inputs eventually recovers any non-equivariant decoder. Conversely, by only including a few of them, the decoder deviates from \(G\)-equivariance in only a few degrees of freedom. Hence, the number \(B\) of additional fixed inputs determines to which extent the decoder can become non-equivariant.

There are a myriad of ways in which \((t_{b})_{b=1}^{B}\) can be incorporated into existing architectures for \(\). If \(\) is a CNN or a \(G\)-equivariant CNN, the additional inputs become additional channels, which can be either concatenated or added to the dataset embedding. We use the latter approach in our implementation. For more general \(\), such as that used in the TE-TNP, we can employ effective alternative choices, as discussed in Appendix B. Theorem 3 also intimately connects to positional embeddings in transformer-based LLMs (Vaswani et al., 2017) and vision transformers (ViT; Dosovitskiy et al., 2021). These architectures consist of stacked multi-head self-attention (MHSA) layers, which are permutation equivariant with respect to the input tokens. However, after tokenising individual words or pixel values into tokens, the underlying transformer which processes these tokens should not be equivariant with respect to the permutations of words or pixels, as their position is important to the overall structure. Following Theorem 3, we can add word- or pixel-position-specific inputs to break this equivariance. This corresponds exactly to the usual positional embeddings that are regularly used. This connection between breaking equivariance and positional encodings was also noted by Lim et al. (2023), where they interpret several popular positional encodings as group representations that can help incorporate approximate equivariance.

**Recovering equivariance out-of-distribution.** We stress that equivariance is crucial for models to generalise beyond the training distribution--this is the'shared' component that is inherent to the system we are modelling. Whilst the non-equivariant component is able to learn local symmetry-breaking features that are revealed with sufficient data, these features do not reveal themselves outside the training domain. To obtain optimal generalisation performance, we desire that the model ignores the non-equivariant component outside of the training domain and instead reverts to equivariant predictions. We achieve this in the following way: (i) During training, set \(t_{b}=0\) entirely with some fixed probability. This allows the model to learn and produce predictions for the equivariant component of the underlying system wherever the fixed additional inputs are zero. (ii) Forcefully set the additional fixed inputs to zero outside the training domain: \(t_{b}()=0\) for \(\) not close to any training data.3 This forces the model to revert to equivariant predictions outside the training domain, which should substantially improve the model's ability to generalise.

Our approach to recovering equivariance out-of-distribution also connects to Wang et al.'s (2022) notion of \(\)-approximate equivariance. Let \(_{}\) be the learned neural process, and let \(_{}\) be the neural process obtained by setting \(t_{b}=0\) in \(_{}\). If the predictions of \(_{}\) are not too different from those of \(_{}\), because the fixed additional inputs only affect the predictions in a limited way, then \(_{}\) is \(\)-approximately equivariant in the sense of Wang et al. (2022). See Appendix D.

Related Work

**Group equivariance and equivariant neural processes.** Group equivariant deep learning architectures are ubiquitous in machine learning, including CNNs , group equivariant CNNs , transformers  and GNNs . This is testament to the usefulness of incorporating inductive biases into models, with a significant body of research demonstrating improved sample complexity and generalisation capabilities . There exist a number of NP models which realise these benefits. Notably, the ConvCNP , RCNP , and TE-TNP  all build translation equivariance into NPs through different architecture choices. More general equivariances have been considered by both the EquivCNP  and SteerCNPs , both of which consider architectures similar to the ConvCNP.

**Approximate group equivariance.** Two methods similar to ours are those of  and van der Ouderaa et al. , who develop approximately equivariant architectures for group equivariant and steerable CNNs . We demonstrate in Appendix C that both approaches are specific examples of our more general approach. Finzi et al.  and Kim et al.  obtain approximate equivariance through a modification to the loss function such that the equivariant subspace of linear layers is favoured. These methods are less flexible than our approach, which can be applied to _any equivariant architecture_. Romero and Lohit  only enforce strict equivariance for specific elements of the symmetry group considered. Their approach hinges on the ability to construct and sample from probability distributions over elements of the groups, which both restricts their applicability and complicates their implementation. An orthogonal approach to imbuing models with approximate equivariance is through data augmentation. Data augmentation is trivial to implement; however, previous work  has demonstrated that both equivariant and approximately equivariant models achieve better generalisation bounds than data augmentation. Kim et al.  propose a similar approach to data augmentation, replacing a uniform average over group transformations with an expectation over an input-dependent probability distribution. Implementation of this distribution is involved, and must be hand-crafted for each symmetry group.

## 5 Experiments

In this section, we evaluate the performance of a number of approximately equivariant NPs derived from existing strictly equivariant NPs in modelling both synthetic and real-world data. We provide detailed descriptions of the architectures and datasets used in Appendix E.4 Throughout, we postfix to the name of each model the group it is equivariant with respect to, with the postfix \(\) denoting approximate equivariance with respect to group \(G\). We shall also omit reference to the dimension when denoting a symmetry group (e.g. \(T(n)\) becomes \(T\)). In all experiments, we compare the performance of three TNP-based models : non-equivariant, \(T\), and \(\); three ConvCNP-based models : \(T\), \(\) using the approach described in Section 3.1, and \(\) using the relaxed CNN approach of Wang et al. ; and two EquivCNP-based models : \(E\) and \(\). For experiments involving a large number of datapoints, we replace the TNP-based models with pseudo-token TNP-based models [PT-TNP; Ashman et al. 20].

### Synthetic 1-D Regression With the Gibbs Kernel

We begin with a synthetic 1-D regression task with datasets drawn from a Gaussian process (GP) with the Gibbs kernel . The Gibbs kernel similar to the squared exponential kernel, except the lengthscale \((x)\) is a function of position \(x\). The non-stationarity of the kernel implies that the predictive map is not translation equivariant, hence we expect an improvement of approximately equivariant NPs with respect to their equivariant counterparts. We construct each dataset by first sampling a change point, either side of which the GP lengthscale is either small (\((x)=0.1\)) or large (\((x)=4.0\)). The range from which the context and target points are sampled is itself randomly sampled, so that the change point is not always present in the data. We sample \(N_{c}\{1,64\}\) and the number of target points is set as \(N_{t}=128\). See Appendix E for a complete description.

We evaluate the log-likelihood on both the in-distribution (ID) training domain and on an out-of-distribution (OOD) setting in which the test domain is far away from the change point. Table 1 presents the results. We observe that the approximate equivariant models are able to: 1) recover the performance of the non-equivariant TNP within the non-equivariant ID regime; and 2) generalise as well as the equivariant models when tested OOD. We provide an illustrative comparison of the predictive distributions for each model in Figure 1. More examples can be found in Appendix E. When transitioning from the low-lengthscale to the high-lengthscale region, the equivariant predictive distributions behave as though they are in the low-lengthscale region. This is due to the ambiguity as to whether the high-lengthscale region has been entered. In contrast, the approximately equivariant models are able to learn that a change point always exists at \(x=0\), resolving this ambiguity.

We show in Appendix E.1 that the approximately equivariant models deviate only slightly from strict equivariance, thus being equivariant in the approximate sense as we describe in Section 3.1. We also analyse the performance of the models with increasing number of basis functions in Appendix E.1. The biggest improvement is obtained when going from \(0\) to \(1\) basis function, highlighting the importance of relaxing strict equivariance. We generally find that a few basis functions suffice to capture the symmetry-breaking features, but using more fixed inputs than necessary does not hurt performance, thus justifying in practice the use of a'sufficiently high number' of fixed inputs.

### Smoke Plumes

There are inherent connections between symmetries and dynamical systems, yet it is also true that real world dynamics rarely exhibit perfect symmetry. Motivated by this, we investigate the utility of

    &  &  \\  Model & ID Log-lik. (\(\)) & OOD Log-lik. (\(\)) & Log-lik. (\(\)) \\  TNP & \(\) & \(-1.3734 0.002\) & \(4.299 0.008\) \\ TNP (\(T\)) & \(-0.500 0.004\) & \(\) & \(4.181 0.011\) \\ TNP (\(T\)) & \(\) & \(\) & \(\) \\  ConvCNP (\(T\)) & \(-0.499 0.004\) & \(-0.442 0.006\) & \(3.637 0.041\) \\ ConvCNP (\(T\)) & \(-0.430 0.004\) & \(\) & \(3.827 0.011\) \\ RelaxedConvCNP (\(T\)) & \(\) & \(\) & \(\) \\  EquivCNP (\(E\)) & \(-0.504 0.004\) & \(-0.443 0.007\) & \(4.194 0.015\) \\ EquivCNP (\(E\)) & \(\) & \(\) & \(\) \\   

Table 1: Average test log-likelihoods (\(\)) for the synthetic 1-D GP and 2-D smoke experiments. For the 1-D dataset we used the regular TNP, while for the 2-D experiment we used the PT-TNP. Results are grouped together by model class. Best in-class result is bolded.

Figure 1: A comparison between the predictive distributions on a single synthetic 1-D regression dataset of the TNP-, ConvCNP-, and EquivCNP-based models. For the approximately equivariant models, we plot both the modelâ€™s predictive distribution (blue), as well as the predictive distributions obtained without using the fixed inputs (red). The dotted black lines indicate the target range.

approximate equivariance in the context of modelling symmetrically-imperfect simulations generated from partial differential equations. We consider a setup similar to Wang et al. (2022)--we construct a dataset of \(128 128\) 2-D smoke simulations, computing the air flow in a closed box with a smoke source. Besides the closed boundary, we also introduce a fixed spherical obstacle through which smoke cannot pass, and we sample the position of the spherical smoke inflow out of three possible locations. These three components break the symmetry of the system. We consider 25,000 different initial conditions generated through PhiFlow (Holl et al., 2020). We use 22,500 for training and the remaining for testing. We randomly sample the smoke sphere radius \(r\{5,30\}\) and the buoyancy coefficient \(B_{}\). For each initial condition, we run the simulation for a fixed number of steps and only keep the last state. We sub-sample a \(32 32\) patch from each state to construct a dataset. We sample the number of context points according to \(N_{c}\{10,250\}\) and set the remaining datapoints as target points. Table 1 compares the average test log-likelihood of the models. As in the 1-D regression experiment, the approximately equivariant versions of each model outperform both the non-equivariant and equivariant versions, demonstrating the effectiveness of our approach in modelling complex symmetry-breaking features. We provide illustrations of the predictive means for each model in Appendix Figure 6.

### Environmental Data

As remarked in Section 1, aspects of modelling climate systems adhere to symmetries such as equivariance. However, there are also unknown local effects which may be revealed by sufficient data. We explore this empirically by considering a real-world dataset derived from ERA5 (Copernicus Climate Change Service, 2020), consisting of surface air temperatures for the years 2018 and 2019. Measurements are collected at a latitudinal and longitudinal resolution of \(0.5^{}\), and temporal resolution of an hour. We also have access to the surface elevation at each coordinate, resulting in a 4-D input (\(_{n}^{4}\)). We consider measurements collected from Europe and from central US.5. We train each model on Europe's 2018 data, and test on both Europe's and central US' 2019 data. Because the CNN-based architectures have insufficient support for 4-D convolutions, we first consider a 2-D experiment in which the inputs consist of longitudes and latitudes, followed by a 4-D experiment consisting of all four inputs upon which the transformer-based architectures are evaluated.

**2-D Spatial Regression.** We sample datasets spanning \(16^{}\) across each axis. Each dataset consists of a maximum of \(N=1024\) datapoints, from which the number of context points are sampled according to \(N_{c}\{ N/100, N/3\}\). The remaining datapoints are set as target points. Table 2 presents the average test log-likelihood on the two regions for each model. We observe that approximately equivariant models outperform their equivariant counterparts when tested on the same geographical region as the training data. As the central US data falls outside the geographical region of the training data, we zero-out the fixed inputs, so that the predictions for the approximately equivariant models depend solely on their equivariant component. Surprisingly, they also outperform their equivariant counterparts. This suggests that incorporating approximate equivariance acts to regularise the equivariant component of the model, improving generalisation in finite-data settings such as this. In Figure 2, we compare the predictions of the PT-TNP and EquivCNP-based models for a single test dataset with the ground-truth data and the equivariant predictions made by the same models with the fixed inputs zeroed out. The predictive means of both models are almost indistinguishable from ground-truth. We can gain valuable insight into the effectiveness of our approach by comparing a plot of the difference between the approximately equivariant and the equivariant predictions (Figures 1(f) and 1(i)) to that of the elevation map for this region (Figure 1(c)). As elevation is not provided as an input, yet is crucial in predicting surface air temperature, the approximately equivariant models can infer the effect of local topographical features on air temperature through their non-equivariant component. As with the 1-D GP experiment, we analyse the degree of approximate equivariance and the performance with an increasing number of fixed inputs in Appendix E.3, drawing similar conclusions.

**4-D Spatio-Temporal Regression.** In this experiment, we sample datasets across 4 days with measurements every day and spanning \(8^{}\) across each axis. Each dataset consists of a maximum of \(N=1024\) datapoints, from which the number of context points are sampled according to \(N_{c}\{ N/100, N/3\}\). The remaining datapoints are set as target points. We provide results in Table 2. Similar to the 2-D experiment, we observe that the approximately equivariant PT-TNP outperforms both the equivariant PT-TNP and non-equivariant PT-TNP on both testing regions.

## 6 Conclusion

The contributions of this paper are two-fold. First, we develop novel theoretical results that provide insights into the general construction of approximately equivariant operators which is agnostic to the choice of symmetry group and choice of model architecture. Second, we use these insights to construct approximately equivariant NPs, demonstrating their improved performance relative to non-equivariant and strictly equivariant counterparts on a number of synthetic and real-world regression problems. We consider this work to be an important step towards understanding and developing approximately equivariant models. However, more must be done to rigorously quantify and control the degree to which these models depart from strict equivariance. Further, we only considered simple approaches to incorporating approximate equivariance into equivariant architectures, and provided empirical results for relatively small-scale experiments. We look forward to addressing each of these limitations in future work.

    &  &  \\  Model & Europe (\(\)) & US (\(\)) & Europe (\(\)) & US (\(\)) \\  PT-TNP & \(1.14 0.01\) & \(<-10^{6}\) & \(0.94 0.01\) & \(<-10^{26}\) \\ PT-TNP (\(\)) & \(1.06 0.01\) & \(\) & \(1.14 0.01\) & \(0.73 0.01\) \\ PT-TNP (\(\)) & \(\) & \(\) & \(\) & \(\) \\  ConvCNP (\(\)) & \(1.11 0.01\) & \(0.12 0.02\) & - & - \\ ConvCNP (\(\)) & \(\) & \(0.15 0.02\) & - & - \\ RelaxedConvCNP (\(\)) & \(\) & \(\) & - & - \\  EquivCNP (\(E\)) & \(1.27 0.01\) & \(0.64 0.02\) & - & - \\ EquivCNP (\(\)) & \(\) & \(\) & - & - \\   

Table 2: Average test log-likelihoods (\(\)) for the 2-D and 4-D environmental regression experiment. Results are grouped together by model class. Best in-class result is bolded.

Figure 2: A comparison between the predictive distributions of the equivariant (left column) and approximately equivariant (middle column) components of the PT-TNP (\(\)) and EquivCNP (\(\)) models on a single (cropped) test dataset from the 2-D environmental data experiment.