# Characterizing the Optimal \(0-1\) Loss for Multi-class Classification with a Test-time Attacker

Sihui Dai\({}^{1*}\) Wenxin Ding\({}^{2*}\) Arjun Nitin Bhagoji\({}^{2}\) Daniel Cullina\({}^{3}\)

**Ben Y. Zhao\({}^{2}\) Haitao Zheng\({}^{2}\) Prateek Mittal\({}^{1}\) \({}^{1}\)**Princeton University \({}^{2}\)University of Chicago \({}^{3}\)Pennsylvania State University

{sihuid,pmittal}@princeton.edu

{wenxind, abhagoji}@uchicago.edu

{ravenben, htzheng}@cs.uchicago.edu

cullina@psu.edu

###### Abstract

Finding classifiers robust to adversarial examples is critical for their safe deployment. Determining the robustness of the best possible classifier under a given threat model for a fixed data distribution and comparing it to that achieved by state-of-the-art training methods is thus an important diagnostic tool. In this paper, we find achievable information-theoretic lower bounds on robust loss in the presence of a test-time attacker for _multi-class classifiers on any discrete dataset_. We provide a general framework for finding the optimal \(0-1\) loss that revolves around the construction of a conflict hypergraph from the data and adversarial constraints. The prohibitive cost of this formulation in practice leads us to formulate other variants of the attacker-classifier game that more efficiently determine the range of the optimal loss. Our valuation shows, for the first time, an analysis of the gap to optimal robustness for classifiers in the multi-class setting on benchmark datasets.

## 1 Introduction

Developing a theoretical understanding of the vulnerability of classifiers to adversarial examples [27; 12; 8; 4] generated by a test-time attacker is critical to their safe deployment. Past work has largely taken one of two approaches. The first has focused on generalization bounds on learning in the presence of adversarial examples, by trying to determine the sample complexity of robust learning [10; 25; 21]. The second has been to characterize the lowest possible loss achievable within a specific hypothesis class [22; 5; 6] for binary classification for a specified data distribution and attacker. The hypothesis class of choice is often the set of all possible classification functions. The optimal loss thus determined is a lower bound on robustness for classifiers used in practice, allowing practitioners to measure progress for defenses and step away from the attack-defense arms race [19; 31; 9].

In this paper, we take on the second approach and propose methods to find the lowest possible loss attainable by any measurable classifier in the presence of a test-time attacker in the multi-class setting. The loss thus obtained is referred to as the _optimal loss_ and the corresponding classifier, the _optimal classifier_. We extend previous work [5; 6] which was restricted to the binary setting, allowing us to compare the robustness of multi-class classifiers used in practice to the optimal loss.

Our **first contribution** is thus _extending the conflict-graph framework for computing lower bounds on robustness to the multi-class setting_. In this framework, given a dataset and attacker, we construct a _conflict hypergraph_ which contains vertices representing training examples in the dataset, and hyperedges representing overlaps between adversarial neighborhoods around each training example. Using this hypergraph, we construct a linear program whose optimal value is a lower bound on the \(0-1\) loss for all classifiers and whose solution is the optimal classifier. The lower bound on robustness is thus achievable.

In practice, however, we find that the full multi-class formulation of the lower bound, although exact, can lead to prohibitively large optimization problems. Thus, we vary the information available to either the attacker or classifier to find alternative lower bounds that are quicker to compute. Our **second contribution** is the _development and analysis of more efficient methods to determine the range of loss obtained by the optimal classifier_. (see Table 1). We also interpret these methods as classifier-attacker games. In particular, we find lower bounds on the optimal loss by aggregating the set of all binary classifier-only lower bounds as well as by using truncated hypergraphs (hypergraphs with a restriction on the maximum hyperedge degree). We also upper bound the optimal loss with a generalization of the Caro-Wei bound (1) on a graph's independent set size. The gap between the lower and upper bounds on the optimal loss allows us to determine the range within which the optimal loss lies.

To analyze the performance of classifiers obtained from adversarial training [19; 31], we compare the loss obtained through adversarial training to that of the optimal classifier. We find a loss differential that is greatly exacerbated compared to the binary case [5; 22]. In addition, we also determine the cases where, in practice, the bounds obtained from game variants are close to the optimal. We find that while the aggregation of binary classifier-only bounds leads to a very loose lower bound, the use of truncated hypergraphs can significantly speed up computation while achieving a loss value close to optimal. This is validated using the Caro-Wei upper bound, with the lower and upper bounds on the optimal loss closely matching for adversarial budgets used in practice. Thus, our **final contribution** is an _extensive empirical analysis of the behavior of the optimal loss for a given attacker, along with its lower and upper bounds_. This enables practitioners to utilize our methods even when the optimal loss is computationally challenging to determine.

The rest of the paper is organized as follows: SS2 provides the characterization of the optimal loss; SS3 proposes several upper and lower bounds on the optimal loss; SS4 computes and compares the optimal loss to these bounds, as well as the performance of robustly trained classifiers and SS5 concludes with a discussion of limitations and future work.

## 2 Characterizing Optimal 0-1 Loss

In this section, we characterize the optimal \(0-1\) loss for any discrete distribution (_e.g._ training set) in the presence of a test-time attacker. This loss can be computed as the solution of a linear program (LP), which is defined based on a hypergraph constructed from the classification problem and attacker constraint specification. The solution to the LP can be used to construct a classifier achieving the optimal loss, and _lower bounds_ the loss attainable by any particular learned classifier.

### Problem Formulation

**Notation.** We consider a classification problem where inputs are sampled from input space \(\), and labels belong to \(K\) classes: \(y=[K]=\{1,...,K\}\). Let \(P\) be the joint probability over \(\). Let \(_{}\) denote the space of all soft classifiers; i.e. specifically, for all \(h_{}\) we have that \(h:^{K}\) and \(_{i=1}^{K}h(x)_{i}=1\) for all \(x\). Here, \(h(x)_{i}\) represents the probability that the input \(x\) belongs to the \(i^{}\) class. We use the natural extension of \(0-1\) loss to soft classifiers as our loss function: \((h,(x,y))=1-h(x)_{y}\). This reduces to \(0-1\) loss when \(h(x)\{0,1\}^{K}\). 1

**The adversarial classification game.** We are interested in the setting where there exists a test-time attacker that can modify any data point \(x\) to generate an adversarial example \(\) from the neighborhood \(N(x)\) of \(x\). An instance of the game is specified by a discrete probability distribution \(P\),2 hypothesis class \(_{}\), and neighborhood function \(N\). We require that for all \(x\), \(N(x)\) always contains \(x\). The goal of the classifier player is to minimize expected classification loss and the goal of the attacker is to maximize it. The optimal loss is

\[L^{*}(P,N,)=_{h}_{(x,y) P}_{  N(x)}(h,(,y))=1-_{h}_{(x,y) P}_{ N(x)}h()_{y}. \]

Alternative hypothesis classes.In general, for \(^{}\), we have \(L^{*}(P,N,) L^{*}(P,N,^{})\). Two particular cases of this are relevant. First, the class of hard-decision classifiers is a subset of the class of soft classifiers (\(_{}\)). Second, for any fixed model parameterization (ie. fixed NN architecture), the class of functions represented by that parameterization is another subset. Thus, optimal loss over \(_{}\) provides a lower bound on loss for these settings.

### Optimal loss for distributions with finite support

Since we would like to compute the optimal loss for distributions \(P\)_with finite support_, we can rewrite the expectation in Equation 1 as an inner product. Let \(V\) be the support of \(P\), i.e. the set of points \((x,y)\) that have positive probability in \(P\). Let \(p^{V}\) be the probability mass vector for \(P\): \(p_{v}=P(\{v\})\). For a soft classifier \(h\), let \(q_{N}(h)^{V}\) be the vector of robustly correct classification probabilities for vertices \(v=(x,y) V\), _i.e._\(q_{N}(h)_{v}:=_{ N(x)}h()_{y}\). Rewriting (1) with our new notation, we have \(1-L^{*}(P,_{},N)=_{h_{}}p^ {T}q_{N}(h)\). This is the maximization of a linear function over all possible vectors \(q_{N}(h)\). In fact, the convex hull of all correct classification probability vectors is a polytope and this optimization problem is a linear program, as described next.

**Definition 1**.: _For a soft classifier \(h\), the correct-classification probability achieved on an example \(v=(x,y)\) in the presence of an adversary with constraint \(N\) is \(q_{N}(h)_{v}=_{ N(x)}h()_{y}\)._

_The space of achievable correct classification probabilities is \(_{,N,}^{}\), defined as_

\[_{,N,}=_{h}_{v }[0,q_{N}(h)_{v}]\]

In other words we say that \(q^{}^{}\) is achievable when there exists \(h\) such that \(q^{} q_{N}(h)\). The inequality appears because we will always take nonnegative linear combinations of correct classification probabilities.

Characterizing \(_{,N,}\) allows the minimum adversarial loss achievable to be expressed as an optimization problem with a linear objective:3

\[1-L^{*}(P,N,_{})=_{h_{}} _{v P}[q_{N}(h)_{v}]=_{h_{}}p^{T}q _{N}(h)=_{q_{,N,}}p^{T}q. \]

(6) characterized \(_{,N,_{soft}}\) in the two-class case and demonstrated that this space can be captured by linear inequalities. We now demonstrate that this also holds for the multi-class setting.

### Linear Program to obtain Optimal Loss

In order to characterize \(_{,N,_{soft}}\), we represent the structure of the classification problem with a _conflict hypergraph_\(_{,N}=(,)\), which records intersections between neighborhoods of points in \(\). The set of vertices \(V\) of \(_{,N}\) is the support of \(P\). \(\) denotes the set of hyperedges of the graph. For a set \(S\), \(S\) (i.e. \(S\) is a hyperedge in \(_{,N}\)) if all vertices in \(S\) belong to different classes and the neighborhoods of all vertices in \(S\) overlap: \(_{(x,y) S}N(x)\). Thus, the size of each hyperedge is at most \(K\), \(\) is downward-closed (meaning if \(e\) and \(e^{} e\), then \(e^{}\)), and every \(v\) is a degree 1 hyperedge.

Using the conflict hypergraph \(_{,N}\), we can now describe \(_{,N,_{soft}}\).

**Theorem 1** (Feasible output probabilities (Adapted from (6))).: _The set of correct classification probability vectors for support points \(\), adversarial constraint \(N\), and hypothesis class \(_{soft}\) is_

\[_{,N,_{soft}}=\{q^{}: q,\;Bq\} \]

_where \(B^{}\) is the hyperedge incidence matrix of the conflict hypergraph \(G_{,N}\)._See Supplementary SSA for proof. This characterization of \(_{,N,_{}}\) allows us to express optimal loss as a linear program for any dataset and attacker using Eq. 24.

**Corollary 1** (Optimal loss as an LP).: _For any distribution \(P\) with finite support,_

\[1-L^{*}(P,N,_{})=_{q}p^{T}q\ \ q 0,\ Bq 1. \]

Duality and adversarial strategies.The dual linear program is

\[_{z}^{T}z z 0, B^{T}z p.\]

Feasible points in the dual linear program are fractional coverings of the vertices by the hyperedges (24). (See Section 3.4 and Supplementary SSB for more discussion of fractional packings and coverings.) An adversarial strategy can be constructed from a feasible point \(z\) as follows. For each hyperedge \(e\), chose an example \((e)\) such that \((e) N(x)\) for all \((x,y) e\). From the definition of the conflict hypergraph, a choice is always available. A randomized adversarial strategy consists of conditional distributions for the adversarial example \(\) given the natural example \(x\). When the adversary samples the natural example \((x,y)\), the adversary can select \((e)\) with probability at least \(vz_{e}}{p_{v}}\). Note that \(B_{e,v}z_{e}\) is the amount of coverage of \(v\) coming from \(e\). This is nonzero only for \(e\) that contain \((x,y)\). A conditional distribution satisfying these inequalities exists because \(_{e}B_{e,v}z_{e}=(B^{T}z)_{v} p_{v}\).

Thus for a vertex \(v\) such that \((B^{T}z)_{v}=p_{v}\), there is only one choice for this distribution. For a vertex \(v\) that is over-covered, i.e. \((B^{T}z)_{v}>p_{v}\), the adversary has some flexibility. If \(v\) is over-covered in some minimal cost covering, by complementary slackness \(q_{v}=0\) in every optimal \(q\), so the optimal classifiers do not attempt to classify \(v\) correctly.

Three-class minimal examples.Corollary 1 demonstrates that the optimal loss for the multi-class problem is influenced by hyperedges between vertices which reflect higher order interactions between examples. Figure 1 shows an important distinction between two types of three-way interactions of examples.

We have \(1-L^{*}(P,_{soft},N)=(p_{u},p_{v},p_{w},)\) while \(1-L^{*}(P^{},_{soft},N)=(p^{}_{u},p^{}_{v},p^ {}_{w})\). The presence or absence of the size-three hyperedge affects the optimal loss if and only if the example probabilities are close to balanced, i.e. all at most \(\).

It is instructive to consider the optimal classifiers and adversarial strategies in the two cases. For \(\), when \( p_{u}\), the classifier \(h\) with \(q_{N}(h)=(1,0,0)\) is optimal. One such classifier is the constant classifier \(h(x)=(1,0,0)\). The optimal cover satisfies \(z_{\{u,v\}}+z_{\{u,w\}}=p_{u}\), \(z_{\{u,v\}} p_{v}\), \(z_{\{u,w\}} p_{w}\), \(z_{\{v,w\}}=0\). Thus when the adversary

   & **Summary of Method** & **Location** \\ 
**Optimal 0-1 loss** & LP on conflict hypergraph & ยง2.3 \\ 
**Lower bounds for optimal 0-1 loss** & LP on truncated conflict hypergraph & ยง3.1 \\  & Combining binary classification bounds & ยง3.2 \\ 
**Upper bound for optimal 0-1 loss** & Generalization of Caro-Wei bound & ยง3.5 \\  

Table 1: Summary of methods for computing the optimal \(0-1\) loss and efficient bounds on this value.

Figure 1: Two possible conflict structures involving three examples, each from a different class. In the right case, all subsets of \(^{}=\{u^{},v^{},w^{}\}\) are hyperedges in the conflict hypergraph \(_{^{},N}\). In the left case, \(\{u,v,w\}\) is not a hyperedge in \(_{,N}\), but all other subsets of \(\) are.

samples \(v\) or \(w\), it always produces an adversarial example that could be confused for \(u\). When \((p_{u},p_{v},p_{w})\), any classifier \(h\) with \(q_{N}(h)=(,,)\) is optimal. To achieve these correct classification probabilities, we need \(h()=(,,0)\) for \( N(u) N(v)\), \(h()=(,0,)\) for \( N(u) N(w)\), etc.. The cover \(z_{\{u,v\}}=p_{u}+p_{v}-\), \(z_{\{u,w\}}=p_{u}+p_{w}-\), and \(z_{\{v,w\}}=p_{v}+p_{w}-\) is optimal and has cost \(\). The adversary produces examples associated with all three edges.

For \(^{}\), things are simpler. The cover \(z_{\{u,v,w\}}=(p_{u},p_{v},p_{w})\) is always optimal. When \(p_{u}(p_{v},p_{w})\), the classifier that returns \((1,0,0)\) everywhere is optimal.

## 3 Bounding the Optimal 0-1 Loss

While Corollary 1 characterizes the optimal loss, it may be computationally expensive to construct conflict hypergraphs in practice for a given dataset and to solve the linear program. Thus, we discuss several methods of bounding the optimal loss from the LP in Corollary 1, which are computationally faster in practice (SS4.2).

### Lower bounds on multiclass optimal loss via truncated hypergraphs

The edge set of the hypergraph \(\) can be very large: there are \(_{i[K]}(1+|V_{i}|)\) vertex sets that are potential hyperedges. Even when the size of the edge set is reasonable, it is not clear that higher order hyperedges can be computed from \(\) efficiently. To work around these issues, we consider hypergraphs with bounded size hyperedges: \(^{ m}=(,^{ m})\) where \(^{ m}=\{e:|e| m\}\). We refer to these hypergraphs as _truncated hypergraphs_. In the corresponding relaxation of (4), \(B\) is replaced by \(B^{ m}\), the incidence matrix for \(^{ m}\). Since \(^{ m}\), this relaxation provides a lower bound on \(L^{*}(P,N,_{})\).

Classification with side-information.This relaxation has an interpretation as the optimal loss in a variation of the classification game with side information.

**Definition 2**.: _In the example-dependent side information game with list length \(m\), the adversary samples \((x,y) P\), then selects \(\) and \(C\) such that \(y C\) and \(|C|=m\). We call \(C\) the side information. The classifier receives both \(\) and \(C\), so the classifier is a function \(h:}{m}^{K}\), where \(}{m}\) is the set of \(m\)-element subsets of \(\). Let_

\[L^{*}(m,P,,N)=_{h}_{(x,y) P} _{ N(x)}_{C}{m}:y C}(1-h(,C)_{y})\]

_be the minimum loss in this game._

To illustrate this, consider the distribution \(P^{}\) from Figure 1 with \(m=2\). The adversary can select some \( N(u^{}) N(v^{}) N(w^{})\), but the classifier will use the side-information to eliminate one of the three classes. The classifier is in the same situation it would be if the distribution were \(P\) and the size-three hyperedge was absent.

**Definition 3**.: _For classifiers using class list side-information, the correct-classification probability is defined as follows: \(q_{m,N}(h)_{(x,y)}=_{ N(x)}_{C:y C}h( ,C)_{y}\). The set of achievable correct-classification probabilities is \(_{m,,N,}=_{h}_{v }[0,q_{m,N}(h)_{v}]\)._

When \(m=K\), the minimization over \(C\) is trivial and \(q_{m,N}(h)\) reduces to \(q_{N}(h)\).

**Theorem 2** (Feasible output probabilities in the side-information game).: _The set of correct classification probability vectors for side-information of size \(m\), support points \(\), adversarial constraint \(N\), and hypothesis class \(_{soft}\) is_

\[_{m,,N,_{soft}}=\{q^{}:q,\;B^{ m}q\} \]

_where \(B^{ m}^{}\) is the hyperedge incidence matrix of the conflict hypergraph \(G^{ m}_{,N}\)._

The proof can be found in Supplementary SSA.

Using the feasible correct classification probabilities in Theorem 2, we can now write the LP for obtaining the optimal loss for classification with side-information:

**Corollary 2** (Optimal loss for classification with side information / truncation lower bound).: \[1-L^{*}(P,N,_{})=_{q}p^{T}q\ \ s.t\,q 0,\ B^{ m}q 1.\]

### Lower bounds on multiclass optimal loss via lower bounds for binary classification

For large training datasets and large perturbation sizes, it may still be computationally expensive to compute lower bounds via LP even when using truncated hypergraphs due to the large number of edge constraints. Prior works [(5; 6)] proposed methods of computing lower bounds for \(0-1\) loss for binary classification problems and demonstrate that their algorithm is more efficient than generic LP solvers. We now ask the question: _Can we use lower bounds for binary classification problems to efficiently compute a lower bound for multi-class classification?_

Consider the setting where we obtain the optimal \(0-1\) loss for all one-versus-one binary classification tasks. Specifically, for each \(C\), the binary classification task for that class pair uses example distribution \(P|Y C\) and the corresponding optimal loss is \(L^{*}((P|Y C),N,_{soft})\). What can we say about \(L^{*}(P,N,_{soft})\) given these \(\) numbers?

This question turns about to be related to another variation of classification with side information.

**Definition 4**.: _In the class-only side-information game, the adversary samples \(y P_{y}\), then selects \(C\), then samples \(x P_{x|y}\) and selects \( N(x)\). Let \(L^{*}_{}(m,P,,N)\) be the minimum loss in this game._

In the example-dependent side-information game from Section 3.1, the adversary's choice of \(C\) can depend on both \(x\) and \(y\). In the class-only variation it can only depend on \(y\). For the class only game, we will focus on the \(m=2\) case.

To make the connection to the binary games, we need to add one more restriction on the adversary's choice of side information: for all \(y,y^{}[C=\{y,y^{}\}|Y=y]=[C=\{y,y^{}\}|Y=y^{}]\). This ensures that the classifier's posterior for \(Y\) given \(C\) is \([Y=y|C]=[Y=y]/[Y C]\).

**Theorem 3**.: _The optimal loss in the class-only side-information game is \(L^{*}_{}(2,P,N,)=_{s}_{i,j}Pr[Y=i]a_{i,j}s_{i,j}\) where \(a_{i,j}=L^{*}(P|(y\{i,j\}),,N)\) and \(s^{[K][K]}\) is a symmetric doubly stochastic matrix: \(s 0\), \(s=s^{T}\), \(s=\)._

The proof in is Supplementary SSA. The variable \(s\) represents the attacker's strategy for selecting the class side information. When the classes are equally likely, we have a maximum weight coupling problem: because the weights \(a\) are symmetric, the constraint that \(s\) be symmetric becomes irrelevant.

### Relationships between games and bounds

The side information games provide a collection of lower bounds on \(L^{*}(P,N,_{})\). When \(m=1\), the side information game becomes trivial: \(C=\{y\}\) and the side information contains the answer to the classification problem. Thus \(L^{*}(1,_{soft})=L^{*}_{}(1,_{soft})=0\). When \(m=K\), \(C=\) and both the example-dependent and class-only side information games are equivalent to the original game, so \(L^{*}(P,N,)=L^{*}(K,P,N,)=L^{*}_{}(K,P,N, )\). For each variation of the side-information game, the game becomes more favorable for the adversary as \(m\) increases: \(L^{*}(m,P,n,) L^{*}(m+1,P,N,)\) and \(L^{*}_{}(m,P,N,) L^{*}_{}(m+1,P,N,)\). For each \(m\), it is more favorable for the adversary to see \(x\) before selecting \(C\), _i.e._\(L^{*}_{}(m,P,N,) L^{*}(m,P,N,)\).

### Optimal Loss for Hard Classifiers

Since \(_{}_{}\), \(L^{*}(m,P,N,_{}) L^{*}(P,N,_{})\). The optimal loss over hard classifiers is interesting both as a bound on the optimal loss over soft classifiers and as an independent quantity. Upper bounds on \(L^{*}(P,N,_{})\) can be combined with lower bounds from SS3.1 and SS3.2 using small values of \(m\) to pin down \(L^{*}(m,P,N,_{})\) and establish that larger choices of \(m\) would not provide much additional information.

A hard classifier \(h:\{0,1\}^{[K]}\) has \(0,1\)-valued correct classification probabilities. When we apply the classifier construction procedure from the proof of Theorem 1 using an integer-valued\(q\) vector, we obtain a hard classifier. Thus the possible correct classification probabilities for hard classifiers are \(_{,N,_{soft}}\{0,1\}^{[K]}\). These are exactly the indicator vectors for the independent sets in \(^{ 2}\): the vertices included in the independent set are classified correctly and the remainder are not. Formally, we can express hard classifier loss as:

\[1-L^{*}(P,N,_{})=_{S:S^{ 2}}P(S). \]

Finding the maximum weight independent set is an NP hard problem, which makes it computationally inefficient to compute optimal hard classifier loss.

Two-class versus Multi-class hard classificationThere are a number of related but distinct polytopes associated with the vertices of a hypergraph (24). The distinctions between these concepts explain some key differences between two-class and multi-class adversarial classification. See Supplementary SSB for full definitions of these polytopes.

When \(K=2\), the conflict hypergraph is a bipartite graph. For bipartite graphs, the fractional vertex packing polytope, which has a constraint \(_{i e}q_{i} 1\) for each edge \(e\), coincides with the independent set polytope, which is the convex hull of the independent set indicators. Due to this, in the two class setting, \(_{,N,_{soft}}\) is the convex hull of \(_{,N,_{hard}}\), hard classifiers achieve the optimal loss, and optimal hard classifiers can be found efficiently.

As seen in Theorem 1, for all \(K\) the fractional vertex packing polytope characterizes performance in soft classification problem. However, for \(K>2\), it becomes distinct from the independent set polytope. An independent set in a hypergraph is a subset of vertices that induces no hyperedges. In other words, in each hyperedge of size \(m\), at most \(m-1\) vertices can be included in any independent set. Because the edge set of the conflict hypergraph is downward-closed, only the size-two hyperedges provide binding constraints: the independent sets in \(\) are the same as the independent sets in \(^{ 2}\). Thus the concept of a hypergraph independent set is not truly relevant for our application.

There is a third related polytope: the fractional independent set polytope of \(^{ 2}\), which has a constraint \(_{i S}q_{i} 1\) for each clique \(S\) in \(^{ 2}\). The fractional independent set polytope of \(^{ 2}\) is contained in the fractional vertex packing polytope of \(\): every hyperedge in \(\) produces a clique in \(^{ 2}\) but not the reverse. This inclusion could be used to find an upper bound on optimal soft classification loss.

Furthermore, when \(K>2\) the fractional vertex packing polytope of the conflict hypergraph, i.e. \(_{,N,_{soft}}\), can have non-integral extreme points and thus can be strictly larger than the independent set polytope. The first configuration in Figure 1 illustrates this. Thus the soft and hard classification problems involve optimization over different spaces of correct classification probabilities. Furthermore, maximum weight or even approximately maximum weight independent sets cannot be efficiently found in general graphs: the independent set polytope is not easy to optimize over.

In Section 3.5, we will use an efficiently computable lower bound on graph independence number.

### Upper bounds on hard classifier loss via Caro-Wei bound on independent set probability

In SS3.1 and 3.2, we discussed 2 ways of obtaining lower bounds for the loss of soft classifiers for the multi-class classification problem. In this section, we provide an upper bound on the loss of the optimal hard classifier (we note that this is also an upper bound for optimal loss for soft classifiers). In SS3.4, we discussed the relationship between optimal loss achievable by hard classifiers and independent set size. We upper bound the optimal loss of hard classifiers by providing a lower bound on the probability of independent set in the conflict graph.

The following theorem is a generalization of the Caro-Wei theorem (1) and gives a lower bound on the weight of the maximum weight independent set.

**Theorem 4**.: _Let \(\) be a graph on \(\) with adjacency matrix \(A\{0,1\}^{}\) and let \(P\) be a probability distribution on \(\). For any \(w^{}\), \(w 0\), there is some independent set \(S\) with_

\[P(S)_{v:w_{v}>0}w_{v}}{((A+I)w)_{v}}.\]

The proof is in Supplementary SSA.

For comparison, the standard version of the Caro-Wei theorem is a simple lower bound on the independence number of a graph. It states that \(\) contains an independent set \(S\) with \(|S|_{v}1/(d_{v}+1)\) where \(d_{v}\) is the degree of vertex \(v\).

Note that if \(w\) is the indicator vector for an independent set \(S^{}\), the bound becomes \(p^{T}w=P(S^{})\). In general, the proof of Theorem 4 can be thought of as a randomized procedure for rounding an arbitrary vector into an independent set indicator vector. Vectors \(w\) that are nearly independent set indicators yield better bounds.

Theorem 4 provides a lower bound on the size of the maximum independent set in \(^{ 2}\) and thus an upper bound on \(L^{*}(P,n,_{hard})\), which we call \(L_{CW}=1-P(S)\).

## 4 Empirical Results

In this section, we compute optimal losses in the presence of an \(_{2}\)-constrained attacker (12) at various strengths \(\) for benchmark computer vision datasets like MNIST and CIFAR-10 in a \(3\)-class setting. We compare these optimal losses to those obtained by state-of-the-art adversarially trained classifiers, showing a large gap. We then compare the optimal loss in the \(10\)-class setting to its upper and lower bounds (SS3.3), showing matching bounds at lower \(\). In the Supplementary, SSC describes hyperedge finding in practice, SSD details the experimental setup and SSE contains additional results.

### Optimal loss for 3-class problems

Corollary 1 allows us to compute the optimal loss given any dataset (and its corresponding conflict hypergraph). We compute the optimal loss for 3-way classification, due to the computational complexity of finding higher order hyperedges (see SSD.4 of the Supp.). In Figure 2, we plot the optimal loss \(L^{*}(3)\) computed via the LP in Corollary 1 against the loss obtained through TRADES (31) for 3-class MNIST (classes '1', '4', '7') and 3-class CIFAR-10 ('plane', 'bird' and'ship' classes) with 1000 samples per class 5. For MNIST, we train a 3 layer CNN for 20 epochs with TRADES regularization strength \(=1\), and for CIFAR-10, we train a WRN-28-10 model for 100 epochs with \(=6\). We evaluate models using APGD-CE from AutoAttack (9).

From Figure 2, we observe that this gap is quite large even at lower values of \(\). This indicates that there is considerable progress to be made for current robust training methods, and this gap may be due to either the expressiveness of the hypothesis class or problems with optimization. We find that for CIFAR-10, TRADES is unable to achieve loss much better than 0.6 at an \(\) for which the optimal loss is near 0. This gap is much larger than observed by prior work (5; 6) for binary classification, suggesting that _current robust training techniques struggle more to fit training data with more classes_. In SSD.8 of the Supp., we ablate over larger architectures, finding only small improvements at lower values of \(\) and none at higher.

Figure 2: Optimal error for MNIST and CIFAR-10 3-class problems (\(L^{*}(3)\)). \(L^{*}(2)\) is a lower bound computed using only constraints from edges. \(AT\) is the loss for an adversarially trained classifier under the strong APGD attack (9).

### Bounds on optimal loss for 10-class problems

As the number of classes and dataset size increases, the difficulty of solving the LP in Corollary 1 increases to the point of computational infeasibility (see SSD.4 of the Supp.). We use the methods discussed in Section 3 to bound the optimal loss for 10-class problems on MNIST and CIFAR-10 datasets on the full training dataset. We present results for each approximation in Figure 3, with the limitation that for some methods, we are only able to obtain results for smaller values of \(\) due to runtime blowup. We provide truncated hypergraph bounds for CIFAR-100 in SSD.2 of the Supp.

**Lower bounding the optimal loss using truncated hypergraphs (SS3.1):** In Figure 3, we plot the loss lower bound obtained via by truncating the hypergraph to consider only edges (\(L^{*}(2)\)), up to degree 3 hyperedges (\(L^{*}(3)\)), and up to degree 4 hyperedges (\(L^{*}(4)\)). Computing the optimal loss would require computing degree 10 hyperedges. However, we find at small values of \(\), there is little difference in these bounds despite the presence of many higher degree hyperedges. This indicates that the use of use of higher degree hyperedges may not be critical to get a reasonable estimate of the optimal loss. For example, for CIFAR-10 at \(=3\), we observe 3M degree 3 hyperedges and 10M degree 4 hyperedges, but these constraints have no impact on the computed lower bound. To understand the impact of hyperedges, we provide the count of hyperedges for each value of \(\) and plots of the distribution of optimal classification probabilities per vertex in SSD.3 of the Supp. From Figure 2, we find that the difference \(L^{*}(2)\) and \(L^{*}(3)\) does not occur until the loss reaches above 0.4 for the 3-class problem.

_Takeaway:_ In practice, we _do not lose information from computing lower bounds with only edges in the conflict hypergraph_.

**Lower bounding the optimal loss using the \(1\)v\(1\) binary classification problems (SS3.2):** We can use the algorithm from Bhagoji et al.(6) to efficiently compute \(1\)v\(1\) pairwise optimal losses to find a lower bound on the 10-class optimal loss (\(L^{*}_{CO}(2)\)). From Theorem 3, we use maximum weight coupling over these optimal losses to find a lower bound. Optimal loss heatmaps for each pair of classes in MNIST and CIFAR-10 are in SSD.5 of the Supp. The efficiency of the \(1\)v\(1\) computation allows us to compute lower bounds at larger values of \(\) in Figure 3.

_Takeaway:_ From Figure 3, we find that _while this lower bound is the most efficient to compute, the obtained bound is much looser compared to that from truncated hypergraphs_. This arises from the weak attacker assumed while computing this bound.

**Upper bounding optimal loss via Caro-Wei approximation (SS3.5):** In Figure 3, we also plot the upper bound on \(0-1\) loss for hard classifiers (denoted by \(L_{CW}\)) obtained via applying Theorem 4 with vertex weights obtained from the solution to \(L^{*}(2)\). When \(\) becomes large (\( 3.0\) for MNIST and \( 4.5\) for CIFAR-10), the loss upper bound increases sharply. This indicates the lower bound on the independent set size becomes looser as the number of edges increases, due to the importance of higher-order interactions that are not captured by the approximation. At the small values of \(\) used in practice however, the lower bounds obtained through truncated hypergraphs (\(L^{*}(2)\), \(L^{*}(3)\), and \(L^{*}(4)\)) are close to the value of this upper bound.

Figure 3: Lower bounds on the exact optimal \(10\)-class loss using hyperedges up to degree 2 (\(L^{*}(2)\)), 3 (\(L^{*}(3)\)) and 4 (\(L^{*}(4)\)), as well as maximum weight coupling of pairs of binary \(0-1\) loss lower bounds (\(L^{*}_{}(2)\)). \(L_{CW}\) is an upper bound from the Caro-Wei approximation of the independent set number. The region in grey represents the range of values we would expect the true optimal loss \(L^{*}(10)\) to fall under.

_Takeaways:_ (i) We do not lose much information from not including all hyperedges at small \(\) values as _the upper and lower bounds are almost tight_; (ii) At these small values of \(\), _we do not expect much difference in performance between hard classifiers and soft classifiers._

**Comparing loss of trained classifiers to optimal:** In Figure 3, we also compare the loss obtained by a robustly trained classifier (AT) to our bounds on optimal loss. For both datasets, we see a large gap between the performance of adversarial training and our bounds (including the upper bound from the Caro-Wei approximation \(L_{CW}\)), even at small values of \(\). This suggests that current robust training techniques are currently unable to optimally fit the training data in multiclass classification tasks of interest. In addition, we also checked the performance of state-of-the-art verifiably robust models on these two datasets from a leaderboard (18). For MNIST, the best certifiably robust model has a \(0-1\) loss of \(0.27\) at a budget of \(1.52\) and \(0.44\) at a budget of \(2.0\), while for CIFAR-10, the best certifiably robust model has a \(0-1\) loss of \(0.6\) at a budget of \(1.0\) and \(0.8\) at a budget of \(2.0\). These are much higher than the optimal lower bound that is achievable for these datasets which is \(0\) in all these cases.

_Takeaway:_ Performance of state-of-the-art robust models, both empirical and verifiable, _exhibit a large gap from the range of values predicted by our bounds on optimal \(0-1\) loss, even when \(\) is small._ Future research focusing on developing algorithms to decrease this gap while maintaining generalization capabilities may lead to improvements in model robustness.

## 5 Discussion and Related Work

**Related Work:** When the data distribution satisfies certain properties, Dohmatob (11) and Mahloujifar _et al._ (20) use the 'blowup' property to determine bounds on the robust loss, given some level of loss on benign data. We note that these papers use a different loss function that depends on the original classification output on benign data, thus their bounds are not comparable. Bhagoji _et al._ (5; 6), and Pydi _et al._ (22) provide lower bounds on robust loss when the set of classifiers under consideration is all measurable functions. These works _only provide bounds in the binary classification setting_. Work on verifying robustness (7; 28; 13; 18) provides bounds on the robustness of specific classifiers. Yang _et al._ (30) independently introduced the concept of a 'conflict graph' to obtain robust non-parametric classifiers via an adversarial pruning defense. The closest related work to ours is Trillos _et al._ (29), which uses optimal transport theory to find lower bounds on multi-class classification that are applicable for both continuous and discrete distributions. While their theoretical bounds are exact and more general than ours, accounting for distributional adversaries, their numerically computed bounds via the Sinkhorn algorithm are approximate and converge to the true value only as the entropy regularization decreases. In contrast, we provide methods to directly compute the optimal loss for discrete distributions, along with efficient methods for determining its range to overcome the computational bottlenecks encountered.

**Discussion and Limitations:** Our work in this paper firmly establishes for the multi-class case what was known only in the binary setting before: _there exists a large gap in the performance of current robust classifiers and the optimal classifier_. It also provides methods to bound the loss efficiently in practice, giving practitioners quick means to determine the gap. The question then arises: _why does this gap arise and how can we improve training to decrease this gap?_ This paper, however, does not tackle the problem of actually closing this gap. Possible methods include increasing the architecture size (26), using additional data (15) and using soft-labels (6). A surprising finding from our experiments was that the addition of hyperedges to the multi-way conflict graph did not change the lower bounds much, indicating we are in a regime where multi-way intersections minimally impact optimal probabilities. One major limitation of our work is the computational expense at larger budgets, sample sizes and class sizes. We suspect this is due to the general-purpose nature of the solvers we use and future work should look into developing custom algorithms to speed up the determination of lower bounds.