# Fast Tree-Field Integrators: From Low Displacement

Rank to Topological Transformers

 Krzysztof Choromanski\({}^{1,2}\)

equal contribution

Arijit Sehanobish\({}^{3,}\)

equal contribution

Somnath Basu Roy Chowdhury\({}^{4,}\)

Han Lin\({}^{4,}\)

Avinava Dubey\({}^{5,}\)

Equal contribution

Tamas Sarlos\({}^{5}\)

Snigdha Chaturvedi\({}^{4}\)

\({}^{1}\) Google DeepMind, \({}^{2}\) Columbia University, \({}^{3}\) Independent, \({}^{4}\) UNC Chapel Hill, \({}^{5}\) Google Research.

###### Abstract

We present a new class of fast polylog-linear algorithms based on the theory of structured matrices (in particular _low displacement rank_) for integrating tensor fields defined on weighted trees. Several applications of the resulting _fast tree-field integrators_ (FTFIs) are presented, including (a) approximation of graph metrics with tree metrics, (b) graph classification, (c) modeling on meshes, and finally (d) _Topological Transformers_ (TTs)  for images. For Topological Transformers, we propose new relative position encoding (RPE) masking mechanisms with as few as **three** extra learnable parameters per Transformer layer, leading to **1.0-1.5%+** accuracy gains. Importantly, most of FTFIs are **exact** methods, thus numerically equivalent to their brute-force counterparts. When applied to graphs with thousands of nodes, those exact algorithms provide **5.7-13x** speedups. We also provide an extensive theoretical analysis of our methods.

## 1 Introduction

Matrix-vector multiplication remains a key computational block of virtually all modern machine learning (ML) algorithms. For this reason, decades of research have been dedicated towards making this fundamental operation more efficient. One approach to achieve this goal is through efficient hardware design, e.g., using modern GPU and TPU accelerators . The alternative method involves developing algorithms for efficient matrix-vector multiplication by leveraging either (1) sparse matrices , or (2) structured dense matrices . These algorithms can be applied in modern neural network systems, where weights are pruned to encourage sparsity  or they can be parameterized with structured matrices .

In this work, we aim to accelerate multiplications with a large class of matrices, that we refer to as _\(f\)-distance matrices_, which play an important role in several ML algorithms. Consider a matrix \(_{f}^{}=[f((i,j))]_{i,j=1,,N}^{N N}\), where \((i,j)\) stands for the shortest-path distance between the \(i\)-th and \(j\)-th vertex of an undirected graph \(=(,,)\). Here \(=\{1,...,N\}\) stands for the set of vertices (nodes), \(\) denotes the set of edges, \(:_{+}\) maps them to their positive weights, and \(f:\). We call \(_{f}^{}\) a _\(f\)-distance matrix in \(\)_. Note that if \(f(x)}}{{=}}x\), then \(_{f}^{}\) is the Shortest Path Kernel matrix.

The product \(_{f}^{}\) (where \(^{N}\)) represents a scalar field on \(\) obtained by discretely integrating the field defined by \(\). In this integration, a new field value at a vertex \(v\) is calculated by averaging the old field values at all vertices \(u\), weighted according to the function \(f((v,u))\). This integration canbe extended to general tensor fields by replacing vector \(^{N}\) with a tensor \(^{N d_{1} d_{2}}\):

\[_{f}^{}[i]=_{j()}f( (i,j))[j] \]

We refer to the above procedure as the \(f\)-integration of a field \(\) on \(\). We will use the terms _graph field integration_ (GFI) and _multiplication with \(f\)-distance matrices_ interchangeably throughout the paper. When the graph, \(\), is a tree, we call this procedure (Eq. 1) _tree field integration_. Next, we highlight several applications that rely on multiplications with \(f\)-distance matrices, \(_{f}^{}\).

1. **Interpolation on manifolds:** This task involves predicting unseen values on a manifold from a set of known values. For example, predicting the velocities of all points on a flag with known velocities for a few points (Praff et al., 2021). For a discretized manifold, the interpolated values can be obtained using a weighted average using graph field integration (Eq. 1).
2. **Optimal Transport (OT):** A popular method used to solve the entropic OT problem (Peyre and Cuturi, 2019) is the Sinkhorn algorithm (Eckstein and Nutz, 2022). Sinkhorn relies on multiplications with _cost matrices_, which are special cases of \(f\)-distance matrices for metric spaces induced by shortest-path distances in graphs. This can be efficiently solved using graph field integration.
3. **Topological Transformers (TTs):** Topological Transformers (Choromanski et al., 2022) are extensions of traditional Transformers (Vaswani et al., 2017) for graph inputs. TTs modify the 1-D relative positional encoding (RPE) using "mask matrices", which are \(f\)-distance matrices. We show how these matrices can be efficiently integrated into the attention mechanism (Sec. 4.4).

In the above applications, apart from the graph field integration step, the bottleneck lies in the process of explicitly materializing the \(f\)-distance matrix. Naively performing the integration in Eq 1 consists of two steps: **(a)** computing the \(f\)-distance matrix, \(_{f}^{}\), which requires \(O(N^{3})\) time in the worst case (which we call _preprocessing_), and **(b)** performing the multiplication takes \(O(N^{2})\) time. This is prohibitively expensive while using large graphs.

In this paper, we introduce a new class of fast polylog-linear algorithms for graph field integration that uses low displacement rank (LDR) matrices (Thomas et al., 2018; Chandrasekaran et al., 2018). To summarize, our primary contributions are given below:

1. We provide the first **exact** polylog-linear multiplication algorithms called **Fast Tree-Field Integrators** (FTFIs), for general weighted trees and a rich class of maps \(f\), including rational, trigonometric, exponential and exponentiated quadratic functions (Sec. 3.2).
2. We show how Fast Tree-Field Integrators can be applied to support fast computations on general graphs by approximating graph metrics with tree metrics (Sec. 4).
3. We show that FTFs are **5.7-10x** faster than baseline graph field integration methods for large-scale graphs (Sec. 4.1 and 4.2).
4. We showcase the efficacy of FTFs in several applications including graph classification (Sec. 4.2), interpolation on meshes (Sec. 4.2), and Topological Vision Transformers (TVTs) (Sec. 4.4). For TVTs, we propose new relative position encoding (RPE) masking mechanisms by introducing only **three** extra learnable parameters, which leads to **1.0-1.5%** accuracy gains. We provide an exhaustive evaluation on Vision Performers (**25** models on multiple datasets). Some of our best models use exponentiated quadratic functions \(f\), which has not been applied in this context before.

For completeness, we also propose approximate FTFI extensions via _Non-Uniform FFT_ (NU-FFT) (Kircheis et al., 2023) and random Fourier features (RFFs) (Rahimi and Recht, 2007) (Sec. A.2).

## 2 Related work

Efficient graph field integration (Eq. 1) has been studied by prior works for different classes of matrices. For example, Al-Mohy and Higham (2011) considered exponentiated adjacency matrix-vector multiplication, Spielman and Teng (2012) targeted symmetric diagonally dominant matrices (e.g., Laplacian), Arrigo et al. (2018) analyzed matrices that are power series of random walk kernels. In contrast to these approaches, Saad and Schultz (1986) proposed general iterative methods for solving certain linear systems using Arnoldi's iterations. However, These iterative methods can suffer from convergence issues. Williams (2007) showed that it is possible to pre-process any boolean matrix to achieve sub-quadratic matrix-vector multiplication.

The general problem of computing the action of a matrix on a vector, where the matrix is the graph kernel, in sub-quadratic time is intractable, except for a few special cases (Al-Mohy and Higham, 2011; Choromanski et al., 2023). In this work, we embed the graph \(G\) under consideration in a tree (replacing the graph metric by the underlying _tree metric_). Then, we leverage the tree structure to approximate the action of the kernel on a given vector by providing **exact** integration on a tree.

Previous works (Bartal et al., 2022, 2019; Abraham et al., 2008; Bartal, 1998) have used the theory of _tree metrics_ (TMs) in several applications in mathematics and computer science. TMs are widely used to embed a complex metric space (e.g., a Riemannian manifold) into a more tractable one, while approximately preserving (all or most of the) pairwise distances. They find applications in distributed & online algorithms (Khan et al., 2008; Bubeck et al., 2018), biology (Mossel, 2007), vision, robotics (Athitsos and Sclaroff, 2003), and ML (e.g., metric spaces' regression (Gottlieb et al., 2011)).

Tree metrics for fast matrix multiplication:Applying tree metrics (TM) to compute approximate \(_{f}^{G}\) is a natural approach to scale up matrix multiplications. If a TM approximates the metric space well, then the derived embeddings should have low distortion. However, in the worst-case scenario, this is not true for deterministic _tree embeddings_. A natural alternative is to sample trees from probabilistic distributions, which are shown to provide logarithmic distortion in expectation (Fakcharoenphol et al., 2004; Bartal et al., 2022). This can be further improved to constant distortion for certain classes of metrics, e.g., celebrated _snowflake metrics_(Leeb, 2016). For graph metrics defined by shortest-path distances, there exist spanning trees providing constant average distortion (over all pairs of nodes). These spanning trees can be constructed as _near minimum weight spanning trees_(Bartal et al., 2016). Unfortunately, explicit application of _any_ tree metric still requires \(O(N^{2})\) time (impractical for large \(N\)) to: **(1)** compute all shortest-path distances via the breadth-first-search algorithm (BFS), even if sub-quadratic methods were used to construct a tree (e.g. minimum spanning tree), **(2)** store the matrix, and **(3)** perform matrix-vector multiplications. We provide more details about work related to graph field integration in Appendix B.

## 3 Fast Tree-Field Integrators (FTFI)

In this section, we present our approach for performing efficient field integration on a tree, which we call _fast tree field integrator_. We begin by introducing the concept of integrator trees (ITs), which is a

Figure 1: Pictorial representation of the IntegratorTree (see: Sec 3.1) data structure for the nine-vertex input tree \(\) on the left. Numbers in blue next to the input tree denote the weights of its edges. Leaves of the IntegratorTree object represent \(f\)-transformed (element-wise) distance matrices: \(_{0},_{1},_{2},_{3}\) for sub-trees induced by vertex-sets: \(\{1,2,4\},\{1,3,0\},\{5,7,8\}\) and {5,6,0} respectively. Different _levels_ correspond to different distances from the pivot point.

specialized decomposition of a tree using the theory of _balanced separators_ (Sec 3.1). Subsequently, we leverage these integrator trees to execute efficient integration on a tree via a _divide-and-conquer algorithm_ (Sec 3.2).

### IntegratorTrees (ITs) - preliminaries

To support fast integration for various tensor fields \(^{N d_{1} d_{s}}\) defined on a given input tree \(\), we first design a special data structure that we refer to as an _IntegratorTree_ (IT). An object of this type is constructed only once per \(\), regardless of the number of tensor fields used. An IT is a rooted binary tree. To avoid confusion, we will refer to its vertices as _nodes_, reserving term _vertices_ for those of \(\). Each node of IT corresponds to the induced sub-tree \(\) of \(\). For every non-leaf node corresponding to some \(\), a _pivot_ point \(p\) along with two sub-trees: \(_{}\) and \(_{}\) are constructed. The following needs to be satisfied:

* \(|_{x}||}{4}\) for \(x\{\}\), \(\),
* \(_{x}_{y}=\{p\}\) (\(||\) denotes the number of vertices).

The next lemma shows that every tree \(\) with \(|| 6\) has the above decomposition and it can be efficiently found.

**Lemma 3.1** (**Pivoting**).: _If \(\) is a tree with \(|| 6\), then \(\) admits a decomposition (\(_{},_{},p\)) given above and it can be constructed in **linear** time._

The algorithmic proof is provided in Appendix A.1 and uses standard tools from the theory of balanced separators.

The _left child_ of the non-leaf node for \(\) corresponds to \(_{}\) and the _right child_ to \(_{}\). In addition to these two pointers, a non-leaf node also contains eight extra fields, partitioned into two groups, one corresponding to its left child and one to its right children. The fields corresponding to the left child are as follows:

* **Left-ids:** an array of the ids (in \(\)) of those vertices that are in \(_{}\), mapping the ids of vertices in \(_{}\) to the original ids in \(\) (each sub-tree uses consecutive numbers from \(0\) as ids locally).
* **Left-d:** an array of different shortest-path **d**istances from the pivot point to the vertices in \(_{}\).
* **Left-id-d:** an array mapping the ids of vertices (in \(_{}\)) to the indices in left-d of their corresponding distances from the pivot point.
* **Left-s:** a corresponding array of the ordered sub-sets of ids (in \(_{}\)) of vertices within a particular distance from the pivot point.

Fields corresponding to the right child are defined similarly. The leaf nodes of the IT consist only of the \(f\)-transformed (element-wise) distance matrices \(\) for their corresponding sub-trees (see: Fig 1). In principle, the leaf nodes of IT correspond to sub-trees with less than \(t=6\) vertices each. In practice, we choose higher \(t\), for more efficient integration (see: discussion in Sec. 4.1).

Time & space complexity of constructing ITs:From what we have said so far, it is clear that an IT can be constructed by applying _breadth first search_ (BFS) and the linear algorithmic procedure for constructing the decomposition from Lemma 3.1. Note that every vertex of the input tree appears in the logarithmic number of nodes in the IT since the size of the sub-tree is at most \(\) the size of its parent in IT. We conclude that IT for the given input tree \(\) can be computed in \(O(N(N))\) time, where \(N\) stands for the number of vertices \(||\) of \(\).

### Integrating with IntegratorTrees

We are ready to explain how ITs allow us to efficiently integrate any given tensor field \(^{N d_{1} d_{s}}\) defined on \(\) for a wide class of function \(f:\). We will apply a _divide-and-conquer_ strategy.

We start in the root node of IT. If that node is a leaf then the \(f\)-transformed distance matrix is stored and can be directly used for matrix-tensor multiplication. If this node is not a leaf, then it encodes the decomposition \((_{},_{},p)\). Take some \(v(_{})\). Note that the value \(_{f}^{}[v]\) of the new field in \(v\) after \(f\)-integration is given as follows for \(=(_{})\{p\}\):

\[(_{})}f(( v,j))[j]}_{_{}(v)}+}f((v,j))[j]}_{_{}(v) }. \]

To compute the new values of the field for nodes \(v_{}\), one needs to:

1. Compute the contribution to it from \(_{}\) (\(_{}(v)\)-terms). This can be done simply by applying Eq. 2 recursively for \(_{}\), which means traversing to the left child of the root.
2. Add the so-called _cross-terms_ contributions coming from the vertices of \(\) (\(_{}(v)\)-terms).

The key observation is that the latter (cross-term) contributions can be retrieved simply by computing \(^{}\), where: (1) \(^{k l}\) with \(k\) and \(l\) being the sizes of the node's left-d and right-d arrays respectively. \((i,j)=f([i]+[j])\), and (2) Let \(b_{j}}}{{=}}|[j]|\) where \(||\) refers to the size of the subset. Then \(^{}^{l d_{1} d_{s}}\) is defined as follows:

\[^{}[j]}}{{=}}_{z=0}^{b _{j}-1}[[[j][z]]]. \]

Given the structure of IT, tensor \(^{}\) can be computed in linear time. Note that the following holds:

\[_{}(v)=(^{})[(v)]-f( [(v)])^{}, \]

where \((v)=[v]\). Analogous analysis can be derived for \(v_{}\), with matrix \(^{}\) replacing \(\). Thus the overall time complexity of the cross-terms computations is determined by the algorithm for matrix-tensor multiplications with matrices \(\) and \(^{}\).

2.1 The case for structured matrices: multiplications with \(,^{}\) and cordiality

Matrices \(,^{}\) are of the form: \([f(x_{i}+y_{j})]_{i=1,,a}^{j=1,,b}\) for some sequences \(X=(x_{i})_{i=1}^{a}\), \(Y=(y_{j})_{j=1}^{b}\) and \(a,b_{+}\).

**Definition 3.2** (cordial functions).: A function \(f:\) is _\(d\)-cordial_ (or: _cordial_ if \(d\) is not specified), if there exists \(d\) such that matrix-vector multiplication with a matrix \(=[f(x_{i}+y_{j})]_{i=1,,a}^{j=1,,b}\) can be conducted in time \(O((a+b)^{d}(a+b))\) for every \((x_{i})_{i=1}^{a}\), \((y_{j})_{j=1}^{b}\).

Next, we demonstrate the importance of cordial functions in our FTFI framework.

**Lemma 3.3** (\(f\)-integration with cordial functions).: _If \(f\) is \(d\)-cordial then \(f\)-integration for the general weighted tree of \(N\) vertices can be conducted in time \(O(N^{d+1}(N))\)._

Proof.: Denote by \(T(N)\) time complexity for running FTFI on the \(N\)-vertex tree. We have the following recursive formula for \(T\), where \( c\):

\[T(N) T(cN)+T((1-c)N)+O(N^{d}(N)) \]

This is implied by the fact that: (1) the size of each sub-tree is at most \(\) the size of its parent, (2) the computation across left and right children is dominated by multiplications with matrices \(\) and \(^{}\). The solution of this recursion leads to the statement. 

Next, we show some practical implications of Lemma 3.3, where tree weights are **completely arbitrary**. Additional results are given in Sec. A.2.3.

Rational functions:We claim that every rational \(f\) is \((2+)\)-cordial for any \(>0\). We will use Lemma 1 from  stating that: given any set of \(b\) rational functions \(R_{j}(x)=(x)}{Q_{j}(x)}\) and \(\{x_{i}\}_{i=1}^{a}\), one can compute the \(a\) values \(_{j=1}^{b}R_{j}(x_{i})\) in time \(O((a+b)^{2}(b)((b)))\) (by applying FFT). For a given vector \(^{b}\), it thus suffices to define: \(R_{j}(x)=v_{j}f(x+y_{j})\) and that lemma can be applied to efficiently compute \(\). We conclude that for any \(>0\), \(f\)-integration can be conducted in \(O(N^{3+}(N))\) time for \(N\)-vertex weighted trees and any rational \(f:\) (see also: Sec. 4.3, Sec. 4.2, Sec. 4.4).

Polynomial functions:The above result on rational functions clearly applies also to polynomial \(f\), but here we can do better. We show that \(f\) is \(0\)-cordial. Assume that \(f(x)=_{t=0}^{B}a_{t}x^{t}\). We have: \(=_{t=0}^{B}_{l=0}^{t}a_{t}_{t,t-l}}\), where matrix \(_{u,v}^{a b}\) is defined as an outer-product of two vectors: \((x_{1}^{u},...,x_{a}^{u})^{a}\) and \((y_{1}^{v},...,y_{b}^{v})^{b}\). Thus each \(_{u,v}\) supports linear matrix-vector multiplication (via associativity property). The proof is completed, since \(B\) is a constant. We conclude that \(f\)-integration can be conducted in \(O(N(N))\) time for \(N\)-vertex weighted trees and any polynomial \(f:\) (see: Fig. 2 and Fig 9).

Exponential functions:Take \(f(x)=( x)\). Then \(\) is an outer-product of two vectors: \((( x_{i}))_{i=1}^{a}^{a}\) and \((( y_{j}))_{j=1}^{b}^{b}\). The remaining analysis and conclusion is thus the same as for the polynomial case (see also: Sec. 4.4).

Function:\(f(x)=\): (c) is a constant) We claim that \(f\) is \(2\)-cordial. In that setting, matrix \(\) satisfies: \((i,j)=)( y_{j})}{(x_{i}+)+(y_{j}+)}\) and thus is a _Cauchy-like_ LDR, supporting fast \(O(N^{2}(N))\) matrix-vector multiplication (Victor Y. Pan, 2000). We conclude that \(f\)-integration can be conducted in \(O(N^{3}(N))\) time for \(N\)-vertex weighted trees and \(f(x)=\) (see: Fig. 2).

Functions \(f(x)=(ux^{2}+vx+w)\) and trees with positive rational weights:Now matrix \(\) can be re-written as \(=(w)_{1}_{2}\), where \(_{1}^{a a}\) and \(_{2}^{b b}\) are diagonal, with diagonal entries given by sequences \(\{(ux_{i}^{2}+vx_{i})\}_{i=1}^{a}\) and \(\{(uy_{j}^{2}+vy_{j})\}_{j=1}^{b}\) respectively, and furthermore \(\) is the _generalized Vandermonde matrix_ (GVM) (using arbitrary nonnegative integers as exponents). It is defined as: \((i,j)=r_{i}^{s_{j}}\), where \(r_{i}=(}{q})\) and \(s_{j}=y_{j}q\). As in the previous case, the embedding trick can be applied, but we will use it only for columns. That effectively leads to the completion of the set of exponents \(\{s_{j}\}\) to the set of consecutive integers starting from \(0\) and a regular Vandermonde matrix, that supports \(O(N^{2}(N))\) matrix-vector multiplication, replacing GVM. The benefit of this embedding, as compared to the previous one, is that even though it still increases the number of columns by a multiplicative factor of \(p\), the number of rows does not change. Therefore, for \(p(N)\), substantial computational speedups are achieved (see: Sec. 4.4).

## 4 Experiments

In this section, we outline the experimental setup and report the performance of FTFI across various settings. For all the experiments, we only consider minimum spanning tree (MST) as an approximation of our graph. Specifically, we design experiments to answer these research questions:

Figure 2: Pictorial representations of the main concepts behind efficient matrix-vector multiplications \(\) with \(^{5 4}\), for the polynomial \(f\) and \(f(x)=\). In the polynomial case, \(\) is re-written as a sum of low-rank outer-product matrices corresponding to terms of different degrees (e.g., constant, linear, quadratic, etc.). Matrix associativity property is applied for efficient calculations (dotted-border blocks indicating the order of computations). In the second case, \(\) is high-rank, but the so-called _low displacement rank operator_\(_{D_{1},D_{2}}:_{1}- _{2}\) for diagonal \(_{1},_{2}\) can be applied to make it a low-rank outer-product matrix. The multiplication with \(\) can be efficiently performed using the theory of LDR matrices (Thomas et al., 2018).

* **How efficient are FTFIs for tree field integration?**
* **How does the approximation quality of FTFI compare to other integration algorithms?**
* **How can we further improve the approximation quality in FTFI?**
* **How can we use FTFI in real-world large-scale settings?**

### Runtime Efficiency of FTFI

The main goal of this experiment is to evaluate the speedups obtained by FTFI as compared to brute-force tree field integrator (BTFI) i.e. the explicit calculation of Eq 1 on a tree. We consider two classes of graphs: **(a)**_synthetic_, obtained from a path-graph by adding random edges and **(b)**_mesh graphs_ from Thingi10K [Zhou and Jacobson, 2016] dataset. For BTFI, we compute the MST and then integrate a random scalar field \(\) on the vertices of the MST. Since BTFI & FTFI are numerically equivalent, we report the pre-processing time and integration as a function of vertex count (\(N\)) in Fig. 3. We observe that FTFI achieves up to **13x** speedups for 20K-vertex meshes and **5.7x+** for synthetic graphs with over 10K vertices compared to BTFI.

### Approximation Quality of FTFI

**Interpolation on meshes.** We compare the efficiency of FTFI with baselines on the _normal vector prediction task_. Every node of the considered mesh \(\) with a vertex-set \(\), is associated with a location \(_{i}^{3}\) and a vertex normal \(_{i}^{3}\). For each mesh, we randomly select a subset \(^{}\) with \(|^{}|=0.8||\) and mask out their vertex normals (set as zero vectors). The interpolation task involves predicting the vertex normals of each masked node \(i^{}\) as: \(_{i}=_{j^{}}_{j }(i,j)_{j},\) where \(_{f}(w,v)=f((w,v))\), with \((w,v)\) being the shortest path distance between node \(w\) and \(v\), and \(f\) is a rational function \(f(x)=1/(1+ x^{2})\). We perform a grid search to set hyperparameter \(\) for each mesh and report the result with the highest cosine similarity between predicted and ground truth vertex normals, averaged over all the nodes. We run tests on **40 meshes** of the 3D-printed objects with a wide range of sizes from the Thingi10K dataset (details in Appendix D.3). We compare FTFI with BTFI, low-distortion tree-based algorithms such as Bartal Trees [Bartal, 1996] and FRT trees [Fakcharoenphol et al., 2004a] alongside the state-of-the-art method for graph-field integration, the Separator Factorization (SF) algorithm [Choromanski et al., 2023]. We also compare against the baseline BGFI which entails explicitly materializing the kernel matrix of \(\) and then performing matrix tensor multiplication with a tensor field \(\) defined by the \(_{i}\)'s.

Preprocessing involves building specific tree structures (FRT, Bartal), calculating the kernel matrices (BGFI, BTFI), or creating specialized data structures (SF, FTFI) for efficient later use. The first two plots in Fig. 4 shows the pre-processing time and cosine similarity for various algorithms applied to meshes of different sizes. FTFI is the fastest in terms of pre-processing time and achieves competitive performance in terms of cosine similarity (between predicted and actual vertex normals) when compared with the SF algorithm while being numerically equivalent to BTFI. FTFI is a few orders of magnitude faster than BTFI and the tree-based methods while maintaining accuracy.

**Graph classification.** Graph kernels have been widely used for graph classification tasks in previous works [Kriege et al., 2020, Nikolentzos et al., 2021]. We compare the classification results obtained using the approximate kernel from FTFI with those from the exact SP kernel. In this setting, we use the Shortest Path (SP) kernel, \(f((i,j))\). We perform experiments on a wide range of bioinformatics and social networks datasets like D&D, Mutag, Reddit, Imdb, among others. We follow [de Lara and Pineau, 2018] and construct the graph feature for both kernels by using the smallest \(k\) eigenvalues (\(k\) is a hyperparameter). This feature set is then used for classification, using

Figure 3: Runtime comparison of FTFI with BTFI as a function of the number of vertices, \(N\). **Left:** Synthetic graphs. **Right**: Mesh-graphs from Thingi10K. The speed is not necessarily monotonic in \(N\) as it depends on the distribution of lengths of the shortest paths. For each graph, 10 experiments were run (std. shown via dotted lines).

a random forest classifier. We observe that FTFI achieves significant speed improvements while achieving similar accuracy compared to its brute-force counterpart, BGFI (see Fig. 5). We provide more details about the experimental setup and baselines Appendix D.4. We also report additional experiments on meshes and point clouds in Appendix D.1.

### Improving approximation quality with learnable \(f\)-distance matrices

We propose to further improve the approximation quality of FTFI by learning a \(f\)-distance matrix on metrics derived from the MST. As an application, we choose _general graph metrics_, where our goal is to learn the shortest-path distance \(d_{v,w}\) between a given pair of nodes \((v,w)\) in a graph. Given a \(f\)-distance matrix and tree-derived metric \(_{v,w}\) the objective is to learn a mapping to minimize

\[_{(v,w)}[(d_{v,w}-f^{a_{0},...,a_{t}}_{b_{0},...,b_{s}}(_{v,w}))^{2}]. \]

Rather than using a fixed \(f\), we parameterize and train it. We consider rational function \(f\):

\[f^{a_{0},...,a_{t}}_{b_{0},...,b_{s}}(x)=+a_{1}x+...+a_{t}x^{t}}{b_ {0}+b_{1}x+...+b_{s}x^{s}}, \]

where \(a_{0},...,a_{t},b_{0},...,b_{s}\) are trainable parameters.

Figure 4: Speed (pre-processing time) and accuracy (cosine similarity) comparison of the FTFI and other baselines for vertex normal prediction on meshes. Cosine similarity of BFFI and FTFI almost overlaps. The last two figures are qualitative examples showcasing the tradeoff between cosine similarity and preprocessing time for meshes of sizes 3K and 5K nodes respectively.

Figure 5: Trade-off plot comparing graph classification accuracy and feature processing time for the classifiers using FTFI and BGFI. FTFI achieves similar accuracy as BGFI while significantly reducing fp time across most datasets. We report the reduction in FTFI’s processing time (\(\)x%) compared to BGFI using a dotted line.

Training dataset \(\). For a graph \(\), we randomly sample vertices. The training dataset consists of tuples of the form: \((v,w,d_{v,w},_{v,w})\), where \(v,w\) are randomly sampled vertices. Each data point can be constructed in time \(O(N(N))\), or even \(O(N)\) if weights are in \(\)[Thorup, 1997].

Final evaluation. To evaluate the quality of the approximation, we compute the relative Frobenius norm error: \(=_{}^{T}-_{}^{G}\|_{ }}{\|_{}^{G}\|_{}}\), where \(\|\|_{}\) stands for the _Frobenius norm_, \(\) is a tree for a given graph \(\) and \(\) is an identity function (see: our notation from Sec. 1). It quantifies how closely the distance matrix of \(\) is approximated by the \(f\)-distance matrix of \(\). Computing \(\) is expensive and our training does not rely on it. Our empirical results show that the relative error, \(\), can be substantially improved by using the light-weight MSE training loss (defined in Eq. 6).

We report the evaluation error for these experiments in Fig. 6 (with additional results in Fig. 8 in the Appendix). We observe that a rational function with quadratic numerator and denominator provides strong performance across different graphs. We notice that increasing the training set to \(>100\) data points does not have a substantial impact on the final error. Estimating the coefficients of \(f\) provides approximation improvements across all graphs in as few as **40 training steps**.

These above results show that tree-based estimators are expressive enough to emulate integration on arbitrary graphs. This expressive power can be further enhanced by pairing them with "nonlinear" functions \(f\). Thus, they explain why the presented techniques are relevant for general graphs.

### Large Scale Transformer Experiments using FTFI

For large-scale applications of FTFI, we select Topological Vision Transformers (TopViT), [Choromanski et al., 2022], and leverage it for efficient incorporation of masking within ViTs. We provide detailed description of masked Transformers in Appendix C.

**Topological Vision Transformers with trees :** We propose an extension to TopViT that seamlessly integrates FTFI. In this extension, we model the mask matrix as an \(f\)-distance matrix (with learnable \(f\)) defined on the minimum spanning tree (MST) obtained from the 2D grid graph image encoding, where vertices correspond to different patches. We parameterize \(f\) as \(f_{g}^{t}}}{{=}}g(_{i=0}^{t}a_{t}x^{t})\). We use the linear attention mechanism introduced in Performers [Choromanski et al., 2021], where the attention kernel is written as: \((,)=()^{}()\) for a deterministic \(:^{d_{QK}}\), applied element-wise. We experiment with different values of hyperparameters \(g\), \(t\), \(\) and cross-heads parameter sharing strategies as shown in Table 1 (synced indicates that RPE-parameters are shared across different attention heads).

We run experiments on ImageNet and Places365 datasets using ViT-B/16 (see Table 1). For all the kernels, our variants beat the baselines. For \((x)=x^{4}\), the best variant applies an exponentiated quadratic function, for which we apply Vandermonde matrices (see: discussion in Sec. 3.2.1). Our

Figure 6: **Left: Relative Frobenius norm error as a function of the number of training iterations for different sizes \(n\) and learnable quadratic \(f\). Middle: Comparison of the training of different rational functions \(f\) with num:d defining the degree of the numerator and den:d, the degree of the denominator for the synthetic graph obtained from a path on \(N=800\) by adding 600 random edges and assigning random weights taken from \((0,1)\). Right: constructed similarly, but for a sampled mesh graphs from Thingi10k dataset.**

best variant across all kernels (**78.79%**) provides **2%** accuracy gains over the best baseline (**76.76%**). In the synced setting, we use only **three** extra learnable parameters per layer (shared in all attention heads across all layers) and obtain **1-1.5%** accuracy gains. In the asynced setting, we use a small set of **36** extra learnable parameters per layer (3 extra parameters per head). Overall, we observe that FTFI improves the approximation quality within Transformers with a minimal number of parameters. We provide additional discussions on the ViT results for ImageNet in Appendix D.5.1 and for Places365 in Appendix D.5.2.

Additional results on the I-Naturalist dataset, where we outperform various low-rank attention baselines, are provided in Appendix D.5.3.

Larger Transformer models:We scale our experiments to run on the larger ViT-L architectures and evaluate on ImageNet. In this setting, we use RPE mechanism with \(g=\) and \(t=1\) (that provided strong performance in previous experiments) and asynced strategy. We observe that FTFI provides **7%** accuracy improvement (see: Fig. 7).

Further results on Video Transformer (ViViT)  are provided in Appendix D.6. We also provide additional experiments including Gromov-Wasserstein distance computation  (see Sec. D.2), along with code pointers (Appendix D).

## 5 Conclusion

We provided a new class of algorithms for fast and exact integration of tensor fields defined on weighted trees, relying on the theory of structured (in particular low displacement rank) matrices. We showed how those algorithms can be applied for accurate integration on general graphs, in particular via their minimum weight spanning trees. We presented several applications of the presented methods, from graph classification and interpolation on meshes, through graph metric approximation to Topological Vision Transformers. Our methods provide significant (5-13x) speedups while maintaining the quality of their exact counterparts.

## 6 Author Contributions

KC conceived the idea behind FTFI, proved the theoretical results, implemented FTFI algorithm and ran the vision experiments in this paper. AS integrated the FTFI algorithm in the GW style algorithms and ran some graph and point cloud classification tasks. SBRC ran graph classification experiments as well as experiments on the CUBES dataset. HL ran the experiments on the meshes. AD helped develop methods, and along with TS and SC acted as senior advisors for the project. All authors contributed to the writing of the manuscript.

    &  &  \\  & \)} & \)} & \)} &  &  \\   & \)} &  &  &  &  &  &  &  &  &  &  &  &  \\   Na & NA & NA & 76.23 & NA & NA & NA & 75.03 & NA & NA & NA & 76.37 & NA & NA & NA & NA & 76.76 & NA & NA & NA & 54.80 \\ ✓ & \(\) & \(1\) & 77.28 & ✓ & \(\) & 1 & 76.66 & ✓ & \(\) & 1 & 77.84 & ✗ & \(\) & 1 & **78.79** & ✗ & \(\) & 1 & 56.69 \\ ✓ & \(\) & \(2\) & 76.60 & ✓ & \(\) & 2 & 75.91 & ✓ & \(\) & 2 & 77.23 & ✗ & \(