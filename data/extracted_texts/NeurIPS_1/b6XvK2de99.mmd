# One-Step Diffusion Distillation via

Deep Equilibrium Models

 Zhengyang Geng

Carnegie Mellon University

zgeng2@cs.cmu.edu

Equal Contribution. Correspondence to Zhengyang Geng and Ashwini Pokle

Equal Contribution. Correspondence to Zhengyang Geng and Ashwini Pokle

Carnegie Mellon University

apokle@cs.cmu.edu

J. Zico Kolter

Carnegie Mellon University

Bosch Center for AI

zkolter@cs.cmu.edu

###### Abstract

Diffusion models excel at producing high-quality samples but naively require hundreds of iterations, prompting multiple attempts to distill the generation process into a faster network. However, many existing approaches suffer from a variety of challenges: the process for distillation training can be complex, often requiring multiple training stages, and the resulting models perform poorly when utilized in single-step generative applications. In this paper, we introduce a simple yet effective means of distilling diffusion models _directly_ from initial noise to the resulting image. Of particular importance to our approach is to leverage a new Deep Equilibrium (DEQ) model as the distilled architecture: the Generative Equilibrium Transformer (GET). Our method enables fully offline training with just noise/image pairs from the diffusion model while achieving superior performance compared to existing one-step methods on comparable training budgets. We demonstrate that the DEQ architecture is crucial to this capability, as GET matches a \(5\) larger ViT in terms of FID scores while striking a critical balance of computational cost and image quality. Code, checkpoints, and datasets are available here.

## 1 Introduction

Diffusion models  have demonstrated remarkable performance on a wide range of generative tasks such as high-quality image generation  and manipulation , audio synthesis , video , 3D shape , text , and molecule generation . These models are trained with a denoising objective derived from score matching , variational inference , or optimal transport , enabling them to generate clean data samples by progressively denoising the initial Gaussian noise during the inference process. Unlike adversarial training, the denoising objective leads to a more stable training procedure, which in turn allows diffusion models to scale up effectively . Despite the promising results, one major drawback of diffusion models is their slow generative process, which often necessitates hundreds to thousands of model evaluations . This computational complexity limits the applicability of diffusion models in real-time or resource-constrained scenarios.

In an effort to speed up the slow generative process of diffusion models, researchers have proposed distillation methods  aimed at distilling the multi-step sampling process into a more efficient few-step or single-step process. However, these techniques often come with theirown set of challenges. The distillation targets must be carefully designed to successfully transfer knowledge from the larger model to the smaller one. Further, distilling a long sampling process into a few-step process often calls for multiple training passes. Most of the prevalent techniques for online distillation require maintaining dual copies of the model, leading to increased memory and computing requirements. As a result, there is a clear need for simpler and more efficient approaches that address the computational demands of distilling diffusion models without sacrificing the generative capabilities.

In this work, our objective is to streamline the distillation of diffusion models while retaining the perceptual quality of the images generated by the original model. To this end, we introduce a simple and effective technique that distills a multi-step diffusion process into a single-step generative model, using solely noise/image pairs. At the heart of our technique is the Generative Equilibrium Transformer (GET), a novel Deep Equilibrium (DEQ) model  inspired by the Vision Transformer (ViT) [25; 75]. GET can be interpreted as an infinite depth network using weight-tied transformer layers, which solve for a fixed point in the forward pass. This architectural choice allows for the adaptive application of these layers in the forward pass, striking a balance between inference speed and sample quality. Furthermore, we incorporate an almost parameter-free class conditioning mechanism in the architecture, expanding its utility to class-conditional image generation.

Our direct approach for distillation via noise/image pairs generated by a diffusion model, can, in fact, be applied to both ViT and GET architectures. Yet, in our experiments, we show that the GET architecture, in particular, is able to achieve substantially better quality results with smaller models. Indeed, GET delivers perceptual image quality on par with or superior to other complex distillation techniques, such as progressive distillation [68; 88], in the context of both conditional and unconditional image generation. This leads us to explore the potential of GETs further. We preliminarily investigate the scaling law of GETs--how its performance evolves as model complexity, in terms of parameters and computations, increases. Notably, GET exhibits significantly better parameter and data efficiency compared to architectures like ViT, as GET matches the FID scores of a \(5\) larger ViT, underscoring the transformative potential of GET in enhancing the efficiency of generative models.

To summarize, we make the following key contributions:

* We propose Generative Equilibrium Transformer (GET), a deep equilibrium vision transformer that is well-suited for _single-step_ generative models.
* We streamline the diffusion distillation by training GET directly on noise/image pairs sampled from diffusion models, which turns out to be a simple yet effective strategy for producing one-step generative models in both unconditional and class-conditional cases.
* For the first time, we show that implicit models for generative tasks can outperform classic networks in terms of performance, model size, model compute, training memory, and speed.

## 2 Preliminaries

Deep Equilibrium Models.Deep equilibrium models  compute internal representations by solving for a fixed point in their forward pass. Specifically, consider a deep feedforward model with \(L\) layers:

\[^{[i+1]}=f_{}^{[i]}(^{[i]};)i=0,...,L-1 \]

where \(^{n_{x}}\) is the input injection, \(^{[i]}^{n_{z}}\) is the hidden state of \(i^{th}\) layer, and \(f_{}^{[i]}:^{n_{x} n_{z}}^{n_{z}}\) is the feature transformation of \(i^{th}\) layer, parametrized by \(\). If the above model is weight-tied, _i.e.,_\(f_{}^{[i]}=f_{}, i\), then in the limit of infinite depth, the output \(^{[i]}\) of this network approaches a fixed point \(^{*}\):

\[_{i}f_{}(^{[i]};)=f_{}( ^{*};)=^{*} \]

Deep equilibrium (DEQ) models  directly solve for this fixed point \(^{*}\) using black-box root finding algorithms like Broyden's method , or Anderson acceleration  in the forward pass. DEQs utilize implicit differentiation to differentiate through the fixed point analytically. Let \(g_{}(^{*};)=f_{}(^{*};)- ^{*}\), then the Jacobian of \(^{*}\) with respect to the model weights \(\) is given by

\[^{*}}{}=-( (^{*},)}{^{*}})^{-1}(^{*};)}{} \]Computing the inverse of Jacobian can quickly become intractable as we deal with high-dimensional feature maps. One can replace the inverse-Jacobian term with cheap approximations [27; 28; 29] without sacrificing the final performance.

Diffusion Models.Diffusion models [93; 94; 22; 22] or score-based generative models [95; 96] progressively perturb images with an increasing amount of Gaussian noise and then reverse this process through sequential denoising to generate images. Specifically, consider a dataset of i.i.d. samples \(p_{}\), then the diffusion process \(\{(t)\}_{t=0}^{T}\) for \(t[0,T]\) is given by an Ito SDE :

\[=(,t)t+g(t) \]

where \(\) is the standard Wiener process, \((,t):^{d}^{d}\) is the drifting coefficient, \(g():\) is the diffusion coefficient, and \((0) p_{}\) and \((T)(0,I)\). All diffusion processes have a corresponding deterministic process known as the probability flow ODE (PF-ODE)  whose trajectories share the same marginal probability densities as the SDE. This ODE can be written as:

\[=-(t)(t)_{} p( ,(t))t \]

where \((t)\) is the noise schedule of diffusion process, and \(_{} p(,(t))\) represents the score function. Karras et al.  show that the optimal choice of \((t)\) in Eq. (5) is \((t)=t\). Thus, the PF-ODE can be simplified to \(/t=-t_{} p(, (t))=(-D_{}(;t))/t\), where \(D_{}(,t)\) is a denoiser function parametrized with a neural network that minimizes the expected \(L_{2}\) denoising error for samples drawn from \(p_{}\). Samples can be efficiently generated from this ODE through numerical methods like Euler's method, Runge-Kutta method, and Heun's second-order solver .

## 3 Generative Equilibrium Transformer

We introduce the Generative Equilibrium Transformer (GET), a Deep Equilibrium (DEQ) vision transformer designed to distill diffusion models into generative models that are capable of rapidly sampling images using only a single model evaluation. Our approach builds upon the key components and best practices of the classic transformer , the Vision transformer (ViT) , and the Diffusion transformer (DiT) . We will now describe each component of the GET in detail.

Figure 1: **Generative Equilibrium Transformer (GET).** (_Left_) GET consists of two major components: Injection transformer and Equilibrium transformer. The Injection transformer transforms noise embeddings into an input injection for the Equilibrium transformer. The Equilibrium transformer is the equilibrium layer that takes in noise input injection and an optional class embedding and solves for the fixed point. (_Right_) Details of transformer blocks in the Injection transformer (**Inj**) and Equilibrium transformer (**DEQ**), respectively. Blue dotted boxes denote optional class label inputs.

Get.Generative Equilibrium Transformer (GET) directly maps Gaussian noises \(\) and optional class labels \(\) to images \(}\). The major components of GET include an injection transformer (InjectionT, Eq. (7)) and an equilibrium transformer (EquilibriumT, Eq. (8)). The InjectionT transforms tokenized noise embedding \(\) to an intermediate representation \(\) that serves as the input injection for the equilibrium transformer. The EquilibriumT, which is the equilibrium layer, solves for the fixed point \(^{}\) by taking in the noise injection \(\) and an optional class embedding \(\). Finally, this fixed point \(^{}\) is decoded and rearranged to generate an image sample \(}\) (Eq. (9)). Figure 1 provides an overview of the GET architecture. Note that because we are directly distilling the entire generative process, there is no need for a time embedding \(t\) as is common in standard diffusion models.

\[, =(),\;();\;\; \] \[ =(,)\] (7) \[^{} =(^{},, )\] (8) \[} =(^{}) \]

Noise Embedding.GET first converts an input noise \(^{H W C}\) into a sequence of 2D patches \(^{N(P^{2} C)}\), where \(C\) is the number of channels, \(P\) is the size of patch, \(H\) and \(W\) denotes height and width of the original image, and \(N=HW/P^{2}\) is the resulting number of patches. Let \(D=P^{2} C\) denote the width of the network. We follow ViT to use a linear layer to project the \(N\) patches to \(D\) dimensional embedding. We add standard sinusoidal position encoding  to produce the noise embedding \(\). Position encoding plays a crucial role in capturing the spatial structure of patches by encoding their relative positional information.

InjectionT & EquilibriumT.Both InjectionT and EquilibriumT are composed of a sequence of Transformer blocks. InjectionT is called only once to produce the noise injection \(\), while EquilibriumT defines the function \(f_{}\) of the implicit layer \(^{}=f_{}(^{},,)\) that is called multiple times--creating a weight-tied computational graph--until convergence. A linear layer is added at the end of InjectionT to compute the noise injection \(_{l}^{N 3D}\), \(l[L_{e}]\), for each of the \(L_{e}\) GET blocks in EquilibriumT. For convenience, we overload the notation \(_{l}\) and \(\), in the subsequent paragraphs.

Transformer Block.GET utilizes a near-identical block design for the noise injection (InjectionT) and the equilibrium layer (EquilibriumT), differing only at the injection interface. Specifically, the transformer block is built upon the standard Pre-LN transformer block , as shown below:

\[ =+(( ),)\] \[ =+(())\]

Here, \(^{N D}\) represents the latent token, \(^{N 3D}\) is the input injection, LN, FFN, and Attention stand for Layer Normalization , a 2-layer Feed-Forward Network with a hidden dimension of size \(D E\), and an attention  layer with an injection interface, respectively.

For blocks in the injection transformer, \(\) is equal to the class embedding token \(^{1 3D}\) for conditional image generation, i.e., \(=\) for conditional models, and \(=\) otherwise. In contrast, for blocks in the equilibrium transformer, \(\) is the broadcast sum of noise injection \(^{N 3D}\) and class embedding token \(^{1 3D}\), i.e., \(=+\) for conditional models and \(=\) otherwise.

We modify the standard transformer attention layer to incorporate an additive injection interface before the query \(^{N D}\), key \(^{N D}\), and value \(^{N D}\),

\[,, =_{i}+\] \[ =(,,)\] \[ =_{o}\]

where \(_{i}^{D 3D}\), \(_{o}^{D D}\). The injection interface enables interactions between the latent tokens and the input injection in the multi-head dot-product attention (MHA) operation,

\[^{}=(_{q}+_{q})(_{k}+_{k})^{}=_{q}_{k}^{ }^{}+_{q}_{k}^{}+_{q}_{k}^{}^{}+_{q}^{}_{k}, \]

where \(_{q},_{k}^{D D}\) are slices from \(_{i}\), and \(_{q},_{k}^{N D}\) are slices from \(\). This scheme adds no more computational cost compared to the standard MHA operation, yet it achieves a similar effect as cross-attention and offers good stability during training.

Image Decoder.The output of the GET-DEQ is first normalized with Layer Normalization . The normalized output is then passed through another linear layer to generate patches \(}^{N D}\). The resulting patches \(}\) are rearranged back to the resolution of the input noise \(\) to produce the image sample \(}^{H W C}\). Thus, the decoder maps the features back to the image space.

## 4 Experiments

We evaluate the effectiveness of our proposed Generative Equilibrium Transformer (GET) in offline distillation of diffusion models through a series of experiments on single-step class-conditional and unconditional image generation. Here, we use "single-step" to refer to the use of a single model evaluation while generating samples. We train and evaluate ViTs and GETs of varying scales on these tasks. GETs exhibit substantial data and parameter efficiency in offline distillation compared to the strong ViT baseline. Note that owing to the computational resources required to fully evaluate models, we report all our results on CIFAR-10 ; extensions to the ImageNet-scale  are possible, but would require substantially larger GPU resources.

### Experiment setup

We will first outline our data collection process, followed by an in-depth discussion of our offline distillation procedure. We also include a brief summary of training details and evaluation metrics. For detailed network configs and training specifics, please refer to the Appendix.

Data Collection.For unconditional image generation on CIFAR-10 , we generate 1M noise/image pairs from the pretrained unconditional EDM Karras et al. . This dataset is denoted as EDM-Uncond-1M. As in EDM, we sample 1M images using Heun's second-order deterministic solver . Generating a batch of images takes 18 steps or 35 NFEs (Number of Function Evaluations). Overall, this dataset takes up around 29 GB of disk space. The entire process of data generation takes about _4 hours_ on 4 NVIDIA A6000 GPUs using Pytorch  Distributed Data Parallel (DDP) and a batch size of 128 per GPU. In addition to unconditional image generation, we sample 1M noise-label/image pairs from the conditional VP-EDM Karras et al.  using the same settings. This dataset is denoted as EDM-Cond-1M. Both the datasets will be released for future studies.

Offline Distillation.We distill a pretrained EDM  into ViTs and GETs by training on a dataset \(\) with noise/image pairs sampled from the teacher diffusion model using a reconstruction loss:

\[()=_{,}\|-G_{}()\|_{1}\]

where \(\) is the desired ground truth image, \(G_{}()\) is unconditional ViT/GET with parameters \(\), and \(\) is the initial Gaussian noise. To train a class-conditional GET, we also use class labels \(\) in addition to noise/image pairs:

\[()=_{,,}\|-G_{}^{c}(,)\|_{1}\]

Figure 2: **Data and Parameter Efficiency of GET**:(a) (_Left_) GET outperforms PD and a 5× larger ViT in fewer iterations, yielding better FID scores. Additionally, longer training times lead to improved FID scores. (b) (_Right_) Smaller GETs can achieve better FID scores than larger ViTs, demonstrating DEQ’s parameter efficiency. Each curve in this plot connects models of different sizes within the same model family at identical training iterations, as indicated by the numbers after the model names in the legend.

where \(G^{c}_{}()\) is class-conditional ViT/GET with parameters \(\). As is the standard practice, we also maintain an exponential moving average (EMA) of weights of the model, which in turn is used at inference time for sampling.

Training Details and Evaluation Metrics.We use AdamW  optimizer with a learning rate of 1e-4, a batch size of 128 (denoted as 1\(\)BS), and 800k training iterations, which are identical to Progressive Distillation (PD) . For conditional models, we adopt a batch size of 256 (2\(\)BS). No warm-up, weight decay, or learning rate decay is applied. We convert input noise to patches of size \(2 2\). We use 6 steps of fixed point iterations in the forward pass of GET-DEQ and differentiate through it. For the \((1)\) memory mode, we utilize gradient checkpoint  for DEQ's computational graph. We measure image sample quality for all our experiments via Frechet inception distance (FID)  of 50k samples. We also report Inception Score (IS)  computed on 50k images. We include other relevant metrics such as FLOPs, training speed, memory, sampling speed, and the Number of Function Evaluations (NFEs), wherever necessary.

### Experiment Results

We aim to answer the following questions through extensive experiments: 1) Can offline distillation match online distillation for diffusion models using GETs? 2) What is the scaling behavior of GET

Figure 4: Uncurated CIFAR-10 image samples generated by (_Left_) (a) unconditional GET and (_Right_) (b) class-conditional GET. Each row corresponds to a class in CIFAR-10.

Figure 3: (a) (_Left_) **Sampling speed of GET**: GET can sample faster than large ViTs, while achieving better FID scores. The size of each individual circle is proportional to the model size. For GETs, we vary the number of iterations in the Equilibrium transformer (2 to 6 iterations). The trends indicate that GETs can improve their FID scores by using more compute. (b) (_Right_) **Compute efficiency of GET**: Larger GET models use training compute more efficiently. For a given GET, the training budget is calculated from training iterations. Refer to Table 2 for the exact size of GET models.

[MISSING_PAGE_FAIL:7]

distillation. The initial \(12\) passes use \(50\)K iterations, and the last pass uses \(100\)K iterations. Each step of PD uses \(2\) teacher model NFEs. Thus, the overall number of teacher model NFEs can be evaluated as \(2 128\) (batch size) \(\) (\(12\) passes \( 50\)K + \(100\)K) = \(179\)M samples. The number of NFEs of the teacher model increases to \(1.433\)B if we assume that each of \(8\) TPUs use a batch size of \(128\). Consistency distillation  needs \(409.6\)M teacher model NFEs (\(512\) batch size \(\)\(800\)K iterations = \(409.6\)M). In addition, the perceptual loss requires _double_ NFEs as the teacher model.

One-Step Image Generation.We provide results for unconditional and class-conditional image generation on CIFAR-10 in Table 1 and Table 3, respectively. GET outperforms a much more complex distillation procedure--PD with classifier-free guidance--in class-conditional image generation. GET also outperforms PD and KD in terms of FID score for unconditional image generation. This effectiveness is intriguing, given that our approach for offline distillation is relatively simpler when compared to other state-of-the-art distillation techniques. We have outlined key differences in the experimental setup between our approach and other distillation techniques in Table 5.

We also visualize random CIFAR-10  samples generated by GET for both unconditional and class-conditional cases in Figure 4. GET can learn rich semantics and world knowledge from the dataset, as depicted in the images. For instance, GET has learned the symmetric layout of dog faces solely using reconstruction loss in the pixel space, as shown in Figure 4(b).

   Method & NFE \(\) & FID \(\) & IS \(\) \\  Diffusion Models & & & \\  DDPM  & 1000 & 3.17 & 9.46 \\ Score SDE  & 2000 & 2.2 & 9.89 \\ DDIM  & 10 & 13.36 & - \\ EDM  & 35 & 2.04 & 9.84 \\  GANs & & & \\  StyleGAN2  & 1 & 8.32 & 9.18 \\ StyleGAN-XL  & 1 & 1.85 & - \\  Diffusion Distillation & & & \\  KD  & 1 & 9.36 & 8.36 \\ PD  & 1 & 9.12 & - \\ DFND  & 1 & 4.12 & - \\ TRACT-EDM  & 1 & 4.17 & - \\ PD-EDM  & 1 & 8.34 & 8.69 \\ CD-EDM (LPIPS)  & 1 & 3.55 & 9.48 \\  Consistency Models & & & \\  CT  & 1 & 8.70 & 8.49 \\ CT  & 2 & 5.83 & 8.85 \\  Ours & & & \\  GET-Base & 1 & 6.91 & 9.16 \\   

Table 1: Generative performance on unconditional CIFAR-10.

   Model & FID\(\) & IS\(\) & Params\(\) & FLOPs\(\) & Training Mem\(\) & Training Speed\(\) \\  ViT-Base & 11.49 & 8.61 & 85.2M & 23.0G & 10.1GB & 4.83 iter/sec \\ GET-Mini & 10.72 & 8.69 & 19.2M & 15.2G & 9.2GB & 5.79 iter/sec \\ GET-Mini-\((1)\) & - & - & - & - & 5.0GB & 4.53 iter/sec \\   

Table 4: Benchmarking GET against ViT on unconditional image generation on CIFAR-10. For the first time, implicit models for generative tasks _strictly_ surpass explicit models in all metrics. Results are benchmarked on 4 A6000 GPUs using a batch size of 128, 800k iterations, and PyTorch  distributed training protocol. Training Mem stands for training memory consumed per GPU. \((1)\) symbolizes the \((1)\) training memory mode, which differs only in training memory and speed.

   Models & Params & NFE \(\) & FID \(\) & IS \(\) \\  GET-Tiny & 8.6M & 1 & 15.19 & 8.37 \\ GET-Mini & 19.2M & 1 & 10.72 & 8.69 \\ GET-Small & 37.2M & 1 & 8.00 & 9.03 \\ GET-Base & 62.2M & 1 & 7.42 & 9.16 \\ GET-Base+ & 83.5M & 1 & 7.19 & 9.09 \\  More Training & & & \\  GET-Tiny-4\(\)Iters & 8.9M & 1 & 11.47 & 8.64 \\ GET-Base-2\(\)BS & 62.2M & 1 & 6.91 & 9.16 \\   

Table 2: Generative performance of GETs on unconditional CIFAR-10.

## 5 Related Work

Distillation techniques for diffusion models.Knowledge distillation (KD)  proposed to distill a multi-step DDIM  sampler into the pretrained UNet by training the student model on synthetic image samples. There are several key differences from this work: Our approach does not rely on temporal embeddings or generative pretrained weights and predicts images instead of noises. Further, GET is built upon ViT , unlike the UNet in KD. Additionally, we demonstrate the effectiveness of our approach on both unconditional and class-conditional image generation.

Progressive distillation (PD)  proposes a strategy for online distillation to distill a \(T\)-step teacher DDIM  diffusion model into a new \(T/2\) step student DDIM model, repeating this process until one-step models are achieved. Transitive closure time-distillation (TRACT)  generalizes PD to distill \(N>2\) steps together at once, reducing the overall number of training phases. Consistency models  achieve online distillation in a single pass by taking advantage of a carefully designed teacher and distillation loss objective.

Diffusion Fourier neural operator (DFNO)  maps the initial Gaussian distribution to the solution trajectory of the reverse diffusion process by inserting the temporal Fourier integral operators in the pretrained U-Net backbone. Meng et al.  propose a two-stage approach to distill classifier-free guided diffusion models into few-step generative models by first distilling a combined conditional and unconditional model, and then progressively distilling the resulting model for faster generation.

Fast sampler for diffusion models.While distillation is a predominant approach to speed up the sampling speed of existing diffusion models, there are alternate lines of work to reduce the length of sampling chains by considering alternate formulations of diffusion model [48; 50; 94; 96; 103], correcting bias and truncation errors in the denoising process [9; 10; 90], and through training-free fast samplers at inference [24; 43; 50; 59; 65; 108]. Several works like Improved DDPM , SGM-CLD , EDM  modify or optimize the forward diffusion process so that the reverse denoising process can be made more efficient. Diffusion Exponential Integrator Sampler (DEIS)  uses an exponential integrator over the Euler method to minimize discretization error while solving SDE. DPM-Solver , and GENIE  are higher-order ODE solvers that generate samples in a few steps.

## 6 Limitations

Our method for offline distillation relies on deterministic samplers to ensure a unique mapping between initial noise \(\) and image \(\). As a result, it cannot be directly applied to stochastic samplers which do not satisfy this requirement. However, this limitation also applies to many other distillation techniques, as they cannot maintain their fidelity under stochastic trajectories [11; 66; 88; 97].

   Model & FID \(\) & IS \(\) & BS & Training Phases & \#Models & Trajectory & Teacher \\   KD \({}^{}\) & 9.36 & - & \(4\) & \(1\) & \(1\) & ✗ & DDIM \\ PD  & 9.12 & - & \(1\) & \(_{2}(T)\) & \(2\) & ✓ & DDIM \\ DFNO \({}^{}\) & 4.12 & - & \(2\) & \(1\) & \(1\) & ✓ & DDIM \\ TRACT  & 14.40 & - & \(2\) & \(1\) & \(1\) & ✓ & DDIM \\ TRACT  & 4.17 & - & \(2\) & \(2\) & \(1\) & ✓ & EDM \\ PD-EDM [88; 97] & 8.34 & 8.69 & \(4\) & \(_{2}(T)\) & \(2\) & ✓ & EDM \\ CD\({}^{}\) & 3.55 & 9.48 & \(4\) & \(1\) & \(3\) & ✓ & EDM \\ Ours\({}^{}\) & 7.42 & 9.16 & \(1\) & \(1\) & \(1\) & ✗ & EDM \\ Ours\({}^{}\) & 6.91 & 9.16 & \(2\) & \(1\) & \(1\) & ✗ & EDM \\  Guided Distillation  & 7.34 & 8.90 & \(4\) & \(_{2}(T)+1\) & \(3\) & ✓ & DDIM \\ Ours\({}^{}\) & 6.25 & 9.40 & \(2\) & \(1\) & \(1\) & ✗ & EDM \\   

Table 5: Comparison of relevant training and hyperparameter settings for common distillation techniques. GET requires neither multiple training phases nor any trajectory information. We only count the number of models involved in the forward pass and exclude EMA in #Models. \({}^{}\) indicates offline distillation techniques. \({}^{}\)For CD, we count the VGG network used in the perceptual loss .

Conclusion

We propose a simple yet effective approach to distill diffusion models into generative models capable of sampling with just a single model evaluation. Our method involves training a Generative Equilibrium Transformer (GET) architecture directly on noise/image pairs generated from a pre-trained diffusion model, eliminating the need for trajectory information and temporal embedding. GET demonstrates superior performance over more complex online distillation techniques such as progressive distillation [68; 88] in both class-conditional and unconditional settings. In addition, a small GET can generate higher quality images than a \(5\) larger ViT, sampling faster while using less training memory and fewer compute FLOPs, demonstrating its effectiveness.

## 8 Acknowledgements

Zhengyang Geng and Ashwini Pokle are supported by grants from the Bosch Center for Artificial Intelligence.