# Localized Zeroth-Order Prompt Optimization

Wenyang Hu\({}^{@sectionsign}\), Yao Shu\({}^{}\), Zongmin Yu\({}^{}\), Zhaoxuan Wu\({}^{@sectionsign@paragraphsign}\), Xiaoqiang Lin\({}^{}\),

Zhongxiang Dai\({}^{}\), See-Kiong Ng\({}^{@sectionsign}\), Bryan Kian Hsiang Low\({}^{}\)

Dept. of Computer Science, National University of Singapore, Republic of Singapore\({}^{}\)

Institute of Data Science, National University of Singapore, Republic of Singapore\({}^{@sectionsign}\)

Guangdong Lab of AI and Digital Economy (SZ)\({}^{}\)

Singapore-MIT Alliance for Research and Technology, Republic of Singapore\({}^{@paragraphsign}\)

School of Data Science, The Chinese University of Hong Kong, Shenzhen\({}^{}\)

wenyang.hu@u.nus.edu, shuyao@gml.ac.cn

{yuzongmin, wu.zhaoxuan, xiaoqiang.lin}@u.nus.edu

daizhongxiang@cuhk.edu.cn, seekiong@nus.edu.sg, lowkh@comp.nus.edu.sg

Equal contribution.

###### Abstract

The efficacy of large language models (LLMs) in understanding and generating natural language has aroused a wide interest in developing prompt-based methods to harness the power of black-box LLMs. Existing methodologies usually prioritize a global optimization for finding the global optimum, which however will perform poorly in certain tasks under budget constraints. This thus motivates us to re-think the necessity of finding a global optimum in prompt optimization. To answer this, we conduct a thorough empirical study on prompt optimization and draw two major insights. Contrasting with the rarity of global optimum, local optima are usually prevalent and well-performed, which can be more worthwhile for efficient prompt optimization (**Insight I**). The choice of the input domain, including both the generation and the representation of prompts, affects the identification of well-performing local optima (**Insight II**). Inspired by these insights, we propose a novel algorithm, namely _localized zeroth-order prompt optimization_ (ZOPO), which incorporates a Neural Tangent Kernel-based derived Gaussian process into standard zeroth-order optimization for an efficient search of well-performing local optima in prompt optimization. Remarkably, ZOPO outperforms existing baselines in terms of both the optimization performance and the query efficiency, which we demonstrate through extensive experiments. Our implementation is available at [https://github.com/allen4747/ZOPO](https://github.com/allen4747/ZOPO).

## 1 Introduction

Large language models (LLMs) have demonstrated remarkable capabilities for understanding and generating natural languages . Thanks to the instruction-following abilities of LLMs , prompting--adding crafted, discrete prompts, or namely natural language text, to the input emerges as an effective and lightweight approach to direct LLMs to generate specific, desired responses . Such an approach is of particular interest when users interact with state-of-the-art LLMs like ChatGPT  and GPT-4 , which can only be accessed through black-box APIs (i.e., the interface of black-box LLMs only accepts discrete texts as input). So, prompt optimization becomes a critical effort in pursuing the optimal performance of black-box LLMs on downstream tasks.

Although human knowledge may subjectively guide prompt designs , this process is commonly time-intensive and its results are not always desirable in practice. To mitigate such human effortsand achieve better performance in optimizing crafted prompts, random sampling , Bayesian optimization [3; 18], and evolutionary algorithms  have been proposed to generate and select well-performing prompts automatically. However, most of these existing strategies prioritize _global optimization_, dedicating substantial portions of the query budget to explore the entire search space for the global optima and consequently making it query-inefficient in practice. Meanwhile, these strategies typically implement their prompt optimization across various input domains (i.e., natural texts [10; 48] or hidden embeddings [3; 18]), resulting in diverse performance outcomes in practice. These results consequently inspire us to re-think the questions about the necessity of finding a global optimum and the essence of the input domain for efficient and effective prompt optimization.

To answer these questions, we provide a thorough empirical study on prompt optimization. Firstly, we visualize the performances of some randomly sampled prompt candidates on various tasks to show that, in contrast to the scarcity of global optima, local optima are commonly prevalent and perform reasonably well, making them more valuable for query-efficient prompt optimization (**Insight I** in our Sec. 3.1). Secondly, we visualize the estimated accuracy distributions for a number of prompt candidates and the corresponding function surfaces using various embeddings as their representation. The results demonstrate that the selection of the input domain, including both the generation and representation of prompt candidates, will influence the identification of high-performing prompts, especially those local optimal ones (**Insight II** in our Sec. 3.2). These insights consequently highlight the importance of local optima and input domain for efficient and effective prompt optimization.

Inspired by these insights, we novelly propose the _Localized Zeroth-Order Prompt Optimization_ (ZOPO) algorithm for a considerably improved prompt optimization as evidenced by Fig. 1. Motivated by Insight II, we first propose a general domain transformation that utilizes LLMs for prompt generation and existing embedding models for transforming these generated prompts into their corresponding hidden representations, which thereby enjoys not only the remarkable generation ability from any type of LLMs (white/black-box) but also the impressive representation ability from existing embedding models for our prompt optimization (Sec. 4.1). Inspired by Insight I, we then leverage a cutting-edge zeroth-order optimization (ZOO) method enhanced by a derived Gaussian process for efficient gradient estimation  to underpin our localized prompt optimization, which goes one step further by incorporating the Neural Tangent Kernel (NTK)  to handle the complex and high-dimensional prompt optimization tasks (Sec. 4.2). Lastly, we present an uncertainty-informed local exploration method designed to improve the gradient estimation in our derived NTK-GP framework, thereby augmenting the practical performance of the ZOPO algorithm (Sec. 4.3).

To summarize, the contributions of our work include:

* To the best of our knowledge, we are the first to conduct a thorough empirical study in prompt optimization to underscore the value of local optima and the essence of input domain for efficient and effective prompt optimization (Sec. 3).
* Drawing on the insights gained from our empirical study, we design the ZOPO algorithm (Sec. 4) which outperforms existing baselines in optimization performance and query efficiency.
* We conduct extensive studies to confirm the efficacy of our algorithmic framework and elucidate the underlying principles or insights of our ZOPO algorithm (Sec. 5).

## 2 Problem Setup

Given an NLP task that is characterized by a data distribution \(\) and a black-box LLM \(f()\), e.g., ChatGPT , discrete prompt optimization aims to generate a piece of human-readable text, namely the prompt \(v\), which will then be applied to the black-box LLM \(f()\) along with a test input \(x\) such that the queried LLM output \(f([v;x])\) is able to correctly predict the ground-truth label \(y\) for each \((x,y)\). This problem is then commonly framed as a black-box maximization problem over the discrete language input domain \(\)[3; 18]:

\[_{v}F(v)_{(x,y)_{V}}\ [ (f([v;x]),y)] \]

where \((f([v;x]),y)\) is applied to measure the alignment between the LLM output \(f([v;x])\) and the groundtruth \(y\), and \(_{V}\) is the validation set sampled from \(\). Note that the performance of the optimal instruction found on \(_{V}\) (i.e., \(_{v}F(v)\)) will be evaluated on a held-out test set \(_{T}\).

## 3 Empirical Study on Prompt Optimization

### Local Optima vs. Global Optimum

In prompt optimization, methods like [3; 18] are generally more effective than the others [10; 48], which is usually contributed to their usage of Bayesian optimization, a popular global optimization strategy, that is able to find the global optimum in low-dimensional problems . However, these methods will perform poorly in certain prompt optimization tasks, e.g., cause_and_effect and informal_to_formal from , indicating that they typically fail to find the global optimum in these tasks given a limited query budget. This is likely because substantial portions of the budget are wasted in these methods to explore the entire search space for the global optimum. However, _is it really necessary to find the global optimum in query-efficient prompt optimization?_

To answer this question, we have employed a 3-dimensional scatter plot to visualize the performance (differentiated by colors) for 300 randomly sampled prompt candidates on various tasks, whose prompt embeddings (i.e., the last token embedding as in ) are reduced by t-distributed stochastic neighbor embedding (t-SNE) (see more details in our Appx. D.1.1). The results are in Fig. 2 which shows that the global optimum (i.e., the points achieving the highest accuracy) is consistently rare for a range of prompt optimization tasks, making it extremely challenging to achieve this global optimum in practice. In contrast, prompt optimization often features a number of local optima (e.g., the points achieving accuracy higher than 80% in taxonomy_animal of Fig. 2). Importantly, these local optima commonly enjoy relatively good performances, suggesting that local optima shall be more worthwhile to obtain in prompt optimization, especially for the scenarios of limited query budgets, as summarized below.

### Essence of Input Domain

Besides, existing works [3; 10; 18] typically implement their prompt optimization across various input domains, leading to a wide range of performances in practice. These results thus inspire us to ask: _How essential is the input domain for finding well-performing prompts, particularly the local optimal ones?_ Thoroughly exploring this question is fundamental for the design of a well-performing prompt optimization algorithm.

To answer this, we first visualize the accuracy distributions of 300 prompt candidates that are randomly generated by Vicuna-13B and ChatGPT for various tasks to study the essence of prompt generation in Fig. 3 (more details in Appx. D.1.2). Fig. 3 reveals that the prompt candidates produced by ChatGPT (a black-box model) generally exhibit better performance than those produced by Vicuna-13B (a white-box model), which has been widely applied in [3; 18] for prompt optimization. Importantly, ChatGPT demonstrates a greater likelihood of generating locally optimal prompts (e.g., the ones of accuracy higher than 0.8 in taxonomy_animal of Fig. 3). These results indicate that the ability togenerate well-performing local optima in prompt optimization usually varies for different models. So, the selection of the prompt generation model is crucial for finding well-performing optima.

We then investigate the function surface (i.e., accuracy landscape) using two different embeddings for prompt candidates in Fig. 4 (more details in Appx. D.1.2) where the embeddings are mapped into a 2-dimensional domain using the t-SNE for better visualization. Interestingly, Fig. 4 unveils that different embeddings will convey a varying number of well-performing local optima in practice. Particularly, the last token embedding is usually able to produce a larger number of well-performing local optima than the SBERT (i.e., a popular sentence embedding transformer ) embedding, making it easier to enjoy a good prompt optimization performance on this domain, as validated in Tab. 8. This therefore implies that the choice of the prompt embedding model is also essential for the finding of well-performing optima. In all, we conclude our aforementioned insights as below.

## 4 The ZOPO Algorithm

Given the insights established in our Sec. 3, we then propose our _Localized Zeroth-Order Prompt Optimization_ (ZOPO) algorithm (Algo. 1) for a better-performing as well as more query-efficient prompt optimization. Specifically, following our Insight II, we first develop a more general transformation for the input domain of prompt optimization (Sec. 4.1), which can enjoy both the remarkable generation ability from any type of LLMs (white/black-box) and the impressive representation ability from many NLP models. Subsequent to this transformation, inspired by our Insight I, we propose to use zeroth-order optimization (ZOO) with a derived NTK Gaussian process inspired from  to find well-performing local optima (Sec. 4.2). Lastly, we introduce an uncertainty-informed local exploration technique to refine the gradient estimation in our derived NTK Gaussian process, aiming to enhance the performance of our ZOPO algorithm in practice (Sec. 4.3).

### A More General Input Domain Transformation

As introduced in our Sec. 3.2, the choice of input domain (including the generation and representation of candidates) significantly influences the ultimate performance in prompt optimization: Black-box LLMs (e.g., ChatGPT) typically enjoy an advanced generation ability and different embedding models (e.g., SBERT) have varying representative capacity for prompt optimization. This naturally inspires us to develop an improved _domain transformation_ that can utilize not only the remarkable generation ability from white/black-box LLMs but also the impressive representation ability from certain NLP models for our prompt optimization. To achieve this, we propose to make use of the prompt \(v\) generated from a LLM \(g()\) and subsequently transform it into a continuous hidden representation \(z^{d}\) by other sentence embedding model \(h()\) for the optimization, i.e., \(v=h^{-1}(z)\), where (1) can then be re-framed as

\[_{z}(z)=_{(x,y)}\ [(f([h^{-1}(z);x]),y)]. \]

Figure 4: The function surfaces using the last token (Vicuna-13B) or SBERT embedding.

Figure 3: The validation accuracy distribution of prompts generated by Vicuna-13B or ChatGPT on various instruction induction tasks, where the vertical dotted line is the mean performance.

```
1:Input: the prompt generation model \(g()\), the prompt embedding model \(h()\), size of prompt candidates \(m\), iteration number \(T\), prompt candidate set \(=\), prompt embedding set \(=\)
2:repeat
3:\(v g([_{}])\)
4:\(z h(v)\)
5:if\(v\)then\(\{v\}\), \(\{z\}\)
6:until\(||=m\)
7:for\(t=1\)to\(T\)do
8:if\(_{A_{t}}(z_{t})=1\)then do uncertainty-informed local exploration in Sec. 4.3
9:\(z_{t+1}=_{}(z_{t}+_{t}_{t}(z_{t}))\)
10: Query \(z_{t+1}\) to yield \((z_{t+1})\)
11:endfor
12:\(z^{*}_{z_{t+1}:T}(z)\)
13:Return\(h^{-1}(z^{*})\)
```

**Algorithm 1** The ZOPO Algorithm

Of note, our input domain transformation and (2) enjoy a number of major advantages compared with previous works: _(a)_ Different from the direct optimization over the discrete and complex language space \(v\) in  where optimization algorithms in the numerical domain can hardly be applied, our transformed input domain leads to a dense numerical space of lower dimension and therefore allows the usage of query-efficient optimization algorithms for (2) (e.g., our Algo. 1). _(b)_ Different from the potential many-to-one mapping in the previous works [3; 18], i.e., the same discrete prompt \(v\) may be generated by various continuous soft prompts \(s\), we develop a one-to-one mapping where one prompt generally has a unique hidden representation \(z\), which thus can help eliminate the redundant queries during optimization and ultimately lead to more query-efficient prompt optimization. _(c)_ Our domain transformation with an independent generation and representation process is capable of enjoying the remarkable generation ability from any type of LLMs (white/black-box) and the impressive representation ability from many embedding models whereas previous works are highly restricted to the LLMs, thus leading to a wider application.

Practical Implementations.Before the start of the optimization on (2), we usually generate numerous prompt _candidates_\(=\{v\}\) and their corresponding representations \(=\{z\}\) (line 2-6 of Algo. 1), where \(\) can be produced by an embedding model \(h()\). We store \((z,v)\) in key-value pairs for constructing the one-to-one inverse mapping \(h^{-1}()\). Two practical methods are considered here for prompt generation: _(a)_ Feeding randomly sampled soft prompts \(s^{d}\) and a few demonstrations \(_{}\) into a white-box LLM \(g()\). _(b)_ Sampling the output distribution of a black-box LLM \(g()\) given a generation template filled with \(_{}\). Specifically, if we consider the generation method in _(a)_, \(z\) can be chosen as the last token embedding from \(g()\) or the soft prompt \(s\) when generating \(v\). Here \(h()\) then represents a mapping function from \(v\) to \(z\).

### Local Optimization with Derived NTK-GP

As local optima are more prevalent than global optimum and can exhibit compelling performance for prompt optimization tasks (Sec. 3.1), we propose to apply zeroth-order optimization (ZOO), particularly gradient descent using estimated gradients, for a well-performing local prompt optimization on our transformed input domain \(\) in Sec. 4.1. Unfortunately, existing ZOO algorithms are typically query-inefficient as many additional queries are required for gradient estimation in every gradient descent update [9; 23]. In light of this, we resort to the most recent ZoRD algorithm  where a localized surrogate model will be applied for query-efficient gradient estimations.

According to , given a well-specified kernel function \(k(,)\) such that the function \(\) is sampled from a Gaussian process \((0,k(,))\) or alternatively \(_{G(0,k(,))}_{z}|(z)-G(z)|=0\) and the observed value \(r\) of function \(\) follows the Gaussian noise \((0,^{2})\), then conditioned on the history of function queries \(_{t}\{(z_{},r_{})\}_{=1}^{t}\) of size \(t\), \(\) follows a derived Gaussian Process \(((),(,))\), i.e.,

\[(_{t}(),_{t}^{2}(, )), \]

in which the mean function \(_{t}()\) and the covariance function \(_{t}^{2}(,)\) are defined as

\[_{t}(z) _{t}(z)^{}(_{t}+^{2} )^{-1}_{t}\;, \] \[_{t}^{2}(z,z^{})  k^{}(z,z^{})-_{t}(z)^{} (_{t}+^{2})^{-1}_{t}(z^{})\;.\]

Here, \(_{t}(z)^{}[_{z}k(z,z_{})]_{t=1}^{t}\) is a \(d t\)-dimensional matrix, \(_{t}[k(z_{},k(z_{^{}})]_{, ^{}=1}^{t}\) is a \(t t\)-dimensional matrix, \(_{t}^{}[r_{}]_{=1}^{t}\) is a \(t\)-dimensional column vector, and \(k^{}(z,z^{})_{z}_{z^{}}k(z, z^{})\) is a \(d d\)-dimensional matrix. As a result, \(_{t}(z)\) can be applied to estimate the gradient of the black-box function \(\) at input \(z\).

Of note, the underlying black-box function \(\) here is highly related to deep neural networks (DNN), more specifically transformers. It naturally inspires us to apply the Neural Tangent Kernel (NTK)  theory for a better approach to the aforementioned assumption of a well-specified kernel function \(k(,)\). This is because it has been widely proven that NTK is capable of well characterizing the predictions of neural networks  and therefore should be a better-specified kernel in the setting of prompt optimization than the simple kernel (i.e., Matern kernel) applied in ZoRD . Specifically, given a neural network \((,z)\) parameterized by \(^{p}\), we employ the following empirical NTK as the kernel in (3) and (4):

\[k(z,z^{})=_{}(,z)^{}_{}(,z)_{=_{0}} \]

where \(_{0}\) is the initialized parameter of neural network \(\). By incorporating (5) into (4), we realize the derived NTK-GP for the gradient estimation in our prompt optimization.

Based on this derived NTK-GP, we finally apply standard first-order optimization (e.g., stochastic gradient descent) with projected gradients for our local prompt optimization. Specifically, in every iteration \(t\) of our Algo. 1, the next promising prompt candidate will be selected via:

\[v_{t+1}=h^{-1}(_{}(z_{t}+_{t}_{t}(z_{t}))) \]

where \(_{}(z)_{z^{}}\|z -z^{}\|\) is the projection function that projects the updated \(z^{d}\) into domain \(\) and \(_{t}\) is learning rate.

Practical Implementations.Following the localized modeling principle, only the neighbors of \(z\) in the query history \(_{t}\) are used to calculate the gradient \(_{t}(z)\). As we do not know the exact DNN for the underlying black-box function \(\), we propose to approximate it using a small DNN, which can work well thanks to the theoretically guaranteed universal approximation ability of DNNs . Our experiments in Sec. 5.3 will further validate the effectiveness of this implementation.

### Uncertainty-Informed Local Exploration

Though the derived NTK-GP allows us to estimate the gradient at _any_\(z\) according to , we introduce the following Prop. 1 to demonstrate that the error in gradient estimation at a specific input \(z\) implies considerable variability, which is strongly correlated with the number of historical queries that are _effectively_ relevant for the gradient estimation at the specific input \(z\). This insight, in turn, motivates the creation of our uncertainty-informed local exploration approach, as opposed to the adoption of the virtual update mechanism described in  for our prompt optimization strategy.

**Proposition 1**.: _Assume \(k(z,z^{})\) and \(\|k^{}(z,z)\|\) for any \(z,z^{}\). Let \((0,1)\) and \(N_{z,}\{z^{}\{z_{}\}_{=1}^{t}\|_{ z}k(z^{},z)\|^{2}\}\) for given input \(z\), the following holds with a probability of at least \(1-\),_

\[\|_{t}(z)- F(z)\|^{2}\|_{t}^{2}(z)\| -/|N_{z,}|}\]

_where \(=d+2(+1)(1/)\) and \(_{t}^{2}(z)_{t}^{2}(z,z)\)._

[MISSING_PAGE_FAIL:7]

### Instruction Induction

Instruction induction tasks are commonly used to investigate the prompt optimization performance by assessing LLM's zero-shot in-context learning ability in previous works [3; 18; 48]. Although our ZOPO is a general prompt optimization method given any prompt generation strategy, here we follow the same setting of prompt generation from INSTINCT and InstructZero, only for **fair comparison**. We also adopt the last token embedding from Vicuna-13B as the prompt embedding (same as INSTINCT). Here Vicuna-13B is used to generate task-specific prompts by feeding random soft prompts. More experimental details are deferred to Appx. C.3.

**Superior performance of ZOPO.** For better distinguishability, we follow the experimental setting from Lin et al.  to display the results on 20 challenging tasks reported in Tab. 1, where ZOPO significantly outperforms all baseline methods. Particularly, our ZOPO performs the best in 14 out of the 20 tasks presented, while achieving the best performance profile across different \(\) (see Fig. 1) compared with all baseline methods. For more results on all 30 tasks, refer to Tab. 3 in Appx. D.2, where the ZOPO consistently outperforms existing methods.

**Connecting ChatGPT with ZOPO.** With our proposed domain transformation, we empirically demonstrate that ZOPO is capable of performing numerical optimization on ChatGPT-generated prompts. Specifically, we use the same generation method as in APE  to generate task-specific prompts (i.e., \(\)) from ChatGPT, and use a popular embedding model SBERT to provide the corresponding sentence embeddings (i.e., \(\)) for \(\). Then we apply ZOPO to perform optimization over the given \(\) and \(\), which we name ZOPO\({}_{}\). The result of ZOPO\({}_{}\) compared against other baselines is shown in Tab. 1, with the corresponding performance profile shown in Fig. 9 in App. D.2. Fig. 9 demonstrates that ZOPO\({}_{}\) significantly outperforms other baselines, achieving the best performance in 10 out of the 20 tasks as shown in Tab. 1. Specifically, ZOPO\({}_{}\) achieves significantly higher accuracy on some challenging tasks such as second_word_letter and sentence_similarity, which we attribute to the high-quality of prompt candidates generated by ChatGPT. This is also consistent with our discussion on the input domain in Sec. 3.2. Here we could not draw a direct comparison between ZOPO and ZOPO\({}_{}\), as the Vicuna last token embedding is specifically associated with the prompt generation process in ZOPO and cannot be applied to ZOPO\({}_{}\). However, using either ZOPO or ZOPO\({}_{}\) is sufficient to outperform baseline methods, which also provides the flexibility of prompt optimization in practice. Future research may consider employing better embeddings to further improve the performance of ZOPO\({}_{}\).

**ZOPO has better query efficiency.** To justify that our local optimization method is more _query-efficient_, we compare ZOPO against baselines at different query budget scales. The result shown in Fig. 5 illustrates that ZOPO generally achieves better performance with the same number of queries compared with other baseline methods and yields superior performance upon convergence. We notice that ZOPO achieves lower validation accuracy yet higher test accuracy on the taxonomy_animal task than INSTINCT, which suggests ZOPO likely has better generalization ability. More results on other tasks in Fig. 10 in Appx. D.2 also indicate that ZOPO has consistent advantages.

### Improving Chain-of-Thought Prompt

Reasoning prompts, e.g., "Let's think step by step" (denoted as hand-craft), have been shown effective in improving LLMs' zero-shot multi-step reasoning performance [13; 15]. We show that ZOPO can find a better chain-of-thought prompt across different arithmetic reasoning tasks, as evidenced in Tab. 2. Particularly, ZOPO produces a better prompt "Let's find the solution by using the given information." on GSM8K  compared to other baselines, improving the performance from \(71.80\) (hand-craft) to \(75.36\). Refer to Appx. C.4 for more experimental details.

Figure 5: Comparison of the query efficiency between ZOPO and baselines. The first and second rows show the test and validation accuracies.

### Ablation Study

**Verifying the essence of input domain.** To fairly validate the importance of input domain on prompt generation, we compare the optimization performances with different prompts generated by Vicuna-13B and ChatGPT respectively, using the same embedding model SBERT (i.e., \(h()\)). The result is shown in Table. 7 in Appx. D.6, with the performance profile in Fig. 11 suggesting that applying ZOPO on ChatGPT-generated prompts is better. We ascribe its better performance to ChatGPT's remarkable prompt generation ability. This confirms the importance of the input domain on prompt generation in our Insight II.

Besides, different embeddings (i.e., \(\)) of the same prompt candidates can potentially affect the function landscape as shown in Fig. 4. Thus, we need to study the performance of ZOPO using different embedding representations given the same set of prompts. We consider four different embeddings here: the last token embedding from Vicuna-13B, the OpenAI embedding provided through an API , the SBERT embedding, and a randomly projected embedding baseline. We observe from Tab. 8 in Appx. D.6 that, although last token embedding is generally better, there are certain tasks that OpenAI and SBERT embeddings perform equally well or better. Besides, random embedding shows a distinct lesser performance. This again highlights the importance of using more structured embeddings for prompt optimization and indicates the optimal choice of embedding can be _task-dependent_. We discuss how we might find the best embedding model and further show the generality of ZOPO by experimenting with more embedding models in Appx. D.6.

**Study of NTK-GP and uncertainty-informed local exploration.** We conducted additional experiments to validate the NTK-GP (Sec. 4.2) and uncertainty-informed local exploration (Sec. 4.3) components of ZOPO. We evaluated the impact of these components by testing two variants of the ZOPO algorithm: (a) replacing the NTK component with Matern kernel (as in ZoRD), and (b) removing the uncertainty-informed local exploration. Comparisons of these variants against the original ZOPO on instruction induction tasks (see Tab. 11 in Appx. D.7) highlight the significant contributions of these components to ZOPO's overall effectiveness.

**Additional results.** The results on the GLUE benchmark in Appx. D.3 consistently validate the superior performance of ZOPO. We also demonstrate that ZOPO can handle prompt optimization in the _few-shot_ setting in Appx. D.4. We conduct further experiments to show ZOPO generalize to _different combinations_ of prompt generation models and black-box LLMs in Appx. D.5. We also perform an ablation study to examine the impact of a larger size of the generated prompt candidates (i.e., \(||\)) on ZOPO and ZOPO\({}_{}\) in Appx. D.8, which suggests a relatively small set of strong

   Method & Task & Best prompt & Score \\  hand-craft & AQUA-RAT & Let’s think step by step. & 52.36 \\ InstructZero & AQUA-RAT & Let’s break down the problem. & 54.33 \\ INSTINCT & AQUA-RAT & I have a new solution. & **54.72** \\ EvoPrompt & AQUA-RAT & Let’s utilize the substitution method to find a solution, then try it out together. & 52.76 \\ ZOPO & AQUA-RAT & Let’s find the solution by breaking down the problem. & **54.72** \\  hand-craft & SVAMP & Let’s think step by step. & 76.25 \\ InstructZero & SVAMP & Let’s use the equation. & 79.50 \\ INSTINCT & SVAMP & Let’s use our brains. & **81.00** \\ EvoPrompt & SVAMP & Let’s break down the issue at hand using promptal methods to gain a thorough analysis. & 79.50 \\ ZOPO & SVAMP & Let’s use logic to solve the problem. & **81.00** \\  hand-craft & GSM8K & Let’s think step by step. & 71.80 \\ InstructZero & GSM8K & Let’s use the prompt to solve the problem. & 74.30 \\ INSTINCT & GSM8K & Let’s think about it. & 74.53 \\ EvoPrompt & GSM8K & Let’s attempt to analyze the situation and give it a shot. & 74.53 \\ ZOPO & GSM8K & Let’s find the solution by using the given information. & **75.36** \\   

Table 2: The performance of the best zero-shot CoT prompt found by different methods on three reasoning tasks.

prompt candidates (e.g., \(||=500\)) is sufficient (compared with size 1000 or 2000). Additionally, we provide more demonstrations of our empirical findings in Sec. 3 on other tasks in Appx. D.1.

## 6 Related Work

**Soft Prompt Tuning.** To control LLMs to perform specific downstream tasks (e.g., reasoning), soft prompt tuning  is usually used as a lightweight method to fine-tune the LLMs by only optimizing a continuous vector prepended to the input tokens using gradient descent, in contrast to fine-tuning millions to billions of model parameters . However, when the gradient information of the model is inaccessible, gradient-free prompt tuning methods  are developed to alleviate human efforts in prompt design. Still, those efforts to optimize soft prompts have conventionally relied on the white-box access to the embedding layers of LLMs, making it inapplicable to state-of-the-art LLMs like ChatGPT  and GPT-4  that can only be accessed through black-box APIs (i.e., only accept natural language as input).

**Discrete Prompt Optimization.** We refer to the process of optimizing discrete prompts as "prompt optimization", which is also a more practical setting as black-box LLMs only accept discrete inputs. Reinforcement learning-based methods  focus on discrete token optimization but rely on the output distribution of the LLMs, which is not accessible in black-box API LLMs (e.g., ChatGPT). Zhou et al.  instead makes use of LLMs to produce promising candidate prompts through resampling without applying specific optimizations. Some recent works, EvoPrompt  and PB , further extend this model-free approach to evolutionary algorithms and design meta-prompts that explicitly ask the LLM to perform iterative mutation and crossovers of existing prompt candidates. Besides, Pryzant et al.  ask the LLM to perform implicit gradient descent on existing prompts; OPRO  uses the LLM as an implicit optimizer to perform prompt optimization, where its designed meta-prompt takes in the optimization trajectory and instructs the LLM to output a promising prompt candidate. However, these methods rely on powerful LLMs (e.g., GPT-3.5) and typically require a large number of iterations and queries to perform well. In this regard, InstrucLZero  leverages the induction ability from other white-box LLM \(g()\) for generating the task-specific prompt \(v\) that is conditioned on a continuous soft prompt \(s^{d}\). After that, the optimization on \(v\) can be transformed into an optimization on the soft prompt \(s\), where BO algorithms are employed for a global black-box optimization. INSTINCT  further employs neural bandit algorithms and the last token embeddings from the white-box LLM to further improve the prompt optimization performance. However, these works prioritize a global optimization approach that emphasizes the exploration of the entire space. With an empirical understanding of the underlying target function (i.e., the black-box API LLMs), we propose a localized ZOO method that is in contrast to the global optimization approaches.

## 7 Conclusion

In this work, we first provide a thorough empirical study to understand the characteristics of the target function, and then propose our ZOPO algorithm for prompt optimization. ZOPO embraces a ZOO approach in pursuit of finding local optima efficiently. Extensive experiments on 30 instruction induction tasks, 3 reasoning tasks, and the GLUE benchmark demonstrate the efficacy of ZOPO, and ablation studies also validate the design principles and the generality of ZOPO. Besides, we propose a domain transformation that connects powerful LLMs with remarkable embedding models, which provides the flexibility of choices of input domains in prompt optimization. A limitation of this paper is the lack of principle to select LLMs and embedding models in our input domain transformation for better-performing prompt optimization, which we aim to explore in future work.