# Conservative Offline Policy Adaptation in Multi-Agent Games

Chengjie Wu\({}^{1}\), Pingzhong Tang\({}^{12}\), Jun Yang\({}^{3}\), Yujing Hu\({}^{4}\),

Tangjie Lv\({}^{4}\), Changjie Fan\({}^{4}\), Chongjie Zhang\({}^{5}\)

\({}^{1}\)Institute for Interdisciplinary Information Sciences, Tsinghua University

\({}^{2}\)Turingsense

\({}^{3}\)Department of Automation, Tsinghua University

\({}^{4}\)Fuxi AI Lab, NetEase

\({}^{5}\)Department of Computer Science & Engineering, Washington University in St. Louis

wucj19@mails.tsinghua.edu.cn

{kenshin,yangjun603}@tsinghua.edu.cn

{huyujing,hzlvtangjie,fandchangjie}@corp.netease.com

chongjie@wustl.edu

###### Abstract

Prior research on policy adaptation in multi-agent games has often relied on online interaction with the target agent in training, which can be expensive and impractical in real-world scenarios. Inspired by recent progress in offline reinforcement learning, this paper studies offline policy adaptation, which aims to utilize the target agent's behavior data to exploit its weakness or enable effective cooperation. We investigate its distinct challenges of distributional shift and risk-free deviation, and propose a novel learning objective, conservative offline adaptation, that optimizes the worst-case performance against any dataset consistent proxy models. We propose an efficient algorithm called Constrained Self-Play (CSP) that incorporates dataset information into regularized policy learning. We prove that CSP learns a near-optimal risk-free offline adaptation policy upon convergence. Empirical results demonstrate that CSP outperforms non-conservative baselines in various environments, including Maze, predator-prey, MuJoCo, and Google Football.

## 1 Introduction

Reinforcement learning (RL) has shown promise in learning collaborative or adversarial policies in multi-agent games, including cooperative multi-agent reinforcement learning , and learning in adversarial environments such as Texas hold'em and MOBA games . Research also recognizes the importance of adapting policy to different players in multi-agent games . For instance, in zero-sum games, although an approximate Nash equilibrium strategy optimizes worst-case performance, it can be overly conservative when competing with opponents of limited rationality and loses opportunities to exploit . This problem is known as opponent exploitation . In cooperative games, the agent may be required to cooperate with other competent players that have diverse behaviors , instead of just a specific group of teammates. The problem is investigated in recent literature such as ad-hoc cooperation  and zero-shot coordination . In this paper, **policy adaptation** refers to the general ability to learn to collaborate with or exploit other participants (called target agents) in multi-agent games.

This paper focuses on a specific problem of **offline policy adaptation**. In this setting, interaction with the target agent is not available in training. Instead, we leverage the target agent's behavior dataset and maintain access to the game environment during training. The problem definition is illustratedin Figure 1 (left). This setting is suitable for many practical scenarios where offline information about the target agent is provided. It lies between the fully transparent case, where the target agent's policy is available, and the fully invisible zero-shot adaptation case. For instance, in competitions such as Go, soccer, and MOBA games, playing with the target opponent is not an option until the actual game commences. However, professional human players have the intelligence to analyze their opponents' and teammates' strategic characteristics by studying past records, enhance their skills, and make adaptations accordingly before the competition. Moreover, training AIs with humans in applications that involve human-AI interaction is exorbitant. Additionally, it can be unsafe and poses ethical risks when humans interact with an inadequately trained AI in certain circumstances.

Although offline policy adaptation models a wide range of applications, it has not been adequately investigated both theoretically and empirically in previous literature. For instance, some opponent exploitation research assumes direct interaction in training for explicit opponent model estimation  or policy gradient calculation . However, this assumption is strong because it allows queries of the target agent's policy for any input. Other works study exploiting totally unknown opponents  or collaborating with completely unseen teammates  in a zero-shot manner. These methods do not take advantage of possible prior information about the target agent, and their performances heavily depend on the construction of a diverse and representative policy population constructed for training, which itself is a challenging problem .

In this paper, we identify and formalize the distinct challenges in offline policy adaptation, and show that previous opponent exploitation methods, which disregard these challenges, are inadequate for effectively addressing the problem. Firstly, offline policy adaptation inherits the _distributional shift_ challenge from traditional offline RL. A straightforward approach to address offline policy adaptation is by initially creating an estimation of the target agent (e.g., utilizing behavior cloning) from data and subsequently learning an adaptation policy based on the proxy model. We name this method _BC-First_. However, since the dataset usually will not expose all aspects of the real policy of the target agent due to limited dataset size, the proxy model can differ arbitrarily from the real target on out-of-distribution (OOD) states. The discrepancy between the proxy in training and the real target in testing causes the distributional shift problem for the adaptation policy. As shown in Figure 1, _BC-First_ overfits the dataset, which leads to erroneous generalization on OOD states and results in significant performance degradation in evaluation. Our algorithm outperforms the baseline and improves in both training and testing as the number of training step increases. Secondly, since the adaptation policy has access to the environment in training, unlike standard offline RL, we show that "conservatism"  for addressing distributional shift may not be sufficient in this setting. It is possible to benefit from deviation in multi-agent games if the worst-case rewards obtainable outside the dataset exceed those inside the dataset. We coin this challenge as _risk-free deviation_.

To address these challenges, we propose a novel learning objective, **conservative offline adaptation**, which optimizes the worst-case performance when maintaining a conservative stance towards the target agent on the policy level. This objective provides a unified approach to addressing the challenges of distributional shift and risk-free deviation. To optimize this objective, we introduce a simple yet efficient algorithm, **Constrained Self-Play (CSP)**, which simultaneously trains an adaptation policy and a proxy model subject to regularizations. We prove that CSP produces a near-optimal risk-free adaptation policy upon convergence, given that the target agent's policy within the support of the dataset can be well approximated. Empirically, we evaluate the effectiveness of our algorithm in four environments: a didactic maze environment, predator-prey in MPE , a competitive two-agent

Figure 1: **Left.** Problem illustration. **Mid and right.** Winning rates in Google Football 3vs1 (attacker). The x-axis is the training steps. We run evaluations against **(mid)** proxy model in training and **(right)** real target agent. Shaded areas represent standard error. The non-conservative method quickly overfits the proxy model. Our method CSP outperforms baseline in testing. Please refer to Appendix G.7 for more explanations.

MuJoCo environment [2; 10] requiring continuous control, and Google Football . Our results show that non-conservative methods confront difficulties regarding offline policy adaptation. The experiment results demonstrate that our algorithm significantly outperforms baselines.

## 2 Related Work

**RL in multi-agent games.** Multi-agent reinforcement learning (MARL) under the centralized training decentralized execution (CTDE) paradigm have shown promising performances in learning coordination behavior for cooperative multi-agent tasks such as SMAC  and Google Football . Representative works consist of policy gradient algorithms [6; 48], and value decomposition methods [34; 40]. There also have been extensive research in competitive environments such as Texas hold'em and MOBA games. Some works design theoretically sounded RL algorithms which are guaranteed to converge to approximate NE . Some works propose RL methods based on self-play [45; 46; 50] to train max-min policies which show strong empirical performances.

**Offline RL.** Offline reinforcement learning  learns purely from batched data. Representative works take a conservative view of out-of-distribution state-action pairs to mitigate the distributional shift problem. BCQ  uses VAE to propose candidate actions that are within the support of the dataset. IQL  uses expectile regression to learn Q function to avoid querying out-of-distribution actions. CQL  puts regularization on the learning of Q function to penalize out-of-distribution actions.

**Opponent modeling.** Opponent modeling, sometimes also called opponent exploitation, is a broad and extensively studied area [1; 28]. The offline adaptation problem studied in this paper is a largely unstudied yet significant and challenging problem within the broader scope of opponent modeling. Some previous works assume access to the target agent's policy in training, and employ deep reinforcement learning to exploit the opponent in continuous control tasks using policy gradients estimated from direct interaction [2; 10; 12]. Some works, including RNR  and SES  study safe exploitation given an estimated opponent model. They keep the exploitation policy close to NE in order to minimize its own exploitability. Some papers, like GSCU , study zero-shot generalization ability to exploit totally unknown opponents. Additionally, this paper is orthogonal to some other opponent modeling subareas, including designing efficient models for predicting opponent behavior [13; 33], and addressing non-stationary opponents . To our best knowledge, there lacks a thorough investigation into the offline adaptation problem.

Please refer to Appendix B for more discussion.

## 3 Preliminary

For simplicity, we consider a two-player fully observable game \(G=(N,S,A,P_{M},r,p_{0},)\)[37; 38], where \(N\) is a set of two players, \(S\) is a set of states, \(p_{0}\) is initial state distribution, and \(\) is the discount factor. We use \(A_{i}\) to denote the available actions for player \(i\), and \(A=(A_{1},A_{2})\). The transition probability is denoted by \(P_{M}:S A S\). We use \(r_{i}:S A\) to denote the reward function for player \(i\), and \(r=(r_{1},r_{2})\). In fully cooperative scenarios, \(r_{1}(s,)=r_{2}(s,)=r(s,)\), which is termed multi-agent MDP (MMDP) . In zero-sum games, \(r_{1}(s,)+r_{2}(s,)=0\). Player \(i\)'s policy \(_{i}(s,)\) is a distribution over action space \(A_{i}\) conditioned on state \(s S\). The value function \(V_{i}^{(_{1},_{2})}(s)\) is the expected discounted accumulated reward that player \(i\) will achieve under joint policy profile \((_{1},_{2})\) at state \(s\). Similarly, the action value function \(Q_{i}^{(_{1},_{2})}(s,)\) calculates the expected return when players play the joint action \(=(a_{1},a_{2})\). Let \(d_{_{1},_{2}}(s)\) be the discounted visitation frequency of state \(s\) under policy profile \((_{1},_{2})\): \(d_{_{1},_{2}}(s)=_{t=0}^{}^{t}P(s_{t}=s)\). We use \(J_{i}^{(_{1},_{2})}\) to denote the return for player \(i\) under policy profile \((_{1},_{2})\).

Without loss of generality, assume that we are aimed at learning an adaptation policy for player 1 (P1), while player 2 (P2), called the target agent, is playing a unknown fixed strategy \(_{B}\). In offline policy adaptation, we do not have access to \(_{B}\). Instead, a dataset \(D\) of \(_{B}\)'s behavior data is given. Formally, \(D=F_{p}(\{_{i}\})\), where \(F_{p}\) denotes extra data processing1, and \(_{i}=\{s_{0},a_{0},s_{1},\}\) records states and corresponding actions of an episode collected by \(_{B}\) playing with rollout policies in \(_{c}\). We do not make assumptions on the data collecting process, therefore, \(_{c}\) can contain any polices ranging from a weak random policy to a strong NE policy. Analogous to offline RL and robust RL, the dataset quality varies given different rollout policies, dataset sizes, and processing methods, which results in different problem difficulties. For instance, if the dataset reveals adequate information about the target's behavior under diverse situations, BC-First can be enough. The challenges of offline policy adaptation becomes prominent if the dataset only contains limited knowledge of \(_{B}\).

The purpose of offline adaptation is to learn to adapt to unknown \(_{B}\) given a fixed batch of data \(D\). Notably, different from offline RL, the access to the environment itself is still granted in training. If not, such setting can be reduced to standard imitation learning or offline reinforcement learning [44; 29], because the target's behavior can be absorbed into environment dynamics. With the advantage of having access to the environment, this setting encourages conservative exploitation whenever risk-free deviation is possible.

## 4 Conservative Offline Adaptation

To address the problem of offline adaptation, we first examine the extrapolation error in policy evaluation between joint policies \((,_{B})\) and \((,)\) in section 4.1. Here, \(\) denotes the learning policy, \(_{B}\) represents the true target policy, and \(\) denotes any proxy model that estimates \(_{B}\). This extrapolation error poses a risk to non-conservative policy adaptation algorithms, rendering them unsafe. However, traditional offline RL methods that aim to minimize the impact of extrapolation error tend to be excessively conservative, which is suboptimal in this context. To allow for beneficial deviations from the dataset, we introduce a novel objective of conservative offline adaptation in section 4.2. This approach maximizes the worst-case performance against any dataset-consistent proxy (defined below). Finally, we propose a practical Constrained Self-Play (CSP) algorithm in section 4.3 and formally prove that it achieves near-optimal conservative offline adaptation. Please note that all proofs pertaining to this section can be found in Appendix A.

### Extrapolation Error in Offline Adaptation

We first analyze the extrapolation error in policy evaluation between the joint policies \((,_{B})\) and \((,)\), which results in the overfitting problem shown in Figure 1. For theoretical analysis, we first introduce the dataset consistent policy in Definition 4.1. We will show that this assumption can be relaxed in algorithm design in a later section. Inspired by offline RL, in Proposition 4.2, we find that the extrapolation error in offline policy adaptation can also be decomposed in a Bellman-like recursion [8; 44]. We focus solely on the value function of our learning agent (Player 1), and omit the subscript in the rewards and value functions for simplicity.

**Definition 4.1**.: _Dataset consistent policy. A policy \(\) is said to be consistent with \(_{B}\) on dataset \(D\) iff. \((s,a)=_{B}(s,a)\), \((s,a) D\). We denote the set of all dataset consistent policies as \(_{D}\)._

**Proposition 4.2**.: _The performance gap of evaluating policy profile \((,)\) and \((,_{B})\) at state \(s\) is \((s)=V^{,_{B}}(s)-V^{,}(s)\), which can be decomposed in a Bellman-like recursion:_

\[(s)=&_{a_{1},a_{2},s^{ }}(a_{1}|s)P_{M}(s^{}|s,)(_{B}(a_{2}|s)-(a_{2} |s))(r(s,,s^{})+ V^{,}(s^{})) \\ &+_{a_{1},a_{2},s^{}}(a_{1}|s)_{B}(a_{2}|s)P_{M}(s ^{}|s,)(s^{}) \]

**Theorem 4.3**.: _We use \(=_{s_{0}}p_{0}(s_{0})(s_{0})\) to denote the overall performance gap between policy profile \((,)\) and \((,_{B})\). In any 2-player fully observable game, for all reward functions, \(=0\) if and only if \(_{B}(a|s)=(a|s), s,s.t.\)\(d_{,_{B}}(s)>0\)._

Theorem 4.3 proves that the extrapolation error vanishes if and only if the estimation \(\) perfectly matches \(_{B}\) on any state which can be visited by \((,_{B})\). If extrapolation error exists, improvement of \(\) against proxy \(\) in training will not guarantee monotonic improvement in testing when \(\) is evaluated with \(_{B}\) (as shown in Figure 1). Because we make no assumptions on \(_{B}\) outside \(D\), we cannot bound \(\|(a|s)-_{B}(a|s)\|\) for \(s D\). Therefore, it requires that \(d_{,_{B}}(s)=0, s D\).

**Corollary 4.4**.: _Given a dataset consistent policy \(_{D}\), which is consistent with \(_{B}\) on dataset \(D\), the extrapolation error vanishes if \( s D\), \(d_{,_{B}}(s)=0\)._Corollary 4.4 advocates excessive conservatism, similar to standard offline RL, to exclude any state-action pairs outside the dataset. It requires to constrain both \(\) and \(\) such that the trajectories of \((,)\) stay within \(D\). However, this approach comes at the cost of relinquishing opportunities to exploit environmental access and explore the potential benefit of risk-free deviation from the dataset.

### Conservative Offline Adaptation

In order to avoid being overly conservative and allow beneficial deviation from the dataset, we propose a novel learning objective called conservative offline adaptation in Definition 4.5.

**Definition 4.5**.: _Conservative offline adaptation (COA). Given an unknown policy \(_{B}\), and a dataset \(D\) of its behavior data, conservative offline adaptation optimizes the worst-case performance against any possible dataset-consistent policy:_

\[_{}_{}J(,),\ s.t.\ _{D}. \]

The adaptation policy \(^{*}\) is an optimal risk-free adaptation policy if it is a solution to Objective (2).

To show that COA enables risk-free deviation, we decompose the return into two parts in Definition 4.6: (1) the return that is solely realized within the dataset, and (2) the return that arises from deviation.

**Definition 4.6**.: _Within-dataset return & off-dataset return. The expected return of a joint policy profile \((_{1},_{2})\) can be decomposed into the sum of within-dataset return \(J_{D}(_{1},_{2})\) and off-dataset return \(J_{F}(_{1},_{2})\):_

\[ J(_{1},_{2})&=_{ (_{1},_{2})}_{t}^{t}r(s_{t},_{t},s_{t+1})\\ &=_{(_{1},_{2})}_{t t_{D}^{}} ^{t}r(s_{t},_{t},s_{t+1})+_{(_{1},_{2})} _{t>t_{D}^{}}^{t}r(s_{t},_{t},s_{t+1})\\ &=J_{D}(_{1},_{2})+J_{F}(_{1},_{2}), \]

_where \(t_{D}^{}=_{t^{}}\{ t t^{},s_{t} D\}\) is a random variable indicates the last time step in a trajectory \(\) such that the state is still contained in dataset \(D\)._

For any policy \(\), and any dataset consistent policy \(_{D}\), \(J_{D}(,)=J_{D}(,_{B})\) because \(\) and \(_{B}\) behave exactly the same until the first time an OOD state is reached according to the definition. Thus, the objective 2 is equivalent to \(_{}J_{D}(,_{B})+_{}J_{F}(,),\ s.t.\ _{D}\). The adaptation policy \(\) learns to exploit or cooperate with the target agent's policy exposed by dataset \(D\). Meanwhile, the max-min optimization of \(_{}_{}J_{F}(,)\) encourages \(\) to explore opportunities outside the dataset while maintaining a conservative stance towards the target agent at the policy level. It provides a unified approach to automatically decide whether to deviate depending on the magnitude of within-dataset return and worst-case off-dataset return. In the extreme case, if \(_{}J_{F}(,)\) is much larger than \(J_{D}(,_{B})\), \(\) can ignore the dataset completely and try to obtain rewards outside the dataset. Additionally, suppose that \((^{*},^{*})\) is the solution to COA, then the extrapolation error \(=J(^{*},_{B})-J(^{*},^{*})=J_{F}(^{*},_{B})-J_{F} (^{*},^{*}) 0\), which suggests that the testing performance will not be worse than training performance.

### Constrained Self-Play

Directly optimizing the objective of COA is hard because in deep reinforcement learning, satisfying the dataset consistency assumption regarding the proxy model \(\) can be difficult, particularly for stochastic policies with limited data. We relax this assumption and propose a simple yet efficient algorithm, Constrained Self-Play (CSP). We prove that CSP can achieve near-optimal risk-free policy adaptation. We start by showing in Theorem 4.7 that the gap in return is bounded by the KL divergence of two policies of the target agent.

**Theorem 4.7**.: _In a two-player game, suppose that \(\) is player 1's policy, \(\) and \(\) are player 2's policies. Assume that \(_{s}D_{}((|s)|(|s))\). Let \(R_{M}\) be the maximum magnitude of return obtainable for player 1 in this game, and let \(\) be the discount factor. We use \(J(,)\) and \(J(,)\) to denote the return for player 1 when playing policy profiles \((,)\) and \((,)\) respectively. Then,_

\[J(,)-J(,)-R_{M}(1+}). \]With this result, we propose to optimize objective 5, which substitutes the hard dataset consistent constraint in objective 2 with KL-divergence.

\[_{}_{}J(,)\;s.t.\;_{s D}D_{}(_{B}(|s )\|(|s)) \]

We propose the Constrained Self-Play (CSP) algorithm, which optimizes objective 5 through learning both an adaptation policy and a proxy model with self-play and meanwhile minimizing the KL divergence between \(\) and \(_{B}\) on the dataset with regularization. The algorithm is shown in Algorithm 1 in Appendix C. CSP utilizes soft BC regularization to minimize the KL-divergence: \(_{}_{}\{J(,)+C_{}_{(s,a)  D}[-(a|s)]\}\). It is because BC is equivalent to KL-divergence minimization [18; 9]. By increasing the coefficient \(C_{}\), CSP prioritizes KL-divergence minimization. In the max-min optimization, the proxy model is trained adversarially against our adaptation policy by setting the proxy's reward function to be the negative of our agent's reward. We use MAPPO  as the basic learning algorithm for players on both sides.

We prove in Theorem 4.8 that CSP approximates the optimal risk-free offline adaptation policy by some constant upon convergence. The gap depends on the maximum KL-divergence between the learned proxy \(\) and the target \(_{B}\) on \(s D\). The gap decreases as \(\) becomes a more precise estimation of \(_{B}\) within \(D\). As a sanity check, the gap vanishes when \(\) perfectly matches \(_{B}\) in \(D\).

**Theorem 4.8**.: _Let \(^{*}\) be the optimal risk-free offline adaptation policy at the convergence of the optimization of objective 2, and let \(\) be the policy at the convergence of objective 5. Then the worst-case adaptation performance of \(\) is near-optimal:_

\[_{_{D}}J(^{*},)_{_{D}}J( ,)_{_{D}}J(^{*},)-R_{M}(1+}). \]

## 5 Didactic Maze Example

In this section, we use an adversarial grid-world maze game to demonstrate the challenges posed by offline policy adaptation and explain the superiority of our algorithm. Figure 2 displays the maze environment, where Player 1 (P1) and Player 2 (P2) start from the labeled positions and can move in four directions within the maze. The grids marked with numbers are terminal states with indicated rewards. There are five corridors (2 for P1 and 3 for P2), each with different lengths and exponential final rewards2. For simplicity, we use the notation P2 \(\) 16 to denote P2's policy which going downwards to the state with terminal reward 16. The first agent reaching any terminal state wins the reward, and the opponent receives the negative reward as a penalty. The game ends immediately. We assume that P1 wins if P1 and P2 arrive at terminal states simultaneously. If no one reaches a terminals in 10 steps, they both get a penalty of -500. Both agents have full observation of the game states. Assume that P1 is the opponent, and we want to find exploiting policies for P2.

We consider four baselines: (1) **BC-First** as explained in section 1; (2) **Self-Play (max-min)**: approximating the pure max-min policy of P2 using self-play, which does not exploit; (3) **Dataset-Only**: completely following the dataset; and (4) **Oracle**: training an exploitation policy directly against the real target \(_{B}\). Please refer to Appendix D for detailed learning curves in this environment.

**Case 1: Inadequacy of Non-Conservative Methods** We show in this case that non-conservative algorithms easily overfit to an erroneous proxy model, lured to exploit "fake weaknesses" of the

Figure 2: The didactic maze example. The solid line indicates the testing trajectory of adaptation policy produced by CSP, while the dashed line indicates sub-optimal adaptation trajectories: **(left)** BC-First in maze case 1 and **(right)** Self-Play in maze case 2.

opponent. Suppose that P1's real policy \(_{B}\) is as follows: (1) if P1 observes P2 \(\) 16, P1 \(\) 4; (2) if P1 observes P2 \(\) 256, P1 \(\) 64; (3) otherwise P1 does not move. This is a reasonably strong policy for P1 which tries to exploit P2. Suppose the dataset is collected with \(_{B}\) and P2's rollout policy is P2 \(\) 256. Therefore, in the dataset, P1 is only observed going downwards. The proxy model trained by BC thus erroneously assumes that P1 \(\) 64 under any conditions. The results are shown on the left side of Table 1. The BC-First method makes a disastrous generalization and responds with a P2 \(\) 16 policy. Trying to exploit the "fake weaknesses" of the opponent results in failure in testing against the real opponent. However, CSP discovers that P2 \(\) 16 is unsafe and learns to safely win the game by playing P2 \(\) 1. Both CSP and max-min policy achieve the optimal solution. The Dataset-Only method is overly conservative and only gets -64. The trajectories of CSP and BC-First when exploiting the real target are shown on the left side of figure 2.

Case 2: Performing Risk-Free AdaptationIn this case, we show that CSP outperforms pure Self-Play (which does not exploit the opponent) by discovering risk-free exploitation opportunities. Suppose that P1 always plays an aggressive policy \(_{B}\): P1 \(\) 64. The dataset \(D\) is collected by \(_{B}\) with P2 playing P2 \(\) 16 and P2 \(\) 256 randomly. The results are shown on the right side of Table 1, and the trajectories of CSP and Self-Play are shown on the right side of Figure 2. The max-min policy produced by Self-Play cannot exploit P1's reckless policy. The generalization of the BC model is indeed correct in this specific situation, so BC-First is also optimal. Our algorithm recognizes that P2 \(\) 16 is a risk-free exploitation because the dataset has witnessed that P1 still plays P1 \(\) 64 even if P2 \(\) 16 and successfully exploits the opponent.

It is noteworthy that we use the same hyperparameters for CSP in both cases and our algorithm learns different adaptation policies based solely on the different datasets used. It further validates that our algorithm is able to extract useful information from the dataset, and to perform risk-free adaptations.

## 6 Experiment

We conduct extensive experiments on the predator-prey in MPE , a competitive two-player MuJoCo environment [2; 10] that requires continuous control, and Google Football . In all experiments, we compare CSP with (1) _BC-First_, and (2) _Self-Play_, which produces an approximate max-min conservative policy of the game. Self-Play can also be viewed as an ablation of CSP which does not utilize an offline dataset to adapt to targets. We also compare with multi-agent imitation learning (MAIL), and previous opponent exploitation method RNR , which does not perform conservative offline exploitation. More experiment details are available in Appendix G. We focus on performing opponent exploitation in competitive games. However, we also discuss the use of CSP for cooperative tasks in Appendix E.

### Experiments in Predator-Prey and MuJoCo

In the predator-prey environment, there are three adversarial agents, one good agent, and two obstacles. The good agent is penalized for being hit by adversaries, whereas the adversaries are rewarded. In experiments presented in Table 2, our agent controls either the good agent or the adversaries, respectively. We use a pre-trained model as target \(_{B}\). We report the episode rewards in evaluation with \(_{B}\). We use three types of datasets (10R, 100R, 100S) to demonstrate the impact of dataset quality in offline adaptation. The prefix number indicates the number of trajectories in the dataset. Letter "R" indicates that the dataset is collected by \(_{B}\) playing with a random policy, while "S" indicates that the dataset is collected by \(_{B}\) playing with another well-trained opponent.

   CASE 1 & Train & Test (against \(_{B}\)) & CASE 2 & Train & Test (against \(_{B}\)) \\  BC-First & 16 & -4 & BC-First & 16 & 16 \\ Self-Play & 1 & 1 & Self-Play & 1 & 1 \\ Dataset-Only & -64 & -64 & Dataset-Only & 16 & 16 \\
**CSP (Ours)** & 1 & 1 & **CSP (Ours)** & 16 & 16 \\  Oracle & 1 & 1 & Oracle & 16 & 16 \\   

Table 1: The training and test performances in maze environment **(left)** case 1 and **(right)** case 2.

Intuitively, 10R has the lowest dataset quality, while 100S has the highest. We train the Oracle directly against \(_{B}\) as an upper bound. Self-Play and Oracle do not use datasets. MAIL imitates the dataset collection policy. It fails to learn meaningful behavior on datasets 10R and 100R where the dataset collection policy is a random policy. Table 2 shows that CSP outperforms baselines and achieves comparable performances with Oracle, regardless of the dataset quality. CSP effectively learns to exploit the opponent safely, while the non-conservative method BC-First fails to do so. Moreover, BC-First is significantly more sensitive to dataset quality. In the controlling adversaries scenario, even with the 100S dataset, BC-First still performs worse than Self-Play, which does not exploit at all. It shows that the non-conservative method overfits the proxy model severely.

The YouShallNotPassHumanoids environment  is a MuJoCo-based two-player competitive game where the runner's objective is to pass the blocker and reach the red line on the opposite side. In our experiments, our agent acts as the runner, and the blocker is the target opponent for exploitation. In Table 3, we use four independently pre-trained blocker models as opponent targets. Since the exploitability of different opponent models can vary significantly, results against different opponents are not directly comparable. So we report results for all targets. CSP successfully exploits the opponent, and significantly outperforms the baselines for all targets. Additionally, we include a visualization of CSP's learned behavior in Appendix G.

### Experiments in Google Football

Google Football  is a popular and challenging benchmark for MARL. In our experiments, the adaptation policy controls all players in one team, while the opponent target policy controls the other team. We conduct experiments in 4 scenarios: _academy_3_vs_1_with_keeper_ (3vs1, where our agent acts as either the defender or the attacker), _academy_run_pass_and_shoot_with_keeper_ (RPS, defender), and _academy_counterattack_easy_ (defender). We report the winning rates of adaptation policies for five independently pre-trained opponent targets.

The experiment results in Table 4 show that our algorithm CSP outperforms baselines for almost all targets in all scenarios. Notably, the BC-First algorithm even

    &  &  \\  Dataset & 10R & 100R & 100S & 10R & 100R & 100S \\ 
**CSP(Ours)** & **-16.7\(\)0.4** & **-17.5\(\)0.4** & **-17.1\(\)0.1** & **34.6\(\)1.6** & **33.6\(\)3.2** & **34.7\(\)4.1** \\ BC-First & -29.8\(\)4.0 & -21.8\(\)1.6 & -18.5\(\)0.4 & 21.3\(\)0.8 & 25.2\(\)0.9 & 27.8\(\)1.9 \\ Self-Play & -17.6\(\)1.5 & -17.6\(\)1.5 & -17.6\(\)1.5 & 29.7\(\)2.8 & 29.7\(\)2.8 & 29.7\(\)2.8 \\ MAIL & -59.0\(\)0.6 & -54.2\(\)1.0 & -37.9\(\)1.4 & 4.3\(\)0.3 & 4.1\(\)0.2 & 25.8\(\)0.7 \\  Oracle & -16.0\(\)0.3 & -16.0\(\)0.3 & -16.0\(\)0.3 & 34.3\(\)6.2 & 34.3\(\)6.2 & 34.3\(\)6.2 \\   

Table 2: Testing episode rewards of adaptation policy in predator-prey of MPE.

Figure 3: The average test-train performance gap in four scenarios of Google Football. A negative gap indicates the occurrence of unsafe exploitation.

    & Method & 1 & 2 & 3 & 4 \\   & **CSP(Ours)** & **0.72 \(\) 0.06** & **0.65 \(\) 0.04** & **0.85 \(\) 0.08** & **0.88 \(\) 0.02** \\  & BC-First & 0.65 \(\) 0.13 & 0.46 \(\) 0.14 & 0.71 \(\) 0.17 & 0.72 \(\) 0.07 \\  & Self-Play & 0.54 \(\) 0.20 & 0.59 \(\) 0.14 & 0.52 \(\) 0.02 & 0.60 \(\) 0.19 \\   & **CSP(Ours)** & **878 \(\) 166** & **676 \(\) 100** & **1203 \(\) 209** & **1274 \(\) 73** \\  & BC-First & 693 \(\) 347 & 179 \(\) 456 & 848 \(\) 373 & 858 \(\) 185 \\   & Self-Play & 419 \(\) 527 & 500 \(\) 395 & 318 \(\) 106 & 601 \(\) 514 \\   

Table 3: Experiments in MuJoCo YouShallNotPassHumans environment. We show the winning rates and episode rewards against 4 independent target opponents..

performs worse than Self-Play, which does not utilize the opponent's information to exploit. It indicates that previous non-conservative algorithms are not guaranteed to use the opponent's information safely in offline adaptation.

In Figure 3, we show the gap of performances between testing and training: \(J(,_{B})-J(,)\). A negative gap indicates that although \(\) learns to exploit \(\) during training, such exploitation is risky and can not be guaranteed in testing. The results demonstrate the significant challenges that offline policy adaptation poses for baseline exploitation algorithms. BC-First overfits the proxy model in training and experiences a severe performance drop in testing. Conversely, CSP has a positive gap on average, indicating its ability to find conservative exploitation opportunities. Because CSP optimizes for the worst-case performance, it tends to produce a proxy model that is possibly stronger than the real target. Thus, its evaluation performances are even higher than those in training.

### Comparison with Non-Conservative Opponent Exploitation

We compare CSP with the previous safe exploitation method RNR , which minimizes the distance to NE when exploiting. RNR assumes that the opponent plays an NE policy with probability \(1-p\) and follows an estimated opponent model with \(p\). It then learns an exploitation policy. However, RNR does not optimize the conservative offline adaptation objective, and it still fails to handle offline policy adaptation due to the neglect of possible errors in the estimated opponent model. In Figure 4, the results show that CSP outperforms RNR for every dataset and every value of \(p\). RNR(0) is equivalent to Self-Play, and RNR(1) is equivalent to BC-First. Results for these two have already been reported in Table 2. The results further support our claim that previous methods struggle with offline policy adaptation and highlight the efficiency and significance of our algorithm.

   Scenario & Method & 1 & 2 & 3 & 4 & 5 \\   & **CSP(Ours)** & **0.9 \(\) 0.06** & **0.6 \(\) 0.12** & **0.45 \(\) 0.04** & **0.32 \(\) 0.11** & **0.76 \(\) 0.16** \\  & BC-First & 0.64 \(\) 0.04 & 0.46 \(\) 0.09 & 0.2 \(\) 0.01 & 0.16 \(\) 0.04 & 0.56 \(\) 0.09 \\  & Self-Play & 0.29 \(\) 0.09 & 0.34 \(\) 0.1 & 0.26 \(\) 0.16 & 0.29 \(\) 0.18 & 0.3 \(\) 0.13 \\   & **CSP(Ours)** & **0.81 \(\) 0.14** & **0.88 \(\) 0.02** & **0.83 \(\) 0.07** & **0.84 \(\) 0.05** & **0.78 \(\) 0.08** \\  & BC-First & **0.83 \(\) 0.03** & 0.74 \(\) 0.21 & 0.68 \(\) 0.07 & 0.79 \(\) 0.11 & 0.71 \(\) 0.15 \\  & Self-Play & 0.75 \(\) 0.15 & 0.76 \(\) 0.08 & 0.73 \(\) 0.13 & 0.7 \(\) 0.21 & 0.74 \(\) 0.08 \\  RPS defender & **CSP(Ours)** & 0.51 \(\) 0.33 & **0.71 \(\) 0.07** & **0.56 \(\) 0.13** & **0.38 \(\) 0.07** & **0.79 \(\) 0.09** \\  & BC-First & 0.51 \(\) 0.24 & 0.41 \(\) 0.07 & 0.34 \(\) 0.18 & **0.38 \(\) 0.04** & 0.71 \(\) 0.02 \\  & Self-Play & **0.57 \(\) 0.14** & 0.36 \(\) 0.04 & 0.25 \(\) 0.04 & **0.37 \(\) 0.1** & 0.76 \(\) 0.03 \\  Counter-attack & **CSP(Ours)** & **0.93 \(\) 0.02** & **0.88 \(\) 0.04** & **0.81 \(\) 0.25** & **0.81 \(\) 0.02** & **0.75 \(\) 0.06** \\  & BC-First & 0.7 \(\) 0.17 & 0.52 \(\) 0.07 & 0.53 \(\) 0.09 & 0.69 \(\) 0.14 & 0.5 \(\) 0.09 \\ defender & Self-Play & 0.55 \(\) 0.13 & 0.41 \(\) 0.1 & 0.46 \(\) 0.09 & 0.36 \(\) 0.08 & 0.36 \(\) 0.07 \\   

Table 4: Winning rates of offline adaptation policy in 4 scenarios of Google Football: 3vs1 (defender), 3vs1 (attacker), RPS (defender), and Counterattack Easy (defender). For each scenario, we experiment with 5 independent target opponents.

Figure 4: Comparison with RNR  in predator-prey, where our agent controls **(left)** adversaries and **(right)** good agent respectively. The y-axis represents the episode reward in testing.

Conclusion

We provide a thorough analysis of the unique challenges in offline policy adaptation, which are often neglected by previous literature. We propose the novel learning objective of conservative offline adaptation that optimizes the worst-case performance against any dataset-consistent opponent proxy. We propose an efficient algorithm that learns near-optimal conservative adaptation policies. Extensive empirical results in Maze, predator-prey, MuJoCo, and Google Football demonstrate that our algorithm significantly outperforms baselines.

One limitation of our work is that optimizing worst-case performance in real-world applications may not always be necessary since participants are not fully rational in practice. One future direction to settle this problem is to extend offline adaptation to non-stationary opponents, which is orthogonal to our current study. Those methods typically focus on adapting policy according to inferred opponent information in evaluation, while our algorithm prevents performance degradation in testing against a consistent opponent. One way is to use our algorithm to train a population of risk-free exploitation policies against opponents of different types and adapt exploitation policy in testing according to the opponent's posterior distribution. We discuss potential negative social impacts in Appendix F.