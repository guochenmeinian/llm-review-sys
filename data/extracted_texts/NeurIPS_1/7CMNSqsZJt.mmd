# ContextCite: Attributing Model Generation to Context

Benjamin Cohen-Wang,   Harshay Shah,   Kristian Georgiev,   Aleksander Madry

MIT

{bencw,harshay,krisgrg,madry}@mit.edu

Equal contribution.

###### Abstract

How do language models use information provided as context when generating a response? Can we infer whether a particular generated statement is actually grounded in the context, a misinterpretation, or fabricated? To help answer these questions, we introduce the problem of _context attribution_: pinpointing the parts of the context (if any) that _led_ a model to generate a particular statement. We then present ContextCite, a simple and scalable method for context attribution that can be applied on top of any existing language model. Finally, we showcase the utility of ContextCite through three applications: (1) helping verify generated statements (2) improving response quality by pruning the context and (3) detecting poisoning attacks. We provide code for ContextCite at [https://github.com/MadryLab/context-cite](https://github.com/MadryLab/context-cite).

## 1 Introduction

Suppose that we would like to use a language model to learn about recent news. We would first need to provide it with relevant articles as _context2_. We would then expect the language model to interact with this context to answer questions. Upon seeing a generated response, we might ask: is everything accurate? Did the model misinterpret any of the context or fabricate anything? Is the response actually _grounded_ in the provided context?

Answering these questions manually could be tedious--we would need to first read the articles ourselves and then verify the statements. To automate this process, prior work has focused on teaching models to generate _citations_: references to parts of the context that _support_ a response . They typically do so by explicitly training or prompting language models to produce citations.

In this work, we explore a different type of citation: instead of teaching a language model to cite its sources, can we directly identify the pieces of information that it actually _uses_? Specifically, we ask:

_Can we pinpoint the parts of the context (if any) that \(}\) to a particular generated statement?_

We refer to this problem as _context attribution_. Suppose, for example, that a language model misinterprets a piece of information and generates an inaccurate statement. In this case, context attribution would surface the misinterpreted part of the context. On the other hand, suppose that a language model uses knowledge that it learned from pre-training to generate a statement, rather than the context. In this case, context attribution would indicate this by not attributing the statement to any part of the context.

Unlike citations generated by language models, which can be difficult to validate , in principle it is easy to evaluate the efficacy of context attributions. Specifically, if a part of the context actually led to a particular generated response, then removing it should substantially affect this response.

### Our contributions

**Formalizing context attribution (Section 2).** We begin this work by formalizing the task of _context attribution_. Specifically, a context attribution method assigns a score to each part of the context indicating the degree to which it is responsible for a given generated statement. We provide metrics for evaluating these scores, guided by the intuition that removing high-scoring parts of the context should have a greater effect than removing low-scoring parts of the context.

**Performing context attribution with ContextCite (Sections 3 and 4).** Next, we present ContextCite, a simple and scalable method for context attribution that can be applied on top of any existing language model (see Figure 1). ContextCite learns a _surrogate model_ that approximates how a language model's response is affected by including or excluding each part of the context. This methodology closely follows prior work on attributing model behavior to features [8; 9; 10] and training examples [11; 12]. In the context attribution setting, we find that it is possible to learn a _linear_ surrogate model that (1) faithfully models the language model's behavior and (2) can be efficiently estimated using a small number of additional inference passes. The weights of this surrogate model can be directly treated as attribution scores. We benchmark ContextCite against various baselines on a diverse set of generation tasks and find that it is indeed effective at identifying the parts of the context responsible for a given generated response.

**Applying context attribution (Section 5).** Finally, we showcase the utility of ContextCite through three applications:

1. _Helping verify generated statements_ (Section 5.1): We hypothesize that if attributed sources do not also _support_ a generated statement, then it is less likely to be accurate. We find that using ContextCite sources can greatly improve a language model's ability to verify the correctness of its own statements.
2. _Improving response quality by pruning the context_ (Section 5.2): Language models often struggle to correctly use individual pieces of information within long contexts [13; 14]. We use ContextCite to select only the information that is most relevant for a given query, and then use this "pruned" context to regenerate the response. We find that doing so improves question answering performance on multiple benchmarks.
3. _Detecting context poisoning attacks_ (Section 5.3): Language models are vulnerable to context poisoning attacks: adversarial modifications to the context that can control the model's response to a given query [15; 16; 17; 18; 19]. We illustrate that ContextCite can consistently identify such attacks.

## 2 Problem statement

In this section, we will introduce the problem of context attribution (Section 2.1) and define metrics for evaluating context attribution methods (Section 2.2). To start, we will consider attributing an entire generated response--we will discuss attributing specific statements in Section 2.3.

Figure 1: ContextCite. Our context attribution method, ContextCite, traces any specified generated statement back to the parts of the context that are responsible for it.

**Setup.** Suppose that we use a language model to generate a response to a particular query given a context. Specifically, let \(p_{}\) be an _autoregressive_ language model: a model that defines a probability distribution over the next token given a sequence of preceding tokens. We write \(p_{}(t_{i} t_{1},,t_{i-1})\) to denote the probability of the next token being \(t_{i}\) given the preceding tokens \(t_{1},,t_{i-1}\). Next, let \(C\) be a context consisting of tokens \(c_{1},,c_{|C|}\) and \(Q\) be a query consisting of tokens \(q_{1},,q_{|Q|}\). We generate a response \(R\) consisting of tokens \(r_{1},,r_{|R|}\) by sampling from the model conditioned on the context and query. More formally, we generate the \(i^{}\) token \(r_{i}\) of the response as follows:

\[r_{i} p_{}(\, c_{1},,c_{|C|},q_{1},,q_{|Q|},r_{1},,r_{i-1})@note{footnote}{In practice, we may include additional tokens, e.g., to specify the beginning and end of a user's message.}.\]

We write \(p_{}(R C,Q)\) to denote the probability of generating the entire response \(R\)--the product of the probabilities of generating the individual response tokens--given the tokens of a context \(C\) and the tokens of a query \(Q\).

### Context attribution

The goal of context attribution is to attribute a generated response back to specific parts of the context. We refer to these "parts of the context" as _sources_. Each source is just a subset of the tokens in the context; for example, each source might be a document, paragraph, sentence, or even a word. The choice of granularity depends on the application--in this work, we primarily focus on _sentences_ as sources and use an off-the-shelf sentence tokenizer to partition the context into sources4.

A _context attribution method_\(\) accepts a list of \(d\) sources \(s_{1},,s_{d}\) and assigns a score to each source indicating its "importance" to the response. We formalize this task in the following definition:

**Definition 2.1** (_Context attribution_).: Suppose that we are given a context \(C\) with sources \(s_{1},,s_{d}\) (where \(\) is the set of possible sources), a query \(Q\), a language model \(p_{}\) and a generated response \(R\). A _context attribution method_\((s_{1},,s_{d})\) is a function \(:^{d}^{d}\) that assigns a score to each of the \(d\) sources. Each score is intended to signify the "importance" of the source to generating the response \(R\).

**What do context attribution scores signify?** So far, we have only stated that scores should signify how "important" a source is for generating a particular statement. But what does this actually mean? There are two types of attribution that we might be interested in: _contributive_ and _corroborative_. _Contributive_ attribution identifies the sources that _cause_ a model to generate a statement. Meanwhile, _corroborative_ attribution identifies sources that support or imply a statement. There are several existing methods for corroborative attribution of language models [1; 2; 4; 5]. These methods typically involve explicitly training or prompting models to produce citations along with each statement they make.

In this work, we study _contributive_ context attributions. These attributions give rise to a diverse and distinct set of use cases and applications compared to corroborative attributions (we explore a few in Section 5). To see why, suppose that a model misinterprets a fact in the context and generates an inaccurate statement. A corroborative method might not find any attributions (because nothing in the context supports its statement). On the other hand, a contributive method would identify the fact that the model misinterpreted. We could then use this fact to help verify or correct the model's statement.

### Evaluating the quality of context attributions

How might we evaluate the quality of a (contributive) context attribution method? Intuitively, a source's score should reflect the degree to which the response would change if the source were excluded. We introduce two metrics to capture this intuition. The first metric, the _top-k log-probability drop_, measures the effect of excluding the highest-scoring sources on the probability of generating the original response. The second metric, the _linear datamodeling score_ (LDS) , measures the extent to which attribution scores can predict the effect of excluding a random subset of sources.

To formalize these metrics, we first define a _context ablation_ as a modification of the context that excludes certain sources. To exclude sources, we choose to simply remove the corresponding tokens from the context5. We write \((C,v)\) to denote a context \(C\) ablated according to a vector\(v\{0,1\}^{d}\) (with zeros specifying the sources to exclude). We are now ready to define the _top-\(k\) log-probability drop_:

**Definition 2.2** (_Top-\(k\) log-probability drop_).: Suppose that we are given a context attribution method \(\). Let \(v_{k}()\) be an ablation vector that excludes the \(k\) highest-scoring sources according to \(\). Then the _top-\(k\) log-probability drop_ is defined as

\[k()=}(R C,Q)}_{}-}(R (C,v_{k}()),Q)}_{k }. \]

The top-\(k\) log-probability drop is a useful metric for comparing methods for context attribution. In particular, if removing the highest-scoring sources of one attribution method causes a larger drop than removing those of another, then we consider the former method to be identifying sources that are more important (in the contributive sense).

For a more fine-grained evaluation, we also consider whether attribution scores can accurately rank the effects of ablating different sets of sources on the log-probability of the response. Concretely, suppose that we sample a few different ablation vectors and compute the _sum_ of the scores corresponding to the sources that are included by each. These summed scores may be viewed as the "predicted effects" of each ablation. We then measure the rank correlation between these predicted effects and the actual resulting probabilities. This metric, known as the _linear datamodeling score_ (LDS), was first introduced by Park et al.  to evaluate methods for data attribution.

**Definition 2.3** (_Linear datamodeling score_).: Suppose that we are given a context attribution method \(\). Let \(v_{1},,v_{m}\) be \(m\) randomly sampled ablation vectors and let \(f(v_{1}),,f(v_{m})\) be the corresponding probabilities of generating the original response. That is, \(f(v_{i})=p_{}(R(C,v_{i}),Q)\). Let \(_{}(v)=(s_{1},,s_{d}),v\) be the sum of the scores (according to \(\)) corresponding to sources that are included by ablation vector \(v\), i.e., the "predicted effect" of ablating according to \(v\). Then the _linear datamodeling score_ (LDS) of a context attribution method \(\) can be defined as

\[()=(),,f(v_{m})\}}_{},_{}(v_{1}),,_{}(v_{m})\}}_{}), \]

where \(\) is the Spearman rank correlation coefficient .

### Attributing selected statements from the response

Until now, we have discussed attributing an entire generated response. In practice, we might be interested in attributing a particular statement, e.g., a sentence or phrase. We define a _statement_ to be any contiguous selection of tokens \(r_{i},,r_{j}\) from the response. To extend our setup to attributing specific statements, we let a context attribution method \(\) accept an additional argument \((i,j)\) specifying the start and end indices of the statement to attribute. Instead of considering the probability of generating the _entire_ original response, we consider the probability of generating the selected statement. Formally, in the definitions above, we replace \(p_{}(R C,Q)\) with

\[p_{}(,\ \,r_{j}}_{} C,Q,,\ \,r_{i-1}}_{}).\]

## 3 Context attribution with ContextCite

In the previous section, we established that a context attribution method is effective insofar as it is able to predict the effect of including or excluding certain sources. In other words, given an ablation vector \(v\), a context attribution method should inform how the probability of the original response,

\[f(v):=p_{}(R(C,v),Q),\]

changes as a function of \(v\). The design of ContextCite is driven by the following question: can we find a simple _surrogate model_\(\) that approximates \(f\) well? If so, we could use the surrogate model \(\) to understand how including or excluding subsets of sources would affect the probability of the original response (assuming that \(\) is simple enough). Indeed, surrogate models have previously been used in this way to attribute predictions to training examples [11; 12; 23; 24], model internals [25; 26], and input features [8; 9; 10]; we discuss connections in detail in Appendix C.1. At a high-level, our approach consists of the following steps:

**Step 1**: Sample a "training dataset" of ablation vectors \(v_{1},,v_{n}\) and compute \(f(v_{i})\) for each \(v_{i}\).
**Step 2**: Learn a surrogate model \(:\{0,1\}^{d}\) that approximates \(f\) by training on the pairs \((v_{i},f(v_{i}))\).
**Step 3**: Attribute the behavior of the surrogate model \(\) to individual sources.

For the surrogate model \(\) to be useful, it should (1) faithfully model \(f\), (2) be efficient to compute, and (3) yield scores attributing its outputs to the individual sources. To satisfy these desiderata, we find the following design choices to be effective:

* **Predict _logit-scaled_ probabilities**: Fitting a regression model to predict probabilities directly might be problematic because probabilities are bounded in \(\). The logit function (\(^{-1}(p)=\)) is a mapping from \(\) to \((-,)\), making logit-probability a more natural target for regression.
* **Learn a _linear_ surrogate model**: Despite their simplicity, we find that linear surrogate models are often quite faithful. With a linear surrogate model, each weight signifies the effect of ablating a source on the output. As a result, we can directly cast the weights of the surrogate model as attribution scores. We illustrate an example depicting the effectiveness of a linear surrogate model in Figure 2 and provide additional randomly sampled examples in Appendix B.2.
* **Learn a _sparse_ linear surrogate model**: Empirically, we find that a generated statement can often be explained well by just a handful of sources. In particular, Figure 2(a) shows that the number of sources that are "relevant" to a particular generated statement is often small, even when the context comprises many sources. Motivated by this observation, we induce sparsity in the surrogate model via Lasso. As we illustrate in Figure 2(b), this enables learning a faithful linear surrogate model even with a small number of ablations. For example, the surrogate model in Figure 2 uses just \(32\) ablations even though the context comprises \(98\) sources (in this case, sentences).
* **Sample ablation vectors uniformly**: To create the surrogate model's training dataset, we sample ablation vectors uniformly from the set of possible subsets of context sources.

We summarize the resulting method, ContextCite, in Algorithm 1. See Figure 2 for an example of ContextCite attributions; we provide additional examples in Appendix B.2.

Figure 2: An example of the _linear_ surrogate model used by ContextCite. On the left, we consider a context, query, and response generated by L1ama-3-8B about weather in Antarctica. In the middle, we list the weights of a linear surrogate model that estimates the logit-scaled probability of the response as a function of the context ablation vector (3); ContextCite casts these weights as attribution scores. On the right, we plot the surrogate model’s predictions against the actual logit-scaled probabilities for random context ablations. Two sources appear to be primarily responsible for the response, resulting in four “clusters” corresponding to whether each of these sources is included or excluded. These sources appear to interact _linearly_—the effect of removing both sources is close to the sum of the effects of removing each source individually. As a result, the linear surrogate model faithfully captures the language model’s behavior.

## 4 Evaluating ContextCite

In this section, we evaluate whether ContextCite can effectively identify sources that cause the language model to generate a particular response. Specifically, we use the evaluation metrics described in Section 2.2--top-\(k\) log-probability drop (1) and linear datamodeling score (LDS) (2)--to benchmark ContextCite against a varied set of baselines. See Appendix A.5 for the exact setup and Appendix B.3 for results with additional models, datasets, and baselines.

**Datasets.** Generation tasks can differ in terms of (1) context properties (e.g., length, complexity) and (2) how the model uses in-context information to generate a response (e.g., summarization, question answering, reasoning). We evaluate ContextCite on up to \(1,000\) random validation examples from each of three representative benchmarks:

1. _TyDi QA_ is a question-answering dataset in which the context is an entire Wikipedia article.
2. _Hotpot QA_ is a _multi-hop_ question-answering dataset where answering the question requires reasoning over information from multiple documents.
3. _CNN DailyMail_ is a dataset of news articles and headlines. We prompt the language model to briefly summarize the news article.

**Models.** We use ContextCite to attribute responses from the instruction-tuned versions of Llama-3-BB and Phi-3-mini.

**Baselines.** We consider three natural baselines adapted from prior work on model explanations. We defer details and additional baselines that we found to be less effective to Appendix A.5.1.

Figure 3: **Inducing sparsity improves the surrogate model’s sample efficiency. In CNN DailyMail , a summarization task, and Natural Questions , a question answering task, we observe that the number of sources that are “relevant” for a particular statement generated by Llama-3-BB is small, even when the context comprises many sources (Figure 2(a)). Therefore, inducing sparsity via Lasso yields an accurate surrogate model with just a few ablations (Figure 2(b)). See Appendix A.4 for the exact setup.**1. _Leave-one-out_: We consider a leave-one-out baseline that ablates each source individually and compute the log-probability drop of the response as an attribution score. Leave-one-out is an oracle for the top-\(k\) log-probability drop metric (1) when \(k=1\), but may be prohibitively expensive because it requires an inference pass for every source.
2. _Attention_: A line of work on explaining language models leverages attention weights . We use a simple but effective baseline that computes an attribution score for each source by summing the average attention weight of individual tokens in the source across all heads in all layers.
3. _Gradient norm_: Other explanation methods rely on input gradients . Here, following Yin and Neubig , we estimate the attribution score of each source by computing the \(_{1}\)-norm of the log-probability gradient of the response with respect to the embeddings of tokens in the source.
4. _Semantic similarity_: Finally, we consider attributions based on semantic similarity. We employ a pre-trained sentence embedding model  to embed each source and the generated statement. We treat the cosine similarities between these embeddings as attribution scores.

**Experiment setup.** Each example on which we evaluate consists of a context, a query, a language model, and a generated response. As discussed in Section 2.3, rather than attributing the entire response to the context, we consider attributing individual _statements_ in the response to the context. Specifically, given an example, we (1) split the response into sentences using an off-the-shelf tokenizer , and (2) compute attribution scores for each sentence. Then, to evaluate the attribution scores, we measure the top-\(k\) log-probability drop for \(k=\{1,3,5\}\) (1) and LDS (2) for each sentence separately, and then average performances across sentences. Our experiments perform this evaluation for every combination of context attribution method, dataset, and language model. We evaluate ContextCite with \(\{32,64,128,256\}\) context ablations.

**Results.** In Figure 4, we find that ContextCite consistently outperforms baselines, even when we only use \(32\) context ablations to compute its surrogate model. While the attention baseline approaches the performance of ContextCite with Llama-3-8B, it fares quite poorly with Phi-3-mini suggesting that attention is not consistently reliable for context attribution. ContextCite also attains high LDS across benchmarks and models, indicating that its attributions accurately predict the effects of ablating sources.

## 5 Applications of ContextCite

In Section 4, we found that ContextCite is an effective (contributive) context attribution method. In other words, it identifies the sources in the context that _cause_ the model to generate a particular statement. In this section, we present three applications of context attribution: helping verify generated statements (Section 5.1), improving response quality by pruning the context (Section 5.2), and detecting poisoning attacks (Section 5.3).

### Helping verify generated statements

It can be difficult to know when to _trust_ statements generated by language models . In this section, we investigate whether ContextCite can help language models verify the accuracy of their own generated statements.

**Approach.** Our approach builds on the following intuition: if the sources identified by ContextCite for a particular statement do not _support_ it, then the statement might be inaccurate. To operationalize this, we (1) use ContextCite to identify the top-\(k\) most relevant sources and (2) provide the same language model with these sources and ask it if we can conclude that the statement is correct. We treat the model's probability of answering "yes" as a verification score.

**Experiments.** We apply our verification pipeline to answers generated by Llama-3-8B for \(1,000\) random examples from each of two question answering datasets: HotpotQA  and Natural Questions . We provide the language model with the top-\(k\) most relevant sources (for a few different values of \(k\)) and measure its AUC for predicting whether its generated answer is accurate. As a baseline, we provide the model with the entire context and measure this AUC in the same manner. In Figure 5, we observe that the verification scores obtained using the top-\(k\) sources are substantially higher than those obtained from using the entire context. This suggests that context attribution can be used to help language models verify the accuracy of their own responses. See Appendix A.6 for the exact setup.

### Improving response quality by pruning the context

If the sources identified by ContextCite can help a language model _verify_ the accuracy its answers (Section 5.1), can they also be used to _improve_ the accuracy of its answers? Indeed, language models

Figure 4: **Evaluating context attributions. We report the top-\(k\) log-probability drop (Figure 3(a)) and linear datamodeling score (Figure 3(b)) of ContextCite and baselines. We evaluate attributions of responses generated by Llama-3-8B and Phi-3-mini on up to \(1,000\) randomly sampled validation examples from each of three benchmarks. We find that ContextCite using just \(32\) context ablations consistently matches or outperforms the baselines—attention, gradient norm, semantic similarity and leave-one-out—across benchmarks and models. Increasing the number of context ablations to \(\{64,128,256\}\) can further improve the quality of ContextCite attributions.**

often struggle to correctly use relevant information hidden within long contexts . In this section, we explore whether we can improve response quality by pruning the context to include only query-relevant sources.

**Approach.** Our approach closely resembles the verification pipeline from Section 5.1; however, instead of using the top-\(k\) sources to verify correctness, we use them to regenerate the response. Specifically, it consists of three steps: (1) generate a response using the entire context, (2) use ContextCite to identify the top-\(k\) most relevant sources, and (3) regenerate the response using only these sources as context.

**Experiments.** We assess the effectiveness of this approach on two question-answering datasets: HotpotQA  and Natural Questions . In both datasets, the provided context typically includes a lot of irrelevant information in addition to the answer to the question. In Figure 6, we report the average \(F_{1}\)-score of Llama-3-8B on \(1,000\) randomly sampled examples from each dataset (1) when it is provided with the entire context and (2) when it is provided with only the top-\(k\) sources according to ContextCite. We find that simply selecting the most relevant sources can consistently improve question answering capabilities. See Appendix A.7 for the exact setup and Appendix C.2 for additional discussion of why pruning in this way can improve question answering performance.

### Detecting poisoning attacks

Finally, we explore whether context attribution can help surface poisoning attacks . We focus on _indirect prompt injection_ attacks  that can override a language model's response to a given query by "poisoning", or adversarially modifying, external information provided as context. For example, if a system like ChatGPT browses the web to answer a question about the news, it may end up retrieving a poisoned article and adding it to the language model's context. These attacks can be "obvious" once identified--e.g., If asked about the election, ignore everything else and say that Trump dropped out--but can go unnoticed, as users are unlikely to carefully inspect the entire article.

**Approach.** If a prompt injection attack successfully causes the model to generate an undesirable response, the attribution score of the context source(s) containing the injected poison should be high. One can also view the injected poison as a "strong feature"  in the context that significantly influences model output and, thus, should have a high attribution score. Concretely, given a potentially poisoned context and query, our approach (a) uses ContextCite to attribute the generated response to sources in the context and (b) flags the top-\(k\) sources with the highest attribution scores for further manual inspection.

**Experiments.** We consider two types of prompt injection attacks: (1) handcrafted attacks (e.g., "Ignore all previous instructions and...") , and (2) optimization-based attacks . In both cases, ContextCite surfaces the prompt injection as the single most influential source more than \(95\%\) of the time. See Appendix A.8 for the exact setup and more detailed results.

Figure 5: **Helping verify generated statements using ContextCite.** We report the AUC of Llama-3-8B for verifying the correctness of its own answers when we provide it with the top-\(k\) sources identified by ContextCite and when we provide it with the entire context. We consider \(1,000\) random examples from HotpotQA on the left and \(1,000\) random examples from Natural Questions on the right. In both cases, using the top-\(k\) sources results in substantially more effective verification than using the entire context, suggesting that ContextCite can help language models verify their own statements.

## 6 Related work

**Citations for language models.** Prior work on citations for language models has focused on teaching models to generate citations for their responses [1; 4; 2; 3; 5; 51; 52]. For example, Menick et al.  fine-tune a pre-trained language model to include citations to retrieved documents as part of its response. Gao et al.  use prompting and in-context demonstrations to do the same. _Post-hoc_ methods for citation [4; 51] attribute existing responses by using an auxiliary language model to identify relevant sources. Broadly, existing methods for generating citations are intended to be _corroborative_ in nature; citations are evaluated on whether they _support_ or imply a generated statement [53; 6; 7; 54]. In contrast, ContextCite--a _contributive_ attribution method--identifies sources that _cause_ a language model to generate a given response.

**Explaining language model behavior.** Related to context attribution is the (more general) problem of explaining language model behavior. Methods for explaining language models have used attention weights [37; 38], similarity metrics  and input gradients [42; 55], which we adapt as baselines. The explanation approaches that are closest in spirit to ContextCite are ablation-based methods, often relying on the Shapley value [8; 9; 56; 57; 58]. In particular, Sarti et al.  quantify context reliance in machine translation models by comparing model predictions with and without context; this may be viewed as a coarse-grained variant of the context ablations performed by ContextCite. Concurrently to our work, Qi et al.  extend the method of Sarti et al.  to study context usage in retrieval-augmented generation pipelines, yielding attributions for answers to questions.

**Understanding model behavior via surrogate modeling.** Several prior works employ _surrogate modeling_ to study different aspects of model behavior. For example, data attribution methods use linear surrogate models to trace model predictions back to individual training examples [11; 12; 62; 63] or in-context learning examples [23; 24]. Similarly, methods for identifying input features that drive a model prediction [8; 9; 10] or for attributing predictions back to internal model components [25; 26] have also leveraged surrogate modeling. Many of the key design details of ContextCite, namely, learning a sparse linear surrogate model and predicting the effect of ablations, were previously found to be effective in other settings by these prior works. We provide a detailed discussion of the connections between ContextCite and these methods in Appendix C.1.

## 7 Conclusion

We introduce the problem of _context attribution_ whose goal is to trace a statement generated by a language model back to the specific parts of the context that _caused_ the model to generate it. Our proposed method, ContextCite, leverages linear surrogate modeling to accurately attribute statements generated by any language model in a scalable manner. Finally, we present three applications of ContextCite: (1) helping verify generated statements (2) improving response quality by pruning the context and (3) detecting poisoning attacks.

Figure 6: **Improving response quality by constructing query-specific contexts.** On the left, we show that filtering contexts by selecting the top-\(\{2,,16\}\) query-relevant sources (via ContextCite) improves the average \(F_{1}\)-score of Llama-3-8B on \(1,000\) randomly sampled examples from the Hotpot QA dataset. Similarly, on the right, simply replacing the entire context with the top-\(\{8,,128\}\) query-relevant sources boosts the average \(F_{1}\)-score of Llama-3-8B on \(1,000\) randomly sampled examples from the Natural Questions dataset. In both cases, ContextCite improves response quality by extracting the most query-relevant information from the context.

Acknowledgments

The authors would like to thank Bagatur Askaryan, Andrew Ilyas, Alaa Khaddaj, Virat Kohli, Maya Lathi, Guillaume Leclerc, Sharut Gupta, Evan Vogelbaum for helpful feedback and discussions. Work supported in part by the NSF grant DMS-2134108 and Open Philanthropy.

## Appendix

* A Experiment details
* A.1 Implementation details
* A.2 Models
* A.3 Datasets
* A.4 Learning a _sparse_ linear surrogate model
* A.5 Evaluating ContextCite
* A.6 Helping verify generated statements
* A.7 Improving response quality by pruning the context
* A.8 Detecting poisoning attacks
* B Additional results
* B.1 Random examples of ContextCite attributions
* B.2 Linear surrogate model faithfulness on random examples
* B.3 Additional evaluation
* B.4 ContextCite for larger models
* B.5 Word-level ContextCite
* C Additional discussion
* C.1 Connections to prior methods for understanding behavior via surrogate modeling
* C.2 Why does pruning the context improve question answering performance?
* C.3 Computational efficiency of ContextCite
* C.4 Limitations of ContextCite
Experiment details

### Implementation details

We run all experiments on a cluster of A100 GPUs. We use the scikit-learn implementation of Lasso for ContextCite, always with the regularization parameter alpha set to 0.01. When splitting the context into sources or splitting a response into statements, we use the off-the-shelf sentence tokenizer from the nltk library . Our implementation of ContextCite is available at [https://github.com/MadryLab/context-cite](https://github.com/MadryLab/context-cite).

### Models

The language models we consider in this work are Llama-3-{8/70}B, Mistral-7B and Phi-3-mini. We use instruction-tuned variants of these models. We use the implementations of language models from HuggingFace's transformers library . Specifically, we use the following models:

* Llama-3-{8/70}B: meta-llama/Meta-Llama-3-{8/70}B-Instruct
* Mistral-7B: mistralai/Mistral-7B-Instruct-v0.2
* Phi-3-mini: microsoft/Phi-3-mini-128k-instruct

When generating responses with these models, we use their standard chat templates, treating the prompt formed from the context and query as a user's message.

### Datasets

We consider a variety of datasets to evaluate ContextCite spanning question answering and summarization tasks and different context structures and lengths. We provide details about these datasets and preprocessing steps in this section. Some of the datasets, namely Natural Questions and TyDi QA, contain contexts that are longer than the maximum context window of the models we consider. In particular, Llama-3-8B has the shortest context window of \(8,192\) tokens. When evaluating, we filter datasets to include only examples that fit within this context window (with a padding of \(512\) tokens for the response).

**CNN DailyMail** is a news summarization dataset. The contexts consists of a news article and the query asks the language model to briefly summarize the articles in up to three sentences. We use the following prompt template:

Context: {context}

Query: Please summarize the article in up to three sentences.

**Hotop QA.** is a _multi-hop_ question-answering dataset in which the context consists of multiple short documents. Answering the question requires combining information from a subset of these documents--the rest are "distractors" containing information that is only seemingly relevant. We use the following prompt template:

Title: {title_1}

Content: {document_1}

...

Title: {title_n}

Content: {document_n}

Query: {question}

**MS MARCO** is question-answering dataset in which the question is a Bing search query and the context is a passage from a retrieved web page that can be used to answer the question. We use the following prompt template:Context: {context}

Query: {question}

Natural Questions is a question-answering dataset in which the questions are Google search queries and the context is a Wikipedia article. The context is provided as raw HTML; we include only paragraphs (text within <p> tags) and headers (text within <h[1-6]> tags) and provide these as context. We filter the dataset to include only examples where the question can be answered just using the article. We use the same prompt template as MS MARCO.

TyDi QA is a multilingual question-answering dataset. The context is a Wikipedia article and the question about the topic of the article. We filter the dataset to include only English examples and consider only examples where the question can be answered just using the article. We use the same prompt template as MS MARCO.

#### a.3.1 Dataset statistics.

In Table 1, we provide the average and maximum numbers of sources in the datasets that we consider.

#### a.3.2 Partitioning contexts into sources and ablating contexts

In this section, we discuss how we partition contexts into sources and perform context ablations. For every dataset besides Hotpot QA, we use an off-the-shelf sentence tokenizer  to partition the context into sentences. To perform a context ablation, we concatenate all of the included sentences and provide the resulting string to the language as context. The Hotpot QA context consists of multiple documents, each of which includes annotations for individual sentences. Furthermore, the documents have titles, which we include in the prompt (see Appendix A.3). Here, we still treat sentences as sources and include the title of a document as part of the prompt if at least one of the sentences of this document is included.

### Learning a _sparse_ linear surrogate model

In Figure 3, we illustrate that ContextCite can learn a faithful surrogate model with a small number of ablations by exploiting underlying sparsity. Specifically, we consider CNN DailyMail and Natural Questions. For \(1,000\) randomly sampled validation examples for each dataset, we generate a response with Llama-3-BB using the prompt templates in Appendix A.3. Following the discussion in Section 2.3, we split each response into sentences and consider each of these sentences to be a "statement." For the experiment in Figure 2(a), for each statement, we ablate each of the sources individually and consider the source to be relevant if this ablation changes the probability of the statement by a factor of at least \(=2\). For the experiment in Figure 2(b), we report the average root mean squared error (RMSE) over these statements for surrogate models trained using different numbers of context ablations. See Appendices A.2 and A.3 for additional details on datasets and models.

  
**Dataset** & **Average number of sources** & **Maximum number of sources** \\  MS MARCO & 36.0 & 95 \\ Hotpot QA & 42.0 & 94 \\ Natural Questions & 103.3 & 353 \\ TyDi QA & 165.8 & 872 \\ CNN DailyMail & 32.4 & 172 \\   

Table 1: The average and maximum numbers of sources (in this case, sentences) among the up to \(1,000\) randomly sampled examples from each of the datasets we consider.

### Evaluating ContextCite

See Appendices A.1 to A.3 for details on implementation, datasets and models for our evaluations.

#### a.5.1 Baselines for context attribution

We provide a detailed list of baselines for context attribution in this section. In addition to the baselines described in Section 4, we consider additional attention-based and gradient-based baselines. We provide evaluation results including these baselines in Appendix B.3.

1. _Average attention_: We compute average attention weights across heads and layers of the model. We compute the sum of these average weights between every token of a source and every token of the generated statement to attribute as an attribution score. This is the attention-based baseline that we present in Figure 4.
2. _Attention rollout_: We consider the more sophisticated attention-based explanation method of Abnar and Zuidema . Attention rollout seeks to capture the _propagated_ influence of each token on each other token. Specifically, we first average the attention weights of the heads within each layer. Let \(A_{}^{n n}\) denote the average attention weights for the \(\)'th layer, where \(n\) is the length of the sequence. Then the propagated attention weights for the \(\)'th layer, which we denote \(_{}^{n n}\), are defined recursively as \(_{}=A_{}_{-1}\) for \(>1\) and \(_{1}=A_{1}\). Attention rollout computes an "influence" of token \(j\) on token \(i\) by computing the product \((A_{0}A_{1} A_{L})_{ij}\) where \(L\) is the total number of layers. When the model contains residual connections (as ours do), the average attention weights are replaced with \(0.5A_{}+0.5I\) when propagating influences.
3. _Gradient norm_: Following Yin and Neubig , in Section 4 we estimate the attribution score of each source by computing the \(_{1}\)-norm of the log-probability gradient of the response with respect to the embeddings of tokens in the source. In Appendix B.3, we also consider the \(_{2}\)-norm of these gradients, but find this to be slightly less effective.
4. _Gradient times input_: As an additional gradient-based baseline, we also consider taking the dot product of the gradients and the embeddings following Shrikumar et al.  in Appendix B.3, but found this to be less effective than the gradient norm.
5. _Semantic similarity_: Finally, we consider attributions based on semantic similarity. We employ a pre-trained sentence embedding model  to embed each source and the generated statement. We treat the cosine similarities between these as attribution scores.

### Helping verify generated statements

In Section 5.1, we explore whether ContextCite can help language models verify the accuracy of their own generated statements. Specifically, we first use ContextCite to identify a set of the top-\(k\) most relevant sources. We then ask the language model whether we can conclude that the statement is accurate based on these sources. The following are additional details for this experiment:

1. _Datasets and models._ We evaluate this approach on two question-answering datasets: HotpotQA  and Natural Questions . For each of these datasets, we evaluate the \(_{1}\) score of instruction-tuned Llama-3-8B (Figure 6) on \(1,000\) randomly sampled examples from the validation set.
2. _Question answering prompt._ We modify the prompts outlined for HotpotQA and Natural Questions in Appendix A.3 to request the answer as a short phrase or sentence. This allows us to assess the correctness of the generated answer. [frame=single, left=0pt, right=0pt, top=0pt, bottom=

[MISSING_PAGE_EMPTY:16]

[MISSING_PAGE_FAIL:17]

question-answering, we turn our attention to summarization. In particular, the query for each of the context articles is

Query: Briefly summarize this article.

We then sample a random place within the context article and insert a twenty-character placeholder, which we then optimize with GCG to maximize the likelihood of the model outputting

Response: I am sorry, I cannot assist with that. <|endoftext|>

Given the long contexts, as well as the fact that we insert the adversarial tokens in the middle of the context (and not as a suffix), we observe a very low success rate of these optimization-based attacks. In particular, we report a success rate of just \(2\%\). We then filter only the prompts containing a successful attack, and construct a dataset, which we provide in our code release. Due to the high computational cost of the GCG attack (as well as the low success rate), this dataset is also small in size (22 samples, filtered down from 1000 GCG attempts, each on a random Wikipedia article).

Qualitatively, ContextCite fails to surface the GCG-optimized sentence as the one with the highest attribution score when the attack is not fully successful. For example, rather than outputting the target response, for one of the contexts, Phi-3-mini instead generates Python code to give a summary of the article:

Response: ''python def summarize_article(article_text):  # Split the article into sentences sentences sentences = article_text.split('. ')  # Extract the main points  main_points = []  for sentence in sentences:  if 'Pittsburgh Steelers' in sentence or 'Baltimore Ravens' in sentence:  main_points.append(sentence)  # Create a summary summary =''.join(main_points)  return summary

We found another failure mode to be noteworthy as well. When using the Wikipedia article about Tupper Lake in New York, ContextCite finds the sentence

Roger Allen LaPorte, Vietnam War protester, immolated himself in front of the United Nations building.

as the main source leading Phi-3-mini to refuse to summarize the article. Indeed, the model refuses to discuss this sensitive topic even without the GCG-optimized prompt.

**Optimization-based attacks on**Llama3-8B**. Finally, we mount the prompt injections attack NeuralExec developed by Pasquini et al. . In short, the attack consists of generating a universal optimized prompt injection which surrounds a "payload" message. The goal of the optimized prompt injection is to maximize the likelihood of the payload message being picked up by the model. One can view the NeuralExec attack as an optimization-based counterpart to the handcrafted attacks we consider .

For Llama3-8B, the universal (i.e., independent of the context) prompt injection is 

[MISSING_PAGE_EMPTY:19]

Additional results

### Random examples of ContextCite attributions

In this section, we provide ContextCite attributions for randomly selected examples from a few datasets. For each example, we randomly select a sentence from the response to attribute and display the \(4\) sources with the highest attribution scores.

[MISSING_PAGE_EMPTY:21]

### Linear surrogate model faithfulness on random examples

On the right side of Figure 2, we show the actual logit-probabilities of different context ablations as well as the logit-probabilities predicted by a linear surrogate model. In that example, the linear surrogate model is quite faithful. In this section, we provide additional randomly sampled examples from CNN DailyMail (see Figure 7), Natural Questions (see Figure 8), and TyDi QA (see Figure 9). We use \(256\) context ablations to train the surrogate model, and observe that a linear surrogate model is broadly faithful across these benchmarks.

Figure 7: The predicted logit-probabilities of a surrogate model trained on \(256\) context ablations on randomly sampled examples from the CNN DailyMail, a summarization benchmark.

Figure 8: The predicted logit-probabilities of a surrogate model trained on \(256\) context ablations on randomly sampled (answerable) examples from the Natural Questions, a question answering benchmark.

### Additional evaluation

Using the same experiment setup as in Section 4, we evaluate ContextCite on additional models (Phi-3-mini) and additional benchmarks (TyDi QA and MS MARCO), and also compare it to additional baselines: \(_{2}\)-gradient norm, gradient-times-input, and attention rollout . In Figure 10 and Figure 11, we show that ContextCite consistently outperforms the baselines across all models on the top-\(k\) log-probability drop metric and the linear datamodeling score, respectively.

Figure 9: The predicted logit-probabilities of a surrogate model trained on \(256\) context ablations on randomly sampled (answerable) English examples from the TyDi QA, a question answering benchmark.

Figure 10: **Evaluating ContextCite on additional models and benchmarks using the top-\(k\) log-probability drop metric (1). We compare ContextCite to additional baselines (\(_{2}\)-gradient norm, gradient-times-input, and attention rollout) on three models (Llama-3-8B, Phi-3-mini, Mistral-7B) and two additional benchmarks (TyDi QA and MS-MARCO). Each row corresponds to a different benchmark and each column corresponds to a different model. Across all benchmarks and models, ContextCite (with just \(32\) calls) consistently outperforms the baselines on the top-\(k\) log-probability drop metric, which measures the effect of ablating the top-\(k\) context sources with the highest attribution scores. Similar to our results in Figure 3(a), increasing the number of context ablations to \(\{64,128,256\}\) can further improve the quality of ContextCite attributions in this setting as well.**

### ContextCite for larger models

Our evaluation suite for ContextCite in Section 4 consists of models with up to \(8\) billion parameters. In this section, we conduct a more limited evaluation of ContextCite for a larger model, Llama-3-70B. We find that ContextCite is effective even at this larger scale.

#### b.4.1 Evaluation of ContextCite for Llama-3-70B

In Figure 12, we evaluate ContextCite for Llama-3-70B on the CNN DailyMail and Hotpot QA benchmarks using the top-\(k\) log-probability drop metric (1) and the linear datamodeling score (2). We use the same evaluation setup as in Section 4, but use a subset of the baselines and only use \(32\) context ablations for ContextCite due to computational cost. We find that ContextCite consistently outperforms baselines.

Figure 11: **Evaluating ContextCite on additional models and benchmarks using the linear datamodeling score (2).** Like in Figure 10, we compare ContextCite to additional baselines (\(_{2}\)-gradient norm, gradient-times-input, and attention rollout) on three models (Llama-3-8B, Phi-3-mini, Mistral-7B) and two additional benchmarks (TyDi QA and MS-MARCO). Each row corresponds to a different benchmark and each column corresponds to a different model. Across all benchmarks and models, ContextCite (with just \(32\) calls) consistently outperforms the baselines on the linear datamodeling score, which quantifies the extent to which context attributions predict the effect of ablating the context sources on the model response. Similar to our results in Figure 3(b), increasing the number of context ablations to \(\{64,128,256\}\) further improves the quality of ContextCite attributions in this setting as well.

#### b.4.2 Random examples of ContextCite for Llama-3-70B

In this section, we provide ContextCite attributions for Llama-3-70B for randomly selected examples. For each example, we randomly select a sentence from the response to attribute and display the \(4\) sources with the highest attribution scores.

Figure 12: **Evaluating word-level context attributions.** We report the top-\(k\) log-probability drop (Figure 12a) and linear datamodeling score (Figure 12b) of ContextCite and baselines. We evaluate attributions of responses generated by Llama-3-70B on \(1,000\) randomly sampled validation examples from each of CNN DailyMail and Hotpot QA.

[MISSING_PAGE_EMPTY:29]

### Word-level ContextCite

In this work, we primarily focus on _sentences_ on sources for context attribution. In this section, we briefly explore using ContextCite to perform context attribution with individual words as sources on the DROP benchmark . We find that ContextCite can provide effective word-level attributions, but may require a larger number of context ablations.

#### b.5.1 Evaluation of word-level ContextCite

In Figure 13, we evaluate word-level ContextCite on the DROP benchmark using the top-\(k\) log-probability drop metric (1) and the linear datamodeling score (2). We use the same evaluation setup as in Section 4. While ContextCite matches or outperforms baselines, we find that it attains lower absolute values for the linear datamodeling score. This may be because word-level attributions are less sparse: a given generated statement may depend on many individual words within the context. It may also be because there are much stronger dependencies between words than between sentences, rendering a linear surrogate model less faithful.

#### b.5.2 Random examples of word-level ContextCite

In this section, we provide word-level ContextCite attributions for Llama-3-BB for randomly selected examples. For each example, we randomly select a sentence from the response to attribute and display the \(4\) sources with the highest attribution scores.

Figure 13: **Evaluating word-level context attributions.** We report the top-\(k\) log-probability drop (Figure 12(a)) and linear datamodeling score (Figure 12(b)) of ContextCite and baselines. We evaluate attributions of responses generated by Llama-3-BB on \(1,000\) randomly sampled validation examples from the DROP benchmark.

[MISSING_PAGE_EMPTY:32]

    \\  
**Score** & **Source (from the context)** \\ 
**14.1** &... tight end Tony Gonzalez. After a **scoreless** third quarter, \\  & Chicago would tie the... \\ 
**7.09** &... the Chicago Bears. After a **scoreless** first quarter, Atlanta \\  & would trail early... \\ 
**1.54** &..., the Falcons went home for a **Week** 6 Sunday night duel with the \\  & Chicago... \\ 
**1.5** &... pass to tight end Tony Gonzalez. **After** a scoreless third quarter, \\  & Chicago would... \\ 
**1.14** &... road win over the 49ers, the **Falcons** went home for a Week 6 \\  & Sunday... \\ 
**1** &... hooking up with tight end Greg Olsen **on** a 2 - yard touchdown. \\  &... \\ 
**0.913** &... the game in the fourth quarter with **Cutler** hooking up with tight \\  & end Greg Olsen... \\ 
**0.865** &... running back Michael Turner got a 5 \$ yard touchdown run. \\  & Afterwards,... \\   

Additional discussion

### Connections to prior methods for understanding behavior via surrogate modeling

ContextCite attributes a language model's generation to individual sources in the context by learning a _surrogate model_ that simulates how excluding different sets of sources affects the model's output. The approach of learning a surrogate model to predict the effects of ablations has previously been used to attribute predictions to training examples [11; 23; 24], model internals , and input features [8; 9; 10]. For example, Ilyas et al.  learn a surrogate model to predict how excluding different training examples affects a model's output on a particular test example.

One key design choice shared by many of these methods is to learn a _linear_ surrogate model (whose input is an ablation mask). A linear surrogate model is easily interpretable, as its weights may be cast directly as attributions. Another key design choice is to induce _sparsity_ in the surrogate model, typically by learning with Lasso. Sparsity can further improve interpretability and may also decrease the number of samples needed to learn a faithful surrogate model. We find these design choice to be effective in the context attribution setting and adopt them for ContextCite. In the remainder of this section, we discuss detailed connections between ContextCite and a few closely related methods: LIME , Kernel SHAP , and datamodels .

**LIME .**

LIME (Local Interpretable Model-agnostic Explanations) is a method for attributing predictions of black-box classifiers to features. It does so by learning a local surrogate model that simulates the classifier's behavior in a neighborhood around a given prediction.

Specifically, consider a classifier \(f\) that maps a \(d\)-dimensional input in \(^{d}\) to a binary classification score \(\). Given an input \(x^{d}\) to explain, LIME considers how ablating different features (by setting their value to zero) affects the model's prediction. To do so, LIME learns a surrogate model to predict the original model's classification score given the ablation vector \(\{0,1\}^{d}\) denoting which sources to exclude.

To learn a surrogate model, LIME first collects a dataset of ablated inputs \(x_{i}^{d}\), corresponding ablation masks \(z_{i}\{0,1\}^{d}\) and corresponding model outputs \(f(x_{i})\). It then runs Lasso on the pairs \((z_{i},f(x_{i}))\), yielding a sparse linear surrogate model \(:\{0,1\}^{d}\). A key design choice of LIME is that the surrogate model is _local_. The pairs \((z_{i},f(x_{i}))\) are weighted according to a similarity kernel \(_{x}\) (selected heuristically) to emphasizes pairs that are close to the original input \(x\).

Roughly speaking, if sources from the context are interpreted as features, ContextCite may be viewed as an extension of LIME to the generative setting with a uniform similarity kernel. The uniform similarity kernel leads to a _global_ surrogate model: it approximates the mode behavior for arbitrary ablations, instead of just for ablations where a small number of sources are excluded. We observe empirically that in the context attribution setting, a global surrogate model is often faithful (see Section 3).

**Kernel SHAP .**

Lundberg and Lee  propose SHAP (SHapley Additive exPlanations) to unify methods for additive feature attribution. Additive feature attribution methods assign a weight to each feature in a model's input and explain a model's prediction as the sum of these weights (LIME is an additive feature attribution method). They show that there exists unique additive feature attribution values (which they call SHAP values) that satisfy a certain set of desirable properties; these unique attribution values correspond to the Shapley values  measuring the contribution of each feature to the model output.

To estimate SHAP values, Lundberg and Lee  propose Kernel SHAP, a method that uses LIME with a specific choice of similarity kernel that yields SHAP values. Specifically, in order for LIME to estimate SHAP values, they show that the similarity kernel for an ablation vector \(v\) should be

\[_{}(v)=|v|(d-|v|)}\]

where \(d\) is the number of features and \(|v|\) is the number of non-zero elements of the ablation vector \(v\).

Using the same setup as in Appendix B.3, we compare the Kernel SHAP estimator (which uses Lasso with samples weighted according to \(_{}\)) to the ContextCite estimator (which uses Lasso with a uniform similarity kernel) in Figure 14. We use the implementation of Kernel SHAP from the PyPI package shap. We find that the ContextCite estimator results in a more faithful surrogate model than the Kernel SHAP estimator for context attribution (in terms of top-\(k\) log probability drop for different values of \(k\)).

**Datamodels.** The datamodeling framework  seeks to understand on how individual training examples affect a model's prediction on a given test example, a task called _training data attribution_. Specifically, a datamodel is a surrogate model that predicts a model's prediction on a given test example given a mask specifying which training examples are included or excluded. The surrogate model estimation method used by ContextCite closely matches that of datamodels (the only difference being that ContextCite samples ablation vectors uniformly, while datamodels samples ablation vectors with a fixed ablation rate \(\)).

Figure 14: Comparing the effectiveness of the ContextCite and Kernel SHAP estimators for learning a surrogate model. We report the top-\(k\) log probability drops (see Equation 1) for surrogate models learned using the ContextCite estimator and the Kernel SHAP estimator (using the implementation of Lundberg and Lee ). We find that the ContextCite estimator consistently identifies more impactful sources, and, in particular, when the number of context ablations is small. Error bars denote 95% confidence intervals.

In the in-context learning setting, "training examples" are provided to a model as context before it is queried with a test example. Datamodels have previously been used to study in-context learning [23; 24]. If one thinks of in-context learning as sources, this form of training data attribution is a special case of context attribution.

More broadly, understanding how a model uses unstructured information presented in its context is conceptually different from understanding how a model uses its training examples. Some of the applications of context attribution are analogous to existing applications of training data attribution. For example, selecting query-relevant in-context information based on context attribution (see Section 5.2) is analogous to selecting training examples based on training data attribution . However, other applications, such as helping verify the factuality of generated statements (see Section 5.1) do not have clear data attribution analogies.

### Why does pruning the context improve question answering performance?

In Section 5.2, we show that providing only the top-\(k\) most relevant ContextCite sources for a language model's _original_ answer to a question can improve the quality of its answer. We would like to note that the sources identified by ContextCite are those that were used to generate the original response. If the original response is incorrect, it may be surprising that providing only the sources that led to this response can improve the quality of the response.

To explain why pruning the context does improve question answering performance, we consider two failure modes associated with answering questions using long contexts:

1. The model identifies the wrong sources for the question and answers incorrectly.
2. The model identifies the correct sources for the question but _misinterprets_ information because it is distracted by other irrelevant information in the context.

Intuitively, pruning the context to include only the originally identified sources can help mitigate the second failure mode but not the former. The fact that pruning the context in this way _can_ improve question answering performance suggests that the second failure mode occurs and that mitigating it can thus improve performance.

### Computational efficiency of ContextCite

Most of the computational cost of ContextCite comes from creating the surrogate model's training dataset. Hence, the efficiency of ContextCite depends on how many ablations it requires to learn a faithful surrogate model. We find that ContextCite requires just a small number of context ablations to learn a faithful surrogate model--in our experiments, \(32\) context ablations suffice. Thus, attributing responses using ContextCite is \(32\) more expensive than generating the original response. We note that the inference passes for each of these context ablations can be fully parallelized. Furthermore, because ContextCite is a _post-hoc_ method that can be applied to any existing response, a user could decide when they would like to pay the additional computational cost of ContextCite to obtain attributions. When we use ContextCite to attribute multiple statements in the response, we use the same context ablations and inference calls. In other words, there is a fixed cost to attribute (any part of) a generated response, after which it is very cheap to attribute specific statements.

#### c.3.1 Why do we only need a small number of ablations?

We provide a brief justification for why \(32\) context ablations suffice, even when the context comprises many sources. Since we are solving a linear regression problem, one might expect the number of ablations needed to scale _linearly_ with the number of sources. However; in our sparse linear regression setting, we have full control over the covariates (i.e., the context ablations). In particular, we ablate sources in the context independently and each with probability \(1/2\). This makes the resulting regression problem "well-behaved." Specifically, this lets us leverage a known result (see Theorems 7.16 and 7.20 of Wainwright ) which tells us that we only need \(O(k(d))\) context ablations, where \(d\) is the total number of sources and \(k\) is the number of sources with non-zero relevance to the response. In other words, the number of context ablations we need grows very slowly with the total number of sources. It only grows linearly with the number of sources that the model relies on when generating a particular statement. As we show empirically in Figure 2(a), this number of sources is often small.

### Limitations of ContextCite

In this section, we discuss a few limitations of ContextCite.

**Potential failure modes.** Although we find a _linear_ surrogate model to often be faithful empirically (see Figure 2, Appendix B.2), this may not always be the case. In particular, we hypothesize that the linearity assumption may cease to hold when many sources contain the same information. In this case, a model's response would only be affected by excluding every one of these sources. In practice, to verify the faithfulness of the surrogate model, a user of ContextCite could hold out a few context ablations to evaluate the surrogate model (e.g., by measuring the LDS). They could then assess whether ContextCite attributions should be trusted.

Another potential failure mode of ContextCite is attributing generated statements that follow from previous statements. Consider the generated response: "He was born in 1990. He is 34 years old." with context mentioning a person born in 1990. If we attribute the statement "He was born in 1990." we would likely find the relevant part of the context. However, if we attribute the statement "He is 34 years old." we might not identify any attributed sources, despite this statement being grounded in the context. This is because this statement is conditioned on the previous statement. Thus, in this case there is an "indirect" attribution to the context through a preceding statement that would not be identified by the current implementation of ContextCite.

**Unintuitive behaviors.** A potentially unintuitive behavior of ContextCite is that it can yield a low attribution score even for a source that supports a statement. This is because ContextCite provides contributive attributions. Hence, if a language model already knows a piece of information from pre-training and does not rely on the context, ContextCite would not identify sources. This may lead to unintuitive behaviors for users.

**Validity of context ablations.** In this work, we primarily consider sentences as sources for context attribution and perform context ablations by simply removing these sentences. One potential problem with this type of ablation is _dependencies_ between sentences. For example, consider the sentences: "John lives in Boston. Charlie lives in New York. He sometimes visits San Francisco." In this case, "He" refers to Charlie. However, if we ablate just the sentence about Charlie, "He" will now refer to "John." There may be other ablation methods that more cleanly remove information without changing the meaning of sources because of dependencies.

**Computational efficiency.** As previously discussed, attributing responses using ContextCite is \(32\) more expensive than generating the original response. This may be prohibitively expensive for some applications.

### NeurIPS Paper Checklist

The checklist is designed to encourage best practices for responsible machine learning research, addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove the checklist: **The papers not including the checklist will be desk rejected.** The checklist should follow the references and follow the (optional) supplemental material. The checklist does NOT count towards the page limit.

Please read the checklist guidelines carefully for information on how to answer these questions. For each question in the checklist:

* You should answer [Yes], [No], or [NA].
* [NA] means either that the question is Not Applicable for that particular paper or the relevant information is Not Available.
* Please provide a short (1-2 sentence) justification right after your answer (even for NA).

**The checklist answers are an integral part of your paper submission.** They are visible to the reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it (after eventual revisions) with the final version of your paper, and its final version will be published with the paper.

The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation. While "[Yes] " is generally preferable to "[No] ", it is perfectly acceptable to answer "[No] " provided a proper justification is given (e.g., "error bars are not reported because it would be too computationally expensive" or "we were unable to find the license for the dataset we used"). In general, answering "[No] " or "[NA] " is not grounds for rejection. While the questions are phrased in a binary way, we acknowledge that the true answer is often more nuanced, so please just use your best judgment and write a justification to elaborate. All supporting evidence can appear either in the main paper or the supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification please point to the section(s) where related material for the question can be found.

IMPORTANT, please:

* **Delete this instruction block, but keep the section heading "NeurIPS paper checklist",**
* **Keep the checklist subsection headings, questions/answers and guidelines below.**
* **Do not modify the questions and only use the provided macros for your answers**.

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: See experiments. Guidelines:
* The answer NA means that the abstract and introduction do not include the claims made in the paper.
* The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers.
* The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings.
* It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: See paper.

Guidelines:

* The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper.
* The authors are encouraged to create a separate "Limitations" section in their paper.
* The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be.
* The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated.
* The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon.
* The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size.
* If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness.
* While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [NA] Justification: We do not have theoretical results. Guidelines:

* The answer NA means that the paper does not include theoretical results.
* All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced.
* All assumptions should be clearly stated or referenced in the statement of any theorems.
* The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition.
* Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material.
* Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Yes, see appendices. Guidelines:

* The answer NA means that the paper does not include experiments.
* If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not.
* If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable.

* Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed.
* While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code** Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: Our code is open-source (see abstract). We use publicly available datasets and models. Guidelines:
* The answer NA means that paper does not include experiments requiring code.
* Please see the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* While we encourage the release of code and data, we understand that this might not be possible, so no is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark).
* The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines ([https://nips.cc/public/guides/CodeSubmissionPolicy](https://nips.cc/public/guides/CodeSubmissionPolicy)) for more details.
* The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc.
* The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why.
* At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable).
* Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: Yes, see appendices Guidelines:* The answer NA means that the paper does not include experiments.
* The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them.
* The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: Our empirical findings account for statistical significance. Guidelines:
* The answer NA means that the paper does not include experiments.
* The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper.
* The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).
* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Yes, see appendices. Guidelines:
* The answer NA means that the paper does not include experiments.
* The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage.
* The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute.
* The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics [https://neurips.cc/public/EthicsGuidelines?](https://neurips.cc/public/EthicsGuidelines?) Answer: [Yes] Justification: Guidelines:
* The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.

* If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics.
* The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [Yes] Justification: Yes, see appendices. Guidelines:
* The answer NA means that there is no societal impact of the work performed.
* If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.
* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: We do not release data or models that have a high risk for misuse. Guidelines:
* The answer NA means that the paper poses no such risks.
* Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters.
* Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images.
* We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: Yes, we cite all the datasets and pre-trained models used in the paper.

Guidelines:

* The answer NA means that the paper does not use existing assets.
* The authors should cite the original paper that produced the code package or dataset.
* The authors should state which version of the asset is used and, if possible, include a URL.
* The name of the license (e.g., CC-BY 4.0) should be included for each asset.
* For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided.
* If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.
* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [Yes] Justification: See appendices. Guidelines:
* The answer NA means that the paper does not release new assets.
* Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc.
* The paper should discuss whether and how consent was obtained from people whose asset is used.
* At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: We do not have crowdsourcing experiments or research with human subjects. Guidelines:
* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper.
* According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: We do not require IRB approval for our work. Guidelines:* The answer NA means that the paper does not involve crowdsourcing nor research with human subjects.
* Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper.
* We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution.
* For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.