# Adversarial Training from Mean Field Perspective

Soichiro Kumano

The University of Tokyo

kumano@cvm.t.u-tokyo.ac.jp &Hiroshi Kera

Chiba University

kera@Chiba-u.jp &Toshihiko Yamasaki

The University of Tokyo

yamasaki@cvm.t.u-tokyo.ac.jp

###### Abstract

Although adversarial training is known to be effective against adversarial examples, training dynamics are not well understood. In this study, we present the first theoretical analysis of adversarial training in random deep neural networks without any assumptions on data distributions. We introduce a new theoretical framework based on mean field theory, which addresses the limitations of existing mean field-based approaches. Based on the framework, we derive the (empirically tight) upper bounds of \(_{q}\) norm-based adversarial loss with \(_{p}\) norm-based adversarial examples for various values of \(p\) and \(q\). Moreover, we prove that networks without shortcuts are generally not adversarially trainable and that adversarial training reduces network capacity. We also show that the network width alleviates these issues. Furthermore, the various impacts of input and output dimensions on the upper bounds and time evolution of weight variance are presented.

## 1 Introduction

Adversarial training  is one of the most effective approaches against adversarial examples . Various studies aimed to improve the performance of adversarial training , leading to numerous observations. Adversarial training improves accuracy for adversarial examples but decreases it for clean images . Moreover, it requires additional training data  and achieves high robust accuracy in a training dataset but not in a test dataset . To improve the reliability of adversarial training and address the aforementioned challenges, a theoretical analysis is essential. Specifically, it is crucial to gain insight into network evolution during adversarial training, conditions for adversarial trainability, and differences between adversarial and standard training.

However, the theoretical understanding of network training is challenging, even for standard training, due to the non-convexity of loss surface and optimization stochasticity. Recent studies employed the mean field theory and analyzed the early stage of standard training for randomly initialized deep neural networks (random networks) . Some studies explored network trainability regarding gradient vanishing/explosion  and dynamical isometry . Others examined network representation power . The theoretical results of the early stage of training have been empirically observed to fit well with fully trained networks , partially supported by recent theoretical results . However, existing mean field-based approaches cannot manage the probabilistic properties of an entire network (e.g., the distribution of a network Jacobian) and dependence between network inputs and parameters, which are crucial for analyzing adversarial training.

In this study, we propose a mean field-based framework that addresses the aforementioned limitations (Thm 4.1), and apply it to the adversarial training analysis. While previous studies on adversarialtraining rely on strong assumptions (e.g., Gaussian data [14; 80] and linear classifiers [76; 107]), the proposed framework includes various scenarios (e.g., \(_{p}\) norm-based adversarial examples and deep neural networks with or without shortcuts, i.e., residual or vanilla networks) without any assumptions on data distributions. Our analysis reveals various adversarial training characteristics that have been only experimentally observed or are unknown. The results are summarized as follows.

Upper bounds of adversarial loss.We derive the upper bounds of adversarial loss, quantifying the adverse effect of adversarial examples for various combinations of \(_{q}\)-adversarial loss with the \(_{p}\)-norm \(\)-ball (\(p,q\{1,2,\}\)) (Thm 5.1). Numerical experiments confirm the tightness of these bounds. We also investigate the impacts of input and output dimensions on these bounds, and discover that for the \((p,q)=(2,)\) combination, the bound is independent of these dimensions.

Time evolution of weight variance.We present the time (training step) evolution of weight variance in training (Thms 5.4 and G.9). Weight variance has been used to assess training properties [49; 71; 81]. Our analysis indicates that adversarial training significantly regularizes weights and exhibits consistent weight dynamics across different norm choices.

Vanilla networks are not adversarially trainable under mild conditions.We show that gradient vanishing occurs in vanilla networks (without shortcuts) with large depths and small widths, making them untrainable via adversarial training, even with careful weight initialization (Thm 5.7). This contradicts standard training, where deep vanilla networks can be trained with proper initialization [81; 100]. However, residual networks are adversarially trainable _even without proper initialization_ (Thm 5.8 and Prop G.10), proving the importance of shortcuts for adversarial training.

Degradation of network capacity and role of network width.As adversarial robustness requires high network capacity , deep networks can be used in adversarial training. However, we reveal that network capacity, measured based on the Fisher-Rao norm , sharply degrades in deep networks during adversarial training (Thms 5.9 and G.14). Specifically, we confirm that the capacity at training step \(t\) is \((L-tL^{2}/N)\), where \(L\) and \(N\) denote the network depth and width, respectively. Interestingly, this result contrasts the roles of depth and width, i.e., the depth increases the initial capacity but decays it as training proceeds, whereas the width preserves it. While our theory is validated only during the initial stages of training, our experiments confirm that the adversarial robustness after full training is significantly influenced by network width (cf. Tabs. A5 and A6) as demonstrated in our theorems.

Other contributions.Furthermore, we show the followings cases: (a) Equality of the adversarial loss is obtained instead of inequality (upper bound) under several assumptions (Props G.2 and G.3). (b) Adversarial training leads to faster weight decay and less stable gradients compared with \(_{2}\) weight regularization. (c) Capacity degradation is discussed for metrics other than the Fisher-Rao norm. (d) Adversarial risk cannot be mitigated while maintaining trainability and expressivity. (e) Discussion on ReLU-like activations extends to Lipschitz continuous activations under certain conditions. (f) A single-gradient descent attack can find adversarial examples that flip the prediction of a binary classifier (Prop K.1). Contributions (b)-(f) are found in Appx. K.

Although this study focuses on adversarial training, our theoretical framework can be applied to other training methods that consider the probabilistic property of an entire network and dependence between network inputs and parameters. Consequently, we believe that this study can potentially contribute to the theoretical understanding of adversarial training and various deep learning methods.

## 2 Related work

Here, we summarize the full version in Appx. A. A technical discussion follows in Sec. 5.1.

Mean field theory.Mean field theory in machine learning investigates the training dynamics of random networks in chaotic and ordered phases . Networks can be trained at the boundary between these phases . The theory has been extended to networks with shortcuts [105; 106], recurrent connections [15; 69], and batch normalization . It has been employed to study dynamical isometry [15; 69; 70; 79], and a subsequent study achieved training of 10,000-layer networks without shortcuts . Moreover, the theory has been applied to analyze network representation power [49; 71; 101]. However, existing mean field-based analysis cannot handle the properties of an entire network and input-parameter dependence, which is a drawback for some deep learning methods, e.g., adversarial training. Thus, we propose a new framework to address these limitations.

Adversarial training.Various questions related to adversarial training have been theoretically addressed by some studies, including the robustness-accuracy trade-off [29; 47; 75; 76; 92; 113], generalization gap [6; 50; 102; 107], sample complexity [1; 14; 62; 80; 110], large model requirement , and enhanced transfer learning performance . However, these results are obtained in limited settings (e.g., Gaussian data and linear classifiers) and are not easily extended to deep neural networks or realistic data distributions. To explore more general settings, recent studies used the neural tangent kernel theory [4; 46; 55]. In the kernel regime, adversarial training, even with a heuristic attack, finds a robust network [35; 115]. In our study, we investigate adversarial training dynamics based on a mean field perspective, covering general multilayered networks with or without shortcuts and without assumptions about data distributions.

## 3 Preliminaries

### Setting

Notations are summarized in Tab. 1. For an integer \(n\), let \([n]:=\{1,,n\}\). In this study, we focus on random deep neural networks with ReLU-like activations, called random ReLU-like networks. This is formally defined as follows:

**Definition 3.1** (ReLU-like network).: A network is called a ReLU-like network if all its activation functions are \((z):=uz\) for \(z 0\) and \(vz\) for \(z<0\), with \(u,v\).

ReLU-like activations [33; 57] are widely used in theoretical and practical applications [42; 45; 53; 85; 109]. In Appx. K, we extend our theorems to networks with Lipschitz continuous activations.

A ReLU-like network, \(:^{d}^{K}\), comprises \(L\) trainable layers and two non-trainable layers for adjusting input and output dimensions. The input layer projects \(^{}^{d}\) to an \(N\)-dimensional vector \(^{(0)}^{N}\) using the random matrix \(^{}^{N d}\). Subsequently, \(L\) consecutive affine transformations and activations are applied by \(:^{N}^{N}\). Then, \((^{(0)})\) is multiplied by a random matrix \(^{}^{K N}\) to obtain the output vector \((^{})\). Finally, the network function is provided by \((^{}):=^{}(^{ }^{})\). We assume that \(d\) and \(K\) are sufficiently large, and each entry of \(^{}\) and \(^{}\) is i.i.d. and sampled from Gaussians \((0,1/d)\) and \((0,1/N)\), respectively.

An \(L\)-layer neural network \(\) comprises weights \(^{(l)}=(W^{(l)}_{ij})^{N N}\) and biases \(^{(l)}=(b^{(l)}_{1},,b^{(l)}_{N})^{}^{N}\), where \(l[L]\) denotes the layer index. The network is assumed to possess a sufficiently large width (i.e., \(N\) is sufficiently large). The \(l\)-th pre- and post-activation are defined as \(^{(l)}:=^{(l)}^{(l-1)}+^{(l)}\) and \(^{(l)}:=(^{(l)})\), respectively, where ReLU-like activation \(\) operates entry-wise. The weight \(W^{(l)}_{ij}\) and bias \(b^{(l)}_{i}\) are i.i.d. and sampled from \((0,^{2}_{w}/N)\) and \((0,^{2}_{b})\), respectively. The network function is represented by Eq. (1). For a residual network setting, refer to Appx. C.

\[(^{}):=^{}(^{(L)}((^{(1)}^{}^{}+ {b}^{(1)}))+^{(L)}). \]

### Background

Mean field theory.Mean field theory employs probabilistic methods to analyze the properties of random deep neural networks. It assumes that \(^{(l)}\) follows a Gaussian, justified by the central limit theorem when width \(N\) is sufficiently large . Here, we review the mean field-based approach to analyze the forward and backward dynamics of a network. Let \(:^{d}\) represent the loss function. The mean squared pre-activation \([(h^{(l)}_{i})^{2}]\) and gradient \(^{(l)}:=[((^{})/ x^ {(l)}_{i})^{2}]\), where \(i\) denotes the neuron index, are calculated as follows [71; 81]:

\[[(h^{(l)}_{i})^{2}]=^{2}_{w}[(h^{(l-1)}_{i})^{2} ]+^{2}_{b},^{(l)}=^{2}_{w}[^{} (h^{(l+1)}_{i})^{2}]^{(l+1)}. \]These equations represent the dynamics between adjacent layers. We can infer that gradients vanish when \(_{w}^{}[^{}(h_{i}^{(l+1)})^{2}]<1\) and explode when \(_{w}^{2}[^{}(h_{i}^{(l+1)})^{2}]>1\), indicating that a network is trainable only if \(_{w}^{2}[^{}(h_{i}^{(l+1)})^{2}] 1\).

Adversarial training.We define adversarial loss as follows:

\[_{}(^{}) :=_{\|\|_{p}}\|(^{ }+)-(^{})\|_{q} \] \[=_{\|\|_{p}}\|^{} (^{(L)}((^{(1)}^{}(^{}+)+^{(1)}))+^{(L)})\|_{q}, \]

where \(>0\) and \(p,q\{1,2,\}\). The adversarial loss aims to minimize the difference between the network outputs of natural and adversarial inputs. Networks are trained by minimizing the sum of the standard loss \(_{}:^{d}\) (e.g., cross-entropy loss), where \(\) denotes a label set, and the adversarial loss \(_{}\). The mean field analysis typically assumes gradient independence for loss functions (Appx. B) . We use this assumption for \(_{}\), but not \(_{}\). Although Eq. (3) differs from the standard adversarial loss based on cross-entropy , our definition is employed in more robust methods, e.g., TRADES  and is theoretically simpler to analyze. Therefore, herein, the aforementioned loss is analyzed. However, even this simplified definition (Eq. (3)) poses a challenge for theoretical analysis due to the complex nested structure of a deep neural network (cf. Eq. (4)).

## 4 Theoretical framework

### Limitations of existing mean field-based approaches

We propose a new theoretical framework based on mean field theory to analyze adversarial training. Here, we describe two limitations of existing mean field-based approaches, e.g., Eq. (2).

Layer-wise approach.Existing approaches focus on the dynamics between adjacent layers (cf. Eq. (2)). However, analyzing the adversarial loss (Eq. (3)) requires a framework that handles the probabilistic properties of an entire network instead of adjacent layers. This analysis becomes difficult due to the complex nested structure of networks (cf. Eqs. (1) and (4)). For example, there is no clarity on the the probabilistic behavior of \((^{})\), distribution of \((^{}+)-(^{})\), and dependence between \((^{})\) and inputs. We need a framework that disentangles the nested structure of networks and manages the probabilistic properties of an entire network. Recent studies on the mean field theory studies  have concepts related to ours; the comparative analysis is given in Sec. 5.1.

Difficulty in analyzing input-parameter dependence.The analysis of adversarial training requires consideration of input-parameter dependence since adversarial perturbations are designed based on network parameters (cf. Eq. (3)). However, this cannot be readily addressed using existing approaches because their frameworks (e.g., Eq. (2)) do not offer a clear view of the dependence between perturbation \(\) and network parameters \(W_{1,1}^{(1)}\), \(W_{1,2}^{(1)}\), \(\), and \(W_{N,N}^{(L)}\).

The proposed framework resolves these limitations and provides a simple network representation that allows us to capture the entire network property with clear input-parameter dependence.

### Proposed framework

The current mean field-based approaches cannot capture the probabilistic properties of an entire network due to the complex nested structure of deep neural networks. Moreover, it is difficult to consider the dependence between inputs and numerous number of parameters. To address these limitations, we employ a linear-like representation of a ReLU-like network and propose its probabilistic properties. As ReLU-like networks are piecewise linear, a vanilla ReLU-like network can be represented as:

\[(^{}) =(^{})^{}+( ^{}), \] \[(^{}) :=^{}(^{}(^{(L)}( ^{})))^{(L)}(^{}(^{(L-1 )}(^{})))^{(L-1)}^{(1)}^{}, \]where \((\,\,)\) denotes a diagonal matrix and \((^{ in})\) is defined similar to \((^{ in})\) (cf. Eq. (A48)). For a residual network, Eq. (A73) can be referred. Importantly, this representation does not rely on approximations, e.g., Taylor expansions. As \(^{(l)}\) and \(^{(l)}\) are randomly sampled, \(^{(l)}(^{ in})\) and \(^{(l)}(^{ in})\) denote a random matrix and vector, respectively. Unlike the original network definition (Eq. (1)), this representation (Eq. (5)) is non-nested and focuses only two parameters, thereby simplifying network analysis. Remarkably, we show the following properties of \((^{ in})\) and \((^{ in})\):

**Theorem 4.1** (Properties and distributions of \((^{ in})\) and \((^{ in})\)).: _Suppose that the width \(N\) is sufficiently large. Then, for any \(^{ in}^{d}\), (1) \((^{ in})\) and \((^{ in})\) are independent. (II) each entry of \((^{ in})\) and \((^{ in})\) is i.i.d. and follows the Gaussian below:_

\[J(^{ in})_{ij} 0,}{d}, a(^{ in})_{i} 0,_{b}^{2}_{k=1}^{L}^{k-1}, \]

_where \(:=(u^{2}+v^{2})/2\) (cf. Defn 3.1) and \(\) is \(_{v}:=_{w}^{2}\) for vanilla networks and \(_{r}:=1+_{w}^{2}\) for residual networks._

A significance of this theorem lies in that **despite being functions of \(^{ in}\), the distributions of \((^{ in})\) and \((^{ in})\) do not depend on \(^{ in}\).1 In other words, although \((^{ in})\) and \((^{ in})\) are determined by (a fixed) \(^{ in}\) for an initialized network, they become different for each sampling of weights and biases, and the selection of these values is independent of \(^{ in}\). Besides, \((^{ in})\) and \((^{ in})\) are independent and their distributions are Gaussian, which exhibits convenient properties. A sketch of proof is given in Appx. D and formal one is in Appxs. E and F.

To validate Thm 4.1, we conducted a numerical experiment and present the results in Fig. 1. We randomly sampled 10,000 vanilla ReLU networks and computed \(J(^{ in})_{1,1}\) for each network using the identical input \(^{ in}\). Additional experimental results can be found in Appx. L.

Broader applicability.The proposed framework (Thm 4.1) manages an entire network using only two Gaussians. While we focus on adversarial training, Thm 4.1 can be valuable for other deep neural network analyses based on the mean field theory. For example, contrastive learning  can be investigated, as it aims to minimize the distance between original and positive samples while maximizing it for negative samples. In this context, instead of considering the adversarial loss, \(\|(^{ in}+)-(^{ in})\|\), we can analyze loss functions, e.g., \(\|(^{ in}_{ pos})-(^{ in}_{ ori})\|\) and \(\|(^{ in}_{ neg})-(^{ in}_{ ori})\|\), where \(^{ in}_{ ori}\), \(^{ in}_{ pos}\), and \(^{ in}_{ neg}\) represent the original, positive, and negative samples, respectively. The complex nested structure of a network makes it challenging to consider the difference between two network outputs; however, Thm 4.1 helps theoretically manageable analyses.

## 5 Analysis of adversarial training

The proof of each theorem is described in Appx. G.

Figure 1: Distribution of \(J(^{ in})_{1,1}\) in the vanilla ReLU network with \(d=1,000\), \(K=1\), \(N=5,000\), \(L=10\), \(_{w}^{2}=2\), and \(_{b}^{2}=0.01\). The blue histogram represents the experimental results (10,000-time samplings), and the orange curve is predicted by Thm 4.1.

### Upper bounds of adversarial loss

As the ReLU-like network \(\) is locally linear and its input Jacobian at \(^{}\) is \((^{})\) (cf. Eq. (5)), we can consider a more tractable form of the adversarial loss instead of Eq. (4) as follows:

\[_{}(^{})_{ ^{d},()_{q}} ()_{q}=_{ ^{d}}()_{p,q}, \]

where \(()_{p,q}:=_{ _{p}=1}()_{q}\) denotes the \((p,q)\)-operator norm of \(()\). Using Thm 4.1, which describes the property of \(()\), we transform Ineq. (8) and obtain the following:

**Theorem 5.1** (Upper bounds of adversarial loss).: _Suppose that the input dimension \(d\), output dimension \(K\), and width \(N\) are sufficiently large. Then, for any \(^{}^{d}\), the following inequality holds:_

\[_{}(^{})_{p,q }^{L/2}=_{p,q}(_{W }W^{2})^{L/2}&()\\ _{p,q}(1+_{W}W^{2})^{L/2}&( ), \]

_where \(:=\{W^{(1)}_{1,1},W^{(1)}_{1,2},,W^{(L)}_{N,N}\}\) denotes the set of all network weights. The constant \(_{p,q}\) for each norm pair \((p,q)\) is described in Tab. 1._

For some choices of \((p,q)\), we cannot derive upper bounds, and thus, Tab. 1 contains blank. Numerical experiments show the tightness of the bounds (cf. Fig. 2). The theorem indicates that (i) the bounds increase linearly with the perturbation budget \(\), (ii) the effects of the input and output dimensions depend on the norms \((p,q)\) (cf. Tab. 1), and (iii) network depth \(L\) exponentially impacts the bounds, with \(=1\) as a threshold between order and chaos. Further, the square sum of the weights in Ineq. (9) suggests that adversarial training exhibits a weight regularization effect, which is compared to \(_{2}\) weight regularization in Appx. I. Besides, we derive equality rather than inequality (upper bound) under specific assumptions (e.g., small \(\)) for some \((p,q)\) in Appx. K, indicated by \(\) and \(\) in Tab. 1.

The input and output dimensions, \(d\) and \(K\), influence the bounds through the \((p,q)\)-dependent parameter \(_{p,q}\). In Tab. 1, \(_{p,q}\) displays a wide range of dependencies on \(d\) and \(K\). When \(d\) and \(q=\), \(d\) significantly affects the two phases of the adversarial loss, where \(p=2\)**marks the transition point from order (\(p=1\)) to chaos (\(p=\))**. In contrast, under realistic assumptions with \(d K\), \(K\) affects negligibly. Interestingly, **the dimensions do not impact the upper bounds when \((p,q)=(2,)\)**. In practice, we scale the perturbation budget and adversarial loss according to the choice of \((p,q)\), respectively, and discuss the scaling effects in Appx. K.

Comparison with other studies.We can consider studies on global Lipschitz of networks in certified adversarial defenses  and spectral regularization  to analyze Ineq. (8). A key difference is that the proposed probabilistic approach contradicts their deterministic approach. By imposing probabilistic constraints on network parameters, we can obtain exponentially tighter, interpretable, and more theoretically manageable bounds, which facilitates the subsequent section's discussion. The mathematical comparison is described in Appx. H.

Results obtained from  can be used to analyze the Jacobian's singular value distribution. Compared to their approaches, which are limited to \((p,q)=(2,2)\), the proposed method offers greater generality and flexibility, providing upper bounds for various \((p,q)\). Moreover, Thm 4.1 enables the derivation of equality instead of inequality (upper bound), Props G.2 and G.3, which is not achievable using the approaches mentioned in the aforementioned studies because it cannot incorporate perturbation-Jacobian dependence. Moreover, we do not consider their assumption that the variance \([h^{(l)}_{i}]\) is constant for all \(l[L]\), which is often difficult to satisfy.

Further, we refer to , which established a theoretical link between adversarial training and the \((p,q)\)-operator norm of a Jacobian. Their findings support Ineq. (8) in training scenarios using heuristic attacks, e.g., projected gradient descent . In this study, we derive concrete upper bounds beyond their theoretical link, enabling further investigation of adversarial training properties.

### Time evolution of weight variance

Weight variance plays a critical role in determining deep neural network properties . We substitute the adversarial loss definition (Eq. (3)) with \(_{}:=_{p,q}(t)^{L/2}\), where \(t 0\) denotes thecontinuous training step. Considering gradient descent with an infinitely small learning rate (gradient flow), the model parameter \((t)\) at step \(t\) is updated as:

\[(t)}{t}:=-_{}}{(t)}-_{}}{ (t)}. \]

We make the following assumption.

**Assumption 5.2**.: For \(0 t T N\), model parameters are independent, and weight and bias follow Gaussian \((0,_{w}^{2}(t)/N)\) and \((0,_{b}^{2}(t))\), respectively.

This assumption ensures that the properties of the model parameters remain close to their initial values during the early stages of training (\(t T\)). Under Asm 5.2, the original and proposed mean field theories (Thm 4.1) remain valid during training. For a moderately small value of \(T\), Asm 5.2 is not strong because the model parameters change minimally and retain their initialized states with sufficiently small learning rates. Recent neural tangent kernel studies partially supported this assumption , and it is known that random network theories align well with fully trained networks . For example, \(T=160\) is reasonable in a specific training setting (cf. Fig. 4).

Now, we summarize other assumptions for reference as follows:

**Assumption 5.3**.: The input dimension \(d\), output dimension \(K\), and width \(N\) are sufficiently large. We apply Asm B.1 to the standard loss function \(_{}\). The adversarial loss is defined as \(_{}:=_{p,q}(t)^{L/2}\) (cf. Thm 5.1).

Based on the aforementioned settings, we obtain the time evolution of the weight variance.

**Theorem 5.4** (Weight time evolution of vanilla network in adversarial training).: _Suppose that Asms 5.2 and 5.3 hold. Then, the time evolution of \(_{w}^{2}\) of a vanilla network in adversarial training is given by:_

\[_{w}^{2}(t)=1- _{}(0)^{L/2-1}}{N}t_{w}^{2}(0). \]

A similar result is obtained for residual networks (Thm G.9). The theorem reveals that the weight variance linearly decreases with \(t\), which can be attributed to the weight regularization effect of adversarial training (cf. Sec. 5.1). In addition, the norm pair \((p,q)\) affect only time-invariant constant \(_{p,q}\), and the dynamics of the weight variance can be represented as a consistent function of \(t\) regardless of \((p,q)\). In other words, **adversarial training exhibits consistent weight dynamics irrespective of norm selection**, with a scale factor varying.

### Vanilla networks are not adversarially trainable under mild conditions

We show that in adversarial training, vanilla networks can fit a training dataset in limited cases (small depth and large width), but residual networks can in most cases. This result suggests that residual networks are better suited for adversarial training. First, we present the trainability condition based on the concept in .

**Definition 5.5** (\((M,m)\)-trainability condition).: A network is said to be \((M,m)\)-trainable if a network satisfies \(m^{(0)}/^{(L)} M\), where \(0 m 1\) and \(M 1\).2

The value \(^{(l)}\) denotes the squared length of the gradient in the \(l\)-th layer. A near-zero \(^{(0)}/^{(L)}\) suggests gradient vanishing, while a large value implies gradient explosion. Hence, Defn 5.5 is directly linked to successful training. In contrast to the existing definition, \(^{(l-1)}/^{(l)} 1\), Defn 5.5 incorporates \(M\) and \(m\) for the subsequent discussion. Then, we establish specific \((M,m)\)-trainability conditions for ReLU-like networks.

**Lemma 5.6** (Vanilla and residual \((M,m)\)-trainability condition).: _Suppose that the width \(N\) is sufficiently large. Then, the \((M,m)\)-trainability conditions for vanilla and residual networks are respectively given by:_

\[m^{1/L}_{w}^{2} M^{1/L}\,(), _{w}^{2} M^{1/L}-1\,(). \]Using the weight time evolution (Thm 5.4) and \((M,m)\)-trainability condition (Lemma 5.6), the following theorem can be readily derived.

**Theorem 5.7** (Vanilla networks are not adversarially trainable).: _Consider a vanilla network. Suppose that Asms 5.2 and 5.3 hold, and the \((M,m)\)-trainability condition holds at \(t=0\) and \(_{w}^{2}(0)=1\). If_

\[T)N}{_{p,q}}, \]

_then there exists \(0< T\) such that the \((M,m)\)-trainability condition does not hold for \( t T\)._

This indicates **the potential untrainability of vanilla networks in adversarial training, even when satisfying the \((M,m)\)-trainability condition at initialization, contradicting with standard training where extremely deep networks can be trained if initialized properly **. This issue arises from the inconsistency between the trainability condition of a vanilla network, i.e., \(m^{1/L}_{w}^{2}\) (cf. Lemma 5.6) and monotonically decreasing nature of \(_{w}^{2}(t)\) in adversarial training (cf. Thm 5.4). As stated in Ass 5.2, we assume that \(T\) is small. Therefore, if the right-hand term of Eq. (13) becomes large, the assumption and Thm 5.7 are violated. In summary, **for large \(L\) (many layers) and \(\) (large perturbation constraint), vanilla networks are not adversarially trainable**. Moreover, **a large \(N\) (a wide network) can mitigate this issue in vanilla networks**. For example, the vanilla network with \(L=20\), \(N=256\), and \(=0.3\) is not adversarially trainable; however, when the width is increased to \(512\), it becomes trainable (cf. Fig. 4). In contrast, we can claim the following for residual networks:

**Theorem 5.8** (Residual networks are adversarially trainable).: _Consider a residual network. Suppose that Asms 5.2 and 5.3 hold, and the \((M,m)\)-trainability condition holds at \(t=0\) and \(_{w}^{2}(0) 1\). Then, \((M,m)\)-trainability condition always holds for \(0 t T\)._

This occurs due to the \((M,m)\)-trainability condition for residual networks, which does not have any lower bound (cf. Lemma 5.6), and the monotonically decreasing nature of \(_{w}^{2}\) (cf. Thm G.9). Besides, residual networks are adversarially trainable even without careful weight initialization (Prop G.10). These theorems highlight the robust stability of adversarial training in residual networks.

### Degradation of network capacity

We demonstrate that adversarial training degrades network capacity. We use the Fisher-Rao norm as a metric , with alternative metrics discussed in Appx. K. The Fisher-Rao norm is defined as:

\[\|\|_{}:=^{}(^{ }),(^{}):=_{i=1}^{K} (^{})}{}^{ }(^{})}{}, \]

where \(:=(W_{11}^{(1)},W_{12}^{(1)},,W_{NN}^{(L)})^{}\) represents the vector of all the network weights and \(\) denotes the empirical Fisher information matrix when only one training data point is considered (for simplicity). The Fisher-Rao norm is preferred over other norm-based capacity metrics  owing to its invariance to node-wise rescaling . We present the following theorem based on :

**Theorem 5.9** (Adversarial training degrades network capacity).: _Consider a vanilla network. Suppose that Asms 5.2 and 5.3 hold, Asm B.1 is applied to the network output, and the \((M,m)\)-trainability condition holds at \(t=0\) and \(_{w}^{2}(0)=1\). Assume \(\|^{}\|_{2}=\) and \(_{b}^{2}(t)=0\). Then, the expectation of the Fisher-Rao norm is given by:_

\[[\|(t)\|_{}]=LK1- L}{N}t. \]

A related result for residual networks is presented in Thm G.14. As described in , adversarial robustness necessitates high capacity. However, Thm 5.9 indicated that capacity decreases linearly with step \(t\) in adversarial training. To address this conflict, large depth (large \(L\)), which increase the initial capacity with \((L)\), can be considered, but this scenario accelerates the degradation speed with \((L^{2})\). To preserve capacity, we must increase the width \(N\) accordingly. Consequently, **to achieve high capacity in adversarial training, it is necessary to increase not only the number of layers \(L\) but also the width \(N\) to keep \(L^{2}/N\) constant**. Although Thms 5.9 and G.14 have been established in the early stages of training, numerical experiments proved that the adversarial robustness following full training is significantly influenced by network width (cf. Tabs. A5 and A6).

## 6 Experimental results

We validate Thms 5.1, 5.4 and 5.7 via numerical experiments. The vanilla ReLU networks were initialized with \(_{w}^{2}=2\) and \(_{b}^{2}=0.01\) to meet the \((M,m)\)-trainability conditions (Lemma 5.6). To verify Thms 5.4 and 5.7, we used MNIST . Setup details and more results are given in Appx. L.

**Verification of Thm 5.1.** We created adversarial examples for initialized networks and computed the adversarial loss (Eq. (3)). As shown in Fig. 2, the upper bounds in Thm 5.1 were considerably tight. Some samples slightly exceeded the upper bounds because we used the finite network width while the infinite width is assumed (cf. Appx. L).

**Verification of Thm 5.4.** We trained vanilla networks normally (with and without \(_{2}\) regularization) and adversarially. The time evolution of weight variance is shown in Fig. 3. Adversarial training significantly reduces weight variance, whereas wide width (i.e., large \(N\)) suppresses it. The validity of Thm 5.4, which forms the basis of our theorems such as Thms 5.7 and 5.9, supports our theorems.

**Verification of Thm 5.7.** We trained vanilla networks considering various depth and width settings and monitored the training accuracy. As shown in Fig. 4, it was difficult for vanilla networks to fit the training dataset when the depth was large and the width was small; increased width helps in fitting. Although we currently lack a theoretical prediction of the boundary between trainable and untrainable areas determined based on Eq. (13), empirical evidence suggests that \(T=160\) is relevant.

## 7 Limitations

The mean field theory offers valuable insight into network training. However, its applicability is restricted to the initial stages of training. Although recent studies suggested empirically [25; 84] and theoretically  that the analysis of early-stage training extends well to full training, the strict relationship is yet to be explored. Our results have the same limitations. Although some theorems accurately capture the behavior during the initial stages of training and even after full training (cf. Tabs. A5 and A6), as training progresses, some theorems begin to diverge from the actual behavior (cf. Fig. A17). Another caveat is that the mean field theory assumes infinite network width, which is practically infeasible. Empirically, our theorems hold well when the width approximately exceeds 1,000 (cf. Fig. A7), while Thm 5.1 requires larger width approximately exceeds 10,000 for \((p,q)=(2,2)\) (cf. Fig. A14). These limitations also derive from the mean field theory and are not unique to our study. Despite these limitations, we consider that this study provides a powerful theoretical framework that extends the applicability of the mean field theory to various training methods, and the results obtained for adversarial training are insightful.

## 8 Conclusions

We proposed a framework based on the mean field theory and conducted a theoretical analysis of adversarial training. The proposed framework addressed the limitations of existing mean field-based approaches, which could not handle the probabilistic properties of an entire network and dependence between network inputs and parameters [71; 81]. Based on this framework, we examined adversarial training from various perspectives, unveiling upper bounds of adversarial loss, relationships between adversarial loss and network input/output dimensions, the time evolution of weight variance, trainability conditions, and the degradation of network capacity. The theorems of this study were validated via numerical experiments. The proposed theoretical framework is highly versatile and can help analyze various training methods, e.g., contrastive learning.