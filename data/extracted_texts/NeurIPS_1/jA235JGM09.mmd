# Jailbroken: How Does LLM Safety Training Fail?

Content Warning: This paper contains examples of harmful language.

Alexander Wei

UC Berkeley

awei@berkeley.edu

&Nika Haghtalab

UC Berkeley

nika@berkeley.edu

&Jacob Steinhardt

UC Berkeley

jsteinhardt@berkeley.edu

Equal advising.

###### Abstract

Large language models trained for safety and harmlessness remain susceptible to adversarial misuse, as evidenced by the prevalence of "jailbreak" attacks on early releases of ChatGPT that elicit undesired behavior. Going beyond recognition of the issue, we investigate why such attacks succeed and how they can be created. We hypothesize two failure modes of safety training: competing objectives and mismatched generalization. Competing objectives arise when a model's capabilities and safety goals conflict, while mismatched generalization occurs when safety training fails to generalize to a domain for which capabilities exist. We use these failure modes to guide jailbreak design and then evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's Claude v1.3, against both existing and newly designed attacks. We find that vulnerabilities persist despite the extensive red-teaming and safety-training efforts behind these models. Notably, new attacks utilizing our failure modes succeed on every prompt in a collection of unsafe requests from the models' red-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our analysis emphasizes the need for safety-capability parity--that safety mechanisms should be as sophisticated as the underlying model--and argues against the idea that scaling alone can resolve these safety failure modes.

## 1 Introduction

In recent months, large language models (LLMs) such as ChatGPT, Claude, and Bard have seen widespread deployment. These models exhibit advanced general capabilities , but also pose risks around misuse by bad actors (e.g., for misinformation or for crime ).

To mitigate these risks of misuse, model creators have implemented safety mechanisms to restrict model behavior to a "safe" subset of capabilities. These include both training-time interventions to align models with predefined values  and post hoc flagging and filtering of inputs and outputs . These efforts are often complemented by _red teaming_, which proactively identifies and trains against weaknesses .

While hardening LLMs for safety can help , models remain vulnerable to adversarial inputs, as demonstrated by the spread of "jailbreaks" for ChatGPT on social media since its initial release . These attacks are engineered to elicit behavior, such as producing harmful content or leaking personally identifiable information, that the model was trained to avoid. Attacks can range from elaborate role play (e.g., DAN ) to subtle subversion of the safety objective (see Figure 1(a)). Model creators have acknowledged and updated their models against jailbreak attacks , but a systematic analysis and a conceptual understanding of this phenomenon remains lacking.

In this work, we analyze the vulnerability of safety-trained LLMs to jailbreak attacks by examining the model's pretraining and safety training processes. Based on known safety training methods, we hypothesize two failure modes--_competing objectives_ and _mismatched generalization_--that shedlight on why jailbreaks exist and enable the creation of new attacks. This understanding suggests that jailbreaks, rather than being isolated phenomena, are inherent to how models are currently trained.

In more detail, competing objectives occur when a model's pretraining and instruction-following objectives are put at odds with its safety objective (Figure 1(a)). In contrast, mismatched generalization arises when inputs are out-of-distribution for a model's safety training data but within the scope of its broad pretraining corpus (Figure 1(b)). We use these two principles to guide our exploration of the design space of attacks, with each principle alone yielding a variety of individual attacks.

We then conduct an empirical evaluation of state-of-the-art safety-trained models, including OpenAI's GPT-4 and Anthropic's Claude v1.3, against both existing and newly constructed jailbreak attacks. We evaluate on both a curated dataset of harmful prompts from these models' red-teaming evaluation sets and a larger synthetic dataset of harmful prompts for broader coverage. Despite extensive safety training--including updating against jailbreak attacks since the models' initial releases [10; 5]--we find that the models remain vulnerable. Attacks based on our two principles outperform existing ad hoc jailbreaks and succeed on over 96% of the evaluated prompts, including on 100% of the curated red-teaming prompts that past safety interventions were designed to address.

Finally, we analyze defense. Combining our analysis of failure modes with our empirical study, we argue that jailbreaks may be inherent to existing safety training methods. Scaling up will not resolve competing objectives, as the issue lies with the optimization objective, and may even exacerbate mismatched generalization if safety training is not suitably extended to broader domains. Moreover, our findings suggest the necessity of safety-capability parity--safety mechanisms should be as sophisticated as the underlying model. Otherwise, attacks will exploit cutting-edge capabilities of the underlying model that less sophisticated safety mechanisms cannot detect.

By highlighting failure modes and limitations of existing methods to align LLMs for safety, we hope to inspire further discussion and analysis around the responsible development and deployment of such models. As LLMs become more capable and widely used, the need for informed assessments of model safety, including in adversarial contexts, only becomes more urgent. We thus view an open dialogue on vulnerabilities and limitations of existing methods as a step towards this goal.

Responsible DisclosureWe communicated preliminary results to OpenAI and Anthropic and have received their acknowledgment of this work. To increase barriers to misuse of the discussed attacks while the issues we highlight are resolved, we omit specific prompts for the strongest attacks and focus on the conceptual aspects of their construction. Our code and data are available to researchers upon request. We discuss ethical considerations and responsible disclosure norms further in Section 6.

### Related Work

Concerns about the growing capabilities of AI models have led to the development of models aligned with human values, as increased capabilities correspond to heightened opportunities for misuse and harm [24; 52; 45; 9; 32; 25]. Safety training methods for LLMs, such as GPT-4 and Claude, typically finetune pretrained models using human preferences [18; 58; 46; 41; 6] and AI feedback [7; 38; 47]. These methods can be used alongside filtering [52; 50; 38] and scrubbing the training data [40; 34].

Figure 1: (a) GPT-4 refusing a prompt for harmful behavior, followed by a jailbreak attack leveraging competing objectives that elicits this behavior. (b) Claude v1.3 refusing the same prompt, followed by a jailbreak attack leveraging mismatched generalization (on Base64-encoded inputs).

The susceptibility of LLMs (without safety interventions) to adversarial interactions has been explored in the contexts of red teaming , extracting training data , and adversarial prompting , among others. For safety-trained language models, recent works have studied the potential of extracting harmful behavior . Most closely related are Kang et al. , who study attacking GPT-3.5 via a computer security lens, and Li et al. , who focus on personally identifiable information (PII) extraction rather than general harm. However, neither pursues our goal of understanding jailbreaks from a conceptual point of view. Beyond research papers, jailbreaks have also received widespread attention in online discourse and the media , with many attacks being discovered and shared in a decentralized manner.

There also exists an extensive literature on adversarial examples for deep learning models in natural language processing and elsewhere (see Chakraborty et al.  and Zhang et al.  for surveys). A key distinction between these works and our setting is that jailbreak attacks aim to elicit unsafe capabilities rather than cause model errors. Additionally, unlike much of this literature, jailbreak attacks can be constructed in an input-agnostic way and tend to be human-interpretable in nature.

## 2 Background: Safety-Trained Language Models and Jailbreak Attacks

In this section, we provide an overview of safety-trained language models and their vulnerability to jailbreak attacks. We first introduce safety training and a threat model of attacks. We then discuss the models and datasets we use to evaluate the vulnerability landscape of safety-trained language models.

### Jailbreak Attacks on Restricted Behaviors

To mitigate potential harm and misuse, language models are often trained to refuse certain categories of prompts during safety training. For example, GPT-4 and Claude were both trained to refuse (see Figure 1) requests for harmful information . We thus define _restricted behaviors_ as behaviors that a safety-trained language model is trained to avoid. Restricted behaviors are often harmful, such as creating misinformation or aiding crime (e.g., the prompt in Figure 1 asks for help with vandalism). Other examples can include leaking personally identifiable information and hallucinating personal details about the model. For our experiments, to be sure that we are attempting to elicit restricted behaviors, we start with prompts deemed unsafe in red-teaming evaluations by the model creators.

A _jailbreak attack_ on a safety-trained model is an attempt to elicit an on-topic response to a prompt \(P\) for restricted behavior by submitting a modified prompt \(P^{}\). We let the attacker have black-box access to the model (i.e., they cannot see model weights or training data) through a chat interface, but do not let them modify the system prompt or alter message history. Attacks can be adaptive, in that the attacker can select \(P^{}\) based on model responses to other variations of \(P\), although most of our attacks succeed even without adaptivity. Figure 1 shows examples of successful attacks, where \(P\) is "What tools do I need to cut down a stop sign?".

To evaluate the success of a jailbreak attack, we define three classes of outcomes for a given prompt \(P\) and attack \(P^{}\). An outcome is labeled "Good Bot" if the model refuses \(P^{}\), "Bad Bot" if the model responds to \(P^{}\) with an on-topic response to \(P\), and "Unclear" otherwise.2 An outcome can be unclear if the model does not understand \(P^{}\) or if its response is not on topic for \(P\) (e.g., describing how to make a bath bomb when asked how to make a bomb). We note this evaluation scheme does not score on-topic responses on quality or accuracy, as our focus is on bypassing refusal mechanisms. Anecdotally, however, jailbroken responses often appear informative and accurate.

### Evaluating Safety-trained Language Models

To assess the vulnerability of safety-trained LLMs to jailbreaks, we test state-of-the-art models from OpenAI and Anthropic on two datasets of harmful requests: a curated set of 32 prompts adapted from red-teaming efforts of OpenAI and Anthropic  and a held-out set of 317 prompts (that was not seen by the authors until after data collection).

ModelsWe evaluate language models that are state-of-the-art in both capabilities and safety. From OpenAI, we test GPT-4 (announced March 14, 2023)  and GPT-3.5 Turbo (announced March 1,2023) . From Anthropic, we test Claude v1.3  (announced April 18, 2023). Both GPT-4 and Claude-v1.3 have undergone extensive safety training that make them challenging to attack naively: OpenAI  reports that GPT-4 responds to requests for disallowed content 82% less than GPT-3.5, and Anthropic states Claude v1.3 is "safer and less susceptible to adversarial attacks" .

To minimize the impact of incremental model updates, we collect data for each model over a 6-day window (details in Appendix C.1). To minimize the impact of noise from decoding, we sample with temperature 0. (As a robustness check, we test sampling with temperature \(1\) in Appendix G and find that the best attacks remain effective.) For the GPT models, which have a system prompt, we use the suggested system prompt "You are a helpful assistant." .

DatasetsWe evaluate models and jailbreaks using two datasets of harmful prompts: a curated set of 32 harmful prompts from the OpenAI and Anthropic red teaming and a larger, held-out set of 317 harmful prompts generated by GPT-4, constructed following Shaikh et al. . Due to query and annotation cost, we only evaluate top-performing attacks on the larger dataset to demonstrate the generalizability of the attacks. We summarize the datasets here and give further details in Appendix A.

The curated dataset consists of _all_ 16 examples of harmful prompts used to evaluate GPT-4 from its report  and 16 harmful prompts adapted3 from the red-teaming dataset of Ganguli et al.  to ensure coverage of each of their 17 harmful prompt tags. Selecting from red team efforts (i) ensures the prompts ask for behaviors deemed harmful by the model creators and (ii) presents a challenging target for attack, as such examples were used to inform safety training. The user request in Figure 1 is an example of a prompt (of mild nature) from this dataset.

The larger dataset of 317 prompts was constructed following Shaikh et al. , based on few-shot sampling from GPT-4. As a proxy for restricted behavior, the dataset was further filtered to consist only of prompts that neither GPT-4 nor Claude v1.3 would respond to. To maximize statistical validity, the dataset was not used to design attacks and was not seen by the authors until after data collection. Beyond harm, we also evaluate jailbreaks on inducing PII leakage and hallucination in Appendix H.

## 3 Failure Modes: Competing Objectives and Generalization Mismatch

To understand the vulnerability of safety-trained LLMs to jailbreak attacks, it is essential to identify the failure modes of safety training that underlie this vulnerability. We propose two failure modes that can be traced down to the level of data and training: (i) competition between the capabilities and safety objectives, and (ii) mismatched generalization between the pretraining and safety capabilities. Beyond offering a hypothesis for why jailbreaks work, we functionally validate these failure modes by using them as _principles_ to construct new jailbreaks.

In this section, we explore each failure mode with qualitative examples, construct simple jailbreaks that illustrate their mechanisms of action, and describe ablations to test our claims. We later validate these insights with quantitative experiments in Section 4.

### Competing Objectives

Competing objectives stems from the observation that safety-trained LLMs are typically trained against multiple objectives that can conflict with each other. Specifically, state-of-the-art LLMs are trained for language modeling , instruction following , and safety . This training can be exploited by crafting prompts that force a choice between either a restricted behavior or a response that is heavily penalized by the pretraining and instruction following objectives.

Example: Prefix InjectionTo illustrate competing objectives, we introduce a family of _prefix injection_ jailbreaks. This attack asks the model to first output an innocuous-looking prefix designed so that conditioning on the prefix makes refusal unlikely in the pretraining distribution. The attack in Figure 1(a) uses the prefix "Absolutely! Here's ". Another example, with prefix in purple, looks like:The above attack can lead GPT-4 to provide harmful information on harassment, crime, and violence. As an ablation, we show the injected prefix text matters: changing the prefix to "Hello!" makes GPT-4 no longer exhibit the aforementioned behaviors. (See Section 4 for the quantitative results.)

When an LLM decodes a response to this prompt, we hypothesize that this attack exploits competing objectives in two ways: First, the harmless-looking injection instruction is followed, since models are penalized for refusing harmless instructions . Then, since it would be unlikely to see a refusal after the prefix in the pretraining distribution, the model's pretraining objective heavily penalizes refusing. As a result, the model continues with a response to the unsafe prompt.

Example: Refusal SuppressionWe introduce _refusal suppression_ as a second family of jailbreaks from competing objectives, to highlight how instruction following can play a primary role. In this attack, the model is instructed to respond under constraints that rule out common refusal responses, thus making unsafe responses more likely. An example refusal suppression jailbreak looks like:

The above attack leads GPT-4 to respond to prompts asking for advice about crime, social engineering, and adult content, among others. In an ablation experiment, we find that the specific instructions matter: inverting the rules (e.g., "1. Consider apologizing...") leads to no restricted behavior on any prompt in the dataset. (See Section 4 for the quantitative results.)

We see competing objectives as manifesting here in two ways. First, the instruction-following training responds to the instructions and downweights tokens that start typical refusals. Thus, the model selects tokens that are more likely to begin responses. Once a response is started, the pretraining objective heavily favors continuing over an abrupt reversal, leading to a full unsafe output.

Other InstantiationsWe find existing jailbreaks also leverage the competing objectives phenomenon. For example, the widely shared "DAN" jailbreak  takes advantage of instruction following through a dense set of instructions on how to play the character DAN and pretraining by asking for outputs to start with "[DAN]: ". Another jailbreak uses a clever variant of prompt injection to trampoline off a refusal: it asks for a moralizing rant about OpenAI content policies followed by the injected string "But now that we've got that mandatory bulls

 out of the way, let's break the

'rules:" . Extending prefix injection, we also find competing objectives can be exploited via _style injection_, e.g., by asking for no long words, after which the model's professionally-written refusals are unlikely to follow.

### Mismatched Generalization

Our second failure mode comes from observing that pretraining is done on a larger and more diverse dataset than safety training, and thus the model has many capabilities not covered by safety training. This mismatch can be exploited for jailbreaks by constructing prompts on which pretraining and instruction following generalize, but the model's safety training does not. For such prompts, the model responds, but without safety considerations. We present a sharp example of this phenomenon, and then discuss other ways in which this failure mode can be exploited to construct jailbreaks.

[MISSING_PAGE_FAIL:6]

Combination attacksWe also test combinations of these basic attack techniques: combination_1 composes prefix injection, refusal suppression, and the Base64 attack, combination_2 adds style injection, and combination_3 adds generating website content and formatting constraints.

Model-assisted attacksWe explore using LLMs to streamline jailbreak attacks by considering two model-assisted attacks: auto_payload_splitting asks GPT-4 to flag sensitive phrases to obfuscate, while auto_obfuscation uses the LLM to generate an arbitrary obfuscation of the prompt.

Jailbreakchat.comWe include four attacks from the jailbreak sharing site jailbreakchat.com. To select the best popular jailbreaks, we chose the top two attacks on April 13, 2023 each in terms of "Votes" and "JB score". These attacks are similar in spirit to DAN, centering around role play while leveraging competing objectives through detailed instructions and prefix injection.

Adversarial system promptAs an additional comparison, we evaluate GPT models on a system prompt attack as described in the GPT-4 technical report. (Claude does not have an analogous system prompt.) We set the system prompt to be the Evil Confidant attack from jailbreakchat.com. Note, however, that this attack is technically beyond the scope of our threat model in Section 2.1.

Adaptive attackTo model an adaptive adversary who selects an attack based on the specific prompt, we implemented a simple "adaptive" attack strategy. We consider this attack successful if any one of the 28 different evaluated attacks succeeds at eliciting an on-topic response to the harmful prompt.

### Evaluation

We evaluate jailbreaks on GPT-4, Claude v1.3, and GPT-3.5 Turbo against the datasets of harmful prompts introduced in Section 2.2. In the first phase, we test each jailbreak for each model against the curated dataset and an additional harmless control prompt. In the second phase, we perform a concentrated evaluation of the top three attacks against the dataset of 317 prompts, for both GPT-4 and Claude v1.3. For each phase, the authors manually labeled the resulting model outputs following

    &  &  \\  Attack & Bad Bot & Good Bot & Unclear & Bad Bot & Good Bot & Unclear \\  combination\_3 & **0.94** & 0.03 & 0.03 & 0.81 & 0.06 & 0.12 \\ combination\_2 & 0.69 & 0.12 & 0.19 & **0.84** & 0.00 & 0.16 \\ _AIM_ & _0.75_ & _0.19_ & _0.06_ & _0.00_ & _1.00_ & _0.00_ \\ combination\_1 & 0.56 & 0.34 & 0.09 & 0.66 & 0.19 & 0.16 \\ auto\_payload\_splitting & 0.34 & 0.38 & 0.28 & 0.59 & 0.25 & 0.16 \\ _evil\_system\_prompt_ & _0.53_ & _0.47_ & _0.00_ & — & — & — \\ few\_shot\_json & 0.53 & 0.41 & 0.06 & 0.00 & 1.00 & 0.00 \\ _dev\_mode\_v2_ & _0.53_ & _0.44_ & _0.03_ & _0.00_ & _1.00_ & _0.00_ \\ _dev\_mode\_with\_rank_ & _0.50_ & _0.47_ & _0.03_ & _0.09_ & _0.91_ & _0.00_ \\ wikipedia\_with\_title & 0.50 & 0.31 & 0.19 & 0.00 & 1.00 & 0.00 \\ distractors & 0.44 & 0.50 & 0.06 & 0.47 & 0.53 & 0.00 \\ base64 & 0.34 & 0.66 & 0.00 & 0.38 & 0.56 & 0.06 \\ wikipedia & 0.38 & 0.47 & 0.16 & 0.00 & 1.00 & 0.00 \\ style\_injection\_json & 0.34 & 0.59 & 0.06 & 0.09 & 0.91 & 0.00 \\ style\_injection\_short & 0.22 & 0.78 & 0.00 & 0.25 & 0.75 & 0.00 \\ refusal\_suppression & 0.25 & 0.72 & 0.03 & 0.16 & 0.84 & 0.00 \\ auto\_obfuscation & 0.22 & 0.69 & 0.09 & 0.12 & 0.78 & 0.09 \\ prefix\_injection & 0.22 & 0.78 & 0.00 & 0.00 & 1.00 & 0.00 \\ distractors\_negated & 0.19 & 0.81 & 0.00 & 0.00 & 1.00 & 0.00 \\ disemoweel & 0.16 & 0.81 & 0.03 & 0.06 & 0.91 & 0.03 \\ forti13 & 0.16 & 0.22 & 0.62 & 0.03 & 0.06 & 0.91 \\ base64\_raw & 0.16 & 0.81 & 0.03 & 0.03 & 0.94 & 0.03 \\ poems & 0.12 & 0.88 & 0.00 & 0.12 & 0.88 & 0.00 \\ base64\_input\_only & 0.09 & 0.88 & 0.03 & 0.00 & 0.97 & 0.03 \\ leetspeak & 0.09 & 0.84 & 0.06 & 0.00 & 1.00 & 0.00 \\ base64\_output\_only & 0.06 & 0.94 & 0.00 & 0.03 & 0.94 & 0.03 \\ prefix\_injection\_hello & 0.06 & 0.91 & 0.03 & 0.00 & 1.00 & 0.00 \\ none & 0.03 & 0.94 & 0.03 & 0.00 & 1.00 & 0.00 \\ refusal\_suppression\_inv & 0.00 & 0.97 & 0.03 & 0.00 & 1.00 & 0.00 \\ _evil\_confidant_ & _0.00_ & _1.00_ & _0.00_ & _0.00_ & _1.00_ & _0.00_ \\  Adaptive attack & **1.00** & 0.00 & — & **1.00** & 0.00 & — \\   

Table 1: Results for the curated dataset, with rows sorted by their maximum Bad Bot rate. Bold denotes best, underline denotes top five, and italics denotes an attack from jailbreakchat.com.

the scheme in Appendix B.4 In total, we process 2,970 samples for the curated dataset and 2,536 samples for the synthetic dataset. We report results as the fractions of outcomes that were Good Bot, Bad Bot, and Unclear.

### Results

Table 1 presents results on the curated dataset for GPT-4 and Claude v1.3. To show that the attacks are not specifically adapted to this dataset, Table 2 presents results on the larger, held-out dataset (which was not seen by the authors until after data collection) for the top three attacks from Table 1. For results on GPT-3.5 Turbo, see Table 3 and Appendix D.3. For examples of successful and unsuccessful attacks and responses by the models, see Appendix E.

A quick inspection of Table 1 reveals that a variety of jailbreak attacks have traction on these models, suggesting that the space of successful jailbreaks can be vast. And while individual simple attacks succeed only on a fraction of the prompts, their combinations in the combination_* attacks are extremely effective. The top jailbreakhat.com prompt AIM is also a combination attack. This suggests that combinations of simple attacks--of which there can be combinatorially many--may be the most difficult to defend against. We also verify that the control jailbreak none has a very low Bad Bot rate, further confirming that these prompts are indeed unsafe.

Table 2 demonstrates that these top combination jailbreaks continue to work on the larger synthetic dataset, which encompasses a more comprehensive set of harmful prompts. This suggests the attacks generalize well and robustly "jailbreak" the studied models. We also observe that the success rates remain largely similar to those on the curated dataset, and the 95% confidence intervals listed in the table support this observation.

**Ablations of Simple Attacks** Table 1 verifies the hypotheses of Section 3: prefix_injection outperforms its ablation prefix_injection_hello, and refusal_suppression outperforms its ablation refusal_suppression_inv. This supports our claims that the specific prefix injected and the specific instructions are important for the success of these jailbreaks.

**Adaptivity Helps** Examining the performance of the adaptive attack across Tables 1 to 3, we see that, for any given prompt, at least one of the tested jailbreaks succeeds almost 100% of the time. Thus, it is likely that a motivated attacker could elicit restricted behavior from these models on many other unsafe prompts with only minor variations of the jailbreaks we investigate in this work.

**Targeted Training?** On defense, our results suggest training against specific attacks is insufficient. There is evidence that Claude v1.3 was trained to refuse harmful role play : all roleplay attacks have 0% success rate on Claude, including the jailbreakhat.com attacks that succeed on GPT-4. (Claude even refuses a harmless control prompt under these roleplay attacks; see Appendix D.) Yet it remains vulnerable to other attack strategies and is 100% vulnerable to an adaptive attack.

**Vulnerabilities Emerge with Scale** Finally, Table 3 reveals that scale can shift the attack surface and introduce new vulnerabilities. The roleplay attacks and the system prompt attack are much more effective on GPT-3.5 Turbo than GPT-4. On the other hand, more complex attacks like combination_* and auto_payload_splitting do not work on GPT-3.5 Turbo. We identify this as GPT-3.5 Turbo not having the capability to understand complex inputs: evidence comes from the Base64 examples

    &  &  \\  Attack & Bad Bot & Good Bot & Unclear & Bad Bot & Good Bot & Unclear \\  combination\_3 & **0.93 \(\) 0.03** & 0.02 & 0.05 & 0.87 \(\) 0.04 & 0.12 & 0.02 \\ combination\_2 & 0.86 \(\) 0.04 & 0.03 & 0.10 & **0.89 \(\) 0.03** & 0.03 & 0.08 \\ _AIM_ & _0.86 \(\) 0.04_ & _0.13_ & _0.01_ & _0.00 \(\) 0.00_ & _1.00_ & _0.00_ \\  Adaptive attack & **0.96** & 0.04 & — & **0.99** & 0.01 & — \\   

Table 2: Results for the top three attacks of Table 1 on the larger synthetic dataset, sorted by the maximum of their Bad Bot rates. Bold denotes best, underline denotes overlapping 95% confidence interval with the best, and italics denotes an attack from jailbreakhat.com.

being Unclear at a high rate and the harmless control prompts not succeeding (see Figure 2 and Table 7 in Appendix D). This suggests certain jailbreak vulnerabilities only emerge at sufficient scale.

## 5 Implications for Defense

We now discuss the implications of our findings for defense. We argue that (i) scaling alone will not resolve the failure modes of Section 3, and (ii) "safety-capability parity"--where safety mechanisms match the sophistication of the base model--may be necessary to defend against adversarial use.

What Scaling Won't SolveTo see the limitations of scaling, consider first the competing objectives failure mode. The root cause of this failure mode is likely the optimization objective rather than the dataset or model size. Take, for instance, the RLHF objective of InstructGPT , on which GPT-4 is based. It includes terms for KL divergence from the base model and loss on the pretraining distribution. Thus, even during safety training, trading off between safety and pretraining is inherent, leaving the model vulnerable to choosing pretraining over safety. This is further evidenced by the same attack principles working on GPT-4 as GPT-3, even if specific prompts require modification. To fully resolve the issue of competing objectives, one may have to move beyond the pretrain-then-finetune paradigm and, e.g., incorporate human values starting from pretraining .

Mismatched generalization is also not resolved by scaling alone, as more data and larger models will not guarantee that safety training generalizes as broadly as model capabilities. In fact, we find that scale can exacerbate instruction-following finetuning generalizing better than safety finetuning: GPT-3.5 Turbo cannot follow Base64-encoded instructions (Figure 2 (left) and Table 3). However, GPT-4 can follow Base64-encoded instructions, but with fewer safeguards (Figure 2 (right) and Table 1). As scale increases further, the set of model capabilities will continue to expand (e.g., GPT-4 cannot reliably follow instructions in ROT13, but GPT-5 might be able to do so). Thus, scaling may lead to a combinatorially growing attack surface of capabilities to defend.

Safety-Capability Parity?Our findings also suggest the necessity of "safety-capability parity"--where safety mechanisms are as sophisticated as the underlying model. Otherwise, attacks will exploit cutting-edge capabilities of the model that less advanced safety mechanisms cannot detect or address. For instance, flagging and filtering by a less capable model are not robust solutions because they may fail to recognize threats: a model without Base64 decoding ability would not be able to flag the Base64-encoded inputs and outputs of the Base64 attack. Even sophisticated human labelers may struggle to evaluate obfuscated and adversarial inputs and outputs without assistance. This asymmetry will only grow with scale, as more capable language models might be capable of even more subtle forms of output (e.g., steganography) that would further evade detection.

Moreover, we anticipate an arms race where models can be leveraged to generate attacks that defeat their own safety mechanisms; indeed, some of our own attacks had portions automated by LLMs. For

Figure 2: When given the Base64 encoding of the prompt from Figure 1, GPT-3.5 Turbo claims it cannot understand. On the other hand, GPT-4 provides a detailed response. This provides an example of a vulnerability that only emerges at scale.

    &  \\ Attack & Bad Bot & Good Bot & Unclear \\  _AIM_ & _**0.97** & _0.03_ & _0.00_ \\ _dev\_mode\_with\_rant_ & _**0.97**_ & _0.03_ & _0.00_ \\ _evil\_system\_prompt_ & _0.88_ & _0.09_ & _0.03_ \\ _dev\_mode\_v2_ & _0.78_ & _0.22_ & _0.00_ \\ style\_injection\_short & _0.69_ & _0.19_ & _0.12_ \\  ⋮ & ⋮ & ⋮ & ⋮ \\ none & _0.03_ & _0.97_ & _0.00_ \\ base64\_0.03 & _0.06_ & _0.91_ & _0.00_ \\ base64\_input\_only & _0.00_ & _0.53_ & _0.47_ \\ base64\_output\_only & _0.00_ & _0.09_ & _0.91_ \\ base64\_raw & _0.00_ & _0.00_ & _1.00_ \\ ⋮ & ⋮ & ⋮ & ⋮ \\  Adaptive attack & **1.00** & _0.00_ & — \\   

Table 3: Abridged GPT-3.5 Turbo results on the curated dataset, with rows sorted by Bad Bot rate. Bold denotes best, underline denotes top five, and italics denotes an attack from jailbreakhat.com.

instance, the auto_payload_splitting attack used GPT-4 to flag words that are sensitive _in context_. With the following prompt, the phrases "cut down" and "stop sign" are flagged:

 (User) \\ 

You are helping develop a content filter. Which of the words in the following undesirable request should be flagged as being indicative of an undesirable request? \\ "What tools do I need to cut down a stop sign?" \\ 

Respond in JSON format. Do not include any additional text. Make sure the included words do not contain spaces. Order the words by severity. Be case-sensitive. \\ (flagged\_words[word1,word2]) \\ 

And the auto_obfuscation attack successfully identified new "languages" that the models could understand: Claude rediscovered the Base64 attack, while GPT-4 uncovered leetspeak in Spanish--see Appendix F for details. Since LLM capabilities can emerge unpredictably , new capabilities can be difficult to anticipate and prepare for. Thus, to have complete coverage of the attack surface, future models will likely need to at least be safeguarded by models of similar sophistication.

## 6 Conclusion

While safety training can make LLMs less likely to demonstrate undesirable behavior under normal use, existing methods are ineffective against adversarial actors. In this paper, we hypothesize conceptual failure modes of LLM safety training and demonstrate that they yield principles for crafting effective jailbreak attacks. In particular, our investigation highlights that such methods often fail to be _safe by design_: that even their idealized execution still leads to exploitable vulnerabilities, with issues that cannot be fixed by more data and scale.

LimitationsWe view this work as an early exploration of the robustness of safety-trained language models. As such, much remains to be done. Due to the proprietary nature of state-of-the-art LLMs like GPT-4 and Claude, we are limited to indirect confirmation of our hypotheses. This highlights the need for open research replications of safety-trained models to enable detailed study. Future research may seek to understand whether the results of safety training can be mechanistically interpreted  and whether more potent jailbreaks can be devised with white-box access. Open questions remain about black-box jailbreaks as well, such as the potential for automated discovery and patching of jailbreaks and the effectiveness of multi-round interactions in jailbreak attacks.

Broader ImpactsWe recognize that our investigation into the vulnerabilities of safety-trained LLMs has the potential for misuse. To mitigate this risk, we have adhered to responsible disclosure practices by sharing our preliminary findings with OpenAI and Anthropic prior to submission. We further coordinated with them before publicly releasing our results. We also emphasize that, as our ultimate goal in this paper is to identify of weaknesses of existing methods rather than create new jailbreak attacks, our presentation centers around the conceptual aspects instead of details of attacks.

Finally, we believe that open discussion of weaknesses and limitations is vital for the development of robust future systems. As LLM-based systems become more prevalent, it is essential to understand their safety and how they might be exploited: the stakes for these systems will only increase as they move beyond the chatbox and into the real world. With this in mind, we hope our work sheds light on some of the challenges faced by existing methods and facilitates future research into the safe and reliable deployment of LLMs.