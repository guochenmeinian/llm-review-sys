# A Theory of Transfer-Based Black-Box Attacks: Explanation and Implications

Yanbo Chen, Weiwei Liu

School of Computer Science, Wuhan University

National Engineering Research Center for Multimedia Software, Wuhan University

Institute of Artificial Intelligence, Wuhan University

Hubei Key Laboratory of Multimedia and Network Communication Engineering, Wuhan University

{yanbo.acad,liuweiwei863}@gmail.com

Correspondence to: Weiwei Liu <liuweiwei863@gmail.com>.

###### Abstract

Transfer-based attacks  are a practical method of black-box adversarial attacks in which the attacker aims to craft adversarial examples from a source model that is transferable to the target model. Many empirical works  have tried to explain the transferability of adversarial examples from different angles. However, these works only provide ad hoc explanations without quantitative analyses. The theory behind transfer-based attacks remains a mystery.

This paper studies transfer-based attacks under a unified theoretical framework. We propose an explanatory model, called the _manifold attack model_, that formalizes popular beliefs and explains the existing empirical results. Our model explains why adversarial examples are transferable even when the source model is inaccurate as observed in Papernot et al. . Moreover, our model implies that the existence of transferable adversarial examples depends on the "curvature" of the data manifold, which further explains why the success rates of transfer-based attacks are hard to improve. We also discuss our model's expressive power and applicability.

## 1 Introduction

Machine learning (ML) models are vulnerable to adversarial examples . It has been observed that adversarial examples of one model can fool another model , i.e., adversarial examples are transferable. Utilizing this transferability, researchers have developed some practical black-box attack methods, in which they first obtain a source model and then use the adversarial examples of the source model to attack the target model . This type of attack is referred to in the literature as transfer-based attacks (TBA).

TBAs have many intriguing properties. For instance, Papernot et al.  find that the adversarial examples of an _inaccurate_ source model can transfer to the target model with a non-negligible success rate. Besides, the success rates of TBAs are constantly lower than other methods of black-box attacks ; the low success rate seems to be an intrinsic property of TBAs.

Previous works have tried to explain these properties from different perspectives . Unfortunately, these works only provide ad hoc explanations for the properties of TBAs without quantitative analyses under a unified framework. For example, Dong et al.  suggest that the low success rates of TBAs are due to the adversarial examples of the source model falling into a "non-adversarial region" of the target model, while the properties of such regions are not discussed quantitatively. As for the cause of transferability, a self-evident explanation is that the source and target models have similar decision boundaries . However, these similarities are hard to characterize, and it is even harder to associatethe decision boundary similarity with the properties of TBAs. More importantly, this explanation contradicts the abovementioned experimental results in Papernot et al. , i.e., adversarial examples are also transferable when the source and target model does not have similar decision boundaries. We refer the readers to Section 2 for more related works. In summary, existing analyses make good sense under their respective settings, but it is unclear whether they are in accord with each other. It is natural to ask the following question:

_Can we find a unified theoretical framework that explains the properties of TBAs?_

This paper gives a positive answer to this question. More specifically, we propose an explanatory model, called the _manifold attack model_, that formalizes popular beliefs and explains the existing empirical results in a unified theoretical framework. Our model assumes that the natural data lies on a low-dimensional manifold  denoted by \(\); see Section 2 for more related works on this assumption. As the central part of our model, we specify a hypothesis class \(_{}\) and assume that both the source and the target model come from \(_{}\). The construction of \(_{}\) is motivated by the following two widespread beliefs. In order to classify unseen data correctly, ML models are believed to have extracted _semantic information_ (i.e., the ground truth) of the natural data after training. On the other hand, ML models are also able to learn non-robust features that generalize well in test phrases . These features reflect the _geometrical information_ of the training data .

In our model, the hypothesis class \(_{}\) is designed to capture both semantic and geometric information of the data. As we will define in Section 4, the classifiers in \(_{}\) can be decomposed into the product of a basis classifier \(f_{b}\) and a multiplier \(\), i.e., \(f=f_{b}\) for \( f_{}\). We characterize the semantic information as separated sets \(A^{1},A^{2},,A^{k}\) (for a \(k\)-class classification task) and capture this information by letting \(f_{b}\) take different values on these sets. The geometrical information \(G\) is interpreted as the "approximated shape" of the data manifold, and we let \(\) concentrate around \(G\). By multiplying \(f_{b}\) and \(\) together, we obtain a classifier that captures the semantic and geometry information of the training data. In brief, our model assumes that both the source and target models come from a specified hypothesis class and the natural data is drawn from a low-dimensional manifold.

Despite its simple form, the manifold attack model provides us with powerful tools to theoretically analyze the properties of TBAs. More specifically, Theorem 4.8 proves that the off-manifold adversarial examples are transferable when the source model is inaccurate, which explains the empirical results from Papernot et al. . By further discussing the existence of off-manifold adversarial examples, we theoretically explain why the success rates of TBAs are hard to improve. Moreover, Theorem 4.13 quantitatively discusses the relationship between the existence of transferable adversarial examples and the "curvature" of the data manifold \(\), which formalizes the explanation in Dong et al. .

In addition to the explanatory results, we further discuss the expressive power and the possible extensions of our model in general applications. Section 5.1 proves that our model is extensible, i.e., we can replace the hypothesis class \(_{}\) by the family of ReLU networks while proving similar results. Our model builds a bridge between the less developed theory behind TBAs and the huge amount of theoretical works analyzing ReLU networks. Furthermore, Section 5.2 demonstrate that the expressive power of our model is strong enough for the study of TBAs. We also provide a detailed discussion on the applicability of our model in Appendix A.

In summary, we propose a model that is theoretically tractable and consistent with existing results. It formalizes widespread beliefs and explains the properties of TBAs under a unified framework. The remainder of this paper is structured as follows. Sections 2 and 3 introduce the related works and terminologies, respectively. We propose our model and present the explanatory results in Section 4. Section 5 makes further discussions on our model. Section 6 summarizes this paper. Some remarks and the omitted proofs can be found in the appendix.

## 2 Related Works

The related works of TBAs and the low-dimensional manifold assumption are summarized as follows.

### Transfer-Based Attacks

The research on the adversarial robustness of ML can be roughly divided into adversarial attacks , defenses , and the analyses of the robustness of existing methods . Adversarialattacks can be divided into two classes based on whether the attackers have the gradient information of the target model, i.e., white-box  and black-box  attacks. TBAs are one of the two main approaches to performing black-box attacks on ML models. Apart from TBAs, the other main approach is the optimization-based attack that approximates the gradient of the target model and performs white-box attacks thereafter [28; 29; 30]. While TBAs require much less information from the target model (and thus more practical) than optimize-based attacks, the success rates of TBAs are constantly lower, even when the source models are almost accurate [2; 3; 5; 6; 11].

There are a few theoretical works that have tried to explain the properties of TBAs. However, these works rely heavily on either simple models or strong assumptions. Apart from those mentioned in Section 1, the seminal work of Goodfellow et al.  tries to explain the transferability of adversarial examples using linear models. Charles et al.  theoretically prove, in the context of linear classifiers and two-layer ReLU networks, that transferable adversarial examples exist. However,  does not explain those properties of TBAs mentioned in Section 1. Gilmer et al.  assume that the natural data is drawn from a "concentric spheres dataset", which can be viewed as a special form of the low-dimensional data manifold. In comparison to previous works, our model requires milder assumptions on both data distribution and hypothesis class, while providing a more detailed discussion of the properties of TBAs.

### The Low-Dimensional Manifold Assumption

The low-dimensional manifold assumption is commonly seen in many areas of research, ranging from classic approximation theory [13; 14] to computer vision [15; 16]. Under this assumption, it is intuitive to divide the adversarial examples into two groups based on whether they lie on or off the data manifold. Both on-manifold [15; 33] and off-manifold [32; 34] adversarial examples have been studied in many previous works. In most cases, a classifier is simultaneously vulnerable to both on- and off-manifold adversarial examples .

In this paper, we discuss the transferability of both on- and off-manifold adversarial examples. We are mainly motivated by the techniques from Zhang et al.  and Li et al. . The main results of Zhang et al.  decompose the adversarial risk based on the position of the adversarial example and discuss the existence of different types of adversarial examples; Zhang et al.  provides us with useful tools to analyze the existence of transferable adversarial examples. Li et al.  consider robust generalization and approximate continuous classifiers by ReLU networks. In this paper, we prove that the hypothesis class \(_{}\) in our model can also be approximated by ReLU networks.

## 3 Problem Setup

This section introduces the basic setups and terminologies. We use a black triangle sign (\(\)) to indicate the end of assumptions, definitions, or remarks.

### Notations

Let \(^{d}\) be the real vector space equipped with the \(l_{p}\)-norm, where \(p[1,+]\). In this paper, we consider the \(k\)-class classification problems (\(k 2\)) on \(^{d}\). Denote the data-label pairs by \((,y)\), in which \(y\) is chosen from some label space \(\). By convention, we let \(=\{-1,1\}\) in binary classification and \(=\{1,2,,k\}\) when \(k>2\). Unless otherwise specified, we assume that the natural data lie on a low-dimensional manifold \(^{d}\). More specifically, we adopt the following assumption.

**Assumption 1** (Low-dimensional Manifold).: Let \(^{d}\) be a compact smooth manifold2 and assume that the dimension of \(\) is less than \(d\). Let \(D\) be some given continuous distribution that is supported on \(\). We say that a random variable \(\) is _natural data_ if \( D\). \(\)

Denote the support set of \(D\) by \((D)\). With slight abuse of notation, let \(y\) also be a mapping from \((D)\) to \(\) that assigns each natural data \( D\) with a true label \(y=y()\). Notice that only natural data is endowed with a true label. Since \((D)^{d}\), we will make no distinction between \(^{d}\) and \(^{d}\) in the remainder of this paper without loss of generality (WLOG).

Let \(d_{p}\) be the metric induced by the \(l_{p}\)-norm and \(B(;r)\) the \(l_{p}\)-ball around \(\) with radius \(r\). For any subset \(S^{d}\), let \(d_{p}(,S):=_{^{} S}d_{p}(, ^{})\) be the distance between \(S\) and \(\). Let the classifiers \(f\) be functions defined on \(^{d}\). When \(k>2\), we consider \(f()=(f^{(1)}(),f^{(2)}(),,f^{(k)}())^{T}\) that maps \(^{d}\) to \(f()^{k}\). For ease of notation, let \(y(f,):=_{1 i k}f^{(0)}()\) be the output class of \(f\) for \(^{d}\). Since \(D\) is continuous, \(y(f,)\) take unique value w.p. (with probability) \(1\) when \( D\). In binary classification problems, we consider \(f:^{d}\) and \(y(f,)=(f())\).

Given perturbation radius \(\) and natural data \( D\), we call \(_{a} B(;)\) an _adversarial example_ of \(f\) at \(\) if \((f,)(f,_{a})\). Given classifier \(f\), let the _standard risk_ of \(f\) be \(R_{}(f):=_{ D}[y(f,) y( )]\), and the _adversarial risk_ of \(f\) with regard to (w.r.t.) \(\) is defined as

\[R_{}(f;):=_{ D}[ _{a} B(;)s.t.\ y(f,) y(f,_{a})]. \]

Since TBA is often against accurate target models in practice, our paper assumes that the standard risk of the target model \(f_{t}\) is \(R_{}(f_{t})=0\) for the sake of simplicity. Given such accurate \(f_{t}\), the goal of a TBA is to find adversarial examples of \(f_{t}\). Specifically, the attacker needs to obtain a source model \(f_{s}\) and craft adversarial examples of \(f_{s}\) using white-box attacks. Given natural data \( D\) and an adversarial example \(_{a} B(;)\) of \(f_{s}\) at \(\), we say that \(_{a}\)_transfers_ to \(f_{t}\) if \(y(f_{t},) y(f_{t},_{a})\).

### The On- and Off-Manifold Adversarial Examples

In this paper, the on- and off-manifold adversarial examples are defined as follows.

**Definition 3.1** (On- and off-manifold adversarial examples).: For any given classifier \(f\), perturbation radius \(\) and natural data \( D\), let \(_{a} B(;)\) be an adversarial example of \(f\) at \(\). We call \(_{a}\) an _on-manifold_ adversarial example \(_{a}\), or an _off-manifold_ adversarial example if \(_{a}\). \(\)

We say that a classifier \(f\)_suffers from_, or _is vulnerable to_ (on-, off-manifold) adversarial examples if \( D,_{a} B(;)\) such that \(_{a}\) is an (on-, off-manifold) adversarial examples of \(f\) at \(\). If \(f\) does not suffer from (on-, off-manifold) adversarial examples, then we consider it to be _robust against_ (on-, off-manifold) adversarial examples.

In most cases, the adversarial examples of \(f_{t}\) are regarded as imperceptible to humans (e.g., in a cat-or-dog classification task, the adversarial examples of cat images still look like cats), which implies that adversarial examples are not naturally generated data. Otherwise, the adversarial examples would be endowed with the same label as their corresponding natural data, which leads to a contradiction since \(R_{}(f_{t})=0\). In other words, adversarial examples should not lie in the support of \(D\). To ensure this, we additionally assume that the natural data with different true labels are separated from each other. Notice that there is an intrinsic connection between the natural data \(\) and its true label \(y()\), which is often referred to as "semantic information" of the natural data. In this paper, we formalize the _semantic information of the natural data_ as a family of sets \(\{A^{j}:j\}\), where

\[A^{j}:=\{(D):y()=j\},\  j. \]

In practice, the semantic information \(A^{i}\) and \(A^{j}\) is often separated if \(i j\) (e.g., Remark A.1). In this paper, we assume that the semantic information is separated in the following sense.

**Assumption 2** (Separated Semantic Information).: Given \(>0\), for \( i j\), we assume that \(A^{i}\) and \(A^{j}\) are \(2\)-separated. Here, for any given subsets \(E,F^{d}\) and \(r>0\), we say that \(E\) and \(F\) are \(r\)_-separated_ if \(d_{p}(_{1},_{2}) r\) for \(_{1} E\) and \(_{2} F\). \(\)

With the help of Assumption 2, it is easy to check that the adversarial examples would not lie in the support set of \(D\) if \(<\). In the rest of this paper, we will treat \(\) and \(\) as fixed parameters and always assume that \(<\).

## 4 The Manifold Attack Model

Our proposed model specifies a hypothesis class that has two components. The first one is a semantic classifier that tries to capture the semantic information of the natural data. Similar to Equation (2), we call \(\{A^{j}_{f}:j\}\) the _semantic information learned by a classifier \(f\)_ if \(y(f,)=j\) for \( A^{j}_{f}\) and \( j\). To avoid trivial discussion, we assume that \(A^{j}_{f}\) for \( j\). The following definition specifies a family of classifiers that learn separated semantic information.

**Definition 4.1** (Semantic classifier).: We call \(f_{b}\) a _semantic classifier_ if there is a family of pairwise \(2\)-separated set \(\{A^{j}_{f}:j\}\) such that \(\{A^{j}_{f}:j\}\) is the semantic information learned by \(f_{b}\). 

It is easy to check that \(R_{}(f_{b})=0\) if \(A^{j} A^{j}_{f}\) for \( j\). Intuitively, the accuracy of a classifier depends on how well it learns the semantic information of the natural data.

The second component of our model is a function that captures the geometric structure of the data.

**Definition 4.2** (Concentration multiplier).: Given \(G^{d}\), we call \(\) a _concentration multiplier_ around \(G\) if \(()=1\) for \( G\), and \((_{1})<(_{2})\) for \(_{1},_{2}^{d}\) with \(d_{p}(_{1},G)>d_{p}(_{2},G)\). 

Intuitively, the geometric information \(G\) can be interpreted as the "approximated shape" of the natural data. The concentration multipliers would assign a much lower confidence score to those data points that are outside of or far away from \(G\). In the ideal case, \(G\) precisely captures the shape of \(\), which enables \(\) to detect the off-manifold data points (i.e., the out-of-distribution (OOD) data ).

As mentioned in Section 1, ML models often capture both the semantic and geometrical information of the natural data. Therefore, we assume that the source and target models can be decomposed into the product of a semantic classifier and a concentration multiplier. More specifically, we consider the hypothesis class defined as follows.

**Definition 4.3**.: Consider the \(k\)-class classification problems (\(k 2\)) on \(^{d}\). Let \(_{b}\) and \(\) be a collection of semantic classifiers and concentration multipliers, respectively. When \(k>2\), let \(_{}\) be the family of all classifiers \(f:^{d}^{k}\) that satisfy

\[f^{(j)}()=f^{(j)}_{b}()() ^{d} j. \]

In the binary classification case, we let \(f()=f_{b}()()\) for \(^{d}\). Here, \(f_{b}\) and \(\) are chosen from \(_{b}\) and \(\), respectively. Since the semantic information is contented in the manifold, we additionally assume that \(_{j}A^{j}_{f} G\), in which \(\{A^{j}_{f}:j\}\) is the semantic information learned by \(f_{b}\) and \(G\) is the approximated shape of \(\) learned by \(\). 

Definitions 4.1 to 4.3 together with Assumptions 1 and 2 establish the abstract framework for the manifold attack model. This model formalizes how a classifier captures the semantic and geometric information from natural data. It is worth noticing that choices of \(f_{}\) in Definition 4.3 remains largely arbitrary if we choose \(_{b}\) (and \(\)) to be the family of all possible semantic classifiers (and concentration multiplier) introduced in Definition 4.1 (and Definition 4.2). In fact, for any \(f\) that learns \(2\)-separated semantic information \(\{A^{j}_{f}:j\}\) and any \(\) such that \((f()=0)(()=0)\) for \(^{d}\), we can simply let \(f^{(j)}_{b}()=f^{(j)}()^{-1}()\) for \( 0\). It is easy to check that \(f_{b}\) is a semantic classifier as long as \(A^{j}_{f} G\) for \( j\).

The generality in Definition 4.3 comes at the expense of theoretical intractability. Take the semantic classifier for example. In Definition 4.1, \(f_{b}\) cannot be determined by the semantic information it learns. Moreover, \(f_{b}\) cannot even be parameterized according to this definition. There is an implicit tradeoff between generality and theoretical tractability: quantitative analysis of specific scenarios might need more delicate definitions. Next, we introduce one specific combination of \(f_{b}\) and \(\) to carry on analyzing the properties of TBAs.

For the sake of simplicity, we only present analyses and results in the context of binary classification in Sections 4 and 5. For ease of notation, let \(A:=\{:y()=1\}\) and \(B:=\{:y()=-1\}\), i.e., the semantic information of binary natural data. The following proposition specifies a sub-family of semantic classifiers:

**Proposition 4.4** (semantic classifier, binary case).: _Given \(2\)-separated sets \(A_{f},B_{f}\). Define:_

\[f_{b}()=f_{b}(;A_{f},B_{f}):=(,B_{f} )-d_{p}(,A_{f})}{d_{p}(,B_{f})+d_{p}(,A_{f})}. \]

_Then, \(f_{b}\) is a semantic classifier. In particular, we can obtain from Equation (4) that \(f_{b}()>0\) if \(\) is closer (w.r.t. \(d_{p}\)) to \(A_{f}\) than \(B_{f}\) and \(f_{b}()<0\) otherwise._

Let \(_{}\) the collection of all semantic classifiers defined by Equation (4) for all possible \(2\)-separated \(A_{f},B_{f}\). The following proposition shows that a semantic classifier can be accurate and robust.

**Proposition 4.5**.: _Take \(A_{f}=A\) and \(B_{f}=B\) in Equation (4) and denote the corresponding classifier by \(f_{b}^{*}\). Then, for any given \(>0\), we have \(R_{}(f_{b}^{*})=R_{}(f_{b}^{*},)=0\)._

Similar to Proposition 4.4, we specify a distance-based family of concentration multipliers.

**Proposition 4.6** (Concentration multiplier, binary case).: _For any given \(r>0\) and \(G^{d}\), denote_

\[()=(;r,G):=(,G)}{r+d_{p}( ,G)},\ ^{d}. \]

_Then \(()\) is a concentration multiplier around \(G\)._

Propositions 4.4 and 4.6 introduce a combination of \(f_{b}\) and \(\) in binary case. Both \(f_{b}\) and \(\) can be parameterized by the semantic and geometrical information it learns. It is also possible to extend our analysis to other scenarios by specifying other combinations of \(f_{b}\) and \(\), which is out of the scope of our paper. In the following sections, we adopt the following assumption.

**Assumption 3**.: The source and target model are chosen from the hypothesis class given by Definition 4.3, where \(_{b}\) and \(\) are the function classes introduced by Propositions 4.4 and 4.6. 

With slight abuse of notation, we denote the hypothesis class in Assumption 3 by \(_{}\). Intuitively, for any \(f=f_{b}_{}\), the semantic classifier controls the accuracy of \(f\). The similarity between \(A,B\) and \(A_{f},B_{f}\) reflects how well \(f_{b}\) fits the natural data. Moreover, the overlapping area \((A B)(A_{f} B_{f})\) should contain the training data of \(f\) (and thus be non-empty) if \(f\) has reached zero training error. The concentration multiplier helps classification by assigning lower confidence scores on those \( G\), but it might also bring in adversarial examples. Since we only impose mild conditions on the choice of \(G\) (i.e., \(A_{f} B_{f} G\)), \(_{}\) can capture complicated geometrical information. We will come back to the expressive power of \(_{}\) at Section 5.2.

### The Transferability of Adversarial Examples

As a warm-up, we study the robustness of \(f_{}\) against on- and off-manifold adversarial examples without specifying \(f_{b}\) and \(\). The following proposition shows that the vulnerability of \(f_{}\) to on- and off-manifold adversarial examples are different.

**Proposition 4.7**.: _Let \(f=f_{b}\). Given \(R_{}(f;) 0\), we can obtain (w.p. \(1\)) that_

1. \(f\) _suffers from off-manifold adversarial examples._
2. _if_ \(f_{b}\) _captures the semantic information of the natural data (i.e.,_ \(A A_{f}\) _and_ \(B B_{f}\)_), then_ \(f\) _is robust against on-manifold adversarial examples._

Figure 1: A visualization of the manifold, semantic information, and adversarial examples. The manifold \(\) is represented by the grid surface. For clarity, we only visualize the above half of \(\) and omit the below half. **Left**: the \(2\)-separated semantic information \(A,B\) is painted in dark blue. According to Equation (6), the adversarial examples of \(f_{t}=f_{b}^{*}_{}\) lie in the shaded bodies in light blue, which is outside of the data manifold. **Right**: the semantic information \(A,B\) of the natural data is represented as the light blue area on \(\). The raised parts upon \(\) are \(_{}(A_{f})\) (painted in orange) and \(_{}(B_{f})\) (painted in dark blue). The intersections of \(\) and \(_{}(A_{f})\) (or \(_{}(B_{f})\)) is \(A_{f}\) (or \(B_{f}\)), which are indicated by the areas surrounded by the inner contour lines. On the outer contour line of \(A_{f}\) and \(B_{f}\), we have \(_{}=0\) since \(d_{2}(,_{}(A_{f}\ \ B_{f}))=\).

_Remark_.: The results in Proposition 4.7 hold w.p. 1 since our proof ignores some minor events (e.g., \(f()=0\)) that occur w.p. 0. For the sake of simplicity, we will not specify those events that happen almost surely in the rest of this paper.

The first result of Proposition 4.7 implies that \(f_{}\) suffers from adversarial examples if and only if \(f\) suffers from off-manifold adversarial examples. In Section 4.2, we will discuss the existence of transferable adversarial examples based on this result.

We explain the properties of TBAs by proving the existence of \(f_{}\) that satisfies desired properties. First of all, we introduce a sub-family of \(_{}\) that is robust against on-manifold adversarial examples. The following concentration multipliers force \(f_{b}\) to concentrate around the data manifold, which brings off-manifold adversarial examples to \(f_{b}\) without introducing on-manifold adversarial examples. For any unspecified \((0,1)\), let

\[_{}():=(;,)= (,)}{+d_{p}(,)}. \]

Consider the classifiers induced by \(_{}\), i.e., \(f=f_{b}_{}\). It is clear from Equation (6) that \(f()=f_{b}()\) when \(\). Combining this with the separated assumptions of semantic information, we can see that \(_{}\) blocks the on-manifold adversarial examples. Figure 1 demonstrates that the target model \(f_{t}=f_{b}^{*}_{}\) is only vulnerable to off-manifold perturbations. The following proposition shows that off-manifold adversarial examples are transferable even if the source model is inaccurate, which explains the phenomenon in Papernot et al. .

**Theorem 4.8**.: _Consider TBAs with perturbation radius \((0,]\), target model \(f_{t}=f_{b}^{*}_{}\) and source model \(f_{s}=f_{b}_{}\), \(f_{b}_{b}\). Denote the semantic information of \(f_{b}\) by \(A_{f}\) and \(B_{f}\). Then, for \( A B\), all adversarial examples (if exist) of \(f_{s}\) at \(\) are transferable if \( A_{f} B_{f}\)._

Theorem 4.8 proves that all of the adversarial examples of \(f_{s}\) are transferable if the concentration multipliers of \(f_{t}\) and \(f_{s}\) are identical, given that \((A B)(A_{f} B_{f})\). In particular, \(f_{s}\) is not necessarily accurate, which is consistent with the empirical results in Papernot et al. . Theorem 4.8 also implies that the vulnerability of ML models might be due to the non-robust geometrical information. Such vulnerability is transferable between models that learn similar geometrical information.

Next, we turn to the on-manifold adversarial examples. Proposition 4.7 shows that we cannot find an \(f_{}\) such that \(f\) suffers from only on-manifold adversarial examples. Instead, we specify a family of concentration multipliers that blocks out off-manifold adversarial examples that is "directly above" the manifold. In this case, we consider Euclidean space \(^{d}\) with \(l_{2}\)-norm and inner product \(_{1},_{2}:=_{1}^{} _{2}\). For \(\), let \(T_{}^{d}\) be the _tangent space_ of \(\) at \(\), i.e., the space spanned by the possible tangent directions that pass through \(\). Now that we are considering an inner product space, let \(N_{}^{d}\) be the _normal space_ at \(\) such that \( N_{}\) and \( T_{}\), we have \(,=0\). Denote \(_{r}(S):=\{^{}^{d}:  S\ s.t.\ d_{2}(,^{})<r,\ ^{}-  N_{}\}\) for any given \(r>0\) and \(S\). We call \(_{r}(S)\) a _tubular neighborhood_ of \(S\) if for \(^{}_{r}(S)\), there is an unique \( S\) such that \(^{}- N_{}\). For any semantic classifier \(f_{b}\) with semantic information \(A_{f}\) and \(B_{f}\), define

\[_{}()=_{}(;f_{b}):=( ;,N_{}(A_{f} B_{f}) \]

and consider \(f=f_{b}_{}(;f_{b})\). According to Equation (7), \(_{}\) blocks the off-manifold adversarial examples of \(f\) that is "directly above" \(A_{f}\) and \(B_{f}\). We use Figure 1 to visualize our idea. The following proposition provides a sufficient condition for adversarial examples that are not transferable.

**Proposition 4.9**.: _Consider TBA with perturbation radius \((0,]\), target model \(f_{t}=f_{b}^{*}_{}(,f_{b}^{*})\) and source model \(f_{s}=f_{b}_{}(;f_{b})\), \(f_{b}_{b}\). Denote_

\[S_{}:=(A A_{f})(B B_{f}),\ S_{}:=(A B_ {f})(B A_{f}). \]

_Then, for \( A B\), we have_

1. _if_ \(B(,) S_{} S_{}\)_, then_ \(f_{t}\) _and_ \(f_{s}\) _are both robust against adversarial examples;_
2. _if_ \(B(,) A B\)_and_ \( A_{f} B_{f}\)_, then the adversarial examples of_ \(f_{s}\) _at_ \(\) _(if exists) cannot transfer to_ \(f_{t}\)_._

We interpret \(S_{}\) (or \(S_{}\)) as the "_correct (or wrong) semantic information_" captured by \(f_{b}\). When \(S_{}=(D)\) and \(S_{}=\) (i.e., \(f_{b}\) captures the semantic information of the natural data), the first result of Proposition 4.9 implies that a large part of \((D)\) is robust against adversarial examples if we block the off-manifold adversarial examples that are "directly above" the semantic information. The second result of Proposition 4.9 implies that the potentially transferable adversarial examples are mostly located inside of the following set

\[(A B)(S_{} S_{})^{c}=(A B) (A_{f} B_{f})^{c}, \]

or at least close to its boundary. Here, we interpret \((A B)(A_{f} B_{f})^{c}\) as the semantic information not contained in the training data.

In summary, the results in this section together provide a general view of the role that on- and off-manifold adversarial examples play in TBAs. Although both on- and off-manifold adversarial exist, ML models seem to be more vulnerable to off-manifold adversarial examples, and the off-manifold adversarial examples seem to play a more important role in TBAs.

### The Existence of Transferable Adversarial Examples

Dong et al.  argue that the low success rates of TBAs are possibly due to the adversarial examples of \(f_{s}\) falling into a "non-adversarial region" of \(f_{t}\). In this section, we try to formalize this explanation and study the properties of such non-adversarial regions.

According to Proposition 4.7, \(f_{}\) suffers from adversarial examples if and only if \(f\) suffers from off-manifold adversarial examples. This section is devoted to discussing the possible non-existence of off-manifold examples. Before we delve into this problem, let us introduce the notion of robust radius that is initially studied in the certified robustness problems .

**Definition 4.10** (Robust Radius, binary case).: For any classifier \(f\), the _robust radius_ of \(f\) is defined as the minimum \(r 0\) such that for \( A B\) and \(_{1},_{2} B(,r)\), we have \(f(_{1})f(_{2}) 0\).

Denote the robust radius of \(f\) by \(r_{}(f)\). In our model, the robust radius of \(f_{}\) is controlled by \(\). Based on this notion, the following example demonstrates how the existence of off-manifold adversarial examples depends on the shape of the data manifold \(\).

**Example 4.11**.: We use the same setting as Proposition 4.9 and consider the target model \(f_{t}=f_{b}^{*}_{}\). Given \(0<r_{1}<r_{2}<<r_{3}\), denote \(_{1}=_{}(;r_{1},)\) and \(_{2}=_{}(;r_{2},)\). Using Equation (6), we obtain that \(r_{}(f_{b}^{*}_{1})=r_{1}<\) and \(r_{}(f_{b}^{*}_{2})=r_{2}<\). However, \(r_{}(f)<\) does not imply that all \( A B\) are vulnerable to adversarial examples.

We demonstrated our idea in Figure A.1. Denote the dark blue surface in Figure A.1 by \(_{r_{2}}:=\{:d_{2}(,)=r_{2}\}\). Observe that \(r_{2}\) is of the same magnitude as the "curvature" of \(\), we can find a \(_{0} A B\) such that \(d_{2}(,_{r_{2}})=r_{3}>\), i.e., \(_{0}\) is not vulnerable to adversarial examples. Note that all of such \(_{0}\) together form the non-adversarial region in Dong et al. . 

Example 4.11 shows that the existence of the adversarial examples of \(f=f_{b}_{}\) depends on the "curvature" of \(\), which seems to be a rather agnostic result. However, we can quantify the "curvature" of \(\) by the following lemma.

**Lemma 4.12** (Bredon ).: _Let \(^{d}\) be a compact smooth manifold, then there is a \(>0\) such that \(_{}\) is a tubular neighborhood of \(\)._

Lemma 4.12 can be viewed as a variant of the tubular neighborhood theorem (cf. Bredon ). The constant \(\) is decided by \(\) and can be used to evaluate the "curvature" of \(\) in Example 4.11. More specifically, the following proposition provides a sufficient condition for the existence of the off-manifold adversarial examples.

**Theorem 4.13**.: _Given perturbation radius \((0,]\) and target model \(f_{t}=f_{b}^{*}_{}(;r,)\). Let \(\) be the constant specified in Lemma 4.12. Then, for \( A B\), the off-manifold adversarial example of \(f_{t}\) at \(\) exists if \(r<\)._

Theorem 4.13 establishes a quantitative relationship between the existence of off-manifold examples and the "curvature" of the manifold.

## 5 Discussion

### Approximation with ReLU Networks

In applications, the target and source models of TBAs are mostly neural networks, e.g., ReLU networks. In this subsection, we use ReLU networks (cf. Definition 5.1) to approximate the classifiers \(f_{}\). Moreover, we recover some of the results in Section 4 based on the approximated classifier.

We start by introducing the family of ReLU networks. Denote the _rectified linear unit (ReLU)_ by \(():=\{,\}\), where \(\) is the zero vector in \(^{d}\) and \(\{,\}\) takes the entry-wise maximum of \(\) and \(\). The family of deep ReLU networks defined as follows.

**Definition 5.1** (Deep ReLU networks).: For any bias vector \(=(b_{1},,b_{L})\) and sequence of weight matrices \(=(_{1},,_{L})\), we call function \(:^{d}\) a _(deep) ReLU networks_ parametrized by \(\) and \(\) if

\[()=b_{L}+_{L}(b_{L-1}+_{L-1}( (b_{1}+_{1}))) \]

for \(^{d}\). Let \(W_{i}\) be a matrix with \(m_{i}\) rows and \(n_{i}\) columns. We have \(n_{1}=d\), \(m_{L}=1\), and \(m_{i}=n_{i+1}\) (\(1 i L-1\)).

The non-constant entries in \(\) and \(\) are called the _parameters_ of \(\). We say that a function is _induced by ReLU networks_ if it can be expressed by the linear combination of finite many ReLU networks.

It has been proven  that ReLU networks can approximate (w.r.t. the sup-norm) continuous functions with a bounded domain to any precision. Here, we denote the sup-norm of a function \(f:S^{d}\) by \(\|f\|_{}:=_{ S}|f()|\). Specifically, we have:

**Lemma 5.2** (Li et al. ).: _Given \(l\)-Lipschitz function \(f:^{d}[-1,1]\) and precision \(>0\), there is a ReLU network \(\) with \((l/)^{d}(d^{2}+d(1/))\) parameters that satisfies \(\|f-\|_{}\)._

Lemma 5.2 provides a useful tool to approximate the Lipschitz functions from \(^{d}\) to \([-1,1]\). It is straightforward to check the Lipschitzness of \(f_{}\). As a result, given \(>0\) and \(f_{}\), we can find an ReLU network \(\) such that \(\|f-\|_{}<\). Unfortunately, \(\) can no longer be decomposed into the product of \(f_{b}\) and \(\), which invalidates most of the results in Section 4. We have to make a detour to approximate \(f_{}\). As a first step, we approximate \(f_{b}\) and \(\) by \(}\) and \(\), respectively.

**Corollary 5.3**.: _Let \(f_{b}\) be a semantic classifier with semantic information \(A_{f}\) and \(B_{f}\) that satisfy a \(2\)-separated property. Given \(>0\), there is a ReLU network \(}\) with \(((1/)^{d})(d^{2}+d(1/))\) parameters such that \(\|f_{b}-}\|_{}\)._

**Corollary 5.4**.: _Given \(>0\), \(r>0\) and \(S^{d}\), there is a ReLU network \(\) with \(((1/r)^{d})(d^{2}+d(1/))\) parameters that can approximate \((;r,S)\) to precision \(\)._

Corollaries 5.3 and 5.4 approximate \(f_{b}\) and \(\) with ReLU networks. It remains to approximate the product of two ReLU networks using the following lemma.

**Lemma 5.5** (Yarotsky ).: _There is a function \(:[-1,1]^{2}[-1,1]\) induced by ReLU network with \((^{2}(^{-1}))\) parameters such that \((x,y)=0\) if \(xy=0\), and_

\[_{x,y}|(x,y)-xy|. \]

The above lemma constructs a ReLU network that efficiently approximates the multiplying operator in \(^{2}\). The following proposition shows that we can approximate \(f_{}\) with ReLU networks to any precision.

**Proposition 5.6**.: _Given \(,,,r>0\), for any \(f_{}\), there is a ReLU network \(\) with_

\[(\{,\}^{d})(d^{2}+d())+(^{2}()) \]

_parameters that satisfies \(\|f-\|_{}\)._

The following theorem partially recovers the results in Theorem 4.8 with ReLU networks.

**Theorem 5.7**.: _Consider TBAs with perturbation radius \((0,/2]\), target model \(f_{t}=f_{b}^{*}_{}\) and source model \(f_{s}=f_{b}_{}\), \(f_{b}_{b}\). Denote the semantic information of \(f_{b}\) by \(A_{f}\) and \(B_{f}\). Given \( 0.1\), let \(_{t}\) and \(_{s}\) be ReLU networks that satisfy_

\[\|_{t}-f_{f}\|_{},\|_{s}-f_{s}\|_{} . \]_Then, for \((A B)(A_{f} B_{f})\), the adversarial examples \(_{a}\) (if exist) of \(_{s}\) satisfies_

\[_{t}()_{t}(_{a}) 2(1+ )^{2}+2^{2}. \]

Notice that Equation (14) does not imply that the adversarial examples are transferable. Instead, it can only reduce the confidence in the decision made by \(_{t}\). The results in Theorem 4.8 cannot be fully recovered. When \(f\) is close to zero, whether the approximated ReLU network \(\) is greater or less than zero is hard to decide.

The following theorem recovers Proposition 4.9 after modifying some parameters.

**Theorem 5.8**.: _Consider TBAs with perturbation radius \((0,/2]\), target model \(f_{t}=f_{b}^{*}_{}\) and source model \(f_{s}=f_{b}_{}\), \(f_{b}_{b}\). Denote the semantic information of \(f_{b}\) by \(A_{f}\) and \(B_{f}\). Given \( 0.1\), let \(_{t}\) and \(_{s}\) be ReLU networks that satisfy Equation (13). Then, for \( A B\), we have_

1. _if_ \(B(,) S_{} S_{}\)_, then_ \(_{t}\) _and_ \(_{s}\) _are both robust against adversarial examples;_
2. _if_ \(B(,) A B\) _and_ \( A_{f} B_{f}\)_, then the adversarial examples of_ \(_{s}\) _at_ \(\) _(if exists) cannot transfer to_ \(_{t}\)_._

By approximating the classifiers in \(_{}\), we build a bridge between the less-developed theory behind TBAs and the huge amount of theoretical works analyzing ReLU networks.

### The Expressiveness of the Manifold Attack Model

In learning theory, the expressive power of a hypothesis class \(\) evaluates the performance of \(f\) in fitting natural data; see  for a comprehensive overview. Previous works have tried to explain the transferability based on less expressive hypothesis classes, e.g., linear classifiers  and the linear combination of "features" . In this subsection, we study the expressive power of our model.

The main goal of TBAs is not to fit the natural data. Instead, for any well-trained classifier \(f^{*}:^{d}\{-1,1\}\), i.e., \(R_{}(f^{*})=0\), the goal of our model is to find \(f_{}\) such that the adversarial examples of \(f\) fits those of \(f^{*}\). More specifically, we have

**Proposition 5.9**.: _For any classifier \(f^{*}\) with \(R_{}(f^{*})=0\) and perturbation radius \((r_{}(f^{*}),)\), there is \(f_{}\) such that 1) \(R_{}(f)=R_{}(f^{*})\), and 2) for \( A B\), if \(_{a}\) is an adversarial example of \(f^{*}\) at \(\), then exists \(_{a}^{*} B(_{a},r_{}(f^{*})/4)\) such that \(_{a}^{*}\) is an adversarial example of \(f\)._

The above proposition implies that our proposed model can cover a wide range of classifiers and is thus sufficient for the study of TBAs on manifolds.

## 6 Conclusion

This paper explains the properties of TBAs under a unified theoretical framework. We suggest that off-manifold adversarial examples play a major role in TBAs. In particular, we show that off-manifold adversarial examples are transferable when the source model is inaccurate. We also prove that the non-existence of off-manifold adversarial examples is one of the reasons why the success rates of TBAs are hard to improve.