# Finding Transformer Circuits with Edge Pruning

Adithya Bhaskar  Alexander Wettig  Dan Friedman  Danqi Chen

Princeton Language and Intelligence (PLI), Princeton University

adithyab@princeton.edu

{awettig, dfriedman, danqic}cs.@princeton.edu

###### Abstract

The path to interpreting a language model often proceeds via analysis of circuits--sparse computational subgraphs of the model that capture specific aspects of its behavior. Recent work has automated the task of discovering circuits. Yet, these methods have practical limitations, as they rely either on inefficient search algorithms or inaccurate approximations. In this paper, we frame automated circuit discovery as an optimization problem and propose _Edge Pruning_ as an effective and scalable solution. Edge Pruning leverages gradient-based pruning techniques, but instead of removing neurons or components, it prunes the _edges_ between components. Our method finds circuits in GPT-2 that use less than half the number of edges compared to circuits found by previous methods while being equally faithful to the full model predictions on standard circuit-finding tasks. Edge Pruning is efficient even with as many as 100K examples, outperforming previous methods in speed and producing substantially better circuits. It also perfectly recovers the ground-truth circuits in two models compiled with Tracr. Thanks to its efficiency, we scale Edge Pruning to CodeLlama-13B, a model over \(100\) the scale that prior methods operate on. We use this setting for a case study comparing the mechanisms behind instruction prompting and in-context learning. We find two circuits with more than \(99.96\%\) sparsity that match the performance of the full model and reveal that the mechanisms in the two settings overlap substantially. Our case study shows that Edge Pruning is a practical and scalable tool for interpretability and sheds light on behaviors that only emerge in large models.1

## 1 Introduction

Mechanistic interpretability strives to understand models via bottom-up descriptions of their components (e.g., attention heads and MLPs in Transformers (Vaswani et al., 2017)). This typically proceeds via the identification and analysis of a circuit (Olah et al., 2020; Elhage et al., 2021)--a sparse computational subgraph of the model that captures the aspects of its behavior we wish to study. The arduous process of identifying circuits (e.g., Wang et al. (2023)) was recently automated by ACDC (Conmy et al., 2023) and EAP (Syed et al., 2023). However, ACDC uses an expensive greedy search that ablates each edge to estimate its importance. It cannot scale to datasets beyond a few hundred examples or to billion-parameter models. EAP, on the other hand, uses gradient-based linear approximations of activation patching to estimate the importance of all edges simultaneously. While fast, these first-order approximations often sacrifice faithfulness to the full model. Besides, this approach ignores the impact of the presence/absence of other edges on the score.

In this paper, we frame circuit discovery as an optimization problem and tackle it via gradient-based pruning, rather than discrete search or first-order approximations. As such, we adapt pruning for the goal of circuit discovery instead of model compression. Rather than components, we prune theedges between components and replace missing edges with counterfactual activations from corrupted examples. We enable this by replacing the residual stream of a Transformer (Figure 0(a)) with a _disentangled residual stream_, which retains a list of all previous activations. This allows us to introduce edge masks that determine from which components to read. We then leverage discrete optimization techniques such as \(L_{0}\) regularization  to optimize these edge masks and produce sparse circuits (Figure 0(c)).

We evaluate our approach, Edge Pruning, on four fronts: (1) we measure how faithfully the discovered circuits describe the behavior of the full model, (2) we verify if it can recover ground-truth circuits in Tracr models  compiled from known program descriptions, (3) we evaluate how the method scales to more examples and (4) we assess its ability to find extremely sparse circuits in multi-billion parameter models. On four standard circuit-finding tasks, Edge Pruning finds circuits in GPT-2 Small  that are consistently more faithful to the full model and have better task performance than circuits found by prior methods. The gap is especially pronounced on more complex tasks like multi-template IOI , where we find circuits that have \(2.65\) fewer edges but describe model outputs just as faithfully as the circuit found by the next-best method. We show that Edge Pruning scales effectively to a version of IOI with 100K examples, where it outperforms prior methods in terms of speed and performance. Edge Pruning also perfectly recovers ground-truth circuits in two models compiled from known program descriptions with Tracr.

Finally, we establish that Edge Pruning scales to CodeLlama-13B --100\(\) the size of models typically tackled by automated circuit discovery methods--in a case study. Specifically, we compare the mechanisms behind instruction-prompting and in-context learning  on Boolean Expressions--a task adapted from the BBH  benchmark. Edge Pruning finds circuits with just \(0.04\%\) of model edges that match the model's performance in either setting. Interestingly, the few-shot circuit performs well when instruction-prompted, and vice versa. The two circuits also have a substantial overlap (\(62.7\%\) edges of the sparser circuit), and the circuit formed by this intersection also performs significantly above chance on the task. We infer that the model relies on shared mechanisms in the two settings. This case study demonstrates how Edge Pruning can inform the analysis of phenomena that only emerge in large models.

In summary, our contributions are as follows:

1. We propose Edge Pruning, an effective and scalable method for automated circuit finding.
2. We demonstrate that Edge Pruning is competitive with or better than state-of-the-art methods on simple tasks, and significantly superior on more complex ones, in terms of faithfulness and performance. Edge Pruning also scales well with more examples. Further, it perfectly recovers ground-truth circuits in two Transformers compiled by Tracr.

Figure 1: Edge Pruning disentangles the residual stream and optimizes continuous masks on the read operations via gradient descent. Discretizing the masks to \(\{0,1\}\) yields the final circuit. The full model corresponds to the case where all masks equal \(1\).

3. We scale Edge Pruning to CodeLlama-13B--a model over \(100\) larger than GPT-2 Small--on a task adapted from BBH. Our case study finds that mechanisms underlying in-context learning and instruction-prompting in CodeLlama-13B for this task overlap significantly.

## 2 Background: Circuit Discovery

The goal of circuit discovery is to facilitate a mechanistic understanding of Transformers by identifying the subset of a model's computational graph that is most relevant to a particular model behavior. In this section, we define the computational graph of a Transformer, formalize the objective for circuit discovery, and discuss the approaches of previous work.

The computational graph of Transformers.The Transformer architecture consists of a sequence of layers, namely attention layers and MLPs, which operate on the _residual stream_ (Figure 0(a)) . The \(i\)'th layer \(f_{i}\) reads the current state of the residual stream \(h_{i}\), computes its activations \(y_{i}=f_{i}(h_{i})\), and applies it as an additive update to the residual stream \(h_{i+1}=h_{i}+y_{i}\). We can expand this recurrence to make the dependence on prior outputs explicit:

\[y_{i}=f_{i}(y_{0}+_{j=1}^{i-1}y_{j}), \]

where \(y_{0}\) is the initialization of the residual stream with the input embeddings. We can represent the dependencies between layers as directed edges in a _computational graph_, where the edge \(j i\) denotes the connection between the output of layer \(j\) to the input of layer \(i\). Note that the computational graph may be defined at a more granular level. For instance, Conmy et al.  split attention layers into multiple parallel attention heads, and represents each head by four interconnected nodes. The query/key/value nodes receive separate input edges from previous layers, and the output node has outbound edges to downstream layers. We also follow this convention.

Circuits as subgraphs.A circuit is a computational subgraph \(\), where \(\) and \(\) denote the set of edges in the circuit and full model, respectively . How do we model a Transformer with a missing edge \(j i\)? Instead of simply removing the term \(y_{i}\) from the sum of inputs to node \(i\), we adopt the approach of _interchange ablation_. For each example \(x\), the user provides a corrupted example \(\), which should consist of a small change to \(x\) that would result in a different label in the task. We use \(\) as input to the full model to compute corrupted activations \(_{j}\) for all nodes. When an edge \(j i\) is removed from a circuit, we replace the contribution of \(y_{j}\) at the input of node \(i\) with the corrupted activation \(_{j}\). This ensures that the summed activations remain in-distribution  and it frames the decision to remove an edge as a counterfactual intervention .

Circuit discovery.The goal of circuit discovery  is to find a sparse subgraph that describes the behavior of the full model on a particular task. We use \(p_{}(y x,)\) to denote the output of the Transformer circuit given original and corrupted examples \(x,\), and denote the output of the full model as \(p_{}(y x)\) as the output of the full model. Formally, circuit discovery has the objective,

\[_{}_{(x,)}[D(p_{ }(y x) p_{}(y x,))], 1-||/|| c \]

where the constraint enforces a target sparsity of the circuit. \(\) denotes the task distribution of interest, for which the user curates pairs of clean and corrupted examples \((x,)\) that differ in crucial task features. The loss function \(D\) should capture the discrepancy between the outputs of the full model and the circuit; for language models, a natural choice is the KL divergence between token predictions.

Previous approaches.We now discuss how previous methods approximate this combinatorial optimization problem and the limitations of their approaches.

1. **ACDC** proposes to solve the above objective using _greedy search_--at each iteration, ACDC evaluates the effect of removing each edge individually, and removes any edge whose effect on the target metric is less than a specified threshold. This fails to capture the relative importance of edges and their interaction. Furthermore, the number of steps of the algorithm scales linearly with the number of edges, which is prohibitive at larger model sizes (e.g., CodeLlama-13B with \(3.88\)M edges).

[MISSING_PAGE_FAIL:4]

Details of the Edge Pruning processOur formulation of pruning is based on that used by CoFi Pruning (Xia et al., 2022). Specifically, we model the masks \(\) based on the hard concrete distribution as done by Louizos et al. (2018):

\[(,1-)\]

\[=(}{1-}+ )\]

\[}=(r-l)+l\]

\[=(1,(0,}))\]

where \(\) refers to the sigmoid function, \(=10^{-6}\), and \(\) indicates that the logarithm is applied element-wise. We fix the temperature \(=\). The last two lines stretch the distribution to \([l,r]=[-0.1,1.1]\) and accumulate the "excess" probability on either side to \(0\) and \(1\), respectively. The log alphas \(\) are the learnable parameters in this formulation.

Following, Wang et al. (2020), a target sparsity is enforced via a Lagrangian term (Louizos et al., 2018). If the current sparsity is \(s\), the term, parametrized by a reference value \(t\) is

\[_{s}=_{1}(t-s)+_{2}(t-s)^{2}\]

\(_{1}\) and \(_{2}\) are also updated during training via gradient _ascent_ to keep the regularization tight. We vary the value of \(t\) throughout training, linearly increasing it from \(0\) to a target value, as outlined in Appendix A. Although it may be useful to think of \(t\) as a "target" sparsity, it is only a number. The runs usually converge to a value slightly below \(t\), so it is prudent to set it to a value _greater than_ 1--although \(s\) can then never reach the target value, it will be pushed to higher sparsities.

We have two sets of masks \(z\). The first set associates a \(0-1\) value \(z_{e}\) with each edge \(e(n_{1},n_{2})\) in the computational graph. The second set tags each _node_ of the graph \(n\) with a \(0-1\) value \(z_{n}\). The latter specifies whether a node is "active", i.e., producing output. In effect, the presence of an edge \(e(n_{1},n_{2})\) is determined by the binary mask

\[_{(n_{1},n_{2})}=z_{(n_{1},n_{2})} z_{n_{1}}\]

We initially only used edge masks but found that the method would have difficulty converging to high sparsities (i.e., end up at low sparsities). Introducing a second set of masks allows the process to eliminate many edges quickly, accelerating the removal of unimportant components. However, the lagrangian above only applies to the edge masks. This is fine since the node masks can only remove further edges, not introduce new ones on top of those chosen by the edge masks. The final loss is

\[=_{}+_{,s}\]

## 4 Experiments

### Experimental Setup

Methods.We compare **Edge Pruning** with a KL loss to **ACDC** and **EAP** in our experiments. Both are outlined in Section 2. We do not compare to other pruning-based methods, as Conny et al. (2023) found them to perform much worse than ACDC. We list the hyperparameters used in Appendix A. The experiments in this section are all performed on GPT-2 Small (117M).

Tasks.Prior works evaluate their methods on the same examples used to find circuits. In a departure from this convention, we separate each dataset into train, validation, and test splits, to avoid artifacts caused by overfitting. We use the following tasks.

* **Indirect Object Identification (IOI-t1 and IOI)**(Wang et al., 2023) is a task with instances of the format "_Friends Juana and Kristi found a mango at the bar. Kristi gave it to \(\) Juana_". Conny et al. (2023) use a version with a single template, which we refer to as **IOI-t1**--this version has \(50\) examples in each split. We also compare the methods on a variant (**IOI**) with \(30\) templates found on HuggingFace2. We randomly select \(200\) examples each for the train and validation splits, and \(36,084\) examples for the test split.

* **Greater Than (GT)**(Hanna et al., 2023) consists of examples of the format "_The war lasted from the year \(1743\) to \(17 xy\)_". The objective of the task is to place a greater probability on the continuations \(44,45,,99\) than \(00,01,,42\). Our dataset spans \(5\) templates, \(120\) choices for nouns, and the years \(1100\) through \(2199\). It has \(150\) examples in the train and validation splits, and \(12,240\) examples in the test split.
* **Gendered Pronoun (GP)**(Athwin et al., 2023) consists of statements of the form "So Evan is a really great friend, isn't \(\) he". We use the templates from the original Colab notebook used by Athwin et al. (2023), but generate more examples as they only work with \(5\). We use the top \(1,000\) most popular baby names for boys and girls each in the year \(2000^{3}\) to generate a dataset with \(150\) train and validation examples each, and \(378\) test examples.
* **Tracr**(Lindner et al., 2023) compiles programs written in the RASP (Weiss et al., 2021) programming language into few-layer Transformers. We evaluate Edge Pruning on how well it recovers ground-truth circuits for two Tracr programs--xproportion (proportion of x's in the prefix) and reverse (reversing a list). Both tasks were discussed in Weiss et al. (2021) and used by Conmy et al. (2023) in their evaluation.

Evaluation.A circuit is faithful to model behavior on a task if we can corrupt all model edges outside the circuit while retaining the model's outputs (Hanna et al., 2024). We corrupt non-circuit edges with interchange ablation and evaluate the methods' faithfulness as the **KL divergence** between model and circuit outputs. Specifically, we corrupt an example by swapping the placeholder value in the same template with a random example from the dataset. We appraise the circuits' performance on IOI-t1, IOI, and GP via the **Logit Difference**\( P()- P()\) between the correct and misleading name/pronoun. For GT, we evaluate the **Probability Difference**\(P(yy+1:99)-P(00:yy-1)\) between the correct and incorrect ranges. All metrics on GT work with predictions restricted to the set \(\{00,01,,99\}\). We always take unrestricted predictions over the entire model vocabulary on other tasks. All non-Tracr experiments use a GPT-2 Small model. Appendix B evaluates additional metrics--including circuit overlap with manually found circuits.

Figure 2: The faithfulness of the methods, given the KL divergence between the model and obtained circuits (_lower is better_). On IOI-t1 and GP, Edge Pruning is competitive at low sparsities and better at high sparsities. It outperforms both ACDC and EAP by a significant margin on IOI and GT.

### Results

This section compares the three methods on our primary faithfulness and performance metrics. We report additional metrics in Appendix B, and Appendix F shows some circuits found by Edge Pruning.

Edge Pruning outperforms prior methods on more complex tasks.Edge Pruning is competitive on IOI-t1 and GP in terms of faithfulness at low sparsities, and slightly better at higher sparsities (Figure 2). It is considerably more faithful on IOI and GT than both ACDC and EAP, especially at higher sparsities. In particular, ACDC does worse than randomly choosing between the two names (KL divergence \(0.69\)) at high sparsities on IOI, whereas Edge Pruning remains better. We hypothesize that the relative simplicity of IOI-t1 and GP--one template or small output space (he/she)--renders local (ACDC) or first-order (EAP) approximations good proxies, potentially explaining the edge of Edge Pruning on IOI and GT. A similar trend is seen in performance (Figure 3): Edge Pruning finds better-performing circuits on all four tasks. Specifically, on IOI, Edge Pruning finds a circuit of \(98.8\%\) sparsity that is as faithful and performs as well as the one found by ACDC at \(96.8\%\) sparsity--using over \(2.65\) fewer edges. Interestingly, EAP scales better to higher sparsities than ACDC on GT, delivering respectable performance even at \(99.5\%\) sparsity.

Edge Pruning can scale to 100K examples.We investigate how the methods scale to more examples at representative sparsities. To this end, we create a large version of the IOI dataset's train

    &  &  &  &  \\   & & **KL \(\)** & **Time (s) \(\)** & **KL \(\)** & **Time (s) \(\)** & **KL \(\)** & **Time (s) \(\)** \\  ACDC & 96.6 \(\) 0.1 & 0.92 & 18,783 & 0.88 & 40,759 & - & - \\ EAP & 96.6 \(\) 0.1 & 3.47 & **21** & 3.66 & **43** & 3.78 & 12,260 \\ Edge Pruning & 96.6 \(\) 0.1 & **0.25** & 2,756 & **0.22** & 2,931 & **0.20** & **3,042** \\   

Table 1: Scaling to a larger IOI dataset: ACDC improves with more examples but its runtime scales prohibitively. EAP is fast but cannot perform as well. Edge Pruning scales effectively to \(100\)K examples, where it is the fastest and most faithful method. All runs use one NVIDIA H100 GPU.

Figure 3: Comparison of circuit performance between methods. We report the Logit Difference \( P()- P()\) for IOI-t1, IOI and GP, and the probability difference \(P(yy+1:99)-P(00:yy-1)\) for GT. Higher is better for all plots. Edge Pruning finds better-performing circuits on all four tasks. The dashed line indicates the performance of the full model.

split with 100K examples. We hold the number of gradient descent steps for Edge Pruning fixed (Appendix A). Although its runtime would scale linearly with more epochs, at 100K examples all approaches see almost all examples once.4 Thus, the time reported in Table 1 represents the relative overhead of each method. ACDC shows clear improvements with more examples, but cannot scale well due to prohibitive runtime. EAP, on the other hand, is fast even with more examples. However, it underperforms the other two methods significantly. Edge Pruning efficiently uses more examples and demonstrates both the least runtime and the highest faithfulness by far with \(100\)k examples. We therefore conclude that Edge Pruning is a good fit for complex or mixture distributions where more examples may be needed to specify model behavior.

Edge Pruning finds ground-truth circuits in Tracr programs.To check if Edge Pruning can find the ground-truth circuits, we use Tracr (Lindner et al., 2023) to compile two example programs--xproportion and reverse--as Transformers. The former yields a 2-layer Transformer that outputs, at each position, the fraction of x's seen so far. The latter yields a 3-layer Transformer that can reverse lists. We use zero ablation following Conmy et al. (2023) (more details in Appendix A). Edge Pruning achieves perfect reconstruction of both circuits (Figure 4).

Edge Pruning is robust to variance in random initializationAppendix D finds that both the resulting sparsity and the faithfulness of the circuits found by Edge Pruning are remarkably consistent across different random initializations of masks. We also investigate there the question of whether multiple different circuits can exist for a given task, and if Edge Pruning can find them.

## 5 Case Study: Scaling to 13B Parameters

We have seen that Edge Pruning can scale efficiently with more examples. We next investigate if it can scale with _model size_. This is increasingly important, given the recent interest in interpreting multi-billion parameter models (Lieberum et al., 2023; Prakash et al., 2024). Current methods used to interpret such models, while undeniably indispensable, have limitations: path patching (Goldowsky-Dill et al., 2023) identifies important subsets of components but falls short of producing edge-level circuits. Distributed Alignment Search (Geiger et al., 2024; Wu et al., 2023) can verify proposed symbolic execution graphs and align them with the model but requires prior knowledge of the correct symbolic graph, which is nontrivial to obtain.

On the other hand, pruning can scale to large models using model parallelism (Xia et al., 2024). We thus apply Edge Pruning to a case study on CodeLlama-13B (Roziere et al., 2024)--a model over \(100\) larger than GPT-2--with a real task. We are inspired by Prakash et al. (2024), who compare base and fine-tuned LMs and find that finetuning enhances existing mechanisms. Instead of comparing base and fine-tuned models, we compare mechanisms in the _same_ model with different prompting schemes. Specifically, _we ask whether the same mechanisms underlie (zero-shot) instruction prompted

Figure 4: The canonical ground-truth circuits for the Tracr-compiled xproportion and reverse programs. Edge Pruning recovers both circuits perfectly.

and few-shot behavior_ for the task-model pair we study. This case study serves a dual purpose. It demonstrates the scalability of Edge Pruning as a method. It also illustrates how circuit-finding methods may fit into the interpretability arsenal. We are interested in three research questions: (RQ1) Can Edge Pruning find edge-sparse circuits in a 13B model? (RQ2) To what extent do the circuits for instruction and few-shot prompting share the same edges? (RQ3) Does the instruction-prompted circuit perform well when used in a few-shot manner, and vice versa?

Task and model setup.We work with the task _Boolean Expressions_ from the BBH [Suzgun et al., 2022] benchmark suite. This task consists of instances of the form "(_(not False) and False) or (False and True) is \(\) False_". The original dataset only has \(250\) examples, so we programmatically generate an in-house version of the task. Our dataset has \(3840,767\), and \(3070\) examples in the train, validation, and test splits respectively. Each instance has between \(3\) and \(6\) literals, with a maximum nesting depth of \(3\) and at most \(2\) consecutive _nots_. We use \(3\) demonstrations for the few-shot setting. The prompts used for the instruction-prompted and few-shot settings are provided in Appendix E. Our model is the instruction-finetuned version of CodeLlama-13B.5 It achieves accuracies of \(82\%\) and \(89.25\%\) in the instruction-prompted (IP) and few-shot (FS) settings, respectively.

(RQ1) Edge Pruning produces extremely sparse circuits.We next apply Edge Pruning to the described settings. We isolate one circuit when instruction prompting and one with the few-shot prompt (hyperparameters in Appendix A, which also highlights other optimizations like distributed training and gradient checkpointing). The circuit discovered in the IP setting has \(1,041\) edges, corresponding to a \(99.97\%\) edge sparsity. That discovered in the FS setting has \(1,464\) edges, equivalent to \(99.96\%\) edge sparsity. The discovered circuits are evaluated in Table 2. Despite using less than \(0.04\%\) of the edges, the circuits closely match the performance of the full model--the few-shot circuit achieves an accuracy of \(87.25\%\) and performs within \(2\%\) of the full model (when prompted few-shot). The instruction-prompted circuit is accurate within \(2.75\%\) of the full model.

(RQ2) The circuits have a high overlap, and their intersection performs well.We appraise the intersection of the IP and FS circuits next. The two circuits share \(653\) edges, accounting for \(62.7\%\) of the edges of the sparser (instruction prompted) circuit--this corresponds to an intersection over \(1,700\) larger than expected by random chance. We further evaluate the circuit formed by this intersection in the instruction prompted and few-shot settings (Table 2). It performs well in the instruction prompted setting, and worse than the model (but still significantly above chance) when prompted few-shot.

(RQ3) The circuits demonstrate strong performance in cross-evaluation.We note from Table 2 that the circuit found with few-shot prompting shows strong performance even when instruction prompted. Analogously, the instruction-prompted circuit also performs well in the fewshot setting.

Our case study suggests that the same mechanism (as represented by the intersection above) explains a large part of the performance in both settings--i.e., they do not proceed via disjoint mechanisms. However, the performance gap between the FS and IP \(\) FS circuits is still sizable. Further, we see modest drops in cross-evaluation--e.g., from \(87.25\%\) when evaluating the FS circuit few-shot to \(75.75\%\) in the instruction prompted setting. This suggests that additional components are needed to

    &  &  &  \\   & & **Instr. prompted** & **Fewshot** & **Instr. prompted** & **Fewshot** \\  Full model & 3872820 & 82.00 & 89.25 & 100.00 & 100.00 \\ Instruction prompt (IP) & 1041 & 79.25 & 74.50 & 90.00 & 79.00 \\ Fewshot (FS) & 1464 & 75.75 & 87.25 & 84.50 & 91.25 \\  IP \(\) FS & 653 & 72.50 & 68.25 & 79.75 & 72.50 \\   

Table 2: Edge pruning finds circuits with 0.03-0.04% of the edges in CodeLlama-13B that match the performance of the full model. The circuits perform well in cross-evaluation and overlap highly, hinting that the same mechanisms explain large parts of instruction-prompted and few-shot behavior.

complete the picture. A complete mechanistic description of the components in the two circuits is an exciting avenue for future work, but beyond the scope of this case study.

Manual analysis of the CodeLlama-13B circuit.Interpreting a circuit in such a large model--even if very sparse-- remains a challenging task. We isolate a small region of the circuit and identify curious behavior in it in Appendix F, leading to an intriguing conjecture. Nonetheless, we believe that a thorough study requires more analysis, which is beyond the scope of this paper (but makes for exciting future work).

## 6 Related Work

Circuits.By reducing a large model to a sparse subgraph, circuits help interpret internal model computations (Olah et al., 2020; Elhage et al., 2021), and several visualization tools have been developed to aid this process (Sakarvadia et al., 2023; Katz and Belinkov, 2023; Tufanov et al., 2024). Circuits were originally found manually (Hanna et al., 2023; Athwin et al., 2023), but this has recently been automated by tools like ACDC (Conmy et al., 2023). ACDC uses activation patching (Vig et al., 2020) to knock out unimportant edges. Other approaches instead estimate the importance of each edge via attribution scores (Nanda, 2022); this approach was used by EAP (Syed et al., 2023). Ferrando and Voita (2024) use attribution patching to identify domain-specific model components in Llama-2-7B. Kramar et al. (2024) note that attribution patching may lead to incorrect approximations, and propose a variant with reduced error. In concurrent work, Hanna et al. (2024) argue that faithfulness metrics are better for evaluating circuits than measuring overlap with manual circuits. Recent work has explored other notions of a circuit. Inspired by the fact that Sparse Autoencoders (SAEs) can find human-interpretable features in LM activations (Cunningham et al., 2023), Marks et al. (2024) find circuits over these features. Wu et al. (2023) align computation in Alpaca (Taori et al., 2023) with a proposed symbolic algorithm (Geiger et al., 2024). Our method is orthogonal to these developments.

Pruning.Pruning (LeCun et al., 1989) drops parameters or layers of a language model for space efficiency and potential speedups. _Structured pruning_(Wang et al., 2020; Xia et al., 2022) imposes some regularity on the resulting subnetworks, such as an equal fraction of preserved parameters in each layer. Doing so allows it to achieve substantial speedups on GPU hardware at the cost of lower compression. In contrast, unstructured pruning (LeCun et al., 1989; Hassibi and Stork, 1992) does not impose such constraints. _Channel pruning_(He et al., 2017) is a form of structured pruning that prunes input channels in vision models, which has been adapted for neural architecture search (e.g. Li et al., 2022). Pruning has occasionally been used as part of an interpretability effort, but mostly at the level of neurons (Michel et al., 2019; Jain et al., 2023), or less commonly, attention heads/MLPs (Cao et al., 2021). Our work finds circuits by pruning the edges between components instead.

## 7 Conclusions

In this paper, we introduce Edge Pruning to find circuits by pruning edges between components. We find that it discovers sparse, faithful circuits, and we demonstrate its scalability to large datasets and large models. We close by discussing its limitations, and how future work may address them.

Limitations.We acknowledge that with small datasets, approximation-based approaches like EAP are faster than Edge Pruning. Circuit discovery with Edge Pruning may also require more GPU memory than these methods--especially at scale--where we use \(32\) H100 GPUs for CodeLlama-13B (Appendix A). Future work may precede Edge Pruning with a fast, approximate method like EAP to balance efficiency and performance. We note that even at very high sparsities, circuits for large models can still have hundreds of edges, and their full interpretation remains challenging. Further automating interpretability (Bills et al., 2023) is a compelling avenue for future research. Finally, we note that even with perfect faithfulness to the model outputs, a circuit can misrepresent the necessary computations in the full model, thus leading to interpretability illusion (Makelov et al., 2024). Better metrics are needed to reveal these possibilities in practice.

Societal and ethical impact.Our work aims to facilitate the process of understanding and explaining large foundation models, which is crucial for their continued safe development and deployment. We do not foresee Edge Pruning being used towards adverse societal or ethical ends.