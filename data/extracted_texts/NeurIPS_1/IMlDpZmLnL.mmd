# A Comprehensive Analysis on the Learning Curve

in Kernel Ridge Regression

 Tin Sum Cheng, Aurelien Lucchi

Department of Mathematics and Computer Science

University of Basel, Switzerland

tinsum.cheng@unibas.ch, aurelien.lucchi@unibas.ch

Anastasis Kratsios

Department of Mathematics

McMaster University and The Vector Institute

Ontario, Canada

kratsioa@mcmaster.ca

&David Belius

Faculty of Mathematics and Computer Science

UniDistance Suisse

Switzerland david.belius@cantab.ch

###### Abstract

This paper conducts a comprehensive study of the learning curves of kernel ridge regression (KRR) under minimal assumptions. Our contributions are three-fold: 1) we analyze the role of key properties of the kernel, such as its spectral eigen-decay, the characteristics of the eigenfunctions, and the smoothness of the kernel; 2) we demonstrate the validity of the Gaussian Equivalent Property (GEP), which states that the generalization performance of KRR remains the same when the whitened features are replaced by standard Gaussian vectors, thereby shedding light on the success of previous analyzes under the Gaussian Design Assumption; 3) we derive novel bounds that improve over existing bounds across a broad range of setting such as (in)dependent feature vectors and various combinations of eigen-decay rates in the over/underparameterized regimes.

## 1 Introduction

Kernel ridge regression (KRR) is a central tool in machine learning due to its ability to provide a flexible and efficient framework for capturing intricate patterns within data. Additionally, it stands as one of the earliest endeavors in statistical machine learning, with ongoing research into its generalization properties . Over the past few years, kernels have experienced a resurgence in importance in the field of deep learning theory , partly because many deep neural networks (DNNs) can be interpreted as approaching specific kernel limits as they converge .

One central topic in machine learning theory is the **learning curve** of the regressor in the fixed input dimensional setting as the sample size grows to infinity. Formally: let \(n\) be the sample size, \(=(n)\) be the ridge regularization parameter depending on \(n\) and \(_{n}\) be the test error/excess risk of the ridge regression. For large \(n\), the test error \(_{n}\) should decay with \(n\) as \(_{n}=_{n,}(g(n))\) for some function \(g:\) such that \(g(n)0\). The decay of \(g\) with respect to \(n\) provides an upper bound on the learning curve of the ridge regressor and will be the main focus of this paper. To conduct our analysis, we concentrate on several crucial properties of the kernel, including its spectral eigen-decay and the characteristics of the eigenfunctions, which we will elaborate on next.

Properties of the eigenfunctionsIn a series of studies , the feature vectors are substituted by random Gaussian vectors following the Gaussian Design (GD) Assumption, and thelearning curve is derived using the Replica method. In a separate body of research [29; 30; 31], it is demonstrated that _similar_ learning curves occur for Holder continuous kernels under an assumption called the Embedding Condition (EC) (see Section A for more details). Consequently, there is a fundamental mismatch between the distribution of the feature vector and the Gaussian random vectors used in [10; 16; 34]: in the former case, each coordinate is highly dependent on the others, whereas in the Gaussian Design case, each coordinate operates independently from the others. Astonishingly, however, both settings share similar learning curves. This phenomenon, initially identified by [21; 3; 42; 20], is termed the _Gaussian Equivalence Property_. This prompts the question:

_Q1: When and why does the Gaussian Equivalence Property exist?_

Spectral eigen-decayMany recent papers [10; 35; 44; 36] have attempted to characterize the test error solely by the (kernel) eigenspectrum decay. It is for instance common to differentiate between different eigenspectrum decays: [29; 30; 31] assumes the embedding condition (EC) and Holder continuity to kernel with polynomial eigen-decay;  assumes either polynomial or exponential eigen-decay (with noiseless labels) under the Maximum Degree-of-Freedom (MaxDof) Assumption;  assumes some concentration and the so-called hypercontractivity on eigenfunctions.

However,  pointed out that the characterization of generalization performance solely by the spectral eigen-decay might oversimplify the generalization performance of ridge regression. In relation to the second question, we further ask:

_Q2: Under what conditions is the generalization error fully determined by the eigen-decay?_

Additional assumptions and settingsIn addition to the two properties above, other hyperparameters or settings, including capacity of the kernels/feature vectors, the ridge regularization decay, the source condition of the target function, the noise level in the output label, and the amount of over-parameterization, play an important role in the analysis of the learning curve of ridge regression. Within the present body of research, various papers establish bounds on the test error of KRR across diverse assumptions and settings (we refer the reader to Section A for further elaboration). It is therefore of significant interest to ask:

_Q3: Is there a unifying theory explaining the generalization under minimal assumptions?_

ContributionsWe address questions _Q1-3_ through the following contributions:

1. **Unified theory:** We provide a unifying theory of the test error of KRR across a wide variety of settings (see Subsection 2.2 and Table 1 in Section 3).
2. **Validation and GEP:** We show that the generalization performance with independent (Gaussian) features and dependent (kernel) features coincides asymptotically and it solely depends on the eigen-decay under strong ridge regularization, hence validating the Gaussian Equivalent Property (GEP) (see Subsection 3.2).
3. **New and sharpened bounds:** We provide novel bounds of the KRR test error that improve over prior work across various settings (see Subsections 3.2).
4. **Smoothness and generalization:** We relate the spectral eigen-decay to kernel smoothness (see Appendix B.2) and hence to the kernel's generalization performance.

## 2 Setting

In this section, we introduce the basic notation for ridge regression, which includes high-dimensional linear regression and kernel ridge regression as special cases.

### Notations

Suppose \(p\{\}\). Let \(=(x_{k})_{k=1}^{p}^{p}\) be a random (feature) vector sampled from some distribution \(\) on \(^{p}\). Let \(n\) be an integer and denote by \(_{1},...,_{n}\) n i.i.d. draw of \(\). Denote the input matrix \(^{n p}\) to be a matrix with rows \(_{i}^{}\). By fixing an orthonormal basis, we assume that the covariance matrix is diagonal:

\[}}{{=}}_{} [^{}]=(_{1}, _{2},...,_{p})^{p p},\]where the eigenvalues \(_{1}_{2}..._{p}>0\) is a decreasing sequence of positive numbers. 1

We also assume that \(^{n}\) is an output vector such that

\[=^{*}+ \]

where \(^{*}^{p}\) is a deterministic vector, \(^{n}\) is a random vector whose entries are i.i.d. drawn from a centered random variable \(\) with variance \([^{2}]=^{2}<\) and independent to \(\).

Then the linear regressor

\[}()}}{{=}}^{}(^{}+n_{n})^ {-1} \]

is the minimizer of the empirical mean square loss (MSE) problem:

\[_{^{p}}\| -\|_{2}^{2}+\| \|_{2}^{2}, \]

where \( 0\) is the ridge2. This paper focuses on bounding the test error, with which we can analyse the learning curve of the regressor. To do so, we use the following well-known bias-variance decomposition.

**Definition 2.1** (Bias-variance decomposition).: Consider input-output pairs \((,)\) of sample size \(n\) and a ridge \( 0\). Define the test error \(\) to be the population mean squared error between the regressor and the true label averaged over noise.

\[}}{{=}}_{,}[(^{}}( )-^{}^{*})^{2}] \]

Note that \(\) is a random variable depending on the samples \((,)\) and the ridge \( 0\). Hence, we can also view \(=_{n}\) as a random variable indexed in \(n\), where the samples \((,)\) are \(n\) i.i.d. drawn input-output pairs and \(\) is chosen to depend on \(n\).

We decompose the test error into a bias \(\) and variance \(\), which is typical for most KRR literature :

\[=+ \]

where \(}}{{=}}_{}[(^{}}( ^{*})-^{}^{*})^{2} ],\ }}{{=}}_{ ,}[(^{}}())^{2}].\)

_Remark 2.2_ (Noiseless labels).: If there is no noise in the label, that is, \(=0\), the test error \(\) is simply the bias \(\). Hence, the analysis of the bias term \(\) in this paper is directly applicable to the noiseless labels setting.

We now summarize the combinations of assumptions and settings made in this paper.

### Assumptions and Settings

Polynomial/exponential eigen-decayWe consider two types of spectral decay rates, namely, _polynomial_ and _exponential_ decay rates, because: 1) polynomial eigen-decay is, roughly speaking, equivalent to the case where the RKHS is comprised of at most finitely many continuous derivatives; 2) the exponential eigen-decay is, possibly up to a canonical change in the relevant function space, equivalent to the case where the RKHS consists of smooth (infinitely differentiable) functions. For the formal definition of the eigen-decay, see Assumptions (PE) and (EE). For further details and explanation on the relationship between eigen-decay and smoothness, we refer the reader to Section B.

Source conditionMany previous works  include the so-called source condition as assumptions on the target. If the task is proper, that is, \(^{*}=^{1}\), we have \(s 1\). More generally, a larger source coefficient \(s\) implies a smoother target \(^{*}\) in the RKHS \(\).

**Definition 2.3** (Interpolation space).: Let \(s 0\) be a real number. Define the interpolation space

\[^{s}}}{{=}}\{ ^{p}:\|\|_{^{1- s}}<\}.\]

**Assumption (SC)** (Source Condition).: _The source coefficient of a target coefficient \(^{*}\) is defined as_

\[s=\{t>0:^{*}^{t}\}.\]

See Subsection A.5 for more elaborations for the source coefficient \(s\) in polynomial or in exponential decay.

Strong/weak ridgeWe set the ridge \(=(n) 0\) to depend on the sample size \(n\). The ridge is considered strong (relative to the eigen-decay) if \(_{\{n,p\}}\), that is, if \(/_{\{n,p\}}0\); otherwise, it is considered weak. Intuitively, the ridge is weak when it is negligible compared to the entries in the kernel matrix, effectively making it ridgeless.

To summarize the assumptions discussed previously, let \((_{k})_{k=1}^{p}\) be the eigenvalues of the kernel \(K\), and \(^{*}}}{{=}}(_{k}^ {*})_{k=1}^{p}\) the coefficients of the target function being learned in the eigen-basis defined by \(K\). Then we assume either of the following assumptions:

**Assumption (PE)** (Polynomial Eigen-decay).: _Assume that \(_{k}=_{k}(k^{-1-a})\), \(|_{k}^{*}|=_{k}(k^{-r})\), \(=_{n}(n^{-b})\) for some constants \(a,b,r>0\), where \(a+2 2r\) unless specified. 3 Hence, if Assumption (SC) holds, the source coefficient is \(s=\). We call the ridge \(\) strong if \(b(0,1+a]\), and weak if \(b(1+a,]\), under the convention that \(b=\) implies \(=0\)._

**Assumption (EE)** (Exponential Eigen-decay).: _Assume that \(_{k}=_{k}(e^{-ak})\), \(_{k}^{*}=_{k}(e^{-rk})\), \(=_{n}(e^{-bn})\) for some constants \(a,b,r>0\), where \(a 2r\) unless specified. 4 Hence, if Assumption (SC) holds, the source coefficient is \(s==+1\). We call the ridge \(\) strong if \(b(0,a]\), and weak if \(b(a,]\), under the convention that \(b=\) implies \(=0\)._

Generic/independent featuresOur analysis centers on the assumptions regarding feature vectors, with a focus on the dependencies between coordinates, particularly exploring two cases:

1. Generic features (GF): include the cases where the feature vectors are dependent on each other, for example, the feature vectors from the following kernels: * dot-product kernels on hyperspheres; * kernels with bounded eigenfunctions; * radial base function (RBF) and shift-invariant kernels; * kernels on hypercubes, satisfy Assumption (GF). Most previous literature  have assumptions that only a proper subset of the above kernels satisfies. Therefore, we believe that we are operating under the minimal assumptions that exist in the field.
2. Independent features (IF): replace the feature vector with sub-Gaussian random vector with independent coordinates. A special case is the Gaussian Design assumption (GD) used in literature .

For further explanations regarding the assumptions, we refer the reader to Section A.

## 3 Main result

We first present an overview of the test error bounds across various properties, assumptions, and regimes. Our main results, summarized in Table 1, describe the learning curve in the over-parameterized regime, in terms of the bias \(\) and variance \(\) decomposition (see Equation (5)). Then, we will discuss the implications of our results in depth.

### Overview

Table 1 summarizes many of our results in the over-parameterized regime under various combinations of the assumptions described in subsection 2.2. The bounds are expressed in terms of the sample size \(n\) as \(_{n}()\) or \(}_{n}()\) (ignoring logarithmic terms). Whenever we can also prove a matching lower bound, we replace \(_{n}()\) with \(_{n}()\). We write \(()_{+}}}{{=}}\{,0\}\).

Before delving into the detailed discussion of our comprehensive results in Subsection 3.2, let us highlight some important observations from Table 1.

Asymptotic boundsThe upper bound illustrates the asymptotic relationship between the test error and sample size \(n\) as well as the following constants: \(a\) related to the eigen-decay, \(b\) related to the ridge, \(r\) related to the target function and \(^{2}\) related to the label noise.

Independent feature (IF) versus generic features (GF)The bounds in both cases coincide under strong ridge (see the left columns of Table 1); meanwhile, under weak ridge (see the right columns of Table 1), the bounds with generic features are looser than those with independent features. In Subsection 3.2, we will explain the necessity of this difference and hence showcase the previous limitations in the literature, which has widely adopted the Gaussian Design Assumption (GD) under the weak ridge/interpolation regime.

Novel bound of bias under weak ridgeA notably sophisticated bound (on the upper right corner of Table 1)

\[=(n^{-(1+a)s}),&s>1\\ }(n^{-(\{2(r-a),2-a\})_{+}}),&s 1 \]

is novel to the best of our knowledge. Our improvement compared to previous literature  under various eigen-decay (in terms of \(a\)) and target coefficients' decay (in terms of \(r\)) is shown in Figure 1. By comparison, we can see that the decay of our novel bound in Equation 6 is faster than previous results. Also, we prove that the upper bound in the middle green region,where \(s(1,2)\), is sharp.

Experimental validations of the results in Table 1 are given in Section 6.

In the under-parameterized regime, the bias \(\) and variance \(\) terms can be bounded similarly as in over-parameterized regime. We postpone the details of these results to Section F.

### Detailed discussion

In this subsection, we elaborate more on the details of our results shown in Table 1.

Independent and generic featuresTable 1 indicates that the test error exhibits the same upper bound with either independent or generic features under strong ridge conditions in the over-parameterized regime. This similarity arises from the bounds in both cases being derived from the

   &  &  \\  Feature &  &  &  &  \\  & \)} & \((n^{-bs})\) & \((n^{-bs})\) & \((n^{-(1+a)s})\) & \((n^{-(1+a)s}),&s>1\\ (n^{-(\{2(r-a),2-a\})_{+}}),&s 1\) \\  & \)} & \((^{2}n^{-1+})\) & \((^{2}n^{-1+})\) & \((^{2})\) & \(}(^{2}n^{2a})\) \\  & & \((^{-bsn})\) & \((^{-bsn})\) & \((^{-bsn}),\,s>1\) & \((^{-bsn}),\,s>1\) \\   & & \((^{2}n^{-1+})\) & \((^{2}n^{-1+})\) & & catastrophic overfitting \\  

Table 1: _Learning curve in the over-parameterized regime (\(p>n\)): \(n\) is the sample size, \(a,r>0\) define the eigen-decay rates of the kernel and target function, \(b>0\) controls the decay rate of the ridge regularization parameter (Assumptions (PE) and (EE)), \(^{2}}}{{=}} [^{2}]\) is the noise level, and \(s>0\) is a technical parameter often determined by \(a\) and \(r\) (e.g. under Assumption (SC)). Here \(}}{{=}}\{s,2\}\)._

Results in blue indicate either previously unstudied regimes or improvements in available rates in a studied regime. See Table 6 for more comparisons and Subsection 2.2 for details on various settings.

same Master inequalities that we will introduce later (see Section 4). However, under weak ridge conditions, the empirical kernel spectrum displays qualitative differences, as reported in . From Figure 2, we can see that \(=(1)\) with Laplacian kernel in the left plot and \(\) diverges with the neural tangent kernel (with 1 hidden layer) in the right plot. Hence under weak ridge and polynomial eigen-decay, the case distinction of the bound

\[=(^{2}),&,\\ (^{2}n^{2a}),& \]

in Table 1 is necessary, as Assumption (GF) includes the cases of Gaussian Design (GD) (or more generally independent features (IF)) and Laplacian kernel which yields \(=(1)\), the so-called tempered overfitting from ; as well as the case of neural tangent kernel (NTK) which yields \(\), the so-called catastrophic overfitting. In particular, our proof shows that the Gaussian Equivalent Property (GEP) does not hold under weak ridge.

We are now prepared to address the first question posed in Section 1:

_Q1: When and why does the Gaussian Equivalence Property (GEP) exist?_

Figure 1: Phase diagram of the bound (Equation (6)) of the bias term \(\) under weak ridge and polynomial eigen-decay. \(_{k}=_{k}(k^{-1-a})\), \(|_{k}^{*}|=_{k}(k^{-r})\), for some \(a,r>0\). Our result (Propositions D.5+D.6+E.1) is on the left, which improves over previous result from  (Proposition D.6) on the right. On the left plot, the range of the source coefficient \(s=\) in Assumption (SC) is shown in gray font in each colored region.

Figure 2: Variance \(\) against sample size \(n\) for the Laplacian kernel (left) and the neural tangent kernel with 1 hidden-layer (right) defined on the unit 2-disk, validating Equation (7) where the variance with generic features (GF) can be as good as with independent features (IF) (\(=_{n}(1)\)) or qualitatively different (\(\)). See Section 6 for more details.

**The Master inequalities provide the same non-asymptotic bounds for both cases under a strong ridge.** However, GEP does not hold under weak ridge! 5

In particular, our work implies that previous works  under the Gaussian Design assumption (GD) can be applied only when the ridge is strong.

Upon finalizing the preparation for this paper, we became aware of concurrent research conducted by , which also concerns the Gaussian Equivalence Property (GEP) in the non-asymptotic setting. For more comparison between their assumptions and ours, we refer the reader to Section A.

Importance of ridgeTo address the second question posed in Section 1, it is evident that in either the under-parameterized setting with any ridge (see Section F) or the over-parameterized regime with a strong ridge (see Table 1), the bounds for both \(\) and \(\) remain the same, irrespective of the features:

_Q2: Under what conditions is the generalization error fully determined by the eigen-decay?_

**Either (i) in the under-parameterized setting; or (ii) with a strong ridge in the over-parameterized regime.**

Several results  have suggested that the test error bound can be characterized by the covariance spectrum \(\), but they implicitly require the ridge \(>0\) to be larger than some threshold. This paper clearly demonstrates the necessity of the presence of a strong ridge in such analyses.

From Table 1, we can see that the eigen-decay also affects the test error qualitatively. As mentioned in , the bias term \(\) decays polynomially (or, respectively, exponentially) when the eigen-decay is polynomial (or, respectively, exponential). However, we prove that \(\) decays only polynomially at a rate at most \(()\), regardless of the eigen-decay. Hence, in a noisy setting with polynomial eigen-decay, one can find an optimal ridge \(=_{n}(n^{-}})\) to balance both terms \(\) and \(\) as in . In contrast, in noisy settings with exponential eigen-decay, \(\) dominates the test error.

The bound of the variance term \(\) with exponential eigen-decay under weak ridge is omitted in Table 1 due to the so-called catastrophic overfitting phenomenon observed in .

Improved upper boundDuring our research, we discovered that we can improve the upper bound of \(\) (Equation 6) in the over-parameterized regime with polynomial decay, weak ridge, and Assumption (GF) by adapting the result from  and integrating it with the insights from  (see the upper right corner in Table 1 or 6, and Figure 1 for visualization).

**Theorem 3.1** (Improved upper bound).: _Suppose Assumption (GF) holds. Assume the eigen-spectrum and the target coefficient both have polynomial decay, that is, \(_{k}=_{k}(k^{-1-a})\) and \(_{k}^{*}=_{k}(k^{-r})\). Let \(s=\) be the source coefficient defined in Definition \((SC)\). Then the kernel ridgeless regression has the following learning rate for the bias term \(\):_

\[=_{n}(n^{-(r-a)_{+}})&s<1;\\ _{n}(n^{-(2r+a)})&1 s 2;\\ _{n}(n^{-2(1+a)})&s>2.\]

_where \(n\) is the sample size and \(()_{+}}}{{=}}(,0)\)._

For a detailed explanation of our improvements, refer to our novel Proposition D.5 and the known Proposition D.6 from  in Section D.

Lower bound of test errorIt is of theoretical interest to provide lower bounds as well. For independent features, we can prove that the upper bound is tight using the result from , generalizing the result from , which only showed upper bounds in Gaussian Design Assumption (GD). However, for generic features, we can only provide lower bounds in some but not all settings, using the result from . We summarize our results in Table 2.

See Section E for details on our proof of matching lower bounds. We note that there is some KRR literature, such as [31; 36; 19], that discusses matching upper and lower bounds of test error under more assumptions, which is beyond the scope of this paper. For a comparison of assumptions in different papers, see Section A.

Finally, we summarize the above discussion by answering the third question we raised in Section 1:

_Q3: Is there a unifying theory on the generalization performance under minimal assumptions?_

**Yes, this paper considers assumptions (IF) and (GF) which cover a wide range of kernel settings under any regularization and source conditions.**

## 4 Proof sketch

All the above results in Table 1 can be derived using the following proof strategy:

1. We prove a concentration result on the whitened features \(\) under Assumptions \(()\) or \(()\) (see Section G).
2. Using the above result, we bound the condition number of the (truncated) kernel matrix (see Section G) as in [7; 14], which will be used to bound the test error in the next step.
3. Combining the bound of the condition number with the result from [6; 46], which we call the _Master inequalities_ (see the paragraph below for details), we can compute the non-asymptotic test error bound for various settings (see Section D).
4. In the over-parameterized regime, we derive the asymptotic behavior of the learning curves by plugging in the eigen-decay and the choice of the target function and ridge in the above non-asymptotic bound (see Section D).
5. Using the results from , we are able to show that the asymptotic upper bound for independent features \(()\) is tight (see section E).
6. For generic features \(()\), we only provide tight bound results in limited settings (see section E). As shown in Figure 2 in Section 3, the generic feature Assumption \(()\) includes a broad variety of features where a universal matching lower bound does not exist.
7. In the under-parameterized regime, the Master inequalities also give upper bounds for \(\) and \(\), which might not be tight if the ridge \(\) is strong. However, we present another way to obtain tight bounds without the Master inequalities (see Section F for more details).

We summarize the proof techniques with a flowchart in Figure 4.

Master InequalitiesWe will now briefly introduce the key inequality used in our analysis (further details can be found in the Appendix). We will use the subscript \(>k\) or \( k\) to refer to the submatrix of a matrix consisting of columns with index \(>k\) or \( k\) (see definition C.3). The analysis relies on two crucial matrices: \(_{k}=_{>k}_{>k}^{}+n_{n} ^{n n}\), defined for any \(k\), which is employed to partition the spectrum, and a whitened input matrix \(}}{{=}}^{-1/2}^{n p}\). Additionally, we require the following definitions.

**Definition 4.1** (Concentration coefficients ).: Define the quantities:

\[}}{{=}}_{>k}_{}+s_{1}(_{k})}{s_{n}(_ {k})};}}{{=}}( _{<k}^{}_{ k})}{s_{k}(_{ k}^{} _{ k})};}}{{=}} (_{ k}^{}_{ k})}{n},\]

where \(s_{i}()\) denotes the \(i\)-th largest singular value of the matrix.

   &  &  \\  Feature & & (IF) & (GF) & (IF) & (GF) \\ Poly (PE) or Exp (EE) & \(\) & \(\) & \(\) & \(\) & \(\) (when \(1 s 2\)) \\  & \(\) & \(\) & unknown & \(\) & \(\) because of Figure 2 \\  

Table 2: The table shows whether the lower bound is matching the upper bound deduced in this paper.

**Definition 4.2** (Effective ranks ).: Let \(k\). Define two quantities:

\[r_{k}}}}{{=}}[ _{>k}]}{\|_{>k}\|_{} }=^{p}_{l}}{_{k+1}},\ \ \ \ R_{k}}}}{{=}}[_{>k}]^{2}}{[_{>k}^{2}]}=^{p}_{l})^{2}}{_{l=k+1}^{p}_{l}^{2}}.\]

The Master inequalities provide upper bounds on the bias and the (scaled) variance term of the test error in the following form:

\[ (^{2}^{-1}+}{}) \|_{>k}^{*}\|_{_{>k}}^{2}+(^{2}^{-2}+ ^{2}^{-1})(_{k})^{2}}{n^{2}}\|_{ k }^{*}\|_{_{ k}^{-1}}^{2};\] \[/^{2} ^{2}(^{2}^{-1}+[_{>k}_{>k}^{2} _{>k}^{}]}{n[_{>k }^{2}]}()^{2}}{nR_{k}()}).\]

The term appearing in the above bound can be categorized into two distinct components: the "probably constant" part (highlighted in blue) and the "decay" part (highlighted in gray). The "probably constant" part consists of terms that, with high probability, are bounded both below and above by positive constants--these bounds represent a primary contribution of this paper. On the other hand, the "decay" part can be approximated using basic calculus, once a specific constant \(k\), smaller than the sample size \(n\), is selected. Together, these two parts allow us to derive the KRR learning rate for all combinations of eigen-spectra, features, and ridge parameters discussed in the preceding sections. Furthermore,  provides a corresponding lower bound under certain assumptions, demonstrating that the decay of the upper bound matches that of the lower bound. Establishing that Assumption (IF) satisfies these assumptions is another key contribution of this work. For the formal definitions of the terms in the above inequalities, we refer the reader to Propositions C.6 and C.7 in the appendix.

## 5 Related work

We briefly discuss related previous works and compare them with our results in the over-parameterized regime: (see Table 6 for more visual illustration)

1.  considered the upper bound of \(\) and \(\) in polynomial eigen-decay under any ridge and under the Gaussian Design Assumption. Our result proves _both_ the matching upper and lower bound with the same decay rate under a weaker assumption (IF). This implies that we validate the Gaussian Equivalence Property for Sub-Gaussian Design.
2.  proved tight upper bounds of \(\) and \(\) for Holder continuous kernels under polynomial eigen-decay, strong ridge and the so-called Embedding condition (EC).  recovered the upper bounds under Assumption (GF).
3. For polynomial decay under weak ridge,  provided a tight upper bound of \(\) when the source condition \(s>1\); while  provided a loose bound regardless of \(s\). Hence we modify the proof in  under Assumption (GF) instead and combine it with  to obtain a novel upper bound on the bias.
4.  also provided an upper bound of \(\) under polynomial decay, weak ridge and Assumption (GF).
5.  showed that \(\) is bounded above and below by positive constants under polynomial eigen-decay, weak ridge and Assumption (IF), and it exhibits the so-called tempered overfitting from .
6.  provided tight upper bounds of \(\) for both polynomial and exponential eigen-decay for kernels under the so-called Maximal Degree-of-Freedom (MaxDoF) Assumption. We recover their result under Assumption (GF) instead of Assumption (MaxDoF).
7. We apply the result from  and  to obtain a matching lower bound in some settings and strengthen our result from \(_{n}()\) to \(_{n}()\).

## 6 Experiments

Due to page constraints, this section focuses solely on experiments validating the Gaussian Equivalent Property (GEP). For detailed experiments on other contributions, refer to Section I.

We consider a simple example: let \(_{k}=()^{-1-a}\) and \(_{k}()=()\) such that \(\|_{k}\|_{L^{2}_{}}=1\) for \(=\); let \(^{*}_{k}=()^{-r}\). For \(p=\) and \(a=1\), the regression coincides with kernel ridge regression with kernel \(k(x,x^{})=\{x,x^{}\}\) defined on the interval \(\). Similar experiments have been conducted on this kernel \(k\) by [30; 33]. However, to simulate regression for independent features (IF), the feature rank \(p\) must be finite. In the following experiment, we choose \(p=2000\), and the sample size \(n\) ranges from 100 to 1000, with ridge parameter \(=()^{-b}\) where \(b[0,1+a]\).

In Figure 3, our experiment demonstrates the GEP, as the learning curves with kernel features (Sine features) and independent features (Gaussian features \((0,I_{p})\) or Rademacher features \((\{ 1\})^{p}\)) coincide and match the theoretical decay.

## 7 Conclusion

In this paper, we present a unifying theory and a comprehensive analysis of the learning curve of kernel ridge regression under various settings. We elucidate the coincidence of learning curves between the Gaussian Design (GD) setting (or more generally, independent features (IF)) and the kernel setting (or more generally, generic features (GF)), and validate the Gaussian Equivalence Property under strong ridge. In addition to recovering previous results, we also improve test error bounds under specific circumstances, thus filling a gap in the existing literature.

Future potential workOur results also raise several theoretical questions. For instance: i) The upper bound in several regions of the phase diagram in Figure 1 is not known to be sharp. It would be of interest to either improve it or find matching lower bounds in those regions; ii) Why is there a qualitative difference in overfitting with the Laplacian kernel and the neural tangent kernel, as shown in Figure 2? Can one further distinguish different cases in the generic feature Assumption (GF) to explain such differences? Additionally, how tight would the bound \(=(^{2}n^{2a})\) in Eq. (7) be?

Addressing these questions could provide deeper insights into the behavior of kernel ridge regression under various conditions and contribute to further advancing our understanding in machine learning.

Figure 3: _Decay of the bias term \(\) and the variance term \(\) under different ridge decays and target coefficient decays._ All features demonstrate the same theoretical decay, validating the GEP for independent features.