# Diffusion Twigs with Loop Guidance

for Conditional Graph Generation

 Giangiacomo Mercatali \({}^{}\)

HES-SO Geneve

University of Manchester

giangiacomo.mercatali@hesge.ch

&Yogesh Verma \({}^{}\)

Aalto University

yogesh.verma@aalto.fi

Andre Freitas

Idiap Research Institute

University of Manchester

NBC, CRUK Manchester Institute

andre.freitas@idiap.ch

Equal Contribution. Order decided via coin flip.Work done while at the University of Manchester

###### Abstract

We introduce a novel score-based diffusion framework named _Twigs_ that incorporates multiple co-evolving flows for enriching conditional generation tasks. Specifically, a central or _trunk_ diffusion process is associated with a primary variable (e.g., graph structure), and additional offshoot or _stem_ processes are dedicated to dependent variables (e.g., graph properties or labels). A new strategy, which we call _loop guidance_, effectively orchestrates the flow of information between the trunk and the stem processes during sampling. This approach allows us to uncover intricate interactions and dependencies, and unlock new generative capabilities. We provide extensive experiments to demonstrate strong performance gains of the proposed method over contemporary baselines in the context of conditional graph generation, underscoring the potential of Twigs in challenging generative tasks such as inverse molecular design and molecular optimization. Code is available at [https://github.com/Aalto-QuML/Diffusion_twigs](https://github.com/Aalto-QuML/Diffusion_twigs).

## 1 Introduction

Conditional graph generation is a fundamental problem in scientific domains such as _de novo_ drug design  and material design . However, searching for new molecules with desired physicochemical properties poses significant challenges to traditional brute-force methods due to the vast combinatorial spaces . With the advent of neural networks , deep generative models have emerged as a powerful tool for learning informative conditional representations of molecules, facilitating the development of _in silico_ methods for chemical design .

Score-based diffusion generative models (SGMs) and denoising probabilistic diffusion models (DDPMs)  have recently emerged as powerful techniques for training deep networks on graph-structured data, with applications spanning molecular design , molecular docking , molecular dynamics simulations , protein folding , and backbone modeling . Notably, diffusion models exhibit superior capabilities for _conditional_ graph generation, excelling in both discrete  and continuous  settings. The training of the mentioned conditional diffusion models is achieved by two types of diffusion guidance algorithms: _classifier-based guidance_, which involves training a separate property predictor model alongside the diffusion model; and _classifier-free guidance_, which integrates scores from both unconditional and conditional diffusion models. While these guidance techniques have been found to be effective, the algorithm design is not tailored to encompass the intricate hierarchical or multi-resolution elements inherent in conditional generation. Consequently, it is plausible that this inadequacy may contribute to suboptimal representations, particularly notable in tasks such as conditional graph generation. The recent success of hierarchical diffusion flows in various domains, such as modeling interactions between node and edge features , multi-resolution modeling , decision-making , and conditional image generation  underscores the need to integrate hierarchical information beyond the capabilities of classifier-based and classifier-free guidance.

We assert that conditional diffusion models for structured spaces, such as graphs, could be enhanced with _hierarchical conditional processes_. Specifically, rather than treating heterogeneous structural and label information uniformly within the hierarchy, we advocate for the co-evolution of multiple processes with _distinct roles_ (asymmetric). These roles encompass a primary process governing the structural evolution alongside multiple secondary processes responsible for driving conditional content. We aim to propose an alternative to existing conditional graph diffusion techniques (outlined in Table 1) by bestowing the models with finer control over two key aspects: 1) the evolution of structural graph components, including nodes and edges, and 2) the co-adaptation of the graph structure in conjunction with one or more associated properties.

Towards this objective, we present a novel diffusion framework for conditional generation named Twigs, drawing analogies from the trunk and offshoots of a tree. Concretely, we establish a central _trunk_ process governing a primary variable, which interacts with several _stem_ processes, each associated with a secondary variable. In contrast with classifier-free and classifier-based methodologies, a novel conditional mechanism, termed _loop guidance_, orchestrates information exchange between the trunk and the stem processes (refer to Figure 1). Our methodology facilitates the acquisition of flexible representations, capitalizing on the disentanglement of intricate interactions and dependencies. We formalize our framework by drawing upon the theory of denoising score matching  and leveraging tools derived from stochastic differential equations (SDEs) . The effectiveness of Twigs is substantiated through compelling empirical validation across various conventional constrained generation tasks, utilizing both molecular and generic graph datasets.

### Contributions

In summary, this paper makes the following key contributions:

Figure 1: **Overview of the proposed method (Twigs). We define two types of diffusion processes: (1) multiple _Stem_ processes \((s_{_{i}})\), which unravel the interactions between graph structure and single properties, and (2) the _Trunk_ process, which orchestrates the combination of the graph structure score from \(s_{}\) with the stem process contributions from \(s_{_{i}}\). During the forward process, the structure \(_{s}\) and the properties \(\{_{i}\}_{k}\) co-evolve toward noise. In each step of the reverse process, the structure is first denoised and subsequently used to denoise the properties (indicated by the green-dashed line). Such de-noised properties are then utilized, in turn, to further denoise the structure (red line), in a process that resembles a _guidance loop_.**

* **(Conceptual and methodological)** The introduction of a new score-based, end-to-end trainable, non-autoregressive generative model Twigs designed for acquiring conditional representations. Our approach enables precise guidance of multiple property-conditioned diffusion processes.
* **(Technical)** We present a robust mathematical framework, including a novel strategy called _loop guidance_, that employs tools from Stochastic Differential Equations (SDEs) to derive both the forward diffusion process and its corresponding reverse SDE for conditional generation. This framework is designed to seamlessly integrate additional contexts as conditioning information.
* **(Empirical)** We showcase the versatility of the proposed diffusion mechanism (Twigs) through extensive empirical evidence across various challenging conditional graph generation tasks, consistently surpassing contemporary baselines.

## 2 Related works

In Table 1 we provide an overview of the similarities and differences between Twigs and related methods. We refer the reader to Appendix E for additional related work.

**Diffusion guidance** is typically applied to regulate the diffusion process for conditional generation. Previous approaches that perform class-conditional generation are divided into classifier-based , and classifier-free guidance . While some works model diffusion with multiple flows [5; 37; 46], they treat nodes and edges in a symmetric way; i.e., they associate multiple flows for nodes and edges that have equivalent contributions (in other words, these flows have the same roles). We instead abstract graph properties as secondary processes that branch from, and interact with, the main process that pertains to the graph structure. In addition, while other guidance methods are related [18; 40; 52], they do not leverage multiple diffusion flows. To our knowledge, the proposed method is the first to incorporate multiple diffusion flows in a hierarchical fashion for conditional generation. We formalize in Table 2 how Twigs differs, mathematically, from classifier-free and classifier-based methods.

**Conditional Diffusion for Graphs** Recent advancements in generative modeling have prominently featured score-based techniques (SGM), utilizing diffusion or stochastic differential equations (SDEs) [19; 32; 35; 37; 48], including for graph generation [3; 5; 13; 14; 15; 18; 26; 40; 45; 46; 52; 72; 75; 82]. Guidance methods have been adopted in conditional molecule generation settings. The works from Hoogeboom et al. , Huang et al. [28; 28], Xu et al.  are classifier-free approaches, while Bao et al. , Vignac et al. , Lee et al.  focus on classifier-based methods. Diverging from these approaches, we explicitly model the dynamic interaction between primary variables (e.g., graph structure) and dependent variables (e.g., graph properties) using dedicated diffusion processes to achieve more expressive representations and improve performance for conditional generation.

## 3 Diffusion Twigs

Method overviewWe extend score-based techniques  for training conditional diffusion models over graphs. Differently from current guidance methods, as summarised in Table 2, we leverage a finer control over the structure and graph properties to diffuse multiple hierarchical processes, toward achieving a more robust representation. Our method, Twigs, defines a _trunk_ process over the primary

   Method & Conditional & Asymmetric & Multiple flows & Continuous (SDEs) \\  GDSS  & ✗ & ✗ & ✓ & ✓ \\ EEGSDE  & ✓ & ✗ & ✗ & ✓ \\ MOOD  & ✓ & ✗ & ✗ & ✓ \\ JODO  & ✓ & ✗ & ✗ & ✓ \\ EDGE  & ✗ & ✗ & ✓ & ✗ \\ GraphMaker  & ✓ & ✗ & ✓ & ✗ \\ Nisonoff et al.  & ✓ & ✗ & ✗ & ✗ \\ Gruver et al.  & ✓ & ✗ & ✗ & ✓ \\ Klarner et al.  & ✓ & ✓ & ✗ & ✗ \\  Twigs (ours) & ✓ & ✓ & ✓ & ✓ \\   

Table 1: **Comparison of related methodologies**. Twigs is the first method that enables a seamless orchestration of multiple asymmetric property-oriented hierarchical diffusion processes via SDEs.

variable (graph structure) \(_{s}\), and a _stem_ process over each dependent variable \(_{i}\) (e.g., graph property). We achieve the desired flexibility with a variable \(_{s}\) that encompasses both node features and the adjacency matrix as well as the coordinates. The details of the dimensions of \(_{s}\) are given in Section B.1 for the 3D case, and in Section B.2 for the 2D case.

Forward processWe define multiple forward processes within a hierarchy that co-evolves data and properties into noise. The _trunk_ forward process for the graph structure \(_{s}\) is defined as

\[_{s}=_{s}(_{s,t},t)t+g_{s}(t ) \]

where \(_{s}\) and \(g_{s}\) are corresponding diffusion and drift functions, and \(\) is the Wiener noise. The _stem_ forward process over the \(k\) dependent variables \(=\{_{1},,_{k}\}\) is defined as

\[(t)=_{1}(t)\\ \\ _{k}(t)=_{p}( _{1,t},_{s,t},t)t+g_{p}(t) \\ \\ _{p}(_{k,t},_{s,t},t)t+g_{p}(t)  \]

Here, \(_{p}\) and \(g_{p}\) denote the diffusion and drift functions, respectively, for the \(k\) stem processes. Collectively, along with the trunk forward process, they constitute Twigs. These operations introduce random Gaussian noise, iteratively, to the data toward a prior (typically Gaussian) distribution.

Reverse ProcessThe Twigs reverse process starts from the prior distribution (Gaussian noise) towards the data distribution. A key difference with Song et al.  is that here our variable \(_{t}\) comprises both structure _and properties_, leading to the following modification of the overall diffusion process:

\[d_{t}=[f(_{t},t)-g_{t}^{2}_{_{t}} p_{ t}(_{t})]dt+g_{t}d}_{t}= \{_{s,t},\{_{i,t}\}_{i=1}^{k}\}\;. \]

We derive Equation (3) in Section A.1. The joint distribution over the trunk and stem processes is assumed to factorize as

\[p_{t}(_{s,t},_{1,t},...,_{k,t})=p_{t}( _{s,t})_{i=1}^{k}\;p_{t}(_{i,t}_{s,t})\;. \]

In turn, the score function simplifies as in Equation (5), leading to the decomposition in Equation (6),

\[_{_{t}} p_{t}(_{s,t},_{1,t},, _{k,t})=_{_{t}} p_{t}(_{s,t})+_{i= 1}^{k}\;_{_{t}} p_{t}(_{i,t}_{s,t}) \]

\[_{t}=[(_{t},t)-g_{t}^{2}(_{ _{t}} p_{t}(_{s,t})+_{i=1}^{k}_{_ {t}} p_{t}(_{i,t}_{s,t}))]t+g_{t}} \]

Conditional modelingWe expand our proposed approach to enable conditional generation with an external context \(_{C}=\{_{c} c C\}\), where \(C\{1,,k\}\). The context can be represented as a scalar or vector, describing a particular value associated with a data-dependent variable. For example, in case of molecules, it could represent one or more of the \(k\) properties such as the Synthetic Accessibility (SA) score or the Quantitative Estimate of Drug likeness (QED). This extension modifies the joint distribution for the score function in Equation (5).

   Method & **Diffusion Scheme** & **Approach** \\    } & \(_{s}\) & \(_{s}-(_{s,t})t+g(t) \) & \(_{_{s}, p}(_{s,t},[_{s,t}])-_{ _{s}, p}(_{s,t})+_{_{s}, p}( _{s,t}_{s,The reverse SDE for \(_{t}=\{_{s,t},\{_{i,t}\}_{k}\}\) give an external conditioning context \(_{C}\) is shown below (details in Appendix A.2).

\[_{t}\!=\![(_{t},t)\!-\!g_{t}^{2}_ {_{t}} p_{t}(_{t},_{C})]t+g_{t} } \]

We resort to the following factorization of the distribution, conditioned on the context \(_{C}\):

\[p_{t}(_{s,t},\{_{i,t}\}_{k},_{C})=_{i}^{k}p _{t}(_{i,t}_{s,t},_{C})p_{t}(_{s, t},_{C})\]

As a result, the factorization of the score function \(_{_{t}} p_{t}(_{s,t},\{_{i,t}\}_{k}, _{C})\) amounts to

\[_{_{t}} p_{t}(_{s,t},_{C})\!+\!\! _{i C}^{k}_{_{t}} p_{t}(_{i,t}_{s,t})\!+\!\!_{c}^{C}_{i}^{k}_{i=c}_{_{t}}  p_{t}(_{i,t}_{s,t},_{c}) \]

The above-factorized score function parameterizes our reverse diffusion process, thus offering a novel approach to integrate external contextual information into conditional generation.

TrainingWe propose to train Twigs by incorporating the factorization from Equation (8) within a score-matching objective function . Algorithm 1 shows the training procedure to learn two types of time-dependent score-based models: \(_{,t}\), which approximates the trunk variable, and \(_{_{i},t}\) which approximates the coupling between the stem variable and the trunk variable. The objective function for optimizing the score networks \(_{},_{_{i}}\), is given as follows:

\[_{,_{i}}_{t}\{_{_{t}}(t)_{_{0}}_{_{t}|_{0}}\|_{ ,t}(_{s,t},_{c})\!+\!\!_{i}^{k}_{ _{i},t}(_{i,t},_{s,t},_{c})\!-\!\!_{ _{t}} p_{t}(_{t},_{C})\|_{2}^{2}\} \]

where \(_{_{0}}=_{_{s,0},_{i,0}}\) and \(_{_{t}}=_{_{s,t},t}\). It is worth noting that the influence introduced by the variable \(s_{_{i}}\) provides the directions for the diffusion model to converge into distributions with the desired properties. Such property-oriented knowledge operates in conjunction with the structural information provided by \(_{}\), resulting in a novel form of guidance that is orchestrated by a branching diffusion process, named _Loop guidance_.

```
Input: Dataset \(\), iterations \(n_{}\), batch size \(B\), number of batches \(n_{B}\), \(K\) properties to consider Initialize parameters \(s_{,t},\{s_{_{i},t}\}_{i=1}^{K}\) for Score Networks for\(k=1,,n_{}\)do for\(b=1,,n_{B}\)do \(t(0,1]\) \(_{b}=\{(_{s,t},\{_{i,t}\}_{i=1}^{K})_{l=1}^{B}, _{C}\}\) \(_{b}\) Eq. 9 endfor \(,\{_{i}\}_{i=i}^{K}(}_{b= 1}^{n_{B}}_{b})\) endfor
```

**Algorithm 1** Training Twigs

SamplingGiven a trained conditional Twigs model, our generative process begins by sampling an external context or conditioning value \(_{C}\), which can also be supplied externally. We then simulate the reverse diffusion process, similar to the one described in Equation 8, but with a modified score function to generate the data. The proposed algorithm for generating new data samples with Twigs is given in Algorithm 2 and involves a loop of updates between processes: the stem score network \(s_{_{i}}\) evolves the property \(_{i}\), integrating information from the structure \(_{s}\), and subsequently, the updated property information from \(s_{_{i}}\) is integrated into the main process by the score network \(s_{}\).

## 4 Experiments

We conduct a set of comprehensive experiments to demonstrate that Twigs improves over contemporary conditional generation methods. Benchmarks include: molecule generation conditioned over single (SS 4.1), and multiple (SS 4.2) properties on QM9, as well as molecule optimization on ZINC250K (SS 4.3), and network-graph generation conditioned on desired properties (SS 4.4).

### Single Quantum properties on QM9

**Setup.** We evaluate the effectiveness of Twigs for generating molecules with a single desired quantum property, sourced from the QM9 dataset , specifically, we consider \(C_{v}\), \(\), \(\), \(\), \(_{}\) and \(_{}\). To ensure consistency and comparability with the baselines, which include JODO , EDM , EEGSDE , GeoLDM , TEDMol , EquiFM , we adhere to the identical dataset preprocessing, training/test data partitions, and evaluation metrics outlined by Huang et al. . Regarding parameterization of Twigs, we follow the attention architecture defined in Section B.1 with a single stem process.

**Results.** In Table 3, we report the Mean Absolute Error (MAE) results, and in Table 4, the Novelty, Atom stability and Molecule stability. Our method outperforms all the evaluated baselines across the specified properties. In Figure 2, the bottom row provides a Kernel Density Estimation (KDE) visualization which shows that Twigs achieves a more accurate distribution for the property values when compared with JODO, while the top row shows some 3D molecule samples by our model.

    & Novelty\(\) & Atom Stability\(\) & Mol Stability\(\) & Novelty\(\) & Atom Stability\(\) & Mol Stability\(\) \\   & \)} &  \\  EDM & 83.64\(\)0.30 & 98.25\(\)0.025 & 80.82\(\)0.32 & 83.93\(\)0.11 & 98.17\(\)0.013 & 80.25\(\)0.49 \\ EEGSDE & 83.53\(\)0.10 & 98.25\(\)0.030 & 80.83\(\)0.33 & 83.85\(\)0.200 & 98.18\(\)0.025 & 80.25\(\)0.15 \\ TEDMol & 83.82\(\)0.10 & 98.27\(\)0.040 & 80.83\(\)0.040 & 84.88\(\)0.040 & 98.22\(\)0.040 & 80.31\(\)0.040 \\ JODO & 91.21\(\)0.22 & 97.74\(\)0.290 & 91.75\(\)0.11 & 91.22\(\)0.020 & 99.02\(\)0.025 & 92.86\(\)0.15 \\ Twigs & **93.16\(\)0.10** & **99.14\(\)0.040** & **92.72\(\)0.07** & **92.90\(\)0.030** & **99.25\(\)0.025** & **93.91\(\)0.030** \\  EDM & 83.93\(\)0.45 & 98.30\(\)0.040 & 81.95\(\)0.27 & 84.35\(\)0.33 & 98.17\(\)0.077 & 79.61\(\)0.32 \\ EEGSDE & 84.09\(\)0.27 & 98.18\(\)0.050 & 80.99\(\)0.290 & 84.44\(\)0.33 & 98.19\(\)0.030 & 79.81\(\)0.200 \\ TEDMol & 84.92\(\)0.040 & 98.19\(\)0.040 & 79.82\(\)0.040 & 84.58\(\)0.040 & 98.22\(\)0.040 & 80.97\(\)0.040 \\ JODO & 91.02\(\)0.170 & 98.42\(\)0.020 & 93.32\(\)0.040 & 91.38\(\)0.020 & 98.19\(\)0.350 & 92.02\(\)0.030 \\ Twigs & **92.70\(\)0.040** & **99.31\(\)0.010** & **94.12\(\)0.31** & **93.02\(\)0.21** & **99.26\(\)0.041** & **94.11\(\)0.200** \\   &  &  \\  EDM & 84.56\(\)0.47 & 98.13\(\)0.010 & 79.33\(\)0.30 & 84.62\(\)0.25 & 98.26\(\)0.040 & 81.34\(\)0.29 \\ EEGSDE & 84.19\(\)0.320 & 98.26\(\)0.030 & 80.95\(\)0.35 & 84.83\(\)0.30 & 98.14\(\)0.01 & 80.00\(\)0.210 \\ TEDMol & 85.82\(\)0.040 & 98.42\(\)0.040 & 82.03\(\)0.040 & 84.90\(\)0.040 & 98.31\(\)0.040 & 81.40\(\)0.040 \\ JODO & 90.15\(\)0.020 & 98.74\(\)0.050 & 94.03\(\)0.320 & 90.78\(\)0.420 & 98.84\(\)0.040 & 94.02\(\)0.030 \\ Twigs & **92.88\(\)0.10** & **99.28\(\)0.12** & **94.12\(\)0.02** & **92.48\(\)0.15** & **99.29\(\)0.17** & **94.11\(\)0.35** \\   

Table 4: Novelty, atom & molecule stability for QM9 single property.

Figure 2: First row: Samples by Twigs for 3D molecules conditioned on single properties on QM9. Second row: KDE and KL divergence results between target and predicted properties.

### Multiple Quantum properties on QM9

**Setup.** This experiment evaluates the capability to combine multiple desired properties in the generated molecule. Specifically we follow Huang et al.  and consider all possible combinations of properties involving \(\): (\(C_{v}\), \(\)), (\(,\)), (\(,\)). Since we model two properties, we test our Twigs with two stem networks within the attention architecture described in Section B.1. We benchmark against several contemporary baselines, including EDM , EEGSDE  and JODO .

**Results.** In Table 5, we present the Mean Absolute Error (MAE) results obtained from the property predictors introduced by Huang et al.  for the various property pairs under consideration. The superior performance of Twigs across all baselines reinforces the findings from the single property experiment (Section 4.1), emphasizing the benefits of learning multiple hierarchical stem processes.

### Molecule optimization on ZINC250K

**Setup.** The goal is to generate molecules from the ZINC250K dataset that exhibit optimal binding affinity, drug-likeness, and synthesizability for the following five target proteins: _parp1, fa7, 5ht1b, braf, jak2_. We adhere to the evaluation protocol established by Lee et al. , which involves generating 3000 molecules and assessing them using two metrics that constrain the desired properties, including docking score (DS), drug-likeness (QED), and synthetic accessibility (SA).

The first metric, _Novel hit ratio (%)_, represents the fraction of unique _hit molecules_ that have a maximum Tanimoto similarity of less than 0.4 with the training molecules. Hit molecules are defined as those meeting the criteria: DS < (the median DS of the known active molecules), QED > 0.5, and SA < 5. The second metric, _Novel top 5% docking score_, is the average DS of the top 5% unique molecules that satisfy QED > 0.5 and SA < 5, with a maximum similarity of less than 0.4 to the training molecules.

Figure 4: Molecules generated by Twigs from ZINC250k conditioned on fa7 (top), parp1 (bottom).

    & \(C_{v}\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  EDM & 1.097(\(\) 0.007) & 1.156(\(\) 0.011) & 683(\(\) 1) & 1.130(\(\) 0.007) & 2.76(\(\) 0.01) & 1.158(\(\) 0.002) \\ EEGSDE & 0.981(\(\) 0.005) & 0.912(\(\) 0.006) & 563(\(\) 3) & 0.866(\(\) 0.003) & 2.61(\(\) 0.01) & 0.855(\(\) 0.007) \\ TEDMol & 0.645(\(\) 1\(\)s) & 0.836(\(\) 1\(\)s) & 489(\(\) 1\(\)s) & 0.843(\(\) 1\(\)s) & 2.27(\(\) 0\(\)s) & 0.809(\(\) 0\(\)s) \\ JODO & 0.634(\(\) 0.002) & 0.716(\(\) 0.006) & 350(\(\) 4) & 0.752(\(\) 0.006) & 1.52(\(\) 0.01) & 0.717(\(\) 0.006) \\  Twigs & **0.602**(\(\) 0.001) & **0.708**(\(\) 0.002) & **343**(\(\) 2) & **0.740**(\(\) 0.003) & **1.46**(\(\) 0.01) & **0.712**(\(\) 0.002) \\   

Table 5: MAE (\(\)) for conditional generation on QM9 with multiple properties.

Figure 3: Samples of multiple-property conditional molecules by Twigs (\(C_{v}\) and \(\)) for QM9.

**Baselines.** We consider REINVENT : a reinforcement learning (RL) model that utilizes a prior sequence model, MORLD : a RL model that uses QED and SA scores as intermediate rewards and docking scores as final rewards, HierVAE : a VAE-based model that utilizes hierarchical molecular representation and active learning, GDSS : a score-based diffusion model that evolves nodes and edge information with a system of SDEs, MOOD : a score-based diffusion model based on GDSS that trains an additional property predictor to improve conditional generation. For MOOD we consider the version without the out-of-distribution (OOD) control, to have a fair comparison with our method. For Twigs we follow the GCN-based architecture described in Section B.2, with multiple stem processes (one for each target protein).

**Results.** In Table 6 we report the results for top 5% docking scores. We observe that Twigs achieves the highest score across all properties, excluding braf, where it achieves the second-best score after MOOD. In Table 7 we report the results for Novel hit ratio. The outcomes confirm that our model is improving the performance substantially over all the considered properties, except for braf, on which Twigs is the second-best performing model after MOOD. In Figure 4, we provide some samples of the molecules obtained by Twigs with the respective QED, SA, and docking score. Additionally, in Table 13 we report the MAE values for generating molecules with a desired target protein property, and in Table 14 we compare the inference cost of Twigs against MOOD.

### Generation of Network graphs with desired properties

**Setup.** We follow the data processing delineated by Jo et al.  and provide results for the Community-small  and Enzymes datasets . To test the capabilities to generate conditional graphs, we extract four properties via the NetworkX library , including density, clustering, assortativity, and transitivity. Considering a graph \(G\) with \(n\) nodes and \(m\) edges, we have: (1) Density: \(d=\), (2) Clustering coefficient: the average \(C=_{v G}c_{v}\). (3) Assortativity: measures the similarity of connections in the graph with respect to the node degree. (4) Transitivity: the fraction of all possible triangles present in \(G\). Possible triangles are identified by the number of "triads" (two edges with a shared vertex). The transitivity is \(T=3\).

**Baselines.** In terms of baselines, we first consider two versions of MOOD  (two OOD coefficients), and we train the property predictors using the codes from the authors. Our second baseline is GDSS , which we modify to be equipped with a classifier-free guidance scheme. We also consider the version of GDSS based on transformers, which leverages the graph-multi-head attention . Finally, we consider Digress , which is a classifier-based guidance diffusion model based on attention mechanisms. We parameterize our Twigs model with our GCN architecture described in Section B.2, with a single stem process.

   Model & _parp1_ & _fa7_ & _5ht1b_ & _braf_ & _jak2_ \\  REINVENT & 8.702(\(\) 0.523) & 7.205(\(\) 0.264) & 8.770(\(\) 0.316) & 8.392(\(\) 0.400) & 8.165(\(\) 0.277) \\ MORLD & 7.532(\(\) 0.260) & 6.263(\(\) 0.165) & 7.869(\(\) 0.650) & 8.040(\(\) 0.337) & 7.816(\(\) 0.133) \\ HierVAE & 9.487(\(\) 0.278) & 6.812(\(\) 0.274) & 8.081(\(\) 0.252) & 8.978(\(\) 0.525) & 8.285(\(\) 0.370) \\ GDSS & 9.967(\(\) 0.028) & 7.775(\(\) 0.309) & 9.459(\(\) 0.101) & 9.224(\(\) 0.068) & 8.926(\(\) 0.089) \\ MOOD & 10.409(\(\) 0.030) & 7.947(\(\) 0.034) & 10.487(\(\) 0.069) & **10.421**(\(\) 0.050) & 9.575(\(\) 0.075) \\  Twigs & **10.449**(\(\) 0.009) & **8.182**(\(\) 0.012) & **10.542**(\(\) 0.025) & **10.343**(\(\) 0.030) & **9.678**(\(\) 0.032) \\   

Table 6: Novel top 5% docking score on ZINC250K. Best is **boldfaced**, second-best is in \(}\).

   Model & _parp1_ & _fa7_ & _5ht1b_ & _braf_ & _jak2_ \\  REINVENT & 0.480(\(\) 0.344) & 0.213(\(\) 0.081) & 2.453(\(\) 0.561) & 0.127(\(\) 0.088) & 0.613(\(\) 0.167) \\ MORLD & 0.047(\(\) 0.050) & 0.007(\(\) 0.013) & 0.880(\(\) 0.735) & 0.047(\(\) 0.040) & 0.227(\(\) 0.118) \\ HierVAE & 0.553(\(\) 0.214) & 0.007(\(\) 0.013) & 0.507(\(\) 0.278) & 0.207(\(\) 0.220) & 0.227(\(\) 0.127) \\ GDSS & 1.933(\(\) 0.208) & 0.368(\(\) 0.103) & 4.667(\(\) 0.306) & 0.167(\(\) 0.134) & 1.167(\(\) 0.281) \\ MOOD & 3.400(\(\) 0.117) & 0.433(\(\) 0.063) & 11.873(\(\) 0.521) & **2.207**(\(\) 0.165) & 3.953(\(\) 0.383) \\  Twigs & **3.733**(\(\) 0.081) & **0.900**(\(\) 0.012) & **16.366**(\(\) 0.029) & 1.933(\(\) 0.023) & **5.100**(\(\) 0.312) \\   

Table 7: Novel hit ratio (\(\)) results on ZINC250K.

**Results.** Table 8 reports the MAE average of three runs, demonstrating that Twigs consistently outperforms the considered baselines on all cases across the two datasets. MOOD is the second-best performing model in the majority of the cases. We further strengthen the MAE results by providing in Figure 5 (bottom) the KDE plots of the property distributions of the graph generated by Twigs and MOOD. The Figure demonstrates that Twigs can achieve a higher fidelity to the data, which is also confirmed by the lower KL divergence values. Figure 5 (top) depicts some random graph samples generated by Twigs.

### Ablation study on multiple properties

**Setup.** Assuming conditional independence among the properties \(\), \(_{}\), \(_{}\), \(\), \(\), and \(C_{v}\) given the molecular graph can simplify the modeling process. This assumption leverages the fact that the molecular graph captures the essential structural dependencies, allowing us to treat the properties as independent for computational efficiency and ease of interpretation, even if slight interdependencies exist.

**Results.** Here we show that such modeling assumption can work practically. Table 9 reports the MAE on molecular graphs for QM9 on three properties, showing that our method consistently achieves lower error on all the properties. Table 10 shows that on generic graphs Twigs can achieve lower MAE on all the considered cases, in the cases of two and three properties.

    &  &  \\  Model & Density & Clustering & Assortativity & Transitivity & Density & Clustering & Assortativity & Transitivity \\  GDSS & 2.95 & 12.1 & 19.6 & 11.4 & 8.04 & 2.53 & 1.98 & 2.55 \\ GDSS-T & 2.30 & 11.5 & 19.2 & 10.1 & 9.25 & 3.27 & 2.03 & 2.68 \\ Digress & 2.34 & 10.6 & 17.8 & 9.42 & 8.04 & 2.39 & 1.95 & 2.55 \\ MOOD-1 & 2.35 & 11.1 & 18.8 & 10.5 & 7.94 & 2.34 & 1.83 & 2.12 \\ MOOD-4 & 2.12 & 11.3 & 16.7 & 8.76 & 7.98 & 2.44 & 1.99 & 2.43 \\  Twigs & **2.07** & **9.67** & **15.2** & **8.35** & **7.35** & **2.23** & **1.72** & **2.03** \\   

Table 8: MAE (\(\)) values on Community-small and Enzymes, conditioned on single properties.

Figure 5: Visualization of Community-small and Enzymes datasets. First and second rows: samples generated by Twigs. Third and fourth rows: KDE plots and corresponding KL divergence values.

  
**Model** & \(\) & \(\) & \(\) \\  JODO & 2.749 (\(\) 0.03) & 1.162 (\(\) 0.04) & 717 (\(\) 5) \\ Twigs & **2.544** (\(\) 0.05) & **1.094** (\(\) 0.02) & **640** (\(\) 3) \\   

Table 9: MAE values over three properties for QM9.

### Training time

In Table 11 we study the impact of multiple diffusion flows on the community-small and Enzymes datasets. Specifically, we report the average time for the overall training for Twigs with one and three secondary diffusion flows. We observe that our models encounter a small overhead compared to GDSS and Digress, however, we believe it is a good tradeoff because it achieves a lower MAE.

## 5 Conclusion, Broader Implications, and Limitations

We introduced a novel approach to model conditional information within generative models tailored for graph data. Twigs incorporates the novel mechanism of _loop guidance_ to control the overall generative process by first bifurcating the diffusion flow into multiple stem processes and then re-integrating them into the trunk process, resembling a loop. Our experimental results showcase the performance gains of Twigs when compared to current state-of-the-art baselines across various conditional graph generation tasks.

Conditional generation is fast emerging as one of the most exciting avenues within machine learning and would benefit from techniques beyond classifier-based and classifier-free schemes, making our method applicable to settings beyond this work. Indeed, while the current work has focused on graph settings, Twigs might find use in other domains (e.g., image, text, and audio). However, whether Twigs is effective in such settings needs to be investigated in future works.

Training multiple properties (stem processes) might require training additional parameters, incurring additional computation and training time. Our ablation study on training time due to multiple processes (Section 4.6) suggests that Twigs could provide a good tradeoff (lower MAE compared to some prominent existing methods at the expense of small additional computational overhead).

Finally, assuming factorization of the distribution over stem processes conditioned on the trunk process might not always be realistic. Our experiments in Section 4.5 suggest that Twigs might still be able to achieve a strong performance when considering multiple properties. In case some prior knowledge is available about some properties that violate this assumption, we could, in principle, adapt Twigs by grouping them into a single stem process while factorizing with the remaining ones.