# Implicit Bias of Gradient Descent for Logistic Regression at the Edge of Stability

Jingfeng Wu

Johns Hopkins University

Baltimore, MD 21218

uuujf@jhu.edu

&Vladimir Braverman

Rice University

Houston, TX 77005

vb21@rice.edu

&Jason D. Lee

Princeton University

Princeton, NJ 08544

jasonlee@princeton.edu

###### Abstract

Recent research has observed that in machine learning optimization, gradient descent (GD) often operates at the _edge of stability_ (EoS) , where the stepsizes are set to be large, resulting in non-monotonic losses induced by the GD iterates. This paper studies the convergence and implicit bias of constant-stepsize GD for logistic regression on linearly separable data in the EoS regime. Despite the presence of local oscillations, we prove that the logistic loss can be minimized by GD with _any_ constant stepsize over a long time scale. Furthermore, we prove that with _any_ constant stepsize, the GD iterates tend to infinity when projected to a max-margin direction (the hard-margin SVM direction) and converge to a fixed vector that minimizes a strongly convex potential when projected to the orthogonal complement of the max-margin direction. In contrast, we also show that in the EoS regime, GD iterates may diverge catastrophically under the exponential loss, highlighting the superiority of the logistic loss. These theoretical findings are in line with numerical simulations and complement existing theories on the convergence and implicit bias of GD for logistic regression, which are only applicable when the stepsizes are sufficiently small.

## 1 Introduction

_Gradient descent_ (GD) is a foundational algorithm for machine learning optimization that motivates many popular algorithms. Theoretically, the behavior of GD is well understood when the stepsize is small. In this regard, one of the most classic results is the _descent lemma_ (see, e.g., Section 1.2.3 in ):

**Lemma** (Descent lemma, simplified version).: _Suppose that \(_{_{}}^{2}L()_{2} ^{1}\), then_

\[L(_{+}) L()-(1-/2)\| L( )\|_{2}^{2},\;\;_{+}:=-  L().\]

When the targeted function is smooth (such as logistic regression) and the stepsize is _small_ (\(0<</2\)), the descent lemma ensures a monotonic decrease of the function value by performing each GD step. Building upon this, a sequence of iterates produced by GD with small stepsizes provably minimizes the function value in various settings (see, e.g., Lan (2020)).

For a more modern example, a recent line of research has established the _implicit bias_ of GD with small stepsizes (see , Ji and Telgarsky (2018) and references thereafter). Specifically, they consider GD for optimizing logistic regression (besides other loss functions) onlinearly separable data. When the stepsizes are sufficiently small, the GD iterates are shown to decrease the risk monotonically (by a variant of the descent lemma); moreover, the GD iterates tend to align with a direction that maximizes the \(_{2}\)-margin of the data (Soudry et al., 2018; Ji and Telgarsky, 2018). The margin-maximization bias of small-stepsize GD sheds important light on understanding the statistical benefits of GD, as a large margin solution often generalizes well (Bartlett et al., 2017; Neyshabur et al., 2018).

Nonetheless, in practical machine learning optimization, especially in deep learning, the empirical risk (or training loss) often varies _non-monotonically_ (while being minimized in the long run) -- the local risk oscillation is not only caused by the algorithmic randomness but is more an effect of using _large stepsizes_, as it happens for deterministic GD (with large stepsizes) as well (Wu et al., 2018; Xing et al., 2018; Lewkowycz et al., 2020; Cohen et al., 2021). This phenomenon is showcased in Figures 1(a) and 2(a), and is referred to by Cohen et al. (2021) as the _edge of stability_ (EoS). The observation sets a non-negligible gap between practical and theoretical GD setups, where in practice, GD is run with large stepsizes that lead to local risk oscillations, but in theory, GD is only considered with sufficiently small stepsizes, predicting a monotonic risk descent (with a few exceptions, which will be discussed later in Section 2). A tension remains to be resolved:

_Is the convergence of risk under local oscillation merely a "lucky" occurrence, or is it predictable based on theory?_

Contributions.In this work, we study the behaviors of GD in the EoS regime in arguably the simplest setting for machine learning optimization -- logistic regression on linearly separable data. We show that with _any_ constant stepsize, while the induced risks may oscillate locally, GD must minimize the risk in the long run at a rate of \((1/t)\), where \(t\) is the number of iterates. In addition, we show that the direction of the GD iterates (with any constant stepsize) must align with a max-margin direction (the hard-margin SVM direction) at a rate of \((1/(t))\). These results explain how GD minimizes a risk non-monotonically, and complement existing theories (Soudry et al., 2018; Ji and Telgarsky, 2018) on the convergence and implicit bias of GD, which are only applicable when the stepsizes are sufficiently small.

Some additional notable contributions are

1. We also show that, when projected to the orthogonal complement of the max-margin direction, the GD iterates (with any constant stepsize) converge to a fixed vector that minimizes a strongly convex potential at a rate of \((1/(t))\). This characterization is conceptually more interpretable than an existing version (Soudry et al., 2018).
2. We show that in the EoS regime, GD can diverge catastrophically if the logistic loss is replaced by the exponential loss. This is in stark contrast to the small-stepsize regime, where the behaviors

Figure 1: The behaviors of GD for optimizing a neural network. We randomly sample \(1,000\) data from the MNIST dataset and then use GD to train a \(4\)-layer fully connected network to fit those data. We use the cross-entropy loss, i.e., the multi-class version of the logistic loss. The sub-figures (a), (b), and (c) report the training loss, test accuracy, and sharpness (see, \(\| L(_{t})\|_{2}\)) along the GD trajectories, respectively. The red curves correspond to GD with a large stepsize \(=0.1\), where the training losses oscillate locally and the sharpness can exceed \(2/=20\). The green curves correspond to GD with a small stepsize \(=0.01\), where the training losses decrease monotonically and the sharpnesses are always below \(2/=200\). Moreover, (c) suggests that large-stepsize GD achieves better test accuracy than small-stepsize GD, consistent with larger-scale deep learning experiments (Goyal et al., 2017). More details of the experiments can be found in Appendix D.

of GD are known to be similar under any exponentially-tailed losses including both the logistic and exponential losses (Soudry et al., 2018; Ji and Telgarsky, 2018). The difference in the EoS regime provides insights into why the logistic loss is preferable to the exponential loss in practice.
3. From a technical perspective, we develop a new approach for analyzing GD with large stepsizes. Our approach views the GD iterates as a coupling of two orthogonal iterates, one along a max-margin direction and the other along the orthogonal complement of the max-margin direction. The former iterates tend to infinity and the latter iterates approximate "imaginary" GD iterates that minimize a strongly convex potential with a _decaying_ stepsize scheduler, controlled by the former iterates. Our techniques for analyzing large-stepsize GD can be of independent interest.

## 2 Related Works

In this section, we discuss papers related to our work.

**Implicit bias.** We first review a set of papers on the implicit bias of GD (with small stepsizes).

Along this line, Soudry et al. (2018) are the very first to show that GD converges along a max-margin direction when minimizing the empirical risk of an exponentially-tailed loss function (such as the logistic and exponential losses), a linear model, and linearly separable data. Then, an alternative analysis is provided by Ji and Telgarsky (2018), which also deals with non-separable data. These two works directly motivate us for considering GD for logistic regression on linearly separable data. However, there are at least three notable differences between our work and theirs. Firstly, their results only apply to GD with small stepsizes, while our results apply to GD with _any_ constant stepsize. Secondly, their theories predict no difference between the logistic and exponential losses (as they are limited to the small-stepsize regime). Quite surprisingly, we prove that in the EoS regime, GD can diverge catastrophically under the exponential loss. Thirdly, from a technical viewpoint, their implicit bias analysis is built upon the risk convergence analysis, which further relies on a monotonic risk descent argument, hence only applies to small stepsizes. In comparison, we come up with a new approach that allows analyzing the implicit bias under risk oscillations; the long-term risk convergence is simply a consequence of the implicit bias results. Hence our techniques can accommodate any constant stepsize. See Section 5 for more discussions.

Subsequent works have extended the results by Soudry et al. (2018); Ji and Telgarsky (2018) to other algorithms such as momentum-based GD (Gunasekar et al., 2018; Ji et al., 2021) and SGD (Nacson et al., 2019), and homogenous but non-linear models (Gunasekar et al., 2017; Ji and Telgarsky, 2019; Gunasekar et al., 2018; Nacson et al., 2019; Lyu and Li, 2020) and non-homogenous models (Nacson et al., 2019). All these theories require the stepsizes to be small or even infinitesimal, in a regime away from our focus, the EoS regime.

It is worth noting that Nacson et al. (2019) consider GD with an increasing stepsize scheduler that achieves a faster margin-maximization rate than constant-stepsize GD. However, their stepsize at each iteration is still appropriately small, resulting in a monotonic risk descent by a variant of the descent lemma.

**Edge of stability.** The risk oscillation phenomenon has been observed in several deep learning papers (Wu et al., 2018; Xing et al., 2018; Lewkowycz et al., 2020), and the work by Cohen et al. (2021) coins the term, _edge of stability_ (EoS), that formally refers to it. In the remainder of this part, we focus on reviewing the current theoretical progress in understanding EoS.

Zhu et al. (2023) rigorously characterize EoS for a two-dimensional function \((u,v)(u^{2}v^{2}-1)^{2}\). Chen and Bruna (2022) study EoS for a one-dimensional function \(u(u^{2}-1)^{2}\) and for a special two-layer single-neuron network. Similar to these two works, Kreisler et al. (2023) study EoS in a 1-dimensional linear network. Ahn et al. (2022) consider functions \((u,v)(uv)\), where \(\) is assumed to be convex, even, and Lipschitz; notably, they show a statistical gap between the small-stepsize regime and the EoS regime. Finally, Even et al. (2023), Andriushchenko et al. (2023) consider the regularization effect of large stepsizes in a diagonal linear network. Compared to their settings, our problem, i.e., logistic regression, is a natural machine-learning problem with fewer artifacts (if any).

EoS has also been theoretically investigated for general functions (Ma et al., 2022; Ahn et al., 2022; Damian et al., 2022; Wang et al., 2022), but these theories are often subject to subtle assumptions that are hard to interpret or verify. Specifically, Ma et al. (2022) require the function to grow "subquadratically". Ahn et al. (2022) assume the existence of a "forward invariant subset" near the set of minima of the function. Damian et al. (2022) assume a negative correlation between the gradient direction and the largest eigenvalue direction of the Hessian. Wang et al. (2022) consider a two-layer neural network but require the norm of the last layer parameter and the sharpness to change in the same direction along the GD trajectory. Indirectly connected to EoS, the work by Kong and Tao (2020) shows a chaotic behavior of GD with a non-small stepsize when optimizing a "multi-scale" loss function. In comparison, our assumptions are more natural and interpretable.

Besides, the work by Lyu et al. (2022) considers EoS induced by GD for scale-invariant loss, e.g., a network with normalization layers and weight decay, and the work by Wang et al. (2022) shows a balancing effect in matrix factorization induced by GD with a constant stepsize that is nearly \(4/\|^{2}L(_{0})\|_{2}\) and is larger than \(2/\|^{2}L(_{0})\|_{2}\). The objectives in their works are different from ours, i.e., logistic regression.

The unstable convergence has also been studied for normalized GD (Arora et al., 2022) and regularized GD (Bartlett et al., 2022). These algorithms are apart from our focus on the vanilla GD.

Finally, the work by Liu et al. (2023) considers logistic regression with non-separable data (such that the objective is strongly convex), where GD with sufficiently large stepsize diverges. In contrast, we consider logistic regression with separable data, where GD with an arbitrarily large stepsize still converges.

## 3 Preliminaries

We use \(^{d}\) to denote a feature vector and \(y\{ 1\}\) to denote a binary label, respectively. Let \((_{i},y_{i})_{i=1}^{n}\) be a set of training data. Throughout the paper, we assume that \((_{i},y_{i})_{i=1}^{n}\) is _linearly separable_(Soudry et al., 2018).

**Assumption 1** (Linear separability).: _Assume there is \(^{d}\) such that \(y_{i}_{i}^{}>0\) for \(i=1,,n\)._

Let \(^{d}\) be the parameter of a linear model. In _logistic regression_, we aim to minimize the following empirical risk

\[L():=_{i=1}^{n}1+(-y_{i}_{i},),^{d}.\]

We study a sequence of iterates \((_{t})_{t 0}\) produced by constant-stepsize _gradient descent_ (GD), where \(_{0}\) denotes the initialization and the remaining iterates are sequentially generated by:

\[_{t}=_{t-1}- L(_{t-1}), t 1,\] (GD)

where \(>0\) is a constant stepsize. We are especially interested in a regime where \(\) is very large such that \(L(_{t})\) oscillates as a function of \(t\). For the simplicity of presentation, we will assume that \(_{0}=0\). Our results can be easily extended to allow general initialization.

The following notations are useful for presenting our results.

**Definition 1** (Margins and support vectors).: Under Assumption 1, define the following notations:

1. Let \(\) be the max-\(_{2}\)-margin (or max-margin in short), i.e., \[:=_{\|\|_{2}=1}_{i[n]}\;y_{i}_{i},.\]
2. Let \(}\) be the hard-margin support-vector-machine (SVM) solution, i.e., \[}:=_{^{d}}\|\|_{2},\; \;\;\;y_{i}_{i}, 1,\;i=1, ,n.\] It is clear that \(}\) exists and is uniquely defined (see, e.g., Section 5.2 in Mohri et al. (2018)). Note that \(\|}\|_{2}=1/\) and \(}/\|}\|_{2}\) is a max-margin direction. Also note that by duality, \(}\) can be written as (see, e.g., Section 5.2 in Mohri et al. (2018)) \[}=_{i}_{i} y_{i}_{i}, _{i} 0.\]* Let \(\) be the set of indexes of the support vectors, i.e., \[:=\{i[n]:y_{i}_{i},}/\|}\|_{2}=\}.\]
* If there exists non-support vector (\([n]\)), let \(\) be the second smallest margin, i.e., \[:=_{i}\;y_{i}_{i},}/\|}\|_{2}.\]

It is clear from the definitions that \(>>0\). In addition, from the definitions we have

\[_{i}_{i}=_{i}_{i} y_{i} _{i}^{}}=\|}\|_{2}^{2}=}.\]

In addition to Assumption 1, we make the following two mild assumptions to facilitate our analysis.

**Assumption 2** (Regularity conditions).: _Assume that:_

* \(\|_{i}\|_{2} 1,\;i=1,,n\)_._
* \(\{_{i},\;i=1,,n\}=d\)_._

Assumption 2 is only made for the convenience of presentation. In particular, Assumption 2(A) can be made true for any dataset by scaling the data vectors with a factor of \(_{i}\|_{i}\|_{2}\). Without Assumption 2(B), our theorems still hold under a minor revision by replacing all the vectors of interests with their projections to \(\{_{i},i=1,,n\}\).

**Assumption 3** (Non-degenerate data).: _In addition to Assumption 1, assume that_

* \(\{_{i},\;i\}=\{_{i },\;i=1,,n\}\)_._
* _There exist_ \(_{i}>0,i\) _such that_ \(}=_{i}_{i} y_{i}_{i}\)_._

Assumption 3 has been used in Soudry et al. (2018) (see their Theorem 4), which requires that the support vectors span the dataset and are associated with strictly positive dual variables. Assumption 3(B) holds _almost surely_ for every linearly separable dataset sampled from a continuous distribution according to Appendix B in Soudry et al. (2018). Assumption 3 provides convenience to our analysis, but we conjecture it might not be necessary. Removing/relaxing Assumption 3 is left as a future work.

### Space Decomposition

Conceptually, our analysis is built on a novel space decomposition viewpoint, which relies on the following lemma.

**Lemma 3.1** (Non-separable subspace).: _Suppose that Assumptions 1, 2, and 3 hold. Then \((_{i},y_{i})_{i}\) is not linearly separable in the subspace orthogonal to the max-margin direction \(}/\|}\|_{2}\). That is, for every \(\) such that \(,}=0,\) there exist \(i,j\) such that \(y_{i}_{i},<0,\;y_{j} _{j},>0.\)_

Proof of Lemma 3.1.: By Assumption 3 and \(,}=0\), we have

\[0=,}=_{i}_{i}  y_{i}_{i}^{}.\]

By Assumptions 2 and 3 we have

\[\{y_{i}_{i},\;i\}=\{ _{i},\;i\}=\{_{i},\;i=1,, n\}=d,\]

so there must exist \(i\) such that \(y_{i}_{i}^{} 0\). Without loss of generality, assume that \(y_{i}_{i}^{}<0\). Then since \(_{i}>0\) for \(i\) by Assumption 3, there must exist \(j\) such that \(y_{j}_{j}^{}>0\). 

Lemma 3.1 shows that, although the dataset can be (linearly) separated by \(}\), it cannot be separated by _any_ vector orthogonal to \(}\). This motivates us to decompose the \(d\)-dimensional ambient space into a \(1\)-dimensional "separable" subspace and a \((d-1)\)-dimensional "non-separable" subspace. This idea is formally realized as follows.

Fix \(d-1\) orthogonal vectors \(_{1},,_{d-1}^{d}\) such that \(}/\|}\|_{2},_{1},, _{d-1}\) forms an orthogonal basis of the ambient space \(^{d}\). Then define two _projection operators_:

\[^{d} ^{}}/\|}\|_{2},\]\[}^{d}^{d-1}(^{}_{1},,^{}_{d-1}).\]

The two operators together define a natural space decomposition, i.e., \(^{d}=(^{d})(^{d})\). Moreover, \((_{i}),y_{i}_{i=1}^{n}\) are linearly separable with an max-\(_{2}\)-margin \(\) according to Definition 1, and \(}(_{i}),y_{i}_{i}\) (hence \(}(_{i}),y_{i}_{i=1}^{n}\)) are non-separable according to Lemma 3.1. So the decomposition of space can also be understood as the decomposition of data features into "max-margin features" and "non-separable features".

In what follows, we will call \((^{d})\) the _max-margin subspace_ and \(}(^{d})\) the _non-separable subspace_, respectively. In addition, we define a "margin offset" that quantifies to what extent the "non-separable features" are not separable.

**Definition 2** (Margin offset for the non-separable features).: Under Assumptions 1, 2, and 3, it holds that \(}(_{i}),y_{i}_{i}\) is non-separable. Let \(b\) be a _margin offset_ such that

\[-b:=_{}^{d-1},\;\|}\|=1}_{ i}\;\;y_{i}}(_{i}), \;}.\]

Then \(b>0\) due to the non-separability. The definition immediately implies that:

\[}^{d-1},\;i\;y_{i}}(_{i}),\;}-b\|}\|_{2}.\]

Comparison to Ji and Telgarsky (2018).The work by Ji and Telgarsky (2018) also conducts space decomposition (see their Section 2). However, our approach is completely different from theirs. Firstly, they consider a non-separable dataset but we consider a linearly separable dataset. Secondly, at a higher level, they decompose the "dataset" (into two subsets), while we decompose the "features" (into two kinds of features). More specifically, Ji and Telgarsky (2018) first group the non-separable dataset into the "maximal linearly separable subset" and the complement, non-separable subset, then decompose the ambient space according to the subspace spanned by the non-separable subset and its orthogonal complement. In comparison, we consider a linearly separable dataset and decompose the ambient space according to a max-margin direction (i.e., \(\)) and its orthogonal complement (i.e., \(}\)).

## 4 Main Results

We are now ready to present our main results. All proofs are deferred to Appendix C. To begin with, we provide the following theorem that captures the behaviors of constant-stepsize GD for logistic regression on linearly separable data.

**Theorem 4.1** (The implicit bias of GD for logistic regression).: _Suppose that Assumptions 1, 2, and 3 hold. Consider \((_{t})_{t 0}\) produced by \(()\) with initilization\({}^{2}\)\(_{0}=0\) and constant stepsize \(>0\). Then there exist positive constants \(c_{1},c_{2},c_{3}>0\) that are upper bounded by a polynomial of \(e^{},e^{n},e^{1/b},1/,1/(-),1/,e^{/ }}\) but are independent of \(t\), such that:_

1. _The risk is upper bounded by_ \[L(_{t}) c_{1}/t, t 3.\]
2. _In the max-margin subspace,_ \[(_{t})(t)/+(^{2}/2)/,  t 1.\]
3. _In the non-separable subspace,_ \[}(_{t})_{2} c_{2}, t 0.\]
4. _In addition, in the non-separable subspace,_ \[G}(_{t})- G() c_{3}/ (t), t 3,\] _where_ \(G()\) _is a strongly convex potential defined by_ \[G():=_{i}-y_{i} {}(_{i}),\;,^{d-1}.\]Note that Theorem 4.1 applies to GD with _any_ positive constant stepsize, therefore allowing GD to be in the EoS regime. We next discuss the implications of Theorem 4.1 in detail.

**Risk minimization.** Theorem 4.1(A) guarantees that the GD iterates minimize the logistic loss at a rate of \((1/t)\) for any constant stepsize, even for those large stepsizes that cause local risk oscillations. This result explains the risk convergence of GD in the EoS regime, as illustrated in Figure 2, and is also consistent with the observations in neural network experiments (see Figure 1).

**Margin maximization.** Theorem 4.1(B) shows that the GD iterates, when projected to the max-margin direction, tend to infinity at a rate of \(((t))\). Moreover, Theorem 4.1(C) shows that the GD iterates, when projected to the non-separable subspace, are uniformly bounded. These two results together imply that the direction of the GD iterates will tend to a max-margin direction, i.e., the hard-margin SVM direction, at a rate of \((1/(t))\). Therefore, the implicit bias of GD that maximizes the \(_{2}\)-margin is consistent in both the EoS regime and the small-stepsize regime (Soudry et al., 2018; Ji and Telgarsky, 2018).

**Iterate convergence in the non-separable subspace.** Theorem 4.1(D) shows that the GD iterates, when projected to the non-separable subspace, converge to the minimizer of a strongly convex potential \(G()\). Here, \(G()\) measures the exponential loss of a parameter on the support vectors with their non-separable features. This provides a more precise characterization of the implicit bias of GD: the direction of the GD iterates converges to the hard-margin SVM direction, moreover, the limit of the projections of the GD iterates to the orthogonal complement to the hard-margin SVM direction minimizes the exponential loss on the non-separable features of the support vectors.

**Comparison to Theorem 9 in Soudry et al. (2018).** Theorem 9, in particular, equation (18), in Soudry et al. (2018)_indirectly_ characterizes the convergence of GD iterates in the non-separable subspace. It reads in our notations that: \(}:=_{t}(_{t}-}(t))\) exists and satisfies

\[i,\ \ (-y_{i}_{i },\ })=_{i},_{i}}. \]

In Appendix A, we show that Theorem 4.1(D) is equivalent to condition (1) in terms of describing \(}(_{})\). Despite their equivalence, (1) is less interpretable than Theorem 4.1(D), as (1) entangles an effect of \((_{})\) with \(}(_{})\), while Theorem 4.1 completely decouples \((_{})\) and \(}(_{})\). In particular, (1) seems to suggest \(}(_{})\) to be a function of stepsize \(\) since \(}\) depends on \(\). However, this is only an illusion brought by the lack of interpretability of (1); it is clear that \(}(_{})\) is independent of \(\) according to Theorem 4.1(D).

**Exponential loss.** Until now, our theory for GD is consistent for large and small stepsizes. However, this is a particular benefit thanks to the design of the logistic loss, and may not hold for other losses. Our next result suggests that, in the EoS regime where the stepsizes are large, GD can diverge catastrophically under the exponential loss.

Figure 2: The behaviors of GD for logistic regression. We randomly sample \(1,000\) data with labels “\(0\)” and “\(8\)” from the MNIST dataset and then use GD to perform logistic regression on those data. The sub-figures (a) and (b) report the risk (i.e., the logistic loss) and sharpness (i.e., \(\| L(_{t})\|_{2}\)) along the GD trajectories, respectively. For GD with stepsizes \(\) larger than or equal to \(0.1\), the training losses oscillate locally and the sharpnesses can exceed \(2/\). For GD with a small stepsize \(=0.01\), the training losses decrease monotonically and the sharpnesses are always below \(2/\). More details of the experiments can be found in Appendix D.

**Theorem 4.2** (The catastrophic divergence of GD under the exponential loss).: _Consider a dataset of two samples, where_

\[_{1}=(,\;1), y_{1}=1;_{2}=(,\;-1),  y_{2}=1.\]

_It is clear that \((_{i},y_{i})_{i=1,2}\) is linearly separable and \((1,0)\) is the max-margin direction. Consider a risk defined by the exponential loss:_

\[L(w,):=(-y_{1}_{1},)+(-y_{2} _{2},)=e^{- w}e^{-} +e^{},\;\;=(w,).\]

_Let \((w_{t},_{t})_{t 0}\) be the iterates produced by GD with constant stepsize \(\) for optimizing \(L(w,)\). If_

\[0 w_{0} 2,|_{0}| 1, 0<<1/4, 4,\]

_then:_

1. \(L(w_{t},_{t})\)_._
2. \(w_{t}\)_._
3. _For every_ \(t 0\)_,_ \(|_{t}| 2 w_{t}\)_._
4. _Moreover, the sign of_ \(_{t}\) _flips every iteration._

_As a consequence, \((w_{t},_{t})_{t 0}\) diverge in terms of either magnitude or direction; in particular, the direction of \((w_{t},_{t})_{t 0}\) cannot converge to the max-margin direction (which is \((1,0)\))._

Theorem 4.2 shows that with a large constant stepsize, the GD iterates no longer minimize the risk defined by the exponential loss and no longer converge along the max-margin direction. In fact, the directions of the GD iterates flip every step, thus the direction of the GD iterates necessarily _diverges_, resulting in no meaningful implicit bias at all.

In the EoS regime, large-stepsize GD still behaves nicely under the logistic loss (Theorem 4.1) but can behave catastrophically under the exponential loss (Theorem 4.2). From a mathematical standpoint, this difference is rooted in the fact that the gradient of the logistic loss is uniformly bounded while the gradient of the exponential loss could be extremely large. From a practical standpoint, it provides insights into why the logistics loss (and its multi-class version, the cross-entropy loss) is preferable to the exponential loss in practice.

The different behaviors of large-stepsize GD under the logistic and exponential losses also sharply contrast the EoS regime with the small-stepsize regime. Because in the small-stepsize regime, the convergence and implicit bias of GD are known to be similar under any exponentially-tailed losses, including the logistic and exponential losses (Soudry et al., 2018; Ji and Telgarsky, 2018).

## 5 Techniques Overview

The proofs of Theorems 4.1 and 4.2 are deferred to Appendix C. In this section, we explain the proof ideas of Theorem 4.1 by analyzing a simple dataset considered in Theorem 4.2 (the treatment to the general datasets can be found in Appendix B). But this time we work with the logistic loss instead of the exponential loss, that is,

\[L(w,)=(1+e^{- w-})+(1+e^{- w+}).\]

Then the GD iterates can be written as

\[w_{t+1}=w_{t}- g_{t},_{t+1}=_{t}- _{t},\]

where

\[g_{t}:=-+_{t}}}+-_{t}}},_{t}:=-+_{t}}}--_{t}}}.\]

For simplicity, assume that

\[w_{0}=0,|_{0}|>0.\]

Different from Soudry et al. (2018); Ji and Telgarsky (2018), our approach begins with showing the implicit bias (despite that the risk may oscillate). The long-term risk convergence is then simply a consequence of the implicit bias results.

Step 1: \((_{t})_{t 0}\) is uniformly bounded.Observe that \(_{t}\) and \(_{t}\) always share the same sign and that \(|_{t}| 1\), so we have

\[|_{t+1}|=|_{t}|-|_{t}| |_{t}|,\;|_{t}|}|_{t}|,\; }.\]

By induction, we get that \(|_{t}|_{t 0}\) is uniformly bounded by \(\{|_{0}|,\;\}=(1)\).

Step 2: \(w_{t}(t)/\).We turn to study the max-margin subspace. It is clear that \(g_{t} 0\) for every \(t 0\). So we have \(w_{t} 0\) by induction. Moreover, we have

\[-}{}=-_{t}}}{1+e^{- w_{t}- _{t}}}++_{t}}}{1+e^{- w_{t}+_ {t}}} e^{- w_{t}} e^{-_{t}}+e^{- w_{t}} e^{ _{t}} e^{- w_{t}}(1),\]

where the last inequality is because \(|_{t}|\) is uniformly bounded. We also have

\[-}{} =-_{t}}}{1+e^{- w_{t}-_{ t}}}++_{t}}}{1+e^{- w_{t}+_{t}}} 0.5 \{1,e^{- w_{t}}e^{-_{t}}\}+0.5\{1,e^{- w_{ t}}e^{_{t}}\}\] \[ 0.5\{1,e^{- w_{t}}e^{-_{t}}+e^{- w _{t}}e^{_{t}}\} 0.5\{1,e^{- w_{t}}\}=0.5 e^{-  w_{t}},\]

where the third inequality is because \(e^{-_{t}}+e^{_{t}} 1\) and the last equality is because \(w_{t} 0\). Putting these together, we have

\[g_{t}- e^{- w_{t}}(1)\;\;w_{t+1 } w_{t}- e^{- w_{t}}(1)\; \;w_{t}=(t)/(1). \]

Step 3: \(_{t}(- w_{t}) G(_{t})\).We turn back to the non-separable subspace. Note that \(_{t}\) is an odd function of \(_{t}\). Without loss of generality, let us assume \(_{t} 0\) in this part. Notice that

\[\;a>1,\;f(t):=-\;t 0. \]

Then we have

\[_{t}=e^{- w_{t}}(}+e^{- _{t}}}-}+e^{_{t}}}) e^{- w_{t} }(_{t}}}-_{t}}})=:e^{-  w_{t}} G(_{t}),\]

where the inequality is by (3), and \(G():=e^{}+e^{-}\) is defined as in Theorem 4.1(D). On the other hand, since \(|_{t}|\) is bounded and \(w_{t}\) is increasing (and tends to infinity), there must exist a time \(t_{0}\) such that \(e^{- w_{t}} e^{-|_{t}|}\) for every \(t t_{0}\). Then for \(t t_{0}\) we have

\[_{t} =e^{- w_{t}}(}+e^{-_{t}}}-}+e^{_{t}}}) e^{- w_{t}} (_{t}}}-_{t}}+e^{_{t }}})\] \[=e^{- w_{t}}_{t}}-e^{-_{t}}}{2 e^{-2_{t}}+2} e^{- w_{t}}_{t}}-e^{-_{t}}}{4 }=: e^{- w_{t}} G(_{t}),\]

where the first inequality is by (3) and \(e^{- w_{t}} e^{-_{t}}\), and the last inequality is because we assume \(_{t} 0\). Putting these together, and using (2), we obtain that

\[\;t t_{0},\;\;_{t+1}=_{t}-_{t} G (_{t}),\;\;_{t} e^{- w_{t}} (1)(1)/t. \]

Step 4: a modified descent lemma.Using (4) and Taylor's expansion, we have

\[\;t t_{0},\;\;G(_{t+1}) G(_{t})-_{t} \| G(_{t})\|^{2}+_{t}^{2}\|  G(_{t})\|^{2} G(_{t})+},\]

where \(:=_{||\{|_{0}|,\}}\|^{2}G()\|_{2 }=(1)\). Taking a telescoping sum from \(t\) to \(T\), we have

\[\;T t t_{0}, G(_{T}) G(_{t})+ (1)/t. \]

Step 5: the convergence of \(_{t}\).What remains is adapted from classic convergence arguments. Choose \(_{*}= G()\), then

\[\|_{t+1}-_{*}\|_{2}^{2} =\|_{t}-_{*}\|_{2}^{2}-2_{t} _{t}-_{*}, G(_{t})+_{t}^{2}\| G(_{t})\|_{2}^{2}\] \[\|_{t}-_{*}\|_{2}^{2}-2_{t}(G(_{t })-G(_{*}))+(1)/t^{2}, t t_{0},\]where the equality is by (4), and the inequality is because of the convexity of \(G()\), \(|_{t}|(1)\), and (4). Taking a telescoping sum, we have

\[_{t=t_{0}}^{T}2_{t}(G(_{t})-G(_{*}))\|_{t_ {0}}-_{*}\|_{2}^{2}-\|_{T+1}-_{*}\|_{2}^{2}+_{t=t_{0}} ^{T}(1)/t^{2}(1).\]

Combing the above with (5) and using \(_{t}(1)/t\) from (4), we get

\[_{t=t_{0}}^{T}_{t}(G(_{T})-G(_{*}))_{t=t_{0} }^{T}_{t}(G(_{t})-G(_{*}))+_{t=t_{0}}^{T}_{t} (1)/t(1).\]

Finally, since \(_{t=t_{0}}^{T}_{t}(1)((T)-(t_{0}))\) according to (4), we get that \(G(_{T})-G(_{*})(1)/((T)-(t_{0}))\).

Step 6: risk convergence.The long-term risk convergence result can be easily established by making use of the implicit bias results we have obtained so far.

## 6 Conclusion

We consider constant-stepsize GD for logistic regression on linearly separable data. We show that with _any_ constant stepsize, GD minimizes the logistic loss; moreover, the GD iterates tend to infinity when projected to a max-margin direction and tend to a fixed minimizer of a strongly convex potential when projected to the orthogonal complement of the max-margin direction. We also show that GD with a large stepsize may diverge catastrophically if the logistic loss is replaced by the exponential loss. Our theory explains how GD minimizes a risk non-monotonically.

## Acknolwdgement

We thank the anonymous reviewers for their helpful comments and Alexander Tsigler for pointing out several typos. VB is partially supported by the Ministry of Trade, Industry and Energy(MOTIE) and Korea Institute for Advancement of Technology (KIAT) through the International Cooperative R&D program. JDL acknowledges the support of the ARO under MURI Award W911NF-11-1-0304, the Sloan Research Fellowship, NSF CCF 2002272, NSF IIS 2107304, NSF CIF 2212262, ONR Young Investigator Award, and NSF CAREER Award 2144994.