# EgoTracks: A Long-term Egocentric Visual Object Tracking Dataset

Hao Tang\({}^{1}\), Kevin J Liang\({}^{1}\), Kristen Grauman\({}^{1,2}\), Matt Feiszli\({}^{1,*}\), Weiyao Wang\({}^{1,*}\)

FAIR, Meta\({}^{1}\), UT Austin\({}^{2}\), Equal Contribution\({}^{*}\)

{haotang, kevinjliang, grauman, mdf, weiyaowang}@meta.com

###### Abstract

Visual object tracking is key to many egocentric vision problems. However, the full spectrum of challenges of egocentric tracking faced by an embodied AI is under-represented in many existing datasets, which tend to focus on short, third-person videos. Egocentric video has several distinguishing characteristics from those commonly found in past datasets: frequent large camera motions and hand interactions with objects commonly lead to occlusions or objects exiting the frame, and object appearance can change rapidly due to widely different points of view, scale, or object states. Embodied tracking is also naturally long-term, and being able to consistently (re-)associate objects to their appearances and disappearances over as long as a lifetime is critical. Previous datasets under-emphasize this re-detection problem, and their "framed" nature has led to adoption of various spatiotemporal priors that we find do not necessarily generalize to egocentric video. We thus introduce EgoTracks, a new dataset for long-term egocentric visual object tracking. Sourced from the Ego4D dataset, EgoTracks presents a significant challenge to recent state-of-the-art single-object trackers, which we find score more poorly on our new dataset than existing popular benchmarks, according to traditional tracking metrics. We further show improvements that can be made to the STARK tracker to significantly increase its performance on egocentric data, resulting in a baseline model we call EgoSTARK. We publicly release our annotations and benchmark ([https://github.com/EG04D/episodic-memory/tree/main/EgoTracks](https://github.com/EG04D/episodic-memory/tree/main/EgoTracks)), hoping our dataset leads to further advancements in tracking.

## 1 Introduction

First-person or "egocentric" computer vision aims to capture the real-world perceptual problems faced by an embodied AI; it has drawn strong recent interest as an underserved but highly relevant domain of vision, with important applications ranging from robotics  to augmented and mixed reality . Visual object tracking (VOT), long a fundamental problem in vision, is a core component to many egocentric tasks, including tracking the progress of an action or activity, (re-)association of objects in one's surroundings, and predicting future states of the environment. Yet, while the VOT field has made many significant advancements over the past decade, tracking in egocentric video remains underexplored. This lack of attention is in large part due to the absence of a large-scale egocentric tracking dataset for training and evaluation. While the community has proposed a number of popular tracking datasets in recent years, including OTB , TrackingNet , GOT-10k , and LaSOT , we find that the strong performance that state-of-the-art trackers achieve on these benchmarks does not translate well to egocentric video, thus establishing a strong need for such a tracking dataset.

We attribute this performance gap to the many unique aspects of egocentric views compared to the more traditional third-person views of previous datasets. In contrast to intentionally "framed" video, egocentric videos are often uncurated, meaning they tend to capture many attention shifts betweenactivities, objects, or locations. Due to the first-person perspective, large head motions from the camera wearer often result in objects repeatedly leaving and re-entering the field of view; similarly, hand manipulations of objects  leads to frequent occlusions, rapid variations in scale and pose, and potential changes in state or appearance. Furthermore, egocentric video tends to be long (sometimes representing the entire life of an agent or individual), meaning the volume of the aforementioned occlusions and transformations scales similarly. These characteristics all make tracking objects in egocentric views dramatically more difficult than scenarios commonly considered in prior datasets, and their absence represents an evaluation blindspot.

Head motions, locomotion, hand occlusions, and temporal length lead to several challenges. First, frequent object disappearances and reappearances causes the problem of _re-detection_ within egocentric tracking to become especially critical. Many previous tracking datasets primarily focus on short-term tracking in third-person videos, providing limited ability to evaluate many of the challenges of long-term egocentric tracking due to low quantity and length of target object disappearances. As a result, competent re-detection is not required for strong performance, leading many recent short-term trackers to neglect it, instead predicting a bounding box for every frame, which may lead to rampant false positives or tracking the wrong object. Additionally, the characteristics of short-term third-person video have also induced designs relying on gradual changes in motion and appearance. As we later show (Section 5.2), many of the motion, context, and scale priors made by previous short-term tracking algorithms fail to transfer to egocentric video.

Notably, re-detection, occlusions, and longer-term tracking have long been recognized as difficult for VOT as a field, leading to recent benchmark construction efforts  emphasizing these aspects. We argue that egocentric video provides a natural source for these challenges at scale while also representing a highly impactful application for tracking, therefore constituting a significant opportunity. We thus present **EgoTracks**: a large-scale long-term egocentric visual object tracking dataset for training and evaluating long-term trackers. Seeking a realistic challenge, we source videos from Ego4D , a large-scale dataset consisting of unscripted, in-the-wild egocentric videos of daily-life activities. The result is a large-scale dataset to evaluate the tracking and re-detection ability of SOT models, with more than 22,028 tracks from 5708 average 6-minute videos. This

Figure 1: A video from the proposed EgoTracks dataset, with yellow clip segments marking when the object (blowtorch) is visible. Note the frequent disappearances and reappearances of the object over an 8 minute video, with lengthy absences, necessitating re-detection to track accurately without false positives. The egocentric nature of the video includes the camera-wearer interacting with the object (Occurrence 2), resulting in significant hand occlusions and dramatic changes in pose.

Figure 2: EgoTracks is an order of magnitude larger than past long-term VOT datasets, with significantly more tracks (circle area) and object disappearances/appearances in longer videos.

constitutes the first large-scale dataset for visual object tracking in egocentric videos in diverse settings, providing a new, significant challenge compared with previous datasets.

We perform a thorough analysis of our new dataset and its new characteristics relative to prior benchmarks, demonstrating its difficulty and the need for further research to develop trackers capable of handling long-term egocentric vision. Our experiments reveal remaining open problems and insights towards promising directions in egocentric tracking. Leveraging these intuitions, we propose multiple simple yet effective changes, such as adjustment of spatiotemporal priors, egocentric data finetuning, and combining multiple templates. We apply these proposed strategies on a state-of-the-art (SOTA) STARK tracker , training a strong tracker dedicated to long-term egocentric tracking: **EgoSTARK**. We hope Ego-STARK can serve as a strong baseline and facilitate future research.

We make the following contributions:

1. We present EgoTracks, the first large-scale long-term object tracking dataset with diverse egocentric scenarios. We analyze its uniqueness in terms of evaluating the re-detection performance of trackers.
2. We conduct comprehensive experiments to understand the performance of many state-of-the-art trackers on the EgoTracks validation set and observe that due to the biases and evaluation blindspots of existing third-person datasets, they tend to struggle.
3. We perform an analysis of what makes a good tracker for long-form egocentric video. Applying these learnings to the STARK tracker , we produce a strong baseline we call EgoSTARK, which achieves significant improvements (+15% F-score) on EgoTracks.

## 2 Related work

### Visual object tracking datasets

Visual object tracking studies the joint spatial-temporal localization of objects in videos. From a video and a predefined taxonomy, multiple object tracking (MOT) models simultaneously detect, recognize, and track multiple objects. For example, MOT  tracks humans, KITTI  tracks pedestrians and cars, and TAO  tracks a large taxonomy of 833 categories. In contrast, single object tracking (SOT) follows a single object via a provided initial template of the object, without any detection or recognition. Thus, SOT is often taxonomy-free and operates on generic objects. The community has constructed multiple popular benchmarks to study this problem, including OTB , UAV , NfS , TC-128 , NUS-PRO , GOT-10k , VOT , and TrackingNet .

While these SOT datasets mainly consist of short videos (e.g. a few seconds), long-term tracking has been increasingly of interest. Tracking objects in longer videos (several minutes or more) poses unique challenges, e.g. significant transformations, displacements, disappearances, and reappearances. On top of localizing the object when visible, the model also must produce no box when the object is absent, and then re-localize the same object when it reappears. OxUVA  is one of the first to benchmark longer videos (average 2 minutes), with 366 evaluation-only videos. LaSOT  scales this to 1400 videos with more frequent object reappearances. Concurrently, VOT-LT  includes frequent object disappearances and reappearances in 50 purposefully selected videos.

Our EgoTracks focuses on long-term SOT and presents multiple critical and unique attributes: 1) significantly larger scale, with **5708** videos of an average **6 minutes (Figure 2); 2) more frequent disappearances & reappearances (avg. **17.7** times) happening in natural, real-world scenarios; 3) data sourced from egocentric videos shot in-the-wild, involving unique challenging situations, such as large camera motions, diverse perspective changes, hand-object interactions, and frequent occlusions.

### Single object tracking methodologies

Many modern approaches use convolutional neural networks (CNNs), either with Siamese network  or correlation-filter based  architectures. With recent successes in vision tasks like classification  and detection , Transformer architecture  for tracking have also become popular. For example, TransT  uses attention-based feature fusion to combine features of the object template and search image. More recently, several works utilize Transformers as direct predictors to achieve a new state of the art, such as STARK , ToMP  and SBT . These models tokenize frame features from a ResNet  encoder, and use a Transformer to predict the bounding box and object presence score with the feature tokens. These methods are often developed on short-term SOT datasets and assume that the target object stays in the field of view with minimum occlusions. On the other hand, long-term trackers  are designed to cope with the problem of re-detecting objects in their reappearances. Designed to be aware of potential object disappearances, these approaches search the whole image for its reappearance.

### Tracking in egocentric videos

Multiple egocentric video datasets have been introduced in the past decades [12; 29; 41; 71; 63; 23], offering a host of interesting challenges, many of which require associating objects across frames: activity recognition [36; 46; 85; 80; 27], anticipation [22; 24; 28], video summarization [16; 41; 42; 52], human-object interaction [14; 50], episodic memory , visual query , and camera-wearer pose inference . To tackle these challenges, tracking is leveraged in many methodologies [29; 14; 51; 42; 50], yet few works have been dedicated to this fundamental problem on its own. [19; 20] have started to recognize the challenges of egocentric object tracking and might be the most related work to ours. The major difference, however, is the scale of the dataset: [19; 20] contain 150 tracks intended only for evaluation, while EgoTracks is 100\(\) larger (see Table 1), containing 20k tracks with training and evaluation splits. Also, while past efforts have sourced videos from the kitchen-heavy EPIC-KITCHEN , EgoTracks sources videos from Ego4D , which has more diverse scenarios. EgoTracks provides a unique, large-scale testbed for developing tracking methods dedicated to egocentric videos; our improved baseline EgoSTARK also serves as a potential plug-and-play module to solve other tasks where object association is desired.

In egocentric video understanding, Ego4D  and EPIC-KITCHENS VISOR  are closely related. Ego4D contains the largest collection of egocentric videos in-the-wild; EgoTracks is annotated on a subset of Ego4D. In addition, Ego4D proposes many novel tasks, such as Episodic Memory, with tracking identified as a core component. VISOR was introduced concurrently, annotating short-term (12 sec on average) videos from EPIC-KITCHENS  with instance segmentation masks. We believe EgoTracks offers multiple unique values complementary to EPIC-VISOR: long-term tracking (6 min vs. 12 sec), significantly larger-scale (5708 video clips vs. 158), and more diversified video sources (80+ scenes vs. kitchen-only; see Figure 4).

## 3 The EgoTracks dataset

We present EgoTracks: a large-scale long-term egocentric single object tracking dataset, consisting of a total of 22028 tracks from 5708 videos. We follow the same data split as the Ego4D Visual Queries (VQ) 2D benchmark (see Appendix A for details).

### Ego4D visual queries (VQ) benchmark

Ego4D  is a massive-scale egocentric video dataset, consisting of 3670 hours of diverse daily-life activities of consenting participants in an in-the-wild format; the videos have personally identifiable information removed and were screened for offensive content. The dataset is accompanied by multiple benchmarks, but the most relevant task for our purposes is episodic memory's 2D VQ task: Given an egocentric video and a cropped image of an object, the goal is to localize when and where the object was last seen in the video, as a series of 2D bounding boxes in consecutive frames. This task is closely related to long-term tracking: finding an object in a video given a visual template is identical

  
**Dataset** & **Video Hours** & **Avg. Length (s)** & **Ann. FPS** & **Ann. Type** & **Egocentric** & 
 **SOTA** \\ **(P/AO)\({}^{*}\)** \\  \\  ImageNet-Vid  & 15.6 & 10.6 & 25 & mask & No & \\ Y-VOS  & 5.8 & 4.6 & 5 & mask & No & -/83.6  \\ DAVIS 17  & 0.125 & 3 & 24 & mask & No & -/86.3  \\ TAO  & 29.7 & 36.8 & 1 & mask & No & \\ UVO  & 2.8 & 10 & 30 & mask & No & -/73.7  \\ EPIC-KITCHENS & & 36 & 12\({}^{**}\) & 0.9 & mask & **Yes** & -/74.2  \\ VISOR & & & & & & \\ GOT-10k  & 32.8 & 12.2 & 10 & bbox & No & -/75.6  \\ Ox1vA  & 14.4 & 141.2 & 1 & bbox & No & \\ LaSOT  & 31.92 & 82.1 & 30 & bbox & No & 80.3/-  \\ TrackingNet  & 125.1 & 14.7 & 28 & bbox & No & 86/-  \\ TREK-150 [19; 20] & 0.45 & 10.81 & 60 & bbox & **Yes** & -/50.5 [19; 20] \\
**EgoTracks (Ours)** & **602.9** & **367.9** & 5 & bbox & **Yes** & 45/54.1 \\   

* *: P: Precision, AO: average overlap (J-Score for mask-based datasets). *†: Original video is 720s.

Table 1: **Object tracking datasets comparison.** In addition to larger scale than previous datasets, the scenarios captured by EgoTracks represent a significantly harder challenge for SOTA trackers, suggesting room for improved tracking methodology.

to the re-detection problem in long-term tracking. Moreover, Ego4D's baselines rely heavily on tracking methods: Siam-RCNN  and KYS  for global and local tracking, respectively.

**Shortcomings.** While highly related, the VQ dataset is not immediately suitable long-term tracking. In particular, the VQ annotation guidelines were roughly the following: 1) identify three different objects that appear multiple times in the video; 2) annotate a query template for each object, which should contain the entire object without any motion blur; 3) annotate an occurrence of the object that is temporally distant from the template. Thus, these annotations are not exhaustive over time (they are quite sparse), limiting their applicability to tracking. On the other hand, the selection criteria result in a strong set of candidate objects, which we leverage to build EgoTracks.

### Annotating VQ for long-term tracking

We thus start with the VQ visual crop and response track, asking annotators to first identify the object represented by the visual crop, the response track, and object name. From the video's start, we instruct the annotators to draw a bounding box around the object each time it appears. Because annotators must go through each video in its entirety, which contain an average of \(\)1800 frames at 5 frames per second (FPS), this annotation task is labor-intensive, taking roughly 1 to 2 hours per track. An important aspect of this annotation is its exhaustiveness: the entire video is densely annotated for the target object, and any frame without a bounding box is considered as a negative. Being able to reject negatives examples is an important component to re-detection in real-world settings, as false positives can impact certain applications as much as false negatives.

**Quality Assurance.** All tracks are quality checked by expert annotators after the initial annotations. To measure the annotation quality, we employ multi-review on a subset of the validation set. Three independent reviewers are asked to annotate the same video. We find the overlaps between these independent annotations are high (\(>0.88\) IoU). Further, since EgoTracks has a focus on re-detection, we check the temporal overlap of object presence and find it to be consistent across annotators. In total, the entire annotation effort represented roughly 86k worker-hours of effort.

### Tracklet attributes

In addition to the bounding box annotations, we also label certain relevant attributes to allow for different training strategies or deeper analysis of validation set performance. We annotate the following three attributes per occurrence (see Figure 3 for examples and Table 2 for statistics):

* is_active: In Ego4D, the camera wearer often interacts with relevant objects with their hands. Objects in the state of being handled pose a challenge for tracking algorithms due to frequent occlusion and rapid changes in pose.
* is_transformed: Objects in Ego4D may undergo transformations, such as deformations and state changes. Such instances require being able to quickly adapt to the tracked object having a new appearance.
* is_recognizable: Due to occlusions, motion blur, scale, or other conditions, some objects in Ego4D can be extremely difficult to recognize without additional context. We thus annotate if the object is recognizable solely based on its appearance, without using additional context information (_e.g._ other frames).

    & **Total number** & **Percentage** \\  All Tracks & 17593 & 100\% \\ is\_active & 3963 & 22.52\% \\ is\_transformed & 1080 & 6.13\% \\ is\_recognizable & 17557 & 99.79\% \\   

Table 2: **Track attributes** in train/val sets.

Figure 3: **EgoTracks tracklet attribute examples.**_Left_: Micropipette on a bench (top) versus actively used (bottom). _Middle_: A paint can (top) is opened (bottom). _Right_: A blowtorch (top) requiring context from other frames to identify due to distance and motion blur (bottom).

### Data diversity

Figure 4 shows the diversity of EgoTracks in terms of both the actions the videos contain and name of the object tracks. Ego4D  videos depict a wide variety of in-the-wild activities (Figure 4, left), captured in locations all around the world. As such, they represent a far more diverse set of scenes than other egocentric datasets , thus requiring models to learn more general representations.

There was no fixed taxonomy for which objects to track. Instead, annotators were asked to choose objects that are "interesting" , which were named freely, resulting in considerable object diversity. We extracted the nouns from these object names, resulting in around 1000+ unique names. Figure 4, right shows the wide array of objects tracked in EgoTracks.

## 4 Analysis of state-of-the-art SOT trackers

We compare the performance of several off-the-shelf tracking models on EgoTracks's validation set. Identifying STARK  as the one with the best performance, we conduct further ablation studies under different settings using STARK to further understand its behavior.

### Evaluation protocols and metrics

**Evaluation Protocols.** We introduce several evaluation protocols for EgoTracks, consisting of different combinations of the initial template, evaluated frames, and the temporal direction in which the tracker is run. For the initial template, we consider two choices:

Figure 4: EgoTracks is a large-scale egocentric dataset of diverse scenarios (left) and objects (right).

Figure 5: Qualitative results of different trackers. EgoTracks presents significant challenges for all trackers, due to drastic viewpoint changes, occlusions, changes in scale, head motion etc.

* **Visual Crop Template (VCT)**: The visual crop images were specifically chosen to be high-quality views of the target and served as our annotators' references for identifying the object throughout the videos. Thus, they make ideal candidates for initializing a tracker.
* **Occurrence First Frame Template (OFFT)**: The tracker is initialized with the first frame of each occurrence (see \(}\) below). While this may result in a lower quality view of the object, temporal proximity to subsequent frames means it may be closer in appearance.

Note that we exclude the template frame from the calculation of any evaluation metrics. We also consider several choices for the evaluated frames and temporal direction:

* **Video Start Forward (\(}\)**): The tracker is evaluated on every frame of the video in causal order, starting from the first frame. This represents a tracker's ability to follow an object through a long video.
* **Visual Crop Forward/Backward (\(}\))**: The tracker is run on the video twice, once starting at the visual crop frame and running forward and time, and a second time running backwards. This represents an alternative way of covering every frame in the video, but with closer visual similarity between **VCT** initialization and the first frames encountered by the tracker.
* **Occurrences Only Forward (\(}\))**: The tracker is only evaluated on the object occurrences, when the object is visible. This simplifies the tracking task and allows us to dis-entangle the challenge of re-detection from that of simply tracking in an egocentric clip.

We specify protocols by concatenating the appropriate descriptors. We primarily consider **VCT-\(}\)**, **VCT-\(}\)**, **VCT-\(}\)**, and **OFFT-\(}\)** (Table 3) in our experiments.

**Metrics.** We adopt common metrics in object tracking, including F-score, precision, and recall; details can be found in . Trackers are ranked mainly by the F-score. We additionally consider average overlap (AO), success, precision, and normalized precision as short-term tracking metrics .

### Sot trackers struggle on EgoTracks

We compare the performance of select trackers on EgoTracks with the **VCT-\(}\)** evaluation protocol. Given the breadth of tracking algorithms, we do not aim to be exhaustive but select high-performing representatives of different tracking principles. KYS  and DiMP  are two short-term tracking algorithms that maintain an online target representation. ToMP , STARK  and MixFormer [9; 10] are three examples of the SOTA short-term trackers based on Transformers. GlobalTrack  is a global tracker that searches the entire search image for re-detection. LTMU  is a high performance long-term tracker that combines a global tracker (GlobalTrack) with a local tracker (DiMP). Siam R-CNN  leverages dynamic programming to model a full path of history for long-term. The performance of these trackers on EgoTracks are summarized in Table 4. AO in this table is equivalent to recall at the probability threshold 0. Qualitative results are shown in Figure 5.

  
**Method** & **AO** & **F-score** & **Precision** & **Recall** & **FPS** \\  KYS  & 16.09 & 13.09 & 12.50 & 13.74 & 20 \\ DiMP  & 16.45 & 11.84 & 10.31 & 13.91 & 43 \\ GlobalTrack \({}^{}\) & 23.63 & 20.40 & 31.28 & 15.14 & 6 \\ LTMU \({}^{}\) & 29.33 & 27.46 & 37.28 & 21.74 & 13 \\ ToMP  & 30.93 & 20.95 & 19.63 & 22.46 & 24.8 \\ Siam-RCNN \({}^{}\) & 37.48 & 35.38 & 52.80 & 26.67 & 4.7 \\ MixFormer (MixViT-L, ConvMAE) [9; 10] & 27.93 & 25.54 & 28.30 & 23.27 & 10 \\ STARK  - Res50 & 35.99 & 30.48 & 34.70 & 27.17 & 41.8 \\ STARK  - Res101 & 35.03 & 30.18 & 35.30 & 26.35 & 31.7 \\  
**Tracking by Detection \({}^{}\)** & & & & & \\ Mask R-CNN +Oracle & 60.00 & - & - & - & \\ GGN +Oracle & 75.92 & - & - & - & \\ GGN+InstEmb & 15.19 & 9.92 & 11.75 & 8.58 & \\    \({}^{}\): trackers with re-detection

Table 4: **EgoTracks performance comparison.** Off-the-shelf, all trackers perform poorly, demonstrating the new challenges of EgoTracks. Higher performance from tracking by detection methods + Oracle imply that instance association, not detection, is one of the primary challenges.

[MISSING_PAGE_FAIL:8]

### Evaluating EgoSTARK on EPIC-VISOR

To demonstrate egocentric object tracking generalization from training on EgoTracks, we perform further quantitative evaluations on the EPIC-VISOR dataset , an egocentric dataset focused on active objects in kitchen scenes. We observe that training with EgoTracks leads to both Siam R-CNN and STARK showing improvements in tracking on EPIC-VISOR (Table 8), demonstrating the value of EgoTracks as a large-scale pre-training dataset. By additionally incorporating the improvements we propose in Section 5, EgoSTARK achieves further improvements on EPIC-VISOR.

## 5 Egocentric tracking design considerations

Observing that existing trackers do not perform well on EgoTracks, we perform a systematic exploration of priors and other design choices for egocentric tracking. Though not specifically designed for long-term tracking, Section 4 suggests STARK  to be the most competitive tracker on EgoTracks. We focus on this tracker for additional analysis, suggesting improvements to egocentric performance.

### Egocentric finetuning is essential

We first demonstrate how various trackers trained on third-person videos can significantly benefit from finetuning on EgoTracks. As shown in Table 7, all methods gain improvement on F-score ranging from 6% - 10%. In addition, as shown in Table 9, finetuning on the VQ response track subset improves the F-score from 30.48% to 33.53%, while using the full EgoTracks annotation further improves the F-score by 4.67% to 38.2%. This demonstrates that: 1) finetuning with egocentric data helps close the exocentric-egocentric domain gap; 2) training on full EgoTracks provides further gains, showing the value of our training set.

### Third-person spatiotemporal priors fail

Modern SOTs find certain assumptions on object motion, appearance, and surroundings helpful on past datasets, but some of these design choices translate poorly to long-term egocentric videos.

**Search window size.** An example is local search. Many trackers assume the tracked object appears within a certain range of its previous location. Thus, for efficiency, these methods often search within a local window of the next frame. This is reasonable in high FPS, smooth videos with relatively slow motion, commonly in previous short-term tracking data, but in egocentric videos, the object's pixel coordinates can change rapidly (frequent large head motions), and re-detection becomes a key problem. Therefore, we experiment with expanded search regions beyond what are common in past methods. As we expand search size from 320 to 800, we see dramatic improvements (Table 9): STARK is able to locate objects that were previously outside search window due to rapid movements.

**Multiscale augmentations.** The characteristics of egocentric video also affect common SOT assumptions of object scale. Many trackers are trained with the assumption that an object's scale is consistent with the template image and between adjacent frames. However, large egocentric camera

    & **Method** & **AO** & **F-score** & **Precision** & **Recall** \\   & STARK & 35.99 & 30.48 & 34.70 & 27.17 \\  & STARK - R on VQ & 38.94 & 33.53 & 39.13 & 29.33 \\  & STARK - R on EgoTracks & 44.25 & 38.20 & 42.06 & 24.39 \\   & STARK - R on VQ & 38.94 & 33.53 & 39.13 & 29.33 \\  & STARK - R on VQ & 38.94 & 33.53 & 39.13 & 29.33 \\   & STARK - R on \(\) 4mike & 48.41 & 49.12 & 42.68 & 41.30 \\  & STARK - R on \(\) 320 & 35.99 & 34.86 & 34.70 & 27.17 \\  & search\_size - R on \(\) 380 & 48.21 & 39.69 & 43.95 & 36.19 \\  & search\_size - R on \(\) 640 & 52.09 & 42.39 & 46.23 & 39.15 \\   & 38.5 & 34.08 & 37.47 & 47.60 & 40.45 \\   & STARK & 38.00 & 34.83 & 47.60 & 40.45 \\   

Table 8: Evaluating EgoTracks on EPIC-VISOR .

    & **Method** & **AO** & **F-score** & **Precision** & **Recall** \\   & STARK & 35.99 & 30.48 & 34.70 & 27.17 \\  & STARK - R on VQ & 38.94 & 33.53 & 39.13 & 29.33 \\  & STARK - R on EgoTracks & 44.25 & 38.20 & 42.06 & 24.39 \\   & STARK - R on VQ & 38.94 & 33.53 & 39.13 & 29.33 \\  & STARK - R on VQ & 38.94 & 33.53 & 39.13 & 29.33 \\   & STARK - R on \(\) 4mike & 48.41 & 49.12 & 42.68 & 41.30 \\  & STARK - R on \(\) 320 & 35.99 & 34.86 & 34.70 & 27.17 \\  & search\_size - R on \(\) 380 & 48.21 & 39.69 & 43.95 & 36.19 \\  & search\_size - R on \(\) 640 & 52.09 & 42.39 & 46.23 & 39.15 \\   & 38.5 & 34.08 & 37.41 & 47.60 & 40.45 \\   & STARK & 38.94 & 33.53 & 39.13 & 29.33 \\   & 28.3 & 10.4 & 64.00 & 52.49 & 42.39 & 46.23 \\   & 28.2 & 12.58 & 800 & 54.08 & 43.74 & 47.60 \\   

Table 9: Train/test-time hyperparameters comparison.

motions, locomotion, and hand interactions with objects (_e.g_. bringing an object to one's face, as in eating) can translate to objects rapidly undergoing large changes in scale. We thus propose adding scale augmentations during training, randomly resizing the search image by a factor of \(s[0.5,1.5]\). While simple, we find this dramatically improves performance on EgoTracks, improving STARK's AO by nearly \(10\%\) and F-score by more than \(8\%\) (Table 9).

**Context ratio.** Past SOT works have found that including some background can be helpful for template image feature extraction, with twice the size of the object being common. We experiment with different context ratios to see if this rule of thumb transfers to egocentric videos. Because of the local window assumption, the sizes of the template and search images are related: \(}{}=}{}=\). The template image size is set to a fixed size \(128 128\). When changing the context ratio, we carefully control the other parameters for a fair comparison. The results are shown in Table 10. Among all three parameters - **CR**, **SRR**, and **SIS**, the search region size (determined by **SRR** and **SIS**) has the highest impact on the F-score. This is expected because there are frequent re-detections, which require the tracker to search in a larger area for the object, rather than just within the commonly used local window. Varying the **CR** has mixed results so we adhere to the common practice of using a **CR** of 2.

## 6 Conclusion

We present EgoTracks, the first large-scale dataset for long-term egocentric visual object tracking in diverse scenes. We conduct extensive experiments to understand the performance of state-of-the-art trackers on this new dataset, and find that they struggle considerably, possibly in part due to overfitting to some of the simpler characteristics of existing benchmarks. We thus propose several adaptations for the egocentric domain, leading to a strong baseline that we call Ego-STARK, which has vastly improved performance on EgoTracks. By publicly releasing this dataset, we hope to encourage advancements in the field of long-term tracking and draw more attention to the challenges of long-term and egocentric videos.

**Challenges and future directions.** Based on our experiments in Tables 4 and 5, we find re-detection to be a key challenge of long-term tracking, especially in egocentric video, where objects frequently go in and out of view, or are exposed to high motion blur. We see a few promising future directions:

* **Stronger features**: The sizable gap in performance between the Oracle and InstEmb variants of Tracking by Detection (Table 4) illustrates the impact of insufficiently discriminative features. Better features, for example with geometric keypoints, optical flow, or long-term trajectories , would allow improved association of objects and thus re-detection.
* **Leveraging spatial signals**: Camera trajectories can provide powerful spatiotemporal priors to trackers. For example, knowledge of camera pose can help re-localize a object whose position does not change while out of view.
* **Global, multi-view object representations**: Egocentric videos, with their diverse camera trajectories and tendency to capture interactions with objects, often offer significantly more varied viewpoints of objects than traditional third-person tracking datasets. In the latter, object appearances tend to be more constant, so prior tracking methods can often get away with a single image template. The aforementioned characteristics of egocentric video necessitate more robustness, and a challenging egocentric tracking dataset like EgoTracks represents an opportunity to develop more global, view-variant object representations learned in an online fashion. A simple version can be found in Appendix D, where we augmented EgoSTARK by fusing multiple templates, which we find indeed improved EgoTracks results.