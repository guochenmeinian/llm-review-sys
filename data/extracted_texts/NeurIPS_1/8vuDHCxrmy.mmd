# OpenMask3D:

Open-Vocabulary 3D Instance Segmentation

 Ayca Takmaz\({}^{1}\)

Eliasabetta Fedele\({}^{1}\)

Robert W. Sumner\({}^{1}\)

Marc Pollefeys\({}^{1,2}\)

Federico Tombari\({}^{3}\)

Francis Engelmann\({}^{1,3}\)

\({}^{1}\)ETH Zurich \({}^{2}\)Microsoft \({}^{3}\)Google

openmask3d.github.io

Equal contribution.

###### Abstract

We introduce the task of open-vocabulary 3D _instance_ segmentation. Current approaches for 3D instance segmentation can typically only recognize object categories from a pre-defined closed set of classes that are annotated in the training datasets. This results in important limitations for real-world applications where one might need to perform tasks guided by novel, open-vocabulary queries related to a wide variety of objects. Recently, open-vocabulary 3D scene understanding methods have emerged to address this problem by learning queryable features for each point in the scene. While such a representation can be directly employed to perform _semantic_ segmentation, existing methods cannot separate multiple object _instances_. In this work, we address this limitation, and propose OpenMask3D, which is a zero-shot approach for open-vocabulary 3D _instance_ segmentation. Guided by predicted class-agnostic 3D instance masks, our model aggregates per-mask features via multi-view fusion of CLIP-based image embeddings. Experiments and ablation studies on ScanNet200 and Replica show that OpenMask3D outperforms other open-vocabulary methods, especially on the long-tail distribution. Qualitative experiments further showcase OpenMask3D's ability to segment object properties based on free-form queries describing geometry, affordances, and materials.

## 1 Introduction

3D instance segmentation, which is the task of predicting 3D object instance masks along with their object categories, has many crucial applications in fields such as robotics and augmented reality. Due to its significance, 3D instance segmentation has been receiving a growing amount of attention in recent years. Despite the remarkable progress made in 3D instance segmentation methods , it is noteworthy that these methods operate under a closed-set paradigm, where the set of object categories is limited and closely tied to the datasets used during training.

We argue that there are two key problems with _closed-vocabulary_ 3D instance segmentation. First, these approaches are limited in their ability to understand a scene beyond the object categories seen during training. Despite the significant success of 3D instance segmentation approaches from recent years, these closed-vocabulary approaches may fail to recognize and correctly classify novel objects. One of the main advantages of open-vocabulary approaches, is their ability to zero-shot learn

Figure 1: **Open-Vocabulary 3D Instance Segmentation. Given a 3D scene _(top)_ and free-form user queries _(bottom)_, our OpenMask3D segments object instances and scene parts described by the open-vocabulary queries.**categories that are not present at all in the training set. This ability has potential benefits for many applications in fields such as robotics, augmented reality, scene understanding and 3D visual search. For example, it is essential for an autonomous robot to be able to navigate in an unknown environment where novel objects can be present. Furthermore, the robot may need to perform an action based on a free-form query, such as "find the side table with a flower was on it", which is challenging to perform with the existing closed-vocabulary 3D instance segmentation methods. Hence, the second key problem with closed-vocabulary approaches is their inherent limitation to recognize only object classes that are predefined at training time.

In an attempt to address and overcome the limitations of a closed-vocabulary setting, there has been a growing interest in open-vocabulary approaches. A line of work [20; 43; 45] investigates open-vocabulary 2D image segmentation task. These approaches are driven by advances in large-scale model training, and largely rely on recent foundation models such as CLIP  and ALIGN  to obtain text-image embeddings. Motivated by the success of these 2D open-vocabulary approaches, another line of work has started exploring 3D open-vocabulary scene understanding task [24; 32; 52], based on the idea of lifting image features from models such as CLIP , LSeg , and OpenSeg  to 3D. These approaches aim to obtain a task-agnostic feature representation for each 3D point in the scene, which can be used to query concepts with open-vocabulary descriptions such as object semantics, affordances or material properties. Their output is typically a heatmap over the points in the scene, which has limited applications in certain aspects, such as handling object instances.

In this work, we propose OpenMask3D, an open-vocabulary 3D instance segmentation method which has the ability to reason beyond a predefined set of concepts. Given an RGB-D sequence, and the corresponding 3D reconstructed geometry, OpenMask3D predicts 3D object instance masks, and computes a _mask-feature_ representation. Our two-stage pipeline consists of a class-agnostic mask proposal head, and a mask-feature aggregation module. Guided by the predicted class-agnostic 3D instance masks, our mask-feature aggregation module first finds the frames in which the instances are highly visible. Then, it extracts CLIP features from the best images of each object mask, in a multi-scale and crop-based manner. These features are then aggregated across multiple views to obtain a feature representation associated with each 3D instance mask. Our approach is intrinsically different from the existing 3D open-vocabulary scene understanding approaches [24; 32; 52] as we propose an instance-based feature computation approach instead of a point-based one. Computing a _mask-feature_ per object instance enables us to retrieve object instance masks based on their similarity to any given query, equipping our approach with open-vocabulary 3D instance segmentation capabilities. As feature computation is performed in a zero-shot manner, OpenMask3D is capable of preserving information about novel objects as well as long-tail objects better, compared to trained or fine-tuned counterparts. Furthermore, OpenMask3D goes beyond the limitations of a closed-vocabulary paradigm, and allows segmentation of object instances based on free-form queries describing object properties such as semantics, geometry, affordances, and material properties.

Our contributions are three-fold:

* We introduce the open-vocabulary 3D instance segmentation task in which the object instances that are similar to a given text-query are identified.
* We propose OpenMask3D, which is the first approach that performs open-vocabulary 3D instance segmentation in a zero-shot manner.
* We conduct experiments to provide insights about design choices that are important for developing an open-vocabulary 3D instance segmentation model.

## 2 Related work

**Closed-vocabulary 3D semantic and instance segmentation.** Given a 3D scene as input, 3D semantic segmentation task aims to assign a semantic category to each point in the scene [2; 3; 4; 9; 14; 15; 22; 28; 29; 31; 38; 42; 44; 46; 47; 53; 54; 63; 64; 66; 68; 70; 73]. 3D instance segmentation goes further by distinguishing multiple objects belonging to the same semantic category, predicting individual masks for each object instance [13; 16; 25; 27; 34; 41; 58; 62; 65; 67; 74]. The current state-of-the-art approach on the ScanNet200 benchmark [10; 57] is Mask3D , which leverages a transformer architecture to generate 3D mask proposals together with their semantic labels. However, similar to existing methods, it assumes a limited set of semantic categories that can be assigned to an object instance. In particular, the number of labels is dictated by the annotations provided in the training datasets, 200 - in the case of ScanNet200 . Given that the English language encompassesnumerous nouns, in the order of several hundred thousand , it is clear that existing closed-vocabulary approaches have an important limitation in handling object categories and descriptions.

**Foundation models.** Recent multimodal foundation models  leverage large-scale pretraining to learn image representations guided by natural language descriptions. These models enable zero-shot transfer to various downstream tasks such as object recognition and classification. Driven by the progress in large-scale model pre-training, similar foundation models for images were also explored in another line of work , which aim to extract class-agnostic features from images. Recently, steps towards a foundation model for image segmentation were taken with SAM . SAM has the ability to generate a class-agnostic 2D mask for an object instance, given a set of points that belong to that instance. This capability is valuable for our approach, especially for recovering high-quality 2D masks from projected 3D instance masks, as further explained in Sec. 3.2.2.

**Open vocabulary 2D segmentation.** As large vision-language models gained popularity for image classification, numerous new approaches  have emerged to tackle open-vocabulary or zero-shot image semantic segmentation. One notable shift was the transition from image-level embeddings to pixel-level embeddings, equipping models with localization capabilities alongside classification. However, methods with pixel-level embeddings, such as OpenSeg  and OV-Seg , strongly rely on the accuracy of 2D segmentation masks, and require a certain degree of training. In our work, we rely on CLIP  features without performing finetuning or any additional training, and compute 2D masks using the predicted 3D instance masks.

**Open-vocabulary 3D scene understanding.** Recent success of 2D open-vocabulary segmentation models such as LSeg , OpenSeg , and OV-Seg has motivated researchers in the field of 3D scene understanding to explore the open vocabulary setting . OpenScene  uses per-pixel image features extracted from posed images of a scene and obtains a point-wise task-agnostic scene representation. On the other hand, approaches such as LERF  and DFF  leverage the interpolation capabilities of NeRFs  to extract a semantic field of the scene. However, it is important to note that all of these approaches have a limited understanding of object _instances_ and inherently face challenges when dealing with instance-related tasks.

## 3 Method

**Overview.** Our OpenMask3D model is illustrated in Fig. 2. Given a set of posed RGB-D images captured in a scene, along with the reconstructed scene point cloud \(\), OpenMask3D predicts 3D instance masks with their associated per-mask feature representations, which can be used for querying instances based on open-vocabulary concepts \(\). Our OpenMask3D has two main building blocks: a _class agnostic mask proposal head_\(\) and a _mask-feature computation module_\(\). The class-agnostic mask proposal head predicts binary instance masks over the points in the point cloud. The mask-feature computation module leverages pre-trained CLIP  vision-language model in order to compute meaningful and flexible features for each mask. For each proposed instance mask, the mask-feature computation module first selects the views in which the 3D object instance is highly visible. Subsequently, in each selected view, the module computes a 2D segmentation mask guided by the projection of the 3D instance mask, and refines the 2D mask using the SAM  model. Next, the CLIP encoder is employed to obtain image-embeddings of multi-scale image-crops bounding the computed 2D masks. These image-level embeddings are then aggregated across the selected frames in order to obtain a mask-feature representation. Sec. 3.1 describes the class agnostic mask proposal head, and Sec. 3.2 describes the mask-feature computation module.

The key novelty of our method is that it follows an _instance-mask oriented_ approach, contrary to existing 3D open-vocabulary scene understanding models which typically compute _per-point_ features. These point-feature oriented models have inherent limitations, particularly for identifying object _instances_. Our model aims to overcome such limitations by introducing a framework that employs class agnostic instance masks and aggregates informative features for each object _instance_.

**Input.** Our pipeline takes as input a collection of posed RGB-D images captured in an indoor scene, and the reconstructed point cloud representation of the scene. We assume known camera parameters.

### Class agnostic mask proposals

The first step of our approach involves generating \(M\) class-agnostic 3D mask proposals \(_{}^{},,_{}^{}\). Let \(^{P 3}\) denote the point cloud of the scene, where each 3D point is represented with its corresponding 3D coordinates. Each 3D mask proposal is represented by a binary mask \(_{}^{}=(m_{i1}^{3D},,m_{iF}^{3D})\) where \(m_{ij}^{3D}\{0,1\}\) indicates whether the \(j\)-th point belongs to \(i\)-th object instance. To generate these masks, we leverage the transformer-based mask-module of a pre-trained 3D instance segmentation model , which is frozen during our computations. The architecture consists of a sparse convolutional backbone based on the MinkowskiUNet , and a transformer decoder. Point features obtained from the feature backbone are passed through the transformer decoder, which iteratively refines the instance queries, and predicts an instance heatmap for each query. In the original setup,  produces two outputs: a set of \(M\) binary instance masks obtained from the predicted heatmaps, along with predicted class labels (from a predefined closed set) for each mask. In our approach, we adapt the model to exclusively utilize the binary instance masks, discarding the predicted class labels and confidence scores entirely. These _class-agnostic_ binary instance masks are then utilized in our mask-feature computation module, in order to go beyond semantic class predictions limited to a closed-vocabulary, and obtain open-vocabulary representations instead. Further details about the class-agnostic mask proposal module are provided in Appendix A.1.

### Mask-feature computation module

Mask-feature computation module aims to compute a task-agnostic feature representation for each predicted instance mask obtained from the class-agnostic mask proposal module. The purpose of this module is to compute a feature representation that can be used to query open-vocabulary concepts. As we intend to utilize the CLIP text-image embedding space and maximally retain information about long-tail or novel concepts, we solely rely on the CLIP visual encoder to extract image-features on which we build our mask-features.

As illustrated in Fig. 3, the mask-feature computation module consists of several steps. For each instance mask-proposal, we first compute the visibility of the object instance in each frame of the RGB-D sequence, and select the top-\(k\) views with maximal visibility. In the next step, we compute a 2D object mask in each selected frame, which is then used to obtain multi-scale image-crops in order to extract effective CLIP features. The image-crops are then passed through the CLIP visual encoder to obtain feature vectors that are average-pooled over each crop and each selected view, resulting in the final mask-feature representation. In Sec. 3.2.1, we describe how we select a set of top-\(k\) frames for each instance. In Sec. 3.2.2, we describe how we crop the frames, based on the object instance we want to embed. In Sec. 3.2.3, we describe how we compute the final mask-features per object.

#### 3.2.1 Frame selection

Obtaining representative _images_ of the proposed object instances is crucial for extracting accurate CLIP features. To achieve this, we devise a strategy to select, for each of the \(M\) predicted instances, a subset of representative frames (Fig. 3, ) from which we extract CLIP features. In particular, our devised strategy selects frames based on their visibility scores \(s_{ij}\) for each mask \(i\) in each view \(j\). Here, we explain how we compute these visibility scores.

Figure 2: **An overview of our approach. We propose OpenMask3D, the first open-vocabulary 3D instance segmentation model. Our pipeline consists of four subsequent steps: 1⃝ Our approach takes as input posed RGB-D images of a 3D indoor scene along with its reconstructed point cloud. 2⃝ Using the point cloud, we compute class-agnostic instance mask proposals. 3⃝ Then, for each mask, we compute a feature representation. 4⃝ Finally, we obtain an open-vocabulary 3D instance segmentation representation, which can be used to retrieve objects related to queried concepts embedded in the CLIP  space.**

Given a mask proposal \(^{3D}}\) corresponding to the \(i\)-th instance, we compute the visibility score \(s_{ij}\) for the \(j\)-th frame using the following formula:

\[s_{ij}=(i,j)}{_{j^{}}((i,j^{ }))}\]

Here, \((i,j)\) represents the number of points from mask \(i\) that are visible in frame \(j\). Note that we assume that each mask is visible in at least one frame. We compute \((i,j)\) using the following approach. First, for each 3D point from the \(i\)-th instance mask, we determine whether the point appears in the camera's field of view (FOV) in the \(j\)-th frame. To achieve this, we use intrinsic (\(\)) and extrinsic (\([]\)) matrices of the camera from that frame to project each 3D point in the point cloud to the image plane. We obtain corresponding 2D homogeneous coordinates \(}=(u,v,w)^{}=([] \), where \(=(x,y,z,1)^{}\) is a point represented in homogeneous coordinates. We consider a point to be in the camera's FOV in the \(j\)-th frame if \(w 0\), and the \(\) value falls within the interval \([0,W-1]\) while \(\) falls within \([0,H-1]\). Here \(W\) and \(H\) represent the width and height of the image, respectively. Next, it is important to note that 3D points that are in the camera's FOV are not necessarily visible from a given camera pose, as they might be occluded by other parts of the scene. Hence, we need to check whether the points are in fact _visible_. In order to examine whether a point is occluded from a given camera viewpoint, we compare the depth value of the 3D point deriving from the geometric computation (i.e. \(w\)) and the measured depth, i.e. depth value of the projected point in the depth image. We consider points that satisfy the inequality \(w-d>k_{}\) as occluded, where \(k_{}\) is a hyper-parameter of our method. If the projection of a point from mask \(i\) is not occluded in the \(j\)-th view, it contributes to the visible point count, \((i,j)\). Once we compute mask-visibility scores for each frame, we select the top \(k_{}\) views with the highest scores \(s_{ij}\) for each mask \(i\). Here, \(k_{}\) represents another hyperparameter.

#### 3.2.2 2D mask computation and multi-scale crops

In this section, we explain our approach for computing CLIP features based on the frames selected in the previous step. Given a selected frame for a mask, our goal is to find the optimal image crops from which to extract features, as shown in (Fig. 3, ). Simply considering all of the projected points of the mask often results in imprecise and noisy bounding boxes, largely affected by the outliers (please see Appendix A.2.2). To address this, we use a class-agnostic 2D segmentation model, SAM , which predicts a 2D mask conditioned on a set of input points, along with a mask confidence score.

SAM is sensitive to the set of input points (see Appendix A.2.3, A.2.4). Therefore, to obtain a high quality mask from SAM with a high confidence score, we take inspiration from the RANSAC algorithm  and proceed as described in Algorithm 1. We sample \(k_{}\) points from the projected points, and run SAM using these points. The output of SAM at a given iteration \(r\) is a 2D mask (\(^{2D}}\)) and a mask confidence score (\(_{r}\)). This process is repeated for \(k_{rounds}\) iterations, and the 2D mask with the highest confidence score is selected.

Next, we use the resulting mask \(^{2D}}\) to generate \(L=3\) multi-level crops of the selected image. This allows us to enrich the features by incorporating more contextual information from the surroundings.

Figure 3: **Mask-Feature Computation Module. For each instance mask, ) we first compute the visibility of the instance in each frame, and select top-\(k\) views with maximal visibility. In ), we compute a 2D object mask in each selected frame, which is used to obtain multi-scale image-crops in order to extract effective CLIP features. ) The image-crops are then passed through the CLIP visual encoder to obtain feature vectors that are average-pooled over each crop and ) each selected view, resulting in the final mask-feature representation.**

```
\(_{*} 0\), \(_{*}^{}\), \(r 0\) while\(r<k_{rounds}\)do  Sample \(k_{sample}\) points among the projected points at random  Compute the mask \(_{}^{}\) and the score \(_{r}\) based on the sampled points using SAM if\(_{r}>\), then \(_{*}_{r}\), \(_{*}^{}_{}^{}\) endif \(r r+1\) endwhile
```

**Algorithm 1** - 2D mask selection algorithm

Specifically, the first bounding box \(^{}=(x_{1}^{1},y_{1}^{1},x_{2}^{1},y_{2}^{1})\) with \(0 x_{1}^{1}<x_{2}^{1}<W\) and \(0 y_{1}^{1}<y_{2}^{1}<H\) is a tight bounding box derived from the 2D mask. The other bounding boxes \(^{}\) and \(^{}\) are incrementally larger. In particular, the 2D coordinates of \(^{}\) for \(i=2,3\) are obtained as follows.

\[x_{1}^{l} =(0,x_{1}^{1}-(x_{2}^{1}-x_{1}^{1}) k_{exp} l)\] \[y_{1}^{l} =(0,y_{1}^{1}-(y_{2}^{1}-y_{1}^{1}) k_{exp} l)\] \[x_{2}^{l} =(x_{2}^{1}+(x_{2}^{1}-x_{1}^{1}) k_{exp} l,W-1)\] \[y_{2}^{l} =(y_{2}^{1}+(y_{2}^{1}-y_{1}^{1}) k_{exp} l,H-1)\]

Note that here \(l\) represents the level of the features and \(k_{exp}=0.1\) is a predefined constant.

#### 3.2.3 CLIP feature extraction and mask-feature aggregation

For each instance mask, we collect \(k L\) images by selecting top-\(k\) views and obtaining \(L\) multi-level crops as described in Sec. 3.2.1 and Sec. 3.2.2. Collected image crops are then passed through the CLIP visual encoder in order to extract image features in the CLIP embedding space, as shown in (Fig. 3, ). We then aggregate the features obtained from each crop that correspond to a given instance mask in order to get an average per-mask CLIP feature (Fig. 3, ). The computed features are task-agnostic, and can be used for various instance-based tasks by encoding a given text or image-based query, using the same CLIP model we employed to encode the image crops.

## 4 Experiments

In this section, we present quantitative and qualitative results from our method OpenMask3D. In Sec 4.1, we quantitatively evaluate our method, and compare OpenMask3D with supervised 3D instance segmentation approaches as well as existing open-vocabulary 3D scene understanding models we adapted for the 3D instance segmentation task. Furthermore, we provide an ablation study for OpenMask3D. In Sec. 4.2, we share qualitative results for open-vocabulary 3D instance segmentation, demonstrating potential applications. Additional results are provided in the Appendix.

### Quantitative results: closed-vocabulary 3D instance segmentation evaluation

We evaluate our approach on the closed-vocabulary 3D instance segmentation task. We conduct additional experiments to assess the generalization capability of OpenMask3D.

#### 4.1.1 Experimental setting

**Data.** We conduct our experiments using the ScanNet200  and Replica  datasets. We report our ScanNet200 results on the validation set consisting of 312 scenes, and evaluate for the 3D instance segmentation task using the closed vocabulary of 200 categories from the ScanNet200 annotations. Rozenberszki et al.  also provide a grouping of ScanNet200 categories based on the frequency of the number of labeled surface points in the training set, resulting in 3 subsets: _head_ (66 categories), _common_ (68 categories), _tail_ (66 categories). This grouping enables us to evaluate the performance of our method on the long-tail distribution, making ScanNet200 a natural choice as an evaluation dataset. To assess the generalization capability of our method, we further experiment with the Replica  dataset, and evaluate on the _office0, office1, office2, office3, office4, room0, room1, room2_ scenes.

**Metrics.** We employ a commonly used 3D instance segmentation metric, average precision (AP). AP scores are evaluated at mask overlap thresholds of \(50\%\) and \(25\%\), and averaged over the overlap range of \([0.5:0.95:0.05]\) following the evaluation scheme from ScanNet . Computation of the metrics requires each mask to be assigned a prediction confidence score. We assign a prediction confidence score of \(1.0\) for each predicted mask in our experiments.

**OpenMask3D implementation details.** We use posed RGB-depth pairs for both the ScanNet200 and Replica datasets, and we process 1 frame in every 10 frames in the RGB-D sequences. In order to compute image features on the mask-crops, we use CLIP  visual encoder from the ViT-L/14 model pre-trained at a 336 pixel resolution, which has a feature dimensionality of 768. For the visibility score computation, we use \(k_{threshold}=0.2\), and for top-view selection we use \(k_{view}=5\). In all experiments with multi-scale crops, we use \(L=3\) levels. In the 2D mask selection algorithm based on SAM , we repeat the process for \(k_{rounds}=10\) rounds, and sample \(k_{sample}=5\) points at each iteration. For the class-agnostic mask proposals, we use the Mask3D  model trained on ScanNet200 instances, and exclusively use the predicted binary instance masks in our pipeline. We do not filter any instance mask proposals, and run DBSCAN  to obtain spatially contiguous clusters, breaking down masks into smaller new masks when necessary. This process results in a varying number of mask proposals for each scene. For further implementation details about OpenMask3D, please refer to Appendix A. Computation of the mask-features of a ScanNet scene on a single GPU takes 5-10 minutes depending on the number of mask proposals and number of frames in the RGB-D sequence. Note that once the per-mask features of the scene are computed, objects can be queried in real-time (\(\) 1-2 ms) with arbitrary open-vocabulary queries.

**Methods in comparison.** We compare with Mask3D , which is the current state-of-the-art on the ScanNet200 3D instance segmentation benchmark. We also compare against recent open-vocabulary 3D scene understanding model variants (2D Fusion, 3D Distill, 2D/3D Ensemble) from OpenScene . Note that since OpenScene only generates a per-point feature vector (without any per-instance aggregation), it is not possible to directly compare it with our method. To address this, we extended OpenScene by averaging its per-point features within each instance mask generated by our mask module (see Appendix C for more details).

**Class assignment.** Our open-vocabulary approach does _not_ directly predict semantic category labels per each instance mask, but it instead computes a task-agnostic feature vector for each instance, which can be used for performing a semantic label assignment. In order to evaluate our model on the closed-vocabulary 3D instance segmentation task, we need to assign each object instance to a semantic category. Similar to OpenScene , we compute cosine similarity between mask-features and the text embedding of a given query in order to perform class assignments. Following Peng et al. , we use prompts in the form of "a {} in a scene", and compute text-embeddings using CLIP model ViT-L/14(336px)  for each semantic class in the ScanNet200 dataset. This way, we compute a similarity score between each instance and each object category, and assign instances to the category with the closest text embedding.

#### 4.1.2 Results and analysis

**3D closed-vocabulary instance segmentation results.** We quantitatively evaluate our approach on the closed-vocabulary instance segmentation task on the ScanNet200  and Replica  datasets. Closed-vocabulary instance segmentation results are provided in Tab. 1 and Tab. 2.

   Model & Image Features & AP & AP\({}_{50}\) & AP\({}_{25}\) & head (AP) & common (AP) & tail (AP) \\   & & & & & & & \\ Mask3D  & - & 26.9 & 36.2 & 41.4 & 39.8 & 21.7 & 17.9 \\  & & & & & & & \\ OpenScene  (2D Fusion) + masks & & & & & & & & \\ OpenScene  (3D Distill) + masks & & & & & & & & \\ OpenScene  (2D/3D Ens.) + masks & & & & & & & \\ OpenScene  (2D Fusion) + masks & & & & & & & & \\ OpenMask3D (Ours) & & & & & & & & \\   

Table 1: **3D instance segmentation results on the ScanNet200 validation set.** Metrics are respectively: AP averaged over an overlap range, and AP evaluated at 50% and 25% overlaps. We also report AP scores for head, common, tail subsets of ScanNet200. Mask3D  is fully-supervised, while OpenScene  is built upon on 2D models (LSeg , OpenSeg ) trained on labeled datasets for 2D semantic segmentation. Since OpenScene  does not provide instance masks, we aggregate its per-point features using class-agnostic masks. OpenMask3D outperforms other open-vocabulary counterparts, particularly on the long-tail classes.

State-of-the-art fully supervised approach Mask3D  demonstrates superior performance compared to open-vocabulary counterparts. While this gap is more prominent for the _head_ and _common_ categories, the difference is less apparent for the _tail_ categories. This outcome is expected, as Mask3D benefits from full-supervision using the closed-set of class labels from the ScanNet200 dataset. Furthermore, as there are more training samples from the categories within the _head_ and _common_ subsets (please refer to  for the statistics), the fully-supervised approach is more frequently exposed to these categories - resulting in a stronger performance. When we compare OpenMask3D with other open-vocabulary approaches, we observe that our method performs better on 6 out of 6 metrics. Notably, OpenMask3D outperforms other open-vocabulary approaches especially on the _tail_ categories by a significant margin. Our instance-centric method OpenMask3D, which is specifically designed for the open-vocabulary 3D instance segmentation task, shows stronger performance compared to other open-vocabulary approaches which adopt a point-centric feature representation. Differences between the feature representations are also illustrated in Fig. 5.

**How well does our method generalize?** As the mask module is trained on a closed-set segmentation dataset, ScanNet200 , we aim to investigate how well our method generalizes beyond the categories seen during training. Furthermore, we aim to analyze how well our method generalizes to out-of-distribution (OOD) data, such as scenes from another dataset, Replica . To demonstrate the generalization capability of our approach, we conducted a series of experiments. First, we analyze the performance of our model when we use class-agnostic masks from a mask-predictor trained on the 20 original ScanNet classes , and evaluate on the ScanNet200 dataset. To evaluate how well our model performs on "unseen" categories, we group the ScanNet200 labels into two subsets: _base_ and _novel_ classes. We identify ScanNet200 categories that are semantically similar to the original ScanNet20 classes (e.g. chair and folded-chair, table and dining-table), resulting in 53 classes. We group all remaining object classes that are not similar to any class in ScanNet20 as "novel". In Tab. 3, we report results on seen ("base") classes, unseen ("novel") classes, and all classes. Our experiments show that the OpenMask3D variant using a mask proposal backbone trained on a smaller set of object annotations from ScanNet20 can generalize to predict object masks from a significantly larger set of objects (ScanNet200), resulting in only a marginal decrease in the performance. In a second experiment, we evaluate the performance of OpenMask3D on out-of-distribution data from Replica, using a mask predictor trained on ScanNet200. The results from this experiment, as presented in Tab. 2, indicate that OpenMask3D can indeed generalize to unseen categories as well as OOD data. OpenMask3D using a mask predictor module trained on a smaller set of objects seems to perform reasonably well in generalizing to various settings.

**Ablation study.** In Tab. 4, we analyze design choices for OpenMask3D, _i.e._, _multi-scale cropping_ and _2D mask segmentation_. 2D mask segmentation refers to whether we use SAM  for refining

   Model & AP & AP\({}_{50}\) & AP\({}_{25}\) \\  OpenScene  (2D Fusion) + masks & \(10.9\) & \(15.6\) & \(17.3\) \\ OpenScene  (3D Distill) + masks & \(8.2\) & \(10.5\) & \(12.6\) \\ OpenScene  (2D/3D Ens.) + masks & \(8.2\) & \(10.4\) & \(13.3\) \\ OpenMask3D (Ours) & \(\) & \(\) & \(\) \\   

Table 2: **3D instance segmentation results on the Replica  dataset**. To assess how well our model generalizes to other datasets, we use instance masks from the mask proposal module trained on ScanNet200, and test it on Replica scenes. OpenMask3D outperforms other open-vocabulary counterparts on the Replica dataset.

    &  &  &  \\  Method & Mask Training & AP & AP\({}_{50}\) & AP\({}_{25}\) & AP & AP\({}_{50}\) & AP\({}_{25}\) & AP & tail (AP) \\  OpenScene  (2D Fusion) + masks & ScanNet20 & \(7.6\) & \(10.3\) & \(12.3\) & \(11.1\) & \(15.0\) & \(17.7\) & \(8.5\) & \(6.1\) \\ OpenScene  (3D Distill) + masks & ScanNet20 & \(1.8\) & \(2.3\) & \(2.7\) & \(10.1\) & \(13.4\) & \(15.4\) & \(4.1\) & \(0.4\) \\ OpenScene  (2D/3D Ens.) + masks & ScanNet20 & \(2.4\) & \(2.8\) & \(3.3\) & \(10.4\) & \(13.7\) & \(16.3\) & \(4.6\) & \(0.9\) \\ OpenMask3D (Ours) & ScanNet20 & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  OpenMask3D (Ours) & ScanNet200 & \(15.0\) & \(19.7\) & \(23.1\) & \(16.2\) & \(20.6\) & \(23.1\) & \(15.4\) & \(14.9\) \\   

Table 3: **3D instance segmentation results using masks from mask module trained on ScanNet200 annotations, evaluated on the ScanNet200 dataset . We identify 53 classes (such as chair, folded chair, table, dining table...) that are semantically close to the original ScanNet20 classes , and group them as “Base”. Remaining 147 classes are grouped as “Novel”. We also report results on the full set of labels, titled “All”.**the 2D mask from which we obtain a 2D bounding box. When SAM is not used, we simply crop the tightest bounding box around the projected 3D points. The ablation study shows that both multi-scale cropping and segmenting 2D masks to obtain more accurate image crops for a given object instance positively affect the performance. Additional experiments analyzing the importance of the number of views (k) used in _top-k view selection_ are provided in Appendix B.1.

**How well would our approach perform if we had _perfect_ masks?** Another analysis we conduct is related to the class-agnostic masks. As the quality of the masks plays a key role in our process, we aim to quantify how much the performance could improve if we had "perfect" oracle masks. For this purpose, we run OpenMask3D using ground truth instance masks from the ScanNet200 dataset instead of using our predicted class-agnostic instance masks. In a similar fashion for the baselines, we use the oracle masks to aggregate OpenScene per-point features to obtain per-mask features. We first compare against the fully-supervised Mask3D performance (Tab. 5, row 1). In a second experiment, we also supply oracle masks to Mask3D (Tab. 5, row 2) to ensure a fair comparison. For this experiment, we perform Hungarian matching between the predicted masks and oracle masks discarding all class-losses, and we only match based on the masks. To each oracle mask, we assign the predicted class label of the closest-matching mask from Mask3D. In Tab. 5, it is evident that the quality of the masks plays an important role for our task. Remarkably, our feature computation approach that is _not_ trained on any additional labeled data, when provided with oracle masks, even surpasses the performance of the fully supervised method Mask3D (by \(+15.0\)% AP) and Mask3D with oracle masks (by \(+10.7\)% AP) on the _long-tail_ categories. Overall, this analysis indicates that our approach has indeed promising results which can be further improved by higher quality class-agnostic masks, without requiring any additional training or finetuning with labeled data.

### Qualitative results

In Fig. 4, we share qualitative results from our approach for the open-vocabulary 3D instance segmentation task. With its zero-shot learning capabilities, OpenMask3D is able to segment a given query object that might not be present in common segmentation datasets. Furthermore, object properties such as colors, textures, situational context and affordances are successfully recognized by OpenMask3D. Additional qualitative results in this direction are provided in Appendix D.

In Fig. 5, we provide qualitative comparisons between our instance-based open-vocabulary representation and the point-based representation provided by OpenScene . Our method OpenMask3D computes the similarity between the query embedding and per-mask feature vectors for each object _instance_, which results in crisp instance boundaries. This is particularly suitable for the use cases in which one needs to identify object instances. Additional analysis and further details about the visualizations are provided in Appendix C.

  
2D Mask & Multi-Scale & AP & AP\({}_{50}\) & AP\({}_{25}\) & head(AP) & common(AP) & tail(AP) \\  ✗ & ✗ & \(12.9\) & \(16.6\) & \(19.5\) & \(15.1\) & \(12.2\) & \(11.3\) \\ ✗ & ✓ & \(14.3\) & \(18.4\) & \(21.1\) & \(16.1\) & \(13.6\) & \(12.9\) \\ ✓ & ✗ & \(14.1\) & \(18.3\) & \(21.5\) & \(16.0\) & \(13.0\) & \(13.4\) \\ ✓ & ✓ & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 4: **OpenMask3D Ablation Study. _2D mask_ and _multi-scale crop_ components. _2D mask_ refers to whether SAM  was employed for computing 2D masks. Results are reported on the ScanNet200  validation set.

   Model & Oracle & Img. Feat. & AP & head(AP) & common(AP) & tail(AP) \\  _Closed-vocabulary_, _full sup._ & & & & & & \\ Mask3D  & ✗ & – & \(26.9\) & \(39.8\) & \(21.7\) & \(17.9\) \\ Mask3D  & ✓ & – & \(35.5\) & \(55.2\) & \(27.2\) & \(22.2\) \\ _Open-vocabulary_ & & & & & & \\ OpenScene  (2D Fusion) + masks & ✓ & OpenSeg  & \(22.9\) & \(26.2\) & \(22.0\) & \(20.2\) \\ OpenScene  (2D Fusion) + masks & ✓ & LSeg  & \(11.8\) & \(26.9\) & \(5.2\) & \(1.7\) \\ OpenMask3D (Ours) & ✓ & CLIP  & \(\) & \(\) & \(\) & \(\) \\   

Table 5: **3D instance segmentation results on the ScanNet200 validation set, using oracle masks.** We use ground truth instance masks for computing the per-mask features. We also report results from the fully-supervised close-vocabulary Mask3D (row 2) whose predicted masks we match to the oracle masks using Hungarian matching, and assign the predicted labels to oracle masks. On the long-tail categories, OpenMask3D outperforms even the fully supervised Mask3D with oracle masks.

### Limitations.

The experiments conducted with oracle masks indicate that there is room for improvement in terms of the quality of 3D mask proposals which will be addressed in future work. Further, since the per-mask features originate from images, they can only encode scene context visible in the camera frustum, lacking a global understanding of the complete scene and spatial relationships between all scene elements. Finally, evaluation methodologies for systematically assessing open-vocabulary capabilities still remain a challenge. Closed-vocabulary evaluations, while valuable for initial assessments, fall short in revealing the true extent of open-vocabulary potentials of proposed models.

## 5 Conclusion

We propose OpenMask3D, the first open-vocabulary 3D instance segmentation model that can identify objects instances in a 3D scene, given arbitrary text queries. This is beyond the capabilities of existing 3D semantic instance segmentation approaches, which are typically trained to predict categories from a closed vocabulary. With OpenMask3D, we push the boundaries of 3D instance segmentation. Our method is capable of segmenting object instances in a given 3D scene, guided by open-vocabulary queries describing object properties such as semantics, geometry, affordances, material properties and situational context. Thanks to its zero-shot learning capabilities, OpenMask3D is able to segment multiple instances of a given query object that might not be present in common segmentation datasets on which closed-vocabulary instance segmentation approaches are trained. This opens up new possibilities for understanding and interacting with 3D scenes in a more comprehensive and flexible manner. We encourage the research community to explore open-vocabulary approaches, where knowledge from different modalities can be seamlessly integrated into a unified and coherent space.

Figure 4: **Qualitative results from OpenMask3D. Our open-vocabulary instance segmentation approach is capable of handling different types of queries. Novel object classes as well as objects described by colors, textures, situational context and affordances are successfully retrieved by OpenMask3D.**

Figure 5: **Heatmaps showing the similarity between given text queries and open-vocabulary scene features. Input 3D scene and query (_left_), per-point similarity from OpenScene (_middle_) and per-mask similarity from OpenMask3D (_right_). Dark red means high similarity, and dark blue means low similarity with the query text.**

**Acknowledgments and disclosure of funding.** Francis Engelmann is a postdoctoral research fellow at the ETH AI Center. This project is partially funded by the ETH Career Seed Award "Towards Open-World 3D Scene Understanding", and Innousisse grant (48727.1 IP-ICT). We sincerely thank Jonas Schult for helpful discussions, and Lorenzo Liso for the help with setting up our live demo.