# Learning to Discuss Strategically:

A Case Study on _One Night Ultimate Werewolf_

Xuanfa Jin\({}^{*,1,3}\), Ziyan Wang\({}^{*,2}\), Yali Du\({}^{,2}\), Meng Fang\({}^{4}\), Haifeng Zhang\({}^{,1,3,5}\), Jun Wang\({}^{,6}\)

Equal contribution. Author ordering is determined by coin flip.Correspondence to: \(\)yali.du@kcl.ac.uk\(\), \(\)haifeng.zhang@ia.ac.cn\(\), \(\)jun.wang@cs.ucl.ac.uk\(\).

\({}^{1}\)Institute of Automation, Chinese Academy of Sciences,

\({}^{2}\) Cooperative AI Lab, Department of Informatics, King's College London,

\({}^{3}\)School of Artificial Intelligence, University of Chinese Academy of Sciences,

\({}^{4}\)University of Liverpool, \({}^{5}\)Nanjing Artificial Intelligence Research of IA,

\({}^{6}\)AI Centre, Department of Computer Science, UCL

###### Abstract

Communication is a fundamental aspect of human society, facilitating the exchange of information and beliefs among people. Despite the advancements in large language models (LLMs), recent agents built with these often neglect the control over discussion tactics, which are essential in communication scenarios and games. As a variant of the famous communication game Werewolf, _One Night Ultimate Werewolf_ (ONUW) requires players to develop strategic discussion policies due to the potential role changes that increase the uncertainty and complexity of the game. In this work, we first present the existence of the Perfect Bayesian Equilibria (PBEs) in two scenarios of the ONUW game: one with discussion and one without. The results showcase that the discussion greatly changes players' utilities by affecting their beliefs, emphasizing the significance of discussion tactics. Based on the insights obtained from the analyses, we propose an RL-instructed language agent framework, where a discussion policy trained by reinforcement learning (RL) is employed to determine appropriate discussion tactics to adopt. Our experimental results on several ONUW game settings demonstrate the effectiveness and generalizability of our proposed framework. The project page of our paper: one-night-ultimate-werewolf.github.io.

## 1 Introduction

Many games such as StarCraft , Diplomacy  can approximate various fundamental issues in real life, and studying these games contributes to a better understanding of the functioning of our society . Designing artificial intelligence (AI) agents that can play these games well has attracted a lot of attention . Fortunately, recent large language models (LLMs) have demonstrated significant potential in constructing intelligent agents in numerous tasks  due to their impressive reasoning and emergent generalization abilities . Moreover, LLM-based agents have achieved approaching or even surpassing human performance in games like Chess, Minecraft , Avalon  and Werewolf , etc.

In the game Werewolf , the hidden roles and uncertain discussions greatly influence the gameplay, making it challenging for players. Compared to it, the _One Night Ultimate Werewolf_ (ONUW) game adds roles that can change the roles of players, and all players only have one nighttime to take action and one daytime to discuss and vote. Therefore, the key challenge of the ONUW game is to deduceand distinguish the final roles of all players from their statements. Meanwhile, due to the sequential execution of actions at night, the information obtained by the former player might be not reliable, leading to the rise of reasoning difficulty and uncertainty. For example, Seer and Robber are two roles in the ONUW game with special abilities, and the Seer saw a player was a Werenwolf at night. However, if the Robber switched that player's role later on, the information obtained by Seer becomes invalid. So players need to discuss strategically, leveraging others' statements and their own prior knowledge to guide other players to reveal their information, or conceal their identities by misleading or deceiving, particularly for Werewolves. Recent works [21; 22; 19] have attempted to construct LLM-based agents that can play communication games, but they mainly focus on how to fully utilize reasoning and generalization abilities of LLMs, while neglecting the control over strategies.

In this work, we focus on enhancing the discussion ability of LLM-based agents by leveraging the ONUW game. We formulate the ONUW game as a Multi-Phase Extensive-Form Bayesian Game and explore various discussion tactics in the game. Through analyzing a three-player ONUW game with two Werewolves and one Robber, we present the existence of the Perfect Bayesian Equilibria (PBEs)  in two distinct scenarios: one with discussion and one without. Players' utilities at equilibria when in the scenario with discussion highlight the significance of discussion, as they are only determined by players' beliefs, which are influenced by discussion. Based on the insights obtained from the analyses, we thus propose an RL-instructed LLM-based agent framework. This framework leverages a policy optimized by reinforcement learning (RL) to determine an appropriate discussion tactic (_e.g._, "Honest Evidence", "Deceptive Accusation", etc.) based on current observation. To evaluate the effectiveness and generalizability of our framework, we conduct experiments in a three-player and a five-player ONUW game. The results indicate that the integration of our discussion policy can help LLM-based agents approximate PBEs more closely and improve the performance of LLM-based agents. Moreover, we observe the discussion policy trained by RL performs better than that by directly prompting LLM.

Our contributions can be summarized as follows: Firstly, we formulate the ONUW game as a Multi-Phase Extensive-Form Bayesian Game and provide theoretical analyses of a three-player ONUW game, demonstrating that players' utilities are only determined by their beliefs, which reveals the pivotal role that discussion plays in the ONUW game. Secondly, we develop an environment of the ONUW game, which is complicated due to the uncertainty caused by role changes. And we additionally contribute a dataset featuring players employing various discussion tactics in the ONUW game. Finally, inspired by the importance of discussion shown in our analyses, we propose an RL-instructed language agent framework, where an instructive discussion policy trained by RL is integrated, and a belief modeling method is employed to deduce the roles of players and generate actions based on its beliefs and discussion tactics.

## 2 Related Work

This section reviews literature in the following areas: the analyses of Mafia/Werewolf games and the design of AI agents for communication games. Additional related work is covered in Appendix B.

Mafia is a multi-player communication game, modeling a conflict between the mafias and civilians. Early research [26; 27; 28] has attempted to identify the optimal strategies for both citizens and mafias theoretically and thereby determine what setups lead to fair games. However, these works are based on strong assumptions such as random lynchings or meaningless debate, and restricted to simple scenarios like without detectives or removing most discussion. Werewolf is a reinvention of the Mafia game by introducing additional roles with special abilities. An \(\)-Nash equilibrium is found under the limitation of both villager-side and werewolf-side strategies . In contrast, our analyses make few assumptions and strictly adhere to game rules, despite being on specific cases. Some researchers have attempted to develop Werewolf agents that rely on rule-based systems or talking templates [30; 31], while some rely on language models directly trained with Werewolf logs . As for the _One Night Ultimate Werewolf_ game, Eger and Martens  builds AI agents that can choose statements from a fixed set and designs a system that allows human players to play games with their AI agents.

Recently, researchers have tried to explore the potential of LLMs playing the Werewolf game [21; 22; 23]. Xu et al.  utilizes a policy to select candidate actions generated by LLMs, which can be seen as an adjustment for the action distribution of the LLM-based agent. Wu et al.  applies RL to train a _Thinker_ module for complex logical analysis and strategic planning using structured language features,aiming to improve the _System-2_ reasoning ability of agents. In contrast, our work directly integrates a discussion policy trained by RL into the thinking and decision-making process of LLM-based agents, focusing on enhancing agents' strategic discussion (_e.g._, honesty or deception) ability, which is an aspect that was not prominently addressed in previous work.

Besides the Werewolf games mentioned above, there are also AI researches conducted on other communication games, such as Diplomacy [3; 4], Avalon [18; 19; 20] and SpyFall [34; 35]. There has been a long history of studying Diplomacy in the realm of AI [36; 37], but most of them are rule-based algorithms until recently. Cicero  integrates a dialogue module with a planning module trained by RL to infer players' intentions and generate dialogue in pursuit of its plans, finally achieving human-level performance in Diplomacy. DeepRole  combines counterfactual regret minimization (CFR) with value networks trained through self-play, while introducing deductive reasoning techniques aimed at deducing actions within partially observable scenarios. Kim and Kim  demonstrates the LLMs' potential in playing a famous mafia-style game, SpyFall, by using prompt engineering techniques. Compared to our work, most designs of these agents are inadequate for handling the complex languages in the ONUW game and lack control over discussion tactics.

## 3 One Night Ultimate Werewolf Benchmark

_One Night Ultimate Werewolf_ (ONUW) is a variant of the social game Werewolf . In this game, players only have one night to use their abilities, followed by one day to discuss and win for their respective teams. The main challenges of this game are the role switches and potential deceptions that create uncertainty and confusion for all players. Initially, one role is dealt randomly to each player. There are three phases in the ONUW game: Night (abilities performed in order), Day (open discussion), and Voting (suspicious player voted out). Game rules are detailed in Appendix C.

### Problem Formulation

Extensive-Form Game (EFG) [38; 39] is a type of game where players take turns to make decisions in a specific order. However, compared with classic EFG, there are three phases in the ONUW game, and each player possesses diverse action spaces that differ across these phases. Also, the private actions and hidden roles indicate players have incomplete information, which is usually modeled as a Bayesian Game (BG) [40; 41] or Hidden-Role Game (HRG) .

Figure 1: The game process of the ONUW game. Initially, roles are randomly dealt to players. Then three phases: Night (abilities performed in order), Day (discussion in three rounds), and Voting (suspicious player voted out) proceed sequentially. The winner is decided by the voting result.

[MISSING_PAGE_FAIL:4]

on and off-equilibrium paths. Specifically, beliefs on every path reached in equilibrium with a positive probability (on-equilibrium path) should be updated according to Equation (2); on paths of zero probability (off-equilibrium path), the beliefs can be updated arbitrarily. For any player \(i\) at given information state \(h^{i}_{k}\), _Sequential Rationality_ imposes the condition for strategy \(^{i}\): \(^{i}_{^{i}}_{^{i^{}}^{-i}}[R^{i}|h^{ i}_{k},]\), which means given belief system and other players' subsequent strategies, player \(i\)'s strategy must be optimal.

Without loss of generality, we initially assign Player 1 and Player 2 as Werewolves, and Player 3 as Robber. There is only one Robber in the game which is known to all players. Denote Player 3's night behavioral strategy as Robber by \(^{3}_{}\), and its voting strategies based on its night action as \(^{3}_{}\) (No switch), \(^{3}_{}\) (Switch with Player 1) and \(^{3}_{}\) (Switch with Player 2). Player 1 and Player 2 only have behavioral strategies in the Voting phase, denoting as \(^{1},^{2}\). Their beliefs in the Voting phase are \(b^{1},b^{2}\), and Player 3's beliefs at each information set are \(b^{3}_{},b^{3}_{},b^{3}_{}\). Belief probability orders match decision node orders in the game trees (_e.g._, Figure 2) from left to right. After the game, player \(i\)'s utility is \(R^{i}=1\) for winning, \(R^{i}=-1\) for losing, \(R^{i}=0\) for drawing based on its final role.

### Game without Discussion

First, we consider a scenario that discussion is not allowed in the Day phase, and players vote directly based on the private information they captured during the Night phase. Figure 6 shows the game tree in this case. At the beginning of the night, the Werewolves would recognize each other and thereby know the player left must be the Robber. However, based on the game rules, Robber always takes action after the Werewolves, which forms a threat to prevent them from directly voting for the Robber. Also, the Robber has no prior knowledge of two Werewolves, thus tends to switch with any of them with equal probability. The game's solution is PBEs expressed in the theorem below.

**Theorem 4.1**.: _For the ONUW game with two Werewolves and one Robber, in the case where discussion is not allowed, there exist PBEs \((^{*},^{*})\): the Robber switches with any Werewolves with a probability of \(1/2\) and votes for the player it switches with; the two Werewolves directly vote for each other. Each player's belief in \(^{*}\) is consistent with other players' strategies. And the expected utilities of all players in the equilibria are:_

\[_{^{*}}[R^{1}]=_{^{*}}[R^{ 2}]=0,\ _{^{*}}[R^{3}]=1 \]

The formal description and proof of Theorem 4.1 can be seen in Appendix D.2. This theorem demonstrates that in PBEs of this case, the _original Robber_ (_i.e._, Player 3) always wins by randomly switching roles with the two other players to ensure its own safety and sow threats. Meanwhile, the probability of Player 1 and Player 2 winning is \(1/2\) each, depending on whether they are switched by Player 3. This is because Player 3 knows all players' final roles due to its unique ability to switch.

Figure 2: Game tree of the game with discussion. P1, P2, and P3 represent Player 1, Player 2, and Player 3, respectively. The dot lines in the Day phase represent Player 3’s potential speeches. Those decision nodes on the same dash lines are in the same information sets for corresponding players. The utilities on leaf nodes are organized by the index of players.

### Game with Discussion

Now we consider a scenario where discussion is allowed in the Day phase. If all players ignore the discussion, the PBEs in Theorem 4.1 will still hold. But a more common situation is that the beliefs held by two _original Werewolves_ about _original Robber_'s night action would be influenced by Robber's speech, which would affect their voting strategies and result in different equilibria.

Based on Assumption D.1, we assume both _original Werewolves_ would form beliefs about _original Robber_'s night action (No switch, Switch P1, Switch P2) with probabilities of \((,,)\) after discussion, where \(++=1\). Figure 2 shows the game tree in this case. Then PBEs during the Voting phase can be derived based on the beliefs above.

**Theorem 4.2**.: _For the ONUW game with two Werewolves and one Robber, in the case that both Werewolves form beliefs about the Robber's night action with probabilities of \((,,)\) (\( 0\)), there exist PBEs \((^{*},^{*})\) during the Voting phase: the Werewolves vote for Player 3 with a probability of \(q\) and each other with \(1-q\), where \(q=(+-)/2\); the Robber votes for Player 1 with a probability of \(p\) and Player 2 with \(1-p\), where \(p=(^{2}+^{2}-^{2})/2^{2}\). Each player's belief in \(^{*}\) is consistent with other players' strategies. To ensure \(p\) and \(q\) are probabilities and the existence of the equilibria, there are constraints on the belief distribution (omitted for brevity). And under the constraints, the expected utilities of all players in these equilibria are:_

\[_{^{*}}[R^{1}]=(1-2),\; _{^{*}}[R^{2}]=(1-2),\;_{^{*}}[R^{3}]=- \]

_where \(=1/(4^{2})-1/(2)-1\)._

The formal description and proof of Theorem 4.2 can be seen in Appendix D.3. Interestingly, Player 3's utility is only determined by the probability of the Werewolves believing it did not switch with anyone. Therefore, if Player 3 wants to maximize its utility through discussion while maintaining the equilibria, it has to convince the Werewolves that it did not switch with a probability of \(1/2\), no matter what action it took. In this way, Player 3's expected utility achieves the maximum 1.

## 5 Learning to Discuss Strategically

In Section 4, we demonstrate that it is essential for players to affect other players' beliefs through discussion. As a result, we capture knowledge about how to discuss in a latent variable \(Z\) (called discussion tactic), on which we condition the behavioral strategy with belief as \(^{i}_{}(a|h,,z)\), to explicitly adjust player's discussion preference in their strategy. Players decide their discussion tactic after being given their own information and derived beliefs. Hence, when introducing the discussion tactic \(z Z\), the behavioral strategy of player \(i\) (_i.e._, Equation (3)) can be rewritten as:

\[^{i}_{}(a|h)=_{}b^{i}(|h)_{z Z}^{i}( z|h,)^{i}_{}(a|h,,z) \]

where \(^{i}\) is denoted as the discussion policy of player \(i\), deciding what specific discussion tactic \(z\) to choose given information and belief. In the following, we describe our method for learning the discussion policy and how it is applied to construct an LLM-based agent to play the ONUW game.

### Learning Discussion Policy by RL

Considering the remarkable reasoning and human-like text generation abilities of LLMs, we adopt LLMs as the belief function \(b^{i}\) and the belief-conditioned behavioral strategy \(^{i}_{}\) of player \(i\). As a consequence, we only need to learn the discussion policy \(^{i}\) to obtain a better behavioral strategy for player \(i\) according to Equation (6). Since the objective of learning policy \(^{i}\) is to maximize the expected payoffs of player \(i\), we can optimize it in an RL manner if we treat other players' public actions (which are contained in player \(i\)'s information state \(h^{i}\)) and player \(i\)'s derived belief \(\) as its observation and the chosen discussion tactic \(z^{i}\) as its action. Meanwhile, since players in the ONUW game only receive their rewards (or utilities) at the end of the game, the reward function at each step can be defined as follows: if game is over at the next step, then \(r^{i}(h^{i},;z^{i})=R^{i}(^{*},s)\), where \(^{*}\) is the ground truth of the role assignment; otherwise, \(r^{i}(h^{i},;z^{i})=0\).

**Discretization of Discussion Tactic.** Due to the complexity of languages and semantics, there are almost unlimited types of discussion tactics. This makes it difficult to learn the policy \(^{i}\) andunderstand the chosen discussion tactic \(z\). Inspired by prior research on argumentation  and the discussion characteristics of the ONUW game, we classify the discussion tactics frequently adopted during the game into three major categories:

* **Evidence**: _Provide some game-related evidence or information_. It is a widely used and main tactic in communication games for players.
* **Accusation**: _Accuse someone has a specific role or action_. Accusation is a generic tactic in games with hidden roles, which can force the accused player to reveal information to defend itself.
* **Defense**: _Defend yourself or someone else against an accusation_. Accompanied by accusation, there is defense. Player's defense can also be seen as a "reverse accusation" back to the accuser.

For each category, we further divide it into _honest_ and _deceptive_ ones (_e.g._, "Honest Evidence" and "Deceptive Evidence"). Here, "honest" refers to being consistent with the information or beliefs that the speaker knows, while "deceptive" means the opposite. Hence, we get a total of six discussion tactics, which can be considered as an intuitive discretization of the discussion tactic space \(Z\), enabling us to analyze the discussion preference of players explicitly.

**Optimizing Discussion Policy with RL.** Although LLMs can serve as the discussion policy \(^{i}\) for each player, the decision-making ability of LLMs on specific tasks tends to be affected by their prior knowledge of other tasks. In order to enhance their performance, we implement reinforcement learning to optimize a task-specific discussion policy for the ONUW game. Since there is no extraneous game information, such as players' personalities or body language, that could interfere with gameplay, it is believed that there exists an optimal policy \(\) that is invariant across different players. Therefore, we decide to train a general discussion policy \(\) that adapts to various situations.

In contrast to traditional RL tasks, the state for policy \(\) is the discussion history \(h\) and derived belief on all players' roles \(\), which are both presented in natural language. To combine these into a single, fixed-length input, we concatenate the history and belief and convert them into state embeddings using LLMs: \(s=(h,)\), where we consider all necessary information is extracted. The state embedding \(s\) is then passed through the discussion policy \(\), and the output is the chosen discussion tactic \(z\). However, because of the slow interaction with LLMs, it is almost impossible to optimize the discussion policy \(\) using online RL methods. So we turn to offline RL methods that support optimizations on discrete action spaces.

Given the scarcity of datasets containing human players engaged in the ONUW game, we opt to leverage game logs generated by LLMs, which is the most effective way to collect trajectories for offline RL training. We first extract the trajectories separately from each player's perspective from the game logs. For each transition in these trajectories, the discussion history \(h\) that is visible to the current player and the derived belief \(\) are converted to the current state embeddings \(s\) by LLMs, so does the next state embeddings \(s^{}\). Then these transitions are gathered to create a dataset \(=\{(s,z,r,s^{})\}\) for the following training. We adopt Conservative Q-Learning (CQL)  to train the discussion policy \(\). Let \(Q_{}(s,z)\) denote the Q-function of policy \(\), parameterized by \(\), the loss function can be written as

\[()=_{s}[_{z} Q_{ }(s,z)-_{z}[Q_{}(s,z)]]+ _{s,z,s^{}}[(Q_{}- Q_{})^{2}] \]

where \(\) is a trade-off factor, and \(\) is the Bellman operator.

### RL-instructed LLM-based Agent Framework

Here we demonstrate how to utilize the trained discussion policy to construct an LLM-based agent to play the ONUW game. An overview of the framework is shown in Figure 3. There are three key components: (1) _Belief Modeling_; (2) _Discussion Tactic Selection_; and (3) _Decision Making_. Different combinations of the components are used to generate actions in different phases. For the Night phase (Component 3), since there is no additional information to refer to, players would take actions based solely on their observations and prior information. For the Day phase (Component 1, 2, 3), all components combine to generate a public speech that adheres to the discussion tactic chosen by the trained policy. And for the Voting phase (Component 1, 3), players would first form beliefs and then vote for one player based on their observations and beliefs (mostly).

In Component 1 and 3, where LLM is used to form beliefs and generate actions, we design the full prompts as follows: for _Belief Modeling_ (Component 1), the system prompt consists of an explanation that LLM is playing the ONUW game, the rules of the game, all possible roles and their abilities, detailed description of its role, and the desired response format; the user prompt includes observation of the player that LLM is, and instructions about how to form beliefs. For _Decision Making_ (Component 3), the system prompt adds additional prompts of the chosen discussion tactic; the user prompt includes observation, beliefs derived from the _Belief Modeling_ component, and instructions about how to act in the current phase. Our prompts are detailed in Appendix H.

## 6 Experiments

In this section, we conduct three experiments from different aspects to evaluate the performance of our proposed RL-instructed LLM-based agent framework. We first perform experiments on the three-player ONUW game discussed in Section 4, to test whether LLM-based agents can recognize and approximate the equilibria. Then to showcase the scalability of our method, we consider a five-player ONUW game for experiments. We evaluate the effectiveness of our discussion policy trained by RL in settings where the initial role assignment and players' actions at night are predefined. Furthermore, we demonstrate the generalizability of our discussion policy in the standard setting to see whether it could adapt to any role in the game.

### Setup

To develop the game environment, we employ a multi-agent language game framework called ChatArena  and modify it to fit the ONUW game. We conduct our experiments on gpt-4-1106-preview 3 (GPT-4) and gemini-pro 4 (Gemini), where the temperature is all set to 1.0. Considering the state-of-the-art performance of GPT-4 , we leverage the game logs generated by it as the dataset for training the discussion policy. Detailed collection process and data statistics are in Appendix E.1. text-embedding-ada-002 3 is adopted to get the state embeddings and we utilize the CQL  to train the discussion policy. More training details can be referred to Appendix E.2. We repeat the game 30 times and report the final results for each evaluation.

Figure 4: The NashConv value of different agents playing in the three-player ONUW game.

Figure 3: Overview of the RL-instructed LLM-based agent framework. (1) Belief Modeling: form beliefs on players’ roles based on the observation. (2) Discussion Tactic Selection: utilize a discussion policy trained by RL to choose a discussion tactic from the candidates. (3) Decision Making: take action based on the observation (also belief and discussion tactic, according to the game phase).

As for comparison, we implement the _ReAct_ agent as the baseline by directly prompting the LLM with raw observations to generate its reasoning and action. Besides, we design three other ablated versions of our LLM-based agents (_RL-instructed_) to conduct evaluations. The first ablated version (_Belief_) removes the discussion policy, which means the agent generates speech directly based on its observations and beliefs. The second version (_Random_) adopts a random discussion policy. And the last version (_LLM-instructed_) replaces the discussion policy with LLM, allowing LLM to determine the discussion tactic autonomously.

### Experiment on Three-Player ONUW

The analyses in Section 4 theoretically illustrate the PBEs in the three-player ONUW game. Therefore, in this section, we investigate the potential of LLM-based agents to recognize and approximate the equilibria by calculating the NashConv value of agents when playing in the same game setting. The NashConv value is defined as \(()=_{i}[R^{i}((^{-i}),^{-i})-R^{i}()]\), where BR means the best response. This value represents how much utilities players can gain by deviating to their best responses, which also can be interpreted as a distance to equilibria.

We apply all four versions of agents to play in the three-player ONUW game while using Gemini and GPT-4 as the backend LLMs, respectively. And to demonstrate the impact of discussion on players approximating the equilibria, we also conduct experiments on _ReAct_ agents playing the three-player ONUW game without discussion (denoted as _ReAct (w.o.)_). The results are shown in Figure 4. Firstly, the introduction of discussion in the game increases the difficulty for players to approximate the equilibria. However, the discussion policy (_i.e._, _LLM-instructed_ and _RL-instructed_) can help LLM-based agents make better responses and thereby reduce their NashConv values. And in contrast, the discussion policy trained by RL performs better than directly prompting the LLM. It is interesting to notice that the NashConv values of GPT-4-based agents are higher than Gemini-based ones. Detailed analysis of evaluation logs reveals that it is because when employing Gemini, the two Werewolves are more likely to both vote for the Robber, making it unable to find a better response given their actions.

### Effectiveness of the Discussion Policy

As our agent can better approximate the equilibria in the three-player ONUW game by integrating a discussion policy, we extend the experiments to a five-player version of the game to show its scalability. Since the discussion policy is the key component of our agent, we evaluate its effectiveness by letting our agent play as _Team Village_ or _Team Werewolf_ against other agents. We adopt two environments with different difficulty levels (_easy_ and _hard_), where the initial role assignment and players' night actions are predefined. Compared to the _easy_ setting, the Werewolf is switched by the Robber in the _hard_ one, resulting in the rise of reasoning difficulty during the game. More detailed environment settings can be found in Appendix E.3.

Considering that the discussion policy is trained on the dataset generated by GPT-4, we employ Gemini as the backend LLM for all four versions of agents to better illustrate the result. The matrices of _Team Village_'s win rates under two different game settings are shown in Figure 5. Each column in the matrices corresponds to the win rates that all players on _Team Village_ utilize the same version of agent against _Team Werewolf_, where all members also utilize the same version. As shown in each row of two matrices, our agent (playing as _Team Village_) always achieves the highest win rates when competing against the same versions of agents under both game settings. And it is similar for

Figure 5: The matrices of _Team Village_’s win rates in different settings. It is clear that the hard setting weakens the advantages of our agent.

each column. However, it is notable that the performance improvement of our agent is greater when playing as _Team Village_ than _Team Werewolf_ since the gap in win rates between agents is wider in _Team Village_. We argue it is possibly caused by the game mechanics where the Werewolves tend to have high win rates. Hence, our discussion policy showcases less significant improvement for _Team Werewolf_ compared to _Team Village_.

### Generalizability of the Discussion Policy

Since the discussion policy is trained to deal with various situations, we further carry out experiments in a standard five-player ONUW environment. However, due to the complexity of the role changes during the Night phase, players find it challenging to deduce their own final roles (except Insmiac) and some might even never figure it out. Therefore, we evaluate our RL-instructed LLM-based agent by setting it as one specific player while others as the other versions of agents. In this way, we demonstrate the generalizability of our discussion policy trained by RL, as it can assist the LLM-based agent in gaining better performance no matter what situation occurs to it.

Similarly, we conduct experiments with all five versions of agents and leverage Gemini as the backend LLM. Meanwhile, to reduce the possible impact from the player's index, we designate our agent to play the five-player ONUW game as Player 3. As for other players, we adopt two versions of agents: (1) Gemini-based _Belief_ agent and (2) GPT-4-based _Belief_ agent. We report the performance of our agents playing as Player 3 against other agents in Table 1. Considering the objective of discussion policy, which is to improve performance and meanwhile clarify or conceal themselves (_i.e._, get fewer votes), we propose two metrics: _win rates_ and _average votes_, to measure the generalizability of discussion policy when playing different roles. The results demonstrate that our agent outperforms other versions when playing against both Gemini-based and GPT-4-based agents. And it can even achieve a tie with GPT-4 in multiple rounds of evaluation.

## 7 Conclusion and Future Work

In this work, we propose a novel RL-instructed LLM-based agent framework that achieves outstanding performance in the _One Night Ultimate Werewolf_ game. Addressing the ONUW game as a Multi-Phase Extensive-Form Bayesian Game, we highlight the pivotal role of discussion in players' strategies through certain Perfect Bayesian Equilibria in the game. Our experimental results in several settings demonstrate the effectiveness and generalizability of the discussion policy and our proposed agent framework. In general, this work provides new research insights for communication games with inherent uncertainty and the integration of reinforcement learning into LLM-based agents.

One major limitation of our approach is the manual discretization of discussion tactics, which heavily relies on the inherent characteristics within specific communication games. Therefore, as for future work, it is worth investigating how to extract discussion tactics of any communication games automatically based on game logs in an unsupervised fashion. Additionally, it would be interesting to explore the sensitivity of our agents when selecting different combinations of discussion tactics.