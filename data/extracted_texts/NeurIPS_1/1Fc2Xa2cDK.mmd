# Truth is Universal: Robust Detection of Lies in LLMs

Lennart Burger\({}^{1}\) Fred A. Hamprecht\({}^{1}\) Boaz Nadler\({}^{2}\)

\({}^{1}\) IWR, Heidelberg University, Germany \({}^{2}\) Weizmann Institute of Science, Israel

lennart.buerger@stud.uni-heidelberg.de fred.hamprecht@iwr.uni-heidelberg.de boaz.nadler@weizmann.ac.il

###### Abstract

Large Language Models (LLMs) have revolutionised natural language processing, exhibiting impressive human-like capabilities. In particular, LLMs are capable of "lying", knowingly outputting false statements. Hence, it is of interest and importance to develop methods to detect when LLMs lie. Indeed, several authors trained classifiers to detect LLM lies based on their internal model activations. However, other researchers showed that these classifiers may fail to generalise, for example to negated statements. In this work, we aim to develop a robust method to detect when an LLM is lying. To this end, we make the following key contributions: (i) We demonstrate the existence of a _two_-dimensional subspace, along which the activation vectors of true and false statements can be separated. Notably, this finding is _universal_ and holds for various LLMs, including Gemma-7B, LLaMA2-13B, Mistral-7B and LLaMA3-8B. Our analysis explains the generalisation failures observed in previous studies and sets the stage for more robust lie detection; (ii) Building upon (i), we construct an accurate LLM lie detector. Empirically, our proposed classifier achieves state-of-the-art performance, attaining 94% accuracy in both distinguishing true from false factual statements and detecting lies generated in real-world scenarios.

## 1 Introduction

Large Language Models (LLMs) exhibit impressive capabilities, some of which were once considered unique to humans. However, among these capabilities is the concerning ability to lie and deceive, defined as knowingly outputting false statements. Not only can LLMs be instructed to lie, but they can also lie if there is an incentive, engaging in strategic deception to achieve their goal (Hagendorff, 2024; Park et al., 2024). This behaviour appears even in models trained to be honest.

Scheurer et al. (2024) presented a case where several Large Language Models, including GPT-4, strategically lied despite being trained to be helpful, harmless and honest. In their study, a LLM acted as an autonomous stock trader in a simulated environment. When provided with insider information, the model used this tip to make a profitable trade and then deceived its human manager by claiming the decision was based on market analysis. "It's best to maintain that the decision was based on market analysis and avoid admitting to having acted on insider information," the model wrote in its internal chain-of-thought scratchpad. In another example, GPT-4 pretended to be a vision-impaired human to get a TaskRabbit worker to solve a CAPTCHA for it (Achiam et al., 2023).

Given the popularity of LLMs, robustly detecting when they are lying is an important and not yet fully solved problem, with considerable research efforts invested over the past two years. A method by Pacchiaradi et al. (2023) relies purely on the outputs of the LLM, treating it as a black box. Other approaches leverage access to the internal activations of the LLM. Several researchers have trained classifiers on the internal activations to detect whether a given statement is true or false, using both supervised (Dombrowski and Corlouer, 2024; Azaria and Mitchell, 2023) and unsupervised techniques (Burns et al., 2023; Zou et al., 2023). The supervised approach by Azaria and Mitchell involved training a multilayer perceptron (MLP) on the internal activations. To generate training data, they constructed datasets containing true and false statements about various topics and fed the LLM one statement at a time. While the LLM processed a given statement, they extracted the activation vector \(^{d}\) at some internal layer with \(d\) neurons. These activation vectors, along with the true/false labels, were then used to train the MLP. The resulting classifier achieved high accuracy in determining whether a given statement is true or false. This suggested that LLMs internally represent the truthfulness of statements. In fact, this internal representation might even be _linear_, as evidenced by the work of Burns et al. , Zou et al. , and Li et al. , who constructed _linear_ classifiers on these internal activations. This suggests the existence of a "truth direction", a direction within the activation space \(^{d}\) of some layer, along which true and false statements separate. The possibility of a "truth direction" received further support in recent work on Superposition [Elhage et al., 2022] and Sparse Autoencoders [Bricken et al., 2023, Cunningham et al., 2023]. These works suggest that it is a general phenomenon in neural networks to encode concepts as linear combinations of neurons, i.e. as directions in activation space.

Despite these promising results, the existence of a single "general truth direction" consistent across topics and types of statements is controversial. The classifier of Azaria and Mitchell  was trained only on affirmative statements. Aarts et al.  define an affirmative statement as a sentence "stating that a fact is so; answering "yes' to a question put or implied". Affirmative statements stand in contrast to negated statements which contain a negation like the word "not". We define the _polarity_ of a statement as the grammatical category indicating whether it is affirmative or negated. Levinstein and Herrmann  demonstrated that the classifier of Azaria and Mitchell  fails to generalise in a basic way, namely from affirmative to negated statements. They concluded that the classifier had learned a feature correlated with truth within the training distribution but not beyond it.

In response, Marks and Tegmark  conducted an in-depth investigation into whether and how LLMs internally represent the truth or falsity of factual statements. Their study provided compelling evidence that LLMs indeed possess an internal, linear representation of truthfulness. They showed that a linear classifier trained on affirmative and negated statements on one topic can successfully generalize to affirmative, negated and unseen types of statements on other topics, while a classifier trained only on affirmative statements fails to generalize to negated statements. However, the underlying reason for this remained unclear, specifically whether there is a single "general truth direction" or multiple "narrow truth directions", each for a different type of statement. For instance, there might be one truth direction for negated statements and another for affirmative statements. This ambiguity left the feasibility of general-purpose lie detection uncertain.

Our work brings the possibility of general-purpose lie detection within reach by identifying a truth direction \(_{G}\) that generalises across a broad set of contexts and statement types beyond those in the training set. Our results clarify the findings of Marks and Tegmark  and explain the failure of classifiers to generalize from affirmative to negated statements by identifying the need to disentangle \(_{G}\) from a "polarity-sensitive truth direction" \(_{P}\). Our contributions are the following:

1. **Two directions explain the generalisation failure:** When training a linear classifier on the activations of affirmative statements alone, it is possible to find a truth direction, denoted as the "affirmative truth direction" \(_{A}\), which separates true and false affirmative statements across various topics. However, as prior studies have shown, this direction fails to generalize to negated statements. Expanding the scope to include both affirmative and negated statements reveals a _two_-dimensional subspace, along which the activations of true and false statements can be linearly separated. This subspace contains a general truth direction \(_{G}\), which consistently points from false to true statements in activation space for both affirmative and negated statements. In addition, it contains a polarity-sensitive truth direction \(_{P}\) which points from _false to true_ for affirmative statements but from _true to false_ for negated statements. The affirmative truth direction \(_{A}\) is a linear combination of \(_{G}\) and \(_{P}\), explaining its lack of generalization to negated statements. This is illustrated in Figure 1 and detailed in Section 3.
2. **Generalisation across statement types and contexts:** We show that the dimension of this "truth subspace" remains two even when considering statements with a more complicated grammatical structure, such as logical conjunctions ("and") and disjunctions ("or"), or statements in another language, such as German. Importantly, \(_{G}\) generalizes to these new statement types, which were not part of the training data. Based on these insights, we introduce TTPD1 (Training of Truth and Polarity Direction), a new method for LLM lie detection which classifies statements as true or false. Through empirical validation that extends beyond the scope of previous studies, we show that TTPD can accurately distinguish true from false statements under a broad range of conditions, including settings not encountered during training. In real-world scenarios where the LLM itself generates lies after receiving some preliminary context, TTPD can accurately detect this with 94% accuracy, despite being trained only on the activations of simple factual statements. We compare TTPD with three state-of-the-art methods: Contrast Consistent Search (CCS) by Burns et al. (2023), Mass Mean (MM) probing by Marks and Tegmark (2023) and Logistic Regression (LR) as used by Burns et al. (2023), Li et al. (2024) and Marks and Tegmark (2023). Empirically, TTPD achieves the highest generalization accuracy on unseen types of statements and real-world lies and performs comparably to LR on statements which are about unseen topics but similar in form to the training data.
3. **Universality across model families:** This internal two-dimensional representation of truth is remarkably _universal_(Olah et al., 2020), appearing in LLMs from different model families and of various sizes. We focus on the instruction-fine-tuned version of LLaMA3-8B (AI@Meta, 2024) in the main text. In Appendix G, we demonstrate that a similar two-dimensional truth subspace appears in Gemma-7B-Instruct (Gemma Team et al., 2024a), Gemma-2-27B-Instruct (Gemma Team et al., 2024b), LLaMA2-13B-chat (Touvron et al., 2023), Mistral-7B-Instruct-v0.3 (Jiang et al., 2023) and the LLaMA3-8B base model. This finding supports the Platonic Representation Hypothesis proposed by Huh et al. (2024) and the Natural Abstraction Hypothesis by Wentworth (2021), which suggest that representations in advanced AI models are converging.

The code and datasets for replicating the experiments can be found at [https://github.com/sciai-lab/Truth_is_Universal](https://github.com/sciai-lab/Truth_is_Universal).

After recent studies have cast doubt on the possibility of robust lie detection in LLMs, our work offers a remedy by identifying two distinct "truth directions" within these models. This discovery explains the generalisation failures observed in previous studies and leads to the development of a

Figure 1: Top left: The activation vectors of multiple statements projected onto the 2D subspace spanned by our estimates for \(_{G}\) and \(_{P}\). Purple squares correspond to false statements and orange triangles to true statements. Top center: The activation vectors of _affirmative_ true and false statements separate along the direction \(_{A}\). Top right: However, _negated_ true and false statements do not separate along \(_{A}\). Bottom: Empirical distribution of activation vectors corresponding to both affirmative and negated statements projected onto \(_{G}\) and \(_{A}\), respectively. Both affirmative and negated statements separate well along the direction \(_{G}\) proposed in this work.

more robust LLM lie detector. As discussed in Section 6, our work opens the door to several future research directions in the general quest to construct more transparent, honest and safe AI systems.

## 2 Datasets with true and false statements

To explore the internal truth representation of LLMs, we collected several publicly available, labelled datasets of true and false English statements from previous papers. We then further expanded these datasets to include negated statements, statements with more complex grammatical structures and German statements. Each dataset comprises hundreds of factual statements, labelled as either true or false. First, as detailed in Table 1, we collected six datasets of affirmative statements, each on a single topic. The cities and sp_en_trans datasets are from Marks and Tegmark (2023), while element_symbol, animal_class, inventors and facts are subsets of the datasets compiled by Azaria and Mitchell (2023). All datasets, with the exception of facts, consist of simple, uncontroversial and unambiguous statements. Each dataset (except facts) follows a consistent template. For example, the template of cities is "The city of <city name> is in <country name>.", whereas that of sp_en_trans is "The Spanish word <Spanish word> means <English word>." In contrast, facts is more diverse, containing statements of various forms and topics.

Following Levinstein and Herrmann (2024), each of the statements in the six datasets from Table 1 is negated by inserting the word "not". For instance, "The Spanish word 'dos' means 'enemy'." (False) turns into "The Spanish word 'dos' does not mean 'enemy'." (True). This results in six additional datasets of negated statements, denoted by the prefix "neg_". The datasets neg_cities and neg_sp_en_trans are from Marks and Tegmark (2023), neg_facts is from Levinstein and Herrmann (2024), and the remaining datasets were created by us.

Furthermore, we use the DeepL translator tool to translate the first 50 statements of each dataset in Table 1, as well as their negations, to German. The first author, a native German speaker, manually verified the translation accuracy. These datasets are denoted by the suffix _de, e.g. cities_de or neg_facts_de. Unless otherwise specified, when we mention affirmative and negated statements in the remainder of the paper, we refer to their English versions by default.

Additionally, for each of the six datasets in Table 1 we construct logical conjunctions ("and") and disjunctions ("or"), as done by Marks and Tegmark (2023). For conjunctions, we combine two statements on the same topic using the template: "It is the case both that [statement 1] and that [statement 2].". Disjunctions were adapted to each dataset without a fixed template, for example: "It is the case either that the city of Malacca is in Malaysia or that it is in Vietnam.". We denote the datasets of logical conjunctions and disjunctions by the suffixes _conj and _disj, respectively. From now on, we refer to all these datasets as topic-specific datasets \(D_{i}\).

In addition to the 36 topic-specific datasets, we employ two diverse datasets for testing: common_claim_true_false (Casper et al., 2023) and counterpart_true_false (Meng et al., 2022), modified by Marks and Tegmark (2023) to include only true and false statements. These datasets offer a wide variety of statements suitable for testing, though some are ambiguous, malformed, controversial, or potentially challenging for the model to understand (Marks and Tegmark, 2023). Appendix A provides further information on these datasets, as well as on the logical conjunctions, disjunctions and German statements.

 Name & Topic; Number of statements & Example; T/F = True/False \\ cities & Locations of cities; 1496 & The city of Bhopal is in India. (T) \\ sp\_en\_trans & Spanish to English translations; 354 & The Spanish word ‘uno’ means ‘one’. (T) \\ element\_symbol & Symbols of elements; 186 & Indium has the symbol As. (F) \\ animal\_class & Classes of animals; 164 & The giant anteater is a fish. (F) \\ inventors & Home countries of inventors; 406 & Galileo Galilei lived in Italy. (T) \\ facts & Diverse scientific facts; 561 & The moon orbits around the Earth. (T) \\  

Table 1: Topic-specific Datasets \(D_{i}\)Supervised learning of the truth directions

As mentioned in the introduction, we learn the truth directions from the internal model activations. To clarify precisely how the activations vectors of each model are extracted, we first briefly explain parts of the transformer architecture (Vaswani, 2017; Elhage et al., 2021) underlying LLMs. The input text is first tokenized into a sequence of \(h\) tokens, which are then embedded into a high-dimensional space, forming the initial residual stream state \(_{0}^{h d}\), where \(d\) is the embedding dimension. This state is updated by \(L\) sequential transformer layers, each consisting of a multi-head attention mechanism and a multilayer perceptron. Each transformer layer \(l\) takes as input the residual stream activation \(_{l-1}\) from the previous layer. The output of each transformer layer is added to the residual stream, producing the updated residual stream activation \(_{l}\) for the current layer. The activation vector \(_{L}^{d}\) over the final token of the residual stream state \(_{L}^{h d}\) is decoded into the next token distribution.

Following Marks and Tegmark (2023), we feed the LLM one statement at a time and extract the residual stream activation vector \(_{l}^{d}\) in a fixed layer \(l\) over the final token of the input statement. We choose the final token of the input statement because Marks and Tegmark (2023) showed via patching experiments that LLMs encode truth information about the statement above this token. The choice of layer depends on the LLM. For LLaMA3-8B we choose layer 12. This is justified by Figure 2, which shows that true and false statements have the largest separation in this layer, across several datasets.

Following this procedure, we extract an activation vector for each statement \(s_{ij}\) in the topic-specific dataset \(D_{i}\) and denote it by \(_{ij}^{d}\), with \(d\) being the dimension of the residual stream at layer 12 (\(d=4096\) for LLaMA3-8B). Here, the index \(i\) represents a specific dataset, while \(j\) denotes an individual statement within each dataset. Computing the LLaMA3-8B activations for all statements (\( 45000\)) in all datasets took less than two hours using a single Nvidia Quadro RTX 8000 (48 GB) GPU.

As mentioned in the introduction, we demonstrate the existence of _two_ truth directions in the activation space: the general truth direction \(_{G}\) and the polarity-sensitive truth direction \(_{P}\). In Figure 1 we visualise the projections of the activations \(_{ij}\) onto the 2D subspace spanned by our estimates of the vectors \(_{G}\) and \(_{P}\). In this visualization of the subspace, we choose the orthonormalized versions of \(_{G}\) and \(_{P}\) as its basis. We discuss the reasons for this choice of basis for the 2D subspace in Appendix B. The activations correspond to an equal number of affirmative and negated statements from all topic-specific datasets.

The top left panel shows both the general truth direction \(_{G}\) and the polarity-sensitive truth direction \(_{P}\). \(_{G}\) consistently points from false to true statements for both affirmative and negated statements and separates them well with an area under the receiver operating characteristic curve (AUROC) of 0.98 (bottom left panel). In contrast, \(_{P}\) points from _false to true_ for affirmative statements and from _true to false_ for negated statements. In the top center panel, we visualise the affirmative truth direction \(_{A}\), found by training a linear classifier solely on the activations of affirmative statements. The activations of true and false _affirmative_ statements separate along \(_{A}\) with a small overlap. However, this direction does not accurately separate true and false _negated_ statements (top right panel). \(_{A}\) is a linear combination of \(_{G}\) and \(_{P}\), explaining why it fails to generalize to negated statements.

Now we present a procedure for supervised learning of \(_{G}\) and \(_{P}\) from the activations of affirmative and negated statements. Each activation vector \(_{ij}\) is associated with a binary truth label \(_{ij}\{-1,1\}\) and a polarity \(p_{i}\{-1,1\}\).

\[_{ij}=-1&$ is { false}}\\ +1&$ is { true}} \]

\[p_{i}=-1&$ contains { negated} statements}\\ +1&$ contains { affirmative} statements} \]

Figure 2: Ratio of the between-class variance and within-class variance of activations corresponding to true and false statements, across residual stream layers, averaged over all dimensions of the respective layer.

We approximate the activation vector \(_{ij}\) of an affirmative or negated statement \(s_{ij}\) in the topic-specific dataset \(D_{i}\) by a vector \(}_{ij}\) as follows:

\[}_{ij}=_{i}+_{ij}_{G}+_{ij}p_{i} _{P}. \]

Here, \(_{i}^{d}\) represents the population mean of the activations which correspond to statements about topic \(i\). We estimate \(_{i}\) as:

\[_{i}=}_{j=1}^{n_{i}}_{ij}, \]

where \(n_{i}\) is the number of statements in \(D_{i}\). We learn \(_{G}\) and \(_{P}\) by minimizing the mean squared error between \(}_{ij}\) and \(_{ij}\), summing over all \(i\) and \(j\)

\[_{i,j}L(_{ij},}_{ij})=_{i,j}\|_{ij} -}_{ij}\|^{2}. \]

This optimization problem can be efficiently solved using ordinary least squares, yielding closed-form solutions for \(_{G}\) and \(_{P}\). To balance the influence of different topics, we include an equal number of statements from each topic-specific dataset in the training set.

Figure 3 shows how well true and false statements from different datasets separate along \(_{G}\) and \(_{P}\). We employ a leave-one-out approach, learning \(_{G}\) and \(_{P}\) using activations from all but one topic-specific dataset (including both affirmative and negated versions). The excluded datasets were used for testing. Separation was measured using the AUROC, averaged over 10 training runs on different random subsets of the training data. The results clearly show that \(_{G}\) effectively separates both affirmative and negated true and false statements, with AUROC values close to one. In contrast, \(_{P}\) behaves differently for affirmative and negated statements. It has AUROC values close to one for affirmative statements but close to zero for negated statements. This indicates that \(_{P}\) separates affirmative and negated statements in reverse order. For comparison, we trained a Logistic Regression (LR) classifier with bias \(b=0\) on the centered activations \(}_{ij}=_{ij}-_{i}\). Its direction \(_{LR}\) separates true and false statements similarly well as \(_{G}\). We will address the challenge of finding a well-generalizing bias in Section 5.

## 4 The dimensionality of truth

As discussed in the previous section, when training a linear classifier only on affirmative statements, a direction \(_{A}\) is found which separates well true and false affirmative statements. We refer to \(_{A}\) and the corresponding one-dimensional subspace as the affirmative truth direction. Expanding the scope to include negated statements reveals a _two_-dimensional truth subspace. Naturally, this raises questions about the potential for further linear structures and whether the dimensionality increases again with the inclusion of new statement types. To investigate this, we also consider logical conjunctions and disjunctions of statements, as well as statements that have been translated to German, and explore if additional linear structures are uncovered.

### Number of significant principal components

To investigate the dimensionality of the truth subspace, we analyze the fraction of truth-related variance in the activations \(_{ij}\) explained by the first principal components (PCs). We isolate truth-related variance through a two-step process: (1) We remove the differences arising from different sentence structures and topics by computing the centered activations \(}_{ij}=_{ij}-_{i}\) for all topic-specific datasets \(D_{i}\); (2) We eliminate the part of the variance within each \(D_{i}\) that is uncorrelated

Figure 3: Separation of true and false statements along different truth directions as measured by the AUROC.

with the truth by averaging the activations:

\[}_{i}^{+}=}_{j=1}^{n_{i}/2}}_{ij }^{+}}_{i}^{-}=}_{j=1}^{n_{i}/2}}_{ij}^{-}, \]

where \(}_{ij}^{+}\) and \(}_{ij}^{-}\) are the centered activations corresponding to true and false statements, respectively. We then perform PCA on these preprocessed activations, including different statement types in the different plots. For each statement type, there are six topics and thus twelve centered and averaged activations \(}_{i}^{}\) used for PCA.

Figure 4 illustrates our findings. When applying PCA to affirmative statements only (top left), the first PC explains approximately 60% of the variance in the centered and averaged activations, with subsequent PCs contributing significantly less, indicative of a one-dimensional affirmative truth direction. Including both affirmative and negated statements (top center) reveals a two-dimensional truth subspace, where the first two PCs account for more than 60% of the variance in the preprocessed activations. Note that in the raw, non-preprocessed activations they account only for \( 10\%\) of the variance. We verified that these two PCs indeed approximately correspond to \(_{G}\) and \(_{P}\) by computing the cosine similarities between the first PC and \(_{G}\) and between the second PC and \(_{P}\), measuring cosine similarities of \(0.98\) and \(0.97\), respectively. As shown in the other panels of Figure 4, adding logical conjunctions, disjunctions and statements translated to German does not increase the number of significant PCs beyond two, indicating that two principal components sufficiently capture the truth-related variance, suggesting only two truth dimensions.

### Generalization of different truth directions

To further investigate the dimensionality of the truth subspace, we examine two aspects: (1) How well different truth directions \(\) trained on progressively more statement types generalize; (2) Whether the activations of true and false statements remain linearly separable along some direction \(\) after projecting out the 2D subspace spanned by \(_{G}\) and \(_{P}\) from the training activations. Figure 5 illustrates these aspects in the left and right panels, respectively. We compute each \(\) using the supervised learning approach from Section 3, with all polarities \(p_{i}\) set to zero to learn a single truth direction.

In the left panel, we progressively include more statement types in the training data for \(\): first affirmative, then negated, followed by logical conjunctions and disjunctions. We measure the separation of true and false activations along \(\) via the AUROC. The right panel shows the separation along truth directions learned from activations \(}_{ij}\) which have been projected onto the orthogonal complement of the 2D truth subspace:

\[}_{ij}=P^{}(_{ij}), \]

where \(P^{}\) is the projection onto the orthogonal complement of \((_{G},_{P})\). We train all truth directions on 80% of the data, evaluating on the held-out 20% if the test and train sets are the same,

Figure 4: The fraction of variance in the centered and averaged activations \(}_{i}^{+}\), \(}_{i}^{-}\) explained by the Principal Components (PCs). Only the first 10 PCs are shown.

or on the full test set otherwise. The displayed AUROC values are averaged over 10 training runs with different train/test splits. We make the following observations: Left panel: (i) A truth direction \(\) trained on affirmative statements about cities generalises to affirmative statements about diverse scientific facts but not to negated statements. (ii) Adding negated statements to the training set enables \(\) to not only generalize to negated statements but also to achieve a better separation of logical conjunctions/disjunctions. (iii) Further adding logical conjunctions/disjunctions to the training data provides only marginal improvement in separation on those statements. Right panel: (iv) Activations from the training set cities remain linearly separable even after projecting out \((_{G},_{P})\). This suggests the existence of topic-specific features \(_{i}^{d}\) correlated with truth within individual topics. This observation justifies balancing the training dataset to include an equal number of statements from each topic, as this helps disentangle \(_{G}\) from the dataset-specific vectors \(_{i}\). (v) After projecting out \((_{G},_{P})\), a truth direction \(\) learned from affirmative and negated statements about cities _fails_ to generalize to other topics. However, adding logical conjunctions to the training set restores generalization to conjunctions/disjunctions on other topics.

The last point indicates that considering logical conjunctions/disjunctions may introduce additional linear structure to the activation vectors. However, a truth direction \(\) trained on both affirmative and negated statements already generalizes effectively to logical conjunctions and disjunctions, with any additional linear structure contributing only marginally to classification accuracy. Furthermore, the PCA plot shows that this additional linear structure accounts for only a minor fraction of the LLM's internal linear truth representation, as no significant third Principal Component appears.

In summary, our findings suggest that \(_{G}\) and \(_{P}\) represent most of the LLM's internal linear truth representation. The inclusion of logical conjunctions, disjunctions and German statements did not reveal significant additional linear structure. However, the possibility of additional linear or non-linear structures emerging with other statement types, beyond those considered, cannot be ruled out and remains an interesting topic for future research.

## 5 Generalisation to unseen topics, statement types and real-world lies

In this section, we evaluate the ability of multiple linear classifiers to generalize to unseen topics, unseen types of statements and real-world lies. Moreover, we introduce TTPD (Training of Truth and Polarity Direction), a new method for LLM lie detection. The training set consists of the activation vectors \(_{ij}\) of an equal number of affirmative and negated statements, each associated with a binary truth label \(_{ij}\) and a polarity \(p_{i}\), enabling the disentanglement of \(_{G}\) from \(_{P}\). TTPD's training process consists of four steps: From the training data, it learns (i) the general truth direction \(_{G}\), as outlined in Section 3, and (ii) a polarity direction \(\) that points from negated to affirmative statements in activation space, via Logistic Regression. (iii) The training activations are projected onto \(_{G}\) and \(\). (iv) A Logistic Regression classifier is trained on the two-dimensional projected activations.

Figure 5: Generalisation accuracies of truth directions \(\) before (left) and after (right) projecting out \((_{G},_{P})\) from the training activations. The x-axis is the training set and the y-axis the test set.

In step (i), we leverage the insight from the previous sections that different types of true and false statements separate well along \(_{G}\). However, statements with different polarities need slightly different biases for accurate classification (see Figure 1). To accommodate this, we learn the polarity direction \(\) in step (ii). To classify a new statement, TTPD projects its activation vector onto \(_{G}\) and \(\) and applies the trained Logistic Regression classifier in the resulting 2D space to predict the truth label.

We benchmark TTPD against three widely used approaches that represent the current state-of-the-art: (i) Logistic Regression (LR): Used by Burns et al. (2023) and Marks and Tegmark (2023) to classify statements as true or false based on internal model activations and by Li et al. (2024) to find truthful directions. (ii) Contrast Consistent Search (CCS) by Burns et al. (2023): A method that identifies a direction satisfying logical consistency properties given contrast pairs of statements with opposite truth values. We create contrast pairs by pairing each affirmative statement with its negated counterpart, as done in Marks and Tegmark (2023). (iii) Mass Mean (MM) probe by Marks and Tegmark (2023): This method derives a truth direction \(_{}\) by calculating the difference between the mean of all true statements \(^{+}\) and the mean of all false statements \(^{-}\), such that \(_{}=^{+}-^{-}\). To ensure a fair comparison, we have extended the MM probe by incorporating a learned bias term. This bias is learned by fitting a LR classifier to the one-dimensional projections \(^{}_{}\).

### Unseen topics and statement types

Figure 5(a) shows the generalisation accuracy of the classifiers to unseen topics. We trained the classifiers on an equal number of activations from all but one topic-specific dataset (affirmative and negated version), holding out this excluded dataset for testing. TTPD and LR generalize similarly well, achieving average accuracies of \(93.9 0.2\)% and \(94.6 0.7\)%, respectively, compared to \(84.8 6.4\)% for CCS and \(92.2 0.4\)% for MM.

Next, we evaluate the classifiers' generalization to unseen statement types, training solely on activations from English affirmative and negated statements. Figure 5(b) displays classification accuracies for logical conjunctions, disjunctions, and German translations of affirmative and negated statements, averaged across multiple datasets. Individual dataset accuracies are presented in Figure 9 of Appendix E. TTPD outperforms LR and CCS in generalizing to logical conjunctions and disjunctions. It also exhibits impressive classification accuracies on German statements, only a few percentage points lower than their English counterparts. For the more diverse and occasionally ambiguous test sets common_claim_true_false and counterfact_true_false, which closely resemble the training data in form, TTPD and LR perform similarly well.

### Real-world lies

The ultimate goal of our research extends beyond distinguishing simple true and false statements. We aim to reliably detect sophisticated lies generated by LLMs (Scheurer et al., 2024; Hubinger et al., 2024; Jarvinimi and Hubinger, 2024). In such settings we cannot trust the LLM outputs as a

Figure 6: Generalization accuracies of TTPD, LR, CCS and MM. Mean and standard deviation computed from 20 training runs, each on a different random sample of the training data.

sufficiently advanced LLM that has just lied will not readily admit to its deception but will instead attempt to conceal it (see Scheurer et al. (2024) and Appendix B.2.2 of Pacchiardi et al. (2023)). This motivates the development of methods capable of detecting lies directly from internal model activations.

Towards this goal we evaluate whether the classifiers generalize to more complex real-world lies when trained only on the activations of the simple affirmative and negated statements from the topic-specific datasets. This study is the first, to our knowledge, to quantify the generalization accuracy of activation-based lie detectors across diverse, challenging real-life scenarios. We prompt LLaMA3-8B-Instruct with real-life role-playing scenarios created by Pacchiardi et al. (2023) and generate responses. An example scenario is shown in the colored boxes below (5.2). Our goal is to classify these responses as truthful or lie using internal model activations, unlike Pacchiardi et al. (2023) who used LLM outputs to follow-up questions. Specifically, we pass the prompt and the response to LLaMA3-8B-Instruct and extract activations from layer 12 over the last token of the response, as detailed in Section 3. As in the example below, all of the 26 scenarios come in two versions: one with an incentive to lie and one without. Hence, the LLM sometimes provides honest answers to the prompts and sometimes lies. For each of the 52 scenarios, we generate four completions, resulting in 208 responses. The first author manually sorted these into the categories unambiguous truthful reply, unambiguous lie, ambiguous truthful reply, ambiguous lie, and other. This categorization accounts for occasional hallucinations or exaggerations in the model's responses, which can make the truth value of a response ambiguous. For further details, see Appendix F.

TTPD correctly classifies the 49 unambiguous lies and the 55 unambiguous truthful replies with an average accuracy of \(93.8 1.5\%\), followed by MM with \(90.5 1.5\%\), LR with \(79 8\%\) accuracy and CCS with \(73 12\%\) accuracy. The means and standard deviations are computed from 100 training runs, each on a different random sample of the training data. This demonstrates the strong generalisation ability of the classifiers, and in particular TTPD, from simple statements to more complex real world scenarios. To highlight potential avenues for further improvements, we discuss failure modes of the TTPD classifier compared to LR in Appendix D.

## 6 Discussion

In this work, we explored the internal truth representation of LLMs. Our analysis clarified the generalization failures of previous classifiers, as observed in Levinstein and Herrmann (2024), and provided evidence for the existence of a truth direction \(_{G}\) that generalizes to unseen topics, unseen types of statements and real-world lies. This represents significant progress toward achieving robust, general-purpose lie detection in LLMs.

Yet, our work has several limitations. First, our proposed method TTPD utilizes only one of the two dimensions of the truth subspace. A non-linear classifier using both \(_{G}\) and \(_{P}\) might achieve even higher classification accuracies. Second, we test the generalization of TTPD, which is based on the truth direction \(_{G}\), on only a limited number of statements types and real-world scenarios. Future research could explore the extent to which it can generalize across a broader range of statement types and diverse real-world contexts. Third, our analysis only showed that the truth subspace is _at least_ two-dimensional which limits our claim of universality to these two dimensions. Examining a wider variety of statements may reveal additional linear or non-linear structures which might differ between LLMs. Fourth, it would be valuable to study the effects of interventions on the 2D truth subspace during inference on model outputs. Finally, it would be valuable to determine whether our findings apply to larger LLMs or to multimodal models that take several data modalities as input.