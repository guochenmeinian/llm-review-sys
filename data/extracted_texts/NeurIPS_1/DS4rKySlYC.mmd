# Causal Interpretation of Self-Attention

in Pre-Trained Transformers

 Raanan Y. Rohekar

Intel Labs

raanan.yehezkel@intel.com

&Yaniv Gurwicz

Intel Labs

yaniv.gurwicz@intel.com

&Shami Nisimov

Intel Labs

shami.nisimov@intel.com

All authors contributed equally.

###### Abstract

We propose a causal interpretation of self-attention in the Transformer neural network architecture. We interpret self-attention as a mechanism that estimates a structural equation model for a given input sequence of symbols (tokens). The structural equation model can be interpreted, in turn, as a causal structure over the input symbols under the specific context of the input sequence. Importantly, this interpretation remains valid in the presence of latent confounders. Following this interpretation, we estimate conditional independence relations between input symbols by calculating partial correlations between their corresponding representations in the deepest attention layer. This enables learning the causal structure over an input sequence using existing constraint-based algorithms. In this sense, existing pre-trained Transformers can be utilized for zero-shot causal-discovery. We demonstrate this method by providing causal explanations for the outcomes of Transformers in two tasks: sentiment classification (NLP) and recommendation.

## 1 Introduction

Causality plays an important role in many sciences such as epidemiology, social sciences, and finance [25; 34]. Understanding the underlying causal mechanisms is crucial for tasks such as explaining a phenomenon, predicting, and decision making. An automated discovery of causal structures from observed data alone is an important problem in artificial intelligence. It is particularity challenging when latent confounders may exist. One family of algorithms [35; 7; 8; 46; 30], called constraint-based, recovers in the large sample limit an equivalence class of the true underlying graph. However, they require a statistical test to be provided. The statistical test is often sensitive to small sample size, and in some cases it is not clear which statistical test is suitable for the data. Moreover, these method assume that data samples are generated from a single causal graph, and are designed to learn a single equivalence class. There are cases where data samples may be generated by different causal mechanisms and it is not clear how to learn the correct causal graph for each data sample. For example, decision process may be different among humans. Data collected about their actions may not be represented by a single causal graph. This is the case for recommender systems that are required to provide a personalized recommendation for a user, based on her past actions. In addition, it is desirable that such automated systems will provide a tangible explanation to why the specific recommendation was given.

Recently, deep neural networks, based on the Transformer architecture  have been shown to achieve state-of-the-art accuracy in domains such as natural language processing [9; 3; 45], vision [10; 19], and recommender systems [36; 18]. The Transformer is based on the attention mechanism, where it calculates context-dependent weights for a given input . Given an input consisting of a sequence of symbols (tokens), the Transformer is able to capture complex dependencies, but it is unclear how they are represented in the Transformer nor how to extract them.

In this paper we bridge between structural causal models and attention mechanism used in the Transformer architecture. We show that self-attention is a mechanism that estimates a linear structural equation model in the deepest layer for each input sequence independently, and show that it represents a causal structure over the symbols in the input sequence. In addition, we show that an equivalence class of the causal structure over the input can be learned solely from the Transformer's estimated attention matrix. This enables learning the causal structure over a single input sequence, using existing constraint-based algorithms, and utilizing existing pre-trained Transformer models for zero-shot causal discovery. We demonstrate this method by providing causal explanations for the outcomes of Transformers in two tasks: sentiment classification (NLP) and recommendation.

## 2 Related Work

In recent years, several advances in causal reasoning using deep neural networks were presented. One line of work is causal modeling with neural networks [(42; 12)]. Here, a neural network architecture follows a given causal graph structure. This neural network architecture is constructed to model the joint distribution over observed variables [(12)] or to explicitly learn the deterministic functions of a structural causal model [(42)]. It was recently shown that a neural network architecture that explicitly learns the deterministic functions of an SCM can be used for answering interventional and counterfactual queries [(43)]. That is, inference in rungs 2 and 3 in the ladder of causation [(26)], also called layers 2 and 3 in the Pearl causal hierarchy [(2)]. Nevertheless, this architecture requires knowing the causal graph structure before training, and supports a single causal graph.

In another line of work, an attention mechanism is added before training, and a causal graph is inferred from the attention values at inference [(17; 21)]. Attention values are compared against a threshold, and high values are treated as indication for a potential causal relation. Nauta et al. [(21)] validate these potential causal relations by permuting the values of the potential cause and measuring the effect. However, this method is suitable only if a single causal model underlay the dataset.

## 3 Preliminaries

First, we provide preliminaries for structural causal models and the attention mechanism in the Transformer-based neural architecture. Throughout the paper we denote vectors and sets with bold-italic upper-case (e.g., \(\)), matrices with bold upper-case (e.g., \(\)), random variables with italic upper-case (e.g., \(X\)), and models in calligraphic font (e.g., \(\)). Different sets of letters are used when referring to structural causal models and attention (see Appendix A-Table 1 for a list).

### Structural Causal Models

A structural causal model (SCM) represents causal mechanisms in a domain [(24; 35; 28)]. An SCM is a tuple \(\{,,,P()\}\), where \(=\{U_{1},,U_{m}\}\) is a set of latent exogenous random variables, \(=\{X_{1},,X_{n}\}\) is a set of endogenous random variables, \(=\{f_{1},,f_{n}\}\) is a set of deterministic functions describing the values \(\) given their direct causes, and \(P()\) is the distribution over \(\). Each endogenous variable \(X_{i}\) has exactly one unique exogenous cause \(U_{i}\) (\(m=n\)). The value of an endogenous variable \(X_{i}\), \( i[1,,n]\) is

\[X_{i} f_{i}(_{i},U_{i}) \]

(the left-arrow indicates an assignment due to cause-effect relation), where \(_{i}\) is the set of direct causes (parents in the causal graph) of \(X_{i}\). A graph \(\) corresponding to the SCM consists of a node for each variable, and a directed edge for each direct cause-and-effect relation, as evident from \(\). If the graph is directed and acyclic (DAG) then the SCM is called _recursive_.

In a special subclass of SCMs, referred in this paper linear-Gaussian SCM, a node value is a linear combination of its parents, and exogenous nodes are independently and normally distributed additive noise. Let \(\) be a weight matrix, where \((i,j)\) is the weight of parent node \(X_{j}\) determining the child node \(X_{i}\) and \((i,k)=0\) if \(X_{k}\) is not a parent of \(X_{i}\), and \(\) be a diagonal matrix, where \((i,i)\) is the weight of exogenous node \(U_{i}\). Then, \( i[1,,n]\)

\[X_{i}(i,)+(i,i)U_{i}, \]and in matrix form \(=+\). In the case of a recursive SCM, with nodes sorted topologically (ancestors before their descendants), the weight matrix \(\) is strictly lower triangular (zeros on the diagonal). In this paper we learn the causal graph, described by the non-zero elements of \(\), using constraint-based causal discovery approach, which generally requires assuming the following.

**Definition 1** (Causal Markov).: _In a causally Markov graph, a variable is independent of all other variables, except its effects, conditional on all its direct causes._

**Definition 2** (Faithfulness).: _A distribution is faithful to a graph if and only if every independence relation true in the distribution is entailed by the graph._

### Self-Attention Mechanism

Attention is a mechanism that predicts context dependent weights (32) for the input sequence. In this paper we analyse the attention mechanism used by Vaswani et al. (39). Here, the input is a matrix \(\) where row vector \((i,\,\,)\) is the embedding of symbol \(s_{i}\) in an input sequence \(=[s_{1},,s_{n}]\). The attention matrix is calculated by \(=softmax(_{QK}^{})\), such that the rows of \(\) sum to 1. Values matrix is computed by \(=_{V}\), where row \((i,\,\,)\) is the value vector of input symbol \(s_{i}\). We treat \(\) as a set of projection of \(\) on column vectors \(_{V}\). Finally, the output embeddings are \(=\), where the \(i\)-th output embedding \(_{i}\) is the \(i\)-th row of \(\).

A Link between Pre-trained Self-Attention and a Causal Graph underlying an Input Sequence of Symbols

In this section we first describe self-attention as a mechanism that encodes correlations between symbols in an unknown structural causal model (Section 4.1). After establishing this relation, we present a method for recovering an equivalence class of the causal graph underlying the input sequence (Section 4.2). We also extend the results to the multi-head and multi-layer architecture (Section 4.3).

### Self-Attention as Correlations between Nodes in a Structural Causal Model

We now describe how self-attention can be viewed as a mechanism that estimates the values of observed nodes of an SCM. We first show that the covariance over the outputs of self-attention is similar to the covariance over observed nodes of an SCM. Specifically, we model relations between symbols in an input sequence using a linear-Gaussian SCM at the output of the attention layer. In an SCM, the values over endogenous nodes, in matrix form, are \(=+\), which means

\[=(-)^{-1}. \]

Since \(\) is a strictly lower-triangular weight matrix, \((-)^{-1}\) is a lower unitriangular matrix, which is equal to the sum of a geometric series

\[(-)^{-1}=_{k=0}^{n-1}^{k}. \]

Thus, the \((i,j)\) element represents the sum of directed-paths' weights, for all directed paths from \(X_{j}\) to \(X_{i}\) having length up to \(n-1\). The weight of a directed path is the product of the weights of edges along the path. In case some of the nodes are latent confounders, then the matrix in Equation 4 may not be triangular, due to spurious associations. Equation 3 represents a system with input \(\), output \(\) and weights \((-)^{-1}\). The covariance matrix of the output is

\[_{}&=[( -_{})(-_{})^{}]=\\ &=[(-)^{-1}(-_{})(-_{})^{}((-)^{-1} )^{}]=\\ &=(-)^{-1}[(-_{})(-_{})^{}]((-)^{-1} )^{}=\\ &=((-)^{-1})_{}(( -)^{-1})^{}, \]

where \(_{}=(-)^{-1}_{}\), and \(_{}\) is the covariance matrix of exogenous variables \(\).

An attention layer in a Transformer architecture, estimates an attention matrix \(\) and a values matrix \(\) from embeddings \(\) of an input sequence of symbols \(=[s_{1},,s_{n}]^{}\). The output embeddings is calculated by \(=\). It is important to note that during self-attention training, an inductive bias is used in the form of learning separate representations for the attention matrix \(\) and values matrix \(\). It can be seen that the \(j\)-th element of the \(i\)-th row-vector, \((i,j)\) is the projection of the symbol \(s_{i}\) input embedding \((i,\,)\) on column vector \(_{V}(\,,j)\). In other words, an embedding is projected on the direction of vector \(_{V}(\,,j)\). Thus, each column of \(\) can be seen as a projection of the input embeddings. Moreover, the attention matrix is shared with all projections for calculating the output embeddings. That is, for the \(j\)-th projection the output embedding is

\[(\,\,,j)=(\,\,,j). \]

Denote \(_{j}(\,\,,j)\) and \(_{j}(\,\,,j)\) corresponding to projection \(j\). Covariance of \(_{j}\) is

\[_{_{j}} =[(_{j}-_{_{j}})(_{j}-_{_{j}})^{}]\] \[=[(_{j}-_{_{j}})(_{ j}-_{_{j}})^{}]^{}= \] \[=_{_{j}}^{},\]

where \(_{_{j}}\) is the covariance matrix of input embeddings projected on vector \(_{V}(\,\,,j)\). Since columns of \(\) describe different projections corresponding to independent columns of \(_{V}\), and from the central limit theorem it follows that \(_{}\), which leads to

\[_{}=^{}. \]

How does attention matrix \(\) and a values vector \(_{j}\) translate into a structural causal model? We interpret the attention matrix \(\) and values vector \(\) as components of an SCM, such that the covariance matrix of its endogenous variables is equal to the covariance matrix of the attention layer's output, \(_{}=_{}\). From Equation 5 and Equation 8,

\[^{}=((-)^{-1}) _{}((-)^{-1})^{}. \]

The attention matrix \(\) measures the total effect each variable has on another variable, for any projection \((\,\,,j)\) of the input embeddings. Similarly, matrix \((-)^{-1}\) in an SCM measures the effect one variable has on another variable through all directed paths in the causal graph (Equation 4) for any value of the exogenous variables \(\). Thus, form Equation 9 matrix \(\) is analogous to \((-)^{-1}\) and \((\,\,,j)\) to value assignment for \(\), which serves as context. Both \(\) and \((-)^{-1}\) are not required to be triangular as latent confounders may introduce spurious associations, such that variables might be required to estimate the values of variables earlier in the topological order. In addition, the symbols in the sequence \(\) are not assumed to be topologically ordered.

### Attention-based Causal-Discovery (ABCD)

In Section 4.1 we show that self-attention learns to represent pairwise associations between symbols of an input sequence, for which there exists a linear-Gaussian SCM that has the exact same pair-wise associations between its nodes. However, can we recover the causal structure \(\) of the SCM solely from the weights of a pre-trained attention layer? We now present the Attention-Based Causal-Discovery (ABCD) method for learning an equivalence class of the causal graph that underlies a given input sequence.

Causal discovery (causal structure learning) from observed data alone requires placing certain assumptions. Here we assume the causal Markov (24) and faithfulness (35) assumptions. Under these assumptions, constraint-based methods use tests of conditional independence (CI-tests) to learn the causal structure (27; 35; 8; 7; 30; 31; 22). A statistical CI-test is used for deciding if two variables are statistically independent conditioned on a set of variables. Commonly, partial correlation is used for CI-testing between continuous, normally distributed variables with linear relations. This test requires only a pair-wise correlation matrix (marginal dependence) for evaluating partial correlations (conditional dependence). We evaluate the correlation matrix from the attention matrix. From Equation 8 the covariance matrix of output embeddings is \(_{}=^{}\), and (pair-wise) correlation coefficients are \(_{i,j}=_{}(i,j)/_{}(i,i) _{}(j,j)}\). Unlike kernel-based CI tests (1; 13; 14; 37; 48), we do not need to explicitly define or estimate the kernel, as it is readily available by a single forward-pass of the input sequence in the Transformer. This implies the following. Firstly, our CI-testing function is inherently learned during the training stage of a Transformer, by that enjoying the efficiency in learning complex models from large datasets. Secondly, since attention is computed for each input sequence uniquely, CI-testing is unique to that specific sequence. That is, conditional independence is tested under the specific context of the input sequence.

Finally, the learned causal graph represents an equivalence class in the form of a partial ancestral graph (PAG) [29; 47], which can also encode the presence of latent confounders. A PAG represents a set of causal graphs that cannot be refuted given the data. There are three types of edge-marks (at some node \(X\)): an arrow-head '\(>X\)', a tail '\(\)', and circle '\( X\)' which represent an edge-mark that cannot be determined given the data. Note that reasoning from a PAG is consistent with every member in the equivalence class it represents.

What is the relation between the equivalence class learned by ABCD and the causal graph underlying the input sequence \(=[s_{1},,s_{n}]\)?

**Definition 3** (SCM-consistent probability-space).: _Let \(_{n}=\{[s_{1},,s_{n}]:s_{i}\}\) be a sample space, where \(\) is a countable set of symbols. Let \(_{n}\) be an event space, and \(P_{n}\) be a probability measure. For a finite \(n\), the probability space \((_{n},_{n},P_{n})\) is called SCM-consistent if every sequence \(_{n}\), was generated by a corresponding SCM with graph \(_{}\)._

**Assumption 1**.: _Attention mechanism \(\) is trained on sequences sampled from an SCM-consistent probability space, such that each input symbol is uniquely represented by its output embedding._

Many training methods, such as MLM , satisfy Assumption 1 as they train the network to predict input symbols using their corresponding output embeddings in the deepest layer. This assumption is required for relating the causal graph learned from the deepest attention layer to the causal graph underlying the input sequence.

**Theorem 1**.: _If \(\) is a sequence from an SCM-consistent probability space used to train \(\), \(_{}\) is the causal graph of the SCM from which \(\) was generated, and \(\) the attention matrix for input sequence \(\) in the deepest layer, then \(_{}\) is in the equivalence class learned by a sound constraint-based algorithm that uses covariance matrix \(_{}=^{}\) for partial correlation CI-testing._

**Corollary 1**.: _Nodes \(X_{i}\) and \(X_{j}\) are d-separated in \(_{}\) conditioned on \(^{}\) if and only if symbols \(s_{i}\) and \(s_{j}\) are d-separated in \(_{}\) conditioned on \(^{}\), a subset containing symbols corresponding to \(^{}\)._

Corollary 1 is useful for cases in which the graph over the input symbols \(_{}\), as well as the structural equation model described by \(_{}\), does not have a causal meaning. In these cases, one can reason about conditional independence relations among input symbols.

### Multiple Attention Layers and Multi-Head Attention as a Structural Causal Model

Commonly, the encoder module in the Transformer architecture  consists of multiple self attention layers executed sequentially. A non-linear transformation is applied to the the output embeddings of one attention layer before feeding it as input to the next attention layer. The non-linear transformation is applied to each symbol independently. In addition each attention layer consists of multiple self-attention heads, processing the input in parallel using head-specific attention matrix. The output embedding of each symbol is then linearly combined along the heads. It is important to note that embedding of each symbol is processed independently of embeddings of other symbols. The only part in which an embedding of one symbol is influenced by embeddings of other symbol is the multiplication by the attentnion matrix.

Thus, a multi-layer, multi-head, architecture can be viewed as a deep graphical model, where exogenous nodes of an SCM are estimated by a previous attention layer. The different heads in an attention layer learn different graphs for the same input, where their output embeddings are linearly combined. This can be viewed as a mixture model. We recover the causal graph from the last (deepest) attention layer, and treat earlier layers as context estimation, which is encoded in the values of the exogenous nodes (Figure 1).

### Limitations of ABCD

The presented method, ABCD, is susceptible to two sources of error affecting its accuracy. The first is prediction errors of the Transformer neural network, and the second is errors from the constraint-based causal discovery algorithm that uses the covariance matrix calculated in the Transformer. The presence of errors from the second source depends on the first. For a Transformer with perfect generalization accuracy, and when causal Markov and faithfulness assumptions are not violated, the second source of errors vanishes, and a perfect result is returned by the presented ABCD method.

From Assumption 1, the deepest attention layer is expected to produce embeddings for each input symbol, such that the input symbol can be correctly predicted from its corresponding embedding (one-to-one embedding). However, except for trivial cases, generalization accuracy is not perfect. Thus, the attention matrix of an input sequence, for which the Transformer fails to predict the desired outcome, might include errors. As a result, the values of correlation coefficients calculated from the attention matrix might include errors.

Constraint-based algorithms for causal discovery are generally proved to be sound and complete when a perfect CI-test is used. However, if a CI-test returns an erroneous result, the constraint-based algorithm might introduce additional errors. This notion is called stability [(35)], which is informally measured by the number of output errors as a function of the number of CI-tests errors. In fact, constraint-based algorithm differ from one another in their ability to handle CI-test errors and minimize their effect. Thus, inaccurate correlation coefficients used for CI-testing might lead to erroneous independence relations, which in turn may lead to errors in the output graph.

We expect that as Transformer models become more accurate with larger training datasets, the accuracy of ABCD will increase. In this paper's experiments we use existing pre-trained Transformers and common datasets, and use the ICD algorithm [(30)] that was shown to have state-of-the-art accuracy.

## 5 ABCD with Application To Explaining Predictions

One possible application of the proposed ABCD approach is to reason about predictions by generating causal explanations from pre-trained self-attention models such as BERT [(9)]. Specifically, we seek an explaining set that consists of a subset of the input symbols that are claimed to be solely responsible for the prediction. That is, if the effect of the explaining set on the prediction is masked, a different (alternative) prediction is provided by the neural network.

It was previously claimed that attention cannot be used for explanation [(16)]; however, in a contradicting paper [(41)], it was shown that explainability is task dependent. A common approach is to use the attention matrix to learn about input-output relations for providing explanations [(33; 5; 4)]. These often rely on the assumption that inputs having high attention values with the class token influence the output [(44; 6)]. We claim that this assumption considers only marginal statistical dependence and ignores conditional independence and explaining-away that may arise due to latent confounders.

To this end, we propose CLEANN (**C**auss**L** Explanations from **A**ttention in **N**eural **N**etworks). A description of this algorithm is detailed in Appendix B, and an overview with application to explaining movie recommendation is given in Figure 4. See also Nisimov et al. [(23)] for more details.

Figure 1: A network with three attention layers. Early attention layers (0 and 1) estimate context, which is encoded in the values of exogenous nodes for the last layer [(2)].

Empirical Evaluation

In this section we demonstrate how a causal graph constructed from a self-attention matrix in a Transformer based model can be used to explain which specific symbols in an input sequence are the causes of the Transformer output. We experiment on the tasks of sentiment classification, which classifies an input sequence, and recommendation systems, which generates a candidate list of recommended symbols (top-\(k\)) for the next item. For both experiments in this section we compare our method against two baselines from (38): (1) _Pure-Attention_ algorithm (Pure-Atten.), that uses the attention weights directly in a hill-climbing search to suggest an explaining set, (2) _Smart-Attention_ algorithm (Smart-Attn.), that adapts _Pure-Attention_, where a symbol is added to the explanation only if it reduces the score gap between the original prediction's score and the second-ranked prediction's score. Implementation tools are in [https://github.com/IntelLabs/causality-lab](https://github.com/IntelLabs/causality-lab).

### Evaluation metrics

We evaluate the quality of inferred explanations using two metrics. One measures minimality of the explanation, and the other measures how specific is the explanation to the prediction.

#### 6.1.1 Minimal explaining set

An important requirement is having the explaining set minimal in the number of symbols (40). The reason for that is twofold. (1) It is more complicated and less interpretable for humans to grasp the interactions and interplay in a set that contains many explaining symbols. (2) In the spirit of occur's razor, the explaining set should not include symbols that do not contribute to explaining the prediction (when faced with a few possibles explanations, the simpler one is the one most likely to be true). Including redundant symbols in the explaining set might result in a wrong alternative predictions when the effect of this set is masked.

#### 6.1.2 Influence of the explaining set on replacement prediction

The following metric is applicable only to tasks that produce multiple predictions, such as multiple recommendations by a recommender system (e.g., which movie to watch next), as opposed to binary classification (e.g., sentiment) of an input. Given an input sequence consisting of symbols, \(=\{s_{1},,s_{n}\}\), a neural network suggests \(_{n+1}\), the 1st symbol in the top-\(k\) candidate list. CLEANN finds the smallest explaining set within \(\) that influenced the selection of \(_{n+1}\). As a consequence, discarding this explaining set from that sequence should prevent that \(_{n+1}\) from being selected again, and instead a new symbol should be selected in replacement (replacement symbol). Optimally, the explaining set should influence the rank of only that 1st symbol (should be downgraded), but not the ranks of the other candidates in the top-\(k\) list. This requirement implies that the explaining set is unique and important for the isolation and counterfactual explanation of only that 1st symbol, whereas the other symbols in the original top-\(k\) list remain unaffected, for the most part. It is therefore desirable that after discarding the explaining set from the sequence, the new replacement symbol would be one of the original (i.e. before discarding the explaining set) top-\(k\) ranked symbols, optimally the 2nd.

### CLEANN for Sentiment classification

We exemplify the explainability provided by our method for the task of sentiment classification of movie reviews from IMDB (20) using a pre-trained BERT model (9) that was fine-tuned for the task. The goal is to propose an explanation to the classification of a review by finding the smallest explaining set of word tokens (symbols) within the review. Figure 2 exemplifies the explanation of an input review that was classified as negative by the sentiment classification model. We compare between our method and two other baselines on finding a minimal explaining set for the classification. We see that both baselines found the word 'bad' as a plausible explanation for the negative sentiment classification, however in addition, each of them finds an additional explaining word ('pizza', 'cinema') that clearly has no influence on the sentiment in the context it appears in the review. Contrary to that, our method finds the words 'bad' and 'but' as explaining the negative-review classification. Indeed, the word 'but' may serve as a plausible explanation for the negative review at the 2nd part of the sentence, since it negates the positive 1st part of the sentence (about the pizza). Additionally, Figure 2(b) showsthe corresponding attention map at the last attention layer of the model for the given input review. In addition, in Figure 2(c) we present the corresponding matrix representation of the learned graph (PAG). We can see that despite high attention values of the class token [cls] with some of the word tokens in the review (in the first row of the attention map), some of these words are found not to be directly connected to the class token in the causal graph produced by our method, and therefore may not be associated as influencing the outcome of the model.

Table 1 and Figure 3 show how the length of a review influences the explaining set size. CLEANN produces the smallest explaining sets on average, where statistical significance was tested using Wilcoxon signed-ranks test at significance level \(=0.01\). For an increasing review length it is evident that the two baselines increase their explaining set, correspondingly. Even if the additional explaining tokens are of negative/positive context, they are not the most essential and prominent word token of this kind. Contrary to that, the explaining set size produced by our method is relatively stable, with a moderate increase in the explaining set size, meaning that it keeps finding the most influential words in a sentence that dominates the decision put forward by the model.

    &  \\ Algorithm & 20 & 30 & 40 & 60 \\  Pure-Attention & 5.52 (4.34) & 8.88 (7.59) & 12.71 (10.30) & 13.14 (12.47) \\ Smart-Attention & 3.62 (2.45) & 4.63 (3.11) & 6.03 (3.34) & 6.41 (4.74) \\ CLEANN & **2.82** (1.71) & **3.48** (2.18) & **3.58** (1.83) & **3.53** (1.54) \\   

Table 1: Mean and standard deviation (in parenthesis) of explanation set sizes for different review lengths (number of tokens in the input sequence). Each column is calculated using 25,000 reviews. CLEANN provides explaining sets having the smallest size on average, with statistical significance tested using the Wilcoxon signed-ranks test at significance level \(=0.01\) (indicated in bold).

Figure 3: Comparison between CLEANN and the baseline methods on the distribution of explaining set size over the various reviews. The reviews are sorted by their corresponding explanation size on the horizontal axis. CLEANN consistently finds smaller sets for: (a) review length = 20, (b) review length = 30, (c) review length = 40, (d) review length = 60.

Figure 2: Comparison between the explaining sets produced by the methods given a negative review (a) Notice the implausible explaining tokens ‘cinema’ and ‘pizza’ found by the baseline methods. In contrast, in addition to the token ‘bad’, CLEANN found the token ‘but’, which is a plausible explanation as it is a negation to positive part of the sentence that precedes it. (b) Attention matrix produced for the review by the model. (c) A matrix representation of the learned graph (PAG) produced by CLEANN (green: arrow head, yellow: arrow tail, blue: circle mark).

### CLEANN for recommendation system

We assume that the human decision process, for selecting which items to interact with, consists of multiple decision pathways that may diverge and merge over time. Moreover, they may be influenced by latent confounders along this process. Formally, we assume that the decision process can be modeled by a causal DAG consisting of observed and latent variables. Here, the observed variables are user-item interactions \(\{s_{1},,s_{n}\}\) in a session \(S\), and latent variables \(\{H_{1},H_{2},\}\) represent unmeasured influences on the user's decision to interact with a specific item. Examples for such unmeasured influences are user intent and previous recommendation slates presented to the user.

We exemplify the explainability provided by our method for the task of a recommendation system, and suggest it as a means for understanding of the complex human decision-making process when interacting with the recommender system. Using an imaginary session that includes the recommendation, we extract an explanation set from the causal graph. To validate this set, we remove it from the original session and feed the modified session into the recommender, resulting in an alternative recommendation that, in turn, can also be explained in a similar manner. Figure 4 provides an overview of our approach.

For empirical evaluation, we use the BERT4Rec recommender , pre-trained on the MovieLens 1M dataset  and estimate several measures to evaluate the quality of reasoned explanations.

**Minimal explaining set:** Figure 5(a) compares the explaining set size for the various sessions produced by CLEANN and the baseline methods. It is evident that the set sizes found by CLEANN are smaller. Figure 5(b) shows the difference between the explaining set sizes found by the baseline method and CLEANN, as calculated for each session, individually. Approximately 25% of the sessions are with positive values, indicating smaller set sizes for CLEANN, zero values shows equality between the two, and only 10% of the sessions are with negative values, indicating smaller set sizes for Pure-Attention.

**Influence of the explaining set on replacement recommendation:** Figure 6(a) compares the distribution for positions of replacement recommendations, produced by CLEANN and the baseline methods, in the _original_ top-5 recommendations list. It is evident that compared to the baseline methods, CLEANN recommends replacements that were ranked higher (lower position) in the original top-5 recommendations list. In a different view, Figure 6(b) shows the relative gain in the number of sessions for each position, achieved by CLEANN compared to the baseline methods. There is a trend line indicating higher gains for CLEANN at lower positions. That is, the replacements are more aligned with the original top-5 recommendations. CLEANN is able to isolate a minimal explaining set that influences mainly the 1st item from the original recommendations list.

Figure 4: An overview of the presented approach. As an example, \(\) is a pre-trained BERT4Rec recommender . (a) Given a set of \(n\) movies (symbols) \([s_{1},,s_{n}]\) the user interacted with, and the \((n+1)^{th}\) movie masked, the recommender \(\) recommends \(_{n+1}\). Next, in the abductive inference stage (b) the recommendation is treated as input in addition to the observed user-movie interactions, that is, input: \([s_{1},,s_{n},_{n+1}]\), and the attention matrix is extracted. (c) A causal structure learning stage in which the attention matrix is employed for testing conditional independence in a constraint-based structure learning algorithm. The result is a Markov equivalence class. Here, as an example, a causal DAG (a member of the equivalence class) is given, where \(H_{1}\) and \(H_{2}\) are latent confounders. (d) A causal explanation reasoned from the causal graph.

## 7 Discussion

We presented a relation between the self-attention mechanism used in the Transformer neural architecture and the structural causal model often used to describe the data-generating mechanism.

One result from this relation is that, under certain assumptions, a causal graph can be learned for a single input sequence (ABCD). This can be viewed as utilizing pre-trained models for _zero-shot causal-discovery_. An interesting insight is that the only source of errors while learning the causal structure is in the estimation of the attention matrix. Estimation is learned during pre-training the Transformer. Since in recent years it was shown that Transformer models scale well with model and training data sizes, we expect the estimation of the attention matrix to be more accurate as the pre-training data increases, thereby improving the accuracy of causal discovery.

Another result is that the causal structure learned for an input sequence can be employed to reason about the prediction for that sequence by providing _causal explanations_ (CLEANN). We expect learned causal graphs to be able to answer a myriad of causal queries (24), among these are personalized queries that can allow a richer set of predictions. For example, in recommender systems, assuming that the human decision process consists of multiple pathways that merge and split, by identifying independent causal pathways, recommendations can be provided for each one, independently.

The results of this paper contribute to the fields of causal inference and representation learning by providing a relation that bridges concepts from these two domains. For example, they may lead to 1) causal reasoning in applications for which large-scale pre-trained models or unlabeled data exist, and 2) architectural modifications to the Transformer to alleviate causal inference.

Figure 5: Explaining set size. (a) Comparison between CLEANN and the baseline methods on the explaining set size (sorted by size along the horizontal axis) for the various sessions. Set sizes found by CLEANN are smaller. (b) The difference between set sizes found by the baseline method and CLEANN, presented for each individual session. Positive values are in favor of CLEANN.

Figure 6: Positions of the replacement recommendations within the original top-5 recommendations list. (a) Position distribution of the replacements: compared to the baseline methods, CLEANN recommends replacements which are ranked higher in the original top-5 recommendations. (b) The relative gain in the number of sessions for each position, achieved by CLEANN. There is a trend line indicating higher gains for CLEANN at lower positions (replacements are more aligned with the original top-5 recommendation).