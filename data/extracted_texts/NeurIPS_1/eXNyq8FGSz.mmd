# Active Learning of _General_ Halfspaces:

Label Queries _vs_ Membership Queries

 Ilias Diakonikolas

University of Wisconsin-Madison

ilias@cs.wisc.edu

Equal Contribution

Equal Contribution

Daniel M. Kane

University of California, San Diego

dakane@cs.ucsd.edu

&Mingchen Ma

University of Wisconsin-Madison

mingchen@cs.wisc.edu

###### Abstract

We study the problem of learning _general_ (i.e., not necessarily homogeneous) halfspaces under the Gaussian distribution on \(^{d}\) in the presence of some form of query access. In the classical pool-based active learning model, where the algorithm is allowed to make adaptive label queries to previously sampled points, we establish a strong information-theoretic lower bound ruling out non-trivial improvements over the passive setting. Specifically, we show that any active learner requires label complexity of \((d/((m)))\), where \(m\) is the number of unlabeled examples. Specifically, to beat the passive label complexity of \((d/)\), an active learner requires a pool of \(2^{(d)}\) unlabeled samples. On the positive side, we show that this lower bound can be circumvented with membership query access, even in the agnostic model. Specifically, we give a computationally efficient learner with query complexity of \((\{1/p,1/\}+d(1/))\) achieving error guarantee of \(O(+)\). Here \(p[0,1/2]\) is the bias and \(\) is the 0-1 loss of the optimal halfspace. As a corollary, we obtain a strong separation between the active and membership query models. Taken together, our results characterize the complexity of learning general halfspaces under Gaussian marginals in these models.

## 1 Introduction

In Valiant's PAC learning model , the learner is given access to random labeled examples and aims to find an accurate approximation to the function that generated the labels. The standard PAC model is "passive" in the sense that the learner has no control over the selection of the training set. Here we focus on _interactive_ learning between a learner and a domain expert that can potentially lead to significantly more efficient learning procedures. A standard such paradigm is (pool-based) active learning , where the learner has access to a large pool of unlabeled examples \(S\) and has the ability to (adaptively) select a subset of \(S\) and obtain their labels. We will henceforth refer to this type of data access as _label query_ access. An even stronger interactive model is that of PAC learning with _membership queries_. A _membership query (MQ)_ allows the learner to obtain the value of the target function on _any_ desired point in the support of the marginal distribution. This model captures the ability to perform experiments or the availability of expert advice. While in active learning, the learner is allowed to query the labels of previously sampled points from \(S\), in MQ learning the learner has black-box access to the target function. We refer the reader to Appendix A for formal definitions of these two learning models. Roughly speaking, whenthe size of \(S\) becomes exponentially large (so that it is a good cover of the space), the model of active learning "converges" to the model of learning with MQs. This intuitive connection will be useful in the proceeding discussion.

Active learning is motivated by the availability of large amounts of unlabeled data at low cost. As such, the typical goal in this model is to develop algorithms with qualitatively improved label complexity (compared to passive learning) at the expense of a larger -- but, ideally, still reasonably bounded -- set of unlabeled data. Over the past two decades, a large body of work in theoretical machine learning has studied the possibilities and limitations of active learning in a variety of natural and important settings; see, e.g. .

A prototypical setting where active learning leads to substantial savings is for the task of learning _homogeneous_ Linear Threshold Functions (LTFs) or halfspaces. An LTF is any function \(h:^{d}\{ 1\}\) of the form \(h(x)=(w x+t)\), where \(w S^{d-1}\) is called the weight vector and \(t\) is called the threshold. If \(t=0\), the halfspace is called homogeneous. The problem of learning halfspaces is one the classical problems in machine learning, going back to the Perceptron algorithm  and has had a great impact on many other influential techniques, including SVMs  and AdaBoost .

For the class of homogeneous halfspaces under well-behaved distributions (including the Gaussian and isotropic log-concave distributions), prior work has established that \(O(d(1/))\) label queries suffice, where \(d\) is the dimension and \(\) is the desired accuracy . Moreover, there are computationally efficient algorithms with near-optimal label complexity for this task , even in the agnostic model that achieve error \(O(+)\). Unfortunately, this logarithmic dependence on \(1/\) breaks down for general (potentially biased) halfspaces. Intuitively, this holds because if the bias of a halfspace (the probability mass of the small class) is \(p\), then we need to obtain at least \(1/p\) labeled examples before we see the first point in the small class. This implies an information-theoretic label complexity lower bound of \((\{1/p,1/\}+d(1/))\), even for realizable PAC learning under the uniform distribution on the sphere. Hanneke et al.  showed an information-theoretic label complexity upper bound of \(((1/p)d^{3/2}(1/))\) for general halfspaces under the uniform distribution on the sphere (via an exponential-time algorithm).

In summary, prior to this work, the possibility that there is an active learner with label complexity \(O(d(1/)+\{1/p,1/\})\) and unlabeled sample complexity \((d/)\) remained open. Our first main result is an information-theoretic lower bound ruling out this possibility.

**Theorem 1.1** (Main Lower Bound).: _For any active learning algorithm \(\), there is a halfspace \(h^{*}\) that labels \(S\) with bias \(p\) such that if \(\) makes less than \((d/(p(m)))\) label queries over \(S\), a set of \(m\) i.i.d. points drawn from \(N(0,I)\), then with probability at least \(2/3\) the halfspace \(\) output by \(\) has error more than \(p/2\) with respect to \(h^{*}\)._

In particular, if \(p\) is chosen as \(((1/))\), learning a \(p\)-bias halfspace with error \(C\) (for any fixed constant \(C\)) would require a learning algorithm to either make \((d/)\) label queries or have a pool of \(2^{d}\) unlabeled examples. Our information-theoretic lower bound essentially shows that the active setting does not provide non-trivial advantages for the class of general halfspaces, unless the learner is allowed to obtain exponentially many unlabeled examples. (As already mentioned, in this extreme setting, the active learning model approximates PAC learning with MQs.) This motivates the study of learning halfspaces in the stronger model with MQs, where better upper bounds may be attainable.

To circumvent the aforementioned lower bound, we consider the stronger model of PAC learning with MQs. We are interested in understanding the query complexity of learning general halfspaces under the Gaussian distribution. We study this question in the agnostic learning model and establish the following positive result, the proof of which can be found in Appendix G:

**Theorem 1.2** (Main Algorithmic Result).: _Consider the problem of agnostic PAC learning halfspaces with membership queries under the Gaussian distribution. There is an algorithm such that for every labeling function \(y(x)\) and for every \(,(0,1)\), it makes \(M=_{6}(\{1/p,1/\}+d(1/))\)\({}^{*}\)memberships queries, runs in \((d,M)\) time, where \(p\) is the bias of the optimal halfspace \(h^{*}\), and outputs an \( H\) such that with probability at least \(1-\), \(() O(+)\)._In other words, we provide a computationally efficient constant factor agnostic query learner with query complexity \((\{1/p,1/\}+d(1/))\). Due to known \(d^{(1/)}\) complexity lower bounds for achieving optimal error of \(+\), the majority of work  in the passive PAC model has focused on designing efficient learners achieving a constant factor approximation of \(O(+)\)These passive learning algorithms have sample complexity \((d,1/)\). Note that, by Theorem1.1, it is impossible to modify these algorithms (for general halfspaces) to achieve an active learner with low label complexity.

In the realizable setting under the Gaussian distribution, a learner may query many points that are extremely far from the origin to find examples from the small class with few queries. However, such an algorithm is quite fragile to even a tiny amount of noise. In particular, the query complexity achieved by our algorithm establishing Theorem1.2 is nearly optimal in the agnostic setting.

On the one hand, \((d(1/))\) queries are required because describing a halfspace up to error \(\) requires \(d(1/)\) bits of information . On the other hand, we argue that the overhead term of \((\{1/p,1/\})\) cannot be avoided in the agnostic setting. Such a statement can be deduced from a lower bound of : they showed that in the realizable setting, any algorithm requires at least \(((1/p)^{1-o(1)})\) MQs to see the first example from the small class (where \(p\) is the bias of the target halfspace with respect to the uniform distribution on the unit ball); they also showed a similar lower bound of \((1/p)\) if the underlying distribution is the uniform distribution over the unit sphere. As the dimension \(d\) increases, the standard Gaussian distribution is very well approximated by the uniform distribution over a \(d\)-dimensional sphere with radius \(\). Thus, an exponentially small level of noise would make every query far from this sphere contain no useful information. This allows us to show that, under the Gaussian distribution with a tiny amount of label noise, \(((1/p)^{1-o(1)})\) queries are needed to see a single example from the small class. The proof of this statement is essentially identical to the argument in  for unit ball. The reader is referred to that work for the details.

### Preliminaries

For a halfspace \(h(x)=(w x+t)\), \(w S^{d-1},t>0\), we use \(p(t)=_{x N(0,I)}(h(x)=-1)\) to denote its bias. For a halfspace \(h(x)\), we define its Chow-Parameter under the standard Gaussian distribution to be \(_{x N(0,I)}\,xh(x)\). Let \(y(x):^{d}\{ 1\}\) be a (randomized) labeling function for examples in \(^{d}\). We denote by \((h)=_{x N(0,I)}(h(x) y(x))\) to be the error of the hypothesis \(h\) and \(=_{h H}(h)\), where \(H\) is the class of halfspaces over \(^{d}\). We will use \(h^{*}\) to denote the halfspace with an error equal to \(\). When there is no confusion, we will use \(p\) to denote the bias of the optimal halfspace \(h^{*}\).

Let \(D_{x}\) be a distribution over \(^{d}\), \(y(x)\) be a labeling function over \(^{d}\) and \(S=\{(x_{i},y(x_{i}))\}_{i=1}^{m}\) be a set of i.i.d. examples drawn from the distribution \(D\) over \(^{d}\{ 1\}\) such that the marginal distribution of \(D\) is \(D_{x}\). A membership query takes an \(x\) in the support of \(D_{x}\) as input and outputs \(y(x)\). A label query takes an \(x_{i}\), where \((x_{i},y(x_{i})) S\) as input and outputs \(y(x_{i})\). A learning algorithm \(\) is allowed to use membership queries/label queries and aims to output a halfspace hypothesis \(\) such that \(() O(+)\) by making as few queries as possible.

## 2 Nearly-Tight Lower Bound on Label Complexity: Proof of Theorem1.1

In this section, we prove our information-theoretic lower bound on the label complexity of active learning general halfspaces under the Gaussian distribution.

Before presenting our proof, we provide high-level intuition behind Theorem1.1 and the strategy of our proof. Previous work, see, e.g. , showed that if \(S\) is a set of examples drawn uniformly from the unit sphere, and if \(h^{*}\) is a halfspace with bias \(p\) that is chosen uniformly, the following holds: no matter which query strategy a learning algorithm \(\) uses, for the first \(r\) queries, in expectation only \(pr\) of them fall into the small cap on the sphere cut by \(h^{*}\). Thus, if \(\) makes less than \(1/(2p)\) queries, it will with constant probability not see any negative examples; and it is therefore impossible to learn the target halfspace.

[MISSING_PAGE_FAIL:4]

By Lemma 2.3, we know that by choosing \(k=O(d/(m)(t^{*})^{4})\), with probability at least \(2/3\), for every \(k\)-tuple of examples \(x_{1},,x_{k} S\), \(\|{AA^{}-dI}\|_{2} d/(t^{*})^{2}\), where \(A^{k d}\) is a matrix with row vectors \(x_{1},,x_{k}\). By Lemma 2.2, we know that every \(k\)-tuple of examples \(x_{1},,x_{k} S\) has a probability \(^{k}\), which is at most \(O(p p)^{k}\) to be labeled all negative by the random halfspace \(h^{*}\). Notice that every query algorithm can be expressed as a binary tree \(T\). Each node of the tree represents an example where the algorithm makes queries at a time. If the example at node \(v\) is negative, then the algorithm will query the left child of \(v\), and otherwise it will query the right child of \(v\). The algorithm stops making queries when either it has queried \(r\) examples or it has queried \(k\) negative examples. In particular, for a given search algorithm, there are at most \(\) different possible outcomes where it successfully finds \(k\) negative examples. Furthermore, for each of the possible outcomes, there is a set of \(k\) examples in \(S\) that correspond to the \(k\) negative examples the algorithm finds. Thus, the probability that the algorithm successfully finds \(k\) negative examples is bounded above by the probability that there exists one of the \(\)\(k\)-tuples of examples in \(S\) that are all labeled negative by \(h^{*}\). Such a probability can be bounded above by

\[^{k}O(p(1/p))^{k} 2/3\,\]

if \(r O(k/p(1/p))=O(d/(p(m)(1/p))\). By Lemma 2.1, we know that if we can make \(O(d/(p(m)(1/p))\) label queries to learn a \(p\)-biased halfspace up to error \(p/2\) over a set \(S\) of \(m/2\) Gaussian examples, then we can use \(O(d/(p(m)(1/p))\) queries to find \(d\) negative examples among \(m\) Gaussian points. This leads to a contradiction. Thus, the label complexity of the learning problem is \((d/(p(m)))\), as desired.

## 3 Robust Learning of General Halfspaces with MQs: Proof of Theorem 1.2

In this section, we present our main algorithmic result, Theorem 1.2. We refer the readers to Appendix G for the full proof of Theorem 1.2. Throughout the paper, we will assume for convenience that the noise level \(\). Such an assumption can be made without loss of generality, as discussed in Appendix C.1. We first present our main algorithm, Algorithm 1. Algorithm 1 will maintain a list of \((1/)\) candidate hypotheses at least one of which has error \(O(+)\). We will then use a standard tournament approach to find an accurate hypothesis among them.

```
Input: error parameter \((0,1)\), confidence parameter \((0,1)\) Output: halspace \((x)=( x+),\) where \( S^{d-1},>0\) \(\)\(\) Create a list of candidate hypothesises \(\)  Use \((\{1/p,1/\})\) queries to estimate \(p\) by some \(\) such that \( p 2\) (or verify \(p<C\) and return \(+1\), the constant hypothesis).  Let \(t_{a},t_{b}>0\) such that a halfspace with threshold \(t_{a}\) has bias \(2\) and with threshold \(t_{b}\) has bias \(\).  Build grid points \(t_{a}=t_{0}<t_{1}<<t_{}=t_{b}\) such that \(|t_{i+1}-t_{i}|=1/(2(1/)), i-1\). \(\)Guess the true threshold \(t^{*}\) with \(t^{}\{t_{0},t_{1},\}\) for\(j=0,,\)do  Repeat the following procedure \((1/)(1/)\) times \(w_{0}(,t_{j},/(1/))\)\(\)Find a \(w_{0} S^{d-1}\) as a warm start \((w_{T},)(w_{0},t_{j},./(1/))\)\(\)Find a \(w_{T} S^{d-1}\) close enough to \(w^{*}\) and \(\) close enough to \(t^{*}\) based on \(w_{0}\)\(\{(w_{T} x+)\}\)\(\)Add a new candidate hypothesis to \(\)  Find a good hypothesis \(\) from \(\) using Lemma C.1, a standard tournament approach return\(\)
```

**Algorithm 1** Query Learning Halfspace(Efficient Agnostic Learning Halfspaces with Queries)

At the beginning of Algorithm 1, we will use random queries to approximately estimate the bias \(p\) of the optimal halfspace up to a constant factor. As we will discuss in Appendix C.2, such an estimation can be done with only \((\{1/p,1/\})\) queries by applying a doubling trick to the coin estimation problem. In particular, if we find \(p<C\), we can directly output a constant hypothesis as it haserror only \(O()\). Since \(t^{*}\) is unknown to us, such an approach can prevent us from using some \(t^{}\) which is much larger than \(t^{*}\) in the rest of the learning procedure, which will potentially lead to a larger query complexity. With such a \(\), \(t^{*}\) will fall into a reasonable range \([t_{a},t_{b}]\). We next partition \([t_{a},t_{b}]\) into a grid of size \(O(1/(1/))\) and use each of the grid points as an initial guess of \(t^{*}\). In particular, at least one of these grid points \(t_{j}\) is \(O(1/(1/))\) close to \(t^{*}\). Although such a \(t_{j}\) is not accurate enough to be used in the final output hypothesis, as \(t^{*}\), we will show later that such a \(t_{j}\) is enough for us to use it to learn \(w^{*},t^{*}\) accurately. Suppose now we have such a good \(t_{j}\). We will design two subroutines that make use of \(t_{j}\) to produce a good hypothesis \((w_{T} x+)\). The first algorithm will take \(t_{j}\) and the noise level \(\) as its input and produce a unit vector \(w_{0}\) as an initialization. We will show in Section3.2 that as long as \(|t_{j}-t^{*}| 1/(1/)\), we can with probability at least \(1/(1/)\) produce some \(w_{0}\) such that \((w_{0},w^{*}) O(1/t_{j})\). By repeating such an initialization algorithm \((1/)\) times, with high probability one of these runs will succeed. In particular, such an algorithm has a query complexity of \((1/p+d(1/))\). Now assume we have such a \(w_{0}\) as a warm-start. Our second subroutine is to refine the direction \(w_{0}\) and the threshold \(t_{j}\). More specifically, we will maintain a unit vector \(w_{i}\) such that \(_{i}=(w_{i},w^{*})\) and an upper bound \(_{i}\) for \((_{i}/2)\). In each round of the refining algorithm, we will use \((d)\) queries to update \(w_{i}\). In particular, in each round \(_{i}\) will decrease by a constant factor and thus after at most \(T=((1/))\) rounds, we will have \((_{T}/2)_{T}=C(t_{j}^{2}/2)\). As we will show in Section3.1, provided the correct \(t^{*}\), \((w_{T} x+t^{*})\) is at most \(O()\) far from \(h^{*}\). However, to output a good hypothesis, we still need to learn \(t^{*}\) up to a high accuracy. When \(t^{*}\) is small, we even have to estimate \(t^{*}\) up to error \(O()\), which typically needs many queries. However, as we will show in Section3.1, given \(w_{T}\) close enough to \(w^{*}\), we are able to combine the localization technique used in  with this fact to learn \(t^{*}\) using only \(O((1/))\) queries. This gives an overview of Algorithm1 and its query complexity.

### Refining A Warm-Start

We will start by discussing how to refine a warm start \(w_{0}\) by proving the following theorem. The proof of the theorem and the main algorithm, Algorithm3 can be found in AppendixD.5.

**Theorem 3.1**.: _Let \(h^{*}(x)=(w^{*} x+t^{*})\) be a halfspace such that \((h^{*})=\). Let \(t^{},w_{0} S^{d-1}\) be inputs of Algorithm3. If \(t^{}-1/(1/) t^{*} t^{}\), \(t^{}((t^{})^{2}/2) 1/(C)\) and \(((w_{0},w^{*})/2)_{0}:=\{1/t^{},1/2\}\), then Algorithm3 makes \(M=_{}(d(1/))\) membership queries, runs in \((d,M)\) time, and outputs \((w_{T},)\) such that with probability at least \(1-O()\), \(((w_{T} x+)) O()\)._

As we discussed in Section3, we will assume we have some \(t^{}\) such that \(t^{}-1/(1/) t^{*} t^{}\) and some \(w_{0}\) such that \((_{0}/2)_{0}=\{1/t^{},1/2\}\), i.e., some initial knowledge of \(t^{*},w^{*}\). Our algorithm runs in iterations and will maintain some \(w_{i}\) in round \(i\). We will maintain some unit vector \(w_{i}\) and use \( w_{i}-w^{*}=2(_{i}/2)\) to measure the progress made by Algorithm3. The method we use to update \(w_{i}\) is a simple projected gradient descent algorithm. Specifically, we will construct a random vector \(G_{i}\) over \(^{d}\) such that \(G_{i} w_{i}\) and in expectation \(g_{i}=\,G_{i}\) has bounded length and a good correlation with respect to \(w^{*}\). We will show in the following lemma that by estimating \(\,G_{i}\) up to constant error with \(_{i}\) and using the update rule \(w_{i+1}=_{S^{d-1}}(w_{i}+_{i}_{i})\), we are able to significantly decrease \(_{i}\). The proof of Lemma3.2 can be found in AppendixD.1.

**Lemma 3.2**.: _Let \(w^{*},w_{i} S^{d-1}\) such that \(w^{*}=a_{i}w_{i}+b_{i}u\), where \(u S^{d-1},u w_{i},a_{i},b_{i}>0,a_{i}^{2}+b_{i}^{2}=1\). Let \(_{i}=(w_{i},w^{*})\). Let \(G_{i}\) be a random vector drawn from some distribution \(\) such that with probability \(1\), \(G_{i} w_{i}\). Let \(g_{i}\) be the mean of \(G_{i}\). Let \(_{i}\) be the empirical mean of \(G_{i}\) and \(_{i}>0\). The update rule \(w_{i+1}=_{S^{d-1}}(w_{i}+_{i}_{i})\) satisfies the following property._

\[ w_{i+1}-w^{*}^{2} w_{i}-w^{*} ^{2}-2_{i}b_{i}g_{i} u+2_{i}b_{i}_{i}-g_{i} +_{i}^{2}_{i}^{2}.\]

_Furthermore, if \((_{i}/2)_{i}(0,1)\) and there exist constant \(c_{1},c_{2}\) such that \(g_{i} u c_{1}/10\), \(_{i} c_{1}\) and \( g_{i}-_{i} c_{2} c_{1}/40\), then there exist constant \(C_{1},C_{2}>8\) such that by taking \(_{i}=_{i}/C_{1}\) and \(_{i+1}=(1-1/C_{2})_{i}\), it holds that \((_{i+1}/2)_{i+1}\). In particular, if \((_{i}/2) 3_{i}/4\) and \(_{i} c_{1}\) then \((_{i+1}/2)_{i+1}\) always holds._

In the rest of the section, we will show that as long as \(w_{i}\) is not good enough, we can always efficiently construct a random vector \(G_{i}\) whose expectation points to the correct direction and we can use very few queries to estimate its expectation up to a desired accuracy. We adapt the localization technique used in  to achieve this goal.

#### 3.1.1 Finding A Good Gradient via Localization

In the \(i\)-th round of Algorithm3, we write \(w^{*}=a_{i}w_{i}+b_{i}u_{i}\), where \(u_{i} S^{d-1},u_{i} w_{i}\), \(a_{i},b_{i}>0,a_{i}^{2}+b_{i}^{2}=1\). Recall that \(_{i}\) is an upper bound we maintain for \((_{i}/2)\). We will construct the random gradient as follows

\[G_{i}:=_{w_{i}^{}}zy(A_{i}^{1/2}z-w_{i}),\]

where \(z N(0,I)\), \(A_{i}=I-(1-_{i}^{2})w_{i}w_{i}^{t}\) and \((0,t^{})\) is a scalar. To see why \(G_{i}\) is a good choice, we will start by analyzing \(G_{i}\) assuming the noise rate \(=0\). To simplify the notation, denote by \(_{i}(z)=((a_{i}w_{i}+b_{i}u_{i}/_{i})z+(t^{*}-a)/_{i})\) and \(}=_{z N(0,I)}_{w_{i}^{}}z_{i}(z)\). A simple calculation gives us the following result.

**Fact 3.3**.: _Let \(h(x)=(w x+t)\) be a halfspace. Let \(v S^{d-1}\) such that \(w=av+bu\), where \(a,b>0,a^{2}+b^{2}=1\), \(u S^{d-1},u v\). Let \(s,>0\) be real numbers and define \(A=I-(1-^{2})vv^{t}\). For each \(z^{d}\), define \(:=A^{1/2}z-sv\). Then \(h()=(z)\), where \(\) is the following halfspace_

\[(z)=((av+bu/) z+(t-as)/)\.\]

Fact3.3 implies that if \(=0\), then it always holds that \(f_{i}(z):=y(A_{i}^{1/2}z-w_{i})=_{i}(z)\), \( z^{d}\) and we can view \(z\) as examples labeled by a halfspace \(_{i}(z)\). In particular, \(_{z N(0,I)}\,zf_{i}(z)\) is the Chow-Parameter of the halfspace \(_{i}(z)\) under the standard Gaussian distribution.

**Fact 3.4** (Lemma C.3 in ).: _Let \(h(x)=(w x+t)\), where \(w S^{d-1}\) be a halfspace. Then \(_{z N(0,I)}\,zh(z)=}/2)w}\)._

By Fact3.4, in the noiseless case, \(_{z N(0,I)}\,zf_{i}(z)\) is parallel to \((a_{i}w_{i}+b_{i}u_{i}/_{i})\) with length \(((-T_{i}^{2}))\), where \(T_{i}=-a_{i}}{_{i}^{2}+b_{i}^{2}/_ {i}^{2}}}\) and \(g_{i}=}\) is exactly the \(u_{i}\) component of the Chow-Parameter. In particular, if \(T_{i}\) is constant, then by estimating \(g_{i}\) using \(}\) up to a small constant error using \((d)\) queries, we are able to use Lemma3.2 to improve \(w_{i}\). Assuming we set \(=t^{*}\), as \(_{i}t^{} 1\) and \(b_{i} O(_{i})\), it is easy to check \(T_{i}\) can be bounded by some universal constant. However, as we mentioned before, we only know \(|t^{}-t^{*}|\), when \(w_{i}\) getting close to \(w^{*}\), \(_{i}\) could become very small and an error of \(1/(1/)\) could potentially blow up \(T_{i}\), making the signal we want quite small. Such an issue is problematic for the algorithm, especially when \(f_{i}(z)\) is a noisy version of \(_{i}(z)\). To overcome such an issue, we prove the following structural lemma in AppendixD.2 showing that we can always check whether the choice of \(\) is good or not, by looking at the bias of \((z)\), using \((1)\) queries. Using this method, we can perform a binary search for \(\) to find a correct choice in at most \((1/)\) rounds. Furthermore, as long as we select the correct \(\), it must hold that \(|-t^{*}| O(_{i})\). In particular, as \(_{T}=C((t^{})^{2}/2)\), such a \(\) is a good enough estimate for \(t^{*}\) to be used in the final hypothesis.

**Lemma 3.5**.: _Let \(w^{*},w_{i} S^{d-1}\) such that \(w^{*}=a_{i}w_{i}+b_{i}u_{i}\), where \(u_{i} S^{d-1},u w_{i},a_{i},b_{i}>0,a_{i}^{2}+b_{i}^{2}=1\). Let \(t^{*},t^{},_{i},\) be positive real numbers such that \(0 t^{*} t^{}\), \((_{i}/2)_{i},\) and \(_{i}t^{} 1\). Define \(T_{i}:= z t}{_{i}^{2}+b_{i}^{2}/ _{i}^{2}}}\), \(_{i}(z)=((a_{i}w_{i}+b_{i}u_{i}/_{i})z+(t^{*}-a)/_{i})\) and \(}=_{z N(0,I)}_{w_{i}^{}}z_{i}(z)\) for some \([0,t^{}]\). Then the following three properties hold._

1. _There exists an interval_ \(I_{t^{}}[0,t^{}]\) _of length at least_ \(_{i}\) _such that for every_ \( I_{t^{}},|T_{i}| 5\)_._
2. _When_ \(|T_{i}| 6\)_, it holds that_ \(} u_{i}=\|}\|\) _and_ \(e^{-19}b_{i}/_{i}\|}\| 2e^{-19}\)_._
3. _For every_ \(|-t^{*}|>40_{i}\) _and_ \(<t^{}\)_,_ \(|T_{i}|>10\)_._

#### 3.1.2 Robustness Analysis

So far, we have only considered the case when \(=0\). Due to the presence of noise, it is impossible for us to estimate \(}=_{z N(0,I)}\,_{w_{i}^{}}z_{i}(z)\) because we only have a noisy version \(f_{i}(z)\) of \(_{i}(z)\). In this section, we will show that as long as \(w_{i}\) is close to \(w^{*}\) and \(|t^{}-t^{*}| 1/(1/)\), the probability that for a Gaussian point \(z\), \(_{i}(z) f_{i}(z)\) is at most a tiny constant. This is incomparable with the bias of \(_{z}(z)\) if \(\) is chosen correctly, and does not affect the algorithm too much. We start with the following lemma which bounds the probability of \(_{i}(z) f_{i}(z)\).

**Lemma 3.6**.: _Let \(h^{*}(x)=(w^{*} x+t^{*})\) be a halfspace such that \((h^{*})=\). Let \(,_{i},t^{}\) be real numbers such that \( t^{}\) and \(_{i}t^{} 1,_{i} 1/2\). Let \(w^{*}=a_{i}w_{i}+b_{i}u_{i}\), where \(u_{i} S^{d-1},u w_{i},a_{i},b_{i}>0,a_{i}^{2}+b_{i}^{2}=1\). Define \(_{i}(z)=((a_{i}w_{i}+b_{i}u_{i}/_{i})z+(t^{*}-a )/_{i})\) and \(f_{i}(z)=y(A_{i}^{1/2}z-w_{i})\). Then \(_{z N(0,I)}(_{i}(z) f_{i}(z))( ^{2}/2+4)/_{i}\). In particular, if \(_{i} C((t^{})^{2}/2)\), for some sufficient large constant \(C\), then there is a sufficiently small constant \(c\) such that \(_{z N(0,I)}(_{i}(z) f_{i}(z)) c e^{-40}\)._

The proof of Lemma3.6 leverages the \((v,s,)\)- rejection procedure introduced in  (see Appendix D.3). We will use Lemma3.6 to analyze the gradient descent approach we described in the presence of noise. Formally, we establish the following lemma (see Appendix D.4 for the proof).

**Lemma 3.7**.: _Let \(w^{*},w_{i} S^{d-1}\) such that \(w^{*}=a_{i}w_{i}+b_{i}u_{i}\), where \(u_{i} S^{d-1},u w_{i},a_{i},b_{i}>0,a_{i}^{2}+b_{i}^{2}=1\). Let \(t^{*},t^{},_{i},\) be positive real numbers such that \(0 t^{*} t^{}\), \((_{i}/2)_{i},\)\(_{i} C((t^{})^{2}/2)\), and \(_{i}t^{} 1\). Let \(h^{*}(x)=(w^{*} x+t^{*})\) be a halfspace such that \((h^{*})=\). Define \(T_{i}:=-a_{i}}{_{i}^{2}+b_{i}^{2}/ _{i}^{2}}}\), \(_{i}(z)=((a_{i}w_{i}+b_{i}u_{i}/_{i})z+(t^{*}-a )/_{i})\), \(_{i}=_{z N(0,I)}_{w_{i}^{}}z_ {i}(z)\) and \(g_{i}=_{z N(0,I)}_{w_{i}^{}}zf_{i}(z)\), where \(f_{i}(z)=y(A_{i}^{1/2}z-w_{i})\) for some \([0,t^{}]\). Let \(_{i}:=_{z N(0,I)}(_{i}(z) f_{i}(z))\) and \(p_{i}\) be the probability that \(f_{i}(z)=-1\). Then the following two properties hold._

1. _If_ \(p_{i}(e^{-18},1-e^{-18})\)_, then_ \(|T_{i}|<6\) _and if_ \(|T_{i}|<5\)_, then_ \(p_{i}(e^{-16},1-e^{-16})\)_._
2. \(g_{i} u_{i}_{i} u_{i}-2_{i})}\) _and_ \(\|g_{i}\|\|_{i}\|+2_{i})}\)_._

Lemma3.7 says as the noise level is small, it will not affect the structure lemma we established in Lemma3.5 too much, and thus we are able to find the correct threshold \(\) by checking the probability of \(f_{i}(z)=-1\). Furthermore, as long as we choose the correct threshold \(\), \(g_{i}\), the noisy version of \(_{i}\) still satisfies the conditions in the statement of Lemma3.2 and thus can be used to improve \(w_{i}\).

### Finding A Good Initialization

In Section3.1, we have shown that given some \(w_{0}\) non-trivially close to \(w^{*}\) and some \(t^{}\) such that \(t^{}- t^{*} t^{}\), we can use Algorithm3 to learn a good hypothesis with high probability. In this section, we show how to find such a good initialization \(w_{0}\) using a few membership queries. The most common way to get such a warm-start is by robustly estimating the Chow-Parameter (see for example ) using Fact3.4. Such an approach does not work for general halfspaces because the length of the length of the Chow-Parameter can be as small as \((p)\), and thus needs roughly \(d/p\) random queries to estimate. In this section, we show how to overcome such an issue using a label smoothing technique, which has been useful in related problems . The main results in this step can be summarized as follows. The proof of Theorem3.8 is deferred to Appendix E.2

**Theorem 3.8**.: _Let \(h^{*}(x)=(w^{*} x+t^{*})\) and \(y(x)\) be any labeling function such that \((h^{*})= 1/C\) for some large enough constant \(C\). If \(|t-t^{*}| 1/(1/)\), then with probability at least \(1/3\), Algorithm2 makes \(M=(1/p+d(1/))\), runs in \((d,M)\) time, and outputs some \(w_{0}\) such that \(((w_{0},w^{*})/2)\{\{1/t,1/2\},O(\}\), where \(=/p\)._

Due to the space limitations, here we only consider the case when \(t^{*}\) is not extremely large, which roughly covers the regime when \( 1/t\). This suffices to capture some of the ideas and illustrate the power of the smoothed labeling. For the case when \(>1/t\), we are still able to find such a warm start by leveraging the smoothed label method in combination with the technique used in Section3.1 in a more complicated way. We postpone this analysis to Appendix F. Our algorithm, Algorithm2, to find a warm start is presented as follows.

To analyze Algorithm2, we introduce the following definitions and notations.

**Definition 3.9** (Smoothed Label).: _Let \(x^{d}\) be a point and \(y(x)\) be any labeling function. For \(\), define the random variable \(=}x+ z\), where \(z N(0,I)\). The smoothed label of \(x\) with parameter \(\) is defined as \((x):=y()\)._

We will require the following fact (whose proof follows via a direct calculation):

**Fact 3.10**.: _Let \(h^{*}(x)=(w^{*} x+t^{*})\) be a halfspace. Let \(x,z^{d}\) and define \(:=}x+ z\). Then \((z):=h^{*}()=(w^{*} z+(t^{*}+}w^{*} x)/)\) is another halfspace for \(z\) with threshold \((t^{*}+}w^{*} x)/\)._

Let \(h^{*}=(w^{*} x+t^{*})\) be an optimal halfspace and let \(y(x)\) be any labeling function such that \((h^{*})=\). For \(x^{d}\), we denote by \((x):=(h^{*}()(x))\), the noise level of the smoothed label. Assuming that we are given a random negative example \(x_{0}\), then with constant probability, it is close to the decision boundary, i.e., \(w^{*} x_{0}(-t^{*}-},-t^{*})\). This implies that the threshold of \(\), the halfspace corresponding to the smoothed label at \(x_{0}\), is between \((-1,1)\). Moreover, the Chow-Parameter of \(\) under the standard Gaussian distribution is parallel to \(w^{*}\) with a constant length, by Fact 3.4. If \(=0\), then for every \(t\), we only need another \((d(1/))\) queries to estimate the Chow-Parameter of \(\) up to error \(O(1/t)\); thus, we get a warm start \(w_{0}\) such that \((_{0}/2) 1/t\), given \(|t-t^{*}|\) is small. Therefore, the total number of queries we use to run Algorithm 2 is \((1/p+d(1/p))\). However, in general, it is impossible to estimate \(w^{*}\) up to arbitrary accuracy -- even using an infinite number of queries -- because of the presence of noise. In fact, using a random \(x_{0}\) is important for Algorithm 2 to succeed. If we are given some adversarially selected \(x_{0}\), even if it is close to the decision boundary, the above method can easily fail. This is because almost all the queries we made are in a small neighborhood of \(x_{0}\) and could be corrupted by noise arbitrarily. However, we show in Appendix E.1 that, with a probability at least \(2/3\), the noise level \((x_{0})\) of the smoothed label around \(x_{0}\) is at most \(O(/p)\), if \(x_{0}\) is a random example given \(y(x_{0})=-1\); and thus we can still estimate \(w^{*}\) to a desired accuracy provided \(/p\) is not too large.

**Lemma 3.11**.: _Let \(h^{*}(x)=(w^{*} x+t^{*})\) be a halfspace and \(y(x)\) be any labeling function such that \((h^{*})=\). Let \(x N(0,I)\) conditioned on \(y(x)=-1\) be a Gaussian example with a negative label. If \(p>C\) for some large enough constant \(C\), then with probability at least \(1/2\) we have \((x) 5/p\) and \(w^{*} x(-t^{*}-1/t^{*},-t^{*})\)._

Finally, we briefly discuss how to obtain a warm start when the threshold \(t^{*}\) is very large. The details of this method can be found in Appendix F. By Theorem 3.8, when \(p\) is small, we are only able to get some \(w_{0}\) such that \(((w_{0},w^{*})) O()\) for \(=/p\). One possible approach is to use the localization technique we use in Section 3.1 to refine such \(w_{0}\). However, such an approach fails because after localization the noise rate would be possibly larger than the length of the Chow-Parameter that we want to estimate. This makes it impossible for us to learn the useful signal. On the other hand,  gave a randomized localization method that can make the expected noise level sufficiently smaller than the length of the Chow-Parameter we want to estimate; and thus will succeed with constant probability in each round of refinement. Unfortunately, such an approach cannot be used in a query-efficient manner, because to implement such a method we need to know \((w_{i},w^{*})\) up to an error \(1/(1/)\), in each round of refinement. This implies that if we make a random guess of \((w_{i},w^{*})\), the probability of success in each round drops to only \(1/(1/)\), which requires to rerun the whole algorithm too many times in order to succeed once.

Such an issue could be addressed in a similar but more complicated way to the method we use in Lemma 3.5, by looking at the bias of the halfspace after localization. The second issue is that even the noise level is smaller than the length of the Chow-Parameter we want to estimate, the length of the Chow-Parameter is only \(1/p^{c}\), for some small constant \(c\), as we can only make \(_{0}\) smaller than some small constant. This still requires us to use \(d/p^{c}\) queries to estimate it. Such an issue can again be addressed using the smoothed label method, where we use only \(1/p^{c}\) queries to search a small class example and use another \((d)\) queries to estimate the Chow-Parameter. Importantly, even such a method only succeeds with constant probability overall. As the refinement stage only runs for \(O((1/))\) rounds, we only need to rerun the entire algorithm \(O((1/))\) times to succeed once.