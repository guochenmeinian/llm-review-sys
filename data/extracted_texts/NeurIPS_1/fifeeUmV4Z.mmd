# Generative Noisy-Label Learning by Implicit Discriminative Approximation with Partial Label Prior

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

The learning with noisy labels has been addressed with both discriminative and generative models. Although discriminative models have dominated the field due to their simpler modeling and more efficient computational training processes, generative models offer a more effective means of disentangling clean and noisy labels and improving the estimation of the label transition matrix. However, generative approaches maximize the joint likelihood of noisy labels and data using a complex formulation that only indirectly optimizes the model of interest associating data and clean labels. Additionally, these approaches rely on generative models that are challenging to train and tend to use uninformative clean label priors. In this paper, we propose a new generative noisy-label learning approach that addresses these three issues. First, we propose a new model optimisation that directly associates data and clean labels. Second, the generative model is implicitly estimated using a discriminative model, eliminating the inefficient training of a generative model. Third, we propose a new informative label prior inspired by partial label learning as supervision signal for noisy label learning. Extensive experiments on several noisy-label benchmarks demonstrate that our generative model provides state-of-the-art results while maintaining a similar computational complexity as discriminative models. _Code will be available once paper is accepted._

## 1 Introduction

Deep neural network (DNN) has achieved remarkable success in computer vision [13; 21], natural language processing (NLP) [10; 51] and medical image analysis [24; 38]. However, DNNs often require massive amount of high-quality annotated data for supervised training , which is challenging and expensive to acquire. To alleviate such problem, some datasets have been annotated via crowdsourcing , from search engines , or with NLP from radiology reports . Although these cheaper annotation processes enable the construction of large-scale datasets, they also introduce noisy labels for model training, resulting in performance degradation. Therefore, novel learning algorithms are required to robustly train DNN models when training sets contain noisy labels.

The main challenge in noisy-label learning is that we only observe the data, represented by random variable \(X\), and respective noisy label, denoted by variable \(\), but we want to estimate the model \(p(Y|X)\), where \(Y\) is the hidden clean label variable. Most methods proposed in the field resort to two discriminative learning strategies: sample selection and noise transition matrix. _Sample selection_[1; 12; 22] optimises the model \(p_{}(Y|X)\), parameterised by \(\), with maximum likelihood optimisation restricted to pseudo-clean training samples, as follows

\[^{*}=_{p(X,)}[(X,) p_{}(|X)],(X=,=})=1,Y=}\\ 0,, \]and \(P(X,)\) is the distribution used to generate the noisy-label and data points for the training set. Note that \(_{P(X,)}[(X,) p_{}( |X)]_{P(X,Y)}[p_{}(Y|X)]\) if the function \((.)\) successfully selects the clean-label training samples. Unfortunately, \((.)\) usually relies on the _small-loss hypothesis_ for selecting \(R\%\) of the smallest loss training samples, which offers little guarantees of successfully selecting clean-label samples. Approaches based on noise _transition matrix_ aim to estimate a clean-label classifier and a label transition, as follows:

\[^{*}=_{}_{P(X,)}[_{Y}p(|X)]=_{_{1},_{2}}_{P(X,)}[ _{Y}p_{_{1}}(|Y,X)p_{_{2}}(Y|X)], \]

where \(=[_{1},_{2}]\), \(p_{_{1}}(|Y,X)\) represents a label-transition matrix, often simplified to be class-independent with \(p_{_{1}}(|Y)=p_{_{1}}(|Y,X)\). Since we do not have access to the label transition matrix, we need to estimate it from the noisy-label training set, which is challenging because of identifiability issues , making necessary the use of anchor point  and regularisations .

On the other hand, generative learning models  assume a generative process for \(X\) and \(Y\), as described in Fig. 1. These methods are trained to maximise the data likelihood \(p(,X)=_{Y,Z}p(X|Y,Z)p(|Y,X)p(Y)p(Z)dYdZ\), where \(Z\) denotes a latent variable representing a low-dimensional representation of the image, and \(Y\) is the latent clean label. This optimisation requires a variational distribution \(q_{}(Y,Z|X)\) to maximise the evidence lower bound (ELBO): with

\[_{1}^{*},_{2}^{*},^{*}=_{_{1},_{2},} _{q_{}(Y,Z|X)}[(p_{_{1}}(X|Y,Z)p_{_{2} }(|X,Y)p(Y)p(Z)/q_{}(Y,Z|X))], \]

where \(p_{_{1}}(X|Y,Z)\) denotes an image generative model, \(p_{_{2}}(|X,Y)\) represents the label transition model, \(p(Z)\) is the latent image representation prior (commonly assumed to a standard normal distribution), and \(p(Y)\) is the clean label prior (usually assumed to be a non-informative prior based on a uniform distribution). Such generative strategy is sensible because it disentangles the true and noisy labels and improves the estimation of the label transition model . A limitation of the generative strategy is that it optimises \(p(,X)\) instead of directly optimising \(p(X|Y)\) or \(p(Y|X)\). Also, compared with the discriminative strategy, the generative approach requires the generative model \(p_{_{1}}(X|Y,Z)\) that is challenging to train. This motivates us to ask the following question: **Can we directly optimise the generative goal \(p(X|Y)\), with a similar computational cost as the discriminative strategy and accounting for an informative prior for the latent clean label \(Y\)?**

In this paper, we propose a new generative noisy-label learning method to directly optimise \(p(X|Y)\) by maximising \(_{q(Y|X)}[ p(X|Y)]\) using a variational posterior distribution \(q(Y|X)\). This objective function is decomposed into three terms: a label-transition model \(_{q(|)}[ p(}|,)]\), an image generative model \(_{q(|)}[|) p()}{q(|)}]\), and a Kullback-Leibler (KL) divergence regularisation term. We implicitly estimate the image generative term with the discriminative model \(q(Y|X)\), bypassing the need to train a generative model. Moreover, our formulation allows the introduction of an instance-wise informative prior \(p(Y)\) inspired by partial-label learning . This prior is re-estimated at each training epoch to cover a small number of label candidates if the model is certain about the training label. Conversely, when the model is uncertain about the training label, then the label prior will cover a large number of label candidates, which also serve as a regularisation of noisy label training. Our formulation only requires a discriminative model and a label transition model, making it computationally less expensive than other generative approaches . Overall, our contributions can be summarized as follows:

* We introduce a new generative framework to handle noisy-label learning by directly optimising \(p(X|Y)\).
* Our generative model is implicitly estimated with a discriminative model, making it computationally more efficient than previous generative approaches .
* Our framework allows us to place an informative instance-wise prior \(p(Y)\) for latent clean label \(Y\). Inspired by partial label learning , \(p(Y)\) is constructed for maintaining high coverage for latent clean label and regularise uncertain sample training.

We conduct extensive experiments on both synthetic and real-world noisy-label benchmarks that show that our method provides state-of-the-art (SOTA) results and enjoy a similar computational complexity as discriminative approaches.

Related Work

**Sample selection.** The discriminative learning strategy based on sample selection from (1) needs to handle two problems: 1) the definition of \((.)\), and 2) what to do with the samples classified as noisy. Most definitions of \((.)\) resort to classify small-loss samples  as pseudo-clean [1; 4; 12; 15; 22; 30; 34; 40]. Other approaches select clean samples based on the K nearest neighbor classification in an intermediate deep learning feature spaces [31; 39], distance to the class-specific eigenvector from the gram matrix eigen-decomposition using intermediate deep learning feature spaces , uncertainty measures , or prediction consistency between teacher and student models . After sample classification, some methods will discard the noisy-label samples for training [4; 15; 30; 34], while others use them for semi-supervised learning . The main issue with this strategy is that it does not try to disentangle the clean and noisy-label from the samples.

**Label transition model.** The discriminative learning strategy based on the label transition model from (2) depends on a reliable estimation of \(p(|Y,X)\)[6; 32; 44]. Forward-T  uses an additional classifier and anchor points from clean-label samples to learn a class-dependent transition matrix. Part-T  estimates an instance-dependent model. MEDITM  uses manifold regularization for estimating the label-transition matrix. In general, the estimation of this label transition matrix is under-constrained, leading to the identifiability problem , which is addressed with the formulation of anchor point , or additional regularisation .

**Generative modelling.** Generative modeling for noisy-label learning [3; 11; 50] explores different graphical models (see Fig. 1) to enable the estimation of clean labels per image. Specifically, CausalNL  and InstanceGM  assume that the latent clean label \(Y\) causes \(X\), and the noisy label \(\) is generated from \(X\) and \(Y\). Alternatively, NPC  assumes that \(X\) causes \(Y\) and proposes a post-processing calibration for noisy label learning. One drawback of generative modeling is that instead of directly optimising the models of interest \(p(X|Y)\) or \(p(Y|X)\), it optimises the joint distribution of visible variables \(p(X,)\). Even though maximising the likelihood of the visible data is sensible, it only produces the models of interest as a by-product of the process. Furthermore, these methods require the computationally complex training of a generative model, and usually rely on non-informative label priors.

**Clean label prior.** Our clean-label prior \(p(Y)\) constrains the clean label to a set of label candidates for a particular training sample. Such label candidates change aims to 1) increase clean label coverage, and 2) represent uncertainty of the prior. Increase coverage improve the chances of including latent clean label in supervision. For noisy samples, increase the number of candidates in \(p(Y)\) regularise noisy label training. Such dynamic prior distribution may resemble Mixup , label smoothing  or re-labeling  techniques that are commonly used in label noise learning. However, these approaches do not simultaneously follow the two design principles mentioned above. Mixup  and label smoothing  are effective approaches for designing soft labels for noisy label learning, but both aim to increase coverage, disregarding label uncertainty. Re-labeling switches the supervisory training signal to a more likely pseudo label, so it is very efficient, but it has limited coverage.

**Partial label learning** In partial label learning (PLL), each image is associated with a candidate label set defined as a partial label . The goal of PLL is to predict the single true label associated with each training sample, assuming that the ground truth label is one of the labels in its candidate set. PICO  uses contrastive learning in an EM optimisation to address PLL. CAV  proposes class activation mapping to identify the true label within the candidate set. PRODEN  progressively identifies the true labels from a candidate set and updates the model parameter. The design of our informative clean label prior \(p(Y)\) is inspired from PLL, but unlike PLL, there is no guarantee that the multiple label candidates in our prior contain the true label. Furthermore, the size of our candidate label set is determined by the probability that the training sample label is clean, where a low probability induces a prior with a large number of candidates for regularising training.

Figure 1: Generative noisy-label learning models and their corresponding optimisation goal, where the red arrow indicates the different causal relationships between \(X\) and \(Y\). Left is CausalNL/InstanceGM [50; 11], middle is NPC  and right is ours.

## 3 Method

We denote the noisy training set as \(=\{(_{i},}_{i})\}_{i=1}^{||}\), where \(_{i}^{H W C}\) is the input image of size \(H W\) with \(C\) colour channels, \(}_{i}\{0,1\}^{||}\) is the observed noisy label. We also have \(\) as the unobserved clean label. We formulate our model with generative model that starts with the sampling of a label \( p(Y)\). This is followed by the clean-label conditioned generation of an image with \( p(X|Y=)\), which are then used to produce the noisy label \(} p(|Y=,X=)\) (hereafter, we omit the variable names to simplify the notation). Below, in Sec. 3.1, we introduce our model and the optimisation goal. In Sec. 3.2 we describe how to construct informative prior, and the overall training algorithm is presented in Sec. 3.3.

### Model

We aim to optimize the generative model \( p(|)\), which can be decomposed as follows:

\[ p(|)=},, )}{p(}|,)p()}. \]

In (4), \(p()\) represents the prior distribution of the latent clean label. The optimisation of \(p(|)\) can be achieved by introducing a variational posterior distribution \(q(|)\), with:

\[ p(|)&= },,)}{q(|)}+ |)}{p(}|,) p()},\\ &_{q(|)}[ p(| )]=_{q(|)}[},,)}{q(|)}]+ q(|)||p(}|, )p(), \]

where \([.]\) denotes the KL divergence, and

\[_{q(|)}[}, ,)}{q(|)}]=_{q(|)}[ p(}|,)]+ _{q(|)}[|) p()}{q(|)}]. \]

Based on Eq. (5) and (6), the expected log likelihood of \(p(|)\) is defined as

\[_{q(|)}[ p(|) ]=_{q(|)}[ p(}| ,)]-[q(|)\|p( |)p()]+[q(| )\|p(}|,)p()]. \]

In Eq. (7), we parameterise \(q(|)\) and \(p(}|,)\) with neural networks, as depicted in Figure 2. The generative model \(p(|)\) usually requires to model infinite number of samples based on conditional label and a generative model is hard to capture such relationship. However, since noisy label learning is a discriminative task and classification performance is our primary goal, the generation can be approximated with with finite training samples, which is given training set. More specifically, we defines \(p(|)\) only on data points \(\{_{i}\}_{i=1}^{||}\) by maximising \(-[q(|)\|p(|)p( )]\) for a fixed \(q(|)\), with the optimum achieved by:

\[p(|)=|)}{_{i=1}^{||}q(|_{i})}. \]

Hence, the generative conditional \(p(|)\) can only represent the values of \(\) within training set given the latent labels in \(\). This allow us transform discriminative model into implicit generative model without additional computation cost.

### Informative prior based on partial label learning

In Eq. (7), the clean label prior \(p()\) is required. As mentioned in Sec. 2, we formulate \(p()\) inspired from PLL . However, it is worth noting that PLL has the partial label information available from the training set, while we have to dynamically build it during training. Therefore, the clean label prior \(p()\) for each training sample is designed so that the hidden clean label has a high probability of being selected during most of the training. On one hand, we aim to have as many label candidates as possible during the training to increase the chances that \(p()\) has a non-zero probability for the latent clean label. On the other hand, including all labels as candidates is a trivial solution that doesnot represent a meaningful clean label prior. These two seemingly contradictory goals target the maximisation of label coverage and minimisation of label uncertainty, defined by:

\[=|}_{i=1}^{||}_{j=1}^{| |}(_{i}(j) p_{i}(j)>0),=|}_{i=1}^{||} (p_{i}(j)>0), \]

where \((.)\) is the indicator function. In (9), coverage increases by approximating \(p(Y)\) to a uniform distribution, but uncertainty is minimised when the clean label \(_{i}\) is assigned maximum probability. In general, training samples for which the model is certain about the clean label, should have \(p(_{i})=1\), while training samples for which the model is uncertain about the clean label, should have \(p(_{i})<1\) with other candidate labels with probability \(>0\). Therefore, the clean label prior is defined by:

\[p_{i}(j)=}_{i}(j)+_{i}(j)+_{i}(j)}{ Z}, \]

where \(Z\) is a normalisation factor to make \(_{j=1}^{||}p_{i}(j)=1\), \(}_{i}\) is the noisy label in the training set, \(_{i}\) denotes the label to increase coverage, and \(_{i}\) represents the label to increase uncertainty, both defined below. Motivated by the early learning phenomenon , where clean labels tend to be fit earlier in the training than the noisy labels, we maximise coverage by sampling from a moving average of model prediction for each training sample \(_{i}\) at iteration \(t\) with:

\[_{i}^{(t)}=_{i}^{(t-1)}+(1-) {}_{i}^{(t)}, \]

where \(\) and \(}^{(t)}\) is the softmax output from the model that predicts the clean label from the data input \(_{i}\). For Eq. (11), \(_{i}^{(t)}\) denotes the categorical distribution of the most likely labels for the \(i^{th}\) training sample, which can be used to sample the one-hot label \(_{i}(_{i}^{(t)})\). The minimisation of uncertainty depends on our ability to detect clean-label and noisy-label samples. For clean samples, \(p(_{i})\) should converge to a one-hot distribution, maintaining the label prior focused on few candidate labels. For noisy samples, \(p(_{i})\) should be close to a uniform distribution to keep a large coverage of candidate labels. To compute the probability \(w_{i}\) that a sample contains clean label, we use the sample selection approaches based on the unsupervised classification of loss values . Then the label \(_{i}\) is obtained by sampling from a uniform distribution of all possible labels proportionally to its probability of representing a noisy-label sample, with

\[_{i}(,(|| (1-w_{i}))), \]

where \((||(1-w_{i}))\) represents the number of samples to be drawn from the uniform distribution rounded up to the closest integer.

### Training

We can now return to the optimisation of Eq. (7), where we define the neural networks \(g_{}:^{||-1}\) that outputs the categorical distribution for the clean label in the probability simplex space \(^{||-1}\) given an image \(\), and \(f_{}:^{||-1}^{| |-1}\) that outputs the categorical distribution for the noisy training label given an image and the clean label distribution from \(g_{}(.)\). The first term in the right-hand side (RHS) in Eq. (7) is optimised with the cross-entropy loss:

\[_{CE}(,,)=| K}_ {(_{i},}_{i})}_{j=1}^{K}_{ CE}(}_{i},f_{}(_{i},}_{i,j})). \]

Figure 2: Training pipeline of our method. Shaded variables \(\) and \(}\) are visible, and unshaded variable \(\) is latent. \(p()\) is constructed to approximate \(\).

where \(\{}_{i,j}\}_{j=1}^{K}(g_{}(_{i}))\), with \((.)\) denoting a categorical distribution. The second term in the RHS in Eq. (7) uses the estimation of \(p(|)\) from Eq. (8) to optimise the KL divergence:

\[_{PRI}(,)=|}_{( _{i},}_{i})}[g_{}(_{i})\|c_{i}(_{i})}{_{j}g_{}( _{j})}_{i}], \]

where \(_{i}=[p_{i}(j=1),...,p_{i}(j=||)]^{||-1}\) is the clean label prior defined in Eq. (10), \(c_{i}\) is a normalisation factor, and \(\) is the element-wise multiplication. The last term in the RHS of Eq. (7) is the KL divergence between \(q(|)\) and \(p(}|,)p()\), which represents the gap between \(_{q(|)}[ p(|)]\) and \(_{q(|)}[}, ,)}{q(|)}]\). According to the expectation-maximisation (EM) derivation [8; 18], the smaller this gap, the better \(q(|)\) approximates the true posterior \(p(|)\), so the loss function associated with this third term is:

\[_{KL}(,,)=|}_{( _{i},}_{i})}[g_{} (_{i})\|f_{}(_{i},g_{}(_{i})) _{i}]. \]

Our final loss to minimise is

\[(,,)=_{CE}(,,)+ _{PRI}(,)+_{KL}(,,). \]

After training, a test image \(\) is associated with a class with \(g_{}()\). An interesting point about this derivation is that the implicit approximation of \(p(|)\) enables the minimisation of the loss in (16) using regular stochastic gradient descent instead of a more computationally complex \(EM\) algorithm .

## 4 Experiments

We show experimental results on instance-dependent synthetic and real-world label noise benchmarks with datasets CIFAR10/100 . We also test on three instance-dependent real-world label noise datasets, namely: Animal-10N , Red Mini-ImageNet , and Clothing1M .

### Datasets

**CIFAR10/100** contain a training set with 50K images and a testing of 10K images of size 32 \(\) 32 \(\) 3, where CIFAR10 has 10 classes and CIFAR100 has 100 classes. We follow previous works  and synthetically generate instance-dependent noise (IDN) with rates in {0.2, 0.3, 0.4,0.5}. **CIFAR10N/CIFAR100N** is proposed by  to study real-world annotations for the original CIFAR10/100 images and we test our framework on {aggre, random1, random2, random3, worse} types of noise on CIFAR10N and {noisy} on CIFAR100N. **Red Mini-ImageNet** is a real-world dataset  containing 100 classes, each containing 600 images from ImageNet, where images are resized to 32 \(\) 32 pixels from the original 84 \(\) 84 to enable a fair comparison with other baselines . **Animal 10N** is a real-world dataset containing 10 animal species with five pairs of similar appearances (wolf and coyote, etc.). The training set size is 50K and testing size is 10K, where we follow the same set up as . **Clothing1M** is a real-world dataset with 100K images and 14 classes. The labels are automatically generated from surrounding text with an estimated noise ratio of 38.5%. The dataset also contains clean samples for training and validation but we only use clean test for measuring model performance.

### Practical considerations

We follow commonly used experiment setups for all benchmarks described in Sec. 4.1. 1 For the hyper-parameter setup, \(K\) in (13) is set to 1, and \(\) in Eq. (11) is set to 0.9. For \(w\) in Eq. (12), we follow the commonly used Gaussian Mixture Model (GMM) unsupervised classification from . For warmup epochs, \(w\) is randomly generated from a uniform distribution. Note that the approximation of the generative model from (8) is done within each batch, not the entire the dataset. Also, the minimisation of \(_{PRI}(.)\) can be done with the reversed KL using \([c_{i}(_{i})}{_{j}g_{ }(_{j})}_{i}\|g_{}(_{i} )]\).

[MISSING_PAGE_FAIL:7]

achieve competitive performance on low noise rates and up to 16% improvements for high noise rates. For CIFAR100, we consistently improve 2% to 4% in all noise rates. Compared with the previous SOTA generative model CausalNL , our improvement is significant for all noise rates. The superior performance of our method indicates that our implicit generative modelling and clean label prior construction is effective when learning with label noise.

**Real-world benchmarks.** In Tab.3, we show the performance of our method on the CIFAR10N/100N benchmark. Compared with other single-model baselines, our method achieves at least 1% improvement on all noise rates on CIFAR10N, and it has a competitive performance on CIFAR100N. The Red Mini-ImageNet results in Tab.4 (left) show that our method achieves SOTA results for all noise rates with 2% improvements using a single model and 6% improvements using the ensemble of two models. The improvement is substantial compared with previous SOTA FaMUS  and DivideMix . In Tab.4(right), our single-model result on Animal-10N achieves 1% improvement with respect to the single-model SELFIE . Considering our approach with an ensemble of two models, we achieve a 1% improvement over the SOTA Nested+Co-teaching . Our ensemble-model result on Clothing1M in Tab.5 shows a competitive performance of 74.4%, which is 2% better than the previous SOTA generative model CausalNL .

### Analysis

**Ablation** The ablation analysis of our method is shown in Tab.6 with the IDN problems on CIFAR10. First row (\(_{CE}\)) shows the results of the training with a cross-entropy loss using the training samples and labels in \(\). The second row (\(_{CE}\) + \(_{CE\_PRI}\) +\(_{KL}\)) shows the result of our method, replacing the KL divergence in \(_{PRI}\) as defined in (14), by a soft version of cross entropy loss. Next, the third row (\(_{CE}\) + \(_{PRI}\) + \(_{KL}\)) shows our method with the loss defined in (16). As mentioned in Sec. 4.2, these two forms provides similar solution where the model and implicit posterior are close and \(_{PRI}\) reverse generally performs better. In the fourth row (\(_{CE}\) + \(_{PRI}\) reversed) by optimising the lower bound to \(_{q(|)}[ p(|)]\) and finally the last row by optimising the whole objective function from (16) in the last row (\(_{CE}\) + \(_{PRI}\) reversed + \(_{KL}\) (Ours)). In general, notice that the reversed \(_{PRI}\) improves the results; the KL divergence in \(_{PRI}\) works better than the CE loss; and the optimisation of the whole loss in (16) is better than optimising the lower bound, which justifies the inclusion of \(_{KL}()\) in the loss.

**Coverage and uncertainty visualisation** We visualise coverage and uncertainty from Eq. (9) at each training epoch for IDN CIFAR10/100 and CIFAR10N setups. In all cases, label coverage increases as training progresses, indicating that our prior tends to always cover the clean label. In fact, coverage reaches nearly 100% for CIFAR10 at 20% IDN and 97% for 50% IDN. Furthermore, for CIFAR100 at 50% IDN, we achieve 82% coverage, and for CIFAR10N "worse", we reach 92% coverage. In terms of uncertainty, we notice a steady reduction as training progresses for all problems, where the uncertainty values tend to be slightly higher for the problems with higher noise rates and more classes. For instance, uncertainty is between 2 and 3 for the for CIFAR10's IDN benchmarks, increasing to be

    &  \\   & 0.2 & 0.4 & 0.6 & 0.8 \\  CE & 47.36 & 42.70 & 37.30 & 29.76 \\ Mixup  & 49.10 & 46.40 & 40.58 & 33.58 \\ DivideMix  & 50.96 & 46.72 & 43.14 & 34.50 \\ MentorMix  & 51.02 & 47.14 & 43.80 & 33.46 \\ FaMUS  & 51.42 & 48.06 & 45.10 & 35.50 \\  Ours & 53.34 & 49.56 & 44.08 & 36.70 \\ Ours ensemble & **57.56** & **52.68** & **47.12** & **39.54** \\   

Table 4: Test accuracy (%) on Red Mini-ImageNet (Left) with different noise rates and baselines from FaMUS , and on Animal-10N (Right), with baselines from . Best results are highlighted.

   CE & Forward  & PTD-R-V  & ELR  & kMEIDTM  & CausalNL  & Our ensemble \\ 
68.94 & 69.84 & 71.67 & 72.87 & 73.34 & 72.24 & **74.35** \\   

Table 5: Test accuracy (%) on the test set of Clothing1M. Results are obtained from their respective papers. We only use the noisy training set for training. Best results are highlighted.

between 2 and 4 for CIFAR10N. For CIFAR100's IDN benchmarks, uncertainty is between 20 and 30. These results suggest that our prior clean label distribution is effective at selecting the correct clean label while reducing the number of label candidates during training.

**Training time comparison** One of the advantages of our approach is its efficient training algorithm, particularly when compared with other generative and discriminative methods. Tab. 7 shows the training time for competing approaches on CIFAR100 with 50% IDN and Clothing1M using the hardware specified in Sec. 4.2. In general, our method has a smaller training time than competing approaches, being \(1.4\) faster than CausalNL , \(3\) faster than DivideMix , and and 13\(\) faster than InstanceGM .

## 5 Conclusion

In this paper, we presented a new learning algorithm to optimise a generative model represented by \(p(X|Y)\) that directly associates data and clean labels instead of maximising the joint data likelihood, denoted by \(p(X,)\). Our optimisation implicitly estimates \(p(X|Y)\) with the discriminative model \(q(Y|X)\) eliminating the inefficient generative model training. Furthermore, we introduce an informative label prior for maintaining high coverage of latent clean label and regularise noisy label training. Results on synthetic and real-world noisy-label benchmarks show that our generative method has SOTA results, but with complexity comparable to discriminative models.

A limitation of the proposed method that needs further exploration is a comprehensive study of the model for \(q(Y|X)\). In fact, the competitive results shown in this paper are obtained from fairly standard models for \(q(Y|X)\) without exploring sophisticated noisy-label learning techniques. In the future, we will use more powerful models for \(q(Y|X)\). Another issue of our model is the difficulty to estimate \(p(X|Y)\) in real-world datasets containing images of high resolution. We will study more adequate ways to approximate \(p(X|Y)\) in such scenario using data augmentation strategies to increase the scale of the dataset.

    &  \\   & \(20\%\) & \(30\%\) & \(40\%\) & \(50\%\) \\  \(_{CE}\) & 86.93 & 82.42 & 76.68 & 58.93 \\ \(_{CE}+_{CE\_ PRI}+_{KL}\) & 85.96 & 82.74 & 78.34 & 73.72 \\ \(_{CE}+_{PRI}+_{KL}\) & 91.36 & 90.88 & 90.25 & 88.77 \\ \(_{CE}+_{PRI}\) reversed & 92.40 & 90.23 & 87.75 & 80.46 \\ \(_{CE}+_{PRI}\) reversed + \(_{KL}\) (Ours) & 92.65 & 91.96 & 91.02 & 89.94 \\   

Table 6: Ablation analysis of our proposed method. Please see text for details.

    & CE & DivideMix  & CausalNL  & InstanceGM  & Ours \\  CIFAR & 2.1h & 7.1h & 3.3h & 30.5h & 2.3h \\ Clothing1M & 4h & 14h & 10h & 43h & 4.5h \\   

Table 7: Running times of various methods on CIFAR100 with 50% IDN and Clothing1M using the hardware listed in Sec. 4.2.

Figure 3: Coverage (Cov) and uncertainty (Unc) for (a) CIFAR10-IDN (20% and 50%), (b) CIFAR100-IDN (20% and 50%), and (c) CIFAR10N (”Worse” and ”Aggre”). Y-axis shows coverage (left) and uncertainty (right). The dotted vertical line indicates the end of warmup training.