# Multimodal Large Language Models Make Text-to-Image Generative Models Align Better

Xun Wu\({}^{1}\), Shaohan Huang\({}^{18}\), Guolong Wang\({}^{2}\), Jing Xiong\({}^{3}\), Furu Wei\({}^{1}\)

\({}^{1}\) Microsoft Research Asia, \({}^{2}\) University of International Business and Economics

\({}^{3}\) The University of Hong Kong

xunwu@microsoft.com, shaohanh@microsoft.com, fuwei@microsoft.com

###### Abstract

Recent studies have demonstrated the exceptional potentials of leveraging human preference datasets to refine text-to-image generative models, making it to generate more human-preferred images. Despite these advances, current human preference datasets are either prohibitively expensive to construct or suffer from a lack of diversity in preference dimensions, resulting in limited applicability for instruction tuning in open-source text-to-image generative models and hinder further exploration. To address these challenges, we first leverage multimodal large language models to create **VisionPrefer**, a fine-grained preference dataset that captures multiple preference aspects (prompt-following, aesthetic, fidelity, and harmlessness). Then we train a corresponding reward model, **VP-Score**, over VisionPrefer to guide the tuning of text-to-image generative models. The preference prediction accuracy of VP-Score is validated to be comparable to that of human annotators. To validate the effectiveness of VisionPrefer and VP-Score, we adopt two reinforcement learning methods, Proximal Policy Optimization (PPO) and Direct Policy Optimization (DPO), to supervised fine-tune generative models, and extensive experimental

Figure 1: **Fine-grained feedback from multimodal large language model help to yield more human-preferred images. Left: Output generated by the baseline text-to-image generative model. Right: Output generated by the baseline model optimized with fine-grained feedback from multimodal large language model. We illustrate improvements in generation quality across four aspects: _Prompt-Following_, _Aesthetic_, _Fidelity_ and _Harmlessness_. See in Appendix for more visualization examples.**results demonstrate that VisionPrefer significantly improves text-image alignment in compositional image generation across diverse aspects, e.g., aesthetic, and generalizes better than previous human-preference metrics across various image distributions. Our findings indicates that the integration of AI-generated synthetic data as a supervisory signal is a promising avenue for achieving improved alignment with human preferences in text-to-image generative models. VisionPrefer and VP-Score are available at [https://github.com/yushuiwx/VisionPrefer.git](https://github.com/yushuiwx/VisionPrefer.git).

## 1 Introduction

Text-to-image generative models [23; 21; 19; 26] have experienced rapid advancements in recent years. For example, large-scale text-to-image diffusion models, exemplified by Imagen  and DALL-E 2 , have demonstrated the capability to generate high-quality and creative images when provided with textual prompts. However, despite recent progress, current generative models still exhibit misalignment with human preferences, such as conflicts with text prompts or incorrect content . A pivotal approach to addressing this issue is utilizing Reinforcement Learning from Human Feedback (RLHF) [16; 27; 1] to supervised fine-tune text-to-image generative models with preference data [13; 3; 5; 17].

Preference data is crucial for aligning generative models with text prompts. However, existing human-crafted preference datasets, such as HPD v2 [33; 32] and Pick-a-Pic , either provide only broad, general preference comparisons without fine-grained, accurate preference evaluations, or they are limited in size. Additionally, using humans for preference annotation is expensive and time-consuming, restricting progress in this research area.

Drawing inspiration from recent research utilizing AI-generated preference data as training supervise signal for Large Language Models (LLMs) alignment on Natural Language Processing domain [12; 2], we pose the following question:

_Can Multimodal Large Language Models act as a Human-Aligned Preference Annotator_

_for Text-to-Image Generation?_

These multimodal large language models (MLLMs), trained on web-scale text and text-image pairs, have already demonstrated formidable capabilities in image understanding. To this end, we first introduce VisionPrefer, a publicly available AI-generated dataset that features millions of finely-grained preferences concerning model-generated images. Compared with existing human preference datasets, VisionPrefer offers the following benefits:

* **Scalability & Low Cost**: As shown in Table 1, VisionPrefer encompasses 1.2 M preference choices across 179 K pairs of images, establishing it as the largest text-to-image generation preference dataset to date. Additionally, because VisionPrefer is annotated by MLLMs, it can be easily expanded further and the construction cost is significantly lower than human annotation.
* **Fine-grained preference**: To more accurately and diversely evaluate the preference scores of generated images, we carefully develop a detailed preference annotation guideline for MLLMs, which covers four distinct aspects: _Prompt-Following_, _Fidelity_, _Aesthetic_, and _Harmlessness_. The detail requirement for each aspect is presented at Table 3.
* **Comprehensive feedback formats**: Unlike existing benchmarks that provide only rankings or preference indices, our VisionPrefer not only provides preference rankings but also includes preference scores and textual explanations for the preference annotations from each aspect, which makes VisionPrefer more versatile, e.g., allowing it to serve as a textual guiding resource for image re-editing and refinement.

Building on the VisionPrefer, we conducted an extensive investigation into its most effective utilization. First, we developed a preference reward model named VP-Score optimized on VisionPrefer, trained to evaluate generated images based on their likelihood of being preferred by humans. Experimental results demonstrate that VP-Score exhibits a competitive correlation with human preferences compared to existing human preference reward models. Moreover, we employ two reinforcement learning methods, Proximal Policy Optimization (PPO) and Direct Policy Optimization (DPO), to enhance generative models to better align with human preferences. As illustrated in Figure 1, VisionPrefer markedly enhances text-image alignment in compositional image generation across diverse aspects, such as aesthetics. In summary, our contributions are as follows:* We construct VisionPrefer, a large-scale, high-quality, and fine-grained preference dataset for text-to-image generative alignment. Compared with existing preference datasets, VisionPrefer has the advantages of scalability, fine-grained annotations, and comprehensive feedback format.
* Based on VisionPrefer, we propose a reward model, VP-Score, which achieves a competitive correlation with human preferences with other automated human preference metrics.
* Experimental results demonstrate the effectiveness of both VisionPrefer and VP-Score. Additionally, we provide a comprehensive analysis of them to gain a deeper understanding of how AI-generated synthetic data and models trained on such data impact future research in this domain.

## 2 Related Work

**Text-to-Image Generative Models Alignment.** While existing text-to-image generative models often generate images that do not closely match human preferences, thus alignment in the context of diffusion has garnered increasing attention [28; 10; 5; 17; 28]. There are two main types of text-to-image generative models alignment algorithms: (i) _Proximal Policy Optimization (PPO)_. For example, reward weighted method  first explores using human feedback to align text-to-image models with human preference. ReFL  trains a reward model, ImageNetward, using human preferences and subsequently utilizes it for fine-tuning. (i) _Direct Policy Optimization (DPO)_. DPOK  fine-tunes text-to-image diffusion models by using policy gradient to maximize the feedback-trained reward. ZO-RankSGD  optimizes diffusion in an online fashion with human ranking feedback. RAFT  and AlignProp  tune the generative model to directly increase the reward of generated images. Several manually annotated preference datasets are proposed to support above algorithms [32; 34; 10]. Their overall statistics are shown in Table 1. These manually annotated data have two drawbacks. First, manual annotation need heavy cost, leading to a small-size set. Second, manual annotations are prone to specific biases .

**Reinforcement Learning from AI Feedback.** introduced the idea of Reinforcement Learning from AI Feedback (RLAIF), which used LLM-labeled preferences in conjunction with human-labeled preferences to jointly optimize for the two objectives of helpfulness and harmlessness. Recent works have also explored related techniques for generating rewards from LLMs [20; 11; 35]. These works demonstrate that LLMs can generate useful signals for reinforcement learning fine-tuning. However, RLAIF for text-to-image generative model alignment is less explored.  leveraged MLLMs to assess the alignment between generated images and input texts, focusing on aspects like object number and spatial relationship. T2I-CompBench  utilized MLLMs like BLIP-VQA to evaluate the text-to-Image generative models. Our work diverges from previous works in two principal ways. (1) Previous approaches provided limited data, sometimes even less than what's annotated manually (e.g., 6K in ). (2) Prior methods had limited aspects in alignment, lacking consideration for aspects such as fidelity .

## 3 VisionPrefer

We introduce VisionPrefer, a fine-grained preference dataset constructed by collecting feedback from MLLMs annotators. The collection pipeline of VisionPrefer is shown in Figure 2, which mainly consists of three steps: prompt generation, image generation and preference generation.

**Step-1: Prompt Generation.** We generate prompts based on DiffusionDB , a large-scale text-to-image prompt benchmark containing 1.5M user-written prompts following two steps: (1) Polish. As discussed in , a significant portion of the prompts in the DiffusionDB is biased towards certain

    & **Corresponding** &  &  & **Preference** & **Open** & **Fine** & **Feedback Format** \\  & **Reward Model** & & & **Cheotes** & **Source?** & **Grained?** & **Ranking** & Text & Scalar \\  RichHF-18K  & \(\) & Human & 18K & 18K & ✗ & ✓ & ✓ & ✗ & ✗ \\ HPD v1  & HPS v1 & Discord users & 25K & 25K & ✓ & ✗ & ✓ & ✗ & ✗ \\ HPD v2  & HPS v2 & Human Expert & 108K & 798K & ✗ & ✓ & ✗ & ✗ \\ ImageRewardDB  & ImageNetward & Human Expert & 9K & 137K & ✓ & ✓ & ✓ & ✗ & ✗ \\ Pick-a-Fe (v2)  & PickScore & Web users & 59K & 851K & ✓ & ✗ & ✓ & ✗ & ✗ \\ VisionPrefer (ours) & VP-Score & GPT-4 V(sion) & **179K** & **1.2M** & – & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Statistics of existing preference datasets for text-to-image generative models. “Fine-grained” denote containing preference regarding multiple aspects or not.

styles (e.g., "Greg Rutkowski" appears in around 15% of prompts). We utilize GPT-4 to polish prompts in DiffusionDB to obtain unbiased prompts following instructions shown in Appendix F.1. (2) NSFW Filting. We employ state-of-the-art NSFW detectors  to compute an NSFW score for each prompt and filter out prompts that exceed a certain threshold, following . After these two steps, we combine both polished prompts and the original prompts in DiffusionDB as our final prompt benchmark, which contains 179K prompts.

**Step-2: Image Generation.** We generate images using different text-to-image generative models (see details in Appendix C.2) by sampling textual prompts constructed in Step-1 as input. For each prompt, we generate four images by randomly selecting different generative models from the model pools with different classifier-free guidance scale values, to ensure high diversity. This diversity allows for a comprehensive evaluation of a preference prediction model's generalization capability and facilitates the training of a more generalizable model. Finally, we obtain 0.716M images.

**Step-3: Preference Generation.** We employ state-of-the-art multimodal large language model, GPT-4 V, to provide three types of feedback: (1) _Scalar scores_ that indicate the fine-grained quality regarding multiple aspects, (2) _Preference ranking_ according to the scalar scores, and (3) _Textual explanations_ that give detailed guidance on how to improve the completion, encompassing four distinct fine-grained aspects namely: _Prompt-Following_, _Aesthetic_, _Fidelity_, and _Harmlessness_ for each generated image (See the example in Table 12). Detailed input instructions for GPT-4 V to annotate preference labels are in Appendix F.2. Finally, we obtain 1.2M preference choices.

## 4 Experiments

In this section, we first train a corresponding reward model named VP-Score and evaluate it on existing human-preference datasets (SS 4.1). Next, we enhance existing text-to-image generative models by adopting two reinforcement learning algorithms (SS 4.2) to validate the efficacy of VisionPrefer and VP-Score. After that, we design a simple pipeline to edit generated images with the textual explanations in VisionPrefer(SS 4.3).

### Reward Modeling

**Training Setting.** We train the VP-Score over VisionPrefer. VP-Score adopts the same model structure as ImageReward , which is a open-source human-preference reward model and utilizes BLIP  as the backbone. Similarly to training the reward model for the language model , we formulate the preference annotations in VisionPrefer as rankings. Specifically, we employ the average scores of each sample in VisionPrefer across four aspects as the final preference score, and then we have \(k\) images ranked generated by the same prompt \(\) according to final preference score (the best to the worst are denoted as \(_{1}_{2}..._{k}\)). For each comparison, if \(_{i}\) is better and \(_{j}\) is worse, the loss function can be formulated as:

\[()=-_{(,_{i},_{j}) }[((f_{}(, _{i})-f_{}(,_{j})) )] \]

where \(f_{}(,)\) is a scalar value of reward model for prompt \(\) and image \(\).

**Evaluation Results.** We evaluate the preference prediction accuracy on the test sets among three human preference datasets: ImageRewardDB , HPD v2  and Pick-a-Pic . Furthermore,

Figure 2: VisionPrefer construction pipeline. We sample textual prompts and text-to-image generative models from pools to generate diverse comparison data, then query GPT-4 V with detailed illustrations for fine-grained and high-quality annotations in both textual and numerical formats.

to better demonstrate the model's generalization performance, we computed the harmonic mean of accuracy across three sets for each model as an overall indicator of model performance. We use the CLIP score , BLIP score , Aesthetic score , ImageReward , HPS , HPS v2  and PickScore  as baselines to compare with the VP-Score.

The results are presented at Table 2. Our VP-Score demonstrates strong competitiveness compared to the current state-of-the-art reward models trained on human preference data. It achieves the second-best average performance among all preference reward models, following only HPS v2. Moreover, our model achieves optimal performance on the ImageRewardDB dataset, achieving a 0.6 performance gain compared to HPS v2. These results validate that leveraging fine-grained feedback provided by MLLMs enables learning a proficient human preference reward model.

### Fine-tuning Text-to-Image Generative Models

We aim to leverage the constructed preference dataset to align the performance of generative models more closely with human preferences. We utilize two popular reinforcement learning methods for fine-tuning: (1) Proximal Policy Optimization (PPO), where select ReFL  as our PPO implementation to adjust the generative model. (2) Direct Preference Optimization (DPO), which allows for direct model fine-tuning using preference data without a reward model, employing D3PO  as our DPO implementation. We use Stable Diffusion v1.5  as the text-to-image generative model.

**Training Setting.** For PPO experiments, we randomly sample 20,000 real user prompts from DiffusionDB  and 10,000 prompts in ImageRewardDB  as the training dataset. We compare VP-Score against five open-source reward models, including ImageReward , PickScore , and HPS v2 , all trained on large-scale preference datasets (see Table 1). All models are fine-tuned with identical data and settings for consistency. For DPO experiments, we compare our VisionPrefer along with three open-source large-scale human-annotated preference datasets, ImageRewardDB , HPD  and Pick-a-Pic  (see in Table 1). Notably, VisionPrefer scores are averaged across four aspects for fine-tuning, with an analysis on individual aspect scores presented in SS 5. Both PPO and DPO use the same test benchmarks: 400 real user prompts from DiffusionDB , 200 prompts from ImageRewardDB  and 400 prompts from HPD v2 . Further details are in Appendix D.

**PPO Results.** First, we visualize the evolution of various metrics as the model training steps increase when using our VP-Score as the reward function at Figure 3. As training progresses, all metrics, including human preference metrics like HPS v2, show an increasing trend. This indicates consistency

   Model & ImageRewardDB & HPD v2 & Pick-a-Pic & Average \\  CLIP ViT-H/14  & 57.1 & 65.1 & 60.8 & 60.82 \\ Aesthetic  & 57.4 & 76.8 & 56.8 & 62.44 \\ ImageReward  & 65.1 & 74.0 & 61.1 & 66.31 \\ HPS  & 61.2 & 77.6 & 66.7 & 67.84 \\ PPickScore  & 62.9 & 79.8 & **70.5** & 70.40 \\ HPS v2  & 65.7 & **83.3** & 67.4 & **71.32** \\ VP-Score & **66.3** & 79.4 & 67.1 & 20.46 \\   

Table 2: Preference prediction accuracy across the test sets of ImageRewardDB, HPD v2 and Pick-a-Pic. The Aesthetic Classifier makes prediction without seeing the text prompt. The best performance is in bold, and the second-best performance is underlined.

Figure 4: Win rates of generative models optimized with VP-Score compared to other reward models on three test benchmarks for PPO experiments. ‘Tie’ indicates instances where annotators think two images are of comparable quality.

Figure 3: Evaluation results of the text-to-image model’s generation quality across multiple reward models when maximizing scores from VP-Score during the PPO training process. All scores are normalized for a better visualization.

between our VP-Score and other human preference metrics, demonstrating that VP-Score reliably aligns generative model outputs with human preferences.

Then, we conduct a human preference study. Specifically, we use these fine-tuned generative models to generate 64 images for each prompt in evaluation dataset, followed by a top-3 selection by the corresponding reward models. Finally, ten human annotators rank these selected images. The results are presented at Figure 4, detailed win count and win rates can be found in Table 4. We observed that VP-Score fine-tuned generative model's Win+Tie ratio exceeds 50% when compared to all other models across all three test benchmarks, including some trained on large-scale human preference datasets like HPS v2. This suggests that, compared to other human preference reward model, VP-Score can serve as a reliable and competitive reward model for fine-tuning generative models to produce outputs closer to human preferences. This further underscores the effectiveness and competitiveness of VisionPrefer.

The corresponding qualitative results shown at Figure 5 demonstrate that VisionPrefer fine-tuned generative model can generate images that are more aligned to text and with higher fidelity and avoid toxic contents. More qualitative results can be found in Figure 13.

**DPO Results.** We conduct a human preference study using the same procedure as PPO experiments at Figure 6, detailed win count and win rates can be found in Table 5. We found that the Win+Tie ratio of the generative model optimized on our VisionPrefer, when compared to the other three large-scale human datasets, exceeds 50%, substantiating the competitiveness of our VisionPrefer against human-annotated preference data. We show the qualitative results in Figure 7. The results indicate that fine-tuning the generative model directly on our VisionPrefer using DPO yields performance comparable to that of fine-tuning the generative model on large-scale human-annotated preference dataset (e.g., Pick-a-Pick). Specifically, the generated results are more aligned with human preferences, exhibiting increased visual detail, better conformity to input prompts. More qualitative results can be found in Figure 14. These experimental outcomes collectively affirm the efficacy of using preference data generated by MLLMs.

### Editing Generated Images with VisionPrefer

Unlike existing preference datasets, our VisionPrefer not only provides preference rankings and scores for images but also includes corresponding textual explanations. This makes VisionPrefer

Figure 5: Qualitative results for PPO experiments. SD 1.5 denotes the Stable Diffusion v1.5 model without any fine-tune. See Appendix for more samples.

Figure 6: Win rates of generative models optimized with VP-Score compared to other reward models on three test benchmarks for DPO experiments. ‘Tie’ indicates instances where annotators think two images are of comparable quality.

more versatile, enabling applications such as editing images based on textual explanations to better align with human preferences.

We design an image editing pipeline as in Figure 8, which contains two main steps: (1) Integrate the textual explanations from VisionPrefer into a specific prompt template and input it into LLMs (e.g., GPT-4), encouraging the LLMs to output one or more concise editing instructions to address the issues raised in the textual explanations. (2) Input these generated editing instructions into an image editing model (e.g., InstructPix2Pix ), guiding it to edit images and address issues. This pipeline is simple in structure and can perform better by substituting more advanced LLMs and editing models.

**Training Setting.** To explore the effectiveness of the textual explanations provided by VisionPrefer, we randomly selected 200 text-image pairs with scores below 3 (out of a maximum score of 5) as test cases. Following the outlined process, we performed image edits and evaluate the results before and after editing using existing reward models and conducted human studies with 10 participants.

**Results.** The evaluation results for images before and after editing are summarized in Figure 9. We found that both the reward models and human studies indicate a Win ratio > 50%, demonstrating that the edited images are superior to the original ones. This also validates the effectiveness of the textual explanations provided by VisionPrefer and the design of our pipeline.

## 5 Analysis

**Which MLLMs is the Best Annotator?**

The annotation of VisionPrefer heavily relies on GPT-4 V. Although many researchers pointed out that GPT-4 V capable of providing meticulous judgments and feedback , we still concern whether the GPT-4 V preferences are qualified. We then conduct a probing experiment by utilizing different MLLMs, GPT-4 V, Gemini-pro-V and LLaVA 1.6-34B, to provide their preference on two existing human-preference datasets (HPD  and ImageRewardDB ). The corresponding pair-wise preference prediction accuracy is shown in Figure 10 (a). We observed that the accuracy

Figure 8: Overview of the proposed image editing pipeline with the textual explanations in VisionPrefer.

Figure 7: Qualitative results for DPO experiments. SD 1.5 denotes the Stable Diffusion v1.5 model without any fine-tune. See Appendix for more samples.

of GPT-4 V surpasses that of both LLaVA 1.6-34B and Gemini-pro-V on both datasets, achieving accuracy rates exceeding or approaching 70%, and LLaVA 1.6-34B notably scoring significantly lower than the former two. According to previous research , the agreement rate between qualified human annotators is also around 70% (65.3% for ImageRewardDB and 78.1% for HPD). Therefore, the probing experiment validates that GPT-4 V can be a well human-aligned annotator, thus ensure the quality and reliability of our VisionPrefer.

To further validate the efficacy on preference annotation ability of GPT-4 V, we utilize Gemini-pro-V and LLaVA 1.6-34B to collect similar amount of data (1.2 M pair-wise preference choices) following the same collection pipeline described in Section 3. Then we train the corresponding reward model on these two datasets and show the performance in Figure 10 (b). As we can see, consistent with the aforementioned conclusion, the testing accuracy of the reward model trained on data annotated by GPT-4 V exhibits the highest performance, followed by Gemini-pro-V. This demonstrates that GPT-4 V is currently the most proficient annotator for text-to-image generation.

**Encouraging GPT-4 V(ision) for Enhanced Annotations.**

* **Prompt Manner.** As described in Section 3, during the construction of VisionPrefer, we encourage GPT-4 V to directly output scores for various aspects (e.g., aesthetic) of each image (denoted as score feedback). Another straightforward prompting manner (denoted as rank feedback) is encourage GPT-4 V to directly provide a ranking of images (\(\) and \(\)) in a certain aspect (i.e., \(\), \(\), or \(=\)). It is interesting to explore which prompting manner is best suit for AI Annotators. We randomly sampled 1,000 samples from ImageRewardDB and 500 samples from HPD, and utilized the two aforementioned prompt manners to ascertain GPT-4 V's annotations. The results are presented at Figure 10 (c), we observe that in both datasets, the accuracy achieved using score feedback is higher than that achieved using rank feedback.
* **Temperature \(\).** Temperature \(\) is a hyperparameter used in multimodal large language models (e.g., GPT-4 V) to control the randomness and creativity of the generated results. A lower value of the temperature parameter will lead to a more predictable and deterministic output, while a higher value will produce a more random and surprising output. We investigate the influence of the variation in \(\) on both the accuracy of annotation and annotation consistency (where the same input yields identical annotation results). The results are shown in Figure 10 (d), we observe a decrease in accuracy as \(\) increases, indicating that lower values of \(\) should be set when conducting preference annotations. Furthermore, as \(\) increases, annotation consistency continues to decline, which is sensible because more randomness in the results leads to different preference outcomes for identical inputs over time.

**Fine-Grained Feedback Leads to Better Results.**

* **Better Reward Modeling.** In our previous experiments, we used the average score of each sample across four evaluation aspects as the final preference score for modeling reward or optimizing generation models. Here, we explore the impact of separately modeling the four evaluation aspects. We first train four reward models on these four different aspect in VisionPrefer, namely VP-Score-P,

Figure 10: (a) Pair-wise preference prediction accuracy comparison across three MLLMs on two human-preference datasets (b) Results of preference prediction accuracy for reward models trained on preference datasets annotated by different MLLMs annotators. (c) Preference prediction accuracy for score feedback and ranking feedback. (d) Visualization depicting the variation of annotation accuracy and consistency with changes in temperature \(\). (e) Preference prediction accuracy among reward models trained on different aspects of preference data in VisionPrefer.

VP-Score-A, VP-Score-F and VP-Score-H, respectively. The corresponding preference accuracy are presented at Figure 10(e). We can observe that the accuracy of reward models individually trained using a single aspect preference data is consistently lower than VP-Score, which validates the effectiveness of our approach in designing four evaluation aspects to model the preference level.
* **Better Prompt-Following.** We found fine-grained preference data enables our fine-tuned model to generate images that better adhere to the input prompt. For instance, as shown in Figure 11, the top-3 sampled images generated from our fine-tuned model all satisfy the "holding" prompt requirement, whereas only HPS v2 among baseline models achieves this.
* **More Aesthetically Pleasing.** Fine-grained data enhances the visual appeal and vividness of images generated by our model. As shown in Figure 12, our results exhibit enhanced luminosity, dynamic sensation, and increased detail, aligning more closely with human aesthetic preferences.
* **Enhance Image Safety.**Using unsafe prompts from , we generated 1K images and assessed safety using the Diffusion library's NSFW detector. The NSFW ratio for models fine-tuned with VP-Score (4.4%) was substantially lower compared to HPS v2 (21.1%) and PickScore (22.3%) fine-tuned models, indicating our preference scoring's effectiveness in reducing harmful content generation.

**More related details and ablation studies about the effectiveness of fine-grained feedback can be found in Appendix B**.

## 6 Conclusion

In this paper, we explore utilizing MLLMs to construct a large-scale high-quality feedback dataset, VisionPrefer, for diffusion models alignment and refining. Costly experiments conducted across various experimental settings have validated the efficacy of VisionPrefer. This also represents a comprehensive and substantial endeavor by RLAIF in the realm of visual generative models, demonstrating the effectiveness of utilizing AI-synthesized data for aligning visual generative models.

Figure 11: Fine-grained feedback enables our model (denoted as VP-Score) to generate results that better align with the input prompt. See Appendix for more samples.

Figure 12: Fine-grained feedback enhances the aesthetic and vividness of the our results (denoted as VP-Score). SD 1.5 denotes the Stable Diffusion v1.5 model without any fine-tune. See Appendix for more samples.