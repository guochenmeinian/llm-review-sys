# Provably and Practically Efficient Adversarial

Imitation Learning with General Function Approximation

Tian Xu\({}^{*}\)

Equal contribution. Emails: xut@lamda.nju.edu.cn and zhangzl@lamda.nju.edu.cn.

Zhilong Zhang\({}^{*}\)

Jun-Yan Liu\({}^{*}\)

\({}^{1}\)National Key Laboratory for Novel Software Technology, Nanjing University, China

\({}^{2}\)School of Artificial Intelligence, Nanjing University, China

\({}^{3}\)Polixir.ai

###### Abstract

As a prominent category of imitation learning methods, adversarial imitation learning (AIL) has garnered significant practical success powered by neural network approximation. However, existing theoretical studies on AIL are primarily limited to simplified scenarios such as tabular and linear function approximation and involve complex algorithmic designs that hinder practical implementation, highlighting a gap between theory and practice. In this paper, we explore the theoretical underpinnings of online AIL with general function approximation. We introduce a new method called optimization-based AIL (OPT-AIL), which centers on performing online optimization for reward functions and optimism-regularized Bellman error minimization for Q-value functions. Theoretically, we prove that OPT-AIL achieves polynomial expert sample complexity and interaction complexity for learning near-expert policies. To our best knowledge, OPT-AIL is the first provably efficient AIL method with general function approximation. Practically, OPT-AIL only requires the approximate optimization of two objectives, thereby facilitating practical implementation. Empirical studies demonstrate that OPT-AIL outperforms previous state-of-the-art deep AIL methods in several challenging tasks. 1

## 1 Introduction

Sequential decision-making tasks are prevalent in real-world applications, where agents seek policies that maximize long-term returns. Reinforcement learning (RL)  provides a well-known framework for developing effective policies through trial and error. However, RL often necessitates carefully designed reward functions and typically requires millions of interactions with the environment to achieve satisfactory performance . In contrast, imitation learning (IL) offers a more sample-efficient approach to learning effective policies by mimicking expert demonstrations, bypassing the need for explicit reward functions. As a result, IL has gained popularity and demonstrated success in a wide range of real-world applications such as recommendation systems  and generalist robot learning .

IL encompasses two main categories of methods: behavioral cloning (BC) and adversarial imitation learning (AIL). BC employs supervised learning to directly infer expert policies from demonstrationdata . In contrast, AIL utilizes an adversarial learning process to replicate the expert's state-action distribution. This process involves the learner recovering an adversarial reward to maximize the policy value gap and subsequently learning a policy that minimizes this gap under the recovered reward. Building on these foundational principles, numerous practical algorithms have been developed , achieving significant empirical advancements.

From these empirical advances, a notable observation is that AIL often significantly outperforms BC . To better understand this phenomenon, recent research has focused on the theoretical underpinnings of AIL , particularly in the online setting. This research examines both _expert sample complexity_ (the number of expert trajectories required) and _interaction complexity_ (the number of trajectories needed when interacting with the environment), both of which are crucial for practical applications. In the tabular setting, the best-known complexity result is achieved in . They developed the MB-TAIL algorithm, which leverages advanced distribution estimation, achieving the expert sample complexity \(}(H^{3/2}||/)\) and interaction complexity \(}(H^{3}||^{2}||/^{2})\), where \(||\) and \(||\) are the state space size and action space size, respectively, \(H\) is the horizon length and \(\) is the desired value gap. Furthermore,  investigated the AIL theory in the linear function approximation setting. Notably, the BRIG approach proposed in  uses linear regression for policy evaluation and achieves the expert sample complexity \(}(H^{2}d/^{2})\) and interaction complexity \(}(H^{4}d^{3}/^{2})\), where \(d\) is the feature dimension. For a complete summary of related results, please refer to Table 1.

Despite substantial theoretical advances, there still exists a gap between theory and practice in AIL. First, prior theoretical analysis primarily focuses on restricted settings such as tabular  or linear function approximation , which deviate from practice where AIL approaches often operate with general function approximation (e.g., neural network approximation). Besides, most previous theoretical works involve algorithmic designs such as count-based  or covariance-matrix-based  bonuses, which are tailored to their respective settings. Implementing such algorithmic designs in practical settings, where neural network approximation is employed, presents significant challenges .

**Contribution.** This paper aims to bridge the gap between theory and practice in AIL by developing a provably efficient algorithm with general function approximation and providing a practical implementation equipped with neural networks.

First, we introduce a new AIL approach called optimization-based adversarial imitation learning (OPT-AIL) and provide a comprehensive theoretical analysis for general function approximation. The core of OPT-AIL involves minimizing two key objectives. To recover the reward, OPT-AIL solves an online optimization problem using a no-regret approach. For policy learning, inspired by , OPT-AIL infers the Q-value functions by minimizing the optimism-regularized Bellman

   Setting & Algorithm & Expert Sample & Interaction \\  & & Complexity & Complexity \\  General Function & BC 3  & \(}((_{[]}| _{}|)}{^{2}})\) & \(0\) \\  Tabular MDPs & OAL  & \(}(||}{^{2}})\) & \(}(||^{2}||}{ ^{2}})\) \\  Tabular MDPs & MB-TAIL  & \(}(||}{^{2}})\) & \(}(||^{2}||}{ ^{2}})\) \\  Linear Mixture MDPs & OGAIL  & \(}(d^{2}}{^{2}})\) & \(}(d^{2}}{^{2}})\) \\  Linear MDPs & BRIG  & \(}(d}{^{2}})\) & \(}(d^{2}}{^{2}})\) \\  General Function & OPT-AIL & \(}((_{[]}| (_{}))}{^{2}})\) & \(}(d_{}(_{ []}|(_{})(_{ }))+H^{2}}{^{2}})\) \\   

Table 1: A summary of the expert sample complexity and interaction complexity. Here \(H\) is the horizon length, \(\) is the desired imitation gap, \(||\) is the state space size, \(||\) is the action space size, \(||\) is the cardinality of the finite policy class \(\), \(d\) is the dimension of the feature space, \(d_{}\) is the generalized eluder coefficient, \((_{h})\) and \((_{h})\) are the covering numbers of the reward class \(_{h}\) and Q-value class \(_{h}\), respectively. We use \(}\) to hide logarithmic factors.4error and then derives the corresponding greedy policies. Under mild assumptions, we prove that OPT-AIL achieves the expert sample complexity \(}(H^{2}(_{h[H]}(_{h}))/ ^{2})\) and interaction complexity \(}((H^{4}d_{}(_{h[H]}( _{h})(_{h}))+H^{2})/^{2})\). Here \(d_{}\) is the generalized eluder coefficient, originally proposed in  to measure the complexity of RL with function approximation, which we adapt to the AIL setting. \((_{h})\) and \((_{h})\) are the covering numbers of the reward class \(_{h}\) and Q-value class \(_{h}\), respectively. To our best knowledge, OPT-AIL is the first provably efficient AIL approach with general function approximation.

Furthermore, we offer a practical implementation of OPT-AIL, demonstrating its competitive performance on standard benchmarks. Notably, OPT-AIL only requires the approximate optimization of two objectives, simplifying its practical implementation with deep neural networks. Leveraging this advantage, we implement OPT-AIL using neural network approximations and compare its performance against prior state-of-the-art (SOTA) deep AIL methods, which often lack theoretical guarantees. Experimental results indicate that OPT-AIL outperforms SOTA deep AIL approaches on several challenging tasks within the DMControl benchmark.

## 2 Related Works

Adversarial Imitation Learning.The theoretical foundations of AIL have been extensively explored in numerous studies [1; 52; 48; 60; 67; 41; 45; 33; 40; 63; 50; 56; 64; 57]. Early research [1; 52; 48; 41; 60; 67; 62; 50; 56] focused on ideal settings where the transition function is known or an exploratory data distribution is available, primarily addressing expert sample efficiency. Notably, under mild conditions,  proved that AIL can achieve a horizon-free imitation gap bound \((\{1,|/N}\})\), where \(N\) denotes the number of expert trajectories. Recently, a new research direction has emerged that addresses more practical scenarios, specifically online AIL with unknown transitions [45; 33; 64; 57]. This line of work investigates both expert sample complexity and interaction complexity. These recent advancements were discussed in the previous section and thus will not be reiterated here. Most existing theoretical works focus on either tabular [41; 45; 64] or linear function approximation settings [33; 57], and often lack practical implementations due to algorithmic designs tailored to specific settings. In contrast, this work simultaneously provides theoretical guarantees for general function approximation and offers a practical implementation that demonstrates competitive performance.

On the empirical side, there has been extensive research [19; 27; 28; 16; 26; 15] developing practical AIL approaches that leverage general function (or neural network) approximation. A seminal method in this field is generative adversarial imitation learning (GAIL) . In GAIL, a discriminator is trained to distinguish between samples from expert demonstrations and those generated by a policy, while the policy (or generator) learns to maximize the reward signal provided by the discriminator. More recently,  proposed inverse Q-Learning (IQLearn), which achieves SOTA performance across a diverse set of tasks. However, these practical methods often lack rigorous theoretical guarantees for general function approximation.

General Function Approximation in Reinforcement Learning.Our work is closely related to a body of research focused on general function approximation in RL [38; 21; 59; 23; 32]. Notably,  proposed an algorithmic framework that incorporates a unified objective to balance exploration and exploitation in RL, demonstrating a sublinear regret bound. In this paper, we adapt this algorithmic design to address several RL sub-problems within the context of AIL. Unlike the RL setting, where a fixed reward is provided in advance, AIL involves inferring the reward function from expert demonstrations and environment interactions collected by the learning policies. Therefore, our work requires developing a theoretical analysis for the joint learning process of both rewards and policies, highlighting a unique challenge in AIL compared to traditional RL.

## 3 Preliminaries

Markov Decision Process.In this paper, we consider episodic Markov Decision Processes (MDPs), represented by the tuple \(=(,,P,r^{},H,s_{1})\). Here, \(\) and \(\) denote the state and action spaces, respectively. \(H\) signifies the planning horizon, while \(s_{1}\) stands for the fixed initial state. The set \(P=\{P_{1},,P_{H}\}\) characterizes the non-stationary transition function of this MDP. Specifically, \(P_{h}(s_{h+1}|s_{h},a_{h})\) determines the probability of transiting to state \(s_{h+1}\) given state \(s_{h}\) and action \(a_{h}\) at time step \(h\), where \(h[H]\). Similarly, \(r^{}=\{r^{}_{h},,r^{}_{H}\}\) outlines the reward function of this MDP. Without loss of generality, we assume \(r^{}_{h}:\) for \(h[H]\). A non-stationarypolicy is denoted by \(=\{_{1},,_{H}\}\) with \(_{h}:()\), where \(()\) denotes the probability simplex. Here, \(_{h}(a|s)\) represents the probability of selecting action \(a\) in state \(s\) at time step \(h\), for \(h[H]\).

The quality of policy \(\) is evaluated by policy value:

\[V^{}=[_{h=1}^{H}r_{h}^{}(s_{h},a_{h}) a_{h}_{h}(|s_{h}),s_{h+1} P_{h}(|s_{h},a_{h}), h [H]].\]

We denote the Q-value function of policy \(\) at time step \(h\) as \(Q_{h}^{}:\), where \(Q_{h}^{}(s,a)=_{}[_{=h}^{H}r_{}^{}(s_ {},a_{})|s_{h}=s,a_{h}=a]\). The optimal Q-value function \(Q_{h}^{}:\) is defined as \(Q_{h}^{}(s,a):=_{}Q_{h}^{}(s,a)\). It is known that \(Q_{h}^{}\) is the fixed point of Bellman operator \(_{h}\): \(Q_{h}^{}(s,a)=(_{h}Q_{h+1}^{})(s,a):=r_{h}^{ }(s,a)+_{s^{} P_{h}(|s,a)}[_{a^{}}Q_{h+1}^{}(s^{},a^{})]\). In other words, \(Q^{}\) has zero Bellman error, i.e., \(Q_{h}^{}(s,a)-(_{h}Q_{h+1}^{})(s,a)=0\).

**Imitation Learning.** The essence of IL lies in acquiring a high-quality policy _without_ the reward function \(r^{}\). In pursuit of this objective, we typically posit the existence of a near-optimal expert policy \(^{}\) capable of interacting with the environment to generate a dataset, comprising \(N\) trajectories each of length \(H\): \(^{}=\{=(s_{1},a_{1},s_{2},a_{2},,s_{H},a_{H}) :a_{h}_{h}^{}(|s_{h}),s_{h+1} P_{h}(|s_{h},a_{h }), h[H]\}\). Subsequently, the learner leverages this dataset \(^{}\) to mimic the behavior of the expert and thereby derives an effective policy. The quality of this imitation is measured by the _imitation gap_: \(V^{^{}}-V^{}\), where \(\) represents the learned policy. Essentially, we hope that the learned policy can perfectly mimic the expert such that the imitation gap is small.

AIL is a prominent class of IL methods that imitates expert behavior through an adversarial learning process defined by \(_{}_{r}V_{r}^{^{}}-V_{r}^{}\), where \(V_{r}^{}\) denotes the value of policy \(\) under reward \(r\). In this framework, AIL infers a reward function that maximizes the value gap between the expert policy and the learning policy. Subsequently, it learns a policy that minimizes this value gap using the inferred reward. Essentially, AIL involves solving several RL sub-problems, as the outer optimization problem concerning the policy is equivalent to an RL problem under the inferred reward \(r\).

**General Function Approximation.** This work considers AIL with general function approximation. In this setup, the learner first has access to a reward class \(=_{1}_{2} _{H}\) with \( h[H],_{h}( )\) to infer the reward. We assume that \(\) captures the unknown true reward.

**Assumption 1** (Realizability of \(\)).: _The unknown true reward lies in the reward class, i.e., \(r^{}\)._

Besides, the learner also has access to a Q-value function class \(=_{1}_{2} _{H}\) with \( h[H],_{h}( [0,H])\), which is used for solving several RL sub-problems under different inferred rewards in AIL. Since there is no reward in the \(H+1\) step, we always set \(Q_{H+1} 0\). Below, we present two standard assumptions about the function class \(\) that are commonly adopted in the literature of RL with function approximation .

**Assumption 2** (Realizability of \(\)).: _For reward \(r\), \(Q^{,r}\), where \(Q^{,r}\) denotes the optimal Q-value function under reward \(r\)._

**Assumption 3** (Bellman Completeness of \(\)).: _For reward \(r\), \(_{h}^{r}_{h+1}_{h},\  h[H]\), where \(_{h}^{r}\) denotes the Bellman operator under reward \(r\) and \(_{h}^{r}_{h+1}=\{_{h}^{r}Q_{h+1}:Q_{h+1} _{h+1}\}\)._

In short, Assumption 2 states that the Q-value class \(\) should capture the optimal Q-value function, while Assumption 3 indicates the closeness of \(\) under Bellman update. It is easy to verify that Assumptions 1, 2 and 3 are more general than the tabular MDP , linear mixture MDP  and linear MDP  assumptions used in previous works.

When the function class contains a finite number of elements, its cardinality can be used to quantify its "size". However, for general function approximation, where the function class may contain an infinite number of elements, we utilize the standard \(\)-covering number  to measure its complexity.

**Definition 1** (\(\)-covering number).: _For function class \(()\), the \(\)-covering number of \(\), denoted as \(_{}()\), is defined as the minimum integer \(n\) such that there exists a finite subset \(^{}\) with \(|^{}|=n\) such that for any function \(f\), there exists \(f^{}^{}\) satisfying that \(_{x}|f(x)-f^{}(x)|\)._Optimization-based Adversarial Imitation Learning

In this section, we introduce a provably efficient method called Optimization-Based Adversarial Imitation Learning (OPT-AIL). In Section 4.1, we delve into the core components of OPT-AIL, which involves online optimization for reward functions and optimism-regularized Bellman error minimization for Q-value functions. We discuss the underlying principles and provide theoretical guarantees with general function approximation. Thanks to its easy-to-implement merit, we provide a practical implementation of OPT-AIL using stochastic-gradient-based methods in Section 4.2.

### Theoretical Analysis of OPT-AIL

In this section, we present our provably efficient method OPT-AIL with general function approximation; see Algorithm 1 for an overview. To start with, recall that our theoretical goal is to ensure the algorithm can output a policy with \(\)-imitation gap by using finite expert samples and environment interactions. To obtain the final policy, we leverage the standard online-to-batch conversion technique . Specifically, during the learning process, the learner iteratively generates a sequence of rewards \(\{r^{k}\}_{k=1}^{K}\) and policies \(\{^{k}\}_{k=1}^{K}\), and outputs the policy \(\) that is uniformly sampled from \(\{^{k}\}_{k=1}^{K}\). To analyze the imitation gap of \(\), we leverage the following standard error decomposition lemma.

**Lemma 1**.: _Consider a sequence of rewards \(\{r^{k}\}_{k=1}^{K}\) and policies \(\{^{k}\}_{k=1}^{K}\), and the policy \(\) is uniformly sampled from \(\{^{k}\}_{k=1}^{K}\). Then it holds that_

\[V^{^{}}-V^{}=_{k=1}^{K} (V_{r^{}}^{^{}}-V_{r^{}}^{^{ }}-(V_{r^{k}}^{^{}}-V_{r^{k}}^{^{}} ))}_{}+_{k=1}^{K} (V_{r^{k}}^{^{}}-V_{r^{k}}^{^{k}})}_{}. \]

Lemma 1 suggests that to achieve a small imitation gap, it is crucial to control both reward error and policy error. Specifically, reward error quantifies the distance between the true reward \(r^{}\) and the learned reward \(r^{k}\) through the imitation gap. Besides, policy error measures the value difference between the expert policy \(^{}\) and the learned policy \(^{k}\) under the inferred reward \(r^{k}\). Notably, this policy error differs from the concept of regret in RL [23; 32], where the reward is fixed.

To theoretically minimize the reward error and policy error, we consider an iterative approach, in which each iteration first updates the reward and subsequently derives the policy. The subsequent parts detail the reward and policy updates, which involve solving two optimization problems.

```
0: Reward class \(\), Q-value class \(\), initialized reward \(r^{0}\), policy \(^{0}\) and dataset \(^{0}=\).
1:for\(k=1,2,,K\)do
2: Apply \(^{k-1}\) to roll out a trajectory \(^{k-1}\) and append it to the dataset \(^{k}=^{k-1}\{^{k-1}\}\).
3: Obtain \(r^{k}\) by running a no-regret algorithm to solve the online optimization problem with observed loss functions \(\{^{i}(r)\}_{i=0}^{k-1}\) up to an error \(_{}^{r}\), where \(^{i}(r)=_{r}^{^{i}}-_{r}^{^{}}\).
4: Obtain \(Q^{k}\) by solving the following optimization problem up to an error \(_{}^{Q}\). \[_{Q}^{k}(Q):=^{k}(Q)-_{a }Q_{1}(s_{1},a),\]

 where \(^{k}(Q)=_{h=1}^{H}_{h}(Q_{h},Q_{h+1};^{k},r^{k})-_{Q_{h}^{}_{h}}_{h}(Q_{h}^{},Q_{h+1};^{k},r^{k})\).
5: Obtain \(^{k}\) by \(_{h}^{k}(s)=*{argmax}_{a}Q_{h}^{k}(s,a)\).
6:endfor
7:\(\) sampled uniformly from \(\{^{k}\}_{k=1}^{K}\).
```

**Algorithm 1** Optimization-based Adversarial Imitation Learning

**Reward Update via Online Optimization (Line 3 in Algorithm 1).** The goal of this step is to control the reward error. More concretely, in iteration \(k\), we aim to learn a reward \(r^{k}\) such that the error \(V_{r}^{^{k}}-V_{r^{k}}^{^{}}-(V_{r^{k}}^{^{}}-V_{r ^{}}^{^{}})\) is small. We formulate this problem as an _online_ optimization problem. In iteration \(k\), using the previously observed loss functions \(\{V_{r}^{^{i}}-V_{r}^{^{}}\}_{i=0}^{k-1}\), the reward learner selects \(r^{k}\) and then observes the current loss function \(V_{r}^{^{k}}-V_{r}^{^{}}\). Moreover, since the previous expected loss functions \(\{V_{r}^{^{i}}-V_{r}^{^{}}\}_{i=0}^{k-1}\) are not available, we instead minimize the _estimated_ loss functions. In particular, we leverage expert demonstrations \(^{}\) and the trajectory \(^{i}\) collected by policy \(^{i}\) to establish an unbiased estimation \(^{i}(r)=_{r}^{^{i}}-_{r}^{^{k}}\) for \(V_{r}^{^{i}}-V_{r}^{^{k}}\), where

\[_{r}^{^{i}}=_{h=1}^{H}r_{h}(s_{h}^{i},a_{h}^{i}),\ _{r}^{^{k}}=_{^{}}_{h=1}^{H}r_{h}((s_{h}),(a_{h})).\]

Here \(((s_{h}),(a_{h}))\) is the state-action pair of trajectory \(\) visited in time step \(h\) and \(^{i}=\{s_{1}^{i},a_{1}^{i},,s_{H}^{i},a_{H}^{i}\}\) is the trajectory collected by policy \(^{i}\). The ultimate goal of the reward learner is to minimize the cumulative losses \(_{k=1}^{K}_{r}^{^{k}}-_{r}^{^{k}}\). To achieve this goal, we employ a no-regret algorithm . In the following part, we formally define the reward optimization error resulting from running the no-regret algorithm.

**Definition 2** (Reward Optimization Error).: _For any sequence of policies \(\{^{k}\}_{k=1}^{K}\), the no-regret reward optimization algorithm sequentially outputs rewards \(r^{1},,r^{K}\). The reward optimization error \(_{}^{r}\) is defined as \(_{}^{r}:=(1/K)_{r}_{k=1}^{K} _{r}^{^{k}}-_{r}^{^{k}}-(_{r}^{^{k}}- _{r}^{^{k}})\)._

The reward optimization error, as defined above, aligns with the standard average regret in online optimization , a concept not extensively explored in the context of AIL. When the loss functions \(\{^{k}(r)\}_{k=0}^{K}\) are convex functions and the reward class \(\) is a convex set, we can apply online projected gradient descent  as the no-regret algorithm, which ensures the reward optimization error \(_{}^{r}=(1/)\). As for non-convex functions and sets, employing Follow-the-Perturbed-Leader can similarly achieve \(_{}^{r}=(1/)\).

**Policy Update via Optimism-Regularized Bellman-error Minimization (Lines 4-5 in Algorithm 1).** The target of policy updates is to control the policy error. In iteration \(k\), the policy learner aims to recover a policy \(^{k}\) such that the policy error \(V_{r^{k}}^{^{k}}-V_{r}^{^{k}}\) is small, where \(r^{k}\) is the recently recovered reward function. This is essentially an RL problem under reward function \(r^{k}\). Building upon , we leverage a model-free approach, based on Bellman error minimization, to solve this RL sub-problem. In particular, we first learn Q-value functions by solving the optimization problem of

\[_{Q}^{k}(Q):=^{k}(Q)-_{a }Q_{1}(s_{1},a),\]

\[\ ^{k}(Q)=_{h=1}^{H}_{h}(Q_{h},Q_{h+1}; ^{k},r^{k})-_{Q_{h}^{}_{h}}_{h}( Q_{h}^{},Q_{h+1};^{k},r^{k}),\]

where \(_{h}(Q_{h},Q_{h+1};^{k},r^{k})=_{i=0}^{k-1}(Q_{h}(s _{h}^{i},a_{h}^{i})-r_{h}^{k}-_{a^{}}Q_{h+1}(s_{h+1}^{ i},a^{}))^{2}\), \(^{k}=\{^{i}\}_{i=0}^{k-1}\) with \(^{i}=\{s_{1}^{i},a_{1}^{i},,s_{H}^{i},a_{H}^{i}\}\) and \(>0\) is the regularization coefficient. As shown in , \(^{k}(Q)\) is the estimated squared Bellman error of \(Q\) with respect to reward \(r^{k}\) and dataset \(^{k}\). In this optimization problem, the main objective \(^{k}(Q)\) ensures a small Bellman error while the regularization term \(_{a}Q_{1}(s_{1},a)\) tends to search an optimistic Q-value function for encouraging exploration. It is worth noting that Algorithm 1 only requires approximately solving the optimization problem up to an error \(_{}^{Q}\) with \(_{}^{Q}=^{k}(Q^{k})-_{Q} ^{k}(Q)\). After obtaining the Q-value function \(Q^{k}\), we derive \(^{k}\) as the greedy policy of \(Q^{k}\).

**Theoretical Guarantee of OPT-AIL.** In the above part, we have explained the algorithmic mechanisms of OPT-AIL. Now we present the theoretical guarantee. To ensure the sample efficiency of solving RL sub-problems within AIL, we make a structural assumption on the underlying MDP. In particular, we assume that the MDP has a small generalized eluder coefficient. This coefficient, introduced in , quantifies the inherent difficulty of learning the MDP with function approximation in RL. We adapt this concept to AIL where the reward function is changing.

**Assumption 4** (Low generalized eluder coefficient ).: _We assume that given an \(>0\), the generalized eluder coefficient \(d_{}()\) is the smallest \(d\) (\(d 0\)) such that for any sequence of \(\{r^{k}\}_{k=1}^{K}\), \(\{Q^{k}\}_{k=1}^{K}\) and the corresponding greedy policies \(\{^{k}\}_{k=1}^{K}\),_

\[_{k=1}^{K}Q_{1}^{k}(s_{1},^{k})-V_{r^{k}}^{^{k}} _{ 0}_{k=1}^{K}_{i=1}^{k-1} [._{h=1}^{H}(Q_{h}^{k}(s_{h},a_{h})-_ {h}^{^{k}}Q_{h+1}^{k}(s_{h},a_{h}))^{2}|^{i}]+ {2}\] \[++ HK,\]

_where \(Q_{1}^{k}(s_{1},^{k}):=_{a_{1}^{k}_{1}(|s_{1})}[Q_{1}^{ k}(s_{1},a_{1})]\)._Intuitively, a low generalized eluder coefficient ensures that the prediction error \(Q_{1}^{k}(s_{1},^{k})-V_{^{k}}^{^{k}}\) for \(^{k}\) can be effectively controlled by the Bellman error on the dataset generated by historical policies \(\{^{i}\}_{i=1}^{k-1}\). As demonstrated in , the MDPs with low generalized eluder coefficient form a rich class of MDPs, which covers many well-known MDP instances such tabular MDPs, linear MDPs  and MDPs with low Bellman eluder dimension . Now we are ready to present the theoretical guarantee of OPT-AIL.

**Theorem 1**.: _Under Assumptions 1, 2, 3 and 4. For any fixed \((0,1]\) and \((0,1]\), consider Algorithm 1 with \(=c_{1}(4KHN_{}()_{}( )/)+K^{2}H^{3})/d_{}}\), where \(d_{}:=d_{}(/H)\), \(:=c_{2}^{2}/(H^{2}d_{}+H)\), \(c_{1}\) and \(c_{2}\) are absolute constants. Then with probability at least \(1-\), we have that \(V^{^{k}}-V^{}+^{r}_{}+( ^{Q}_{}/)\) if the expert sample complexity and interaction complexity satisfy_

\[N (_{h[H]}_{}(_{h})/)}{^{2}},\] \[K d_{}(Hd_{}_{h [H]}_{}(_{h})_{}(_{h})/ ())+H^{2}(1/)}{^{2}}.\]

The proof of Theorem 1 can be found in Appendix B.2. Theorem 1 indicates that when \(d_{}=(1)\), OPT-AIL achieves the expert sample complexity \(}(H^{2}(_{h[H]}_{}(_{h}))/^{2})\) and interaction complexity \(}(H^{4}d_{}(_{h[H]}_{ }(_{h})_{}(_{h}))/^{2})\) in the general function approximation setting. To our best knowledge, OPT-AIL is the first provably efficient online AIL algorithm for general function approximation.

Notably, OPT-AIL improves over BC  by an order of \((H)\), suggesting that OPT-AIL can still provably mitigate the compounding errors issue in BC for general function approximation. When restricting Theorem 1 to linear MDPs with dimension \(d\), OPT-AIL can achieve the expert sample complexity \(}(H^{2}d/^{2})\) and interaction complexity \(}(H^{5}d^{2}/^{2})\). Furthermore, when \(\) and \(\) are neural network classes commonly employed in practice, we can obtain the corresponding complexity result by plugging the covering number bound of neural networks  into Theorem 1. Finally, OPT-AIL only requires the approximate optimization of two objectives, thereby facilitating a practical implementation with neural networks, which will be presented in the next section.

Although Theorem 1 produces desirable outcomes, it does have some limitations. One of the limitations is that Theorem 1 requires the Bellman completeness condition for the Q-value class (i.e., Assumption 3). Nevertheless, recent advances  in RL have developed new techniques to remove this assumption. We leave the extension of these techniques to AIL for future work.

### Practical Implementation of OPT-AIL

In this section, we provide a practical implementation for OPT-AIL, which is based on the stochastic-gradient-based methods; see Algorithm 2 for an overview. We elaborate on the practical reward update and policy update in detail as follows.

```
1:Initialized reward \(r^{0}\), Q-value \(Q^{0}\), target Q-value \(^{0}=Q^{0}\), policy \(^{0}\) and dataset \(^{0}=\).
2:for\(k=1,2,,K\)do
3: Apply \(^{k-1}\) to roll out a trajectory \(^{k-1}\) and append it to the dataset \(^{k}=^{k-1}\{^{k-1}\}\).
4: Update the reward function by \(r^{k} r^{k-1}-_{r}^{k}(r)\) from Eq. (2).
5: Update the Q-value function by \(Q^{k} Q^{k-1}-_{Q}^{k}(Q)\) from Eq. (3).
6: Update the policy by \(^{k}^{k-1}+_{}^{k}()\), where \(^{k}():=_{^{k}}[_{h=1}^{H}Q_{h}^{k}(s _{h},)]\).
7: Update the target Q-value by \(^{k} Q^{k}+(1-)^{k-1}\)
8:endfor
```

**Algorithm 2** Practical Implementation of OPT-AIL

**Practical Reward Update.** Here we detail a practical implementation of the reward update by applying an online optimization approach. Recall that in line 3 of Algorithm 1, a no-regret algorithm is employed to solve the online optimization problem. To implement this mechanism, we choose the classical online optimization approach Follow-the-Regularized-Leader (FTRL)  as the no-regretapproach. In iteration \(k\), FTRL minimizes the sum of all historical loss functions with a regularization.

\[_{r}^{k}(r):&=_{ i=0}^{k-1}^{i}(r)+(r)\\ &=k_{^{k}}_{h=1}^{ H}r_{h}(s_{h}^{i},a_{h}^{i})-_{^{k}} _{h=1}^{H}r_{h}(s_{h}^{i},a_{h}^{i})+(r), \]

where \(_{}[]\) denotes the empirical distribution of dataset \(\). Here \((r)\) is the regularization term. In practice, we choose \((r)\) as the gradient penalty  of the reward model, which can help stabilize the learning process . According to Eq. (2), the reward learner aims to maximize the value gap between the expert policy and all previous policies.

Besides, as indicated in Eq. (2), all historical samples \(^{k}\) are utilized for the reward update. This learning style is exactly off-policy reward learning [27; 28]. In particular, applying FTRL for the reward update and off-policy reward learning share the same main objective. Previous practical works [27; 28] found that this off-policy reward learning works well in practice, but could not give an explanation. In this work, we justify this off-policy learning style from an online optimization perspective: performing off-policy learning, which aligns with the FTRL approach, can effectively control the reward optimization error.

**Practical Policy Update.** To implement the policy update in practice, we adopt the actor-critic framework [14; 17; 27]. In particular, we maintain a policy model \(\) and a Q-value model \(Q\). Recall in line 4 of Algorithm 1, the Q-value function is learned by minimizing the optimism-regularized Bellman error. To implement this principle, following [12; 8], we leverage the temporal difference loss  of the Q-value model and its delayed target to approximate the theoretical Bellman error. Then we arrive at the following objective.

\[_{Q}^{k}(Q):=_{^{k}}[ _{h=1}^{H}Q_{h}(s_{h},a_{h})-r_{h}^{k}-_{h+1}^{k-1}(s_{h+1}, ^{k-1})^{2}]- Q_{1}(s_{1},^{k-1}). \]

Here \(}=\{}_{1},,}_{H}\}\) is the delayed target Q-value model. Besides, we define that \(_{h+1}^{k-1}(s_{h+1},^{k-1}):=_{a^{}_{h+1}^{ k-1}(|s_{h+1})}[}_{h+1}(s_{h+1},a^{})]\) where the previous greedy policy \(^{k-1}\) is used to approximate the maximum operator . Consequently, we derive the greedy policy by optimizing the objective of \(_{}^{k}():=_{^{k}}[_{h=1}^{H}Q _{h}^{k}(s_{h},)]\).

## 5 Experiments

In this section, we evaluate the expert sample efficiency and environment interaction efficiency of OPT-AIL through experiments. Below, we provide a brief overview of the experimental set-up, with detailed information available in Appendix C due to space constraints.

### Experiment Set-up

**Environment.** We conduct experiments on 8 tasks sourced from the feature-based DMControl benchmark , a leading benchmark in IL that offers a diverse set of continuous control tasks. For each task, we adopt online DrQ-v2  to train an agent with sufficient environment interactions and regard the resultant policy as the expert policy. Then we roll out this expert policy to collect expert demonstrations. Each algorithm is tested over five trials with different random seeds, and in each run, we evaluate the policy return using Monte Carlo approximation with 10 trajectories.

**Baselines.** Existing theoretical AIL approaches like MB-TAIL  and OGAIL  include count-based or covariance-based bonuses, making it challenging to implement these designs when using neural network approximations. Thus, we do not include these methods in our experiments. Instead, we compare OPT-AIL with prior deep IL methods, including BC , IQLearn , PPLL , FILTER  and HyPE , despite that most of them lack theoretical guarantees. Notably, IQLearn, FILTER and HyPE represent prior SOTA deep AIL approaches. To ensure a fair comparison, we implement all these methods within the same codebase. For detailed implementations, please refer to Appendix C.

### Experiment Results

**Expert Sample Efficiency.** Figure 1 shows the performance of the learned policies after 500k environment interactions with varying numbers of expert trajectories. First, OPT-AIL significantly outperforms BC, verifying the theoretical claim that OPT-AIL can mitigate the compounding errors issue inherent in BC for general function approximation. Moreover, OPT-AIL consistently matches or exceeds the performance of prior SOTA AIL methods on all tasks. Notably, OPT-AIL demonstrates outstanding performance in scenarios with limited expert demonstrations, a common occurrence in real-world applications. In particular, when there is only one expert trajectory, our method uniquely achieves expert or near-expert performance on tasks like Finger Spin, Walker Run and Hopper Hop.

Figure 1: Overall performance on 8 DMControl tasks over 5 random seeds following 500k interactions with the environment. Here the \(x\)-axis is the number of expert trajectories and the \(y\)-axis is the return. The solid lines are the mean of results while the shaded region corresponds to the standard deviation over 5 random seeds. Same as the following figures.

Figure 2: Learning curves on 8 DMControl tasks over 5 random seeds using 1 expert trajectory. Here the \(x\)-axis is the number of environment interactions and the \(y\)-axis is the return.

**Environment Interaction Efficiency.** Figure 2 displays the learning curves of different algorithms with \(1\) expert trajectory. Compared with prior SOTA AIL approaches, OPT-AIL achieves comparable or better performance regarding interaction efficiency on all \(8\) tasks. Notably, on Hopper Hop, Walker Run and Walker Run, OPT-AIL can achieve near-expert performance with substantially fewer environment interactions compared with prior approaches. We also demonstrate the superior interaction efficiency of OPT-AIL with other numbers of expert trajectories; please refer to Appendix D for additional results.

## 6 Conclusions

To narrow the gap between theory and practice in adversarial imitation learning, this paper investigates AIL with general function approximation. We develop a new AIL approach termed OPT-AIL, which centers on performing online optimization for reward functions and optimism-regularized Bellman error minimization for Q-value functions. In theory, OPT-AIL achieves polynomial expert sample complexity and interaction complexity for general function approximation. In practice, OPT-AIL only requires approximately solving two optimization problems, which enables an efficient implementation. Our experiments demonstrate that OPT-AIL outperforms prior SOTA methods in several challenging tasks, highlighting its potential to bridge theoretical soundness with practical efficiency.

In tabular MDPs, the currently optimal expert sample complexity is \((H^{3/2}/)\), which is better than \((H^{2}/^{2})\) attained in this paper. Therefore, a promising and valuable future direction would be to develop more advanced AIL approaches that achieve this expert sample complexity in the setting of general function approximation. Besides,  established a horizon-free imitation gap bound for AIL in tabular MDPs. Thus it is interesting to explore horizon-free bounds for AIL with general function approximation.

## 7 Acknowledgements

We thank Ziniu Li and Yichen Li for their helpful discussions and feedback. This work was supported by the Fundamental Research Program for Young Scholars (PhD Candidates) of the National Science Foundation of China (623B2049) and Jiangsu Science Foundation (BK20243039).