# ImageNet-Hard: The Hardest Images Remaining from a Study of the Power of Zoom and Spatial Biases in Image Classification

ImageNet-Hard: The Hardest Images Remaining from a Study of the Power of Zoom and Spatial Biases in Image Classification

 Mohammad Reza Taesiri

University of Alberta

mtaesiri@gmail.com

&Giang Nguyen

Auburn University

nguyengiangbkhn@gmail.com

&Sarra Habchi

Ubisoft

sarra.habchi@ubisoft.com

&Cor-Paul Bezemer

University of Alberta

bezemer@ualberta.ca

&Anh Nguyen

Auburn University

anh.ng8@gmail.com

###### Abstract

Image classifiers are information-discarding machines, by design. Yet, how these models discard information remains mysterious. We hypothesize that one way for image classifiers to reach high accuracy is to zoom to the most discriminative region in the image and then extract features from there to predict image labels, discarding the rest of the image. Studying six popular networks ranging from AlexNet to CLIP, we find that proper framing of the input image can lead to the correct classification of 98.91% of ImageNet images. Furthermore, we uncover positional biases in various datasets, especially a strong center bias in two popular datasets: ImageNet-A and ObjectNet. Finally, leveraging our insights into the potential of zooming, we propose a test-time augmentation (TTA) technique that improves classification accuracy by forcing models to explicitly perform zoom-in operations before making predictions. Our method is more interpretable, accurate, and faster than MEMO, a state-of-the-art (SOTA) TTA method. We introduce ImageNet-Hard, a new benchmark that challenges SOTA classifiers including large vision-language models even when optimal zooming is allowed.

## 1 Introduction

Since the release of AlexNet in 2012 , deep neural networks have set many ImageNet (IN)  accuracy records . While many papers reported improved learning algorithms or architectures, little is known about how the inner workings of image classifiers actually evolve. The success is often attributed to a network's ability to detect more objects  and a variety of facets of each object (_i.e._, invariance to style, pose, and form changes) . By aggregating the information from all the visual cues in a scene, a classifier somehow chooses a better label for the image. For example, Figs. 13-14 in  show that a model detects both dogs and cats in the same image and only discards the dog features right before the classification layer to arrive at a tiger cat prediction.

When processing an image, a network may implicitly **zoom** in or out (defined in Sec. 3) to the most discriminative image region _ignoring_ the rest of the image (Fig. 1a), and then extract that localized region's features to predict image labels. We hypothesize that the improved image classification may largely be due to the networks accurately zooming to the discriminative areas (_e.g._, junco and magpie birds in Fig. 1a) rather than more accurately describing them (_i.e._ generating better features of these two birds). In this work, we present supporting evidence for our zooming hypothesis.

We conduct a thorough study to test the effects of zooming in and out on the classification accuracy of six network architectures on six ImageNet-scale benchmarks. Our main findings also include:

1. A major, surprising finding is that **state-of-the-art, IN-trained models can accurately predict up to \(98.91\%\) of ImageNet samples when an optimally-zoomed image is provided**. The remaining few hundred IN images (\(0.39\%\)) that are never correctly labeled by any model (despite optimal zooming) include mostly _ill-posed_ and _rare_ images (Sec. 4.1).
2. ImageNet-A  and ObjectNet  both exhibit a substantial center bias. For example, by only upsampling and center-cropping each ImageNet-A image, ResNet-50's accuracy increases dramatically from \(0.09\%\) to \(14.58\%\) (Sec. 4.2).
3. Zooming can be leveraged as an inductive bias at test time to improve ImageNet classification accuracy. That is, integrating zoom transformations into MEMO , a leading test-time augmentation method, yields consistently higher accuracy than the baseline ResNet-50 models and also MEMO with default transformations on multiple datasets (Sec. 4.3).

Our findings show that the accuracy of image classifiers can be improved by finding an optimal zoom setting first and then classifying that crop alone (Fig. 0(a)). Motivated by this insight, we build **ImageNet-Hard1**, a new 1000-way classification benchmark that challenges state-of-the-art (SOTA) classifiers despite the application of optimal zooming (Sec. 4.4). In other words, we collect images from seven existing ImageNet-scale benchmarks where OpenAI's CLIP ViT-L/14  misclassifies even when allowed to try 324 zooming settings. Interestingly, SOTA classifiers that operate at 224\(\)224 resolution perform poorly on ImageNet-Hard (below 19% accuracy). Analyzing misclassifications on ImageNet-Hard reveals a major remaining challenge in the era of SOTA classifiers of Transformers , EfficientNets [69; 36], and large vision-language models (Sec. 4.5).

## 2 Related Work

Learning to Zoom in image classificationLeveraging zoom-in or crops of an image has a long history of improving fine-grained image classification with approaches varying from combining multiple crops at different resolutions [18; 73], using multiple crops of the object (i.e., part-based classification) [16; 37; 84; 68; 36] to warping the input image [55; 31; 32]. We note that a common

Figure 1: **(a) Each subfigure shows an input image, predicted label, and confidence score from an ImageNet classifier (top and middle: ResNet-50 ; bottom: ViT-B/32 ). With the standard center-crop transform, all 3 samples were misclassified (left-most column). Adjusting framing via zooming yields correct predictions. (b) The zooming process correctly classifies a snail ImageNet-A image. We uniformly adjust the input query imageâ€™s smaller dimension to match the target scale \(S\). We then partition the image into a \(3 3\) grid, generating \(9\) crops centered at grid-cell centers (_i.e._, \(\) anchor points) and feed each crop to the original image classifier.**

prior definition of "zoom" [55; 31; 32; 70] is to first divide an input image into a grid and then warp the image, distorting the aspect ratio of the objects in the image. In contrast, our zoom procedure utilizes only two functions: resize and crop, maintaining the original aspect ratio.

Furthermore, to our knowledge, we are the first to perform a zoom study on ImageNet-scale datasets (ImageNet , ImageNet-A , ObjectNet , etc) while prior zoom approaches [16; 37; 84; 68; 84; 36; 32] exclusively focus on non-ImageNet, fine-grained classification (e.g. classifying birds or dogs). Due to such differences in the image distribution of interests, prior works mostly study _zooming in_ (which benefits fine-grained classification) while we study _both_ zooming in and out.

**Test-time data augmentation (TTA)** is a versatile technique that could help estimate uncertainty [65; 7; 6] and improve classification accuracy [38; 67; 23; 52; 46; 62; 34; 14]. When test inputs are sampled from unseen, non-training distributions, augmenting the data often improve a model's generalization to new domains [74; 83]. A simple TTA method is 10-crop evaluation  in which 5 patches of \(224 224\) px along with their horizontal reflections (resulting in 10 patches) are extracted from the original image. An alternative way to leverage the marginal output distributions over augmented data is to use them as gradient signals to update the classifier's parameters [75; 83]. We employ this approach to update the model during test time, with patches being zoom-based augmentations.

**Biases in ImageNet and datasets** Resize-then-center-crop has been a pre-processing standard for ImageNet classification since AlexNet . This pre-processing exploits the known center bias of ImageNet. While ImageNet has been shown to contain a variety of biases in image labels [10; 71], object poses , image quality , we are the first to examine the _positional_ biases of the out-of-distribution (OOD) benchmarks for ImageNet classifiers and find a strong center bias in ImageNet-A and ObjectNet that could affect how the community interprets progress on these OOD benchmarks.

Perhaps the closest to us is a preprint by Li et al.  that shows that cropping to the main object can improve model accuracy on ImageNet-A . Yet, unlike , we study six ImageNet-scale datasets, both zooming in and zooming out, and we propose a new dataset of ImageNet-Hard.

## 3 Method

**Zoom definition** To zoom in or out of the image, we only use _resize_ and _crop_ operations. Initially, we uniformly resize the test image so that the smaller dimension matches the target scale of \(S\). Then, we define a \(3 3\) grid on the image to divide it into \(9\) patches. We perform a _CenterCrop_ operation at the center (\(\) in Fig. 1b) of each patch to extract a \(224 224\) px crop from each of the nine locations (see Python code in Appendix A.1). In the _CenterCrop_ step, zero-padding is used when the content to be cropped is smaller than \(224 224\). Overall, at each target scale \(S\), we generate 9 crops (Fig. 1b).

We test \(36\) different values of \(S\) ranging from \(10\) to \(1024\) px, resulting in a total of \(36 9=324\) different zoomed versions for each image. Based on initial scale factor \(S\), we define three groups: (1) _zoom-out_ group contains all augmented crops where \(S<224\); (2) _zoom-in_ group contains all augmented crops where \(S>224\); and (3) _zoom-224_ group contains the \(9\) patches where \(S=224\).

**Benchmark datasets** We use the ImageNet (IN)  dataset with both the original and ImageNet-ReaL  (ReaL) labels. For each IN image, we use the union of the IN and ReaL labels (IN+ReaL) to complement each other to reduce noise in IN labels. We further examine the effects of zoom-based transformations on four popular OOD benchmarks: (a) natural adversarials (ImageNet-A ), (b) image renditions (ImageNet-R ), (c) black-and-white sketches (ImageNet-Sketch ), and (d) viewpoint-and-background-controlled samples (ObjectNet ). We refer to these as benchmarks as IN-A, IN-R, IN-S, and ON, respectively, in the rest of the paper.

**Classifiers** We study the effects of zoom-based transformations on six popular image classifiers in the last decade: AlexNet , VGG-16 , ResNet-18 & ResNet-50 , ViT-B/32 , and OpenAI's CLIP-ViT-L/14 . The inclusion of the 11-year-old AlexNet provides a baseline for the power of deep features (when given the right region to look at). Predicted labels from CLIP-ViT-L/14 are acquired using its standard zero-shot classification setup (Appendix A.5).

[MISSING_PAGE_FAIL:4]

3. **Illusions** (Fig. 1(c)): Images have misleading elements, like a shadow appearing as a staircase, leading to misclassification.
4. **Many objects** (Fig. 1(d)): Images displaying several classes of objects but not all classes are listed in the set of groundtruth labels.

**OOD datasets pose a significant challenge to** IN-trained models **despite optimal zooming.** Across IN-A, IN-R, and ON, all IN-trained models perform far below the 324-crop random baseline (\(100\%\)) with the highest score being \(79.28\%\) (Tab. 1). In contrast, CLIP reaches far better scores than IN-trained models (\(98.45\%\) on IN-A and \(99.20\%\) on IN-R; Tab. 1). Our result suggests that OOD images (_e.g._ objects in unusual poses or renditions) require a more robust feature extractor to recognize besides zooming. And that CLIP was trained on an Internet-scale dataset  and thus is much more familiar with a variety of poses, styles, and shapes of objects .

Among the 324 zoom transformations, for each (classifier, dataset) pair, we initially construct a bipartite graph connecting transforms to images based on their correct classification. With this graph, we employ the iterative, greedy minimum-set cover algorithm  to compute the minimum set of transforms required to achieve the upper-bound accuracy detailed in Sec. 4.1. Through this process, we discover that, on average, only 70% of the transforms are essential. Furthermore, we identify the **top-36 zoom transforms most important to classification** (see visualizations in Appendix D.1). More details on this process can be found in Appendix B.4.

The upper-bound accuracy using 36 crops (Tab. 0(b)) is only _slightly lower_ than that when using all \(324\) crops but is substantially higher than (1) the standard 1-crop, _e.g._\(85.19\%\) vs. \(56.16\%\) for AlexNet on IN (Tab. 0(b)); and (2) the random baseline (_i.e._\(3.6\%\) for IN). Our result confirms that these 36 zoom transforms are indeed important to classification (not because models are given 36 random trials per image) and that studying them might reveal interesting insights into the datasets.

As our 324 transforms include both zoom-in and zoom-out, we further analyze the contribution of each zoom type to each dataset. We find that, across 7 datasets, zoom-in is more useful than zoom-out. And that **zoom-out is the most important to abstract images**_i.e._, of IN-R and IN-S (Appendix B.6).

### ImageNet-A and ObjectNet suffer from a severe center bias

The standard image pre-processing for IN-trained models involves resizing the image so its smaller dimension is \(256\), then taking the center \(224 224\) crop of the resized image . While suitable for ImageNet, this pre-processing may not be optimal for every OOD dataset, not allowing a model to fully utilize off-center visual cues (which optimal zooming could). Leveraging the minimum set of transforms obtained in Appendix B.4, we quantify which spatial locations (out of \(9\) anchors; Fig. 0(b)) contain the most discriminative features in each dataset. That is, we compute the upper-bound accuracy for each of the \(9\) anchor points per dataset and discover biases in some benchmarks.

ExperimentFor each image, we have \(9\) anchors (Fig. 0(b)) and the originally \(K=36\) zoomed versions per anchor as defined in Sec. 3. Yet, after reducing to the minimum set (Appendix B.4), \(K\) averages at \(25\), over all datasets, and \(10 K 31\). Here, we count the probability that the \(K\) zoomed versions per anchor lead to a correct prediction. In other words, we compute the upper-bound accuracy as in Sec. 4.1 but for each anchor separately.

Figure 3: Upper-bound accuracy (%) of ResNet-50 at each of the \(9\) zoom locations. The large gaps between the center and eight off-center locations on IN-A and ON demonstrate a center bias, which is much smaller in IN (Appendix B.2) and IN-R (b). The values in parentheses indicate the delta with respect to the center crop.

ResultsFirst, as expected, the upper-bound accuracy for each anchor (Fig. 3) is consistently lower than when all 9 anchors are allowed (Tab. 0(c)). Second, across all \(6\) datasets, the center anchor consistently achieves the highest upper-bound accuracy versus the other \(8\) locations (Fig. 3 and Appendix B.2), indicating a center bias in all datasets. However, we find this bias is small in IN, IN-R, and IN-S but large in IN-A and ON (_i.e._ the largest difference between center accuracy and the lowest off-center accuracy is around -25 and -23 points, respectively; whereas for other datasets, it is around (-1) to (-5) points, as shown in Fig. 3).

The center bias in ObjectNet can be explained by the fact that the images were captured using smartphones with aspect ratios of 2:3 or 9:16 (Appendix D.3.3). Overall, such strong center bias in IN-A and ON may not be desirable since improvements on these two benchmarks may be attributed to learning to zoom to the center as opposed to the intended quest of recognizing objects in unusual forms (IN-A) or poses (ON). By merely upscaling the image and center cropping, we can achieve higher accuracy using nearly all the same models on these two datasets (Figs. 9 and 10).

We also find that, during test time, center-zooming (Appendix B.5) increases the top-1 accuracy of all IN-trained models but not CLIP, even on IN-A and ON images. This observation is intriguing considering these OOD datasets contain more distracting objects than ImageNet images (Appendix C.2) and therefore, center-zooming _should_ de-clutter the scene for more accurate classification. However, CLIP prefers a specific zoom scale that provides sufficient background for object recognition--it struggles to identify a single object in a tightly-cropped image . Future research should examine whether this "zoom bias" of CLIP is due to its image- or text-encoder, or both.

### Test-time augmentation of MEMO with _only_ zoom-in transforms improves accuracy

Aggregating model predictions over zoom-in versions of the input image during test time leads to higher top-1 accuracy on IN, IN-ReaL, IN-A and ON, but lower accuracy on IN-R and IN-S (Appendix B.7). However, interestingly, always zooming out on IN-R and IN-S abstract images also hurts accuracy, suggesting that an adaptive zooming strategy might be a better approach.

Here, we test building such an adaptive test-time zooming strategy by modifying MEMO , a SOTA test-time augmentation method that finetunes a pre-trained classifier at _test_ time to achieve a more accurate prediction. Specifically, MEMO finds a network that produces a low-entropy predicted label over a set of \(K=16\) augmented versions of the test image \(I\) and then runs this finetuned model on \(I\) again to produce the final prediction. It does this by applying different augmentations to the test point \(I\) to get augmented points \(I_{1},,I_{K}\), passing these through the model to obtain predictive distributions, and updating the model parameters by minimizing the entropy of the averaged marginal distribution over predictions. While improving accuracy, MEMO requires a pre-defined set of diverse augmentation transforms (_e.g._ sheer, rotate, and solarize in AugMix ). Yet, the hyperparameters for each type of transform are hard-coded, and the contribution of each transform to improved classification accuracy is unknown.

We improve MEMO's accuracy and interpretability by replacing AugMix transforms with only zoom-in functions. Intuitively, a model first looks at all zoomed-in frames of the input image (at different zoom scales and locations) and then decides to achieve the most confident prediction.

ExperimentMEMO relies on AugMix , which applies a set of 13 image transforms, such as translation, rotation, and color distortion, to an original image at varying intensities, and then _chains_ them together to create \(K=16\) new augmented images (examples in Appendix D.5).

We replace AugMix with RandomResizedCrop (RRC), which takes a random crop of the input image (_i.e._ at a random location, random rectangular area, and a random aspect ratio) and then resizes it to the fixed 224\(\)224 (_i.e._ the network input size). RRC basically implements a random zoom-in function (examples in Appendix D.5).

We compare the original MEMO  (which uses AugMix) and our version that uses RRC on five benchmarks (IN, IN-A, IN-R, IN-S, and ON). We follow the same experimental setup as in  (_e.g._\(K=16\)). Specifically, we test three ResNet-50 variants that were pre-trained using distinct augmentation techniques.2We utilize Grad-Cam  to understand the impact of MIMO on the network's attentions within the final layer, both before and after modification. Specifically, our investigation seeks test our hypothesis concerning the model's focus on the regions of interest within an image.

**Results** Both our MEMO + RRC and the original MEMO + AugMix  consistently outperform the baseline models, which do not use MEMO, on all five datasets (Tab. 2). That is, when combined with MEMO, zoom-in transforms implemented via RRC are also helpful in classifying IN-S and IN-R images--where we previously find zoom-in to _not_ help in mean/max aggregation (Appendix B.7).

On average, over all three models and five datasets, our RRC outperforms AugMix by \(+0.28\) points, with a larger impact on IN-A, where it achieves a mean improvement of \(+1.10\) points (Tab. 2). Our results show that zoom-in alone can be a useful inductive bias, helping improve downstream image classification. In contrast, some of the transformations among the 13 transform functions in AugMix may not be essential to the results of Zhang et al.  (no ablation studies of transformations were provided in ) and are less effective than our zoom-in.

Figure 4 shows Grad-CAM visualizations for three samples, providing evidence of how the network's behavior changes before and after the MEMO update. For an image of a pug, the network initially focused on a kitchen appliance, failing to detect the object correctly. After applying the MEMO modification, it refocused on the dog, classifying it accurately. Similarly, in an image of a fox squirrel, the network initially had a diffuse focus but refocused on the fox squirrel after the update. These results demonstrate the effectiveness of the MEMO modification in guiding the network's attention or encouraging the model to perform an implicit zoom on the regions of interest, thereby improving its classification performance.

### ImageNet-Hard: A benchmark with images that remain unclassifiable, even after 324 zoom attempts

Existing ImageNet-scale benchmarks followed one of the following three construction methods: (1) perturbing real images with the aim of making them harder for models to classify (_e.g._, ImageNet-C  and DAmageNet ); (2) collecting the real images that models misclassify (_e.g._, IN-A, ImageNet-O ); or (3) setting up a highly-controlled data collection process (_e.g._, IN-S and ON). Yet, none of such benchmarks explicitly challenge models on the ability to recognize a well-framed object in an image (_i.e._, no zooming required). For example, ON is supposed to test the recognition of objects in unusual poses but the cluttered background in ON images is actually a major reason for misclassification (Sec. 4.2). Furthermore, the results in Tab. 1 suggest that given optimal zooming, these existing benchmarks only challenge IN-trained models but not the Internet-scale vision-language models (_e.g._CLIP) anymore. We propose ImageNet-Hard, a novel ImageNet-scale benchmark that challenges existing and future SOTA models. ImageNet-Hard is a collection of images that the SOTA CLIP-ViT-L/14 fails to correctly classify even when 324 zooming attempts are provided.

    &  &  &  \\   & IN & IN-A & IN-R & IN-S & ON & IN & IN-A & IN-R & IN-S & ON & IN & IN-A & IN-R & IN-S & ON \\  ResNet-50  & 76.13 & 0.00 & 36.17 & 24.09 & 35.92 & 77.27 & 0.83 & **41.28** & **27.63** & 38.38 & **77.50** & **1.31** & 40.81 & 27.53 & **38.85** \\ DeepAugAugMix  & 75.82 & 3.87 & 46.77 & 32.62 & 34.81 & 76.27 & 5.35 & 50.79 & **35.70** & 36.42 & **76.38** & **5.76** & **50.80** & 35.65 & **36.64** \\ MedMix+CutMix  & 79.04 & 7.97 & 35.52 & 23.96 & 38.59 & 79.38 & 11.21 & **40.65** & **27.07** & 40.62 & **79.49** & **13.61** & 40.41 & 26.80 & **41.43** \\ \(mean std\) & & & & 36.75 \(\) 24.75 & & & & 39.26 (\(\) 2.51) & \(\) 24.32 & & & **39.54** (\(\) 27.99) & \(\) **24.10** & \\   

Table 2: MEMO + RRC (_i.e._ random zoom-in transforms) outperforms baselines and MEMO .

Figure 4: Grad-CAM for the activation of the last convolutional layer of a ResNet-50 before and after the MEMO update suggests that the network attends to the object of interest after the update.

#### 4.4.1 ImageNet-Hard construction

**Initial data collection** We take CLIP-ViT-L/14 (the highest-performing model in Tab. 1) and run the zooming procedure to find "Unclassifiable images" (defined in Sec. 4.1) from the following six datasets: IN-V2 , IN-ReaL, IN-A, IN-R, IN-S, and ON. That is, for each image \(x\), we generate 324 zoomed versions of \(x\) and feed them into CLIP-ViT-L/14. We add \(x\) to ImageNet-Hard only if none of the 324 versions are correctly classified.

**Adding ImageNet-C** The original IN-C  are the original IN images but center-cropped to 224 \(\) 224 px, which significantly makes the classification task unnecessarily more ill-posed (_e.g._, by adding Gaussian noise to a crop where the main object is already removed).

To find a subset of IN-C images for adding into ImageNet-Hard, we first re-generate ImageNet-C by adding the 19 types of corruption noise to IN without resizing the original IN images. Second, we run CLIP-ViT-L/14 on all 19 corruption types and manually select a subset of six diverse and lowest-accuracy corruption groups: Impulse Noise, Frost, Fog, Snow, Brightness, and Zoom Blur. We repeat the initial data collection process for these 6 image sets of IN-C.

**Groundtruth labels** After the above procedure, our dataset contains 13,925 images collected from IN-ReaL, IN-V2, IN-A, IN-C, IN-R, IN-S, and ON (see the distribution in Appendix E.1). ImageNet-Hard presents a 1000-way classification task where the 1000 classes are from ImageNet. We manually inspect all images and remove 295 samples that are obviously ill-posed (e.g. an entirely black image but labeled great white shark in IN-S Fig. A60), arriving at a total of 13,630 ImageNet-Hard images. A sample contains only one groundtruth label from its original datasets except for IN and IN-C images, which have a set of IN+ReaL labels. Each IN or IN-C image is considered correctly labeled by a model if its top-1 predicted label is among the groundtruth labels.

**Refining groundtruth labels via human feedback** Label noise is still present in IN and OOD benchmarks despite cleaning efforts . Since ImageNet-Hard contains images misclassified by CLIP-ViT-L/14, our manual inspection confirms many misclassified images have debatable labels.

To ameliorate the issue, we orchestrate a human feedback study for eliminating images with inaccurate labels. First, the first author examine every image and flag 3,133 images as ambiguous and needs verification. Then, we have two groups of annotators to help verify the labels (by choosing Accept, Reject, or Not Sure). Group A is composed of three students, each examine all 3,133 images where Group B is composed of 38 students, each examine 50 randomly-selected images. Our inter-annotator aggregation procedure merges labels from both groups and results in 2,280 images removed (out of 3,133 originally flagged), leaving ImageNet-Hard at a total of 11,350 images.

That is, we accept an image \(x\) if one of the two conditions is satisfied: (1) when all 3/3 group-A annotators accept \(x\); or (2) when 2/3 group-A annotators accept \(x\) and all group-B reviewers of \(x\) accept \(x\) (assuming at least 1 group-B annotator reviews \(x\); otherwise \(x\) will be rejected).

Inspired by IN-ReaL , we further clean up the labels by eliminating 370 images associated with the labels sunglass, sunglasses, tub, bathtub, cradle, bassinet, projectile, and missile, _i.e._, the classes that often contain similar images that belong to more than one class. After this refinement, the final ImageNet-Hard dataset contains a total of **10,980 images**.

**4K version** We utilize GigaGAN  to upscale every image in our final dataset and construct ImageNet-Hard-4K, which is aimed to facilitate future research into how a super-resolution step may improve image classification results (_e.g._, to classify an object when the image is blurry).

**Release** ImageNet-Hard and ImageNet-Hard-4K are released on HuggingFace (see samples in Fig. A49) under MIT License. Code for evaluating models on ImageNet-Hard is on GitHub.

#### 4.4.2 ImageNet-Hard challenges SOTA classifiers, especially those operating at 224\(\)224

Here, we evaluate the standard 1-crop, top-1 accuracy of SOTA classifiers on ImageNet-Hard. We use the image pre-processing function defined by each classifier. In addition to the 6 models in Sec. 4.1, we also test CLIP-ViT-L/14@336px , EfficientNet (B0@224px and B7@600px) , and EfficientNet-L2@800px . CLIP-ViT-L/14@336px, EfficientNet-B7@600px, and EfficientNet-L2@800px are state-of-the-art models that operate at high resolutions of 336\(\)336, 600\(\)600, and 800\(\)800 respectively. In addition, our evaluation includes models from the OpenCLIP family .

**Results** Tab. 3 shows fairly low top-1 accuracy by various classifiers on ImageNet-Hard. First, all well-known IN-trained classifiers that operate at 224\(\)224 perform poorly between 7.34% (AlexNet) and 18.52% accuracy (ViT-B/32).

Since ImageNet-Hard is based on a collection of images that OpenAI's CLIP ViT-L/14@224px mislabels, this classifier's accuracy on our dataset is only 1.86%. Yet, interestingly, CLIP-ViT-L/14@336px also performs poorly at 2.02% (Tab. 3). Furthermore, all 68 tested OpenCLIP models perform poorly, with an accuracy below 16% (see details in Appendix E.6).

Separately, we observe a trend that models operating at a higher resolution tend to perform better on ImageNet-Hard with EfficientNet-L2@800px scoring highest at 39.00% (compared to 88.40%  on the original ImageNet). Overall, all models perform substantially worse on ImageNet-Hard (Tab. 3) than on other ImageNet-scale datasets (see Tab. A6; 1-crop). This result is expected because ImageNet-Hard is a set of hard cases collected from those OOD benchmarks.

**ImageNet-Hard-4K** We find that when upsampling images to 4K using GigaGAN  and downsampling them back to the resolution of each classifier does not help but even hurt the accuracy slightly (Tab. A13). Given that GigaGAN performs remarkably well, this result suggests ImageNet-Hard is different from typical fine-grained animal classification where improving the texture details increases classification accuracy . The next section (Sec. 4.5) sheds light on model failures on ImageNet-Hard, revealing challenges posed to future SOTA models.

### Analysis of Image Classification Errors

Motivated by the fact that EfficientNet-L2 is the best classifier on ImageNet-Hard, we qualitatively analyze its failure cases to characterize the challenge posed by our benchmark. Specifically, we provide gpt-3.5-turbo  with a pair of EfficientNet-L2's top-1 (incorrect) label and the groundtruth label and ask it to categorize the error into "common" or "rare" based on the labels' semantic similarity (see Appendix E.3 for full details). For instance, mislabeling bucket into barrel is common (as two objects are quite related) while mislabeling cloak into jigsaw puzzle is rare.

**Results** See Appendix E.4 for samples of wrong labels that EfficientNet-L2 most frequently misclassifies into. We find that 39.4% of EfficientNet-L2's misclassifications on the ImageNet-Hard dataset are "common", while 60.6% are "rare".

**A. Common** group captures model confusion between two related classes (e.g. two fish species: clownfish and rock beauty; Fig. 4(a)). Yet, another source of problem for these "errors" is the debatable groundtruth labels, which may require domain-expert annotators to verify and rectify .

**B. Rare** group captures errors where the model confusion is between two semantically distant classes (_e.g._, llama\(\)pectrum; Fig. 4(b)). This often happens with abstract images or objects in unusual

Figure 5: ImageNet-Hard samples misclassified by EfficientNet-L2@800px can be categorized into two groups: (a) **Common**: the top-1 label is related to the groundtruth label; and (b) **Rare**: the top-1 label is semantically far from the groundtruth label. See Figs. A56 and A57 for more samples.

   Classifier & Accuracy & Classifier & Accuracy & Classifier & Accuracy \\  AlexNet & 7.34 & ViT-B/32 & 18.52 & CLIP-ViT-L/14@224px & 1.86 \\ VGG-16 & 12.00 & EfficientNet-B0@224px & 16.57 & CLIP-ViT-L/14@336px & 2.02 \\ ResNet-18 & 10.86 & EfficientNet-B7@600px & 23.20 & OpenCLIP-ViT-bigG-14 & 15.93 \\ ResNet-50 & 14.74 & EfficientNet-L2@800px & 39.00 & OpenCLIP-ViT-L-14 & 15.60 \\   

Table 3: Top-1 accuracy (%) on ImageNet-Hard of IN-trained models and those trained on larger, non-ImageNet datasets (black). All models operate at 224\(\)224 unless otherwise specify.

poses  or forms . Classifying this group of images is challenging and sometimes requires a strong understanding of context and reasoning capabilities.

## 5 Discussion and Conclusion

LimitationsBy manual inspection, we estimate 14.7% of labeling noise, which ImageNet-Hard inherits from the source datasets.

Our study rigorously analyzed the zooming effect on six known classifiers and image classification benchmarks. We first demonstrate that previous state-of-the-art classifiers, as old as AlexNet , could potentially achieve near \(90\%\) accuracy with optimal zooming. This sparks the intriguing question of whether image classifiers' evolution over the past ten years is about mastering where and at what scale to zoom (instead of enhancing feature extractors, a.k.a. representation learning ). Through another lens, we probe the evolution by analyzing the implicit zooming mechanisms that deep classifiers apply to input images. This perspective diverges from , which studied the progression of representation learning from CNNs to ViTs.

We are the first to document the spatial biases of existing benchmarks. Notably, IN-A and ON contain a large center bias and simply zooming to the center will de-clutter the scene and yield a high accuracy (\(24.69\%\) for ViT-B/32 on IN-A; Tab. A5), which is competitive with state-of-the-art trained models (_e.g._\(24.1\%\) of Robust ViT ) and much higher than state-of-the-art TTA techniques (_e.g._\(11.21\%\) of MEMO ; Tab. 2). Our simple, but strong zoom-in baselines on IN-A and ON motivate future research into better-controlled benchmarks that more explicitly test models on a set of pre-defined properties. Our proposed TTA method with zoom-in transforms (MEMO + RRC) is not only more accurate but also more interpretable and faster to run (Tab. A7) than the original MEMO.

Finally, we introduce ImageNet-Hard (Sec. 4.4), a new challenging dataset for SOTA IN-trained and vision-language classifiers.