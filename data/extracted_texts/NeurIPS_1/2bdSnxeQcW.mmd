# Exclusively Penalized Q-learning for Offline Reinforcement Learning

Junghyuk Yeom\({}^{*}\) Yonghyeon Jo\({}^{*}\) Jungmo Kim Sanghyeon Lee Seungyul Han\({}^{}\)

Graduate School of Artificial Intelligence

UNIST

Ulsan, South Korea 44919

{junghyukyum,yonghyeonjo,jmkim22,sanghyeon,syhan}@unist.ac.kr

###### Abstract

Constraint-based offline reinforcement learning (RL) involves policy constraints or imposing penalties on the value function to mitigate overestimation errors caused by distributional shift. This paper focuses on a limitation in existing offline RL methods with penalized value function, indicating the potential for underestimation bias due to unnecessary bias introduced in the value function. To address this concern, we propose Exclusively Penalized Q-learning (EPQ), which reduces estimation bias in the value function by selectively penalizing states that are prone to inducing estimation errors. Numerical results show that our method significantly reduces underestimation bias and improves performance in various offline control tasks compared to other offline RL methods.

## 1 Introduction

+
Footnote â€ : \(*\) indicates equal contribution and \(\) indicates the corresponding author: Seungyul Han.

Special thanks to Whiyoung Jung from LG AI Research for providing experimental data used in this work.

the \(Q\)-function for policy actions and provides a bonus to the \(Q\)-function for actions in the dataset. Consequently, CQL selects more actions from the dataset, effectively reducing overestimation errors without policy constraints.

While CQL has demonstrated outstanding performance across various offline tasks, we observed that it introduces unnecessary estimation bias in the value function for states that do not contribute to overestimation. This issue becomes more pronounced as the level of penalty increases, resulting in performance degradation. To address this issue, this paper introduces a novel Exclusively Penalized Q-learning (EPQ) method for efficient offline RL. EPQ imposes a threshold-based penalty on the value function exclusively for states causing estimation errors to mitigate overestimation bias without introducing unnecessary bias in offline learning. Experimental results demonstrate that our proposed method effectively reduces both overestimation bias due to distributional shift and underestimation bias due to the penalty, allowing a more accurate evaluation of the current policy compared to the existing methods. Numerical results reveal that EPQ significantly outperforms other state-of-the-art offline RL algorithms on various D4RL tasks .

## 2 Preliminaries

### Markov Decision Process and Offline RL

We consider a Markov Decision Process (MDP) environment denoted as \(:=(,,P,R,)\), where \(\) is the state space, \(\) is the action space, \(P\) represents the transition probability, \(\) is the discount factor, and \(R\) is the bounded reward function. In offline RL, transition samples \(d_{t}=(s_{t},a_{t},r_{t},s_{t+1})\) are generated by a behavior policy \(\) and stored in the dataset \(D\). We can empirically estimate \(\) as \((a|s)=\), where \(N\) represents the number of data points in \(D\). We assume that \(_{s D,a}[f(s,a)]_{s D,a}[f(s,a)]=_{s,a D}[f(s,a)]\) for arbitrary function \(f\). Utilizing only the provided dataset without interacting with the environment, our objective is to find a target policy \(\) that maximizes the expected discounted return, denoted as \(J():=_{s_{0},a_{0},s_{1},}[G_{0}]\), where \(G_{t}=_{l=t}^{}^{l-t}R(s_{l},a_{l})\) represents the discounted return.

### Distributional Shift Problem in Offline RL

In online RL, the optimal policy that maximizes \(J()\) is found through iterative policy evaluation and policy improvement [2; 3]. For policy evaluation, the action value function is defined as \(Q^{}(s_{t},a_{t}):=_{s_{t},a_{t},s_{t+1},}[_{l=t }^{}^{l-t}R(s_{l},a_{l})|s_{t},\;a_{t}]\). \(Q^{}\) can be estimated by iteratively applying the Bellman operator \(^{}\) to an arbitrary \(Q\)-function, where \((^{}Q)(s,a):=R(s,a)+_{s^{} P(|s,a ),\;a^{}(|s^{})}[Q(s^{},a^{})]\). The \(Q\)-function is updated to minimize the Bellman error using the dataset \(D\), given by \(_{s,a D}[(Q(s,a)-^{}Q(s,a))^{2}]\). In offline RL, samples are generated by the behavior policy \(\) only, resulting in estimation errors in the \(Q\)-function for policy actions not present in the dataset \(D\). The policy \(\) is updated to maximize the \(Q\)-function, incorporating the estimation error in the policy improvement step. This process accumulates positive bias in the \(Q\)-function as iterations progress .

### Conservative \(Q\)-learning

To mitigate overestimation in offline RL, conservative Q-learning (CQL)  penalizes the \(Q\)-function for the policy actions \(a\) and increases the \(Q\)-function for the data actions \(a\) while minimizing the Bellman error, where the \(Q\)-loss function of CQL is given by

\[_{s,a,s^{} D}[(Q(s,a)-^{ }Q(s,a))^{2}]+_{s D}[_{a}[ Q(s,a)]-_{a}[Q(s,a)]], \]

where \( 0\) is a penalizing constant. From the value update in (1), the average \(Q\)-value of data actions \(_{a}[Q(s,a)]\) becomes larger than the average \(Q\)-value of target policy actions \(_{a}[Q(s,a)]\) as \(\) increases. As a result, the policy will tend to choose the data actions more from the policy improvement step, effectively reducing overestimation error in the \(Q\)-function .

Methodology

### Motivation: Necessity of Mitigating Unnecessary Estimation Bias

In this section, we focus on the penalization behavior of CQL, one of the most representative penalty-based offline RL methods, and present an illustrative example to show that unnecessary estimation bias can occur in the \(Q\)-function due to the penalization. As explained in Section 2.3, CQL penalizes the \(Q\)-function for policy actions and increases the \(Q\)-function for data actions in (1). When examining the \(Q\)-function for each state-action pair \((s,a)\), the \(Q\)-value increases if \((a|s)>(a|s)\); otherwise, the \(Q\)-value decreases as the penalizing constant \(\) becomes sufficiently large .

To visually demonstrate this, Fig. 1 depicts histograms of the fixed policy \(\) and the estimated behavior policy \(\) for various \(\) and \(\) at the initial state \(s_{0}\) on the Pendulum task with a single-dimensional action space in OpenAI Gym tasks , as cases (a), (b), and (c), along with the estimation bias in the \(Q\)-function for CQL with various penalizing factors \(\). In this example, for all states except the initial state, we consider \(==(-2,2)\). In each case, CQL only updates the \(Q\)-function with its penalty to evaluate \(\) in an offline setup, as shown in equation (1), and we plot the estimation bias of CQL, which represents the average difference between the learned \(Q\)-function and the expected return \(G_{0}\).

From the results in Fig. 1, we observe that CQL suffers from unnecessary estimation bias in the \(Q\)-function for cases (a) and (b). In both cases, the histograms illustrate that policy actions are fully contained in the dataset \(\), suggesting that the estimation error in the Bellman update is unlikely to occur even without any penalty. However, CQL introduces a substantial negative bias for actions near \(0\) where \((0|s_{0})>(0|s_{0})\) and a positive bias for other actions. Furthermore, the bias intensifies as the penalty level \(\) increases. In order to mitigate this bias, reducing the penalty level \(\) to zero may seem intuitive in cases like Fig. 1(a) and Fig. 1(b). However, such an approach would be inadequate in cases like Fig. 1(c). In this case, because policy actions close to 0 are rare in the dataset, penalization is necessary to address overestimation caused by estimation errors in offline learning. Furthermore, this problem may become more severe in actual offline learning situations, as the policy continues to change as learning progresses, compared to situations where a fixed policy is assumed.

### Exclusively Penalized Q-learning

To address the issue outlined in Section 3.1, our goal is to selectively give a penalty to the \(Q\)-function in cases like Fig. 1(c), where policy actions are insufficient in the dataset while minimizing unnecessary bias due to the penalty in scenarios like Fig. 1(a) and Fig. 1(b), where policy actions are sufficient in the dataset. To achieve this goal, we introduce a novel exclusive penalty \(_{}\) defined by

\[_{}:=^{,}(s)}_{}(a|s)}-1)}_{}, \]

where \(f_{}^{,}(s)=_{a(|s)}[x_{}^{}]\) is a penalty adaptation factor for a given \(\) and policy \(\). Here, \(x_{}^{}=(1.0,(-((a|s)-)))\) represents the amount of adaptive penalty that is reduced as log \(\) exceeds the threshold \(\). Thus, the adaptation factor \(f_{}^{,}\) indicates the average penalty that policy actions should receive. If the probability of estimated behavior policy \(\) for policy actions exceeds the threshold \(\), i.e., policy actions are sufficiently present in the dataset, then \(x_{}^{}\) will be

Figure 1: Histograms of \(\) and \(\) (left axis), and the estimation bias of CQL with various \(\) (right axis) at \(s_{0}\) for three cases: (a) \(=(-2,2)\) and \(=N(0,0.2)\) (b) \(=N(-1,0.3)+N(1,0.3)\) and \(=N(1,0.2)\) (c) \(=N(-1,0.3)+N(1,0.3)\) and \(=N(0,0.2)\), where \((-2,2)\) represents a uniform distribution and \(N(,)\) denotes a Gaussian distribution with mean \(\) and standard deviation \(\).

smaller than 1 and reduce the amount of penalty as much as the amount by which \(\) exceeds the threshold \(\) to avoid unnecessary bias introduced in Section 3.1. Otherwise, it will be 1 due to \((1.0,)\) to maintain the penalty since policy actions are insufficient in the dataset. The latter penalty term \((a|s)}-1\), positive if \((a|s)>(a|s)\) and otherwise negative, imposes a positive penalty on the \(Q\)-function when \((a|s)>(a|s)\), and otherwise, it increases the \(Q\)-function since the penalty is negative, as the Q-penalization method considered in CQL .

To elaborate further on our proposed penalty, Fig. 2(a) depicts the log-probability of \(\) and the thresholds \(\) used for penalty adaptation, with \(N\) representing the number of data points. In Fig. 2(a), if the log-probability \(\) of an action \(a\) exceeds the threshold \(\), this indicates that the action \(a\) is sufficiently represented in the dataset. Thus, we reduce the penalty for such actions. Furthermore, as shown in Fig. 2(a), when the number of actions increase from \(N_{1}\) to \(N_{2}\), the threshold for determining "enough data" decreases from \(_{1}\) to \(_{2}\), even if the data distribution remains unchanged.

Furthermore, to explain the role of the threshold \(\) in the proposed penalty \(_{}\), we consider two thresholds, \(_{1}\) and \(_{2}\). In Fig. 2(b), which illustrates the proposed penalty adaptation factor \(f_{_{1}}^{,}\) and \(f_{_{2}}^{,}\) for thresholds \(_{1}\) and \(_{2}\), \(x_{_{1}}^{}\) is larger than \(x_{_{2}}^{}\) because \(_{1}>_{2}\). As a result, in the case of \(_{1}\), \(_{_{1}}\) only reduces the penalty for \(_{3}\). In other words, \(f_{_{1}}^{_{1},}=f_{_{1}}^{_{2},}=1,\) and \(f_{_{1}}^{_{3},}<1\). On the other hand, as the number of data samples increases from \(N_{1}\) to \(N_{2}\), more actions generated by the behavior policy \(\) will be stored in the dataset, so policy actions are more likely to be in the dataset. In this case, the threshold should be lowered from \(_{1}\) to \(_{2}\). As a result, \(\) exceeds the threshold \(_{2}\) in the support of all policies \(_{i}\), and \(_{_{2}}\) reduces the penalty in the support of all policies \(_{i}\), i.e., \(f_{_{2}}^{_{3},}<f_{_{2}}^{_{1},}<f_{_ {2}}^{_{2},}<1\). Thus, even without knowing the exact number of data samples, the proposed penalty \(_{}\) allows adjusting the penalty level appropriately according to the given number of data samples based on the threshold \(\).

Now, we propose exclusively penalized Q-learning (EPQ), a novel offline RL method that minimizes the Bellman error while imposing the proposed exclusive penalty \(_{}\) on the \(Q\)-function as follows:

\[_{Q}\ _{s,a,s^{} D}[(Q(s,a)-\{^{ }Q(s,a)-_{}\})^{2}]. \]

Then, we can prove that the final \(Q\)-function of EPQ underestimates the true value function \(Q^{}\) in offline RL if \(\) is sufficiently large, as stated in the following theorem. This indicates that the proposed EPQ can successfully reduce overestimation bias in offline RL, while simultaneously alleviating unnecessary bias based on the proposed penalty \(_{}\).

**Theorem 3.1**.: _We denote the \(Q\)-function converged from the \(Q\)-update of EPQ using the proposed penalty \(_{}\) in (3) by \(^{}\). Then, the expected value of \(^{}\) underestimates the expected true policy value, i.e., \(_{a}[^{}(s,a)]_{a}[Q^{}(s,a )], s D\), with high probability \(1-\) for some \((0,1)\), if the penalizing factor \(\) is sufficiently large. Furthermore, the proposed penalty reduces the average penalty for policy actions compared to the average penalty of CQL._

Figure 2: An illustration of our exclusive penalty: (a) The log-probability of \(\) and the thresholds \(_{1}\) and \(_{2}\) according to the number of data samples \(N_{1}\) and \(N_{2}\), where \(N_{1}<<N_{2}\). (b) The penalty adaptation factor \(f_{}^{,}\) which represents the amount of adaptive penalty, indicating how much \(\) exceeds the threshold \(\). Three different policies \(_{i},\ i=1,2,3\), are considered.

**Proof)** Proof of Theorem 3.1 is provided in Appendix A.

In order to demonstrate the \(Q\)-function convergence behavior of the proposed EPQ in more detail, we revisit the previous Pendulum task in Fig. 1. Fig. 4 shows the histogram of \(\) and the penalty adaptation factor \(f_{}^{,}(s)\) for Gaussian policy \(=N(,0.2)\), where \(\) varies from \(-2\) to \(2\), with varying \(\). In Fig. 4(a), \(f_{}^{,}(s)\) should be less than 1 for any policy mean \(\) since all policy actions are sufficient in the dataset. In 3(b), \(f_{}^{,}(s)\) is less than 1 only if the \(\) probability near the policy mean \(\) is high, and otherwise, \(f_{}^{,}(s)\) is 1, which indicates the lack of policy action in the dataset. Thus, the result shows that \(f_{}^{,}(s)\) reflects our motivation in Section 3.1 well. Moreover, Fig. 4 compares the estimation bias curves of CQL and EPQ with \(=10\) in the scenarios presented in Fig. 1. CQL exhibits unnecessary bias for situations in Fig. 4(a) and Fig. 4(b) where no penalty is needed, as discussed in Section 3.1. Conversely, our proposed method effectively reduces estimation bias in these cases while appropriately maintaining the penalty for the scenario in Fig. 4(c) where penalization is required. This experiment demonstrates the effectiveness of our proposed approach, and the subsequent numerical results in Section 4 will numerically show that our method significantly reduces estimation bias in offline learning, resulting in improved performance.

### Prioritized Dataset

In Section 3.2, EPQ effectively controls the penalty in the scenarios depicted in Fig. 4. However, in cases where the policy is highly concentrated on one side, as shown in Fig. 4, the estimation bias may not be completely eliminated due to the latter penalty term \(}-1\) in \(_{}\), as \(\) significantly exceeds \(\). This situation, detailed in Fig. 5, arises when there is a substantial difference in the \(Q\)-function values among data actions. As the policy is updated to maximize the \(Q\)-function, the policy shifts towards the data action with a larger \(Q\), resulting in a more significant penalty for CQL. To furtheralleviate the penalty to reduce unnecessary bias in this situation, instead of applying a penalty based on \(\), we introduce a penalty based on the prioritized dataset (PD) \(^{Q}(Q)\). As shown in Fig. 5(a), which illustrates the difference between the original data distribution \(\) and the modified data distribution \(^{Q}\) after applying PD, \(^{Q}\) prioritizes data actions with higher \(Q\)-values within the support of \(\). According to Fig. 5(a), when the policy \(\) focuses on specific actions, the penalty \(-1\) increases significantly, as depicted in Fig. 5(b). In contrast, by applying PD, \(\) is adjusted to approach \(^{Q}(Q)\), aligning the data distribution more closely with the policy \(\). Consequently, we anticipate that the penalty will be significantly mitigated, as the difference between \(\) and \(^{Q}\) is much smaller than the difference between \(\) and \(\). Following this intuition, we modify our penalty using PD as \(_{}\), \({}_{PD}:=f_{}^{,}(s)((a| s)}-1)\). It is important to note that the penalty adaptation factor \(f_{}^{,}(s)\) remains unchanged since we use all data samples in the dataset for \(Q\) updates. Additionally, we consider the prioritized dataset for the Bellman update to focus more on data actions with higher \(Q\)-function values for better performance as considered in . Then, we can derive the final \(Q\)-loss function of EPQ with PD as

\[L(Q)=_{s,s^{} D,a^ {Q}}[(Q-\{^{}Q-_{,\ PD}\})^{2}] \] \[=_{s,s^{} D,a,a^{} }[w_{s,a}^{Q}\{(Q(s,a)-^{}Q(s, a))^{2}+ f_{}^{,}(s)(Q(s,a^{})-Q(s,a)) \}]+C,\]

where \(w_{s,a}^{Q}=^{Q}(a|s)}{(a|s)}=_{s^{}(|s)}(Q(s,a^{}))]}\) is the importance sampling (IS) weight, \(C\) is the remaining constant term, and the detailed derivation of (4) is provided in Appendix B.1. The ablation study in Section 4.3 will show that EPQ performs better when prioritized dataset \(^{Q}\) is considered.

### Practical Implementation and Algorithm

Now, we propose the implementation of EPQ based on the value loss function (4). Basically, our implementation follows the setup of CQL . For policy, we utilize the Gaussian policy with a Tanh\(()\) layer proposed by Haarnoja et al.  and update the policy to maximize the \(Q\)-function with its entropy. Then, the policy loss function is given by

\[L()=_{s D,\ a}[-Q(s,a)+(a|s)]. \]

Based on the \(Q\)-update in (4) and the policy loss function (5), we summarize the algorithm of EPQ as Algorithm 1. More detailed implementation, including the calculation method of the IS weight \(w_{s,a}^{Q}\) and redefined loss functions for the parameterized \(Q\) and \(\), is provided in Appendix B.2.

```
1: Offline dataset \(D\)
2: Train the behavior policy \(\) based on behavior cloning (BC)
3: Initialize \(Q\) and \(\)
4:for gradient step \(k=0,1,2,3,\)do
5: Sample batch transitions \(\{(s,a,r,s^{})\}\) from \(D\).
6: Calculate the penalty adaptation factor \(f_{}^{,}(s)\) and IS weight \(w_{s,a}^{Q}\)
7: Compute losses \(L(Q)\) in Equation (4) and \(L()\) in Equation (5)
8: Update the policy \(\) to minimize \(L()\)
9: Update the \(Q\)-function \(Q\) to minimize \(L(Q)\)
10:endfor
```

**Algorithm 1** Exclusively Penalized Q-learning

## 4 Experiments

In this section, we evaluate our proposed EPQ against other state-of-the-art offline RL algorithms using the D4RL benchmark , commonly used in the offline RL domain. Among various D4RL tasks, we mainly consider Mujoco locomotion tasks, Adroit manipulation tasks, and AntMaze navigation tasks, with scores normalized from \(0\) to \(100\), where \(0\) represents random performance and \(100\) represents expert performance.

[MISSING_PAGE_FAIL:7]

, as well as reported results from other baseline algorithms according to Ma et al. . For CQL, reproducing its performance is challenging, so we also include reproduced CQL performance labeled as CQL (reprod.) from Ma et al. . Any missing experimental results have been filled in by re-implementing each baseline algorithm. For our algorithm, we explored various penalty control thresholds \(\{c,\;c\}\), where \(\) represents the log-density of \(()\). For Mujoco tasks, the EPQ penalizing constant is fixed at \(=20.0\), and for Adroit tasks, we consider either \(=5.0\) or \(=20.0\). To ensure robustness, we run our algorithm with four different seeds for each task. Table 1 displays the average normalized returns and corresponding standard deviations for compared algorithms. The performance of EPQ is based on the best hyperparameter setup, with additional results presented in the ablation study in Section 4.3. Further details on the hyperparameter setup are provided in Appendix C.

The results in Table 1 shows that our algorithm significantly outperforms the other constraint-based offline RL algorithms in all considered tasks. In particular, in challenging tasks such as Adroit tasks and AntMaze tasks, where rewards are sparse or intermittent, EPQ demonstrates remarkable performance improvements compared to recent offline RL methods. This is because EPQ can impose appropriate penalty on each state, even if the policy and behavior policy varies depending on the timestep as demonstrated in Section 3.2. Also, we observe that our proposed algorithm shows a large increase in performance in the 'Hopper-random', 'Hopper-medium', and 'Halfcheetah-medium' environments compared to CQL, so we will further analyze the causes of the performance increase in these tasks in the following section. For adroit tasks, the performance of CQL (reprod.) is too low compared to CQL (paper), so we provide the enhanced version of CQL in Appendix E, but the result in Appendix E shows that EPQ still performs better than the enhanced version of CQL.

### The Analysis of Estimation Bias

In Section 4.1, EPQ outperforms CQL baselines significantly across various D4RL tasks based on our proposed penalty in Section 3. To analyze the impact of our overestimation reduction method on performance enhancement, we compare the estimation bias for EPQ and CQL baselines with various penalizing constants \(\{0,1,5,10,20\}\) on 'Hopper-random', 'Hopper-medium', and 'Halfcheetah-medium' tasks. In Fig. 6(a), we depict the squared value of estimation bias, obtained from the difference between the \(Q\)-value and the empirical average return for sample trajectories generated by the policy, to show both overestimation bias and underestimation bias. In the experiment shown in Fig. 6(a), the estimation bias in CQL with \(=0\) became excessively large, causing the gradients to explode and resulting in forced termination of the training. Fig. 6(b) illustrates the corresponding normalized average returns, emphasizing learning progress after \(200\)k gradient steps.

Figure 6: Analysis of proposed method

In Fig. 6(a), we observe an increase in estimation bias for CQL as the penalizing constant \(\) rises, attributed to unnecessary bias highlighted in Fig. 1. Reducing \(\) to nearly 0 in CQL, however, fails to effectively mitigate overestimation error, leading to a divergence of the \(Q\)-function in tasks such as 'Hopper-random' and 'Hopper-medium', as shown in Fig. 1. Conversely, EPQ demonstrates superior reduction of estimation bias in the \(Q\)-function compared to CQL baselines for all tasks in Fig. 6(a), indicating its capability to mitigate both overestimation and underestimation bias based on the proposed penalty. As a result, Fig. 6(b) shows that EPQ significantly outperforms all CQL variants on 'Hopper-random', 'Hopper-medium', and 'Halfcheetah-medium' tasks.

### Ablation Study

To understand the impact of EPQ's components and hyperparameters, we conduct ablation studies to evaluate each component and the penalty control threshold \(\) on the 'Hopper-random', 'Hopper-medium', and 'HalfCheetah-medium' tasks where our proposed method showed a significant performance improvement compared to the baseline CQL.

**Component Evaluation:** In Section 3, we introduced two variants of the EPQ algorithm: EPQ (w/o PD), which does not incorporate a prioritized dataset as in equation (3), and EPQ (with PD), which leverages a prioritized dataset based on \(^{Q}\) as in equation (4). In Fig. 7(a), we compare the performance of EPQ (w/o PD), EPQ (with PD), and the CQL baseline to analyze the impact of each component. EPQ (w/o PD) still outperforms CQL, demonstrating that the proposed penalty \(_{}\) in Section 3.2 enhances performance by efficiently reducing overestimation without introducing unnecessary estimation bias, as discussed in Section 3.2. Additionally, Fig. 7(a) shows that EPQ (with PD) outperforms EPQ (w/o PD) significantly in the HalfCheetah-medium task, indicating that the proposed prioritized dataset contributes to improved performance, as anticipated in Section 3.3.

**Penalty Control Threshold \(\):** As discussed in Section 3.2, EPQ can dynamically control the penalty amount based on the penalty control threshold \(\), as illustrated in Fig. 2, even in the absence of knowledge about the exact number of data samples. Fig. 7(b) demonstrates the performance of EPQ with various penalty control thresholds \([0.2,0.5,1.0,2.0,5.0,10.0]\), where \(\) represents the log-density of \(()\). Note that \(\) is negative, so \(=10.0\) is the lowest threshold while \(=0.2\) is the highest. The results indicate that in tasks like Hopper-medium, where a variety of actions are not sufficiently sampled, a higher threshold performs better. Conversely, in tasks like Hopper-random, where a broad range of actions is sampled, a lower threshold is more effective. An exception is the HalfCheetah-medium task, which, despite having fewer action variations, visits a diverse range of states. This may result in lower overestimation errors for OOD

Figure 7: Additional ablation studies on the Hopper-random, Hopper-medium, and Halfcheetah-medium tasks are presented. The best hyperparameter in the paper is denoted by the orange curve.

actions, benefiting from a lower threshold. Furthermore, the performance on the considered tasks appears to be surprisingly less sensitive to changes in \(\). We initially expect that performance might be sensitive to \(\) since it reflects the fixed number of data samples, but the results indicate that the performance is not significantly affected by variations in \(\). Moreover, EPQ algorithms with different \(\) consistently outperform the CQL baseline, highlighting the superiority of the proposed method.

## 5 Related Works

### Constraint-based Offline RL

In order to reduce the overestimation in offline learning, several constraint-based offline RL methods have been studied. Fujimoto et al.  propose a batch-constrained policy to minimize the extrapolation error, and Kumar et al. , Wu et al.  limits the distribution based on the distance of the distribution, rather than directly constraining the policy. Fujimoto and Gu  restricts the policy actions to batch data based on the online algorithm TD3 . Furthermore, Kumar et al. , Yu et al.  aims to minimize the probability of out-of-distribution actions using the lower bound of the true value. By predicting a more optimistic cost for tuples within the batch data, Xu et al.  provides stable training for offline-based safe RL tasks. On the other hand, Ma et al.  utilizes mutual information to constrain the policy.

### Offline Learning based on Data Optimality

In offline learning setup, the optimality of the dataset greatly impacts the performance . Simply using \(n\)-\(\%\) BC, or applying weighted experiences, [33; 34] which utilize only a portion of the data based on the evaluation results of the given data, fails to exploit the distribution of the data. Based on Haarnoja et al. , Reddy et al. , Garg et al.  uses the Boltzmann distribution for offline learning, training the policy to follow actions with higher value in the imitation learning domain [38; 39]. Kostrikov et al.  and Xiao et al.  argue that the optimality of data can be improved by using expectile regression and in-sample SoftMax, respectively. Additionally, methods that learn the value function from the return of the data in a supervised manner have been proposed [41; 28; 42; 43].

### Value Function Shaping

In offline RL, imposing constraints on the policy can decrease the performance, thus Kumar et al. , Lyu et al.  impose penalties on out-of-distribution actions by structuring the learned value function as a lower bound to the actual values. Additionally, Fakoor et al.  addresses the issue by imposing a policy constraint based on divergence and suppressing overly optimistic estimations on the value function, thereby preventing excessive expansion of the value function. Moreover, Wu et al.  predicts the instability of actions through the variance of the value function, imposing penalties on the out-of-distribution actions, while Lyu et al.  replaces the \(Q\) values for out-of-distribution actions with pseudo \(Q\)-values and Agarwal et al. , An et al. , Bai et al. , Lee et al.  mitigates the instability of learning the value function by applying ensemble techniques. In addition, Ghosh et al.  interprets the changes in MDP from a Bayesian perspective through the value function, thereby conducting adaptive policy learning.

## 6 Conclusion

To mitigate overestimation error in offline RL, this paper focuses on exclusive penalty control, which selectivelys gives the penalty for states where policy actions are insufficient in the dataset. Furthermore, we propose a prioritized dataset to enhance the efficiency of reducing unnecessary bias due to the penalty. As a result, our proposed method, EPQ, successfully reduces the overestimation error arising from distributional shift, while avoiding underestimation error due to the penalty. This significantly reduces estimation bias in offline learning, resulting in substantial performance improvements across various D4RL tasks.