# \(Lv\)-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K

Tao Yuan\({}^{1}\), Xuefei Ning\({}^{2,}\), Dong Zhou\({}^{1}\), Zhijie Yang\({}^{1}\), Shiyao Li\({}^{1,2}\),

**Minghui Zhuang\({}^{1}\), Zheyue Tan\({}^{1}\), Zhuyu Yao\({}^{1}\), Dahua Lin\({}^{3,4}\),**

**Boxun Li\({}^{1,}\), Guohao Dai\({}^{1,5,}\), Shengen Yan\({}^{1}\), Yu Wang\({}^{2,}\)

\({}^{1}\) Infinigence-AI, \({}^{2}\) Tsinghua University, \({}^{3}\) Shanghai Artificial Intelligence Laboratory

\({}^{4}\) The Chinese University of Hong Kong, \({}^{5}\) Shanghai Jiao Tong University

###### Abstract

State-of-the-art large language models (LLMs) are now claiming remarkable supported context lengths of \(256k\) or even more. In contrast, the average context lengths of mainstream benchmarks are insufficient (\(5k\)-\(21k\)), and they suffer from potential knowledge leakage and inaccurate metrics, resulting in biased evaluation. This paper introduces \(LV\)-Eval, a challenging long-context benchmark with five length levels (\(16k\), \(32k\), \(64k\), \(128k\), and \(256k\)) reaching up to \(256k\) words. \(LV\)-Eval features two main tasks, single-hop QA and multi-hop QA, comprising 11 bilingual datasets. The design of \(LV\)-Eval has incorporated three key techniques, namely confusing facts insertion (CFI), keyword and phrase replacement (KPR), and keyword-recall-based metric design. The advantages of \(LV\)-Eval include controllable evaluation across context lengths, challenging test instances with confusing facts, mitigated knowledge leakage, and more objective evaluation. We evaluate 12 LLMs on \(LV\)-Eval and conduct ablation studies on the benchmarking techniques. The results reveal that: (i) Commercial LLMs generally outperform open-source LLMs when evaluated within length levels shorter than their claimed context length. However, their overall performance is surpassed by open-source LLMs with longer context lengths. (ii) Extremely long-context LLMs, such as Yi-6B-200k and Llama3-8B-1M, exhibit a relatively gentle degradation of performance, but their absolute performances may not necessarily be higher than those of LLMs with shorter context lengths. (iii) LLMs' performances can significantly degrade in the presence of confusing information, especially in the pressure test of "needle in a haystack". (iv) Issues related to knowledge leakage and inaccurate metrics introduce bias in evaluation, and these concerns are alleviated in \(LV\)-Eval. All datasets and evaluation codes are released at: [https://github.com/infinigence/LVEval](https://github.com/infinigence/LVEval).

## 1 Introduction

Large language models (LLMs) have demonstrated exceptional performance on a variety of natural language processing tasks. The ability of long-context understanding is crucial for LLMs to deal with tasks based on longer contexts, such as books, lengthy chat history, and so on. Recently, extensive efforts have been devoted in enlarging the supported context length (i.e., the number of tokens that the model can accept as input) of LLMs. These efforts have pushed the supported context length of LLMs from \(2k\) tokens to \(32k\) tokens [1; 2; 3; 4; 5], and some models have achieved a remarkable context length of \(128k\) and \(200k\)[6; 7; 8].

In contrast to the rapid evolution of the models' supported context length, existing benchmarks have lagged behind. The average word count in current long-context benchmarks typically falls within the range of \(32k\), considerably shorter compared to the supported context lengths of state-of-the-art long-context models. Moreover, previous benchmarks primarily consist of _unaltered_ public documents and articles. This could be problematic for two reasons: (i) the data might be involved in LLMs' training processes, and (ii) the facts within them might be common-sense facts found in other training resources. The presence of this issue, known as "knowledge leakage" , can lead to models answering questions with memorization or common-sense knowledge instead of understanding long-range contexts. Last but not least, the automatic metrics employed in most of the existing benchmarks are susceptible to the variations in answer format and the inclusion of irrelevant words. Such metrics struggle to accurately assess the answer quality.

To address these issues, we propose _LV_-Eval, a bilingual benchmark with up to \(256k\) words. _LV_-Eval incorporates distractions and confusions to make the test more challenging, replaces keywords and rephrases sentences to prevent knowledge leakage, and employs a more accurate metric. We summarizes the key characteristics of _LV_-Eval as follows:

* **Sufficiently long context length to evaluate state-of-the-art models**: _LV_-Eval comprises 5 length levels with word counts of \(16k\), \(32k\), \(64k\), \(128k\), and \(256k\). Test instances across these levels share the same set of question-answer (QA) pairs, and only differ in the context content and length. Testing on the same QA pairs with different context lengths facilitates a controllable evaluation of models' long-context ability.
* **Incorporation of distraction and confusion to increase difficulty**: When constructing the context for each test instance, we mix up distracting documents and supporting documents. This approach evaluates the model's ability in pinpointing key information in a large bunch of distracting texts. In addition, we insert confusing facts generated by GPT-4 and revised by human annotators into the context. This assesses the model's capability to accurately reason in the presence of interference.
* **Keyword and phrase replacement to mitigate knowledge leakage**: To mitigate the biased evaluation of long-context ability caused by knowledge leakage, we replace the keywords and phrases in the context and QA pairs. The replacement rules are annotated by human annotators. In this way, _LV_-Eval requires LLMs to rely on the understanding of context to answer questions rather than relying on memorization or common-sense knowledge.
* **Keyword-recall-based metric for more objective scoring**: Existing \(N\)-gram metrics such as the F1 score are sensitive to the format variations and non-informative words in the answer, which results in inaccurate scores. To address this, we manually annotate answer keywords and a blacklist of unrelated words. The golden answers are the critical words or sentences extracted from original ground-truth (GT) answers, while the word blacklist contains common and non-informative words such as 'the', 'a', 'of', and so on. The metric calculation follows a two-stage procedure: the first stage calculates the recall of golden answer keywords. if the recall exceeds a certain threshold, the second stage will remove all the blacklisted words and then calculate the F1 score between the prediction and the GT answer. This metric design can get scores with higher objectivity.

Findings.We evaluate 12 LLMs on _LV_-Eval and summarize the main findings as follows: (i) Commercial LLMs generally outperform open-source LLMs when evaluated within length levels shorter than their claimed context length. However, their overall performance is surpassed by

 Benchmark & \#Datasets & Avg \#Words & Min/Max Words & Length Levels & Opt. Metric & Lang. \\  ZeroSCROLLS  & 10 & 13,556 & 1,023/320,774 & none & & en \\ LoGLE  & 7 & 21,247 & 10,927/246,182 & none & & en \\ L-Eval  & 20 & 12,993 & 2,119/170,256 & none & & en \\ BAMBOO  & 10 & 5,067 & 229/14,858 & \(4k\),\(4k\),\(16k\) & & en+zh \\ LongBench  & 21 & 9,486 & 128/71,954 & \(0\)–\(4k\),\(4k\)–\(8k\),\(8k\)+ & & en+zh \\  _LV_-Eval & 11 & 102,380 & 11,896/387,406 & \(16k\),\(32k\),\(64k\),\(128k\),\(256k\) & & en+zh \\  

Table 1: Comparison of different long-context benchmarks. We count the number of words for the English datasets and the number of characters for the Chinese datasets. The punctuation marks are taken into account, while tabs, blank spaces, and newlines are not included.

open-source LLMs with longer context lengths. (ii) Extremely long-context LLMs, such as Yi-6B-200k and Llama3-8B-1M, exhibit a relatively gentle degradation of performance, but their absolute performances may not necessarily be higher than those of LLMs with shorter context lengths. (iii) LLMs' performances can significantly degrade in the presence of confusing information, especially in the pressure test of "needle in a haystack". (iv) Issues related to knowledge leakage and inaccurate metrics introduce bias in evaluation, and these concerns are alleviated in \(LV\)-Eval.

## 2 Related Work

Long-Context Benchmarks.Table 1 provides a summary of existing long-context benchmarks, including ZeroScrolls , LooGLE , L-Eval , BAMBOO , and LongBench . ZeroScrolls, LooGLE, and L-Eval are monolingual benchmarks without explicit length level partition. Their average word counts are \(\)14\(k\), \(\)21\(k\) and \(\)13.5\(k\), respectively. In order to evaluate the model's capability across various context lengths, BAMBOO and LongBench have designed various length levels. However, the word counts (\(\)5\(k\), \(\)9.5\(k\)) of the contexts in these two benchmarks are notably smaller than the supported context length of state-of-the-art long-context models, making them unsuitable for evaluating the claimed extremely long-context understanding ability. In contrast, \(LV\)-Eval contains five length levels, up to \(256k\) words, each with the same set of QA pairs for controllable evaluation.

In terms of metric design, L-Eval introduces a length-instruction-enhanced metric to mitigate the undesired impact of the answer length on metric scores. Additionally, L-Eval proposes to use LLMs to assist in scoring. In \(LV\)-Eval, we ask human annotators to mark the answer keywords and create a non-informative word blacklist, and propose a two-stage metric to focus more on the answer keywords while reducing the influences of non-informative words.

Long-Context Techniques.Considerable efforts have been devoted to enhancing the long-context abilities of LLMs. One line of work focuses on making LLMs have extended context sizes without fine-tuning and behave normally on inputs longer than their training context lengths. The design and extrapolation method of the position encoding module  is crucial for this goal. Besides, several sparse attention techniques  have also been proposed to avoid model collapse. These sparse attention techniques also alleviate the quadratic complexity w.r.t. the sequence length.

There are many other strategies aimed at enabling LLMs to effectively leverage long input contexts. The most commonly utilized strategy is long-context fine-tuning . For instance, YaRN  conducts fine-tuning with \(64k\) and \(128k\) context lengths starting with Llama2-7B/13B, and Yi-6B-200k  is trained with \(200k\) context length starting with its \(4k\) variant. Other strategies include the recurrent- or memory-based architecture , and the retrieval- or summarization-based context compression techniques , and so on.

In this work, we evaluate LLMs of diverse context sizes, ranging from \(4k\) to \(200k\), most of which have incorporated advanced position encoding design and undergone long-context fine-tuning.

   Task & Dataset & CFI & \#KPR & AK & Language & \#QA pairs & \#Contexts \\   & **lic-mixup** & ✓ & & ✓ & zh & 197 & 985 \\  & **loogle-SD-mixup** & & & ✓ & en & 160 & 800 \\  & **cmrc-mixup** & & 786 & & zh & 200 & 1,000 \\  & **multifieldqa-en-mixup** & ✓ & 476 & ✓ & en & 101 & 505 \\  & **multifieldqa-zh-mixup** & ✓ & 424 & ✓ & zh & 133 & 665 \\  & **factrecall-en** & ✓ & 3 & ✓ & en & 1 & 200\(\)5 \\  & **factrecall-zh** & ✓ & 3 & ✓ & zh & 1 & 200\(\)5 \\   & **dureader-mixup** & & & & zh & 176 & 880 \\  & **loogle-CR-mixup** & & & ✓ & en & 99 & 495 \\  & **loogle-MR-mixup** & & & ✓ & en & 139 & 695 \\  & **hotpotwikiqa-mixup** & ✓ & 232 & ✓ & en & 124 & 620 \\   

Table 2: Data statistics of \(LV\)-Eval. The abbreviations “CFI”, “KPR”, “AK” stand for “Confusing Fact Insertion”, “Keyword and Phrase Replacement”, and “Answer Keywords”, respectively. “#KPR” is the number of KPR rules. Note that in **factrecall-en** and **factrecall-zh**, all QA pairs are the same across all test instances, i.e., there is only one unique QA pair for each of the two datasets.

## 3 _Lv_-Eval Benchmark

\(LV\)-Eval focuses on two types of QA tasks: single-hop QA and multi-hop QA, and is comprised of 11 QA datasets (6 in English and 5 in Chinese). The data statistics for _LV_-Eval are outlined in Table 2. Each test instance in _LV_-Eval comprises three parts: a context (\(C\)), a question (\(Q\)), and a GT answer (\(A\)), where \(C\) is a synthetic document containing the information required to answer \(Q\).

Datasets in _LV_-Eval are constructed with existing public datasets as the source, except for factrecall-en and factrecall-zh, which are constructed using the data from _PG19_ dataset and _Journey to the West_ book. Each dataset consists of five subsets of different lengths: \(16k\), \(32k\), \(64k\), \(128k\), and \(256k\). All five subsets share the same question-answer (QA) pairs, meaning there are five contexts of varying lengths for each QA pair. This allows for a controllable evaluation of models' long-context ability when testing the same set of questions with different context lengths. In total, _LV_-Eval comprises 1,729 QA pairs and 1,729\(\)5 = 8,645 synthetic contexts.

Figure 2 illustrates the construction process of _LV_-Eval. For **factrecall-en** and **factrecall-zh**, we write one QA pair for each dataset. For the rest 9 out of the 11 datasets, we first choose a specific number of QA pairs from existing QA datasets (Section 3.1). Then, for each unique QA pair, we go through three procedures to construct the context (Section 3.2):

1. **Context mixing up** (Section 3.2.1): We first construct five contexts of different lengths by mixing up supporting documents corresponding to the QA pair and several distracting documents. For **factrecall-en** and **factrecall-zh**, we mix the supporting evidence of the single QA pair with distracting documents from two books. For other datasets, the distracting documents are unrelated to the question and are chosen from the context documents corresponding to non-selected QA pairs in the same source dataset.
2. **Confusing Facts Insertion (CFI)** (Section 3.2.2): Then, in some datasets, we introduce confusing facts by generating them with GPT-4, manually revising them, and randomly inserting these into the context. These confusing facts bear similarities to the original supporting facts but are factually different, without contradicting the original information. This helps make the test instances more challenging.
3. **Keyword and Phrase Replacement (KPR)** (Section 3.2.3): Finally, to reduce the impacts of knowledge leakage on evaluation results, we manually replace some keywords and phrases in the context and the QA pairs.

When evaluating the generated answer, to mitigate the bias in existing metrics, we manually annotate the keywords in the GT answer and adjust the metric to focus more on the keywords (Section 3.3).

### Data Source and QA Pair Construction

We construct 11 datasets (see Table 2) using public data sources, including Long-instruction-en2zh , HotpotQA , 2WikiMultihopQA , DuReader , LooGLE , LongBench , CMRC 2018 , MultiFieldQA , PG-19  and the book of _Journey to the West_. The construction of QA pairs in each dataset is elaborated in Appendix A.

Figure 1: The construction process of _LV_-Eval. “CF” is short for “Confusing Fact”.

### Context Construction

#### 3.2.1 Context Mixing Up

Can the LLMs identify the key evidences to answer the target question within a long context? To assess this ability, as shown in Figure 2, \(LV\)-Eval randomly mixes the supporting documents with various distracting documents to generate five contexts of varying length for a given QA pair. For 9 out of the 11 datasets (excluding **factrecall-en** and **factrecall-zh**), the distracting documents are chosen from the contexts corresponding to the non-selected QA pairs in the source dataset. For **factrecall-en** and **factrecall-zh**, the distracting documents are extracted from the _PG-19_ dataset and the book of _Journey to the West_.

For each length level, we sample distracting documents one by one until the cumulative word count meets the desired length level. Then, we shuffle the supporting and distracting documents, prepend a string "Passage i" to the \(i\)-th document, and concatenate them to form the final context.

Note that in **hotpotwikiqa-mixup** and **dureader-mixup**, where multiple supporting documents exist for each QA pair, instead of regarding the multiple supporting documents a single unit, we disperse and shuffle all supporting and distracting documents.

#### 3.2.2 Confusing Facts Insertion

Can the LLMs identify the key evidences correctly if there are confusing facts in the context? To assess this ability, we apply CFI in **hotpotwikiqa-mixup**, **lic-mixup**, **multifieldqa-en-mixup**, **multifieldqa-zh-mixup**, **factrecall-en**, and **factrecall-zh**, which inserts similar, factually different, non-contradictory facts into the context. These facts might mislead less meticulous models, leading them to generate incorrect answers.

The generation process of the confusing facts goes as follows. Firstly, we use the question and answer as the input, and prompt GPT-4  to generate two descriptions that are close to the original fact. The prompt for GPT-4 is shown in Figure A7. Then, we ask human annotators to resolve any conflicts in the generated facts. As illustrated in Figure 3.2, the generated confusing fact "Albert Einstein was an Italian astronomer" is in conflict with the original fact. Therefore, the human annotator revise it to "Albert Beverley was an Italian astronomer". After this generation and revising process, we insert the confusing facts into a randomly picked position between two sentences in the context.

Figure 2: Steps for CFI. Firstly, we prompt GPT-4 to generate two descriptions that are close to the original fact. Then we ask human annotators to resolve any conflicts in the generated facts. For example, the first generated confusing fact "Albert Einstein was an Italian astronomer" is in conflict with the original fact and the human annotator revise it to "Albert Beverley was an Italian astronomer". Finally, the confusing facts are inserted into a randomly position in the context.

#### 3.2.3 Keyword and Phrase Replacement

Knowledge leakage is an important concern in LLM evaluation . On the one hand, the test data are usually collected from open-access sources, and we cannot fully rule out the possibility of their being involved in some LLMs' training process. On the other hand, some common-sense questions can be answered without referencing the provided context. Consequently, LLMs might rely on memorization and common-sense knowledge to answer the questions rather than fully understanding the context. This will cause inflated benchmark scores to overrate the long-context ability of models.

To mitigate the influences of knowledge leakage on the evaluation results, we conduct KPR according to manually crafted rules in **hotpotwikiqa-mixup**, **cmrc-mixup**, **multifieldqa-en-mixup**, **multifieldqa-zh-mixup**, **factrecall-en**, and **factrecall-zh**. Specifically, given a QA pair, the annotators are asked to select keywords or phrases for replacement and write a substitute for each. After the selected keywords and phrases are replaced throughout the entire context, the annotators review the modified context to check and resolve any conflicts: If there are conflicts, the annotators are asked to revise the replacement rule until all conflicts are resolved. One example of the KPR process is shown in Figure 3.2.2. See Table 2 for the statistics of the number of replacement rules.

### Metric Design

The quality evaluation of natural language generation is challenging. Current \(N\)-gram metrics, such as the F1 score, treat all words equally. The neglect of differences in word importance leads to evaluation bias. For example, in the sentence "Attention is all you need", the word "attention" carries the key information and is more important. However, the answer "Attention matters" will get a lower score than the answer "CNN is all you need", which is not what we expected. To this end, we adopt a two-stage metric calculation process.

Specifically, to evaluate an answer \(A^{}\), we first calculate the recall of several "answer keywords" in \(A^{}\). When the recall exceeds a certain threshold (0.2 for Chinese dataset, 0.4 for English datasets), we calculate the F1 score between \(A^{}\) and GT answer \(A\) as the final score for \(A^{}\). otherwise, \(A^{}\) gets a zero score. We manually annotate the answer keywords in the GT answer \(A\) for **hotpotwikiqa-mixup**, **lic-mixup**, **loogle-CR-mixup**, **loogle-MR-mixup**, **loogle-SD-mixup**, **multifieldqa-en-mixup**, and **multifieldqa-zh-mixup**. Figure 6 (a) shows an example, demonstrating how this two-stage calculation helps avoid some inflated high evaluation scores.

When calculating the F1 score between \(A^{}\) and \(A\) in the second stage, we exclude common but non-informative words like 'the, 'a', 'of', and so on. The word blacklist is constructed as follows. We first summarized the word counts in the generations of Llama2-7B-Chat-hf and ChatGLM3

Figure 3: Steps for KPR. First, given a QA pair, the annotators are asked to select keywords or phrases to replace and write a substitute for each. Then, the selected keywords and phrases are replaced throughout the context and QA pair. Finally, annotators will check the modified context. If there is any conflict, the annotators are asked to revise the replacement rule until all conflicts are resolved.

6B-32K on all datasets and chose the top 100 words that matched the GT answer most frequently. Then, we manually annotate the non-informative words from the 100 words to construct the blacklist. Figure A6 (b) shows an example of how the word blacklist aids in calibrating the evaluation scores.

## 4 Evaluation

Models and Inference.We evaluate 2 commercial and 10 open-source LLMs on _LV_-Eval. Their information is summarized in Table A4. We follow the official implementation of all LLMs to conduct their inferences. Greedy sampling is used for generating tokens. For LLMs with a context window size smaller than the length of the data context, we truncate the data context in the middle, and concatenate the head and the tail of the context as input, ensuring that the QA instructions are fully contained within the input.

Metrics.For all tasks except **dureader-mixup** and **cmrc-mixup**, we evaluate the generated answers with our keyword-recall-based F1 metric, utilizing the annotated answer keywords and word blacklist. For **cmrc-mixup**, we omit the manual annotation of answer keywords since the answers in this dataset is already concise. Therefore, we use the F1 metric with word blacklist. In the case of **dureader-mixup**, where the GT answer lengths are relatively long, we do not manually annotate the answer keywords and use the ROUGH-L metric with the word blacklist.

### Compare LLMs on _Lv_-Eval

Figure 4 (a) shows the average scores across all 11 datasets of 12 LLMs at different length levels. We can see that (i) Commercial models do not always perform better than open-source models. For instance, ChatGLM3-6B-32k attains the highest accuracy on \(16k\) and \(32k\). (ii) Models exhibit distinct score trends. From the average scores in Figure 4 and the task-specific scores in Table A2, we can see that the model with the largest context window size, Llama3-8B-1M, exhibits the slowest decline of performance from \(16k\) to \(128k\). For example, its scores at the length level \(16k\) is lower than ChatGLM3-6B-32k and BlueLM-7B-32k-Chat. Nevertheless, as the length of input context increases, Llama3-8B-1M retains a higher score than these two models that need to truncate the input context. The similar phenomenon can be observed between Yi-6B-200 and two GPTs.

Figure 4 (b) shows the average scores across all 5 length levels of 12 LLMs on 5 types of tasks. We can see that (i) LLMs attain lower scores on multi-hop QA tasks compared to single-hop QA tasks. (ii) Confusing facts insertion adds complexity to the tasks, particularly evident in single-hop QA and single-hop confusion QA. See Appendix B for more detailed results.

Figure 4: Overall results on different length levels and types of datasets. (a) Average scores across all datasets of 12 LLMs at 5 length levels. (b) Average scores across all length levels of 12 LLMs on 5 types of datasets. “CQA” refers to datasets with CFI.

### Ablation Study of _Lv_-Eval Techniques

Confusing facts insertion.Table A4, A5, and A6 show the scores of multiple LLMs on dataset with and without CFI. We can see that (i) On **multifieldqa-en-mixup** and **multifieldqa-zh-mixup**, CFI leads to a notable degradation in the scores of LLMs. However, CFI in the **hotpotwikiqmixup** dataset does not result in severe degradation. (ii) Table A5 and A6 show that a strong model, ChatGLM3-6B-32k, exhibits the most substantial score degradation on data with CFI. For instance, the score of ChatGLM3-6B-32k degrades from \(41.46\) to \(31.97\) (a degradation of \(9.49\)) on the \(16k\) length level of **multifieldqa-en-mixup**, while the score degradation of other 5 LLMs falls within the range \([0.47,4.89]\). This observation suggests that current powerful LLMs may even be more susceptible to confusing information in the context. Future research is needed to enhance the models' ability to discern information that appears similar but is in fact unrelated. (iii) As the length of the input context increases, the score degradation becomes smaller. This phenomenon can be attributed to two factors: the truncation of confusing facts and a decrease in baseline performance.

Keyword and phrase replacement.The technique of KPR aims to eliminate the knowledge leakage and common-sense memorization of LLMs. Intuitively, for datasets sourced from Wikipedia and other widely used corpus, the risk of knowledge leakage is higher. From the results in Table A4, A5, and A6, we observe that: (i) KPR brings notable degradation of LLM scores on these three datasets suggesting that knowledge leakage exists in open-source corpus and can be mitigated by KPR. (ii) The extent of degradation is relatively consistent across different length levels.

We conduct an additional experiment to illustrate the knowledge leakage issue and the impact of KPR in Table 3. Specifically, we compare three settings: (i) Directly querying the LLMs to answer the question without the context ("direct (w.o. KPR)"). (ii) Applying KPR to the QA pair, and directly querying the LLMs without the context ("direct (w. KPR)"). (iii) Applying KPR to the QA pair and the context, and querying the LLMs to answer the question with the context ("w. context (w. KPR)").

Table 3 shows that without KPR, some LLMs can achieve a considerable score even without context. For instance, Yi-6B-200k and ChatGLM3-6B-32k achieve scores of 16.11 and 12.24, respectively, through memorization or common-sense knowledge. Applying KPR decreases the score without context (6.06 for Yi-6B-200k and 4.96 for ChatGLM3-6B-32k). This helps mitigate the influence of memorization or common-sense knowledge on the assessment of long-context understanding ability.

Case study on the fact-recall tasks.The **factrecall-en** and **factrecall-zh** datasets are constructed to evaluate the enhanced "needle in a haystack"  ability. The traditional "needle in a haystack" evaluation is basically a retrieval task, asking LLMs to find the answer or passkey in long context, which is too simple for majority of LLMs that they can easily get high scores after task oriented training. Therefore we enhance the "needle in a haystack" evaluation with CFI and KPR to assess LLM's positional consistency of retrieval while challenging their comprehension and anti-interference abilility. We show the ablation results of CFI and KPR in Figure 5 and Table A7. From the first column of sub-figure in Figure 5, we can see that ChatGLM3-6B-32k attains high accuracy on

    &  &  \\   & & \(16k\) & \(32k\) & \(64k\) & \(128k\) & \(256k\) \\   & direct (w. KPR) & & & 2.43 & & \\  & direct (w.o. KPR) & & & 3.52 & & \\  & w. context (w. KPR) & 3.99 & 1.30 & 1.84 & 0.81 & 0.75 \\   & direct (w. KPR) & & & 4.96 & & \\  & direct (w.o. KPR) & & & 12.24 & & \\  & w. context (w. KPR) & 16.98 & 14.76 & 9.02 & 8.31 & 6.68 \\   & direct (w. KPR) & & & & 6.06 & \\  & direct (w.o. KPR) & & & 16.11 & & \\  & w. context (w. KPR) & 23.55 & 18.94 & 9.94 & 7.66 & 2.01 \\   

Table 3: Ablation results for KPR. “direct (w. KPR)”: Apply KPR and direct query without context; “direct (w.o. KPR)”: Direct query without context; “w. context (w. KPR)”: Apply KPR and query with context. Note that there is only one result in the first two rows in each section of the table, since the results of direct querying without context do NOT depend on the context length.

datasets without CFI and KPR, as long as the input context length is within its context size (32k). However, when either CFI (second column of sub-figure) or KPR (third column sub-figure) is applied, the retrieval accuracy decreases. The accuracy experiences a more severe degradation when both CFI and KPR are applied, particularly evident in **factrecall-zh**, where a performance collapse is observed. This indicates that there is room for improvement in the model's ability to accurately identify a specific piece of information from a long context in the presence of interference, and the original "needle in a haystack" could not be suit for reasonably evaluating long context capability.

**Keyword-recall-based metric.** For a given length level \(L_{d}\) of the dataset, if the single key information is uniformly distributed in the context, an LLM with a context window size \(L_{m}\) can only observe the key information for approximately \(}{L_{d}}\) of the time. Thanks to our KPR technique, we can expect that the LLM cannot get the correct answer through memorization or common sense. Then, ideally, we would not expect to see a metric score much higher than \(}{L_{d}}\). However, as shown in Table A3, when using the original F1 metric, due to the undesired matching of non-keywords and non-informative words, the metric score can be a lot higher than \(}{L_{d}}\). For instance, ChatGLM3-6B-32k achieves a score of \(26.43\%\) on the \(256k\) length level of the **cmrc-mixup** dataset, which significantly exceeds \(}{L_{d}}=12.5\%\). Fortunately, our keyword-recall-based metric with the word blacklist returns a score that aligns more closely with human expectations and is more reasonable.

## 5 Limitations and Negative Societal Impacts

\(LV\)-Eval includes QA and the "needle in a haystack" tasks, but does not encompass other task types such as summarization. Additionally, due to the high cost, we do not test some of the most recent LLMs, such as GPT-4-128k. As we release all the test data, one can intentionally overfit the benchmark by training on the test data to get a high score. In this case, training on \(LV\)-Eval datasets with KPR might lead to mistakes in common-sense knowledge, resulting in a very unreliable evaluation. Furthermore, a full evaluation on \(LV\)-Eval can cause a large token overhead (about 700M tokens for GPT-4's tokenizer), leading to considerable carbon emissions.

Figure 5: Ablation results of the “needle in a haystack” task on ChatGLM3-6B-32k. (a) **factrecall-en.** (b) **factrecall-zh.** In each of (a)(b), from left to right, the four sub-figures show the results of w.o. “CFI and KPR”, “w. CFI only”, “w. KPR only”, and “w. both CFI and KPR”, respectively. These results illustrate that CFI and KPR are effective in improving the task difficulty.