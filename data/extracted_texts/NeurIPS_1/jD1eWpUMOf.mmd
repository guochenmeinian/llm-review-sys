# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

like negative guidance [] or altering the conditional distribution towards another anchor/surrogate concept [].

Despite notable advancements in the field of concept erasing, fine-tuned diffusion models often exhibit a _lack of robustness_. In particular, recent studies [], [] have shown that concepts trained to be erased can easily be regenerated through meticulously designed prompts, referred to as adversarial prompts. Consider the example shown in the first row of Fig. 1, although the model has been fine-tuned to exclude "nudity" from its outputs, it inadvertently reproduces nude images when faced with slightly modified, adversarial prompts. This reveals a fundamental weakness in current concept erasing methods: the embedded knowledge of the concept within the models could be hidden rather than forgotten. This vulnerability poses a significant risk when considering the deployment of diffusion models in real-world scenarios and calls for new solutions. However, how to improve the robustness performance has been a challenging problem yet to be solved.

With the above problem in mind, we first examine why fine-tuned diffusion models fail to be robust against adversarial prompts. We analyze the behavior of hidden states within these models. By tracing the activation of concept-related feature representations in neural networks, we have found that current fine-tuning techniques merely deactivate the generation of concept-related hidden states rather than eliminating them entirely. This deactivation is fragile, as input perturbations can reactivate these hidden states, allowing for the regeneration of supposedly erased content. This implies that the internal pathways for generating concept-related hidden states stay intact, even though they are _temporarily inactive_. To enhance robustness, we propose a simple solution: pruning specific parameters to sever these generation pathways completely. If we can strategically _zero out certain parameters_, we may _cut off the routes_ that lead to the reactivation of concept-related hidden states, even in the face of adversarial prompts.

To achieve the above goal, we develop a differentiable pruning strategy for robust concept erasing. Specifically, we parameterize a mask for each parameter and define the training objective with a standard concept-erasing objective, such as ESD [] and AC []. We then employ back-propagation to optimize the mask, allowing the concept erasing loss to determine which parameters should be pruned. That is, we integrate the erasing and pruning into a single objective. In this way, we can achieve two goals _simultaneously_: 1) minimizing the loss associated with concept erasing, effectively eliminating the concept knowledge within models, and 2) severing the pathways that could potentially reactivate the concept-related hidden states, ensuring robustness against adversarial prompts.

The enhanced robustness of our proposed method, compared to previous approaches, has been empirically validated across three widely-used test environments: the erasure of nudity, style, and objects, as detailed in Section 4. We find that our method achieves comparable or even superior performance in the concept erasing rate on normal prompts and significantly improves the robustness performance on adversarial prompts, crafted by attack methods including UnlearnDiff [], [] and P4D []. A summarized comparison between the SOTA fine-tuning based method ESD and our method P-ESD is reported in Fig. 2. Notably, we also empirically find that the sparsity of pruning is well controlled under to a small portion (e.g., less than 0.01%) of parameters and does not sacrifice generation quality on other concepts.

Figure 1: Left panel: semantic illustration of prior concept erasing methods (the top row) and our method (the bottom row). Right panel: concrete examples illustrate the vulnerability of prior concept-erasing methods and the robustness of our method.

We summarize our contributions as follows:

* We analyze why fine-tuning-based erasing is vulnerable to adversarial attacks, offering insights to improve the robustness of concept erasing in diffusion models.
* We develop a new concept-erasing paradigm based on pruning to enhance robustness. This approach integrates erasing and pruning into a single objective and can be easily applied to existing concept-erasing objectives.
* Experiments demonstrate that our method significantly improves the robustness of diffusion models across three test beds while maintaining the ability to generate standard concepts.

## 2 Related Work

### Concept erasing in diffusion models

The task of concept erasing, or generally the removal of undesirable image generation, is introduced in . There are two kinds of methods: inference-based and training-based. For the former, there is no need to update the model's parameters. In this vein,  proposed designing a safety guidance to steer the generation in the opposite direction for unsafe prompts.  proposed applying an NSFW safety filter to detect sensitive prompts before generation. On the other hand, training-based approaches are believed to be safer as they aim to make the model forget undesirable knowledge within the parameters. To name a few,  explored the use of negative guidance in text-to-image diffusion models to reduce the conditional generation probability.  showed that modifying the conditional distribution of the target concept to that of another anchor/surrogate concept also performs well. Note that closed-form solutions are available for  since they merely update the linear projection layer in the cross-attention module.

Concept erasing in text-to-image diffusion models is similar to the concept of machine unlearning, which aims to remove the impact of certain data subsets from a trained model, as outlined in . While both processes share the goal of mitigating undesired influences, they differ in focus. Concept erasing specifically targets the modification of content in generated images, as highlighted in .

### Neural network pruning

Pruning  is a compression technique commonly used to remove redundant components (e.g., weights or neurons) in neural networks. It is effective in reducing the number of neural network parameters, thereby improving computational efficiency on edge devices . Typically, pruning strategies are designed to prune "less important" parameters while preserving the acquired abilities . Different from them, our framework requires to prune critical parameters associated the concept for removal. We are motivated by previous studies  that pruned neural networks are sparse, which can reduce the correlation among dominant features and thereby enhance robustness. They demonstrated that pruning is beneficial for adversarial robustness in machine _learning_, particularly in _classification_ tasks. In contrast, our focus is on the robustness of concept _erasing_ in _generative_ models.

Figure 2: Concept erasure rates of the fine-tuning-based ESD method  and our proposed pruning-based P-ESD method, with both methods applying the same erasing objective. Higher values indicate better performance. The results show that our method significantly outperforms the fine-tuning based approach, especially when facing adversarial prompts.

## 3 Robust Concept Erasing

### Preliminary

Diffusion models, trained on vast amounts of unfiltered Internet data , often acquire the capability to generate content that may include offensive images and copyrighted artworks. To mitigate these unintended consequences, the framework of **concept erasing** has been introduced in . In particular, this framework aims to fine-tune the diffusion model to disable its generation ability for concepts deemed undesirable or inappropriate. Concretely, existing methods update model parameter \(\) to override the prediction of the text prompt \(c\) (associated with the erased concept) to a new target \(y\):

\[_{}_{}()=_{x_{t},c,t}[ \|_{}(x_{t},c,t)-y\|_{2}^{2}]. \]

where \(_{}\) is the denoising network, and \(x_{t}\) is the noisy image input at time step \(t\). In this way, the probability of generating undesirable concepts are reduced in the denoising process. We explain how existing methods can be substantiated in the above framework.

* For the ESD (Erasing Stable Diffusion) , it uses the target value \[y=_{^{*}}(x_{t},c_{},t)-[_{^{*}} (x_{t},c,t)-_{^{*}}(x_{t},c_{},t)],\] (2) where \(c_{}\) is the null text for unconditioned generation and \(^{*}\) is the parameter for an non-erased diffusion model. Using the terminology from classifier-free guidance generation, this target value guides the generation in the opposite direction of the erased concept.
* Another famous method is AC (Ablating Concept) , which uses the target value from the prediction of text prompt \(c^{*}\) for an anchor concept: \[y=(_{}(x_{t},c^{*},t)).\] (3) This anchor concept is semantically similar to the erased concept but is removed with the target concept. For example, to erase "Grumpy Cat", \(c\) could be "A cute little Grumpy Cat" and \(c^{*}\) is "A cute little cat".

### Vulnerability of Concept Erasing

Although existing concept erasing methods are effective on normal prompts, they are vulnerable to adversarial prompts . We provide such examples in Fig.  and Fig.  A critical question arises: why do these fine-tuning-based erasing methods fail to be robust when faced adversarial prompts? In this section, we explore the underlying reasons for this weakness by analyzing the model's internal hidden states, specifically focusing on how concepts emerge and dissipate within the diffusion model.

We believe that concept generation in the produced images is primarily controlled by certain parameters in the denoising network that interact with the inputs to yield concept-related feature representations and final images. We realize that it is challenging to provide a complete depiction of this process, but it is possible to identify such concept-related feature representations and trace their behaviors to get some insights. For a denoising network consists of many ResNet blocks (e.g., 22 in SD-v1.4), we trace the outputs of blocks (post-activation). Provided text prompts \(c\) containing the concept to be erased, we measure the change of hidden states by:

\[_{,i}=_{x_{t},c}[\|z_{,i}^{*}(x_{t},c,t)-z_{ ,i}(x_{t},c,t)\|_{1}], \]

Figure 3: Visualization of intrinsic vulnerability of fine-tuned models. More visualization examples are provided in Appendix A.1where \(z^{}_{,i}\) and \(z_{,i}\) denote the outputs of the \(\)-th block and \(i\)-th channel in the original model and erased model (e.g., by ESD) respectively. A large value of \(\) indicates that such a channel is modified a lot by the erasing method and more correlated with concept generation. For each block, we identify and focus on the channels with the most largest values of \(\) as concept-related hidden states. An example is provided in Fig. 2, where we demonstrate the erasing of the concept "tench". More examples are provided in Appendix [A.1]

Fig. 2 reveals the following mechanism: concept-erasing methods like ESD can effectively deactivate the hidden states related to concept generation, successfully removing the undesired concept from the generated image under normal prompts. However, when an adversarial perturbation is introduced to the input prompt, the model's generation pathways for these hidden states are reactivated, causing the undesired concept to reemerge. This observation implies that the internal pathways for generating concept-related hidden states stay intact, even though they are _temporarily inactive_. We believe this drawback is inherent to _fine-tuning methods, which merely update parameters to change the denoising network's output, but do not sever internal pathways of concept-related hidden states_.

Our observation also leads to an interesting question: could we directly remove the identified channels (set the value of channels to zero) to prevent the reactivation of concept-related hidden states? We have experimented with this and found that while this approach is effective for some adversarial prompts, it simultaneously impairs the model's ability to generate non-targeted concepts. Detailed results of this approach are provided in Appendix [A.1] We believe this failure stems, in part, from the _polysemantic nature of channels_. As output units, they may be responsible for a mixture of multiple concepts, not just the one we aim to remove. This prompts us to explore a more principled strategy: selectively pruning weights to disrupt the generation pathways of concept-related hidden states, as discussed in the following section.

### Pruning for Robust Concept Erasing

In this section, we introduce a parameter-pruning-based strategy to achieve robust concept erasing. Previous studies on neural network pruning typically target the removal of "less important" connections, often identified through their minimal impact on overall model performance. Different from them, our work innovatively integrates the erasing and pruning into a unified objective, and prune critical parameters associated the concept for removal.

Here, let \(^{*}^{p}\) denote the parameter of the original diffusion model. We introduce hard masks \(M_{ hard}\{0,1\}^{p}\), which has the same dimension as \(^{*}\). The training objective remains to minimize a concept erasing loss function for the denoising network, but the optimization variables are now the masks:

\[_{M_{ hard}\{0,1\}^{p}}_{ erase}=_{x_{t},c,t}[\|_{^{*} M_{ hard}}(x_{t},c,t)-y\|_ {2}^{2}], \]

where \(\) means element-wise multiplication. The masks are applied to parameters (weights and biases) in convolution and linear layers to selectively enable or disable the connections within these layers. For parameters with special roles, such as those in layer normalization, masks are not applied. The design of the target variable \(y\) is flexible and can be adapted to various existing methods, such as ESD and AC.

By solving Eq. 5, we can achieve two goals _simultaneously_: 1) minimizing the loss associated with concept erasing, effectively eliminating the concept knowledge within models, and 2) cutting off the pathways that could potentially reactivate the concept-related hidden states, ensuring robustness against adversarial prompts.

**Practical Algorithm.** Despite good properties of Eq. 5, the optimization problem involves discrete optimization and is hard to solve. To address this challenge, we propose convert it to a continuous optimization problem and employ gradient-based optimization algorithms such as AdamW . In particular, We parameterize the hard mask to be soft via the sigmoid function:

\[M_{ soft}(m)=^{p}, \]

where \(>0\) is a fixed temperature coefficient (usually \(=10\)) controlling the slope of the sigmoid function, and \(m^{p}\) is the trainable parameter to be optimized with same dimension as \(^{*}\). Other parameterization techniques may also be applicable, but we find that the sigmoid transformation works well in our experiments. Then we solve the following continuous optimization problem 

[MISSING_PAGE_FAIL:6]

the criterion called _Concept Erasure Rate (CER)_, which indicates the rate at which the diffusion model successfully erases a specified concept from its generated images. A higher rates means better performance in achieving concept erasure.

**Attack methods:** In all three scenarios, we implement two recently proposed attack methods: P4D  and UnlearnDiff , which use a local search method to find an adversarial prompt for concept regeneration. The prepended prompt perturbation is set as 5 tokens for erasing nudity, and 3 tokens for erasing style and object. For each prompt, we conduct 10 attacks on samples drawn from 10 timesteps, selected at intervals of 5 steps across 50 diffusion steps. Details of attack configuration is provided in Appendix A.3

### Erasing Nudity

We evaluate models on erasing nudity using the same test prompts as , derived from the "sexual" category of the I2P dataset  with nudity scores above 0.75. NudeNet  is then used to detect nudity in the generated images.

We report the average concept erase rate over these test prompts in Tab. . Quite interestingly, we find that our method not only improves the concept erasing rate on normal test prompts but also the adversarial prompts. Note that the concept erasing on adversarial prompts are challenging: the performance of all methods we tested dropped on adversarial prompts compared to that with normal test prompts. Nevertheless, we find that our P-ESD is still robust among baselines. Specifically, the concept erasure rates on adversarial prompts improves by over 30% compared to existing methods. These results demonstrate that our proposed method serves as an effective strategy for enhancing the robustness of concept erasing in the nudity task.

### Erasing Style

In this section, we consider to remove the artist style, a more abstract concept. Following , we choose to examine the effectiveness of various methods in erasing the "Van Gogh" style from diffusion model. There are 50 test prompts. The success of concept erasing is evaluated using a style classifier to check if the "Van Gogh" style is among the top-3 predictions for images generated by the model after concept erasing has been applied.

We report the results in Tab. . Among the fine-tuning-based methods, ESD emerges as the most effective aimed at erasing style. However, it is still inferior to our proposed P-ESD method, which outperforms ESD by over 30% when tested against adversarial prompts.

### Erasing Objects

In this section, we focus on removing various objects, including "tench," "church," and "garbage truck". For each object class, we use 50 test prompts from , generated by ChatGPT.

The results are presented in Tab. . Among baselines, UCE outperforms both FMN and ESD. However, by integrating pruning into ESD, P-ESD exhibits enhanced performance over ESD on adversarial

    & UCE & RACE & SPM & AC & P-AC & ESD & P-ESD \\  _Normal Prompts_ & 0.80 & 0.83 & 0.47 & 0.60 & 0.63 & 0.80 & **0.95** \\  _Adversarial Prompts:_ & & & & & & & \\ UnlearnDiff & 0.14 & 0.49 & 0.08 & 0.17 & 0.36 & 0.40 & **0.86** \\ P4D & 0.13 & 0.50 & 0.08 & 0.26 & 0.42 & 0.39 & **0.82** \\   

Table 1: Concept erasure rate for erasing nudity. A larger number means a better erasing.

    & UCE & RACE & SPM & AC & P-AC & ESD & P-ESD \\  _Normal Prompts_ & 0.28 & 0.56 & 0.36 & 0.82 & 0.80 & 0.84 & **1.00** \\  _Adversarial Prompts:_ & & & & & & & \\ UnlearnDiff & 0.04 & 0.20 & 0.12 & 0.42 & 0.62 & 0.52 & **0.90** \\ P4D & 0.06 & 0.18 & 0.10 & 0.46 & 0.62 & 0.56 & **0.86** \\   

Table 2: Concept erasure rate for erasing style.

prompts and competes favorably with UCE. This suggests that our pruning-based approach offers greater robustness than fine-tuning when optimizing the same erasing objective.

### Analysis of the Proposed Method

In addition to evaluating the concept erasing rate on adversarial prompts, we aim to explore the model's internal robustness. To do this, we assess the sensitivity score of concept-related hidden states identified using Eq. 4. This score is based on the magnitude of activation value changes when exposed to normal prompts, \(c\), versus adversarial prompts, \(c_{}\). Intuitively, we expect the concept-related hidden states to remain stable with a low sensitivity score under adversarial attacks, such that they would not easily reactivate. Specifically, for each feature \(_{,i}\) located at the \(\)-th layer and \(i\)-th channel in the erased model, we define its sensitivity score at denoising timestep \(t\) as:

\[_{,i}=_{x_{t},c}[\|z_{,i}(x_{t},c,t)-z_{,i}(x_{t},c_{},t)\|_{1}], \]

A large value of \(\) implies that the activation of the hidden state is significantly affected by the prompt change, thus indicating its vulnerability to input variations.

In Fig. 4 we compare the sensitivity scores of fine-tuning-based and pruning-based erasing methods, namely ESD and P-ESD. These scores are computed over five timesteps, selected at intervals of 10 steps across a total of 50 diffusion steps. We find that P-ESD consistently reduces the sensitivity score throughout the denoising process, suggesting greater internal robustness compared to ESD.

## 5 Conclusion

In this paper, we develop a new pruning strategy to address the robustness issue in existing concept erasing frameworks. Our method selectively prunes parameters critical to targeted concepts, demonstrating superior performance over existing approaches. This work aims to mitigate risks associated with deploying diffusion models in real-world scenarios where adversarial prompts may be encountered. Future research will explore extending these techniques to improve robustness in other model types beyond diffusion models.

    &  &  &  \\   & FMN & UCE & SPM & ESD & P-ESD & FMN & UCE & SPM & ESD & P-ESD & FMN & UCE & SPM & ESD & P-ESD \\  _Normal Prompts_ & 0.64 & **1.00** & 0.94 & **1.00** & **1.00** & 0.48 & **0.94** & 0.54 & 0.86 & 0.88 & 0.54 & 0.98 & 0.92 & 0.98 & **1.00** \\  _Adversarial Prompts_: & & & & & & & & & & & & & & & & & \\    } &  & **0.96** & 0.42 & 0.78 & 0.92 & 0.12 & **0.74** & 0.08 & 0.58 & 0.64 & 0.08 & 0.84 & 0.64 & **0.90** & 0.86 \\  & & 0.14 & 0.92 & 0.56 & 0.86 & **0.98** & 0.16 & 0.64 & 0.10 & 0.64 & **0.68** & 0.04 & 0.88 & 0.56 & 0.82 & **0.94** \\   

Table 3: Concept erasing rate for erasing objects.

Figure 4: Sensitivity score comparison between ESD and P-ESD. The sensitivity scores are averaged on concept-related hidden states from each layer.