# Feature Adaptation for Sparse Linear Regression

Jonathan A. Kelner

MIT

kelner@mit.edu. This work was supported in part by NSF Large CCF-1565235, NSF Medium CCF-1955217, and NSF TRIPODS 1740751.

Frederic Koehler

fkoehler@stanford.edu. This work was supported in part by NSF award CCF-1704417, NSF award IIS-1908774, and N. Anari's Sloan Research Fellowship

Raghu Meka

raghum@cs.ucla.edu. This work was supported in part by NSF CAREER Award CCF-1553605 and NSF Small CCF-2007682

Dhruv Rohatgi

MIT

###### Abstract

Sparse linear regression is a central problem in high-dimensional statistics. We study the correlated random design setting, where the covariates are drawn from a multivariate Gaussian \(N(0,)\), and we seek an estimator with small excess risk.

If the true signal is \(t\)-sparse, information-theoretically, it is possible to achieve strong recovery guarantees with only \(O(t n)\) samples. However, computationally efficient algorithms have sample complexity linear in (some variant of) the _condition number_ of \(\). Classical algorithms such as the Lasso can require significantly more samples than necessary even if there is only a single sparse approximate dependency among the covariates.

We provide a polynomial-time algorithm that, given \(\), automatically adapts the Lasso to tolerate a small number of approximate dependencies. In particular, we achieve near-optimal sample complexity for constant sparsity and if \(\) has few "outlier" eigenvalues. Our algorithm fits into a broader framework of _feature adaptation_ for sparse linear regression with ill-conditioned covariates. With this framework, we additionally provide the first polynomial-factor improvement over brute-force search for constant sparsity \(t\) and arbitrary covariance \(\).

## 1 Introduction

Sparse linear regression is a fundamental problem in high-dimensional statistics. In a natural random design formulation of this problem, we are given \(m\) independent and identically distributed samples \((X_{i},y_{i})_{i=1}^{m}\) where each sample's covariates are drawn from an \(n\)-dimensional Gaussian random vector \(X_{i} N(0,)\), and each response is \(y_{i}= X_{i},v^{*}+_{i}\) for independent noise \(_{i} N(0,^{2})\) and a \(t\)-sparse ground truth regressor \(v^{*}^{n}\), where \(t\) is much smaller than \(n\). The goal1 is to output a vector \(^{n}\) for which the _excess risk_

\[( X_{0},-y_{0})^{2}-^{2}=(-v^{*})^ {}(-v^{*})=:\|-v^{*}\|_{}^{2}\]

is as small as possible, where \((X_{0},y_{0})\) is an independent sample from the same model.

Without the sparsity assumption, the number of samples needed to achieve small excess risk (say, \(O(^{2})\)) is linear in the dimension; with \(O(n)\) samples, simple and computationally efficient algorithms such as ordinary least squares achieve the statistically optimal excess risk \(O(n}{m})\). Sparsityallows for a significant statistical improvement: ignoring computational efficiency, it is well known that there is an estimator \(\) with excess risk \(O(t n}{m})\) as long as \(m=(t n)\) (see e.g. [13; 33]; Theorem 4.1 in ).

The catch is that computing this estimator involves a brute-force search over \(\) possibilities (i.e., the possible supports for \(v^{*}\)). At first glance, this combinatorial search may seem unavoidable if we wish to take advantage of sparsity. Indeed, similar problems are notoriously difficult: the only non-trivial algorithms for e.g., learning \(t\)-sparse parities with noise still require \(n^{(t)}\) time [29; 37]. However, it is a celebrated fact that for sparse linear regression, computationally efficient methods such as Lasso and Orthogonal Matching Pursuit can avoid this combinatorial search and still achieve very strong theoretical guarantees under conditions such as the Restricted Isometry Property (see e.g. [7; 10; 5; 4; 3; 1]). In the random design setting we consider, the Lasso is known to achieve optimal statistical rates (up to constants) when the covariance matrix \(\) is _well-conditioned_[32; 46].

What about when \(\) is ill-conditioned? In contrast with the statistically optimal estimator, Lasso and its cousins provably _require_ sample complexity scaling with (some variant of) the condition number of \(\) (see e.g. Theorem 14 in  or Theorem 6.5 in ). And with a few exceptions (e.g., in some settings with special graphical structure ) there has been little progress on designing new efficient algorithms for sparse linear regression with ill-conditioned \(\) (see Section 4 for further discussion). For a general covariance \(\), no algorithm is even known that can achieve sample complexity \(f(t) n^{1-}\) (for an arbitrary function \(f\)) without brute-force search.

A computationally efficient algorithm that approaches the optimal statistical rate for _arbitrary_\(\) might be too much to hope for. While no computational lower bounds are known, even in restricted computational models such as the Statistical Query model,6 the related _worst-case_ problem of finding a \(t\)-sparse solution to a system of linear equations requires \(n^{(t)}\) time under standard complexity assumptions . So it is plausible, though not certain, that some assumptions on \(\) are necessary. In this work - inspired by a long tradition (in random matrix theory, statistics, graph theory, and other areas) of studying matrices with a spectrum that is split between a large "bulk" and a small number of outlier "spike" eigenvalues [28; 39; 43] - we identify a broad generalization of the standard well-conditionedness assumption, under which brute-force search can still be avoided.

### Beyond well-conditioned \(\)

Say that \(\) has eigenvalues \(_{1}_{n}\), and that the sparsity \(t\) is a constant.7 Then standard bounds for Lasso require sample complexity \((_{n}/_{1}) O( n)\). But if the covariates contain even a single approximate linear dependency, then \(_{n}/_{1}\) may be arbitrarily large. Moreover, if the dependency is sparse (e.g. two covariates are highly correlated), then there is a natural choice of \(v^{*}\) for which Lasso provably fails (see Theorem 6.5 of ). Indeed, this phenomenon is not just a limitation of the analysis; Lasso fails empirically as well, even for very small \(t\) (see Figure 2 in Appendix H for a simple example with \(t=3\)).

Such dependencies arise in applications ranging from finance (e.g., where some pairs of stocks or ETFs may be highly correlated, and an investor may be interested in the differences) to genomic data (where functionally related genes may have highly correlated expression patterns). Two-sparse dependencies can be directly identified by looking at the covariance matrix; see Section 4 for some discussion of previous research in this direction. But as \(t\) increases, naive methods for identifying \(t\)-sparse dependencies quickly become computationally intractable. With domain knowledge, it may be possible to manually identify and correct such dependencies, but this process would also be time-consuming. Thus, we ask the following question: instead of assuming that \(_{n}/_{1}\) is bounded, suppose that there are constants \(d_{}\) and \(d_{h}\) so that \(_{n-d_{h}}/_{d_{}+1}\) is bounded, i.e. the spectrum of \(\) has only \(d_{}\) outliers at the low end, and only \(d_{h}\) outliers at the high end. Can we still design an algorithm that achieves sample complexity \(O( n)\) without resorting to brute-force search?

Main result.We give a positive answer: an algorithm for sparse linear regression that is both computationally and statistically efficient for covariance matrices with a small number of "outlier"eigenvalues. In particular, this means we can handle a few approximate dependencies among the covariates (quantified by the number of eigenvalues below a threshold). In comparison, Lasso and other classical algorithms cannot tolerate even a single sparse approximate dependency. Our main algorithmic result is the following:

**Theorem 1.1**.: _Let \(n,t,d_{},d_{h},L\) and \(,>0\). Let \(^{n n}\) be a positive semi-definite matrix with (non-negative) eigenvalues \(_{1}_{n}\). Let \(v^{*}^{n}\) be any \(t\)-sparse vector. Let \((X_{i},y_{i})_{i=1}^{m}\) be independent with \(X_{i} N(0,)\) and \(y_{i}= X_{i},v^{*}+_{i}\), where \(_{i} N(0,^{2})\)._

_Let \(n_{}:=t(_{n-d_{h}}/_{d+1})(nL/)+t^{O(t)}d_{l }+d_{h}\). Given \(\), \(t\), \(d_{}\), \(\), and \((X_{i},y_{i})_{i=1}^{m}\), there is an estimator \(^{n}\) that has excess risk_

\[\|-v^{*}\|_{}^{2} O(n_{}L}{m} )+2^{-L}\|v^{*}\|_{}^{2}\]

_with probability at least \(1-\), so long as \(m(n_{}L)\). Moreover, \(\) can be computed in time \((n)\)._

Specifically, taking \(L(m\|v^{*}\|_{}^{2}/^{2})\), the time complexity is dominated by \(L\) eigendecompositions and \(L\) calls to a Lasso program, for overall runtime \((n^{3})\) (see Algorithm 2). This is substantially faster than the brute-force method (which takes \(O(n^{t})\) time) even for small values of \(t\).

The excess risk decays at rate \((^{2}n_{}/m)\) (hiding the logarithmic factor), which is near the statistically optimal rate of \((^{2}t/m)\) so long as \(n_{}\) is small, i.e. \(t\) is small and only a few eigenvalues lie outside a constant-factor range. In our analysis, we prove that the standard Lasso estimator can already tolerate a few _large_ eigenvalues -- the main algorithmic innovation is needed to tolerate a few _small_ eigenvalues, which turns out to be much trickier. Notice that when \(d_{}=d_{h}=0\) we recover standard Lasso guarantees up to the factor of \(L\); thus, Theorem 1.1 morally represents a generalization of classical results.

We also show how to achieve a different trade-off between time and samples, eliminating the dependence on \(d_{}\) in sample complexity at the cost of larger runtime:

**Theorem 1.2**.: _In the setting of Theorem 1.1, let \(n^{}_{}:=t(_{n-d_{h}}/_{d_{}+1})(nL/ )+t^{2}(t)+d_{h}\). Given \(\), \(t\), \(d_{}\), \(\), and \((X_{i},y_{i})_{i=1}^{m}\), there is an estimator \(^{n}\) that has excess risk_

\[\|-v^{*}\|_{}^{2} O(n^{}_{}L}{m})+2^{-L}\|v^{*}\|_{}^{2}\]

_with probability at least \(1-\), so long as \(m(n^{}_{}L)\). Moreover, \(\) can be computed in time \((n,m,d^{t}_{},t^{t^{2}})\)._

Discussion & limitations.We discuss two limitations of the above results. First, both results incur exponential dependence on the sparsity \(t\) (in the sample complexity for Theorem 1.1, and the runtime for Theorem 1.2), which may be suboptimal. For Theorem 1.1, we remark that in practice the algorithm may not suffer this dependence (see e.g. Figure 1), and it is possible that the analysis can be tightened. For Theorem 1.2, we emphasize that the runtime is still fundamentally different than brute-force search: in particular, it's _fixed-parameter tractable_ in \(t\) and \(d_{}\).

Second, both results require that \(\) is known. Thus, they are only applicable in settings where we either have a priori knowledge, or can estimate \(\) accurately because a large amount of unlabelled data is available. At a high level, this limitation is due to the need to compute the eigendecomposition of \(\), which cannot be approximated from the empirical covariance of a small number of samples.

For simplicity, we have stated our results in terms of Gaussian covariates and noise, but this is not a fundamental limitation. We expect it is possible to prove similar results in the sub-Gaussian case at the cost of making the proof longer -- for instance, by building upon the techniques from  and related works.

Pseudocode & simulation.See Algorithm 1 for complete pseudocode of AdaptedBP(), a simplification of the method for the noiseless setting \(=0\). In Figure 1 we show that AdaptedBP() significantly outperforms standard Basis Pursuit (i.e. Lasso for noiseless data ) on a simple example with \(n=1000\) variables, \(d_{}=10\) sparse approximate dependencies, and a ground truth regressor with sparsity \(t=13\). The covariates \(X_{1:1000}\) are all independent \(N(0,1)\) except for \(10\) disjoint triplets \(\{(X_{i},X_{i+1},X_{i+2}):i=1,4,,28\}\), each of which has joint distribution

\[X_{i}:=Z_{i}; X_{i+1}=Z_{i}+0.4Z_{i+1}; X_{i+2}=Z_{i+1}+0.4Z_{i+2}\]

where \(Z_{i},Z_{i+1},Z_{i+2} N(0,1)\) are independent. The (noiseless) responses are \(y=6.25(X_{1}-X_{2})+2.5X_{3}+}_{i=991}^{1000}X_{i}\). See Appendix I for implementation details.

### Organization

In Section 2 we give an overview of the proofs of Theorem 1.1 and 1.2 (the complete proofs and full algorithm pseudocode are given in Appendix C). In Section 3 we discuss our other results obtained via feature adaptation. Section 4 covers related work.

## 2 Proof techniques

We obtain Theorems 1.1 and 1.2 as outcomes of a flexible algorithmic approach for tackling sparse linear regression with ill-conditioned covariates: _feature adaptation_. As a pre-processing step, adapt

Figure 1: Basis Pursuit (BP) versus Adapted BP in a simple synthetic example with \(n=1000\) covariates. The \(x\)-axis is the number of samples. The \(y\)-axis is the out-of-sample prediction error (averaged over \(10\) independent runs, and error bars indicate the standard deviation).

or augment the covariates with additional features (i.e. well-chosen linear combinations of the covariates). Then, to predict the responses, apply \(_{1}\)-regularized regression (Lasso) over the new set of features rather than the original covariates. In other words, we algorithmically change the _dictionary_ (set of features) used in the Lasso regression. See Section 4 for a comparison to past approaches.

We start by explaining the goals of feature adaptation for general \(\), and then show how we achieve those desiderata when \(\) has few outlier eigenvalues. More precisely, the main technical difficulty is in dealing with the small eigenvalues, so in this proof overview we focus on the case where the only outliers are small eigenvalues. Complete proofs of Theorems 1.1 and 1.2 are in Appendix C.

### What makes a good dictionary: the view from weak learning

Obviously, the feature adaptation approach generalizes Lasso. Surprisingly, even though the sample complexity of the standard Lasso estimator is thoroughly understood, the basic question of whether for _every_ covariate distribution (i.e. every \(\)) there _exists_ a good dictionary remains wide-open. To crystallize the power of feature adaptation, we introduce the following notion of a "good" dictionary. We suggest considering the simplified setting of \(\)_-weak learning_, where the goal is just to find some \(\) so that the predictions \( X,\) are \(\)-correlated with the ground truth \( X,v^{*}\) when \(X N(0,)\). Moreover, we focus first on the existential question (rather than the algorithmic question of finding the dictionary). We will return to the setting of Theorems 1.1 and 1.2 later. For now, in the weak learning setting, a good dictionary (when the covariate distribution is \(N(0,)\)) is one that satisfies the following covering property, but is not too large:

**Definition 2.1**.: Let \(^{n n}\) be a positive semi-definite matrix and let \(t,>0\). A set \(\{D_{1},,D_{N}\}^{n}\) is a \((t,)\)-dictionary for \(\) if for every \(t\)-sparse \(v^{n}\), there is some \(i[N]\) with

\[| v,D_{i}_{}|\|v\|_{}\|D_{ i}\|_{},\]

where we define \( x,y_{}:=x^{} y\) and \(\|x\|_{}^{2}:=x^{} x\) for any \(x,y^{n}\). Let \(_{t,}()\) be the size of the smallest \((t,)\)-dictionary.

The relevance of the covering number \(_{t,}()\) is quite simple: given a \((t,)\)-dictionary \(\) for \(\), and given samples \((X_{i},y_{i})_{i=1}^{m}\), the weak learning algorithm can simply output the vector \(\) that maximizes the empirical correlation between the predictions \( X_{i},\) and the responses \(y_{i}\). So long as there are enough samples for empirical correlations to concentrate, Definition 2.1 guarantees success. Formally, allowing for preprocessing time to compute the dictionary, \(O()\)-weak learning is possible in time \(_{t,}()(n)\), with \(O(^{-2}_{t,}())\) samples (Proposition A.5).

Hypothetically, bounding \(_{t,}()\) may not be _necessary_ to develop an efficient sparse linear regression algorithm. However, all assumptions on \(\) that are currently known to enable efficient sparse linear regression also immediately imply bounds on \(_{t,}\) (see Appendix G). For example, when \(\) is well-conditioned, the standard basis is a good dictionary of size \(n\) (Fact A.4).

In contrast, the only known bounds for arbitrary \(\) (until the present work) are \(_{t,1/}() t\) (the brute-force dictionary, which includes a \(\)-orthonormal basis for every set of \(t\) covariates) and \(_{t,1/}() n\) (a \(\)-orthonormal basis for all \(n\) covariates, which doesn't take advantage of sparsity and corresponds to algorithms such as Ordinary Least Squares). Thus, the following basic question - when can we improve upon these trivial bounds - seems central to understanding when brute-force search can be avoided in sparse linear regression:

**Question 2.2**.: _How large is \(_{t,}()\) for an arbitrary positive semi-definite \(^{n n}\)? Are there natural families of ill-conditioned \(\) (and functions \(f,g\)) for which \(_{t,1/f(t)}() g(t)(n)\)?_

### Constructing a good dictionary when \(\) has few small eigenvalues

We now address Question 2.2 in the setting where \(\) has a small number of eigenvalues that are much smaller than \(_{n}\). In this setting, the standard basis may not be a good dictionary. For example, if two covariates are highly correlated, their difference may not be correlated with any of them. Nonetheless, we can prove the following covering number bound:

**Theorem 2.3**.: _Let \(n,t,d\). Let \(^{n n}\) be a positive semi-definite matrix with eigenvalues \(_{1}_{n}\). Then \(_{t,}() t(7t)^{2t^{2}+d}t^{t}+n\), where \(=}/_{n}}\)._In particular, when \(t=O(1)\) and \(\) is well-conditioned except for \(O(1)\) outliers \(_{1},,_{d}\), we get a linear-size dictionary just as in the case where \(\) is well-conditioned. In fact, the desired \((t,)\)-dictionary can be constructed efficiently. Our key lemma shows that when \(\) has few small eigenvalues, there is a small subset of covariates that "causes" all of the sparse approximate dependencies - in the sense that the \(_{2}\) norm of any sparse vector, _excluding_ the mass on the subset, can be upper bounded in terms of the \(\)-norm of the vector. Moreover, there is an efficient algorithm that finds a superset of these covariates. Formally, we prove the following:

**Lemma 2.4**.: _Let \(n,t,d\). Let \(^{n n}\) be a positive semi-definite matrix with eigenvalues \(_{1}_{n}\). Given \(\), \(d\), and \(t\), there is a polynomial-time algorithm_ IterativePeeling() _producing a set \(S[n]\) with the following guarantees:_

* _For every_ \(t\)_-sparse_ \(v^{n}\)_, it holds that_ \(\|v_{[n] S}\|_{2} 3_{d+1}^{-1/2}\|v \|_{}\)_._
* \(|S|(7t)^{2t+1}d\)_._

Once this set \(S\) has been found, the dictionary is simply the standard basis \(\{e_{1},,e_{n}\}\), together with a \(\)-orthonormal basis for every set of \(t\) covariates in \(S\). By guarantee (a), we can prove that every \(t\)-sparse vector correlates with some element of this dictionary under the \(\)-inner product. By guarantee (b), the dictionary is much smaller than the brute-force dictionary that contains a basis for all \(\) sets of \(t\) covariates. Together, this gives an algorithmic proof for Theorem 2.3.

**Intuition for** IterativePeeling().** We compute the set \(S\) via a new iterative method which leverages knowledge of the small eigenspaces of \(\). See Algorithm 1 for the pseudocode. To compute \(S\), the algorithm IterativePeeling() first computes the orthogonal projection matrix \(P\) that projects onto the subspace spanned by the top \(n-d\) eigenvectors of \(\). Starting with the set of coordinates that correlate with \((P)\), the procedure then iteratively grows \(S\) in such a way that at each step, a new participant of each approximate sparse dependency is discovered, but \(S\) does not become too much larger.

The intuition is as follows: as a preliminary attempt, we could identify all \(O(d)\) coordinates that correlate (with respect to the standard inner product) with the lowest \(d\) eigenspaces of \(\). If e.g. the covariates have a sparse dependency

\[X_{1}+X_{2}=0,\]

then \(\) contains the vector \(e_{1}+e_{2}\), so the coordinates \(\{e_{1},e_{2}\}\) will be correctly discovered. Unfortunately, if \(\) contains a more complex sparse dependency such as

\[^{-1}(X_{1}-X_{2})-X_{3}-X_{4}=0\]

where \(>0\) is very small, then this heuristic will discover \(\{e_{1},e_{2}\}\) but miss \(\{e_{3},e_{4}\}\). For this example, the solution is to notice that \(e_{3}\) and \(e_{4}\)_do_ correlate with the subspace spanned by \(()\{e_{1},e_{2}\}\) (which contains \(e_{3}+e_{4}\)). In general, if \(S\) is the set of coordinates discovered thus far, then by finding basis vectors that correlate with an appropriate subspace (of dimension at most \(|S|\)), we can efficiently augment \(S\) with at least one new coordinate from each \(t\)-sparse approximate dependency, without making \(S\) bigger by more than a factor of \(O(t)\). Iterating this augmentation \(t\) times therefore provably identifies all problematic coordinates.

To formalize this intuition, the following lemma will be needed to bound how much \(S\) grows at each iteration; it shows that the number of coordinates that correlate with a low-dimensional subspace is not too large (proof deferred to Appendix B):

**Lemma 2.5**.: _Let \(V^{n}\) be a subspace with \(d:= V\). For some \(>0\) define_

\[S=\{i[n]:_{x V\{0\}}}{\|x\|_{2} }\}.\]

_Then \(|S| d/^{2}\). Moreover, given a set of vectors that span \(V\), we can compute \(S\) in time \((n)\)._

We also define the set of vectors \(v\) that have unusually large norm outside a set \(S\), compared to \(Pv}\), which is the distance from \(v\) to the subspace spanned by the bottom \(d\) eigenvectors of \(\):

**Definition 2.6**.: For any matrix \(P^{n n}\) and subset \(S[n]\), define \(_{P,S}:=\{v^{n}:\|v_{S^{c}}\|_{2}>3Pv}\}\).

We then formalize the guarantee of each iteration of IterativePeeling() as follows:

**Lemma 2.7**.: _Let \(n,t\) and let \(P:n n\) be an orthogonal projection matrix. Suppose \( 1\) and \(K[n]\) satisfy_

1. \(P_{ii} 1-1/(9t^{2})\) _for all_ \(i K\)_,_
2. \(|(v) K|\) _for every_ \(v B_{0}(t)_{P,K}\)_._

_Then there exists a set \(_{P}(K)\) with \(|_{P}(K)| 36t^{2}|K|\) such that_

\[|(v)(_{P}(K) K)|-1\]

_for all \(v B_{0}(t)_{P,K}\). Moreover, given \(P\), \(K\), and \(t\), we can compute \(_{P}(K)\) in time \((n)\)._

**Proof sketch.** We define the set

\[_{P}(K):=\{a[n] K:_{x\{ Pe_{i},i K\}\{0\}}|}{\|x\|_{2}} 1/(6t)\}.\]

It is clear from Lemma B.2 (applied with parameters \(V:=\{Pe_{i}:i K\}\) and \(:=1/(6t)\)) that \(|_{P}(K)| 36t^{2}|K|\), and that \(_{P}(K)\) can be computed in time \((n)\). It remains to show that \(|_{P}(v)(_{P}(K) K)|-1\) for all \(v B_{0}(t)\).

Consider any \(v B_{0}(t)_{P,K}\). Then \(\|v_{K^{c}}\|_{2}>3\|Pv\|_{2}\). It's sufficient to show that \(_{P}(K)\) contains some \(j(v) K\), i.e. that there is some \(j(v) K\) such that \(e_{j}\) correlates with \(\{P_{i}:i K\}\). We accomplish this by showing that \(v_{K^{c}}\) correlates with \(Pv_{K}=_{i K}v_{i}P_{i}\).

At a high level, the reason for this is that \(v_{K^{c}}\) is close to \(Pv_{K^{c}}\) (since \(P_{i} e_{i}\) for \(i K^{c}\)), and \(Pv=Pv_{K}+Pv_{K^{c}}\) is much smaller than \(Pv_{K^{c}} v_{K^{c}}\), so \(Pv_{K}\) and \(Pv_{K^{c}}\) must be highly correlated. See Appendix B for the full proof. \(\)

We can now complete the proof of Lemma 2.4 by repeatedly invoking Lemma B.4.

**Proof of Lemma 2.4.** Let \(=_{i=1}^{n}_{i}u_{i}u_{i}^{}\) be the eigendecomposition of \(\), and let \(P:=_{i=d+1}^{n}u_{i}^{}\) be the projection onto the top \(n-d\) eigenspaces of \(\). Set \(K_{t}=\{i[n]:P_{ii}<1-1/(9t^{2})\}\). Because \((P)=n-d\) and \(P_{ii} 1\) for all \(i[n]\), it must be that \(|K_{t}| 9t^{2}d\). Also, for any \(v B_{0}(t)_{P,K_{t}}\) we have trivially by \(t\)-sparsity that \(|(v) K_{t}| t\).

Define \(K_{t-1}\) to be \(K_{t}_{P}(K_{t})\) where \(_{P}(K_{t})\) is as defined in Lemma B.4; we have the guarantees that \(|K_{t-1}|(1+36t^{2})|K_{t}|\) and \(|_{P}(v) K_{t}| t-1\) for all \(v B_{0}(t)_{P,K_{t}}\). Since \(K_{t-1} K_{t}\), it holds that \(_{P,K_{t-1}}_{P,K_{t}}\), and thus \(|_{P}(v) K_{t}| t-1\) for all \(v B_{0}(t)_{P,K_{t-1}}\). Moreover, since \(K_{t-1} K_{t}\), it obviously holds that \(P_{ii} 1-1/(9t^{2})\) for all \(i K_{t-1}\). This means we can apply Lemma B.4 with \(:=t-1\) and \(K:=K_{t-1}\) and so iteratively define sets \(K_{t-2} K_{1} K_{0}[n]\) in the same way. In the end, we obtain the set \(K_{0}[n]\) with \(|K_{0}| 9t^{2}d(1+36t^{2})t\) and \((v) K_{0}\) for all \(v B_{0}(t)_{P,K_{0}}\). The latter guarantee means that in fact \(B_{0}(t)_{P,K_{0}}=\). So for any \(t\)-sparse \(v^{n}\) it holds that

\[\|v_{K_{0}^{c}}\|_{2} 3Pv} 3_{d+1}^{-1/2}  v}\]

where the last inequality holds since \(_{d+1}P\). \(\)

### Beyond weak learning

So far, we have sketched a proof that if \(\) has few outlier eigenvalues, then there is an efficient algorithm to compute a good dictionary (as in Theorem 2.3). This gives an efficient \(\)-weak learning algorithm (via Proposition A.5). However, our ultimate goal is to find a regressor \(\) with prediction error going to \(0\) as the number of samples increases. Definition 2.1 is not strong enough to ensure this.8 However, it turns out that the dictionary constructed in Theorem 2.3 in fact satisfies a stronger guarantee9 that _is_ sufficient to achieve vanishing prediction error:

**Definition 2.8**.: Let \(^{n n}\) be a positive semi-definite matrix and let \(t,B>0\). A set \(\{D_{1},,D_{N}\}^{n}\) is a \((t,B)\)-\(_{1}\)-representation for \(\) if for any \(t\)-sparse \(v^{n}\) there is some \(^{N}\) with \(v=_{i=1}^{N}_{i}D_{i}\) and \(_{i=1}^{N}|_{i}|\|D_{i}\|_{} B\|v\|_{}\,.\)

With this definition in hand, we can actually prove the following strengthening of Theorem 2.3:

**Lemma 2.9**.: _Let \(n,t,d\). Let \(^{n n}\) be a positive semi-definite matrix with eigenvalues \(_{1}_{n}\). Then \(\) has a \((t,7}/{_{d+1}}})\)-\(_{1}\)-representation \(\) of size at most \(n+t(7t)^{2t^{2}+t}d^{t}\). Moreover, \(\) can be computed in time \(t^{O(t^{2})}d^{t}(n)\)._

**Proof sketch.** Let \(S\) be the output of \((,d,t)\). The dictionary \(\) consists of the standard basis, together with a \(\)-orthogonal basis for each set of \(t\) coordinates from \(S\). The bound on \(||\) comes from the guarantee \(|S|(7t)^{2t+1}d\). For any \(t\)-sparse vector \(v^{n}\), we know that \(v_{S^{c}}\) is efficiently represented by the standard basis (because Theorem B.1 guarantees that \(\|v_{S^{c}}\|_{2} O(_{d+1}^{-1/2}\|v\|_{})\)), and \(v_{S}\) is efficiently represented by one of the \(\)-orthonormal bases. See Appendix B for the full proof. \(\)

Why is the above guarantee useful? If each \(D_{i}\) is normalized to unit \(\)-norm, then the condition of \((t,B)\)-\(_{1}\)-representability is equivalent to \(\|\|_{1} B\|v\|_{}\). That is, with respect to the new set of features, the regressor \(\) has bounded \(_{1}\) norm. Thus, if we apply the Lasso with a set of features that is a \((t,B)\)-\(_{1}\)-representation for \(\), then standard "slow rate" guarantees hold (proof in Section A):

**Proposition 2.10**.: _Let \(n,m,N,t\) and \(B>0\). Let \(^{n n}\) be a positive semi-definite matrix and let \(\) be a \((t,B)\)-\(_{1}\)-representation of size \(N\) for \(\), normalized so that \(\|v\|_{}=1\) for all \(v\). Fix a \(t\)-sparse vector \(v^{*}^{n}\), let \(X_{1},,X_{m} N(0,)\) be independent and let \(y_{i}= X_{i},v^{*}+_{i}\) where \(_{i} N(0,^{2})\). For any \(R>0\), define_

\[*{argmin}_{w^{N}:\|w\|_{1} BR }\|Dw-y\|_{2}^{2}\]

_where \(D^{n N}\) is the matrix with columns comprising the elements of \(\), and \(^{m n}\) is the matrix with rows \(X_{1},,X_{m}\). So long as \(m=((n/))\) and \(\|w^{*}\|_{}[R/2,R]\), it holds with probability at least \(1-\) that_

\[\|D-w^{*}\|_{}^{2}=O(B\|w^{*}\|_{ }}+(4/)} {m}+\|w^{*}\|_{}^{2}(n)}{m}).\]

Combining Proposition 2.10 with Lemma 2.9 shows that there is an algorithm with time complexity \(t^{O(t^{2})}d^{t}(n)\) and sample complexity \(O((t)(_{n}/_{d+1})(n)(d))\) for finding a regressor with squared prediction error \(o(^{2}+\|v^{*}\|_{}^{2})\). This is a simplified version of Theorem 1.2. The full proof involves additional technical details (e.g. more careful analysis to take care of large eigenvalues, and to avoid needing an estimate \(R\) for \(\|w^{*}\|_{}\)) but the above exposition contains the central ideas. Theorem 1.1 similarly computes the set \(S\) from Lemma 2.4 but uses it to construct a different dictionary: the standard basis, plus a \(\)-orthonormal basis for \(S\).10 See Appendix C for the full proofs and pseudocode.

## 3 Additional Results

We now return to Question 2.2 and ask whether there are other families of ill-conditioned \(\) for which we can prove non-trivial bounds on \(_{t,}()\).

First, we ask what can be shown for _arbitrary_ covariance matrices. We prove that _every_ covariance matrix \(\) satisfies a non-trivial bound \(_{t,1/O(t^{3/2} n)}() O(n^{t-1/2})\). In fact, building on tools from computational geometry, we show the stronger result that \(\) has a \((t,O(t^{3/2} n))\)-\(_{1}\)-representation that of size \(O(n^{t-1/2})\), that is computable from samples in time \((n^{t-(1/t)})\) forany constant \(t>1\) (Theorem D.5). As a corollary, we provide the first sparse linear regression algorithm with time complexity that is a polynomial-factor better than brute force, and with near-optimal sample complexity, for any constant \(t\) and arbitrary \(\) (proof in Section D):

**Theorem 3.1**.: _Let \(n,m,t,B\) and \(>0\), and let \(^{n n}\) be a positive-definite matrix. Let \(w^{*}^{n}\) be \(t\)-sparse, and suppose \(\|w^{*}\|_{}[B/2,B]\). Suppose \(m(t n)\). Let \((X_{i},y_{i})_{i=1}^{m}\) be independent samples where \(X_{i} N(0,)\) and \(y_{i}= X_{i},w^{*}+N(0,^{2})\). Then there is an \(O(m^{2}n^{t-1/2}+n^{t-(1/t)}^{O(t)}n)\)-time algorithm that, given \((X_{i},y_{i})_{i=1}^{m}\), \(B\), and \(^{2}\), produces an estimate \(^{n}\) satisfying, with probability \(1-o(1)\),_

\[\|-w^{*}\|_{}^{2}(}{ }+\|_{}t^{3/2}}{}+\|_{}^{2}t^{3}}{m}).\]

Second, one goal is to improve "sample complexity" (i.e. obtain \(\) without dependence on condition number) without paying too much in "time complexity" (i.e. retain bounds on \(_{t,}\) that are better than \(n^{t}\)). To this end, we prove that the dependence on \(\) in the correlation level (see Fact A.4) can actually be replaced by dependence on \(\) in the dictionary size (proof in Appendix E):

**Theorem 3.2**.: _Let \(n,t\). Let \(^{n n}\) be a positive-definite matrix with condition number \(\). Then \(_{t,1/3^{t+1}}() 2^{O(t^{2})}^{2t+1} n\)._

In particular, for any constant \(t=1/\), our result shows that there is a nearly-linear size dictionary with _constant_ correlations even for covariance matrices with _polynomially-large_ condition number \( n^{/100}\). While we are not currently aware of an efficient algorithm for computing the dictionary, the above bound nonetheless raises the interesting possibility that there may be a sample-efficient and computationally-efficient weak learning algorithm under a super-constant bound on \(\).

## 4 Related work

Dealing with correlated covariates.There is considerable work on improving the performance of Lasso in situations where some clusters of covariates are highly correlated . These methods can work well for two-sparse dependencies, but generally do not work as well for higher-order dependencies -- hence they cannot be used to prove our main result. The approach of  is perhaps the closest in spirit to ours. They perform agglomerative clustering of correlated covariates, orthonormalize the clusters with respect to \(\), and apply Lasso (or solve an essentially equivalent group Lasso problem). This method fails, for example, when there is a single three-sparse dependency, and the remaining covariates have some mild correlations. Depending on the correlation threshold, their method will either aggressively merge all covariates into a single cluster, or fail to merge the dependent covariates.

Feature adaptation and preconditioning.Generalizations of Lasso via a preliminary change-of-basis (or explicitly altering the regularization term) have been studied in the past, but largely not to solve sparse linear regression per se; instead the goal has been using \(_{1}\) regularization to encourage other structural properties such as piecewise continuity (e.g. in the "fused lasso", see  for some more examples). An exception is recent work showing that a "sparse preconditioning" step can enable Lasso to be statistically efficient for sparse linear regression when the covariates have a certain Markovian structure . Our notion of feature adaptation via dictionaries generalizes sparse preconditioning, which corresponds to choosing a non-standard basis in which \(\) becomes well-conditioned and the sparsity of the signal is preserved.

Statistical query (SQ) model; sparse halfspaces.From the complexity standpoint, \(_{t,}()\) is a covering number and therefore closely corresponds to a packing number \(_{t,}()\) (see Section A.1 for the definition). This packing number is essentially the _(correlational) statistical dimension_, which governs the complexity of sparse linear regression with covariates from \(N(0,)\) in the (correlational) SQ model (see e.g.  for exposition of this model). Whereas strong \(n^{(t)}\) SQ lower bounds are known for related problems such as sparse parities with noise , no non-trivial (i.e. super-linear) lower bounds are known for sparse linear regression. Relatedly, in a COLT open problem, Feldman asked whether any non-trivial bounds can be shown for the complexity of weak learning sparse halfspaces in the SQ model . Our results also yield improved bounds for weakly SQ-learning sparse halfspaces over certain families of multivariate Gaussian distributions.

Improving brute-force for arbitrary \(\).Several prior works have suggested improvements on brute-force search for variants of \(t\)-sparse linear regression . However, all of these have limitations preventing their application to the general setting we address in Theorem 3.1. Specifically,  requires \((n^{t})\) preprocessing time on the covariates;  require noiseless responses; and  has time complexity scaling with \(^{m}n\) (since our random-design setting necessitates \(m(t n)\), their algorithm has time complexity much larger than \(n^{t}\)).