# GraphTrail: Translating Gnn Predictions into Human-Interpretable Logical Rules

Burouj Armgaan, Manthan Dalmia

Department of Computer Science & Engineering

IIT Delhi, India

csz228001@iitd.ac.in,

manthandalmia2@gmail.com

&Sourav Medya

Department of Computer Science

University of Illinois, Chicago, USA

medya@uic.edu

&Sayan Ranu

Department of Computer Science & Engineering and Yardi School of AI

IIT Delhi, India

sayanranu@cse.iitd.ac.in

###### Abstract

Instance-level explanation of graph neural networks (Gnns) is a well-studied area. These explainers, however, only explain an instance (e.g., a graph) and fail to uncover the combinatorial reasoning learned by a Gnn from the training data towards making its predictions. In this work, we introduce GraphTrail, the first _end-to-end_, post-hoc, global Gnn explainer that translates the functioning of a black-box Gnn model to a boolean formula over the (sub)graph-level concepts without relying on local explainers. GraphTrail is unique in automatically mining the discriminative subgraph-level concepts using _Shapley values_. Subsequently, the Gnn predictions are mapped to a human-interpretable boolean formula over these concepts through _symbolic regression_. Extensive experiments across diverse datasets and Gnn architectures demonstrate significant improvement over existing global explainers in mapping Gnn predictions to faithful logical formulae. The robust and accurate performance of GraphTrail makes it invaluable for improving GNNs and facilitates adoption in domains with strict transparency requirements.

## 1 Introduction and Related Works

Gnns have witnessed widespread adoption for graph-level prediction tasks due to their impressive performance . Unfortunately, like other deep-learning models, Gnns are considered black boxes due to their lack of transparency and interpretability. This lack of interpretability presents a significant barrier to their adoption in critical domains such as healthcare, finance, and law enforcement. Additionally, the ability to explain predictions is crucial for understanding potential flaws in the model and generating insights for further refinement.

**Existing Works:** To introduce interpretability of Gnns, several algorithms have been proposed in the literature . Fig. G in the Appendix, which was originally presented in , and now updated by us with more recent works, presents the taxonomy of Gnn explainability research. As observed, a vast majority of explainers focus on _instance-level_ explanations.

Instance-level (or local) explainers  take a graph as input and identify components within this graph--such as a subgraph--that maximally influence the prediction made by a model. This instance-level focus limits its ability to extract patterns utilized by Gnns at a global level across a multitude of graphs and how these patterns are combined into a single decision-making rule. The objective of our work is to develop an end-to-endglobal explainer that _(1)_ mines the subgraph concepts used by a black-box Gnn model, and then _(2)_ uncovers the boolean logic used by the Gnn over these concepts to make its predictions.

Works on global Gnn explainers are limited . XGnn  and GnnInterpreter  are generative-modeling based global explainers. Both generate a graph that maximally aligns with a specified class label. While this generated graph likely contains important features used by the Gnn in making its predictions, it may not actually be present in the dataset, limiting its utility for analyzing specific predictions. Additionally, it does not produce a human-interpretable rule explaining the class attributions made by the Gnn. Finally, XGnn requires domain-specific validity-rules as input, which affects its generalizability and results in inferior performance . Another work  evaluates which base _concepts_ are detected by the neurons of a Gnn model during predictions and their relative importance. These concepts can be subgraphs or node-level properties, such as degrees. However, this method lacks the ability to automatically mine the concepts. More importantly, this method also does not generate a human-interpretable rule for the decision-making process of the Gnn.

The closest work to ours is GLGExplainer, which shares the objective of providing a global explanation of the Gnn through a boolean formula over subgraph-level concepts. However, there are significant limitations that need to be addressed:

* **Dependency on instance-level explainers:** GLGExplainer does not mine the subgraph concepts in an end-to-end manner. Instead, it relies on an instance-level explainer (e.g. PGExplainer) to provide these concepts, over which it searches for the combinatorial formula mapping to the Gnn predictions. This dependency creates a disconnect with the objective, as instance-level explainers lack a global understanding of the model. Our proposed approach develops an end-to-end pipeline that mines concepts based on global trends.
* **Lack of interpretability due to vector-level concepts:** In GLGExplainer, each concept in the formula corresponds to a feature vector and not a subgraph. These vectors represent the embedding of a cluster of subgraphs generated by the instance explainer. Hence, in its original form, the formula is not human-interpretable. To convert into a human-interpretable formula, GLGExplainer randomly selects a subgraph from the cluster, assuming all subgraphs in a cluster are similar. Our investigation (SS 4) reveals that this assumption is rarely true in practice, compromising both interpretability and efficacy.
* **Lack of robustness:** GLGExplainer shows significant variation in the formula based on the training split used. As our analysis in SS 4 reveals, due to the reliance on instance-level explanations, even when data are drawn from the same distribution, the base concept candidates vary, and consequently so does the eventual formula.

At this juncture, we note that our work is distinct from the line of research on explainable Gnns . Explainable Gnns are designed to make explainable predictions rather than explaining the predictions of a black-box Gnn.

**Contributions:** In this work, we present an end-to-end, post-hoc, global Gnn explainer called GraphTrail (TRAnslating Gnn Prediction into human-Interpretable Logical Rules)1, which addresses the limitations outlined above. Specifically,

* **Problem formulation:** We formulate the problem of translating a message-passing Gnn model for graph classification into a human-interpretable logic formula over subgraph concepts. Unlike existing works, in our formulation, the concepts are not assumed to be an input generated through a decoupled algorithm.
* **Novel methodology:** We develop GraphTrail, which uses a mix of several innovative insights. First, GraphTrail exploits the fact that a message passing Gnn decomposes a graph into a set of computation trees. This enables GraphTrail to limit the exploration of concepts from an exponential subgraph search space to a linear space of computation trees. The global impact of computation trees is assessed using _Shapley values_, and then mapped to a boolean formula over concepts using _symbolic regression_.
* **Empirical analysis:** Extensive experiments across a diverse set of datasets, Gnn architectures and pooling function, demonstrate GraphTrail to significantly surpass existing global explainers in efficacy, human-interpretability, data efficiency, and robustness.

## 2 Preliminaries and Problem Formulation

**Definition 1** (Graph).: _A graph is defined as \(=(,,)\) over a node set \(\), edge set \(=\{(u,v) u,v\}\) and a node feature matrix \(=\{_{v} v\}\) where \(_{v}^{d}\) is the set of features characterizing each node._

Two graphs are termed identical if they are _isomorphic_ to each other.

**Definition 2** (Graph Isomorphism).: _Graph \(_{1}\) is isomorphic to graph \(_{2}\) (denoted as \(_{1}_{2}\)) if there exists a bijection between their node sets that preserves the edge connectivity and node features. Specifically, \(_{1}_{2} f:_{1} _{2}\) such that: \((1)\)\(f\) is a bijection, \((2)\)\(_{v}=_{f(v)},\) where \(_{v}_{1},_{f(v)}_{2}\) and \((3)\)\((u,v)_{1}\) if and only if \((f(u),f(v))_{2}\)._

Graph \(_{1}\) is _subgraph isomorphic_ to \(_{2}\), denoted as \(_{1}_{2}\), if \(f\) is an _injection_ and condition (3) is modified to \((u,v)_{1}\) if \((f(u),f(v))_{2}\).

**Definition 3** (Graph Classification).: _In graph classification, we are given a set of train graphs \(_{tr}=\{_{1},,_{m}\}\), where each graph \(_{i}\) is tagged with a class label \(_{i}\) from the set \(\{_{1},,_{c}\}\). The objective is to train a Gnn model \(\) such that given a graph with an unknown class label, the label prediction error is minimized._

Error may be measured using any of the known metrics such as cross-entropy loss, negative log-likelihood, etc. Hereon, we implicitly assume \(\) to be a _message-passing_Gnn. We assume the Gnn\(\) returns a \(c\)-dimensional distribution over the class labels, where \(()_{j}\) is the probability of the \(j\)-th class, and \(()^{*}=_{j\{1,2,,c\}}\{()_{j}\}\) denotes the predicted class label.

**Definition 4** (Concepts ).: _Concepts refer to semantically meaningful units of information within the data that humans use to analyze and make decisions about that dataset. In the context of graph classification, these concepts correspond to subgraphs._

**Problem 1** (Global Gnn explanation through boolean logic over concepts).: _Let \(\) be a set of graphs, where each graph is labeled with a class from the set \(\{_{1},,_{c}\}\). Given a trained Gnn\(\), our objective is to learn a set of \(c\) boolean formulas \(\{f_{1},,f_{c}\}\), over subgraph-level concepts such that for any graph \(\), if \(()^{*}=_{i}\), then \(f_{i}()=\)TRUE and \( j i,\ f_{j}()=\)FALSE. The candidate space of concepts includes all unique subgraphs of the dataset, i.e., \(=\{, \}\)._

The proposed problem formulation surfaces two key challenges:

1. _How do we extract the concepts?_ Concept extraction poses a significant challenge due to the exponential size of the candidate space. In the worst case, a graph with \(n\) nodes can have \(2^{n}\) possible subgraphs. Furthermore, if there are \(n\) subgraphs in the dataset, identifying the _unique_ subgraphs requires performing \((n^{2})\) graph isomorphism tests. The computation cost of graph isomorphism grows exponentially with graph size.
2. _How do we uncover the boolean logic over the concepts?_ The number of boolean formulas associated with a set of symbols (concepts) increases exponentially with the size of the set. Hence, the scalability challenge is further exacerbated.

## 3 GraphTrail: Proposed Global Explainer

Fig. 1 presents the pipeline of GraphTrail. GraphTrail builds on the observation that any message passing Gnn decomposes a graph of \(n\) nodes into \(n\)_computation trees_. Consequently, only the subgraphs corresponding to these computation trees are processed by the Gnn and the rest of the subgraphs are irrelevant to the Gnn's predictions. This property allows us to reduce the candidate space size from exponential to linear. Subsequently, the impact of each of the computation trees is assessed through its _Shapley Value_, and the top-\(k\) trees are sent to the logic formulator. The Boolean logic is revealed by _the symbolic regression_ over these computation trees. Symbolic regression aims to discover concise closed-form mathematical equations that best fit a given set of data . In our context, we constrain the regressor over computation trees with boolean operators and fit to the predictions of Gnn. We next elaborate on each of these steps.

### Computation Framework of Message-passing GNNs

Gnns aggregate messages layer by layer. If \(_{v}^{|F|}\) is the input feature vector for node \(v\), then the \(0^{th}\) layer representation of node \(v\) is set to \(^{0}_{v}=_{v}\). In each of the subsequent layers \(\)

[MISSING_PAGE_EMPTY:4]

**Lemma 1**.: _In an \(L\)-layered Gnn, the computation tree \(^{L}_{v}\) is sufficient to compute node embedding \(^{L}_{v}\), \( v\)._

Proof.: In a single layer of message-passing, a node \(v\) receives messages from each of its immediate neighbors. Over \(L\) layers, \(v\) collects messages from nodes within \(L\) hops. The computation tree \(^{L}_{v}\) contains all such paths of length up to \(L\) and hence is sufficient to compute \(^{L}_{v}\). 

**Lemma 2**.: \(^{L}_{v}^{L}_{u}^{L}_{v}=^{L}_{u}\)_._

Proof.: The expressive power of a message-passing Gnn is upper bounded by the _Weisfeiler-Lehman test (1-WL)_. This implies that if the \(L\)-hop neighborhoods of two nodes are indistinguishable by 1-WL, then their representations will be the same. The 1-WL test cannot differentiate between nodes with identical computation trees . 

Owing to Lemma 1 and Lemma 2, given a graph \(=(,,)\), we can decompose it into a multiset of computation trees \(_{}=\{^{L}_{v} v\}\), compute node embeddings \(^{L}_{v},\ \  v\), and then aggregate them into graph embedding \(_{}\) (Eq. 4) without incurring any loss of information. This computation structure enables us to shrink the candidate space of concepts, originally defined in Prob. 1, to the set of unique computation trees, i.e., \(=_{}_{}\). The reformulation of the concept space imparts several desirable side-effects.

* First, the size of the candidate space of concepts reduces from exponential to \(_{}||\), which is linear to the graph size (i.e., number of nodes) and the number of graphs in the dataset.
* Second, while graph isomorphism has a computational cost exponential to the graph size, rooted-tree isomorphism can be performed in time linear to the number of edges in the tree (See App. A for details). Therefore, distilling all computation trees to only the unique ones can be done in linear time.
* Third, Gnn may map non-isomorphic \(L\)-hop node neighborhoods (subgraphs) to isomorphic computation trees (See Fig. 2). This further reduces the number of concept candidates and, consequently, the computational burden.

### Mining Concepts

To mine concepts from the candidate space, we assess their _Shapley values_. Shapley value  is a co-operative game theoretic technique where players form coalitions and receive payouts based on their contribution to the outcome. Mathematically, the Shapley value \(_{i}()\) of player \(i\) is its average marginal contribution across all possible combinations of features.

\[_{i}(V)=_{S\{1, p\}\{i\}}(V(S\{i\})-V(S)) \]

Here, \(p\) is the total number of players, \(S\) represents a subset of players excluding player \(i\) and the _value function_\(V(.)\) quantifies the outcome.

With the hypotheses that some combination of computation trees is responsible for Gnn's predictions, we aim to leverage Shapley values in identifying this combination. Consequently, the players correspond to computation trees, and the value function corresponds to the performance of the Gnn when restricted to the chosen subset of computation trees. We set the value function as the _negated cross-entropy error_ of Gnn \(\)'s prediction (one may use other accuracy measures as well). Thus, Eq. 5 is re-expressed as:

Figure 2: The figure illustrates the process of constructing the computation trees of nodes \(v_{1}_{1}\) and \(u_{1}_{2}\) for \(L=2\). The colors of the nodes represent the node labels. Note that although \(v_{1}\) and \(u_{1}\) are embedded in non-isomorphic \(L\)-hop neighborhoods, their computation trees are isomorphic.

\[_{i}() =_{ S,S\{C_{i}\}}|-|S|-1)!}{||!}(V(S\{C_{i}\})-V(S)) \] \[V(S) =_{}_{j=1}^{c}_{j}^{}((^{S})_{j}) \]

In Eq. 6, \(=\{C_{1},,C_{m}\}\) is the set of unique computation trees, and \(C_{i}\) denotes the \(i\)-th one. Eq. 7 represents the negated cross-entropy error. In Eq. 7, we slightly overload the notation introduced in Def. 3 as follows: we assume that the class label information is maintained as a \(c\)-dimensional one-hot vector. Thus, \(_{j}^{}\) is \(1\) if the true class label of \(\) is \(j\)-th label. The Gnn\(\) returns a distribution over the class labels, where \((^{S})_{j}\) is the probability of the \(j\)-th class, when \(\) is embedded by only considering the computation trees within \(S\). Mathematically, this means modifying Eq. 4 as follows:

\[_{}^{S}=(\{_{v}^{L}\ | \  v_{v}^{L} S\}) \]

Finally, we select the top-\(k\) computation trees with the highest Shapley value as concepts. We discuss our procedure for selecting \(k\) in the next section (SS 3.4).

#### 3.3.1 Computational Tractability

While we are able to map the problem of concept mining to Shapley Value computations, several computational challenges arise. Firstly, Shapley value computation necessitates sampling all possible subsets of players (computation trees), leading to exponential computation complexity. Secondly, we need to compute \(_{}^{S}\), i.e., the graph embedding corresponding to each subset of trees \(S\). The naive approach involves: **(1)** enumerating computation trees rooted in each node, which consumes \((||)\) time, **(2)** performing \((|S|||)\) tree isomorphisms to detect computation trees contained within \(S\) and then **(3)** computing Eq. 8. Given that Shapley computation requires drawing multiple samples of \(S\) (Eq. 6), the above pipeline for each sample \(S\) is prohibitively slow.

Fortunately, there are works to circumvent the first challenge via sampling a small set of coalitions with probabilistic approximations . We use the sampling strategy outlined in  (see App. B for details).

The second challenge is unique since computing Shapley on computation trees of Gnns has not been studied. GraphShap , the only other work to study Shapley value in the context of explaining graph classification, looks at the specific class of identity-aware graphs, where nodes with unique identities recur across multiple graph views. More importantly, GraphShap is not customized for Gnns and hence the context of computation tree does not arise.

To address the bottleneck of computing Eq. 8, we project each graph into a _concept vector_.

**Definition 6** (Concept vector).: _Let \(=\{C_{1},,C_{m}\}\) be the set of unique computation trees. The concept vector \(_{}_{ 0}^{m}\) of graph \(=(,,)\) is an \(m\)-dimensional vector where the \(i\)-th dimension represents the number of computation trees in \(\) that are isomorphic to the \(i\)-th concept candidate \(C_{i}\). Mathematically, \(_{}[i]=|\{v\ |\ _{v}^{L} C_{i}\}|\)._

The "concept encoding" box in Fig. 1 illustrates this operation. From Lemma 1, the embedding of the root node of \(C_{i}\) depends solely on \(C_{i}\). Hence, we pre-compute and map each \(C_{i}\) into an embedding \(_{i}\). By combining Lemma 2 with Eq.4, we deduce that for both MeanPool and SumPool, the two most common aggregators can be computed in \(O(|S|)\) time. Specifically,

\[\ \ _{}^{S} =_{C_{i} S}_{}[i]_{i} \] \[\ \ _{}^{S} =|}_{C_{i} S}_{}[i] _{i} \]

Owing to Lemma 1 and Lemma 2, computing the graph embedding simply involves \(|S|\) look-ups of vectors instead of fresh rounds of message-passing.

### Generation of Logical Rules through Symbolic Regression

Let us denote the top-\(k\) computation trees with the \(k\) highest Shapley values as \(^{*}\). To discover logical rules over \(^{*}\), we perform _symbolic regression_. Symbolic regression is a branch of regression analysis that aims to discover symbolic expressions from experimental data . Formally, given a set of input-output pairs \(\{(_{i},y_{i})\}_{i=1}^{n}\), where \(_{i}^{d}\) is an input vector, and \(y_{i}\) is the output value (or label), and a set of operators, such as addition, subtraction, logarithm, trigonometric functions, boolean logic, etc., the goal is to find a symbolic equation \(e\) and corresponding function \(f_{e}\) such that \( i,\ y_{i} f_{e}(_{i})\).

To perform symbolic regression over computation trees, we first project the concept vector of each graph \(=(,,)\) on the top-\(k\) trees \(^{*}\). This distilled concept vector is denoted as \(^{*}_{}_{>0}^{k}\), where \(^{*}_{}[i]=|\{v^{L}_{v} C _{i}\}|\), with \(C_{i}\) representing the \(i^{}\) ranked computation tree in \(^{*}\) based on Shapley value. While symbolic regression can accommodate a wide array of operators, mathematical operations such as addition or multiplication are not meaningful when applied to computation trees. Hence, to facilitate human interpretability, we restrict to the boolean operators: conjunction (\(\)), disjunction (\(\)), negation (\(\)), and XOR (\(\)). We aim to learn a symbolic function \(f_{c}\) for each class label \(c\) predicted by the Gnn. Let \(_{c}=\{()^{*}=c\}\) be the subset of graphs where the predicted label is \(c\). The symbolic function \(f_{c}\) is identified by minimizing a multi-objective loss function that balances prediction error and model complexity. Formally,

\[(f_{c})=Error(f_{c})+()Complexity(f_{c}) \]

\(Complexity(f_{c})\) corresponds to the number of boolean operators and variables in \(f_{c}\). \(\) is a weighting hyper-parameter. Error corresponds to the number of disagreements with the Gnn. Formally,

\[Error(f_{c})=_{_{c}}1-f_{c}( )+_{^{}_{e}}f_{c}() \]

Since \(f_{c}\) is a boolean function, it returns either TRUE (equivalently \(1\)) or FALSE (\(0\)).

Symbolic regression poses a combinatorial optimization challenge, as the number of potential functions increases exponentially with the number of symbols, which is \(k\) in our case. Fortunately, the area is rich with several studies . We use , which leverages a multi-population evolutionary algorithm to efficiently optimize and identify symbolic expressions (See App. C for details).

As depicted in Fig. 1, following the generation of boolean functions, GraphTrail concludes.

**Identifying \(k\) - the number of concepts:** The higher the value of \(k\), the more expressive symbolic regression is to fit to the prediction data by Gnn. On the other hand, the computation cost grows monotonically with \(k\). To optimize this balance, we start with a small value of \(k\), and incrementally increase it until the accuracy plateaus, similar to using the patience parameter to determine the number of epochs in machine learning model training.

## 4 Experiments

In this section, we evaluate GraphTrail and benchmark its performance in translating predictions of various Gnn architectures across diverse datasets and pooling layers. More experiments can be found in the appendix. The codebase of GraphTrail is shared at [https://github.com/idea-iitd/GraphTrail](https://github.com/idea-iitd/GraphTrail).

### Experimental Setup

**Baselines:** As discussed in SS 1, GLGExplainer is the only existing algorithm that fits a logical formula to Gnn predictions, making it our primary baseline. Recall, GLGExplainer generates a formula over vectors representing embeddings of subgraph clusters rather than individual subgraphs, rendering it non-interpretable. The formula is applied to an input graph by first processing it through an instance-level explainer, embedding the explanation subgraphs into feature space, and assigning them to the closest cluster. These cluster vectors are then considered present in the input graph. To make the formula interpretable, each cluster vector is replaced by the subgraph closest to the cluster representation in the embedding space. A subgraph (concept) \(_{S}\) in the formula is considered present in an input graph \(\) if \(_{S}\). We term this version GLGExplainer-iso.

We use the authors' implementation of GLGExplainer with the recommended parameters and employ PGExplainer as the instance-level explainer, as suggested. The implementation of PGExplainer is available in Pytorch Geometric.

**GraphTrail-S:** Shapley value computation brings the computational bottleneck in the pipeline, which involves calculating the global Shapley value for each computation tree on each graph. This process can become impractical for datasets with a large number of graphs. To address this issue, we propose GraphTrail-S, an efficient version of GraphTrail. Note that we are interested in the ranking according to the Shapley values and not the values themselves. If a subset of the dataset can emulate the entire dataset, the generated ranking is expected to remain the same. Thus, GraphTrail-S creates a stratified split of the training dataset that maintains the class ratios and computes the global Shapley values using this subset. The rest of the pipeline remains unchanged, allowing for a more efficient yet effective method. This is also shown in the results from Table 1 where GraphTrail-s is slightly inferior to GraphTrail.

**Datasets:** We use four benchmark datasets listed in Table C (App. D). While NCI1 , MUTAG , and Mutagenicity  are collections of molecules, BAMultiShapes  is a synthetic dataset specifically curated to benchmark global Gnn explainers and associated with ground truth logical explanations. Further details are presented in App. D.

**Evaluation framework:** While we benchmark against various Gnn architectures and Pool layers (Eq. 4), the default architecture is set to Gat for MUTAG and Mutagenicity and Gin for the other two. We use SumPool as the default across datasets. The accuracies of the Gnn architectures across all Pool choices are presented in Table D in the Appendix. Details on the hardware platform, train-val-test splits and other parameters are provided in App. E. All experiments have been repeated with three seeds and we report the means and standard deviations.

**Metrics:** We evaluate explainer faithfulness using _Fidelity_; the ratio of graphs where the logical formula matches the Gnn class label. Additional metrics can be found in the appendix.

### Fidelity

Is GraphTrail faithful in translating Gnn through logical rules? Table 1 provides the answer with several key observations.

First, both GraphTrail and GraphTrail-s significantly outperform GLGExplainer. GraphTrail computes Shapley values by evaluating all subsets of concepts, while GraphTrail-s employs a faster, sampling-based method. The key reason for the superior performance of GraphTrail and GraphTrail-s over GLGExplainer lies in aligning the concept definitions with the fundamental computational units of a Gnn, namely computational trees. GraphTrail then mines these concepts with a global perspective by analyzing their Shapley values. In contrast, GLGExplainer depends on instance-level explainers, which lack the ability to detect global patterns.

Second, GLGExplainer-iso consistently underperforms compared to GLGExplainer. This is because the clusters of instance-level subgraph explanations in GLGExplainer-iso are not _pure_ (See App. I). Although they are grouped based on embedding distance, their structures vary significantly. Representing a cluster by an exemplar subgraph fails to capture the diversity effectively.

Finally, GLGExplainer results do not match the original results reported in . In App. H, we discuss the causes, including the identification of test data leakage in the Mutagenicity data set.

### Robustness

_Varying architectures and pooling mechanisms._ In Table 2, we present the robustness of GraphTrail and GLGExplainer across various Gnn architectures and Pool layers. GraphTrail achieves

    & BAMultiShapes & MUTAG & Mutagenicity & NCI1 \\  GLG & 0.48 \(\) 0.02 & 0.74 \(\) 0.10 & 0.62 \(\) 0.03 & 0.57 \(\) 0.04 \\ GLG-iso & 0.00 \(\) 0.00 & 0.61 \(\) 0.20 & 0.53 \(\) 0.06 & 0.49 \(\) 0.05 \\ G-Trail-S & 0.86 \(\) 0.01 & 0.78 \(\) 0.08 & 0.72 \(\) 0.01 & 0.72 \(\) 0.02 \\ G-Trail & **0.87 \(\) 0.02** & **0.82 \(\) 0.09** & **0.72 \(\) 0.01** & **0.72 \(\) 0.02** \\   

Table 1: Average fidelity of the formulae across three seeds. The best and second best results are shown in bold and underlined, respectively. G-Trail and GLG represents GraphTrail and GLGExplainer respectively.

    &  &  \\  & GCN & GAT & Gin & SUM & MEAN & MAX \\   & G-Trail & GLG & G-Trail & GLG & G-Trail & GLG & G-Trail & GLG & G-Trail & GLG & G-Trail & GLG \\  BAMultiShapes & **0.81** & 0.49 & **1.00** & 0.93 & **0.87** & 0.48 & **0.87** & 0.48 & **0.88** & 0.50 & **0.89** & 0.52 \\ MUTAG & **0.87** & 0.80 & **0.82** & 0.74 & **0.87** & 0.55 & **0.82** & 0.74 & **0.77** & 0.54 & **0.78** & 0.74 \\ Mutagenicity & **0.72** & 0.63 & **0.72** & 0.62 & **0.73** & 0.64 & **0.72** & 0.62 & **0.70** & 0.62 & **0.69** & 0.68 \\ NCI1 & **0.75** & 0.59 & **0.73** & 0.65 & **0.72** & 0.57 & **0.72** & 0.57 & **0.70** & 0.59 & **0.66** & 0.62 \\   

Table 2: Comparison of Fidelity across Gnn architectures and Pool layers. The best result in each dataset and category is highlighted in bold.

higher fidelity than GLGExplainer across all scenarios. This result further substantiates the superior ability of GraphTrail in faithfully translating Gnn outcomes.

_Varying \(k\)._ Fig. 3 explores the relationship between the number of mined concepts (\(k\)) and the fidelity achieved by GraphTrail across various datasets. Recall that higher values of \(k\) allow for greater expressivity in symbolic regression. Consistent with this, the results exhibit an increasing trend, with fidelity approaching saturation at \(k=50\) for most datasets.

### Data Efficiency

In Fig. 4, we assess the explainers' effectiveness in low-data regimes by varying the train data volume on the \(x\)-axis and measuring Fidelity on the \(y\)-axis. We expect Fidelity to decrease with lesser data. In GraphTrail, the decrease in Fidelity with lesser data is minor, indicating robust efficacy even with limited data and that it can be used in low-resource settings. Notably, the higher average Fidelity and lower standard deviation in GraphTrail across sizes indicate enhanced stability.

### Visual Analysis of Rules

Fig. 5 presents the rules inferred by GraphTrail and GLGExplainer for the datasets MUTAG, Mutagenicity, and BAMultiShapes.

**MUTAG**: According to , compounds with rings and electron-attracting group elements conjugated with nitro groups enhance mutagenicity. GraphTrail identifies both the ring structures as well as the nitro group as identifiers of mutagenicity in this dataset. In contrast, GLGExplainer fails to identify the ring structure.

**Mutagenicity:** Prior work [55; 45; 29; 2] often limits the mutagenic behavior in this dataset to \(NO_{2}\) and \(NH_{2}\), i.e., the nitro and amine groups. However, in , eight general toxicophores are capable of identifying 75% of all mutagens in the dataset (see Fig. F ). As seen in Fig. 5, without any supervision, GraphTrail identifies and incorporates six of them in its formula, viz. aromatic nitro, aromatic amine, nitroso, unsubstituted heteroatom-bonded heteroatom, azo-type, and polycyclic aromatic systems. The azo-type and the unsubstituted heteroatom-bonded heteroatom groups have not been identified explicitly but are potential extensions of two of the identified structures.

**BAMultiShapes**: BAMultiShapes has known ground truth logical rules for its class labels. While GraphTrail does not perfectly recover these rules, GraphTrail's inferred concepts and their logical combination exhibit greater similarity to the ground truth compared to GLGExplainer's results. It should be noted that explainers try to find what the model has learned rather than the ground truth. One striking observation is that GLGExplainer's extracted concepts bear no similarity to the ground truth and two of the prototypes have no graphs in their clusters and are therefore empty. In contrast, GraphTrail identifies concepts that are isomorphic to the grid and house motifs present in the ground truth.

## 5 Conclusions, Limitations and Future Directions

In this work, we have designed an end-to-end, post-hoc, global graph neural network (Gnn) explainer called GraphTrail. We have formulated the problem of translating a message-passing Gnn model

Figure 4: Results on the test set fidelity averaged over three seeds while varying the training data size.

Figure 3: Impact of \(k\) (number of concepts) on Fidelity.

for graph classification into a human-interpretable logic formula over subgraph concepts without assuming the concepts are pre-generated through a separate algorithm. GraphTrail leverages several novel insights such as the decomposition of graphs into computation trees by message-passing Gnns, which reduces the search space for concepts from exponential number of all possible subgraphs to a linear space of computation trees. GraphTrail evaluates the impact of these computation trees using Shapley values and then creates a boolean formula via symbolic regression only with the important ones. Extensive empirical analyses across diverse datasets and different Gnn architectures demonstrate GraphTrail's effectiveness in capturing the underlying logical relationships within complex Gnn models for a wide range of scenarios in a human-interpretable form.

**Limitations:** While both GraphTrail and GLGExplainer express rules as logical combinations of concept's presence or absence, they currently lack the ability to capture the multiplicity of a concept's occurrence within a graph. Finally, we believe attempting to interpret Gnns with subgraph level concepts creates a dilemma between interpretability and faithfulness to the computational mechanism of Gnns. To elaborate, in GraphTrail, since symbolic regression generates the formula over computation trees, we map it back to the graph space, by replacing each computation tree with the most frequent \(L\)-hop ego graph that generates this tree. Note, Gnns operate at the level of computation trees. Given any formula at a subgraph level, a new formula can be generated with identical fidelity, by simply replacing each subgraph concept with one that produces the same set of computation trees (recall Fig. 2). However, when concepts are lowered to computation tree level, human interpretability is compromised. It is worth exploring how the best balance between human interpretability  and staying faithful to Gnn computation structure can be obtained.

Figure 5: Visual inspection of the formulae generated by GraphTrail and GLGExplainer. The encircled numbers in Mutagenicity are toxicophore IDs from Fig. F (in Appendix). The structures with two IDs in Mutagenicity can be extended to both toxicophores.

Acknowledgements

Burouj Armgaan acknowledges the support of the Prime Minister's Research Fellowship (PMRF) for funding this research.