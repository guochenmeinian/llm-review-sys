# _VastTrack_: Vast Category Visual Object Tracking

Liang Peng\({}^{1*}\), Junyuan Gao\({}^{1*}\), Xinran Liu\({}^{1,3*}\), Weihong Li\({}^{1,2,3*}\), Shaohua Dong\({}^{4*}\), Zhipeng Zhang\({}^{5}\), Heng Fan\({}^{4}\), Libo Zhang\({}^{1,2,3}\)

\({}^{1}\)Institute of Software Chinese Academy of Sciences \({}^{2}\)Hangzhou Institute for Advanced Study

\({}^{3}\)University of Chinese Academy of Sciences \({}^{4}\)University of North Texas \({}^{5}\)KargoBot

\({}^{*}\)Equal contribution \({}^{}\)Equal advising and co-last authors \({}^{}\)Corresponding author

###### Abstract

In this paper, we propose a novel benchmark, named _VastTrack_, aiming to facilitate the development of general visual tracking via encompassing abundant classes and videos. VastTrack consists of a few attractive properties: **(1)**_Vast Object Category_. In particular, it covers targets from 2,115 categories, significantly surpassing object classes of existing popular benchmarks (_e.g._, GOT-10k with 563 classes and LaSOT with 70 categories). Through providing such vast object classes, we expect to learn more general object tracking. **(2)**_Larger scale_. Compared with current benchmarks, VastTrack provides 50,610 videos with 4.2 million frames, which makes it to date the largest dataset in term of the number of videos, and hence could benefit training even more powerful visual trackers in the deep learning era. **(3)**_Rich Annotation_. Besides conventional bounding box annotations, VastTrack also provides linguistic descriptions with more than 50K sentences for the videos. Such rich annotations of VastTrack enable the development of both vision-only and vision-language tracking. In order to ensure precise annotation, each frame in the videos is manually labeled with multi-stage of careful inspections and refinements. To understand performance of existing trackers and to provide baselines for future comparison, we extensively evaluate 25 representative trackers. The results, not surprisingly, display significant drops compared to those on current datasets due to lack of abundant categories and videos from diverse scenarios for training, and more efforts are urgently required to improve general visual tracking. Our VastTrack, the toolkit, and evaluation results are publicly available at [https://github.com/HengLan/VastTrack](https://github.com/HengLan/VastTrack).

## 1 Introduction

Visual tracking is a fundamental computer vision problem with many applications such as surveillance and robotics. The ultimate goal for tacking is to localize the target of an _arbitrary_ category in an _arbitrary_ scenario from a sequence, given its initial position, which we term _universal visual object tracking_. For such goal, numerous trackers have been proposed in recent decades . In particular, with the introduction of several large-scale tracking benchmarks (_e.g._, ) in the deep learning era, considerable advancements (_e.g._, ) have been seen in the visual tracking community. Despite this, it remains challenging to achieve universal tracking.

One important reason is relatively _restricted_ number of object categories in current tracking benchmarks. The objects in the real world are from _countless_ categories. To achieve general visual tracking like humans, the tracker is expected to "SEE" various sequences from an extremely large set of object categories during training to acquire the generalization ability. Nevertheless, the categories in existing large-scale benchmarks are rather _limited_. For example, the popular TrackingNet  and LaSOT  comprise respectively 27 and 70 categories (see Fig. 1), which fall short for training universally generalizable trackers. Another popular dataset GOT-10k  aims to handle this by largely expanding the number of object categories to 563. Despite its success in advancing generic-purposetracking, the 563 object categories are still insufficient to represent massive diversity of categories present in the real world. Besides training, a real general tracking system requires evaluation on videos of vast object categories, which can help mitigate biases to certain classes for more faithful assessment in real applications. Nevertheless, the test sets of existing large-scale benchmarks (_e.g._, ) all consist of _less than_ 100 categories, which may not be enough for faithful assessment of general tracking.

Besides rich categories, abundant videos with high-quality annotations are crucial for learning robust visual trackers. Particularly, as a tracking model becomes larger and more complicated, _e.g._, from CNNs  to Transformer , more videos are demanded to unleash the power of deep network for achieving robustness and generality. While there have been extensive efforts to develop tracking datasets, they are comparatively small in scale or limited in annotation quality. For example, currently the largest (in term of video number) benchmark  with _precise_ annotations has only 10K videos, which may still be inadequate for training generalized trackers, as evidenced by enhanced performance  on it when using extra training videos. Although another benchmark  offers more videos, its annotations are not precise, which may degrade performance.

More recently, language has demonstrated great potential to enhance robustness of general tracking, and the resulted paradigm, the so-called _vision-language tracking_ (_e.g._, ), has attracted increasing attention. For learning a robust and general vision-language tracker, it is crucial to provide ample videos with visual and linguistic annotations. Although there are several datasets (_e.g._, ) created for this goal, the number of linguistic sentences are limited in scale (_e.g._, 1.4K in  and 2K in ), which may impede the exploration of more general vision-language tracking.

In order to alleviate the aforementioned limitations in existing datasets for developing more general visual tracking, we propose _VastTrack_, an innovative large-scale benchmark for **Vast**-category short-term object **Tracking** via comprising abundant classes and video from diverse scenarios. In particular, VastTrack makes the following efforts for facilitating the development of general object tracking:

**(1) _Vast Object Category_:** To enrich the diversity in object categories for general tracking, VastTrack consists of videos from 2,115 classes, which largely surpasses category number in popular benchmarks such as GOT-10k  with 563 classes and LaSOT  with 70 classes, as displayed in Fig. 1. To our best of our knowledge, VastTrack is the richest tracking dataset with the largest number of categories. With such vast object classes, we expect to accelerate the exploration towards more general tracking.

**(2) _Larger Scale_:** For learning robust universal tracking, our VastTrack offers 50,610 video sequences with 4.2 million frames, which makes it so far the largest and the most diverse tracking dataset in terms of the numbers of videos and targets compared to existing datasets (_e.g._), as shown in Fig. 1. Such a larger scale and diversity of VastTrack in videos and targets can potentially benefit training more powerful trackers, particularly Transformer-based models, in the deep learning era.

**(3) _Rich and Precise Annotations_:** Considering the benefits of language for enhancing general object tracking, VastTrack offers both standard bounding box annotations and rich linguistic specifications for the sequences, and thus enables exploration of both the vision-only and vision-language universal tracking. Compared with current benchmarks (_e.g._,  with 1.4K and  with 2K sentences) for vision-language tracking, the proposed VastTrack provides over 50K descriptions, a magnitude order larger than , of more and diverse targets for better vision-language tracking. In addition, to ensure precise annotations, each video in VastTrack is manually labeled with multi-round refinements.

In order to understand the performance of existing trackers on VastTrack and to provide baseline for future comparison, we extensively evaluate 25 recent representative algorithms in a hybrid protocol in which the test videos have partial overlap with the training sequences (as described later) and conduct

Figure 1: Summary of representative benchmarks, comprising OTR-2013/2015 , TC-128 , UAV123 , NUS-PRO , UAV20L , VOT-2017 , OxUvA , GOT-10k , TrackingNet , and VastTrack. We can clearly see that VastTrack is _larger_ than all other datasets by containing 2,115 object categories and 50,610 videos. _Best viewed in color for all figures_.

in-depth analysis. The evaluation reveals that, not surprisingly, current top-performing object trackers significantly degrade on the more challenging VastTrack. For example, the success scores of existing state-of-the-art trackers, _e.g._, SeqTrack , MixFormer , and OSTrack , degrade from 0.725, 0.724, and 0.711 on LaSOT  to 0.396 (with a drop of 0.329), 0.395 (with a drop of 0.329), and 0.336 (with a drop of 0.375) on VastTrack. This demonstrate the challenge in achieving universal tracking for current trackers, and more efforts are desired to improve general-purpose object tracking.

By releasing VastTrack, we expect to offer a new large-scale platform with abundant videos from vast categories for facilitating the development of more general and universal tracking and its applications. In summary, our main _contributions_ are as follows: \(\) We introduce a new benchmark VastTrack that covers 2,115 object categories to facilitate more general tracking; \(\) VastTrack provides a large scale of 50,610 videos which could benefit developing more powerful deep trackers; \(\) Rich annotations in VastTrack enable exploration of both vision-only and vision-language tracking; \(\) Evaluation of 25 trackers is conducted to understand VastTrack and provides baselines for future comparison.

## 2 Related Work

**Visual Tracking Benchmarks.** Benchmarks have been crucial for development of tracking. Early tracking benchmarks are usually in small scale and mainly aim at the evaluation purpose for fairly comparing different algorithms. OTB-2013  is the first tracking benchmark by introducing 51 videos and later extended in  by adding new sequences. VOT  presents a series of challenges to compare trackers in different aspects. TC-128  contains 128 colorful videos to study the impact of color information on tracking models. NIS  assesses tracking performance by providing 100 sequences with high frame rate. UAV123 and UAV20L  respectively consist of 123 and 20 videos captured by unmanned aerial vehicle for tracking performance evaluation. NUS-PRO  offers 365 videos to assess trackers on rigid target objects. OxUvA  contains 366 sequences for evaluating long-term tracking performance of different algorithms. From a different perspective than opaque tracking benchmarks, TOTB  collects 225 videos for investigating transparent object tracking.

Despite facilitating tracking, early datasets are limited in scale and cannot provide videos for training deep tracking. To alleviate this, several large-scale benchmarks have been introduced in recent years. TrackingNet  presents a large-scale dataset with around 30K videos for training deep tracking. However, its annotations are generated using a tracker, which may be inaccurate and thus degrade the training of deep tracking. LaSOT  comprises 1,400 long-term videos with precise dense annotations, and is later extended in  by adding more videos. Notably, it provides both bounding box and language annotation to enable both vision-only and vision-language tracking. GOT-10  contributes a large benchmark with around 10K sequences from 563 classes. Despite advancing deep tracking, 563 object classes may still be insufficient to represent massive categories in the real world.

VastTrack is related to the aforementioned large-scale datasets but provides a more diverse and larger platform with more than 50K videos from 2,115 categories, which aims to accelerate the exploration towards universal and general tracking. Tab. 1 shows the comparison of VastTrack with other datasets.

    &  &  &  &  &  &  &  &  \\  & & & & & & & & & & & & \\  & & & & & & & & & & & & \\  & & & & & & & & & & & & \\   OTB-2013 & 2013 & 10 & 50 & 578 & 29K & 16.4 min & ✗ & 11 & ✗ & 30.6\_fps_ & ST & Eva. \\ OTB-2015 & 2015 & 16 & 100 & 590 & 59K & 32.8 min & ✗ & 11 & ✗ & 30.6\_fps_ & ST & Eva. \\ TC-128 & 2015 & 27 & 128 & 429 & 55K & 30.7 min & ✗ & 11 & ✗ & 30.6\_fps_ & ST & Eva. \\ NUS-PRO  & 2016 & 17 & 365 & 371 & 135K & 75.2 min & ✗ & 12 & ✗ & 30.6\_fps_ & ST & Eva. \\ UAV213 & 2016 & 9 & 123 & 915 & 113K & 62.5 min & ✗ & 12 & ✗ & 30.6\_fps_ & ST & Eva. \\ UAV20L & 2016 & 5 & 120 & 29.34 & 59K & 32.6 min & ✗ & 12 & ✗ & 30.6\_fps_ & LT & Eva. \\ NIS  & 2017 & 17 & 100 & 3,830 & 383K & 26.6 min & ✗ & 9 & ✗ & 240 \_fps_ & ST & Eva. \\ VOT-2017 & 2017 & 24 & 60 & 356 & 21K & 11.9 min & ✗ & 24 & ✗ & 30.6\_fps_ & ST & Eva. \\ OKuVA  & 2018 & 22 & 366 & 4,235 & 15.5M & 14.4 hours & ✗ & 6 & ✗ & 30.6\_fps_ & LT & Eva. \\ TrackingNet  & 2018 & 27 & 30,643 & 471 & 14.43M\({}^{*}\) & 140.0 hours & ✗ & 15 & ✗ & 30.6\_fps_ & ST & Tra/Eva. \\ LaSOT  & 2019 & 70 & 1,400 & 2,053 & 3.52M & 32.5 hours & ✓ & 14 & ✓ & 30.6\_fps_ & LT & Tra/Eva. \\ TNL-K  & 2021 & 169\({}^{*}\) & 2,000 & 62 & 1.2adm & 11.5 hours & ✓ & 17 & ✓ & 30.6\_fps_ & ST & Tra/Eva. \\ GOT-10x & 2021 & 563 & 9,935 & 149 & 1.45M & 40.0 hours & ✓ & 6 & ✗ & 10.6\_fps_ & ST & Tra/Eva. \\ 
**VastTrack** & 2024 & 2,115 & 50,610 & 83 & 4.20M & 194.4 hours & ✓ & 10 & ✓ & 6.6\_fps_ & ST & Tra/Eva. \\   

Table 1: Comparison of VastTrack with other datasets. “ST” and “LT” indicate short- and long-term tracking. “Tra.” and “Eav.” mean training and evaluation. \(b\): TrackingNet  is semi-automatically labeled using a tracker. \(\): The number of object classes in TNL2K  is obtained by our statistic.

**Vision Benchmarks with Vast Categories.** Benchmarks with vast object categories are desired for learning general vision systems. Numerous such benchmarks have been introduced for various vision tasks. For example, the well-known ImageNet  consists of 1,000 classes for image recognition. Open Image  covers 600 categories for object detection. LVIS  comprises 1,203 classes for the tasks of object detection and instance segmentation. TAO  contains 833 categories for general multi-object tracking. The recently proposed V3Det  contributes a new dataset with 13,204 object classes with the goal of facilitating the general detection system development.

In the similar spirit with the above vast category benchmarks, we introduce VastTrack that comprises 2,115 object classes and more than 50K sequences for visual tracking. To the best of our knowledge, VastTrack is so far the largest tracking benchmark regarding the categories and videos and we hope it can serve as a cornerstone dataset for developing more general object tracking systems.

## 3 The Proposed VastTrack

### Construction Principle

The goal of VastTrack is to develop a unique large-scale platform with abundant object categories and video sequences with rich as well as precision annotations for facilitating the development of more general tracking. For this purpose, we follow principles below in constructing our VastTrack:

* _Vast Object Category._ One key motivation of VastTrack is to facilitate more universal object tracking with a rich class diversity. To this end, we hope that the new benchmark covers at least 2,000 object classes, containing common target objects suitable for visual tracking in our life.
* _Larger Scale._ Abundant sequences are crucial for training deep trackers. We expect VastTrack to include at least 50K videos with an average video length of at least 80 frames. Such a scale, greatly larger than current datasets, can potentially benefit training more powerful deep trackers.
* _Rich Annotation._ One of important goals of VastTrack is to develop a comprehensive platform that supports both vision-only and vision-language tracking. Considering this, both bounding boxes and language specifications will be provided to boost tracking in different directions.
* _High Quality._ The quality of annotation is crucial for both training and evaluation. To ensure high quality of VastTrack, we manually label each video with multi-round inspections and refinements.

### Data Acquisition

VastTrack aims to cover abundant categories for tracking. To this end, 2,115 categories are selected for building VastTrack. These object categories are chosen from different sources, including classes in ImageNet  and V3Det , WordNet , and Wikipedia, and organized in a hierarchical tree structure. Note that, each selected category is verified by an expert (_e.g._, a PhD or MS student working on the related topic) to ensure that it is suitable for the tracking task. Compared with existing datasets, the object classes of VastTrack are more diverse and more desired for universal tracking as discussed before. Please refer to **supplementary material** for details of object classes in VastTrack.

After determining all object categories of VastTrack, we then search for the sequences of each class from YouTube under Creative Commons licence. The reason to use YouTube for sourcing videos is because it is currently the largest the video platform and many videos come from the real world. Initially, we gather more than 66K sequences. Then, we carefully inspect each video for the availability for visual tracking task, and finally pick out 50,610 sequences. For each qualified video, we remove the irrelevant content from it, and only retain an usable clip for tracking. Note that, unlike LaSOT  in which each category has the same number of videos, the sequence number of each class is not equal, forming a long-tail distribution (see Fig. 2) that is more universal in real world and could encourage learning more practical and general visual trackers .

Figure 2: The number of videos in each object class forms a long-tail distribution, which is common and universal in our real world.

Eventually, we develop a new benchmark, VastTrack, by covering 2,115 categories. It contains 50,610 videos with 4.2 million frames with an average sequence length of 83 frames. Because of limited space, we display detailed distribution of video length of VastTrack in the **supplementary material**. Please **notice** that, VastTrack is focused on short-term tracking by offering abundant object classes and sequences. Despite this, it can still be used for training long-term temporal trackers, as evidenced by the effectiveness of short-term videos in [27; 43] for learning robust trackers on both long-/short scenarios. In order words, diversity and quantity of objects and videos may be more crucial for deep tracking. It is worth **noting** that, although the frame rate (_i.e._, 6 _fps_ as in Tab. 1) of VastTrack is less than that of traditional benchmarks, this may not impact the training and evaluation of tracking much. Specifically, on the training side, since most of current tracking frameworks adopt the training mechanism of frame sampling with an interval (_e.g._, 100 frames), our VastTrack can be used for training by choosing a suitable interval (probably less than the interval used for traditional datasets). On the evaluation side, according to the analysis in , labeling at different frame rate, even at 1 _fps_, does not adversely affect the robustness of tracking evaluation.

### Annotation

We follow the similar principle as in [16; 15] for the bounding box annotation of a sequence: given the initial target object, in each frame, if the object shows up in the view, a labeler manually draws its (axis-aligned) bounding box as the tightest one to fit any visible part of the target; otherwise an absence label, either _out-of-view_ or _full occlusion_, is given to the frame. Notice that, for some categories such as "_Kite_" and "_Yo-Yo_", the string does not belong to the target object to track, and thus will not be included in the annotated bounding box.

Guided by the above principle, we compile an annotation team with a few experts and a qualified labeling group, and adopt a multi-step mechanism, including manual labeling, visual inspection and refinement. In the first step, after experts label the initial target in the first frame, the annotation group starts to label the target in all other frames in the video. Notice that, to ensure consistency, each video

Figure 4: Statistics of annotations on object motion (image (a)), relative area compared to the initial object (image (b)), IoU of targets in adjacent frames (image (c)), and size of targets (image (d)).

Figure 3: Visualization of several annotation examples in the proposed VastTrack.

is labeled (and refined if necessary) by the same annotator. After this, in the second step, the experts verify the completed annotations from the first step. If the annotation is not unanimously agreed by a validation team (formed by two or three experts), it will be returned back to the original labeler for refinement in the third step. Throughout the annotation process, the second and third steps are repeated for multiple rounds, which ensures high-quality annotations of VastTrack. Fig. 4 displays several annotation examples. In Fig. 4, we show the distributions of target motion, relative area to the initial object, Intersection over Union (IoU) between targets in adjacent frames, and the size of object. From these statistics, we can see that objects moves fast and varies rapidly in the videos of VastTrack.

Considering the benefits of language in improving tracking (_e.g_. [35; 24; 19; 61]), we offer language specifications, besides box annotations, for videos in VastTrack, aiming to facilitate the development of vision-language tracking. In specific, a sentence of natural language that describes color information, behavior, and surroundings of the object as well as optionally its interaction with other objects is given as the linguistic annotation for the video (see Fig. 3 for examples). Although there have been datasets for similar goal (_e.g_. [16; 50] as in Tab. 1), the scale is limited by containing 1.4K  and 2K sentences . Differently, VastTrack offers over 50K videos with richer linguistic specifications for different objects, and thus may benefit learning more powerful vision-language trackers.

### Attributes

To enable further in-depth analysis, we offer ten attributes for _test_ videos in VastTrack, including (1) invisibility (INV), assigned when object is partially or fully invisible due to occlusion or out of view, (2) deformation (DEF), assigned when target is deformable, (3) rotation (ROT), assigned when object rotates, (4) aspect ratio change (ARC), assigned when ratio of bounding box aspect ratio is outside [0.5, 2], (5) illumination variation (IV), assigned when illumination in object region heavily varies, (6) scale variation (SV), assigned when ratio of bounding box is outside [0.5, 2], (7) fast motion (FM), assigned when target center moves larger than its size in last frame, (8) motion blur (MB), assigned when blur in object regions occurs (9) background clutter (BC), assigned when the similar appearance (not necessarily the same class of target) as target appears, and (10) low resolution (LR), assigned when target region is less than 1,000 pixels. For each video, a 10D binary vector is adopted to indicate the presence of an attribute, _i.e_., "1" for presence, "0" otherwise.

The distribution of attributes for the test videos of VastTrack is shown in Fig. 5. We can see that the most common challenge is scale variation, involved with 2,956 videos. In addition, invisibility due to partial or full occlusion or out-of-view and fast motion frequently occur with 2,879 and 2,865 videos.

### Dataset Split and Evaluation Protocol

**Dataset Split.** VastTrack has 50,610 videos, with 47,110 videos for training in VastTrackTra and the rest 3,500 for testing in VastTrackTst. Tab. 2 displays comparison of training and testing sets. In dataset split, we try to keep distributions of training and testing sets similar. Please note, the reason to use 3,500 videos (\(\)7% of total) in VastTrackTM is to keep it relatively compact so that evaluation of trackers can be fast, similar to the popular GOT-10k  in which 420 videos out of around 10K are for testing (\(\)4.2% of the total). Although VastTrackTst has only 3,500 videos, it is representative by including rich categories and various scenarios for evaluation, and much larger and more diverse compared to other testing sets in video number and classes, making evaluation more reliable.

**Evaluation Protocol.** Unlike the _full overlap_[16; 43] or _one-shot_, we utilize a _hybrid_ protocol wherein part of object classes (not videos) in test set have overlap with training set, while the rest classes remains unseen. The reason is that, in real world, humans often track objects from both

    & **Classes** & **Videos** &  **Mean** \\ **frames** \\  & 
 **Total** \\ **frames** \\  \\   VastTrackTst & 702 & 3,500 & 106.3 & 372K \\ VastTrackTra & 1,974 & 47,110 & 81.2 & 3.82M \\   

Table 2: Comparison of _training_ and _testing_ sets.

Figure 5: Distribution of videos per attribute.

frequently seen and unseen categories. To develop human-like trackers, we adopt such a hybrid protocol for VastTrack with 561 overlap classes and 141 completely unseen classes in test set.

## 4 Experiments

**Evaluation Metric.** Following [53; 16; 43], we use _one-pass evaluation_ (OPE) and compare different trackers using three metrics, including _precision_ (PRE), normalized precision (NPRE), and success (SUC). In specific, PRE measures center position distance between tracking results and groundtruth in pixels, and trackers are ranked by PRE on a preset threshold, _e.g._, 20 pixels. To mitigate influence of video resolutions, NPRE is calculated by normalizing PRE using target region. Different from PRE and NPRE, SUC measures Intersection over Union (IoU) between tracking results and groundtruth, and is computed by the percentage of frames in which the IoU is larger than a threshold, _e.g._, 0.5.

### Evaluated Trackers

To understand existing approaches on VastTrack and also to offer baselines for comparison, we evaluate 25 representative trackers, which are classified into three types: **(i) CNN-based** that achieves object tracking using only CNN architecture, consisting of SiamFC , ATOM , SiamRPN++ , SiamBAN , DiMP , SiamCAR , PrDiMP , STMTrack , Ocean , RTS , and

Figure 6: Evaluation results of 25 trackers on VastTrack\({}_{}\) using PRE, NPRE, and SUC.

Figure 7: Qualitative results of eight representative trackers on different sequences. We observe that these trackers drift to the background region or even lose the target due to different challenges in videos such as BC, SV, DEF, INV, MB, ROT, and LR. More efforts are desired to improve tracking.

AutoMatch ; **(ii) CNN-Transformer-based** that implements visual tracking via hybrid CNN and Transformer architectures, including STARK , TrSiam , TransT , and ToMP ; **(iii) Transformer-based** that tracks the target through leveraging a pure Transformer architecture. The tracking approaches in this category consist of OSTrack , SwinTrack , MixFormer  and MixFormerV2 , SimTrack , SeqTrack , ARTrack , DropMAE , and ROMTrack  as well as GRM . We conduct the evaluations on a workstation with an Intel Xeon w9 CPU and 4 Nvidia A6000 GPUs. Please note, all trackers are assessed as they are, without modifications. A detailed summary of these trackers are shown in the **supplementary material** due to limited space.

We understand that tracking is a rapidly evolving field with numerous trackers proposed every year. To keep the evaluations on VastTrack as up-to-date as possible, we try our best to maintain lead board on our project page by continuously including newly published trackers (_e.g._, ) from major avenues. Please refer to our project webpage for more details.

### Evaluation Results

**Overall Performance.** We evaluate 25 trackers on VastTrack, including many recent Transformer-based methods. Note that, for evaluation, each tracker is evaluated as it is, without any modification. The evaluation results are reported in Fig. 6. We can see that, SeqTrack achieves the best performance on all three metrics with 0.402 PRE, 0.429 NPRE, and 0.396 SUC scores, MixFormer displays the second best results with 0.398 PRE, 0.424, and 0.395 SUC scores, and DropMAE obtains the third best results with 0.365 PRE, 0.397 NPRE, and 0.375 SUC scores. All these three trackers are developed based on vision Transformer architecture, showing its power in feature learning for tracking. Notably, although RTS does not employ Transformer architecture for tracking, it still achieves promising results with 0.331 PRE, 0.364 NPRE, and 0.355 SUC scores, even better than a few Transformer trackers like OSTrack with 0.315 PRE, 0.345 NPRE, and 0.336 SUC scores and SwinTrack with 0.303 PRE, 0.342 NPRE, and 0.330 SUC scores. We argue this is because RTS adopts tracking-by-segmentation which is beneficial for tracking object with extreme aspect ratio. Note that, the recent MixFormerV2 with 0.330 PRE, 0.365 NPRE, and 0.352 SUC scores performs worse than its previous version MixFormer, because it leverages much lighter network for efficiency. An interesting observation is that, SiamRPM++, a seminal Siamese tracker, surprisingly outperforms many its extensions such as SiamCAR, Ocean, and SiamBAN, showing its generality to some extent.

**Qualitative Evaluation.** In addition to the quantitative evaluation in the main text and this appendix, we further show qualitative results on VastTrack. Specifically, we demonstrate visualizations of eight representative trackers, including SeqTrack, MixFormer, RTS, OSTrack, SwinTrack, TransT, SimaRPN++, and SiamFC in different attributes such as _scale variation_, _deformation_, _rotation_, _aspect ratio change_, _background clutter_, _invisibility_, _blur_, _fast motion_, and _low resolution_ in Fig. 7. As displayed Fig. 7, we can observe that, although the trackers can deal with some challenges in the video sequences, they may still fail in more complicated scenarios where multiple challenges occur simultaneously, which indicates that more efforts are desired to improve existing approaches towards universal visual tracking.

**Discussion.** The evaluation shows some useful observations: (1) _Feature network._ As shown in Fig. 6, we observe that, the top five trackers are based on Vision Transformer architecture, which reveals that the exploration of more powerful feature network is still an important direction for improving tracking. This is consistent with findings in other benchmarks. Despite adopting powerful feature network, the performance is still far from satisfaction, compared to that on other benchmarks (as shown later). We argue this is caused by the lack of universal large-scale training of more general object categories for tracking. (2) _Temporal information_. Videos contain abundant temporal information which is important for tracking. However, this is largely ignored to some extend owing to the great success of Siamese tracking in recent years. Especially, even without using temporal information, many trackers still achieve state-of-the-art performance. However, from Fig. 6, we see that, the top three trackers all leverage temporal information for tracking, which indicates the crucial role of temporal cue for tracking. We hope, through evaluation results on VastTrack, researchers can pay more attention in developing robust tracking by incorporating temporal cues.

**Attribute-based Performance.** To better analyze different trackers, we further perform attribute-based evaluation on ten challenges. Fig. 8 shows the attribute-based results for the two most common attributes of SV and INV (based on number of videos in each attribute) and two difficult attributes of LR and IV using SUC. As in Fig. 8 (a), SeqTrack, MixFormer, and DropMAE achieves the best three results on SV/INV with 0.369/0.368, 0.367/0.365, and 0.347/0.347 scores in SUC, which is consistent with their performance in overall evaluation. As in Fig. 8 (b), SeqTrack, MixFormer, and DropMAT are the best three trackers on IV. An interesting finding is, IV is considered to be easy for tracking . Nevertheless, our result shows difference. We argue, this is because IV in VasTrack usually occurs in low-light condition with complicated background, which degrades tracking performance. This also shows, extreme illumination change still need to be carefully dealt with. LR is the most difficult challenge in VasTrack, because it may result in low-quality feature extraction. On LR, MixFormer, SeqTrack, and ROMTrack achieve the best three results with 0.236, 0.232, and 0.231 SUC scores.

**Comparison on Meta Category.** Fig. 9 compares different meta categories (or coarse classes in the **supplementary material**) using SUCmeta, calculated using SUC scores of all trackers on a certain meta class. From Fig. 9, we can see that, some common categories such as boat, aircraft, vehicle and human parts are relatively easy for tracking, while rare classes like weapon, fruit, and various balls are hard to locate probably due to the lack of enough training videos from these categories, which indicates the need of more object categories for learning general tracking systems.

Due to space limitation, more evaluation and analysis can be seen in the **supplementary material**.

### Comparison to Other Benchmarks

Compared to existing benchmarks, the proposed VastTrack is more challenging due to the requirement of tracking object from more classes (in test). We present a comparison of VastTrack and other popular large-scale tracking benchmarks including TrackingNet , LaSOT , and TNL2K . Please note that, GOT-10k  here is not compared because it adopts different metrics for evaluation. Tab. 3 reports the results of the top 15 trackers on our VastTrack and their results on TrackingNet, LaSOT, and TNL2K using SUC. From Tab. 3, we observe that, all the compared trackers have a heavy performance drop on VastTrack. For

    &  \\   & TrackingNet & LaSOT & TNL2K & VastTrack \\  &  &  &  & (Ours) \\   SeqTrack  & 0.855 & 0.725 & 0.578 & 0.396 \\ MixFormer  & 0.854 & 0.724 & 0.533 & 0.395 \\ DropMAE  & 0.841 & 0.718 & 0.569 & 0.375 \\ ROMTrack  & 0.841 & 0.714 & 0.604 & 0.370 \\ GRM  & 0.840 & 0.699 & 0.611 & 0.363 \\ ARTrack  & 0.843 & 0.708 & 0.575 & 0.356 \\ RTS  & 0.816 & 0.697 & 0.599 & 0.355 \\ MixFormerV2  & 0.834 & 0.706 & 0.506 & 0.352 \\ ToMP  & 0.815 & 0.685 & 0.584 & 0.349 \\ SimTrack  & 0.834 & 0.705 & 0.556 & 0.344 \\ OSTrack  & 0.839 & 0.711 & 0.559 & 0.336 \\ STARK  & 0.820 & 0.671 & 0.525 & 0.334 \\ SwinTrack  & 0.811 & 0.672 & 0.559 & 0.330 \\ TfSiam  & 0.781 & 0.624 & 0.523 & 0.326 \\ PrDiMP  & 0.758 & 0.598 & 0.470 & 0.310 \\   

Table 3: Comparison to other datasets.

Figure 8: Evaluation on the two most common attributes (a) and difficult attributes (b) using SUC.

Figure 9: Comparison on different meta categories using SUC score.

instance, SeqTrack, the best tracker on VastTrack, achieves high SUC scores of 0.855, 0.725, and 0.578 on TrackingNet, LaSOT, and TNL2K, while degrades to 0.396 on VastTrack with 0.459, 0.329, and 0.182 drops. OSTrack drops from 0.839, 0.711, and 0.559 SUC scores on TrackingNet, LaSOT, and TNL2K to 0.336 on VastTrack. SwinTrack degrades from 0.811, 0.672, and 0.559 on the existing benchmarks to 0.330 on VastTrack. Likewise, other trackers suffer similar drops, which shows the challenge for current trackers and there is still a long way for improving tracking.

In addition, we see an interesting observation about the relative performance of trackers from Tab. 3. Specifically, we see that a few trackers such as OSTrack and SimTrack performing better on LaSOT may perform relatively worse than others such as GRM, RTS, and ToMP on VastTrack. We argue that the possible reason is the abilities of different trackers in dealing with overfitting for object categories, which shows the need of more diverse videos with different classes in learning more general tracking.

### Retraining Experiments with VastTrack

In order to demonstrate the effectiveness of VastTrack in improving existing methods, we retrain two trackers, consisting of SiamRPN++  and OSTrack , using the training set of VastTrack through fine-tuning. Tab. 4 displays the results. As in Tab. 4, we clearly see that, after further training, the SUC score of SiamRPN++ is clearly improved from 0.281 to 0.298 with performance gains of 1.7% on VastTrack, from 0.496 to 0.528 with 3.2% gains on LaSOT, from 0.733 to 0.762 with 2.9% gains on TrackingNet, and from 0.413 to 0.446 with 3.3% gains on TNL2K. Besides, for OSTrack, the SUC score is boosted from 0.336 to 0.362 with 2.6% improvements on VastTrack, from 0.711 to 0.722 with 1.1% gains on LaSOT, from 0.839 to 0.852 with 1.3% gains on TrackingNet, and form 0.559 to 0.579 with 2.0% gains on TNL2K. Note that, OSTrack is already strong on LaSOT but still enhanced using VastTrack. All these experiments validate the effectiveness of VastTrack in improving tracking. In the future, we will further explore the potential of VastTrack for advancing tracking performance.

## 5 Conclusion

In this paper, we propose a novel large-scale benchmark, dubbed VastTrack, to facilitate the development of more general object tracking. To this goal, VastTrack contains abundant object categories and video sequences. Specifically, it covers 2,115 classes and 50,610 videos with 4.2 million frames. To the best of our knowledge, VastTrack is to date the largest benchmark regarding class diversity and video number. Besides, VastTrack offers both bounding box annotations and language descriptions, which enables exploring both vision-only and vision-language tracking. In order to ensure the high quality, VastTrack is manually annotated with multi-round of careful inspection and refinement. We evaluate 25 trackers to analyze existing methods on VastTrack and to offer baselines for comparison. The evaluation results reveal that more efforts are needed for general tracking. By releasing VastTrack, we expect to provide a cornerstone dataset for developing more general object tracking.

**Limitation.** Despite the vast object categories and larger scale of our VastTrack, there are limitations. First, given the proposed large-scale VastTrack with vast object classes, a baseline that outperforms other trackers is not provided. Second, since most video sequences in VastTrack are relative short, it may not be suitable for long-term tracking performance evaluation, though it can be used for training long-term temporal trackers. Considering that our primary goal in this work is to offer a new benchmark with vast object categories, we leave these questions to further work by designing new trackers and by developing a new subset for long-term tracking evaluation.

**Social Impact.** By releasing VastTrack, researchers from the tracking community are able to leverage it as a platform for training more general tracking systems as well as for assessing and comparing different models, which could facilitate the deployment of tracking in more real-world applications.

**Acknowledgment.** Libo Zhang was supported by National Natural Science Foundation of China (No. 62476266). Heng Fan was not supported by any fund for this work.

    &  &  \\   & SUC w/o & SUC w/ & SUC w/o & SUC w/ \\  & retraining & retraining & retraining & retraining \\   VastTrack & 0.281 & 0.298 (0.17\%) & 0.336 & 0.362 (12.6\%) \\ LaSOT & 0.496 & 0.528 (0.32\%) & 0.711 & 0.722 (11.1\%) \\ TrackingNet & 0.733 & 0.762 (12.9\%) & 0.839 & 0.852 (11.3\%) \\ TNL2K & 0.413 & 0.446 (\(\)3.3\%) & 0.559 & 0.579 (12.0\%) \\   

Table 4: Further training with VastTrack.