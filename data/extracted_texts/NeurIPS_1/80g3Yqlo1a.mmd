# Fast Scalable and Accurate Discovery of DAGs Using the Best Order Score Search and Grow-Shrink Trees

Fast Scalable and Accurate Discovery of DAGs Using the Best Order Score Search and Grow-Shrink Trees

 Bryan Andrews

Department of Psychiatry & Behavioral Sciences

University of Minnesota

Minneapolis, MN 55454

andr1017@umn.edu

&Joseph Ramsey

Department of Philosophy

Carnegie Mellon University

Pittsburgh, PA 15213

jdramsey@andrew.cmu.edu

Ruben Sanchez-Romero

Center for Molecular and Behavioral Neuroscience

Rutgers University

Newark, NJ 07102

ruben.saro@rutgers.edu

&Jazmin Camchong

Department of Psychiatry & Behavioral Sciences

University of Minnesota

Minneapolis, MN 55454

camch002@umn.edu

&Erich Kummerfeld

Institute for Health Informatics

University of Minnesota

Minneapolis, MN 55454

erichk@umn.edu

###### Abstract

Learning graphical conditional independence structures is an important machine learning problem and a cornerstone of causal discovery. However, the accuracy and execution time of learning algorithms generally struggle to scale to problems with hundreds of highly connected variables--for instance, recovering brain networks from fMRI data. We introduce the best order score search (BOSS) and grow-shrink trees (GSTs) for learning directed acyclic graphs (DAGs) in this paradigm. BOSS greedily searches over permutations of variables, using GSTs to construct and score DAGs from permutations. GSTs efficiently cache scores to eliminate redundant calculations. BOSS achieves state-of-the-art performance in accuracy and execution time, comparing favorably to a variety of combinatorial and gradient-based learning algorithms under a broad range of conditions. To demonstrate its practicality, we apply BOSS to two sets of resting-state fMRI data: simulated data with pseudo-empirical noise distributions derived from randomized empirical fMRI cortical signals and clinical data from 3T fMRI scans processed into cortical parcels. BOSS is available for use within the TETRAD project which includes Python and R wrappers.

## 1 Introduction

We present a permutation-based algorithm, Best Order Score Search (BOSS), for learning a Markov equivalence class of directed acyclic graphs (DAGs). A novel method for caching intermediate calculations for permutation-based algorithms, Grow-Shrink Trees (GSTs), is also developed and presented in this paper. Our implementation of BOSS using GSTs scales well in both accuracy and time to higher numbers of variables and graph densities. We demonstrate that in particular, this method scales at least to the complexity of dense cortical parcellations of fMRI data.

In real world systems, it is not unusual to have hundreds or thousands of variables, each of which may be causally connected to dozens of other variables. Models of functional brain imaging (fMRI), functional genomics, electronic health records, and financial systems are just a few examples. In order for structure learning methods to have an impact on these real world problems, they need to be not only highly accurate but also highly scalable in both dimensions.

This work follows earlier research on permutation-based structure learning [12; 22; 24; 26]. A previous permutation-based method, Greedy Relaxations of the Sparsest Permutation (GRaSP) , demonstrated strong performance on highly connected graphs, but struggled to scale to sufficiently large numbers of variables. By using BOSS and GSTs, we are able to overcome this challenge, while maintaining nearly identical performance to GRaSP. As our simulations show, other popular methods fall short on accuracy, scalability, or both, for such large, highly connected models.

In the remainder of this paper, we provide background for our approach in Section 2, followed by a discussion of GSTs in Section 3. We then introduce BOSS in Section 4 and validate it by comparing it to several alternatives. We show that BOSS using GSTs compares favorably to a number of best-performing combinatorial and gradient-based learning algorithms under a broad range of conditions, up to 1000 variables with an average degree of 20 in Section 5. Finally, to demonstrate its practicality, we apply BOSS to two sets of resting-state fMRI data: simulated data with pseudo-empirical noise distributions derived from randomized empirical fMRI cortical signals, and clinical data from 3T fMRI scans processed into 379 cortical parcels in Section 6. Section 7 provides a brief discussion.

### Our Contributions

Our novel contributions to the field include the following:

1. We present a new data structure, Grow-Shrink Trees (GSTs), for efficiently caching results of permutation-based structure learning algorithms. Using GSTs dramatically speeds up many existing permutation-based methods such as GRaSP.
2. We present a new structure learning algorithm, the Best Order Score Search (BOSS), which has similar performance as GRaSP (and thus superior performance to other DAG-learning methods), but is more convenient to use than GRaSP because it has fewer tuning parameters and is faster and more scalable.
3. We prove that BOSS is asymptotically correct and provide an implementation within the TETRAD project.
4. We present validations of BOSS's finite sample performance via standard graphical model simulations and on both real and simulated fMRI data.

## 2 Background

The following conventions are used throughout this paper: \(V\) denotes a non-empty finite set of variables which double as vertices in the graphical context, lowercase letters \(a,b,c, V\) denote variables or singletons, uppercase letters \(A,B,C, V\) denote sets, and \(X\) denotes a collection of random variables indexed by \(V\) whose joint probability distribution is denoted by \(P\).

Probabilistic conditional independence between the members of \(X\) corresponding to disjoint sets \(A,B,C V\) is denoted \(A\!\!\! B C P\) and reads \(X_{A}\) and \(X_{B}\) are independent given \(X_{C}\).

### Permutations

A _permutation_ is a sequence of variables \(= a,b,c,\). Let \(v V\) and \(i\) (\(i|V|\)). If \(\) is a permutation of \(V\), then \(\) is equipped with two methods: \(.(v)\) which returns the index of \(v\) in \(\), and \(.(v,i)\) which returns the permutation obtained by removing \(v\) from \(\) and reinserted at position \(i\). Furthermore, \(_{}(v)\{w V\ :\ .(w)<.(v)\}\) is the _prefix_ of \(v\).

### DAG Models

A _directed acyclic graph_ (DAG) model is a set probabilistic models whose conditional independence relations are described graphically by a DAG. In general, the vertices and edges of a DAG represent variables and conditional dependence relations, respectively. We use the notation \(=(V,E)\) where \(\) is a graph, \(V\) is a vertex set, and \(E\) is an edge set. In a DAG, the edge set is comprised of ordered pairs that represent directed edges1 and contains no directed cycles.

Let \(=(V,E)\) be a DAG and \(v V\):

\[_{}(v) \{w V\;:\;w v\;\;\;\;\}\] \[_{}(v) \{w V\;:\;v w\;\;\;\;\}\]

are the _parents_ and _children_ of \(v\), respectively.

Graphical conditional independence between disjoint subsets \(A,B,C V\) can be read off of a DAG using _\(d\)-separation_ and is denoted \(A\!\!\! B C\!\!\!\!\!\!\!\!\!\!\!\!\!\). This criterion admits the concept of a _Markov equivalence class_ (MEC) which (in the context of this paper) is a collection of DAGs that represent the same conditional independence relations.

### Causal Discovery

DAGs are connected to causality by the _causal Markov_ and _causal faithfulness_ assumptions. A DAG is _causal_ if it describes the true underlying causal process by placing a directed edge between a pair of variables if and only if the former directly causes the latter.

_Causal Markov_: If \(=(V,E)\) is causal for a collection of random variables \(X\) indexed by \(V\) with probability distribution \(P\), then for disjoint subsets \(A,B,C V\):

\[A\!\!\! B C\!\!\!\!\!\!\!\!\! A \!\!\! B C\!\!\!\!\! P\,.\]

More generally, this implication is called the _Markov property_ and we say DAGs satisfying this property _contains_ the distribution. Moreover, a DAG \(\) is _subgraph minimal2_ if no subgraph of \(\) contains \(P\).

_Causal faithfulness_: If \(=(V,E)\) is causal for a collection of random variables \(X\) indexed by \(V\) with probability distribution \(P\), then for disjoint subsets \(A,B,C V\):

\[A\!\!\! B C\!\!\! P\, A\!\!\! B  C\!\!\!\!\!\!\!\!\!\!\! C\!\!\!\!\!\!.\]

Accordingly, under the causal Markov and causal faithfulness assumptions, finding the MEC of the causal DAG is equivalent to identifying the simplest model that contains the data generating distribution. From this point of view, causal discovery is a standard model selection problem which we address using the Bayesian information criterion (BIC) .

Let \(\) be a DAG and \(}}{{}}P\) be a dataset drawn from a member of a curved exponential family:

\[(,) =_{v V}(_{v},_{ _{}(v)})\] \[=_{v V}_{v|_{}(v)}(_ {})-\,|_{} \!\!\!\!(n)\]

where \(>0\) is a penalty discount, \(\) is the log-likelihood function, and \(_{}\) is the maximum likelihood estimate of the parameters; for details on curved exponential families, we refer reader to . Importantly, the BIC is _consistent_.

**Proposition 1**.: _Haughton  If \(P\) is a member of a curved exponential family and \(}}{{}}P\), then the BIC is maximized in the large sample limit by DAG models containing \(P\) that minimize the dimension of the parameters space._

Notably, Gaussian and multinomial DAG models form curved exponential families in which the causal DAG minimizes the dimension of the parameters space, and thereby maximize the BIC . For these distributions, DAG models belonging to the same MEC correspond to the same DAG model,so it is common for algorithms to return a graphical representation of the MEC called a CPDAG rather than a DAG. Choosing between DAG models in a MEC requires additional information.

We greedily search over permutations of variables using grow-shrink (GS)  to project and score DAGs from permutations. Algorithm 1 (grow) and Algorithm 2 (shrink) give the details of GS while Algorithm 3 (project) details the process of projecting a permutation to a DAG.

```
Input:data:\(\)var:\(v\)prefix:\(Z\)Output:parents:\(W\)\(W\)repeat\(w_{z Z}(_{v}, _{W z})\)if\(w\)then\(W W w\) until\(w=\)
```

**Algorithm 1**\((,v,Z)\)

```
Input:data:\(\)var:\(v\)parents:\(W\)Output:parents:\(W\)repeat\(w_{w W}(_{v}, _{W w})\)if\(w\)then\(W W w\) until\(w=\)
```

**Algorithm 2**\((,v,W)\)

However, how do we know that the causal DAG will even be constructed by project? This is guaranteed by the follow results. Let \(P\) is a member of a curved exponential family satisfying causal Markov and causal faithfulness:

* If \(\) is the causal DAG, then \(\) is subgraph minimal .
* If \(\) is a DAG and \(}}{{}}P\) then \(\) is subgraph minimal if and only if there exists a permutation \(\) such that \(=(,)\) in the large sample limit .

These results admit the following strategy: (1) Search over permutations while using GS and the BIC to construct and score subgraph minimal DAG from these permutations. (2) Return the MEC of the subgraph minimal DAG that maximizes the BIC. The question is how to do so in a way that is both comprehensive and efficient.

## 3 Grow-Shrink Trees

Grow-shrink trees (GSTs) are tree data structures for caching the results of the grow and shrink subroutines of GS . This data structure is compatible with many permutation-based structure learning algorithms, including BOSS and GRaSP. As an initialization step, a GST is constructed for each variable in the dataset which are then queried during search rather than running GS. Each tree has a root node representing the empty parent set. Parents are accumulated by traversing edges of the tree, with every node in the tree corresponding to a "grown" parent set. Each grown parent set corresponds to running the grow subroutine for some prefix. The shrink subroutine is also run and cached at each node of the tree.

The benefit of using GSTs is in how efficiently they store information needed for running GS. In the grow subroutine, nodes are added to the GST one at a time and scored. A new child node is added to the tree for each possible addition, and the child nodes are sorted in descending order relative to their scores. After all candidate parents have been added to the tree, the first child in the sorted list that is also in the prefix is chosen and the corresponding edge traversed. The shrink subroutine can then be run and the removed parents and scores can be cached.

### An Example

This section walks through the usage of a GST applied to a toy example. Minimal subgraphs are depicted in Figure 1 of which (1a) is the true DAG. Figure 2 outlines the process of growing a GST for \(a\) while scoring permutations. In the following, we identify nodes in the GST by their path from the root. All ambiguities in sorting the branches of a node are resolved in alphabetical order. We proceed in sequence, but in theory some of these steps could be performed in parallel.

In what follows we run GS on a series of permutations while caching the results in a GST. We describe the nodes of the tree by the sequence of variables traversed to reach them from the root. We use colors to track our progress: light gray parts of the tree denote unexplored paths with no cached scores, dark gray parts of the tree denote explored paths with cached scores. Colored edges denote the paths considered during the execution of GS: red for rejected and blue for accepted. Colored nodes denote the same, but are only colored if they do not have a cached value and require a new score calculation. Numbered edges directed out of a parent node record the order of preference for the children from high to low. To simplify this example, we do not cache the results of shrink. In (2a) we initialize the tree by adding the root node \(a\).

Our first permutation to evaluate is \( b,d,a,c\) which is shown in (2b) and whose minimal subgraph is depicted in (1b). Node \(a\) has not been expanded, so we score and sort nodes \(ab\), \(ac\), and \(ad\). Node \(ab\) scored the highest and is contained in the prefix, so we travel to node \(ab\). Node \(ab\) has not been expanded, so we score and sort nodes \(abc\) and \(abd\). Node \(abd\) scored the highest and is contained in the prefix, so we travel to node \(abd\). At this point no more variables are available in the prefix so we have completed growing. We run shrink at node \(abd\) which removes nothing so the GS result is \(\{b,d\}\).

Our next permutation to evaluate is \( c,d,a,b\) which is shown in (2c) and whose minimal subgraph is depicted in (1c). Node \(a\) has already been expanded so we check its outgoing edges. Node

Figure 1: Minimal subgraphs.

Figure 2: A Grow-shrink Treehas the highest score but is not contained in the prefix. Node \(ad\) has the second highest score and is contained in the prefix, so we travel to node \(ad\). Node \(ad\) has not been expanded, so we score and sort nodes \(adc\). Node \(c\) has the highest score and is contained in the prefix, so we travel to node \(adc\). At this point no more variables are available in the prefix so we have completed growing. We run shrink at node \(adc\) which removes nothing so the GS result is \(\{c,d\}\).

Our next permutation to evaluate is \( c,a,b,d\) which is shown in (2d) whose minimal subgraph is depicted in (1d). Node \(a\) has already been expanded so we check its outgoing edges. Node \(ab\) has the highest score but is not contained in the prefix. Node \(ad\) has the second highest score but is not contained in the prefix. Node \(abc\) has the second highest score and is contained in the prefix, so we travel to node \(abc\). At this point no more variables are available in the prefix so we have completed growing. We run shrink at node \(ac\) which removes nothing so the GS result is \(\{c\}\).

Our next permutation to evaluate is \( b,c,a,d\) which is shown in (2e) whose minimal subgraph is depicted in (1e). Node \(a\) has already been expanded so we check its outgoing edges. Node \(ab\) has the highest score and is contained in the prefix, so we travel to node \(ab\). \(ab\) has already been expanded so we check its outgoing edges. Node \(abd\) has the highest score but is not contained in the prefix. Node \(abc\) has the second highest score and is contained in the prefix, so we travel to node \(abc\). At this point no more variables are available in the prefix so we have completed growing. We run shrink at node \(abc\) which removes nothing so the GS result is \(\{b,c\}\).

Our last permutation to evaluate is \( c,b,d,a\) which is shown in (2f) and whose minimal subgraph is depicted in (1a). Node \(a\) has already been expanded so we check its outgoing edges. Node \(ab\) has the highest score and is contained in the prefix, so we travel to node \(ab\). Node \(ab\) has already been expanded so we check its outgoing edges. Node \(d\) has the highest score and is contained in the prefix, so we travel to node \(abd\). Node \(abd\) has not been expanded, so we score node \(abdc\). No scores result in an improvement to the overall score so we have completed growing. We run shrink at node \(abd\) which removes nothing so the GS result is \(\{b,d\}\).

## 4 Best Order Score Search

Similar to GES, BOSS uses a two phase search procedure . The first phase uses the best-move method, which takes a variable as input and greedily moves it to the position in the current permutation that maximizes the score. In this phase, best-move is repeatedly applied to each variable, one at a time, until there are no more moves that increase the score. This phase concludes with the find-compelled procedure of  which converts the DAG into a CPDDAG. The second phase of BOSS is BES which is exactly second phase of GES. The BES step is optional but guarantees asymptotic correctness if executed.

```
Input:\(:\)\(:\)\(:\) Output:\(:\) \(()\) repeat \(.()$}\) foreach\(v\)do \((,,v)\) until\(=.()\) \(.()\) \(()\) if\(=\)then \((,)\)
```

**Algorithm 4**\((,,)\)

**Proposition 2**.: _Let \(P\) be a member of a curved exponential family satisfying causal Markov and causal faithfulness. If \(}}{{}}P\) then \((,,)\) returns the MEC of the causal DAG for all initial permutation \(\) in the large sample limit._

Proof.: Since project returns a subgraph minimal DAG, it contains \(P\). Accordingly, asymptotic correctness follows from the correctness of BES.

In Algorithm 4 (BOSS) and Algorithm 5, GST constructs a collection of GSTs, denoted \(\), which contains one GST for each variable. The collection \(\) is equipped with project and score methods which runs the GS algorithm to project and score a permutation, respectively.

## 5 Simulations

We evaluated the speed and performance of BOSS on simulated data compared to other algorithms: GRaSP, fGES, PC, DAGMA, and LiNGAM. Our evaluation used the performance metrics tabulated in Table 1 which were originally proposed by . All algorithms were run on an Apple M1 Pro processor with 16G of RAM. The results reported in the main text are abridged but the complete results are available in the Supplement.

We evaluated our implementation of BOSS using a BIC score with \(=2\) and no BES step as it did not appear to improve performance. For GRaSP, we modified (to use GSTs) and used the TETRAD implementation with the same parameters as the authors and a BIC score with \(=2\). . For fGES we used the TETRAD implementation with default parameters and a BIC score with \(=2\). We also used the implementation of PC in TETRAD with default parameters using a BIC score with \(=2\) as a conditional independence oracle . For DAGMA we used the authors' Python implementation with the parameters reported in their paper but changed the threshold to 0.1  and used the technique described in  to resolve cycles. Moreover, the output of DAGMA was converted to a CPDAG for linear Gaussian simulations. For LiNGAM we used the authors' Python implementation with default parameters .

We generated Erdos-Renyi DAGs by applying an arbitrary order to vertices of an Erdos-Renyi graph. For scale-free networks, first we generated an Erdos-Renyi DAG and then redrew the parents of each vertex according to the Barabasi-Albert model . This resamples the out-degrees distribution to be scale-free while keeping in-degree distribution constant. This procedure was motivated by the results in Figure 5. Edge weights were sampled from a uniform distribution in the interval [-1.0, 1.0] and error distributions were generated with standard deviations sampled from a uniform distribution in the interval [1.0, 2.0]. The complete simulation details, including figures plotting the simulated scale-free in/out degree distributions, are include in the Supplement. Additionally, the data are available for download at: [https://github.com/cmu-phil/boss](https://github.com/cmu-phil/boss).

Figure 3 compares BOSS against GRaSP, fGES, PC, and DAGMA on linear Gaussian data generated from (3a) Erdos-Renyi and (3b) scale-free networks. The output of DAGMA was converted to a CPDAG for these simulations. Lam et al.  attribute the excellent performance of GRaSP to it being robust against the ubiquity of almost-violations of causal faithfulness . Due to the algorithmic and performance parallels between GRaSP and BOSS, we conjecture that a similar argument could be made for BOSS.

Figure 4a compares BOSS against DAGMA and LiNGAM on linear exponential and linear Gumbel data from scale-free networks. Interestingly, LiNGAM, which takes advantage of non-Gaussian signal in data, does not perform appreciably better than BOSS.

Figure 4b compares BOSS against GRaSP on linear Gaussian data generated from scale-free networks with a focus on scalability; runs exceeding two hours were cancelled and are not reported. Here we see that BOSS maintains a high level of accuracy while scaling much better than GRaSP. It is also

   True & Estimated & Adjacency & Orientation \\   & \(a b\) & tp & tp,tn \\  & \(a b\) & tp & fp,fn \\  & \(a b\) & tp & fn \\  & \(a b\) & fn & fn \\   & \(a b\) & fp & fp \\  & \(a b\) & fp & fp \\   & \(a b\) & fn & \\   

Table 1: Metricsimportant to note that nearly all of these simulations were infeasible for GRaSP before implementing it with GSTs. Table 2, the corresponding table, only reports results for BOSS since the two algorithms have nearly identical performance on all statistics except for running time. Full results are tabulated in the Supplement.

## 6 Validation on fMRI

The high spatial coverage of fMRI's has allowed researchers to study brain function at different scales, from voxels to cortical parcellations to functional systems (such as default mode or visual systems). Given the potential differences in spatial dimension, the expected connectivity density of functional brain networks remains unclear. For example, even after keeping only the 75th percentile stronger connections, structural connectivity networks from 90 cortical regions  had 1012 connections on average. Since structural connectivity supports functional connectivity, we could expect empirical

Figure 4: Mean statistics over 20 repetitions: scale-free, 100 variables, average degree 20 (when not varied), and sample size 1,000.

Figure 3: Mean statistics over 20 repetitions: 100 variables and sample size 1,000.

functional connectivity networks to be in that order of magnitude. Previous studies applying causal discovery methods to fMRI simulated networks with a high number of variables and connections have shown that while the adjacency recovery precision of these methods can be very high, they usually show a low adjacency recall [16; 20]. Despite possibly having low recall, the limited applications of causal discovery methods to real world data [3; 18] often recover models with average degree greater than 20. Considering this limitation and the likelihood that real fMRI brain networks have high average connectivity (degree), causal discovery methods capable of reducing the number of false-negative connections will substantially improve future analysis of fMRI data.

To demonstrate its practical utility in this important real-world domain, we apply BOSS to two types of resting-state fMRI data: simulated data with pseudo-empirical noise distributions derived from randomized empirical fMRI cortical signals and clinical data from 3T fMRI scans processed into cortical parcels.

### Simulated fMRI

We simulated fMRI data following the approach in . Networks were based on a directed random graphical model that prefers common causes and causal chains over colliders. 40 networks were simulated with 200 variables and an average degree of 10. Edge weights were sampled from a uniform distribution in the interval \([0.1,0.4]\), randomly setting \(10\%\) of the coefficients to their negative value. Pseudo-empirical noise terms were produced by randomizing fMRI resting-state data across data points, regions, and participants from the Human Connectome Project (HCP). Using pseudo-empirical terms better captures the marginal distributional properties of the empirical fMRI. One thousand data points were generated from this procedure. These data are available for download at: [https://github.com/cmu-phil/boss](https://github.com/cmu-phil/boss).

Accuracy and timing results are shown in Table 3 for BOSS, fGES, DAGMA, and LiNGAM. These results show that BOSS has by far the best BIC score of the group, compared to the BIC score of the true model, and that the running time of BOSS is very reasonable for a problem of this size. Precisions and recalls for BOSS are quite high, for both adjacencies and orientations. The poor performance of LiNGAM is due to insufficient non-Gaussian signal. More details are included in the Supplement.

### Clinical fMRI

We applied BOSS to 171 3-Tesla resting state fMRI scans from patients beginning treatment for alcohol use disorder . Before applying BOSS, the data were cleaned and parcellated into 379 variables representing biologically interpretable and spatially contiguous regions . More details on the data collection, cleaning, and processing can be found in . Figure 5 reports the in/out-degree distributions of the resulting models. The solid line depicts the median degree across all graphs and

   Variables & Algorithm & BIC\(\) & Adj Pre & Adj Rec & Ori Pre & Ori Rec \\   &  & 2 & 0.98 & 0.80 & 0.97 & 0.80 \\  & & 4 & 1.00 & 0.72 & 0.99 & 0.72 \\  & & 8 & 1.00 & 0.57 & 0.99 & 0.57 \\   &  & 2 & 0.97 & 0.80 & 0.97 & 0.80 \\  & & 4 & 1.00 & 0.73 & 1.00 & 0.72 \\  & & 8 & 1.00 & 0.59 & 1.00 & 0.58 \\   

Table 2: Mean statistics over 10 repetitions: scale-free, average degree 20, and sample size 1,000.

   Algorithm & Adj Pre & Adj Rec & Ori Pre & Ori Rec & \(\)BIC & Edges & Seconds \\  BOSS & 0.99 & 0.94 & 0.96 & 0.90 & 211.79 & 951.40 & 15.46 \\ fGES & 0.97 & 0.60 & 0.70 & 0.43 & 7784.48 & 617.35 & 5.16 \\ DAGMA & 1.00 & 0.69 & 0.98 & 0.67 & 3080.85 & 687.15 & 54.58 \\ LiNGAM & 0.54 & 0.94 & 0.35 & 0.62 & 3868.93 & 1752.75 & 582.03 \\   

Table 3: fMRI simulated data with pseudo-empirical errorsthe model are scale-free, with a small number of hub vertices with much higher degree than other vertices. Scale-free connectivity is consistent with biological expectations and prior connectivity analysis on fMRI data . More details are included in the Supplement.

## 7 Discussion

We have proposed a successor to the GRaSP algorithm  that retains its high accuracy but is faster and more scalable. BOSS implemented with GSTs comfortably scales to least 1000 variables with an average degree of at least 20. It can comfortably and informatively analyze data from 400 or even 1000 densely connected fMRI cortical parcellations. We show in simulations that our method is highly accurate for Erdos-Renyi graphs as well as scale-free graphs. Despite being developed for the linear Gaussian case, BOSS also performs well in the linear non-Gaussian case. BOSS is fast and accurate on simulated fMRI data and can rapidly produce informative and plausible models from clinical fMRI data. BOSS is available for use within the TETRAD project which includes Python and R wrappers .

The success of BOSS presents an opportunity to pursue further theoretical work showing how BOSS differs from GRaSP and what general lessons we may learn for constructing successor algorithms to BOSS. There is substantial room for additional improvements, as BOSS does have several limitations. For example, BOSS cannot handle most forms of unmeasured confounding, so it will be valuable to explore ways of adding this functionality while maintaining its accuracy and scalability. It will also be informative to apply BOSS to other types of data such as functional genomic data, financial data, and electronic health records.