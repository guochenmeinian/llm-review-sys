# One-step differentiation of iterative algorithms

Jerome Bolte

Toulouse School

of Economics,

Universite Toulouse

Capitole,

Toulouse, France. &Edouard Pauwels

Toulouse School

of Economics (IUF),

Toulouse, France. &Samuel Vaiter

CNRS &

Universite Cote d'Azur,

Laboratoire J. A. Dieudonne.

Nice, France.

###### Abstract

In appropriate frameworks, automatic differentiation is transparent to the user at the cost of being a significant computational burden when the number of operations is large. For iterative algorithms, implicit differentiation alleviates this issue but requires custom implementation of Jacobian evaluation. In this paper, we study one-step differentiation, also known as Jacobian-free backpropagation, a method as easy as automatic differentiation and as efficient as implicit differentiation for fast algorithms (e.g., superlinear optimization methods). We provide a complete theoretical approximation analysis with specific examples (Newton's method, gradient descent) along with its consequences in bilevel optimization. Several numerical examples illustrate the well-foundedness of the one-step estimator.

## 1 Introduction

Differentiating the solution of a machine learning problem is an important task, e.g., in hyperparameters optimization , in neural architecture search  and when using convex layers . There are two main ways to achieve this goal: _automatic differentiation_ (AD) and _implicit differentiation_ (ID). Automatic differentiation implements the idea of evaluating derivatives through the compositional rules of differential calculus in a user-transparent way. It is a mature concept  implemented in several machine learning frameworks . However, the time and memory complexity incurred may become prohibitive as soon as the computational graph becomes bigger, a typical example being unrolling iterative optimization algorithms such as gradient descent . The alternative, implicit differentiation, is not always accessible: it does not solely rely on the compositional rules of differential calculus (Jacobian multiplication) and usually requires solving a linear system. The user needs to implement custom rules in an automatic differentiation framework (as done, for example, in ) or use dedicated libraries such as  implementing these rules for given models. Provided that the implementation is carefully done, this is most of the time the gold standard for the task of differentiating problem solutions.

Contributions.We study a _one-step Jacobian_ approximator based on a simple principle: differentiate only the last iteration of an iterative process and drop previous derivatives. The idea of dropping derivatives or single-step differentiation was explored, for example, in  and our main contribution is a general account and approximation analysis for Jacobians of iterative algorithms. One-step estimation constitutes a rough approximation at first sight and our motivation to study is its ease-of-use within automatic differentiation frameworks: no custom Jacobian-vector products or vector-Jacobian products needed as long as a stop_gradient primitive is available (see Table 1).

We conduct an approximation analysis of one-step Jacobian (Corollary 1). The distance to the true Jacobian are produced by the distance to the solution (i.e., the quality of completion of the optimization phase) and the lack of contractivity of the iteration mapping. This imprecision has to be balanced with the ease of implementation of the one-step Jacobian. This suggests that one-step differentiation is efficient for small contraction factors, which corresponds to fast algorithms. We indeed show that one-step Jacobian is asymptotically correct for super-linearly convergent algorithms (Corollary 2) and provide similar approximation rate as implicit differentiation for _quadratically convergent algorithms_ (Corollary 3). We exemplify these results with hypergradients in bilevel optimization, conduct a detailed complexity analysis and highlight in Corollary 4 the estimation of approximate critical points. Finally, numerical illustrations are provided to show the practicality of the method on logistic regression using Newton's algorithm, interior point solver for quadratic programming and weighted ridge regression using gradient descent.

Related works.Automatic differentiation  was first proposed in the forward mode in  and its reverse mode in . The study of the behaviour of differentiating iterative procedure by automatic differentiation was first analyzed in  and  in the optimization community. It was studied in the machine learning community for smooth methods , and nonsmooth methods . Implicit differentiation is a recent highlight in machine learning. It was shown to be a good way to estimate the Jacobian of problem solutions, for deep equilibrium network , optimal transport , and also for nonsmooth problems , such as sparse models . Inexact implicit differentiation was explored in . Truncated estimation of the "Neumann series" for the implicit differentiation is routinely used  and truncated backpropagation was investigated in  for the gradient descent iterations. In the very specific case of min-min problems,  studied the speed of convergence of automatic, implicit, and analytical differentiation.

The closest work to ours is  - under the name _Jacobian-free backpropagation_ - but differs significantly in the following ways. Their focus is on single implicit layer networks, and guarantees are qualitative (descent direction of [22, Theorem 3.1]). In contrast, we provide quantitative results on abstract fixed points and applicable to any architecture. The idea of "dropping derivatives" was proposed in  for meta-learning, one-step differentiation was also investigated to train Transformer architectures , and to solve bilevel problems with quasi-Newtons methods .

 p{199.2pt}}  
**Input:** & **Input:** & \(x_{0}\), \(k>0\). \\ \( x_{0}()\), \(k>0\). & **Eval:** & **Eval:** \\  & **sub\_gradient** & **for**\(i=1,,k\)**do** \\  & \(x_{i}=F(x_{i-1},)\) & \(x_{i}=F(x_{i-1},)\) \\  & **Return:**\(x_{k}\) & **Return:**\(x_{k}()\) \\
**Differentiation:** & Custom implicit VJP / & **Differentiation:** \\  & **JVP**. & **native autodiff on **Eval** \\   

Table 1: Qualitative comparison of differentiation strategies. Native autodiff refers to widespread primitives in differentiable programming (e.g. grad in JAX). Custom JVP/VJP refers to specialized libraries such as jaxopt or qpth implicit differentiation in specific contexts.

## 2 One-step differentiation

### Automatic, implicit and one-step differentiation

Throughout the text \(F^{n}^{m}^{n}\) denotes a recursive algorithmic map from \(^{n}\) to \(^{n}\) with \(m\) parameters. For any \(^{m}\), we write \(F_{}:x F(x,)\) and let \(F_{}^{k}\) denote \(k\) recursive composition of \(F_{}\), for \(k\). The map \(F_{}\) defines a recursive algorithm as follows

\[x_{0}()^{n} x_{k+1}()=F(x_{k}( ),), \]

We denote by \(J_{x}F_{}\) the Jacobian matrix with respect to the variable \(x\). The following assumption is sufficient to ensure a non degenerate asymptotic behavior.

**Assumption 1** (Contraction): Let \(F^{n}^{m}^{n}\) be \(C^{1}\), \(0<1\), and \(^{n}\) be nonempty convex closed, such that for any \(^{m}\), \(F_{}()\) and \(\|J_{x}F_{}\|_{}\).

**Remark 1**: The main algorithms considered in this paper fall in the scope of smooth optimization. The algorithmic map \(F_{}\) is associated to a smooth parametric optimization problem given by \(f^{n}^{m}\) such that \(f_{} x f(x,)\) is strongly convex, uniformly in \(\). Two examples of algorithmic maps are given by gradient descent, \(F_{}(x)=x- f_{}(x)\), or Newton's \(F_{}(x)=x-^{2}f_{}(x)^{-1} f_{}(x)\) for positive step \(>0\). For small step sizes, gradient descent provides a contraction and Newton's method provides a local contraction, both fall in the scope of Assumption 1. Other examples include inertial algorithms such as the Heavy Ball method , which has to be considered in phase space and for which a single iteration is not contracting, but a large number of iteration is (see _e.g._[38; 35]).

The following lemma gathers known properties regarding the fixed point of \(F_{}\), denoted by \(()\) and for which we will be interested in estimating derivatives.

**Lemma 1**: _Under Assumption 1, for each \(\) in \(^{m}\) there is a unique fixed point of \(F_{}\) in \(\) denoted by \(()\), which is a \(C^{1}\) function of \(\). Furthermore, for all \(k\), we have \(\|x_{k}()-()\|^{k}\|x_{0}()-() \|^{k}-F_{}(x_{0})\|}{1-}\)._

This is well known, we briefly sketch the proof. The mapping \(F_{}\) is a \(\) contraction on \(\) - use the convexity of \(\) and the intermediate value theorem. Banach fixed point theorem ensures existence and uniqueness, differentiability is due to the implicit function theorem. The convergence rate is classical. We are interested in the numerical evaluation of the Jacobian \(J_{}()\), thus well-defined under Assumption 1.

The automatic differentiation estimator \(J^{}x_{k}()=J_{}x_{k}()\) propagates the derivatives (either in a forward or reverse mode) through iterations based on the piggyback recursion , for \(i=1,,k-1\),

\[J_{}x_{i+1}()=J_{x}F(x_{i}(),)J_{}x_{i}()+ J_{}F(x_{i}(),). \]

Under assumption 1 we have \(J^{}x_{k}() J_{}()\) and the convergence is asymptotically linear [25; 8; 35; 41; 14]. \(J^{}x_{k}()\) is available in differentiable programming framework implementing common primitives such as backpropagation.

The implicit differentiation estimator \(J^{}x_{k}()\) is given by application of the implicit function theorem using \(x_{k}\) as a surrogate for the fixed point \(\),

\[J^{}x_{k}()=(I-J_{x}F(x_{k}(),))^{-1}J_{}F( x_{k}(),). \]

By continuity of the derivatives of \(F\), we also have \(J^{}x_{k}() J_{}()\) as \(k\), (see _e.g._[27, Lemma 15.1] or [12, Theorem 1]). Implementing \(J^{}x_{k}()\) requires either manual implementation or dedicated techniques or libraries [4; 3; 45; 28; 20; 12] as the matrix inversion operation is not directly expressed using common differentiable programming primitives. A related estimator is the Inexact Automatic Differentiation (IAD) estimator which implements (2) but with Jacobians evaluated at the last iterates \(x_{k}\), which can be seen as an approximation of \(J^{}\)[35; 19].

The one-step estimator \(J^{}x_{k}()\) is the Jacobian of the fixed point map for the last iteration

\[J^{}x_{k}()=J_{}F(x_{k-1}(),). \]Contrary to automatic differentiation or implicit differentiation estimates, we do not have \(J^{}x_{k}() J_{}()\) in general as \(k\), but we will see that the error is essentially proportional to \(\), and thus negligible for fast algorithms for which the estimate is accurate.

From a practical viewpoint, the three estimators \(J^{}\), \(J^{}\) and \(J^{}\) are implemented in a differentiable programming framework, such as \(\), thanks to a primitive \(\), as illustrated by Algorithms 1, 2 and 3. The computational effect of the \(\) primitive is to replace the actual Jacobian \(J_{x}F(x_{i}(),)\) by zero for chosen iterations \(i\{1,,k\}\). Using it for all iterations except the last one, allows one to implement \(J^{}\) in (4) using Algorithm 3. This illustrates the main interest of the one-step estimator: it can be implemented using any differentiable programming framework which provides a \(\) primitive and does not require custom implementation of implicit differentiation. Figure 1 illustrates an implementation in \(\) for gradient descent.

### Approximation analysis of one step differentiation for linearly convergent algorithms

The following lemma is elementary. It describes the main mathematical mechanism at stake behind our analysis of one-step differentiation.

**Lemma 2**: _Let \(A^{n n}\) with \(\|A\|_{}<1\) and \(B,^{n m}\), then_

\[(I-A)^{-1}B-=A(I-A)^{-1}B+B-.\]

_Moreover, we have the following estimate,_

\[\|(I-A)^{-1}B-\|_{}\|B\|_{}+\|B-\|_{}.\]

**Proof :** First for any \(v^{n}\), we have \(\|(I-A)v\|\|Iv\|-\|Av\|(1-)\|v\|\), which shows that \(I-A\) is invertible (the kernel of \(I-A\) is trivial). We also deduce that \(\|(I-A)^{-1}\|_{} 1/(1-)\). Second, we have \((I-A)^{-1}-I=A(I-A)^{-1}\), since \(((I-A)^{-1}-I)(I-A)=A\), and therefore

\[A(I-A)^{-1}B+B-=((I-A)^{-1}-I)B+B-=(I-A)^{-1}B-.\]

The norm bound follows using the submultiplicativity of operator norm, the triangular inequality and the fact that \(\|A\|_{}\) and \(\|(I-A)^{-1}\|_{} 1/(1-)\). \(\)

**Corollary 1**: _Let \(F\) and \(\) be as in Assumption 1 such that \( F(x,)\) is \(L_{F}\) Lipschitz and \(x J_{}F(x,)\) is \(L_{J}\) Lipschitz (in operator norm) for all \(x^{n}\). Then, for all \(^{m}\),_

\[\|J^{}x_{k}()-J_{}()\|_{} }{1-}+L_{J}\|x_{k-1}-()\|. \]

**Proof :** The result follows from Lemma 2 with \(A=J_{x}F((),)\), \(B=J_{}F((),)\) and \(=J_{}F(x_{k-1},)\) using the fact that \(\|B\|_{} L_{F}\) and \(\|B-\|_{} L_{J}\|()-x_{k-1}\|\). \(\)

Figure 1: Implementation of Algorithms 1, 2 and 3 in \(\). \(F\) is a gradient step of some function \(f\). The custom implementation of implicit differentiation is not explicitly stated. The function \(\) is present in \(\).\(\) and \(\) computes the full Jacobian using forward-mode AD.

**Remark 2** (Comparison with implicit differentiation): In  a similar bound is described for \(J^{}\), roughly under the assumption that \(x F(x,)\) also has \(L_{J}\) Lipschitz Jacobian, one has

\[\|J^{}x_{k}()-J_{}()\|_{} L_{F}}{(1-)^{2}}\|x_{k}-()\|+}{1-} \|x_{k}-()\|. \]

For small \(\) and large \(k\), the main difference between the two bounds (5) and (6) lies in their first term which is of the same order whenever \(\) and \(L_{J}\|-x_{k-1}\|\) are of the same order.

Corollary 1 provides a bound on \(\|J^{}x_{k}()-J_{}()\|_{}\) which is asymptotically proportional to \(\). This means that for fast linearly convergent algorithms, meaning \( 1\), one-step differentiation provides a good approximation of the actual derivative. Besides, given \(F\) which satisfies Assumption 1, with a given \(<1\), not specially small, one can set \(_{}=F^{K}_{}\) for some \(K\). In this case, \(\) satisfies assumption 1 with \(=^{K}\) and the one-step estimator in (4) applied to \(\) becomes a \(K\)-steps estimator on \(F\) itself, we only differentiate through the \(K\) last steps of the algorithm which amounts to truncated backpropagation .

**Example 1** (Gradient descent): Let \(f^{n}^{m}\) be such that \(f(,)\) is \(\)-strongly convex (\(>0\)) with \(L\) Lipschitz gradient for all \(^{m}\), then the gradient mapping \(F(x,) x-_{x}f(x,)\) satisfies Assumption 1 with \(=\{1-, L-1\}\), smaller than \(1\) as long as \(0<<2/L\). The optimal \(=2/(L+)\) leads to a contraction factor \(=1-2/(L+)\). Assuming that \(^{2}_{x}f\) is also \(L\) Lipschitz, Corollary 1 holds with \(L_{F}=L_{J}=2L/(+L) 2\). For step size \(1/L\), Corollary 1 holds with \(L_{F}=L_{J} 1\) and \(=1-/L\). In both cases, the contraction factor \(\) is close to \(0\) when \(L/ 1\), for well conditioned problems. As outlined above, we may consider \(_{}=F^{K}_{}\) in which case Corollary 1 applies with a smaller value of \(\), recovering the result of [42, Proposition 3.1]

### Superlinear and quadratic algorithms

The one-step Jacobian estimator in (4) as implemented in Algorithm 3 is obviously not an exact estimator in the sense that one does not necessarily have \(J^{}x_{k}() J_{}()\) as \(k\). However, it is easy to see that this estimator is exact in the case of exact single-step algorithms, meaning \(F\) satisfies \(F(x,)=()\) for all \(x,\). Indeed, in this case, one has \(J_{x}F(x,)=0\) and \(J_{}F(x,)=J_{}()\) for all \(x,\). Such a situation occurs, for example, when applying Newton's method to an unconstrained quadratic problem. This is a very degenerate situation as it does not really make sense to talk about an "iterative algorithm" in this case. It turns out that this property of being "asymptotically correct" remains valid for very fast algorithms, that is, algorithms that require few iterations to converge, the archetypal example being Newton's method for which we obtain quantitative estimates.

#### 2.3.1 Super-linear algorithms

The following is a typical property of fast converging algorithms.

**Assumption 2** (Vanishing Jacobian): Assume that \(F^{n}^{m}^{n}\) is \(C^{1}\) and that the recursion \(x_{k+1}=F_{}(x_{k})\) converges globally, locally uniformly in \(\) to the unique fixed point \(()\) of \(F_{}\) such that \(J_{x}F((),)=0\).

Note that under Assumption 2, it is always possible to find a small neighborhood of \(\) such that \(\|J_{x}F_{}\|_{}\) remains small, that is, Assumption 1 holds locally and Lemma 1 applies. Furthermore, it is possible to show that the derivative estimate is asymptotically correct as follows.

**Corollary 2** (Jacobian convergence): _Let \(F^{n}^{m}^{n}\) be as in Assumption 2. Then \(J^{}x_{k}() J_{}()\) as \(k\), and \(J^{}()=J_{}()\)._

**Proof :** Since \(J_{x}F((),)=0\), implicit differentiation of the fixed point equation reduces to \(J_{}()=J_{}F((),)\), and the result follows by continuity of the derivatives. \(\)

**Example 2** (Superlinearly convergent algorithm): Assume that \(F\) is \(C^{1}\) and for each \(>0\), there is \(R>0\) such that \(\|F_{}(x)-()\|\|x-()\|\) for all \(x,\) such that \(\|x-()\| R\). Then Corollary 2 applies as for any \(v\)

\[J_{x}F((),)v=_{t 0}()+tv,)- ()}{t}=0.\]

#### 2.3.2 Quadratically convergent algorithms

Under additional quantitative assumptions, it is possible to obtain more precise convergence estimates similar to those obtained for implicit differentiation, see Remark 2.

**Corollary 3**: _Let \(F\) be as in Assumption 2 such that \(x J_{(x,)}F(x,)\) (joint jacobian in \((x,)\)) is \(L_{J}\) Lipschitz (in operator norm). Then, the recursion is asymptotically quadratically convergent and for each \(k 1\),_

\[\|J^{ OS}x_{k}()-J_{}()\|_{ op} L_{J}\|x_{k -1}()-()\|. \]

**Proof :** Following the same argument as in the proof of Corollary 2, we have

\[\|J^{ OS}x_{k}()-J_{}()\|_{ op}=\|J_{}F(x _{k}(),)-J_{}F((),)\|_{ op} L_{J} \|x_{k-1}()-()\|. \]

As for the quadratic convergence, we may assume that \(()=0\) and drop the \(\) variable to simplify notations. We have \(F(0)=0\) and for all \(x\),

\[F(x)=_{0}^{1}J_{x}F(tx)xdt\|x\|_{0}^{1}\|J_{x}F(tx)\|_{ op}dt \|x\|^{2}L_{J}_{0}^{1}tdt=\|x\|^{2}}{2}.\]

Thus \(L_{J}/2\|x_{k+1}\|[L_{J}/2\|x_{k}\|]^{2}\), and asymptotic quadratic convergence follows. \(\)

**Example 3** (Newton's algorithm): Assume that \(f^{n}^{m}\) is \(C^{3}\) with Lipschitz derivatives, and for each \(\), \(x f(x,)\) is \(\)-strongly convex. Then Newton's algorithm with backtracking line search satisfies the hypotheses of Corollary 3, see [15, Sec. 9.5.3]. Indeed, it takes unit steps after a finite number of iterations, denoting by \(()\) the unique solution to \(_{x}f(x,)=0\), for all \(x\) locally around \(()\)

\[F(x,)=x-_{xx}^{2}f(x,)^{-1}_{x}f(x,).\]

We have, \(_{xx}^{2}f(x,)F(x,)=_{xx}^{2}f(x,)x-_{x}f( x,)\), differentiating using tensor notations,

\[_{xx}^{2}f(x,)J_{x}F(x,)=^{3}f(x,)[x,, ]-^{3}f(x,)[F(x,),,]\]

so that \(J_{x}F((),)=0\) and Lipschitz continuity of the derivatives of \(f\) implies Lipschitz continuity of \(J_{x}F(x,)\) using the fact that \(_{xx}^{2}f(x,) I\) and that matrix inversion is smooth and Lipschitz for such matrices. The quadratic convergence of the three derivative estimates for Newton's method is illustrated on a logistic regression example in Figure 2.

## 3 Hypergradient descent for bilevel problems

Consider the following bilevel optimization problem

\[_{} g(x()) x()_{y }f(y,),\]

where \(g\) and \(f\) are \(C^{1}\) functions. We will consider bilevel problems such that the inner minimum is uniquely attained and can be described as a fixed point equation \(x=F(x,)\) where \(F\) is as in Assumption 1. The problem may then be rewritten as

\[_{} g(x()) x()=F(x(), ), \]

Figure 2: Newton’s method quadratic convergence.

see illustrations in Remark 1. Gradient descent (or hyper-gradient) on (9) using our one-step estimator in (4) consists in the following recursion

\[_{l+1}=_{l}- J^{}x_{k}(_{l})^{T} g(x_{k}( _{l})), \]

where \(>0\) is a step size parameter. Note that the quantity \(J^{}x_{k}()^{T} g(x_{k})\) is exactly what is obtained by applying backpropagation to the composition of \(g\) and Algorithm 3, without any further custom variation on backpropagation. Note that one in practice may use amortized algorithms, such as . This section is dedicated to the theoretical guarantees which can be obtained using such a procedure, proofs are postponed to Appendix A.

### Complexity analysis of different hypergradient strategies

We essentially follow the complexity considerations in [27, Section 4.6]. Let \(C_{F}\) denote the computation time cost of evaluating the fixed-point map \(F\) and \(>0\) be the multiplicative overhead of gradient evaluation, in typical applications, \( 5\) (cheap gradient principle ). The time cost of evaluating the Jacobian of \(F\) is \(n C_{F}\) (\(n\) gradients). Forward algorithm evaluation (i.e., \(x_{k}\)) has computational time cost \(kC_{F}\) with a fixed memory complexity \(n\). Vanilla piggyback recursion (2) requires \(k-1\) full Jacobians and matrix multiplications of costs \(n^{2}m\). The forward-mode of AD has time complexity \(k C_{F}m\) (compute \(m\) partial derivatives each of them cost \(\) times the time to evaluate the forward algorithm), and requires to store the iterate vector of size \(n\) and \(m\) partial derivatives. The reverse-mode of AD has time complexity \(k C_{F}\) (cheap gradient principle on \(F_{}^{k}\)) and requires to store \(k\) vectors of size \(n\). Implicit differentiation requires _one_ full Jacobian (\( C_{Fn}\)) and solution of _one_ linear system of size \(n n\), that is roughly \(n^{3}\). Finally, one-step differentiation is given by only differentiating a single step of the algorithm at cost \( C_{F}\). For each estimate, distance to the solution will result in derivative errors. In addition, automatic differentiation based estimates may suffer from the burn-in effect  while one-step differentiation will suffer from a lack of contractivity as in Corollary 1. We summarize the discussion in Table 2. Let us remark that, if \(C_{F} n^{2}\), then reverse AD has a computational advantage if \(k n\), which makes sense for fast converging algorithms, but in this case, one-step differentiation has a small error and a computational advantage compared to reverse AD.

**Remark 3** (Implicit differentiation: but on which equation?): Table 2 is informative yet formal. In practical scenarios, implicit differentiation should be performed using the simplest equation available, not necessarily \(F=0\). This can significantly affect the computational time required. For instance, when using Newton's method \(F=-[^{2}f]^{-1} f\), implicit differentiation should be applied to the gradient mapping \( f=0\), not \(F\). In typical application \(C_{ f}=O(n^{2})\), and the dominant cost of implicit differentiation is \(O(n^{3})\), which is of the same order as the one-step differentiation as \(C_{F}=O(n^{3})\) (a linear system needs to be inverted). However, if the implicit step was performed on \(F\) instead of \( f\), it would incur a prohibitive cost of \(O(n^{4})\). In conclusion, the implicit differentiation phase is not only custom in terms of the implementation, but also in the very choice of the equation.

   Method & Time & Memory & Error sources \\  Piggyback recursion & \(kn( C_{F}+nm)\) & \(n(n+m)\) & suboptimality + burn-in \\ AD forward-mode & \(k C_{F}m\) & \(n+m\) & suboptimality + burn-in \\ AD reverse-mode & \(k C_{F}\) & \(kn\) & suboptimality + burn-in \\ Implicit differentiation & \( C_{F}n+n^{3}\) & \(n\) & suboptimality \\ One-step differentiation & \( C_{F}\) & \(n\) & suboptimality + lack of contractivity \\  Forward algorithm & \(kC_{F}\) & \(n\) & suboptimality \\   

Table 2: Time and memory complexities of the estimators in Section 2.1 (up to multiplicative constant). \(F\) has time complexity denoted by \(C_{F}\) and we consider \(k\) iterations in \(^{n}\) with \(m\) parameter. \(\) is the multiplicative overhead of evaluating a gradient (cheap gradient principle).

### Approximation analysis of one step differentiation

The following corollary, close to [42, Prop. 3.1], provides a bound on the one-step differentiation (Algorithm 3) gradient estimator for (9). The bound depends on \(\), the contraction factor, and distance to the solution for the inner algorithm. The proof is given in Appendix A.

**Corollary 4**: _Let \(F^{n}^{m}^{n}\) be as in Corollary 1 and consider the bilevel problem (9), where \(g\) is a \(C^{1}\), \(l_{g}\) Lipschitz function with \(l_{}\) Lipschitz gradient. Then,_

\[_{}(g)()-J^{}x_{k}()^{T } g(x_{k})l_{g}}{1-}+L_{J}l_{g}\|x_{k-1 }-()\|+L_{F}l_{}\|()-x_{k}\|.\]

### Approximate critical points

The following lemma is known, but we provide a proof for completeness in Appendix A.

**Lemma 3**: _Assume that \(h^{m}\) is \(C^{1}\) with \(L\) Lipschitz gradient and lower bounded by \(h^{*}\). Assume that for some \(>0\), for all \(l\), \(_{l+1}-_{l}+ h(_{l}) .\) Then, for all \(K\), \(K 1\), we have_

\[_{l=0,,K}\| h(_{l})\|^{2}^{2}+)-h^{*})}{K+1}.\]

Combining with Corollary 4, it provides a complexity estimate for the recursion in (10).

**Corollary 5**: _Under the setting of Corollary 4, consider iterates in (10) with \(k 2\) and assume the following_

* \(_{}\|x_{0}()-F_{}(x_{0}())\| M\)_, for some_ \(M>0\)_._
* \(F\) _is_ \(L_{F}\) _Lipschitz and_ \(J_{(x,)}F\) _is_ \(L_{J}\) _Lipschitz jointly in operator norm._
* \(g\) _is_ \(C^{1}\)_,_ \(l_{g}\) _Lipschitz with_ \(l_{}\) _Lipschitz gradient._
* \(g\) _is lower bounded by_ \(g^{*}\)_._

_Then setting \(=(L_{F}l_{g}+(L_{J}l_{g}+L_{F}l_{})M^{k-2})\), for all \(K\),_

\[_{l=0,,K}\|_{}(g)(_{l})\|^{2} ^{2}+)(_{0})-g^{*})}{K+1}.\]

The level of approximate criticality is the sum of a term proportional to \(\) and a term inversely proportional to \(K\). For large values of \(k\) (many steps on the inner problem), \(K\) (many steps on the outer problem), approximate criticality is essentially proportional to \( L_{F}l_{g}/(1-)\) which is small if \(\) is close to \(0\) (_e.g._ superlinear algorithms).

## 4 Numerical experiments

We illustrate our findings on three different problems. First, we consider Newton's method applied to regularized logistic regression, as well as interior point solver for quadratic problems. These are two fast converging algorithms for which the results of Section 2.3 can be applied and the one-step procedure provides accurate estimations of the derivative with a computational overhead negligible with respect to solution evaluation, as for implicit differentiation. We then consider the gradient descent algorithm applied to a ridge regression problem to illustrate the behavior of the one step procedure for linearly convergent algorithms.

Logistic regression using Newton's algorithm.Let \(A^{N n}\) be a design matrix, the first column being made of 1s to model an intercept. Rows are denoted by \((a_{1},,a_{N})\). Let \(x^{n}\) and \(y\{-1,1\}^{N}\). We consider the regularized logistic regression problem

\[_{x^{n}}_{i=1}^{N}_{i}( a_{i},x \,y_{i})+\|x_{-1}\|^{2}, \]where \(\) is the logistic loss, \( t(1+(-t))\), \(>0\) is a regularization parameter, and \(x_{-1}\) denotes the vector made of entries of \(x\) except the first coordinate (we do not penalize intercept). This problem can be solved using Newton's method which we implement in jax using backtracking line search (Wolfe condition). Gradient and Hessian are evaluated using jax automatic differentiation, and the matrix inversion operations are performed with an explicit call to a linear system solver.

We denote by \(x()\) the solution to problem (11) and try to evaluate the gradient of \(\|x()\|^{2}/2\) using the three algorithms presented in Section 2.1. We simulate data with Gaussian class conditional distributions for different values of \(N\) and \(n\). The results are presented in Figure 3 where we represent the time required by algorithms as a function of the number of parameters required to specify problem (11), in our case size of \(A\) and size of \(y\), which is \((n+1)N\).

Figure 3 illustrates that both one-step and implicit differentiation enjoy a marginal computational overhead, contrary to algorithmic differentiation. In this experiment, the one-step estimator actually has a slight advantage in terms of computation time compared to implicit differentiation.

Interior point solver for quadratic programming:The content of this section is motivated by elements described in , which is associated with a pytorch library implementing a standard interior point solver. Consider the following quadratic program (QP):

\[_{x^{n}}x^{T}Qx+q^{T}x Ax =, Gx h, \]

where \(Q^{n n}\) is positive definite, \(A^{m n}\) and \(G^{p n}\) are matrices, \(q^{n}\), \(^{m}\) and \(h^{p}\) are vectors. We consider \(x()\) the solution of problem (12) as a function of \(\), the right-hand side of the equality constraint. We implemented in jax a standard primal-dual Interior Point solver for problem (12). Following , we use the implementation described in , and we solve linear systems with explicit calls to a dedicated solver. For generic inputs, this algorithm converges very fast, which we observed empirically. Differentiable programming capacities of jax can readily be used to implement the automatic differentiation and one-step derivative estimators without requiring custom interfaces as in . Indeed, implicit differentiation for problem (12) was proposed in  with an efficient pytorch implementation. We implemented these elements in jax in order to evaluate \(J_{}x()\) using implicit differentiation. More details on this experiment are given in Appendix B.

We consider evaluating the gradient of the function \(\|x()\|^{2}/2\) using the three algorithms proposed in Section 2.1. We generate random instances of QP in (12) of various sizes. The number of parameters needed to describe each instance is \(n(n+1)+(n+1)m+(n+1)p\). The results are presented in Figure 3 where we represent the time required by algorithms as a function of the number of parameters required to specify problem (12). In all our experiments, the implicit and one-step estimates agree up to order \(10^{-6}\). From Figure 3, we

Figure 3: Timing experiment to evaluate one gradient. Left: Differentiable QP in (12), one step and implicit estimators agree up to an error of order \(10^{-16}\). Right: Newton’s method on logistic regression (11), one step and implicit estimators agree up to an error of order \(10^{-12}\). Label “None” represent solving time and “Autodiff”, “Implicit” and “One step” represent solving time and gradient evaluation for each estimator in Section 2.1. The mean is depicted using shading to indicate standard deviation estimated over 10 runs.

see that both one-step and implicit differentiation enjoy a marginal additional computational overhead, contrary to algorithmic differentiation.

Weighted ridge using gradient descent.We consider a weighted ridge problem with \(A^{N n}\), \(y^{N}\), \(>0\) and a vector of weights \(^{N}\):

\[()=_{x^{n}}f_{}(x)=_{i= 1}^{N}_{i}(y_{i}- a_{i},x)^{2}+\|x\|^{2}.\]

We solve this problem using gradient descent with adequate step-size \(F(x,)=x- f_{}(x)\) with \(x_{0}()=0\), and we consider the \(K\)-step truncated Jacobian propagation \(=F_{}^{K}\) with \(K=1/\) where \(\) is the effective condition number of the Hessian. Figure 4 benchmarks the use of automatic differentiation, one-step differentiation, and implicit differentiation on the data set cpusmall provided by LibSVM  for two types of step-sizes. We monitor both quantities \(\|x_{k}()-()\|^{2}\) for the iterates, and \(\|J_{g}x_{k}()-J_{}()\|^{2}\) for the Jacobian matrices. As expected, implicit differentiation is faster and more precise, it is our gold standard which requires _custom implicit system to be implemented_ (or the use of an external library). For large steps, autodiff suffers from the burn-in phenomenon described in , which does not impact the one step estimator. Therefore, for a fixed time budget, the one step strategies allows to obtain higher iterate accuracy and similar, or better, Jacobian accuracy. In the small step regime, one step differentiation provides a trade-off, for a fixed time budget, one obtains better estimates for the iterate and worse estimates for the Jacobian matrices. Our results suggest that \(K\)-step truncated backpropagation allows to save computation time, at the cost of possibly degraded derivatives compared to full backpropagation, in line with .

## 5 Conclusion

We studied the one-step differentiation, also known as Jacobian-free backpropagation, of a generic iterative algorithm, and provided convergence guarantees depending on the initial rate of the algorithm. In particular, we show that one-step differentiation of a quadratically convergent algorithm, such as Newton's method, leads to a quadratic estimation of the Jacobian. A future direction of research would be to understand how to extend our findings to the nonsmooth world as in  for linearly convergent algorithms.

Figure 4: Differentiation of gradient descent for solving weighted Ridge regression on cpusmall. Top line: condition number of 1000. Bottom line: condition number of 100. Left column: small learning rate \(\). Right column: big learning rate \(\). Dotted (resp. filled) lines represent the error of the iterates (resp. of the Jacobians).