# ECLipsE: Efficient Compositional Lipschitz Constant Estimation for Deep Neural Networks

Yuezhu Xu

Edwardson School of Industrial Engineering

Purdue University

West Lafayette, IN, USA

xu1732@purdue.edu

&S. Sivaranjani

Edwardson School of Industrial Engineering

Purdue University

West Lafayette, IN, USA

sseetha@purdue.edu

###### Abstract

The Lipschitz constant plays a crucial role in certifying the robustness of neural networks to input perturbations. Since calculating the exact Lipschitz constant is NP-hard, efforts have been made to obtain tight upper bounds on the Lipschitz constant. Typically, this involves solving a large matrix verification problem, the computational cost of which grows significantly for both deeper and wider networks. In this paper, we provide a compositional approach to estimate Lipschitz constants for deep feed-forward neural networks. We first obtain an _exact_ decomposition of the large matrix verification problem into smaller sub-problems. Then, leveraging the underlying cascade structure of the network, we develop two algorithms. The first algorithm explores the geometric features of the problem and enables us to provide Lipschitz estimates that are comparable to existing methods by solving small semidefinite programs (SDPs) that are only as large as the size of each layer. The second algorithm relaxes these sub-problems and provides a closed-form solution to each sub-problem for extremely fast estimation, altogether eliminating the need to solve SDPs. The two algorithms represent different levels of trade-offs between efficiency and accuracy. Finally, we demonstrate that our approach provides a steep reduction in computation time (as much as several thousand times faster, depending on the algorithm for deeper networks) while yielding Lipschitz bounds that are very close to or even better than those achieved by state-of-the-art approaches in a broad range of experiments*. In summary, our approach considerably advances the scalability and efficiency of certifying neural network robustness, making it particularly attractive for online learning tasks.

Footnote *: [https://github.com/YuezhuXu/ECLipsE](https://github.com/YuezhuXu/ECLipsE)

## 1 Introduction

The Lipschitz constant, which quantifies how a neural networks output varies in response to changes in its inputs, is a crucial measure in providing robustness certificates  on downstream tasks such as ensuring resilience against adversarial attacks , stability of learning-based models or systems with neural network controllers , enhancing generalizability , improving gradient-based optimization methods and controlling the rate of learning . The problem of calculating the exact Lipschitz constant is NP-hard . Therefore, efforts have been made to estimate tight upper bounds for the Lipschitz constant of feed-forward neural networks (FNNs)  and other architectures such as convolutional neural networks (CNNs) . Typical approaches include formulating a polynomial optimization problem  or bounding the Lipschitz constant via quadratic constraints and semidefinite programming (SDP) , which in turn requires solving a large-scale matrix verification problem whose computational complexity grows significantly withboth the depth and width of the network. These approaches have also motivated the development of methods to design neural networks with certifiable robustness guarantees [19; 23; 24; 25].

**Contribution.** In this paper, we provide a scalable compositional approach to estimate Lipschitz constants for deep feed-forward neural networks. We demonstrate steep reductions in computation time (as much as several thousand times faster than the state-of-the-art depending on the experiment), while obtaining Lipschitz estimates that are very close to or even better than those achieved by state-of-the-art approaches. Specifically, we develop two algorithms, representing different levels in the trade-off between accuracy and efficiency, allowing for application-specific choices. The first algorithm, **ECLipsE**, involves estimating the Lipschitz constant through a compositional layer-by-layer solution of small SDPs that are only as large as the weight matrix in each layer. The second algorithm, **ECLipsE-Fast**, provides a _closed-form solution_ to estimate the Lipschitz constant, completely eliminating the need to solve any matrix inequality SDPs. Both algorithms provably guarantee the existence of solutions at each step to generate tight Lipschitz estimates. In summary, our work significantly advances scalability and efficiency in certifying neural network robustness, making it applicable to a variety of online learning tasks.

**Theoretical Approach.** We begin with the large matrix verification SDP for Lipschitz constant estimation under the well-known framework LipSDP . To avoid handling a large matrix inequality, we employ a sequential Cholesky decomposition technique to obtain an _exact_ decomposition of the large matrix verification problem into a series of smaller, more manageable sub-problems that are only as large as the size of the weight matrix in each layer. Then, observing the cascade structure of the neural network, we develop (i) algorithm **ECLipsE**, which characterizes the geometric features of the optimization problem and enables us to provide an accurate Lipschitz estimate and (ii) algorithm **ECLipsE-Fast**, which further relaxes the sub-problems, and yields a closed-form solution for each sub-problem that altogether eliminates the need to solve any SDPs, resulting in extremely fast implementations.

**Related Work.** The simplest way to estimate the Lipschitz constans is to provide a naive upper bound using the product of induced weight norms, which is rather conservative . Another approach is to utilize automatic differentiation to approximate a bound, which is not a strict upper bound, although it is often so in practice . Additionally, compositions of nonexpansive averaged operators and affine operators , Clarke Jacobian based approaches and other methods focusing on local Lipschitz constants  have also been studied. Recently, optimization-based approaches such as sparse polynomial optimization  and SDP methods such as the canonical LipSDP framework  have been successful in providing tighter Lipschitz bounds. SDP-based methods specifically exploit the slope-restrictedness of the activation functions to cast the problem of estimating a Lipschitz constant as a linear matrix verification problem. However, the computational cost of such methods explodes as the number of layers increases. A common strategy to address this is to ignore some coupling constraints among the neurons to reduce the number of decision variables, yielding a more scalable algorithm at the expense of estimation accuracy . Another strategy is to exploit the sparsity of the SDP using graph-theoretic approaches to decompose it into smaller linear matrix inequalities (LMI) . Along similar lines,  and  employ a dissipativity-based method and dynamic convolutional partition respectively to derive layer-wise LMIs that are applicable to both FNNs and CNNs. Very recent developments also focus on enhancing the scalability of SDP-based implementations through eigenvalue optimization and memory improvement , which are compatible with autodiff frameworks such as PyTorch and TensorFlow.

## 2 Problem Formulation and Background

**Notation.** We define \(_{N}=\{1,,N\}\), where \(N\) is a natural number excluding zero. A symmetric positive-definite matrix \(P^{n n}\) is represented as \(P>0\) (and as \(P 0\), if it is positive semi-definite). We denote the largest singular value or the spectral norm of matrix \(A\) by \(_{max}(A)\). The set of positive semi-definite diagonal matrices is written as \(_{+}\).

### Problem Formulation

We consider a feedforward neural network (FNN) of \(l\) layers with input \(z^{d_{0}}\) and output \(y^{d_{l}}\) defined as \(y=f(z)\). The function \(f\) is recursively formulated with layers \(_{i},i_{l}\), defined as

\[_{i}:\,z^{(i)}=(v^{(i)}) i_{l-1}, _{l}:\,y=f(z)=z^{(l)}=v^{(l)}, z^{(0)}=z, \]where \(v^{(i)}=W_{i}z^{(i-1)}+b_{i}\) with \(W_{i}\) and \(b_{i}\) representing the weight and bias for layer \(_{i}\) respectively, and \(:^{d_{i}}^{d_{i}}\) is a nonlinear _activation function_ that acts element-wise on its argument. The last layer \(_{l}\) is termed the _output layer_. We denote the number of neurons in layer \(_{i}\) by \(d_{i}\), \(i_{l}\).

**Definition 1**.: _A function \(f:^{d_{0}}^{d_{l}}\) is Lipschitz continuous on \(^{d_{0}}\) if there exists a constant \(L>0\) such that \(\|f(z_{1})-f(z_{2})\|_{2} L\|z_{1}-z_{2}\|_{2}, z_{1},z_{2} \). The smallest positive \(L\) satisfying this inequality is termed the Lipschitz constant of the function \(f\)._

Without loss of generality, we assume \(W_{i} 0,i_{l}\), as any weights being 0 will lead to the trivial case where the output corresponding to any input will remain the same after that layer. Our goal is to provide a scalable approach to give an efficient and accurate upper bound for the Lipschitz constant \(L>0\). Note that the proofs of all the theoretical results in this paper are included in Appendix A.

### Preliminaries

We begin with a slope-restrictedness property satisfied by most activation functions, which is typically leveraged to to derive SDPs for Lipschitz certificates .

**Assumption 1** (Slope-restrictedness).: _For the neural network defined in (1), the activation function \(\) is slope-restricted in \([,]\), \(<\) in the sense that \( v_{1},v_{2}^{n}\), we have \((v_{1}-v_{2})(v_{1})-(v_{2})(v_{1}-v_{2})\) element-wise. Consequently, we have that for \(_{+}\),_

\[v_{1}-v_{2}\\ (v_{1})-(v_{2})^{T}p&-m\\ -m&v_{1}-v_{2}\\ (v_{1})-(v_{2}) 0, p=, m=(+ )/2. \]

Now, we can obtain an upper bound for the Lipschitz constant as follows; this result is equivalent to the well-known LipSDP framework .

**Theorem 1** (LipSDP).: _For the FNN (1) satisfying Assumption 1, if there exists \(F>0\) and positive diagonal matrices \(_{i}_{+}\), \(i_{l-1}\) such that with \(p=\) and \(m=\),_

\[I+pW_{1}^{T}_{1}W_{1}&-mW_{1}^{T}_{1}&0&...&0\\ -m_{1}W_{1}&_{1}+pW_{2}^{T}_{2}W_{2}&-mW_{2}^{T}_{ 2}&...&0\\ 0&-m_{2}W_{2}&_{2}+pW_{3}^{T}_{3}W_{3}&...&0\\ &&\\ 0&...&-m_{l-2}W_{l-2}&_{l-2}+pW_{l-1}^{T}_{l-1}W_{l-1}&- mW_{l-1}^{T}_{l-1}\\ 0&0&...&-m_{l-1}W_{l-1}&_{l-1}-FW_{l}^{T}W_{l}>0, \]

_then \(\|z_{2}^{(l)}-z_{1}^{(l)}\|_{2}\|z_{2}^{(0)}-z_ {1}^{(0)}\|_{2}\), which provides a sufficient condition for the Lipschitz constant \(L\) to be upper bounded by \(\)._

_Remark_ 1. LipSDP provides three variants that tradeoff accuracy and efficiency, namely, LipSDP-Network, LipSDP-Neuron, and LipSDP-Layer, whose scalability increases sequentially at the expense of decreased accuracy. However,  provides a counterexample showing that the Lipschitz estimate from LipSDP-Network is not a strict upper bound; thus, only LipSDP-Neuron, and LipSDP-Layer are valid. Theorem 1 here directly corresponds to LipSDP-Neuron. If all \(_{i}\), \(i_{l-1}\) in (3) are set to multiples of identity matrices, that is, \(_{i}I\), \(i_{l-1}\), then it corresponds to LipSDP-Layer.

Assumption 1 holds for all commonly used activation functions; for example, it holds with \(=0\), \(=1\), that is, \(p=0,m=1/2\) for the ReLU, sigmoid, tanh, exponential linear functions. Therefore, we focus on this case in this work.

## 3 Methodology

We now develop two fast compositional algorithms based on LipSDP-Layer and Lipschitz-Neuron respectively. Both algorithms are not only scalable and significantly faster, but also provide comparable estimates for the Lipschitz constant.

### Exact Decomposition

We circumvent direct solution of the large matrix inequality in (3), which becomes computationally prohibitive as the FNN (1) grows deeper. Instead, we develop a sequential block Cholesky decomposition method, akin to the technique introduced in , also expanded in [32; 33]. We first restate Lemma 2 of  below.

**Theorem 2** (Restatement of Lemma 2 of ).: _A symmetric block tri-diagonal matrix defined as_

\[_{1}&_{2}&0&...&&0\\ _{2}^{T}&_{2}&_{3}&...&&0\\ 0&_{3}^{T}&_{2}&_{3}&...&0\\ &&&&\\ 0&...&0&_{l-1}^{T}&_{l-1}&_{l}\\ 0&...&&0&_{l}^{T}&_{l}, \]

_is positive definite if and only if \(X_{i}>0, i\{0\}_{l-1},\) where_

\[X_{i}=_{i}&i=0,\\ _{i}-_{i}^{T}X_{i-1}^{-1}_{i}&i _{l-1}. \]

**Theorem 3**.: _Let \(P_{l}\) be defined as in (3) with \(p=0,m=1/2\). Then, the Lipschitz certificate \(P_{l}>0\) holds if and only if the following sequence of matrix inequalities is satisfied:_

\[M_{i}>0, i_{l-2}, M_{l-1}-FW_{l}^{T}W_{l}>0, \]

_where_

\[M_{i}=I&i=0\\ _{i}-_{i}W_{i}(M_{i-1})_{l}^{-1}W_{i}^{T}_{i}&i _{l-1}. \]

Theorem 3 provides an **exact decomposition** of (3), and allows us to establish necessary and sufficient conditions through small matrix inequalities that scale with the size of the weight matrices of each layer, rather than that of the entire network. To accurately estimate the Lipschitz constant, we need to decide on \(_{i},i_{1-1}\) that generate a tight upper bound at the last stage. In other words, we want \(M_{l-1}-FW_{l}^{T}W_{l}>0\) to yield the smallest estimate for \(\). In the following subsection, we provide compositional algorithms to decide the appropriate \(_{i},i_{1-1}\) sequentially, so that we only need to solve one small problem corresponding to each layer.

### Compositional Algorithms

We first propose two practical algorithms here. The theory supporting the algorithms and the geometric intuition are deliberately deferred, and will be thoroughly discussed in a the next subsection.

The first algorithm, **ECLipsE**, explores the geometric features that enables us to provide an accurate Lipschitz estimate by solving small semidefinite programs (SDPs), which are of the size of the weight matrices on each layer. The second algorithm, **ECLipsE-Fast** relaxes the sub-problems at each stage and yields a closed-form solution for each sub-problem that makes it extremely fast. These algorithms represent different trade-offs between efficiency and accuracy; one may choose **ECLipsE** if pursuing accuracy, and **ECLipsE-Fast** for applications where time is of the essence.

We observe in (7) that \(M_{i}\) is obtained in a recursive manner and depends on \(_{i}\) and \(M_{i-1}\), \(i_{l-1}\). Therefore, we decide \(_{i}\) and then calculate \(M_{i}\) for \(i_{l-1}\) sequentially. Thus, these two algorithms can be implemented layer-by-layer in a compositional manner.

Concretely, for **ECLipsE**, we obtain \(_{i}\), \(i_{l-1}\) at each stage \(i\) using the information from the next layer, i.e. \(W_{i+1}\), by solving the following _small SDP_:

\[_{c_{i}}\ c_{i}\ _{i}-c_{i}W_{i+1}^{T}W _{i+1}&_{i}(W_{i}(M_{i-1})^{-1}W_{i}^{T})^{}\\ (W_{i}(M_{i-1})^{-1}W_{i}^{T})^{}_{i}&I >0,\ _{i}_{+},\ c_{i}>0 \]

For **ECLipsE-Fast**, \(_{i}\) is reduced to \(_{i}I\), \(i_{l-1}\) and \(_{i}\) is calculated in _closed-form_ as

\[_{i}=(W_{i}(M_{i-1})^{-1}W_{i}^{T})}. \]

Note that this _completely eliminates_ the need to solve matrix inequality _SDPs_ altogether. At last, after all \(_{i}\)s, \(i_{l-1}\) are decided, we obtain the smallest \(1/F\), which yields the smallest Lipschitz estimate \(L=\), as follows

\[1/F=_{max}(W_{l}^{T}W_{l}(M_{l-1})^{-1}). \]_Remark_ 2. We choose to directly calculate the smallest \(1/F\) rather than first derive the largest \(F\). This is because obtaining the largest \(F\) first involves taking the inverse of \(W_{l}^{T}W_{l}\), which can cause numerical issues due to potential singularity of \(W_{l}^{T}W_{l}\). In contrast, directly calculating the smallest \(1/F\) involves taking the inverses of \(M_{l-1}\), which is already guaranteed to be strictly positive definite at layer \(l-1\) when deciding \(_{l-1}\).

We summarize the algorithms as one in Algorithm 1. Algorithms **ECLipsE** and **ECLipsE-Fast** are respectively preferable based on whether the priority is on accuracy or speed.

```
0: Weights \(\{W_{i}\}_{i=1}^{l}\) from a FNN (1) with activation function slope-restricted in \(\)
0: Lipschitz estimate \(L\)
1: Set \(M_{0}=I\)
2:for\(i=1,2,...,l-1\)do
3:ifECLipsE (pursuing accuracy) then
4: Obtain \(_{i}\) from the optimal solution of (8)
5:elseifECLipsE-Fast (pursuing speed) then
6: Obtain \(_{i}\) from (9)
7:\(_{i}_{i}I\)
8:endif
9: Obtain \(M_{i}\) from (7) with \(_{i}\) and \(M_{i-1}\)
10:endfor
11: Obtain \(1/F\) from (10)
12:Return\(L=\)
```

**Algorithm 1**ECLipsE and **ECLipsE-Fast**

### Theory

Now we dive into the cascade structure of feed-forward neural networks and demonstrate the theory behind the two algorithms. We analyze the compositional algorithms in Section 3.2 in a backward manner, starting with the output layer. After all \(_{i}\), \(i_{l-1}\) are decided, \(M_{i}>0\), \(i_{l-2}\) hold. From Theorem 3, it remains to guarantee that \(M_{l-1}-FW_{l}^{T}W_{l}>0\), and consequently, (10), for which we state the following result.

**Proposition 1**.: _For given \(_{i}\), \(i_{l-1}\) that satisfies \(M_{i}>0\), \(i_{l-2}\), the tightest upper bound for Lipschitz constant is \(L=(W_{l}^{T}W_{l}(M_{l-1})^{-1})}\)._

Now, at stage \(l-1\), when deciding \(_{l-1}\), \(_{i}\), \(i_{l-2}\) are fixed and thus \(M_{l-2}\) is fixed. According to Proposition 1, we would like to choose \(_{l-1}\) such that \(_{max}(W_{l}^{T}W_{l}(M_{l-1})^{-1})\), where \(M_{l-1}\) is a function of \(_{l-1}\), is as small as possible. We have the following result.

**Lemma 1**.: _If \(M_{i}>0\), then \(W_{i+1}^{T}W_{i+1}(M_{i})^{-1}\) and \(W_{i+1}(M_{i})^{-1}(W_{i+1})^{T}\) share the same non-zero eigenvalues._

Note that at stage \(i\), it is guaranteed that \(M_{i}>0\). Taking \(i=l-1\), Lemma 1 infers that it is equivalent to minimize \(_{max}(W_{l}(M_{l-1})^{-1}W_{l}^{T})\) when deciding on \(_{l-1}\). Note that \(M_{l-1}>0\), and consequently, the existence of \(M_{l-1}^{-1}\) is already guaranteed when we reach the last stage. For the sake of conciseness, we define \(_{i} W_{i}(M_{i-1})^{-1}W_{l}^{T} i_{l -1}\). From (7), \(M_{i}=_{i}-_{i}_{i}_{i}\). We further write out the recursive expression for \(_{i}\) as

\[_{i+1}=W_{i+1}(M_{i})^{-1}W_{i+1}^{T}=W_{1}W_{1}^{T}&i =0\\ W_{i+1}(_{i}-_{i}_{i}_{i})^{-1}W_{ i+1}^{T}&i_{l-1}. \]

**Lemma 2**.: _For any constant \((0,1)\), any \(_{i}_{+}\) that satisfies \(M_{i}=_{i}-_{i}_{i}_{i}>0\) is also a feasible solution for \(_{i}_{i}-_{i}(_{ i})_{i}>0\). In other words, the feasible region \(\{_{i}:M_{i}>0,_{i}_{+}\}\{_{i}: _{i}>0,_{i}_{+}\}\)._

Lemma 2 gives us the observation that a contraction \(_{i}_{i},(0,1)\) yields a larger feasible space for \(_{i}_{+}\) to ensure \(M_{i}>0\). Meanwhile, (11) shows that for any given \(_{i}\), a smaller \(_{i}\) leads to a smaller \(_{i+1}\) for the next stage. We can characterize how'small' \(_{i}\) is by its spectral norm \(_{max}(_{i})\). Then, minimizing \(_{max}(_{i})\) aligns with our goal of minimizing \(_{max}(W_{l}^{T}W_{l}(M_{l-1})^{-1})=_{max}(W_{l} (M_{l-1})^{-1}W_{l}^{T})=_{max}(_{l})\) at the last stage. In other words, a smaller \(_{1}\) at the start will generally translate to a tighter Lipschitz estimate at output layer if we always choose to minimize the spectral norm \(_{max}(F_{i})\) at each stage.

Now we focus on how to specifically optimize \(_{i}\), \(i_{l-1}\). At stage \(i\), the goal is to seek for the \(_{i}\) that minimizes \(_{max}(_{i+1})\), where \(_{i+1}=W_{i+1}(_{i}-_{i}_{i} _{i})^{-1}W_{i+1}^{T}\) as in (11). Note that \(M_{i-1}\) and \(_{i}\) are already fixed and can be regarded as constants at the \(i\)-th stage.

**Proposition 2**.: _If there exists a singular matrix \(N 0\) such that \(M_{i}=c_{i}W_{i+1}^{T}W_{i+1}+N\), with constant \(c_{i}>0\), then \(_{max}(_{i+1})=1/c_{i}\), \( i_{l-1}\)._

In other words, we need to find the largest \(c_{i}>0\) to minimize \(_{max}(_{i+1})=1/c_{i}\). Recall that \(M_{i}=_{i}-_{i}_{i}_{i}\) is a function of \(_{i}\). We state the following proposition that is used to derive the small sub-problems at each stage.

**Proposition 3**.: _Consider the following optimization problem for \( i_{l-1}\)._

\[_{c_{i}} c_{i}_{i}- _{i}W_{i}(M_{i-1})^{-1}W_{i}^{T}_{i}-c_{i}(W_{i+1}^{T}W_{i+1})> 0,_{i}_{+}, c_{i}>0 \]

_Then, the optimal value \(c_{i}\) is the largest constant such that \(M_{i}\) can be written as \(M_{i}=c_{i}W_{i+1}^{T}W_{i+1}+N\), where \(N\) is some singular matrix such that \(N 0\). Moreover, the feasible region for the optimization problem is always nonempty._

**Geometric Analysis:** We illustrate the process of achieving the largest \(c_{i}>0\) in Fig. 1. We geometrically represent a positive semidefinite matrix by the ellipsoid generated by the transformation of a unit ball in the Euclidean space by the matrix. For simplicity of exposition, we refer to this ellipsoid as the'shape' of the matrix. We plot the shapes of \(M_{i}\) and \(W_{i+1}^{T}W_{i+1}\) in green and blue, respectively, in 2D. The positive definiteness of the constraint in (12) is equivalent to the ellipsoid of \(W_{i+1}^{T}W_{i+1}\) being contained in the ellipsoid corresponding to \(M_{i}/c_{i}\). Specifically, when \(c_{i}>1\), Fig. 0(a) demonstrates the maximum contraction of \(M_{i}\), corresponding to the largest \(c_{i}\), such that ellipsoid of \(W_{i+1}^{T}W_{i+1}\) is still contained in ellipsoid of \(c_{i}M_{i}\). Similarly, for the case where \(c_{i}<1\), Fig. 0(b) demonstrates the minimum extent (the smallest \(1/c_{i}\)) to which \(M_{i}\) needs to expand, such that the ellipsoid of \(W_{i+1}^{T}W_{i+1}\) is contained. Algebraically, in both cases, \(c_{i}\) is the ratio of the lengths of the green and pink arrows. By Proposition 2, the resulting ellipsoid (depicted in pink) is \(M_{i}/c_{i}=W_{i+1}^{T}W_{i+1}+N/c_{i}\) for both cases, and is tangent to the ellipsoid of \(W_{i+1}^{T}W_{i+1}\). Moreover, the vector pointing from the origin to the tangency point aligns with the direction of eigenvectors (the grey vector \(v\) in the plots) corresponding to the zero eigenvalues of the singular matrix \(N 0\).

Combining Proposition 2 and 3, we can derive an optimization problem to sequentially find the appropriate \(_{i}\), \(i_{l-1}\). The first constraint in (12) is quadratic in \(_{i}\), which makes it unattractive for practical purposes. Therefore, we apply the Schur Complement to transform it into the linear matrix inequality (LMI) constraint in (8). Thus, the optimization problem in Proposition 3 becomes equivalent to the SDP in (8), yielding algorithm **ECLipsE**. Notice that there are several ways to write the Schur complement of the constraint in (12). We choose this specific structure to avoid singularity of the diagonal entries and ensure positive definiteness.

**ECLipsE-Fast** achieves remarkable speed by further reducing \(_{i}\), \(i_{l-1}\) to a multiple of identity matrix \(_{i}I\), where \(_{i}>0\), and by relaxing the sub-problems. While our goal remains to minimize \(_{max}(_{i+1})=_{max}(W_{i+1}(_{i}- {4}_{i}_{i}_{i})^{-1}W_{i+1}^{T})\), we intentionally disregard information from \(W_{i+1}\), and instead focus solely on minimizing the spectral norm of \((_{i}-_{i}_{i}_{i})^{-1}\)

Figure 1: Geometric Analysis of **ECLipsE**

Roughly speaking, a smaller \(_{max}((_{i}-_{i}_{i}_{i})^{-1})\) yields a smaller \(_{max}(_{i+1})\). This relaxation allows us to derive a closed-form solution for \(_{i}\), \(i_{l-1}\) as follows.

**Proposition 4**.: _Choosing \(_{i}=(_{i})}>0\) minimizes \(_{max}((_{i}-_{i}_{i}_{i })^{-1})\) where \(_{i}=_{i}I\) under the constraint that \(M_{i}=_{i}-_{i}_{i}_{i}>0\). Moreover, this closed-form solution for \(_{i}\) always satisfies \(M_{i}>0\), \(i_{l-1}\)._

By the definition of \(_{i}\), Proposition 4 matches with (9), yielding algorithm **ECLipsE-Fast**. Although this relaxation may result in a loss of tightness, the closed-form solution offers the advantage of significantly increased computational speed.

_Geometric Analysis:_ We now demonstrate the geometric analysis behind the development of **ECLipsE-Fast** and compare it with **ECLipsE** in the case where \(c_{i}>1\) (Fig. 2). We also include the case \(c_{i}<1\) in Appendix B. The key idea behind **ECLipsE-Fast** is that instead of keeping the shape of \(M_{i}\) fixed, and contrasting the ellipsoid itself, as in **ECLipsE**, we first find the largest inscribed ball (dark green) for the ellipsoid of \(M_{i}\). Then, we contract this ball to the maximum extent such that it still contains \(W_{i+1}^{T}W_{i+1}\). The resulting ball (dark blue) is precisely the smallest circumscribing ball for the ellipsoid of \(W_{i+1}^{T}W_{i+1}\). Note that this approach serves as an approximation for the process of contraction depicted in Fig. (b)b (corresponding to **ECLipsE**), thus yielding a smaller \(c_{i}\). We use this approximation to achieve a closed-form solution, which significantly increases the computational speed.

_Remark 3_.: In Lemma 2, the analysis initially fixes the shape of \(_{i}\). However, when optimizing \(_{i}\), the shape of the feasible region depends on \(_{i}\), which can vary with different \(_{i-1}\), \(i_{l}\). Thus, this approximation, which allows for a scalable distributed algorithm to solve the centralized problem (3) introduces an unavoidable but minor tradeoff in achieving global optimality.

## 4 Experiments

We implement our algorithms++ on randomly generated neural networks and ones trained on the MNIST dataset. The details of the experimental setup, and training of the neural networks (both randomly generated and trained on the MNIST dataset) are described in Appendix D.

Footnote ‡: [https://github.com/YuezhuXu/ECLipsE](https://github.com/YuezhuXu/ECLipsE)

**Baselines.**++ For **ECLipsE**, \(_{i}\), \(i_{l-1}\) can have different diagonal entries, which benchmarks to LipSDP-Neuron. For **ECLipsE-Fast**, \(_{i}=_{i}I\), \(i_{l-1}\), which benchmarks to LipSDP-Layer. Additionally, we compare our Lipschitz estimates to the naive upper bound \(L_{naive}=_{i=1}^{l}\|W_{i}\|_{2}\), CPLip  and LipDiff . The codes for these baselines are available at . Note that LipDiff is accelerated using a node with 2 NVIDIA A100 GPUs (80G) and 512 GB of memory.

Footnote ‡: Note that SeqLip  is also an often-used benchmark; however, we do not consider it since it does not represent a true upper bound for the Lipschitz constant. We also note that we do not include Chordal-LipSDP  as a baseline, since only the case where \(=0\) in that work is valid, and all other cases, are no longer valid in certifying the Lipschitz constant as discussed in Remark 1 as well as .

### Randomly Generated Neural Networks

We first consider randomly generated networks, where the number of layers are chosen from \(\{2,5,10,20,30,50,75,100\}\), and number of neurons are chosen from \(\{20,40,60,80,100\}\), amounting to a total of 40 experiments for each algorithm (including the baselines). We quantify the computation time and tightness of the Lipschitz bounds (raw data in Appendix E). The Lipschitz bounds presented in the following figures are normalized to the trivial upper bound for ease of comparison.

Figure 2: Comparison between **ECLipsE-Fast** and **ECLipsE** with \(c_{i}>1\)

**Case 1: Varying network depth (number of layers).** We select a network with 80 neurons per layer, and demonstrate the scalability of our algorithm as network depth increases. Note that all baseline approaches fail to provide a Lipschitz estimate within a computational cutoff time of 15 min for networks larger than this size (see results in Appendix E). As the number of layers increases, the computation time for CPLip algorithm explodes (the algorithm does not return a Lipschitz estimate within the cutoff time beyond 20 layers); however, CPLip provides the most accurate estimates in smaller networks. LipDiff provides inadmissible Lipschitz estimates even for moderate networks, returning as much as 10-100 times the trivial bound (see Table (a)a, Appendix E for the estimates). Also, while LipDiff has similar computational time for smaller networks, computational time grows for deeper networks as recorded in Appendix E Table (b)b. Consequently, we do not include these results in the plots. LipSDP-Neuron and LipSDP-Layer are also scalable to some extent; however, they fail for a networks of 30 and 50 layers respectively. In contrast, the computation time for **ECLipsE** and **ECLipsE-Fast** stays low and grows only linearly with respect to the number of layers (Fig. (b)b). Notably, **ECLipsE-Fast** is significantly faster (thousands of times) than LipSDP-Layer, owing to the closed-form solution at each stage, while **ECLipsE** is also considerably faster than LipSDP-Neuron. The Lipschitz estimates given by algorithms **ECLipsE** and **ECLipsE-Fast** are very close to the ones from LipSDP-Neuron and LipSDP-Layer respectively (Fig. (a)a), and outperform the trivial bound. As the number of layers increases, the normalized Lipschitz estimates are smaller, indicating that our algorithms are well-suited to very deep networks.

**Case 2: Varying neural network width (number of neurons per layer).** We now examine the performance of our algorithms for wider (more hidden neurons per layer), rather than deeper networks (with more layers), and demonstrate the results for networks with 20 and 50 layers respectively (Fig. 4). While the complete raw data is presented in Appendix E, we discuss the results for 20 and 50 layer networks here, since they represent the network sizes where different baselines fail to return Lipschitz estimates beyond the computation cutoff time of 15 min. Note that while LipDiff also manages to generate estimates for all network sizes in our 50 layers case, it once again provides inadmissible Lipschitz constants, returning as much as \(10^{4}-10^{6}\) times the trivial bound. Therefore, we do not include these results in Fig. 4 (see Tables (a)a and (b)b in Appendix E for the estimates and computation time.) We can observe from Figs. (b)b and (b)b that the computation time needed for CPLip, LipSDP-Layer, and LipSDP-Neuron significantly increases with the number of neurons, while the computation time of our method still grows linearly. Meanwhile, the Lipschitz estimates from algorithms **ECLipsE** and **ECLipsE-Fast** are close to the ones from LipSDP-Neuron and LipSDP-Layer respectively (Figs. (a)a and (c)c). Thus, we can conclude that our method significantly improves scalability for wider neural networks.

**Case 3: Comparison with LipSDP implementations.** In order to address the scalability issue as the size of the network grows, LipSDP utilizes a splitting approach, where the network is split into smaller sub-networks and the Lipschitz constants for each sub-network are composed at the end to obtain the final estimate. We benchmark our approach with respect to the performance of LipSDP-Layer and LipSDP-Neuron considering different sub-network sizes. Note that _our algorithms do not require any splitting_, since they remain scalable to large networks. As the FNNs are larger than the ones in previous cases, we change the cutoff time to 30 minutes. We conduct two sets of experiments

Figure 3: Performance of **ECLipsE-Fast** and **ECLipsE**, with respect to baselines for increasing network depth, with 80 neurons per layer. The red x markings indicate that the algorithm fails to provide an estimate within the computational cutoff time beyond this network size.

to study how our algorithms perform on considerably deep neural networks and how network width affects these results.

In the first set of experiments, we consider FNNs with 100 layers, with the number of neurons chosen from the set {80,100,120,140,160}. The splitting sizes for LipSDP-Neuron and LipSDP-Layer are 3, 5 and 10. We represent different FNN sizes by shapes and different algorithms by the color in Fig. 5.

By plotting the normalized Lipschitz estimates and computation times on the two axes, we illustrate how efficient and accurate an algorithm is by how close the corresponding data point is to the origin. We observe that all the data points for **ECLipsE-Fast** are at the leftmost extreme of the plot, indicating that it is the most efficient algorithm. Further, **ECLipsE-Fast** also outperforms the red cluster (LipSDP-layer with the network split into 3) in both tightness and speed. Comparing data points of the same shape, **ECLipsE-Fast** outperforms LipSDP-Layer for all sub-network splits both in terms of the Lipschitz estimate and the computation time. Finally, the data points corresponding to **ECLipsE** are clustered at the bottom left, demonstrating that it is relatively more accurate and efficient than all LipSDP methods, no matter how the network is split.

In the second set of experiments, we explore even wider networks. Specifically, we choose a fairly deep neural network with 50 layers and vary the width from 150 to 1000. The splitting size for LipSDP-Neuron and LipSDP-Layer is 5. The resulting Lipschitz estimates (normalized with respect to trivial upper bounds) and the computation time are provided in Tables 3(a) and 3(b) of Appendix

Figure 4: Performance of **ECLipsE-Fast** and **ECLipsE** with respect to baselines as network width increases, for a randomly generated network with 20 layers ((a) and (b)) and 50 layers ((c) and (d)). The Red x markings indicate that the algorithms fail to provide an estimate within the computational cutoff time of 15 min beyond this network size.

Figure 5: Computation time vs estimation accuracy for **ECLipsE**, **ECLipsE-Fast** and LipSDP splitting with different sub-network sizes.

E due to space limitations. From these results, we observe that **ECLipsE-Fast** is extremely fast even for very wide networks, with a running time of only 15.63 seconds for a network width of 1000, while the computation time for LipSDP-Layer grows significantly. Also, while **ECLipsE** fails when the width reaches 300, it is comparable to LipSDP-Neuron split into 5 sub-networks in terms of time performance.

_Remark_ 4. We notice that when the neural networks are significantly wide, **ECLipsE** takes more than 30 minutes while **ECLipsE-Fast** remains efficient. This observation can be explained by examining the computational complexity of these algorithms. Note that we directly state the computational complexity of each algorithm here for brevity; the detailed derivations are included in Appendix C. Suppose a neural network has \(n\) hidden layers with \(m\) neurons. Then, the computational cost for LipSDP and **ECLipsE** are \(O(n^{4}m^{4})\) and \(O(nm^{4})\) respectively. We can observe that the complexity is significantly decreased in terms of the depth, but is the same in terms of the width, immediately indicating the advantage for deep networks. Nevertheless, as \(m\) grows, the difference between \(O(n^{4}m^{4})\) and \(O(nm^{4})\) is still drastically enhanced, especially with large \(n\). More importantly, for **ECLipsE-Fast**, the computational cost drops to \(O(nm^{3})\). This is the fastest one can expect if the weights on each layer are treated as a whole.

### Neural Networks Trained on MNIST.

We now demonstrate our algorithms on four networks trained on the MNIST dataset (see Appendix D for details) to achieve an accuracy of at least 97%. The resulting networks are not very deep (3 layers), with 100, 200, 300, and 400 neurons. We set a computational cutoff time of 30 min to obtain Lipschitz estimates. As described in the note on Baselines earlier in this section, **ECLipsE** is benchmarked against LipSDP-Neuron and **ECLipsE-Fast** is benchmarked against the faster LipSDP-Layer due to their mathematical structure. From Fig. 5(b), we can see that **ECLipsE-Fast** is significantly faster than LipSDP-Layer, while **ECLipsE** is also considerably faster than LipSDP-Neuron. Note that all algorithms provide very similar Lipschitz estimates (Fig. 5(a)). Therefore, for networks that are not very deep, such as those in this example, **ECLipsE-Fast** is the optimal choice, since it significantly outperforms all algorithms in terms of speed, while the approximation error due to the closed-form solution is not too significant compared to the baselines.

## 5 Conclusion

We propose a scalable approach to estimate Lipschitz constants for deep neural networks by developing a new matrix decomposition that yields two fast algorithms. Our experiments demonstrate that our algorithms significantly outperform the state-of-the-art in terms of computation speed, while providing comparable Lipschitz estimates. We envision that further computational speedup can be achieved through sparse matrix multiplication and eigenvalue estimation techniques, and leveraging autodiff frameworks, along the lines of . While we can unroll the convolutional layers in CNN structure to a large fully connected neural network layer to apply **ECLipsE** and **ECLipsE-Fast** to estimate Lipschitz constant, better compositional methods that are tailored to feature the convolutional layers are expected for future work. Similarly, other architectures, such as residual networks, present additional challenges due to their unique structures and will be considered in future research.

Figure 6: Performance of **ECLipsE-Fast** and **ECLipsE**, with respect to baselines for increasing number of neurons, for a 3-layer network trained on MNIST. The red x markings indicate that the algorithm fails to provide an estimate within the computational cutoff time beyond this network size.