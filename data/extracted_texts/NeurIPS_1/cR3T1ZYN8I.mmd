# A Hitchhiker's Guide to Fine-Grained Face Forgery Detection Using Common Sense Reasoning

Niki Maria Foteinopoulou\({}^{1}\) Enjie Ghorbel\({}^{1,2}\) Djamila Aouada\({}^{1}\)

\({}^{1}\)CVI\({}^{2}\), SnT, University of Luxembourg

\({}^{2}\)Cristal Laboratory, National School of Computer Sciences, University of Manouba

{niki.foteinopoulou, enjie.ghorbel, djamila.aouada}@uni.lu

###### Abstract

Explainability in artificial intelligence is crucial for restoring trust, particularly in areas like face forgery detection, where viewers often struggle to distinguish between real and fabricated content. Vision and Large Language Models (VLLM) bridge computer vision and natural language, offering numerous applications driven by strong common-sense reasoning. Despite their success in various tasks, the potential of vision and language remains underexplored in face forgery detection, where they hold promise for enhancing explainability by leveraging the intrinsic reasoning capabilities of language to analyse fine-grained manipulation areas. For that reason, few works have recently started to frame the problem of deepfake detection as a Visual Question Answering (VQA) task, nevertheless omitting the realistic and informative multi-label setting. With the rapid advances in the field of VLLM, an exponential rise of investigations in that direction is expected. As such, there is a need for a clear experimental methodology that converts face forgery detection to a Visual Question Answering (VQA) task to systematically and fairly evaluate different VLLM architectures. Previous evaluation studies in deepfake detection have mostly focused on the simpler binary task, overlooking evaluation protocols for multi-label fine-grained detection and text-generative models. We propose a multi-staged approach that diverges from the traditional binary evaluation protocol and conducts a comprehensive evaluation study to compare the capabilities of several VLLMs in this context. In the first stage, we assess the models' performance on the binary task and their sensitivity to given instructions using several prompts. In the second stage, we delve deeper into fine-grained detection by identifying areas of manipulation in a multiple-choice VQA setting. In the third stage, we convert the fine-grained detection to an open-ended question and compare several matching strategies for the multi-label classification task. Finally, we qualitatively evaluate the fine-grained responses of the VLLMs included in the benchmark. We apply our benchmark to several popular models, providing a detailed comparison of binary, multiple-choice, and open-ended VQA evaluation across seven datasets. [https://nickyfoot.github.io/hitchhikerseguide.github.io/](https://nickyfoot.github.io/hitchhikerseguide.github.io/)

## 1 Introduction

Recent developments in deep generative modelling have resulted in hyper-realistic synthetic images/videos with no clear visible artefacts, making the viewers question whether they can still trust their eyes. Unfortunately, despite its relevance in a wide range of applications, such technology poses a threat to society as it can be used for malicious activities . In a world where synthetic images of a person, known as deepfakes, can easily be generated, it becomes crucial to fight misinformation not only by identifying manipulated images/videos in an automated manner but also by explaining the decision behind this classification to reinstate trust in Artificial Intelligence.

Numerous successful works on deepfake detection have been proposed in the literature to tackle the risks of face forgery . Existing methods primarily rely on deep binary classifiers, resulting in black-box models that predict whether an input is real or fake. Consequently, explaining why those predictions are being made is not straightforward. To handle this issue, a few interpretable deepfake detection methods have been introduced by examining attention maps or weight activations  to identify fine-grained areas of manipulation; however, these are based on a post-hoc analysis and thereby do not intrinsically incorporate an explainable mechanism. On the other hand, Vision Large Language Models (VLLMs) have emerged as a pioneering branch of generative Artificial Intelligence (AI), showcasing advancements in common sense reasoning and an inherent explainability that arises from the intrinsic nature of language . They have demonstrated impressive capabilities in tasks such as Visual Question Answering (VQA)  and the generation of descriptive content for downstream applications , hence bridging the gap between vision-language understanding and contextual reasoning. However, despite these achievements, the explainable power of VLLMs remains under-explored in the field of deepfake detection, with only a handful of works mostly exploring the vision and language capabilities for the binary classification of fake/real images  all of which are evaluated on different benchmarks and metrics. To the best of our knowledge, Zhang _et al_.  is the only work employing a VQA approach in deepfake detection by proposing to extend the FF++ dataset with captions in natural language generated by humans. However, this work targets only one manipulated region at once, while deepfakes can incorporate several stacked manipulations . Moreover, the provided augmented FF++ dataset does not allow for cross-dataset evaluation in a VQA setting without an extensive annotation effort, making it difficult to investigate the generalisation aspect. In addition, the fine-grained evaluation in previous works is limited as the more challenging open-ended VQA task is not explored.

Explainable fine-grained detection -that is, identifying manipulation beyond the binary fake/real decision- in natural language is still in its infancy. However, as VLLM works for deepfake detection are expected to appear following the overwhelming success of foundation models in other tasks, two research questions need to be addressed: 1) _"To what extent can existing VLLMs detect deepfake images and what rationale supports the decision?"_ and 2) _"How do we fairly and comprehensively evaluate VLLMs in the fine-grained task?"_. In deepfake detection, benchmarks have mainly focused on binary or multi-class decisions and discriminative networks , making them unsuitable to answer these research questions. Indeed, they do not propose a unified method to match the generated responses to fine-grained multi-label categories. Similarly, existing benchmarks in Visual Question Answering (VQA)  primarily address multi-class tasks, which may not be suitable for the multi-label nature of fine-grained deepfake detection as highlighted in .

In this work, our objective is to conduct a thorough quantitative and qualitative evaluation of VLLMs for the task of fine-grained multi-label deepfake detection in a systematic and scientific approach, employing a multi-stage protocol without costly human captioning efforts. In the first stage, we assess the models' performance on the binary task using various prompts while also evaluating the model's sensitivity to the provided instructions. In the second stage, we delve into multi-label fine-grained detection, aiming to identify areas of manipulation within a multiple-choice Visual Question Answering (VQA) framework, i.e. what areas from a given list are identified as manipulated. Subsequently, in the third stage, we extend our investigation by converting the fine-grained detection task into an open-ended question -that is, identifying areas without instructing the model to select from a list of categories. Here, as the task is a multi-label problem, we compare two matching strategies: a) using the cosine similarity between the generated text and ground truth labels and b) counting the occurrence of the class name in the generated text. Finally, we qualitatively evaluate the fine-grained responses generated by the VLLMs included in our benchmark, providing nuanced and new insights into their performance.

The main contributions of this work can be thus summarised as follows:

* We introduce a novel evaluation protocol for deepfake detection under the Visual Question Answering (VQA) **multi-label setting** and without the use of human annotations. This is different from  that is based on a succession of yes/no questions for fine-grained areas, resulting in a binary classification setting and relies on a relatively small dataset which cannot be extended without costly annotation efforts. In addition, our multi-stage protocol allows for open-ended VQA evaluation, which is a more challenging task. To the best of our knowledge, this is the first work to do so in the field of deepfake detection, offering a fresh perspective on explainability through fine-grained multi-label analysis.
* We present a systematic, unified evaluation study of current state-of-the-art (SOTA) VLLMs, facilitating consistent assessment across different models. This framework is designed to be extendable to any existing or future deepfake dataset, ensuring fair and comprehensive comparisons with future models, thus promoting transparency and reproducibility in the evaluation process.
* Through extensive comparison and analysis of the tested models and an ensemble of models, our study yields new insights into the capabilities and limitations of VLLMs in the context of deepfake detection. We will use these insights to advance research in the domain and hope to inform future developments and optimisations in model design and evaluation strategies.

## 2 Related Work

**DeepFake generation** encompasses various forms of facial manipulation, including face reenactment , face swapping , and entirely generated face images . **Deepfake detection** algorithms classify samples as real or fake , relying on artefacts left by manipulation methods, often analysed qualitatively for explainability . However, this qualitative analysis happens on a secondary stage and primarily depends on human observers. While generative methods often use natural language instructions , explaining manipulations in natural language -a natural extension of the generation process to detection- is still an emerging area.

With the rise of VLLMs, recent works  explore vision and language for face forgery detection, primarily focusing on binary detection in a retrieval setting, with fewer  examining fine-grained areas usually as a secondary task. The latter have relied on generated pseudo-fake datasets to improve generalisation , which have a major drawback -that is, the use of pseudo-fake datasets hampers fair comparisons and does not reflect the current state-of-the-art in deepfake generation.

Several **VLLMs** foundation models , bridge the gap between vision and language. These are typically trained on very large datasets with general knowledge. As the computational and data resources needed to train VLLMs from scratch are very high, numerous works leverage the pre-trained networks in three main directions: a) exploring the latent feature space  of vision and language, b) parameter efficient training in a downstream task  and c) evaluating foundation models in new domains .

**Benchmarks** for classification tasks  in a VQA setting typically address the multi-class paradigm, which may not be appropriate for addressing explainability in DeepFake detection by adopting a multi-label fine-grained strategy, as several areas can be manipulated at once. A few preliminary works in DeepFake detection  benchmark ChatGPT41 and Gemini2; however, these have primarily focused on the more straightforward binary task and did not explore the reasoning capabilities of VLLMs for fine-grained labels. Furthermore, both these works focused on VLLMs that are not open-sourced with limited information regarding their training set and architecture; thus, it is not possible to assess whether the benchmarks are, in fact, zero-shot or whether they have been trained on deepfake-related image-language pairs. Zhang _et al_.  propose extending FF++ annotations with captions in natural language using human annotators. However, this method is limited to binary decisions, while a given deepfake image can undergo several manipulations. Furthermore, it does not explore the open-ended VQA setting and does not offer a method for cross-dataset evaluation without a costly annotation process. Within DeepFake detection, the vast majority of benchmarking works  have focused on binary discriminative networks and are therefore not fit to evaluate the capabilities of generative models such as VLLM, particularly for fine-grained labels.

In a nutshell, the main novelty of this benchmark compared to previous works  is threefold: 1) it converts the multi-label classification task of face forgery detection to a VQA task so that VLLM's common sense reasoning capabilities can be evaluated, 2) it systematically and consistently assesses VLLM capabilities on nine binary and three fine-grained benchmarks and 3) is offering an open source and extendable framework for future zero-shot or task-specific VLLMs, that ensures a fair comparison.

## 3 Common Sense Reasoning for Face Forgery Detection

**Preliminaries:** We formalise the language generation process of VLLM architectures, akin to standard VQA models, where the model is prompted with an image and a query to produce an auto-regressive answer. Given an image \(_{v}^{H W C}\) and a text prompt \(_{t}^{L d}\) as input, a sentence \(\) is generated represented as a sequence of word tokens. The generation can be represented by the function \(p(|_{v},_{t})=_{j=0}^{||}p(_{j}|_{<j},_{v},_{t})\), where \(H W C\) represent the image dimension, \(L\) is the number of tokens, \(d\) is the embedding dimension, \(=()_{0 j<||}\) is the generated sentence, and \(|.|\) the cardinality. In VQA tasks, the model response aims to match human annotations. This task differs from typical classification problems due to the diverse semantic nature of questions and answers in natural language. The evaluation protocol is outlined for the binary case in Section 3.1 and for open-ended evaluation and multiple-choice of fine-grained labels in Section 3.2. An overview of the proposed method is shown in Figure 1.

### Binary Classification to VQA

In binary classification, the task is to predict whether the image sample is a product of face forgery. We create a benchmark to assess VLLM capability in binary Deepfake detection by transforming the discriminative task into a VQA problem. We consider only the positive category for each image \(_{v}\) to generate the relevant instruction; that is, we limit the prompt to identifying whether an image is a Deepfake and not whether it is a genuine sample. The prompt used is in the form:

\[_{t}=[s_{i}] \]

where \(_{i}\) denotes a set of standard terms used to describe deep fakes in the English language. The synonyms are employed to assess the reasoning capabilities of each tested model by investigating whether the understanding of the model is robust to the given instruction.

Figure 1: Overview of the proposed benchmarking method, using multiple stages to evaluate the performance of VLLMs in the context of deepfake detection. In the first stage (a), we assess the binary classification performance of VLLMs. In the second stage (b), we perform a fine-grained classification using multiple-choice instruction. In the third and final stage (c), we ask the model to identify fine-grained areas in open-ended VQA. The image example 3is a sample from the SeqDeepFake dataset , and responses are generated using Llava-1.5 

### Fine-Grained Labels:

Fine-grained labels typically refer to manipulation areas. Predicting them necessitates the use of multi-label classification, as multiple areas can be manipulated at once. Following the initial binary prompt, a follow-up prompt to explain what areas of manipulation are identified is given to the VLLM with the same image, as shown in Fig. 1. We propose using two versions of follow-up prompts, one as an open-ended question and one as a multiple-choice. Specifically, the open-ended follow-up prompt follows the template:

\[_{t}=[_{i}]\  \]

For the multiple-choice instruction, we follow the template:

\[_{t}=[_{0},,_{||}],\ [_{i}]\  \]

where \(_{i}\) is the class name of the \(i\)-th class from the set of target classes \(\).

### Matching Strategies:

To evaluate the generated responses, we employ several matching methods depending on the task. The stricter one uses an _Exact Match (EM)_ approach, that estimates whether the generated sentence \(\) is exactly equal to the class name \(cls_{i}\):

\[p(})=1&_{i}\\ 0&_{i} \]

where \(}\) is the prediction for the \(i\)-th class. In the given task, an answer is considered correct only if the model output exactly matches the class names or 'Yes/'No' in the binary case. As the responses tend to be longer for fine-grained classification and reflect reasoning in natural language for a multi-label problem, a more appropriate matching strategy is to consider a response correct if the class name is _Contained_ in the response, as proposed by Xu _et al._ - that is \(p(})=1,_{i}\) and \(0\) otherwise. We extend this to include synonyms of class names, as several ways exist to describe some areas (e.g. "Bangs" could also be described as "Hair"). Finally, we propose adapting the text-to-text score (_CLIP distance_) proposed by Conti _et al._ for the multi-label task. This is done by using a sigmoid function over the cosine similarity matrix of the prediction embeddings and class name embeddings (obtained with a CLIP  text encoder), using an empirical temperature \(t\) of \(0.5\) so that \(p(})=(<,_{i}>)\). The symbol \(<.,.>\) denotes the cosine similarity of the text embeddings and \((.)\) is the sigmoid function.

## 4 Experimental Set-Up

### Tested VLLMs

We select four open-source state-of-the-art VLLMs to be included in this benchmark; specifically, we include L1aVa-1.5  (an improved version of the L1aVa architecture ), BLIP2  and finally InstructBlip  with Flan-T5 and Flan-T5-xxl language generators . Finally, for the binary task, we include the CLIP  performance as a baseline and compare it against an ensemble of BLIP2  and L1aVa-1.5  following the ensembling strategies for VQA tasks , and GPT4v as an upper bound4. Experiments using GPT4v are performed on a subset of 5k samples selected from each dataset, and thus, the results may be susceptible to some degree of bias from the sampling, which needs to be considered when comparing the models. The selection of the VLLM is guided by three factors. First, we select architectures with publicly available weights and training methods to ensure transparency and fairness in the evaluation. Second, we include methods that generate output in Natural Language rather than a set of features or classification predictions. Finally, we select methods that have achieved state-of-the-art performance on several zero-shot tasks. Additional model details, such as the number of parameters and pre-training information of the tested models, can be found in Appendix A.

### Datasets

We evaluate the performance of our method on seven published challenging benchmarks and one pseudo-fake dataset; more specifically, seven datasets for binary detection and two for the fine-grained task. All are evaluated at the frame level, as in previous image works [80; 52; 49]. **FF++:** consists of over 20k images of DeepFake images from 1000 videos, using four types of manipulation methods: Deepfakes, Face2Face, FaceSwap and NeuralTextures. The dataset is split into train, validation, and test with an 80%, 10%, and 10% split, respectively. **DFDC:** is composed of 5k videos of real and manipulated faces split into 4,464 unique training clips and 780 unique test clips. **Celeb-DF:** includes 590 original videos collected from YouTube with subjects of different ages, ethnic groups and genders, and 5639 corresponding manipulated videos. **WildDeepFake:** is a challenging dataset for in-the-wild detection, which consists of 7,314 face sequences extracted from 707 videos that are collected completely from the internet. **StyleGAN:** Two sub-sets consist of curated images generated with StyleGAN3  and StyleGAN2  along with original ones for binary detection of facial images. **SeqDeepFake:** dataset consists of 85k sequential manipulated face images based on two representative facial manipulation techniques, facial components manipulation  and facial attributes manipulation . The labels include annotations of manipulation sequences with diverse sequence lengths. **R-splicer:** Augmenting real data by generating pseudo-fake images is a common practice in deepfake detection [36; 49; 11; 66; 89; 40]. Such methods simulate characteristic face-swap artefacts using simplistic operations on a predefined set of regions. In this work, we use a spliced dataset of 59k images to evaluate fine-grained labels of five regions -entire face, mouth, nose, eyes, eyebrows- as implemented by Mejri _et al_. .

### Metrics

**Accuracy** and the Area Under the Receiver Operating Characteristics (ROC) Curve (**AUC**) are the most common metrics used in DeepFake detection [82; 52; 65]. However, as the datasets in the task are massively imbalanced, we also use the harmonic mean of Precision and Recall (**F1-score**) for the binary task. Furthermore, we note that as AUC is a measure of the classifier's performance at different thresholds, it has very limited value in the VQA task where matching strategies result in polarised decisions; however, we include it for reference. In the fine-grained task, we use mean Average Precision (**mAP**), AUC and F1-score as indicators of classification performance.

## 5 Results

### Binary Classification

**Robustness to different prompts:** We use seven synonyms for the positive class: "manipulated", "deepfake", "synthetic", "altered", "fabricated", "face forgery" and "falsified". As the binary task is simple and the instruction format is a 'Yes' or 'No' question, we use _EM_ as a matching approach in this evaluation. In Fig. 2, we see the performance of each tested model under the binary detection setting on the two sub-sets of SeqDeepFake  and the R-splicer dataset using the three best performing synonyms: "manipulated", "synthetic" and "altered". The first observation is that no VLLM clearly outperforms others across all datasets and metrics. However, we see that BLIP-2  has the most robust performance to the given instruction, even though it is the smallest in terms of parameters. Furthermore, the additional parameters of T5-xxl  do not seem to aid the task compared to the base InstructBLIP  with T5 generator, as the base model performs comparably better across most benchmarks. We theorise that as the VLLMs have not been explicitly trained on image-language pairs of manipulated images, a large number of parameters on the language generation leads to more hallucinations [25; 79] for this simple but abstract task. Compared to the CLIP , models appear to have competitive performance with the exception of InstructBLIP with T5xxl LLM. When both base models, i.e. BLIP-2  and LlaVa , have relatively good performance, the ensemble shows marginal improvement, particularly in terms of Accuracy and F1; however, this is not consistent therefore we do not continue the investigation to fine-grained labels. The detailed performance of all models and synonyms across all datasets and additional analysis on CLIP  features can be found in the Appendix.

**Overall model performance:** We average the performance of the three best-performing synonyms on all nine benchmarks in Fig. 3. No model clearly outperforms others across all metrics anddatasets; however, we can observe competitive performance from BLIP-2  on the binary task, even though it is the smallest model in terms of parameters. We also see that all models struggle with the more challenging in-the-wild datasets, such as CelebDF , which highlights the need for further development to achieve adequate generalisation. Performance of GPT4v should be treated as an upper bound as we cannot assess whether the model has been trained on samples of the selected datasets. We see that GPT4v vastly outperforms the selected VLLMs on three benchmarks and has comparable performance on the rest, with the exception of FF++.

**Vision Encoder Finetuning:** We finetune contrastively the vision encoder of LlaVa on FF++ using a sigmoid loss  over an ensemble of prompts for the real/fake categories, and evaluate as described in the previous section. Training details for the vision encoder can be found in Appendix C. The

Figure 3: Exact Match _(EM)_ Performance of each VLLM on all nine benchmarks

Figure 2: Exact Match _(EM)_ Performance of each VLLM in terms of Accuracy (top), AUC (mid) and F1-score (bottom) for the top 3 synonyms “manipulated”, “synthetic” and “altered”

architecture with the fine-tuned vision encoder shows improved within dataset and cross-dataset performance as shown in Tab. 1. Even without detailed captions or updating the LLM weights, we see there are still gains from a task specific vision encoder, particularly in terms of F1-score with an average improvement of nearly \(4\%\) within dataset and nearly \(2\%\) cross-dataset (for SeqDeepFake, CelebDF and StyleGAN2).

**Metrics:** In terms of the selected metrics, following the initial intuition, there is limited information we can get from the standard Accuracy and AUC used in the binary task. Both are heavily skewed by the label distribution, which is typically imbalanced in deepfake datasets; however, the latter may also not be fit for VLLMs as AUC measures performance at different thresholds, which are not present with _EM_ and _contains_ matching strategies. As such, we argue that for the task at hand, the F1-score -and consequently robust to imbalance metrics- are more appropriate.

### Finegrained Evaluation

For the fine-grained task, we evaluate the performance of the selected models in the open and closed vocabulary settings. The fine-grained labels are evaluated on samples where the ground truth is positive - i.e., on DeepFake samples.

**Open-Ended VQA:** We first evaluate the selected VLLMs under the open vocabulary VQA setting on the three fine-grained datasets. The results using _contains_ and _CLIP distance_ matching are reported in Tab. 1(a) and Tab. 1(b) respectively. An _EM_ strategy is not possible in multi-label tasks, so no such evaluation is performed. No model clearly outperforms others across all metrics and datasets. In fact, we can observe that, in most cases, they have comparable performance. This holds true for both _contains_ and _CLIP distance_ metrics. In terms of matching strategy, using the _CLIP distance_ consistently and greatly improves recall, as is evident by the improvement in the F1-score and explicitly shown in Appendix H. This matching approach slightly lowers the mAP and AUC scores compared to the _contains_ metrics; however, using the cosine distance to match the open-ended responses to the class categories semantically may offer a more reliable output for the class of interest, as seen by the F1-score.

**Multiple choice VQA:** The performance of the VLLMs on the multiple-choice instruction is shown in Fig. 4. Even though the open-ended setting is theoretically more challenging, the performances of all tested models are comparable to each other and worse on the multiple-choice instruction for both mAP and AUC. Regarding the F1-score, however, LlaVa  consistently performs better than other models. Under the multiple-choice setting, we observe that the models tend to mention all label names, which raises the number of False Positives -a significant limitation of the multiple choice setting- or respond with "All of them" or "None of them", which makes matching of any sort more challenging and is reflected even more in the lower F1 score. Appendix G includes detailed metrics for each category.

    &  &  \\  & Acc. & F1 & Acc. & F1 \\  FF++ & 64.54 & 73.10 & 64.57 (+0.03\%) & 76.83 (+3.73\%) \\  SeqDeepfake Attr. & 58.92 & 66.15 & 61.22 (+2.30\%) & 68.03 (+1.88\%) \\ SeqDeepfake Comp. & 84.62 & 90.57 & 84.24 (-0.37\%) & 90.20 (-0.38\%) \\ R-Splicer & 87.11 & 92.50 & 87.31 (+0.20\%) & 92.62 (+0.12\%) \\ DFDC & 54.02 & 58.24 & 53.86 (-0.16\%) & 58.65 (+0.41\%) \\ CelebDF & 35.67 & 41.81 & 37.60 (+1.93\%) & 43.53 (+1.72\%) \\ DFW & 53.35 & 61.56 & 53.45 (+0.10\%) & 61.90 (+0.34\%) \\ StyleGAN2 & 33.67 & 38.62 & 35.20 (+1.53\%) & 39.72 (+1.10\%) \\ StyleGAN3 & 39.80 & 45.66 & 39.30 (-0.50\%) & 45.74 (+0.08\%) \\   

Table 1: Binary performance of LlaVa-1.5  with a fine-tuned Vision Encoder against the zero-shot baseline.

### Qualitative Evaluation

As the BertScore  is shown to correlate with human evaluation, we first present the Bertscore precision, recall, and F1 scores achieved by each model for the fine-grained open-ended responses compared with ground truth references that have been formatted using the prompt: "The areas that are \([_{i}]\) are \([ls_{i}]\)". The results of this evaluation, along with the score of human annotators  on a subset of the R-Splicer dataset, are shown in Tab. 3. As in previous sections, no model clearly outperforms others across all benchmarks; however, we see that Llava-1.5  has the most competitive performance for most benchmarks, closely followed by InstructBLIP . This is consistent with qualitative evaluations on VQA tasks .

**Overall performance:** In the simpler binary setting, BLIP-2 is more robust to instruction than other models with more parameters; however, when it comes to fine-grained evaluation, larger models show an advantage in reasoning and identifying areas of manipulation in the open-ended and multiple-choice settings. It is, however, worth noting that no model clearly outperforms others across all metrics and datasets. All of the results presented are based on zero-shot evaluations, where models are tested without being specifically trained for deepfake detection. Despite this, the models are able to leverage a semantic mapping between language and visual input from their very vast pre-training, giving them an inherent concept of "real" versus "fake." This capability suggests that these models possess some degree of understanding when it comes to identifying deepfakes. However, this general understanding falls far behind that of task-specific models. When we fine-tune the vision encoder, there is a notable improvement in performance. The vision-language models can

Table 2: Model performance on open-ended fine-grained detection using a) _contains_ and b _CLIP_ matching

Figure 4: Assessment of model performance in multiple-choice settings, in terms of a) mAP, b) AUC and c) F1 during multiple-choice evaluation with _contains_ matching.

better capture details and nuances in the input data, which enhances their deepfake detection capabilities. Nevertheless, due to the scarcity of high-quality captions and large-scale vision-language datasets tailored to deepfake detection, the improvements remain limited and only in the binary task. Overall, addressing these limitationsby creating specialised datasets and foundation modelscould lead to substantial advancements in this area.

**Limitations and Future Work:** As the models in this work are all evaluated under zero-shot settings, their performance is below that of purpose-build networks seen in previous works , particularly for more challenging in-the-wild datasets. This further highlights the need for task specific models and more fine-grained deepfake datasets, which is a key finding of the experiments conducted in this work. A significant limitation is the lack of detailed language descriptions in datasets, making qualitative evaluation harder. Additionally, current datasets lack fine-grained labels, restricting assessments of manipulations to pseudo-fakes and SeqDeepFake . Furthermore, as both the pertaining and evaluation datasets are not unbiased, the performance of all VLLMs is susceptible to the bias of the datasets, which is not addressed in this or previous benchmarks . Identifying these shortcomings is important for future works on the task, particularly as VLLMs gain traction.

## 6 Conclusion

In conclusion, our proposed benchmark has several contributions; first and foremost, we propose a method to transform deepfake detection into a VQA problem beyond binary classification to leverage common sense reasoning as an inherent explainability mechanism. We show how this can be achieved in both a multiple-choice and open-ended VQA -with the latter being the most important use-case for new and unknown face forgery methods. This approach is used to evaluate a multi-label problem that is not typical of classic VQA. By doing so, we can systematically and consistently evaluate the common sense reasoning capabilities of current and future VLLMs in fine-grained deepfake detection.

Our selection of metrics and matching strategies allows for a fair evaluation of the proposed task. In particular, we include metrics that are robust to imbalance in both the binary and multi-label fine-grained tasks. Even though VLLMs in a zero-shot evaluation do not outperform purpose-built methods, the generated responses include reasoning, therefore holding promise for significant contributions in explainable deepfake detection, confirming the initial motivation behind examining the use of such models for the task and understanding the current capabilities. Moreover, as this benchmark can be extended in terms of models and datasets, it allows for a systematic and fair comparison of new language generation methods for explainable deepfake detection.

**Ethics statement:** The authors of this paper acknowledge the crucial role of ethical considerations in AI research and development. Our dedication lies in upholding principles of fairness and impartiality. Recognising the societal implications of generative technology (including VLLMs), we commit to transparency by openly communicating our findings and advancements with the research community.

Table 3: Open-ended qualitative evaluation with human annotators in Tab. a and BertScore  in Tab. b- d