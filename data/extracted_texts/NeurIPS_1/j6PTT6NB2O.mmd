# Stress-Testing Long-Context Language Models

with Lifelong ICL and Task Haystack

 Xiaoyue Xu

Tsinghua University

xiaoyue.xu.me@gmail.com &Qinyuan Ye

University of Southern California

qingyuany@usc.edu &Xiang Ren

University of Southern California

xiangren@usc.edu

Equal Contribution.

###### Abstract

We introduce Lifelong ICL, a problem setting that challenges long-context language models (LMs) to learn a sequence of language tasks through in-context learning (ICL). We further introduce Task Haystack, an evaluation suite dedicated to assessing and diagnosing how long-context LMs utilizes contexts in Lifelong ICL. When given a task instruction and test inputs, long-context LMs are expected to leverage the relevant demonstrations in the Lifelong ICL prompt, avoid distraction and interference from other tasks, and achieve test accuracies that are not significantly worse than those of the Single-task ICL baseline.

Task Haystack draws inspiration from the widely-adopted "needle-in-a-haystack" (NIAH) evaluation, but presents distinct new challenges. It requires models (1) to utilize the contexts at a deeper level, rather than resorting to simple copying and pasting; (2) to navigate through long streams of evolving topics and tasks, proxying the complexities and dynamism of contexts in real-world scenarios. Additionally, Task Haystack inherits the controllability of NIAH, providing model developers with tools and visualizations to identify model vulnerabilities effectively.

We benchmark 14 long-context LMs using Task Haystack, finding that frontier models like GPT-4o still struggle with the setting, failing on 15% of cases on average. Most open-weight models further lack behind by a large margin, with failure rates reaching up to 61%. In our controlled analysis, we identify factors such as distraction and recency bias as contributors to these failure cases. Further, performance declines when task instructions are paraphrased at test time or when ICL demonstrations are repeated excessively, raising concerns about the robustness, instruction understanding, and true context utilization of long-context LMs. We release our code and data to encourage future research that investigates and addresses these limitations.1

## 1 Introduction

Recent advances in model architecture , hardware-aware optimization , training procedure , and data engineering  have enabled large language models (LLMs) to handle extended contexts, reaching up to 32 thousand tokens or even millions . These advancements have opened up new opportunities and potential usecases for LLMs. However, while long-context LM development strides forward, effective evaluation methods have not kept pace. Systematically evaluating long-context LMs' ability to leverage such long contexts remains an open challenge.

Current evaluation approaches fall into two major categories. The first involves constructing benchmarks with real-world long-context tasks . While valuable, creating these benchmarks is time-consuming and particularly challenging when scaling the input context length to millions of tokens. The second approach employs synthetic evaluations like the "needle-in-a-haystack" (NIAH) test  or key-value retrieval tests . For example, in the NIAH evaluation, a piece of information ("The special magic number is 12345") is planted in a haystack of irrelevant contexts (Paul Graham essays; Graham 2024) and the model is evaluated on answering a question about the information ("What's the special magic number?"). Although useful for initial assessment, these tests primarily measure simple copying-and-pasting capabilities and fail to capture whether models are able to utilize the context at a deeper level.

In this work, we offer new perspectives to long-context LM evaluation by introducing Lifelong ICL, a new problem setting that challenges these models to learn a sequence of tasks via in-context learning (ICL). Further, we introduce Task Haystack, an accompanying evaluation suite designed for systematic diagnosis of context utilization (Fig. 1). In Task Haystack, a long-context LM will be evaluated on a collection of tasks, with Lifelong ICL prompts and Single-task ICL prompts respectively. A model "passes" the test if its accuracies with Lifelong ICL prompts are not significantly lower than when using Single-task ICL prompts. The overall pass rate, averaged across tasks and different lifelong stream permutations, serves as the key metric of Task Haystack.

Task Haystack presents unique challenges not fully covered by existing benchmarks. Firstly, Task Haystack requires deeper understanding of the relevant context for accurate predictions. This goes beyond simple retrieval capabilities tested by NIAH-style benchmarks, which often rely on basic copying and pasting. Secondly, Task Haystack features high information density, meaning that every piece of information in the context might be crucial for successful prediction at test time. This differs from evaluation suites in which the important information ("needle") is positioned conspicuously, allowing models to exploit shortcuts . Thirdly, existing benchmarks fall short in capturing the dynamics of shifting topics within the context , which can pose challenges in real-world applications of long-context models--such as a 24/7 personal assistant that must resume previous conversations amid a long, evolving stream of topics. While not fully realistic, Task Haystack serves as a useful proxy for evaluating this aspect.

We extensively evaluate 14 long-context models on Task Haystack. While all models achieve near-perfect scores on the original NIAH test, none reach satisfactory performance on our proposed evaluation. Among the compared models, GPT-4o and Gemini-1.5-Flash lead with an average pass rate of 85%, significantly outperforming most open-weight models. Llama-3.1-70B, the best-performing open-weight model, follows closely with an average pass rate of 80%. To understand the root causes behind these failure cases, we conduct controlled experiments that isolate factors like recency bias (models favoring information at the end of the context) and distractability (models

Figure 1: **Lifelong ICL and Task Haystack.** Lifelong ICL presents long-context LMs with a sequence of tasks, each containing a task instruction and a few demonstrations. At test time, the model is given a previously seen task instruction and then makes predictions on the test input directly. A long-context LM “passes” the Task Haystack test when its accuracies in Lifelong ICL (Task 1+2+3) are not significantly worse than accuracies of the Single-task ICL baseline (Task 2 only).

getting distracted by irrelevant information). The results confirm that both factors contribute to performance degradation on Task Haystack. Additionally, we find that model performance declines when instructions are paraphrased at test time and when few-shot ICL demonstrations of a single task are repeated multiple times. These observations highlight the limitations of current long-context LMs in terms of robustness, instruction understanding, and context utilization.

We hope that Lifelong ICL and Task Haystack serve as useful resources and testbeds for evaluating, diagnosing, and understanding long-context LMs. Further, we anticipate that the limitations and vulnerabilities exposed in this paper will inspire innovations in long-context LM development.

## 2 Related Work

Long-Context LM Evaluation.Early studies on long-context modeling primarily rely on perplexity-based evaluations (Beltagy et al., 2020; Press et al., 2022). Subsequent research has indicated that such evaluation is limited in reflecting a model's effectiveness in downstream applications (Sun et al., 2021; Hu et al., 2024). Recent efforts have led to the development of comprehensive benchmarks for evaluating long-context models, which can be divided into realistic and synthetic categories. Realistic benchmarks, exemplified by (Zero)SCROLLS (Shaham et al., 2022, 2023), comprise tasks that require processing long inputs collected from real-world scenarios. These tasks are typically sourced from established datasets and include various task types such as summarization and question answering, or developed from inherently lengthy corpus such as novel (Zhang et al., 2024), grammar books (Tanzer et al., 2024) and code repository (Jimenez et al., 2024). In the category of synthetic benchmarks, the needle-in-a-haystack (NIAH) (Kamradt, 2023) evaluation is widely adopted for evaluating context utilization (Gemini Team, 2024; Anthropic, 2024; Liu et al., 2024; Fu et al., 2024; Levy et al., 2024, _i.a._). Ruler (Hsieh et al., 2024) expands on the NIAH test with multi-key and multi-value retrieval, and adds two new tasks that involve multi-hop tracing and aggregation. Hybrid benchmarks is a middle-field that incorporate both realistic and synthetic elements. An example is LongBench (Bai et al., 2024), which includes synthetic tasks based on realistic text, such as counting unique passages appearing in the context. Our proposed Task Haystack can be seen as a hybrid benchmark, with a realistic touch as (1) it is built upon realistic language tasks; (2) it proximates the challenge of navigating through evolving topics and tasks.

Evaluating Long-Context LMs with Many-Shot ICL.Several recent works have explored incontest learning with long-context LMs by scaling the number of training examples (_i.e._, shots). Bertsch et al. (2024) conducted a systematic study of long-context ICL with up to 2,000 shots, demonstrating many-shot ICL as a competitive alternative to retrieval-based ICL and fine-tuning. Additionally, it offers the advantage of caching demonstrations at inference time, unlike instance-level retrieval methods. While Bertsch et al. (2024) focus on classification tasks, Agarwal et al. (2024) showed the effectiveness of many-shot ICL on generative and reasoning tasks, and established new state-of-the-art results on practical applications such as low-resource translation with the Gemini 1.5 Pro model. However, there are still limitations to many-shot ICL. Li et al. (2024) introduce LongICLBench, a suite of 6 classification tasks with many (20+) classes, and find that current long-context LMs still struggle with these tasks. Orthogonal to this line of work on scaling _number of examples_ for one single task, we focus on scaling the _number of tasks_ in our Lifelong ICL setting.

Lifelong Learning in NLP.Lifelong learning, or continual learning, refers to the problem setting where a model learns continuously from data streams (Biesialska et al., 2020; Shi et al., 2024). Lifelong ICL is largely inspired by this line of work and challenges long-context models to learn continuously from a sequence of language tasks. However, unlike prior works that use gradient-based fine-tuning (de Masson d'Autume et al., 2019; Jin et al., 2021; Scialom et al., 2022; Mehta et al., 2023), Lifelong ICL is a new exploration that uses in-context learning as the underlying "learning" algorithm. It also stands out from Coda-Forno et al. (2023) and Ye et al. (2024) by focusing on evaluating long-context LMs and scaling the input length from 4k to up to 32k tokens. A primary challenge in lifelong learning is catastrophic forgetting, the tendency of a model to forget previously acquired tasks upon learning new tasks (Kirkpatrick et al., 2017). Our proposed Task Haystack evaluation focuses an analogous phenomenon, as the model may struggle to recall earlier information in a lengthy context, leading to a performance decline.

Problem Setting

In the following, we establish the notations and the problem setting of Lifelong ICL in SS3.1. We will begin by defining notations of in-context learning (ICL) of _one single task_\(T\). We will then build upon these foundations and introduce Lifelong ICL with _a collection of tasks_\(\). In SS3.2, we further introduce our Task Haystack evaluation protocol, provide the definition of the key metric named "pass rate," and describe our strategies to account for the instabilities in ICL experiments.

### Lifelong ICL

In-context Learning.In-context learning is a method that adapts LMs to perform a language task by providing prompts containing input-output pairs (Brown et al., 2020). In this paper, we define a language task \(T\) as a tuple of \((D^{train},D^{test},d)\), where \(D^{train}\) is the training set, \(D^{test}\) is the test set, \(d\) is a textual task description (_i.e._, instruction). We first create a task-specific prompt \(p\) by concatenating the task description and the \(k\)-shot examples in \(D^{train}\), _i.e._, \(p=d x_{1}^{train} y_{1}^{train} x_{k}^{train}  y_{k}^{train}\). Then, to make a prediction on the test input \(x^{test}\), we concatenate the task-specific prompt and the test input (_i.e._, \(p x^{test}\)), and query the language model \(\) to generate the prediction \(\). We denote this process as \(=(x^{test}|p)\) to highlight that the prediction is made by conditioning on the task-specific prompt \(p\).

Task Collection and Task Permutation.The definition above introduces how ICL is performed with one single task \(T\). In Lifelong ICL, an LM is expected to learn from a collection of \(n\) tasks, denoted as \(=\{T_{i}\}_{i=1}^{n}\). To enable this, we first create a random permutation \(a=(a_{1},a_{2},,a_{n})\), thus the tasks in \(\) will be ordered as \((T_{a_{1}},T_{a_{2}},,T_{a_{n}})\). For example, when \(n=3\), one possible permutation \(a\) is \((3,1,2)\), so that the tasks are ordered as \((T_{3},T_{1},T_{2})\).

Lifelong ICL.Given a permutation \(a\), we first create the task-specific prompt \(p_{a_{i}}\) for each task \(T_{a_{i}}\), and then create the Lifelong ICL prompt \(p_{l}\) by concatenating all task-specific prompts, _i.e._, \(p_{l}=p_{a_{1}} p_{a_{2}} p_{a_{n}}\). At test time, for _each_ task \(T_{a_{i}}\) in \(\), the model will be queried to perform generate the prediction as \(=(x_{test}|p_{l} d_{a_{i}})\). Note that we append the task description \(d_{a_{i}}\) after the Lifelong ICL prompt \(p_{l}\) at test time, to ensure the model is informed of the task at hand. See Fig. 1 for an illustrative example with 3 tasks.

### Task Haystack

Evaluation Principle.For a test task \(T_{a_{i}}\), we anticipate that long-context LMs can effectively utilize the in-context examples of that task, _i.e._, \(p_{a_{i}}\), which is a substring of the Lifelong ICL prompt \(p_{l} d_{a_{i}}\). To evaluate this, we compare the model performance on task \(T_{a_{i}}\) when conditioning on \(p_{l} d_{a_{i}}\) and \(p_{a_{i}}\), and expect the former to be not significantly worse than the latter. In other words, the Single-task ICL prompt \(p_{a_{i}}\) is the "needle" in the Lifelong ICL prompt \(p_{l}\) (_i.e._, the "task haystack").2

Addressing ICL Instability with Multiple Runs.One challenge in Task Haystack evaluation is the notorious instability of ICL. To account for this, our experiments will be carried out with 5 random samples of the permutation \(a\) and 5 randomly-sampled few-shot training set \(D_{train}\) for each task. This allows us to obtain a performance matrix of size \((t,p,r)\) for Lifelong ICL, where \(t\) is the task index, \(p\) is the permutation index, and \(r\) is the few-shot sample index.3 We will also obtain a matrix of size \((t,r)\) for the Single-task ICL baseline.

Evaluation Metrics.For an **overall measurement**, we introduce an **overall pass rate**. For each permutation \(a\) and each task \(T_{a_{i}}\), we will get two groups of performance metrics, when using Single-task ICL and Lifelong ICL respectively. Each group contains 5 metrics, corresponding to the 5 randomly-sampled few-shot training set \(D_{train}\). The model passes the test (_i.e._, scores 1) when the the Lifelong ICL group is not significantly worse than the Single-task ICL group, captured by a two-sided t-test with \(p=0.05\). The model scores 0 otherwise. The overall pass rate will be computed by averaging the scores over the different permutations and tasks. We provide more details of the definition and discuss its limitations in SSA.3.

For a **fine-grained analysis**, our experiment results allow us to **visualize the pass rates grouped by the position in the task stream, by the task, or by the task permutation**. This enables straightforward visualizations as popularized by the needle-in-a-haystack test, providing an convenient tool to diagnose and uncover the vulnerabilities of long-context LMs. See Fig. 24 for an example.

## 4 Experiment Details

Task Selection.While the problem setting in SS3 is generic and admits any language task, in this work we instantiate the setting with a narrower task distribution for initial exploration. Our key considerations include:4

* We focus on classification tasks, as they allow standardized evaluation. Additionally, a large body of past work investigates ICL empirically or mechanistically using classification tasks .
* We select classification tasks with fewer than 20 categories and input text shorter than 1000 tokens, to avoid excessively long single-task prompts that dominate the whole context window .
* We focus on English tasks, since most long-context LMs are not optimized for multilingual usage.

After careful manual selection, we obtain a collection of 64 classification tasks, covering a wide range of domains and label spaces. We provide a snippet of 16 tasks in Table 1 and provide detailed descriptions of all 64 tasks, including their references and license information, in Table 6.

Models.We evaluate eleven open-weight long-context LMs on Task Haystack: Mistral-7B (32k) , FILM-7B (32k) , Llama-2-7B (32k) , Llama-2-7B (80k) , Llama-3-8B/70B (1048k) , Llama-3-1-70B (128k) , Yi-6B/9B/34B (200k) , and Command-R-35B (128k) . These models represent various long-context modeling techniques, model size, and base pre-trained models. Additionally, we evaluate three closed models, GPT-3.5-Turbo (16k) and GPT-4o (128k) from OpenAI, and Gemini-1.5-Flash (1048k) from Google DeepMind . We provide the detailed descriptions of these models in Table 5 in SSA.1.

Controlling the Context Length.We consider creating long contexts with two strategies: **(1) Scale-Shot**: scaling the number of in-context examples (\(n_{shot}\)); **(2) Scale-Task**: scaling the number of tasks (\(n_{task}\)). In the first setting, we fix \(n_{task}=16\) and experiment with \(n_{shot}\{1,2,3,4,5,6,7,8\}\). We use the 16 tasks listed in Table 1 in the main body of the paper.5 In the second setting, we fix \(n_{shot}=2\) and experiment with \(n_{task}\{8,16,24,32,40,48,56,64\}\). Note that to ensure the in-context examples are balanced and every class is covered, \(n_{shot}=2\) refers to using 2 examples _per class_ for in-context learning. In both scaling settings, we are able to effectively create contexts of sizes ranging from 4k to 32k tokens.6

We defer additional implementation and engineering details in SSA.4.

   emo & covid\_fake\_news & logical\_fallacy\_detection & dbpedia\_14 \\ amazon\_massive\_scenario & news\_data & semeval\_abs\_restaurant & amazon\_counterfactual\_en \\ bra\_action & boolq & this\_is\_not\_a\_dataset & insi\_ence\_questions \\ clickbait & yahoo\_answers\_topics & pun\_detection & wiki\_qa \\   

Table 1: **A Snippet of 16 tasks used in our experiments.** See Table 6 for the full list of 64 tasks. The 16 tasks in this table are used for the Scale-Shot experiments in Table 2.

[MISSING_PAGE_EMPTY:6]

[MISSING_PAGE_FAIL:7]

Anthropic, 2024]. However, the improvements only close about half the gap between Baseline and Recall, suggesting that recency bias contributes to but does not fully explain the performance gap.

(b) Distraction.We examine the effect of irrelevant context, by contrasting Baseline with Random. The results indicate that prepending an irrelevant long text will influence the performance negatively, which corroborates with recent work investigating the robustness of language models [Levy et al., 2024]. Further, Replay can be seen as prepending a long prefix of mostly irrelevant tasks before performing Single-task ICL (Baseline), and thus the gap between Replay and Baseline may be interpreted as caused by prepending irrelevant contexts.

(c) Long-context Input.We further compare Baseline, Random, Repeat settings altogether, where Random introduces _irrelevant_ context and Repeat includes only _relevant_ context. Perhaps surprisingly, performance drops in the Repeat setting (-1.3% for Mistral-7B and -3.1% for FILM-7B), where both distractions and recency biases are absent. This observation raises concerns on whether longer inputs are more likely to trigger failure modes and give rise to undesired behaviors in general. While more evidence is needed to derive a conclusion, we suggest that long-context LM users be cautious about including everything in the context window, and we recommend using external filtering or retrieval models when necessary.

Dependency on Task Instructions and ICL Demonstrations.In the Remove setting, we remove the task instruction and the ICL examples of the test task from the Lifelong ICL prompt, to investigate whether the models are relying on such information. We observe a clear performance drop in the Remove setting (-3.3% for Mistral-7B and -4.1% for FILM-7B compared to Recall), suggesting that the models are able to locate and make use of the "needle" to some extent in the Recall setting, but not doing it precisely so that the performance can match with the Single-task ICL baseline.

The Paraphrase setting further allows us to explore how models make use of task instructions. We observe a decline in performance in the Paraphrase setting compared to Recall. This confirms that the models locate the "needle" by retrieving identical instructions in the context. However, the performance gap indicates that models mainly rely on pattern matching rather than deeper understanding of the instructions, which might limit their broader utility in practical applications.

Repeated ICL as "Multi-epoch" ICL.We conduct further investigation with the Random, Repeat, Repeat+Shuffle setting, by varying the size of the context and the number of repetitions. Results are reported in Fig. 5. Interestingly, model performance first increases and then dips when running in-context learning for multiple "epochs." One direct takeaway is that repeating the ICL examples multiple times can potentially improve performance, which may have practical utilities in certain low-data high-inference-budget regimes. However, model performance starts to degrade after repeating more than 8 times. This phenomenon can be interpreted in two ways: (1) It is a known issue that repetition may lead to model degeneration [Nasr et al., 2023]; Repeat+Shuffle can possibly alleviate this issue by introducing slight variations in each repeat, which explains why Repeat+Shuffle outperforms Repeat in general. (2) It is also possible that the model "overfits" to the few-shot training data after multiple "epochs", analogous to the common observations in gradient-based fine-tuning. We invite future work to investigate the working mechanism of ICL in this "multi-epoch" setting.

### Additional Observations and Analysis

Tasked learned via ICL are more easily influenced.While examining Task Haystack results, we find that the passing and failing behaviors are highly task-specific. For example, in Fig. 24, Mistral-7B (32k) fails on news_data and insincere_questions in all permutations, meanwhile passes on more popular tasks like boolq and yahoo_answer_topics. We hypothesize that models may have memorized some of the tasks during pre-training or post-training, making these tasks less subjective to performance drop in Lifelong ICL. Alternatively, a task may be too challenging for the model to learn through ICL, and thus it passes the test by maintaining low performance in both Single-task ICL and Lifelong ICL settings.

To account for these situations, we split all tasks into 2 groups for each model. Tasks of which 4-shot performance is significantly better than 1-shot performance are classified as ICL-effective tasks, and the remaining tasks are considered to be ICL-ineffective. We report the pass rates for each model on these two groups in Table 4. For 10 out of 12 models, pass rates on ICL-effective tasks are lower than pass rates on ICL-ineffective tasks, suggesting that these models tend to "forget" tasks that are newly acquired, and that the overall pass rates may be an overestimate.

Trends of positive task transfer.While our study mainly focus on undesired performance degradation in Lifelong ICL, which is analogous to the catastrophic forgetting phenomenon in lifelong learning, we also observe trends of positive forward and backward transfers, two desired properties of lifelong learning.7 In our pass rate design, we deliberately choose _two-sided_ t-test to account for both performance gains and drops. We observe positive transfers in Fig. 2, represented by the blue-colored cells in the 1-shot (4k) column and the last row (94% depth). Similar observations can be made with Llama-2 (32k) in Fig. 12 and GPT-4o in Fig. 22. Additionally, Mistral-7B achieves +3.4% performance gain in the Remove setting compared to the Zero-shot baseline (Fig. 4). We consider these as initial evidence for positive transfer in Lifelong ICL, and invite more rigorous analysis to further explore the properties of Lifelong ICL.

## 6 Discussion

Intended Use.We anticipate Lifelong ICL and Task Haystack to be used for evaluating and diagnosing newly released long-context LMs. However, as our findings in Sections 5.1 and 5.3 suggest, the ICL accuracy and pass rate might be affected if the model has been trained on the tasks used in our evaluation. To ensure responsible use, we encourage users to (1) investigate and report any potential data contamination; (2) report pass rates on ICL-effective/ineffective groups respectively, as done in SS5.3. Additionally, it is possible to use targeted data engineering to improve pass rates on Task Haystack. For fair comparisons, we recommend that users disclose whether their training data contains sequences in a format similar to Task Haystack evaluation.

Limitations.(1) As an initial exploration in the Lifelong ICL setting, we primarily focuses on English-only text classification tasks. This potentially limits a comprehensive assessment of model capabilities across various challenges. To get a more complete picture, the evaluation suite may be

   Model &  ICL-erf. \\ N \\  &  ICL-ineff. \\ pass \\  &  ICL-ineff. \\ pass \\  &  All \\ pass \\  &  Model \\ N \\  &  ICL-eff. \\ pass \\  &  ICL-ineff. \\ N \\  & 
 All \\ pass \\  \\  Mistral-7B (32k) & 5 & 36.0 & 11 & 81.8 & 67.5 & Cmd-R-35B (128k) & 5 & 40.0 & 11 & 58.2 & 52.5 \\ FHL-7B (32k) & 2 & 40.0 & 14 & 77.1 & 72.5 & Yi-6B (200k) & 6 & 46.6 & 10 & 42.0 & 43.8 \\ Llama-2-7B (32k) & 6 & 33.3 & 10 & 46.0 & 41.2 & Yi-9B (200k) & 6 & 50.0 & 10 & 72.0 & 63.7 \\ Llama-2-7B (80k) & 3 & 80.0 & 13 & 100.0 & 96.3 & Yi-34B (200k) & 3 & 46.7 & 13 & 67.7 & 63.8 \\ Llama-3-8B (1048k) & 6 & 40.0 & 10 & 90.0 & 71.3 & GPT-3.5-Turbo (16k) & 5 & 44.0 & 11 & 70.9 & 62.5 \\ Llama-3-70B (1048k) & 4 & 35.0 & 12 & 65.0 & 57.5 & GPT-4o (128k) & 6 & 96.7 & 10 & 76.0 & 83.7 \\   

Table 4: **Pass Rates on ICL-effective/ineffective Tasks.** Results are computed in the 16-task 4-shot setting. We define ICL-effective tasks as tasks whose 4-shot performance is significantly better than its 1-shot performance. In general, ICL-effective tasks have lower pass rates.

improved by including more diverse tasks categories (_e.g._, question answering, conditional generation (Ye et al., 2021)), modalities (_e.g._, vision (Sharma et al., 2024), speech), and languages. We encourage future research to build upon our foundation and explore these more complex settings. (2) This work simplifies the lifelong learning stream by assuming a sequential order, clear task boundaries, and a fixed number of examples per class for each task. Real-world scenarios likely involve a more dynamic learning stream, without clear task boundaries or assumptions on the sequential order. In SSB.6, we conduct preliminary experiments by interleaving examples of multiple tasks in the context. Future work may explore more realistic lifelong learning streams with increased complexity. (3) Finally, due to computational constraints, our evaluation utilizes 5 random permutations of tasks order and 5 different random samples of few-shot training sets. Experimenting with a larger number of samples could potentially reduce the randomness inherent in the results and increase the reliability of the findings. Additionally, we limit our evaluation to up to 32k input tokens. Stress-testing long-context models with their full context lengths may reveal further limitations of these models.

Ethics Statement.This work leverages openly available datasets that were carefully reviewed by the authors to mitigate potential data privacy and security concerns. To the best of our knowledge, the datasets we use do not contain personally identifiable information. Some datasets contain offensive content when the underlying task is offensive content (_e.g._, hate speech) classification. We emphasize that these datasets are used solely for evaluation purposes. As our research does not involve model training or the release of new models, the risk of amplifying biases within the data is minimal.

## 7 Conclusion

In this paper, we introduced Lifelong ICL, a novel problem setting for long-context LMs, and developed Task Haystack, a concrete evaluation suite focusing on evaluating and diagnosing long-context LMs in the Lifelong ICL setting. Our experiments with 14 long-context LMs revealed that while these models excel at needle-in-a-haystack style evaluation, their ability to utilize the context flexibly and contextually remains limited. Through our controlled analysis, we dissected and quantified factors such as recency biases and distractions that contribute to performance drops. We also identified performance degradation when repeating ICL examples or using paraphrased instructions, highlighting a fundamental vulnerability in current long-context models.

Our results demonstrate that Task Haystack still poses significant challenges for newly-released long-context models. We hope that Lifelong ICL and Task Haystack will serve as valuable tools for diagnosing and advancing the development of future long-context LMs. Additionally, we consider our work as an exploratory step towards backprop-free algorithms in lifelong learning settings.