# Rotating Features for Object Discovery

Sindy Lowe

AMLab

University of Amsterdam &Phillip Lippe

QUVA Lab

University of Amsterdam &Francesco Locatello

Institute of Science and

Technology Austria (ISTA)

&Max Welling

AMLab

University of Amsterdam

###### Abstract

The binding problem in human cognition, concerning how the brain represents and connects objects within a fixed network of neural connections, remains a subject of intense debate. Most machine learning efforts addressing this issue in an unsupervised setting have focused on slot-based methods, which may be limiting due to their discrete nature and difficulty to express uncertainty. Recently, the Complex AutoEncoder was proposed as an alternative that learns continuous and distributed object-centric representations. However, it is only applicable to simple toy data. In this paper, we present Rotating Features, a generalization of complex-valued features to higher dimensions, and a new evaluation procedure for extracting objects from distributed representations. Additionally, we show the applicability of our approach to pre-trained features. Together, these advancements enable us to scale distributed object-centric representations from simple toy to real-world data. We believe this work advances a new paradigm for addressing the binding problem in machine learning and has the potential to inspire further innovation in the field.

## 1 Introduction

Discovering and reasoning about objects is essential for human perception and cognition , allowing us to interact with our environment, reason about it, and adapt to new situations . To represent and connect symbol-like entities such as objects, our brain flexibly and dynamically combines distributed information. However, the binding problem remains heavily debated, questioning how the brain achieves this within a relatively fixed network of neural connections .

Most efforts to address the binding problem in machine learning center on slot-based methods . These approaches divide their latent representations into "slots", creating a discrete separation of object representations within a single layer of the architecture at some arbitrary depth. While highly interpretable and easy to use for downstream applications, this simplicity may limit slot-based approaches from representing the full diversity of objects. Their discrete nature makes it challenging to represent uncertainty in the separation of objects; and it remains unclear how slots may be recombined to learn flexible part-whole hierarchies . Finally, it seems unlikely that the brain assigns entirely distinct groups of neurons to each perceived object.

Recently, Lowe et al.  proposed the Complex AutoEncoder (CAE), which learns continuous and distributed object-centric representations. Taking inspiration from neuroscience, it uses complex-valued activations to learn to encode feature information in their magnitudes and object affiliation in their phase values. This allows the network to express uncertainty in its object separations and embedsobject-centric representations in every layer of the architecture. However, due to its two-dimensional (i.e. complex-valued) features, it is severely limited in the number of object representations it can separate in its one-dimensional phase space; and due to the lack of a suitable evaluation procedure, it is only applicable to single-channel inputs. Taken together, this means that the CAE is not scalable, and is only applicable to grayscale toy data containing up to three simple shapes.

In this paper, we present a series of advancements for continuous and distributed object-centric representations that ultimately allow us to scale them from simple toy to real-world data. Our contributions are as follows:

1. We introduce _Rotating Features_, a generalization of complex-valued features to higher dimensions. This allows our model to represent a greater number of objects simultaneously, and requires the development of a new rotation mechanism.
2. We propose a new evaluation procedure for continuous object-centric representations. This allows us to extract discrete object masks from continuous object representations of inputs with more than one channel, such as RGB images.
3. We show the applicability of our approach to features created by a pretrained vision transformer . This enables Rotating Features to extract object-centric representations from real-world images.

We show how each of these improvements allows us to scale distributed object-centric representations to increasingly complicated data, ultimately making them applicable to real-world data. Overall, we are pioneering a new paradigm for a field that has been largely focused on slot-based approaches, and we hope that the resulting richer methodological landscape will spark further innovation.

## 2 Neuroscientific Motivation

Rotating Features draw inspiration from theories in neuroscience that describe how biological neurons might utilize their temporal firing patterns to flexibly and dynamically bind information into coherent percepts, such as objects [15; 20; 21; 50; 53; 65; 79]. In particular, we take inspiration from the temporal correlation hypothesis [66; 67]. It posits that the oscillating firing patterns of neurons (i.e. brain waves) give rise to two message types: the discharge frequency encodes the presence of features, and the relative timing of spikes encode feature binding. When neurons fire in synchrony, their respective features are processed jointly and thus bound together dynamically and flexibly.

Figure 1: Rotating Features. We propose to extend standard features by an extra dimension \(n\) across the entire architecture, including the input (highlighted in blue, here \(n=3\)). We then set up the layer structure within \(f_{}\) in such a way that the Rotating Featuresâ€™ magnitudes \(m\) learn to represent the presence of features, while their orientations learn to represent object affiliation.

Previous work in machine learning [48; 58; 59; 60; 61] has taken inspiration from these neuroscientific theories to implement the same two message types using complex-valued features: their magnitudes encode feature presence, and their relative phase differences encode feature binding. However, the one-dimensional orientation information of complex-valued features constrains their expressivity, and existing approaches lack evaluation procedures that scale beyond single-channel inputs. As a result, they are limited to grayscale toy datasets. To overcome these limitations, we propose several advancements for distributed and continuous object-centric representations, which we describe in detail in the next section.

## 3 Rotating Features

We create Rotating Features by augmenting standard features with additional dimensions and by creating a layer structure that parses the magnitude and multi-dimensional orientation of the resulting vectors separately. In the following, we describe how we represent, process, train and evaluate Rotating Features such that their magnitudes learn to represent the presence of features and their orientations learn to represent object affiliation.

### Representing Rotating Features

In a standard artificial neural network, each layer produces a \(d\)-dimensional feature vector \(}^{d}\), where \(d\) might be further subdivided to represent a particular structure of the feature map (e.g. \(d=c h w\), where \(c,h,w\) represent the channel, height and width dimensions, respectively). To create Rotating Features, we extend each scalar feature within this \(d\)-dimensional feature vector into an \(n\)-dimensional vector, thus creating a feature matrix \(}^{n d}\). Given the network structure described below, its magnitude vector \(\|}\|_{2}^{d}\) (where \(\|\|_{2}\) is the L2-norm over the rotation dimension \(n\)) behaves similarly to a standard feature vector and learns to represent the presence of certain features in the input. The remaining \(n-1\) dimensions represent the orientations of the Rotating Features, which the network uses as a mechanism to bind features: features with similar orientations are processed together, while connections between features with different orientations are suppressed. Note, that we represent Rotating Features in Cartesian coordinates rather than spherical coordinates, as the latter may contain singularities that hinder the model's ability to learn good representations1.

Figure 2: Rotating Features applied to real-world images. By implementing a series of advancements, we scale continuous and distributed object representations from simple toy to real-world datasets.

### Processing Rotating Features

We create a layer structure \(f_{}\) that processes Rotating Features in such a way that their magnitudes represent the presence of certain features, while their orientations implement a binding mechanism. To achieve this, we largely follow the layer formulation of the CAE , but generalize it from complex-valued feature vectors \(_{}^{d}\) to feature matrices of the form \(_{}^{n d}\) both in the implementation and in the equations below; and introduce a new mechanism to rotate features. This allows us to process Rotating Features with \(n 2\).

**Weights and Biases** Given the input \(_{}^{n d_{}}\) to a layer with \(n\) rotation dimensions and \(d_{}\) input feature dimensions, we apply a neural network layer parameterized with weights \(^{d_{} d_{}}\) and biases \(^{n d_{}}\), where \(d_{}\) is the output feature dimension, in the following way:

\[=f_{}(_{})+ ^{n d_{}} \]

The function \(f_{}\) may represent different layer types, such as a fully connected or convolutional layer. For the latter, weights may be shared across \(d_{}\) and \(d_{}\) appropriately. Regardless of the layer type, the weights are shared across the \(n\) rotation dimensions, while the biases are not. By having separate biases, the model learns a different orientation offset for each feature, providing it with the necessary mechanism to learn to rotate features. Notably, in contrast to the CAE, the model is not equivariant with respect to global orientation with this new formulation of the rotation mechanism: when there is a change in the input orientation, the output can change freely. While it is possible to create an equivariant rotation mechanism for \(n 2\) by learning or predicting the parameters of a rotation matrix, preliminary experiments have demonstrated that this results in inferior performance.

**Binding Mechanism** We implement a binding mechanism that jointly processes features with similar orientations, while weakening the connections between features with dissimilar orientations. This incentivizes features representing the same object to align, and features representing different objects to take on different orientations.

To implement this binding mechanism, we utilize the same weights as in Eq. (1) and apply them to the magnitudes of the input features \(_{}_{2}^{d_{}}\):

\[=f_{}(_{}_ {2})^{d_{}} \]

We then integrate this with our previous result by calculating the average between \(\) and the magnitude of \(\):

\[_{}=0.5_{2} +0.5^{d_{}} \]

This binding mechanism results in features with similar orientations being processed together, while connections between features with dissimilar orientations are suppressed. Given a group of features of the same orientation, features of opposite orientations are masked out, and misaligned features are gradually scaled down. This effect has been shown by Lowe et al. , Reichert & Serre , and we illustrate it in Fig. 3. In contrast, without the binding mechanism, the orientation of features does not influence their processing, the model has no incentive to leverage the additional rotation dimensions, and the model fails to learn object-centric representations (see Appendix D.3). Overall, the binding mechanism allows the network to create streams of information that it can process separately, which naturally leads to the emergence of object-centric representations.

**Activation Function** In the final step, we apply an activation function to the output of the binding mechanism.

Figure 3: Effect of the binding mechanism. We start by randomly sampling two column vectors \(,^{n}\) with \(_{2}=_{2}=1\). Assuming \(d_{}=3,d_{}=1\) and \(f_{}\) is a linear layer, we set \(_{}=[,,]\), weights \(=[,,]^{T}\) and biases \(=[0,...,0]^{T}\). Subsequently, we plot the cosine similarity between \(\) and \(\) on the x-axis, and \(_{}\) and \(_{2}\) on the y-axis, representing the magnitudes of the layerâ€™s output before applying the activation function with (blue) and without (orange) the binding mechanism. Without the binding mechanism, misaligned features are effectively subtracted from the aligned features, resulting in smaller output magnitudes for \(_{2}\). The binding mechanism reduces this effect, leading to consistently larger magnitudes in \(_{}\). In the most extreme scenario, features with opposite orientations (i.e., with a cosine similarity of -1) are masked out by the binding mechanism, as the output magnitude (\(\)) would be the same if \(_{}=[,,]\).

nism \(_{}\), ensuring it yields a positive-valued result. By rescaling the magnitude of \(\) to the resulting value and leaving its orientation unchanged, we obtain the output of the layer \(_{}^{n d_{}}\):

\[_{} =((_{})) ^{d_{}} \] \[_{} =}{_{2}} _{}^{n d_{}} \]

### Training Rotating Features

We apply Rotating Features to vision datasets which inherently lack the additional \(n\) dimensions. This section outlines the pre-processing of input images to generate rotating inputs and the post-processing of the model's rotating output to produce standard predictions, which are used for training the model.

Given a positive, real-valued input image \(^{}^{c h w}\) with \(c\) channels, height \(h\), and width \(w\), we create the rotating feature input \(^{n c h w}\) by incorporating \(n-1\) empty dimensions. We achieve this by setting the first dimension of \(\) equal to \(^{}\), and assigning zeros to the remaining dimensions. Subsequently, we apply a neural network model \(f_{}\) that adheres to the layer structure of \(f_{}\) to this input, generating the output features \(=f_{}()^{n d_{}}\).

To train the network using a reconstruction task, we set \(d_{}=c h w\) and extract the magnitude \(_{2}^{c h w}\) from the output. We then rescale it using a linear layer \(f_{}\) with sigmoid activation function. This layer has weights \(_{}^{c}\) and biases \(_{}^{c}\) that are shared across the spatial dimensions, and applies them separately to each channel \(c\):

\[}=f_{}(_{2}) ^{c h w} \]

Finally, we compute the reconstruction loss \(=(^{},})\) by comparing the input image \(^{}\) to the reconstruction \(}\) using the mean squared error (MSE).

### Evaluating Object Separation in Rotating Features

While training the network using the Rotating Features' magnitudes, their orientations learn to represent "objectness" in an unsupervised manner. Features that represent the same objects align, while those representing different objects take on distinct orientations. In this section, we outline how to process the continuous orientation values of the output Rotating Features \(^{n c h w}\), in order to generate a discrete object segmentation mask. This approach follows similar steps to those described for the CAE , but introduces an additional procedure that enables the evaluation of Rotating Features when applied to inputs with \(c 1\).

As the first step, we normalize the output features such that their magnitudes equal one, mapping them onto the unit (hyper-)sphere. This ensures that the object separation of features is assessed based on their orientation, rather than their feature values (i.e. their magnitudes):

\[_{}=}{_{ 2}}^{n c h w} \]

Subsequently, we mask features with small magnitudes, as they tend to exhibit increasingly random orientations, in a manner that avoids trivial solutions that may emerge on feature maps with \(c>1\). For instance, consider an image containing a red and a blue object. The reconstruction \(\) would be biased towards assigning small magnitudes to the color channels inactive for the respective objects. If we simply masked out features with small magnitudes by setting them to zero, we would separate the objects based on their underlying color values, rather than their assigned orientations.

To avoid such trivial solutions, we propose to take a weighted average of the orientations across channels, using the thresholded magnitudes of \(\) as weights:

\[_{}^{i,j,l} =1&\|\|_{2}^{i,j,l}>t\\ 0& \] \[_{} =^{c}_{}^{i} _{}^{i}}{_{i=1}^{c}_{}^{i}+} ^{n h w} \]

where we create the weights \(_{}\) by thresholding the magnitudes \(_{2}\) for each channel \(i[1,...,c]\) and spatial location \(j[1,...,h]\), \(l[1,...,w]\) using the threshold \(t\) and then compute the weighted average across channels \(c\). Here, \(\) denotes the Hadamard product, \(\) is a small numerical constant to avoid division by zero, and \(_{}\) is repeated across the \(n\) rotation dimensions.

Lastly, we apply a clustering procedure to \(_{}\) and interpret the resulting discrete cluster assignment as the predicted object assignment for each feature location. Our experiments demonstrate that both \(k\)-means and agglomerative clustering achieve strong results, thus eliminating the need to specify the number of objects in advance.

## 4 Experiments

In this section, we evaluate whether our proposed improvements enable distributed object-centric representations to scale from simple toy to real-world data. We begin by outlining the general settings common to all experiments, and then proceed to apply each proposed improvement to a distinct setting. This approach allows us to isolate the impact of each enhancement. Finally, we will highlight some advantageous properties of Rotating Features. Our code is publicly available at github.com/loeweX/RotatingFeatures.

**General Setup** We implement Rotating Features within a convolutional autoencoding architecture. Each model is trained with a batch-size of 64 for 10,000 to 100,000 steps, depending on the dataset, using the Adam optimizer . Our experiments are implemented in PyTorch  and run on a single Nvidia GTX 1080Ti. See Appendix C for more details on our experimental setup.

**Evaluation Metrics** We utilize a variety of metrics to gauge the performance of Rotating Features and draw comparisons with baseline methods. Reconstruction performance is quantified using mean squared error (MSE). To evaluate object discovery performance, we employ Adjusted Rand Index (ARI) [34; 57] and mean best overlap (MBO) [56; 64]. ARI measures clustering similarity, where a score of 0 indicates chance level and 1 denotes a perfect match. Following standard practice in the object discovery literature, we exclude background labels when calculating ARI (ARI-BG) and evaluate it on instance-level masks. MBO is computed by assigning the predicted mask with the highest overlap to each ground truth mask, and then averaging the intersection-over-union (IoU) values of the resulting mask pairs. Unlike ARI, MBO takes into account background pixels, thus

Figure 4: Qualitative and quantitative performance comparison on the 4Shapes dataset. **(a)** In comparison to a standard autoencoder (AE) and the Complex AutoEncoder (CAE), Rotating Features (RF) create sharper reconstructions (1st and 3rd row). Additionally, Rotating Features separate all four shapes, while the CAE does not (2nd and 4th row). **(b)** Larger rotation dimensions \(n\) lead to better reconstruction performance (top) and better object discovery performance (bottom), with \(n 6\) resulting in a perfect separation of all shapes (mean \(\) sem performance across four seeds).

measuring the degree to which masks conform to objects. We assess this metric using both instance-level (MBO\({}_{i}\)) and class-level (MBO\({}_{c}\)) masks.

### Rotating Features Can Represent More Objects

**Setup**: We examine the ability of Rotating Features to represent more objects compared to the CAE  using two datasets. First, we employ the 4Shapes dataset, which the CAE was previously shown to fail on. This grayscale dataset contains the same four shapes in each image (\(\), \(\), \(\), \(\)). Second, we create the 10Shapes dataset, featuring ten shapes per image. This dataset is designed to ensure the simplest possible separation between objects, allowing us to test the capacity of Rotating Features to simultaneously represent many objects. To achieve this, we employ a diverse set of object shapes (\(\), \(\), \(\) with varying orientations and sizes) and assign each object a unique color and depth value.

**Results**: As depicted in Fig. 4, Rotating Features improve in performance on the 4Shapes dataset as we increase the size of the rotation dimension \(n\). For the CAE and Rotating Features with \(n=2\), where the orientation information is one-dimensional, the models struggle to distinguish all four objects. However, as the rotation dimension grows, performance enhances, and with \(n=6\), the model perfectly separates all four objects. Additionally, we observe that the reconstruction performance improves accordingly, and significantly surpasses the same autoencoding architecture using standard features, despite having a comparable number of learnable parameters.

Moreover, Rotating Features are able to differentiate 10 objects simultaneously, as evidenced by their ARI-BG score of 0.959 \(\) 0.022 on the 10Shapes dataset (Fig. 5). In Appendix D.5, we provide a detailed comparison of Rotating Features with \(n=2\) and \(n=10\) applied to images with increasing numbers of objects. Additionally, to determine if Rotating Features can distinguish even more objects, we conduct a theoretical investigation in Appendix D.1. In this analysis, we reveal that as we continue to add points to an \(n\)-dimensional hypersphere, the minimum cosine similarity achievable between these points remains relatively stable after the addition of the first ten points, provided that \(n\) is sufficiently large. This implies that once an algorithm can separate ten points (or objects, in our case) on a hypersphere, it should be scalable to accommodate more points. Consequently, the number of objects to separate is not a critical hyperparameter of our model, in stark contrast to slot-based architectures.

### Rotating Features Are Applicable to Multi-Channel Images

**Setup**: We explore the applicability of Rotating Features and our proposed evaluation method to multi-channel inputs by creating an RGB(-D) version of the 4Shapes dataset. This variation contains the same four shapes per image as the original dataset, but randomly assigns each object a color. We vary the number of potential colors, and create two versions of this dataset: an RGB version and an RGB-D version, in which each object is assigned a fixed depth value.

**Results**: As illustrated in Fig. 6, Rotating Features struggle to distinguish objects in the RGB version of the 4Shapes dataset when the number of potential colors increases. This appears to stem from their tendency to group different shapes together based on shared color. In Appendix D.6, we observe the same problem when applying Rotating Features to commonly used object discovery datasets (multi-dSprites and CLEVR). When depth information is added, this issue is resolved, and Rotating Features successfully separate all objects. This demonstrates that our proposed evaluation procedure makes Rotating Features applicable to multi-channel inputs, but that they require higher-level information in their inputs to reliably separate objects.

### Rotating Features Are Applicable to Real-World Images

Inspired by the previous experiment, which demonstrates that Rotating Features and our proposed evaluation procedure can be applied to multi-channel inputs but require higher-level features for optimal object discovery performance, we apply them to the features of a pretrained vision transformer.

Figure 5: Rotating Features learn to separate all ten objects in the 10Shapes dataset.

SetupFollowing Seitzer et al. , we utilize a pretrained DINO model  to generate higher-level input features. Then, we apply an autoencoding architecture with Rotating Features to these features. We test our approach on two datasets: the Pascal VOC dataset  and FoodSeg103 , a benchmark for food image segmentation. On the Pascal VOC dataset, we do not compare ARI-BG scores, as we have found that a simple baseline significantly surpasses previously reported state-of-the-art results on this dataset (see Appendix D.4). On FoodSeg103, we limit the evaluation of our model to the MBO\({}_{c}\) score, as it only contains class-level segmentation masks.

ResultsThe qualitative results in Fig. 2 highlight the effectiveness of Rotating Features in segmenting objects across both real-world datasets. Table 1 details the object discovery performance of Rotating Features compared to various models on the Pascal VOC dataset. The strongest performance on this dataset is achieved by the _DINOSAUR Transformer_ model , a Slot Attention-based model with an autoregressive transformer decoder that is applied to DINO pretrained features. Closer to our setting (i.e., without autoregressive models), the _DINOSAUR MLP_ model combines Slot Attention with a spatial broadcast decoder (MLP decoder) and achieves the second-highest performance reported in literature in terms of MBO\({}_{c}\) and MBO\({}_{i}\) scores. The performance of _Rotating Features_, embedded in a comparatively simple convolutional autoencoding architecture, lies in between these two Slot Attention-based models. Rotating Features also outperform a baseline that applies \(k\)-means directly to the DINO features (_DINO \(k\)-means_), showing that they learn a more meaningful separation of objects. Finally, the performance gain of Rotating Features over the original CAE model applied to the DINO features (_DINO CAE_) highlights their improved object modelling capabilities. For reference, object discovery approaches applied directly to the original input images (_Slot Attention_, _SLATE_, _Rotating Features -DINO_) struggle to segment the objects in this dataset and achieve performances close to a _Block Masks_ baseline that divides images into regular rectangles. To verify that Rotating Features work on more settings, we also apply them to the FoodSeg103 dataset. Here, they achieve an MBO\({}_{c}\) of \(0.484 0.002\), significantly outperforming the block masks baseline (\(0.296 0.000\)). Overall, this highlights that Rotating Features learn to separate objects in real-world images through unsupervised training; thus scaling continuous and distributed object representations from simple toy to real-world data for the first time.

   Model & MBO\({}_{i}\) & MBO\({}_{c}\) \\  Block Masks & 0.247 & 0.259 \\ Slot Attention & 0.222 Â± 0.0048 & 0.237 Â± 0.008 \\ SLATE & 0.310 Â± 0.004 & 0.324 Â± 0.004 \\ Rotating Features â€“DINO & 0.282 Â± 0.0046 & 0.320 Â± 0.006 \\  DINO \(k\)-means & 0.363 & 0.405 \\ DINO CAE & 0.329 Â± 0.009 & 0.374 Â± 0.010 \\ DINOSAUR Transformer & 0.440 Â± 0.0048 & 0.512 Â± 0.008 \\ DINOSAUR MLP & 0.395 Â± 0.000 & 0.409 Â± 0.000 \\  Rotating Features & 0.407 Â± 0.001 & 0.460 Â± 0.001 \\   

Table 1: Object discovery performance on the Pascal VOC dataset (mean Â± sem across 5 seeds). Rotating Featuresâ€™ scores indicate a good separation of objects on this real-world dataset. Baseline results adapted from Seitzer et al. .

Figure 6: Rotating Features on the 4Shapes RGB(-D) dataset. When applied to RGB images with more colors, object discovery performance in ARI-BG degrades (right; mean Â± sem across 4 seeds) as the Rotating Features tend to group objects of the same color together (left). By adding depth information to the input (RGB-D), this problem can be resolved, and all objects are separated as intended.

[MISSING_PAGE_FAIL:9]

the SlotAttention model and the Rotating Features model evaluated with \(k\)-means degrade considerably as the number of objects in the test images increases. This problem can be circumvented by evaluating Rotating Features with agglomerative clustering. After determining the distance threshold on the training dataset, this approach preserves a fairly consistent performance across varying numbers of objects in each scene, considerably outperforming the other approaches that require the number of objects to be set in advance. In summary, our results suggest that Rotating Features can generalize beyond the number of objects observed during training, even when the number of objects in the test images is unknown.

**Uncertainty Maps** Since Rotating Features learn continuous object-centric representations, they can express and process uncertainty in their object separations. We evaluate this by plotting the L2 distance of each normalized output feature to the closest \(k\)-means center in Fig. 8. This reveals higher uncertainty in areas of object overlap, object boundaries and incorrect predictions.

**Training Time** Rotating Features are very efficient to train. On a single Nvidia GTX 1080Ti, the presented Rotating Features model on the Pascal VOC dataset requires less than 3.5 hours of training. For comparison, the DINOSAUR models  were trained across eight Nvidia V100 GPUs.

## 5 Related Work

Numerous approaches attempt to solve the binding problem in machine learning , with most focusing on slot-based object-centric representations. Here, the latent representation within a single layer of the architecture is divided into "slots", creating a discrete separation of object representations. The symmetries between slots can be broken in various ways: by enforcing an order on the slots , by assigning slots to spatial coordinates , by learning specialized slots for different object types , by using an iterative procedure  or by a combination of these methods . Slot Attention , which uses an iterative attention mechanism, has received increasing interest lately with many proposed advancements  and extensions to the video domain . Recently, Seitzer et al.  proposed to apply Slot Attention to the features of a pretrained DINO model, successfully scaling it to real-world datasets.

In comparison, there has been little work on continuous and distributed approaches for object discovery. A line of work has made use of complex-valued activations with a similar interpretation of magnitude and orientation to our proposed Rotating Features, starting from supervised  and weakly supervised methods , to unsupervised approaches . However, they are only applicable to very simple, grayscale data. Concurrent work  proposes a contrastive learning method for the Complex AutoEncoder, scaling it to simulated RGB datasets of simple, uniformly colored objects. With the proposed Rotating Features, new evaluation procedure and higher-level input features, we create distributed and continuous object-centric representations that are applicable to real-world data.

## 6 Conclusion

**Summary** We propose several key improvements for continuous and distributed object-centric representations, scaling them from toy to real-world data. Our introduction of Rotating Features, a new evaluation procedure, and a novel architecture that leverages pre-trained features has allowed for a more scalable and expressive approach to address the binding problem in machine learning.

**Limitations and Future Work** Rotating Features create the capacity to represent more objects at once. However, this capacity can currently only be fully leveraged in the output space. As we show in Appendix D.2, within the bottleneck of the autoencoder, the separation of objects is not pronounced enough, yet, and we can only extract up to two objects at a time. To overcome this limitation, further research into novel inference methods using Rotating Features will be necessary. In addition, when it comes to object discovery results, Rotating Features produce slightly inferior outcomes than the state-of-the-art DINOSAUR autoregressive Transformer model, while still surpassing standard MLP decoders. Nonetheless, we hope that by exploring a new paradigm for object-centric representation learning, we can spark further innovation and inspire researchers to develop novel methods that better reflect the complexity and richness of human cognition.