# ETO:Efficient Transformer-based Local Feature Matching by Organizing Multiple Homography Hypotheses

Junjie Ni\({}^{1}\)   Guofeng Zhang\({}^{1}\)1   Guanglin Li\({}^{1}\)   Yijin Li\({}^{1}\)

**Xinyang Liu\({}^{1}\)   Zhaoyang Huang\({}^{2}\)   Hujun Bao\({}^{1}\)**

\({}^{1}\)State Key Lab of CAD&CG, Zhejiang University  \({}^{2}\)CUHK MMLab

###### Abstract

Recent developments have led to the emergence of transformer-based approaches for local feature matching, resulting in enhanced accuracy of matches. However, the time required for transformer-based feature enhancement is excessively long, which limits their practical application. In this paper, we propose methods to reduce the computational load of transformers during both the coarse matching and refinement stages. During the coarse matching phase, we organize multiple homography hypotheses to approximate continuous matches. Each hypothesis encompasses several features to be matched, significantly reducing the number of features that require enhancement via transformers. In the refinement stage, we reduce the bidirectional self-attention and cross-attention mechanisms to unidirectional cross-attention, thereby substantially decreasing the cost of computation. Overall, our method demonstrates at least 4 times faster compared to other transformer-based feature matching algorithms. Comprehensive evaluations on other open datasets such as Megadepth, YFCC100M, ScanNet, and HPatches demonstrate our method's efficacy, highlighting its potential to significantly enhance a wide array of downstream applications.

## 1 Introduction

Local feature matching  is a fundamental problem in the field of computer vision and plays a significant role in downstream applications, including but not limited to SLAM , 3D reconstruction , visual localization , and object pose estimation . However, traditional CNN-based methods  often fail under extreme conditions due to the lack of a global receptive field, thus meeting failure under dramatic changes in scale, illumination, viewpoint, or weakly-textured scenes.

Recently, some methods  forgo traditional CNN-based approaches and base on Transformer  for better modeling the long-range dependencies. However, Transformer is widely known for its high computational complexity especially when it is applied in vision tasks where the computational complexity grows quadratically in the number of the input image tokens (i.e., patches). To reduce the inherent complexity associated with the Transformer, these methods generally adopt the coarse-to-fine strategy and incorporate more computationally efficient variants of the Transformer, such as the Linear Transformer . Nevertheless, the computational overhead remains substantial and severely hinders the application demanding low-latency operations, such as tracking , or those requiring the processing of extensive datasets, such as large-scale mapping .

In this paper, we propose to solve the efficiency problem of transformer-based local feature matching. Our insights are twofold. First, we propose to introduce the homography hypothesis in the pipeline. The homography hypothesis is a kind of piece-wise smooth prior to the scene that has long been explored in the vision tasks . It allows us to create larger patches and reduce the tokens number that need to be processed in the Transformer. However, it is non-trivial since the regular shape introduced by the homography hypothesis can bring significant errors, especially along the boundary. Besides, how to supervise the training of multiple homography hypotheses with the absence of ground truth remains a problem. Second, We empirically find it is redundant that the previous methods employ multiple self- and cross-attention in their fine-level stage since the coarse-level stage has conducted sufficient propagation. As a result, the computation complexity can be further reduced.

Specifically, we propose ETO, the Efficient Transformer-based Local Feature Matching by Organizing Multiple Homography Hypotheses. ETO follows previous methods [20; 19] and employs a two-stage coarse-to-fine pipeline. It first establishes matches at the patch level and then refines the matches to the sub-pixel level. In the first stage, ETO learns to predict a set of hypotheses, each encompassing multiple patches to be matched. We approximately assume that each patch to be matched within one hypothesis is on the same plane, and thus describe these matches under the homography transformation, as illustrated in Fig. 2. The homography hypotheses allow us to reduce the image tokens (patches) that are fed to the Transformer. For a typical image with a resolution of 640 \(\) 480, Previous methods feed 80 \(\) 60 tokens to the transformer with 1/8 resolution, while we only need to feed 20 \(\) 15 with 1/32 resolution, which brings a significant speed up. To reduce the possible error due to the regular shape of the homography hypotheses, ETO subdivides the patches into multiple sub-patches and re-selects the correct hypothesis for each sub-patch. We model the problem of re-selection as a segmentation problem . After that, ETO refines the matches in the second stage. Unlike previous methods that employ multiple self- and cross-attention, ETO only conducts one cross-attention, and the size of query tokens it use is much smaller than previous methods. We call it uni-directional cross-attention. Empirically we find uni-directional cross attention converges significantly faster at training while providing much higher efficiency. As shown in Fig. 1, ETO outpaces existing methods, achieving 4-5 times faster than LoFTR  and 2-3 times more rapid than LightGlue  while maintaining a comparable accuracy with them.

Our contributions can be summarized as follows. 1) We introduce multiple homography hypotheses for the local feature matching problem, which can greatly compress the number of tokens involved in the transformer. 2) We introduce uni-directional cross-attention in the refinement stage. This structure provides fast inference efficiency while maintaining accuracy. 3) Our method not only matches the performance of other transformer-based approaches on diverse open-source datasets such as Megadepth , YFCC100M , ScanNet , and HPatches , but it also operates at a significantly higher speed, outpacing all compared methods.

## 2 Related Works

**Transformer-based Local Feature Matching.** To find sparse correspondence between two images under diverse viewpoint movement conditions, traditional hand-crafted [34; 1; 2] or early learning-based approaches methods [17; 35] usually match keypoints [36; 37] with their descriptors after

Figure 1: **ETO goes beyond the Pareto curve between accuracy and Efficiency.** This figure shows the performance of different state-of-the-art methods on YFCC100M. We take into account the time for the extraction of feature and their description. LightGlue and LightGlue* are different settings of LightGlue.

Figure 2: As demonstrated in the figure, there exists a correspondence between two red regions on the sphere. In contrast to uniform hypotheses, homography hypotheses approximate the correspondence function better, which allows for more precise matching results with fewer computational resources.

detecting them . These kinds of matching pipelines are highly dependent on the description of unique feature points, and they fail naturally at dramatic viewpoint changes or poorly textured scenes. Thanks to transformer , researchers now have a toolbox to enhance the feature descriptors with global information. Earlier researches  integrate the information that where other key points are to each descriptors of key points. Then, [38; 39] modeling the mapping relationship as a continuous 2-d function. Sun _et al._ constructs a global matching pipeline for each unit on the feature map with Transformer [21; 40; 41; 42]. The following works refine this pipeline with optical flow  or more efficient attention structure . There exists other attempts. [44; 45; 46] follow another technical route without transformer. They try to merge the gap between optical flow [47; 41] and local feature matching with the concept of confidence. Our approach is based on a similar feature extractor of , while we parameterize more information for the units on the feature map, and finally extend the correspondence relationship to the homography relationship between them. The concept of parameterized units on the feature map for local feature matching is introduced by Ni _et al._, but they parameterize the units only with scale.

**Parameterization in Local Feature Matching.** Conventional techniques, as demonstrated in previous works [34; 48; 35], when confronted with appearance differences due to changes in viewpoint, try to construct feature descriptors which are invariant to these changes, involving scale, normal and rotation, etc. For rule-based methods , they create hand-crafted descriptors with scale space analysis . For learning-based methods , they input many image pairs with different viewpoints, letting the neural network learn about the invariance of these changes in appearance. Nevertheless, to fully mitigate the impact of these appearance changes, it is essential to accurately estimate their effects. Recent efforts [38; 18; 50] have attempted to directly estimate the scale differences between images and resize the images to enhance the refinement stage of feature matching. In a similar route,  has focused on directly estimating rotation to calibrate the local features. Unlike the methods aimed solely at discovering more invariant feature descriptors, we argue that parameterized local appearance changes not only pose a challenge in finding accurate matches but also provide a direct avenue to achieve more accurate and more efficient matches over a broader spatial extent. It allows us to locally parameterize the image to many planes, thus creating multiple homography relationships, not just point-to-point correspondence.

**Acceleration in Local Feature Matching.** In the realm of local feature matching based on the Transformer architecture, three predominant strategies have been mainly employed to enhance computational efficiency in the past. The first approach involves substituting the Softmax function for the Optimal Transport algorithm introduced by . The second approach seeks to replace the full Transformer with a linear Transformer . LoFTR  incorporates both of these strategies, however, it introduces a larger number of matching units compared to SuperGlue , resulting in a considerably slower performance than . The third strategy focuses on reducing the number of layers and units within the Transformer. LightGlue  introduces early termination [52; 53] and progressive unit selection strategies to accelerate computation, yielding significant improvements in speed. However, the performance of LightGlue heavily relies on SuperPoint , which puts a ceiling on its acceleration. In contrast, ETO relies on a more precise parameterized model, achieving higher coarse matching accuracy with a feature map whose resolution is 16 times smaller than LoFTR , which distinguishes ETO from another concurrent work Efficient LoFTR  that accelerates the algorithm with adaptive token selection on the feature map of the same resolution. Furthermore, during the fine matching stage, we introduce a uni-directional cross-attention mechanism, allowing us to achieve higher matching speed while sacrificing only a minimal amount of fine stage accuracy.

## 3 Method

Fig. 3 presents our comprehensive feature matching process, organized into three structured modules. These modules are interconnected through feature extractors inspired by U-Net  and local attributes generated by neural networks. For each unit \(i\) on a H/32 \(\) W/32 resolution feature map \(M_{1}\), we estimate the attributes of homography hypotheses \(H_{i}\). For each unit \(j\) on H/8\(\)W/8 resolution feature map \(M_{2}\), it re-selects the optimal homography hypotheses \(_{j}\) from nearby 9 hypotheses to minimize the projection errors. For the chosen unit \(k_{j}\) on a H/2\(\)W/2 resolution feature map \(M_{3}\), we fix its center point \(P^{s}_{j}\) at the source image and refine the coordinates of its projected point \(P^{t}_{j}\) at target image according to \(_{j}\), then get the final matches \(P^{t*}_{j}\). We introduce the feature extractor in Sec. 3.1, Sec. 3.2 details the estimation of the hypotheses. In Sec. 3.3 we describe the segmentation of the feature map, Sec. 3.4 delves into refining the matches, while Sec. 3.5 states our supervision methodology.

### Feature Extraction

Following , we use ResNet-18  as the basic feature extractor to get the feature map with the resolution of \(N=/32/32\), while we assume that the resolution of the source image and target image is the same. Here we get \(N=/32/32\) features and then perform stacked self-attention and cross attention layers between these \(N\) tokens to compute the feature map \(M_{1}\). Although there will be \(N\) (more than 9) possible patches, our method is mainly performed on locally adjacent patches, so we will omit N in the future and take the local 3\(\)3 patches to illustrate our method (as shown in Fig. 3). Then, we follow  to upsample the feature map \(M_{1}\) with a U-Net  like structure. So, we can obtain the feature maps \(M_{2}\) and \(M_{3}\) at \(1/8\) and \(1/2\) scale, respectively.

### Hypothesis Estimation

Traditional semi-dense feature matching methods [20; 19] often divide an image into thousands of units. For each unit, they perform bipartite graph matching . Contrarily, we argue that bipartite graph matching can be extended to the local homography transformation as hypotheses that cover multiple units to be matched. This approach's merit lies in two folds: achieving more precise matches estimated during the first stage and reducing the number of units which are involved in transformer.

For each unit \(i\) on \(M_{1}\) of source images, it is equipped with a feature \(f_{i}^{1}\), a confidence score \(c_{i}\), and a set of hypothesis homography parameters \(H_{i}\) (including the source positions \(p_{i}^{t}^{2}\), target positions \(p_{i}^{t}^{2}\), rotation \(r_{i}^{1}\), scale \(s_{i}^{1}\) and perspective \(q_{i}^{4}\)). And the unit on target images is indicated by \(a\).

**Homography Matrix.** Initially, we outline the methodology for estimating each unit's local attributes (\(p_{i}^{s},p_{i}^{t},r_{i},s_{i},q_{i},c_{i}\)) and subsequently use these attributes to formulate the local homography matrix. Among these local attributes, the scale \(s_{i}\), rotation \(r_{i}\), and perspective \(q_{i}\) are more related to the feature itself and are regressed directly from features \(f_{i}\) on the source image through an MLP network. In contrast, target coordinates \(p_{i}^{t}\), and confidence scores \(c_{i}\) are more related with the feature map of target images. They are acquired by first identifying the unit \(a_{i}^{*}\) with maximal similarity among all target units, and the similarity is defined as the cosine similarity of initial features \(f\) on the source image and target image. Following this, we construct new features \(\) by executing the group-wise correlation  within the neighborhood of the target units on \(M_{1}\).

\[_{i}=^{*})}{}(<f_{i}^{1},f_{ }^{1}>_{g}). \]

where \(<*,*>_{g}\) is the group-wise correlation , and the group size in our method is 8, \(Neighbor\) represents the 5\(\)5 neighborhood of unit \(a_{i}^{*}\), \(\) indicates the operation of concatenation. With the new features \(\), we use an MLP to process it to get the target position \(p_{i}^{t}\) and confidence \(c_{i}\). With these attributes, in order to compute the homography matrix, we establish four target points \(B_{i}^{t}\) that correspond to four predetermined reference points:

\[B_{i}^{t}=p_{i}^{t}+((B_{i}^{s},q_{i}),r_{i})*s_{i}. \]

Figure 3: Given the source image \(\) and target image \(\), we first use a U-Net like feature extractor to get images’ feature map at different resolution: \(M_{1}\) (H/32 \(\) W/32), \(M_{2}\) (H/8 \(\) W/8) and \(M_{3}\) (H/2 \(\) W/2). We use local \(3 3\) patches to illustrate our method: **(a)** We estimate homography hypotheses \(H_{i}\) for every feature after performing transformer. **(b)** We segment the map from these hypotheses to minimize projection errors. With the applied homography matrix \(_{j}\), we can project the chosen source point \(P_{j}^{s}\) to target point \(P_{j}^{t}\). **(c)** We update the \(P_{j}^{t}\) to \(P_{j}^{t*}\) after a uni-directional cross attention. The training process is split into two parts, the coarse and the fine. We train the coarse part with \(L_{H}\), while training the fine part with \(L_{s}\) and \(L_{r}\).

Here \(B^{s}_{i}\) are four imaginary points on source image, while \(B^{t}_{i}\) are the corresponding target points of \(B^{s}_{i}\) on the target image. \(B^{t}_{i}\) is computed from \(B^{s}_{i}\) by following operation: \(\) is the operation of rotation with parameter \(r_{i}\), \(\) is the perspective transformation operation with parameter \(q_{i}\). These operations allow each variable within the homography matrix \(H_{i}\) to be deduced from 8 projection equations of 4 correspondence \(B^{t}_{i}=H_{i}B^{s}_{i}\). Details regarding the specific implementation methods for rotation and perspective transformations will be included in the supplementary materials.

### Segmentation

To propagate the homography hypotheses predicted by \(M_{1}\) to a more detailed resolution. We introduce a segmentation operation at the feature map \(M_{2}\) with the resolution \(/8/8\). Segmentation is a per-unit classification task, and we predict a class for each unit \(j\) on \(M_{2}\). Here, we only consider locally adjacent \(3 3\) patches, and all possible classes is defined as \(=\{H_{i}|i=1...9\}\). This classification (segmentation) involves that, for each unit \(j\), selecting a hypothesis from \(\) that minimizes the projection error at the center of unit \(j\). After our proposed segmentation stage, each sub-unit \(j\) can find the hypotheses \(_{j}\) that make its error smallest in all possible hypotheses \(\). We illustrate the intuitive process of segmentation at Fig. 4.

Our proposed **segmentation** differs from traditional semantic segmentation. Instead of aiming for a specific semantic category, it targets a dynamic geometric relationship. To find the relationship, we introduce a new cosine similarity matrix \(C_{j}\) between the local feature \(f^{2}_{j}\) on \(M_{2}\) and all features \(f^{1}_{i}\) on \(M_{1}\), Positional encoding is employed during this phase to enhance local features, which is indispensable here because the hypothesis in \(\) are not equivalent. To predicting the class \(_{j}\) by finding the maximum \(C_{j}\), we generate the computed groundtruth \(_{j}\) as follows:

\[_{j}=}{argmin}\,||P^{t}_{j}-H_{i}P^{s }_{j}||. \]

where \(_{j}\) is the optimal hypothesis that minimize the projection error. Then, we use focal loss \(L_{s}\) to minimize the segmentation error between predicted \(_{j}\) and the computed groundtruth \(_{j}\). The probability of focal loss is set to \(C_{j}\).

### Refinement

Following , to enhance efficiency, only one of the points within each unit \(j\) is selected for refinement, which we denote the source and target point as \(P^{s}_{j}\) and \(P^{t}_{j}\). Given \(P^{t}_{j}=_{j}P^{s}_{j}\), the refinement stage finds the offset \( P^{t}_{j}\) of each target point \(P^{t}_{j}\) relative to a fixed source point \(P^{s}_{j}\). With the feature \(f^{3}\) from \(M_{3}\), conventional techniques unfold features \(f^{3}_{k}\) from local regions in both source image and target image, followed by self-attention and cross-attention. We claim that this process is unnecessarily slow. Here we eliminate self-attention and reduce cross-attention from a bi-directional process to a uni-directional one. Specifically, the feature \(^{3}_{j}\) is computed by querying the features \(f^{3}_{k}\) within the neighborhood of the original target point \(P^{t}_{j}\), and become the final feature vectors \(^{3}_{j}\) for the point \(P_{j}\) on the images. We illustrate this process in supplementary materials. With the proof of experiments in Sec. 4.4, we find that refining a single feature in the local region of \(M_{2}\) is enough to get expected results. Our findings indicate that this approach can largely diminish the computational load of attention mechanisms while still preserving highly accurate matching outcomes.

Finally, following , we process one fixed element of the final feature vector \(^{3}_{j}\) via cross-attention as the confidence score \(c_{j}\) for the corresponding set of matches. It is supervised by if the error final match is larger than a threshold. The coordinates for the matched pairs on the source and target

Figure 4: Any unit \(j\) on \(M_{2}\) should be classified for a hypotheses in \(\) to minimize projection error. Each \(H_{i}\) describes a plane.

images are \((P^{s}_{j},P^{t*}_{j})\). Therefore we can define the supervision of refinement as:

\[L_{r}=|P^{t*}_{j}-^{t}_{j}|_{2}+BCE(c_{j}). \]

where BCE is binary cross entropy , which is a commonly utilized loss function for binary classification problems. Here we use BCE to recognize reliable matches. Here \(^{t}_{j}\) is the ground truth value of \(P^{t*}_{j}\).

### Supervision

**Indirect supervision for the homography hypothesis.** Instead of supervising the attributes of the hypothesis directly, our approach employs indirect supervision by monitoring the correspondences of sampling points that are linked via the homography transformation. This design offers the advantage of leveraging an excessive number of ground truth matches to efficiently train a network focused on estimating a set of homography parameters. Using the ground truth camera pose and depth from datasets, we can get real matched points \(^{s}\) in source and \(^{t}\) in target images. We sample the matched points to train our method. For each points \(p\) in \(^{s}\), we select only 3\(\)3 adjacent hypotheses \(=\{H_{1},...,H_{9}\}\) around it on \(M_{1}\). where \(H_{5}\) is in the hypothesis region's center and represents the hypothesis of the region containing point \(p\). Similar to the necessity of segmentation stated in Sec. 3.3, direct supervision which applies \(H_{5}\) to every \(p\) could result in avoidable errors, which arise from the mismatch between the irregular boundaries of planes in the real world and the grid-structured unit \(i\) on the source image. However, given that these sampling points are merely an auxiliary tool for the loss function, we can directly utilize the ground truth coordinates of the matches to supervise and eliminate classifying each sampling point at this stage. For each point \(p\), we assume that it satisfies a certain homography transformation \(_{p}\), and \(_{p}\) satisfies the following defination:

\[_{p}=}{argmin}\,|H_{i}p^{s}-p^{t}|_{ 1}. \]

We denote \(p\) in the source image as \(p^{s}\) and target image as \(p^{t}\), we use the following error to optimize our method:

\[e_{p}=|_{p}p^{s}-p^{t}|_{1}. \]

where \(|*|_{1}\) is the L1 norm error.

**Classification or Correspondence Loss for Hypotheses.**\(H_{i}\) is calculated on the base of identifying the matched unit \(a^{*}_{i}\) for unit \(i\) in target image. Therefore, if the estimated \(a^{*}_{i}\) significantly deviates from the ground truth \(_{i}}\), \(H_{i}\) would be entirely incorrect. In such conditions, we use classification loss to enhance the feature similarity between \(a^{*}_{i}\) and \(_{i}}\). In the opposite case, directly supervising the point correspondences calculated through \(H_{i}\) yields better results. The methodology is detailed as follows:

\[ q_{1}&=\{i|_{1}<|a^{*}_{i}- ^{*}_{i}|_{}\},\\ q_{2}&=\{i|_{1}|a^{*}_{i}-^{*}_{i} |_{}\},\\ L_{H}&=1-CosSim(f^{1}_{i},f^{1}_{a^{* }_{i}})&,i q_{1},\\ _{p P_{i}}e_{p}&,i q_{2}, \]

where \(|*|_{}\) denotes the computation of infinity norm, \(CosSim\) is the cosine similarity of two features in the feature map \(M_{1}\). \(P_{i}\) is the set of sampled points \(p\) that apply \(H_{i}\) as \(_{p}\).

**Two-stage Training Process.** In the entire feature matching process, we divide the training process into two stages, the coarse stage and the fine stage. The fine stage will freeze all the parameters of the coarse stage during training. The coarse stage includes the homography hypotheses estimation, while the fine stage includes segmentation and refinement. The losses used in these two parts are:

\[ L_{coarse}=L_{H},\\ L_{fine}=L_{s}+L_{r}. \]

### Implementation Details

For feature extracting, we use Resnet-18 , then we perform transformer  five times at \(M_{1}\). We implement uni-directional cross-attention once in the process of refinement. We train our outdoor model and indoor model respectively. The outdoor model is trained on the Megadepth  dataset, while the indoor model is trained on a mixed dataset of Megadepth and ScanNet . The training process is divided into three stages: the first stage is training on data of 640x480 resolution; in the second stage, the longer side is scaled to 640, and some images are rotated by 90 degrees for adaptation training for the coarse; the third stage involves training the fine on 640x480 data. The learning rate used in the first stage is 1e-4. In the second stage is 5e-5, and in the third stage is 3e-4. Both models are trained using three RTX 3090 for 80 hours, with a batch size of 24 in the first stage and 16 in both the second and third stages. We perform all inferences using PyTorch, merely following the implementation of LightGlue to pre-compile the transformer in the coarse stage with PyTorch.

## 4 Experiment

We conducted these evaluations on four different datasets for outdoor and indoor relative pose estimation and homography estimation. These experiments demonstrate superior performance on various downstream tasks.

### Homography Estimation

As our first experiment, we evaluate our quality of correspondences and the ability to fit the homography matrix for planar scenes on the HPatches  dataset.

**Experimental Setup.** We conducted comparative experiments using the image matching toolbox proposed by . Our experiments were configured to replicate the settings outlined for SuperPoint, SuperGlue , and LoFTR  as shown in this toolbox. For LightGlue , we follow their open-source code settings. To estimate the homography, we employed the RANSAC algorithm with a threshold of 0.25 pixels, leveraging the OpenCV library. To comprehensively assess the performance of each method, we considered three key metrics: the proportion of matched points with an error within a 1-pixel threshold, the average corner distance for estimated homography matrices measuring less than 1/3/5 pixels, and the average computational time. These metrics were chosen to simultaneously evaluate the matching accuracy, homography estimation precision, and computational efficiency of the methods. We perform this experiment on a RTX2070 GPU, and we turn off all acceleration options for pytorch implementations, such as flash attention and precompilation. In order to get as close as possible to a real usage scenario, here, we do not use a warm-up operation when measuring the computing speed.

**Dataset.** HPatches  contains 52 sequences under significant illumination changes and 56 sequences that exhibit large variations in viewpoints. All images are resized with longer dimensions equal to 640.

**Results.** We compare ETO with SuperPoint , SuperGlue , LightGlue  and LoFTR . According to Table. 1, our experimental results demonstrate that our method excels in homography estimation accuracy compared to SuperGlue, achieving lower errors within a 1-pixel threshold when compared to both SuperGlue and LightGlue. Furthermore, our approach is significantly faster, outpacing all other methods several times in the evaluation.

### Outdoor Pose Estimation

We assess the efficacy of our approach for relative pose estimation in the same setting using two distinct datasets: YFCC100M  and Megadepth  for outdoor scenes.

    &  &  & time \\   & 1px(\%) & 3px(\%) & 5px(\%) & 1px(\%) & ms \\  LoFTR  & 46 & 77 & 86 & 63 & 218 \\ SP +LG  & 44 & 73 & 85 & 51 & 101 \\ SP +SG  & 41 & 72 & 82 & 47 & 79 \\ SP +search & 38 & 68 & 81 & 32 & 81 \\  Ours & 42 & 72 & 82 & 52 & **53** \\   

Table 1: Evaluation on HPatches  for homography estimation.

**Experimental setup.** We report the pose accuracy in terms of AUC metric at multiple thresholds (\(5^{}\),\(10^{}\),\(20^{}\)) and runtime for every approach, and the RANSAC threshold here is set as 0.25 pixel for all methods. All of the evaluations here are conducted on a RTX2080ti. We turn on flash-attention for LightGlue and turn on the pre-compilation to accelerate the transformer for LightGlue and ETO. Here LightGlue  is slower than SuperGlue  for the reason that following the default configuration LightGlue extracts 2048 keypoints and resizes the resolution of images to 1024, while SuperGlue extracts only 1024 keypoints and keep the resolution of images as the same. LightGlue* apply the setting of SuperGlue. It is imperative to highlight that our method encompasses 4800 points to be matched here, which is the same as LoFTR , ASpanFormer , and Quadtree . To ensure an accurate representation of the actual computation speed, we initiate a warm-up phase for each method, consisting of 10 iterations, prior to conducting measurements.

**Dataset.** YFCC100M  encompasses an extensive repository comprising 100 million media assets. For our evaluation, we following  and focus on a subset of YFCC100M, specifically four handpicked image collections featuring prominent landmarks, in accordance with the criteria outlined in  and . MegaDepth, on the other hand, comprises a dataset containing one million Internet-sourced images depicting 196 distinct outdoor scenes. To ensure the integrity of our evaluation protocol, in line with the guidelines presented in , we randomly select 1000 image pairs, guaranteeing that none of these pairs have been used in the training processes of any existing methods. All images in Megadepth and YFCC100M are resized with a resolution equal to 640*480.

**Results.** We compare ETO with SuperPoint , SuperGlue , LightGlue , LoFTR , ASpanFormer  and Quadtree . According to the results shown in Table 2, on the easier outdoor cases in MegaDepth, the accuracy of our method for pose estimation is lower than advanced detector-free method but is higher than any detector-based approaches, while our runtime is at most 23% of the detector-free methods, 81% of the detector-based methods and 90% of the CNN-based methods. While on the more difficult outdoor cases in YFCC100M, the performance of our model is much better than detector-based methods and is comparable with detector-free methods. And still, our superiority on runtime is preserved.

### Indoor Pose Estimation

We evaluate our method for indoor pose estimation with ScanNet-1500  following [36; 20].

**Experimental Setup.** Just like outdoor cases, we report the pose accuracy in terms of the AUC metric at multiple thresholds (\(5^{}\),\(10^{}\),\(20^{}\)) and runtime for every approach. However, here we set all of the RANSAC thresholds as 0.5 pixels. All of the images are resized with longer dimensions equal to 640. This evaluation is conducted on RTX2080ti. We have done a warm-up here in measuring the efficiency.

**Dataset.** The ScanNet dataset represents a comprehensive indoor RGB-D collection encompassing 1,613 distinct sequences that cumulatively offer 2.5 million unique views. Each view within this dataset is meticulously annotated with a corresponding ground truth camera pose and depth map. We follow the same training and testing split used by .

**Results.** We compare our approach with SuperPoint , SuperGlue , LoFTR , ASpanFormer  and Quadtree . The results are demonstrated in Table 3. We find that our results are comparable with LoFTR and are superior to SuperPoint+search and SuperPoint+SuperGlue, while much faster than any other methods.

    &  &  \\   & @\(5^{}\) & @\(10^{}\) & @\(20^{}\) & ms & @\(5^{}\) & @\(10^{}\) & @\(20^{}\) & ms \\  ASpanFormer  & 58.6 & 72.2 & 81.7 & 158.5 & 44.5 & 63.5 & 78.1 & 155.5 \\ Quadtree  & 58.6 & 72.1 & 81.5 & 147.6 & 44.7 & 63.9 & 78.2 & 159.4 \\ LoFTR  & 57.5 & 71.2 & 80.8 & 93.2 & 44.7 & 63.6 & 78.3 & 96.3 \\ SP +LG  & 51.5 & 67.7 & 78.9 & 64.2 & 36.1 & 56.2 & 73.1 & 60.8 \\ SP +LG*  & 47.1 & 64.0 & 77.3 & 26.9 & 29.2 & 48.8 & 67.0 & 27.2 \\ SP +SG  & 43.2 & 60.0 & 72.8 & 43.9 & 29.7 & 49.6 & 67.9 & 48.7 \\ SP +search & 28.8 & 43.4 & 56.6 & 23.7 & 14.0 & 27.0 & 42.2 & 24.6 \\ RoMa  & 64.8 & 77.4 & 86.1 & 689 & * & * & * & * \\ Tiny-RoMa  & 36.2 & 53.6 & 67.5 & 29.0 & * & * & * & * \\  Ours & 51.7 & 66.6 & 77.4 & **21.0** & **44.8** & **64.0** & **78.8** & **22.1** \\   

Table 2: Evaluation on Megadepth  and YFCC100M  for outdoor pose estimation.

### Ablation Studies

To evaluate the impact of each design component on the overall structure, we perform an ablation study using the MegaDepth dataset. We systematically add each design element one at a time. The quantitative results are presented in Table 4.

**Base32 w/o Homography** We match the units on \(M_{1}\) at H/32 * W/32 resolution and permit the target of unit centroid to be continuous, and we can compute it as Section. 3.2, while other parameters for the unit are still fixed. We output four virtual correspondences as matches. While it offers rapid processing, it does not achieve a high level of accuracy.

**Base8 w/o Homography** We set this ablation experiment as the coarse matching of LoFTR . Here we match every possible 8*8 units. We output the center of corresponding units as matches. It is more accurate but too slow.

**Base32 w/ Homography.** Following Section. 3.2, we estimate the whole homography matrix and output four virtual correspondences as matches. It performs better than the coarse matching of LoFTR  while providing higher efficiency at the same time,

**Basic Refinement w/ Segmentation.** Following , we set a layer of transformer between 25 tokens on these two images and try to refine our results while the transformer is trained for 12 hours, which is the same as the training time of our uni-directional attention for the refinement stage. While full attention execution speed is considerably slower than that of uni-directional attention, its accuracy is merely comparable with the latter.

**Uni-directional w/o Segmentation.** Here we directly choose the homography hypotheses \(H_{5}\) for each unit \(j\) which is in the center. Then, we conduct the refinement as the same. The results show that the segmentation stage significantly improves the accuracy.

## 5 Conclusion and Limitations.

In this paper, we propose Efficient Transformer-based Local Feature Matching by Organizing Multiple Homography hypotheses (ETO). ETO tries to approximate a continuous corresponding function with multiple homography hypotheses with fewer tokens fed to the transformer. Multiple datasets demonstrate that ETO delivers nearly comparable performance in relative pose estimation and homography estimation with other transformer-based methods, while its speed surpasses all of them by a large margin. However, there remains significant space for improvement in ETO's matching accuracy. Next, we could explore an end-to-end training mode, which would allow for further enhancement of the feature extractor at a fine-grained level. Moreover, we believe that intermediate-level features can provide not only segmentation information but also data conducive to more precise matching. Finally, the form of parametric scheme we present here may not be optimal and complete for homography transformation, so we will continue to explore better parametric schemes. These strategies are expected to enable our method to compete with approaches like PATS  and DKM  in terms of matching precision, without considerably compromising speed.

**Acknowledgements.** This work was partially supported by NSF of China (No. 61932003).

   } &  &  \\   & @5° & @10° & @20° & ms \\   ASpanFormer  & 24.5 & 45.0 & 62.8 & 160.0 \\ Quadtree  & 23.9 & 43.0 & 60.2 & 145.9 \\ LoFTR  & 21.4 & 40.3 & 57.2 & 94.2 \\ SuperPoint +SuperGlue  & 13.7 & 29.8 & 47.2 & 63.1 \\ Superpoint +search & 8.0 & 18.3 & 29.8 & 27.1 \\  Ours & 20.1 & 40.4 & 59.8 & **24.2** \\   

Table 4: Ablation study based on Megadepth  for outdoor pose estimation.

   } &  &  \\   & @5° & @10° & @20° & ms \\  Base32 w/o Homography & 9.5 & 21.3 & 36.3 & 8.5 \\ Base8 w/o Homography (LoFTR coarse) & 25.7 & 41.8 & 57.7 & 58.9 \\ Base32 w/ Homography & 28.5 & 46.2 & 61.4 & 8.5 \\ Basic Refinement w/ segmentation & 51.0 & 66.6 & 77.6 & 32.8 \\ Uni-directional w/o segmentation & 42.1 & 59.0 & 72.0 & 21.2 \\ Full & 51.7 & 66.6 & 77.4 & 22.0 \\   

Table 3: Evaluation on Scannet  for indoor pose estimation.