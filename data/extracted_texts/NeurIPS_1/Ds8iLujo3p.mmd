# Ecosystem-level Analysis of Deployed Machine Learning Reveals Homogeneous Outcomes

Connor Toups

Stanford University

Equal contribution.

&Rishi Bommasani

Corresponding author: nlprishi@stanford.edu.

Kathleen A. Creel

Northeastern University

&Sarah H. Bana

Chapman University

&Dan Jurafsky

Stanford University

&Percy Liang

Stanford University

###### Abstract

Machine learning is traditionally studied at the model level: researchers measure and improve the accuracy, robustness, bias, efficiency, and other dimensions of specific models. In practice, however, the societal impact of any machine learning model depends on the context into which it is deployed. To capture this, we introduce _ecosystem-level analysis_: rather than analyzing a single model, we consider the collection of models that are deployed in a given context. For example, ecosystem-level analysis in hiring recognizes that a job candidate's outcomes are determined not only by a single hiring algorithm or firm but instead by the collective decisions of all the firms to which the candidate applied. Across three modalities (text, images, speech) and eleven datasets, we establish a clear trend: deployed machine learning is prone to _systemic failure_, meaning some users are exclusively misclassified by all models available. Even when individual models improve over time, we find these improvements rarely reduce the prevalence of systemic failure. Instead, the benefits of these improvements predominantly accrue to individuals who are already correctly classified by other models. In light of these trends, we analyze medical imaging for dermatology, a setting where the costs of systemic failure are especially high. While traditional analyses reveal that both models and humans exhibit racial performance disparities, ecosystem-level analysis reveals new forms of racial disparity in model predictions that do not present in human predictions. These examples demonstrate that ecosystem-level analysis has unique strengths in characterizing the societal impact of machine learning.1

## 1 Introduction

Machine learning (ML) is pervasively deployed. Systems based on ML mediate our communication and healthcare, influence where we shop or what we eat, and allocate opportunities like loans and jobs. Research on the societal impact of ML typically focuses on the behavior of individual models. If we center people, however, we recognize that the impact of ML on our lives depends on the aggregate result of our many interactions with ML models.

In this work, we introduce _ecosystem-level analysis_ to better characterize the societal impact of machine learning on people. Our insight is that when a ML model is deployed, the impact on users depends not only on its behavior but also on the behavior of other models and decision-makers (left of Figure 1). For example, the decision of a single hiring algorithm to reject or accept a candidate doesnot determine whether or not the candidate secures a job; the outcome of her search depends on the decisions made by all the firms to which she applied. Likewise, in selecting consumer products like voice assistants, users choose from options such as Amazon Alexa, Apple Siri, or Google Assistant. From the user's perspective, what is important is that at least one product works.

In both settings, there is a significant difference from the user's perspective between _systemic failure_, in which _zero_ systems correctly evaluate them or work for them, and any other state. The difference in marginal utility between zero acceptances and one acceptance is typically much higher than the difference between one acceptance and two acceptances. This non-linearity is not captured by average error metrics. For example, imagine that three companies are hiring and there are ten great candidates. All three companies wrongly reject the same two great candidates for a false negative error rate of 20%. Now imagine that each company wrongly rejects _different_ candidates. The second decision ecosystem has the same false negative error rate, 20%, but no systemic failures or jobless candidates.

Ecosystem-level analysis is a methodology that centers on the _failure matrix_ (\(F\); right of Figure 1): \(F\) encapsulates the outcomes individuals receive from all decision-makers. Of special interest are _systemic failures_: exclusively negative outcomes for individuals such as misclassifications or rejections from all decision-makers [Bommasani et al., 2022]. To establish general trends in deployed machine learning, we draw upon a large-scale audit [**HAPI**; Chen et al., 2022a] that spans three modalities (images, speech, text), three commercial systems per modality, and eleven datasets overall. Because **HAPI** contains predictions from some of the largest commercial ML providers - including Google, Microsoft, and Amazon - and the models evaluated are deployed models that real users interact with, evaluating **HAPI** has real-world implications.

Across all settings, ecosystem-level analysis reveals a consistent pattern of _homogeneous outcomes_. In each of the **HAPI** datasets, many instances are classified correctly by all three commercial systems and many instances are classified incorrectly by all three systems. The pattern of outcomes across the decision ecosystem is _homogenous_ if the rates of systemic failure and consistent classification success significantly exceed the rate predicted by independent instance-level behavior. Since the commercial systems analyzed in this dataset are popular and widely used, being failed by all systems in the dataset has societally meaningful consequences.

Ecosystem-level analysis enriches our understanding not only of the status quo, but also of how models change over time. In particular, it allows us to ask, when individual models improve, how do ecosystem-level outcomes change? Since **HAPI** tracks the performance of the same systems over a three-year period, we consider all cases where at least one of the commercial systems improves. For example, Amazon's sentiment analysis API reduced its error rate on the waimai dataset by 2.5% from 2020 to 2021; however, this improvement did not decrease the systemic failure rate at all. Precisely 0 out of the model's 303 improvements are on instances on which all other models had failed. These findings generalize across all cases: on average, just 10% of the instance-level improvement of a single commercial system occurs on instances misclassified by all other models. This is true even though systemic failures account for 27% of the instances on which the models _could have_ improved. Thus most model improvements do not significantly reduce systemic failures.

Figure 1: **Ecosystem-level analysis. Individuals interact with decision-makers (_left_), receiving outcomes that constitute the failure matrix (_right_).**

To build on these trends, we study medical imaging, a setting chosen because the costs to individuals of systemic failure of medical imaging classification are especially high. We compare outcomes from prominent dermatology models and board-certified dermatologists on the **DDI** dataset (Daneshjou et al., 2022): both models and humans demonstrate homogeneous outcomes, though human outcomes are more homogenous. Given established racial disparities in medicine for both models and humans, fairness analyses in prior work show that both humans and models consistently perform worse for darker skin tones (e.g. Daneshjou et al. (2022) show lower ROC-AUC on **DDI**). Ecosystem-level analysis surfaces new forms of racial disparity in models that do not present in humans: models are more _homogenous_ when evaluating images with darker skin tones, meaning that all systems agree in their correct or incorrect classification, whereas human homogeneity is consistent across skin tones.

Our work contributes to a growing literature on the _homogeneous outcomes_ of modern machine learning (Ajunwa, 2019; Engler, 2021; Creel and Hellman, 2022; Bommasani et al., 2022; Fishman and Hancox-Li, 2022; Wang and Russakovsky, 2023; Jain et al., 2023). While prior work conceptualizes these phenomena, our work introduces new methodology to study these problems and provides concrete findings for a range of ML deployments spanning natural language processing, computer vision, speech, and medical imaging. Further, by centering individuals, we complement established group-centric methods (Barocas and Selbst, 2016; Buolamwini and Gebru, 2018; Koenecke et al., 2020), unveiling new forms of racial disparity. Ecosystem-level analysis builds on this existing work, providing a new tool that contributes to holistic evaluations of the societal impact of machine learning.

Developing better methodologies for ecosystem-level analysis of deployed machine learning systems is important for two reasons. First, systemic failures in socially consequential domains could exclude people from accessing goods such as jobs, welfare benefits, or correct diagnoses. Individuals who are failed by only one model can gain informal redress by switching to another model, for example by seeking second doctor's opinion or switching banks. Individuals failed by _all_ models cannot. Socially consequential systemic failures can happen due to reliance on APIs, such as image recognition APIs used to identify cancers, speech recognition APIs used to verify individuals for banking, or facial recognition APIs used to unlock devices. Systemic failures can also occur in algorithmic decision-making systems such as those used for hiring, lending, and criminal justice. The social importance of avoiding systemic failures in all of these systems is clear.

Second, as decision-makers become more likely to rely on the same or similar algorithms to make decisions (Kleinberg and Raghavan, 2021) or to use the same or similar components in building their decision pipelines (Bommasani et al., 2022), we believe that the prevalence of systemic failures could increase. Measuring systemic failures as they arise with the tools we present in this paper will expand our understanding of their prevalence and likely causes.

## 2 Ecosystem-level Analysis

How individuals interact with deployed machine learning models determines ML's impact on their lives. In some contexts, individuals routinely interact with multiple ML models. For example, when a candidate applies to jobs, they typically apply to several firms. The decision each company makes to accept or reject the candidate may be mediated by a single hiring algorithm. In other contexts, individuals select a single model from a set of options. For example, when a consumer purchases a voice assistant, they typically choose between several options (e.g. Amazon Alexa, Google Assistant, Apple Siri) to purchase a single product (e.g. Amazon Alexa). Centering people reveals a simple but critical insight: exclusively receiving negative outcomes, as when individuals are rejected from every job or unable to use every voice assistant, has more severe consequences than receiving even one positive outcome.

### Definition

Recognizing how ML is deployed, we introduce _ecosystem-level analysis_ as a methodology for characterizing ML's cumulative impacts on individuals. Consider \(N\) individuals that do, or could, interact with \(k\) decision-makers that apply \(\) labels according to their decision-making processes \(h_{1},,h_{k}\). Individual \(i\) is associated with input \(x_{i}\), label \(y_{i}\), and receives the label \(_{i}^{j}=h_{j}(x_{i})\) from decision-maker \(j\).

Outcomes.Define the _failure matrix_\(F\{0,1\}^{N k}\) such that \(F[i,j]=[_{i}^{j} y_{i}]\). The _failure outcome profiles_\(_{i}\) for individual \(i\), which we refer to as the outcome profile for brevity, denotes \(F[i,:]\).

The _failure rate_\(_{j}\) for decision-maker \(j\) is \(_{j}=^{N}F[i,j]}{N}\) (i.e. the empirical classification error in classification). For consistency, we order the entries of decision-makers (and, thereby, the columns of the failure matrix) in order of ascending failure rate: \(F[:,1]\) is the outcome profile associated with the decision-maker with the fewest failures and \(F[:,k]\) is the outcome profile associated with the decision-maker with the most failures. The failure matrix is the central object in ecosystem-level analysis (see Figure 1).

Systemic Failures.Individual \(i\) experiences _systemic failure_ if they exclusively experience failure across the domain of interest: \(F[i,:]=[1,,1]\). Not only are systemic failures the worst possible outcomes, but they also often result in additional harms. If an individual applying to jobs is rejected everywhere, they may be unemployed. If no commercial voice assistant can recognize an individual's voice, they may be fully locked out of accessing a class of technology. In our ecosystem-level analysis, we focus on systemic failures as a consequential subset of the broader class of _homogeneous outcomes_Bommasani et al. (2022).

## 3 Homogeneous Outcomes in Commercial ML APIs (HAPI)

To establish general trends made visible through ecosystem-level analysis, we draw upon a large-scale three-year audit of commercial ML APIs (**HAPI**; Chen et al., 2022a) to study the behavior of deployed ML systems across three modalities, eleven datasets, and nine commercial systems.

### Data

Chen et al. (2022a) audit commercial ML APIs, tracking predictions across these APIs when evaluated on the same eleven standard datasets over a period of three years (2020 - 2022). We consider ML APIs spanning three modalities (text, images, speech), where each modality is associated with a task (SA: sentiment analysis, FER: facial emotion recognition, SCR: spoken command recognition) and 3 APIs per modality (e.g. IBM, Google, Microsoft for spoken command recognition). The models evaluated are from Google (SA, SCR, FER), Microsoft (SCR, FER), Amazon (SA), IBM (SCR), Baidu (SA), and Face++ (FER). Additionally, each modality is associated with three to four datasets, amounting to eleven datasets total; further details are deferred to the supplement.

To situate our notation, consider the digit dataset for spoken command recognition and the associated APIs (IBM, Google, Microsoft). For each instance (i.e. image) \(x_{i}\) in digit, the outcome profile \(_{i}\{0,1\}^{3}\) is the vector of outcomes. The entries are ordered by ascending model failure rate: \(F[:,1]\) corresponds to the most accurate model (Microsoft) and \(F[:,3]\) corresponds to the least accurate model (Google).

Descriptive statistics.To build general understanding of model performance in **HAPI**, we provide basic descriptive statistics (Table 1). For most datasets, all APIs achieve accuracies within 5-10% of each other (exceptions include digit, yelp, imdb). Interestingly, we often find the systemic failure rate is roughly half the failure rate of the most accurate model \(h_{1}\).

    &  &  &  \\  & rafdb & afnet & xkpu & per- & fluent & digit & aminst & shop & yelp & imdb & walmal \\ 
**Dataset size** & 15.3k & 287.4k & 31.5k & 6.4k & 30.0k & 2.0k & 30.0k & 62.8k & 20.0k & 25.0k & 12.0k \\
**Number of classes** & 7 & 7 & 7 & 31 & 10 & 10 & 2 & 2 & 2 & 2 \\  \(h_{1}\) **failure rate** (i.e. error) & 0.283 & 0.277 & 0.272 & 0.156 & 0.019 & 0.217 & 0.015 & 0.078 & 0.043 & 0.136 & 0.110 \\ \(h_{2}\) **failure rate** (i.e. error) & 0.343 & 0.317 & 0.348 & 0.316 & 0.025 & 0.259 & 0.015 & 0.095 & 0.111 & 0.219 & 0.151 \\ \(h_{3}\) **failure rate** (i.e. error) & 0.388 & 0.359 & 0.378 & 0.323 & 0.081 & 0.472 & 0.043 & 0.122 & 0.486 & 0.484 & 0.181 \\
**Systemic failure rate** & 0.152 & 0.178 & 0.181 & 0.066 & 0.01 & 0.129 & 0.002 & 0.039 & 0.021 & 0.043 & 0.065 \\   

Table 1: **Basic statistics on HAPI datasets** including the (observed) systemic failure rate (i.e. fraction of instances misclassified by all models).

### Ecosystem-level Behavior

In order for a measure of systemic failure to be useful, it must be (i) meaningful and (ii) comparable across systems. A challenge to the meaningfulness of any proposed metric is that systemic failures occur more often in an ecosystem with many inaccurate models. A metric for systemic failure that primarily communicated the aggregate error rates of models in the ecosystem would not be meaningful as an independent metric. It also would not support goal (ii) because we could not compare the rates of systemic failure across ecosystems with varying model accuracies. It would be difficult to identify a system with a 'large' rate of systemic failure because the systemic failure properties would be swamped by the error rates of the models in the ecosystem. Therefore, achieving meaningfulness and comparability requires the metric to incorporate error correction.

Assuming model independence is a helpful baseline because it adjusts for model error rates without making assumptions about the correlation between models in the ecosystem. To avoid assumptions and for the sake of simplicity, therefore, we juxtapose the _observed_ behavior with a simple theoretical model in which we assume models fail independently of each other. Under this assumption, the distribution of the _baseline_ number of model failures \(t\{0,,k\}\) follows a Poisson-Binomial distribution parameterized by their failure rates (Equation 2).

The baseline of independence also means that our metric does not attempt to quantify whether it is "reasonable" that the models all fail on some instances. For example, some instances might be harder (or easier) than others, making it more likely that all models will fail (or succeed) to classify that instance. However, "hardness" is observer-relative. What is hard for one class of models might be easy for another class of model, and likewise with humans with particular capabilities or training. Therefore 'correcting' the metric to account for hardness would relativize the metric to the group of humans or class of models for whom that instance is hard. We choose independence as a baseline to be neutral on this point. However, we depart from independence in Appendix SSA.3, exploring how a baseline that assumes some level of correlation between models can more accurately model the observed distribution of ecosystem-level outcomes.

Comparing the true observed distribution of ecosystem-level outcomes with the baseline distribution helps illuminate how correlated outcomes are across models. Below we define \(P_{}\) (Equation 1) and \(P_{}\) (Equation 2).

\[P_{}(t) =^{N}[t=_{j=1}^{k}F[i,j] ]}{N} \] \[P_{}(t) =(_{1},,_{k})[t] \]

Finding 1: Homogenous Outcomes.In Figure 1(a), we compare the observed and baseline distributions for the spoken command recognition dataset digit. We find the observed ecosystem-level outcomes are more clearly _homogenous_ compared to the baseline distribution: the fraction of instances that receive either extreme outcome (all right or all wrong) exceeds the baseline rate. These findings generalize to all the datasets (Figure 1(b)): the observed rate always exceeds the baseline rate for the homogeneous outcomes (above the line \(y=x\)) and the reverse mostly holds for intermediary outcomes.

## 4 Do Model Improvements Improve Systemic Failures?

The performance of a deployed machine learning system changes over time. Developers serve new versions to improve performance (Chen et al., 2022b, a), the test distribution shifts over time (Koh et al., 2021), and the users (sometimes strategically) change their behavior (Bjorkegren et al., 2020). In spite of this reality, most analyses of the societal impact of machine learning only consider static models.

Ecosystem-level analysis provides a new means for understanding how models change and how those changes impact people. When models change, what are the broader consequences across their model ecosystem? Do single-model improvements on average improve ecosystem-level outcomes by reducing systemic failures? And to what extent are the individuals for whom the model improves the same individuals were previously systemically failed?Setup.Chen et al. (2022) evaluated the performance of the commercial APIs on the same eleven evaluation datasets each year in 2020-2022. Of all year-over-year comparisons, we restrict our attention to cases where one of the three APIs for a given task improves by at least 0.5% accuracy.2 Let \(h_{}\) denote the model that improved. We identify the instances that \(h_{}\) initially misclassified in the first year as _potential improvements_ and the subset of these instances that \(h_{}\) correctly classified in the second year as _improvements_. Considering the initial distribution of failures for \(h_{}\), we can ask where does the \(h_{}\) improve? We answer this by comparing the distribution of outcome profiles for the _other_ models (besides \(h_{}\)) between the potential improvement and improvement sets.

Figure 3: **Model improvement is not concentrated on systemic failures.** When a model improves, we compare the distribution of outcome profiles of the other two models on its initial failures (_potential improvements_) to the distribution on the instances it improved on (_observed improvements_). Across all improvements, including Amazon’s improvement on waimai (_left_), there is a clear over-improvement on [\(\), \(\)] (above \(y=x\) on _right_) and under-improvement on [X, X] (below the identity line on _right_).

Figure 2: **Homogeneous outcomes. Ecosystem-level analysis surfaces the general trend of _homogeneous outcomes_: the observed rates that all models succeed/fail consistently exceeds the corresponding baseline rates. Figure 1(a) shows that models in the DIGIT dataset are more likely to all fail or all succeed on an instance than baseline. Figure 1(b) shows that across all datasets, systemic failure (red dots) and consistent success (blue dots) of all three models on an instance are both more common than baseline, whereas intermediate results are less common than baseline.**

Finding 2: Model improvements make little progress on systemic failures.As a case study, we consider Amazon's improvement on the waimai dataset from 2020 to 2021. In Figure 2(a), from left to right, [X, X] indicates the other APIs (Baidu, Google) both fail, [X, \(\)] and [\(\), X] indicate exactly one of the other APIs succeed, and [\(\), \(\)] indicates both succeed. The majority (64.7%) of Amazon's improvement is on instances already classified correctly by the other two APIs, far exceeding the fraction of potential improvements that were classified correctly by Baidu and Google (34.1%) in 2020. In contrast, for systemic failures, the improvement of 11.6% falls far short of the potential improvement of 36.7%. In fact, since models can also fail on instances they previously classified correctly, the model's improvement on systemic failures is even worse in terms of net improvement. Amazon's improvement amounts to _no net reduction of systemic failures_: the model improves on 78 systemic failures but also regresses on 78 instances that become new systemic failures, amounting to no net improvement. The Baidu and Google APIs similarly show little improvement on systemic failures even as models improve.

This finding is not unique to Amazon's improvement on the waimai dataset: in the 11 datasets we study, we observe the same pattern of homogeneous improvements from 2020-2022. In Figure 2(b), we compare the observed improvement distribution3 (\(y\) axis) to the potential improvement distributions (\(x\) axis) across all model improvements. We find a clear pattern: systemic failures (the [X, X] category) are represented less often in the observed improvement set than in the potential improvement set. This finding indicates that when models improve, they _under-improve_ on users that are already being failed by other models. Instead, model improvements especially concentrate on instances where both other models succeeded already.

Ecosystem-level analysis in the context of model improvements disambiguates two plausible situations that are otherwise conflated: does single-model improvement (i) marginally or (ii) substantively reduce systemic failures? We find the reduction of systemic failures is consistently marginal: in every case, the reduction fails to match the the distribution seen in the previous year (i.e. every [X, X] red point is below the line in Figure 2(b)).

## 5 Ecosystem-level Analysis in Dermatology (DDI)

Having demonstrated that ecosystem-level analysis reveals homogeneous outcomes across machine learning deployments, we apply the ecosystem-level methodology to medical imaging. We consider this setting to be an important use of ecosystem-level analysis because machine learning makes predictions that inform the high-stakes treatment decisions made by dermatologists.

### Data

Daneshjou et al. (2022) introduced the Diverse Dermatology Images (**DDI**) dataset of 656 skin lesion images to evaluate binary classification performance at detecting whether a lesion was malignant or benign. Images were labelled with the ground-truth based on confirmation from an external medical procedure (biopsy). In addition, each image is annotated with skintone metadata using the Fitzpatrick scale according to one of three categories: Fitzpatrick I & II (light skin), Fitzpatrick III & IV (medium skin), and Fitzpatrick V & VI (dark skin). We use the predictions from Daneshjou et al. (2022) on **DDI** for two prominent ML models (ModelDerm (Han et al., 2020) and DeepDerm (Esteva et al., 2017)) and two board-certified dermatologists.4 We defer further details to Appendix C.

### Results

Finding 3: Both humans and models yield homogeneous outcomes; humans are more homogeneous.We compare _observed_ and _baseline_ ecosystem-level outcomes on **DDI** for models (Figure 3(a)) and humans (Figure 3(b)). Consistent with the general trends in **HAPI**, model predictions yield homogeneous outcomes. For human predictions from board-certified dermatologists, we also see that outcomes are homogeneous. However, in comparing the two, we find that humans yield even more homogeneous outcomes. We take this to be an important reminder: while we predict that models are likely to produce homogeneous outcomes, we should also expect humans to produce homogeneous outcomes, and in some cases more homogeneous outcomes.

**Finding 4: Ecosystem-level analysis reveals new racial disparities in models but not humans.** Standard analyses of machine learning's impact focus on model performance across groups (e.g. Buolamwini and Gebru, 2018; Koenecke et al., 2020). In AI for medicine, several works have taken such an approach towards understanding fairness (Obermeyer et al., 2019; Seyyed-Kalantari et al., 2021; Kim et al., 2022; Colwell et al., 2022). Daneshjou et al. (2022) demonstrate racial disparities for both model and human predictions on **DDI** and find that predictive performance is worst for darkest skin tones, aligning with broader trends of racial discrimination in medicine and healthcare (Vyas et al., 2020; Williams and Wyatt, 2015; Fiscella and Sanders, 2016).

Ecosystem-level analysis can build on these efforts. We conduct the same analysis from Figure 4 stratified by skin tone. In previous experiments, we had observed systemic failures across the whole population. Here we measure systemic failures for subpopulations grouped by skin tone. In Figure 5, we plot the difference between the observed and baseline rates on the \(y\) axis: the **All** bars (_blue_) reproduce the homogeneity results from Figure 4.

Figure 4: **Homogeneous outcomes for models and humans. Consistent with HAPI, model predictions (_left_) yield homogenous outcomes on DDI. Human predictions (_right_) are even more homogenous than models.**

Figure 5: **Racial disparities for models but not humans. We stratify ecosystem-level analysis in DDI by the three skin tone categories, plotting the difference between _observed_ and _baseline_ rates. Models (_left_) show clear racial disparities, exhibiting the most homogeneity for the darkest skin tones, whereas humans (_right_) show no significant racial disparity.**

Strikingly, ecosystem-level-analysis surfaces an important contrast between model behavior (_left_) and human behavior (_right_). Models (Figure 4(a)) are most homogenous for darkest skin tones (Fitzpatrick V & VI; _dark brown_) and least homogeneous for the lightest skin tones (Fitzpatrick I & II; _cream_): The observed systemic failure rate for the darkest skin tones is 8.2% higher than the baseline, while for the lightest skin tones it is 1.5% lower than the baseline. By contrast, humans (Figure 4(b)) show no significant variation as a function of skin tone.

Ecosystem-level analysis therefore identifies a new form of racial disparity not previously documented. Critically, while prior works show racial disparities for both models and humans, here we find a form of racial disparity that is salient for models but not present for humans. We note that, in absolute terms, homogeneity is higher for all racial groups in human predictions than model predictions, though human predictions don't display significant differences in homogeneity across racial group. This demonstrates that ecosystem-level analysis can reveal new dimensions of fairness, allowing stakeholders to identify the metrics that are most relevant in their context, be that model error or systemic failure rate. Our tool helps researchers and stakeholders evaluate the holistic ecosystem of algorithmic - and human - judgements that ultimately shapes outcomes for those subject to algorithmic judgement.

## 6 Commentary

While ecosystem-level analysis reveals new dimensions of machine learning's societal impact, it also opens the door for further questions. We prioritize two here, deferring further discussion and a longer related work section to the supplement. How can we _explain_ the pervasive homogeneous outcomes we have observed in deployed machine learning? And what are the _implications_ of this work for both researchers and policymakers?

### Explanations for Homogeneous Outcomes

Our findings provide evidence across several settings for homogeneous outcomes in deployed machine learning (Finding 1; SS3) that are mostly unabated by model improvement (Finding 2; SS4).

Data-centric explanations.We posit that "example difficulty" may give rise to homogeneous outcomes and provide three analyses that instantiate variants of this hypothesis.

First, _human ambiguity_ on the ground-truth may predict homogeneous outcomes. To test this, we make use of the ten human annotations per example in the fer+ dataset within **HAPI**. We find that the systemic failure rate is monotonically correlated with annotator disagreement with the majority label. This suggests that some systemic failures are correlated with the ambiguity or "hardness" of the image. However, we find that even some of the least ambiguous images, namely images on which all or most annotators agree, have systemic failures.. This indicates that human ambiguity is only partially explanatory of ecosystem-level model outcomes. We explore this further in SSA.1.

Second, _human error_ may predict homogeneous outcomes. To test this, we compare human predictions on **DDI** with the ground truth biopsy results. We stratify ecosystem-level analysis by dermatologist performance, comparing (i) instances both dermatologists get right, (ii) instances they both get wrong, and (iii) instances where they disagree and exactly one is right. We find that when both dermatologists fail, there continues to be outcome homogenization. However, when both dermatologists succeed, there is no homogeneity and the observed rates almost exactly match the baseline rates for every image. This suggests human error is also partially predictive of ecosystem-level model outcomes. We explore this further in SSA.2.

Finally, more _expressive theoretical models_ can better capture the observed trends than our simple full instance-level independence model. We introduce a two-parameter model. \(\) fraction of instances are categorized as difficult and the remaining \(1-\) are easy. A model's failure rate \(_{j}\) over all examples scales to \((1+)_{j}\) on hard examples and \((1-)_{j}\) on easy examples. Partitioning examples in this way, while continuing to assume instance-level independence, inherently homogenizes: models are more likely to systemically succeed on easy instances and systemically fail on hard instances. To best fit the **HAPI** data, the resulting average \(\) (\(0.2-0.3\)) and \(\) (\(1-4\), meaning these examples are \(2-5\) harder) values are quite extreme. In other words, for this theoretical model to fit the data, a significant fraction (\( 25\%\)) would need to be considerably harder (\( 3.5\)) than the overall performance. We explore this further in SSA.3.

These analyses contribute to an overarching hypothesis that example _difficulty_ partially explains homogeneous outcomes. While we discuss the construct of difficulty in the supplement, we draw attention to two points. First, difficulty is largely in the eye of the beholder: what humans or models perceive as difficult can differ [e.g. adversarial examples; Goodfellow et al., 2014]. Thus while example difficulty could be caused by inherent properties of the example (such as noisy speech or corrupted images), it could just as well be due to model properties, such as all the models having made similar architectural assumptions or having parallel limitations in their training data. Second, whether or not homogeneous outcomes are caused by example difficulty does not change their societal impact. The consequences of systemic failure can be material and serious (e.g. unemployment).

Model-centric Explanations and Algorithmic Monoculture.An alternative family of explanations put forth in several works is that correlated outcomes occur when different deployed machine learning models share training data, model architectures, model components, learning objectives or broader methodologies [Ajunwa, 2019, Engler, 2021, Bommasani et al., 2021, Creel and Hellman, 2022, Fishman and Hancox-Li, 2022, Bommasani et al., 2022, Wang and Russakovsky, 2023]. Such _algorithmic monoculture_[Kleinberg and Raghavan, 2021, Bommasani et al., 2022] in fact appears to be increasingly common as many deployed systems, including for the tasks in **HAPI**, are derived from the same few foundation models [Bommasani et al., 2023, Madry, 2023]. Unfortunately, we are unable to test these hypotheses because we know very little about these deployed commercial systems, but we expect they form part of the explanation and encourage future work in this direction.

Implications for Researchers.Our paper shows that ecosystem-level research can reveal previously-invisible social impacts of machine learning. We believe this methodology concretizes the impact of decision-making in real contexts in which individuals would typically be affected by decisions from many actors (e.g. job applications, medical treatment, loan applications, or rent pricing). As we demonstrate in **DDI**, our methodology applies equally to human-only, machine-only, and more complex intertwined decision-making. We anticipate understanding of homogeneous outcomes arising from any or all of these sources will be valuable.

Implications for Policymakers.Given the pervasive and persisting homogeneous outcomes we document, there may be a need for policy intervention. In many cases no single decision-maker can observe the decisions made by others in the ecosystem, so individual decision-makers (such as companies) may not know that systemic failures exist. In addition, systemic failures are not currently the responsibility of any single decision-maker, so no decision-maker is incentivized to act alone. Consequently, policy could implement mechanisms to better monitor ecosystem-level outcomes and incentivize ecosystem-level improvement. In parallel, regulators should establish mechanisms for recourse or redress for those currently systemically failed by machine learning [see Voigt and von dem Bussche, 2017, Cen and Raghavan, 2023].

## 7 Conclusion

We introduce ecosystem-level analysis as a new methodology for understanding the cumulative societal impact of machine learning on individuals. Our analysis on **HAPI** establishes general trends towards homogeneous outcomes that are largely unaddressed even when models improve. Our analysis on **DDI** exposes new forms of racial disparity in medical imaging that arise in model predictions but not in human predictions. Moving forward, we hope that future research will build on these empirical findings by providing greater theoretical backing, deeper causal explanations, and satisfactory sociotechnical mitigations. To ensure machine learning advances the public interest, we should use approaches like ecosystem-level analysis to holistically characterize its impact.