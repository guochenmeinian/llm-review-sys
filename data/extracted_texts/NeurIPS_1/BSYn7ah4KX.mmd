# Bias Amplification in Language Model Evolution:

An Iterated Learning Perspective

 Yi Ren

UBC

renyi.joshua@gmail.com &Shangmin Guo

University of Edinburgh

s.guo@ed.ac.uk &Linlu Qiu

MIT

linluqiu@mit.edu &Bailin Wang

MIT

bailin.wang28@gmail.com &Danica J. Sutherland

UBC & Amii

dsuth@cs.ubc.ca

###### Abstract

With the widespread adoption of Large Language Models (LLMs), the prevalence of iterative interactions among these models is anticipated to increase. Notably, recent advancements in multi-round on-policy self-improving methods allow LLMs to generate new examples for training subsequent models. At the same time, multi-agent LLM systems, involving automated interactions among agents, are also increasing in prominence. Thus, in both short and long terms, LLMs may actively engage in an evolutionary process. We draw parallels between the behavior of LLMs and the evolution of human culture, as the latter has been extensively studied by cognitive scientists for decades. Our approach involves leveraging Iterated Learning (IL), a Bayesian framework that elucidates how subtle biases are magnified during human cultural evolution, to explain some behaviors of LLMs. This paper outlines key characteristics of agents' behavior in the Bayesian-IL framework, including predictions that are supported by experimental verification with various LLMs. This theoretical framework could help to more effectively predict and guide the evolution of LLMs in desired directions. The code for experiments is available at [https://github.com/Joshua-Ren/iICL](https://github.com/Joshua-Ren/iICL).

## 1 Introduction

Recent large language models (LLMs) have shown remarkable instruction-following ability and an increasing number of applications; it is thus reasonable to expect they are likely to become more widespread. Moreover, _interactions_ between LLMs (either multiple models, or different generations of the same model) may also become very commonplace in the near future. In fact, many recent works consider iterative on-policy self-data-augmentation solutions to break through the bottleneck of human-generated supervisions, e.g., self-instruct (Y. Wang et al., 2022), self-refine (Madaan et al., 2023), hypothesis refinement (Qiu et al., 2024), self-distill (C. Xu et al., 2023), self-instruct (Y. Wang et al., 2022), self-reward (Weizhe et al., 2024), self-feedback (W. Xu et al., 2024), RAFT (Dong et al., 2023), ReST (Gulcehre et al., 2023), iterated DPO (Xiong, Dong, Ye, Zhong, et al., 2023), OAIF (Guo et al., 2024), SPIN (Z. Chen et al., 2024), and many more. Whether the model's knowledge is updated through in-weight or in-context mechanisms, these methods involve an LLM learning from a corpus (comprising data examples or evaluations) generated by another LLM (or itself), and subsequently transferring this acquired knowledge to others. Looking towards the long term, the future Internet may (for better or worse) contain substantial portions of LLM-generated text, which will, in turn, be employed for training the subsequent generation of models. It thus seems important to begin studying what this process will mean for future models.

Although these self-improving methods demonstrate considerable improvements on various benchmarks, a systematic understanding of why they work and what is their limitations is still missing. Some analysis of knowledge distillation might bring insights (Mobahi et al., 2020), as learning from data generated by another model is a type of distillation. But precisely analyzing the LLM's behavior on specific samples becomes increasingly difficult as it grows more complex. Instead, a behavioral-level analysis might be fruitful, akin to how the Bayesian framework can aid in comprehending the human cognitive system (T. L. Griffiths et al., 2023). By conceptualizing the LLM as an intelligent agent, we can draw parallels between its behaviors and the cultural evolution observed in humans. Iterated learning (IL), a framework proposed to study the evolution of knowledge and beliefs through a chain of learning among Bayesian agents (Kirby et al., 2007), stands out as a promising candidate for achieving our goals.

In this paper, we start by introducing the Bayesian-IL framework, demonstrating that agents engaged in such a process gradually amplify bias in their priors. This amplification process can be steered by introducing an interaction phase that "filters" or "re-ranks" the messages generated by the agents. Next, we theoretically justify that the in-context behavior of LLMs can be approximated by a Bayesian update, establishing a crucial link to the LLM system. To validate our claims, we conduct numerous experiments across different settings. Depending on the beneficial or detrimental nature of the bias, we propose various strategies to guide the evolution of LLM. The key contributions of this work are: 1) establishing the first Bayesian analysis of the full interactive learning process (including an interaction phase); 2) applying this framework to LLM agents and describing their evolution theoretically; 3) validating the theory and demonstrating how to guide LLM's evolution using experiments. We believe that our analysis can enhance our understanding of LLMs, and aid in designing more effective algorithms for alignment, bias mitigation or amplification, and similar tasks.

## 2 Background and Related Work

### Iterated Learning

Iterated learning (IL) is a hypothetical procedure to simulate _how the tendency of specific properties of human culture or language gradually emerges and becomes dominant_. It is based on studying the behaviors of a chain of intelligent agents. From the perspective of an individual agent, the process involves initially acquiring knowledge from its predecessor (_imitation_), refining its beliefs while using them to conduct tasks (_interaction_ with the world), and subsequently imparting its knowledge to the agents in the next generation (_transmission_). Cognitive scientists have applied this framework to explain various evolutionary phenomena of human society, including the emergence of compositionality in human language (Kirby et al., 2015), patterns in human object categorisation (T. L. Griffiths et al., 2008), and the evolution of color naming systems (Carlsson et al., 2023). The framework has also seen recent success with neural network agents, including in emergent communication (Guo et al., 2019; Ren et al., 2020), machine translation (Y. Lu et al., 2020), visual question answering (Vani et al., 2021), large vision-language models (Chenhao et al., 2024), and representation learning (Ren et al., 2023), indicative that this framework could also be useful for more general deep learning systems, like LLMs.

### Related On-policy Self-data-augmentation Methods

While the theoretical guarantees for the Bayesian-IL framework studied in this paper rely on several assumptions, we posit that the behaviors observed for many recent "iterative self-data-augmentation" methods in LLM can be at least partially explained by the theory. We will now overview the basic assumptions, and how they fit with recent LLM approaches (more discussions in Appendix A).

First, the theory assumes "self-evolution," where all agents in different generations share the same initial knowledge. Methods like self-refine (Madaan et al., 2023) and hypothesis refinement (Qiu et al., 2024), which require the LLM to refine its output by the feedback from an identical LLM for several rounds, satisfy this assumption. Self-distill (C. Xu et al., 2023) and self-instruct (Y. Wang et al., 2022), if the models involved in different generations are the same, do as well. On the contrary, the super-alignment setting (Burns et al., 2023), where a stronger model is trained using the data generated by another weaker model, do not strictly fit with our analysis. However, if all the models are trained using a similar corpus, so that their initial knowledge should be similar, our analysis might still hold partially.

The theory also assumes the information transferred among agents is in the form of data examples, as in RAFT (Dong et al., 2023) and ReST (Gulcehre et al., 2023). Both methods consider a multi-generation data-transferring process, during which the bias is introduced by re-ranking the transferred data. Methods like self-reward (Weizhe et al., 2024) and self-refine (Madaan et al., 2023), which requires one agent to evaluate another agent's response, do not directly fit this assumption. However, if we also consider the evaluation as part of the data generated by the agent, the Bayesian-IL framework can still bring some insights. Furthermore, as analyzed in Ren and Sutherland (2024) that many preference alignment methods like direct preference optimization (DPO, Rafailov et al. (2024)) will naturally amplify the preference hidden in the pretrained model's prior. Then, those multiple-generation DPO variants, e.g., iterative DPO (Xiong et al., 2023; Zhang et al., 2023), might face a more serious risk of amplifying malicious bias.

In summary, although the assumptions of our Bayesian-IL framework might not be satisfied by all practical algorithms, the general trends depicted by it, e.g., the bias amplification, the necessity of a good interaction phase, etc., would still hold. Please refer to Appendix A for more discussions.

## 3 Bayesian Analysis of Iterated Learning

### Notations and Basic Behaviors of Bayesian Agents

We denote a data pair as \(d=(x,y)\), where \(d=\), with \(x\) and \(y\). The \((x,y)\) pair can be question and answer in a QA problem, the input and label in a supervised setting, or any type of prompt and output for in-context learning. The hypothesis \(h:\) describes the mapping between all possible \(x\) and their corresponding \(y\). Note that \(h\) can be either explicit or implicit, depending on the task. For instance, in inductive reasoning, \(h\) represents the rule determining the output from input examples and is explicit, as the model can directly generate it using natural language. Conversely, in self-data-augmentation, where \(x\) is a topic and \(y\) is a paragraph generated based on \(x\), \(h\) is likely to be implicit. In this context, \(h\) can be highly abstract with varying interpretations, such as the level of conciseness, helpfulness, or even the writer's preference for using rhyme.

Consider a general Bayesian agent whose behavior can be depicted by two basic procedures: _learning_ and _sampling_. Learning involves updating the agent's knowledge based on observations, while sampling is a procedure wherein the agent generates data based on its knowledge. In this context, the agent's knowledge is encapsulated by its posterior over the hypotheses, i.e., \(P_{lm}(h)\).

In Bayesian learning, we assume the agent holds a prior \(P_{0}(h)\) at the beginning. Its posterior after observing \(=(x_{i},y_{i})_{i=1}^{N}\) is calculated as

\[P_{lm}(h)=P(h) p( h) P_{0}(h), \]

where \(p( h)\) is the likelihood of these \(N\) data pairs under a specific \(h\); this is usually hard to calculate in practice.

Assume the agent holds a posterior \(P_{lm}(h)\) during sampling. Then, given the input signal \(x\), we can sample the corresponding \(y P_{lm}(y x)\). Based on the fact that \(h\) determines the relationship between \(x\) and \(y\), the above sampling procedure is equivalent to \(y_{h P_{lm}(h)}[p(y h,x)]\), which can be rewritten as \(h P_{lm}(h);\ y h p(y h,x)\). Following the definition of \(d\) and the assumption that \(x\) is uniformly distributed, the sampling procedure above is equivalent to \(d P_{lm}(d) p(d h) P_{lm}(h)\). If we instead first decide the most probable \(h\) rather than sampling \(h\) from the agent (maximum a posteriori (MAP), as is perhaps common subconscious behavior for humans), we then generate \(d\) by

\[d p(d h^{*}),\ h^{*}=*{argmax}_{h}P_{lm}(h). \]

### Iterated Learning of Bayesian Agents

Iterated learning is a hypothetical process simulating how human language gradually evolves to become more efficient when transferred and utilized across generations. Typically, iterated learning repeats of the following three phases, as illustrated in Figure 1: an _imitation phase_, where an ignorant agent learns from the data generated by its predecessor; an _interaction phase_, where this agent uses the knowledge to accomplish the task, and hence refine its knowledge; and a _transmission phase_,where this agent generates useful data for the next generation. Combing with Section 3.1, we can get a picture of how \(h\) and \(d\) evolve as follows.

_Initialization:_ at the beginning of the \(t\)th generation, a new agent\({}_{t}\), whose belief on \(h\) follows a prior distribution \(P_{0}(h)\), is initialized. In lab experiments, \(P_{0}(h)\) represents the belief of a well-educated participant who has not been previously involved in the target experiment. In in-context learning, a well-trained LLM also holds a complex and informative \(P_{0}(h)\) based on the enormous corpus it is trained on and the task instructions in the prompt.

_Imitation phase:_ after initialization, agent\({}_{t}\) then updates its knowledge by observing \(N\) data samples \(^{t-1}\). Following the above learning procedure, the model's posterior should be \(P(h^{t-1})\).

_Interaction phase_: in this phase, the agent will accomplish specific tasks to refine its knowledge. The tasks involved in this phase can be diverse and complex. For example, in lab experiments (Kirby et al., 2015) and emergent communication (Ren et al., 2020), the agent plays a Leiws referential game (Lewis, 2008) to rule out hypotheses representing a non-bijection between \(\) and \(\); in representation learning (Ren et al., 2023), the agent directly conducts supervised learning on the downstream task to inhibit insufficient representations. Although it is hard to precisely formalize the behavior of the agent under these tasks precisely, their goals are consistent: we expect to "rule out" unsuitable hypotheses with carefully designed interactions. In an idealized setting, we might expect the agent's posterior to become proportional to \((h_{})P(h^{t-1})\), where \(()\) is an indicator function and \(_{}\) is the subset of hypotheses that can accomplish the tasks. Broadly speaking, refining \(h\) or filtering \(^{t}\) using the feedback from humans, LLM, or the environment, which is common in the aforementioned iterative self-data-augmentation methods, is also a type of task implicitly constraining \(h_{}\).

_Transmission phase_: agent\({}_{t}\) now comes to the transmission phase, where it generates multiple data samples \(^{t}\) for the next generation. The agent will first select the most probable hypothesis based on its current belief and then generate data samples, i.e., \(d^{t}_{i} p(d h^{t*})\), where \(h^{t*}=_{h_{}}P(h^{t-1})\). This accomplishes one generation of iterated learning.

### Amplifying Biases is a Double-Edged Sword

In iterated learning, we repeat the phases mentioned above to get better \(h\) and \(\). The limiting behavior of this process can be described by the following proposition. In short, the bias in \(P_{0}(h)\) is guaranteed to be amplified generation-by-generation. Imposing appropriate \(_{}\) (mainly through a carefully designed interaction phase) can control it.

**Proposition 1**.: _Consider several Bayesian agents sharing the same prior \(P_{0}(h)\) are conducting iterated learning for \(T\) generations. If \(T\) is sufficiently large, any agent\({}_{t}\) with \(t>T\) will have_

\[P_{lm}(h)(h=h^{T*})\]

_where \(h^{T*}\) is a stationary point (e.g. a local maximum) of \(P_{0}(h)\) subject to \(h_{}\)._

To prove this, we first analyze iterated learning without the interaction phase. By drawing parallels between IL and EM (expectation-maximization) algorithms, we can prove that \(h^{T*}\) converges to the \(h\) with the maximum prior probability. We then consider the interaction phase, which introduces a "selection" pressure to rule out those \(h_{}\). By proving this process does not break the converging behavior of a non-interacting iterated learning, we achieve this proposition. (Please refer to Appendix B for more details.)

Figure 1: Examples of practical LLM systems that require knowledge transfer among different generations and how we use Bayesian agents to approximate their behaviors. 1, 2, and 3 denotes the imitation, interaction and transmission phases respectively.

This proposition describes an inevitable bias amplification procedure as long as the model keeps learning from the data sampled from itself (via Bayesian update, distilling, or imitating, as long as the learning increases the model's confidence in these samples). However, in practical applications, we should bear in mind that bias amplification is a double-edged word. If this bias is beneficial, like the simplicity bias in compositional language experiments, IL will help the model generate the "correct messages" more robustly. Imagine if we only have two possible hypotheses, i.e., \(h_{good}\) and \(h_{bad}\), where \(P_{0}(h_{good})\) is slightly larger than \(P_{0}(h_{bad})\). Then by sampling \(y p(y h,x) P_{0}(h)\), half of the chance we will get incorrect \(y\) coming from \(h_{bad}\). Although we can get \(y=*{argmax}_{y}p(y h,x) P_{0}(h)\) by using an extremely small temperature, the diversity of \(y\) provided by the likelihood will then disappear, which is not what we expected. How could we get samples that are both diverse and correct? Iterated learning can help with this problem by providing a posterior where \(P_{lm}(h_{good}) P_{lm}(h_{bad})\). With this posterior, sampling from \(p(y h,x) P_{lm}(h)\) would be similar to sampling from \(p(y h_{good},x)\), which solves our problem.

Conversely, amplifying bias also negatively influences the system in several ways. Besides the cases where the bias is malicious (it can be solved by designing an appropriate interaction phase where \(h_{bad}_{}\)), it also influences the model's creativity. Imagine we have multiple good \(h\) where \(P_{0}(h_{g1})>P_{0}(h_{g2})>P_{0}(h_{bad})\), then IL will let us lose \(h_{g2}\) even its prior is only slightly smaller than \(h_{g1}\). Such a mode decay phenomenon is quite similar to the "recursion curse" mentioned in (Shumailov et al., 2023): a more peaky \(P_{lm}(h)\) will make those non-dominating \(h\) have a smaller probability, hence it is harder to keep these modalities during evolution. Touvron et al. (2023) also mentioned that iteratively fine-tuning would harm the creativity of the model. The solution could be early stopping the iterated learning or manually introducing more \(y\) that comes from \(h_{g2}\) during imitation.

In summary, to guide the LLM to self-evolve in an expected direction, we need a good \(P_{0}(h)\), a carefully designed interaction phase, and an appropriate evolving time.

## 4 LLM-based Agents in Iterated Learning

### LLM Behaves like a Bayesian Agent when Sampling

To transfer the Bayesian-IL analysis to LLM, we start by showing that the sampling and learning behaviors of an LLM agent can be depicted by Bayesian inference, following a few-shot in-context learning (ICL) scenario demonstrated in (Xie et al., 2022). In this setting, the message feed to the agent would be an instruction prompt **w** followed by \(N\) examples, i.e., \(_{N}=(x_{i},y_{i})_{i=1}^{N}\). In other words, sampling \(y\) given the prompt, the examples, and the question \(x_{}\) can be represented as:

\[y P_{lm}(y x_{},_{N},) P_{ lmw}(y x_{},_{N}), \]

where \(P_{lmw}\) is the model's belief conditioned on the instruction **w**. If we call \(_{N}\) as \(^{t-1}\) (i.e., assume the examples are generated by agents in the previous generation) and assume the test question \(x_{}\) is uniformly distributed, sampling new data based on instruction and few-shot examples can be expressed as \(d^{t} P_{lmw}(d^{t-1})\), which is similar to the transmission phase in iterated learning.

Formally linking ICL and Bayesian-IL poses a non-trivial challenge, however, because the theoretical guarantee of Bayesian-IL relies on obtaining the MAP estimate of \(h\) at each generation. This is not immediately evident in ICL. Inspired by Xie et al. (2022), we first de-marginalize this posterior predictive distribution using the latent variable \(h\), and then achieve the following proposition:

**Proposition 2**.: _Consider that agent \(A\) is conducting in-context learning. If the prompt examples in \(^{t-1}\) are generated by another agent \(B\) with the same prior knowledge (e.g., they come from the same checkpoint and use the same prompt), sampling from the posterior predictive distribution of agent \(A\), i.e., \(d^{t} P_{}(d^{t-1})\), can be decomposed into: 1.) \(h^{t*}*{argmax}_{h}P_{}(h^{t-1})\), and 2.) \(d^{t} P_{}(d h^{t*})\), where \(h\) is a hidden variable that describes the mapping between \(x\) and \(y\)._

The proof is in Appendix B.3. This proposition bridges LLM and Bayesian agents using its context behavior, which we believe is a ubiquitous procedure in any LLM system, irrespective of the subsequent information-updating strategy or the final target. For example, in an LLM-agent system, where no in-weights update exists, the model interacts with other agents (e.g., the human, the internal block of an LLM agent, or the environment) by generating responses based on the prompt and dialoghistory. For LLM's finetuning, where various parameter updating strategies exist, the model also generates responses given the prompts, which is well depicted by the in-context behavior. Although the assumptions in this proposition do not exactly hold for all LLM systems, we believe our analysis can still roughly depict important trends of them. Please refer to Appendix A for more discussions.

### LLMs in Different Algorithms have a Similar Target to Bayesian Learning

We then check the learning procedure. First, in a pure in-context learning setting like self-instruct (Y. Wang et al.2022), self-refinement (Madaan et al.2023), hypothesis search (Qiu et al.2024), etc., the learning can be modeled by calculating the posterior \(P_{lmw}(h^{t-1})\), which is identical to the Bayesian learning discussed previously. Then, for those algorithms that require in-weights updates, like self-reward (Weizhe et al.2024), self-play instruction tuning (Z. Chen et al.2024), iterative DPO (Xiong, Dong, Ye, Wang, et al.2023), etc., the LLM might update its \(P_{lmw}(h)\) using different loss functions. However, as all of these methods contain a procedure that encourages the models to increase their likelihood of the training samples generated by their predecessors, we should expect \(P_{lmw}(^{t-1})\) to be increased after learning. As a result, the equivalent posterior \(P_{lmw}(h)\) will implicitly favor those \(h\) that can generate \(^{t-1}\), which aligns with the Bayesian learning target.

## 5 Experimental Verifications when the Hypothesis is Explicit

We directly verify our analysis above using an inductive reasoning task called Abstract Causal REasoning (Chi Zhang et al.2021, ACRE), where all LLM agents update their knowledge via ICL. In this task, the model needs to infer and generate the shared rule by summarizing several input-output pairs. Specifically, assume there are \(M\) different objects, say [A,B,C]. One data pair \(d=(x,y)\) is composed of an input \(x\), i.e., a list of a subset of these objects, and an output \(y\) that represents the status of the light (could be on, off, or undetermined). In this experiment, the existence of a specific object triggers the light to be on. The roles played by different objects are expressed by the rule \(h\). For example, in the learning stage in generation-\(t\), the model sees three data pairs \(^{t-1}\): ([B,C],undetermined), ([B],off), and ([A,B,C],on). We then expect the model to guess a rule like \(h^{t}=\){A:on, B:off, C:undetermined}, which means A can trigger the light to be on, B cannot, and C is not sure. In the sampling stage, we will feed the above \(h^{t}\) together with the instructions to the model and hope it generates more examples following \(p( h^{t})\). Hence the model's output might be ([A,C],on), ([A,B],on), and ([C],undetermined). Treating the above examples as \(^{t}\), the model in the next generation can induce the corresponding \(h^{t+1}\) by selecting the hypothesis with the largest of \(P_{lmw}(h^{t})\). To ensure the generalizability of our analysis, we conduct experiments on GPT3.5, GPT4, Claude3-haiku, and Mixtral-8x7b. Please refer to Figure 2 and Appendix D.3 for more details.

### How the Knowledge of LLM Agents Evolves

**Convergence of the posterior.** We start from the guarantees mentioned in Proposition 1 under an _imitation-only_ setting. In this experiment, we choose \(M=5\) to better illustrate the posterior distribution \(P_{lmw}(h)\) (there are \(3^{5}=243\) possible \(h\)). Thanks to the instruction-following ability, all LLMs we considered always return rules in the correct format, where the probabilities of all format-related tokens are almost one. We can then calculate \(P_{lmw}(h)\) or \(P_{lmw}(h)\) by multiplying the probabilities of specific tokens in their response (see Appendix D.1 for more details).

We first demonstrate the convergence of \(P_{lmw}(h)\), i.e., \(P_{lmw}(h)()\), which can be supported by the decreasing of the posterior's entropy, i.e., \(H(P_{lmw}(h))\). As illustrated in the first panel in Figure 3, \(H(P_{lmw}(h))\) gradually decreases to almost zero as iterated learning goes on, which verifies our theory that \(P_{lmw}(h)\) will converge to a one-hot-like distribution1. Smaller temperature \(\) makes the convergence faster, which matches our intuitions as well. To better illustrate how different \(h\) evolves during iterated learning, similar to what we did for the compositional language experiment in Appendix C.2, we also provide the probability of all possible \(h\) in a similar fashion. Note that for this problem, it is impossible to get the prior distribution \(P_{0}(h)\), because we must give the model at least one example as \(^{0}\). So in the rightmost two panels in Figure 3, we compare the posterior of the first and the sixth generations and see that the posterior becomes sparser.

**Converged \(h\) under different likelihood and priors.** We then show how iterated learning amplifies specific biases implied in the prior, i.e., \(h^{T*}*{argmax}_{h}P_{0}(h)\), and how the bias and likelihood influence the converging behavior. Note that \(P_{0}(h)\) represents LLM's belief given the instruction prompt **w**, where the few-shot examples are not included. Thanks to the phenomenon mentioned in (McCoy et al., 2023), where the confidence of LLM's prediction is heavily influenced by its degree of familiarity with the output phrases, we can manipulate the prompt to create spurious correlations and hence implicitly control bias in \(P_{0}(h)\)2. Specifically, we change the name of the last object from "E" to "screen" and add a sentence like "Turn off the screen after the experiment." in the instruction prompt. Then all \(h\) with screen:off would have higher prior under this prompt. We use six different prompts to introduce different levels of biases (see Appendix D.2 for more details).

We then control the strength of the likelihood by selecting different \(h^{*}\), i.e., the ground truth rule we want to recover. For the strong likelihood case, we select \(h^{*}\) where four objects are being on while there is only one in the weak likelihood case. The status of screen in both cases is undetermined. Due to the nature of the ACRE task, i.e., the existence of an on-object in the input will trigger the light on, there might be more examples whose outputs are on when the likelihood is strong. Then it is harder for the model to amplify the prior bias that favors the status of screen to be off. Because the likelihood and prior compete with each other during iterated learning, as illustrated by Equation (1).

This competing relationship can be well depicted by the middle two panels in Figure 3, where we track the probability of \(P_{lmw}()\) at the end of each generation. The converging speed under different settings correlates with the level of prior bias well. Furthermore, we find it is easier for the bias to be amplified when the likelihood is weaker, as five out of six curves converge to one in the right panel. This trend is more clear in Figure 12 and 13, where curves with the same level of bias are shown together. These results give us a good picture of how the likelihood and prior bias interact with each other during evolution and also verify the correctness of the Bayesian-IL framework for LLM agents. Plus, we plot the histograms of \(P_{lmw}(h)\) under weak-likelihood-high-bias case in the rightmost two panels in Figure 3, which also demonstrates the amplified bias (the blue region grows).

**Influence of the interaction phase and \(_{}\).** Finally, we introduce the interaction phase and show that \(h^{T*}*{argmax}_{h_{}}P_{0}(h)\). Two mechanisms are considered in this experiment: self-refine (Madaan et al., 2023), where the feedback comes from the model's own response; and hypothesis-search (Qiu et al., 2024), where the feedback comes from an external ground-truth interpreter. We can consider the self-refine as using an _imperfect_\(_{}\). In both settings, the LLM refines its proposed \(h^{t}\) at the end of each generation by checking and reporting whether this \(h^{t}\) can explain all samples in \(^{0}\) (details in Appendix D.3).

In this experiment, we give the model 8 different examples in \(^{0}\), where all these examples can be explained by both \(h^{*}\) and \(\). We first select an \(\) from all 162 candidates (\(3^{4} 2\)) and then create \(h^{*}\) by changing the value of screen to off. Under this setting, both \(h^{*}\) and \(\) belong to \(_{}\) (i.e., mappings

   &  &  &  &  \\   & Corf. & \(r_{20}=\) & \(\)ff & BOTH & Corf. & \(r_{20}=\) & \(\)ff & BOTH & Corf. & \(r_{20}=\) & \(\)ff & BOTH \\  imitation only & 7.45 & 7.0\% & 10.\% & 5.64 & 11.1 & 8.0\% & 9.0\% & 6.4\% & 5.0\% & 30\% & 5.3\% & 2.01 & 20\% & 10\% \\ w. self-refine & 7.0\% & 0.60 & 40\% & 20\% & 6.6\% & 1.11 & 95\% & 35\% & 7.40\% & 7.60\% & 60\% & 15\% & \\ w. hypo-search & 7.7\% & 0.21 & 80\% & 45\% & 7.4\% & 0.66 & 100\% & 35\% & 7.5\% & 50.67 & 90\% & 50\% & 6.5\% & 9.7\% & 30\% \\  

Table 1: Results when adding different interaction phases. Column “BOTH” represents the ratio of converged \(h^{T}\) who correctly predict all 8 examples in \(^{0}\) and have screen:off (i.e., \(r_{20}=\)off). The Mixtral model does not have self-refine results, as it violates the instructions too much.

Figure 2: Demonstration of conducting iterated ICL on the ACRE task.

with perfect training accuracy in \(^{0}\)) and \(h^{*}\) is what we want our model to converge to. See Table 1, where we run experiments under 10 different \(h^{*}\) and report three quantitative metrics of the last iteration, i.e., \(h^{T*}\). We first report the number of correct predictions (mean and standard error) in \(^{0}\), which demonstrates how well the method constrains \(h^{T*}_{}\). The imitation-only method performs the worst, which warns us if the LLM keeps learning from the corpus generated by the agents in previous generations without any evaluation or filtering, even the training accuracy on given \(^{0}\) would be harmed. Because hallucination or incorrectness can aggregate through generations. Adding the interaction phase can mitigate this problem efficiently, which is why most of the related works contain a "data-selection" or "data-reranking" phase. The fact that the hypo-search outperforms self-refine indicates the importance of an appropriate \(_{}\), which means a good reward (or evaluating) model is crucial for these iterated training methods. Another metric is the ratio of \(h^{T*}\) with screen:off, which measures how well the bias is amplified (we here assume this bias is beneficial and wish it to be amplified, as the compositionality bias in emergent communication example showed in Appendix C.2). We find all these methods can amplify the bias to some extent and hypo-search also performs the best. Last, combined with the requirement of good training accuracy and amplifying bias, we report the ratio that the algorithm successfully chose \(h^{T*}=h^{*}\). As illustrated in the last column of the table, adding an interaction phase with good \(_{}\) always brings benefits.

In summary, this section verifies the correctness of the proposed analysis in LLM agents when the hypothesis is observable. The results remind us to pay more attention to whether the bias is beneficial or not and to design a better interaction phase as well.

## 6 Experimental Verifications when the Hypothesis is Implicit

Section 5 demonstrates that the Bayesian-IL framework can predict the behavior of LLM agents when \(h\) is explicitly defined and utilized when generating new examples. This section considers a hidden \(h\) scenario that is more general in most LLM systems. We start from a few-shot self-data-augmentation task, where the LLM keeps generating new examples to augment the data pool. In this process, \(h\) is implicitly selected when the few-shot examples are given, as stated in Proposition 2.

**Experimental settings.** We choose a scenario where on-policy self-data-augmentation is repeated for several generations. Consider using an LLM to generate multiple examples of an acronym-trainstorm task, where each example \(d\) is composed of an acronym and the corresponding word list, e.g., Acronym:IL; List:["infinite","loop"]. The \(h\) determines the properties of \(d\). We hold a data pool \(_{}\), which contains 20 random samples as \(^{0}\) at the beginning of the experiment. In each generation, the model will generate 20 extra examples based on the data generated by itself in the previous round, i.e., \(^{t} P_{mw}(^{t-1})\). The generated \(^{t}\) will be pushed into \(_{}\), which simulates a scenario in which the available data keeps growing when we conduct self-data augmentation. In this experiment, \(h\) is hidden and might have different interpretations. We consider that \(h\) represents two types of acronyms, i.e., \(h_{}\), where the acronym is a common word and \(h_{}\) otherwise. As the training data of the LLMs in our experiments is private, we instead use the ranking of the frequency of a word that appeared in common English corpus3 as an approximation. We categorize a word as "easy" if its ranking is below 60,000; otherwise, we label it as "hard".

**Bias in prior is amplified during IL.** Many recent works observe that the LLM prefers to output more common words (i.e., those with higher frequency in the pertaining corpus) (McCoy et al. 2023;

Figure 3: Left: the mean and standard deviations of \(H(P_{Imw}(h))\) of experiments with different \(h^{*}\) and \(^{0}\) (5 different seeds). Middle two: the probability of screen being off, where different colors represent six different levels of spurious bias. Right: the histogram of all \(P_{Imw}(h)\) in the first and sixth generation, where the bars are colored based on the value of the last object in \(h\).

Z. Wu et al.2023), which can be considered as a bias towards \(h_{}\), i.e., \(P_{0}(h_{})>P_{0}(h_{})\). Since \(h\) is hidden and we cannot directly observe it like in the previous experiment, we instead track three quantities: 1.) the proportion of easy samples in all 20 samples for each \(^{t}\); 2.) the average ranking of \(^{t}\), where all hard examples are ranked 60,001; and 3.) the average length of the acronyms for \(^{t}\). As in the leftmost three panels Figure 4, the aforementioned bias is gradually amplified during iterated learning whatever the initial proportion of the easy samples in \(^{0}\) is.

**Interaction phase when \(h\) is hidden.** As \(h\) is inaccessible, which forbid us to directly apply hypog-search or self-refine, we instead add a filter on the transmitted data across different generations, which plays a similar role as the interaction phase. Specifically, we use a sampled \(}_{}(d h_{})\) to replace original \(^{t-1}\) in the "imitation-only" setting. Based on how we sample \(}\), different constraints on \(_{}\) are implicitly imposed. We compare the behavior of five different settings, they are: 1.) \(_{random}\), where \(}\) is randomly sampled from \(_{}\); 2.) \(_{hard}\) where only hard examples can be sampled; 3.) \(_{easy}\), opposite to the hard setting; 4.) \(_{easy}\), where the easy acronyms with longer lengths are more likely to be sampled; 5.) \(_{easy}\), opposite to the easy-long setting.

See the first several columns of Table 2 that show the ratio of easy examples in \(^{t}\). Compared with the random setting, all methods expect \(_{}\) finally converges to \(^{t}\) with more easy examples, which means the bias towards easier acronyms would be amplified when \(_{}\) doesn't impede it. On the contrary, using \(_{}\) successfully restrain this bias, as the average number of easy samples in \(^{t}\) is even lower than that in \(^{0}\). We can also design composite \(_{}\) by choosing two properties of the data. For example, \(_{easy}\) constrains the samples with hard and short outputs, which is why they have more easy but long examples in their \(^{t}\).

In summary, this experiment verifies that the Bayesian-IL framework still works when \(h\) is hidden: the bias is amplified generation by generation, implicitly imposing \(_{}\) can still guide the evolution direction. Please also refer to Appendix E for more results and discussions.

## 7 Experiments on In-Weights Learning: On-Policy DPO as an Example

Besides the manually designed experiments in the previous two sections, here we verify our analysis in a real preference-tuning task using on-policy DPO (Guo et al.2024). In each round of the training, the model first samples multiple responses given the prompt (similar to the sampling stage in IL). Then, these responses are evaluated and ranked by another LLM annotator based on their level of helpfulness (the interaction phase in IL). Finally, we select the highest (lowest) ranked samples as the chosen (rejected) response and use a standard DPO algorithm (Rafailov et al.2024) to train the policy network (the imitation phase in IL). As described before, each update of the on-policy DPO algorithm can be considered as one generation in iterated learning, because the model keeps updating its parameters using the responses generated by itself. Ranking the responses based on helpfulness is equivalent to imposing a \(_{}\). As a result, the phenomenon of bias amplification, the guiding

Figure 4: Leftmost three: experiments in Section 6. First: how the ratio of easy samples changes in \(^{t}\). \(N_{e}\) is the number of easy examples in \(^{0}\). Second: how the average ranking of acronyms changes. Third: how the average length of acronyms changes. Rightmost two: results of on-policy DPO in Section 7. Fourth: average length of the responses. Fifth: win rate against the SFT baseline.

effect of the interaction phase design, and the influence of spurious correlation, should still hold in this practical setting.

To verify our analysis, we finetune a pretrained llama-2-7B model (Touvron et al. 2023) using Antropic-HH dataset (Bai et al. 2022) following the on-policy DPO recipe provided in (Guo et al. 2024). We study the length bias demonstrated in (Dubois et al. 2024), which means the LLM tends to prefer longer responses when answering questions. We first show that such a bias will be significantly amplified by a multi-generation self-improvement method (on-policy DPO) compared with a non-self-iterated method (RLHF, (Ouyang et al. 2022)). As demonstrated by the blue curve and the dotted line of the fourth panel in Figure 4, the average length of the responses from the model trained using on-policy DPO is much larger than the SFT baseline and RLHF counterparts. With the increase of the win rate against SFT4, the average response length also keeps increasing. To restrain this bias, we impose \(_{}\) by adding a sentence like "you are a laconic agent and prefer concise answers" to the annotator LLM, just like how we manipulate the spurious correlation between screen and off in Section 5. Then, combining with the existing interaction phase that requires \(h_{}\), this design is equivalently imposing a constraint of \(h_{}_{}\). Hence as illustrated by the orange curves in the last two panels, the on-policy DPO can then generate shorter responses (the increasing speed is also restrained) while keeping a high level of helpfulness. However, if our constraints of the length are too strong, which makes \(_{}_{}=\), the model's helpfulness will then be significantly harmed, as demonstrated by the pink curves in these two panels.

In summary, we find all our analysis on the Bayesian-IL still holds for a practical preference-tuning task: the biases would be amplified and a suitable interaction phase can control it as long as we can figure out them. However, some biases are inevitably hidden and are also amplified during LLM's evolution. Hence how to pinpoint these biases, or finding a method that can restrain malicious biases even without explicitly knowing them, would be interesting directions to explore in the future.

## 8 Conclusion

This paper examines the potential and ongoing evolutions of LLM agents by drawing parallels with human cultural evolution, where the latter is a well-established subject in cognitive science. By demonstrating that the sampling and learning procedures of LLMs in various algorithms can be effectively approximated by Bayesian inference, we successfully apply the Bayesian-IL framework to elucidate and steer the evolution of LLM agents. The presented theory and accompanying experiments not only provide deeper insights into LLM behavior from a top-down perspective but also hold the potential to inspire the design of more efficient self-evolution algorithms.