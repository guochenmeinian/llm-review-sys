# MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with Reinforcement Learning

Zeyuan Ma\({}^{1}\), Hongshu Guo\({}^{1}\), Jiacheng Chen\({}^{1}\), Zhenrui Li\({}^{1}\), Guojun Peng\({}^{1}\),

**Yue-Jiao Gong\({}^{1,}\)1, Yining Ma\({}^{2}\), Zhiguang Cao\({}^{3}\) \({}^{1}\)**South China University of Technology

\({}^{2}\)National University of Singapore

\({}^{3}\)Singapore Management University

{scut.crazynicolas, guohongshu369, jackchan9345}@gmail.com,

zhenrui.li@outlook.com, {pgj20010419, gongyuejiao}@gmail.com,

yiningma@u.nus.edu, zhiguangcao@outlook.com

###### Abstract

Recently, Meta-Black-Box Optimization with Reinforcement Learning (MetaBBOR-RL) has showcased the power of leveraging RL at the meta-level to mitigate manual fine-tuning of low-level black-box optimizers. However, this field is hindered by the lack of a unified benchmark. To fill this gap, we introduce MetaBox, the first benchmark platform expressly tailored for developing and evaluating MetaBBOR-RL methods. MetaBox offers a flexible algorithmic template that allows users to effortlessly implement their unique designs within the platform. Moreover, it provides a broad spectrum of over 300 problem instances, collected from synthetic to realistic scenarios, and an extensive library of 19 baseline methods, including both traditional black-box optimizers and recent MetaBBO-RL methods. Besides, MetaBox introduces three standardized performance metrics, enabling a more thorough assessment of the methods. In a bid to illustrate the utility of MetaBox for facilitating rigorous evaluation and in-depth analysis, we carry out a wide-ranging benchmarking study on existing MetaBBO-RL methods. Our MetaBox is open-source and accessible at: [https://github.com/GMC-DRL/MetaBox](https://github.com/GMC-DRL/MetaBox).

## 1 Introduction

Black Box Optimization (BBO) is a class of optimization problems featured by its objective function that is either unknown or too intricate to be mathematically formulated. It has a broad range of applications such as hyper-parameter tuning , neural architecture searching , and protein-docking . Due to the _black-box_ nature, the optimizer has no access to the mathematical expression, gradients, or any other structural information related to the problem. Instead, the interaction with the black-box problem is primarily realized through querying inputs (i.e., a solution) and observing outputs (i.e., its objective value).

Traditional solvers for BBO problems include population-based optimizers such as genetic algorithms , evolutionary strategies [5; 6; 7], particle swarm optimization [8; 9], and differential evolution [10; 11; 12; 13]. The Bayesian Optimization (BO) [14; 15] is also commonly used. However, with limited knowledge of the problem, these optimizers lean on carefully hand-crafted designs to strike a balance between exploration and exploitation when seeking the optimal solution.

To eliminate the burdensome task of manual fine-tuning, recent research has proposed the concept of **Meta-Black-Box Optimization (MetaBBO)**, which aims to refine the black-box optimizers byidentifying optimal configurations or parameters that boost the overall performance across various problem instances within a given problem domain. This leads to a bi-level optimization framework, where the meta-level enhances the performance of low-level black-box optimizers. The meta-level approaches include supervised learning (MetaBBO-SL) , reinforcement learning (MetaBBO-RL) , and self-referential search (MetaBBO-SR) . Among them, MetaBBO-RL models the optimizer fine-tuning as a Markov Decision Process (MDP) and learns an agent to automatically make decisions about the algorithmic configurations. By automating the tuning process, MetaBBO-RL not only significantly reduces the time and effort needed for customizing algorithms to specific unseen problems, but also potentially enhances the overall optimization performance.

Despite the success, MetaBBO-RL calls for a unified benchmark platform. Upon examining the recently proposed MetaBBO-RL approaches , we found that while several approaches claim state-of-the-art performance, they lack a comprehensive benchmark and comparison using a standardized, unified testbed. As a result, identifying which approach truly excels under specific conditions poses a significant challenge, thereby hindering further progress and advancement in this field.

To bridge the gap, we introduce MetaBox, the first benchmark platform for MetaBBO-RL. The blueprint of MetaBox is illustrated in Figure 1, aiming to offer researchers a convenient means to develop and evaluate the MetaBBO-RL approaches. Specifically, we have made the following efforts:

1. **To simplify the development of MetaBBO-RL and ensure an automated workflow:** 1) We introduce a _MetaBBO-RL Template_, depicted at the top left of Figure 1. It comprises two main components: the meta-level RL agent and the low-level optimizer, where we provided several implemented works (e.g., ) following a unified interface protocol. 2) We automate the _Train-Test-Log_ procedure in MetaBox, shown in the middle of Figure 1. The users simply need to complete their _MetaBBO-RL template_, specify the target problem set, and select several baselines for comparison. Initiating the entire automated process is as straightforward as executing the _run_experiment()_ command. This provides users with considerable flexibility in implementing different types of internal logic for their MetaBBO-RL algorithms while benefiting from the automated workflow.
2. **To facilitate broad and standardized comparison studies:** 1) We integrate a large-scale _MetaBox testsuite_ consisting of over 300 benchmark problems with diverse landscape characteristics (such as single-/multi- modal, non-ill/ill- conditioned, strong/weak global structured, and noiseless/noisy). Showing in the left centre of Figure 1, the _MetaBox testsuite_ inherits problem definitions from the well-known COCO  platform and the Protein-Docking benchmark (version \(4.0\)) , with several modifications to adapt to the MetaBBO paradigms. 2) We develop a _Baseline Library_, located at the bottom left in

Figure 1: Blueprint of our MetaBox platform. MetaBox offers template scripts for quick-start and, once completed by the user, it carries out an automatic Train-Test-Log process on the testsuites for comparison. Results such as performance scores, optimization curves, and comparative tables are automatically generated and made available for review.

Figure 1. The library currently encompasses a wide range of classic optimizers [6; 8; 9; 10; 14; 30; 31] and up-to-date MetaBBO-RL approaches [19; 20; 21; 22; 23; 24; 25]. We additionally integrate a MetaBBO-SL approach  to provide an extended comparison. Notably, all of the baselines are implemented by our _MetaBBO-RL Template_. This ensures consistency and allows for a fair and standardized comparison among the different approaches.
3. **To comprehensively evaluate the effectiveness of MetaBBO-RL approaches:** 1) We propose three _Standardized Metrics_ to evaluate both optimization performance and learning effectiveness of a MetaBBO-RL approach, including a novel Aggregated Evaluation Indicator (AEI) that offers a holistic view of the optimization performance, a Meta Generalization Decay (MGD) metric that measures the generalization of a learned approach across different problems, and a Meta Transfer Efficiency (MTE) metric that quantifies the transfer learning ability. 2) We conduct a tutorial large-scale comparison study using _Baseline Library_, evaluate them on _MetaBox testsuite_ by the proposed _Standardized Metrics_. Several key findings reveal that the pursuit of state-of-the-art performance continues to present challenges for current MetaBBO-RL approaches. Nonetheless, our MetaBox platform provides valuable insights and opportunities for researchers to refine and improve their algorithms.

To summarize, MetaBox provides the first benchmark platform for the MetaBBO-RL community (**novel**). It is fully open-sourced, offering template scripts that facilitate convenient development, training, and evaluation of MetaBBO-RL algorithms (**automatic**). Furthermore, MetaBox provides diverse benchmark problems and an extensive collection of integrated baseline algorithms, which will continue to expand through regular maintenance and updates (**extendable**).

## 2 Background and Related Work

As shown in 2, MetaBBO methods operate within a bi-level optimization framework designed to automate the fine-tuning process for a given BBO optimizer. Distinguishing themselves from conventional BBO techniques, MetaBBO methods introduce a novel meta-level as an automatic decision process. The purpose is to alleviate the need for labor-intensive manual fine-tuning of low-level BBO optimizers. Typically, they require the ability to generalize behaviors to address previously unseen problems through extensive training in a given problem distribution. Concretely: **1) At the meta level**, the meta optimizer (e.g., an RL agent) dynamically configures the low-level optimizer based on the current optimization status at that particular time step. Then, the meta optimizer evaluates the performance of the low-level optimizer over the subsequent optimization steps, referred to as meta performance. The meta optimizer leverages this observed meta performance to refine its decision-making process, training itself through the maximization of accumulated meta performance, thereby advancing its meta objective. **2) At the lower level**, the BBO optimizer receives a designated algorithmic configuration from the meta optimizer. With this configuration in hand, the low-level optimizer embarks on the task of optimizing the target objective. It observes the changes in the objective values across consecutive optimization steps and transmits this information back to the meta optimizer, thereby contributing to the meta performance signal.

**MetaBBO-RL.** It leverages reinforcement learning at the meta-level to configure the low-level black-box optimizer for boosting meta-performance  over a problem distribution. It involves three components: an RL agent (policy) \(_{}\), a backbone black-box optimizer \(\), and a dataset \(\) following certain problem distribution. The environment \(Env\) in MetaBBO-RL is formed by coupling the optimizer with a problem from the dataset, i.e., \(Env:=\{,f=sample()\}\), where \(f\) represents the sampled problem. Unlike traditional RL tasks [32; 33], the environments in MetaBBO-RL not only include the problem of optimizing but also include the low-level optimizer itself. At each step \(t\), the agent queries the optimization status \(s_{t}\), which includes the current solutions produced by \(\) and their evaluated values by \(f\). The policy \(_{}\) takes \(s_{t}\) as input and suggests an action \(a_{t}=_{}(s_{t})\) to configure the optimizer \(\). Note that the action space of different MetaBBO algorithms can vary largely, as it could involve determining solution update strategy [19; 20; 23], generating hyperparameter [21; 24; 25], or both of the above two . Subsequently, \(Env\) executes \(a_{t}\) to obtain the

Figure 2: Illustration of the bi-level optimization procedure of MetaBBO.

next optimization status \(s_{t+1}\) and the reward \(r_{t}\) that measures the meta-performance improvement of \(\) on problem \(f\). The meta-objective of MetaBBO-RL is to learn a policy \(_{}\) that maximizes the expectation of the accumulated meta-performance improvement \(r_{t}\) over the problem distribution \(\), \(_{f,_{}}[_{t=0}^{T}r_{t}]\), where \(T\) denotes the predefined evaluation budget, typically referred to the _maxFEs_ parameter as in the existing BBO benchmarks [28; 34].

**Other MetaBBO methods.** Apart from MetaBBO-RL, two other paradigms include MetaBBO-SL [16; 17; 18] and MetaBBO-SR [26; 27], which all belong to the MetaBBO community but leverage different approaches at the meta-level. Existing MetaBBO-SL methods consider a recurrent neural network (RNN) that helps determine the next solutions at each step [16; 17; 18]. However, they often encounter a dilemma in setting the horizon of RNN training: a longer length improves the difficulty of training, whereas a shorter length requires breaking the entire optimization process into small pieces, often resulting in unsatisfactory results. MetaBBO-SR utilizes black-box optimizers (such as an evolution strategy ) at both the meta and low levels to enhance the overall optimization performance. Since the black-box optimizers themselves can be computationally expensive, the MetaBBO-SR approaches may suffer from limited efficiency due to their inherently nested structure.

**Related BBO benchmarks.** Existing benchmarks for MetaBBO are currently absent. However, there are several related BBO benchmark platforms, including COCO  and IOHprofiler [38; 39] for continuous optimization, ACLib  for algorithm configuration, Olympus  for planning tasks, and Bayesmark  for hyper-parameter tuning. These platforms provide testsuites, logging tools and some baselines for users to compare or refer to. In addition to these platforms, some BBO competitions provide a series of problems to evaluate specific aspects of algorithms, such as the GECCO BBOB workshop series (based on COCO ), the NeurIPS BBO challenge  (based on Bayesmark ), the IEEE CEC competition series , and the Zigzag BBO [42; 43] that consists of highly challenging problems. In contrast, our proposed MetaBox introduces a novel benchmark platform specifically tailored for MetaBBO-RL. It provides an algorithm development template, an automated execution philosophy, a wide range of testsuites, an extensive collection of baselines, specific MetaBBO performance metrics, and powerful visualization tools. In Table 1, we compare MetaBox and other related benchmarks to showcase the novelty of this work.

## 3 MetaBox: Design and Resources

### Template coding and workflow automation

The core structure of MetaBox is presented on the left of Figure 3, shown in the form of a UML class diagram . Drawing from the recent MetaBBO-RL, we abstract the _MetaBBO-RL Template_ into two classes: the reinforcement \(Agent\) and the backbone _Optimizer_ (both marked in pale orange). To develop or integrate a new MetaBBO-RL approach, users are required to specify their settings in the attributes \(Agent.config\) and \(Optimizer.config\), and implement the following interfaces: 1) \(Agent.train\_episode(env)\) for the training procedure, 2) \(Agent.rollout\_episode(env)\) for the rollout procedure, and 3) \(Optimizer.update(action,problem)\) for the optimization procedure. Once these interfaces are set, the implemented templates seamlessly integrate with other components, requiring no extra adjustments. Except for the _Agent_ and _Optimizer_ classes, the other classes are hidden from users, simplifying coding tasks and ensuring code consistency.

    & \#Problem & \#Baseline & Template & Automation & Customization & Visualization & _RLSupport_ \\  COCO  & 54+0 & 2 & ✓ & ✓ & \(\) & ✓ & \(\) \\ CEC [34; 36; 37] & 28+0 & 0 & \(\) & \(\) & \(\) & \(\) & \(\) \\ IOHprofiler [38; 39] & 24+0 & 0 & ✓ & \(\) & ✓ & ✓ & \(\) \\ Bayesmark [40; 41] & 0+228 & 10 & ✓ & ✓ & \(\) & \(\) & \(\) \\ Zigzag [42; 43] & 4+0 & 0 & \(\) & \(\) & ✓ & \(\) & \(\) \\  MetaBox & 54+280 & 19 & ✓ & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparison to BBO benchmarks. We report _#Problem_: the number of problems (_#synthetic_ + _#realistic_); _#Baseline_: the number of baselines; _Template_: Template coding support; _Automation_: automated train/test workflow support; _Customization_: configurable settings; _Visualization_: visualization tools support; and _RLSupport_: Gym-style  RL benchmark.

Given an implemented approach based on _MetaBBO-RL Template_, the right side of Figure 3 illustrates the entirely automated _Train-Test-Log_ workflow in MetaBox. The process initiates when a user launches the function \(run\_experiment()\). Then, function \(Trainer.train()\) executes an outer-loop for meta-level iterations, where a runtime RL environment (\(env\)) is instantiated by pairing a problem from the trainset with an optimizer instance. The \(Agent.train\_episode(env)\) function is then called to start an inner-loop for the BBO iterations, during which the optimizer in \(env\) follows the _action_ informed by \(Agent\) and applies its \(update(action,problem)\) function to optimize the target BBO problem until the predefined function evaluation budget is exhausted. The meta-iteration ends when the \(Agent\) has trained its policy model for \(M\) steps. Next, the \(Tester.test()\) function is called to evaluate the learned policy on the testset through \(N\) independent runs, recording the statistic results. Finally, the \(Logger.log()\) function restores all saved training and testing results. These basic results are further post-processed into comprehensive metrics for in-depth analysis. The key to this automation resides in the universal internal-call bridging the _MetaBBO-RL Template_ and the _Train-Test-Log_ workflow, thereby obviating the need for users to grapple with complex coding tasks.

### Testsuites

The testsuites integrated into MetaBox are briefly described as follows (more details in Appendix A):

**Synthetic.** It consists of \(24\) functions in five groups: separable, moderate, ill-conditioned, multi-modal with global structure, and multi-modal with weakly structured. They are collected from the _coco:bbob_ function set in the renowned COCO platform .

**Noisy-Synthetic.** It consists of \(30\) noisy functions from the _coco:bbob-noisy_ function set , by extending the Synthetic set with different noise models and levels. The noises come from three different models: Gaussian noise, Uniform noise and Cauchy noise .

**Protein-Docking.** It is extracted from the Protein-Docking benchmark , with \(280\) instances of different protein-protein complexes. These problems are characterized by rugged objective landscapes and are computationally expensive to evaluate.

Besides, we make the following efforts to adapt the above testsuites, while enhancing usability and flexibility: 1) We treat each testsuite as a dataset and split it into training and testing sets according to a particular proportion, referred to as _difficulty_. This proportion determines the level of difficulty in generalizing or transferring the learned knowledge to unseen instances during testing. We provide two modes for controlling this aspect: _easy_ mode, allocating 75% of the selected instances for training, and _difficult_ mode, designating 25% of the selected instances for training. 2) Instead of providing predefined problem dimension candidates, such as {2,3,5,10,20,40} in COCO  and {10,30} in CEC2021 , we introduce an additional control parameter _problem-dim_ to support customized problem dimension. 3) We also recognize that there is a potential user group (e.g., those in )

Figure 3: The core structure and workflow of MetaBox. **Left:** UML Class diagram of MetaBox. Users can inherit from MetaBBO-RL templates highlighted in orange to enable polymorphism. **Right:** The automated Train-Test-Log workflow. The Agent directs the low-level (inner loop) optimization and trains itself on meta-level (outer loop), followed by testing and post-processing.

who require access to gradients of the objective during the training of their approaches2. To cater to this need, we provide a PyTorch function interface with tensor calculation support, enabling users to incorporate back-propagation in their methods.

### Baseline library

MetaBox leverages a total of \(19\) baselines, categorized into three types: 1) **Classic Optimizer:** Random Search (RS), Bayesian Optimization (BQ) , Covariance Matrix Adaptation Evolution Strategy (CMA-ES) , Differential Evolution (DE)  and its self-adaptive variants JDE21, MadDE , NL-SHADE-LBC , Particle Swarm Optimization (PSO)  and its self-adaptive variants GLPSO , sDMSPSO , SAHLPSO . 2) **MetaBBO-RL:** the Q-learning [48; 49; 50] styled approaches DEDDQN , DEDQN , RLHPSDE , QLPSO , and the Policy Gradient [51; 52] styled methdos LDE , RLPSO , RLEPSO . 3) **MetaBBO-SL:** RNN-OI .

The majority of the classic baselines are reproduced by referring to the originally released codes (when available) or the respective original papers. We meticulously filled the _Optimizer_ template (see Section 3.1) with their specific internal logic. However, there are a few exceptions: CMA-ES, DE and PSO are implemented by calling APIs of DEAP  (an evolutionary computation framework with LGPL-3.0 license), and BO is implemented by the Scikit-Optimizer  (a BO solver set with BSD-3-Clause license). This showcases the compatibility of MetaBox with existing open-sourced optimization codebases. For MetaBBO-RL, we first encapsulate the internal logic of the RL agent into the _Agent_ template (see Section 3.1), and then encapsulate the internal logic of the backbone optimizer into the _Optimizer_ template. This showcases the ease of integrating various MetaBBO-RL methods into MetaBox. In addition, we provide an example of a MetaBBO-SL approach, RNN-OI, to showcase that MetaBox is compatible with other MetaBBO methods.

### Performance metrics

In MetaBox, we implement three standardized metrics to evaluate the performance of MetaBBO-RL approaches from aspects of both the BBO performance and the training efficacy.

**AEL.** The MetaBBO approaches can be evaluated using traditional BBO performance metrics, including the best objective value, the budget to achieve a predefined accuracy (convergence rate), and the runtime complexity [28; 36; 39]. In addition to analyzing them separately, we propose the Aggregated Evaluation Indicator (AEI), a unified scoring system to provide users with a comprehensive assessment. Aggregating these metrics together is challenging due to the significant variation in values across different metrics and problems. To address this, the AEI normalizes and combines the three metrics in the following way. We test a MetaBBO-RL approach on \(K\) problem instances for \(N\) repeated runs and then record the basic metrics (usually pre-processed by a min-max conversion for consistency towards AEI maximization, refer to Appendix B for details): best objective value \(v^{k,n}_{obj}\), consumed function evaluation times \(v^{k,n}_{fes}\), and runtime complexity \(v^{k,n}_{com}\). To make the values more distinguishable and manageable, they have been first subjected to a logarithmic transformation:

\[v^{k,n}_{*}=(v^{k,n}_{*}). \]

Then, Z-score normalization is applied:

\[Z^{k}_{*}=_{n=1}^{N}_{*}-_{*}}{_{*}}, \]

where \(_{*}\) and \(_{*}\) are calculated by using RS as a baseline. Finally, the AEI is calculated by:

\[AEI=_{k=1}^{K}e^{Z^{k}_{obj}+Z^{k}_{com}+Z^{k}_{res}}, \]

where Z-scores are first aggregated, then subjected to an inverse logarithmic transformation, and subsequently averaged across the test problem instances. A higher AEI indicates better performance of the corresponding MetaBBO-RL approach.

**MGD.** We then introduce the Meta Generalization Decay (MGD) metric, so as to assess the generalization performance of MetaBBO-RL for unseen tasks. Given a model that has been trained on a problem set \(B\) and its AEI on the corresponding testset as \(AEI_{B}\), we train another model on a problem set \(A\) and record its AEI on the testset \(B\) as \(AEI_{A}\). The \(MGD(A,B)\) is computed by:

\[MGD(A,B)=100(1-}{AEI_{B}})\%, \]

where a smaller \(MGD(A,B)\) indicates that the approach generalizes well from \(A\) to \(B\). Note that MGD has neither symmetry nor transitivity properties.

**MTE.** When zero-shot generalization is unachievable due to significant task disparity, the Meta Transfer Efficiency (MTE) metric is proposed to evaluate the transfer learning capacity of a MetaBBO-RL approach. We begin by maintaining checkpoints of a MetaBBO-RL approach trained on a problem set \(B\). We locate the checkpoint with the highest cumulative return, recording its index as \(T_{ scratch}\). Next, we pre-train a checkpoint of the MetaBBO-RL approach on another problem set \(A\), then load this checkpoint back and continue the training on the problem set \(B\) until it reaches the same best-accumulated return, recording the current index as \(T_{ finetune}\). The \(MTE(A,B)\) is calculated by:

\[MTE(A,B)=100(1-}{T_{ scratch}})\%, \]

where a larger \(MTE(A,B)\) indicates that the knowledge learned in \(A\) can be easily transferred to solve \(B\), while an MTE value less than or equal to zero indicates potential negative transfer issues. Similar to MGD, the MTE has neither symmetry nor transitivity properties.

## 4 Benchmarking Study

MetaBox serves as a valuable tool for conducting experimental studies, allowing researchers to 1) benchmark specific groups of MetaBBO algorithms, 2) tune their algorithms flexibly, and 3) perform in-depth analysis on various aspects including generalization and transfer learning abilities. In this section, we provide several examples to illustrate the use of MetaBox in experimental studies.

### Experimental setup

To initiate the fully automated _Train-Test-Log_ process of MetaBox, users need to follow two steps: 1) indicate environment parameters, including _problem-type_, _problem-dim_ and _difficulty_, to inform MetaBox about the problem set, its dimension and the train-test split used in the comparison study; and 2) specify the maximum number of training steps (\(M\)), the number of independent test runs (\(N\)), and the reserved function evaluation times (_maxFEs_) for solving a problem instance. Once these two steps are completed, users can trigger the _run-experiment_() to enjoy the full automation process. In the exemplary study below, \(M\), \(N\) and _maxFEs_ are set to \(1.5 10^{6}\), \(51\), and \(2 10^{4}\), respectively, unless specified otherwise. All results presented are obtained using a machine of Intel i9-10980XE CPU with \(32\)GB RAM. Note that MetaBox is also compatible with other platforms. We provide default control parameters for each baseline, which are listed in Appendix C.

### Comparison of different baseline (Meta)BBO methods

We train all MetaBBO baselines (7 MetaBBO-RL and \(1\) MetaBBO-SL) in the _Baseline Library_ on Synthetic and Protein-Docking testsuites, using both _easy_ and _difficult_ train-test split. We then test these baselines against the classic black-box optimizers available in our library and calculate the AEI of each algorithm. The results are depicted in Figure 4. On the Synthetic testsuites, two MetaBBO-RL methods, namely LDE and RLEPSO, show competitive performance against classic baselines. Specifically, LDE outperforms JDE21, the winner of IEEE CEC 2021 BBO Competitions , on the Synthetic-difficult testsuites, while RLEPSO outperforms both JDE21 and NL-SHADE-LBC, the winner of IEEE CEC 2022 BBO Competitions , on the Synthetic-easy testsuites. However, for this particular problem set, CMA-ES remains a strong baseline. As we shift to more realistic testsuites (Protein-docking of 280 instances), most baselines show varying degrees of performance drop. Notably, CMA-ES is inferior to a number of algorithms such as DEDQN, LDE, RLEPSO, and GLPSO. In general, we observe that MetaBBO-RL methods are more robust than classic hand-craftedoptimizers when tested on different problem sets. For example, DEDQN generally performs the best for the Protein-Docking problems, ranking first and third in _difficult_ and _easy_ modes, respectively. This suggests that the classic optimizers might be only tailored to a specific family of problems, while MetaBBO-RL learns to enhance the low-level optimizer through meta-level learning, thereby exhibiting stronger robustness. Due to the space limitation, we leave additional comparison results, including those on other testsuites, as well as the detailed sub-metrics such as running time, algorithm complexity, and per-instance optimization results in Appendix D.

### Hyper-tuning a MetaBBO-RL approach

Tuning a MetaBBO-RL requires efforts in configuring both the meta-level RL _Agent_ and the low-level black-box _Optimizer_. Fortunately, our MetaBox provides _Template coding_ with convenient interfaces to assist users in accomplishing this task. To illustrate this, we conduct a \(2 2\) grid search to hyper-tune the MetaBBO-RL approach LDE . We noticed that LDE achieved lower AEI scores on Synthetic-easy testsuites than some classic optimizers such as JDE21 and MadDE (see Figure 4). In our study, we investigate both REINFORCE  and PPO  RL agents for LDE, setting the population size (a hyper-parameter of DE) to either 30 or 50 (where the original version of LDE uses

Figure 4: AEI scores of baselines, with error bars denoting the robustness across different tasks. **Top:** Results on Synthetic testsuites with _easy_ or _difficult_ difficulty. **Bottom:** Results on Protein-Docking testsuites with _easy_ or _difficult_ difficulty.

Figure 5: Hyper-tuning for the MetaBBO-RL approach LDE  using a \(2 2\) grid search. **Left:** The average return during the training over \(10\) trials. **Middle:** The normalized cost over \(51\) independent runs during the test. **Right:** The corresponding AEI scores during the test.

REINFORCE and a population size of \(50\)). Figure 5 depicts the average return along the training, the optimization curve during testing, and the AEI scores of these four LDE versions. It turns out that LDE with the PPO agent and a population size of \(30\) achieves higher scores and surpasses JDE21 and MadDE. This shows that, by leveraging the capabilities of MetaBox and conducting hyper-tuning studies, researchers are able to pursue the best configurations for their MetaBBO-RL methods.

### Investigating generalization and transfer learning performance

Existing MetaBBO-RL methods [19; 20; 21; 22; 23; 24; 25] rarely assess their generalization and transfer ability, although they are two crucial aspects for evaluating the effectiveness of a learning-based method. To bridge this gap, we have introduced two explicit indicators (namely the MGD and MTE) in MetaBox to measure the level of learning achieved by a MetaBBO-RL algorithm. In Figure 6, we display the values of MGD and MTE for the MetaBBO-RL approach LDE.

**Generalization performance.** Results in the MGD plot (the left part of Figure 6) show that the pre-trained LDE model possesses a strong generalization ability. When the model is pre-trained on the Noisy-Synthetic testsuites, it exhibits a positive MGD on the Synthetic testsuites, with only a \(2.222\%\) drop in AEI, which is considered acceptable. Additionally, the LDE model trained on the Protein-Docking-easy or Synthetic-easy testsuites both exhibits exceptional generalization to the Noisy-Synthetic-easy testsuites, as evidenced by an MGD of \(-7.006\%\) and \(-7.049\%\). Such robust generalization abilities are highly desirable in practical applications.

**Transfer performance.** We fine-tune the LDE pre-trained on the Noisy-Synthetic testsuites for the Synthetic testsuites. The progress of this fine-tuning process is depicted in the MTE plot (the right part of Figure 6), showing the average return over 10 trials. The results reveal that the pre-trained LDE requires fewer learning steps to arrive at the peak return level than the LDE trained from scratch, with the MTE value of \(20\%\). Compared with the MGD value of \(2.222\%\) when zero-shotting the pre-trained LDE for the Synthetic testsuites, it can be noticed that the MGD and MTE capture two different aspects of the learning effectiveness. By considering both measures, researchers can gain deeper insights into the learning behavior and effectiveness of their MetaBBO-RL approaches.

## 5 Discussion and Future Work

We propose MetaBox, a benchmark platform for Meta-Black-Box Optimization with Reinforcement Learning (MetaBBO-RL). It intends to, 1) provide the first unified benchmark platform, 2) simplify coding towards efficient researching, 3) provide broad testsuites and baselines for comprehensive comparison, and 4) provide novel evaluation metrics for in-depth analysis.

Our preliminary experimental investigations lead to the following key observations. First, although hand-crafted optimizers currently outperform learning-based optimizers (e.g., MetaBBO-RL), they are mostly designed to specific problem sets and become less effective when applied to other problem domains. In contrast, MetaBBO-RL exhibits adaptability across various optimization scenarios,

Figure 6: Meta performance (MGD and MTE) of LDE  across different tasks, tested under the _easy_ mode. **Left:** Logits on \(i\)-th row and \(j\)-th column is the \(MGD(i,j)\), the smaller the better. **Right:** The average return over \(10\) trials is compared between the LDE models pre-trained and trained from scratch. The annotated MTE score shows that it saves 0.2x learning steps through fine-tuning.

showing stronger robustness across different problem domains. This is a significant advantage, as it allows MetaBBO-RL to perform well on diverse problem sets without the need for much manual customization. Second, there is still room for further improvement in MetaBBO-RL, by discovering more effective designs in both meta-level agents and low-level optimizers. We find that even basic modifications, such as incorporating a PPO agent and adjusting the population size, can bring a significant performance boost. Third, interpreting the generalization and transfer effects in MetaBBO-RL can be challenging. While we were able to observe positive and negative effects in terms of MGD and MTE, rationally interpreting and understanding them is still difficult. Hence, additional studies are needed to interpret the learning effects of MetaBBO-RL in a more comprehensible manner.

While our MetaBox presents a significant advancement, we also acknowledge its potential limitations. It is important to note that the evaluation of BBO performance is not a one-size-fits-all endeavour. Different practical applications may have varying preferences and additional concerns, such as solution robustness, solution diversity, parallelization and scalability. These nuances extend beyond the scope of a single evaluation metric. In conjunction with the proposed AEI, MetaBox is committed to exploring more pragmatic and advanced evaluation criteria. Moreover, the problem sets in MetaBox will be expanded to include more diverse and eye-catching BBO tasks. Additionally, proactive maintenance and updates to MetaBox, including the extension of the baseline library, will also be pursued to ensure it is up-to-date with the latest methodologies in the field.