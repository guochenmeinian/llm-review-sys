# Membership Inference on Text-to-image Diffusion Models via Conditional Likelihood Discrepancy

Shengfang Zhai\({}^{1,2}\), Huanran Chen\({}^{3,6}\), Yinpeng Dong\({}^{3,6}\), Jiajun Li\({}^{1,2}\),

Qingni Shen\({}^{1,2}\), Yansong Gao\({}^{4}\), Hang Su\({}^{3,5}\), Yang Liu\({}^{7}\)

\({}^{1}\)School of Software and Microelectronics, Peking University

\({}^{2}\)PKU-OCTA Laboratory for Blockchain and Privacy Computing, Peking University

\({}^{3}\)Dept. of Comp. Sci. and Tech., Institute for AI, BNRist Center, THBI Lab, Tsinghua University

\({}^{4}\)The University of Western Australia \({}^{5}\)Zhongguancun Laboratory, Beijing, China

\({}^{6}\)RealAI \({}^{7}\)Nanyang Technological University

{zhaisf, jiajun.lee}@stu.pku.edu.cn huanran.chen@outlook.com

{dongyinpeng, suhangss}@tsinghua.edu.cn

qingnishen@ss.pku.edu.cn garrison.gao@uwa.edu.au

yangliu@ntu.edu.sg

Corresponding authors.

###### Abstract

Text-to-image diffusion models have achieved tremendous success in the field of controllable image generation, while also coming along with issues of privacy leakage and data copyrights. Membership inference arises in these contexts as a potential auditing method for detecting unauthorized data usage. While some efforts have been made on diffusion models, they are not applicable to text-to-image diffusion models due to the high computation overhead and enhanced generalization capabilities. In this paper, we first identify a conditional overfitting phenomenon in text-to-image diffusion models, indicating that these models tend to overfit the conditional distribution of images given the corresponding text rather than the marginal distribution of images only. Based on this observation, we derive an analytical indicator, namely **C**onditional **L**ikelihood **D**iscrepancy (**CLiD**), to perform membership inference, which reduces the stochasticity in estimating memorization of individual samples. Experimental results demonstrate that our method significantly outperforms previous methods across various data distributions and dataset scales. Additionally, our method shows superior resistance to overfitting mitigation strategies, such as early stopping and data augmentation.

## 1 Introduction

Text-to-image diffusion models have achieved remarkable success in the guided generation of diverse, high-quality images based on text prompts, such as Stable Diffusion , DALLE-2 , Imagen , and DeepFloyd-IF . These models are increasingly adopted by users to create photorealistic images that align with desired semantics. Moreover, they can generate images of specific concepts  or styles  when fine-tuned on relevant datasets. However, the impressive generative capabilities of these models depend heavily on high-quality image-text datasets, which involve collecting image-text data from the web. This practice raises significant privacy and copyright concerns in the community . The pretraining and fine-tuning processes of text-to-image diffusion models can cause copyright infringement, as they utilize unauthorized datasets published by human artists or stock-image websites .

Membership inference (also known as the membership inference attack) is widely used for auditing privacy leakage of training data [4; 53], defined as determining whether a given data point has been used to train the target model. Dataset owners can thus leverage membership inference to determine if their data is being used without authorization [14; 39].

Previous works [5; 15; 16; 17; 28; 38] have attempted membership inference on diffusion models. Carlini et al.  employ LiRA (Likelihood Ratio Attack)  to perform membership inference on diffusion models. LiRA requires training multiple shadow models to estimate the likelihood ratios of a data point from different models, which incurs high training overhead (e.g., 16 shadow models for DDPM  on CIFAR-10 ), making it neither scalable nor applicable to text-to-image diffusion models. Other query-based membership inference methods [15; 17; 28; 38] design and compute indicators to evaluate whether a given data point belongs to the member set. These methods require only a few or even a single shadow model, making them scalable to larger text-to-image diffusion models. However, these methods mainly estimate model memorization for data points and do not fully utilize the conditional distribution of image-text pairs. Consequently, they achieve limited success only under excessively high training steps and fail under real steps or common data augmentation methods (Tab. 2), which do not reflect real training scenarios. Text-to-image diffusion models have demonstrated excellent performance in zero-shot image generation [1; 42; 46], indicating their strong generalization, which makes it difficult to distinguish membership by directly measuring overfitting to data points. And due to the stochasticity of diffusion training loss [22; 46], this kind of measuring becomes more challenging.

To address the challenges, we firstly identify a **Conditional Overfitting** phenomenon of text-to-image diffusion models with empirical validation, where the models exhibit more significant overfitting to the conditional distribution of the images given the corresponding text than the marginal distribution of the images only. It inspires the revealing of membership by leveraging the overfitting difference. Based on it, we propose to perform membership inference on text-to-image diffusion models via **C**onditional **L**ikelihood **D**iscrepancy (**CLiD**). Specifically, CLiD quantifies overfitting difference analytically by utilizing Kullback-Leibler (KL) divergence as the distance metric and derives a membership inference indicator that estimates the discrepancy between the conditional likelihood of image-text pairs and the likelihood of images only. We approximate the likelihoods by employing Monte Carlo sampling on their ELBOs (Evidence Lower Bounds), and design two membership inference methods: a threshold-based method CLiD\({}_{th}\) and a feature vector-based method CLiD\({}_{vec}\).

We conduct extensive experiments on three text-to-image datasets [32; 35; 66] with various data distributions and dataset scales, using the mainstream open-sourced text-to-image diffusion models [11; 47] under both fine-tuning and pretraining settings. First, our methods consistently outperform existing baselines across various data distributions and training scenarios, including fine-tuning settings and the pretraining setting. Second, our experiments on fine-tuning settings with different training steps (Sec. 4.2) reveal that excessively high step/image ratios cause overfitting, leading to hallucination success; and we develop a more realistic pretraining setting following [13; 16], where our experiments reveal the insufficient effect of existing membership inference works [15; 17; 28; 38]. Third, our comparison experiment with varying training steps (Sec. 4.3) indicates that the effectiveness of membership inference grows with higher step/image ratios and should be evaluated under reasonable settings for realistic results. Next, ablation studies further demonstrate the effect of our CLiD indicator, even with fewer query count, our method still outperforms baseline methods (Fig. 3). Last, experiments show that our methods exhibit stronger resistance to data augmentation, and exhibit resistance to even adaptive defenses.

## 2 Diffusion Model Preliminaries

**Denoising Diffusion Probabilistic Model (DDPM)** learns the data distribution \(_{0} q()\) by reversing the forward noise-adding process. For the forward process, DDPM defines a Markov process of adding Gaussian noise step by step:

\[q(_{t}|_{t-1}):=(_{t};}_{t-1},_{t}), \]

where \(_{t}(0,1)\) is the hyperparameter controlling the variance. For the reverse process, DDPM defines a learnable Markov chain starting at \(p(_{T})=(_{T};,)\) to generate \(_{0}\):

\[p_{}(_{0})=_{_{1:T}}p(_{T})_{t=1}^ {T}p_{}(_{t-1}|_{t})\;_{1:T},  p_{}(_{t-1}|_{t})=(_{t-1} ;_{}(_{t},t),_{t}^{2}), \]where \(_{t}^{2}\) is the untrained time-dependent constant. \(\) represents the trainable parameters. To maximize \(p_{}(_{0})\), DDPM optimizes the Evidence Lower Bound (ELBO) of the log-likelihood [22; 33]:

\[ p_{}(_{0})_{q(_{1:T}|_{0 })}[(_{0:T})}{q(_{1:T}_{0})}]=-_{,t}[||_{}(_{t},t)- ||^{2}]+C, \]

where \((0,)\), \(t(1,...,T)\) and \(C\) is a constant. \(_{t}\) is obtained from Eq. (1), and \(_{}\) is a function approximator intended to predict the noise \(\) from \(_{t}\). Omitting the untrainable constant in Eq. (3) and taking its negative yields the loss function of training DDPM.

**Conditional diffusion models**[40; 21; 46]. To achieve controllable generation ability, text-to-image diffusion models incorporate the conditioning mechanism into the model, which are also known as conditional diffusion models, enabling them to learn conditional probability as:

\[p_{}(_{0}|)=_{_{1:T}}p(_{T} )_{t=1}^{T}p_{}(_{t-1}|_{t},)\;_{1:T}, \]

where \(\) denotes the embedding of condition. For text-to-image synthesis, \(:=()\), where \(\) and \(\) denote the text input and the text encoder, respectively. Similar to Eq. (3), through derivation , we can obtain the ELBO of the conditional log-likelihood:

\[ p_{}(_{0}|)-_{,t}[|| _{}(_{t},t,)-||^{2}]+C. \]

## 3 Methodology

In this section, we detail the proposed **C**onditional **L**ikelihood **D**iscrepancy (**CLiD**) method. We first introduce the threat model of query-based membership inference in Sec. 3.1. We then identify the conditional overfitting phenomenon with experimental validation in Sec. 3.2. We further drive the membership inference indicator based on CLiD in Sec. 3.3 and design two practical membership inference methods in Sec. 3.4. We finally provide the implementation details in Sec. 3.5.

### Threat Model

We use the standard security game of membership inference on image-text data following previous work [4; 5; 38; 48]. We define a challenger \(\) and an adversary \(\) who performs membership inference. \(\) samples a member set \(D_{}\) and trains or fine-tunes a text-to-image diffusion model \(f_{}\) (i.e., target model) with \(D_{}\). The rest of \(\) is denoted by hold-out set \(D_{}= D_{}\). For a given data point \((,)\), \(\) designs an algorithm \(\) to yield a membership prediction:

\[(,,f_{})=[^{ }(,,f_{})>], \]

where \(^{}\) denotes an indicator function that reflects membership information, and \(\) denotes a tunable decision threshold of query-based membership inference [15; 17; 28; 38].

We consider a grey-box setting 2 consistent with previous query-based methods [15; 17; 28; 38]. This setting assumes that \(\) has access to the intermediate outputs of models without knowledge of specific model parameters. For the given image-text data point \((,)\), we assume that \(\) and \(\) always correspond within the dataset \(\). This assumption is evident in scenarios where dataset copyright owners perform membership inference to audit usage. And we also consider a weaker assumption of conducting membership inference without the groundtruth text in Sec. 4.6.

Conversely, challenger \(\) can mitigate the effectiveness of membership inference during training by utilizing data augmentation or even adaptive defense methods, which we discuss in Sec. 4.5. Our work primarily focuses on fine-tuning scenarios because the weights of pretrained models are readily available, making this scenario more prone to copyright risks [41; 56]. Numerous projects are implemented by fine-tuning open-source models on specific datasets [3; 24; 60; 64]. We also conduct experiments on pretrained text-to-image diffusion models (Tab. 3) to demonstrate the effectiveness of our method even in pretraining scenarios.

### Conditional Overfitting Phenomenon

The rationale behind previous studies primarily hinges on the overfitting of diffusion models to training data (usually image data \(\)) . This overfitting tends to result in lower estimation errors for images in the member set (training data) compared to those in the hold-out set during the diffusion process. Various indicators  are designed based on this to expose membership information. Specifically, let \(q_{}\) and \(q_{}\) represent the image distributions of the member set and the hold-out set, respectively. \(p\) represents the diffusion models' estimated distribution, and \(D\) denotes a distance metric (which will be specified later). This rationale can be formulated as:

\[D(q_{}(),p()) D(q_{}(),p ()). \]

However, if considering the membership inference on text-to-image diffusion models with image-text data \((,)\), we emphasize the following assumption:

**Assumption 3.1** (Conditional overfitting phenomenon).: _The overfitting of text-to-image diffusion models to the conditional distribution of \((,)\) is more salient than to the marginal distribution of \(\):_

\[_{}[D(q_{}(|),p (|))-D(q_{}(|),p(| ))]}_{}}(),p())-D(q_{}(),p())}_{}. \]

Empirically, we validate this assumption by using Frechet Inception Distance (FID)  as the metric \(D\), i.e., \(D_{FID}\). We calculate \(D_{FID}(q(|),p(|))\) using the MS-COCO  dataset on a fine-tuned Stable Diffusion  model. Then by gradually truncating the original condition text to \(\{2/3,1/3,Null\}\) to obtain \(^{*}\), we calculate \(D_{FID}(q(|^{*}),p(|^{*}))\) as a stepwise approximation of \(D_{FID}(q(),p())\). In Fig. 1, we report the FID scores of synthetic images under different conditions of member set and hold-out set. A smaller FID value indicates a closer match between model distributions and dataset distributions. From Fig. 1 (a), it can be observed that for the full condition, the FID difference between the member set and the hold-out set is consistently higher than that for the truncated conditions, which validates our assumptions. We also demonstrate the validation utilizing other metrics in Appendix A.

We further compute the change in FID after truncating the condition and observe that the change in FID of the member set is consistently greater than that of the hold-out set (Fig. 1 (b)), which inspires revealing membership by this overfitting discrepancy. Recalling the aim of text-to-image diffusion model is to fit a latent space mapping from text to image, image data augmentation is commonly used to enhance the model generalization. For instance, the official fine-tuning script of Hugging-Face  employs Random-Crop and Random-Flip as the default augmentation . However, few trainers disturb the text condition as it is discrete and such disturbance would result in a decline of model utility (Sec. 4.5). Therefore, we believe that leveraging this phenomenon contributes to addressing the challenges of the strong generalization of text-to-image diffusion models with the resistance to data augmentation.

### Condition Likelihood Discrepancy

In this section, we derive a membership inference indicator for a given individual sample based on Assumption 3.1. Calculating FID requires sampling lots of images from the \(p\) distribution, which is impractical under membership inference scenarios. Instead, we employ Kullback-Leibler (KL) divergence as the distance metric, which is widely used and computationally convenient (the usage of other metrics is discussed in Appendix C). Then we have the following theorem:

**Theorem 3.2**.: _(Proof in Appendix B) When using \(D=D_{KL}\) as distance metric, Assumption 3.1 is equivalent to:_

\[_{q_{}(,)}[ p(|)- p()]_{q_{}(,)}[  p(|)- p()]+_{H}, \]

Figure 1: FID values and the FID differences of synthetic images (\(2500/2500\) samples for member/hold-out set) under different conditions of member set and hold-out set.

_where_

\[_{H}=H(q_{}())+_{}[H(q_{}( |))]-H(q_{}())-_{}[ H(q_{}(|))]. \]

Let us define:

\[(,)= p(|)- p( ). \]

If \(_{H}\) is negligible, then according to Eq. (9), it holds that \(_{q_{}}[()]_{q_ {}}[()]\), where \(\) is a constant intermediate between the left-hand side and right-hand side. Membership inference is then posed as follows: given an input instance \((,)\), measuring \((,)\) to predict how probable it is that the input is a sample from \(q_{}\) rather than \(q_{}\). Intuitively, if \(()\) exceeds a threshold \(\), the instance is likely from \(q_{}\); otherwise, it belongs to \(q_{}\). In the community of membership inference methods [4; 5; 6; 15; 17; 28; 38; 65], setting such a threshold \(\) is a standard practice to differentiate between the two distributions. Therefore, we can utilize the indicator \((,)\) for membership inference. Since Eq. (11) actually involves measuring the likelihood discrepancy under different conditions of diffusion models, we call it **C**onditional **L**ikelihood **D**iscrepancy (**CLiD**).

In order to calculate the likelihoods in Eq. (11) for a given data point \((,)\), we utilize the ELBOs in Eq. (3) and Eq. (5) as an approximation of the log-likelihoods:

\[(,)=_{t,}[||_{ }(_{t},t,_{})-||^{2}]- _{t,}[||_{}(_{t},t,)- ||^{2}], \]

where \(_{}\) denotes an empty text condition input used to estimate the approximation of \( p_{}()\).

### Implementation of CLiD-MI

In practice, calculating Eq. (12) needs a Monte Carlo estimate for data point by sampling \(N\) times using \((t_{i},_{i})\) pairs, with \(_{i}(,)\) and \(t_{i}\). Performing two Monte Carlo estimations independently incurs high computational costs, resulting in \(2 N\) query count, where \(N\) is typically a large number to ensure accurate estimation. To simplify computation, we instead perform Monte Carlo estimation on the difference of the ELBOs inspired by :

\[(,)=_{t,}[||_{ }(_{t},t,_{})-||^{2}-||_{ }(_{t},t,)-||^{2}]. \]

In experiments, to further mitigate randomness, we also consider diverse reduced conditions along with \(_{}\), forming the reduced condition set \(=\{_{1}^{*},_{2}^{*}...,_{k}^{*}\}\), where we set \(_{k}^{*}=_{}\). Then we compute multiple condition likelihood discrepancies:

\[_{,,_{i}^{*}}=_{t, }[||_{}(_{t},t,_{i}^{*})- ||^{2}-||_{}(_{t},t,)-||^{ 2}], \]

where \(_{i}^{*}\). In subsequent parts, we employ their mean or treat them as feature vectors to reveal membership information. We will introduce how to obtain \(\) in Sec. 3.5.

**Combining \(p_{}(|)\) for further enhancement.** Recall that the practical significance of sample likelihood is the probability that a data point originates from the model distribution, which essentially can also be used to assess membership. Due to the monotonicity of the log function, we can also use ELBO of Eq. (5) to estimate \(p_{}(|)\):

\[_{,}=-_{t,}[||_ {}(_{t},t,)-||^{2}]. \]

Additionally, this estimation can reuse results from estimating Eq. (14), thus obviating any additional query counts. Next, we consider two strategies to combine Eq. (14) and Eq. (15) to construct the final membership inference method.

**Threshold-based attack-CLiD\({}_{th}\).** First, we normalize the two indicators to the same feature scale. Due to the outliers in the data, we use Robust-Scaler: \((a_{i})=(a_{i}-)/IQR\), where \(a_{i}\) denotes the \(i\)-th value, \(\) denotes the mean and IQR (interquartile range) is defined as the difference between the third quartile (Q3) and the first quartile (Q1) of the feature. Then we have:

\[_{_{th}}(,)=[ (_{i}^{k}_{,, _{i}^{*}})+(1-)(_{, })>], \]

where \(k\) denotes the total number of reduced \(^{*}\) (i.e., \(k=||\)), and \(\) is a weight parameter.

**Vector-based attack-CLiD\({}_{rec}\).** We combine the estimated values of Eq. (14) and Eq. (15) to obtain the feature vectors corresponding to each data point:

\[=(_{,,_{1}^{*}}, _{,,_{2}^{*}}_{ ,,_{k}^{*}},_{,} ). \]We use a simple classifier to distinguish feature vectors in order to determine the membership of the samples:

\[_{_{vec}}(,)=[_{}()>], \]

where \(_{}\) denotes the predict confidence of the classifier.

### Practical Considerations

**Reducing conditions to obtain \(^{*}\).** We consider three methods for diverse reduction: (1) Simply taking the first, middle, and last thirds of the sentences as text inputs. (2) Randomly adding Gaussian noises with various scales to the text embeddings. (3) Calculating the importance of words in the text [55; 57] and replacing them with "pad" tokens by varying proportions in descending order. For all three methods, we additionally use the null text input as \(^{*}_{k}\). These methods are all effective and we use (3) with \(k=4\) in subsequent experiments (details in Appendix D).

**Monte Carlo sampling.** Let \(M\) and \(N\) denote the Monte Carlo sampling numbers of estimating \((,)\) and \(_{,,^{*}}\), respectively. We set \(M=N\) to achieve result reuse between Eq. (14) and Eq. (15), reducing the number of Monte Carlo sampling. Hence the overall query count of one data point is \(M+K N\). Significant effects can be observed even when \(M,N=1\) (Fig. 3).

**Classifiers of \(_{vec}\).** Due to the simplicity of the feature vectors, we do not need a neural network as the classifier . Simpler classifiers help to prevent overfitting. In our experiments, we utilize XGBoost  and utilize its predict confidence.

## 4 Experiments

### Setups

**Datasets and models.** For the fine-tuning setting, we select \(416/417\) samples on Pokemon , \(2500/2500\) samples on MS-COCO  and \(10,000/10,000\) samples on Flickr  as the member/hold-out dataset, respectively. These three datasets involve diverse data distributions and dataset scales. We use the most widely used text-to-image diffusion model, Stable Diffusion v1-43, as the target model to fine-tune it on these three datasets. For the pretraining setting, we conduct experiments on Stable Diffusion v1-54 using the processed LAIION dataset  (detailed in Sec. 4.2) to minimize distribution shift [13; 16].

**Fine-tuning setups.** For fine-tuning, previous membership inference on text-to-image diffusion models usually relies on strong overfitting settings. To evaluate the performance more realistically, we consider the two following setups: (1) _Over-training._ Following the previous works [15; 17; 28], we fine-tune 15,000 steps on Pokemon datasets, and 150,000 steps on MS-COCO and Flickr (with only \(2500/2500\) dataset size). (2) _Real-world training._ Considering that trainers typically do not train for such high steps, we recalibrate the steps based on the training steps/dataset size ratio (approximately 20) of official fine-tuning scripts on Huggingface . Thus, we train 7,500 steps, 50,000 steps and 200,000 steps for the Pokemon, MS-COCO and Flickr datasets, respectively. Additionally, we employ the default data augmentation (Random-Crop and Random-Flip ) in training codes  to simulate real-world scenarios.

**Baselines.** We broadly consider existing member inference methods applicable to text-to-image diffusion models as our baselines: Loss-based inference , \(_{}\) (SecMI) , PIA , \(_{}\) (PFAMI)  and an additional method of directly conducting Monte Carlo estimation (M. C.) on Eq. (15) for comparison. For all baselines, we use the parameters recommended in their papers. We omit some membership inference methods for generative models [6; 19; 36], as they have been shown ineffective for diffusion models in previous works [15; 17].

**Evaluation metrics.** We follow the widely used metrics of previous works [4; 5; 15; 17; 28], including ASR (i.e., the accuracy of membership inference), AUC and the True Positive Rate (TPR) when the False Positive Rate (FPR) is 1% (i.e., TPR@1%FPR).

**Implementation details.** Our evaluation follows the setup of representative membership inference works [4; 5]. It is important to note that some implementations [26; 29] of previous works assume access to a portion of the exact member set and the hold-out set to obtain a threshold for calculating ASR or to train a classification network . This assumption does not align with real-world scenarios. Therefore, we strictly adhere to the fundamental assumption of membership inference [4; 17]: knowing only the overall dataset without any knowledge of the member/hold-out split. Hence, we first train a shadow model to obtain the \(\) for Eq. (16), classifiers for Eq. (18) and the threshold \(\) for calculating ASR with auxiliary datasets of the same distribution. Then we perform the test on the target models. Other implementation details are provided in Appendix D.

### Main Results

**Over-training setting (fine-tuning).** In Tab. 1, models are trained for excessive steps on all three datasets, resulting in significant overfitting. We observe that under this over-training scenario, both of our methods nearly achieve ideal binary classification effectiveness. For instance, \(_{th}\) achieves over 99% ASR, AUC and TPR@1%FPR value on the MS-COCO dataset . With this training setup, the metrics for different baselines are very similar. Even the simplest loss-based method  (with the query count of 1) also yields satisfactory results compared with other high query count methods. Therefore, we emphasize: _This unrealistic over-training setting fails to adequately reflect the effectiveness differences among various membership inference methods_.

**Real-world training setting (fine-tuning).** In Tab. 2, we adjust the training steps simulating real-word training scenario  and utilize default data augmentation . The best value of ASR and AUC of baseline methods decreases to around 65%, and the best value of TPR@1%FPR decreases to around 5%, indicating insufficient effectiveness of previous member inference methods in real-world training scenarios of text-to-image diffusion models. In contrast, our methods maintain ASR above 86% and AUC above 93%, exceeding the best baseline values by about 30%. The TPR@1%FPR of our methods exceeds the best baseline values by 50%-60%. The results demonstrate the effectiveness of our methods across various data distributions and scales in real-world training scenarios.

**Pretraining setting.** For the pretraining setting, we adopt a stringent and realistic membership inference setting based on previous works [13; 16]. (1) To ensure the distribution consistency between the member and hold-out set, we respectively select \(2500\) samples from the LAION-Aesthetics v2 5+ and LAION-2B MultiTranslated  as member/hold-out set following ; (2) We filter out samples containing non-English characters to ensure there are no other "distinguishable tails"  in

    &  &  &  \\   & ASR & AUC & TPR@1\%FPR & ASR & AUC & TPR@1\%FPR & ASR & AUC & TPR@1\%FPR & \\  Loss & 81.92 & 89.98 & 32.28 & 81.90 & 90.34 & 40.80 & 83.76 & 91.79 & 25.77 & 1 \\ PIA & 68.56 & 75.12 & 5.08 & 68.56 & 75.12 & 5.08 & 83.37 & 90.95 & 13.31 & 2 \\ M. C. & 82.04 & 89.77 & 36.04 & 83.32 & 91.37 & 41.20 & 79.35 & 86.78 & 23.74 & 3 \\ SecMl & 83.00 & 90.81 & 50.64 & 62.96\({}^{}\) & 89.29 & 48.52 & 80.49 & 90.64 & 9.36 & 12 \\ PFAMI & 94.48 & 98.60 & 78.00 & 90.64 & 96.78 & 50.96 & 89.86 & 95.70 & 65.35 & 20 \\   \(_{th}\) & 99.08 & **99.94** & **99.12** & 91.42 & 97.39 & **74.00** & **97.96** & 99.28 & **97.84** & 15 \\ \(_{rec}\) & **99.74** & 99.31 & 95.20 & **91.78** & **97.52** & 73.88 & 97.36 & **99.46** & 96.88 & 15 \\   

* When conducting SecM , we observe that the thresholds obtained on the shadow model sometimes do not transfer well to the target model.

Table 1: Results under _Over-training_ setting. We mark the best and second-best results for each metric in **bold** and underline, respectively. Additionally, the best results from baselines are marked in blue for comparison.

    &  &  &  \\   & ASR & AUC & TPR@1\%FPR & ASR & AUC & TPR@1\%FPR & ASR & AUC & TPR@1\%FPR & \\  Loss & 56.28 & 61.89 & 1.92 & 54.91 & 56.60 & 1.83 & 61.03 & 65.96 & 2.82 & 1 \\ PIA & 54.10 & 55.52 & 1.76 & 51.96 & 52.73 & 1.28 & 58.34 & 59.95 & 2.64 & 2 \\ M. C. & 57.98 & 61.97 & 2.64 & 54.92 & 56.78 & 2.16 & 61.10 & 66.48 & 3.84 & 3 \\ SecMl & 60.94 & 65.40 & 3.92 & 55.60 & 63.85 & 2.76 & 61.28 & 65.56 & 0.84 & 12 \\ PFAMI & 57.36 & 60.39 & 2.72 & 54.68 & 56.13 & 1.80 & 58.94 & 63.53 & 5.76 & 20 \\   \(_{th}\) & 88.88 & 96.13 & **67.52** & 87.12 & 94.74 & 53.56 & **86.79** & **93.28** & **61.39** & 15 \\ \(_{rec}\) & **89.52** & **96.30** & 66.36 & **88.86** & **95.33** & **53.92** & 85.47 & 92.61 & 59.95 & 15 \\   _{th}\) exhibits a significantly faster increase in effectiveness trajectory. By \(25,000\) steps, \(_{th}\) effectively exposes membership information, whereas other baselines achieve similar results only at around \(150,000\) steps. This demonstrates that our method can effectively reveal membership information when the overfitting degree of the text-to-image diffusion model is much weaker.

### Ablation Study

To conduct an ablation study, we vary the Monte Carlo sampling count in Eq. (14) and Eq. (15), perform \(_{th}\) with MS-COCO dataset under _real-world training_ setting and report the AUC values in Fig. 3. To further compare the effects of Eq. (14) and Eq. (15), we discard each term in Eq. (16) and denote it as \(M/N=0\). We also include the result of the best baseline, SecMI , as a comparison.

**Effect of \(_{,,^{*}}\).** In Fig. 3, results of "M=1, N=0" and "M=1, N=1" show a significant improvement of membership inference by including \(_{,,^{*}}\). Results of "M=5, N=0" and "M=1, N=1" further show that the method utilizing \(_{,,^{*}}\) performs much better under the same sampling numbers. Additionally, the results of "M=0, N=1" and "M=1, N=1" indicates that only considering both Eq. (14) and Eq. (15) achieves the optimal performance.

    &  &  \\    & ASR & AUC & TPR@1\%FPR & \\  Loss & 51.78 & 50.90 & 1.75 & 1 \\ PIA & 52.13 & 52.42 & 1.25 & 2 \\ M. C. & 53.18 & 53.96 & 1.25 & 3 \\ SecMI & 57.43 & 58.59 & 2.45 & 12 \\ PFAMI & 59.08 & 61.11 & 1.45 & 20 \\  \(_{th}\) & **64.53** & **67.82** & **5.01** & 15 \\   

Table 3: The performance of membership inference methods on Stable Diffusion v1-5  in pretraining setting. We utilize the processed LAION dataset to ensure the distribution consistency between member / hold-out sets [13; 16]. The best results are highlighted in **bold**.

Figure 3: Performance of \(_{th}\) and SecMI under various Monte Carlo sampling numbers (i.e., query count). The legend labels are sorted in ascending order by AUC values.

Figure 2: Effectiveness trajectory on various training steps.

**Monte Carlo sampling numbers.** In Fig. 3, we observe that when setting \(M=N\), the performance improves as the number of Monte Carlo sampling increases. And the performance is improved slightly when \(M,N>3\). Hence, we set \(M,N=3\) to ensure the balance between a low query count and satisfied performance. Moreover, the experiment results of "M=1, N=1" and "SecMI" also demonstrate: CLiD\({}_{th}\) outperforms previous works even with a much fewer query count.

### Resistance to Defense

Since data augmentation is commonly used in training and can mitigate the effectiveness of membership inference , we use it to evaluate the performance of methods under defense. As the baseline methods already exhibit weak performance under _real-world training_ setting, we opt not to incorporate additional data augmentation. Instead, we remove the default data augmentation from training scripts  to observe the effectiveness change of different methods. We fine-tune Stable Diffusion models for 50,000 steps with MS-COCO, report the metrics, and calculate the metrics changes in Tab. 4. We observe that the effectiveness of all membership inference methods declines after data augmentation is introduced during training. Note that PFAMI  exhibits the highest sensitivity to data augmentation since it infers membership by probability fluctuation after images are perturbed, which also explains its significant performance decline between Tab. 1 and Tab. 2. Compared to the baselines, our method exhibits the smallest decrease, which indicates its stronger resistance to data augmentation.

**Adaptive defense.** We further consider adaptive defense: assuming the trainers are aware of our methods and perturb the text of image-text datasets before training. We consider the following adaptive defense methods: (1) rephrasing the original text6, (2) randomly deleting 10%, 30% words in text, and (3) shuffling 50% of the image-text mappings in the dataset. In Tab. 5, we observe that except for _shuffling_, the other adaptive defense methods have almost no effect on CLiD\({}_{th}\). And _shuffling_ damages the model utility (too high FID values), rendering this defense meaningless.

### Weaker Assumption

Although in Sec. 3.1 we assume that the adversary can access the entire image-text pairs based on the real-world data usage auditing scenario, we also consider a weaker assumption: the adversary can only access the image without the corresponding text.

In this scenario, we first generate pseudo-text corresponding to the images using an image captioning model (BLIP  in our experiments), and then conduct CLiD-MI based on the image-pseudo_text pairs. In Tab. 6, we observe that our method still broadly outperforms baselines. We believe this is because the pseudo-text preserves the image's key semantics, keeping our methods effective.

    &  &  \\   & ASR & AUC & TPR@1\%FPR & ASR (\(\)) & AUC (\(\)) & TPR@1\%FPR (\(\)) \\  Loss & 66.54 & 72.73 & 7.72 & 56.28 (-10.26) & 61.89 (-10.84) & 1.92 (-5.80) \\ PIA\({}^{}\) & 56.56 & 59.28 & 2.00 & 54.10 (-2.46) & 55.52 (-3.76) & 1.76 (-0.24) \\ SecMI & 72.02 & 81.07 & 13.72 & 60.94 (-11.08) & 65.40 (-15.08) & 3.92 (-9.80) \\ PFAMI & 79.20 & 87.05 & 18.44 & 57.36 (-21.84) & 60.39 (-26.66) & 2.72 (-15.72) \\   _{th}\)} & 96.76 & 99.47 & 91.72 & 88.88 (**-7.88**) & 96.13 (**-3.34**) & 67.52 (-24.20)\({}^{}\) \\   

\({}^{}\)We omit the discussion of PIA as it shows no effectiveness at this training steps, with the metrics consistently approximating random guessing.

\({}^{}\)The TPR@1%FPR value changes significantly here because its ROC curve is very sharp when FPR close to \(0\).

Table 4: The performance of different methods under no augmentation and default augmentation.

    & _{th}\) on MS-COCO} &  \\    & ASR & AUC & TPR@1\%FPR & \\  None & 88.88 & 96.13 & 67.52 & 13.17 \\  Reph & 85.32 & 93.83 & 55.67 & 13.58 / +0.41 \\ Del-1 & 86.40 & 93.59 & 59.52 & 13.18 / -0.01 \\ Del-3 & 83.91 & 91.52 & 52.03 & 12.92 / -0.25 \\ Shuffle & 65.89 & 67.37 & 0.15 & 18.26 / +5.09\({}^{}\) \\   

\({}^{}\)Compared to other methods, the increase in FID caused by shuffling is unacceptable for generative models.

Table 5: Effectiveness of CLiD\({}_{th}\) in adaptive defense. We calculate the FID  with \(10,000\) unseen MS-COCO samples to assess the model utility.

## 5 Related Works

**Copyright protection in text-to-image synthesis.** To protect the copyright of text-to-image models, several works [67; 68] propose inserting backdoors to embed watermarks in text-to-image models. To protect the copyright of image-text datasets, some works [50; 52; 69] incorporate imperceptible perturbations to render the released datasets unusable. Other works [12; 56] utilize the backdoor or watermark to track the usage of image-text datasets. In contrast, our method indicates the possibility of auditing the unauthorized usage of individual image-text data points utilizing membership inference.

**Membership inference on diffusion models.** In the grey-box or white-box setting, Carlini et al.  firstly conduct membership inference on unconditional diffusion models by conducting LiRA (Likelihood Ratio Attack) , with the requirement of training multiple shadow models. Matsumoto et al.  make the first step by utilizing division loss to conduct query-based membership inference. Some works [15; 28] leverage the DDIM  deterministic forward process  to access the posterior estimation errors of diffusion models. And Fu et al.  leverage the probability fluctuations by perturbing image samples. Few works consider the black-box settings [41; 62]. However, these studies either assume partial knowledge of member set data  or assume extensive fine-tuning steps  (\(100 500\) epochs), both of which do not align with real-world scenarios.

**Memorization detection in text-to-image models.** A similar work  detects token memorization by inspecting the magnitude of text-conditional predictions, but differs from ours by lacking in-depth rationale analysis and a rigorous membership inference setup with randomly selected member/hold-out sets.

## 6 Conclusion

In this paper, we identify the phenomenon of conditional overfitting in text-to-image models and propose **CLiD-MI**, the membership inference framework on text-to-image diffusion models utilizing the derived indicator, conditional likelihood discrepancy. Experimental results demonstrate the superiority of our method and its resistance against early stopping and data augmentation. Our method aims to inspire a new direction for the community regarding unauthorized usage auditing.

**Limitations:** Due to the limited availability of open-source text-to-image diffusion models, evaluations under the pretraining setting are not sufficient. Considering fine-tuning setting involves a multi step/image ratio, we acknowledge that the superiority of **CLiD-MI** over the baselines in the pretraining setting is not as evident compared to fine-tuning setting. We emphasize our experiments under pretraining setting (Tab. 3) reveal the hallucination success of existing works and encourage future research to focus on this more challenging and practical scenario.