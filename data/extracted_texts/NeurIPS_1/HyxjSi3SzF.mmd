# Communication Bounds for the Distributed Experts Problem

Zhihao Jia

Carnegie Mellon University

zhihao@cmu.edu

&Qi Pang

Carnegie Mellon University

qipang@cmu.edu &Trung Tran

University of Pittsburgh

tbt@pitt.edu &David Woodruff

Carnegie Mellon University

dwoodruf@cs.cmu.edu &Zhihao Zhang

Carnegie Mellon University

zhihao23@cs.cmu.edu &Wenting Zheng

Carnegie Mellon University

wenting@cmu.edu

###### Abstract

In this work, we study the experts problem in the distributed setting where an expert's cost needs to be aggregated across multiple servers. Our study considers various communication models such as the message-passing model and the broadcast model, along with multiple aggregation functions, such as summing and taking the \(_{p}\) norm of an expert's cost across servers. We propose the first communication-efficient protocols that achieve near-optimal regret in these settings, even against a strong adversary who can choose the inputs adaptively. Additionally, we give a conditional lower bound showing that the communication of our protocols is nearly optimal. Finally, we implement our protocols and demonstrate empirical savings on the HPO-B benchmarks.

## 1 Introduction

Online prediction with expert advice is an indispensable task in many fields, including bandit learning (Auer et al., 2002; Lattimore and Szepesvari, 2020), online optimization (Shalev-Shwartz et al., 2012; Hazan et al., 2016), robot control (Doyle et al., 2013), and financial decision making (Dixon et al., 2020). The problem involves \(n\) experts making individual predictions and receiving corresponding costs on each of \(T\) days. On each day, we choose an expert based on the historical costs of the experts on previous days, and we receive the cost of the selected expert on that day. The objective is to compete with the best single expert in hindsight, i.e., to minimize the average _regret_, defined as the additional cost the algorithm incurs against the best expert in a horizon of \(T\) days. It is known that the _Exponential Weights Algorithm_ (EWA) and _Multiplicative Weight Update_ (MWU) method achieve an optimal regret of \(O(})\) given all historical information, even in the presence of a strong adversary Arora et al. (2012). With less information, the _exponential-weight algorithm for exploration and exploitation_ (Exp3) achieves near-optimal regret \(O(})\) in the adversarial bandit setup, where only the cost of one expert is observed on a single day.

For a large number of experts and days, it may not be feasible to run classical low-regret algorithms. Motivated by this, recent work (Srinivas et al., 2022; Peng and Zhang, 2022; Woodruff et al., 2023; Peng and Rubinstein, 2023; Aamand et al., 2023) considers the experts problem in the _data stream model_, where the expert predictions are typically streamed through main memory, and a small summary of historical information is stored.

In this paper, we consider an alternative model in the big data setting, namely, the distributed model, where expert costs are split across \(s\) servers, and there is a central coordinator who can run a low-regretalgorithm. However, communicating with different servers is expensive, and the goal is to design a low communication protocol that achieves low regret.

A _motivating example_ is a distributed online optimization problem, where different servers hold different samples, and each expert could correspond to a different model in an optimization problem over the union of the samples as in the HPO-B real-world benchmark (Arango et al., 2021). In this case, it is natural for the cost of an expert to be the sum of the costs of the expert across all servers. The goal is thus to minimize the cumulative costs in an online fashion by choosing models on a daily basis. Another example of an aggregation function could be the maximum across servers; indeed, this could be useful if there is a maximum tolerable cost on the servers, which we would like not to exceed. For our lower bounds, we also ask the protocol to be able to tell at least if the cost of the expert it chose on a given day is non-zero; this is a minimal requirement of all existing algorithms, such as MWU or Exp3, which update their data structure based on such a cost. It is also desirable in applications such as the experts problem where one wants to know if the prediction was right or wrong.

In our setting, a coordinator needs to choose an expert based on historical interactions with \(s\) servers each day. We focus on two widely studied communication models, namely, the _message-passing model_ with two-way communication channels and the _broadcast model_ with a broadcast channel. In the message-passing model, the coordinator initiates a round of interaction with a given server, and the messages exchanged are only seen by the coordinator and that particular server. The coordinator then decides who speaks next and repeats this process. The broadcast model is also commonly studied in practice and theory. It can be viewed as a model for single-hop wireless networks. In the broadcast model, each message exchanged is seen by all servers and the coordinator. We note that the broadcast model was a central communication model studied for clustering in Chen et al. (2016).

As in the distributed online learning setup, we can view each server as a database, where it possibly receives new data daily. The costs of the \(n\) experts on a day then correspond to \(n\) possibly different functions of the data on that day. We note that the costs may be explicitly given or implicit functions of the data, and if the latter, they may only need to be computed as required by the protocol.

We aim to achieve a near-optimal regret versus communication tradeoff in this setting over a horizon of \(T\) days. Given the memory-efficient streaming algorithms of Srinivas et al. (2022); Peng and Zhang (2022) and the close connection between streaming algorithms and communication-efficient protocols, one might think that implementing a streaming algorithm in our settings is optimal. While we could run a streaming algorithm, a critical difference here is that the coordinator is not memory-bounded and thus can afford to store a weight for each expert. While it cannot run EWA or MWU, which would require \((sn)\) communication per day, it can run a distributed Exp3 algorithm, which samples a single expert and thus has low communication, but maintains a weight locally for all \(n\) experts using \((n)\) memory. We stress _this is not possible in the streaming model_.

    &  \\  &  \\  Algorithms & DEWA-S & DEWA-M & DEWA-L & DEWA-L \\  Agg Func & sum & max & \(_{p>2}\) & \(_{p}(1+<p 2)\) \\  Broadcast & \((})+O(Ts)\) & \((}+Ts)\) & \((}+Ts)\) & \((}+Ts)\) \\  Message-Passing & \((}+Ts)\) & \((}+Ts)\) & \((}+Ts)\) & - \\   

Table 1: Summary of our constant probability communication upper bounds.

    &  \\  & (T)\)} \\  Algorithms & DEWA-S-P & DEWA-M-P & DEWA-L-P & DEWA-L-P \\  Agg Func & sum & max & \(_{p>2}\) & \(_{p}(1+<p 2)\) \\  Broadcast & \((}+Ts)\) & \((}+Ts)\) & \((}+Ts)\) & \((}+Ts)\) \\  Message-Passing & \((}+Ts)\) & - & - & - \\   

Table 2: Summary of our high probability communication upper bounds.

With \(s\) servers in the message-passing model and with sum aggregation, a straightforward implementation of EWA achieves an optimal regret \(O(})\) with a trivial communication cost of \((nTs)\).

A distributed Exp3 algorithm achieves \(O(})\) regret with a total communication cost of \((Ts)\). Here \((f)\) denotes \(f^{O(1)}(nTs)\). A natural question is whether these bounds are tight and what the optimal regret versus communication tradeoff is.

We summarize our results in Table 1, Table 2 and Table 3. We assume \(R[(()^{}),(()^{})]\) for DEWA-L as well as DEWA-L-P when \(1+<p 2\), and \(R[(}),(})]\) for the others. All upper bounds hold unconditionally against strong adversarial cost streams. Our upper bounds hold unconditionally against strong adaptive adversarial cost streams, where an adversary chooses its (distributed) cost vector after seeing the distribution that the algorithm uses to sample experts on that day. Also, with a memory bound on the local servers, our lower bounds hold against weaker oblivious adversarial cost streams, where the loss vectors of all days are fixed in advance. A memory-bound on individual devices, excluding the coordinator, is natural, as one should view the coordinator as a more powerful machine than the individual servers. Empirically, we also provide comprehensive evaluations over real world (HPO-B Arango et al. (2021)) as well as synthetic data traces to demonstrate the effectiveness of our methods.

## 2 Related Work

**Online learning with expert advice.** The Multiplicative Weights Update (MWU) method's first appearance dates back to the early 1950s in the context of game theory Brown and Von Neumann (1950); Brown (1951); Robinson (1951). The exact form of MWU is carried out by adding randomness, which efficiently solves two-player zero-sum games (Grigoriadis and Khachiyan, 1995). Ordentlich and Cover (1998) further proves the optimality of such algorithms under various scenarios. The algorithm has later been adopted in a wide range of applications (Cesa-Bianchi and Lugosi, 2006; Freund and Schapire, 1997; Christiano et al., 2011; Garber and Hazan, 2016; Klivans and Meka, 2017; Hopkins et al., 2020; Ahmadian et al., 2022), including the experts problem. See the comprehensive survey on MWU by Arora et al. (2012).

**Multi-armed bandits.** Similar to the experts problem, Multi-armed bandits (MAB) is another fundamental formulation in sequential optimization since its appearance in Thompson 1933; Robbins 1952. Unlike the experts problem, where each expert's cost is revealed each day, MAB limits players to observing only the cost of one expert (arm) each day. Both stochastic and adversarial MAB problems have been studied extensively (Audibert et al., 2009; Garivier and Cappe, 2011; Korda et al., 2013; Degenne and Perchet, 2016; Agrawal and Goyal, 2017; Kaufmann, 2018; Lattimore and Szepesvari, 2020; Auer et al., 2002; Auer, 2002). As we mainly consider adversarial cost streams, the Exponential-weight algorithm for Exploration and Exploitation (Exp3) and its Upper Confidence Bound (UCB) variant are most relevant due to their effectiveness in achieving near-optimal regret in the presence of adversaries (Auer et al., 2002).

**Distributed learning with expert advice.**Kanade et al. 2012 also study the expert problem under a coordinator-server model. However, the results are incomparable as Kanade et al. 2012 only considers the special case where the cost is allocated to one server rather than an arbitrary number of servers,

    & Lower bounds \\  & w/ a constant probability \\  Agg Func & \(_{p}(1 p)\) \\  Broadcast & \((}+Ts)\) \\  Message-Passing & \\   

Table 3: Summary of our communication lower bounds. We assume \(R[O(}),O(})]\). All lower bounds hold against oblivious adversarial cost streams with a memory bound \(M=O(}+1)\) on the servers.

which makes their setup a special case under our more general scheme. Also, our lower-bound proof is against oblivious adversaries rather than adaptive adversaries, as in Kanade et al. (2012), which is more challenging to prove. Detailed comparisons with Kanade et al. (2012) are described in Section C.

Hillel et al. 2013; Szorenyi et al. 2013 give a distributed MAB setting where arms on each server share the same cost distribution, and the goal is to find the best arm cooperatively. Shahrampour et al. 2017; Landgren et al. 2016; Bistritz & Leshem 2018, on the other hand, assume the costs on each server are i.i.d. across days while being different for different servers. Cesa-Bianchi et al. 2016 considers a setup where servers are nodes on a connected graph and can only talk to neighboring nodes while restricting the cost for each arm on the servers to be the same within one day. Korda et al. 2016 studies the multi-agent linear bandit problem in a peer-to-peer network where agents share the same group of arms with i.i.d. costs across days. Some works also consider the setup where servers need to compete against each other, which is outside of our scope (Anandkumar et al., 2011; Besson & Kaufmann, 2018; Bubeck et al., 2020; Wang et al., 2020). Unlike most of these setups, we make no assumptions about the costs across days and servers.

**Distributed functional monitoring.** The coordinator-server communication model is also commonly seen in the distributed functional monitoring literature (Cormode et al., 2011; Woodruff & Zhang, 2012; Arackaparambil et al., 2009; Cormode et al., 2012; Chan et al., 2012), where the goal is to approximate function values, e.g., frequency moments, across streams with minimal communication. We note that the goal of the distributed experts problem is different in that the focus is on expert selection rather than value estimation, and the algorithms in the distributed functional monitoring literature, to the best of our knowledge, are not directly useful here.

## 3 Preliminaries and Notation

We use \(T\) to denote the total number of days, \(n\) the number of experts, and \(s\) the number of servers. \(l^{t}_{i,j}\) represents the cost observed at step \(t\) for expert \(i\) on the \(j\)-th server. \(\) denotes an estimate to \(l\) and \([n]\) denotes \(\{1,2,,n\}\). A word of memory is represented as \(O()\) bits and we use \(()\) to suppress \(^{O(1)}{(nTs)}\) factors. We refer to the Exponential Weight Algorithm (EWA) and Multiplicative Weights Update (MWU) method interchangeably.

### Distributed Experts Problem

In the single server expert problem, each expert \(e_{i}\), \(i[n]\) has its cost \(l^{t}_{i}\) on day \(t\). Based on the history, an algorithm \(\) needs to select one expert \(e_{(t)}\) for each day before the outcome is revealed on that day. The goal for the single server expert problem is to minimize the average regret defined as: \(R()=(_{t=1}^{T}l^{t}_{(t)}-_{i^ {*}}_{t=1}^{T}l^{t}_{i^{*}}).\)

In the distributed setting, we have \(s\) servers and one coordinator where the cost \(l^{t}_{i}\) now depends on costs \(l^{t}_{i,j}\) observed locally across all the servers. The coordinator selects the expert for the next day based on any algorithm \(\) of its choice. For each \(j[s]\), the \(j\)-th server can receive or compute its cost \(l^{t}_{i,j},i[n]\) for the \(i\)-th expert on day \(t\). The actual cost for the \(i\)-th expert on day \(t\) is defined as \(l^{t}_{i}=f(l^{t}_{i,1},l^{t}_{i,2},,l^{t}_{i,s})\), where \(f()\) is an aggregation function. We assume the costs \(l^{t}_{i,j}\) are non-negative. We consider three natural choices of \(f()\): 1. the summation function \(l^{t}_{i}=_{j=1}^{s}l^{t}_{i,j}\) and an integer power of the sum function \(l^{t}_{i}=(_{j=1}^{s}l^{t}_{i,j})^{q}\) 2. the maximum/minimum function \(l^{t}_{i}=_{j[s]}l^{t}_{i,j}\) 3. the \(_{p>1}\) norm function, \(l^{t}_{i}=(_{j=1}^{s}(l^{t}_{i,j})^{p})^{ },p>1\). In the distributed setting, regret is defined as in the single server setup with \(l^{t}_{i}=f(l^{t}_{i,1},l^{t}_{i,2},,l^{t}_{i,s})\). Without loss of generality, we normalize \(l^{t}_{i},l^{t}_{i,j} 0\). In practice, if \(l^{t}_{i}[0,]\), the regret will increase by a factor of \(\) accordingly, which only affects the scale of the regret and preserves optimality. Note that the cost vector for all the experts is observed by the corresponding local server. Furthermore, we explore the distributed experts problem in two different communication models:

**Message-passing model.** For the message-passing model, the coordinator can initiate a two-way private channel with a specific server to exchange messages. Messages can only be seen by the coordinator and the selected server. The coordinator then decides which server to speak to next and repeats based on the protocol.

**Broadcast model.** In the broadcast model, the coordinator communicates with all servers using a broadcast channel. Again, the communication channel can only be initiated by the coordinator.

We further assume local servers have a memory bound of \(M\) in what they can store from previous days, which is a more practical scenario as discussed in Srinivas et al. (2022); Peng and Zhang (2022). We leave the definition and description of strong adaptive adversaries and the EWA algorithm in Definition A.1 and Appendix A.2 accordingly.

## 4 Proposed Algorithms

### Overview

In the message-passing model, we let \(b_{e}[n]\) be a hyper-parameter of our choice. We first propose a baseline algorithm DEWA-S that can achieve \((}})\) regret with constant probability using \(O(T(b_{e}+s))\) total communication when the aggregation function is the summation function or an integer power of sum function. The intuition for the baseline algorithm is to get an unbiased estimation of the experts' underlying cost by sending a signal to the coordinator with a probability that is proportional to the local cost, which is simple yet effective. We further introduce the full algorithm DEWA-S-P that achieves \((}})\) regret with probability \(1-(T)}\) using \((T(b_{e}+s))\) total communication. Both DEWA-S and DEWA-S-P work in the broadcast model with the same guarantees since the message-passing model is only more costly.

In the broadcast model, we propose DEWA-M-P that achieves \((}})\) regret with probability \(1-(T)}\) and using only \((T(b_{e}+s))\) overall communication when the aggregation function is the maximum function. Besides the summation aggregation function, we leverage a random-walk-based communication protocol to find out the aggregated cost with a minimal communication cost. Since all of our protocols use (and require) at least \(Ts\) communication, the coordinator can figure out the exact cost for the selected expert on each day by querying each of the \(s\) servers for that expert's cost on that day. Lastly, we propose DEWA-L-P that achieves \(O((})^{}+})\) regret with probability \(1-(T)}\) and using only \((T(b_{e}+s))\) overall communication when the aggregation function is the \(_{p}\)-norm function for any fixed constant \(0< 1\) such that \(1+<p\). The algorithm employs the idea of embedding \(_{p}\) into \(_{}\), thus efficiently estimating the aggregated cost using the previously introduced DEWA-M-P. For all our bounds, \(b_{e}[n]\) is a hyperparameter that trades off the communication with the optimal regret we can get. For instance, setting \(b_{e}=o(1)\) can achieve a regret of \(R=(})\) and setting \(b_{e}=o(n)\) can achieve a regret of \(R=(})\). Thus, setting \(b_{e}=o(})\) can achieve the optimal communication bound we provide in Table 1 and Table 2.

### Dewa-S

We describe DEWA-S in Algorithm 1. The intuition is to obtain an unbiased estimate \(^{t}\) for \(l^{t}\) using limited communication and then run EWA based on our estimate. More precisely, we use the following estimator to estimate \(l^{t}\) on day \(t\): \(^{t}_{i}=}(_{j=1}^{s}^{t}_{i,j}^{t}_{i,j})\), where \(^{t}_{i,j}\) are i.i.d. Bernoulli random variables following \(^{t}_{i,j}(}{n})\), and the \(^{t}_{i,j}\) are sampled from Bernoulli(\(l^{t}_{i,j}\)). As \(l^{t}_{i},l^{t}_{i,j} 0\), Bernoulli(\(l^{t}_{i,j}\)) is a valid distribution. We can easily verify that this is an unbiased estimator: \([^{t}_{i}]=[}(_{j=1}^{s}^{t }_{i,j}^{t}_{i,j})]=}(_{j=1}^{s}[^{t} _{i,j}][^{t}_{i,j}])=}_{j=1}^{s}_{j=1}^{s }l^{t}_{i,j}}{n}=l^{t}_{i}\). The same sampling technique can be used to obtain an unbiased estimator of \(l^{t}_{i}\) when the aggregation function is an integer power of the sum over local costs, where each monomial in the expansion of the aggregation function is unbiasedly estimated by taking the product of sampled local costs. On each day, we only incur communication cost \(O(s+_{i=1}^{n}}{n}_{j=1}^{t}l^{t}_{i,j}) O(b_{e}+s)\). Thus, the overall communication cost is \(O(T(b_{e}+s))\).

```
Input: learning rate \(\), sampling budget \(b_{e}\);  Initialize \(^{0}_{i}=0, i[n]\); for\(t=1\)to\(T\)do  Coordinator chooses expert \(i\) with probability \(p(i)^{t-1}_{i})}\); for\(j=1\)to\(s\)do  Coordinator initiates private channel with server \(j\); for\(i=1\)to\(n\)do  Server \(j\) observes cost \(l^{t}_{i,j}\) and samples \(^{t}_{i,j}(}{b})\), \(^{t}_{i,j}(l^{t}_{i,j})\);  Server \(j\) sends tuples \((i,j)\) to the coordinator if \(^{t}_{i,j}=1,^{t}_{i,j}=1\) and clears its memory;  Coordinator calculates \(^{t}_{i}=}{b_{e}}(_{j=1}^{s}^{t}_{i,j}^{t}_{ i,j})\);  Update \(_{i}\) by \(^{t}_{i}=^{t-1}_{i}+^{t}_{i}, i[n]\);
```

**Algorithm 1** DEWA-S-P

### Dewa-S-P

As we are using unbiased estimators instead of actual costs, we only obtain the desired regret with constant probability. In order to achieve near-optimal regret with high probability, we propose DEWA-S-P in Algorithm 2. The idea is to run multiple baseline algorithms in parallel to boost the success probability, where we regard each baseline algorithm as a meta-expert. As each meta-expert has constant success probability, the probability that they all fail is exponentially small in the number of meta-experts. Thus, by running EWA on the meta-experts, we can follow the advice of the best meta-expert and achieve near-optimal regret with high probability.

```
Input: learning rate \(_{}\), sampling budget \(b_{e}\), failure rate \(1/(T)\);  Let \(K=(T))}\), initialize \(K\) baseline algorithms \(_{k}\) and let \(L^{0}_{k}=0,k[K]\); for\(t=1\)to\(T\)do  Coordinator chooses expert according to \(_{k}(t)\) with probability \(p(k)}L^{t-1}_{k})}\);  Coordinator updates memory states for all \(_{k}\) according to Algorithm 1;  Coordinator receives cost \(l^{t}_{_{k}(t)}=_{j=1}^{s}l^{t}_{_{k}(t),j}\);  Update all \(L_{k}\) by \(L^{t}_{k}=L^{t-1}_{k}+l^{t}_{_{k}(t)}\);
```

**Algorithm 2** DEWA-S-P

More precisely, to obtain \(1-(T)}\) success probability, we initiate \((T))}\) meta-experts \(_{k},k(T))}\) at the start of the algorithm. Each meta-expert runs its own DEWA-S independently across \(T\) days. The cost of the \(k\)-th meta-expert on day \(t\) is defined to be the cost the expert \(_{k}\) selects on the same day, which is denoted as \(l^{t}_{_{k}(t)}\). With the definition of the cost for the meta-experts, we can then run EWA on the meta-experts.

The meta-level EWA needs to know the actual cost \(l^{t}_{_{k}(t)}\) from the \(s\) servers of each meta-expert in order to recover the best meta-expert with \(1-(T)}\) success probability. Therefore for DEWA-S-P, on each day, we incur a communication cost of \((s+(b_{e}+s)(T))})=(b_{e}+s)\), and the overall communication is \((T(b_{e}+s))\).

### Dewa-M-P

We propose DEWA-M described in Algorithm 3 that achieves a near-optimal regret versus communication tradeoff up to log factors for the maximum aggregation function in the broadcast model.

The intuition of DEWA-M is that for each expert, if we walk through the servers in a random order and only update \(^{t}_{i}\) if we encounter \(l^{t}_{i,j}>^{t}_{i}\), then with high probability, we only need a small number of updates per expert. This cannot be achieved in the message-passing model due to the fact that broadcasting \(^{t}_{i}\) requires \((s)\) communication per expert. In contrast, no communication is required for broadcasting \(^{t}_{i}\) in the broadcast model. In fact, with probability \(1-\), each expert will update at most \(O((s/))\) times. By setting \(=(T)}\) and applying a union bound over our sampling budget \(b_{e}\) and number \(T\) of days, we have the desired low communication with probability at least \(1-(T)}\). More precisely, we have the following theorem (see detailed proof in Section B.1):

**Theorem 4.1**.: _For a sampling budget \(b_{e}[n]\), with probability \(1-\), the communication cost for DEWA-M is \((T(b_{e}+s))\)._

Even though we have a high probability guarantee with minimal communication, we still only have a constant probability guarantee for achieving optimal regret \(O(T}})\). We can boost the success probability using the same trick as in Algorithm 2 by initiating \(((T))\) copies of DEWA-M as meta-experts and running EWA on top of them. We refer to the high-probability version as DEWA-M-P. We thus have the following theorem (see detailed proof in Section B.2):

**Theorem 4.2**.: _For a sampling budget \(b_{e}[n]\), with probability \(1-\), the communication cost for DEWA-M-P is \((T(b_{e}+s))\)._

### Dewa-L-P

In this section, we present DEWA-L (Algorithm 4) for the \(_{p>1}\) norm aggregation function in the broadcast model. The key idea of DEWA-L is to embed \(_{p}\) into \(_{}\) using the min-stable property of exponential distribution. More specifically, if \(E_{i}\) is a standard exponential random variable, then \(_{j}_{i,j}^{t})^{p}}{E_{j}}_{i}^{t})^{ p}}{E}\) where \(E\) is also a standard exponential random variable. Therefore, we can employ DEWA-M to efficiently compute \(_{i}^{t})^{p}}{E}\), and obtain an unbiased estimator of \(_{i}^{t}\) by normalizing.

```
Input: learning rate \(\), sampling budget \(b_{e}\);  Coordinator initializes \(_{i}^{0}=0, i[n]\); for\(t=1\)to\(T\)do  Coordinator chooses expert \(i\) with probability \(p(i)(-_{i}^{t-1})\);  Coordinator randomly chooses \(b_{e}\) experts with corresponding IDs \(_{e}=\{t(1),t(2),,t(b_{e})\}\);  Coordinator initializes \(_{i}^{t}=0, i[n]\);  Coordinator permutes \([s]\) randomly and denotes the resulting sequence as \(S_{t}\) for\(j\)in\(S_{t}\)do  Coordinator initiates channel with server \(j\);  Server \(j\) samples \(E_{j}(1)\); for\(i=1\)to\(n\)do  Server \(j\) observes cost \(_{i,j}^{t}\) and computes \(c_{i,j}^{t}=_{i,j}^{t})^{p}}{E_{j}}\);  Server \(j\) sends \(c_{i,j}^{t}\) to the coordinator if \(_{i,j}^{t}>c_{i}^{t}\) and \(i_{e}\);  Server \(j\) cleans memory buffer;  Coordinator updates \(c_{i}^{t}=_{j}c_{i,j}^{t}\) with received \(c_{i,j}^{t}\);  Coordinator computes \(_{i}^{t}=)^{b_{e}}}_{i}^{t})^{1/p}}{[(E)-1/p]}\), where \(E(1)\);  Update \(_{i}\) by \(_{i}^{t}=_{i}^{t-1}+_{i}^{t}, i[n]\);
```

**Algorithm 4** DEWA-LIt is not hard to see that the communication cost of DEWA-L stays the same as DEWA-M. In terms of regret, if we fix any constant \(0< 1\) such that \(1+<p\), DEWA-L achieves a vanishing regret \(R=O((})^{})\) with constant probability. Note that, for all \(_{p}\)-norm functions with \(p>2\), by choosing \(=1\), we obtain a near-optimal regret versus communication tradeoff up to a \(\) factor \(R=O(}})\). Again, to get the high probability regret guarantee of DEWA-L, we propose DEWA-L-P that initiates \((T))}\) copies of DEWA-L as meta-experts and runs EWA on top of them. More precisely, we have the following theorem with the same proof as Theorem 4.2:

**Theorem 4.3**.: _For a sampling budget \(b_{e}[n]\), with probability \(1-\), the communication cost for DEWA-L-P is \((T(b_{e}+s))\)._

## 5 Formal Guarantees

We present formal regret analyses of DEWA-S, DEWA-S-P, DEWA-M-P and DEWA-L-P. We show that DEWA-S can achieve regret \(R=O(}})\) with probability at least \(9/10\), DEWA-S-P and DEWA-M-P can achieve regret \(R=O(}{Tb_{e}}})\) with probability at least \(1-(T)}\), and lastly DEWA-L-P can achieve regret \(R=O((})^{}+})\) with probability at least \(1-(T)}\) for any fixed constant \(0< 1\) such that \(1+<p\).

We then give a communication lower bound, which holds even in the broadcast model, for both summation and maximum aggregation functions with a memory bound on the individual servers. It holds for oblivious adversarial cost streams, and thus also for strong adversarial cost streams and the message-passing model. We use the communication lower bound for the \(\)-DIFFDIST problem Srinivas et al. (2022) but adapt it to our setting. By reducing the \(\)-DIFFDIST problem to the distributed experts problem, we prove that any protocol for achieving \(R\) regret with constant probability requires total communication at least \((})\). It will follow that DEWA-S, DEWA-M and DEWA-L \((p>2)\) are near-optimal in their communication for all regret values \(R[O(}),O(})]\).

### Upper Bound

We state our regret upper bounds for DEWA-S in Theorem 5.1, DEWA-S-P in Theorem 5.2, DEWA-M-P in Theorem 5.3 and DEWA-L-P in Theorem 5.4. The detailed corresponding proofs can be found in Section B.

**Theorem 5.1**.: _For \(b_{e}[n]\), DEWA-S achieves regret \(R=O(}})\) with probability at least \(\) for the distributed experts problem in the message passing model with the summation aggregation function and for strong adaptive adversarial cost streams._

**Theorem 5.2**.: _DEWA-S-P achieves regret \(R=O(}{Tb_{e}}})\) with probability at least \(1-(T)}\) for the distributed experts problem in the message passing model with the summation aggregation function and for strong adaptive adversarial cost streams._

Notice that the total communication cost for DEWA-S-P is \((T(b_{e}+s))\). Thus DEWA-S-P can achieve the same regret as EWA with a high probability guarantee when \(b_{e}=n\), but requires only \((T(n+s))\) communication instead of \((nTs)\) communication. DEWA-S-P further generalizes to the case when \(b_{e}<n\).

**Theorem 5.3**.: _DEWA-M-P achieves regret \(R=O(}{Tb_{e}}})\) with probability at least \(1-(T)}\) for the distributed experts problem in the broadcast model with maximum aggregation function and for strong adaptive adversarial cost streams._

**Theorem 5.4**.: _Fix any constant \(0< 1\) such that \(1+<p\), DEWA-L-P achieves regret \(R=O((})^{}+})\) with probability at least \(1-(T)}\) for the distributed experts problem in the broadcast model with \(_{p}\) norm aggregation function and for strong adaptive adversarial cost streams._

[MISSING_PAGE_FAIL:9]