# Extending Video Masked Autoencoders to 128 frames

Nitesh B. Gundavarapu\({}^{1*}\) Luke Friedman\({}^{1*}\) Raghav Goyal\({}^{2*}\) Chaitra Hegde\({}^{1*}\)

**Eirikur Agustsson\({}^{1}\) Sagar Waghmare\({}^{1}\) Mikhail Sirotenko\({}^{1}\) Ming-Hsuan Yang\({}^{1}\) Tobias Weyand\({}^{1}\) Boqing Gong\({}^{1}\) Leonid Sigal\({}^{2}\)**

\({}^{1}\)Google Research \({}^{2}\)University of British Columbia

{ngundavarapu,lbfried,cvhegde,eirikur}@google.com

{rgoyal14,lsigal}@cs.ubc.ca

Equal primary contributionWork done as Student Researcher at Google ResearchWork done as Visiting Research Faculty at Google Research

###### Abstract

Video understanding has witnessed significant progress with recent video foundation models demonstrating strong performance owing to self-supervised pre-training objectives; Masked Autoencoders (MAE) being the design of choice. Nevertheless, the majority of prior works that leverage MAE pre-training have focused on relatively short video representations (16 / 32 frames in length) largely due to hardware memory and compute limitations that scale poorly with video length due to the dense memory-intensive self-attention decoding. One natural strategy to address these challenges is to subsample tokens to reconstruct during decoding (or _decoder masking_). In this work, we propose an effective strategy for prioritizing tokens which allows training on longer video sequences (128 frames) and gets better performance than, more typical, random and uniform masking strategies. The core of our approach is an adaptive decoder masking strategy that prioritizes the most important tokens and uses quantized tokens as reconstruction objectives. Our adaptive strategy leverages a powerful MAGVIT-based tokenizer that jointly learns the tokens and their priority. We validate our design choices through exhaustive ablations and observe improved performance of the resulting long-video (128 frames) encoders over short-video (32 frames) counterparts. With our long-video masked autoencoder (LVMAE) strategy, we surpass state-of-the-art on Diving48 by \(3.9\) points and EPIC-Kitchens-100 verb classification by \(2.5\) points while relying on a simple core architecture and video-only pre-training (unlike some of the prior works that require millions of labeled video-text pairs or specialized encoders).

## 1 Introduction

Long video understanding has witnessed growing interest with various aspects of the problem having been explored by recent works . This includes incorporating (i) efficient attention mechanisms , (ii) designing memory modules  to reason over context from the past, and (iii) approaching the problem from video-language perspective by proposing benchmarks  and architectural choices that effectively leverage the interplay between the modalities .

The majority of recent models rely on foundational models pre-trained in a self-supervised manner based on videos and/or, for vision-language counterparts, video-text pairs; these foundational models, once pre-trained, can then be fine-tuned for a specific task at hand (_e.g._, action classification). MaskedAutoencoders (MAE) have emerged as a simple and effective strategy to masked video modeling  in this context. The standard MAE setup involves encoding a small fraction of visible / unmasked tokens (_e.g._, \(10\%\)) of an input video, and decoding the remaining masked tokens (_e.g._, \(90\%\)) using dense space-time attention decoder blocks. However, for longer video sequences, reconstructing all the masked tokens at the decoding stage quickly leads to out-of-memory (OOM) due to quadratic complexity of Transformers (feasible only for \(<64\) frames for moderate video resolution and current consumer hardware).

As a result, most existing MAE-based video understanding approaches focus on learning representations that encode only a few frames (16  / 32 ) at a time. This limits their ability to understand actions and events spanning longer time horizons. Existing work has worked around the short context limitation by segmenting longer input video into short chunks and feeding them into the video model sequentially. Inference results from these chunks are then combined using late fusion. For example, "multi-crop" evaluations simply average-pool predictions over all chunks. However, this is clearly limiting. Long-context video models have the potential to understand longer actions (like complex diving routines), whole activities consisting of a sequence of actions and eventually entire story arcs.

Attempts to reduce computational cost and memory consumption, which bottleneck video MAE scalability, have recently started to emerge. These efforts can be characterized by either (i) reconstructing a subset of tokens at the decoding stage or _decoder masking_ (VideoMAEv2 ), or by (ii) sub-sampling and focusing on a subset of "informative" tokens instead of using the entire set of spatio-temporal tokens (_e.g._, focusing on measures of objectiveness  or motion  as proxies). The resulting gains have been shown to reduce training time and hardware requirement , and can be reinvested to scale model size . However, scaling the input video along the temporal dimension has received little-to-no attention besides scaling input frames to a higher spatial resolution .

In this work, we build on  which performs masking at both the encoding and decoding stages. In particular, we focus on decoder masking considering its impact on memory and scalability, and propose a **content-dependent adaptive masking strategy** for videos in the MAE setup (see Figure 1). The core idea is a token-importance (or _saliency_) scheme, learned jointly with adaptive quantization, that establishes a rank-order of video tokens, which is then used to select higher-ranking tokens for decoding. The jointly learned quantization is shown to also be effective, compared to raw pixels, in defining the targets for reconstruction - further improving performance.

Our primary contributions are as follows:

1. We design a content-dependent adaptive masking strategy and demonstrate that given a low-token budget, it outperforms prior uniform and motion-based masking strategies.
2. The memory savings obtained from the low-token budget enables pre-training over long-videos (128 frames), allowing us to ask the question of exactly how much benefit long videos bring in the context of MAE pretraining. We observe that the long-video MAE (128 frames) model consistently outperforms short-video MAE (32 frames), as measured using downstream fine-tuning performance; including when a short-video MAE is fine-tuned with longer context (_e.g._, 32 frame pre-trained MAE fine-tuned on 128 frames).
3. Leveraging these findings, we demonstrate state-of-the-art performance with our long-video MAE (LVMAE) approach on conventional action classification benchmarks that are known to require longer-range motion understanding (EPIC-Kitchens-100  and Diving48 )

Figure 1: **Left: Proposed Long Video MAE Decoder Masking. We leverage a jointly trained adaptive tokenizer and importance module to define a decoder mask and token targets for a video MAE pre-training strategy. The resulting sparsification in tokens (only 15%) allows pre-training with long videos (128-frames) and results in substantial performance gains. Right: Decoder masking and memory in long-video (128 frames) pre-training. We report memory and FLOPs per-device for a batch size of 1 using different decoder mask ratios and ViT-B architecture.**

using standard ViT encoders and just one temporal crop - without relying on any language supervision or labels during pre-training.

## 2 Related work

**Masked Autoencoders (MAE).** Masked autoencoders have initially been proposed as an effective mechanism for learning self-supervised representations in masked image modeling (MIM) . Following their success in the image domain, several video MAE-based representation learning extensions have been proposed. Specifically, ST-MAE  extends MIM to spatio-temporal patches in clips - treating space-time blocks as tokens, while VideoMAE  explores frame-level and tube-level masking strategies; both leverage an extremely high masking ratio (\( 90\)%) compared to image counterparts (\( 60\)%). Models that leverage joint image-video pre-training have also been introduced (_e.g._, BEVT , OmniMAE ). However, scalability to high-resolution and long videos remains a substantial challenge, owing to the quadratic complexity of attention mechanisms.

**MAE Reconstruction Objectives.** While most video MAE models are trained to reconstruct the pixels of masked video patches [13; 15], there are a few notable exceptions. Mainly, BEVT  proposed to reconstruct discrete token targets obtained using a VQ-VAE  patch tokenizer, while MVD  focuses on masked feature modeling and distillation with high-level features as targets, obtained using a _teacher_ model. Compared to BEVT, we see even greater gains when reconstructing discrete token targets by using a more powerful tokenizer (MAGVIT ) and an improved, jointly trained, tokenization and masking scheme.

**Efficiency by Token Reduction.** Sub-sampling salient tokens, based on a learned or an off-the-shelf strategy during training, is shown to give faster convergence and require less resources [17; 16; 26]. ObjectViViT  extracts object bounding boxes using an off-the-shelf object detector, and by placing emphasis on tokens belonging to objects was able to achieve lower token utilization and obtain competitive performance on the Epic Kitchens  and SSv2 benchmarks . Token Learner  observes and corrects for token redundancy in higher layers of ViT  using learned modules.

Similar to the above approaches, we use token importance to inform our proposed masking. Differently, we don't sub-sample tokens, but selectively decode them during the training phase in a more _generalizable_ dual masking setup. Concurrent EVEREST  is the most comparable to our work. It attempts to learn what tokens in the spatio-temporal volume are more informative using a heuristic based on distance in feature space (as a proxy for motion). In the short 16-frame regime they find that sub-sampling such tokens in MAE setup leads to lower memory and resource consumption, while maintaining performance. Different from , our token saliency scheme is learnt separately and independently in a MAGVIT tokenizer  setup, while EVEREST learns token-saliency during MAE pre-training itself. The latter approach can bias the method towards selecting _easy_ tokens for the MAE reconstruction objective, undermining the learning, which is not an issue for our method. By learning token importance jointly with the tokenizer we incur a negligible extra cost, on top of the tokenizer computation, and as noted earlier get significant additional performance improvements by using the tokens as targets as opposed to RGB in .

**Motion in Video MAE.** Building on the above, attempts to identify and attribute relevance to video tokens involved in motion compared to static background has seen growing interest [29; 30], since motion is fundamental to understanding videos. In particular, MGMAE  and MGM  show that an informed encoder masking based on motion-cues at patch-level makes the MAE reconstruction task account for motion, and found faster convergence and improved performance on motion-centric video datasets such as SSv2 . Specifically, MGM  uses H.264 codec  to extract patch-level motion vectors, and mask out tokens involved in motion during encoding, and MGMAE  uses online optical flow extractor (RAFT ), to warp and guide encoder masking using flow. Unlike these approaches, we do not model motion explicitly, and any apparent motion information in our mask is a byproduct of our data-driven token importance learning scheme.

**Long Videos and Masking.** As discussed above, masking tokens during training reduces memory requirement and this fact is exploited in several video understanding works for scaling model size [33; 34]. However, expanding these techniques for long-videos has received limited attention. Recently, LongViViT  leveraged random masking during contrastive pre-training on video-language tasks, and observed best performance-memory trade-off. Specifically, in order to convert short-video encoder to long-videos, they fine-tune the last four layers of ViViT  on long videos (\(128\) frames) along with \(75\%\) random masking. Similar to LongViViT , we utilize masking to pre-train over long videos. However, we focus on masked autoencoders instead of contrastive learning for long-videos and pre-train _all the layers_ of our model, instead of partial freezing, in order to capture dense space-time correspondence in long-videos. Further, the overall training procedure of LongViViT  is considerably more complex requiring first pre-training with short videos and then further pre-training with long ones. We on the other hand, not only have a simpler scheme that can directly pre-train a MAE model with long videos (128 frames), but show that this is critical for improved performance.

**Long Videos and Memory**. Memory-based approaches [5; 6] attempt to form a compressed representation of the past activations or _memory_, which is then incorporated into current window or time step, effectively prolonging the context length over which reasoning and predictions are formed. Orthogonal to these works, we expand the local context of clips during pre-training from 16 to 128 frames; this can be combined with a memory module to increase the global context window.

## 3 Approach

We first give the background on MAE and the dual masking, initially introduced in . We then focus on describing our proposed adaptive importance masking strategy and discuss how it can be leveraged to train with up to a 128 frame context window.

### Background: MAE and Dual Masking

An input video \(^{3 F H W}\) is first partitioned into non-overlapping spatio-temporal patches, and tokenized using a patch embedding layer (typically a 3DConv) to give tokens \(=\{T_{i}\}_{i=1}^{N}\), where \(T_{i}^{d}\) is an \(i^{th}\) token with added positional encoding, \(N\) is total number of tokens, \(d\) is the hidden dimension, and \(F\), \(H\), \(W\) are number of frames, height and width respectively of the input video. Using an encoder mask \(_{e}\{0,1\}^{N}\), the unmasked / visible tokens are selected \(^{u}=\{T_{i}\}_{i(1-_{e})}\), with \(N^{e}\) the total number of unmasked encoder tokens. A vanilla ViT encoder  is applied to the unmasked tokens to give encoded visible tokens \(=_{enc}(^{u})\). In the usual case, we obtain input to the decoder \(^{c}\) by combining the encoded tokens \(\) with learnable masked tokens \(=\{M_{i}\}_{i_{e}}\), where \(M_{i}^{d}\) is [MASK] token embedding with positional embedding. The overall objective is to reconstruct the masked-out tokens using the unmasked encoded tokens. However, with Dual Masking , a decoder mask \(_{d}\{0,1\}^{N}\) is used to select tokens to reconstruct \(^{c}=\{M_{i}\}_{i(1-_{d})}\), with \(N^{d}\) the total number of unmasked decoder tokens where \(N^{e}<<N^{d}\) and \(N^{e}+N^{d}<<N\). Then, the combined tokens are reconstructed using a vanilla ViT decoder \(}=_{dec}(^{c})\). Finally, the Mean Squared Error (MSE) loss is computed between the original and reconstructed pixels,

\[=_{e}_{d}|}_{i_{e}_{d}}|}_{i}-_{i}|^{2}. \]

Alternatively, if vector quantized tokenization is employed, the loss becomes,

\[=_{e}_{d}|}_{i_{e}_{d}}|}_{i}-VQ()_{i}|^{2}, \]

where \(VQ()\) is a vector quantization mapping typically trained separately using VQ-VAE or variants. In the proposed approach \(VQ()\) takes the form of adaptive FSQ-MagViT described in Section 3.3. Further, we want to highlight that encoder and decoder masking strategies need not be the same.

### Masking Strategies

The above dual masking formulation requires both encoder and decoder masking. A number of strategies have been explored, but largely fall into two categories: _content agnostic_ and _informative_.

_Content agnostic_ masking strategies leverage either a fixed or randomized scheme which is agnostic of the video content. Fixed strategies comprise of (i) grid-based _uniform_ masking , which keeps every \(k\)-th row/column along spatial dimension; and (ii) _frame_ masking  where all tokens from every \(k\)-th frame are kept in an attempt to reduce temporal redundancy. The choice between the two depends on the assumptions regarding relative importance of spatial vs. temporal information.

(iii) Randomized strategies [13; 15] leverage sampling instead, which results in a form of data augmentation, since the same clip encountered in different epochs would result in a different mask. Under low-token budget, the above mentioned approaches are shown to perform competitively, and at the same time, gain memory efficiency .

Masking approaches mentioned above are effective, but at their core are sub-optimal as they fail to take into account the content of the video itself. Our approach falls into an _informative_ masking class of strategies, which instead of sub-sampling overall set of tokens, leverage an importance function to produce a relative rank-order (or priority) on input tokens and use this ordering when selecting which tokens to encode/decode. We denote the token-importance (or saliency) as \(^{N}\), where \(N\) is the total number of tokens in an input video. In particular, \(\) is a probability distribution over tokens, which gives us a rank-order of tokens. Letting \(\) be a probability distribution allows both deterministic (\(k\) most probable) and stochastic forms of informed masking. Prior work has relied on optical flow [29; 31] as proxy for importance, which in this work we leverage as one of our baselines. We, on the other hand, propose a more general adaptive FSQ-MagViT scheme that learns importance of tokens concurrently with tokenization (see Section 3.3), leveraging a MAGVIT  tokenizer.

While the above mentioned strategies can be used to generate both encoder and decoder masks in the dual masking MAE  (with the potential mild overhead of keeping the two sets of tokens disjoint), in this paper we focus on decoder masking in particular. This design choice is motivated by the observation that decoder masking has a much more significant impact on the memory usage and scalability of the overall MAE framework (see Figure 1) (right). Our focus on informative and effective decoder masking allows us to pre-train on long-video sequences (128 frames) - see Figure 1 (left). To this end we keep the encoder masking for most experiments in this paper relatively simple.

**Encoder masking**. Motivated by results in  for most experiments in the paper (unless otherwise stated) we use randomized _tube_ masking with ratio of 90% (_i.e._, encoder only sees 10% of patches).

**Decoder masking**. We generate Adaptive decoder mask \(_{d}\) based on the intuition that reconstructing high-salient tokens is more meaningful for the MAE pre-training and for downstream fine-tuning tasks. We select top-\(k\) salient tokens from \(\) and set them as visible (indicated as 0's) in the decoder mask \(_{d}\{0,1\}^{N}\), where \(k=(1-_{d}) N\) and \(_{d}\) is decoder masking ratio. In addition to selecting top-\(k\) tokens, we also include a small fraction (\(_{r}\)) of tokens sampled _randomly_, which we observed to improve performance, similar to ObjectViViT . Therefore, \(N^{d}=(1-_{d}) N+_{r} N\). Illustration of resulted mask is shown in Appendix A.6.

### Adaptive Finite Scalar Quantized VAE for Saliency and Reconstruction Targets

Our adaptive token selection module, shown in Figure 2, combines an effective CNN-based tokenizer and a differentiable top-\(k\) selection layer.

**Tokenizer.** For the tokenizer we combine MAGVIT , which is a powerful 3D-CNN based video tokenizer and Finite Scalar Quantization (FSQ) , a simple alternative to VQ that easily scales

Figure 2: **Illustration of Adaptive FSQ-MagViT Training.** FSQ-MagViT adaptive tokenizer includes MAGVIT encoder and CNN-based token scorer with a differentiable top-\(k\) selection layer designating importance of tokens. During tokenizer training unselected tokens zeroed out and video is reconstructed using MAGVIT decoder. We then freeze this adaptive tokenizer and use it to generate target tokens for scalable pre-training of video MAE.

up to large codebooks. We refer to this combination as FSQ-MagViT and provide more details in Appendix A.5.1.

**Token Scorer** For the token selection, we learn a CNN-based Token scoring module followed by a differentiable top-\(k\) layer, resulting in the mask which we apply to the FSQ-MagViT tokens. 1 More precisely, we embed the video to a feature space with two CNN layers, strided both spatially and temporally to match the resolution of the tokens from FSQ-MagViT encoder. From this CNN feature, we obtain pairwise Euclidean distances between spatial locations in adjacent frames, measuring how much each token in the \(i\)-th frame differs from the corresponding token in the (\(i-1\))-th frame. This distance is considered as token importance as it signifies the extent of underlying change in the video as done in .

**Adaptive FSQ-MagViT** We get the soft-tokens from FSQ-MagViT and token importance from the token scoring module. We keep the tokens with the top-\(k\) largest distances (\(k=768\)) and mask out the remaining \(N-k\) tokens, replacing their values with 0. For the special case of the first frame we keep all the tokens during training. These masked tokens are then fed through FSQ and the MagViT decoder. Similar to MAGVIT , we optimize for pixel reconstruction loss, GAN loss and perceptual loss by training the FSQ-MagViT and token scorer end-to-end.

### Implementation details

First, we train our adaptive token selection module on Kinetics600  data on \(16\) frame clips. Once trained, we keep this module frozen. Next, we follow standard MAE pre-training with a couple of changes (i) we reconstruct the top-\(k\) tokens selected by the token selection module (and a small number of randomly selected tokens), and (ii) we reconstruct the latent embeddings from the tokenizer instead of RGB pixels. Note, we consciously choose to decouple the learning of token selection and MAE itself so that the gradients do not bias token selection towards selecting easily unmaskable tokens and vice-versa. Further implementation details are in Appendix A.5.1.

## 4 Experiments

We experiment with LVMAE on conventional video action classification benchmarks that have potential to benefit from long-video encoders: EPIC-Kitchens-100 (EK100)  and Diving48 (D48) . EK100  contains \( 90\)K video clips in total at \(25\) FPS with \(3.7\) secs avg. duration, and \( 15\%\) videos with at least \(5\) secs (=\(125\) frames) duration. The task is to classify nouns (=\(300\)) and verbs (=\(97\)) together. D48  contains videos with \(158\) avg. number of frames, and vary from \(24\) to \(822\) frames, and task to classify among \(48\) fine-grained dive categories . Results on additional datasets are presented in Appendix A.2.

### Decoder is the most memory intensive stage in long-video MAE

In Figure 1 (right), we show the memory and compute characteristics of ViT-B MAE architecture as we vary the decoder masking ratio for a video length of 128 frames. We fix the encoder masking ratio to 90% for this comparison. Even though the encoder has 3X more layers, decoder starts dominating for long videos due to the high number of tokens and quadratic scaling. We find that the decoder masking ratio should be close to the encoder masking ratio to respect compute and memory constraints imposed by the long-video pre-training regime.

### Adaptive decoder masking strategy outperforms on short videos

In Table 1, we compare the proposed Adaptive masking with several alternative decoder masking strategies on short videos (\(32\) frames) using both RGB pixels and FSQ-MagViT tokens as reconstruction targets on the EK100 dataset. We establish a baseline using the default MAE configuration which involves random tubes  as encoder mask (with mask ratio \(90\%\)), and decoding all the masked tokens, denoted as decoder mask ratio None. Then we experiment with the low-budget setting,\(N^{d}=0.85N\) (i.e. \(15\%\) token budget), where tokens to be decoded are selected using different saliency schemes described below.

**Random and Uniform**. The Random importance scheme selects tokens randomly from the spatio-temporal volume. For the Uniform saliency scheme, we form decoder-visible tokens by picking frames uniformly using a step size of \(7\) to select \(1/7^{th}\) of frames or roughly \(15\%\) token budget for decoding. We observe competitive performance to the baseline, consistent with VideoMAEv2 .

**Flow and EVEREST**. For the Flow importance scheme, we identify parts of the input video that contain motion. First, we obtain pixel-level Optical Flow \([-1,1]^{2 T H W}\) for the video using RAFT , containing both \(x\) and \(y\) displacement fields. We then use the pixel-level Flow to obtain a probability distribution of motion in token space to generate masks. Specifically, following the same patchify operation used for RGB video, for each spatio-temporal patch in Flow, we take the mean of absolute value for all pixels in the patch across \(x\) and \(y\) coordinates, and normalize across all patches to obtain \(^{N}\), where \(N\) is the total number of tokens. For the proposed Adaptive scheme and the Flow scheme, we choose \(_{d}=90\%\) and we add a small amount of random tokens, \(_{r}=5\%\) as we found it improves the results. For EVEREST we follow details of .

Although, Flow and EVEREST strategies provide more informed token prioritization based on explicit motion vectors or learned pixel distance, respectfully, they don't perform as well as the proposed Adaptive masking approach. This is in part due to inaccuracies in prioritization that may result from optical flow when, for example, background motion is involved. Overall, we make a few observations: (1) with only \(15\%\) decoder token budget, our proposed adaptive scheme bridges the gap with, and even improves on, vanilla VideoMAE which decodes all tokens with \(100\%\) token budget (None), and (2) in addition to random and uniform schemes, that are content agnostic, our proposed strategy outperforms over content-informed approaches (Flow and EVEREST), demonstrating its effectiveness, and lastly (3) the above trends hold over **both** pixel and token reconstruction objectives. As a side note, our findings corroborate with other related works  that reconstructing higher-level targets outperform RGB pixel reconstruction for MAE. The masking strategy comparisons upon scaling to 128 frames are presented in Table 4 and illustrate similar trends.

### Adaptive decoder masking enables pre-training over long videos

Using the best performing adaptive decoder masking strategy and FSQ-MagViT reconstruction targets, we reinvest the memory efficiency gained using just \(15\%\) token budget towards extending the input number of frames to 128. Table 2 shows performance on long-videos (\(128\) frames). We first highlight that default VideoMAE with no decoder masking encounters out of memory error due to large number of spatio-temporal tokens, demonstrating the need for decoder masking.

Overall, we observe that: (1) 128 frame pre-training outperforms 32 frames pre-training when fine-tuned on 128 frames; (2) 128 frame pre-training outperforms the typical 32 frames multi-crop evaluation. These two findings establish the significant benefit unlocked by our proposed model owing to long-video MAE pre-training, often ignored by previous models due to short context lengths.

   Saliency scheme & Masking & RGB & FSQ-MagViT \\  None & & 39.87 \( 0.14\) & 42.63 \( 0.07\) \\ Random & ✓ & 39.51 \( 0.11\) & 42.20 \( 0.07\) \\ Uniform  & ✓ & 39.92 \( 0.14\) & 41.73 \( 0.05\) \\ Flow & ✓ & 39.10 \( 0.12\) & 42.18 \( 0.16\) \\ EVEREST  & ✓ & 36.15 \( 0.14\) & 39.24 \( 0.08\) \\ Adaptive (Ours) & ✓ & **40.58**\( 0.08\) & **43.21**\( 0.09\) \\   

Table 1: **Decoder masking strategies on short-videos (32 frames) on EK100. We report fine-tuning top-1 action classification performance of MAE pre-trained models using pixel (RGB) or token (FSQ-MagViT) reconstruction targets. For decoder masking, we consistently use \(15\%\) as the token budget. We find that (1) compared to no decoder mask (None) with \(100\%\) budget, uniform masking scheme (Uniform ), random masking scheme (Random), and decoder masking using Optical Flow (Flow) perform competitively with the lower token budget, and (2) we obtain best results with our proposed Adaptive decoder masking scheme (Adaptive). _Note that fine-tuning performance is reported on 1 temporal crop._**

### Comparison with prior state-of-the-art works

In Tables 2(a) and 2(b), we compare our proposed method, dubbed LVMAE, against the state-of-the-art on EPIC-Kitchens-100 and Diving48 respectively. For these comparisons, we first pre-train our model on unlabeled videos from Kinetics710  data using 128 frames and adaptive masking strategy. This is followed by pre-training and fine-tuning on respective datasets at 128 frames similar to Sec.4.3. We additionally scale the encoder size to ViT-L. Further experiment details are in the Appendix. Note that existing SOTA methods use tailor-made architectures and/or pre-train their models using large-scale supervised pre-training data.

In Table 2(a), we show that our proposed model improves current SOTA on EPIC-Kitchens Top-1 Verb classification by +2.5 points using standard ViT architecture and just a single crop. On the Top-1 Noun classification, our model lags behind MTV-B  pre-trained on 60 million labeled video clips and a specialized multi-view architecture, TAdaFormer  pre-trained on supervised Kinetics710 and recently published Avion  that pre-trains on large-scale egocentric data namely Ego4D. We would like to point out that large-scale labeled pre-training helps nouns more than verbs. The EPIC-Kitchens noun categories such as hands, gloves, spoon, knife etc. routinely appear as annotations in large-scale pre-training datasets such as ImageNet21k , Ego4D  etc. (e.g. 282/300 nouns from EPIC-Kitchens-100 also appear in ImageNet21k). As noted by Verbs-In-Action , verbs are relatively scarce in existing datasets. This explains why existing SOTA that use large-scale datasets excel at noun classification. On the other hand, our approach doesn't use large-scale pre-training datasets and learns long-range spatio-temporal dynamics to push SOTA on verb classification. Furthermore, in Table 2(a), we find that if we add a supervised pre-training stage to our model using medium-scale dataset, we can bridge the gap on Noun classification while maintaining SOTA on Verb classification.

In Table 2(b), we show that our proposed model improves the absolute state-of-the-art on Diving48 dataset which contains complicated diving sequences by 3.9 points. It's worthwhile to note that the current SOTA method, MC-ViT  effectively uses 27M video-text pairs while we use \( 1\)M unlabeled videos and just 15K labeled videos.

### Ablation study

In Table 3(a), we ablate our decoder masking strategy by pre-training models at 32 frames and report the performance on EPIC-Kitchens-100 dataset using a ViT-B backbone. Similar to , we find that reconstructing a small amount of random tokens helps in improving performance. Since reconstructing random tokens adds stochasticity to the sampling process, we hypothesize it can help with overfitting. On the other hand, it is sample inefficient to only reconstruct random tokens and combining these two approaches strikes a balance. We limit this ablation study to a sampling ratio of 15%, respecting memory constraints for extending to 128 frame pre-training.

In Table 3(c), we gradually increase the number of frames and report the effect on EPIC-Kitchens-100 dataset. We find significant improvements in accuracy as we increase the number of frames from 16 to 32 to 64. However, as we move from 64 frames to 128 frames, the marginal improvement is small. This is expected as such videos form the tail-end of the distribution.

    &  &  \\  Saliency & Fine-tuning (128 frames) & Fine-tuning (128 frames) & Fine-tuning (32 frames) & Fine-tuning (32 frames) \\ scheme & Eval (128 frames x 1 crop) & Eval (128 frames x 1 crop) & Eval (32 frames x 4 crops) \\   & EK100 & D48 & EK100 & D48 & EK100 & D48 \\  None & out of memory & 44.5 & 85.7 & 44.1 & 76.8 \\ Adaptive (Ours) & **47.3** & **87.9** & 45.0 & 83.2 & 45.0 & 75.7 \\   

Table 2: **Decoder masking enables training over long-video (128 frames)**. We report fine-tuning top-1 action classification performance of long-video MAE pre-trained models using token (FSQ-MagViT) as reconstruction target. _Note that 128 frames fine-tuning is performed using random-tube masking with \(20\%\) masking and evaluation is reported on 1 temporal crop. Refer to Appendix A.5.3 for the fine-tuning details._In Table 3(e), we compare our model's performance with the current SOTA model, Avion , on videos of different lengths using the EPIC-Kitchens-100 Verbs benchmark. For this study, we use the Large version of our model. We observe sustained performance improvements over SOTA with longer durations signifying our model's capability on longer sequences.

In Table 3(b), we ablate the choice of targets for MAE pre-training. For this experiment, we first pre-train several models using 32 frames with no decoder masking. We find that when we shift from standard RGB targets to MAGVIT targets, the Noun-Verb Top-1 accuracy improves by 3.3%. Further, we notice a small drop from switching from MAGVIT targets to Adaptive FSQ-MagViT targets. In the last row, we show that when we use the adaptive decoder masking strategy with Adaptive FSQ-MagViT targets, we recover the performance. In effect, our proposed adaptive masking strategy

Table 3: **Comparison to State-of-the-art (SOTA). In these tables we compare to a broad set of approaches, many of which use additional (labeled) data in pre-training or specialized modules.**allows us to retain the performance boosts from MAGVIT targets at very high masking ratios, and thereby scale these gains to 128 frames effectively and surpass state-of-the-art. Unless otherwise mentioned, we always use Adaptive FSQ-MagViT as targets for all of our experiments.

In Table 3(d), we compare our proposed adaptive decoder masking strategy with other strategies at 128 frames using FSQ-MagViT as targets. We find that our proposed adaptive masking is best as we scale the number of frames from 32 to 128 on both EPIC-Kitchens-100 and Diving48 datasets.

## 5 Limitations and broader impact

In this work, we restrict ourselves to relatively small datasets and model sizes and leave the exploration about large-scale pre-training, joint training of image and video datasets, higher-capacity models, etc. to future work. Furthermore, while the 128 frames in this work are a big leap from prior works' focus on 16 to 32 frames, we anticipate more significant improvements to handle longer videos. Using efficient decoders (and encoders) or combining our long local-context with memory offers an alternate path to scaling MAEs, which is orthogonal to our approach to some degree.

Improved long-video understanding can potentially revolutionize how users interact with video content since it can enable AI to reason across complex events and nuances. This potential will benefit accessibility, efficient content creation, recommendation, moderation, etc. Meanwhile, as with many machine learning models, our proposed method can be biased by the data. In addition, some applications (e.g., surveillance) might negatively impact society, and we urge users and researchers to deal with such use cases responsibly.

## 6 Conclusion

In this paper we present a relatively simple but highly effective adaptive masking strategy for video MAE pre-training that allows us to pre-train on long videos (128 frames). Our approach is based on a novel MAGVIT-based tokenization strategy that also learns importance of tokens. Further, we are the first, to our knowledge, to show that long-video MAE pre-training is not only possible but leads to better encodings. Using our approach we are able to achieve state-of-the-art downstream performance, despite using a simple architecture and video only pre-training.

Table 4: **Ablation study**. We ablate a number of important design choices (please see text for details).

Acknowledgements

We thank Chris Duvarney for finding better hyper-parameters to our models. We are grateful to Huisheng Wang, Nisarg Kothari, Philip Mansfield and Hartwig Adam for their continued support. We sincerely acknowledge Anurag Arnab and the Scenic team for the excellent framework.