# Shopping MMLU: A Massive Multi-Task Online Shopping Benchmark for Large Language Models

Yilun Jin\({}^{1,2}\), Zheng Li\({}^{1}\), Chenwei Zhang\({}^{1}\), Tianyu Cao\({}^{1}\), Yifan Gao\({}^{1}\), Pratik Jayarao\({}^{1}\)

**Mao Li\({}^{1}\), Xin Liu\({}^{1}\), Ritesh Sarkhel\({}^{1}\), Xianfeng Tang\({}^{1}\), Haodong Wang\({}^{1}\), Zhengyang Wang\({}^{1}\) Wenju Xu\({}^{1}\), Jingfeng Yang\({}^{1}\), Qingyu Yin\({}^{1}\), Xian Li\({}^{1}\), Priyanka Nigam\({}^{1}\), Yi Xu\({}^{1}\) Kai Chen\({}^{2}\), Qiang Yang\({}^{2}\), Meng Jiang\({}^{1,3}\), Bing Yin\({}^{1}\)**

\({}^{1}\) Amazon.com \({}^{2}\) HKUST \({}^{3}\) University of Notre Dame

yilun.jin@connect.ust.hk, {amzzhe,cwzhang,caoty,yifango,psj}@amazon.com

{maoamz,xliucr,rssarkhe,wanghaod,zhengywa}@amazon.com, tangxianfeng@outlook.com

{xuwenju,jingfe,qingyy,xianlee,nigamp,yxaamzn}@amazon.com

{kaichen,qyang}@cse.ust.hk, mjiang2@nd.edu, alexbyin@amazon.com

Work done partially during Yilun's internship at Amazon. Authors 4-15 ordered alphabetically. Correspondence: Yilun Jin (yilun.jin@connect.ust.hk), Zheng Li (amzzhe@amazon.com)

###### Abstract

Online shopping is a complex multi-task, few-shot learning problem with a wide and evolving range of entities, relations, and tasks. However, existing models and benchmarks are commonly tailored to specific tasks, falling short of capturing the full complexity of online shopping. Large Language Models (LLMs), with their multi-task and few-shot learning abilities, have the potential to profoundly transform online shopping by alleviating task-specific engineering efforts and by providing users with interactive conversations. Despite the potential, LLMs face unique challenges in online shopping, such as domain-specific concepts, implicit knowledge, and heterogeneous user behaviors. Motivated by the potential and challenges, we propose Shopping MMLU, a diverse multi-task online shopping benchmark derived from real-world Amazon data. Shopping MMLU consists of 57 tasks covering 4 major shopping skills: concept understanding, knowledge reasoning, user behavior alignment, and multi-linguality, and can thus comprehensively evaluate the abilities of LLMs as general shop assistants. With Shopping MMLU, we benchmark over 20 existing LLMs and uncover valuable insights about practices and prospects of building versatile LLM-based shop assistants. Shopping MMLU can be publicly accessed at [https://github.com/KL4805/ShoppingMLU](https://github.com/KL4805/ShoppingMLU). In addition, with Shopping MMLU, we host a competition in KDD Cup 2024 2 with over 500 participating teams. The winning solutions and the associated workshop can be accessed at our website [https://amazon-kddcup24.github.io/](https://amazon-kddcup24.github.io/).

## 1 Introduction

Machine learning (ML) has been applied to various user-oriented online services, such as online communities, streaming services, etc, with online shopping being among the most successful ones. In recent years, ML methods are applied to various online shopping tasks, such as user queries , sessions , reviews , product attributes , etc. To facilitate the development of ML methods, many benchmarks are designed  to lower the barrier for researchers and engineers to develop and evaluate novel solutions to real-world online shopping tasks.

Online shopping is complex with numerous entities, relations, and tasks. For example, products are associated with _attributes, attribute values_, and _product categories_. Users interact with products with various behaviors such as _queries, clicks_, and _purchases_. Therefore, online shopping creates _multi-task learning_ problems involving a joint understanding and modeling of these entities. Moreover, the entities and tasks are not fixed but expand over time with new users and services, such as the expansion of Amazon from shopping to streaming services, creating _few-shot_ learning problems in the process. However, the multi-task, few-shot learning nature of online shopping is not sufficiently captured by existing works and benchmarks which mainly design task-specific models and datasets.

Large language models (LLMs) emerge as promising solutions to the multi-task, few-shot learning problem of online shopping. Recent works like GPT-3, T5, and FLAN  have shown that a single LLM can perform various text-related tasks with state-of-the-art performances, and can also generalize to unseen tasks with few-shot examples or even task descriptions only. These results motivate us to explore LLM-based solutions for online shopping with two advantages. First, we train a single LLM for all tasks instead of multiple task-specific models, which mitigates task-specific engineering efforts. Second, the trained LLM can seamlessly adapt to emerging tasks with only few-shot examples, lowering the costs for collecting large-scale data for model re-training. Moreover, LLM-based shop assistants can also improve user experiences by giving real-time interactive feedback to customer questions.

Despite the promising capabilities, LLM solutions for online shopping face specific challenges. We highlight the unique characteristics of tasks in online shopping with examples in Figure 1.

**Domain-specific Short Texts.** Texts in online shopping contain domain-specific entities, such as brands, models, etc., which may be challenging for general LLMs, especially without specific context.

**Implicit Knowledge.** Complex implicit knowledge and reasoning is required in online shopping to understand whether two products are compatible, or whether two brands produce similar items. Thus, it is challenging for LLMs to understand and adequately use the knowledge to perform reasoning.

**User Behaviors.** Aside from texts, implicit user behaviors exist (e.g. purchase, view, query-then-click, etc.) in online shopping. While implicit user behaviors are vital in understanding user intentions, general LLMs may not understand them as they rarely exist in pre-training data.

**Multi-lingual Tasks.** Online shopping spans a large number of countries, creating contents and tasks in multiple languages, which are challenging for LLMs trained with mostly English.

Figure 1: Distinctive characteristics of online shopping with real-world examples.

Motivated by the above potentials and challenges, we propose Shopping MMLU, a diverse multi-task online shopping benchmark for LLMs. Shopping MMLU consists of 57 tasks and 20,799 questions curated with real-world Amazon data and covers an extensive range of shopping entities like products, categories, attributes, queries, reviews, sessions, etc. We re-formulate all tasks in Shopping MMLU as text-to-text generation to accommodate LLM-based solutions. Furthermore, to enable fine-grained analysis of model capabilities, we split Shopping MMLU into 4 shopping skills corresponding to the characteristics shown in Figure 1: _shopping concept understanding_, _shopping knowledge reasoning_, _user behavior alignment_, and _multi-lingual abilities_. We benchmark over 20 LLMs on Shopping MMLU to explore the potential of building LLM-based online shop assistants. Our experimental results uncover valuable insights for domain-specific LLMs in online shopping, such as task-wise correlations, transferability of general knowledge, effects of instruction fine-tuning, and in-context learning.

We believe that Shopping MMLU can inspire and facilitate the transition from task-specific efforts to versatile LLM-based methods in online shopping. Moreover, as the characteristics of online shopping (Figure 1) exist in other user-oriented services as well, we also expect that the insights uncovered by Shopping MMLU would benefit efforts to build domain-specific LLMs in a wider range of fields.

## 2 Related Work

Online Shopping DatasetsWe summarize related online shopping datasets in Table 1. Previously, online shopping datasets often focus on one or several closely related tasks, e.g. MAVE  for attribute value extraction, Amazon-M2  for session-based recommendation, Amazon-ESCI  for query-product matching, etc. Consequently, they fail to reflect the multi-task nature of online shopping as their coverage of tasks and skills is limited.

More recently, multi-task online shopping datasets are curated to build versatile LLM-based shop assistants, such as EComInstruct for EconGPT  and ECInstruct for eCeLLM . Both EComInstruct and ECInstruct reformulate online shopping tasks into text-to-text generation and fine-tune a single LLM to perform all tasks. However, despite being multi-task datasets, EComInstruct solely focuses on shopping concept understanding (12 out of 12 tasks), while ECInstruct primarily tackles user behavior alignment (8 out of 10 tasks). Therefore, their coverage of skills in online shopping is still limited compared to Shopping MMLU, especially in reasoning and multi-lingual abilities.

LLMs for Online ShoppingShopping websites house various texts such as product titles, descriptions, reviews, ads, etc., motivating an extensive study of LLM solutions to online shopping tasks, such as recommendation , ranking , named entity recognition , etc. However, these methods are limited to specific tasks without fully exploring the multi-task nature of LLMs.

More recent works leverage instruction fine-tuning (IFT) to adapt general domain LLMs to online shopping, such as EconGPT and eCeLLM. However, as shown in Table 1, the capabilities of EconGPT and eCeLLM may be limited as their training data cover a limited range of shopping tasks and skills.

Web Agent Benchmarks for Online ShoppingLLMs bring about exciting prospects in developing agents that can perform sequential decision making and task execution following text instructions. As online shopping websites are diverse, realistic, and interactive environments, many benchmarks are developed upon them, such as WebShop  and WebArena  where agents are required

   Dataset &  Unified Text-Gen \\ Formulation \\  & \# Tasks &  Concept \\ Understanding \\  &  Knowledge \\ Reasoning \\  & 
 User \\ Behavior \\  & \# Languages \\  MAVE  & No & 1 & Partially & No & No & 1 \\ Amazon-M2  & No & 3 & No & No & Partially & 6 \\ Amazon ESC1  & No & 3 & No & No & Partially & 3 \\   EComInstruct-Test (EcomGPT)  & Yes & 12 & Yes & No & No & 2 \\ ECInstruct (eCeLLM)  & Yes & 10 & Partially & No & Yes & 1 \\ 
**Shopping MMLU** & **Yes** & **57** & **Yes** & **Yes** & **Yes** & **6** \\   

Table 1: Comparison between Shopping MMLU and related online shopping datasets. “Partially” means that the skill is covered with a limited number of tasks.

to perform shopping tasks such as purchasing products and summarizing reviews. We believe that Shopping MMLU is complementary to these agent benchmarks, as agents should first gain sufficient knowledge of online shopping before executing composite decision making tasks.

## 3 Dataset and Task Description

In this section, we present the overall design of Shopping MMLU, featuring 57 tasks across 4 key skills based on real-world Amazon data. We present the raw data sources used, the task taxonomy, task designs, and evaluation metrics. Finally, we describe our efforts to improve the quality of Shopping MMLU.

### Raw Data Sources

Shopping MMLU is curated primarily with real-world, internal or public  Amazon data, such as product catalogs, reviews, browse sessions, queries, etc. We remove all IDs (e.g. userID, sessionID, etc.) to ensure anonymity. We also use Claude 2  to synthesize data for some tasks that do not involve concrete product or user data. Details of raw data sources are given in Appendix A.2.

### Online Shopping Tasks

In this section, we introduce the task designs of Shopping MMLU, including the taxonomy, task types, and evaluation metrics.

#### 3.2.1 Task Taxonomy

Shopping MMLU consists of 57 tasks across 4 shopping skills corresponding to Figure 1. Moreover, we divide each skill into sub-skills to enable more fine-grained evaluation. A simplified taxonomy is shown in Figure 2, with the full taxonomy in Figure 9 in the Appendix. We introduce each skill and their sub-skills as follows, and leave more details in Appendix A.3.

Shopping Concept Understanding("Concept" for short). Online shopping concepts such as brands and product models are domain-specific and not often seen in pre-training. Moreover, they often appear in short texts (e.g. queries, attribute-value pairs) and thus no sufficient contexts are given to help understand them. Hence, failing to understand these concepts compromises the performance of LLMs on downstream tasks. We include the following sub-skills in this skill: _concept normalization_, _elaboration_, _relational inference_, _sentiment analysis_, _information extraction_, and _summarization_.

Shopping Knowledge Reasoning("Reasoning" for short). This skill focuses on understanding and applying various implicit knowledge to perform reasoning over products and their attributes. For example, calculations such as the total volume of a product pack require numeric reasoning, and finding compatible products requires multi-hop reasoning among various products over a product knowledge graph. Based on the specific type of reasoning required, we split this skill into three sub-skills, _numeric_, _commonsense_, and _multi-hop_ reasoning.

User Behavior Alignment("Behavior" for short). Accurately modeling user behaviors is a crucial skill in online shopping. A large variety of user behaviors exist in online shopping, including queries, clicks, add-to-carts, purchases, etc. Moreover, these behaviors are generally implicit and not expressed in text. Consequently, LLMs trained with general texts encounter challenges in aligning with the heterogeneous and implicit user behaviors as they rarely observe such inputs during pre-training. We further design the following sub-skills to reflect such heterogeneous behaviors: _query-query relation_, _query-product relation_, _sessions_, _purchases_, and _reviews & QAs_.

Figure 2: A brief taxonomy of Shopping MMLU including all skills and sub-skills.

Multi-lingual Abilities("Multi-lingual" for short). Multi-lingual models are desired in online shopping as they can be deployed in multiple marketplaces without re-training. Therefore, we design the skill of multi-lingual online shopping, consisting of _multi-lingual concept understanding_ and _multi-lingual user behavior alignment_.

#### 3.2.2 Task Types

We include 5 types of tasks in Shopping MMLU for a comprehensive evaluation of shopping skills, including _multiple choice, retrieval, ranking, named entity recognition_, and _generation_. Due to different format requirements, each type of task requires specific prompts such that the evaluated LLMs follow the instructions and generate valid answers, which we show in Appendix A.5.

Evaluation MetricsWe use _accuracy_ for multiple choice tasks, _hit rate@3_ for retrieval tasks, _normalized discounted cumulative gain (NDCG)_ for ranking tasks, and _micro F1_ for named entity recognition tasks. For generation tasks, we apply _ROUGE-L_ scores for extraction tasks (i.e. the answer is a sub-string of the input), _BLEU_ scores for translation tasks, and _sentence transformer similarity_ for other generation tasks. Details of the metrics are introduced in Appendix A.4. We take an average of all task-wise metrics (i.e. macro average) as the score of a skill.

### Data Quality Control

Datasets of online shopping are either defined by human behaviors or are human-labeled, and thus may contain noise or errors. To address the issue, we manually inspect all data samples to ensure the validity of the questions. We also remove potentially offensive contents and all links to images and videos in product descriptions and reviews. Details of data filtering are described in Appendix A.6.

## 4 Experiments and Analyses

In this section, we present our experimental setup, results, and analyses based on Shopping MMLU. Our experiments uncover the following insights:

* Proprietary LLMs remain the state-of-the-arts on Shopping MMLU, with Claude-3 Sonnet performing the best overall. However, strong open-source LLMs have caught up with proprietary ones like ChatGPT.
* Tasks and skills in Shopping MMLU, and hence online shopping share much knowledge in common, as indicated by the highly positive correlations between pairwise tasks and skills in Shopping MMLU.
* General knowledge transfers well to the specific domain of online shopping. Strong models on general LLM benchmarks remain strong on Shopping MMLU.
* IFT improves the performance on Shopping MMLU in most cases. However, general domain IFT may lead to overfitting and hence compromise the contained knowledge in strong base models, while domain-specific IFT works only on strong base models and observed tasks and skills.
* Few-shot learning remains challenging on Shopping MMLU. In-context examples lead to worse performances for many models and tasks.

### Experimental Setup

We apply zero-shot evaluation for Shopping MMLU for three main reasons. First, zero-shot evaluation resembles the real-world scenario where customers directly enter their questions without creating few-shot examples. Second, zero-shot evaluation rules out variances brought by different few-shot examples. Finally, all evaluated models achieve non-trivial results under zero-shot evaluation on Shopping MMLU. All models are tested with the same prompts.

### Evaluated Models

We evaluate LLMs with various sizes and training methods to uncover insights about how to build domain-specific LLMs. Details of model access is given in Appendix B.1. Evaluated models include:Proprietary ModelsWe evaluate ChatGPT , Claude-2 , and Claude-3 Sonnet , which are state-of-the-art LLMs trained with general domain data and provide insights on how well LLMs can solve domain-specific online shopping problems with general knowledge only.

Open-Source General ModelsOpen-source LLMs can be categorized as _base_ and _chat models_. Base models refer to LLMs that are only pre-trained with next-token prediction without any moderation techniques, while chat models often undergo IFT such that they follow the input instructions. We include both base and chat models to see how the instruction following abilities of chat models transfer from the general domain to the specific domain of online shopping. Specifically, we consider **LLaMA2** (7/13/70B, base and chat) , **LLaMA3** (8/70B, base and instruct) , **Mistral** (7/8x7B, base and instruct) , **QWen1.5** (4/7/14/72B) , and **Phi-2** models.

Domain-specific ModelsWe evaluate eCeLLM-S, M, and L models that are fine-tuned with domain-specific online shopping IFT data (ECInstruct ) over Phi-2, Mistral-7B, and LLaMA2-13B, respectively, to see how domain-specific IFT helps improve model performances on Shopping MMLU.

### Overall Performance

We show the scores of all evaluated models on each skill of Shopping MMLU in Table 2. Due to space limitations, we omit detailed task-wise scores. We draw the following insights from Table 2.

First, **proprietary LLMs remain the state-of-the-art, while open-source LLMs are catching up**. Claude-3 Sonnet performs the best across all models, followed by Claude-2 and ChatGPT. Overall, these proprietary LLMs remain the strongest even in the specific domain of online shopping. We also observe that LLaMA3-70B-Instruct and QWen1.5-72B perform on par with ChatGPT and Claude-2, demonstrating the potential of building powerful LLM shop assistants with public resources.

Second, **Shopping MMLU is a challenging benchmark**. While eCeLLMs outperform GPT-4 on their dataset ECInstruct , they are still far behind ChatGPT on Shopping MMLU, showing that Shopping MMLU is a more complex and challenging benchmark for online shopping than ECInstruct.

    **Model** \\ **Type** \\  &  **\# Params.** \\  &  **Model** \\ **Understanding** \\  &  **Shopping Concept** \\ **Reasoning** \\  &  **Shopping Knowledge** \\ **Allignment** \\  &  **User Behavior** \\ **Adilities** \\  &  **Multi-lingual** \\ **Abilities** \\  \\   & N/A & Claude-3 Sonnet & **80.75** & **71.63** & **70.17** & **67.76** \\  & Claude-2 & 75.46 & 65.50 & 63.53 & 65.24 \\  & ChatGPT & 75.63 & 64.97 & 59.79 & 60.81 \\   & & LLaMA3-70B-Instruct & **75.24** & **69.29** & **67.67** & 62.00 \\  & & QWen1.5-72B & 71.67 & 68.92 & 64.12 & **64.84** \\  & & LLaMA3-70B & 69.59 & 63.56 & 55.77 & 58.95 \\  & & LLaMA2-70B-chat & 61.84 & 40.73 & 44.20 & 47.04 \\  & & LLaMA2-70B & 61.05 & 55.87 & 43.24 & 47.85 \\  & & Mixrat-8x7b & 59.43 & 54.32 & 55.31 & 44.69 \\   & & QWen1.5-14B & **67.22** & **60.92** & **54.92** & 55.21 \\  & & eCeLLM-L & 61.54 & 54.84 & 54.55 & **59.64** \\  & & Vicuna-13B & 59.64 & 52.63 & 49.81 & 49.64 \\  & & LLaMA2-13B-chat & 51.79 & 45.01 & 39.95 & 42.99 \\  & & LLaMA2-13B & 45.86 & 39.47 & 39.43 & 44.23 \\   & & LLaMA3-8B-Instruct & **65.26** & **56.84** & **54.88** & 55.37 \\  & & LLaMA3-8B & 58.02 & 49.74 & 44.16 & 51.03 \\  & & QWen1.5-7B & 58.89 & 52.34 & 49.81 & 50.14 \\  & & eCeLLM-M & 63.29 & 48.94 & 53.78 & **56.08** \\  & & Zephyr & 61.65 & 52.57 & 44.73 & 45.35 \\  & & Mistral-7B-instruct & 62.03 & 46.36 & 42.21 & 43.32 \\  & & Mistral-7B & 55.82 & 46.69 & 46.27 & 41.47 \\  & & Vicuna-7B & 53.46 & 45.06 & 41.11 & 43.82 \\  & & LLaMA2-7B-chat & 51.67 & 43.48 & 41.42 & 40.43 \\  & & LLaMA2-7B & 38.22 & 32.81 & 32.56 & 27.71 \\   **Cb** \\ **cSB** \\  } & QWen1.5-4B & **57.21** & **52.56** & **42.74** & **49.78** \\  & Phi-2 & 49.34 & 42.83 & 36.38 & 32.91 \\   & & eCeLLM-S & 49.40 & 39.06 & 36.33 & 32.79 \\   

Table 2: Overall scores (%) on Shopping MMLU across all evaluated models. The best performances in LLMs with similar number of parameters are shown in **bold**.

Finally, **domain-specific models are not always strong**. While eCeLLMs perform better on Shopping MMLU than their base models (eCeLLM-M/Mistral-7B, eCeLLM-L/LLaMA2-13B), they are not always strong compared to LLMs with similar numbers of parameters. For example, among \(\)13B LLMs, eCeLLM-L generally performs worse than QWen1.5-14B; among \(\)7B LLMs, eCeLLM-M generally performs worse than LLaMA3-8B-Instruct. These facts indicate that LLMs with proper training in the general domain already excel in online shopping without domain-specific tuning.

### How 'Multi-task' is Online Shopping?

According to , the key of multi-task learning is to leverage _useful information in multiple tasks_ to improve _the performances of all tasks_. Consequently, the more the shared knowledge, the more likely we can jointly improve all tasks in Shopping MMLU and build versatile LLM-based shop assistants. Thus, in this section, we analyze the extent to which knowledge is shared among tasks in Shopping MMLU by analyzing the score correlations between pairwise tasks and skills.

We first analyze the task-wise score correlations. Let \(_{i}\) be the scores achieved by all evaluated LLMs on task \(i\), the score correlation between tasks \(i\) and \(j\) is defined as \(c_{ij}=(_{i},_{j})\). The distribution of \(c_{ij}\) is shown in Figure 3(a). As shown, the scores of most task pairs (1589 out of 1596) are positively correlated. Moreover, with an average of 0.557 and a standard deviation of 0.209, the score correlations are significantly positive, indicating a notable amount of shared knowledge among tasks in Shopping MMLU. We analyze task pairs with negative correlations in Appendix B.3.

We similarly compute the score correlations between pairwise skills and plot them in Figure 3(b). As shown, all skills are positively correlated with each other with correlations of at least 0.9. The observation further underscores the multi-task nature of Shopping MMLU and the potential of jointly improving online shopping skills as a whole with unified solutions.

### How to Build LLM-based Shop Assistants?

In this section, we analyze various LLM moderation techniques, including model scaling, IFT, and in-context learning, to see whether and how they are helpful in improving the performances of LLMs on the specific domain of online shopping.

#### 4.5.1 General Knowledge Transfers Well to Online Shopping

The field of LLMs advances at a rapid pace, yielding models with increasingly powerful capabilities. Therefore, we analyze whether the specific domain of online shopping benefits from the advancing LLMs and their increasing general knowledge and abilities. We calculate the score correlations between each skill in Shopping MMLU and the Open LLM Leaderboard , consisting of MMLU, GSM8K, Winogrande, HellaSwag, TruthfulQA, and ARC [10; 7; 23; 50; 36; 6]. The correlations are shown in Figure 4(a), where all skills show strongly positive correlations with the Open LLM Leaderboard scores. The high correlations indicate that general knowledge transfers well to the specific domain of online shopping, and that powerful LLM-based shop assistants should be established upon strong base models.

Figure 3: Task and skill-wise score correlations of Shopping MMLU.

The smooth transfer from general knowledge to the domain of online shopping is also observed in the effects of model scaling, which are shown in Figure 4(b). We observe consistent improvements on Shopping MMLU as LLMs within each family (LLaMA2, LLaMA3, and QWen1.5) increase in size.

#### 4.5.2 Effects of Instruction Fine-tuning

In this section, we analyze the effects of IFT  on Shopping MMLU. We analyze both general domain and domain-specific IFT to understand whether the instruction following ability transfers from the general domain to online shopping, and how domain-specific IFT achieves further improvement.

General domain IFTWe analyze LLaMA2 and LLaMA3 models to study the impact of general domain IFT on Shopping MMLU. We plot the relation between scores of base models and the improvements brought by IFT (e.g. LLaMA3-8B-Instruct VS Base) in Figure 5. We only plot the average values across all 4 skills and leave details in Appendix B.4. We make the following observations.

First, **general domain IFT helps in most cases**. Among the 5 models tested, IFT leads to performance improvements on 4 of them, indicating that the instruction following ability brought by general domain IFT often transfers to the specific domain of online shopping. Second, **IFT data and recipe matters**. Comparing LLaMA2 and LLaMA3, we find that LLaMA3 models generally benefit more from IFT, which can be attributed to the better instruction data with 'careful curation' used to tune LLaMA3 . Finally, **general domain IFT is less helpful on stronger base models.** Within each model family, IFT leads to less improvements on stronger base models. Notably, IFT leads to performance decline on LLaMA2-70B. We hypothesize that as base models gets stronger, they may overfit to the relatively small IFT dataset during IFT, resulting in the catastrophic forgetting of helpful knowledge.

Domain-specific IFTAs shown in Table 2, while eCeLLMs perform better than their base models with domain-specific IFT, they do not compare favorably against strong general domain LLMs (LLaMA3 and QWen1.5). Therefore, we analyze the reasons underlying the limited improvements and shed light on how domain-specific IFT data should be curated. We show the comparisons between eCeLLMs and their base models in Figure 6. We also include Zephyr and Vicuna-13B, which are tuned with general domain IFT over Mistral-7B and LLaMA2-13B, respectively. We make the following observations.

* **Domain-specific IFT only works on strong base models**. As shown in Figure 6(a), eCeLLM-S fails to improve over its base model Phi-2, while in Figure 6(b) and 6(c), both eCeLLM-M and

Figure 4: General knowledge transfers well to online shopping.

Figure 5: Relation between base model scores and improvements of IFT on Shopping MMLU.

L outperform their base models. The observation indicates that domain-specific IFT works only on sufficiently strong base models, which echoes the phenomenon in general domain IFT .
* **Domain-specific IFT only works on observed tasks and skills**. As shown in Figure 6(b) and 6(c), eCeLLMs primarily improve over their counterparts on "Behavior" and "Multi-lingual" skills, which should be attributed to their IFT datasets. In eCLMstruct used to tune eCeLLMs, 8 out of 10 tasks belong to the "Behavior" skill. Therefore, it is not surprising that eCeLLMs perform well on the "Behavior" skill. We also hypothesize that the knowledge transfer from English to other languages leads to the improvement on the "Multi-lingual" skill, as this skill consists heavily of multi-lingual user behavior alignment tasks. However, eCeLLMs achieve limited improvements on "Concept" and "Reasoning" skills, showing that domain-specific IFT only works on skills included in the IFT data and does not generalize well to unseen skills. Therefore, domain-specific IFT data should be curated with sufficient diversity and coverage.

We also test eCeLLM-M and L on 5 general LLM benchmarks, MMLU, HellaSwag, Winogrande, TruthfulQA, and GSM8K to analyze why domain-specific IFT fails to generalize to unseen skills. Results are shown in Figure 7, where eCeLLMs perform worse than their base models in most general LLM benchmarks. Thus, domain-specific IFT fails to improve or even compromises the model's general knowledge, which may explain their inability to generalize to unseen skills.

#### 4.5.3 Effects of In-context Learning

LLMs are capable of learning from few-shot examples in prompts, known as _in-context learning_. As few-shot learning is common in online shopping, such as cold-start users, we analyze how well LLMs adapt to unseen tasks with few-shot examples and thus solve the few-shot learning problem. We select representative subsets of models and tasks for the analysis (details in Appendix B.5). For each selected task, we split the dataset into a training set of 20 samples, and the rest as test sets. We evaluate under 0-, 1-, and 5-shot settings and show results in Figure 8. For each setting, we randomly sample few-shot examples from the training set and show the mean score of 5 random seeds. We observe the following phenomena despite the mixed results.

First, **in-context learning is not generally helpful on Shopping MMLU**. We observe that in many cases, adding few-shot examples fails to improve model performances. Even worse, for some models

Figure 6: Comparison between domain-specific eCeLLMs and their base models on Shopping MMLU.

Figure 7: Scores of eCeLLM and their base models on general LLM benchmarks.

and skills, in-context examples lead to worse scores (e.g. ChatGPT, Vicuna-13B and eCeLLM-L in Figure 8(c)). The observation indicates that few-shot learning in online shopping remains challenging even with strong LLMs. Second, **in-context learning does not help reasoning tasks.** We observe from Figure 8(b) that in-context learning fails to improve the performance of any model on shopping knowledge reasoning tasks. We further explore this observation with chain-of-thought (CoT) prompting , whose results are shown in Appendix B.5.

## 5 Conclusion and Future Work

This paper presents Shopping MMLU, a multi-task online shopping benchmark for LLMs aiming to facilitate LLMs-based solutions to a unified, multi-task modeling of online shopping. Shopping MMLU features a wide range of online shopping skills, tasks, and entities, and thus is suitable for researchers and practitioners to comprehensively evaluate their solutions of domain-specific LLM online shop assistants. With Shopping MMLU, we perform extensive experiments on over 20 LLMs, whose results uncover valuable insights on building domain-specific LLMs for online shopping, such as task- and skill-wise relations, general knowledge, instruction fine-tuning, and in-context learning.

Shopping MMLU triggers a series of future work. In Appendix C we show that state-of-the-art proprietary LLMs still lags behind task-specific methods on some tasks of Shopping MMLU, motivating advanced training recipes and data for LLMs in online shopping. We also discuss broader impacts and limitations in Appendix C.