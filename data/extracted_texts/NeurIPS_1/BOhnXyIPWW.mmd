# Locally Private and Robust Multi-Armed Bandits

Xingyu Zhou

Wayne State University

xingyu.zhou@wayne.edu

Work done during research intern at Wayne State University

Wei Zhang

Texas A&M University

komo@tamu.edu

###### Abstract

We study the interplay between local differential privacy (LDP) and robustness to Huber corruption and possibly heavy-tailed rewards in the context of multi-armed bandits (MABs). We consider two different practical settings: LDP-then-Corruption (LTC) where each user's locally private response might be further corrupted during the data collection process, and Corruption-then-LDP (CTL) where each user's raw data may be corrupted such that the LDP mechanism will only be applied to the corrupted data. To start with, we present the first tight characterization of the mean estimation error in high probability under both LTC and CTL settings. Leveraging this new result, we then present an almost tight characterization (up to log factor) of the minimax regret in online MABs and sub-optimality in offline MABs under both LTC and CTL settings, respectively. Our theoretical results in both settings are also corroborated by a set of systematic simulations. One key message in this paper is that LTC is a more difficult setting that leads to a worse performance guarantee compared to the CTL setting (in the minimax sense). Our sharp understanding of LTC and CTL also naturally allows us to give the first tight performance bounds for the most practical setting where corruption could happen both before and after the LDP mechanism. As an important by-product, we also give the first correct and tight regret bound for locally private and heavy-tailed online MABs, i.e., without Huber corruption, by identifying a fundamental flaw in the state-of-the-art.

## 1 Introduction

The Multi-Armed Bandit (MAB) problem (Berry and Fristedt, 1985) offers a fundamental approach for sequential decision-making under uncertainty based on only bandit feedback. Take online advertising as an illustrative example, where the advertising platform (i.e., the central learner) sequentially and adaptively displays ads (i.e., arm) based on users' reward feedback (e.g., engagement score) so as to maximize the cumulative rewards. In practice, several important factors have to be considered when designing real-world MAB algorithms, as illustrated below using online advertising.

**Privacy.** The raw engagement score (which is calculated based on clicks, purchases, and time spent viewing the ad, etc.) from a user's device may lead to privacy leakage. For instance, when the ad is about medicine on some rare or uncommon disease, a high engagement score might imply interest or association with the uncommon disease. Such privacy leakage may lead to unintended personal and social consequences as well as trust issues on the platform. One principled way to mitigate it is via local differential privacy (LDP) (Kasiviswanathan et al., 2011; Duchi et al., 2018), i.e., each user's device locally adds a suitable amount of noise (depending on the privacy mechanism and budget) to obfuscate the raw feedback before sending it out from the device (see the yellow region in Fig. 1).

**Robustness.** Another important factor in real-world scenarios is the robustness of MAB algorithms under both possibly heavy-tailed feedback and adversary corruption.

_Heavy-tailed feedback._ The engagement score in our example could often be heavy-tailed, i.e., non-negligible probabilities of observing extremely high values. This might happen due to some special events and seasons (e.g., Black Friday) or influencer interaction.

_Adversary corruption._ There could be malicious attacks on the engagement scores during the collection of users' feedback, e.g., with some probability, each score could be replaced by any _arbitrary_ value, i.e., Huber corruption (Huber, 1964). On the other hand, corruption can also happen on each user's side before transmission, e.g., one could manipulate or spoof interactions to skew scores. Most practically, corruption can also happen both before and after the data transmission.

To tackle the above privacy and robustness issues in MABs, there has been a large related literature, which, however, mainly investigates the two issues in an isolated way (see Appendix B for details). Motivated by this, in this work, we are particularly interested in the following question:

_Is there any interesting interplay between privacy and robustness in MABs?_

**Our contributions.** We give an affirmative answer to the above question by unveiling a fundamental interplay between privacy protection (in particular, local differential privacy (LDP)) and robustness under Huber corruption and heavy-tailedness. Our main message is a separation result between two MAB settings that differ in the order of privacy protection and corruption, i.e., LDP-then-corruption (LTC) vs. Corruption-then-LDP (CTL). That is, under LTC, corruption happens after LDP mechanism while under CTL, corruption happens before the LDP mechanism (see Fig. 1). To obtain our separation result for the two settings, we take the following principled approach:

**1.** We first study the mean estimation problem - a cornerstone step in the analysis of stochastic MABs - under both LTC and CTL settings. We give the first tight characterization of the estimation error in high probability, in terms of privacy budget, corruption level, and heavy-tailedness. Specifically, we first establish lower bounds on the minimax error rate in high probability and then propose a unified optimal algorithm that achieves matching worst-case upper bounds for both settings. The key observation here is that the mean estimation error under LTC is larger than that under CTL and moreover the gap becomes larger as the privacy requirement becomes stronger. Further, our sharp results on LTC and CTL also naturally enable us to give tight performance bounds for the most practical setting, C-LDP-C, where corruption happens both before and after LDP, see (3) in Fig. 1.

**2.** Leveraging the above tight mean estimation results, we then study both online MABs and offline MABs under both LTC and CTL. We present an almost tight characterization (up to log factor) of the corresponding minimax performances (i.e., regret in online MABs and sub-optimality in offline MABs) by deriving lower bounds and proposing almost optimal algorithms. As in mean estimation, there is a separation between LTC and CTL, i.e., LTC is a more difficult setting that leads to worse performance in the minimax sense, highlighting the interesting interplay between privacy and robustness in MABs. All of these results also allow us to easily handle the C-LDP-C setting.

**3.** Along the way, several results could be of independent interest. First, our optimal locally private and robust mean estimators can be applied to many other applications beyond MABs. Moreover, as an important by-product, we identify a fundamental flaw in the regret upper bound of state-of-the-art locally private online MABs with heavy tails (i.e., without corruption), and give the first correct one.

**Related Work.** We discuss the most relevant related work in the main body and relegate a detailed discussion to Appendix B. LDP with bounded/sub-Gaussian reward is first introduced to MABs in Ren et al. (2020) and later it was generalized to the heavy-tailed rewards (Tao et al., 2022). Robust MABs under Huber corruption have been recently studied in Kapoor et al. (2019); Mukherjee et al. (2021); Basu et al. (2022); Agrawal et al. (2023) while robust MABs concerning heavy-tailed reward date back to Bubeck et al. (2013). However, these work only study privacy and robustness separately. To the best of our knowledge, there are only two very recent work that consider privacy and robustness in MABs simultaneously. In Wu et al. (2023), the authors consider the central DP model where the raw non-private feedback received by the central learner can be first corrupted under Huber model. This is in sharp contrast to our local DP model, which is not only stronger but allows us to study

Figure 1: The interplay between privacy and robustness (heavy-tailed data and corruption).

the order of corruption and privacy. In Charisopoulos et al. (2023), the authors study linear bandits (which includes MAB as a special case) under LDP and then Huber corruption (i.e., LTC setting). As will be discussed in Section 4, their regret bound is sub-optimal and worse than ours when reduced to the MAB case. Note that we also study the CTL setting, which in turn allows us to study the most practical setting C-LDP-C. Finally, our work is inspired by recent advances in (locally) private and robust mean estimation (Li et al., 2022; Cheu et al., 2021; Chhor and Sentenac, 2023). Our key contributions are the first _high-probability_ concentration bounds for both CTL and LTC settings.

## 2 Problem Setup

In this section, we formally introduce the three problems considered in this paper: mean estimation, online and offline MABs, under the constraints of both LDP and robustness (including heavy tails and Huber corruption). To start with, we introduce the privacy and corruption models.

**Definition 1** (\(\)-LDP, Duchi et al. (2018)).: For a privacy parameter \(\), the random variable \(\) is an \(\)_-locally differentially private_ view of \(X\) via privacy channel/mechanism \(Q\) if

\[_{S(}),x,x^{}} S X=x)}{Q( S X=x^{})} e^{ },\]

where \((})\) denotes an appropriate \(\)-field on \(}\). In this case, we also say that the conditional distribution (privacy channel) \(Q\) is an \(\)-LDP privacy mechanism. We write \(_{}\) as the set of all \(\)-LDP mechanisms (channels).

**Definition 2** (\(\)-Huber corruption, Huber (1964)).: Given a parameter \([0,1/2)\) and a distribution \(D\) on inliers, the output distribution under \(\)-Huber model is \(O=(1-)D+ E\). That is, a sample from \(O\) returns a sample from \(D\) with probability \(1-\) and otherwise returns a sample from some (unconstrained and unknown) corruption distribution \(E\). We write \(_{}(D)\) as the set of all possible \(\)-Huber corruptions (channels) of inlier distribution \(D\).

With the two definitions in hand, we can introduce the two main settings in this paper: (i) LDP-then-Corruption (LTC) vs. (ii) Corruption-then-LDP (CTL), as also illustrated in Fig. 1.

**Definition 3** (LTC vs. CTL).: We consider the following interplay between privacy and corruption.

(i) **LDP-then-Corruption (LTC):** Each user \(i[n]\) first generates an \(\)-LDP view of raw data \(X_{i}\). Then, the private data \(Y_{i}\) from each device is independently corrupted by an \(\)-Huber channel that outputs \(Z_{i}\) to the central analyzer/agent.

(ii) **Corruption-then-LDP (CTL):** Each user's raw data \(X_{i}\) is first independently corrupted by an \(\)-Huber model. Then, the corrupted data \(Y_{i}\) passes through an \(\)-LDP mechanism at each device that outputs \(Z_{i}\) to the central analyzer/agent.

Under both settings, we aim to design \(\)-LDP mechanisms for user devices and central analyzers that ensure local privacy and robustness against \(\)-Huber corruption and heavy-tailed data distributions. The two settings also naturally enable us to study the most practical setting C-LDP-C.

**Mean estimation.** As in Duchi et al. (2018), given a real number \(k>1\), we consider the following class of possibly heavy-tailed distributions

\[_{k}:=\{P_{X P}[X][-1,1]_{X P}[|X|^{k}] 1\}. \]

That is, \(k\) controls the tail behavior of the distribution with smaller \(k\) meaning heavier of the tails. Given any distribution \(P_{k}\), our goal is to estimate its mean \((P)\) as accurately as possible. In contrast to the standard case where the analyzer has access to \(i.i.d\) samples \(\{X_{i}\}_{i=1}^{n}\) from \(P\), the analyzer in this paper now only observes samples \(\{Z_{i}\}_{i=1}^{n}\) that are both private and corrupted view of \(\{X_{i}\}_{i=1}^{n}\). Specifically, we are interested in the _high probability_ error under our two different settings (LTC vs. CTL), as formally defined below.

**Definition 4** (Minimax mean estimation error rate).: Given \(>0\) and sample size \(n>0\), the minimax mean estimation error rate of the class \(_{k}\) under \(\)-LDP and \(\)-Huber corruption is defined as follows

\[_{}^{*}(k,,,n)\!:=\!\{>0_{Q _{}}_{_{n}}_{P_{k} }_{C_{}(P)}[|_{n}-(P)|> ]\}, \]where \(_{n}\) is a measurable function of \(\{Z_{i}\}_{i=1}^{n}\), i.e., private and corrupted view of \(n\)\(i.i.d\) samples \(\{X_{i}\}_{i=1}^{n}\) from \(P_{k}\) that pass through \(\)-LDP channel \(Q\) and \(\)-Huber corruption channel \(C\). We write \(_{,}}^{*}(k,,,n)\) and \(_{,}}^{*}(k,,,n)\) for the settings of LTC and CTL.

Intuitively speaking, \(_{}^{*}\) represents the minimal error rate that any \(\)-LDP estimator can achieve with high probability \(1-\) for all distributions \(P_{k}\) and all \(\)-Huber corruption models, hence taking inf over \(Q\) and \(_{n}\) and sup over distribution and corruption. Thus, the goal in our mean estimation problem is to design an optimal \(\)-LDP mechanism \(Q^{}\) at each user's side and an optimal analyzer \(_{n}^{*}\) at the central analyzer in order to attain the minimax mean estimate error rate in (2).

**Online MABs.** At each round \(t[T]\), the central learner/analyzer chooses an action/arm \(a_{t}[K]\) according to a policy \(\) and receives a reward sample \(X_{t}\) that is drawn from some distribution \(P_{a_{t}}\) with unknown mean \(r(a_{t}):=(P_{a_{t}})\). Here, the policy is \(=\{_{t}\}_{t=1}^{T}\) and \(_{t+1}\) is a measurable function of the data received by the end of round \(t\), i.e., for each \(t[T]\), \(_{t}=\{(a,X^{(a)}(t))\}_{a[K]}\) where \(X^{(a)}(t):=\{X^{(a)}_{1},,X^{(a)}_{N_{a}(t)}\}\) and \(_{a K}N_{a}(t)=t\). That is, for each round \(t\), \(X^{(a)}(t)\) groups together all \(N_{a}(t)\) rewards from each arm \(a[K]\) where \(N_{a}(t)\) is the total number of times that arm \(a\) has been pulled by time \(t\). The goal in online MABs is to characterize the minimax _clean_ regret under our LTC and CTL settings defined below.

**Definition 5** (Minimax clean regret).: Let \((k):=\{\{P_{a}\}_{a K} P_{a}_{k}\}\) be the class of \(K\)-armed MAB instances with inlier distributions for each arm in \(_{k}\). Then, the minimax clean regret is defined as

\[R^{*}(k,,,T):=_{Q_{}}_{}_{I (k)}_{C_{}(I)}[T r(a ^{})-_{t=1}^{T}r(a_{t})], \]

where \(a_{t+1}\) is a measurable function (via \(\)) of private and corrupted dataset \(\{(a,Z^{(a)}(t))\}_{a[K]}\). Here, for any arm \(a[K]\) and \(t[T]\), \(Z^{(a)}(t):=\{Z^{(a)}_{1},,Z^{(a)}_{N_{a}(t)}\}\) is the private and corrupted view of \(N_{a}(t)\) samples of \(P_{a}\) that pass through \(\)-LDP channel \(Q\) and \(\)-Huber corruption channel \(C\). We write \(R^{*}_{}}(k,,,T)\) and \(R^{*}_{}}(k,,,T)\) for the settings of LTC and CTL, respectively.

The goal in online MABs is to design an optimal \(\)-LDP mechanism \(Q^{}\) and optimal learning policy \(^{}\) so as to attain the minimax clean regret in (3).

_Remark 1_.: As standard in the literature (Wu et al., 2023; Chen et al., 2022; Niss and Tewari, 2020), \(r()\) in (3) is the mean of inlier distributions while the randomness in the expectation is generated by both privacy and corruption.

**Offline MABs.** In the offline case, the analyzer cannot interact with users and instead, it is given a batch pre-collected dataset \(=\{(a_{i},X_{i})\}_{i=1}^{N}\) sampled from some joint distribution of a behavior policy \(\) and reward distributions \(\{P_{a}\}_{a[K]}\). As in Rashidinejad et al. (2021), we assume a finite concentrability coefficient \(^{}\) such that \(1/(a^{})^{}\), where \(a^{}\) is the optimal arm that has the largest mean and \(^{}\) captures deviation between the behavior distribution \(\) and the distribution induced by the _optimal_ policy. The goal here is to characterize the minimax sub-optimality under our LTC and CTL settings defined below.

**Definition 6** (Minimax sub-optimality).: Let

\[(^{},k):=\{(,\{P_{a}\}_{a K})\,|\,P_{a} _{k}1/(a^{})^{}\}\]

be the class of \(K\)-armed MAB instances with distributions in \(_{k}\) and concentrability coefficient \(^{}\). Then, the minimax sub-optimality is defined as

\[^{*}(^{},k,,,N):=_{Q _{}}_{}_{I(^{},k)} _{C_{}(I)}[|r(a^{})-r( )|], \]

where \(\) is a measurable function of private and corrupted dataset \(\{(a,Z^{(a)})\}_{a[K]}\) and \(Z^{(a)}:=\{Z^{(a)}_{1},,Z^{(a)}_{N_{a}}\}\) is the private and corrupted view of \(N_{a}\) samples of \(P_{a}\) that pass through \(\)-LDP channel \(Q\) and \(\)-Huber corruption channel \(C\). We write \(^{*}_{}}(^{},k,,,N)\) and \(^{*}_{}}(^{},k,,,N)\) for LTC and CTL, respectively.

We remark that we assume the batch data is collected by an \(\)-LDP mechanism that can be specified by the learner. Note that as in the standard case, we do not control the behavior policy \(\) other than a finite \(^{}\). The goal here is to design an optimal \(\)-LDP mechanism \(Q^{}\) (which protects local privacy for any users offering batch data) and optimal offline learning algorithm \(^{}\).

Mean Estimation

We start with our first problem - mean estimation under privacy and robustness constraints. Our main result in this section is the following theorem that characterizes the minimax error rate (cf. Def. 4)

**Theorem 1** (Mean Estimation).: _Given any fixed \((0,1/2)\)2, \(\), \([0,1/2)\) and \(k>1\), we have that for all large enough \(n\),_

\[^{*}_{,}(k,,,n) =(()^{1-1/k}+( })^{1-1/k}),\] \[^{*}_{,}(k,,,n) =(^{1-1/k}+(})^{1-1/k}).\]

_Remark 2_.: To the best of our knowledge, this is the first high-probability concentration bound for mean estimation under both LTC and CTL, which tightly captures the dependence on the corruption level \(\), privacy budget \(\) and heavy-tail parameter \(k\), simultaneously. It can be seen that for LTC setting, there is an additional \((1/)^{1-1/k}\) factor, which implies that introducing LDP guarantee first would make it more vulnerable to corruption/data manipulation attacks. Interestingly, for a fixed \(\), this additional vulnerability due to LDP decreases as the tail becomes heavier, which offers additional insight into the interplay of privacy, heavy-tailedness, and robustness. Our LTC result also complements the result in Cheu et al. (2021), which considers the bounded case (i.e., \(k=\)) under constant probability only rather than our high probability guarantee. On the other hand, for CTL, we note that the impact of corruption and privacy is _separable_. Our high probability bound for CTL complements the error bound in terms of mean-square error (MSE) only in Li et al. (2022).

To establish Theorem 1, we first establish the following lower bounds, with full proof in Appendix E.

**Proposition 1** (Lower Bounds).: _Given any fixed \((0,1/2)\), \(\), \([0,1/2)\), \(k>1\) and \(n\) large enough, for all \(\)-LDP mechanism \(Q\) and all estimator \(_{n}\), there exists a distribution \(P_{k}\) and \(\)-Huber corruption channel \(C_{}(P)\) such that with probability at least \(\)_

_(i) For LTC:_ \(|_{n}-(P)|(( )^{1-1/k}+(})^{1-1/k })\)_,_

_(ii) For CTL:_ \(|_{n}-(P)|(^{1-1/k}+(})^{1-1/k})\)_,_

_where recall that \(_{n}\) is a measurable function of \(\{Z_{i}\}_{i=1}^{n}\), i.e., private and corrupted view of \(i.i.d\) samples \(\{X_{i}\}_{i=1}^{n}\) from \(P_{k}\) obtained from \(\)-LDP channel \(Q\) and \(\)-Huber corruption channel \(C\)._

Proof sketch.: We provide a summary of the key steps in the proof. Essentially, we divide the proof into two parts. First, we consider the case without corruption and aim to establish the second term in the bound. To this end, we will leverage tools from information theory in an novel way, e.g., maximal coupling, strong data processing inequality of LDP, and Bretagnolle-Huber inequality between TV and KL distance. Then, we turn to give the first term related to corruption. To this end, we will leverage a folklore but important fact about Huber model. Roughly speaking, this fact says that given two inlier distributions \(D_{1}\) and \(D_{2}\) that satisfy \((D_{1},D_{2}) O()\), then after \(\)-Huber channel, one cannot distinguish between \(D_{1}\) and \(D_{2}\). Another important fact is that \(\)-LDP channel is a "contraction" channel in terms of TV distance, i.e., \((M_{1},M_{2}) O()(P_{1},P_{2})\) where \(M_{1}\), \(M_{2}\) are induced marginals of \(P_{1},P_{2}\) after any \(\)-LDP channel. 

**Key intuition behind the separation between LTC and CTL.** Building upon the above proof, one can immediately see that under the LTC setting, due to the "contraction" of LDP, one can choose two distributions that have a larger mean difference by a factor of \(1/\), while still guaranteeing that after \(\)-Huber corruption, they are indistinguishable, _hence explaining the key difference of \(1/\) between LTC and CTL._ We also provide another understanding of the separation from the attack perspective (see more details in Appendix A). The key idea here is that each single data attack in the LTC setting will lead to an additional \(1/\) factor compared to CTL setting. This is mainly because any \(\)-LDP mechanism on binary data can be simulated by random response mechanism (Kairouz et al., 2015).

We now turn to upper bounds, centering around the following key question: _Can we design a simple algorithm that can achieve optimal errors for all LTC, CTL, and even C-LDP-C in a unified way?_ We give an affirmative answer via Algorithm 1. It consists of a local randomizer at each user's side and an analyzer at the central side. The task of \(Q\) is to guarantee that its output is an \(\)-LDP view of its input. To this end, for each input \(U_{i}\), it first truncates it into \(_{i}\) using a properly chosen threshold \(M\). Then, it converts the real number to binary data via random rounding. Next, it applies random response technique to generate the final output \(_{i}\), i.e., with probability \(}{e^{}+1}\), outputs a number of the same sign (with additional scaling for unbiasedness); otherwise flips the sign. Upon receiving the final input \(\{Z_{i}\}_{i=1}^{n}\), the analyzer \(\) first simply filters out the data if it is out of the bounded range and then returns the sample mean.

For LTC and CTL, the only difference in Algorithm 1 would be the truncation value \(M\). The performance bounds for both settings under Algorithm 1 are given below. See Appendix F for proof.

**Proposition 2** (Upper Bounds).: _Given any fixed \((0,1)\), \(\), \([0,1/2)\) and \(k>1\), for any distribution \(P_{k}\) and any \(\)-Huber channel \(C_{}\), Algorithm 1 satisfies that the mechanism \(Q\) is \(\)-LDP and each returned estimate \(_{n}\) guarantees that with probability at least \(1-\)_

_(i) For LTC:_ \(|_{n}-(P)| O(() ^{1-1/k}+(})^{1- 1/k})\)_,_

_(ii) For CTL:_ \(|_{n}-(P)| O(^{1-1/k}+(})^{1-1/k})\)_,_

_where (i) holds for \(M\!=\!\{()^{1/k},(}{})^{1/k}\}\) and all \(n 3(1/)/\), if \(>0\); otherwise for all \(n\) and \(M=(}{})^{1/k}\).(ii) holds for \(M=\{()^{1/k},(}{})^{1/k}\}\) and \(n 3(1/)/\), if \(>0\); otherwise for \(n(1/)\) and \(M=(}{})^{1/k}\)._

**Corruption-LDP-Corruption (C-LDP-C).** Our tight characterization of LTC and CTL immediately helps us understand the C-LDP-C setting, where corruption happens both before and after LDP. In particular, it is easy to see that the minimax lower bound for LTC would be a valid lower bound for the more difficult C-LDP-C setting. It turns out that this lower bound is also tight since it is matched by Algorithm 1 with the same parameter choice \(M\) as in the LTC setting, see Appendix G.

```
1:Procedure:\(\)-LDP mechanism \(Q\)
2://Input:\(U_{i}\), parameters:\(M\), \(\)
3://Output: private view \(_{i}\)
4: Truncate: \(_{i}=U_{i}(|U_{i}| M)\)
5: Random rounding: \(U^{}_{i}=M&w.p._{i}/M}{2}\\ -M&w.p._{i}/M}{2}\)
6: Random response: \(_{i}=+1}{e^{}-1}U^{ }_{i}&w.p.}{e^{}+1}\\ -+1}{e^{}-1}U^{}_{i}&w.p.+1}\)
7:Return\(_{i}\)
8:Procedure: Analyzer \(\)
9://Input:\(\{Z_{i}\}_{i=1}^{n}\), parameters:\(M\), \(\)
10://Output: estimator \(_{n}\)
11:Return\(_{n}=_{i=1}^{n}Z_{i}(|Z_{i}| M +1}{e^{}-1})\)
```

**Algorithm 1** A Unified Algorithm

**How to choose parameter \(M\) in practice.** First, we note that for the bounded case (\(k=\)), \(M=1\) across all three settings, independent of other parameters. This implies that Algorithm 1 can adaptively guarantee optimal minimax rates for LTC, CTL, and C-LDP-C without prior knowledge of the specific setting and other parameter like \(\). Second, for certain applications, one may have prior knowledge of the underlying setting (see Appendix C.3). In this case, one can have a performance gain if it is under the CTL setting. Also, as mentioned above, we see that choosing the \(M\) as in LTC can automatically help to handle the C-LDP-C setting. Finally, the dependence on \(\) in \(M\) is fine since it is a known privacy parameter while the dependence on the unknown parameter \(\) is a little bit annoying. A quick practical fix is to use an estimated upper bound on \(\). In theory, the story of whether one can remove it in our case is complicated, see the discussion in Appendix C.2.

_Remark 3_ (Burn-in period).: Under Algorithm 1, when \(>0\), the concentration kicks in when the sample size \(n\) is larger than a threshold. This type of burn-in period also exists in previous concentration results under the Huber model, though in different contexts (e.g., non-private case in Chen et al. (2022) or central model of DP in Wu et al. (2023)) or with different estimators (e.g., trimmed mean in Mukherjee et al. (2021)).

_Remark 4_ (Random response vs. Laplace mechanism).: One may wonder if the standard Laplace mechanism can be applied in replace of the random response for \(\)-LDP in \(Q\). The answer depends on the setting and the analyzer \(\). For CTL, one can still derive a similar optimal concentration bound as in Proposition 2 by the concentration of Laplace noise. On the other hand, for LTC, simply replacing random response with Laplace mechanism in \(Q\) will lead to an additional \((1/)\) factor. This aligns with the fact that truncation-based estimators even cannot achieve optimal mean estimation for Gaussians under corruption (Diakonikolas and Kane, 2023). The above discussion indicates another difference between LTC and CTL, i.e., the choice of \(\)-LDP mechanisms.

As two interesting applications of our mean estimation results, we will study both online MABs and offline MABs in the next two sections, highlighting again the sharp differences between LTC and CTL settings, in terms of regret and sub-optimality performance, respectively.

## 4 Online MABs

For online MABs, our main result is the following theorem that gives an almost tight characterization (up to log factor) of its minimax clean regret (cf. Def. 5) for both LTC and CTL settings.

**Theorem 2** (Online MABs).: _Given any \(\), \([0,1/2)\) and \(k>1\), we have for all large enough \(T\),_

\[R^{*}_{,}(k,,,T) =(T( )^{1-1/k}+T^{}(})^{ }),\] \[R^{*}_{,}(k,,,T) =(T^{1-1/k}+T^{} (})^{}).\]

_Remark 5_.: For both settings, due to corruption, the minimax clean regret (i.e., problem-independent regret) has a linear dependence on \(T\), as in previous works under Huber corruption (Wu et al., 2023; Chen et al., 2022). The key here is to capture the tight factor in front of \(T\), where the additional \(1/\) factor in LTC again demonstrates the sharp difference between the two settings as in the mean estimation problem. As before, one can obtain the same rate for C-LDP-C from the LTC setting.

To prove the above theorem, we start with the corresponding lower bounds (see App. H for proof).

**Proposition 3** (Regret Lower bounds).: _Let \(\), \([0,1/2)\), \(k>1\) and \(T\) be large enough. Then, the minimax clean regrets satisfy the following results._

_(i) LTC:_ \(R^{*}_{}(k,,,T)(T()^{1-1/k}+T^{}(})^{})\)_;_

_(ii) CTL:_ \(R^{*}_{}(k,,,T)(T^{1- 1/k}+T^{}(})^{ })\)_._

**Comparisons to related work.** We first remark that Tao et al. (2022) studied a similar case but without corruption (i.e., \(=0\)) and established a lower bound on the order of \(((})^{1-1/k}T^{1/k})\) (for \(k(1,2]\) when adapted to our setting), which is weaker concerning \(T\) compared to our lower bound. In Tao et al. (2022), the authors also claimed to achieve their lower bound via some arm-elimination algorithm, which now becomes _ungrounded_ given our tighter lower bound. That is, since for a large enough \(T\), our lower bound is even larger than their upper bound for fixed \(\), \(k\) and \(K\) (e.g., \(T^{3/4}\) vs. \(\) for \(k=2\), see further discussion in Appendix C.3). Another recent work Wu et al. (2023) also studies online MABs with both privacy and Huber corruption but under the _weaker_ central model of DP. In particular, the true reward from each user may be first corrupted before being observed by the central learner, who is then responsible for taking care of privacy guarantees. That is, the central learner has access to users' raw (corrupted) data rather than only a private view of data as in our LDP case. Under this strictly weaker privacy model, Wu et al. (2023) establish the following lower bound on the minimax clean regret: \((+(K/)^{1-}\,T^{}+T^{1- })\). Compared to our CTL setting, one can see that our stronger LDP privacy incurs a larger privacy cost.

Now, let us turn to our proposed algorithm (i.e., Algorithm 2) for achieving matching regret upper bounds (up to log factor). Algorithm 2 is a variant of upper confidence bound (UCB)-based algorithm (cf. Auer et al. (2002)), which computes the UCB index for each arm at each round \(t[T]\) and then selects the one with the highest UCB, i.e., optimism in the face of uncertainty. To construct a valid UCB, we resort to our mean estimation results in the last section. In particular, we will need Algorithm 1 to compute the private and robust sample mean \(_{a,N_{a}(t)}(t)\) for each arm \(a[K]\) at each round \(t\), where \(N_{a}(t)\) be the number of pulls of arm \(i\) by the beginning of time \(t\). Then, the bonus term (i.e., radius of the confidence bound) \(_{a}(t)\) comes from the high probability mean estimation error established in Proposition 2. Note that due to burn-in period of the concentration results, Algorithm 2 has an additional exploration period to guarantee that the number of arm pulls is larger than a threshold (line 4). The following proposition formally states the regret guarantees of Algorithm 2 with the proof given in Appendix I.

**Proposition 4** (Regret Upper Bounds).: _Let \(\), \((0,1/2)\), \(k>1\) and \(T\) be large enough. Then, for any \(1/2>\), the expected clean regret of Algorithm 2 satisfies the following guarantees._

_(i) LTC:_ \(R_{}(k,,,T) O(T(} {})^{1-1/k}+(})^{ }T^{}+})\)_;_

_(ii) CTL:_ \(R_{}(k,,,T) O(T^{1-1/k}+ (})^{}T^{ }+})\)_._

**Comparisons to related work.** First, for \(=0\), our result with a direct modification of the burn-in period gives a regret bound that only has the term \(O((})^{}T^{})\). This is the first _correct_ regret bound for locally private heavy-tailed MABs, i.e., without corruption, fixing the aforementioned issue in the state-of-the-art in Tao et al. (2022) (see more discussions in Appendix C.3). Second, it is worth comparing our result to a recent similar result in Charisopoulos et al. (2023), where the authors present regret for linear bandits under LTC setting. Their result is worse than ours when reduced to MAB with bounded rewards, as the scaling with respect to \(\) is \(\) in the first linear term rather than our \(\). Another minor difference is that our algorithm is anytime while their algorithm is not.

**Other extensions.** Although we mainly focus on minimax regret (i.e., problem-independent bound) in this paper, under some conditions of corruption level and the minimum mean gap, Algorithm 2 is also able to offer some problem-dependent bounds (see Appendix I). In the case that the corruption parameter \(\) is very small but not equal to zero, one can tune the choice of \(\) (hence truncation threshold \(M\)) to balance the first and third terms in the bound. Similar comments and observations have been made in related work as in Chen et al. (2022); Wu et al. (2023).

## 5 Offline MABs

In this section, we study offline MABs as another application of our high probability mean estimation results developed in Section 3. We establish both lower bounds and almost matching upper bounds for locally private offline MABs with corruptions. To the best of our knowledge, this is the first result on offline MABs with heavy-tailed rewards, even without privacy and corruption.

**Proposition 5** (Sub-optimality Lower Bounds).: _Let \(\), \([0,1/2)\), \(k>1\) and \(N\) be large enough. Then, for \(^{} 2\), the minimax expected sub-optimality satisfies the following results._

_(i) LTC:_ \(^{*}_{}(^{},k,,,N)\!\! (()^{1-1/k}\!+\!(}{N}})^{1-1/k})\)_;_

_(ii) CTL:_ \(^{*}_{}(^{},k,,,N) (^{1-1/k}+(}{N}} )^{1-1/k})\)_;_

Now, let us turn to our proposed algorithm, which is able to achieve a matching expected sub-optimality (up to log factor) for both LTC and CTL settings. Our algorithm is a simple variant of the classic Lower Confidence Bound (LCB)-based algorithm as in Rashidinejad et al. (2021), i.e., pessimism in the offline setting. The key difference compared to Rashidinejad et al. (2021) is our new private and robust estimator (line 8) and penalty term (line 10), which come from our high probability mean estimation error. Another modification is due to our burn-in period of concentration result (line 4). Putting all of these together, Algorithm 3 is able to achieve the following guarantees on the expected sub-optimality, which almost matches the lower bound in Proposition 5. See the App. K and J for proofs of the upper and lower bounds.

**Proposition 6** (Sub-optimality Upper Bounds).: _Let \(\), \((0,1/2)\), \(k>1\) and \(=1/N\). Then, for all finite \(^{} 1\) and large enough \(N\) and \(N_{a}\). \( 3(1/)/\), the expected sub-optimality of Algorithm 3 satisfies_

_(i) LTC:_ \(_{}(^{},k,,,N)\!\!O (()^{1-1/k}\!+\!((KN)}{N}})^{1-1/k})\)_;_

_(ii) CTL:_ \(_{}(^{},k,,,N) O( ^{1-1/k}+((KN)}{ N}})^{1-1/k})\)_._

For the case of \(=0\), as before one can simply choose to use the mean estimate result for \(=0\) as shown in Proposition 2 and adjust the burn-in period accordingly. This will lead to a bound that only has the second term in the above upper bounds. For \(^{} 2\), one can observe that the upper bound of Algorithm 3 almost matches the lower bounds in Proposition 5 for both LTC and CTL settings. However, when \(^{}[1,2)\) (i.e., good coverage case), it is known that the performance of LCB is worse than imitation learning, i.e., simply returning the most frequently selected arm in the offline dataset (when there is no privacy and corruption) (Rashidinejad et al., 2021). We leave it to future work to give a tight characterization of the sub-optimality when \(^{}[1,2)\). Moreover, the proof of Proposition 6 also naturally gives us high-probability bounds without specifying \(=1/N\) in the end.

## 6 Simulations

Beyond our theoretical results, we have also conducted a set of simulations for our three problems. Our theoretical results capture the worst-case performance (i.e., minimax rates). Thus, for simulations, we are particularly interested in the following two questions: (i) _Can we simulate the worst-case scenario and test the performance of our proposed algorithms?_ and (ii) _How about their performance in non-worst-case scenarios?_ We give detailed answers to both questions for all three problems in Appendix A, which offers additional insights into the interplay between privacy and robustness.

## 7 Concluding Remarks

To conclude, we have demonstrated an interesting interplay between privacy and robustness in three problems: mean estimation, online and offline MABs. The punchline across three problems is thatcorruption after any LDP mechanism becomes easier, i.e., the same amount of corruption leads to a worse performance when compared to the case where Huber corruption happens before LDP mechanisms. We also give the first set of results for the most practical C-LDP-C setting.

Some interesting future directions include (i) improving the sub-optimal result for linear bandit in Charisopoulos et al. (2023) by following existing private linear bandits (Li et al., 2022, 2024) along with the assumption of bounded reward; (ii) generalizing it to other privacy models such as shuffle DP (Chowdhury and Zhou, 2022c); (iii) studying the case where the heavy-tailedness is characterized by the central moment rather than the raw moment currently considered in our paper; (iv) extending the results to locally private and robust reinforcement learning by building upon existing results such as Chowdhury and Zhou (2022a); Liao et al. (2023); Zhou (2022).