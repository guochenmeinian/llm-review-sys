# A Unified Framework for U-Net Design and Analysis

Christopher Williams \({}^{1}\) Fabian Falck \({}^{*,1,2}\) George Deligiannidis \({}^{1}\)

Chris Holmes \({}^{1,2}\) Arnaud Doucet \({}^{1}\) Saifuddin Syed \({}^{1}\)

\({}^{1}\)University of Oxford \({}^{2}\)The Alan Turing Institute

{williams,fabian.falck,deligian,cholmes, doucet,saifuddin.syed}@stats.ox.ac.uk

Equal contribution.

###### Abstract

U-Nets are a go-to neural architecture across numerous tasks for continuous signals on a square such as images and Partial Differential Equations (PDE), however their design and architecture is understudied. In this paper, we provide a framework for designing and analysing general U-Net architectures. We present theoretical results which characterise the role of the encoder and decoder in a U-Net, their high-resolution scaling limits and their conjugacy to ResNets via preconditioning. We propose Multi-ResNets, U-Nets with a simplified, wavelet-based encoder without learnable parameters. Further, we show how to design novel U-Net architectures which encode function constraints, natural bases, or the geometry of the data. In diffusion models, our framework enables us to identify that high-frequency information is dominated by noise exponentially faster, and show how U-Nets with average pooling exploit this. In our experiments, we demonstrate how Multi-ResNets achieve competitive and often superior performance compared to classical U-Nets in image segmentation, PDE surrogate modelling, and generative modelling with diffusion models. Our U-Net framework paves the way to study the theoretical properties of U-Nets and design natural, scalable neural architectures for a multitude of problems beyond the square.

## 1 Introduction

U-Nets (see Figure 1) are a central architecture in deep learning for continuous signals. Across many tasks as diverse as image segmentation , Partial Differential Equation (PDE) surrogate modelling  and score-based diffusion models , U-Nets are a go-to architecture yielding state-of-the-art performance. In spite of their enormous success, a framework for U-Nets which characterises for instance the specific role of the encoder and decoder in a U-Net or which spaces these operate on is lacking. In this work, we provide such a framework for U-Nets. This allows us to design U-Nets for data beyond a square domain, and enable us to incorporate prior knowledge about a problem, for instance a natural basis, functional constraints, or knowledge about its topology, into the neural architecture.

**The importance of preconditioning.** We begin by illustrating the importance of the core design principle of U-Nets: _preconditioning_. Preconditioning informally means that initialising an optimisation problem with a 'good' solution greatly benefits learning . Consider a synthetic example using ResNets  which are natural in the context of

Figure 1: A resolution 2 U-Net (Def. 1). If \(E_{i}=_{V_{i}}\), this is a Multi-ResNet (see Def. 3).

U-Nets as we will show in SS2.3: we are interested in learning a ground-truth mapping \(w:V W\) and \(w(v)=v^{2}\) over \(V=[-1,1]\) and \(W=\) using a ResNet \(R^{}(v)=R^{}(v)+R(v)\) where \(R^{},R:V W\). In Figure 2 [Left] we learn a standard ResNet with \(R^{}(v)=v\) on a grid of values from \(V W\), i.e. with inputs \(v_{i} V\) and regression labels \(w_{i}=w(v_{i})=v_{i}^{2}\). In contrast, we train a ResNet with \(R^{}(v)=|v|\) [Right] with the same number of parameters and iterations. Both networks have been purposely designed to be weakly expressive (see Appendix B.5 for details). The standard ResNet [Left] makes a poor approximation of the function, whilst the other ResNets [Right] approximation is nearly perfect. This is because \(R^{}(v)=|v|\) is a 'good' initial guess or _preconditioner_ for \(w(v)=v^{2}\), but \(R^{}(v)=v\) is a 'bad' one. This shows the importance of encoding good preconditioning into a neural network architecture and motivates us studying how preconditioning is used in U-Net design.

In this paper, we propose a mathematical framework for designing and analysing general U-Net architectures. We begin with a comprehensive definition of a U-Net which characterises its components and identifies its self-similarity structure which is established via preconditioning. Our theoretical results delineate the role of the encoder and decoder and identify the subspaces they operate on. We then focus on ResNets as natural building blocks for U-Nets that enable flexible preconditioning from lower-resolutions inputs. Our U-Net framework paves the way to designing U-Nets which can model distributions over complicated geometries beyond the square, for instance CW-complexes or manifolds, or diffusions on the sphere , without any changes to the diffusion model itself (see Appendix A). It allows us to enforce problem-specific constraints through the architecture, such as boundary conditions of a PDE or a natural basis for a problem. We also analyse why U-Nets with average pooling are a natural inductive bias in diffusion models.

More specifically, our _contributions_ are as follows: (a) We provide the first rigorous definition of U-Nets, which enables us to identify their self-similarity structure, high-resolution scaling limits, and conjugacy to ResNets via preconditioning. (b) We present Multi-ResNets, a novel class of U-Nets over a hierarchy of orthogonal wavelet spaces of \(L^{2}()\), for compact domain \(\), with no learnable parameters in its encoder. In our experiments, Multi-ResNets yield competitive and often superior results when compared to a classical U-Net in PDE modelling, image segmentation, and generative modelling with diffusion models. We further show how to encode problem-specific information into a U-Net. In particular, we design U-Nets incorporating a natural basis for a problem which enforces boundary conditions on the elliptic PDE problem, and design and demonstrate proof-of-concept experiments for U-Nets with a Haar wavelet basis over a triangular domain. (c) In the context of diffusion models, we analyse the forward process in a Haar wavelet basis and identify how high-frequency information is dominated by noise exponentially faster than lower-frequency terms. We show how U-Nets with average pooling exploit this observation, explaining their go-to usage.

## 2 U-Nets: Neural networks via subspace preconditioning

The goal of this section is to develop a mathematical framework for U-Nets which introduces the fundamental principles that underpin its architecture and enables us to design general U-Net architectures. All theoretical results are proven in Appendix A. We commence by defining the U-Net.

### Anatomy of a U-Net

**Definition 1**.: **U-Net.** Let \(V\) and \(W\) be measurable spaces. A _U-Net_\(=(,,,,,U_{0})\) comprises six components:

1. _Encoder subspaces:_\(=(V_{i})_{i=0}^{}\) are nested subsets of \(V\) such that \(_{i}V_{i}=V\).
2. _Decoder subspaces:_\(=(W_{i})_{i=0}^{}\) are nested subsets of \(W\) such that \(_{i}W_{i}=W\).
3. _Encoder operators:_\(=(E_{i})_{i=1}^{}\) where \(E_{i}:V_{i} V_{i}\) denoted \(E_{i}(v_{i})=_{i}\).
4. _Decoders operators:_\(=(D_{i})_{i=1}^{}\) where \(D_{i}:W_{i-1} V_{i} W_{i}\) at resolution \(i\) denoted \(D_{i}(w_{i-1}|v_{i})\). The \(v_{i}\) component is called the _skip connection_.

Figure 2: The importance of _preconditioning_.

5. _Projection operators:_\(=(P_{i})_{i=0}^{}\), where \(P_{i}:V V_{i}\), such that \(P_{i}(v_{i})=v_{i}\) for \(v_{i} V_{i}\).
6. _Bottleneck_: \(U_{0}\) is the mapping \(U_{0}:V_{0} W_{0}\), enabling a compressed representation of the input.

The _U-Net of resolution_\(i\) is the mapping \(U_{i}:V_{i} W_{i}\) defined through the recursion (see Figure 3):

\[U_{i}(v_{i})=D_{i}(U_{i-1}(P_{i-1}(_{i}))|_{i}), i=1,2,. \]

We illustrate the Definition of a U-Net in Figure 1. Definition 1 includes a wide array of commonly used U-Net architectures, from the seminal U-Net , through to modern adaptations used in large-scale diffusion models , operator learning U-Nets for PDE modelling , and custom designs on the sphere or graphs . Our framework also comprises models with multiple channels (for instance RGB in images) by choosing \(V_{i}\) and \(W_{i}\) for a given resolution \(i\) to be product spaces \(V_{i}=_{k=1}^{M}V_{i,k}\) and \(W_{i}=_{k=1}^{M}W_{i,k}\) for \(M\) channels when necessary2. Remarkably, despite their widespread use, to the best of our knowledge, our work presents the first formal definition of a U-Net. Definition 1 not only expands the scope of U-Nets beyond problems confined to squared domains, but also naturally incorporates problem-specific information such as a natural basis, boundary conditions or topological structure as we will show in SS3.2 and 3.3. This paves the way for designing inherently scalable neural network architectures capable of handling complicated geometries, for instance manifolds or CW-complexes. In the remainder of the section, we discuss and characterise the components of a U-Net.

Encoder and decoder subspaces.We begin with the spaces in \(V\) and \(W\) which a U-Net acts on. Current literature views U-Nets as learnable mappings between input and output tensors. In contrast, this work views U-Nets and their encoder and decoder as operators on spaces that must be chosen to suit our task. In order to perform computations in practice, we must restrict ourselves to subspaces \(V_{i}\) and \(W_{i}\) of the potentially infinite-dimensional spaces \(V\) and \(W\). For instance, if our U-Net is modelling a mapping between images, which can be viewed as bounded functions over a squared domain \(=\)3, it is convenient to choose \(V\) and \(W\) as subspaces of \(L^{2}()\), the space of square-integrable functions  (see Appendix C). Here, a data point \(w_{i}\) in the decoder space \(W_{i}\) is represented by the coefficients \(c_{i,j}\) where \(w_{i}=_{j}c_{i,j}e_{i,j}\) and \(\{e_{i,j}\}_{j}\) is a (potentially orthogonal) basis for \(W_{i}\). We will consider precisely this case in SS3.1. The projected images on the subspace \(V_{i},W_{i}\) are still functions, but piece-wise constant ones, and we store the values these functions obtain as 'pixel' tensors in our computer  (see Figure 22 in Appendix A).

Role of encoder, decoder, projection operators.In spite of their seemingly symmetric nature and in contrast to common understanding, the roles of the encoder and decoder in a U-Net are fundamentally different from each other. The decoder on resolution \(i\) learns the transition from \(W_{i-1}\) to \(W_{i}\) while incorporating information from the encoder on \(V_{i}\) via the skip connection. The encoder \(E_{i}\) can be viewed as a change of basis mapping on the input space \(V_{i}\) at resolution \(i\) and is not directly tied to the approximation the decoder makes on \(W_{i}\). This learned change of basis facilitates the decoder's approximation on \(W_{i}\). In SS3.1, we will further extend our understanding of the encoder and discuss its implications for designing U-Nets. The projection operators serve to extract a compressed input. They are selected to suit task-specific needs, such as pooling operations (e.g. average pooling, max pooling) in the context of images, or orthogonal projections if \(V\) is a Hilbert space. Note that there is no embedding operator4, the operator facilitating the transition to a higher-resolution space, explicitly defined as it is invariably the natural inclusion of \(W_{i-1}\) into \(W_{i}\).

Self-similarity of U-Nets via preconditioning.The key design principle of a U-Net is preconditioning. The U-Net of resolution \(i-1\), \(U_{i-1}\) makes an approximation on \(W_{i-1}\) which is input of and preconditions \(U_{i}\). Preconditioning facilitates the transition from \(W_{i-1}\) to \(W_{i}\) for the decoder. In the encoder, the output of \(P_{i-1}E_{i}\) is the input of \(U_{i-1}\). When our underlying geometry is refinable (such as a square), we may use a refinable set of basis functions. In the standard case of a U-Net with average pooling on a square domain, our underlying set of basis functions are (Haar) wavelets (see SS2.3) - refinable basis functions defined on a refinable geometry. This preconditioned design of

Figure 3: Recursive structure of a U-Net.

U-Nets reveals a self-similarity structure (see Figure 3) inherent to the U-Net when the underlying space has a refinable geometry. This enables both an efficient multi-resolution approximation of U-Nets  and makes them modular, as a U-Net on resolution \(i-1\) is a coarse approximation for a U-Net on resolution \(i\). We formalise this notion of preconditioning in Proposition 1 in SS2.3.

### High-resolution scaling behavior of U-Nets

Given equation (1), it is clear that the expressiveness of a U-Net \(\) is governed by the expressiveness of its decoder operators \(\). If each \(D_{i}\) is a universal approximator , then the corresponding U-Nets \(U_{i}\) likewise have this property. Assuming we can represent any mapping \(U_{i}:V_{i} W_{i}\) as a U-Net, our goal now is to comprehend the role of increasing the resolution in the design of \(\), and to discern whether any function \(U:V W\) can be represented as a high-resolution limit \(_{i}U_{i}\) of a U-Net. We will explore this question in the context of regression problems of increasing resolution.

To obtain a tractable answer, we focus on choosing \(W\) as a _Hilbert space_, that is \(W\) equipped with an inner product. This allows us to define \(\) as an increasing sequence of _orthogonal_ subspaces of \(W\). Possible candidates for orthogonal bases include certain Fourier frequencies, wavelets (of a given order) or radial basis functions. The question of which basis is optimal depends on our problem and data at hand: some problems may be hard in one basis, but easy in another. In SS4, Haar wavelets are a convenient choice. Let us define \(\) as infinite resolution data in \(V W\) and \(_{i}\) as the finite resolution projection in \(V_{i} W_{i}\) comprising of \((v_{i},w_{i})=(P_{i}(v),Q_{i}(w))\) for each \((v,w)\). Here, \(P_{i}\) is the U-Net projection onto \(V_{i}\), and \(Q_{i}:W W_{i}\) is the orthogonal projection onto \(W_{i}\). Assume \(U^{*}\) and \(U^{*}\) are solutions to the finite and infinite resolution regression problems \(_{U_{i}_{i}}_{i}(U_{i}|_{i})\) and \(_{U}(U|)\) respectively, where \(_{i}\) and \(\) represent the sets of measurable functions mapping \(V_{i} W_{i}\) and \(V W\). Let \(_{i}\) and \(\) denote the \(L^{2}\) losses:

\[_{i}(U_{i}|_{i})^{2}_{i}|} _{(w_{i},v_{i})_{i}} w_{i}-U_{i}(v_{i})^{2}, (U|)^{2}|}_{(w,v )} w-U(v)^{2}.\]

The following result analyses the relationship between \(U^{*}_{i}\) and \(U^{*}\) as \(i\) where \(_{i|j}\) and \(_{|j}\) are the losses above conditioned on resolution \(j\) (see Appendix A).

**Theorem 1**.: _Suppose \(U^{*}_{i}\) and \(U^{*}\) are solutions of the \(L^{2}\) regression problems above. Then, \(_{i|j}(U^{*}_{i})_{|j}(U^{*})\) with equality as \(i\). Further, if \(Q_{i}U^{*}\) is \(V_{i}\)-measurable, then \(U^{*}_{i}=Q_{i}U^{*}\) minimises \(_{i}\)._

Theorem 1 states that solutions of the finite resolution regression problem converge to solutions of the infinite resolution problem. It also informs us how to choose \(V_{i}\) relative to \(W_{i}\). If we have a \(\) where for the decoders on \(\), the \(W_{i}\) component of \(U^{*}\), \(Q_{i}U^{*}\), relies solely on the input up to resolution \(i\), then the prediction from the infinite-dimensional U-Net projected to \(W_{i}\) can be made by the U-Net of resolution \(i\). The optimal choice of \(V_{i}\) must be expressive enough to encode the information necessary to learn the \(W_{i}\) component of \(U^{*}\). This suggests that if \(V_{i}\) lacks expressiveness, we risk efficiency in learning the optimal value of \(U^{*}_{i}\). However, if \(V_{i}\) is too expressive, no additional information is gained, and we waste computational effort. Therefore, we should choose \(V_{i}\) to encode the necessary information for learning information on resolution \(i\). For example, when modelling images, if we are interested in low-resolution features on \(W_{i}\), high-resolution information is extraneous, as we will further explore in SS4 in the context of diffusion models.

### U-Nets are conjugate to ResNets

Next, our goal is to understand why ResNets are a natural choice in U-Nets. We will uncover a conjugacy between U-Nets and ResNets. We begin by formalising ResNets in the context of U-Nets.

**Definition 2**.: **ResNet, Residual U-Nets.** Given a measurable space \(X\) and a vector space \(Y\), a mapping \(R:X Y\) is defined as a _ResNet_ preconditioned on \(R^{}:X Y\) if \(R(x)=R^{}(x)+R^{}(x)\), where \(R^{}(x)=R(x)-R^{}(x)\) is the _residual_ of \(R\). A _Residual U-Net_ is a U-Net \(\) where \(,\) are sequences of vector spaces, and the encoder and decoder operators \(,\) are ResNets preconditioned on \(E^{}_{i}(v_{i})=v_{i}\) and \(D^{}_{i}(w_{i-1}|v_{i})=w_{i-1}\), respectively.

A preconditioner initialises a ResNet, then the ResNet learns the residual relative to it. The difficulty of training a ResNet scales with the deviation from the preconditioner, as we saw in our synthetic experiment in SS1. In U-Nets, ResNets commonly serve as encoder and decoder operators. In encoders,preconditioning on the identity on \(V_{i}\) allow the residual encoder \(E_{i}^{}\) to learn a change of basis for \(V_{i}\), which we will discuss in more detail in SS3.1. In decoders, preconditioning on the identity on the lower resolution subspace \(W_{i-1}\) allows the residual decoder \(D_{i}^{}\) to learn from the lower resolution and the skip connection. Importantly, ResNets can compose to form new ResNets, which, combined with the recursion (1), implies that residual U-Nets are conjugate to ResNets.

**Proposition 1**.: _If \(\) is a residual U-Net, then \(U_{i}\) is a ResNet preconditioned on \(U_{i}^{}(v_{i})=U_{i-1}(_{i-1})\), where \(_{i-1}=P_{i-1}(E_{i}(v_{i}))\)._

Proposition 1 states that a U-Net at resolution \(i\) is a ResNet preconditioned on a U-Net of lower resolution. This suggests that \(U_{i}^{}\) learns the information arising from the resolution increase. We will discuss the specific case \(E_{i}=_{V_{i}}\), and \(U^{}(v_{i})=U_{i-1}(P_{i-1}(v_{i}))\) in SS3.1. Proposition 1 also enables us to interpret \(_{i}U_{i}\) from SS2.2 as a ResNet's 'high-resolution' scaling limit. This is a new scaling regime for ResNets, different to time scaled Neural ODEs , and warrants further exploration in future work. Finally, we provide an example of the common _Residual U-Net_.

**Example 1**.: **Haar Wavelet Residual U-Net.** A _Haar wavelet residual U-Net \(\)_ is a residual U-Net where: \(V=W=L^{2}()\), \(V_{i}=W_{i}\) are multi-resolution Haar wavelet spaces (see Appendix C.3), and \(P_{i}=_{V_{i}}\) is the orthogonal projection.

We design Example 1 with images in mind, noting that similar data over a squared domain such as PDE (see SS5) are also applicable to this architecture. We hence choose Haar wavelet  subspaces of \(L^{2}()\), the space of square integrable functions and a Hilbert space, and use average pooling as the projection operation \(P_{i}\). Haar wavelets will in particular be useful to analyse why U-Nets are a good inductive bias in diffusion models (see SS4). U-Net design and their connection to wavelets has also been studied in [24; 25; 26].

## 3 Generalised U-Net design

In this section, we provide examples of different problems for which our framework can define a natural U-Net architecture. Inspired by Galerkin subspace methods , our goal is to use our framework to generalise the design of U-Nets beyond images over a square. Our framework also enables us to encode problem-specific information into the U-Net, such as a natural basis or boundary conditions, which it no longer needs to learn from data, making the U-Net model more efficient.

### Multi-ResNets

**A closer look at the U-Net encoder.** To characterise a U-Net in Definition 1, we must in particular choose the encoder subspaces \(\). This choice depends on our problem at hand: for instance, if the inputs are images, choosing Haar wavelet subspaces is most likely favourable, because we can represent and compress images in a Haar wavelet basis well, noting that more complex (orthogonal) wavelet bases are possible [28; 29]. What if we choose \(\) unfavourably? This is where the encoder comes in. While the encoder subspaces \(\) define an initial basis for our problem, the encoder learns a _change of basis_ map to a new, implicit basis \(_{i}=E_{i}(v_{i})\) which is more favourable. This immediately follows from Eq. (1) since \(U_{i}\) acts on \(V_{i}\) through \(_{i}\). The initial subspaces \(\) can hence be viewed as a prior for the input compression task which the encoder performs.

Given our initial choice of the encoder subspaces \(\), the question whether and how much work the encoders \(\) have to do depends on how far away our choice is from the optimal choice \(}\) for our problem. This explains why the encoders \(E_{i}\) are commonly chosen to be ResNets preconditioned on the identity \(E_{i}^{}=_{V_{i}}\), allowing the residual encoder \(E_{i}^{}\) to learn a change of basis. If we had chosen the optimal sequence of encoder subspaces, the residual operator would not have to do any work; leaving the encoder equal to the precondition \(E_{i}=_{V_{i}}\). It also explains why in practice, encoders are in some cases chosen significantly smaller than the decoder , as a ResNet encoder need not do much work given a good initial choice of \(\). It is precisely this intuition which motivates our second example of a U-Net, the Multi-Res(olution) Res(idual) Network (_Multi-ResNet_).

**Definition 3**.: **Multi-ResNets.** A Multi-ResNet is a residual U-Net with encoder \(E_{i}=_{V_{i}}\).

**Example 2**.: **Haar Wavelet Multi-ResNets.** A Haar Wavelet Multi-ResNet is a Haar Wavelet Residual U-Net with encoder \(E_{i}=_{V_{i}}\).

We illustrate the Multi-ResNet, a novel U-Net architecture, in Figure 1 where we choose \(E_{i}=_{V_{i}}\). Practically speaking, the Multi-ResNet simplifies its encoder to have no learnable parameters, and simply projects to \(V_{i}\) on resolution \(i\). The latter can for the example of Haar wavelets be realised by computing a multi-level Discrete Wavelet Transform (DWT) (or equivalently average pooling) over the input data . Multi-ResNets allow us to save the parameters in the encoder, and instead direct them to bolster the decoder. In our experiments in SS5.1, we compare Multi-ResNets to Residual U-Nets and find that for PDE surrogate modelling and image segmentation, Multi-ResNets yield superior performance to Residual U-Nets as Haar wavelets are apparently a good choice for \(\), while for other problems, choosing Haar wavelets is suboptimal. Future work should hence investigate how to optimally choose \(\) for a problem at hand. To this end, we will discuss natural bases for \(\) and \(\) for specific problems in the remainder of this section.

### U-Nets which guarantee boundary conditions

Next, our main goal is to show how to design U-Nets which choose \(\) in order to encode constraints on the output space directly into the U-Net architecture. This renders the U-Net more efficient as it no longer needs to learn the constraints from data. We consider an example from _PDE surrogate modelling_, approximating solutions to PDE using neural networks, a nascent research direction where U-Nets already play an important role , where our constraints are given boundary conditions and the solution space of our PDE. In the _elliptic boundary value problem_ on \(=\), the task is to predict a weak (see Appendix A) PDE solution \(u\) from its forcing term \(f\) given by

\[ u=f, u(0)=u(1)=0, \]

where \(u\) is once weakly differentiable when the equation is viewed in its weak form, \(f L^{2}()\) and \( u\) is the Laplacian of \(u\). In contrast to Examples 1 and 2, we choose the decoder spaces as subspaces of \(W=^{1}_{0}\), the space of one weakly differentiable functions with nullified boundary condition, a Hilbert space (see Appendix A), and choose \(V=L^{2}()\), the space of square integrable functions. This choice ensures that input and output functions of our U-Net are in the correct function class for the prescribed problem. We now want to choose a basis to construct the subspaces \(\) and \(\) of \(V\) and \(W\). For \(\), just like in Multi-ResNets in Example 2, we choose \(V_{j}\) to be the Haar wavelet space of resolution \(j\), an orthogonal basis. For \(\), we also choose a refinable basis, but one which is natural to \(^{1}_{0}\). In particular, we choose \(W_{i}=\{_{k,j}:j k,k=1,,2^{i-1}\}\) where

\[_{k,j}(x)=(2^{k}x+j/2^{k}),(x)=2x 1_{[0,1/2)}(x)+(2-2 x) 1_{[1/2,1]}(x). \]

This constructs an orthogonal basis of \(^{1}_{0}\), illustrated in Figure 4, which emulates our design choice in Section 3.1 where the orthogonal Haar wavelet basis was beneficial as \(W\) was \(L^{2}\)-valued. Each \(_{k,j}\) obeys the regularity and boundary conditions of our PDE, and consequently, an approximate solution from our U-Net obeys these functional constraints as well.

These constraints are encoded into the U-Net architecture and hence need not be learned from data. This generalised U-Net design paves the way to broaden the application of U-Nets, analogous to the choice of bases for Finite Element  or Discontinuous Galerkin methods .

Figure 4: Refinement of an orthogonal basis for \(^{1}_{0}=\{_{0,0},_{1,0},_{1,1}\}\). We visualise the graphs of basis functions defined in (3): [Left] \(_{0,0}=\), [Top Right] \(_{1,0}\), and [Bottom Right] \(_{1,1}\). When increasing resolution, steeper triangular-shaped basis functions are constructed.

### U-Nets for complicated geometries

Our framework further allows us to design U-Nets which encode the geometry of the input space right into the architecture. This no longer requires to learn the geometric structure from data and enables U-Nets for complicated geometries. In particular, we are motivated by _tessellations_, the partitioning of a surface into smaller shapes, which play a vital role in modelling complicated geometries across a wide range of engineering disciplines . We here focus on U-Nets on a triangle due to the ubiquitous use of triangulations, for instance in CAD models or simulations, but note that our design principles can be applied to other shapes featuring a self-similarity property. We again are interested in finding a natural basis for this geometry, and characterise key components of our U-Net.

In this example, neither \(W\) nor \(V\) are selected to be \(L^{2}()\) valued on the unit square (or rectangle) \(\). Instead, in contrast to classical U-Net design in literature, \(W=V=L^{2}()\), where \(\) is a right-triangular domain illustrated in Figure 5 [Left]. Note that this right triangle has a self-similarity structure in that it can be constructed from four smaller right triangles, continuing recursively. A refinable Haar wavelet basis for this space can be constructed by starting from \(_{i,0}=_{}-_{}\) for \(j=0\) as illustrated in Figure 5 [Left]. This basis can be refined through its self-similarity structure to define each subspace from these basis functions via \(W_{i}=V_{i}=\{_{k,j}\,:\,j k,\ k=1,,2^{i-1}\}\) (see Appendix A for details). In SS5.2, we investigate this U-Net design experimentally. In Appendix A we sketch out how developing our U-Net on triangulated manifolds enables score-based diffusion models on a sphere  without any adjustments to the diffusion process itself. This approach can be extended to complicated geometries such as manifolds or CW-complexes as illustrated in Figure 5 [Right].

## 4 Why U-Nets are a useful inductive bias in diffusion models

U-Nets are the go-to neural architecture for diffusion models particularly on image data, as demonstrated in an abundance of previous work [8; 9; 10; 11; 12; 38; 39; 40; 41]. However, the reason why U-Nets are particularly effective in the context of diffusion models is understudied. Our U-Net framework enables us to analyse this question. We focus on U-Nets over nested Haar wavelet subspaces \(=\) that increase to \(V=W=L^{2}()\), with orthogonal projection \(Q_{j}:W W_{i}\) on to \(W_{i}\) corresponding to an average pooling operation \(Q_{i}\) (see Appendix C). U-Nets with average pooling are a common choice for diffusion models in practice, for instance when modelling images [8; 9; 38; 39; 40]. We provide theoretical results which identify that high-frequencies in a forward diffusion process are dominated by noise exponentially faster, and how U-Nets with average pooling exploit this in their design.

Let \(X W\) be an infinite resolution image. For each resolution \(i\) define the image \(X_{i}=Q_{i}X W_{i}\) on \(2^{i}\) pixels which can be described by \(X_{i}=_{k}X_{i}^{(k)}_{k}\), where \(=\{_{k}:k=1,,2^{i}\}\) is the standard (or 'pixel') basis. The image \(X_{i}\) is a projection of the infinite resolution image \(X\) to the finite resolution \(i\). We consider the family of denoising processes \(\{X_{i}(t)\}_{i=1}^{}\), where for resolution \(i\), the process \(X_{i}(t)=_{k}X_{i}^{(k)}(t)_{k} W_{i}\) is initialised at \(X_{i}\) and evolves according to the denoising diffusion forward process (DDPM, ) at each pixel \(X_{i}^{(k)}(t)}X_{i}^{(k)}+} ^{(k)}\) for standard Gaussian noise \(^{(k)}\). We now provide our main result (see Appendix A for technical details).

**Theorem 2**.: _For time \(t 0\) and \(j i\), \(Q_{i}X_{j}(t)}{{=}}X_{i}(t)\). Furthermore if \(X_{i}(t)=_{j=0}^{i}^{(j)}(t)_{j}\), be the decomposition of \(X_{i}(t)\) in its Haar wavelet frequencies (see Appendix C). Each component \(^{(j)}(t)\) of the vector has variance \(2^{j-1}\) relative to the variance of the base Haar wavelet frequency._

Figure 5: U-Nets encoding the topological structure of a problem. [Left] A refinable Haar wavelet basis with basis functions on a right triangle, \(_{i,j=0}=_{}-_{}\). [Right] A sphere and a Möbius strip meshed with a Delaunay triangulation [35; 36]. Figures and code as modified from .

Theorem 2 analyses the noising effect of a forward diffusion process in a Haar wavelet basis. It states that the noise introduced by the forward diffusion process is more prominent in the higher-frequency wavelet coefficients (large \(k\)), whereas the lower-frequency coefficients (small \(k\)) preserves the signal. Optimal recovery of the signal in such scenario has been investigated in , where soft thresholding of the wavelet coefficients provides a good \(L^{2}\) estimator of the signal and separates this from the data. In other words, if we add i.i.d. noise to an image, we are noising the higher frequencies faster than the lower frequencies. In particular, high frequencies are dominated by noise _exponentially_ faster. It also states that the noising effect of our diffusion on resolution \(i\), compared to the variance of the base frequency \(i=0\), blows up the higher-frequency details as \(i\) for any positive diffusion time.

We postulate that U-Nets with average pooling exploit precisely this observation. Recall that the primary objective of a U-Net in denoising diffusion models is to separate the signal from the noise which allows reversing the noising process. In the 'learning noise' or \(\) recovery regime, the network primarily distinguishes the noise from the signal in the input. Yet, our analysis remains relevant, as it fundamentally pertains to the signal-to-noise ratio. Through average pooling, the U-Net discards those higher-frequency subspaces which are dominated by noise, because average pooling is conjugate to projection in a Haar wavelet basis [20, Theorem 2]. This inductive bias enables the encoder and decoder networks to focus on the signal on a low enough frequency which is not dominated by noise. As the subspaces are coupled via preconditioning, the U-Net can learn the signal which is no longer dominated by noise, added on each new subspace. This renders U-Nets a computationally efficient choice in diffusion models and explains their ubiquitous use in this field.

## 5 Experiments

We conduct three main experimental analyses: (A) Multi-ResNets which feature an encoder with no learnable parameters as an alternative to classical Residual U-Nets, (B) Multi-resolution training and sampling, (C) U-Nets encoding the topological structure of triangular data. We refer to Appendix B.4 for our Ablation Studies, where a key result is that U-Nets crucially benefit from the skip connections, hence the encoder is successful and important in compressing information. We also analyse the multi-resolution structure in U-Nets, and investigate different orthogonal wavelet bases. These analyses are supported by experiments on three tasks: (1) Generative modelling of images with diffusion models, (2) PDE Modelling, and (3) Image segmentation. We choose these tasks as U-Nets are a go-to and competitive architecture for them. We report the following performance metrics with mean and standard deviation over three random seeds on the test set: FID score  for (1), rollout mean-squared error (r-MSE)  for (2), and the Sorensen-Dice coefficient (Dice)  for (3). As datasets, we use MNIST , a custom triangular version of MNIST (MNIST-Triangular) and CIFAR10 for (1), Navier-stokes and Shallow water equations  for (2), and the MICCAI 2017 White Matter Hyperintensity (WMH) segmentation challenge dataset  for (3). We provide our PyTorch code base at [https://github.com/FabianFalck/unet-design](https://github.com/FabianFalck/unet-design). We refer to Appendices B, and D for details on experiments, further experimental results, the datasets, and computational resources used.

Figure 6: PDE modelling and image segmentation with a Multi-ResNet. [Left,Middle] Rolled out PDE trajectories (ground-truth, prediction, \(L^{2}\)-error) from the Navier-Stokes [Left], and the Shallow Water equation [Middle]. Figure and code as modified from [6, Figure 1]. [Right] MRI images from WMH with overlayed ground-truth (green) and prediction (blue) mask.

### The role of the encoder in a U-Net

In SS3.1 we motivated Multi-ResNets, Residual U-Nets with identity operators as encoders over Haar wavelet subspaces \(=\) of \(V=W=L^{2}()\). We analysed the role of the encoder as learning a change of basis map and found that it does not need to do any work, if \(\), the initial basis, is chosen optimally for the problem at hand. Here, we put exactly this hypothesis derived from our theory to a test. We compare classical (Haar wavelet) Residual U-Nets with a (Haar wavelet) Multi-ResNet. In Table 1, we present our results quantified on trajectories from the Navier-stokes and Shallow water PDE equations unrolled over several time steps and image segmentation as illustrated in Figure 6. Our results show that Multi-ResNets have competitive and sometimes superior performance when compared to a classical U-Net with roughly the same number of parameters. Multi-ResNets outperform classical U-Nets by \(29.8\%\), \(12.8\%\) and \(3.4\%\) on average over three random seeds, respectively. In Appendix B.1, we also show that U-Nets outperform FNO , another competitive architecture for PDE modelling, in this experimental setting.

For the practitioner, this is a rather surprising result. We can simplify classical U-Nets by replacing their parameterised encoder with a fixed, carefully chosen hierarchy of linear transformation as projection operators \(P_{i}\), here a multi-level Discrete Wavelet Transform (DWT) using Haar wavelets, and identity operators for \(E_{i}\). This 'DWT encoder' has no learnable parameters and comes at almost no computational cost. We then add the parameters we save into the decoder and achieve competitive and--on certain problems--strictly better performance when compared with a classical U-Net. However, as we show for generative modelling with diffusion models in Appendix B.1, Multi-ResNets are competitive with, yet inferior to Residual U-Nets, because the initial basis which \(\) imposes is suboptimal and the encoder would benefit from learning a better basis. This demonstrates the strength of our framework in understanding the role of the encoder and when it is useful to parameterise it, which depends on our problem at hand. It is now obvious that future empirical work should explore how to choose \(\) (and \(\)) optimally, possibly eliminating the need of a parameterised encoder, and also carefully explore how to optimally allocate and make use of the (saved) parameters in Multi-ResNets .

  
**Dataset** & **Neural architecture** & **\# Params.** & **r-MSE \(\) / Dice**\(\) \\   & Residual U-Net & 34.5 M & \(0.0057 2 10^{-5}\) \\  & Multi-ResNet, no params. added in dec. _(ours)_ & 15.7 M & \(0.0107 9 10^{-5}\) \\  & Multi-ResNet, saved params. added in dec. _(ours)_ & 34.5 M & \(}\) \\   & Residual U-Net & 34.5 M & \(0.1712 0.0005\) \\  & Multi-ResNet, no params. added in dec. _(ours)_ & 15.7 M & \(0.4899 0.0156\) \\  & Multi-ResNet, saved params. added in dec. _(ours)_ & 34.5 M & \(\) \\   & Residual U-Net & 2.2 M & \(0.8069 0.0234\) \\  & Multi-ResNet, no params. added in dec. _(ours)_ & 1.0 M & \(0.8190 0.0047\) \\   & Multi-ResNet, saved params. added in dec. _(ours)_ & 2.2 M & \(\) \\   

Table 1: Quantitative performance of the (Haar wavelet) Multi-ResNet compared to a classical (Haar wavelet) Residual U-Net on two PDE modelling and an image segmentation task.

Figure 7: Preconditioning enables multi-resolution training and sampling of diffusion models.

### Staged training enables multi-resolution training and inference

Having characterised the self-similarity structure of U-Nets in SS2, a natural idea is to explicitly train the U-Net \(\) on resolution \(i-1\) first, then train the U-Net on resolution \(i\) while preconditioning on \(U_{i-1}\), continuing recursively. Optionally, we can freeze the weights of \(_{i-1}\) upon training on resolution \(i-1\). We formalise this idea in Algorithm 1. Algorithm 1 enables training and inference of U-Nets on multiple resolutions, for instance when several datasets are available. It has two additional advantages. First, it makes the U-Net modular with respect to data. When data on a higher-resolution is available, we can reuse our U-Net pretrained on a lower-resolution in a principled way. Second, it enables to checkpoint the model during prototyping and experimenting with the model as we can see low-resolution outputs of our U-Net early.

In Figure 7 we illustrate samples from a DDPM diffusion model  with a Residual U-Net trained with Algorithm 1. We use images and noise targets on multiple resolutions (CIFAR10/MNIST: \(\{4 4,8 8,16 16,32 32\}\)) as inputs during each training stage. We observe high-fidelity samples on multiple resolutions at the end of each training stage, demonstrating how a U-Net trained via Algorithm 1 can utilise the data available on four different resolutions. It is also worth noting that training with Algorithm 1 as opposed to single-stage training (as is standard practice) does not substantially harm performance of the highest-resolution samples: (FID on CIFAR10: staged training: \(8.33 0.010\); non-staged training: \(7.858 0.250\)). We present results on MNIST, Navier-Stokes and Shallow water, with Multi-ResNets, and with a strict version of Algorithm 1 where we freeze \(E_{i}^{}\) and \(D_{i}^{}\) after training on resolution \(i\) in Appendix B.2.

### U-Nets encoding topological structure

In SS3.3, we showed how to design U-Nets with a natural basis on a triangular domain, which encodes the topological structure of a problem into its architecture. Here, we provide proof-of-concept results for this U-Net design. In Figure 8, we illustrate samples from a DDPM diffusion model  with a U-Net where we choose \(\) and \(\) as Haar wavelet subspaces of \(W=V=L^{2}()\) (see SS3.3), ResNet encoders and decoders and average pooling. The model was trained on MNIST-Triangular, a custom version of MNIST with the digit and support over a right-angled triangle. While we observe qualitatively correct samples from this dataset, we note that these are obtained with no hyperparameter tuning to improve their fidelity. This experiment has a potentially large scope as it paves the way to designing natural U-Net architectures on tessellations of complicated geometries such as spheres, manifolds, fractals, or CW-complexes.

## 6 Conclusion

We provided a framework for designing and analysing U-Nets. Our work has several limitations: We put particular emphasis on Hilbert spaces as the decoder spaces. We focus on orthogonal wavelet bases, in particular of \(L^{2}()\) or \(L^{2}()\), while other bases could be explored (e.g. Fourier frequencies, radial basis functions). Our framework is motivated by subspace preconditioning, with requires the user to actively design and choose which subspaces they wish to precondition on. Our analysis of signal and noise concentration in Theorem 2 has been conducted for a particular, yet common choice of denoising diffusion model, with one channel, and with functions supported on one spatial dimension only, but can be straight-forwardly extended with the use of a Kronecker product. Little to no tuning is performed how to allocate the saved parameters in Multi-ResNet in SS5.1. We design and empirically demonstrate U-Nets on triangles, while one could choose a multitude of other topological structures. Lastly, future work should investigate optimal choices of \(\) for domain-specific problems.

Figure 8: U-Nets encode the geometric structure of data.