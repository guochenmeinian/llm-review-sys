# Advancing Open-Set Domain Generalization Using Evidential Bi-Level Hardest Domain Scheduler

Kunyu Peng\({}^{1}\), Di Wen\({}^{1}\), Kailun Yang\({}^{2}\), Ao Luo\({}^{3}\), Yufan Chen\({}^{1}\), Jia Fu\({}^{4,5}\),

M. Saquib Sarfraz\({}^{1,6}\), Alina Roitberg\({}^{7}\), Rainer Stiefelhagen\({}^{1}\)

\({}^{1}\)Karlsruhe Institute of Technology \({}^{2}\)Hunan University \({}^{3}\)Waseda University

\({}^{4}\)KTH Royal Institute of Technology \({}^{5}\)RISE Research Institutes of Sweden

\({}^{6}\)Mercedes-Benz Tech Innovation \({}^{7}\)University of Stuttgart

Correspondence: kailun.yang@hnu.edu.cn

###### Abstract

In Open-Set Domain Generalization (OSDG), the model is exposed to both new variations of data appearance (domains) and open-set conditions, where both known and novel categories are present at test time. The challenges of this task arise from the dual need to generalize across diverse domains and accurately quantify category novelty, which is critical for applications in dynamic environments. Recently, meta-learning techniques have demonstrated superior results in OSDG, effectively orchestrating the meta-train and -test tasks by employing varied random categories and predefined domain partition strategies. These approaches prioritize a well-designed training schedule over traditional methods that focus primarily on data augmentation and the enhancement of discriminative feature learning. The prevailing meta-learning models in OSDG typically utilize a predefined sequential domain scheduler to structure data partitions. However, a crucial aspect that remains inadequately explored is the influence brought by strategies of domain schedulers during training. In this paper, we observe that an adaptive domain scheduler benefits more in OSDG compared with prefixed sequential and random domain schedulers. We propose the **E**vidential **Bi-**L**evel **H**ardest **D**omain **S**cheduler (EBiL-HaDS) to achieve an adaptive domain scheduler. This method strategically sequences domains by assessing their reliabilities in utilizing a follower network, trained with confidence scores learned in an evidential manner, regularized by max rebiasing discrepancy, and optimized in a bi-level manner. We verify our approach on three OSDG benchmarks, _i.e._, PACS, DigitsDG, and OfficeHome. The results show that our method substantially improves OSDG performance and achieves more discriminative embeddings for both the seen and unseen categories, underscoring the advantage of a judicious domain scheduler for the generalizability to unseen domains and unseen categories. The source code is publicly available at [https://github.com/KPeng9510/EBiL-HaDS](https://github.com/KPeng9510/EBiL-HaDS).

## 1 Introduction

Open-Set Domain Generalization (OSDG) is a challenging task where the model is exposed to both: domain shift and category shift. Recent OSDG works often take a meta-learning approach  which simulates different cross-domain learning tasks during training. These methods conventionally use a _predefined_ sequential domain scheduler to create meta-train and meta-test domains within each minibatch. But is fixing the meta-learning domain schedule a priori the best way to go? As a step to explore this, our work investigates the new idea of _adaptive domain scheduler_, which dynamically adjusts the training order based on ongoing model performance and domain difficulty.

OSDG is critical for many real-world applications with changing conditions, ranging from healthcare  and security  to autonomous driving . Despite the remarkable success of deep learning, the recognition quality often deteriorates when facing out-of-distribution samples. This problem is amplified in OSDG settings, where the model faces a dual challenge of identifying and rejecting unseen categories, _e.g._, by delivering low confidence score in such cases [27; 46] while simultaneously generalizing well to unseen data appearances (domain shift). Historically, research efforts in OSDG have predominantly focused on the latter, developing methods to adapt models to varying domain conditions. Strategies to improve domain generalization include the use of Generative Adversarial Networks (GANs) , contrastive learning , and metric learning . MLDG  for the first time proposed to use meta-learning to handle OSDG tasks. However, all these works follow the OSDG protocols where different source domains preserve different distributions of known categories, which diverges the domain gaps for different categories. This divergence makes the challenge more inclined towards the domain generalization aspect. Wang _et al._ revised these benchmarks for OSDG, standardizing the category distribution across source domains to achieve a more balanced evaluation of both domain generalization and open set recognition challenges. This revision includes established open-set recognition methods such as Adversarial Reciprocal Points Learning (ARPL) . Recently, Wang _et al._ introduced an effective meta-learning approach named MEDIC, featuring a binary classification and a _predefined_ sequential domain scheduler for the data partition during meta-train and -test stages.

However, these existing meta-learning-based OSDG approaches, _i.e._, MEIDC  and MLDG , do not consider how the order in which domains are presented during training affects model generalization. We believe this overlooks the potential to dynamically adapt the domain scheduler used for data partition based on certain criteria, such as domain difficulty, which could result in a more targeted training strategy and, therefore, better outcomes. In this paper, we observe that different ordering strategies for domain presentation used for data partition during the meta-training and testing phases lead to significant variations in OSDG performance, emphasizing the critical role of domain scheduling in optimizing model generalization.

To bridge this gap, we introduce a new training strategy named the **E**vidential **Bi-**L**evel **H**ardest **D**omain **S**cheduler (EBiL-HaDS), which allows _dynamically adjusting the order of domain presentation_ during data partitioning in the meta-training and -testing phases. The key idea of our method is to quantify domain reliability, defined as the aggregated confidence of the model on the samples across unseen domains, which will then be used as the main criterion for the data partition. To assess the domain reliability, we incorporate a secondary follower network to assess the domain reliability alongside the primary network. This allows for prioritizing the optimization of meta-learning on less reliable domains, facilitating an adaptive domain scheduler-based data partitioning. This follower network is trained using bi-level optimization, which involves a hierarchical setup where the solution to a lower-level optimization problem (evaluating domain reliability) serves as a constraint in an upper-level problem (meta-learning objective). Optimization of the follower network is guided by confidence scores generated through our proposed max rebiased evidential learning method, which adjusts the confidence by amplifying the differences between the decision boundaries of different classes. As a result, the follower network can better quantify the reliability of each domain based on how distinct and consistent the classification boundaries are, improving the ability to generalize to unseen domains. EBiL-HaDS enhances cross-domain generalizability and differentiation of seen and unseen classes by prioritizing training on less reliable domains through adaptive domain scheduling.

Our experiments demonstrate the effectiveness of domain scheduling via EBiL-HaDS on three established datasets: PACS , DigitsDG , and OfficeHome , which span a variety of image classification tasks. Results demonstrate that EBiL-HaDS significantly improves model generalizability in open-set scenarios, enhancing domain generalization and the model's ability to distinguish between known and unknown categories in new domains. This performance surpasses that of both random and standard sequential domain scheduling methods during training, underscoring EBiL-HaDS's potential to advance current OSDG capabilities in deep learning.

## 2 Related Work

We simultaneously address two challenges: domain generalization and open-set recognition. Domain generalization is a task that expects a model to generalize well to unseen domains while leveragingmultiple seen domains for training [48; 52]. Open-set recognition, on the other hand, aims to reject unseen categories at test-time, _e.g._, by delivering low confidence scores in such cases .

Domain generalization methods usually alleviate the domain gap with techniques such as data augmentation [55; 40; 65; 18; 64; 34; 35], contrastive learning [57; 24; 28; 43], domain adversarial learning , domain-specific normalization , and GANs-based methods [10; 32]. For open-set recognition, common approaches include logits calibration [3; 42], evidential learning [59; 53; 62; 2], reconstruction-based approaches [58; 22], GANs-based methods , and reciprocal point-based approaches [8; 9].

A considerable cluster of research utilizes source domains that encompass diverse categories for training, as highlighted in [14; 47; 4; 8; 36; 61], where each category poses unique domain generalization challenges. The primary focus of these methodologies is to improve domain generalizability, and they tend to allocate less attention to the complexities associated with open-set scenarios. In this setting, the categories involved in each source domain may not be the same. ODG-Net proposed by Bose _et al._ leverages GAN to synthesize data from the merged training domains to improve cross-domain generalizability. SWAD proposed by Chen _et al._ uses models averaged across various training epochs. Katsumata _et al._ propose to use metric learning to get discriminative embedding space which benefits the open-set domain generalization. Wang _et al._ introduce a new MEDIC model together with a new formalization of the open-set domain generalization protocols, where the source domains share the same categories defined as seen. This benchmark definition balances the impact of the model's open-set recognition and domain generalization performance in evaluation, which is adopted in our work.

Newer OSDG approaches show great promise of meta-learning strategies for improving cross-domain generalization [54; 46]. Yet, these works mainly utilize a fixed, sequential scheduling of source domains during training [54; 46]. Existing works in curriculum learning indicate that using a specific training order at the instance level can benefit the model performance on various tasks [56; 26; 17; 37; 20; 41; 28; 45]. However, existing curriculum learning approaches usually operate at the instance level (scheduling individual dataset instances within standard training) and are not designed for OSDG tasks, while we focus on domain-based scheduling by quantifying the domain difficulty in meta-learning. The influence of domain scheduling in the OSDG task remains unexplored. This paper, for the first time, examines the effects of guiding the meta-learning process with an _adaptive domain scheduler_, named EBiL-HaDS, which achieves data partition based on a domain reliability measure estimated by a follower network, trained in a bi-level manner with the supervision from the confidence score optimized by a novel max rebiased discrepancy evidential learning.

## 3 Method

The most challenging domain is chosen to perform data partitioning for the meta-task reservation during meta-learning. In our proposed EBiL-HaDS, we first utilize max rebiased discrepancy evidential learning (Sec. 3.1) to achieve more reliable confidence acquisition, which is subsequently used as the supervision for the reliability prediction of the follower network. During the training stage, our method makes use of two networks with identical architectures: one serves as the main feature extraction network, and the other functions as the follower network, aiming to assess the domain reliability. The follower network is optimized in a bi-level manner alongside the main network (Sec. 3.2). Hardest domain selection is accomplished by aggregating votes for samples from each domain for the randomly selected reserved classes using the follower network (Sec. 3.3).

To optimize domain scheduling, we first define the term domain reliability, as the degree to which data from a domain consistently aids in improving the model's accuracy and generalizability across unseen domains. An important step is therefore to adaptively rate the domain reliability during training. To achieve this, we employ two parallel networks: the main network used for feature extraction, and the follower network, which assesses the reliability of different domains based on the refined confidence metrics. This follower network plays a central role in our adaptive domain scheduling strategy: it employs a voting process to identify and select the most challenging domains - those that exhibit the least reliability according to its assessments. After selecting the hardest domain, the data is divided into two sets. One set includes data from more challenging domains outside the reserved classes and data from more reliable domains within the reserved classes, which together form the meta-training set. The complementary partitions of the meta-training set are used as the meta-testing set. Both networks are simultaneously optimized through a bi-level training approach, meaning that the outcome of a lower-level optimization problem (evaluating domain reliability) is a constraint in an upper-level problem (meta-learning objective). The entire pipeline during training when using the proposed EBil-HaDS is depicted in Alg. 1.

### Max Rebiased Discrepancy Evidential Learning

The domain scheduler we propose leverages a follower network to ascertain reliability measurements, based on reliability evaluations conducted before the start of each epoch for all source domains. As such, confidence calibration is crucial for the main network's functionality. Evidential learning, which has been extensively applied across various domains such as action recognition  and image classification , effectively calibrates these confidence predictions. However, a notable limitation of evidential learning is its propensity for overfitting, leading to suboptimal performance .

To address these challenges, we propose to regularize the evidential learning by novel rebiased discrepancy maximization, which is employed for the confidence calibration of the main network to encourage diverse decision boundaries. This method involves training dual decision-making heads designed to exhibit rebiased maximized discrepancies. The aim is to foster the development of both informative and dependable decision-making capabilities within the leveraged deep learning model. Let \(\) denote a batch of data used in training, \(M_{}\) denote the feature extraction backbone, \(R_{_{1}}\) and \(R_{_{2}}\) denote the two rebiased layers, and \(\) denote the Gaussian kernel to reproduce the Hilbert space. We first calculate the max rebiased discrepancy regularization by Eq. 1,

\[_{RB}(;)=_{i\{1,2\}}[(R_{_{i}}(M_{}()),R_{_{i}}(M_{}())) ]-2*[(R_{_{1}}(M_{}()),R_{ _{2}}(M_{}()))]. \]

We aim to maximize the above loss function to achieve the maximum discrepancy between the embeddings extracted from the two rebiased layers. This maximization encourages the learned evidence from the two layers to diverge from each other, thereby capturing open-set domain generalization cues from two different perspectives. Deep evidential learning is then applied to the conventional classification head, providing an additional constraint to achieve more reliable confidence calibration, as described in Eq. 2.

\[_{RBE}(,;)=_{i\{1,2\}}[_{ c=1}^{}[_{c}( S_{i}-(R_{_{i}}(M_ {}())_{c}+1))]]-_{RB}( ;) \]

where \(S_{i}=_{c=1}^{C}(Dir(p|R_{_{i}}(M_{}())_{ c}+1)\) denotes the strength of a Dirichlet distribution, \(_{c}\) is the one-hot annotation of sample \(\) from class \(c\), \(p\) is the predicted probability. The two rebiased layers are engineered to capture distinct evidence by employing max discrepancy regularization. By averaging the logits produced by the two prediction heads on the top of the two rebiased layers for the conventional classification on the seen categories, we can harvest the final estimated confidence score. This score is subsequently utilized to supervise the follower network, as elaborated in the following subsections.

### Follower Network for Reliability Learning

To establish an adaptive domain scheduler for the OSDG task, the most straightforward approach would involve training a network to directly predict the sequence in which domains are employed during the training phase for sample selection. However, this method does not facilitate gradient computation, thereby preventing the direct optimization of the scheduler network.

In this work, we propose an alternative method where a follower network is trained to assess the reliability of each sample, utilizing predicted confidence scores derived from max discrepancy evidential learning as supervision. Throughout the training process, we employ samples from various domains to collectively assess reliability. Additionally, we utilize a follower network, denoted as \(M_{}\), which mirrors the architecture of the main network, but with classification heads replaced by one regression head. \(\) indicates all the parameters in the main network, including the parameters from the backbone, rebiased layers, and heads. We aim to solve the optimization task, as shown in Eq. 3.

\[^{*}=*{arg\,min}_{}_{m}(M_{}( ),^{*} M_{^{*}}())\ \ \ \ \ ^{*}=*{arg\,min}_{}L_{f}(M_{}(),M_{ }()), \]

where \(^{*}\) indicates the instance-wise reliability which serves as the weight for each instance during the loss calculation. Substituting the best response function \(^{*}()=*{arg\,min}_{}L_{f}(M_{}( ),M_{}())\) provides a single-level problem, as shown in Eq. 4.

\[^{*}=*{arg\,min}_{}L_{m}(,^{*}()), \]

where \(L_{m}\) denotes classification loss (\(L_{CLS}\)) and \(L_{RBE}\). \(L_{f}\) denotes the regression loss (\(L_{REG}\)).

### Hardest Domain Scheduler during Training

We illustrate the details of the training procedure by the proposed domain scheduler in Alg. 1. We adopt the meta-training framework outlined by MLDG , integrating our proposed domain scheduler to facilitate the data partition of meta-tasks. In this approach, optimization is achieved using both the meta-train and -test sets, characterized by distinct data distributions. For each domain present in the training dataset, we sample a batch that encompasses the reserved categories. Subsequently, we identify the most challenging domain by determining which domain exhibits the lowest reliability under the selected seen categories. This procedure is accomplished by the calculation of the expected reliability as in Eq. 5.

\[d^{*}=*{arg\,min}_{d}(\{_{d}|d\}),\ _{d}=_{c^{*}}[[1+_{i=1}^{N_{c}^{*}} (_{i}^{(c,d)}))}{N_{c}^{*}}]*(0.1+*_{d})], \]

where \(d^{*}\) denotes the estimated hardest domain. \(N_{c}^{*}\) and \(^{*}\) denote the number of samples from domain \(d\) and the number of selected known categories at the start of one epoch. \(\) denotes the known domains used during the training procedure. \(_{i}^{(c,d)}\) indicates the \(i\)-th sample from class \(c\) and domain \(d\). \(_{d}\) indicates the schedule frequency for domain \(d\) in the past training period, which considers the balance of different domains.

[MISSING_PAGE_FAIL:6]

[MISSING_PAGE_FAIL:7]

training, enhancing generalization to unseen domains, which is observed from the above experimental analyses. Significant performance gains in challenging DG splits, such as the unseen Cartoon domain, demonstrate its effectiveness in handling extreme domain shifts. Consistent metric improvements highlight EBiL-HaDS's versatility across various OSDG challenges.

Further experiments on a different backbone, _i.e._, ResNet50 , are delivered in Table 2, where our method contributes \(1.08\%\), \(0.94\%\), and \(1.27\%\) performance improvements of close-set accuracy, H-score, and OSCR for binary classification head and consistent performance improvements for the conventional classification head. EBiL-HaDS contributes more performance improvements when we compare the experimental results on ResNet18 with ResNet50  for the PACS dataset, which illustrates that the EBiL-HaDS is more helpful in alleviating the generalizability issue of model-preserving light-weight network structure since network with small size is hard to optimize and obtain the generalizable capabilities on challenging unseen domains.

We further conduct ablation on model architecture on ResNet152 and ViT base model , where our proposed method is compared with the MEDIC and other challenging baselines, _i.e._, ARPL, MLDG, SWAD, and ODG-Net. From Table 2 and Table 1 we can observe an obvious OSDG performance improvement by increasing the complexity of the leveraged feature extraction backbone. However, when we compare Table 3 and Table 2, some baseline approaches trained with ResNet152 backbone even show performance decay on the major evaluation metric OSCR. This observation demonstrates that most OSDG methods face the overfitting issue when using a very large backbone, which is a critical issue for open-set challenging to recognize samples from unseen categories, especially in an unseen domain. The MEDIC-bcls approach shows \(6.48\%\) performance degradation on OSCR when we replace the backbone from ResNet50 to ResNet152. Using the proposed BHiL-HaDS to achieve a more reasonable task reservation in meta-learning procedure, BHiL-HaDS-bcls delivers \(86.25\%\) in terms of OSCR on ResNet152  backbone, where the OSCR performance of BHiL-HaDS-bcls on ResNet50 is \(86.12\%\).

This observation shows that our proposed adaptive domain scheduler can make the meta-learning effective on large complex models by reserving reasonable task for model optimization. Additional experiments on the ViT base model  (patch size 16 and window size 224) are provided in Table 4, where we observe that our proposed method can deliver consistent performance gains on the transformer architecture.

This observation is further validated by the experimental results on DigitsDG where a smaller network structure, _i.e._, ConvNet , is used, as shown in Table 5. Our method contributes \(3.74\%\), \(4.83\%\), and \(4.72\%\) performance improvements of close-set accuracy, H-score, and OSCR for binary classification head and \(3.74\%\), \(12.29\%\), and \(5.64\%\) performance improvements of close-set accuracy, H-score, and OSCR for the conventional classification head. Consistent performance improvements are shown in Table 6 on the OfficeHome. We further observe that using EBiL-HaDS, the optimized model can contribute a distinct separation between confidence scores of the model on the unseen categories and seen categories in the test unseen domain, showing the benefits of a reasonable domain scheduler for OSDG.

close-set accuracy, H-score, and OSCR for binary classification head and \(1.82\%\), \(2.73\%\), and \(3.68\%\) performance improvements of close accuracy, H-score, and OSCR for conventional classification head. The significant OSDG performance improvements highlight the importance of the confidence score learned by the max rebiased discrepancy evidential learning in supervising the follower network, ensuring the promising reliability prediction. Then we use a sequential scheduler and keep the \(L_{RBE}\) in the third part of Table 7 (w/o DGS), where our approach outperforms this variant by \(2.04\%\), \(3.92\%\), and \(3.89\%\) of close accuracy, H-score, and OSCR for binary classification head and \(2.04\%\), \(3.31\%\), and \(5.07\%\) of these metrics for conventional classification head. This observation shows the importance of the proposed domain scheduler for the OSDG task and highlights the effect of using meta-learning trained with a reasonable data partition. Both ablations show better OSDG performances compared with MEDIC, confirming the benefit of each component. We further deliver

    &  &  &  &  &  \\  & Acc & H-score & OSCR & Acc & H-score & OSCR & Acc & H-score & OSCR & Acc & H-score & OSCR & Acc & H-score & OSCR \\  SequentialSched-sh & 97.89 & 67.39 & 66.71 & 71.14 & 48.44 & 58.337 & 76.00 & 51.20 & 55.58 & 88.11 & 64.00 & 70.62 & 84.12 & 57.98 & 70.19 \\ SequentialSched-shcls & 97.89 & 83.20 & 96.58 & 71.14 & **64.08** & 58.26 & 76.00 & 58.77 & 57.60 & 88.11 & 62.24 & 72.91 & 83.28 & 66.30 & 71.15 \\ Random-sh & 98.39 & 52.93 & 94.21 & 20.92 & 52.70 & 52.41 & 77.92 & 59.68 & 57.95 & 88.33 & 44.11 & 75.66 & 83.89 & 52.35 & 70.06 \\ Random-sh & 98.39 & 75.67 & 94.22 & 70.92 & 57.23 & 54.87 & 77.92 & 57.54 & 61.06 & 88.33 & 68.34 & 74.81 & 83.39 & 64.20 & 71.25 \\  EBL-HtPS-chs & **95.80** & 87.40 & 97.49 & **74.28** & 56.58 & **60.86** & **80.33** & 61.27 & 62.84 & **93.97** & **75.82** & 78.14 & **87.02** & 70.27 & 75.83 \\ EBL-HtPS-chs & **99.50** & **91.63** & **97.58** & **74.28** & 60.72 & 59.39 & **80.33** & **62.23** & **63.38** & **93.37** & 75.77 & **79.28** & **87.02** & **72.59** & **75.87** \\   

Table 8: Comparison with different domain schedulers on DigidsDG with open-set ratio 6:4.

Figure 1: Comparison of open-set confidence using ResNet18  on PACS. _Photo_ is the unseen domain. We use red and blue colors to denote unseen and seen categories.

Figure 3: Ablation of different open-set ratios on DigitsDG dataset by ConvNet  backbone, where SA indicates Split Ablation. Regarding all the splits, Case 1 (denoted by red color) indicates using 7, 8, 9, 10 as unseen categories. In (a) and (b), Case 2 (denoted by blue color) and Case 3 (denoted by gray color) indicate that using 0, 1, 2, 3 and 2, 3, 4, 5 as unseen categories. In (c) and (d), Case 2 and Case 3 indicate using 7, 8, 9 and 8, 9 as unseen categories, respectively.

more ablations, _e.g._, the benefits brought by the max discrepancy regularization term, comparison with recent curriculum learning approaches, and ablation of the rebiased layers in the appendix.

### Comparison of Different Domain Schedulers for OSDG Task

We present several comparison experiments in Table 8 to demonstrate the efficacy of various domain schedulers when applying meta-learning to the OSDG task. We compare our proposed EBiL-HaDS with both the sequential domain scheduler and the random domain scheduler. The sequential domain scheduler selects domains in a fixed order for batch data partitioning, while the random scheduler assigns domains randomly. Results show that EBiL-HaDS significantly outperforms both the random and sequential domain schedulers. Specifically, EBiL-HaDS achieves performance improvements of \(3.13\%\), \(8.39\%\), and \(4.63\%\) in closed accuracy, H-score, and OSCR for the binary classification head, and \(3.13\%\), \(17.92\%\), and \(5.77\%\) in these metrics for the conventional classification head compared to the random scheduler. This ablation demonstrates that our scheduler enables the model to converge to a more optimal region, which outperforms both predefined fixed-order (sequential) and maximally random (random) schedulers, underscoring the importance of a well-designed domain scheduler in meta-learning for the OSDG. More ablations on domain schedulers are supplemented in the appendix.

### Analysis of the TSNE Visualizations of the Latent Space

In Figure 2, we deliver the TSNE  visualization of the latent space of MEDIC and our approach on the OSDG splits, _i.e._, _photo_ and _art_ as unseen domains. Unseen and seen categories are denoted by red and other colors. we observe that the model trained by our method delivers a more compact cluster for each category and the unseen category is more separable regarding the decision boundary in the latent space. Our method's ability to improve the generalizability of the model is particularly noteworthy. The well-structured latent space facilitates better transfer learning capabilities, allowing the model to adapt more efficiently to new, unseen categories. This characteristic is especially beneficial in dynamic environments where the data distribution can change over time. In essence, the effectiveness of our approach in achieving a more discriminative and generalizable latent space can be directly linked to the sophisticated data partitioning achieved through EBiL-HaDS. This demonstrates the profound influence that carefully designed domain schedulers can have on the overall performance of deep learning models, emphasizing the need for thoughtful consideration in their implementation.

### Ablation of the Open-Set Ratios and the Number of Unseen Categories

We first conduct the ablation towards different unseen categories with a predefined open-set ratio in Figure 2(a) and Figure 2(b) of close-set accuracy and the OSCR for open-set evaluation, where the performance of the ODG-NET, binary classification head, and conventional classification head of MEDIC method and our method are presented. We then conduct the ablation towards different numbers of unseen categories in Figure 2(c) and Figure 2(d) of close-set accuracy and the OSCR for open-set evaluation, where the performance of the ODG-NET, binary classification head, and classification head of MEDIC method and our method are presented. From the experimental results and comparisons, we can find consistent performance improvements, indicating the high generalizability of our approach across different open-set ratios and unseen category settings.

## 5 Conclusion

In this study, we introduce the Evidential Bi-Level Hardest Domain Scheduler (EBiL-HaDS) for the OSDG task. EBiL-HaDS is designed to create an adaptive domain scheduler that dynamically adjusts to varying domain difficulties. Extensive experiments on diverse image recognition tasks across three OSDG benchmarks demonstrate that our proposed solution generates more discriminative embeddings. Additionally, it significantly enhances the performance of state-of-the-art techniques in OSDG, showcasing its efficacy and potential for broader applications for deep learning models.

**Limitations and Societal Impacts.** EBiL-HaDS positively impacts society by enhancing model awareness of out-of-distribution categories in unseen domains, leading to more reliable decisions. It emphasizes the importance of domain scheduling in OSDG. However, the method may still result in misclassification and biased predictions, potentially causing negative effects. EBiL-HaDS relies on source domains with unified categories and has so far only been tested on image classification.