# Use Perturbations when Learning from Explanations

Juyeon Heo

University of Cambridge

jh2324@cam.ac.uk

&Vihari Piratla

University of Cambridge

vp421@cam.ac.uk

Equal Contribution

&Matthew Wicker

Alan Turing Institute

Adrian Weller

Alan Turing Institute and

University of Cambridge

###### Abstract

Machine learning from explanations (MLX) is an approach to learning that uses human-provided explanations of relevant or irrelevant features for each input to ensure that model predictions are _right for the right reasons_. Existing MLX approaches rely on local model interpretation methods and require strong model smoothing to align model and human explanations, leading to sub-optimal performance. We recast MLX as a robustness problem, where human explanations specify a lower dimensional manifold from which perturbations can be drawn, and show both theoretically and empirically how this approach alleviates the need for strong model smoothing. We consider various approaches to achieving robustness, leading to improved performance over prior MLX methods. Finally, we show how to combine robustness with an earlier MLX method, yielding state-of-the-art results on both synthetic and real-world benchmarks. Our implementation can be found at: [https://github.com/vihari/robust_mlx](https://github.com/vihari/robust_mlx).

## 1 Introduction

Deep neural networks (DNNs) display impressive capabilities, making them strong candidates for real-world deployment. However, numerous challenges hinder their adoption in practice. Several major deployment challenges have been linked to the fact that labelled data often under-specifies the task (D'Amour et al., 2020). For example, systems trained on chest x-rays were shown to generalise poorly because they exploited dataset-specific incidental correlations such as hospital tags for diagnosing pneumonia (Zech et al., 2018; DeGrave et al., 2021). This phenomenon of learning unintended feature-label relationships is referred to as _shortcut learning_(Geirhos et al., 2020) and is a critical challenge to solve for trustworthy deployment of machine learning algorithms. A common remedy to avoid shortcut learning is to train on diverse data (Shah et al., 2022) from multiple domains, demographics, etc, thus minimizing the underspecification problem, but this may be impractical for many applications such as in healthcare.

Enriching supervision through human-provided explanations of relevant and irrelevant regions/features per example is an appealing direction toward reducing under-specification. For instance, a (human-provided) explanation for chest x-ray classification may highlight scanning artifacts such as hospital tag as irrelevant features. Learning from such human-provided explanations (MLX) has been shown to avoid known shortcuts (Schramowski et al., 2020). Ross et al. (2017) pioneered an MLX approach based on regularizing DNNs, which was followed by several others (Schramowski et al., 2020; Rieger et al., 2020; Stammer et al., 2021; Shao et al., 2021). Broadly, existing approaches employ a model interpretation method to obtain per-example feature saliency, andregularize such that model and human-provided explanations align. Since saliency is unbounded for relevant features, many approaches simply regularize the salience of irrelevant features. In the same spirit, we focus on handling a specification of irrelevant features, which we refer to as an explanation hereafter.

Existing techniques for MLX employ a local interpretation tool and augment the loss with a term regularizing the importance of irrelevant region. Regularization-based approaches suffer from a critical concern stemming from their dependence on a local interpretation method. Regularization of local, i.e. example-specific, explanations may not have the desired effect of reducing shortcuts globally, i.e. over the entire input domain (see Figure 1). As we demonstrate both analytically and empirically that previous MLX proposals, which are all regularization-based, require strong model smoothing in order to be globally effective at reducing shortcut learning.

In this work, we explore MLX using various robust training methods with the objective of training models that are robust to perturbations of irrelevant features. We start by framing the provided human explanations as specifications of a local, lower-dimensional manifold from which perturbations are drawn. We then notice that a model whose prediction is invariant to perturbations drawn from the manifold ought also to be robust to irrelevant features. Our perspective yields considerable advantages. Posing MLX as a robustness task enables us to leverage the considerable body of prior work in robustness. Further, we show in Section 4.1 that robust training can provably upper bound the deviation on model value when irrelevant features are perturbed without needing to impose model smoothing. However, when the space of irrelevant features is high-dimensional, robust-training may not fully suppress irrelevant features as explained in Section 4.2. Accordingly, we explore combining both robustness-based and regularization-based methods, which achieves the best results. We highlight the following contributions:

* We theoretically and empirically demonstrate that existing MLX methods require strong model smoothing owing to their dependence on local model interpretation tools.
* We study learning from explanations using robust training methods. To the best of our knowledge, we are the first to analytically and empirically evaluate robust training methods for MLX.
* We distill our insights into our final proposal of combining robustness and regularization-based methods, which consistently performed well and reduced the error rate over the previous best by 20-90%.

## 2 Problem Definition and Background

We assume access to a training dataset with \(N\) training examples, \(_{T}=\{(^{(i)},y^{(i)})\}_{i=1}^{N}\), with \(^{(i)}^{d}\) and \(y^{(i)}\) label. In the MLX setting, a human expert also specifies input mask \(^{(n)}\) for an example \(^{(n)}\) where non-zero values of the mask identify _irrelevant_ features of the input \(^{(n)}\). An input mask is usually designed to negate a known shortcut feature that a classifier is exploiting. Figure 2 shows some examples of masks for the datasets that we used for evaluation. For example, a mask in the ISIC dataset highlights a patch that was found to confound with non-cancerous images. With the added human specification, the augmented dataset contains triplets of example, label and mask, \(_{T}=\{(^{(i)},y^{(i)},^{(i)})\}_{i=0}^{N}\). The task therefore is to learn a model \(f(;)\) that fits observations well while not exploiting any features that are identified by the mask \(\).

The method of Ross et al. (2017) which we call Grad-Reg (short for Gradient-Regularization), and also other similar approaches (Shao et al., 2021; Schramowski et al., 2020) employ an local interpretation method (E) to assign importance scores to input features: \(IS()\), which is then regularized with an \(()\) term such that irrelevant features are not regarded as important. Their training loss takes the form shown in Equation 1 for an appropriately defined task-specific loss \(\).

\[IS()  E(,f(;)).\] \[() _{n=1}^{N}\|IS(^{(n)})^{( n)}\|^{2}.\] \[^{*} =_{}\{_{n}(f(^{(n)}; ),y^{(n)})\ +()+\|\|^{2} \}. \]We use \(\) to denote element-wise product throughout. CDEP (Rieger et al., 2020) is slightly different. They instead use an explanation method that also takes the mask as an argument to estimate the contribution of features identified by the mask, which they minimize similarly.

**About obtaining human specification mask.** Getting manually specifying explanation masks can be impractical. However, the procedure can be automated if the nuisance/irrelevant feature occurs systematically or if it is easy to recognize, which may then be obtained automatically using a procedure similar to Liu et al. (2021); Rieger et al. (2020). A recent effort called Salient-Imagenet used neuron activation maps to scale curation of such human-specified masks to Imagenet-scale (Singla and Feizi, 2021; Singla et al., 2022). These efforts may be seen as a proof-of-concept for obtaining richer annotations beyond content labels, and towards better defined tasks.

## 3 Method

Our methodology is based on the observation that an ideal model must be robust to perturbations to the irrelevant features. Following this observation, we reinterpret the human-provided mask as a specification of a lower-dimensional manifold from which perturbations are drawn and optimize the following objective.

\[^{*}=_{}_{n}\{(f(^{(n)}; ),y^{(n)})..+_{:\|\|_{ }}(f(^{(n)}+(^{ (n)});),y^{(n)})\} \]

The above formulation uses a weighting \(\) to trade off between the standard task loss and perturbation loss and \(>0\) is a hyperparameter that controls the strength of robustness. We can leverage the many advances in robustness in order to approximately solve the inner maximization. We present them below.

**Avg-Ex**: We can approximate the inner-max with the empirical average of loss averaged over \(K\) samples drawn from the neighbourhood of training inputs. Singla et al. (2022) adopted this straightforward baseline for supervising using human-provided saliency maps on the Imagenet dataset. Similar to \(\), we use \(\) to control the noise in perturbations as shown below.

\[^{*}=_{}_{n}\{(f(^{(n)}; ),y^{(n)})+_{_{j}(0, ^{2}I)}^{K}(f(^{(n)}+(_{j} ^{(n)});),y^{(n)})\}\]

**PGD-Ex**: Optimizing for an estimate of worst perturbation through projected gradient descent (PGD) (Madry et al., 2017) is a popular approach from adversarial robustness. We refer to the approach of using PGD to approximate the second term of our loss as PGD-Ex and denote by \(^{*}(^{(n)},,^{(n)})\) the perturbation found by PGD at \(^{(n)}\). Given the non-convexity of this problem, however, no guarantees can be made about the quality of the approximate solution \(^{*}\).

\[^{*}=_{}_{n}\{(f(^{(n)}; ),y^{(n)})+(f(^{(n)}+(^{*}(^{(n)},,^{(n)}));),y^{(n)})\}\]

**IBP-Ex**: Certified robustness approaches, on the other hand, minimize a certifiable upper-bound of the second term. A class of certifiable approaches known as interval bound propagation methods (IBP) (Mirman et al., 2018; Gowal et al., 2018) propagate input intervals to function value intervals that are guaranteed to contain true function values for any input in the input interval.

We define an input interval for \(^{(n)}\) as \([^{(n)}-^{(n)},^{(n)}+^{(n )}]\) where \(\) is defined in Eqn. 2. We then use bound propagation techniques to obtain function value intervals for the corresponding input interval: \(^{(n)},^{(n)}\), which are ranges over class logits. Since we wish to train a model that correctly classifies an example irrespective of the value of the irrelevant features, we wish to maximize the minimum probability assigned to the correct class, which is obtained by combining minimum logit for the correct class with maximum logit for incorrect class: \((^{(n)},y^{(n)},^{(n)},^{(n)};) ^{(n)}}^{(n)}+^{(n)}( -}^{(n)})\) where \(}^{(n)}\{0,1\}^{c}\) denotes the one-hot transformation of the label \(y^{(n)}\) into a c-length vector for \(c\) classes. We refer to this version of the loss as IBP-Ex, summarizedbelow.

\[^{(n)},^{(n)}=IBP(f(;),[^{(n )}-^{()},^{(n)}+ ^{()}])\] \[(^{(n)},y^{(n)},^{(n)},^{( n)};)^{(n)}}^{(n)}+^{(n)} (-}^{(n)})\] \[^{*}=_{}_{n}(f(^{(n)}; ),y^{(n)})+((^{(n)},y^{(n)}, ,;),y^{(n)}) \]

Despite its computational efficiency, IBP is known to suffer from scaling issues when the model is too big. Consequently, it is better to use IBP-Ex only when the model is small (less than four layers of CNN or feed-forward) and if computational efficiency is desired. On the other hand, we do not anticipate any scaling issues when using PGD-Ex.

**Combined robustness and regularization**: PGD-Ex+Grad-Reg, IBP-Ex+Grad-Reg. We combine robustness and regularization by simply combining their respective loss terms. We show the objective for IBP-Ex+Grad-Reg below, PGD-Ex+Grad-Reg follows similarly.

\[^{*}=_{}_{n}(f(^{(n)};),y^{( n)})+((^{(n)},y^{(n)},, ;),y^{(n)})+(). \]

\(()\) and \(,\) are as defined in Eqn. 5 and Eqn. 3 respectively. In Section 4, 5.2, we demonstrate the complementary strengths of robustness and regularization-based methods.

## 4 Theoretical Motivation

In this section, we motivate the merits and drawbacks of robustness-based over regularization-based methods. Through non-parametric analysis in Theorems 1, 2, we argue that (a) regularization methods are robust to perturbations of irrelevant features (identified by the mask) only when the underlying model is sufficiently smoothed, thereby potentially compromising performance, (b) robust training upper-bounds deviation in function values when irrelevant features are perturbed, which can be further suppressed by using a more effective robust training. Although our analysis is restricted to nonparametric models for the ease of analysis, we empirically verify our claims with parametric neural network optimized using a gradient-based optimizer. We then highlight a limitation of robustness-based methods when the number of irrelevant features is large through Proposition 1.

### Merits of Robustness-based methods

Consider a two-dimensional regression task, i.e. \(^{(n)}\) and \(y\). Assume that the second feature is the shortcut that the model should not use for prediction, and denote by \(^{(n)}_{j}\) the \(j^{th}\) dimension of \(n^{th}\) point. We infer a regression function \(f\) from a Gaussian process prior \(f GP(f;0,K)\) with a squared exponential kernel where \(k(x,)=(-_{i}-_{j})^{2}}{ _{i}^{2}})\). As a result, we have two hyperparameters \(_{1},_{2}\), which are length scale parameters for the first and second dimensions respectively. Further, we impose a Gamma prior over the length scale: \(_{i}^{-2}(,)\).

Figure 1: Illustration of the uneasy relationship between Grad-Reg and smoothing strength. (b) The decision boundary is nearly vertical (zero gradient wrt to nuisance y-axis value) for all training points and yet varies as a function of y value when fitted using Grad-Reg with \(=0\). (c) Grad-Reg requires strong model smoothing (\(=1\)) in order to translate local insensitivity to global robustness to y-coordinate. (d) IBP-Ex, on the other hand, fits vertical pair of lines without any model smoothing.

**Theorem 1** (Grad-Reg).: _We infer a regression function \(f\) from a GP prior as described above with the additional supervision of \([ f()/ x_{2}]|_{^{(i)}}=0, i[1,N]\). Then the function value deviations to perturbations on irrelevant feature are lower bounded by a value proportional to the perturbation strength \(\) as shown below._

\[f(+[0,]^{T})-f() (x_{1}^{2}x_{2}^{6}+ x_{1}^{2}x_{2}^{5}) \]

Full proof of Theorem 1 is in Appendix A, we provide the proof outline below.

Proof sketch.: We exploit the flexibility of GPs to accommodate observations on transformed function values if the transformation is closed under linear operation (Hennig et al., 2022) (see Chapter 4). Since gradient is a linear operation (contributors, 2022), we simply revise the observations \(y\) to include the \(N\) partial derivative observations for each training point with appropriately defined kernel. We then express the inferred function, which is the posterior \(f(_{T})\) in closed form.

We then derive a bound on function value deviations to perturbation on the second feature and simplify using simple arithmetic and Bernoulli's inequality to derive the final expression. 

We observe from Theorem 1 that if we wish to infer a function that is robust to irrelevant feature perturbations, we need to set \(\) to a very small value. Since the expectation of gamma distributed inverse-square length parameter is \([^{-2}]=\), which we wish to set very small, we are, in effect, sampling functions with very large length scale parameter i.e. strongly smooth functions. This result brings us to the intuitive takeaway that regularization using Grad-Reg applies globally only when the underlying family of functions is sufficiently smooth. The result may also hold for any other local-interpretation methods that is closed under linear operation. One could also argue that we can simply use different priors for different dimensions, which would resolve the over-smoothing issue. However, we do not have access to parameters specific to each dimension in practice and especially with DNNs, therefore only overall smoothness may be imposed such as with parameter norm regularization in Eqn. 1.

We now look at properties of a function fitted using robustness methods and argue that they bound deviations in function values better. In order to express the bounds, we introduce a numerical quantity called coverage (C) to measure the effectiveness of a robust training method. We first define a notion of inputs covered by a robust training method as \(}\{,(f( ;),y)<\}\) for a small positive threshold \(\) on loss. We define coverage as the maximum distance along second coordinate between any point in \(\) and its closest point in \(}\), i.e. \(C_{}_{ }}|_{2}-}_{2}|\). We observe that C is small if the robust training is effective. In the extreme case when training minimizes the loss for all points in the input domain, i.e. \(}=\), then C=0.

**Theorem 2**.: _When we use a robustness algorithm to regularize the network, the fitted function has the following property._

\[|f(+[0,]^{T})-f()| 2C_{ max}f_{max}. \]

\(_{max}\) _and \(f_{max}\) are maximum values of \( x_{2}\) and \(f()\) in the input domain (\(\)) respectively._

Proof sketch.: We begin by estimating the function deviation between an arbitrary point and a training instance that only differ on \(x_{2}\), which is bounded by the product of maximum slope (\(_{} f()/ x_{2}\)) and \( x_{2}\). We then estimate the maximum slope (lipschitz constant) for a function inferred from GP. Finally, function deviation between two arbitrary points is twice the maximum deviation between an arbitrary and a training instance that is estimated in the first step. 

Full proof is in Appendix B. The statement shows that deviations in function values are upper bounded by a factor proportional to \(C\), which can be dampened by employing an effective robust training method. We can therefore control the deviations in function values without needing to regress \(\) (i.e. without over-smoothing).

Further remarks on sources of over-smoothing in regularization-based methods.We empirically observed that the term \(()\) (of Eqn. 1), which supervises explanations, also has a smoothing effect on the model when the importance scores (IS) are not well normalized, which is often the case. This is because reducing IS(\(\)) everywhere will also reduce saliency of irrelevant features.

Empirical verification with a toy dataset.For empirical verification of our results, we fit a 3-layer feed-forward network on a two-dimensional data shown in Figure 1 (a), where color indicates the label. We consider fitting a model that is robust to changes in the second feature shown on y-axis. In Figures 1 (b), (c), we show the Grad-Reg fitted classifier using gradient (\( f/ x_{2}\) for our case) regularization for two different strengths of parameter smoothing (0 and 1 respectively). With weak smoothing, we observe that the fitted classifier is locally vertical (zero gradient along y-axis), but curved overall (Figure 1 (b)), which is fixed with strong smoothing (Figure 1 (c)). On the other hand, IBP-Ex fitted classifier is nearly vertical without any parameter regularization as shown in (d). This example illustrates the need for strong model smoothing when using a regularization-based method.

### Drawbacks of Robustness-based methods

Although robust training is appealing in low dimensions, their merits do not transfer well when the space of irrelevant features is high-dimensional owing to difficulty in solving the inner maximization of Eqn. 2. Sub-par estimation of the maximization term may learn parameters that still depend on the irrelevant features. We demonstrate this below with a simple exercise.

**Proposition 1**.: _Consider a regression task with \(D+1\)-dimensional inputs \(\) where the first D dimensions are irrelevant, and assume they are \(x_{d}=y,d[1,D]\) while \(x_{D+1}(y,1/K)\). The MAP estimate of linear regression parameters \(f()=_{d=1}^{D+1}w_{d}x_{d}\) when fitted using Avg-Ex are as follows: \(w_{d}=1/(D+K), d[1,D]\) and \(w_{D+1}=K/(K+D)\)._

We present the proof in Appendix C. We observe that as D increases, the weight of the only relevant feature (\(x_{D+1}\)) diminishes. On the other hand, the weight of the average feature: \(_{d=1}^{D}x_{d}\), which is \(D/(D+K)\) approaches 1 as \(D\) increases. This simple exercise demonstrates curse of dimensionality for robustness-based methods. For this reason, we saw major empirical gains when combining robustness methods with a regularization method especially when the number of irrelevant features is large such as in the case of Decoy-MNIST dataset, which is described in the next section.

## 5 Experiments

We evaluate different methods on four datasets: one synthetic and three real-world. The synthetic dataset is similar to decoy-MNIST of Ross et al. (2017) with induced shortcuts and is presented in Section 5.2. For evaluation on practical tasks, we evaluated on a plant phenotyping (Shao et al., 2021) task in Section 5.3, skin cancer detection (Rieger et al., 2020) task presented in Section 5.4, and object classification task presented in Section 5.5. All the datasets contain a known spurious feature, and were used in the past for evaluation of MLX methods. Figure 2 summarises the three datasets, notice that we additionally require in the training dataset the specification of a mask identifying irrelevant features of the input; the patch for ISIC dataset, background for plant dataset, decoy half for Decoy-MNIST images, and label-specific irrelevant region approved by humans for Salient-Imagenet.

### Setup

#### 5.1.1 Baselines

We denote by ERM the simple minimization of cross-entropy loss (using only the first loss term of Equation 1). We also compare with G-DRO(Sagawa et al., 2019), which also has the objective of avoiding to learn known irrelevant features but is supervised through group label (see Section 6). Although the comparison is unfair toward G-DRO because MLX methods use richer supervision of per-example masks, it serves as a baseline that can be slightly better than ERM in some cases.

Figure 2: Sample images and masks for different datasets. The shown mask corresponds to the image from the second column.

**Regulaization-based methods.** Grad-Reg and CDEP, which were discussed in Section 2. We omit comparison with Shao et al. (2021) because their code is not publicly available and is non-trivial to implement the influence-function based regularization.

**Robustness-based methods.** Avg-Ex, PGD-Ex, IBP-Ex along with **combined robustness and regularization methods**. IBP-Ex+Grad-Reg, PGD-Ex+Grad-Reg that are described in Section 3.

#### 5.1.2 Metrics

Since two of our datasets are highly skewed (Plant and ISIC), we report (macro-averaged over labels) accuracy denoted "Avg Acc" and worst accuracy "Wg Acc" over groups of examples with groups appropriately defined as described below. On Salient-Imagenet, we report using average accuracy and Relative Core Sentivity (RCS) following the standard practice.

**Wg Acc.** Worst accuracy among groups where groups are appropriately defined. Different labels define the groups for decoy-MNIST and plant dataset, which therefore have ten and two groups respectively. In ISIC dataset, different groups are defined by the cross-product of label and presence or absence of the patch. We denote this metric as "Wg Acc", which is a standard metric when evaluating on datasets with shortcut features (Sagawa et al., 2019).

**RCS** proposed in Singla et al. (2022) was used to evaluate models on Salient-Imagenet. The metric measures the sensitivity of a model to noise in spurious and core regions. High RCS implies low dependence on irrelevant regions, and therefore desired. Please see Appendix E.2 for more details.

#### 5.1.3 Training and Implementation details

**Choice of the best model.** We picked the best model using the held-out validation data. We then report the performance on test data averaged over three seeds corresponding to the best hyperparameter.

**Network details.** We use four-layer CNN followed by three-fully connected layers for binary classification on ISIC and plant dataset following the setting in Zhang et al. (2019), and three-fully connected layers for multi classification on decoy-MNIST dataset. Pretrained Resnet-18 is used as the backbone model for Salient-Imagenet.

More details about network architecture, datasets, data splits, computing specs, and hyperparameters can be found in Appendix E.

### Decoy-MNIST

Decoy-MNIST dataset is inspired from MNIST-CIFAR dataset of Shah et al. (2020) where a very simple label-revealing color based feature (decoy) is juxtaposed with a more complex feature (MNIST image) as shown in Figure 1. We also randomly swap the position of decoy and MNIST parts, which makes ignoring the decoy part more challenging. We then validate and test on images where decoy part is set to correspond with random other label. Note that our setting is different from decoy-mnist of CDEP (Rieger et al., 2020), which is likely why we observed discrepancy in results reported in our paper and CDEP (Rieger et al., 2020). We further elaborate on these differences in Appendix G.

  Dataset \(\) &  &  &  &  \\  Method\(\) & Avg Acc & Wg Acc & Avg Acc & Wg Acc & Avg Acc & Wg Acc & Avg Acc & RCS \\  ERM & 15.1 & 10.5 & 71.3 & 54.8 & 77.3 & 55.9 & 96.4 & 47.9 \\ G-DRO & 64.1 & 28.1 & 74.2 & 58.0 & 66.6 & 58.5 & - & - \\  Grad-Reg & 72.5 & 46.2 & 72.4 & 68.2 & 76.4 & 60.2 & 88.3 & 52.5 \\ CDEP & 14.5 & 10.0 & 67.9 & 54.2 & 73.4 & 60.9 & - & - \\  Avg-Ex & 29.5 & 19.5 & 76.3 & 64.5 & 77.1 & 55.2 & - & - \\ PGD-Ex & 67.6 & 51.4 & 79.8 & 78.5 & **78.7** & **64.4** & 93.8 & 58.7 \\ IBP-Ex & 68.1 & 47.6 & 76.6 & 73.8 & 75.1 & 64.2 & - & - \\  P+G & **96.9** & **95.8** & 79.4 & 76.7 & **79.6** & **67.5** & **94.6** & **65.0** \\
1+G & **96.9** & **95.0** & **81.7** & **80.1** & **78.4** & **65.2** & - & - \\  

Table 1: Macro-averaged (Avg) accuracy and worst group (Wg) accuracy on (a) decoy-MNIST, (b) plant dataset, (c) ISIC dataset along with relative core sensitivity (RCS) metric on (d) Salient-Imagenet. Statistically significant numbers are marked in bold. Please see Table 4 in Appendix for standard deviations. I+G is short for IBP-Ex+Grad-Reg and P+G for PGD-Ex+Grad-Reg.

We make the following observations from Decoy-MNIST results presented in Table 1. ERM is only slightly better than a random classifier confirming the simplicity bias observed in the past (Shah et al., 2020). Grad-Reg, PGD-Ex and IBP-Ex perform comparably and better than ERM, but when combined (IBP-Ex+Grad-Reg,PGD-Ex+Grad-Reg) they far exceed their individual performances.

In order to understand the surprising gains when combining regularization and robustness methods, we draw insights from gradient explanations on images from train split for Grad-Reg and IBP-Ex. We looked at \(s_{1}=[\|^{(n)} ^{(n)})}{^{(n)}}\|]\) and \(s_{2}=[\|^{(n)} ^{(n)})}{^{(n)}}\|]\), where \([]\) is median over all the examples and \(\|\|\) is \(_{2}\)-norm. For an effective algorithm, we expect both \(s_{1},s_{2}\) to be close to zero. However, the values of \(s_{1},s_{2}\) is 2.3e-3, 0.26 for the best model fitted using Grad-Reg and 6.7, 0.05 for IBP-Ex. We observe that Grad-Reg has lower \(s_{1}\) while IBP-Ex has lower \(s_{2}\), which shows that Grad-Reg is good at dampening the contribution of decoy part but also dampened contribution of non-decoy likely due to over-smoothing. IBP-Ex improves the contribution of the non-decoy part but did not fully dampen the decoy part likely because high dimensional space of irrelevant features, i.e. half the image is irrelevant and every pixel of irrelevant region is indicative of the label. When combined, IBP-Ex+Grad-Reg has low \(s_{1},s_{2}\), which explains the increased performance when they are combined.

### Plant Phenotyping

Plant phenotyping is a real-world task of classifying images of a plant leaf as healthy or unhealthy. About half of leaf images are infected with a Cercospora Leaf Spot (CLS), which are the black spots on leaves as shown in the first image in the second row of Figure 2. Schramowski et al. (2020) discovered that standard models exploited unrelated features from the nutritional solution in the background in which the leaf is placed, thereby performing poorly when evaluated outside of the laboratory setting. Thus, we aim to regulate the model not to focus on the background of the leaf using binary specification masks indicating where the background is located. Due to lack of out-of-distribution test set, we evaluate with in-domain test images but with background pixels replaced by a constant pixel value, which is obtained by averaging over all pixels and images in the training set. We replace with an average pixel value in order to avoid any undesired confounding from shifts in pixel value distribution. In Appendix F.5, we present accuracy results when adding varying magnitude of noise to the background, which agree with observations we made in this section. More detailed analysis of the dataset can be found in Schramowski et al. (2020).

Table 1 contrasts different algorithms on the plant dataset. All the algorithms except CDEP improve over ERM, which is unsurprising given our test data construction; any algorithm that can divert focus from the background pixels can perform well. Wg accuracy of robustness (except AvgEx) and combined methods far exceed any other method by 5-12% over the next best baseline and by 19-26% accuracy point over ERM. Surprisingly, even Avg-Ex has significantly improved the performance over ERM likely because spurious features in the background are spiky or unstable, which vanishes with simple perturbations.

We visualize the interpretations of models obtained using SmoothGrad (Smilkov et al., 2017) trained with five different methods for three sample images from the train split in Figure 3. As expected, ERM has strong dependence on non-leaf background features. Although Grad-Reg features are all on the leaf, they appear to be localized to a small region on the leaf, which is likely due to over-smoothing effect of its loss. IBP-Ex, IBP-Ex+Grad-Reg on the other hand draws features from a wider region and has more diverse pattern of active pixels.

### ISIC: Skin Cancer Detection

ISIC is a dataset of skin lesion images, which are to be classified cancerous or non-cancerous. Since half the non-cancerous images in the dataset contains a colorful patch as shown in Figure 2, standard

Figure 3: Visual heatmap of salient features for different algorithms on three sample train images of Plant data. Importance score computed with SmoothGrad (Smilkov et al., 2017).

DNN models depend on the presence of a patch for classification while compromising the accuracy on non-cancerous images without a patch (Codella et al., 2019; Tschandl et al., 2018). We follow the standard setup and dataset released by Rieger et al. (2020), which include masks highlighting the patch. Notably, (Rieger et al., 2020) employed a different evaluation metric, F1, and a different architecture -- a VGG model pre-trained on ImageNet. This may explain the worse-than-expected performance of CDEP from Rieger et al. (2020). More details are available in Appendix G.

Traditionally, three groups are identified in the dataset: non-cancerous images without patch (NCNP) and with patch (NCP), and cancerous images (C). In Table 2, we report on per-group accuracies for different algorithms. Detailed results with error bars are shown in Table 5 of Appendix F. The Wg accuracy (of Table 1) may not match with the worst of the average group accuracies in Table 2 because we report average of worst accuracies. We now make the following observations. ERM performs the worst on the NPNC group confirming that predictions made by a standard model depend on the patch. The accuracy on the PNC group is high overall perhaps because PNC group images that are consistently at a lower scale (see middle column of Figure 2 for an example) are systematically more easier to classify even when the patch is not used for classification. Although human-explanations for this dataset, which only identifies the patch if present, do not full specify all spurious correlations, we still saw gains when learning from them. Grad-Reg and CDEP improved NPNC accuracy at the expense of C's accuracy while still performing relatively poor on Wg accuracy. Avg-Ex performed no better than ERM whereas PGD-Ex, IBP-Ex, IBP-Ex+Grad-Reg, and improved Wg accuracy over other baselines. The reduced accuracy gap between NPNC and C when using combined methods is indicative of reduced dependence on patch. We provide a detailed comparison of PGD-Ex and IBP-Ex in Appendix F.3

### Salient-Imagenet

Salient-Imagenet (Singla and Feizi, 2021; Singla et al., 2022) is a large scale dataset based on Imagenet with pixel-level annotations of irrelevant (spurious) and relevant (core) regions. Salient-Imagenet is an exemplary effort on how explanation masks can be obtained at scale. The dataset contains 232 classes and 52,521 images (with about 226 examples per class) along with their core and spurious masks. We made a smaller evaluation subset using six classes with around 600 examples. Each of the six classes contain at least one spurious feature identified by the annotator. Please refer to Appendix E.4 for more details. We then tested different methods for their effectiveness in learning a model that ignores the irrelevant region. We use a ResNet-18 model pretrained on ImageNet as the initialization.

In Table 1, we report RCS along with overall accuracy. Appendix F.2 contains further results including accuracy when noise is added to spurious (irrelevant) region. The results again substantiate the relative strength of robustness-methods over regularization-based. Furthermore, we highlight the considerable improvement offered by the novel combination of robustness-based and regularization-based methods.

### Overall results

Among the regularization-based methods, Grad-Reg performed the best while also being simple and intuitive.

Robustness-based methods except Avg-Ex are consistently and effortlessly better or comparable to regularization-based methods on all the benchmarks with an improvement to Wg accuracy by 3-10% on the two real-world datasets. Combined methods are better than their constituents on all the datasets readily without much hyperparameter tuning.

In Appendix, we have additional experiment results; generality to new explanation methods (Appendix F.6) and new attention map-based architecture (Appendix F.7), and sensitivity to hyperparameters of PGD-Ex on Plant and ISIC dataset (Appendix F.4) and other perturbation-based methods on Decoy-MNIST dataset (Appendix F.8).

  Method & NPNC & PNC & C \\  ERM & 55.9 & 96.5 & 79.6 \\ Grad-Reg & 67.1 & 99.0 & 63.2 \\ CDEP & 72.1 & 98.9 & 62.2 \\ Avg-Ex & 62.3 & 97.8 & 71.0 \\ PGD-Ex & 65.4 & 99.0 & 71.7 \\ IBP-Ex & 68.4 & 98.5 & 67.7 \\  I+G & 66.6 & 99.6 & 68.9 \\ P+G & 69.6 & 98.8 & 70.4 \\  

Table 2: Per-group accuracies on ISIC.

Related Work

**Sub-population shift robustness.**Sagawa et al. (2019) popularized a problem setting to avoid learning known spurious correlation by exploiting group labels, which are labels that identify the group an example. Unlike MLX, the sub-population shift problem assumes that training data contains groups of examples with varying strengths of spurious correlations. For example in the ISIC dataset, images with and without patch make different groups. Broadly, different solutions attempt to learn a hypothesis that performs equally well on all the groups via importance up-weighting (Shimodaira (2000); Byrd and Lipton (2019)), balancing subgroups(Cui et al. (2019); Izmailov et al. (2022); Kirichenko et al. (2022)) or group-wise robust optimization (Sagawa et al. (2019)). Despite the similarities, MLX and sub-population shift problem setting have some critical differences. Sub-population shift setting assumes sufficient representation of examples from groups with varying levels of spurious feature correlations. This assumption may not always be practical, e.g. in medical imaging, which is when MLX problem setting may be preferred.

Learning from human-provided explanations.Learning from explanations attempts to ensure that model predictions are "right for the right reasons" (Ross et al., 2017). Since gradient-based explanations employed by Ross et al. (2017) are known to be unfaithful (Murphy, 2023) (Chapter 33) (Wicker et al., 2022), subsequent works have proposed to replace the explanation method while the overall loss structure remained similar. Shao et al. (2021) proposed to regularize using an influence function, while Rieger et al. (2020) proposed to appropriately regularize respective contributions of relevant or irrelevant features to the classification probability through an explanation method proposed by Singh et al. (2018). In a slight departure from these methods, Selvaraju et al. (2019) used a loss objective that penalises ranking inconsistencies between human and the model's salient regions demonstrated for VQA applications. Stammer et al. (2021) argued for going beyond pixel-level importance scores to concept-level scores for the ease of human intervention. On the other hand, Singla et al. (2022) studied performance when augmenting with simple perturbations of irrelevant features and with the gradient regularization of Ross et al. (2017). This is the only work, to the best of our knowledge, that explored robustness to perturbations for learning from explanations.

**Robust training.**Adversarial examples were first popularized for neural networks by Szegedy et al. (2013), and have been a significant issue for machine learning models for at least a decade (Biggio and Roli, 2018). Local methods for computing adversarial attacks have been studied (Madry et al., 2017), but it is well known that adaptive attacks are stronger (i.e., more readily fool NNs) than general attack methods such as PGD (Tramer et al., 2020). Certification approaches on the other hand are guaranteed to be worse than any possible attack (Mirman et al., 2018), and training with certification approaches such as IBP have been found to provide state-of-the-art results in terms of provable robustness (Gowal et al., 2018), uncertainty (Wicker et al., 2021), explainability (Wicker et al., 2022), and fairness (Benussi et al., 2022).

## 7 Conclusions

By casting MLX as a robustness problem and using human explanations to specify the manifold of perturbations, we have shown that it is possible to alleviate the need for strong parameter smoothing of earlier approaches. Borrowing from the well-studied topic of robustness, we evaluated two strong approaches, one from adversarial robustness (PGD-Ex) and one from certified robustness (IBP-Ex). In our evaluation spanning seven methods and four datasets including two real-world datasets we found that PGD-Ex and IBP-Ex performed better than any previous approach, while our final proposal IBP-Ex+Grad-Reg and PGD-Ex+Grad-Reg of combining IBP-Ex and PGD-Ex with a light-weight interpretation based method respectively have consistently performed the best without compromising compute efficiency by much.

**Limitations.** Detecting and specifying irrelevant regions per-example by humans is a laborious and non-trivial task. Hence, it is interesting to see the effects of learning from incomplete explanations, which we leave for future work. Although robustness-based methods are very effective as we demonstrated, they can be computationally expensive (PGD-Ex) or scale poorly to large networks (IBP-Ex).

**Broader Impact.** Our work is a step toward reliable machine learning intended for improving ML for all. To the best of our knowledge, we do not foresee any negative societal impacts.

Acknowledgements

MW acknowledges support from Accenture. MW and AW acknowledge support from EPSRC grant EP/V056883/1. AW acknowledges support from a Turing AI Fellowship under grant EP/V025279/1, and the Leverhulme Trust via CFI. We thank Pingfan Song, Yongchao Huang, Usman Anwar and multiple anonymous reviewers for engaging in thoughtful discussions, which helped greatly improve the quality of our paper.