# Segment Any Change

Zhuo Zheng\({}^{1}\), Yanfei Zhong\({}^{2}\), Liangpei Zhang\({}^{2}\), Stefano Ermon\({}^{1}\)

\({}^{1}\)Stanford University

\({}^{2}\)Wuhan University

zhuozheng@cs.stanford.edu

Corresponding authors: Stefano Ermon, Yanfei Zhong

###### Abstract

Visual foundation models have achieved remarkable results in zero-shot image classification and segmentation, but zero-shot change detection remains an open problem. In this paper, we propose the segment any change models (AnyChange), a new type of change detection model that supports zero-shot prediction and generalization on unseen change types and data distributions. AnyChange is built on the segment anything model (SAM) via our training-free adaptation method, bitemporal latent matching. By revealing and exploiting intra-image and inter-image semantic similarities in SAM's latent space, bitemporal latent matching endows SAM with zero-shot change detection capabilities in a training-free way. We also propose a point query mechanism to enable AnyChange's zero-shot object-centric change detection capability. We perform extensive experiments to confirm the effectiveness of AnyChange for zero-shot change detection. AnyChange sets a new record on the SECOND benchmark for unsupervised change detection, exceeding the previous SOTA by up to 4.4% F\({}_{1}\) score, and achieving comparable accuracy with negligible manual annotations (1 pixel per image) for supervised change detection. Code is available at [https://github.com/Z-Zheng/pytorch-change-models](https://github.com/Z-Zheng/pytorch-change-models).

## 1 Introduction

The Earth's surface undergoes constant changes over time due to natural processes and human activities. Some of the dynamic processes driving these changes (e.g., natural disasters, deforestation, and urbanization) have huge impact on climate, environment, and human life Zhu et al. (2022). Capturing these global changes via remote sensing and machine learning is a crucial step in many sustainability disciplines Yeh et al. (2020); Burke et al. (2021).

Deep change detection models have yielded impressive results via large-scale pre-training Manas et al. (2021); Wang et al. (2022); Mall et al. (2023); Zheng et al. (2023) and architecture improvements Chen et al. (2021); Zheng et al. (2022). However, their capabilities depend on training data and are limited to specific application scenarios. These models cannot generalize to new change types and data distributions (e.g., new geographic areas) beyond those seen during training.

This desired level of generalization on unseen change types and data distributions requires change detection models with zero-shot prediction capabilities. However, the concept of _zero-shot change detection_ has not been explored so far in the literature. While we are in the era of "foundation models" Bommasani et al. (2021) and have witnessed the emergence of large language models (LLMs) and vision foundation models (VFMs) (e.g., CLIP Radford et al. (2021) and Segment Anything Model (SAM) Kirillov et al. (2023)) with strong zero-shot prediction and generalization capabilities via prompt engineering, zero-shot change detection is still an open problem.

To close this gap, we present _Segment Any Change_, the first change detection model with zero-shot generalization on unseen change types and data distributions. Our approach builds on SAM, the firstpromptable image segmentation model, which has shown extraordinary zero-shot generalization on object types and data distributions. While SAM is extremely capable, it is non-trivial to adapt SAM to change detection and maintain its zero-shot generalization and promptability due to the extreme data collection cost of large-scale change detection labels that would be required to enable promptable training as in the original SAM.

To resolve this problem, we propose a training-free method, _biemporal latent matching_, that enables SAM to segment changes in bitemporal remote sensing images while inheriting these important properties of SAM (i.e., promptability, zero-shot generalization). This is achieved by leveraging intra-image and inter-image semantic similarities that we empirically discovered in the latent space of SAM when applying SAM's encoder on unseen multi-temporal remote sensing images. The resulting models, _AnyChange_, are capable of segmenting any semantic change.

AnyChange can yield class-agnostic change masks, however, in some real-world application scenarios, e.g., disaster damage assessment, there is a need for object-centric changes, e.g. to detect how many buildings are destroyed. To enable this capability we propose a point query mechanism for AnyChange, leveraging SAM's point prompt mechanism and our bitemporal latent matching for filtering desired object changes. The user only needs a single click on a desired object, AnyChange with the point query can yield change masks centered on this object's semantics, i.e., from this object class to others and vice versa, thus achieving object-centric change detection.

We demonstrate the zero-shot prediction capabilities of AnyChange on several change detection datasets, including LEVIR-CD (Chen and Shi, 2020), S2Looking (Shen et al., 2021), xView2 (Gupta et al., 2019), and SECOND (Yang et al., 2021). Due to the absence of published algorithms for zero-shot change detection, we also build baselines from the perspectives of zero-shot change proposal, zero-shot object-centric change detection, and unsupervised change detection. AnyChange outperforms other zero-shot baselines implemented by DINOv2 (Oquab et al., 2023) and SAM

Figure 1: **Zero-Shot Change Detection with AnyChange** on a wide range of application scenarios in geoscience. Each subfigure presents the pre-event image, the post-event image, and their change instance masks in order. The boundary of each change instance mask is rendered by cyan, and meanwhile, these change masks are also drawn on pre/post-event images to show more clearly where the change occurred. The color of each change mask is used to distinguish between different instances.

with different matching methods in terms of zero-shot change proposal and detection. From the unsupervised change detection perspective, AnyChange beats the previous state-of-the-art model, I3PE (Chen et al., 2023), setting a new record of 48.2% F\({}_{1}\) on SECOND. We show some qualitative results in Fig. 1, demonstrating the zero-shot prediction capabilities of AnyChange on a wide range of application scenarios (i.e., urbanization, disaster damage assessment, de-agriculturalization, deforestation, and natural resource monitoring). The contributions of this paper are summarized as follows:

* **AnyChange**, the first zero-shot change detection model, enables us to obtain both instance-level and pixel-level change masks either in a fully automatic mode or interactively with simple clicks.
* **Bitemporal Latent Matching**, a training-free adaptation method, empowers SAM with zero-shot change detection by leveraging intra-image and inter-image semantic similarities of images in SAM's latent space.
* **Zero-Shot Change Detection** is explored for the first time. We demonstrate the effectiveness of AnyChange from four perspectives, i.e., zero-shot change proposal, zero-shot object-centric change detection, unsupervised change detection, and label-efficient supervised change detection, achieving better results over strong baselines or previous SOTA methods.

## 2 Related Work

**Segment Anything Model**(Kirillov et al., 2023) is the first foundation model for promptable image segmentation, possessing strong zero-shot generalization on unseen object types, data distributions, and tasks. The training objective of SAM is to minimize a class-agnostic segmentation loss given a series of geometric prompts. Based on compositions of geometric prompts, i.e., prompt engineering, SAM can generalize to unseen single-image tasks in a zero-shot way, including edge detection, object proposal, and instance segmentation. Our work extends SAM with zero-shot change detection for bitemporal remote sensing images via a training-free adaptation method, extending the use of SAM beyond single-image tasks.

**Segment Anything Model for Change Detection.** SAM has been used for change detection via a "parameter-efficient fine-tuning" (PEFT) paradigm (Mangrulkar et al., 2022), such as SAM-CD (Ding et al., 2023) that used Fast-SAM (Zhao et al., 2023) as a frozen visual encoder and fine-tuned adapter networks and the change decoder on change detection datasets in a fully supervised way. This model does not inherit the most two important properties of SAM, i.e., promptability and zero-shot generalization. Fine-tuning in a promptable way with large-scale training change data may achieve these two properties, however, collecting large-scale bitemporal image pairs with class-agnostic

Figure 2: **Segment Any Change Models, AnyChange**. SAM forward: given grid points \(\{_{i}\}\) as prompts and input images, SAM produces object masks \(\{_{t,i}\}\) and image embedding \(}\) on the image at time \(t\). **Bitemporal Latent Matching** does a bidirectional matching to compute the change confidence score for each change proposal, and then top-k sorting or thresholding is applied for zero-shot change proposal and detection. **Point Query** allows users to click some points (the case of two points in this subfigure) with the same category to filter class-agnostic change masks via semantic similarity for object-centric change detection.

change annotations is non-trivial (Tewkesbury et al., 2015; Zheng et al., 2023), thus no such method exists in the current literature. Our work introduces a new and economic adaptation method for SAM, i.e., training-free adaptation, guaranteeing these two properties with zero additional cost, making zero-shot change detection feasible for the first time.

Unsupervised Change Detection.The most similar task to zero-shot change detection is unsupervised change detection (Coppin and Bauer, 1996), however zero-shot change detection is a more challenging task. They both require models to find class-agnostic change regions, and the main difference is that zero-shot change detection also requires models to generalize to unseen data distributions. From early model-free change vector analysis (CVA) (Bruzzone and Prieto, 2000; Bovolo and Bruzzone, 2006) to advanced deep CVA (Saha et al., 2019) and I3PE (Chen et al., 2023), unsupervised change detection methods have undergone a revolution enabled by deep visual representation learning. These model-based unsupervised change detection methods need to re-train their models on new data distributions. Our proposed model is training-free and can achieve comparable or even better performance for unsupervised change detection.

## 3 Segment Any Change Models

This paper introduces _Segment Any Change_ to resolve the long-standing open problem of zero-shot change detection (see Sec. 3.1). As illustrated in Fig. 2, we propose a new type of change detection models that support zero-shot prediction capabilities and generalization on unseen change types and data distributions, allowing two output structures (instance-level and pixel-level), and three application modes (fully automatic, semi-automatic with a custom threshold, and interactive with simple clicks). AnyChange achieves the above capabilities building on SAM in a training-free way.

### Background

**Preliminary: Segment Anything Model** (SAM) is a promptable image segmentation model with an image encoder of ViT (Dosovitskiy et al., 2021), a prompt encoder, and a mask decoder based on transformer decoders. As Fig. 2(a) shows, given as input a single image at time \(t\), the image embedding \(_{t}\) is extracted from its image encoder. Object masks \(\{_{t,i}\}\) can be obtained by its mask decoder given \(_{t}\) and dense point prompts obtained by feeding grid points \(\{_{i}\}\) into its prompt encoder.

**Problem Formulation: Zero-Shot Change Detection** is formulated at the pixel and instance levels. Our definition of "zero-shot" is similar to that of SAM, i.e., the model without training on change detection tasks can transfer zero-shot to change detection tasks and new image distributions. \(=\{0,1\}\) denotes a set of classes of non-change and change. A pixel position or instance area belongs to the change class if the corresponding semantic categories at different times are different. This means that the model should generalize to unseen change types, even though all change types are merged into a class of 1. The input is a bitemporal image pair \(_{t},_{t+1}^{h w*}\), where the image size is \((h,w)\) and \(*\) denotes the channel dimensionality of each image. For pixel level, the model is expected to yield a pixel-level change mask \(^{h w}\). For instance level, the model is expected to yield an arbitrary-sized set of change masks \(\{_{i}\}\), where each instance \(_{i}\) is a polygon.

### Exploring the Latent Space of SAM

Motivated by the experiments in Kirillov et al. (2023) on probing the latent space of SAM, we known there are potential semantic similarities between mask embeddings in the same natural image. We further explore the latent space for satellite imagery from both intra-image and inter-image perspectives, thus answering the following two questions:

_Q1: Do semantic similarities exist on the same satellite image?_ Empirically, we find they do. We show this semantic similarity in two ways, i.e., visualizing the first components of principal components analysis (PCA) (Oquab et al., 2023) and probing the latent space (Kirillov et al., 2023), as Fig. 3 (a) shows. Observing the first three PCA components, geospatial objects with the same category have a similar appearance in this low-dimensional subspace. This suggests that this satellite image embedding from SAM encodes the semantics of geospatial objects reasonably well. Furthermore, we do the same latent space probing experiment as Kirillov et al. (2023) did, but on satellite images, and present the results in Fig. 3 (a) (bottom). We compute the mask embedding of object proposals and manually select three proposals with different categories (water, building, and bareland) as queries.

By matching the query embedding with other mask embeddings, we obtain the most similar object proposals with the query proposal. We find that the most similar object proposals mostly belong to the same category as the query.

_Q2: Do semantic similarities exist on satellite images of the same location collected at different times?_ Empirically, we find they do. To verify this, we introduce a new satellite image from the same geographic area but at a different time \(t_{1}\). The above satellite image is captured at time \(t_{2}\). Different from the above latent space probing experiment, we use three object proposals with different spatial positions from the image at \(t_{1}\), as queries. These three queries have the same category, i.e., building. By matching with all proposals from the image at \(t_{2}\), we obtain three basically consistent results (F\({}_{1}\) of 68.1%\(\)0.67% and recall of 96.2%\(\)0.66%), as shown in Fig. 3 (b). This suggests that this semantic similarity exists on the satellite images at different times, even though the images have different imaging conditions because they were taken at different times.

From the above empirical study, we find that there are intra-image and inter-image semantic similarities in the latent space of SAM for unseen satellite images. These two properties are the foundation of our training-free adaptation method.

### Bitemporal Latent Matching

Based on our findings, we propose a training-free adaptation method, namely _bitemporal latent matching_, which bridges the gap between SAM and remote sensing change detection without requiring for training or architecture modifications.

The main idea is to leverage the semantic similarities in latent space to identify changes in bitemporal satellite images. Given the image embedding \(_{t}\) and object proposals \(_{t}=\{_{t,i}\}_{i[1,2,,N_{t}]}\) generated from SAM on the satellite image at time \(t\), each object proposal \(_{t,i}^{h w}\) is a binary mask. We can compute the mask embedding \(_{t,i}=_{t}[_{t,i}] R^{d_{m}}\) by averaging the image embedding \(_{t}\) over all non-zero positions indicated by the object proposal \(_{t,i}\). Next, we introduce a similarity metric to measure the semantic similarity. To do this, we need to consider the statistical properties of the image embeddings from SAM. In particular, SAM's image encoder uses layer normalization (Ba et al., 2016). This means that the mask embedding has zero mean and unit variance if we drop the affine transformation in the last layer normalization, i.e., the variance \(D(_{t,i})=d_{m}^{-1}_{j}(_{t,i}[j])^{2}=1\), thus we have the mask embedding's \(_{2}\) norm \(\|_{t,i}\|_{2}=}\), which is a constant since \(d_{m}\) is the channel dimensionality. Given this, cosine similarity is a suitable choice to measure similarity between two mask embeddings since they are on a hypersphere with a radius of \(}\), and differences are encoded by their directions. Therefore, we propose to use negative cosine similarity as the change confidence score \(c(_{i},_{j})\) for mask embeddings \(_{i}\) and \(_{j}\):

\[c(_{i},_{j})=-_{i}_{j}}{d_{m}} \]

The next question is which two mask embeddings to use to compute the change confidence score. The real-world change is defined at the same geographic location from time \(t\) to \(t+1\). This means

Figure 3: Empirical evidence on semantic similarity on (a) the same satellite image and (b) the satellite images at different times. **(a)** The visualization of the first three PCA components indicates the objects with the same category are well matched with each other in SAM’s latent space; Three red point queries confirm the existence of semantic similarity on the same satellite image. **(b)** The object proposals (indicated by red points) from the satellite image at \(t_{1}\) as queries are used to match all proposals from the satellite image at \(t_{2}\). (best viewed with zoom, especially for the point query)

that it is comparable only if two mask embeddings cover the approximate same geographic region. Therefore, we additionally compute the mask embedding \(}_{t+1,i}=_{t+1}[_{t,i}]^{d_{m}}\) on the image embedding \(_{t+1}\) using the same object proposal \(_{t,i}\). We then compute the change confidence score \(c(_{t,i},}_{t+1,i})\) for the change at \(_{t,i}\) from \(t\) to \(t+1\).

Since we need to guarantee the temporal symmetry (Zheng et al., 2021, 2022) of class-agnostic change, we propose to match the object proposals bidirectionally. To this end, the change confidence score \(c(_{t+1,i},}_{t,i})\) for the change at \(_{t+1,i}\) from \(t+1\) to \(t\) is also computed, where \(}_{t,i}=_{t}[_{t+1,i}]^{d_{m}}\) is computed on the image embedding \(_{t}\) with the same object proposal \(_{t+1,i}\). Afterwards, we can match object proposals \(_{t}\) and \(_{t+1}\) bidirectionally, and \((N_{t}+N_{t+1})\) change proposals with their confidence score are obtained in total. We propose to finally obtain change detection predictions by sorting by confidence scores and selecting the top-k elements or by angle thresholding. The pseudo-code of Bitemporal Latent Matching is in Appendix B.

### Point Query Mechanism

To empower AnyChange with interactive change detection with semantics, we combine our bitemporal latent matching with the point prompt mechanism of SAM, thus yielding the point query mechanism. Given as input a set of single-temporal point \(\{_{t,i}\}=(x_{t,i},y_{t,i})\) with the same category, \(t\) denote that this point belongs to the image at time \(t\), and \((x,y)\) indicates the spatial coordinate of image domain. The object proposals \(\{_{_{t,i}}\}\) can be obtained via SAM's point prompts. Following our bitemporal latent matching, we then compute their average mask embedding \(}_{_{t}}=n^{-1}_{1}^{n}_{_{t,i}}\) and match it with all proposals \(\{_{t},_{t+1}\}\) via cosine similarity. In this way, the object-centric change detection results can be obtained via a custom angle threshold.

## 4 Experiments

In this section, we demonstrate the two most basic applications of AnyChange, i.e., (i) zero-shot change proposal and detection and (ii) change data engine. We conduct experiments from these two perspectives to evaluate our method.

### Zero-Shot Object Change Proposals

**Datasets:** We use four commonly used change detection datasets to evaluate AnyChange. The first three, i.e., LEVIR-CD (Chen and Shi, 2020), S2Looking (Shen et al., 2021), and xView2 (Gupta et al., 2019), are building-centric change detection datasets. SECOND (Yang et al., 2021) is a multi-class (up to 36 change types) urban change detection dataset with full annotation. For zero-shot object proposal evaluation, we convert their labels into binary if the dataset has multi-class change labels.

**Metrics:** Conventional change detection mainly focuses on pixel-level evaluation using F\({}_{1}\), precision, and recall. More and more real-world applications have started to focus on instance-level change, also called "change parcel". Based on this requirement and AnyChange's capability of predicting instance change, we adapt the evaluation protocol of the zero-shot object proposal (Kirillov et al., 2023) for the zero-shot change proposal since they have the same output structure. The metric is mask AR@1000 (Lin et al., 2014). Note that change proposals are class-agnostic, therefore, for the first three building-centric datasets, we cannot obtain accurate F\({}_{1}\) and precision due to incomplete "any change" annotations. These two metrics are only used for reference and to see whether the model is close to the naive baseline (predict all as "change" class, and vice versa). Here we mainly focus on recall for both pixel- and instance-levels.

**Baselines:** AnyChange is based on SAM, however, there is no SAM or other VFM2-based zero-shot change detection model that can be used for comparison in the current literature. For a fair comparison, we build three strong baselines based on DINov2 (Oquab et al., 2023) (a state-of-the-art VFM) or SAM. The simple baseline is CVA (Bruzzone and Prieto, 2000), a model-free unsupervised change detection method based on \(_{2}\) norm as dissimilarity and thresholding. We build "DINov2+CVA", an improved version with DINov2 using an idea similar to DCVA (Saha et al., 2019). We build "SAM+Mask Match", which follows the macro framework of AnyChange and replaces the latent match with the mask match that adopts the IoU of masks as the similarity. "SAM+CVA Match" follows the same idea of AnyChange but adopts the negative \(_{2}\) norm of feature difference as the similarity to compute pixel-level change map via SAM feature-based CVA first. The instance-level voting is then adopted to obtain change proposals. We also build each "Oracle" version of AnyChange as an upper bound, where we fine-tune SAM with LoRA (\(r=32\)) (Hu et al., 2022) and train a change 

[MISSING_PAGE_FAIL:7]

-1.9%, -1.4%) on these four datasets. We believe this sensitivity to radiation variation is acceptable for most applications.

### Zero-shot Object-centric Change Detection

Zero-shot object change proposal yields class-agnostic change masks. The point query mechanism can convert the class-agnostic change into object-centric change via simple clicks of the user on a single-temporal image, thus providing an interactive mode for AnyChange. This step is typically easy for humans. Here we evaluate the point query on three building-centric change datasets.

**Results:** We demonstrate the effect of the point query in Fig. 4. Without the point query, AnyChange yields class-agnostic change masks, including building changes, vegetation changes, etc. With a single-point query on a building, we can observe that change masks unrelated to the building are filtered out. Further clicking two more buildings to improve the stability of mask embedding, we find the building changes previously missed are successfully recovered. Table 4 quantitatively reflects this mechanism. With a single-point query, the zero-shot performances on three datasets significantly gain \( 15\%\) F\({}_{1}\) score. This improvement hurts recall as a cost, however it achieves a better trade-off between precision and recall.

After increasing to three-point queries, the recalls of the model on three datasets get back to some extent, and the model has comparable precision with the single-point query. The zero-shot performances on three datasets gain \( 3\%\) F\({}_{1}\) further. These results confirm the effectiveness of the point query as a plugin for AnyChange to provide an interactive mode.

### AnyChange as Change Data Engine

AnyChange can provide pseudo-labels for unlabeled bitemporal image pairs with zero or few annotation costs. To evaluate this capability, we conduct experiments on two typical tracks for remote sensing change detection: supervised object change detection and unsupervised change detection.

**Training recipe:** On S2Looking, we train the model on its training set with pure pseudo-labels of AnyChange with ViT-H with a single-point query. All training details follow Zheng et al. (2023) except the loss function. On SECOND, we train the model on its training set with pure pseudo-labels of AnyChange with ViT-H via fully automatic mode, and other details follow Zheng et al. (2022). For

    &  &  &  \\ Method & F\({}_{1}\) & Prec. & Rec. & Enc. & F\({}_{1}\) & Prec. & Rec. & F\({}_{1}\) & Prec. & Rec. & Rec. \\  AnyChange-H & 23.0 & 13.3 & 85.0 & 6.4 & 3.3 & 93.2 & 9.4 & 5.1 & 62.2 \\ + Point Query & & & & & & & & & & & & & \\
1 point query & 38.6 & 30.9 & 51.4 & 21.8 & 16.9 & 34.1 & 25.1 & 24.8 & 25.4 & & & & & \\  & \(\)15.6 & & & & \(\)15.4 & & & & & & & & \\
3 point queries & 42.2 & 28.5 & 81.1 & 24.0 & 14.5 & 68.5 & 28.1 & 20.0 & 47.2 & & & & & \\  & \(\)19.2 & & & & \(\)17.6 & & & & & & & & \\   

Table 4: **Zero-shot Object-centric Change Detection.** All results of introducing semantics from the point query are accurate estimations since the detected changes are object-centric.

    &  &  &  &  \\ Direction & F\({}_{1}\) & Prec. & Rec. & max ARI & F\({}_{1}\) & Prec. & Rec. & max ARI & F\({}_{1}\) & Prec. & Rec. & max ARI & F\({}_{1}\) & Prec. & Rec. & max ARI \\  bidirectional & 23.4 & 13.7 & 83.0 & 32.6 & 7.4 & 3.9 & 94.0 & 48.3 & 13.4 & 7.6 & 59.3 & 27.8 & 44.6 & 30.5 & 83.2 & 27.0 \\ from \(t\) to \(t+1both these two cases, the loss function is BCE loss with a label smoothing of 0.4. See Appendix C.2 for more details.

**Supervised Object Change Detection Results.** Table 5 presents standard benchmark results on the S2Looking dataset, which is the one of most challenging building change detection datasets. From a data-centric perspective, we use the same architecture with different fine-tuned data. The model architecture adopts ResNet18-based ChangeStar (1\(\)96) (Zheng et al., 2021) due to its simplicity and good performance. We set the fine-tuning on 100% ground truth as the upper bound, which achieved 66.3% F\({}_{1}\). We can observe that fin-tuning on the pseudo-labels of AnyChange with ViT-H yields 40.2% F\({}_{1}\) with 3,500-pixel annotations3. Leveraging AnyChange, we achieve 61% of the upper bound at a negligible cost (\( 10^{-5}\)% full annotations). We also compare it to the model trained with fewer annotations (1% and 0.1%). We find that their performances are reduced by a significant amount, and are inferior to the model trained with pseudo-labels from AnyChange. This confirms the potential of AnyChange as a change data engine for supervised object change detection.

**Unsupervised Change Detection Results.** AnyChange's class-agnostic change masks are natural pseudo-labels for unsupervised change detection. We also compare our AnyChange with unsupervised change detection methods. In Table 6, we find that AnyChange with ViT-B in a zero-shot setting improves over the previous state-of-the-art method, I3PE (Chen et al., 2023). To learn the biases on the SECOND dataset, we trained a ChangeStar (1\(\)256) model with pseudo-labels of AnyChange on the SECOND training set. The setting follows I3PE

Figure 4: Examples of **Point Query Mechanism**. The effects of w/o point query, one-point query, and three-point queries are shown in sequence from left to right. (best viewed digitally with zoom, especially for the red points)

   Method & Backbone & Fine-tuning on & \#labeled pixels\({}_{i}\) & F\({}_{1}\) \(\) & Prec. & Rec. & \#Params. & \#Flops \\  FC-Siam-Diff (Daudt et al., 2018) & - & 100\% GT & \(1.2 10^{10}\) & 13.1 & 83.2 & 15.7 & 1.3M & 18.7G \\ STANet (Chen \& Shi, 2020) & R-18 & 100\% GT & \(1.2 10^{10}\) & 45.9 & 38.7 & 56.4 & 16.9M & 156.7G \\ CDNet (Chen et al., 2021a) & R-18 & 100\% GT & \(1.2 10^{10}\) & 60.5 & 67.4 & 54.9 & 14.3M & - \\ BiT (Chen et al., 2021b) & R-18 & 100\% GT & \(1.2 10^{10}\) & 61.8 & 72.6 & 53.8 & 11.9M & 34.7G \\ ChangeStar (1\(\)96) (Zheng et al., 2021) & R-18 & 100\% GT & \(1.2 10^{10}\) & 66.3 & 70.9 & 62.2 & 16.4M & 65.3G \\
**+ Changen-90k** (Zheng et al., 2023) & R-18 & 100\% GT & \(1.2 10^{10}\) & 67.1 & 70.1 & 64.3 & 16.4M & 65.3G \\ ChangeStar (1\(\)96) (Zheng et al., 2021) & MiT-B1 & 100\% GT & \(1.2 10^{10}\) & 64.3 & 69.3 & 59.9 & 18.4M & 67.3G \\
**+ Changen-90k** (Zheng et al., 2023) & MiT-B1 & 100\% GT & \(1.2 10^{10}\) & 67.9 & 70.3 & 65.7 & 18.4M & 67.3G \\  ChangeStar (1\(\)96) & R-18 & 1\% GT & \(1.4 10^{8}\) & 37.2 & 63.1 & 26.3 & 16.4M & 65.3G \\ ChangeStar (1\(\)96) & R-18 & 0.1\% GT & \(4.7 10^{6}\) & 9.2 & 10.0 & 8.5 & 16.4M & 65.3G \\ ChangeStar (1\(\)96) (Ours) & R-18 & AnyChange & \(3.5 10^{3}\) & 40.2 & 40.4 & 39.9 & 16.4M & 65.3G \\   

Table 5: **Supervised Object Change Detection**. Comparison with the state-of-the-art change detectors on the **S2Looking** test. “R-18”: ResNet-18. The amount of Flops was computed with a float32 tensor of shape  as input.

(Chen et al., 2023), thus we align their backbone and use ResNet50 for ChangeStar (1\(\)256). The results show that two variants of unsupervised Notably, based on pseudo-labels of AnyChange with ViT-B, our model set a new record of 48.2% F\({}_{1}\) on the SECOND dataset for unsupervised change detection. Besides, we obtain some useful insights for unsupervised change detection: (_i_) deep features from VFMs significantly assist unsupervised change detection models since DINOv2+CVA beats all advanced competitors except I3PE. Before our strong baseline, DINOv2+CVA, CVA has been always regarded as a simple and ineffective baseline for unsupervised change detection. (_ii_) dataset biases are helpful for in-domain unsupervised change detection since our model trained with pseudo-labels achieves higher performance than these pseudo-labels. This indicates the model learns some biases on the SECOND dataset, which may be change types and style.

## 5 Conclusion

We present the segment any change models (AnyChange), a new type of change detection model for zero-shot change detection, allowing fully automatic, semi-automatic with custom threshold, and interactive mode with simple clicks. The foundation of all these capabilities is the intra-image and inter-image semantic similarities in SAM's latent space we identified on multi-temporal remote sensing images. Apart from zero-shot change detection, we also demonstrated the potential of AnyChange as the change data engine and demonstrated its superiority in unsupervised and supervised change detection. AnyChange is an out-of-the-box zero-shot change detection model, and a step forward towards a "foundation model" for the Earth vision community.