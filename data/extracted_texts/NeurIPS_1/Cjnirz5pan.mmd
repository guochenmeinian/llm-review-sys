# SAFE: Slow and Fast Parameter-Efficient Tuning for Continual Learning with Pre-Trained Models

Linglan Zhao\({}^{,*}\) Xuerui Zhang\({}^{@sectionsign,*}\) Ke Yan\({}^{,*@sectionsign}\) Shouhong Ding\({}^{}\) Weiran Huang\({}^{,@sectionsign}\)

\({}^{}\) MIFA Lab, Qing Yuan Research Institute, SEIEE, Shanghai Jiao Tong University

\({}^{}\) Tencent YouTu Lab \({}^{@sectionsign}\) Zhejiang University

{linglanzhao, kerwinyan, ericshding}@tencent.com

xrzhang0121@zju.edu.cn, weiran.huang@outlook.com

###### Abstract

Continual learning aims to incrementally acquire new concepts in data streams while resisting forgetting previous knowledge. With the rise of powerful pre-trained models (PTMs), there is a growing interest in training incremental learning systems using these foundation models, rather than learning from scratch. Existing works often view PTMs as a strong initial point and directly apply parameter-efficient tuning (PET) in the first session for adapting to downstream tasks. In the following sessions, most methods freeze model parameters for tackling forgetting issues. However, applying PET directly to downstream data cannot fully explore the inherent knowledge in PTMs. Additionally, freezing the parameters in incremental sessions hinders models' plasticity to novel concepts not covered in the first session. To solve the above issues, we propose a Slow And Fast parameter-Efficient tuning (SAFE) framework. In particular, to inherit general knowledge from foundation models, we include a transfer loss function by measuring the correlation between the PTM and the PET-applied model. After calibrating in the first session, the slow efficient tuning parameters can capture more informative features, improving generalization to incoming classes. Moreover, to further incorporate novel concepts, we strike a balance between stability and plasticity by fixing slow efficient tuning parameters and continuously updating the fast ones. Specifically, a cross-classification loss with feature alignment is proposed to circumvent catastrophic forgetting. During inference, we introduce an entropy-based aggregation strategy to dynamically utilize the complementarity in the slow and fast learners. Extensive experiments on seven benchmark datasets verify the effectiveness of our method by significantly surpassing the state-of-the-art. Code will be available at [https://github.com/MIFA-Lab/SAFE](https://github.com/MIFA-Lab/SAFE).

## 1 Introduction

Continual Learning (CL) requires deep learning models to incrementally incorporate new concepts from open-world data streams, while retaining previously learned knowledge. This presents a more challenging yet practical setting compared to traditional deep learning, which typically recognizes only closed-set categories. A variety of methods have been proposed for continual learning, including regularization-based , rehearsal-based , and dynamic network-based approaches . These methods often assume that the model is trained from scratch, resulting in a substantial performance gap when compared to the joint training upper-bound.

Most recently, with the emergence of powerful pre-trained models, there has been growing interest in utilizing these foundational models as starting points for continual learning . Pre-Trained Models (PTMs) which are often trained on vast datasets, encapsulate a wealth of general knowledge, effectively enhancing the performance of deep learning models in continual learning scenarios. As shown in the left part of Fig. 1(a), for adapting PTMs from pre-training datasets to continual learning datasets, prevailing works resort to parameter-efficient tuning (PET) techniques  in the first session. To restrain catastrophic forgetting, in incremental sessions, these works set parameters of the adapted model frozen  and only update the classification weights in a training-free manner (_i.e._, without gradient updates) to accommodate novel classes.

However, the above methods have two main limitations. First, direct parameter-efficient tuning in the first session will largely lose the general knowledge inherent in PTMs. This is because PTMs are pre-trained on a multitude of datasets while the dataset in the first session only contains relatively limited samples. Without proper transfer mechanisms, the knowledge from PTMs may be overwritten by the adapted model, which impedes the model's generalizability to unseen classes. Second, freezing parameters in the following sessions will hinder the plasticity of the model to further absorb new concepts not learned in the first session, resulting in a sub-optimal solution. Although several efforts have been made to mitigate the second limitation, existing works still face certain constraints such as additional storage requirement , inferior online branch performance  and linearly increased model complexity .

Based on the above observations, in this paper, we propose Slow And Fast parameter-Efficient tuning (SAFE) to address existing challenges. In particular, SAFE demonstrates a unified framework that effectively inherits the generalizability of PTMs using slow parameter-efficient tuning (S-PET) and provides sufficient plasticity to learn task-specific knowledge in each incremental session using the fast one (F-PET). Meanwhile, SAFE does not require storing class distributions for data replay and only incurs constant-level additional computation and memory costs.

To achieve the above goals, SAFE employs distinct strategies for the first and subsequent sessions. In the first session, we focus on explicitly transferring general knowledge from pre-trained models (PTMs) by introducing a knowledge transfer loss. This involves computing a correlation matrix between feature embeddings from the PTM and the model with parameter-efficient tuning (PET). The diagonal elements of this matrix are maximized to ensure that the features remain consistent across both models, effectively aligning the PET-applied model's performance with that of the PTM. Simultaneously, minimizing the off-diagonal elements reduces redundancy in the embeddings, enhancing feature discriminability. After this tuning process, parameters can retain generalizable knowledge from the PTM. To prevent forgetting this knowledge, these trained parameters are subsequently frozen, with only the classification weights being updated, thus designating this model as the _slow_ learner.

Figure 1: Comparisons of (a) prevailing PTM-based CL methods  and our Slow And Fast parameter-Efficient tuning (SAFE). The right part (b) illustrates several parameter-efficient tuning (PET) blocks: Adapter , Scale & Shift (SSF) , and Visual Prompt Tuning (VPT) .

In the incremental sessions, to address the plasticity limitations of the slow learner, we introduce a _fast_ learner capable of continuously integrating new concepts. Given the persistent challenge of catastrophic forgetting in continual learning, the slow learner guides the training of the fast learner. Concretely, we employ a feature alignment loss to minimize the distance between the embeddings of both learners on a hypersphere. Additionally, a cross-classification loss is proposed to ensure compatibility between the features of the fast learner and the classification weights of the slow learner, and vice versa. This approach allows the fast learner to assimilate new knowledge without storing exemplars or distributions, while also mitigating forgetting. For robust predictions, an entropy-based aggregation strategy is implemented during inference to dynamically leverage the complementary strengths of the slow and fast learners.

To summarize, the contributions of our paper are three-fold:

* To inherit the generalizable knowledge in PTMs that has been overlooked in existing continual learning works, we propose to explicitly transfer knowledge from the PTM to a slow learner. Once trained, the slow learner can generalize well to classes in incremental sessions.
* For improving the plasticity of CL models, we include a fast learner with guidance from the slow learner to continuously incorporate novel concepts. Moreover, by aggregating both slow and fast learners into a unified framework SAFE, robust predictions can be further made.
* The superiority of SAFE is validated on seven continual learning datasets where our method consistently achieves remarkable state-of-the-art performance. For example, our method surpasses the second-best result on ImageNet-A over \(4\%\).

## 2 Related Work

**Continual Learning.** Traditional continual learning (CL) aims at continuously updating models with data streams from scratch. Existing strategies involve regularization-based approaches [17; 20; 47; 51] which prevent forgetting by regularizing network weights or predictions, rehearsal-based approaches which replay historical data stored in a fixed-sized buffer [4; 9; 14; 25; 31], and architecture-based approaches [1; 43; 55; 44] which dynamically expand models for novel classes. Among these methods, a recent attempt to preserve knowledge based on slow and fast complementary theory has been proposed [3; 22; 45]. Nevertheless, these approaches typically require adjusting all model parameters, which increases the computational burden of the learning process. Contrarily, our Slow And Fast parameter-Efficient tuning (SAFE) framework only requires much fewer learnable parameters as well as fewer resources, while obtaining more favorable performance.

**Continual Learning with Pre-Trained Models.** With the emergence of powerful pre-trained models (PTMs), it has become a hot topic to integrate pre-trained models with CL [29; 53] for better performance. Prompt-based methods [30; 33; 39; 41; 42] utilize prompt tuning to adapt PTMs to new tasks. However, these methods are tailored for Transformers [7; 37] and require an expanding prompt pool with the arrival of new data. First session adaptation methods [2; 23; 52] adapt PTMs solely in the first session and then freeze the model afterward to suppress forgetting [26; 32]. Nevertheless, these works lack plasticity for classes in subsequent sessions. Contrarily, another line of works focuses on continual adjustment [8; 35; 49; 54] to accommodate evolving information. However, the above approaches either require storing data distributions [35; 49] for replay, only obtain inferior online branch performance , or linearly increase complexity with incremental sessions . Compared to existing works, our method provides a flexible framework that boosts generalizability by inheriting PTM's knowledge in the first session and maintains plasticity for incremental classes with constant complexity in a replay-free manner.

## 3 Method

### Problem Definition

Following previous works [8; 23; 49; 52], in this paper, we mainly consider PTM-based CL under a class-incremental learning setting. Formally, the model is trained sequentially on a series of incremental sessions, where \(^{t}=\{(x_{t}^{t},y_{t}^{t})\}_{i=1}^{N_{t}}\{^{t },^{t}\}\) represents the \(t\)-th training set composed of \(N_{t}\) samples, for \(t\{1,2,,T\}\). The sample and label space of \(^{t}\) are denoted by \(^{t}\) and \(^{t}\), where \(^{t}\) is disjoint between different sessions, _i.e._, \( i,j\) and \(i j\), \(^{i}^{j}=\). We follow the replay-free setting, where only \(^{t}\) is accessible in session \(t\). After training in the \(t\)-th session, the model is evaluated on all the seen classes so far: \(^{1:t}=^{1}^{2}^{t}\). In addition, we also validate our method on domain-incremental learning setting, where the data distribution between sessions shifts significantly, _i.e._, \( i,j\) and \(i j\), \(P(^{i}) P(^{j})\), \(^{i}=^{j}\).

### Overall Architecture

For tackling the stability-plasticity dilemma in CL, we draw inspiration from the _complementary learning systems_ theory  to develop a Slow And Fast parameter-Efficient tuning (SAFE) framework, as depicted in Fig. 2. In the first session, the slow learner is tuned to inherit the general knowledge from PTM and is frozen afterward. In the following sessions, the slow learner only updates its classification head using imp , which acts like the _neocortex_ to slowly incorporate novel knowledge without forgetting. Complementary to this, the fast learner with learnable parameters rapidly encodes novel information as the _hippocampus_ for adapting to new classes.

Formally, features extracted from PTM, slow learner and fast learner are denoted as \(f_{l}=_{l}(x)^{d}\), where \(l\{,,\}\) and \(d\) is the feature dimension. To leverage the knowledge of PTMs with few learnable parameters and resources, feature extractors for the slow and fast learners are trained using parameter-efficient tuning (PET) [6; 21; 16] which are referred to as S-PET and F-PET, respectively. Consistent with prior works [23; 52; 54], we mainly consider three types of PETs: Adapter , SSF , and VPT , shown in the right part of Fig. 1.

The classification weights in session \(t\) for the slow and fast learners are symbolized by \(W_{l}^{d|_{1:t}|}\), \(l\{,\}\), where \(|_{1:t}|\) is the number of classes seen so far from session \(1\) to session \(t\). For the slow learner, \(W_{}\) is learned in the first session and expanded using feature centroids of training samples within the same classes  afterward to preserve learned general knowledge. Contrarily, \(W_{}\) is trainable as CL progresses for the plasticity purpose.

In the following sections, we provide the details of slow and fast learner training in Section 3.3 and Section 3.4. After that, discussions about model inference are presented in Section 3.5.

### Slow Learner

Benefiting from pre-training on large-scale resources, pre-trained models (PTMs) inherently possess strong generalizability for downstream tasks. Previous works [23; 35; 52] typically view the PTM as a preferable starting point for continual learning. To bridge the distribution gap between pre-training datasets and downstream datasets, these methods often directly apply PET to PTMs.

Figure 2: An overview of our SAFE framework. In the first session, PTM transfers knowledge to the slow learner for better generalization. In sessions \(t>1\), the fast learner is guided by the slow learner for enhanced plasticity. During inference, robust predictions are made by dynamic aggregation.

However, without proper transfer mechanisms, models directly tuned on downstream data cannot effectively inherit the general knowledge from PTMs. More seriously, the intrinsic knowledge in PTM may be overwritten during adaptation to the recent dataset, since it often contains relatively limited samples. To solve the above issues, we propose to effectively squeeze out information from PTMs and explicitly transfer it to adapted models.

Concretely, in the first session, we calculate the cross-correlation matrix \(^{d d}\) between the features of the slow learner and the PTM:

\[_{i,j}=}_{k=1}^{N_{b}}[_{}(x_{k}) ]_{i}[_{}(x_{k})]_{j}, \]

where \(N_{b}\) is the batch size, \(d\) is the feature dimension and "\(\)" denotes element-wise multiplication1. Moreover, \(i\) and \(j\) index the dimensions of the features and matrices. In fact, the correlation matrix characterizes the relationship between feature embeddings of PTM and the slow learner. The \(i\)-th row and \(j\)-th column of \(\) measures the correlation between the \(i\)-th feature dimension (also termed as channel or pattern in the literature) of the PTM and the \(j\)-th feature dimension of the slow learner.

To encourage the PET-applied model to mimic the performance of the PTM, we maximize the elements in the diagonal. This maximizing term ensures the slow learner can learn invariant feature components that match the statistics of the PTM:

\[_{}=_{i=1}^{d}(1-_{i,i})^{2}. \]

Additionally, we reduce the redundancy between patterns in embeddings to enhance discriminability. This can obtained by decreasing the off-diagonal elements in \(\) with \(_{}\):

\[_{}=_{i=1}^{d}_{j i} _{i,j}^{2}. \]

Combined with the classification loss \(_{}\) using cross-entropy (CE):

\[_{}=}_{i=1}^{N_{b}}(W_{ }^{}_{}(x_{i}),y_{i}), \]

where "\(\)" denotes matrix multiplication, the overall loss function during the first training session is defined as:

\[_{}=_{}+_{ }_{}+_{}_{ }. \]

In Eq. (5), \(_{}\) and \(_{}\) are the balancing hyper-parameters. Intuitively, the joint optimization of three losses makes the adapted model simultaneously acquire distribution-specific knowledge based on \(_{}\) and inherit general knowledge of the PTM using \(_{}\) and \(_{}\). As a result, the slow model can better generalize to incoming classes even unseen in the first training session.

### Fast Learner

Although solely using the slow learner with general features already obtains competitive performance, the plasticity of the model is hindered due to its frozen parameters in the following sessions. To strike a balance between stability and plasticity, we adopt the fast learner to continuously learn episodic information for novel classes. However, updating representations without data reply will lead to semantic drift [35; 46; 49], causing catastrophic forgetting of previously learned knowledge. Existing works to address this problem either store additional data distributions [35; 49] or require sophisticated drift estimations after each session [35; 46]. Compared to previous works, our method imposes no such constraints, and aligns the models before and after updates in a single embedding space, essentially addressing semantic drift.

First, the fast learner is trained with guidance from the slow learner using feature alignment to preserve prior representations. Specifically, the distance of feature embedding from both models is minimized on a hypersphere to alleviate forgetting:

\[_{}=}_{i=1}^{N_{b}}(1-(_{ }(x_{i}),_{}(x_{i}))), \]

where \(N_{b}\) is the batch size and \(\) denotes cosine similarity of two vectors.

Furthermore, we utilize cross-classification which contains a fast-to-slow loss and a slow-to-fast loss to maintain previous decision boundaries. For fast-to-slow calibration, we feed features from the fast learner to the classification layer of the slow learner. This objective makes features from the fast model compatible with the decision boundaries of the slow one to suppress semantic drift. Moreover, since the classification weight vector of each class can be viewed as a _prototype_ of that class [28; 34], we also use these vectors as inputs for further preserving knowledge from previous sessions:

\[_{}=}_{i=1}^{N_{b}}(W_{ }^{}_{}(x_{i}),y_{i})+_{1:t-1}|}_{j=1}^{|_{1:t-1}|}(W_{}^{} W_{}^{(j)},j), \]

where \(W_{}^{(j)}^{d}\) denotes the \(j\)-th column of \(W_{}\), which is also the prototype for class \(j\) in the fast learner. Similarly, slow-to-fast loss \(_{}\) can be derived by swapping the \(\) and slow terms in Eq. (7). After that, the cross-classification loss can be defined as \(_{}=_{}+_{}\).

Along with the classification loss \(_{}\) in Eq. (4) applied to the fast learner, the optimization objective in the incremental phase is defined as:

\[_{}=_{}+_{}+_{}_{}, \]

where \(_{}\) is the balancing hyper-parameter. The loss function \(_{}\) smoothly adapts the fast learner to new knowledge while enforcing consistency with previously acquired knowledge, which boosts the plasticity of the model without severe forgetting.

### Model Inference

Since the slow learner inherits general knowledge and the fast learner contains task-adaptive knowledge, we can obtain robust predictions by utilizing the complementarity of them. We first introduce the inference using a single learner and then provide aggregation strategy based on both learners.

**Single-learner-based Inference.** Following previous work [23; 26], instead of directly using the classification weights \(W_{l}\) and features \(_{l}(x)\), \(l\{,\}\) for prediction, we take advantage of second-order statistics and prototype information for better performance. Formally, given a test sample \(x\), the predicted logits of each learner \(_{l}\) are calculated as:

\[_{l}=_{l}^{}(G+ I)^{-1} h_{l}(x) ^{|_{1:t}|}, \]

where \(\) is a hyper-parameter for regularization, \(h_{l}(x)^{M}\) is projected feature of \(x\) and classification weights \(_{l}^{M|_{1:t}|}\) is composed of summations of projected features with same class labels. Gram matrix \(G^{M M}\) is cumulated based on training data from session \(1\) to \(t\):

\[G=_{s=1}^{t}_{i=1}^{N_{t}}h_{l}(x_{i}^{*}) h_{l}(x_{i}^{*})^{ },\ h_{l}(x_{i}^{*})=(W_{}^{}_{l}(x_{i}^{*})). \]

In Eq. (10), \(W_{}^{d M}\) is the projection matrix with each column sampled from \((0,^{2}I)\), \(\) is a nonlinear activation function and \(I\) denotes the identity matrix. Mathematically, Eq. (9) defines a more general form of regular linear prediction. When \(W_{}\) is \(I\) and \(\) is not applied, it degrades to a ridge regression . Moreover, if \(G\) is removed, the classifier further reduces to NCM .

**Aggregation-based Inference.** As discussed in the above sessions, slow and fast learners excel in handling classes from different sessions. Due to its plasticity, the fast learner can better recognize categories from the latest several sessions but shows limited performance on the old ones caused by potential forgetting. Contrarily, despite limited novel concept adaptation, the slow learner can capture historical knowledge thanks to its stability. Intuitively, when dealing with proficient categories, the model exhibits higher confidence in predictions. Motivated by this, we use entropy to measure the confidence and dynamically aggregate the logits for robust predictions.

Given a test sample, we compute the entropy of predictions using \(=-_{i}p_{i} p_{i}\) for each learner, obtaining \(_{}\) and \(_{}\), where \(p=()\) is predicted probability. As lower entropy indicates less uncertainty in predictions, the confidence of each learner can be represented by \([_{},_{}]=([- _{},-_{}])\), where \(\) is a scalar to control the peakiness of output distributions. After that, the aggregated logits \(_{}\) automatically assign higher weights to predictions with higher confidence, and can be obtained using a convex combination:

\[_{}=_{}_{}+_{}_{}. \]

Finally, the prediction is obtained using the index of the max element in \(_{}\) in session \(t>1\), while using \(_{}\) instead in session \(1\) since the fast learner is not available in that session.

## 4 Experiments

In this section, we first introduce the implementation details of our proposed method SAFE and then compare it to the state-of-the-art on seven popular benchmark datasets. After that, detailed ablative experiments are conducted to validate the effectiveness of each component.

### Experimental Setups

**Datasets and Evaluation.** Following previous methods [23; 52; 54], our evaluations are conducted on seven benchmark datasets: CIFAR100 , ImageNet-R (IN-R) , ImageNet-A (IN-A) , CUB200 , Omnibenchmark (OB) , VTAB  and DomainNet . Previous state-of-the-art PTM-based CL methods are chosen for comparison, including L2P , DualPrompt , CODAPPrompt , ADAM , RanPAC , SSIAT , and SLCA . We adopt final accuracy \(_{T}\) and average accuracy \(_{avg}=_{t=1}^{T}_{t}\) as evaluation metrics.

**Implementation Details.** Consistent to existing works , we adopt ViT-B/16-IN1K and ViT-B/16-IN21K as the PTM and apply Adapter , SSF  or VPT  for parameter-efficient tuning (Appendix A). In each session, we train the model for 20 epochs using SGD optimizer, weight decay of 0.0005, momentum of 0.9, and a cosine annealing schedule where learning rate starts from 0.01 and decays to 0. The batch size is set to 48. In addition, \(\) in Eq. (9) is selected based on the performance on the training data similar to . For other hyper-parameters used in our method, we find \(_{}=0.1\), \(_{}=100\), \(_{}=50\), \(=1\) is a reasonable set of default choices. Detailed hyper-parameter sensitivity analyses are provided in Appendix D.

### Comparisons with State-of-The-Arts

In this section, we compare the proposed method SAFE with several state-of-the-art approaches across seven datasets: CIFAR100, ImageNet-R, ImageNet-A, Omnibenchmark, CUB200, VTAB and DomainNet. For fairness, all methods are implemented with the same ViT  backbones.

The class-incremental learning results from the final session are reported in Table 2. As shown in Table 2, our method consistently achieves the best performance among all benchmarks. Notably, we significantly surpass the second-best result on ImageNet-A by \(4.4\%\). When compared to methods storing additional data distributions for replay [35; 49], our method is replay-free and can still outperform these methods by a significant margin. In addition, we improve the average accuracy over six datasets by \(2.1\%\) compared to the previous best approach . The aforementioned superiority can contribute to the generalizability and plasticity of our method within a unified framework.

For domain-incremental learning, results on DomainNet with 6 different domains are summarized in Table 1. Our method SAFE can outperform the second-best result by \(1.2\%\), demonstrating that the

   Method & Final Acc. \\  L2P  & 40.2 \\ S-iPrompts  & 50.6 \\ ADuM  & 50.3 \\ RanPAC  & 66.6 \\  Slow learner & 67.04 \\ Fast learner & 67.49 \\ SAFE (ours) & **67.82** \\   

Table 1: Performance on DomainNet.

proposed framework is applicable to scenarios where the data distribution of the first task diverges significantly from that of subsequent tasks.

### Ablation Study

To investigate the factors contributing to the success of SAFE, we validate the effectiveness of our key components: the slow learner (SL) in Section 3.3, the fast learner (FL) in Section 3.4, and the entropy-based aggregation in Section 3.5. Experiments are primarily conducted on IN-A dataset.

**Effectiveness of the Slow Learner.** We assess the effectiveness of the slow learner from three perspectives. Firstly, as depicted in Table 3, when the slow learner is added to the baseline , the final accuracy increases by \(3.2\%\) and the average accuracy increases by \(2.1\%\). This observation verifies that the slow learner can generalize well to the incremental classes.

Secondly, we expect the slow learner to inherit generalizability from the PTM. To dive deeper into this aspect, we visualize the embeddings of five unseen classes and five seen classes by T-SNE  after the first session adaptation. As shown in Fig. 3, the embedding space of the slow learner exhibits distinct separation between the seen and unseen classes. Note that the feature distributions with SL in the grey ellipse become more separable compared with the baseline method. This illustrates the successful integration of generalization capabilities from the PTM into the slow learner.

Furthermore, we explore other alternatives for transferring generalizability, including feature alignment (FA) by distilling PTM's features, logits alignment (LA) by distilling PTM's predictions, and second-order statistics alignment (SSA) by distilling PTM's covariance. Table 5 presents the average and final accuracy of the substitutions on IN-A, with the best results highlighted in bold. It is observed that our slow learner can consistently outperform these variations, validating its superiority.

**Effectiveness of the Fast Learner.** As shown in the third row of Table 3, compared to the slow learner, using only the fast learner can obtain \(1.1\%\) improvements in the final accuracy. This indicates

   Method & Final & Avg \\  Features Concatenate & 65.59 & 73.22 \\ Logits Add & 65.90 & 73.31 \\ Logits Max & 66.03 & 73.46 \\ Entropy-based Aggregate & **66.56** & **74.71** \\   

Table 4: Ablation study of aggregation.

   Method & Replay & CIFAR & IN-R & IN-A & CUB & OB & VTAB & Avg \\  SLCA  & w/ & 91.5 & 77.0 & 59.8 & 84.7 & 73.1 & 89.2 & 79.2 \\ SSIAT  & & 91.4 & 79.6 & 62.2 & 88.8 & - & 94.5 & - \\  L2P  & & 84.6 & 72.5 & 42.5 & 65.2 & 64.7 & 77.1 & 67.8 \\ DualPrompt  & & 81.3 & 71.0 & 45.4 & 68.5 & 65.5 & 81.2 & 68.8 \\ CODAPPrompt  & w/o & 86.3 & 75.5 & 44.5 & 79.5 & 68.7 & 87.4 & 73.7 \\ ADaM  & & 87.6 & 72.3 & 52.6 & 87.1 & 74.3 & 84.3 & 76.4 \\ EASE  & & 87.8 & 76.2 & 55.0 & 86.8 & 74.9 & 93.6 & 79.1 \\ RanPAC  & & 92.2 & 78.1 & 61.8 & 90.3 & 79.9 & 92.6 & 82.5 \\  SAFE (ours) & w/o & **92.8** & **81.0** & **66.6** & **91.1** & **80.9** & **95.0** & **84.6** \\   

Table 2: Performance comparisons on six class-incremental learning datasets. The final accuracy (%) of each dataset is reported in the table, and the last column presents the averaged accuracy over all the datasets. Methods with/without data replay are noted using “w/” and “w/o”, respectively.

Figure 3: Comparisons with T-SNE visualization.

that the fast learner is properly guided by the slow learner, and thus can continuously adapt to novel classes with suppressed forgetting.

Subsequently, we present the necessity of each regularization term in the fast learner. As shown in Table 6, without \(_{s}\) and \(_{}\), the performance drops to lower than \(10\%\) due to catastrophic forgetting. To alleviate forgetting, both \(_{s}\) and \(_{}\) are applied. Specifically, solely using \(_{s}\) results in an improvement of \(3.1\%\) compared to the baseline, while using only \(_{}\) yields a gain of \(3.9\%\) over the baseline. Moreover, with all the proposed loss functions, the fast learner can obtain the best performance, validating the effectiveness of each regularization term.

**Effectiveness of Aggregation.** As shown in the last row of Table 3, the combination of the slow and fast learners presents the best result. This observation is consistent with the complementary learning systems theory  that memory necessitates the presence of both a slow learner and a fast learner for improved performance.

To gain deeper insights into the necessity of both learners, we elaborate on their final accuracy of classes from each session. In Fig. 4, the slow learner, mimicking the neocortex, initially stores structured information and performs well on relatively old classes (0-119). Conversely, the fast learner, resembling the hippocampus, swiftly adapts novel concepts and excels in more recent classes (120-199). From this perspective, combining these two complementary learners leverages their strengths across the training process, resulting in superior model performance.

In addition, Fig. 5 illustrates how the aggregated model dynamically leverages the strengths of both learners. Concretely, the horizontal axis represents the class indices to which each test sample belongs, while the vertical axis shows the average aggregation weights of each learner assigned to these test samples. It is observed from Fig. 5 that, for classes 120-199, the fast learner consistently shows higher weights, which is consistent with its superior classification accuracy in these classes as

   Method & FT & \(_{}\) & \(_{}\) & Final & Avg \\  Baseline & & & & 62.21 & 72.31 \\ Finetune directly & ✓ & & & 8.16 & 30.73 \\ Finetune w/ \(_{}\) & ✓ & ✓ & & 65.31 & 73.88 \\ Finetune w/ \(_{}\) & ✓ & & ✓ & 66.07 & 74.20 \\ Fast Learner & ✓ & ✓ & ✓ & **66.49** & **74.50** \\   

Table 6: Ablation study of the fast learner.

Figure 4: Validations on the necessity of the aggregation on IN-R. We provide detailed classification accuracy of test samples from different sessions. Results of the slow learner, the fast learner and SAFE are presented for comparison.

Figure 5: Aggregation weights for the slow learner and fast learner on IN-R.

   Method & Final & Avg \\  Baseline & 62.21 & 72.31 \\ Baseline + FA & 62.81 & 73.35 \\ Baseline + LA & 64.06 & 73.70 \\ Baseline + SSA & 63.20 & 73.00 \\ Baseline + \(_{}\) (Slow Learner) & **65.44** & **74.41** \\   

Table 5: Ablation study of the slow learner.

depicted in Fig. 4. For classes 0-119, the slow learner obtains higher weights, generally aligning with its demonstrated stability and better performance on these classes shown in Fig. 4. By adaptively balancing the contributions of both learners, our method achieves a harmonious trade-off between stability and adaptability.

Moreover, we undertake detailed comparisons to other merging strategies to validate the effectiveness of our aggregation choice. As depicted in Table 4, we compare our entropy-based aggregation with three alternatives: feature concatenation, logits addition, and logits max. We report the final and average accuracy, where the results elucidate that the entropy-based aggregation fully leverages both learners and achieves the best performance.

### Memory Usage

In this section, we investigate the number of learnable parameters in different methods and report the parameter-performance comparison. Since no exemplars are stored in our method, the primary storage cost is attributed to the trainable model parameters introduced by parameter-efficient tuning (PET). Although PET entails additional parameters, it is still small relative to the overall size of the pre-trained model (PTM). Moreover, as the parameter-performance trade-off shown in Fig. 6, our method SAFE utilizes a similar scale of parameters as existing PTM-based methods while achieving substantial performance improvements.

## 5 Conclusion

In this paper, we introduced SAFE, a Slow And Fast parameter-Efficient tuning framework for continual learning. Our approach leverages the inherent knowledge in pre-trained models (PTMs) while maintaining model plasticity for novel concepts. By incorporating a transfer loss function, we ensure the preservation of general knowledge from PTMs. In the first session, we calibrate slow efficient tuning parameters to enhance the model's ability to generalize to new classes. To balance stability and plasticity, we fix the slow efficient tuning parameters and continuously update the fast ones, employing a cross-classification loss with feature alignment to prevent catastrophic forgetting. During inference, we introduce an entropy-based aggregation strategy for dynamic utilization of the complementarity between the slow learner and the fast learner. Extensive experiments on seven benchmark datasets demonstrate that our method significantly surpasses the state-of-the-art, validating the effectiveness of our approach.

**Limitations:** Our approach is built upon RanPAC , and as such, it shares some of the same limitations. For instance, our method relies on a strong feature extractor to effectively inherit generalizability from PTMs, making it less suitable for scenarios where training needs to be performed from scratch or starting from rather small tasks. Additionally, our method introduces three hyper-parameters to balance the loss functions during training, as previously discussed. While our experiments demonstrate that a set of default values works well across the benchmark datasets evaluated in our work, we acknowledge that these choices might not be optimal when applied to datasets with essentially different statistical characteristics. Furthermore, slowly updating the slow learner periodically, rather than keeping it fixed in subsequent sessions, may further enhance the model's adaptability and could be a promising direction for future research.