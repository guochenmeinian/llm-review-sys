# Evaluating Copyright Takedown Methods for Language Models

Boyi Wei\({}^{*}\)\({}^{1}\)  Weijia Shi\({}^{*}\)\({}^{2}\)  Yangsibo Huang\({}^{*}\)\({}^{1}\)

Noah A. Smith\({}^{2}\)  Chiyuan Zhang  Luke Zettlemoyer\({}^{2}\)  Kai Li\({}^{1}\)  Peter Henderson\({}^{1}\)

\({}^{1}\)Princeton University \({}^{2}\)University of Washington

[https://cotaeval.github.io/](https://cotaeval.github.io/)

Equal Contribution.

###### Abstract

Language models (LMs) derive their capabilities from extensive training on diverse data, including potentially copyrighted material. These models can memorize and generate content similar to their training data, posing potential concerns. Therefore, model creators are motivated to develop mitigation methods that prevent generating protected content. We term this procedure as _copyright takedowns_ for LMs, noting the conceptual similarity to (but legal distinction from) the Digital Millennium Copyright Act (DMCA) takedown This paper introduces the first evaluation of the feasibility and side effects of copyright takedowns for LMs. We propose CoTaEval, an evaluation framework to assess the effectiveness of copyright takedown methods, the impact on the model's ability to retain uncopyrightable factual knowledge from the training data whose recitation is embargoed, and how well the model maintains its general utility and efficiency. We examine several strategies, including adding system prompts, decoding-time filtering interventions, and unlearning approaches. Our findings indicate that no tested method excels across all metrics, showing significant room for research in this unique problem setting and indicating potential unresolved challenges for live policy proposals.

## 1 Introduction

Large language models (LLMs) are trained on massive amounts of data, largely drawn from across the web (Bommasani et al., 2021). In most countries, explicit policies regarding training on copyrighted material have been lagging behind the development of LLM training techniques. In the US, model creators often cite the fair use doctrine, a legal defense (developed before the LLM era) that allows the use of copyrighted data without permission under certain circumstances (Lemley and Casey, 2021). Nonetheless, litigation has swept the United States and abroad as copyright owners challenge the use of their content for training and deploying foundation models--e.g., _Tremblay v. OpenAI, Inc.,_ (2023); _Kadney v. Meta Platforms, Inc._ (2023). Generally, there is less legal risk, and a more likely fair use defense, if models do not output content substantially similar to the training data (Henderson et al., 2023; Sag, 2023; Lee et al., 2024).

Thus, model creators increasingly seek to use guardrails that prevent their models from regurgitating content. An example is Github Copilot, a code completion model, provides a duplication detection filter. When turned on, "GitHub Copilot checks code completion suggestions with their surrounding code of about 150 characters against public code on GitHub. If there is a match, or a near match, the suggestion is not shown" (GitHub, 2023b). OpenAI's ChatGPT appears to have a similar filter for some types of content, as well as training the model to reject requests that may ask for infringing outputs (Henderson et al., 2023). Such post-training mitigation strategies will be an essential aspect of model deployments. Even if model creators possess licenses and filter pre-training data, they mayunwittingly include copyrighted material that the model could regurgitate. For example, consider if a company licenses Reddit data for training. There is no guarantee that Reddit posts are not themselves infringing, and tracing the provenance of every piece of content is nearly impossible. Therefore, model deployers require a strategy to prevent models from outputting content that are too similar to specific copyrighted data, which they may only notice after training is complete. Noting the conceptual similarity to _DMCA Takedown_, we refer to this procedure as a **copyright takedown** for LMs, or simply _takedown_ when there is no ambiguity. Note unlike a DMCA Takedown, copyright takedown is not a formally defined legal term, and in this paper specifically refers to the post-training procedures applied to prevent an LM from generating texts that are too similar to specific contents. Legal scholars suggest that a takedown mechanism may be a necessary and effective part of future policymaking (Henderson et al., 2023; Pasquale and Sun, 2024; Lee et al., 2024). Yet, a key question remains: _Can "takedown" of copyrighted content be operationalized in the context of large language models?_

This paper introduces the first evaluation of the feasibility and side effects of copyright takedowns in language models. Our benchmark, CoTaEval, considers potential regurgitation of blocklisted content due to both memorized content and content retrieved through retrieval-augmented generation (RAG, Lewis et al., 2020; Shi et al., 2024) or tool-based approaches (Thoppilan et al., 2022).1CoTaEval assumes a "blocklist" of content that the model should not generate, as if requested by the copyright owner, and evaluates the model's ability to avoid generating the exact or substantially similar content. We evaluate interventions based on their ability to: (1) prevent similar outputs to blocklisted data (_low similarity_); (2) prevent downstream impacts on the ability to generate uncompyrightable factual content found in blocklisted data, (_high utility_); and (3) ensure the efficiency of the model (_low overhead_) (see Figure 1). A key difference from prior work, which evaluates whether methods remove all information about a piece of training data (Maini et al., 2024), is that our work evaluates whether interventions prevent near-similar outputs while retaining uncompyrightable information such as factual content present in the copyrighted material--it is perfectly acceptable to output uncompyrightable fact in a piece of blocklisted content, just as humans can learn and regurgitate facts.2 This work makes the following key contributions:

**A taxonomy of causes of undesirable regurgitation and takedown methods.** We identify two primary causes: memorization and retrieval augmentation (SS2.1), introduce the term of copyright takedown for LMs, referring to a mechanism to prevent generation that are too similar to certain requested content during deployment, and compile a taxonomy of takedown methods (SS2.2), ranging from 1) _generic prevention_ such as System Prompt, to 2) _decoding-time interventions_ such as MemFree (Ippolito et al., 2023), R-CAD, which downweights copyrighted content based on Shi et al. (2024); or Top-\(k\) Perturbation, which injects random noise to the top tokens during decoding, and 3) _training-based interventions_ such as machine unlearning (Golatkar et al., 2020; Thudi et al., 2022; Liu et al., 2022; Rafailov et al., 2024)

**An evaluation suite.** We introduce CoTaEval, the first benchmark to evaluate the feasibility and side effects of takedowns (SS3). CoTaEval mainly covers books and news articles, two types of textual content that frequently raise concerns. It supports evaluating copyright concerns from

Figure 1: **Effective takedown methods should prevent models from generating text matching the blocklisted content (low similarity) while preserving uncopyrightable facts and fair use information (high utility).**

memorization and retrieval using eight metrics. It also quantifies takedown side effects on model utility with three metrics and measures efficiency impacts.

**An evaluation of takedown methods and implications** We evaluate the performance of takedown methods on CoTaEval (SS4), highlighting the following implications for deploying language models:

* System Prompt and MemFree offer some mitigation but cannot completely prevent undesirable regurgitation.
* Machine unlearning and Top-\(k\) Perturbation reduces regurgitation but significantly compromises factual knowledge from the blocklisted content.
* R-CAD is effective for takedown but comes at the cost of efficiency and risk of utility drop.

Therefore, while the implementation of copyright takedown mechanisms is desirable, as highlighted by recent policy discussions, our evaluation suggests that current off-the-shelf methods are not yet sufficient. These findings point to the pressing need for further research in this area.

## 2 Copyright and Language Models

Recent litigation (_Tremblay v. OpenAI, Inc._, 2023; _Kadrey v. Meta Platforms, Inc._, 2023; _Chabon v. OpenAI, Inc._, 2023; _DOE 1 v. GitHub, Inc._, N.D. Cal. 2022) has pointed to two scenarios where a LM deployment might lead to copyright concerns: (1) content is memorized within the model's parameters during training, and (2) content is incorporated as additional context during retrieval-augmented generation (SS2.1). These scenarios motivate the study of takedown methods (SS2.2).

### Causes to Regurgitation of Copyrighted Contents

**Memorization.** Language models are known to memorize and regurgitate portions of the data they were trained on (Carlini et al., 2019, 2021, 2023; Zhang et al., 2023; Nasr et al., 2023). Recent work by Min et al. (2023) proposes a solution where non-permissive data is offloaded into an external database, while the model's parameters are only trained on permissive data. However, this proposal does not fully solve the problem: 1) ensuring that all training data is actually permissive is very difficult, if not impossible, and 2) it does not address the risks posed by retrieval augmentation, as discussed next.

**Retrieval-augmented generation (RAG).** In addition to potentially memorizing content baked into their training data, modern language models also risk regurgitating protected content by retrieving and incorporating material from external sources they can access during runtime. Retrieval-augmented generation (RAG, Lewis et al., 2020) has been employed in many systems (Shi et al., 2024; Asai et al., 2023; Yasunaga et al., 2023; Lin et al., 2024), enabling them to search large knowledge bases or the open web, retrieve relevant information, and include it in their generation. With this capability, these models can locate, retrieve, and reproduce protected content while generating responses. Notably, ongoing lawsuits, such as _The New York Times Company v. Microsoft Corporation_ (2023), highlight that web search and retrieval-based methods are a significant source of potential issues related to copyright. While providing snippets from retrieved content (e.g., search previews) is permissible (e.g. in the US), generating entire contents from web pages in the response may not be.

### Takedown Methods for Language Models

The copyright owner could request the language models to refrain from generating content that are overly similar to their own data. While there is no legal obligation yet in most countries today, model deployers are highly motivated to develop such capabilities. We refer to this procedure as a **copyright takedown** for LMs, and the requested content from copyright owner as the _blocklisted content_. This can be achieved by copyright owners providing a blocklist of content that models should not generate, enabling deployers to implement takedown methods to ensure models refrain from generating any content from this blocklist.

Our evaluation considers three types of takedown methods that intervene at different stages of the language model: 1) strategies that generally try to prevent the regurgitation of training data without specifying a blocklist (SS2.2.1), 2) methods that prevent the generation of blocklisted content during decoding (SS2.2.2), and 3) training-based interventions like unlearning (SS2.2.3). Some of these specifically target undesirable regurgitation caused by memorization, while others are better suited for the scenario of RAG, and yet others can handle both scenarios (see Table 1).

#### 2.2.1 Generic Prevention Strategies

**System Prompt.** The System Prompt (Anthropic, 2023) is the initial set of instructions given to the language model to guide the model in understanding the operational rules it must follow during interactions. It has been leveraged by model deployers to generally direct the model to avoid generating certain types of content (rather than taking down particular pieces of content). See an example snippet from Bing Chat (Microsoft, 2023), a production-level model, below:

"You are a helpful, respectful and honest assistant. You must not reply with content that violates copyrights for books, news articles, or song lyrics."

**Top-\(k\) Perturbation.** Another general method to prevent the generation of memorized content or content from the context is to perturb the top tokens during generation, such as by adding Gaussian noise to the logits in top-\(k\) sampling.

#### 2.2.2 Decoding-Time Takedowns

Copyright takedown can also occur during the decoding phase, with strategies aimed at penalizing the generation of content similar to blocklisted copyrighted materials.

**MemFree.**MemFree decoding (Ippolito et al., 2023) aims to prevent the verbatim regurgitation of blocklisted content. At each step, it checks whether the model's chosen next token would create an \(n\)-gram found in the blocklist. If it would, the model selects the next highest-ranked token and checks again, continuing this process until a token that does not trigger a \(n\)-gram match is sampled.

**Reversed Context Aware Decoding (R-CAD).** Context-aware decoding (Shi et al., 2024) enables LMs to up-weight context during decoding to reduce hallucination. Applying this method in _reverse_ for blocklisted material (namely down-weighing blocklisted materials) could reduce specific regurgitation by downweighting the retrieved blocklisted materials in the context. Consider: if we let the model \(\) generate response \(\) based on the query \(\), then the \(i\)th token of the response can be sampled from the distribution \(y_{i} p_{}(y_{i},_{<i}) _{}(y_{i},_{<i})\). R-CAD aims to remove the "distribution" induced by the blocklisted content \(\), it will retrieve the content \(\) from the blocklisted content datastore,3 and sample \(y_{i}\) from the distribution \(y_{i}[(1+)_{}(y_{t} |,_{<i})-_{}(y_{t} |,_{<i})]\), where \(\) is the weight of adjustment.

#### 2.2.3 Training-based Takedowns (Unlearning)

Machine unlearning (Cao and Yang, 2015; Guo et al., 2020) is a technique that aims to transform an existing trained model into one that behaves as though it had never been trained on certain data. This approach can be used to make the model forget the blocklisted materials they were exposed to during training. Most unlearning methods require a forget set (the data to be removed) and a retain set (the data to be kept). In our context, the forget set consists of copyrighted content that the model deployer wants to remove, while the retain set includes verified licensed content from a similar distribution. We evaluate four mainstream unlearning methods highlighted in Maini et al. (2024), including _Gradient ascent_ (UnlearningGA; Thudi et al., 2022), _Gradient Difference_ (UnlearningGD; Liu et al., 2022), _KL minimization_ (UnlearningK; Golatkar et al., 2020), and _Preference Optimization_ (UnlearningPO; Rafailov et al., 2024). More details about these methods can be found in Appendix A.2. Note that the objective of unlearning is to ensure that the unlearned model behaves as thought it had never encountered the forget set (Cao and Yang, 2015), mimicking an oracle model trained without the blocklisted content. Although these methods may prevent the verbatim generation of copyrighted content, their current design does not ensure that factual information contained within that content is preserved.

  
**Stage** & **Method** & **Memorization** & **RAG** \\   Generic \\ Prevention \\  &  System Prompt \\ Top-\(k\) Perturbation \\  &  ✓ \\ ✓ \\ ✓ \\  \\   Decoding-Time R-CAD \\  &  ✓ \\ ✓ \\ ✓ \\  \\   Training-Based \\  &  UnlearningGA \\ ✓ \\ ✓ \\  &  ✓ \\ ✓ \\  \\   Training-Based \\  &  UnlearningGD \\ ✓ \\ ✓ \\  & 
 ✓ \\ ✓ \\ ✓ \\  \\   

Table 1: **Summary of takedown strategies and their applicable scenarios.** Unlearning methods and R-CAD apply only to memorization scenarios. MemFree, Top-\(k\) Perturbation, and System Prompt apply to both scenarios.

## 3 The CoTaEval Evaluation Pipeline

To evaluate the effectiveness of copyright takedown methods, we propose a new evaluation pipeline CoTaEval (**C**opyright **T**akedown **E**valuation). CoTaEval uses books and news articles as evaluation corpus and considers both the memorization and RAG scenarios (SS3.1). The effectiveness of different takedown methods is quantified based on three desiderata that we propose: **low similarity**, **high utility**, and **low overhead** (SS3.2).

### Evaluation Corpus and Target Scenarios

**Evaluation Corpus.** Our evaluation focuses on two prevalent types of text often involved in copyright-related cases: _news articles_ and _books_. For the _news articles_ domain, we use the NewsQA dataset (Trischler et al., 2017), which consists of CNN articles paired with questions and answers derived from those articles. For the _books_ domain, we use the BookSum dataset (Kryscinski et al., 2022), where each example includes a book chapter along with a summary of that chapter's content. Table 2 provides examples of each corpus.

**Target Scenarios.** We evaluate the two scenarios discussed in SS2: (1) When the blocklisted content is memorized in the model parameters (referred to as _Memorization_). We simulate this by fine-tuning the original model on blocklisted content and then running the evaluation. (2) When the blocklisted content is provided as additional context during retrieval-augmented generation (referred to as _RAG_). Here, we use the original model but present blocklisted content as the retrieved context to simulate the retrieval of the specific material in the evaluation. More details are provided in SS4.1.

### Metrics

We divide each corpus into two parts: blocklisted content \(_{}\), which the model should avoid generating, and in-domain content \(_{}\), which is from the same domain as \(_{}\) but not subject to takedown requests. We note three key criteria for effective takedown methods and evaluate them respectively:

* **Low Similarity** (SS3.2.1): Following the takedown, the model must avoid generating content that is too similar to the content in \(_{}\).
* **High Utility** (SS3.2.2): Post-takedown, the model should retain essential factual knowledge from both \(_{}\) and \(_{}\), because facts are not copyrightable (_Harper & Row, Publishers, Inc. v. Nation Enterprises_, 1985; _Feist Publications, Inc. v. Rural Tel. Serv. Co._, 1991).4 * **Low Overhead** (SS3.2.2): The process of takedown should not impose significant computational overhead, ensuring it can be feasibly implemented. This includes both a one-time offline cost (e.g., modifying the model or database) and an online cost (e.g., modification to the decoding process) incurred during each model interaction.

#### 3.2.1 Risk Evaluation

Copyright-related concerns are more likely to occur when content generated by a model is "substantially similar" to the blocklisted material. As such, we measure the risk via a variety of similarity measures. For each example \(x\) in the blocklisted content, we split it into a length-\(l\)_hint_\(x_{[:l]}\) and

  
**Corpus** & **Original datapoint** & **Risk Eval** & **Unity Eval** & **Overall** \\   & Friends and colleagues of _k_papic & **Hint**: Friends and colleagues of _k_papic & **Question**: Who is founder of \\  & founder Steve Jobs sent their & founder & **Apple?** & \\  & condiances Wednesday after his death at the age of 56. & **Output**: Steve Jobs sent their com & **Answer**: Steve Jobs \\  & defiances Wednesday after he passed & **way**: & & **MMU \&** \\   & Mrs Dursley had a sister called & **Hint**: Mrs Dursley had a sister & **Question**: Sumarize this & MT-Bench \\  & Lily Potter. She and her husband James Potter had a son & **Output**: called Lily Potter. She and & paragraph. & **Summary**: Lily Potter and \\  & called Harry Potter. They lived far & **Sums**: Potter are Harry Potter. They lived far & James Potter are Harry Potter’s parents. They lived & \\  & did not speak to them. & **to** them. & & **Summary**: Lily Potter and \\  & & & & **James Potter are Harry Potter’s parents. They lived & \\  & & & & **James Potter are Harry Potter’s parents. They lived & \\  & & & & **for** the Dursley. & \\   

Table 2: **Overview of the CoTaEval’s risk and utility evaluations.** For risk evaluation, we input “hint” and ask the model for completion. For utility evaluation, we ask the model to do question-answering for news and do summarization for books. We also evaluate the models general utility with MMLU and MT-Bench. Overlapping sequences between the generated content and the ground truth are highlighted in greenthe _ground truth_ continuation \(x_{[l+1:]}\). The model \(f\) is then prompted with \(x_{[l:]}\), and the generated continuation \(f(x_{[l:]})\) is compared to \(x_{[l+1:]}\) to assess potential risk. Given that any insufficient transformation of blocklisted content can lead to potential copyright concerns (Lemley and Casey, 2021; Sag, 2023; Henderson et al., 2023), CoTAeVal adopts eight similarity metrics covering both lexical and semantic similarity to evaluate the similarity between the generated \(f(x_{[l:]})\) and the ground truth continuation \(x_{[l+1:]}\) (see Figure 2):

* _Exact match_ is measured using two metrics: the length of character-level **L**ongest **C**ommon **S**ubsequence (LCS) \(_{}^{v}\) and the length of word-level LCS \(_{}^{w}\).
* _Near duplicate_ is measured using five metrics: ROUGE-1, ROUGE-L (Lin, 2004), the length of word level **A**ccumulated **C**ommon **S**ubsequences (ACS) \(_{}^{w}\), Levenshtein Distance \(_{}\)(Levenshtein et al., 1966), and MinHash similarity \(_{}\)(Broder, 1997).
* _Semantic similarity_\(_{}\) is captured by cosine similarity between the generated content and the blocklisted content using an off-the-shelf embedding model5. 
More details about these metrics are provided in Appendix B.2. It is important to note that legal judgments of infringement often require case-by-case analysis. While these metrics may not be dispositive of infringement, they are potential indicators of high-risk, potentially infringing, outputs.

#### 3.2.2 Utility and Efficiency Evaluation

**Utility Evaluation.** Our utility evaluation encompasses factual knowledge preservation of blocklisted and in-domain content, as well as general utility:

* _Blocklisted and in-domain content utility._ To evaluate whether the model still retains uncopyrightable factual knowledge after takedown, we assess its performance on downstream knowledge-intensive tasks that are unlikely to result in copyright concerns. This evaluation is conducted on both the blocklisted content \(_{}\) and the in-domain content \(_{}\) (not subject to takedown requests). For news articles, we ask the model to answer questions related to factual information within the articles and measure performance using the word-level F1 score between the output and the ground truth for QA tasks. For books, we ask the model to briefly summarize a book chapter and measure its performance using the ROUGE-L score, by comparing the output with the ground truth summary.
* _General utility._ Additionally, we measure the model's general utility using MMLU (Hendrycks et al., 2020) and MT-Bench (Zheng et al., 2024), two widely adopted benchmarks that evaluate the model's knowledge and reasoning abilities across a diverse range of subjects and tasks.

More details on segmenting datasets and prompting methods for utility evaluation are in Appendix B.3.

**Efficiency Evaluation.** We also evaluate the computational efficiency of takedown methods during inference. This is crucial because these methods should not significantly slow down the model's response time or require excessive computational resources. For a fair comparison, when evaluating the efficiency, we limit the model to generate a fixed number of tokens, and report the average inference speed across examples from news articles or books.

## 4 Experiments

In this section, we use CoTAeVal to evaluate copyright takedown methods detailed in SS2.2. We introduce our experimental setup in SS4.1 and present our results and observations in SS4.2.

Figure 2: **CoTAeVal investigates three scenarios of undesirable regurgitation motivated from copyright concerns: (a) exact match, (b) near-duplicate match, and (c) generation of text semantically similar. Verbatim matching sequences are highlighted in green, and semantic similar sequences are highlighted in yellow.**

### Experiment Setup

**Models.** Our evaluation focuses on open language models, as modifying either the training or decoding process is often necessary for most takedown methods, which are not always feasible with proprietary models. We evaluate three models in the RAG setting: Llama2-7B-chat and Llama2-7DB-chat (Touvron et al., 2023).6 For the memorization setting, we evaluate the Llama2-7B-chat model finetuned on news articles (see Appendix B.1 for more details).7

**Methods.** We evaluate eight takedown methods as detailed in Table 1. We notice that all methods except for System Prompt entail hyperparameters, so we conduct a hyperparameter search and report the one that achieves the best trade-off between risk reduction and utility preservation (see Appendix B for details). We use greedy decoding for all methods.

**Metrics.** The risk evaluation reports the win rate for each of our eight metric discussed in SS3.2, showcasing the method's overall effectiveness in reducing generation of text similar to blocklisted content. The win rate is defined as the probability that a given method will outperform another randomly sampled method under a (metric, example) pair. We aggregate these metrics by calculating an average win rate using \(1000\) examples for the news articles domain and \(500\) examples for the books domain, demonstrating the overall effectiveness of the takedown methods. The utility evaluation reports the average value with confidence intervals for four utility scores mentioned in SS3.2.2. We use \(500\) examples in the news articles domain and \(200\) examples in the books domain for both blocklisted and in-domain utility evaluation. More details are provided in Appendix B.3. We report the calibrated average inference speed (compared to Vanilla) for efficiency evaluation.

### Results and Observations

Table 3 presents the evaluation results for the RAG setting, while Table 4 for the memorization setting. Figure 3 shows the violin plot for selected metrics for the RAG setting and the memorization setting.

Table 3: **Evaluation of takedown methods in the RAG scenario, where the blocklisted content is provided as additional input context.** We report confidence intervals for utility evaluation. A [after qu] indicates better performance. On average, System Prompt and MemFree help balance the reduction of undesirable regurgitation while maintaining utility and efficiency, while Top-\(k\) Perturbation will sacrifice utility a lot when it works. The only difference between news and books on MMLU/MT-Bench is MemFree, as the Bloom filter stores different blocklisted content for each domain. See Appendix D.2 for examples when MemFree is triggered in MT-Bench.

As we observe similar behaviors between Llama2-70B-chat and Llama2-7B-chat, our analysis below focuses on Llama2-7B-chat. Overall, none of the takedown methods excel across all metrics; each has its drawbacks, either in effectively reducing similarity to blocklisted content (win rates for each similarity metric are available in Appendix C) or in maintaining utility and efficiency. Our key observations are summarized as follows.

**System Prompt and MemFree offer some mitigation but cannot completely prevent undesirable regurgitation.** A system prompt provides general guidance for model behavior. In our experiment, we evaluate six options of system prompts,8 with the best one reported in Table 3 and Table 4. We observe that it effectively increases the chances that the model rejects outputting blocklisted content, and it is particularly effective in the RAG scenario within the news domain, as suggested by the highest win rate in reducing risk among all tested methods (see Table 3). However, it still fails occasionally; the model does not correctly reject every instance. Figure 3 shows that certain cases still exhibit a high \(_{}^{w}\), \(_{}^{w}\), \(_{}\), and a low \(_{}^{w}\) after the intervention. (see Appendix D.1 for qualitative examples).

MemFree can reduce the similarity to blocklisted content while generally preserving utility, particularly for exact matching measurement, as it employs a Bloom-filter-based detection algorithm, which identifies elements that exactly match those stored in the Bloom filter. This is verified by a high win rate for \(_{}^{w}\) (see Figure 3). However, minor misspellings, extra whitespace, or additional newline characters cannot be captured by the exact match detector and can thus easily bypass detection. In fact, we observe that MemFree tends to apply these modifications to bypass exact match (see Appendix D.2), which does not actually reduce the risk. Consequently, it struggles to effectively prevent other forms of matching, such as near-duplicates, as suggested by the lower win rate on metrics such as \(_{}^{w}\), which captures the accumulated length for all common sequences (see Figure 3).

**Unlearning and Top-\(k\) Perturbation reduce similarity but significantly compromises factual knowledge from the blocklisted content.** Unlearning aims to post-edit models without retraining from scratch to erase content that needs to be taken down. Although some of the unlearning methods show their capability to reduce the similarity to blocklisted content (for example, \(_{}\) and \(_{}\)), we find they have several downsides. First, most of the unlearning methods are hyperparameter sensitive, an ideal unlearning result requires an extensive hyperparameter search across the learning rate and training epochs, which usually takes much time and computation (See

Figure 3: **Violin plots of \(_{}^{w}\), \(_{}^{w}\), \(_{}\), and \(_{}\) for (a) RAG scenario and (b) memorization scenario, evaluated on Llama2-7B-chat model on news articles domain. The short horizontal line indicates the mean value for each method. The large maximum values of \(_{}^{w}\), \(_{}^{w}\), and \(_{}\), along with the low minimum value of \(_{}\), demonstrate that System Prompt and MemFree cannot completely prevent undesirable regurgitation in both scenarios.**

[MISSING_PAGE_FAIL:9]

Mitigations for Copyright Concerns.Few solutions have been proposed to technically address the copyright and transparency issues associated with LMs. Min et al. (2023) suggest training a parametric language model on an open-source corpus and augmenting it with a non-parametric datastore containing copyrighted materials, which would be queried only during inference. Although their proposal eliminates undesirable regurgitation due to memorization in model weights, it does not tackle the scenario where blocklisted content is retrieved and prepended to the context, as the model may still copy the retrieved context verbatim. Decoding time methods like Mem-Free decoding (Ippolito et al., 2023) and GitHub Copilot's duplication detection filter (GitHub, 2023b) check generated sentences on the fly and prevent the model from generating verbatim copies. However, both methods cannot capture non-consecutive verbatim matches, potentially resulting in a false sense of privacy and copyright protection. Hans et al. (2024) propose goldfish loss to mitigate the copyright issues of the LMs, which only computes the loss on the tokens where its golden fish mask is \(1\) when training LMs. Though it shows some promise on the exact match and ROUGE-L score, its effectiveness on other metrics mentioned in CoTaEval requires further verification. Liu et al. (2024) propose SHEID, an agent-based mitigation strategy that can guide the LMs to refuse and warn the user when requesting the model to generate copyrighted materials.

Detecting Pretraining Data.Elazar et al. (2023) and Marone and Van Durme (2024) have proposed frameworks to inspect and analyze the training corpora of language models, providing insights into the composition and characteristics of the data used during the training process. Shi et al. (2023) propose a method to detect whether a piece of text has been used during the pretraining of language models, and used this tool to identify a collection of books that were likely used by OpenAI during training. Additionally, Wei et al. (2024) propose a data watermarking approach, allowing copyright holders to detect whether their proprietary data has been used in model training.

## 6 Conclusion

In this work, we propose CoTaEval, a comprehensive framework for evaluating copyright takedown methods for LMs. CoTaEval enables us to assess whether a takedown method achieves the desired outcomes: low similarity to blocklisted content, high utility, and minimal overhead. Through CoTaEval, we discover that none of the mainstream takedown methods excel across all metrics. This finding highlights the need for further research to develop improved takedown methods and address potential unresolved challenges in live policy proposals.

Limitations.While CoTaEval is an initial effort to evaluate copyright takedown methods, there is room for improvement in future studies. First, the metrics we provided only offer an indication of the extent to which the generated content may have copyright issues, rather than establishing a uniform measurement. Future work could focus on a more detailed exploration of legal standards for potential copyright concerns. Additionally, our benchmark covers two content categories (news and books), which may not fully represent the diverse scenarios encountered in real-world applications. Future research should aim to include a wider range of content types to enhance the evaluation's comprehensiveness and utility. Third, we have not explored the scalability of the mitigation mechanisms we propose. Future studies should consider the capacity to scale these mechanisms to accommodate larger volumes of blocklisted content. Fourth, there are potentially other important aspects of utility that we did not evaluate. For example, even if the blocklisted content contains a cover letter, the LM should not lose the ability to generate cover letters after the takedown procedure is applied. Finally, we did not evaluate some of the latest unlearning methods, such as RMU (Li et al., 2024) and Negative Preference Optimization (Zhang et al., 2024). Future research can utilize CoTaEval to assess the effectiveness of these methods and their potential side effects.

Societal Impacts.Our work seeks to provide an evaluation of whether content can be "taken down" - a process that prevents models from generating copyrighted content. However, we do not take a position on endorsing this approach as the definitive solution for managing complex legal scenarios. Legal scholars often suggest that takedown mechanisms should be part of a broader strategy that includes additional licensing schemes to compensate for challenges in authenticating the provenance of content on a large scale, as illustrated by our introduction's Reddit example. Moreover, relying solely on takedown procedures might not fully address concerns related to labor or intellectual property rights. It is crucial to clarify that our research does not advocate for takedowns as the sole approach, nor does it claim to resolve the intricate issues surrounding copyright.