# Cones 2: Customizable Image Synthesis

with Multiple Subjects

Zhiheng Liu\({}^{1}\)\({}^{}\)  Yifei Zhang\({}^{2}\)\({}^{*}\)\({}^{}\)  Yujun Shen\({}^{3}\)  Kecheng Zheng\({}^{3}\)  Kai Zhu\({}^{1,4}\)

**Ruili Feng\({}^{1,4}\)  Yu Liu\({}^{4}\)  Deli Zhao\({}^{4}\)  Jingren Zhou\({}^{4}\)  Yang Cao\({}^{1}\)\({}^{}\) \({}^{1}\)**USTC \({}^{2}\)SJTU \({}^{3}\)Ant Group \({}^{4}\)Alibaba Group

Equal contribution.Work performed during internship at Alibaba DAMO Academy.Corresponding author.

###### Abstract

Synthesizing images with user-specified subjects has received growing attention due to its practical applications. Despite the recent success in single subject customization, existing algorithms suffer from high training cost and low success rate along with increased number of subjects. Towards controllable image synthesis with multiple subjects as the constraints, this work studies how to efficiently represent a particular subject as well as how to appropriately compose different subjects. We find that the text embedding regarding the subject token already serves as a simple yet effective representation that supports arbitrary combinations without any model tuning. Through learning a residual on top of the base embedding, we manage to robustly shift the raw subject to the customized subject given various text conditions. We then propose to employ layout, a very abstract and easy-to-obtain prior, as the spatial guidance for subject arrangement. By rectifying the activations in the cross-attention map, the layout appoints and separates the location of different subjects in the image, significantly alleviating the interference across them. Both qualitative and quantitative experimental results demonstrate our superiority over state-of-the-art alternatives under a variety of settings for multi-subject customization. Project page can be found here.

## 1 Introduction

The remarkable achievements of text-to-image generation models , have garnered widespread attention due to their ability to generate high-quality and diverse images. To allow synthesizing images with user-specified subjects, customized generation techniques  propose to fine-tune the pre-trained models on a few subject-specific images. Despite the notable success in single subject customization , multi-subject customization remains seldom explored but better aligns with the practical demands in real life.

Recent studies  have investigated multi-subject customization through joint training, which tunes the model with all subjects of interest simultaneously. Such a strategy has two drawbacks. First, they require learning separate models for each subject combination, which may suffer from exponential growth when the number of subjects increase. For example, the customization of objects \(\{,,\}\) fails to inherit the knowledge obtained from the customization of objects \(\{,\}\). Second, different subjects may interfere with each other, causing the issues that some subjects fail to show up in the final synthesis or the subject attribute gets confused among subjects (_e.g._, a cat with the features of another dog). This phenomenon is particularly evident when the semantic similarity between subjects is high (see Fig. 4).

In this work, we present Cones 2, a novel approach for multi-subject customization using a pre-trained text-to-image diffusion model. Our method utilizes a simple yet effective representation to register a subject and enables the arbitrary composition of various subjects without requiring any model retraining. To that end, we decompose the challenging task of multi-subject customization into two components: how to efficiently represent a subject and how to effectively combine different subjects. Given a set of subjects and their photos (3-5 for each), our goal is first to bind the characteristic of each specific subject to a "plugin" that can be used flexibly. Driven by this, we fine-tune the text encoder part of a pre-trained text-to-image diffusion model with images of a specific subject, making the tuned model can customize this specific subject. Moreover, we propose a text-embedding-preservation loss, which limits the output of the tuned text encoder to only differ from the original text encoder in token embedding regarding the specific subject. Then we calculate the mean difference between the tuned text encoder with the original text encoder to derive the residual token embedding which can robustly shift the raw category to the customized subject (_e.g._, dog \(\) customized dog).

To effectively combine different subjects, we propose a layout guidance method to control the generation process. More formally, we employ pre-defined layout, a very abstract and easy-to-obtain prior, to guide different subjects to show up in different positions by rectifying the activation in the cross-attention maps. We encourage all subjects to show up in the final synthesis by strengthening the activations of the target subject. Simultaneously, to prevent the subject attribute gets confused, we weaken the activations of the irrelevant subjects. In addition, to make this easy to implement in practice, we define the layout as a set of subject bounding boxes with subject annotation, which describes the spatial composition of the customized subjects and is easy for users to specify in advance. Through our method, users can compose various subjects arbitrarily with a pre-defined layout (see Fig. 1).

Our method is evaluated under a variety of settings for multi-subject customization involving extensive subject categories such as pets, scenes, decorations, _etc._ Qualitative and quantitative results demonstrate that, compared to existing baselines, our method exhibits competitive performance in terms of both text alignment to input prompt and visual similarity to the target images. It is noteworthy that our method even facilitates the customization of a larger number of subjects (_e.g._, six subjects in Fig. 1), which is a far more challenging setting in practice.

## 2 Related work

**Large-scale text-conditioned image synthesis.** Synthesizing images from the language description has received growing attention due to its ability to generate high-quality and diverse images. Earlier

Figure 1: **Customizable image generation** with the subjects listed on the left. **Cones 2** is highlighted from three aspects. (1) Using a simple yet effective representation to register a subject, we can compose various subjects arbitrarily _without any model tuning_. (2) Employing spatial layout, which is very easy to obtain in practice, as a guidance, we can _control the specific location_ of each subject and meanwhile _alleviate the interference_ across subjects. (3) Our method achieves appealing performance even under some _challenging settings_, such as customizing the synthesis with six or more subjects and exchanging the sunglasses on the two dogs.

works  explored the utilization of language description into GAN as a condition on specific domains under the closed-world assumption. With the development of diffusion models [22; 23] and large-scale multi-modality models , text-conditioned image synthesis has shown remarkable improvement in an open-vocabulary text description. Specifically, GLIDE , DALLE2 , StableDiffusion  and Imagen  are representative diffusion models that can produce photorealistic outputs. Autoregressive models such as DALLE , Make-A-Scene , CogView  and Parti  have also shown exciting results. Although these models demonstrate an unparalleled ability to synthesize images, they require time-consuming iterative processes to achieve high-quality image sampling. Recent large text-to-image GAN models such as StyleGAN-T , GALIP , and GigaGAN  also demonstrated unprecedented semantic generation, which is orders of magnitude faster when sampling.

**Customized image generation.** Thanks to the significant progress of large-scale text-to-image models, users can adopt these well-trained models to generate customized images with user-specified subjects. There are two earliest attempts to solve the customized generation through few-shot images of one specific subject, _i.e._ Text Inversion  and DreamBooth . Concretely, Text Inversion  represents a new subject by learning an extra identifier word and adding this word to the dictionary of the text encoder. DreamBooth  binds rare new words with specific subjects through few-shot fine-tuning the whole Imagen  model. To compose multiple new concepts together, Custom  chooses to only optimize the parameters of the cross-attention in the StableDiffusion  model to represent new concepts and then joint trains for the combination of multiple concepts. In addition, Cones  associates customized subjects with activating a small cluster of neurons in the diffusion model. Although both Custom  and Cones  have explored combination multi-subject customization, they suffer from high training costs and low success rates along with the increased number of subjects. In this work, we study how to efficiently represent a particular subject as well as how to appropriately compose different subjects. Specifically, learning a residual on top of the base embedding can represent a new concept, and the introduction of layout into the attention map can help the model generate more accurate user-specified subjects. We find these design choices lead to better results in directly composing different subjects than joint training.

**Spatial guidance in diffusion models.** To further enhance the controllability of synthesizing images, some works [28; 29; 30; 31; 32] have tried to explore how to guide the generation process by more spatial information. Composer  directly adds spatial information as a condition input during the training phase. ControlNet  and T2I-Adapters  add spatial information to the pre-trained model by training a new adapter. Prompt-to-prompt  presents a training-free edit method by editing the cross-attention. In addition, a diffusion-based image translation  keeps the generated spatial structure by limiting the cross-attention map. Inspired by these works, we also adopt the layout as the spatial guidance for subject arrangement that can well appoint and separate the location of different subjects in the image, significantly alleviating the interference across them.

## 3 Method

Given a set of subjects and their photos (3-5 for each) from different views, we aim to generate new images of any combination containing those subjects vividly and precisely. We accomplish this by combining subject-specific residual token embeddings with a pre-trained diffusion model and guiding the generation process with a layout. The overall framework is presented in Fig. 2. Specifically, we represent each subject as a residual token embedding shifted from its base category. Adding the residual token embedding to the base category embedding can yield the corresponding subject in the generated images. We present how to get this residual token embedding in Sec. 3.2. At inference time, subjects failing to show up and the subject attribute getting confused among subjects are two key problems in multi-subject customized generation. To address these issues, we present a method of composing subjects by leveraging layout guidance in Sec. 3.3.

### Text-conditioned diffusion model

Diffusion models learn a data distribution by the gradual denoising of a variable sampled from a Gaussian distribution. This corresponds to learning the reverse process of a fixed-length Markov chain. In text-to-image tasks, the training objective of a conditional diffusion model \(_{}\) can be simplified as a reconstruction loss,

\[L_{}=_{,,( 0,1),t}[\|_{}(_{t},E(),t)- \|_{2}^{2}], \]where \(t()\) is the time variable, \(E\) is a pre-trained text encoder and \(_{t}=_{t}+_{t}\) is a noised image from the ground-truth image \(\). The parameters \(_{t}\) and \(_{t}\) are coefficients formulating the forward diffusion process. The model \(_{}\) is conditioned on the text embedding \(E()\) and \(t\). The text embedding \(E()\) is injected into the model \(_{}\) through the cross-attention mechanism. At inference time, the network \(_{}\) is sampled by iteratively denoising \(_{T}(0,)\) using either deterministic samplers  or stochastic sampler .

### Representing subjects with residual token embedding

**Representing subjects with residual text embedding.** Our goal is first to represent each subject with a residual text embedding among the output domain of the text encoder. An ideal residual text embedding \(^{}\) that can robustly shift the raw category to a specific subject. For example, the model \(_{}\) with embedding input \((E()+^{}_{})\) can truly generate a photo of specific "dog". One way to get this objective is to calculate an embedding direction vector  from a source (original) text encoder to the target (fine-tuned) text encoder \(E^{}\). The fine-tuned text encoder \(E^{}\) needs to be able to customize subject \(s\) combined with the original diffusion model \(_{}\). Similarly as DreamBooth , \(E^{}\) can be trained with the subject-preservation loss, as

\[L_{}(E^{})=_{(,) D_{ s},(0,1),t}[\|_{}(_{t},E^ {}(),t)-\|_{2}^{2}], \]

where \(D_{s}=\{(_{j}^{s},s)|_{j}^{s}  X^{s}\}\) is the reference few-shot data of subject \(s\).

**Regularization with a text-embedding-preservation loss.** The residual text embedding obtained according to the previous section can only perform single-subject customized generation. Since those residual text embeddings are applied to the entire text, any two of them can admit significant conflicts so that they cannot be combined together directly while carrying out inference. Therefore, we propose a text-embedding-preservation loss to make the residual text embedding mainly act on the text embedding regarding the subject token. The core idea is to minimize the difference between \(E^{}\) and \(E\) for tokens apart from the subject token \(s\). Take the "dog" case above as an example, we sample 1,000 sentences \(C_{}=\{^{i}\}_{i=1}^{1000}\) containing the word "dog" using ChatGPT , like "a dog on the beach", and then minimize the difference between \(E^{}\) and \(E\) for all the token besides "dog". In detail, given any caption (_e.g._\(=\)), we split its text embedding into a sequence (\(E()=(E()_{},E()_{},,E( )_{})\)). Then we wish \(\|E()_{p}-E_{p}^{()}\|_{2}^{2}=0\) for any \(p\) that is not equal to "dog". Namely, the text-embedding-preservation loss is a regularization, as

\[L_{}(E^{})=_{ C_{}}[ _{p,p s}\|E^{}()_{p}-E( )_{p}\|_{2}^{2}], \]

Figure 2: **Illustration of the proposed approach.** (a) We first learn a residual token embedding (_e.g._, \(^{}_{}\)) on top of the base embedding to register a user-specified subject, which allows composing various subjects arbitrarily without further model tuning. (b) Given a layout as the spatial guidance, we then arrange the subjects by rectifying the activations in cross-attention maps, which enables the control of the location of each subject and reduces the interference between them.

where \(p\) traverses all tokens inside sentence \(\) except the subject token \(s\). Our complete training objective then comes as

\[L=L_{}+ L_{}, \]

where \(\) controls for the relative weight of the text-embedding-preservation term. As shown in Fig. 1(a), after the customized text encoder is obtained, we derive the _residual token embedding_ of "dog" via computing the average shift of \(E^{}()_{}\) over these 1,000 sentences from \(E()_{}\), as

\[^{}_{}=}|}_{  C_{}}(E^{}()_{}-E( )_{}). \]

**Inference with residual token embedding.** The residual token embedding we get aforementioned can be used directly in any subject combinations involving them without further tuning. As shown in Fig. 1(b), when we do customized generation with \(N\) specific subjects \(s_{1},s_{2},,s_{N}\), all we need is to fetch the pre-computed \(^{}_{s_{1}},^{}_{s_{2}},,^{ }_{s_{N}}\) and add them to the token embedding, as

\[E^{}()_{s_{i}}=E()_{s_{i}}+^{}_{s_{i}},i=1,N. \]

In fact, the operation in Eq. (6) is all in the token dimension. This characteristic endows our method with significant convenience and high efficiency for large-scale applications. On the one hand, any pre-trained residual token embedding \(^{custom}_{i}\) can be used repeatedly and combined with another \(^{custom}_{j}\). On the other hand, for each subject, we merely need to store a float32 vector, getting rid of storing large parameters as in previous methods .

### Composing subjects with layout guidance

The text-to-image diffusion models  commonly inject the text embedding \(E()\) to its diffusion model \(_{}\) via the cross-attention mechanism. The attention map among a cross-attention layer is \(=(_{Q}(_{t}))(_{K}  E())\), where \((_{t})\) denotes the transformed image feature and \(_{Q},_{K}\) denotes the parameters for computing query and key. The cross-attention map directly affects the spatial layout of the final generation . Below we will discuss how to improve the quality of customized generation by rectifying the activations in the cross-attention map.

**Strengthening the signal of target subject.** One issue in multi-subject customization is that some subjects may fail to show up. We argue that this is caused by insufficient activations in the cross-attention map of these subjects. To avoid this, we choose to strengthen the signal of the target subject in the region where we want it to show up.

**Weakening the signal of irrelevant subject.** Another issue in multi-subject customization is that the subject attribute gets confused among subjects, _i.e._ the subjects in generated images may contain characteristics from the other subjects. We argue that this is due to the overlapping activation regions of different subjects in the cross-attention map. To avoid this, we choose to weaken the signal of each subject appearing in the region of the other subjects.

**Layout-guided iterative generation process.** Combining the above two ideas, we present a method to guide the generation process according to a pre-defined layout \(\). In practice, we define the layout \(\) as a set of subject bounding boxes and then get the guidance layout \(M_{s}\) for each subject \(s\). In detail, as shown in Fig. 1(b) we divide \(M_{s}\) into different regions: we set the value of \(M_{s}\) to a positive value \(^{+}^{+}\) in the region where we want the subject \(s\) to show up (denote as \(R_{s}^{}\)) and set the value of \(M_{s}\) to a negative value \(^{-}^{-}\) in the region that is irrelevant to the subject \(s\) (denote as \(R_{s}^{}\)). At the inference time, we replace all the output of cross-attention with edited results at every generation step, as

\[(,,)=(\{(t)_{s_{i}}|i=1,,N\})( _{V} E()), \]

where \(\) denotes the operation that adds the corresponding dimension of \(\) and \(\), which is also visualized in Fig. 1(b) and \((t)\) is a concave function controlling the edit intensity at different time \(t\). The implementation details refer to Algorithm 1.

## 4 Experiments

### Experimental setups

**Datasets.** For fair and unbiased evaluation, we select subjects from previous papers [15; 18; 16; 17] spanning various categories for a total of 15 customized subjects. It consists of two scenes, five pets and eight objects. We perform extensive experiments on various combinations of subjects, explaining the superiority of our approach.

**Evaluation metrics.** We evaluate our approach with two following metrics for customized generation proposed in Textual Inversion . (1) Image similarity, which measures the visual similarity between the generated images and the target subjects. For multi-subject generation, we calculate the image similarity of the generated images and each target subject separately and finally calculate the mean value. (2) Textual similarity, which evaluates the average CLIP  similarity between all generated images and their textual prompts. To this end, we use a variety of prompts with different settings to generate images, including modifying the scenes, attributes, and relation between subjects.

**Baselines.** To evaluate our generation quality, we compare our approach with three state-of-art competitors, _i.e._, _DreamBooth_ that fine-tunes all parameters in diffusion model; _Custom diffusion_ that optimizes the newly added word embedding in text encoder and a few parameters in diffusion model, namely the key and value mapping from text to latent features in the cross-attention; and _Cones_ that finds a small cluster of neurons in diffusion model corresponding each customized subject. As Custom diffusion and Cones demonstrated, we omit Textual Inversion  as it performs much less competitively. And the implementation details of our approach and those of baselines are also reported in the Appendix A.

### Main results

In this section, to demonstrate the superiority of our approach, we conduct experiments on authentic images from diverse categories, including objects, pets, backgrounds, _etc._. We not only present the qualitative results between our approach and other baselines but also showcase quantitative comparison. This further substantiates the effectiveness of our approach.

**Qualitative comparison.** As depicted in Fig. 3, we present a collection of generated images featuring two to four subjects. For single-subject generation, as shown in Appendix B.1, our approach achieves comparable results to competing methods while requiring significantly less storage space. However, as the number of subjects increases, the other three methods fail to include certain subjects and exhibit attribute confusion, resulting in generated images that deviate from the reference images. In contrast, our approach consistently produces highly visually accurate images for all subjects. It is important to note that our approach utilizes learned single-subject residual token embeddings for seamless combinations without retraining, thereby avoiding exponential training costs associated with the other methods. The next section will discuss this in detail.

**Quantitative comparison.** In the context of generating customized subjects with varying numbers, we have carefully selected four evaluation metrics: textual similarity, visual similarity, required storage space, and computational complexity. As shown in Tab. 1, for single-subject generation, our approach exhibits slightly lower visual and textual similarity compared to DreamBooth whileremaining comparable to the other two methods. However, as the number of subjects increases, our approach consistently outperforms the other methods across all four evaluation metrics. This demonstrates its effectiveness in capturing subject characteristics, maintaining fidelity to the given prompt, and achieving higher efficiency in practical applications. Please note that the storage space and computational complexity presented in Tab. 1 for our approach assume that there are no existing learned single-subject residual token embeddings. However, in practice, if we already have required subject-specific residual token embeddings for multi-subject generation, no additional storage space or computational complexity is needed. This ability to seamlessly combine existing models without retraining is a unique advantage of our approach. In contrast, other methods require new storage space and training time for generating multiple new subjects.

**User study.** We conduct a user study to further evaluate our approach. The study investigates the performance of the four methods on the multi-object customization task. For each task and each method, we generated 80 images from 4 subjects combination, 4 conditional prompts, and 5 random seeds, resulting in 1,280 generated images for the whole user study. We presented two sets of questions to participants to evaluate image similarity and textual similarity. Taking the prompt "A cat and a dog on the beach" as an example, where "cat" and "dog" are customized subjects, we provided reference images of the customized subjects and asked participants questions like: "Does the image contain the customized cat?" to evaluate visual similarity. For textual similarity, based on the textual description, we presented questions like "Does the image contain a cat and a dog?". As shown in Tab. 2, our approach is most preferred by users regarding both image and text alignment.

### Towards challenging cases

In this section, we further illustrate our superiority by showcasing two scenarios: generating a larger number of customized subjects and generating subjects with high semantic similarity that other methods fail to achieve.

Figure 3: **Qualitative comparison of multi-subject generation ability between our approach and baselines. Our approach surpasses existing alternatives with higher success rate in generating these subjects and less attribute confusion among different subjects.**

**Customization with similar subjects.** To illustrate the superiority of our approach in mitigating attribute confusion among multiple customized subjects, we select Cones, one of the methods that perform well in multi-subject generation among the three baselines, and compare the generated images with our approach in challenging cases shown in Fig. 4. We observe that when the raw categories of the customized subjects have high semantic similarities, especially in the case of two customized dogs, Cones exhibits a notable propensity for attribute confusion to arise. In contrast, our approach demonstrates excellent performance in both visual and textual similarities.

**Customization with a large number of subjects.** As shown in Fig. 3, we observe a significant decrease in the quality of generated images by other methods as the number of customized subjects increases. However, our approach shows a relatively smaller impact. Therefore, in Fig. 5, we present the generated images with an increased number of customized subjects, further demonstrating the effectiveness of our approach.

### Ablation studies

**Verify the effect of strength and weaken cross-attention map.** We conduct ablation experiments to examine the individual effects of strengthening the target subject region and weakening irrelevant subject regions as shown in Fig. 6. We observe that strengthening the target subject alone can lead to attribute confusion between subjects. For instance, it may result in a customized subject exhibiting attributes of another subject. However, when only irrelevant subjects are weakened, certain subjects may fail to show up or exhibit a lack of specific attributes. Furthermore, for simple combinations like "mug + teapot," satisfactory results could be achieved with 30 steps of guidance. However, for more challenging combinations such as "cat + dog," 50 steps of guidance were required to achieve better attribute binding results.

    &  &  &  &  \\   &  Text \\ Alignment \\  &  Image \\ Alignment \\  &  Text \\ Alignment \\  &  Image \\ Alignment \\  &  Text \\ Alignment \\  &  Image \\ Alignment \\  &  Text \\ Alignment \\  & 
 Image \\ Alignment \\  \\ 
**Single** &  71.35\(\%\) \\  &  **71.50\(\%\)** \\  &  **76.85\(\%\)** \\  &  67.60\(\%\) \\  &  76.85\(\%\) \\  &  69.60\(\%\) \\  &  75.05\(\%\) \\  & 
 69.05\(\%\) \\  \\ 
**Two Subjects** &  52.58\(\%\) \\  &  43.13\(\%\) \\  &  59.88\(\%\) \\  &  46.83\(\%\) \\  &  62.55\(\%\) \\  &  57.50\(\%\) \\  &  **77.87\(\%\)** \\  & 
 **69.75\(\%\)** \\  \\ 
**Three Subjects** &  57.78\(\%\) \\  &  31.83\(\%\) \\  &  58.20\(\%\) \\  &  34.28\(\%\) \\  &  64.87\(\%\) \\  &  37.94\(\%\) \\  &  **79.20\(\%\)** \\  & 
 **64.42\(\%\) \\  \\ 
**Four Subjects** &  36.42\(\%\) \\  &  25.63\(\%\) \\  &  40.73\(\%\) \\  &  25.44\(\%\) \\  &  42.10\(\%\) \\  &  28.75\(\%\) \\  &  **77.38\(\%\)** \\  & 
 **59.08\(\%\)** \\  \\   

Table 2: **User study.** The value represents the percentage of users that score positive for the image generated corresponding to the given questions. The results show that our approach is the most preferred by users for multi-subject customization, on both image and text alignment.

    &  &  &  &  \\   &  Text \\ Alignment \\  &  Image \\ Alignment \\  &  Text \\ Alignment \\  &  Image \\ Alignment \\  &  Text \\ Alignment \\  &  Image \\ Alignment \\  &  Text \\ Alignment \\  & 
 Image \\ Alignment \\  \\ 
**Single Subject** &  71.35\(\%\) \\  &  **71.50\(\%\)** \\  &  **76.85\(\%\)** \\  &  67.60\(\%\) \\  &  76.85\(\%\) \\  &  69.60\(\%\) \\  &  75.05\(\%\) \\  & 
 69.05\(\%\) \\  \\ 
**Two Subjects** &  52.58\(\%\) \\  & 
 43.13\(\%\) \\

**Verify the effect of guidance in generation process.** Recent studies [37; 38] have also demonstrated that incorporating guidance during the sampling process leads to superior generation results. We select _Cones_, which exhibits relatively better performance among competing methods, as our baseline, and compare it with the state-of-the-art semantic guidance approach as well as our guidance approach. As shown in Fig. 7, combining the Attend and Excite  improves the generation quality compared to cones, and further improvement is achieved when combine with our guidance approach. However, overall, our approach outperforms others, showcasing the best performance. This proves that the problem of attribute confusion in other methods can't be solved by simply adding a guidance algorithm while combining our residual token embedding with our guidance algorithm can solve it.

Figure 4: **Visualizations of customization with challenging cases.** When the subjects that need to be customized belong to the category with high semantic similarity (shown in the first row) or even the same category (shown in the second row), the baseline using joint training has a serious attribute confusion problem, while our approach circumvents this problem.

Figure 5: **Visualizations of customization with a large number of subjects.** Here we show diverse generation results of customizing 5 and 6 subjects.

## 5 Conclusion

This paper proposes a novel approach for multi-subject customization. Our method combines subject-specific residual token embeddings with a pre-trained diffusion model and utilizes easy-to-obtain layout prior to guiding the generation process. This allows us to combine individually learned subject-specific residual token embeddings for multi-subject customization without retraining. Our method consistently delivers exceptional performance even in challenging scenarios, including the customization of image synthesis with six subjects and the customization of semantically similar subjects. Through qualitative and quantitative experiments, we demonstrate our superiority over existing state-of-the-art methods in various settings of multi-subject customization. These results highlight the effectiveness and robustness of our method.