# Discovering Creative Behaviors through DUPLEX:

Diverse Universal Features for Policy Exploration

Borja G. Leon

Iconic

borja@iconicgames.ai

&Francesco Riccio

Sony AI

francesco.riccio@sony.com

&Kaushik Subramanian

Sony AI

kaushik.subramanian@sony.com

&Peter R. Wurman

Sony AI

peter.wurman@sony.com

&Peter Stone

Sony AI

The University of Texas at Austin

peter.stone@sony.com

Work done at Sony AI.

###### Abstract

The ability to approach the same problem from different angles is a cornerstone of human intelligence that leads to robust solutions and effective adaptation to problem variations. In contrast, current RL methodologies tend to lead to policies that settle on a single solution to a given problem, making them brittle to problem variations. Replicating human flexibility in reinforcement learning agents is the challenge that we explore in this work. We tackle this challenge by extending state-of-the-art approaches to introduce DUPLEX, a method that explicitly defines a diversity objective with constraints and makes robust estimates of policies' expected behavior through successor features. The trained agents can (i) learn a diverse set of near-optimal policies in complex highly-dynamic environments and (ii) exhibit competitive and diverse skills in out-of-distribution (OOD) contexts. Empirical results indicate that DUPLEX improves over previous methods and successfully learns competitive driving styles in a hyper-realistic simulator (i.e., GranTurism(tm) 7) as well as diverse and effective policies in several multi-context robotics MuJoCo simulations with OOD gravity forces and height limits. To the best of our knowledge, our method is the first to achieve diverse solutions in complex driving simulators and OOD robotic contexts. DUPLEX agents demonstrating diverse behaviors can be found at [https://ai.sony/publications/Discovering-Creative-Behaviors-through-DUPLEX-Diverse-Universal-Features-for-Policy-Exploration/](https://ai.sony/publications/Discovering-Creative-Behaviors-through-DUPLEX-Diverse-Universal-Features-for-Policy-Exploration/).

## 1 Introduction

In non-stationary complex environments, reinforcement learning (RL)  agents are compelled to exhibit flexible and diverse behaviors to robustly adapt to different scenarios and interact with other actors . To this end, a growing community is researching methodologies to train agents that, unlike conventional RL , can solve tasks with a diverse set of near-optimal strategies . Such methodologies are explicitly crafted to enhance the exploration of the state-action space, equippingagents with the capability to discover qualitatively diverse solutions for a given task distribution. For example, when planning a route from home to work, a human commuter might identify one route that uses the highway and one that sticks to side streets. While using the highway may be optimal in expectation, using the alternative route may be called for different contexts that influence the traffic report, or weather forecast.

Existing approaches struggle to generalize to highly dynamic environments and are designed to learn in single-task settings . To alleviate this, we introduce _Diverse Universal features for PoLicy EXploration_ (DUPLEX), an algorithm that trains an agent to optimize a set of diverse policies over different contexts. In this work, contexts comprise two components: a task requirement and a description of the current environment dynamics. To better ground such concepts, let us refer to the previous commuter example. In this case, we would refer to task requirements as time-to-completion. On the other hand, factors such as the weather forecast and traffic report would form part of the environment dynamics DUPLEX builds on top of the strengths of diversity learning  in RL and universal estimators (UE)  to introduce a novel methodology that preserves both performance and diversity in highly dynamic environments and multi-context settings - which is key to enabling human-level task execution. For example, if the commuter is a human that injures their leg, they can immediately balance on one leg and even jump forward without using the injured leg. We aim to transfer this adaptability to agents in different contexts. To this end, we adopt the Contextual Markov Decision Process (CMDP) framework , where different episodes correspond to different contexts. As shown in Figure 1, DUPLEX is designed to receive three inputs representing the encoding of the context, policy, and state. Contexts are represented as feature vectors and uniformly sampled from a discrete set of predefined task requirements and dynamics settings (e.g., weather condition); the \(i\)-th policy in the set of policies \(\) is selected and passed to the model (e.g., different routes); and the state comes directly from the environment observation. Then, the critic output is split into different heads to support the estimation of the extrinsic and intrinsic rewards, and successor features (SFs) . We refer to the reward coming from the environment as the extrinsic reward, while the metric computed to promote diversity is the intrinsic reward. It is worth noting that, at the beginning of each training episode, we uniformly sample a context from \(C\) and a policy from \(\) to collect experience. DUPLEX then iteratively trains the set of policies to maximize both the distance in their successor features (guided by intrinsic rewards) and the task performance (given by the extrinsic rewards). We rely on SFs to measure distances as they, by definition, represent the state features that a given policy is expected to experience along a trajectory, and thus, they are an intuitive way to quantify policy diversity.

Hence, our main contribution is DUPLEX, a novel algorithm that contributes to diversity learning in RL by improving on previous work to better preserve the diversity vs. near-optimality trade-off in highly-dynamic environments and multi-context settings. We evaluate our approach on the real-time, physically realistic, car-racing simulator GranTurismo(tm) 7 (GT)  and on two multi-task MuJoCo environments with changing dynamics (Walker2D, Ant) . Our experimental results indicate that

Figure 1: DUPLEX data flow while training three policies over a set of two contexts. At each iteration, we provide three inputs to our multi-policy agent: (i) a context vector describing task requirements and environment dynamics in the current episode \(c\); (ii) an encoding of the policy used in the episode \(z\); (iii) and the current state of the environment \(s\). The critic network returns estimates for the intrinsic and extrinsic rewards and successor features to drive diverse behavior discovery. Finally, the algorithm samples policies in \(\) uniformly and rolls them out to collect more experience.

DUPLEX improves over previous state-of-the-art RL diversity baselines  and UE baselines [8; 9]. In fact, DUPLEX is the only algorithm to learn diverse competitive policies in GT and outperforms other baselines when evaluated in out-of training distribution (OOD) contexts. Moreover, we conduct a detailed ablation study that isolates the effects of each of DUPLEX's main components.

## 2 Related Work

In this work we build on top of two main bodies of research: diversity learning and universal function approximators. We organize related work accordingly.

Diversity.Diversity learning is gaining attention within the research community, due to the important benefits it yields to autonomous agents. Such benefits include generalization , exploration [15; 16], creativity , and self-play **(author?)**. In this work, we extend the application of diversity learning to highly-dynamic environments and multi-context settings. A central precursor to our work is DOMiNO . DOMiNO employs RL to maximize extrinsic and intrinsic diversity rewards, which are combined with Lagrange multipliers and Van der Waals (VdW) forces to balance diversity and performance. DOMiNO has two variants: one bases its diversity objective on the average of input features which grants stability at the cost of storing a moving average for every task - limiting scalability. The second variant uses the critic to estimate SFs, which solves the scalability issue at the cost of instability due to the added learned target. Intuitively, such limitations prevent DOMiNO from being effective at discovering diverse behaviors in CMDPs. Further, the Lagrange multipliers and VdW forces can limit diversity in complex environments where creative behaviors need significant exploration before providing satisfying returns . Such limitations become more evident in highly dynamic environments such as GT (see Section 5). Our algorithm is instead designed to tackle these limitations.

Quality-Diversity (QD) is another active line of work within diversity learning that involves evolutionary methods. QD aims to generate large collections of diverse and high-performing solutions, primarily through evolutionary optimization [18; 19]. In QD, diversity is measured through a descriptor space defined by the user . Some of the main approaches in QD include MAP-Elites [21; 22], local competition  and more recently those that combine RL with QD [24; 25; 26]. Closer to our research, **(author?)** introduce a variant for MAP-Elites to learn diverse behaviors for multiple tasks within the training distribution. Our framework diverges from QD in several ways. First, QD algorithms emphasize diversity through evolutionary strategies, while our method relies on maximizing a reward objective. Second, QD requires the user to define a diversity descriptor, while we only restrict diversity to be in the near-optimal space. Third, one of our main goals is finding competitive diverse behaviors for OOD tasks and dynamics, an objective that has not been studied in Quality-Diversity (QD).

Universal Function Approximators.To make more robust estimations of the policies' expected behaviors, and thus to better quantify diversity, our work pivots around universal function approximator (UE). UE research is grounded on factoring the value estimates separating states from tasks  and policies . Notably, **(author?)** improve the formalization of successor features , an estimation of state-action visitation, by conditioning their estimation on both task and policies. Still, approaches based on SFs are brittle in complex domains [29; 30], requiring sophisticated network architectures [31; 32] to work effectively. Akin to **(author?)**, we adopt a similar approach to transferring learning of SFs to continuous domains by combining them with SAC . However, we achieve an improved estimation of the expected SFs by employing the average of the critic outputs and by adding the entropy term to the SFs learning objective. We find that the addition of the entropy term is a novel component that DUPLEX carries with and it is key to improving robustness and performance (see Appendix B).

## 3 Background and Notation

We briefly introduce the main building blocks of DUPLEX and the notation we adopt. First, we review basic concepts of multi-task RL and explain how it can be represented by the contextual-MDP framework. Next, we describe how universal estimators are used to enhance generalization across contexts (especially when context enumeration is impractical).

Multitask RL.We consider training a multi-policy RL agent that solves a CMDP  represented as a tuple \(=,,P,R,,_{C},_{S}\), where \(,\) are the state, action spaces respectively, \(R\) is the reward function, \(P\) is the unknown transition function, \(_{S}\) is the initial state distribution conditioned on the context, and \(_{C}\) is the context distribution. We use context \(c\{u,w\}\) to summarize both information about the particular dynamics \(u\) of that environment (e.g., the effect of gravity), and information about the task \(w\) in the current episode (e.g., position constraints). Every episode starts with a context \(c_{C}\) and an initial state \(s_{0}_{S}( c)\). Then, at each time step \(t\), the agent selects an action \(a_{t}\) according to its policy \(( s_{t},c)\), receives a reward \(r_{t} R(s_{t},a_{t},c)\) and transitions to the next state \(s_{t+1} P( s_{t},a_{t},c)\). Policies are characterized by their state-action occupancy, i.e., how often a policy visits a state-action pair. We consider two state-action occupancy metrics: \(d^{}_{_{c}}(s,a)=_{T} _{t=1}^{T}_{_{c}}(s_{t}=s)(s,a,c)\) for the average occupancy metric and \(d^{}_{_{c}}(s,a)=(1-)_{t=1}^{}^{t} _{_{c}}(s_{t}=s)(s,a,c)\) for the discounted case - where \(\) is the probability measure of states at \(t\) induced by \(\) in \(c\). The objective of our algorithm is to find a set of diverse policies that maximize the expected return in every context \(_{d_{_{c}}}_{s,a,c}r(s,a,c)d_{_{c}}(s,a)\), where \(\) is the set of admissible distributions .

Universal estimators.When aiming to solve multiple tasks, we follow the common approach of decomposing the input of the neural network to facilitate transfer learning between tasks and policies. Methods that are relevant for our work include "Universal Value Function Approximators" (UVFA) , which add a task-descriptor vector \(w\) as input to a value function approximator parameterized by \(\), \(V_{}(s,a,w)\). If \(V_{}\) is smooth w.r.t. \(w\), then \(V_{}\) is expected to generalize across tasks within the training task space. Akin to previous works , we assume that every state-action pair is correlated with observable _features_ known as "cumulants" \((s,a,c)^{d}\). These cumulants can either be given through relevant properties within the state2 or be learned. In a similar fashion to value functions, we can define _expected features_\(_{_{c}}(s,a)=_{s^{},a^{} d_{_{c}}(s,a)} (s^{},a^{},c)^{d}\), which we will refer to as average (\(^{}\)) or discounted (\(^{}\)) expected features when using \(d^{}_{_{c}}\) and \(d^{}_{_{c}}\) respectively. In the discounted case, \(^{}\) are also known as "successor features" (SFs). This latter formalization is key to our work and also to USFA , a general framework that combines UVFA-like task decomposition with the SFs, and exploits generalized policy improvement algorithm (GPI) . Importantly, USFA disentangles both tasks and policies by giving as input a vector \(z\) that is a representation of the current policy \(z=e()\), where \(e\) is an encoding function - in our approach \(e\) encodes policies as one-hot vectors \(z\).

## 4 Learning Near-Optimal Diverse Behaviors in Multi-Goal Continuous Settings

DUPLEX is a method designed to provide diverse solutions to complex tasks by simultaneously maximizing both the true reward and dissimilarity within a set of policies \(\) across the different environments of a CMDP. Thus, according to previous work  we define diversity as:

**Definition 4.1** (Diversity): Diversity\(()\) _is a metric of dissimilarity among policies in a set \(\) with a common goal. Formally, if \(_{_{i}}\) and \(_{_{j}}\) are a function of state-occupancy of relevant features of any two policies in \(\), then their dissimilarity is given by \(||_{_{i}}-_{_{j}}||\). A non-zero value of this norm indicates dissimilarity, with larger values indicating greater divergence between the policies. Mathematically, diversity is defined as the sum of the minimum L2 dissimilarity norms in \(\):_

\[()=_{ _{i},_{j},\\ i=j}||_{_{i}}-_{_{j}}||_{2}^{2}\]

Using this definition, we formulate our learning problem and describe DUPLEX's main contributions in the following sections. We first extend the formulation of diversity learning to context-conditioned environments, then we introduce a novel mechanism to more efficiently control the diversity-performance trade-off, and finally, we describe how to improve SFs estimation.

### Context-conditioned Diversity Learning

Similar to **(author?)**, our objective is to maximize diversity within a set of policies \(\). We express the distances between policies in \(\) in terms of expected features \(\), which are a function of their state-occupancy (). We measure \(\) distances following Def. 4.1 and use a context-based Hausdorff distance  to enforce context-conditioned diversity within \(\). We aim at training an RL agent that, given a context \(c\), discovers a set of \(n\) near-optimal policies \(=\{_{c}^{i}\}_{i=1}^{n}\) by successfully optimizing:

\[_{}\ ()\ \ d_{_{c}}  r_{e}_{e},_{c} \]

where \(d_{_{c}}\) is the occupancy metric of a context-conditioned policy, \(r_{e}\) is the environment reward (or _extrinsic_ reward), \(\) is a hyper-parameter defining the near-optimality region that we refer to as _optimality-ratio_, and \(_{e}\) is the value of a target policy. In practice, the target policy refers to a policy in \(\) that ignores the diversity objective and is trained exclusively to maximize the extrinsic rewards. As in , the policy value \(v\) expresses the expected reward accumulated by a policy \(\). According to Definition 4.1, diversity is a distance over a set of occupancy metrics \(:\{^{|S||A||C|}\}^{n} \) and Eq. 1 is defined to maximize the diversity of the occupancy metrics and preserve the near-optimality of the policies in \(\).

To promote diversity between different policies, we adopt the repulsive reward from  and extend it to be conditioned on the context. More formally, given the tuple \( s,a,^{i},c\) and a cumulant function \(\), the diversity reward that we maximize for is:

\[r_{d}^{i}(s,a,c)=(s,a,c)(_{_{c}^{i}}-_{_{c}^{i}}) \]

where \(_{_{c}^{i}}\) are the expected features from policy \(^{i}\) at \((s,a,c)\) and \(_{c}^{i}\) refers to the policy with the closest expected features to \(^{i}\) in \((s,a,c)\) according to Def.4.1. Intuitively, since contexts \(c_{C}\) are fixed for each training episode, this reward encourages the algorithm to train policies that visit different state-action pairs within each context \(c\), thus, promoting context-conditioned diversity.

#### 4.1.1 Stabilising Diversity across Different Domains

Eq. 2 expands DOMiNO's repulsive force to operate across different contexts. However, the scale of intrinsic rewards varies greatly through different environments (i.e., different contexts will inherently yield different successor features). We need to stabilize fluctuations of such rewards when working with diverse contexts. We introduce two constraints to the learning objective that modulate the intrinsic reward \(r_{I}\): a _dynamic intrinsic reward factor_\(\) that scales \(r_{d}\) to target a factor of the moving average of the general extrinsic value and a _soft-lower bound_\(\) that limits the search of diverse policies to a near-optimal subspace. We then define the DUPLEX intrinsic reward as:

\[r_{I}= r_{d} \]

where \(\) and \(\) are computed independently by the algorithm.

Dynamic intrinsic reward factor.\(\) scales \(r_{d}\) proportionally to the sum of extrinsic values of policies in \(\). Formally, \(r_{d} v_{_{}}=_{i=1}^{n}v_{_{}}^{i}\) with \(v_{_{}}^{i}=_{_{}}v_{_{}}^{i}+(1-_{v_{}})r_{e,t}^{i}\) where, \(_{v_{}}\) weights the contribution of the extrinsic value of the i-\(th\) policy and its immediate extrinsic reward. Finally, \(r_{e,t}^{i}\) is the extrinsic reward at \(t\) when the agent acts under policy \(i\). Intuitively, we are scaling the intrinsic rewards according to the average extrinsic value that the set of policies is achieving. Hence, at each algorithmic iteration, \(\) is updated as follows:

\[_{t}=_{}_{t}^{}+(1-_{})_{(t-1)} \]

where \(_{}\) is the update rate of \(\), and \(^{}=|v_{_{}}/v_{d_{}}|(1-)\) is the target value of the update, where \(v_{d_{}}\) is the return value based on diversity intrinsic reward \(r_{d}\) while \(v_{_{}}\) is based on the extrinsic reward \(r_{e}\) instead. Through the optimality ratio \(\), \(\) minimizes the domain dependency by scaling the intrinsic rewards as a factor of the extrinsic objective and the optimality ratio \(\). Regarding the update rate \(_{}\), once we converged to a stable value, we kept it fixed across all experiments and domains.

Soft-lower bound.While \(\) preserves a relationship between extrinsic and intrinsic rewards, it does not prevent policies in \(\) from exploring regions of the search space that are too far from the near-optimality regions of the target policies. To limit this possibility, we explored Lagrangian-constrained optimization but found it unsatisfactory in complex domains like GT. Hence, we introduce \(\) to bound the near-optimal subspace for each policy using:

\[=\{_{k}(_{}}-_{ }}}{|_{}}+l|})\}_{i=1}^{n} \]

where \(_{k}(x)=1/(1+e^{-kx})\) and \(\) is a hyper-parameter indicating the reward region we are interested in exploring, \(k\) regulates how "soft" is the bound, \(n\) is the number of policies, \(l\) is a small constant to prevent division by zero, and \(v_{}}\) is the average extrinsic value. Intuitively, \(\) limits the exploration of diverse behaviors to a near-optimal area defined by the threshold \(\). We find that introducing a sigmoid-based limit provides a more stable solution than Lagrangian-constrained optimization (See Figure 2). Similarly to \(_{}\), once we found a stable value for \(k\), it remained fixed for all experiments.

### Estimating Successor Features

Since we rely on \(\) to determine policy diversity (Def. 4.1), it is fundamental to estimate \(\) reliably. Additionally, as mentioned in Section 3, it is not tractable to keep \(^{}\) for each context and task since they may be infinite. Thus building on top of UE, we exploit discounted expected features (SFs) \(^{}(s,a,z,c)\) and incorporate an extra head in the critic to estimate SFs \(^{}(s,a,z,c)\).

However, as in related work , we found that the critic struggles to correctly estimate \(^{}\) in complex settings. Specifically, since intrinsic rewards are bootstrapped over the expected difference of successor features, the accuracy in the SFs estimation plays a major role. Taking inspiration from the SAC algorithm , we incorporate an entropy term in the expected features objective to account for the stochastic component within the learned policies.

For policy \(i\) in context \(c\), our SFs at \(s_{t}\) are computed as:

\[^{,i}(s_{t},a_{t},c)=_{t}+_{_{c}}_{k=t+1}^{ }^{k-t}_{k}+_{H}H_{c}^{i}(|s,c)  \]

where \(_{H}\) is the entropy weight. Intuitively, unless the critic is confident that the actor will visit a state-action pair more frequently than others, the network will be encouraged to estimate that the policy will maximize the entropy. We formulate our temporal difference loss for the critic output \(^{}\) to account for the entropy term as:

\[_{}(^{}_{_{j}}(_{c}^{ z}))=^{z}}{}^{}_{_{j}}(s,a,c,z)-y(,s^{},c,z)^{2 } \]

where, similar to SAC, we have two critic estimates \(j=\{1,2\}\), but instead of taking the minimum of the two, our target is obtained from the average of the estimates. Then, we define \(y(,s^{},c,z)\) as:

\[y(,s^{},c,z)=(t)+(} {}_{},j}(s^{},^{}_{z},c)- _{}^{z}(^{}_{z}|s^{},c)) \]

with \(^{}_{z}_{}^{z}(|s^{},c)\). The motivation lies in the different impacts of overestimation of values and successor features. When predicting values, overestimation directly influences the policy since it is trained to maximize advantage. However, in DUPLEX, SFs assume a distinct role: the policy is not incentivized to select actions with the highest successor features but rather those that differ the most from the SFs of the remaining policies (subject to not sacrificing too much performance, equation 1). Consequently, precision becomes of greater significance for \(^{7}\), and taking the average emerges as a more reliable solution.

The introduction of entropy improves stability in the estimation of SFs through most of the training process and, as we report in the experimental section, it supports OOD generalization. However, as learning stabilizes, we typically encounter phases where the estimated difference between policies rapidly increases its order of magnitude. For this reason, we also introduce a fixed upper bound to \(_{}\) when estimating SFs. Details on the upper bound, DUPLEX pseudocode, and hyper-parameters are included in Appendix C and D.

Summarizing, DUPLEX enhances diversity learning in context-conditioned environments through four key components: (i) _dynamic intrinsic reward factor_ (\(\)) that balances diversity with extrinsic rewards and avoids environment-specific tuning; (ii) _soft lower bound_ (\(\)) that constrains policies to optimize for diversity only within a target near-optimal region; (iii) _entropy regularization_ (\(_{H}\)) in successor feature estimation to improve robustness; and (iv) averaging over critic estimates for computing diversity rewards. In Section 5 we demonstrate how these features improve both performance and diversity of the learned policies.

## 5 Experiments

The objectives of our experiments are to demonstrate that DUPLEX: 1) is the first successful method in learning diverse driving styles in highly-dynamic physics simulators such as GT (Figure 2); 2) when compared against previous state-of-the-art diversity approaches in RL, yields a better performance vs. diversity trade-off in canonical physics simulators (Figure 3); 3) exhibits diverse and effective behaviors in OOD dynamics and tasks (Figure 4); and finally, 4) improves on previous baselines in estimating SFs (Figure 5 and 6). All results in MuJoCO are obtained from five training runs. GT experiments are more computationally demanding requiring a minimum of seven days to converge, so the results are obtained from three training runs.

Baselines.Our main baseline is DOMiNO , which is the state-of-the-art of diversity learning in RL. Then, since DOMiNO is not designed for OOD generalization, our experiments in OOD settings include as baselines the two most popular frameworks within UE literature: UVFA  and USFA . Note that, throughout the section and given the two versions of DOMiNO, we will refer to

Figure 2: Results in GT. a) is the reward-diversity trade off, DUPLEX/DOMiNO_best_policy refers to \(_{_{e}}()\) of every run, while DUPLEX/DOMiNO_avg refers to \(_{_{e}}()\) of that run. Differently from DOMiNO, DUPLEX is able to generate _both_ competitive and diverse behaviors. Figure b) illustrates the number of policies that finish laps (active). Figure c) min lap times of the diverse policies. Note that the number of active policies is important to showcase that all policies are actively searching for near-optimal diverse behaviors through different epochs. Applying our soft lower bound (\( 0\)) is key to focusing diversity in the region of interest.

DOMiNO\({}_{avg}\) as the version of the algorithm using averaged cumulants as successor features, and as DOMiNO\({}_{est}\) as the version of the algorithm that estimates SFs.

Benchmarks.We use three benchmarks. The first is a racing track in GT, where the goal is to have different driving styles while completing fast laps. Achieving diverse and competitive driving styles in such realistic simulator  can unlock multiple applications both in gaming and self-driving. Second, a set of experiments is conducted in the MuJoCo's Walker2D and Ant environments where we compare DUPLEX against DOMiNO\({}_{avg}\) in the same environments used in . Finally, in the last benchmark, we evaluate our approach in a multi-context version of MuJoCo's Walker2D and Ant environments. We extend the default versions of these scenarios to include a three-dimensional context \(c\) where the inputs represent: the gravity coefficient and, the upper and lower height thresholds we would like the agent to respect while moving. The goal of this benchmark is to find diverse policies in OOD contexts while guaranteeing competitive performance in task execution. Note that,

Figure 4: Results in MuJoCo Walker (Top) and Ant (bottom) multitask environments. On the y-axis, we report the reward accumulated by the agent while on the x-axis normalized diversity according to Def. 4.1. Ideally, we want to be as close as possible to the top right region. OOD dynamics refers to gravities that are at least 40% stronger or weaker than the strongest and weakest gravities seen in training, respectively. OOD tasks represent tasks where the agent needs to walk forward below a height 20% lower than the lowest height seen in training. Only DUPLEX and UVFA are evaluated in the Test since the other algorithms already failed in training.

Figure 3: Walker single-task results. Both DUPLEX and DOMiNO do not estimate SFs and use \(_{_{e}}^{}\). On the left, we show results from the best policy in each variant while on the right, the average reward across all policies. Dots and lines represent mean and standard deviation, respectively. It is worth reporting that the vanilla SAC implementation scores 5576.24\(\)256.783 when evaluated at the same training epoch.

to have a fair comparison against the baselines, only the last benchmark configures DUPLEX to estimate SFs through \(^{}\). Additional details about the metrics and the environments are reported in Appendix A.

Results in GranTurismo(tm) 7.As in , we use QRSAC as the base RL algorithm for both DOMiNO\({}_{avg}\) and DUPLEX. Figure 2 (top-left) shows that only DUPLEX learns diverse policies in the near-optimal region. Figure 2(c) illustrates how this diversity translates into diverse lap times. Moreover, Figure 2(b) provides evidence of the benefits that our lower bound \(\) formulation carries with it. When using \(=0\), the most diverse policies are "busy" finding different ways to _not_ finish the track. Furthermore, we see DUPLEX results oppose to DOMiNO that instead collapses all policies to a similar behavior even when using an extremely forgiving value of the optimality ratio \(\). It is worth mentioning, that DOMiNO fails to provide diversity even if it implements a Lagrangian constraint optimization objective that should support more adaptability. For example, when configuring \(=0.01\), DOMiNO should consider as "acceptable" any diverse policy achieving 1% of the value of the target policy \(_{e_{}}\). But still, the algorithm struggles to find diverse policies.

Results in MuJoCo.To further validate the contribution of DUPLEX to diversity learning, we include a direct comparison of DUPLEX and DOMiNO\({}_{avg}\) while configuring the Walker2D environment that the authors present in . Within this set of experiments, both algorithms have been configured to work with \(^{}\) which fosters the opportunity to ablate the benefits of our constrained \(r_{I}\) (Eq. 3) isolated from the impact of estimating SFs. Figure 3 includes the results from both frameworks with multiple optimality ratios \(\). Moreover, to provide an exhaustive evaluation, we also configure DOMiNO\({}_{avg}\) with various Van Der Waal (VdW) distance hyper-parameters. Such a parameter modulates the desired distance between all the policies in \(\) (see  for details). Finally, DUPLEX is able to find a better Pareto-frontier for the quality-diversity objective (Figure 3 right), while achieving better near-optimal policies than the baseline (Figure 3 left).

Results in multi-context MuJoCo.Figure 4 reports the results of our baselines in multi-context scenarios when evaluated within- and out-of training distribution. In the within-distribution setting (Figure 4 left-hand side) only DUPLEX shows good performance and diversity while USFA and DOMiNO fail at both learning competitive policies and providing diversity. As we report in the USFA ablation (B), DUPLEX succeeds in such benchmark due to the entropy term introduced to make the estimation of SFs more robust (Eq. 7). Additionally, note that USFA and UVFA do not optimize for diversity, and their diversity score is expected to be close to zero. Then, Figure 4 (right-hand side) reports the generalization effectiveness of DUPLEX and UVFA when operating out-of training distribution. Note that USFA and DOMiNO do not qualify in this setting as they failed to learn already in the training set. We observe that in all the OOD scenarios DUPLEX outperforms UVFA and, most importantly, it covers a bigger range of diversity values while completing the tasks.

Ablation of successor-feature estimation.Figure 5 illustrates the contribution of \(^{}\) and the different features incorporated by DUPLEX. We have evidence that all features that compose DUPLEX are key to guaranteeing robust learning of diverse behaviors in highly-dynamic environments and

Figure 5: Ablation of DUPLEX in Multitask Walker. Reward and diversity scores of the best trained policy are reported on the left, while averaged values of all policies are on the right.

multi-context settings. Notably, USFA can also effectively solve multi-context MuJoCo environments when akin to DUPLEX, adds the entropy term (see Appendix B).

## 6 Conclusions

DUPLEX provides a powerful technique that enables learning of diverse near-optimal behaviors. Our experimental session shows that (differently from other baselines) DUPLEX (i) learns effective diverse behaviors in hyper-realistic complex domains (such as GranTurismo) and (ii) generalizes to OOD in challenging multi-context MuJoCo environments by demonstrating competitive diverse behaviors.

**Limitations and Future Directions.** There is potential to refine and enhance the methodology presented in this work. Nevertheless, we perceive the current limitations of DUPLEX not as setbacks, but as exciting opportunities that pave the way for new research avenues. For example, (1) can we generate more than twenty diverse policies and still guarantee meaningful diversity? (2) DUPLEX carries an additional cost due to the need to compute SF distances of each policy pair in our set, thus how can we improve sample efficiency and keep a low computational footprint? (3) DUPLEX does not impose any exploration strategy on each policy, and thus, can we control in what measure a particular policy will be different? And finally, (4) can we combine the strengths of different on a single solution? This latter question is very interesting to us. We suppose that once diverse policies have been learned, each of them retains different useful skills (e.g. different commuting routes) that can be combined dynamically to reconstruct a single agent capable of acting optimally and tackling unseen portions of the state-space.

Finally, the presented approach holds significant promise for advancing research in diverse policy learning, scalability, and efficient transfer and exploration [36; 37] across varied contexts. We note also that RL algorithms may also risk reinforcing biases and require careful implementation to ensure transparency and fairness. Diversity-learning algorithms might result in training policies with potentially unethical behaviors that are harder to predict. These are reasons that motivate us to pursue controllability while learning diverse policies in this and future works.