# Randomized Exploration in Cooperative Multi-Agent Reinforcement Learning

Hao-Lun Hsu, Weixin Wang, Miroslav Pajic, Pan Xu

Duke University

{hao-lun.hsu,weixin.wang,miroslav.pajic,pan.xu}@duke.edu

Equal contribution.

###### Abstract

We present the first study on provably efficient randomized exploration in cooperative multi-agent reinforcement learning (MARL). We propose a unified algorithm framework for randomized exploration in parallel Markov Decision Processes (MDPs), and two Thompson Sampling (TS)-type algorithms, CoopTS-PHE and CoopTS-LMC, incorporating the perturbed-history exploration (PHE) strategy and the Langevin Monte Carlo exploration (LMC) strategy respectively, which are flexible in design and easy to implement in practice. For a special class of parallel MDPs where the transition is (approximately) linear, we theoretically prove that both CoopTS-PHE and CoopTS-LMC achieve a \(}(d^{3/2}H^{2})\) regret bound with communication complexity \(}(dHM^{2})\), where \(d\) is the feature dimension, \(H\) is the horizon length, \(M\) is the number of agents, and \(K\) is the number of episodes. This is the first theoretical result for randomized exploration in cooperative MARL. We evaluate our proposed method on multiple parallel RL environments, including a deep exploration problem (_i.e.,_\(N\)-chain), a video game, and a real-world problem in energy systems. Our experimental results support that our framework can achieve better performance, even under conditions of misspecified transition models. Additionally, we establish a connection between our unified framework and the practical application of federated learning.

## 1 Introduction

Multi-Agent Reinforcement Learning (MARL) has emerged as a potent tool with wide-ranging applications in diverse fields including robotics , gaming , and numerous real-world systems . This is particularly evident in cooperative scenarios, where MARL's effectiveness is enhanced through both direct and indirect communication channels among agents. This requires MARL algorithms to adeptly and flexibly coordinate communications to optimize the benefits of cooperation. One of the classical challenges in MARL is balancing exploration and exploitation so that agents effectively utilize existing information while acquiring new knowledge. Recent literature highlights the intricacies of this balance, focusing on cooperative exploration strategies  and dynamic exploitation tactics . Achieving this equilibrium is crucial for the practical deployment of MARL systems in real-world scenarios, where unpredictability and the need for rapid adaptation are prevalent .

Optimism in the Face of Uncertainty (OFU) is a popular strategy to address the exploration-exploitation problem . OFU strategy leads to numerous upper confidence bound (UCB)-type algorithms in contextual bandits , single-agent reinforcement learning , and more recently multi-agent reinforcement learning . These algorithms compute statistical confidence regions for the model or the value function, given the observed history, and perform the greedypolicy with respect to these regions, or upper confidence bounds. Though UCB-based methods give out strong theoretical results, they often have poor performance in practice . For example, Wang et al.  demonstrates that computing the confidence bonus necessitates advanced sensitivity sampling and the expensive computation makes the practical applications inefficient. It is worth noting that UCB is mostly constructed based on a linear structure . NeuralUCB is a notable attempt at a nonlinear version while it is infeasible in terms of computational complexity .

Inspired by Thompson Sampling (TS) , posterior sampling for reinforcement learning (RL)  involves maintaining a posterior distribution over the parameters of the Markov Decision Processes (MDP) model parameters. Although conceptually simple, most existing TS methods require the exact posterior or a good Laplacian approximation . Recently, there have been advancements in randomized exploration with approximate sampling. One important method is perturb-history exploration (PHE) strategy, which involves introducing random perturbations in the action history of the agent . This randomized exploration approach diversifies the agent's experience, aiding in learning more robust strategies in environments with uncertainty and variability. Another effective method is Langevin Monte Carlo (LMC) method . Notably, Ishfaq et al.  maintains the simplicity and scalability of LMC, making it applicable in deep RL algorithms by approximating the posterior distribution of the \(Q\) function.

Despite the aforementioned advancements of randomized exploration in bandits and single-agent RL, there remains a scarcity of research on randomized exploration within cooperative MARL, which motivates us to present the first investigation into provably efficient randomized exploration in cooperative MARL, with both theoretical and empirical evidence. We specifically focus on the applicability in parallel MDPs, aiming to facilitate faster learning and to improve policy optimization with the same state and action spaces, allowing for leveraging similarities across MDPs. We theoretically and empirically demonstrate that randomized exploration strategies can be extended to the multi-agent setting and the benefit of randomized exploration instead of UCB can be significant from single-agent to multi-agent setting.

In summary, **our contributions** are as follows:

* We propose a unified algorithm framework for learning parallel MDPs, and apply two TS-related strategies PHE and LMC for exploration, which leads to the CoopTS-PHE and CoopTS-LMC algorithms. Unlike conventional TS, which suffers from sampling errors due to Laplace approximation and expensive posterior computation , our proposed algorithms only require adding standard Gaussian noises to the dataset (CoopTS-PHE) or the gradient (CoopTS-LMC) when performing Least-Square Value Iteration, which is efficient in computation and avoids sampling bias due to the Laplace approximation. Notably, both algorithms are easily implementable which are more practical than UCB-based algorithms in deep MARL.
* When reduced to linear parallel MDPs, we theoretically prove that both CoopTS-PHE and CoopTS-LMC with linear function approximation can achieve a regret bound \(}d^{3/2}H^{2}+ \) with communication complexity \(}(d+K/)MH\), where \(d\) is the feature dimension, \(H\) is the horizon length, \(M\) is the number of agents, \(K\) is the number of episodes for each agent, and \(\) is a parameter controlling the communication frequency. When \(=(K/dM)\), our algorithms attain \(}d^{3/2}H^{2}\) regret with \(}(dHM^{2})\) communication complexity. This result matches the best communication complexity in cooperative MARL , and the best regret bounds for randomized RL in the single-agent setting (\(M=1\)) . A comprehensive comparison with baseline algorithms on episodic, non-sationary, linear MDPs is presented in Table 1.
* We further extend our theoretical analysis to the misspecified setting where both the transition and reward are approximately linear up to an error \(\) and the MDPs could be heterogeneous across agents, which is a generalized notion of misspecification . We theoretically prove when \(=\), the cumulative regret for CoopTS-PHE matches the result in the linear homogeneous MDP setting. Simultaneously, when \(=\), the cumulative regret for CoopTS-LMC matches the result in the linear homogeneous MDP setting. This result indicates that CoopTS-PHE has a slightly higher tolerance on the model misspecification than CoopTS-LMC.
* We conduct extensive experiments on various benchmarks with comprehensive ablation studies, including \(N\)-chain that requires deep exploration, Super Mario Bros task in a misspecified setting, and a real-world problem in thermal control of building energy systems. Our empirical evaluation demonstrates that our randomized exploration strategies outperform existing deep \(Q\)-network(DQN)-based baselines. We also show that these strategies in cooperative MARL can be adapted to the existing federated RL framework when data transitions are not shared.

## 2 Preliminary

In parallel Markov Decision Processes (MDPs), \(M\) agents interact independently with their respective discrete-time MDPs, sharing the same but independent state and action spaces. Each agent might have its unique reward functions and transition kernels. Specifically, for agent \(m\), the associated MDP is defined by the tuple \((,,H,_{m},r_{m})\). Here \(\) and \(\) are the state and action spaces respectively, \(H\) is the horizon length, \(_{m}=\{_{m,h}\}_{h[H]}\) and \(r_{m}=\{r_{m,h}\}_{h[H]}\) are the sets of transition kernels and reward functions. For step \(h[H]\), \(_{m,h}(|s,a)\) is the probability measure over the next state given current state-action pair \((s,a)\), \(r_{m,h}:\) is the deterministic reward function. The policy \(_{m}=\{_{m,h}\}_{h[H]}\) is a sequences of decision rules where \(_{m,h}:\) is the deterministic policy at step \(h\).

For agent \(m\), given any policy \(\) and transition \(\), to evaluate the policy effectiveness in the \(m^{}\) MDP, we define value function \(V^{}_{m,h}(s)_{}[_{h^{}=h}^{H}r_{m,h^{ }}(s_{m,h^{}},a_{m,h^{}})|s_{m,h}=s]\) and \(Q\) function \(Q^{}_{m,h}(s,a)_{}[_{h^{}=h}^{H}r_{m,h^{ }}(s_{m,h^{}},a_{m,h^{}})|s_{m,h}=s,a_{m,h}=a]\) for any \((h,s,a)[H]\). The optimal policy is defined as \(^{*}_{m}\), and we denote \(V^{*}_{m,h}(s)=V^{^{*}_{m}}_{m,h}(s)\). For each \(k[K]\), at the beginning of episode \(k\), each agent \(m\) receives the initial state \(s^{k}_{m,1}\) chosen arbitrarily by the environment. For each step \(h[H]\) in this episode, each agent \(m\) observes its current state \(s^{k}_{m,h}\), selects an action \(a^{k}_{m,h}\) based on policy \(^{k}_{m,h}\), receives a reward \(r_{m,h}(s^{k}_{m,h},a^{k}_{m,h})\), and then transitions to the next state \(s^{k}_{m,h+1}\) based on the transition probability measure \(_{m,h}(|s^{k}_{m,h},a^{k}_{m,h})\). The reward defaults to \(0\) when the episode terminates at step \(H+1\). The goal of agents is to minimize the cumulative group regret after \(K\) episodes, which is defined as

\[(K)=_{m}_{k=1}^{K}V^{*}_{m,1} s^{k}_{m,1}-V^{^{k}_{m}}_{m,1}s^{k}_{m,1}.\]

## 3 Algorithm Design

In this section, we first present a unified algorithm framework for conducting randomized exploration in cooperative MARL. Then we introduce two practical randomized exploration strategies.

### Unified Algorithm Framework

A unified algorithm framework is presented in Algorithm 1, where each agent executes Least-Square Value Iteration (LSVI) in parallel and makes decisions based on collective data obtained from communication between each agent and the server. Before we describe the details of our algorithm, we first define notations about the datasets stored on each agent's local machine and the server.

   Setting & Algorithm & Regret & Average Regret & Randomized Exploration & Generalizable to Deep RL & Communication Complexity \\   & OPT-RLSVI  & \(}(d^{2}H^{})\) & \(}(d^{2}H^{})\) & \(}\) & ✗ & – \\  & LSVI-UCB  & \(}(d^{}H^{2})\) & \(}(d^{}H)\) & ✗ & ✗ & – \\  & LSVI-PHE  & \(}(d^{}H^{2})\) & \(}(d^{}H)\) & \(}\) & \(}\) & – \\  & LMC-LSVI  & \(}(d^{}H^{2})\) & \(}(d^{}H)\) & \(}\) & \(}\) & – \\  & LSVI-ASE  & \(}(dH^{2})\) & \(}(dH)\) & \(}\) & \(}\) & – \\   & Coop-LSVI  & \(}(d^{}H^{2})\) & \(}(d^{}H)\) & ✗ & ✗ & \(dHM^{3}\) \\  & Asyn-LSVI  & \(}(d^{}H^{2})\) & \(}(d^{}H)\) & ✗ & ✗ & \(dHM^{2}\) \\  & **CoopTS-PHE (Ours)** & \(}(d^{}H^{2})\) & \(}(d^{}H)\) & \(}\) & \(}\) & \(dHM^{2}\) \\  & **CoopTS-LMC (Ours)** & \(}(d^{}H^{2})\) & \(}(d^{}H)\) & \(}\) & \(}\) & \(dHM^{2}\) \\   

Table 1: Comparison on episodic, non-stationary, linear MDPs. We define the average regret as the cumulative regret divided by the total number of samples (transition pairs) used by the algorithm. Here \(d\) is the feature dimension, \(H\) is the episode length, \(K\) is the number of episodes, and \(M\) is the number of agents in a multi-agent setting.

```
1:Initialization: set \(U_{h}^{}(k),U_{m,h}^{}(k)=\).
2:for episode \(k=1,...,K\)do
3:for agent \(m\)do
4: Receive initial state \(s_{m,1}^{k}\).
5:\(V_{m,H+1}^{k}() 0\).
6:\(\{Q_{n,h}^{k}(,)\}_{h=1}^{H}\)Randomized ExplorationAlgorithm 2 or Algorithm 3
7:for step \(h=1,...,H\)do
8:\(a_{m,h}^{k}_{a}Q_{m,h}^{k}(s_{m,h}^{k},a)\).
9: Receive \(s_{m,h+1}^{k}\) and \(r_{m,h}\).
10:\(U_{m,h}^{}(k) U_{m,h}^{}(k)s_{m,h }^{k},a_{m,h}^{k},s_{m,h+1}^{k}\).
11:ifCondition then
12:SYNCHRONIZE \(\) True.
13:endif
14:endfor
15:endfor
16:ifSYNCHRONIZE then
17:for step \(h=H,...,1\)do
18:\(\)AGENT: Send \(U_{m,h}^{}(k)\) to SERVER.
19: SERVER: \(U_{h}^{}(k)_{m}U_{m,h}^{}(k)\).
20: SERVER: \(U_{h}^{}(k) U_{h}^{}(k) U_{h}^{ }(k)\).
21: SERVER: Send \(U_{h}^{}(k)\) to each AGENT.
22:\(\)AGENT: Set \(U_{m,h}^{}(k)\).
23:endfor
24:endif
25:endfor
```

**Algorithm 1** Unified Algorithm Framework for Randomized Exploration in Parallel MDPs

Index notationWe define \(k_{s}(k)\) (denoted as \(k_{s}\) when no ambiguity arises) as the last episode before episode \(k\) where synchronization happens. For episode \(k\) and step \(h\), we define three datasets:

\[U_{h}^{}(k) =s_{n,h}^{},a_{n,h}^{},s_{n,h+1}^{} }_{n,[k_{s}]}, \] \[U_{m,h}^{}(k) =s_{m,h}^{},a_{m,h}^{},s_{m,h+1}^{} }_{=k_{s}+1}^{k-1},\] (3.1b) \[U_{m,h}(k) =U_{h}^{}(k) U_{m,h}^{}(k). \]

By definition, \(U_{h}^{}(k)\) is the dataset that is shared across all agents due to the latest synchronization at episode \(k_{s}\). \(U_{m,h}^{}(k)\) is the unique data collected by agent \(m\) since episode \(k_{s}\). Then \(U_{m,h}(k)\) is the total dataset available for agent \(m\) at the current time. Let \((k)=|U_{m,h}(k)|\) be the total number of data points. For the simplicity of notation, we also re-order the data points in \(U_{m,h}(k)\), and rename the tuple \((s_{m,h}^{},a_{m,h}^{},s_{m,h+1}^{})\) as \((s^{l},a^{l},s^{ l})\) such that we have \(U_{m,h}(k)=_{l=1}^{(k)}(s^{l},a^{l},s^{ l})\). In fact, this can be done by the following one-to-one mapping

\[l_{m,k}(n,)=(-1)M+n& k_{s},\\ (M-1)k_{s}+&k_{s}< k-1. \]

Therefore, we use indices \((s,a,s^{}) U_{m,h}(k)\) and \(l[(k)]\) interchangeably for the summation over set \(U_{m,h}(k)\).

Algorithm interpretationAt a high level, each episode \(k\) in Algorithm 1 consists of two stages. The first stage (Lines 3-15) is parallelly executed by all agents and the second stage (Lines 16-24) involves the communication among agents and the server.

In the first stage (Lines 3-15) of Algorithm 1, each agent \(m\) operates in two parts. The first part (Line 6) updates estimated \(Q\) functions \(\{Q_{m,h}^{k}\}_{h=1}^{H}\) through LSVI with a randomized exploration strategy (Algorithm 2 or Algorithm 3, which will be introduced in Section 3.2). In particular, given the estimated value functions \(V_{m,h+1}^{k}()=_{a A}Q_{m,h}^{k}(,a)\) at step \(h+1\), we perform one step robust backward Bellman update to obtain \(V^{k}_{m,h}()\) at step \(h\). And we initialize \(V^{k}_{m,H+1}()\) to be \(0\) (Line 5). In the second part (Lines 7-14), after obtaining the estimated \(Q\) functions, in each step \(h\) we execute the greedy policy with respect to \(Q^{k}_{m,h}\) and collect new data points which are added to the local dataset \(U^{}_{m,h}(k)\) (Lines 8-10). Then we verify the synchronization condition (Lines 11-13). In this paper, we mainly use three types of synchronization rules. (1) We can synchronize every \(c\) episode where \(c\) is a user-defined constant, which is easy to implement in practice. (2) We can also synchronize at the episode of \(b^{1},b^{2},...,b^{n}\), with \(b\) representing the base of the exponential function. This is guided by the intuition that agents require more transitions urgently at the early learning stages. (3) Additionally, if we have a feature mapping \((s,a):^{d}\), based on (3.1), we define the following empirical covariance matrices.

\[\,^{k}_{h}&= _{(s^{l},a^{l},s^{}) U^{}_{m}(k)}s ^{l},a^{l}s^{l},a^{l}^{},\\ \,^{k}_{m,h}&=_{(s^{l},a^ {l},s^{}) U^{}_{m,h}(k)}s^{l},a^{l} s^{l},a^{l}^{},\\ ^{k}_{m,h}&=\,^{k} _{h}+\,^{k}_{m,h}+.\]

We synchronize as long as the following condition is met:

\[\,^{k}_{h}+\, ^{k}_{m,h}+)}{(\,^{k}_ {h}+)})}, \]

where \(\) is a communication control factor. In our experiments, we try all three rules and compare their performance, which is discussed in detail in Appendix K.1.

The second stage (Lines 16-24) is executed only when the synchronization condition is satisfied. First, all the agents upload their local transition set \(U^{}_{m,h}(k)\), i.e., the newly collected local data after the last synchronization, to the server. Then, the server gathers all information together in \(U^{}_{h}(k)\) and sends it back to each agent. Finally, each agent resets the local transition set \(U^{}_{m,h}(k) 0\). Now agent \(m\) can access the dataset \(U_{m,h}(k)=\,U^{}_{h}(k) U^{}_{m,h}(k)\), which contains the historical data of all agents up to last synchronization and its local dataset.

### Randomized Exploration Strategies

When we update the model parameter and estimate \(Q\) functions in Algorithm 1 (Line 6), we use exploration strategies to avoid suboptimal policies. Previous work adopted Upper Confidence Bound (UCB) exploration in the linear function class [24; 56] to estimate the \(Q\) function \(Q^{k}_{m,h}^{H}_{h=1}\). Although UCB-based methods come with strong theoretical guarantees, they often perform poorly in practice [16; 61; 60]. Moreover, UCB requires precise computation of the confidence set, which is usually hard to be implemented beyond the linear structure. In contrast, randomized exploration strategies offer more robust performance, flexibility in design, ease of implementation, and do not require a linear structure.

We approximate the \(Q\) functions with the following function class \(=\{f_{}:|f_{}(s,a)=f(;(s,a))\}\), where \(^{d}\) is the parameter and \(^{d}\) is a feature mapping associated with state-action pairs. Now we define the loss function for estimating the \(Q\) functions.

\[L^{k}_{m,h}()=_{l=1}^{(k)}Lr^{l}_{h}+V^{k}_{m, h+1}({s^{}}^{l}),f;^{l}+\| \|^{2}, \]

where \(r^{l}_{h}=r_{h}s^{l},a^{l}\), \(^{l}=s^{l},a^{l}\), and \(L\) is a user-specified loss function.

Perturbed-History ExplorationThe first strategy we use in Algorithm 1 is called the perturbed-history exploration [45; 47; 32], displayed in Algorithm 2. We refer to the resulting algorithm as CoopTS-PHE. In particular, we optimize the following randomized loss function, where we add random Gaussian noises to the rewards and regularizer in (3.4).

\[^{k,n}_{m,h}()=_{l=1}^{(k)}L(r^{l }_{h}+^{k,l,n}_{h})+V^{k}_{m,h+1}({s^{}}^{l}),f; ^{l}+\|+^{k,n}_{h}\|^{2}, \]

where \(^{k,l,n}_{h}}{}(0,^{2})\), \(^{k,n}_{h}(,^{2})\), and \(n[N]\). Then we obtain the following perturbed estimated parameter

\[}^{k,n}_{m,h}=*{argmin}_{ ^{d}}^{k,n}_{m,h}(). \]Note that we repeat the above steps for \(n=1,,N\) to obtain independent copies of parameters, which is referred to as the multi-sampling process . Then we obtain the estimated \(Q\) function \(Q_{m,h}^{k}\) based on Line 7 in Algorithm 2. Finally, by maximizing \(Q_{m,h}^{k}\) over action space \(\), we obtain the estimated value function \(V_{m,h}^{k}\).

```
1:Input: multi-sampling number \(N^{+}\), function class \(=\{f_{}:| f_{}(s,a)=f(;(s,a))\}\).
2:for step \(h=H,...,1\)do
3:for\(n=1,...,N\)do
4: Sample \(\{_{h}^{k,l,n}\}_{l[(k)]}}{} (0,^{2})\) and \(_{h}^{k,n}(,^{2})\) independently.
5: Solve \(}_{m,h}^{k,n}\) according to (3.6).
6:endfor
7:\(Q_{m,h}^{k}_{n[N]}f} _{m,h}^{k,n};,H-h+1}^{+}\).
8:\(V_{m,h}^{k}()_{a}Q_{m,h}^{k}(,a)\).
9:endfor
10:Output: \(\{Q_{m,h}^{k}(,),V_{m,h}^{k}(,)\}_{h=1}^{H}\).
```

**Algorithm 2** Perturbed-History Exploration

Langevin Monte Carlo ExplorationNext we introduce the Langevin Monte Carlo exploration strategy  in Algorithm 3, which stems from the Langevin dynamics . Combining it with Algorithm 1 leads to our second proposed algorithm, CoopTS-LMC. Specifically, we update the model parameter iteratively. For iterate \(j=1,,J_{k}\), the update is given by

\[_{m,h}^{k,j,n}=_{m,h}^{k,j-1,n}-_{m,k} L_{m,h}^{k }_{m,h}^{k,j-1,n}+_{m,k}^{-1}} _{m,h}^{k,j,n}, \]

where \(L_{m,h}^{k}\) is defined in (3.4), \(_{m,h}^{k,j,n}^{d}\) is a standard Gaussian noise, \(_{m,k}\) is the learning rate, and \(_{m,k}\) is the inverse temperature parameter. We similarly use the multi-sampling trick to obtain \(N\) independent estimators and estimate \(Q\) function \(Q_{m,h}^{k}\) by truncation based on Line 10 in Algorithm 3.

```
1:Input: multi-sampling number \(N^{+}\), function class \(=\{f_{}: |f_{}(s,a)=f(;(s,a))\}\), step sizes \(\{_{m,k}\}_{m,k[K]}\), inverse temperature parameters \(\{_{m,k}\}_{m,k[K]}\).
2:for step \(h=H,...,1\)do
3:for\(n=1,...,N\)do
4:\(_{m,h}^{k,0,n}=_{m,h}^{k-1,J_{k-1},n}\).
5:for\(j=1,...,J_{k}\)do
6: Sample \(_{m,h}^{k,j,n}}{}(,)\).
7: Update \(_{m,h}^{k,j,n}\) by (3.7).
8:endfor
9:endfor
10:\(Q_{m,h}^{k}_{n[N]}f_{m,h}^{k,J_ {k},n};,H-h+1}^{+}\).
11:\(V_{m,h}^{k}()_{a A}Q_{m,h}^{k}(,a)\).
12:endfor
13:Output: \(\{Q_{m,h}^{k}(,),V_{m,h}^{k}(,)\}_{h=1}^{H}\).
```

**Algorithm 3** Langevin Monte Carlo Exploration

## 4 Theoretical Analysis

### Homogeneous Parallel Linear MDPs

We provide theoretical analyses of our algorithms in the linear structure under the assumption of linear function approximation and linear MDP setting. We first present the definition of linear MDPs.

**Definition 4.1** (Linear MDP ).: An MDP(\(,,H,,r\)) is a linear MDP with feature map \(:^{d}\), if for any \(h[H]\), there exist \(d\) unknown measures \(_{h}=(_{h}^{1},...,_{h}^{d})\) over \(\) and an unknown vector \(_{h}^{d}\) such that for any \((s,a)\),

\[_{h}(|s,a)=(s,a),_{h}() , r_{h}(s,a)=(s,a),_{h} .\]

Without loss of generality, we assume that for all \((s,a)\), \(\|(s,a)\| 1\) and \(\{\|_{h}()\|,\|_{h}\|\}\).

Throughout the analyses in this section, we assume the homogeneous parallel MDPs setting where all agents share the same linear MDP defined in Definition 4.1. We also provide the results when the MDPs across agents are approximately linear and heterogeneous, which is deferred to Section 4.2 due to the space limit. Under the linear MDP assumption, it is known that the \(Q\)-function admits a linear form [36, Proposition 2.3]. Consequently, we choose the loss function \(L\) in (3.4) to be the \(l_{2}\) loss and approximate the \(Q\) function in the linear function class \(f(;^{l})=^{}^{l}\).

Now we first present the regret bound for CoopTS-PHE.

**Theorem 4.2**.: Under Definition 4.1, choose \(L\) to be \(l_{2}\) loss and linear function class \(f(;^{l})=^{}^{l}\) in (3.4). In CoopTS-PHE (Algorithm 1+Algorithm 2), let \(N=()/(c_{0})\) where \(=}(d)\) and \(c_{0}=(1)\), \(()\) is the cumulative distribution function (CDF) of the standard normal distribution. Let \(=1\) and \(0<<1\). Under the determinant synchronization condition (3.3), we obtain the following cumulative regret

\[(K)=}d^{}H^{2} +,\]

with probability at least \(1-\).

**Remark 4.3**.: When we choose \(=(K/dM)\) in the synchronization condition (3.3), the cumulative regret of CoopTS-PHE becomes \(}(d^{3/2}H^{2})\), which matches the result of UCB exploration . When \(M=1\), the regret becomes \(}(d^{3/2}H^{2})\), which matches the existing best randomized single-agent result . Note that if there is no communication at all and agents act independently, with the same number of learning rounds (or samples), the cumulative regret becomes \(}(M d^{3/2}H^{2})\). By incorporating communication, our regret bound in Theorem 4.2 is lower than that of the independent setting by a factor \(\). A similar strategy called rare-switching update with a determinant synchronization condition has also been adopted in parallel bandit problems .

Similarly, we have the following result for CoopTS-LMC.

**Theorem 4.4**.: Under Definition 4.1, choose \(L\) to be \(l_{2}\) loss and linear function class \(f(;^{l})=^{}^{l}\) in (3.4). In CoopTS-LMC (Algorithm 1+Algorithm 3), let \(N=()/(c^{}_{0})\) where \(c^{}_{0}=1-1/2\) and \(=}(d)\). Let \(1/}=}H\) for all \(m\), \(=1\), and \(0<<1\). For any episode \(k[K]\) and agent \(m\), let the learning rate \(_{m,k}=1/4_{}_{m,h}^{k}\), the update number \(J_{k}=2_{k}(4HKMd)\) where \(_{k}=_{}_{m,h}^{k}/_{} _{m,h}^{k}\) is the condition number of \(_{m,h}^{k}\). Under the determinant synchronization condition (3.3), we have

\[(K)=}d^{}H^{2} +,\]

with probability at least \(1-\).

**Remark 4.5**.: Note that CoopTS-PHE and CoopTS-LMC have the same order of regret. Hence the discussion in Remark 4.3 also applies to CoopTS-LMC. We would also like to highlight that our results are the first rigorous regret bounds for randomized MARL algorithms.

From the perspective of technical novelty, our analysis of randomized MARL algorithms is different from that of UCB-based algorithms  because the model prediction error here contains randomness, causing a more complex probability analysis and an additional approximation error. We would also like to point out that in proofs for both CoopTS-LMC and CoopTS-PHE  we use a new \(\)-covering technique to prove that the optimism lemma holds for all \((s,a)\) instead of just the state-action pairs encountered by the algorithm, which is essential for the regret analysis. This was ignored by previous works  and its follow-up works  that use the same regret decomposition technique. Furthermore, the multi-agent setting and the communications from synchronization in our algorithms also significantly increase the challenges in our analysis compared to randomized exploration in the single-agent setting .

Next we present the communication complexity of Algorithm 1 with synchronization condition (3.3).

**Lemma 4.6**.: The total number of communication rounds between the agents and the server in Algorithm 1 is bounded by \(=}((d+K/)MH)\). Moreover, the total number of transferred random bits only has a logarithmic dependence on the number of episodes \(K\).

**Remark 4.7**.: We provide a refined analysis in Appendix C to get this improved result based on that of , which studied the same communication procedure as ours. When we choose \(=(K/dM)\), the communication complexity reduces to \(}(dHM^{2})\), which only has a logarithmic dependence on the number of episodes \(K\). Additionally, we provide a rigorous analysis to show that the algorithm only needs to communicate logarithm number of random bits throughout the learning process.

Note that Min et al.  studied the asynchronous setting where only one agent is active in each episode, giving out the regret \(}(d^{3/2}H^{2})\) with the communication complexity \(}(dHM^{2})\). It is interesting to see that our algorithm, though in the synchronous setting, has the same communication complexity as the asynchronous variant. This implies that the asynchronous algorithm can only circumvent current communication by delaying it to the future but does not decrease the communication complexity. In fact, the synchronous setting can learn the policy better in our work, which is indicated by comparison of the average regret (the cumulative regret divided by the total number of samples used by the algorithm) in Table 1. By achieving a matched communication complexity, we find that synchronous and asynchronous settings have their own advantages and cannot replace each other. This phenomenon can help us better understand the properties of these two communication schemes.

### Misspecified Setting

In this part, we extend our theoretical analysis to the misspecified setting. In this setting, the transition functions \(_{m,h}\) and the reward functions \(r_{m,h}\) are heterogeneous across different MDPs, which is slightly more complicated than the homogeneous setting. Moreover, instead of assuming the transition and reward are linear, we only require each individual MDP is a \(\)-approximate linear MDP  where both the transition and reward are approximately linear up to an controlled error \(\).

**Definition 4.8** (Misspecified Parallel MDPs).: For any \(0< 1\), and for any agent \(m\), the corresponding \((,,H,_{m},r_{m})\) is a \(\)-approximate linear MDP with a feature map \(:^{d}\), for any \(h[H]\), there exist \(d\) unknown (signed) measures \(_{h}=_{h}^{(1)},,_{h}^{(d)}\) over \(\) and an unknown vector \(_{h}^{d}\) such that for any \((s,a)\), we have

\[_{m,h}( s,a)-(s, a),_{h}()_{},\] \[r_{m,h}(s,a)-(s,a),_{h} ,\]

where \(\|\|_{}\) is the total variation norm, for two distributions \(P_{1}\) and \(P_{2}\), we define it as: \(\|P_{1}-P_{2}\|_{}=_{}|P_{1} ()-P_{2}()|\). Without loss of generality, we assume that \(\|(s,a)\| 1\) for all \((s,a)\), and \(\|_{h}()\|,\|_{h}\|} \) for all \(h[H]\) and \(m\).

**Remark 4.9**.: Note that our misspecified setting defined in Definition 4.8 is a generalized notion of misspecification in . Moreover, our misspecified setting is also more general and cover the small heterogeneous setting mentioned in . The triangle inequality can easily be used to derive small heterogeneous setting from our misspecified setting, but not vice versa.

Next we state our regret bound for CoopTS-PHE in the misspecified setting.

**Theorem 4.10** (Misspecified Regret Bound for CoopTS-PHE).: In CoopTS-PHE (Algorithm 1+Algorithm 2), under Definition 4.8 and determinant synchronization condition (3.3), with the same initialization with Theorem 4.2, we obtain the following cumulative regret

\[(K)=}d^{}H^{2} ++dH^{2}M+ ,\]

with probability at least \(1-\).

**Remark 4.11**.: When we choose \(=\), the cumulative regret becomes \(}d^{}H^{2}+\). This matches the result of Theorem 4.2 in the linear MDP setting. Similarly, we can have the following result for CoopTS-LMC.

**Theorem 4.12** (Misspecified Regret Bound for CoopTS-LMC).: In CoopTS-LMC (Algorithm 1+Algorithm 3), under Definition 4.8 and determinant synchronization condition (3.3), with the same initialization with Theorem 4.4 except that \(1/}=}H+H \), we obtain the following cumulative regret

\[(K)=}d^{}H^{2} ++d^{}H^{2}M +,\]

with probability at least \(1-\).

**Remark 4.13**.: When \(=\), the cumulative regret becomes \(}d^{}H^{2}+\). This matches the result of Theorem 4.4 in the linear MDP setting. By comparing Theorems 4.10 and 4.12, we find the result of CoopTS-LMC has an extra \(\) factor worse than that of CoopTS-PHE, causing the chosen \(\) in CoopTS-PHE has an extra \(\) order over that in CoopTS-LMC. This indicates that CoopTS-PHE has better performance tolerance for the misspecified setting.

## 5 Experiments

In this section, we present an empirical evaluation of our proposed randomized exploration strategies (_i.e.,_ CoopTS-PHE and CoopTS-LMC) with deep \(Q\)-networks (DQNs)  as the core algorithm on varying tasks under multi-agent settings compared with several baselines: vanilla DQN, Double DQN , Bootstrapped DQN , and Noisy-Net ). Given that all experiments are conducted under multi-agent settings unless explicitly specified as a single-agent or centralized scenario, we denote CoopTS-PHE as "PHE" and CoopTS-LMC as "LMC" in both experimental contexts and figures. Note that we run all our experiments on Nvidia RTX A5000 with 24GB RAM. The implementation of this work can be found at [https://github.com/panxulab/MARL-CoopTS](https://github.com/panxulab/MARL-CoopTS)

### \(N\)-chain

The \(N\)-chain  comprises a sequence of \(N\) states denoted as \(\{s_{l}\}_{l=1}^{N}\). Assuming the existence of \(m\) agents, all initiating their trajectories from \(s_{2}\), this study explores the dynamics of their movement within the chain. At each time step, agents face the decision to move either left or right. Notably, each agent incurs a nominal reward of \(r=0.001\) upon reaching state \(s_{1}\), while a more substantial reward of \(r=1\) is obtained upon reaching the terminal state \(s_{N}\). The illustration of \(N\)-chain environment is shown in Appendix K.1. With a horizon length of \(N+9\), the optimal return is \(10\). We consider \(N=25\) with the communication among agents in Figure 1 following the synchronization approach in Algorithm 1. In Figure 1(a), we show that PHE and Bootstrapped DQN result in higher average episode return among all agents while LMC can also eventually converge to a similar reward.

Upon increasing the number of agents to \(m=3\), we show in Figure 1(b) that our randomized exploration methods outperform all other baselines. Notably, the fluctuation in PHE is observed to be less pronounced against LMC. This observation lends support to our theoretical framework regarding performance tolerance in the misspecified setting, as detailed in Section 4.2. The complete results for \(N\)-chain and ablation studies can be found in Appendix K.1.

### Super Mario Bros

Environmental heterogeneity, arising from various sources, is a prevalent challenge in practical scenarios. In Section 4.2, we illustrate the extension of homogeneous parallel MDP to the misspecified

Figure 1: Comparison among different exploration strategies in different environments. (a)-(b): \(N\)-chain with \(N=25\). (c)-(d): Super Mario Bros. All results are averaged over 10 runs and the shaded area represents the standard deviation.

setting. In the Super Mario Bros task , we examine a scenario where four agents, denoted as \(m=4\), engage in learning within distinct environments. Despite these environments sharing the same state space \(\), action space \(\), and reward function, their characteristics are different described in Appendix K.2. The primary objective of the Super Mario Bros task is to train an agent capable of advancing as far-right and rapidly as possible without collisions or falls. Utilizing preprocessed images as input states, agents aim to select optimal actions from a set of \(7\) discrete actions.

Figure 1(c) visually depicts that both randomized exploration strategies outperform other baselines in cooperative parallel learning. Notably, We observe that the superiority of LMC gets significant against PHE unlike the results in \(N\)-chain in Figures 1(a) and 1(b). In the case of PHE, Gaussian noise is introduced to the reward before applying the Bellman update, which can be viewed as a method empirically approximating the posterior distribution of the \(Q\) function using a Gaussian distribution. However, it is crucial to note that in practical scenarios, unlike the \(N\)-chain setting, Gaussian distributions may not always provide an accurate approximation of the true posterior of the \(Q\) function . Here, transitions are shared among the four agents whenever the synchronization condition in (3.3) is met. We also conducted extra experiments in this task extending our proposed method to federated learning shown in Figure 1(d) with details in Appendix K.2.

### Thermal Control of Building Energy Systems

Finally, we assess the efficacy of our randomized exploration strategies through their application to a practical task within a sustainable energy system: BuildingEnv, as outlined in . BuildingEnv is designed to manage the heating supply in a multi-zone building, which involves addressing real-world physical constraints and accounting for environmental shifts over time. The objective is to meet user-defined temperature specifications while simultaneously minimizing overall electricity consumption. We defer the environment details to Appendix K.3.

With the availability of different cities in varying weather types, we conduct experiments on multiple cities in parallel and share their data following Algorithm 1 for each exploration strategy. During the evaluation, we deploy those trained policies to the environment of each city/weather respectively. We include all methods as well as random action in Figure 2 for a fair comparison. Specifically, we sample action randomly from action space for random action. We display the distribution of the return with probability density in violin plots, indicating that our PHE and LMC can perform better with a higher mean. Additional results for other cities can be found in Appendix K.3.

## 6 Conclusion

We proposed a unified algorithm framework for provably efficient randomized exploration in parallel MDPs. By combining this unified algorithm framework with two TS-type randomized exploration strategies, PHE and LMC, we obtained two algorithms for parallel MDPs: CoopTS-PHE and CoopTS-LMC. These two algorithms are both flexible in design and easy to implement in practice. Under the linear MDP setting, we derived the theoretical regret bounds and communication complexities of CoopTS-PHE and CoopTS-LMC. This is the first result for randomized exploration in cooperative MARL, matching the best existing regret bounds for single-agent RL [32; 33]. We also extended our theoretical analysis to the misspecified setting. Our experiments on diverse RL parallel environments verified that randomized exploration improves the balance between exploration and exploitation in both homogeneous and heterogeneous settings. Future research directions includes extending our randomized exploration algorithm to fully decentralized or federated learning settings. Additionally, developing a more communication-efficient algorithm to reduce the substantial communication costs in the general function class setting is another potential direction.

Figure 2: Evaluation performance at Tampa (hot humid) in building energy systems. All results are averaged over 10 runs.