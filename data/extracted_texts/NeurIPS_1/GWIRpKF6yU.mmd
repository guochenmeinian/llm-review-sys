# Structured Neural-PI Control with End-to-End Stability and Output Tracking Guarantees

Wenqi Cui\({}^{1}\) Yan Jiang\({}^{1}\) Baosen Zhang\({}^{1}\) Yuanyuan Shi\({}^{2}\)

\({}^{1}\)University of Washington, WA 98195 \({}^{2}\)University of California San Diego, CA 92093

wenqiciu@uw.edu jiangyan@uw.edu zhangbao@uw.edu yyshi@eng.ucsd.edu

###### Abstract

We study the optimal control of multiple-input and multiple-output dynamical systems via the design of neural network-based controllers with stability and output tracking guarantees. While neural network-based nonlinear controllers have shown superior performance in various applications, their lack of provable guarantees has restricted their adoption in high-stake real-world applications. This paper bridges the gap between neural network-based controllers and the need for stabilization guarantees. Using equilibrium-independent passivity, a property present in a wide range of physical systems, we propose neural Proportional-Integral (PI) controllers that have provable guarantees of stability and zero steady-state output tracking error. The key structure is the strict monotonicity on proportional and integral terms, which is parameterized as gradients of strictly convex neural networks (SCNN). We construct SCNN with tunable softplus-\(\) activations, which yields universal approximation capability and is also useful in incorporating communication constraints. In addition, the SCNNs serve as Lyapunov functions, providing end-to-end stability and tracking guarantees. Experiments on traffic and power networks demonstrate that the proposed approach improves both transient and steady-state performances, while unstructured neural networks lead to unstable behaviors.

## 1 Introduction

Learning-based methods have the potential to solve difficult problems in control and have received significant attention from both the machine learning and control communities. A common problem in many applications is to design controllers that-with as little control effort as possible-stabilize a system at a prescribed output. A canonical example is the LQR problem and its variants, which finds optimal linear controllers for linear systems .

For nonlinear systems, the problem is considerably harder. Neither of the two control goals, system stabilization and output tracking at the steady state, is easy to achieve. Their combination, optimizing controller costs while guaranteeing stability and output tracking at the steady state, is even more challenging. For example, vehicles in a platoon should stay in formation (stability) while cruising at the desired speed (output tracking) . The optimization is then done over the set of all controllers that achieves both of these goals, e.g., reaching a platoon while consuming the least amount of fuel. Another pertinent application is the stability of electric grids with high amounts of renewables. Unlike systems with conventional generators, the power electronic interfaces of the inverters need to be actively controlled such that the grid synchronizes to the same frequency (output tracking), at the minimum operational costs. Currently, learning becomes a popular tool to parameterize nonlinear controllers as neural networks and train them to optimize performance, but provable guarantees on stability and steady-state error for these controllers are nontrivial to obtain .

Many real-world applications have multiple (possibly a continuous set of) equilibria that may be of interest. For example, a group of vehicles may need to cruise at different speeds. Moreover, theinternal states may not be directly accessible and sometimes there are communication limit. These constraints make it difficult to enforce stability by middle steps including learning a Lyapunov function and learning a model of the system (for details, see related work in Section 1.1). Therefore, we seek to achieve "end-to-end" guarantees: the neural network-based controller should guarantee stability and steady-state error _by constructions_, for a range of possible tracking points. There are some works that showed a class of monotone controllers can provide stability guarantees for varying equilibria [8; 9; 10], but they rely on tailor-made Lyapunov functions that are found in specific applications. Especially, these results are limited to single-input and single-output (SISO) control. In practice, however, lots of complex systems need controllers that are multiple-input and multiple-output (MIMO), sometimes with high input and output dimensions.

Moreover, current learning-based approaches typically focus on optimizing transient performance (the cost and speed at which a system reaches an equilibrium), often overlooking the steady-state behavior (cost at the equilibrium state) [11; 12; 13]. In classical control terminology, these controller is proportional. Steady-state requirements are difficult to incorporate since training can only occur over a finite horizon. To enforce output tracking at the steady-state, an integral term is commonly needed to drive the system outputs to the desired value at the steady state [14; 15; 16; 17]. This reasoning underlies the widespread use of linear Proportional-Integral (PI) controllers in practical applications [18; 19; 20]. However, linear parameterization inherently limits the controllers' degrees of freedom, potentially leading to suboptimal performance. This work addresses the following question: _Can we learn nonlinear controllers that guarantee transient stability and zero steady-state error for MIMO systems?_

**Contributions.** Clearly, it is not possible to design a controller for all nonlinear systems and the answer to the question above depends on the class of systems under consideration. This paper focuses on systems that satisfy the equilibrium independent passivity (EIP) [21; 22], which is present in many critical societal-scale systems including transportation , power systems , and communication network . This abstraction allows us to design generalized controllers without considering the detailed physical system dynamics. We propose a structured Neural-PI controller that has provable guarantees of stability and zero steady-state output tracking error. The key structure we use is strictly monotone functions with vector-valued inputs and outputs. Experiments on traffic and power systems demonstrate that Neural-PI control can reduce the transient cost by at least 30% compared to the best linear controllers and maintain stability guarantees. Unstructured neural networks, on the other hand, lead to unstable behaviors. We summarize our major contributions as follows.

1. We propose a generalized PI structure for neural network-based controllers in MIMO systems, with proportional and integral terms being strictly monotone functions constructed through the gradient of strictly convex neural networks (SCNN). We construct SCNN with a tunable softplus-\(\) activation function and prove their universal approximation capability in Theorem 1.
2. For a multi-agent system with an underlying communication graph, we show how to restrict the controllers to respect the communication constraints through the composition of SCNNs.
3. Using EIP and the monotone functions structured by SCNNs, we design a generalized Lyapunov function that works for a range of equilibria. This provides end-to-end guarantees on asymptotic stability and zero steady-state output tracking error proved in Theorem 2. The structured neural networks can be trained for transient optimization without jeopardizing the guarantees, establishing a framework for coordinating transient optimization and steady-state requirement.

### Related works

**Learning-based control with stability guarantees.** Recently, there has been a large interest in enforcing stability in learning-based controller [26; 27; 28; 29; 30]. Many works add soft penalties on the violation of stability conditions in the cost function, but it is nontrivial to certify stability for all the possible initial states in a compact set . Some recent works learn a Lyapunov function and use it to certify stability through a satisfiability modulo theories (SMT) solver [26; 27; 28; 29]. But this approach is difficult to scale to high-dimensional systems and the learned Lyapunov function only works for a single equilibrium. For control-affine systems, the work in  designs feedback linearizing policy with integral control to guarantee stabilizing to a range of equilibria. But it requires that all the state are accessible and many systems are not control affine. Our proposed controller guarantees stability for a set of equilibria _by construction_, and only needs access to the outputs (not the full states).

**Optimizing long-term behavior.** To regulate long-term behavior, existing works train neural networks using a cost function defined over a long time horizon [32; 33]. However, it is difficult to quantify how long the trajectory is enough to reach the steady state, thus enforcing steady-state tracking performance by adding a long-term cost is challenging. Our proposed controller enforces steady-state tracking _by construction_, instead of relying on training.

**Algorithm to tune MIMO PI controller.** Classical Proportional Integral (PI) control structures are widely used in industrial applications, while tuning PI controller parameters in MIMO systems is tedious and relies on heuristic rules . Learning-based methods become popular to tune PI parameters, although still restricted to linear control gains . Our contribution is the more generalized PI control and the MIMO neural network for parameterization.

**Monotone neural network.** Even though scalar-valued monotone functions have been studied in the learning community , their generalization to the vector-valued case has not. Numerous papers studied vector-valued functions that are monotone in every input, that is, \(()(^{})\) if \(^{}\). This is only a small subclass of vector-valued monotone functions that defined as functions satisfying \((()-(^{}))^{}(-^{}) 0\). In this paper, we construct the general class of monotone functions (often called monotone operators)  that satisfy the inner product inequality.

## 2 Background and Preliminaries

We consider a dynamic system described by:

\[}=(,),=(), \]

with state \((t)^{n}\), control action \((t)^{m}\), and output \((t)^{v}\). We also sometimes omit the time index \(t\) for simplicity1. In many practical applications, not all of the internal states are directly accessible. Hence, we consider control actions that follow output-feedback control laws of the form \(=()\), which is a static function of the output \(\) and not the state \(\).

A state \(^{*}\) such that \((^{*}\),\(^{*})=_{n},^{*}=(^{*})\), \(^{*}=(^{*})\) is called an equilibrium since the system stops changing at this state. Throughout this paper, the superscript \(*\) indicates the equilibrium values of variables.

**Definition 1** (Local asymptotic stability , Definition 4.1).: _The system (1) is asymptotically stable around an equilibrium \(^{*}\) if, \(>0\), \(>0\) such that \(\|(0)-^{*}\|<\) ensures \(\|(t)-^{*}\|<\), \( t 0\), and \(^{}>0\) such that \(\|(0)-^{*}\|<^{}\) ensures \(_{t}\|(t)-^{*}\|=0\)._

One of the main tools to prove asymptotic stability is the Lyapunov's direct method :

**Lemma 1** (Lyapunov functions and asymptotic stability , Theorem 4.1).: _Consider the system (1) with an equilibrium \(^{*}^{n}\). Suppose there exists a continuously differentiable function \(V:\) that satisfies the following conditions: 1) \(V(^{*})=0,V()>0,^{*}\); 2) \(()=(_{}V())^{}} 0, \) with the equality holds when \(=^{*}\). Then the system is asymptotically stable around \(^{*}\)._

The key challenge to using Lyapunov theory is in constructing such a function and verifying the satisfaction of the conditions in Lemma 1. An important part of this paper is to show how to systematically use neural networks to construct Lyapunov functions for a class of nonlinear systems.

Besides stability, we are often interested in achieving a specific equilibrium such that the output converges to a prescribed setpoint.

**Definition 2** (Output tracking to \(}\)).: _The dynamical system (1) is said to track a setpoint \(}\) if \(_{t}(t)=}\)._

For output tracking, the dimension of the input \(\) should be at least the dimension of the output \(\), otherwise there is not enough degrees of freedom in the input to track all the outputs. If the dimensions of the input are strictly larger than the output, it is always possible to define "dummy" outputs (e.g., by appending zeros) to match the input dimension. Therefore, it is common to assume the same input and output dimensions . In the remainder of this paper, we make the following assumption.

**Assumption 1** (Identical input and output dimension).: _The input \(\) and the output \(\) have the same dimension, namely, \((t)\) and \((t)\) are both vectors in \(^{m}\)._We will show that strictly monotone functions play an important role in guaranteeing stability and output tracking in a range of systems. Using neural networks for constructing monotone functions from \(\) to \(\) has been well-studied in [36; 37]. However, since we consider MIMO systems in this paper, we use a generalized notion of monotonicity for vector-valued functions as defined in .

**Definition 3** (Strictly monotone function ).: _A continuous function \(:^{m}^{m}\) is strictly monotone on \(^{m}\) if \((()-())^{}(-) 0,\, ,\), with the equality holds if and only if \(=\)._

In this paper, we will show a way to construct (strictly) monotone functions using the gradient of (strictly) convex neural networks.

## 3 Problem Statement

### Transient and steady-state requirements

We aim to optimize the controller in \(\) to improve the transient response after disturbances while guaranteeing steady-state performance, as shown in Figure 1(a). By steady-state performance, we mean the system should settle at the desired setpoint \(}\). By transient performance, we want the system to quickly reaching the steady state with small control efforts.

The inverted pendulum in Figure 1(b) serves as a good motivating example. The input \(u\) is the force on the pendulum. The objectives include 1) the angle \(y\) should reach a setpoint \(\) (e.g., \(5^{}\)) and stays there; 2) minimizing the control cost and deviation of \(y\) from \(\) before reaching the steady state. These objectives are common in many real-world applications, such as vehicle platooning (Fig. 1(b)) and power system control in our experiments.

**Transient performance optimization.** During the transient period, our goal is to quickly stabilize the system to the desired setpoint \(}\), while trading off with using large control efforts in \(\). Let \(J()\) be the cost function of the system. 2 The transient optimization problem up to time \(T\) is,

\[_{}_{t=0}^{T}J((t)-},(t)), \] \[)}:}=( ,),=()\,,\] (2b) \[:_{t}(t)=} \]

Note that in (2a) we sum up to time \(T\), which should be long enough to cover the transient period. Since we require tracking in (2c), the exact value of \(T\) is not critical in the optimization problem. In

Figure 1: (a) The controller aims to improve transient performances after disturbances while guaranteeing stabilization to the desired steady-state output \(}\). (b) Two examples of physical systems with output tracking tasks. (c) We provide end-to-end stability and output tracking guarantees by enforcing stabilizing PI structure in the design of neural networks. The key structure is strictly monotone functions, which are parameterized by the gradient of SCNNs. (d) The transient performance is optimized by training the neural networks.

practice, the system dynamics (1) (and sometimes even the cost function) can be highly nonlinear and nonconvex. Therefore, the current state-of-the-art is to learn \(\) by parameterizing it as a neural network and training it by minimizing the cost in (2a) [43; 44]. But the key challenge with applying these neural network-based controllers is guaranteeing stability and output tracking [45; 26; 30]. Even if the learned policy may appear "stable" during training, it is not necessarily stable during testing. This can be observed in both the vehicle and power system experiments in Section 6.

**Stability for a range of tracking points.** We emphasize that the setpoint \(}\) may vary in practice (e.g., the setpoint velocity of vehicles may change), and thus the controller is required to guarantee stability and output tracking to a range of equilibria corresponding to the setpoints. This is difficult to achieve through existing works that enforce stability by learning a Lyapunov function [26; 27; 30], since the learned Lyapunov function is for a single equilibrium (normally setup as \(^{*}=0_{n}\)). These methods also require that all the states \(\) are observed, which may not be achieved in practice.

**Communication requirement.** A typical constraint in a large system is limits on communications. For example, vehicles in a group may only be able to measure the output of their neighbors (line-of-sight) or nodes in a power system may only have real-time communication from a subset of other nodes. Therefore, the control action \(_{i}\) may be constrained to be a function of a subset of the outputs \(\). We show later in Subsection 4.3 about how these communication constraints can be naturally accommodated in our controller design.

### Bridging controller design and stability through passivity analysis

As shown in Figure 1(a), optimizing transient and steady-state performances are two problems in two different time-scales. Therefore, coordinating them is a central challenge.

**End-to-end performance guarantees.** We propose to overcome this challenge through a structured controller design that provides end-to-end stability and tracking guarantees, as shown in Figure 1(c-d). "End-to-end" means that the guarantees are provided by construction, and do not depend on how the controller is trained. Thus, the neural networks can be trained to optimize the transient performance without jeopardizing the stability and steady-state guarantees. In particular, the construction works for a range of equilibria and can conveniently incorporate communication characteristics.

Instead of working on specific systems individually, we seek to find a family of physical systems that allows us to derive a generalized controller design. It turns out that the notion of equilibrium independent passivity (EIP) [21; 22] provides a concise and useful abstraction of physical systems for stability analysis. This paper thus focuses on systems satisfying the following assumptions:

**Assumption 2** (Equilibrium-Independent Passivity  ).: _The system (1) is strictly equilibrium-independent passive (EIP), which satisfies: (i) there exists a nonempty set \(^{*}^{m}\) such that for every equilibrium \(^{*}^{*}\), there is a unique \(^{*}^{n}\) such that \((^{*},^{*})=_{n}\), and (ii) there exists a positive definite storage function \(S(,^{*})\) and a positive scalar \(\) such that_

\[S(^{*},^{*})=0(, ^{*})-\|-^{*}\|^{2}+(- ^{*})^{}(-^{*}). \]

The EIP property in Assumption 2 has been found in a large class of physical systems, including transportation , power systems [24; 46], robotics , communication , and others. We will conduct experiments on vehicle platoons and power systems to show how EIP can be validated.

The condition (3) of EIP systems provides us a generalized approach to construct Lyapunov functions without knowing the specifics of the dynamics \(()\). In turn, we are able to find the right structure for the controllers. In Section 4, we show what these structures are for PI controllers and how to enforce such structures in the design of neural networks. Then Section 5 formally demonstrates the theoretical guarantees and the training procedure for transient optimization.

## 4 Structured Neural-PI Control

In this section, we construct controllers with a proportional and an integral term, which are both vector-valued strictly monotone functions parameterized by the gradient of strictly convex functions. Then, we present a neural network architecture that is strictly convex by construction and can conveniently incorporate communication constraints.

### Generalized PI control

To realize output tracking, we introduce an integral variable \(}=}-\). Intuitively, \(\) is the accumulated tracking error and will remain unchanged at the steady-state when \(=}\). On this basis, we consider a generalized PI controller \(=(}-)+()\). The first component is the proportional term, where \((}-)\) is a function of the tracking error \(}-\) between the current output \(\) and desired output \(}\). The second component is the integral term \(()\) as a function of the integral of historical tracking errors. Intuitively, the proportional term drives \(\) close to \(}\), and the integral term ensures the tracking error equals zero at the steady state.

**Generalized PI Controller.**_Compactly, the control law is_

\[ =(}-)}_{}+()}_{} \] \[} =}-. \]

**Remark 1**.: _The above controller can be envisioned as a nonlinear generalization of widely adopted linear Proportional-Integral (PI) controllers, where \((}-):=_{P}(}-)\), \(():=_{I}()\) with \(_{P}\) and \(_{I}\) being the proportional and integral coefficients (scalar for SISO control). Linear PI control laws are almost always used in existing works , but the performance of linear PI controllers can be poor for large-scale nonlinear systems._

To achieve provable stability guarantees with output tracking, we need to construct a Lyapunov function that works for the closed-loop system formed by (1) and (4). Therefore, we further design structures in the proportional function \(()\) and the integral function \(()\) that can be utilized in Lyapunov stability analysis. The structures are strictly monotone functions, which generalizes conventional linear functions. In the next subsection, we will show how to parameterize strictly monotone functions. In section 5.1, we will show how the PI controllers structured with monotone functions provide end-to-end stability and output tracking guarantees.

### Strictly monotone function through gradients of strictly convex neural networks

It is not trivial to parameterize strictly monotone functions since this is an infinite-dimensional function space. Scalar-valued monotone functions mapping from \(\) to \(\) has been proposed, but it is difficult to extend these designs to MIMO systems.

In this paper, we propose a new parameterization of strictly monotone functions by leveraging the fact in Proposition 1 that the gradient of a strictly convex function is strictly monotone.

**Proposition 1** (Gradients of strictly convex functions).: _Let \(g:^{m}\) be a strictly convex function, then \((_{}g()-_{}g())^{}(-) 0\)\(,^{m}\), with equality holds if and only if \(=\). Namely, \( g:^{m}^{m}\) is a strictly monotone increasing function._

Hence, the strictly monotone property of \(()\) and \(()\) can be guaranteed by design if they are the gradient of a strictly convex function, as shown by Figure 2. The next proposition shows how to construct a strictly convex neural network (SCNN).

**Proposition 2** (Strictly convex neural networks).: _Consider a function \(g(;):^{m}\) parameterized by \(k\)-layer, fully connected neural network with the input \(^{m}\). The output \(_{l}\) of layer \(l=0,,k-1\) and the function \(g(;)\) is represented as_

\[_{l+1}=_{l}(W_{l}^{(o)}_{l}+W_{l}^{(z)}+b_{l} ), g(;)=o_{k} \]

_where \(_{0},W_{0}^{(o)} 0\), \(=\{W_{0:k-1}^{(z)},W_{1:k-1}^{(o)},b_{0:k-1}\}\) are trainable weights and biases, and \(_{i}\) are non-linear activation functions. The function \(g(;)\) is strictly convex in \(\) provided that all \(W_{1:k-1}^{(o)}\) are positive, \(W_{0}^{(z)}\) is nonzero, and all functions \(_{l}\) are strictly convex and increasing._

Figure 2: Strictly monotone function constructed by strictly convex neural networks (SCNN).

The construction of SCNN follows the general structure of input-convex neural networks [49; 50], where the conditions on activation functions and weights are modified to achieve _strictly_ convexity. The proof follows from the fact that positive sums of strictly convex functions are also strictly convex and that the composition of a strictly convex and convex increasing function is also strictly convex.

The next theorem demonstrates the universal approximation property of SCNN in Proposition 2.

**Theorem 1** (Universal approximation ).: _Let \(\) be a compact domain in \(^{m}\) and \(q():\) be a Lipschitz continuous and strictly convex function. For any \(>0\), there exists a function \(g(;):\) constructed by (5) where the activation function is the softplus-\(\) function_

\[_{l}^{}(x):=(1+e^{ x}), \]

_such that \(|q()-g(;)|<\) for all \(\)._

The proof is given in Appendix A.1. We sketch the proof as follows. We leverage the results in [49; 50] that the structure (5) with ReLU activation is a universal approximation for any convex function. However, ReLU activations are not strictly convex, and thus we design the Softplus-\(\) activation. By showing that the structure (5) with Softplus-\(\) activation can approximate neural networks with the ReLU activations arbitrarily closely when \(\) is sufficiently large, we then prove that the structure (5) with Softplus-\(\) can universally approximate any strictly convex function.

Notably, \(\) in the activation function is a tunable parameter. Hence, we write down the SCNN (5) with activation function being Softplus-\(\) function in (6) as \(g(;,)\), where \(\) is an extra trainable parameter.

Thus, we parameterize the structured Neural-PI control law \(=(}-)+()\) in (4) as

\[(}-) =_{}g^{(P)}(;^{(P)},^{(P)})|_{ =}-}, \] \[() =_{}g^{(I)}(;^{(I)},^{(I)})|_{ =}\,.\]

This way, \(()\) and \(()\) by construction are strictly monotone. In particular, the convex functions \(g^{(P)}(;^{(P)},^{(P)})\), \(g^{(I)}(;^{(I)},^{(I)})\) play a vital role in constructing a generalized Lyapunov function and showing the satisfaction of the Lyapunov condition in Subsection 5.1. This subsequently provides end-to-end stability guarantees that do not dependent on the training algorithm.

### Embedding Communication Constraints

For some large-scale physical systems, not all of the control inputs have access to all of the output measurements at real-time. Therefore, some \(u_{i}\)'s cannot be a function of the full vector \(\) and \(\) because of this lack of global communication, as elaborated in Subsection 3.1.

Constructing the controllers as the gradient of convex functions provides us with a convenient way to embed these communication constraints. Let \(\) be the set of indexes with communications. Take \(m=4\) where \(=(z_{1},z_{2},z_{3},z_{4})\) and the proportional term \(=_{}g^{(P)}(;^{(P)},^{(P)})\) as an example. Figure 3(a) shows the case with global communication, where \(=\{(1,2,3,4)\}\) and \(\) can be a function of the full \(\). Figure 3(b) shows the case without communication where

Figure 3: Communication embedded controller. Take \(m=4\) where \(=(z_{1},z_{2},z_{3},z_{4})\) and P network \(=_{}g^{(P)}(;^{(P)},^{(P)})\) as an example. (a) Global communication, where \(=\{(1,2,3,4)\}\) and \(\) can be a function of the full \(\). (b) Decentralized, \(=\{(1),,(4)\}\) and \(p_{i}\) is a function of \(z_{i}, i\). (c) Partial communication, \(=\{(1,2),(2,3,4)\}\), where there exists communication within \((1,2)\) and \((2,3,4)\).

\(=\{(1),,(4)\}\) and \(p_{i}\) can only be a function of \(z_{i}\) for all \(i\). Figure 3(c) shows the case \(=\{(1,2),(2,3,4)\}\), where there exists communication within the indexes \((1,2)\) and within \((2,3,4)\). By defining SCNN \(g(_{_{j}};_{j}^{(P)},_{j}^{(P)})\) for each group in \(_{j}\), the gradient \(_{}g(_{_{j}};_{j}^{(P)},_{j}^{(P)})\) will only be a function of \(_{_{j}}\) and thus satisfying the commutation constraints. Therefore, we construct the functions in (7) as

\[_{}g^{(P)}(;^{(P)},^{(P)})=_{}_{_{j}}g^{(P)}(_{_{j}};_{j}^{ (P)},_{j}^{(P)}), \]

where \(^{(P)}:=\{_{1}^{(P)},,_{||}^ {(P)}\}\), \(^{(P)}:=\{_{1}^{(P)},,_{||}^{(P)}\}\), and \(_{j}^{(P)},_{j}^{(I)}\) are parameters of the SCNNs for group \(_{j}\) within the communication network.

The function \(g^{(I)}(;^{(I)},^{(I)})\) is also constructed in a similar way as \(g^{(P)}(;^{(P)},^{(P)})\) in (8) to incorporate the communication constraints in \(\). Strict convexity still holds since a sum of strictly convex functions is also strictly convex.

## 5 Training with End-to-End Guarantees

### End-to-end stability and output-tracking guarantees

The convex function \(g^{(I)}(;^{(I)},^{(I)})\) constructed from SCNNs can be utilized to construct a Lyapunov function for proving stability using the Bregman distance defined in the following Lemma :

**Lemma 2** (Bregman distance).: _The Bregman distance associated with the convex function \(g^{(I)}(;^{(I)},^{(I)})\) is defined by_

\[B(,^{*};^{(I)},^{(I)})=g^{(I)}(;^ {(I)},^{(I)})-g^{(I)}(^{*};^{(I)},^{(I)})-_{ }g^{(I)}(^{*};^{(I)},^{(I)})^{}(- ^{*}),\]

_which is positive definite with equality holds if and only if \(=^{*}\)._

The Bregman distance allows us to construct Lyapunov functions without specifying the equilibrium \(^{*}\). Combining the storage function in Assumption 2 and \(B(,^{*};^{(I)},^{(I)})\) in Lemma 2, next theorem shows that the Neural-PI controller stabilizes the system to the desired output \(}\).

**Theorem 2**.: _Suppose Assumption 2 holds and the input \(\) follows the structured PI control law \(=(}-)+()\) where \(()\) and \(()\) are constructed as the gradients of strictly convex neural networks in (7). If the system (1) has a feasible equilibrium, then the system is locally asymptotically stable around it. In particular, the steady-state outputs track the setpoint \(}\), namely \(^{*}=}\)._

Theorem 2 shows that Neural-PI control has provable guarantees on stability and zero steady-state output tracking error from the structures in (7). Detailed proof could be found in Appendix A.2 and we sketch the proof below. At an equilibrium, the right side of (4b) equals to zero gives \(^{*}=}\). We show that an equilibrium is asymptotically stable by constructing a Lyapunov function \(V(,)|_{^{*},^{*}}=S(,^{*})+B(, ^{*};^{(I)},^{(I)})\). Since \(()=_{}g^{(I)}(;^{(I)},^{(I)})\) by construction in (7), the time derivative of the Bregman distance term is \((,^{*};^{(I)},^{(I)})=(()-(^{*}))^{}(-(-^{*}))\). Combining with the fact \((,^{*})-\|-^{*}\| ^{2}+(-^{*})^{}(-^{*})\) (from the EIP assumption), and \(=(}-)+()\), we can conclude that the Lyapunov stability condition holds.

**Remark 2**.: _The satisfaction of the Lyapunov condition does not depend on the specifics of \(()\) (as long as it is EIP), making the stability certification robust to parameter changes for systems satisfying Assumption 2. We will demonstrate this in the experiment on power system control._

### Training to improve transient performances

The Neural-PI controller can be trained by most model-based or model-free algorithms, since the stability and output-tracking are guaranteed by construction. Fig 4 visualizes the detailed construction and the computation graph in the dynamical system in (1). The trainable parameters \(:=\{^{(P)},^{(P)},^{(I)}, ^{(I)}\}\) are contained in \(()\) and \(()\) functions, where both are parameterized as the gradients of SCNNs. \(=_{}-}g^{(P)}(}-;^{(P) },^{(P)})+_{}g^{(I)}(;^{(I)},^{(I)})\) serves as control signal in the system defined in (1) that evolves through time. The loss function is defined as

\[Loss()=_{t=0}^{T}J((t)-},(t)), \]where \(J()\) is the transient cost function defined in (2a). The parameters \(\) are optimized via gradient descent to minimize this loss function (9).

## 6 Experiments

We end the paper with case studies demonstrating the effectiveness of the proposed Neural-PI control in two large-scale systems: vehicle platooning and power system frequency control. Detailed problem formulation, verification of assumptions, simulation setting, and results are provided in Appendix B.1 and B.2 in the supplementary material. All experiments are run with an NVIDIA Tesla T4 GPU with 16GB memory. Code is available at this link.

### Vehicle platooning

**Experiment setup.** We conduct experiments on the vehicle platooning problem in Figure 1(b) with 20 vehicles (\(m=20\)), where \(^{m}\) is the control signal to adjust the velocities of vehicles and the output \(^{m}\) is their actual velocities. The state is \(=(,)\), where \(^{m}\) is the relative position of vehicles. The dynamic model is \(}=\), \(}=}(-(-^{0})+}(-}^{}))\), where \(}\), \(}\), \(}\), \(\), \(\) are constant matrices with their physical meaning given in Appendix B.1.1. The vector \(^{0}^{m}\) reflects the default velocity of vehicles. In Appendix B.1.2, We verify that this system is EIP (i.e., satisfying Assumption 2) using the storage function \(S(,^{*})=(-^{*})^{}}^{-1}}^{-1}(-^{*})+^{ }}^{}\). The objective is for the vehicles to reach the same setpoint velocity quickly with acceptable control effort. We train for 400 epochs, where each epoch trains with the loss (9) averaged on 300 trajectories, and each trajectory evolves 6s from random initial velocities.

**Controller performance.** We compare the performance of 1) Neural-PI: the learned structured Neural-PI controllers parametrized by (7) with three layers and 20 neurons in each hidden layer. 2) DenseNN: Dense neural networks (5) the same as Neural-PI with unconstrained weights. Both of them have no communication constraints. 3) Linear-PI: linear PI control where \((}-):=_{P}(}-)\), \(():=_{I}()\) with \(_{P}\) and \(_{I}\) being the trainable proportional and integral coefficients. Figure 5(a) shows the transient and steady-state costs on 100 testing trajectories starting from randomly generated initial states. Neural-PI attains much lower costs even though the weights in DenseNN are not constrained. Compared with Linear-PI, Neural-PI achieves similar steady-state cost (since it retains all the stability guarantees of classical linear PI control), but reduces the transient cost by approximately 30%. Given \(=5\)m/s, Figure 5(b) and 5(c) show the dynamics of selected nodes under DenseNN and Neural-PI, respectively. All the outputs track under Neural-PI but they are unstable under DenseNN (even though the training cost was not prohibitively high). In particular, DenseNN appears to be stable until about 10s, but states blows up quickly after that. Therefore, enforcing stabilizing structures is essential.

### Power systems frequency control

**Experiment Setup.** The second experiment is the power system frequency control on the IEEE \(39\)-bus New England system , where \(^{m}\) is the control signal to adjust the power injection from generators and the output \(^{m}\) is the rotating speed (i.e., frequency) of generators. The objective is to stabilize generators at the required frequency \(}=60\)Hz at the steady state while minimizing the transient control cost. The state is \(=(,)\), where \(^{m}\) is the rotating angle of

Figure 4: The computation graph for training the Neural-PI controllers.

generators. The dynamics of the system is \(}=\), \(}}=-}(-})-+-}(^{})\), where \(}\), \(}\)\(}\), \(,\) are constant matrices with their physical meaning given in Appendix B.2.1. The vector \(\) is the net load of the system. In Appendix B.2.2, We verify that this system is EIP (i.e., satisfying Assumption 2) using the storage function \(S(,^{*})=(-^{*})^{}}(-^{*})-_{^{}}}((^{ })-(^{}^{*}))-(}( ^{}^{*}))^{}(-^{*}))\). Below we show the impact of communication constraints on the performance of Neural-PI control. More simulation results can be found in Appendix B.2.3 to demonstrate 1) Neural-PI control is robust to parameter changes, disturbances and noises. 2) Neural-PI significantly reduces the number of sampled trajectories to train well compared with DenseNN.

**Impact of communication constraints.** Most systems do not have fully connected real-time communication capabilities, so the controller needs to respect the communication constraints and we show the flexibility of Neural-PI control under different communication structures. We compare the performance of Neural-PI controller where 1) all the nodes can communicate 2) half of the nodes can communicate and 3) none of the nodes can communicate (thus the controller is decentralized), respectively. The transient and steady-state costs are illustrated in Figure 6(a). Neural-PI-Full achieves the lowest transient and steady-state cost. Notably, the steady-state cost significantly decreases with increased communication capability. The reason is that communication serves to better allocated control efforts such that they can maintain output tracking with smaller control costs. The frequency dynamics are shown in Figure 6(b)-(d), where under all settings Neural-PI controllers can stabilize to the setpoint (60Hz) and it converges the fastest under full communication.

## 7 Conclusions

This paper proposes structured Neural-PI controllers for multiple-input multiple-output dynamic systems. We parameterize the P and I terms as the gradients of strictly convex neural networks. For a wide range of dynamic systems, we show that this controller structure provides end-to-end stability and zero output tracking error guarantees. Experiments demonstrate that the Neural-PI control law retains all the stability guarantees of classical linear PI control, but achieves much lower transient cost. Unstructured neural networks, however, lead to unstable behavior and much higher costs. The theoretic guarantees of Neural-PI control also significantly reduce the amount of data required to train well. Since classical PI control is widely utilized in real-world applications, we expect that the controllers can be transferred to real-world scenarios. Potential barriers to the application in real-world scenarios include the verification of EIP when a storage function is difficult to find and provable guarantees on the robustness to noises. These are all important future directions for us.

Figure 5: (a) The average transient cost and steady-state cost with error bar on 100 testing trajectories starting from randomly generated initial states. (b) The dynamics of DenseNN. (c) The dynamics of Neural-PI.

Figure 6: (a) The average transient and steady-state cost with error bar on 100 testing trajectories for Neural-PI controller with different communication constraints. The dynamics of Neural-PI where (b) all nodes can communicate, (c) half of nodes can communicate, (d) none nodes can communicate.