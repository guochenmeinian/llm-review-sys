# Neuronal Competition Groups with Supervised STDP

for Spike-Based Classification

Gaspard Goupy

Univ. Lille, CNRS, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France

Pierre Tirilly

Univ. Lille, CNRS, Centrale Lille, UMR 9189 CRIStAL, F-59000 Lille, France

Ioan Marius Bilasco

###### Abstract

Spike Timing-Dependent Plasticity (STDP) is a promising substitute to backpropagation for local training of Spiking Neural Networks (SNNs) on neuromorphic hardware. STDP allows SNNs to address classification tasks by combining unsupervised STDP for feature extraction and supervised STDP for classification. Unsupervised STDP is usually employed with Winner-Takes-All (WTA) competition to learn distinct patterns. However, WTA for supervised STDP classification faces unbalanced competition challenges. In this paper, we propose a method to effectively implement WTA competition in a spiking classification layer employing first-spike coding and supervised STDP training. We introduce the Neuronal Competition Group (NCG), an architecture that improves classification capabilities by promoting the learning of various patterns per class. An NCG is a group of neurons mapped to a specific class, implementing intra-class WTA and a novel competition regulation mechanism based on two-compartment thresholds. We incorporate our proposed architecture into spiking classification layers trained with state-of-the-art supervised STDP rules. On top of two different unsupervised feature extractors, we obtain significant accuracy improvements on image recognition datasets such as CIFAR-10 and CIFAR-100. We show that our competition regulation mechanism is crucial for ensuring balanced competition and improved class separation.

## 1 Introduction

Neuromorphic computing  with Spiking Neural Networks (SNNs)  is a promising solution to address the high energy consumption of Artificial Neural Networks (ANNs) on von Neumann architectures . However, direct training of SNNs on neuromorphic hardware faces a major constraint: implementing network-level communication is difficult and requires significant circuitry overhead . As a result, the learning mechanisms should be local, i.e., with weight updates based only on the activity of the two neurons that the synapse connects.

Training SNNs to achieve state-of-the-art (SOTA) performance is typically accomplished with adaptations of backpropagation (BP) . However, these methods are challenging to implement on neuromorphic hardware since they employ non-local learning . In addition, they only rely on supervised learning, making them highly dependent on labeled data. We believe that machine learning algorithms should minimize this dependence on supervision by leveraging unsupervised feature learning . Hence, an appealing classification system may comprise both unsupervised and supervised components, for feature extraction and classification, respectively.

Hebbian learning  is an unsupervised and local alternative to BP, inspired by the principal form of plasticity observed in biological synapses. Specifically, Spike Timing-Dependent Plasticity (STDP)  is a form of Hebbian learning where the time difference between the input and output neuron spikes defines synaptic plasticity. STDP could solve all the aforementioned limitations of BP,making it more suitable for on-chip training on neuromorphic hardware [11; 12]. STDP is particularly effective with first-spike coding [13; 14], where neurons can fire at most once per sample. Using one spike per neuron presents several advantages, including energy efficiency [15; 16], fast information transfer , and high information capacity . While primarily used for unsupervised feature learning [19; 20; 21], STDP can be extented to supervised learning [22; 23; 24]. As a result, SNNs can perform classification tasks by combining unsupervised STDP for feature extraction and supervised STDP for classification [25; 26; 27; 28; 29]. Employing the same type of local learning rule for both feature extraction and classification ensures consistency and may facilitate hardware implementation.

Unsupervised STDP is commonly paired with Winner-Takes-All (WTA) competitive learning to promote the discovery of distinct patterns [30; 19; 31; 20]. In a WTA framework with first-spike coding, lateral inhibition is implemented to ensure that only the first neuron to fire receives a weight update. In addition, homeostatic mechanisms, such as threshold adaptation, must be employed to regulate the competition among neurons [32; 20; 33; 34]. For supervised STDP, WTA competition is also appealing as it may improve the learning capabilities of a classification layer with multiple neurons per class . Specifically, intra-class WTA can promote the learning of various class-specific patterns. However, supervised STDP classification with WTA competition has been poorly studied in the literature [22; 35] and presents unbalanced competition challenges. Indeed, there is a lack of competition regulation methods, and regular threshold adaptation rules can lead to unfair decision-making since output neurons may use different thresholds for inference.

In this paper, we address WTA-based competitive learning in supervised STDP. We aim to implement effective WTA competition in a spiking classification layer employing first-spike coding and SOTA supervised STDP rules. Our main contributions can be summarized as follows:

1. We introduce the Neuronal Competition Group (NCG), an architecture that improves classification capabilities by promoting the learning of various patterns per class. In the classification layer, each class is mapped to an NCG: a group of neurons using intra-class WTA and competition regulation.
2. To ensure both balanced intra-class competition and fair decision-making, we design a competition regulation mechanism based on two-compartment thresholds. Neurons are equipped with a fixed threshold for decision-making, along with an adaptive threshold used to regulate the frequency at which they update their weights on samples of their class.
3. To validate our architecture with input features of varying quality, we incorporate NCGs into spiking classification layers placed on top of two Hebbian-based feature extractors. Using NCGs with SOTA supervised STDP rules, we obtain significant accuracy gains on image recognition datasets: MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100. We show that our competition regulation mechanism is crucial for ensuring balanced competition and improved class separation.

The source code is publicly available at: [https://gitlab.univ-lille.fr/fox/snn-ncg](https://gitlab.univ-lille.fr/fox/snn-ncg).

## 2 Related Work

Supervised Training with STDPSupervised training of SNNs with STDP introduces an error signal  that is used to guide the STDP updates. Several supervised adaptations of STDP are reported in the literature [37; 25; 38; 39; 34; 23; 40]. Yet, the aforementioned rules are designed to train SNNs with multiple spikes per neuron, which is not as efficient as first-spike coding. The literature exploring supervised STDP training of SNNs with one spike per neuron is limited [22; 24; 29]. Reward-Modulated STDP (R-STDP)  involves supervised training by adjusting the sign of STDP. The employed error is fairly simple (\(+1\) or \(-1\)), resulting in inaccurate weight updates. SSTDP  and S2-STDP  are more recent methods that compute temporal errors to adjust both the sign and the intensity of weight updates, making them more accurate. However, unlike R-STDP, these methods cannot be used with various neurons per class in a classification layer.

Competitive Learning for ClassificationEmploying groups of neurons is an effective approach for improving the learning capabilities of a classification layer [41; 42; 43]. To maximize knowledge within the layer and learn distinct patterns, WTA-based competitive learning can be employed . While WTA competition is widely adopted in unsupervised learning [30; 19; 31; 20], its application to supervised learning is limited. Prior work [32; 44; 45] implemented WTA competition at the classification layer but only with one neuron per class, making it impossible to learn various class specific patterns. Conversely, reward-based approaches , such as R-STDP, are the only methods that implement WTA with multiple neurons per class. Through lateral inhibition, neurons compete for weight updates, both within the same class (intra-class WTA) and across different classes (inter-class WTA). Intra-class WTA enables neurons to learn patterns from distinct samples. However, inter-class WTA prevents accurate control over the time difference between the spikes of target and non-target neurons (i.e. neurons mapped or not to the class), as only one neuron is updated per sample. In , solely intra-class WTA and two neurons per class were employed to promote specialization toward target and non-target samples. Nonetheless, to the best of our knowledge, no prior work solely employed intra-class WTA to promote the learning of various class-specific patterns.

Competition RegulationIn WTA-based competitive learning, it is crucial to implement regulation (also called homeostatic) mechanisms to ensure balanced competition among neurons . A simple solution is to use dropout  on the output neurons, as done with R-STDP , where some neurons of each class are randomly deactivated during training to encourage weight updates on distinct samples. Yet, this solution is not optimal due to its stochastic nature. Other regulation mechanisms involve threshold adaptation , by increasing or reducing thresholds to promote or discourage firing. While threshold adaptation is an effective solution to ensure balanced competition, using different thresholds across neurons may prevent fair decision-making since their firing time is tied to their thresholds. Prior work employed multiple thresholds per neuron  but the authors did not incorporate threshold adaptation mechanisms. In this work, we draw inspiration from multi-thresholds and threshold adaptation to design a competition regulation mechanism based on two-compartment thresholds, ensuring both balanced competition and fair decision-making.

## 3 Preliminaries

### Neuron Model

To align with first-spike coding, we use the Single-Spike Integrate-and-Fire (SSIF) model , where neurons can fire at most once per sample. Since each neuron emits a single spike, the intensity of its activation is encoded via a firing timestamp: the most activated neuron fires first. The membrane potential \(V_{j}\) of a neuron \(n_{j}\) is expressed as:

\[(t)}{ t} =_{i}W_{ij} S_{i}(t) \] \[S_{i}(t) =1&V_{i}(t)\\ 0&,\]

where \(t\) is the timestamp, \(S_{i}(t)\) indicates the presence or absence of a spike from input neuron \(n_{i}\) at timestamp \(t\), and \(W_{ij}\) is the weight of the synapse from \(n_{i}\) to \(n_{j}\). When the membrane potential of a neuron reaches its firing threshold \(\), the neuron emits a spike, resets its membrane potential to zero, and remains deactivated until the next sample is shown. In our simulations, firing timestamps are represented by floating-point values to align with event-driven neuromorphic hardware.

### Spiking Classification Layer

The spiking classification layer is a fully-connected architecture comprising, for a \(C\)-class problem, \(N=C M\) neurons \((n_{1},,n_{N})\), where \(M\) is the number of neurons per class. Each neuron \(n_{j}\) is mapped to a class \(c_{j}\). Aligned with the SSIF model, we employ first-spike-based decision-making: the first output neuron to fire predicts the class. This method removes the need to propagate the entire input for inference, which can reduce computation time and the number of generated spikes. Formally, the prediction \(\) of the SNN is defined as:

\[ =c_{j^{*}} \] \[j^{*} =*{argmin}_{j[1,N]}(t_{j}),\]

where \(t_{j}\) denotes the firing timestamp of neuron \(n_{j}\). If multiple neurons fire at the same timestamp, the one with the highest membrane potential is selected. In practice, the method employed to select a neuron in the event of a tie has little effect on performance. In this work, the classification layer is placed on top of an unsupervised feature extraction network.

### Supervised STDP Training

Neurons of the classification layer are trained with a supervised STDP rule. At the end of a sample presentation, weights of non-inhibited neurons are updated with an error-modulated additive STDP:

\[ W_{ij}=e_{j} A^{+}&t_{j} t_{i}\\ e_{j} A^{-}&, \]

where \( W_{ij}\) is the weight change (such as \(W_{ij}:=W_{ij}+ W_{ij}\)), \(e_{j}\) is the error of neuron \(n_{j}\), \(A^{+}>0\) and \(A^{-}<0\) are the learning rates. These two learning rates control learning speed and determine the relative importance of long-term potentiation (\(A^{+}\)) versus long-term depression (\(A^{-}\)) in the learning process. Weights are manually clipped in \([w_{},w_{}]\) after each update to ensure that they remain within a controlled range.

R-StdpReward-Modulated STDP (R-STDP)  is a rule combined with WTA competition. For each sample, only the first neuron to fire receives a weight update. The error is \(e_{j}=+1\) if \(n_{j}\) is mapped to the class of the sample, \(e_{j}=-1\) otherwise. In practice, a classification layer trained with R-STDP requires multiple neurons per class to achieve reasonable performance. R-STDP is usually employed in conjunction with adaptive learning rates to reduce overfitting, and dropout to facilitate the learning of various patterns per class .

SstdpSupervised STDP (SSTDP)  is a rule with SOTA performance. It is employed with one neuron per class and without WTA. This rule provides high adaptability to input data by dynamically computing temporal errors for each sample, based on the average firing time \(\) in the layer:

\[e_{j}=t_{j}-\{t_{j},-g\} &c_{j}=y\\ \{t_{j},+g\}&c_{j} y, \]

where \(y\) is the class of the sample, and \(g\) is a hyperparameter that controls the desired distance from \(\). The optimal value of \(g\) partly depends on the input spike distribution: a narrower distribution requires a smaller \(g\). For each sample, due to the \(\) and \(\) functions, only the target neuron firing after \(-g\) and the non-target neurons firing before \(+g\) update their weights.

S2-StdpStabilized Supervised STDP (S2-STDP)  addresses two limitations of SSTDP: the limited number of updates per epoch and the saturation of firing timestamps toward the maximum firing time. In this rule, neurons are trained to fire at desired timestamps instead of time ranges:

\[e_{j}=t_{j}--g&c_{j}=y\\ +g&c_{j} y. \]

This enables more accurate control over the output firing times and reduces the saturation effect. Also, weight normalization is used to keep a similar weight average across neurons during learning .

## 4 Methods

### Neuronal Competition Group

Training a classification layer involves teaching neurons to recognize a pattern specific to their class from the input samples. Different samples from a given class can contain distinct, mutually exclusive patterns or combinations of patterns. Learning all these patterns concurrently with one neuron can be challenging and impose strong generalization constraints on its weights, especially when using a single supervised layer. Employing multiple neurons per class to learn various class-specific patterns may reduce these constraints and enable the emergence of more specialized patterns that better represent the training set distribution. Building on this concept, we introduce the Neuronal Competition Group (NCG), an architecture promoting the learning of various class-specific patterns through intra-class WTA and competition regulation.

The NCG architecture, illustrated in Figure 1, augments a classification layer by mapping each class to an NCG instead of independent neurons. An NCG is a group of \(M\) neurons that aim to learn different patterns from samples of their mapped class. Neurons of an NCG are interconnected withlateral inhibition, such as, for a given sample, the first neuron to fire within a group emits an inhibitory signal that prevents the other ones from firing. Lateral inhibition induces competitive learning through intra-class WTA: only the first neuron to fire undergoes the weight update. There is no lateral inhibition between NCGs (i.e. inter-class WTA). Hence, each sample triggers exactly one weight update per NCG. Removing inter-class WTA enables more accurate control over the time difference between the spikes of target and non-target neurons, which can improve class separation . Each time a sample is presented during training, competition regulation is triggered in the NCG mapped to the class of the sample. This mechanism ensures balanced competition within the NCGs, which facilitate the learning of various class-specific patterns.

### Competition Regulation

Preliminary experiments highlighted that intra-class WTA competition provided by lateral inhibition is not enough to ensure balanced competition. In practice, for each NCG, one neuron tends to dominate the others, receiving the majority of the weight updates from samples of its class. Although threshold adaptation can be employed to regulate competition , in a decision-making context, different thresholds between neurons may lead to unfair decisions because predictions are based on the first spike. To ensure both balanced intra-class competition and fair decision-making, we introduce a competition regulation mechanism based on two-compartment thresholds.

In the classification layer, all the neurons are equipped with an identical and fixed threshold, denoted as the test threshold \(\). This threshold remains fixed to ensure fair decision-making during inference, as the class is predicted by the neuron that fires first. On top of that, neurons are equipped with an additional varying threshold, denoted as the training threshold \(^{}\). Neurons switch to their \(^{}\) only when they are exposed to samples of their class during training. Otherwise, they always employ \(\), both for inference and for samples of other classes during training. \(^{}\) is the key component to balance intra-class competition: it can be increased or decreased to encourage or reduce neuron firing on samples of its class. Each time a neuron receives a weight update from a sample of its class,

Figure 1: Spiking classification layer with Neuronal Competition Groups (NCGs). In this layer, each class is mapped to an NCG and the prediction is based on the first spike. An NCG is a group of \(M\) neurons connected with lateral inhibition to enable intra-class WTA competition: the first neuron to fire inhibits the other ones and undergoes a weight update based on a temporal error (which depends on the learning rule considered). The sign and amplitude of the error pushes neurons to fire earlier (positive sign) or later (negative sign). Competition regulation occurs only within the NCG mapped to the class of the input sample to ensure balanced competition among neurons on samples of their class. NCGs improve the classification capabilities of a layer by promoting the learning of various patterns per class.

competition regulation is triggered across neurons of its NCG. Their \(^{}\) are updated as follows:

\[^{}_{j} =+_{}&t_{j}= \{t_{1},,t_{M}\}\\ -_{}& \] \[^{}_{j} :=\{_{j},^{}_{j}+^{ }_{j}\},\]

where \(^{}_{j}\) and \(_{j}\) are the training and test thresholds of neuron \(n_{j}\), \(M\) is the number of neurons in the NCG, \(_{}\) is the threshold learning rate, and \(t_{j}\) is the firing timestamp of neuron \(n_{j}\). If several neurons fire at the same timestamp, the one with the highest membrane potential is selected (this has no impact on performance). \(^{}\) is reset to \(\) between epochs and its minimum achievable value is \(\). These two components ensure that neurons learn patterns consistent with \(\), which is the threshold that they use for inference. \(_{}\) defines the strength of competition regulation: higher values favor more balanced competition but may deteriorate pattern learning since \(^{}\) tend to increase within an epoch. It should be chosen together with the initial threshold (a higher threshold may require a higher \(_{}\)). To achieve better convergence and robustness, an annealing factor \(_{}\) can be added to reduce \(_{}\) after each epoch, such as \(_{}:=_{}_{}\). \(_{}\) affects the number of epochs during which competition regulation occurs and should be adjusted according to \(_{}\): higher \(_{}\) requires lower \(_{}\).

### Neuron Labeling

In , WTA competition enhances a classification layer with two neurons per class and S2-STDP training by naturally promoting, for each class, neuron specialization toward target or non-target samples. This behavior can also be implemented with NCGs, but it requires explicit neuron labeling to ensure that all neurons but one specialize toward samples of their class. In such cases, one neuron within each NCG can be labeled as non-target, whereas the others can be labeled as target. All the neurons are connected with lateral inhibition but only target neurons are connected with competition regulation. Hence, if the non-target neuron fires first for a sample of the class, it prevents target neurons from updating their weights and applying competition regulation. Regardless of the class of the sample, a non-target neuron \(n_{j}\) winning the competition always updates its weights as if \(c_{j} y\) in Equation 5. STDP training remains unchanged for target neurons. However, in Equation 6, \(M\) must be updated as it refers to the number of target neurons. In Supplementary Material (Section 1), we provide the overall algorithm for training a spiking classification layer with our proposed methods.

## 5 Experiments

### Experimental Setup

#### 5.1.1 Datasets

We select four image recognition datasets of growing complexity: MNIST , Fashion-MNIST , CIFAR-10 , and CIFAR-100 . MNIST and Fashion-MNIST comprise \(28 28\) grayscale images, \(60,000\) samples for training and \(10,000\) for testing, categorized into \(10\) classes. CIFAR-10 and CIFAR-100 comprise \(32 32\) RGB images, \(50,000\) for training and \(10,000\) for testing. They consist of, respectively, \(10\) and \(100\) classes.

#### 5.1.2 Classification Pipeline

Our classification system consist of a feature extractor trained with unsupervised Hebbian-based learning, followed by a spiking classification layer trained with supervised STDP. Training is layer-wise: the feature extractor is trained entirely before the training of the classification layer starts. The complete pipeline of our classification system is illustrated in Supplementary Material (Section 2.1).

#### 5.1.3 Unsupervised Feature Extractors

To improve image representation before classification without labeled data, we consider two Hebbian-based unsupervised feature extractors built on Convolutional Neural Networks (CNNs):

1. STDP-CSNN , a single-layer spiking CNN trained with STDP;
2. SoftHebb-CNN , a three-layer non-spiking CNN trained with SoftHebb.

Employing various feature extractors allows us to validate our methods with input features of varying quality. These two feature extractors are SOTA in their category (spiking/non-spiking), share local learning properties, and offer different baseline performances. In particular, SoftHebb-CNN, while not spike-based, is a relevant alternative for exploring classification using features provided by effective multi-layer local learning. The extracted feature maps are flattened to match the fully-connected architecture of the classification layer. Since SoftHebb-CNN is not spike-based, its output features are encoded into spike timestamps with a form of first-spike coding . STDP-CSNN outputs \(4,608\) features for MNIST/Fashion-MNIST, and \(6,272\) for CIFAR-10/100. SoftHebb-CNN outputs \(13,824\) features for MNIST/Fashion-MNIST, and \(24,576\) for CIFAR-10/100. Aligned with first-spike coding, each feature is a single floating-point spike timestamp in \(\). Additional details are reported in Supplementary Material (Section 2.2).

#### 5.1.4 Spiking Classification Layers

We train fully-connected spiking classification layers with three existing supervised STDP rules designed for one spike per neuron: R-STDP , SSTDP , and S2-STDP . We incorporate the NCG architecture into classification layers trained with SSTDP and S2-STDP, denoted as SSTDP+NCG and S2-STDP+NCG, respectively. R-STDP is incompatible with NCGs since it requires inter-class WTA for weight convergence. Unless otherwise specified, we set \(M=5\) neurons per class for NCG-based methods, which is the smallest value providing, on average, near-optimal performance on the evaluated datasets (see Section 3.2 of Supplementary Material). We evaluated R-STDP with both \(M=5\) and \(M=20\), the value providing near-optimal performance for this rule. With S2-STDP+NCG, one neuron of each NCG is labeled as non-target, as detailed in Section 4.3.

#### 5.1.5 Protocol

We divide our experimental protocol into two phases: hyperparameter optimization and evaluation. In both phases, we employ an early stopping mechanism (with a patience \(\)) during training to prevent overfitting. For hyperparameter optimization, we construct a validation set from the training set by randomly selecting, for each class, a percentage \(\) of its samples. Then, we use the gridsearch algorithm to optimize the hyperparameters of the spiking classification layer (for each rule, dataset, and feature extractor). No gridsearch is performed on CIFAR-100: we employ the optimized hyperparameters from CIFAR-10, given the similarities between the two datasets. Additional details regarding hyperparameters are provided in Supplementary Material (Section 2.3). For evaluation, we employ the K-fold cross-validation strategy. We divide the training set into \(K\) subsets and train \(K\) models, each using a different subset for validation while the remaining \(K-1\) subsets are used for training. Each model is trained with a different seed. Then, we evaluate the trained models on the test set and we compute the mean test accuracy and standard deviation (1-sigma). We use \(=10\), \(K=10\) and \(=\) (i.e. we allocate 10% of the training sets for validation).

### Accuracy Comparison

We compare, in Table 1, the performance of the different STDP-based methods for training a spiking classification layer (see Section 5.1.4) placed on top of each unsupervised feature extractor (see Section 5.1.3). Our proposed NCG architecture consistently improves the performance of SSTDP and S2-STDP across all datasets and feature extractors. The accuracy improvement tends to scale with the complexity of the dataset. With S2-STDP and the STDP-CSNN feature extractor, we measure an increase of \(1.18\) pp on MNIST, \(2.83\) pp on Fashion-MNIST, \(5.33\) pp on CIFAR-10, and \(6.51\) pp on CIFAR-100. S2-STDP always outperforms SSTDP and enables higher accuracy improvement when paired with NCG as it leverages neuron labeling. While S2-STDP surpasses R-STDP when the input features are well-captured, it falls behind in scenarios involving lower-quality features (CIFAR-10 with STDP-CSNN, CIFAR-100), as R-STDP can learn various patterns per class. S2-STDP+NCG effectively bridges this gap, outperforming R-STDP on both simpler and harder tasks while requiring four times fewer neurons per class. When R-STDP is used with the same number of neurons as S2-STDP+NCG, the accuracy gap is even larger. These results highlight that WTA-based competitive learning in a supervised context can be achieved without inter-class WTA. Employing solely intra-class WTA and accurate STDP updates enables more effective training.

Regarding the literature on SNNs with fully-supervised local-based learning, SOTA performance is achieved by STDi-BP (one spike per neuron)  on MNIST (\(99.20\)% with a 3-layer SNN) as well as Fashion-MNIST (\(92.80\)% with a 4-layer SNN), and by EMSTDP (multiple spikes per neuron)  on CIFAR-10 (\(64.40\)% with a 4-layer SNN). We did not find any work reporting results on CIFAR-100. For approaches combining unsupervised and supervised local learning, SOTA performance is achieved by R-STDP  on MNIST (\(97.20\)% with a 3-layer SNN) and by Sym-STDP  on Fashion-MNIST (\(85.31\)% with a 2-layer SNN). We did not find any work reporting results on CIFAR-10/100. Our best models, comprising 4-layer networks with only one supervised layer, achieve \(99.17\)% on MNIST, \(91.86\)% on Fashion-MNIST, and \(79.55\)% on CIFAR-10. Our results closely match or surpass fully-supervised SOTA work and outperform semi-supervised SOTA work. Yet, it is important to acknowledge the role of the feature extractor in the final performance. There remains a huge gap between local-based and global-based approaches in terms of accuracy. In Supplementary Material (Section 4), we compare our methods with global-based approaches to highlight that, despite the accuracy gap, local-based methods show greater computational efficiency, lower memory usage, reduced energy consumption, and easier hardware implementation, justifying further exploration.

### Ablation Study

We conduct, in Table 2, an ablation study on S2-STDP+NCG to evaluate each component of our methods. _M-1_ and _M-5_ represent S2-STDP+NCG with \(M=1\) (one neuron per class, which is similar to S2-STDP) and \(M=5\), without competition regulation and neuron labeling. _CR-1_ denotes our competition regulation mechanism with a single threshold per neuron (i.e. \(^{}=\) in Equation 6),

   Dataset & Method & Neurons per class & Accuracy (Mean\(\)Std \%) \\   &  & 5 & \(96.82 0.29\) & \(97.72 0.26\) \\  & & 20 & \(97.49 0.12\) & \(98.24 0.15\) \\   & SSTDP & 1 & \(96.44 0.09\) & \(98.52 0.16\) \\  & SSTDP+NCG _(ours)_ & 5 & \(97.30 0.09\) & \(98.96 0.06\) \\   & S2-STDP & 1 & \(97.74 0.06\) & \(98.81 0.09\) \\  & S2-STDP+NCG _(ours)_ & 5 & \(\) & \(\) \\   &  & 5 & \(78.40 0.89\) & \(87.32 0.76\) \\  & & 20 & \(82.17 0.38\) & \(88.06 0.29\) \\   & SSTDP & 1 & \(85.26 0.17\) & \(89.36 0.24\) \\   & SSTDP+NCG _(ours)_ & 5 & \(87.59 0.11\) & \(91.06 0.10\) \\   & S2-STDP & 1 & \(85.89 0.27\) & \(90.61 0.19\) \\  & S2-STDP+NCG _(ours)_ & 5 & \(\) & \(\) \\   &  & 5 & \(62.12 0.62\) & \(74.12 0.34\) \\  & & 20 & \(65.92 0.68\) & \(75.54 0.57\) \\   & SSTDP & 1 & \(60.87 0.53\) & \(76.57 0.58\) \\   & SSTDP+NCG _(ours)_ & 5 & \(64.05 0.48\) & \(78.53 0.32\) \\   & S2-STDP & 1 & \(61.08 0.17\) & \(76.90 0.27\) \\   & S2-STDP+NCG _(ours)_ & 5 & \(\) & \(\) \\   &  & 5 & \(32.07 0.38\) & \(48.27 0.36\) \\  & & 20 & \(34.77 0.44\) & \(49.25 0.48\) \\    & SSTDP & 1 & \(28.49 0.49\) & \(48.73 0.39\) \\   & SSTDP+NCG _(ours)_ & 5 & \(31.19 0.27\) & \(49.81 0.23\) \\    & S2-STDP & 1 & \(29.39 0.19\) & \(49.17 0.29\) \\   & S2-STDP+NCG _(ours)_ & 5 & \(\) & \(\) \\   

Table 1: Accuracy of spiking classification layers trained with STDP-based methods, on top of Hebbian-based unsupervised feature extractors.

as commonly used in WTA-based SNNs . Thresholds are not clipped nor reset between epochs, and the learned values are used for inference. _CR-2_ denotes our competition regulation with two-compartment thresholds. \(L\) is neuron labeling. _Drop_ is dropout on the output neurons, an alternative competition regulation mechanism employed with R-STDP . For each method, we optimized hyperparameters with gridsearch (see Section 5.1.5). Results on both Fashion-MNIST and CIFAR-10 show that each component of our methods (cf. _M-5, CR-2, L_) brings an individual and significant accuracy gain. Competition regulation tends to be crucial for benefiting from improved class separation, especially with STDP-CSNN. The accuracy gain gets lower with SoftHebb-CNN as the extracted features exhibit higher class separability. Neuron labeling enhances the performance of our models through neuron specialization. Our competition regulation mechanism based on two-compartment thresholds (cf. _CR-2_) outperforms the existing threshold adaptation with one threshold (cf. _CR-1_), as well as dropout (cf. _Drop_). In a first-spike-based decision-making context, we find that learning thresholds (cf. _CR-1_) is not mandatory for successfully learning various patterns. Instead, it is more important to ensure fair decision-making with fixed thresholds and use threshold adaptation as a competition regulation mechanism only. In Supplementary Material, we provide an ablation study on SSTDP+NCG with similar results (Section 3.4), as well as additional studies on the impact of neuron labeling (Section 3.1) and hyperparameters (Section 3.3).

### Impact of Competition Regulation

In this section, we show that competition regulation is crucial for ensuring balanced competition and improved class separation. Figure 2 illustrates the number of updates per epoch received by the neurons of class \(0\) trained with S2-STDP+NCG, with and without competition regulation, on CIFAR-10. Target (resp. non-target) updates are triggered by samples of the class (resp. another class). Without competition regulation, no competition takes place between target neurons. Target

Table 2: Ablation study on S2-STDP+NCG. \(M\) is the number of neurons per class, _CR_ is competition regulation with 1 or 2 thresholds, \(L\) is neuron labeling, and _Drop_ is dropout.

Figure 2: Number of weight updates per epoch received by the neurons of class \(0\) trained with S2-STDP+NCG, with and without competition regulation, on CIFAR-10. \(n_{1}\) to \(n_{4}\) are labeled as target neurons and \(n_{5}\) is labeled as non-target. The features are extracted with STDP-CSNN.

neuron \(n_{2}\) receives the majority of the target updates, while the other target neurons \(n_{1}\), \(n_{3}\), \(n_{4}\) assume the role of non-target neurons (i.e. receive mainly non-target updates), as they are inhibited by \(n_{2}\) on samples of the class. With competition regulation, the target neurons (\(n_{1}\) to \(n_{4}\)) effectively specialize toward samples of their class, while the non-target neuron (\(n_{5}\)) specializes toward samples of other classes. Regarding target neurons, we observe a balanced competition in their target updates, illustrating the effectiveness of our competition regulation mechanism. In Supplementary Material (Section 3.5), we show similar results for other classes and datasets, as well as for SSTDP+NCG.

In another experiment, we analyze the weights trained using S2-STDP+NCG, with and without competition regulation. Figure 3 shows t-SNE  visualizations of the learned weights on CIFAR-10. Without competition regulation, there is a single cluster at the center, comprising the weights of the neurons that receive few target updates during training. With competition regulation, the weights of the target neurons tend to form, for each class, distinct clusters. The spread of their clusters suggests that they have learned various class-specific patterns. The weights of the non-target neurons form a single cluster at the center since their weights are very similar. This similarity arises because non-target neurons, regardless of their class, are trained to fire at the same desired timestamp. In Supplementary Material (Section 3.5), we further show that competition regulation increases the intra-class distinctiveness among weights, which improves class separation.

## 6 Discussion

The NCG architecture implements effective intra-class WTA in a spiking classification layer employing first-spike coding and supervised STDP training. Our competition regulation mechanism based on two-compartment thresholds ensures both balanced competition and fair decision-making. We showed that this mechanism improves class separation and achieves better performance than existing regulation methods. As a result, NCGs significantly increased the accuracy of SOTA supervised STDP rules. This work highlights that more effective supervised competitive learning can be achieved without inter-class WTA. Also, the success in learning various patterns per class via threshold adaptation does not depend on learning various thresholds.

In this work, supervised STDP rules are employed in the classification layer to ensure consistency with the training of the feature extraction network. However, our contributions focus on the architecture of the classification layer, which is independent of the learning rule used to train it. Thus, NCGs may theoretically be used with any other rule designed for training SNNs with one spike per neuron. We performed preliminary experiments with S4NN , a gradient-based rule, and observed that the addition of NCGs led to accuracy improvements consistent with our previous results (see Section 3.6 of Supplementary Material). Yet, further research is required to validate the effectiveness of NCGs with gradient-based rules.

While NCGs successfully improve the performance of a classification layer, they also come with several limitations. First, they increase the costs in terms of parameters, computation, and hardware. The computational overhead of NCGs scales linearly with the number of neurons. In hardware design, they introduce another overhead due to the additional connections, both to the previous layer and within the layer. Second, increasing the number of neurons strengthens specialization on the training set, especially when faced with a higher number of input features (cf. SoftHebb-CNN). This behavior limits the generalization capabilities of our models and requires additional research to fully exploit their potential in these scenarios. Third, the NCG architecture applies only to the output layer of a network. Nevertheless, this work is the first to introduce WTA and competition regulation mechanisms specifically designed for classification. It establishes the relevance of such mechanisms in this context, laying the foundations for future research on WTA-based supervised competitive learning in multi-layer networks.

Figure 3: t-SNE plots of the weights learned with S2-STDP+NCG on CIFAR-10, with and without competition regulation. Crosses and circles respectively represent the weights of target and non-target neurons, and colors indicate classes. The features are extracted with STDP-CSNN.