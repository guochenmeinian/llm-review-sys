# Language Models Encode Collaborative Signals in Recommendation

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Recent studies empirically indicate that language models (LMs) encode rich world knowledge beyond mere semantics, attracting significant attention across various fields. However, in the recommendation domain, it remains uncertain whether LMs implicitly encode user preference information. Contrary to the prevailing understanding that LMs and traditional recommender models learn two distinct representation spaces due to a huge gap in language and behavior modeling objectives, this work rethinks such understanding and explores extracting a recommendation space directly from the language representation space. Surprisingly, our findings demonstrate that item representations, when linearly mapped from advanced LM representations, yield superior recommendation performance. This outcome suggests a homomorphic relationship between the language representation space and an effective recommendation space, implying that collaborative signals may indeed be encoded within advanced LMs. Motivated by these findings, we propose a simple yet effective collaborative filtering (CF) model named **AlphaRec**, which utilizes language representations of item textual metadata (_e.g.,_ titles) instead of traditional ID-based embeddings. Specifically, AlphaRec is comprised of three main components: a multilayer perceptron (MLP), graph convolution, and contrastive learning (CL) loss function, making it extremely easy to implement and train. Our empirical results show that AlphaRec outperforms leading ID-based CF models on multiple datasets, marking the first instance of such a recommender with text embeddings achieving this level of performance. Moreover, AlphaRec introduces a new text-based CF paradigm with several desirable advantages: being easy to implement, lightweight, rapid convergence, superior zero-shot recommendation abilities in new domains, and being aware of user intention.

## 1 Introduction

Language models (LMs) have achieved great success across various domains , prompting a critical question about the knowledge encoded within their representation spaces. Recent studies empirically find that LMs extend beyond semantic understanding to encode comprehensive world knowledge about various domains, including game states , lexical attributes , and even concepts of space and time  through language modeling. However, in the domain of recommendation where the integration of LMs is attracting widespread interest , it remains unclear whether LMs inherently encode relevant information on user preferences and behaviors. One possible reason is the significant difference between the objectives of language modeling for LMs and user behavior modeling for recommenders .

Currently, one prevailing understanding holds that general LMs and traditional recommenders encode two distinct representation spaces: the language space and the recommendation space (_i.e.,_ user and item representation space), each offering potential enhancements to the other forrecommendation tasks . On the one hand, when using LMs as recommenders, aligning the language space with the recommendation space could significantly improve the performance of LM-based recommendation . Various alignment strategies are proposed, including fine-tuning LMs with recommendation data , incorporating embeddings from traditional recommenders as a new modality of LMs , and extending the vocabulary of LMs with item tokens . On the other hand, when using LMs as the enhancer, traditional recommenders greatly benefit from from leveraging text representations , semantic and reasoning information , and generated user behaviors . Despite these efforts, explicit explorations of the relationship between language and recommendation spaces remain largely unexplored.

In this work, we rethink the prevailing understanding and explore whether LMs inherently encode user preferences through language modeling. Specifically, we test the possibility of directly deriving a recommendation space from the language representation space, assessing whether the representations of item textual metadata (_e.g.,_ titles) obtained from LMs can independently achieve satisfactory recommendation performance. Positive results would imply that user behavioral patterns, such as **collaborative signals** (_i.e.,_ user preference similarities between items) , may be implicitly encoded by LMs. To test this hypothesis, we employ linear mapping to project the language representations of item titles into a recommendation space (see Figure 0(a)). Our observations include:

* Surprisingly, this simple linear mapping yields high-quality item representations, which achieve exceptional recommendation performance (see Figure 0(b) and experimental results in Section 2).
* The clustering of items is generally preserved from the language space to the recommendation space (see Figure 0(c)). For example, movies with the theme of superheroes and monsters are gathering in both language and recommendation spaces.
* Interestingly, the linear mapping effectively reveals preference similarities that may be implicit or even obscure in the language space. For instance, while certain movies, such as those of homosexual movies (illustrated in Figure 0(c)), show dispersed representations in the language space, their projections through linear mapping tend to cluster together, reflecting their genres affiliation.

These findings indicate a homomorphic relationship between the language representation space of LMs and an effective item representation space for recommendation. Motivated by this insight, we propose a new text-based recommendation paradigm for general collaborative filtering (CF), which utilizes the pre-trained language representations of item titles as the item input and the average historical interactions' representations as the user input. Different from traditional ID-based CF models  that heavily rely on trainable user and item IDs, this paradigm solely uses

Figure 1: Linearly mapping item titles in language representation space into recommendation space yields superior recommendation performance on Movies & TV  dataset. (0(a)) The framework of linear mapping. (0(b)) The recommendation performance comparison between leading CF recommenders and linear mapping. (0(c)) The t-SNE  visualizations of movie representations, with colored lines linking identical movies or user intention across language space (left) and linearly projected recommendation space (right).

pre-trained LM embeddings and completely abandons ID-based embeddings. In this paper, to fully explore the potential of advanced language representations, we adopt a simple model architecture consisting of a two-layer MLP with graph convolution, and the popular contrastive loss, InfoNCE , as the objective function. This model is named **AlphaRec** for its originality and a series of good properties.

Benefiting from paradigm shifts from ID-based embeddings to language representations, AlphaRec presents three desirable advantages. First, AlphaRec is notable for its simplicity, lightweight, rapid convergence, and exceptional recommendation performance (see Section 4.1). We empirically demonstrate that, for the first time, such a simple model with embeddings from pre-trained LMs can outperform leading CF models on multiple datasets. This finding strongly supports the possibility for developing language-representation-based recommender systems. Second, AlphaRec exhibits a strong zero-shot recommendation capability across untrained domains (see Section 4.2). By co-training on three Amazon datasets (Books, Movies & TV, and Video Games) , AlphaRec can achieve performance comparable to the fully-trained LightGCN on entirely different platforms (MovieLens-1M  and BookCrossing ), and even exceed LightGCN in a completely new domain (Amazon Industrial), without additional training on these target datasets. This capability underscores AlphaRec's potential to develop more general recommenders. Third, AlphaRec is user-friendly, offering a new research paradigm that enhances recommendation by leveraging language-based user feedback (see Section 4.3). Endowed with its inherent semantic comprehension of language representations, AlphaRec can refine recommendations based on user intentions expressed in natural language, enabling traditional CF recommenders to evolve into intention-aware systems through a straightforward paradigm shift.

## 2 Uncovering Collaborative Signals in LMs via Linear Mapping

In this section, we aim to explore whether LMs implicitly encode collaborative signals in their representation spaces. We first formulate the personalized item recommendation task, then detail the linear mapping and its empirical findings. Empirical evidence indicates a homomorphic relationship between the representation spaces of advanced LMs and effective recommendation spaces.

**Task formulation.** Personalized item recommendation with implicit feedback aims to select items \(i\) that best match user \(u\)'s preferences based on binary interaction data \(=[y_{ui}]\), where \(y_{ui}=1\) (\(y_{ui}=0\)) indicates user \(u\) has (has not) interacted with item \(i\). The primary objective of recommendation is to model the user-item interaction matrix \(\) using a scoring function \(:\), where \(_{ui}\) measures \(u\)'s preference for \(i\). The scoring function \(_{ui}=s_{}(_{u},_{i})\) comprises three key components: pre-existing features \(_{u}\) and \(_{i}\) for user \(u\) and item \(i\), a representation learning module \(_{}(,)\) parametrized by \(\), and a similarity function \(s(,)\). The representation learning module \(_{}\) transfers \(u\) and \(i\) into representations \(_{u}\) and \(_{i}\) for similarity matching \(s(_{u},_{i})\), and the Top-\(K\) highest scoring items are recommended to \(u\).

Different recommenders employ various pre-existing features \(_{u},_{i}\) and representation learning architecture \(_{}(,)\). Traditional ID-based recommenders use one-hot vectors as pre-existing features \(_{u},_{i}\). The choice of ID-based representation learning architecture \(_{}\) can vary widely, including ID-based embedding matrix , multilayer perception , graph neural network , and variational autoencoder . The commonly used similarity function is cosine similarity \(s(_{u},_{i})=_{u}^{}_{i}}{ \|_{u}\|\|_{i}\|}\), which we adopt in this paper.

**Linear mapping.** Building on the extensive knowledge encoded by LMs, we explore utilizing LMs as feature extractors, leveraging the language representations of item titles as initial item feature \(_{i}\). For initial user feature \(_{u}\), we use the average of the title representations of historically interacted items, defined as \(_{u}=_{u}|}_{i_{u}}_{i}\), where \(_{u}\) is the set of items user \(u\) has interacted with. Detailed procedures for obtaining these language-based features are provided in Appendix B.2. We select a trainable linear mapping matrix \(\) as the representation learning module \(_{}\), setting \(_{u}=_{u}\) and \(_{i}=_{i}\). To learn the linear mapping \(\), we adopt the InfoNCE loss  as the objective function, which has demonstrated state-of-the-art performance in both ID-based  and LM-enhanced collaborative filtering (CF) recommendations  (refer to Equation (4) for the formula). The overall framework of the linear mapping process is illustrated in Figure 0(a). We directly use linearly mapped representations \(_{u}\) and \(_{i}\) to calculate the user-item similarity \(s(_{u},_{i})\) for recommendation. High performance on the test set would suggest that collaborative signals (_i.e.,_ user preference similarities between items) have been implicitly encoded in the language representation space [67; 10].

**Empirical findings.** We compare the recommendation performance of the linear mapping method with three classical CF baselines, matrix factorization (MF) [54; 68], MultVAE , and LightGCN  (see more details about baselines in Appendix C.2.1). We report three widely used metrics Hit Ratio (HR@\(K\)), Recall@\(K\), Normalized Discounted Cumulative Gain (NDCG@\(K\))) to evaluate the effectiveness of linear mapping, with \(K\) set by default at 20. We evaluate a wide range of LMs, including BERT-style models [4; 5], decoder-only language models [6; 69], and LM-based text embedding models [70; 71] (see Appendix B.1 for details about used LMs).

Table 1 reports the recommendation performance yielded by the linear mapping on three Amazon datasets , comparing with classic CF baselines. We observe that the performance of most advanced text embedding models (_e.g.,_ text-embeddings-3-large  and SFR-Embedding-Mistral ) exceed LightGCN on all datasets. We further empirically prove that these improvements do not merely come from the better feature encoding ability (refer to Appendix B.3). These findings indicate the homomorphic relationship between the language representation space of advanced LMs and an effective item representation space for recommendation. Moreover, with the advances in LMs, the performance of item representation linearly mapped from LMs exhibits a rising trend, gradually surpassing traditional ID-based CF models. Representations from early BERT-style models (_e.g.,_ BERT  and RoBERTa ) only show weaker or equal capabilities compared with MF, while the performance of decoder-only LMs (_e.g.,_ Llama-7B  ) start to match MultVAE and LightGCN.

## 3 AlphaRec

This finding of space homomorphic relationship sheds light on building advanced CF models purely based on LM representations without introducing ID-based embeddings. To be specific, we try to incorporate only three simple components (_i.e.,_ nonlinear projection , graph convolution  and contrastive learning (CL) objectives ), to develop a simple yet effective CF model called AlphaRec. It is important to highlight that our approach is centered on exploring the potential of LM representations for CF by integrating essential components from leading CF models, rather than deliberately inventing new CF mechanisms. We present the model structure of AlphaRec in Section 3.1, and compare AlphaRec with two popular recommendation paradigms in Section 3.2.

### Method

We present how AlphaRec is designed and trained. Generally, the representation learning architecture \(_{}(,)\) of AlphaRec is simple, which only contains a two-layer MLP and the basic graph convolution operation, with language representations as the input features \(_{u},_{i}\). The cosine similarity is used as the similarity function \(s(,)\), and the contrastive loss InfoNCE [56; 57] is adopted for optimization. For simplicity, we consistently adopt text-embeddings-3-large  as the language representation model, for its excellent language understanding and representation capabilities.

**Nonlinear projection.** In AlphaRec, we substitute the linear mapping matrix delineated in Section 2 with a nonlinear MLP. This conversion from linear to nonlinear is non-trivial, for the paradigm shift from ID-based embeddings to LM representations, since nonlinear transformation helps in excavating more comprehensive collaborative signals from the LM representation space with rich semantics (see

    &  &  &  \\  & Recall & NDCG & HR &  & NDCG & HR &  & NDCG & HR \\  MF (Bendle et al., 2012) & 0.0437 & 0.0391 & 0.2476 & 0.0568 & 0.0519 & 0.3437 & 0.0323 & 0.0195 & 0.0864 \\ MultiVAE (Liang et al., 2018) & 0.0722 & 0.0597 & 0.3418 & 0.0853 & 0.0776 & 0.4434 & 0.0098 & 0.0531 & 0.2211 \\ LightGCN (He et al., 2021) & 0.0723 & 0.0608 & **0.3489** & 0.0849 & 0.0747 & 0.4397 & 0.1007 & 0.0590 & 0.2281 \\  Linear Mapping & & & & & & & & & \\
**BERT** & 0.0226 & 0.0194 & 0.1240 & 0.0415 & 0.0399 & 0.2362 & 0.0524 & 0.0309 & 0.1245 \\
**RoBERTa** & 0.0247 & 0.0209 & 0.1262 & 0.0406 & 0.0387 & 0.2277 & 0.0578 & 0.0338 & 0.1339 \\
**Llama2-7B** & 0.0662 & 0.0559 & 0.3176 & 0.1027 & 0.0955 & 0.4952 & 0.1249 & 0.0729 & 0.2746 \\
**Mistral-7B** & 0.0650 & 0.0544 & 0.3124 & 0.1039 & 0.0963 & 0.4994 & 0.1270 & 0.0687 & 0.2428 \\
**text-embedding-adv2** & 0.0515 & 0.0436 & 0.2570 & 0.0926 & 0.0874 & 0.4563 & 0.1176 & 0.0683 & 0.2579 \\
**text-embeddings-3-large** & 0.0735 & 0.0608 & 0.3355 & 0.1109 & 0.1023 & 0.5200 & 0.1367 & **0.0793** & **0.2928** \\
**SFR-Embedding-Mistral** & **0.0738** & **0.0610** & 0.3371 & **0.1152** & **0.1065** & **0.5327** & **0.1370** & 0.0787 & 0.2927 \\   

Table 1: The recommendation performance of linear mapping comparing with classical CF baselines.

discussions about this in Appendix C.2.3) . Specifically, we project the language representation \(_{i}\) of the item title to an item space for recommendation with the two-layer MLP, and obtain user representations as the average of historical items:

\[_{i}^{(0)}=_{2}(_{1}_{i}+_{1})+_{2},_{u}^{(0)}=_{u}|}_{i_{u}}_{i}^{(0)}. \]

**Graph convolution.** Graph neural networks (GNNs) have shown superior effectiveness for recommendation [52; 55], owing to the natural user-item graph structure in recommender systems . In AlphaRec, we employ a minimal graph convolution operation  to capture more complicated collaborative signals from high-order connectivity [73; 74; 55; 72] as follows:

\[_{u}^{(k+1)}=_{i_{u}}_ {u}|}_{i}|}}_{i}^{(k)},_{i}^{(k+1) }=_{u_{i}}_{i}|}_{u}|}}_{u}^{(k)}. \]

The information of connected neighbors is aggregated with a symmetric normalization term \(_{u}|}_{i}|}}\). Here \(_{u}\) (\(_{i}\)) denotes the historical item (user) set that user \(u\) (item \(i\)) has interacted with. The features \(_{u}^{(0)}\) and \(_{i}^{(0)}\) projected from the MLP are used as the input of the first layer. After propagating for \(K\) layers, the final representation of a user (item) is obtained as the average of features from each layer:

\[_{u}=_{k=0}^{K}_{u}^{(k)},_ {i}=_{k=0}^{K}_{i}^{(k)}. \]

**Contrastive learning objective.** The introduction of contrasting learning is another key element for the success of leading CF models. Recent research suggests that the contrast learning objective, rather than data augmentation, plays a more significant role in improving recommendation performance [66; 75; 65]. Therefore, we simply use the contrast learning object InfoNCE  as the loss function without any additional data augmentation on the graph [76; 57]. With cosine similarity as the similarity function \(s(_{u},_{i})=_{u}_{i}}{\| _{u}\|\|_{i}\|}\), the InfoNCE loss [56; 76; 77] is written as:

\[_{}=-_{(u,i)^{+}}}{+_{j_{u}}}. \]

Here, \(\) is a hyperparameter called temperature , \(^{+}=\{(u,i)|y_{ui}=1\}\) denoting the observed interactions between users \(\) and items \(\). And \(_{u}\) is a randomly sampled subset of negative items that user \(u\) does not adopt.

### Discussion of Recommendation Paradigms

We compare the language-representation-based AlphaRec with two popular recommendation paradigms in Table 2 (see more discussion about related works in Appendix A).

**ID-based recommendation (ID-Rec) [52; 54].** In the traditional ID-based recommendation paradigm, users and items are represented by ID-based learnable embeddings derived from a large number of user interactions. While ID-Rec exhibits excellent recommendation capabilities with low training and inference costs [62; 76], it also has two significant drawbacks. Firstly, these ID-based embeddings learned in specific domains are difficult to transfer to new domains without overlapping users and items , thereby hindering zero-shot recommendation capabilities. Additionally, there is a substantial gap between ID-Rec and natural languages , which makes ID-based recommenders hard to incorporate language-based user intentions and further refine recommendations accordingly.

**LM-based recommendation (LM-Rec) [15; 16; 24].** Benefitting from the extensive world knowledge and powerful reasoning capabilities of LMs [7; 79], the LM-based recommendation paradigm has gained widespread attention [11; 13]. LM-Rec tends to convert user interaction history into text prompts as input for LMs, utilizing pre-trained or fine-tuned LMs in a text generation pattern to recommend items. LM-Rec demonstrates zero-shot and few-shot abilities and can easily understand language-based user intentions. However, LM-Rec faces significant challenges. Firstly, the LM-based model architecture leads to huge training and inference costs, with real-world deployment difficulties.

Additionally, limited by the text generation paradigm, LM-based models tend to perform candidate selection  or generate a single next item . It remains difficult for LM-Rec to comprehensively rank the entire item corpus or recommend multiple items that align with user interests.

**Language-representation-based recommendation.** We argue that AlphaRec follows a new CF paradigm, which we term the language-representation-based paradigm. This paradigm replaces the ID-based embeddings in ID-Rec with representations from pre-trained LMs, employing feature encoders to map LM representations directly into the recommendation space. Few early studies lie in this paradigm, including using BERT-style LMs to learn universal sequence representations , or adopting the same model architecture as ID-Rec with simple input features replacement . These early explorations, which are mostly based on BERT-style LMs, are usually only applicable in certain specific scenarios, such as the transductive setting with the help of ID-based embeddings . This phenomenon is consistent with our previous findings in Section 2, indicating that BERT-style LMs may fail to effectively encode collaborative signals. We point out that AlphaRec is the first recommender in the language-representation-based paradigm to surpass the traditional ID-based paradigm on multiple tasks, faithfully demonstrating the effectiveness and potential of this paradigm.

## 4 Experiments

In this section, we aim to explore the effectiveness of AlphaRec. Specifically, we are trying to answer the following research questions:

* **RQ1:** How does AlphaRec perform compared with leading ID-based CF methods?
* **RQ2:** Can AlphaRec learn general item representations, and achieve good zero-shot recommendation performance on entirely new datasets?
* **RQ3:** Can AlphaRec capture user intention described in natural language and adjust the recommendation results accordingly?

### General Recommendation Performance (RQ1)

**Motivation.** We aim to explore whether the language-representation-based recommendation paradigm can outperform the ID-Rec paradigm. An excellent performance of AlphaRec would shed light on the research line of building representation-based recommenders in the future.

**Baselines.** We only consider ID-based baselines in this section. We ignore LM-based methods due to two practical difficulties: the huge inference cost on datasets with millions of interactions and the task limitation of candidate selection or next item prediction. In addition to classic baselines (_i.e.,_ MF, MultVAE, and LightGCN) introduced in section 2, we consider two categories of leading ID-based CF baselines: CL-based CF methods: SGL , BC Loss , XSimGCL  and LM-enhanced methods: KAR , RLMRec . See more details about baselines in Appendix C.2.1.

**Results.** Table 3 presents the performance of AlphaRec compared with leading CF baselines. The best-performing methods are bold, while the second-best methods are underlined. Figure 1(a) and Figure 1(b) report the training efficiency and ablation results. We observe that:

* **AlphaRec consistently outperforms leading CF baselines by a large margin across all metrics on all datasets.** AlphaRec shows an improvement ranging from 6.79% to 9.75% on Recall@20 compared to the best baseline RLMRec . We further conduct the ablation study to explore the reason for its success (see more ablation results in Appendix C.2.2). As shown in Figure 1(b), each component in AlphaRec contributes positively. Specifically, the performance degradation caused by replacing the MLP with a linear weight matrix (w/o MLP) indicates that nonlinear transformations can further extract the implicit collaborative signals encoded in the LM representation space.

   Recommendation Paradigms & Training Cost & Zero-shot Ability & Intention-aware Ability \\  ID-based & Low & ✗ & ✗ \\ LLM-based & High & ✔ & ✔ \\ Language-representation-based & Low & ✔ & ✔ \\   

Table 2: Comparison of recommendation paradigmsMoreover, the performance drop from replacing InfoNCE loss  with BPR loss  (w/o CL) and removing the graph convolution (w/o GCN) suggests that explicitly modeling the collaborative relationships through the loss function and model architecture can further enhance recommendation performance. These findings suggest that, by carefully designing the model to extract collaborative signals, the language-representation-based paradigm can surpass the ID-Rec paradigm.
* **The incorporation of semantic LM representations into traditional ID-based CF methods can lead to significant performance improvements.** We note that two LM-enhanced CF methods, KAR and RLMRec, both show improvements over CL-based CF methods. Nevertheless, the combination of ID-based embeddings and LM representations in these methods does not yield higher results than purely language-representation-based AlphaRec. We attribute this phenomenon to the fact that the performance contribution of these methods mainly comes from the LM representations, which is consistent with the previous findings .
* **AlphaRec exhibits fast convergence speed.** We find that the convergence speed of AlphaRec is comparable with, or even surpasses, CL-based methods with data augmentation (_e.g.,_ SGL  and XSimGCL ). Meanwhile, methods based solely on graph convolution (LightGCN ) or CL objective (BC Loss ) show relatively slow convergence speed, indicating that introducing these modules may not lead to convergence speed improvement. Therefore, we attribute the fast convergence speed of AlphaRec to the homomorphic relationship between the LM representation space and a good recommendation space, so only minor adjustments to the LM representations are needed for recommendation.

### Zero-shot Recommendation Performance on Entirely New Datasets (RQ2)

**Motivation.** We aim to explore whether AlphaRec has learned general item representations , which enables it to perform well on entirely new datasets without any user and item overlap.

**Task and datasets.** In zero-shot recommendation , there is not any item or user overlap between the training set and test set , which is different from the research line of cross-domain recommendation in ID-Rec . We jointly train AlphaRec on three source datasets (_i.e.,_ Books, Movies & TV, and Video Games), while testing it on three completely new target datasets (_i.e.,_

    &  &  &  \\  & Recall & NDCG & HR & Recall & NDCG & HR & Recall & NDCG & HR \\  MF (Rendle et al., 2012) & 0.0437 & 0.0391 & 0.2476 & 0.0568 & 0.0519 & 0.3377 & 0.0323 & 0.0195 & 0.0864 \\ MultiVAE (Liang et al., 2018) & 0.0722 & 0.0597 & 0.3418 & 0.0853 & 0.0776 & 0.4434 & 0.0908 & 0.0531 & 0.2211 \\ LightGCN (Hu et al., 2021) & 0.0723 & 0.0608 & 0.3439 & 0.0549 & 0.0747 & 0.4397 & 0.1007 & 0.0530 & 0.2281 \\  SGL (Wu et al., 2021) & 0.0789 & 0.0657 & 0.3734 & 0.0916 & 0.0838 & 0.4680 & 0.1089 & 0.0634 & 0.2449 \\ BC Loss (Zhang et al., 2022) & 0.0915 & 0.0779 & 0.4045 & 0.1039 & 0.0943 & 0.5037 & 0.1145 & 0.0668 & 0.2561 \\ XSimGCL (Yu et al., 2024) & 0.0879 & 0.0745 & 0.3918 & 0.1057 & 0.0984 & 0.5128 & 0.1138 & 0.0662 & 0.2550 \\  KAR (Xi et al., 2023) & 0.0852 & 0.0734 & 0.3834 & 0.1084 & 0.1001 & 0.5134 & 0.1181 & 0.0693 & 0.2571 \\ RLMRec (Ren et al., 2024) & 0.0928 & 0.0774 & 0.0492 & 0.1119 & 0.1013 & 0.5301 & 0.1384 & 0.0809 & 0.2997 \\ 
**AlphaRec** & **0.0991\({}^{*}\)** & **0.0828\({}^{*}\)** & **0.4185\({}^{*}\)** & **0.1221\({}^{*}\)** & **0.1144\({}^{*}\)** & **0.5587\({}^{*}\)** & **0.1519\({}^{*}\)** & **0.0894\({}^{*}\)** & **0.3207\({}^{*}\)** \\  Imp.\% over the best baseline & \(6.79\%\) & \(5.34\%\) & \(2.27\%\) & \(9.12\%\) & \(10.75\%\) & \(5.40\%\) & \(9.75\%\) & \(10.51\%\) & \(7.01\%\) \\   

Table 3: The performance comparison with ID-based CF baselines. The improvement achieved by AlphaRec is significant (\(p\)-value \(<<\) 0.05).

Figure 2: (2a) The bar charts show the number of epochs needed for each model to converge. AlphaRec tends to exhibit an extremely fast convergence speed. (2b) The effect of each component in AlphaRec on Books dataset.

Movielens-1M , Book Crossing , and Industrial ) without further training on these new datasets. (see more details about how we train AlphaRec on multiple datasets in Appendix C.3.1).

**Baselines.** Due to the lack of zero-shot recommenders in the field of general recommendation, we slightly modify two zero-shot methods in the sequential recommendation , ZESRec  and UniSRec , as baselines. We also incorporate two strategy-based CF methods, Random and Pop (see more details about these baselines in Appendix C.3.2).

**Results.** Table 4 presents the zero-shot recommendation performance comparison on entirely new datasets. The best-performing methods are bold and starred, while the second-best methods are underlined. We observe that:

* **AlphaRec demonstrates strong zero-shot recommendation capabilities, comparable to or even surpassing the fully trained LightGCN.** On datasets from completely different platforms (_e.g.,_ MovieLens-1M and Book Crossing), AlphaRec is comparable with the fully trained LightGCN. On the same Amazon platform dataset, Industrial, AlphaRec even surpasses LightGCN, which we attribute to the possibility that AlphaRec implicitly learns unique user behavioral patterns on the Amazon platform . Conversely, ZESRec and UniSRec exhibit a marked performance decrement compared with AlphaRec. We attribute this phenomenon to two aspects. On the one hand, BERT-style LMs  used in these works may not have effectively encoded collaborative signals, which is consistent with our findings in Section 2. On the other hand, components designed for the next item prediction task in sequential recommendation  may not be suitable for capturing the general preferences of users in CF scenarios.
* **The zero-shot recommendation capability of AlphaRec generally benefits from an increased amount of training data, without harming the performance on source datasets.** As illustrated in Figure 8, the zero-shot performance of AlphaRec, when trained on a mixed dataset, is generally superior to training on one single dataset . Additionally, we also note that training data with themes similar to the target domain contributes more to the zero-shot performance. For instance, the zero-shot capability on MovieLens-1M may primarily stem from Movies & TV. Furthermore, we discover that AlphaRec, when trained jointly on multiple datasets, hardly experiences a performance decline on each source dataset. These findings further point to the general recommendation capability of a single pre-trained AlphaRec across multiple datasets. The above findings also offer a potential research path to achieve general recommendation capabilities, by incorporating more training data with more themes. See more details about these results in Appendix C.3.3.

### User Intention Capture Performance (RQ3)

**Motivation.** We aim to investigate whether a straightforward paradigm shift enables pre-trained AlphaRec to perceive text-based user intentions and refine recommendations.

**Task and datasets.** We test the user intention capture ability of AlphaRec on MovieLens-1M and Video Games. In the test set, only one target item remains for each user , with one intention query generated by ChatGPT  (see the details about how to generate and check these intention queries in Appendix C.4.1). In the training stage, we follow the same procedure as illustrated in Section 2 to train AlphaRec. In the inference stage, we obtain the LM representation \(_{u}^{Intention}\) for each user intention query and combine it with the original user representation to get a new user representation as \(}_{u}^{(0)}=(1-)_{u}^{(0)}+_{ u}^{Intention}\). This new user representation is sent into the freezed AlphaRec for recommendation. We report a relatively small \(K\) = 5 for all metrics to better reflect the intention capture accuracy.

    & & &  &  &  \\  & & & Recall & NDCG & HR & Recall & NDCG & HR & Recall & NDCG & HR \\   & MF (Rendle et al., 2012) & 0.0344 & 0.0225 & 0.0521 & 0.1855 & 0.3765 & 0.9634 & 0.0316 & 0.0317 & 0.2382 \\  & MultiVAE (Liang et al., 2018) & 0.0751 & 0.0459 & 0.1125 & 0.2039 & 0.3741 & 0.9740 & 0.0736 & 0.0634 & 0.3716 \\  & LightGCN (He et al., 2021) & 0.0785 & 0.0533 & 0.1078 & 0.2019 & 0.4017 & 0.9715 & 0.0630 & 0.0588 & 0.3475 \\   & Random & 0.0148 & 0.0061 & 0.0248 & 0.0068 & 0.0185 & 0.2611 & 0.0039 & 0.0036 & 0.0443 \\  & Pop & 0.0216 & 0.0087 & 0.0396 & 0.0253 & 0.0679 & 0.5439 & 0.0119 & 0.0101 & 0.1157 \\  & ZESRec (Digi et al., 2021) & 0.0326 & 0.0272 & 0.0628 & 0.0274 & 0.0787 & 0.5786 & 0.0155 & 0.0143 & 0.1347 \\  & UniSRec (He et al., 2022) & 0.0452 & 0.0350 & 0.0862 & 0.0278 & 0.1412 & 0.7135 & 0.0396 & 0.0332 & 0.22454 \\  & **AlphaRec** & **0.0913\({}^{}\)** & **0.0873\({}^{}\)** & **0.1277\({}^{}\)** & **0.1486\({}^{}\)** & **0.3215\({}^{}\)** & **0.9296\({}^{}\)** & **0.0660\({}^{}\)** & **0.0545\({}^{}\)** & **0.381\({}^{}\)** \\   & \(157.09\%\) & \(127.69\%\) & \(30.29\%\) & \(66.67\%\) & \(64.16\%\) & \(37.78\%\) & \(101.55\%\) & \(63.71\%\) & \(47.97\%\) \\   

Table 4: The zero-shot recommendation performance comparison on entirely new datasets. The improvement achieved by AlphaRec is significant (\(p\)-value \(<<\) 0.05).

**User intention capture results.** Table 5 represents the user intention capture experiment results, compared with the baseline TEM . Clearly, the introduction of user intention (w Intention) significantly refines the recommendations of the pre-trained AlphaRec (w/o Intention). Moreover, AlphaRec outperforms the baseline model TEM by a large margin, even without additional training on search tasks. We further conduct a case study on MovieLens-1M to demonstrate how AlphaRec captures the user (see more case study results in Appendix C.4.3). As shown in Figure 2(a), AlphaRec accurately captures the hidden user intention for "Godfather", while keeping most of the recommendation results unchanged. This indicates that AlphaRec captures the user intention and historical interests simultaneously.

**Effect of the intention strength \(\).** By controlling the value of \(\), AlphaRec can provide better recommendation results, with a balance between user historical interests and user intent capture. Figure 2(b) depicts the effect of \(\). Initially, as \(\) increases, the recommendation performance rises accordingly, indicating that incorporating user intention enables AlphaRec to provide better recommendation results. However, as the \(\) approaches 1, the recommendation performance starts to decrease, which suggests that the user historical interests learned by AlphaRec also play a vital role. The similar effect of \(\) on Video Games is discussed in Appendix C.4.4.

## 5 Limitations

There are several limitations not addressed in this paper. On the one hand, although we have demonstrated the excellence of AlphaRec for multiple tasks on various offline datasets, the effectiveness of online employment remains unclear. On the other hand, although we have successfully explored the potential of language-representation-based recommenders by incorporating essential components in leading CF models, we do not elaboratively focus on designing new components for CF models.

## 6 Conclusion

In this paper, we explored what knowledge about recommendations has been encoded in the LM representation space. Specifically, we found that the advanced LMs representation space exhibits a homomorphic relationship with an effective recommendation space. Based on this finding, we developed a simple yet effective CF model called AlphaRec, which exhibits good recommendation performance with zero-shot recommendation and user intent capture ability. We pointed out that AlphaRec follows a new recommendation paradigm, language-representation-based recommendation, which uses language representations from LMs to represent users and items and completely abandons ID-based embeddings. We believed that AlphaRec is an important stepping stone towards building general recommenders in the future.1

Figure 3: User intention capture experiments on MovieLens-1M. (2(a)) AlphaRec refines the recommendations according to language-based user intention. (2(b)) The effect of user intention strength \(\).

    &  &  \\  & HR05 & NDCG05 & HR05 & NDCG05 \\  TEM (Bi et al., 2020) & 0.2738 & 0.1973 & 0.2212 & 0.1425 \\ AlphaRec (w/o Intention) & 0.0793 & 0.0498 & 0.0663 & 0.0438 \\ AlphaRec (w Intention) & **0.4704*** & **0.3738*** & **0.2569*** & **0.1862*** \\   

Table 5: The performance comparison in user intention capture.