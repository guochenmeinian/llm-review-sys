# Token-Scaled Logit Distillation for

Ternary Weight Generative Language Models

Minsoo Kim\({}^{1}\) Sihwa Lee\({}^{1}\) Janghwan Lee\({}^{1}\)

**Sukjin Hong\({}^{1,2}\) Du-Seong Chang\({}^{2}\) Wonyong Sung\({}^{3}\) Jungwook Choi\({}^{1}\)1 \({}^{1}\)Hanyang University, Seoul, Republic of Korea \({}^{2}\)KT, Seoul, Republic of Korea \({}^{3}\)Seoul National University, Seoul, Republic of Korea {minsoo2333, macto94, hwaniio288, choij}@hanyang.ac.kr {sukjin.hong, dschang}@kt.com, wysung@snu.ac.kr**

Corresponding Author

###### Abstract

Generative Language Models (GLMs) have shown impressive performance in tasks such as text generation, understanding, and reasoning. However, the large model size poses challenges for practical deployment. To solve this problem, Quantization-Aware Training (QAT) has become increasingly popular. However, current QAT methods for generative models have resulted in a noticeable loss of accuracy. To counteract this issue, we propose a novel knowledge distillation method specifically designed for GLMs. Our method, called token-scaled logit distillation, prevents overfitting and provides superior learning from the teacher model and ground truth. This research marks the first evaluation of ternary weight quantization-aware training of large-scale GLMs with less than 1.0 degradation in perplexity and achieves enhanced accuracy in tasks like common-sense QA and arithmetic reasoning as well as natural language understanding.2

## 1 Introduction

Generative language models (GLMs) have made impressive strides in text generation, understanding, and reasoning, attracting significant attention in the field . However, deploying GLMs remains a challenge due to their enormous model sizes. There is rising interest in practical GLMs with less than 10 billion parameters. Their capability can be improved through instruction fine-tuning . For instance, Alpaca  showed that a fine-tuned 7 billion-parameter model can match the text generation performance of a 175 billion parameter GLM, highlighting the potential of smaller, more manageable models.

Since practical GLMs still contain billions of parameters, there is extensive research into model compression techniques for their efficient deployment. One such method is post-training quantization (PTQ), which simplifies the process by reducing bit-precision to 8 or 4 bits without the need for fine-tuning pre-trained GLMs . This approach has gained traction due to its straightforward and fast processing time. However, it's been observed that these techniques cause a significant decrease in accuracy when the parameter count drops below 10 billion or when the bit-precision falls under 4 bits. As a result, there's a clear need for a more reliable quantization approach for GLMs with sub-4bit precision.

In response, we propose an alternative method, quantization-aware training (QAT), to address the issues PTQ poses for fine-tuned GLMs. QAT is a prevalent quantization technique that counteractsaccuracy loss and attains a high compression rate for efficient deployment . Notably, successful fine-tuning of sub-4bit natural language understanding models has been achieved through layer-to-layer (L2L) knowledge distillation (KD), a method used to offset errors resulting from aggressive quantization, such as binary or ternary weights [19; 20; 21; 22; 23]. However, applying QAT to GLM has limited success. While  introduced a token-level contrastive loss and  offered initial insights into the challenges of quantizing GLMs, both studies encountered substantial increases in perplexity in language modeling. Furthermore, no existing studies apply QAT to GLMs with billions of parameters, primarily due to the expensive nature of training with KD.

This study delves into the fundamental challenges of applying QAT to fine-tuned GLMs. We identify two main issues. First, the structure of a self-attention map in masked self-attention causes cumulative quantization errors across tokens, which conventional L2L KD struggles to compensate for. Second, the teacher-forcing mechanism  used in fine-tuning Transformer decoder necessitates ground-truth loss (GT Loss) - a factor largely overlooked in previous QAT methods - but including GT Loss risks overfitting. Our investigation reveals that logit distillation can overcome the limitations of L2L KD in token prediction recovery by reforming intermediate representations. Additionally, we found that applying token-wise logit scaling can significantly mitigate the risk of overfitting.

Drawing from our findings, we introduce a novel KD technique known as Token-Scaled Logit Distillation (TSLD), designed to enhance QAT for ternary quantization inference. We evaluate TSLD across a range of GLMs - originating from GPT-2 , OPT  and LLaMA  - of various sizes, including 7 billion models for the first time. The results show that TSLD achieves comparable, if not superior, performance in language modeling on ternary and 4-bit inference. When TSLD is applied to reasoning tasks, it surprisingly prevents overfitting to achieve task accuracy that is at least on par, if not better. These remarkable outcomes underline the potential of our proposed TSLD method in facilitating the deployment of ultra-low precision GLMs.

## 2 Related Work

**Fine-tuning for Generative Language Model.** GLMs are renowned for their unparalleled text generation, comprehension, and reasoning capabilities [1; 2; 3; 4; 5; 6; 7]. Studies reveal that their performance can be enhanced through instruction fine-tuning methods like Prefix-Tuning  or using natural language instructions and examples . Fascinatingly, instruction-tuned models, including smaller ones, can outperform larger counterparts in specific tasks [9; 12; 8]. However, the vast parameter count in these models, compared to popular models such as BERT , can restrict their practical utility. To mitigate this, we suggest investigating efficient, lightweight techniques for GLMs encompassing up to 7 billion parameters.

**Quantization for Generative Language Model.** Quantization, a method that minimizes the inference cost of large models by utilizing a limited number of bits to represent weights, has been recently applied to GLMs [13; 14; 15; 16; 17]. This process has considerably cut down GPU memory usage and execution time. Two main types of quantization exist, quantization-aware training (QAT) and post-training quantization (PTQ), which differ in their requirement for re-training or fine-tuning. Although QAT has been effectively used in Transformer encoder models [22; 25], its application to GLMs poses challenges , with observed performance declines when applied to decoder-only models like GLMs . This paper assesses the imbalance of quantization errors based on attention traits and language model generation patterns. We demonstrate that QAT can be conducted without substantial performance loss across various tasks, even in models exceeding one billion parameters.

**Knowledge Distillation for Language Model Compression.** Knowledge distillation (KD) is a prevalent transfer learning framework that imparts knowledge from a larger "teacher" model to a smaller "student" model, and it is effectively used to curb accuracy decline in models compressed through quantization-aware training (QAT) [28; 19; 21; 22; 23]. In encoder models, KD trains the quantized model (student) using the full-precision model's (teacher's) intermediate representation as the objective in QAT. Despite its effectiveness, this method requires more memory due to the intermediate representation from the teacher model and has been less explored in decoder models such as GLM [25; 24]. This paper introduces a novel, less memory-intensive KD method applicable to models with up to 7 billion parameters. We offer a thorough analysis of the teacher's information transfer in the decoder model and suggest a QAT-based KD method that retains minimal performance degradation, even when applying ternary weights to various GLMs.

Background and Challenges

### Transformer Layer

Generative language models  are built with Transformer layers . A standard Transformer layer includes two main sub-modules: Multi-Head Attention (MHA) and Feed-Forward Network (FFN). Input to the \(l\)-th Transformer layer is \(_{l}^{n d}\) where \(n\) and \(d\) are the sequence length and hidden state size, respectively. Let \(N_{H}\) be the number of attention heads and \(d_{h}=d/N_{H}\). \(_{h}^{Q},_{h}^{K},_{h}^{V}^{d d _{h}}\) are the weight parameters projecting \(_{l}\) into Query (\(_{h}=_{l}_{h}^{Q}\)), Key (\(_{h}=_{l}_{h}^{K}\)), and Value (\(_{h}=_{l}_{h}^{V}\)), respectively. The attention score (\(_{h}\)) is computed with the dot product of the projected Query and Key (\(_{h}=_{h}_{h}^{}\)). The normalized version of this result is then passed through the softmax function and multiplied by the Value to get the output as \(_{h}=(_{h}\ /})_{h}\). Then, the output of the Multi-Head Attention (MHA) is defined as follows:

\[(_{l})=(_{1},...,_{N_{H }})^{O}. \]

FFN consists of two fully-connected layers with weight parameters \(^{1}\) and \(^{2}\):

\[(_{l})=(_{l}^{1}+b^{1}) ^{2}+b^{2}. \]

Therefore, the operations at the \(l\)-th Transformer layer can be defined as:

\[_{l}=_{l}+((_{l})); _{l+1}=_{l}+((_{l })). \]

### QAT with KD for Transformer Decoders

QAT emulates inference-time quantization during training to learn parameters robust to the quantization error. In particular, ternary quantization represents all the weight parameters (\(^{Q}\), \(^{K}\), \(^{V}\), \(^{O}\), \(^{1}\), \(^{2}\)) into ternary values \(\{+1,0,-1\}^{k}\) along with a scale factor \(\) for sub-2bit inference at deployment. In this work, we follow the approach of TWN  that analytically estimates the optimal \(\) and \(\) to minimize \(\|-\|_{2}^{2}\), where \(=()\) and \(k\) is the number of elements of the weight parameters.

Due to aggressive bit-reduction, ternary quantization causes significant accuracy loss. KD can help compensate for accuracy degradation, where the original full-precision model works as a teacher to guide the training of the quantized model as a student. In case of Transformer models, prior works [19; 21; 22; 31] applied KD on every Transformer layer's output activation \(_{l}\) as well as attention scores \(_{l}\) with mean squared error (MSE) loss, denoted as \(L_{L2L}\):

\[L_{L2L}=_{l=1}^{L+1}(_{l}^{S},_{l}^{T})+ _{l=1}^{L}(_{l}^{S},_{l}^{T}), \]

where superscripts \(S\) and \(T\) represent the student and teacher models, respectively.

The final output logits of the student (\(^{S}\)) and the teacher (\(^{T}\)) are used to compute the cross-entropy loss. Given that \(N\) is the total number of the tokens in input sequences, and \(V\) is the vocabulary size the language model can recognize and generate. Using the softmax function, we can convert each model's \(i_{th}\) token prediction logit output into probability distributions, which are then utilized for the loss for logit distillation(\(L_{logit}\)):

\[_{i}=_{i,j}}}{_{j=1}^{V}e^{_{i,j}}}, L_{logit}=-_{n=1}^{N}_{i=1}^{V}_{n,i}^{ T}(_{n,i}^{S}). \]

The overall loss for KD is generally computed as \(L_{KD}=L_{L2L}+L_{logit}\), without GT Loss as noted in previous studies [19; 22; 31]. Yet, some methods utilize only \(L_{logit}\). Our study underscores the necessity of integrating \(L_{logit}\) and the GT Loss for an effective application of QAT in Transformer decoders.

### Quantization Challenges on GLMs

In this section, we compare the computations of Transformer encoders and decoders to deepen our understanding of the fresh challenges that surface within the realm of GLMs.

**Cumulative Quantization Errors in Causal Attention.** Causal attention, which integrates masking into self-attention to avoid referencing future tokens, is vital for causal language modeling tasks. To comprehend the quantization characteristics of GLMs, we contrast the computation procedures of the self-attention map. For a Transformer encoder, the quantization error reflected on the self-attention map due to the process of projecting Query, Key, and Value is evenly spread across tokens because of the token-parallel nature of computing inherent in Transformer encoders. However, the mask in the causal attention accumulates quantization errors from each token, creating uneven errors in the output computation.

Fig. 1(a) contrasts the attention mechanism of the encoder and decoder model under quantization. The encoder's self-attention is illustrated at the top of the figure, decomposed into self-attention and quantization error (\(Q_{err}\)) components. The combined attention probabilities are utilized in a weighted sum with the Value, where tokens 1 and 3 (in a blue and red box respectively) are affected by an identical number of attention probabilities with quantization error. Conversely, the decoder's causal attention, shown at the bottom, uses only the attention probabilities of the current token and its preceding ones. For instance, the Value for token 1 in the bottom of Fig. 1(a) (in a blue box) uses only two attention probabilities affected by quantization error, while token 3 (in a red box) includes those from all preceding tokens. This illustration highlights that causal attention inherently leads to a disproportionate accumulation of quantization errors in the latter tokens. Thus, we need a decoder QAT strategy that addresses this imbalance within the causal attention module. In Section 4.1, we assess the limitations of current KD methods in managing cumulative quantization errors and introduce an enhanced logit distillation error compensation mechanism.

**Necessity of Ground Truth Loss.** In fine-tuning, encoder models, often used in Natural Language Understanding (NLU), and decoder models, common in Natural Language Generation (NLG), employ distinct mechanisms for receiving GT Loss, as shown in Fig.1(b). Encoder models for NLU tasks use a single special token to compute cross-entropy loss with a limited number of classes , as depicted on the left of Fig.1(b). On the other hand, decoder models in NLG tasks predict each subsequent token, transforming each token's representation into a logit vector with a class size equivalent to the vocabulary size, often exceeding 50k , shown on the right of Fig. 1(b). This process allows decoder models to obtain GT Loss for each input token, providing detailed token-level prediction information. Given these differences, there is a compelling need to consider the necessity of GT Loss in the decoder model's QAT in a token-wise manner. However, previous QAT  on the decoder models neglects the consideration of GT Loss due to a perceived degradation in performance when GT Loss is utilized. Accordingly, Section 4.2 offers an in-depth analysis of the interplay between KD and GT Loss during the QAT.

Figure 1: (a) Illustration of attention mechanism in the encoder (top) and decoder models (bottom). (b) Left: performing NLU task  by encoder model. Right: performing language modeling task by decoder model with teacher-forcing (input token is independent of the previously generated token)

## 4 Method

### Logit Distillation for Cumulative Quantization Error

**Motivation.** The inherent nature of causal attention, where each token representation builds upon the representation of the previous tokens, presents previously unseen challenges when applying quantization to decoder models. For a clearer understanding of the decoder model, we conduct a comparative analysis with the encoder model to examine the impact of quantization error on the model. In Fig. 2 (a), the quantization error of the encoder self-attention map exhibits a widespread presence of errors due to the absence of masking in self-attention, and the per-token quantization errors along the layers also show irregular patterns depending on the token index. However, in Fig. 2 (b), the heat map of the decoder model reveals an increasing brightness of quantization errors as we move toward the later tokens. When examining the token index, the phenomenon of quantization errors accumulating toward the later tokens becomes even more pronounced. This previously unconsidered phenomenon of token quantization error accumulation in the decoder model is a crucial feature to consider in GLM QAT. Reflecting on this feature, we analyze the effectiveness of prior KD methods for language modeling and explore suitable KD approaches for the decoder model. Analysis on cumulative quantization error for a wider variety of GLMs can be found in Appendix A.3.

**Comparison of KD Methods for Decoder QAT.** Building on a deeper comprehension of the decoder model, we evaluate the efficiency of current KD methods for QAT in decoders and propose an enhanced KD approach informed by our decoder model analysis. We analyze how two different KD methods, Layer-to-Layer distillation (L2L KD) and logit distillation (Logit KD), tackle systematic outliers in QAT , using the min-max dynamic range per token and per channel of each layer's intermediate output. As shown in Fig. 2(c) left, both KD methods demonstrate distinct strategies in addressing the teacher model's systematic outliers. While L2L distillation guides the QAT process to mirror the outliers of the teacher model, Logit KD deviates from this pattern, generating new outliers not seen in the teacher model. These outliers consistently emerge in specific channel indices where the teacher model's outliers are present. Additionally, to compare the relative token attending order within each QAT model's self-attention map, we employ a ranking ratio comparison method . This technique conveys the average relative importance of a single token within each attention map. As depicted in Fig. 2(c) middle, the L2L KD method closely mirrors the teacher model's ranking changes. However, the Logit KD method exhibits substantial variation in this ranking shift within a certain head range.

Figure 2: Comparison of weight quantization error on attention map through MSE loss in (a) encoder (BERT-base, RTE task) and (b) decoder (GPT-2, PTB task) model. (c) Left: min-max dynamic range per layer (Logit Distill vs L2L Distill). Middle: attention ranking ratio comparison (FP vs Logit Distill vs L2L Distill). Right: per layer token-wise logit distance and MSE distance

**Logit Distillation for Token-wise Prediction Recovery.** We further analyze the QAT model's token logit distributions. Since token representations evolve along the layers to form the next token's probability , we assess each layer's logit distribution and the logit distance from the teacher model. As depicted in Fig. 2(c), L2L KD creates a token representation that closely mirrors the teacher model in both logit distribution and mean-squared error (MSE) distance during mid-layer stages but fails to match the final logit distribution. Conversely, Logit KD, despite diverging from the teacher model's logit distribution in the middle layers, accurately reproduces the final logit distribution. These observations highlight Logit KD's distinct mechanism for token-wise prediction recovery, managing cumulative quantization error in decoder models. In intermediate layers, Logit KD varies the attention values across channels as shown in Fig.2(c), leading to a diverging token representation from the FP model, with this middle stage adjustment acting to counteract accumulated quantization errors in later tokens. Consequently, Logit KD aligns the final logit distribution for each token, crucial for the accuracy of causal language modeling. Therefore, Logit KD, aligning with the characteristics of the decoder model, stands out as a natural choice for QAT. The subsequent section will delve into previously unexamined issues encountered by Logit KD in decoder QAT.

### Token-Scaled Logit Distillation for Avoiding Overfitting with GT Loss

**Motivation.** This section tackles the overfitting problem arising from the combination of Logit KD and GT Loss during QAT. We also investigate the probabilistic behavior displayed by the decoder model in language modeling tasks. The study by  highlights instances where employing GT Loss and Logit KD adversely impacts the performance of decoder QAT. To understand this issue better, we conduct tests using Logit KD both independently and combine with ground truth loss. As depicted in Fig. 3 (a), overfitting is observed in the QAT when both ground truth loss and Logit KD are applied.

**Understanding Causes of Overfitting.** To better understand the causes of overfitting, we analyze the logit output for each token that the teacher model generates during language modeling. From the logit information by the teacher model, we derive the probability distribution (\(_{i}^{T}=(_{i}^{T})\)) for \(i_{th}\) token prediction. Based on this distribution, we further calculate cross-entropy (\(-y_{n,i}(_{n,i}^{T})\)), confidence score (\((_{i}^{T})\)) and entropy (\(-_{i}_{i}^{T}(_{i}^{T})\)) for each token prediction. These

Figure 3: (a) Training/evaluation loss curve with different QAT-KD methods. (b) token-wise prediction statistics scatter plot (left: cross entropy loss and token confidence, right: cross entropy loss and token entropy). (c) Impact of TSLD: per-token scale and cross-entropy loss (left: per-token scale, Right: per-token cross-entropy loss). Analysis utilizes OPT-125m for PTB task language modeling. See Appendix A.2 for further analysis on other GLMsmetrics unveil a confidence disparity in language modeling--a trend uniformly observed across decoder models of varying scales. The cross-entropy loss of token prediction logits plotted against the probability confidence score, as illustrated in Fig. 3 (b), distinctly demarcates the _Low Confidence Region_ (blue box) with low probability confidence and high cross-entropy loss from the _High Confidence Region_ (yellow box) with high probability confidence and low cross-entropy loss. This observation implies a potential overlap between high-confidence Logit KD and the role of GT Loss cross-entropy, suggesting that redundant information from high-confidence Logit KD might mirror the effects of ground truth loss, thereby contributing to the overfitting observed.

**Token-Scaled Logit Distillation (TSLD).** Based on investigations into the probabilistic relation of token predictions and overfitting in QAT, we propose an adaptive KD method that adjusts Logit KD based on token confidence. This approach utilizes the phenomenon of confidence disparity in token predictions from the teacher model. Our method, called Token-Scaled Logit Distillation (TSLD), de-emphasizes Logit KD for high-confidence tokens to prevent overfitting in QAT while emphasizing Logit KD for low-confidence tokens possessing with a high entropy probability distributions. Specifically, low-scaled Logit KD (high confidence, low entropy) effectively reduces the overlap with GT Loss, leading to an improvement in overfitting. On the other hand, high-scaled logit KD (low confidence, high entropy) emphasizes the distillation of more informative token prediction distribution from the teacher model, which has rich soft label information.

\[CE_{n}^{T}=-_{i=1}^{V}y_{n,i}(_{n,i}^{T}), _{n}=_{n}^{T}/}}{_{k=1}^{N}e^{_{k}^{T}/ }} \]

\[L_{TSLD}=_{n=1}^{N}(_{n}-_{i=1}^{V}_ {n,i}^{T}(_{n,i}^{S})) \]

The implementation of TSLD is straightforward. By considering the relationship between token confidence and token prediction cross entropy loss in Fig 3(b), we can determine the \(n_{th}\) token scale values (scale\({}_{n}\)) based on the cross entropy loss of the teacher model (\(CE_{n}^{T}\)) using softmax function as shown in Eq. 6. Note that \(y_{n,i}\) is a ground truth label where \(y_{n,i}=1\) if token \(i\) is the true next token at position \(n\) and \(\) is the temperature parameter for softmax function. The scale of the each token (scale\({}_{n}\)) is then applied in the logit distillation by multiplying it with the cross entropy loss between the student and teacher models as shown in Eq. 7. As depicted in Fig. 3(c) left, the scale values for token-specific Logit KD are determined adaptively based on the per-token cross-entropy loss of the teacher model. In the right graph of Fig. 3(c), we can observe that tokens with higher cross entropy loss in the teacher model correspond to higher scale values in the per token scale graph, compared to Logit KD method applying the same scale value (\(1/N\)) across all tokens as shown in the left graph of Fig. 3(c).

TSLD brings about two significant effects by applying different scales based on confidence disparity, with negligible computational overhead. As shown in Fig. 3(a), TSLD de-emphasizes Logit KD for high-confidence tokens, thereby preventing overfitting. Conversely, for low-confidence tokens possessing a high entropy probability distribution, TSLD emphasizes the Logit KD. This action allows the student model to more closely mimic the teacher model's cross-entropy loss as seen in the right graph of Fig. 3(c). A detailed analysis of the TSLD method's computational cost can be found in Appendix A.1.

## 5 Experiments

### Experimental Settings

In this section, we evaluate the effectiveness of TSLD in the QAT of various sizes of decoder models with sub-4bit quantization. We've set up comparative experiments to demonstrate the proficiency of our TSLD method against existing PTQ and other QAT KD methods. Our findings illustrate that TSLD substantially enhances both the language modeling performance (measured by Perplexity or PPL) and the accuracy in tasks related to reasoning (common-sense QA and arithmetic) and natural language understanding.

**Task and Models**. We evaluate our proposed method for language modeling (PTB ), commonsense QA tasks (PIQA , OpenbookQA , ARC_easy , ARC_challenge  and arithmetic reasoning based text-generation task (GSM8K ). Additionally, our assessment extends to Natural Language Understanding (NLU) task (GLUE ), ensuring a comprehensive analysis. Our benchmark models encompass widely used GLMs, such as GPT-2 , OPT , GPT-Neo  and LLaMA  with various sizes ranging from 0.1B to 7B parameters.

**Fine-Tuning Settings**. In fine-tuning the language modeling task, we employ a chunk-based pre-processing method: all training datasets are concatenated, then split into shorter chunks defined by input sequence length. For reasoning task fine-tuning, we utilize a sentence-based approach, concatenating each dataset's question and answer parts to form new sentences, individually used as the fine-tuning dataset. Detailed hyper-parameter settings and other specifics are in Appendix C.2. Experiments are conducted on an A100-40GB GPU. Our QAT experiments start with models that have undergone task-specific fine-tuning. During quantization, the KD process employs the FP fine-tuned model as the teacher model, while the quantized model acts as the student.

**Implementation Settings**. We devise a QAT-KD framework that leverages pipeline parallelism with PyTorch Pipe API enabling the training of models with capacities exceeding 1.3 billion. We apply weight quantization to the matrix multiplication layers in each decoder layer of the GLM. We conduct experiments on L2L KD  but encounter out-of-memory problems for models with more than 1.3B parameters in A100-40GB GPU. This issue is believed to arise due to the requirement for both the teacher and student models to store the outputs generated by all of their respective intermediate layers during the knowledge distillation process. GPU memory consumption comparisons for each QAT-KD method can be found in the Appendix A1.

### Evaluation on Language Modeling Task

Table 1 outlines the performance comparison of TSLD with leading PTQ and QAT methods  for language modeling of PTB dataset. For 4-bit weight quantization, OPTQ sees a notable performance drop in GPT-2 and OPT models up to 6.7 billion parameters, aligning with the original paper's observations . However, QAT methods show lower perplexity due to weight parameters fine-tuned for robust reduced-precision inference. QuantGPT , which exclusively uses Logit KD achieves impressive perplexity, whereas Logit+GT KD sees degradation. Conversely, TSLD offers

    & Quantization & Optimization &  &  \\  & Method & Method & 0.1B & 0.3B & 0.8B & 1.5B & 0.1B & 1.3B & 2.7B & 6.7B \\   & & 20.91 & 18.21 & 15.20 & 14.26 & 18.17 & 13.75 & 11.43 & 10.21 \\   &  & PTQ & OPTQ  & 22.41 & 19.35 & 17.26 & 15.86 & 19.75 & 14.30 & 11.82 & 11.73 \\   & &  & Logit  & 20.98 & 18.54 & 16.79 & 15.42 & 17.60 & **13.73** & 11.82 & 11.20 \\  & & & Logit+GT & 21.51 & 18.58 & 15.49 & 14.89 & 19.63 & 15.03 & 12.58 & 11.78 \\  & & & TSLD & **19.95** & **17.53** & **15.32** & **14.50** & **17.45** & 13.90 & **11.59** & **11.00** \\   &  & 1.2L+Logit  & 23.79 & 21.21 & 17.80 & 15.82 & 20.47 & 17.62 & 14.67 & 11.75 \\  & & & Logit  & 22.84 & 19.87 & 16.46 & 15.27 & 18.86 & 14.80 & 12.26 & 11.33 \\  & & & Logit-GT & 23.80 & 20.20 & 17.77 & 16.52 & 21.62 & 16.41 & 13.20 & 12.41 \\  & & & TSLD & **21.74** & **18.57** & **16.14** & **15.02** & **18.58** & **14.60** & **11.97** & **11.17** \\   

Table 1: Perplexity comparison in GPT-2 and OPT series across various model sizes (0.1B to 6.7B) on the PTB dataset with QAT-KD (tensor-wise) and PTQ (channel-wise) quantization methods

    &  &  &  &  &  \\   &  &  &  &  &  &  &  &  \\  OPT-2.7B FP16 & 76.71 & 10.91 & 49.60 & 26.16 & 66.12 & 7.41 & 37.20 & 8.96 & 20.39 & 2.07 \\  Logit  & 74.32 & 11.69 & 45.40 & 29.41 & 58.92 & 9.05 & 31.91 & 12.38 & 20.02 & **2.03** \\ GT+Logit & 74.97 & 12.10 & 46.20 & 31.08 & 58.84 & 8.66 & 32.16 & 12.04 & 19.56 & 2.12 \\ TSLD & **75.62** & **11.35** & **46.81** & **28.93** & **59.39** & **8.12** & **33.45** & **11.05** & **20.24** & **2.03** \\   &  &  &  \\   &  &  &  &  &  \\   &  &  &  &  &  &  &  \\  Logit  & 21.01 & 21.08 & **1.93** & 12.22 & 25.47 & **1.52** \\ TSLD & **19.27** & **24.49** & 2.14 & **11.60** & **26.23** & **1.52** \\   

Table 2: Top: Results for the OPT-2.7B model fine-tuned on common-sense QA and arithmetic reasoning task using various QAT-KD (tensor-wise) methods. Bottom: QAT-KD (channel-wise) results on language modeling task and arithmetic reasoning task across various GLM models.

the lowest perplexity, underlining token-wise scaling method's effectiveness in incorporating GT knowledge. Remarkably, TSLD's performance boost allows QAT models to match full-precision performance across various capacity ranges in all decoder models.

For 2-bit weight quantization, L2L KD sees significant accuracy degradation, and Logit+GT KD suffers from overfitting. TSLD outperforms Logit KD  across all model sizes, maintaining PPL degradation of no more than 1.0 from the baseline. We also tested the general applicability of TLSD on popular open-sourced GLM models (GPT-Neo-1.3B , LLaMA-7B ) in language modeling PTB task. Table 2-below indicates that TSLD consistently surpassed the competitor, Logit KD . Notably, 2-bit TSLD utilizes simple ternary weight quantization, which is hardware-friendly.

### Evaluation on Reasoning Task

We assess the effectiveness of our proposed method in commonsense QA (PIQA, OpenbookQA, ARC_easy, ARC_challenge) and reasoning-based text-generation task (GSM8K) employing the LM Evaluation Harness framework from EleutherAI . Given the capacity requirements for reasoning tasks, we use OPT-2.7B/6.7B and LLaMA-7B models as baselines, rather than smaller models. Table 2 presents a performance comparison of 2-bit weight quantization with different KD methods. In commonsense QA tasks, TSLD consistently showcased the lowest perplexity and, consequently, the highest accuracy, drawing the 2-bit quantization results even closer to FP performance as shown in Table 2-top.

Considering the GSM8K task, Table 2 reveals that TSLD outperformed Logit KD in terms of perplexity and accuracy with OPT-2.7B/6.7B and LLaMA models. Notably, while QuantGPT (Logit KD) achieves comparable or better perplexity, its reasoning task accuracy is lower, potentially due to insufficient GT information. Conversely, TSLD achieves excellent reasoning accuracy while maintaining competitive perplexity, underscoring TSLD's ability to balance language modeling and reasoning performance through its token-wise scaling to avoid overfitting. The generated text sample results of the GSM8K task are provided in the Appendix D.

### Evaluation on Language Understanding Task

We fine-tune the decoder model for Natural Language Understanding (NLU) tasks using a language modeling approach as illustrated in Fig. 1(b). In our experiments outlined in Table 3, we compare the performance of the latest PTQ methods (AWQ , OPTQ ) and QAT-KD methods with OPT-1.3B model. For 4-bit quantization, the PTQ technique shows a noticeable degradation in performance compared to QAT results, excluding SST-2 task. TSLD achieves the lowest perplexity and the highest accuracy across all the experiments except SST-2, where its accuracy is in-par with the full-precision case. These findings demonstrate that TSLD can robustify the performance of 4-bit quantized GLMs for various NLU tasks, while 4-bit PTQ may suffer from performance degradation.

In ternary quantization, TSLD consistently outperforms the alternative QAT-KD methods for all the cases, demonstrating its superior performance in bridging the accuracy gap with the full-precision cases. Interestingly, ternary TSLD even achieved similar or superior accuracy compared to 4-bit PTQ in many tasks (e.g., CoLA, MRPC, SST-2), highlighting its benefits on both accuracy and memory savings.

   &  QAT KD \\ Method \\  } &  &  &  &  \\   & & ACC (\(\)) & PPL (\(\)) & ACC (\(\)) & PPL (\(\)) & ACC (\(\)) & PPL (\(\)) & ACC (\(\)) & PPL (\(\)) \\   & OPT-1.3B FP16 & 61.03 & 1.34 & 81.92 & 2.58 & 94.26 & 2.00 & 76.53 & 3.94 \\   & OPTQ  & **54.61** & **1.36** & **80.14** & **2.43** & **95.07** & **2.02** & **56.32** & **3.96** \\  & AWQ  & 13.63 & 1.45 & 66.42 & 3.49 & 94.26 & **2.02** & 54.51 & 4.72 \\    & Logit  & 50.76 \(\)2.35 & 1.36 & 81.94 \(\)1.68 & 2.62 & 93.57 \(\)0.23 & 2.09 & 75.23 \(\)0.83 & 4.34 \\  & GT-Logit & **54.07** \(\)0.34 & **1.34** & 81.37 \(\)0.51 & 2.60 & 93.54 \(\)0.21 & 2.11 & 75.31 \(\)0.47 & 4.09 \\  & TSLD & **56.33** \(\)0.89 & **1.34** & **83.33** \(\)1.22 & **2.52** & **94.05** \(\)0.19 & **2.04** & **75.97** \(\)0.33 & **0.05** \\   & Logit  & 48.72 \(\)2.68 & 1.37 & 81.62 \(\)0.62 & 2.79 & 93.08 \(\)0.35 & 2.11 & 74.15 \(\)3.56 & 4.72 \\  & GT+Logit & 50.10 \(\)1.38 & **1.34** & 82.10 \(\)0.59 & 2.65 & 92.77 \(\)0.28 & 2.14 & 73.79 \(\)1.16 & 4.44 \\   & TSLD & **54.47** \(\)1.47 & **1.34** & **82.20** \(\)0.84 & **2.63** & **93.92** \(\)0.29 & **2.06** & **75.31** \(\)0.54 & **4.36** \\  

Table 3: Results for the OPT-1.3B model fine-tuned on GLUE  using different QAT-KD methods with five random seed tests. Channel-wise quantization is applied in both PTQ and QAT-KD.

### Ablation Study

**Reduced-Precision Kernels Execution Time.** We developed custom CUDA kernels to enhance inference speed with applied (2-,4-,8-bit) quantization. Like OPTQ, we packed the weights to minimize the model size and load overhead. Our kernel eliminates the need for weight unpacking during the model forward pass, resulting in a speedup shown in Table 4. We tested our kernel mainly on models larger than 6.7B, where weight load overhead is notably high. The reported times are the average execution time for 10,000 kernel runs on a single A100-80GB GPU. For the FP32 baseline, we used PyTorch's nn.Linear. As shown in Table 4, our 2-bit kernel for the 175B model can potentially accelerate a single matrix multiplication operation by an average of approximately 6.1 times compared to FP32.

**Quantization Granularity Impact.** To account for output channel weight variations, channel-wise quantization is used [15; 14]. By integrating our QAT-KD approach with channel-wise quantization, we can achieve further performance enhancement. An interesting observation emerges: the gains from channel-wise quantization vary by the type of GLM. As illustrated in Table 5, for the GPT-2 and OPT series, the PPL performance increase due to channel-wise quantization is less than 1. However, for GPT-Neo and LLaMA, the performance enhancement effect resulting from channel-wise quantization is significantly pronounced. This variation in performance gains suggests distinct channel-wise weight distributions across different GLM models. A detailed analysis of the weight distribution for each GLM is addressed in the Appendix A.4.

## 6 Conclusion

We introduce token-scaled logit distillation, a new approach for Quantization-Aware Training of Generative Language Models. This method effectively reduces overfitting and enhances learning from the teacher model and ground truth. Importantly, this research is the first to evaluate ternary quantization-aware training on large-scale GLMs, achieving less than 1.0 perplexity degradation and preserving commonsense QA and arithmetic reasoning task accuracy.

   Model config. &  &  &  \\  Input channel & 4096 & 4096 & 16384 & 5120 & 5120 & 20480 & 12288 & 12288 & 49152 \\ Output channel & 4096 & 16384 & 4096 & 5120 & 20480 & 5120 & 12288 & 49152 & 12288 \\  FP32 baseline & 0.067 & 0.201 & 0.194 & 0.100 & 0.285 & 0.287 & 0.391 & 1.472 & 1.522 \\ 
8-bit & 0.039 & 0.068 & 0.068 & 0.043 & 0.095 & 0.092 & 0.121 & 0.407 & 0.395 \\ (\(\) speedup) & \( 1.71\) & \( 2.93\) & \( 2.83\) & \( 2.32\) & \( 3.01\) & \( 3.13\) & \( 3.23\) & \( 3.61\) & \( 3.85\) \\ 
4-bit & 0.030 & 0.057 & 0.057 & 0.041 & 0.076 & 0.075 & 0.096 & 0.326 & 0.318 \\ (\(\) speedup) & \( 2.23\) & \( 3.52\) & \( 3.40\) & \( 2.45\) & \( 3.75\) & \( 3.82\) & \( 4.07\) & \( 4.51\) & \( 4.78\) \\ 
2-bit & 0.025 & 0.053 & 0.053 & 0.039 & 0.072 & 0.064 & 0.077 & 0.221 & 0.232 \\ (\(\) speedup) & \( 2.68\) & \( 3.77\) & \( 3.65\) & \( 2.57\) & \( 3.95\) & \( 4.48\) & \( 5.07\) & \( 6.66\) & \( 6.56\) \\   

Table 4: Kernel execution time (msec)

    &  &  &  &  & LLaMA \\  & & 0.1B & 0.3B & 0.8B & 0.1B & 1.3B & 1.3B & 7B \\   & 20.91 & 18.21 & 15.20 & 18.17 & 13.75 & 17.62 & 8.76 \\   & Tensor-wise & 21.74 & 18.57 & 16.14 & 18.58 & 14.60 & 30.60 & 12.31 \\  & Channel-wise & **21.30** & **18.48** & **15.97** & **18.42** & **14.46** & **19.27** & **11.60** \\   

Table 5: Comparison of tensor-wise and channel-wise quantization across various GLMs (GPT-2, OPT, GPT-Neo and LLaMA). The TSLD KD method is employed in this experiments