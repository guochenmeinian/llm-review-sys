# Treeffuser: Probabilistic Predictions via Conditional Diffusions with Gradient-Boosted Trees

Nicolas Beltran-Velez\({}^{*}\)\({}^{1}\) Alessandro Antonio Grande\({}^{*}\)\({}^{2,3}\) Achille Nazaret\({}^{*}\)\({}^{1,3}\)

Alp Kucukelbir\({}^{1,4}\)

David Blei\({}^{1,5}\)

\({}^{1}\)Department of Computer Science, Columbia University, New York, USA

\({}^{2}\)Halvorsen Center for Computational Oncology, Memorial Sloan Kettering

Cancer Center, New York, USA

\({}^{3}\)Irving Institute for Cancer Dynamics, Columbia University, New York, USA

\({}^{4}\)Fero Labs, New York, USA

\({}^{5}\)Department of Statistics, Columbia University, New York, USA

Equal contribution, authors listed in alphabetical order.

###### Abstract

Probabilistic prediction aims to compute predictive distributions rather than single point predictions. These distributions enable practitioners to quantify uncertainty, compute risk, and detect outliers. However, most probabilistic methods assume parametric responses, such as Gaussian or Poisson distributions. When these assumptions fail, such models lead to bad predictions and poorly calibrated uncertainty. In this paper, we propose Treeffuser, an easy-to-use method for probabilistic prediction on tabular data. The idea is to learn a conditional diffusion model where the score function is estimated using gradient-boosted trees. The conditional diffusion model makes Treeffuser flexible and non-parametric, while the gradient-boosted trees make it robust and easy to train on CPUs. Treeffuser learns well-calibrated predictive distributions and can handle a wide range of regression tasks--including those with multivariate, multimodal, and skewed responses. We study Treeffuser on synthetic and real data and show that it outperforms existing methods, providing better calibrated probabilistic predictions. We further demonstrate its versatility with an application to inventory allocation under uncertainty using sales data from Walmart. We implement Treeffuser in [https://github.com/blei-lab/treeffuser](https://github.com/blei-lab/treeffuser).

## 1 Introduction

In this paper, we develop a new method for probabilistic prediction from tabular data. This problem is important to many fields, such as power generation , finance , and healthcare . It drives decision processes such as supply chain planning , risk assessment , and policy-making .

Example: Manufacturing plants measure information as they operate. This information--properties of raw materials, operational flows, temperatures, and level measurements--collectively determine the output of the plant . From this data, how can manufacturers adapt operations to reduce emissions while maximizing profits? The answer requires both good predictions and estimates of uncertainty, so as to trade off the risk of failure with the reward of lower emissions and higher profit. More broadly, such industrial workflow problems often rely on predictions from vast amounts of tabular data--observations of variables arranged in a table .

Our method builds on two ideas: diffusions  and gradient-boosted trees . Diffusions accurately estimate conditional distributions, but existing methods are not designed for tabular data. Decision trees excel at analyzing tabular data, but do not provide probabilistic predictions. Our method, which we call _Treeffuser_, combines the advantages of both formalisms. It defines an accurate tree-based diffusion model for probabilistic prediction which can easily handle large datasets.

Fig. 1 shows Treeffuser in action. We feed it data from three complex distributions-a multimodal response with varying components, an inflated response with shifting support, and a multivariate response with dynamic correlations. For each task, Treeffuser uses trees to learn a diffusion model and then outputs samples from the target conditional. These samples fully capture the complexity of the response functions.

Treeffuser exhibits several advantages:

* It is nonparametric. It makes few assumptions about the form of the conditional response distribution. For example, it can estimate response distributions that are multivariate, multimodal, skewed, and heavy-tailed.
* It is efficient. Diffusions can be slow to train . Treeffuser relies on trees and is fast. For instance, in section 5.3, Treeffuser trains from a table with 112,000 observations and 27 variables in 53 seconds on a laptop.
* It is accurate. On benchmark datasets, Treeffuser outperforms NGBoost , IBUG , quantile regression  and deep ensembles . It provides better probabilistic predictions, including more precise quantile estimations and accurate mean predictions.

The rest of the paper is organized as follows. Section 2 reviews diffusions and gradient-boosted trees. Section 3 integrates these two ideas into Treeffuser and justifies the method. Section 4 discusses related work. Section 5 studies synthetic and standard benchmark datasets.

## 2 Background

Treeffuser combines two ideas: diffusion models and gradient-boosted trees (GBTs). This section provides a gentle introduction to both topics. Familiar readers can skip ahead to Section 3.

### Diffusion Models

A diffusion model is a generative model that learns an arbitrarily complex distribution \(()\). It consists of two processes: forward and reverse diffusion.

The forward process takes the target distribution \(()\) and continuously transforms it into a simple distribution. It does so by progressively adding noise to samples from \(\):

\[\{(0),\\ (t)T(t)=f((t),t)t+g(t)(t),. \]

Figure 1: Samples \(\) from Treeffuser vs. true densities, for multiple values of \(\) under three different scenarios. Treeffuser captures arbitrarily complex conditional distributions that vary with \(\).

where Eq. (1) defines a stochastic differential equation (SDE) with a standard Brownian motion \((t)\), and drift and diffusion functions \(f\) and \(g\). The time horizon \(T\) is large, such that the resulting marginal distribution \(p_{}:=p_{T}((T))=_{^{}}p_{T}((T) {y}(0)=^{})(^{})^{}\) is agnostic to \(\).

The reverse process transforms the simple distribution back into the target distribution by denoising perturbed samples from \(p_{}\). It posits the following model, which runs backward from \(T\) to 0:

\[\{}(T) p_{},\\ }_{t}=[f(}(t),t)-g(t)^{2}_{}(t)} p_{t}(}(t))]t+g(t)}(t), . \]

where \(}(t)\) is a standard Brownian with reversed time. Anderson  shows that \(}(t)}{{=}}(t)\) for each time \(t\), and so \(}(0)\). This means that by drawing a sample from \(p_{}\) and then numerically approximating the reverse process in Eq. (2), we obtain samples from \(\).

However, the score function, \(_{} p_{t}()\), is usually unknown. Vincent  shows it can be estimated from the trajectories of the forward process as the minimizer of the following objective:

\[[_{} p_{t}()]=*{ arg\,min}_{s}_{t}_{}_{p_{0t}} [\|_{(t)} p_{0t}((t)(0))-s((t),t)\|^{2}], \]

where \(\) is the set of all possible functions of \(\) indexed by time \(t\), and \(p_{0t}\) denotes the conditional distribution of \((t)\) given \((0)\). In practice, we set \(t()\) and choose the drift \(f\) and diffusion function \(g\) so that \(p_{0t}\) is Gaussian. We detail our choice of \(f\) and \(g\) in Appendix C.

We approximate the expectations in Eq. (3) with the empirical distribution of the observed \(\) for \(_{}\), and with the known Gaussian trajectories \((t)(0)=\) for \(_{p_{0t}}\). The objective can then be minimized by parametrizing the score \(s\) with any function approximator, such as a neural network or, as we do, with trees. By estimating the score function, we effectively learn the distribution \(\).

For a more detailed introduction to diffusion models, we provide an expanded version of this section in Appendix A.

### Gradient Boosted Trees

Consider the following common machine learning task. Given variables \((,b)\), where \(b\) is scalar, learn the function \(F^{*}()\) that best approximates \(b\) from \(\):

\[F^{*}=*{arg\,min}_{F}_{(,b)}L(F( ),b), \]

where \(L\) is a loss function measuring the quality of \(F\), such as the squared loss \(L(u,v)=(u-v)^{2}\). Gradient-boosted trees (GBTs) are a widely used non-parametric machine learning model for this task . GBTs use decision trees  as simple building block functions \(f:^{d}\) to form a good approximation \(\) of \(F^{*}\). GBTs sequentially build approximations \(_{i}\) defined as

\[_{i}()=_{m=0}^{i-1} f_{m}(), \]

where \((0,1)\) is a parameter akin to a learning rate.

Each decision tree \(f_{i}\) is constructed to minimize the squared error between the current approximation \(_{i}\) and the negative gradient of the loss function:

\[f_{i}=*{arg\,min}_{f}_{,b}[ (f()+(),b)}{ F_{i}()} )^{2}]. \]

As the number of trees \(i\) increases, the approximation \(_{i}\) becomes a better minimizer of Eq. (4). Various modifications to this basic algorithm have been proposed, including different loss functions in Eq. (4), higher order optimizations for Eq. (6) and general heuristics for faster training .

## 3 Probabilistic Prediction via Treeffuser

Treeffuser tackles probabilistic prediction by learning a diffusion model with gradient-boosted trees. It is particularly well-suited to modeling tabular data. Trees offer useful inductive biases, natural handling of categorical and missing data, and fast and robust training procedures. Diffusions eliminate the need for restrictive parametric families of distributions and protect against model misspecification.

Denote an independently distributed set of observations as \(=\{(_{i},_{i})\}_{i=1}^{n}\). Treeffuser forms the predictive distribution \(()\) as a function of inputs \(\). We first introduce conditional diffusion models and discuss the conditional score estimation problem for both univariate and multivariate outcomes. We then outline the training and sampling procedures for Treeffuser.

### The Conditional Diffusion Model

Treeffuser produces a distribution over \(\) for each value of \(\) using conditional diffusion models. Unlike approaches that guide an unconditional model to achieve conditionality, Treeffuser follows the line of work that directly fits the conditional score function [24; 25; 26].

Conditional diffusion models.The diffusion models introduced in Section 2.1 target the marginal distribution of \(\). Here, we extend them to conditional distributions \(_{}():=()\).

To model \(_{}\), assign a diffusion process \(_{}(t)\) to each value of \(\). These processes share the same diffusion equation but have different boundary conditions corresponding to their target conditionals:

\[\{_{}(0)_{}(),\\ }_{}(t)=f(_{}(t),t)t+g(t) (t).. \]

As before, \(f\) and \(g\) are simple functions such that \(_{}(T) p_{}\) for all \(\). Denote the marginal distribution of \(_{}(t)\) as \(p_{,t}\). For two time points \(t\) and \(u\), where \(t>u\), denote the conditional distribution \(_{}(t)_{}(u)\) as \(p_{ut}\). (Note how \(p_{ut}\) does not vary in \(\).)

To match each \(\)-dependent forward SDE we have an \(\)-dependent reverse SDE of the form:

\[}_{}(t)=[f(}_{}(t),t)-g(t )^{2}_{}_{}(t)} p_{,t}(}_{}(t))]t+g(t)}(t), \]

where the score function \( p_{,t}\) now also depends on \(\). Similar to unconditional diffusions, by estimating the _conditional_ score, we can sample from \(_{}\) by first sampling \(}_{}(T)\) from \(p_{}()\) and then solving the corresponding reverse SDE.

Conditional score estimation objective.Estimating the conditional score follows a similar strategy as the unconditional version, but with the added requirement of simultaneously estimating the score for all \(\). Recall that by Eq. (3), for a fixed \(\), the function \(s^{*}_{}\) defined by

\[s^{*}_{}=*{arg\,min}_{s}_{t} _{_{}(_{}(0))}_{p_{0t}(_{ }(t)|_{}(0))}[\|_{_{}(t)} p_{0t}( _{}(t)_{}(0))-s(_{}(t),t)\|^{2}]\]

satisfies \(s^{*}_{}(_{},t)=_{_{}(t)} p_{,t} (_{}(t))\) for all \(\), \(t\) and the fixed \(\). Intuitively, if we allow \(s\) to also take \(\) as input, we can gather all of these individual optimization problems into a single large problem by taking an additional expectation over \(\), that is:

\[*{arg\,min}_{S^{+}}_{()}_{t}_{(_{}(0))}_{p_{0t}}[\| _{_{}(t)} p_{0t}(_{}(t)_{}(0 ))-S(_{}(t),t,)\|^{2}]. \]

Here, \(^{+}\) represents the set of functions that take \(\) as an extra input along \(\) and \(t\). The uppercase \(S\) emphasizes that \(S\) takes \(\) as input, contrary to lowercase \(s\). The validity of this objective is given by the following result.

**Theorem 1** (Optimal Conditional Objective).: _Define \(S^{*}\) as the solution of Eq. (9). Then, for almost all \(,,t\) with respect to \((,)\) and the Lebesgue measure on \(t[0,T]\), we have_

\[S^{*}(,t,)=_{} p_{,t}(). \]

We refer to Eq. (9) as _the conditional score objective_. The proof is provided in Appendix B.

### The Trees

Treeffuser uses gradient-boosted trees (GBTs) to minimize Eq. (9). As GBTs work on scalar outputs, we separate the conditional score objective into an equivalent set of \(d_{y}\) independent scalar-valuedsub-problems

\[S_{k}^{*}=*{arg\,min}_{S_{k}}_{t}_{ (,)}_{p_{0t}(_{}(t)|)}[( (_{}(t))}{ y_{}(t )_{k}}-S_{k}(_{}(t),t,))^{2}], \]

where \(_{k}\) denotes the \(k\)-th component of vector \(\).

Recall that the drift and diffusion functions of the forward process are chosen such that \(p_{0t}\) is Gaussian. Let \(m=m(t;)\) and \(=(t)\) denote the corresponding mean and standard deviation, respectively. Treeffuser replaces the partial derivative in Eq. (11) with its closed-form expression as a function of \(m\) and \(\). Treeffuser further reparametrizes \(S(,t,)\) with \(U(,t,)/(t)\) and defines \(h(,t,)=m(t;)+(t)\), the process by which a sample \(\) at time \(0\) gets diffused into a sample at time \(t\) with Gaussian noise \(\). The optimization problems in Eq. (11) are then:

\[ k\{1,...,d_{y}\}, U_{k}^{*}=*{arg\,min}_{U_{k}} _{(,)}_{t}_{(0,I_{d_{y}})}[(_{k}+U_{k}(h(,t,),t,))^{2} ]. \]

The next theorem justifies how the individual problems in Eq. (12) estimate the conditional score.

**Theorem 2** (Treeffuser One-Dimensional Objectives).: _Denote \(U^{*}=(U_{1}^{*},...,U_{d_{y}}^{*})\). Then for almost all \(,,t\) with respect to \((,)\) and the Lebesgue measure on \(t[0,T]\), we have_

\[_{} p_{,t}()=(,t,)}{( t)}. \]

Each problem in Eq. (12) is a GBT problem where the notation within Eq. (4) corresponds to \(F:=U_{k}\), \(:=(h(,t,),t,)^{d_{y}+1+d_{x}}\), \(b:=-_{k}\) and \(L\) is the square loss. We note that the noise scaling reparametrization, \(S=U/\), is key to stabilizing the learning process; see the ablation study in Appendix G.

Finally, Treeffuser approximates the expectations in Eq. (12) with Monte Carlo sampling. For each sample \((,)\) from the dataset \(\), Treefuser samples \(R\) pairs of \((t,)()(0,I_{d_{y}})\) and creates new datasets \(^{k}\) containing \(R n\) datapoints of the form \(((h(,t,),t,),-_{k})\), one \(^{k}\) per dimension of \(\). Then, each of these datasets is given to a standard GBT algorithm, such as LightGBM  or XGBoost . Our implementation of Treeffuser uses LightGBM. Algorithm 1 details this procedure.

```
Data: GBTs \((U_{1},...,U_{d_{y}})\), input instance \(\),  discretization steps \(n_{d}\), SDE-specific \((p_{},T,f,g,)\) Result: A sample \(()\) \( T/n_{d}\) \( p_{}()\) \(t T\) for\(i=1,,n_{d}\)do \((0,I_{d_{y}})\) \( f(,t)-g(t)^{2}U(,t,)/(t;)\) \(-(+g(t))\) \(t t-\) return\(\)
```

**Algorithm 2**Treeffuser Sampling

### Sampling and Probabilistic Predictions

Treeffuser provides samples from the probabilistic predictive distribution \(()\). It does so as in standard unconditional models by plugging in the GBT-estimated conditional score from Eq. (12)into a numerical approximation of the SDE in Eq. (8). While Treeffuser is compatible with any SDE solver, our implementation leverages Euler-Maruyama  due to its good balance between accuracy and the number of function evaluations. In general, we found good performance with as few as 50 steps. Algorithm 2 implements this procedure.

The samples generated by Treeffuser can then be used to estimate means, quantiles, probability intervals, expectations, or any other quantity of interest that depends on the response distribution.

### Limitations

The design of Treeffuser offers advantages in terms of usability, versatility, and robustness. But it also comes with a few limitations. First, the diffusion process is theoretically defined to only model continuous responses \(\). However, count and other forms of discrete responses are common in the probabilistic modeling of tabular data. While our experiments show that this limitation does not prevent Treeffuser from outperforming comparable methods on these kinds of data, there may be opportunities for further improvement with direct modeling of discrete outcomes. Second, Treeffuser does not offer a closed-form density and must solve an SDE to generate samples. This sampling process, in contrast with the fast training, can become expensive when many samples per datapoint \(\) are required.

## 4 Related Work

Treeffuser builds on advances in diffusion models to form probabilistic predictions from tabular data.

Diffusion models.Diffusion models excel at learning complex unconditional distributions on a range of data, such as images [28; 29; 30], molecules , time series , and graphs . A common task is conditional generation, where the goal is to generate samples from a distribution conditioned on features. There are two approaches to this objective. One approach is to use guidance methods by which the score of an unconditional diffusion model is altered during generation to mimic the score of the conditional distribution [34; 35; 11]. This approach is especially popular for inverse problems [36; 37; 11]. Another approach is to train a conditional model from the start, incorporating the conditioning information during training [38; 39; 24; 25; 26]. This is the approach adopted by Treeffuser.

Treeffuser contributes to a recent line of work that applies diffusions to tabular data. This includes deep learning approaches for data generation , probabilistic regression , and missing data imputation . Among these methods, CARD  uses neural-net based diffusions for probabilistic predictions and thus is most similar to Treeffuser in scope. We attempted to include it in our experiments using the implementation from Lehmann , but returned very poor results. We therefore excluded it from our testbed. The imputation of missing data has been recently extended to gradient-boosted trees .

Probabilistic prediction for tabular dataProbabilistic prediction for tabular data can be classified into parametric and non-parametric methods based on their assumptions about the likelihood shape. Parametric tree-based methods include XGBoost  and PBGM . XGBoost uses natural gradients to optimize a scoring rule, while PBGM sequentially updates the mean and standard deviation for predictions. DRFs obtain maximum likelihood estimates by using this criteria to choose splits. Neural-based parametric methods include Bayesian Neural Networks , MC Dropout , and Deep Ensembles . Notably these methods are all indirectly or directly Bayesian. Another approach, normalizing flows, transforms a latent distribution via an invertible neural network  and has been applied to tabular data . Non-parametric methods are often tree-based, such as Quantile Regression Forests , Distributional Random Forests (DRF) , and IBUG . Quantile Regression Forests approximate the inverse cumulative distribution function by minimizing pinball loss, while DRF and IBUG use a tree-based similarity metric to weight training data for predictions. These methods are baselines in our empirical studies, with detailed descriptions in Appendix F.1.

## 5 Empirical studies

We demonstrate Treeffuser across three settings: synthetic data, standard UCI datasets , and sales data from Walmart. We find that Treeffuser outperforms state-of-the-art methods [15; 17; 50];

[MISSING_PAGE_FAIL:7]

a proper scoring rule, but is sensitive to the estimation of the tail densities . Also, CRPS can readily be estimated from samples of \(p(y)\), which is our setting with the non-parametric methods we evaluate. We evaluate CRPS by generating 100 samples from \(p(y)\) for each \(\). For evaluating multivariate responses, we report the average marginal CRPS over each dimension.

We also measure the quality of point predictions for each model. This is the ability to predict conditional means \([y]\). We approximate \([y]\) using 50 samples and evaluate the accuracy using the mean absolute error (MAE) and the root mean squared error (RMSE).

Experimental setup.We performed 10-folds cross-validation. For each fold, we tuned the hyperparameters of the methods using Bayesian optimization for 25 iterations, using 20% of the current fold's training data as a validation set. Additional Bayesian optimization's iterations did not change the results. We detail the search space of hyperparameters for each method in Appendix F.2.

Results.Table 1 presents CRPS by dataset and method. Treeffuser consistently provides the most accurate predictions across datasets, as measured by CRPS. Notably, it outperforms other methods even when initialized with its default parameters. There is no consistent runner-up: among parametric methods, deep ensemble does overall better than NGBoost and IBUG; quantile regression does well on some datasets but underperforms in others.

We find that Treeffuser also returns the best point predictions of \([y]\), as reported in Table 4. For comparison, we report the accuracy of point predictions from vanilla XGBoost and LightGBM in Table 6 in Appendix F.3. These methods do not provide probabilistic predictions but are tailored for point predictions. As expected, XGBoost and LightGBM outperform or tie with all the probabilistic methods. In particular, they often tie with Treeffuser, suggesting that Treeffuser provides probabilistic prediction without sacrificing average point predictions.

Finally, we conducted an ablation study to investigate the impact of the noise-scaling reparametrization of the score function on Treeffuser's performance. As detailed in Appendix G, noise scaling is key to achieving top accuracy and stability.

### Sales forecasting with long tails and count data

We further demonstrate the applicability of Treeffuser on a publicly available dataset  for sales forecasting under uncertainty. The goal is to forecast the number of units sold for a product given features such as its price, its type (e.g., food, cloths), and past sales.

This task is challenging due to zero inflation and long tails in the distribution of item sales. (E.g., umbrella sales are typically low but can spike during rainy weeks.) It is even more challenging for a diffusion model like Treeffuser, which is designed for continuous responses and not count data.

We use five years of sales data from ten Walmart stores (a large American retail chain). We randomly select 1,000 products, training on 112,000 data points from the first 1,862 days and evaluating 10,000 other data points for the remaining 30 days.

In addition to the previous baselines, we include NGBoost Poisson, a parametric model specifically designed for count data. We evaluate the predictions returned by each method in three ways.

CRPS and accuracy metrics.First, we compute the same evaluation metrics as in the previous experiments. We benchmark against the methods in the experiment and the methods in section 5.2.

   model & Deep & IBUG & NGBoost Poisson & Quantile & Treeffuser & Treeffuser \\ metric & ensembles & & & regression & & (no tuning) \\  CRPS & \(7.05\) & \(7.75\) & \(6.86\) & \(7.11\) & \(\) & \(\) & \( 10^{-1}\) \\ RMSE & \(\) & \(2.16\) & \(2.33\) & \(2.88\) & \(\) & \(\) & \( 10^{0}\) \\ MAE & \(\) & \(1.04\) & \(\) & \(1.01\) & \(\) & \(\) & \( 10^{0}\) \\   

Table 2: Walmart dataset metrics (lower is better). The evaluation is on the last 30 days of data. Treeffuser provides the best probabilistic predictions alongside NGBoost Poisson. Deep ensembles excel at point predictions. Powers of tens are factorized out of each row in rightmost column.

The results are reported in Table 2. We find that Treeffuser again proves competitive, achieving a better CRPS than all methods and comparable MAE and RMSE.

Posterior predictive checks.Second, we perform held-out predictive checks , examining six statistics on the number of items sold: the total count of zero sales, the highest sales figure, and sales figures at the 0.99, 0.999, and 0.9999 quantiles (lower quantiles are well captured by all methods). We produce probabilistic predictions of these quantities by returning their empirical distributions as induced by the samples generated by the models. Fig. 2 compares the observed values against the probabilistic predictions. Treefuser best captures the proportion of zeros and performs as well as NGBoost-Poisson in modeling the behavior of the tails.

Newsvendor model.Finally, we illustrate the practical relevance of accurate probabilistic predictions with an application to inventory management, using the newsvendor model . Assume that every day we decide how many units \(q\) of an item to buy. We buy at a cost \(c\) and sell at a price \(p\). However, the demand \(y\) is random, introducing uncertainty in our decision. The goal is to maximize the expected profit:

\[_{q}p\;[(q,y)]-cq.\]

The solution to the newsvendor problem is to buy \(q=F^{-1}()\) units, where \(F^{-1}\) is the quantile function of the distribution of \(y\).

We apply this model to evaluate the inventory decisions induced by each method on the Walmart dataset. To compute profits, we use the observed prices and assume a margin of 50% over all products. We let Treeffuser, NGBoost-Poisson, and quantile regression learn the conditional distribution of the demand of each item, estimate their quantiles, and thus determine the optimal quantity to buy.

Fig. 3 plots the cumulative profits over the last 30 days of data. Treeffuser outperforms quantile regression by a large margin and performs comparably to NGBoost-Poisson. This is coherent with our PPC results in Fig. 2, showing better quantile estimations for Treeffuser. This demonstrates that Treeffuser delivers competitive probabilistic predictions even for count data responses, a scenario it was not specifically designed to handle.

### Runtime Performance Overview

We measure Treeffuser's performance in terms of training and inference speed across different datasets. On the M5 dataset, using default parameters, Treeffuser completed training in 53.2 seconds on a MacBook Pro M3 Max and generated 10,000 samples (one per test data point) in 2.53 seconds. We conducted additional benchmarking experiments on both the UCI and M5 datasets, with results detailed in Appendix H. (Further discussion on the model's time complexity is also provided in that appendix.) Details about the computational resources used in all of our experiments are available in Appendix D.

Figure 2: Posterior predictive checks for Treeffuser, NGBoost Poisson, and quantile regression. Red dashed line shows the realized value on the test set. Treeffuser best captures the inflation point at zero and performs well on the tails.

## 6 Conclusion

We have introduced Treeffuser, a new model for probabilistic prediction from tabular data.

Treeffuser combines conditional diffusions models with gradient-boosted trees. It can capture arbitrarily complex distributions without requiring any data-specific modeling or tuning. It is amenable to fast CPU learning and naturally handles categorical data and missing values. We have demonstrated that Treeffuser outperforms state-of-the-art methods in probabilistic regression across datasets and metrics. These characteristics make Treeffuser a flexible, easy-to-use, and robust model for probabilistic predictions.

One limitation of our diffusion-based approach is the need to numerically solve an SDE to generate samples, which can be costly when producing many samples. Recent advances, such as progressive distillation  and consistency models , address this issue. Applying these methods to Treeffuser is a direction for future work.