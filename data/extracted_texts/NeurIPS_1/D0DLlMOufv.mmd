# MSA Generation with Seqs2Seqs Pretraining:

Advancing Protein Structure Predictions

 Le Zhang\({}^{1,3}\), Jiayang Chen\({}^{4}\), Tao Shen\({}^{5}\), Yu Li\({}^{4}\), Siqi Sun\({}^{1,2}\)

\({}^{1}\) Fudan University \({}^{2}\) Shanghai Artificial Intelligence Laboratory \({}^{3}\) Mila, Universite de Montreal

\({}^{4}\) The Chinese University of Hong Kong \({}^{5}\) Zelixir Biotech

le.zhang@mila.quebec, liyu@cse.cuhk.edu.hk, siqisun@fudan.edu.cn

Works done during internship at Shanghai Artificial Intelligence Laboratory, co-first authors

###### Abstract

Deep learning models like AlphaFold2 (Jumper et al., 2021) have revolutionized protein structure prediction, achieving unprecedented accuracy. However, the dependence on robust multiple sequence alignments (MSAs) continues to pose a challenge, especially for proteins that lack a wealth of homologous sequences. To overcome this limitation, we introduce MSA-Generator, a self-supervised generative protein language model. Trained on a sequence-to-sequence task using an automatically constructed dataset, MSA-Generator employs protein-specific attention mechanisms to harness large-scale protein databases, generating virtual MSAs that enrich existing ones and boost prediction accuracy. Our experiments on CASP14 and CASP15 benchmarks reveal significant improvements in LDDT scores, particularly for complex and challenging sequences, enhancing the performance of both AlphaFold2 and RoseTTAFold. The code is released at [https://github.com/lezhang7/MSAGen](https://github.com/lezhang7/MSAGen).

## 1 Introduction

The challenge of protein structure prediction (PSP), a central issue in structural biology, has undergone remarkable transformation thanks to advances in deep learning. Among these developments, AlphaFold2 (AF2) stands out for its exceptional performance, primarily attributed to its effective use of multiple sequence alignments (MSAs) (Jumper et al., 2021). MSAs are constructed by querying a protein sequence against extensive databases using sophisticated search algorithms, resulting in collections of homologous sequences that encapsulate evolutionary information. This information serves as the cornerstone for many PSP models. However, not all protein sequences have a wealth of homologous counterparts. In such cases, even the most advanced search algorithms may struggle to construct high-quality MSAs, thereby limiting the performance of MSA-dependent models like AF2 (Jumper et al., 2021; Wang et al., 2022), as illustrated in fig. 1.

Inspired by the generative capabilities of language models (Raffel et al., 2020; Touvron et al., 2023; Chung et al., 2022; Chen et al., 2021), we recognize their potential to extend beyond textual data. Viewing protein sequences through this lens, we propose an innovative approach to generating virtual yet constructive MSAs, akin to generating text. These novel alignments provide supplemental evolutionary information, thereby enhancing the efficacy of protein structure predictions. Within the realm of PSP, tertiary structure prediction remains a critical challenge, occupying a central role in molecular biology by revealing intricate protein functions and interactions. While secondary structure predictions (Rost and Sander, 1993) offer valuable insights, it is the tertiary predictions that provide a comprehensive understanding of a protein's complex conformation.

For bio-tasks such as protein structure prediction, gene sequence alignment, RNA secondary structure determination, microbial community analysis, and evolutionary tree construction, where enhancing downstream performance necessitates multiple sequences, we introduce the sequences-to-sequences (seqs2seqs) generation task. Unlike the conventional sequence-to-sequence (seq2seq) tasks--e.g., machine translation, which requires a strict one-to-one correspondence between a source sequence \(x\) and a target sequence \(y\)--seqs2seqs is designed for flexibility. The task aims to generate multiple coherent sequences from a given sequences. Each generated sequence preserves patterns from the input, but there's no strict correspondence between the input and output. Instead, we prioritize maintaining interconnected patterns across them. This design endows the task with a self-supervised nature due to its intrinsic adaptability.

In the context of protein, such flexibility enables easy extraction of a portion of the MSA as the source, with the remainder acting as the target. Harnessing search algorithms, our framework adeptly extracts source and target data from comprehensive protein databases, paving the way for self-supervised pre-training. To our knowledge, this marks an initial step in tapping into self-supervised generative pretraining to bolster protein structure prediction accuracy.

We introduce MSA-Generator, a protein language model pre-trained using the seqs2seqs as its pretext task. Specialized in simultaneously generating multiple sequences, it effectively captures global structural information from input MSAs. This approach facilitates rapid, _de novo_ sequence creation, improves inferior MSAs (see fig. 1), and boasts adaptability across various protein domains, adeptly navigating computational challenges.

To summarize the main contribution of this article:

* **Innovative unsupervised Seqs2Seqs Task Proposition:** We propose the unsupervised sequences-to-sequences (seqs2seqs) task, a promising approach for generating informative protein sequences, with potential applications extending to other areas like RNA. Integrated with search algorithms, this task streamlines generative pre-training by automating data retrieval from expansive protein databases.
* **Launch of MSA-Generator Model:** MSA-Generator, our state-of-the-art generative protein model, is uniquely devised to employ the seqs2seqs task on a self-curated dataset. It is optimized for multi-sequence generation, skillfully extracting global MSA insights, and has demonstrated adaptability across diverse protein domains.
* **Robust Empirical Validation:** We validate our approach's potency, showcasing significant improvements on AlphaFold2 and RoseTTAFold using CASP14 and CASP15 benchmarks. This signifies a practical leap forward in tackling the intricate challenge of protein folding. It also showcases the promise of seqs2seqs pretraining in the field of bioinformatics.

Figure 1: (**Top Left**) Certain protein sequences lack rich homologs, leading to poor MSA quality with conventional search algorithms. (**Top Right**) We propose a generative model MSA-Generator to produce informative MSA for these queries, offer a potential solution to such challenge.

Related Work

Protein Structure PredictionProteins, despite their immense diversity, are composed of just 20 unique amino acids. Their physical structures, which dictate their functions and characteristics, are fundamental to understanding the essence of life. While the field has witnessed significant advancements like AF2 (Jumper et al., 2021) and RoseTTAFold (Baek et al., 2021), challenges remain. The success of AF2 can be ascribed to its adept utilization of MSAs, constructed by search algorithms such as DeepMSA (Zhang et al., 2019), JackHMMER (Johnson et al., 2010) and MMseqs2 (Steineger and Soding, 2017) across vast databases including UniRef (Suzek et al., 2007) and BFD based on a protein query sequence. Conversely, single-sequence prediction methodologies (Chowdhury et al., 2021; Lin et al., 2022; Chowdhury et al., 2022; Wu et al., 2022) often underperform in comparison. However, for protein queries devoid of extensive family representations, obtaining quality MSAs is challenging. Consequently, the proficiency of MSA-driven techniques diminishes. In this context, our proposed method leverages generative techniques to combat the paucity of homologs in protein sequences, presenting a solution when traditional techniques falter.

Protein Language ModelsLanguage models, initially designed for language processing, have found a expanding role in bioinformatics, primarily for protein sequence representation. This improvement in performance largely stems from the adaptation of the masked language modeling (MLM) strategy, a concept inspired by BERT (Devlin et al., 2019). The ProtTrans series, which includes models such as ProtBert, ProtTXL, ProtXLNet, and ProtT5 (Elnaggar et al., 2021), along with ProteinBERT (Brandes et al., 2022) and ESM models like ESM-1b (Rives et al., 2021) and ESM-2 (Lin et al., 2022), exemplifies the transformative role of Transformer architectures in this field. Further reinforcing the connection between language models and protein structure prediction, research has demonstrated associations between protein representations learned by these models and contact maps, revealing evolutionary patterns that are crucial to the success of AlphaFold2 (AF2) (Vig et al., 2020; Rao et al., 2020). These findings have shifted research focus back towards leveraging multiple sequence alignments (MSAs) rather than relying solely on single sequences. A notable example is the MSA Transformer (Rao et al., 2021), which applies the MLM strategy specifically to MSAs, thereby capturing rich evolutionary information and advancing the field of protein structure prediction.

Protein Sequence GenerationBeyond masked language modeling (MLM), a variety of generative techniques exist, each with its distinct objectives and methodologies. For instance, Potts models (Figliuzzi et al., 2018; Russ et al., 2020) are crafted specifically for individual MSA sets from which they're derived (Zhang et al., 2022). However, their shortcomings in adapting to different MSA sets (Sgarbossa et al., 2022) have spurred the development of generative language models. An exemplar, ESMPair (Chen et al., 2022), constructs MSAs of Interologs by classifying sequences based on their taxonomic lineage. In contrast, both ProGen (Madani et al., 2020) and ProGen2 (Nijkamp et al., 2022) focus on single-sequence generation, sidestepping the integral component of MSA. Another research direction involves VAE-based models (Riesselman et al., 2018; McGee et al., 2021; Sinai et al., 2017), originally developed for mutation evaluation. The challenge of efficiently sampling from distributions to create diverse and long sequences limits their application to downstream tasks. A study by Sgarbossa et al. (2022) utilized the MSA Transformer in a repetitive mask-and-reveal methodology, which unfortunately led to a compromise in sequence diversity. Of notable mention is EvoGen (Zhang et al., 2022), aiming parallelly at producing virtual MSAs for Protein Structure Prediction. However, EvoGen uniquely operates as a meta-generative model, requiring guidance from Alphafold2 to hone its MSA generation prowess.

Despite the lengthy context associated with MSA Generation, our work **connects self-supervised learning with MSA generation**. We highlight generative MSA pretraining, introducing an unsupervised sequences-to-sequences task specifically tailored for efficient MSA generation. To the best of our understanding, this marks a significant stride in the realm of protein sequence generation.

## 3 Sequences-to-Sequences Generative Pretraining

We present the Sequences-to-Sequences generation task and the methodology for automatic dataset construction in section 3.1. Details of the proposed MSA-Generator are provided in section 3.2.

In section 3.3, we delve into the ensemble approach of MSA-Generator for optimizing the Protein Structure Prediction (PSP) task.

### Sequences-to-Sequences Generation For Protein Sequences

Addressing the challenge of sparse homologous matches in Multiple Sequence Alignments (MSA) within real-world databases, we introduce the Sequences-to-Sequences (seqs2seqs) Task, designed for creating virtual MSAs. This approach differs from the conventional sequence-to-sequence frameworks used in machine translation, which typically enforce a strict one-to-one correspondence between source sequence \(x\) and target sequence \(y\). Instead, seqs2seqs allows for more flexibility, focusing on **identifying shared intrinsic patterns** between source sequences \(X\) and target sequences \(Y\). For protein MSAs, this involves **integrating extensive evolutionary data**, both across and within the sequences, to reveal co-evolutionary relationships.

The inherent adaptability of the seqs2seqs model permits the self-supervised nature of the task and seamless gathering of substantial quantities of source and target sequences from protein sequence databases. This is achieved by deploying sequence searching algorithms like JackHMMER (Johnson et al., 2010) and MMSeqs2 (Steinegger & Soding, 2017). Our process began with selecting sequences from the UniRef90 database (Suzek et al., 2007) as initial queries. Subsequently, the JackHMMER algorithm (Johnson et al., 2010) was employed iteratively to identify homologous sequences within the database, based on the query sequences following MSA dataset construction pipeline of AlphaFold2. This process was iterated until no additional sequences emerged, searching parameters are detailed in appendix C. For every batch of sequences retrieved, a random selection was made, designating query with some as the source \(X\) and the remainder as the target \(Y\), as illustrated in fig. 2. Notably, the assurance of co-evolutionary relationships is intrinsically facilitated by the search algorithm's mechanism.

### Sequences-to-Sequences Architecture

Figure 3: **MSA-Generator Overview** (Top) Overview of the architecture, processing pipeline, and module attention operations. (Bottom) Illustration of the attention mechanism. A red star represents a single query position, and the red boxes indicate keys and values utilized in attention processing and calculations.

Figure 2: Difference between seq2seq and seqs2seqs and Automated Data Collection Process.

We pretrain a transformer-based model (Vaswani et al., 2017), denoted as MSA-Generator, via the unsupervised Sequences-to-Sequences task. The MSA-Generator framework incorporates an encoder-decoder structure. The encoder contextualizes the input MSA data, while an auto-aggressive decoder produces sequences derived from this context (refer to section 3). To capture expansive evolutionary information from the input MSA both horizontally and vertically, the encoder integrates the tied-row and column attention mechanism (Rao et al., 2021). As the decoder concurrently generates multiple sequences--interacting with each other and the input MSA--it is enhanced with two additional modules beyond the conventional transformer. The _Cross-Row Attention_ is designed to efficiently acquire a global representation by amalgamating comprehensive states. Meanwhile, to emphasize the vital conservative trait of amino acids (Dayhoff et al., 1978; Henikoff and Henikoff, 1992; Jones et al., 1992), we introduce the _Cross-Column Attention_, which, during the generation of the token at time step \(t\), directs its attention to the \(t\)-th token of all input sequences.

Tied-Row AttentionBuilding upon the foundation laid by the MSA Transformer (Rao et al., 2021), we incorporate a shared-attention mechanism. This is achieved by aggregating the attention map weights across each sequence from MSA \(^{D L}\) prior to applying the softmax function. Notably, each sequence utilizes the same attention weight matrix. For the \(d\)-th row, the associated query, key, and value matrices are denoted as \(Q_{d}\), \(K_{d}\), and \(V_{d}^{L h}\), respectively. These matrices are derived via three distinct learned projection matrices. The computation of the shared attention weight matrix is formulated as:

\[W_{TR}=(_{d=1}^{D}K_{d}^{T}}{(D,h)} )^{L L} \]

In this context, \((D,h)=\) serves as the square-root normalization. This normalization is crucial in mitigating potential linear scaling of attention weights with the sequences. The resultant representation for the \(d\)-th row is obtained through \(W_{TR}V_{d}\).

It's important to highlight that, in our decoder, we deliberately bypass the tied-row attention. This decision aids in maintaining diversity in the generated sequences. Instead, we lean towards a conventional self-attention mechanism.

Cross-Row AttentionContrary to tasks like machine translation, where the target attends only to a single input during decoding, the essence of seqs2seqs lies in discerning intrisic patterns common to both source and target sequences. This necessitates a holistic comprehension of the input, implying that when generating a sequence, the decoder should attend to the entirety of the input. A naive concatenation would yield a representation with dimensions \(^{D L h}\), rendering it computationally expensive and thus impractical.

To address this, we introduce an efficient strategy that calculates the depth-wise average pooling of encoder hidden states \(H_{enc}\), represented as \(H_{c}=_{d=1}^{D}H_{enc}^{d}^{L h}\). This serves as a global representation of the input and is crucial for cross-attention during decoding. Here, \(K_{c}=H_{c}W_{k}\) and \(V_{c}=H_{c}W_{v}\) signify the key and value matrices, while \(Q=X_{dec}W_{q}\) stands for query matrix projected from decoder hidden states \(X_{dec}\). The Cross-Row attention is:

\[(Q,K_{c},V_{c})=(^{T}}{})V_ {c} \]

Each sequence generation process can access comprehensive information, attending to the same keys and values, mirroring the co-evolutionary patterns of the input MSA simultaneously thus **permitting fast parallel generation of multiple sequences** (middle at bottom in fig. 3).

Self/Cross -Column AttentionIn MSA, each column represents residues or nucleotides at a specific position across sequences, revealing conserved regions essential for understanding biological functions, structural stability, or evolutionary significance (Dayhoff et al., 1978; Jones et al., 1992; Henikoff and Henikoff, 1992). Drawing inspiration from the vertical-direction attention proposed in Ho et al. (2019), we introduce a self-column attention in encoder for a comprehensive representation akin to Rao et al. (2021), and a cross-column attention in decoder to capture conservation characteristics.

To facilitate both attention mechanisms, the representation matrix \(X^{D L h}\) needs to be transposed prior to the execution of self and cross attention:

\[(Q_{col},K_{col},V_{col})=((K_{col}^{T}}{})V_{col})^{T} \]

For the self-column attention, projections from \(X^{T}\) yield \(Q_{col},K_{col},V_{col}^{L D h}\). In contrast, for the cross-column attention, \(Q_{col}\) is determined from decoder hidden states as \(X_{dec}^{T}W_{q}\), whereas \(K_{col}\) and \(V_{col}\) are projected from encoder hidden states as \(H_{enc}^{T}\) (see fig. 3 bottom right).

Pre-training objectiveWe employ the seqs2seqs task to pretrain MSA-Generator. For a given source MSA \(X^{D L}\), the loss is computed with respect to the target MSA \(Y^{D^{} L}\) as follows:

\[_{seqs2seqs}=- L}_{d=0}^{D^{}} _{l=0}^{L} P(y_{l}^{d}|y_{<l}^{d},X) \]

It's crucial to note that each sequence \(y Y\) is generated referencing the entire source matrix \(X\), and this generation occurs in parallel owing to the thoughtful design of the architecture.

Pretrained MSA-Generator adopts 12 transformer encoders/decoders with 260M parameters, 768 embedding size, and 12 heads. It's pretrained with ADAM-W at a \(5e^{-5}\) rate, 0.01 linear warm-up, and square root decay for 200k steps on 8 A100 GPUs, batch size of 64, using a dataset containing 2M MSAs constructed as described in section 3.1.

### Generation and Ensemble Strategy

During inference, for every query sequence \(x\), we initially use a search algorithm to assemble a MSA denoted as \(X\). This is subsequently inputted into MSA-Generator to produce MSA \(Y\). The concatenated MSA \(X Y\) serves as input for the subsequent task. Nucleus sampling (Holtzman et al., 2019), set with _top-p=50_ and _top-k=10_, is implemented to foster unique sequences and curtail redundancy.

For the purpose of optimizing the PSP task and yielding informative sequences, we adopt pLDDT as our selection criterion, leveraging our ability for swift MSA generation. pLDDT (Kryshtafovych et al., 2019; Jumper et al., 2021) measures the accuracy of predicted inter-residue distances for each residue in a protein, serves as a confidence indicator, with elevated scores hinting at potentially more accurate predictions. Utilizing pLDDT, we enhance each MSA through multiple runs, computing corresponding pLDDT scores for each. The MSA with the premier pLDDT score is subsequently selected as the optimal ensemble result and employed to determine the prediction accuracy relative to the ground truth.

## 4 Empirical Validation

### Setup

The tertiary structure of a protein is pivotal, directly determining its functionality. In structural biology, the tertiary structure not only reveals the overall conformation of a protein but also inherently includes insights from secondary structures (Rao et al., 2021; Jones, 1999), such as the arrangement and orientation of \(\)-helices and \(\)-sheets, and from contact predictions (Wang et al., 2017), denoting the spatial interactions between amino acid pairs. In essence, the tertiary structure offers a holistic view, allowing direct inference of localized structural features and interactions between amino acids. Leveraging tools like AlphaFold2 lets us directly obtain this comprehensive structural data, thereby bypassing intermediary steps like secondary structure and contact prediction. Given the centrality of tertiary structure prediction in protein functional studies, we have prioritized this task and adopted Local Distance Difference Tests 2 (LDDT) (Mariani et al., 2013) and pLDDT as our metric for evaluating prediction accuracy. We also adopt Template modeling score (TM-Score) and Global distance test (GDT-TS) to measure global structure prediction accuracy. Specifically, we assess MSA-Generator by comparing protein tertiary structures predicted from PSP algorithm, namely AlphaFold2 and RoseTTAFold, with various input MSAs. Demonstrating the usefulness of generated MSAs and the efficacy of MSA-Generator.

Benchmark & DatasetWe employ CASP14/15 as our test set, a prestigious dataset that encompasses proteins from a broad spectrum of biological families. The creation of a vast protein structure prediction dataset is prohibitively expensive, and given that AF2 has already trained on all previously available structures, this dataset emerges as the best evaluation benchmark. It's important to highlight that **sequences from CASP14/15 aren't part of our pretraining dataset**, and our evaluations precede the AF2 version updated with CASP14/15 information.

Our primary interest lies in challenging protein sequences devoid of homologues, rendering traditional search algorithms ineffective. For every query in our test dataset, we use JackHMMER to search within UniRef90, which contains 70 million sequences, in order to gather related homologues. We define two scenarios: (1) _artificial challenging MSAs_, where we purposefully pick the top 15 homologues for each test set query as an _artificial gold standard_. From these, 5 homologues are further sampled as the _artificial baseline_, offering a synthetic challenge. (2) _real-world challenging MSAs_, which includes 20 sequences from test set, each with homologues less than 20, significantly challenge PSP algorithms. All assessments are executed in a zero-shot setting.

### Are Virtual MSAs as good as Real MSAs?

We employ _Artificial challenging MSAs_ to thoroughly compare our generated virtual MSAs with conventional searched real MSAs to demonstrate that virtual MSAs can closely approximate real ones in downstream tasks. Specifically, for every _Baseline_ MSA, we deploy MSA-Generator to produce a _Virtual_ MSA that poses the same depth of the _Real_ MSA. For each MSA, we employ an ensemble of three runs using the strategy outlined in section 3.3. Refer to fig. 4 for the results.

The fig. 4 (a) shows that the LDDT distribution of the _baseline_ suffers a sharp decline when reduced from 15 to 5 sequences. This underscores the importance of MSA quality for cutting-edge PSP algorithms. Yet, when supplemented by MSA-Generator's generative virtual MSA, the gap to the _Real_ narrows considerably, reflecting a LDDT enhancement of 12.8. This underscores the effectiveness of our generated MSAs.

A more granular observation in fig. 4 (b) illustrates that the majority of _baseline_ data points are below the diagonal, while most _Virtual_ points sit above it, some even considerably outpacing the _Real_. fig. 4 (c) illustrates the statistics on improvements in intervals. It's evident that our generative virtual MSAs effectively improve results for 72.8% of protein sequences (over Baseline). Remarkably, nearly half of the generated virtual MSAs even outperform the real searched MSAs. This emphasizes the importance of generative MSAs and the potential of the seqs2seqs task in uncovering co-evolutionary patterns within bio-sequences. Among them, there are even 6 generative virtual MSA that surpass real MSA by more than 30 LDDT, including most notable T1032-D1 (**+46.02** LDDT), T1054-D1 (**+46.8** LDDT), and T1070-D2 (**+61.22** LDDT), suggesting that without generative virtual MSA,

Figure 4: **Artificial Challenging Cases Results from AlphaFold2** (a) Violin plots of LDDT distribution. (b) x-axis represents LDDT of _artificial gold standard_, and the y-axis represents LDDT of _artificial baseline_ and _artificial augmentation_. Dashed-line represents 95% confidence intervals (c) Pie chart of LDDT improvements in intervals. The inner circle represents a comparison with the baseline, while the outer circle represents a comparison with the real.

current PSA algorithm may fail on these queries. Comprehensive results for individual MSAs can be found in the Appendix.

### Real-World MSA Challenges

Our ultimate goal is to produce high-quality MSAs for protein sequences with few homologues. Current search algorithms often fail to construct quality MSAs for these, making PSP algorithms similarly struggle with accurate predictions. The _Real-world challenging MSAs_ evaluation is devised to test the efficacy of the seqs2seqs generative pretraining approach in addressing this challenge. For this, we curate sequences with fewer than 20 homologues using the search method detailed in section 4.1. For every identified MSA, we employ MSA-Generator to generate an MSA of identical depth across three independent runs. Subsequently, we measure the ensemble LDDT by inputting them to the PSP algorithm following section 3.3. For comparison, our benchmarks include the strong single-sequence folding technique, ESMFold (Lin et al., 2022) and OmegaFold (Wu et al., 2022); we apply the same evaluation set up for the iterative unmasking strategy highlighted in (Sgarbossa et al., 2022); and generation with Potts models (Figliuzzi et al., 2018).

Table 1 presents the pLDDT, LDDT, TM-Score and GDT-TS improvements achieved through various MSA generation techniques across different models. The single-sequence-based models lags behind MSA-based strategies. AF2 consistently outperforms RoseTTAFold in both metrics. Potts models, intriguingly, don't demonstrate a pronounced ability to produce effective MSAs. This is evidenced by their marginally reduced average performance in both metrics, echoing findings from (Sgarbossa et al., 2022; Rao et al., 2020). While iterative unmasking with MSA Transformer (Sgarbossa et al., 2022) can generate usable MSAs in certain scenarios, the MSAs it produces are often less diverse due to the inherent unmasking process, thus limiting its enhancement potential.

In contrast, the method we propose demonstrates significant enhancements across all metrics in both models. This suggests that the MSAs produced by our approach are predominantly informative. In particular, 75% of the MSAs experienced substantial improvement, leading to an average increase in LDDT scores of 4.3 for CASP14 and 8.1 for CASP15. Our observations reveal a consistent 6% improvement in both TM-Score and GDT-TS for CASP14, alongside a 6% increase in TM-Score and a 4% rise in GDT-TS for CASP15. These results underscore our method's efficacy in enhancing both local and global structure predictions.

Remarkable LDDT enhancements were observed in T1093-D1 (with 3 homologous), moving from 45.5 to 70.77, and in T1113 (with 10 homologous), rising from 32.6 to 80.6. However, in specific MSAs, like T1094-D2 (7 homologous, pLDDT +2.8, LDDT -0.1), T1099-D1 (8 homologous, pLDDT +1.11, LDDT -0.5), and T1178 (15 homologous, pLDDT +2.2, LDDT -12.3), an elevation in pLDDT was offset by a decline in LDDT. This suggests that pLDDT might not always be a consistent indicator for selection strategy outlined in section 3.3. Detailed

    &  &  \\   & pLDDT\(\) & LDDT\(\) & TM-Score \(\) & GDT-TS \(\) & pLDDT\(\) & LDDT\(\) & TM-Score\(\) & GDT-TS \(\) \\   \\ ESMFold & 43.3 & 41.9 & 0.56 & 0.47 & 46.0 & 53.4 & 0.47 & 0.41 \\ OmegaFold & - & 43.1 & 0.51 & 0.44 & - & 49.6 & 0.44 & 0.39 \\   \\ RoseTTAFold & 63.5 & 51.3 & 0.56 & 0.51 & 62.6 & 52.1 & 0.53 & 0.46 \\ RoseTTAFold+Potts Generation & 63.2 & 48.9 & 0.56 & 0.50 & 94.4 & 51.0 & 0.52 & 0.46 \\ RoseTTAFold+Herune Unmasking & 63.9 & 52.2 & 0.57 & 0.53 & 65.3 & 55.3 & 0.55 & 0.48 \\ RoseTTAFold+MSA-Generator & 68.9\({}_{+-4}\) & 56.3\({}_{-0.8}\) & 0.61\({}_{-0.5}\) & 0.56\({}_{-0.8}\) & 58.4\({}_{-4.3}\) & 0.58.5\({}_{-0.5}\) & 0.54.5 \\  \\ AlphaFold2_+Potts Generation & 63.1 & 53.2 & 0.59 & 0.54 & 63.1 & 53.6 & 0.55 & 0.49 \\ AlphaFold2+Potts Generation & 63.8 & 50.9 & 0.60 & 0.55 & 64.5 & 52.6 & 0.56 & 0.48 \\ AlphaFold2+Lirex Unmasking & 65.2 & 54.6 & 0.61 & 0.57 & 69.5 & 57.3 & 0.57 & 0.51 \\ AlphaFold2+MSA-Generator & 71.6\({}_{+-4.4}\) & 57.5\({}_{+-4.3}\) & 0.65\({}_{-0.6}\) & 0.60\({}_{-0.6}\) & 73.7\({}_{-8.6}\) & 63.7\({}_{-8.1}\) & 0.61\({}_{-0.6}\) & 0.53\({}_{-4.5}\) \\   

Table 1: **Real-World MSA Challenges** average pLDDT, LDDT and RMSD enhancement scores, averaged over 3 runs; avg. Det. represents average depth of MSA.

  
**Orphan25** & **pLDDT** & **LDDT** & **TM-Score** & **GDT-TS** \\  AlphaFold24 & 77.2 & 61.6 & 0.61 & 0.62 \\ AlphaFold2+Potts Generation & 68.9 & 49.3 & 0.49 & 0.43 \\ AlphaFold2+Iterative Unmasking & 78.9 & 62.5 & 0.64 & 0.63 \\ AlphaFold2+MSA-Generator & 81.8 & 66.4 & 0.69 & 0.67 \\   

Table 2: Comparison of AlphaFold2 variants on the Orphan25 dataset.

results for each individual MSA are available in the appendix E A more challenging aspect of our study involves enhancing the prediction of structures for orphan protein sequences. To evaluate our method in this context, we included results from an orphan protein family, Orphan25 Wang et al. (2022b). We employed MMseqs2 to search against the UniRef30 and ColabFoldDB databases Mirdita et al. (2022), with ColabFoldDB being an extension of BFD/MGnify, enriched with metagenomic sequences from diverse environments. By selecting sequences without homologues, we identified a test set comprising 10 proteins (6WKY, 6WL0, 6XA1, 6XN9, 6XYI, 7A5P, 7AL0, 7JJV). The outcomes, summarized in table 2, indicate that our approach demonstrates substantial benefits for orphan protein sequences. This suggests that our method is particularly advantageous for challenging inputs.

### Re-Evaluating pLDDT as a Selection Metric

We sought to examine the effectiveness of pLDDT as a criterion. As previously highlighted, for specific proteins, improvements in pLDDT do not necessarily correlate with increases in LDDT. To delve deeper, we conducted an experiment where LDDT was directly calculated for each enhanced MSA in section 4.3. We then selected the highest LDDT as the output, bypassing the use of pLDDT as an intermediary metric.

The disparity between pLDDT-based and LDDT-based predictions shown in fig. 5 suggests that pLDDT may not always be the best criterion. A noticeable gap exists between the two criteria (evident in the blue and red regions). For example, proteins T1064-D1 (depth=9) and T1122 (depth=1) exhibit significant gaps between scores chosen by LDDT versus pLDDT, highlighted in red boxes. Interestingly, for T1178 (depth=15), the highest pLDDT selection scores -12.3 against the baseline, while LDDT selection results in a +2.7. This implies that some generated MSAs, even with lower pLDDT scores, can enhance the LDDT. This indicates that our approach has untapped potential that could benefit from more nuanced selection criteria. The ideal situation would be for both predictions to produce the same significant improvement, as emphasized by the green boxes and visualized in fig. 5 (Right). Notably, MSA-Generator shows notable improvements for protein sequences with few homologs, emphasizing its utility in real-world protein folding challenges.

Figure 5: (Left) LDDT improvement selected by different criteria; (Right) Protein structure visualization with pLDDT and LDDT selected by Best-pLDDT.

Figure 6: Ablation and MSA feature.

### MSA Diversity & Conservation

We directly evaluated the generated MSAs based on two key characteristics: **diversity** and **conservation**. All experiments setup are consist with section 4.3. To measure diversity, we analyzed the average Shannon Entropy across MSA columns. Compared to the Iterative Unmask method (Sgarbossa et al., 2022), our technique, as illustrated in fig. 5(b) (left), consistently yields higher Entropy, suggesting increased diversity. For conservation, we examined the Position-Specific

Scoring Matrix (PSSM) (Altschul et al., 1997) of both original and MSA-Generator-generated MSAs. The PSSM gauges conservation of specific amino acids. The **Pearson correlation coefficient** between the searched real and generated virtual PSSMs, shown in fig. 5(b) (right), highlights the retention of amino acid conservation in generative virtual MSAs (see fig. 7 for visualization of conversation). The outcomes highlight MSA-Generator's effectiveness as an MSA generator and demonstrate the broader capabilities of the seqs2seqs framework.

### Ablation Study

Following section 4.3 setup, we investigated the impact of Ensemble Runs and Sequence Augmentation Factor on PSP outcomes, as shown in fig. 5(a). While additional ensemble runs lead to improved pLDDT scores, gains in LDDT plateau after three runs, even with higher computational expenses. Hence, we chose 3 runs to optimize performance and efficiency. Furthermore, a higher augmentation factor does not always yield better results; the identical input MSA might lack new insights, and extra sequences risk introducing noise.

## 5 Conclusion

We present an unsupervised seqs2seqs task, accompanied by an automated dataset construction pipeline, designed to pre-train MSA-Generator for simultaneous multi-sequence generation. Rigorous experimentation underscore the effectiveness, diversity, and conservation feature of generated virtual MSAs, amplifying the prowess of stalwarts like AlphaFold2 in scenarios where conventional methods come up short. Furthermore, our approach demonstrates generalization across a wide array of protein sequence families in a zero-shot fashion. Our findings highlight the immense promise of the unsupervised seqs2seqs task, pointing towards its prospective utility in a broader spectrum of bio-sequences, thereby amplifying its benefits.

## Limitation

While MSA-Generator demonstrates promising improvements in protein structure prediction, there are several limitations to consider.

First, we did not conduct scale-up experiments to explore the effects of increasing model size and expanding the pre-training dataset. It is possible that larger models and more comprehensive training data could further enhance the generative capabilities for sequence prediction, potentially improving structure prediction accuracy. Second, our current approach relies on selecting the maximum pLDDT score from multiple runs, which may not be the most optimal method and introduces inefficiency due to the need for repeated predictions. Developing a more refined metric to assess MSA quality or better predict the reliability of the final structure could significantly improve efficiency and consistency in future work. Lastly, although our model effectively handles proteins with limited homologous sequences, the generated MSAs typically do not achieve substantial depth. Increasing the depth of these generated MSAs would require higher computational resources, as the complexity scales with MSA depth. Addressing this challenge will be crucial for extending the applicability of our approach to a broader range of protein sequences.