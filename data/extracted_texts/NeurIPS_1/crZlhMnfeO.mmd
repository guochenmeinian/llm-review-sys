# RayDF: Neural Ray-surface Distance Fields with Multi-view Consistency

Zhuoman Liu Bo Yang Yan Luximon Ajay Kumar Jinxi Li

vLAR Group, The Hong Kong Polytechnic University

{zhuo-man.liu, jinxi.li}@connect.polyu.hk bo.yang@polyu.edu.hk

Corresponding Author

###### Abstract

In this paper, we study the problem of continuous 3D shape representations. The majority of existing successful methods are coordinate-based implicit neural representations. However, they are inefficient to render novel views or recover explicit surface points. A few works start to formulate 3D shapes as ray-based neural functions, but the learned structures are inferior due to the lack of multi-view geometry consistency. To tackle these challenges, we propose a new framework called **RayDF**. It consists of three major components: 1) the simple ray-surface distance field, 2) the novel dual-ray visibility classifier, and 3) a multi-view consistency optimization module to drive the learned ray-surface distances to be multi-view geometry consistent. We extensively evaluate our method on three public datasets, demonstrating remarkable performance in 3D surface point reconstruction on both synthetic and challenging real-world 3D scenes, clearly surpassing existing coordinate-based and ray-based baselines. Most notably, our method achieves a \(1000\) faster speed than coordinate-based methods to render an \(800 800\) depth image, showing the superiority of our method for 3D shape representation. Our code and data are available at [https://github.com/vLAR-group/RayDF](https://github.com/vLAR-group/RayDF)

## 1 Introduction

Learning accurate and efficient 3D shape representations is crucial for many cutting-edge applications in the fields of machine vision and robotics. Recent advances in 3D coordinate-based neural representations including occupancy fields (OF) , un/signed distance fields (U/SDF) , and radiance fields (NeRF) , have shown great potential to recover complex shapes from RGB/D images and/or point clouds. Although these methods and their variants have achieved excellent performance in a wide range of downstream tasks such as shape reconstruction , novel view synthesis , and scene understanding , obtaining 3D shapes or 2D views from their trained networks is computationally expensive, due to the requirement of extensive network evaluations to regress surface points.

Very recently, a number of works start to represent 3D models as ray-based neural functions. By simply taking individual light rays as input, the methods LFN  and NeuLF  learn to directly predict the radiance (RGB) values, while PRIF  and DDF  straightly estimate the surface hitting points of input rays. Compared with coordinate-based representations, these ray-based methods are clearly more efficient to infer 3D geometry and render 2D views, as every single ray only needs to query the trained network once forward. Nevertheless, the learned 3D geometries by these methods are still lack of fidelity, primarily because they fail to explicitly take into account geometry consistency across multiple views, resulting in the network likely over-fitting individual training rays but unable to generalize to unseen rays in testing.

In this paper, we aim to address this key issue of ray-based neural representations by explicitly integrating multi-view geometry consistency into the network design. In particular, we introduce anew pipeline as shown in Figure 1. It consists of two independent neural networks together with a particular training module: 1) the main ray-surface distance network, 2) an auxiliary dual-ray visibility classifier, and 3) a multi-view consistency optimization module.

The **main network** simply takes a ray as input and directly infers the distance between ray origin and its hitting point on the surface. Basically, this network elegantly represents 3D shapes as ray-based implicit functions, denoted as _ray-surface distance fields_, keeping the unique advantage in efficiency for shape extraction and rendering. The **auxiliary network** takes a pair of rays as input and predicts their mutual visibility. In fact, this is a simple binary classifier, aiming at distinguishing whether any two rays hit at the same surface point or not, _i.e_., the mutual visibility. Having a trained auxiliary network at hand, the **multi-view consistency optimization module** specifies how to effectively leverage the learned dual-ray visibility to train the main network, thus driving learned ray-surface distances to be multi-view consistent from any seen or unseen viewing angles.

Since our ray-surface distance fields primarily aim at representing accurate 3D shapes, the whole pipeline is designed to be trained on depth images, although light fields (radiance) can be optionally learned in parallel if color images are also available in training. In this regard, the closest works to ours are PRIF  and DDF , neither of which explicitly considers the multi-view geometry consistency. Overall, compared with all existing coordinate-based representations, our method keeps the superiority in efficiency thanks to the ray-based formulation. Compared with the existing ray-based approaches, ours excels at learning accurate 3D geometries thanks to the multi-view consistency for **ray**-surface **distance** fields. Our method is called **RayDF** and our contributions are:

* We employ a straightforward ray-surface distance field for representing 3D shapes. This formulation is inherently more efficient than existing coordinate-based representations.
* We design a new dual-ray visibility classifier to learn the spatial relationships of any pair of rays, enabling the learned ray-surface distance fields to be multi-view geometry consistent.
* We demonstrate superior 3D shape reconstruction accuracy and efficiency on multiple datasets, showing significantly better results than existing coordinate-based and ray-based baselines.

## 2 Related Work

**Explicit 3D Shape Representations:** Classic methods to recover explicit 3D geometry of objects and scenes mainly include SfM  and SLAM systems such as Colmap  and ORB-SLAM . Another classic method is space carving [45; 37], which involves the process of carving out the voxel grid by utilizing multiple distinct views to obtain a robust approximation within the voxel space. To model explicit 3D structures, deep learning-based methods have shown impressive progress in recovering voxel grids [15; 80; 81], point clouds , octree , polygon meshes [34; 26] and shape primitives . A comprehensive survey of these methods can be found in [6; 28]. Thanks to the large-scale datasets  for training sophisticated neural networks [29; 31], these methods demonstrate excellent results in many downstream tasks such as shape reconstruction [76; 70], generation , and semantic scene perception [67; 17; 25]. However, the fidelity of these discrete shape representations is primarily limited by their spatial resolutions and memory footprint.

**Coordinate-based Implicit 3D Shape Representations:** To avoid the discretization issue of traditional 3D representations, there has been a growing interest in developing implicit neural 3D representations using simple MLPs to represent continuous 3D shapes. Inspired by the seminal _coordinate-based_ methods [11; 46; 54] which have shown great success in encoding high-quality 3D shapes, a plethora of follow-up works have been developed to tackle various vision tasks in [65; 79; 2; 12; 13; 20; 33; 43; 42; 27; 83]. These coordinate-based representations can be generally classified as: 1) occupancy fields (OF) [46; 11], 2) signed distance fields (SDF) , 3) unsigned

Figure 1: The general workflow and components of our framework.

distance fields (NDF) [14; 74], and 4) radiance fields (NeRF) . More related methods can be found in surveys [24; 18]. Among them, the OF/SDF/NDF based methods demonstrate exceptional accuracy in recovering continuous 3D structures thanks to the simple level-set formulation, while the NeRF based methods achieve an unprecedented level of fidelity in rendering 2D views thanks to the successful volume rendering equation. However, all these coordinate-based representations inevitably require dense 3D location sampling and evaluations on trained networks to regress explicit surface points, though advanced techniques [41; 57; 69; 39; 30; 56; 50; 82; 48] can mitigate this issue somewhat but at the expense of a large memory increase. In this paper, our RayDF only needs a single network evaluation to regress every surface point, while still achieving on par or better reconstruction accuracy with existing OF/SDF/NDF methods.

**Ray-based Implicit 3D Shape Representations:** To overcome the inefficiency of coordinate-based 3D representations, a number of works start to formulate 3D shapes as ray-based neural functions via MLPs. Ray-based methods simply take individual rays as input and directly output either radiance values (RGB), _i.e. light fields_, or surface points, _i.e. distance fields_. LFN  and NeuLF  are among the early works of light fields to encode 3D representations from observed color images. Other light field works include [22; 61; 78; 52; 68; 75; 9; 1]. PRIF , DDF  and DRDF  are the early ray-based distance fields to learn 3D shapes from observed depth scans. Although these methods have shown very encouraging results for novel view rendering or surface reconstruction, they are usually limited to datasets with small baselines across multi-views. Basically, this is because the multi-view geometry consistency is not taken into account in their network designs. By contrast, in our RayDF pipeline, we explicitly introduce a dual-ray visibility classifier to aid our ray-surface distance fields to be multi-view shape consistent during training.

## 3 RayDF

### Overview

Our pipeline consists of two networks and an optimization module. As shown in Figure 2, for the main network: **ray-surface distance field \(f_{}\)**, it takes a single oriented ray \(\) as input, and directly regresses the distance \(d\) between ray starting point and its surface hitting point. For the input ray \(\), we opt for the conventional spherical parameterization , as it supports querying from 360\({}^{}\) viewing angles. In particular, a fix-sized sphere is predefined with a relatively large diameter \(D\) as a convex hull in which the target 3D scene is bounded. Each surface point \(\) of the target scene can be regarded as a directional ray penetrating the sphere with two intersection points on the sphere. Each intersection point is parameterized by two variables specifying the angles with regard to the sphere center. Then, for each ray, we have a 4D parameterization \(=(^{in},^{in},^{out},^{out})\). Formally, the ray-surface distance field is defined below and implementation details are in Appendix A.1.

\[d=f_{}(),=(^{in},^{in},^{ out},^{out}), \]

For the auxiliary network: **dual-ray visibility classifier \(h_{}\)**, it takes a pair of rays as input and simply predicts their mutual visibility, aiming at explicitly modeling the mutual spatial relationships between any pairs of rays. This network, once well-trained, will play a key role in the third component: **multi-view consistency optimization**. Detailed designs are discussed in Sections 3.2&3.3.

### Dual-ray Visibility Classifier

The main ray-surface network alone can indeed well fit many training ray-distance pairs, but there is no mechanism driving its output distances, _i.e._, surface geometry, to be consistent across multiple views, especially for unseen views. For example, as illustrated in the leftmost block of Figure 3, if

Figure 2: The left block illustrates the spherical parameterization of an input ray \(\), and the right block shows our simple MLP-based ray-surface distance field \(f_{}\).

two input rays \(_{1}\) and \(_{2}\) are mutually visible in the scene, both rays must hit at the same surface point, then the corresponding ray-surface distances \(d_{1}\) and \(d_{2}\) must satisfy a transformation equation \(_{1}^{in}+d_{1}_{1}^{}=}{y_{1}}=_{2}^{in}+d _{2}_{2}^{}\), where \(^{}=^{out}-^{in}}{\|^{out}-^{in}\|}\) and \(^{*}=(^{*}^{*}\\ ^{*}^{*})\), \(*\{in,out\}\) (More details in Section A.1.3), according to both rays' angles. Similarly, if input rays \(_{1}\) and \(_{3}\) are mutually invisible, then they should never satisfy the transformation equation because both never meet at the same surface point. From this fundamental principle, we can see that the mutual visibility between any two rays is crucial to inform the ray-surface distance network to be multi-view consistent.

To this end, we design a binary classifier to discriminate the visibility of an input pair of rays. A naive idea is to simply concat two rays as input, feeding into MLPs to regress 0/1, where 1 represents _visible_ and 0 otherwise. However, such a design fails to retain the symmetry of two rays. Here symmetry means that the visibility of two rays must be invariant to the input order of two rays. With this point, as illustrated in the right block of Figure 3, our visibility classifier \(h_{}\) is formally defined as:

\[h_{}: MLP_{S}^{in},_{1}^{in},_{1}^ {out},_{1}^{out})+g(_{2}^{in},_{2}^{in},_{2}^{out},_{2 }^{out})}{2} k(x_{1},y_{1},z_{1}) 0/1 \]

where \((_{1}^{in},_{1}^{in},_{1}^{out},_{1}^{out})\) and \((_{2}^{in},_{2}^{in},_{2}^{out},_{2}^{out})\) are the parameterizations of two input rays \(_{1}\) and \(_{2}\) respectively; \(g()\) is a shared single fully-connected layer followed by an average pooling, thus guaranteeing the symmetry of input rays. Note that, the surface hitting point of \(_{1}\), _i.e._, \((x_{1},y_{1},z_{1})\), is also encoded via a separate single fully-connected layer \(k()\), followed by a concatenation operation denoted by \(\), thus enhancing the pooled ray features, as we empirically find that such an enhancement could notably improve the classifier's accuracy. Implementation details are in Appendix A.1.

### Multi-view Consistency Optimization

With the designed main ray-surface distance network \(f_{}\) and the auxiliary dual-ray visibility classifier \(h_{}\) at hand, we introduce the crucial multi-view consistency optimization module to train both networks. In particular, given \(K\) posed depth images (\(H W\)) of a static 3D scene as the whole training data, our training module consists of two stages.

**Stage 1 - Training Dual-ray Visibility Classifier**

The key to training this classifier is to create correct data pairs. First of all, all raw depth values are converted to ray-surface distance values. For a specific \(i^{th}\) ray (pixel) in the \(k^{th}\) image, we project its ray-surface point back to the remaining \((K-1)\) scans, obtaining the corresponding \((K-1)\) distance values. We set 10 millimeters as the _closeness_ threshold to determine whether the projected \((K-1)\) rays are visible in the \((K-1)\) images. In total, we generate \(K*H*W*(K-1)\) pairs of rays together with 0/1 labels. The standard cross-entropy loss function is adopted to optimize our dual-ray visibility classifier. More details on training data generation and implementation are in Appendix A.1.3.

Note that, this classifier is trained in a scene-specific fashion. Once the network is well-trained, it basically encodes the relationships between any two rays of a specific scene into network weights.

**Stage 2 - Training Ray-surface Distance Network**

The ultimate goal of our whole pipeline is to optimize the main ray-surface network and drive it to be multi-view geometry consistent. However, this is non-trivial as simply fitting the network with ray-surface data points cannot generalize to unseen rays, which can be seen in our ablation study in Section 4.5. In this regard, we fully leverage the well-trained visibility classifier to aid our training of ray-surface distance network. Particularly, this stage consists of the following key steps:

Figure 3: The leftmost block illustrates the mutual visibility of a pair of rays. The remaining block shows the symmetric design of our dual-ray visibility classifier \(h_{}\).

* Step 1: All depth images are converted to ray-surface distances, generating \(K*H*W\) training ray-distance pairs for a specific 3D scene.
* Step 2: As illustrated in Figure 4, for a specific training ray (\(,d\)), called _primary ray_, we uniformly sample \(M\) rays \(\{^{1}^{n}^{M}\}\), called _multi-view rays_, in a ball centering at the surface point \(\). We then calculate the distance between surface point \(\) and the bounding sphere along each of \(M\) rays, obtaining multi-view distances \(\{^{1}^{m}^{M}\}\). This can be easily achieved according to the given distance \(d\) in the training set. \(M\) is simply set as \(20\) and more details are in Appendix A.4.
* Step 3: We establish \(M\) pairs of rays \(\{(,,^{1})(,,^{m})(,,^{M})\}\) and then feed them into the well-trained visibility classifier \(h_{}\), inferring their visibility scores \(\{v^{1} v^{m} v^{M}\}\).
* Step 4: We feed the primary ray and all sampled \(M\) multi-view rays \(\{,^{1}^{m}^{M}\}\) into the ray-surface distance network \(f_{}\), estimating their surface distances \(\{,^{1}^{m}^{M}\}\). Since the network \(f_{}\) is randomly initialized, thus the estimated distances are inaccurate in the beginning.
* Step 5: We design the following multi-view consistency loss function to optimize the ray-surface distance network until convergence: \[_{mv}=^{M}v^{m}+1}(|-d|+_{m=1}^{M}( |^{m}-^{m}|*v^{m}))\] (3) Basically, this simple loss drives the network to not only fit the primary ray-surface distance (seen rays in the training set), but also satisfy that the visible multi-view rays (unlimited unseen rays in the training set) also have accurate distance estimations.

### Surface Normal Derivation and Outlier Points Removal

In the above Sections 3.1&3.2&3.3, we have two network designs and an optimization module to train them separately. Nevertheless, we empirically find that the main ray-surface distance network may predict inaccurate distance values, particularly for rays near sharp edges. Essentially, this is because the actual ray-surface distances may be discontinuous at sharp edges given extreme viewing angle changes. This shape discontinuity is actually a common challenge for almost all existing implicit neural representations, because modern neural networks can only model continuous functions in theory.

Fortunately, a nice property of our ray-surface distance field is that the normal vector at every estimated 3D surface point can be easily derived in a closed-form expression using auto differentiation of the network. In particular, given an input ray \(=(^{in},^{in},^{out},^{out})\), and its estimated ray-surface distance \(\) from network \(f_{}\), the corresponding normal vector \(\) of that estimated surface point can be derived as a specific function shown below.

\[=Q}{},,D,\ . \]

With this normal vector, we may choose to add an additional loss to regularize the estimated surface points to be as smooth as possible. Yet, we empirically find that the overall performance improvement on an entire 3D scene is rather limited, as these extremely discontinuous cases are actually sparse.

In this regard, we turn to simply removing the predicted surface points, _i.e._, outliers, whose normal vectors' Euclidean distances are larger than a threshold in the network inference stage. In fact, PRIF  also adopts a similar strategy to filter out outliers. Note that, advanced smoothing or interpolating techniques may be integrated to improve our framework, which is left for future exploration.

## 4 Experiments

Our method is evaluated on three types of public datasets: 1) the object-level synthetic Blender dataset from the original NeRF paper , 2) the scene-level synthetic DM-SR dataset from the recent DM-NeRF paper , and 3) the scene-level real-world ScanNet dataset .

**Baselines:** We carefully select the following six successful and representative implicit neural shape representations as our baselines: 1) OF , 2) DeepSDF , 3) NDF , 4) NeuS , 5) DS-NeRF , 6) LFN , and 7) PRIF . The OF/DeepSDF/NDF/NeuS methods are coordinate-based level-set methods, showing outstanding performance in modeling 3D structures. DS-NeRF

Figure 4: Multi-view ray sampling.

is a depth-supervised NeRF , inheriting excellent capability in rendering 2D views. LFN and PRIF are two ray-based methods with superior efficiency in generating 2D views. We note that there are many sophisticated variants of these baselines, achieving SOTA performance on various datasets. Nevertheless, we do not intend to comprehensively compare with them, basically because many of their techniques such as more advanced implementations, adding additional conditions, replacing with more powerful backbones, _etc._, can be easily integrated into our framework as well. We leave these potential improvements for future exploration but only focus on our vanilla ray-surface distance field in this paper. For a fair comparison, all baselines are supervised with the same amount of depth scans as ours, carefully trained from scratch in the same scene-specific fashion. More details about the implementation of all baselines and possible minor adaptations are in Appendix A.3.1.

**Metrics:** For evaluation metrics of shape reconstruction, we report: 1) the per ray-surface **absolute distance error (ADE)** in centimeters averaged across all testing images, 2) the **Chamfer distance (CD)** between the whole reconstructed structure and the ground truth shape for each scene. As to our method, we use the existing TSDF fusion  to obtain a full mesh from predicted depths at the testing views, on which we uniformly sample 30K points. Another 30K points are uniformly sampled from the ground truth 3D mesh for calculating CD. The outlier point removal is only applied to clean the reconstructed point clouds when calculating CD. No additional post-processing steps are employed when computing all ADE scores. Apparently, ADE is more accurate to evaluate the surface estimation of our method, because the CD scores may be biased due to the extra TSDF fusion. In this regard, the CD scores are just presented as a complementary metric to show the general shape quality in the whole 3D scene space. More details about how to obtain the reconstructed 3D full shape for each baseline and our method are in Appendix A.3.2. For evaluation metrics of appearance reconstruction, _i.e._ novel view synthesis of 2D color images, the standard **PSNR**, **SSIM**, and **LPIPS** scores are reported following NeRF . We present **Accuracy** (%) and **F1-Score** (%) as evaluation metrics of the dual-ray visibility classifier. For qualitative results, refer to Figure 5 and Appendix A.7.

### Efficiency of RayDF

Similar to the existing ray-based methods LFN and PRIF, our RayDF also has the superiority in efficiency to generate 2D images or recover the explicit surface points. To quantitatively compare the efficiency with baselines, we conduct a simple experiment to generate an \(800 800\) depth image on a computer with a single NVIDIA RTX 3090 GPU card and a CPU of AMD Ryzen 7. As shown in Table 1, it can be seen that, not surprisingly, the coordinate-based methods OF DeepSDF/NDF/NeuS/ DS-NeRF are extremely slow to render a high-resolution depth image. In particular, OF  needs a large number of small steps to gradually approach the surface point along a given light ray direction, while DeepSDF  and NDF  rely on expensive sphere tracing to regress the points. NeuS  and DS-NeRF  need extra post-processing steps to calculate depth values from densities. By contrast, our RayDF and the existing ray-based methods achieve more than 1000\(\) faster speed to render a dense depth view.

### Evaluation on Blender Dataset

The Blender dataset from NeRF  consists of pathtraced images of 8 synthetic objects with complicated geometry and realistic materials. Each object has 100 views for training and 200 novel views for testing. Each image has \(800 800\) pixels. Since the released dataset does not include depth images, we just use the provided Blender files to generate additional depth scans of the same resolution exactly following the original 2D view poses. Note that, the physical sizes of these models are about 2.5 meters in length/height/width, so the sphere diameter \(D\) is set as 3 meters in our method. For the baselines and our method, we conduct the following two groups of experiments.

* 3D Shape Representation Only:** Since our RayDF needs depth scans in training to learn continuous 3D shape representations, we conduct this group of experiments only on multi-view depth images. All baselines are also trained on the same number of depth views.
* 3D Shape and Appearance Representations:** Our RayDF is also flexible to add a parallel branch to output radiance field, _i.e._, RGB values, so that the continuous appearance representations can be simultaneously learned. In this regard, we conduct this group of experiments on both RGB images and depth images. The baselines NeuS/ DS-NeRF/ LFN/ PRIF are also trained on the same RGBD images for comparison. The other three methods OF/ DeepSDF/ NDF are not

    & Time \\  OF  & 286.057 \\ DeepSDF  & 17.590 \\ NDF  & 28.310 \\ NeuS  & 32.793 \\ DS-NeRF  & 25.612 \\ LFN  & 0.017 \\ PRIF  & **0.013** \\
**RayDF (Ours)** & 0.019 \\   

Table 1: Rendering time consumption (seconds).

included here, because it is non-trivial to add RGB supervision due to their level-set formulation.

Details of the parallel branch for all methods are in Appendix A.3.

**Analysis:** Table 2 shows the quantitative comparison. We can see that: 1) Our RayDF achieves significantly better results for explicit surface recovering on both groups of experiments, especially on the most important ADE metric, demonstrating the clear superiority over both coordinate and ray based baselines. 2) Our method also achieves comparable performance with DS-NeRF for novel view synthesis, being better than LFN and PRIF and showing the flexibility of our framework.

### Evaluation on DM-SR Dataset

The DM-SR dataset from the recent DM-NeRF paper  consists of 8 synthetic complex 3D indoor rooms. The room types and designs follow Hypersim dataset , and the rendering trajectories for both training and testing images follow the Blender dataset of NeRF . For each 3D scene, there are 300 views for training and 100 novel views for testing. Each view has \(400 400\) pixels. Each scene has a physical size of about 10 meters in length/height/width, so the sphere diameter \(D\) is set as 11 meters in our method. Similarly, we conduct the following two groups of experiments.

* 3D Shape Representation Only:** We conduct this group of experiments only on multi-view depth images for all methods.
* 3D Shape and Appearance Representations:** We conduct this group of experiments on both RGB images and depth images for NeuS/ DS-NeRF/ LFN/ PRIF and our method.

**Analysis:** From Table 3, it can be seen that our method again surpasses all baselines on the most critical ADE metric in both groups of experiments, though the rough metric CD scores are just comparable with baselines potentially due to the inaccuracy incurred by external TSDF fusion. In the meantime, our method still obtains high-quality novel view synthesis, without noticeably sacrificing shape representation when trained with both RGB images and depth scans in Group 2.

### Evaluation on ScanNet Dataset

We further evaluate our method on the challenging real-world 3D dataset ScanNet . We randomly select 6 scenes for evaluation. Originally, each scene has more than 2000 RGBD scans in a relatively

    &  &  \\  & \))\(\)} &  & \))\(\)} &  &  &  \\  & & & mean / median & & & & mean / median & \\  OF  & 10.57 & 2.982 / 0.706 & - & - & - & - & - & - \\ DeepSDF  & 12.95 & 3.382 / 0.679 & - & - & - & - & - & - \\ NDF  & 12.14 & **2.976** / 0.831 & - & - & - & - & - & - \\ NeuS  & 11.88 & 4.756 / 0.907 & 12.10 & 4.662 / 0.938 & **27.19** & 0.910 & 0.100 \\ DS-NeRF  & 13.22 & 117.270 / 1.135 & 14.64 & 143.295 / 1.760 & 26.63 & **0.933** & **0.063** \\ LFN  & 24.47 & 89.425 / 15.681 & 12.33 & 60.289 / 1.230 & 23.20 & 0.888 & 0.125 \\ PRIF  & 14.68 & 20.764 / 1.677 & 14.56 & 21.279 / 1.693 & 23.31 & 0.874 & 0.152 \\
**RayDF (Ours)** & **7.97** & 3.388 / **0.663** & **8.17** & **3.295 / 0.755** & 26.52 & 0.910 & 0.099 \\   

Table 2: Quantitative results of all baselines and our method in experiments of Groups 1&2. All scores are averaged out across 8 scenes of the Blender dataset .

    &  &  \\  & \))\(\)} &  & \))\(\)} &  &  &  \\  & & mean / median & & & & mean / median & \\  OF  & 15.83 & 11.402 / 4.888 & - & - & - & - & - \\ DeepSDF  & 16.97 & **11.281** / 5.087 & - & - & - & - & - \\ NDF  & 22.41 & 12.300 / 5.911 & - & - & - & - & - \\ NeuS  & 9.94 & 12.744 / **4.620** & 11.66 & 15.017 / 5.308 & **33.22** & 0.965 & 0.054 \\ DS-NeRF  & 10.77 & 25.380 / 6.032 & 10.77 & 25.548 / 6.102 & 31.83 & **0.977** & **0.026** \\ LFN  & 18.30 & 13.673 / 5.372 & 18.51 & **14.085** / 5.349 & 30.86 & 0.930 & 0.111 \\ PRIF  & 11.89 & 25.993 / 5.159 & 11.77 & 24.842 / **5.156** & 31.01 & 0.933 & 0.111 \\
**RayDF (Ours)** & **7.41** & 14.272 / 5.353 & **7.97** & 14.251 / 5.300 & 30.32 & 0.940 & 0.113 \\   

Table 3: Quantitative results of all baselines and our method in experiments of Groups 1&2. All scores are averaged out across 8 scenes of the DM-SR dataset .

[MISSING_PAGE_FAIL:8]

**Analysis:** Table 5 (1) clearly shows that, without the aid of our dual-ray visibility classifier, the main ray-surface distance field completely collapses and cannot predict reasonable distance values for novel rays in the test set. In Table 5 (2) and (3), with the input of surface point position, the classifier achieves higher accuracy and higher F1-score, thus providing the ray-surface distance network with more accurate visibility information to predict precise distance values. In addition, concatenating two input rays directly disrupts the inherent symmetry of input rays. This is shown in Table 5 (4), where the classifier trained in this way achieves high accuracy but exhibits a low F1-score. This indicates that such a classifier is significantly less robust than the one trained with symmetric input rays. Table 6 demonstrates that a less robust classifier results in the ray-surface distance network predicting inaccurate distances.

In addition to the dual-ray visibility classifier, we also perform various ablation studies on our entire pipeline on the Blender dataset.

**Ablation on Multi-view Rays Sampling:** We conduct an ablation study with different values of \(M\{10,20,40\}\) to determine the optimal number of multi-view rays for training a ray-surface distance network. Table 7 demonstrates that increasing the sampled multi-view rays enhances the accuracy of the ray-surface distance network. Although \(M=40\) achieves lower ADE and CD values, the substantial GPU memory and extended training time outweigh the limited performance improvement. Therefore, we opt to sample multi-view rays with \(M=20\) to train our ray-surface distance network in all main experiments.

**Ablation on Sparse Depth Supervision:** With the advancement of existing techniques of depth estimation from RGB images, it is quite feasible to obtain sparse depth signals using existing techniques such as SfM or learning-based monocular depth estimators. In this regard, we additionally provide experimental results using sparse depth supervision. In Table 8, our method maintains satisfactory performance, even with only 1% depth values during training. We hypothesize that such robustness comes from our multi-view consistency constraint, because many depth values in the training set may be redundant thanks to our effective classifier.

**Ablation of One-stage RayDF:** In the paper, we adopt a two-stage training strategy for our dual-ray visibility classifier and ray-surface distance network. For a comparison, we simply train both networks simultaneously. However, not surprisingly, the performance of one-stage training drops noticeably as shown in Table 9. This drop in performance is primarily because the classifier is inaccurate at the early stage and unlikely to provide effective constraints for the ray-surface distance network, given the similar number of training steps. Nevertheless, exploring one-stage training remains an intriguing direction for our future research efforts.

For a more comprehensive analysis, we provide extensive ablation studies and additional qualitative as well as detailed quantitative results of each scene in the Blender dataset in Appendix A.4.

## 5 Conclusion

In this paper, we have shown that it is truly possible to efficiently and accurately learn 3D shape representations by using a multi-view consistent ray-based framework. In contrast to the existing coordinate-based methods, we instead use a simple ray-surface distance field to represent 3D shape geometries, which is further driven by a novel dual-ray visibility classifier to be multi-view shape consistent. Extensive experiments on multiple datasets demonstrate the extremely high rendering efficiency and outstanding performance of our approach. It would be interesting to extend our framework with more advanced techniques such as faster implementation and additional regularizations.

    &  & CD (\( 10^{-3}\)) \(\) \\  & & mean / median \\ 
1\% & 8.54 & **3.301** / 0.937 \\
5\% & 8.70 & 3.250 / 0.920 \\
10\% & 8.68 & 3.313 / 0.924 \\
**100\% (RayDF)** & **7.97** & 3.388 / **0.663** \\   

Table 8: Ablation study on sparse depth supervision.

    & ADE\(\) & CD (\( 10^{-3}\)) \(\) \\  & & mean / median \\  one-stage & 12.41 & 4.032 / **0.659** \\
**two-stage (RayDF)** & **7.97** & **3.388** / 0.663 \\   

Table 9: Ablation study on one/two-stage training.

Figure 5: Qualitative results of all methods on the three datasets. More qualitative results can be found in Appendix A.7 and our project page: [https://vlar-group.github.io/RayDF.html](https://vlar-group.github.io/RayDF.html)

**Acknowledgement:** This work was supported in part by Research Grants Council of Hong Kong under Grants 15225522 & 25207822 & 15606321, in part by National Natural Science Foundation of China under Grant 62271431, and in part by The Hong Kong Polytechnic University under Grant P0031501.