# Facing Off World Model Backbones:

RNNs, Transformers, and S4

Fei Deng

Rutgers University

fei.deng@rutgers.edu

&Junyeong Park

KAIST

jyp10987@kaist.ac.kr

&Sungjin Ahn

KAIST

sungjin.ahn@kaist.ac.kr

Correspondence to sungjin.ahn@kaist.ac.kr.

###### Abstract

World models are a fundamental component in model-based reinforcement learning (MBRL). To perform temporally extended and consistent simulations of the future in partially observable environments, world models need to possess long-term memory. However, state-of-the-art MBRL agents, such as Dreamer, predominantly employ recurrent neural networks (RNNs) as their world model backbone, which have limited memory capacity. In this paper, we seek to explore alternative world model backbones for improving long-term memory. In particular, we investigate the effectiveness of Transformers and Structured State Space Sequence (S4) models, motivated by their remarkable ability to capture long-range dependencies in low-dimensional sequences and their complementary strengths. We propose S4WM, the first world model compatible with parallelizable SSMs including S4 and its variants. By incorporating latent variable modeling, S4WM can efficiently generate high-dimensional image sequences through latent imagination. Furthermore, we extensively compare RNN-, Transformer-, and S4-based world models across four sets of environments, which we have tailored to assess crucial memory capabilities of world models, including long-term imagination, context-dependent recall, reward prediction, and memory-based reasoning. Our findings demonstrate that S4WM outperforms Transformer-based world models in terms of long-term memory, while exhibiting greater efficiency during training and imagination. These results pave the way for the development of stronger MBRL agents.

## 1 Introduction

The human brain is frequently compared to a machine whose primary function is to construct models of the world, enabling us to predict, plan, and react to our environment effectively . These mental representations, referred to as world models, are integral to essential cognitive functions like decision-making and problem-solving. Similarly, one of the pivotal tasks in artificial intelligence (AI) systems that aim for human-like cognition is the development of analogous world models.

Model-Based Reinforcement Learning (MBRL)  has emerged as a promising approach that builds world models through interaction with the environment. As a fundamental component of MBRL, these world models empower artificial agents to anticipate the consequences of their actions and plan accordingly, leading to various advantages. Notably, MBRL offers superior sample efficiency, mitigating the high data requirements commonly associated with model-free methods. Moreover, MBRL exhibits enhanced exploration, transferability, safety, and explainability , making it well-suited for complex and dynamic environments where model-free methods tend to struggle.

The effectiveness and characteristics of world models crucially depend on their backbone neural network architecture. In particular, the backbone architecture dictates the model's capabilities of capturing long-term dependencies and handling stochasticity in the environment. Additionally, it affects the compactness of memory footprint and the speed of future prediction rollouts. Furthermore, in visual MBRL that finds extensive practical applications, the backbone architecture holds even greater significance than it does in state-based MBRL. This is due to the need to deal with high-dimensional, unstructured, and temporal observations.

Nevertheless, choosing the appropriate backbone architecture for visual MBRL has become a considerable challenge due to the rapidly evolving landscape of deep architectures for temporal sequence modeling. This includes the recent advancements in major backbone architecture classes, notably Transformers  and the Structured State Space Sequence models such as S4  and S5 .

Traditionally, Recurrent Neural Networks (RNNs)  have been the go-to backbone architecture, thanks to their efficient use of computational resources in processing sequential data. However, RNNs tend to suffer from vanishing gradient issues , limiting their long-term memory capacity. Recently, Transformers  have demonstrated superior sequence modeling capabilities in multiple domains, including natural language processing and computer vision . Their self-attention mechanism grants direct access to all previous time steps, thereby enhancing long-term memory. Moreover, Transformers offer parallel trainability and exhibit faster training speeds than RNNs. However, their quadratic complexity and slow generation speed pose challenges when dealing with very long sequences. To address these limitations, the S4 model has been proposed, offering both parallel training and recurrent generation with sub-quadratic complexity. In the Long Range Arena  benchmark consisting of low-dimensional sequence modeling tasks, the S4 model outperforms many Transformer variants in both task performance and computational efficiency.

In this paper, we present two primary contributions. Firstly, we introduce S4WM, the first and general world model framework that is compatible with any Parallelizable SSMs (PSSMs) including S4, S5, and other S4 variants. This is a significant development since it was unclear whether the S4 model would be effective as a high-dimensional visual world model, and if so, how this could be achieved. To this end, we instantiate the S4WM with the S4 architecture to manage high-dimensional image sequences, and propose its probabilistic latent variable modeling framework based on variational inference. Secondly, we conduct the first empirical comparative study on the three major backbone architectures for visual world modeling--RNNs, Transformers, and S4. Our results show that S4WM outperforms RNNs and Transformers across multiple memory-demanding tasks, including long-term imagination, context-dependent recall, reward prediction, and memory-based reasoning. In terms of speed, S4WM trains the fastest, while RNNs exhibit significantly higher imagination throughput. We believe that by shedding light on the strengths and weaknesses of these backbones, our study contributes to a deeper understanding that can guide researchers and practitioners in selecting suitable architectures, and potentially inspire the development of novel approaches in this field.

## 2 Related Work

**Structured State Space Sequence (S4) Model.** Originally introduced in , S4 is a sequence modeling framework that solves all tasks in the Long Range Arena  for the first time. At its core is a structured parameterization of State Space Models (SSMs) that allows efficient computation and exhibits superior performance in capturing long-range dependencies both theoretically and empirically. However, the mathematical background of S4 is quite involved. To address this, a few recent works seek to simplify, understand, and improve S4 . It has been discovered that S4 and Transformers have complementary strengths . For example, Transformers can be better at capturing local (short-range) information and performing context-dependent operations. Therefore, hybrid architectures have been proposed to achieve the best of both worlds. Furthermore, S4 and its variants have found applications in various domains, such as image and video classification , audio generation , time-series generation , language modeling , and model-free reinforcement learning . Our study introduces the first world model compatible with S4 and its variants (more generally, parallelizable SSMs) for improving long-term memory in MBRL. We also investigate the strengths and weaknesses of S4 and Transformers in the context of world model learning.

**World Models.** World models  are typically implemented as dynamics models of the environment that enable the agent to plan into the future and learn policies from imagined trajectories. RNNs have been the predominant backbone architecture of world models. A notable example is RSSM , which has been widely used in both reconstruction-based  and reconstruction-free  MBRL agents. With the advent of Transformers , recent works have also explored using Transformers as the world model backbone . While Transformers are less prone to vanishing gradients  than RNNs, their quadratic complexity limits their applicability to long sequences. For example, recent works  use a short imagination horizon of \(\)\(20\) steps. In contrast, S4WM can successfully imagine hundreds of steps into the future with sub-quadratic complexity. We also develop an improved Transformer-based world model that can deal with long sequences by employing Transformer-XL .

**Agent Memory Benchmarks.** While many RL benchmarks feature partially observable environments, they tend to evaluate multiple agent capabilities simultaneously  (_e.g._, exploration and modular skill learning), and may be solvable with a moderate memory capacity . Additionally, some benchmarks are designed for model-free agents , and may contain stochastic dynamics that are not controlled by the agents, making it hard to separately assess the memory capacity of world models. The recently proposed Memory Maze  focuses on measuring long-term memory and provides benchmark results for model-based agents. We build upon Memory Maze and introduce additional environments and tasks to probe a wider range of memory capabilities. Another recent work, TECO , also introduces datasets and a Transformer-based model for evaluating and improving long-term video prediction. Our work has a different focus than TECO in that we stress test models on extremely long sequences (up to \(2000\) steps), while TECO considers more visually complex environments, with sequence lengths capped at \(300\). Our experiment setup allows using relatively lightweight models to tackle significant challenges involving long-term memory. We include a comparison with TECO in Appendix F.

## 3 Background

S4  and its variants  are specialized parameterizations of linear state space models. We first present relevant background on linear state space models, and then introduce the S4 model.

**Linear State Space Models (SSMs)** are a widely used sequence model that defines a mapping from a 1-D input signal \(u(t)\) to a 1-D output signal \(y(t)\). They can be discretized into a sequence-to-sequence mapping by a step size \(\). The continuous-time and discrete-time SSMs can be described as:

\[&$}(t)=(t)+u(t)\\ y(t)=(t)+u(t),&& _{k}=}_{k-1}+ }_{k}\\ y_{k}=}_{k}+}_{k} . \]

Here, the vectors \((t)\) and \(_{k}\) are the internal hidden state of the SSM, and the discrete-time matrices \(},},}, }\) can be computed from their continuous-time counterparts \(,,,\) and the step size \(\). We will primarily deal with the discrete-time SSMs, which allow efficient autoregressive generation like RNNs due to the recurrence in \(_{k}\).

Unlike RNNs, however, linear SSMs can offer parallelizable computation like Transformers. That is, given the input sequence \(u_{1:T}\), the output sequence \(y_{1:T}\) can be computed in parallel across time steps by a discrete convolution  or a parallel associative scan . In this work, we define the class of **Parallelizable SSMs (PSSMs)** to be the SSMs that provide the following interface for both parallel and single-step computation:

\[\ _{1:T},_{T}=(_{1:T}, _{0})\,\ _{k},_{k}=(_{k},_{k-1})\, \]

where the inputs \(_{k}\) and outputs \(_{k}\) can be vectors.

**The S4 model** aims to use SSMs for deep sequence modeling, where the matrices \(,,,\) and the step size \(\) are learnable parameters to be optimized by gradient descent. Because SSMs involve computing powers of \(}\), which is in general expensive and can lead to the exploding/vanishing gradients problem , SSMs with a randomly initialized \(A\) perform very poorly in practice .

To address these problems, S4 parameterizes \(A\) as a Diagonal Plus Low-Rank (DPLR) matrix : \(=-^{*}\), where \(\) is a diagonal matrix, \(P\) is typically a column vector (with rank 1), and \(^{*}\) is the conjugate transpose of \(P\). This parameterization allows efficient computation of powers of \(}\)while also including the HiPPO matrices , which are theoretically derived based on continuous-time memorization and empirically shown to better capture long-range dependencies. In practice, S4 initializes \(\) to the HiPPO matrix. To cope with vector-valued inputs and outputs (_i.e._, \(_{k},_{k}^{H}\)), S4 makes \(H\) copies of the SSM, each operating on one dimension, and mixes the outputs by a position-wise linear layer. Follow-up works further simplify the parameterization of \(\) to a diagonal matrix , and use a multi-input, multi-output SSM for vector-valued sequences .

```
0: context \((_{0:C},_{1:C})\), query \(_{C+1:T}\)\(\)Encode context
1:for context step \(t=0,,C\) parallel do
2:\(_{t} q(_{t}\,|\,_{t})=( _{t})\)
3:endfor
4:\(\)Prepare inputs to S4 blocks
5:for time step \(t=1,,T\) parallel do
6:\(_{t}=([_{t-1},_{t}])\)
7:endfor
8:\(\)Encode history by S4 blocks
9:\(_{1:T},_{T}=(_{1:T},_{0})\)\(\)Compute prior and decode posterior latents
10:for time step \(t=1,,T\) parallel do
11:\(p(_{t}\,|\,_{<t},_{ t})=(_{t})\)
12:\(}_{t}=([_{t},_{t}])\)
13:endfor
14:Compute objective by Equation (9)
```

**Algorithm 2** S4WM Imagination

## 4 S4WM: A General World Model for Parallelizable SSMs

We consider an agent interacting with a partially observable environment. At each time step \(t\), the agent receives an image observation \(_{t}\). It then chooses an action \(_{t+1}\) based on its policy, and receives the next observation \(_{t+1}\). For simplicity, we omit the reward here.

We aim to model \(p(_{1:T}_{0},_{1:T})\), the distribution of future observations given the action sequence. We note that it is not required to model \(p(_{0})\), as world model imagination is typically conditioned

Figure 1: We propose S4WM, the first S4-based world model for improving long-term memory. S4WM efficiently models the long-range dependencies of environment dynamics in a compact latent space, using a stack of S4 blocks. This crucially allows fully parallelized training and fast recurrent latent imagination. S4WM is a general framework that is compatible with any parallelizable SSM including S5 and other S4 variants.

[MISSING_PAGE_FAIL:5]

in model-based agents in terms of long-term imagination, context-dependent recall, reward prediction, and memory-based reasoning. We believe that our investigation provides more insights than the final performance alone, and paves the way for model-based agents with improved memory.

To this end, we develop a diverse set of environments shown in Figure 2, each targeting a specific memory capability. The environments are based on the 3D Memory Maze  and the 2D MiniGrid , both with partial observations. The world models are learned from an offline dataset collected by a scripted policy for each environment. This allows the world models to be evaluated independently of the policy learning algorithms.

Specifically, for each episode, the environment is regenerated. To simplify the evaluation of world model imagination, we design the data collecting policy to consist of a _context_ phase and a _query_ phase. In the context phase, the policy fully traverses the environment, while in the query phase, the policy revisits some parts of the environment. For evaluation, we use unseen episodes collected by the same policy as training. The world model observes the context phase, and is then evaluated on its imagination given the action sequence in the query phase. Because the environments are deterministic and have moderate visual complexity, and the context phase fully reveals the information of the environment, it suffices to use the mean squared error (MSE) as our main evaluation metric.

In the following, we first motivate our choice of the baselines through a comparison of speed and memory consumption, and then introduce and present the results for each environment in detail.

### Baselines

**RSSM-TBTT.** RSSM  is an RNN-based world model backbone used in state-of-the-art MBRL agents . Recently,  show that training RSSM with truncated backpropagation through time (TBTT) can lead to better long-term memory ability. We follow their implementation and denote the model as RSSM-TBTT.

Figure 3: Comparison of speed and memory usage during training and imagination. S4WM is the fastest to train, while RSSM-TBTT is the most memory-efficient during training and has the highest throughput during imagination.

Figure 2: Partially observable 3D (Top) and 2D (Bottom) environments for evaluating memory capabilities of world models, including long-term imagination, context-dependent recall, reward prediction, and memory-based reasoning.

**TSSM-XL.** TSSM  is the first Transformer-based world model for improving long-term memory. It was originally evaluated on sequences of length \(\)\(100\). In this paper, we seek to evaluate on much longer sequences (up to \(2000\) steps), and it is impractical to feed the entire sequence to the vanilla Transformer . Therefore, we use Transformer-XL  as the backbone and denote the model as TSSM-XL. It divides the full sequence into chunks, and maintains a cache of the intermediate hidden states from processed chunks. This cache serves as an extended context that allows modeling longer-term dependencies.

**Speed and Memory Usage.** We note that the cache length \(m\) is a crucial hyperparameter of TSSM-XL. A larger \(m\) can potentially improve the memory capacity, at the cost of slower training and higher memory consumption. To ensure a fair comparison in our experiments in the next few sections, we first investigate the speed and memory usage of S4WM, RSSM-TBTT, and TSSM-XL with several \(m\) values. Details can be found in Appendix I. As shown in Figure 3, S4WM and TSSM-XL trains much faster than RSSM-TBTT due to their parallel computation during training, while RSSM-TBTT is much more memory-efficient. For imagination, RSSM-TBTT achieves \(\)\(10\) throughput compared to S4WM and TSSM-XL. While S4WM also uses recurrence during imagination, its multi-layered recurrence structure with MLPs in between slows down its performance. As for memory usage, the decoder takes up most of the memory decoding all steps in parallel, leading to similar memory consumption of all models.

Based on our investigation, TSSM-XL with a cache length \(m=128\) is the closest to S4WM in terms of speed and memory usage. Therefore, we use TSSM-XL with \(m=128\) for all subsequent experiments. We provide a more thorough investigation with larger cache lengths in Appendix B.2.

### Long-Term Imagination

The ability of world models to perform long-term imagination is crucial to long-horizon planning. While many RL benchmarks can be tackled with short-term imagination of \(\)\(15\) steps , here we seek to understand the long-term imagination capability of world models and explore their limits by letting the world models imagine hundreds of steps into the future.

    &  &  &  \\   & Recon. & Gen. & Recon. & Gen. & Recon. & Gen. \\  & MSE (\(\)) & MSE (\(\)) & MSE (\(\)) & MSE (\(\)) & MSE (\(\)) & MSE (\(\)) \\  RSSM-TBTT & \(\) & \(62.2\) & \(\) & \(219.4\) & \(\) & \(323.1\) \\ TSSM-XL & \(2.5\) & \(62.9\) & \(2.4\) & \(224.4\) & \(2.6\) & \(360.4\) \\ S4WM & \(1.8\) & \(\) & \(1.7\) & \(\) & \(1.8\) & \(\) \\   

Table 1: Evaluation of long-term imagination. Each environment is labeled with (context steps | query steps). All models obtain good reconstruction, while S4WM is much better at long-term generation up to \(500\) steps. All models struggle in the Ten Rooms environment.

Figure 4: Long-term imagination in the Four Rooms environment. While RSSM-TBTT and TSSM-XL make many mistakes in wall colors, object colors, and object positions, S4WM is able to generate much more accurately, with only minor errors in object positions.

To this end, we develop three environments with increasing difficulty, namely Two Rooms, Four Rooms, and Ten Rooms, based on the 3D Memory Maze . The top-down views are shown in Figure 2. In the context phase, the data collecting policy starts from a random room, sequentially traverses all rooms, and returns to the starting room. In the query phase, the policy revisits each room in the same order as the context phase.

As shown in Table 1, all models obtain good reconstruction, while S4WM is much better in the Two Rooms and Four Rooms environment for long-term generation up to \(500\) steps. We demonstrate the superior generation quality of S4WM in Figure 4. All models are able to capture the high-level maze layout. However, RSSM-TBTT and TSSM-XL make many mistakes in details such as wall colors, object colors, and object positions, while S4WM is able to generate much more accurately, with only minor errors in object positions. We further show the per step generation MSE in Figure 5. S4WM is able to maintain a relatively good generation quality for up to \(500\) steps, while RSSM-TBTT and TSSM-XL make large generation errors even within \(50\) steps. We notice a periodic drop in the generation MSE. This is when the agent moves from one room to another through a narrow corridor where the action sequence is less diverse.

We also find that all models struggle in the Ten Rooms environment where the context length is \(1101\) and the query length is \(900\). This likely reaches the sequence modeling limits of the S4 model, and we leave the investigation of more sophisticated model architectures to future work.

### Context-Dependent Recall

Humans are able to recall past events in great detail. This has been compared to "mental time travel" . Motivated by this, we develop a "teleport" version of the Two Rooms, Four Rooms, and Ten Rooms environments. After the initial context phase, the agent is teleported to a random point in history, and is asked to recall what happened from that point onwards, given the exact same action sequence that the agent took.

To succeed in this task, the agent needs to figure out where it is teleported by comparing the new observations received after the teleport to its own memory of the past. In other words, the content

Figure 5: Generation MSE per imagination step. Each environment is labeled with (context steps l query steps). S4WM maintains a relatively good generation quality for up to \(500\) steps, while RSSM-TBTT and TSSM-XL make large generation errors even within \(50\) steps.

Figure 6: Evaluation of context-dependent recall in teleport environments. Each environment is labeled with (context steps l query steps). We provide up to \(20\) observations after the teleport as additional contexts. TSSM-XL performs the best in the Two Rooms environment where the context phase is short, and is able to recall successfully without additional observations. When the context phase is longer, S4WM performs the best.

to recall depends on the new observations. Transformers have been shown to be better than S4 at performing such context-dependent operations in low-dimensional sequence manipulation tasks  and synthetic language modeling tasks . We investigate this in the context of world model learning, with high-dimensional image inputs.

To help the model retrieve the correct events in history, we provide up to \(20\) observations after the teleport as additional contexts. The generation MSE of the recall is reported in Figure 6. TSSM-XL performs the best in the Two Rooms environment where the context phase is short, and is able to recall successfully without additional observations. When the context phase is longer as in Four Rooms and Ten Rooms, S4WM performs the best. We visually show the recall quality with \(20\) observations after the teleport in Figure 18. In the Two Rooms environment, both TSSM-XL and S4WM are able to recall accurately. However, only S4WM is able to maintain such recall quality in the more challenging Four Rooms environment.

### Reward Prediction

To facilitate policy learning within imagination, world models need to accurately predict the rewards. In this section, we evaluate the reward prediction accuracy over long time horizons. To decouple the challenges posed by 3D environments from long-term reward prediction, we use the visually simpler 2D MiniGrid  environment.

Specifically, we develop the Distracting Memory environment, which is more challenging than the original MiniGrid Memory environment, due to distractors of random colors being placed in the hallway. A top-down view is shown in Figure 2. Each episode terminates when the agent reaches one of the squares on the right. A reward of \(1\) is given if the square reached is of the same color as the square in the room on the left. Otherwise, no reward is given. In the context phase, the data collecting policy starts in the middle of the hallway, then traverses the hallway and returns to the starting position. In the query phase, the policy goes to one of the two squares on the right uniformly at random. To accurately predict the reward, the world model must learn to ignore the distractors while keeping track of the agent's position.

We report two types of reward prediction accuracy in Table 2. The inference accuracy is measured when the model takes the full sequence of observations as input (including both the context and the query phases). This evaluates the model's ability to capture long-range dependencies independently of the imagination quality. In contrast, the imagination accuracy is evaluated within the model's imagination, conditioned on the observations in the context phase and additionally the action sequence in the query phase.

    &  &  &  \\   & Inference Accuracy (\(\)) & Imagination Accuracy (\(\)) & Inference Accuracy (\(\)) & Imagination Accuracy (\(\)) & Inference Accuracy (\(\)) & Imagination Accuracy (\(\)) \\  TSSM-TBTT & \(47.9\%\) & \(49.6\%\) & \(48.7\%\) & \(48.4\%\) & \(50.4\%\) & \(52.2\%\) \\ TSSM-XL & \(\%\) & \(51.2\%\) & \(99.9\%\) & \(51.3\%\) & \(50.4\%\) & \(51.0\%\) \\ S4WM & \(\%\) & \(\%\) & \(\%\) & \(\%\) & \(\%\) & \(\%\) & \(\%\) \\   

Table 2: Reward prediction accuracy in the Distracting Memory environments. Each environment is labeled with (context steps \(|\) query steps). S4WM succeeds in all environments. TSSM-XL has limited success when observing the full sequence, but fails to predict rewards within imagination. RSSM-TBTT completely fails.

Figure 7: Imagination in the Distracting Memory environment of width \(100\). RSSM-TBTT and TSSM-XL fail to keep track of the agent’s position, leading to inaccurate reward prediction within imagination.

Our results show that only S4WM is able to accurately predict rewards within imagination. TSSM-XL has limited success when observing the full sequence, but fails to imagine future rewards accurately. RSSM-TBTT completely fails, and its reward prediction is close to random guessing. Our visualization of model imagination in Figure 7 reveals that the failure of TSSM-XL and RSSM-TBTT is mainly due to their inability to keep track of the agent's position.

### Memory-Based Reasoning

In the previous experiments, the model's memory of the environment can largely be kept fixed after the context phase. In this section, we explore the setting where the memory needs to be frequently updated in order to reason about the future.

We develop the Multi Doors Keys environment, where the agent collects keys to unlock doors. A top-down view is shown in Figure 2. Each time a door is unlocked, the corresponding key will be consumed, so it cannot be used to unlock other doors of the same color. The agent is allowed to possess multiple keys. In the context phase, the agent visits all keys and doors without picking up any keys. In the query phase, the agent attempts to unlock two random doors after picking up each key. After all keys are picked up, the agent will try to unlock each door once again. To successfully predict the outcome when the agent attempts to unlock a door, the world model must constantly update its memory when a key is picked up or consumed.

Since the environment is visually simple, we find the generation MSE to be a good indicator of how well the model predicts the future door states. As reported in Table 3 and visually shown in Figure 19, S4WM performs well on all environments, demonstrating its ability to keep updating the memory, while both RSSM-TBTT and TSSM-XL struggle.

## 6 Conclusion

In this paper, we introduced S4WM, the first PSSM-based visual world model that effectively expands the long-range sequence modeling ability of S4 and its variants from low-dimensional inputs to high-dimensional images. Furthermore, we presented the first comparative investigation of major world model backbones in a diverse set of environments specifically designed to evaluate critical memory capabilities. Our findings demonstrate the superior performance of S4WM over RNNs and Transformers across multiple tasks, including long-term imagination, context-dependent recall, reward prediction, and memory-based reasoning.

**Limitations and Future Work.** We primarily focused on visually simple and deterministic environments to limit the computation cost and simplify the evaluation process. Future work could explore more sophisticated model architectures and proper evaluation metrics for complex and stochastic environments. In addition, we mainly evaluated imagination quality and did not test world models in conjunction with policy learning. Future work could develop and thoroughly test MBRL agents based on S4WM. To demonstrate the potential of S4WM for policy learning, we provide offline probing results in Appendix D and conduct a skill-level MPC experiment in Appendix E. We find that S4WM outperforms RSSM in offline probing when it is instantiated with S5, and leads to higher task success rates when used for planning. In Appendix G, we additionally demonstrate that the long-term imagination quality can be further improved by instantiating S4WM with S5, showing the potential of our general S4WM framework for incorporating more advanced parallelizable SSMs.

    &  &  &  \\   & Recon. & Gen. & Recon. & Gen. & Recon. & Gen. \\  & MSE (↓) & MSE (↓) & MSE (↓) & MSE (↓) & MSE (↓) \\  RSSM-TBTT & \(0.05\) & \(5.16\) & \(0.04\) & \(6.36\) & \(0.03\) & \(6.28\) \\ TSSM-XL & \(0.05\) & \(1.27\) & \(0.03\) & \(6.05\) & \(\) & \(9.24\) \\ S4WM & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 3: Memory-based reasoning in the Multi Doors Keys environments. Each environment is labeled with (context steps l query steps). S4WM performs well on all environments, while others struggle.