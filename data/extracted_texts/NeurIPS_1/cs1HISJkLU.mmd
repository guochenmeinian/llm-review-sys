# A Versatile Diffusion Transformer with Mixture of Noise Levels for Audiovisual Generation

Gwanghyun Kim\({}^{1,,*}\)&Alonso Martinez\({}^{2}\)&Yu-Chuan Su\({}^{2}\)

Brendan Jou\({}^{2}\)&Jose Lezama\({}^{2}\)&Agrim Gupta\({}^{3,*}\)&Lijun Yu\({}^{4,*}\)

Lu Jiang\({}^{4,*}\)&Aren Jansen\({}^{2}\)&Jacob Walker\({}^{2}\)&Krishna Somandepalli\({}^{2,}\)

\({}^{1}\)Seoul National University \({}^{2}\)Google DeepMind \({}^{3}\)Stanford University \({}^{4}\)Carnegie Mellon University

###### Abstract

Training diffusion models for audiovisual sequences allows for a range of generation tasks by learning conditional distributions of various input-output combinations of the two modalities. Nevertheless, this strategy often requires training a separate model for each task which is expensive. Here, we propose a novel training approach to effectively learn arbitrary conditional distributions in the audiovisual space. Our key contribution lies in how we parameterize the diffusion timestep in the forward diffusion process. Instead of the standard fixed diffusion timestep, we propose applying variable diffusion timesteps across the temporal dimension and across modalities of the inputs. This formulation offers flexibility to introduce variable noise levels for various portions of the input, hence the term _mixture of noise levels_. We propose a transformer-based audiovisual latent diffusion model and show that it can be trained in a task-agnostic fashion using our approach to enable a variety of audiovisual generation tasks at inference time. Experiments demonstrate the versatility of our method in tackling cross-modal and multimodal interpolation tasks in the audiovisual space. Notably, our proposed approach surpasses baselines in generating temporally and perceptually consistent samples conditioned on the input. Project page: avdit2024.github.io

## 1 Introduction

Recent years have witnessed a remarkable surge in the development and exploration of multimodal diffusion models. Prominent examples include text-to-image (T2I) , text-to-video (T2V) . Despite notable advancements, generating sequences across multiple modalities, like video and audio, remains challenging and is an open research area.

Introducing a time axis to static data paves the way for diverse multimodal sequential tasks including cross-modal generation (e.g., audio-to-video), multimodal interpolation, and audiovisual continuation as shown in Fig 1. Each task can be further divided based on various input-output combinations of the modalities, leading to a number of conditional distributions. For example, with video data \(_{0}^{1:N}\) and audio data \(_{0}^{1:N}\) of length \(N\), The complexity of configurations grows with tasks like audiovisual continuation, \(p(_{0}^{(n_{c}+1:N)},_{0}^{(n_{c}+1:N)}|_{0}^{(1:n_{c})}, {y}_{0}^{(1:n_{c})})\), where \(n_{c}\) is the input frame length used for conditioning, and multimodal interpolation, \(p(_{0}^{(n_{x}^{c})},_{0}^{(n_{y}^{c}) }|_{0}^{(n_{x})},_{0}^{(n_{y})})\), where \(_{x}\) and \(_{y}^{(n_{x})}\) are the input-output combinations of the modalities, leading to a number of conditional distributions. For example, with video data \(_{0}^{1:N}\) and audio data \(_{0}^{1:N}\) of length \(N\), the complexity of configurations grows with tasks like audiovisual continuation, \(p(_{0}^{(n_{c}+1:N)},_{0}^{(n_{c}+1:N)}|_{0}^{(1:n_{c})}, {y}_{0}^{(1:n_{c})})\), where \(n_{c}\) is the input frame length used for conditioning, and multimodal interpolation, \(p(_{0}^{(n_{x}^{c})},_{0}^{(n_{y}^{c}) }|_{\(_{y}\) are input index sets. Training separate models for each variation is expensive and impractical. A more efficient training approach would be to learn these conditional distributions in a single model without explicitly enumerating them, i.e., in a task-agnostic manner.

Unconditional diffusion models like MM-Diffusion  show potential for learning conditional distributions implicitly, but rely on inference adjustments . This limits performance, as seen in MM-Diffusion's struggle to generate temporally consistent sequences (see Fig. 2). While UniDiffuser  and Versatile Diffusion  offer methods for joint and conditional text-image distributions, effectively capturing temporal dynamics of audio and video remains an open challenge.

Here, we propose a multimodal diffusion framework that empowers a single model to learn diverse conditional distributions. This paves the way for a versatile framework for multimodal diffusion, tackling various generation tasks. Our core idea is that, applying variable noise levels across modalities and time segments 3 enables a single model to learn arbitrary conditional distributions. This formulation offers flexibility to train diffusion models with a _mixture of noise levels_ i.e., **MoNL**, which introduces variable noise levels across various portions of the input. It has a number of advantages over previous approaches: it requires minimal modifications to the original denoising objective simplifying implementation, task-agnostic training, and support for conditional inference of a given task specification without any inference-time modifications.

We apply this approach for audiovisual generation by developing a diffusion transformer, **AVDiT**. To address the computational complexity of high-dimensional audio and video signals, we implement MoNL in the low-dimensional latent space learned by the MAGVIT-v2  for video and the SoundStream  for audio. Importantly, the temporal structure in these latent representations enables us to apply variable noise levels. We also introduce a transformer-based network for joint noise prediction. Transformers are a natural choice for our implementation due to their proficiency to model multimodal data  capturing complex temporal and cross-modal relationships.

We assess the capability of MoNL to model various distributions in the audiovisual space by evaluating cross-modal tasks (audio-to-video and video-to-audio generation), and conditioning on small portions (audiovisual continuation and interpolation tasks). For these tasks, we show that the AVDiT trained

Figure 1: Our Audiovisual Diffusion Transformer trained with Mixture of Noise Levels tackles diverse AV generation tasks in a single model; see avdit2024.github.io for video demos.

Figure 2: Comparing conditional inference for AV-continuation for MM-Diffusion (left) and Ours (right) on Landscape dataset. Our approach excels at generating temporally consistent sequences.

with MoNL outperforms conventional methods including unconditional and conditional generation models, demonstrating the versatility of our task-agnostic framework as shown in Fig. 1. Notably, qualitative and quantitative evaluations highlight the ability of our framework to generate temporally consistent sequences, as illustrated in Fig. 2.

## 2 Background

Diffusion Models for Multivariate series data:Consider an example of a video diffusion model where the input is a sequence of image frames. In general, this task is modeling multivariate series data (i.e., image representations) of \(d\)-dimensions with \(N\) elements (\(_{0}\). of image frames), henceforth referred to as _time-segments_. Thus, the multivariate series data, \(_{0}=_{0}^{1:N}^{N d} q(_{0})\) can be represented as a sequence of time-segments, where \(x_{0}^{n}^{d}\) is the \(n\)-th time-segment and \(d\)-dimensional representation.

During the forward process of diffusion models [44; 16], the original data \(_{0}\) is corrupted by gradually injecting noise in a sequence of \(T\) timesteps. The noisy data \(_{t}\) at time \(t\) can be written as \(_{t}=_{t}^{1:N}=_{t}}_{0}+_{t}}_{x}\). Here, \(_{x}=_{x}^{1:N}(,)\) is Gaussian noise injected to the sequence and \(_{t}\) is the noise schedule, \(_{t}=1-_{t}\) controls the _noise level_ at each step with \(_{t}=_{i=1}^{t}_{i}\). Each noisy time-segment can be represented as \(x_{t}^{n}=_{t}}x_{0}^{n}+_{t}} _{x}^{n}\). During the reverse process, the data is sampled through a chain of reversing the transition kernel \(q(_{t-1}|_{t})\) that is estimated by \(p_{}(_{t-1}|_{t})=(_{t-1}|(_{t},t),_{t}^{2})\), where \((_{t},t)=_{t}}(_{t}-}{_{t}}}_{}(_{t},t))\). The training objective is to learn a residual denoiser \(_{}\) at each step as:

\[_{}_{t,_{0},_{x}}\|_{ }(_{t},t)-_{x}\|_{2}^{2}, \]

where \(t(\{1,2,,T\})\) is the diffusion timestep.

Multimodal Diffusion Models:Unconditional joint generation (generating all modalities simultaneously) and conditional generation (generating one modality conditioned on the rest) are commonly used for multimodal diffusion. Typically, separate models are trained for each task as described below:

Diffusion models for joint generation.For simplicity, let us assume two modalities \(_{0}\), \(_{0}\). The objective in joint generation is to model the joint data distribution, denoted as \(q(_{0},_{0})\). To learn this, a joint noise prediction network, denoted as \(_{}\) is defined by rewriting Eq. 1 as follows:

\[_{}_{t,_{0},_{0},_{x},_{y}}\|_{}(_{t},_{t},t)-[_{x},_{y}]\|_{2}^{2}, \]

where \((_{0},_{0})\) is a random data point, \([,]\) denotes concatenation, \(_{x},_{y}(,)\), and \(t(\{1,2,,T\})\). Diffusion models trained with this objective can perform conditional sampling \(q(_{0}|_{0})\) using inference-time tricks [18; 40].

Conditional training of diffusion models.To learn conditional distributions, expressed as \(q(_{0}|_{0})\), a noise prediction network \(_{}\) conditioned on \(_{0}\) is adopted from Eq. 2:

\[_{}_{t,_{0},_{0},_{x}}\|_{}(_{t},_{0},t)-_{x}\|_{2}^{2}. \]

Separate conditional models need to be trained for every pair of modalities and input configurations.

## 3 Mixture of Noise Levels (MoNL)

We introduce a novel framework for learning a wide range of conditional distributions within multimodal data by using a mixture of noise levels. The key idea is to formulate the timestep \(t\) (Eq. 1) that determines a noise level in the forward diffusion as a vector. Then, we present representative strategies for variable noise levels. We then show how conditional inference can be performed without additional training. Finally, assembling all these components, we present our versatile audiovisual diffusion transformer (AVDiT).

### Variable Noise Levels across Modality and Time

Formally, let \(M\) represent the number of modalities with sequence representations (latent spaces or raw data). Without loss of generality, assume the representations in each modality have \(N\) time segments4. Let us further assume they have the same embedding dimension \(d\) (which in practice can be achieved by projecting the noisy input from each modality to the desired dimension). The entire sequence can then be simplified as \(_{0}^{M N d}=_{0}^{(1:M,1:N)} q(_{ 0})\), where \(z_{0}^{(m,n)}^{d}\) denotes the \(n\)-th time-segment of \(m\)-th modality. For reference, Sec. 2 represents two modalities of multivariate series data, \(_{0}^{1:N}\) and \(_{0}^{1:N}\), using this notation as \(_{0}^{(1,1:N)}\) and \(_{0}^{(2,1:N)}\), respectively.

We posit that training a single model to support learning arbitrary conditional distributions can be realized by using variable noise levels for each modality \(m\) and time segment \(n\) of the input space \(_{0}\). We introduce the diffusion timestep vector as \(=^{(1:M,1:N)}^{M N}\) to match the dimensionality of the multimodal inputs, where each element \(t^{(m,n)}[1,T]\) determines the timestep, and in turn the level of noise added to the corresponding element \(z_{0}^{(m,n)}\) of the input \(_{0}\).

Recall (from Sec. 2) that in a unimodal case, the goal was to learn the transition kernel \(q(_{t-1}|_{t})\) parameterized by \(p_{}(_{t-1}|_{t})=(_{t-1}|( {x}_{t},t)),_{t}^{2})\). Analogously, by introducing a timestep vector \(^{M N}\), our goal is now to learn a general transition matrix between the various modalities and time-segments in \(_{0}\) at each step:

\[p_{}([z_{t^{(1,1)}-1}^{(1,1)},,z_{t^{(M,N)}-1}^{(M,N)}]|[z_{t^{ (1,1)}}^{(1,1)},,z_{t^{(M,N)}}^{(M,N)}]) \]

Figure 4: Illustration of the conditional inference in our framework for (a) cross-modal generation and (b) multimodal interpolation.

Figure 3: Overview of (a) diffusion training with variable noise levels per time-segment and per modalities, and (b) the mixture of noise levels. Intensity of the color is used to indicate variable noise levels applied to multimodal input. The original input data \(_{0}\) consists of \(M\) modalities and \(N\) time-segments. \(_{0}\) is then perturbed with noise \(\) per a noise level determined by a diffusion timestep vector \(\) to create noisy data \(_{t}\), which is input to the noise prediction network.

Then, for diffusion training, we draw a Gaussian noise sequence \(=^{(1:M,1:N)}\). Each noise element \(^{(m,n)}\) is then added to the corresponding element of the original data \(z_{0}^{(m,n)}\) with noise level determined by \(t^{(m,n)}\) as follows:

\[z_{t^{(m,n)}}^{(m,n)}=_{t^{(m,n)}}}z_{0}^{(m,n)}+_{t^{(m,n)}}}^{(m,n)} \]

Then, the joint and conditional training objectives in Eqs. 2 and 3 can be generalized with a single noise prediction objective to learn the joint distribution \(}\) as follows:

\[_{}_{,_{0},}\|}([z_{t^{(1,1)}}^{(1,1)},,z_{t^{(M,N)}}^{(M,N)}],)-\|_{2}^{2}, \]

where \(_{0} q(_{0})\) is the multimodal input and \(\) is the diffusion timestep vector.

### Representative Stratgies for Variable Noise Levels

Using the generalized view of multimodal noise prediction described in Eq. 6, we now examine various strategies for variable noise levels during the forward diffusion. One can imagine an arbitrarily large number of timestep candidates in the vector space of \(\) drawn as functions of time-segments of the multivariate series and modalities. Here, we explore four designs to create a mixture of noise levels as illustrated in Fig. 3(b). Let us assume we have final diffusion timestep vector for training, \(_{}^{M N}\) where each element \(t_{}^{(i,j)}\) is sampled from \((\{1,2,,T\})\),

* Vanilla: Same timestep is assigned to all the time-segments and modalities. This is analogous to performing joint learning as \(t^{(m,n)}=t_{}^{(1,1)}\), and would be the straightforward way to extend the vanilla distillation approach for the multimodal case.
* Per Modality (Pm): Variable timesteps are assigned for each modality, but all time-segments in a given modality have the same timestep as \(t^{(m,n)}=t_{}^{(m,1)}\). This is expected to promote cross-modal generation tasks. This is a generalization of the UniDiffuser  approach for sequences.
* Per Time-segment (Pt): Variable timesteps are assigned as \(t^{(m,n)}=t_{}^{(1,n)}\) by keeping track of the corresponding time-segments across modalities. Intuitively, this should promote better temporal consistency.
* Per Time-segment and Per-modality (Ptm): Variable timesteps are assigned for each time-segment and modality as \(t^{(m,n)}=t_{}^{(m,n)}\). This would promote better temporal correspondence between modalities.

To enable learning a wide range of conditional distributions, we create a training paradigm where a timestep is uniformly randomly selected from the mixture. Specifically, we refer to this training paradigm as **MoNL**. A schematic of the overall training process is depicted in Fig. 3(a), with related pseudocodes in Algorithms 1 and 2 in the Appendix. Also, theoretical background on MoNL is detailed in Appendix G.

Figure 5: Schematic of (a) the proposed approach, and (b) AV-transformer for joint noise prediction.

### Conditional Inference

Once the general transition kernel \(p_{}\) is learned in Eq. 4, we investigate the model's ability to handle arbitrary conditional distributions. We achieve this by selectively injecting inputs during inference based on the task specification, i.e., clean (no noise) inputs for conditional portions with \(t^{(m,n)}=0\), and noisy inputs for generating desired portions of the input with the current diffusion step \(t^{(m,n)}=t\).

Consider the case of cross-modal generation (Fig. 4(a)), to generate a sequence of \(M-m_{c}\) modalities conditioned on \(m_{c}(1,M)\) modalities, we set timestep elements of \(M-m_{c}\) modalities as \(t\) and those of \(m_{c}\) conditioning modalities as \(0\), which achieves:

\[p_{}_{t-1}^{(m_{c}+1:M,1:N)}|_{t}^{(m_{c}+1:M,1: N)},_{0}^{(1:m_{c},1:N)} \]

Similarly, for multimodal interpolation (Fig. 4(b)), to generate \(N-n_{c}\) time-segments of all modalities jointly, conditioned on \(n_{c}(1,N)\) time-segments, we set the timestep for the \(N-n_{c}\) time-segments as \(t\), and for the conditioning \(n_{c}\) time-segments as \(0\), which achieves \(p_{}_{t-1}^{(1:M,n_{c}+1:N)}|_{t}^{(1:M,n_{c}+1: N)},_{0}^{(1:M,1:n_{c})}\). Unconditional joint generation is also possible by setting each timestep as the same \(t\), to estimate the transition kernel, \(p_{}_{t-1}^{(1:M,1:N)}|_{t}^{(1:M,1:N)}\). Intuitively, our mixture of noise levels is analogous to self-supervised learning which bypasses the need for predefined tasks during training but enables a deeper understanding of multimodal temporal relationships. See also Sec. E for the discussion on classifier-free guidance in the Appendix.

## 4 Audiovisual Latent Diffusion Transformer (AVDiT)

Our model consists of two key components: (1) latent space representations from audio and video autoencoders, and (2) an Audiovisual diffusion transformer (AVDiT) for joint noise prediction.

Latent Space Representations:For a video of \(1+L_{v}\) frames, represented as \(^{(1+L_{v}) H W C}\), we use MAGVIT-v2 , a causal autoencoder to achieve efficient spatial and temporal compression. MAGVIT-v2 results in a low-dimensional representation, \(_{0}^{(1+l_{v}) h w d_{v}}\), by a compression factor of \(r_{s}==\) in space and \(r_{t_{v}}=}{l_{v}}\) in time. Crucially, the use of causal 3D convolutions ensures that the embedding for a given frame is solely influenced by preceding frames, preventing flickering artifacts common in frame-level autoencoders.

For audio with \(L_{a}\) frames, \(^{L_{a}}\), we use SoundStream , a state-of-the-art neural audio autoencoder. We use the latents \(_{0}^{l_{a} d_{a}}\) prior to quantization as audio latents, a compression rate of \(r_{t_{a}}=}{l_{a}}\) in time. The _time-segments_ in our formulation refer to the \(1+l_{v}\) and \(l_{a}\) temporal dimensions in the video and audio latent spaces respectively.

Audiovisual Transformer for Joint Noise Prediction:Transformers  are a natural fit for multimodal generation as they can: (1) efficiently integrate multiple modalities and their interactions [58; 10], (2) capture intricate spatiotemporal dependencies [8; 5], and (3) have shown impressive video generation capabilities [12; 25]. Inspired by these benefits, we introduce AVDiT, a noise prediction network for latent diffusion as described in Fig. 5. AVDiT utilizes the timestep embedding similar to the condition signal used in W.A.L.T . The Transformer first processes the timestep embeddings and positional encodings to create an embedding of the timestep vector. This embedding serves as a conditioning signal and is utilized to dynamically calculate the scaling and shifting parameters for AdaLN during the Transformer Layer Normalization step. This enables the normalization to incorporate the conditioning information of variable noise levels. We first consider the \(l_{a}\) and \(1+l_{v}\) time-dimensions for audio and video embeddings respectively. When applying MoNL, we can easily keep track of the corresponding time segments among the \(l_{a}\) and \(1+l_{v}\) dimensions, given the temporal compression factors in each modality. The noisy latents are then linearly projected matching the final dimension \(d\) by adding appropriate spatiotemporal positional embeddings for video and temporal positional embeddings for audio, resulting in \(d\) dimensional embeddings for each modality which are then concatenated.

## 5 Related Work

**Video diffusion models.** Diffusion models have revolutionized image [45; 35; 16; 39] and video generation with pixel-space [18; 17; 43] and latent-space [14; 56; 6; 9; 12] approaches. Recently, W.A.L.T  pushed the boundaries using transformer-based latent diffusion with joint image-video training. Tackling diverse audio-video generation tasks remains largely unexplored. With an AVDiT trained with MoNL, our unified approach empowers a single model to handle a range of tasks.

**Audio generative models.** Audio generation soared with WaveNet 's autoregressive approach. Adversarial audio generation [27; 42; 26] emerged. Combining this with differentiable quantization [38; 47; 1] led to end-to-end neural codecs for efficient audio compression [21; 57]. Recently, diffusion models joined the fray, some using continuous latent spaces [29; 19; 11], others exploring discrete space . Our AVDiT uses continuous embeddings from SoundStream for audio latents.

**Multimodal generative modeling.** While multimodal diffusion models [37; 41; 54; 39; 17; 23] have achieved impressive results, the field has primarily focused on the visual domain and audiovisual generation remains less explored. Existing approaches for audio-to-video [53; 30; 53] and video-to-audio [20; 33] generation typically learn task-specific conditional models, limiting their flexibility. To address this, recent works [40; 50] propose more versatile audiovisual models. However, they did not examine multimodal interpolation tasks, which we explore in this work. Tasks such as AV-continuation are critical to understand a model's capability to generate a temporally consistent multimodal sequence retaining the object consistency from the condition input.

## 6 Experiments

### Datasets

**Monologues dataset** consists of 19.1 million videos for training and 25K videos for evaluation, each with a single person talking. The videos are center-cropped to a \(256 256\) resolution. This dataset includes a range of person appearances along with rich verbal and non-verbal communication cues. This dataset is ideally situated to assess concepts such as audiovisual gestural synchrony and multimodal expressions which are key components of human communication and interactions.

Figure 6: Full length examples of our AVDiT trained with MoNL on the Monologue dataset. Samples were generated from unseen conditions at 8fps at 128\(\)128 and are shown at the same rate.

Figure 7: Unlike MM-Diffusion (left) where clothes and appearance is altered in the continuation (red arrow), our AVDiT with MoNL (right) maintains subject consistency in the AIST++ dataset.

**AIST++** is a subset of AIST  and contains 1,020 street dance videos (5.2 hours). The videos were segmented into in 8,233 samples for train and 110 for test at 10fps following Ruan et al. . **Landscape** contains natural scenes from 928  videos which were segmented into 5,400 samples for train and 600 samples for test at 10fps. We conduct most of the experiments on the Monologues due to its size and diversity. We use AIST++ and Landscape for comparison with MM-Diffusion.

### Evaluation Settings

**Tasks.** We study three sets of tasks: (1) Joint audio-video (AV) generation (Joint): (2) Cross-modal generation: Audio-to-video (A2V) and Video-to-audio (V2A), and (3) AV interpolation generative tasks: AV-inpaint where a 1.5s clip is interpolated given one video frame, and 0.125s audio at the beginning and four video frames and 0.5s audio at the end, and AV-continue to fill out 1.5 seconds of AV given the first 5 video frames and corresponding 0.625s of audio.

**Baselines.** On the Monologues dataset, we compare the performance of AVDiT trained with MoNL versus three baselines: Vanilla (Eq. 2), Conditional models separately trained for each task, and the per modality model (Sec 3.2) which may be considered as a generalization of the UniDiffuser approach for sequences. We enabled the Vanilla model to generate cross-modal and multimodal interpolation outputs by using the replacement method . We also benchmark MoNL AVDiT against UNet-based MM-Diffusion (MMD) , the sole published work with a released model that tackles both audio and video generation within a single model. While a direct comparison between U-Nets and our transformer architecture is inherently challenging due to their distinct design principles, we show that MoNL AVDiT surpasses this strong U-Net baseline, demonstrating the effectiveness of the transformer architecture in this domain. We restrict our quantitative evaluation to A2V and V2A tasks because MMD fails to generate temporally consistent sequences in case of continuation tasks (see Figs. 2 and 7 for example).

**Quantitative evaluation.** We use Frechet video distance (FVD) as our video evaluation metric following Yu et al. . Similarly, we use Frechet audio distance (FAD) as the audio evaluation metric following Ruan et al. . Because we use latent space representations for the video and audio, we also report the FVD and FAD scores between reconstructed signal and the original signal as "ground-truth" scores as the performance upper bound. While we preferred user studies for assessing audio-video alignment as existing metrics miss subtle synchrony like dance moves matching music beats or gestures aligning with speech patterns, we computed AV-align score , limiting them to the open-domain Landscape dataset for the comparison with MMD.

**User studies.** We conducted user studies to evaluate the quality of generated content. We adopt the two axes of measurement introduced by Ruan et al.  namely audio/video quality and audio-video alignment, and introduce a third one, "subject consistency" to assess whether the person in the generated content is plausibly consistent with the input. For stimuli, we used a total of 360 generated

    &  &  &  &  &  &  \\  & FAD \(\) & FVD \(\) & FVD \(\) & FAD \(\) & FAD \(\) & FVD \(\) & FAD \(\) & FVD \(\) & FAD \(\) & FVD \(\) \\  Conditional (task-specific) & 7.1 & **63.6** & 49.4 & 11.5 & 5.3 & 15.9 & 7.4 & 12.1 & 7.8 & 35.3 \\ Per modality & 7.0 & 84.4samples (not cherry picked) balanced across A2V, V2A, AV-continue and AV-inpaint tasks and for AVDiT trained with three approaches: MoNL, Vanilla and Per modality on Monologues dataset. The tasks were assessed on a 5-point Likert scale. We also compared rater preference for MoNL AVDiT vs. MMD with 30 videos and 5 raters per video. Raters were presented with generations from the two methods randomized as two options, A/B and were asked to pick one option for each of the three dimensions instead of using a 5-point scale. See more details on implementation and experimental setup in Secs. B and C in the Appendix.

### Results

**Qualitative results.** As displayed in Figs. 1 and 6, Our AVDiT model trained with MoNL achieves impressive performance on various tasks within a single framework, including audio-to-video, video-to-audio, joint generation, multimodal continuation and interpolation with flexible input settings, generating temporally consistent videos. Notably, ours preserves clothing and appearance attributes during continuation tasks, unlike MMD which can alter these (see Figs. 2 and 7). More qualitative results and comparisons are available in Figs. 13, 14 and 15, and at avdi2024.github.io.

**Quantitative results.** As shown in Table 1, on average across all tasks, AVDiT trained with MoNL outperforms all baselines, demonstrating its versatility to learn diverse conditional distributions in a task-agnostic manner. MoNL excelled at generating samples that are temporally and perceptually consistent with the conditioning input, in the case of AV-inpaint and AV-continue tasks, where other baselines generally failed. Per-modality approach surpassed MoNL for A2V and V2A tasks consistent with the findings in Bao et al.  likely because conditional distributions in these cases only need to capture cross-modal associations and not necessarily the underlying temporal dynamics. Unsurprisingly, the vanilla diffusion model trained for joint generation exhibited superior performance in this specific scenario but served as a lower-bound of performance for all other tasks. Finally, MoNL performed better than (if not on-par with) task-specific models for all conditional tasks.

As evident from Table 2, MoNL outperformed MMD in terms of the FAD and FVD metrics across all tasks on the AIST++ and Landscape datasets, as estimated using the code provided by Ruan et al. . The significantly better audio generation in our model, likely due to the combination of MoNL and our choice of the SoundStream audio autoencoder, is also reflected in the ground-truth FAD scores for audio reconstruction. In case of video reconstruction quality, (ground-truth FVD) on AIST++, our choice of autoencoder was inferior to MMD, possibly due to the small dataset size. Qualitatively, we observed that the MAGVIT-v2 reconstructions eliminated flickering across frames but the reconstruction of small face regions in AIST++ dance videos was blurry. These findings should be interpreted cautiously due to several factors: the limited size of the AIST++ and Landscape training splits, our use of a transformer backbone versus MMD's coupled U-Nets, and our use of pretrained autoencoders for latent space representations. On the Landscape dataset, AV-align results demonstrate that our model achieves better alignment compared to MMD, which aligns with the findings from the user study below.

**User studies.** A comparison of the distribution of Likert scores across all tasks for the three approaches we compared is shown in Fig. 8. Pairwise Mann-Whitney \(U\) tests were conducted with Bonferroni correction for multiple comparisons to assess statistical difference. Across all axes, raters preferred samples generated from MoNL over that of Vanilla or Per-modality (Pm) approaches. Examining task-specific trends (see Fig. 12 in the Appendix), for the cross-modal tasks, Pm was rated significantly higher than Vanilla, and there was no significant difference between MoNL and Pm (except for the V2A task on AV alignment). For multimodal interpolation tasks, MoNL was rated significantly higher than Pm. In line with quantitative results, these results suggest that MoNL excelled at generating samples, that are perceptually and temporally consistent with the input conditioning.

As indicated in Table 3, our MoNL AVDiT outperformed MMD in user studies, especially in consistency where ours showed improved consistency along factors such as person's appearance

Figure 8: Comparative analysis across AVDiT models from the user study on AV quality, AV alignment and person consistency. The * indicates statistically significant pairwise difference at \(p<0.01\) after multiple correction.

or the attire. MMD was preferred slightly more in V2A tasks, possibly because the Soundstream autoencoder we used for audio was not optimized for music generation like in MMD.

**Ablations.** Recall that MoNL training randomly selects one of the four timestep designs described in Sec. 3.2. We compare MoNL with Per time-segment Pt, and Per time-segment and Per modality Ptm and Pt/Pm/Ptm that excludes vanilla from the timestep mixture, approaches separately as shown in Table 1. Overall, Ptm noise excelled at inpainting and continuation tasks though it was not on par with per-modality approach for cross-modal tasks. In general, Pt does not perform well by itself. In our experiments, we also observed that the combination of Pm, Pt, Ptm and Pt/Pm/Ptm was sufficient for comparable performance on most tasks except for unconditional joint generation. Adding the Vanilla approach to the mixture of timesteps improved performance for unconditional joint generation while not substantially compromising the performance on other tasks.

## 7 Conclusion

We propose a unified approach for multimodal diffusion using a mixture of noise levels (**MoNL**) for generating and manipulating sequences across modalities and time. This empowers a single model to handle diverse tasks like audio-video continuation, interpolation, and cross-modal generation. We show that an audiovisual latent diffusion transformer (**AVDiT**) trained with MoNL achieves state-of-the-art performance in audiovisual-sequence generation, providing new opportunities for expressive and controllable multimedia content creation.

See Sec. A in the Appendix for discussions on limitations and considerations.