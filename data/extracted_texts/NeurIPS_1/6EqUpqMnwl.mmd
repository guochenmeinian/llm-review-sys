# Revisiting Scalarization in Multi-Task Learning:

A Theoretical Perspective

 Yuzheng Hu\({}^{1}\)1 Ruicheng Xian\({}^{1}\)1 Qilong Wu\({}^{1}\)1 Qiuling Fan\({}^{2}\) Lang Yin\({}^{1}\) Han Zhao\({}^{1}\)

\({}^{1}\)Department of Computer Science \({}^{2}\)Department of Mathematics

University of Illinois Urbana-Champaign

{yh46,xrian2,qilong3,qiuling2,langyin2,hanzhao}@illinois.edu

###### Abstract

Linear scalarization, i.e., combining all loss functions by a weighted sum, has been the default choice in the literature of multi-task learning (MTL) since its inception. In recent years, there is a surge of interest in developing Specialized Multi-Task Optimizers (SMTOs) that treat MTL as a multi-objective optimization problem. However, it remains open whether there is a fundamental advantage of SMTOs over scalarization. In fact, heated debates exist in the community comparing these two types of algorithms, mostly from an empirical perspective. To approach the above question, in this paper, we revisit scalarization from a theoretical perspective. We focus on linear MTL models and study whether scalarization is capable of _fully exploring_ the Pareto front. Our findings reveal that, in contrast to recent works that claimed empirical advantages of scalarization, scalarization is inherently incapable of full exploration, especially for those Pareto optimal solutions that strike the balanced trade-offs between multiple tasks. More concretely, when the model is under-parametrized, we reveal a _multi-surface_ structure of the feasible region and identify necessary and sufficient conditions for full exploration. This leads to the conclusion that scalarization is in general incapable of tracing out the Pareto front. Our theoretical results partially answer the open questions in Xin et al. (2021), and provide a more intuitive explanation on why scalarization fails beyond non-convexity. We additionally perform experiments on a real-world dataset using both scalarization and state-of-the-art SMTOs. The experimental results not only corroborate our theoretical findings, but also unveil the potential of SMTOs in finding balanced solutions, which cannot be achieved by scalarization.

## 1 Introduction

When labeled data from multiple tasks are available, multi-task learning (MTL) aims to learn a joint model for these tasks simultaneously in order to improve generalization (Caruana, 1997; Zhang and Yang, 2021; Ruder, 2017). One common approach towards this goal is learning a joint feature representation that is shared among multiple tasks, where each task then uses a separate task-specific head for its own prediction. The hope is that by sharing a joint feature representation, the effective sample size for feature learning will be larger than that for single-task learning, hence leading to better generalization across related tasks. This simple and natural idea has proven to be successful in many applications, including object segmentation (Misra et al., 2016), natural language processing (Collobert and Weston, 2008), autonomous driving (Yu et al., 2020), to name a few.

One typical approach to learning the shared feature model from multiple tasks is called _linear scalarization_, where the learner provides a set of convex coefficients (in the form of a probability vector) tolinearly combine the loss from each task and reduces the learning problem into a scalar optimization problem. Since its inception, scalarization has been the default approach for MTL (Caruana, 1997; Ruder, 2017; Zhang and Yang, 2018; Crawshaw, 2020; Zhang and Yang, 2021), mainly due to its simplicity, scalability, as well as empirical success. However, in recent years, variants of the multiple gradient descent algorithm (MGDA) (Fliege and Svaiter, 2000) have been proposed and used in MTL (Sener and Koltun, 2018; Yu et al., 2020; Chen et al., 2020; Liu et al., 2021; Zhou et al., 2022). Referred to as specialized multi-task optimizers (SMTOs), the main idea behind them is to cast the learning problem explicitly as a multi-objective (multi-criteria) optimization problem, wherein the goal is to enforce convergence to a Pareto optimal solution across tasks. In the literature, a main driving force behind these studies has been the belief that they are better than linear scalarization when _conflicting gradients_ among tasks are present (Yu et al., 2020; Liu et al., 2021). Nevertheless, recent empirical results tell an opposite story (Kurin et al., 2022; Xin et al., 2022; Lin et al., 2022)--with proper choices of hyperparameters and regularization techniques, scalarization matches or even surpasses SMTOs. Hence, it largely remains an open problem regarding whether there is an inherent advantage of using SMTOs for MTL over scalarization, at least from a theoretical perspective.

In this paper, we revisit scalarization from a theoretical perspective. We study whether scalarization is capable of fully exploring the Pareto front in linear MTL. Technically, we aim to understand the following question on _full exploration_:

_For every Pareto optimum \(v\) on the Pareto front induced by multiple tasks, does there exist a weighted combination of task losses such that the optimal solution of the linearly scalarized objective corresponds to \(v\)?_

Our fundamental thesis is that, if the answer to the above question is affirmative, then there is no inherent advantage of SMTOs over scalarization on the representation level; otherwise, there might be cases where SMTOs can converge to Pareto optimal solutions that cannot be obtained via any scalarization objective. Our findings suggest that, in contrast to recent works that claimed empirical advantages of scalarization, it is inherently incapable of full exploration. Specifically, our contributions are as follows:

1. We reveal a _multi-surface_ structure of the feasible region, and identify a common failure mode of scalarization, which we refer to as "gradient disagreement" (Section 3.1). This serves as an important first step towards understanding when and why scalarization fails to achieve a specific Pareto optimum.
2. We provide necessary and sufficient conditions for scalarization to fully explore the Pareto front, which lead to the conclusion that scalarization is incapable of tracing out the Pareto front in general (Section 3.2). To the best of our knowledge, we are the first to establish both necessary and sufficient conditions for full exploration in the non-convex setting.
3. We proceed to discuss implications of our theoretical results (Section 4.1), demonstrating how they answer the open questions in Xin et al. (2022), and provide a remedy solution via randomization to counteract the fundamental limitation of scalarization (Section 4.2).
4. We additionally perform experiments on a real-world dataset using both scalarization and state-of-the-art SMTOs (Section 5). The experimental results not only corroborate our theoretical findings, but also demonstrate the potential of SMTOs in finding balanced solutions.

## 2 Preliminaries

We introduce the setup of multi-task learning (MTL) and relevant notations, along with essential background on the optimal value of the scalarization objective and the concept of Pareto optimality.

Problem setup.Following prior works, we consider multi-task learning with \(k\) regression tasks (Wu et al., 2020; Du et al., 2021). The training examples \(\{(x_{j},y_{1}^{j},,y_{k}^{j})\}_{j=1}^{n}\) are drawn i.i.d. from some \((_{1}_{k})\), where \(^{p}\) is the _shared_ input space and \(_{i}\) is the output space for task \(i\). We denote the label of task \(i\) as \(y_{i}=(y_{i}^{1},,y_{i}^{n})^{}^{n}\), and concatenate the inputs and labels in matrix forms: \(X=[x_{1},,x_{n}]^{}^{n p}\) and \(Y=[y_{1},,y_{k}]^{n k}\).

We focus on linear MTL, in which a linear neural network serves as the backbone for prediction. Despite its simplicity, this setting has attracted extensive research interest in the literature (Maurer et al., 2006; Maurer et al., 2016; Wu et al., 2020; Du et al., 2021; Tripuraneni et al., 2021). Without loss of generality, the model that will be used for our study is a two-layer linear network3, which consists of a shared layer \(W^{p q}\) and a task-specific head \(a_{i}^{q}\) for each task. The prediction for task \(i\) on input \(x\) is given by \(f_{i}(x,W,a_{i}) x^{}Wa_{i}\). We adopt Mean Squared Error (MSE) as the loss function, and the training loss for task \(i\) is given by \(L_{i}(W,a_{i})\|XWa_{i}-y_{i}\|_{2}^{2}\).

Notation.We use \([k]\) to denote the index set \(\{1,,k\}\). For two square matrices \(M\) and \(N\) with the same dimension, we write \(M N\) if \(M-N\) is positive semi-definite, and \(M N\) if \(M-N\) is positive definite. We denote \((D)\) as the diagonal matrix whose diagonal entries are given by \(D=\{d_{i}\}_{i[k]}\), a set of real numbers; and \((V)\) as the linear space spanned by \(V=\{v_{i}\}_{i[k]}\), a set of vectors. For two vectors with the same size, we use \(\) and \(\) to denote entry-wise multiplication and division. When there is no ambiguity, operations on vectors (e.g., absolute value, square square root) are performed entry-wise. For any matrix \(M\), \(M^{}\) stands for its Moore-Penrose inverse. We use \((M)\) and \((M)\) to denote its column space and rank, respectively. The expression \(\|\|\) denotes the \(_{2}\) norm of a vector, \(,\) the inner product of two vectors, \(\|\|_{F}\) the Frobenius norm of a matrix, and \(()\) the spectral radius of a square matrix.

Optimal value of scalarization (restated from Wu et al. (2020)).The objective function of scalarization is a weighted sum of losses: \(_{i=1}^{k}_{i}L_{i}(W,a_{i})\). Here, the convex coefficients \(\{_{i}\}_{i[k]}\) satisfy \(_{i} 0\) and \(_{i}_{i}=1\), and are typically obtained via grid search (Vandenhende et al., 2021). For each task, we can decompose its loss based on the orthogonality principle (Kay, 1993)

\[L_{i}(W,a_{i})\|XWa_{i}-y_{i}\|_{2}^{2}=\|XWa_{i}-_{i}\|^{2}+ \|_{i}-y_{i}\|^{2}, \]

where \(_{i}=X(X^{}X)^{}X^{}y_{i}\) is the optimal predictor for task \(i\). Denote \(=(\{}\}_{i=1}^{k})\), \(A=[a_{1},,a_{k}]^{q k}\), \(=[_{1},,_{k}]^{n k}\). The scalarization objective can be compactly expressed as

\[\|(XWA-Y)\|_{F}^{2}=\|(XWA-)\|_{F}^{2}+\|(-Y) \|_{F}^{2}. \]

Throughout this paper, we assume the optimal linear predictions \(\{_{i}\}_{i[k]}\) are linearly independent.4 This leads to two distinct cases, depending on the width of the hidden layer \(q\):

1. When \(q k\), the minimum value of the first term is zero, i.e., every task can simultaneously achieve its optimal loss \(\|_{i}-y_{i}\|^{2}\).
2. When \(q<k\), the minimum value of the first term is given by the rank-\(q\) approximation. In particular, let \(=_{i=1}^{k}_{i}u_{i}v_{i}^{}\) be the SVD of \(\). According to the Eckart-Young-Mirsky Theorem (Eckart and Young, 1936), the best rank-\(q\) approximation of \(\) under the Frobenius norm is given by the truncated SVD: \(_{q,}_{i=1}^{q}_{i}u_{i}v_{i}^{}\). Hence, the minimum training risk is \(\|(_{q,}-)\|_{F}^{2}+\|(-Y)\|_{F}^{2}\). This regime, referred to as the _under-parameterized_ regime, constitutes the main focus of our study.

Pareto optimality.In the context of MTL, a point \(^{*}\) (shared model parameters) is a _Pareto optimum_ (PO) if there does not exist a \(^{}\) that achieves strictly better performance on at least one task and no worse performance on all other tasks compared to \(^{*}\). Formally, for a set of objectives \(\{L_{i}\}_{i[k]}\) that need to be minimized, if \(L_{i}(^{})<L_{i}(^{*})\) for some point \(^{}\), then there must exist another task \(j\) such that \(L_{j}(^{})>L_{j}(^{*})\). The _Pareto front_ (a.k.a. Pareto frontier) is the collection of objective values \((L_{1}(),,L_{k}())\) corresponding to all PO.

## 3 Linear scalarization through the lens of exploring the Pareto front

In this section, we provide a fine-grained analysis of the feasible region (formally defined in Section 3.1), revealing its multi-surface structure. We proceed to analyze whether scalarization is capable of tracing out the Pareto front, and identify necessary and sufficient conditions for full exploration.

**Reformulation.** Recall that minimizing Equation (1) is equivalent to minimizing the squared loss \(\|XWa_{i}-_{i}\|^{2}\), where \(_{i}=X(X^{}X)^{}X^{}y_{i}\) is the optimal linear predictor for task \(i\). Additionally, the task-specific heads can be optimized independently for each task. Thus, if we let \(Z=XW\) and \(a_{i}=(Z^{}Z)^{}Z^{}_{i}\) (the optimal task-specific head), then minimizing Equation (1) is equivalent to

\[_{Z=XW}\|Z(Z^{}Z)^{}Z^{}_{i}-_{i}\|^{2}, \]

which can be further transformed into

\[_{P_{Z}}\,_{i}^{}P_{Z}_{i}, P_{Z}=Z(Z^{}Z)^{ }Z^{}.\]

Therefore, the original MTL problem can be reformulated into the following multi-objective problem

\[_{P_{Z}}(_{1}^{}P_{Z}_{1},,_{k}^{} P_{Z}_{k}), \]

where \(P_{Z}\) is a projection matrix with rank at most \(q\) (i.e., hidden layer width) and maps to the column space of \(X\). Each quadratic term can be further expressed as \(\|P_{Z}_{i}\|^{2}\). From here it is clear (also see Section 2) that when \(q k\) (the over-parametrized regime), the \(k\) quadratic terms can be maximized simultaneously, meaning that the network has sufficient capacity to fit the target tasks. In this case, the Pareto front reduces to a _singleton_, and can be achieved by scalarization with any choice of convex coefficients. In fact, this holds true even for general non-linear networks as long as it is wide enough, i.e., over-parametrized, and we defer more discussions to Appendix A.2.

The main focus of our study, in contrast, lies in the _under-parametrized regime_\(q<k\), where various tasks compete for the network's capacity for better prediction. We comment that this regime has been the focus of previous work and demonstrated to be beneficial in MTL (Wu et al., 2020; Wang et al., 2022). We study two extreme cases within the under-parameterized regime, \(q=1\) and \(q=k-1\). By doing so, we anticipate that our results will hold for other \(k\)'s in between.

### The multi-surface structure of the feasible region

The objective function of task \(i\) is \(\|P_{Z}_{i}\|^{2}\), i.e., the square of the projection of \(_{i}\) on \(P_{Z}\). Since we are interested in the Pareto front, we can assume w.l.o.g. that \((P_{Z})=q\) and \((P_{Z})(\{_{i}\}_{i[k]})\) (otherwise there is a loss of model capacity, and the corresponding point will no longer be a PO). Consequently, we define the feasible region as

\[_{q}:=\{(}^{}P_{Z}_{1},,}^{}P_{Z}_{k})^{}\,P_{Z}^{2}=P_{Z},\, (P_{Z})=q,\,(P_{Z}) (\{_{i}\}_{i[k]})\}. \]

We now provide a fine-grained characterization of \(_{1}\) (corresponding to \(q=1\)) in the following theorem, which also serves as the basis for \(q=k-1\).

**Theorem 3.1**.: _Let \(G=^{}\) and \(Q=G^{-1}\). Define_

\[Q_{i_{1} i_{l}}:=D_{i_{1} i_{l}}QD_{i_{1} i_{l}}, 0  l k,\,1 i_{1}<<i_{l} k, \]

_where \(D_{i_{1} i_{l}}\) is a diagonal matrix with all diagonal entries equal to \(1\), except for the entries at positions \(i_{1},,i_{l}\) which are equal to \(-1\). Consider the following surface parameterized by \(v\):_

\[E_{i_{1} i_{l}}:=\{v:^{}Q_{i_{1} i_{l}}= 1\}. \]

Figure 1: Feasible MSEs of a three-task linear MTL problem with \(q=1\), formed by a union of multiple feasible MSEs surfaces defined in Theorem 3.1. Note that the balanced PO located at the intersection of the surfaces is not achievable by scalarization. See Appendix A.1 for implementation details.

Figure 2: When restricted to a single surface, each contains a (different) optimum point, and the gradients (w.r.t. minimizing total MSE) on each surface disagree at their intersection.

_We have:_

\[_{1}=_{0 l k}_{1 i_{1}<<i_{l} k}E_{i _{1} i_{l}}, \]

_i.e., \(_{1}\) is the union of \(2^{k-1}\) surfaces in the non-negative orthant (note \(E_{i_{1} i_{l}}=E_{ i_{l}}}\) by symmetry)._

To help the readers gain some intuitions on the _multi-surface structure_ demonstrated in Theorem 3.1, we visualize the feasible region of a three-task (i.e., \(k=3\)) example in Figure 1 (more details at the end of the subsection). We also provide a proof sketch below (detailed proof in Appendix A.3).

Proof sketch of Theorem 3.1.A rank-\(1\) projection matrix can be expressed as \(ss^{}\) with \(\|s\|=1\). The feasible region can therefore be equivalently characterized as

\[_{1}=\{(_{1},s^{2},, _{k},s^{2})^{}\ \|s\|=1,\ s(\{_{i}\}_{i[k]})\}. \]

Define \(v_{i}=_{i},s\) and \(v=(v_{1},,v_{k})^{}\). We will show the set of \(v\) forms the boundary of an ellipsoid determined by \(Q\). As a consequence, the set of \(|v|\) is the union of multiple surfaces, each corresponding to the boundary of an ellipsoid (reflection of \(Q\)) in the non-negative orthant. The reflection is represented by the diagonal matrix \(D_{i_{1} i_{l}}\) in the theorem statement. Finally, \(_{1}\), which is the set of \(v^{2}\), can be obtained by squaring the coordinates in the previous set. This finishes the proof of Theorem 3.1.

Since the orthogonal complement of a \(1\)-dimensional subspace of \((\{_{i}\}_{i[k]})\) is a \((k-1)\)-dimensional subspace, we immediately have the following result (proof deferred to Appendix A.4).

**Theorem 3.2**.: _Let \(t:=(\|_{1}\|^{2},,\|_{k}\|^{2})^{}\). Consider the following surface parametrized by \(v\):_

\[I_{i_{1} i_{l}}=\{v:^{}Q_{i_{1} i_{l}}=1\}. \]

_We have:_

\[_{k-1}=_{0 l k}_{1 i_{1}<<i_{l} k }I_{i_{1} i_{l}}, \]

_i.e., \(_{k-1}\) is the union of \(2^{k-1}\) surfaces in the non-negative orthant (\(I_{i_{1} i_{l}}=I_{ i_{l}}}\) by symmetry)._

Theorems 3.1 and 3.2 not only offer us a glimpse into the shape of the feasible region but also provide valuable insights into the fundamental limitation of linear scalarization. The key observation is that, scalarization fails to reach the _intersection_ points of two (or more) surfaces when the gradients with respect to the two (or more) surfaces disagree (i.e., they lie in different directions). We refer to this phenomenon as _gradient disagreement_, and provide an illustrating example in Figure 2. Essentially, the multi-surface structure that we have revealed implies the abundance of intersection points on the feasible region \(_{q}\). Moreover, if any of these points were to be an PO, scalarization would not be able to trace out the Pareto front (see Appendix A.5 for a detailed explanation).

We illustrate this observation using a concrete example with three tasks in Figure 1. The feasible region consists of four surfaces, among which \(E_{}\) is dominated by other surfaces. The intersection of \(E_{1},E_{2},E_{3}\) (the red point) is a PO that achieves balanced performance across tasks, yet scalarization cannot reach this point since it lies in a valley.

### Necessary and sufficient conditions for full exploration

In this section, we provide an in-depth analysis of the Pareto front, which is a subset of the feasible region. Concretely, we develop necessary and sufficient conditions for scalarization to fully explore the Pareto front.

We first review some important concepts that will be frequently used in this section. Denote \(:=\{Q_{i_{1} i_{l}}\ |\ 0 l k,\ 1 i_{1}<<i_{l} k\}\) as the collection of matrices which define the surfaces in the previous section. Denote \(:=\{G_{i_{1} i_{l}}=Q_{i_{1} i_{l}}^{-1}\ |\ 0 l  k,\ 1 i_{1}<<i_{l} k\}\). Note \(G_{i_{1} i_{l}}=D_{i_{1} i_{l}}^{}D_{i_{1}  i_{l}}=(D_{i_{1} i_{l}})^{}(D_{i_{1} i_{l}})\). Therefore, \(\) is a collection of gram matrices which measure the _task similarity_, and each element is obtained by flipping the signs of some optimal predictors. We use \(\) and \(\) to denote the collection of surfaces defined in Equations (7) and (10), respectively. We also formally define a class of matrix as follows.

**Definition 3.3** (Doubly non-negative matrix).: _A doubly non-negative matrix \(M\) is a real positive semi-definite (PSD) matrix with non-negative entries._

Our analysis begins with an observation that the distribution of PO across different surfaces plays a vital role in the success of scalarization. Specifically, we want to avoid the "bad" case like the one demonstrated in Figure 1, where a PO lies on the intersection of multiple surfaces. A natural solution to circumvent this issue is to force the Pareto front to lie on a _single_ surface. We therefore draw our attention to this particular scenario, and provide a necessary and sufficient condition for such configuration in the following lemma.

**Lemma 3.4**.: _When \(q=1\), the Pareto front of the feasible region belongs to a single surface \(E^{*}\), if and only if the corresponding \(G^{*}=(Q^{*})^{-1}\) is doubly non-negative. We refer to the existence of such a doubly non-negative \(G^{*}\) in \(\) as \(\)1._

We outline the proof sketch below. The detailed proof can be found in Appendix B.1.

Proof sketch of Lemma 3.4.For the _if_ part, we employ a geometric argument. Specifically, denote the acute cone formed by the optimal predictors as \(\). We will show that, if \(s\) (a unit vector in Equation (9)) does not belong to the dual cone \(^{*}\), then its reflection w.r.t. the dual cone \(s^{}\) is a better point than \(s\). As a consequence, we can remove the absolute value from the proof sketch of Theorem 3.1. This further implies that the Pareto front belongs to a single surface.

For the _only if_ part, we consider the points which achieve maximum value along an axis. We will show these points are PO. Finally, we will show that if \(\)1 does not hold, then these points must belong to different surfaces in \(\).

We have a similar result for \(q=k-1\). The details of the proof are deferred to Appendix B.2.

**Lemma 3.5**.: _When \(q=k-1\), the Pareto front of the feasible region belongs to a single surface \(I^{*}\), if and only if the corresponding \(Q^{*}\) is doubly non-negative. We refer to the existence of such a doubly non-negative \(Q^{*}\) in \(\) as \(\)2._

Lemmas 3.4 and 3.5 provide a refined characterization of how the PO is distributed within the feasible region: unless \(\)1 (resp. \(\)2) holds, the Pareto front will belong to multiple surfaces in \(\) (resp. \(\)). This easily leads to the existence of a PO on the intersection of different surfaces (see Figure 1), which will render the failure of scalarization as a consequence. It is therefore natural for us to conjecture that \(\)1 (resp. \(\)2) is necessary for full exploration. We make the following assumption regarding the topology of the Pareto front.

**Assumption 3.6**.: _For \(q=1\) and \(q=k-1\), we assume the Pareto front is path-connected, i.e., for every pair of PO \(x\) and \(y\), there exists a continuous curve on the Pareto front with \(x\) and \(y\) being the two endpoints._

Assumption 3.6, combined with Lemma 3.4 (resp. Lemma 3.5), will help locate a PO on the intersection of two surfaces when \(\)1 (resp. \(\)2) does not hold. We make an additional technical assumption to circumvent degenerated cases.

**Assumption 3.7**.: _We assume \(P\) (the intersection point) is a relative interior point (Rockafellar, 1997, Section 6) of the Pareto front._

We are now ready to present the main results in this section (full proofs in Appendices B.3 and B.5).

**Theorem 3.8** (Necessity of \(\)1 (resp. \(\)2)).: _For \(q=1\) (resp. \(q=k-1\)), under Assumptions 3.6 and 3.7, \(\)1 (resp. \(\)2) is a necessary condition for scalarization to fully explore the Pareto front._

Proof sketch of Theorem 3.8.Assumption 3.7 allows us to perform dimensionality analysis in a small neighborhood of the intersection point \(P\), from which we can demonstrate the existence of a PO lying on the intersection of two surfaces with gradient disagreement. This point cannot be achieved by linear scalarization.

**Remark 3.9**.: _Assumptions 3.6 and 3.7 are mainly used to simplify analysis, and they are not necessary for Theorem 3.8 to hold. As a matter of fact, we can show the necessity of \(\)1 (resp. \(\)2) when \(k=3\) without these assumptions. We refer the readers to Appendix B.4 for more details._

Finally, without relying on the above assumptions, we can show that \(\)1 and \(\)2 are sufficient.

**Theorem 3.10** (Sufficiency of C1 (resp. C2)).: _For \(q=1\) (resp. \(q=k-1\)), C1 (resp. C2) is a sufficient condition for scalarization to fully explore the Pareto front._

Proof sketch of Theorem 3.10.For \(q=1\), we consider the tangent plane at a PO and attempt to show it lies above the feasible region. This is transformed into demonstrating the positive semi-definiteness of a matrix, which can be conquered by applying the Perron-Frobenius theorem (Shaked-Monderer and Berman, 2021, Section 1.3) on non-negative matrices. For \(q=k-1\), we show the boundary of the feasible region is concave, and apply the supporting hyperplane theorem (Boyd et al., 2004, Section 2.5.2) to conclude the proof.

## 4 Discussion

We discuss the implications of our theoretical results, and provide a remedy solution for linear scalarization to address its inability of full exploration.

### Implications of the theoretical results

Fundamental limitation of linear scalarization.From Theorem 3.8, it is clear that linear scalarization faces significant challenges in fully exploring the Pareto front in the case of \(q=1\) and \(q=k-1\), since both C1 and C2 require the existence of a doubly non-negative matrix, which is a very small subset of the PSD matrix. To illustrate this point, we consider a simplified probabilistic model: when the optimal predictors \(\{_{i}\}_{i[k]}\) are drawn independently from a symmetric distribution centered at the origin, the probability that C1 holds is \(2^{-O(k^{2})}\), which decays exponentially with the number of tasks. Thus, we comment that C1 and C2 in general does not hold in practical cases.

Towards understanding when and why scalarization fails.Among the vast literature that studies Pareto optimality in MTL, there is a noticeable absence of a systematic and rigorous exploration of _when and why scalarization fails to reach a Pareto optimum_. Prior work typically resorts to hypothetical examples (e.g., Figure 4.9 in (Boyd et al., 2004)) to demonstrate the failure of scalarization. Our study, which identifies the multi-surface structure of the feasible region and reveals the phenomenon of gradient disagreement in a concrete setting, serves as a pioneering step in addressing this gap in the literature.

A closer examination of the failure mode.For the extreme under-parametrized case (\(q=1\)), on the one hand, we can prove that scalarization is able to achieve points which attain maximum value along one direction (i.e., best performance on one task). On the other hand, by examining Figure 1, we shall see that scalarization is incapable of exploring the interior points (i.e., balanced performance across all tasks). This suggests that scalarization has the tendency to _overfit_ to a small fraction of tasks, and fails to strike the right balance between different tasks. We provide more empirical evidence to support this argument in Section 5.

Criteria for practitioners.Theorem 3.10 demonstrates that C1 and C2 are sufficient for scalarization to fully explore the Pareto front. As such, these conditions can serve as valuable criteria for practitioners when deciding which tasks to combine in linear MTL. Note a naive enumeration of \(\) or \(\) will lead to a time complexity of \(O(2^{k})\). We instead provide an \(O(k^{2})\) algorithm checking whether C1 holds (Algorithm 1 in Appendix C). In essence, the algorithm aims to find a set of flipping signs \(\{s_{i}\}_{i[k]}\{+,-\}\) that ensure positive or zero correlation between all pairs in \(\{s_{i}_{i}\}_{i[k]}\). It scans through the predictors in ascending order and either determines the signs \(s_{j}\) for each optimal predictor \(_{j}\) if not determined yet, or checks for conflicts between signs that are already determined. A sign can be determined when the predictors \(_{i}\) and \(_{j}\) are correlated. A conflict indicates the impossibility of finding such a set of flipping signs, thus falsifying C1. A similar algorithm can be devised for C2 with an additional step of Cholesky decomposition beforehand, leading to an \(O(k^{3})\) time complexity.

Open questions in Xin et al. (2022).We (partially) answer the open questions in Xin et al. (2022).

Q1. "Is it the case that neural networks trained via a combination of scalarization and standard first-order optimization methods are not able to reach the Pareto Frontier?"

A1. For Linear MTL, our answer is a definitive "Yes". We demonstrate that linear scalarization with arbitrary convex coefficients cannot trace out all the Pareto optima in general. Our results are strong in the sense that they hold on a _representation_ level, meaning that they are independent of the specificoptimization algorithm employed. This suggests that the weakness we have revealed in scalarization is fundamental in nature.

Q2. "Is non-convexity adding additional complexity which makes scalarization insufficient for tracing out the Pareto front?"

A2. Yes, even some mild non-convexity introduced by two-layer linear networks will render scalarization insufficient. On the other hand, we do believe the multi-surface structure that we have unveiled in Section 3.1 is both universal and fundamental, which also contributes to the observed non-convexity in practical scenarios. In essence, what we have done partly is to penetrate through the surface and gain a holistic view of the feasible region. Notably, the key obstacle in linear scalarization, as revealed by our analysis, lies in the phenomenon of _gradient disagreement_. This cannot be readily discerned by solely examining the surface.

We make two additional comments to further clarify our standing point in comparison to this work.

1. We do not aim to refute the claims in Xin et al. (2022); as a matter of fact, we consider their empirical findings highly valuable. Instead, we would like to emphasize a discrepancy between their theory and experiments. Given that we have shown the conclusion--that linear scalarization can fully explore the PF--no long holds in the non-convex setting, their theory does not account for their experiments on modern neural networks. Therefore, we urge the research community to develop a new theory elucidating the empirical efficacy of linear scalarization.
2. While the differing settings render the results from these two papers non-comparable, they collectively offer a broader perspective on the pros and cons of scalarization and SMTOs. We believe this is of great significance to both researchers and practitioners.

### A remedy solution

Given the fundamental weakness of linear scalarization in fully exploring the Pareto front, one might wonder whether there exists any model class where linear scalarization is sufficient to fully explore the Pareto front. The answer is affirmative if we allow ourselves to use _randomized multi-task neural networks_. More specifically, given two MTL networks \(f^{(0)}\) and \(f^{(1)}\) over the same input and output spaces and a fixed constant \(t\), we can construct a randomized MTL network as follows. Let \(S U(0,1)\) be a uniform random variable over \((0,1)\) that is independent of all the other random variables, such as the inputs. Then, consider the following randomized neural network:

\[f(x):=f^{(0)}(x)&S t\\ f^{(1)}(x)& \]

Essentially, the randomized network constructed by Equation (12) will output \(f^{(0)}(x)\) with probability \(t\) and \(f^{(1)}(x)\) with probability \(1-t\). Now if we consider the loss of \(f()\), we have

\[_{}_{S}[\|f(X)-Y\|_{2}^{2}] =_{S}_{}[\|f(X)-Y\|_{2}^{2}]\] \[=t_{}[\|f(X)-Y\|_{2}^{2} S t]+(1-t)_{}[\|f(X)-Y\|_{2}^{2} S>t]\] \[=t_{}[\|f^{(0)}(X)-Y\|_{2}^{2}]+(1-t)_{ }[\|f^{(1)}(X)-Y\|_{2}^{2}], \]

which means that the loss of the randomized multi-task network is a convex combination of the losses from \(f^{(0)}()\) and \(f^{(1)}()\). Geometrically, the argument above shows that randomization allows us to "convexify" the feasible region and thus the Pareto front will be convex. Now, by a standard application of the supporting hyperplane theorem (Boyd et al., 2004, Section 2.5.2) we can see that every point on the Pareto front can be realized via linear scalarization. We empirically verify the effectiveness of randomization in Appendix D.3.

We comment that randomization is a powerful tool that has wide applicability in machine learning. For instance, it has achieved great success in online learning, specifically in the Follow-the-Perturbed-Leader algorithm (Kalai and Vempala, 2005). It also appears in the literature of fairness to achieve the optimal regressor or classifier (Zhao and Gordon, 2022; Xian et al., 2023). Here we further demonstrate that it can be applied to MTL to facilitate the exploration of linear scalarization. In essence, randomization increases the representation power of the original neural network.

Experiments

In this section, we corroborate our theoretical findings with a linear MTL experiment, where we show that linear scalarization fails to fully explore the Pareto front on a three-task problem that does not satisfy \(\) with \(q=1\). In comparison, SMTOs can achieve balanced Pareto optimum solutions that are not achievable by scalarization.

Dataset and setup.We use the SARCOS dataset for our experiment (Vijayakumar and Schaal, 2000), where the problem is to predict the torque of seven robot arms given inputs that consist of the position, velocity, and acceleration of the respective arms. For ease of visualization, we restrict the tasks to arms 3, 4, and 5; in particular, they do not satisfy \(\).

Our regression model is a two-layer linear network with hidden size \(q=1\) (no bias). To explore the portion of the Pareto front achievable by linear scalarization, we fit 100,000 linear regressors with randomly sampled convex coefficients and record their performance. These are compared to the solutions found by SMTOs, for which we use MGDA (Desideri, 2012) and MGDA-UB (Sener and Koltun, 2018). Specifically, to guarantee their Pareto optimality, we run the SMTOs with full-batch gradient descent till convergence (hence each SMTO method has one deterministic solution). Additional details, including hyperparameter settings, can be found in Appendix D.1.

Results.In Figure 3, we plot the MSEs achieved by scalarization under randomly sampled weights. We observe that the solutions achieved by scalarization are concentrated near the vertices and partly along the edges, indicating their tendency to overfit to 1-2 of the tasks. Scalarization also fails to explore a large portion of the Pareto front in the center--which have more balanced performance across task; the inability of full exploration corroborates the necessary condition (Theorem 3.8) discussed in Section 3. In contrast, SMTOs are capable of finding solutions not achievable by scalarization that are located in the center of the Pareto front. This showcases the relative advantage and practical utility of SMTOs in MTL over scalarization, especially in scenarios where equitable performance on all tasks are desired. To strengthen our results, we experiment with multiple random initializations and observe consistent results (see Appendix D.2).

Discussion.We would like to note that we are not endorsing any specific SMTO algorithm, such as MGDA or MGDA-UB. As a matter of fact, we hypothesize that no single algorithm is universally effective (also see (Chen et al., 2023)), notably in overcoming the limitation of scalarization in terms of full exploration. Instead, by revealing the limitation of scalarization and the potential benefits of _some_ SMTO algorithms, we aim to strengthen SMTO research, challenge the notion that linear scalarization is sufficient, and advocate for a more balanced progression in the field of MTL.

## 6 Limitations and future directions

Limitations.There are two major limitations of our work. First, while the techniques we develop in this paper are highly non-trivial, we only cover the two extreme cases \(q=1\) and \(q=k-1\) in the under-parametrized regime, and the analysis for general \(q<k\) is missing. Second, the setting of our study is relatively restricted. Specifically, we focus on linear MTL where different tasks share the same input. A more realistic setting should factor in different inputs for different tasks (Wu et al., 2020), as well as some non-linearity in the model architecture (Kurin et al., 2022).

Future directions.We expect the geometry of the feasible region to transition smoothly as we increase \(q\), meaning that it will not abruptly collapse into a _single_ surface. Providing a rigorous basis for this assertion would facilitate the analysis for general \(q\)'s, thereby complementing our study. Regarding the assumption of shared input, while our analysis persists under the 'proportional covariance assumption' adopted by Wu et al. (2020), we find this assumption excessively restrictive,

Figure 3: Linear scalarization has the tendency to overfit to a subset of tasks by finding _peripheral_ blue points of the Pareto front. MGDA (magenta) and MGDA-UB (red) converge to more balanced PO in the _interior_.

and hope future work could relax it further. The generalization to neural networks with ReLU activation is interesting but also challenging, and it is unclear whether the techniques and results developed in this paper can be extended to the non-linear setting. Another interesting avenue to explore is interpreting the multi-surface structure we have uncovered, which we hypothesize to have connections with game theory. Finally, our paper dedicates to demonstrating the weaknesses of linear scalarization, while we consider it an important future direction to theoretically analyze the advantage of SMTOs, as partially revealed in our experiments. In all, we hope our study can initiate a line of works on MTL, which provide theoretical justifications for the usage of specific algorithms beyond empirical evaluations across various settings.

## 7 Related work

SMTOs.There is a surge of interest in developing specialized multi-task optimizers (SMTOs) in the literature. One of the earliest and most notable SMTOs is the multiple-gradient descent algorithm (MGDA) (Desideri, 2012). For each task, MGDA solves an optimization problem and performs updates using the gradient of the objective function with both shared and task-specific parameters. However, MGDA cannot scale to high dimension (e.g., neural networks), and computing the gradients for every task is prohibitively expensive. To counter these drawbacks, Sener and Koltun (2018) introduce a scalable Frank-Wolfe-based optimizer which is more efficient. Another prominent issue in MTL is that the gradients from multiple tasks could conflict with each other and the performance of some tasks could be significantly affected. To this end, Yu et al. (2020) propose _gradient surgery_ to mitigate the negative interference among tasks; Chen et al. (2020) and Liu et al. (2021) propose to manipulate the gradients or the gradient selection procedures; Fernando et al. (2022) devise a stochastic gradient correction method and prove its convergence. More recently, Navon et al. (2022) cast the gradient update as a Nash bargaining game, yielding well-balanced solutions across the PF.

Empirical comparison of SMTOs and scalarization. A recent line of works empirically compare SMTOs and linear scalarization. Their central claim is that the performance of linear scalarization is comparable with the state-of-the-art SMTOs despite the added complexity of SMTOs. Xin et al. (2022) argue that multi-task optimization methods do not yield any improvement beyond what can be achieved by linear scalarization with carefully-tuned hyperparameters; Kurin et al. (2022) show that the performance of linear scalarization matches or even surpasses the more complex SMTOs when combined with standard regularization techniques. In addition to classification tasks, Vandenhende et al. (2021) conduct extensive experiments on dense prediction tasks, i.e., tasks that produce pixel-level predictions. They conclude that avoiding gradient competition among tasks can actually lead to performance degradation, and linear scalarization with fixed weights outperforms some SMTOs.

Exploration of the Pareto front. It is well-known that minimizing scalarization objectives with positive coefficients yields Pareto optimal solutions (Boyd et al., 2004), while using non-negative coefficients yields weak Pareto optimal ones (Zadeh, 1963). On the other hand, exploration of the _entire_ Pareto front has been a long-standing and difficult open problem in MTL. For the simplest case, if all loss functions are convex or the achievable region is a convex set, then scalarization can fully explore the PF (Boyd et al., 2004; Xin et al., 2022). A slightly weaker condition for full exploration is directional convexity (Holtzman and Halkin, 1966), established by Lin (1976). Several recent works in MTL develop algorithms to generate PO subject to user preferences, in particular on trade-offs among certain tasks (Lin et al., 2019; Mahapatra and Rajan, 2020; Momma et al., 2022). There is also a line of empirical works that focuses on designing algorithms to efficiently explore the PF (Ma et al., 2020; Lin et al., 2020; Liu et al., 2021; Navon et al., 2021; Ruchte and Grabocka, 2021; Ye and Liu, 2022). As far as we know, we are the first to establish both necessary and sufficient conditions for full exploration in the non-convex setting.

## 8 Conclusion

In this paper, we revisit linear scalarization from a theoretical perspective. In contrast to recent works that claimed consistent empirical advantages of scalarization, we show its inherent limitation in fully exploring the Pareto front. Specifically, we reveal a multi-surface structure of the feasible region, and identify necessary and sufficient conditions for full exploration in the under-parametrized regime. We also discuss several implications of our theoretical results. Experiments on a real-world dataset corroborate our theoretical findings, and suggest the benefit of SMTOs in finding balanced solutions.