# Provable Benefits of Complex Parameterizations

for Structured State Space Models

 Yuval Ran-Milo\({}^{1}\)  Eden Lumbroso\({}^{1}\)  Edo Cohen-Karlik\({}^{1}\)

**Raja Giryes\({}^{1}\)  Amir Globerson\({}^{1}\)  Nadav Cohen\({}^{1}\)**

###### Abstract

Structured state space models (SSMs), the core engine behind prominent neural networks such as S4 and Mamba, are linear dynamical systems adhering to a specified structure, most notably diagonal. In contrast to typical neural network modules, whose parameterizations are real, SSMs often use complex parameterizations. Theoretically explaining the benefits of complex parameterizations for SSMs is an open problem. The current paper takes a step towards its resolution, by establishing formal gaps between real and complex diagonal SSMs. Firstly, we prove that while a moderate dimension suffices in order for a complex SSM to express all mappings of a real SSM, a much higher dimension is needed for a real SSM to express mappings of a complex SSM. Secondly, we prove that even if the dimension of a real SSM is high enough to express a given mapping, typically, doing so requires the parameters of the real SSM to hold exponentially large values, which cannot be learned in practice. In contrast, a complex SSM can express any given mapping with moderate parameter values. Experiments corroborate our theory, and suggest a potential extension of the theory that accounts for selectivity, a new architectural feature yielding state of the art performance.1

## 1 Introduction

_Structured state space models (SSMs)_ are the core engine behind prominent neural network architectures such as S4 , Mamba , LRU , Mega , S5  and more [23; 31; 38; 34]. In their typical form, SSMs can be thought of as single-input single-output linear dynamical systems, wherein the state transition matrix has a specified structure, most notably diagonal [22; 23; 41; 50; 37; 20]. A salient characteristic of SSMs is that their parameterizations are often _complex_ (take values in \(\)), in contrast to typical neural network modules whose parameterizations are conventionally real (take values in \(\)).

There has been mixed evidence regarding benefits of complex parameterizations over real parameterizations for SSMs. Some prior works have demonstrated that complex parameterizations are essential for strong performance [22; 41], whereas others have shown that in various settings real parameterizations lead to comparable (and in some cases better) performance [37; 20]. It was conjectured  that in the context of _diagonal SSMs_ (namely, SSMs with diagonal state transition matrix), complex parameterizations are preferable for continuous data modalities (_e.g._, audio, video), whereas for discrete data modalities (_e.g._, text, DNA) real parameterizations suffice. Unfortunately, to date, formal support for this conjecture is lacking. The extent to which complex parameterizations benefit diagonal SSMs remains to be an open question.

In this paper, we take a step towards theoretically addressing the foregoing question. Specifically, we provide two theoretical contributions establishing provable benefits of complex parameterizationsfor diagonal SSMs. Our first contribution establishes that, although both real and complex diagonal SSMs are _universal_--in the sense that both can precisely express any _linear time-invariant_ (_LTI_) mapping up to any time \(t\) when their dimensions are equal to or greater than \(t\)--there is a strong separation between the SSMs in terms of expressiveness. Namely, denoting the dimensions of the real and complex SSMs by \(n_{}\) and \(n_{}\), respectively, we prove that for any \(n_{}\), there are various oscillatory mappings expressible by the complex SSM which cannot be approximately expressed up to time \(t\) by the real SSM unless \(n_{}\) is on the order of \(t\), which may be arbitrarily larger than \(n_{}\). This is in stark contrast to the fact that all mappings expressible by the real SSM can be precisely expressed (up to any time) by the complex SSM whenever \(n_{} n_{}\).

Given the prevalence of overparameterization in machine learning, one may question how consequential the above separation (between the real and complex SSMs) is in practice. Indeed, in an overparameterized regime where a given LTI mapping is to be approximated up to a given time \(t\) and \(n_{},n_{} t\), universality implies that both the real and complex SSMs can precisely express the mapping up to time \(t\). Accordingly, in this overparameterized regime, it is a priori unclear whether the complex SSM offers an advantage over the real SSM. Our second contribution shows that it does. Specifically, we prove a surprising result by which, if the given mapping satisfies a mild condition, then in order to approximately express the mapping up to time \(t\), the real SSM must have dimension or parameter magnitude exponential in \(t\). This is in stark contrast to the complex SSM, which can precisely express the given mapping up to time \(t\) with dimension and parameter magnitudes that are at most linear in \(t\). The aforementioned mild condition is satisfied by the canonical copy mapping, by a basic oscillatory mapping, and with high probability by a random (generic) mapping. In such important cases, practical learning of the given mapping necessitates using a complex SSM.

Our theory is corroborated by controlled experiments, demonstrating that complex parameterizations for SSMs significantly improve performance. We also evaluate SSMs with _selectivity_--a new architectural feature yielding state of the art performance . Our experiments with selectivity portray a more nuanced picture: complex parameterizations are beneficial for some tasks, whereas for others, selectivity allows real parameterizations to achieve comparable (and in some cases better) performance. These findings align with the mixed evidence reported in the literature. Moreover, they suggest a potential extension of our theory that accounts for selectivity and may elucidate this evidence, thereby fully delineating the benefits of complex parameterizations for SSMs.

## 2 Preliminaries

### Notations

We use non-boldface lowercase letters for denoting scalars (_e.g._\(\), \(c\), \(n\)), boldface lowercase letters for denoting vectors (_e.g._\(^{n}\), \(^{n}\)), and non-boldface uppercase letters for denoting matrices (_e.g._\(A^{n,m}\), \(B^{n,m}\)). Series (finite or infinite) of scalars, vectors or matrices are viewed as functions of time and denoted accordingly (_e.g._\(((t)^{n})_{t}\), \((A(t)^{n,m})_{t=1,2,,k}\)). For series of scalars, we also use as notation boldface uppercase letters (_e.g._\(=(s(t))_{t}\), \(=(i(t))_{t=1,2,,k}\)). Given \(k\) and a series of scalars \(\) whose length is greater than or equal to \(k\), we use \(_{k}\) to denote the \(k\)th element of \(\), and \(_{:k}\) to denote the truncation of \(\) to length \(k\) (_i.e._ the series comprising the first \(k\) elements of \(\)), allowing ourselves to regard this truncated series as a vector of dimension \(k\). For \(k\{\}\), we use \([k]\) as shorthand for the set \(\{1,2,,k\}\). Given a complex number \(c\), we denote its magnitude by \(|c|_{ 0}\), its phase by \((c)[0,2)\), its real part by \((c)\), and its imaginary part by \((c)\) (meaning \(c=|c|(i(c))=(c)+i(c)\)). We let \(\) and \(\) stand for vectors whose entries are all zeros and all ones, respectively, with dimension to be inferred from context. The Hadamard (element-wise) product, defined between vectors or matrices of the same size, and between scalar series of the same length, is denoted by \(\). The convolution operator, defined between two (finite or infinite) scalar series, is denoted by \(*\). Namely, given two scalar series \(=(s(t))_{t[k]},=((t))_{t[]}\) of lengths \(k,\{\}\) respectively, \(*}\) is the scalar series of length \(k+-1\) whose \(m\)th element, for \(m[k+-1]\), is given by \(_{t=\{m-+1,1\}}^{\{m,k\}}s(t)^{n,1}\), an _input matrix_; and \(C^{1,n}\), an _output matrix_.23 Given the values of \(A\), \(B\) and \(C\), the SSM realizes a mapping \(_{n,(A,B,C)}:^{}^{}\) which receives as input a real scalar series \((u(t))_{t}\), and produces as output a real scalar series \((y(t))_{t}\) defined through the following recursive formula:

\[(t)=A(t-1)+Bu(t), y(t)=C_{ }(t), t\,, \]

where \(((t)^{n})_{t}\) is a vector series of _states_, and \((0)=^{n}\). If \(=\) we say that the SSM is _real_, and if \(=\) we say that it is _complex_.4 We refer to the SSM as _stable_ if all eigenvalues of \(A\) have magnitude strictly smaller than one; otherwise we refer to the SSM as _unstable_. For convenience, we often identify an SSM with the triplet \((A,B,C)\) holding its parameter matrices, and regard the (single column) matrices \(B\) and \(C^{}\) as vectors.

Perhaps the most prominent form of structure imposed on SSMs is _stable diagonality_, _i.e._ stability combined with diagonality . Accordingly, unless stated otherwise, we assume that the state transition matrix \(A\) of an SSM is diagonal and has entries with magnitude strictly smaller than one.

### Linear Time-Invariant Mappings

Let \(:^{}^{}\) be a mapping from the space of (infinite) real scalar series to itself. We say that \(()\) is _linear_ if for all \(\) and \(,^{}\) it holds that \((+})=()+(})\). For every \(k\), define the _\(k\) step delay_\(_{k}:^{}^{}\) to be the operator that adds \(k\) preceding zeros to the series it receives as input.5 We say that the mapping \(()\) is _linear time-invariant_ (_LTI_) if it is linear, and it commutes with \(_{k}()\) (meaning \((_{k}())=_{k}(())\)) for every \(k\). It is well known  that if \(()\) is LTI then it is given by \(()=*()\), where \(:=(1,0,0,)^{}\) is the _impulse_ series, and \(()\) is referred to as the _impulse response_ of \(()\). Conversely, for any \(^{}\), the mapping defined by \(*\) is LTI, and its impulse response is \(\).

We will identify LTI mappings with their impulse responses. More specifically, for any \(k\), we identify an LTI mapping up to time \(k\), with the truncation of its impulse response to length \(k\). Accordingly, for any LTI mappings \((),()\) and any \(_{ 0}\), we say that \(()\)_\(\)-approximates_\(()\)_up to time \(k\)_ if \(\|()_{:k}-()_{:k}\|_{1}\). If the latter inequality holds with \(=0\), we also say that \(()\)_matches_\(()\)_up to time \(k\)_.

Let \((A,B,C)\) be an SSM of dimension \(n\), realizing the mapping \(_{n,(A,B,C)}:^{}^{}\) (see Section 2.2). It is straightforward to see that \(_{n,(A,B,C)}()\) is LTI, and that its impulse response is given by:

\[_{n,(A,B,C)}()=(CB),(CAB),(CA^{2}B),\,. \]

For real and complex settings (_i.e._ for \(=\) and \(=\)), we will study the extent to which varying \((A,B,C)\) as well as \(n\), can lead \(_{n,(A,B,C)}()\) to \(\)-approximate different LTI mappings up to different times.

## 3 Theoretical Analysis

Throughout this section, we consider a real SSM \((A_{},B_{},C_{})\) of dimension \(n_{}\) realizing the mapping \(_{n_{},(A_{},B_{},C_{})}()\), and a complex SSM \((A_{},B_{},C_{})\) of dimension \(n_{}\) realizing the mapping \(_{n_{},(A_{},B_{},C_{})}()\) (see Section 2.2).

### Universality

It is known (see, _e.g._, ) that the real SSM is _universal_, in the sense that it can precisely express any LTI mapping up to any time \(t\) when its dimension is equal to or greater than \(t\). Trivially, this implies the same for the complex SSM. Proposition 1 below formalizes these facts for completeness.

**Proposition 1**.: _Let \(:^{}^{}\) be an arbitrary LTI mapping, and let \(t\). Then, the following holds for both \(=\) and \(=\). If \(n_{} t\), there exist assignments for \((A_{},B_{},C_{})\) with which \(_{n_{},(A_{},B_{},C_{})}()\) matches \(()\) up to time \(t\).6_

Proof sketch (proof in Appendix D.1).: Beginning with the real SSM (\(=\)), the proof shows that \(_{n_{},(A_{},B_{},C_{})}()\) matches \(()\) up to time \(t\) if \(V(A_{})(C_{}^{} B_{})=() _{:t}\), where \(V(A_{})\) is a Vandermonde matrix that has full rank when the diagonal entries of \(A_{}\) are distinct. Assigning \(A_{}\) this way, there must exist \(^{n_{}}\) with which \(V(A_{})=()_{:t}\). Assigning \(B_{}=\) and \(C_{}^{}=\) concludes the proof for the real SSM. The complex SSM (\(=\)) can be treated analogously. 

### Separation in Expressiveness

Proposition 2 and Theorem 1 below together establish that although both the real and complex SSMs are universal (see Section 3.1), there is a strong separation between the two in terms of expressiveness. Proposition 2 formalizes an obvious fact: all mappings expressible by the real SSM can be precisely expressed (up to any time) by the complex SSM whenever \(n_{} n_{}\) (_i.e._, whenever the dimension of the complex SSM is equal to or greater than the dimension of the real SSM). Theorem 1 proves a much less obvious result: for any \(n_{}\), there are various oscillatory mappings (_i.e._ mappings with oscillatory impulse responses) expressible by the complex SSM which cannot be approximately expressed up to time \(t\) by the real SSM unless \(n_{}\) is on the order of \(t\), which may be arbitrarily larger than \(n_{}\).

**Proposition 2**.: _Consider an arbitrary assignment for \((A_{},B_{},C_{})\), and assume that \(n_{} n_{}\). Then, there exist assignments for \((A_{},B_{},C_{})\) with which \(_{n_{},(A_{},B_{},C_{})}()= _{n_{},(A_{},B_{},C_{})}()\)._

Proof.: It suffices to prove the sought after result for \(n_{}=n_{}\), since one can effectively reduce the dimension of the complex SSM by zeroing out entries of \(B_{}\) (or \(C_{}\)). Assuming that \(n_{}=n_{}\), we may assign to \((A_{},B_{},C_{})\) the values of \((A_{},B_{},C_{})\). Under this assignment \(_{n_{},(A_{},B_{},C_{})}()= _{n_{},(A_{},B_{},C_{})}()\), as required. 

**Theorem 1**.: _Let \(t\) and \(_{ 0}\). Assume without loss of generality that \(n_{}=1\),7 in which case \(A_{}\), \(B_{}\) and \(C_{}\) can be regarded as scalars. Suppose \(|((A_{}))| 0.2\), \(|A_{}| 0.5^{1/t}\) and \(|B_{} C_{}| 1\). Then, if \(_{n_{},(A_{},B_{},C_{})}()\)\(\)-approximates \(_{n_{},(A_{},B_{},C_{})}()\) up to time \(t\), it must be that \(n_{} t/9-1-4\)._

Proof sketch (proof in Appendix D.2).: The idea behind the proof is as follows. The complex SSM realizes an oscillatory mapping, in the sense that elements \(1,3,,2 t/2-1\) of its impulse response alternate \((t)\) times between being greater than or equal to \(1/4\), and being smaller than or equal to \(-1/4\). The real SSM on the other hand is limited in its ability to realize oscillations, insofar as elements \(1,3,,2 t/2-1\) of the impulse response it gives rise to are a linear combination of decaying exponentials, and therefore can change sign at most \((n_{})\) times. Combining these two observations leads to the desired result. 

### Separation in Practical Learnability

Let \(:^{}^{}\) be an LTI mapping with bounded impulse response, which we would like to \(\)-approximate up to time \(t\) for some \(_{ 0}\) and \(t\). Assume that the dimensions of the real and complex SSMs are greater than or equal to \(t\). By Proposition 1, both the real and complex SSMs can express mappings that match \(()\) up to time \(t\), and in particular that achieve the desired approximation. The current subsection establishes that despite this parity in terms of expressiveness, there is a strong separation between the real and complex SSMs in terms of _practical learnability_.

Section 3.3.1 proves that under a mild condition on \(()\), in order for the real SSM to achieve the desired approximation, either its dimension or the magnitude of its parameters must be exponential in \(t\). Section 3.3.2 then explains that such exponentiality impedes practical learning via gradient descent. Finally, Section 3.3.3 proves that in stark contrast to the real SSM, the complex SSM can achieve the desired approximation with dimension and parameter magnitudes that are at most linear in \(t\).

#### 3.3.1 Real Parameterizations Suffer from Exponentiality

Definition 1 below formalizes the notion of _forward difference_ for a real scalar series--a discrete analogue of derivative for a differentiable real function. Our main theoretical result, Theorem 2, then establishes that if forward differences associated with \(()\)--the impulse response of \(()\)--satisfy a certain condition, then in order for the real SSM to express a mapping that \(\)-approximates \(()\) up to time \(t\), either the dimension of the real SSM \(n_{}\) or the magnitude of its parameters \((B_{},C_{})\) must be exponential in \(t\). Roughly speaking, the aforementioned condition on forward differences associated with \(()\) is that there exists some \(d(t)\) such that the \(d\)th forward difference of the restriction of \(()\) to either odd or even elements has magnitude greater than \(2^{d}\). Perhaps surprisingly, this condition is especially mild, as the magnitude of the \(d\)th forward difference of a real scalar series typically scales exponentially with \(d\). Several important cases where the condition is satisfied are presented below.

**Definition 1**.: Let \(\) be a real scalar series of length \(k\{\}\). The _forward difference_ of \(\), denoted \(^{(1)}\), is the scalar series of length \(k-1\) whose \(m\)th element, for \(m[k-1]\), is given by \(_{m+1}-_{m}\). For \(d\{2,3,,k-1\}\), the _\(d\)th forward difference_ of \(\), denoted \(^{(d)}\), is recursively defined to be the forward difference of \(^{(d-1)}\).

**Theorem 2**.: _Suppose \(_{n_{},(A_{},B_{},C_{})}()\)\(\)-approximates \(()\) up to time \(t\). Then:_

\[n_{}\|C_{} B_{}\|_{}_{ d,m,\,d+m t/2\\ \{\}}\{2^{d+2\{d,m\}}(2^ {-d}(()|_{})_{m}^{(d)}-)\}\,, \]

_where: \(()|_{}\) and \(()|_{even}\) are the restrictions of the impulse response \(()\) to odd and even elements, respectively; and \((()|_{})_{m}^{(d)}\) and \((()|_{even})_{m}^{(d)}\) stand for the mth element of the \(d\)th forward difference of \(()|_{}\) and \(()|_{}\), respectively._

Proof sketch (proof in Appendix D.3).: The idea behind the proof is as follows. The restrictions of the impulse response of \(_{n_{},(A_{},B_{},C_{})}()\) to odd and even elements--_i.e._, \(_{n_{},(A_{},B_{},C_{})}()|_{}\) and \(_{n_{},(A_{},B_{},C_{})}()|_{}\), respectively--are each a linear combination of \(n_{}\) decaying exponentials, where the coefficients of the linear combination have absolute value no greater than \(\|C_{} B_{}\|_{}\). Forward differences of decaying exponentials are exponentially small. Therefore, by linearity of forward differences, requiring \(_{n_{},(A_{},B_{},C_{})}()|_{}\) or \(_{n_{},(A_{},B_{},C_{})}() |_{even}\) to have a forward difference that is not exponentially small implies an exponentially large lower bound on \(n_{}\|C_{} B_{}\|_{}\). When \(_{n_{},(A_{},B_{},C_{})}()\)\(\)-approximates \(()\) up to time \(t\), forward differences of \(_{n_{},(A_{},B_{},C_{})}() |_{}\) and \(_{n_{},(A_{},B_{},C_{})}()|_{even}\) are close to those of \(()|_{}\) and \(()|_{}\), respectively. We thus conclude that if \(()|_{}\) or \(()|_{}\) has a forward difference that is not especially small, then \(n_{}\|C_{} B_{}\|_{}\) must be exponentially large. This conclusion is formalized via Equation (3), the sought after result. 

Special cases.Theorem 2 implies that the real SSM suffers from exponentiality (namely, its dimension \(n_{}\) or the magnitude of its parameters \((B_{},C_{})\) must be exponential in \(t\) in order for it to express a mapping that \(\)-approximates \(()\) up to time \(t\)) in various important cases. Indeed, Corollaries 1 and 2 below respectively show that the real SSM suffers from exponentiality when \(()\) is a canonical copy (delay) mapping, and with high probability when \(()\) is a random (generic) mapping. In light of Theorem 1 (namely, of the inability of the real SSM to compactly approximate various oscillatory mappings expressible by the complex SSM), it is natural to ask if the real SSM suffers from exponentiality in cases where \(()\) is oscillatory, _i.e._ where its impulse response oscillates. Corollary 3 below shows that exponentiality indeed transpires in a case where \(()\) is a basic oscillatory mapping. On the other hand, there are simple cases where \(()\) is oscillatory yet exponentiality does not take place, _e.g._ the case where the impulse response of \(()\) is \((+1,-1,+1,-1,)\).8 Precise delineation of the type of oscillations that lead to exponentiality is deferred to future work (see Section 6).

**Corollary 1**.: _Suppose \(t 9\) and \(()=_{((t-1)/2]}()\), where as defined in Section 2.3, \(_{((t-1)/2]}()\) is the \((t-1)/2\) step delay mapping. Assume also that \( 1/8\,\). Then, if \(_{n_{},(A_{},B_{},C_{})}()\)\(\)-approximates \(()\) up to time \(t\), it must hold that:_

\[n_{}\|C_{}^{} B_{}\|_{} 2^{t /2}32\,\,. \]

Proof sketch (proof in Appendix D.4).: The proof computes forward differences associated with \(_{(t-1)/2}()\) (impulse response of \(_{(t-1)/2}()\)), and plugs them into Theorem 2 (Equation (3)). 

**Corollary 2**.: _Let \(_{>0}\), and let \(^{}\) be generated by a random process where each element of \(\) is independently drawn from a uniform distribution over the interval \([-,]\). Suppose that \(t 8\) and that \(()\) is the mapping whose impulse response is \(\) (i.e., \(()\) is defined by \(()=*\)). Let \(p(0,1)\), and assume that \(\). Then, with probability at least \(1-p\), if \(_{n_{},(A_{},B_{},C_{})}()\)\(\)-approximates \(()\) up to time \(t\), it must hold that:_

\[n_{}\|C_{}^{} B_{}\|_{} 2^{t /2}/8\,. \]

Proof sketch (proof in Appendix D.5).: The proof derives lower bounds (holding with probability at least \(1-p\)) on forward differences associated with \(\), and plugs them into Theorem 2 (Equation (3)). 

**Corollary 3**.: _Suppose that \(()=(+1\,,\ 0\,,-1\,,\ 0\,,+1\,,\ 0\,,-1\,,\ 0\,,)\) and \( 0.5\). Then, if \(_{n_{},(A_{},B_{},C_{})}()\)\(\)-approximates \(()\) up to time \(t\), it must hold that:_

\[n_{}\|C_{}^{} B_{}\|_{} 2^{3t /4-4}\,. \]

Proof sketch (proof in Appendix D.6).: The proof computes forward differences associated with the series \((+1\,,\ 0\,,-1\,,\ 0\,,+1\,,\ 0\,,-1\,,\ 0\,,)\), and plugs them into Theorem 2 (Equation (3)). 

#### 3.3.2 Exponentiality Impedes Practical Learning

For any value of \(t\) that is not especially small, exponentiality in \(t\) for the real SSM as put forth in Section 3.3.1--_i.e._, exponentiality in \(t\) of the dimension of the real SSM \(n_{}\) or the magnitude of its parameters \((B_{},C_{})\)--impedes practical learning. This impediment is obvious in the case where \(n_{}\) is exponential in \(t\) (in this case, it is impractical to even store the parameters of the real SSM, let alone learn them). Appendix B treats the complementary case, _i.e._ it shows that learning is impractical when the required values for the parameters \((B_{},C_{})\) are exponential in \(t\) (this is deferred to an appendix due to space constraints). The results of Section 3.3.1 therefore imply that the real SSM cannot practically learn a mapping that \(\)-approximates \(()\) up to time \(t\) under important choices of \(()\).

#### 3.3.3 Complex Parameterizations Do Not Suffer from Exponentiality

Section 3.3.1 established that under a mild condition on \(()\), in order for the real SSM to express a mapping that \(\)-approximates \(()\) up to time \(t\), either the dimension of the real SSM \(n_{}\) or the magnitude of its parameters \((B_{},C_{})\) must be exponential in \(t\). Proposition 3 below proves that in stark contrast, for any choice of \(()\) (whose impulse response \(()\) is bounded), the complex SSM can express mappings that match \(()\) up to time \(t\) with dimension \(n_{}\) and magnitude of parameters \((B_{},C_{})\) that are at most linear in \(t\).

**Proposition 3**.: _For any choice of \(n_{}\) greater than or equal to \(t\), there exist assignments for \((A_{},B_{},C_{})\) with which \(_{n_{},(A_{},B_{},C_{})}()\) matches \(()\) up to time \(t\), and wherein:_ (i)_\(\|B_{}\|_{2} 2\|()_{t}\|_{2}\)_; and_ (ii)_\(\|C_{}^{}\|_{2} 1\)_._9__Proof sketch (proof in Appendix D.7).: The proof employs the theory of _discrete Fourier transform_ (_DFT_). It begins by assigning (scaled versions of) the \(t\)th roots of unity to the diagonal entries of \(A_{}\). Then, it uses the inverse DFT formula to derive assignments for \(B_{}\) and \(C_{}\) leading \(_{n_{},(A_{},B_{},C_{})}()\) to match \(()\) up to time \(t\). Finally, the proof applies Plancheral theorem to show that the derived assignments for \(B_{}\) and \(C_{}\) satisfy the desired criteria. 

## 4 Experiments

This section presents controlled experiments corroborating our theory. Section 4.1 demonstrates that complex parameterizations significantly improve performance of SSMs in the theoretically analyzed setting. Section 4.2 shows that this improvement extends to a real-world setting beyond our theory. Finally, Section 4.3 evaluates SSMs with _selectivity_--a new architectural feature yielding state of the art performance [20; 31; 4; 57]. The experiments with selectivity portray a nuanced picture: complex parameterizations are beneficial for some tasks, whereas for others, selectivity allows real parameterizations to achieve comparable (and in some cases better) performance. These findings align with mixed evidence reported in the literature (see Section 1). Moreover, they suggest a potential extension of our theory that accounts for selectivity and may elucidate this evidence, thereby fully delineating the benefits of complex parameterizations for SSMs.

For conciseness, we defer some of the details behind our implementation to Appendix F. Code for reproducing our experiments is available at [https://github.com/edenlum/SSMComplexParamBenefits](https://github.com/edenlum/SSMComplexParamBenefits).

### Theoretically Analyzed Setting

To empirically demonstrate our theoretical findings, we trained the analyzed real and complex SSMs (see Section 2.2) to approximate up to time \(t\) the mapping \(()\) (see Section 2.3), with \(t\) varying and with the following choices for \(()\): the canonical copy (delay) mapping from Corollary 1; the random (generic) mapping from Corollary 2; and the basic oscillatory mapping from Corollary 3. Throughout, the dimension of the real or complex SSM (\(n_{}\) or \(n_{}\), respectively) was set to at least \(t\), which, by universality (Section 3.1), implies that the SSM can express a mapping that precisely matches \(()\) up to time \(t\). Our theory (Section 3.3) establishes that despite this parity between the real and complex SSMs in terms of expressiveness, there is a strong separation between the SSMs in terms of practical learnability. In particular, with the above choices of \(()\), there are exponential barriers that apply only to the real SSM, and prevent its training from being able to yield a mapping that closely approximates \(()\) up to time \(t\). Tables 1 and 2 present results obtained with the real and complex SSMs, respectively. They confirm the predictions of our theory.

### Real-World Setting

To empirically demonstrate the benefits of complex parameterizations for SSMs in settings beyond our theory, we evaluated the prominent S4 neural network architecture  on the real-world sequential

  
**Optimizer** & **Approx. of Copy** & **Approx. of Random** & **Approx. of Oscillatory** \\  Adam & \(0.767\) & \(0.536\) & \(0.812\) \\ AdamW & \(0.772\) & \(0.541\) & \(0.809\) \\ RAdam & \(0.767\) & \(0.538\) & \(0.811\) \\   

Table 1: In accordance with our theory, the analyzed real SSM (see Section 2.2) cannot practically learn to closely approximate \(()\) up to time \(t\) under important choices of \(()\), even when \(t\) is moderate. This table reports the approximation error attained by the real SSM, _i.e._ the minimum \(\) with which a mapping learned by the real SSM \(\)-approximates \(()\) up to time \(t\) (see Section 2.3), when \(t=32\) and \(()\) varies over the following possibilities: the canonical copy (delay) mapping from Corollary 1; the random (generic) mapping from Corollary 2; and the basic oscillatory mapping from Corollary 3. Learning was implemented by applying one of three possible gradient-based optimizers—Adam , AdamW  or RAdam —to a loss as in our theory (see Appendix C). For each choice of \(()\), reported approximation errors are normalized (scaled) such that a value of one is attained by the trivial zero mapping. Each configuration was evaluated with five random seeds, and its reported approximation error is the minimum (best) that was attained. The dimension of the real SSM (\(n_{}\)) was set to \(1024\); other choices of dimension led to qualitatively identical results. For further implementation details see Appendix F.1.

CIFAR-10 dataset from the widely recognized Long Range Arena benchmark . Our implementation is based on the official S4 repository,10 where unless stated otherwise, hyperparameters (pertaining to the neural network architecture and its training) were kept at their default values. A single run with complex parameterization yielded a test accuracy of \(89.10\%\), significantly higher than the highest test accuracy of \(78.27\%\) attained with real parameterization across three random seeds. Modifying the optimizer and initialization scheme with the real parameterization did not improve the test accuracy--see Appendix F.2 for details. The overarching conclusion from our theory--namely, that SSMs benefit from complex parameterizations--thus extends to this real-world setting.

### Selectivity

A new architectural feature for SSMs that yields state of the art performance is _selectivity_[20; 31; 4; 57]. In its original form--proposed as part of the Mamba neural network architecture --selectivity amounts to replacing the parameters \(B\) and \(C\) (see Section 2.2), as well as an additional _discretization_ parameter \(_{>0}\),2 by certain functions of the input \((u(t))_{t}\). We empirically study the impact of complex parameterizations on SSMs with selectivity by evaluating a Mamba neural network on two synthetic tasks regarded as canonical in the SSM literature [27; 20]: _(i) copy_, which was shown by our theory (Section 3.3) and earlier experiments (Section 4.1) to pose difficulties for real parameterizations in SSMs with no selectivity; and _(ii) induction-head_, which can be seen as a generalization of copy in which the delay is input-specified (rather than being fixed). Our implementation is based on a widely adopted Mamba repository easily amenable to modification.11 Unless stated otherwise, repository hyperparameters (pertaining to the neural network architecture and its training) were kept at their default values. Further details concerning our implementation, including detailed descriptions of the copy and induction-head tasks, can be found in Appendix F.3.

Our first experiment with the Mamba neural network compared real and complex parameterizations for the underlying SSMs, with selectivity included. On the copy task, across three random seeds for each configuration, the highest accuracy attained with the real parameterization was \(80.17\%\), whereas the lowest accuracy attained with the complex parameterization was \(93.05\%\). This gap in performance in favor of the complex parameterization aligns with our theoretical and empirical findings for SSMs without selectivity. In stark contrast, on the induction-head task, there is no such gap (in fact, there is a slight advantage to the real parameterization): across three random seeds for each configuration, accuracies attained with the real parameterization ranged between \(97.35\%\) and \(98.3\%\), whereas those attained with the complex parameterization ranged between \(93.93\%\) and \(97.64\%\). These results align with mixed evidence reported in the SSM literature, by which complex parameterizations are essential for strong performance on some tasks [22; 41; 22], whereas on others, real parameterizations lead to comparable (and in some cases better) performance [37; 20].

To gain insight into the induction-head task not benefiting from the complex parameterization, we conducted an ablation experiment with partial versions of selectivity (_i.e._, where not all of the parameters \(B\), \(C\) and \(\) were replaced by functions of the input). The results of this experiment, reported in Table 3, reveal that when selectivity is fully or partially removed (more precisely, when

   \(t\) & **Approx. of Copy** & **Approx. of Random** & **Approx. of Oscillatory** \\  \(32\) & \(1.6 10^{-5}\) & \(6.3 10^{-5}\) & \(1.6 10^{-4}\) \\ \(64\) & \(1.7 10^{-5}\) & \(1.6 10^{-5}\) & \(3.7 10^{-4}\) \\ \(128\) & \(4.9 10^{-5}\) & \(4.8 10^{-5}\) & \(2.6 10^{-3}\) \\ \(256\) & \(6.8 10^{-4}\) & \(7.6 10^{-4}\) & \(1.7 10^{-3}\) \\   

Table 2: In contrast to the analyzed real SSM, and in alignment with our theory, the analyzed complex SSM (see Section 2.2) can practically learn to closely approximate \(()\) up to time \(t\) under important choices of \(()\) and various choices of \(t\). This table reports approximation errors attained by the complex SSM. It ablences to the description of Table 1, with the following exceptions (all designed to stress the superiority of the complex SSM over the real SSM): _(i)_ only Adam optimizer was used; _(ii)_ in addition to \(32\), \(t\) also took the values \(64\), \(128\) and \(256\); _(iii)_ for each configuration, the reported approximation error is the maximum (worst) that was achieved across the random seeds; and _(iv)_ the dimension of the complex SSM (\(n_{}\)) was set to \(t\) (higher dimensions led to qualitatively identical results). For further implementation details see Appendix F.1.

\(B\), \(C\) or both are not replaced by functions of the input), the complex parameterization regains its advantage. This suggests that selectivity, which is not covered by our theory, may be the key factor enabling real parameterizations to perform as well as complex parameterizations for SSMs on certain tasks. In other words, selectivity may be the dominant factor behind the aforementioned evidence in the SSM literature being mixed. In Section 7 we outline a potential extension of our theory that accounts for selectivity and may elucidate this evidence, thereby fully delineating the benefits of complex parameterizations for SSMs.

## 5 Related Work

SSMs are closely related to linear dynamical systems--a classic object of study in areas such as systems theory  and control theory . Although there exists extensive literature concerning properties of real and complex linear dynamical systems [10; 5; 6; 9], this literature does not readily establish benefits of complex parameterizations for SSMs, primarily due to the following reasons: _(i)_ the output of a complex SSM is turned real by disregarding imaginary components (see Section 2.2), therefore it differs from a complex linear dynamical system; and _(ii)_ the structures typically imposed on state transition matrices of SSMs (_e.g._ stable diagonality; see Section 2.2) are generally uncommon in the literature on linear dynamical systems.

SSMs can be viewed as a special case of recurrent neural networks , which received significant theoretical attention [46; 39; 25; 11; 53]. In this context, several works focused specifically on SSMs [42; 30; 24; 2; 28; 58; 12; 32; 55]. However, to our knowledge, the only prior work to formally and explicitly treat benefits of complex parameterizations for SSMs is . The treatment of  (see Section 4.1 therein) can be viewed as a special case of ours. Indeed,  considered a task that, using our notation (see Section 2.2), amounts to linearly reconstructing an input element \(u(t)\) from the state \((t^{})\) of an SSM, where \(t,t^{},t^{} t\). This is equivalent to assigning the output matrix of the SSM \(C\) such that the mapping realized by the SSM \(_{n,(A,B,C)}()\) is a canonical copy (delay) mapping. Roughly speaking,  showed that this task requires linear operations with exponential parameters if the SSM is real, whereas linear operations with moderate parameters suffice if the SSM is complex. The same result follows from our Corollary 1 and Proposition 3. We stress that our theory goes far beyond this result, for example in that it covers various mappings beyond copy, including a random (generic) mapping--see Section 3.3 for details.

With regards to related empirical work, the literature includes several experimental comparisons between real and complex parameterizations for SSMs [20; 22; 42]. Nonetheless, to our knowledge, the controlled experiments we conducted (see Section 4) are reported herein for the first time.

  
**Selective \(B\)** & **Selective \(C\)** & **Selective \(\)** & **Real Accuracy (\%)** & **Complex Accuracy (\%)** \\  Yes & Yes & Yes & \(97.35\) to \(98.3\) & \(93.93\) to \(97.64\) \\ Yes & Yes & No & \(97.9\) to \(98.62\) & \(90.18\) to \(95.86\) \\ Yes & No & Yes & \(61.82\) to \(71.28\) & \(91.93\) to \(96.77\) \\ Yes & No & No & \(49.91\) to \(52.5\) & \(58.93\) to \(73.77\) \\ No & Yes & Yes & \(56.78\) to \(69.54\) & \(92.52\) to \(96.91\) \\ No & Yes & No & \(41.01\) to \(43.89\) & \(57.44\) to \(64.67\) \\ No & No & Yes & \(15.48\) to \(26.33\) & \(68.54\) to \(79.71\) \\ No & No & No & \(23.86\) to \(29.62\) & \(37.61\) to \(50.19\) \\   

Table 3: Ablation experiment demonstrating that real parameterizations can compare (favorably) to complex parameterizations for SSMs with selectivity, but complex parameterizations become superior when selectivity is fully or partially removed. This table reports test accuracies attained by a Mamba neural network  on a synthetic induction-head task regarded as canonical in the SSM literature [27; 20]. Evaluation included multiple configurations for the SSMs underlying the neural network. Each configuration corresponds to either real or complex parameterization, and to a specific partial version of selectivity—_i.e._, to a specific combination of parameters that are selective (replaced by functions of the input), where the parameters that may be selective are: the input matrix \(B\); the output matrix \(C\); and a discretization parameter \(\). For each configuration, the highest and lowest accuracies attained across three random seeds are reported. Notice that when both \(B\) and \(C\) are selective, the real parameterization compares (favorably) to the complex parameterization, whereas otherwise, the complex parameterization is superior. For further implementation details see Appendix F.3.

## 6 Limitations

While this paper offers meaningful contributions regarding benefits of complex parameterizations for SSMs, it is important to acknowledge several of its limitations. First, while we establish a separation between real and complex parameterizations in terms of expressiveness (Section 3.2), we do not quantify how prevalent this separation is, _i.e._, what proportion of the mappings expressible with complex parameterizations cannot be compactly approximated with real parameterizations. Second, while we prove that real parameterizations suffer from exponentiality that impedes practical learning (Sections 3.3.1 and 3.3.2), and that complex parameterizations do not suffer from exponentiality (Section 3.3.3), we do not formally establish practical learnability with complex parameterizations--our evidence for this is purely empirical (Section 4). Third, our theory does not treat all fundamental aspects of learning where real and complex parameterizations may differ, for example it does not treat implicit bias of gradient-based optimization . Fourth, our experiments (Section 4) include only a single real-world setting. Finally, while we establish that a separation between real and complex parameterizations in terms of practical learnability takes place in three important cases (see Corollaries 1 to 3), these cases are likely far from being exhaustive. Indeed, Theorem 2 provides a mild sufficient condition for separation in terms of practical learnability--namely, that certain forward differences are not especially small--and we believe it is possible to apply analytical tools (_e.g._, the Norlund-Rice integral ) for translating this condition into interpretable properties satisfied in various important cases beyond those considered. Pursuing the latter direction, and more broadly, addressing the aforementioned limitations, are regarded as important directions for future work.

## 7 Discussion

The extent to which complex parameterizations benefit SSMs is an important open question in machine learning. Evidence in the literature is mixed: while some works demonstrate that complex parameterizations are essential for strong performance, others show that in various settings, real parameterizations lead to comparable (and in some cases better) performance. It was conjectured by Gu and Dao  that complex parameterizations are preferable for continuous data modalities (_e.g._, audio, video), whereas for discrete data modalities (_e.g._, text, DNA) real parameterizations suffice.

Since a complex SSM includes twice as many parameters as a real SSM of the same dimension, a priori, one might expect that a real SSM would benefit from becoming complex similarly to how it would benefit from doubling its dimension. Our theory showed that this is not the case, and in fact the former benefit far exceeds the latter. Indeed, we established separations between real and complex SSMs, by which a real SSM can only match a complex SSM if either the dimension of the real SSM or the number of iterations required for its training is exponentially large. Experiments corroborated our theory, and suggested that selectivity--a new architectural feature yielding state of the art performance--may be the dominant factor behind the aforementioned evidence in the literature being mixed.

We now outline a potential extension of our theory that accounts for selectivity. Roughly speaking, the separations we established between real and complex SSMs arise from a gap in their ability to express oscillations, _i.e._, to express frequency components in their impulse response: while a complex SSM can easily express any frequency, a real SSM struggles to do so. Adding selectivity to a real SSM makes its parameters input-dependent, resulting in what can be viewed as an input-dependent impulse response. We hypothesize that this dependence allows importing frequency components from the input to the impulse response. If confirmed, this hypothesis would imply that when the input data is sufficiently rich in frequency content, selectivity can endow real SSMs with all the benefits we proved for complex SSMs. Such an outcome aligns with the conjecture of Gu and Dao : continuous data modalities often consist of only low frequencies, whereas discrete data modalities typically have a "whiter spectrum," _i.e._, a more uniform mix of frequencies .

We believe that extending our theory as described may elucidate the mixed evidence in the literature, thereby fully delineating the benefits of complex parameterizations for SSMs.