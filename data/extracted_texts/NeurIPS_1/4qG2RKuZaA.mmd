# Censored Sampling of Diffusion Models Using

3 Minutes of Human Feedback

 TaeHo Yoon\({}^{1}\)   Kibeom Myoung\({}^{1}\)   Keon Lee\({}^{4}\)   Jaewoong Cho\({}^{4}\)   Albert No\({}^{3}\)   Ernest K. Ryu\({}^{1,2}\)

\({}^{1}\)Department of Mathematical Science, Seoul National University

\({}^{2}\)Interdisciplinary Program in Artificial Intelligence, Seoul National University

\({}^{3}\)Department of Electronic and Electrical Engineering, Hongik University

\({}^{4}\)KRAFTON

###### Abstract

Diffusion models have recently shown remarkable success in high-quality image generation. Sometimes, however, a pre-trained diffusion model exhibits partial misalignment in the sense that the model can generate good images, but it sometimes outputs undesirable images. If so, we simply need to prevent the generation of the bad images, and we call this task censoring. In this work, we present censored generation with a pre-trained diffusion model using a reward model trained on minimal human feedback. We show that censoring can be accomplished with extreme human feedback efficiency and that labels generated with a mere few minutes of human feedback are sufficient. Code available at: [https://github.com/tetrzim/diffusion-human-feedback](https://github.com/tetrzim/diffusion-human-feedback).

## 1 Introduction

Diffusion probabilistic models [19; 12; 42] have recently shown remarkable success in high-quality image generation. Much of the progress is driven by scale [35; 36; 38], and this progression points to a future of spending high costs to train a small number of large-scale foundation models  and deploying them, sometimes with fine-tuning, in various applications. In particular use cases, however, such pre-trained diffusion models may be misaligned with goals specified before or after the training process. An example of the former is text-guided diffusion models occasionally generating content with nudity despite the text prompt containing no such request. An example scenario of the latter is deciding that generated images should not contain a certain type of concepts (for example, human faces) even though the model was pre-trained on images with such concepts.

Fixing misalignment directly through training may require an impractical cost of compute and data. To train a large diffusion model again from scratch requires compute costs of up to hundreds of thousands of USD [30; 29]. To fine-tune a large diffusion model requires data size ranging from 1,000  to 27,000 .1 We argue that such costly measures are unnecessary when the pre-trained model is already capable of sometimes generating "good" images. If so, we simply need to prevent the generation of "bad" images, and we call this task _censoring_. (Notably, censoring does not aim to improve the "good" images.) Motivated by the success of reinforcement learning with human feedback (RLHF) in language domains [9; 50; 43; 33], we perform censoring using human feedback.

In this work, we present censored generation with a pre-trained diffusion model using a reward model trained on extremely limited human feedback. Instead of fine-tuning the pre-trained diffusion model, we train a reward model on labels generated with a **few minutes of human feedback** and perform guided generation. By not fine-tuning the diffusion model (score network), we reduce both compute

## 1 Introduction

Figure 1: Uncensored baseline vs. censored generation. Setups are precisely defined in Section 5. Due to space constraints, we present selected representative images here. Full sets of non-selected samples are shown in the appendix.

and data requirements for censored generation to negligible levels. (Negligible compared to any amount of compute and man-hours an ML scientist would realistically spend building a system with a diffusion model.) We conduct experiments within multiple setups demonstrating how minimal human feedback enables removal of target concepts. The specific censoring targets we consider are: A handwriting variation ("crossed 7"s) in MNIST ; Watermarks in the LSUN  church images; Human faces in the ImageNet  class "tench"; "Broken" images in the generation of LSUN bedroom images.

Contribution.Most prior work focus on training new capabilities into diffusion models, and this inevitably requires large compute and data. Our main contribution is showing that a very small amount of human feedback data and computation is sufficient for guiding a pre-trained diffusion model to do what it can already do while suppressing undesirable behaviors.

### Background on diffusion probabilistic models

Due to space constraints, we defer the comprehensive review of prior works to Appendix D. In this section, we briefly review the standard methods of diffusion probabilistic models (DPM) and set up the notation. For the sake of simplicity and specificity, we only consider the DPMs with the variance preserving SDE.

Consider the _variance preserving (VP)_ SDE

\[dX_{t}=-}{2}X_{t}dt+}dW_{t}, X_{0} p_ {0} \]

for \(t[0,T]\), where \(_{t}>0\), \(X_{t}^{d}\), and \(W_{t}\) is a \(d\)-dimensional Brownian motion. The process \(\{X_{t}\}_{t[0,T]}\) has the marginal distributions given by

\[X_{t}}}{{=}}}X_{0}+}_{t},_{t}=e^{-_{0}^{t}_{s}ds},\, _{t}(0,I)\]

for \(t[0,T]\)[39, Chapter 5.5]. Let \(p_{t}\) denote the density for \(X_{t}\) for \(t[0,T]\). Anderson's theorem  tells us that the the reverse-time SDE by

\[d_{t}=_{t}(- p_{t}(_{t})- {2}_{t})dt+}d_{t}, {X}_{T} p_{T},\]

where \(\{_{t}\}_{t[0,T]}\) is a reverse-time Brownian motion, satisfies \(_{t}}}{{=}}X_{t} p_{t}\).

In DPMs, the initial distribution is set as the data distribution, i.e., \(p_{0}=p_{}\) in (1), and a _score network_\(s_{}\) is trained so that \(s_{}(X_{t},t) p_{t}(X_{t})\). For notational convenience, one often uses the _error network_\(_{}(X_{t},t)=-}s_{}(X_{t},t)\). Then, the reverse-time SDE is approximated by

\[d_{t}=_{t}(}}_{ }(_{t},t)-_{t})dt+}d_{t},_{T}(0,I)\]

Figure 1: (Continued) Uncensored baseline vs. censored generation. Setups are precisely defined in Section 5. Due to space constraints, we present selected representative images here. Full sets of non-selected samples are shown in the appendix.

for \(t[0,T]\).

When an image \(X\) has a corresponding label \(Y\), classifier guidance [40; 12] generates images from

\[p_{t}(X_{t}\,|\,Y) p_{t}(X_{t},Y)=p_{t}(X_{t})p_{t}(Y\,|\,X_{t})\]

for \(t[0,T]\) using

\[_{}(_{t},t) =_{}(_{t},t)-} p_{t}(Y\,|\,_{t})\] \[d_{t} =_{t}(}}_{ }(_{t},t)-_{t})dt+}d_{t},_{T}(0,I),\]

where \(>0\). This requires training a separate time-dependent classifier approximating \(p_{t}(Y\,|\,X_{t})\). In the context of censoring, we use a reward model with binary labels in place of a classifier.

## 2 Problem description: Censored sampling with human feedback

Informally, our goal is:

Given a pre-trained diffusion model that is partially misaligned in the sense that generates both "good" and "bad" images, fix/modify the generation process so that only good images are produced.

The meaning of "good" and "bad" depends on the context and will be specified through human feedback. For the sake of precision, we define the terms "benign" and "malign" to refer to the good and bad images: A generated image is _malign_ if it contains unwanted features to be censored and is _benign_ if it is not malign.

Our assumptions are: (i) the pre-trained diffusion model does not know which images are benign or malign, (ii) a human is willing to provide minimal (\( 3\) minutes) feedback to distinguish benign and malign images, and (iii) the compute budget is limited.

Mathematical formalism.Suppose a pre-trained diffusion model generates images from distribution \(p_{}(x)\) containing both benign and malign images. Assume there is a function \(r(x)(0,1)\) representing the likelihood of \(x\) being benign, i.e., \(r(x) 1\) means image \(x\) is benign and should be considered for sampling while \(r(x) 0\) means image \(x\) is malign and should not be sampled. We mathematically formalize our goal as: Sample from the censored distribution

\[p_{}(x) p_{}(x)r(x).\]

Human feedback.The definition of benign and malign images are specified through human feedback. Specifically, we ask a human annotator to provide binary feedback \(Y\{0,1\}\) for each image \(X\) through a simple graphical user interface shown in Appendix E. The feedback takes 1-3 human-minutes for the relatively easier censoring tasks and at most 10-20 human-minutes for the most complex task that we consider. Using the feedback data, we train a _reward model_\(r_{} r\), which we further detail in Section 3.

Evaluation.The evaluation criterion of our methodology are the human time spent providing feedback, quantified by direct measurement, and sample quality, quantified by precision and recall.

In this context, _precision_ is the proportion of benign images, and _recall_ is the sample diversity of the censored generation. Precision can be directly measured by asking human annotators to label the final generated images, but recall is more difficult to measure. Therefore, we primarily focus on precision for quantitative evaluation. We evaluate recall qualitatively by providing the generated images for visual inspection.

## 3 Reward model and human feedback

Let \(Y\) be a random variable such that \(Y=1\) if \(X\) is benign and \(Y=0\) if \(X\) is malign. Define the time-independent reward function as

\[r(X)=(Y=1\,|\,X).\]```
0: Images: malign \(\{X^{(1)},,X^{(N_{M})}\}\), benign \(\{X^{(N_{M}+1)},,X^{(N_{M}+N_{B})}\}\) (\(N_{M}<N_{B}\)) for\(k=1,,K\)do  Randomly select with replacement \(N_{M}\) benign samples \(X^{(N_{M}+i_{1})},,X^{(N_{M}+i_{N_{M}})}\).  Train reward model \(r_{_{k}}^{(k)}\) with \(\{X^{(1)},,X^{(N_{M})}\}\{X^{(N_{M}+i_{1})},,X^{(N_{M}+i_{N_{M}}) }\}\). endfor return\(r_{}=_{k=1}^{K}r_{_{k}}^{(k)}\)
```

**Algorithm 1** Reward model ensemble

As we later discuss in Section 4, time-dependent guidance requires a time-dependent reward function. Specifically, let \(X p_{}\) and \(Y\) be its label. Let \(\{X_{t}\}_{t[0,T]}\) be images corrupted by the VP SDE (1) with \(X_{0}=X\). Define the time-dependent reward function as

\[r_{t}(X_{t})=(Y=1\,|\,X_{t})t[0,T].\]

We approximate the reward function \(r\) with a _reward model_\(r_{}\), i.e., we train

\[r_{}(X) r(X) r_{}(X_{t},t) r_{t}(X _{t}),\]

using human feedback data \((X^{(1)},Y^{(1)}),,(X^{(N)},Y^{(N)})\). (So the time-dependent reward model uses \((X_{t}^{(n)},Y^{(n)})\) as training data.) We use weighted binary cross entropy loss. In this section, we describe the most essential components of the reward model while deferring details to Appendix F.

The main technical challenge is achieving extreme human-feedback efficiency. Specifically, we have \(N<100\) in most setups we consider. Finally, we clarify that the diffusion model (score network) is not trained or fine-tuned. We use relatively large pre-trained diffusion models [12; 36], but we only train the relatively lightweight reward model \(r_{}\).

### Reward model ensemble for benign-dominant setups

In some setups, benign images constitute the majority of uncensored generation. Section 5.2 considers such a _benign-dominant_ setup, where 11.4% of images have stock photo watermarks and the goal is to censor the watermarks. A random sample of images provided to a human annotator will contain far more benign than malign images.

To efficiently utilize the imbalanced data in a sample-efficient way, we propose an ensemble method loosely inspired by ensemble-based sample efficient RL methods [23; 6]. The method trains \(K\) reward models \(r_{_{1}}^{(1)},,r_{_{K}}^{(K)}\), each using a shared set of \(N_{M}\) (scarce) malign images joined with \(N_{M}\) benign images randomly subsampled bootstrap-style from the provided pool of \(N_{B}\) (abundant) benign data as in Algorithm 1. The final reward model is formed as \(r_{}=_{k=1}^{K}r_{_{k}}^{(k)}\). Given that a product becomes small when any of its factor is small, \(r_{}\) is effectively asking for unanimous approval across \(r_{_{1}}^{(1)},,r_{_{K}}^{(K)}\).

In experiments, we use \(K=5\). We use the same neural network architecture for \(r_{_{1}}^{(1)},r_{_{K}}^{(K)}\), whose parameters \(_{1},,_{K}\) are either independently randomly initialized or transferred from the same pre-trained weights as discussed in Section 3.3. We observe that the ensemble method significantly improves the precision of the model without perceivably sacrificing recall.

### Imitation learning for malign-dominant setups

In some setups, malign images constitute the majority of uncensored generation. Section 5.3 considers such a _malign-dominant_ setup, where 69% of images are tench (fish) images with human faces and the goal is to censor the images with human faces. Since the ratio of malign images starts out high, a single round of human feedback and censoring may not sufficiently reduce the malign ratio.

Therefore, we propose an imitation learning method loosely inspired by imitation learning RL methods such as DAgger . The method collects human feedback data in multiple rounds and improves the reward model over the rounds as described in Algorithm 2. Our experiment of Section 5.3 indicates that 2-3 rounds of imitation learning dramatically reduce the ratio of malign images. Furthermore,imitation learning is a practical model of an online scenario where one continuously trains and updates the reward model \(r_{}\) while the diffusion model is continually deployed.

Ensemble vs. imitation learning.In the benign-dominant setup, imitation learning is too costly in terms of human feedback since acquiring sufficiently many (\( 10\)) malign labels may require the human annotator to go through too many benign labels (\( 1000\)) for the second round of human feedback and censoring. In the malign-dominant setup, one can use a reward model ensemble, where reward models share the benign data while bootstrap-subsampling the malign data, but we empirically observe this to be ineffective. We attribute this asymmetry to the greater importance of malign data over benign data; the training objective is designed so as our primary goal is to censor malign images.

### Transfer learning for time-independent reward

To further improve human-feedback efficiency, we use transfer learning. Specifically, we take a ResNet18 model [17; 18] pre-trained on ImageNet1k  and replace the final layer with randomly initialized fully connected layers which have 1-dimensional output features. We observe training all layers to be more effective than training only the final layers. We use transfer learning only for training time-independent reward models, as pre-trained time-dependent classifiers are less common. Transfer learning turns out to be essential for complex censoring tasks (Sections 5.2, 5.4 and 5.5), but requires guidance techniques other than the simple classifier guidance (see Section 4).

## 4 Sampling

In this section, we describe how to perform censored sampling with a trained reward model \(r_{}\). We follow the notation of Section 1.1.

Time-dependent guidance.Given a time-dependent reward model \(r_{}(X_{t},t)\), our censored generation follows the SDE

\[_{}(_{t},t)& =_{}(_{t},t)-}  r_{t}(_{t})\\ d_{t}&=_{t}(}}_{}(_{t},t)- _{t})dt+}d_{t},_{T}(0,I) \]

for \(t[0,T]\) with \(>0\). From the standard classifier-guidance arguments [42, Section I], it follows that \(X_{0} p_{}(x) p_{}(x)r(x)\) approximately when \(=1\). The parameter \(>0\), which we refer to as the _guidance weight_, controls the strength of the guidance, and it is analogous to the "gradient scale" used in prior works . Using \(>1\) can be viewed as a heuristic to strengthen the effect of the guidance, or it can be viewed as an effort to sample from \(p^{()}_{} p_{}r^{}\).

Time-independent guidance.Given a time-independent reward model \(r_{}(X_{t})\), we adopt the ideas of universal guidance  and perform censored generation via replacing the \(_{}\) of (2) with

\[_{}(_{t},t)& =_{}(_{t},t)-}  r(_{0}),\\ _{0}&=[X_{0}\,|X_{t}=_{ t}]=_{t}-}_{}(_{t},t)}{ }} \]for \(t[0,T]\) with \(>0\). To clarify, \(\) differentiates through \(_{0}\). While this method has no mathematical guarantees, prior work  has shown strong empirical performance in related setups.2

Backward guidance and recurrence.The prior work  proposes _backward guidance_ and _self-recurrence_ to further strengthen the guidance. We find that adapting these methods to our setup improves the censoring performance. We provide the detailed description in Appendix G.

## 5 Experiments

We now present the experimental results. Precision (censoring performance) was evaluated with human annotators labeling generated images. The human feedback time we report includes annotation of training data for the reward model \(r_{}\), but does not include the annotation of the evaluation data.

### MNIST: Censoring 7 with a strike-through cross

In this setup, we censor a handwriting variation called "crossed 7", which has a horizontal stroke running across the digit, from an MNIST generation, as shown in Figure 0(a). We pre-train our own diffusion model (score network). In this benign-dominant setup, the baseline model generates about 11.9% malign images.

We use 10 malign samples to perform censoring. This requires about 100 human feedback labels in total, which takes less than 2 minutes to collect. We observe that such minimal feedback is sufficient for reducing the proportion of crossed 7s to \(0.42\%\) as shown in Figure 0(b) and Figure 1(a). Further details are provided in Appendix H.

Ablation studies.We achieve our best results by combining the time-dependent reward model ensemble method described in Section 3.1 and the universal guidance components (backward guidance with recurrence) detailed in Appendix G. We verify the effectiveness of each component through an ablation study, summarized in Figure 1(a). Specifically, we compare the censoring results using a reward model ensemble (labeled "**Ensemble**" in Figure 1(a)) with the cases of using (i) a single reward model within the ensemble (trained on 10 malign and 10 benign images; labeled "**Single**") and (ii) a standalone reward model separately trained on the union of all training data (10 malign and 50 benign images; labeled "**Union**") used in ensemble training. We also show that the backward and recurrence components do provide an additional benefit (labeled "**Ensemble+Universal**").

Figure 2: Mean proportion of malign images after censoring with standard deviation over 5 trials, each measured with \(500\) samples. Reward ensemble outperforms non-ensemble models, and the universal guidance components further improve the results. **Left:** Censoring “crossed 7” from MNIST. Before censoring, the proportion is 11.9%. The mean values of each point are: 1.30%, 0.98%, 0.60%, and **0.42%. Right:** Censoring watermarks from LSUN Church. Before censoring, the proportion is 11.4%. The mean values of each point are: 3.02%, 3.84%, 1.36%, and **0.76%**.

### LSUN church: Censoring watermarks from latent diffusion model

In the previous experiment, we use a full-dimensional diffusion model that reverses the forward diffusion (1) in the pixel space. In this experiment, we demonstrate that censored generation with minimal human feedback also works with latent diffusion models (LDMs) , which perform diffusion on a lower-dimensional latent representation of (variational) autoencoders. We use an LDM3 pre-trained on the \(256 256\) LSUN Churches  and censor the stock photo watermarks. In this benign-dominant setup, the baseline model generates about 11.4% malign images.

Training a time-dependent reward model in the latent space to be used with an LDM would introduce additional complicating factors. Therefore, for simplicity and to demonstrate multiple censoring methods, we train a time-_independent_ reward model ensemble and apply time-independent guidance as outlined in Section 4. To enhance human-feedback efficiency, we use a pre-trained ResNet18 model and use transfer learning as discussed in Section 3.3. We use 30 malign images, and the human feedback takes approximately 3 minutes. We observe that this is sufficient for reducing the proportion of images with watermarks to 0.76% as shown in Figure 0(d) and Figure 1(b). Further details are provided in Appendix I.

Ablation studies.We achieve our best results by combining the time-independent reward model ensemble method described in Section 3.1 and the universal guidance components (recurrence) detailed in Appendix G. As in Section 5.1, we verify the effectiveness of each component through an ablation study, summarized in Figure 1(b). The label names follow the same rules as in Section 5.1. Notably, on average, the "single" models trained with 30 malign and 30 benign samples outperform the "union" models trained with 30 malign and 150 malign samples.

### ImageNet: Tench (fish) without human faces

Although the ImageNet1k dataset contains no explicit human classes, the dataset does contain human faces, and diffusion models have a tendency to memorize them . This creates potential privacy risks through the use of reverse image search engines . A primary example is the ImageNet class "tench" (fish), in which the majority of images are humans holding their catch with their celebrating faces clearly visible and learnable by the diffusion model.

In this experiment, we use a conditional diffusion model4 pre-trained on the \(128 128\) ImageNet dataset  as baseline and censor the instances of class "tench" containing human faces (but not other human body parts such as hands and arms). In this malign-dominant setup, the baseline model generates about 68.6% malign images.

We perform 3 rounds of imitation learning with 10 malign and 10 benign images in each round to train a single reward model. The human feedback takes no more than 3 minutes in total. We observe that this is sufficient for reducing the proportion of images with human faces to \(1.0\%\) as shown in Figure 0(f) and Figure 3. Further details are provided in Appendix J.

Ablation studies.We verify the effectiveness of imitation learning by comparing it with training the reward model at once using the same number of total samples. Specifically, we use 20 malign and 20 benign samples from the baseline generation to train a reward model (labeled "**non-imitation (20 malign)**" in Figure 2(a)) and compare the censoring results with round 2 of imitation learning; similarly we compare training at once with 30 malign and 30 benign samples (labeled "**non-imitation (30 malign)**") and compare with round 3. We consistently attain better results with imitation learning. As in previous experiments, the best precision is attained when backward and recurrence are combined with imitation learning (labeled "**30+Univ**").

We additionally compare our censoring method with another approach: rejection sampling, which simply generates samples from the baseline model and rejects samples \(X\) such that \(r_{}(X)\) is less than the given acceptance threshold. Figure 2(b) shows that rejection sampling yields worse precision compared to the guided generation using the same reward model, even when using the conservative threshold 0.8. We also note that rejection sampling in this setup accepts only 28.2% and 25.5% of the generated samples respectively for thresholds 0.5 and 0.8 on average, making it suboptimal for situations where reliable real-time generation is required.

### LSUN bedroom: Censoring broken bedrooms

Generative models often produce images with visual artifacts that are apparent to humans but are difficult to detect and remove via automated pipelines. In this experiment, we use a pre-trained diffusion model5 trained on \(256 256\) LSUN Bedroom images  and censor "broken" images as perceived by humans. In Appendix K, we precisely define the types of images we consider to be broken, thereby minimizing subjectivity. In this benign-dominant setup, the baseline model generates about 12.6% malign images.

This censoring task is the most difficult one we consider, and we design this setup as a "worst case" on the human work our framework requires. We use 100 malign samples to train a reward-model ensemble. This requires about 900 human feedback labels, which takes about 15 minutes to collect. To enhance human-feedback efficiency, we use a pre-trained ResNet18 model and use transfer learning as discussed in Section 3.3. We observe that this is sufficient for reducing the proportion of malign images to 1.36% as shown in Figure 0(h) and Figure 4. Further details are provided in Appendix K.

Ablation studies.We achieve our best results by combining the (time-independent) reward ensemble and backward guidance with recurrence. We verify the effectiveness of each component through an ablation study summarized in Figure 4. We additionally find that rejection sampling, which rejects a sample \(X\) such that \(_{k=1}^{K}r_{_{k}}^{(k)}(X)\) is less than a threshold, yields worse precision compared to the guided generation using the ensemble model and has undesirably low average acceptance ratios of 74.5% and 55.8% when using threshold values 0.5 and 0.8, respectively.

Figure 4: Mean proportion of malign (broken) bedroom images with standard deviation over 5 trials, each measured with 500 samples. Before censoring, the malign proportion is 12.6%. The mean values of each point are: 8.68%, 8.48%, 4.56%, **1.36%**, 4.16%, and 2.30%.

Figure 3: Mean proportion of malign tench images (w/ human face) with standard deviation over 5 trials, each measured with 1000 samples. **Left:** Before censoring, the proportion is 68.6%. Using imitation learning and universal guidance, it progressively drops to 17.8%, 7.5%, 2.2%, and **1.0%**. Non-imitation learning is worse: with 20 and 30 malign images, the proportions are 10.7% and 6.8%. **Right:** With acceptance thresholds 0.5 and 0.8, rejection sampling via reward models from round 1 produces 32.0% and 29.8% of malign images, worse than our proposed guidance-based censoring.

### Stable Diffusion: Censoring unexpected embedded texts

Text-to-image diffusion models, despite their remarkable prompt generality and performance, are known to often generate unexpected and unwanted artifacts or contents. In this experiment, we use Stable Diffusion6 v1.4 to demonstrate that our methodology is readily applicable to aligning text-to-image models. The baseline Stable Diffusion, when given the prompt "A photo of a human", occasionally produces prominent embedded texts or only display texts without any visible human figures (as in Figure 0(i)). We set the output resolution to \(512 512\) and censor the instances of this behavior. In this benign-dominant setup, the baseline model generates about 23.7% malign images.

This censoring problem deals with the most complex-structured model and demonstrates the effectiveness of our methodology under the text-conditional setup. We use 100 malign samples to train a reward-model ensemble. This requires about 600 human feedback labels, which takes no more than 5 minutes to collect. We use a pre-trained ResNet18 model and use transfer learning as discussed in Section 3.3. We observe that this is sufficient for reducing the proportion of malign images to 1.24% as shown in Figure 0(j) and Figure 5. Further details are provided in Appendix L.

Ablation studies.We achieve our best results by combining the time-independent reward model ensemble method described in Section 3.1 and the universal guidance components (recurrence) detailed in Appendix G. We verify the effectiveness of each component through an ablation study, summarized in Figure 5. Similarly as in Section 5.2, we observe that the "single" models outperform the "union" models on average.

Note on the guidance weight \(\).We speculate that the effective scale of the guidance weight \(\) grows (roughly doubles) with 1) significant scale growth in terms of data size and 2) the introduction of new modality (e.g. unconditional or class-conditional \(\) text-conditional model). We use \(=1.0\) for the simplest task of Section 5.1, while we use \(=2.0\) for Sections 5.2 and 5.4 where the data size grows to \(256 256\). For this section where we use text-conditioning, we use \(=4.0\).

## 6 Conclusion

In this work, we present censored sampling of diffusion models based on minimal human feedback and compute. The procedure is conceptually simple, versatile, and easily executable, and we anticipate our approach to find broad use in aligning diffusion models. In our view, that diffusion models can be controlled with extreme data-efficiency, without fine-tuning of the main model weights, is an interesting observation in its own right (although the concept of guided sampling itself is, of course, not new [40; 12; 32; 35]). We are not aware of analogous results from other generative models such as GANs or language models; this ability to adapt/guide diffusion models with external reward functions seems to be a unique trait, and we believe it offers a promising direction of future work on leveraging human feedback with extreme sample efficiency.

Figure 5: Mean proportion of malign images (with embedded text) with standard deviation over 5 trials, each measured with 500 samples. Before censoring, the malign proportion is 23.7%. The mean values of each point are: 7.10%, 7.95%, 3.86%, **1.24%**.