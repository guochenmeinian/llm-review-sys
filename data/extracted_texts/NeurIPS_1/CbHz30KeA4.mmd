# Taming "data-hungry" reinforcement learning?

Stability in continuous state-action spaces

Yaqi Duan

Department of Technology, Operations, and Statistics

Stern School of Business, New York University

New York, NY 10012

yaqi.duan@stern.nyu.edu

Martin J. Wainwright

Laboratory for Information and Decision Systems, Statistics and Data Science Center

Department of Electrical Engineering and Computer Science, and Department of Mathematics

Massachusetts Institute of Technology Cambridge, MA 02139

wainwright@gmail.com

###### Abstract

We introduce a novel framework for analyzing reinforcement learning (RL) in continuous state-action spaces, and use it to prove fast rates of convergence in both off-line and on-line settings. Our analysis highlights two key stability properties, relating to how changes in value functions and/or policies affect the Bellman operator and occupation measures. We argue that these properties are satisfied in many continuous state-action Markov decision processes. Our analysis also offers fresh perspectives on the roles of pessimism and optimism in off-line and on-line RL.

## 1 Introduction

Many domains of science and engineering involve making a sequence of decisions over time, with previous decisions influencing the future in uncertain ways . For instance, clinicians managing diabetes  or engineers optimizing plasma control in tokamak systems  must develop policies that adapt based on evolving conditions and lead to desirable outcomes over a longer period. Markov decision processes (MDPs) and reinforcement learning (RL) provide frameworks and methods for estimating effective policies for such sequential problems. While RL excels in data-rich scenarios such as competitive gaming (e.g., AlphaGo and its extensions ), its application in data-scarce areas like healthcare  and finance  remains challenging due to lack of history, or underlying non-stationarity. With limited data, characterizing and improving the _sample complexity_ of RL methods becomes critical.

Considerable research effort has been devoted to studying RL sample complexity in many settings. Existing studies for either the generative or the off-line settings (e.g., ) give procedures that, when applied to a dataset of size \(n\), yield a value gap that decays at the rate \(1/\). In the on-line setting, there are various procedures that yield cumulative regret that grows at the rate \(\) (e.g., ). In contrast, the main result of this paper is to formalize conditions, suitable for RL in continuous domains, under which _much faster rates can be obtained using the same dataset_, achieving a value gap decay of \(1/n\) and reducing regret growth to \( T\).

As revealed by our analysis, these accelerated rates depend on certain _stability properties_, ones that--as we argue--are naturally satisfied in many control problems with continuous state-actionspaces. Roughly speaking, these conditions ensure that the evolution of the dynamic system depends in a "smooth" way on the influence of decision policy. Such notions of stability should be expected in various controlled systems with continuous state-action spaces. In robotics, for example, a minor torque or motion perturbation that occurs during a single step should not cause a notable deviation from the intended trajectory. Similarly, in clinical treatment, slight deviations in medication dosage should not significantly compromise effectiveness or safety.

### A simple illustrative example: Mountain Car

The "Mountain Car" problem, a benchmark continuous control task, illustrates the acceleration phenomenon and underlying stability. In this task, as shown in Figure 1(a), a car must reach the top of a hill by adjusting its acceleration within the interval \([-1,1]\). We employed fitted \(Q\)-iteration (FQI) with carefully selected linear basis functions to derive near-optimal policies with off-line data. This learning procedure exhibits a value sub-optimality decay at a rate of \(1/n\), a significant improvement over the classical rate of \(1/\), as detailed in Figure 1(b). (See Appendix D for further explanation. The experiment ran for 3 days on two laptops, each equipped with an Apple M2 Pro CPU and 16 GB RAM.) In this example, slight perturbations in the driving policy lead to only modest changes in future trajectories, which shows the stability. Our theoretical analysis confirms that fast rates are achievable in this and similar continuous control tasks when such stability properties are present.

### Contributions of this paper

With this high-level perspective in mind, let us summarize the key contributions of this paper.

Fast rate of convergence:We develop a framework for analyzing RL in continuous state-action spaces, and use it to prove a general result (Theorem 1) under which fast rates can be obtained. The key insight is that stability conditions lead to upper bounds on the value sub-optimality that are proportional to the _squared_ norm of Bellman residuals. In the off-line setting, this quadratic scaling improves convergence from a rate of \(n^{-}\) to \(n^{-1}\), while in on-line learning, it enhances the regret bound from \(\) to \( T\).

Reconsidering pessimism and optimism principles:Our framework provides a novel perspective on the roles of pessimism  and optimism  in off-line and on-line RL. Our theory reveals that there are settings in which _neither pessimism nor optimism_ are required for effective policy optimization--in particular, they are not required as long as one has a sufficiently accurate pilot estimate policy. Moreover, our analysis shows that some procedures based on certainty

Figure 1: Illustration of the “fast rate” phenomenon using FQI on the Mountain Car problem. Each **red point** in the plot represents the average value sub-optimality \(J(^{})-J(_{n})\) from \(T=80\) Monte Carlo trials, with the shaded area showing twice the standard errors. The **blue dashed line** is a least-squares fit to the last \(6\) data points, yielding a \(95\%\) confidence interval of \((-1.084,-0.905)\) for the slope, significantly faster than the typical \(-0.5\) “slow rate”.

equivalence can achieve fast-rate convergence, showing that the benefits gained from incorporating additional pessimism or optimism measures may be limited in this context.

### Related work

In this section, we discuss related work having to do with fast rates in optimization and statistics.

Fast rates in stochastic optimization and risk minimization:For many statistical estimators (e.g., likelihood methods, empirical risk minimization), it is well-understood that the local geometry around the optimum determines whether fast rates can be obtained. For instance, when the loss function exhibits some form of strong convexity (such as exp-concave loss) or strict saddle properties, it can lead to significant reductions in additive regret from \(()\) to just \(( T)\) in stochastic approximation (e.g., ), or a decrease in the error rate from \(n^{-}\) to \(n^{-1}\) in empirical risk minimization [23; 14]. These fast rate phenomena rely on a form of stability, one which relates the similarity of functions to the closeness of their optima. Our work develops a new framework for analyzing value-based RL methods, focusing on identifying specific stability conditions and inherent curvature properties that promote fast rate convergence in RL, similar to the role of stability analysis in statistical learning.

Fast rates in reinforcement learning:In the RL literature, there are various lines of work related to fast rates, but the underlying mechanisms are typically different from those considered here. For problems with discrete state-action spaces, there is a line of recent work [17; 16; 33; 25] that performs gap/marginal-dependent analyses of RL algorithms. However, such separation assumptions are not helpful for continuous action spaces. Other work for discrete state-action spaces  has shown convergence rates in off-line RL are influenced by data quality, with a nearly-expert dataset enabling faster rate. In contrast, our analysis reveals that for off-line RL in continuous domains, fast convergence can occur whether or not the dataset has good coverage properties.

An important sub-class of continuous state-action problems are those with linear dynamics and quadratic reward functions (LQR for short). For such problems, it has been shown [24; 29] that sub-optimality can be connected with the squared error in system identification. Our general theory can also be used to derive guarantees for LQR problems, as we explore in more detail in a follow-up paper . Stability also arises in the analysis of (deterministic) policy optimization and Newton-type algorithms [26; 3], where it is possible to show superlinear convergence in a local neighborhood. This accelerated rate stems from the smoothness of the on-policy transition operator \(^{_{f}}\) with respect to changes in the value function \(f\); for instance, see condition (10) in Puterman and Brumelle . Our framework exploits related notions of smoothness, but is tailored to the stochastic setting of reinforcement learning, in which understanding the effect of function approximation and finite sample sizes is essential.

## 2 Fast rates for value-based reinforcement learning

Let us now set up and state the main result of this paper. We begin in Section 2.1 with background on Markov decision processes (MDPs) and value-based methods, before turning to the statement of our main result in Section 2.2. In Section 2.3, we provide intuition for why stability leads to faster rates, and discuss consequences for both the off-line and on-line settings of RL.

### Markov decision processes and value-based methods

Basic set-up:We consider an episodic Markov decision process (MDP) defined by a quadruple \(,,}=\{_{h}\}_{ h=1}^{H-1},\{r_{h}\}_{h=1}^{H}\). We assume that the rewards \(r_{h}:\) are known; however, this condition can be relaxed. A policy \(_{h}\) at time \(h\) is a mapping from any state \(s\) to a distribution \(_{h}( s)\) over the action space \(\). If the support of \(_{h}( s)\) is a singleton, we also let \(_{h}(s)\) denote the single action to be chosen at state \(s\). Given an initial distribution \(_{1}\) over the states at time \(h=1\), the _expected reward_ obtained by choosing actions according to a policy sequence \(=(_{1},,_{H})\) is given by \(J() J(;_{1})=\!_{ _{1},}_{h=1}^{H}r_{h}(S_{h},A_{h})\), where \(S_{1}\!\!_{1}\), \(S_{h+1}_{h}( S_{h},A_{h})\) and \(A_{h}_{h}( S_{h})\) for \(h=1,2,,H\). Our goal is to estimate an _optimal policy_\(^{}_{}J()\).

Value functions and Bellman operators:Starting from a given state-action pair \((s,a)\) at stage \(h\), the expected return over subsequent stages defines the _state-action value function_\(Q_{h}^{}(s,a):\,=_{}_{h^{}=h}^{H}r(S_{h^{ }},A_{h^{}})\;\;S_{h}=s,A_{h}=a\,\). The sequence of functions \(^{}=(Q_{1}^{},,Q_{H}^{})\) known as the \(Q\)_-functions_ associated with \(\).

The \(Q\)-functions \(^{}\) have an important connection with the _Bellman evaluation operator_ for \(\). For any policy \(\) and stage \(h\), we introduce a linear transition operator \((_{h}^{}f)(s,a):\,=_{S}f(s^{},a^{ })\;_{h}(ds^{}\;\;s,a)\,_{h+1}(da^{}\; s ^{})\) for any function \(f^{S}\). With this notation, the _Bellman evaluation operator_ at stage \(h\) takes the form

\[(_{h}^{}f)(s,a):\,=r_{h}(s,a)+(_{h}^{}f)(s,a). \]

From classical dynamic programming, the \(Q\)-functions \(^{}\) must satisfy the Bellman relations \(Q_{h}^{}(s,a)=(_{h}^{}Q_{h+1}^{})(s,a)\) for \(h=1,,H-1\).

Bellman principle for optimal policies:Under mild regularity conditions, there is at least one policy \(^{}\) such that, for any other policy \(\), we have \(Q_{h}^{^{}}(s,a) Q_{h}^{}(s,a)\), for any \(h[H]\), and uniformly over all state-action pairs \((s,a)\). Any optimal policy \(^{}\) must be greedy with respect to the optimal \(Q\)-function \(^{}\). By classical dynamic programming, the optimal \(Q\)-function \(^{}\) is obtained by setting \(Q_{H}^{}=r_{H}\), and then recursively computing \(Q_{h}^{}=_{h}^{}\;Q_{h+1}^{}\) for \(h=H-1,,2,1\), with the _Bellman optimality operator_ defined as

\[(_{h}^{}\;f)(s,a):\,=r_{h}(s,a)+_{h}_{a^ {}}f(S^{},a^{})\;\;s,a S^{}_{h}( s,a). \]

Value-based RL methodsThe main result of this paper applies to a broad class of methods for reinforcement learning. They are known as _value-based_, due to their reliance on the following two step approach for approximating an optimal policy \(^{}\): (1) Construct an estimate \(}=(_{1},,_{H})\) of the optimal value function \(^{}=(Q_{1}^{},,Q_{H}^{})\). (2) Use \(}\) to compute the greedy-optimal policy \(_{h}(s)_{a}_{h}(s,a)\) for \(h=1,2,,H\). It should be noted that there is considerable freedom in the design of a value-based method, since different methods can be used to approximate value functions in Step 1. Rather than applying to a single method, our main result applies to a very broad class of these methods.

Underlying any value-based method is a class \(\) of functions \((s,a) f(s,a)\) used to approximate the state-action value functions.1 We assume that the function class \(\) is rich enough--relative to the Bellman evaluation operators--to ensure that for any greedy policy \(\) induced by some \(=(f_{1},,f_{H})^{H}\), we have the inclusion \(_{h}^{}\) for \(h=1,,H-1\). We see that this condition depends on the structure of the transition distributions \(_{h}( s,a)\). In many practical examples, the reward function itself has some number of derivatives, and these transition distributions perform some type of smoothing, so that we expect that the output of the Bellman update, given a suitably differentiable function, will remain suitably differentiable.

### Stable problems have fast rates

We now turn the central question in understanding the behavior of any value-based method:

\[}$}\\ )-J(})$?}\]

At a high level, existing theory provides guarantees of the following type: if the \(Q\)-function estimates are \(\)-accurate for some \((0,1)\), then the value gap is bounded by a quantity proportional to \(\). In contrast, our main result shows that when the MDP is stable in a suitable sense, the value gap can be upper bounded by a quantity proportional to \(^{2}\). This _quadratic as opposed to linear scaling_ encapsulates the "fast rate" phenomenon of this paper.

Our analysis isolates two key stability properties required for faster rates; both are Lipschitz conditions with respect to a certain norm. Here we define them with respect to the \(L^{2}\)-norm induced by the state-action occupation measure induced by the optimal policy--namely

\[\|f\|_{h}: =_{^{}}[f^{2}(S_{h},A_{h})]}f^{2}, \]

and over a neighborhood \(\) of the optimal \(Q\)-value function \(^{}\).

Bellman stability:The first condition measures the stability of the Bellman optimality operator (2): in particular, we require that there is a scalar \(_{h}^{}\) such that

\[\|_{h}^{}\:f_{h+1}-_{h}^{}\:Q_{h+1}^{ }\|_{h}\;\;_{h}^{}\:\|f_{h+1}-Q_{h+1}^{} \|_{h+1}\] ( **Stb** ( \[\] ))

for any \(\). Moreover, for any pair \((h,h^{})\) of indices such that \(1 h<h^{} H-1\), we define

\[_{h,h^{}}(^{}): =_{h}^{}\:_{h+1}^{}_{h^{}-1}^{ }\:.\]

Condition **(Stb** ( \[\] ) is directly linked to the stability of estimating the \(Q\)-function \(^{}\). In typical estimation procedures, such as approximate dynamic programming, the estimation is carried out iteratively in a backward manner, so that it is important to control the propagation of estimation errors across the iterations. Condition **(Stb** ( \[\] ) captures this property, since it implies that

\[\|_{h}^{}\:_{h+1}^{}\:_{ h^{}-1}^{}\:f_{h^{}}-_{h}^{}\:_{h+1}^{ }\:_{h^{}-1}^{}\:Q_{h}^{}\|_{h}\; \;_{h,h^{}}(^{})\|f_{h^ {}}-Q_{h^{}}^{}\|_{h^{}},\]

which shows how the estimation error \((f_{h^{}}-Q_{h^{}}^{})\) at step \(h^{}\) can be controlled in terms of estimation error at an earlier time step \(h h^{}\).

Occupation measure stability:Our second condition is more subtle, and is key in our argument. Let us begin with some intuition. Consider two sequences of policies

\[(_{1}^{},,_{h-1}^{},_{h}^{},_{h+1}^{ },,_{h^{}}^{})( _{1}^{},,_{h-1}^{},_{h},_{h+1}^{},,_{h ^{}}^{})\]

that only differ at the \(h\)-th step, where \(_{h}^{}\) has been replaced by \(_{h}\). These two policy sequences induce Markov chains whose distributions differ from stage \(h\) onwards, and our second condition controls this difference in terms of the difference \(\|f_{h}-Q_{h}^{}\|_{h}\) between the two \(Q\)-functions \(f_{h}\) and \(Q_{h}^{}\) that induce \(_{h}\) and \(_{h}^{}\), respectively.

We adopt \(_{h}^{}\) as a convenient shorthand for the transition operator \(_{h}^{^{}}\), and define the multi-step transition operator \(_{h,h^{}}^{}:=_{h}^{}\:_{h+1 }^{}_{h^{}-1}^{}\). Using this notation, for any \(h^{} h+1\), we require that there is a scalar \(_{h,h^{}}(^{})\) such that

\[_{g\\ \|g\|_{h^{}}>0}_{ ^{}}\![(_{h,h^{}}^{}g )(S_{h},_{h}^{}(S_{h}))-(_{h,h^{}}^{ }g)(S_{h},_{h}(S_{h}))]|}{\|g\|_{h^{}}}_{h,h^{}}(^{})\:-Q_{h}^{ }\|_{h}}{\|Q_{h}^{}\|_{h}}\:\:())\]

for any \(\). The renormalization in this definition serves to enforce a natural scale invariance.

With these notions of stability in hand, we are now equipped to state our main result. Taking as input a value function estimate \(}\), it relates the induced value gap to the _Bellman residuals_\(_{h}^{}\:_{h+1}-_{h}\). Note that these residuals are a way of quantifying proximity to the optimal value function \(^{}\), which has Bellman residual zero by definition. We assume that \(}\) has Bellman residuals bounded as

\[\|\:_{h}^{}\:_{h+1}-_{h}\:\|_ {h}\;\;_{h}h=1,2,,H-1\] (4a) for some sequence \[=(_{1},,_{H-1},_{ H}=0)\] that satisfies the constraint \[_{h}_{h^{}=h+1}^{H}_{h^{}} h=1,2,,H-1. \]

This last condition means that the Bellman residual \(_{h}\) is larger than or equal to the average of the bounds established after step \(h+1\). It is natural because estimating at step \(h\) is at least as challenging as a stage \(h^{}>h\); indeed, any such state \(h^{}\) occurs earlier in the dynamic programming backward iteration process. As a special case, the bound (4b) holds when \(_{h}=\) for all stages.

With this set-up, we have the following guarantee in terms of the stability coefficients \(_{h,h^{}}(^{})\) and \(_{h,h^{}}(}^{})\) from conditions **(Stb** (\[\] ) and **(Stb** ( \[\] )).

**Theorem 1**.: _There is a neighborhood of \(^{*}\) such that for any value function estimate \(}\) with \(\)-bounded Bellman residuals (4a), the induced greedy policy \(}\) has value gap bounded as_

\[J(^{})-J(})\ \ 2_{h=1}^{H-1}_{ h}\|_{h}}\,_{h^{}=h}^{H-1}_{h,h^{}}(^{ })\ _{h^{}}}_{h^{}=h}^{H-1}_{h,h^{}}(^{})\ _{h^{}}}. \]

See Appendix A for the proof.

Treating dependence on the stability coefficients as constant, the main take-away is that value sub-optimality is bounded above by a quantity proportional to the _squared_ norm of the Bellman residuals. Concretely, if the Bellman residuals are uniformly upper bounded by some \(\), then equation (5) leads to an upper bound of the form

\[J(^{})-J(}) c\;H^{3}\;^{2},\]

where \(c\) is a universal constant. Due to the quadratic scaling in the Bellman residual error \(\), this bound is substantially tighter than the linear in \(\) rates afforded by a conventional analysis.

### Intuition for fast rates: Smoothness and cancelling terms in the telescope bound

Why does "fast rate" phenomenon formalized in Theorem 1 arise? The fast rates proved in this paper are established by a novel argument, starting from a known telescope bound, which we begin by stating. Given a \(Q\)-function estimate \(}=_{1},,_{H}\), let \(}\) denote the induced greedy policy. Then the value gap of \(}\) with respect to an arbitrary comparator policy \(\) is bounded as

\[J()-J}\ \ _{h=1}^{H-1} _{}-_{}} ^{}_{h}\ _{h+1}-_{h}(S_{h},A_{h})\,. \]

This result follows by a "telescope" relation induced by the structure of the Bellman updates.3 For completeness, we provide a proof of the telescope bound in Appendix E.2.

A key feature of inequality (6) is the difference of two expectations \(_{}-_{}}\), corresponding to the occupation measures under \(\) versus \(}\). In standard uses of this inequality, an initial argument is used to guarantee that one of these expectations is negative, and so can be dropped .

In contrast, the proof of our Theorem 1 exploits a more refined approach, one that handles the difference of expectations directly. Doing so can be beneficial--and lead to "fast rates"-- because various terms in this difference can cancel each other out. Specifically, under the smoothness conditions that underlie Theorem 1, when applying the telescope inequality (6) with comparator \(=^{}\), we show that the discrepancy between the occupation measures associated with \(^{}\) and \(}\) is of the _same order_ as the Bellman residual associated with \(}\). Note that the Bellman residuals of \(}\) already appear on the right-hand side of inequality (6), so that this fortuitous cancellation can be exploited--along with a number of auxiliary results laid out in the proof--so as to upper bound the value gap by a quantity proportional to the squared Bellman residual \(^{2}\).

It is worthwhile making an explicit comparison of our cancellation approach with the more standard uses of the telescope relation, which typically consider only one portion of the Bellman residuals (e.g., ). We do so in the following two subsections.

#### 2.3.1 Pessimism for off-line RL

In the off-line instantiation of RL, the goal is to learn a "good" policy based on a pre-collected dataset \(\). Note that no further interaction with the environment is permitted, hence the notion of the learning being off-line. More precisely, an _off-line dataset_\(\) of size \(n\) consists of quadruples

\[=s_{h,\,i},a_{h,\,i},s^{}_{h,\,i},r_{h,\,i} }_{i=1}^{n}\,,\]

where \(s_{h,\,i}\) and \(a_{h,\,i}\) represent the \(i\)-th state and action at the \(h\)-th step in the MDP; \(s^{}_{h,\,i}\) is the successive state; and \(r_{h,\,i}=r_{h}(s_{h,\,i},a_{h,\,i})\) denotes the scalar reward. Note that while the successivestates are defined by transition dynamics, and the rewards by the reward function, there are no restrictions on how the state-action pairs \((s_{h,\,i},a_{h,\,i})\) are collected. That is, they need not have been generated by any fixed policy, but may have collected from some ensemble of behavioral policies, or even adaptively by human experts. The goal of off-line reinforcement learning is to use the \(n\)-sample dataset \(\) so as to estimate a policy \(}}_{n}\) that (approximately) maximizes the expected return \(J(}_{n})\). We expect that--at least for a sensible method for estimating \(}_{n}\)--the value gap \(J(^{})\ -\ J(}_{n})\) should decay to zero as \(n\) increases to infinity, and we are interested in understanding this rate of decay.

The use of pessimism is standard in off-line RL algorithms. Its purpose is to mitigate risks associated with "poor coverage" of the off-line dataset. For instance, the naive approach of simply maximizing \(Q\)-function estimates based on an off-line dataset can behave poorly when certain portions of the state-action space are not well covered by the given dataset. The pessimism principle suggests to form a _conservative estimate_ of the value function--say with

\[_{h}(s,a)_{h}^{}\;_{h+1}(s,a)\] (7a) with high probability over state-action pairs \[(s,a)\]. Thus, the estimated value \[_{h}(s,a)\] is an underestimate of the Bellman update, a form of conservatism that protects against unrealistically high estimates due to poor coverage. Doing so in the appropriate way ensures that \[-_{}}_{h}^{} \;_{h+1}-_{h}(S_{h},A_{h}) 0. \]

Applying this upper bound to the inequality (6) yields the sub-optimality bound

\[J()-J(})\ \ _{h=1}^{H-1} _{}_{h}^{}\;_{h+1}-_{h}(S_{h},A_{h})\,.\]

Upper bounds derived in this manner only contain one portion of the Bellman residual. When the value functions are approximated in a parametric way (e.g., tabular problems, linear function approximation), this line of analysis leads to value sub-optimality decaying at a "slow" \(1/\) rate in terms of the sample size \(n\) (e.g., ). In contrast, an application of Theorem 1 can lead to value gaps bounded by \(1/n\).

#### 2.3.2 Optimism in on-line RL

In the setting of on-line RL, a learning agent interacts with the environment in a sequential manner, receiving feedback in the form of rewards based on its actions. At the beginning, the learner possesses no prior knowledge of the system's dynamics. In the \(t\)-th episode, the agent learns an optimal policy \(}^{(t)}\) using existing observations, implements the policy and collects data \(s_{h}^{(t)},\,a_{h}^{(t)},\,r_{h}^{(t)}}_{h=1}^{H}\) from the new episode. In each round, the system starts at an initial state \(s_{1}^{(t)}\) independently drawn from a fixed distribution \(_{1}\).

In this on-line setting, it is common to measure the performance of an algorithm by comparing it, over the \(T\) rounds of learning, with an oracle that knows and implements an optimal policy. At each round \(t\), we incur the _instantaneous regret_\(J(^{})-J(}^{(t)})\), where \(^{}\) is any optimal policy. Over \(T\) rounds, we measure performance in terms of the _cumulative regret_

\[\{}^{(t)}\}_{t=1}^{T}:= _{\;}\ \ _{t=1}^{T}J()-J(}^{(t)})}\ =\ _{t=1}^{T}J( ^{})-J(}^{(t)})}}_{}. \]

In a realistic problem, the cumulative regret of any procedure grows with \(T\), and our goal is to obtain algorithms whose regret grows as slowly as possible.

In contrast to off-line RL, the on-line setting allows for exploring state-action pairs that have been rarely encountered; doing so makes sense since they might be associated with high rewards. Principled exploration of this type can be effected via the _optimism principle_: one constructs function estimates such that

\[_{h}(s,a)_{h}^{}\;_{h+1}(s,a) \]with high probability over state-action pairs.4 Note that \(_{h}(s,a)\) is optimistic in the sense that it is an over-estimate of the Bellman update \(^{}_{h}\;_{h+1}(s,a)\). In this way, we can ensure that

\[_{}^{}_{h}\;_{h+1}- _{h}(S_{h},A_{h}) 0. \]

Combining this inequality with the telescope bound (6) allows one to upper bound the regret as

\[\{}^{(t)}\}_{t=1}^{T}\;=\;_{t= 1}^{T}J(^{})-J(}^{(t)})}\;\; _{t=1}^{T}_{h=1}^{H-1}_{}^{(t)}} _{h}-^{}_{h}\;_{h+1}(S_{h },A_{h})\,.\]

which only includes a single portion of the Bellman residual. In the case of tabular or linear representations of the \(Q\)-functions, it results in a regret rate of \(\) (e.g., see the papers [18; 20]). In contrast, an appropriate use of Theorem 1 leads to regret growing only as \((T)\), which corresponds to a much better guarantee.

In summary, then, the fast rates obtained in this paper are based on a different approach than the standard pessimism or optimism principles. Since we deal directly with the difference of expectations in the bound (6), there is no need to nullify either of them through the use of these principles. However, it should be noted that we are assuming smoothness conditions that allow us to control this difference. As we discuss in the sequel, such smoothness conditions rule out certain "hard instances" used in past work on lower bounds (e.g. [18; 20; 21; 37]).

## 3 Consequences for linear function approximation

In this section, we explore some consequences of our general theory when applied to value-based methods using (finite-dimensional) linear function approximation.

Let \(:^{d}\) be a given feature map on the state-action space, and consider linear expansions of the form \(f_{}(s,a)=(s,a),\;_{j=1}^{d}w_{j} _{j}(s,a)\) where \(^{d}\) is a weight vector. We adopt the conventional assumption that \(\|(s,a)\|_{2} 1\) and \(r_{h}(s,a)\) for all state-action pairs. Defining the linear function class \(:\,=\,f_{}^{d}}\), we note that the Minkowski difference class \(\) is equal to \(\).

In our analysis of linear approximation, we make use of the norm \(\|f\|_{h}:\,=_{^{}}[f^{2}(S_{h},A_{h})]}\), corresponding to \(L^{2}\)-norm under the occupation measure induced by the optimal policy \(^{}\).

### Consequences for off-line RL

We now turn to some implications of Theorem 1 for off-line reinforcement learning. Let us recall the off-line setting: for each \(h=1,,H-1\), we are given a dataset \(_{h}=\{(s_{h,i},a_{h,i},s^{}_{h,i},r_{h,i})\}_{i=1}^{n}\) of quadruples, from which we can compute estimates \(}=(_{h})_{h=1}^{H}\) with certain Bellman residuals \(\{_{h}\}_{h=1}^{H-1}\), which then appear in the bound (5). The remaining factors on the right-hand side of inequality (5) do not depend on the dataset itself (but rather on structural properties of the MDP). Consequently, in terms of statistical understanding, the main challenge is to establish high-probability bounds on the Bellman residuals \(\{_{h}\}_{h=1}^{H-1}\) for a particular estimator.

#### 3.1.1 Fitted \(Q\)-iteration (FQI)

As an illustration, let us analyze the use of _fitted Q-iteration_ (FQI) for computing estimates of the \(Q\)-function. At a given stage \(h=1,,H-1\), we can use the associated data \(_{h}\) to define a regularized objective function

\[_{h}f,\,g:\,=_{h}|} _{(s_{h,i},a_{h,i},s^{}_{h,i},r_{h,i})_{h}}\!\! f(s_{h,i},\,a_{h,i})-r_{h,i}+_{a}g(s^{}_{h,i},a) }^{2}\,+_{h}^{2}(f)\,.\]

Here \(g\) represents the target function from stage \(h+1\), and it defines the targeted responses \(y_{h,i}(g):\,=r_{h,i}+_{a}g(s^{}_{h,i},a)\). For a given target \(g\), we obtain a \(Q\)-function estimate for stage \(h\) by minimizing the functional \(f_{h}(f,g)\). Given that our objective is defined with a quadratic cost, doing so can be understood as a regression method for estimating the conditional expectation that underlies the Bellman update--viz. \(_{h}^{}\,g(s,a)=[y_{h,i}(g)(s_{h,\,i},\,a_{h,\,i})=(s,a)\,]\). Here \(_{h}^{2}(f)=_{h}\|\|_{2}^{2}\) for \(f=(),\,\) is a regularizer, with \(_{h} 0\) being the regularization weight. Given this set-up, we can generate a \(Q\)-function estimate \(}=(_{1},,_{H})\) by first initializing \(_{H}=r_{H}\), and then recursively computing \(_{h}=*{arg\,min}_{f}_{h} f,\,_{h+1}\), for \(h=H-1,H-2,,2,1\).

#### 3.1.2 Fast rates for FQI-based estimates

In the analysis here, we assume that the dataset consists of i.i.d. tuples (but this can be relaxed as needed). We now state a corollary of Theorem 1, applicable to value function estimates based on FQI with ridge regression.

**Corollary 1** (Fast rates for ridge-based FQI).: _For FQI based on ridge regression, with a sufficiently large sample size \(n\) and with suitable choices of the regularization parameters \(\{_{h}\}_{h=1}^{H-1}\), the bound (5) from Theorem 1 holds with_

\[_{h}=c\,\,(dH/)} \]

_with probability at least \(1-\)._

We omit the proof of Corollary 1, as it follows from standard ridge regression analysis.

Fast rates and comparisons to past work:So as to be able to compare with results from past work, let us consider some consequences of the bound (10) under the following assumptions: (i) \(_{h,h^{}}(^{})=(1)\); (ii) \(_{h,h^{}}(^{})=()\); (iii) \(\|_{h}^{}\|_{h} H-h+1\). Then it can be shown that the bound from Corollary 1 takes the form

\[J(^{})-J(})\;\;c\;d^{3/2}\,H^{3}\,n^{-1}\, (dH/),\] (11a) and is valid for a sample size \[n cd^{2}H^{3}\]. Alternatively stated, Corollary 1 guarantees that for FQI using ridge regression with \[d\] -dimensional function approximation, the number of samples \[n()\] required to obtain \[\] -optimal policy is at most \[n_{}() d^{}H^{3}/+d^{2}H^{3},\] (11b) where we use \[\] to denote a scaling that ignores constants and logarithmic factors.

Let us compare this guarantee to related work by Zanette et al. , who analyzed the use of pessimistic actor-critic methods for linear function classes. When translated into the notation of our paper, their analysis established5 a sample complexity of the order \(n_{}() d^{2}H^{3}/^{2}\). Consequently, we see that once the target error \(\) is relatively small--\((0,1)\)--then stable MDPs can exhibit a much smaller \((1/)\) sample complexity.

It should be noted that past work (e.g., ) has established \((1/^{2})\)-lower bounds on the sample complexity of estimating \(\) policies in the off-line setting. However, these lower bounds _do not_ contradict our fast rate guarantee (11b), because the "hard instances" used in these lower bound proofs violate the stability condition (**Sth**(\(\))). In particular, even infinitesimally small perturbations in policy lead to occupation measures that are significantly different.

When is pessimism necessary?An interesting aspect of the guarantee from Corollary 1 is that it provides guarantees for off-policy RL (and with fast rates) using a method that does _not_ incorporate any form of pessimism. This is a sharp contrast with many other methods for off-policy RL, such as pessimistic forms of \(Q\)-learning and actor-critic methods (e.g., ).

To be clear, as noted following the bound (11a), the guarantee from Corollary 1 requires the sample size to be lower bounded as \(n cd^{2}H^{3}\). In contrast, pessimistic schemes only require a sample size sufficiently large to ensure validity of the Bellman residual upper bounds that underlie Corollary 1--meaning that \(n d\) up to logarithmic factors. Thus, the pessimism principle can be useful for problems with smaller sample sizes.

### Consequences for on-line RL

In this section, we explore some consequences of Theorem 1 for on-line reinforcement learning. We begin by describing a two-stage procedure6 that allows us to convert the risk bounds for FQI from off-line RL into regret in on-line RL:

**Phase 1**: (_Exploration_) In the initial \(T_{0}\)f episodes, the focus is purely on exploration, resulting in an estimate of \(Q\)-function denoted as \(}^{(T_{0})}\).
**Phase 2**: (_Fine-tuning_) For \(k=0,1,,K-1\) with \(K\!:=\!_{2}(T/T_{0})\), repeat:

* In the \(t\)-th episode, for each \(t=T_{0}\:2^{k}+1,,T_{0}\:2^{k+1}\), execute the greedy policy induced by function \(}^{(T_{0}\:2^{k})}\).
* Update the \(Q\)-function estimate \(}^{(T_{0}\:2^{k+1})}\) using FQI based on observations collected from episodes \(T_{0}\:2^{k}+1,T_{0}\:2^{k}+2,,T_{0}\:2^{k+1}\).

We assume the burn-in time \(T_{0}\) is large enough so as to ensure the pilot \(Q\)-function estimate \(}^{(T_{0})}\) obtained in Phase 1 falls within a certain "absorbing" region \(()\) around \(^{*}\). Under these conditions, we have the following bound on the regret.

**Corollary 2**.: _For FQI based on ridge regression with rewards in \(\), with a sufficiently large burn-in time \(T_{0}\) and with suitable choices of the regularization parameters \(\{_{h}\}_{h=1}^{H-1}\), the two-phase scheme achieves regret bounded as_

\[(T)\;\;c\{T_{0} H\;+\;d\;H^{4}\; T\; \;(dHK/)\}\]

_with probability at least \(1-\)._

See Appendix C.1 for the proof.

Sharper bound on regret:The leading term (as \(T\) grows) in the regret bound grows as \( T\), which is much smaller than the typical \(\)-rate found in past work [18; 20]. The \(\) rate has been shown to be unimprovable in general, but the worst-case instances[18; 20] that lead to \(\)-regret violate the stability conditions used in our analysis.

When is optimism needed?The use of optimism--by adding bonuses to the current value function estimates so as to encourage exploration--underlies many schemes in on-line RL. An interesting take-away from Corollary 2 is that under the stability conditions highlighted by our theory, it is possible to achieve excellent regret bounds without the use of optimism. In our two-phase scheme, the only exploration occurs in Phase 1. All other data is simply collected using the greedy policy induced by the current \(Q\)-function estimate. A well-designed exploration scheme--one that might incorporate the optimism principle--is necessary only during the burn-in Phase 1.

## 4 Discussion

This paper introduces a novel approach for the analysis of value-based RL methods for continuous state-action spaces. Our analysis highlights two key stability properties of MDPs under which much sharper bounds on value sub-optimality can be guaranteed. Our analysis offers fresh perspectives on the commonly used pessimism and optimism principles, in off-line and on-line settings respectively.

Our study leaves open various questions for future work. First, our main result (Theorem 1) has consequences for linear quadratic control, to be described in an upcoming paper . It provides insight into the role of covariate shift in linear quadratic control, as well as efficient exploration in the on-line setting. Second, our current statistical analysis focused on i.i.d. data with linear function approximation. It is interesting to consider the extensions to dependent data and non-parametric function approximation (e.g. kernels, boosting, and neural networks). Third, while this paper has provided upper bounds, it remains to address the complementary question of lower bounds for policy optimization over the classes of stable MDPs isolated here. Last, to better align our framework with real-world scenarios, we intend to go beyond the idealized completeness condition used in this paper, and treat the role of model mis-specification.