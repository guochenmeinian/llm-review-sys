# Confidence Regulation Neurons in Language Models

Alessandro Stolfo

ETH Zurich

Equal contribution. Correspondence to stolfoa@ethz.ch, bpwu1@sheffield.ac.uk.

Ben Wu

University of Sheffield

&Wes Gurnee

MIT

Yonatan Belinkov

Technion

&Xingyi Song

University of Sheffield

&Mrinmaya Sachan

ETH Zurich

Neel Nanda

###### Abstract

Despite their widespread use, the mechanisms by which large language models (LLMs) represent and regulate uncertainty in next-token predictions remain largely unexplored. This study investigates two critical components believed to influence this uncertainty: the recently discovered entropy neurons and a new set of components that we term token frequency neurons. Entropy neurons are characterized by an unusually high weight norm and influence the final layer normalization (LayerNorm) scale to effectively scale down the logits. Our work shows that entropy neurons operate by writing onto an _unembedding null space_, allowing them to impact the residual stream norm with minimal direct effect on the logits themselves. We observe the presence of entropy neurons across a range of models, up to 7 billion parameters. On the other hand, token frequency neurons, which we discover and describe here for the first time, boost or suppress each token's logit proportionally to its log frequency, thereby shifting the output distribution towards or away from the unigram distribution. Finally, we present a detailed case study where entropy neurons actively manage confidence in the setting of induction, i.e. detecting and continuing repeated subsequences.

## 1 Introduction

As large language models (LLMs) increasingly permeate high-stakes applications, the lack of transparency in their decision-making processes poses significant vulnerabilities and risks . Understanding the basis of these models' decisions, especially how they regulate confidence in their predictions, is crucial not only for advancing model development but also for ensuring their safe deployment . LLMs have been empirically shown to be fairly well calibrated: on question-answering tasks, token-level probabilities of a model prediction generally match the probability of the model being correct . This raises the question of whether LLMs possess mechanisms for general-purpose calibration to mitigate the risks associated with overconfident predictions.

Significant research has been conducted on estimating model uncertainty in neural networks  and specifically in LLMs . While many studies focus on quantifying and calibrating model confidence [49, 40, 47, 45, 65, _inter alia_], there is little research into the internal mechanisms LLMs might use to calibrate their predictions.

In this work, we explore two types of components in Transformer-based language models that we believe serve a calibration function: the recently identified entropy neurons and a new class of components that we term _token frequency neurons_. Entropy neurons have been brought to light by recent work  and are characterized by their high weight norm and their low composition with the unembedding matrix despite being in the final layer. The low composition with the unembedding matrix would suggest that they play a minor role in the next-token prediction.

However, their high weight norm, despite LLMs being trained with weight decay , indicates that these neurons must be important for performance. This leads us to infer that these neurons may play a calibration role, as hypothesized by Gurnee et al. . We show that entropy neurons work by writing to an effective null space of the unembedding matrix and leverage the layer normalization  that is applied to the residual stream. This way, these components effectively modulate the entropy of the model's output distribution in a manner that only minimally affects the actual model prediction (Figure 1). We observe the presence of entropy neurons in GPT-2 , Pythia , Phi-2 , Gemma 2B , and LLaMA2 7B , demonstrating for the first time their role across different model families and scales.2

Motivated by the importance of the token frequency distribution (i.e., the unigram distribution) in language modeling [11; 51], we identify a novel class of neurons that boost or suppress each token's logit proportionally to its frequency. We provide evidence showing that these neurons, which we term token frequency neurons, modulate the distance between the model's output distribution and the unigram distribution, which the model defaults to in settings of high uncertainty.

As a case study, we analyze the activation of these neurons in scenarios involving induction (i.e., the repetition of subsequences in the input; 57). Our results show that entropy neurons increase the output distribution's entropy, thus decreasing the model's confidence in its predictions for repeated sequences. This represents a hedging mechanism, aimed at mitigating loss spikes on confidently wrong predictions.

Our contributions are as follows: (1) We show that models have dedicated circuitry to calibrate their confidence, with dedicated neurons in the final layer: the previously discovered entropy neurons and novel token frequency neurons. (2) We explore the mechanism by which each neuron family affects confidence (SS3 and SS4). Notably, we find that the model learns a low-rank effective null space for the unembedding that entropy neurons write to, and the final LayerNorm scale is used to calibrate the output logits. This striking phenomenon would have been missed by the assumptions made in standard methods such as direct logit attribution [16; 24; 46]. (3) We study how these neurons are used in practice to regulate confidence (SS5). As expected, neurons that lower confidence worsen performance when models are correct and improve it when models are incorrect. We close with a mechanistic case study of how induction heads  use entropy neurons to control confidence when detecting and continuing repeated text (SS6).3

## 2 Background

In this section, we provide an overview of the Transformer architecture , focusing on the components relevant to our analysis. Given a vocabulary \(\), we denote an autoregressive language model as \(:\), where \(\) is the space of probability distributions over \(\). \(\) takes as input a token sequence \(x=[x_{1},...,x_{m}]^{m}\), and outputs a probability distribution \(P_{}:\) to predict the next token in the sequence. In this work, we focus on decoder-only Transformer-based models, which represent the backbone of the most capable current AI systems [58; 26]. The Transformer architecture consists of two core components: the multi-head self-attention and a multi-layer perceptron (MLP). These two components read information in from and write out to the residual stream (i.e., the per-token hidden state consisting of the sum of all previous component outputs) .

Figure 1: **Entropy and Prediction.** We mean ablate final-layer neurons across 4000 tokens and measure the variation in the entropy of the model’s output \(P_{}\) against average change of model’s prediction (\(_{x}P_{}(x)\)). We identify a set of neurons whose effect depends on LayerNorm (red points; metric described in §3.2), and which affect the model’s confidence (quantified as entropy of \(P_{}\)) with minimal impact on the prediction.

Mlp.Central to this study is the structure of the MLP layers of the transformer.4 Given a normalized residual stream hidden state \(^{d_{}}\), the output of an MLP layer is

\[()=_{}(_{}+_{})+_{}\;, \]

where \(_{}^{T},_{}^{d_{ } d_{}}\) are learned weight matrices, \(_{}\) and \(_{}\) are learned biases. The function \(\) denotes an element-wise nonlinear activation function, typically a GeLU . In this paper, the term _neuron_ refers to an entry in the MLP hidden state. We denote specific neurons in the model using the format <layer>.<index>. We use \(\) to refer to the _activation values_ of the neurons (i.e., the output of the activation function \(\)). Furthermore, we use \(_{}^{(i)}^{d_{}}\) to indicate the output weights of neuron \(i\) (i.e., the \(i\)-th column of \(_{}\)).

LayerNorm.Layer normalization (LayerNorm) is a commonly employed technique to improve the stability of the training process in deep neural networks . Given a hidden state \(^{d_{}}\), LayerNorm applies the transformation

\[()=-m()}{( )+}}+. \]

In words, LayerNorm centers a hidden state by subtracting its mean \(m()=}}_{j}_{j}\), re-scales it by its \(\)-adjusted standard deviation (\(()+}\), where \(\)), and applies an element-wise affine transformation determined by the learned parameters \(,^{d_{}}\).

Unembedding.After the final transformer block, the final state of the residual stream is passed through a final LayerNorm and then projected onto the vocabulary space through an unembedding matrix \(_{}^{|| d_{}}\). The logits obtained by mapping the residual stream final state onto the vocabulary space are then passed through a softmax function to convert them into a probability distribution \(P_{}\). Finally, the loss is computed using a function \(:_{+}\) (typically cross entropy for autoregressive language models). Following previous work , we employ standard weight pre-processing techniques . In particular, we "fold" LayerNorm trainable parameters into the \(_{}\) and \(_{}\) matrices. We report the details about weight pre-processing in Appendix C.

## 3 Entropy neurons

Prior research examining individual neurons in GPT-2 language models  identified a category of neurons notable for their high norm and limited interaction with the unembedding matrix. Katz and Belinkov  speculate that these neurons may act as regularizers, contributing a constant bias to the residual stream. Gurnee et al.  independently rediscovered these neurons, termed _entropy neurons_, and suggest that they play a role in regulating the entropy of the model's output distribution. They show that interventions increasing the activation value of these neurons affect the entropy of the output distribution to a larger extent compared to other neurons. Despite these insights, the natural activation effects of these neurons on standard language inputs have not been examined, and _how_ they carry out this entropy-modulating function remains unclear. In this section, we explore the mechanism of action of these neurons within the model.

### Identifying entropy neurons

Our initial step is to identify which neurons are entropy neurons. We do this by searching for neurons with a high weight norm and a minimal impact on the logits. To detect minimal effect on the logits, we follow the heuristic used by Gurnee et al. , analyzing the variance in the effect of neurons on the logits. In particular, we project the last-layer neuron weights in GPT-2 Small onto the vocabulary space. This projection, sometimes referred to as logit attribution, represents an approximation of the neuron's effect on the final prediction logits [55; 24; 15]. Then, we compute the variance of the normalized projection. That is, given a neuron with output weights \(_{}\), we compute

\[(_{})=(_{}_{}}{\|_{}\|_{}\|_{}\|}), \]where \(\|\|_{ dim=1}\) indicates column-wise norm.

This measure allows us to quantify how specific is the direct effect of a neuron on the output logits, with the underlying intuition that a diffused contribution (i.e., close to a constant added to all logits before applying the softmax) results in a small effect on the output probabilities. In the last layer of GPT-2 Small, we identify a subset of neurons with particularly low variance but substantial norm (Figure 2a). Since the optimization of the model's weights preserves these high-norm low-composition neurons (that are expensive in terms of weight decay penalty ), it is likely that these neurons are important and exert their effect through a different mechanism than directly modifying the logits.

### Mechanism of action

Given the low composition with the unembedding matrix, we hypothesize that most of the effect of entropy neurons on the model's output is mediated by the re-scaling of the residual stream performed by the final LayerNorm. We test this hypothesis using a causal mediation analysis formulation [60; 64], illustrated in Figure 2b. We distinguish the _total_ effect that a neuron has on the model's output (represented by the two causal paths stemming from the neuron node in Figure 2b) from its _direct_ effect (the green arrow in Figure 2b), which is not mediated by the change in the LayerNorm scale. Our hypothesis posits that the difference between the former and the latter is significantly larger for entropy neurons than for normal neurons.

We carry out an ablation experiment where we intervene on the activation value of a specific neuron by fixing it to its mean value across a reference distribution while constraining the LayerNorm scaling coefficient to remain constant. More formally, consider a neuron with index \(i\{1,2,,d_{ mlp}\}\) and denote by \(_{i}\) its activation value. Given an input \(x\), denote by \(\) the last hidden state in the model (i.e., the output of the last transformer block), and denote by \(^{-i}\) the last hidden state in the model obtained after mean-ablating neuron \(i\):

\[^{-i}=+(}_{i}-_{i})_{ out}^{(i)}, \]

where \(}_{i}\) is the mean activation value computed over a subset of \(\).

Figure 2: **Identifying and Analyzing Entropy Neurons.** (a) Neurons in GPT-2 Small displayed by their weight norm and variance in logit attribution. Entropy neurons (red) have high norm and low logit variance. (b) Causal graph showing the total effect and direct effect (bypassing LayerNorm) of a neuron on the model’s output. (c) Comparison of total and direct effects on model loss for entropy neurons and randomly selected neurons. (d) Singular values and cosine similarity between neuron output weights and singular vectors of \(_{ U}\). (e) Entropy neurons (red) show significant LayerNorm-mediated effects and high projection onto the null space (\(\)). (f) Relationship between \(\) and the LayerNorm-mediated effect in LLaMA2 7B. \(\) is computed with \(k=40 0.01*d_{ model}\). Color represents absolute change in entropy upon ablation (\( H\)).

We quantify the total effect of neuron \(i\) upon mean ablation by measuring the absolute variation in the model's loss \(\),5 specifically:

\[(i)=_{x}_{ }(),x-_{ }(^{-i}),x, \]

where the expectation is taken over a uniformly sampled subset of a corpus \(\). Similarly, we quantify the direct effect of neuron \(i\) by preventing the LayerNorm denominator from varying:

\[_{}(i)=_{x} _{}(),x- _{}^{-i}-m(^{-i})}{ ()+}}+ ,x, \]

For each neuron, we measure the total and direct effects when its activation value is set to the mean across a dataset of 25600 tokens from the C4 Corpus  (additional experimental details are provided in Appendix D). We compare these metrics for six selected entropy neurons against those from 100 randomly selected neurons. The results (Figure 2c) show that in the randomly selected neurons there is no significant difference between their total and direct effects. On the other hand, the difference between these two quantities is substantial for entropy neurons, and in some cases the direct effect represents only a small fraction of the total effect.

These findings represent an interesting and novel example of how language models can use LayerNorm to indirectly manipulate the logit values. Such a mechanism could easily be overlooked by conventional analyses, which typically focus on direct logit attribution and fail to account for the normalization effects imposed by LayerNorm [16; 24; 46].

### The unembedding has an effective null space

The unembedding is a linear map to a significantly higher-dimensional space (e.g., from 768 to 50,257 dimensions in GPT-2). Therefore, it is surprising that the output of a high-norm neuron would have little effect on the logits. We hypothesize that there is a subspace of the residual stream with an unusually low impact on the output that entropy neurons write onto. To investigate this, we compute the singular value decomposition (SVD) of the unembedding matrix \(_{}=^{T}\).

Analyzing the singular values in \(\) (thick blue line in Figure 2d), we observe that the bottom values are extremely small. In particular, we notice a sharp drop to values close to 0 around index 755. This observation indicates a remarkable phenomenon within the model: the training procedure optimizes the weights to preserve what effectively functions as a null space within \(_{}\). This finding is striking, as it suggests that the model deliberately limits its representational power by projecting a set of residual stream directions onto a small neighborhood of the 0-vector.

Next, we study the directions within the residual stream that entropy neurons write to. We do this by computing the cosine similarity between the output weights (\(_{}\)) for each neuron and singular vectors \(^{d_{} d_{}}\). The results, illustrated by the colored lines in Figure 2d, show that all the entropy neurons write almost exclusively to directions within the effective null space. These findings illustrate how entropy neurons leverage this null space to add norm to the residual stream without directly affecting the logit values.

### Universality across model families

The relationship observed between entropy neurons and the unembedding null space suggests that entropy neurons may be identified by analyzing the proportion of their norm that is projected onto such a null space. To test this, given a neuron \(i\), we compute the fraction \(_{i}\) of the neuron's norm projected onto the effective null space \(_{}^{T}\). In particular, we define the effective null space of \(_{}\) as the space spanned by its bottom \(k\) singular vectors, i.e., \(_{}^{T}:=_{:,d_{}-k}^{T}, ,_{:,d_{}}^{T}\), and we compute \(_{i}=\|_{}^{T}_{}^{(i)}\|/\| _{}^{(i)}\|\). Then, we measure the quantity \(1-(i)/(i)\), which represents the fraction of the neuron's effect that is mediated by the final LayerNorm.6 Wecompare these two quantities for each neuron at the last layer in GPT-2 Small (Figure 2e), with \(k=12\). The results show that neurons with the largest projections onto the null space have the most significant LayerNorm-mediated effects on the model's output. These findings connect with recent work highlighting the importance of the bottom \(_{}\) singular vectors  and represent further evidence supporting our hypothesis about the entropy neurons' mechanism of action.

We extend our investigation of entropy neurons to models beyond the GPT-2 family. In Figure 2f, we analyze the proportion of norm projected onto the bottom singular vectors of \(_{}\) (i.e., \(\)) and the influence of LayerNorm on the output for neurons in the last layer of LLaMA2 7B .7 Similar to our observations with GPT-2 Small, the results indicate the existence of a distinct group of neurons that predominantly write onto the effective null space of \(_{}\). These neurons significantly influence the model's output via LayerNorm, confirming the presence of entropy neurons across different model families. As entropy neurons act by re-scaling the residual stream and have no effect that is specific to a subset of the tokens, their activation has a large effect on the entropy of the output distribution but little impact on the relative ranking of the tokens. That is, they affect the model's confidence in the prediction without affecting the actual token being predicted. This is illustrated in Figure 1. We repeat these analyses on GPT-2 Medium, Pythia 410M and 1B, Phi 2 , and Gemma 2B , obtaining overall consistent results (Appendix F).

## 4 Token frequency neurons

The token frequency distribution (i.e., the distribution of unigrams over a corpus) can serve as a reliable baseline for next-token prediction, especially in scenarios of high uncertainty . Thus, we hypothesize that a useful general-purpose mechanism for regulating confidence in language models could involve modulating the distance between the output distribution and the token frequency distribution. We explore this hypothesis using the 410M-parameter Pythia model, for which the training corpus has been publicly released (The Pile; 20). For this corpus, we compute the empirical token frequency distribution \(P_{}\). We observe that the entropy of the model's output distribution is in fact negatively correlated with its Kullback-Leibler (KL) divergence from the empirical token frequency distribution: as the confidence of the model decreases (higher entropy), its output distribution tends to be closer to the empirical token frequency distribution (Figure 3a).

As for entropy neurons, we aim to identify neurons whose operational mechanism relies on a particular subspace--in this case, the direction corresponding to the token frequency distribution. We obtain such a direction by computing the logit vector \(_{}^{||}\) by centering the log-probability values for each token in \(\). That is, \(_{,i}=(_{,i})-m(( _{}))\), where \(_{}^{||}\) the vector of the token frequency probabilities values, and \(m:^{||}\) indicates the mean function.

To identify the neurons that rely on this direction to modulate the model's output, we conduct an ablation experiment similar to the one described in SS3.2. In this experiment, we mean-ablate neurons and assess the absolute change in model loss while preserving the value of the logits along the

Figure 3: **Token Frequency Neurons in Pythia 410M.** (a) \(_{}(P_{}||P_{})\) and Entropy are correlated negatively. (b) Scatter plot of neurons highlighting token frequency neurons (in green), with high effect on \(_{}(P_{}||P_{})\), significantly mediated by the token frequency direction. (c) Box plots showing substantial difference in total vs. direct effect in token frequency neurons.

direction of \(_{}\) as constant. Denote by \(=_{}()\) the logits produced by the model on input \(x\), and denote by \(^{-i}=_{}(^{-i})\) the value of the logits obtained on the same input after the ablation of a neuron \(i\) (where \(^{-i}\) is defined in Eq. (4)). To measure the effect of neuron \(i\) that is not mediated by the token frequency direction (i.e., its _direct_ effect), we obtain the adjusted value of the post-ablation logits \(^{-i}\) as

\[}^{-i}=^{-i}+^{T}_{ })-(^{-iT}_{})}{\|_{ }\|^{2}}_{}. \]

Then we compute the total and direct effects of neuron \(i\) as

\[(i)=_{x} ,x-^{-i},x,\;\;\;_{}(i)=_{x},x- }^{-i},x. \]

In Figure 3b, we report the results comparing the token frequency-mediated effect against the average absolute change in the KL divergence between \(P_{}\) and \(P_{}\) for neurons at the final layer of Pythia 410M. We also highlight the entropy neurons with the highest LayerNorm-mediated effect (see Appendix F for comprehensive results on entropy neurons in Pythia 410M). We observe that there are neurons whose effect on \(P_{}\) is substantially mediated by the token frequency direction (i.e., positive value along the \(x\)-axis in Figure 3b). These neurons, upon ablation, lead to significant variation in \(_{}(P_{}\|P_{})\) (large positive value along the \(y\)-axis in Figure 3b), comparable to the variations caused by some entropy neurons.

These results validate the presence of components (which we term _token frequency neurons_) that affect the model's output distribution by bringing it closer or away from the token frequency distribution by _directly_ modulating it. Figure 3c focuses on 5 selected token frequency neurons and shows their impact on the loss and its decrease upon the inhibition of the token frequency direction (Eq. (7)), which parallels the LayerNorm mediation for entropy neurons discussed in SS3. We additionally show the presence of token frequency neurons in GPT-2 Small and Pythia 1B in Appendix G.

## 5 Examples of neuron activity

We have shown that confidence-regulation neurons improve model performance by calibrating its output across the entire distribution. However, it remains unclear what this looks like in practice. To better illustrate the role that these neurons play in language models, we provide additional analyses about the changes in the model output induced by particular entropy and token frequency neurons.

Entropy neurons.In GPT-2 Small, we examine the change in entropy induced by one of the strongest entropy neurons identified in SS3 (11.2378). To study this, we conduct an experiment in which we clip the activation value of the neuron to its mean and measure the resulting change in the model output.8 We analyze the difference in loss caused by the ablation of the neuron on

Figure 4: **Examples of Neuron Activity in Language Models.** (a) Change in loss after ablation of entropy neuron 11.2378 in GPT-2 Small. Color indicates reciprocal rank (\(\)) of the correct token prediction. (b) Activation of neuron 11.2378 on an example from the C4 Corpus. The neuron mitigates a loss spike at the token “Mes,” after which the model predicts “otherapy.” (c) Change in entropy and KL divergence on correct tokens (\(=1\)) post ablation of neuron 23.417 in Pythia 410M. The neuron increases entropy and aligns the model’s output with the token frequency distribution.

individual tokens and compare it to the initial loss value. This analysis is depicted in Figure 4a, which also illustrates the reciprocal rank of the correct token prediction in the model's output distribution (\(\)). These observations suggest that the entropy-increasing function of neuron 11.2378 acts as a hedging mechanism: it marginally increases the loss on correct model predictions (low initial loss) but prevents the loss from spiking when the model confidently predicts a wrong token (high initial loss). In Figure 4b, we provide an example of input, taken from the C4 corpus , on which the model confidently predicts a wrong token (_"otherapy"_), and neuron 11.2378 activates to mitigate the loss spike. We perform the same analysis for the strongest entropy neuron in LLaMA2 7B in Appendix F.1.

Token frequency neurons.We next focus on neuron 23.417 in Pythia 410M (identified in SS4). This is a token frequency neuron: when its activation is increased, it pushes the model's output \(P_{}\) towards the token frequency distribution \(P_{}\). This is illustrated by Figure 4c, showing the KL divergence between the two distributions pre- and post-ablation. As with increased entropy, bringing \(P_{}\) closer to \(P_{}\) decreases the model's confidence (positive values along the \(x\)-axis correlate with negative \(y\) values in Figure 4c). This leads to an increase in loss on correct predictions, reflected in the negative change in loss upon ablation (darker points in Figure 4c). Interestingly, we find that the projection of this neurons' \(_{}\) onto the frequency direction \(_{}\) is negative, suggesting that this neuron is suppressing common tokens and promoting rare ones. This indicates that the model is on average biased to predict common tokens even more frequently than their frequency would dictate. This way, lowering confidence by pushing \(P_{}\) closer to \(P_{}\) requires suppressing the token frequency direction, which is the opposite of what one would have naively expected.

## 6 Case study: Induction

As a more detailed case study to examine the active role and confidence-regulating function of entropy neurons, we focus on the setting of induction. Induction occurs when there is a repeated subsequence of tokens in the model's input, and the model must detect the earlier repeat and continue it. Mechanistically, this is implemented by specialized attention heads, called induction heads [16; 57], which attend from some token A to the token B that immediately follows A in the early occurrence and predict B (AB...A \(\) B). Repeated text frequently occurs in natural language (e.g., someone's name) and, during a repeated subsequence, the next token can often be predicted with very high confidence.

### Entropy neurons

To analyze this phenomenon, we create input sequences by selecting 100 tokens from C4  and duplicating them to form a 200-token input sequence. Across 100 such sequences, we measure GPT-2 Small's performance and observe a significant decrease in both average loss and entropy during the second occurrence of the sequence (solid lines in Figure 5a). Additionally, we track the activation values of the six entropy neurons in GPT-2 Small analyzed in SS3 (dotted lines in Figure 5a). For four of these neurons (11.584, 11.2378, 11.2123, and 11.2910), we note substantial change in average activation values, suggesting that they may play an important role. Furthermore, among final-layer neurons, entropy neurons exhibit the largest change in activation during induction (Appendix H.3, Figure 11).

Figure 5: **Entropy Neurons on Induction.** (a) Activations, entropy, and loss across duplicated 200-token input sequences. (b) The effect of clip mean-ablation of specific entropy neurons. Neuron 11.2378 shows the most significant impact, with up to a 70% reduction in entropy. (c) BOS ablation of induction heads: Upon the ablation of three induction heads in GPT-2 Small, the activation of entropy neuron 11.2378 decreases substantially.

To further explore the specific effects of entropy neurons on the model's output entropy and loss, we conduct an experiment where, during the second occurrence of the sequence, we clip the activation values of the neurons to their mean values from the first occurrence. The results (Figure 4(b)) reveal that while the ablation of some neurons such as 11.584 and 11.2123 leads to a slight increase in output entropy, the most significant impact was observed with neuron 11.2378. This results in up to a 70% reduction in entropy, suggesting its role in boosting entropy to counterbalance the model's high confidence during induction. We additionally study the effect of token frequency neurons during induction and we report the results in Appendix H.2, revealing that these neurons also lead to a significant change in entropy compared to randomly selected neurons. We repeat these analyses in naturally occurring induction cases identified in the C4 corpus, obtaining consistent results (Appendix H.1).

### Induction head-entropy neuron interaction

In order to verify that induction heads have a causal effect on the activation of our entropy neurons, we perform _BOS ablations_. That is, motivated by the observation that attention heads frequently attend to BOS when inactive (e.g., when induction is not occurring) , we set the attention pattern of an attention head to always attend to the first token of the sequence (i.e., BOS) and record the neuron's activation. We perform this procedure on the top 3 induction heads, selected according to the prefix matching score introduced by Olsson et al. . In GPT-2 Small, these heads are L5H1, L5H5, and L6H9, respectively. As baselines, we randomly select 6 heads in layers 5 and 6 that, like our induction heads, have mean attention to BOS > 0.6 but have prefix matching score < 0.1. For both entropy neurons and baselines, we ablate heads individually and in combinations of up to 3 heads.

Figure 4(c) shows that ablating L5H1 (ind. head 1) leads to a significant decrease in 11.2378's mean activation. Ablating L5H5 (ind. head 2) or L6H9 (ind. head 3) individually does not make a substantial difference, but ablating them alongside L5H1 further decreases activation, bringing 11.2378 close to its activation on the first 100 tokens. We perform additional BOS ablations for other entropy neurons in Appendix H and further study the induction head-entropy neuron interaction in Appendix I, providing preliminary evidence that entropy neurons respond to an internal 'induction-has-occurred' signal that is produced by induction heads.

## 7 Related work

Uncertainty in language models.Uncertainty quantification for machine learning models has been extensively studied to assess the reliability of model predictions [19; 21]. With the increasing adoption of LLMs, accurately determining their confidence has drawn significant attention . Existing research has explored various strategies to calibrate model output, including ensembling model predictions [76; 34], fine-tuning [39; 40], and prompting models to explicitly express their confidence [47; 40; 65; 69]. Additionally, efforts have been made to evaluate the truthfulness [9; 3; 50] and uncertainty  of model outputs by inspecting internal representations. However, the investigation of internal, general-purpose mechanisms determining model confidence remains underexplored.

Interpretability of LLMs' components.The analysis of language models' internal components and mechanisms is an increasingly popular area of research (we refer to Ferrando et al.  for an overview). Prior work has explored how to attribute and localize model behavior [73; 23; 52], and to identify specific algorithms that models implement to perform tasks [54; 75; 29; 67; 12]. Additionally, many studies have focused on investigating individual neurons [41; 61; 14; 2; 13; 74; 27; 7]. However, neurons are not always the correct unit of analysis, as they can be polysemantic [56; 17]. Recent work has employed sparse autoencoders (SAEs) to find more interpretable linear combinations of neurons [79; 8; 36]. Despite this, in the context of entropy modulation, prior findings of relevant neurons motivated us to focus on neurons.

Closest related work.An anomalous class of neurons characterized by high norm and low composition with the unembedding matrix was observed by Katz and Belinkov , though this was not linked to the regulation of output distribution entropy. This class of neurons was independently rediscovered by Gurnee et al.  while investigating the universality of neurons in GPT-2 models. Our work connects the concept of entropy neurons to the presence of an effective null space represented by the bottom singular vectors of \(_{}\). Our findings highlight the significance of this seemingly unimportant subspace within the residual stream and align with the observations of Cancedda , who associated the bottom \(_{}\) singular vectors with the phenomenon of the attention sink .

Conclusion

This paper presents an analysis of the mechanisms by which LLMs manage and express uncertainty, focusing on two specific components: entropy neurons and token frequency neurons. We show that entropy neurons, with their high weight norm and low direct interaction with the unembedding matrix, affect the model's output via the final LayerNorm. We also introduce and investigate token frequency neurons, which adjust the model's output by aligning it closer to or further from the token frequency distribution. In a case study on induction, we demonstrate the practical implications of these neurons. We show that one example role of entropy neurons is acting as a hedging mechanism to manage confidence in repetitive sequence scenarios. Some limitations of our work include focusing only on two types of components in the neuron basis, relying on proxies for confidence, and observing varying effects across models. We thoroughly discuss the limitations of our work in Appendix A. This study represents the first thorough investigation into the mechanisms that LLMs might use for confidence calibration, providing insights that can guide further research in this area.

## Author Contribution

Alessandro and Ben led the project, carried out the experiments, and wrote the paper. Alessandro proposed the causal formulation and conducted most of the experiments validating the entropy neurons' mechanism of action. He also performed experiments to identify entropy neurons and token frequency neurons across multiple models and wrote the majority of SSSS 1 to 5. Ben carried out numerous validation experiments to ensure the robustness of our findings and obtained preliminary results on the universality of entropy neurons. Ben also led the experiments for the induction case study and wrote most of SS6 and Apps. H and I. Wes gave frequent and detailed feedback on experiment design and paper write-up. Yonatan, Xingyi, and Mrinmaya provided valuable input on experiment design and presentation of the results. Neel supervised the project and provided guidance at all stages on experiment design and analysis, in addition to editing the paper.