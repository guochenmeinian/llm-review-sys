# On the Gini-impurity Preservation

For Privacy Random Forests

 Xin-Ran Xie, Man-Jie Yuan, Xue-Tong Bai, Wei Gao, Zhi-Hua Zhou

National Key Laboratory for Novel Software Technology, Nanjing University, China

School of Artificial Intelligence, Nanjing University, China

{xiexr,yuanmj,baixt,gaow,zhouzh}@lamda.nju.edu.cn

These authors contribute equally.

###### Abstract

Random forests have been one of the successful ensemble algorithms in machine learning. Various techniques have been utilized to preserve the privacy of random forests, such as anonymization, differential privacy, homomorphic encryption, etc. This work takes one step towards data encryption by incorporating some crucial ingredients of learning algorithm. Specifically, we develop a new encryption to preserve data's Gini impurity, which plays an important role during the construction of random forests. The basic idea is to modify the structure of binary search tree to store several examples in each node, and encrypt the data features by incorporating label and order information. Theoretically, our scheme is proven to preserve the minimum Gini impurity in ciphertexts without decrypting, and we also present the security guarantee for encryption. For random forests, we encrypt data features based on our Gini-impurity-preserving scheme, and take the homomorphic encryption scheme CKKS to encrypt data labels owing to their importance and privacy. We finally present extensive empirical studies to validate the effectiveness, efficiency and security of our proposed method.

## 1 Introduction

From the pioneer work , random forests have been one successful ensemble algorithm [2; 3; 4], with diverse applications such as ecology , computational biology , objection recognition , remote sensing , computer vision , etc. The basic idea is to construct a large number of random trees individually and make prediction based on an average of their predictions. Numerous variants of random forests have been developed to improve performance under different settings [10; 11; 12; 13; 14; 15; 16; 17; 18; 19; 20; 21; 22], as well as theoretical understandings on the success of random forests [23; 21; 24; 25; 26; 27]. The splitting criterion, such as Gini impurity and information gain, has been one of the most important ingredient during the construction of random forests [1; 28].

Various techniques have been adopted to preserve the privacy of random forests, especially for sensitive tasks such as medical diagnosis, financial predictions, and so on. For example, differential privacy  has been successfully applied to preserve the privacy of random forests [30; 31] and decision trees [32; 33; 34], by adding certain noise perturbations. Another relevant approach is the secure multi-party computation for random forests and decision tree [35; 36; 37; 38; 39], where the privacy is preserved by multi-party joint computation over respective data inputs without leakage.

Homomorphic encryption [40; 41; 42; 43] has been one of the most important cryptosystems in privacy-preserving computing [44; 45; 46; 47]. Based on such scheme, various algorithms have been developed to train privacy random forests and decision trees [48; 49; 50; 51; 52], while some other methods only considered inference without training due to computational costs [53; 54; 55; 56; 57; 58]. In addition, LeFevre et al.  tookthe anonymization  for random forests by grouping similar attributes so as to hardly identify specific individual information.

This work takes one step towards data encryption by incorporating some crucial ingredients of learning algorithm, and main contributions can be summarized as follows:

* We present a new encryption to preserve data's Gini impurity, and the basic idea is to modify the structure of binary search trees to maintain several samples on each node, and encrypt data's features by incorporating label and order information. Our scheme could change the data frequencies, which is also beneficial for data security.
* Theoretically, we prove the preservation of minimum Gini impurity in ciphertexts without decryption, which plays an important role on the construction of random forests. Our scheme also satisfies the security against Gini-impurity-preserving chosen plaintext attack.
* We focus on the privacy random forests in the popular client-server protocol, and take our Gini-impurity-preserving encryption for data features. We adopt homomorphic encryption CKKS to encrypt data labels. Our encrypted decision tree takes smaller communication and computational complexities, as shown in Table 1.
* Extensive experiments show that our encrypted random forests take significantly better performance than prior privacy random forests via encryption, anonymization and differential privacy, and are comparable to original (plaintexts) random forests without encryption. Our encrypted random forests make a good balance between computational cost and data security.

The rest of this work is constructed as follows: Section 2 introduces relevant work. Section 3 presents an encryption on data's Gini impurity. Section 4 proposes the encrypted random forests. Section 5 conducts extensive experiments. Section 6 concludes with future work.

## 2 Relevant Work

Homomorphic Encryption (HE) is a cryptosystem, which allows operations on encrypted data without access to a secret key . We can perform some mathematical operations such as addition and multiplication operations on encrypted data without revealing sensitive information. Given an encryption function \(E()\) and a decryption function \(D()\), the HE scheme provides two operators \(\) and \(\) such that, for every pair of plaintexts \(x_{1}\) and \(x_{2}\),

\[D(E(x_{1}) E(x_{2}))=x_{1}+x_{2} D (E(x_{1}) E(x_{2}))=x_{1} x_{2}\;,\]

where \(+\) and \(\) denote standard addition and multiplication operations, respectively.

Various HE schemes have been developed during the past years, e.g., ElGamal , Paillier , CKKS  encryption, etc. Relevant techniques have been successfully applied to machine learning tasks such as regression problem [69; 70], neural network [71; 72; 73; 74; 75], collaborative filtering , etc. Generally, HE schemes are accompanied with high computational costs, and one main challenge is to maintain a good trade-off among security, effectiveness and computational cost in real applications.

   & Training communication & Training comp. complexity & Predictive communication & Predictive comp. complexity & Privacy \\   & Rounds & Bandwidth & Client & Server & Rounds & Bandwidth & Client & Server & of model \\  SMCDT  & \(O()\) & \(O( r)\) & \(O( r)\) & \(O( r)\) & \(O(1)\) & \(O(1)\) & \(O(1)\) & \(O(h)\) & âœ— \\ PPD3  & \(O()\) & \(O( r^{2}jrn)\) & \(O(^{2}jrn)\) & \(O(^{2}jrn)\) & \(O(1)\)Secure Multi-Party Computation (SMC)  is another cryptographic technique to jointly compute a function from multiple private inputs with confidential, which has been used for machine learning to protect privacy data, such as neural network , \(k\)-means clustering , random forests and decision trees , etc. Differential privacy is introduced to preserve individual privacy by taking statistically inconsequential changes to data , and relevant techniques have been utilized in neural network , random forests  and decision trees .

We introduce some notations used in this work. Write \([]=\{1,2,,\}\) for integer \( 2\). Let \(^{d}\) and \(=[]\) denote the feature and label space, respectively. A training sample is given by \(S_{n}=\{(_{1},y_{1}),(_{2},y_{2}),...,(_{n},y_{n})\}\). Let \(|A|\) be the cardinality of set \(A\), and \([\![]\!]\) denotes the corresponding encrypted value. Let \((,^{2})\) be a normal distribution of mean \(\) and variance \(^{2}\).

## 3 An Encryption for Gini Impurity

This section presents the first encryption to preserve the minimum Gini impurity over encrypted data without decryption. For simplicity, we give the detailed encryption on one-dimensional feature by incorporating label information, and make similar considerations for other dimensions.

### Theoretical Analysis for Gini Impurity

Let \(A=\{(a_{1},y_{1}),,(a_{n},y_{n})\}\) be a dataset with labels \(y_{i}[]\), and define the Gini value as

\[(A)=1-_{y[]}p_{y}^{2}\;,\]

where \(p_{y}\) denotes the proportion of the label \(y\). Let \(A_{a}^{l}=\{(a_{i},y_{i}) a_{i} a,(a_{i},y_{i}) A\}\) and \(A_{a}^{r}=\{(a_{i},y_{i}) a_{i}>a,(a_{i},y_{i}) A\}\) be the left and right subsets of \(A\) w.r.t. a splitting point \(a\), respectively. We define the Gini impurity w.r.t. dataset \(A\) and splitting point \(a\) as

\[I_{G}(A,a)=w_{l}(A_{a}^{l})+w_{r}(A_{a}^{r})\;, \]

where \(w_{l}=|A_{a}^{l}|/n\) and \(w_{r}=|A_{a}^{r}|/n\). Let \(I_{G}^{*}(A)\) be the minimum Gini impurity of dataset \(A\), i.e.,

\[I_{G}^{*}(A)=_{a}\{I_{G}(A,a)\}\;. \]

The minimum Gini impurity plays a crucial role on nodes splitting during the construction of random forests. We re-sort dataset \(A\) with a non-decreasing order for \(a_{1},a_{2},,a_{n}\) as follows:

\[A=\{(a_{(1)},y_{(1)}),(a_{(2)},y_{(2)}),,(a_{(n)},y_{(n)})\}\;, \]

where \(a_{(1)} a_{(2)} a_{(n)}\), and \(y_{(1)},y_{(2)},,y_{(n)}\) denote their corresponding labels. By incorporating label information, we partition dataset \(A\) into several datasets \(_{1},_{2},,_{s}\) as follows:

\[_{1} = \{(a_{(1)},y_{(1)}),,(a_{ k_{1}},y_{(k_{ 1})})\}\;,\] \[_{2} = \{(a_{(k_{1}+1)},y_{(k_{1}+1)}),,,(a_{ k_{1}+k _{2}},y_{(k_{1}+k_{2})})\}\;,\] \[\] \[_{s} = \{(a_{(k_{1}+k_{2}++k_{s-1}+1)},y_{ k_{1}+k_{2 }++k_{s-1}+1}),,(a_{ n},y_{ n} )\}\;.\]

Here, any two adjacent datasets have different labels, and all samples have an identical label in one dataset \(_{j}\), i.e., \(y_{ i}=y_{ i^{}}\) for every \((a_{ i},y_{ i})_{j}\) and \((a_{ i^{}},y_{ i^{}})_{j}\).

Figure 1: A simple illustration for our encryption: each plaintext is encrypted into a ciphertext vector \((c_{i},e_{i,j})\). Here, random numbers \(c_{1}<c_{2}<<c_{s}\) are introduced to preserve the Gini impurity for random forests, and we take homomorphic encryption scheme for \(e_{i,j}=(k_{},j)\) in Eqn. (5), which is helpful for decryption.

We consider two important factors in encryption: i) preservation of the minimum Gini impurity \(I_{G}^{*}(A)\) over the encrypted data, and ii) a cryptosystem for encoding and decoding data. Based on such recognition, we introduce the following encryption, for every example \((a_{ i},y_{ i})_{j}\),

\[ a_{ i}= a_{  i}_{1}, a_{ i}_{2} =(c_{1},(k_{},i))&\;j=1 \;,\\ (c_{j},(k_{},i-k_{1}--k_{j-1}))&\;2 j s\;. \]

Here, \(c_{1},c_{2},,c_{s}\) are random numbers s.t. \(c_{1}<c_{2}<<c_{s}\), which aim to preserve the minimum Gini impurity. We take the homomorphic encryption scheme CKKS with a public key \(k_{}\) for \( a_{ i}_{2}=(k_{},i-k_ {1}--k_{j-1})\) in Eqn. (5), and it is useful for decryption. Figure 1 presents a simple illustration for our encryption, and the detailed decryption is given in Appendix A.

We now present our main theorem as follows:

**Theorem 1**.: _We have \(I_{G}^{*}(A)=I_{G}^{*}(A^{})\), for re-sort dataset \(A\) by Eqn. (3) and for the corresponding encrypted dataset \(A^{}=\{( a_{ 1}_{1},y_{ 1 }),,( a_{ n}_{1},y_{ n })\}\) from Eqns. (4)-(5)._

This theorem shows that our encryption could preserve the minimum Gini impurity over encrypted data. The detailed proof is presented in Appendix B, which involves the proof of piecewise monotonicity of \(I_{G}(A,a)\) w.r.t. splitting point \(a\), and then solves the minimum splitting point on plaintexts, as well as the corresponding point on encrypted data.

### Binary Search Tree for Encryption

We now present new binary search tree to encrypt \(a_{1},,a_{n}\) dynamically, especially for un-ordered dataset \(A=\{(a_{1},y_{1}),,(a_{n},y_{n})\}\), or when example \((a_{i},y_{i})\) arrives in a streaming data. We begin with an alternative structure for binary search tree to maintain several samples on a node from Eqns. (4)-(5), rather than previous only one sample [88; 89]. Our new structure is given by

\[\{;\;\;}}_{1},}_{2};\;\;}}\}\;.\]

The _samples_ stores one or multiple samples from \(A\), and _cipher\({}_{1}\)_ and _cipher\({}_{2}\)_ are the first and second ciphertext in Eqn. (5), and _left_ and _right_ denote left and right child of the current node, respectively. We initialize an empty tree \(=\) and set its _cipher\({}_{1}=c_{}/2\)_ with \(c_{}=2^{_{2}n}\), and then we construct binary search tree iteratively. We maintain an interval \([t_{},t_{}]\) in each iteration so as to keep the increasing order of ciphertexts \(c_{1},c_{2},,c_{s}\) in Eqn. (5). During the \(i\)-th iteration, we receive a sample \((a_{i},y_{i})\), and then take two steps as follows:

**Step-I: Search a node for sample \((a_{i},y_{i})\) in binary search tree \(\)**

Let \(t\) be a node pointer with the initialization of the root of \(\). We search a path downward in \(\) by comparing with \(a_{i}\), and the search will terminate when \(t\) is a leaf node or an empty node.

For an internal node \(t\), the search continues to its left child and updates \(t_{}=t._{1}\) if

\[t. a_{i}< \{a_{j}(a_{j},y_{j}) t..\}\ ;\]

and the search continues to its right child and updates \(t_{}=t._{1}\) if

\[t. a_{i} >\{a_{j}(a_{j},y_{j}) t..\}\ ;\]

otherwise, the search terminates. This procedure can be easily implemented with a while loop.

It is necessary to consider two special cases after the above search. We update \(t=t.\) if

\[t.,\ a_{i}<\{a_{j}:(a_{j},y_{j}) t.\}\ \ y_{i}=y_{j}\ \ (a_{j},y_{j}) t... \]

In a similar manner, we update \(t=t.\) if

\[t.,\ a_{i}>\{a_{j}:(a_{j},y_{j}) t.\}\ \ y_{i}=y_{j}\ \ (a_{j},y_{j}) t... \]

**Step-II: Update the binary search tree \(\)**

After Step-I, we could find a node \(t\) for sample \((a_{i},y_{i})\) and the corresponding interval \([t_{},t_{}]\). We directly append the example \((a_{i},y_{i})\) into \(t.\) if \(y_{i}=y_{j}\) for every \((a_{j},y_{j}) t.\); otherwise, it is necessary to split the node \(t\) according to \(a_{i}\).

We initialize an empty node \(l\) with \(l.=\{(a_{j},y_{j}) t. a_{j}<a_{i}\}\), and it is sufficient to consider \(l.\). If \(t.\), then we set

\[l._{1}=(t.._{1}+t._ {1})/2+ t.._{1}<l._{1}<t._{1}\, \]

and update \(l.=t.\), \(t.=l\); otherwise, we set

\[l._{1}=(t_{}+t._{1})/2+ l._{1}(t_{},t._{1})\, \]

and update \(t.=l\). Here, \(\) is a random number sampled from \((0,1)\), and notice that we may randomly sample \(\) multiple times so that the condition holds in Eqns (8)-(9), respectively.

We make similar update for the right child of node \(t\): initialize an empty node \(r\) with \(r.=\{(a_{j},y_{j}) t. a_{j}>a_{i}\}\), and consider \(r.\). If \(t.\), then we set

\[r._{1}=(t._{1}+t.._{ 1})/2+t._{1}<r._{1}<t.. _{1}\;, \]

and update \(r.=t.\), \(t.=r\); otherwise, we set

\[r._{1}=(t._{1}+t_{})/2+r._{1}(t._{1},t_{})\;, \]

and update \(t.=r\). Algorithm 2 presents the detailed descriptions on the splitting of node \(t\).

Algorithm 1 presents an overview of our Gini-impurity-preserving encryption, and the decryption is given in Appendix A. Our scheme does not only keep the minimum Gini impurity, but also change frequencies to prevent decryption from frequencies, which is also beneficial for encryption . Our scheme takes an average of \(O(n n)\) computational complexity, since it requires \(O( n)\) and \(O(1)\) computational complexities to search and update a node in each iteration, respectively. Finally, the average and worst space complexities are \(O( n)\) and \(O(n)\) for our encryption, respectively.

```
Input: Encrypted datasets \( S_{n}^{t}\), available splitting feature and position \( s_{i=1}^{2}\), and secret key \(k_{}\) Output: index \(i^{*}\)  %% Server: for\(i[]\)do  Calculate Gini impurity \(I_{G}( S_{n}^{t}, s_{i})\) from Eqn. (12) w.r.t splitting feature and position \( s_{i}\) endfor  Send ciphertexts \(\{I_{G}( S_{n}^{t}, s_{i})\}_{i[ ]}\) to the client  %% Client:  Get the decrypted \(\{(k_{},I_{G}( S_{n}^{t}, s _{i}))\}_{i[]}\)  Set \(i^{*}=-1\) if \((k_{},I_{G}( S_{n}^{t}, s _{i}))=0\) for every \(i[]\); otherwise, set \(i^{*}\) by Eqn. (13)  Send \(i^{*}\) to the server
```

**Algorithm 3** Finding the best splitting feature and position

### Security Analysis

For ciphertext vector \( a_{1}\), since the security of \( a_{2}\) has been analyzed in homomorphic encryption CKKS . Following semantic security against chosen plaintext attacks , we define a security game \(_{}\):

* An adversary chooses two sequences with distinct plaintexts \(\{a_{1}^{0},,a_{n}^{0}\}\) and \(\{a_{1}^{1},,a_{n}^{1}\}\), and sends them to a challenger;
* The challenger flips an unbiased coin \(b\{0,1\}\) to select \(\{a_{1}^{b},,a_{n}^{b}\}\), and randomly sets their corresponding labels \(\{y_{1}^{b},,y_{n}^{b}\}\) with each \(y_{i}^{b}\) drawn independently and uniformly over \([]\). The challenger encrypts \(\{a_{1}^{b},,a_{n}^{b}\}\) by Eqns. (4) and (5), and sends the ciphertexts to the adversary;
* The adversary outputs a guess of \(b\), i.e., which sequence is selected for encryption.

We then introduce the security against Gini-impurity-preserving chosen plaintext attack as follows.

**Definition 2**.: A scheme is said to be indistinguishable under Gini-impurity-preserving chosen plaintext attack if the probability of outputs with the correct guess \(b\) is negligible for the adversary \(\) in \(_{}\), that is,

\[[(_{})=b]<1/2+\;.\]

The following theorem shows that our encrypted plaintexts sequences are indistinguishable.

**Theorem 3**.: _Our scheme for the first ciphertexts \( a_{1}_{1}, a_{2}_{1},,  a_{n}_{1}\) in Section 3.2 is security against Gini-impurity-preserving chosen plaintext attack._

The detailed proof is presented in Appendix C, and the basic idea is inspired from . We take induction on \(n\) to show that data point \((a_{i+1}^{b},y_{i+1})\) affects the constructed binary search trees with the same probability as \(b=0\) and \(b=1\), and then the ciphertexts of data points \((a_{i+1}^{b},y_{i+1})\) also follow the same distribution, i.e.,

\[P( a_{1}^{0},, a_{i+1}^{0}|a _{1}^{0},,a_{i+1}^{0})=P( a_{1}^{1}, , a_{i+1}^{1}|a_{1}^{1},,a_{i+1}^{1})\;.\]

## 4 Encrypted Random Forests

For encrypted random forests, we follow the popular client-server protocols . A client encrypts training and testing data, and transfers encrypted data to an honest-but-curious server. The server trains random forests from the encrypted data with the aid of client, and finally returns predictions on encrypted testing data.

#### Encryption for training and testing datasets

Recall training data \(S_{n}=\{(_{1},y_{1}),,(_{n},y_{n})\}\) with \(_{i}=(x_{i,1},,x_{i,d})\). The client constructs \(d\) binary search trees \(_{1},_{2},,_{d}\) according to Algorithm 1 over different dimensional features and labels in \(S_{n}\), where \(_{j}\) is used to encrypt features \(\{x_{1,j},,x_{n,j}\}\) for \(j[d]\).

We take the homomorphic encryption CKKS  to encrypt training labels \(y_{1},,y_{n}\). Each label \(y_{i}\) is encoded with a vector of size \(\) by one-hot method, and we encrypt the vector by homomorphic encryption CKKS with a public key \(k_{pub}\). The ciphertexts \([y_{i}]=[[y_{i,1}],,[y_{i,}]]\) is given by

\[[\![y_{i,j}]\!]=\{(k_{pub},1)&j =y_{i},\\ (k_{pub},0)&..\]

We obtain the final training data \([\![S_{n}]\!]=\{(_{1},[\![y_{1}]\!],,( _{n},[\![y_{n}]\!])\}\).

Let \(_{n^{}}=\{}_{1},,}_{n^{}}\}\) be a testing data with instance \(}_{i}=(_{i,1},,_{i,d})\). For every plaintext \(_{i,j}\) with \(i[n^{}]\) and \(j[d]\), we search a node \(t\) in the binary search tree \(_{j}\), similarly to the node search (Step-I) in Section 3.2, and obtain its ciphertext \([\![_{i,j}]\!]=[t.cipher,(k_{pub},i)]\). We have the encrypted testing data \([\![_{n^{}}]\!]=\{[\![}_{1}]\!],,[\![}_{n^{}}]\!]\}\).

#### Construction on encrypted random forests

Encrypted random forests consist of individual decision trees \(_{1},,_{m}\), where each tree \(_{i}\) is constructed as follows. We first take a bootstrap sample \([\![S^{}_{n}]\!]\) from \([\![S_{n}]\!]\), and initialize \(_{i}\) with one node of data \([\![S^{}_{n}]\!]\). We repeat the following procedure recursively for each leaf node, until the number of training samples is smaller than \(\), or all instances have the same label in the leaf node:

* Select a \(k\)-subset \(\) from \(d\) available features randomly without replacement;
* Find the best splitting feature in \(\) and position by Gini impurity from the encrypted data;
* Split the current node into left and right children via the best splitting position and feature.

Such construction is essentially similar to original random forests , whereas we require a different way to find the best splitting feature and position based on Gini impurity from the encrypted data.

Let \(t\) be the current leaf node for further splitting with the encrypted training data \([\![S^{t}_{n}]\!][\![S_{n}]\!]\), and \([\![s]\!]_{1},,[\![s]\!]_{d}\) denote all possible splitting features and positions in the scope of the corresponding feature subset \(\) from \([\![S^{t}_{n}]\!]\). Here, the information of feature and position can be derived from the corresponding index \(i[\![j]\!]\) and subset \(\).

For each \(i[\![j]\!]\), the server partitions the current encrypted training data \([\![S^{t}_{n}]\!]\) into left and right subsets, i.e., \([\![S^{t}_{n}]\!]_{i}^{t}\) and \([\![S^{t}_{n}]\!]_{i}^{t}\), according to the splitting feature and position \([\![s]\!]_{i}\). Let \(n_{l}\) and \(n_{r}\) be the number of training examples in \([\![S^{t}_{n}]\!]_{i}^{t}\) and \([\![S^{t}_{n}]\!]_{i}^{t}\), respectively, and denote by

\[[\![S^{t}_{n}]\!]_{i}^{t}=\{([\![^{t}_{1}]\!],[\![y^{t}_{1}]\!]),,( [\![^{t}_{n_{l}}]\!],[\![y^{t}_{n_{l}}]\!])\}[\![S^{ t}_{n}]\!]_{i}^{t}=\{([\![^{r}_{1}]\!],[\![y^{r}_{1}]\!]),,([\![^{r}_{n _{r}}]\!],[\![y^{r}_{n_{r}}]\!])\}\;.\]

From Eqn. (1), we have Gini impurity

\[I_{G}([\![S^{t}_{n}]\!],[\![s]\!]_{i})=}{n_{l}+n_{r}} I _{G}([\![S^{t}_{n}]\!]_{i}^{t})}{n_{l}+n_{r}}  I_{G}([\![S^{t}_{n}]\!]_{i}^{t})\;, \]

  Datasets & \#Inst & \#Feat & Datasets & \#Inst & \#Feat & Datasets & \#Inst & \#Feat & Datasets & \#Inst & \#Feat \\  wdbc & 569 & 30 & adver & 3,279 & 1,558 & allerons & 13,750 & 41 & adult & 48,842 & 14 \\ cancer & 569 & 31 & bibtex & 7,396 & 1,836 & house & 22,784 & 16 & mnist & 70,000 & 780 \\ breast & 699 & 9 & plp@b & 7,797 & 617 & a@a & 32,563 & 123 & miniboone & 72,998 & 51 \\ diabetes & 768 & 8 & pendigits & 10,992 & 16 & amazon & 32,769 & 9 & runwalk & 88,588 & 6 \\ german & 1,000 & 24 & phish & 11,055 & 30 & bank & 45,211 & 17 & covtype & 581,012 & 54 \\  

Table 2: Datasets

[MISSING_PAGE_FAIL:8]

## 5 Experiment

We conduct experiments on 20 datasets2 as summarized in Table 2. Most datasets have been well-studied in previous random forests. In addition to the original (plaintexts) random forests , we compare with six state-of-the-art privacy-preserving random forests in recent years.

* AnonyRFs: random forests based on anonymization with a top-down greedy search ;
* DiffPrivRFs: random forests based on differential privacy ;
* PPD-ERTs: extremely randomized trees from distributed structured data ;
* PivotRFs: random forests based on a hybrid of threshold partially homomorphic encryption and secure multiparty computation techniques ;
* MulPRFs: random forests based on the secure multiparty computation ;
* HELdpRFs: random forests with fully homomorphic encryption and low-degree polynomial approximations .

For all random forests, we train 100 individual decision trees, and randomly select \(\) candidate features during node splitting. We set \(=10\) for datasets of size smaller than 20,000 for our encrypted random forests; otherwise, set \(=100\), following . For multi-class datasets, we take the one-vs-all method for MulPRFs, since it is limited to binary classification. Other parameters are set according to their respective references, and more details can be found in Appendix D.

**Experimental comparisons**

The performance is evaluated by five trials of 5-fold cross validation, and final prediction accuracies are obtained by averaging over these 25 runs, as summarized in Table 3. It is evident that our encrypted random forests take comparable performance with original random forests  on plaintexts, which nicely supports our Theorem 1 on the preservation of minimum Gini impurity in the construction of random forests. Our encrypted random forests are also comparable to MulPRFs if they can obtain results within \(10^{6}\) seconds (about 11.6 days), since MulPRFs are essentially similar to original random forests, yet with different implementation of secure multi-party computation.

As can be seen from Table 3, our random forests take significantly better performance than AnonyRFs and DiffPrivRFs, since the win/tie/loss counts show that our random forests win for most times and never lose. This is because AnonyRFs combine features by anonymization, while DiffPrivRFs add perturbations to features via differential privacy, therefore, both of them cause information lost in privacy process. Our random forests also achieve better performance than PivotRFs, since PivotRFs have to limit trees' depth for random forests due to heavy computations for HE and communications for secure multi-party computation.

Our random forests also outperform PPD-ERTs and HELdpRFs if results are obtained in \(10^{6}\) seconds, since PPD-ERTs adopt completely-random splitting, rather than selecting the minimum Gini impurity, while HELdpRFs take homomorphic encryption on features and employ low-degree polynomial approximation. Those approaches have modified the structures of original random forests.

Figure 2: Comparisons of training running time on different random forests. Notice that the y-axis is in log-scale, and full black columns imply that no result was obtained after running out \(10^{6}\) seconds (about 11.6 days).

#### Running time

All experiments are performed by c++ on the Ubuntu with 256GB main memory (AMD Ryzen Threadripper 3970X). We compare the training running time of our encrypted random forests and others, and the average CPU time (in seconds) is shown in Figure 2.

As expected, original random forests take the least running time over raw datasets without privacy preservation. Our encrypted random forests take larger running time than AnonyRFs and DiffPrivRFs because they are essentially similar to original random forests, yet with some simple modifications or perturbations on features. Our encrypted random forests take better performance and higher security.

Our encrypted random forests take smaller running time than PPD-ERTs, PivotRFs, MulPRFs and HELdpRFs, in particular for large datasets or high-dimensional datasets, where no results are obtained even after running out \(10^{6}\) seconds (almost 11.6 days). Because PPD-ERTs, PivotRFs and MulPRFs require expensive communication cost for multi-parity computation, while PivotRFs and HELdpRFs take heavy computation costs on HE scheme.

#### Security analysis

We present security analysis for the first ciphertext \( a_{1}\) in ciphertext vector \( a=( a_{1}, a_{2})\), and the second ciphertext \( a_{2}\) can be ensured by HE scheme. We compare with four state-of-the-art encryptions: differential privacy , anonymization , order-preserving scheme  and HE scheme . Here, we present results of six datasets and randomly selecting one feature, and trends are similar on other dimensions and datasets. More results can be found in Appendix D.

Figure 3 shows the comparison results, and we take the bitwise leakage matrices to measure the security as in : the more red the area, the higher the security. As expected, HE scheme presents the highest security, yet with heavy computational costs, for example, no results are obtained for datasets of size exceeding 3000 even after running out \(10^{6}\) seconds. It is also observed that our scheme presents higher security than the other three schemes, since those schemes simply present perturbations, compression or preserve the entire order information regardless of learning ingredients. In comparison, our scheme could make a good balance between security and computational cost.

## 6 Conclusion

This work takes one step on data encryption from some crucial ingredients of learning algorithm. We present a new encryption to preserve data's Gini impurity, which plays a crucial role during the construction of random forests. For random forests, we encrypt data features based on our Gini-impurity-preserving scheme, and take the homomorphic encryption scheme CKKS to encrypt data labels. Both theoretically and empirically, we validate the effectiveness, efficiency and security of our proposed method. An interesting work is to exploit other learning ingredients, such as gini index and information gain, for data encryption in the future.