# Amortized Active Causal Induction with Deep Reinforcement Learning

Yashas Annadani\({}^{1,2}\)  Panagiotis Tigas\({}^{3}\)  Stefan Bauer\({}^{1,2}\)  Adam Foster

\({}^{1}\) Helmholtz AI, Munich \({}^{2}\) Technical University of Munich

\({}^{3}\) OATML, University of Oxford

###### Abstract

We present Causal Amortized Active Structure Learning (CAASL), an active intervention design policy that can select interventions that are adaptive, real-time and that does not require access to the likelihood. This policy, an amortized network based on the transformer, is trained with reinforcement learning on a simulator of the design environment, and a reward function that measures how close the true causal graph is to a causal graph posterior inferred from the gathered data. On synthetic data and a single-cell gene expression simulator, we demonstrate empirically that the data acquired through our policy results in a better estimate of the underlying causal graph than alternative strategies. Our design policy successfully achieves amortized intervention design on the distribution of the training environment while also generalizing well to distribution shifts in test-time design environments. Further, our policy also demonstrates excellent zero-shot generalization to design environments with dimensionality higher than that during training, and to intervention types that it has not been trained on.

## 1 Introduction

Infer, design and experiment is a three step loop in the empirical scientific discovery paradigm. Causal induction (a.k.a. causal structure learning), the problem of finding causal relationships present in data, also falls under this paradigm when experiments in the form of interventions are permissible . Causal structure learning has gained increasing importance in empirical sciences, for example in single-cell biology, where perturbation experiments like gene knockouts can be carried out with high-precision . Such interventions are not only more informative to infer the underlying causal graph than just observational data, but in certain cases essential to go beyond the Markov equivalence class , making the problem of design of interventions both relevant and important. For the problem of structure learning with interventions, however, inference and design both involve significant challenges. For instance, inference of the causal graph from data usually involves search over the space of graphs with a likelihood (usually weighted by a prior) or score function , which is slow and not robust to violations of data generation assumptions . The design of informative interventions, on the other hand, utilizes the inferred causal graph from existing data to select promising designs and rank them according to a scoring criterion. This scoring criterion is usually based on an approximation of mutual information between the unknown causal graph and the interventional data , which

Figure 1: Causal Amortized Structure Learning (CAASL) is an active intervention design method that directly proposes the next intervention to perform by just a forward-pass of the transformer based policy.

also involves the (interventional) data likelihood. In problems related to empirical sciences where causal structure learning is essential, like inferring a gene regulatory network with gene knockouts or knockdowns, the likelihood of the data is typically intractable. While progress has been made in terms of likelihood-free inference of causal graphs [38; 32], existing intervention design algorithms have been largely restricted to likelihood-based strategies.

With a focus on addressing practical intervention design challenges that arise in empirical sciences like inferring the gene regulatory network, in this work, we propose an intervention design method called CAASL that significantly differs from existing approaches. Instead of following the infer, design and experiment loop, we amortize the intervention design procedure by training a single design network policy, based on the transformer , which encodes key design space symmetries. During test-time, our trained policy directly predicts the next intervention to perform by just a forward-pass of the data collected so far, without the need to undergo slow and expensive inference of the causal graph corresponding to that data. We train the transformer policy with Soft Actor-Critic (SAC)  to maximize cumulative rewards over a fixed number of design iterations (budget), thereby making the policy adaptive. The choice of a good reward function is essential for informative designs. We discuss various reward function choices, primarily based on an estimate of the true causal graph obtained from a likelihood-free amortized causal structure learning approach. Both our policy and the reward function only require access to a simulator of the design environment. Further, we present connections of our approach to amortized sequential Bayesian experimental design . We demonstrate that the reward function is related to an approximation of expected information gain based on the amortized posterior distribution over causal graphs. As such, CAASL is an intervention design method for performing sample efficient causal structure learning, but is not a new causal structure learning method in itself.

On synthetic data and the single-cell gene expression simulator SERGIO , we empirically study various aspects of our trained policy--the amortization performance on training distribution of the design environment as well as on design environments with distribution shifts from the training environment. We find that our policy obtains better causal structure learning performance for a given budget than alternate intervention strategies. Overall, we observe excellent generalization capability of the transformer for intervention design, similar to what has been demonstrated in other domains [10; 31; 58]. The robustness of the amortized policy opens up the possibility for lab-in-the-loop intervention design for single-cell data, wherein a single network can propose informative interventions across different cell lines and experimental conditions.

Our contributions are:

* We propose an amortized active intervention design method for causal structure learning based on a transformer parameterized policy that encodes key design space symmetries.
* Based on the AVICI  amortized causal structure inference model, we propose a reward function for training the policy with reinforcement learning that does not require access to the likelihood.
* We demonstrate the superiority of CAASL by performing extensive evaluation on various in-distribution and out-of-distribution settings in the synthetic and SERGIO  gene regulatory network simulator environments.

## 2 Background and Related Work

Structural Causal Models.Let \(=\{y_{1},,y_{d}\}\) be the random variables of interest associated with the vertices of a graph \(G\). Let \(A\{0,1\}^{d d}\) be the adjacency matrix corresponding to \(G\). A Structural Causal Model (SCM)  is a framework for causality which consists of a set of equations in which each variable \(y_{i}\) is a deterministic function of its direct causes \(y_{_{G}(i)}\) as well as an exogenous noise variable \(_{i}\) with a distribution \(P_{_{i}}\)

\[y_{i} f_{i}(_{_{G}(i)},_{i};_{i}). \]

The functions \(f_{i}\), with parameters \(_{i}\), are mechanisms that relate how the direct causes affect the variable \(y_{i}\). The structural assignments are typically assumed to be acyclic, with \(G\) being a directed acyclic graph whose edges indicate direct causes. In addition, an SCM defines the likelihood of any data sample \(\) under this model, denoted as \(p(\{A,\})\). Further, we assume that the SCM is causally sufficient, i.e. all the variables are measurable (but can be missing at random), and the noise variables are mutually independent.

Interventions.The SCM framework admits reasoning about effects of interventions on any variable in \(\). Most notable types of intervention include a perfect (do) intervention, and a shift intervention . A perfect intervention on any variable \(y_{i}\) corresponds to changing the structural equation of that variable to the desired value, \(y_{i} v_{i}\). It is denoted by the do-operator  as do(\(y_{i}=v_{i}\)). In a shift intervention, the conditional mean of the interventional variable \([y_{i}_{_{}(i)}]\) is shifted by \(v_{i}\). The likelihood of any data under an intervention \(I\) is denoted as \(p(\{A,\},I)\). For perfect and shift interventions, \(I\) can be parameterized as a \(d 2\) dimensional matrix, where the first column corresponds to one-hot encoding of whether a particular variable is intervened or not, and the second column corresponds to the value (or the shift) of the intervention corresponding to each potential intervention target.

Causal Structure Learning.The problem of causal structure learning corresponds to estimating \(A\) (and other parameters of the SCM \(\)) given samples from \(p_{}\). In general, there could be multiple models (and hence graphs) that can be consistent with a given joint distribution over \(\), which necessitates causal structure learning with interventional data . There are various approaches, either based on independence tests , or graph search by maximizing a score function (likelihood of the data with certain assumptions on the SCM) . Reinforcement learning has also been used for search over graphs with a score function , however it differs entirely from our approach wherein we use RL for intervention design. Alternately, based on the tractable likelihood, there are also causal structure learning methods that estimate the posterior distribution \(q(A)\) of graphs  for a dataset \(\) that is sampled from \(p_{}\).

Likelihood-Free Amortized Causal Structure Learning (AVICI) .More recently, instead of inferring causal graphs over specific datasets, amortized posterior inference of causal graphs has also been studied . In particular, the amortized posterior from Lorch et al. , called AVICI, makes use of a transformer to directly predict the posterior \(q(A)\) by just a forward-pass of any dataset. The amortized posterior, parameterized as a product of independent Bernoulli random variables over the presence of edges in the causal graph, is trained from a simulator without having access to the likelihood of the data. Since the simulator provides the ground truth value of \(A\), the amortized posterior can be trained by maximum likelihood of graph edges with a combination of observational and interventional data. Empirically it has been shown that the AVICI model can amortize over datasets with different dimensionalities \(d\), while also generalizing to new datasets that have not been seen during training. Since it is computationally cheap to obtain the posterior distribution with AVICI, we use it for computing the reward for our intervention design policy.

Active Intervention Design.Active intervention design is the problem of designing interventional experiments to obtain data that enables causal structure learning in a sample efficient manner (under a fixed budget). While adaptive strategies have been explored , these approaches still require intermediate inference of the SCM, and are also not amortized. Intervention design based on Bayesian optimal experimental design  has also been considered, although only with additive noise models, which enable likelihood evaluation . Reinforcement learning has also been used in intervention design , however, they have been limited to non-amortized or small scale settings.  highlight the usefulness of intervention design in an amortized framework for causal discovery. In contrast to earlier work, we demonstrate the applicability of our method to single-cell simulated gene expression data, wherein the mechanisms are defined by differential equations and also include technical noise (section 5.2).

## 3 Amortized Intervention Design

We first present our active intervention design strategy with reinforcement learning, the corresponding amortized network and its training. In Section 4.1, we then present connections of our reward to sequential Bayesian experimental design.

Setting.Given a budget \(T\), intervention design is the problem of finding a sequence of informative interventions with a policy \(I_{1},,I_{T}\) that results in an estimate of the causal graph that is close to \(A\). For any intervention \(I\), a causal model defines a generative model of the data with likelihood \(p(\{A,\},I)\) and prior \(p(A,)\). We indicate initial (observational) data, if available, as \(_{0}=\{_{0}^{(i)}\}_{i=1}^{n_{0}}\) and the corresponding interventions with \(I_{0}\), where \(I_{0}=\{\}^{n_{0}}\) if the initial data is fully observational. Let \(h_{t}^{(n_{0}+t) d 2}\) denote the interventional history \((_{0},I_{0}),,(_{t},I_{t})\), obtained by concatenation of \(\) and first column of \(I\) that correspond to interventional targets. We do not explicitly encode intervention values in history, since for a do intervention, the intervention values are already present in \(\)1. Existing intervention design strategies like  approximate a posterior on \(\{A,\}\) at each step \(t\), approximate expected information gain (EIG)  and greedily maximize it to compute \(I_{t+1}\). Details of this greedy approach are given in Appendix A.1.

### Intervention Design with Reinforcement Learning

In this work, we instead treat intervention design as a Reinforcement Learning (RL) problem and train a single policy network \(_{}\) with parameters \(\) to obtain a sequence of adaptive interventions \(I_{1},,I_{T}\) for any underlying causal graph with adjacency matrix \(A\). In order to do so, we first describe the RL environment under which the interventions are performed.

Intervention Design Environment.Similar to Blau et al. , we define an interventional design environment as a Hidden-Parameter Markov Decision Process (Hi-MDP) . The Hi-MDP we use, \((\{A,\})\), has hidden parameters \(\{A,\}\) and can be fully described by the tuple \((,,,,,R,,\)\(p_{})\). The state-space \(\) consists of the histories \(s_{t}=h_{t}\), the initial state \(=h_{0}=(_{0},I_{0})\) corresponds to initial data, the action-space \(\) corresponds to interventions \(a_{t}=I_{t}\) and \(\) describes the space of all causal models (graphs and parameters) with prior \(p_{}=p(A,)\). The hidden parameters are sampled for each episode at the beginning from the prior. \(\) is the discount factor. In a HiP-MDP, the transition function \(\) and reward \(R\) depend on the hidden parameters. The transition function \((h_{t} h_{t-1},I_{t},\{A,\})\) is Markovian, and it involves two operations: (1) sampling interventional data \(_{t} p(\{A,\},I_{t})\), and (2) updating the history state \(h_{t}=[h_{t-1},(_{t},I_{t})]\). For a reward function \(R(h_{t},I_{t},h_{t-1},\{A,\})\) that we define below, intervention design corresponds to finding the parameters \(\) of the amortized policy that maximizes the expected cumulative reward of all interventions:

\[_{} *{}_{_{},,p(A,)} [_{t=1}^{T}^{t-1}R(h_{t},I_{t},h_{t-1},\{A, \})] \] \[\ \ I_{t}_{}(h_{t-1})\]

Reward Function.For the purpose of amortized intervention design, a good reward function should be cheap to evaluate while leading to informative interventions. In this work, we propose to utilize the estimate of the causal graph from an amortized causal graph posterior \(q( h_{t})\). In particular, we use the pretrained AVICI model . AVICI is a transformer based neural network trained with (interventional) data from a simulator to directly predict the probability of presence or absence of any edge in the causal graph by just a forward pass of the data, without requiring access to the likelihood. For any history \(h_{t-1}\), we define the reward for performing intervention \(I_{t}\) and reaching state \(h_{t}\) as the improvement in the number of correct entries in the predicted adjacency matrix of the AVICI model:

\[R(h_{t},I_{t},h_{t-1},\{A,\})=*{}_{q(|h_{t })}[_{i,j}[_{i,j}=A_{i,j}]]-R(h_{t-1 },I_{t-1},h_{t-2},\{A,\}) \]

where \([]\) is the indicator function and \(R(h_{0},I_{0},\{A,\})=*{}_{q(|h_{0})}[ _{i,j}[_{i,j}=A_{i,j}]]\). We note that our choice of reward function revolves around obtaining a good estimate for the causal graph, \(A\); we do not (directly) reward learning about \(\).

The above RL problem for intervention design is intuitive: reward the intervention in proportion to the improvement it brings in terms of number of correct entries of the adjacency matrix from the amortized posterior. Also, for any \(t\), the cumulative reward, eq. (2), for \(=1\) of all interventions including \(I_{0}\), which includes an additional term \(R(h_{0},I_{0},\{A,\})\), is simply the number of correct entries of the adjacency matrix predicted by the amortized posterior model for \(h_{t}\). This reward telescoping was inspired by Blau et al. . We also show in Section 4.1 that this reward function is also related to an approximation of multi-step EIG, the quantity of interest in sequential Bayesian experimental design .

### Policy

Architecture.In order for the policy to achieve amortization and generalize to new environments not seen during training, it should encode key design space symmetries. In particular, for the problem of intervention design, the interventions should be permutation equivariant to ordering of the variables and permutation invariant to the ordering of the history. This can be ensured by a transformer architecture  wherein self-attention is applied alternately--once over the variable axis and next over the samples axis . More precisely, we input \(h_{t}^{(n_{0}+t) d 2}\) and apply self-attention2 over first the \(n_{0}+t\) axis and next over the \(d\) axis. This ensures that the history representation is permutation equivariant over both the axes . After multiple layers of alternating self-attention, we apply max pooling over the samples (dim. \(n_{0}+t\)) axis, which gives an encoding of size \(l\) of the history \(B_{t}^{d l^{}}\) that respects the desired symmetries. The same symmetries apply for amortized causal structure learning, hence the reward model AVICI also leverages the alternate attention architecture. The history embedding \(B_{t}\) is then passed through a multi-layer perceptron, whose outputs parameterize the logits of the Gaussian-Tanh distribution , from which the interventions are sampled. In our setting, we model both the intervention targets and intervention values, hence \(I_{t}\) is \(d 2\) dimensional. Gaussian-Tanh samples range from -1 to 1. We use \(I_{t}[:,0]\) to encode the interventional targets by discretizing the values to a binary mask (0 and 1) by thresholding at 0, where 1 indicates intervention on the variable \(y_{i}\). If an intervention on \(y_{i}\) is active, the value to intervene with is given by \(I_{t}[i,1]\).

Training.Training the policy involves addressing two main challenges: computing the reward in eq. (3) since \(\{A,\}\) would be unknown for real datasets, and optimizing this reward, which is discrete. In order to address the first challenge, we simulate interventional data \(_{t} p(\{A^{},^{}\},I_{t})\) for a sample \(\{A^{},^{}\} p(A,)\) from the prior using a _simulator_. Such simulators exist for single cell gene regulatory networks (e.g. ) and are becoming increasingly widespread in other

Figure 2: Schematic diagram illustrating the proposed CAASL policy along with the AVICI model  for computing the reward for interventions designed.

domains [24; 2]. The reward model AVICI is pretrained on datasets from the prior \(p(\{A,\})\) using the same simulator. During training of the policy, we only use the pretrained reward model for inference and do not update its parameters. To address the second challenge, we train our policy using Soft-Actor Critic (SAC) , an off-policy reinforcement learning algorithm that does not require rewards to be differentiable. We use the REDQ version of SAC to improve sample efficiency . REDQ trains multiple Q-function networks to optimize the reward. For each Q-function network, we use a transformer based history state encoder with architecture similar to that in the policy, but the weights are not shared. This is beneficial because the same equivariance-invariance properties that hold for the policy should also hold for the Q-function.

Inference.Deploying the policy in a real (i.e. not simulated) environment amounts to a rollout of the policy through interaction with the real environment. This requires just a forward pass of policy network for each time step \(t\). Note that we do not need intermediate Bayesian inference or other estimation of the causal graph on the collected data.

## 4 Choice of Reward Function

### Connection to Sequential Bayesian Experimental Design

As discussed in Appendix A, the problem we tackle has a connection to likelihood-free sequential Bayesian experimental design [22; 30]. With the aim of gathering data to learn about the causal graph \(A\), the multi-step expected information gain (EIG) can be written

\[(A;_{})=}_{_{},, ,p(\{A,\})}[ p(A h_{t})]+. \]

Since the posterior \(p(A h_{t})\) is intractable, we could replace it by an approximate posterior \(q(A h_{t})\). This gives rise to the Barber-Agakov (BA) bound [6; 21], which was recently explored in a sequential context by Blau et al. . This tells us that we have an EIG lower bound by using \(q\) in place of \(p\):

\[(A;_{})}_{_{},, ,p(\{A,\})}[ q(A h_{t})]+. \]

We can interpret eq. (5) in simple terms--taking \( q(A h_{t})\) as a reward function is equivalent to optimizing a lower bound on the EIG. Although eq. (5) implies that we only receive a reward on the final state \(h_{t}\), it is possible to rewrite this using telescoping rewards  exactly as we do in eq. (3). The BA bound therefore represents the closest point of comparison between the method we outline in Section 3 and sequential Bayesian experimental design. As with the BA bound, we make use of an amortized approximate posterior distribution \(q(A h_{t})\) that works backwards from data \(h_{t}\) to predict the graph that might have generated it. Unlike the BA bound, however, we use the adjacency matrix accuracy to compare the true \(A\) to samples \(\) from the amortized posterior, rather than computing the log-likelihood of the true graph under that amortized posterior, \( q(A h_{t})\). We found that this worked better in practice. Nevertheless, we see a close relationship between the approach we take and the methods of sequential Bayesian experimental design.

### Other Possible Reward Functions

Any target metric for causal structure learning like structural hamming distance computed on the amortized posterior could be used as a reward function. Depending on the application, domain specific causal graph objectives could also be considered. While a large number of possibilities exist, we use expected number of correct entries of adjacency matrix, eq. (3), as the reward for training CAASL. As opposed to Structural Hamming Distance (SHD) and Area Under Precision Recall Curve (AUPRC), eq. (3) is straightforward to compute and parallelize.

## 5 Experiments

We train CAASL policy on two challenging environment domains: 1. Synthetic design environment with a causal model defined by linear mechanisms and additive noise, and 2. SERGIO , a single-cell simulator corresponding to any gene regulatory network. For each domain, we train a single CAASL policy on a distribution of design environments with \(d=10\). A distribution of intervention design environments is defined by the choice of prior over causal models \(p(A,)\), which includes priors over graphs \(A\) (e.g. Erdos-Renyi ), mechanism parameters \(\) and noise. We define an Out-of-Distribution (OOD) environment as any environment with the choice of prior (either over graphs, mechanisms parameters or noise) that is different from training. In addition to these distribution shifts, we also consider OOD environments wherein the priors remain the same, but either the dimensionality of the data \(d\) increases (i.e. \(d>10\)), or the performed intervention type changes. Precise choice of training and OOD testing distributions are given in Appendix C. All evaluation experiments are conducted on environments with causal model parameters that CAASL has never seen during training, regardless of whether the environment is in-distribution or OOD. In addition, all evaluation is done by just forward passing the history through the policy.

Baselines.We compare our approach with two amortized strategies: Random and Observational. Random corresponds to obtaining data from random interventions, while Observational corresponds to collecting more observational data. The random baseline is shown to be very competitive for the problem of active causal structure learning, especially when the experimental budget or the density of graphs is sufficiently high . For the synthetic design environment domain, we also compare with DiffCBED  and SS Finite . These intervention strategies use likelihood of the data to perform designs. So in certain OOD synthetic design environments and the single-cell simulator SERGIO where the likelihood is not available, we omit these baselines. DiffCBED and SS Finite rely on an approximate causal graph posterior to design interventions. As suggested in , we use bootstrapped GIES  as the approximate posterior distribution for these baselines. For an evaluation task on 100 random design environments with a budget of 10, DiffCBED and SS Finite methods require approximate posterior inference of the causal graph 1000 times. The performance of DiffCBED and SS Finite are limited by the performance of the underlying approximate posterior inference method they rely on. In general, approximate posterior inference for causal structure is a computationally expensive problem with significant limitations .

Metrics.All evaluation is done on 100 random test environments. As CAASL is an intervention design method, we measure the cumulative rewards with \(=1\) (returns) obtained from the graph predicted by the amortized posterior. However, for the sake of completeness, we also measure structure learning related metrics like the Structural Hamming Distance (SHD), the Area under Precision Recall Curve (AUPRC) and F1 score (Edge F1) between the graph predicted by the amortized posterior and the true graph . Precise definition of these metrics is provided in

Figure 3: Visualization of the rollout of the trained CAASL policy on a randomly sampled environment with \(n_{0}=50\) initial observational samples. Colored circles indicate nodes with a do intervention. The policy selects interventions that mostly correspond to the variables with a child in the ground truth graph. At \(t=2\), the policy selects the only child \(y_{1}\), which breaks all direct causal effects. This gives lesser information about the overall causal model. After this, \(y_{1}\) is never chosen. Initially, the policy is exploratory wrt targets and exploitative wrt values. This trend is reversed as the episode progresses. The policy is trained on environments with \(d=2\), therefore it has not seen any graphs with \(d=3\) before.

Appendix G.2. We find that in most cases all the metrics are correlated. Therefore, unless otherwise mentioned, we only report the returns and relegate the other metrics to Appendix G.2.

### Synthetic Design Environment

Training Distribution of the Design Environment.We train CAASL on synthetically generated data, wherein \(p(A,)\) consists of linear SCMs with additive homoskedastic Gaussian noise. The dimensionality during training is \(d=10\). The prior over causal graphs is Erdos-Renyi , with 3 edges per node in expectation. The prior over linear coefficients is chosen such that the marginal variance of each variable is close to \(1\). This is done to ensure that structure learning algorithms are not sensitive to the scale of the data . During training, an intervention exclusively corresponds to a do intervention. Further, we set \(n_{0}=50\) and the budget \(T\) is fixed to \(10\).

Training Details.We train CAASL with \(4\) layers of alternating attention for the transformer, followed by a max pooling operation over the history, to give an embedding with size \(l=32\). SAC related hyperparameters are tuned based on performance on held-out design environments. Details of the architecture, hyperparameter tuning and optimizer is given in Appendix D. For the reward model, we use AVICI that is pretrained on random linear additive noise datasets.

Amortization Performance.We test on novel environments with hidden parameters sampled from the training prior \(p(A,)\). Results are provided in fig.4. We find that our policy significantly outperforms the random baseline in terms of returns as well as more common structure learning metrics like the SHD, AUPRC and Edge F1. For instance, our method achieves returns close to \(76\) with just \(10\) interventional samples, while the random baseline achieves close to \(72\). Other intervention strategies like DiffCBED  and SS Finite  perform worse, while still making use of the likelihood and performing intermediate inference of causal structure.

Zero-Shot OOD Generalization.We also test the trained CAASL policy on environments when the prior changes. All results correspond to zero-shot performance, obtained by just a forward forward pass of the trained policy. fig.5 presents the returns of CAASL alongside other applicable baselines. We consider shifts which become increasingly different from training: (1) the graph prior changes from Erdos-Renyi  to Scale-Free  (fig.5 (a)), (2) apart from the graph, prior over

Figure 4: Amortization results of various intervention strategies on 100 random test environments. CAASL significantly outperforms other intervention strategies. Shaded area represents 95% CI.

Figure 5: Zero-shot OOD returns of CAASL on 100 random environments with distribution shift coming from (a) graphs, (b) graphs and mechanisms, (c) graphs, mechanisms and noise, (d) noise changes from homoskedastic to heteroskedastic, and finally (e) intervention changes from do to a shift intervention. CAASL outperforms other intervention strategies. Shaded area represents 95% CI.

mechanisms also change (fig. 5 (b)), (3) apart from graph and mechanisms, the noise distribution changes from Gaussian to Gumbel (fig. 5 (c)). We find that our policy achieves better performance than random strategy by a significant margin. Further, our method also outperforms DiffCBED and SS Finite which explicitly optimize for designs corresponding to these environments. In addition to these OOD settings, we also consider OOD environments in which the prior remains the same, but the noise is heteroskedastic instead of homoskedastic (fig. 5 (d)). Although the random strategy is very competitive, CAASL performs better. Finally, we consider OOD environments wherein the intervention design suggested by the policy during testing is used for performing a shift intervention instead of a do (fig. 5 (d)). CAASL performs better than baselines even in this setting.

Slightly different to the above OOD environments, we also consider OOD environments in which the dimensionality of the data changes during testing, but the prior remains the same. fig. 6 presents the results, with further details in fig. 8. CAASL obtains better returns on average than random at all points of acquisition. The relative performance of CAASL decreases as \(d\) increases (up to \(d=30\)) from training, although it still performs better than random.

### Single-Cell Gene Regulatory Network Environment

In this setting, we train a CAASL policy based on the single-cell gene expression simulator SERGIO . Given a causal graph that corresponds to interaction between different genes in terms of their transcription regulation, SERGIO simulates expressions of genes that correspond to steady state of differential equations that govern the interaction between the genes. Each variable entry indicates the count of mRNA that is produced corresponding to that gene, similar to the output of modern single-cell RNA sequencing (scRNA-seq) technological platforms . In addition, SERGIO can be extended to support interventions. Interventions in this setting correspond to either gene knockouts, wherein the transcription rate of the intervened gene is actively set to 0, or gene knockdown, wherein the intervened gene's transcription rate is actively halved. Since there is no value selection in this setting, the dimensionality of the policy is \(d\) instead of \(d 2\). SERGIO also simulates technical noise such that the statistics of the data match that obtained from real scRNA-seq platforms. Some of the technical noise includes dropouts (missingness of the data), library size effects and random outlier effects. Most notably, atleast \(70\%\) of the data is missing in most single cell platforms. Therefore, in this domain, not only is the likelihood intractable, but also there is high amount of missing data. We do not impute the missing data, but just encode it with \(0\).

Training Distribution of the Design Environment.For training, we set \(d=10\), with \(n_{0}=50\) observational (wild-type) data with budget \(T=10\). The statistics of the data corresponds to 10X Chromium platform  wherein around 74% of the data is dropped out. The prior over causal graphs is set to Erdos-Renyi  with 3 edges per node on average. An intervention exclusively corresponds to a gene knockout. We provide details of the simulator in appendix B.2 and the training prior parameters in appendix C.2.

Training Details.We train CAASL with 3 layers of alternate attention, followed by a max pooling operation, giving an embedding of size \(l=32\). Just like in the synthetic linear domain, SAC related hyperparameters are tuned based on performance on held-out design environments. Details are given in appendix D. Once trained, we perform a forward pass of the history through the policy to obtain intervention designs for all test environments. For the reward model, we use AVICI that is pretrained on this simulator with post-noise data statistics matching that of 10X chromium platform.

Figure 6: Zero-Shot OOD generalization results when dimensionality \(d\) changes for synthetic environment. For training, \(d=10\). Left: Zero-Shot test returns with \(d=20\). Right: Relative mean zero-shot returns of CAASL wrt random for different \(d\). Results on 100 random environments. Shaded area represents 95% CI.

Amortization Performance.The in-distribution amortization performance is presented in fig. 7(a). After 5 acquisitions, CAASL obtains better returns than random.

Zero-Shot OOD Generalization.We test the CAASL policy when the environment is subject to various test-time distribution shifts. Robustness to distribution shifts is important in real world-settings, where experimental conditions can change. We consider 4 different OOD environments: (1) the prior over graphs changes from Erdos-Renyi to Scale-free (fig. 7(b)), (2) The perturbation platform changes to Drop-Seq , wherein among other noise parameters, the amount of missing data increases from 74 to 85% (fig. 7(c)), (3) The intervention type changes from knockout to knockdown (fig. 7(d)) and, (4) Noisy knockout interventions, where there is a 10% chance that either the intended gene does not get knocked out, or an off-target gene is knocked out (fig. 7(e)). We find that CAASL shows excellent robustness to these distribution shifts, and obtains better returns than baselines. When the intervention type changes, the random baseline is still competitive. An interesting observation is that the for the OOD graph and the OOD noise setting, the model shows exploratory behavior in the beginning where the returns decrease, but later becomes better than random. Robustness to various distribution-shifts demonstrates the generality of the policy.

Limitations.In the SERGIO gene regulatory network environment, we found that the random strategy is very competitive in general. In particular, for the setting of zero-shot OOD generalization with increasing data dimensionality, the performance of random strategy is on par with CAASL (fig. 9). We hypothesize that since almost \(74\%\) of data is missing, the incorporated design space symmetries might not be as relevant, which might limit the extent of zero-shot generalization. In addition, while the empirical results in the zero-shot OOD settings are extremely encouraging, there are no theoretical guarantees on the performance of CAASL in this setting.

## 6 Conclusion

We have presented an amortized and adaptive intervention design strategy CAASL, that does not require intermediate inference of the causal graph. CAASL is based on a policy parameterized by the transformer which is permutation equivariant to ordering of the variables and permutation invariant to ordering of the collected data. Through various experiments, including on a simulator which respects the data statistics of real gene-expression readouts, we find that our method shows excellent amortized intervention design and zero-shot generalization to significant distribution shifts. The achieved performance motivates intervention design in more complex settings - high-throughput experiments with large batch sizes and utilization of existing real offline data for designing interventions.