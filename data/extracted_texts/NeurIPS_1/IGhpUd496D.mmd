# Provable Editing of Deep Neural Networks using Parametric Linear Relaxation

Zhe Tao

University of California, Davis

Davis, CA 95616, USA

zhetao@ucdavis.edu

&Aditya V. Thakur

University of California, Davis

Davis, CA 95616, USA

avthakur@ucdavis.edu

###### Abstract

Ensuring that a DNN satisfies a desired property is critical when deploying DNNs in safety-critical applications. There are efficient methods that can verify whether a DNN satisfies a property, as seen in the annual DNN verification competition (VNN-COMP). However, the problem of provably editing a DNN to satisfy a property remains challenging. We present PREPARED,1 the first efficient technique for provable editing of DNNs. Given a DNN \(\) with parameters \(\), input polytope \(\), and output polytope \(\), PREPARED finds new parameters \(}\) such that \(\). \((;})\) while minimizing the changes \(\|}-\|\). Given a DNN and a property it violates from the VNN-COMP benchmarks, PREPARED is able to provably edit the DNN to satisfy this property within 45 seconds. PREPARED is efficient because it relaxes the NP-hard provable editing problem to solving a linear program. The key contribution is the novel notion of Parametric Linear Relaxation, which enables PREPARED to construct tight output bounds of the DNN that are parameterized by the new parameters \(}\). We demonstrate that PREPARED is more efficient and effective compared to prior DNN editing approaches **i)** using the VNN-COMP benchmarks, **ii)** by editing CIFAR10 and TinyImageNet image-recognition DNNs, and BERT sentiment-classification DNNs for local robustness, and **iii)** by training a DNN to model a geodynamics process and satisfy physics constraints.

## 1 Introduction

Ensuring that a DNN is correct and avoids harmful behaviors is critical when deploying them, especially in safety-critical contexts . In such scenarios, it is vital to guarantee that a DNN satisfies a given property, e.g., a safety specification . Towards this end, DNN verifiers aim to determine whether a DNN satisfies the given property . There are efficient methods for DNN verification that can handle a wide-range of DNN architectures and properties, as seen in the annual DNN verification competition (VNN-COMP) . However, the success of DNN verification reveals the next challenge: provably editing a DNN to satisfy a property. Formally, we define the provable editing problem as:

**Definition 1.1**.: Given a DNN \(\) with parameters \(\), an input polytope \(}}{{=}}\{ \}\) and an output polytope \(}}{{=}}\{ \}\), the **provable editing problem** is to find new parameters \(}\) that

\[\|}-\| \). \((;}) \]

Provable editing can be used, for instance, to ensure that a classifier DNN is locally robust for an input **x**. In this case, the input polytope \(}}{{=}}\{^{ }\|-^{}\|_{}\}\) is the \(L^{}\) ball around **x** withradius \(\), and the output polytope \(}}{{=}}\{_{j  l}_{j}<_{l}\}\) requires that the output label is \(l\); viz., \(=l\). Moreover, provable editing can be used either during training to guarantee that the DNN satisfies a property, or post-deployment to repair a given DNN.

The provable editing problem is challenging due to the presence of the universal quantifier; viz., finding new parameters such that _for all_ inputs in the polytope \(\), the output of the edited network lies in the polytope \(\). There are no existing approaches for efficiently and effectively solving the provable editing problem. Regularization-based approaches [26; 10; 28; 24] encode the property into the loss during training, but are unable to guarantee the property-satisfaction. SMT-based approaches [14; 16] provide guarantees, but are not efficient because they directly solve an NP-hard problem. Prior LP-based approaches [41; 42] are efficient because they can only handle a restricted class of provable editing problems; e.g., they assume that the vertices of the input polytope or the linear regions of the DNN can be efficiently enumerated, or the DNN architecture can be modified.

This paper presents PREPARED, the _first efficient_ technique for **provable editing** of DNNs that runs in **polynomial time** (Theorem 3.4). Given a DNN and a property it violates from the VNN-COMP'22 benchmarks, PREPARED is able to provably edit the DNN to satisfy this property within 45 seconds.

PREPARED approaches the problem of provable editing by constructing tight parametric bounds of the output \((;})\) for all inputs \(\) in the input polytope \(\)_in terms of the parameters_\(}\), and then constraining these parametric bounds to lie within the output polytope \(\). These parametric bounds are constructed compositionally per layer of the DNN, and are expressed as linear constraints, so that efficient Linear Programming (LP) solvers can be used to find an optimal solution.

Our _key insight_ is to represent the parametric bounds indirectly via _underapproximations of the epigraph and hypograph_ of the DNN layer. Consider the ReLU layer \(}}{{=}}( )\). Prior approaches [49; 39] overapproximate its upper bound with a linear function (where in Figure 1(a)) using constant input bounds \(,\). Such a linear relaxation is loose, and is only sound within the given constant input bounds \(,\). Our approach is able to _exactly_ represent the upper bound of ReLU output _for any_ input upper bound variable \(\) (where in Figure 1(b)). This is done by capturing the epigraph (where in Figure 1(c)) of \(()\) with a linear constraint.

Our _key contribution_ is a novel _Parametric Linear Relaxation_ for DNN layers, which defines tight parametric bounds using linear constraints in terms of the layer parameters. Consider \(}}{{=}}}\), which is a simplified representation of an Affine layer. We would like to construct bounds for all \(,\) in terms of the variable parameter \(}\). Prior approaches could achieve a sound, but loose linear relaxation that is not in terms of the layer parameters (Figure 2(a)). In contrast, our approach _exactly_ represents the output bounds without any relaxation (Figure 2(b)) building upon our key insight of capturing the epigraph and hypograph of the DNN layer _in terms of the parameters_ (Figure 2(c)).

We evaluate PREPARED **i)** using the VNN-COMP benchmarks, **ii)** by editing CIFAR10 and TinyImageNet image-recognition DNNs, and BERT sentiment-classification DNNs for local robustness, and **iii)** by training a DNN to model a geodynamics process and satisfy physics constraints. Because there were no prior efficient approaches for provable editing, we implemented new _provable fine-tuning_ baselines by integrating prior DNN editing approaches with a verifier in the loop. PREPARED outperformed such baselines demonstrating its effectiveness and efficiency.

## 2 Related work

**Regularized training**[20; 45; 30; 10; 19; 22; 47] incorporates constraints as regularization into the training loss, but does not guarantee constraint-satisfaction in the trained DNN. For the DNN editing problem we are addressing, which involves universally-quantified logical formulas, DL2  is the state-of-the-art regularized training method.

**Certified training**[26; 15; 36; 2; 31; 28; 24] is a type of regularized training geared towards adversarial robustness. SABR and STAPS are state-of-the-art certified training approaches. However, they also do not guarantee constraint-satisfaction.

**SMT-based editing approaches**[14; 23; 16] directly solve the NP-hard provable editing problem using an SMT solver. Thus, they are inefficient and do not scale beyond small DNNs. For instance, the recent DeepSaDe approach  incorporates an SMT solver during training, but only uses the SMT solver to edit the last layer. However, it still can take \(500\) longer than regularized training.

**LP-based editing approaches**[41; 42] relax the provable editing problem to solving an LP problem, which makes them efficient. However, prior approaches can only handle a restricted class of provable editing problems. The state-of-the-art provable editing approach APRNN is efficient for editing input points. However, it does not scale to the general provable editing problem of Definition 1.1 due to the need for enumerating vertices of the input polytope. PRDNN was the precursor to APRNN and suffers from similar limitations; moreover, PRDNN modifies the architecture of DNN and requires enumeration of linear regions to edit an input polytope. REASSURE repairs a linear region of a DNN by adding a patch DNN for each such linear region. Thus, it also requires enumeration of linear regions to edit an input polytope. Further, it has been shown to be unsound and incomplete (Section 5 of ), and suffers from significant runtime and memory overheads (Section 7.8 of ).

**DNN verification**[38; 39; 49; 44; 37; 48; 46; 9; 4] aims to determine whether a DNN satisfies a given property. As shown in Section 4, provable editing can be used when a verifier determines that a DNN violates a property. Most verification techniques use linear relaxations to bound the DNN output. However, these linear relaxations cannot be used for provable editing because they are not parameterized in terms of the DNN parameters. Instead, verification techniques assume that the parameters are constant and focus on designing linear relaxations for activation layers like ReLU [34; 43]. However, it is not clear how even these activation layer relaxations could be used for provable editing. For instance, these relaxations require constant input bounds; loose constant input bounds result in loose linear relaxations. In the context of provable editing, if the ReLU layer follows an Affine layer with editable parameters, then the constant input bounds to the ReLU layer would be very loose (e.g., \([-,]\)) in order to be sound for all possible edits. Hence, the resulting linear relaxation would also be loose. In contrast, our technique does not require constant input bounds for any layer, and produces an exact upper bound for the ReLU layer (Theorem 3.9). Applying our Parametric Linear Relaxation to DNN verification remains future work.

## 3 Provable editing of DNNs using Parametric Linear Relaxation

We use \(\) to denote a scalar, \(}^{m}\) to denote a column vector, and \(}^{n m}\) to denote a matrix. Blue-colored variables with a dot denote LP decision variables, e.g., \(}},}},}},}}\), etc. The proofs for all theorems can be found in Appendix A.

**Definition 3.1** (Dnn).: Let \(}}}{{=}}(};)\) denote an \(L\)-layer deep neural network (DNN) \(\) with parameters \(\), input \(}\) and output \(}\). The DNN is defined iteratively as \(}^{(+1)}}}{{=}}^{()}(}^{()};^{()})\) for each layer \(^{()}\) with parameters \(^{()}\), where \(}^{(0)}}}{{=}}}\), \(}}}{{=}}}^{(L)}\)and \(0<L\). \(\)

For ease of exposition, we defer our approach for the general provable editing problem of Definition 1.1 to Appendix B. In this section, we consider the following provable interval editing problem, where the input and output constraints are constant interval bounds:

**Definition 3.2**.: Given a DNN \(\) with parameters \(\), constant input bounds \([}},}}]\) and constant output bounds \([}},}}]\), the _provable interval editing problem_ is to find new parameters \(}\) that

\[\|}-\|} [}},}}].\;(};})[}},}}] \]

### Parametric Linear Relaxation

Consider a DNN \(\) with variable parameters \(}\) and input bounds \([}},}}]\), our goal is to construct sound parametric output bounds \([}},}}]\) of \(\) in terms of \(}\) and \([}},}}]\). The exact definition is

\[}}\{(};})\;|\;}}} }}\}}} \{(};})\;|\;}}}}}\} \]

However, this universally-quantified non-linear formula is expensive to solve [6; 7]. Our key contribution is a novel _Parametric Linear Relaxation_, a _poly-size linear formula_ that implies Equation 3.

**Definition 3.3**.: The _Parametric Linear Relaxation_\(}_{}(}},}},}},}};})\) for \(\) is a poly-size linear formula that implies \(}[}},}}]\). \((};})[}}, }}]\). \(\)

In other words, any satisfying assignment \((}},}},}},}},)\) of the formula \(}_{}(}},}},}},}};})\) yields sound output bounds \([}},}}]\) for \(\), viz., \(}[}},}}]\). \((};)[}},}}]\). The size of the formula \(}_{}(}},}},}},}};})\) is polynomial in the size of the DNN, i.e., the number of parameters, layers, and the input and output dimensions of each layer.

### Provable editing via Parametric Linear Relaxation

The following theorem shows how Parametric Linear Relaxation can be used to solve the provable interval editing problem.

**Theorem 3.4**.: _Given a provable interval editing problem (Definition 3.2) for DNN \(\) and parameters \(\) with input bounds \([},}]\) and output bounds \([},}]\), let \(_{}(}, },},};})\) be a Parametric Linear Relaxation for \(\). Then the following linear program can be solved in polynomial time in the size of the DNN \(\), and whose solution is a solution to the provable interval editing problem:_

\[\|}-\|_{}},}, {},};} }}}} \]

### Compositional construction of Parametric Linear Relaxation

**Definition 3.5**.: Given an \(L\)-layer DNN \(\) with parameters \(}\), constant input bounds \([},}]\) and Parametric Linear Relaxation \(}_{^{()}}\) for each layer \(^{()}\), we define the _compositional Parametric Linear Relaxation_\(}_{}}, },},};}\) as follows, where \([},}]}}{{=}}[}^{(L)},}^{(L)}]\) and \([}^{(0)},}^{(0)}]}}{{=}}[},}]\):

\[}_{}},},},};}}}{{=}}_{0<L} }_{^{()}}(}^{(+ 1)},}^{(+1)},}^{()};}^{()} \]

**Theorem 3.6**.: _Definition 3.5 is a Parametric Linear Relaxation for the DNN \(\)._

By Theorems 3.4 and 3.6, we see that to solve the provable editing problem, we just need to construct Parametric Linear Relaxations \(}_{^{()}}(}, },},};})\)_for each DNN layer \(^{()}\). In practice, \(}_{^{()}}(}, },},};})\) is defined by separate formulas \(}_{^{()}}(}, },};})\) and \(_{^{()}}(}, },};})\) for the upper and lower bounds, respectively: \(}_{^{()}}(}, },},};})}}{{=}}}_{^{()}}(},},};})_{ ^{()}}(},},};})\). For brevity, we describe how to construct Parametric Linear Relaxation for ReLU and Affine layers. Appendix C presents Parametric Linear Relaxations for Tanh, Sigmoid and ELU layers.

### Parametric Linear Relaxation for ReLU layer

**Definition 3.7**.: \(}}}{{=}}( )\) with input \(^{m}\) and output \(^{m}\) is defined as \(}}{{=}}\{,0\}\).

**Definition 3.8**.: For a ReLU layer with variable input bounds \([},}]\), its Parametric Linear Relaxation is defined as \(}_{}(},},},})}}{{=}}_{}(},}) _{}(},})\) where:

\[}_{}(},})}}{{=}}} }} 0_{}(},})}}{{=}}}= } \]

and \(^{m}\) are constants such that \(0_{i} 1\). \(\)

Because ReLU is a convex function, as seen in Figure 1(c), the upper bound constraint \(}_{i}}_{i}}_{i} 0\) for each \(}_{i}\) exactly captures the epigraph \( )}}}}}}}}}\))) of \((}_{i})\). For each lower bound \(}_{i}\), we use a linear relaxation \(}_{i}=_{i}}_{i}\) (\(},}]\) and variable parameters \(}\), \(}\), its Parametric Linear Relaxation is defined as \(_{}(},},},},},}) }}}{{=}}_{ }(},},},},})_{}(},},},},})\) where:

\[_{}(},}, },},},}) }}}{{=}}_{j}}_{j}=_{i}}_{ji}+}_{j}\ \ }}}\ \ }}} \] \[_{}(}, },},},}) }}}{{=}}_{j}}_{j}=_{i}}_{ji}+}_{j}\ \ }}}\ \ }}}\]

The core of this definition is constructing parametric bounds \([},}]\) for the multiplication \(}\) for all \([},}]\) in terms of the variable weight \(}\). As seen in Figure 2(c), for all \([},}]\), the upper bound () of \(}\) is defined by a piecewise-linear convex function \((})}}}{{=}}\{}},}}\}\). Hence, the upper bound constraint \(}}} }}}\)_exactly_ captures the epigraph () of \((})\). Similarly, the lower bound () of \(}\) is defined by a piecewise-linear concave function \((})}}}{{=}}\{}},}}\}\). Hence, the lower bound constraint \(}}} }}}\)_exactly_ captures the hypograph () of \((})\).

**Theorem 3.12**.: _Definition 3.11 is a Parametric Linear Relaxation for the Affine layer with constant input bounds \([},}]\) that captures the exact lower and upper bounds._

#### 3.5.2 Affine layer with variable input bounds

We freeze the parameter weight \(}\) to be constant \(\) (e.g., the original weight) when the input bounds are variable to avoid non-linearity.

**Definition 3.13**.: For an Affine layer with variable input bounds \([},}]\), constant weight \(\) and variable bias \(}\), its Parametric Linear Relaxation is defined as \(}_{}(},},},},,})}}}{{=}}_{}(},},},,}) _{}(},},},,})\) where:

\[_{}(},},},}) }}}{{=}}_{j}}_{j}=_{i}}_{ji}+}_{j} \] \[_{}(},},},,}) }}}{{=}}_{j}}_{j}=_{i}}_{ji}+}_{j}\]

Using constant weight \(\) avoids non-linearity due to the multiplication \(\) where \(\) is universally-quantified in variable bounds \([},}]\). Thus, the bounds \([},}]\) of \(\) for all \([},}]\) are determined by the sign of the constant \(\); viz., \([},}]}}}{{=}}[},}]\) if \( 0\), otherwise \([},}]}}}{{=}}[},}]\).

**Theorem 3.14**.: _Definition 3.13 is a Parametric Linear Relaxation for the Affine layer with variable input bounds \([}},}}]\) that captures the exact upper and lower bounds._

### Parametric Linear Relaxation for Tanh, Sigmoid and ELU layers

Our approach can also handle other activation layers like Tanh, Sigmoid and ELU. Figure 3 illustrates their Parametric Linear Relaxations with detailed description deferred to Appendix C.

### On scalability

As described in Appendix B, our approach can restrict the edits to only the last \(k\) layers of the DNN \(\), freezing the parameters of the first \(L-k\) layers. Consequently, the resulting Parametric Linear Relaxation is a linear formula whose size is polynomial in the size of _only the last \(k\) editable layers \(^{(k:L)}\)_instead of the entire DNN \(\). This flexibility enables our approach to scale to large DNNs and BERT transformers, as demonstrated in Section 4.

## 4 Experimental evaluation

We have implemented PREPARED in PyTorch and use Gurobi as the LP solver. We demonstrate the effectiveness and efficiency of PREPARED on different tasks that use a wide-range of DNN architectures and properties. All experiments were run on a machine with Dual Intel Xeon Silver 4216 Processor 16-Core 2.1GHz with 384 GB of memory, SSD and RTX-A6000 with 48 GB of GPU memory running Ubuntu 20.04. Additional details about these experiments are in Appendix D.

**Baselines.** Because there were no efficient prior approaches for provable editing, we implemented _provable fine-tuning_PFT(\(\)) baselines that combine prior (non-provable) DNN editing approaches with

Figure 4: Results of **(a)** single-property and **(b)** all-properties editing problems from VNN-COMP\({}^{}\)22.

a verifier in the loop: the editing stops when the verifier confirms that the DNN satisfies the property (Appendix D.1). We consider the state-of-the-art DNN editing approaches DL2, APRNIN, SABR and STAPS as introduced in Section 2. We use PFT(DL2) to denote the provable fine-tuning baseline instantiated with DL2. We use verifiers \(\),\(\)-CROWN, MN-BaB, or DeepT, depending on the task. To the best of our knowledge, ours is the _first_ to provide a comprehensive evaluation of such verifier-in-the-loop baselines for provable editing.

### Provable editing on VNN competition benchmarks

Setup.We compare PREPARED against PFT(DL2) and PFT(APRNN) on VNN-COMP22 benchmarks . The VNN-COMP22 benchmarks consist of DNNs along with one or more of their associated safety properties. For verification, the task would be to determine whether a DNN satisfies each of its properties. In the context of provable editing, we use these benchmarks in two scenarios: **i) Single-property editing**: Given a DNN and a property it violates, edit it to satisfy this single property with a time limit of 600 seconds. There are 423 such DNN-property instances. **ii) All-properties editing**: Given a DNN that violates at least one of its associated properties, edit it to satisfy the conjunction of all (satisfied or violated) properties associated with it with a time limit of 3600 seconds. There are 66 such DNN-property instances.

PFT(SABR) and PFT(STAPS) are not used because they do not handle all properties and DNN architectures in this experiment. In particular, SABR and STAPS have not discussed how to handle general logical formula with disjunctions, and their current implementations are designed for local robustness training. PFT(SABR) and PFT(STAPS) are used for local robustness editing in Section 4.2.

Metrics.The **effectiveness** is measured using the number of provably edited instances, and the **efficiency** is measured using the runtime. We were unable to determine the impact on the predictive performance (accuracy), because VNN-COMP does not come with such evaluation metrics and data.

Results.As shown in Figure 4, PREPARED_is the best provable editing approach_, significantly outperforming PFT(DL2) and PFT(APRNN) in terms of both _effectiveness_ and _efficiency_. As shown in the cactus plots (**Left** in Figures 4(a) and 4(b)), PREPARED can provably edit more instances in less time. As shown in the speed-up plots (**Middle** and **Right** in Figures 4(a) and 4(b)), PREPARED completes most problems in 10 seconds, taking a maximum of 45 seconds for single-property editing; it achieves from 10x to more than 100x speed-up over PFT(DL2) and PFT(APRNN). Specifically, PREPARED succeeds on all 423 single-property editing problems, while PFT(DL2) and PFT(APRNN) succeed on 185 and 117 problems, respectively. PREPARED succeeds on 62 out of 66 multi-property editing problems, while PFT(DL2) and PFT(APRNN) only succeed on 45 and 48 problems, respectively.

### Local robustness editing for image-recognition DNNs

**Definition 4.1**.: Consider a classifier DNN \(:^{m}^{n}\) and a dataset \(\) of input-label pairs \((,l)^{m}\). The DNN \(\)_correctly classifies_ an input-label pair \((,l)\) iff \(*{arg\,max}()=l\), and the _standard accuracy_ of a DNN on a dataset \(\) is the percentage of correctly classified input-label pairs in \(\). Given a perturbation \(\), the DNN \(\) is _\(L^{}\)-locally-robust_ on an input-label pair \((,l)\) iff \(\) correctly classifies all perturbed inputs \(^{}\) in the \(L^{}\) box centered at \(\) with perturbation \(\), viz., \(^{}^{m}\). \(\|^{}-\|_{} *{arg\,max}(^{})=l\). The _certified accuracy_ of a DNN on a dataset \(\) is the percentage of \(L^{}\)-locally-robust input-label pairs in \(\).

Setup.We compare PREPARED against PFT(DL2), PFT(SABR), and APRNIN on \(L^{}\)-local-robustness editing for image-recognition DNNs. We edit CNN7 DNNs for CIFAR10 and TinyImageNet, trained in prior work , to be locally-robust for images in the edit set. The CIFAR10 DNN has 17.2M parameters, 79.24% standard accuracy and 75.77% certified accuracy (\(=0.5\)/255). The TinyImageNet DNN has 51.9M parameters, 28.85% standard accuracy and 20.46% certified accuracy (\(=1\)/255). For CIFAR10 and TinyImageNet, the edit sets consist of 50 misclassified fog-corrupted images from CIFAR10-C and TinyImageNet-C, respectively. The generalization sets consist of 200 variant images with different corruption-levels, four variants per image in the edit set. We use MN-BaB to compute the certified accuracy.

Metrics.**Efficacy** is measured using the certified accuracy (Definition 4.1) on the edit set. The original certified accuracy on the edit set is 0%. Efficacy is the most important metric: _a provable edit must guarantee 100% efficacy._ The **standard accuracy** and **certified accuracy** on the full test set are also important metrics: a good provable edit should have high accuracy. However, those accuracy metrics are relevant only if efficacy is 100%. The **generalization** accuracy on the generalization set measures how well the edit generalizes to inputs that are similar to the edit set. However, the generalization accuracy is relevant only if the efficacy is 100% and the standard and certified accuracies are good; that is, the predictive power of the edited DNN should not be sacrificed for better generalization. The **runtime** metric measures the time taken to edit the DNN.

Results.As shown in Table 1, PREPARED_is overall the best approach_. PREPARED achieves the best standard and certified accuracy, good generalization and short runtime. In particular, PFT(SABR) and PFT(STAPS) have significantly lower standard and certified accuracy; PFT(DL2) takes significantly longer time, and was unable to achieve 100% efficacy in four hours in the TinyImageNet experiment; APRIN was unable to achieve 100% efficacy.

### Local robustness editing for sentiment classification BERT transformers

Setup.We compare PREPARED against PFT(DL2) and APRNN on provable \(L^{}\) local-robustness editing for BERT sentiment-classification transformers. We conduct this experiment on the Stanford Sentiment Treebank (SST)  DNNs. We use DeepT  as the verifier in this experiment. We take the "wider" 12-layer BERT transformer used in the DeepT paper . The standard accuracy of this network is \(84.07\%\). Because DeepT cannot handle all sentences in the SST dataset with arbitrary \(\), we construct two editing sets from the SST validation set: all 66 verifiable sentences with \(=1\)e-4, where 60 of them are certified to be locally-robust; all 26 verifiable sentences with \(=5\)e-4, where 24 of them are certified to be locally-robust. PFT(SABR) and PFT(STAPS) are not compared in this experiment because SABR and STAPS do not handle the BERT transformer architecture.

Metrics.**Efficacy** is measured using the certified accuracy (Definition 4.1) on the edit set. Because DeepT is incomplete, we can only compute a lower bound of the efficacy. Efficacy is the most important metric: _a provable edit must guarantee 100% efficacy._ The original efficacy (Og.ffic.) is also presented in Table 2. The **standard accuracy** on the full test set is also an important metric: a good provable edit should have high accuracy. However, the standard accuracy is relevant only if efficacy is 100%. The **runtime** metric measures the time taken to edit the DNN.

Results.As seen in Table 2, PREPARED is the only approach that achieves 100% efficacy in this task. PREPARED also achieves good accuracy and short runtime. Both PFT(DL2) and APRNN were unable to achieve 100% efficacy, and decreased the efficacy in most experiments.

    &  &  \\   & Effic. & Acc. & Cert. & Gen. & Time & Effic. & Acc. & Cert. & Gen. & Time \\  PFT(DL2) & 50/60 & 73.59\% & 70.42\% & 197/200 & 4780s & 47/60 & 22.83\% & 15.06\% & 199/200 & 14400s\({}^{*}\) \\ PFT(SABR) & 50/60 & 57.10\% & 52.72\% & **200/200** & 38s & 50/60 & 20.24\% & 13.38\% & **197/200** & 230s \\ PFT(STAPS) & 50/60 & 57.38\% & 54.26\% & 197/200 & **15s** & 50/60 & 20.84\% & 13.14\% & 191/200 & **112s** \\ APRIN & 50/60 & 78.96\% & 75.14\% & 113/200 & 20s & 12/60 & 28.75\% & 18.61\% & 115/200 & 417s \\ PREPARED & 50/60 & **75.25\%** & **71.45\%** & 189/200 & 88s & 50/60 & **28.16\%** & **16.80\%** & 149/200 & 663s \\   ^{*}\) Timeout in four hours.} \\ 

Table 1: Local robustness editing for image-recognition DNNs. Comparison of efficacy (Effic.), standard (Acc.), certified (Cert.) and generalization (Gen.) accuracy. Rows with <100% efficacy are shaded. Best results are **highlighted**.

    &  &  \\   & Og. & Effic. & Effic. & Acc. & Time & Og. & Effic. & Effic. & Acc. & Time \\  PFT(DL2) & & 52/66 & 82.97\% & 12108s & & 23/26 & 79.67\% & 5610s \\ APRNN & **60/66** & 61/66 & 83.47\% & 6s & 29/26 & 23/26 & 81.32\% & 5s \\ PREPARED & **60/66** & **83.52\%** & **981s** & & **26/26** & **82.42\%** & **326s** \\   

Table 2: Local robustness editing for sentiment-classification transformers. Comparison of the lower bound of efficacy before (Og.ffic.) and after edit (Effic.), as well as the stand accuracy (Acc.). Rows with <100% efficacy are shaded. Best results are **highlighted**.

### Provable training for physics-plausible DNNs

**Numerical model.** We consider a classic model of a buoyancy-driven mantle flow with a central circular plume , which is an incompressible flow with variable viscosity. The state of the system is a vector field \(}}{{=}}(u,v,,)\) of velocity field \(}}{{=}}(u,v)\), viscosity \(\) and density \(\), over space \((x,y)\) and time \(t\). We consider the following two physics constraints: **i) Conservation of mass \(=0\)** from the **continuity** equation; **ii) Dirichlet boundary condition \(=0\)** on the boundary \(\), where \(\) is the outward normal of \(\).

**Setup.** We compare GD(PREPARED), a combination of gradient-descent (GD) and PREPARED, against DL2, GD and GD(APRNH) on the task of provably training a DNN to model the geodynamics process and satisfy physics constraints. We discretize the space into a 31x31 grid and implement the numerical model to generate data for 150 time steps. We evenly split the data into training, validation and test sets. We train a ReLU ResNet with one single residual block of four conv2d layers with 32 channels and 3x3 kernel size. For DL2, we add the physics constraints as an unsupervised-learning regularization to the supervised training. For GD(APRNH) and GD(PREPARED), we first use vanilla gradient-descent (GD) to train a base model, then edit it with the constraints. Specifically, GD(APRNH) edits all training points, and GD(PREPARED) edits the convex-hull of all training points to satisfy the constraints.

**Metrics.** The **relative error** on the test set, the constraint-satisfaction errors, viz., **continuity error**, for the conservation of mass, and **boundary-condition error**, and the **runtime** are the metrics.

**Results.** As shown in Table 3, GD(PREPARED) _is overall the best approach._ GD(PREPARED) has good relative error, and its constraint-satisfaction error is significantly better than other approaches. Notably, its continuity error is at the same magnitude as the reference data, close to the theoretical value, zero, and is \(10^{5}\) to \(10^{8}\) times better than other approaches. GD achieves the best relative error and the shortest runtime. DL2 has the worst errors and takes significantly longer amount of time.

## 5 Conclusion

We have presented PREPARED, the first efficient approach for provable editing of DNNs that runs in polynomial time. We presented novel methods for constructing tight parametric bounds for the DNN output using Parametric Linear Relaxation, enabling PREPARED to use an LP solver to find an edit. To demonstrate its effectiveness and efficiency, PREPARED was used to provably edit DNNs and properties from the VNN-COMP\({}^{}\)22 benchmark, provably edit CIFAR10 and TinyImageNet image-recognition DNNs, and BERT sentiment-classification DNNs for local robustness, and provably train a geodynamics DNN to satisfy physics constraints.

**Societal impact.** PREPARED represents a step towards the goal of ensuring safety, trustworthiness, and reliability of DNNs, which is crucial given their use in autonomous safety-critical systems. However, being a general technique for editing DNNs, it could be used to remove or add malicious behavior such as bias, backdoor attacks, etc.

**Limitations.** PREPARED requires as input a property that we wish the DNN to satisfy. These properties would need to formally encode the notions of safety, reliability, or trustworthiness, which may not always be possible or easy to do. e.g., defining safety properties for LLM-based chatbots. However, the machine learning and formal methods communities are making progress towards this goal; e.g., developing proxy specifications for complex properties that are easier to define and learning safety specifications from data .

   Method & Rel. Err. & Cont. Err. & BC Err. & Time \\  Ref. & — & \(1.42 10^{-12}\) & \(1.74 10^{-12}\) & — \\ DL2 & \(1.59\)\% & \(1.58 10^{-4}\) & \(5.08\) & \(14\,674\)s \\ GD & \(\) & \(1.59 10^{-5}\) & \(5.86 10^{-1}\) & **395s** \\ GD(APRNH) & \(0.50\)\% & \(5.57 10^{-7}\) & \(4.96 10^{-5}\) & \(570\)s \\ GD(PREPARED) & \(0.50\)\% & \(}\) & \(}\) & \(684\)s \\   

Table 3: Global physics property repair for geodynamics DNN. Comparison of relative error (Rel. Err.), continuity (Cont. Err.) and boundary-condition (BC Err.) errors. Errors on the test set (Ref.) are shaded. Best results are **highlighted**.