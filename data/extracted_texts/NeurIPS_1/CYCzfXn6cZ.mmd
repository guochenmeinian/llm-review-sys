# Survival Permanental Processes for

Survival Analysis with Time-Varying Covariates

Hideaki Kim

NTT Human Informatics Laboratories

NTT Corporation

hideaki.kin@ntt.com

###### Abstract

Survival or time-to-event data with time-varying covariates are common in practice, and exploring the non-stationarity in covariates is essential to accurately analyzing the nonlinear dependence of time-to-event outcomes on covariates. Traditional survival analysis methods such as Cox proportional hazards model have been extended to address the time-varying covariates through a counting process formulation, although sophisticated machine learning methods that can accommodate time-varying covariates have been limited. In this paper, we propose a non-parametric Bayesian survival model to analyze the nonlinear dependence of time-to-event outcomes on time-varying covariates. We focus on a computationally feasible Cox process called _permanental process_, which assumes the square root of hazard function to be generated from a Gaussian process, and tailor it for survival data with time-varying covariates. We verify that the proposed model holds with the representer theorem, a beneficial property for functional analysis, which offers us a fast Bayesian estimation algorithm that scales linearly with the number of observed events without relying on Markov Chain Monte Carlo computation. We evaluate our algorithm on synthetic and real-world data, and show that it achieves comparable predictive accuracy while being tens to hundreds of times faster than state-of-the-art methods.

## 1 Introduction

Survival or time-to-event data analysis has been widely applied for analyzing the dependence of survival time, the time until an event occurs, on covariates. They have a wide ranging list of applications in reliability engineering , finance , marketing , and especially, clinical research .

An essential part of the survival data analysis is to estimate a _hazard function_, that is, the instantaneous probability of events occurring at any particular time, as a regression function of covariates. Given a hazard function, we can evaluate the impact of covariates on survival times, and assess the degree of risk of experiencing an event in the future, through the survival function, i.e., the probability of no events occurring during a specified interval. The literature contains a vast number of studies that have proposed survival models to estimate hazard functions, most of which assume that covariates are time-invariant (e.g., gender and age at the time of diagnosis in clinical research): they range from the classical Cox proportional hazards model  to modern machine learning models based on generalized boosting machines , random forests , Gaussian processes , and deep neural networks . However, time-varying covariates are common in survival data, and the rapid advances in data collection allow us to access long-term and high temporal resolution covariate data. This is encouraging researchers to explore the non-stationarity in covariates for the accurate estimation/prediction of time-to-event outcomes. An important application of survival analysis with time-varying covariates is what-if analysis: through simulations of future events under various covariate functions over time, we can find the optimal covariate function or policy that would yield a desirable future state. For example, in reliability engineering applications where the events are the failures of machines, time-varying covariates could be the maintenance schedule and the room temperature/humidity controlled by air conditioners. The maintenance manager can optimize the schedules of maintenance and air conditioning by balancing the risk of failure and the costs of maintenance and air conditioning. If survival models for static covariates were applied to non-stationary data, then the static survival models would fail to estimate the underlying dependence of the hazard function on covariates, resulting in unreliable decision-making. Therefore, survival models that accommodate time-varying covariates are needed. This paper assesses survival data with time-varying covariates to estimate a hazard function that takes covariates into account.

The standard survival model is the Cox proportional hazards model [5; 6; 40] (CoxPH), which forms the logarithm of the hazard function as a linear combination of covariates. The original CoxPH assumes that covariates are time-invariant and have a constant log-linear effect over time on the hazard function. It was extended to address time-varying covariates through the technique of counting process formulation . Although the extended CoxPH is very efficient to compute, it fails to address the nonlinear dependence of survival times on covariates, which is often the case in real-world data. To rectify this limitation, two non-parametric machine learning models have been proposed: generalized boosted models  and random forest-based models [47; 48], both of which adopt the extended CoxPH as a learner/tree, and construct non-parametric predictors in an ensemble manner. They succeeded in addressing the nonlinear dependence of survival times on time-varying covariates, but the large number of trees needed for accurate predictions tend to make the algorithm too slow and ineffective for real-time applications.

In this paper, we propose a novel Bayesian survival model to estimate a hazard function as a nonlinear regression function of covariates from survival data with time-varying covariates. To construct a scalable algorithm, we utilize _permanental process_, a doubly-stochastic point process which assumes the square root of hazard function to be generated from a Gaussian process; its computational advantages have been highlighted by recent studies [12; 18; 24; 30; 45]. We tailor it for survival data in the counting process format, and show that the tailored process, which we call the _survival permanental process_, holds the representer theorem : the maximum a posteriori estimator of the latent function can be represented as a finite linear combination of a transformed kernel product evaluated on the event time points. The representer theorem offers us a feasible Bayesian estimation algorithm that scales linearly with the number of events observed without relying on Markov Chain Monte Carlo computation. Furthermore, we derive the predictive distribution and the marginal likelihood in one feasible form, enabling us to implement the uncertainty evaluation and the hyper-parameter optimization in a Bayesian manner. To the best of our knowledge, it is the first time to exploit the representer theorem for survival data analysis. We provide python codes to reproduce the results in this paper1.

In Section 2, we introduce the survival permanental process (SurvPP) and construct a scalable Bayesian estimation algorithm for it. In Section 3, we outline related work. In Section 4, we compare SurvPP with reference survival models on synthetic and real-world data, and confirm that our algorithm achieves comparable predictive accuracy while being tens to hundreds of times faster than state-of-the-art survival models. Finally, Section 5 states our conclusions.

## 2 Methods

### Survival Permanental Processes

We assume that a right-censored dataset with time-varying covariates, \(=\{(_{u}(t),T_{u},_{u})\}_{u=1}^{U}\), is observed, where \(_{u}(t):(0,T_{u}]^{d}\), \(T_{u}\), and \(_{u}\{0,1\}\) are the map of \(d\)-dimensional covariate, the end time of observation, and the indicator that represents whether an individual experienced an event (\(_{u}=1\)) or was right-censored (\(_{u}=0\)) at \(T_{u}\), for individual \(u\{1,2,,U\}\), respectively. We consider tailoring a permanental process for the survival data, where latent function, \(x():\), is generated from a Gaussian process (GP) on covariate space \(\), and an event time for each individual \(u\) is generated from a point process with hazard function, \(_{u}(t)\)such that

\[p(x()|)=|x())\:(x()|k)} {x()\:p(|x())\:(x()|k)}, \]

\[ p(|x())=_{u=1}^{U}_{u}_{u}(T _{u})-_{0}^{T_{u}}_{u}(t)dt,_{u}(t)=x^{2}(_{u}(t)), \]

where \((x()|k)\) represents a GP with kernel function \(k(,^{})\), \(p(|x())\) is the likelihood function of the point process, while \(x()\) in the denominator represents the integral over the function or the infinite-dimensional variable \(x()\). We call the model defined by (1-2) the _survival permanental process_ (SurvPP). Hazard function \(_{u}()\) represents an instantaneous probability of events occurring at each point in time, and our goal is to estimate the functional form of the hazard function over covariate domain, \(x^{2}()\), from the survival data \(\).

Given a hazard function over covariate domain, \(x^{2}()\), and a covariate map for individual \(u\), \(_{y}(t)\), we can evaluate the survival function and the probability density distribution of event time at arbitrary time point \(t\), denoted by \(S(t)\) and \(P_{e}(t)\), respectively, as follows:

\[S(t)=-_{0}^{t}x^{2}(_{u}(s))ds, P_{e}(t)= x^{2}(_{u}(t))-_{0}^{t}x^{2}(_{u}(s))ds. \]

By using (3), we can perform survival analyses that include the analysis of the expected duration of event time, and the assessment of the degree of risk that an individual will experience the event before a specified period.

### Counting Process Format of Data

Traditionally, survival data with time-varying covariates have taken the _counting process format_. The format assumes that, for each individual \(u\), covariates are measured at \(J_{u}\) finite representative time points and can be regarded as constant between successive time points,

\[_{u}(t)=_{u}^{j}, t(s_{u}^{j},\:s_{u}^{j+1}], j=0, ,J_{u}-1, \]

where \((s_{u}^{0},s_{u}^{J_{u}})=(0,T_{u})\). It then splits individual \(u\)'s record, \((_{u}(t),T_{u},_{u})\), into \(J_{u}\) pseudo-individual records as in

\[\{(s_{u}^{j},\:s_{u}^{j+1},\:_{u}^{j},\:_{u}^{j})\}_{j=0}^{J_{u}-1}, _{u}^{j}=_{u}(j=J_{u}-1), \]

where \(()\) represents the indicator. Joining the pseudo-individual records of \(U\) individuals together results in the counting process format of data:

\[=\{(T_{j}^{0},\:T_{j}^{1},\:_{j},\:_{j})\}_{j=1}^{J}, J =_{u=1}^{U}J_{u}. \]

In this paper, we employ the counting process format of (6), and rewrite the likelihood function of SurvPP (2) as

\[ p(|x())=_{j=1}^{J}_{j} x^{2}(_{ j})-(T_{j}^{1}-T_{j}^{0})x^{2}(_{j})=_{n=1}^{N} x^{2}( }_{n})-_{j=1}^{J}_{j}x^{2}(_{j}) \]

where \(_{j}\) and \(\{}_{n}\}_{n=1}^{N}\) represent the durations of observation and the covariates observed at event times, respectively:

\[_{j}=T_{j}^{1}-T_{j}^{0},\{}_{n}\}_{n=1}^{N}=\{_ {j}|_{j}=1,1 j J\}, N=_{j=1}^{J}_{j}=_{u=1}^{U} _{u}. \]

Note that the number of events, \(N\), is usually much smaller than the number of pseudo-individual records: \(N J\).

In practice, there are two ways of defining the representative time points \(\{s_{u}^{j}\}_{j=0}^{J_{u}}\): one is as the points that were observed, which might be sparse over time due to measurement constraints; the other is as the denser points obtained by using interpolation methods (e.g., splines). This paper assumes the latter, and thus various values of \(J_{u}\) were considered in the synthetic data experiment (see Section 4). Although the counting process format of input assumes piecewise stationarity in covariates, this assumption is not so strong because we can adopt the representative time points in any density if necessary.

### Maximum _A Posteriori_ Estimator

We consider the problem of obtaining the maximum _a posteriori_ (MAP) estimator of \(x()\), denoted by \(()\), to maximize the posterior probability (1). We derive \(()\) through the approach of using the path integral representation of GP ,

\[(x()|k)\ x()=|}} -_{}\!\!k^{*}(, ^{})x()x(^{})dd^{}\ x(), \]

where \(\) and \(_{}k^{*}(,^{}) d^{}\) are the integral operator with kernel function \(k(,^{})\) and its inverse operator, respectively: \(^{(*)}x()=_{}k^{(*)}(,^{})x (^{})d^{}\), \(k^{*}(,^{})=(-^{})\). Here \(||\) represents the function determinant  of \(\), defined by the product of its eigenvalues . Using the representation of (9), we write the posterior of SurvPP (1-2) in the following functional form,

\[p(x()|)x=)}-S x(),()-|| x, \]

where \(Sx(),()\) is the _action integral_, defined by

\[Sx(),()=_{}\!\![ x()()+_{j=1}^{J}x^{2}()_{j }(-_{j})-2_{n=1}^{N}|x()|(- }_{n})]\!d, \]

and \(()=_{}k^{*}(,^{})x(^ {})d^{}\). Then we apply calculus of variations to the action integral, where the functional derivative of \(S(x(),())\) on the MAP estimator \(()\) should be equal to zero: \(()}()+()}()=0\), which results in the exact MAP estimator,

\[()=2_{n=1}^{N}h(,}_{n})v_{n}, v_{n}= (}_{n})^{-1}, \]

where \(h(,^{})\) is a transformed kernel function that solves a discretized Fredholm integral equation of the second kind ,

\[h(,^{})+2_{j=1}^{J}k(,_{j})_{j}h(_{j},^{})=k(,^{}). \]

See Appendix A for the detailed derivations. Equation (12) shows that the MAP estimator of SurvPP involves the representer theorem under the transformed kernel function \(h(,^{})\), and thus the Bayesian estimation reduces to a finite-dimensional optimization problem. Here, we call \(h(,^{})\) the equivalent kernel following studies by Flaxman et al. , Walder & Bishop , and Kim et al. . Given the equivalent kernel, the unknown coefficients \(v_{n}\) in (12) solve the simultaneous quadratic equations derived from (12),

\[r_{n}\  2\ v_{n}_{n^{}=1}^{N}h(}_{n},}_{n^{}})v_{n^{}}-1=0, n=1,2,,N. \]

In this paper, we estimate a set of coefficients, \(\{v_{n}\}_{n=1}^{N}\), by solving a minimization problem of the mean of the squared residuals, \(_{n=1}^{N}|r_{n}|^{2}/N\), with a popular gradient descent algorithm, _Adam_.

### Equivalent Kernels

The equivalent kernel \(h(,^{})\) solves the discretized Fredholm integral equation (13). When the kernel function of GP has degenerate form with rank \(M(<)\) such that

\[k(,^{})=_{m=1}^{M}_{m}()_{m}(^{ })=()^{}(^{}), \]it is easily shown that the discretized Fredholm integral equation (13) can be solved analytically [2; 24] as,

\[h(,^{})=()^{}(_{M}+2)^{-1}(^{}),=_{j=1}^{J}_{j}(_{j })(_{j})^{}, \]

where \(_{M}\) is the \(M M\) identity matrix, and \(()=(_{1}(),_{2}(),,_{M}())^ {}\). Note that \(\) is defined by a sum of outer products, which can be rewritten through a matrix-matrix multiplication as

\[=^{},=}(_{1}),,}(_{J}). \]

Empirically, implementation by matrix-matrix multiplication (17) is substantially faster than the sum of outer products when \(J 1\).

When \(k(,^{})\) has degenerate form with rank \(M\), the relation (16) shows that the equivalent kernel \(h(,^{})\) also has degenerate form obtained through Cholesky decomposition:

\[h(,^{})=(())^{}((^{})),^{}=(_{M}+2)^{-1}. \]

The degenerate equivalent kernel (18) offers fast Bayesian estimation that scales linearly with \(N\) (see Section 2.6). In this paper, we used the random feature map [37; 42; 43] of a Gaussian kernel to obtain a degenerate form of kernel (\(M\)\(=\)\(500\)).

If the covariate map, \(_{u}(t)\), can be assumed to be smooth enough over time, a more accurate evaluation of \(\) than equation (16) is possible (see Appendix D.1), but this was not exploited in the main experiments.

### Predictive Distribution and Marginal Likelihood

One of the advantages of GP models over non-Bayesian approaches is that they can provide predictive distributions and marginal likelihoods, which enable us to perform uncertainty evaluations, hyper-parameter optimization, and model selection in Bayesian manner. Following the methodology with the path integral representation of GP [23; 24], SurvPP (1-2) adopts a Laplace approximation in the functional space, and finds the approximate form of the predictive distribution and the marginal likelihood. We only show the results due to space limitations. For details, see Appendix B.

The marginal likelihood, \(p()\), is given as

\[ p()=||-|_{N}+^{-1}| -|_{M}+2|^{-1}+( 2-1)N, \]

where \(_{N}\) is the identity matrix with size \(N\), and

\[_{nn^{}}=(2v_{n}^{2})^{-1}_{nn^{}},\ \ _{nn^{ }}=h(}_{n},}_{n^{}}),\ \ ()=(h( ,}_{1}),,h(,}_{N}))^{}. \]

The predictive distribution of intensity function on a covariate value, \(p(=x^{2}())\), is given as

\[p_{,}()=}^{-1}(- /),=2^{2}+}{^{2}+},\ =^{2}+)^{2}}{2(2^{2}+)}, \]

where \(\) is the abbreviation of \(()\), and the predictive variance, \(\), is defined by \(=h(,)-()^{}(+)^{-1}()\). (21) represents the predictive distribution of the hazard function as a regression function of covariates. It is worth noting that the posterior process of SurvPP is given by a permanental process, and we can evaluate the distribution of the survival function and perform a risk-aware survival analysis by generating random samples of the estimated hazard function of covariates, \(()\).

### Computational Complexity

The computational complexity of evaluating the equivalent kernel (16) for covariate pair \((,^{})\) is \((M^{3}+dJM+JM^{2})\), where \((dJM)\) and \((JM^{2})\) come from the computation of feature maps, \(_{m}(_{j})\), and the sum of outer products (17), respectively.

When the equivalent kernel is given in degenerate form with rank \(M\) (\(<N\)) such that \(h(,^{})=_{m=1}^{M}_{m}()_{m}(^{ })\), the objective function to be minimized in MAP estimation, \(_{n=1}^{N}|r_{n}|^{2}/N\)incurs a linear computation with the number of observed events \(N\), that is, \((NM)\), for each evaluation in gradient descent algorithms: The vector of residual, \(=(r_{1},,r_{N})^{}\), can be expressed as \(((^{}))(2)-\), where \(\) is the \(N M\) matrix defined by \(_{nm}=_{m}(_{n})\), and \(\) represents the Hadamard product. \(\) costs \((dNM)\).

Given an equivalent kernel with degenerate form, the evaluation of the predictive variance, \(=h(,)-()^{}(+)^{-1}()\), needs the computation of \((M^{3}+NM^{2})\): \(N N\) matrix \(\) can be decomposed into a product of \(N M\) matrix \(\) and its transpose as \(=^{}\). The matrix inversion is transformed as \((+^{})^{-1}=^{-1}-^{-1}(_{M}+ ^{}^{-1})^{-1}^{}^{-1}\) through the Woodbury matrix identity, which costs \((M^{3}+NM^{2})\). Note that \(^{-1}\) and \(^{}^{-1}\) cost \((NM)\) because \(\) is a diagonal matrix.

In computing the marginal likelihood (19), the complexities of the first, the second, and the third terms are \((N)\), \((M^{3}+NM^{2})\), and \((M^{3}+JM^{2})\), respectively, where the matrix determinant lemma is used in computing the second term.

In total, the computational complexity of SurvPP is \((NMQ+(N+J)(d+M)M+M^{3})\), where \(Q\) is the number of gradient descent iterations. This feasible computation, which scales linearly with the number of observed events, \(N\), the data size, \(J\), and the covariate dimensionality, \(d\), is achieved by exploiting the representer theorem. It should be emphasized here that data size \(J\) is not part of the gradient descent iteration term. This is a clear advantage over conventional survival models which naively incur \(((dN+dJ+JN)Q)\) computation: Typically, \(J\) is larger than \(N\) in a time-varying covariate scenario (see the counting process format in Section 2.2), and the discrepancy between \(J\) and \(N\) becomes more substantial when the measurements of covariates are made more frequently or/and over a longer period of time.

## 3 Related Work

**Survival Models for Time-invariant Covariates:** The most popular survival model is the Cox proportional hazards model  (CoxPH), which is a semi-parametric model that forms the hazard function as

\[(t)=h(t)(_{1}y_{1}+_{2}y_{2}+), \]

where the base hazard function, \(h(t)\), is obtained by the non-parametric Breslow/Fleming-Harrington estimator , and the log-linear regression coefficient \((_{1},,)\) is estimated by maximizing the partial likelihood function. CoxPH is very efficient to compute and scales linearly with the number of events , but cannot address the nonlinear dependence of survival times on covariates. To overcome this limitation, a vast number of survival models have been proposed that replace the log-linear parametric function (22) with a non-linear one, such as generalized boosted models , random survival forests , Gaussian process models , and deep neural network models . For a comprehensive review, see Wang et al. .

**Survival Models for Time-varying Covariates:** The counting process format of input  plays a central role in extending static survival models into those suitable for time-varying covariates. For each individual, the counting process format splits her/his observation period into multiple short intervals, assigns a constant value of covariate to each interval, and marks the intervals as being censored when no event is present, resulting in a set of pseudo-individual right/left-censored observations with a constant covariate (see Section 2.2). CoxPH was extended to accommodate the counting process format, and the extended CoxPH can estimate the hazard function based on survival data with time-varying covariates . To alleviate the simplest assumption in the extended CoxPH, generalized boosted models  and random survival forests , both of which are ensemble approaches, have been also extended to accommodate the counting process format, where the extended CoxPH is employed as a weak learner or a tree. While they can address the nonlinear dependence of survival times on time-varying covariates, the large number of learners/trees needed for accurate predictions could make the algorithms too slow and ineffective for real-time applications. Also, Cygu et al. reported that random survival forests for time-varying covariates required substantial amounts of computer memory for large datasets .

**Exogeneity and Endogeneity of Covariates:** When considering survival analysis in the presence of time-varying covariates, we need to distinguish between exogenous and endogenous covariates. Kalbfleisch and Prentice  define an endogenous (internal) covariate as the output of a stochastic process that is generated by the individual under study. In contrast, an exogenous (external) covariate is not influenced by the individual under study. Diggle et al.  suggest using similar but slightly different definitions. The primary purpose of survival analysis with exogenous covariates is to estimate a hazard function as an explicit function of covariates, and to make predictions of failure times under various possible _future_ covariate functions (i.e., what-if analysis). In contrast, the primary purpose of survival analysis with endogenous covariates is to make predictions of failure times by using the _past_ observations of covariates, where joint modeling approaches with recurrent neural networks, which jointly model the stochastic process of covariates and failure times, have been developed intensively . In this paper, we consider exogenous covariates, and survival models for exogenous covariates cannot be compared directly with those for endogenous covariates because the tasks are different. Note that a joint modeling approach for exogenous time-varying covariates has been proposed very recently, which was not included in the benchmark models because it was published a month after this paper's submission.

**Permanental Process:** The permanental process is a variant of Gaussian Cox process that assumes the square root of hazard function to be generated from a Gaussian process . Its computational advantages have recently been highlighted in machine learning research . In particular, the representer theorem in the permanental process has been exploited through the RKHS theory , the Mercer's theorem , and the path integral formulation . Although the key derivation of SurvPP is based on the path integral methodology used for augmented permanental process (APP) , SurvPP is a non-trivial extension of APP: (i) SurvPP can accommodate multiple trials of event sequence data, while APP assumes one trial of ever sequence; (ii) in SurvPP, the end time of observation is a stochastic variable that depends on the time of event occurrence, while APP assumes that the end time of observation is given. We discovered, for the first time, that the representer theorem holds for such a complicated point process or a survival model.

## 4 Experiments

We examined the validity of our proposed model by comparing it with conventional survival models on synthetic and real-world data. As benchmark models, we adopted Cox proportional hazards model (CoxPH), generalized boosted model (GBM), and random forest-based model (RFM): We implemented CoxPH and GBM with the established algorithms provided in the packages survival.coxph  and gbm3.gbmt , respectively; We implemented RFM by using class ltrcrrf in the open R code provided by Yao et al. . The benchmark models can accommodate time-varying covariates via counting process format of input (Surv(t0,t1,event) in survival). For our proposal, we implemented SurvPP by using TensorFlow-2.101. A MacBook Pro with 12-core CPU (Apple M2 Max) was used, where GPU was set as off (tf.device('/cpu:0')) for a

Figure 1: Performance on the dataset of log-linear hazard function \(_{lin}(t)\). (A) Box plot of TLL and AUC as functions of evaluation point: the higher, the better. (B) The CPU times demanded for estimating a hazard function. The error bars represent the standard deviations. For GBM and SurvPP, the average cpu times over 9-points grid search of the hyperparameter are displayed.

fair benchmark comparisons. For GBM and SurvPP, the hyper-parameters were optimized through 9-point grid search: the number of trees and the shrinkage for GBM, and the kernel parameters for SurvPP. For details of the model configurations, see Appendix C.

### Synthetic Data

We created two survival datasets with time-varying 2-D covariate. One was generated from a hazard function with a log-linear dependence on covariates, and the other with a nonlinear dependence on covariates as follows:

\[_{lin}(t)=h(t)1.5y_{1}(t)+3.5y_{2}(t),_{ non}(t)=h(t)2-5(y_{1}^{2}(t)+y_{2}^{2}(t)), \]

where \(h(t)\) is the base hazard function defined by a Weibull hazard function, \(h(t)=2 t^{3/2}\). For each of the datasets, we considered \(U\) individuals, each of which had a 2-D covariate function of time:

\[_{u}(t)=y_{1}^{u}(t),y_{2}^{u}(t)= _{u}(2_{u}t+_{u}),_{u}^{}(2 _{u}^{}t+_{u}^{}), u=1, U, \]

where \(_{u}(_{u}^{})\) and \(_{u}(_{u}^{})\) were sampled uniformly over \(\), \(_{u}\) was sampled uniformly over \(\), and \(_{u}^{}\) was sampled uniformly over \(\). We set the censoring time as \(1\) for all individuals, and an event time generated from (23), denoted by \(t_{*}\), was censored (\(_{u}=0\)) when \(t_{*}\) was over \(1\): \(T_{u}=(t_{*},1)\).

The predictive performances were evaluated based on the dynamic area under the ROC curve (AUC)  and the test log-likelihood (TLL),

\[(t) =^{U}_{u^{}=1}^{U}(T_{u}>t)(T_{u^{}} t)w_{u}(S(t|_{u}()) S(t| _{u^{}}()))}{_{u=1}^{U}(T_{u}>t) _{u=1}^{U}(T_{u} t)w_{u}}, \] \[(t) =_{u=1}^{U}(t T_{u})_{ u}(T_{u}|_{u}())+ S((t,T_{u})|_{u}( )), \]

where \(w_{u}\) is the inverse probability of censoring weight, and \((t|_{u}())\) and \(S(t|_{u}())\) are the estimated hazard and survival functions given covariate map \(_{u}()\), respectively. The dynamic AUC essentially estimates the C-index at each time, and is commonly used in the literature of survival analysis with time-varying covariates.

The evaluation point, \(0 t_{e} 1\), was set as \(t_{e}\{0.3,0.5,0.7,0.9\}\). For each dataset, we randomly split the \(U\) individuals into 10 subgroups, assigned one to test and the others to training data, and

Figure 2: Performance on the dataset of non-linear hazard function \(_{non}(t)\). (A) Box plot of TLL and AUC as functions of evaluation point: the higher, the better. (B) The CPU times demanded for estimating a hazard function. The error bars represent the standard deviations. For GBM and SurvPP, the average cpu times over 9-point grid search of the hyperparameter are displayed.

conducted 10-fold cross evaluation of the predictive performances. We reformatted the training data into counting process format by equally splitting individual \(u\)'s observation periods into \(J_{u}\) sub-region and assuming the covariates to be constant within a sub-region; the survival models were applied to the training data. In this experiment, we considered \(U=10^{3}\) and \(J_{u}\{1,5,10,20,50\}\): the number of observed events \(N\) was \(809\) for \(_{lin}\) and \(818\) for \(_{non}\), respectively.

It should be noted here that when time-varying covariates are considered, the time duration from the entry, \(t\), is itself a time-varying covariate. SurvPP considered a 3-D covariate map, \(}_{u}(t)(y_{1}^{u}(t),y_{2}^{u}(t),t)\), and performed hazard function estimation, while the other models assumed the hazard function to be \(h(t)f((t))\), and \(h(t)\) and \(f()\) were estimated separately.

Figure 1 displays the predictive performance on the dataset of \(_{lin}(t)\). Figure 1A shows that, except for RFM, there were no significant differences in the performances between the compared models, but CoxPH slightly outperformed the other models in TLL at some evaluation periods. The result is plausible because the underlying generative process was perfectly consistent with the log-linear and proportional assumptions of CoxPH. The comparison in TLL between \(J_{u}=5\) and \(J_{u}=20\) suggests that more frequently measured covariates would yield better estimations/predictions in scenarios of time-varying covariates. Therefore, the feasible computational complexity of SurvPP regarding \(J_{u}\) is a clear advantage over the reference models, which is confirmed by Figure 1B.

Figure 2 displays the predictive performance on the dataset of \(_{non}(t)\), showing that the non-parametric models, SurvPP and GBM, significantly outperformed CoxPH, while the performance gaps between SurvPP and GBM were marginal and of no significance. However, Figure 2B shows that SurvPP could estimate the hazard function hundreds of times faster than GBM when covariates were measured frequently (\(J_{u}=50\)), and was even faster than the simplest CoxPH. The preferable scalability of SurvPP's algorithm regarding \(J_{u}\) comes from the fact that \(J_{u}\) (or \(J=_{u}J_{u}\)) is not part of the iterative optimization (Section 2.6), which is a fruit of SurvPP's representer theorem. RFM was comparable with SurvPP and GBM in AUC, but was worse than either of them in TLL. RFM might need more careful tuning of the hyper-parameters, but this was not fully investigated this time.

We conducted additional experiments on independent validation datasets (see Appendix D.2), and on larger synthetic datasets (\(U 10^{5}\)) to examine the model's computation scalability regarding the event number \(N\) (see Appendix D.3).

### Real-world Data

We examined the validity of SurvPP against the benchmark models on two real-world survival data sets, _Mayo Clinic Primary Biliary Cholangitis data_ (PBC) and _Standard And New Antiepileptic

Figure 3: Performances on real-world datasets. TLL (the higher, the better), AUC (the higher, the better), and CPU times (the lower, the better) on PBC dataset (A) and SANAD dataset (B).

_Drugs study data_ (SANAD), provided by R packages survival (LGPL-3)  and joineR (GPL-3) , respectively. In PBC, 312 patients with primary biliary cirrhosis were enrolled in a randomized medical trial at the Mayo Clinic between 1974 and 1984 , where events were the time of death; one static and eleven time-varying covariates were measured at entry and at yearly intervals, which include age at entry, alkaline phosphatase, logarithm of serum albumin, presence of ascites, aspartate aminotransferase, logarithm of serum bilirubin, serum cholesterol, condition of edema, presence of hepatomegaly or enlarged liver, platelet count, logarithm of prothrombin time, and presence or absence of spiders. SANAD was an unblind randomized trial that recruited patients with epilepsy for whom carbamazepine (CBZ) was considered to be standard treatment and they were randomized to CBZ or the newer drug lamotrigine (LTG), where 605 patients were included and event was the time to treatment failure; we adopted calibrated dose as a time-varying covariate, and three static covariates including age of patient at randomization, gender, and randomized treatment (CBZ or LTG); calibrated dose was measured at 166-day intervals on average, and we used a linear interpolation to obtain the values of calibrated dose at which the measurement intervals were regularly trisected. We randomly split the individuals into 10 subgroups, assigned one to test and the others to training data, and conducted 10-fold cross evaluation of the predictive performances.

Figure 3A displays the predictive performance on PBC. It shows that CoxPH achieved very high AUCs (> 0.9), suggesting that the underlying generative process could be consistent with CoxPH's simple assumption of log-linearity and proportionality. Thus the semi-parametric model, CoxPH, is likely to achieve equal or slightly better performances than the nonparametric models, which is consistent with the result. Figure 3B plots the predictive performance on SANAD, where the not so high AUCs of CoxPH suggests that the underlying dependence of intensity on covariates is nonlinear. The figure shows that SurvPP achieved better performance than CoxPH, and achieved comparable performance while being substantially faster than GBM.

## 5 Conclusions

We have proposed a non-parametric Bayesian survival model to address survival data with time-varying covariates. We tailored a permanental process such that the latent hazard function is defined on covariate space and right-censored observations in a counting process format can be handled, which we call the survival permanental process (SurvPP). Through the path integral formulation of Gaussian process, we showed that SurvPP encompasses a representer theorem, and derived a feasible estimation algorithm that scales linearly with the number of observed events. We evaluated SurvPP on synthetic data, confirming that it achieved comparable predictive accuracy while being tens to hundreds of times faster than state-of-the-art methods.

**Limitations & future work:** We examined SurvPP for relatively low-dimensional covariates, and its potential suitability for high-dimensional covariates remains to be clarified. Because SurvPP is based on a normal Gaussian process, the (equivalent) kernel function might be too simple to discover meaningful representations in high-dimensional covariate data. A promising direction is to apply deep kernel learning to SurvPP, where high-dimensional covariates are transformed by non-linear mapping with a deep architecture. A technical issue is that we could not search an appropriate set of (a lot of) parameters of neural networks, because our proposed scheme performs the hyper-parameter optimization by grid search, not by gradient descent. Also, Gaussian kernels, which were used in the paper, naively require a scale parameter for each dimension of data, resulting in a high dimensional kernel parameter for high-dimensional covariate scenarios. The grid search becomes prohibitively costly with high dimensional parameters, but we addressed the problem by a well-known approach that normalizes (e.g., centering and scaling) each dimension of the data and puts a common scale parameter across all dimensions of normalized data. This approach empirically works robustly, but a more sophisticated approach could improve the performance of SurvPP. Variational Bayesian approximations with inducing points might address the technical issues of hyper-parameter optimization, and thus is an important next step in our study.

As in ordinary permanental processes, the nodal line problem could arise in SurvPP: the posterior distribution of the latent variable \(x()\) has many local modes since \( x()\) can lead to similar hazard functions \(\!=\!x^{2}()\), and artificial zero crossings of \(x()\) could happen, especially on locations where the hazard function is low. John and Hensman  have proposed to extend the quadratic link function to include an offset parameter \(\), so that \(()\!=\!(x()+)^{2}\), which is valid for SurvPP.