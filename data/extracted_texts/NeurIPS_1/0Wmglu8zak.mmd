# BubbleML: A Multiphase Multiphysics Dataset and Benchmarks for Machine Learning

Sheikh Md Shakeel Hassan\({}^{1}\)1

Arthur Feeney\({}^{1}\)2

Akash Dhruv\({}^{2}\)Jihoon Kim\({}^{3}\)

Youngjoon Suh\({}^{1}\)Jaiyoung Ryu\({}^{3}\)Yoonjin Won\({}^{1}\)**Aparna Chandramowlishwaran\({}^{1}\)\({}^{1}\)

\({}^{1}\)University of California, Irvine   \({}^{2}\)Argonne National Laboratory   \({}^{3}\)Korea University

Equal contributionsCorresponding authors: {sheikhh1,afeeney,amowli}@uci.edu

###### Abstract

In the field of phase change phenomena, the lack of accessible and diverse datasets suitable for machine learning (ML) training poses a significant challenge. Existing experimental datasets are often restricted, with limited availability and sparse ground truth, impeding our understanding of this complex multiphysics phenomena. To bridge this gap, we present the BubbleML dataset 3 which leverages physics-driven simulations to provide accurate ground truth information for various boiling scenarios, encompassing nuclease pool boiling, flow boiling, and sub-cooled boiling. This extensive dataset covers a wide range of parameters, including varying gravity conditions, flow rates, sub-cooling levels, and wall superheat, comprising 79 simulations. BubbleML is validated against experimental observations and trends, establishing it as an invaluable resource for ML research. Furthermore, we showcase its potential to facilitate the exploration of diverse downstream tasks by introducing two benchmarks: (a) optical flow analysis to capture bubble dynamics, and (b) neural PDE solvers for learning temperature and flow dynamics. The BubbleML dataset and its benchmarks aim to catalyze progress in ML-driven research on multiphysics phase change phenomena, providing robust baselines for the development and comparison of state-of-the-art techniques and models.

## 1 Introduction

Phase-change phenomena, such as boiling, involve complex multiphysics processes and dynamics that are not fully understood. The interplay between bubble dynamics and heat transfer performance during boiling presents significant challenges in accurately predicting and modeling these heat and mass transfer processes. Machine learning (ML) has the potential to revolutionize this field, enabling data-driven discovery to unravel new physical insights , develop accurate surrogate and predictive models , optimize the design of heat transfer systems, and facilitate adaptive real-time monitoring and control .

The applications of ML in this domain are diverse and impactful. Consider the context of high-performance computing in data centers, where efficient cooling is critical. Boiling-based cooling techniques, such as two-phase liquid cooling, offer enhanced heat dissipation capabilities, ensuring reliable and optimal operation of power-intensive electronic components such as GPUs . Boiling also plays a central role in optimizing heat transfer in nuclear reactors, where precise modeling and prediction of boiling dynamics contribute to advancing the safety and efficiency of nuclear power systems . Furthermore, boiling processes play a vital role in thermal desalination methods that provide clean drinking water in water-scarce regions . These advancements in pivotal areas suchas thermal management, energy efficiency, and heat transfer applications, driven by ML techniques have far-reaching implications, empowering us to design more sustainable energy systems, enhance environmental preservation efforts, and advance engineering capabilities across various domains.

To train data-driven ML algorithms effectively, we need large, diverse, and accurately labeled datasets. However, obtaining high-fidelity datasets that encompass a wide range of phase-change phenomena and operating conditions is a significant challenge. Boiling processes are highly sensitive to factors like surface properties, pressure, orientation, and working fluid composition . Additionally, the chaotic nature of vapor interactions and occlusions makes quantifying boiling processes inherently difficult. Specialized experimental setups, involving instrumentation, sensors, and high-speed visualization techniques, come with substantial costs, further limiting the availability of extensive and accurate large-scale experimental data . As a result, only a few well-funded research laboratories have access to precise ground truth data, and even then, this data often lacks fidelity and fails to capture detailed microscale dynamics, such as local bubble-induced turbulence and its impact on overall heat transfer. This scarcity of high-fidelity datasets poses challenges in designing accurate ML models for multiphase and phase change processes. While scientific ML (SciML) approaches can incorporate physical knowledge and constraints into the training process to reduce some of this data burden , the validation and quantification of uncertainty still rely on the availability of ground truth data. Therefore, there is an urgent need for open, diverse, and large-scale datasets to develop robust models and advance research in multiphysics problems such as phase change phenomena.

Simulations have played a key role as the third pillar of science in overcoming the inherent challenges faced by experimental studies in various scientific domains. High-fidelity multiscale data from simulations complement and enhance experimental measurements. In the field of phase change, simulations

Figure 1: **BubbleML Dataset.** Diverse two-phase boiling phenomena with ground truth for key physical variables–velocity, temperature, and pressure. (a) Single bubble rising from a nucleation site on the heater surface. (b) Chaotic multi-bubble dynamics–merging and splitting. (c) Flow boiling transitions from bubbly to slug regime with increasing inlet velocity. Velocity and temperature fields are obtained by solving equations 1a and 1b, pressure field is obtained by solving the Poisson equation which ensures that continuity is satisfied. The physical quantities are in non-dimensional units.

have successfully modeled transport equations for momentum, energy, and phase transition, enabling accurate measurements of velocity, pressure, and temperature fields around bubbles [11; 12]. As a result, simulations serve as powerful tools for understanding and quantifying boiling. However, SciML researchers often set up their own simulations to generate ground truth solutions for training and testing their models rather than relying on open-source benchmark datasets. This practice is common even among impactful papers [10; 13; 14; 15]. While this approach is reasonable for studying specific, simple partial differential equations (PDEs), real-world applications of PDE solvers and simulations often involve large-scale systems with complex multiphase physics and a combination of Dirichlet and Neumann boundary conditions . These real-world problems require significant domain expertise, engineering time, and computational resources. Independently performing such simulations is impractical and even infeasible for many ML researchers. This difficulty in dataset generation has led to a drought of SciML research to study _real-world_ physics problems. Prior efforts to build benchmark datasets have primarily focused on single- and multiphysics problems with single-phase dynamics [17; 18; 19; 20].

As a response to the above challenges, we introduce the BubbleML Dataset 4, an extensive and innovative collection of data generated through Flash-X simulations . This dataset encompasses a wide range of boiling phenomena, including nucleate boiling of single bubbles, merging bubbles, flow boiling in different configurations, and subcooled boiling. Figure 1 is a visual glimpse into the diverse range of physical phenomena and variables in the dataset. To further enhance its applicability, the dataset covers various gravity conditions ranging from earth gravity to gravity at the International Space Station, different heater temperatures, and different inlet velocities. In total, we present around 80 simulations, each capturing a specific combination of parameters and conditions. In summary, the key contributions are as follows:

**Multiphase and Multiphysics Dataset.** A comprehensive dataset encompassing a range of two-phase (liquid-vapor) phase change phenomena in boiling, with a focus on bubble and flow dynamics.

**Real-world Validation.** Validation against experimental data to ensure the dataset's accuracy and reliability. This validation process enhances the dataset's fidelity and establishes a strong connection between simulation and real-world phenomena.

**Diverse Downstream Tasks.** BubbleML is designed to facilitate diverse downstream applications. To demonstrate its potential, we present two benchmark tasks: optical flow for learning bubble dynamics and neural PDE solvers for modeling temperature and flow dynamics.

## 2 Related Work

**Scientific Machine Learning Datasets.** There have been several efforts to develop benchmark datasets for scientific machine learning tasks [17; 18; 19; 20; 22; 23]. Notably, the ERA5 atmospheric reanalysis dataset , curated by the European Center for Medium-Range Weather Forecasting (ECMWF) provides hourly estimates of a large number of atmospheric, land, and oceanic climate variables since 1940. It is the most popular publicly available source for weather forecasting, facilitating the training of neural weather models such as FourCastNet , GraphCast , and ClimaX . PDEBench  provides an impressive collection of datasets for 11 PDEs commonly encountered in computational fluid dynamics. Boundary conditions in scientific simulations play a crucial role in capturing the dynamics of the underlying physical systems. The majority of datasets in PDEBench utilize periodic boundary conditions. Although some datasets encompass Neumann or Dirichlet boundary conditions, none consider a combination of both which presents a noteworthy gap in accurately modeling real-world scenarios. Another challenging problem is the modeling of turbulent Kolmogorov flows and the dataset generated using JAX-CFD  is gaining popularity in benchmarking neural flow models [28; 29]. BlastNet  generated using DNS solver, S3D  focuses on simulating the behavior of a single fluid phase solving for compressible fluid dynamics, combustion, and heat transfer. AirfRANS  is a dataset for studying the 2D incompressible steady-state Reynolds-Averaged Navier-Stokes equations over airfoils. Current datasets have made commendable strides in addressing single- and multiphysics scenarios, and provide a valuable foundation for developing and evaluating SciML algorithms. Nonetheless, their scope falls short of capturing the range of behaviors and phenomena encountered in phase change physics.

In contrast, BubbleML focuses on capturing the complex dynamics and physics associated with multiphase phenomena, particularly in the context of phase change simulations. Unlike many existing datasets that predominantly utilize a single type of boundary condition, BubbleML incorporates a combination of Dirichlet and Neumann boundary conditions . This inclusion enables researchers to explore and model scenarios where multiple boundary conditions coexist, enhancing the realism and applicability of the dataset. Moreover, the presence of "jump" conditions along the liquid-vapor interface adds an additional layer of complexity. These conditions arise due to surface tension effects and require careful modeling to accurately capture the interface behavior . By incorporating such challenges, BubbleML provides a realistic and demanding testbed for ML models.

**Optical Flow Datasets.** Optical flow estimation, a classical ill-posed problem  in image processing, has witnessed a shift from traditional methods to data-driven deep learning approaches. Middlebury  is a dataset with dense ground truth for small displacements, while KITTI2015  provides sparse ground truth for large displacements in real-world scenes. MPI-Sintel  offers synthetic data with very large displacements, up to 400 pixels per frame. However, these datasets are relatively small for training deep neural networks. FlyingChairs , a large synthetic dataset, contains around 22,000 image pairs generated by applying affine transformations to rendered chairs on random backgrounds. FlyingThings3D  is another large synthetic dataset with approximately 25,000 stereo frames of 3D objects on different backgrounds.

While these datasets have been instrumental in advancing data-driven optical flow methods, they primarily focus on rigid object motion in visual scenes and do not address the specific challenges posed by multiphase simulations. Efforts have been made to capture non-rigid motion in nature, such as piece-wise rigid motions seen in animals . In boiling, the non-rigid dynamics of bubbles and the motion of liquid-vapor interfaces play a crucial role in the distribution and transfer of thermal energy. The BubbleML dataset provides a unique opportunity to explore and develop optical flow algorithms tailored to phase change dynamics. Unlike existing datasets, it offers a diverse range of bubble behaviors, including merging, growing, splitting, and complex interactions (see Figure 1). The ability to accurately predict and forecast bubble dynamics has practical implications in various fields.

## 3 BubbleML: A Multiphase Multiphysics Dataset for ML

We first introduce the preliminary concepts underlying the SciML learning problem and give insights into the simulation and PDEs in this domain. Then, we present an overview of the dataset pipeline.

### Preliminaries

A common application for SciML is approximating the solution of _boundary value problems_ (BVPs). BVPs are widely used to model various physical phenomena, including fluid dynamics, heat transfer, electromagnetics, and quantum mechanics . BVPs take the form: \(L[u(x)]=f(x),x\) and \(B[u(x)]=g(x),x\). The goal is to determine the vector-valued solution function, \(u\). \(x\) is a point in the domain \(\) and may include a temporal component. The boundary of the domain is denoted as \(\). The governing equation is described by the PDE operator \(L\), and the forcing function is denoted as \(f\). The _boundary condition_ (BC) is given by the boundary operator \(B\) and the boundary function \(g\). \(B[u]=g\) ensures the existence and uniqueness of the solution.

There are three common types of BCs: periodic, Dirichlet, and Neumann. Periodic BCs enforce the equality of the solution at distinct points in the domain: \(u(x_{1})=u(x_{2})\). Dirichlet BCs specify the values of the solution on the boundary: \(u(x)=g(x)\). Neumann BCs enforce constraints on the derivatives of the solution: \(_{n}u(x)=g(x)\). As seen in Figure 1, BubbleML combines both Dirichlet (no-slip walls, heater, inflow) and Neumann (outflow) boundaries, which impose constraints on flow and temperature dynamics. Additionally, the "jump conditions" that govern the transitions between the liquid and vapor phases use Dirichlet and Neumann boundaries .

### Overview of PDEs and Flash-X Simulation

A comprehensive description of the simulations is well beyond the scope of this paper and can be found in . We provide a concise description here as knowledge of the PDEs is important when training physics-informed models.

The liquid (\(l\)) and vapor (\(v\)) phases of a boiling simulation are characterized by differences in fluid and thermal properties: density, \(\); viscosity, \(\); thermal diffusivity, \(\); and thermal conductivity \(k\). The phases are tracked using a level-set function, \(\), which is positive inside the vapor and negative in the liquid. \(=0\) provides implicit representation of the liquid-vapor interface, \(\) (see Figure 1). The transport equations are non-dimensionalized and scaled using the values in liquid and are given as,

\[}{ t}+=- } P+[}{^{}}}]+}{ ^{2}}+_{u}^{}+S_{P}^{} \] \[+ T= [}{\,} T ]+S_{T}^{} \]

where, \(\), is the velocity, \(P\) is the pressure, and \(T\) is the temperature everywhere in the domain. The Reynolds number (Re), Froude number (Fr), and Prandtl Number (Pr) are constants set for each simulation. Scaled fluid properties like \(^{}\) represent the local value of the phase scaled by the corresponding value in liquid. Therefore, \(^{}\) is \(1\) in liquid phase, and \(_{v}/_{l}\) for vapor phase. The effect of surface tension is modeled using Weber number (We) and incorporated by a sharp pressure jump, \(S_{P}^{}\), at the liquid-vapor interface, \(\). The effects of evaporation and saturation conditions on velocity and temperature, \(_{u}^{}\), and \(S_{T}^{}\), are modeled using a ghost fluid method . For a more detailed discussion of non-dimensional parameters and values, we refer the reader to Appendix D.

The continuity equation is given by, \(=-(^{})^{-1}\), where the mass transfer \(\) is computed using local temperature gradients in liquid and vapor phase, \(=()^{-1}[ T ^{}-k^{} T_{v}^{}]\) where, \(^{}\) is the surface normal vector to the liquid-vapor interface. The Stefan number St, is another constant defined for the simulation and depends on the temperature scaling given by \( T=T_{wall}-T_{bulk}\), and latent heat of evaporation, \(h_{lv}\). Simulation data is scaled to dimensional values using the characteristic length \(l_{0}\), velocity \(u_{0}\), and temperature \((T-T_{bulk})/ T\) scale. Temporal integration is implemented using a fractional step predictor-corrector formulation to enforce incompressible flow constraints. The solver has been extensively validated and demonstrates an overall second-order accuracy in space [31; 32].

In thermal science, _heat flux_ measured as the integral of the temperature gradient across the heater surface (\(= T/ y\)) serves as a vital indicator of boiling efficiency. It reflects the contribution of multiple sub-processes, such as conduction, convection, microlayer evaporation, and bubble-induced turbulence. Identifying and managing the impact of each sub-process to enhance \(\) is an open challenge [4; 43]. _Critical heat flux_ (CHF) signifies peak heat flux before a sharp drop in efficiency occurs due to the formation of a vapor barrier (see Figure 5b). It is arguably the most important design and safety parameter for any heat flux controlled boiling application . Accurate heat flux modeling and prediction of boiling crisis are paramount for the reliability of heat transfer systems [45; 46; 47].

The simulations in this study are implemented within the Flash-X framework [21; 31], and a dedicated environment is provided for running new simulations 5. The repository contains example configuration files for various multiphase simulations, including those used in this dataset. To ensure reproducibility, a lab network has been designed that organizes each study using configuration files for data curation. The lab notebook and Flash-X source code are open-source to allow for community development and contribution, enabling the creation of new datasets beyond the scope of this paper. The simulation archives store HDF5 output files and bash scripts that document software environment and repository tags for reproducibility. The lab notebook also provides an option to package Flash-X simulations as standalone Docker/Singularity containers, which can be deployed on cloud and supercomputing platforms without the need for installing third-party software dependencies. The latter is ongoing work towards software sustainability .

### Dataset Overview

The study encompasses two types of boiling - pool boiling and flow boiling. Pool boiling represents fluid confined in a tank above a heater, resembling scenarios like cooling nuclear waste. The BCs for pool boiling include walls on the left and right, an outlet at the top, and a heater at the bottom. In contrast, flow boiling models water flowing through a channel with a heater, simulating liquid cooling of data center GPUs. There is an inlet BC modeling flow into the system and an outlet. The fluid used for the simulations is FC-72 (perfluorohexane), an electrically insulating and stable fluorocarbon-based fluid commonly used for cooling applications in electronics operating at low temperatures (ranging from \(50^{}\)C to \(100^{}\)C). To explore various phenomena, different parameters such as heater temperature, liquid temperature, inlet velocity, and gravity scale are adjusted in each simulation. Table 1 presents a summary of the dataset. Appendix E provides detailed illustrations of the boundary conditions and descriptions of each simulation for reference.

BubbleML stores simulation output in HDF5 files. Each HDF5 file corresponds to the state of a simulation at a specific instant in time and can be directly loaded into popular tensor types (e.g., PyTorch tensors or NumPy arrays) using BoxKit. BoxKit is a custom Python API designed for efficient management and scalability of block-structured simulation datasets . It leverages multiprocessing and cache optimization techniques to improve the read/write efficiency of data between disk and memory. Figure 2 shows an example of a boiling dataset and the corresponding workflow for enabling downstream tasks like scientific machine learning and optical flow. By operating on simulation data in manageable chunks that fit into memory, BoxKit significantly improves computational performance, particularly when handling large datasets.

Each simulation within the BubbleML dataset tracks the velocities in the \(x\) and \(y\) directions, temperature, and a signed distance function (SDF), \(\), which denotes the distance from the bubble interface. The SDF can be used to get a mask of the bubble interfaces or determine if a point is in the liquid or vapor phase. These variables are stored in HDF5 datasets. For instance, the temperature is stored as a tensor with a shape \(t x y z\) (\(t x y\) in 2D), which allows indexing with \(xyz\)-spatial coordinates or time. The HDF5 files also include any constants or runtime parameters input to the simulation. Some of these parameters, such as thermal conductivity or Reynolds number, are constants used in the PDEs that govern the system. Including these variables and parameters in the dataset enables comprehensive analysis and modeling of the boiling phenomena.

   Dim & Type - Physics & Sims & Domain & Resolution & Timesteps & Size \\  & & & (\(mm^{d}\)) & Spatial & \( t\) & & (GB) \\ 
2D & PB - Single Bubble & 1 & \(4.2 6.3\) & \(192 288\) & 0.5 & 500 & 0.5 \\
2D & PB - Saturated & 13 & \(11.2 11.2\) & \(512 512\) & 1 & 200 & 24.2 \\
2D & PB - Subcooled & 10 & \(8.4 8.4\) & \(384 384\) & 1 & 200 & 10.3 \\
2D & PB - Gravity & 9 & \(11.2 11.2\) & \(512 512\) & 1 & 200 & 16.5 \\
2D & FB - Intel Velocity & 7 & \(29.4 3.5\) & \(1344 160\) & 1 & 200 & 10.7 \\
2D & FB - Gravity & 6 & \(35 3.5\) & \(1600 160\) & 1 & 200 & 10.9 \\
2D & PB - Subcooled\({}_{0.1}\) & 15 & \(8.4 8.4\) & \(384 384\) & 0.1 & 2000 & 155.1 \\
2D & PB - Gravity\({}_{0.1}\) & 9 & \(11.2 11.2\) & \(512 512\) & 0.1 & 2000 & 163.8 \\
2D & FB - Gravity\({}_{0.1}\) & 6 & \(35 3.5\) & \(1600 160\) & 0.1 & 2000 & 108.6 \\
3D & PB - Earth Gravity & 1 & \(8.75^{3}\) & \(400^{3}\) & 1 & 57 & 122.2 \\
3D & PB - ISS Gravity & 1 & \(8.75^{3}\) & \(400^{3}\) & 1 & 29 & 62.6 \\
3D & FB - Earth Gravity & 1 & \(35 3.5^{2}\) & \(1600 160^{2}\) & 1 & 55 & 93.9 \\   

Table 1: Summary of BubbleML datasets and their parameters. \( t\) is the temporal resolution in non-dimensional time (\( t=1=0.008\) seconds). For rationale behind the parameter choices, refer to appendix A.3. PB: pool boiling. FB: flow boiling.

Figure 2: **Dataset Curation and Workflow.** Flash-X multiphase simulations are executed and converted into unblocked HDF5 formats using the BoxKit library. The resulting dataset is publicly available \({}^{3}\), enabling downstream tasks like scientific machine learning and optical flow.

BubbleML follows the FAIR data principles  as outlined in Appendix A.2. It is essential to validate simulations against experimental observations due to inherent approximations in numerical solvers and simplified models of real-world phenomena. Appendix A.4 provides a detailed validation of the BubbleML dataset.

## 4 Benchmarks of BubbleML: Optical Flow and SciML

### Optical Flow

**Generation of Optical Flow Dataset.** Optical flow computes the velocity field of an image based on the relative movement of objects between consecutive frames. This method holds significant implications for downstream tasks, such as extracting side-view boiling statistics and applying SciML to real-world experimental data. Although many datasets capturing spatiotemporal dynamical systems can be repurposed to create optical flow datasets, the inherent non-rigidity of bubbles introduces unique physical phenomena that are not present in other datasets. For instance, consider the scenario where a bubble detaches from a heater surface: the bottom region of the bubble exhibits significantly higher velocity compared to the top region, resulting in a velocity gradient that forces the bubble into a spherical shape. At higher heater temperatures, deformation and detachment processes might occur more frequently, leading to different flow patterns and bubble behaviors illustrated in Figure 1.

We create an optical flow dataset from BubbleML to learn bubble dynamics. Liquid and vapor phases are distinguished using \(\). In the generated image sequences, bubble trajectories are tracked across consecutive timesteps. Note that the optical flow dataset only includes bubble velocities at each timestep. Excluding liquid velocities focuses the learning task on capturing observable objects (bubbles). The bubble velocities in non-dimensional units are converted to pixels per frame units (see Appendix B.1) before being written to the widely used Middlebury  flow format, resulting in a sequence of images and flow files that resemble the Sintel dataset . For training and validation of optical flow models, PyTorch dataloaders are provided for the generated dataset 3. This allows for easy integration and fine-tuning of existing optical flow models using the BubbleML dataset.

**Learning Bubble Dynamics.** We evaluate and fine-tune two state-of-the-art optical flow models, RAFT  and GMFlow , using the BubbleML optical flow dataset (B). We consider three different pre-trained models for each method: the first model is trained exclusively on FlyingChairs (C), the second is trained on FlyingChairs and FlyingThings3D (C+T), and the third model is fine-tuned for the Sintel Benchmark (C+T+S). To assess the performance of the trained models, we measure the end-point error. Table 2 summarizes the results for one dataset. Refer to Appendix B for results on the other datasets.

    &  &  & Things (Val) &  &  &  \\    & & & Clean & & & Clean & &  & F1-EPE & F1-all \\   & RAFT & 0.82 & 9.03 & 2.19 & 4.49 & 9.83 & 37.57 & 4.20 \\  & GMFlow & 0.92 & 10.23 & 3.22 & 4.43 & 17.82 & 56.14 & 4.73 \\   & RAFT & 0.91 & 11.22 & 2.55 & 5.16 & 13.7 & 44.44 & **2.33** \\  & GMFlow & 1.31 & 11.99 & 3.78 & 5.12 & 21.91 & 63.04 & **2.36** \\   & RAFT & 1.15 & 4.39 & 1.40 & 2.71 & 5.02 & 17.46 & 4.72 \\  & GMFlow & 1.26 & 3.48 & 1.50 & 2.96 & 11.60 & 35.62 & 7.98 \\   & RAFT & 1.28 & 7.69 & 1.69 & 2.95 & 9.96 & 23.61 & 2.38 \\  & GMFlow & 1.39 & 3.88 & 1.61 & 2.91 & 14.49 & 43.09 & 2.51 \\   & RAFT & 1.21 & 4.69 & 0.77 & 1.22 & 1.54 & 5.64 & 8.39 \\  & GMFlow & 1.53 & 4.09 & 0.95 & 1.28 & 3.04 & 13.61 & 14.65 \\   & RAFT & 1.37 & 6.59 & 0.89 & 1.60 & 1.83 & 6.44 & 2.34 \\  & GMFlow & 1.65 & 4.49 & 1.07 & 1.45 & 4.06 & 18.99 & 2.56 \\   

Table 2: Results of pre-trained and fine-tuned RAFT and GMFlow models on optical flow data (B) generated from PB-Saturated dataset. A 80:20 split results in 2000 training and 500 validation image pairs. The pre-trained models include C trained on FlyingChairs dataset, C+T trained further on FlyingThings3D, and C+T+S further fine-tuned for the Sintel test benchmark. Model+B represents models fine-tuned on the BubbleML optical flow dataset.

Initially, the pre-trained models exhibit subpar performance on the BubbleML data. To address this, each model is fine-tuned for 3-4 epochs with a low learning rate of \(10^{-6}\). After fine-tuning, we observe a significant improvement in predictions for the test data (see Figures 7 and 8 in Appendix B). While all fine-tuned models tend to converge to similar levels of accuracy for pool boiling datasets, fine-tuning the pre-trained FlyingChairs models (C) on the BubbleML (B) dataset gives the best results. This could be attributed to the similar nature of the datasets consisting of 2D objects in motion. In the case of flow boiling, the best results are achieved by fine-tuning models initially trained for the Sintel benchmark (C+T+S). Flow boiling images have an extremely high aspect ratio (8:1), which is similar to the Sintel (3:1) and the KITTI (4:1) datasets. Note that although training the models on the boiling dataset for more epochs improves performance on our specific task, it adversely affects the models' generalization capabilities, leading to increased errors on the other datasets.

**Open problems.** Error analysis (see Appendix B.3) highlights the shortcomings of state-of-the-art optical flow models in accurately capturing the turbulent dynamics of bubbles. Although fine-tuning improves the overall performance, the high errors at the bubble boundaries remain an ongoing challenge. This underscores the need for novel optical flow models that incorporate physical insights to accurately model the complex and chaotic behavior of boiling. BubbleML bridges the gap for physics-informed optical flow datasets.

### Scientific Machine Learning

**SciML Preliminaries.** SciML baseline experiments use _neural PDE solvers_ to learn temperature and flow dynamics. We focus on two classes of neural PDE solvers: (a) Image-to-image models, widely used in computer vision tasks, such as image segmentation . These may not always be suitable as PDE solvers, since they are limited to fixed resolution, but they remain competitive in many baselines [55; 28]. (b) Neural operators are neural networks that learn a mapping between infinite-dimensional function spaces. As they map functions to functions, neural operators are discretization invariant and can be used on a higher resolution than they were trained . The seminal neural operator is the Fourier Neural Operator (FNO) . Refer to Appendix C for further details.

For both classes of models, we employ the auto-regressive formulation of a forward propagator, denoted as \(\). For timesteps \(\{t_{1},,t_{max}\}\) discretized such that \(t_{k+1}-t_{k}= t\), the forward propagator \(\) maps the solution function \(u\) at \(k\) consecutive time steps \(\{t_{m-k},,t_{m-1}\}\) to the solution at time \(t_{m}\). For brevity, we use \(u([t_{m-k},t_{m-1}])=\{u(t_{m-k}),,u(t_{m-1})\}\). The operator \(\) can be approximated using a neural network \(_{}\) parameterized by \(\). This network is trained using a dataset of \(N\) ground truth solutions \(D=\{u^{(n)}([0,t_{max}]):n=1 N\}\). By applying a standard gradient descent algorithm, we find parameters \(\) minimizing some loss function of the predictions \(_{}\{u^{(n)}([t_{m-k},t_{m-1}])\}\) and the ground truth solutions \(u^{(n)}(t_{m})\). Thus, given solutions for \(k\) initial timesteps of an unseen function \(u\), we can obtain an approximation \(_{}\{u([0,t_{k-1}])\} u(t_{k}=t_{k-1}+ t)\). Using this approximation for \(t_{k}\), we can step forward to get \(_{}\{u([t_{1},t_{k}])\} u(t_{k+1})\). This process is called _rollout_ and is repeated until reaching \(t_{max}\). While in principle, rollout can be done for arbitrary time, the quality of approximation worsens with each step [28; 56]. We implement several strategies that attempt to mitigate this deterioration [57; 58]. However, achieving a long and stable rollout is still an open problem.

**Baseline Implementations.** We implement several baseline image-to-image models--including UNetbench and UNetmod--and neural operators--including FNO, UNO, F-FNO, G-FNO, and T-FNO. Detailed descriptions and comparisons of the models are included in Appendix C.1.

**Training Strategies.** Detailed descriptions for each of the training strategies we used are listed in Appendix C.2. We implement teacher-forcing training , temporal bundling, and the pushforward trick . Models trained with the pushforward trick are prefixed with "P-". A discussion of hyperparameter settings can be found in Appendix C.3.

**Metrics.** We draw inspiration from PDEBench and adopt a large set of metrics that include the Root Mean Squared Error (RMSE), Max Squared Error, Relative Error, Boundary RMSE (BRMSE), and low/mid/high Fourier errors . These metrics provide a comprehensive view of the physical dynamics, which may be missed when only using a global loss metric. For instance, when predicting temperature, we find that the max error can often be very high due to the presence of sharp transitions between hot vapor and cool liquid. Even a one-pixel misalignment in the model's prediction can cause the reported temperature to be the opposite extreme. Metrics that report a global average (i.e., RMSE) could mask these errors because they get damped by the average. We incorporate an additional _physics_ metric: the RMSE along bubble interfaces (IRMSE). Accuracy along both the domain and immersed boundaries is of significant importance. Boundary conditions determine if the solution to a PDE exists and is unique. In the case of the multiphysics BubbleML dataset, accurate modeling of the system requires satisfying the conditions at the liquid-vapor interfaces accurately.

**Learning Temperature Dynamics.** One application of SciML using the BubbleML dataset is to learn the dynamics of temperature propagation within a system. In this context, the system's velocities serve as a sourcing function, influencing the temperature distribution. Notably, UNet-based models perform best across all datasets (see Figure 3b-d). For a complete listing of error metrics for each model and dataset pairing, refer to Appendix C.4. UNet models may have some advantage in predicting the interfaces and boundaries (IRMSE and BRMSE) because they naturally act as edge-detectors. The temperature also propagates smoothly, so it is likely unnecessary to use global filters like the FNO variants. In contrast, FNO models rely on fast Fourier transforms and weight multiplication in the Fourier space, which, while capable of handling global and local information simultaneously, might not be as effective at capturing local, non-smooth features. Several recent studies report similar observations about auto-regressive UNet and FNO variants .

The trained model can be a valuable tool to get fast estimates of heat flux, discussed in section 3.2. Heat flux is influenced by steep temperature gradients and dynamic temporal changes which presents a challenging problem. To further validate our models, we perform cross-validation to predict the heat flux trends observed in Figure 5. For each heat flux prediction, we holdout a simulation and train a forward propagator on the remaining simulations within the dataset. Even with partial training-50 epochs for subcooled boiling models and 100 epochs for saturated boiling models-we achieve compelling results. The heat flux predictions by UNetbench remarkably track the expected trend, as seen in Figure 3a.

**Learning Fluid Dynamics.** As an additional benchmark, we use the BubbleML dataset to train models to approximate both velocity _and_ temperature dynamics. A challenging problem! Detailed results are in Appendix C.5. These follow similar training settings to the temperature-only models. Strikingly, we observe nearly the opposite results to predicting only temperature: the UNetbench model struggles when predicting both velocity and temperature fields jointly, while the UNetmod and the FNO variants perform comparatively better. All the models have difficulty capturing the trails of condensation that form in the temperature field. The vapor trails form but dissipate more quickly than expected. An example rollout of UNetmod, trained using the pushforward trick, is shown in Figure 4. We see that the flow closely aligns with the ground-truth simulation.

**Open Problems.** We reiterate several open problems in SciML that BubbleML offers an avenue to explore. The first is the creation of a new class of _models that can learn multiple interrelated physics_. We find that while UNet architectures work well at predicting temperature and FNO variants work well at predicting velocity, neither excels at joint prediction of temperature and velocity. The CNN-based UNet architectures slightly outperform FNO and its variants when predicting temperature, potentially due to CNN's capacity to naturally act as edge-detectors, and thus handle non-smooth

Figure 3: **Temperature and Heat Flux Prediction.** (a) Cross-validated heat flux \(q/q_{max}\) estimates for subcooled and saturated boiling. (b), (c), and (d) show results for the fully trained forward propagator. In (b), accuracy degradation is minimal, with spikes occurring during timesteps of violent turbulence caused by rapid bubble detachment from the heater surface. (c) and (d) compare frames from the Flash-X simulation and predictions by the forward propagator for subcooled boiling.

interfaces more easily. On the other hand, FNO variants perform quite well at predicting velocities but still do not perform particularly well at temperature estimation, especially in capturing condensation trails. This is related to the second problem: _developing neural operators that can handle nonsmooth and irregularly shaped interfaces_. FNO variants seem to encounter difficulties in modeling temperature fields, which have sharp jumps along bubble interfaces where the temperature transitions from cool liquid to hot vapor. Conversely, the velocity field appears relatively smooth and thus may be composed of lower frequencies better captured by FNO variants. However, these models still miss sharp and sudden changes in velocity along bubble interfaces that are important for accurately modeling long-range dynamics. The third problem is improving _stability during long rollouts_. This is explored within the context of other datasets , but it is particularly relevant for BubbleML. For instance, in subcooled boiling, after bubbles depart from the surface, they undergo condensation and generate vortices that gradually dissipate as they move upstream. To model these extended temporal processes accurately, autoregressive models must be stable across long rollouts. However, we observe that models experience instability, leading them to slowly diverge from the ground truth. The BubbleML dataset presents an opportunity to study these challenges in SciML.

## 5 Conclusions and Limitations

This paper introduces BubbleML, which addresses a critical gap in ML research for multiphase multiphysics systems. By employing physics-driven simulations, the dataset provides precise ground truth information for a number of boiling scenarios, encompassing a wide range of parameters. BubbleML is validated against experimental observations and trends, establishing its reliability in multiphysics phase change research. The two BubbleML benchmarks demonstrate applications in improving the accuracy of optical flow estimation and SciML modeling encountered in multiphase systems. Importantly, BubbleML extends its impact beyond its immediate applications. It resonates with broader challenges in SciML, serving as a foundational platform to study several open problems.

Limitations.Combining datasets might pose challenges due to their varying sizes. Because the resolution scales proportionally with the domain size, the constant relative spacing between grid cells allows the UNet model to be effectively trained on the merged boiling dataset. However, this approach does not extend to FNO, requiring domain decomposition methods  or downscaling strategies  to accommodate variable domain sizes. Note that the dataset is also exclusively composed of simulations due to the unavailability of experimental data with velocity, pressure, and temperature fields. Future work will involve collaboration with experimentalists to augment the dataset.

Figure 4: **Velocity and Temperature Rollout. The left figure shows the first 80 timesteps of P-UNet\({}_{}\)’s rollout, where color indicates velocity and streamlines illustrate direction of flow. Both the flow magnitude and direction align exceptionally well with the ground truth. On the right, (a) and (b) show the rollout errors for temperature and velocity predictions. The prefix “P-” denotes that the model is trained with the pushforward trick . Notably, UNet\({}_{}\) starts with slightly better initial accuracy, but it degrades more quickly than the model trained with the pushforward trick. P-UNet\({}_{}\) behaves more stably during rollout.**