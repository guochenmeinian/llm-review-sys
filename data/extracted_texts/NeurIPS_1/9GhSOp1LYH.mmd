# Leveraging Hallucinations to Reduce Manual Prompt Dependency in Promptable Segmentation

Jian Hu1, Jiayi Lin1, Junchi Yan2, Shaogang Gong1

1School of Electronic Engineering and Computer Science, Queen Mary University of London

2Dept. of CSE & School of AI & Moe Key Lab of AI, Shanghai Jiao Tong University

{jian.hu, jiayi.lin, s.gong}@qmul.ac.uk, yanjunchi@sjtu.edu.cn

[https://lwpyh.github.io/ProMaC/](https://lwpyh.github.io/ProMaC/)

###### Abstract

Promptable segmentation typically requires instance-specific manual prompts to guide the segmentation of each desired object. To minimize such a need, task-generic promptable segmentation has been introduced, which employs a single task-generic prompt to segment various images of different objects in the same task. Current methods use Multimodal Large Language Models (MLLMs) to reason detailed instance-specific prompts from a task-generic prompt for improving segmentation accuracy. The effectiveness of this segmentation heavily depends on the precision of these derived prompts. However, MLLMs often suffer hallucinations during reasoning, resulting in inaccurate prompting. While existing methods focus on eliminating hallucinations to improve a model, we argue that MLLM hallucinations can reveal valuable contextual insights when leveraged correctly, as they represent pre-trained large-scale knowledge beyond individual images. In this paper, we utilize hallucinations to mine task-related information from images and verify its accuracy for enhancing precision of the generated prompts. Specifically, we introduce an iterative **Prompt-Mask**C**ycle generation framework (ProMaC) with a prompt generator and a mask generator. The prompt generator uses a multi-scale chain of thought prompting, initially exploring hallucinations for extracting extended contextual knowledge on a test image. These hallucinations are then reduced to formulate precise instance-specific prompts, directing the mask generator to produce masks that are consistent with task semantics by mask semantic alignment. The generated masks iteratively induce the prompt generator to focus more on task-relevant image areas and reduce irrelevant hallucinations, resulting jointly in better prompts and masks. Experiments on 5 benchmarks demonstrate the effectiveness of ProMaC. Code given in [https://lwpyh.github.io/ProMaC/](https://lwpyh.github.io/ProMaC/).

## 1 Introduction

Current promptable segmentation methods rely on instance-specific manual prompts to guide segmentation, greatly limiting its large-scale application. Recently, a manual-free task-generic promptable segmentation approach was introduced : only a single task-generic prompt is needed for all samples under the same task, e.g., "camouflaged animal" is a task-generic prompt for all images in a camouflaged object detection task. The model segments task-relevant objects in various images based on this generic prompt, significantly reducing the annotation workload.

A task-generic prompt is both coarse and potentially ambiguous, can result in poor segmentation when directly applied. To address this problem, existing methods  utilize the prior knowledge embedded in Multimodal Large Language Models (MLLMs) to infer more detailed, instance-specific prompts, such as bounding boxes or keywords, to guide the segmentation. However, these MLLMs often generate hallucinations due to object co-occurrence priors , mistakenly predicting non-existent elements based on the environment as instance-specific prompts (Fig.1(a)). This can mislead segmentation and degrade model performance. While it is common to consider MLLM's hallucinations as detrimental and should be eradicated , this phenomenon actually demonstrates a MLLM's significant capacity for contextual inference based on prior training. We want to explore MLLM hallucinations as a valuable untapped knowledge resource for scene understanding, critical in complex segmentation scenarios. In practice, when task-related objects are not prominently visible, hallucinations can fill in missing information with plausible predictions based on learned patterns of association. Moreover, they can also extend beyond these familiar patterns, exploring and identifying new relationships within the data that were not explicitly taught during training. This dual ability to replicate and innovate makes hallucinations a valuable asset for enhancing model performance in complex or new situations. This predictive reasoning capacity not only fills perceptual gaps but also enriches the model's understanding, as hallucinations utilize prior knowledge to replicate and discover new patterns, enhancing insight into the target domain (see Fig.1(b)). Despite the potential benefits, using hallucinations to extract useful information from images to aid task remains unexplored.

In this work, instead of direct eliminating hallucinations, we utilize them as prior knowledge to mine extended task-related information from a given test image, performing scene understanding on the image before segmentation, then systematically reduce irrelevant hallucinations iteratively by visual masking verification, optimizing jointly instance-specific prompts and masks. To this end, we introduce an iterative, training-free Prompt-Mask Cycle Generation method (ProMaC) that refines segmentation through cyclic interactions between a prompt and mask generator (see Fig. 2). The prompt generator uses a multi-scale chain-of-thought prompting mechanism, which utilizes hallucinations to hypothesize and visual masking to verify, thereby creating more accurate instance-specific prompts. We trigger the hallucinatory tendencies of MLLMs, the process starts by dividing the image into patches at different scales and positions. Such partial visibilities of objects facilitate MLLMs to hypothesize potential object semantic labels and visual locations based on its prior knowledge. For validating the correctness of these hypotheses, we formulate a visual contrastive reasoning mechanism to generate contrastive images that contain only the background without any potential task-related objects. This helps identify all possible co-occurrence hallucinations caused by the background. By comparing these contrastive images with the original images, the MLLM effectively distinguishes between accurate hypotheses and those influenced by misleading prior knowledge, leading to more reliable prompts. Given the current promptable segmentation models' strength at mask prediction but struggle with label prediction, the mask generator uses mask semantic alignment to ensure that the produced masks align with the task semantics. These aligned masks not only serve as outputs but also guide the prompt generator in subsequent cycles, enhancing both prompt and mask quality continuously. **Our contributions are three-folds:**

1). We introduce a training-free Prompt-Mask Cycle Generation (ProMaC) to perform two tasks: Explore MLLM hallucinations as prior knowledge to enhance contextual scene understanding on each test image; systematically reduce irrelevant hallucinations to verify iteratively and optimize jointly both generated prompts and visual masking in object segmentation.

Figure 1: (a) During MLLM pretraining, leopards often co-occur with grass. If the lion is masked, the model incorrectly identifies it as a leopard based on the grass. (b) Directly inputting the image into MLLM causes the hidden caterpillar being incorrectly predicted as a leaf. Splitting the image results in interested objects being incomplete or absent, prompting MLLM to induce hallucinations and utilize prior knowledge to predict potential task-related objects within the image. Our visual contrastive reasoning then eliminates the hallucinations and validates the gathered predictions, aiding in the accurate identification of the caterpillar.

2). We formulate an iterative optimization method including a prompt generator and a mask generator. To improve prompt relevance, the prompt generator utilizes a multi-scale chain of thought approach. It first leverages hallucinations to expand task-related plausible prompts, then applies visual contrastive reasoning to validate and reduce irrelevant prompts. ProMaC's mask generator overcomes SAM's shortcomings in label prediction by creating masks that align semantically with generated prompts.

3). Comprehensive comparative evaluations on 5 different segmentation tasks with 12 diverse datasets against 22 existing models demonstrate the effectiveness of ProMaC.

## 2 Related Works

**Promptable Segmentation** refers to object segmentation with active interactions from user inputs. Interaction methods vary from points, boxes, to scribbles. SAM , AV-SAM, GroundingSAM  and SEEM  accept video, audio, and multimodal inputs. However, they often rely on manual prompts, which can be unclear and subjective. Even with these prompts, they typically excel only in specific tasks. To address this issue, GenSAM  introduces a manual-free promptable segmentation setting, where only one task-generic prompt is provided. This prompt can be applied to all images within the task for instance-specific segmentation without any additional manual prompting. GenSAM primarily utilizes MLLM to infer the names of task-related objects in the images and uses them as instance-specific prompts for SAM to guide segmentation. However, GenSAM lacks spatial information about objects and may lead to inaccurate prompt predictions in complex scenes.

**Hallucinations in MLLMs** refers to models generate content that does not exist in the input data . This issue often arises from the models leveraging extensive prior training rather than just the immediate input, leading to false predictions on fine-grained details. There are some efforts to mitigate this problem, including refining training processes  and improving model architectures . Other efforts focus on aligning model outputs more closely with actual data, employing feedback mechanisms for real-time adjustments . While current works focus on eliminating hallucinations to enhance performance , our work explores how to utilize hallucinations to expand and reason plausible context and validate them iteratively to remove irrelevant generalizations.

**Visual Marking for MLLMs** has been explored in recent research to prompt MLLMs through manipulation of visual inputs: (i) adding learnable soft tokens to visual inputs for efficient parameter tuning , (ii) using image sequences as demonstrations of a new task , and (iii) overlaying visual markers like masks, boxes, and circles onto visual inputs to ground regions . Our work falls into the third category, employing visual guidance for reasoning. Yang et al.  propose set-of-mark (SoM) prompts, where images are segmented and numbered regions to improve GPT-4V  visual grounding. However, as detailed in Tab.5, we confirm previous findings  that this visual marker approach struggles with open-source MLLMs like LLaVA. Instead of proprietary models  or fine-tuning , our training-free ProMaC uses inpainting task-related regions and contrasting model output distributions to prompt MLLMs.

## 3 Methodology

We introduce ProMaC, a cycle-generation method for segmenting unknown multiple classes of objects training-free with only a single task-generic prompt. Specifically, given an image \(X^{H W 3}\) from a test set, ProMaC employs a task-generic prompt \(P_{g}\) across datasets in the same task to produce

Figure 2: An overview of ProMaC: Masks created iteratively by the mask generator guide the prompt generator to jointly improve instance-specific prompts and visual masking in segmentation.

a final segmentation mask \(M^{H W}\), thereby removing the need for individual supervision for each image. The prompt generator leverages prior knowledge gained to reason and deduce instance-specific prompts, which then guide a mask generator to create masks aligned with task semantics. These masks act both as the current segmentation outcome and as visual markers for generating subsequent prompts. Training-free ProMaC relies solely on test-time adaptation.

### Prompt Generator

Prompt generator employs MLLMs to generate instance-specific prompts based on image content and prior knowledge. It transforms the general prompt \(P_{g}\), into an instance-specific prompt for each individual instance, providing more detailed descriptions of task-relevant objects. MLLM with parameters \(\) receives an image \(X\) and query \(P\) as inputs. \(X\) provides contextual visual information to assist the model in generating a relevant response \(y\) to the query \(P\). The response \(y\) is sampled auto-regressively from the probability distribution conditioned on \(P\) and \(X\) as follows:

\[y_{t} p_{}(y_{t} X,P,y_{<t})(_{}(y_ {t} X,P,y_{<t})) \]

where \(y_{t}\) denotes the token at time step \(t\), and \(y<t\) represents the sequence of generated tokens up to the time step \((t-1)\). In practice, predicted task-relevant objects can often blend into the background due to texture, color, size, or position, leading to inaccuracies in instance-specific prompts. To address this problem, we explore MLLM hallucinations as contextual prior knowledge from pretraining, rather than eliminate them. These hallucinations are particularly useful when direct visual cues are absent or ambiguous, helping the model fill in information gaps and hypothesize potential task-related elements within the image that are not prominent. By revealing these often-overlooked subtle associations, hallucinations provide a more comprehensive scene understanding of the image content. This deeper contextual understanding provide a reasoning context for generating more accurate and relevant instance-specific prompts candidates.Thus, using hallucinations to uncover task-related knowledge helps overcome challenges from visual ambiguities and object camouflage in complex scenes. To this end, we propose a multi-scale chain-of-thought prompting strategy that stimulates hallucinations to leverage prior knowledge, fully extracts task-relevant information, and then uses this information to enhance the precision of the generated instance-specific prompts.

#### 3.1.1 Multi-scale Chain of Thought Prompting

Multi-scale Chain of Thought Prompting consists of two processes: Gathering candidate knowledge and generating accurate instance-specific prompts. To efficiently collect task-relevant information from an image, as shown in Fig. 3, we divide the input image into patches at various scales by cutting

Figure 3: ProMaC consists of a prompt generator and a mask generator for cyclical optimization. The prompt generator employs multi-scale chain-of-thought prompting. It initially use hallucinations for exploring task-related information within image patches. It identifies task-relevant objects and their backgrounds (\(A^{k}_{}\), \(A^{k}_{}\)) along with their locations (\(B^{k}\)). Subsequently, it uses visual contrastive reasoning to refine and finalize instance-specific prompts (\(A^{u}_{i}\), \(B^{u}_{i}\)) by eliminating hallucinations. The mask generator then processes these prompts into the segmentation model (“Seg”), producing a mask aligned with task semantics. This mask further guides the visual contrastive reasoning process, which leverages an inpainting model to eliminate masked regions, creating contrastive images. These images enable the prompt generator to further refine its prompts, enhancing segmentation accuracy.

horizontally, vertically, or by leaving it whole. These patches are then processed by the MLLM to gather preliminary instance-specific prompts. The differing levels of task-relevant object visibility in each patch prompt the MLLM to induce hallucinations. These hallucinations utilize prior knowledge to explore connections between the image data and the associated task, aiding in the detection of potential bounding boxes and object names. The process is computed by:

\[B^{k}=(X^{k},C^{k},P_{B}), A^{k}_{},A^ {k}_{}=(X^{k},C^{k},P_{A}), \]

where \(C^{k}\) is the caption generated by MLLM for the \(k-\)th image patch \(X^{k}\). \(P_{g}\) is task-generic prompt. For bounding box prediction, the prompt \(P_{B}\), which instructs "_This image is from the \(P_{g}\) detection task, output the bounding box of the \(P_{g}\)_.". This guides the MLLM to predict the bounding box \(B^{k}\) of the task-related objects within the patch. For predicting name, the prompt \(P_{A}\), stating "_Output the name of the \(P_{g}\) and its environment in one word_." is used, guiding the MLLM to predict the names of the task-related objects \(A^{k}_{}\) and their backgrounds \(A^{k}_{}\) from each patch. The preliminary data, including object names \(A^{k}_{}\) and bounding boxes \(B^{k}\), gathered from various patches, are compiled into candidate lists \(_{i}\) and \(_{i}\). Here, \(i\) denotes the iteration in the iterative learning cycle. In this process, the hallucinations employed are essentially based on object co-occurrence priors, where objects commonly associated with background elements during pre-training are predicted to be task-relevant, even if they are not present in the current image. This prior knowledge is useful during the knowledge collection stage as it uncovers implicit relationships and details in the image. However, it can also reduce accuracy of the later fine-grained instance-specific prompts generation. Therefore, it is crucial to control these hallucinations in the latter stage to prevent incorrect predictions.

**Visual Contrastive Reasoning.** To mitigate hallucinations caused by object co-occurrence priors, recent research highlights particularly relevant regions of an image to direct MLLMs focus toward task-related elements, thereby minimizing background interference and enhancing model accuracy . To achieve this, visual markers are employed to steer MLLM attention on task-relevant visual regions, thereby reducing hallucinations. While closed-source MLLMs like GPT-4V  can interpret these markers effectively, they are costly and large. In contrast, models like LLaVA  are open-source, but cannot process visual markers such as points or bounding boxes, and employing these markers might disrupt the original pixel data, degrading performance on LLaVA (see Tab. 5). Moreover, accurate pixel-level visual markers are unavailable in our setting. To solve this problem, we aim to enable LLaVA to focus on task-related regions without altering the original pixel data, thereby effectively minimizing hallucinations and enhancing the precision of instance-specific prompts.

Despite the absence of instance-level annotations, promptable segmentation models produce masks with detailed textures, which provide rich positional and textural information about interested regions. We use these masks as visual markers to guide a MLLM to focus on task-related areas during the generation of instance-specific prompts. Inspired by classifier-free guidance , we introduce visual contrastive reasoning (VCR), a training-free visual marking method to help MLLM focus on specific regions, reducing hallucinations. The relevance of a region is assessed by observing MLLM output changes when key areas are excluded. It guides the MLLM to focus on areas with notable changes (bottom of Fig.3). Based on Eq. (1), we derive a probability distribution by comparing original image \(X\) with a modified image, \(X^{}=(X,)\), where region IM is excluded.

\[y_{t}  p_{}(y_{t} X,P,y_{<t})((y_ {t} X,P,y_{<t})}{p_{}(y_{t}(X,),P,y_{<t}) })^{}\] \[[(1+)_{}(y_{t} X,P,y_{<t})-_{}(y_{t}(X,),P,y_{<t})], \]

where \(\) is the level of focus on region IM. A higher \(\) increases emphasis on that region. Following , we set \(=1\) in all tasks. It preserves the integrity of the original image pixels \(X\), while constructing contrastive samples \(X^{}\) that encourage the model to focus on task-related regions. Ideally, \(X^{}\) should exclude task-related objects while maintaining a uniform appearance and overall context with the original image. But directly marking \(X^{}\) disrupts its pixels, making contrastive sample generation challenging.

**Contrastive Sample Generation.** To address it, we employ inpainting, where the mask \(M_{i-1}\) obtained from the previous iteration segmentation is treated as the inpainting mask \(_{i}\) to guide the creation of \(X^{}\). We use a negative prompt \(P_{n}\): "\(A^{}_{i}\), _is a \(P_{g}\)_", to ensure that the inpainted \(X^{}\) does not contain potentially task-related objects \(A^{}_{i}\). Additionally, we use a positive prompt \(P_{p}\): "\(A^{}_{i}\), _high quality, detailed, blended to the original image_.", to ensure consistency between the generatedportion and the surrounding background \(A_{i}^{}\). The corresponding inpainting is defined as:

\[X^{}=F_{in}(X,_{i},P_{p},P_{n}), \]

where \(F_{in}\) represents the inpainting module, and we choose Stable Diffusion to perform this operation. This method ensures the generated \(X^{}\) excludes task-related objects without disrupting the pixel continuity. In the first iteration, since \(_{i}\) does not yet exist, we use bounding box predictions from various patches \(_{i}\) as an alternative. As \(X^{}\) contains only the background, comparing it with \(X\) eliminates co-occurrence hallucination caused by the background and highlights differences in task-related regions, subtly guiding the model to focus on these areas. Finally, we use visual contrastive reasoning to identify accurate instance-specific prompt to guide segmentation as follows,

\[B_{i}^{u}=(X,X^{},C,P_{B}), A_{i}^{u }=(X,X^{},C,P_{A}), \]

where \(\) represents our visual contrastive reasoning, and \(C\) is the caption of the image. The collected knowledge, \(i\) and \(i\), is integrated into the prompt \(P_{A}\) and \(P_{B}\). This process aids in identify the ultimate instance-specific names \(A_{i}^{u}\) and bounding boxes \(B_{i}^{u}\) of the objects.

### Mask Generator

Until now, we described how to use SAM-generated masks as a visual marker to guide the model to focus on task-relevant areas for generating accurate instance-specific prompts. But this method relies on an assumption that the mask accurately delineates task-related regions. However, SAM is trained on large-scale prompt-mask pairs without category labels, it excels at identifying masks based on image textures but lacks label prediction capabilities. Consequently, the SAM-generated mask may not always align with task semantics, yet such alignment is crucial for our method.

#### 3.2.1 Mask Semantic Alignment

We need to utilize texture generalization capabilities of SAM to describe possible task-related objects within the prompt-targeted areas, while also ensuring that the generated masks align with the task semantics. To achieve this, we divide the input image into patches of varying scales using horizontal, vertical, and uncut divisions as outlined in the last section. these processed patches are then reintegrated onto the original image with surrounding areas blacked out, and fed into SAM to ensure it focuses exclusively on the patch. Finally, masks generated from different patches are aggregated based on their relevance to task semantics, providing an accurate representation of task-related objects. The masks for each patch is generated as follows,

\[m_{i}^{k}=((A_{i}^{u},X_{i}),B_{i}^{u},X_{i}^{k}), \]

where mask \(m_{i}^{k}\) is obtained by inputting the corresponding image patch \(X_{i}^{k}\) and associated prompts into SAM during the \(i-\)th iteration. Following , Spatial CLIP maps the text prompt \(A_{i}^{u}\) to regions in the image \(X_{i}\) that correspond to the content of the prompt. The processed images, along with the generated instance-specific text prompts \(A_{i}^{u}\), are the input into CLIP to assess semantic similarity.

\[s(m_{i}^{k})=(m_{i}^{k} X_{i},A_{i}^{u}), \]

the operation \(\) results in retaining only those parts of \(X_{i}\) that are covered by the predicted mask. \(s(m_{i}^{k})\) represents the similarity between masked image and \(A_{i}^{u}\), calculated using CLIP. The similarity scores obtained from different patches are denoted as \(S_{i}=[s(m_{i}^{1}),s(m_{i}^{2}),,s(m_{i}^{k})]\). After normalizing the elements within \(S_{i}\), the closer the normalized \(s(m_{i}^{k})\) is to 1, the more semantically aligned \(m_{i}^{k}\) is with the instance-specific text prompt \(A_{i}^{u}\). Finally, we compute the weighted sum of the normalized \(s(m_{i}^{k})\) and \(m_{i}^{k}\) as follows.

\[M_{i}=_{k=1}^{K}(s(m_{i}^{k})*m_{i}^{k}), \]

\(M_{i}\) is the output mask of the \(i-\)th iteration of \(X\). The generated \(M_{i}\) leverages SAM's mask prediction capabilities to create highly detailed masks. Simultaneously, through the mask semantic alignment process, it ensures that the output mask aligns with the task's semantics, thereby overcoming the limitation of SAM's mask prediction lacking semantic understanding. The mask is applied to the original image as a weight, to generate the next iteration image \(X_{i}\) for segmentation. This excludes irrelevant regions to reduce interference during segmentation.

\[X_{i+1}=w(X_{i} M_{i})+(1-w) X_{i}, \]

where \(w\) is a hyperparameter, which we have assigned a value of 0.3.

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

**Results on MIS and TOD Task.** The MIS task identifies pathological tissues in medical images. We used three datasets: ColonDB  and Kvasir  for polyp image segmentation, and ISIC  for skin lesion segmentation. We compared our approach with others using task-generic prompt settings (see Tab. 2). While other models underperform in medical imaging due to limited generalization, ProMaC improves significantly over the baseline by iteratively mining task-related knowledge. For the TOD task, we evaluated ProMaC on the GSD  and Trans10K-hard  datasets (See Tab. 3(a)). Using the task-generic prompt setting, our method achieves the best results despite challenging scenarios. This demonstrates ProMaC's versatility and adaptability across complex visual tasks.

**Results on OVS and SR Task.** We evaluated ProMaC's effectiveness on the OVS task for multi-class segmentation based on a list of candidate classes. Specifically, we tested it on the validation splits of PASCAL VOC (21 classes) , Pascal Context (59 classes) , and COCO-Object (80 classes) , using LLaVA to identify and confirm the presence of candidate classes. After obtaining masks, we resolved overlaps using the argmax operation based on SAM probabilities. Tab. 3(b) shows how ProMaC compares to other state-of-the-art OVS methods. Unlike some methods trained specifically on these datasets (risking knowledge leaking), ProMaC is not. Yet, ProMaC still outperforms all others on PASCAL VOC and Pascal Context and is competitive on COCO-Object. Additionally, as shown in Tab. 5, we integrated our VCR into LLaVA1.5 for enhanced spatial reasoning. This integration allows LLaVA to better focus on critical areas, thereby boosting performance.

**Module Analysis.** As shown in Tab. 4, we perform an ablation study on the COD and MIS tasks to assess the effects of different modules. "MCoT" is multi-scale chain of thought prompting. "TIP" and "IVP" refer to using only instance-specific text prompts or visual prompts. "VCR" is visual contrastive reasoning, and "MSA" is mask semantic alignment. The first row shows replacing MoCt with just one original image results in reduced performance, highlighting the importance of using hallucinations to extract task-relevant information. The second and third rows show that single modal prompts perform worse than multimodal prompts, highlighting the significance of multimodal prompting. Removing VCR causes a significant drop in performance, indicating that visual prompts are crucial for directing LLaVA's focus on relevant areas during inference. The comparison between the fifth and final rows emphasizes the importance of mask alignment with task semantics. The consistent positive results across tasks confirm the robustness and effectiveness of our approach.

**Parameter Analysis.** Tab. 6(a) examines how iterations influence performance. "cos" measures the cosine similarity between the predicted text prompt and the ground truth class through CLIP. "IoU" assesses the overlap between the predicted bounding box and the ground truth, comparing it against a rectangular outline of the mask. Mask predictions improve and stabilize after the fourth epoch. Tab. 6(b) investigates the effects of various image processing techniques. "Original" uses no modifications,

Figure 4: Visualization of various segmentation methods among various segmentation tasks.

"Halve" divides the image horizontally or vertically into halves, and "Quarters" divides it into four quarter-sized patches. Testing shows that combining "Original" and "Halve" yields the best results by balancing global and local information without excessive fragmentation.

**Visual Marker Strategy.** Tab. 6(c) assesses the impact of different visual marker strategies. "None" uses no visual prompts, while "Bbox" places bounding boxes directly on the image. "VCD" employs previous methods that introduce Gaussian noise into comparison images for contrastive reasoning. Results indicate that bounding boxes decrease performance, suggesting LLaVA struggles with this type of markers. Although VCD methods improve performance, they distort pixel data, making them less effective than our approach. Our VCR generates contrastive samples that focus on task-relevant areas without altering the image, reducing hallucinations and enhancing performance.

**Visualization.** Fig. 4 and Fig.5 visually compares our ProMaC with other methods across 3 tasks and also shows the contrastive images we generated. GenSAM handles clear objects well but struggles with complex background. Although GenSAM performs well in complex backgrounds, but struggles with challenging tasks. ProMaC delivers solid segmentation results across different tasks, and our contrastive images remove task-related regions while maintaining semantic and pixel consistency.

## 6 Conclusion

In this work, we introduce an iterative ProMaC that uses MLLM hallucinations to guide automatic prompt generation, significantly improving segmentation without training. This iterative approach aligns masks with task semantics, enhancing model performance. Testing on multiple benchmarks has demonstrated ProMaC's effectiveness in a wide range of complex segmentation tasks.

Figure 5: Visualization of the generated masks and contrastive samples over iterations.