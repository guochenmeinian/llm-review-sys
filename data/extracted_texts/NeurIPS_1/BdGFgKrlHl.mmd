# Addressing Bias in Online Selection with Limited Budget of Comparisons

Ziyad Benomar

ENSAE, Ecole Polytechnique,

FairPlay joint team

ziyad.benomar@ensae.fr

Evgenii Chzhen

CNRS, LMO, Universite Paris-Saclay

evgenii.chzhen@universite-paris-saclay.fr

Nicolas Schreuder

CNRS, Laboratoire d'informatique

Gaspard Monge (LIGM/UMR 8049)

nicolas.schreuder@cnrs.fr

Vianney Perchet

CREST, ENSAE, Criteo AI LAB

Fairplay joint team

vianney.perchet@normalesup.org

###### Abstract

Consider a hiring process with candidates coming from different universities. It is easy to order candidates with the same background, yet it can be challenging to compare them otherwise. The latter case requires additional costly assessments, leading to a potentially high total cost for the hiring organization. Given an assigned budget, what would be an optimal strategy to select the most qualified candidate? We model the above problem as a multicolor secretary problem, allowing comparisons between candidates from distinct groups at a fixed cost. Our study explores how the allocated budget enhances the success probability in such settings.

## 1 Introduction

Online selection is among the most fundamental problems in decision-making under uncertainty, Multiple problems within this framework can be modeled as variants of the secretary problem , where the decision-maker has to identify the best candidate among a pool of totally ordered candidates, observed sequentially in a uniformly random order. When a new candidate is observed, the decision maker can either select them and halt the process or reject them irrevocably. The optimal strategy is well known and consists of skipping the first \(1/e\) fraction of the candidates and then selecting the first candidate that is better than all previously observed ones. This strategy yields a probability \(1/e\) of selecting the best candidate. A large body of literature is dedicated to the secretary problem and its variants, we refer the interested reader to  for a historical overview of this theoretical problem.

In practice, as pointed out by several social studies, the selection processes often do not reflect the actual relative ranks of the candidates and might be biased with respect to some socioeconomic attributes . To tackle this issue, several works have explored variants of the secretary problem with noisy or biased observations . In particular,  studied the _multi-color secretary problem_, where each candidate belongs to one of \(K\) distinct groups, and only candidates of the same group can be compared. This corresponds for example to the case of graduate candidates from different universities, where the within-group orders are freely observable and can be trusted using a metric such as GPA, but inter-group order cannot be obtained by the same metric. This model, however, is too pessimistic, as it overlooks the possibility of obtaining inter-group orders at some cost, through testing and examination. Taking this into account, we study the multicolor secretary problem with a budget for comparisons, where comparing candidates from the same group is free, and comparing candidatesfrom different groups has a fixed cost of \(1\). We assume that the decision-maker is allowed at most \(B\) comparisons. This budget \(B\) represents the amount of time/money that the hiring organization is willing to invest to understand the candidate's "true" performance. As in the classical secretary problem, an algorithm is said to have _succeeded_ if the selected candidate is the best overall, otherwise, it has _failed_. The objective is to design algorithms that maximize the probability of success.

### Contributions

The paper studies an extension of the _multi-color_ secretary problem (Correa et al., 2021), where comparing candidates from different groups is possible at a cost. This makes the setting more realistic and paves the way for more practical applications, but also introduces new analytical challenges.

In Section 2, we describe a general class of Dynamic-Threshold (DT) algorithms, defined by distinct acceptance thresholds for each group, that can change over time depending on the available budget. First, we examine a particular case where all thresholds are equal, which can be viewed as an extension of the classical \(1/e\)-strategy. However, the analysis is intricate due to additional factors such as group memberships, comparison history, and the available budget. By carefully controlling these parameters, we compute the asymptotic success probability of the algorithm, demonstrating its extremely rapid convergence to the upper bound of \(1/e\) when the budget increases, hence constituting a first efficient solution to the problem.

Subsequently, our focus shifts to the case of two groups, where we explore another particular case of DT algorithms: static double threshold algorithms. These involve different acceptance thresholds for each group, that depend on the groups' proportions and the initial budget, but do not vary during the execution of the algorithm. We prove a recursive formula for computing the resulting success probability, and we exploit it to establish a closed-form lower bound and compute explicit thresholds.

In the two-group scenario, we also derive the optimal algorithm among those that do not utilize the history of comparisons, which we refer to as _memory-less_. We present an efficient implementation of this algorithm and demonstrate, via numerical simulations, that it belongs to the class of DT algorithms when the number of candidates is large. Leveraging this insight, we numerically compute optimal thresholds for the two-group case.

### Related work

The secretary problemThe secretary problem was introduced by Dynkin (1963), who proposed the \(1/e\)-threshold algorithm, having a success probability of \(1/e\), which is the best possible. Since then, the problem has undergone extensive study and found numerous applications, including in finance (Hlynka and Sheahan, 1988), mechanism design (Kleinberg, 2005; Hajiaghayi et al., 2007), Nested Rollout Policy Adaptation (NRPA) (Dang et al., 2023), active learning (Fujii and Kashima, 2016), and the design of interactive algorithms (Sabato and Hess, 2016, 2018). Moreover, the secretary problem has multiple variants (Karlin and Lei, 2015; Bei and Zhang, 2022; Assadi et al., 2019; Keller and Geisser, 2015), and has inspired other works, for instance, related to matching (Goyal, 2022; Dickerson et al., 2019) or ranking (Jiang et al., 2021). A closely related problem is the prophet inequality (Krengel and Sucheston, 1977; Samuel-Cahn, 1984), where the decision-maker sequentially observes values sampled from known distributions, and its reward is the value of the selected item, in opposite the secretary problem where the reward is binary: \(1\) if the selected value is the maximum and \(0\) otherwise. Prophet inequalities also have many applications (Kleinberg and Weinberg, 2012; Chawla et al., 2010; Feldman et al., 2014) and have been explored in multiple variants (Kennedy, 1987; Azar et al., 2018; Bubna and Chiplunkar, 2023; Benomar et al., 2024).

Different information settingsIn some practical scenarios, the secretary problem may present a pessimistic model. Therefore, variants with additional information have been studied. For example, Gilbert and Mosteller (2006) explored a scenario where candidates' values are independently drawn from a known distribution. Other studies have examined potential improvements with other types of information, such as samples (Correa et al., 2021) or machine-learned advice (Antoniadis et al., 2020; Dutting et al., 2021; Benomar and Perchet, 2023). Conversely, some works more closely aligned with ours have investigated more constrained settings. Notably, Correa et al. (2021) introduced the multi-color secretary problem, where totally ordered candidates belong to different groups, and only the partial order within each group, consistent with the total order, can be accessed. Under fairness constraints, they designed an asymptotically optimal strategy for selecting the best candidate. Other settings with only partial information have been studied as well. For example, Monahan (1980, 1982) addressed the optimal stopping of a target process when only a related process is observed, and they designed mechanisms for acquiring information from the target process. However, these works do not assume a fixed budget and instead consider a penalized version of the problem.

Online algorithms with limited adviceThis paper also relates to other works on online algorithms, where the decision-maker is allowed to query a limited number of hints during execution. The objective of these analyses is to measure how the performance improves with the number of permitted hints. Such settings have been studied, for example, in online linear optimization (Bhaskara et al., 2021), caching, (Im et al., 2022), paging (Antoniadis et al., 2023), scheduling (Benomar and Perchet), metrical task systems (Sadek and Elias, 2024), clustering (Silwal et al., 2023), and sorting (Bai and Coester, 2024; Benomar and Coester, 2024). Another related paper by Drygala et al. (2023) studies a penalized version of the Bahncard problem with costly hints.

## 2 Formal problem

We consider a strictly totally ordered set of cardinal \(N\), whose elements will be called _candidates_. We assume that these candidates are observed in a uniformly random arrival order \((x_{1}, x_{N})\), and that they are partitioned into \(K\) groups \(G^{1},,G^{k}\). For all \(t[N]\), we denote by \(g_{t}[K]\) the group of \(x_{t}\), i.e. \(x_{t} G^{g_{t}}\), and we assume that \(\{g_{t}\}_{t[N]}\) are mutually independent random variables

\[(g_{t}=k)=_{k}, t[N], k[K]\;,\]

where \(_{k}>0\) for all \(k[K]\) and \(_{k=1}^{K}_{k}=1\).

We assume that comparing candidates of the same group is free, while comparing two candidates of different groups is costly. To address the latter case, we consider that a budget \(B 0\) is given for comparisons and we propose two models: the algorithm can pay a cost of \(1\) in order to:

1. compare a two already observed candidates \(x_{t}\) and \(x_{s}\) belonging to different group,
2. determine if the current candidate is the best candidate seen so far among all the groups.

For simplicity, we focus on the second model. However, we explain during the paper how our algorithms adapt to the first model and the cost they incur.

When a new candidate arrives, the algorithm can choose to select them, halting the process, or it can choose to skip them, moving on to the next one--hoping to find a better candidate in the future. Once a candidate has been rejected, they cannot be recalled--the decisions are irreversible. Given the total number of candidates \(N\), the probabilities \((_{k})_{k[K]}\) characterizing the group membership, and a budget \(B\), the goal is to derive an algorithm that maximizes the probability of selecting the best overall candidate. We refer to the problem as the \((K,B)\)-secretary problem

### Additional notation

For all \(t<s[N]\), we denote by \(x_{t:s}:=\{x_{t},,x_{s}\}\), and for all \(k[K]\) we denote by \(G^{k}_{t:s}\) the set of candidates of group \(G^{k}\) observed between steps \(t\) and \(s\),

\[G^{k}_{t:s}:=\{x_{r}\ :\ t r sg_{r}=k\}=x_{s:t} G^{k}\.\]

If \(t=1\), then we lighten the notation \(G^{k}_{s}:=G^{k}_{1:s}\). Let \(\) be any algorithm for the \((K,B)\)-secretary problem, we define its stopping time \(()\) as the step \(t\) when it decides to return the observed candidate. We will often drop the explicit dependency on \(\) and write \(\) when no ambiguity is involved. We will say that \(\) succeeded if the selected candidate \(x_{}\) is the best among all the candidates \(\{x_{1},,x_{N}\}\). Let us also define, for any step \(t 1\), the random variables

\[r_{t}=_{t^{}=1}^{t}(x_{t} x_{t^{}},g_{t}=g_{t^{ }}) R_{t}=_{t^{}=1}^{t}(x_{t}  x_{t^{}})\ . \]

Both random variables have natural interpretations: given a candidate at time \(t\), \(r_{t}\) is its _in-group rank_ up to time \(t\), while \(R_{t}\) is its _overall rank_ up to time \(t\). Note that the actual values of \(x_{t}\) do not play a role in the secretary problem and we can restrict ourselves to the observations \(r_{t},g_{t},R_{t}\). While the first two random variables are always available at the beginning of round \(t\), the third one can be only acquired utilizing the available budget. At each step \(t[N]\), the decision-maker observes \(r_{t},g_{t}\) and can perform one of the following three actions:

1. skip: reject \(x_{t}\) and move to the next one;
2. stop: select \(x_{t}\);
3. compare: if the comparison budget is not exhausted, use a comparison to determine if \((R_{t}=1)\)--compare the candidate \(x_{t}\) to the best already seen candidates in the other groups;

Furthermore, if a comparison has been used at time \(t\), the algorithm has to perform stop or skip afterward. We denote respectively by \(a_{t,1}\) and \(a_{t,2}\) the first and second action made by the algorithm at step \(t\). Let us also define \(g_{t}^{*}\) the group to which the best candidate observed until step \(t\) belongs,

\[g_{t}^{*}=*{argmax}_{k[K]}\{ G_{t}^{k}\} t[ N]\,\]

and \(B_{t}\) as the budget available for \(\) at step \(t\)

\[B_{1}=B B_{t}=B_{t-1}-(a_{t,1}= ) t[N]\.\]

In the presence of a non-zero budget, the first time when \(\) decides to make a comparison will be a key parameter in our analysis of the success probability. We denote it by \(_{1}()\),

\[_{1}()=\{t[N]:a_{t,1}=\}\.\]

As with the stopping time, when there is no ambiguity about \(\), we simply write \(_{1}\).

**Remark 2.1**.: _Although we formalized the problem using the variables \(r_{t}\) and \(R_{t}\), the only information needed at any step \(t\) is \((r_{t}=1)\) and \((R_{t}=1)\), i.e we only need to know if the candidate is the best seen so far, in its own group and overall. In practice, \((r_{t}=1)\) can be observed by comparing \(x_{t}\) to the best candidate up to \(t-1\) belonging to \(G^{g_{t}}\), and if this is the case then \((R_{t}=1)\) can be observed by comparing \(x_{t}\) to the best candidate in the other group._

## 3 Dynamic threshold algorithm for \(K\) groups

In this section, we introduce a general family of Dynamic-threshold (DT) algorithms. A DT algorithm for the \((K,B)\)-secretary problem is defined by a finite doubly-indexed sequence \((_{k,b})_{k[K],b B}\) of real numbers in \(\), which determines the _acceptance_ thresholds based on the group of the observed candidate and the available budget. During a run of the algorithm, the thresholds used for each group dynamically change depending on the evolution of the available budget. We denote this algorithm by \(^{B}(_{k,b})_{k[K],b B}\).

Upon the arrival of a new candidate \(x_{t}\), the algorithm observes its group \(g_{t}=k[K]\) and its in-group rank \(r_{t}\), and has an available budget of \(B_{t}=b 0\). If \(t/N<_{k,b}\) or \(r_{t}=0\), the candidate is immediately rejected. Otherwise, if \(t/N_{k,b}\) and \(r_{t}=1\), then the candidate is selected if \(b=0\); and if the budget is not yet exhausted (\(b>0\)), then the algorithm pays a unit cost to observe the variable \((R_{t}=1)\). If this variable is \(1\), indicating a favorable comparison, the candidate is selected; otherwise, it is rejected. A formal description is given in Algorithm 1, and a visual representation for the case of three groups is provided in Figure 1.

### Single-threshold algorithm for \(K\) groups

In this section, we focus on the single-threshold algorithm, a specific case within the family of DT algorithms, where all thresholds are identical across groups and budgets. Initially, the algorithm rejects all candidates until step \(T-1\), where \(T[N]\) is a fixed threshold. Upon encountering a new candidate that is the best within its group, if no budget remains, the candidate is selected. Alternatively, if there is still a budget available, the algorithm utilizes it to determine if the current candidate is the best among all groups. If that is the case, the candidate is then selected. We denote by \(_{T}^{B}\) the single-threshold algorithm with threshold \(T\) and budget \(B\). We demonstrate that this algorithm has an asymptotic success probability converging very rapidly to the upper bound of \(1/e\).

In this first lemma, we prove a recursion formula on the success probability of the single-threshold algorithm, with a threshold \(T= N\) for some \(\).

**Lemma 3.1**.: _The success probability of the single threshold algorithm \(_{T}^{B}\) with threshold \(T= N\) and budget \(B 0\) satisfies the recursion formula_

\[(_{T}^{B})=}{K-1}+ (B 0)(K-1)_{t=T}^{N}}{t^{K+1}}( _{t+1}^{B-1})+O}\.\]

The proof hinges on analyzing the behavior of the algorithm following the first comparison. After that comparison, the algorithm halts if \(R_{t}=1\), and the success probability can be computed in that case. Otherwise, if \(R_{t} 1\), the candidate is rejected, and the algorithm transitions to a new state at step \(t+1\), where the available budget reduces to \(B-1\). Its success probability becomes precisely that of algorithm \(_{t+1}^{B-1}\), with budget \(B-1\) and threshold \(t+1\).

The recursion outlined in Lemma 3.1 can be used to calculate the asymptotic success probability of the single-threshold algorithm \(_{ N}^{B}\) as the number of candidates \(N\) approaches infinity.

**Theorem 3.2**.: _The asymptotic success probability of the single threshold algorithm \(_{T}^{B}\) with threshold \(T= N\) and budget \(B 0\) is_

\[_{N}(_{ N}^{B})=}{K-1}_{b=0}^{B}(}- _{=0}^{b})^{}}{!})\.\]

_In particular,_

\[_{B}_{N}(_{ N }^{B})=(1/)\.\]

Note that, for \(B=\), the asymptotic success probability in the previous theorem corresponds to the success probability of the algorithm with a threshold \( N\) in the secretary problem. Indeed, with an unlimited budget, the decision-maker can assess at each step whether the current candidate is the best so far, and the problem becomes equivalent to the classical secretary problem.

Figure 1: Schematic description of a DT algorithm in the case of 3 groupsAlternative comparison model.In the alternative comparison model presented in Section 2, the single threshold algorithm \(_{T}^{B}\) can be adapted to guarantee the same success probability at the cost of \(K-1\) additional comparisons. After the first \(T\) candidates are rejected, \(K-1\) comparisons are made between the maximums from each group to identify the best candidate so far. The algorithm then keeps track of the best candidate: whenever a new candidate is the best in their group, they are compared to the current best candidate using a single comparison, and the latter is updated accordingly. This approach enjoys the same guarantees as in Theorem 3.2, but with a budget of \(K+B-1\) instead of \(B\).

The next corollary measures how the success probability of the single threshold algorithm, in the setting with \(K\) groups, converges to \(1/e\) as the budget increases.

**Corollary 3.2.1**.: _The success probability of the single-threshold algorithm with threshold \(T= N/e\) and budget \(B 0\) satisfies_

\[_{N}(_{ N/e}^{B})(1-}{(B+1)!})\.\]

_In particular, for all \(>0\), if \(K 1+(e)^{}\), then \(_{N}(_{ N/e}^{B})(1- )/e\)._

This corollary proves that the success probability of \(_{ N/e}^{B}\) converges very rapidly to the upper bound \(1/e\) as \(B\) increases. However, the convergence becomes slower when \(K\) is larger.

Surprisingly, the asymptotic success probability of \(_{ N/e}^{B}\) is not influenced by the proportions \((_{k})_{k[K]}\), but only by the number of groups \(K\). This means that the algorithm does not benefit from the cases where there is a majority group \(G^{k}\) with \(_{k}\) very close to 1, which would make the problem easier. Indeed, it is always possible to achieve a success probability of \(_{k[K]}_{k}/e\) by rejecting all the candidates not belonging to the majority group \(G^{k^{*}}\), and using the classical \(1/e\)-rule counting only elements of \(G^{k^{*}}\). This algorithm can be combined with ours by always running the one with the highest success probability, depending on the available budget, the number of groups, and the group proportions. The resulting algorithm has a success probability that converges to the upper bound \(1/e\) both when \(B\) increases and when \(_{k}_{k}\) converges to \(1\). Nonetheless, due to the very fast convergence of the success probability of the single threshold algorithm to \(1/e\), the improvement brought by having a majority group is only marginal when the budget is sufficient.

As a consequence, the single threshold algorithm surprisingly constitutes a very efficient solution to the problem even with moderate values of the budget. Computing the optimal thresholds remains however an intriguing question, which we explore in the following sections in the case of two groups.

## 4 The case of two groups

In this section, we delve into the particular case of two groups, and we demonstrate how leveraging different thresholds for each group can enhance the success probability. Let \((0,1)\) represent the probability of belonging to group \(G^{1}\), and \(1-\) the probability of belonging to group \(G^{2}\). We examine the success probability of Algorithm \(^{B}(,)\), with threshold \( N\) for group \(G^{1}\) and \( N\) for group \(G^{2}\), having a budget of \(B\) comparisons. This algorithm is a specific instance of the DT family, wherein the thresholds depend only on the group, and not on the available budget. We call it a _static double-threshold_ algorithm.

We assume without loss of generality that \(\), and we denote by \(_{N}\) the event

\[(_{N}): t 1:(||G^{1}_{t}|- t|\,\ ||G^{2}_{ t}|-(1-)t|) 4\.\]

This event provides control over the group sizes at each step. Lemma A.2 guarantees that \(_{N}\) holds true with a probability at least \(1-}\) for \(N 4\). Furthermore, for all \(t[N]\), we denote by \(_{t}^{B}(,)\) the algorithm with acceptance thresholds \(\{ N,t\}\) and \(\{ N,t\}\) respectively for groups \(G^{1}\) and \(G^{2}\), and we denote by \(_{N,t,k}^{B}\) the probability

\[_{N,t,k}^{B}=(_{t}^{B}(,),g^{*}_{t-1}=k_{N}). \]

Similar to the analysis of the single-threshold algorithm, we establish in Lemma C.1 a recursion formula satisfied by \((_{N,t,k}^{B})_{B,t,k}\), which we later utilize to derive lower bounds on the asymptotic success probability of \(^{B}(,)\). To prove this lemma, we study the probability distribution of the occurrence time \(_{1}\) of the first comparison made by \(^{B}_{t}(,)\), and we examine the algorithm's success probability following it. Essentially, if \(_{1}=s\), we can compute the probability of stopping and the corresponding success probability, and the distribution of the state of the algorithm at step \(s+1\), which yields the recursion. Using adequate concentration arguments and Lemma C.1, we show the two following results, giving explicit recursive formulas satisfied by the limit of \(^{B}_{N,t,k}\) when \(N\), respectively for \(k=2\) and \(k=1\).

**Lemma 4.1**.: _For all \(B 0\) and \(w[,]\), the limit \(_{2}^{B}(,;w)=_{N}^{B}_{N, wN ,2}\) exists, and it satisfies the following recursion_

\[_{2}^{B}(,;w) =- w((1-)+)+ }{(1-)w+}_{b=0}^{B}( -_{=0}^{b}}{!})\] \[+(B>0)\,w^{2}_{w}^{} w+(2-)u}{((1-)w+ u)^{2}u^{2}}_{2}^{B-1}( ,;u)du\;.\]

_Moreover, \(^{B}_{N, wN,2}=_{2}^{B}(,;w)+O\! (})\)._

**Lemma 4.2**.: _For all \(B 0\) and \(w[,]\), the limit \(_{1}^{B}(,;w)=_{N}^{B}_{N, wN ,1}\) exists, and it satisfies the following recursion_

\[_{1}^{B}(,;w) = w(1-+)+ }{(1-)w+}_{b=0}^{B}(-_{=0}^{b}}{!})\] \[+(B>0)\,^{2}w_{w}^{}^{B-1}(,;u)}{((1-)w+ u)^{2}u}du\;.\]

_Moreover, \(^{B}_{N, wN,1}=_{1}^{B}(,;w)+O\! (})\)._

We deduce that the asymptotic success probability of Algorithm \(^{B}_{ wN}\), conditioned on the event \(_{N}\), exists and equals \(_{2}^{B}(,;w)+_{2}^{B}(,;w)\). Additionally, by applying Lemma A.3, we eliminate the conditioning on \(_{N}\), thus proving the following theorem.

**Theorem 4.3**.: _For all \(0< 1\), The success probability of Algorithm \(^{B}(,)\) satisfies_

\[(^{B}(,))-O\! (})\] \[=( )+_{b=0}^{B}(-_{=0}^{b} {(1/)^{}}{!})+(B>0)\,_{}^{ }^{B-1}(,;u)du}{u^{2}}\;,\]

_with \(_{2}^{B}(,;)\) defined in Lemma 4.1._

It is possible to use Theorem 4.3 and 4.1 to numerically compute the success probability of \(^{B}(,)\). However, this computation is heavy due the recursion defining \(_{2}^{B}(,;w)\). Moreover, it is difficult to prove a closed expression, and even more to compute the optimal thresholds.

By disregarding the term containing \(^{2}(,;)\) in the theorem, we derive an analytical lower bound expressed as a function of the parameters \(\), \(B\), \(\), and \(\), allowing a more effective threshold selection. In the subsequent discussion, for all \(w(0,1]\) and \(B 0\), we denote by \(S^{B}(w)\) the following sum:

\[S^{B}(w)=_{b=0}^{B}(-_{=0}^{b}}{!})\;.\]

**Corollary 4.3.1**.: _Assume that \( 1/2\). Let \(h^{B}:\{(( )}{})\,,\,\}\), and \(_{B},_{B}\) the thresholds defined as \(_{B}=h^{B}(_{B})\), and \(_{B}\) minimizing the mapping_

\[ h()(()} )+h^{B}() S^{B}()\;,\]

_then the success probability of \(^{B}(_{B},_{B})\) satisfies_

\[_{N}(^{B}(_{B},_ {B}))-\{,(-1)(1-)\}\;.\]Therefore, in contrast to the single-threshold algorithm, the asymptotic success probability of \(^{B}(_{B},_{B})\) approaches \(1/e\) both when the budget increases and when \(\) approaches \(0\) or \(1\).

### Optimal memory-less algorithm for two groups

In the following, an algorithm is called _memory-less_ if its actions at any step \(t[N]\) depend only on the current observations \(r_{t},g_{t},(R_{t}=1)\), the available budget \(B_{t}\), and the cardinals \((|G^{k}_{t-1}|)_{k[K]}\).

We use in this section a dynamic programming approach to determine the optimal memory-less algorithm, which we denote by \(_{*}\).

Unlike previous sections, our analysis here is not asymptotic. By meticulously examining how various variables, including the precise number of candidates observed in each group, influence the success probability of \(_{*}\), we rigorously analyze its state transitions and corresponding success probabilities to determine optimal actions at each step. A full description and analysis of the optimal memory-less algorithm can be found in Section D. Here, we illustrate its actions through Figure 2.

Computing optimal thresholds for the DT algorithm.Upon observing \((r_{t},g_{t})\), \(_{*}\) makes a decision to accept or reject, where acceptance means stop if \(B=0\) and compare otherwise, depending on \(t,|G^{1}_{t}|,B,g_{t}\). Figure 2 shows its acceptance region (dark green), with \(N=500\), \(=0.7\), for \(B\{0,1,2\}\) and for all possible values of \(t[N]\), \(|G^{1}_{t}| t\), and \(g_{t}\{1,2\}\). The x- and y-axes display respectively the step \(t\) and possible group cardinal \(|G^{1}_{t}|\), which implies \(|G^{2}_{t}|\), up to time \(t\). The latter follows a binomial distribution with parameters \((,t)\), which tightly concentrates around its mean \(|G^{1}_{t}| t\) (and \(|G^{2}_{t}|(1-)t\)) even for moderate values of \(t\). Consequently, when \(N\) is large, \(|G^{1}_{t}| t\), and the acceptance region is solely defined by a threshold at the intersection of the acceptance region and the line \(|G^{1}_{t}| t\). This observation implies that \(_{*}\) behaves as an instance of DT algorithms when the number of candidates is large. The corresponding thresholds, which we denote by \((^{*}_{b},^{*}_{b})_{b B}\), are necessarily optimal, and can be estimated as the intersection of the acceptance region for \(G^{k}\) and the line \((t, t)\) for \(k\{1,2\}\).

Alternative comparison model.In the particular case of two groups, both comparison models introduced in Section 2 are equivalent, as freely comparing a candidate with the best in their group and then making one costly comparison with the best candidate from the other group is sufficient to determine if they are the best so far. Therefore, all the results of the current section regarding the static double-threshold algorithm and optimal memory-less algorithm remain true in the alternative comparison model.

## 5 Numerical experiments

In this section, we confirm our theoretical findings via numerical experiments, and we give further insight regarding the behavior of the algorithms we presented and how they compare to each other. In all the empirical experiments of this section, each point is computed over \(10^{6}\) independent trials. The code used for the experiments is available at github.com/Ziyad-Benomar/Addressing-bias-in-online-selection-with-limited-budget-of-comparisons.

### Single-threshold algorithm

Using Theorem 3.2, the optimal threshold, for the single-threshold algorithm, can be computed numerically for fixed \(K\) and \(B\). Figure 3 illustrates the optimal threshold and the corresponding success probability for \(B\{0,,30\}\) and \(K\{2,10,25,50\}\). For any \(K 2\), as the budget grows to infinity, the problem becomes akin to the standard secretary problem, leading the optimal threshold to converge to \(1/e\). However, as discussed in Corollary 3.2.1, the convergence is slower when the number of groups \(K\) is higher.

Moreover, Theorem 3.2 reveals that the asymptotic success probability is independent of the probabilities of belonging to each group, and it is equal to a value smaller than \(1/e\). This indicates a discontinuity of the success probability at the extreme points of the polygon defining the possible values of \((_{k})_{k[K]}\). Figure 4 illustrates this behavior for the case of two groups, with \(N=500\) candidates, and \(B\{0,1,2\}\). On the other hand, while our theoretical results study asymptotic success probabilities, they do not comprehend how the performance of the algorithms varies with the number of candidates. Figure 5 shows that the success probability is better when the number of candidates is small, and it decreases to match the asymptotic expression when \(N\), represented with dotted lines, for \(K\{2,3,4\}\), with \(B=3\).

### The case of two groups

In the case of two groups, Figure 4 shows that, even with a very limited budget, the single-threshold algorithm has a success probability almost indistinguishable from the upper bound \(1/e\). Consequently, in the remaining experiments in the two-group scenario, we restrict ourselves to small budgets \(B 3\).

Figure 4: Single threshold: success probability for \(2\) groups, with \(N=500\) and \(\) Figure 5: Convergence to the asymptotic success probability, with \(_{k}=1/K\) for all \(k[K]\)

Figure 3: Single threshold algorithm: optimal threshold and corresponding success probability

For \(=0.5\), both groups have symmetric roles, and the optimal thresholds \(_{B}\) and \(_{B}\) to choose in the static-threshold algorithm are identical. Hence, the success probability of \(^{B}(_{B},_{B})\) for \(=0.5\) is exactly that of the single-threshold algorithm, which is independent of \(\) (Theorem 3.2). We deduce from this observation and Figure 6 that having different thresholds for each group yields a substantial improvement over the single-threshold algorithm when \(\) is close to \(0\) or \(1\).

Finally, to emphasize that the dynamic programming algorithm \(_{*}\) is equivalent to an instance of DT algorithm for large \(N\), Figure 7 compares the empirical success probabilities of \(_{*}\) (dotted lines) and the DT algorithm (solid lines) with thresholds \((_{B}^{},_{B}^{})_{B 0}\), computed as explained in Section 4.1.

For \(\{0.5,0.7,0.95\}\), the figure confirms that, despite the intricate structure of the optimal memory-less algorithm, it does not surpass the performance of the DT algorithm with optimal thresholds when \(N\) is large. Nonetheless, the analysis of the optimal memory-less algorithm is what enables the numerical computation of the optimal thresholds, as explained previously. Figure 8 shows the optimal thresholds \(_{b}^{},_{b}^{}\) for all \([0.5,1]\) and \(B\{0,1,2\}\).

These thresholds are continuous functions of \(\), both converging to \(1/e\) very rapidly as the budget increases. Indeed, for \(B 1\), they both become very close to \(1/e\), as for \(B=0\), the optimal thresholds are exactly equal to \(_{0}^{}=(-2)\) and \(_{0}^{}=\), which correspond to the optimal thresholds described in Corollary 4.3.1 for \(B=0\) (See the proof of the corollary).

## 6 Conclusion and future work

This paper explores more realistic online selection processes, wherein nuanced factors such as imperfect comparisons and optimal budget utilization are considered. We studied a partially ordered secretary problem, wherein a constrained budget of comparisons is allowed. Our findings encompass both asymptotic and non-asymptotic analyses. Specifically, we explore the asymptotic behavior of the single threshold algorithm with \(K\) groups, demonstrating its high efficiency with a non-zero budget. Furthermore, in the context of two groups, we study the success probability of static double-threshold algorithms, and we present a non-asymptotic optimal memoryless algorithm. Through numerical experimentation, we demonstrate that this algorithm behaves as a DT algorithm, and, leveraging this insight, we show how to numerically compute the optimal DT thresholds. However, a limitation of the paper is that optimal thresholds are only computed in the case of two groups. Future investigation could explore methods for numerically or analytically characterizing optimal DT thresholds with an arbitrary number of groups \(K\).

Figure 7: Empirical success probabilities of \(_{*}\) and the DT algorithm with optimal thresholds