# Graph Contrastive Learning with Stable and Scalable Spectral Encoding

Deyu Bo\({}^{1}\)1, Yuan Fang\({}^{2}\), Yang Liu\({}^{1}\), Chuan Shi\({}^{1}\)2

\({}^{1}\)Beijing University of Posts and Telecommunications, China

\({}^{2}\)Singapore Management University, Singapore

{bodeyu, liuyangjanet, shichuan}@bupt.edu.cn, yfang@smu.edu.sg

###### Abstract

Graph contrastive learning (GCL) aims to learn representations by capturing the agreements between different graph views. Traditional GCL methods generate views in the spatial domain, but it has been recently discovered that the spectral domain also plays a vital role in complementing spatial views. However, existing spectral-based graph views either ignore the eigenvectors that encode valuable positional information, or suffer from high complexity when trying to address the instability of spectral features. To tackle these challenges, we first design an informative, stable, and scalable spectral encoder, termed EigenMLP, to learn effective representations from the spectral features. Theoretically, EigenMLP is invariant to the rotation and reflection transformations on eigenvectors and robust against perturbations. Then, we propose a spatial-spectral contrastive framework (Sp\({}^{2}\)GCL) to capture the consistency between the spatial information encoded by graph neural networks and the spectral information learned by EigenMLP, thus effectively fusing these two graph views. Experiments on the node- and graph-level datasets show that our method not only learns effective graph representations but also achieves a 2-10x speedup over other spectral-based methods.

## 1 Introduction

Graph neural networks (GNNs) have become the _de facto_ framework to encode graph-structured data . However, training high-quality GNNs usually requires a large number of domain-specific labels, which is not feasible in many real-world applications. Therefore, as a paradigm of self-supervised learning, graph contrastive learning (GCL) is proposed to learn node or graph representations without using labels .

Typically, GCL methods first generate different views of a graph, and then contrast the positive views against the negative ones. By minimizing a contrastive loss, GCL methods can learn invariant information from different views for various downstream tasks. Therefore, how to generate ideal graph views is crucial to GCL. Most graph views are obtained by augmenting graphs in the spatial domain, such as dropping nodes and edges, heuristically  or adversarially . Nevertheless, recent studies  argue that spatial perturbations ignore the structural properties, and propose to perturb graph spectrum in the spectral domain.

Generally, the spatial and spectral views represent different information of the graph. The spatial domain captures the feature information and learns local graph representations by propagating the node features along local topology, _i.e._, \(k\)-hop subgraphs. In contrast, the spectral domain covers the global structural information. The eigenvalues and eigenvectors encode the global shapes  andnode absolute positions . Leveraging the agreements between the spatial and spectral views can significantly improve the expressive power and generalization ability of GNNs [37; 47]. However, several reasons hinder the study of spectral view: First, the spectral features are unstable. Previous work  shows that randomly flipping the signs or rotating the coordinates of eigenvectors also satisfies the eigenvalue decomposition, _a.k.a._, the sign and basis ambiguity issues, implying that spectral features are not unique and hard to transfer [15; 37]. Besides, perturbing spectral features is time-consuming. Existing spectral-based methods [18; 17] need to decompose and reconstruct the adjacency matrix, in which the complexity is cubic and quadratic in the number of nodes, respectively.

Therefore, to realize the spatial-spectral contrast, it is natural to ask: _How to encode the spectral view of a graph effectively and efficiently?_ To answer this question, we first propose a spectral encoder, named EigenMLP, which not only inherits the scalability advantage of multilayer perceptrons (MLP), but also adheres to two key design principles. First, EigenMLP resolves the sign ambiguity issue by taking both positive and negative eigenvectors as input. Second, to address the basis ambiguity issue, the weights of EigenMLP are generated by a learnable mapping of the eigenvalues, making them equivariant to the coordinates of eigenvectors. To integrate the representations learned from the spatial and spectral views, we propose a Spatial-Spectral GCL framework (Sp\({}^{2}\)GCL) to maximize the agreements between views and learn effective representations for downstream tasks.

The contributions of our paper are as follows. (1) We propose EigenMLP, a novel spectral encoder to effectively and efficiently learn sign- and basis-invariant representations from spectral features. (2) We theoretically prove that EigenMLP is permutation-equivariant, rotation- and reflection-invariant, and can learn more stable representations against structural perturbations. (3) We propose Sp\({}^{2}\)GCL, a spatial-spectral GCL framework that utilizes the cross-domain contrasts to effectively fuse the feature and structural information learned by spatial GNNs and EigenMLP. (4) Extensive experiments on both node-level and graph-level tasks demonstrate the effectiveness of the proposed framework Sp\({}^{2}\)GCL, and verify the scalability and stability aspects of EigenMLP.

## 2 Related Work

Graph Contrastive Learning.Most GCL methods aim to learn invariant information by maximizing the mutual information between different graph views [1; 36]. There are many ways to generate graph views and we broadly categorize them into the spatial and spectral approaches. In the spatial domain, graph views are usually generated by augmenting the original graphs. For example, GRACE , GCA  and CCA-SSG  propose to augment graphs by randomly dropping edges and nodes, AD-GCL  leverages adversarial training to filter unimportant edges, and JOAO  combines different augmentation strategies automatically. While in the spectral domain, the views are generated by perturbing graph spectrum. For example, MVGRL  heuristically uses a graph diffusion as augmentation, which acts as a low-pass filter, SpCo  proposes to preserve low-frequency components and perturbs high-frequency ones, and SPAN  generates augmentations by maximizing the spectral change. Besides, SFA  analyzes the spectrum of node representations, which is out of the scope of this work. In general, spectral views of graphs may have better performance and interpretability than spatial views but also suffer from high complexity.

Spectral Encoder.Perturbing the graph spectrum is time-consuming. Another approach is to encode the eigenvalues and eigenvectors instead of the eigenspaces. However, eigenvectors suffer from sign and basis ambiguity issues, and using these features directly will affect the stability of the model. Therefore, some spectral encoders are proposed to learn invariant representations from the non-unique spectral features. For example, SAN  uses a Transformer-based  encoder, which is invariant to the order of the bases. BasisNet  uses IGN  to learn permutation-invariant representations from the eigenspaces. PEG  leverages the distance between eigenvectors to reweigh the graph structure and avoids sign and basis ambiguity. However, the complexity of SAN and BasisNet is quadratic, which is difficult to scale to large graphs.

## 3 Preliminaries

Assume that \(=(,)\) is a graph, where \(\) is the node set with \(||=N\) and \(\) is the edge set with \(||=E\). Let \(\{0,1\}^{N N}\) be the adjacency matrix, and \(^{N d}\) be the node feature matrix on \(\). The normalized graph Laplacian \(\) of \(\) is defined as \(=_{n}-^{-}^{-}\), where \(_{n}\) is the \(N N\) identity matrix and \(\) is the degree matrix with \(D_{ii}=_{j}A_{ij}\) for \(i\) and \(D_{ij}=0\) for \(i j\). The eigenvalue decomposition (EVD) of graph Laplacian is defined as \(=^{}\), where \(\)is a diagonal matrix whose diagonal entries \(0_{1}_{N} 2\) are the eigenvalues of \(\), and \(=[_{1},,_{N}]\) are the corresponding eigenvectors.

It is worth noting that in some cases, randomly flipping the signs and rotating the coordinates of eigenvectors may also satisfy EVD , which we refer to as sign and basis ambiguity.

Sign ambiguity.Given a pair of eigenvalue and eigenvector \((_{i},_{i})\), it satisfies \(_{i}=_{i}_{i}\), and \(_{i}=_{i}^{}_{i}=_{(v,v^{}) }(u_{iv}-u_{iv^{}})^{2}\). Therefore, if \(_{i}\) is an eigenvector of \(\), then \(-_{i}\) also satisfies EVD, _i.e._, \(_{i}^{}_{i}=(-_{i})^{}(- _{i})\).

Basis ambiguity.If there is high multiplicity in the eigenvalues, _i.e._, \(_{p+1}==_{p+q}\) for some \(q>1\), then the corresponding eigenvectors \([_{p+1},,_{p+q}]\) lie in an orthogonal group O(\(q\)) \(=\{^{q q}|^{}=^ {}=_{q}\}\). Therefore, for any \(\) O(\(q\)), replacing \([_{p+1},,_{p+q}]\) with \([_{p+1},,_{p+q}]\) also satisfies EVD, _i.e._, \(_{i}=_{j}_{i},p+1 i,j p+q\).

The above two facts state that the eigenvectors of graph Laplacian are not unique. Therefore, the model should consider how to learn sign- and basis-invariant representations from spectral features for better stability and generalization [37; 16; 15].

## 4 Proposed Framework: Sp\({}^{2}\)GCL

In this section, we present the proposed spatial-spectral GCL framework called Sp\({}^{2}\)GCL. We first give a high-level overview on how to represent and contrast the spatial and spectral views of a graph. We then introduce the proposed invariant and equivariant spectral view encoder in detail. Finally, we briefly describe the preprocessing, training, and inference processes.

### Overview of Sp\({}^{2}\)GCL

Views refer to different perspectives of the same data . Unlike previous GCL methods that generate graph views in a single domain, we propose to model the spatial and spectral views separately and further utilize cross-domain contrasts to capture invariant information. Here we first describe how to represent the spatial and spectral views of graphs.

**Spatial View** represents the explicit connectivity of nodes, which can be denoted as \(_{a}=(,)\). Through propagating node features along graph substructures, the spatial view can naturally fuse the topology and content information and learn _local smooth representations_ of a graph.

**Spectral View** indicates the implicit relationships between nodes, which is expressed as \(_{e}=(,)\). The eigenvalues and eigenvectors encode the geometric information and node positions of the graph topology, which can be seen as _global structural information_ of a graph.

Since graphs are non-Euclidean data, it is difficult to directly contrast the spatial and spectral views. Therefore, we need to design suitable encoders to learn different view representations for GCL:

\[_{a}=f(,),_{e}=g(,), \]

where \(f\) and \(g\) are the encoders of the spatial and spectral views, respectively, and \(_{a},_{e}^{N d}\) are the spatial and spectral representation matrices, respectively. These representations can then be used in contrastive learning to learn invariant information across both domains.

The fundamental idea of contrastive learning is to define the positive and negative pairs, from which the model can capture the self-supervised signals. In our framework, we define the spatial and spectral representations of the same node or graph as positive pairs, and those of different nodes or graphs as negative pairs. For graph-level contrasts, we additionally use a readout function to learn the graph-level representations. Then, two projection heads \(_{a},_{e}:^{d}^{d}\) are used to transform the representations into the contrastive space:

\[_{a}=_{a}(_{a}),_{e}=_{e}( _{e}). \]

Subsequently, we employ InfoNCE , a classical contrastive objective function, to maximize the agreements between spatial and spectral representations:

\[=-_{i=1}^{N}(_{a }^{i},_{e}^{i}}}{e^{_{a}^{i},_{e}^ {i}}+_{j i}e^{_{a}^{i},_{e}^{i} }}+_{e}^{i},_{e}^{i}}}{e^ {_{e}^{i},_{e}^{i}}+_{j i}e^{ _{e}^{i},_{e}^{i}}}), \]where \(,\) represents the cosine similarity and \(i\) is the index of nodes or graphs.

Finally, to materialize our contrastive framework \(^{2}\), we must choose or design the encoders \(f\) and \(g\) for the two views. For the spatial encoder \(f\), we directly employ a standard message-passing neural network (MPNN), _e.g._, GraphSAGE  or GIN , which is widely adopted in previous GCL methods. For the spectral encoder \(g\), which is the emphasis of this work, we propose EigenMLP in the next part (Section 4.2) based on several key properties. Note that, on the one hand, although MPNNs can learn useful representations, their expressive power is bounded by the 1-Weisfeiler-Lehman (WL) test . On the other hand, the spectral view encodes the global information, which can help overcome the limitation of the 1-WL test . Hence, from the perspective of expressive power, the spectral view is also complementary to the spatial view.

### Proposed Spectral Encoder: EigenMLP

The spectral encoder is designed to learn stable representations from spectral features. A desirable spectral encoder should have three properties: (1) It can encode both the information of eigenvalues and eigenvectors, which represent different structural information ; (2) It is free of the sign and basis ambiguity issues in spectral features, toward learning stable representations ; (3) It is scalable to large graphs and has linear or sublinear time complexity. These three properties pose great challenges to the design of spectral encoders. Here we propose EigenMLP, which adopts an MLP-based architecture for scalability and addresses the sign and basis ambiguity problems for stability. Table 1 summarizes the differences between the proposed EigenMLP and existing spectral encoders. Specifically, EigenMLP satisfies all of the three properties above, as we elaborate below.

Scalability.In general, MLPs have demonstrated good scalability and have been widely used in learning graph representations . Since EigenMLP employs an MLP-based architecture, it inherits the low complexity and is linear to the number of nodes \(N\).

Stability.MLPs are sensitive to input data, making the outputs vary with respect to the flipping of signs and rotation of coordinates of eigenvectors. This motivates us to increase the stability of MLP while maintaining high efficiency. We adopt two design principles that enable MLPs to learn sign- and basis-invariant representations. Figure 1 illustrates the difference between EigenMLP and MLP.

To address the sign ambiguity issue, we follow the design of SignNet , taking both positive and negative eigenvectors as inputs:

\[}=[((_{i})+(-_{i}))]_{i=1}^{ N}, \]

where \(\) and \(\) are neural networks, \([]\) is the concatenation operator, and \(}\) is the sign-invariant eigenvectors. In practice, the sign-invariant neural networks may slow down model convergence. In this case, we can resort to some heuristics to determine the sign of the eigenvectors .

To solve the basis ambiguity issue, we first observe that each eigenvector has a corresponding eigenvalue, and when the coordinates of the eigenvectors are rotated, the positions of the eigenvalues are also displaced. Therefore, if we replace the weights of MLP with eigenvalues \(=[_{1},_{2},,_{N}]\), the model will be invariant to the rotation of eigenvectors, _i.e._, \(()^{}=^{}^{ }=^{}\). However, directly replacing the learnable weights with fixed eigenvalues is trivial and will greatly

    &  &  &  \\   & EigVal & EigVec & Sign & Basis & Inductive & Complexity \\  MLP  & & & & & & \(\) & \((Nkd)\) \\ SAN  & & & & & & \(\) & \((Nk^{2}d+Nkd)\) \\ BasisNet  & & & & & \(\) & \(\) & \((Nk^{2}d+Nkd)\) \\ PEG  & & & & & \(\) & \(\) & \(\) & \((k^{2}d)\) \\  EigenMLP & & & & & & & \\   

Table 1: Comparison between different spectral encoders.

limit the expressive power of the model. Therefore, we first extend the scalar eigenvalues to their high-dimensional Fourier features . The weights of EigenMLP are then decoded from the Fourier features using a learnable matrix:

\[()=[(),(),(2),(2), (T),(T)]_{}, \]

where \(T\) is the period and \(_{}^{2T d}\) is a learnable matrix. Here \(()\) can be seen as a graph filter and the filtered eigenvalues are equivariant to the coordinates of eigenvectors , which can be used to learn powerful and basis-invariant spectral representations:

\[g(,)=}(), \]

where \(()=[(_{1}),(_{2}),,(_{ N})]^{}^{N d}\) represents all the filtered eigenvalues. The detailed matrix form of Equation (6) is provided in Appendix C. Note that the learnable matrix \(_{}\) is shared between different eigenvalues. Therefore, the size of \(_{}\) is independent of the number of eigenvalues but depends on the period \(T\), which reduces the number of parameters in EigenMLP. More discussions can be found in Section 5.2.

Information.Notably, the invariant layer, _i.e._, Equation (6), in EigenMLP incorporates both eigenvalues and eigenvectors, which can capture both geometric and positional information.

### Preprocessing, Training, and Inference

It is worth noting that running EVD for full eigenvectors has the complexity \((N^{3})\), which is unacceptable for large graphs. Therefore, in the preprocessing, we use the eigenvectors with smallest-\(k\) eigenvalues as a substitute and reduce the complexity to \((N^{2}k)\). Besides, we can pre-calculate the rotation-invariant spectral features \((_{k})\). Therefore, in the training and inference, EigenMLP has the implementation of MLP. Note that the decomposition is only computed once in the training. Therefore, the overhead of the preprocessing should be amortized by each training epoch.

In training, traditional GCL methods need to compute the message-passing twice for different graph views, whose complexity is \((E)\). While in our framework, only the spatial view needs to calculate the message-passing, and the complexity of spectral view is \(}(N)\). Therefore, our framework is faster than others in the training processing. In inference, because we need to calculate the spatial and spectral representations separately, the inference speed is slightly lower than traditional methods. The overheads of training and inference are provided in Section 6.6.

## 5 Deeper Insights

In this section, we provide deeper insights into EigenMLP to understand its effectiveness. Specifically, we prove that EigenMLP is invariant, equivariant, and stable, and can generalize existing spectral augmentations. Besides, we also comment on the connections to previous work.

### Theoretical Results

**Theorem 1**.: _EigenMLP is equivariant to permutation, and invariant to rotation, and reflection._

Proof.: (Permutation) Assume that there are two matrices \(^{(1)},^{(2)}^{N N}\), and \(^{(1)}=^{(2)}^{}\). We have \(^{(1)}=^{(2)}^{}=(^{(2)})(^{(2)})^{}\) such that \(^{(1)}()=^{(2)}()\).

(Rotation) For any rotation matrix \((N)\), the eigenvectors are rotated as \(\), and the corresponding Fourier matrix are permuted as \(^{}()\). Therefore, we have \(^{}()=()\).

(Reflection) Following Proposition 1 in , a continuous function is sign-invariant iff it satisfies \(f(v)=h(v)+h(-v)\). Hence, EigenMLP is invariant to reflection on the signs of eigenvectors. 

**Lemma 1**.: _(Lemma 3.4 in ) For any positive semidefinite \(\) without multiple eigenvalues, set \(\) as the eigenvectors corresponding to the smallest \(k\) eigenvalues and sorted as \(0<_{1}<<_{k}\). For any sufficiently small \(>0\), there exists a perturbation \(\), \(\|\|_{F}<\) such that_

\[_{(k)}\|(+)- \|_{F} 0.99_{1 i k}|_{i+1}-_{i}|^{-1}|| ||_{F}+o(). \]

**Theorem 2**.: _For any sufficiently small \(>0\), there exists a perturbation \(\), \(\|\|_{F}<\) such that_

\[_{(k)}\,||(+)( _{k})-(_{k})||_{F} 0.99T _{1 i k}|_{i+1}-_{i}|^{-1}||||_{F}+o( ). \]

Lemma 1 states that a small structural perturbation will produce changes unbounded from above in non-equivariant spectral features \(\) if there is a small spectral gap. Theorem 2 shows that there is a finite upper bound on the changes of equivariant spectral features \((_{k})\). Therefore, EigenMLP can learn more stable representations against structural perturbations. We present the proof of Theorem 2 in Appendix A. Experiments in Section 6.5 further give an empirical justification of Theorem 2.

**Proposition 1**.: _EigenMLP generalizes existing spectral augmentations._

Proof.: Existing spectral augmentations, such as SpCo  and SPAN , generate graph views by perturbing the graph spectrum. We assume that there is a continuous univariate function \(()\) between the original eigenvalues \(\) and the perturbed eigenvalues \(^{}\), such that \(^{}_{i}=(_{i})\), and the perturbed eigenvalues are still non-negative. Then these spectral augmentations can be represented as:

\[^{}=()^{}=( ()^{})(( )^{})^{}. \]

Note that Equation (5) is a Fourier series and can approximate any continuous functions. Therefore, the function \(()\) is a special case of \(()\), and EigenMLP can approximate the simplex geometry  of the augmentations, _i.e._, \(()^{}\), thus generalizing existing spectral augmentations. 

### Connections to Existing Work

**Hypernetworks** are a class of neural networks used to generate parameters for another network. EigenMLP can be seen as a special case of Hypernetworks, which takes eigenvalues as input and generates equivariant parameters for spectral features. Because Hypernetworks can generate a set of non-shared weights from shared parameters, it can greatly reduce the number of trainable parameters. EigenMLP inherits this advantage, and its number of parameters is only related to the period of the Fourier series, rather than the number of eigenvectors. Therefore, EigenMLP is more efficient than other spectral encoders, including vanilla MLP.

**Spectral GNNs** aim to combine the eigenspaces with filtered eigenvalues, _i.e._, \(_{i=1}^{N}(_{i})_{i}_{i}^{}\), and EigenMLP is used to combine the eigenvectors with filtered eigenvalues, _i.e._, \(_{i=1}^{N}(_{i})_{i}\). Therefore, both methods choose to use eigenvalue mappings as weights to guarantee the equivariance. However, the calculation of eigenspaces has the complexity of \((N^{2})\), which is not suitable for large graphs, whereas EigenMLP is more scalable.

## 6 Experiments

In this section, we conduct three types of experiments, including unsupervised node classification, unsupervised graph prediction, and transfer learning, to verify the effectiveness of Sp\({}^{2}\)GCL. Besides, we also test the stability and time overhead of the proposed method.

### Unsupervised Node Classification

**Datasets.** In the node classification task, we consider using graphs with different scales to evaluate both the effectiveness and scalability of GCL methods. Specifically, for the small graphs (< 50,000), we use Pubmed , Wiki-CS , and Facebook  datasets. For the large graphs (> 50,000), we use Flickr , arXiv , and PPI  datasets. Additional statistics are provided in Appendix B.

**Baselines and Setting.** We compare our model against a wide range of baselines, including semi-supervised GNNs, _e.g._, GCN  and GAT , graph self-supervised learning methods, _e.g._, DGI  and BGRL , GCL with spatial augmentations, _e.g._, MVGRL , GRACE , and CCA-SSG , and GCL with spectral augmentations, _e.g._, SpCo  and SPAN . For the Facebook dataset, we randomly split the nodes into train/validation/test data with a ratio of 1:1:8. For other datasets, we use the public splits for a fair comparison. We use a two-layer GCN as the encoder forall datasets and set the hidden dimension \(d=512\) for all methods. For our model, the spatial encoder is the same as baselines, and we additionally use EigenMLP to learn the spectral representation. In the evaluation, we use a linear classifier to evaluate the performance of all methods, as suggested by . We run all the models 10 times and report the mean accuracy and standard deviation. More details, _e.g._, optimizers, and hyperparameters, are provided in Appendix B.

Results.From Table 2, we can find that Sp\({}^{2}\)GCL consistently outperforms state-of-the-art baselines on 5 out of 6 datasets, which validates the effectiveness of the proposed spatial-spectral contrastive framework. Meanwhile, spectral-based methods are proven to be more effective than spatial-based methods, suggesting that integrating spectral information into GCL can help models learn better representations. However, it is worth noting that neither SpCo nor SPAN works for large graphs, implying that perturbing graph spectrum cannot be scalable to large-scale datasets. Therefore, the application scenarios of these two graph augmentations are limited. On the contrary, our method can be used for large graphs and can be easily trained in a mini-batch manner, which is more scalable than other spectral-based methods. Additionally, We find that Sp\({}^{2}\)GCL does not perform well in the Wiki-CS dataset. The reason is that the node features dominate the classification results while the graph structure contributes less. Therefore, the spectral view cannot complement the spatial view.

### Unsupervised Graph Prediction

Setup.We benchmark our model on the OGB graph property prediction task , which contains three regression datasets and five classification datasets. We choose a series of competitive GCL methods as baselines, including InfoGraph , GraphCL , MVGRL , JOAO , AD-GCL , and SPAN . It is worth noting that SpCo  is not designed for graph-level contrastive learning, so we do not compare with it. We use a five-layer GIN  with a graph pooling layer as the encoder for all methods. Similarly, we additionally use EigenMLP to encode the spectral view for Sp\({}^{2}\)GCL. We use a linear downstream classifier, _e.g._, logistic regression model, to evaluate the performance of different GCL methods, as suggested by .

    &  &  &  \\   & & PubMed & Wiki-CS & Facebook & arXiv & Flickr & PPI \\  GCN & \(,,\) & 79.0 & 77.19±0.12 & 90.65±0.16 & 71.74±0.29 & 49.20±0.31 & 82.28±0.24 \\ GAT & \(,,\) & 79.0±0.3 & 77.65±0.11 & 90.47±0.15 & 71.82±0.23 & 54.48±0.21 & 98.85±0.05 \\  DGI & \(,\) & 76.8±0.6 & 75.35±0.14 & 84.42±0.43 & 70.32±0.25 & 50.59±0.28 & 63.80±0.20 \\ BGRL & \(,\) & 79.6±0.5 & 79.98±0.13 & 89.71±0.35 & 71.54±0.17 & 51.87±0.15 & 73.63±0.16 \\ MVGRL & \(,\) & 80.1±0.7 & 77.52±0.08 & 87.29±0.28 & - & - & 71.45±0.14 \\ GRACE & \(,\) & 80.6±0.4 & 80.14±0.48 & 89.32±0.40 & - & - & 69.71±0.17 \\ CCA-SSG & \(,\) & 81.0±0.4 & 78.85±0.32 & 89.45±0.60 & 71.21±0.20 & 51.66±0.10 & 73.34±0.17 \\  SpCo & \(,,\) & 81.5±0.4 & 79.16±0.27 & 89.98±0.45 & - & - & - \\ SPAN & \(,,\) & 81.5±0.2 & **82.13±0.15** & - & - & - & - \\  Sp\({}^{2}\)GCL & \(,,,\) & **82.3±0.3** & 79.42±0.19 & **90.43±0.13** & **71.83±0.19** & **52.05±0.33** & **74.28±0.22** \\   

Table 2: Node classification on transductive and inductive graphs. Mean accuracy (%) \(\) standard deviation. Bold indicates the best performance and “-” means out-of-memory or cannot be reproduced.

   Task &  &  \\  Dataset & molesol & mollipo & molfreesolv & molluce & mollobpp & molclintox & molcox21 & molsider \\  Supervised & 1.173±0.057 & 0.757±0.018 & 2.755±0.349 & 72.97±4.00 & 68.17±1.48 & 88.14±2.51 & 74.91±0.51 & 57.60±1.40 \\  InfoGraph & 1.34±0.178 & 1.005±0.023 & 1.000±0.819 & 74.74±3.64 & 66.33±2.79 & 64.50±3.52 & 69.74±0.57 & 60.54±0.90 \\ Grapbell. & 1.272±0.089 & 0.910±0.016 & 7.697±2.48 & 74.32±2.70 & 68.22±1.89 & 74.92±4.47 & 72.40±1.01 & 61.76±1.11 \\ MVGRL & 1.433±0.145 & 0.962±0.036 & 9.042±1.982 & 74.20±2.31 & 67.24±1.39 & 73.84±2.45 & 70.48±0.83 & 61.94±0.94 \\ JOAO & 1.285±0.121 & 0.865±0.032 & 5.131±0.722 & 74.43±1.94 & 67.62±1.29 & 78.72±1.42 & 71.83±0.92 & 62.73±0.92 \\ AD-GCL & **1.271±0.087** & 0.842±0.028 & 5.150±0.624 & 76.37±2.03 & 68.21±4.47 & 80.77±3.92 & 71.42±0.73 & 63.91±0.95 \\ SPAN & 1.218±0.052 & **0.802±0.019** & 4.531±0.463 & 76.74±2.02 & **69.59±1.34** & 80.28±2.42 & 72.83±0.62 & **64.87±0.88** \\  Sp\({}^{2}\)GCL & 1.235±0.119 & 0.835±0.026 & **4.144±0.573** & **78.76±1.43** & 68.72±1.53 & **80.88±3.36** & **73.06±0.75** & 64.23±0.96 \\   

Table 3: Results on the graph-level tasks. The best and runner-up results are highlighted with **bold** and underline, respectively. \(\) means lower the better, and \(\) means higher the better.

[MISSING_PAGE_FAIL:8]

GRACE. Nevertheless, the performance is still inferior to that of Sp\({}^{2}\)GCL, thus providing further evidence of the effectiveness of spatial-spectral contrast.

### Stability of EigenMLP and MLP

We conduct stability experiments to evaluate whether EigenMLP and MLP can learn stable representations against perturbations. We consider two types of perturbations: 1) Synthetic perturbation, which applies random reflection \(\) and rotations \(\) on the eigenvectors. 2) Practical perturbation, which decomposes the Laplacian matrix with different tolerances (\(10^{-3},10^{-4},10^{-5}\)). The corresponding eigenvectors are expressed as \(_{3}\), \(_{4}\), and \(_{5}\). Note that synthetic perturbation only changes the signs and coordinates of eigenvectors while practical perturbation is more challenging because it perturbs the values. For clearer results, we only evaluate the performance of spectral representations.

For each type of perturbation, we construct three instances, _i.e._, (\(\), \(\)**II**, \(\)**Q**) or (\(_{3}\), \(_{4}\), \(_{5}\)). The models are trained on one instance and tested on the other two instances on PubMed. We report the results of EigenMLP and show the performance change after switching to MLP in the brackets. From Table 7, we can find that the synthetic perturbation can hardly change the representations learned by EigenMLP but has a great influence on MLP. Additionally, Table 8 shows that practical perturbations can affect both EigenMLP and MLP. Nevertheless, EigenMLP still consistently outperforms MLP across different tolerances, which verifies the stability of EigenMLP.

### Time Overhead

To assess the efficiency, we compare the time overhead of Sp\({}^{2}\)GCL with other GCL methods as well as compare EigenMLP with different spectral encoders. The time costs associated with preprocessing, training, and inference are shown in Table 9. Previous spectrum-based methods have significant time costs in the preprocessing stage, _e.g._, graph diffusion (GD) or Sinkhorn's Iteration (SI), whereas Sp\({}^{2}\)GCL exhibits minimal overhead. In the training stage, Sp\({}^{2}\)GCL outperforms other methods due to the efficient spectral encoding, but it also introduces an additional burden during inference. Table 10 illustrates the overhead imposed by various spectral encoders within the Sp\({}^{2}\)GCL framework. Both MLP and EigenMLP exhibit remarkable efficiency. Conversely, SAN and BasisNet entail excessive time costs due to their quadratic time complexity.

## 7 Conclusion

In this study, we introduce Sp\({}^{2}\)GCL, a novel spatial-spectral GCL framework that learns the consistency between the spatial and spectral views of graphs. To effectively and efficiently learn the spectral view information, we propose EigenMLP, a scalable spectral encoder to learn stable spectral representations from the non-unique spectral features. Extensive experiments on various graph-related tasks demonstrate the effectiveness, efficiency, and stability of the proposed method.

    &  \\   & &  &  &  &  \\   & &  &  &  &  \\   &  &  &  &  &  \\  & &  &  &  &  &  \\  & &  &  &  &  &  \\   & &  &  &  &  &  \\   

Table 8: Practical perturbations: Eigenvectors with different EVD tolerances.

    &  \\   &  &  &  &  &  \\   &  &  &  &  &  \\   & **U** & 78.8 (40.1) & 78.3 (-5.5) & 78.8 (-6.4) \\  & **UII** & 78.5 (-7.8) & 78.9 (-0.4) & 79.0 (-5.2) \\  & **UQ** & 78.7 (-5.1) & 78.8 (-9.3) & 78.9 (+0.0) \\   

Table 7: Synthetic perturbations: Eigenvectors with random reflection \(\) and rotation \(\).

    &  \\   &  &  &  &  \\   &  &  &  &  \\   & **U** & **V** & **78.6 (+0.3) & 73.9 (-1.5) & 73.7 (-7.8) \\  & **U4** & 75.1 (-3.2) & 78.2 (+0.3) & 72.8 (-6.8) \\  & **U5** & 73.7 (-6.9) & 72.0 (-5.0) & 79.7 (-0.5) \\   

Table 9: Time overheads (s) of different GCL methods.

Limitation and Broader ImpactCurrently, we focus on encoding spectral features while ignoring node features. A promising future direction is to unify these two kinds of information to learn effective graph representations. Our work reveals the superiority of integrating spectral information into GCL and may inspire the community to pay more attention to the spectral view of graphs.