# NPCL: Neural Processes for Uncertainty-Aware

Continual Learning

 Saurav Jha

UNSW Sydney

saurav.jha@unsw.edu.au &Dong Gong

UNSW Sydney

dong.gong@unsw.edu.au &He Zhao

CSIRO's Data61

he.zhao@ieee.org &Lina Yao

CSIRO's Data61, UNSW Sydney

lina.yao@data61.csiro.au

D. Gong is the corresponding author.

###### Abstract

Continual learning (CL) aims to train deep neural networks efficiently on streaming data while limiting the forgetting caused by new tasks. However, learning transferable knowledge with less interference between tasks is difficult, and real-world deployment of CL models is limited by their inability to measure predictive uncertainties. To address these issues, we propose handling CL tasks with neural processes (NPs), a class of meta-learners that encode different tasks into probabilistic distributions over functions all while providing reliable uncertainty estimates. Specifically, we propose an NP-based CL approach (NPCL) with task-specific modules arranged in a hierarchical latent variable model. We tailor regularizers on the learned latent distributions to alleviate forgetting. The uncertainty estimation capabilities of the NPCL can also be used to handle the task head/module inference challenge in CL. Our experiments show that the NPCL outperforms previous CL approaches. We validate the effectiveness of uncertainty estimation in the NPCL for identifying novel data and evaluating instance-level model confidence. Code is available at [https://github.com/srvCodes/NPCL](https://github.com/srvCodes/NPCL).

## 1 Introduction

Continual learning (CL) aims to help deep neural networks (DNNs) learn from a stream of non-stationary tasks by retaining the previously acquired knowledge . To achieve this, CL agents target alleviating the _catastrophic forgetting_ issue with restricted computational and memory costs . This requires balancing the plasticity for new knowledge with the stability for old .

To handle forgetting in CL, experience replay (ER) methods  are one effective way to train DNNs on a memory buffer with a subset of the past tasks' experiences. Other than the ER methods, many regularization-based approaches have been proposed to penalize the forgetting on the DNNs' parametric  or representation spaces . However, these may still suffer from interference due to the regularization on the entire parameter space . To address this, parameter isolation methods  define task-specific training components but are usually confined to task incremental CL setups requiring task ID during testing . It is thus challenging for CL agents to maintain transferable and shareable knowledge. Lastly, a hurdle to the real-world deployment of CL agents is their inability to measure predictive uncertainties, which impacts the potential utilization of CL across various practical applications, particularly those with critical safety considerations .

To tackle the above issues, we propose to explore CL models using neural processes (NPs) [13; 14], a class of meta-learners that model tasks as data-generating functions from a stochastic process. NPs learn a prior over functions by marginalizing over a set of data points, or _context_, thus enabling rapid adaptation to new observations through inference on functions. Additionally, their probabilistic nature endows them with reliable uncertainty quantification capabilities [14; 27; 24; 25]. Our motivations to explore NPs for CL are thus two-fold. First, NPs exploit Bayes' theorem, which naturally enables CL through sequential posterior construction. Namely, NPs perform inference over the function space by learning context-based priors, which are updated to posteriors upon observing (additional) _targets_. Second, NPs meta-learn input correlations through a set of latent variables, which could be a key to meta-learn knowledge transfer across multiple correlated tasks. However, NPs face challenges in directly addressing CL tasks, given that (a) the reliance on a single global latent leads to suboptimal modeling of complex CL tasks where multiple correlated tasks could occur simultaneously, (b) NPs cannot directly handle the forgetting of past task correlations arising from the non-static data stream.

To address the above desiderata, we propose Neural Processes for Continual Learning (NPCL), a hierarchical latent variable model with a global latent variable to capture inter-task correlation and task-specific latent variables for finer knowledge. Fig. 1 shows the NPCL exploiting functional correlation among current and past task training samples of ER. The drift of global and past task-specific distributions away from their original forms is the major cause of forgetting in the NPCL. We thus propose to regularize the latent variables to be similar to their old forms and show the merits of regularization over typical parameter-based regularization. We then leverage the uncertainty encoded by the NPCL for the aforesaid CL challenge of task head inference. To this end, we propose using entropy as an uncertainty quantification metric (UQM). The NPCL outperforms previous probabilistic CL models and delivers better or comparable results than state-of-the-art deterministic CL methods, which usually have an edge over their probabilistic counterparts in terms of accuracy. Moreover, our ablations show the enhanced efficacy offered by the NPCL on continual learning settings requiring model calibration and few-shot replay. To study the further usages of the NPCL's uncertainty estimation, we show its out-of-the-box readiness for novel data detection and instance-level confidence evaluation . Lastly, we list the key limitations of the NPCL as an attempt to lay further solid directions for uncertainty-aware continual learning.

## 2 Related Work

**Continual Learning (CL).** Existing CL methods address catastrophic forgetting through three major approaches: (a) _Regularization-based_ methods penalize changes in a model's important weights for previous tasks, such as Elastic Weight Consolidation (EWC) , Synaptic Intelligence (SI) , etc. (b) _Parameter Isolation-based_ methods partition the network's parameters to specialize on individual tasks, e.g., Douillard et al.  learning task-specific tokens for Transformers. (c) _Replay-based_ methods use an episodic memory to preserve a fraction of the past tasks' experience for preventing forgetting while learning on new tasks; e.g., experience replay (ER)  storing past inputs, dark experience replay (DER)  storing past logits, and Yan et al.  using a loss-aware memory for ER. Our method uses (a) via regularization of distributions, (b) via task-specific latent heads, and (c) via replay of past task inputs and distributions.

**Neural Processes (NPs).** NPs were introduced to meta-learn a distribution of a family of functions modeling the data-generating process through their deterministic  and/or latent summaries . Attentive NPs (ANPs)  replaced the averaging operation in NPs with a dot-product attention  to enhance their expressivity. NPs/ANPs rely on a global latent that limits their ability to

Figure 1: **Neural Processes for Continual Learning (NPCL): each training step involves minimizing the distance \(D\) between the context-based prior and the target-based posterior, alongside regularizing the task-specific and global distributions towards their old forms.**

model observations from multiple functions. Some works address this through local latent variables that model fine-grained correlation among a subset of the observations . Recently, multi-task processes (MTPs)  have been studied to model multiple tasks with NPs, owing to the hierarchy of task-specific latent variables conditioned on a global latent. However, existing MTPs cannot directly handle CL problem because (a) MTPs are not designed to learn on sequential tasks and thus do not handle forgetting; (b) MTPs target the multi-task learning problem where the label for an input spans the exhaustive output space of available tasks unlike the CL setup where each input may belong to one specific and unknown task, out of multiple seen tasks.

Besides, the added complexity of variational inference has limited NP applications to mostly proof-of-concept focused regression tasks . The potential of NPs for large-scale classification tasks thus remains largely under-explored except in some recent works. Wang et al. , for instance, leverage the predictive uncertainties of NPs to decide on pseudo labels for unlabeled data in semi-supervised classification. In our work, we use NPs to handle CL with classification tasks, reflecting the benefits of principled Bayesian learning, uncertainty estimation, and easily integrated existing ER.

## 3 Preliminaries: Neural Processes

Given the data \(\{(x^{t},y^{t})\}=(X^{t},Y^{t})^{t}\) of a task \(t\), the goal is to learn the mapping \(F^{t}_{*}:X^{t} Y^{t}\) reflecting the data-generating process. NPs [13; 14]_meta-learn_ the distribution over the mapping functions from the given tasks. This is equivalent to meta-learning the distribution over the predictions \(p(y^{t}_{i}|x^{t}_{i},^{t})\) for the target output \(y^{t}_{i}\) belonging to a target data set \(^{t}\), given the corresponding target input \(x^{t}_{i}\) and a context set \(^{t}\)[14; 23]. To reflect the meta-learning behavior of NPs , the training samples are split into the context set \(|^{t}|=m\) and a target set \(|^{t}|=m+n\) containing context set \(\) and additional samples. NPs learn the Gaussian priors and posteriors using a neural network \(F^{t}_{[;]} F^{t}_{*}\) for the predictive distribution, where \(\) and \(\) parameterize an encoder \(q\) and a decoder \(p\), respectively. This involves deriving a global variable \(z^{G}\) to estimate the prior \(p(z^{G}|^{t};)\), and then maximizing the marginal likelihood \(p(Y^{t}_{}|^{t},X^{t}_{};)\):

\[p(Y^{t}_{}|X^{t}_{},^{t})= p(Y^{t}_{ }|X^{t}_{},z^{G})p(z^{G}|^{t})dz^{G}, \]

where \(p(Y^{t}_{}|X^{t}_{},z^{G})=_{i=1}^{m+n}p(y^{t}_{i}| x^{t}_{i},z^{G})\) is the generative likelihood. In CL with streaming tasks, maintaining the memorization of the task prior \(p(z^{G}|^{t};)\) can help NPs avoid forgetting the \(t\)_-th_ task. Our aim behind enabling NP for CL is to seek a trade-off to preserve such task priors while sharing the parameters among tasks.

## 4 Continual Learning with Neural Processes

CL considers learning from a series of different tasks arriving sequentially, _i.e._, \(^{t} 0 t T-1\). Here, \(^{t}\) can belong to classification tasks with different classes in _class incremental_ CL . Let \(l\) be the cross-entropy (CE) loss for classification, the CL objective for the task \(t\) involves minimizing:

\[^{t}_{CE}=_{(x,y)^{t}}\ l(F_{[;] }(x),y) \]

on all \([0,t]\) tasks seen sequentially. Achieving Eq. (2) is challenging in real-world CL scenarios, where the previous datasets can be unavailable due to constraints on privacy, storage, etc. Learning on the sequential data with varying distributions causes _catastrophic forgetting_. To alleviate the issue, _experience replay_ (ER) is used in CL to store and periodically revisit some past experiences, e.g., samples \((x^{t},y^{t})\) of task \(t\), in a small episodic memory \(\) for replay in the future [8; 6]. In this work, we develop our method with the classical reservoir sampling-based ER  for a task boundary-agnostic updating of \(\).

CL methods with ER solely still suffer from severe forgetting issues [6; 52; 8]; and jointly optimizing parameters on \(^{t}\) and \(\) has several drawbacks [35; 4]. Considering that a deterministic mapping \(F^{t}\) limits capturing the randomness behind the real-world data in a stream, to utilize the meta-learning ability of NPs, we next propose extending models with Eq. (2) and Eq. (1) to arrive at our NPCL model. It allocates small subsets of parameters to learn robust per-task and global priors and uses stochastic factors to meet data-driven challenges such as deducing the right parameters for inference.

### Neural Processes for Continual Learning

Given the task in a stream, we model the CL task based on NPs formulated in Eq. (1). In ER framework with a small memory buffer, where the context and target could be from tasks indexed by \(t\), Eq. (1) can be extended to derive the joint posterior for NPs  as:

\[p(Y_{}^{0:t}|^{0:t},X_{}^{0:t})= p(Y_{ }^{0:t}|X_{}^{0:t},z^{G})p(z^{G}|^{0:t})dz^{G}, \]

where \(z^{G}\) models the joint distribution \(F_{*}^{0:t}\) of CL tasks and is an enabler of the knowledge transfer  between these (see App. A.3 for ELBO). Eq. (3) poses two challenges. First, a labeled context \(\) is needed for inferring predictions as all NPs, which is unprepared in CL setups by default. To overcome this, we use the memory \(\) offered by the ER-based setups as _context_ during inference. Second, jointly modeling \(F_{*}^{0:t}\) ignores the dynamics of per-task stochasticities and is still prone to the bottlenecks of Eq. (2). We address the issue by introducing hierarchical modeling and redefining Eq. (3) in the following.

### NPs with Hierarchical Task-specific Priors for CL

To learn informative task priors while tackling the forgetting issue in CL, we propose a hierarchical modeling of the NP model. We preserve the global latent \(z^{G}\) to induce the direct knowledge transfer and add the task-specific upon the global variable to enhance the capturing of task-specific knowledge in CL. We thus extend Eq. (3) with task-specific latent variables \(z^{t}=(z^{0},..,z^{t})\). As a result, our posterior is a two-step hierarchical latent variable model (Fig. 2) where the global and the per-task latent variables model the inter and intra-task correlations, respectively:

\[p(Y_{}^{0:t}|X_{}^{0:t},^{0:t})= _{t=0}^{T-1}p(Y_{}^{t}|X_{}^{t},z^{t})p(z^{t}|z^{G },^{t})p(z^{G}|^{0:t})dz^{0:t}dz^{G}, \]

where the entire context \(^{0:t}\) is first encoded into \(z^{G}\) and then conditioned on \(z^{G}\), the task-specific context \(C^{t}:=(C^{0},..,C^{t})\) are encoded into their respective latent variables. We refer to Eq. (4) as NP for CL (NPCL). The hierarchical modeling enables NPCL to learn the shareable knowledge via \(z^{G}\) and the task-specific knowledge via \(z^{t}\) in the meta-learning fashion of NPs. Task identity is used in training to specify the task-specific latent variables. Unlike MTP  making predictions of all tasks for all inputs, NPCL needs to specify the corresponding output space for each test sample. We further discuss the relationship between NPCL and NP-based meta-learning in App. B.

### The NPCL Architecture

As standard NPs [14; 27], the training samples are split into a context \(\) and a target set \(\) containing \(\) and additional samples. Given the inputs \(x_{i}\) from \(\) or \(\), we first pass these to a feature extractor

Figure 2: **Overview of the NPCL architecture**: the decoding mechanism differs during training and inference. **Red**, **Cyan**, and **Orange** denote three different tasks.

\(f\). With a slight abuse of notation, we denote the features as \(x_{i}:x_{i}^{|f|}\) and let \(|f|\) denote the dimension. \(x_{i}\) concatenated with the one-hot encoded labels, _i.e._, \([x_{i};y_{i}]\), is fed to the NPCL encoder with a deterministic and a latent path, and then to the decoder (Fig. 2). All the NPCL layers use multi-layer perceptrons (MLPs) projections, _i.e._, \((x):^{|f|}^{|o|}\), where \(|o|\) is the output feature dimension as a hyperparameter. We denote a normal distribution with a mean \(\) and a variance \(^{2}\) by \((,^{2})\); the global and the task-specific distributions are \((_{G},_{G}^{2})\) and \((_{t},_{t}^{2})\).

**Latent Encoder.** The latent path comprises of the projection \(_{i}^{}=([x_{i};y_{i}])\) followed by two attention operations . First, per-task projections form the keys, values and queries to taskwise self-attention layers \(SA_{lat}^{t}\) that produce order-invariant encodings \(s_{i}^{t}\) over the samples of task \(t\). Second, all encodings \(\{s_{i}^{0:t}\}_{i=1}^{n+m}\) serve as the keys, values and queries to cross-attention layers \(CA_{}^{}\) that enrich their order-invariance from intra-task \(s^{t}\) to inter-task \(s^{G}\). \(s^{t}\) and \(s^{G}\) are used to derive the \(N\) and \(M\) Monte Carlo samples of the global \(z^{G}\) and the task-specific latent variables \(z^{t}\), respectively (see App. C for more details) using the reparameterization trick . We set \(M=1\) to enhance the inter-task stochasticity in posterior while retaining superior computational efficiency (see App. E). For each input, we thus get \(N(t+1)\) latent outputs.

**Deterministic Encoder.** The deterministic path is similar to that of the ANP  and outputs an order-invariant representation \(r_{*}\) for target \(x_{*}\) (see App. C).

**Decoder.** Based on the task information, the decoder adopts separate mechanisms during training and inference. At train time, we use the available task labels to filter the \(N\) true latent variables \(\{z_{i}^{t}\}_{i=1}^{N}\), combine them with \(r_{*}\) and \(x_{*}\), and decode the logits \(h_{*}\). We discuss the decoding operation in the testing phase without task ID in Sec. 4.5.

### Learning Objectives for the NPCL

The learning of the NPCL involves variational inference alongside additional regularizations.

**Evidence Lower Bound (ELBO).** The intractability of Eq. (4) leads us to the following ELBO:

\[ p_{}(Y_{}^{}|X_{}^{ {Q.t}},) _{q_{}(z|)}_{t=0}^{T-1} _{q_{}(z^{t}|z^{G},^{t})}[ p_{}(Y_{ }^{t}|X_{}^{t},z^{t})] \] \[-D^{t}q_{}(z^{t}|z^{G},^{t})\|q_{}(z^{t }|z^{G},^{t})-D^{G}q_{}(z^{G}|) \|q_{}(z^{G}|),\]

where \(p_{}(Y_{}^{t}|X_{}^{t},z^{t})\) is approximated by the CE loss. \(D^{t}\) and \(D^{G}\) denote the KL divergence (KLD) between the approximate posterior and prior for the task-specific and global distributions, respectively. We derive the ELBO in App. A.1. We next propose two techniques to counter forgetting in the NPCL. Henceforth, we use \(D\) to denote the Jenshen-Shannon (JS) divergence  between two distributions.

**Global Regularization (GR).** The training data of a CL task \(t\) is dominated by the t-_th_ task samples. For the NPCL, this drifts the global distribution \((_{G}^{t},_{G}^{t})\) of past tasks towards the new task (Fig. 1). We thus regularize their global distribution using the one learned at step \(t-1\):

\[_{}=D(_{G},_{G}^{2})_{t}, (_{G},_{G}^{2})_{t-1} \]

**Task-specific Regularization (TR).** While GR helps preserve the joint distribution of the past tasks, the hierarchy in the NPCL leaves their task-specific distributions to be still prone to forgetting (Fig. 3(a)). This can further amplify the posterior collapse  for past task-specific latent variables during CL training (Fig. 3(b)). To alleviate these, we regularize the learning of previous task distributions as:

\[_{}^{t}=D(_{t},_{t}^{2})_{t}, (_{t},_{t}^{2})_{j}, \]

where \(j\) is the step at which the task \(t\) arrived. Given the reliance of Eq. (6) and Eq. (7) on past distributions, we maintain a separate buffer, which we refer to as the distribution memory \(_{}\), to store the global \((_{G},_{G}^{2})\) and the task-specific distributions \((_{0:t-1},_{0:t-1}^{2})\). \(_{}\) is updated after each incremental training step, where we run an additional pass over the training data of task \(t\) alongside replaying \(\) to record the batchwise averaged global and task-specific means and variances.

**Integrated objective.** Using \(\), \(\), \(\), and \(\) to denote the loss weights, our total loss can be written as:

\[=^{t}|+||}_{(x^{t},y^{t}) ^{t}}(_{}+ D^{t}+ D ^{G})+|}_{(x^{t},y^{t})}_{}+_{}^{t}, \]where CE, \(D^{t}\), and \(D^{G}\) act on the current task data \(^{t}\) and on the buffer \(\) while GR and TR act only on \(\). By setting \(0<\{,,,\}<1\), we resort to using the (respective) cold posteriors .

### Inference with Uncertainty Awareness

NPCL's inference uses \(f\) to obtain the features \(x_{*}\) for the target test images. Although the task identification information is used to train the task-specific module, task identification of test samples is usually unavailable in general real CL tasks (except the restricted task-incremental setting ). Given \(x_{*}\) from the encoder, this leaves us with \(\{z_{i}^{0:t}\}_{i=1}^{N}\) possible modules and the corresponding latent variables to use and infer for obtaining the prediction. A _naive_ solution is to average over \(N*(t+1)\) logits. But as the number of tasks grows, the noise from incorrect task priors would dominate the posterior. We thus propose using entropy as an uncertainty quantification metric (UQM) to filter the logits of the true task head \(^{t}\):

\[h_{*}=*{arg\ min}_{j[1,t]}U(h_{^{j}}),\;\;\;U(h_{})=- _{i N}(i)((i)), \]

where \(\) is the softmax function and \(U\) is the total Shannon entropy  over the \(N\) logits per head. As we use true head \(^{t}_{}\) during training, \(^{t}\) produces low entropy for within distribution data. In light of Eq. (9), the NPCL can be seen as a special case of the mixture-of-expert (MoE) modeling [36; 51], where we leverage uncertainty to select the top-1 expert during inference.

## 5 Experiments

**Datasets.** We evaluate the NPCL on class and domain incremental learning (IL) settings. For class-IL, we use three public datasets: sequential CIFAR10 (S-CIFAR-10) , sequential CIFAR100 (S-CIFAR-100) , and sequential Tiny ImageNet (S-Tiny-ImageNet) . For domain-IL, we use Permuted MNIST (P-MNIST)  and Rotated MNIST (R-MNIST) . S-CIFAR-10, S-CIFAR-100, and S-Tiny-ImageNet host 10, 100, and 200 classes each with 5000, 500, and 500 training images and 1000, 100, and 50 test images per class, respectively. The number of sequential tasks for S-CIFAR-10 is 5 (2 classes per task); for S-CIFAR-100 and S-Tiny-ImageNet is 10 (10 and 20 classes per task, respectively); for P/R-MNIST is 20. P-MNIST creates tasks out of MNIST  by randomly permuting the pixels, and R-MNIST does it by rotating images randomly in \([0,)\).

**Architectures.** For a fair comparison against other methods, we rely on the Mammoth CL benchmark . Our backbone for class-IL experiments is a ResNet-18  without pretraining, while for domain-IL, we rely on a fully connected (FC) network with two hidden layers . The NPCL relies on Xavier initialized  FC layers with two 256-d hidden layers for class-IL and one 32-d layer for domain-IL setups. For class-IL, each FC layer is followed by layer normalization  and ReLU.

**Configuration and hyperparameters.** We train all models using SGD optimizer. The number of training epochs per task for S-Tiny-ImageNet is 100, for S-CIFAR-(10/100) is 50, and that for (P/R)-MNIST is 1. We detail further the configurations, hyperparameters, and their tuning in App. D.

Figure 3: **Analyses on the need for distribution regularization: (a) shows the increasing distances between current distributions of past tasks and their original distributions (learned while the tasks were introduced). (b) and (c) show the effect of global (GR) and task regularization (TR) on the activation of the global and task-specific latent units. Low KLD corresponds to an inactive unit.**

**Baselines.** We employ several CL methods to compare the NPCL with. Regularization-based methods include oEWC  and SI ; knowledge distillation-based methods include iCaRL  and LwF ; rehearsal-based methods are ER , RPC , FDR , DER . Among neural processes, we use the NP , the ANP , and the Single Task (ST) NPCL (see App. A.2) with only per-task latent variables. We use five non-CL benchmarks as upper bounds on the performances: Joint ResNet / NP / ANP / NPCL perform joint training of all tasks using a single task head while the multitask NPCL infers task heads in joint training using Eq. (9). Finally, the _naive_ NPCL inference averages the logits of all task heads.

### Results

Table 1 reports the average accuracy after training on all tasks. Across all settings, the NPCL boosts the performance of the ER and achieves either comparable or better results against the state-of-the-art (SOTA), _e.g._, DER. Compared to the regularization-based oEWC and SI, the NPCL obtains a significant gain in performance. This is because the former methods calculate weight importance, which is liable to changes with new tasks. Regularizing explicitly towards the global and per-task distributions of past tasks helps the NPCL overcome this. Further, on both class and domain-IL, the NPCL stands out in the most challenging setting where the episodic memory size is the smallest. On domain-IL where the shift occurs within the domain instead of the classes, the performance of a number of methods degrade as they forget the relations among a task's classes. Preserving the tasks' distributions helps the NPCL maintain valuable information in this case. Analyzing the backward transfer (BWT) scores  shows that the NPCL's forgetting is competitive or lesser than the SOTA (see Table 10). Lastly, we note that the ST-NPCL with no hierarchy lags in BWT and accuracy due to limited knowledge transfer between tasks.

### Ablation Studies

**Why uncertainty-aware inference works?** For our uncertainty-aware task head inference mechanism to be effective, a CL model must produce probabilities that align well with the ground truth labels of the test samples. We thus ablate the calibration errors for different CL baselines using the well-established Expected Calibration Error (ECE)  and Adaptive Calibration Error (ACE)  metrics. Table 2 shows that the NPCL has the least calibration error across S-CIFAR-10 (\(_{}=200\)) and S-CIFAR-100 (\(_{}=500\)). In general, the probabilistic nature of the ANP  and

   Method &  &  \\  Metric & ECE & ACE & ECE & ACE \\  ER  & 0.4553 & 0.8532 & 0.6459 & 0.9499 \\ DER  & 0.2991 & 0.8391 & 0.2484 & 0.9447 \\ ANP  & 0.34 & 0.8495 & 0.5441 & 0.9477 \\ NPCL (ours) & **0.2103** & **0.8155** & **0.1995** & **0.9421** \\   

Table 2: Model calibration errors averaged across 10 runs.

   Method &  &  &  &  &  \\  & _Class-IL_ & _Class-IL_ & _Class-IL_ & _Domain-IL_ & _Domain-IL_ & _Domain-IL_ \\  Joint ResNet & 92.2\(\)0.15 & 70.4 & 59.9\(\)0.19 & 94.3\(\)0.17 & 96.7\(\)0.04 \\ Joint NP & 91.66\(\)0.11 & 70.8\(\)0.24 & 59.8\(\)0.17 & 95.0\(\)0.21 & 95.37\(\)0.07 \\ Joint ANP & 91.26\(\)0.16 & 70.7 NPCL benefits them in confidence calibration over the deterministic methods with comparable accuracies, _i.e.,_ ER  and DER .

**Uncertainty-Accuracy trade-off.** Fig. 4 ablates the average accuracies and uncertainties of each task head predictions over the test set of each task on S-CIFAR-10 (see App. G.2 for S-CIFAR-100). First, we observe that the accuracy of predictions made by true task heads are, in general, a magnitude higher than the rest. For uncertainty, this trend is reversed. This verifies our assumption that restricting latent heads to learn only their true label distribution makes them more confident in modeling the within-task samples. Second, for recently trained tasks, the uncertainty differences between the true task heads and the rest are greater than the earlier tasks. This, in general, suggests that the extent of forgetting goes _beyond_ a CL model's accuracy and to other aspects of its learning such as its predictive confidence. To support the latter claim, we probe the BWT of uncertainty and see a strong correlation with the BWT of accuracy (see Fig. 7).

**Learning objectives.** Table 3 shows the impact of distribution regularization, with the baseline being the NPCL trained with no regularization. We observe that the baseline performs worse than the ER as the NPCL layers forget more. Including TR in our objectives leads to the single-most gain over the baseline. We further study how these objectives guide the learning of the global and task-specific distributions with training (see App. G.1). We observe that the NPCL w/ TR leads to better learning of the current task as well as preserving the past task distributions but at the cost of drifting the global distribution. The NPCL w/ GR restricts the drift of the global distribution but not for the per-task distributions. The NPCL w/ GR and TR strikes a balance in between.

**Effect of Monte Carlo (MC) samples.** We spot two combinations of the number of global \(N\) and task-specific \(M\) MC samples in favor of performance. Out of these, we choose the one with the superior computational efficiency (see App. E for details).

**Context size.** We study the average accuracy (Fig. 5(a)) and uncertainty (Fig. 5(b)) after training on S-CIFAR-10 with \(||=200\), and then varying the context sizes during inference. Similar to other NPs , we find a positive correlation between context size and performance, indicating that the NPCL utilizes useful information from diverse contexts, thereby reducing its task inference ambiguity.

**Few-shot replay settings.** A key strength of NPs remains their few-shot learning capability. To study how well the NPCL retains this trait against other CL baselines, we ablate their accuracy and ECE  on rehearsal memory sizes of \(5\) and \(10\) (see Table 4). We find that on both memory sizes, the NPCL outperforms ER  and DER . For \(_{size}=5\) on S-CIFAR-100, we observe that the ER outperforms the DER in terms of accuracy. However, the latter still offers more confident predictions (characterized by a lower ECE). This implies that on few-shot CL replay settings while regularizing the predicted logits towards their old forms - as done by the DER - helps improve the predictive confidence over the ER, regularizing the task distributions towards their old forms - as done by the NPCL - remains the superior way to enhance the model's predictive confidence.

**Storage efficiency.** For each task, the NPCL stores two new vectors - task-specific mean and variance, and replaces the global mean and variance with the current global ones. The NPCL storage thus scales constantly in the size \(||\) of the memory. This offers a strong edge on storage efficiency when compared to DER  scaling quadratically, _i.e._, \(|| N_{C}\) where \(N_{C}\) is the total number of classes. For instance, on S-Tiny-ImageNet with \(||=500,N_{C}=200\), the NPCL's cumulative storage amounts to a (flattened) vector of size 6132 (\(256 10 2\) for 256-d means and variances of 10 tasks plus \(256 2\) for 256-d global mean and variance plus 500 for 1-d task labels) while that of DER amounts to 100,000 (\(200 500\) for logits of 500 memory samples), _i.e._, a **93.868%** relative storage efficiency. We report the storage efficiency of the NPCL over DER across all settings in App. G.3.

### Applications of Uncertainty Quantification

The probabilistic nature of the NPCL offers it an edge in leveraging data-driven UQMs. To further study the usage of predictive uncertainties, we conduct two experiments with a trained NPCL model.

**Novel data identification.** Novel data identification seeks to distinguish out-of-distribution data \((_{})\) from in-domain data \((_{})\). For getting makes CL models struggle further on the task . The probabilistic sampling in the NPCL opens the door for leveraging its predictive variances - which are more reliable estimates of aleatoric uncertainty than pointwise predictions . For the \(N\) predicted logits, we thus compute the variances over their softmax scores, \(^{2}((h_{*}))\), and over their uncertainty scores, \(^{2}(U(h_{*}))\). Table 5 evaluates these metrics for ID (S-CIFAR-10) and OOD (first 10 classes of S-CIFAR-100) data after each task. We observe that the variance scores of either metrics on \(_{}\) are up to a magnitude lower than those on \(_{}\). We further observe an overall decrease in the variances with the arrival of further incremental tasks. This could be attributed to the generalization of more low-level features in the novel data as in-domain [18; 16]. We detail further novel data identification experiments in App. G.5.

**Instance-level model confidence evaluation.** The confidence evaluation framework of Han et al.  provides finer granularity for assessing the predictive confidence of classification models (see App. G.6 for more details and normality test). Table 6 shows the results of one run of the framework after training on S-CIFAR-10. Here, we use the task identity to select the latent head per class. We observe the mean prediction interval width (PIW) of the true class label among the correct predictions to be narrower than that of the incorrect predictions, implying that the NPCL's variations of predicted class labels are smaller when the predictions are correct. We also notice a higher accuracy among the test instances rejected by the _t_-test than those not rejected.

   Class & Accuracy &  &  \\   & & Correct & Incorrect & Rejected & Rejected \\ 
1 & 82.30 & 74.17 & 102.21 & 83.37 & 50.00 \\
2 & 94.00 & 62.90 & 79.86 & 94.07 & 80.00 \\
3 & 74.00 & 54.92 & 68.48 & 74.14 & 64.29 \\
4 & 71.50 & 65.42 & 74.32 & 72.06 & 25.00 \\
5 & 84.80 & 92.93 & 106.90 & 85.37 & 22.22 \\
6 & 76.50 & 75.22 & 103.58 & 76.58 & 60.00 \\
7 & 94.20 & 104.9 & 129.56 & 94.39 & 3.00 \\
8 & 90.50 & 81.10 & 127.06 & 91.12 & 22.22 \\
9 & 96.90 & 72.81 & 110.86 & 97.00 & 66.67 \\
10 & 96.30 & 80.60 & 109.56 & 96.48 & 60.00 \\   

Table 6: PIW (multiplied by 100) and \(t-\)test results for the first three classes of S-CIFAR-10 inferred from their respective task heads.

    &  &  \\   &  &  &  &  \\   & Acc. & ECE & Acc. & ECE & Acc. & ECE & Acc. & ECE \\   & ER  & 22.11 & 0.7281 & 25.39 & 0.696 & 9.44 & 0.8003 & 9.69 & 0.8014 \\ DER  & 21.05 & 0.5931 & 25.2 & 0.5107 & 8.96 & 0.4593 & 10.97 & 0.542 \\ NPCL (ours) & **22.98** & **0.4709** & **26.15** & **0.441** & **10.22** & **0.39** & **12.64** & **0.4717** \\   

Table 4: Few-shot replay results: Accuracy (Acc.) and ECE  of different methods with very small buffer sizes of \(5\) and \(10\) for S-CIFAR-10 and S-CIFAR-100 averaged across 3 runs.

    Incremental \\ step \\  } & _{}\) = CIFAR-10, \(_{}\) = CIFAR-100} \\   & \(_{}\) (\(\)) & \(_{}\) (\(\)) & \(_{}\) (H) & \(_{}\) (H) \\ 
1 & \(1e^{-6}\) & \(1e^{-5}\) & \(9.3e^{-6}\) & \(8.4e^{-5}\) \\
2 & \(2.6e^{-6}\) & \(1.4e^{-5}\) & \(6.3e^{-5}\) & \(2.2e^{-4}\) \\
3 & \(2.3e^{-6}\) & \(6.2e^{-6}\) & \(6.7e^{-5}\) & \(2.1e^{-4}

## 6 Limitations

We list the key limitations of the NPCL to facilitate future research directions.

Incompetence of dot-product attention.Similar to the ANP , the NPCL employs the permutation-invariant scaled-dot product attention  to weigh the relevant context and target embeddings. Visualizing the attention weights computed by the cross-attention layers of the deterministic path shows us that the top attended context for the target queries often contain points belonging to other CL tasks (Fig. 11(a)). This _limits_ the performance sensitivity of the NPCL with respect to the increase in context thus resulting in a lag of accuracy behind SOTA on CL setups with larger episodic memory sizes (see Table 1). To further verify the relevance of the attended context, we visualize the self-attention weights of all context points. Fig. 11(b) shows that the lowest or the maximum values in the context dataset have larger weights. Such an observation is in line with existing works pointing that the scaled-dot product attention can derive irrelevant set encodings of the context points and can thus lag at exploiting the context embeddings properly .

Computational overhead.Table 7 compares the number of parameters of the NPCL with ER / DER  where the latter rely solely on the ResNet-18 backbone as they do not exploit parameter isolation for task heads. Overall, the percentage increase in parameter number is \(57.6\%\) for S-CIFAR-10, \(46.57\%\) for S-CIFAR-100 and S-Tiny-ImageNet, and \(55.25\%\) for P/R-MNIST.

Inference time complexity.The reliance on self-attention means that the inference time complexity of the NPCL is \((n*m)\), where \(n\) is the number of context points (sampled from the episodic memory) and \(m\) is the number of target points (the number of test samples). Due to this, the runtime for inference scales polynomially with the number of context points (sampled from the buffer). Table 6 reports the runtime of the NPCL on S-CIFAR-10 and S-CIFAR-100 settings by varying the context sizes. For reference, the first row reports the runtime of ER / DER whose inference complexity is \((1)\) in the memory buffer size.

Incompatibility with logits-based replay.The NPCL is incompatible with logits-based replay because of the stochasticity in the posterior induced by Monte Carlo sampling. Overcoming this could help boost the performance of the NPCL further over SOTA like DER  and DER++ .

## 7 Conclusion

In this paper, we propose Neural Processes for Continual Learning (NPCL), a hierarchical latent variable setup designed to jointly model the task-agnostic and task-specific data-generating functions in continual learning. We study the potential forgetting aspects in the NPCL and propose to regularize the previously learned distributions at a global and a per-task granularity. We demonstrate that using entropy as an uncertainty quantification metric helps the NPCL infer correct task heads and boost the performance of baseline experience replay to even surpass state-of-the-art deterministic models on several CL settings. Our robust ablations show the efficacy of the NPCL for model calibration measurement and few-shot replay in CL. We further study out-of-the-box applications of the uncertainty estimation capabilities of the NPCL for novel data identification and instance-level confidence evaluation. We conclude our ablations by listing the key limitations of the NPCL, which we hope could lay solid directions for further research on uncertainty-aware continual learning.

 
**Method / Dataset** & **S-CIFAR-10** & **S-CIFAR-100** & **S-Tiny-ImageNet** & **P/R-MNIST** \\  ER / DER  & 11,173,962 & 11,220,132 & 11,220,132 & 89,610 \\  NPCL & 19,397,706 & 24,091,556 & 24,091,556 & 162,166 \\  

Table 7: Comparison of the total number of parameters for ER / DER against the NPCL.

 
**Method** & **S-CIFAR-10** \\  ER / DER & 3.72s \\  NPCL\(||=200\) & 19.58s \\ NPCL\(||=500\) & 31.25s \\ NPCL\(||=1000\) & 47.99s \\ NPCL\(||=2000\) & 84.86s \\  

Table 8: Inference time with varying context sizes