# ProSST: Protein Language Modeling with Quantized Structure and Disentangled Attention

Mingchen Li\({}^{2,3,5}\) Yang Tan\({}^{2,3,5,}\)1  Xinzhu Ma\({}^{2,4}\)2  Bozitao Zhong\({}^{1,4}\) Huiqun Yu\({}^{3}\)

Ziyi Zhou\({}^{1}\) Wanli Ouyang\({}^{2,4}\) Bingxin Zhou\({}^{1}\) Pan Tan\({}^{1,2}\) Liang Hong\({}^{1,2,5}\)

\({}^{1}\) Shanghai Jiao Tong University, China

{zy-zhou,bingxin.zhou,hongl3liang}@sjtu.edu.cn, tpan1039@gmail.com,

\({}^{2}\) Shanghai Artificial Intelligence Laboratory, China

{ouyang-wanli,maxinzhu}@pjlab.org.cn

\({}^{3}\) East China University of Science and Technology, China

{lmc,tyang}@mail.ecust.edu.cn, yhg@ecust.edu.cn

\({}^{4}\) The Chinese University of Hong Kong, China

\({}^{5}\) Chongqing Artificial Intelligence Research Institute of Shanghai Jiao Tong University, China

###### Abstract

Protein language models (PLMs) have shown remarkable capabilities in various protein function prediction tasks. However, while protein function is intricately tied to structure, most existing PLMs do not incorporate protein structure information. To address this issue, we introduce ProSST, a Transformer-based protein language model that seamlessly integrates both protein sequences and structures. ProSST incorporates a structure quantization module and a Transformer architecture with disentangled attention. The structure quantization module translates a 3D protein structure into a sequence of discrete tokens by first serializing the protein structure into residue-level local structures and then embeds them into dense vector space. These vectors are then quantized into discrete structure tokens by a pre-trained clustering model. These tokens serve as an effective protein structure representation. Furthermore, ProSST explicitly learns the relationship between protein residue token sequences and structure token sequences through the sequence-structure disentangled attention. We pre-train ProSST on millions of protein structures using a masked language model objective, enabling it to learn comprehensive contextual representations of proteins. To evaluate the proposed ProSST, we conduct extensive experiments on the zero-shot mutation effect prediction and several supervised downstream tasks, where ProSST achieves the state-of-the-art performance among all baselines. Our code and pre-trained models are publicly available 3.

## 1 Introduction

Predicting the functions of proteins is one of the most critical areas in life sciences . In recent decades, protein sequence databases have experienced exponential growth , making it possible to learn the fundamental representations of protein sequences with large-scale models in a data-driven manner. Inspired by pre-trained language models in natural language processing , many pre-trained Protein Language Models (PLMs) have emerged . Benefiting fromremarkable protein representation capabilities, they have become fundamental tools for bioinformatics in protein-related tasks.

The function of a protein is determined by its structure . However, most PLMs mainly focus on modeling protein sequences, neglecting the importance of structural information, and one significant reason for this phenomenon is the lack of structural data. Fortunately, some excellent works, such as AlphaFold  and RoseTTAFold , are proposed, which can accurately predict protein structures. These works significantly expand the protein structure dataset  to millions and enable the pre-training of large-scale structure-aware PLMs. After that, the major challenge is how to effectively integrate protein structure information into PLMs. Specifically, existing structure-aware PLMs [14; 15] first use Foldseek  to convert protein structures into discrete structure tokens and then integrate these structural data into the Transformer architecture. However, despite achieving promising performance on several tasks, this approach still faces two main issues. First, Foldseek encodes the structure of a residue within a protein by considering only the features of its previous and next residues. This representation is insufficient and may overlook subtle differences in the local structure of proteins, such as catalytic sites or binding pockets, which are crucial for protein function . Second, the naive Transformer architecture lacks the ability to explicitly model the relationship between protein sequences and structure token sequences, making it challenging to effectively leverage structural cues.

In this paper, we develop ProSST (**Pro**tein **S**equence-**S**tructure **T**ransformer), a structure-aware pre-trained protein language model. Specifically, ProSST mainly consists of two modules: a structure quantization module and a Transformer with sequence-structure disentangled attention. The structure quantization module is based on a GVP (Geometric Vector Perceptron)  encoder, which can encode a residue structure along with its neighborhoods in its local structure and quantize the encoding vectors into discrete tokens. Compared to Foldseek, which only considers individual residues, this encoder can take into account more information from the micro-environment of residue. The sequence-structure disentangled attention module replaces the self-attention module in the Transformer model. This can make Transformer model explicitly model the relationship between protein sequence tokens and structure tokens, enabling it to capture more complex features of protein sequences and structures. To enable ProSST to learn the contextual representation comprehensively, we pre-train our model with the Masked Language Modeling (MLM) objective on a large dataset containing 18.8 million protein structures. To summarize, our main contributions are as follows:

* We propose a protein structure quantizer, which can convert a protein structure into a sequence of discrete tokens. These token sequences effectively represent the local structure information of residues within a protein.
* We propose a disentangled attention mechanism to explicitly learn the relationship between protein structure and residue, facilitating more efficient integration of structural token sequences and amino acid sequences.

To evaluate the proposed ProSST, we conduct extensive experiments on zero-shot mutation effect prediction and multiple supervised downstream tasks, where the proposed model achieves state-of-the-art results among all baselines. Besides, we also provide detailed ablations to demonstrate the effectiveness of each design in ProSST.

## 2 Related Work

### Protein Representation Models

Based on the input modality, protein representation models can be divided into three categories: sequence-based models, structure-based models, and structure-sequence hybrid models.

**Sequence-based models.** Sequence-based models treat proteins as a sequence of residue tokens, using the Transformer model  for unsupervised pre-training on extensive datasets of sequence. According to the pre-training objective, current models can be further divided into BERT-based models , GPT-based models , and span-mask based models. Specifically, BERT-style models, including ESM-series models [5; 6; 7], ProteinBert, and TAPE , aim to recover the masked tokens in the training phase. The GPT-style models, such as Tranception , ProGen2 , and ProtGPT2 , progressively generate the token sequences in an auto-regressive manner. Lastly, models that use span-mask as the training objective include Ankh , ProtTS , and xTrimo .

**Structure-based models.** Protein structures play a dominant role in protein functionality. Therefore, models leveraging structure information generally get more accurate predictions. Recently, various techniques have been applied in learning protein structure representation, including CNN-based models  and GNN-based models [18; 27; 28; 29; 30], where the GNN-based ones have demonstrated significant versatility in integrating protein-specific features through node or edge attributes. Moreover, the recent advancements in protein folding models [7; 11; 31] enable the structure-based models access to extensive datasets of protein structures. This led to a growing interest in developing PLMs that leverage protein structure cues [14; 15; 32].

**Structure-sequence hybrid models.** Hybrid models, which incorporate both sequence and structure information of proteins, offer more effective representations of proteins. For example, the LM-GVP model employs ProtBERT-BFD  embeddings as input features for the GVP  model, while ESM-GearNet  investigates various methods of integrating ESM-1b  representations with GearNet . Similarly, the recent ProtSSN  model leverages ESM-2  embeddings as input for the EGNN  model, resulting in notable advancements. Both ESM-IFI  and MIF-ST  target inverse folding, utilizing the structure to predict corresponding protein residues, whereas ProstT5  focuses on the transformation between residue sequences and their structure token sequences  as a pre-training objective. SaProt  constructs a structure-aware vocabulary using structure tokens generated by foldseek . Both SaProt and ProstT5 extensively utilize large structure databases  for their pre-training datasets. ProSST is also a hybrid structure-sequence model. Compared to previous work, ProSST develops an advanced structure quantization method and a better attention formulation to leverage the structure cues.

### Protein Structure Quantization

The most intuitive way to represent a protein structure is using continuous features, such as coordinates, dihedral angles and distance map. However, directly using these continuous features in the pre-training may lead to overfitting . This issue arises from the mismatched representations of the structure between the training set (derived from model predictions) and the test set (measured by wet-lab experiments). As the bridge to eliminate this gap, structure quantization has been investigated by a few works. These methods can be divided into two groups based on the way to generate the discrete secondary structure, including the methods based on physical computing, such as DSSP , and the methods based on deep learning, such as Foldseek , which have been successfully applied to structure-aware PLMs [14; 15]. The structure quantization module of ProSST also relies on learning-based approaches but provides a more detailed residue structure representation than Foldseek.

## 3 Method

In this section, we introduce the architecture of ProSST. ProSST mainly contains two modules: structure quantization (Section 3.1) module and a-transformer-based model with sequence-structure disentangled attention. (Section 3.2).

### Structure Quantization Module

The structure quantization module aims to transform a residue's local structure into a discrete token. Initially, the local structure is encoded into a dense vector using a pre-trained structure encoder. Subsequently, a pre-trained k-means clustering model assigns a category label to the local structure based on the encoded vector. Finally, the category label is assigned to the residue as the structure token. The pipeline of structure quantization is shown in Figure 1.

**Structure representation**. We categorize protein structures into two distinct levels: _protein structure_ and _local structure_. Protein structure denotes the complete architecture of a protein, including all its residues. The local structure focuses on specific individual residues. It describes the local environment of a residue by centering on a specific residue and including it along with the nearest 40 residues surrounding it in three-dimensional space . Compared to protein structure, local structures are in finer granularity, which allows for a more accurate descripresidue. Therefore, a protein containing \(L\) residues has one protein structure and \(L\) local structures. Despite the different levels of structure, we can use graphs to represent it. Formally, we represent a structure using graph \(=(,)\), where \(\) and \(\) denote the residue-level nodes and edges, respectively. For any given node \(\), it contains only the structure information of the residue, without any residue type information of the residue itself. This ensures that the structure encoder is solely focused on the structure cues. The edge set \(=\{_{ij}\}\) includes all \(i\), \(j\) for which \(_{j}\) is one of the top-\(40\) nearest neighbors of \(v_{i}\), determined by the distance between their \(\) atoms.

**Structure encoder**. Based on the above-mentioned definition of structure, we use geometric vector perceptrons (GVP)  as the (local) structure encoder. In particular, the GVP can be represented as a structure feature extraction function \(_{}()^{l d}\), where \(l\) is the number of nodes, \(d\) is the embedding dimension, and \(\) is trainable parameters. We integrate GVP with a decoder that includes a position-wise multi-layer perceptron (MLP) to form an auto-encoder model. The entire model is trained using a de-noising pre-training objective. In this process, we perturb \(\) coordinates with 3D Gaussian noise (Figure 1A) and use Brownian motion on the manifold of rotation matrices, according to RF-Diffusion . The model is then tasked with recovering the structure to its original, noise-free state. After being trained on the C.A.T.H dataset  (see Appendix A.2), we exclude the decoder and utilize solely the mean pooled output of the encoder as the final representation of structures. Although the structure encoder is trained on protein structures, it can effectively encode local structures. Therefore, for a graph \(\) of a protein structure, the encoding is: \(=_{i=1}^{l}_{}(_{i})\), where \(_{i}\) represents the graph of the local structure associated with the \(i\)-th residue in the graph \(\), and \(_{}(_{i})^{d}\) is the output of the encoder for the \(i\)-th node. Here, \(^{d}\) is the mean pooled output of the encoder and the vectorized representation of the local structure.

**Local structure codebook**. The structure code book quantizes dense vectors representing protein structure into discrete tokens (Figure 1B). To build this, we employ a structure encoder to embed the local structures of all residues from the C.A.T.H dataset (See in Appendix A.2) into a continuous latent space. Then we apply the \(k\)-means algorithm to identify \(K\) centroids within this latent space, denoted as \(\{_{i}\}_{i=1}^{K}\). These centroids constitute the structure codebook, as shown in Figure 1B. For any local-structure embedding, it is quantized by the nearest vector \(_{j}\) within the codebook and \(j\) serving as the structure token. In this paper, the clustering number \(K\) is also referred to as the structure vocabulary size.

**Protein serialization and quantization.** In general, for a residue at position \(i\) in a protein sequence, we first build a graph \(_{i}\) only based on its local structure, and then use the structure encoder to embed

Figure 1: **The pipeline of structure quantization.** (A) Training of the structure encoder. (B) Local structure clustering and labeling. (C) Converting a protein structure to structure token sequence.

it into a continuous vector \(_{i}\). Then we use the codebook to assign a structure token \(s_{i}\{1,2,...,K\}\) to this vector as the structure token of the residue. Overall, the entire protein structure can be serialized and quantized into a sequence of structure tokens (Figure 1C).

### Sequence-Structure Disentangled Attention

Inspired by DeBerta , we use an expanded form of disentangled attention to combine the attention of residual sequences and structure sequences as well as relative positions. Specifically, for a residue at position \(i\) in a protein sequence, it can be represented by three items: \(_{i}\) denotes its residue token hidden state, \(_{i}\) represents the embedding of residue-level local structure, and \(_{i|j}\) is the embedding of relative position with the token at position \(j\). The calculation of the cross attention \(_{i,j}\) between residue \(i\) and residue \(j\) can be decomposed into nine components by:

\[_{i,j}&=\{_{i},_{i}, _{i|j}\}\{_{j},_{j},_{j|i}\}^{}\\ &=_{i}_{j}^{}+_{i}_{j}^{}+_ {i}_{j|i}^{}\\ &+_{i}_{j}^{}+_{i}_{j}^{}+_ {i}_{j|i}^{}\\ &+_{i|j}_{j}^{}+_{j|i}_{j}^{}+_{j|i}_{j|i}^{}. \]

As formulated in Equation 1, the attention weight of a residue pair can be calculated by separate matrices, including residue tokens, structure tokens, and relative positions. These matrices are utilized for various interactions such as _residue-to-residue, residue-to-structure, residue-to-position, structure-to-structure, structure-to-position, position-to-residue, position-to-structure, and position-to-position_. Since our model concentrates on learning contextual embeddings for residues, the terms structure-to-structure (\(_{i}_{j}^{}\)), structure-to-position (\(_{i}_{j|i}^{}\)), position-to-structure (\(_{j|i}_{j}^{}\)), and position-to-position (\(_{j|i}_{j|i}^{}\)) do not provide relevant information about

Figure 2: Model architecture of ProSST. ProSST is a Transformer-style model and the difference is that ProSST uses disentangled attention instead of self-attention .

residues and thus do not contribute significantly. Consequently, these terms are removed from our implementation of the attention weight calculation. As shown in Figure 2, our sequence-structure disentangled attention mechanism includes 5 types of attention.

In the following part, we use single-head attention as an example to demonstrate the operation of sequence-structure disentangled attention. To begin, we define the relative position of the \(i\)-th to the \(j\)-th residue, denoted as \((i,j)\):

\[(i,j)=0&i-j-L_{max}\\ 2L_{max}-1&i-j L_{max}\\ i-j+L_{max}&, \]

where, \(L_{max}\) represents the maximum relative distance we consider, which is set to \(1024\) in the implementation. Similar to standard self-attention operation , the computation of query, key for structure, residue and relative position, and the value for residue is as follows:

\[^{r}&=_{r}^{q}^ {r}=_{r}^{k}^{r}=_{r}^{v}\\ ^{s}&=_{s}^{q}^{s}=_{s} ^{k}\\ ^{p}&=_{p}^{q}^{p}=_{p} ^{k} \]

and the the attention score \(}_{i,j}\) from residue \(i\) to residue \(j\) can be calculated as follows:

\[}_{i,j}=_{i}^{r} _{j}^{r}}_{}+_{i}^{r}_{j}^{s }}_{}+_{i}^{r}_{(i,j)}^{p}}_{}+_{i}^{r}_{i}^{s }}_{}+_{j}^{r}_{(j,i)}^{p}}_{} \]

where \(_{i}^{r}\) represents the \(i\)-th row of the matrix \(^{r}\), and \(_{j}^{r}\) denotes the \(j\)-th row of \(^{r}\). \(_{i}^{s}\) and \(_{j}^{s}\) are the \(i\)-th and \(j\)-th rows of \(^{s}\) and \(^{s}\), respectively. The term \(_{(i,j)}^{p}\) refers to the row in \(^{p}\) indexed by the relative distance \((i,j)\), and \(_{(j,i)}^{p}\) refers to the row in \(^{p}\) indexed by the relative distance \((j,i)\). To normalize the attention scores, a scaling factor of \(}\) is applied to \(}\). This scaling is crucial for ensuring the stability of model training , particularly when dealing with large-scale language models. All the \(}_{ij}\) form the attention matrix, and the final output residue hidden state is \(_{o}\):

\[_{o}=(}}{})^{r}, \]

which is used as the input for the hidden state of the next layer.

### Pre-Training Objective

ProSST is pre-trained with the structure-conditioned masked language modeling. In this approach, each input sequence \(\) is noised by substituting a fraction of the residues with a special mask token or other residues. The objective of ProSST is to predict the original tokens that have been noise in the input sequence, utilizing both the corrupted sequence and its structure token sequence \(\) as context:

\[}_{MLM}=_{}_{}_{i M}- p(_{i}|_{/M},). \]

We randomly select 15% indices from the set \(\) for nosing and computing loss for back-propagation. At each selected index \(i\), there is an 80% chance of substituting the residue with a mask token, a 10% chance of replacing it with a random residue token, and the remaining residues are unchanged. The training objective is to minimize the negative log-likelihood for each noised residue \(_{i}\), based on the partially noised sequence \(/\) and the _un-noised_ structure tokens, serving as contextual cues. Therefore, to accurately predict the noised tokens, this objective enables the model not only to learn the dependencies between residues but also the relationship between residues and structures. The details of pre-training dataset and hyper-parameter configuration can be found in Appendix A.2.

## 4 Experiments

In this section, we comprehensively evaluate the representation ability of ProSST in several benchmarks, covering zero-shot mutant effective prediction tasks (Section 4.1) and various supervised function prediction tasks (Section 4.2). Additionally, we also provide ablation studies and discussions to further show the effectiveness of the detailed designs in our model (Section 4.3).

### Zero-Shot Mutant Effect Prediction

**Datasets**. To evaluate the effectiveness of ProSST in zero-shot mutant effect prediction, we conduct experiments on ProteinGym  and utilize AlphaFold2  to generate the structures of wild-type sequences. See Appendix A.2 for the details of the dataset and Appendix A.1 for scoring method.

**Baselines**. We compare ProSST with the current state-of-the-art models, including sequence-based models [6; 7; 21; 44; 45; 22; 46], sequence-structure model , inverse folding models [37; 38], evolutionary models [47; 48; 49; 50; 51], and ensemble models [6; 52; 51].

**Results**. Table 1 shows the performance of zero-shot mutant effect prediction on ProteinGYM. Based on the results, we draw several noteworthy conclusions:

* ProSST outperforms all baselines on zero-shot mutant effect predictions of ProteinGYM. We used the non-parametric bootstrap method to calculate the standard error of the difference in Spearman performance between each model and ProSST. The results showed that all standard errors were less than 0.01. This calculation was based on 10,000 bootstrap samples extracted from proteins in the ProteinGym benchmark. Furthermore, ProSST was compared against other models on subsets of ProteinGYM categorized by function, such as stability, activity, binding, and expression. ProSST achieves state-of-the-art (SOTA) performance in the stability, binding, and expression subsets, as detailed in Appendix A.4. Notably, ProSST achieves the best performance in predicting stability, aligning with the previous findings that models incorporating structure information typically perform better in stability predictions .
* The degraded version of ProSST (without structure) gets results similar to other sequence-based models. This demonstrates that the performance improvement of our model stems from the efficient modeling of structure information, rather than other factors such as more powerful backbones.

   Model & Model Type & \(_{s}\) & NDCG \(\) & Top-recall \(\) \\  EVE  & & 0.439 & 0.781 & 0.230 \\ EVmutation  & & 0.395 & 0.777 & 0.222 \\ DeepSequence  & & 0.407 & 0.774 & 0.225 \\ WaveNet  & & 0.373 & 0.761 & 0.203 \\ GEMME  & & 0.457 & 0.777 & 0.211 \\ MSA-Transformer  & & 0.434 & 0.779 & 0.217 \\  Tranception  & & 0.434 & 0.779 & 0.220 \\ RITA  & & 0.372 & 0.751 & 0.193 \\ UniRep  & & 0.190 & 0.647 & 0.139 \\ ESM-1v  & Sequence-based & & 0.374 & 0.732 & 0.211 \\ ESM-2  & & 0.414 & 0.747 & 0.217 \\ ProGen2  & & 0.391 & 0.767 & 0.199 \\ VESPA  & & 0.394 & 0.759 & 0.201 \\  ESM-IF  & Inverse-folding & 0.422 & 0.748 & 0.223 \\ MIF-ST  & & 0.401 & 0.765 & 0.226 \\  Tranception-EVE  & & 0.457 & **0.786** & 0.230 \\ ESM-1v*  & Ensemble Models & 0.407 & 0.749 & 0.211 \\ DeepSequence*  & & 0.419 & 0.776 & 0.226 \\  SaProt  & Sequence-Structure models & 0.457 & 0.768 & 0.233 \\ ProSST & & **0.504** & 0.777 & **0.239** \\   

Table 1: Comparison of zero-shot mutation prediction performance on ProteinGYM benchmark  between ProSST and other models. \(_{s}\) is the Spearman rank correlation.

### Supervised Fine-Tuning Tasks

**Downstream tasks.** For supervised learning, we choose four protein downstream tasks, including thermostability prediction, Metal Ion Binding prediction, protein localization prediction (DeepLoc) and GO annotations prediction (three settings including MF, BO, and CC). More details of the tasks, datasets, and metrics can be found in Appendix A.2

**Baselines.** We compared ProSST with other PLMs including ESM-2, ESM-1b , and the sequence-structure model SaProt  (two parameter versions, 35M and 650M), MIF-ST , as well as the protein structure representation model GearNet  and ESM-GearNet .

**Results.** The results of the supervised fine-tuning tasks are shown in Table 4.2, and we can get the following conclusions:

* ProSST gets the best results among all models with 4 firsts in all 6 settings. For the tasks (settings) of DeepLoc, Metal Ion Binding, ProSST largely surpasses other methods, and ESM-GearNet gets comparable (or slightly better) results for thermostability and GO-BP and GO-CC with ProSST, at the price of more than 6\(\) model size.
* The sequence-structure models, ESM-GearNet, SaProt and ProSST, show better results than other counterparts, which suggests the importance of the structure cues in protein modeling. Furthermore, ProSST is more capable of integrating sequence and structure information of proteins than SaProt, which confirms the effectiveness of our designs.

Combined with the results in Section 4.1, ProSST exhibits powerful ability in multiple settings.

### Ablation Study

In this section, we provide additional ablation studies and discussions to show the necessity and effectiveness of the detailed designs in ProSST. Specifically, we use zero-shot mutant effect prediction on ProteinGYM, supervised downstream task DeepLoc, and the perplexity in the pre-training validation set to conduct corresponding experiments.

**Ablations on quantized structure.** The ablation results of quantized structure are shown in Table 3 and Figure 3(a), and we can get the following findings:

* We can find, as the increases of \(K\) (the size of local structure vocabulary), the performance of ProSST shows an upward trend on all metrics, and most metrics achieve the best results with \(K=2048\). Based on that, we set \(K=2048\) as our default setting.
* As the increase of \(K\), the convergence of ProSST improves progressively (Figure 3(a)), which suggests incorporating structure cues can improve the representation capabilities of models.
* Based on the same network architecture, the proposed structure quantization method (with an appropriate hyper-parameter \(K\)) performs better than Foldseek  and DSSP , which shows the effectiveness of our design.

    & & **DeepLoc** & **Metal Ion Binding** & **Thermostability** & **GO-MF** & **GO-BP** & **GO-CC** \\ 
**Model** & **\# Params** & Acc\% \(\) & Acc\% \(\) & \(_{s}\) & F1-Max \(\) & F1-Max \(\) & F1-Max \(\) \\  ESM-2 & 650M & 91.96 & 71.56 & 0.680 & 0.670 & 0.473 & 0.470 \\ ESM-1b & 650M & 92.83 & 73.57 & 0.708 & 0.656 & 0.451 & 0.466 \\ MIF-ST & 643M & 91.76 & 75.08 & 0.694 & 0.633 & 0.375 & 0.322 \\ GearNet & 42M & 89.18 & 71.26 & 0.571 & 0.644 & 0.481 & 0.476 \\ SaProt-35M & 35M & 91.97 & 74.29 & 0.692 & 0.642 & 0.431 & 0.418 \\ SaProt-650M & 650M & 93.55 & 75.75 & 0.724 & **0.682** & 0.486 & 0.479 \\ ESM-GearNet & 690M & 93.55 & 74.11 & 0.651 & 0.676 & **0.516** & **0.507** \\ ProSST & 110M & **94.32(a,a,a,a)** & **76.37(a,a,a)** & **0.726(a,a,a)** & **0.682(a,a)** & 0.492(a,a)** & 0.501(a,a)** \\   

Table 2: Comparison of supervised fine-tuning on downstream tasks. \(_{s}\) denotes the Spearman correlation coefficient.

    &  &  &  \\   & Acc\% \(\) & \(_{s}\) & NDCG \(\) & Top-Recall \(\) & Perplexity \(\) \\  ProSST & **94.32 (\(\)0.10)** & **0.504** & **0.777** & **0.239** & **9.033** \\ ProSST (- P2R) & 91.31 (\(\)0.14) & 0.478 & 0.778 & 0.227 & 9.173 \\ ProSST (- R2P) & 92.17 (\(\)0.32) & 0.466 & 0.772 & 0.216 & 9.410 \\ ProSST (- R2S) & 90.48 (\(\)0.41) & 0.438 & 0.766 & 0.208 & 12.142 \\ ProSST (- S2R) & 91.27 (\(\)0.20) & 0.475 & 0.779 & 0.226 & 9.355 \\ ProSST (- PE) & 86.05 (\(\)0.65) & 0.095 & 0.634 & 0.126 & 13.885 \\ ProSST (self-attention) & 90.37 (\(\)0.21) & 0.401 & 0.728 & 0.189 & 12.346 \\   

Table 4: Ablation studies on disentangled attention. The term "-S2R" denotes the removal of structure-to-residue in our attention formulation, similar to other terms, and "- PE" denotes the removal of positional encoding. ProSST (self-attention) refers to the model trained with standard attention (with structure cues).

Figure 3: Perplexity curves of ProSST under different settings. We ablate the components of quantized structure and disentangled attention, and show their perplexity curves on the validation set.

    &  &  &  \\   & Acc\% \(\) & \(_{s}\) & NDCG \(\) & Top-Recall \(\) & Perplexity \(\) \\  ProSST (\(K\)=4096) & 93.88 (\(\)0.15) & 0.498 & 0.773 & 0.233 & **8.880** \\ ProSST (\(K\)=2048) & **94.32 (\(\)0.10)** & **0.504** & **0.777** & **0.239** & 9.033 \\ ProSST (\(K\)=1024) & 93.43 (\(\)0.15) & 0.485 & 0.760 & 0.231 & 9.333 \\ ProSST (\(K\)=512) & 93.70 (\(\)0.16) & 0.471 & 0.759 & 0.223 & 9.577 \\ ProSST (\(K\)=128) & 93.14 (\(\)0.04) & 0.469 & 0.753 & 0.228 & 10.021 \\ ProSST (\(K\)=20) & 93.05 (\(\)0.13) & 0.438 & 0.744 & 0.210 & 10.719 \\ ProSST (\(K\)=1) & 89.48 (\(\)0.24) & 0.390 & 0.738 & 0.181 & 12.182 \\ ProSST (\(K\)=0) & 89.77 (\(\)0.26) & 0.392 & 0.741 & 0.184 & 12.190 \\  ProSST (Foldseek) & 93.08 (\(\)0.22) & 0.468 & 0.759 & 0.228 & 10.049 \\ ProSST (DSSP) & 93.16 (\(\)0.16) & 0.439 & 0.760 & 0.204 & 10.009 \\   

Table 3: Ablation studies on quantized structure. We first show the performance of our models with \(K\) centroids of local structures. ProSST (\(K\)=0) refers to the model without structure token sequence. We also replace the proposed quantization method with existing Foldseek and DSSP, and show the results of these variants.

* ProSST (Foldseek), ProSST (DSSP), and all ProSST (\(K\)>0) models significantly surpass ProSST (\(K\)=0) in all metrics, which confirms the importance of the structure cues again.
* There is almost no difference in performance between ProSST (\(K\)=1) and ProSST (\(K\)=0), indicating that the improvement does not come from the rise in parameters of disentangled attention.

**Ablations on disentangled attention.** Here we show detailed ablations and comparisons of disentangled attention in Table 4 and Figure 3(b), and we can get the following observations:

* All items in Equation 4 are necessary to our attention formulation. Also note that 'P2R' attention has the least impact on model capacity, with the Perplexity slightly increasing from 9.033 to 9.173, suggesting that positional attention to amino acids is relatively less critical than other items. Conversely, removing 'R2S' item results in a significant increase in Perplexity from 9.033 to 12.142, underscoring the important role of structure information in enhancing the model's representation capability.
* Compared with standard self-attention, our attention formulation gets better results for all metrics, indicating that explicitly modeling structure cues is crucial for integrating such information. Besides, positional encoding is also necessary in our design.

As we have mentioned in the Section 2, our disentangled should learn the connections between structure and residue sequence. To valid these, we conduct further experiments to analyze disentangled attention in the Appendix A.5.

## 5 Conclusion and Limitations

This paper introduces ProSST, a protein sequence-structure transformer for PLM. ProSST includes two key techniques, protein structure quantization and sequence-structure disentangled attention. The structure quantization module contains an encoder and a k-means clustering model. The encoder is trained with a denoising objective and is utilized for encoding protein structures. Leveraging this encoder, we embed the local structures of each residue within every protein in the C.A.T.H dataset into a continuous latent space. Then we utilize k-means clustering algorithm to obtain \(K\) (default setting is \(2048\)) centroids. These centroids are then utilized to discretize the local structures of residues based on the index of the nearest centroid of its structure embedding vectors. A protein structure can be transformed into a sequence of discrete numbers (or referred to tokens) and each token representing the corresponding local structure of residue. The sequence-structure attention enhances standard self-attention by not only considering self-attention residues but also incorporating attention between residues and structures, and vice versa. This enables the model to learn the relationships between residues and structures, thereby acquiring improved adequate contextual representations of residues. Furthermore, we pre-train ProSST with 18.8 million protein structures using a MLM objective. Experimental results show that ProSST can outperform existing models in ProteinGYM benchmark and other supervised learning tasks. Despite of this, there are some limitations of ProSST. For example, the local structure construction and encoding requires heavy computations. In the future work, we aim to speed up the protein structure quantization process. Another threat is that the structural and sequential data are required for ProSST to derive the final protein representations, since the amount of available structural data is lower than that of sequence data. We provide solutions in the Appendix Section A.6. Additionally, we plan to enhance ProSST by training it with larger structure datasets and expanding its parameter, which may further improve its performance.