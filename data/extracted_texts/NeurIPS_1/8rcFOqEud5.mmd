# ReST-MCTS+

Footnote †: Equal contribution.

LLM Self-Training via Process Reward Guided Tree Search

Dan Zhang\({}^{1}\)

Work done while DZ visited at Caltech.

Sining Zhoubian\({}^{1}\)

Work done while DZ visited at Caltech.

Ziniu Hu\({}^{2}\)

Work done while DZ visited at Caltech.

Yisong Yue\({}^{2}\)

Yuxiao Dong\({}^{1}\)

Jie Tang\({}^{1}\)

\({}^{1}\)The Knowledge Engineering Group (KEG), Tsinghua University;

\({}^{2}\)California Institute of Technology

{zd21,zbsn2}@mails.tsinghua.edu.cn

[https://rest-mcts.github.io/](https://rest-mcts.github.io/)

###### Abstract

Recent methodologies in LLM self-training mostly rely on LLM generating responses and filtering those with correct output answers as training data. This approach often yields a low-quality fine-tuning training set (e.g., incorrect plans or intermediate reasoning). In this paper, we develop a reinforced self-training approach, called **ReST-MCTS+**, based on integrating process reward guidance with tree search MCTS+ for collecting higher-quality reasoning traces as well as per-step value to train policy and reward models. ReST-MCTS++ circumvents the per-step manual annotation typically used to train process rewards by tree-search-based reinforcement learning: Given oracle final correct answers, ReST-MCTS++ is able to infer the correct process rewards by estimating the probability this step can help lead to the correct answer. These inferred rewards serve dual purposes: they act as value targets for further refining the process reward model and also facilitate the selection of high-quality traces for policy model self-training. We first show that the tree-search policy in ReST-MCTS++ achieves higher accuracy compared with prior LLM reasoning baselines such as Best-of-N and Tree-of-Thought, within the same search budget. We then show that by using traces searched by this tree-search policy as training data, we can continuously enhance the three language models for multiple iterations, and outperform other self-training algorithms such as ReSTEM and Self-Rewarding LM. We release all code at [https://github.com/THUDM/ReST-MCTS](https://github.com/THUDM/ReST-MCTS).

Footnote †: Equal contribution.

Footnote †: thanks: Equal contribution.

## 1 Introduction

Large Language Models (LLMs) are mostly trained on human-generated data. But as we approach the point where most available high-quality human-produced text on the web has been crawled and used for LLM training , the research focus has shifted towards using LLM-generated content to conduct self-training [2; 3; 4; 5; 6; 7]. Similar to most Reinforcement Learning (RL) problems, LLM self-training requires a reward signal. Most existing reinforced self-improvement approaches (e.g., STaR , RFT , ReSTEM , V-STaR ) assume to have access to a ground-truth reward model (labels from supervised dataset, or a pre-trained reward model). These approaches use an LLM to generate multiple samples for each question, and assume the one that leads to high reward (correct solution) is the high-quality sample, and later train on these samples (hence self-training). Such procedures can be effective in improving LLM performance, in some cases solving reasoning tasks that the base LLM cannot otherwise solve [8; 9; 10].

However, a key limitation of the above procedure is that even if a reasoning trace results in a correct solution, it does not necessarily imply that the entire trace is accurate. LLMs often generatewrong or useless intermediate reasoning steps, while still finding the correct solution by chance . Consequently, a self-training dataset can often contain many false positives -- intermediate reasoning traces or plans are incorrect, but the final output is correct -- which limits the final performance of LLM fine-tuning for complex reasoning tasks [18; 19]. One way to tackle this issue is to use a value function or reward model to verify reasoning traces for correctness (which then serves as a learning signal for self-training) [1; 12]. However, training a reliable reward model to verify every step in a reasoning trace generally depends on dense human-generated annotations (per reasoning step) , which does not scale well. Our research aims to address this gap by developing a novel approach that automates the acquisition of reliable reasoning traces while effectively utilizing reward signals for verification purposes. Our key research question is: **How can we automatically acquire high-quality reasoning traces and effectively process reward signals for verification and LLM self-training?**

In this paper, we propose ReST-MCTS\({}^{*}\), a framework for training LLMs using model-based RL training. Our proposed approach utilizes a modified Monte Carlo Tree Search (MCTS) algorithm as the reasoning policy, denoted MCTS\({}^{*}\), guided by a trained per-step process reward (value) model. A key aspect of our method is being able to automatically generate per-step labels for training per-step reward models, by performing a sufficient number of rollouts. This labeling process effectively filters out the subset of samples with the highest quality, without requiring additional human intervention. Table 1 summarizes the key distinctions between our approach and previous approaches. We validate experimentally that ReST-MCTS\({}^{*}\) outperforms prior work in discovering good reasoning traces, such as Self-Consistency (SC) and Best-of-N (BoN) under the same search budget on the SciBench  and MATH  benchmarks, which consequently leads to improved self-training.

To summarize, our contributions are:

* We propose ReST-MCTS\({}^{*}\), a self-training approach that generates process rewards searched by MCTS\({}^{*}\). A key step is to automatically annotate the process reward of each intermediate node via sufficient times of rollouts, using MCTS\({}^{*}\). We validate multiple reasoning benchmarks and find that ReST-MCTS\({}^{*}\) outperforms existing self-training approaches (e.g., ReSTEM and Self-Rewarding) as shown in Table 2 and reasoning policies (e.g., CoT and ToT) as shown in Table 4.
* The reward generator in ReST-MCTS\({}^{*}\) leads to a higher-quality process reward model compared to previous process reward generation techniques, e.g., MATH-SHEPHERD, as shown in Table 3.
* Given the same search budget, the search algorithm (MCTS\({}^{*}\)) in ReST-MCTS\({}^{*}\) achieves higher accuracy than Self-Consistency and Best-of-N, as shown in Figure 2.

## 2 Background on Reasoning & Self-Training

We follow the standard setup in LLM-based reasoning. We start with a policy, denoted by \(\), that is instantiated using a base LLM. Given an input problem \(Q\), in the simplest case, \(\) can generate an output sequence, or trace, of reasoning steps \((s_{1},s_{2},,s_{K})(|Q)\) by autoregressively predicting the next token. For simplicity, we assume a reasoning step comprises a single sentence (which itself comprises multiple tokens). We also assume the last output \(s_{K}\) is the final step. LLMs can also be prompted or conditioned to bias the generation along certain traces. For a prompt \(c\), we can write the policy as \((|Q,c)\). This idea was most famously used in chain-of-thought (CoT) .

    &  &  \\   & & Value Label & Train \\     } &  &  &   } \\  & & CoT & \\     } &   } &   } &   } \\  & & & \\   

Table 1: Key differences between existing self-improvement methods and our approach. Train refers to whether to train a reward model.

**Self-Consistency (SC).** Self-Consistency  samples multiple reasoning traces from \(\) and chooses the final answer that appears most frequently.

**Tree-Search & Value Function.** Another idea is to use tree-structured reasoning traces [24; 14], that branch from intermediate reasoning steps. One key issue in using a so-called tree-search reasoning algorithm is the need to have a value function to guide the otherwise combinatorially large search process . Two common value functions include Outcome Reward Models (ORMs) , which are trained only on the correctness of the final answer, and Process Reward Models (PRMs) , which are trained on the correctness of each reasoning step. We assume \(r_{s_{k}}\) is the PRM's output sigmoid score at \(k\)-th step. Our ReST-MCTS\({}^{*}\) approach uses tree-search to automatically learn a good PRM.

**Best-of-N.** As an alternative to Self-Consistency, one can also use a learned value function (PRM or ORM) to select the reasoning trace with the highest value .

**Self-Training.** At a high level, there are two steps to self-training [6; 12]. The first step is generation, where we sample multiple reasoning traces using \(\) (in our case, tree-structured traces). The second step is improvement, where a learning signal is constructed on the reasoning traces, which is then used to fine-tune \(\). The process can repeat for multiple iterations.

**Limitation of Prior Works.** The main challenge in doing reliable self-training is the construction of a useful learning signal. Ideally, one would want a dense learning signal on the correctness of every intermediate reasoning step, which is given by a PRM. Otherwise, with sparse learning signals, one suffers from a credit assignment similar to that in reinforcement learning. Historically, the main challenge with learning a PRM is the lack of supervised annotations per reasoning step. This is the principal challenge that our ReST-MCTS\({}^{*}\) approach seeks to overcome. We describe detailed preliminaries in Appendix A.

## 3 The ReST-MCTS\({}^{*}\) Method

Our approach, ReST-MCTS\({}^{*}\), is outlined in Figure 1 and developed using four main components.

* **MCTS\({}^{*}\)** which performs a tree search with sufficient rollout time under the guidance of the PRM.
* **Process Reward Model** (PRM) which evaluates any partial solution's quality and guides MCTS\({}^{*}\).
* **Policy Model** which generates multiple intermediate reasoning steps for each question.
* **LLM Self-Training,** which uses MCTS\({}^{*}\) to collect reasoning traces, trains policy model on positive samples, and trains process reward model on all generated traces.

### Search-based Reasoning Policy for LLM

**Value \(v_{k}\) for a Partial Solution.** The value (process) reward \(v_{k}\) of the partial solution \(p_{k}=[s_{1},s_{2},,s_{k}]\) should satisfy the following basic qualities:

* Limited range: \(v_{k}\) is constrained within a specific range. This restriction ensures that the values of \(v_{k}\) are bounded and do not exceed a certain limit.
* Reflecting probability of correctness: \(v_{k}\) reflects the probability that a partial solution is a complete and correct answer. Higher values of \(v_{k}\) indicate better quality or a higher likelihood of being closer to a correct answer.
* Reflecting correctness and contribution of solution steps: \(v_{k}\) incorporates both the correctness and contribution of each solution step. When starting from a partial solution, a correct next step should result in a higher \(v_{k}\) compared to false ones. Additionally, a step that makes more correct deductions toward the final answer should lead to a higher \(v_{k}\) value. This property ensures that \(v_{k}\) captures the incremental progress made towards the correct solution and rewards steps that contribute to the overall correctness of the solution.

**Reasoning Distance \(m_{k}\) for a Partial Solution.** To estimate the progress of a solution step, we define the reasoning distance \(m_{k}\) of \(p_{k}\) as the minimum reasoning steps a policy model requires to reach the correct answer, starting from \(p_{k}\). Reasoning distance reflects the progress made as well as the difficulty for a policy to figure out a correct answer based on current steps, thus it can be furtherused to evaluate the quality of \(p_{k}\). However, we point out that \(m_{k}\) can not be directly calculated. It is more like a hidden variable that can be estimated by performing simulations or trace sampling starting from \(p_{k}\) and finding the actual minimum steps used to discover the correct answer.

**Weighted Reward \(w_{s_{k}}\) for a Single Step.** Based on the desired qualities for evaluating partial solutions, we introduce the concept of a weighted reward to reflect the quality of the current step \(s_{k}\), denoted as \(w_{s_{k}}\). Based on the common PRM reward \(r_{s_{k}}\), \(w_{s_{k}}\) further incorporates the reasoning distance \(m_{k}\) as a weight factor, reflecting the incremental progress \(s_{k}\) makes.

**Representations for Quality Value and Weighted Reward.** To determine the quality value \(v_{k}\) of a partial solution at step \(k\), we incorporate the previous quality value and the weighted reward of the current step. By considering the previous quality value, we account for the cumulative progress and correctness achieved up to the preceding step. Therefore, the \(v_{k}\) can be iteratively updated as:

\[v_{k}=\{0,&k=0\\ max(v_{k-1}+w_{s_{k}},0),&else. \]

The weighted reward \(w_{s_{k}}\) of the current step provides a measure of the quality and contribution of that specific step towards the overall solution. Based on \(m_{k}\) (where \(m_{k}=K-k\) and \(K\) is the total number of reasoning steps of a solution \(s\)), previous quality value \(v_{k-1}\), and \(r_{s_{k}}\) in MATH-SHEPHERD , we can update the definition of the weighted reward \(w_{s_{k}}\) iteratively as follows:

\[w_{s_{k}}=}{m_{k}+1}(1-2r_{s_{k}}),\;\;k=1,2, \]

As \(k\) increases, \(m_{k}\) decreases, indicating that fewer reasoning steps are needed to reach the correct answer. This leads to a higher weight placed on the weighted reward of the current step. We can also derive that \(w_{s_{k}}\) and \(v_{k}\) satisfy the expected boundedness shown in the theorem below.

**Theorem 1** (Boundedness of \(w_{s_{k}}\) and \(v_{k}\)).: _If \(r_{s_{k}}\) is a sigmoid score ranged between \(\), then \(w_{s_{k}}\) and \(v_{k}\) defined as above satisfy following boundedness: \(w_{s_{k}} 1-v_{k-1}\), \(v_{k}\)._

_Derivation._ Please refer to the detailed derivation in Appendix B.1.

Therefore, we can conclude that \(w_{s_{k}}\) and \(v_{k}\) has following properties that match our expectations:

**Observation 1**.: _If a reasoning route starting from \(p_{k}\) requires more steps to get to the correct answer, then the single-step weighted reward \(w_{s_{k}}\) is lower._

Figure 1: The left part presents the process of inferring process rewards and how we conduct process reward guide tree-search. The right part denotes the self-training of both the process reward model and the policy model.

**Observation 2**.: \(w_{s_{k}}\) _decreases as the PRM's predicted sigmoid score \(r_{s_{k}}\) rises. Thus, \(w_{s_{k}}\) has a positive correlation with the PRM's prediction of a step's correctness._

**Observation 3**.: \(v_{k} 1 r_{s_{k}} 0,\ m_{k}=0\)_, i.e. \(v_{k}\) converges to upper bound \(1\) only when \(s_{k}\) reaches the correct answer._

Based on the features of \(v_{k}\) and \(w_{s_{k}}\), we can directly predict the quality value of partial solutions and guide search once we have a precise PRM and accurate prediction of \(m_{k}\). In our approach, instead of separately training models to predict \(r_{s_{k}}\) and \(m_{k}\), we simply train a process reward model \(V_{}\) to predict \(v_{k}\), serving as a variant of common PRM. With reward incorporated in the calculation of \(v_{k}\), there is no need to separately train a reward model, saving considerable effort for answer selection.

**Process Reward Model Guided Tree Search MCTS\({}^{*}\).** Tree search methods like  and  require a value function and outcome reward model \(r_{}\) to prune branches, evaluate final solutions and backup value. However, using ORM to evaluate final solutions and backpropagate means every search trace must be completely generated, which is costly and inefficient. Recent work  suggests using a learned LLM value function in MCTS so the backup process can happen in the intermediate step, without the need for complete generations. Their work greatly improves search efficiency but still relies on an ORM to select the final answer. Drawing inspiration from these works, we further propose a new variant of MCTS, namely \(^{*}\), which uses quality value \(v_{k}\) as a value target for a trained LLM-based process reward model and guidance for MCTS as well.

Given the above properties, we can directly use the process reward model \(V_{}\) to evaluate the quality of any partial solution, select, and backpropagate in intermediate nodes. Aside from the use of quality value, we also incorporate a special Monte Carlo rollout method and self-critic mechanism to enhance efficiency and precision, which are explained detailedly in Appendix C.1. We express MCTS\({}^{*}\) as an algorithm that comprises four main stages in each iteration, namely node selection, thought expansion, greedy MC rollout, and value backpropagation. Similar to common MCTS settings, the algorithm runs on a search tree \(T_{q}\) for each single science reasoning question \(q\). Every tree node \(C\) represents a series of thoughts or steps, where a partial solution \(p_{C}\), number of visits \(n_{C}\), and corresponding quality value \(v_{C}\) are recorded. For simplicity, we denote each node as a tuple \(C=(p_{C},n_{C},v_{C})\). An overall pseudo-code for MCTS\({}^{*}\) is presented in Algorithm 2.

### Self-Training Pipeline

As shown in Figure 1, based on the proposed tree search algorithm MCTS\({}^{*}\), we perform self-improvement on the reasoning policy and process reward model. After initialization of the policy \(\) and process reward model \(V_{}\), we iteratively employ them and utilize the search tree \(T_{q}\) generated in the process to generate high-quality solutions for specific science or math questions and conduct a self-improvement process, called ReST-MCTS\({}^{*}\). Our work draws inspiration from the MuZero  framework and applies it to the training of LLMs which we term "MuZero-style learning of LLMs".

**Instruction Generation.** In this stage, initialization starts from an original dataset \(D_{0}\) for the training process reward model \(V_{}\).

\(\)**Collect process reward for process reward model.** The extraction of new value data is relatively more complex, we derive the target quality value of partial solutions of every tree node near a correct reasoning path on the pruned search tree \(T^{{}^{}}_{q}\). We first calculate \(m_{k}\) for every tree node \(C\) that is on at least one correct reasoning trace (including the root) according to its minimum reasoning steps required to get to a correct answer in \(T^{{}^{}}_{q}\). Then, we use the hard estimation in Eq. (11) in  to calculate \(r_{s_{k}}\), i.e. \(r_{s_{k}}=1-r_{s_{k}}^{}\), which means a reasoning step is considered correct if it can reach a correct answer in \(T^{{}^{}}_{q}\). Using \(m_{k}\) and \(r_{s_{k}}\), we are able to derive the value of the partial solution of every node on or near one correct reasoning trace. For each node \(C\) (with partial solution \(p_{C}=[s_{1},s_{2},,s_{k-1}]\)) on at least one correct trace and a relevant forward step \(s_{k}\), we can derive the value \(v_{k}\) using Eq. (1) and weighted reward \(w_{k}\) using Eq. (2), with \(m_{k}\) set to the same as \(m_{k-1}\) if \(r_{s_{k}}^{}=0\) in Eq. (11). A concrete and detailed example of this inferring process is shown in Figure 3. We update all these rewards and values starting from the root and collect all \((Q,p,v)\) pairs to form \(D_{V_{i}}\) in \(i\)-th iteration, which is used for training a process reward model in the next iteration.

\(\)**Collect reasoning traces for policy model.** As shown in Figure 4, the search process produces a search tree \(T_{q}\), consisting of multiple reasoning traces. We first prune all the unfinished branches (branches that do not reach a final answer). Then we verify other traces' final answers acquired in the tree search according to their correctness through simple string matching or LLM judging and select the correct solutions. These verified reasoning traces, as \(D_{G_{i}(A_{j}=a^{*})|_{j=1}^{N}}\) (where \(N\) is the number of sampling solutions, \(A_{j}\) is the \(j\)-th solution, and \(a^{*}\) is the final correct answer) in \(i\)-th iteration, are then used for extracting new training data for policy self-improvement. This process is followed by Eq. (13) (\(i 1\)) to execute the policy self-training.

**Mutual Self-training for Process Reward Model and Policy Model.** Compared to previous work like ReST\({}^{}\), which only concerns self-training for the policy and demonstrates that the policy can improve by iteratively generating new traces and learning from the high-reward ones generated by itself, our work simultaneously improves the process reward model and policy model self-training. With the process reward model's training set \(D_{V_{0}}\) initialized and new problem set \(D_{G}\) given, we can start the iterative self-training process upon \(V_{}\) and \(\). We use \(\) to perform MCTS\({}^{*}\) and generate solutions for \(D_{G}\), with implement details illustrated in Section 3.1. In the \(i\)-th (\(i=1,2,\)) iteration, we train \(V_{}\) with \(D_{V_{i-1}}\) to obtain \(V_{i}\) and train policy model \(_{S_{i-1}}\) on \(D_{G_{i}}\) to generate new generator \(_{S_{i}}\). At the same time, \(D_{G_{i}}\) drives the update of \(V_{i}\) to \(V_{i+1}\). We present iterative self-training that the process reward model and policy model complement each other in Algorithm 1.

```
1:base LLM \(\), original dataset for policy model \(D_{S_{0}}\), original dataset for value model \(D_{0}\), new problem set \(D_{G}\), number of solutions \(N\), \(j\)-th solution \(A_{j}\), correct solution \(a^{*}\), value model \(V_{}\), weighted value function \(w\), quality value function \(v\), number of iterations \(T\).
2:\(_{S_{0}}\) SFT(\(,D_{S_{0}}\)) // fine-tune generator
3:\(D_{V_{0}}\) generate_value_data(\(D_{0},w,v\)) // initialize train set for value model
4:\(V_{0}\) train_value_model(\(V_{},D_{V_{0}}\)) // initialize value model
5:for\(i=1\) to \(T\)do
6:\(D_{G_{i}}\) generate_policy_data(\(_{S_{i-1}}\), \(V_{i-1}\) guided MCTS\({}^{*}\), \(D_{G}\), \(N\)) // generate synthetic data for policy model
7:for\(j=1\) to \(N\)do
8:\(D_{G_{i}(A_{j}=a^{*})}\) label_correctness(\(D_{G_{i}}\)) // match and select correct solutions
9:endfor
10:\(_{S_{i}}\) SFT(\(_{S_{i-1}},D_{G_{i}(A_{j}=a^{*})|_{j=1}^{N}}\)) // self-training policy model
11:\(D_{V_{i}}\) extract_value_data(\(D_{G_{i}}\)) // collect process reward and extract value data
12:\(V_{i}\) train_value_model(\(V_{i-1},D_{V_{i}}\)) // self-training value model
13:endfor
14:\(_{S_{T}},V_{T}\)
```

**Algorithm 1**Mutual self-training ReST-MCTS\({}^{*}\) for value model and policy model.

## 4 Experiments

We validate ReST-MCTS\({}^{*}\) from three perspectives:

\(\)**Self-Training approaches** which use generated samples and evaluated for multiple iterations, such as ReST\({}^{}\) and Self-Rewarding, on in-distribution and out-of-distribution benchmarks under three LLM backbones, as shown in Table 2. ReST-MCTS\({}^{*}\) outperforms existing approaches in each iteration and continuously self-improves with the data it generates.

\(\)**Process reward models** which are compared with the state-of-the-art techniques, such as MATH-SHEPHERD (MS) and SC + MS on GSM8K and MATH500, as shown in Table 3. Results indicate that the ReST-MCTS\({}^{*}\) learns a good PRM and our reward model implements higher accuracy.

\(\)**Tree-Search policy** which are compared on college-level scientific reasoning benchmark under three LLMs, such as CoT and ToT, as shown in Table 4. We also evaluated under the same search budget on MATH and SciBench, such as SC and Best-of-N, as shown in Figure 2. Results show the ReST-MCTS\({}^{*}\) significantly outperforms other baselines despite insufficient budget.

### Initialization of Value Model

To obtain accurate feedback from the environment, we build the value model's initial train set \(D_{V_{0}}\) from a set of selected science or math questions \(D_{0}\) using process reward (value) inference, with no human labeling process required. Then, we finetune the ChatGLM3-6B [27; 28] and Mistral-7B  model on this dataset, respectively, obtaining initial value models that, as variants of PRM, guide the LLM tree search for higher-quality solutions upon both math and science questions.

**Fine-grained dataset for science and math.** Aiming to gather value train data for science, we integrate questions of a lean science dataset \(D_{sci}\) within SciInstruct  into \(D_{0}\). This dataset consists of 11,554 questions, where each question is paired with a correct step-by-step solution. For each question \(q^{(i)}(i=1,2,,N)\) and corresponding solution \(s^{(i)}=s^{(i)}_{1,2,,K_{i}}\) in \(D_{sci}\), we extract all partial solutions to form samples \(d^{(i)}_{k}=[q^{(i)},s^{(i)}_{1,2,,k}(p^{(i)}_{k})](k=1,2,,K_{i})\). To make the value model distinguish false steps, we also employ a LLM policy (ChatGLM2) that is basically incompetent for reasoning tasks of this difficulty to generate single steps \(s^{(i)^{}}_{k+1}\) given \(q^{(i)}\) and \(p^{(i)}_{k}\), obtaining new partial solutions \(p^{(i)^{}}_{k+1}=[s^{(i)}_{1,2,,k},s^{(i)^{}}_{k+1}]\) and new samples \(d^{(i)}_{k,j}=[q^{(i)},s^{(i)}_{1,2,,k},s^{(i)^{}}_{k+1,j}](j=1,2,3)\). For simplicity, the generated steps are regarded as incorrect. In total, we collect \(473.4\)k samples for training the initial value model. Afterward, we derive target quality values for all samples \(d^{(i)}_{k,j}\) and \(d^{(i)}_{k}\) and use them to construct \(D_{V_{0}}\), which is illustrated in Appendix B.1. We adopt an alternative method to generate value train data for math, as shown in Appendix B.1.

   Model & Self-Training Methods & MATH & GPQA\({}_{0}\) diamond & CEval-Hard & Ave. \\    } & 0th iteration (zero-shot) & 20.76 & 27.27 & 26.32 & 24.78 \\  & 0th iteration (few-shot) & 30.00 & 31.31 & 25.66 & 28.99 \\   &  \\   & w/ ReST\({}^{}\) (1st iteration) & 30.84 & 26.77 & 21.05 & 26.22 \\  & w/ Self-Rewarding (1st iteration) & 30.34 & 26.26 & 25.66 & 27.42 \\  & w/ **ReST-MCTS\({}^{*}\) (1st iteration)** & 31.42 & 24.24 & 26.97 & 27.55 \\   & w/ ReST\({}^{}\) (2nd iteration) & 33.52 & 25.25 & 21.71 & 26.83 \\  & w/ Self-Rewarding (2nd iteration) & 33.89 & 26.26 & 23.03 & 27.73 \\  & w/ **ReST-MCTS\({}^{*}\) (2nd iteration)** & 34.28 & 27.78 & 25.00 & **29.02** \\    } & 0th iteration (zero-shot) & 29.34 & 27.78 & 9.87 & 22.33 \\  & 0th iteration (few-shot) & 28.28 & 29.29 & 9.21 & 22.26 \\   &  \\   & w/ ReST\({}^{}\) (1st iteration) & 23.84 & 26.26 & 20.39 & 23.50 \\  & w/ Self-Rewarding (1st iteration) & 25.70 & 27.78 & 19.74 & 24.40 \\  & w/ **ReST-MCTS\({}^{*}\) (1st iteration)** & 31.06 & 26.26 & 17.11 & 24.81 \\   & w/ ReST\({}^{}\) (2nd iteration) & 23.86 & 26.26 & 22.37 & 24.16 \\  & w/ Self-Rewarding (2nd iteration) & 23.90 & 26.77 & 25.00 & 25.22 \\  & w/ **ReST-MCTS\({}^{*}\) (2nd iteration)** & 24.40 & 28.79 & 26.32 & **26.50** \\    } & 0th iteration & 25.18 & 23.74 & 51.97 & 33.63 \\   &  \\   & w/ ReST\({}^{}\) (1st iteration) & 22.72 & 24.75 & 51.32 & 32.93 \\   & w/ Self-Rewarding (1st iteration) & 22.50 & 26.26 & 47.37 & 32.04 \\   & w/ **ReST-MCTS\({}^{*}\) (1st iteration)** & 24.86 & 25.25 & 51.32 & 33.81 \\    & w/ ReST\({}^{}\) (2nd iteration) & 25.86 & 25.25 & 48.68 & 33.27 \\   & w/ Self-Rewarding (2nd iteration) & 23.86 & 28.79 & 48.03 & 33.56 \\   & w/ **ReST-MCTS\({}^{*}\) (2nd iteration)** & 23.90 & 31.82 & 51.97 & **35.90** \\   

Table 2: Primary results by training both policy and value model for multiple iterations. For each backbone, different self-training approaches are conducted separately. This means each approach has its own generated train data and corresponding reward (value) model. Our evaluation is zero-shot only, the few-shot baseline only serves as a comparison.

### Evaluating Self-Improvement of ReST-MCTS\({}^{*}\)

In order to thoroughly examine the influence of ReST-MCTS\({}^{*}\) self-training on varied backbones, we execute 2 iterations of self-training and compare two representative self-training approaches, ReST\({}^{}\), which compares outcome reward with ground-truth answer, and Self-Rewarding, which judges outcome reward by LLMs, upon 3 different base models, namely LLaMA-3-8B-Instruct , Mistral-7B: MetaMATH [29; 31] and SciGLM-6B . Primary results are shown in Table 2. Concerning the dataset for sample generation, since we are primarily interested in the continuous improvement ability of ReST-MCTS\({}^{*}\) in a specific domain, we mainly include math questions in the dataset. For simplicity, we use the same dataset \(D_{G}\) in each iteration. It involves questions selected from a train set of well-known benchmarks including MATH, GSM8K, and TheoremQA . With the policy and value model trained simultaneously on samples generated from \(D_{G}\), we observe that our self-training paradigm enables continuous enhancement of the capabilities of both models on in-distribution and out-of-distribution benchmarks, regardless of which backbone is used.

\(\)**Iterative performance improvement on policy model.** Previous LLM self-training approaches mostly rely on the generating responses of LLM and assume each question with the correct solution is a high-quality sample while the intermediate reasoning steps are wrong or useless in many cases. Therefore, we compare the ReST-MCTS\({}^{*}\) with recent self-training paradigms by generating new samples under different reward (value) supervision strategies. For ReST\({}^{}\) and Self-Rewarding, the default sampling strategy is generating CoT data, with generated data refined according to ground truth or reward provided by the policy, respectively. In comparison, ReST-MCTS\({}^{*}\) generates data samples via MCTS\({}^{*}\), with data refined referring to quality value and ground truth. The results in Table 2 show that all three backbones can be continuously self-improved by data generated by itself, using ReST-MCTS\({}^{*}\) as a paradigm. ReST-MCTS\({}^{*}\) significantly outperforms previous self-training methods ReST\({}^{}\) and Self-Rewarding basically in each iteration. This means the ReST-MCTS\({}^{*}\) can screen out self-generated data of higher quality for better self-improvement.

\(\)**Iterative performance improvement on reward model.** We also compare how our iterative trained policy and value model can improve the overall search results under the same token usage on the test set of MATH . See implementation details in Appendix E.3. We show results in Figure 2 (a), where ReST-MCTS\({}^{*}\) (Iter #1) greatly outperforms most baselines but does not completely surpass Self-Consistency. In comparison, after more iterations of self-training, verification based on the enhanced value model basically outperforms Self-Consistency on every point, achieving the highest accuracy of \(48.5\%\) that significantly exceeds the \(42.5\%\) of Self-Consistency. This indicates the effectiveness of our self-training pipeline.

   Models & Dataset & SC & ORM & SC+ORM & MS & SC + MS & SC + ReST-MCTS\({}^{*}\) (Value) \\   & GSM8K & 83.9 & 86.2 & 86.6 & 87.1 & 86.3 & **87.5** \\  & MATH500 & 35.1 & 36.4 & 38.0 & 37.3 & 38.3 & **39.0** \\   

Table 3: Accuracy of different verifiers on GSM8K test set and MATH500. SC: Self-Consistency, MS: MATH-SHEPHERD. Verification is based on 256 outputs.

Figure 2: Accuracy of different searches on MATH and SciBench with varied sampling budget.

[MISSING_PAGE_FAIL:9]

Related Work

### Large Language Model Training

Large Language Models (LLMs) [36; 37; 38] have emerged as a notable success in various natural language tasks. Recent studies focus on improving the reasoning capabilities of LLMs, including collecting high-quality or larger domain-specific data [39; 40; 41; 42; 10; 43], designing elaborate prompting [22; 44; 45; 46], or training supervised learning [10; 31; 32; 47] or reinforcement learning (RL) [48; 49; 50; 16]. When LLMs are trained with the RL algorithm, the generation of LLMs can be naturally expressed as the Markov Decision Process (MDP) and optimized for specific objectives. According to this formula, InstructionGPT  has achieved remarkable success in optimizing LLMs to align human preferences by utilizing RL from Human Feedback (RLHF) . RLAIF then uses AI feedback to extend RL from human feedback . Our work aims to propose an LLM self-training method via process rewards guided tree search.

### Large Language Model Reasoning

LLM reasoning algorithms include prompt-based chain-of-thought (CoT) , planning-based represented by tree-of-thought (ToT) . Scientific reasoning has several categories to mine the potential of existing large language models, resulting from different performances for problem-solving. Previous studies have attempted to outperform the direct generation. For example, in this paper , an approach for generating solutions in a step-by-step manner is proposed, another model or function is used to select the top-ranked answers, and hallucination is avoided by limiting the output to a narrower set.  presents a majeutic prompting inference method, which can generate abductive explanations of various hypotheses explained by recursion, eliminate contradicting candidates, and achieve logically consistent reasoning. Chain-of-thoughts (CoT)  imitates the thought process like humans to provide step-by-step solutions given a question. Self-Consistency CoT  improves the reliability and Self-Consistency of answers by sampling multiple interpretations from LM and then selecting the final answer that appears most frequently. Tree-of-Thoughts (ToT)  further generalizes the CoT methodology by considering multiple different reasoning paths in the tree and exploring coherent units of thought to execute thoughtful decision-making. In our work, we benchmark hard science reasoning tasks against [22; 24; 34; 35].

## 6 Conclusion

In this paper, we propose ReST-MCTS\({}^{*}\), self-training both policy and process reward model by high-quality samples generated by reward guided tree search. Inferred rewards from the previous iteration are able to refine the process reward model and self-train the policy model with high-quality traces. Experimental results show that the ReST-MCTS\({}^{*}\) outperforms other self-training paradigms and achieves higher accuracy than previous reasoning baselines under the same search budget.

**Limitation:** We discussed limitation in detail at Section H in Appendix. In summary, we need to show the ReST-MCTS\({}^{*}\) can generalize to other reasoning tasks outside of math (like coding, agent, etc); and tasks without ground-truth (dialogue, SWE-Bench , etc). We also need to scale up the proposed value model and further improve the data filtering techniques. One potential idea is to incorporate online RL algorithms that can help perform better self-training for value models and policy models.