# MotionGS: Exploring Explicit Motion Guidance for Deformable 3D Gaussian Splatting

Ruijie Zhu Yanzhe Liang Hanzhi Chang Jiacheng Deng Jiahao Lu Wenfei Yang Tianzhu Zhang Yongdong Zhang

Equal contributionCorresponding author

###### Abstract

Dynamic scene reconstruction is a long-term challenge in the field of 3D vision. Recently, the emergence of 3D Gaussian Splatting has provided new insights into this problem. Although subsequent efforts rapidly extend static 3D Gaussian to dynamic scenes, they often lack explicit constraints on object motion, leading to optimization difficulties and performance degradation. To address the above issues, we propose a novel deformable 3D Gaussian splatting framework called MotionGS, which explores explicit motion priors to guide the deformation of 3D Gaussians. Specifically, we first introduce an optical flow decoupling module that decouples optical flow into camera flow and motion flow, corresponding to camera movement and object motion respectively. Then the motion flow can effectively constrain the deformation of 3D Gaussians, thus simulating the motion of dynamic objects. Additionally, a camera pose refinement module is proposed to alternately optimize 3D Gaussians and camera poses, mitigating the impact of inaccurate camera poses. Extensive experiments in the monocular dynamic scenes validate that MotionGS surpasses state-of-the-art methods and exhibits significant superiority in both qualitative and quantitative results. Project page: [https://ruijiezhu94.github.io/MotionGS_page/](https://ruijiezhu94.github.io/MotionGS_page/).

University of Science and Technology of China / Deep Space Exploration Lab

{ruijiezhu, yzliang, changhz, dengjc, luijiahao}@mail.ustc.edu.cn, {yangwf, tzzhang, zhyd73}@ustc.edu.cn

## 1 Introduction

Dynamic scene reconstruction aims to model the 3D structure and appearance of time-evolving scenes, enabling novel-view synthesis at arbitrary timestamps. It is a crucial task in the field of 3D computer vision, attracting widespread attention from the research community and finding important applications in areas such as virtual/augmented reality and 3D content production. In comparison to static scene reconstruction, dynamic scene reconstruction remains a longstanding open challenge due to the difficulties arising from motion complexity and topology changes.

In recent years, a plethora of dynamic scene reconstruction methods  have been proposed based on Neural Radiance Fields (NeRF) , driving rapid advancements in this field. While these methods exhibit impressive visual quality, their substantial computational overhead impedes their applications in real-time scenarios. Recently, a novel approach called 3D Gaussian Splatting (3DGS) , has garnered widespread attention in the research community. By introducing explicit 3D Gaussian representation and efficient CUDA-based rasterizer, 3DGS has achieved unprecedented high-quality novel-view synthesis with real-time rendering. Subsequent methods  rapidly extend 3DGS to dynamic scenes, also named 4D scenes. Initially, D-3DGS  proposes to iteratively reconstruct the scene frame by frame, but it incurs significant memory overhead. The more straightforward approaches  utilize a deformation field to simulate the motion of objects by moving the 3D Gaussians to their corresponding positions at different time steps. Besides, some methods  do not independently model motion but treat space-time as a whole to optimize. While these methods effectively extend 3DGS to dynamic scenes, they rely solely on appearance to supervise dynamic scene reconstruction, lacking explicit motion guidance on Gaussian deformation. When object motion is irregular (e.g., sudden movements), the model may encounter optimization difficulties and fall into local optima.

Based on the above discussions, we argue that explicit motion guidance is indispensable for the deformation of 3D Gaussians. Benefiting from the advancements in optical flow estimation , a natural solution is to utilize an off-the-shelf optical flow network to provide 2D motion priors . However, the formation of optical flow is affected by both camera motion and object motion, which is not conducive to explicit modeling of object motion. Therefore, it is necessary to separate the optical flow related only to the moving object (_i.e._, motion flow) to guide Gaussian deformation more efficiently. As shown in Figure 1(a), directly using optical flow (column 2) to supervise the Gaussian deformation will inevitably include the contribution of static objects to the optical flow, while using motion flow as supervision (column 3) can easily avoid this. Besides, the estimated camera pose in dynamic scenes is not always accurate. Due to the lack of geometric consistency between adjacent frames for moving objects, using point correspondences on dynamic objects to calculate the camera pose can lead to erroneous offsets, thereby affecting the optimization of 3DGS.

To address the above issues, we propose a novel deformable 3D Gaussian Splitting framework called MotionGS, which explicitly constrains the deformation of 3D Gaussians by extracting the motion priors from optical flow. Our method includes an optical flow decoupling module and a camera pose refinement module. In the optical flow decoupling module, we decouple the 2D optical flow into camera flow and motion flow, as shown in Figure 1(b). The camera flow comes from the camera ego-motion, while the motion flow comes from the motion of dynamic objects. We use the motion flow to directly constrain the deformation of 3D Gaussians (_i.e._, Gaussian flow). Since the calculation of Gaussian flow is directly implemented in the CUDA-based rasterizer, this process is differentiable and efficient. In the camera pose refinement module, we first fix the 3D Gaussians and then utilize photometric consistency loss to backpropagate gradients to camera poses, thereby alternately optimizing 3D Gaussians and camera poses to further enhance the rendering quality.

To sum up, our main contributions are as follows:

* We propose a novel deformable 3D Gaussian framework called MotionGS, which provides explicit motion guidance for deformable 3DGS and achieves high-quality dynamic scene reconstruction with real-time rendering.
* The proposed optical flow decoupling module effectively separates the flow caused solely by object motion, thereby efficiently supervising the deformation of 3D Gaussians. The

Figure 1: **(a) Gaussian flow under different supervision.** We model Gaussian flow under the supervision of optical flow and motion flow respectively. The latter can produce a more direct description of object motion, thereby effectively guiding the deformation of 3D Gaussians. **(b) The decoupling of optical flow.** We decouple the optical flow into motion flow which is only related to object motion and camera flow which is only related to camera motion.

proposed pose refinement module alternately optimizes 3DGS and camera poses, reducing reliance on accurate camera poses and further boosting rendering quality.
* Extensive experiments have demonstrated the effectiveness of the proposed method. Results on the NeRF-DS and HyperNeRF datasets validate the state-of-the-art performance of our approach in dynamic scene reconstruction.

## 2 Related Work

### Novel-View Synthesis (NVS)

Novel view synthesis has been a hot research topic in the field of computer vision and graphics in recent years. NeRF , which represents 3D scene by neural radiance fields, first achieves high-resolution photorealistic results in this field. Despite many subsequent works [23; 24; 25; 26; 27; 28; 29; 30; 31] have been proposed to improve its efficiency and quality, NeRF-based methods still struggle to render high-quality images with real-time rendering speed. Recently, by modeling 3D scenes using a set of anisotropic 3D Gaussian with an efficient rasterizer, 3D Gaussian Splatting (3DGS)  has shown remarkable performance with real-time rendering. Compared to NeRF, 3DGS is an explicit 3D scene representation method with better scalability and editability. Therefore, it has been rapidly extended to other 3D vision tasks, including sparse-view reconstruction [32; 33; 34; 35], 3D generation [36; 37; 38; 39; 40], scene editing [41; 42; 43] and SLAM [44; 45; 46; 47].

### Dynamic Scene Reconstruction

In recent years, various dynamic scene reconstruction approaches have been proposed, which can be broadly categorized into NeRF-based and 3DGS-based methods. NeRF-based works [48; 1; 49; 2; 4; 50; 51; 5] usually map dynamic scenes to a canonical space and render images based on this 3D canonical space. This kind of 4D scene representation is intuitive but requires a well-reconstructed canonical space. Other works propose to use time-varying NeRFs [6; 3; 52; 53; 54] or explicit representations [55; 56; 7; 8; 57; 58; 59] to represent and render dynamic scenes. However, all these NeRF-based methods require frequent point sampling or MLP queries, suffering from long training and rendering time. With the proposal of 3DGS, many works [16; 21; 15; 11; 13; 17; 12; 60; 14; 61; 62; 63] use 3DGS as the fundamental model for 4D scene representation. For instance, D-3DGS  models dynamic scenes by allowing the positions and rotation matrixes of 3DGS to change over time. Deformable 3DGS  uses an MLP to model a deformation field based on time and the canonical Gaussian space. SC-GS  bounds dense 3DGS with sparse control points, calculating the movement of Gaussians in a coarse-to-fine manner. Despite they have performed impressive rendering quality in some dynamic scenes, they lack explicit motion guidance to constrain the movement of Gaussian, resulting in degraded performance in more complex dynamic scenes. Recent works [21; 22] compose the movement of 3D points through their corresponding Gaussians, using 2D flow priors to supervise the deformation of 3DGS. Inspired by them, we decompose the optical flow to obtain more direct motion supervision, thus achieving higher rendering quality.

### NVS with Pose Optimization

Several NVS works [64; 65; 66; 67; 68; 69; 70] have noticed that it is difficult to derive precise camera poses of input images in the real world, so they address novel view synthesis together with camera pose optimization. i-NeRF  initially estimates camera poses by matching the input images. Other methods such as NeRFmm  and Nope-NeRF  use monocular depth priors as guidance to do the joint optimization of NeRF and camera poses. Recently, CF-3DGS  proposes progressive reconstruction and leverages photometric loss to learn the affine transformation of Gaussians to optimize the camera pose. However, these methods are mostly effective only for static scenes and lack support for dynamic scenes. Motivated by these methods, we aim to extend 3DGS to dynamic scenes with pose optimization, thus boosting the rendering quality and robustness.

## 3 Preliminary

In this section, we briefly introduce the modeling and rendering of 3DGS in Section 3.1 and the deformable extension of 3DGS towards dynamic scene reconstruction in Section 3.2.

### 3D Gaussian Splatting

As an explicit 3D representation similar to point clouds, 3DGS models the scene with a set of 3D Gaussians. However, different from point clouds, each 3D Gaussian in the scene has its own opacity \(o\), center position \(^{3 1}\), and covariance matrix \(^{3 3}\). These properties determine the contribution and influence range of 3D Gaussians on rendering. For a position \(x^{3 1}\) in 3D space, the corresponding contribution of a 3D Gaussian on it can be formulated as:

\[G(x)=o e^{-(x-)^{}^{-1}(x-)}. \]

For differentiable optimization, the covariance matrix \(\) can be decomposed into a scaling matrix \(\) and a rotation matrix \(\): \(=^{T}^{T}\), where scaling matrix \(=([s_{x},s_{y},s_{z}])\) and rotation matrix \(\) can be transformed from a quaternion \([r_{w},r_{x},r_{y},r_{z}]\). Then the 3D Gaussians can be splatted to a 2D camera plane through differential gaussian splatting. Specially, given a viewing transform matrix \(W\) and the Jacobian matrix \(J\) of the affine approximation of the projective transformation, we can obtain the 2D covariance matrix \(_{}\) through: \(_{}=JW W^{T}J^{T}\). Similarly, we can obtain the 2D center position \(_{}\) of 3D Gaussians in camera plane. Therefore, given a 2D pixel \(p\), the rendering contribution of a 3D Gaussian on the viewpoint \(W\) can be obtained through a 2D version of (1). To model the appearance of 3D Gaussians, spherical harmonics (SH) are introduced to define the color \(c\). Finally, for each pixel, the rendering results of 3DGS can be derived by calculating the color contribution of all the related Gaussians. This process is known as \(\)-blending:

\[C=_{i}^{N}c_{i}_{i}_{j=1}^{i-1}(1-_{j}), \]

where \(c_{i}\), \(_{i}\) represent the color and density computed from the \(i\)-th 3D Gaussian.

### Deformable 3D Gaussian Splatting

To extend 3DGS to dynamic scenes, an intuitive approach is to utilize a learnable deformation field to fit the movement of objects in the real world through Gaussian deformation. This idea originates from NeRF-based methods such as D-NeRF  and has been effectively applied to 3DGS in recent works [17; 13]. In these deformable 3DGS methods, a deformation network \(\) is typically used to model the movement of the center position of 3D Gaussians. Additionally, due to the inherent properties of 3D Gaussians, the deformation network \(\) may also consider the rotation and scaling factors of 3D Gaussians as they vary over time. Therefore, the deformation of 3D Gaussians can be formulated as:

\[(+,r+ r,s+ s)=(,r,s,t), \]

where \(t\) is the timestamp, \(,r,s\) are the center position, rotation quaternion and scaling factors of 3D Gaussians, and \(, r, s\) are their residuals, respectively. Due to the various implementations of deformable 3DGS, in this paper we focus solely on the deformation aspect without discussing the other designs and specific differences in these works. We select method  as our baseline, leveraging explicit motion guidance and camera pose refinement to further enhance the rendering quality and the robustness in dynamic scenes.

## 4 Methodology

In this section, we first introduce the overall architecture of our approach in Section 4.1. Then the optical flow decoupling module is introduced to derive motion guidance for Gaussian deformation in Section 4.2. The camera pose refinement module is introduced to alternately optimize 3D Gaussians and camera poses in Section 4.3. Finally, the overall loss function is introduced in Section 4.4.

### Overall Architecture

The overall architecture of our method is illustrated in Figure 2. Our method primarily focuses on the reconstruction of monocular dynamic scenes. Firstly, following 3DGS , we initialize camera poses and 3D Gaussians using COLMAP . Given two adjacent frames \(I_{t}\) and \(I_{t+1}\), we compute forward optical flow \(F_{t t+1}\) using an off-the-shelf flow estimation network. Meanwhile, we can obtain the rendered depth map \(D_{t}\) of frame \(I_{t}\) at time \(t\) through the rasterizer. By feeding the depthmap \(I_{t}\), camera poses \(C_{t},C_{t+1}\), and optical flow priors \(F_{t t+1}\) into the optical flow decoupling module, we can calculate the motion flow \(F_{t t+1}^{M}\) solely related to object movement. After predicting the deformation of Gaussians through the deformation network \(\), we obtain the state of 3D Gaussians at time \(t+1\) and render the Gaussian flow \(F_{t t+1}^{G}\) from time \(t\) to \(t+1\) under the assumption of a stationary camera viewpoint for the frame \(I_{t}\). The motion flow should be consistent with the Gaussian flow, thus providing explicit motion guidance to Gaussian deformation. Additionally, since the initialized camera poses may be inaccurate, we add a small residual \( T\) to the relative camera pose \(T\). Leveraging the proposed camera pose refinement module, we cleverly backpropagate gradients to the camera poses, achieving refinement of the camera poses. During training, we alternately optimize 3D Gaussian and camera poses to enhance the rendering quality and robustness in dynamic scenes.

### Optical Flow Decoupling Module

To provide explicit motion guidance for the deformation of Gaussians, we first utilize an off-the-shelf optical flow network to predict 2D motion priors. Since optical flow is influenced by both camera movement and object motion, we decompose it into camera flow and motion flow as illustrated in the Figure 1(b). Camera flow represents the optical flow caused solely by camera movement, assuming the objects in the scene remain stationary. In contrast, motion flow considers the camera as stationary, capturing only the movement of the objects. Essentially, optical flow can be viewed as the vector sum of these two components. By decoupling them, we can effectively isolate object motion, providing precise guidance for Gaussian deformation.

Camera flow and motion flow.We use a schematic diagram Figure 3 to illustrate the detailed calculation process. Camera flow can be directly computed from the camera poses and the depth of the current frame. Specifically, at the timestamp \(t\), we obtain the depth map \(D_{t}\) corresponding to frame \(I_{t}\) directly from 3D Gaussians through the rasterizer. Given the intrinsics \(K_{t}\) and extrinsics \(T_{t}\) of camera \(C_{t}\), we can reproject point \(p_{t}\) from frame \(I_{t}\) to 3D space using its depth \(D_{t}\):

\[x_{t}=T_{t}^{-1}K_{t}^{-1}D_{t}_{t}, \]

where \(_{t}\) is the homogeneous coordinate of \(p_{t}\). Assuming \(x_{t}\) does not move, we can obtain the projection \(p_{t}^{t+1}\) of \(x_{t}\) on frame \(I_{t+1}\):

\[p_{t}^{t+1}=(K_{t+1}T_{t+1}x_{t}), \]

where \(K_{t+1}\) and \(T_{t+1}\) are the intrinsics and extrinsics of camera \(C_{t+1}\), \(()\) projects the 3D coordinates to 2D image planes by dividing the last dimension (depth). Then the camera flow can be

Figure 2: **The overall architecture of MotionGS.** It can be viewed as two data streams: (1) The 2D data stream utilizes the optical flow decoupling module to obtain the motion flow as the 2D motion prior; (2) The 3D data stream involves the deformation and transformation of Gaussians to render the image for the next frame. During training, we alternately optimize 3DGS and camera poses through the camera pose refinement module.

defined as:

\[F_{t t+1}^{C}=p_{t}^{t+1}-p_{t}, \]

which indicates the flow caused solely by camera movement. As the point \(x_{t}\) moves over time, we denote its updated position as \(x_{t+1}\). This new point \(x_{t+1}\) is then projected onto frame \(I_{t+1}\) as \(p_{t+1}\). Thus, the optical flow \(F_{t t+1}\) between two adjacent frame is defined as \(p_{t+1}-p_{t}\). Finally, the motion flow \(F_{t t+1}^{M}\) is derived by subtracting the camera flow from the optical flow:

\[F_{t t+1}^{M}=F_{t t+1}-F_{t t+1}^{C}=p_{t+1}-p_{t}^{t+1}, \]

which also corresponds to the optical flow caused by object movement at a fixed viewpoint.

Gaussian flow.To establish a correspondence between Gaussian deformation and motion flow, we need to splat the Gaussian deformation onto the 2D image plane, which is not implemented in the original 3DGS framework. Inspired by recent work , we introduce the concept of Gaussian flow, denoted as \(F_{t t+1}^{G}\), to describe the 2D projection of Gaussian deformation, and implement it in the CUDA-based rasterizer. The core idea is to model the contribution of Gaussians to the optical flow by first transforming 3D Gaussians to canonical Gaussian space and then transforming them back to the state at the next time step. Please refer to Appendix A.1 for the specific derivation and modeling process of Gaussian flow. Gao _et al._ computes the deformation of 3D Gaussians from time \(t\) to \(t+1\) under the transformation of the camera viewpoint from \(C_{t}\) to \(C_{t+1}\), corresponding to optical flow. Different from it, our Gaussian flow is designed to match the motion flow, representing the deformation of 3D Gaussians from time \(t\) to \(t+1\) fixed under the camera viewpoint \(C_{t+1}\).

Flow loss.To effective constrain the Gaussian deformation, we use a \(_{1}\) loss between motion flow and Gaussian flow for simplicity:

\[_{}=\|sg(F_{t t+1}^{M})-F_{t t+1}^{G} \|, \]

where \(sg()\) means stop gradient. Note that we also stop the gradients of all variables at time \(t\) in the calculation of Gaussian flow for more efficient training.

Discussion.The benefits of decoupling the optical flow are evident. Since motion flow is only related to object motion, it can directly provide motion guidance. More importantly, in some previous works , an off-the-shelf segmentation network is often used to segment out the dynamic objects in the scene (such as humans, animals, cars, etc.). However, such masks are only used in their photometric loss to mask out dynamic regions. In contrast, our motion flow benefits from these dynamic masks more directly. By masking static objects with these masks, we can obtain a clear motion flow for supervising Gaussian deformation. If optical flow is used as motion guidance, this advantage will no longer exist because static objects can also contribute to the optical flow.

### Camera Pose Refinement Module

In monocular dynamic scenes, due to the complexity of motion and sparsity of observations, even widely used methods like COLMAP  cannot accurately estimate camera poses. Since the optimization of 3DGS requires precise camera poses as input, it often performs poorly in complex

Figure 3: **Flow calculation. Figure 4: **Pose refinement on iterative training.**

dynamic scenes. Existing 3DGS-based dynamic scene reconstruction methods rarely take this into account. Inspired by pose-free optimization methods for static scene reconstruction [72; 70], we design the camera pose refinement module. By alternately optimizing 3D Gaussian primitives and camera poses during training, we improve the rendering quality of 3DGS and its robustness in dynamic scenes.

Iterative training.Since the supervision of 3DGS primarily relies on photometric consistency loss, simultaneously optimizing camera parameters and 3DGS can be considered a chicken-and-egg problem. Therefore, similar to Bundle Adjustment, we adopt an alternating optimization strategy to train the model. Specifically, assuming \(G_{t}\) is the Gaussian at time \(t\), we first predict the deformation of the Gaussian using the deformation field \(\). We denote the deformed Gaussian as \(G_{t}^{t+1}\). Since the observation viewpoint changes from time \(t\) to \(t+1\), \(G_{t}^{t+1}\) needs to be transformed once again under the camera \(C_{t+1}\) to render frame \(I_{t+1}\). We denote the transformed Gaussian as \(G_{t+1}\). This transformation process actually corresponds to camera motion. To achieve differentiable optimization, we introduce a small residual \( T\) into the relative pose \(T\) from camera viewpoint \(C_{t}\) to \(C_{t+1}\), treating it as a learnable SE(3) transformation. With this small change, we enable gradients to backpropagate to the camera poses. During the optimization of camera poses, we freeze all attributes of 3D Gaussians to improve training stability and robustness. Then we update the camera poses initialized by COLMAP with the optimized relative camera poses, achieving global pose refinement.

Discussion.While several methods have been proposed for pose-free optimization in static scenes, dynamic scenes present greater challenges due to their inherently under-constrained nature. As a result, to ensure stable and robust optimization, our approach still leverages camera poses computed by COLMAP as an initialization step. This also necessitates the presence of sufficient static features in the scene. Fortunately, static features are commonly found in most real-world environments, particularly in background regions.

### Optimization

Thanks to the integration of optical flow rendering and camera pose gradient computation in our rasterization process, the overall training pipeline of our method is end-to-end differentiable. The overall training loss is given by:

\[=_{}+_{}, \]

where \(_{}\) is the photometric loss used in our baseline , \(\) is the weight of our flow loss.

## 5 Experiment

### Experimental Setup

To highlight the abilities of our method in handling complex dynamic scenes, we select two representative monocular dynamic scene datasets for evaluation: NeRF-DS  and HyperNeRF . Our implementation is mainly based on PyTorch. We use a simple Adam  optimizer to adjust the rotation increment and translation increment of the camera, and the learning rates of the two are set to 3e-3 and 1e-1, respectively. The entire training process requires 20,000 iterations. We set \(\) to 0.5 for NeRF-DS and 0.1 for HyperNeRF scene. The rest of the settings are consistent with the baseline method . All experiments are performed on a single Nvidia RTX 3090 GPU. For more implementation details, please refer to Appendix A.2.

### Results

Following previous methods, we use metrics PSNR, SSIM, and LPIPS for evaluation. For more visualizations, please refer to Appendix A.4.

Results on the NeRF-DS dataset.Table 1 shows the performance comparison results with the state-of-the-art methods on the NeRF-DS dataset. In dynamic monocular scenes, especially in those with rapid movements and high complexity, our method significantly outperforms the baseline method. For example (see Figures 5 and 12), in the plate scene, our method accurately renders the reflections and sharp edges of the moving plate while significantly reducing visual distortions such as floating 

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_EMPTY:9]

dataset, our method reconstructs more plausible results compared to the baseline approach. Unlike static scene datasets (e.g., Tanks & Temples) that use COLMAP to obtain the ground truth of camera poses, we assume that COLMAP may not provide accurate poses for dynamic scene datasets. In this setting, we lack ground truth for a direct quantitative evaluation for refined camera pose. Therefore, we provide visualizations of the pose refinement process in the Figure 8 as qualitative comparison.

## 6 Conclusion

In this paper, we propose MotionGS, a novel deformable 3D Gaussian Splitting framework for explicitly modeling and constraining object motion in dynamic scene reconstruction. The proposed framework includes two key modules: the optical flow decoupling module and the camera poserefinement module. The optical flow decoupling module decouples the motion flow related solely to object motion from the optical flow priors, providing explicit supervision for Gaussian deformation. The camera pose refinement module alternately optimizes 3DGS and camera poses, further enhancing the rendering quality and robustness of our model in dynamic scenes. Quantitative and qualitative results on the NeRF-DS and HyperNeRF datasets strongly demonstrate the contributions and effectiveness of our proposed method. More importantly, the proposed improvements are agnostic to specific network designs, which can be applied to similar deformation-based 3DGS methods. In future work, we aim to develop a 3DGS method that does not rely on camera pose inputs, thereby achieving robust high-quality reconstruction in dynamic scenes.