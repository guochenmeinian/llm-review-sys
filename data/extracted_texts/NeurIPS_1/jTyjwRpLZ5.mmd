# Stochastic Zeroth-Order Optimization under Strongly Convexity and Lipschitz Hessian: Minimax Sample Complexity

Stochastic Zeroth-Order Optimization under Strongly Convexity and Lipschitz Hessian: Minimax Sample Complexity

 Qian Yu

University of California, Santa Barbara

qianyu02@ucsb.edu

&Yining Wang

University of Texas at Dallas

yining.wang@utdallas.edu

&Baihe Huang

University of California, Berkeley

baihe_huang@berkeley.edu

&Qi Lei

New York University

q1518@nyu.edu

&Jason D. Lee

Princeton University

Jasondl@princeton.edu

###### Abstract

Optimization of convex functions under stochastic zeroth-order feedback has been a major and challenging question in online learning. In this work, we consider the problem of optimizing second-order smooth and strongly convex functions where the algorithm is only accessible to noisy evaluations of the objective function it queries. We provide the first tight characterization for the rate of the minimax simple regret by developing matching upper and lower bounds. We propose an algorithm that features a combination of a bootstrapping stage and a mirror-descent stage. Our main technical innovation consists of a sharp characterization for the spherical-sampling gradient estimator under higher-order smoothness conditions, which allows the algorithm to optimally balance the bias-variance tradeoff, and a new iterative method for the bootstrapping stage, which maintains the performance for unbounded Hessian.

## 1 Introduction

Stochastic optimization of an unknown function with access to only noisy function evaluations is a fundamental problem in operations research, optimization, simulation and bandit optimization research, commonly known as _zeroth-order optimization_(Chen et al., 2017), _derivative-free optimization_(Conn et al., 2009; Rios and Sahinidis, 2013) or _bandit optimization_(Bubeck et al., 2021). In this problem, an optimization algorithm interacts sequentially with an oracle and obtains noisy function evaluations at queried points every time. The algorithm produces an approximately optimal solution after \(T\) such evaluations, with its performance evaluated by the expected difference between the function values at the approximate optimal solution produced and the optimal solution. A more rigorous formulation of the problem is given in Sec. 2 below.

Existing works and results on stochastic zeroth-order optimization could be broadly categorized into two classes:

1. **Convex functions**. In the first thread of research, the unknown objective function to be optimized is assumed to be _concave_ (for maximization problems) or _convex_ (for minimization problems). For these problems, with minimal smoothness (e.g. objective function being Lipschitz continuous) it is possible to achieve a sample complexity of \((^{-2})\) for an expected optimization error or \(\), which is also a polynomial function of domain dimension \(d\); see for example the works of Agarwal et al. (2013); Lattimore and Gyorgy (2021); Bubeck et al. (2021);
2. **Smooth functions**. In the second thread of research, the unknown objective function to be optimized is assumed to be highly _smooth_, but not necessary concave/convex. Typical results assume the objective function is Holder smooth of order \(k 1\), meaning that the \((k-1)\)-th derivative of the objective function is Lipschitz continuous. Without additional conditions, the optimal sample complexity with such smoothness assumptions is \((^{-(2+d/k)})\)(Wang et al., 2019), which scales exponentially with the domain dimension \(d\).

In this paper, we study the optimal sample complexity of stochastic zeroth-order optimization when the objective function exhibits both (strong) convexity and a high degree of smoothness. As we have remarked in the first bullet point above, with convexity and Holder smoothness of order \(k=1\) (equivalent to the objective function being Lipschitz continuous), the works of Agarwal et al. (2013); Lattimore and Gyorgy (2021); Bubeck et al. (2021) established an \((^{-2})\) upper bound. With higher order of Holder smoothness, i.e., \(k=2\) (equivalent to the gradient of the objective being Lipschitz continuous), it is shown that simpler algorithms exist but the sample complexity remains \((^{-2})\)(Besbes et al., 2015; Agarwal et al., 2010; Hazan and Levy, 2014), which seemingly suggests the relatively smaller role smoothness plays in the presence of convexity. In this paper we show that with even higher order of Holder smoothness, i.e., \(k=3\) (specifically, the Hessian of the objective being Lipschitz continuous), the optimal sample complexity is improved to \(O(^{-1.5})\), which is significantly smaller than the sample complexity of the convex-without-smoothness setting \((^{-2})\), or the smooth-without-convexity setting \((^{-(2+d/3)})\). More importantly, when the Lipschitzness of Hessian is defined in Frobenius norm (see condition A1), we propose an algorithm that also achieves the optimal dimension dependency, which fully characterizes the optimal sample complexity.

Summary of technical contributions.We developed several important techniques in this paper to achieve the optimal sample complexity when the objective function is strongly convex and has Lipschitz Hessian. First, we show that when estimating the gradient under a stochastic environment, even with an unbounded action space, it could be beneficial to sample with non-isotropic distributions (as opposed to conventional standard Gaussian, or uniform distributions on hyperspheres). Second, we present a new approach to analyze the bias and variance of the hyperellipsoid-sampling-based gradient estimators, which enables obtaining sharp bounds with tight constants and strengthens the best-known results in the higher-order smoothness case. Third, we present a two-stage bootstrap-type framework for the algorithmic design, which extends the perturbative analysis in the final stage to the full regime. This extension relies on a non-trivial modification of Newton's method, and we proved its robustness under stochastic observation. We complete the characterization of the minimax regret by deriving a lower bound using the KL-divergence-based approach.

Additional related works on higher-order smoothness.Recent years have seen increasing attention on exploiting higher order smoothness in bandit optimization. Remarkably, it was shown that when the Holder smoothness condition holds simultaneously for both \(k=2\) and \(k=3\), the optimal sample complexity can be improved to \(O(^{-1.5})\)(Akhavan et al., 2020; Novitskii and Gasnikov, 2021). We list our results together with the most relevant work in Table 1. While this line of work also demonstrates the benefit of higher-order smoothness in improving the sample complexity, their setting is related but slightly different from what we considered in this work. (See reference therein: Bach and Perchet (2016); Akhavan et al. (2020); Novitskii and Gasnikov (2021)). On one hand, the prior work concentrates on projected gradient-descent-like algorithms, which require a Lipschitz gradient (i.e., the \(k=2\) requirement, and we do not). This additional requirement can not be removed by

 Lower Bound &  \\    & Bach and Perchet (2016) & Akhavan et al. (2020) \\ \((}M^{-1}})\) & \(O(dT^{-}M^{-})\) & \(O(d^{2}T^{-}M^{-1})\) \\   & Novitskii and Gasnikov (2021) & **Ours** \\  & \(O(d^{}T^{-}M^{-1})\) & \(}M^{-1})}\) \\ 

Table 1: The dependence of simple regret on \(T\) (number of function evaluations), \(d\) (dimension) and \(M\) (parameter describing strong convexity). Our results are highlighted in comparison to the prior works.

simply replacing the gradient steps with Newton's methods, which can lead to unbounded expectation in simple regret in the stochastic case.1 On the other hand, their results are based on the generalized Holder condition, which is different from our assumption that the Hessian is Lipschitz in Frobenius norm. Therefore we only emphasize the dependence of \(d,T\) and \(M\) in Table 1 and omit other parameters. We provide a detailed comparison on the implication of these results in Appendix A.

Our results are also related to a special case discussed in (Shamir, 2013), which shows that for _quadratic_ functions it is possible to achieve a sample complexity of \((^{-1})\). As quadratic functions are infinitely differentiable with bounded derivatives on orders, they are Holder smooth of any arbitrary order \(k\), which could be regarded as an extreme of the results established in this paper which only require \(k=3\).

Related works on gradient estimators.Gradient estimation serves as a key building block for stochastic zeroth-order optimization algorithms. For instance, a classical one-point estimator was proposed as early as in Flaxman et al. (2005); Blair (1985), where the gradient \( f()\) is estimated based on empirical measures of \(f(+r)\) for some fixed \(r\) and i.i.d. uniformly random \(\) on the unit hypersphere. This was later refined to be two-point estimators, and the sampling distribution of \(\) was generalized to isotropic distributions such as standard Gaussian (e.g., see Agarwal et al. (2010); Bach and Perchet (2016); Zhang et al. (2020)). A majority of prior work focused on the analysis for such estimators under the Lipschitz gradient assumption, where the best guaranteed bound for the bias is at the order of \((r)\), with a polynomial factor dependent on \(d\). The line of works by Bach and Perchet (2016); Akhavan et al. (2020); Novitskii and Gasnikov (2021) also adopted isotropic sampling, and it was shown that with higher-order smoothness of \(k=3\), this bound can be improved to \((r^{2})\). The improvement of sample complexity in our work is mainly due to the tight characterization of our gradient estimator, which covers the special case of isotropic sampling and provides a bound of \(}{2(d+2)}\) in the estimation bias. This strengthens or improves the bounds presented in prior works, and a detailed comparison can be found in Appendix A.

On the other hand, non-isotropic sampling was used as early as in Abernethy et al. (2008), then extended in Saha and Tewari (2011); Hazan and Levy (2014). Primarily, they were used to ensure that the sampling points are contained within a bounded action set. (Suggala et al., 2021) showed the necessity of non-isotropic sampling over quadratic loss function in the adversarial setting. In this work, we essentially demonstrated that non-isotropic sampling can be used to refine a preliminary algorithm by adding a mirror-descent-like final stage. More recently, non-isotropic sampling was also adopted in Lattimore and Gyorgy (2023) to optimize convex and global Lipschitz functions.

Notations.We follow the convention of machine learning theory where \(^{2}f()\) denotes the Hessian of \(f\) at point \(\), while the trace of Hessian is denoted by \((^{2}f())\). This should not be confused with the notation in classical field theory, where \(^{2}f()\) instead denotes the trace of the Hessian. We use \(\|\|_{2}\) to denote vector \(_{2}\) norms, and \(\|\|_{}\) to denote matrix Frobenius norms. We use \(I_{d}\) to denote the identity matrix, and \(S^{d-1}\) to denote the unit hypersphere centered at the origin, both for the \(d\)-dimensional Euclidean space \(^{d}\). We adopt the conventional notations (i.e., \(O\), \(\), \(o\), and \(\)) to describe regret bounds in the asymptotic sense with respect to the total number of samples (denoted by \(T\)).

## 2 Problem Formulation

We consider the stochastic optimization problem under the class of functions that are strongly convex and have Lipschitz Hessian. The goal in this setting is to design learning algorithms to achieve approximately the global minimum of an unknown objective function \(f:^{d}\).

A learning algorithm \(\) can interact with the function by adaptively sampling their value for \(T\) times, and receive noisy observations. At each time \(t[T]\), the algorithm selects \(_{t}^{d}\), and receives the following observation,

\[y_{t}=f(_{t})+w_{t}, \]

where \(\{w_{t}\}_{t=1}^{T}\) are independent random variables with zero mean and bounded variance. Formally, the algorithm can be described by a list of conditional distributions where each \(_{t}\) is selected based on all historical data \(\{_{},y_{}\}_{<t}\) and the corresponding distribution. Then for any \(t\), we assume that \([w_{t}|\{_{},y_{}\}_{<t},_{t}]=0\) and \([w_{t}|\{_{},y_{}\}_{<t},_{t}] 1\) for any \(t\).2 For simplicity, we also adopt a common assumption that the additive noises are subgaussian, particularly, \([|w_{t}|>s|\{_{},y_{}\}_{<t},_{t}] 2e^{-s^{2}}\) for all \(s>0\) and \(t[T]\). However, the subgaussian assumption can be removed by adopting more sophisticated mean-estimation methods (e.g., see Nemirovskii and Yuom (1983); Jerrum et al. (1986); Alon et al. (1999); Lee and Valiant (2022); Yu et al. (2023a)).

We assume that the objective function \(f\) is second-order differentiable. Furthermore, we impose the following conditions.

1. (Lipschitz Hessian). There exist a constant \((0,+)\) such that for all \(,^{}^{d}\), it holds that \(\|^{2}f()-^{2}f(^{})\|_{}\|^{}-\|_{2}\), where \(\|\|_{}\) denotes the Frobenius norm;
2. (Strong Convexity). There exists a constant \(M(0,+)\) such that for any \(^{d}\), the minimum eigenvalue of the Hessian \(^{2}f()\) is greater than \(M\).
3. (Bounded Distance from Initialization to Optimum Point). There exists a constant \(R(0,+)\) such that the infimum of \(f()\) within the hyperball \(\|\|_{2} R\) is identical to the infimum of \(f()\) over the entire \(^{d}\).

In the rest of this paper, we let \((,M,R)\) denote the set of all second-order differentiable functions that satisfy the above conditions, with corresponding constants given by \(,M\), and \(R\). We aim to find algorithms to achieve asymptotically the following minimax simple regret, which measures the expected difference of the objective function on \(x_{T}\) and the optimum.

\[(T;,M,R):=_{}_{f(,M,R)} [f(_{T})-f(^{*})],\]

where \(^{*}\) denotes the global minimum point of \(f\).

## 3 Main Results

**Theorem 3.1**.: _For any dimension \(d\) and constants \(,M,R\), the minimax simple regrets are upper bounded by \(_{T}(T;,M,R) T^{} C (}}{M}d)\), where \(C\) is a universal constant._

**Theorem 3.2**.: _For any fixed dimension \(d\) and constants \(,M,R\), the minimax simple regrets are lower bounded by \(_{T}(T;,M,R) T^{} C (}}{M}d)\) when the additive noises \(w_{1},...,w_{T}\) are standard Gaussian, where \(C\) is a universal constant._

## 4 Proof Ideas for Theorem 3.1

The proposed algorithm operates in two stages (see Algorithm 4). In the first stage, the algorithm uses a small fraction of samples to obtain a rough estimation of the global minimum point. We ensure that the estimation in the first stage is sufficiently accurate with high probability, so that in the following final stage, the objective function can be approximated by a quadratic function and the resulting approximation error can be bounded using tensor analysis.

### Key Techniques and The Final Stage

We first present the key steps of our algorithm, which relies on the subroutines presented in Algorithm 1-3, i.e., GradientEst, BootstrappingEst, and HessianEst. These subroutines estimate the (linearlytransformed) gradients and Hessian functions of \(f\) at any given point by sampling the values of \(f\) on hyperellipsoids. The key ingredient of our proof is the sharp characterizations for the biases and variances of the GradientEst estimator, stated in Theorem 4.1.

```
Input:\(,Z,n\)\(\)\(Z\) is a \(d d\) matrix, return \(}\) as an estimator of \(Z f()\) for\(k 1\) to \(n\)do  Let \(_{k}\) be a point sampled uniformly randomly from the standard hypersphere \(S^{d-1}\)  Let \(y_{+}\), \(y_{-}\) be samples of \(f\) at \(+Z_{k}\) and \(-Z_{k}\), respectively, let \(_{k}=(y_{+}-y_{-})_{k}\) endfor Return\(}=_{k=1}^{n}_{k}\)
```

**Algorithm 1** GradientEst

```
Input:\(,r,n\)\(\) Goal: estimate \( f()\) coordinate wise with \(O(nd)\) samples  Let \(_{1},...,_{d}\) be any orthonormal basis of \(^{d}\) for\(k 1\) to \(d\)do  Let \(y_{+,k}\), \(y_{-,k}\) each be the average of \(n\) samples of \(f\) at \(+r_{k}\) and \(-r_{k}\) respectively  Let \(m_{k}=(y_{+}-y_{-})/2r\)\(\) Estimate the \(k\)th entry endfor Return\(}=\{m_{k}\}_{k[d]}\)
```

**Algorithm 2** BootstrappingEst

```
Input:\(,r,n\)\(\) Goal: estimate \(^{2}f()\) coordinate wise with \(O(nd^{2})\) samples  Let \(_{1},...,_{d}\) be any orthonormal basis of \(^{d}\)  Let \(y\) be the average of \(n\) samples of \(f\) at \(\)  for\(k 1\) to \(d\)do  Let \(y_{+,k}\), \(y_{-,k}\) each be the average of \(n\) samples of \(f\) at \(+r_{k}\) and \(-r_{k}\) respectively  Let \(H_{kk}=(y_{+}+y_{-}-2y)/r^{2}\)\(\) Diagonal entries for\( k+1\) to \(d\)do  Let \(H_{k}=H_{ k}\) be the average of \(n\) samples of \((f(+r_{k}+r_{})+f(-r_{k}-r_{})-f( +r_{k}-r_{})-f(-r_{k}+r_{}))/4r^{2}\)\(\) Off-diagonal entries endfor endfor Let \(_{0}=\{H_{jk}\}_{(i,j)[d]^{2}}\), and \(\) be the matrix with same eigenvectors but with each eigenvalue \(\) replaced by \(\{,M\}\)\(\) Projecting to the set where \(-MI_{d}\) is positive semidefinite Return\(\)
```

**Algorithm 3** HessianEst

**Theorem 4.1**.: _For any fixed inputs \(\), \(Z\), \(n\), and any function \(f\) satisfying the Lipschitz Hessian condition with parameter \(\), the output \(}\) returned by the GradientEst subroutine satisfies the following properties_

\[||[}]-Z f()||_{2}^{3}}{2(d+2)}, \] \[([}])||Z  f()||_{2}^{2}+}{18n}(_{Z}^{3})^ {2}+}{2n}, \]

_where \(_{Z}\) is the largest singular value of \(Z\)._

**Remark 4.2**.: _Inequality (2) provides a sharp characterization for the bias of the gradient estimator, as it can be matched for any \(_{Z}\) and \(d\) with a cubic polynomial \(f\). Inequality (3) is sharp in the asymptotic regime when both \( f\) and \(_{Z}\) approaches zero._

We also provide rough estimates on the high-probability bounds for the BootstrappingEst and the HessianEst functions. Specifically, we show that their errors have sub-Gaussain tails in distribution, as stated in the following theorem.

**Theorem 4.3**.: _For any fixed inputs \(\), \(r\), \(n\), any function \(f\) satisfying the Lipschitz Hessian condition with parameter \(\), and any variable \(K>0\), the outputs \(}\) and \(\) returned by the BootstrappingEst and the HessianEst subroutine satisfy the following conditions._

\[[||}- f()| |_{2} K] 2(-}{r^{4}}{4}+ }}), \] \[[||-^{2}f()| |_{} K] 2(-}{2d^{2}^{2}r^{2 }+}{nr^{4}}}). \]

We postpone the proof of the above theorems to Section 4.2 and Appendix C and proceed to describe how these results are used in the algorithm.

For brevity, let \(}}{M}dT^{-}\) be the minimax regret we aim to achieve, and let \(_{}\) denote the estimator \(\) stored at the end of the first stage. The role of the final stage is to ensure that if \(f(_{})-f(^{*})\) is sufficiently small with high probability, the final result of the proposed algorithm achieves the stated simple regret guarantees. Formally, we require the following achievability result from the Bootstrapping stage.

**Theorem 4.4**.: _For any fixed \(,M\) and \(R\), the result returned by the first stage of Algorithm 4 satisfies_

\[_{T}_{f(,M,R)}[( f(_{})-f(^{*}))^{}]/=0. \]

Note that the above condition implies that \(f(_{})-f(^{*})\) concentrates below \(o(^{})\), which is weaker than the \(O()\) rates stated in our main theorems.3 The bottleneck of the overall algorithm is on the final stage, and one can achieve equation (6) using any suboptimal algorithm with an expected simple regret of \(o(T^{-})\). For example, one can run the suboptimal algorithm twice, estimate their achieved function values by averaging over \(o(T)\) samples, and then choose the outcome with the smaller estimated function value as \(_{}\). In the rest of this section, we prove Theorem 3.1 assuming the correctness of the above theorem. A self-contained proof for Theorem 4.4 is provided in Appendix E.

Before proceeding with the proof, we provide a high-level description of the algorithm in the final stage. At the beginning, we perform a Hessian estimation near \(_{}\) using the HessianEst subroutine with \(O(T)\) samples. From Theorem 4.3, our choice of parameters results in an expected estimation error of \(o(1)\) for sufficiently large \(T\).

The algorithm proceeds to find a real matrix \(Z_{H}\), which essentially serves as a linear transformation on the action domain such that the Hessian of the transformed function is approximately the identity matrix. Note that the projection step in the HessianEst function ensures the eigenvalues of the estimator are no less than \(M\). There is always a valid solution of \(Z_{H}\).

Then, we estimate the gradient at \(_{}\) using the GradientEst subroutine, which samples on a hyperellipsoid with a shape characterized by \(Z_{H}\). We chose the hyperellipsoid sampling in the final stage due to its superior performance in the small-gradient regime compared to coordinate-wise sampling. In contrast, the coordinate-wise estimator is used in the bootstrapping stage to eliminate the dependency of the local gradient on its bias-variance tradeoff, which is beneficial for the non-asymptotic analysis. Particularly, we scale the hyperellipsoid with a carefully designed factor (see the definition of variable \(r_{}\)) to minimize the estimation error. Then, the remaining steps can be interpreted as a modified Newton step, which essentially approximates the global minimum point with a quadratic approximation.

The analysis in our proof relies on the following proposition, which is proved in Appendix D.

**Proposition 4.5**.: _For any given point \(_{}\) and any function \(f\) that satisfies strong convexity and Lipschitz Hessian, let \(}_{}-(^{2}f(_{})) ^{-1} f(_{})\) and \(()\) denote the quadratic approximation\((-})^{}^{2}f(_{})( -})\), we have the following inequality for all \(\) with \(||-_{}||_{2}\)._

\[f()-f^{*} 2()+_{})-f^{*})^{ }}{M^{}}. \]

_Furthermore, if \(\) is generated by the final stage of Algorithm 4 with any parameter values that satisfy \(n_{} d^{3}\), \(n_{}d^{6}}{M^{6}}\) and the first-stage output is set to \(_{}\), then_

\[[()_{}](^{}}{M^{2}n_{}^{}}+}})(f(_{})-f(^{*}))+} }(f(_{})-f(^{*}))^{}+} }{Mn_{}^{}}. \]

Now, we use Proposition 4.5 to prove the achievability result.

Proof of Theorem 3.1 given Theorem 4.4.: First, recall our construction ensures that \(||_{T}-_{}||_{2}\). Inequality (7) can always be applied and we have

\[(T;,M,R)_{f(,M,R)}[2 ()+_{})-f^{*})^{}}{M^{ }}].\]

Then, when \(T\) is sufficiently large, the conditions of (8) holds and we have

\[(T;,M,R) _{f(,M,R)}[(^{}}{M^{2}n_{}^{}}+ }})(f(_{})-f(^{*}))+_{})-f^{*})^{}}{M^{}}]\] \[+}}{Mn_{}^{}}.\]

Note that inequality (6) implies that \([f(_{})-f(^{*})]=o(^{})=o(T^{-})\) and we have that \(n_{}^{-}+n_{}^{-1}=o(T^{-})\). The RHS of the above inequality is dominated by the last term. Hence,

\[_{T}(T;,M,R) T^{}_{T }}T^{}}{Mn_{}^{}}=O(}}{M}d).\]To complete the proof, we show that the proposed algorithm samples the function values of \(f\) at most \(T-1\) times. In the first stage, both BootstrappingEst and HessianEst are executed once per loop, with BootstrappingEst requiring at most \(2d{n_{}}}{5}\) samples and HessianEst requiring at most \(2d^{2}{n_{}}}{5}\) samples each time. Therefore, the total number of samples used in the first stage is bounded by \( T(}{5}+}{5})\). In the final stage, we make one call to both HessianEst and GradientEst, which together require \({n_{}}(2d^{2})+2{n_{}}\) samples. Thus, the overall number of samples is bounded by \(\), which ensures that it is no greater than \(T-1\). 

### Proof of Theorem 4.1

To prove inequality (2), we investigate the following function

\[(r;)_{(S^{d-1})}[ (f(+r)-f(-r))],\]

where \((S^{d-1})\) denotes the uniform distribution on \(S^{d-1}\). Recall that in our algorithm we have \([}]=r(r;)\) if \(Z=rI_{d}\) for some \(r(0,+)\), and by differentiability we have \( f()=_{z 0^{+}}(z;)\). Under this condition, we can bound \(||[}]-r f()||_{2}\) by integration, i.e.,

\[||[}]-r f()||_{2}=r||(r;)-_{z 0^{+}}(z;)||_{2}.r_{0^{+}}^{r} ||(z;)|||_{2}dz. \]

Note that \((z;)\) can be written into the following equivalent form.

\[(z;)=}(f(+z)-f(-z ))}{_{S^{d-1}}||||_{2}},\]

where the integration is with respect to \(\) over the surface \(S^{d-1}\), and \(\) is the vector surface element, i.e., with the magnitude being the infinitesimally small surface area and the direction perpendicular to the surface (pointing outward). The differential of \((z;)\) over \(z\) can be written as

\[(z;) =}((f(+z)-f(-z)))}{_{S^{d-1}}|| ||_{2}}\] \[=}-}(f(+z)-f( -z))}{_{S^{d-1}}||||_{2}}\] \[+}( f( +z)+ f(-z))}{_{S^{d-1}}|| {d}||_{2}}.\]

The gist of this proof is to note that for any \( S\) we have \(\) and \(\) are parallel (i.e., \(\) is parallel to the normal vector of the hypersphere at the same point), so the second term in the integral above on the numerator can be written as

\[_{S^{d-1}}( f(+z)+ f(-z {u})).\]

Hence, by divergence theorem, we have

\[(z;) =}||||_{2}} _{B^{d}}_{}-}I_{d}(f(+z )-f(-z))\] \[+( f(+z)+ f(-z) )\] \[=}(^{2}f( +z)-^{2}f(-z))}{_{S^{d-1}} ||||_{2}}, \]

where \(B^{d}\) denotes the standard hyperball.

Now consider any unit vector \(\). Let \(_{}\) denote the reflection of \(\) with respect to the hyperplane orthogonal to \(\), i.e., \(_{}-2()\). Because the hyperball \(B\) is invariant under the reflection \(_{}\), equation (10) can also be written as

\[(z;)=}_{} (^{2}f(+z_{})-^{2}f(-z _{}))}{_{S^{d-1}}||||_ {2}}. \]

Hence, by averaging equation (10) and (11), we have

\[(z;) =}(^{2} f(+z)-^{2}f(-z))}{_{S^{d-1}}|| ||_{2}}\] \[+}_{} (^{2}f(+z_{})-^{2}f(-z _{}))}{_{S^{d-1}}||||_ {2}}\] \[=}( ^{2}f(+z)-^{2}f(+z_{})) }{_{S^{d-1}}||||_{2}}\] \[+}- (^{2}f(-z)-^{2}f(-z_{ {e}}))}{_{S^{d-1}}||||_{2}}. \]

By the Lipschitz Hessian condition and Cauchy's inequality, the difference between the differential terms above can be bounded as follows.

\[|(^{2}f( z)- ^{2}f( z_{}))| ||^{2}f( z)-^{2}f(  z_{})||_{}\] \[||z-z_{}||_{2}=2z| |. \]

Consequently,

\[|(z;)| _{B^{d}}()^{2}}{_{S^{ d-1}}||||_{2}}=}{d+2}.\]

Note that \(\) can be any unit vector. We have essentially bounded the \(_{2}\) norm of \((z;)\), i.e.,

\[||(z;)||_{2}}{d+2}.\]

As mentioned earlier, when \(Z=rI_{d}\) inequality (2) is obtained by applying this gradient-norm bound to inequality (9).

For general input matrix \(Z\), we can view GradientEst as a subroutine that operates on the same function \(f\) but with a linear transformation applied to the input domain. Formally, let \(f^{}() f(+}(-))\). We have that \(f^{}\) satisfies the Lipschitz Hessian condition with parameter \(\) as well. Therefore, inequality (2) can be obtained following the same analysis by replacing \(f\) with \(f^{}\) and \(Z\) with \(_{Z}I_{d}\).

Now we present the proof for inequality (3). Formally, let \(w_{+}\), \(w_{-}\) be two independent samples of additive noises. Then the trace of covariance matrix of \(}\) can upper bounded using the second moments of single measurements.

\[([}]) _{(S^{d-1}),w_{+},w_ {-}}()^{2}(f(+Z)-f(-Z )+w_{-}-w_{-})^{2}\] \[=}{4n}_{(S^{d-1})} [(f(+Z)-f(-Z))^{2}+2]. \]

The identity above uses the fact that additive noises are unbiased and have bounded variances.

Note that from the Lipschitz Hessian condition, we have that

\[|f( Z)-f_{2}( Z)|||Z||_{ 2}^{3}_{Z}^{3},\]where \(f_{2}\) is the Taylor polynomial of \(f\) expanded at \(\) up to the quadratic terms. Consequently, inequality (14) implies

\[([}]) }{4n}[(|f_{2}(+Z)-f _{2}(-Z)|+_{Z}^{3})^{2}+2]\] \[=}{4n}[(|2Z f( {x})|+_{Z}^{3})^{2}+2]\] \[}{4n}[2|2Z f( )|^{2}+2(_{Z}^{3})^{2}+2]\] \[=||Z f()||_{2}^{2}+}{18n}( _{Z}^{3})^{2}+}{2n}\]

where the expectations are taken of \((S^{d-1})\), and the last equality is due to the well-known fact that \([^{}]=I_{d}\).

## 5 Conclusion and Future Work

In this work, we achieve the first minimax simple regret for bandit optimization of second-order smooth and strongly convex functions. We derived the matching upper and lower bounds and proposed an algorithm that integrates a bootstrapping stage with a mirror-descent stage. Our key technical innovations include a sharp characterization of the spherical-sampling gradient estimator under higher-order smoothness conditions and a novel iterative method for the bootstrapping stage that remains effective with unbounded Hessians.

While these advancements settle the fundamental problem of optimizing second-order smooth and strongly convex functions with zeroth-order feedback, the techniques and insights presented in this paper also pave the way for further research in this domain. One interesting follow-up direction is to generalize our analysis to the online setting for the average regret metric. Additionally, investigating the fundamental tradeoff between simple regret and average regret could yield valuable insights for task-specific algorithmic designs.