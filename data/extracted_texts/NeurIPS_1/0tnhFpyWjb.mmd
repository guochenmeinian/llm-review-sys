# Two-Stage Predict+Optimize for Mixed Integer Linear Programs with Unknown Parameters in Constraints

Xinyi Hu\({}^{1}\), Jasper C.H. Lee\({}^{2}\), Jimmy H.M. Lee\({}^{1}\)

\({}^{1}\)Department of Computer Science and Engineering

The Chinese University of Hong Kong, Shatin, N.T., Hong Kong

\({}^{2}\)Department of Computer Sciences & Institute for Foundations of Data Science

University of Wisconsin-Madison, WI, USA

{xyhu,jlee}@cse.cuhk.edu.hk, jasper.lee@wisc.edu

###### Abstract

Consider the setting of constrained optimization, with some parameters unknown at solving time and requiring prediction from relevant features. Predict+Optimize is a recent framework for end-to-end training supervised learning models for such predictions, incorporating information about the optimization problem in the training process in order to yield better predictions in terms of the quality of the predicted solution under the true parameters. Almost all prior works have focused on the special case where the unknowns appear only in the optimization objective and not the constraints. Hu et al. proposed the first adaptation of Predict+Optimize to handle unknowns appearing in constraints, but the framework has somewhat ad-hoc elements, and they provided a training algorithm only for covering and packing linear programs. In this work, we give a new _simpler_ and _more powerful_ framework called _Two-Stage Predict+Optimize_, which we believe should be the canonical framework for the Predict+Optimize setting. We also give a training algorithm usable for all mixed integer linear programs, vastly generalizing the applicability of the framework. Experimental results demonstrate the superior prediction performance of our training framework over all classical and state-of-the-art methods.

## 1 Introduction

Optimization problems are prevalent in modern society, and yet the problem parameters are not always available at the time of solving. For example, consider the real-world application scenario of stocking a store: as store managers, we need to place monthly orders for products to stock in the store. We want to stock products that sell fast and yield high profits, as much of them as possible, subject to the hard constraint of limited storage space. However, orders need to be placed two weeks in advance of the monthly delivery, and the customer demand next month cannot be known exactly at the time of order placement. In this paper, we consider the supervised learning setting, where the unknown parameters can be predicted from relevant features, and there are sufficient historical (features, parameters) pairs as training data for a prediction model. The goal, then, is to learn a prediction model from the training data such that, if we plug in the estimated parameters into the optimization problem and solve for an _estimated solution_, the estimated solution remains a good solution even after the true parameters are revealed.

The classic approach to the problem would be to train a simple regression model, based on standard losses such as (regularized) \(_{2}\) loss, to predict parameters from the features. It is shown, however, that having a small prediction error in the parameter space does not necessarily mean that the estimated solution performs well under the true parameters. The recent framework of Predict+Optimize, byElmachtoub and Grigas , instead proposes the more effective _regret_ loss for training, which compares the solution qualities of the true optimal solution and the estimated solution under the true parameters. Subsequent works [6; 8; 10; 13; 17; 19; 27] have since appeared in the literature, applying the framework to more and wider classes of optimization problems as well as focusing on speed-vs-prediction accuracy tradeoffs.

However, all these prior works focus only on the case where the unknown parameters appear in the optimization objective, and not in the constraints. The technical challenge for the generalization is immediate: if there were unknown parameters in the constraints, the estimated solution might not even be feasible under the true parameters revealed afterwards! Thus, in order to tackle the Predict+Optimize setting with unknowns in constraints, the recent work of Hu et al.  presents the first such adaptation on the framework: they view the estimated solution as representing a _soft commitment_. Once the true parameters are revealed, corrective action can be taken to ensure feasibility, potentially at a penalty corresponding to the real-life cost of (partially) reneging on a soft commitment. Their framework captures application scenarios whenever such correction is possible, and requires the practitioner to specify both the correction mechanism and the penalty function. These data can be determined and derived from the specific application scenario. As an example, in the product-stocking problem, an additional unknown parameter is the storage space, because it depends on how the current products in the store sell before the new order arrives. We need to place orders two weeks ahead based on predicted storage space. The night before the order arrives, we know the precise available space, meaning that the unknown parameter is revealed. A possible correction mechanism then is to throw away excess products that the store cannot keep, while incurring the penalty that is the retail price of the products, as well as disposal fees.

While the Hu et al.  framework does capture many application scenarios, there are important shortcomings. In their framework, they require the practitioner to specify a correction function that amends an infeasible solution into a feasible solution. However, the derivation of a correction function can be rather ad-hoc in nature. In particular, given an infeasible estimated solution, there may be many ways to transform the solution into a feasible one, and yet their framework requires the practitioner to pick one particular way. This leads to the second downside: it is difficult to give a _general_ algorithmic framework that applies to a wide variety of optimization problems. Hu et al. had to restrict their attention only to packing and covering linear programs, for which they could propose a generic correction function. In this work, we aim to _vastly generalize_ the kinds of optimization problems that Predict+Optimize can tackle under uncertainty in the constraints. In addition, the approach of Hu et al. fails to handle the interesting situation in which post-hoc correction is still desirable when the estimated solution is feasible but not good under the true parameters.

Our contributions are three-fold:

\(\) To mitigate the shortcomings of the prior work, we propose and advocate a new framework, which we call _Two-Stage Predict+Optimize1_, that is both _conceptually simpler_ and _more expressive_ in terms of the class of optimization problems it can tackle. The key idea for the new framework is that the correction function is unnecessary. All that is required is a penalty function that captures the cost of modifying one solution to another. A penalty function is sufficient for defining a correction process: we formulate the correction process itself as a "Stage 2" optimization problem, taking the originally estimated solution as well as the penalty function into account.

\(\) Under this framework, we further propose a general end-to-end training algorithm that applies not only to packing and covering linear programs, but also to all mixed integer linear programs (MILPs). We adapt the approach of Mandi and Guns  to give a gradient method for training neural networks to predict parameters from features.

\(\) We apply the proposed method to three benchmarks to demonstrate the superior empirical performance over classical and state-of-the-art training methods.

## 2 Background

In this section, we give basic definitions for optimization problems and the Predict+Optimize setting , and describe the state-of-the-art framework  for Predict+Optimize with unknown parametersin constraints. The theory is stated in terms of minimization but applies of course also to maximization, upon appropriate negation. Without loss of generality, an _optimization problem_ (OP) \(P\) can be defined as finding:

\[x^{*}=*{arg\,min}_{x}obj(x)C(x)\]

where \(x^{d}\) is a vector of decision variables, \(obj:^{d}\) is a function mapping \(x\) to a real objective value that is to be minimized, and \(C\) is a set of constraints that must be satisfied over \(x\). We call \(x^{*}\) an _optimal solution_ and \(obj(x^{*})\) the _optimal value_. A _parameterized optimization problem (Para-OP)_\(P()\) is an extension of an OP \(P\):

\[x^{*}()=*{arg\,min}_{x}obj(x,)C(x,)\]

where \(^{t}\) is a vector of parameters. The objective \(obj(x,)\) and constraints \(C(x,)\) can both depend on \(\). When the parameters are known, a Para-OP is just an OP.

In the _Predict+Optimize_ setting , the true parameters \(^{t}\) for a Para-OP are not known at solving time, and _estimated parameters_\(\) are used instead. Suppose each parameter is estimated by \(m\) features. The estimation will rely on a machine learning model trained over \(n\) observations of a training data set \(\{(A^{1},^{1}),,(A^{n},^{n})\}\) where \(A^{i}^{t m}\) is a _feature matrix_ for \(^{i}\), in order to yield a _prediction function_\(f:^{t m}^{t}\) predicting parameters \(=f(A)\).

Solving the Para-OP using the estimated parameters, we obtain an _estimated solution_\(x^{*}()\). When the unknown parameters appear in constraints, one major challenge is that the feasible region is only approximated at solving time, and hence the estimated solution may be infeasible under the true parameters. Fortunately, in certain applications, the estimated solution is not a hard commitment, but only represents a soft commitment that can be modified once the true parameters are revealed. Hu et al.  propose a Predict+Optimize framework for such applications. The framework is as follows: i) the unknown parameters are estimated as \(\), and an estimated solution \(x^{*}()\) is solved using the estimated parameters, ii) the true parameters \(\) are revealed, and if \(x^{*}()\) is infeasible under \(\), it is _amended_ into a _corrected solution_\(x^{*}_{}(,)\) while potentially incurring some _penalty_, and finally iii) the solution \(x^{*}_{}(,)\) is evaluated according to the sum of both the objective, under the true parameters \(\), and the incurred penalty from correction.

More formally, a _correction function_ takes an estimated solution \(x^{*}()\) and true parameters \(\) and returns a _corrected solution_\(x^{*}_{corr}(,)\) that is feasible under \(\). A _penalty function_\(Pen(x^{*}() x^{*}_{corr}(,))\) takes an estimated solution \(x^{*}()\) and the corrected solution \(x^{*}_{corr}(,)\) and returns a non-negative penalty. Both the correction function and the penalty function should be chosen according to the precise application scenario at hand. The final corrected solution \(x^{*}_{corr}(,)\) is evaluated using the _post-hoc regret_, which is defined with respect to the corrected solution \(x^{*}_{corr}(,)\) and the penalty function \(Pen(x^{*}() x^{*}_{corr}(,))\). The post-hoc regret is the sum of two terms: (a) the difference in objective between the _true optimal solution_\(x^{*}()\) and the corrected solution \(x^{*}_{corr}(,)\) under the true parameters \(\), and (b) the penalty that the correction process incurs. Mathematically, the post-hoc regret function \(PReg(,):\ ^{t}^{t}_{ 0}\) (for minimization problems) is:

\[PReg(,)=\ obj(x^{*}_{corr}(,),)-obj( x^{*}(),)\ +\ Pen(x^{*}() x^{*}_{corr}(,)) \]

where \(obj(x^{*}_{corr}(,),)\) is the _corrected optimal value_ and \(obj(x^{*}(),)\) is the _true optimal value_.

Given the post-hoc regret as a loss function, the empirical risk minimization principle dictates that we choose the prediction function to be the function \(f\) from the set of models \(\) attaining the smallest average post-hoc regret over the training data:

\[f^{*}=*{arg\,min}_{f}_{i=1}^{n}PReg (f(A^{i}),^{i}) \]

## 3 Two-stage Predict+Optimize Framework

While the prior work by Hu et al.  is the first Predict+Optimize framework for unknowns in constraints, and is indeed applicable to a good range of applications, it has several shortcomings.

First, the framework requires mathematically formalizing both a penalty function and a correction function from the application scenario, and essentially imposes differentiability assumptions on the correction function for the framework to be usable. The ad-hoc nature of writing down a correction function limits the practical applicability of the framework. Second, as a result of needing a single (differentiable) correction function, Hu et al.  needed to restrict their attention to only packing and covering linear programs, in order to derive a general correction function that is applicable to all the instances. This also significantly limits the immediate applicability of their framework. Third, their framework only corrects an estimated solution when it is infeasible under the true parameters. Yet, there are applications where corrections are possible even when the estimated solution were feasible, but just not very good under the true parameters.

In this paper, we advocate using a simpler yet more powerful framework, which we call _Two-Stage Predict+Optimize_, addressing all of the above shortcomings. The simplified perspective will allow us to discuss more easily how to handle the entire class of mixed integer linear programs (MILPs) instead of being restricted to just packing and covering linear programs. Since MILPs include all optimization problems in \(\) (under a reasonable definition of \(\) for optimization problems), our framework is significantly more applicable in practice. We will describe the Two-Stage Predict+Optimize framework below, and discuss its application to MILPs in the next section.

Our framework is simple: we forgo the idea of a correction function and treat correction itself as an optimization problem, based on the penalty function, the estimated solution and the revealed true parameters. Recall the Hu et al. view of Predict+Optimize under uncertainties in constraints: the estimated solution is a form of soft commitment, which can be modified at a cost once the true parameters are revealed. The penalty function describes the cost of changing from an estimated solution to a final solution. The main observation is that, given an estimated solution and the revealed parameters, we should in fact solve a _new_ optimization problem, formed by applying the true parameters to the original optimization, and adding the penalty function to the objective. The final solution from this new optimization thus takes the penalty of correction into account. This approach yields three immediate advantages. First, the practitioner no longer needs to specify a correction function, thus reducing the ad-hoc nature of the framework. Second, even feasible solutions are allowed to be modified after the true parameters are revealed if the penalty of doing so is not infinity. Third, conditioned on the same penalty function, the solution quality from our two-stage optimization approach is always at least as good as that from using any correction function. The last advantage is presented as Proposition A.1.

Now we formally define the Two-Stage Predict+Optimize framework.

**I.** In Stage 1, the unknown parameters are estimated as \(\) from features. The practitioner then solves the _Stage 1_ optimization, which is the Para-OP using the estimated parameters, to obtain the _Stage 1 solution_\(x_{1}^{*}\). The Stage 1 solution should be interpreted as some form of soft commitment, that we get to modify in Stage 2 at extra cost/penalty. Assuming the notation of the Para-OP in Section 2, the Stage 1 OP can be formulated as:

\[x_{1}^{*}\,=*{arg\,min}_{x}\,\,obj(x,)C(x,)\]

**II.** At the beginning of Stage 2, the true parameters \(\) are revealed. The Stage 2 optimization problem augments the original Stage 1 problem by adding a penalty term \(Pen(x_{1}^{*} x_{2}^{*},)\) to the objective, which accounts for the penalty (modelled from the application scenario) for changing from the softly-committed Stage 1 solution \(x_{1}^{*}\) to the new _Stage 2_ and _final_ solution \(x_{2}^{*}\). The Stage 2 OP can then be formulated as:

\[x_{2}^{*}\,=*{arg\,min}_{x}\,\,obj(x,)+Pen(x_{1}^{*} x, )C(x,)\]

Solving the Stage 2 problem yields the final Stage 2 "corrected" solution \(x_{2}^{*}\).

**III.** The Stage 2 solution \(x_{2}^{*}\) is evaluated according to the analogous post-hoc regret, as follows:

\[PReg(,)=\,\,obj(x_{2}^{*},)+Pen(x_{1}^{*} x_{2}^{*}, )-obj(x^{*}(),)\]

where again, \(x^{*}()\) is an optimal solution of the Para-OP under the true parameters \(\). Note that the post-hoc regret depends on _all_ of a) the predicted parameters, b) the induced Stage 1 solution, c) the true parameters and d) the final Stage 2 solution.

To see this new framework applies in practice, the following example expands on the product-stocking problem in the introduction.

**Example 1**.: _Consider the product-stocking problem again, where regular orders have to be placed two weeks ahead of monthly deliveries. Since the available space at the time of delivery is unknown when we place the regular orders, depending on the sales over the next two weeks, we need to make a prediction on the available space to make a corresponding order. We learn the predictor using historical sales records from features such as time-of-year and price. Then, we use the predicted available space to optimize for the regular order we place. This is the Stage 1 solution._

_The night before the order arrives, the unknown constraint parameter, i.e. the precise available space, is revealed. We can then check if we have over-ordered or under-ordered. In the case of over-ordering, we would have to call and ask the wholesale company to drop some items from the order. The company would perhaps allow taking the items off the final bill, but naturally they have a surcharge for last-minute changes. Similarly, if we under-ordered, we might request the wholesale company to send us more products, again naturally with a surcharge for last-minute ordering. The updated order is the Stage 2 decision. The incurred wholesaler surcharges induce the penalty function._

A reader who is familiar with the literature on two-stage optimization problems may note that the above framework is phrased slightly differently from some other two-stage problem formulations. In particular, some two-stage frameworks phrase Stage 1 solutions as _hard_ commitments, and include _recourse variables_ in both stages of optimization to model what changes are made in Stage 2. We show in Appendix A.1 how our framework can capture this other perspective, and in general discuss how problem modelling can be done in our new framework.

The reader may also wonder: what about application scenarios where the (Stage 1) estimated solution is a hard commitment, and there is absolutely no correction/recourse available? In Appendix A.2, we discuss how our framework is _still_ useful and applicable for learning in these situations.

We also give a more detailed comparison, in Appendix A.3, between our new Two-Stage Predict+Optimize framework and the prior Hu et al. framework. Technically, if we _ignored_ differentiability issues, the two frameworks are mathematically equivalent in expressiveness. However, we stress that our new framework is both _conceptually simpler_ and _easier to apply_ to a _far wider_ class of optimization problems. We show concretely in the next section how to end-to-end train a neural network for this framework for all MILPs, vastly generalizing the method of Hu et al. which is restricted to packing and covering (non-integer) linear programs. In addition, Appendix A.3 also states and proves Proposition A.1, that if we fix an optimization problem, a prediction model and a penalty function, then the solution quality from our two-stage approach is always at least as good as using the correction function approach.

## 4 Two-Stage Predict+Optimize on MILPs

In this section, we describe how to give an end-to-end training method for neural networks to predict unknown parameters from features, under the Two-Stage Predict+Optimize framework. The following algorithmic method is applicable whenever _both_ stages of optimization are expressible as MILPs. Due to the page limit, the discussion in this section is high-level and brief, with all the calculation details deferred to Appendix B.

The standard way to train a neural network is to use a gradient-based method. In the Two-Stage Predict+Optimize framework, we use the post-hoc regret \(PReg\) as the loss function. Therefore, for each edge weight \(w_{e}\) in the neural network, we need to compute the derivative \(PReg}{w_{e}}\). Using the law of total derivative, we get

\[PReg(,)}{w_{e}}=.,)}{ x_{2}^{*}}|_{x_{1}^{*}} .^{*}}{ x_{1}^{*}}^{*}}{ }}{ w_{e}}+.,)}{ x_{1}^{*}}|_{x_{2}^{*}} ^{*}}{}}{  w_{e}} \]

As such, we wish to calculate each term on the right hand side.

The easiest term to handle is \(}{ w_{e}}\), since \(\) is the neural network output, and so the derivatives can be directly calculated by standard backpropagation . As for the terms \(.,)}{ x_{2}^{*}}|_{x_ {1}^{*}}\) and \(.,)}{ x_{1}^{*}}|_{x_ {2}^{*}}\), they are easily calculable whenever both the optimization objective and penalty function are smooth, and in fact linear as in the case of MILPs. What remains are the terms \(^{*}}{ x_{1}^{*}}\) and\(^{*}}{}\). The challenge is that \(x_{2}^{*}\) is the solution of a MILP optimization (Stage 2) that uses \(x_{1}^{*}\) as its parameters, i.e., differentiate through a MILP. Similarly, \(x_{1}^{*}\) depends on \(\) through a MILP (Stage 1). Since MILP optima may not change under minor parameter perturbations, the gradients can be either \(0\) or non-existent, which are uninformative. We thus need to compute some approximation in order to get useful training signals.

Our approach, inspired by the work of Mandi and Guns , is to define a new surrogate loss function \(\) that is differentiable and produces informative gradients. Prior works related to learning unknowns in constraints  give ways of differentiating through LPs or LPs with regularizations. These works can be used in place of the proposed approach. However, experiments in Appendix E demonstrate that the proposed approach performs at least as well in post-hoc regret performance as the others, while being faster. We show the construction of the proposed approach below, and note that it does not have a simple closed form. Nonetheless, we can compute its gradients.

The rest of the section assumes that both stages of optimization are expressible as a MILP in the following standard form:

\[x^{*}=*{arg\,min}_{x}c^{}x\ Ax=b,Gx h,x 0,x_{S} \]

with decision variables \(x^{d}\) and problem parameters \(c^{d}\), \(A^{p d}\), \(b^{p}\), \(G^{q d}\), \(h^{q}\). The subset of indices \(S\) denotes the set of variables that are under integrality constraints. Since the unknown parameters may appear in any combination of \(c,A,b,G\) and \(h\) in the Stage 1 optimization for \(x_{1}^{*}\), the surrogate loss function construction needs computable and informative gradients for all of \(}{ c}\), \(}{ A}\), \(}{ b}\), \(}{ G}\) and \(}{ h}\).

We follow the interior-point based approach of Mandi and Guns , used also by Hu et al. . Consider the following convex relaxation of (4), for a _fixed_ value of \( 0\):

\[x^{*}=*{arg\,min}_{x,s}c^{}x-_{i=1}^{d}(x_{i})- _{i=1}^{q}(s_{i})Ax=b,Gx-s=h \]

This is a relaxation of (4) by i) dropping all integrality constraints, ii) introducing slack variables \(s 0\) to turn \(Gx h\) into \(Gx-s=h\) and iii) replacing both the \(x 0\) and \(s 0\) constraints with the logarithm barrier terms in the objective, with multiplier \( 0\). The observation is that the gradients \(\), \(\), \(\), \(\) and \(\) for (5) are all well-defined, computable and informative for a _fixed_ value of \( 0\): Slater's condition holds for (5), and so the KKT conditions must be satisfied at the optimum \((x^{*},s^{*})\) of (5). We can thus compute all the relevant gradients via differentiating the KKT conditions, using the implicit function theorem. We give all the calculation details in Appendix B.

Given the above observation, we then aim to construct the surrogate loss function by replacing the \(x_{1}^{*}\) and \(x_{2}^{*}\), which are supposed to solved using MILP (4), with a) \(_{1}\) that is solved from program (5) relaxation of the Stage 1 optimization problem, using the predicted parameters \(\) and b) \(_{2}\) that is solved from the program (5) relaxed version of Stage 2 optimization, using \(_{1}\) and the true parameters \(\). The only remaining question then, is, which values of \(\) do we use for the two relaxed problems?

Given a MILP in the form of (4), the interior-point based solver of Mandi and Guns  generates and solves (5) for a sequence of decreasing non-negative \(\), with a termination condition that \(\) cannot be smaller than some cutoff value. Thus, we simply choose the cutoff value to use as "\(\)" in (5), which then completes the definition of the surrogate loss \(\).

Algorithmically, we train the neural network on the surrogate loss \(\) as follows: given predicted parameters, we run the Mandi and Guns solver to get the optimal solution \((x^{*},s^{*})\) for the final value of \(\). We can then compute the gradient of the output solution with respect to any of the problem parameters using the calculations in Appendix B, combined with backpropagation, to yield \(}{w_{s}}\) according to Equation (3).

In Appendix C, we give three example application scenarios, along with their penalty functions, that our training approach can handle. These problems are: a) an alloy production problem, for factory trying to source ores under uncertainty in chemical compositions in the raw materials, b) a variant of the classic 0-1 knapsack with unknown weights and rewards, and c) a nurse roster scheduling problem with unknown patient load. We show explicitly in Appendix C how both stages of optimization can be formulated as MILPs for these applications, and apply the Appendix B calculations to yield gradient computation formulas for the surrogate loss \(\) for these problems.

A limitation of our approach is the requirement that both stages must be expressible as MILPs, constraining the optimization objectives to be linear in the MILP decision variables. This contrasts the Hu et al. framework  which handles non-linear penalties. We point out that even MILPs can handle some non-linearity by using extra decision variables: for example, the absolute-value function. Moreover, the Appendix B gradient calculations can be adapted to handle general differentiable non-linear objectives. We present only MILPs as a main overarching application for this paper because of their widespread use in discrete optimization, with readily available solvers.

## 5 Experimental Evaluation

We evaluate the proposed method2 on three benchmarks described in Section 4 and Appendix C. Table 1 reports the relevant benchmark problem sizes. We compare our method (2S) with the state of the art Predict+Optimize method, IntOpt-C , and \(5\) classical regression methods : ridge regression (Ridge), \(k\)-nearest neighbors (\(k\)-NN), classification and regression tree (CART), random forest (RF), and neural network (NN). All of these methods use their classic loss function to train the prediction models. At test time, to ensure the feasibility of the solutions when computing the post-hoc regret, we perform Stage 2 optimization on the estimated solutions for these classical regression methods before evaluating the final solution. Additionally, CombOptNet  is a different method focusing on learning unknowns in constraints, but with a different goal and loss function. We experimentally compare our proposed method with CombOptNet on the 0-1 knapsack benchmark--the only with available CombOptNet code. We also present a qualitative comparison in Section 6.

In the following experiments, we will need to take care to distinguish two-stage optimization as a training technique (Section 4) and as an evaluation framework (Section 3). We will denote our training method as "2S" in the experiments, and when we say "Two-Stage Predict+Optimize" framework, we always mean it as an evaluation framework. 2S is always evaluated according to the Two-Stage Predict+Optimize framework. As explained above, we will also evaluate all the classical training methods using the Two-Stage Predict+Optimize framework. For our comparison with the prior work of Hu et al. , we will also distinguish their training method and evaluation framework. The name "IntOpt-C" always refers to their training method using their correction function. We will simply call their evaluation framework the "Hu et al. framework" or with similar phrasing (see Section 2 to recall details). IntOpt-C will sometimes be evaluated using our new Two-Stage Predict+Optimize framework, and sometimes the prior framework of Hu et al.  using their correction function.

The methods of \(k\)-NN, RF, NN, and IntOpt-C as well as 2S have hyperparameters, which we tune via cross-validation. We include the hyperparameter types and chosen values in Appendix D. In the main paper we only report the prediction performances. See Appendix H for runtime comparisons.

Alloy Production ProblemThe alloy production problem is a covering LP, see Appendix C.1 for the practical motivation and LP model. Since Hu et al.  also experimented on this problem, we use it to compare our 2S method with IntOpt-C , using the same dataset and experimental setting.

We conduct experiments on the production of two real alloys: brass and an alloy blend for strengthening Titanium. For brass, \(2\) kinds of metal materials, Cu and Zn, are required . The blend of the two materials are, proportionally, \(req=[627.54,369.72]\). For the titanium-strengthening alloy, \(4\) kinds of metal materials, C, Al, V, and Fe, are required . The blend of the four materials are proportional to \(req=[0.8,60,40,2.5]\). We use the same real data as that used in IntOpt-C  as numerical values in our experiment instances. In this dataset , each un

 Problem name & Brass alloy production & Titanium-alloy production & 0-1 knapsack & Nurse scheduling problem \\   Dimension of \(\) & 10 & 10 & 10 & 315 \\  Number of constraints & 12 & 14 & 21 & 846 \\  Number of unknown parameters & 20 & 40 & 10 & 21 \\  Number of features (per parameter) & 4096 & 4096 & 4096 & 8 \\ 

Table 1: Relevant problem sizes of the three benchmarks.

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

only compare the proposed 2S training method with the classical approaches, using the Two-Stage Predict+Optimize framework for evaluation. Each NSP instance consists of 15 nurses, 7 days, and 3 shifts per day. The nurse preferences are obtained from the NSPLib dataset , which is widely used for NSP . The number of patients that each nurse can serve in one shift is randomly generated from , representing the fact that each nurse has different capabilities. Given that we are unable to find datasets specifically for the patient load demands and relevant prediction features, we follow the experimental approach of Demirovic et al.  and use real data from a different problem (the ICON scheduling competition) as the numerical values required for our experiment instances. In this dataset, the unknown number of patients per shift is predicted by \(8\) features.

Since there are far fewer features than the previous experiments, for both NN and 2S we use a smaller network structure: a 4-layer fully-connected network with 16 neurons per hidden layer. We use 210 instances for training and 90 instances for testing. Just like the first experiment, we use \(6\) scales of penalty factors (see Appendix C.3 for the penalty function): \(\) with i.i.d. entries drawn uniformly from \([0.25 0.015],[0.5 0.015],[1.0 0.015],[2.0 0.015],[4.0 0.015]\), and \([8.0 0.015]\).

Table 5 reports the mean post-hoc regrets and standard deviations across 10 runs for each approach on the NSP. The table shows that the proposed 2S method again has the best performance among all the training approaches. Our 2S method obtains at least 7.61%, 15.65%, 17.99%, 20.51%, 46.76%, and 62.49% smaller post-hoc regret than other classical methods when the penalty factor is \([0.25 0.015],[0.5 0.015],[1.0 0.015],[2.0 0.015],[4.0 0.015]\), and \([8.0 0.015]\) respectively.

Runtime AnalysisAppendix H gives the training times for each method. Most classical approaches are faster than our 2S method, although as shown their post-hoc regrets are much worse. In alloy production, the only setting where IntOpt-C applies, its running time is shorter but comparable with 2S. In 0-1 knapsack, the only problem with public CombOptNet code, the 2S method is much faster.

## 6 Literature Review

Section 1 already summarized prior works in Predict+Optimize, most of which focus on learning unknowns only in the objective. Only the Hu et al.  framework considers unknowns in constraints.

Here we summarize other works related to learning unknowns in optimization problem constraints, particularly those outside of Predict+Optimize. These works can be placed into two categories.

One category also considers learning unknowns in constraints, but with very different goals and measures of loss. For example, CombOptNet  and Nandwani et al.  focus on learning parameters so as to make the predicted optimal solution (first-stage solution in our proposed framework) as close to the true optimal solution \(x^{*}\) as possible in the solution space/metric. By contrast, our proposed framework explicitly formulates the two-stage framework and post-hoc regret in order to directly capture rewards and costs in application scenarios. Experiments on 0-1 knapsack in Section 5 show that these other methods yield worse predictive performance when evaluated on the post-hoc regret, under the proposed two-stage framework.

Another category gives ways to differentiate through LPs or LPs with regularizations, as a technical component in a gradient-based training algorithm. As mentioned in Section 4, these works can indeed be used in place of our proposed approach in Section 4/Appendix B. However, we point out that: (i) these other technical tools are essentially orthogonal to our primary contribution, which is the two-stage framework (Section 3), and (ii) nonetheless, experiments on the 0-1 knapsack in Appendix E demonstrate that our gradient calculation approach performs at least as well in post-hoc regret performance as other works, while being faster.

## 7 Summary

We proposed Two-Stage Predict+Optimize: a new, conceptually simpler and more powerful framework for the Predict+Optimize setting where unknown parameters can appear both in the objective and in constraints. We showed how the simpler perspective offered by the framework allows us to give a general training framework for all MILPs, contrasting prior work which apply only to covering and packing LPs. Experimental results demonstrate that our training method offers significantly better prediction performance over other classical and state-of-the-art approaches.